{"cell_type":{"df1b56b6":"code","403a5ef6":"code","82559de8":"code","1706b94f":"code","b80b7457":"code","eea90e22":"code","4401db57":"code","ee93f9cc":"code","ca67256d":"code","754a1cdc":"code","a307166c":"code","2752bcf5":"code","952cc56c":"code","829800ec":"code","56b7f472":"code","fb7d264a":"code","71ad2974":"code","25709b4b":"code","409f9cf1":"code","e08dded8":"code","b0b8573d":"code","f8d9da84":"code","fadf5b65":"code","906ae9b4":"code","364aee18":"code","eeeebd3b":"code","efc0e8e7":"code","a3d8ef1d":"code","8134bc8d":"code","eefa0c2a":"code","e8f2c371":"code","7bf1d786":"code","5c6b19cd":"code","7fce2fe0":"code","776e5276":"code","e827de07":"code","dd74526d":"code","1e5d288e":"code","6ea81a5d":"code","c2bb9361":"code","666956dc":"code","23a33c79":"code","2733a06b":"code","e11299cd":"code","6ab80e9c":"code","22f92bd3":"code","611cf1bd":"code","079eb7e5":"code","2a7e5b3b":"code","0ceefa9f":"code","c9be4239":"code","b50542c2":"code","e982f4ac":"code","3130a817":"code","a19330a4":"code","378e90ae":"code","678ab61a":"code","f0cf8c72":"code","e155231b":"code","37aeb66f":"code","dc4afa38":"code","f7260f9b":"code","1713e8e4":"code","1f5005e3":"code","464d24ff":"code","f2d2aa57":"code","fe40498c":"code","12bd8b2d":"code","98746180":"code","19e27d1d":"code","56437571":"code","81937b11":"code","84fd99d9":"code","d0cc9e0b":"code","c9b7de95":"code","093e2405":"code","3f2bb154":"code","bf83235b":"code","43f4ac41":"code","f7568d80":"code","d24e20be":"code","4128160c":"code","ac92e0df":"code","1eb422b0":"code","0441bdc8":"code","3b7696b3":"code","88b167c8":"code","497751d4":"code","3d146b06":"code","c18f3476":"code","4e9825b2":"code","109c9386":"code","0a93c5d2":"code","4eebd199":"code","c0382ddc":"code","27722b06":"code","8747b7ea":"code","782035f9":"code","a788b452":"code","88f02e0a":"code","08206395":"code","2b609cc4":"code","37bf403d":"code","093bdb50":"code","4d0a86fd":"code","95d2f7b7":"code","647f527a":"code","533d87aa":"code","eef5dd1c":"code","0e15ee4f":"code","8941e19f":"code","0fbcf489":"code","8bddbf02":"code","b188fa41":"code","343ab4a0":"code","2927fad7":"code","4338b751":"code","d7ae1660":"code","1afec8fb":"code","c105a51c":"code","789468d3":"code","e75c2745":"code","c7408592":"code","3ccd6776":"code","e1bbd30f":"code","edae4508":"code","4b7cf984":"code","75658078":"code","9ae4c267":"code","6a485daa":"code","f7736931":"code","fda862d8":"code","e39a1cfa":"code","c284e84c":"code","f6f3d554":"code","00122bcc":"code","bdcefe42":"code","acf5f959":"code","337d7214":"code","81fc904c":"code","84487e0e":"code","5c61e07c":"code","9844af2c":"code","36f32d35":"code","7e941cdd":"code","6e202cd8":"code","e27e1407":"code","c1a94e22":"code","02cbaadf":"code","d274f7ec":"code","e71903cf":"code","49c77666":"code","343e7b71":"code","30ccf2b8":"code","1a759dd4":"code","9a624bdf":"code","ca6b2628":"code","47ba9e3b":"code","f5b5f8f9":"code","6d3ab812":"code","a70b8fa5":"code","7e24f362":"code","75e64f1c":"code","377aa908":"code","72498adf":"code","e26888d6":"code","2dced4b2":"code","29365050":"code","7793d113":"code","875e5b65":"code","a9ec56ef":"code","4edac96e":"code","a0ea580d":"code","c41b7ef0":"code","110f003e":"code","e200b5ef":"code","f9d6dc8c":"code","26330b2a":"code","b40830f2":"code","35c7bb9d":"code","80510aca":"code","b3051cb6":"code","adc2cb67":"code","c561f190":"code","8097fadb":"code","240f7cf8":"code","dc8e318b":"code","5d7f8786":"code","94667b6a":"code","c8186652":"code","b6ce06ac":"code","ce5ca8be":"code","39d3a259":"code","563c5f3b":"code","d8b3a45b":"code","a7bc0cc6":"code","cbcfc9db":"code","b3101843":"code","e9fa5e23":"code","c4cdaeae":"code","07aff470":"code","d17503de":"code","297e9797":"code","f46f7d73":"code","befe81a9":"code","21e9302d":"code","88edf145":"code","3b8f36f1":"code","28f303d9":"code","557681bd":"code","8d78a7af":"code","a80522b5":"code","41782243":"code","93311a61":"code","cadb84c9":"code","fdb6805f":"code","af5307b7":"code","505248a8":"code","062bfbb3":"code","e3c5e49c":"code","af2a17ba":"code","4ab5349f":"markdown","825a012f":"markdown","b71f6ea8":"markdown","fbe1b964":"markdown","0275539b":"markdown","ef3aa7d1":"markdown","9c664408":"markdown","ef6efd06":"markdown","d6ea0a08":"markdown","967a16ff":"markdown","1aefbc00":"markdown","b37367ff":"markdown","47ddf670":"markdown","6b4b60af":"markdown","7de131dc":"markdown","09692fd1":"markdown","64455e7c":"markdown","9f51ff53":"markdown","102ef504":"markdown","77becf7c":"markdown","c7cdf2b1":"markdown","ccd27181":"markdown","5b56c608":"markdown","3c273fae":"markdown","acc29d86":"markdown","4443f9c7":"markdown","2afd1452":"markdown","8c493de5":"markdown","40614e81":"markdown","9f1072dd":"markdown","27967746":"markdown","31151fd4":"markdown","1c4f42e2":"markdown","584100cd":"markdown","cbb28bfc":"markdown","a284e05a":"markdown","b1e0f116":"markdown","37d2ae11":"markdown","193dc4ac":"markdown","da18b939":"markdown","ee4cb40e":"markdown","9c35951e":"markdown","ae5ccbe9":"markdown","377b9ede":"markdown","fe8ce734":"markdown","826657ff":"markdown","a787db2b":"markdown","bd8d8fbc":"markdown","f7a44444":"markdown","c2ecde6b":"markdown","6b1fa5d7":"markdown","c59eac82":"markdown","b7e8d290":"markdown","d0b5f02a":"markdown","e0a17904":"markdown","5f41d431":"markdown","d9a9788a":"markdown","bb52ba36":"markdown","090e26ce":"markdown","4b9f1dcd":"markdown","798d0f2e":"markdown","928328c5":"markdown","68c53ee3":"markdown","92afd8f7":"markdown","bce650f9":"markdown","c04883b3":"markdown","8342deaa":"markdown","abd0d0dd":"markdown","2244ea89":"markdown","8a8f8798":"markdown","695d5456":"markdown","cbbb0b6e":"markdown","1c664436":"markdown","2b97a8d5":"markdown","62ea47a1":"markdown","ff0761c3":"markdown","7f051ac2":"markdown","00cb0481":"markdown","434b6d7a":"markdown","a8cf73bf":"markdown","b5b17c3c":"markdown","7a87962a":"markdown","6fec3540":"markdown"},"source":{"df1b56b6":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","403a5ef6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing, impute\nfrom IPython.display import display\n\nplt.style.use('ggplot')","82559de8":"random_state = 42","1706b94f":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col=0)\nX_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col=0)","b80b7457":"column_y = df_train.columns.difference(X_test.columns)[0]\nX = df_train.drop(columns=column_y)\ny = df_train[column_y]","eea90e22":"X.index.intersection(X_test.index)","4401db57":"df_combined = pd.concat([X.assign(set='train'),\n                         X_test.assign(set='test')], axis=0)","ee93f9cc":"df_combined.info()","ca67256d":"pd.concat([X.isna().sum().rename('train'),\n           X_test.isna().sum().rename('test')], axis=1)\\\n    .astype('int')\\\n    .sort_values(['train', 'test'], ascending=False)\\\n    .style.background_gradient(vmin=0, vmax=max([X.shape[0], X_test.shape[0]]))","754a1cdc":"with pd.option_context('display.max_columns', None):\n    display(df_train.sample(10))","a307166c":"columns_num = X.select_dtypes(exclude='object').columns\ncolumns_obj = X.select_dtypes('object').columns","2752bcf5":"len(columns_num), len(columns_obj)","952cc56c":"fig, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(X.isna().T, yticklabels=True, ax=ax, cbar=False)","829800ec":"fig, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(X_test.isna().T, yticklabels=True, ax=ax, cbar=False)","56b7f472":"fig, axs = plt.subplots(ncols=4, nrows=9, figsize=(20, 40))\n\nfor col, ax in zip(columns_num, axs.flat):\n    sns.histplot(data=df_combined,\n                 x=col,\n                 hue='set',\n                 stat='probability',\n                 #multiple='dodge',\n                 common_norm=False,\n                 ax=ax)","fb7d264a":"fig, axs = plt.subplots(ncols=4, nrows=11, figsize=(20, 40))\n\nfor col, ax in zip(columns_obj, axs.flat):\n    sns.histplot(data=df_combined,\n                 y=df_combined[col].fillna('NULL'),\n                 hue='set',\n                 ax=ax)","71ad2974":"dict_string2value = {\n    'Street': {'Pave': 2, 'Grvl': 1},\n    'Alley': {'Pave': 2, 'Grvl': 1, 'NULL':0}, # np.NaN: 0\n    'LotShape': {'Reg':3, 'IR1':2, 'IR2':1, 'IR3':0},\n    'LandContour': {'Lvl':3, 'Bnk':2, 'HLS':1, 'Low':0},\n    'Utilities': {\"AllPub\": 4, \"NoSewr\": 3, \"NoSeWa\":2, \"ELO\": 1, 'NULL':0}, # np.NaN: 0\n    'LandSlope': {\"Gtl\": 3, \"Mod\": 2, \"Sev\": 1},\n    \"ExterQual\": {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1},\n    \"ExterCond\": {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1}, \n    \"BsmtQual\": {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1, 'NULL':0}, # np.NaN: 0\n    \"BsmtCond\": {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1, 'NULL':0}, # np.NaN: 0\n    \"BsmtExposure\": {\"Gd\":4, \"Av\":3, \"Mn\":2, \"No\":1, 'NULL':0}, # np.NaN: 0\n    \"BsmtFinType1\": {\"GLQ\":6, \"ALQ\":5, \"BLQ\":4, \"Rec\":3, \"LwQ\":2, \"Unf\":1, 'NULL':0}, # np.NaN: 0\n    \"BsmtFinType2\": {\"GLQ\":6, \"ALQ\":5, \"BLQ\":4, \"Rec\":3, \"LwQ\":2, \"Unf\":1, 'NULL':0}, # np.NaN: 0\n    \"HeatingQC\": {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1},\n    \"CentralAir\": {\"Y\":1, \"N\":0},\n    \"Electrical\": {\"SBrkr\":5, \"FuseA\":4, \"FuseF\":3, \"FuseP\":2, \"Mix\":1, 'NULL':0}, # np.NaN: 0\n    \"KitchenQual\": {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1, 'NULL':0}, # np.NaN: 0\n    \"Functional\": {\"Typ\":8, \"Min1\":7, \"Min2\":6, \"Mod\":5, \"Maj1\":4, \"Maj2\":3, \"Sev\":2, \"Sal\":1, 'NULL':0}, # np.NaN: 0\n    \"FireplaceQu\": {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1, 'NULL':0}, # np.NaN: 0\n    \"GarageFinish\": {\"Fin\":3, \"RFn\":2, \"Unf\":1, 'NULL':0}, # np.NaN: 0\n    \"GarageQual\": {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1, 'NULL':0}, # np.NaN: 0\n    \"GarageCond\": {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1, 'NULL':0}, # np.NaN: 0\n    \"PavedDrive\": {\"Y\":2, \"P\":1, \"N\":0},\n    \"PoolQC\":  {\"Ex\":4, \"Gd\":3, \"TA\":2, \"Fa\":1, 'NULL':0}, # np.NaN: 0\n}\n\ndict_na_1 = {\n    \"Alley\":0,\n    \"Utilities\":0,\n    \"BsmtQual\":0,\n    \"BsmtCond\":0,\n    \"BsmtExposure\":0,\n    \"BsmtFinType1\":0,\n    \"BsmtFinType2\":0,\n    \"Electrical\":0,\n    \"KitchenQual\":0,\n    \"Functional\":0,\n    \"FireplaceQu\":0,\n    \"GarageFinish\":0,\n    \"GarageQual\":0,\n    \"GarageCond\":0,\n    \"PoolQC\":0,\n}\n\n# dict_1 = set(list(dict_string2value.keys())+list(dict_na_1.keys()))\n\ndef discrete_integer(X):\n    #return X.replace(dict_string2value).fillna(dict_na_1)\n    columns = list(dict_na_1.keys())\n    X = X.copy()\n    X[columns] = X[columns].fillna('NULL')\n    return X.replace(dict_string2value)\n\ndef to_ordered_categories(X):\n    X = X.copy()\n    for k, v in dict_string2value.items():\n        categories = list(v.values())\n        X[k] = pd.Categorical(X[k], categories=categories, ordered=True)\n    return X\n\nordered_categories = list(dict_string2value.keys())\n\ndf_combined_1 = discrete_integer(df_combined)\nX_1 = discrete_integer(X)\nX_test_1 = discrete_integer(X_test)","25709b4b":"sns.catplot(data=X_1[dict_string2value].assign(y=y).melt(id_vars=['y']),\n            x='value',\n            col=\"variable\",\n            y='y',\n            kind='box',\n            sharex=False,\n            col_wrap=5,\n            height=5)","409f9cf1":"X_1[dict_string2value]\\\n    .corrwith(y)\\\n    .sort_values(ascending=False)\\\n    .to_frame()\\\n    .style\\\n    .bar(vmin=-1, align='zero', vmax=1)","e08dded8":"columns_na = df_combined.columns[(df_combined_1.isna().sum() > 0)].sort_values().tolist()\nprint(columns_na)","b0b8573d":"df_combined_1[columns_na]\\\n    .apply('nunique')\\\n    .sort_values()\\\n    .to_frame(name='number of unique values')\\\n    .style\\\n    .bar()","f8d9da84":"columns_na_cat = df_combined_1[columns_na].apply('nunique').loc[lambda x: x<20].index\n\nunique_values = pd.DataFrame(\n    [[c, X_1[c].unique(),[x for x in X_test_1[c].unique() if not(x in X_1[c].unique())]]\n      for c in columns_na_cat],\n    columns=['column name', 'train unique values', 'other values in test']\n).set_index('column name')\n\nunique_values.join([X_1[columns_na_cat].isna().sum().rename('NA train'),\n                    X_test_1[columns_na_cat].isna().sum().rename('NA test')]).style","fadf5b65":"columns_na_num = df_combined_1[columns_na].apply('nunique').loc[lambda x: x>=20].index\n\ndf_combined_1[columns_na_num].describe(include='all')","906ae9b4":"dict_na_2_num = [\n    \"BsmtFinSF1\",\n    \"BsmtFinSF2\",\n    \"BsmtUnfSF\",\n    \"GarageArea\",\n    #\"GarageYrBlt\",\n    #\"LotFrontage\",\n    \"MasVnrArea\",\n    \"TotalBsmtSF\",\n]\ndict_na_2_num = {k:0 for k in dict_na_2_num}\n\ndict_na_2_other = {\n    \"BsmtFullBath\": 0,\n    \"BsmtHalfBath\": 0,\n    # \"Exterior1st\": \"Other\"\n    \"Exterior2nd\": \"Other\",\n    \"GarageCars\": 0,\n    \"GarageType\": \"NA\",\n    \"MasVnrType\": \"None\",\n    \"MiscFeature\": \"NA\", # \"Elev\" is never used\n    \"Fence\":\"NA\",\n}\n\ndef discrete_na(X):\n    return X.fillna(dict_na_2_num).fillna(dict_na_2_other)\n\nX_2 = discrete_na(X_1)\nX_test_2 = discrete_na(X_test_1)\ndf_combined_2 = discrete_na(df_combined_1)","364aee18":"display(X_2\\\n    .loc[:, lambda x: x.isna().sum() > 0]\\\n    .describe(include='all'))\n\ndisplay(X_test_2\\\n    .loc[:, lambda x: x.isna().sum() > 0]\\\n    .describe(include='all'))","eeeebd3b":"X[['LotFrontage']].describe()","efc0e8e7":"sns.catplot(\n    data=df_train.assign(LotFrontage_isna=df_train.LotFrontage.isna())\\\n                 [['LotConfig', 'LotFrontage_isna', 'SalePrice', 'Neighborhood']]\\\n                 .melt(id_vars=['LotFrontage_isna', 'SalePrice']),\n#     data=X.select_dtypes('object').assign(LotFrontage_isna=df_train.LotFrontage.isna(), SalePrice=y).melt(id_vars=['LotFrontage_isna', 'SalePrice']),\n    y='value',\n    #hue='LotFrontage_isna',\n    x='SalePrice',\n    col='variable',\n    hue='LotFrontage_isna',\n    #col_wrap=5,\n    kind='boxen',\n    sharey=False,\n    sharex=True,\n    height=10,\n)","a3d8ef1d":"columns_isna_lotfrontage_different_top = ~(df_train.loc[df_train.LotFrontage.isna(), :].describe(include='object') == df_train.loc[~df_train.LotFrontage.isna(), :].describe(include='object')).loc['top', :]\ncolumns_isna_lotfrontage_different_top = columns_isna_lotfrontage_different_top[columns_isna_lotfrontage_different_top].index.tolist()\ncolumns_isna_lotfrontage_different_top","8134bc8d":"# sns.catplot(\n#     data=df_train.assign(LotFrontage_isna=df_train.LotFrontage.isna())[columns_isna_lotfrontage_different_top+['LotFrontage_isna', 'SalePrice']].melt(id_vars=['LotFrontage_isna', 'SalePrice']),\n# #     data=X.select_dtypes('object').assign(LotFrontage_isna=df_train.LotFrontage.isna(), SalePrice=y).melt(id_vars=['LotFrontage_isna', 'SalePrice']),\n#     x='value',\n#     #hue='LotFrontage_isna',\n#     y='SalePrice',\n#     row='variable',\n#     col='LotFrontage_isna',\n#     #col_wrap=5,\n#     kind='boxen',\n#     sharex=False,\n# )","eefa0c2a":"columns_isna_lotfrontage_different_top = df_train.select_dtypes('number').columns[np.isclose(\n    df_train.loc[df_train.LotFrontage.isna(), :].describe(include='number').loc['mean', :],\n    df_train.loc[~df_train.LotFrontage.isna(), :].describe(include='number').loc['mean', :],\n    rtol=1e-3\n)].tolist()\ncolumns_isna_lotfrontage_different_top","e8f2c371":"# sns.catplot(\n#     data=df_train[columns_isna_lotfrontage_different_top].assign(LotFrontage_isna=df_train['LotFrontage'].isna()).melt(id_vars='LotFrontage_isna'),\n#     x='LotFrontage_isna',\n#     y='value',\n#     col='variable',\n#     col_wrap=5,\n#     kind='box',\n#     sharey=False\n# )","7bf1d786":"def numerical_na(X):\n    return X.fillna({\"LotFrontage\":0})","5c6b19cd":"# columns_cat = df_combined_2\\\n#     .drop(columns='set')\\\n#     .apply('nunique')\\\n#     .loc[lambda x: x<20]\\\n#     .index\n\ncolumns_cat = df_combined_2\\\n  .drop(columns='set')\\\n  .select_dtypes('object')\\\n  .columns\n\nunique_values = pd.DataFrame(\n    [[c, X_2[c].unique(),X_2[c].nunique(), [x for x in X_test_2[c].unique() if not(x in X_2[c].unique())]]\n      for c in columns_cat],\n    columns=['column name', 'train unique values', 'train n unique values', 'other values in test']\n)\n\nunique_values.set_index('column name').style","7fce2fe0":"X_2.query(\"Exterior1st=='Wd Sdng'\")['Exterior2nd'].value_counts()","776e5276":"X_2.query(\"Exterior1st=='WdShing'\")['Exterior2nd'].value_counts()","e827de07":"X_2.query(\"Exterior2nd=='Wd Shng'\")['Exterior1st'].value_counts()","dd74526d":"dict_clean_3 = {\n    \"Exterior2nd\": {\"CmentBd\": \"CemntBd\",\n                    \"Brk Cmn\":\"BrkComm\",\n                    \"Wd Shng\": \"WdShing\"}\n}\n\ndef wrong_categories(X):\n    return X.replace(dict_clean_3)\n\nX_3 = wrong_categories(X_2)\nX_test_3 = wrong_categories(X_test_2)\ndf_combined_3 = wrong_categories(df_combined_2)","1e5d288e":"df_combined_3[[\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\", \"YrSold\"]].describe()","6ea81a5d":"with pd.option_context('display.max_columns', None):\n    display(df_combined[['YearBuilt', 'GarageYrBlt', 'YrSold', 'YearRemodAdd', 'set']].sort_values('GarageYrBlt', ascending=False).head(10))","c2bb9361":"df_combined_3\\\n    .query(\"GarageYrBlt<YearBuilt|YrSold<YearBuilt|YearRemodAdd<YearBuilt\")\\\n    [['YearBuilt', 'GarageYrBlt', 'YrSold', 'YearRemodAdd', 'set']]\\\n    .assign(issue_Garage=lambda x: x['GarageYrBlt']<x['YearBuilt'])","666956dc":"X['GarageYrBlt'].isna().sum()","23a33c79":"print('Nb of rows with no Garage', X['GarageQual'].isna().sum())\nprint('Nb of rows with GarageYrBlt==NaN', X['GarageYrBlt'].isna().sum())","2733a06b":"def wrong_dates(X_init):\n    X = X_init.copy()\n\n    # X.loc[X['YrSold'] < X['YearBuilt'], 'YrSold'] = X.loc[X['YrSold'] < X['YearBuilt'], 'YearBuilt']\n    # X.loc[X['YearRemodAdd'] < X['YearBuilt'], 'YearRemodAdd'] = X.loc[X['YearRemodAdd'] < X['YearBuilt'], 'YearBuilt']\n    # X.loc[X['GarageYrBlt'] < X['YearBuilt'], 'GarageYrBlt'] = X.loc[X['GarageYrBlt'] < X['YearBuilt'], 'YearBuilt']\n    \n    X['GarageYrBlt'] = X['GarageYrBlt'].fillna(0)\n    X.loc[X['GarageYrBlt'] > 2010, 'GarageYrBlt'] = X.loc[X['GarageYrBlt'] > 2010, 'YearRemodAdd']\n    return X","e11299cd":"X['GarageYrBlt'].isna().sum()","6ab80e9c":"def preproc(X):\n    return X.pipe(discrete_integer)\\\n            .pipe(discrete_na)\\\n            .pipe(wrong_categories)\\\n            .pipe(wrong_dates)\\\n            .pipe(to_ordered_categories)#\\\n            #.pipe(numerical_na)","22f92bd3":"X_pre = preproc(X)\nX_test_pre = preproc(X_test)\ndf_combined_pre = preproc(df_combined)","611cf1bd":"X_pre['Utilities']","079eb7e5":"X_pre_cat_list = X_pre.select_dtypes('category').apply(lambda x: x.cat.categories)\nX_test_pre_cat_list = X_test_pre.select_dtypes('category').apply(lambda x: x.cat.categories)\n\nfor column in X_pre_cat_list.index:\n    same_category = X_pre_cat_list[column].equals(X_test_pre_cat_list[column])\n    if same_category:\n        print(column, 'OK')\n    else:\n        print(column, X_pre_cat_list[column], X_test_pre_cat_list[column])","2a7e5b3b":"X_pre.nunique()\\\n    .rename('nunique')\\\n    .to_frame()\\\n    .join([X_pre.isna().sum().rename('isna'),\n           X_pre.dtypes.rename('type'),\n           X_test_pre.isna().sum().rename('isna test')])\\\n    .sort_values('nunique')\\\n    .style.bar()","0ceefa9f":"columns_discrete = X.select_dtypes('object').columns.tolist()\ncolumns_discrete = columns_discrete + ['MSSubClass', 'OverallQual', 'OverallCond']\ncolumns_discrete, len(columns_discrete)","c9be4239":"ordered_categories","b50542c2":"import statsmodels.api as sm\nimport scipy.stats as stats","e982f4ac":"fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\nsns.histplot(x=y, ax=axs[0])\nsm.ProbPlot(y).qqplot(line='s', ax=axs[1], fmt='k')\n\nplt.tight_layout()","3130a817":"fig, ax = plt.subplots(figsize=(10, 10))\nstats.boxcox_normplot(y, -20, 20, plot=ax)\nplt.show()","a19330a4":"maxlog = preprocessing.PowerTransformer().fit(y.to_frame()).lambdas_[0]\nllf = stats.yeojohnson_llf(maxlog, y)\n\nmaxlog, llf","378e90ae":"y_boxcox = preprocessing.PowerTransformer(method='box-cox', standardize=False).fit_transform(y.to_frame())\ny_boxcox = pd.Series(y_boxcox[:,0], index=y.index, name=y.name)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\nsns.histplot(x=y_boxcox, ax=axs[0])\nsm.ProbPlot(y_boxcox).qqplot(line='s', ax=axs[1], fmt='k')\n\nplt.tight_layout()","678ab61a":"X_plot = X_pre.drop(columns=columns_discrete).select_dtypes('number')\n\nfig, axs = plt.subplots(X_plot.shape[1], 2, figsize=(12, 100))\n\npower_info = {\"maxlog\":{}, \"llf\":{}}\n\nfor i, c in enumerate(X_plot.columns):\n    zero_skewed = (X_plot[c] == 0).sum() > (X_plot.shape[0] \/ 3)\n    is_negative = X_plot[c].min() <= 0\n\n    sns.histplot(x=X_plot[c], ax=axs[i, 0])\n    sm.ProbPlot(X_plot[c]).qqplot(line='s', ax=axs[i, 1], fmt='k')\n\n    if zero_skewed:\n        x = X_plot[c].replace(0, np.NaN).dropna()\n    else:\n        x = X_plot[c].dropna()\n\n    if is_negative:\n        maxlog = preprocessing.PowerTransformer().fit(x.to_frame()).lambdas_[0]\n        llf = stats.yeojohnson_llf(maxlog, x)\n    else:\n        maxlog = preprocessing.PowerTransformer(method='box-cox').fit(x.to_frame()).lambdas_[0]\n        llf = stats.boxcox_llf(maxlog, x)\n    power_info[\"maxlog\"][c], power_info[\"llf\"][c] = maxlog, llf\n\n    if zero_skewed:\n        axs[i, 1].set_title(f\"w\/o 0: $\\lambda$={maxlog:.3f}, llf={llf:.3f}\")\n    else:\n        axs[i, 1].set_title(f\"$\\lambda$={maxlog:.3f}, llf={llf:.3f}\")\n\nplt.tight_layout()","f0cf8c72":"pd.DataFrame(power_info).sort_values(\"llf\").style.background_gradient()","e155231b":"area_columns = X.columns[X.columns.str.contains('MiscVal|Area|SF|Porch$', case=True)]\nlen(area_columns)","37aeb66f":"X_pre[area_columns].describe(include='all')","dc4afa38":"def area_scaling(X):\n    X = X.copy()\n    area_columns = X.columns[X.columns.str.contains('MiscVal|Area|SF|Porch$', case=True)]\n    X[area_columns] = preprocessing.power_transform(X[area_columns])\n    return X","f7260f9b":"y_proc = np.log(y)","1713e8e4":"# columns_discrete, ordered_categories\ncolumns_category = X.select_dtypes('object').columns.tolist() + ['MSSubClass']\ncolumns_object = X_pre.select_dtypes('object').columns.tolist() + ['MSSubClass']\ncolumns_numeric = X.drop(columns=columns_object).columns.tolist()\ncolumns_object","1f5005e3":"def force_clean(X=X_pre, X_train=X_pre, final_imputer=True):\n    X_clean = X.copy()\n    X_train = X_train.copy()\n    \n    # Bug in sklearn 0.23 with SimpleImputer for categorical values (convert to numerical)\n    #imputer = impute.SimpleImputer(strategy='most_frequent').fit(X_train[columns_category])\n    #X_clean[columns_category] = imputer.transform(X_clean[columns_category])\n    \n    columns_object = X.select_dtypes('object').columns.tolist()\n    # Temporary solution\n    X_most_frequent = X_train[columns_object].apply(lambda x: x.mode()[0])\n    X_train[columns_object] = X_train[columns_object].fillna(X_most_frequent)\n    X_clean[columns_object] = X_clean[columns_object].fillna(X_most_frequent)\n    \n    encoder = preprocessing.OrdinalEncoder(dtype=int).fit(X_train[columns_object])\n    X_train[columns_object] = encoder.transform(X_train[columns_object])\n    X_clean[columns_object] = encoder.transform(X_clean[columns_object])\n    \n    X_train[columns_object] = X_train[columns_object].astype('category')\n    X_clean[columns_object] = X_clean[columns_object].astype('category')\n    \n    if final_imputer:\n        imputer = impute.KNNImputer().fit(X_train)\n        X_nona = pd.DataFrame(imputer.transform(X_clean), index=X_clean.index, columns=X_clean.columns)\n        X_clean = X_clean.fillna(X_nona)\n    \n    return X_clean\n\nX_clean = force_clean(X_pre, X_pre)\nX_clean_test = force_clean(X_test_pre, X_pre)","464d24ff":"import lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor","f2d2aa57":"#rng = np.random.RandomState(42)\nrng = 42","fe40498c":"#y_encoded, _ = pd.cut(y_proc, 8).factorize()\ny_encoded, _ = pd.qcut(y_proc, 10).factorize()\nsns.histplot(x=y_proc, hue=y_encoded, multiple='stack')","12bd8b2d":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=rng)\ny_encoded, _ = pd.qcut(y, q=10).factorize()\n\ndef get_folds(X_clean=X_clean, y=y_proc, y_encoded=y_encoded):\n    #y_encoded = pd.qcut(preprocessing.power_transform(y.to_frame())[:, 0], q=20)\n    #y_encoded = preprocessing.LabelEncoder().fit_transform(y_encoded)\n    y_encoded, _ = pd.qcut(y, q=10).factorize()\n    return folds.split(X_clean, y_encoded)","98746180":"folds_index = pd.DataFrame(np.full((y.shape[0], 5), -1),\n                           index=y.index,\n                           columns=[f'fold{x}' for x in range(5)])\n\nfor i, (train_index, test_index) in enumerate(get_folds()):\n    folds_index.iloc[train_index,i] = 0\n    folds_index.iloc[test_index, i] = 1\n\nfolds_index.apply(lambda x: x.value_counts())","19e27d1d":"fig, axs = plt.subplots(2, 3, figsize=(20, 10))\naxs = axs.flat\n\nfor i, c in enumerate(folds_index.columns):\n    sns.histplot(x=y_proc,\n                hue=folds_index.iloc[:, i],\n                ax=axs[i])\n\nsns.histplot(x=y_proc, ax=axs[5])","56437571":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.dummy import DummyRegressor","81937b11":" cross_val_score(DummyRegressor(), X_clean, y_proc, cv=5, scoring='neg_root_mean_squared_error')","84fd99d9":"from sklearn.compose import make_column_selector\n\ndef score_models(X, y=y_proc, cv=None, clean=False, pipe=None, return_search=False, return_styled=True):\n    \n    if clean:\n        X = force_clean(X, X)\n    else:\n        X = X.copy()\n    \n    if cv is None:\n        y_encoded, _ = pd.qcut(y, q=10).factorize()\n        cv = folds.split(X, y_encoded)\n    \n    models = [\n        lgb.LGBMRegressor(objective=\"rmse\", random_state=random_state, n_jobs=1),\n        #SVR(),\n        Ridge(),\n        #ElasticNet(),\n    ]\n    \n    estimator = Pipeline([(\"proc\", pipe),\n                          (\"_dtype_\", 'passthrough'),\n                          (\"model\", DummyRegressor())])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror',\n                                 eval_metric='rmse',\n                                 random_state=random_state,\n                                 #enable_categorical=True,\n                                 n_jobs=1)\n        \n    # bugfix XGB<0.15 with `enable_categorical` parameter missing    \n    def convert_cat_to_int(X):\n        if not isinstance(X, pd.DataFrame):\n            return X\n        cat_cols = X.select_dtypes(\"category\").columns\n        X[cat_cols] = X[cat_cols].apply(lambda x: x.cat.codes)\n        return X\n    \n    transformer_cat_to_int = preprocessing.FunctionTransformer(convert_cat_to_int)\n    \n    param_grid = [{'_dtype_':['passthrough'], 'model':models},\n                  {'_dtype_':[transformer_cat_to_int], 'model': [xgb_model]}]\n    \n    gscv = GridSearchCV(\n        estimator=estimator,\n        param_grid=param_grid,\n        scoring='neg_root_mean_squared_error',\n        n_jobs=4,\n        refit=False,\n        cv=cv,\n        error_score='raise',\n        return_train_score=True,\n    )\n    gscv.fit(X, y)\n    \n    def convert_class_to_name(x):\n        if isinstance(x, (str,int, float, bool)):\n            return x\n        else:\n            return type(x).__name__\n\n    r = pd.DataFrame(gscv.cv_results_)\n    r = r.drop(r.filter(regex='^split|^params$|param__dtype_|rank_test_score|std_\\w+_time|score_time', axis=1).columns, axis=1)\n    r = r.applymap(convert_class_to_name).set_index(\"param_model\").sort_index()\n    r[['mean_train_score', 'mean_test_score']] *= -1\n    \n    if return_styled:\n        r = r.style.bar(subset=['mean_fit_time', 'mean_train_score', 'mean_test_score'], vmin=0)\n    \n    if return_search:\n        return (gscv, r)\n    else:\n        return r","d0cc9e0b":"score_models(X_clean)","c9b7de95":"base_score = score_models(X_clean, return_styled=False)\n\ndef test_new(*args, X=X_clean, pipe=None, return_styled=True, base_score=base_score):\n    \n    new_series = []    \n    for i, new in enumerate(args):\n        if isinstance(new, pd.Series):\n            new = new.rename(f'new{i}')\n        new_series.append(new)\n        \n    score = score_models(X.join(new_series), pipe=pipe, return_styled=False)\n    \n    r = base_score.add_prefix('base-').join(score.add_prefix('new-'))\n    r = r.assign(diff_mean=r['new-mean_test_score'] - r['base-mean_test_score'],\n                 diff_std=r['new-std_test_score'] - r['base-std_test_score'])\n    if return_styled:\n        r = r.style.bar(subset=['base-mean_fit_time',\n                                'new-mean_fit_time',\n                                'base-mean_train_score',\n                                'new-mean_train_score',\n                                'base-mean_test_score',\n                                'new-mean_test_score'], vmin=0)\n    return r","093e2405":"test_new(\n    X_clean['GarageType'] == 0,\n    X_clean['FireplaceQu'] == 0,\n    X_clean['TotalBsmtSF'] == 0,\n)","3f2bb154":"X_pre.isna().sum().loc[lambda x: x>0], X_test_pre.isna().sum().loc[lambda x: x>0]","bf83235b":"sns.histplot(X_clean['LotFrontage'].where(X_pre['LotFrontage'].isna()))","43f4ac41":"def test_lot_frontage(X_pre=X_pre):\n    X_pre = X_pre.copy()\n    X_pre[['LotFrontage']] = X_pre[['LotFrontage']].fillna(0)\n    return test_new(X=force_clean(X_pre, X_pre))\ntest_lot_frontage()","f7568d80":"from sklearn.linear_model import RidgeCV\n\ny_lotfrontage = X_pre['LotFrontage']\nX_lotfrontage = X_clean.drop(columns=['LotFrontage'])\n\ny_lotfrontage_train = y_lotfrontage.dropna()\nX_lotfrontage_train = X_lotfrontage.loc[~y_lotfrontage.isna(), :]\nX_lotfrontage_test = X_lotfrontage.loc[y_lotfrontage.isna(), :]\n\nlr_lotfrontage = RidgeCV().fit(X_lotfrontage_train, y_lotfrontage_train)\nlot_frontage = X_pre['LotFrontage'].copy()\nlot_frontage.loc[X_pre['LotFrontage'].isna()] = lr_lotfrontage.predict(X_lotfrontage_test)\n\ntest_new(X=X_clean.assign(LotFrontage=lot_frontage))","d24e20be":"def test_dates_missing(X_clean=X_clean, X=X):\n    X = X.copy()\n    X_clean = X_clean.copy()\n    \n    for col in ['YearRemodAdd', 'GarageYrBlt', 'YrSold']:\n        X_clean.loc[X[col].isna(), col] = X_clean.loc[X[col].isna(), 'YearBuilt']\n    \n    return test_new(X=X_clean)\n\ntest_dates_missing()","4128160c":"test_new(X=force_clean(X_pre, X_pre, final_imputer=False), pipe=impute.KNNImputer())","ac92e0df":"pipe_area = ColumnTransformer([('area', preprocessing.PowerTransformer(), area_columns)],\n                              remainder='passthrough')\ntest_new(pipe=pipe_area)","1eb422b0":"pipe_area = ColumnTransformer([('area', preprocessing.FunctionTransformer(lambda x: np.sqrt(x+1)*(x!=0)), area_columns)],\n                              remainder='passthrough')\ntest_new(pipe=pipe_area)","0441bdc8":"from sklearn.feature_selection import mutual_info_regression\n\ndef show_mi(X, y=y, clean=False):\n    if clean:\n        X = force_clean(X)\n    \n    columns_discrete = X.select_dtypes(['object', 'category'])\n    discrete_features = X.columns.isin(columns_discrete)\n    \n    mi = mutual_info_regression(X, y,\n                                random_state=random_state,\n                                discrete_features=discrete_features)\n\n    s = pd.Series(mi, index=X.columns, name='mutual_info_regression')\\\n          .sort_values(ascending=False)\n          \n    return s, s.to_frame().style.bar()","3b7696b3":"s_mi, mi = show_mi(X_clean)\nmi","88b167c8":"X_mi = X_clean.loc[:, s_mi > 1e-4]\nX_clean.columns.difference(X_mi.columns)","497751d4":"test_new(X=X_mi)","3d146b06":"def proc_univariate(X=X_pre, X_train=X_pre):\n    X = X.copy()\n    #X[['LotFrontage']] = X[['LotFrontage']].fillna(0)\n    X = force_clean(X, X_train)   \n    \n    s_mi, _ = show_mi(X)\n    X = X.loc[:, s_mi > 1e-4]\n    \n    filtered_area_columns = X.columns.intersection(area_columns)\n    sqrt_transform = preprocessing.FunctionTransformer(lambda x: np.sqrt(x+1)*(x!=0))\n    pipe_area = ColumnTransformer([('area', sqrt_transform, filtered_area_columns)],\n                                  remainder='passthrough')\n    return test_new(X=X, pipe=pipe_area)\n\nproc_univariate()","c18f3476":"# columns_numeric = X_clean.select_dtypes(exclude='category').columns.tolist() + ordered_categories\n\nX_corr_num = X_clean[columns_numeric].astype(float).corr(method='spearman')\n\nsns.clustermap(\n    X_corr_num,\n    vmin=-1,\n    vmax=1,\n    cmap='icefire',\n    method='complete',\n    xticklabels=True,\n    yticklabels=True,\n    linewidths=1,\n)","4e9825b2":"def cramers_corrected_stat(s1, s2):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    confusion_matrix = pd.crosstab(s1, s2)\n    \n    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))    \n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr \/ min( (kcorr-1), (rcorr-1)))\n\nX_corr_mix = X_clean.astype(float).corr(method=cramers_corrected_stat).loc[columns_numeric, columns_object]\n\nsns.clustermap(\n    X_corr_mix,\n    vmin=0,\n    vmax=1,\n    #cmap='icefire',\n    method='complete',\n    xticklabels=True,\n    yticklabels=True,\n    linewidths=1,\n)","109c9386":"X_corr_cat = X_clean[columns_object].astype(int).corr(method=cramers_corrected_stat)\n\nsns.clustermap(\n    X_corr_cat,\n    vmin=0,\n    vmax=1,\n    #cmap='icefire',\n    method='complete',\n    xticklabels=True,\n    yticklabels=True,\n    linewidths=1,\n)","0a93c5d2":"s_mi.head(20).to_frame().style.bar(vmin=0, vmax=1)\n# s_mi[:40].plot.bar(figsize=(12, 2), rot=45)\n# s_mi.to_frame().T.style.background_gradient(axis=None, vmin=0, vmax=1)","4eebd199":"def smallest_corr(X, min_value=-1):\n    X_mask = X.copy()\n    if X.shape[0] == X.shape[1]:\n        np.fill_diagonal(X_mask.values, np.nan)\n    X_mask = X_mask.where(lambda x: x.abs() > min_value)\n    \n    X_small = X_mask.dropna(how='all', axis=0)\\\n                     .dropna(how='all', axis=1)\\\n                     .dropna(how='all', axis=0)\n    \n    q_mi = pd.cut(s_mi, bins=5, labels=False)\n    mi_pal = sns.color_palette('Reds_r', len(q_mi.unique()))\n    lut = dict(zip(q_mi.unique(), mi_pal))\n    mi_colors = q_mi.map(lut)\n    \n    # return X_small.style.background_gradient(axis=None, cmap='magma', vmin=0, vmax=1)\n    # return sns.clustermap(X_small.fillna(0), vmin=0, vmax=1, method='complete', row_colors=mi_colors, col_colors=mi_colors)\n    return sns.clustermap(X, vmin=0, vmax=1, method='complete', mask=X_mask.isna(), xticklabels=True, yticklabels=True, linewidths=1, row_colors=mi_colors, col_colors=mi_colors, figsize=(12, 12), cmap='inferno')\n\nsmallest_corr(X_corr_num, 0.5)","c0382ddc":"X_corr_num.where(lambda x: x.abs() > 0.5).dropna(thresh=4, axis=0).index","27722b06":"test_new(\n    X_clean.eval('GrLivArea \/ TotRmsAbvGrd'),\n    #X_clean.eval('(`1stFlrSF` + `2ndFlrSF`) \/ TotRmsAbvGrd'),\n    #X_clean.eval('GrLivArea \/ (FullBath+1)'),\n    #X_clean.eval('GarageArea \/ (GarageCars+1)'),\n    #X_clean.eval('BsmtFinSF1 \/ (BsmtFullBath+1)'),\n    X_clean.eval('TotalBsmtSF - `1stFlrSF`'),\n    #X_clean.eval('TotRmsAbvGrd - BedroomAbvGr'),\n)","8747b7ea":"test_new(\n    #X_clean.astype(int).eval('GarageQual * GarageCond'),\n    #X_clean.astype(int).eval('Fireplaces * FireplaceQu'),\n    #X_clean.astype(int).eval('PoolArea * PoolQC'),\n    X_clean.eval('OverallQual * YearBuilt'),\n)","782035f9":"smallest_corr(X_corr_mix, 0.5)","a788b452":"test_new(\n    pd.get_dummies(X_clean['GarageType']).mul(X_clean['GarageCars'], axis=0)\n)","88f02e0a":"smallest_corr(X_corr_cat, 0.4)","08206395":"from sklearn.decomposition import PCA\n\ndef run_pca(columns=columns_numeric, X_clean=X_clean, s_mi=s_mi):\n    X = X_clean.astype(float).transform(preprocessing.scale)\n    #X = pd.DataFrame(X, columns=X_clean.columns, index=X_clean.index)\n    pca = PCA(random_state=random_state).fit(X[columns])\n        \n    loadings = pd.DataFrame(pca.components_.T, index=X_clean[columns].columns)\n    loadings = pd.concat([s_mi[columns], loadings], axis=1).sort_values(s_mi.name, ascending=False)\n    \n    X_pca = pd.DataFrame(pca.transform(X[columns]), index=X_clean.index)\n    \n    return pca, X_pca, loadings\n\n\ndef plot_pca(pca, X_pca, loadings):\n    g = pd.DataFrame(pca.explained_variance_ratio_).plot.bar(figsize=(12, 6))\n    s = loadings.style.bar(align='zero', vmin=-1, vmax=1)\n    return g, s","2b609cc4":"pca, X_pca, loadings = run_pca()\ng, s = plot_pca(pca, X_pca, loadings)\ng\ndisplay(s)","37bf403d":"test_new(X=pd.concat([X_pca, X_clean.drop(columns=columns_numeric)], axis=1))","093bdb50":"test_new(X_pca)","4d0a86fd":"# for i in range(1, 10):\n#     display(test_new(X_pca.iloc[:, 0:i]))\ndisplay(test_new(X_pca.iloc[:, 0]))","95d2f7b7":"test_new(X_clean.astype(float).eval(\"(TotRmsAbvGrd+TotalBsmtSF)\/(GrLivArea+`1stFlrSF`)\")) # loading 52","647f527a":"test_new(X_clean.astype(float).eval(\"(FireplaceQu+Fireplaces)\/YearBuilt\"))","533d87aa":"test_new(X_clean.astype(float).eval(\"BsmtQual*YearRemodAdd\"))","eef5dd1c":"pca, X_pca, loadings = run_pca(X_corr_num.where(lambda x: x.abs() > 0.6).dropna(thresh=4, axis=0).index)\ng, s = plot_pca(pca, X_pca, loadings)\ns","0e15ee4f":"#for i in range(1, 5):\n#    display(test_new(X_pca.iloc[:, 0:i]))\n    \ntest_new(X_pca.iloc[:, 0])","8941e19f":"sns.catplot(data=X_clean, x='Neighborhood', y=y, kind='boxen')","0fbcf489":"test_new(\n    X_clean.groupby('Neighborhood')['GrLivArea'].transform('mean'),\n    #X_clean.groupby('Neighborhood')['GrLivArea'].transform('std'),\n)","8bddbf02":"test_new(\n    X_clean.groupby('Neighborhood')['GrLivArea'].transform('median'),\n    #X_clean.groupby('Neighborhood')['GrLivArea'].transform('std'),\n)","b188fa41":"sns.catplot(data=X_clean, x='MSSubClass', y=y, kind='boxen')","343ab4a0":"test_new(\n    X_clean.groupby('MSSubClass')['GrLivArea'].transform('mean'),\n    #X_clean.groupby('MSSubClass')['LotArea'].transform('mean'),\n)","2927fad7":"sns.displot(data=X_pre, y='Neighborhood', x='MSSubClass')","4338b751":"test_new(\n    #X_clean.groupby(['MSSubClass', 'Neighborhood'])['LotArea'].transform('median'),\n    X_clean.groupby(['MSSubClass', 'Neighborhood'])['GrLivArea'].transform('median'),\n)","d7ae1660":"test_new(\n    X_clean.groupby(['MSSubClass', 'Neighborhood', 'MSZoning'])['GrLivArea'].transform('mean'),\n    X_clean.groupby(['MSSubClass', 'Neighborhood', 'MSZoning'])['LotArea'].transform('mean'),\n)","1afec8fb":"test_new(\n    #X_clean.groupby(['MSSubClass', 'Neighborhood', 'MSZoning'])['GrLivArea'].transform('median'),\n    X_clean.groupby(['MSSubClass', 'Neighborhood', 'MSZoning'])['OverallQual'].transform(lambda x: x.mode()[0]),\n)","c105a51c":"sns.lmplot(data=X_pre.join(y), x='GrLivArea', y='SalePrice', col='Neighborhood', ci=None, col_wrap=5, height=2)","789468d3":"from sklearn.linear_model import LinearRegression\n\nlr_all = X_clean.join(y).groupby('Neighborhood', group_keys=False).apply(\n    lambda x: pd.Series(LinearRegression().fit(x[['GrLivArea']], x['SalePrice']).predict(x[['GrLivArea']]), index=x.index)\n)\n\ntest_new(lr_all)","e75c2745":"from sklearn.linear_model import LinearRegression\nfrom sklearn.base import TransformerMixin\n\nclass ModelGroupBy(TransformerMixin):\n    def __init__(self, model, cols, groupby_col, **model_kwargs):\n        self.model_ = model\n        if isinstance(cols, list):\n            self.cols_ = cols\n        elif isinstance(cols, str):\n            self.cols_ = [cols]\n        self.groupby_col_ = groupby_col\n        self.model_kwargs_ = model_kwargs\n\n    def fit(self, X, y):\n        self.fitted_models_ = {}\n        self.median_ = y.median()\n        \n        gb = X.join(y).groupby(self.groupby_col_, group_keys=False)\n        for name, group in gb:\n            group_x = group[self.cols_]\n            group_y = group[y.name]\n            self.fitted_models_[name] = self.model_(**self.model_kwargs_).fit(group_x, group_y)\n        \n        return self\n\n    def transform(self, X):\n        s_predict = []\n        \n        gb = X.groupby(self.groupby_col_, group_keys=False)\n        for name, group in gb:\n            group_x = group[self.cols_]\n            if (name in self.fitted_models_.keys()) and group_x.shape[0] > 1:\n                group_model = self.fitted_models_[name]\n                values = group_model.predict(group_x)\n            else:\n                values = self.median_\n            \n            s = pd.Series(values, index=group.index, name=f'model_{self.groupby_col_}')\n            s_predict.append(s)\n            \n        X_predict = pd.concat(s_predict)\n        return X.join(X_predict)","c7408592":"(ModelGroupBy(LinearRegression, cols=\"GrLivArea\", groupby_col='Neighborhood').fit_transform(X_clean, y).iloc[:, -1] == lr_all.sort_index()).all()","3ccd6776":"pipe_lr = ModelGroupBy(LinearRegression, cols=\"GrLivArea\", groupby_col='Neighborhood')\ntest_new(pipe=pipe_lr)","e1bbd30f":"pca, X_pca, loadings = run_pca(area_columns)\ng, s = plot_pca(pca, X_pca, loadings)\ns","edae4508":"test_new(X_pca[[0]])","4b7cf984":"test_new(\n#     X_clean.eval('(WoodDeckSF+OpenPorchSF+EnclosedPorch+`3SsnPorch`+ScreenPorch+PoolArea) \/ (LotArea+1)'), # External bonus area\n    X_clean.eval('GrLivArea \/ LotArea'), # External \/ interior\n    X_clean.eval('GrLivArea + TotalBsmtSF'), # External \/ interior\n    #X_clean.eval('(`1stFlrSF`+`2ndFlrSF`) \/ (TotalBsmtSF+1)'),\n    #X_clean.eval('MasVnrArea\/(GarageArea+1)')\n    #X_clean.eval('LotArea \/ (GrLivArea+TotalBsmtSF+GarageArea)'),\n)","75658078":"pca, X_pca, loadings = run_pca(X.filter(regex='Qual|Cond|PoolQC|FireplaceQu').columns)\ng, s = plot_pca(pca, X_pca, loadings)\ns","9ae4c267":"test_new(X_pca[0])","6a485daa":"test_new(\n    (X_clean[[\"BsmtQual\", \"FireplaceQu\", \"PoolQC\", 'Fence', \"MiscFeature\", \"Alley\"]] != 0).astype(int).sum(axis=1)\n)","f7736931":"test_new(\n    #X_clean.eval(\"OverallCond\/OverallQual\"),\n    X_clean.astype(float).eval(\"(Condition1+Condition2)\/OverallCond\"),\n)","fda862d8":"test_new(\n    (X_clean[[\"BsmtQual\", \"FireplaceQu\", \"PoolQC\", 'OverallQual', 'GarageQual']] != 0).astype(int).sum(axis=1)\n)","e39a1cfa":"from category_encoders import MEstimateEncoder","c284e84c":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","f6f3d554":"test_new(CrossFoldEncoder(MEstimateEncoder, m=1).fit_transform(X_clean, y, cols=[\"MSSubClass\"]))","00122bcc":"test_new(pipe=MEstimateEncoder(m=1, cols=['MSSubClass']))","bdcefe42":"test_new(CrossFoldEncoder(MEstimateEncoder, m=1).fit_transform(X_clean, y, cols=['Neighborhood']))","acf5f959":"test_new(CrossFoldEncoder(MEstimateEncoder, m=1).fit_transform(X_clean, y, cols=['MSZoning']))","337d7214":"def scale_linear(X):\n    X_num_scaled = X[columns_numeric].astype(float)\n    X_cat_scaled = X[columns_object]\n    \n    X_num_scaled = preprocessing.scale(X_num_scaled)\n    encoder = preprocessing.OneHotEncoder(sparse=False)\n    X_cat_scaled = encoder.fit_transform(X_cat_scaled)\n    \n    X_num_scaled = pd.DataFrame(X_num_scaled, index=X.index, columns=columns_numeric)\n    X_cat_scaled = pd.DataFrame(X_cat_scaled, index=X.index, columns=encoder.get_feature_names(columns_object))\n    \n    return X_num_scaled.join(X_cat_scaled)\n\nscale_linear(X_clean)","81fc904c":"test_new(X=scale_linear(X_clean))","84487e0e":"X_int = preprocessing.PolynomialFeatures(interaction_only=True, include_bias=False).fit_transform(X_clean)\nX_int = pd.DataFrame(X_int, index=X_clean.index)\ntest_new(X=X_int)","5c61e07c":"X_clean_scaled = scale_linear(X_clean)\n\nX_int = preprocessing.PolynomialFeatures(interaction_only=True, include_bias=False).fit_transform(X_clean_scaled)\nX_int = pd.DataFrame(X_int, index=X_clean.index)\ntest_new(X=X_int)","9844af2c":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV, Lasso\n\nX_clean_scaled = scale_linear(X_clean)\n\ninteraction_transform = preprocessing.PolynomialFeatures(interaction_only=True, include_bias=False)\nX_inter = interaction_transform.fit_transform(X_clean_scaled)\nX_inter = pd.DataFrame(X_inter,\n                       index=X_clean.index,\n                       columns=interaction_transform.get_feature_names(X_clean_scaled.columns))\n\nselecter = SelectFromModel(LassoCV(n_alphas=10, max_iter=10_000))\nX_inter_select = selecter.fit_transform(X_inter, y_proc)\nX_inter_select = pd.DataFrame(X_inter_select,\n                              index=X_clean.index,\n                              columns=X_inter.columns[selecter.get_support()])","36f32d35":"X_inter.columns.shape, X_inter_select.columns.shape","7e941cdd":"s_inter_coef = pd.Series(selecter.estimator_.coef_, index=X_inter.columns, name='coef')\\\n                .drop(index=columns_numeric)\\\n                .sort_values(ascending=False)\n\ns_inter_coef.head(20).to_frame().style.bar()","6e202cd8":"s_inter_mi, style_inter_mi = show_mi(X_inter_select, y=y_proc)\ns_inter_mi.to_frame().head(40).style.bar()","e27e1407":"test_new(X_inter_select.loc[:, s_inter_mi.drop(index=columns_numeric, errors='ignore').index])","c1a94e22":"X_inter_select_top = X_inter_select.loc[:, s_inter_mi.drop(index=columns_numeric, errors='ignore').head(20).index]\n\ntest_new(X_inter_select_top)","02cbaadf":"test_new(\n    X_clean.astype(float).eval(\"OverallQual*KitchenQual\"),\n    #X_clean.astype(float).eval(\"OverallQual*BsmtQual\"),\n    X_clean.astype(float).eval(\"ExterQual*Fireplaces\"),\n    X_clean.astype(float).eval(\"CentralAir*GarageCars\"),\n    X_clean.astype(float).eval(\"TotalBsmtSF*KitchenAbvGr\"),\n    X_clean.astype(float).eval(\"ExterQual*FireplaceQu\"),\n    X_clean.astype(float).eval(\"FullBath*GarageYrBlt\"),\n    #X_clean.astype(float).eval(\"TotalBsmtSF*Condition2_2\"),\n    X_clean.astype(float).eval(\"TotalBsmtSF*Electrical\"),\n    X_clean.astype(float).eval(\"YearBuilt*CentralAir\"),\n    #X_clean.astype(float).eval(\"GarageCars*MiscFeature_1\"),\n    X_clean.astype(float).eval(\"BsmtQual*BsmtExposure\"),\n    X_clean.astype(float).eval(\"Alley*ExterQual\"),\n    X_clean.astype(float).eval(\"Street*GarageArea\"),\n    X_clean.astype(float).eval(\"ExterCond*GarageArea\"),\n    X_clean.astype(float).eval(\"LandContour*GarageArea\"),\n    X_clean.astype(float).eval(\"CentralAir*FullBath\"),\n    X_clean.astype(float).eval(\"LotShape*FullBath\"),\n    X_clean.astype(float).eval(\"ExterQual*ScreenPorch\"),\n    X_clean.astype(float).eval(\"BsmtHalfBath*KitchenQual\"),\n    X_clean.astype(float).eval(\"LandContour*BsmtQual\"),\n)","d274f7ec":"def z_outliers(X, X_train=X_clean):\n    X_num = X.select_dtypes('number')\n\n    q1 = X_num.quantile(1\/4)\n    q3 = X_num.quantile(3\/4)\n    iqr = q3 - q1\n\n    is_outlier_matrix = (X_num < q1-1.5*iqr) | (X_num > q3+1.5*iqr)\n    return is_outlier_matrix\n    \nis_outlier_matrix = z_outliers(X_clean)\n\ntest_new(is_outlier_matrix.sum(axis=1) > 0)","e71903cf":"from sklearn.ensemble import IsolationForest\n\nclf_outlier = IsolationForest(random_state=random_state).fit(X_clean.astype(float))\n\ntest_new(pd.Series(clf_outlier.predict(X_clean.astype(float)), index=X_clean.index))","49c77666":"y_pred_lr = pd.Series(RidgeCV().fit(X_clean, np.log(y)).predict(X_clean), index=X_clean.index)\nresid = np.log(y) - y_pred_lr\n\noutliers = ((resid - resid.mean()) \/ resid.std()).abs() > 3\nsns.scatterplot(x=np.log(y), y=y_pred_lr, hue=outliers)","343e7b71":"test_new(outliers)","30ccf2b8":"# def drop_outliers(X, y):\n#     return X.drop(index=outliers[outliers].index, errors='ignore'), y.drop(index=outliers[outliers].index, errors='ignore')\n\n# test_new(outliers, pipe=preprocessing.FunctionTransformer(drop_outliers))","1a759dd4":"# X_corr_num = X_clean[columns_numeric].astype(float).corr(method='spearman')\n\ndef test_proc(X=X_clean):\n    X = X.copy()\n    \n    _, X_pca_all, _ = run_pca()\n    _, X_pca_area, _ = run_pca(area_columns)\n    #_, X_pca_num, _ = run_pca(X_corr_num.where(lambda x: x.abs() > 0.6).dropna(thresh=4, axis=0).index)\n    \n    new_columns = [\n        (X[[\"BsmtQual\", \"FireplaceQu\", \"PoolQC\", 'Fence', \"MiscFeature\", \"Alley\"]] != 0).astype(int).sum(axis=1),\n        #X.eval('GrLivArea \/ LotArea'), # External \/ interior\n        #X.eval('GrLivArea + TotalBsmtSF'), # External \/ interior\n        #X.astype(float).eval(\"(FireplaceQu+Fireplaces)\/YearBuilt\"),\n        #X.astype(float).eval(\"BsmtQual*YearRemodAdd\"),\n        X.groupby('Neighborhood')['GrLivArea'].transform('median'),\n        X.eval(\"TotalBsmtSF * YearRemodAdd\"), # Unknown origin\n        #CrossFoldEncoder(MEstimateEncoder, m=1).fit_transform(X_clean, y, cols=['MSZoning']),\n        X_pca_all[0],\n        X_pca_area[0],\n        #X_pca_num[0],\n    ]\n    \n    s_mi, _ = show_mi(X, y=y_proc)\n    X = X.loc[:, s_mi > 1e-4]\n    \n    filtered_area_columns = X.columns.intersection(area_columns)\n    sqrt_transform = preprocessing.FunctionTransformer(lambda x: np.sqrt(x+1)*(x!=0))\n    area_transform = ColumnTransformer([('area', sqrt_transform, filtered_area_columns)],\n                                  remainder='passthrough')\n    \n    #lr_transform = ModelGroupBy(LinearRegression, cols=\"GrLivArea\", groupby_col='Neighborhood')\n    \n    pipe = Pipeline([\n        #('lr', lr_transform),\n        ('me', MEstimateEncoder(m=1, cols=['MSZoning'])),\n        ('area', area_transform)\n    ]) \n        \n    return test_new(*new_columns, X=X, pipe=pipe)\n\ntest_proc()","9a624bdf":"# (X_clean.replace(X_clean.groupby('MSSubClass')['GrLivArea'].agg('mean'))['MSSubClass'] == X_clean.groupby('MSSubClass')['GrLivArea'].transform('mean')).all()","ca6b2628":"from sklearn.feature_selection import mutual_info_regression\n\ndef show_mi(X, y=y, clean=False):\n    if clean:\n        X = force_clean(X)\n    \n    columns_discrete = X.select_dtypes(['object', 'category'])\n    discrete_features = X.columns.isin(columns_discrete)\n    \n    mi = mutual_info_regression(X, y,\n                                random_state=random_state,\n                                discrete_features=discrete_features)\n\n    s = pd.Series(mi, index=X.columns, name='mutual_info_regression')\\\n          .sort_values(ascending=False)\n          \n    return s, s.to_frame().style.bar()","47ba9e3b":"def create_features(X=X_clean, X_train=X_clean, y=y_proc):\n    X = X.copy()\n    X_train = X_train.copy()\n    \n    # PCA - best column numerical\n    #X_corr_num = X_train[columns_numeric].astype(float).corr(method='spearman')\n    #best_num_columns = X_corr_num.where(lambda x: x.abs() > 0.6).dropna(thresh=4, axis=0).index\n    \n    # PCA - fit\n    scaler_pca = preprocessing.StandardScaler()\n    X_train_scaled = pd.DataFrame(scaler_pca.fit_transform(X_train.astype(float)[columns_numeric]), index=X_train.index, columns=columns_numeric)\n    pca_all = PCA(random_state=random_state).fit(X_train_scaled[columns_numeric])\n    pca_area = PCA(random_state=random_state).fit(X_train_scaled[area_columns])\n    #pca_num = PCA(random_state=random_state).fit(X_train_scaled[best_num_columns])\n    \n    # PCA - transform\n    X_scaled = pd.DataFrame(scaler_pca.transform(X.astype(float)[columns_numeric]), index=X.index, columns=columns_numeric)\n    X_pca_all = pd.DataFrame(pca_all.transform(X_scaled[columns_numeric]), index=X.index)\n    X_pca_area = pd.DataFrame(pca_area.transform(X_scaled[area_columns]), index=X.index)\n    #X_pca_num = pd.DataFrame(pca_num.transform(X_scaled[best_num_columns]), index=X.index)\n    \n    # Transformer - ME\n    m_encoder = MEstimateEncoder(m=1, cols=[\"MSZoning\"])\n    m_encoder.fit(X_train, y)\n    X_encoded = m_encoder.transform(X)\n    \n    # # Transformer - \n    #lr = ModelGroupBy(LinearRegression, cols=\"GrLivArea\", groupby_col='Neighborhood')\n    #lr.fit(X_train, y)\n    #X_lr = lr.transform(X)\n    \n    # Transformer - Area\n    area_transform = preprocessing.FunctionTransformer(lambda x: np.sqrt(x+1)*(x!=0))\n    #area_transform = preprocessing.PowerTransformer()\n    area_transform.fit(X_train[area_columns])\n    X[area_columns] = pd.DataFrame(area_transform.transform(X[area_columns]), index=X.index)\n    \n    encoded_1 = X_train.groupby('Neighborhood')['GrLivArea'].agg('median')\n    \n    new_series = [\n        (X[[\"BsmtQual\", \"FireplaceQu\", \"PoolQC\", 'Fence', \"MiscFeature\", \"Alley\"]] != 0).astype(int).sum(axis=1),\n        #X.eval('GrLivArea \/ LotArea'), # External \/ interior\n        #X.eval('GrLivArea + TotalBsmtSF'), # External \/ interior\n        #X.astype(int).eval(\"(FireplaceQu+Fireplaces)\/YearBuilt\"),\n        X.replace(encoded_1)['Neighborhood'], #X.groupby('Neighborhood')['GrLivArea'].transform('median'),\n        #X.eval(\"TotalBsmtSF * YearRemodAdd\"), # Unknown origin\n        #CrossFoldEncoder(MEstimateEncoder, m=1).fit_transform(X_clean, y, cols=['MSZoning']),\n        X_pca_all[0],\n        X_pca_area[0],\n        #X_pca_num[0],\n        X_encoded.iloc[:, 0],\n        #X_lr['model_Neighborhood']\n    ]\n    \n    s_mi, _ = show_mi(X_train)\n    X = X.loc[:, s_mi > 1e-4]\n    #print(s_mi[~(s_mi > 1e-4)]) # Utilities, MiscVal, MiscFeatures, 3SsnPorch, MoSold, MiscVal, PoolQC\n    \n    X = X.join([s.rename(f'new_{i}') for i, s in enumerate(new_series)])\n    \n    return X\n\ntest_new(X=create_features())","f5b5f8f9":"X_pre = preproc(X)\nX_test_pre = preproc(X_test)\n\nX_clean = force_clean(X_pre, X_pre)\nX_clean_test = force_clean(X_test_pre, X_pre)\n\nX_proc = create_features(X_clean, X_clean)\nX_proc_test = create_features(X_clean_test, X_clean)","6d3ab812":"# columns_proc_category = X_proc.columns[X_proc.columns.isin(columns_category)]\n# columns_proc_ordinal = X_proc.drop(columns=columns_proc_category).select_dtypes('category').columns\n# columns_proc_numeric = X_proc.select_dtypes('number').columns\n\ncolumns_proc_category = X_proc.drop(columns=columns_numeric, errors='ignore').columns\ncolumns_proc_numeric = columns_numeric","a70b8fa5":"X_proc_test.isna().sum().loc[lambda x: x>0]","7e24f362":"score_models(X=X_proc)","75e64f1c":"score_models(X=X_proc.loc[~outliers, :], y=y_proc.loc[~outliers])","377aa908":"# X_proc = X_proc.loc[~outliers, :]\n# y_proc = y_proc.loc[~outliers]","72498adf":"y_encoded, _ = pd.qcut(y_proc, q=10).factorize()\n\ndef get_folds(X=X_proc, y=y_proc, y_encoded=y_encoded):\n    y_encoded, _ = pd.qcut(y, q=10).factorize()\n    return folds.split(X, y_encoded)","e26888d6":"from sklearn.linear_model import ElasticNetCV, Lasso, Ridge, RidgeCV\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.svm import LinearSVR","2dced4b2":"def scale(X, X_train=X_proc):\n    X_norm = X.drop(columns=columns_proc_category)\n    X_train_norm = X_train.drop(columns=columns_proc_category)\n    \n    #boxcox = preprocessing.PowerTransformer()\n    #X_train_boxcox = boxcox.fit_transform(X_train.drop(columns=columns_proc_category))\n    scaler = preprocessing.StandardScaler().fit(X_train_norm)\n    \n    X_norm = pd.DataFrame(scaler.transform(X_norm), columns=X_norm.columns, index=X_norm.index)\n    \n    #X_norm = X_norm.join(pd.get_dummies(X[columns_proc_category]))\n    encoder = preprocessing.OneHotEncoder(sparse=False, handle_unknown='ignore').fit(X_train[columns_proc_category])\n    X_norm = X_norm.join(pd.DataFrame(encoder.transform(X[columns_proc_category]),\n                                      columns=encoder.get_feature_names(columns_proc_category),\n                                      index=X.index))\n    return X_norm\n\nX_proc_norm = scale(X_proc)\nX_proc_norm_test = scale(X_proc_test)\nprint('any NaN ?', X_proc_norm.isna().any(axis=None))","29365050":"# model = MLPRegressor(hidden_layer_sizes=(100, 1000, 100), random_state=random_state, max_iter=10_000, solver='adam', activation='tanh', early_stopping=True)\n\n# r = cross_val_score(model, X_proc_norm, y_proc,\n#                     cv=folds.split(X_proc, y_encoded),\n#                     scoring='neg_root_mean_squared_error')\n# print(-r.mean()) # 0.3871994246713676","7793d113":"model_selection_dict = {\n    \"LGBM\": lgb.LGBMRegressor(objective=\"rmse\", random_state=random_state),\n    \"XGB\": xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=random_state),\n    \"RandomForest\": RandomForestRegressor(random_state=random_state),\n    \"ExtraTrees\": ExtraTreesRegressor(random_state=random_state, bootstrap=True),\n    \"SVR\": SVR(),\n    #\"SVR_linear\": LinearSVR(max_iter=10_000),\n    \"ElasticNet\": ElasticNet(max_iter=10_000, random_state=random_state), \n    \"Ridge\": Ridge(),\n}\nmodel_selection_score = {}\n\nfor name, model in model_selection_dict.items():\n    if name in ['SVR']:\n        x = X_proc_norm\n    elif name == 'XGB':\n        x = X_proc.copy()\n        columns_proc_category = x.select_dtypes('category').columns\n        x[columns_proc_category] = x[columns_proc_category].astype(int)\n    else:\n        x = X_proc\n        \n    r = cross_val_score(model, x, y_proc,\n                        cv=folds.split(X_proc, y_encoded),\n                        scoring='neg_root_mean_squared_error')\n    model_selection_score[name] = -r.mean()\n    \npd.Series(model_selection_score).sort_values()","875e5b65":"import optuna","a9ec56ef":"def objective(trial):\n    gamma = trial.suggest_categorical(\"gamma\", ['auto', 'scale'])\n    C = trial.suggest_float(\"C\", 1e-10, 1e10, log=True)\n    epsilon = trial.suggest_float(\"epsilon\", 0.001, 0.5, log=True)\n    \n    kernel = 'rbf' #trial.suggest_categorical(\"kernel\", ['poly', 'rbf'])\n    if kernel=='poly':\n        degree = trial.suggest_int(\"degree\", 2, 5)\n        coef0 = trial.suggest_float(\"coef0\", 0.001, 10.0, log=True)\n    else:\n        degree = 3 # default\n        coef0 = 0.0 # default\n        \n    model = SVR(gamma=gamma,\n                C=C,\n                kernel=kernel,\n                degree=degree,\n                coef0=coef0,\n                epsilon=epsilon)\n\n    r = cross_val_score(model, X_proc_norm, y_proc,\n                        cv=folds.split(X_proc, y_encoded),\n                        scoring='neg_root_mean_squared_error')\n    return -r.mean()\n\nstudy_rbf = optuna.create_study()\nstudy_rbf.optimize(objective, n_trials=30)","4edac96e":"print(\"Best score:\", study_rbf.best_value)\nprint(study_rbf.best_params)\n\nprint(\"Best params:\", study_rbf.best_params)\nprint(\"  Params: \")\nfor key, value in study_rbf.best_params.items():\n    print(\"    {}: {}\".format(key, value))","a0ea580d":"display(optuna.visualization.plot_optimization_history(study_rbf))\ndisplay(optuna.visualization.plot_param_importances(study_rbf))","c41b7ef0":"def objective(trial):\n    gamma = trial.suggest_categorical(\"gamma\", ['auto', 'scale'])\n    C = trial.suggest_float(\"C\", 1e-10, 1e10, log=True)\n    epsilon = trial.suggest_float(\"epsilon\", 0.001, 0.5, log=True)\n    \n    kernel = 'poly' #trial.suggest_categorical(\"kernel\", ['poly', 'rbf'])\n    if kernel=='poly':\n        degree = trial.suggest_int(\"degree\", 2, 5)\n        coef0 = trial.suggest_float(\"coef0\", 0.001, 10.0, log=True)\n    else:\n        degree = 3 # default\n        coef0 = 0.0 # default\n        \n    model = SVR(gamma=gamma,\n                C=C,\n                kernel=kernel,\n                degree=degree,\n                coef0=coef0,\n                epsilon=epsilon)\n\n    r = cross_val_score(model, X_proc_norm, y_proc,\n                        cv=folds.split(X_proc, y_encoded),\n                        scoring='neg_root_mean_squared_error')\n    return -r.mean()\n\nstudy_poly = optuna.create_study()\nstudy_poly.optimize(objective, n_trials=50)","110f003e":"print(\"Best score:\", study_poly.best_value)\nprint(study_poly.best_params)\n\nprint(\"Best params:\", study_poly.best_params)\nprint(\"  Params: \")\nfor key, value in study_poly.best_params.items():\n    print(\"    {}: {}\".format(key, value))","e200b5ef":"display(optuna.visualization.plot_optimization_history(study_poly))\ndisplay(optuna.visualization.plot_param_importances(study_poly))","f9d6dc8c":"ridgecv = RidgeCV(alphas=[1, 5, 10, 50, 100, 500, 1000],\n                       cv=folds.split(X_proc, y_encoded),\n                       scoring='neg_root_mean_squared_error')\n\nridgecv.fit(X_proc, y_proc)\n\nprint(\"RidgeCV alpha:\", ridgecv.alpha_)\n\nridge = Ridge(alpha=ridgecv.alpha_)\nr = cross_val_score(ridge, X_proc, y_proc,\n                    cv=get_folds(),\n                    scoring='neg_root_mean_squared_error')\nprint(-r.mean())","26330b2a":"model_linear_cv = ElasticNetCV(l1_ratio=np.arange(0.1, 1, 0.1),\n                               alphas=np.arange(1, 1e3, 100),\n                               max_iter=10_000,\n                               random_state=random_state, cv=get_folds())\n\nmodel_linear_cv.fit(X_proc, y_proc)\n\nprint(\"ElasticNetCV alpha:\", model_linear_cv.alpha_)\nprint(\"ElasticNetCV l1_ratio:\", model_linear_cv.l1_ratio_)\n\nmodel_linear = ElasticNet(alpha=model_linear_cv.alpha_,\n                          l1_ratio=model_linear_cv.l1_ratio_)\n\nr = cross_val_score(model_linear, X_proc, y_proc,\n                    cv=folds.split(X_proc, y_encoded),\n                    scoring='neg_root_mean_squared_error')\nprint(-r.mean())","b40830f2":"fixed_folds = []\n\nfor train_index, test_index in get_folds():\n    fixed_folds.append((train_index, test_index))","35c7bb9d":"X_proc_xgb = X_proc.select_dtypes('number').join(X_proc.select_dtypes('category').apply(lambda x: x.cat.codes))\nX_proc_test_xgb = X_proc_test.select_dtypes('number').join(X_proc_test.select_dtypes('category').apply(lambda x: x.cat.codes))","80510aca":"dtrain = xgb.DMatrix(X_proc_xgb, label=y_proc, enable_categorical=True)\n\neval_hist = {}\nfor eta in [0.01, 0.03, 0.1, 0.3]:\n    eval_hist[eta] = xgb.cv(\n        params={\"objective\":\"reg:squarederror\", \"eta\":eta, \"eval_metric\":\"rmse\"},\n        dtrain=dtrain,\n        num_boost_round=1_000,\n        early_stopping_rounds=100,\n        folds=fixed_folds,\n        stratified=False,\n        shuffle=False,\n        metrics='rmse',\n        verbose_eval=False,\n    )","b3051cb6":"r_eval_hist_train, r_eval_hist_test = {}, {}\nfor eta, d in eval_hist.items():\n    r_eval_hist_train[eta] = pd.Series(d['train-rmse-mean'])\n    r_eval_hist_test[eta] = pd.Series(d['test-rmse-mean'])\n\nr_eval_hist_train = pd.DataFrame(r_eval_hist_train)\nr_eval_hist_test = pd.DataFrame(r_eval_hist_test)\nr_eval_hist_test.plot(style='-')","adc2cb67":"pd.concat([r_eval_hist_test.idxmin().rename('num_boost'),\n           r_eval_hist_test.min().rename('min_test_score'),\n           r_eval_hist_train.where(r_eval_hist_test.apply(lambda x: x==x.min())).agg('sum').rename('min_train_score')], axis=1)","c561f190":"dtrain = xgb.DMatrix(X_proc_xgb, label=y_proc, enable_categorical=True)\n\neval_hist_booster = {}\nfor booster in [\"gbtree\", \"gblinear\", \"dart\"]:\n    eval_hist_booster[booster] = xgb.cv(\n        params={\"objective\":\"reg:squarederror\", \"eta\":0.03, \"eval_metric\":\"rmse\", \"booster\":booster},\n        dtrain=dtrain,\n        num_boost_round=2_000,\n        early_stopping_rounds=100,\n        folds=fixed_folds,\n        stratified=False,\n        shuffle=False,\n        metrics='rmse',\n        verbose_eval=False,\n    )","8097fadb":"r_eval_hist_train, r_eval_hist_test = {}, {}\nfor booster, d in eval_hist_booster.items():\n    r_eval_hist_train[booster] = pd.Series(d['train-rmse-mean'])\n    r_eval_hist_test[booster] = pd.Series(d['test-rmse-mean'])\n\nr_eval_hist_train = pd.DataFrame(r_eval_hist_train)\nr_eval_hist_test = pd.DataFrame(r_eval_hist_test)\n\npd.concat([r_eval_hist_test.idxmin().rename('num_boost'),\n           r_eval_hist_test.min().rename('min_test_score'),\n           r_eval_hist_train.where(r_eval_hist_test.apply(lambda x: x==x.min())).agg('sum').rename('min_train_score')], axis=1)","240f7cf8":"def objective(trial):\n    dtrain = xgb.DMatrix(X_proc_xgb, label=y_proc)#, enable_categorical=True)\n    \n    params = dict(\n        verbosity=0,\n        objective=\"reg:squarederror\",\n        eval_metric=\"rmse\",\n        eta=0.03,\n        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    \n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"test-rmse\")\n    history = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=2_000,\n        early_stopping_rounds=100,\n        folds=fixed_folds,\n        stratified=False,\n        shuffle=False,\n        callbacks=[pruning_callback]\n    )\n    \n    trial.set_user_attr(\"n_estimators\", len(history))\n    \n    best_score = history[\"test-rmse-mean\"].values[-1]\n    return best_score\n\npruner = optuna.pruners.MedianPruner(n_warmup_steps=100)\nxgb_study = optuna.create_study(pruner=pruner, direction=\"minimize\")\nxgb_study.optimize(objective, n_trials=200)","dc8e318b":"# def objective(trial):\n#     dtrain = xgb.DMatrix(X_proc, label=y_proc, enable_categorical=True)\n    \n#     param = {\n#         \"verbosity\": 0,\n#         \"objective\": \"reg:squarederror\",\n#         \"eval_metric\": \"rmse\",\n#         \"eta\":0.1,\n#         \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n#         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n#     }\n    \n#     if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n#         param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n#         param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n#         param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n#     if param[\"booster\"] == \"dart\":\n#         param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n#         param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n#         param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n#         param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n    \n#     pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"test-rmse\")\n#     history = xgb.cv(\n#         param,\n#         dtrain,\n#         num_boost_round=1_000,\n#         early_stopping_rounds=100,\n#         folds=fixed_folds,\n#         stratified=False,\n#         shuffle=False,\n#         callbacks=[pruning_callback]\n#     )\n    \n#     trial.set_user_attr(\"n_estimators\", len(history))\n    \n#     best_score = history[\"test-rmse-mean\"].values[-1]\n#     return best_score\n\n# pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n# xgb_study = optuna.create_study(pruner=pruner, direction=\"minimize\")\n# xgb_study.optimize(objective, n_trials=5)","5d7f8786":"print(\"Number of finished trials: {}\".format(len(xgb_study.trials)))\n\nprint(\"Best trial:\")\nxgb_trial = xgb_study.best_trial\n\nprint(\"  Value: {}\".format(xgb_trial.value))\n\nprint(\"  Params: \")\nfor key, value in xgb_trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n    \nprint(\"  Number of estimators: {}\".format(xgb_trial.user_attrs[\"n_estimators\"]))","94667b6a":"display(optuna.visualization.plot_optimization_history(xgb_study))\ndisplay(optuna.visualization.plot_param_importances(xgb_study))","c8186652":"dtrain = lgb.Dataset(X_proc, y_proc, categorical_feature=columns_proc_category.tolist(), free_raw_data=False)\n\neval_hist = {}\nfor learning_rate in np.geomspace(1e-3, 1e-1, 5):\n    eval_hist[learning_rate] = lgb.cv(\n        params={\"objective\":\"rmse\", \"learning_rate\":learning_rate},\n        train_set=dtrain,\n        num_boost_round=10_000,\n        early_stopping_rounds=100,\n        folds=get_folds(),\n        stratified=False,\n        shuffle=False,\n        metrics='rmse',\n        verbose_eval=False,\n    )","b6ce06ac":"r_eval_hist = {}\nfor eta, d in eval_hist.items():\n    r_eval_hist[eta] = pd.Series(d['rmse-mean'])\n    \nr_eval_hist = pd.DataFrame(r_eval_hist)\nr_eval_hist.plot()","ce5ca8be":"pd.concat([r_eval_hist.idxmin().rename('num_boost'),\n           r_eval_hist.min().rename('min_test_score')], axis=1)","39d3a259":"import optuna\nfrom optuna.integration import lightgbm as lgb_opt\n\ndtrain = lgb_opt.Dataset(X_proc, y_proc) #, categorical_feature=columns_proc_category.tolist(), free_raw_data=False)\n\nparams = {\n    \"objective\": \"regression\",\n    \"random_state\": random_state,\n    \"metric\": \"rmse\",\n    \"verbosity\": -1,\n    \"learning_rate\": 0.01,\n    \"num_threads\":4,\n}\n\nstudy = optuna.create_study()\nlgb_tuner = lgb_opt.LightGBMTunerCV(\n    params=params,\n    train_set=dtrain,\n    folds=fixed_folds,\n    #shuffle=False,\n    #nfold=5,\n    shuffle=False,\n    stratified=False,\n    #feature_name='auto',\n    #categorical_feature=columns_proc_category,\n    return_cvbooster=True,\n    verbose_eval=-1,\n    show_progress_bar=False,\n    optuna_seed=random_state,\n    early_stopping_rounds=100,\n    num_boost_round=2_000,\n    study=study,\n)\nlgb_tuner.run()","563c5f3b":"optuna.visualization.plot_optimization_history(study)","d8b3a45b":"lgb_tuner.get_best_booster().best_iteration","a7bc0cc6":"print(\"Best score:\", lgb_tuner.best_score)\nbest_params = lgb_tuner.best_params\nprint(\"Best params:\", best_params)\nprint(\"  Params: \")\nfor key, value in best_params.items():\n    print(\"    {}: {}\".format(key, value))","cbcfc9db":"# from sklearn.ensemble import StackingRegressor","b3101843":"# take care, change of attributes names in XGB !\nxgb_params = dict(verbosity=0,\n                  n_jobs=1,\n                  objective=\"reg:squarederror\",\n                  eval_metric=\"rmse\",\n                  learning_rate=0.03,\n                  n_estimators=int(xgb_trial.user_attrs[\"n_estimators\"]*1.1),\n                  **xgb_trial.params)\nprint(\"XGB\", xgb_params)\n\nlgb_params = lgb_tuner.best_params\nlgb_params['n_jobs'] = 1\nlgb_params['n_estimators'] = int(lgb_tuner.get_best_booster().best_iteration*1.1) # 2000\nlgb_params.pop('num_threads')\nprint(\"LGB\", lgb_params)","e9fa5e23":"svr_rbf = SVR(kernel='rbf', **study_rbf.best_params).fit(X_proc_norm, y_proc)\nsvr_poly = SVR(kernel='poly', **study_poly.best_params).fit(X_proc_norm, y_proc)\n#elastic_net = model_linear.fit(X_proc_norm, y_proc)\nridge = ridge.fit(X_proc, y_proc)\nlgbm = lgb.LGBMRegressor(**lgb_params).fit(X_proc, y_proc)\n# xgbm = xgb.train({**xgb_params, **xgb_trial.params}, dtrain,\n#                  num_boost_round=int(xgb_trial.user_attrs[\"n_estimators\"]*1.2))\nxgbm = xgb.XGBRegressor(**xgb_params).fit(X_proc_xgb, y_proc)\n\nmodel_dict = {\n    \"xgbm\": xgbm,\n    \"lgbm\": lgbm,\n    \"svr_rbf\": svr_rbf,\n    \"svr_poly\": svr_poly,\n    #\"elastic_net\": elastic_net,\n    \"ridge\": ridge,\n}","c4cdaeae":"from sklearn.model_selection import cross_val_predict\n\nX_out_all = X_proc.copy()\nX_out_test_all = X_proc_test.copy()\nX_out, X_out_test = pd.DataFrame(), pd.DataFrame()\n\nfinal_score = {}\n\nfor name, model in model_dict.items():\n    if name in [\"svr_rbf\", \"svr_poly\"]:\n        X_train_estimator = X_proc_norm\n        X_test_estimator = X_proc_norm_test\n    elif name in ['xgbm']:\n        X_train_estimator = X_proc_xgb\n        X_test_estimator = X_proc_test_xgb\n    else:\n        X_train_estimator = X_proc\n        X_test_estimator = X_proc_test\n        \n    #model.fit(X_train_estimator, y_proc)\n    #s_train = model.predict(X_train_estimator)\n    s_train = cross_val_predict(model, X_train_estimator, y_proc,\n                                n_jobs=4, cv=get_folds())\n    s_test = model.predict(X_test_estimator)\n    \n    final_score[name] = mean_squared_error(y_proc, s_train, squared=False)\n    print(name, final_score[name])\n    \n    X_out[f'out_{name}'] = pd.Series(s_train, X_proc.index)\n    X_out_test[f'out_{name}'] = pd.Series(s_test, X_proc_test.index)\n    \nX_out_all = X_out_all.join(X_out)\nX_out_test_all = X_out_test_all.join(X_out_test)\n\n# pd.Series(final_score).to_frame().style.bar()","07aff470":"r_same_folds = cross_val_score(ExtraTreesRegressor(), X_out, y_proc,\n                               cv=get_folds(),\n                               scoring='neg_root_mean_squared_error')\n\nr_new_folds_5 = cross_val_score(ExtraTreesRegressor(), X_out, y_proc,\n                              cv=5,\n                              scoring='neg_root_mean_squared_error')\n\nr_new_folds_10 = cross_val_score(ExtraTreesRegressor(), X_out, y_proc,\n                              cv=10,\n                              scoring='neg_root_mean_squared_error')\n\nprint(\"same folds:\", -r_same_folds.mean(), \"random 5 new folds:\", -r_new_folds_5.mean(), \"random 10 new folds:\", -r_new_folds_10.mean())","d17503de":"# r_same_folds = cross_val_score(LinearRegression(), X_out, y_proc,\n#                                cv=get_folds(),\n#                                scoring='neg_root_mean_squared_error')\n\n# r_new_folds = cross_val_score(LinearRegression(), X_out, y_proc,\n#                               cv=5,\n#                               scoring='neg_root_mean_squared_error')\n\n# print(\"same folds:\", -r_same_folds.mean(), \"random 5 new folds:\", -r_new_folds.mean())\n# #>>>same folds: 0.11607714315880906 random new folds: 0.1275152949072265","297e9797":"r_same_folds = cross_val_score(ExtraTreesRegressor(), X_out_all, y_proc,\n                               cv=get_folds(),\n                               scoring='neg_root_mean_squared_error')\n\nr_new_folds_5 = cross_val_score(ExtraTreesRegressor(), X_out_all, y_proc,\n                              cv=5,\n                              scoring='neg_root_mean_squared_error')\n\nr_new_folds_10 = cross_val_score(ExtraTreesRegressor(), X_out_all, y_proc,\n                              cv=10,\n                              scoring='neg_root_mean_squared_error')\n\nprint(\"same folds:\", -r_same_folds.mean(), \"random 5 new folds:\", -r_new_folds_5.mean(), \"random 10 new folds:\", -r_new_folds_10.mean())","f46f7d73":"r_same_folds = cross_val_score(RidgeCV(), X_out_all, y_proc,\n                               cv=get_folds(),\n                               scoring='neg_root_mean_squared_error')\n\nr_new_folds_5 = cross_val_score(RidgeCV(), X_out_all, y_proc,\n                              cv=5,\n                              scoring='neg_root_mean_squared_error')\n\nr_new_folds_10 = cross_val_score(RidgeCV(), X_out_all, y_proc,\n                              cv=10,\n                              scoring='neg_root_mean_squared_error')\n\nprint(\"same folds:\", -r_same_folds.mean(), \"random 5 new folds:\", -r_new_folds_5.mean(), \"random 10 new folds:\", -r_new_folds_10.mean())\n#>>>same folds: 0.11607714315880906 random new folds: 0.1275152949072265","befe81a9":"def objective(trial):\n    \n    max_depth_isnone = trial.suggest_categorical(\"max_depth_isnone\", [True, False])\n    if not max_depth_isnone:\n        max_depth = trial.suggest_int(\"max_depth\", 2, 32, log=True)\n    else:\n        max_depth = None\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 100, log=True)\n    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 64, log=True)\n    max_features = trial.suggest_categorical(\"max_features\", ['auto','sqrt','log2'])\n    n_estimators = trial.suggest_categorical(\"n_estimators\", [100, 1000]) \n    \n    model = ExtraTreesRegressor(criterion='mse',\n                                n_estimators=n_estimators,\n                                max_depth=max_depth,\n                                min_samples_split=min_samples_split,\n                                min_samples_leaf=min_samples_leaf,\n                                max_features=max_features,\n                                random_state=random_state)\n    \n    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state+1)\n    r = cross_val_score(model, X_out_all, y_proc,\n                        cv=folds.split(X_proc, y_encoded),\n                        scoring='neg_root_mean_squared_error')\n    return -r.mean()\n\nstudy_final = optuna.create_study()\nstudy_final.optimize(objective, n_trials=40)","21e9302d":"print(\"Best score:\", study_final.best_value)\nprint(study_final.best_params)\n\nprint(\"Best params:\", study_final.best_params)\nprint(\"  Params: \")\nfor key, value in study_final.best_params.items():\n    print(\"    {}: {}\".format(key, value))","88edf145":"display(optuna.visualization.plot_optimization_history(study_final))\ndisplay(optuna.visualization.plot_param_importances(study_final))","3b8f36f1":"final_best_params = dict(study_final.best_params)\nfinal_best_params.pop('max_depth_isnone')\nif study_final.best_params['max_depth_isnone']:\n    final_best_params['max_depth'] = None\n\nfinal_estimator = ExtraTreesRegressor(criterion='mse', **final_best_params).fit(X_out_all, y_proc)","28f303d9":"r = cross_val_score(final_estimator, X_out_all, y_proc,\n                    cv=get_folds(), scoring='neg_root_mean_squared_error')\n\nprint(\"final score in old-out\/CV:\", -r.mean())","557681bd":"final_estimator.fit(X_out_all, y_proc)\ny_pred = final_estimator.predict(X_out_all)\ny_pred = pd.Series(y_pred, index=y_proc.index, name='predictions')\ny_pred_test = final_estimator.predict(X_out_test_all)\n\nprint(\"final 'training' score on training data:\", mean_squared_error(y_proc, y_pred, squared=False))","8d78a7af":"r = cross_val_score(RidgeCV(), X_out_all, y_proc,\n                    cv=get_folds(), scoring='neg_root_mean_squared_error')\n\nprint(\"final score in old-out\/CV:\", -r.mean())","a80522b5":"final_lr = RidgeCV().fit(X_out_all, y_proc)\ny_pred_lr = final_lr.predict(X_out_all)\ny_pred_lr = pd.Series(y_pred_lr, index=y_proc.index, name='predictions')\ny_pred_test_lr = final_lr.predict(X_out_test_all)\n\nprint(\"final 'training' score on training data:\", mean_squared_error(y_proc, y_pred_lr, squared=False))","41782243":"fig, ax = plt.subplots()\nsns.histplot(x=y_proc, y=y_pred, color='b')\nsns.regplot(x=y_proc, y=y_pred, color='b', truncate=False, line_kws={\"color\":\"k\"}, scatter_kws={\"s\": 1}, ax=ax)","93311a61":"fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\nsns.histplot([y_proc, y_pred], ax=axs[0], color=['r', 'b'])\nsm.ProbPlot(y_proc.values).qqplot(ax=axs[1], fmt='r')\nsm.ProbPlot(y_pred).qqplot(ax=axs[1], fmt='b')\nplt.tight_layout()","cadb84c9":"fig, ax = plt.subplots()\nsns.histplot(x=y_proc, y=y_pred_lr, color='b')\nsns.regplot(x=y_proc, y=y_pred_lr, color='b', truncate=False, line_kws={\"color\":\"k\"}, scatter_kws={\"s\": 1}, ax=ax)","fdb6805f":"fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\nsns.histplot([y_proc, y_pred_lr], ax=axs[0], color=['r', 'b'])\nsm.ProbPlot(y_proc.values).qqplot(ax=axs[1], fmt='r')\nsm.ProbPlot(y_pred_lr).qqplot(ax=axs[1], fmt='b')\nplt.tight_layout()","af5307b7":"# def refinment(y):\n#     y = y.copy()\n#     index_q1 = y < (y.mean() - y.std())\n#     #index_q3 = y > (y.mean() + 2*y.std())\n#     y[index_q1] = y[index_q1] ** 0.999\n#     #y[index_q3] = y[index_q3] ** 1\n#     return y\n\n# y_pred_refi = refinment(y_pred)\n\n# fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n# sns.histplot([y_proc, y_pred_refi], ax=axs[0], color=['r', 'g'])\n# sm.ProbPlot(y_proc.values).qqplot(ax=axs[1], fmt='r')\n# sm.ProbPlot(y_pred_refi).qqplot(ax=axs[1], fmt='b')\n# plt.tight_layout()\n\n# print(\"final score on training data:\", mean_squared_error(y_proc, y_pred_refi, squared=False))\n## >>> final score on training data: 0.09899767369324648","505248a8":"def submit(y_pred, name):\n    ys = pd.DataFrame(np.exp(y_pred), index=X_test.index, columns=[y.name])\n    ys.to_csv(f\"{name}.csv\")\n    return ys\n\ntry:\n    os.mkdir('kaggle\/output\/house-prices-advanced-regression-techniques')\nexcept:\n    pass","062bfbb3":"for name, _ in model_dict.items():\n    submit(X_out_test[f'out_{name}'].values, name)\n    \nX_out_test","e3c5e49c":"submit(y_pred_test, 'final')","af2a17ba":"submit(y_pred_test_lr, 'final_lr')","4ab5349f":"## From target\n\nhttps:\/\/www.kaggle.com\/jack89roberts\/top-7-using-elasticnet-with-interactions\/data#outliers","825a012f":"## Interaction + LassoCV","b71f6ea8":"### Hyperparameter Opt. part 2 with Optuna","fbe1b964":"Power transform normalize the target distribution\nLet's use log(y) from now on","0275539b":"# Modeling","ef3aa7d1":"## Data Cleaning","9c664408":"## Base Model","ef6efd06":"### Category \/ Category","d6ea0a08":"Collection of individual best","967a16ff":"## Feature Correlation","1aefbc00":"# Baseline model","b37367ff":"Manual refinment does not work","47ddf670":"## from the raw data","6b4b60af":"## Target Encoder","7de131dc":"# Feature Engineering - Multivariate","09692fd1":"### Specific issue of Exterior2nd","64455e7c":"Cannot use `sklearn.ensemble.StackingRegressor`, because:\n1. to hyperoptimzie the final_estimator\n2. because some of them need different processing (could use pipelines but would be a hassle)","9f51ff53":"We can notice wronly imputed category in some columns, such as:\n- \"Brk Cmn\" which should be \"Brk Comm\"\n- \"CmentBd\" which should be \"CemntBd\"\n- \"Wd Shng\" which should be \"WdShing\"","102ef504":"## Missing values trial\n### LotFrontage","77becf7c":"## Baseline Processing","c7cdf2b1":"The features which describes an area can be standardized with PowerTransform","ccd27181":"ElasticNetCV converge towards a Ridge model","5b56c608":"## Correlation (MI)","3c273fae":"### Dates","acc29d86":"## XGB Hyperparameter Opt.","4443f9c7":"## Features","2afd1452":"# Processing Summary","8c493de5":"## From features","40614e81":"# Stacking","9f1072dd":"It seems interesting to drop outliers to avoid data noise in the training process...","27967746":"# Imports","31151fd4":"# Outliers","1c4f42e2":"## Ridge CV","584100cd":"## Summary","cbb28bfc":"# Feature Engineering - Univariate","a284e05a":"class is correct (result between one-liner and class for all index transformations is same)","b1e0f116":"## LightGBM Hyperparameter Opt.","37d2ae11":"## Folds","193dc4ac":"## ElasticNet CV HyperParameter Opt.","da18b939":"## Model Selection\n- LightGBM for sure\n- ElasticNet as the linear model (different outputs)\n- 3rd one choice: MLP ?\n\nThen stacking with RF","ee4cb40e":"# Feature Preprocessing","9c35951e":"Our final leaderboard score should be close to the value below:","ae5ccbe9":"## Comparison Train \/ Test","377b9ede":"## Summary","fe8ce734":"### Categorical \/ Numerical\n\n1. https:\/\/stackoverflow.com\/questions\/20892799\/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix\n2. https:\/\/stats.stackexchange.com\/questions\/484299\/how-to-check-the-correlation-between-categorical-and-numeric-independent-variabl\n3. https:\/\/stats.stackexchange.com\/questions\/369783\/how-to-do-a-correlation-matrix-with-categorical-ordinal-and-interval-variable\n4. https:\/\/stats.stackexchange.com\/questions\/108007\/correlations-with-unordered-categorical-variables\n5. https:\/\/medium.com\/@outside2SDs\/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365","826657ff":"The above columns with low NA value (and moreover if all in test) will be replaced by 0 or 'other', as it seems just that the data in test was not cleaned compared to train\nTher other above columns will be replaced by the category available in documentation such as \"None\" or \"NA\"\n\nFor the columns with an \"Other\" option which is never used, we will impute the value later.","a787db2b":"# Data Collection","bd8d8fbc":"# Output visualization","f7a44444":"Matrix without initial columns (only outputs)","c2ecde6b":"# Short EDA before Processing","6b1fa5d7":"## Simple stuff","c59eac82":"## Target\n\nEvaluation score is RMSE(log(y))","b7e8d290":"## Discard low correlation","d0b5f02a":"### Numerical","e0a17904":"## String --> Rate or Condition\nMany features are categories which can be replaced with integers:\n- Ex\tExcellent -> 5\n- Gd\tGood -> 4\n- TA\tTypical\/Average -> 3\n- Fa\tFair -> 2\n- Po\tPoor -> 1\n- NA\tNo Garage -> 0","5f41d431":"Not all features correlate linearly with the target","d9a9788a":"I predict y from each model by hold-out like [sklearn.ensemble.StackingRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingRegressor.html) for proper Stacking (and avoid overfitting)","bb52ba36":"For [Exterior1st, Exterior2nd], from the amount of:\n- [Wd Sdng, Wd Sdng]\n- [Wd Sdng, Wd Shng]\n- [WdShing, Wd Shng]\nI would say that \"Wd Shng\" --> \"WdShing\"","090e26ce":"Matrix with initial columns","4b9f1dcd":"It seems that some missing values could be replaced with 0 or \"other\" or \"none\"\n\nWe distinguish them between:\n- numbers (example: *SF, *Area), we could replace NaN --> 0\n- categories (example: Misc) we could replace NaN --> None\n- categories (example: SaleType) we could replace NaN --> Other","798d0f2e":"With Linear Regression","928328c5":"### Dates","68c53ee3":"I changed the order to add more coherence between the new features and the transformation. Changed area to \"sqrt\" and then calculate new features to keep same unit coherence.","92afd8f7":"## SVR Hyperparameter Opt.","bce650f9":"## PCA","c04883b3":"I plan to add new features such as the difference in years instead of years","8342deaa":"### Hyperparameter Opt. part 1","abd0d0dd":"# EDA Multi-variate","2244ea89":"We can convert some features to a standard distributed feature (with a power transform)\n\nIt is called area_columns but contains `MiscValue` which is a price, converted to match `log(y)`","8a8f8798":"With LinearRegression as final estimator","695d5456":"# EDA Univariate","cbbb0b6e":"It is important to keep initial code values for MSSubClass (could it correlate more to target ?)","1c664436":"### Outliers processing","2b97a8d5":"Box-plot to confirm that the value are in correct order (highest value is higher price)","62ea47a1":"I add the intersections which have a better MI scores than there single counterparts","ff0761c3":"## Cleaning in Pipeline","7f051ac2":"### Wrong Categories","00cb0481":"## Preprocessing Summary","434b6d7a":"## Data Transformation (Power)","a8cf73bf":"## Summary","b5b17c3c":"### NA in numerical columns","7a87962a":"# Submission","6fec3540":"### NA in discrete columns"}}