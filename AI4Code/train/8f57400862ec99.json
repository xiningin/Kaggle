{"cell_type":{"36d27104":"code","8c02b380":"code","8c3860f3":"code","68921ef9":"code","5da6407b":"code","e32e46cd":"code","3870d1ee":"code","50ee296d":"code","54a4b41d":"code","46fd3b1e":"code","ce855b25":"code","e9173d0e":"code","ad85b600":"code","d5acdb04":"code","dbabeab3":"code","c9355730":"code","4df252f3":"code","5b7c0e80":"code","3fdd086d":"code","044fb8cb":"code","d2bde398":"code","42fe5f5e":"code","5bd26c24":"code","11fe71e0":"code","18bba3ca":"code","28075230":"code","ccf4a7c5":"code","f6f5f875":"code","a32e07b9":"code","167b14a3":"code","4e34931e":"code","1aa8491e":"code","ed9a17d7":"code","50ecdccf":"code","38a65180":"code","e64bce3f":"code","69c76d5c":"code","8019132a":"code","60eb58eb":"code","b1e5ea9f":"code","d2480438":"code","c7f1b390":"code","5e11f16d":"markdown","a6683a76":"markdown","3ef768b4":"markdown","4c879538":"markdown","747300d3":"markdown","eb134f40":"markdown","415427fd":"markdown","49c5a488":"markdown","fb95bb58":"markdown","31be1366":"markdown","0e893726":"markdown","aec38649":"markdown","949f87e3":"markdown","c075aefd":"markdown","cc79990d":"markdown","ec1cc598":"markdown","8b164f80":"markdown","1828a4e7":"markdown","f868467b":"markdown","908a7f51":"markdown","72d1c64f":"markdown","7d46de1a":"markdown","a416dbb1":"markdown","b62bfd4a":"markdown","6d215b46":"markdown","bd6cde15":"markdown","185bd28b":"markdown","880157d3":"markdown","f4ecf4c3":"markdown"},"source":{"36d27104":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.callbacks import History\n\nfrom wordcloud import WordCloud, STOPWORDS","8c02b380":"real = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\nreal.head()","8c3860f3":"fake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\nfake.head()","68921ef9":"real['Category'] = 1\nfake['Category'] = 0","5da6407b":"print(real.shape)\nreal.head()","e32e46cd":"print(fake.shape)\nfake.head()","3870d1ee":"dataset = pd.concat([real, fake]).reset_index(drop=True)","50ee296d":"print(dataset.shape)\ndataset.head()","54a4b41d":"dataset.isnull().sum()","46fd3b1e":"dataset['final_text'] = dataset['title'] + dataset['text']\ndataset['final_text'].head()","ce855b25":"dataset['Category'].value_counts()","e9173d0e":"dataset[['Category','subject','final_text']].groupby(['Category','subject']).count()","ad85b600":"plt.figure(figsize=(10,5))\nsns.countplot(x= \"subject\", hue = \"Category\", data=dataset)","d5acdb04":"porter_stemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()","dbabeab3":"stemmed_text = []\nlemmatized_text = []\nfinal_text_result = []\nfor text in dataset['final_text']:\n    result = re.sub('[^a-zA-Z]', ' ', text)\n    result = result.lower()\n    result = result.split()\n    result = [r for r in result if r not in set(stopwords.words('english'))]\n    stemmed_result = [porter_stemmer.stem(r) for r in result]\n    stemmed_text.append(\" \".join(stemmed_result))\n    lemmatized_result = [lemmatizer.lemmatize(r) for r in result]\n    lemmatized_text.append(\" \".join(lemmatized_result))","c9355730":"print(len(stemmed_text))\nprint(len(lemmatized_text))","4df252f3":"plt.figure(figsize=(15,15))\nwc=WordCloud(max_words=2000, width=1600, height=700, stopwords=STOPWORDS).generate(\"\".join(dataset[dataset.Category==1].final_text))\nplt.imshow(wc, interpolation=\"bilinear\")","5b7c0e80":"plt.figure(figsize=(15,15))\nwc=WordCloud(max_words=2000, width=1600, height=700, stopwords=STOPWORDS).generate(\"\".join(dataset[dataset.Category==0].final_text))\nplt.imshow(wc, interpolation=\"bilinear\")","3fdd086d":"def get_prediction(vectorizer, classifier, X_train, X_test, y_train, y_test):\n    pipe = Pipeline([('vector', vectorizer),\n                    ('model', classifier)])\n    model = pipe.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix: \\n\", cm)\n    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))","044fb8cb":"print(\"******USING STEMMED TEXT********\")\nX_train, X_test, y_train, y_test = train_test_split(stemmed_text, dataset['Category'], test_size = 0.3, random_state= 0)\nclassifiers = [LogisticRegression(), SGDClassifier(), MultinomialNB(), BernoulliNB(), LinearSVC(),\n              KNeighborsClassifier(n_neighbors=5), DecisionTreeClassifier(), GradientBoostingClassifier(), \n               RandomForestClassifier(), XGBClassifier()]\nfor classifier in classifiers:\n    print(\"\\n\\n\", classifier)\n    print(\"***********Usng Count Vectorizer****************\")\n    get_prediction(CountVectorizer(), classifier, X_train, X_test, y_train, y_test)\n    print(\"***********Usng TFIDF Vectorizer****************\")\n    get_prediction(TfidfVectorizer(), classifier, X_train, X_test, y_train, y_test)","d2bde398":"print(\"******USING LEMMATIZED TEXT********\")\nX_train, X_test, y_train, y_test = train_test_split(lemmatized_text, dataset['Category'], test_size = 0.3, random_state= 0)\nclassifiers = [LogisticRegression(), SGDClassifier(), MultinomialNB(), BernoulliNB(), LinearSVC(),\n              KNeighborsClassifier(n_neighbors=5), DecisionTreeClassifier(), GradientBoostingClassifier(), \n               RandomForestClassifier(), XGBClassifier()]\nfor classifier in classifiers:\n    print(\"\\n\\n\", classifier)\n    print(\"***********Usng Count Vectorizer****************\")\n    get_prediction(CountVectorizer(), classifier, X_train, X_test, y_train, y_test)\n    print(\"***********Usng TFIDF Vectorizer****************\")\n    get_prediction(TfidfVectorizer(), classifier, X_train, X_test, y_train, y_test)","42fe5f5e":"import tensorflow as tf\nprint(tf.__version__)","5bd26c24":"voc_size = 5000\nonehot_stemmed_text = [one_hot(word, voc_size) for word in stemmed_text]\nprint(len(onehot_stemmed_text))\nonehot_stemmed_text[0]","11fe71e0":"sent_length = 400\nembedded_text = pad_sequences(onehot_stemmed_text, padding='pre', maxlen=sent_length)\nprint(embedded_text)","18bba3ca":"embedding_vector_features = 600\nmodel1 = Sequential()\nmodel1.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))\nmodel1.add(LSTM(100))\nmodel1.add(Dense(1, activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel1.summary()","28075230":"X_final = np.array(embedded_text)\ny_final = dataset['Category']","ccf4a7c5":"X_final.shape,y_final.shape","f6f5f875":"X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.3, random_state = 0)","a32e07b9":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='auto')","167b14a3":"model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size = 256, callbacks=([reduce_lr, early_stop]))","4e34931e":"y_pred = model1.predict_classes(X_test)\nprint(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))","1aa8491e":"cm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix: \\n\", cm)","ed9a17d7":"print(\"Classification Report: \\n\", classification_report(y_test, y_pred))","50ecdccf":"voc_size = 5000\nonehot_lemmatized_text = [one_hot(word, voc_size) for word in lemmatized_text]\nprint(len(onehot_lemmatized_text))\nonehot_lemmatized_text[0]","38a65180":"sent_length = 400\nembedded_text = pad_sequences(onehot_lemmatized_text, padding='pre', maxlen=sent_length)\nprint(embedded_text)","e64bce3f":"embedding_vector_features = 600\nmodel2 = Sequential()\nmodel2.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))\nmodel2.add(LSTM(100))\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel2.summary()","69c76d5c":"X_final = np.array(embedded_text)\ny_final = dataset['Category']","8019132a":"X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.3, random_state = 0)","60eb58eb":"model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size = 256, callbacks=([reduce_lr, early_stop]))","b1e5ea9f":"y_pred = model2.predict_classes(X_test)\nprint(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))","d2480438":"cm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix: \\n\", cm)","c7f1b390":"print(\"Classification Report: \\n\", classification_report(y_test, y_pred))","5e11f16d":"# Table of Contents\n\n1. [Import Packages](#import-packages)\n2. [Read Data](#read-data)\n    * [Create Target based on Real and Fake data](#create-target)\n    * [Concat both real and fake data](#concat-data)\n3. [Data Analysis](#data-analysis)\n    * [Missing value Treatment](#treat-missing-value)\n    * [Merge Title and Text data](#merge-title-text)\n4. [Data Cleaning](#data-cleaning)\n    * [Preprocessing Text to get Stemmed and Lemmatized Corpus](#Preprocess-text)\n    * [WordCloud for label=1 -- Real News](#word-cloud-label-1)\n    * [WordCloud for label=0 -- Fake News](#word-cloud-label-0)\n5. [Classification models using CountVectorizer and TFIDF Vectorizer](#create-models)\n    * [Using CountVectorizer and TFIDF Vectorizer with stemmed text](#counttfidf-stemmed)\n    * [Using CountVectorizer and TFIDF Vectorizer with lemmatized text](#counttfidf-lemmatized)\n6. [LSTM model using One Hot vector](#one-hot-vector-lstm)\n    * [Using One hot representation and Stemmed Text](#one-hot-stemmed)\n    * [Evaluate model](#stemmed-evaluate)\n    * [Using One hot representation and Lemmatized Text](#one-hot-lemmatized)\n    * [Evaluate model](#lemmatized-evaluate)\n7. [Conclusion](#conclusion)\n\n","a6683a76":"**WordCloud for label=0 -- Fake News**\n<a id = \"word-cloud-label-0\"><\/a>","3ef768b4":"# Problem Statement\n\nWe have 2 datasets with Real News data and Fake News data. Each dataset has around 22000 articles. As we merge both, we have around 45000 articles of real and fake. The aim is to build a model to correctly predict if a news is real or fake.","4c879538":"**Evaluate model**\n<a id = \"stemmed-evaluate\"><\/a>","747300d3":"After doing Predictive analysis above using both stemmed text and lemmatized text with Count Vectorizer and TFIDF Vectorizer, we see that the XGB Classifier is giving best results with lemmatized texts and Count Vectorizer with accuracy score of 99.78%","eb134f40":"**Merge Title and Text data**\n<a id = \"merge-title-text\"><\/a>","415427fd":"**Using CountVectorizer and TFIDF Vectorizer with stemmed text**\n<a id=\"counttfidf-stemmed\"><\/a>","49c5a488":"**Missing value Treatment**\n<a id = \"treat-missing-value\"><\/a>","fb95bb58":"**Concat both real and fake data**\n<a id = \"concat-data\"><\/a>","31be1366":"# Data Cleaning\n<a id = \"data-cleaning\"><\/a>","0e893726":"# Data Analysis\n<a id = \"data-analysis\"><\/a>","aec38649":"**Preprocessing Text to get Stemmed and Lemmatized Corpus**\n<a id = \"Preprocess-text\"><\/a>\n\nIn this step we will clean the data that will be used for training. The cleaning will involve these steps-\n1. Removing all the extra information like brackets, any kind of puctuations - commas, apostrophes, quotes, question marks, and more.\n2. Remove all the numeric text, urls\n3. Remove all the stop words like - just, and, or, but, if\n4. Convert all the remaining text in the lower case, separated by space\n5. Generate stemmed text\n6. Generate lemmatized text","949f87e3":"With One hot vector and stemmed text LSTM model is giving 98.7 % accuracy.","c075aefd":"**Using One hot representation and Lemmatized Text**\n<a id = \"one-hot-lemmatized\"><\/a>","cc79990d":"# Classification models using CountVectorizer and TFIDF Vectorizer\n<a id = \"create-models\"><\/a>\n","ec1cc598":"# LSTM model using One Hot vector \n<a id =\"one-hot-vector-lstm\"><\/a>","8b164f80":"**WordCloud for label=1 -- Real News**\n<a id = \"word-cloud-label-1\"><\/a>","1828a4e7":"Below I am creating classification models in 2 steps-\n1. Using stemmed text and both Count Vectorizer and TFIDF Vectorizer\n1. Using lemmatized text and both Count Vectorizer and TFIDF Vectorizer","f868467b":"# Conclusion\n<a id=\"conclusion\"><\/a>\n\nTill this point, we are not getting any benefit in accuracy using LSTM model with Onehot vector. And XGBClassifier with CountVectorizer was giving better results.<br>\nI will create another version of this dataset, where I will use the glove vector and bert.","908a7f51":"As there is no missing value in the dataset, so we we can work directly on title and text columns","72d1c64f":"**Create Target based on Real and Fake data**\n<a id=\"create-target\"><\/a>","7d46de1a":"**Using One hot representation and Stemmed Text**\n<a id = \"one-hot-stemmed\"><\/a>","a416dbb1":"**If you like the kernel please upvote.\nIf you have any questions\/feedback feel free to comment and I will be happy to answer.\n**","b62bfd4a":"**Using CountVectorizer and TFIDF Vectorizer with lemmatized text**\n<a id=\"counttfidf-lemmatized\"><\/a>","6d215b46":"\n<div align='center'><font size=\"6\" color=\"red\"> Misinformation is one of the biggest issues in today's society<\/font><\/div>\n<div align='center'><font size=\"4\" color=\"red\">And Fake news is a real problem\n\n![fake%20news.jpg](attachment:fake%20news.jpg)","bd6cde15":"**Evaluate model**\n<a id = \"lemmatized-evaluate\"><\/a>","185bd28b":"<hr>","880157d3":"# Read Data\n<a id = \"read-data\"><\/a>","f4ecf4c3":"# Import Packages\n<a id = \"import-packages\"><\/a>"}}