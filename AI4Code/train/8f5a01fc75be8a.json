{"cell_type":{"b2b39608":"code","bf79d4f5":"code","5b01b31c":"code","dd648949":"code","87f2d91d":"code","24f97433":"code","80a664a5":"code","b163d776":"code","2165a1d6":"code","f8758a2c":"code","64377553":"code","3a143ef4":"code","4c29ac9f":"code","2b4969b9":"code","d2f2c3e5":"code","91bc93b2":"code","35470256":"code","b756eff2":"code","22abb0c0":"code","6db309bf":"code","7eb567d9":"code","a6f8f3c8":"code","7f4decdb":"code","2c10af5b":"code","16ad1a17":"code","b718cfd4":"code","e7763b26":"code","d4607a04":"code","5c9fb963":"code","dca8ca64":"code","71950211":"code","d0052693":"code","87824e70":"code","0b4f25ff":"code","be9d9dc7":"code","869d4058":"markdown","d538c9d7":"markdown","2d07fa4a":"markdown","e04ddbca":"markdown","65d34643":"markdown","a1dd013c":"markdown","850cb6d9":"markdown","e9d4985a":"markdown","5703f3bc":"markdown","11fb7868":"markdown","b489d968":"markdown","83e6fea8":"markdown","b203b206":"markdown","8fe5f512":"markdown","8d6bf2dc":"markdown","2046ac8d":"markdown","650414c2":"markdown","56e3f12c":"markdown","5d538768":"markdown","2425c747":"markdown","3865415d":"markdown","aeaba233":"markdown","e484351e":"markdown","be9e5d69":"markdown","cf654903":"markdown","35e8448d":"markdown","d0beed92":"markdown","0be626b5":"markdown","0756eb07":"markdown","6a8621cc":"markdown","10c5f08d":"markdown","ebb25c81":"markdown","8bc4b6de":"markdown","fc98dede":"markdown","9a29c87c":"markdown","7bca328c":"markdown"},"source":{"b2b39608":"# Importing the used dependency libraries\n\nimport math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotig graphs library\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.cluster import MiniBatchKMeans # Efficient clustering algorithm\nfrom sklearn.decomposition import TruncatedSVD # SVD decomposition for performing LSA\nfrom sklearn.preprocessing import Normalizer # Normalizer with instance, useful as preprocessing before clustering\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import metrics # used to output siuette score\n\nfrom scipy import stats # using Kruskal Wallis unparametric test\nimport nltk.stem # Stemming tool for processing the transcript tokens\nimport datetime\nimport ast # Parser for python objects\n\n# Our deterministic seed\nSEED = 12\n\n################# Inherit TfidfVectorizer to adapt it to stemming ###################\nenglish_stemmer = nltk.stem.SnowballStemmer('english')\n\nclass StemmedTfidfVectorizer(TfidfVectorizer):\n    \"\"\"Tf-idf Vectorizer which is applying also stemming i.e. it boils down\n    each word to its root. In the semantic analysis we are doing where we don't\n    take into account the parts of speach stemming is a good choice to generalize\n    better over the general meaning the text of interest has.\"\"\"\n    # Overriding the build_analyzer\n    def build_analyzer(self):\n        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n    \n####################################################################################\nclass RatingsVectorizer():\n    \"\"\"Used to parse and convert to vector the ratings for each TED talk. It has\n    to 'book keep' all of the rating terms and their corresponding indices.\"\"\"\n\n    def __init__(self):\n        # It stores as key the rating name and as value\n        # the corresponding index.\n        self.vocabulary_ = dict()\n        \n    def get_feature_names(self):\n        return list(self.vocabulary_.keys())\n    \n    def parse_ratings(self, json_collection):\n        \"\"\"It returns a frequency table where the rows are talks and columns - ratings\"\"\"\n\n        # First pass.\n        for json_string in json_collection:\n            ratings = ast.literal_eval(json_string)\n            \n            for rating in ratings:\n                if rating['name'] not in self.vocabulary_:\n                    self.vocabulary_[rating['name']] = len(self.vocabulary_)\n                    \n        # Second pass.\n        freq_table = np.zeros(shape = (len(json_collection), len(self.vocabulary_)))\n        for i in range(len(json_collection)):\n            ratings = ast.literal_eval(json_collection[i])\n            \n            for rating in ratings:\n                freq_table[i,self.vocabulary_[rating['name']]] = rating['count']\n\n        return freq_table\n            \n########################## Diagnostic Tools ######################\n\ndef pearson_r(x, y):\n    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n\n    # Compute correlation matrix\n    corr_mat = np.corrcoef(x, y)\n\n    # Return entry [0,1]\n    return corr_mat[0,1]\n\ndef k_means_diagnostics(x,ted_df, max_k):\n    \n    def k_wss(km, k, target):\n        wss = 0\n        for i in range(k):\n            y = target[i == km.labels_]\n            mean = y.mean()\n            delta = y - mean\n            wss += np.sum(delta * delta)\n        return wss\n\n    def k_bss(km, k, target):\n        bss = 0\n        mean = target.mean()\n        for i in range(k):\n            y = target[i == km.labels_]\n            delta = y.mean() - mean\n            bss += y.shape[0] * delta * delta\n\n        return bss\n\n    sum_of_squared_distances = []\n    comments_wss_bss = []\n    views_wss_bss = []\n    \n    K = range(2,max_k)\n    for k in K:\n        km = MiniBatchKMeans(n_clusters=k, init='k-means++', n_init=1,\n                             init_size=1000, batch_size=1000, random_state = SEED)\n        km = km.fit(X)\n        sum_of_squared_distances.append(km.inertia_)\n\n        bss_c = k_bss(km, k, ted_df[\"comments\"].values)\n        wss_c = k_wss(km, k, ted_df[\"comments\"].values)\n\n        bss_v = k_bss(km, k, ted_df[\"views\"].values)\n        wss_v = k_wss(km, k, ted_df[\"views\"].values)\n\n        comments_wss_bss.append(wss_c\/bss_c)\n        views_wss_bss.append(wss_v\/bss_v)\n        \n    plt.plot(K, sum_of_squared_distances, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('Sum of Squared Distances')\n    plt.title('Elbow Method For Optimal k')\n    plt.show()\n\n    plt.plot(K, comments_wss_bss, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('WSS\/BSS of the variable \\'comments\\'')\n    plt.title('Elbow Method For Optimal k')\n    plt.show()\n\n    plt.plot(K, views_wss_bss, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('WSS\/BSS of the variable \\'views\\'')\n    plt.title('Elbow Method For Optimal k')\n    plt.show()\n    \ndef cluster_stratification(k, ted_df, order_centroids, df_clust_name, terms):\n\n    for i in range(k):\n        print(\"Cluster %d:\" % i, end='')\n        for ind in order_centroids[i, :10]:\n            print(' %s;' % terms[ind], end='')\n        print()\n\n    plt.figure(1, figsize=(21, 7))\n    tran_count = ted_df.groupby([df_clust_name]).size()\n    tran_count.plot.bar()\n    plt.ticklabel_format(style='plain', axis='y')\n    plt.xticks(rotation=0)\n    plt.xlabel('Cluster indices')\n    plt.ylabel('Count of Ted talks')\n    plt.title('Frequency of the talks within the clusters')\n    plt.show()\n\n    plt.figure(1, figsize=(21, 7))\n    tran_comments = ted_df.groupby([df_clust_name])['comments'].agg('mean')\n    tran_comments.plot.bar()\n    plt.ticklabel_format(style='plain', axis ='y')\n    plt.xticks(rotation=0)\n    plt.xlabel('Cluster indices')\n    plt.ylabel('Average comments count')\n    plt.title('Average comments count within the clusters')\n    plt.show()\n\n    plt.figure(1, figsize = (21, 7))\n    tran_views = ted_df.groupby([df_clust_name])['views'].agg('mean')\n    tran_views.plot.bar()\n    plt.ticklabel_format(style ='plain', axis ='y')\n    plt.xticks(rotation=0)\n    plt.xlabel('Cluster indices')\n    plt.ylabel('Average reviews count')\n    plt.title('Average reviews count within the clusters')\n    plt.show()","bf79d4f5":"ted_meta = pd.read_csv('..\/input\/ted_main.csv')\n\nprint(\"Total count of TED Talk views: {} and comments: {}\"\n      .format(ted_meta['views'].sum(), ted_meta['comments'].sum()))\nprint() \nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(ted_meta.describe())","5b01b31c":"print(ted_meta.isnull().any())\nprint(ted_meta.isnull().sum())\nted_meta.fillna('Not specified', inplace = True)","dd648949":"plt.figure(1)\nplt.title('Box of the talks views')\nboxplot = ted_meta.boxplot(column=['views'])\nprint(boxplot)\nplt.ylabel('View counts')\naxes = plt.gca()\n# We prune the long tail up to 6 000 000 for the sake of visibility\naxes.set_ylim([0,6e6])\n\nplt.show()\n\nplt.figure(2)\nted_meta['views'].plot.hist(grid=True, bins=9, rwidth=0.9, color='#607c8e')\nplt.title('Histogramm of the talks views')\nplt.xlabel('View counts')\nplt.ylabel('Freqency')\nplt.grid(axis='y', alpha=0.75)\nplt.show()","87f2d91d":"tfidf_vectorizer = StemmedTfidfVectorizer(stop_words = 'english', max_df=0.3)\n\n# Inefficient stroring of the word frequencies within the talk description\n# in the form of a dense matrix with row for each talk and a column for each observed word.\ntalks_word_tf_idf = tfidf_vectorizer.fit_transform(ted_meta['description'].values).toarray()\n\nid_to_word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n\ntotal_word_tf_idf = talks_word_tf_idf.sum(axis=0)\n\nword_views_corr = np.empty(talks_word_tf_idf.shape[1])\nfor i in range(talks_word_tf_idf.shape[1]):\n    word_views_corr[i] = pearson_r(talks_word_tf_idf[:,i], ted_meta['views'].values)\n\nword_views_ordered_corr = np.argsort(-np.abs(word_views_corr))\n\nfig = plt.figure(1, figsize=(7, 7))\np = fig.add_subplot(111)\n\nk = 0\ni = 0\nwhile(k < 30 and i < total_word_tf_idf.shape[0]):\n    # We filter just the words with bigger tf_idf than 10\n    if total_word_tf_idf[word_views_ordered_corr[i]] > 10:\n        x  = total_word_tf_idf[word_views_ordered_corr[i]]\n        y = word_views_corr[word_views_ordered_corr[i]]\n        p.scatter(x, y, alpha=0.5)\n        p.text(x, y,id_to_word[word_views_ordered_corr[i]])\n\n        k += 1\n    i += 1\n    \nplt.title('Word TF-IDF vs Talks Views count Correlation')\nplt.xlabel('Overall TF-IDF word mass')\nplt.ylabel('TF-IDF vd Views counts Correlation')\nplt.show()","24f97433":"ted_meta.plot.scatter(x='views',y='duration', alpha=0.4)\nprint(\"The correlation between the 'duration' and 'views': {}\".format(ted_meta['views'].corr(ted_meta['duration'])))","80a664a5":"ted_meta.plot.scatter(x='views',y='languages', alpha = 0.4)\nprint(\"The correlation between the 'languages' and 'views': {}\".format(ted_meta['views'].corr(ted_meta['languages'])))","b163d776":"plt.figure(1, figsize=(21, 7))\nspeaker_views = ted_meta.groupby(['main_speaker'])['views'].agg('sum')\nspeaker_views_top_60 = speaker_views.sort_values(ascending=False).head(60)\nspeaker_views_top_60.plot.bar()\nplt.ticklabel_format(style='plain', axis='y')\nplt.title('Top 60 speakers with most views of their talks')\nplt.xlabel('Speakers')\nplt.ylabel('Total views count')","2165a1d6":"plt.figure(1, figsize=(21, 7))\nspeaker_occupation_views = ted_meta.groupby(['speaker_occupation'])['views'].agg('sum')\nspeaker_occupation_views_top_60 = speaker_occupation_views.sort_values(ascending=False).head(60)\nspeaker_occupation_views_top_60.plot.bar()\nplt.ticklabel_format(style='plain', axis='y')\nplt.title('Top 60 speaker occupations with most talk views')\nplt.xlabel('Occupations')\nplt.ylabel('Total views count')","f8758a2c":"speaker_views_avg = ted_meta.groupby(['main_speaker'])['views'].agg('mean')\nspeaker_occurrenes = ted_meta.groupby('main_speaker')['views'].nunique()\nspeaker_views_avg = speaker_views_avg.rename('avg_views')\nspeaker_occurrenes = speaker_occurrenes.rename('speaker_occurrenes')\n\nspeaker_views = pd.concat([speaker_occurrenes, speaker_views_avg], axis=1).reset_index()\n\nspeaker_views_top_60_occ = speaker_views.sort_values(ascending=False, by = 'speaker_occurrenes').head(60)\n\nfig = plt.figure(1, figsize=(10, 10))\np = fig.add_subplot(111)\nplt.ticklabel_format(style='plain', axis='y')\nfor i in range(speaker_views.shape[0]):\n    x  = speaker_views.at[i,'speaker_occurrenes']\n    y = speaker_views.at[i,'avg_views']\n    p.scatter(x, y, alpha=0.5)\n    p.text(x, y, speaker_views.at[i,'main_speaker'])\n\nplt.title('Speakers with a lot of views and a lot of occurrances')\nplt.xlabel('Number of Occurrances')\nplt.ylabel('Average views count per speaker')\nplt.show()","64377553":"ted_meta[ted_meta['main_speaker'] == 'Ken Robinson']['views']","3a143ef4":"plt.figure(1, figsize=(21, 7))\nevents_views = ted_meta.groupby(['event'])['views'].agg('sum')\nevents_views_top_60 = events_views.sort_values(ascending=False).head(60)\nevents_views_top_60.plot.bar()\nplt.ticklabel_format(style='plain', axis='y')\nplt.title('Top 60 events with most talk views')\nplt.xlabel('Events')\nplt.ylabel('Total views count')\nplt.show()\n","4c29ac9f":"ted_events = ted_meta.groupby('event').size()\nted_events.columns = ['event', 'count']\nted_events = ted_events.sort_values(ascending=False)\n\nplt.figure(figsize=(15,5))\nted_events.head(10).plot.bar()\nplt.title('Top 10 events with most talks')\nplt.xlabel('Events')\nplt.ylabel('Talks count within an event')\nplt.show()","2b4969b9":"plt.figure(figsize=(30,5))\nted_meta.sort_values(by='film_date', ascending=False)[ted_meta['event'].isin(ted_events.head(20).index.values)][['views','event']].boxplot(by='event', grid=False, rot=45, fontsize=15)\naxes = plt.gca()\naxes.set_ylim([0,6e6])\nplt.ylabel('Total views count')\nplt.show()\n\nstats.kruskal(*[group[\"views\"].values for name, group in ted_meta.groupby(\"event\")])","d2f2c3e5":"plt.figure(1)\nted_meta['num_speaker'].plot.hist(grid=True, bins=5, rwidth=0.9, color='#607c8e')\nplt.title('Histogramm of the talks speaker count')\nplt.xlabel('Speaker counts')\nplt.xticks(range(1,5))\nplt.ylabel('Freqency')\nplt.grid(axis='y', alpha=0.75)\n\nplt.figure(2)\nted_meta['num_speaker'].plot.hist(grid=True, bins=5, rwidth=0.9, color='#607c8e')\nplt.title('Log Histogramm of the talks speaker count')\nplt.xlabel('Speaker counts')\nplt.xticks(range(1,5))\nplt.ylabel('Log Freqency')\nplt.yscale('log')\nplt.grid(axis='y', alpha=0.75)","91bc93b2":"ted_meta.plot.scatter(x='views',y='num_speaker', alpha = 0.4)\nprint(\"The correlation between the 'num_speaker' and 'views': {}\".format(ted_meta['views'].corr(ted_meta['num_speaker'])))","35470256":"# Preparing the tags to be parsed by the CountVectorizer(We separate the tags by space and unite\n# the words within a tag by '_'), expecting to be parsed as sentence and treated as sentence tokens.\nted_meta['tags_text'] = ted_meta['tags'].str.replace('(?<=[A-Za-z])(\\s)(?=[A-Za-z])','_')\nted_meta['tags_text'] = ted_meta['tags_text'].str.replace('\\[|\\'|\\]', ' ')\ncount_vectorizer = CountVectorizer()\n\n# Very inefficient stroring of the word frequencies within the talk description,\n# in the form of a dense matrix with row for each talk and a column for each word ever observed.\n# Effectivelly we are using count vectorizer for quick and dirty transforming of the tags into\n# One-hot Encoding, since we have max one tag occurrence in a talk\ntags_frequency = count_vectorizer.fit_transform(ted_meta['tags_text'].values).toarray()\n\nplt.figure(1, figsize=(21, 7))\ntags_df = pd.Series(tags_frequency.sum(axis=0))\ntags_df.name = 'tags_frequency'\ntags_df.index = count_vectorizer.get_feature_names()\ntags_df_top60 = tags_df.sort_values(ascending=False).head(60)\nplt.title('Top 60 most frequent tags')\nplt.xlabel('Tags')\nplt.ylabel('Tags frequency')\ntags_df_top60.plot.bar()","b756eff2":"ted_meta['published_month'] = ted_meta['published_date'].apply(lambda x: datetime.datetime.fromtimestamp( int(x)).strftime('%m'))\n\nplt.figure(1, figsize=(21, 7))\nmonth_views = ted_meta.groupby(['published_month'])['views'].agg('sum')\nmonth_views.plot.bar()\nplt.ticklabel_format(style='plain', axis='y')\nplt.title('Views across Months')\nplt.xlabel('Published Month')\nplt.ylabel('Views Counts')\nplt.show()","22abb0c0":"# The dataset has been published in 2017-09-09 ~ 1504915200(unix)\n\n# Caluclate the approximate age in days\nted_meta['talk_age'] = ted_meta['published_date'].apply(lambda x: int((1504915200 - x)\/(3600 * 24)))\n\nted_meta.plot.scatter(x='views',y='talk_age', alpha = 0.4)\nprint(\"The correlation between the 'talk_age' and 'views': {}\".format(ted_meta['views'].corr(ted_meta['talk_age'])))","6db309bf":"plt.figure(1)\nplt.title('Box of the talks comments counts')\nboxplot = ted_meta[ted_meta['comments'] < 2e3].boxplot(column=['comments'])\nplt.ylabel('View counts')\naxes = plt.gca()\n# We prune the long tail up to 2 000 for the sake of visibility\naxes.set_ylim([0,2e3])\n\nplt.show()\n\nplt.figure(2)\nted_meta[ted_meta['comments'] < 2e3]['comments'].plot.hist(grid=True, bins=9, rwidth=0.9, color='#607c8e')\nplt.title('Histogramm of the talks comments')\nplt.xlabel('View comments')\nplt.ylabel('Freqency')\nplt.grid(axis='y', alpha=0.75)\n\nplt.show()","7eb567d9":"ted_meta.plot.scatter(x='comments',y='views', alpha=0.4)\nprint(\"The correlation between the 'views' and 'comments': {}\".format(ted_meta['comments'].corr(ted_meta['views'])))","a6f8f3c8":"ted_meta.plot.scatter(x='comments',y='duration', alpha=0.4)\nprint(\"The correlation between the 'duration' and 'comments': {}\".format(ted_meta['comments'].corr(ted_meta['duration'])))","7f4decdb":"plt.figure(1, figsize=(21, 7))\nmonth_comments = ted_meta.groupby(['published_month'])['comments'].agg('sum')\nmonth_comments.plot.bar()\nplt.ticklabel_format(style='plain', axis='y')\nplt.title('Comments across Months')\nplt.xlabel('Published Month')\nplt.ylabel('Comments Counts')\nplt.show()","2c10af5b":"ted_meta.plot.scatter(x='comments',y='talk_age', alpha = 0.4)\nprint(\"The correlation between the 'talk_age' and 'comments': {}\".format(ted_meta['comments'].corr(ted_meta['talk_age'])))","16ad1a17":"word_comments_corr = np.empty(talks_word_tf_idf.shape[1])\nfor i in range(talks_word_tf_idf.shape[1]):\n    word_comments_corr[i] = pearson_r(talks_word_tf_idf[:,i], ted_meta['comments'].values)\n\nword_comments_ordered_corr = np.argsort(-np.abs(word_views_corr))\n\nfig = plt.figure(1, figsize=(7, 7))\np = fig.add_subplot(111)\n\nk = 0\ni = 0\nwhile(k < 30 and i < total_word_tf_idf.shape[0]):\n    # We filter just the words with bigger frequency than 10\n    if total_word_tf_idf[word_comments_ordered_corr[i]] > 10:\n        x  = total_word_tf_idf[word_comments_ordered_corr[i]]\n        y = word_comments_corr[word_comments_ordered_corr[i]]\n        p.scatter(x, y, alpha=0.5)\n        p.text(x, y,id_to_word[word_comments_ordered_corr[i]])\n\n        k += 1\n    i += 1\nplt.title('Word TF-IDF vs Talks Comments count Correlation')\nplt.xlabel('Overall TF-IDF word mass')\nplt.ylabel('TF-IDF vs Comments counts Correlation')\nplt.show()","b718cfd4":"ratings_vectorizer = RatingsVectorizer()\nrating_freq = ratings_vectorizer.parse_ratings(ted_meta['ratings'])\nall_talks_ratings_count = rating_freq.sum(axis=0)\n\n# Standardizing the ratings\nprint(rating_freq.std(axis=0, keepdims=True).shape)\nrating_freq = rating_freq \/ rating_freq.std(axis=0, keepdims=True)\n\nid_to_word = dict((v, k) for k, v in ratings_vectorizer.vocabulary_.items())\n\nprint(\"The distinct ratings types count is {}\".format(len(id_to_word)))\n\nratings_comments_corr = np.empty(rating_freq.shape[1])\nfor i in range(rating_freq.shape[1]):\n    ratings_comments_corr[i] = pearson_r(rating_freq[:,i], ted_meta['comments'].values)\n\nratings_comments_ordered_corr = np.argsort(-np.abs(ratings_comments_corr))\n\nfig = plt.figure(1, figsize=(7, 7))\np = fig.add_subplot(111)\n\ni = 0\nwhile(i < all_talks_ratings_count.shape[0]):\n    x  = all_talks_ratings_count[ratings_comments_ordered_corr[i]]\n    y = ratings_comments_corr[ratings_comments_ordered_corr[i]]\n    p.scatter(x, y, alpha=0.5)\n    p.text(x, y,id_to_word[ratings_comments_ordered_corr[i]])\n    i += 1\n    \nplt.ylabel('Correlation')\nplt.xlabel('Ratings Total Count')\nplt.title('Ratings Counts vs Comments Counts Correlation')\nplt.show()\n\nN_CLUSTERS_R = 6\nnormalizer = Normalizer(copy=False)\n\n# Truncated frequency table\nX = normalizer.transform(rating_freq)\n\nk_means_diagnostics(X, ted_meta, 40)\nkm0 = MiniBatchKMeans(n_clusters = N_CLUSTERS_R, init ='k-means++', n_init=1, tol = 0.01,\n                         init_size = 1000, batch_size = 1000, random_state = SEED)\n\nkm0.fit(X)\n\norder_centroids = km0.cluster_centers_.argsort()[:, ::-1]\nterms = ratings_vectorizer.get_feature_names()\n\nfor i in range(N_CLUSTERS_R):\n    print(\"Cluster %d:\" % i, end='')\n    current_mass = 0\n    mass_limit = 0.5 * km0.cluster_centers_[i,].sum()\n    for ind in order_centroids[i,:]:\n        current_mass += km0.cluster_centers_[i,ind]\n        # We filter the most present words up to 50% of thw whole centroid mass\n        if(current_mass > mass_limit):\n            break\n        print(' %s' % terms[ind], end='')\n    print()\n    \nted_meta['rating_clust'] = km0.labels_\n\nplt.figure(1, figsize=(21, 7))\nrating_counts = ted_meta.groupby(['rating_clust']).size()\nrating_counts.plot.bar()\nplt.ticklabel_format(style='plain', axis='y')\nplt.ylabel('Talks Count within Clusters')\nplt.xlabel('Clusters')\nplt.title('Talks Count representation within Raiting Clusters')\nplt.show()\n\nplt.figure(1, figsize=(21, 7))\ndesc_views = ted_meta.groupby(['rating_clust'])['views'].agg('mean')\ndesc_views.plot.bar()\nplt.ticklabel_format(style='plain', axis='y')\nplt.ylabel('Average Views Count within Clusters')\nplt.xlabel('Clusters')\nplt.title('Average Views Count representation within Raiting Clusters')\nplt.show()\n\nplt.figure(1, figsize=(21, 7))\ndesc_comments = ted_meta.groupby(['rating_clust'])['comments'].agg('mean')\ndesc_comments.plot.bar()\nplt.ticklabel_format(style='plain', axis='y')\nplt.ylabel('Average Comments Count within Clusters')\nplt.xlabel('Clusters')\nplt.title('Average Comments Count representation within Raiting Clusters')\nplt.show()","e7763b26":"plt.figure(1, figsize=(21, 7))\nratings_df = pd.Series(all_talks_ratings_count)\nratings_df.name = 'rating_frequency'\nratings_df.index = terms\nratings_df = ratings_df.sort_values(ascending=False)\nratings_df.plot.bar()\nplt.ylabel('Total count of ratings across the talks')\nplt.xlabel('Ratings')\nplt.title('Total ratings count')\nplt.show()","d4607a04":"ted_transcripts = pd.read_csv('..\/input\/transcripts.csv')\nted_transcripts = pd.merge(ted_transcripts, ted_meta[['comments','url']], on='url')","5c9fb963":"tfidf_vectorizer = StemmedTfidfVectorizer(stop_words = 'english', max_df=0.3, min_df=0.01)\n\n# Very inefficient stroring of the word frequencies within the talk description,\n# in the form of a dense matrix with row for each talk and a column for each word ever observed.\ntalks_word_tf_idf = tfidf_vectorizer.fit_transform(ted_transcripts['transcript'].values).toarray()\n\nid_to_word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n\ntotal_word_tf_idf = talks_word_tf_idf.sum(axis=0)\n\nword_comments_corr = np.empty(talks_word_tf_idf.shape[1])\nfor i in range(talks_word_tf_idf.shape[1]):\n    word_comments_corr[i] = pearson_r(talks_word_tf_idf[:,i], ted_transcripts['comments'].values)\n\nword_comments_ordered_corr = np.argsort(-np.abs(word_comments_corr))\n\nfig = plt.figure(1, figsize=(7, 7))\np = fig.add_subplot(111)\n\n\ni = 0\nwhile(i < 10):\n    \n    x  = total_word_tf_idf[word_comments_ordered_corr[i]]\n    y = word_comments_corr[word_comments_ordered_corr[i]]\n    p.scatter(x, y, alpha=0.5)\n    p.text(x, y,id_to_word[word_comments_ordered_corr[i]])\n    i += 1\n    \nplt.title('Word TF-IDF vs Talks Comments count Correlation')\nplt.xlabel('Overall TF-IDF word mass')\nplt.ylabel('TF-IDF vd Views counts Correlation')\nplt.show()","dca8ca64":"plt.figure(1, figsize=(21, 7))\ntr_words_df = pd.Series(total_word_tf_idf)\ntr_words_df.name = 'tr_words_tf_idf'\ntr_words_df.index = list(id_to_word.values())\ntr_words_df_top60 = tr_words_df.sort_values(ascending=False).head(30)\nplt.title('Total TF-IDF word importance of the top 60 words')\nplt.xlabel('Words')\nplt.ylabel('Overall TF-IDF word mass over all the talks')\ntr_words_df_top60.plot.bar()","71950211":"# Inner join between the bot tables. We rop some rows, but it is ok.\nted_df = pd.merge(pd.read_csv('..\/input\/transcripts.csv'), pd.read_csv('..\/input\/ted_main.csv'), on='url')\n\n# Preparing the tags to be parsed by the CountVectorizer(We separate the tags by space and unite\n# the words within a tag by '_'), expecting to be parsed as sentence and treated as sentence tokens.\nted_df['tags_text'] = ted_df['tags'].str.replace('(?<=[A-Za-z])(\\s)(?=[A-Za-z])','_')\nted_df['tags_text'] = ted_df['tags_text'].str.replace('\\[|\\'|\\]', ' ')\n\ncount_vectorizer = CountVectorizer()\n\n# Very inefficient stroring of the word frequencies within the talk description,\n# in the form of a dense matrix with row for each talk and a column for each word ever observed.\n# Effectivelly we are using count vectorizer for quick and dirty transforming of the tags into\n# One-hot Encoding, since we have max one tag occurrence in a talk\ntags_frequency = count_vectorizer.fit_transform(ted_df['tags_text'].values).toarray()","d0052693":"N_COMPONENTS = 40\nN_CLUSTERS_T = 20\n\ntfidf_vectorizer = StemmedTfidfVectorizer(stop_words = 'english', max_df=0.3, min_df=0.01)\ntfidf = tfidf_vectorizer.fit_transform(ted_df['transcript'].values)\n\nnormalizer = Normalizer(copy=False)\nsvd = TruncatedSVD(n_components=N_COMPONENTS, n_iter=10, random_state=SEED)\nlsa = make_pipeline(svd, normalizer)\n\nX = lsa.fit_transform(tfidf)\n\nexplained_variance = svd.explained_variance_ratio_.sum()\nprint(\"Explained variance of the SVD step for {} kept components: {}%\".format(N_COMPONENTS,\n    int(explained_variance * 100)))\n\nprint(\"Bag of words count {}\".format(\n    tfidf.shape[1]))\n\nindex = np.arange(N_COMPONENTS)\nplt.bar(index,svd.explained_variance_ratio_)\nplt.title('Explained Variance Chart')\nplt.xlabel('Sungular value index')\nplt.ylabel('Explained variance ratio')\nplt.show()\n\nk_means_diagnostics(X,ted_df, max_k = 40)\n\nkm0 = MiniBatchKMeans(n_clusters=N_CLUSTERS_T, init='k-means++', n_init=1, tol = 0.01,\n                         init_size=1000, batch_size=1000, random_state = SEED)\n\nkm0.fit(X)\n\noriginal_space_centroids = svd.inverse_transform(km0.cluster_centers_)\norder_centroids = original_space_centroids.argsort()[:, ::-1]\nterms = tfidf_vectorizer.get_feature_names()\n\nted_df['transcript_clust'] = km0.labels_\ncluster_stratification(N_CLUSTERS_T, ted_df, order_centroids, 'transcript_clust', terms)\n","87824e70":"N_COMPONENTS = 40\nN_CLUSTERS_D = 17\n\ntfidf_vectorizer = StemmedTfidfVectorizer(stop_words = 'english', max_df=0.35, min_df=2)\ntfidf = tfidf_vectorizer.fit_transform(ted_df['description'].values)\n\nnormalizer = Normalizer(copy=False)\nsvd = TruncatedSVD(n_components=N_COMPONENTS, n_iter=10, random_state=SEED)\nlsa = make_pipeline(svd, normalizer)\n\nX = lsa.fit_transform(tfidf)\n\nexplained_variance = svd.explained_variance_ratio_.sum()\nprint(\"Explained variance of the SVD step: {}%\".format(\n    int(explained_variance * 100)))\n\nprint(\"Bag of words count {}\".format(\n    tfidf.shape[1]))\n\nindex = np.arange(N_COMPONENTS)\nplt.bar(index,svd.explained_variance_ratio_)\nplt.title('Explained Variance Chart')\nplt.xlabel('Sungular value index')\nplt.ylabel('Explained variance ratio')\nplt.show()\n\nk_means_diagnostics(X, ted_df, max_k = 40)\n\nkm0 = MiniBatchKMeans(n_clusters = N_CLUSTERS_D, init ='k-means++', n_init=1, tol = 0.01,\n                         init_size = 1000, batch_size = 1000, random_state = SEED)\n\nkm0.fit(X)\n\noriginal_space_centroids = svd.inverse_transform(km0.cluster_centers_)\norder_centroids = original_space_centroids.argsort()[:, ::-1]\nterms = tfidf_vectorizer.get_feature_names()\n    \nted_df['description_clust'] = km0.labels_\ncluster_stratification(N_CLUSTERS_D, ted_df, order_centroids, 'description_clust', terms)\n","0b4f25ff":"N_CLUSTERS_TAGS = 6\n\nk_means_diagnostics(tags_frequency, ted_df, max_k = 40)\n\n# We are using MiniBatchKMeans which is well suited in case of sparse examples as in our case with one-hot\n# encoded tags. See the very good paper of the algorithm  http:\/\/www.eecs.tufts.edu\/~dsculley\/papers\/fastkmeans.pdf\nkm = MiniBatchKMeans(n_clusters=N_CLUSTERS_TAGS, init='k-means++', n_init=1, tol=0.01,\n                         init_size=1000, batch_size=1000, random_state = SEED)\n\nkm.fit(tags_frequency)\n\nterms = count_vectorizer.get_feature_names()\ncluster_tags_freq = np.empty((N_CLUSTERS_TAGS,tags_frequency.shape[1]))\nfor i, c in enumerate(km.labels_):\n    cluster_tags_freq[c,:] += tags_frequency[i,:]\n    \n# Normalized by the total tag frequency\ncluster_tags_freq \/= cluster_tags_freq.sum(axis = 0, keepdims = True)\norder_centroids = cluster_tags_freq.argsort()[:, ::-1]\n\nted_df['tag_clust'] = km.labels_\n\ncluster_stratification(N_CLUSTERS_TAGS, ted_df, order_centroids, 'tag_clust', terms)","be9d9dc7":"clust_occ_freq = pd.crosstab(ted_df.tag_clust,ted_df.speaker_occupation)\nterms = clust_occ_freq.columns.values.tolist()\nclust_occ_freq = clust_occ_freq.values\n\n# Normalized by the total tag frequency\nclust_occ_freq = clust_occ_freq \/ clust_occ_freq.sum(axis= 0, keepdims = True)\norder_centroids = clust_occ_freq.argsort()[:, ::-1]\n\nfor i in range(N_CLUSTERS_TAGS):\n    print(\"Cluster %d:\" % i, end='')\n    for ind in order_centroids[i, :10]:\n        print(' %s;' % terms[ind], end='')\n    print()\n","869d4058":"Here we also could have a look at the boxplots of the views count the first 30 appearing events sorted by `film_date`. We limit the y axis for the sake of visibility. At least visually it seems that the events are having some different distribution of the views which might mean that we have some noticable random effect represented by the talks . In the same time it doesn't  seem that there is some time drift so that most recent events tend to have higher view count. Rather, it seems that there is some cyclic pattern across years which doesn't have logical explanation at first sight. In order to verify the significancy of the difference of the ount of views across talks we are conducting Kruskal-Wallis test which is an non-parametric alternative of one-way ANOVA. Here we cannot use ANOVA because its assumption about normally distributed data is violated as we saw from the histogram and the box plot of the views above. Our Null hypothesis that the view counts within each one of the events are coming from the same distribution. As we can judge  from the `p-value` which is giving us a sense what is the probability of having more extreme stochastig dominance of one of the events under the assumption that the Null hypothesis is true. In our case the `p-value` is very small (0e-37!) which means that the observed stochastic dominance under the condition of true Null hypothesis is significantly high with chance of having more extreme one converging to 0. Thus we reject the null hypothesis which means that indeed the events are signiicantly different in terms of `views count`.","d538c9d7":"First lets look at the descriptions of the talks and how they are related to the talks view count. In order to see discover some patterns in the semantics of the descriptions we'll use here simple` 'bag of words'` analysis. `Bag of words` means that we are just analysing the frequency of the words(terms) in each document(in our case talk description)  in order to categorize documents or terms according to their common re-occurrence. In practice the *tf-idf*(**term frequency\u2013inverse document frequency**) coefficients are good for that. Long story short they are not build upon the simple term frequency, but are scaled according to the overal vocabulary size of the documents, so that the terms importance is agnostic to their overal frequency, Also the *tf-idf* coefficient indicates also how rare actually the terms are, where terms which are occurring only in small portion of the documets are having bigger wight in these particular documents.\n\nIn our case we filter out the stop words which are appearing in any texts such as 'the', 'and', etc. and also words, which are appearing in more than 30% of the descriptions.  Once we build the *tf-idf* coefficients for each description and remaining words after the filtering we build a correlation of the *tf-idf* coefficients of each word and the *count of views* of the talks, so that in the end we rank and plot the words which have highest correlation across talks of their *tf-idf*  coefficient and the talk *views count*. This way we have straight forward indicator important words tend to be more frequently in widly viewed talks.\n\nAs we can see the most popular word which is as tends to be in a lot polular talks is the word 'think'. In general the correlation is not that big whereas the highest once reach around 0.1. From not that popular words with high correlation we can see 'happi', 'move' and 'funny. Afterwards  we have some group of words with some correlation with the views count such as 'creativ', 'love', funni', 'case', 'educ'.\n\nIt should be noted that we are using stemmer which boils down the words to their semantic roots, so what we are actually seeing is just grouping of all words in the descriptions with these root. It is a form of summerizing all the words with the same root and possibly same meaning which is good practice when dealing with 'bag of words'.","2d07fa4a":"Q1. `What are the main drivers which attract the interest of the people to the TED Talks?`:\n* Topics: `religion\/existetialism`, `politics`, `education` \n* Count of available languages\n* Speakers occupations\n* Event the talk took place at\n\nQ2. `What are the main drivers which motivate the people to discuss about a particular TED Talk general impression the talk left in the audience?`\n* Talks topics: `religion\/existetialism`, `politics`, `education` \n* Talks duration\n* Talks age\n* Views count\n\n    ","e04ddbca":"We can see that the occupations 'Writer' and 'Psychologist' as speaker occupation  are taking the lead with the most TED talk views. ","65d34643":"We can see that after a certain point from around 30 languages the talks seem to be more viewed.","a1dd013c":"From the scatter plot between the number of speakers and the views count we canno't conclude much because we have really small count of observation for number of speakers greater of 1.","850cb6d9":"The bar chart shows the TED talk frequency in the top 10 event with most talks. We can see that the talk concentration is quite uniformly speard.","e9d4985a":"The scatter plot between the views and the talks age in days doesn't seem to show any tendency which is against the reasoning that the more the talk has been publish the more views it acummulates.","5703f3bc":"Bellow we are looking at the words with the overall tf-idf scores. Which should give us an idea what words are in general frequently important for discriminating the different talks on basis on their bag of words.","11fb7868":"From the summary of the numerical variables bellow the following information is interesting:\n\n* The TED talks vary a lot in terms of number of `comments` and their `duration` length\n* There is\/are some TED talks without any language. It is interesting to see whether that is really the case of it might be a mistake in the data :D\n* The makority of the talks are with just one speaker.\n* There is around 100 magnitude of difference between the talks with minimial and maximum `views` count","b489d968":"Here we look at the speaker count histogram with logarithmized frequency and not transformed frequency. As we can see the bulk of the TED talks have just one speaker and the talks with increasing number of speakers are fewer and fewer,","83e6fea8":"Apparently some months are also more preferable for publishing a ted talk than others.","b203b206":"The barchart bellow show that we do have several tags which are noticeably more frequent than the rest. ","8fe5f512":"As we can see from bellow they are quite some speakers with just some occurrence and a lot of views for their talks with Amy Cuddy way above. Ken Robinson is having a lot of TED talk appearences and in the same time manages to be also widely viewed.","8d6bf2dc":"Now we are again clustering, but this time the tags. Since they are appearing maximum just once each talks, we are using one hot encoding representation for the clustering. We use directly the `MiniBatchKMeans` without any preprocessings, because according to the Google's paper mentioned earlier it is supposed to performe well with sparse data. \n\nThe tendencies spotted are the following:\n\n* As indicated also earlier the `religious\/existential`  topics are really popular in terms of comments and views counts, whereas cluster 5 stands out with most appearing tags: `culture; charter_for_compassion; religion; global_issues; philanthropy; evolutionary_psychology; islam; suicide; buddhism; atheism;`\n* The clluster with the most talks inside contains mostly tags about creativity and personal growth: `culture; charter_for_compassion; religion; global_issues; philanthropy; evolutionary_psychology; islam; suicide; buddhism; atheism;`","2046ac8d":"As shown bellow we have 6 missing values for the variable `speaker_occupation`. We can leave it so since the variable is categorical and won't influence any arithmetical calculations. What is noticable that the occupations are rather arbitrary specified, so that we are having also listing of several occupations for some speakers. Thus special care of this columns should be taken in order to make it good for using them in further analysis or as features in modelling.","650414c2":"Here as well as by the views counts we have some months with visible more accumulated comments than others.","56e3f12c":"As for the views counts the comments counts seem to be highly right skewed with distribuion which visually more resambles poisson than than the views counts.","5d538768":"We do the same procedure for the descriptions of each talk and after retaining 16 cluster we can see that the ones with the most views and comments on average is 8 with most important words `power; talk; show; war; understand; word; languag; polit; countri; stori;`","2425c747":"Latent Semantic Analysis. The core of LSA is constructing of frequency or tf-idf table across documents(in our case talks transcripts) and terms(in our case terns in the talks transcripts) and then using Singular Value Decompositions for dimensionality reduction so that we retain truncated number of features which are containing the squeezed information condensed from the terms or the documents. In our case we want see if we can cluster the talks in some topics summarized from the most important words. Thus we want to reduce the dimensionality of the ratings taken into account for all of the talks. We use the  *tf-idf*, because we have notion of too rare or too frequent words inside of each transcript.\n\nWe retain the first 40 singular values from the SVD containing the most of the information. In our case they contain 16% of the variations in the frequency table. This is pretty decent amount of the whole information, since we are keeping just 40 components which is less than 1% than of the whole bag of words count from our transcripts corpus. Thus we truncate our original frequency matrix, so that we reduce its dimensionality to this 40 components. Afterwards we use k-means clustering to cluster the talks by using our 40 extracted features. Before clustering we normalize the features values within each talk sample, because the k-means clustering is performing better with normalized data. Looking at our critera from before  for choosing `k` we go for `k = 20`\n\nThen looking at the barcharts of the clusters we see that the imbalance of talks count is not that drastic across clusters.  Also clusters 2, 9, 18 seem the most popular ones with the most views and comments counts which are respectively with topics about `brain science`, `education`, and `social\/ political` :) This is quite different picture of what  discovered before where the words about `religion` and `existetialism`.","3865415d":"# Topics analysis","aeaba233":"The barchart bellow shows us which are the most frequently left ratings among all of the talks. We see that  'Inspiring', 'Informative' and 'Fascinating' a by far most popular ratings.","e484351e":"As for the words in the talks descriptions we build below similar correlation plot this time between the ratings counts and comment counts. It shows us that hte rating which is mostly apparent in talks with small comment counts is 'OK'. On the opposite side with most frequent appearence in talks with high comment count is the rating 'Persuasive' with some correlation around 0.2.\n\nHere for the ratings we are also conducting clustering in order to summarize the raitings in some topics which are gatherin the most similar ratings in terms of occurance in that particular topic. For that purpose we are just going to use directly the counts of the raitings for certain tall as features. We devide each rating types counts by the standard deciation within that rating type first and then normalize all the rating types within a talk, so that we are agnostic to the overall popularity of use of the particular rating types and emphasize on the relative to the other ratings frequency within a talk. In order to determine the number of clusters `k` we are fitting clustering with increasing `k` and we are tracking according to the elbow principle which is the last point where we have significant reduction of our metric criteria. Here we use two metric criteria. One is the classic k-means criterion `Sum of squared distances` which is calculated from the sum of the squared distances of the datapoints to the closest centroids, which is also used as a stopping convergence criterion of the standard k-means algorithm in sk-learn. In our case we are using the `MinibatchKMeans` with `tol` parameter set to `0.01` which uses different early stopping criterion explained in the following paper under the notation `epsilon`:  [https:\/\/www.eecs.tufts.edu\/~dsculley\/papers\/fastkmeans.pdf](https:\/\/www.eecs.tufts.edu\/~dsculley\/papers\/fastkmeans.pdf). It is noticable that around k equals 6 we have already solid reduction of the  `Sum of squared distances` We also look at the ration between the Sum of squares within clusters(WSS) and Sum of squares between clusters(BSS) of both comments count and views count. WSS\/BSS is very popular criterion in the Discriminant Functions Analysis and we are borrowing from there as a good indicator how well the clusters minimizing the variance of the comments and the views. As we can see the variance for the comments is reduced pretty fast from initial big values for WSS\/BSS to some really small at k=5. This rapid decrease of WSS\/BSS for the comments in comparison to views can be explained with the fact that as it is the case with the comments also the ratings are left after watching the talk and they are more consistent within each other. \n\nIn the end we go for k=6 six and look at the most dominant ratings in each cluster. What is interesting is that the clusters are realy semantically different, which is mainly to the fact that we standardized the ratings counts within each ratings across all the documents so that we avoided overrepresenting in the clusters the ratings which in general are really frequently used such as 'Inspiring', 'Informative' and 'Fascinating'. We can look up the total frequencies of the ratings in the bar chart bellow.\n\nFrom the barcharts in the end we can see that the clusters are rather imbalanced in terms of talks count within each cluster. Also it is noticeable that some clusters have higher averages of comments and views counts than other clusters. In order to assess the if this difference in averages is significant we can use as  earlier some non-parametric hypothesis testing, but here I don't go further into analysis. ","be9e5d69":"As expected there is high correlation between the views counts and the comments counts, whereas the scatter plot is showing that we have some different slopes around which we have linear tendency between the views counts and the comments counts. This might be a hint that we have some different latebt factors which determine the different slopes we are seeing.","cf654903":"The *TED Talks* concept of sharing ideas in the form of presentations done by people who can help the public by talking about their experience and ideas is amazing approach for raising the global consciousness and awareness for key problems humanity deals with. We can get a sense of the big evoked resonance and interest of the *TED Talks* by looking at the following crunched numbers from the data:\n\n* Total online views count for all of the 2550 TED videos in the dataset is:  **4 330 658 578**\n* Total online comments count for all of the TED videos in the dataset is:  **488 484**\n\nThis huge popularity raises some interesting questions. For instance we will search for an answer of the following three questions in particular. With the question we state also the relevant dataset information which might be a factor in answering the questions.:\n\nQ1. `What are the main drivers which attract the interest of the people to the TED Talks?` In order to answer this question we logically might focus on the following data features, which are telling information about how appealing the TED Talks seemed to the people in order to motivate them to watch the talk at first place: \n\n* `views`: The number of views on the talk can be taken as a main inidcator how interesting the talk appeared to be for the public before starting watching the talk.\n* `description`: A blurb of what the talk is about. As a main source of information about the talk before hand. This seems to be important factor for taking a decision to watch the talk.\n* `duration`: The duration of the talk in seconds. Before whatching a talk the duration seems to be also important decision factor whether to invest your time in watching the talk.\n* `event` The TED\/TEDx event where the talk took place. The event might be also of importance, since some events might be advertised much more in comparison to other events.\n* `languages`: The number of languages in which the talk is available. Higher count of offered languages logically also big factor for more people watch the talk. \n* `main_speaker`: The first named speaker of the talk. If the 'headliner' of the talk is some influential and popular person then more people might be more predispositioned to watch the talk.\n* `num_speaker`: The number of speakers in the talk.  The more speakers the more attention might be attracted.\n* `title`: The title of the talk might also attract attention to the talk and potivate the people to watch it.\n* `published_date`: The Unix timestamp for the publication of the talk on TED.com. The time drift might be also of importance since the TED talks in general might become more and more popular with the time which might make the most recent talks beeing viewed more times. On the other hand the oldest TED talks might accumulate more views than the latest ones, because they've been around for more time.\n* `speaker_occupation`: The occupation of the main speaker. Some people might be more inclined of finding out what a parsone with particular occupation hat to tell them about the particular topic.\n* `tags`: The themes associated with the talk. This feature also seems to be very importand for attracting the inrerst op the public.\n* `related_talks`: A list of dictionaries of recommended talks to watch next. It would be interesting to see if the related talks share also similar popularity.\n\nQ2. `What are the main drivers which motivate the people to discuss about a particular TED Talk general impression the talk left in the audience?` In order to answer this question we logically might focus on the following data features, which actually convey information about the drivers after watching the talk a person to participate in the discussion by leaving a comment: \n\n* `comments`: The number of first level comments made on the talk. This might be preceived as main inidicator of the amount of public resonance and social impact the talk evoked.\n* `duration`: The duration of the talk in seconds. If the duration for instance is too long, this might exceed the attention span of the watcher and discourage her or him to become active on the talk's problematic.\n* `description`: A blurb of what the talk is about. As a main source of information about the talk before hand. Some people after watching the the talk might have been driven to leave a comment for instance just because they want to elaborate not just on the content of the talk but also on the content of the description..\n* `published_date`: The Unix timestamp for the publication of the talk on TED.com. The time drift might be also of importance since the TED talks in general might become more and more popular with the time which might make the most recent talks beeing comments more times. On the other hand the oldest TED talks might accumulate more comments than the latest ones, because they've been around for more time.\n* `views`: The number of views on the talk might be taken as a direct factor which is influencing positively the comments count.\n* `num_speaker`: The number of speakers in the talk. The more speakers might leadt to bigger impact on the audience.\n* `speaker_occupation`: The occupation of the main speaker. Some occupations are more respected than others and in combination with the content of the talk it might leave drive the people to express certain opinion in a form of a comment.\n* `main_speaker`: The first named speaker of the talk. Same as for `speaker_occupation`.\n* `trascripts` form the file `transcripts.csv`: it provides the content of the talk and it is the essentioal information which has impact on the watchers.\n* `ratings`: A stringified dictionary of the various ratings given to the talk (inspiring, fascinating, jaw dropping, etc.). The ratings might be taken as a summarized version of the emotions with which the people left after watching the talk. It might also give us a hint about the sentiment of the first level comments, since we can't rely on their content.\n\nOther unused features features:\n\n* `url`: The URL of the talk. It is used as an id for linking the `transcripts.csv` and `ted_main.csv` tables.\n* `film_date`: The Unix timestamp of the filming. Not that interesting.\n* `name`: The official name of the TED Talk. Includes the title and the speaker. The same information as in `title` and `main_speaker`\n* `related_talks`: A list of dictionaries of recommended talks to watch next. It would be interesting to see if the related talks share also similar popularity.","35e8448d":"Below we are looking how the duration is correlated with the views. It seems extremely lengthy talks have also not that viewes. On the other hand the highly viewed TED talks are around the sweet spot of about 500-1000 seconds. On the other hand all the extreme durations seem to be of talks with not that many views which is reasonable that the people get discouraged to watch a talk which is too long :D","d0beed92":"It seems that as for the duration also the talk age is more correlated with the comments than with the views counts.","0be626b5":"Below we can see the list with speakers with the overall count of views for all of their talks. We can see that the first 7 have acummulated considerably big portion of views :)","0756eb07":"# Q1. What are the main drivers which attract the interest of the people to the TED Talks?","6a8621cc":"Bellow we will analyse the transcript of the ted talks in relation to the comments count of each talks. As done before we use the *tf-idf* coefficients and construct correlation plot to see how the importance of the words used in the transcript is correlated with the comment counts. From the plot it is really obvious that the religion and existential topics in the talks are finding a lot of resonance in the people and the people tend to leave more comments in talks with emphasized words from the transcript such as 'darwing', 'god', 'religion', 'tooth', 'fairi', 'religi'. The words 'tooth' and 'fairi' doe are not that informative since we are lacking the context in which they are used :D","10c5f08d":"# Q2. What are the main drivers which motivate the people to discuss about a particular TED Talk general impression the talk left in the audience?\n","ebb25c81":"As shown below we have higher correlation between the comments counts and duration than between the views counts and the duration we've seen before. It is also noticeable that for some range of comment counts we have extreme duration and that for some range of durations we observe all the extreme comment counts.","8bc4b6de":"When looking at the distribution of the `views` it strikes that it is right skewed with really long tail and thus a lot of outliers after the top whisker of the box-plot. The bulk of our mass we have within the whiskers. The rest are considered as outliers where in our case the outliers continue up to 90 000 000 but we pruned the domain up to 6 000 000 for clarity. From the histogram and the boxplot we can see that the distribution of the views is extremely right skewed and resambles Poisson distribution with some long tail with too extreme values. When we think of it in real world these extreme values are not a surprise since a lot of factors can influence in making a TED talk really popular and acummulating extreme number of veiws. We will investigate further some of these possible factors.","fc98dede":"We look here at the most related occupations to each tag cluster and it is noticable that they are to some scale coherent with the topics derived from the tags. Also the majority of the most cluster relevant speakers occupations seem to be really interesting and rare :)","9a29c87c":"As for the views count we display the most correlated words from the talks descriptions with the comment counts. We see that the highes positively correlated words with the comments counts are 'funni' and 'case'. The word 'design' which is very influential in the descriptions of the talks, is having high negative correlation with the comments count which means that 'design' is important in descriptions coming mainly from talks with small number of comments. In comparison to the views count here two new weord pop up as correclated, namely 'scientist' and 'educ'(educaton)","7bca328c":"Below are displayed the total count of views for the first 60 events. Here it is noticable that the most popular events are the annual TED events and the TEDGlobal events."}}