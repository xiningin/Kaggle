{"cell_type":{"af65e85a":"code","34bb19a9":"code","48d90f7c":"code","7516bac0":"code","b64fdd0f":"code","40271e20":"code","e55061d3":"code","ca8101aa":"code","b8a3d6db":"code","915d591c":"code","c1d0b814":"code","fd9212ac":"markdown","4593eaa1":"markdown","3b41cb60":"markdown","471aea54":"markdown","90c20b44":"markdown","1590eac2":"markdown","39a4d9f1":"markdown","62a1a903":"markdown","676df4ed":"markdown","d75a611a":"markdown","0dd642e3":"markdown"},"source":{"af65e85a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34bb19a9":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import SGDClassifier \nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","48d90f7c":"data=pd.read_csv('..\/input\/titanic\/train.csv')","7516bac0":"data['Embarked']=data['Embarked'].fillna(0)\ndata['Parch']=data['Parch'].fillna(0)\ndata['SibSp']=data['SibSp'].fillna(0)\n\ndata['Age']=data['Age'].fillna(data['Age'].mean())\ndata['Sex']=data['Sex'].fillna(0)\ndata['Pclass']=data['Pclass'].fillna(0)\ndata['Embarked']=data['Embarked'].fillna('0')\n\n\nlblenc=LabelEncoder()\nlblenc.fit(data['Sex'])\ndata['Sex']=lblenc.transform(data['Sex'])\n\n\ndata['Embarked']=data['Embarked'].replace('S',1)\ndata['Embarked']=data['Embarked'].replace('C',2)\ndata['Embarked']=data['Embarked'].replace('Q',3)\n","b64fdd0f":"\ndata=data[['Pclass','Sex','Age','SibSp','Parch','Survived','Embarked']]\nX=data[['Pclass','Sex','Age','SibSp','Parch','Embarked']]\ny=data['Survived']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44, shuffle =True)","40271e20":"\nDTModel_=DecisionTreeClassifier(criterion = 'entropy',max_depth=3,random_state = 33)\nLDAModel_=LinearDiscriminantAnalysis(n_components=1 ,solver='svd')\nSGDModel_=SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5)\nGBCModule_=GradientBoostingClassifier(n_estimators=1000,max_depth=5,random_state=33)#\nMLPModule_=MLPClassifier()\nRFCModule_=RandomForestClassifier(n_estimators=100,max_depth=2, random_state=33)\nSVCModel_=SVC(kernel= 'rbf', max_iter=1000,C=10,gamma='auto')\nLGnModel_=LogisticRegression(penalty='l1',solver='liblinear',C=1.0,random_state=50)","e55061d3":"\nVotingClassifierModel = VotingClassifier(estimators=[('DTModel',DTModel_),('LDAModel',LDAModel_),('SGDModel',SGDModel_),                                                     \n('GBCModule',GBCModule_),('MLPModule',MLPModule_),('RFCModule',RFCModule_),('SVCModel',SVCModel_),('LGnModel',LGnModel_)],\nvoting='hard')\nVotingClassifierModel.fit(X_test, y_test)","ca8101aa":"\nprint('VotingClassifierModel Train Score is : ' , VotingClassifierModel.score(X_train, y_train))\nprint('VotingClassifierModel Test Score is : ' , VotingClassifierModel.score(X_test, y_test))","b8a3d6db":"y_pred = VotingClassifierModel.predict(X_test)\nprint('Predicted Value for VotingClassifierModel is : ' , y_pred)\n","915d591c":"CM = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix is : \\n', CM)","c1d0b814":"sns.heatmap(CM, center = True)\nplt.show()","fd9212ac":"**Apply some feature engineering**","4593eaa1":"**#Calculating Details**","3b41cb60":"**# drawing confusion matrix**","471aea54":"**#Calculating Prediction**","90c20b44":"**#loading models for Voting Classifier**","1590eac2":"**#Calculating Confusion Matrix**","39a4d9f1":"**#Splitting data**","62a1a903":"**Read Data**","676df4ed":"**Voting Classifier using Sklearn**\n\n\n  *  A Voting Classifier is a machine learning model that trains on an ensemble of numerous models\n    and predicts an output (class) based on their highest probability of chosen class as the output.*\n It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n \n**Voting Classifier supports two types of votings\n1.\tHard Voting: In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n2.\tSoft Voting: In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.\n\n\n**Note: Make sure to include a variety of models to feed a Voting Classifier to be sure that the error made by one might be resolved by the other.**\n","d75a611a":"**#loading Voting Classifier**","0dd642e3":"**# Import Libraries**"}}