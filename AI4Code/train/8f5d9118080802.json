{"cell_type":{"282b6207":"code","b5af1fb3":"code","638c7129":"code","3af8ef88":"code","5fd8d565":"code","4ac35cb4":"code","9df203b5":"code","9f236319":"code","4c196279":"code","ff9d198d":"code","0cbc0a52":"code","08ab041c":"code","d8843d81":"code","2c8c194c":"code","64c8b79e":"code","73d10d24":"code","82c8c920":"code","d546d346":"code","0ae89664":"code","59b503e3":"code","3bdb72bd":"code","2e575a7e":"code","3f0f0953":"code","28e41236":"code","86a3f246":"code","832a7532":"code","c04d31b5":"code","c1b1673d":"code","bfe76859":"code","5ae77cb4":"code","aa7e326c":"code","f5140a1b":"code","871521a3":"code","a38914e8":"markdown","6f546473":"markdown","9ce24021":"markdown","cd02cbf7":"markdown","c0078fa2":"markdown","52b86be3":"markdown","19affd13":"markdown","913b1d39":"markdown","b71e8da5":"markdown","cc2b1ee3":"markdown","109c4001":"markdown","f291d567":"markdown","1c643c8c":"markdown","c873b6ea":"markdown","66172f3d":"markdown"},"source":{"282b6207":"# Import the standard toolkit...\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ...and a few NLP specific things\nimport spacy\nfrom spacy import displacy\nfrom wordcloud import WordCloud\n\n# ...and switch on \"in notebook\" charts, and make them a bit bigger!\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# ...then print a silly message to make it clear we're done\nprint(\"Reticulating splines... DONE\")","b5af1fb3":"def show_hipster_biz_name(a, b):\n    from codecs import decode\n    from zlib import adler32\n    from IPython.display import HTML\n    ns = 'mrcule\/zvak\/pbj\/pbea\/cvtrba\/funpxyr\/obngzna\/pbyyne'\n    qs = 'zbhfgnpur\/nagvdhr\/pebpurgrq\/negvfnany'\n    bs = 'pbzof\/glcrjevgref\/fyvccref\/ancxvaf\/jubyrsbbqf'\n    def tr(n, c):\n        c = decode(c, \"rot13\").split('\/')\n        return c[adler32(bytes(n, 'utf8')) % len(c)].title()\n    n = \"{} & {} {} {}\".format(tr(a, ns), tr(b, ns), tr(a+b, qs), tr(b+a, bs))\n    s = \"font-family:serif;font-size:28pt;text-align:center;border:4px double black;padding:10px;\"\n    print(\"Your Hipster business name is:\")\n    display(HTML(\"<h1 style='{}'>{}<\/h1>\".format(s, n)))\n\n########################################################################################\n# Edit the line below to include your name and your neighbour's name, then run the cell\n########################################################################################\n\nshow_hipster_biz_name(\"Your Name\", \"Your Neighbour's Name\")","638c7129":"# Experiment here!","3af8ef88":"# Load up the english language models... this takes a while!\n\nnlp = spacy.load(\"en_core_web_lg\")\nprint(\"{name}: {description}\".format(**nlp.meta))","5fd8d565":"# Okay let's use SpaCy to process a simple sentence\n# The fundamental operation is to create a stuctured \"Doc\" representation of a text. Let's take a look!\n\ntext = u\"Pack my bag with twelve dozen liquor jugs.\"\ndoc = nlp(text)\ndoc.print_tree()","4ac35cb4":"# But since we're in Jupyter we can do a lot better than that!\n# The \"Parts of Speech\" e.g. VERB are drawn from the \"Universal POS Tag\" vocabulary\n# Find out more at http:\/\/universaldependencies.org\/u\/pos\/\n\noptions={'jupyter': True, 'options':{'distance': 120}}\ndisplacy.render(doc, style='dep', **options)","9df203b5":"# Spacy also ships with an \"entity recogniser\" -- it's pretty good!\n\nghostbusters = nlp(u\"In the eponymous 1984 film, New York City celebrated the Ghostbusters with a ticker tape parade.\")\ndisplacy.render(ghostbusters, style=\"ent\", **options)","9f236319":"# Use this cell to explore!\n\ndisplacy.render(nlp(u\"Explore Spacy here!\"), style='dep', **options)","4c196279":"# The larger SpaCy models contain a list of words and their corresponding vectors\n\nprint(\"Document vectors have {} dimensions\".format(len(doc.vector)))\nprint(\"And are not normalized e.g. this has length {}\".format(np.linalg.norm(doc.vector)))","ff9d198d":"# Document vectors capture an intuitive notion of similarity\n# Words that appear similar contexts are considered similar\n\ndef print_comparison(a, b):\n    # Create the doc objects\n    a = nlp(a)\n    b = nlp(b)\n    # Euclidean \"L2\" distance\n    distance = np.linalg.norm(a.vector - b.vector)\n    # Cosine similarity\n    similarity = a.similarity(b)\n    print(\"-\" * 80)\n    print(\"A: {}\\nB: {}\\nDistance: {}\\nSimilarity: {}\".format(a, b, distance, similarity))\n\ntext = \"The cat sat on the mat\"\nprint_comparison(text, \"The feline lay on the carpet\")\nprint_comparison(text, \"Three hundred Theban warriors died that day\")\nprint_comparison(text, \"Ceci n'est pas une pipe\")\n","0cbc0a52":"# Use this cell to explore!\n","08ab041c":"# Document vectors often also have a very interesting property sometimes called \"linear substructure\"\n# Basically you can do arithmetic with words\/concepts!\n\ndef vectorize(text):\n    \"\"\"Get the SpaCy vector corresponding to a text\"\"\"\n    return nlp(text, disable=['parser', 'tagger', 'ner']).vector\n\nfrom heapq import heappush, nsmallest, nlargest\n\ndef get_top_n(target_v, n=5):\n    \"\"\"Figure out the top-N words most similar to the target vector\"\"\"\n    heap = []\n    # SpaCy has a long list of words in `vocab` which we can pick from!\n    for word in nlp.vocab:\n        # Filter out mixed case and uncommon terms\n        if not word.is_lower or word.prob < -15:\n            continue\n        distance = np.linalg.norm(target_v - word.vector)\n        heappush(heap, (distance, word.text))\n    return nsmallest(n, heap)\n\n\nPUPPY, DOG, KITTEN = [vectorize(w) for w in (\"puppy\", \"dog\", \"kitten\")]\n\nget_top_n(DOG - PUPPY + KITTEN)\n","d8843d81":"# We can generalize that into a cute analogy finder\n\ndef print_analogy(a, b, c):\n    \"\"\"A is to B as C is to ???\"\"\"\n    top_n = get_top_n(vectorize(b) - vectorize(a) + vectorize(c))\n    best = [w for (s,w) in top_n if w not in (a,b,c)][0]\n    print(\"{} is to {} as {} is to {}\".format(a, b, c, best))\n    \nprint_analogy(\"queen\", \"king\", \"woman\")\n","2c8c194c":"# Use this cell to explore!","64c8b79e":"from sklearn.datasets import fetch_20newsgroups\nraw_posts = fetch_20newsgroups()\n\nprint(\"Number of posts: {}\".format(len(raw_posts.data)))\n # Source groups are listed in `target_names`\nprint(\"Newsgroups: {}\".format(raw_posts.target_names))\n # Post text is in `data`\nprint(\"Sample post text:\\n{0}\\n{1}\\n{0}\".format('-' * 80, raw_posts.data[19]))\n # Post group is encoded in `target` as an index into `target_names`\nprint(\"Sample post group: {}\".format(raw_posts.target_names[raw_posts.target[19]]))","73d10d24":"# There's quite a lot of junk in there, headers etc. Fortunately sklearn can help a bit...\n# We can pass through a special argument to strip out headers, footers and inline quotes\n\nraw_posts = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\nprint(\"Sample post text:\\n{0}\\n{1}\\n{0}\".format('-' * 80, raw_posts.data[19]))","82c8c920":"# Another key tool in the Python ecosystem is Pandas which is a library for working with tables of data\n# We're going to convert the dataset in to a Panda DataFrame for ease of manipulation\n# Don't worry too much about this -- it's not really our focus today -- but if you're interested you can\n# find out more at http:\/\/pandas.pydata.org\/\n\nposts = pd.DataFrame({'text': raw_posts.data, 'group': [raw_posts.target_names[t] for t in raw_posts.target]})\n\n# Many tools in the Python ecosystem are quite tightly integrated, so once we have DataFrame we can\n# do things like plot it via the standard charting tool `matplotlib` which we importes as `plt` earlier\nposts['group'].value_counts().plot(kind='bar', title=\"Per group document counts\")\nplt.show()","d546d346":"# One way to get a handle on a collection of documents (or \"corpus\") is to look at a wordcloud\n# Thankfully someone has written a little library to help us do that\n\nwc = WordCloud(background_color='white', width=1000, height=400, stopwords=[])\nwc.generate(\" \".join(t for t in posts[posts.group == 'rec.autos'].text)).to_image()","0ae89664":"# Oh dear that wasn't much use... of course common words completely dominate!\n# These are called \"stopwords\". It's common (if a little controversial these days...) to filter them out\n# The wordcloud library we're using supports that\n\nfrom wordcloud import STOPWORDS\nbetter_stopwords = STOPWORDS.union({'may', 'one', 'will', 'also'})\nwc = WordCloud(background_color='white', width=1000, height=400, stopwords=better_stopwords)\nwc.generate(\" \".join(t for t in posts[posts.group == 'rec.autos'].text)).to_image()","59b503e3":"# Okay, that's more like it! Let's eyeball all the groups\n\nfor group in raw_posts.target_names:\n    print(\"Wordcloud for {}\".format(group))\n    display(wc.generate(\" \".join(t for t in posts[posts.group == group].text)).to_image())","3bdb72bd":"# Looks okay, but comp.os.ms-windows.misc appears to be full of garbage\n# Let's cull it (rather crudely...) and take another look\n\nposts = posts[~posts.text.str.contains(\"AX\")]\nfor group in raw_posts.target_names:\n    print(\"Wordcloud for {}\".format(group))\n    display(wc.generate(\" \".join(t for t in posts[posts.group == group].text)).to_image())","2e575a7e":"# Use this cell to explore!","3f0f0953":"# First let's get the documents into a suitable form\n# Build a matrix by \"stacking\" the row vectors from SpaCy\n# Takes about 20 seconds...\n\nfrom sklearn.preprocessing import normalize\n\ndef vectorize(text):\n    # Get the SpaCy vector -- turning off other processing to speed things up\n    return nlp(text, disable=['parser', 'tagger', 'ner']).vector\n\n# Now we stack the vectors and normalize them\n# Inputs are typically called \"X\"\nX = normalize(np.stack(vectorize(t) for t in posts.text))\nprint(\"X (the document matrix) has shape: {}\".format(X.shape))\nprint(\"That means it has {} rows and {} columns\".format(X.shape[0], X.shape[1]))","28e41236":"# Scikit Learn ships with a neat PCA implementation\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX2 = pca.fit_transform(X)\nprint(\"X2 shape is {}\".format(X2.shape))","86a3f246":"# Okay let's take a look at it via matplotlib\n\ndef plot_groups(X, y, groups):\n    for group in groups:\n        plt.scatter(X[y == group, 0], X[y == group, 1], label=group, alpha=0.4)\n    plt.legend()\n    plt.show()\n    \nplot_groups(X2, posts.group, ('comp.os.ms-windows.misc', 'alt.atheism'))","832a7532":"CLUSTERS = 20\n\n# First we fit the model...\nfrom sklearn.cluster import KMeans\nk_means = KMeans(n_clusters=CLUSTERS, random_state=1)\nk_means.fit(X)\n","c04d31b5":"# Then we use it to predict clusters for each document...\n# Again it's common to use yhat for a predicted value -- although we wouldn't expect these to\n# correspond directly to the original groups\nyhat = k_means.predict(X)\n\n# Let's take a look at the distribution across classes\nplt.hist(yhat, bins=range(CLUSTERS))\nplt.show()","c1b1673d":"# To be honest that's not looking very healthy -- ideally we'd see a more even distribution\n# Let's take a look at a couple of the big ones\n\nplot_groups(X2, yhat, (1,14))","bfe76859":"# Okay there are some definite (if rather blurry...) clusters there!\n# Let's have a look at how our clusters relate to the original groups\ndef plot_cluster(c):\n    posts[yhat == c]['group'].value_counts().plot(kind='bar', title=\"Cluster #{}\".format(c))\n    plt.show()\n\n# Some are great matches...\nplot_cluster(0)","5ae77cb4":"# Some are not so great a match, but sensible (why...?)\nplot_cluster(14)","aa7e326c":"# Some are just a bit random!\nplot_cluster(9)","f5140a1b":"# Let's have a look at the wordclouds...\nfor c in range(CLUSTERS):\n    print(\"Wordcloud for category #{}\".format(c))\n    display(wc.generate(\" \".join(posts.text[yhat == c])).to_image())","871521a3":"# Use this cell to explore!","a38914e8":"# Cleaning and visualizing documents\n\nOkay, so that's a quick introduction to SpaCy. Let's look at some documents.\n\nScikit Learn (sklearn) is a brilliant python library for machine learning. You can find out more about it at http:\/\/scikit-learn.org\n\nIt also ships with some handy features for downloading and reading in some standard pedagogical datasets. Let's have a look at the newsgroups collection of documents (or \"corpus\") that we used Scikit Learn to download earlier.","6f546473":"## Over to you: more windows on the data\n\n- What does the wordcloud for the whole corpus look like?\n- How many posts contain the word \"window\"? (Hint: look at how we removed posts containing \"AX\")\n- How do they split out across the groups? (Hint: look at how we drew the barchart for groups earlier)\n- (Tricky!) Can you filter out more junk posts?\n","9ce24021":"## Over to you: comparison shopping\n\n- How does \"cat\" compare with \"feline\"?\n- How does \"good\" compare with \"goods\"? How does it compare with \"bad\"? What's going on?\n- How does \"teh\" compare with \"the\"?","cd02cbf7":"# Topic extraction\n\nWe're going to see if we can automatically infer a set of topics from the corpus. Obviously we'd expect these to be somehow related to the original newgroups, but perhaps there'll be some surprises?","c0078fa2":"Again, there's some confusion, but there are some really strong clusters there! Go us! But how could we do better?\n\n## Over to you: cluster buster\n\n\n\n- Whats going on with cluster #1?\n- Was ist der story with cluster #4?\n- Scroll way up and find where we're defining `CLUSTERS`. Try a smaller value (say 5). What effect does it have?\n- Try a larger (perhaps much larger...) value. What effect does it have?\n- (Tricky!) How could you decide automatically how many clusters to use? (Hint: take a look at the `bench_k_means()` method in http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_digits.html)","52b86be3":"## Visualizing the vectors: 300D Glasses\n\nOur document vectors have 300 dimensions. That's quite difficult to visualize on a 2 dimensional screen!\n\nWe're going to a use a standard technique called \"Principal Components Analysis\" (or PCA) to automatically reduce that to 2 dimensions, so we can get some insight into what's going on.\n\nYou can read more about PCA at https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis","19affd13":"## Over to you: getting comfortable with the Kaggle Kernel\n\n- Click in any cell then click the blue `+` in the toolbar to create a new cell above or below\n- Enter some Python code (if you don't know any try `print(1 + 2)` or copy your neighbour!)\n- Click the `Run` button (or Ctrl-Enter) to execute it\n- Write some code to print the result of multiplying `1337` with `1337` (Hint: use `print(...)`)","913b1d39":"# The workshop is dead long live the workshop!\n\n## Feedback\n\nI'd really love to get your feedback on this workshop (be it good, bad, pull request or bug report)! You can ping me at [joe.halliwell@gmail.com](mailto:joe.halliwell@gmail.com) or even tweet `@joehalliwell` if you're so inclined.\n\n## Other things to try\n\nWe've really just scratched the surface of document clustering here. If you want to dig into the topic further (and dig out further topics), you might like to start by:\n\n- Using TFIDF vectors instead of SpaCy vectors: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html\n- Using Latent Dirichlet Allocation (LDA) instead of k-means: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.LatentDirichletAllocation.html","b71e8da5":"# Document Clustering\n\n## Why we're here\n\nGoals:\n\n- Pique your curiosity about Natural Language Processing (NLP)\n- Introduce core tools in the Python NLP\/Data Science ecosystem\n- Cluster some documents\n- Help your neighbours!\n- Have fun!\n\nNon-goals:\n\n- Finish the whole thing (but you can certainly try...)\n\n## Getting started\n\nClick on the below \"cell\" and hit \"Ctrl-Return\" to run it","cc2b1ee3":"# Document vectors\n\nFor Machine Learning applications it's *often* the case that we want to process a document into a list of numbers or \"vector\".\n\nIt's worth noting that there are many different ways to do this. Also recent advances in \"deep learning\" as well as providing new ways to generate document vectors also offer ways to work more directly with the source text.\n\nBut for now...","109c4001":"# Introducing SpaCy\n\nOkay, let's do some Natural Language Processing (NLP)!\n\nSpaCy is a relative newcomer to the NLP scene, but has made a big splash because:\n- it's fast\n- it ships with high-performance models for a few different languages\n- it's got a very nice API and excellent documentation\n\nThey also have great marketing: https:\/\/spacy.io\/","f291d567":"## Clustering the documents\n\nIt looks like our vectors are doing something vaguely useful, in that there's a visual separate between groups.\n\nNow we'll use the standard \"k-means\" algorithm to automatically identify clusters within the data.\n\nYou can read more about k-means at https:\/\/en.wikipedia.org\/wiki\/K-means_clustering\n","1c643c8c":"## Over to you: SpaCy exploration!\n\n- How does SpaCy handle \"The cat ate the fish\"? Is it correct?\n- How about \"The old man the boat\"?\n- And \"The complex houses married and single soldiers and their families\"? What's going on?\n- Who are the people in \"Saint John met Gina St. John in St John's Wood\"?","c873b6ea":"## Over to you: hipster business name generator\n\nGenerally, you're just going go through this notebook executing the code blocks as you did above.\n\nBut you'll also find a few \"Over to you\" sections scattered throughout. These are a prompt for you to experiment and try things out.\n\nTo get you started just follow the instructions after the big comment `#####` below...","66172f3d":"## Over to you: an analogy is an idea with another idea's hat on\n\n- \"Boy\" is to \"girl\" as \"prince\" is to what?\n- \"Red\" is to \"reddest\" as \"blue\" is to what?\n- What is to \"simile\" as \"analogy\" is to \"metaphor\"? (Sorry for the brain strain, but it's good to stretch a bit no?)\n- Find an example that doesn't work! (It's not too hard :))\n- (Advanced) Try changing the \"-15\" in `get_top_n` to \"-100\". What effect is it having on your examples?\n- (Advanced) Could you adapt this code to find \"opposites\" for words?\n"}}