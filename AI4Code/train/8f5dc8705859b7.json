{"cell_type":{"eb28d6eb":"code","cde2d0dd":"code","05ac9d01":"code","3f38d884":"code","e00ace9f":"code","102a18aa":"code","c7792342":"code","a913ceab":"code","e810df50":"code","395cfaf2":"code","228546d8":"code","3fa66940":"code","4dafc785":"code","7aee4f9b":"code","9d69b511":"code","b6328a56":"code","a1a8980e":"code","575f6ede":"code","84d5985f":"code","8904049d":"code","7f1333fb":"code","b05358f7":"code","80912554":"code","122a463e":"code","c119690d":"code","00ad9952":"code","a0feffb9":"code","3180bb8f":"code","dabe64e9":"code","c7bab400":"code","1a2b4876":"code","f7c85bca":"code","f6a3b951":"code","53dd5836":"code","886c1b99":"code","214f5190":"code","e0107251":"code","595295a9":"code","21736303":"code","a42660e7":"code","aa367190":"code","33531604":"code","d12cf952":"code","801649e5":"code","a8853085":"code","0b0ee0a2":"code","551d16d8":"code","78b932e7":"code","8d0a1e87":"code","a171a677":"code","a3a4165a":"code","a3fc0ab1":"code","3109db82":"code","f43d9cfe":"code","4e978de2":"code","6b4b6fbc":"code","806f312d":"code","217758ef":"code","bccee8ff":"code","be06b37b":"code","23c1ade1":"code","1588710c":"code","28b0cf3c":"code","0ff4ea14":"code","ab056487":"code","80609ed9":"code","0229b87e":"code","6fd1f5ac":"code","1d717a71":"code","37d432f2":"code","5d82bbea":"code","734211b7":"code","47aa98d6":"code","f17a0a5a":"code","132075db":"code","07a19658":"code","b46f85b2":"code","ee66ea9f":"code","597ca10a":"code","9a0000f1":"code","46ff1e9f":"code","a78e4c8d":"code","11b0463b":"code","ffe8ec8b":"code","8c0b9d7c":"code","7f04f3ea":"code","8ca19801":"code","cd71db5e":"code","2b8692f8":"code","4f54344e":"code","43c7bb3c":"code","d3786a67":"code","0aabc884":"markdown","a474ccd5":"markdown","fa3e94ab":"markdown","a7002209":"markdown","7a2a607e":"markdown","10f6572d":"markdown","bf9f03a5":"markdown","3e6083f9":"markdown","fa1df44d":"markdown","e800491f":"markdown","1ac32c6d":"markdown","b807fe36":"markdown","af7d40c1":"markdown","126ffaf1":"markdown","c4b40555":"markdown","d274046b":"markdown","d6c8c814":"markdown","f790fb9e":"markdown","829362b8":"markdown","fe268459":"markdown","1074aed2":"markdown","01836afb":"markdown","fccd41f8":"markdown","068f8729":"markdown","7b17913a":"markdown","7c19960d":"markdown","4f3fb30d":"markdown","0197a843":"markdown","415bf014":"markdown","e1e797b6":"markdown","28f74ffc":"markdown","15e8340a":"markdown","c70b6a61":"markdown","57c84939":"markdown","cc183200":"markdown","0bb79fb6":"markdown","6251d364":"markdown","38fbdfc3":"markdown","0f37e3e8":"markdown","cd7de355":"markdown","3483410c":"markdown"},"source":{"eb28d6eb":"#importinng libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n","cde2d0dd":"#loading data\ndf=pd.read_csv(\"E:\\\\Board INFINITY\\\\Machine learning\\\\ML Project\\\\bank-marketing.csv\")       \n      \ndf.head()","05ac9d01":"#Drop column \"contact\" which is useless\ndf = df.drop('contact', axis=1)","3f38d884":"df.job.value_counts()","e00ace9f":"print(df.pdays.describe())\n","102a18aa":"print(df.pdays.value_counts().head(1))","c7792342":"df.shape","a913ceab":"df_up = df[df.pdays != -1]\n","e810df50":"df_up.pdays.describe()","395cfaf2":"# Delete the rows which column 'poutcome' contains 'other'\ncondition = df.poutcome == 'other'\ndf = df.drop(df[condition].index, axis = 0, inplace = False)\n\ndf[['job','education']] = df[['job','education']].replace(['unknown'],'other')","228546d8":"\n#  Drop customer values with 'other' education\ncondition3 = (df['education'] == 'other')\ndf = df.drop(df[condition3].index, axis = 0, inplace = False)","3fa66940":"bardf=df.groupby(['education'])['balance'].median()\nbardf","4dafc785":"print(bardf.idxmax(),' has highest median value with {}'.format(bardf.max()))\n","7aee4f9b":"from matplotlib import *\nimport seaborn as sns\nplt.figure(figsize=(10,4))\nbardf=pd.DataFrame(bardf)\nmycolors=('y','y','b')\nbardf.plot.barh(color=mycolors)\nplt.ylabel('Education')\nplt.xlabel('Balance')\nplt.title('Grouping Education based on balance')\n","9d69b511":"print(sns.boxplot(x=df['pdays']).set_title('Without filtering'))\n","b6328a56":"print(sns.boxplot(x=df_up['pdays']).set_title('With filtering pdays column'))\n","a1a8980e":"import warnings\nwarnings.filterwarnings('ignore')","575f6ede":"num_var=list(df.select_dtypes(exclude='object').columns)\ndf[num_var]","84d5985f":"cat_var=list(df.select_dtypes(exclude=[np.number]).columns)\ndf[cat_var]","8904049d":"sns.pairplot(df, hue='response', palette=\"husl\")\n","7f1333fb":"res=['yes','no']\nfor i in res:\n    sns.set_style(\"ticks\")\n    fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(25, 21))\n    fig.suptitle(\"Distribution of Categorical Features\\n response->[{}]\".format(i),\n             horizontalalignment='center', y=1.05,\n             verticalalignment='center', fontsize=40)\n    fig.subplots_adjust(top=0.99, bottom=0.01, hspace=1.5, wspace=0.4)\n    for ax, c in list(zip(axes.flat, cat_var)):        \n        sns.countplot(c, data=df[df['response']=='{}'.format(i)],order= df[df['response']=='{}'.format(i)][c].value_counts().index,\n                  ax=ax)\n        for p in ax.patches:\n            ax.annotate(\"{}\".format(p.get_height()), (p.get_x()+0.1, p.get_height()+50),\n                       ha='left', va='bottom', rotation=45)\n        ax.tick_params(labelrotation=90)  \n        plt.sca(ax)\n        plt.yticks(rotation=0)    \n         # ax.axis('off')\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['bottom'].set_visible(True)\n        ax.spines['left'].set_visible(False)","b05358f7":"df_grouped = df.groupby(\"response\")\nclass_name_no = \"no\"\nclass_name_yes = \"yes\"\ndf_grouped_no = df_grouped.get_group(class_name_no)\ndf_grouped_yes =df_grouped.get_group(class_name_yes)\n","80912554":" plt.figure()\nplt.hist(df_grouped_no['age'], bins=50, label=class_name_no)\nplt.hist(df_grouped_yes['age'], bins=50, label=class_name_yes)\nplt.legend()\nplt.title(\"Feature Histogram - Age\")\nplt.xlabel(\"Feature values\")\nplt.ylabel(\"Count\")\n\n","122a463e":"#HEAT MAP\n\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(df.corr(method='spearman'), annot=False, cmap='coolwarm')","c119690d":"def drawhist(df,feature):\n    plt.hist(df[feature])\ndrawhist(df,'pdays')","00ad9952":"cat_var","a0feffb9":"df.response.value_counts()","3180bb8f":"# Here we are converting this target variabe response into 0's and 1's so that it would easier for algorithm to form a model\ndef conver(x):\n    return x.map({'no':0,'yes':1})\ndf[['response']]=df[['response']].apply(conver)\n","dabe64e9":"df.response.value_counts()","c7bab400":"# these are the categorical columns\ncat_var","1a2b4876":"sns.countplot(x='response', data=df)\n","f7c85bca":"sns.countplot(y='job', data=df)\n","f6a3b951":"sns.countplot(x='marital', data=df)\n","53dd5836":"sns.countplot(y='education', data=df)\n","886c1b99":"dist=df.hist(figsize=(12,10)) # display numerical feature distribution\n","214f5190":"#distribution of duration of campaign.\n\ndist_dur_cam = df[['duration','campaign']].plot(kind = 'box',subplots = True, layout = (1,2),sharex = False, sharey = False,title='The Distribution of Duration and Campaign')\nplt.show()","e0107251":"corr_data = df[['age','balance','duration','campaign','month','previous','response']]\ncorr = corr_data.corr()\n\ncor_plot = sns.heatmap(corr,annot=True,cmap='RdYlGn')\nfig.set_size_inches(6,5)\nplt.xticks(fontsize=10,rotation=-30)\nplt.yticks(fontsize=10)\nplt.title('Correlation Matrix')\nplt.show()","595295a9":"high_corr_variables = ['poutcome']\nfor var in high_corr_variables:\n    df[var + '_co'] = (df[var] == 'unknown').astype(int)","21736303":"def cross_tab(df,f1,f2):\n    jobs=list(df[f1].unique())\n    edu=list(df[f2].unique())\n    dataframes=[]\n    for e in edu:\n        dfe=df[df[f2]==e]\n        dfejob=dfe.groupby(f1).count()[f2]\n        dataframes.append(dfejob)\n    a=pd.concat(dataframes,axis=1)\n    a.columns=edu\n    a=a.fillna(0)\n    return a\n\ncross_tab(df,'job','education')","a42660e7":"pd.crosstab(df['pdays'],df['poutcome'], values=df['age'], aggfunc='count', normalize=True)","aa367190":"df=df.drop('poutcome', axis=1)\n","33531604":"lst=['housing','default','loan','targeted']\n\n\n\ndef conver(x):\n    return x.map({'no':0,'yes':1})\n\nfor i in lst:\n    df[[i]]=df[[i]].apply(conver)\n\nlst=['job','education','marital','month']\nfor i in lst:\n    dummy=pd.get_dummies(df[i],drop_first=True)\n    df=pd.concat([df,dummy],axis=1)\n    df.drop([i],axis=1,inplace=True)\n","d12cf952":"df.head()","801649e5":"df.columns","a8853085":"df.response","0b0ee0a2":"X=df.drop('response', axis=1)\nY=df['response']\n","551d16d8":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score","78b932e7":"# 20% of the data will be used for testing\nX_train, X_test, Y_train, Y_test= train_test_split(X, Y, test_size=0.2, random_state=7)","8d0a1e87":"Y_train","a171a677":"from sklearn.feature_selection import RFE\nimport statsmodels.api as sm \n","a3a4165a":"from statsmodels.stats.outliers_influence import variance_inflation_factor","a3fc0ab1":"df.select_dtypes(include=[np.number])","3109db82":"lm = LogisticRegression()\nlm.fit(X_train,Y_train)\n","f43d9cfe":"rfe = RFE(lm, 10)\nrfe = rfe.fit(X_train, Y_train)\nrfe_ = X_train.columns[rfe.support_]\nrfe_","4e978de2":"# Creating  dataframe with RFE selected variables\n\nx_train_rfe = X_train[rfe_]","6b4b6fbc":"def build_model(X,y):\n    X = sm.add_constant(X) \n    #Ading constant\n    lm = sm.OLS(y,X).fit() \n    print(lm.summary()) #summary\n    return X\n    \ndef checkVIF(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return(vif)","806f312d":"x_train_new = build_model(x_train_rfe,Y_train)\n","217758ef":"x_train_rfe","bccee8ff":"checkVIF(x_train_new)\n","be06b37b":"x_train_new = x_train_new.drop('const',axis=1)\n","23c1ade1":"checkVIF(x_train_new)\n","1588710c":"x_train_new = x_train_new.drop('housing',axis=1)\n","28b0cf3c":"x_train_new = build_model(x_train_new,Y_train)\n","0ff4ea14":"x_train_new = x_train_new.drop(columns=['other','jan'])\n","ab056487":"checkVIF(x_train_new)\n","80609ed9":"x_train_new = build_model(x_train_new,Y_train)\n","0229b87e":"x_train_new = x_train_new.drop('const',axis=1)\n","6fd1f5ac":"checkVIF(x_train_new)\n","1d717a71":"feats=list(x_train_new.columns)","37d432f2":"#now  for selected features we have to form a model and check accuracy\nlm_sel = LogisticRegression()\nlm_sel.fit(X_train[feats],Y_train)\n\n","5d82bbea":"auc=[X_train,X_train[feats]]\nresults_c = []\nnames_c = []\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression()))\nfor i in auc:\n        kfold = KFold(n_splits=10, random_state=7)    \n        # train the model\n        cv_results = cross_val_score(LogisticRegression(), i, Y_train, cv=kfold, scoring='accuracy')    \n        results_c.append(cv_results)\n        names_c.append(name)\n        msg = \"%s: %f (%f)\" % (LogisticRegression, cv_results.mean(), cv_results.std())\n        print(msg)","734211b7":"plt.boxplot(results_c)\nplt.show()","47aa98d6":"predictions = lm.predict(X_test)\npred_selected=lm_sel.predict(X_test[feats])","f17a0a5a":"print('For all features')\nprint(accuracy_score(Y_test, predictions))\nprint('For selected features')\nprint(accuracy_score(Y_test, pred_selected))\n","132075db":"rfe=list(x_train_new)\n\nselected=X_train[]","07a19658":"# Confusion Matrix \n\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(Y_test, predictions))","b46f85b2":"# Classification Report\nfrom sklearn.metrics import classification_report\nprint('This is for all features')\nprint(classification_report(Y_test, predictions))","ee66ea9f":"print('This is for selected features')\nprint(classification_report(Y_test, pred_selected))","597ca10a":"rfe=list(x_train_new)\nprint('These are the most important features for our model to make accurate predictions',rfe)","9a0000f1":"from sklearn.ensemble import RandomForestClassifier\n","46ff1e9f":"model = RandomForestClassifier(n_estimators=18)\nmodel.fit(X_train, Y_train)","a78e4c8d":"Y_pred=model.predict(X_test)\nconf_mat=confusion_matrix(Y_test,Y_pred)\nindex=['actual No','actual yes']\ncolumns=['predicted no','predicted yes']\ndemo=pd.DataFrame(conf_mat,index=index,columns=columns)\ndemo","11b0463b":"from sklearn.metrics import *","ffe8ec8b":"print('Recall score',round(recall_score(Y_test,Y_pred),2))\nprint('Precision score',round(precision_score(Y_test,Y_pred),2))","8c0b9d7c":"a=[X_train,X_train[feats]]\n","7f04f3ea":"for i in a:\n    kfold = KFold(n_splits=10, random_state=7)    \n    cv_results = cross_val_score(LogisticRegression(), i, Y_train, cv=kfold, scoring='accuracy')  \n    print('Cross validation score:',cv_results.mean())","8ca19801":"model_s = RandomForestClassifier(n_estimators=18)\nmodel_s.fit(X_train[feats], Y_train)","cd71db5e":"Y_pred_=model_s.predict(X_test[feats])\n","2b8692f8":"print('For all features')\nprint(accuracy_score(Y_test, Y_pred))\nprint('For selected features')\nprint(accuracy_score(Y_test, Y_pred_))","4f54344e":"# this is for all for all features\nprint(classification_report(Y_test, Y_pred))","43c7bb3c":"# this is for all for selected features\nclassification_report(Y_test, Y_pred_)","d3786a67":"print('Precision score for selected faetures',precision_score(Y_test,Y_pred_))","0aabc884":"### Which metric did you choose and why?","a474ccd5":"### Test LR model on the test set\u00b6\n","fa3e94ab":"To investigate more about correlation, a correlation matrix was plotted with all qualitative variables. Clearly, \u201ccampaign outcome\u201d has a strong correlation with \u201cduration\u201d, a moderate correlation with \u201cprevious contacts\u201d, and mild correlations between \u201cbalance\u201d, \u201cmonth of contact\u201d and \u201cnumber of campaign\u201d. Their influences on campaign outcome will be investigated further in the machine learning part.","a7002209":"### Predictive model 2: Random Forest","7a2a607e":"### Train test split","10f6572d":"#### Make a predictive model using random forest technique","bf9f03a5":"### Estimate the model performance using k fold cross validation","3e6083f9":"### The necessary transformations for the categorical variables and the numeric variables","fa1df44d":"### logistic regression achieved an accuracy of about 88%, suggesting a high level of strength of this model to classify the customer response given all the defined customer features.","e800491f":"#### Are the features about the previous campaign data useful?","1ac32c6d":"### Handle variables corresponding to the previous campaign","b807fe36":"### Converting categorical data into dummy variables","af7d40c1":"### Compare the performance of the Random Forest and the logistic model \u2013 ","126ffaf1":"#### Describe the pdays column, make note of the mean, median and minimum values. Anything fishy in the values?","c4b40555":"### bi variate analysis","d274046b":"#### previously we have done prediction by using all features now we have to only selected features","d6c8c814":"The next step after implementing a machine learning algorithm is to find out how effective is the model based on metric and datasets. Different performance metrics are used to evaluate different Machine Learning Algorithms. For example a classifier used to distinguish between images of different objects; we can use classification performance metrics such as, Precision score,accuracy score , recall score and Cross val score etc.\n\nThe machine learning model cannot be simply tested using the training set, because the output will be prejudiced, because the process of training the machine learning model has already tuned the predicted outcome to the training dataset. Therefore in order to estimate the generalization error, the model is required to test a dataset which it hasn\u2019t seen yet; giving birth to the term testing dataset.\n\nTherefore for the purpose of testing the model, we would require a labelled dataset. This can be achieved by splitting the training dataset into training dataset and testing dataset. This can be achieved by various techniques such as, k-fold cross validation,","f790fb9e":"#### Convert the response variable to a convenient form","829362b8":"### Predictive model 1: Logistic regression \n#### Make a predictive model using logistic regression\n\n","fe268459":"#### Make a box plot for pdays. Do you see any outliers?\n","1074aed2":"#### We have outliers which are greater thn 600in pdayss column after filtering also.","01836afb":"#### Plot a horizontal bar graph with the median values of balance for each education level value. Which group has the highest median?","fccd41f8":"\n ### This reveals a clear relationship among age, balance, duration and campaign.","068f8729":"#### Describe the pdays column again, this time limiting yourself to the relevant values of pdays. How different are the mean and the median values?\n\n","7b17913a":"These are the columns obtained with RFE.","7c19960d":"# BANK MARKETING ","4f3fb30d":"### What is the precision, recall, accuracy of your model?","0197a843":"## EXPLORATORY DATA ANALYSIS","415bf014":"#### By taking all features we have got an accuracy using K fold cross validation as %0.89 and By taking a selectde faetures we got 88%.","e1e797b6":"1. Accuracy score for selected features when we use logistic model is \n #### 0.8923\n and when we use Random forest model is\n #### 0.8915\n\n\n2. Precision score for selected features when we use logistic model is\n #### 0.5510\n and when we use Random forest model is\n #### 0.5204\n\n \n3. Cross val score for selected features when we use logistic model is\n #### 0.88557\n and when we use Random forest model is\n #### 0.8855\n","28f74ffc":"For binary classification model evaluation between random forest and logistic\nregression, our work focused on four distinct simulated datasets: \n(1) increasingthe variance in the explanatory and noise variables, \n(2) increasing the number of noise variables, \n(3) increasing the number of explanatory variables, \n(4) increasing the number of observations.\n\nTo benchmark and comparing classification scores\nbetween random forest and logistic regression, metrics such as accuracy, area\nunder the curve, true positive rate, false positive rate, and precision were analyzed. To provide statistical quantification as to whether a difference in model\nperformance is conclusive enough to state the difference is significant or if the\nobserved difference is by random chance","15e8340a":"#### Use RFE to select top n features in an automated fashion (choose n as you see fit)","c70b6a61":"#### Here we have not considering -1 values in pdays column and limiting ourselves , and then now we can observe the difference between mean and median values in before and now.\n#### There is  a huge difference from the previous values :\n#### Previously Mean was of 40 and median = -1 , now after limiting -1 values we have mean=224 and median=194","57c84939":"### Which features are the most important from your model?","cc183200":"#### Are pdays and poutcome associated with the target? ","0bb79fb6":"### Associations of numerical variables","6251d364":"Logistic has got better accuracy score compared to random forset, hence we can say that it has better performance","38fbdfc3":"### Associations of categorical variables","0f37e3e8":"### Which model has better performance on the test set? ","cd7de355":"#### Make suitable plots for associations with numerical features and categorical features\u2019","3483410c":"#### Cross validation score(for all features): 0.8910802412738141\n#### Cross validation score(for selected features): 0.8855786698274526\n"}}