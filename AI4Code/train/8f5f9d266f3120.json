{"cell_type":{"c52420d0":"code","a18d6a2a":"code","c1202d8e":"code","b03e3de9":"code","356669cd":"code","a648c815":"code","4b9416ea":"code","c586140c":"code","9b9d99d8":"code","d15d49fc":"code","38dd7648":"code","a585623e":"code","8821ebc1":"code","e3daa7d4":"code","d1d53fd1":"code","c4af81a2":"code","525e26ce":"code","329598c1":"code","47680157":"code","a55da2f4":"code","db133e3f":"code","345c1c67":"code","cc72bf5a":"code","d535c0fb":"code","fbacfcaa":"code","8876e11b":"code","6238308e":"code","4129ef5c":"markdown","650fc3e7":"markdown","65aafff0":"markdown","3455480c":"markdown","b9674fe2":"markdown","1260a54d":"markdown","791fa9a5":"markdown","52e976f8":"markdown","a667679d":"markdown","cd37559c":"markdown","532cc854":"markdown","113a55ef":"markdown","ffa44f7d":"markdown","20a1255d":"markdown","54f68f61":"markdown"},"source":{"c52420d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a18d6a2a":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c1202d8e":"train= pd.read_csv('..\/input\/titanic\/train.csv')\ntest= pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission= pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntest['Survived']= submission['Survived']","b03e3de9":"train.head()","356669cd":"test.head()","a648c815":"[train.shape, test.shape, submission.shape]","4b9416ea":"train_missing= train.isnull().sum().sort_values(ascending= False)\ntrain_per= (train.isnull().sum()\/train.isnull().count()*100).sort_values(ascending= False)\ntrain_total= pd.concat([train_missing, train_per], axis=1, keys=['Total', 'Percent'])\ntrain_total.head()","c586140c":"test_missing= test.isnull().sum().sort_values(ascending= False)\ntest_per= (test.isnull().sum()\/test.isnull().count()*100).sort_values(ascending= False)\ntest_total= pd.concat([test_missing, test_per], axis=1, keys=['Total', 'Percent'])\ntest_total.head()","9b9d99d8":"#we drop cain column as it has 77% missing values \n#Name column doesnt help us during prediction, so we are dropping name column\ntrain= train.drop('Cabin', axis=1)\ntrain= train.drop('Name', axis=1)","d15d49fc":"#we drop cain column as it has 77% missing values \n#Name column doesnt help us during prediction, so we are dropping name column\ntest= test.drop('Cabin', axis=1)\ntest= test.drop('Name', axis=1)","38dd7648":"plt.figure(figsize=(8,6))\nsns.heatmap(train.corr(), annot=True)","a585623e":"cols= train.columns\nno_of_cols = len(cols)\nprint(\"Number of columns: {}\".format(no_of_cols))\n\ntrain_numeric_cols = [col for col in train.columns if (train[col].dtype in (\"int32\", \"int64\", \"float64\"))]\nprint(\"Numeric columns: {}\".format(train_numeric_cols))\nprint(\"No of numeric columns: {}\".format(len(train_numeric_cols)))\n      \ntrain_categorical_cols = [col for col in train.columns if (train[col].dtype == 'object')]\nprint(\"Categorical columns: {}\".format(train_categorical_cols))\nprint(\"No of categorical columns: {}\".format(len(train_categorical_cols)))","8821ebc1":"cols= test.columns\nno_of_cols = len(cols)\nprint(\"Number of columns: {}\".format(no_of_cols))\n\ntest_numeric_cols = [col for col in test.columns if (test[col].dtype in (\"int32\", \"int64\", \"float64\"))]\nprint(\"Numeric columns: {}\".format(test_numeric_cols))\nprint(\"No of numeric columns: {}\".format(len(test_numeric_cols)))\n      \ntest_categorical_cols = [col for col in test.columns if (test[col].dtype == 'object')]\nprint(\"Categorical columns: {}\".format(test_categorical_cols))\nprint(\"No of categorical columns: {}\".format(len(test_categorical_cols)))","e3daa7d4":"col_fillna= ['Embarked']\nfor col in col_fillna:\n    train[col].fillna('no',inplace=True)","d1d53fd1":"train.fillna(train.median(), inplace=True)\ntest.fillna(test.median(), inplace=True)","c4af81a2":"print(train.isnull().sum().sum())\nprint(test.isnull().sum().sum())","525e26ce":"from sklearn.preprocessing import OrdinalEncoder\noe= OrdinalEncoder()\ntrain[train_categorical_cols]= oe.fit_transform(train[train_categorical_cols])\ntest[test_categorical_cols]= oe.fit_transform(test[test_categorical_cols])","329598c1":"x_train= train.drop('Survived', axis=1)\ny_train= train.Survived.copy()","47680157":"x_test= test.drop('Survived', axis=1)\ny_test= test.Survived.copy()","a55da2f4":"from sklearn.neighbors import KNeighborsClassifier\nkn= KNeighborsClassifier(n_neighbors=5, metric= 'minkowski', p=2)\nkn.fit(x_train, y_train)\n\ny_pred2= kn.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm= confusion_matrix(y_test, y_pred2)\nprint(cm)\naccuracy_score(y_test, y_pred2)","db133e3f":"from sklearn.svm import SVC\nksvc = SVC(kernel = 'rbf', random_state = 0)\nksvc.fit(x_train, y_train)\n\ny_pred3= ksvc.predict(x_test)\n\ncm= confusion_matrix(y_test, y_pred3)\nprint(cm)\naccuracy_score(y_test, y_pred3)","345c1c67":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\n\ny_pred4= nb.predict(x_test)\n\ncm= confusion_matrix(y_test, y_pred4)\nprint(cm)\naccuracy_score(y_test, y_pred4)","cc72bf5a":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\ndt.fit(x_train, y_train)\n\ny_pred6= dt.predict(x_test)\n\ncm= confusion_matrix(y_test, y_pred6)\nprint(cm)\naccuracy_score(y_test, y_pred6)","d535c0fb":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrf.fit(x_train, y_train)\n\ny_pred7= rf.predict(x_test)\n\ncm= confusion_matrix(y_test, y_pred7)\nprint(cm)\naccuracy_score(y_test, y_pred7)","fbacfcaa":"from sklearn.ensemble import GradientBoostingClassifier\nxgb= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\nxgb.fit(x_train, y_train)\n\ny_pred8= xgb.predict(x_test)\n\ncm= confusion_matrix(y_test, y_pred8)\nprint(cm)\naccuracy_score(y_test, y_pred8)","8876e11b":"submission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = y_pred4\nsubmission.to_csv('submission.csv',index=False)","6238308e":"submission.head()","4129ef5c":"# Train Test Split","650fc3e7":"**Navie Bayes**","65aafff0":"**Filling the missing values**","3455480c":"**Random Forest**","b9674fe2":"# Applying Classification model","1260a54d":"# Importing Dataset","791fa9a5":"**Decision Tree**","52e976f8":"**Navie bayes model has the highest accuracy, so we are selecting y_pred4**","a667679d":"# Submission","cd37559c":"# Importing Libraries","532cc854":"**SVC**","113a55ef":"# Data Visualization","ffa44f7d":"**XG Boost**","20a1255d":"**KNeighborsClassifier**","54f68f61":"# Encoding the train and test set"}}