{"cell_type":{"7468ea45":"code","25005f15":"code","eb04d986":"code","38a97293":"code","629ac59c":"code","b2139fb1":"code","87cfcf21":"code","c3bf4621":"code","c4f49ece":"code","e17bc203":"code","ed7a8505":"code","1d475f7d":"code","4f1e11f7":"code","96a6282c":"code","9efd3164":"code","75648ae3":"code","75ff227c":"code","429ca6bf":"code","4feebdbd":"code","ce37046c":"code","4c514b76":"code","f6644f9b":"code","7eeee483":"code","1022e7ee":"code","f2ecf6ed":"code","8e731b77":"code","6a201f2b":"code","5aa42c43":"code","3bfe682b":"markdown","69284f4f":"markdown","31d56615":"markdown","85dffc61":"markdown","0f69da7e":"markdown","8d422026":"markdown","b0ef8f68":"markdown","1493289e":"markdown","80a46b7e":"markdown","c0e34b94":"markdown","094235ed":"markdown","3f2166cd":"markdown","151ee7f5":"markdown","91736ec2":"markdown","97689407":"markdown","aeecbbbe":"markdown","9fbc6ed5":"markdown","b8b44d34":"markdown","90277e16":"markdown","7f7e480e":"markdown","44e59fc6":"markdown","e278d4c2":"markdown","3ffd5a81":"markdown","7392f388":"markdown","d73bf3d4":"markdown","65c8947c":"markdown","63a3e2ae":"markdown","13f7e02f":"markdown","c36627bc":"markdown","d7480c78":"markdown","842e8440":"markdown","3d8f39ea":"markdown","d7e6c406":"markdown","0ac19779":"markdown","b5f22323":"markdown","e2ebf2e4":"markdown","d5aa6a40":"markdown","f388eec8":"markdown","ba95114c":"markdown"},"source":{"7468ea45":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom ipywidgets import interact, interactive, fixed, interact_manual, Layout\nimport ipywidgets as widgets\n\nfrom bokeh.plotting import ColumnDataSource\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import widgetbox, row\noutput_notebook()","25005f15":"# f = lambda x: 0.5 * x**2 # <-- this is the func above..\nf = lambda x: 0.2 + x+ (1\/2)*x**2 -0.2*x**3 # <-- ...but this is funnier to see\nx = np.linspace(-2,6,100) # x points\ny = f(x) # y, function values in x","eb04d986":"plt.plot(x, y)","38a97293":"dx = 1e-6 # variable increment, x differential\ndy = f(x + dx) - f(x) # function values increment, function increrment\nd_y = dy\/dx # derivative of f in x\n\nplt.plot(x, y) # blue\nplt.plot(x, d_y) # orange","629ac59c":"d = lambda f, x, dx: (( f(x+dx) - f(x) ) \/ dx)","b2139fb1":"plt.plot(x,f(x))\nplt.plot(x, d(f, x, dx))","87cfcf21":"x0 = 1\ndx = 4e-2\nlr = 1e-2\ndf_x = d(f,x0,dx)\/dx\nx_hist = []\n\nfor it in range(600):\n    x1 = x0 - lr * df_x\n    df_x = d(f,x1,dx)# x1 -x0\n    x0 = x1\n    x_hist.append(x0)\n\nprint('x = ', x0)\nplt.plot(x_hist);","c3bf4621":"def gd_1(iterations):\n    x0 = 1\n    lr = 1e-2\n    df_x = 1e-6\n\n    x_hist = []\n    for i in range(iterations):\n        x1 = x0 - lr * df_x\n        df_x = d(f,x1,dx)# x1 -x0\n        x0 = x1\n        x_hist.append(x0)\n    \n    #\u00a0Plot #\n    r1.data_source.data= { 'x': range(len(x_hist)), 'y': x_hist }\n    r2.data_source.data['y'] = f(x)\n    pallino.data = {'x':[x0], 'y':[f(x0)]}\n    push_notebook()","c4f49ece":"### Visualization ###\np1 = figure(title=\"x value\", plot_height=300, plot_width=300, y_range=(-5,5),\n           background_fill_color='#efefef', x_axis_label='iterations')\nr1 = p1.line(range(len(x_hist)), x_hist, color=\"#8888cc\", line_width=1.5, alpha=0.8)\npallino = ColumnDataSource(data={'x':[x0], 'y':[f(x0)]})\n\np2 = figure(title=\"Minimum reaching\", plot_height=300, plot_width=600, y_range=(-5,5),\n           background_fill_color='#efefef', x_axis_label='x')\nr2 = p2.line(x, y, color=\"#8888cc\", line_width=1.5, alpha=0.8)\npallino = ColumnDataSource(data={'x':[x0], 'y':[f(x0)]})\np2.circle('x', 'y', source=pallino, size=20, color='orange')\nlayout = row([p1,p2])\n\ninteract(gd_1, iterations=widgets.IntSlider(min=0, max=150, continous_update=False, layout=Layout(width='500px')))       \nshow(layout, notebook_handle=True);","e17bc203":"def gd_1(iterations,lrp):\n    x0 = 1\n    df_x = 1e-6\n    \n    x_hist = []\n    lr = 10**lrp\n    for i in range(iterations):\n        x1 = x0 - lr * df_x\n        df_x = d(f,x1,dx)# x1 -x0\n        x0 = x1\n        x_hist.append(x0)\n\n    # Plot #\n    r1.data_source.data= { 'x': range(len(x_hist)), 'y': x_hist }\n    r2.data_source.data['y'] = f(x)\n    pallino.data = {'x':[x0], 'y':[f(x0)]}\n    push_notebook()","ed7a8505":"### Visualization ###\np1 = figure(title=\"x value\", plot_height=300, plot_width=300, y_range=(-5,5),\n           background_fill_color='#efefef', x_axis_label='iterations')\nr1 = p1.line(range(len(x_hist)), x_hist, color=\"#8888cc\", line_width=1.5, alpha=0.8)\npallino = ColumnDataSource(data={'x':[x0], 'y':[f(x0)]})\n\np2 = figure(title=\"Minimum reaching\", plot_height=300, plot_width=600, y_range=(-5,5),\n           background_fill_color='#efefef', x_axis_label='x')\nr2 = p2.line(x, y, color=\"#8888cc\", line_width=1.5, alpha=0.8)\npallino = ColumnDataSource(data={'x':[x0], 'y':[f(x0)]})\np2.circle('x', 'y', source=pallino, size=20, color='orange')\nlayout = row([p1,p2])\n\ninteract(\n    gd_1,\n    iterations=widgets.IntSlider(min=0, max=150, layout=Layout(width='500px')),\n    lrp=widgets.IntSlider(min=-5, max=3, value=-5, layout=Layout(width='500px')),\n        )       \nshow(layout, notebook_handle=True);","1d475f7d":"x = np.linspace(-2,2,100)\n\n# 1. define the true points\ny_true = 0.5*x # I like this, you choose another one :P\n# 2. define the model\ng = lambda w: w*x # x is given, in this context our variable is w.\n# 3. define the objective function\nJ = lambda y: np.sum( (y - y_true)**2 )\/len(y) # J(w) = J( g(w) )\n# 4. define the derivative of the objective function\ndJ_w = lambda y_new, y, dw: (J(y_new) - J(y))\/dw\n\n\n\n\nw0 = 0.1\nlr = 5e-3\n\n# first step\ny = g(w0)\ndJ_wn = 1e-5\n# loop\nw_hist = []\nJ_hist = []\nfor it in range(400):\n    w1 = w0 - lr * dJ_wn\n    y_new = g(w1)\n    dw = w1 - w0\n    Jn = J(y_new)\n    dJ_wn = dJ_w(y_new,y,dw)\n    w0 = w1\n    y = y_new\n    # store\n    w_hist.append(w0)\n    J_hist.append(Jn)\n\nprint('w_true = 0.5\\nw_pred = ', w0)\nplt.plot(w_hist);\nplt.title('w');","4f1e11f7":"def gd_lr(its, lrp):\n    # params\n    w0 = 0.1\n    lr = 10**lrp\n\n    # 1. define the true points\n    y_true = 0.5*x \n    g = lambda w: w*x \n    d = lambda f, x, dx: (( f(x+dx) - f(x) ) \/ dx)\n    J = lambda y: np.sum( (y - y_true)**2 )\/len(y) # J(w) = J( g(w) )\n    dJ_w = lambda y_new, y, dw: (J(y_new) - J(y))\/dw\n\n\n    y = g(w0)\n    dJ_wn = 1e-5\n    # loop\n    w_hist = []\n    J_hist = []\n    for it in range(its):\n        w1 = w0 - lr * dJ_wn\n        y_new = g(w1)\n        dw = w1 - w0\n        Jn = J(y)\n        dJ_wn = dJ_w(y_new,y,dw)\n        w0 = w1\n        y = y_new\n        # store\n        w_hist.append(w0)\n        J_hist.append(Jn)\n  \n    print('w= 0.5\\nw_calc = ',w0,  '\\ncost= ', J_hist[-1])\n    fig, ax = plt.subplots(1,3, figsize=(12,5));\n    ax[0].plot(range(its), J_hist);\n    ax[0].set_xlabel('iterations');\n    ax[0].set_title('cost');\n    ax[1].plot(range(its), w_hist);\n    ax[1].set_xlabel('iterations');\n    ax[1].set_title('w calculated');\n    ax[2].plot(y);\n    ax[2].plot(y_true);\n    ax[2].set_xlabel('x');\n    ax[2].set_ylabel('y');\n    ax[2].legend(['y_true', 'y']);\n    plt.subplots_adjust(left=0);\n\ninteract(gd_lr, its=widgets.IntSlider(min=1, max=230, continous_update=False), lrp=(-5,2) );","96a6282c":"x = np.linspace(-2,2,100)\n\n# 1. define the true points\ny_true = -10*x + 3*x**2\n# 2. define the model\ng = lambda w1, w2: +w1*x + w2*x**2 # x is given, remember.\n# 3. define the objective function\nJ = lambda w1, w2: np.sum( (g(w1, w2) - y_true)**2 )\/len(y_true) # <-- g(w1,w2), so to indipendently act on each w\n# 4. define the derivative of the objective function\ndJ_w1 = lambda w1, w2, dw1: ( J(w1+dw1, w2) - J(w1, w2) )\/dw1\ndJ_w2 = lambda w1, w2, dw2: ( J(w1, w2+dw2) - J(w1, w2) )\/dw2\n\n\n\n\n\ndef gd_2p(its, lrp):\n    w1 = 0.1\n    w2 = 0.15\n    lr = 10**lrp\n\n    # first step\n    dJ_w1n = 1e-2\n    dJ_w2n = 1e-2\n\n    # loop\n    w_hist = []\n    J_hist = []\n    for it in range(its):\n        w1_new = w1 - lr * dJ_w1n\n        w2_new = w2 - lr * dJ_w2n # you have 2 params here, thus you must update both\n      #   y_new = g(w1_new, w2_new)\n        dw1 = w1_new - w1\n        dw2 = w2_new - w2\n        Jn = J(w1_new, w2_new) \n        dJ_w1n = dJ_w1(w1, w2, dw1); #print(dJ_w1n)\n        dJ_w2n = dJ_w2(w1, w2, dw2)\n\n        w1 = w1_new\n        w2 = w2_new\n        # store\n        w_hist.append([w1, w2])\n        J_hist.append(Jn)\n\n    w_hist = np.vstack(w_hist)\n    print(w_hist.shape)\n    print('w1_true = -10\\nw_pred = ', w_hist[-1,0])\n    # plt.plot(J_hist)#w_hist[:,0]);\n\n    print('w= 0.5\\nw_calc = ',w0,  '\\ncost= ', J_hist[-1])\n\n    fig, ax = plt.subplots(2,2, figsize=(12,8));\n    ax[0,0].plot(range(its), w_hist[:,0]);\n    ax[0,0].set_xlabel('iterations');\n    ax[0,0].set_title('w1 calculated');\n    ax[0,1].plot(range(its), w_hist[:,1]);\n    ax[0,1].set_xlabel('iterations');\n    ax[0,1].set_title('w2 calculated');\n    ax[1,0].plot(x, g(w1,w2));\n    ax[1,0].plot(x, y_true);\n    ax[1,0].set_xlabel('x');\n    ax[1,0].legend();\n    ax[1,0].set_title('functions');\n    ax[1,1].plot(range(its), J_hist);\n    ax[1,1].set_xlabel('iterations');\n    ax[1,1].set_title('J(w1,w2)');\nplt.subplots_adjust(hspace =0.9);\n\ninteract(gd_2p, its=widgets.IntSlider(min=1, max=230, continous_update=False), lrp=(-5,2) );","9efd3164":"from mpl_toolkits.mplot3d import axes3d\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n\n\nE = lambda w1, w2: (g(w1, w2) - y_true)**2\nncts = 100\nw1 = w2 = np.linspace(-12, 5, ncts)\n\n\n\nfig = plt.figure(figsize=(12,12))\nax = fig.gca(projection='3d')\n\n# X, Y, Z = axes3d.get_test_data(0.05)\nX, Y = np.meshgrid(w1,w2); print(X.shape, Y.shape)\nZ = E(X,Y); print(Z.shape)\n#ax.view_init(-5, 10)  #  <---- play with this\nax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.45, color ='yellow')\ncset = ax.contour(X, Y, Z, zdir='z', offset=-100, cmap=cm.coolwarm)\ncset = ax.contour(X, Y, Z, zdir='x', offset=-40, cmap=cm.coolwarm)\ncset = ax.contour(X, Y, Z, zdir='y', offset=40, cmap=cm.coolwarm)\n\nZ = np.ones((ncts, ncts)) * 0\nax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.9, color ='grey')\n\nax.set_xlabel('w1')\n# ax.set_xlim(-40, 40)\nax.set_ylabel('w2')\n# ax.set_ylim(-40, 40)\nax.set_zlabel('E(w1,w2)')\n#ax.set_zlim(-3, 1)   #  <----  play with this\n\nplt.show()\n","75648ae3":"npts = 100\nX = np.array([\n    np.linspace(-2,2,npts),\n    np.linspace(-2,2,npts),\n])\nX.shape","75ff227c":"x1 = X[0]\nx2 = X[1]","429ca6bf":"# define a function and its partial derivatives\nf = lambda x1, x2: x1 + x2**2\ndx1 = lambda f, x1, x2, ep: (( f(x1+ep,x2) - f(x1,x2) ) \/ ep)\ndx2 = lambda f, x1, x2, ep: (( f(x1,x2+ep) - f(x1,x2) ) \/ ep)\n\neps = 1e-7","4feebdbd":"# https:\/\/matplotlib.org\/examples\/mplot3d\/surface3d_demo.html\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\n\ndef plot_f_df(f, x1,x2, fs=15, figsize=(12,18)):\n    fig = plt.figure(figsize=figsize)\n\n\n    # Make data.\n    x1grid, x2grid = np.meshgrid(x1, x2)\n    zgrid = f(x1grid, x2grid)\n\n    d_x1grid = dx1(f, x1grid, x2grid, 1e-1) # <--- eps lower makes the fx1 to be noisy\n    d_x2grid = dx2(f, x1grid, x2grid, eps)\n\n\n    # Data about a line, parallel to x2, on the f(x1,x2) surface\n    rip = 35\n    x1_pc = (x1.max()-x2.min())\/rip\n    x1b = x1_pc * rip\/4\n    x1_mini_rng = x1[ (x1 > -x1_pc + x1b) & (x1 < x1_pc +x1b) ]\n    x1_mini_grid, x2_mini_grid =  np.meshgrid(x1_mini_rng, x2)\n    z_mini_grid = f(x1_mini_grid, x2_mini_grid) + 0.1\n\n\n    # Plot the surface\n    ax1  = plt.subplot(311, projection='3d')\n    ax1.plot_surface(x1grid, x2grid, zgrid, linewidth=0, antialiased=False, alpha=0.7)\n    ax1.plot_surface(x1_mini_grid, x2_mini_grid, z_mini_grid, color='red', linewidth=5 )\n    ax1.set_xlabel('x1', fontsize=fs)\n    ax1.set_ylabel('x2', fontsize=fs)\n    ax1.set_zlabel('f(x1,x2)', fontsize=fs)\n\n    # Plot d_x2 surface\n    ax2  = plt.subplot(312, projection='3d')\n    ax2.plot_surface(x1grid, x2grid, d_x1grid)\n    ax2.set_xlabel('x1', fontsize=fs)\n    ax2.set_ylabel('x2', fontsize=fs)\n    ax2.set_zlabel('f_x1', fontsize=fs)\n\n    # Plot d_x2 surface\n    ax3  = plt.subplot(313, projection='3d')\n    ax3.plot_surface(x1grid, x2grid, d_x2grid)\n    ax3.set_xlabel('x1', fontsize=fs)\n    ax3.set_ylabel('x2', fontsize=fs)\n    ax3.set_zlabel('f_x2', fontsize=fs)\n","ce37046c":"plot_f_df(f, x1,x2)","4c514b76":"f = lambda x1,x2: np.array([ -5*x1, 0.5*x2**2])  # f(x1,x2), here w1_true and w2_true are fixed\n\nmodel = lambda w1, w2, x1, x2: np.array([  w1*x1, w2*x2**2  ])  # f(x1, x2; w1, w2) = w1x1 i + w2x2 j","f6644f9b":"J = lambda y, y_true: np.sum((y - y_true)**2 )\/len(y) \nJ2 = lambda y1, y2, y1_true, y2_true: np.array([ J(y1, y1_true),  J(y2, y2_true)]) # J2 = [J(y1), J(y2)]","7eeee483":"# now we can define the differential of the cost\ndJ2 = lambda J2, y1,y2, y1_old,y2_old, y1_true,y2_true: np.array([  # <- is just an array, please don't get lost\n  J2(y1,y2, y1_true,y2_true)[0] - J2(y1_old,y2_old, y1_true,y2_true)[0], # J2(y1, y_true) - J2(y1_old, y_true) = dJ2\n  J2(y1,y2, y1_true,y2_true)[1] - J2(y1_old,y2_old, y1_true,y2_true)[1]\n])","1022e7ee":"dW = lambda w1_new, w2_new, w1, w2: np.array([w1_new - w1, w2_new - w2])   # [dw1, d2w]","f2ecf6ed":"dJ2_W = lambda dJ, dW: np.array([ dJ[0]\/dW[0], dJ[1]\/dW[1]  ])","8e731b77":"f = lambda x1,x2: np.array([-5*x1, 0.5*x2**2])  # f(x1,x2), here w1_true and w2_true are fixed\nmodel = lambda w1, w2, x1, x2: np.array([  w1*x1, w2*x2**2  ])  # f(x1, x2; w1, w2) = w1x1 i + w2x2 j\n\n\n\niterations = 10\n\ndef gd_plot2(iterations):\n    npts = 100\n    X = np.array([\n      np.linspace(-2,2,npts),\n      np.linspace(-2,2,npts),\n    ])\n    lr = 1e-2\n\n    # Initialize\n    W = [0.1, 0.15]\n    W_new = [0.1+1e-5, 0.15+1e-5]\n    y_true = f(X[0], X[1])\n    y   = model( W[0],W[1], X[0],X[1] )          # I know there are a lot of cuttable things here, but i thought...\n    y_new = model(W_new[0], W_new[1], X[0],X[1]) #    ...it was a simpler way to let follow the reasoning...\n    dWn = dW( W_new[0],W_new[1], W[0],W[1] )     #    ...so, if you want, you can rewrite it and let me know :)\n    dJ2n = dJ2(J2, y_new[0],y_new[1], y[0],y[1], y_true[0],y_true[1])\n    dJ2_Wn = dJ2_W(dJ2n, dWn)\n\n  \n  # Loop\n    W_hist = []\n    J2_hist= []\n    for i in range(iterations):\n        W_new = [ W[0] - lr * dJ2_Wn[0], W[1] - lr * dJ2_Wn[1] ]\n        y_new = model( W_new[0],W_new[1], X[0],X[1] )\n        J2n = J2( y[0],y[1], y_true[0],y_true[1] )\n\n        # update\n        dWn = dW( W_new[0],W_new[1], W[0],W[1] )\n        dJ2n = dJ2(J2, y_new[0],y_new[1], y[0],y[1], y_true[0],y_true[1])\n        dJ2_Wn = dJ2_W(dJ2n, dWn)\n        y = y_new\n        W = W_new\n\n        W_hist.append(W)\n        J2_hist.append(J2n)\n\n    J2_hist = np.vstack(J2_hist)\n    W_hist  = np.vstack(W_hist)\n\n    print('W_true:', [-5, 0.5], '\\nW_calc:', W, '\\nJ:', J2n)\n    fig, ax = plt.subplots(1,2, figsize=(8,6))\n    ax[0].plot(range(iterations), J2_hist[:,0])\n    ax[0].set_xlabel('iterations')\n    ax[0].set_ylabel('J2(w1)')\n    # ax[1].plot(W_hist[:,1], J2_hist[:,1])\n    ax[1].plot(range(iterations), J2_hist[:,1])\n    ax[1].set_xlabel('iterations')\n    ax[1].set_ylabel('J2(w2)')\n\ninteract(gd_plot2, iterations=(0,250,3));","6a201f2b":"## Interacrive visualization of surface shaping through gradient descent\n\ndef gd_2(its):\n    npts = 100\n    X = np.array([\n        np.linspace(-2,2,npts),\n        np.linspace(-2,2,npts),\n    ])\n    lr = 1e-2\n    f = lambda x1,x2: np.array([-5*x1, 0.5*x2**2])  # f(x1,x2), here w1_true and w2_true are fixed\n    model = lambda w1, w2, x1, x2: np.array([  w1*x1, w2*x2**2  ])  # f(x1, x2; w1, w2) = w1x1 i + w2x2 j\n\n    # Initialize\n    W = [0.1, 0.15]\n    W_new = [0.1+1e-5, 0.15+1e-5]\n    y_true = f(X[0], X[1])\n    y   = model( W[0],W[1], X[0],X[1] )\n    y_new = model(W_new[0], W_new[1], X[0],X[1])\n    dWn = dW( W_new[0],W_new[1], W[0],W[1] )\n    dJ2n = dJ2(J2, y_new[0],y_new[1], y[0],y[1], y_true[0],y_true[1])\n    dJ2_Wn = dJ2_W(dJ2n, dWn)\n\n  \n  # Loop\n    W_hist = []\n    J2_hist= []\n    for i in range(its):\n        W_new = [ W[0] - lr * dJ2_Wn[0], W[1] - lr * dJ2_Wn[1] ]\n        y_new = model( W_new[0],W_new[1], X[0],X[1] )\n        J2n = J2( y[0],y[1], y_true[0],y_true[1] )\n\n        # update\n        dWn = dW( W_new[0],W_new[1], W[0],W[1] )\n        dJ2n = dJ2(J2, y_new[0],y_new[1], y[0],y[1], y_true[0],y_true[1])\n        dJ2_Wn = dJ2_W(dJ2n, dWn)\n        y = y_new\n        W = W_new\n\n        W_hist.append(W)\n        J2_hist.append(J2n)\n    return W_hist, J2_hist\n\n\ndef plot_model_W(iterations):\n    f     = lambda x1,x2: np.array([ -5*x1, -x1 + 0.5*x2**2 ])  # f(x1,x2), here w1_true and w2_true are fixed\n    model = lambda w1, w2, x1, x2: np.array([  w1*x1, -x1 + w2*x2**2  ])  # f(x1, x2; w1, w2) = w1x1 i + w2x2 j\n\n    W_hist, J2_hist = gd_2(iterations)\n\n    W = np.vstack(W_hist)\n\n    # Make the Grid\n    X_g = np.meshgrid(X[0], X[1])\n\n\n    # The function has to be calculated over the X grid, but per W single values.\n    Z = model( W[-1][0],W[-1][1], X_g[0],X_g[1] ).sum(axis=0)\n    Y_true = f(X_g[0],X_g[1]).sum(axis=0)\n    #Y_true = model( 0.2,5, X_g[0],X_g[1] ).sum(axis=0); print('Z: ', Z.shape, 'Y_true: ', Y_true.shape)\n\n\n\n    ## Plot\n    fs=15\n    fig = plt.figure(figsize=(9,6))\n\n    # Plot the model\n    ax = plt.subplot(221, projection='3d')\n    ax.plot_surface(X_g[0], X_g[1], Z, color='red', linewidth=5 );\n    ax.set_xlabel('x1', fontsize=fs);\n    ax.set_ylabel('x2', fontsize=fs);\n    ax.set_title('f(x1,x2; w1,w2)', fontsize=fs);\n    ax.set_zlim(-1,130)\n    # Plot the true surface\n    ax = plt.subplot(222, projection='3d')\n    ax.plot_surface(X_g[0], X_g[1], Y_true, color='red', linewidth=5 );\n    ax.set_xlabel('x1', fontsize=fs);\n    ax.set_ylabel('x2', fontsize=fs);\n    ax.set_title('Y_true', fontsize=fs);\n    ax.set_zlim(-1,130);\n\n    J2_hist = np.vstack(J2_hist)\n    ax = plt.subplot(223)\n    xt = np.arange(iterations);# print('xt.shape: ', xt.shape, '. J2: ', J2_hist.shape)\n    ax.plot(xt, J2_hist[:,0])\n    ax.set_xlabel('iterations')\n    ax.set_ylabel('J2(w1)')\n\n    ax = plt.subplot(224)\n    ax.plot(np.arange(iterations), J2_hist[:,1])\n    ax.set_xlabel('iterations')\n    ax.set_ylabel('J(w2)')\n\n#sl =  widgets.IntSlider(min=0, max=150, step=3, value=0)\nsl = widgets.IntSlider(min=0, max=40, step=1, value=1, continuous_update=False)\ninteract(plot_model_W, iterations=sl);\n\n# J2_hist","5aa42c43":"\nnpts = 300\n###############\n\ndef plot_params_space(elev, azim):\n    J = lambda W: (W[0]*X[0] + W[1]*X[1]**2) - (0.2*X[0] + 5*X[1]**2)\n\n    # Make the Grid\n    X = [\n      np.linspace(-2, 2, npts),\n      np.linspace(-2, 2, npts),\n    ]\n    W = [\n      np.linspace(-6, 6, npts),\n      np.linspace(-6, 6, npts),\n    ]\n    W_g = np.meshgrid(W[0], W[1])\n\n    Jn = J(W_g); print('Jn: ', np.shape(Jn))\n\n\n\n\n    ## Plot\n    fs=15\n    fig = plt.figure(figsize=(9,6))\n\n    # Plot the model\n    ax = plt.subplot(111, projection='3d')\n    #angle = 190\n    ax.view_init(elev, azim)\n    ax.plot_surface(W_g[0], W_g[1], Jn, color='red', linewidth=5 );\n    ax.set_xlabel('w1', fontsize=fs);\n    ax.set_ylabel('w2', fontsize=fs);\n    ax.set_title('f(x1,x2; w1,w2)', fontsize=fs);\n    ax.set_zlim(-1,50)\n\nsl1 =  widgets.IntSlider(min=0, max=360, step=1, value=0, continous_update=False)\nsl2 =  widgets.IntSlider(min=0, max=360, step=1, value=0, continous_update=False)\ninteract(plot_params_space, elev=sl1, azim=sl2) \n\n","3bfe682b":"Define the cost function","69284f4f":"#### Bonus: Error function in the Parameters space","31d56615":"First of all I will reproduce the GD for a one dimensional variable and, afterwards I'll extend the reasoning to two dimensions.\n\nI will not abound in theory, since it can be found everywhere.","85dffc61":"The *learning rate* is an important part of the algorithm, because it can boost it or make it fail. To better understand this let's play with its magnitude order and see what happens in number of iterations needed to *converge* or, maybe, the solution will even not converge at all.","0f69da7e":"The way to solve the regression problem in more than one dimension, namely more than one **parameter**, is very similar. Before to start let's have an idea about the representation in space of such a function.","8d422026":"Let's try it, we expect the same result as previously seen.","b0ef8f68":"#### Bonus:  Visualizing the cost surface in the parameters space\nSame as for 2 parameters problem.","1493289e":"Here is the GD core for a 1-dimensional variable *x*, namely the update rule.  \n$$ x_1 = x_0 - lr \\cdot \\frac{dy}{dx} $$\nWe are now going to build piece by piece this operation.","80a46b7e":"Define the weights differentials","c0e34b94":"##### Interactive Gradient Descent","094235ed":"Define the true function and the model","3f2166cd":"In this Kernel I'll reconstruct  the (steepest) Gradient Descent algorithm, from an analytical point of view, trying to be coherent with its formalism, thus **I'll not use straightforwardly the famous (elegant) linear algebra algorithm you can find everywhere**.   \nThis means it will not be an example of performance, but for me it's been an important effort to furtherly understand such a famous method.  \n\nThis notebook is part of my personal \"back to the Deep Learning fundamentals\" journey, during which I want to question my understanding of the underlying mechanism. I hope it will be useful for other people.  \n\nPlease don't hexitate to comment about improvements or imperfection.  \n\nIndex:\n* [GD on 1 parameter function](#gd1p)\n* [GD on 2 parameters function](#gd2p)\n* [GD on 2 dimensions function](#gd2d)","151ee7f5":"It is would be very interesting to see the shape of the Error function vs the parameters, in order to get a feeling about the behaviour of the Error for this model. Anyway pyplot is not the best for doing these things, at least for me, so I let you play by yourself with it. If you are patient (and with a lot of imagination...) you will see the intercept planes at (w1,w2) equal to those of the y_true. Enjoy!","91736ec2":"### <a id=\"gd1p\"> 1-dim (as parameters)<\/a>\n","97689407":"Note how, as the cost keeps decreasing, the model line resembles the true funtion.\n\nYou can try to play with another function too, but in order to not change this code you should write a model with the same nunmber of parameters (1, here).  \nHere is an example:  \n*y_true =* _0.5*x**3 -0.5*x**2 -0.5*x_  \n_y = w*x**3 -w*x**2 -w*x_","aeecbbbe":"###### Interactive visualization of surface shaping through gradient descent","9fbc6ed5":"Now gradient, made by the partial derivatives","b8b44d34":"Let's take a look to our *y=f(x)*","90277e16":"### <a id='gd2d'>2-dim of more (as space dimensions )<\/a>","7f7e480e":"Let's play with parameters as we did before.","44e59fc6":"#### <a id ='derivative'>derivative of a function<\/a>","e278d4c2":"### <a id='gd2p'>2-dim of more (as parameters)<\/a>","3ffd5a81":"#### 2params Function definition","7392f388":"First of all we have to define a linear function *f(x)*.  \nLet's say\n$$ f(x) = 0.5 x^2 $$\nWith *x* defined over all our points space, being (arbitrarily)  _[-2, 2]_.","d73bf3d4":"#### Function visualization","65c8947c":"### Final notes\nIt is likely that I did something wrong with this kernel.  \nIf so, please comment and I will correct.  \nI really hope you can help someone better understand this topic.\n\nHave fun :)","63a3e2ae":"This time our function will be a so formally written\n$$ g(x; w)$$\nbut, for simplicity, we can say that it's  a $$g(w)$$, given *x*.\n\nStarting from the previous example we will define a GD for a **linear regression**.  \nThis time our objective function will be no more the derivative, but the *squared errors sum*, where this is formally defined as\n$$ J(y_{pred}) = \\sum_{i=1}^n(y_{pred,i}-y_i)^2$$, where *y_{pred}* are the points coming out from the *model*: $$y_{pred} = g(w)$$.\n\nTo recap, we have 3 functions here:  \n * a \"true\" one f(x)\n * a regression model g(w), resembling the true one.\n * a objective function J(w), to minimize.\n \n Our update rule now become\n $$w_1 = w_0 - lr \\cdot \\frac{dJ}{dw}$$","13f7e02f":"Note the line in first plot, it is useful to understand the \"bidimensional point of view\" of the function, which is a surface in a 3d space.   \nYou can compare _f(x1,x2) = x1 + x2^2_  with _f(x) = x^2_.","c36627bc":"#### <a id='general_gradient_descent_1'>General Gradient descent<\/a>","d7480c78":"Now let's see an interactive plot.\nWe will set the iterations number and we'll see how the *ball*, i.e. our *x0*, approaches the minimum of the function while the iteretions number grows.","842e8440":"1. #### <a id='gradient_descent_obj'>Gradient descent with specific objective function<\/a>","3d8f39ea":"Define the cost differentials","d7e6c406":"In order to let the method similar to the previous we will use variables as vectors or scalar, not as matrices.","0ac19779":"Now, we want to find the **minimum** of the function. To do so, we look for a *x* value, where the derivative is zero.\n\nIn other words\n$$ dy\/dx $$\nnamely the derivative, is our *objective function* to minimize (its zero is to pursue).","b5f22323":"Let's put all inside a loop to play a bit","e2ebf2e4":"#### <a id='function1'>Function definition<\/a>","d5aa6a40":"Now let's do a step forward: make a derivative function.","f388eec8":"First of all let's get a feeling about a 2D function (surface) in a three dimensional space","ba95114c":"We can compute the derivative of the function relying on the definition of derivative as differentials ratio.\n$$ y' = dy\/dx$$.\n\nThis is implemented as follow:\n1. Choose a small variation interval for *x*, this will be our *variable increment dx*.\n2. Compute the function *f(x+dx)* and **its values** increment with respect to the *f(x)* values.\n3. Execute the ratio"}}