{"cell_type":{"456bcb39":"code","ed05a7ff":"code","52d975d3":"code","cb2ee8ad":"code","0e89d8f3":"code","92f89485":"code","c0bfdfd0":"code","a75475e4":"code","c76a747b":"code","87a5ea84":"code","914257f9":"code","99dd685a":"code","93d22de6":"code","c6526979":"code","546b6c43":"code","29b715f8":"code","3522ea07":"code","79973031":"markdown","93920c0b":"markdown","f7484c8b":"markdown","9b6fd028":"markdown","ce19ff69":"markdown","6ec39a2c":"markdown","ecc99dcd":"markdown","19d08c3f":"markdown","6fa6706c":"markdown","c3e10c22":"markdown","46ce128d":"markdown","b934f6bb":"markdown","021211e5":"markdown","29a47d53":"markdown"},"source":{"456bcb39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","ed05a7ff":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata                  ","52d975d3":"data.isnull().sum()","cb2ee8ad":"f = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\n\nfor _ in f:\n    print(_,\"= \",(data[_]==0).sum())   ","0e89d8f3":"data['BMI']=data.BMI.mask(data.BMI == 0,(data['BMI'].mean(skipna=True)))\ndata['SkinThickness']=data.SkinThickness.mask(data.SkinThickness == 0,(data['SkinThickness'].mean(skipna=True)))\ndata['BloodPressure']=data.BloodPressure.mask(data.BloodPressure == 0,(data['BloodPressure'].mean(skipna=True)))\ndata['Glucose']=data.Glucose.mask(data.Glucose == 0,(data['Glucose'].mean(skipna=True)))\nprint(data.head(15))","92f89485":"data['Age']=data['Age'].astype(int)\ndata.loc[data['Age'] <= 16, 'Age']= 0\ndata.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age'] = 1\ndata.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age'] = 2\ndata.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age'] = 3\ndata.loc[data['Age'] > 64, 'Age'] = 4\n\ndata['Glucose']=data['Glucose'].astype(int)\ndata.loc[data['Glucose'] <= 80, 'Glucose']= 0\ndata.loc[(data['Glucose'] > 80) & (data['Glucose'] <= 100), 'Glucose'] = 1\ndata.loc[(data['Glucose'] > 100) & (data['Glucose'] <= 125), 'Glucose'] = 2\ndata.loc[(data['Glucose'] > 125) & (data['Glucose'] <= 150), 'Glucose'] = 3\ndata.loc[data['Glucose'] > 150, 'Glucose'] = 4\n\ndata['BloodPressure']=data['BloodPressure'].astype(int)\ndata.loc[data['BloodPressure'] <= 50, 'BloodPressure']= 0\ndata.loc[(data['BloodPressure'] > 50) & (data['BloodPressure'] <= 65), 'BloodPressure'] = 1\ndata.loc[(data['BloodPressure'] > 65) & (data['BloodPressure'] <= 80), 'BloodPressure'] = 2\ndata.loc[(data['BloodPressure'] > 80) & (data['BloodPressure'] <= 100), 'BloodPressure'] = 3\ndata.loc[data['BloodPressure'] > 100, 'BloodPressure'] = 4\n\ndata","c0bfdfd0":"data.drop(['Insulin'], axis = 1)","a75475e4":"def bar_chart(feature):\n    Positive = data[data['Outcome']==1][feature].value_counts()\n    Negative = data[data['Outcome']==0][feature].value_counts()\n    df = pd.DataFrame([Positive,Negative])\n    df.index = ['Positive','Negative']\n    df.plot(kind='bar', stacked=True, figsize=(10,5))","c76a747b":"bar_chart('Glucose')","87a5ea84":"bar_chart('BloodPressure')","914257f9":"corrMatrix = data.corr()\nsns.heatmap(corrMatrix, annot=True )\nplt.show()","99dd685a":"X = data.iloc[: , :-1].values\ny = data.iloc[: , -1].values","93d22de6":"from sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X, y , test_size = 0.2,random_state=0 )","c6526979":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","546b6c43":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","29b715f8":"q = []\ni=[3,4,5,6]\nj=[3,4,5,6]\n\nfor a in i:\n    for b in j:\n        #initialising the ANN\n        classifier = Sequential()\n\n        #adding the input layer and the first hidden layer\n        classifier.add(Dense(activation=\"relu\", input_dim=8, units=a, kernel_initializer=\"uniform\"))\n\n        #adding the second hidden layer\n        classifier.add(Dense(output_dim = b, init ='uniform', activation=\"relu\"))\n\n        #adding the output layer\n        classifier.add(Dense(output_dim = 1, init ='uniform', activation=\"sigmoid\"))\n\n        #compiling the ANN\n        classifier.compile(optimizer = \"adam\", loss =\"binary_crossentropy\" , metrics = ['accuracy'])\n\n        #fitting the ANN to the training set\n        classifier.fit(X_train,y_train , batch_size = 10 , nb_epoch = 100)\n        \n        y_pred = classifier.predict(X_test)\n\n        y_pred = (y_pred>0.5)\n        from sklearn.metrics import confusion_matrix\n        cm = confusion_matrix(y_test,y_pred)\n        accuracy=((cm[0,0]+cm[1,1])\/(cm[0,1]+cm[1,1]+cm[0,0]+cm[1,0]))\n        q.append(accuracy)\n        print(q)","3522ea07":"q","79973031":"## Handling Missing Data","93920c0b":"## **5 popular ways for data imputation for cross-sectional datasets:**\n\n**Do Nothing**:\n\nThat\u2019s an easy one. You just let the algorithm handle the missing data. Some algorithms can factor in the missing values and learn the best imputation values for the missing data based on the training loss reduction (ie. XGBoost)However, other algorithms will panic and throw an error complaining about the missing values (ie. Scikit learn \u2014 LinearRegression). In that case, you will need to handle the missing data and clean it before feeding it to the algorithm.\n\n**Imputation Using (Mean\/Median) Values**:\n\nThis works by calculating the mean\/median of the non-missing values in a column and then replacing the missing values within each column separately and independently from the others. It can only be used with numeric data.\n\nPros:\n  * Easy and fast.\n  * Works well with small numerical datasets.\n\nCons:\n  * Doesn\u2019t factor the correlations between features. It only works on the column level.\n  * Will give poor results on encoded categorical features (do NOT use it on categorical features).\n  * Not very accurate.\n  * Doesn\u2019t account for the uncertainty in the imputations\n\n**Imputation Using (Most Frequent) or (Zero\/Constant) Values**:\n\nMost Frequent is another statistical strategy to impute missing values and YES!! It works with categorical features (strings or numerical representations) by replacing missing data with the most frequent values within each column.\n\nPros:\n  * Works well with categorical features.\n\nCons:\n  * It also doesn\u2019t factor the correlations between features.\n  * It can introduce bias in the data.\n\n**Imputation Using Multivariate Imputation by Chained Equation (MICE)**:\n\nThis type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a single imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns.\n\n**Imputation Using Deep Learning (Datawig)**:\n\nThis method works very well with categorical and non-numerical features. It is a library that learns Machine Learning models using Deep Neural Networks to impute missing values in a dataframe. It also supports both CPU and GPU for training.\n\nPros:\n  * Quite accurate compared to other methods.\n  * It has some functions that can handle categorical data (Feature Encoder).\n  * It supports CPUs and GPUs.\n\nCons:\n  * Single Column imputation.\n  * Can be quite slow with large datasets.\n  * You have to specify the columns that contain information about the target column that will be imputed.","f7484c8b":"## Part 2- Data Visualization","9b6fd028":"Now,\n\nI'm going to separate my dependent varaible and the independent variables into different numpy array.","ce19ff69":"If you have reached till here, So i hope you liked my notebook.\n\nIf you learned anything new from this dataset then do give it a upvote.\n\nI'm a rookie and any suggestion in the comment box is highly appreciated.\n\nIf you have any doubt reagrding any part of the notebook, feel free to comment your doubt in the comment box.\n\nWhat are the ideal Number of nodes according to you for the neural network, comment it in the comment box.\n\nDo you have some specific way to decide the number of nodes for hidden layers , Do tell me in the comments.\n\nThank you!!","6ec39a2c":"## Introduction\n\n**Diabetes** is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy\n\nWithout ongoing, careful management, diabetes can lead to a buildup of sugars in the blood, which can increase the risk of dangerous complications, including stroke and heart disease.\n\nDifferent kinds of diabetes can occur, and managing the condition depends on the type. Not all forms of diabetes stem from a person being overweight or leading an inactive lifestyle. In fact, some are present from childhood.\n\n### Context\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n### Content\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n### Inspiration\nCan you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?","ecc99dcd":"## Spliting the dataset for Testing and Training\n\ntrain_test_split is a function in Sklearn model selection for splitting data arrays into two subsets: for training data and for testing data. With this function, you don't need to divide the dataset manually.\n\nBy default, Sklearn train_test_split will make random partitions for the two subsets. However, you can also specify a random state for the operation.","19d08c3f":"## Scaling Of Data\n\nDifferences in the scales across input variables may increase the difficulty of the problem being modeled. An example of this is that large input values (e.g. a spread of hundreds or thousands of units) can result in a model that learns large weight values. A model with large weight values is often unstable, meaning that it may suffer from poor performance during learning and sensitivity to input values resulting in higher generalization error.\n\n*One of the most common forms of pre-processing consists of a simple linear rescaling of the input variables.*","6fa6706c":"## Binning of Data\n\nData binning, bucketing is a data pre-processing method used to minimize the effects of small observation errors. The original data values are divided into small intervals known as bins and then they are replaced by a general value calculated for that bin. This has a smoothing effect on the input data and may also reduce the chances of overfitting in case of small datasets","c3e10c22":"Printing out the all the possibilities for different number of nodes of hidden layers.","46ce128d":"## Part 3- Lets make a ANN\n\n## Importing keras lib and its model","b934f6bb":"## Part 1 - Data Preprocessing\n\n## Importing Important Libraries","021211e5":"Even though we can see that there are no missing data in the dataset but if we look closer in this data, we find out that there are a lot of values as zero in features that make no sense. Like having zero blood pressure is not possible for someone who is alive.\n\nSo, we need to take care of this value. \n\n","29a47d53":"## Importing the Dataset"}}