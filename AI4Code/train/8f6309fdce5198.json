{"cell_type":{"944c1e09":"code","69a8a778":"code","031337b0":"code","a27b7557":"code","f09c0282":"code","80529a79":"code","f42020ba":"code","13acc65a":"code","fc812913":"code","431b4373":"code","f5fb29df":"code","f7c8130f":"code","432ed42e":"code","4281a79a":"code","5bd7f901":"code","ee69c7d0":"code","5327ee7f":"code","4c83e281":"markdown","dc5ff238":"markdown","c67d6640":"markdown"},"source":{"944c1e09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","69a8a778":"import math\nimport re\nfrom sklearn.model_selection import train_test_split","031337b0":"def tokenize(mail):                                                      # separating words\n        mail=mail.lower()                                                    #convert to lowercase\n        all_words= re.findall(\"[a-z0-9]+\",mail)  #Taking all the words\n        return set (all_words)\n    \ndef count_words(training_set):                                           # total count of each word in spam and non spam mails\n        \"\"\" pairs (mail, is_spam) \"\"\"                                        # It will return a dictionary with word as key and list of counts in spam and non spam mails as value.\n        counts={}\n        for index, row in training_set.iterrows():\n            for word in tokenize(row[0]):\n                if word in counts:\n                    counts[word][0 if row[1] else 1]+=1\n                else:\n                    counts[word]=[0,0]\n                    counts[word][0 if row[1] else 1]+=1\n        return counts\n    \ndef word_probabilities(counts, total_spams, total_non_spams, k=0.5):      # It will return a triplet with word and probabilities\n        word_triplet=[]\n        for word in counts:\n            List1=[]\n            List1.append(word)\n            List1.append((counts.get(word)[0]+k)\/(total_spams+2*k))\n            List1.append((counts.get(word)[1]+k)\/(total_non_spams+2*k))\n            word_triplet.append(List1)\n        return word_triplet\n    \ndef spam_probability(word_probs, mail):                                  # Here we will assign the probability to each mail\n        mail_words=[]\n        for x in mail:\n            mail_words += tokenize(x)\n        log_prob_if_spam = log_prob_if_not_spam = 0.0\n        \n        for word, prob_if_spam, prob_if_not_spam in word_probs:\n            if word in mail_words:\n#                 print(word, prob_if_spam, prob_if_not_spam)\n                log_prob_if_spam += math.log(prob_if_spam)\n                log_prob_if_not_spam += math.log(prob_if_not_spam)\n                \n        prob_if_spam = math.exp(log_prob_if_spam)\n        prob_if_not_spam = math.exp(log_prob_if_not_spam)\n        if (prob_if_spam + prob_if_not_spam == 0):\n            return 0\n        return prob_if_spam\/ (prob_if_spam + prob_if_not_spam)","a27b7557":"class NaiveBayesClassifier:\n    def _init_(self, k=0.5):\n        self.k= k\n        self.word_probs = []                                                #Constructor\n                      \n    def train(self, training_set):\n        num_spams = len([spam for spam in training_set.iloc[:,1] if spam])\n        num_non_spams = len(training_set) - num_spams\n                    \n        word_counts= count_words(training_set)\n        self.word_probs = word_probabilities(word_counts, num_spams, num_non_spams, 0.5)\n                    \n    def classify(self, mail):\n        return spam_probability(self.word_probs, mail)\n                     \n    ","f09c0282":"spam_df=pd.read_csv('..\/input\/spam-filter\/emails.csv')\nspam_df.head()                    ","80529a79":"x= spam_df[['text']]\ny=spam_df.spam","f42020ba":"xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.1, random_state = 0)","13acc65a":"xTrain['spam']=yTrain\nxTrain.head()","fc812913":"Naive=NaiveBayesClassifier()\n","431b4373":"Naive.train(xTrain)","f5fb29df":"submission=[]\nfor i, row in xTest.iterrows():\n    prob=Naive.classify(row)\n    \n    if prob>0.6:\n        submission.append(1)\n    else:\n        submission.append(0)","f7c8130f":"xTest['spam']=yTest\nxTest['submission']=submission\nprint(xTest)","432ed42e":"xTest['correct']=np.where((xTest['spam']==xTest['submission']), 1, 0)\nxTest.head()","4281a79a":"score=xTest['correct'].value_counts()","5bd7f901":"score","ee69c7d0":"Accuracy = (score[1]\/(score[1] + score[0]))","5327ee7f":"print(\"{} % of the data has been correctly predicted by our algorthm\".format(Accuracy * 100))","4c83e281":"Now that we have created our own algorithm, lets try it out on spam filter dataset","dc5ff238":"Lets now use bayes theroem to create the naive Bayes clasifier\nWe will be using the example of spam filter to classify a mail as spam or non spam.\n\nLets say S is the event that a mail is spam and D is the event that the mail contains the word \"Discount\". Now using Bayes theorem, we can find the probabilty of a mail being spam if it contains the word discount\n\np(S|D) = p(D|S) p(S)\/ {p(D|S) p(S) + p(D | not S) p(not S)}\n\nHere numerator is probability that mail is spam and contains Discount and denominator is just the probability that the mail contains Discount.\nNow if a mail being spam and not spam are equally likely events then\np(S) = p (not S) =0.5\n\nTherefore\np(S|D) = p(D|S) \/ {p(D|S) + p(D | not S)}\n\nso if 60% of the spam mails have the word Discount and only 2% of the non spam mails have the word discount in it then\np(S|D) = (0.60\/0.62)= 96.77%\n\n\n## More advance filter:\nNow imagine we have more words in our training dataset such as \"Offer\", \"CashBack\" etc. \nNaive Bayes theorem assumes that the presence or absence of words is independent of each other.\n\np(\"Offer\", \"Discount, \"CashBack\" \/ S) = p(\"Offer\"\/S) x p(\"Discount\" \/ S) x p(\"CashBack\" \/ S)\n\nFor now we will use only two words \"Discount\" and \"Cashback\" and can futher generalize it.\n\np(S|D) = p(D|S) \/ {p(D|S) + p(D | not S)}\np(S|C) = p(C|S) \/ {p(C|S) + p(C | not S)}\n\nNow we can multiply each probabilty on the write to get the final outcome.\nBut this multiplication will create very small values with many floating point numbers and can cause problem of underflow where computer won't deal with such small numbers and will consider them as zero.\n\nTo solve it we will use some algebra here, you might recall\nlog(ab) =log(a) +log(b) and exp(log(a)) = a\nTo make the probabilities floating point friendly, we can write it as \nexp(log(p1) + log(p2) + log(p3) + .....)\n\n## Little bit of Smoothing\n\nit might happen that a word in training data always comes in non spam mails for eg: \"easy\" therefore our algorithm will assign zero probability to that word. If that word occurs in spam mail in test data for eg: \" get an easy cashback\", the total probability will become zero and the spam will not get detected.\nTo solve this issue, will will set a dummy constant with some value while calcualting the probabilties so that no word has zero probability.\n\np(\"easy\"|S) = (k + number of spam mails containing \"easy\")\/(2k + nubmer of spam mails)\n\nNext we will implement the algorithm in python.\n","c67d6640":"Before starting to create the algorithm, its important to understand the Bayes theorem which is the base for this algorithm.\nThe most common concepts in probability are 'Dependence and Independence of events' and 'conditional Probabilty'.\nTwo events are independent if occuring of one event does not have any impact on the other event and is represented as \n\np(A,B)= p(A) x p(B).\n\nNow if the two events are not independent, then we can define the probability of event A and event B has already occured as :\n\np(A|B) = p(A,B)\/p(B)\nor\np(A,B)= p(A|B)p(B)\n\nHere if A and B are independent, then from the equation above:\np(A|B)=p(A)\n\nBayes Theorem:\nIf we knwo the probability of occuring A when B has already occured i.e. p(A|B) but we want to know the reverse, probability of occuring B when A has already occured i.e. p(B|A), in that case we will use Bayes theroem which states:\np(A|B)= p(A,B)\/p(B)=p(B|A)p(A)\/p(B)\n\nEvent B can be further split into two mutually exclusive events, \"B and A\" and \"B and not A\"\np(B)= p(B,A) + p(B, not A)\n\nso we can write-\n\np(A|B)=p(B|A)p(A)\/{p(B|A)p(A) + p(B | not A)p(not A)}  ----- and this is how bayes theroes is usually written\n"}}