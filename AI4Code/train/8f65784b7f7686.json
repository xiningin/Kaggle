{"cell_type":{"a097eab8":"code","7fff1a58":"code","48ff1f2a":"code","c912a85c":"code","7f9e9904":"code","88ee3976":"code","c7050f3a":"code","db5f0556":"code","1a3973fe":"code","eb16fd9c":"code","62218e4d":"code","a7a74c17":"code","9edb22c3":"code","1054f115":"code","e5551547":"code","acd195f1":"code","8a7bffa3":"markdown","0c2a6a14":"markdown","f5793c79":"markdown","6230bece":"markdown","8321ebd2":"markdown","1cf28c16":"markdown","81621751":"markdown","11ad672c":"markdown","88308811":"markdown","91c9b6aa":"markdown","96a32dbd":"markdown","75cb01a3":"markdown","58a4f332":"markdown","7ffebc1b":"markdown","753ff792":"markdown","940155fa":"markdown","0696e8d7":"markdown","8f0f0a67":"markdown","11bc8151":"markdown"},"source":{"a097eab8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/socofing\/SOCOFing'):\n    print(dirname, len(filenames))\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7fff1a58":"!nvidia-smi\n!nvcc --version\n!python --version","48ff1f2a":"#IMPORTANT: You might need to update these values based on the outputs of previous cells\n# Please refer to this link to learn how: https:\/\/github.com\/google\/jax#pip-installation\nimport os\n\nos.environ['JAX_CUDA_VERSION'] = 'cuda102'\nos.environ['JAX_BASE_URL'] = 'https:\/\/storage.googleapis.com\/jax-releases\/jax_releases.html'\n\n!pip install --upgrade jax jaxlib==0.1.56+$JAX_CUDA_VERSION -f $JAX_BASE_URL","c912a85c":"from jax.lib import xla_bridge\n\njax_platform = xla_bridge.get_backend().platform\n\nif jax_platform == \"cpu\":\n    print(\"No acceleration is found for JAX. It is not recommended to continue.\")\nelse:\n    print(\"JAX with {} acceleration is found, you may continue.\".format(jax_platform))","7f9e9904":"!pip install trax","88ee3976":"from os import listdir\nfrom os.path import isfile, join\nimport math\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nsrc_folder = \"\/kaggle\/input\/socofing\/SOCOFing\"\n\nreal_filenames = [f for f in listdir(join(src_folder, \"Real\"))\n                  if isfile(join(src_folder, \"Real\", f))]\ndigit_count = math.ceil(math.log10(len(real_filenames) + 1))\n\neasy_filenames = [f for f in listdir(join(src_folder, \"Altered\", \"Altered-Easy\"))\n                  if isfile(join(src_folder, \"Altered\", \"Altered-Easy\", f))]\nmedium_filenames = [f for f in listdir(join(src_folder, \"Altered\", \"Altered-Medium\"))\n                    if isfile(join(src_folder, \"Altered\", \"Altered-Medium\", f))]\nhard_filenames = [f for f in listdir(join(src_folder, \"Altered\", \"Altered-Hard\"))\n                  if isfile(join(src_folder, \"Altered\", \"Altered-Hard\", f))]\n\nprint(\"The very listed filename: {}\".format(real_filenames[0]))\n\nreal = matplotlib.image.imread(join(src_folder, \"Real\", \"1__M_Left_index_finger.BMP\"))\nalt_cr = matplotlib.image.imread(join(src_folder, \"Altered\", \"Altered-Hard\", \"1__M_Left_index_finger_CR.BMP\"))\nalt_obl = matplotlib.image.imread(join(src_folder, \"Altered\", \"Altered-Hard\", \"1__M_Left_index_finger_Obl.BMP\"))\nalt_zcut = matplotlib.image.imread(join(src_folder, \"Altered\", \"Altered-Hard\", \"1__M_Left_index_finger_Zcut.BMP\"))\n\nplt.figure()\nplt.figure(figsize=(24, 25))\nplt.subplot(1, 5, 1, facecolor='w')\nplt.imshow(real, cmap='gray')\nplt.subplot(1, 5, 2, facecolor='w')\nplt.imshow(alt_cr, cmap='gray')\nplt.subplot(1, 5, 3, facecolor='w')\nplt.imshow(alt_obl, cmap='gray')\nplt.subplot(1, 5, 4, facecolor='w')\nplt.imshow(alt_zcut, cmap='gray')","c7050f3a":"from collections import defaultdict\nimport cv2\n\n\n# Each finger (regardless of who it blongs to or which finger it is) will have an entry\n# in this dictionary. And each entry is a list while variations of the finger are entries\n# of the list. Something like: {\"1\": [\"\/path_to_the_file\/1__M_Left_index_finger.BMP\", ...]}\n# Basically, we are grouping all variations of the same fingerprint. One real\n# and the rest altered\nunique_sample = defaultdict(list)\n\nfor fn in real_filenames:\n    unique_sample[fn[:fn.rindex(\".\")]] += [join(src_folder, \"Real\", fn)]\nfor fn in easy_filenames:\n    unique_sample[fn[:fn.rindex(\"_\")]] += [join(src_folder, \"Altered\", \"Altered-Easy\", fn)]\nfor fn in medium_filenames:\n    unique_sample[fn[:fn.rindex(\"_\")]] += [join(src_folder, \"Altered\", \"Altered-Medium\", fn)]\nfor fn in hard_filenames:\n    unique_sample[fn[:fn.rindex(\"_\")]] += [join(src_folder, \"Altered\", \"Altered-Hard\", fn)]\n\n# There are some unwanted pixels around the input images, this function will get rid of them\ndef load_image(file_path):\n    # Load the image grayscale\n    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n    # Get rid of the excess pixels\n    img = img[2:-4, 2:-4]\n    # make sure all the images are of the same size (90 * 97)\n    img = cv2.resize(img, (90, 97))\n    return img\n\n# The number of samples per each group\ntraining_sample_count = 4000\nvalidation_sample_count = 1000\ntest_sample_count = 1000\n\n# \"0000_00\" -> \"\/...\/Real\/1__M_Left_index_finger.BMP\"\nid2path = {}\n# \"0000_00\" -> Mat\/np.array\ndataset_images = {}\n# \"0000\" -> [\"00\", \"01\", ...]\ntraining_ids = defaultdict(list)\n# \"4000\" -> [\"00\", \"01\", ...]\nvalidation_ids = defaultdict(list)\n# \"5000\" -> [\"00\", \"01\", ...]\ntest_ids = defaultdict(list)\n\nfor sample_id, sample in enumerate(unique_sample.items()):\n    for variation_id, f in enumerate(sample[1]):\n        # Polish and move the files into new folders. New filenames: {sample_id}_{variation}.png\n        zero_padded_sample_id = str(sample_id).zfill(digit_count)\n        zero_padded_variation_id = str(variation_id).zfill(2)\n        new_id =  \"{}_{}\".format(zero_padded_sample_id, zero_padded_variation_id)\n        id2path[new_id] = f\n        dataset_images[new_id] = load_image(f)\n\n        # Have the ids in three separate dicitonaries: sample_id -> [variation_id]\n        if sample_id < training_sample_count:\n            training_ids[zero_padded_sample_id] += [zero_padded_variation_id]\n        elif sample_id < training_sample_count + validation_sample_count:\n            validation_ids[zero_padded_sample_id] += [zero_padded_variation_id]\n        else:\n            test_ids[zero_padded_sample_id] += [zero_padded_variation_id]\n\nprint(\"There are {} samples in the training dataset\".format(sum([len(ids) for ids in training_ids.values()])))\nprint(\"There are {} samples in the validation dataset\".format(sum([len(ids) for ids in validation_ids.values()])))\nprint(\"There are {} samples in the test dataset\".format(sum([len(ids) for ids in test_ids.values()])))","db5f0556":"import numpy as np\nimport random as rnd\nimport cv2\n\n\neps = np.finfo(np.float32).eps\n\ndef data_generator(images, dataset, batch_size, image_transformer=None, shuffle=True):\n    \"\"\"Generator function that yields batches of images\n\n    Args:\n        folder (string): The path to the folder holder images\n        batch_size (int): Number of elements per batch.\n        image_transformer (function, optional): If provided is applied to each image before using it\n        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.\n    Yields:\n        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n        NOTE: input1: inputs to the model [img1a, img2a, img3a, ...]\n              input2: targets to the model [img1b, img2b, img3b, ...] i.e. (img1a, img1b) belong to the same fingerprint\n    \"\"\"\n\n    if len(dataset) < batch_size:\n        print(\"Batch size set to \" + str(len(dataset)) + \" as that's the total number of records\")\n        batch_size = len(dataset)\n\n    if shuffle:\n        indices = rnd.sample(list(dataset.keys()), len(dataset))\n    else:\n        indices = list(dataset.keys())\n\n    idx = 0\n    input1 = []\n    input2 = []\n\n    while True:\n        if idx >= len(indices):\n            idx = 0\n            if shuffle:\n                indices = rnd.sample(list(dataset.keys()), len(dataset))\n\n        variations = rnd.sample(dataset[indices[idx]], 2)\n        id1 = indices[idx] + \"_\" + variations[0]\n        id2 = indices[idx] + \"_\" + variations[1]\n        input1.append(id1)\n        input2.append(id2)\n        idx += 1\n\n        if len(input1) == batch_size:\n            b1 = []\n            b2 = []\n            if image_transformer is not None:\n                for f1, f2 in zip(input1, input2):\n                    b1.append(image_transformer(images[f1]))\n                    b2.append(image_transformer(images[f2]))\n            else:\n                for f1, f2 in zip(input1, input2):\n                    b1.append(images[f1])\n                    b2.append(images[f2])\n\n            yield np.array(b1), np.array(b2)\n            input1, input2 = [], []\n\n\ndef prepare_random_image(image, angle1=-45.0, angle2=45.0, scale1=.9, scale2=1.1, dx1=-10, dx2=10, dy1=-10, dy2=10):\n    angle = rnd.uniform(angle1, angle2)\n    scale = rnd.uniform(scale1, scale2)\n    dx = rnd.randint(dx1, dx2)\n    dy = rnd.randint(dy1, dy2)\n    \n    image_center = tuple(np.array(image.shape[1::-1]) \/ 2)\n    rot_mat = cv2.getRotationMatrix2D(image_center, angle, scale)\n    trnl_mat = np.float32([ [0, 0, dx], [0, 0, dy] ])\n    trns_mat = trnl_mat + rot_mat\n    result = cv2.warpAffine(image, trns_mat, image.shape[1::-1],\n                            flags=cv2.INTER_LINEAR,\n                            borderValue=(255, 255, 255))\n    return np.expand_dims(result, axis=-1).astype(np.float32)","1a3973fe":"import os\nimport trax\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.fastmath import numpy as fastnp\nimport numpy as np\nfrom functools import partial\n\n\ndef Siamese(d_model=128):\n    \"\"\"Returns a Siamese model.\n\n    Args:\n        d_model (int, optional): Size of the output encoding vectors. Defaults to 128.\n        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to 'train'.\n\n    Returns:\n        trax.layers.combinators.Parallel: A Siamese model \n        trax.layers.combinators.Serial: The encoder\n    \"\"\"\n\n    def normalize(x):  # normalizes the vectors to have L2 norm 1\n        return x \/ fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n\n    encoder = tl.Serial(\n        tl.Conv(16, (3, 3), padding='VALID'),\n        tl.MaxPool(pool_size=(3, 3), padding='VALID'),\n        tl.Conv(32, (3, 3), padding='VALID'),\n        tl.MaxPool(pool_size=(3, 3), padding='VALID'),\n        tl.Fn('Flatten', lambda x: fastnp.reshape(x, (x.shape[0], np.prod(x.shape[1:])))),\n        tl.Dense(d_model*2),\n        tl.Relu(),\n        tl.Dense(d_model*3),\n        tl.Relu(),\n        tl.Dense(d_model),\n        tl.Fn('Normalize', lambda x: normalize(x)),\n    )\n\n    model = tl.Parallel(encoder, encoder)\n    return model, encoder\n\n\ndef TripletLossFn(v1, v2, margin=0.25):\n    \"\"\"Custom Loss function, where the magic happens\n\n    Args:\n        v1 (numpy.ndarray): Array with dimension (batch_size, d_model)\n        v2 (numpy.ndarray): Array with dimension (batch_size, d_model)\n        margin (float, optional): Desired margin. Defaults to 0.25.\n\n    Returns:\n        jax.interpreters.xla.DeviceArray: Triplet Loss.\n    \"\"\"\n\n    scores = fastnp.dot(v1, v2.T)\n    batch_size = len(scores)\n    positive = fastnp.diagonal(scores)\n    negative_without_positive = scores - (fastnp.eye(batch_size) * 2.0)\n    closest_negative = negative_without_positive.max(axis=1)\n    negative_zero_on_duplicate = fastnp.multiply(scores, 1.0 - fastnp.eye(batch_size))\n    mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=1) \/ (batch_size - 1)\n    triplet_loss1 = fastnp.maximum(0.0, margin + closest_negative - positive)\n    triplet_loss2 = fastnp.maximum(0.0, margin + mean_negative - positive)\n    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n\n    return triplet_loss\n\n\ndef TripletLoss(margin=0.25):\n    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n    return tl.Fn('TripletLoss', triplet_loss_fn)\n\n\ndef accuracy(v1, v2):\n    scores = fastnp.dot(v1, v2.T)\n    m = fastnp.argmax(scores, axis=-1)\n    i = fastnp.arange(len(m))\n    mark_list = fastnp.where(i == m, 1, 0)\n    mark = fastnp.sum(mark_list)\n    return mark \/ len(mark_list)\n\n\ndef train_model(model, loss_fn, lr_schedule, train_generator, val_generator, train_steps, output_dir=None):\n    \"\"\"Training the Siamese Model\n\n    Args:\n        Siamese (function): Function that returns the Siamese model.\n        TripletLoss (function): Function that defines the TripletLoss loss function.\n        lr_schedule (function): Trax multifactor schedule function.\n        train_generator (generator, optional): Training generator. Defaults to train_generator.\n        val_generator (generator, optional): Validation generator. Defaults to val_generator.\n        output_dir (str, optional): Path to save model to. Defaults to 'model\/'.\n\n    Returns:\n        trax.supervised.training.Loop: Training loop for the model.\n    \"\"\"\n    if output_dir is not None:\n        output_dir = os.path.expanduser(output_dir)\n\n    train_task = training.TrainTask(\n        labeled_data=train_generator,\n        loss_layer=loss_fn(),\n        optimizer=trax.optimizers.Adam(0.01),\n        lr_schedule=lr_schedule,\n        n_steps_per_checkpoint=1000,\n    )\n\n    eval_acc_task = training.EvalTask(\n        labeled_data=val_generator,\n        metrics=[loss_fn(), tl.Fn('Accuracy', accuracy)],\n        metric_names=[\"Eval Loss\", \"Eval Acc\"],\n    )\n\n    training_loop = training.Loop(model,\n                                  train_task,\n                                  eval_tasks=[eval_acc_task],\n                                  output_dir=output_dir,\n                                  checkpoint_at=lambda n: n == train_steps) # Only savev the very last step\n\n    return training_loop","eb16fd9c":"batch_size = 768\ntrain_steps = 12000\n\ntrain_generator = data_generator(dataset_images, training_ids, batch_size, image_transformer=prepare_random_image)\nval_generator = data_generator(dataset_images, validation_ids, batch_size, image_transformer=prepare_random_image)\n\nmodel, encoder = Siamese()\n\nlr_schedule = trax.lr.warmup_and_rsqrt_decay(200, 0.0099)\n\ntraining_loop = train_model(model, TripletLoss, lr_schedule, train_generator, val_generator, train_steps)\ntraining_loop.run(train_steps)","62218e4d":"from os.path import basename\n\nstem_ids = [\"{}_{}\".format(sample_id, variation_id)\n            for sample_id, variation_list in test_ids.items()\n            for variation_id in variation_list\n            if \"\/Real\/\" in id2path[\"{}_{}\".format(sample_id, variation_id)]]\ntest_stem = np.stack([prepare_random_image(dataset_images[id])\n                      for id in stem_ids], axis=0)\n\nquery_ids = [\"{}_{}\".format(sample_id, variation_id)\n             for sample_id, variation_list in test_ids.items()\n             for variation_id in variation_list\n             if \"\/Real\/\" not in id2path[\"{}_{}\".format(sample_id, variation_id)]]\ntest_query = np.stack([prepare_random_image(dataset_images[id])\n                      for id in query_ids], axis=0)\n\n# It could be done in one go if Kaggle provided more memory\nchunk_size = 200\nstem_encoded = np.concatenate([encoder(test_stem[i:i + chunk_size, ...])\n                               for i in range(0, test_stem.shape[0], chunk_size)],\n                              axis=0)\nquery_encoded = np.concatenate([encoder(test_query[i:i + chunk_size, ...])\n                                for i in range(0, test_query.shape[0], chunk_size)],\n                               axis=0)\nsimilarity = fastnp.dot(query_encoded, stem_encoded.T)\nmatched = fastnp.argmax(similarity, axis=-1)","a7a74c17":"def plot_result(name, input2include):\n    query_ids2 = [id[:digit_count] for id in query_ids]\n    stem_ids2 = [id[:digit_count] for id in stem_ids]\n\n    results = [1 if query_ids2[input] == stem_ids2[output] else 0\n               for (input, output) in enumerate(matched)\n               if input in input2include]\n    correct_answers = sum(results)\n    total_tests = len(results)\n    acc = correct_answers \/ total_tests * 100.0\n    del results\n\n    print()\n    print(\"Got {} right out of {}, that's {:.2f}% accuracy\".format(correct_answers, total_tests, acc))\n    print()\n\n    scores = similarity.tolist()\n    plt.figure(figsize=(25, 8))\n\n    plt.subplot(1, 3, 1, facecolor='w')\n    correctly_matched_scores = [scores[input][output]\n                                for (input, output) in enumerate(matched)\n                                if query_ids2[input] == stem_ids2[output]\n                                and input in input2include]\n    plt.hist(correctly_matched_scores, bins=100, range=(0.0, 1.0), label=\"Correctly matched\")\n    plt.xlabel('Similarity')\n    plt.ylabel('Number of matched')\n    plt.title(\"Distribution of correctly matched for {} fingerprints (Acc: {:.2f}%)\"\n              .format(name, acc))\n    del correctly_matched_scores\n\n    plt.subplot(1, 3, 2, facecolor='w')\n    incorrectly_matched_scores = [scores[input][output]\n                                  for (input, output) in enumerate(matched)\n                                  if query_ids2[input] != stem_ids2[output]\n                                  and input in input2include]\n    plt.hist(incorrectly_matched_scores, bins=100, range=(0.0, 1.0), label=\"Incorrectly matched\")\n    plt.xlabel('Similarity')\n    plt.ylabel('Number of matched')\n    plt.title(\"Distribution of incorrectly matched for {} fingerprints (Acc: {:.2f}%)\"\n              .format(name, acc))\n\n    scores = similarity[list(input2include), :].reshape((-1,)).tolist()\n    plt.subplot(1, 3, 3, facecolor='w')\n    plt.hist(scores, bins=200, range=(-1.0, 1.0), label=\"Correctly not matched\")\n    plt.xlabel('Similarity')\n    plt.ylabel('Number of matched')\n    plt.title(\"Distribution of all similarities calculated for {} fingerprints (Acc: {:.2f}%)\"\n              .format(name, acc))\n\n    plt.show()","9edb22c3":"print(\"=======================================================\")\nprint(\"= Not considering the difficulty level of the samples =\")\nprint(\"=======================================================\")\n\nplot_result(\"all\", set(range(len(matched))))","1054f115":"print(\"=====================================\")\nprint(\"= Only considering the easy samples =\")\nprint(\"=====================================\")\n\neasy_indices = [i\n                for i, id in enumerate(query_ids)\n                if \"\/Altered-Easy\/\" in id2path[id]]\nplot_result(\"easy\", set(easy_indices))","e5551547":"print(\"=======================================\")\nprint(\"= Only considering the medium samples =\")\nprint(\"=======================================\")\n\neasy_indices = [i\n                for i, id in enumerate(query_ids)\n                if \"\/Altered-Medium\/\" in id2path[id]]\nplot_result(\"medium\", set(easy_indices))","acd195f1":"print(\"=====================================\")\nprint(\"= Only considering the hard samples =\")\nprint(\"=====================================\")\n\neasy_indices = [i\n                for i, id in enumerate(query_ids)\n                if \"\/Altered-Hard\/\" in id2path[id]]\nplot_result(\"hard\", set(easy_indices))","8a7bffa3":"The version of the avaiable Cuda should be taken from the output above and put in the following cell.\n\nPlease refer to the [JAX's Github page](https:\/\/github.com\/google\/jax#pip-installation) if you need to make any changes.","0c2a6a14":"In order to better understand the quality of the answers, there are three histograms plotted as well.\n\n1. On the left: the distribution of the similarities for correctly matched fingerprints.\n1. On the middle: the distribution of the similarities for incorrectly matched fingerprints.\n1. On the right: the distribution of the similarities for all the fingerprints.","f5793c79":"### Histograms for all the test dataset (regardless of their difficulty level)","6230bece":"Finally, installing Trax.","8321ebd2":"## 3. Loading the dataset\n\nFirst let's start by listing all the files. Just for the sake of better understanding what we are dealing here with, let's plot one sample and three of its alterations. You might notice that each image has two extra pixels on the top and left and four extra pixels on the right and bottom.","1cf28c16":"Defining the data generator and an image transformer as data augmentation","81621751":"### Histograms for the easy subset of test dataset","11ad672c":"## 7. Conclusion\n\nA Siamese model was used here to encode the fingerprints into vectors in a way that the dot product of the encoded vectors represents their similarity. The trained model managed to match 8188 altered fingerprints to their 1000 original version with 98% accuracy.\n\nWhile the altered version of the fingerprints in the dataset were split into three difficulty levels, they were the same to the model as it managed to match them with the same accuracy.","88308811":"### 2. Preparing to install Trax\n\nThe model will be implemented using Trax. Since Trax is not provided by Kaggle, it needs to be installed manually. If for you it's already installed, you can skip to step 3.\n\nTrax uses JAX for leveraging the GPU (or TPU) so we need to install JAX as well. FOr that, we'll need to know the version of the Cuda available first.","91c9b6aa":"Just checking to make sure the GPU is accessible","96a32dbd":"# Siamese model for fingerprint identification \/ matching\n\nThis notebook will train a Siamese model to encode a grayscale image of a fingerprint (90*97 pixels) into a vector of size 128. The Siamese model will be trained in a way that the dot product of two such vectors will return the similarity of the corresponding fingerprints.\n\nThe trained model managed to match 8188 test fingerprints (never been seen while training) to 1000 unique test fingerprints with roughly **98% accuracy**.\n\nThe dataset consists of fingerprints for 600 people, all their ten fingers. That results in 6000 unique fingerprints each of which is considered a separate sample here. There are also altered (synthetic) versions of the same fingerprints grouped into 3 levels of difficulties (according to the provider of the dataset), easy, medium, and hard. As it will be shown, a Siamese model requires variations of the same sample and the afforementioned difficulty levels are used for that purpose. Also, some random image transformation is applied to images to improve the robustness of the trained model as an augmentation technique. These random trasformations are also applied to the test images. Without this last part, it would have been too easy of a problem to solve. Basically, if you run the test evaluation multiple times on the same trained model, you'll end up with different results because of the random factor applied to the test dataset.","75cb01a3":"## 4. Defining the Siamese model\n\nA Siamese model, as the name suggests, is composed of two or more towers of layers. Since here the input is of tpye image, CNN is the best choice for the towers. Here, we'll have one tower paralleled twice. The important part of the tower is the very last \"Normalize\" layer which will make sure the output vectors are of L2 norm equal to 1. The importance of this will be revealed once you understand the loss function.\n\nThe loss function used here is a [triplet loss function](https:\/\/en.wikipedia.org\/wiki\/Triplet_loss#:~:text=Triplet%20loss%20is%20a%20loss,a%20negative%20(falsy)%20input.). The loss function takes in two batches of vectors, v1 and v2. This because the model is a Parallel. The general idea of the loss function is to calculate the similarity of its two inputs and reward\/punish pairs of the vectors in v1 and v2 based on the fact whether they are similar or not. The similarity measure used here is [Cosine Similarity](https:\/\/en.wikipedia.org\/wiki\/Cosine_similarity) which is an angualr similarity and it could be calculated simply using a dot product as long as the vectors are of L2 norm of 1. That's why there's a normalization layer at the very end of the Siamese's tower.\n\nOnce trained, the tower can be used as an encoder that encodes the input images into vectors of 128. And thanks to the triplet loss function and the normalization layer, the similarity of such encoded vectors can easily be calculated by a dot product.","58a4f332":"## 1. Let's start\n\nThe following block is a boilerplate cell provided by Kaggle and it will simply count the number of files in each folder. It is worth mentioning that there are different alterations applied to each sample but since the result should have been a valid fingerprint, not all samples were a good candidate to receive all the alterations. That's why the number of files in each folder seems random.\n\nPlease refer to the [dataset's paper](https:\/\/arxiv.org\/abs\/1807.10609) for the complete description.","7ffebc1b":"## 8. Credits\n\nI must admit that the code used here is mostly borrowed from the [Coursera's NLP Specialization](https:\/\/www.coursera.org\/specializations\/natural-language-processing). The main difference is that back in that course, the Siamese model was used on an NLP problem. I just wanted to see how it behaves on an image related problem. And I'm sure you'll agree with me that it worked pretty well.","753ff792":"### Histograms for the medium subset of test dataset","940155fa":"## 6. Evaluate the model\n\nIn order to evaluate the model, the test dataset is split into two sets, the real fingerprints and their alterations. The process is like this:\n\n1. Encode the real fingerprints into vectors. There are exactly 1000 real fingerprints in the test dataset.\n1. Encode the altered versions into vectors as well. There are 8188 altered samples in the test dataset.\n1. Calculate the dot product between the encoded version of the real fingerprints and the altered. This results is a matrix of size (8188, 1000) showing the similarity between each altered sample and all the real versions.\n1. The most similar (largest number) per each row (A.K.A. altered sample) should be the column representing the real fingerprint. Basically, using the similarity matrix, we should be able to tell the real fingerprint of an altered one (that is, if the model works).\n1. The accuracy is calcualted by counting the correctly found real fignerprints per total number of altered ones.\n\nAs mentioned before, the random image transformation is also applied on the test dataset. If this was not done, it would be a much simpler propblem and the accuracy will be higher.","0696e8d7":"### Histograms for the hard subset of test dataset","8f0f0a67":"Next, load the images into memory using OpenCV. I used OpenCV for its simplicity and also later I'll use OpenCV to implement a random transformation playing the role of augmentation. As mentioned before there are some extra pixels in all of the images that are removed. Also, some of the files are not the same size (number of pixels) so a resize will make sure they all will end up exactly the same.\n\nOut of the 6000 real fingerprints, we'll use 4000 for training, 1000 for validation, and 1000 for testing. The altered version of each fingerprint will be used along its original version. So an alteration of a training will also be used for training and an altered version of the test will also be used for testing.\n\nThere are also a bunch of dictionaries populated which will come handy later on.","11bc8151":"## 5. Training the model\n\nBefore training the model, it's worth mentioning the importance of the batch size in this model. Going back to the definition of the triplet loss function, the loss value calculated for each batch is a function of a matrix of size (batch_size, batch_size). This means the bigger the batch size, the more accurate the loss value. Long story short, you should have the batch size as large as your hardware supports. While this improves the training time, it will have a very significant impact on the quality of the trained model as well.\n\nThe training is set to go on for 12000 steps. This should take an hour on Kaggle to complete."}}