{"cell_type":{"ec6c77ab":"code","3a1d9073":"code","ec9758a0":"code","012ecdd4":"code","33070bdf":"code","0125149a":"code","b8a880f6":"code","806964f5":"code","69d58b2f":"code","62e229c1":"code","58d4d083":"code","995a4674":"code","26d7ec51":"code","c107c7e9":"code","80c33c27":"code","9a866166":"code","74df0caf":"code","2be4c734":"code","6c541eff":"code","ac6e7873":"code","34092477":"code","edb70e4d":"code","fae37d5c":"code","1847f852":"code","fda5195b":"code","27ef8e3a":"code","abbd14dd":"code","8b27b3ca":"code","dfd3185a":"code","39077cd1":"code","715e2ecb":"code","10ba68da":"code","9e190fb0":"code","1ed78ecb":"code","3dee5e6e":"code","62661717":"code","64704edd":"code","fec486d2":"code","b72084dd":"code","5b50571f":"markdown","95eeecd1":"markdown","e0278348":"markdown","65353ffe":"markdown","7d85053d":"markdown","c93ff213":"markdown","9e913842":"markdown","8a830394":"markdown","9c94eb53":"markdown","80592329":"markdown","df22b379":"markdown","49cd3fb7":"markdown","58a06250":"markdown","a2b33c4d":"markdown","587e2f2c":"markdown","be9917f4":"markdown","03cb51a5":"markdown","5fe6c9cf":"markdown","f999caa7":"markdown","2746c003":"markdown","238b81d6":"markdown","17b836b8":"markdown","33195fb6":"markdown","aa37571e":"markdown","cc1dfa6f":"markdown","cddd0403":"markdown"},"source":{"ec6c77ab":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport pandas as pd\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import preprocessing","3a1d9073":"teste = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\")\ntreino = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\")\nclear_teste = teste.dropna()\nclear_treino = treino.dropna()","ec9758a0":"clear_treino.head()","012ecdd4":"clear_treino[\"income\"] = clear_treino[\"income\"].map({\"<=50K\": 0, \">50K\":1})\nclear_treino[\"sex\"] = clear_treino[\"sex\"].map({\"Male\": 0, \"Female\":1})","33070bdf":"clear_treino.head()","0125149a":"sns.heatmap(clear_treino.corr(), annot=True, vmin=-1, vmax=1)","b8a880f6":"sns.lineplot('education.num', 'income', data=clear_treino)","806964f5":"sns.lineplot('hours.per.week', 'income', data=clear_treino)","69d58b2f":"sns.pairplot(clear_treino, hue='income')","62e229c1":"names = ['age','fnlwgt','education.num','capital.gain','capital.loss','hours.per.week']\n# Get column names first\n# Create the Scaler object\nscaler = preprocessing.StandardScaler()\n# Fit your data on the scaler object\nx = clear_treino.loc[:, names].values\nx = scaler.fit_transform(x)\nscaled_df = pd.DataFrame(x, columns=names)\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(scaled_df)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\ndata_treino_principal = pd.concat([principalDf, clear_treino[['income']]], axis = 1)\ny = clear_treino.income","58d4d083":"y","995a4674":"sns.pairplot(data_treino_principal, hue='income')","26d7ec51":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.25)","c107c7e9":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","80c33c27":"log_pred = logmodel.predict(X_test)","9a866166":"log_pred","74df0caf":"print(\"Logistic Regression Results\")\nprint(confusion_matrix(y_test, log_pred))\nprint(classification_report(y_test, log_pred))\nprint('Accuracy: ',accuracy_score(y_test, log_pred))","2be4c734":"from sklearn.svm import SVC\nsvm = SVC()\nsvm.fit(X_train,y_train)","6c541eff":"svm_pred = svm.predict(X_test)","ac6e7873":"print(\"SVM metrics\")\nprint(confusion_matrix(y_test, svm_pred))\nprint(classification_report(y_test, svm_pred))\nprint('Accuracy: ',accuracy_score(y_test, svm_pred))","34092477":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(random_state=0, max_depth=5)\ndecision_tree = decision_tree.fit(X_train, y_train)","edb70e4d":"decision_tree_pred= decision_tree.predict(X_test)","fae37d5c":"print('Accuracy: ',accuracy_score(y_test, decision_tree_pred))","1847f852":"from sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score\n\n\ntrainDS = pd.read_csv(\"..\/input\/dataci\/train.csv\", names=[\"Id\",\"latitude\",\"longitude\",\"median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"], sep=r'\\s*,\\s*',engine='python',na_values='?', skiprows=1)\ntrainDS_X = trainDS[[\"Id\",\"latitude\",\"longitude\",\"median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\"]]\ntrainDS_Y = trainDS[[\"median_house_value\"]]\n\n\ntestDS_X = pd.read_csv(\"..\/input\/dataci\/test.csv\", names=[\"Id\",\"latitude\",\"longitude\",\"median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\"], sep=r'\\s*,\\s*',engine='python',na_values='?', skiprows=1)\n\n\n","fda5195b":"plt.scatter(trainDS[\"latitude\"],trainDS[\"median_house_value\"])","27ef8e3a":"regr = linear_model.LinearRegression()\nregr.fit(trainDS_X,trainDS_Y)\ntestDS_Y = regr.predict(testDS_X)","abbd14dd":"print('Coefficients: \\n', regr.coef_)\n\nprint('Interception: \\n', regr.intercept_)","8b27b3ca":"scores = cross_val_score(regr, trainDS_X, trainDS_Y, cv=10)\nscores","dfd3185a":"regRCV = linear_model.RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0, 100], cv=5)\nregRCV.fit(trainDS_X,trainDS_Y)","39077cd1":"regRCV.alpha_","715e2ecb":"regRCV = linear_model.RidgeCV(alphas=[6, 8, 10, 12, 14], cv=5)\nregRCV.fit(trainDS_X,trainDS_Y)\nregRCV.alpha_","10ba68da":"regR = linear_model.Ridge(alpha = 10)","9e190fb0":"regR.fit(trainDS_X,trainDS_Y)","1ed78ecb":"testDS_Y = regR.predict(testDS_X)\nprint('Coefficients: \\n', regR.coef_)\nprint('Interception: \\n', regR.intercept_)\nscoresRidge = cross_val_score(regR, trainDS_X, trainDS_Y, cv=10)\nscoresRidge\n","3dee5e6e":"regLCV = linear_model.LassoCV(alphas=[0.1, 1, 10, 20, 100], cv=5)\n\nregLCV.fit(trainDS_X,trainDS_Y[\"median_house_value\"])\nregLCV.alpha_\n","62661717":"regLCV = linear_model.LassoCV(alphas=[15, 18, 20, 22, 24], cv=5)\n\n\n\nregLCV.fit(trainDS_X,trainDS_Y[\"median_house_value\"])\n\n\nregLCV.alpha_","64704edd":"regL = linear_model.Lasso(alpha=22)\n\n\nregL.fit(trainDS_X,trainDS_Y)\n\n\ntestDS_Y = regL.predict(testDS_X)\n\n\nprint('Coefficients: \\n', regL.coef_)\nprint('Interception: \\n', regL.intercept_)\nscoresLasso = cross_val_score(regL, trainDS_X, trainDS_Y, cv=10)\nscoresLasso\n\n","fec486d2":"testDS_Predict = pd.DataFrame()\n\ntestDS_Predict[\"Id\"] = testDS_X[\"Id\"]\ntestDS_Predict[\"median_house_value\"] = testDS_Y\ntestDS_Predict.head()","b72084dd":"\n\ntestDS_Predict.to_csv(\"testDS_3.csv\", index=False)\n\n","5b50571f":"**4.3 Decision Tree**","95eeecd1":"**3 Verifica\u00e7\u00e3o da rela\u00e7\u00e3o entre features na base de dados**<br><br>Os resultados dessa etapa ser\u00e3o importantes para vislumbrar quais m\u00e9todos se adequar\u00e3o melhor aos dados","e0278348":"**3.2 Gr\u00e1fico do sal\u00e1rio em fun\u00e7\u00e3o das horas trabalhadas por semana **","65353ffe":"**3.1 Gr\u00e1fico do sal\u00e1rio em fun\u00e7\u00e3o da educa\u00e7\u00e3o**","7d85053d":"Correla\u00e7\u00e3o entre sal\u00e1rio e educa\u00e7\u00e3o confirmada","c93ff213":"**4.2 Regress\u00e3o Log\u00edstica**","9e913842":"Podemos identificar que a base fornecida n\u00e3o possui o target para seja poss\u00edvel validar o modelo. Dessa forma, ser\u00e1 necess\u00e1rio utilizar a mesma base, mas subdividida em teste e em treino.\n<br><br>\n25% do dataframe ser\u00e1 utilizados para teste","8a830394":"Regress\u00e3o Linear","9c94eb53":"A correla\u00e7\u00e3o entre as duas features existe, contudo n\u00e3o \u00e9 tao clara","80592329":"**3.1 Visualiza\u00e7\u00e3o da correla\u00e7\u00e3o entre features**<br><br>Ser\u00e1 utilizado o heat map para esta visualiza\u00e7\u00e3o","df22b379":"**4. M\u00e9todos de aprendizagem de m\u00e1quina**\n<br><br>\nUtilizaremos Logistic Regression, Decision Tree e SVM","49cd3fb7":"Nosso hiperpar\u00e2metro deve ser 10","58a06250":"SVM\n<br><br>\nA acur\u00e1cia obtida foi de 82.29%","a2b33c4d":"**PMR3508-2-2019-84**<br><br>Autor: Thomaz Pougy<br><br>O desempenho de outros classificadores na base adult","587e2f2c":"Decision Tree\n<br><br>\nA acur\u00e1cia obtida foi de 82.83%","be9917f4":"Lasso","03cb51a5":"Regress\u00e3o Log\u00edstica\n<br><br>\nA acur\u00e1cia obtida foi de 81.09%","5fe6c9cf":"**2. Importa\u00e7\u00e3o dos arquivos**","f999caa7":"**4.2 SVM**","2746c003":"**4.1 Prepara\u00e7\u00e3o da base de dados **","238b81d6":"Para reduzir o Overfitting, vamos realizar o Ridge","17b836b8":"**1. Bibliotecas necess\u00e1rias**","33195fb6":"**2.1 Transforma\u00e7\u00e3o de dados categ\u00f3ricos para booleanos**<br><br>As features sex e income possuem cada uma duas categorias, dessa forma, vamos transform\u00e1-las em booleanos","aa37571e":"## Parte Extra","cc1dfa6f":"**5. Conclus\u00f5es**\n<br><br>\nPodemos observar que todos os m\u00e9todos de aprendizagem de m\u00e1quina utilizados obtiveram acur\u00e1cia acima de 80%, o que representa um bom resultado no geral. Na an\u00e1lise mais individualizada para cada m\u00e9todo podemos destacar a \u00e1rvore de decis\u00e3o, a qual obteve a maior acur\u00e1cia (+1.83% em rela\u00e7\u00e3o \u00e0 regress\u00e3o log\u00edstica e +0.54% em rela\u00e7\u00e3o ao SVM) e tamb\u00e9m apresentou o menor tempo de processamento.<br><br>\nCom rela\u00e7\u00e3o ao tempo de processamento, destaque negativo para o SVM que obteve uma acur\u00e1cia intermedi\u00e1ria, mas apresentou o maior tempo de processamento, muito maior que os outros dois m\u00e9todos testados.","cddd0403":"\u00c9 poss\u00edvel notar que tanto educa\u00e7\u00e3o quanto horas de trabalho por semana s\u00e3o apontados como forte relacionados \u00e0 feature sal\u00e1rio, contudo vale a pena fazermos alguns gr\u00e1ficos para confirmar"}}