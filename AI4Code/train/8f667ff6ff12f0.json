{"cell_type":{"f79c4728":"code","95ef0cbe":"code","b3d04cc6":"code","7a4ffb05":"code","096097e5":"code","8b752f7f":"code","26f4f66b":"code","0ee6eff3":"code","30e51b00":"code","55ad3f9e":"code","2d13d0f2":"code","0d1ef184":"code","c2f61fde":"code","7f00d3f5":"code","ab2637f6":"code","3c79d4c0":"code","cc5fcbd5":"markdown"},"source":{"f79c4728":"#Import starter packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","95ef0cbe":"#Import train and test sets\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","b3d04cc6":"#Combine into one since we are building random forests and gradient boosted models so we want\n#to capture all levels of the features that could be present\ntrain['test_train'] = 'train'\ntest['test_train'] = 'test'\n\ntrain_test = pd.concat([train, test]).reset_index()","7a4ffb05":"#Basic Feature Engineering\ntrain_test['have_pool'] = train_test.apply(lambda row: 1 if row['PoolArea'] > 0 else 0, axis = 1)\ntrain_test['have_garage'] = train_test.apply(lambda row: 0 if np.isnan(row['GarageYrBlt']) else 1, axis = 1)\ntrain_test['built_after1999'] = train_test.apply(lambda row: 0 if row['YearBuilt'] < 2000 else 1, axis = 1)\ntrain_test['remod_after2004'] = train_test.apply(lambda row: 0 if row['YearRemodAdd'] < 2005 else 1, axis = 1)\n","096097e5":"#Strip out the response for y_train and drop from predictors\n\ny_train = np.log(train.SalePrice)\n\ntrain_test = train_test.drop('SalePrice', axis = 1)\n\n#For float64 type, impute values\n\n#Put missing for NA's in strings\nfor i in train_test.columns:\n    if(train_test[i].dtypes == 'object'):\n        train_test[i] = train_test[i].fillna('missing')\n              \n#Put -999999999 for NA's in numeric\nfor i in train_test.columns:\n    if(train_test[i].dtypes != 'object'):\n        train_test[i] = train_test[i].fillna(-9999999999)","8b752f7f":"#Create dummy variables to feed into models\ntrain_test_dummies = pd.get_dummies(train_test.select_dtypes(include = 'object'))","26f4f66b":"#Clean data for training and testing\ntrain_test_numeric = train_test.select_dtypes(include = [\"int64\", \"float64\"])\nX_train_test = pd.concat([train_test_numeric, train_test_dummies], axis = 1)\nX_train = X_train_test[X_train_test.test_train_train == 1]\nX_test = X_train_test[X_train_test.test_train_test == 1]\nX_train = X_train.drop([\"test_train_train\", 'test_train_train'], axis = 1)\nX_test = X_test.drop([\"test_train_train\", 'test_train_train'], axis = 1)\n\n#Ensure shape is as expected\nprint(X_train.shape)\nprint(X_test.shape)","0ee6eff3":"#Grid search to find optimal hyperparameters for xgboost model\n#from sklearn.model_selection import GridSearchCV\n\n#model_params = {'learning_rate': [0.01],\n                #'n_estimators': [1000, 2000, 3000],\n                #'max_depth' : [3],\n                #'subsample' : [0.8]}\n\n#grid = GridSearchCV(xgb_reg, param_grid = model_params, scoring = 'neg_mean_squared_error', cv = 10)\n\n#grid.fit(X_train, y_train)\n","30e51b00":"#Instantiate xgboost model with best hyperparameters found from grid search\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\ngb_final = XGBRegressor(learning_rate = 0.01,\n                                     n_estimators = 2000,\n                                     max_depth = 3,\n                                     subsample = 0.8,\n                                     random_state = 777)\n\n#Cross Validate Score: Try to beat 0.0152508 \nscore_gb = cross_val_score(gb_final, X_train, y_train, scoring = 'neg_mean_squared_error', cv = 5)\nprint(score_gb.mean() * -1 ** (1\/2))\n","55ad3f9e":"gb_final.fit(X_train, y_train)","2d13d0f2":"#Look at how rmse varies when adding variables by feature importance for xgboost model\nxgb_vars_ordered = list(gb_final.get_booster().get_score(importance_type = 'gain').keys())\n#scores_xgb = []\n#vars_to_use = xgb_vars_ordered[0:150]\n#for i in xgb_vars_ordered[150:171]:\n    #vars_to_use.append(i)\n    #xgb_cross_val_score = cross_val_score(gb_final, X_train[vars_to_use], y_train, scoring = 'neg_mean_squared_error', cv = 5)\n    #scores_xgb.append(xgb_cross_val_score.mean() * -1 ** (1\/2))\n\n#xgb_df_score = pd.DataFrame({'var': vars_to_use[-len(scores_xgb):], 'score':scores_xgb})\n\n#print(xgb_df_score)","0d1ef184":"#Vars obtained from Feature Selection\nvars_to_use = xgb_vars_ordered[0:161]","c2f61fde":"#Plug vars to use in model\nxgb_final_final = gb_final.fit(X_train[vars_to_use], y_train)","7f00d3f5":"#Visual of predictions versus actuals on training set\nimport matplotlib.pyplot as plt\npred_y = gb_final.predict(X_train[vars_to_use])\n\nplt.scatter(pred_y, y_train)\nplt.show()","ab2637f6":"#Prepare test scores\nfinal_pred_y = gb_final.predict(X_test[vars_to_use])\n\nsubmission = pd.DataFrame({'ID': test.Id, 'Saleprice': final_pred_y})\n\nsubmission.Saleprice = np.exp(submission.Saleprice)\n\nprint(submission.head())","3c79d4c0":"#Write final scores to csv\nsubmission.to_csv('xgbsubmission.csv', index = False)","cc5fcbd5":"House prices are in the air! As someone who just went through the home buying process, this challenge caught my eye. Even before meeting with our agent to do showings, there was SO much to consider when buying a house. Words cannot express how refreshing it was to see a challenge where we try and build models to help.\n\nAs much as I like to think that I am now some sort of house pricing expert, I am far from that. This challenge made me realize that what I think is important in a house is not actually what influences its price (at least when the model is already considering so many other factors). When doing feature engineering, some of my \"this is the best feature ever\" moments turned into huge disappointments. It was definitely a good learning experience balancing ingenuity with intuition. Moreover, I have not looked at any notebooks with this dataset yet. I know if I did, there are some pretty smart people that have probably figured out the \"silver bullet\" to getting a top spot on the leaderboard.\n\nI hope everyone enjoys this kernel as much as I loved making it. It might not have the prettiest of graphs, but I have been spending a lot of time learning Python and this was a great step to prove to myself that I can build solid models in it (if you are not familiar with my previous Kaggle work, I lot of it has been in R)."}}