{"cell_type":{"f634e9ed":"code","49769f1f":"code","1b4016d1":"code","4b5bf537":"code","9bb4c5ae":"code","71934ad7":"code","9379ffa4":"code","ab774671":"code","f8d56aa0":"code","c0476c66":"code","ff20cb46":"markdown","d75b1e42":"markdown","40a77ab5":"markdown","19c485d5":"markdown","e9a89243":"markdown","dc26fe50":"markdown","1824efc0":"markdown","7ee41a09":"markdown","a23f6d53":"markdown","89acf968":"markdown","8891a51f":"markdown"},"source":{"f634e9ed":"# import des biblioth\u00e8ques utilis\u00e9es\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import uniform, randint\n\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDRegressor, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import *\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\n\nfrom xgboost import XGBRegressor\n","49769f1f":"# chargement des donn\u00e9es \ndata = pd.read_csv(\"\/kaggle\/input\/refined-data\/data_hard_refined.csv\")\n\n# essai sans le ENERGYSTARScore\n# data = data.drop(['ENERGYSTARScore'], axis=1)\n\nobjectColumns = list(data.dtypes[data.dtypes == object].index)\nnumericColumns = list(data.dtypes[data.dtypes != object].index)\ndata = data.drop(['PropertyName', 'Address','OSEBuildingID', 'TaxParcelIdentificationNumber'], axis=1)\nobjectColumns = list(data.dtypes[data.dtypes == object].index)\nnumericColumns = list(data.dtypes[data.dtypes != object].index)\ny_columns = ['TotalGHGEmissions', 'SiteEnergyUse(kBtu)']\nX = data.drop(y_columns, axis=1)\ny = data[y_columns]\nfor i in y_columns:\n    numericColumns.remove(i)","1b4016d1":"# standardiser les donn\u00e9es\npreprocessor = make_column_transformer((RobustScaler(),numericColumns),(OneHotEncoder(handle_unknown = 'ignore'),objectColumns))\nX_train, X_test, y_train, y_test = train_test_split(X, data['TotalGHGEmissions'], test_size=0.2)\n\nresults = []","4b5bf537":"model = make_pipeline(preprocessor,LinearRegression())\nparameters = {\n    'linearregression__fit_intercept':[True,False], \n    'linearregression__normalize':[True,False], \n    'linearregression__copy_X':[True, False]\n}\ngrid = GridSearchCV(model,parameters, cv=5, n_jobs = 5)\ngrid.fit(X_train, y_train)\nprint('M\u00e9thode: LinearRegression OneHotEncoder RobustScaler')\nprint (\"best score: \", grid.best_score_)\nprint(\"best params: \",grid.best_params_)\ny_pred = grid.predict(X_test)\nprint(\"score de la pr\u00e9diction:\")#, accuracy_score(y_test, y_pred)), \nprint(\"RMSE = \",mean_absolute_error(y_test,y_pred))\nprint(\"MAE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"median abs err = \",median_absolute_error(y_test,y_pred))\n\nresults.append(['LinearRegression', grid.best_score_,mean_absolute_error(y_test,y_pred),np.sqrt(mean_squared_error(y_test,y_pred)),median_absolute_error(y_test,y_pred), grid.best_params_])","9bb4c5ae":"model = make_pipeline(preprocessor,Ridge())\nn_alphas = 10\nalphas = np.logspace(-5, 5, n_alphas)\nparameters = {'ridge__alpha':alphas }\ngrid = GridSearchCV(model,parameters, cv=5, n_jobs = 5)\ngrid.fit(X_train, np.log(y_train))\ny_pred = grid.predict(X_test)\nprint('M\u00e9thode: Ridge OneHotEncoder RobustScaler')\nprint (\"best score: \", grid.best_score_)\nprint(\"best params: \",grid.best_params_)\nprint(\"score de la pr\u00e9diction:\"), \nprint(\"RMSE = \",mean_absolute_error(y_test,y_pred))\nprint(\"MAE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"median abs err = \",median_absolute_error(y_test,y_pred))\n\nresults.append(['Ridge', grid.best_score_,mean_absolute_error(y_test,y_pred),np.sqrt(mean_squared_error(y_test,y_pred)),median_absolute_error(y_test,y_pred), grid.best_params_])","71934ad7":"model = make_pipeline(preprocessor,Lasso(tol=0.5))\nn_alphas = 10\nalphas = np.logspace(-5, 5, n_alphas)\nparameters = {'lasso__alpha':alphas }\ngrid = GridSearchCV(model,parameters, cv=5, n_jobs = 5)\ngrid.fit(X_train, y_train)\ny_pred = grid.predict(X_test)\nprint('M\u00e9thode: Lasso OneHotEncoder RobustScaler')\nprint (\"best score: \", grid.best_score_)\nprint(\"best params: \",grid.best_params_)\nprint(\"score de la pr\u00e9diction:\"), \nprint(\"RMSE = \",mean_absolute_error(y_test,y_pred))\nprint(\"MAE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"median abs err = \",median_absolute_error(y_test,y_pred))\n\nresults.append(['Lasso', grid.best_score_,mean_absolute_error(y_test,y_pred),np.sqrt(mean_squared_error(y_test,y_pred)),median_absolute_error(y_test,y_pred), grid.best_params_])","9379ffa4":"model = make_pipeline(preprocessor,ElasticNet(max_iter=10000,tol=100))\nn_alphas = 10\nalphas = np.logspace(-5, 5, n_alphas)\nl1_ratio = np.logspace(-5, 1, n_alphas)\nparameters = {'elasticnet__alpha':alphas,  'elasticnet__l1_ratio':l1_ratio}\ngrid = GridSearchCV(model,parameters, cv=5, n_jobs = 5)\ngrid.fit(X_train, y_train)\ny_pred = grid.predict(X_test)\nprint('M\u00e9thode: ElasticNet OneHotEncoder RobustScaler')\nprint (\"best score: \", grid.best_score_)\nprint(\"best params: \",grid.best_params_)\nprint(\"score de la pr\u00e9diction:\"), \nprint(\"RMSE = \",mean_absolute_error(y_test,y_pred))\nprint(\"MAE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"median abs err = \",median_absolute_error(y_test,y_pred))\n\nresults.append(['ElasticNet', grid.best_score_,mean_absolute_error(y_test,y_pred),np.sqrt(mean_squared_error(y_test,y_pred)),median_absolute_error(y_test,y_pred), grid.best_params_])","ab774671":"model = make_pipeline(preprocessor,RandomForestRegressor())\nparameters = {'randomforestregressor__n_estimators': [100, 200, 300], 'randomforestregressor__max_depth': [None, 1, 2, 3], 'randomforestregressor__min_samples_split': [1, 2, 3]}\ngrid = GridSearchCV(model,parameters, cv=5, n_jobs = 5)\ngrid.fit(X_train, y_train)\ny_pred = grid.predict(X_test)\nprint('M\u00e9thode: RandomForestRegressor OneHotEncoder RobustScaler')\nprint (\"best score: \", grid.best_score_)\nprint(\"best params: \",grid.best_params_)\nprint(\"score de la pr\u00e9diction:\"), \nprint(\"RMSE = \",mean_absolute_error(y_test,y_pred))\nprint(\"MAE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"median abs err = \",median_absolute_error(y_test,y_pred))\n\nresults.append(['RandomForestRegressor', grid.best_score_,mean_absolute_error(y_test,y_pred),np.sqrt(mean_squared_error(y_test,y_pred)),median_absolute_error(y_test,y_pred), grid.best_params_])","f8d56aa0":"model = make_pipeline(preprocessor,XGBRegressor())\nparameters = { \n    'xgbregressor__learning_rate': [0.05, 0.07, 0.08],\n    'xgbregressor__max_depth': [ 6, 7, 8],\n    'xgbregressor__min_child_weight': [4, 5],\n    'xgbregressor__n_estimators': [100, 200, 1000]\n}\ngrid = GridSearchCV(model,parameters, cv=5, n_jobs = 5)\ngrid.fit(X_train, y_train)\ny_pred = grid.predict(X_test)\nprint('M\u00e9thode: XGBRegressor OneHotEncoder RobustScaler')\nprint (\"best score: \", grid.best_score_)\nprint(\"best params: \",grid.best_params_)\nprint(\"score de la pr\u00e9diction:\"), \nprint(\"RMSE = \",mean_absolute_error(y_test,y_pred))\nprint(\"MAE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"median abs err = \",median_absolute_error(y_test,y_pred))\n\nresults.append(['XGBRegressor', grid.best_score_,mean_absolute_error(y_test,y_pred),np.sqrt(mean_squared_error(y_test,y_pred)),median_absolute_error(y_test,y_pred), grid.best_params_])","c0476c66":"df_results = pd.DataFrame(results,columns=['algorithm', 'best score', 'RMSE', 'MAE', 'median abs err', 'best params'])\ndisplay(df_results)\ndisplay(df_results.sort_values(by=['RMSE'],ascending=True))","ff20cb46":"On ne travaillera que sur la colonne TotalGHGEmissions pour raccourcir le temps de traitement mais le principe est exactement le m\u00eame pour la colonne SiteEnergyUse(kBtu)","d75b1e42":"# <a id=\"XGBRegressor\">XGBRegressor<a\/> ","40a77ab5":"<div style=\"width:100%;text-align: center;\">\n    <img src=\"https:\/\/user.oc-static.com\/upload\/2019\/02\/24\/15510245026714_Seattle_logo_landscape_blue-black.png\" \/>\n<\/div>\n\n# Introduction\n\nVous travaillez pour la ville de Seattle. Pour atteindre son objectif de ville neutre en \u00e9missions de carbone en 2050, votre \u00e9quipe s\u2019int\u00e9resse de pr\u00e8s aux \u00e9missions des b\u00e2timents non destin\u00e9s \u00e0 l\u2019habitation.\n\nDes relev\u00e9s minutieux ont \u00e9t\u00e9 effectu\u00e9s par vos agents en 2015 et en 2016. Cependant, ces relev\u00e9s sont co\u00fbteux \u00e0 obtenir, et \u00e0 partir de ceux d\u00e9j\u00e0 r\u00e9alis\u00e9s, vous voulez tenter de pr\u00e9dire les \u00e9missions de CO2 et la consommation totale d\u2019\u00e9nergie de b\u00e2timents pour lesquels elles n\u2019ont pas encore \u00e9t\u00e9 mesur\u00e9es.\n\nVotre pr\u00e9diction se basera sur les donn\u00e9es d\u00e9claratives du permis d'exploitation commerciale (taille et usage des b\u00e2timents, mention de travaux r\u00e9cents, date de construction..)\n\nVous cherchez \u00e9galement \u00e0 \u00e9valuer l\u2019int\u00e9r\u00eat de l\u2019\"ENERGY STAR Score\" pour la pr\u00e9diction d\u2019\u00e9missions, qui est fastidieux \u00e0 calculer avec l\u2019approche utilis\u00e9e actuellement par votre \u00e9quipe.\n","19c485d5":"# <a id=\"ElasticNet\">ElasticNet<a\/> ","e9a89243":"# <a id=\"RandomForestRegressor\">RandomForestRegressor<a\/> ","dc26fe50":"# <a id=\"Ridge\">Ridge<a\/> ","1824efc0":"# Sommaire\n\n1. [R\u00e9gression lin\u00e9aire](#linear-regression)  \n2. [Ridge](#Ridge)  \n3. [Lasso](#Lasso)  \n4. [ElasticNet](#ElasticNet)  \n5. [RandomForestRegressor](#RandomForestRegressor)  \n6. [XGBRegressor](#XGBRegressor)  \n7. [Conclusion](#conclude)","7ee41a09":"# <a id=\"conclude\">Conclusion<a\/> ","a23f6d53":"# <a id=\"linear-regression\">R\u00e9gression lin\u00e9aire<a\/> ","89acf968":"# <a id=\"Lasso\">Lasso<a\/> ","8891a51f":"## todo:\n* impl\u00e9menter et ultiliser une validation crois\u00e9e pour une r\u00e9gression lin\u00e9aire (https:\/\/www.kaggle.com\/sangwookchn\/regression-techniques-using-scikit-learn)\n* comparez avec des mod\u00e8les naifs\n* Comparez le comportement du lasso, de la r\u00e9gression ridge et de elastic net (ridge + lasso)\n* svm regr\u00e9ssive (https:\/\/www.cours-gratuit.com\/tutoriel-python\/tutoriel-python-matriser-les-svm-avec-scikit-learn#_Toc55988212)\n* XGBregressor (https:\/\/towardsdatascience.com\/predicting-electricity-consumption-with-xgbregressor-a11b71104754 https:\/\/www.kaggle.com\/gayathrydasika\/xgb-regressor-basic https:\/\/www.kaggle.com\/stuarthallows\/using-xgboost-with-scikit-learn)\n* utiliser lazy predict (https:\/\/penseeartificielle.fr\/comparer-algorithme-machine-learning-regression-classification\/)\n* mesurer la diff\u00e9rence de temps d'execution entre toutes les donn\u00e9es et les donn\u00e9es filtr\u00e9es\n* Interet de remplir des valeurs \"nones\" \/ analyse et algo pour les second et troisi\u00e8me usages dans les donn\u00e9es filtr\u00e9es\n* checker les outliers\n* feature selection\n\n\n## id\u00e9es\n\nenergyscore \/ ann\u00e9e de construction \/ perte \/ consommation moyenne par type  \nmap taux de polution + map de consommation \u00e9nerg\u00e9tique   \nlearning curve pour d\u00e9terminer le nombre de deonn\u00e9es n\u00e9cessaires \u00e0 lapr\u00e9diction d'un mod\u00e8le coh\u00e9rent => pas de n\u00e9cessit\u00e9 d'en r\u00e9colter plus conform\u00e9ment \u00e0 la demande (https:\/\/www.youtube.com\/watch?v=w_bLGK4Pteo&list=PLO_fdPEVlfKoHQ3Ua2NtDL4nmynQC8YiS&index=2)\n\ngroupKfold important (https:\/\/www.youtube.com\/watch?v=VoyMOVfCSfc&list=PLO_fdPEVlfKoHQ3Ua2NtDL4nmynQC8YiS&index=3)*\n\nhttps:\/\/www.kaggle.com\/kanncaa1\/machine-learning-tutorial-for-beginners\n\nliens map:  \nhttps:\/\/programminghistorian.org\/en\/lessons\/visualizing-with-bokeh  \nhttps:\/\/docs.bokeh.org\/en\/latest\/docs\/gallery\/texas.html  \nhttps:\/\/towardsdatascience.com\/lets-make-a-map-using-geopandas-pandas-and-matplotlib-to-make-a-chloropleth-map-dddc31c1983d  \nhttps:\/\/towardsdatascience.com\/using-geopandas-for-spatial-visualization-21e78984dc37  \nhttps:\/\/geopandas.org\/gallery\/plotting_basemap_background.html  \nhttps:\/\/ichi.pro\/fr\/utilisation-de-geopandas-pour-la-visualisation-spatiale-37278328347703  \nhttps:\/\/towardsdatascience.com\/using-geopandas-for-spatial-visualization-21e78984dc37  \nhttps:\/\/towardsdatascience.com\/puppies-python-analyzing-geospatial-data-93dd9dc3137  \n"}}