{"cell_type":{"820addf9":"code","69ae8723":"code","1f5effde":"code","77231748":"code","7437af05":"code","dc4bf7a8":"code","9bc6f3eb":"code","807f7b02":"code","cb1d1115":"code","2583c2c3":"code","ccdf05d1":"code","9e8dd2cf":"code","2e56f6e5":"code","d460f333":"code","1424b015":"code","34f5924d":"code","426d336a":"code","6463fc86":"code","0c2c62d5":"code","8201f34d":"code","adb5302e":"code","d1f1e4c3":"code","9bb85afe":"code","1fe1e11a":"code","e450d7cb":"code","d04f7165":"code","7a424514":"code","d3b943c2":"code","e66765f4":"code","49a7c060":"code","adda58f8":"code","0127e591":"code","5e9f3bbb":"code","61ebea04":"code","a18167e0":"code","ee436947":"markdown","369475fb":"markdown","aea69fc4":"markdown","8e609928":"markdown","0f268e89":"markdown","f4210f23":"markdown","31207033":"markdown","33b48650":"markdown","06b12a5a":"markdown","ed86ddee":"markdown","92fce16e":"markdown","3feea53d":"markdown","1a0a0e57":"markdown","39174541":"markdown"},"source":{"820addf9":"from IPython.display import Image\nImage(filename=\"..\/input\/fakenews1\/FakeNewsDetection_ML_Approach.png\")","69ae8723":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport re #Regular expressions \nimport nltk #Natural Language Toolkit\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nimport itertools\n","1f5effde":"nltk.download('stopwords')","77231748":"# Printing the stopwords in English\n# We will have to remove these from the corpus during our analysis and pre-processing\nprint(stopwords.words('english'))","7437af05":"from IPython.display import Image\nImage(filename=\"..\/input\/fakenews2\/Text_PreProcessing_Steps.png\")","dc4bf7a8":"# First let's load the dataset\nnews_dataset = pd.read_csv('..\/input\/fake-news\/train.csv')","9bc6f3eb":"news_dataset.head()","807f7b02":"news_dataset.shape","cb1d1115":"# Checking for missing values\nnews_dataset.isnull().sum()","2583c2c3":"# Let's replace the missing values with empty strings wherever possible\nnews_dataset = news_dataset.fillna('')","ccdf05d1":"news_dataset.isnull().sum()","9e8dd2cf":"# We will use all text fields such as Title, Author & text description to train our model \n# to help predict if it is real or unreliable news\nnews_dataset['text_corpus'] = news_dataset['author']+' '+news_dataset['title']+' '+news_dataset['text']","2e56f6e5":"print(news_dataset['text_corpus'])","d460f333":"news_dataset.head()","1424b015":"# Now we will separate the data and label i.e. text_corpus and label fields\nX = news_dataset.drop(columns='label', axis=1)\nY = news_dataset['label']","34f5924d":"X = news_dataset['text_corpus']\nprint(X)","426d336a":"print(Y)","6463fc86":"X.head()","0c2c62d5":"port_stem = PorterStemmer()\n\ndef stemming(content):\n    # Pick all alphabet characters - lowercase and uppercase...all others such as numbers and punctuations will be removed. Numbers or punctuations will be replaced by a whitespace\n    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n    # Converting all letters to lowercase \n    stemmed_content = stemmed_content.lower()\n    # Converting all to a splitted case or a list\n    stemmed_content = stemmed_content.split()\n    # Applying stemming, so we get the root words wherever possible + remove stopwords as well\n    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n    # Join all the words in final content\n    stemmed_content = ' '.join(stemmed_content)\n    return stemmed_content","8201f34d":"news_dataset['text_corpus'] = news_dataset['text_corpus'].apply(stemming)","adb5302e":"print(news_dataset['text_corpus'])","d1f1e4c3":"# Separating data and label\nX = news_dataset['text_corpus'].values\nY = news_dataset['label'].values","9bb85afe":"print(X)","1fe1e11a":"print(Y)","e450d7cb":"vectorizer = TfidfVectorizer()\nvectorizer.fit(X)\n\nX = vectorizer.transform(X)","d04f7165":"print(X)","7a424514":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.18, stratify=Y, random_state=124)","d3b943c2":"# Training the model\nmodel1 = LogisticRegression()\n\nmodel1.fit(X_train, Y_train)","e66765f4":"# Accuracy Score on Training Data\nX_train_prediction = model1.predict(X_train)\ntraining_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n\nprint('Accuracy score on the training data: ',training_data_accuracy)\n\n# Accuracy Score on Test Data\nX_test_prediction = model1.predict(X_test)\ntest_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n\nprint('Accuracy score on the test data: ',test_data_accuracy)","49a7c060":"# Function to plot confusion Matrix\ndef plot_confusion_matrix(cm, \n                          classes,\n                          normalize=False,\n                          title='Confusion Matrix',\n                          cmap=plt.cm.Blues):\n  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n  plt.title(title)\n  plt.colorbar()\n  tick_marks = np.arange(len(classes))\n  plt.xticks(tick_marks, classes, rotation=45)\n  plt.yticks(tick_marks, classes)\n  \n  if normalize:\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    print(\"Normalized confusion matrix\")\n  else:\n      print('Confusion matrix, without normalization')\n  thresh = cm.max() \/ 2.\n  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n      plt.text(j, i, cm[i, j],horizontalalignment=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n  plt.tight_layout()\n  plt.ylabel('True label')\n  plt.xlabel('Predicted label')","adda58f8":"cm = metrics.confusion_matrix(Y_test, X_test_prediction)\nplot_confusion_matrix(cm, classes=['Fake', 'Real'])","0127e591":"# Classification report for test data\nclassification_report(Y_test, X_test_prediction)","5e9f3bbb":"X_new = X_test[100]\n\nprediction = model1.predict(X_new)\nprint(prediction)\n\nif (prediction[0] == 0):\n  print('The news in Real')\nelse:\n  print('The news is Fake and Unreliable')","61ebea04":"news_dataset[100:101]","a18167e0":"print(Y_test[100])","ee436947":"### Training the model (model1 : Logistic Regression)","369475fb":"## 3.2 Remove Special Characters \n\n- Pick all alphabet characters - lowercase and uppercase...all others such as numbers and punctuations will be removed.\n- Numbers or punctuations will be replaced by a whitespace\n\n## 3.3 Remove Stopwords\n\n- Remove standard english stopwords\n\n## 3.4 Stemming or Lemmatization\n\n- Here we will try to apply stemming - objective is to reduce the word to it's root word.\n- For example: actor, actress, acting, action --> act","aea69fc4":"#### Dataset used - https:\/\/www.kaggle.com\/c\/fake-news\/data\n\n### Dataset Description\n\ntrain.csv: A full training dataset with the following attributes:\n\n* id: unique id for a news article\n* title: the title of a news article\n* author: author of the news article\n* text: the text of the article; could be incomplete\n* label: a label that marks the article as potentially unreliable\n  * 1: unreliable\n  * 0: reliable\n\ntest.csv: A testing training dataset with all the same attributes at train.csv without the label.\n\nsubmit.csv: A sample submission that you can use to submit your prediction results","8e609928":"Stopwords are words which occur frequently in a corpus. e.g a, an, the, in. Frequently occurring words are removed from the corpus for the purpose of text-normalization.","0f268e89":"# 2. Import required libraries","f4210f23":"### Splitting the data into test and train datasets","31207033":"### Model Evaluation","33b48650":"# 1. Goal: Build a system to identify unreliable\/fake news","06b12a5a":"## 3.5 TF-IDF (Term Frequency, Inverse Document Frequency)\n\n### Converting Textual data to Numerical data\n\n* We will use TF-IDF Vectorizer\n* TF-IDF Vectorizer will convert textual data to numerical data","ed86ddee":"The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.","92fce16e":"## 3.1 Load the dataset, Read and Parse Text","3feea53d":"# 3. Data Pre-processing and Analysis\n\n#### Typically, we follow below standard steps from text pre-processing and feature engineering standpoint","1a0a0e57":"# 4. Modeling & Model Evaluation","39174541":"# 5. Making a Prediction"}}