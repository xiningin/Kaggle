{"cell_type":{"a1ea3e8d":"code","ad298e50":"code","6e806b27":"code","c0a2e03f":"code","fc412567":"code","9bc3eed9":"code","8c118c9f":"code","c9a7dbb9":"code","71c4699e":"code","4d8f9413":"code","ef481f20":"code","0404f963":"code","67fce4db":"code","aa077ce9":"code","bc489af2":"code","f859c0d7":"code","b9056b22":"code","9d87bb83":"code","ad9b9a96":"code","d952e6cf":"code","d1ef1494":"code","77b61f65":"code","5fae2715":"code","f238d9dc":"code","bcd7da78":"code","faf4572b":"code","15508601":"code","19a67472":"code","8f0ccf00":"code","3a5bdd4d":"code","5d5a2a26":"code","1676cefe":"code","d1a362f8":"code","104acaae":"code","a29f3ad5":"code","8bdcd3ed":"code","d6888dcb":"code","7f21a371":"code","fd272038":"code","739e84de":"code","3bdd76b9":"code","2d49bd2e":"code","6e2d2579":"code","76e9a827":"code","9bf88a52":"code","26e589e3":"code","14ca3a48":"code","04e3a293":"code","e71980a7":"code","82f20c4c":"code","0a4ea8e3":"code","30c31ae8":"code","c89b5516":"code","74cfad90":"code","9c9cfe38":"code","7c94d347":"code","19e56340":"code","ef0c6514":"code","e548cca0":"code","28fbc023":"code","004e41cd":"code","d5d70ee9":"code","1fbf60a6":"code","a71180b8":"code","17c6bc34":"code","1ef761c0":"code","9d94f527":"code","34186861":"code","7102091f":"code","9a09674f":"code","3e7e99e6":"code","2ea485e4":"code","1091a534":"code","09bd6eb7":"code","9682d096":"code","722eddbc":"code","1d20a31e":"code","c2333d67":"code","e981273a":"code","be427284":"code","8aa3a50a":"code","a807034b":"code","ae9f32f4":"code","5abdddd5":"code","c274fcca":"markdown","91be00ac":"markdown","00ad2846":"markdown","34454f1d":"markdown","e3c5fd81":"markdown","4ed938bf":"markdown","9c4db24a":"markdown","b489114b":"markdown","dbe107a3":"markdown","ade11b21":"markdown","f277b20b":"markdown","eb025629":"markdown","470527ed":"markdown","45e62df7":"markdown","10b0dd27":"markdown"},"source":{"a1ea3e8d":"!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/","ad298e50":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nDATA_DIR = '..\/input\/google-quest-challenge'","6e806b27":"!ls ..\/input","c0a2e03f":"os.listdir(\"..\/input\/roberta-transformers-pytorch\/roberta-base\")","fc412567":"os.listdir(\"..\/input\/qaxlnetbasecasedaugdiffswaanswer\")","9bc3eed9":"os.listdir(\"..\/input\/qaxlnetbasecasedaugdiffswaquestion\")","8c118c9f":"os.listdir(\"..\/input\/qabertbaseuncasedaugdiffswaanswer\")","c9a7dbb9":"os.listdir(\"..\/input\/qabertbaseuncasedaugdiffswaquestion\")","71c4699e":"os.listdir(\"..\/input\/qabertbasecasedaugdiffswaanswer\")","4d8f9413":"os.listdir(\"..\/input\/qabertbasecasedaugdiffswaquestion\")","ef481f20":"os.listdir(\"..\/input\/qabertbasecasedaugdiffv2swa\")","0404f963":"os.listdir(\"..\/input\/qabertuncasedaugdiffv2swa\")","67fce4db":"os.listdir(\"..\/input\/qaxlnetbasecasedaugdiff\")","aa077ce9":"os.listdir(\"..\/input\/qarobertabasecasedaugdiffswaquestion\")","bc489af2":"sub = pd.read_csv(f'{DATA_DIR}\/sample_submission.csv')\nsub.head()","f859c0d7":"TARGET_COLUMNS = sub.columns.values[1:].tolist()\nTARGET_COLUMNS","b9056b22":"train = pd.read_csv(f'{DATA_DIR}\/train.csv')\ntrain.head()","9d87bb83":"test = pd.read_csv(f'{DATA_DIR}\/test.csv')\ntest.head()","ad9b9a96":"import torch\nimport html\n#import torch.utils.data as data\nfrom torchvision import datasets, models, transforms\nfrom transformers import *\nfrom sklearn.utils import shuffle\nimport random\nfrom math import floor, ceil\nfrom sklearn.model_selection import GroupKFold\n\nMAX_LEN = 512\n#MAX_Q_LEN = 250\n#MAX_A_LEN = 259\nSEP_TOKEN_ID = 102\n\nclass QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, df, model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", train_mode=True, labeled=True):\n        self.df = df\n        self.train_mode = train_mode\n        self.labeled = labeled\n        self.max_len = max_len\n        self.content = content\n        bert_tokenizer_path = '..\/input\/pretrained-bert-models-for-pytorch\/' + model_type + '-vocab.txt'\n        xlnet_tokenizer_path = '..\/input\/xlnet-pretrained-models-pytorch\/' + model_type + '-spiece.model'\n        roberta_tokenizer_path = '..\/input\/roberta-transformers-pytorch\/roberta-base\/vocab.json'\n        roberta_tokenizer_merges_file = '..\/input\/roberta-transformers-pytorch\/roberta-base\/merges.txt'\n        if model_type == \"bert-base-uncased\":\n            self.tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)\n        elif model_type == \"bert-base-cased\":\n            self.tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)\n        elif model_type == \"xlnet-base-cased\":\n            self.tokenizer = XLNetTokenizer.from_pretrained(xlnet_tokenizer_path)\n        elif model_type == \"roberta-base\":\n            self.tokenizer = RobertaTokenizer(vocab_file=roberta_tokenizer_path, merges_file=roberta_tokenizer_merges_file)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        token_ids, seg_ids = self.get_token_ids(row)\n        if self.labeled:\n            labels = self.get_label(row)\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\n    def __len__(self):\n        return len(self.df)\n\n    def select_tokens(self, tokens, max_num):\n        if len(tokens) <= max_num:\n            return tokens\n        if self.train_mode:\n            num_remove = len(tokens) - max_num\n            remove_start = random.randint(0, len(tokens)-num_remove-1)\n            return tokens[:remove_start] + tokens[remove_start + num_remove:]\n        else:\n            return tokens[:max_num\/\/2] + tokens[-(max_num - max_num\/\/2):]\n        \n    def trim_input_single_content(self, title, content, max_sequence_length=512, \n                t_max_len=30, c_max_len=512-30-4, num_token=3):\n        \n        content = html.unescape(content)\n        title = html.unescape(title)\n        \n        t = self.tokenizer.tokenize(title)\n        c = self.tokenizer.tokenize(content)\n\n        t_len = len(t)\n        c_len = len(c)\n\n        if (t_len+c_len+num_token) > max_sequence_length:\n\n            if t_max_len > t_len:\n                t_new_len = t_len\n                c_max_len = c_max_len + floor((t_max_len - t_len)\/2)\n            else:\n                t_new_len = t_max_len\n\n            if c_max_len > c_len:\n                c_new_len = c_len \n            else:\n                c_new_len = c_max_len\n\n\n            if t_new_len+c_new_len+num_token > max_sequence_length:\n                raise ValueError(\"New sequence length should be less or equal than %d, but is %d\" \n                                 % (max_sequence_length, (t_new_len+c_new_len+num_token)))\n            \n            # truncate\n            if len(t) - t_new_len > 0:\n                t = t[:t_new_len\/\/4] + t[len(t)-t_new_len+t_new_len\/\/4:]\n            else:\n                t = t[:t_new_len]\n\n            if len(c) - c_new_len > 0:\n                c = c[:c_new_len\/\/4] + c[len(c)-c_new_len+c_new_len\/\/4:]\n            else:\n                c = c[:c_new_len]\n\n        # some bad cases\n        if (len(t) + len(c) + num_token > max_sequence_length):\n            more_token = len(t) + len(c) + num_token - max_sequence_length\n            c = c[:(len(c)-more_token)]\n        \n        return t, c\n            \n    def trim_input(self, title, question, answer, max_sequence_length=MAX_LEN, \n                t_max_len=30, q_max_len=239, a_max_len=239, num_token=4):\n\n        question = html.unescape(question)\n        answer = html.unescape(answer)\n        title = html.unescape(title)\n        \n        t = self.tokenizer.tokenize(title)\n        q = self.tokenizer.tokenize(question)\n        a = self.tokenizer.tokenize(answer)\n\n        t_len = len(t)\n        q_len = len(q)\n        a_len = len(a)\n\n        if (t_len+q_len+a_len+num_token) > max_sequence_length:\n\n            if t_max_len > t_len:\n                t_new_len = t_len\n                a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n                q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n            else:\n                t_new_len = t_max_len\n\n            if a_max_len > a_len:\n                a_new_len = a_len \n                q_new_len = q_max_len + (a_max_len - a_len)\n            elif q_max_len > q_len:\n                a_new_len = a_max_len + (q_max_len - q_len)\n                q_new_len = q_len\n            else:\n                a_new_len = a_max_len\n                q_new_len = q_max_len\n\n\n            if t_new_len+a_new_len+q_new_len+num_token > max_sequence_length:\n                raise ValueError(\"New sequence length should be %d, but is %d\" \n                                 % (max_sequence_length, (t_new_len+a_new_len+q_new_len+num_token)))\n\n            \n            # truncate\n            if len(t) - t_new_len > 0:\n                t = t[:t_new_len\/\/4] + t[len(t)-t_new_len+t_new_len\/\/4:]\n            else:\n                t = t[:t_new_len]\n\n            if len(q) - q_new_len > 0:\n                q = q[:q_new_len\/\/4] + q[len(q)-q_new_len+q_new_len\/\/4:]\n            else:\n                q = q[:q_new_len]\n\n            if len(a) - a_new_len > 0:\n                a = a[:a_new_len\/\/4] + a[len(a)-a_new_len+a_new_len\/\/4:]\n            else:\n                a = a[:a_new_len]\n\n        return t, q, a\n        \n    def get_token_ids(self, row):\n        \n        num_token = 4\n        \n        if self.content == \"Question\":\n            num_token -= 1\n        elif self.content == \"Answer\":\n            num_token -= 1\n        \n        if self.content == \"Question_Answer\":   \n            t_max_len=30\n            q_max_len=int((self.max_len-t_max_len-num_token)\/2)\n            a_max_len=(self.max_len-t_max_len - num_token - int((self.max_len-t_max_len-num_token)\/2))\n        elif self.content == \"Question\":\n            t_max_len=30\n            q_max_len=self.max_len-t_max_len-num_token\n            a_max_len=0\n        elif self.content == \"Answer\":\n            t_max_len=30\n            q_max_len=0\n            a_max_len=self.max_len-t_max_len-num_token  \n        else:\n            raise NotImplementedError\n        \n        if self.content == \"Question_Answer\":\n            t_tokens, q_tokens, a_tokens = self.trim_input(row.question_title, row.question_body, row.answer, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, q_max_len=q_max_len, a_max_len=a_max_len, num_token=num_token)\n        elif self.content == \"Question\":\n            t_tokens, c_tokens = self.trim_input_single_content(row.question_title, row.question_body, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, c_max_len=q_max_len, num_token=num_token)\n        elif self.content == \"Answer\":\n            t_tokens, c_tokens = self.trim_input_single_content(row.question_title, row.answer, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, c_max_len=a_max_len, num_token=num_token)\n        else:\n            raise NotImplementedError\n\n        if self.content == \"Question_Answer\":\n            tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]'] + a_tokens + ['[SEP]']\n        elif ((self.content == \"Question\") or (self.content == \"Answer\")):\n            tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + c_tokens + ['[SEP]']\n        else:\n            raise NotImplementedError\n                \n        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        if len(token_ids) < self.max_len:\n            token_ids += [0] * (self.max_len - len(token_ids))\n        ids = torch.tensor(token_ids)\n        seg_ids = self.get_seg_ids(ids)\n        \n        return ids, seg_ids\n    \n    def get_seg_ids(self, ids):\n        seg_ids = torch.zeros_like(ids)\n        seg_idx = 0\n        first_sep = True\n        for i, e in enumerate(ids):\n            seg_ids[i] = seg_idx\n            if e == self.tokenizer.sep_token_id:\n                if first_sep:\n                    first_sep = False\n                else:\n                    seg_idx = 1\n        pad_idx = torch.nonzero(ids == 0)\n        seg_ids[pad_idx] = 0\n\n        return seg_ids\n\n    def get_label(self, row):\n        #print(row[TARGET_COLUMNS].values)\n        return torch.tensor(row[TARGET_COLUMNS].values.astype(np.float32))\n\n    def collate_fn(self, batch):\n        token_ids = torch.stack([x[0] for x in batch])\n        seg_ids = torch.stack([x[1] for x in batch])\n    \n        if self.labeled:\n            labels = torch.stack([x[2] for x in batch])\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\ndef get_test_loader(model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", batch_size=4):\n    df = pd.read_csv(f'{DATA_DIR}\/test.csv')\n    ds_test = QuestDataset(df, model_type, max_len=max_len, content=content, train_mode=False, labeled=False)\n    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=ds_test.collate_fn, drop_last=False)\n    loader.num = len(df)\n    \n    return loader, ds_test.tokenizer\n        \ndef get_train_val_loaders(model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", batch_size=4, val_batch_size=4, ifold=0):\n    df = pd.read_csv(f'{DATA_DIR}\/train.csv')\n    df = shuffle(df, random_state=42)\n    #split_index = int(len(df) * (1-val_percent))\n    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n    for fold, (train_idx, valid_idx) in enumerate(gkf):\n        if fold == ifold:\n            df_train = df.iloc[train_idx]\n            df_val = df.iloc[valid_idx]\n            break\n\n    #print(df_val.head())\n    #df_train = df[:split_index]\n    #df_val = df[split_index:]\n\n    print(df_train.shape)\n    print(df_val.shape)\n\n    ds_train = QuestDataset(df_train, model_type, max_len=max_len, content=content)\n    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=ds_train.collate_fn, drop_last=True)\n    train_loader.num = len(df_train)\n\n    ds_val = QuestDataset(df_val, model_type, max_len=max_len, content=content, train_mode=False)\n    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=2, collate_fn=ds_val.collate_fn, drop_last=False)\n    val_loader.num = len(df_val)\n    val_loader.df = df_val\n\n    return train_loader, val_loader, ds_train.tokenizer\n\ndef test_train_loader():\n    loader, _, _ = get_train_val_loaders(\"xlnet-base-cased\", 512, \"Question\", 4, 4, 1)\n    for ids, seg_ids, labels in loader:\n        print(ids)\n        print(seg_ids.numpy())\n        print(labels)\n        break\ndef test_test_loader():\n    loader, _ = get_test_loader(\"roberta-base\", 512, \"Question\", 4)\n    for ids, seg_ids in loader:\n        print(ids)\n        print(seg_ids)\n        break","d952e6cf":"test_test_loader()","d1ef1494":"test_train_loader()","77b61f65":"from transformers import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass QuestModel(nn.Module):\n    def __init__(self, model_type=\"xlnet-base-cased\", tokenizer=None, n_classes=30, hidden_layers=[-1, -3, -5, -7, -9]):\n        super(QuestModel, self).__init__()\n        self.model_name = 'QuestModel'\n        self.model_type = model_type\n        self.hidden_layers = hidden_layers\n        if model_type == \"bert-base-uncased\":\n            bert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\n            bert_config = BertConfig.from_json_file(bert_model_config)\n            bert_config.output_hidden_states = True\n            model_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/' + model_type)\n            self.bert_model = BertModel.from_pretrained(model_path, config=bert_config)   \n        elif model_type == \"bert-base-cased\":\n            bert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-cased\/bert_config.json'\n            bert_config = BertConfig.from_json_file(bert_model_config)\n            bert_config.output_hidden_states = True\n            model_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/' + model_type)\n            self.bert_model = BertModel.from_pretrained(model_path, config=bert_config)   \n        elif model_type == \"xlnet-base-cased\":\n            xlnet_model_config = '..\/input\/xlnet-pretrained-models-pytorch\/xlnet-base-cased-config.json'\n            xlnet_config = XLNetConfig.from_json_file(xlnet_model_config)\n            xlnet_config.output_hidden_states = True\n            xlnet_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/xlnet-pretrained-models-pytorch\/' + model_type + '-pytorch_model.bin')\n            self.xlnet_model = XLNetModel.from_pretrained(model_path, config=xlnet_config)   \n        elif model_type == \"xlnet-large-cased\":\n            xlnet_model_config = '..\/input\/xlnet-pretrained-models-pytorch\/xlnet-large-cased-config.json'\n            xlnet_config = XLNetConfig.from_json_file(xlnet_model_config)\n            xlnet_config.output_hidden_states = True\n            xlnet_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/xlnet-pretrained-models-pytorch\/' + model_type + '-pytorch_model.bin')\n            self.xlnet_model = XLNetModel.from_pretrained(model_path, config=xlnet_config)  \n        elif model_type == \"roberta-base\":\n            roberta_model_config = '..\/input\/roberta-transformers-pytorch\/roberta-base\/config.json'\n            roberta_config = RobertaConfig.from_json_file(roberta_model_config)\n            roberta_config.output_hidden_states = True\n            roberta_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/roberta-transformers-pytorch\/roberta-base\/pytorch_model.bin')\n            self.roberta_model = RobertaModel.from_pretrained(model_path, config=roberta_config)  \n            self.roberta_model.resize_token_embeddings(len(tokenizer)) \n        \n        if model_type == \"bert-base-uncased\":\n            self.hidden_size = 768\n        elif model_type == \"bert-large-uncased\":\n            self.hidden_size = 1024\n        elif model_type == \"bert-base-cased\":\n            self.hidden_size = 768\n        elif model_type == \"xlnet-base-cased\":\n            self.hidden_size = 768\n        elif model_type == \"xlnet-large-cased\":\n            self.hidden_size = 1024\n        elif model_type == \"roberta-base\":\n            self.hidden_size = 768\n        else:\n            raise NotImplementedError\n            \n        self.fc_1 = nn.Linear(self.hidden_size * len(hidden_layers), self.hidden_size)\n        self.fc = nn.Linear(self.hidden_size, n_classes)\n            \n        self.selu = nn.SELU()\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n\n    def forward(self, ids, seg_ids):\n        attention_mask = (ids > 0)\n        \n        if ((self.model_type == \"bert-base-uncased\") \\\n            or (self.model_type == \"bert-base-cased\") \\\n            or (self.model_type == \"bert-large-uncased\") \\\n            or (self.model_type == \"bert-large-cased\")):\n        \n            outputs = self.bert_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            hidden_states = outputs[2]\n            \n            # pooled_out = outputs[1] #  N * 768\n        \n            # sequence_out = torch.unsqueeze(outputs[0][:, 0], dim=-1) # N * 512 * 768 * 1, hidden_states[-1]\n            # fuse_hidden = sequence_out\n            \n            # 13 (embedding + 12 transformers) for base\n            # 26 (embedding + 25 transformers) for large\n            \n            # concat hidden\n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = torch.mean(hidden_states[hidden_layer], dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = torch.mean(hidden_states[hidden_layer], dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n                    \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n        \n        elif ((self.model_type == \"xlnet-base-cased\") \\\n            or (self.model_type == \"xlnet-large-cased\")):\n\n            attention_mask = attention_mask.float()\n            outputs = self.xlnet_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            hidden_states = outputs[1]\n            \n            # last_hidden_out = outputs[0]\n            # mem = outputs[1], when config.mem_len > 0\n            \n            # concat hidden, summary_type=\"first\", first_dropout = 0\n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n        \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n        elif (self.model_type == \"roberta-base\"):\n\n            attention_mask = attention_mask.float()\n            outputs = self.roberta_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            # outputs = self.roberta_model(input_ids=ids, attention_mask=attention_mask)\n            hidden_states = outputs[2]\n            \n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n        \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n            \n            \n            \n        for j, dropout in enumerate(self.dropouts):\n            \n            if j == 0:\n                logit = self.fc(dropout(h))\n            else:\n                logit += self.fc(dropout(h))\n                \n        return logit \/ len(self.dropouts)\n    \ndef test_model(model_type=\"bert-base-cased\", hidden_layers=[-1, -3, -5, -7, -9]):\n    x = torch.tensor([[1,2,3,4,5, 0, 0], [1,2,3,4,5, 0, 0]])\n    seg_ids = torch.tensor([[0,0,0,0,0, 0, 0], [0,0,0,0,0, 0, 0]])\n    model = QuestModel(model_type=model_type, hidden_layers=hidden_layers)\n\n    y = model(x, seg_ids)\n    print(y)","5fae2715":"test_model(model_type=\"bert-base-cased\", hidden_layers=[-3, -4, -5, -6, -7])","f238d9dc":"def create_bert_base_uncased_models():\n    models = []\n    for i in range(10):\n        model = QuestModel(model_type=\"bert-base-uncased\", hidden_layers=[-1, -3, -5, -7, -9])\n        model.load_state_dict(torch.load(f'..\/input\/qabertuncasedaugdiffv2swa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_models():\n    models = []\n    for i in range(10):\n        model = QuestModel(model_type=\"bert-base-cased\", hidden_layers=[-1, -3, -5, -7, -9])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbasecasedaugdiffv2swa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qaxlnetbasecasedaugdiff\/fold_{i}_checkpoint.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qaxlnetbasecasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qaxlnetbasecasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\n\ndef create_bert_base_uncased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-uncased\", n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbaseuncasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_uncased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-uncased\", n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbaseuncasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-cased\", n_classes=21, hidden_layers=[-2, -4, -6, -8, -10])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbasecasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-cased\", n_classes=9, hidden_layers=[-2, -4, -6, -8, -10])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbasecasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_question_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qarobertabasecasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_answer_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qarobertabasecasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models","bcd7da78":"from tqdm import tqdm\nimport torch\ndef predict(models, test_loader):\n    all_scores = []\n    with torch.no_grad():\n        for ids, seg_ids in tqdm(test_loader, total=test_loader.num \/\/ test_loader.batch_size):\n            ids, seg_ids = ids.cuda(), seg_ids.cuda()\n            scores = []\n            for model in models:\n                model = model.cuda()\n                outputs = torch.sigmoid(model(ids, seg_ids)).cpu()\n                scores.append(outputs)\n            all_scores.append(torch.mean(torch.stack(scores), 0))\n\n    all_scores = torch.cat(all_scores, 0).numpy()\n    \n    return all_scores","faf4572b":"# test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", batch_size=32)","15508601":"# xlnet_base_cased_models = create_xlnet_base_cased_models()\n# xlnet_base_cased_preds = predict(xlnet_base_cased_models, test_loader)","19a67472":"# del xlnet_base_cased_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","8f0ccf00":"# test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", content=\"Question\", batch_size=32)","3a5bdd4d":"# xlnet_base_cased_question_models = create_xlnet_base_cased_question_models()\n# xlnet_base_cased_question_preds = predict(xlnet_base_cased_question_models, test_loader)","5d5a2a26":"# del xlnet_base_cased_question_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","1676cefe":"# test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", content=\"Answer\", batch_size=32)","d1a362f8":"# xlnet_base_cased_answer_models = create_xlnet_base_cased_answer_models()\n# xlnet_base_cased_answer_preds = predict(xlnet_base_cased_answer_models, test_loader)","104acaae":"# del xlnet_base_cased_answer_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","a29f3ad5":"# xlnet_base_cased_question_answer_preds = np.concatenate([xlnet_base_cased_question_preds, xlnet_base_cased_answer_preds], axis=1)","8bdcd3ed":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Question\", batch_size=32)","d6888dcb":"roberta_base_question_models = create_roberta_base_question_models(tokenizer)\nroberta_base_question_preds = predict(roberta_base_question_models, test_loader)","7f21a371":"del roberta_base_question_models, test_loader, tokenizer\ntorch.cuda.empty_cache()\ngc.collect()","fd272038":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Answer\", batch_size=32)","739e84de":"roberta_base_answer_models = create_roberta_base_answer_models(tokenizer)\nroberta_base_answer_preds = predict(roberta_base_answer_models, test_loader)","3bdd76b9":"del roberta_base_answer_models, test_loader, tokenizer\ntorch.cuda.empty_cache()\ngc.collect()","2d49bd2e":"roberta_base_question_answer_preds = np.concatenate([roberta_base_question_preds, roberta_base_answer_preds], axis=1)","6e2d2579":"# test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", content=\"Question\", batch_size=32)","76e9a827":"# bert_base_cased_question_models = create_bert_base_cased_question_models()\n# bert_base_cased_question_preds = predict(bert_base_cased_question_models, test_loader)","9bf88a52":"# del bert_base_cased_question_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","26e589e3":"# test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", content=\"Answer\", batch_size=32)","14ca3a48":"# bert_base_cased_answer_models = create_bert_base_cased_answer_models()\n# bert_base_cased_answer_preds = predict(bert_base_cased_answer_models, test_loader)","04e3a293":"# del bert_base_cased_answer_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","e71980a7":"# bert_base_cased_question_answer_preds = np.concatenate([bert_base_cased_question_preds, bert_base_cased_answer_preds], axis=1)","82f20c4c":"# test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", content=\"Question\", batch_size=32)","0a4ea8e3":"# bert_base_uncased_question_models = create_bert_base_uncased_question_models()\n# bert_base_uncased_question_preds = predict(bert_base_uncased_question_models, test_loader)","30c31ae8":"# del bert_base_uncased_question_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","c89b5516":"# test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", content=\"Answer\", batch_size=32)","74cfad90":"# bert_base_uncased_answer_models = create_bert_base_uncased_answer_models()\n# bert_base_uncased_answer_preds = predict(bert_base_uncased_answer_models, test_loader)","9c9cfe38":"# del bert_base_uncased_answer_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","7c94d347":"# bert_base_uncased_question_answer_preds = np.concatenate([bert_base_uncased_question_preds, bert_base_uncased_answer_preds], axis=1)","19e56340":"# test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", batch_size=32)","ef0c6514":"# bert_base_cased_models = create_bert_base_cased_models()\n# bert_base_cased_preds = predict(bert_base_cased_models, test_loader)","e548cca0":"# del bert_base_cased_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","28fbc023":"# test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", batch_size=32)","004e41cd":"# bert_base_uncased_models = create_bert_base_uncased_models()\n# bert_base_uncased_preds = predict(bert_base_uncased_models, test_loader)","d5d70ee9":"# del bert_base_uncased_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","1fbf60a6":"# preds = bert_base_uncased_question_answer_preds\n# preds = ((bert_base_uncased_preds + bert_base_cased_preds)\/2.0 \\\n#          + (xlnet_base_cased_preds + xlnet_base_cased_question_answer_preds)\/2.0 \\\n#          + (bert_base_uncased_question_answer_preds + bert_base_cased_question_answer_preds)\/2.0 \\\n#           ) \/ 3.0\n# preds = bert_base_uncased_preds\npreds = roberta_base_question_answer_preds","a71180b8":"# sub[TARGET_COLUMNS] = bert_base_uncased_preds\n# sub.to_csv('submission_bert_base_uncased.csv', index=False)\n# sub[TARGET_COLUMNS] = bert_base_cased_preds\n# sub.to_csv('submission_bert_base_cased.csv', index=False)\n# sub[TARGET_COLUMNS] = xlnet_base_cased_preds\n# sub.to_csv('submission_xlnet_base_cased.csv', index=False)","17c6bc34":"# pred = np.copy(preds)","1ef761c0":"# test = pd.read_csv(f'{DATA_DIR}\/test.csv')","9d94f527":"# import pandas as pd\n# optimization_results = pd.read_csv(\"..\/input\/optyxx\/optimization_resultsX.csv\")","34186861":"# ALL_COLUMNS = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n#                      'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n#                      'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n#                      'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n#                      'question_type_compare', 'question_type_consequence', 'question_type_definition',\n#                      'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n#                      'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n#                      'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n#                      'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n#                      'answer_type_reason_explanation', 'answer_well_written']\n\n# OPTIMIZED_COLUMNS = [\n#     'question_conversational',\n#     'question_has_commonly_accepted_answer',\n#     'question_not_really_a_question',\n#     'question_type_choice',\n#     'question_type_compare',\n#     'question_type_consequence',\n#     'question_type_definition',\n#     'question_type_entity',\n#     'question_type_instructions',\n#     'question_interestingness_self', \n#     'answer_satisfaction'\n# ]\n\n# NON_OPTIMIZED_COLUMNS = list(set(ALL_COLUMNS) - set(OPTIMIZED_COLUMNS))","7102091f":"# for col in NON_OPTIMIZED_COLUMNS:\n#     coeffs = optimization_results.loc[optimization_results.col==col, 'coeffs']\n        \n#     changerow = int(len(test) * coeffs)\n#     colidx = NON_OPTIMIZED_COLUMNS.index(col)\n    \n#     if optimization_results.loc[optimization_results.col==col, 'choice'].values =='decrease':\n#         rowidx = pred[:, colidx].argsort()[:changerow]\n#         pred[rowidx, colidx] = pred[rowidx, colidx] * 0.9\n#     elif optimization_results.loc[optimization_results.col==col, 'choice'].values =='increase':\n#         rowidx = pred[:, colidx].argsort()[-changerow:]\n#         pred[rowidx, colidx] = pred[rowidx, colidx] * 1.1\n#     else:\n#         pass","9a09674f":"sub[TARGET_COLUMNS] = preds","3e7e99e6":"sub.head()","2ea485e4":"test = pd.read_csv(f'{DATA_DIR}\/test.csv')","1091a534":"test = test.set_index('qa_id').join(sub.set_index('qa_id'))","09bd6eb7":"test.head()","9682d096":"from sklearn.preprocessing import MinMaxScaler\n    \ndef postprocessing(oof_df):\n   \n    scaler = MinMaxScaler()\n    \n    # type 1 column [0, 0.333333, 0.5, 0.666667, 1]\n    # type 2 column [0, 0.333333, 0.666667]\n    # type 3 column [0.333333, 0.444444, 0.5, 0.555556, 0.666667, 0.777778, 0.8333333, 0.888889, 1]\n    # type 4 column [0.200000, 0.266667, 0.300000, 0.333333, 0.400000, \\\n    # 0.466667, 0.5, 0.533333, 0.600000, 0.666667, 0.700000, \\\n    # 0.733333, 0.800000, 0.866667, 0.900000, 0.933333, 1]\n    \n    # comment some columns based on oof result\n    \n    ################################################# handle type 1 columns\n    type_one_column_list = [\n       'question_conversational', \\\n       'question_has_commonly_accepted_answer', \\\n       'question_not_really_a_question', \\\n       'question_type_choice', \\\n       'question_type_compare', \\\n       'question_type_consequence', \\\n       'question_type_definition', \\\n       'question_type_entity', \\\n       'question_type_instructions', \n    ]\n    \n    oof_df[type_one_column_list] = scaler.fit_transform(oof_df[type_one_column_list])\n    \n    tmp = oof_df.copy(deep=True)\n    \n    for column in type_one_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.16667, column] = 0\n        oof_df.loc[(tmp[column] > 0.16667) & (tmp[column] <= 0.41667), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.41667) & (tmp[column] <= 0.58333), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.58333) & (tmp[column] <= 0.73333), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.73333), column] = 1\n    \n    \n    \n    ################################################# handle type 2 columns      \n#     type_two_column_list = [\n#         'question_type_spelling'\n#     ]\n    \n#     for column in type_two_column_list:\n#         if sum(tmp[column] > 0.15)>0:\n#             oof_df.loc[tmp[column] <= 0.15, column] = 0\n#             oof_df.loc[(tmp[column] > 0.15) & (tmp[column] <= 0.45), column] = 0.333333\n#             oof_df.loc[(tmp[column] > 0.45), column] = 0.666667\n#         else:\n#             t1 = max(int(len(tmp[column])*0.0013),2)\n#             t2 = max(int(len(tmp[column])*0.0008),1)\n#             thred1 = sorted(list(tmp[column]))[-t1]\n#             thred2 = sorted(list(tmp[column]))[-t2]\n#             oof_df.loc[tmp[column] <= thred1, column] = 0\n#             oof_df.loc[(tmp[column] > thred1) & (tmp[column] <= thred2), column] = 0.333333\n#             oof_df.loc[(tmp[column] > thred2), column] = 0.666667\n    \n    \n    \n    ################################################# handle type 3 columns      \n    type_three_column_list = [\n       'question_interestingness_self', \n    ]\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    oof_df[type_three_column_list] = scaler.fit_transform(oof_df[type_three_column_list])\n    tmp[type_three_column_list] = scaler.fit_transform(tmp[type_three_column_list])\n    \n    for column in type_three_column_list:\n        oof_df.loc[tmp[column] <= 0.385, column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.385) & (tmp[column] <= 0.47), column] = 0.444444\n        oof_df.loc[(tmp[column] > 0.47) & (tmp[column] <= 0.525), column] = 0.5\n        oof_df.loc[(tmp[column] > 0.525) & (tmp[column] <= 0.605), column] = 0.555556\n        oof_df.loc[(tmp[column] > 0.605) & (tmp[column] <= 0.715), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.8), column] = 0.833333\n        oof_df.loc[(tmp[column] > 0.8) & (tmp[column] <= 0.94), column] = 0.888889\n        oof_df.loc[(tmp[column] > 0.94), column] = 1\n        \n        \n        \n    ################################################# handle type 4 columns      \n    type_four_column_list = [\n        'answer_satisfaction'\n    ]\n    scaler = MinMaxScaler(feature_range=(0.2, 1))\n    oof_df[type_four_column_list] = scaler.fit_transform(oof_df[type_four_column_list])\n    tmp[type_four_column_list] = scaler.fit_transform(tmp[type_four_column_list])\n    \n    for column in type_four_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.233, column] = 0.200000\n        oof_df.loc[(tmp[column] > 0.233) & (tmp[column] <= 0.283), column] = 0.266667\n        oof_df.loc[(tmp[column] > 0.283) & (tmp[column] <= 0.315), column] = 0.300000\n        oof_df.loc[(tmp[column] > 0.315) & (tmp[column] <= 0.365), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.365) & (tmp[column] <= 0.433), column] = 0.400000\n        oof_df.loc[(tmp[column] > 0.433) & (tmp[column] <= 0.483), column] = 0.466667\n        oof_df.loc[(tmp[column] > 0.483) & (tmp[column] <= 0.517), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.517) & (tmp[column] <= 0.567), column] = 0.533333\n        oof_df.loc[(tmp[column] > 0.567) & (tmp[column] <= 0.633), column] = 0.600000\n        oof_df.loc[(tmp[column] > 0.633) & (tmp[column] <= 0.683), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.683) & (tmp[column] <= 0.715), column] = 0.700000\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.767), column] = 0.733333\n        oof_df.loc[(tmp[column] > 0.767) & (tmp[column] <= 0.833), column] = 0.800000\n        oof_df.loc[(tmp[column] > 0.883) & (tmp[column] <= 0.915), column] = 0.900000\n        oof_df.loc[(tmp[column] > 0.915) & (tmp[column] <= 0.967), column] = 0.933333\n        oof_df.loc[(tmp[column] > 0.967), column] = 1\n    \n    \n    ################################################# round to i \/ 90 (i from 0 to 90)\n    oof_values = oof_df[TARGET_COLUMNS].values\n    DEGREE = len(oof_df)\/\/45*9\n#     if degree:\n#         DEGREE = degree\n#     DEGREE = 90\n    oof_values = np.around(oof_values * DEGREE) \/ DEGREE  ### 90 To be changed\n    oof_df[TARGET_COLUMNS] = oof_values\n    \n    return oof_df","722eddbc":"test = postprocessing(test)","1d20a31e":"for column in TARGET_COLUMNS:\n    print(test[column].value_counts())","c2333d67":"sub = test[TARGET_COLUMNS].reset_index()","e981273a":"sub[ sub[TARGET_COLUMNS] > 1.0] = 1.0","be427284":"sub.head()","8aa3a50a":"test = pd.read_csv(f'{DATA_DIR}\/test.csv')","a807034b":"n=test['url'].apply(lambda x:(('ell.stackexchange.com' in x) or ('english.stackexchange.com' in x))).tolist()\nspelling=[]\nfor x in n:\n    if x:\n        spelling.append(0.5)\n    else:\n        spelling.append(0.)","ae9f32f4":"sub['question_type_spelling'] = spelling","5abdddd5":"sub.to_csv('submission.csv', index=False)","c274fcca":"## predict with bert-base-uncased question and answer","91be00ac":"## predict with bert-base-cased","00ad2846":"## predict with xlnet-base-cased","34454f1d":"## predict with bert-base-cased question and answer","e3c5fd81":"### Generate Submission","4ed938bf":"## predict with roberta-base question and answer","9c4db24a":"### Define dataset","b489114b":"# Postprocessing","dbe107a3":"### Required Imports\n\nI've added imports that will be used in training too","ade11b21":"# Assign postprocessed result","f277b20b":"**Pytorch BERT baseline**","eb025629":"## Build Model","470527ed":"### bert-base (uncased-v2, cased-v2) swa + xlnet (5 folds) + bert-base-uncased (question + answer) swa + bert-base-cased (question + answer) swa + xlnet (question + answer) swa + roberta (question + answer) + postprocessing (seefun's version)","45e62df7":"## predict with xlnet-base-cased question and answer","10b0dd27":"## predict with bert-base-uncased"}}