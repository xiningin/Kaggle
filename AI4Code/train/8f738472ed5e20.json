{"cell_type":{"45681a07":"code","1df13118":"code","e15b86fe":"code","e01b8966":"code","3e7f95ac":"code","6800151f":"code","650a7922":"code","9da6aa69":"code","afa0aabf":"code","673d7e7f":"code","94238707":"code","71b1af9a":"code","1b31a80c":"code","bb7863fe":"code","70f54d4e":"code","be7f5cef":"code","c6ba6106":"code","37bb34cb":"code","1fef67db":"code","b33e299c":"code","2217fa87":"code","e4833be4":"code","dc31e569":"code","eba3b7cf":"code","e30da554":"code","b2d12851":"code","0fdc0be8":"code","cf78cfa5":"code","88796661":"code","d0f2da1e":"code","84d0c9ef":"code","944b9c45":"code","7ecc6ed0":"code","6c4baff8":"code","dd4a1047":"code","b19929db":"code","55e75f10":"code","c108d72b":"code","ae3d3ad2":"code","81ba3d7f":"code","29cb6fc5":"code","f56558e6":"code","1311ee20":"code","ff5cd84e":"code","b6a0d3f3":"code","34229289":"code","23ef0a0d":"code","95e222dd":"markdown","1da37453":"markdown","dfd762a8":"markdown","9a4e4fe3":"markdown","8b7e55e2":"markdown","1f6c449d":"markdown","ff4b399f":"markdown","38f80693":"markdown","a75911e9":"markdown","44eccc02":"markdown","d12b656f":"markdown","d04b9f33":"markdown","11bf2488":"markdown","d2bcb493":"markdown","275f3377":"markdown","ee9eed71":"markdown"},"source":{"45681a07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\nsns.set_style(\"darkgrid\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Reading Dataset","1df13118":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")","e15b86fe":"trainoriginal = train.copy()\ntestoriginal = test.copy()","e01b8966":"display(train.head())\ndisplay(test.head())\ndisplay(sub.head())","3e7f95ac":"print(\"Size of the train:\", train.shape)\nprint(\"\\nSize of the test:\", test.shape)\nprint(\"\\nSize of the Submission:\", sub.shape)","6800151f":"print('Columns in Train data:\\n\\n', train.columns)\nprint('-'*80)\nprint('\\n\\nColumns in Test data:\\n\\n', test.columns)","650a7922":"print('Datatypes of Train dataset:\\n', train.info())\nprint('-'*50)\nprint('\\n\\nDatatypes of Train dataset:\\n', test.info())","9da6aa69":"train = train.drop(['id'], axis=1)\ntest = test.drop(['id'], axis=1)","afa0aabf":"train.isnull().sum()","673d7e7f":"test.isnull().sum()","94238707":"train['target'].value_counts().sort_index(ascending=True)\n","71b1af9a":"plt.figure(figsize=(8,6))\nsns.countplot(x='target', data=train, order=train['target'].value_counts().index);","1b31a80c":"train['target1'] = train['target'].map({'Class_1':0, 'Class_2':1, 'Class_3':2, 'Class_4':3})\ntrain['feature_0'].value_counts()","bb7863fe":"train['feature_2'].value_counts()","70f54d4e":"train['feature_38'].value_counts().sort_index(ascending=False)","be7f5cef":"plt.figure(figsize=(18,25))\nsns.boxplot(data=train, orient=\"h\");","c6ba6106":"plt.figure(figsize=(18,25))\nsns.boxplot(data=test.iloc[:,1:], orient=\"h\");","37bb34cb":"# Pearson Correlation\nplt.figure(figsize=(18,10))\nsns.heatmap(train.corr(method='pearson'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='coolwarm');","1fef67db":"# Spearman Correlation\nplt.figure(figsize=(18,12))\nsns.heatmap(train.corr(method='spearman'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='coolwarm');","b33e299c":"fig, ax = plt.subplots(figsize=(18, 12))\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nax.text(-1.1, -0.7, 'Correlation between the Features', fontsize=20, fontweight='bold', fontfamily='serif')\nsns.heatmap(corr, mask=mask, annot=False, fmt='.2f', linewidth=0.2, cbar=True, cmap='coolwarm');\n","2217fa87":"# kendall\nfig, ax = plt.subplots(1, 3, figsize=(17 , 5))\n\nfeature_lst = ['feature_0', 'feature_1', 'feature_2','feature_3','feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9']\n\ncorr = train[feature_lst].corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nfor idx, method in enumerate(['pearson', 'kendall', 'spearman']):\n    sns.heatmap(train[feature_lst].corr(method=method), ax=ax[idx],\n            square=True, annot=True, fmt='.1f', center=0, linewidth=2,\n            cbar=False, cmap=sns.diverging_palette(240, 10, as_cmap=True),\n            mask=mask\n           ) \n    ax[idx].set_title(f'{method.capitalize()} Correlation', loc='left', fontweight='bold')     \n\nplt.show()","e4833be4":"train.corr()['target1'].sort_values(ascending=False)","dc31e569":"a = train.drop(['target','target1'], axis=1)\na.corrwith(train['target1']).plot(kind='bar', figsize=(18,11), color=['salmon'])\nplt.title('Correlation b\/n target and Independant features')\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","eba3b7cf":"train.skew()","e30da554":"test.skew()","b2d12851":"# Independant variable\nX = train.iloc[:,:-2]\n\n# Dependant variable\ny = train['target']","0fdc0be8":"# split  data into training and testing sets of 80:20 ratio\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","cf78cfa5":"print(\"Length of X_train is: {X_train}\".format(X_train = len(X_train)))\nprint(\"Length of X_test is: {X_test}\".format(X_test = len(X_test)))\nprint(\"Length of y_train is: {y_train}\".format(y_train = len(y_train)))\nprint(\"Length of y_test is: {y_test}\".format(y_test = len(y_test)))","88796661":"from xgboost import XGBClassifier, plot_importance\nmodel = XGBClassifier(random_state=42, use_label_encoder=True)\nmodel.fit(X, y)","d0f2da1e":"fig, ax = plt.subplots(figsize=(10,10))\n\nplot_importance(model,\n                height=0.5,\n                max_num_features=None,\n                title='Feature importance',\n                xlabel='F score', \n                ylabel='Features',\n                ax=ax)","84d0c9ef":"# Feature Importance\nmain_colors = ['#f03aa5', '#40c2f3', '#c489ce', '#bb3ca9']\n\nf, ax = plt.subplots(1, 1, figsize=(18, 18))\n\nplot_importance(model, \n                max_num_features=None,\n                color=main_colors[0],\n                ax=ax)\nplt.title('Feature Importance', fontsize=20)\nplt.show()","944b9c45":"y_pred_xgb = model.predict_proba(test)","7ecc6ed0":"from lightgbm import LGBMClassifier, plot_importance\nLGB = LGBMClassifier(random_state=42, use_label_encoder=True)\nLGB.fit(X, y)","6c4baff8":"plot_importance(LGB, figsize=(18, 15));","dd4a1047":"pip install -U lightautoml","b19929db":"from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\n","55e75f10":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 3 * 3600 # Time in seconds for automl run\nTARGET_NAME = 'target'","c108d72b":"train_data = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\ntrain_data[TARGET_NAME] = train_data[TARGET_NAME].str.slice(start=6).astype(int) - 1","ae3d3ad2":"def create_gr_feats(data):\n    pass\n    \n\nall_df = pd.concat([train_data, test_data]).reset_index(drop = True)\ncreate_gr_feats(all_df)\ntrain_data, test_data = all_df[:len(train_data)], all_df[len(train_data):]\nprint(train_data.shape, test_data.shape)","81ba3d7f":"%%time\n\ntask = Task('multiclass',)","29cb6fc5":"%%time\n\nroles = {\n    'target': TARGET_NAME,\n    'drop': ['id'],\n}","f56558e6":"%%time \n\nautoml = TabularUtilizedAutoML(task = task, \n                               timeout = TIMEOUT,\n                               cpu_limit = N_THREADS,\n                               reader_params = {'n_jobs': N_THREADS},\n                               configs_list=[\n                                   '..\/input\/lightautoml-configs\/conf_0_sel_type_0.yml',\n                                   '..\/input\/lightautoml-configs\/conf_1_sel_type_1.yml'\n                               ])\noof_pred = automl.fit_predict(train_data, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:5], oof_pred.shape))","1311ee20":"%%time\n\n# Fast feature importances calculation\nfast_fi = automl.get_feature_scores('fast', silent = False)\nfast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (20, 10), grid = True)","ff5cd84e":"submission_xgb = pd.DataFrame(y_pred_xgb, columns=['Class_1','Class_2','Class_3','Class_4'])\nsubmission_xgb['id'] = sub['id']\nsubmission_xgb","b6a0d3f3":"\nsubmission_xgb.to_csv('submission.csv', index=0)","34229289":"%%time\n\ntest_pred = automl.predict(test_data)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred[:10], test_pred.shape))","23ef0a0d":"submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\nsubmission.iloc[:, 1:] = test_pred.data\nsubmission.to_csv('lightautoml.csv', index = False)","95e222dd":"## Dropping Unwanted Columns ","1da37453":"## LGBM","dfd762a8":"## XGBoost","9a4e4fe3":"# Exploratory Data Analysis","8b7e55e2":"## Outliers","1f6c449d":"# Data Modeling","ff4b399f":"### Finding missing values of train & test dataset","38f80693":"## Skewness","a75911e9":"# Final Submission","44eccc02":"## Relation between Features","d12b656f":"# LightAutoML","d04b9f33":"### Below features in negative values:\n* feature_19 (-1, -2)\n* feature_30 (-1)\n* feature_31 (-1)\n* feature_32 (-1, -2)\n* feature_35 (-2)\n* feature_38 (-1, -2, -3, -5, -8)\n* feature_39 (-1, -2, -3, -5)\n* feature_42 (-1, -2)\n\n","11bf2488":"## Missing Values","d2bcb493":"# Reading the datasets","275f3377":"# Importing Library","ee9eed71":"## Analysing the Variables"}}