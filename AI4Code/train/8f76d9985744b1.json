{"cell_type":{"32ad7f28":"code","82225bee":"code","1578f99c":"code","8782e7eb":"code","9c7c3283":"code","7aca2f2f":"code","e2c275ab":"markdown","8f4fd003":"markdown","ec746fc6":"markdown","a0e6ca33":"markdown"},"source":{"32ad7f28":"# Start by importing the bq_helper module and calling on the specific active_project and dataset_name for the BigQuery dataset.\nimport bq_helper\nfrom bq_helper import BigQueryHelper\n# https:\/\/www.kaggle.com\/sohier\/introduction-to-the-bq-helper-package\n\noce_claims = bq_helper.BigQueryHelper(active_project=\"patents-public-data\",\n                                   dataset_name=\"uspto_oce_claims\")","82225bee":"# View table names under the uspto_oce_claims data table\nbq_assistant = BigQueryHelper(\"patents-public-data\", \"uspto_oce_claims\")\nbq_assistant.list_tables()","1578f99c":"# View the first three rows of the patent_claims_fulltext data table\nbq_assistant.head(\"patent_claims_stats\", num_rows=3)","8782e7eb":"# View information on all columns in the patent_claims_fulltext data table\nbq_assistant.table_schema(\"patent_claims_stats\")","9c7c3283":"query1 = \"\"\"\nSELECT\n  AVG(CAST(word_ct AS INT64))\nFROM\n  `patents-public-data.uspto_oce_claims.patent_claims_stats`;\n        \"\"\"\nresponse1 = oce_claims.query_to_pandas_safe(query1)\nresponse1.head(20)","7aca2f2f":"bq_assistant.estimate_query_size(query1)","e2c275ab":"## Example SQL Query\nWhat is the average word count of a patent claim?","8f4fd003":"Interpretting this number, this means my query scanned about ~0.29 GB (or 290 MB) of data in order to return an average table of 20 inventors and their prosecution status from the dataset.","ec746fc6":"# How to Query USPTO OCE Patent Claims Research Data (BigQuery)\n[Click here](https:\/\/www.kaggle.com\/mrisdal\/safely-analyzing-github-projects-popular-licenses) for a detailed notebook demonstrating how to use the bq_helper module and best practises for interacting with BigQuery datasets.","a0e6ca33":"## Importance of Knowing Your Query Sizes\n\nIt is important to understand how much data is being scanned in each query due to the free 5TB per month quota. For example, if a query is formed that scans all of the data in a particular column, given how large BigQuery datasets are it wouldn't be too surprising if it burns through a large chunk of that monthly quota!\n\nFortunately, the bq_helper module gives us tools to very easily estimate the size of our queries before running a query. Start by drafting up a query using BigQuery's Standard SQL syntax. Next, call the estimate_query_size function which will return the size of the query in GB. That way you can get a sense of how much data is being scanned before actually running your query."}}