{"cell_type":{"9497bdb8":"code","7dc421ce":"code","ee64cfa6":"code","2f59a9ce":"code","f16908e8":"code","eacfc96c":"code","a8f364ff":"code","08062861":"code","568b4866":"code","d6b6a79e":"code","52720fac":"code","5d21d2a2":"code","dbbcc333":"code","7c388845":"code","909c38cf":"code","3939e9fc":"code","a15f454e":"code","775b71d6":"code","c2cdc0d2":"code","a82985b5":"code","063ae2bc":"code","b60a295f":"code","aa87685a":"code","171d4b67":"code","bb3379de":"code","5c5c711b":"code","329e2893":"code","ca620a04":"code","9d266bdc":"code","7b2b7ba3":"code","4aef83e5":"code","e7e70b31":"code","65df7a75":"code","de806467":"code","85b50a47":"code","0196cf99":"code","58bf2b88":"code","630a1800":"code","ffd5ce19":"code","3a7700ce":"code","184d1a5e":"code","d82b73c3":"code","e87f937b":"code","cf436d81":"code","6f374622":"markdown","2e225d2e":"markdown","7d72a730":"markdown","1ba83475":"markdown","567a552e":"markdown","9b9ff43b":"markdown","6f8d3d9f":"markdown","c726369a":"markdown","9601114f":"markdown","443d996c":"markdown","780aa9e3":"markdown","48b13ab6":"markdown","0a2d2183":"markdown","35332498":"markdown","cb60f853":"markdown","fe58db0e":"markdown","2bd603e3":"markdown","5322bb05":"markdown","d0698a78":"markdown","df8af41f":"markdown","0d473a68":"markdown","94a4f8f5":"markdown","b828ac26":"markdown","fae37692":"markdown","ff434aca":"markdown","4217b5fa":"markdown","6a2c05e4":"markdown","26dfd216":"markdown","efbf5fe2":"markdown","305218cd":"markdown","203e76fa":"markdown","be426a6a":"markdown","81378909":"markdown","a05723f5":"markdown","5159a71c":"markdown","67e09e16":"markdown","05754c8f":"markdown","0bfdf30c":"markdown","a022a1ee":"markdown","926cb545":"markdown","3df04868":"markdown","82145290":"markdown","868a3f10":"markdown","4fd51d60":"markdown","02233e20":"markdown","c3baa7bc":"markdown"},"source":{"9497bdb8":"import numpy as np \nimport pandas as pd\n\npath_dataset = '\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv'","7dc421ce":"df = pd.read_csv(path_dataset)\ndf.head(3)","ee64cfa6":"print(f'Dataset shape: {df.shape}')","2f59a9ce":"# Check out a detailed description of the data.\n# Mostly interested in the data types and any non-null values\ndf.info()","f16908e8":"df = pd.read_csv(path_dataset, na_values=[' ', ''])\ndf.info()","eacfc96c":"df.dropna(inplace=True)\nprint(f'New dataset shape: {df.shape}')","a8f364ff":"# Check value ranges of data\nfor col in df:\n    print(f'Feature: {col}')\n    print(f'Values: {df[col].unique()[:5]}')\n    print('---')","08062861":"# Get a better overview of the numerical data\ndf.describe()","568b4866":"df = df.drop(columns=['customerID'])","d6b6a79e":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndf[df.select_dtypes(['object']).columns] = (\n    df[df.select_dtypes(['object']).columns].select_dtypes(['object']).apply(\n        lambda x: label_encoder.fit_transform(x)\n    ))\ndf.head()","52720fac":"# Check value ranges once more\ndf.describe()","5d21d2a2":"import matplotlib.pyplot as plt\nimport seaborn as sns","dbbcc333":"plt.figure(figsize=(16, 6))\nplt.xticks(range(-10,100,2))\nplt.title('Tenure')\nsns.violinplot(x=['tenure'], data=df)","7c388845":"plt.figure(figsize=(16, 6))\nplt.title('Monthly Charges')\nplt.xticks(range(5,140,5))\nsns.violinplot(x=['MonthlyCharges'], data=df)","909c38cf":"plt.figure(figsize=(16, 6))\nplt.title('Total Charges')\nplt.xticks(range(0,10000,500))\nsns.violinplot(x=['TotalCharges'], data=df)","3939e9fc":"# First grab the categorical subset of the data to make life easier\n# To get the categorical data programmatically the data types are exploited.\ndf_original = pd.read_csv(path_dataset)\ndf_discrete = df_original[df_original.select_dtypes(['object']).columns]\ndf_discrete = df_discrete.drop(columns=['customerID', 'TotalCharges'])\nfeature_names = df_discrete.columns","a15f454e":"fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=.7, wspace=.4)\n\nindex = 0\nfor row in range(4):\n    for col in range(4):\n        ax = fig.add_subplot(4, 4, index+1)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n        feature_name = feature_names[index]\n        ax = sns.countplot(x=feature_name, data=df_discrete)\n        index+=1","775b71d6":"sns.FacetGrid(df, col='Churn').map(sns.violinplot, 'tenure', order=[0,1])","c2cdc0d2":"sns.FacetGrid(df, col='Churn').map(sns.violinplot, 'MonthlyCharges', order=[0,1])","a82985b5":"sns.FacetGrid(df, col='Churn').map(sns.violinplot, 'TotalCharges', order=[0,1])","063ae2bc":"df_categorical = df.drop(columns=['tenure', 'MonthlyCharges', 'TotalCharges'])","b60a295f":"feature_names = df_categorical.columns\n\nindex = 0\nfor index in range(len(feature_names)):\n    feature_name = feature_names[index]\n    sns.FacetGrid(df_categorical, col='Churn').map(\n        sns.countplot, feature_name, order=df_categorical[feature_name].unique())","aa87685a":"correlations = df.corr()\nfig = plt.figure(figsize=(12,10), dpi=80, facecolor='w', edgecolor='k')\nax = sns.heatmap(correlations, cmap='PiYG', vmin=-1, vmax=1,  annot=True)","171d4b67":"correlations.Churn.sort_values()","bb3379de":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import SelectKBest","5c5c711b":"values = df.values\n\nX = values[:,0:19] # Features\ny = values[:,19] # Targets\n\n# Mutual Information for Classification\nmutual_info = mutual_info_classif(X, y, discrete_features='auto', \n                                    n_neighbors=3, copy=True, random_state=None)\n\n# chi square\nchi_score, chi_pval = chi2(X,y)\n\n# F-measure\nf_score, f_pval = f_classif(X,y)\n\ndata_feature_selection = { 'MutualInfo': mutual_info,\n                        'ChiSquaredScore': chi_score,\n                        'ChiSquaredPVal': chi_pval,\n                        'FScore': f_score,\n                        'FPVal': f_pval\n                        }\n\nfeatures = df.columns[0:19]\n\ndf_feature_selection = pd.DataFrame(data_feature_selection)\ndf_feature_selection.insert(0, 'Feature', features)\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\ndf_feature_selection","329e2893":"fig = plt.figure(figsize=(16, 5), dpi= 80, facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=.1, wspace=.6)\n\nax = fig.add_subplot(1, 2, 1)\nax = sns.barplot(x='MutualInfo', y=\"Feature\", \n                 data=df_feature_selection.sort_values('MutualInfo', ascending=False))\n\nax = fig.add_subplot(1, 2, 2)\nax = sns.barplot(x='FScore', y=\"Feature\", \n                 data=df_feature_selection.sort_values('FScore', ascending=False))","ca620a04":"ax = sns.barplot(x='ChiSquaredScore', y=\"Feature\", \n                 data=df_feature_selection.drop([4,17,18])\n                 .sort_values('ChiSquaredScore', ascending=False))","9d266bdc":"fig = plt.figure(figsize=(16, 5), dpi= 80, facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=.1, wspace=.6)\n\nax = fig.add_subplot(1, 2, 1)\nax = sns.barplot(x='ChiSquaredPVal', y=\"Feature\", \n                 data=df_feature_selection\n                 .sort_values('ChiSquaredPVal', ascending=True))\n\nax = fig.add_subplot(1, 2, 2)\nax = sns.barplot(x='FPVal', y=\"Feature\", data=df_feature_selection\n                 .sort_values('FPVal', ascending=True))","7b2b7ba3":"selector = SelectKBest(score_func = mutual_info_classif, k = 8).fit(X,y)\n\nfeature_indices = selector.get_support(True)\n\nprint(\"Best 8 features (Mutual Information):\")\nfor i in range(len(feature_indices)):\n    index = feature_indices[i]\n    print(df.columns[index])","4aef83e5":"# Extract cols with non binary data\ndf_object_dtypes = df_original.select_dtypes(include=\"object\").copy()\ndf_object_dtypes.drop(columns = ['customerID', 'TotalCharges'], axis=1, inplace=True)\nfeatures_non_binary_categorical = []\nfor col in df_object_dtypes.columns:\n    if(len(df_object_dtypes[col].value_counts()) > 2):\n        features_non_binary_categorical.append(col)\n    else:\n        df_object_dtypes.drop([col], axis=1, inplace=True)\n        \nfeatures_non_binary_categorical","e7e70b31":"from sklearn.preprocessing import OneHotEncoder\ndf_onehotencoded = df.copy()\n\nfor feature in features_non_binary_categorical:\n    col_values = df_onehotencoded[feature].values.reshape(-1,1)\n    col_values_one_hot = OneHotEncoder(sparse=False, categories='auto').fit_transform(col_values)\n    col_values_one_hot = col_values_one_hot.tolist()\n    \n    df_onehotencoded[feature] = col_values_one_hot\n    \ndf_onehotencoded.head()","65df7a75":"# Standardized features\ndf_standardized = ((df-df.mean())\/\n                   df.std())\ndf_standardized.head()","de806467":"from sklearn.model_selection import train_test_split","85b50a47":"features = df.columns[0:19]\ntarget = df.columns[19]\n\nx_train, x_test, y_train, y_test = train_test_split(df[features],\n                                                   df[target],\n                                                   test_size = 0.3,\n                                                   random_state = 10)\n\nprint(f'Shape of training set X: {x_train.shape}')\nprint(f'Shape of training set y: {y_train.shape}')\nprint('---')\nprint(f'Shape of test set X: {x_test.shape}')\nprint(f'Shape of test set y: {y_test.shape}')","0196cf99":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.dummy import DummyClassifier","58bf2b88":"# Preparations needed for the pipelines\n# We want to scale the numerical data and we want to onehot encode the categorical data.\n# For this we make use of a ColumnTransformer\ncategorical_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n                       'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n                       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n\nnumeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n\n# Generate a mask identifying the features that are supposed to be one hot encoded\none_hot_mask = (df_original.drop(columns=['Churn', 'customerID']).dtypes == object).values\n\n#Define the pipeline steps\npreprocessor = make_column_transformer(\n    (StandardScaler(), numeric_features),\n    (OneHotEncoder(sparse=False, categories='auto'), categorical_features),\n    remainder='passthrough'\n)\n\npipeline = Pipeline([\n    ('Preprocessor', preprocessor),\n    (\"KBest\", SelectKBest(mutual_info_classif, k=8)),\n    ('Classifier', LogisticRegression(solver ='liblinear'))\n])","630a1800":"# Print a confusion matrix and values for accuracy, precision, recall and f1-measure\ndef calculate_results(y_test, y_pred):\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n        \n    print(f'Accuracy: {accuracy}')\n    print(f'Precision: {precision}')\n    print(f'Recall: {recall}')\n    print(f'F1-Score: {f1}')\n    \n    return accuracy, precision, recall, f1","ffd5ce19":"# Execute pipeline\npipeline.fit(x_train, y_train)\ny_pred = pipeline.predict(x_test)\n\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True) ","3a7700ce":"_ = calculate_results(y_test, y_pred)","184d1a5e":"# Baseline - always predict 0\npipeline = Pipeline([\n    ('Classifier', DummyClassifier(strategy='constant', constant=0))\n]) \n\npipeline.fit(x_train, y_train)\ny_pred = pipeline.predict(x_test)\n\n#confusion_matrix(y_test, y_pred) - #Sklearn function to create a confusion matrix.\n# pd confusion matrix -> better visualization\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True) ","d82b73c3":"_ = calculate_results(y_test, y_pred)","e87f937b":"gridSearchParams = [\n    {'C':[0.01, 0.03, 0.1, 0.3, 1.0, 1.1, 1.3, 1.33, 1.6],\n     'class_weight':[{0:.1, 1:.9},{0:.2, 1:.8},{0:.3, 1:.7},{0:.4, 1:.6},\n                    {0:.4, 1:.7}, {0:.6, 1:.8}, {0:.5, 1:.8}, 'balanced']\n    }\n]\n\nclassifier = GridSearchCV(LogisticRegression(solver ='liblinear'), gridSearchParams, cv=5)\n\n#Define the pipeline steps\npreprocessor = make_column_transformer(\n    (StandardScaler(), numeric_features),\n    (OneHotEncoder(sparse=False, categories='auto'), categorical_features),\n    remainder='passthrough'\n)\n\npipeline = Pipeline([\n    ('Preprocessor', preprocessor),\n    ('Classifier', classifier)\n])\n\n# Execute pipeline\n_ = pipeline.fit(x_train, y_train)\n\n# Calculate results\ny_pred = pipeline.predict(x_test)\n\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True) ","cf436d81":"p5_measures = calculate_results(y_test, y_pred)\n\nprint(\"Best Hyperparameters:\",classifier.best_params_)","6f374622":"**Observation:**    \nMost customers pay between 18 and 25\u20ac per month. The violin plot shows two local peaks between 48-55 and 75-85\u20ac. Overall the monthly charges feature a high standard deviation and the charges are scattered widely.","2e225d2e":"**Observation:**    \nIn total there are 21 data columns, including int64, float64 and object data types.\nAt first glance the dataset seems to be complete, with 7043 entries in each column.\nJudging from the column names almost all data that should be numerical is either of float or integer data type.\nAn exception is found in the \"TotalCharges\" column, which is indicated as object data type.\nMissing values may be a possible cause for this mismatch.\nTo verify this assumption the dataset is checked once again, replacing empty strings explicitly with NaN-values.","7d72a730":"## Univariate Feature Selection","1ba83475":"## Understand Data: Univariate Distribution Visualization (Numerical Features)\n\nTo gain more insight into the data the distributions will be displayed.","567a552e":"The best features according to the mutual information are now selected with Scikit-Learns kbest algorithm.","9b9ff43b":"**Observation:**  \nSome stronger correlations can be observed in the correlation heatmap, e.g. Tenure with contract and Tenure with Total Charges. The usage of additional services (e.g. streaming and security offers) tend to influence the customer's tenure, their contract type and their total charges.\n\nIn context of the Churn label shows a different picture. The strongest negative correlation is found between churn and the contract type, the strongest positive correlation is between churn and the monthly charges.\n\nThe assumption that wider value distributions with respect to the churn label leads to higher correlation values was verified by the correlation with the features Contract, MonthlyCharges and Tenure. Less distributed data on the other hand leads to lower correlation values (e.g. gender). The assumption that TotalCharges is not a potentially usable feature has been disproved. The absolute correlation value is nearly equal to MonthlyCharges. Additionally there are features with even lower correlation values.","6f8d3d9f":"**Observation:**    \nThe plot indicates that monthly charges impact the customers' decision to churn significantly.\nCustomers with high monthly charges are way more likely to churn than customers with a low amount of monthly charges.\nOn the other hand customers with low monthly charges are more likely to stay.","c726369a":"# Preprocess and Understand Data\n\nAfter ensuring that the dataset can be accessed properly and the data is indeed clean the data may be analyzed more closely.","9601114f":"**Observation:**    \nMost customers stay about 2 to 6 years. Another tenure peak is between 66 and 72 years, indicating some very loyal customers.    \nThe median tenure is at 29 years.    \n50% of customers stay between 9 and 55 years, as indicated by the upper and lower quartiles.\nThis rather big range is sign of a high standard deviation and highly scattered data.","443d996c":"**Observation:**    \nCustomers with low tenure are significantly more likely to churn. At the same time customers who didn't churn are spread over a wide range of tenure.","780aa9e3":"**Observation:**   \nWhen taking p-values into consideration the eight best features are equal to the features shown in the bar plots above, even though their ranking differs in the case of the ChiSquared test.\n\nThe features gender and phone service show very high p-values and therefore are no valuable features.","48b13ab6":"#### 2.3.1 Feature: Tenure","0a2d2183":"**Observation:**    \nMost customers pay between 200\u20ac and 500\u20ac in total.\nValues range up to a maximum of 8684, leading to a high standard deviation.    \nThe median value lies at 1400\u20ac total charges.\n50% of customers pay between 400\u20ac and 3800\u20ac in total, as indicated by the lower and upper quartiles.","35332498":"## Understand Data: Univariate Distribution Visualization (Discrete Values)","cb60f853":"**Observation:**    \nThe amount of total charges does not significantly impact the churn rate.\nThe distributions show no big discrepancy.","fe58db0e":"**Comparison of the test methods:**","2bd603e3":"**Observation:**    \n* Gender does not seem to be a valuable feature for churn prediction. Values are evenly distributed\n* Senior citizens are almost equally likely to churn or to stay. Younger customers are more likely to stay. This assumption might be biased due to a lower amount of senior citizens in the dataset.\n* Customers without partner are a little more likely to churn.\n* Customers with dependants are a little bit less likely to churn.\n* The phone service seems to have little impact on the customer's decision.\n* Multiple lines seem have little impact on the customer's decision.\n* The internet service does impact the churn rate. Fiber optics customers are more likely to churn.\n* Online security seems to be an important feature. Customers without online security are more likely to churn.\n* The lack of online backup services increases the churn probability.\n* The lack of device protection increases churn probability.\n* The lack of tech support increases churn probability.\n* Customers don't churn quite as often when no TV streaming is used due to no booked internet service.\n* Customers don't churn quite as often when no TV streaming is used due to no booked internet service.\n* Month-to-month contracts are almost equally likely to stay or to churn. Longer contracts bind the customer to the company.\n* No paperless billing reduces the risk of a churn.\n* Customers with electronic check billing are most likely to churn.","5322bb05":"# Data Mining Process\n\nThe goal of this notebook is to implement a data mining process chain according to [CRISP](https:\/\/en.wikipedia.org\/wiki\/Cross-industry_standard_process_for_data_mining).    \nFor educational purposes the [Telco Customer Churn](https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn) dataset is analyzed. ","d0698a78":"**Observation:**    \nBoth algorithms compute different feature scores. Some feature scores show an overlap, e.g. Contract and Tenure.\n\nThe Chi Square algorithm is only usable for categorical data. Here contract is scored far ahead of other features, which corresponds to the results of the other performed tests.\nThis suggests that contract might be an important feature.","df8af41f":"**Observation:**    \nSome features are significantly unbalanced, e.g. Dependents, PhoneService, MultipleLines, Contract and Churn.    \nEspecially Churn seems to be critical, since this is the label that's supposed to be predicted.","0d473a68":"**Observation:**    \nTo evalute the model multiple measures are considered:\nThe accuracy alone is misleading, as it requires a symmetric dataset with an equal data distributions. As we can see above always predicting label 0 results in an accuracy of 73%.    \nDepending on the classification problem the F1 score is a better metric. It's the weighted average of Precision and Recall and therefore works best if the cost of false positives and false negatives are nearly equal.","94a4f8f5":"## Categorical Data (conditional on Churn label)","b828ac26":"**Observation:**    \nThe data transformation worked properly. As expected all categorical data is now described as numerical data.","fae37692":"### Feature: Total Charges","ff434aca":"**Observation:**    \nReplacing empty strings with NaNs shows the expected data type of float64 for \"TotalCharges\".    \nThe dataset info now also reveals that 11 values are indeed empty.    \nSince this is only a very small fraction of the dataset the corrupted rows will just be dismissed.","4217b5fa":"**General Observations:**    \nBy looking at the plots Tenure and and MonthlyCharges seem to be valuable features for churn prediction due to their vastly different distributions in context of the churn label.    \nOn the other hand TotalCharges seems to be a less valuable feature.","6a2c05e4":"## Grid Search    \nDetermine class weights and regularization parameters with Grid Search to maximize the model's performance","26dfd216":"**Observation:**    \nBy inspecting the data closer it becomes clear that data types are still not consistent.\nFor example, \"SeniorCitizen\" is of integer type but actually describes categorical data.\nOther categorical data is described as strings.","efbf5fe2":"### Feature: Tenure (conditional on Churn label)","305218cd":"## Transform Data: Scaling","203e76fa":"# Modeling & Evaluation","be426a6a":"# Conclusion\n\nThis kernel analyzes a telco customer churn dataset according to the CRISP-DM standard.\nThe dataset is loaded, preprocessed and transformed to fit the specific needs, e.g. by applying label encoding to non-numeric data or scaling the values.\n\nTo better understand the dataset visualizations of univariate and conditional data distributions were created.\nWith the help of these plots potential interesting features for churn prediction were analyzed.\nThe so found features of interest were validated by inspection of a correlation heatmap.\n\nThe most informative features were selected with three different metrics: The mutual information, the F1 score and the ChiSquared score.\nComparison of the metrics showed clear overlaps of interesting features.\nSalient features of the dataset were determined to be: Tenure, InternetService, OnlineSecurity, DeviceProtection, TechSupport, Contract, MonthlyCharges and TotalCharges.\n\nFinally a model was trained and evaluated with Scikit Learn.\nGridSearch was applied for hyperparameter optimization.\nDue to the unbalanced nature of the dataset the accuracy is a potentially misleading performance metric.\nInstead the F1Score was found to be more reliable.","81378909":"## Check Feature Domains\n\nFirst check out some exemplary values of the dataset","a05723f5":"## Understand Data: Conditional Distribution Visualization\n\nTo get a grasp of what features might be important for the Churn label we visualize conditionally on this variable.[](http:\/\/)","5159a71c":"### Feature: Monthly Charges","67e09e16":"### Feature: Total Charges (conditional on Churn label)","05754c8f":"## Train \/ Test Split","0bfdf30c":"## Data Pipeline","a022a1ee":"## Transform Data: One Hot Encoding\n\nNon-binary nominal data is now one-hot-encoded for consumption by ML algorithms.","926cb545":"## Understand Data: Correlation Analysis","3df04868":"# Data Access\n\nIn the following section I t****ake a look at at the dataset to get a first impression.    \nI check the available data, the data types and scan for any pitfalls, e.g. parsing errors, NaNs or unexpected parsing results.","82145290":"Features will be selected based on univariate statistical tests.","868a3f10":"## Transformation of non-numerical data\n\nTo make the dataset consistent and to allow further processing with ML algorithms the categorical data is converted into a numeric representation.\nFor this purpose the LabelEncoder of scikit-learn is used.    \nThe CustomerId does not contain valuable data for analysis, therefore this feature will be dropped before the transformation.","4fd51d60":"**Observation:**    \nGridSearch determined hyperparameters improving scores across the board.    \nThe accuracy improved from 0.78 to 0.80.    \nPrecision improved from 0.59 to 0.62. Intuitively this means that when the model predicts a customer to churn, it is correct 62% of the time.    \nRecall improved from 0.54 to 0.64. Intuitively this means that the model correctly predicts 64% of all churned customers.    \nThe F1-Score improved from 0.57 to 0.63.","02233e20":"## Fit & Evaluate","c3baa7bc":"### Feature: Monthly Charges (conditional on Churn label)"}}