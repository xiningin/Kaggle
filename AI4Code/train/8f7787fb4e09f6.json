{"cell_type":{"3cead422":"code","89381779":"code","6370ab61":"code","798c8bc1":"code","a2df5ed5":"markdown"},"source":{"3cead422":"import numpy as np # linear algebra\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimport time\n\ndef load_data(path):\n    with np.load(path) as f:\n        x_train, y_train = f['x_train'], f['y_train']\n        x_test, y_test = f['x_test'], f['y_test']\n        return (x_train, y_train), (x_test, y_test)\n\n(x_train, y_train), (x_test, y_test) = load_data('..\/input\/mnist-numpy\/mnist.npz')\n\nx_train, x_test = x_train\/255, x_test\/255","89381779":"model = tf.keras.models.Sequential()\n\n#this is not a conv model. plain ANN. \n#I tried adding one extra dense with 128 neurons and it showed improvement. I also increased the dropout a bit than usual.\n\nmodel.add(tf.keras.layers.Flatten(input_shape=(28,28)))\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n\nmodel.compile(optimizer='adam',\n             loss='sparse_categorical_crossentropy',\n             metrics=['accuracy'])\n\nfit_data = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5)","6370ab61":"plt.plot(fit_data.history['loss'])\nplt.plot(fit_data.history['val_loss'])","798c8bc1":"#generate some random index to test\ntest_indices = np.random.randint(low=1, high=1000, size=10)\n\nfor test_i in test_indices:\n    print(\"Predicted\",model.predict(x_test[test_i].reshape(1,28,28)).argmax(), \" and Actual is: \", y_test[test_i])\n    plt.imshow(x_test[test_i], cmap='gray')\n    plt.show()\n    time.sleep(2)\n    ","a2df5ed5":"Simple good old MNIST data set. I am trying out\/practicing deep learning. So tried it out. Thanks to mnist_data as uploaded by https:\/\/www.kaggle.com\/vikramtiwari .\n"}}