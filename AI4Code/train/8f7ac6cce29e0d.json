{"cell_type":{"fd3e6deb":"code","cfc40894":"code","a059bf2a":"code","54e627a8":"code","925b9023":"code","5fe5b8db":"code","8275a7fb":"code","d45b878d":"code","c534abb7":"code","88441d3d":"code","33d3e85e":"code","e4f471a2":"code","ebbcf8f9":"markdown","24bd2bb4":"markdown","b428bbab":"markdown","1b41d32c":"markdown"},"source":{"fd3e6deb":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cfc40894":"train = pd.read_csv(\"..\/input\/santander-value-prediction-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/santander-value-prediction-challenge\/test.csv\")","a059bf2a":"unique_df = train.nunique().reset_index()\nunique_df.columns = [\"col_name\", \"unique_count\"]\nconstant_df = unique_df[unique_df[\"unique_count\"]==1]\nconstant_df.shape","54e627a8":"X = train.drop(constant_df.col_name.tolist() + [\"ID\", \"target\"], axis=1)\ny = np.log1p(train[\"target\"].values) # Our Evaluation metric for the competition is RMSLE. So let us use log of the target variable to build our models.\n\ntest_2 = test.drop(constant_df.col_name.tolist() + [\"ID\"], axis=1)","925b9023":"from sklearn import preprocessing, model_selection, metrics\nimport lightgbm as lgb","5fe5b8db":"def run_lgb(X_train, y_train, X_val, y_val, test_df):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 30,\n        \"learning_rate\" : 0.01,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.7,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(X_train, label = y_train)\n    lgval   = lgb.Dataset(X_val,   label = y_val  )\n    \n    evals_result = {}\n    \n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=200, evals_result=evals_result)\n    \n    pred_test = model.predict(test_df, num_iteration=model.best_iteration)\n    \n    return pred_test, model, evals_result","8275a7fb":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=99)\n\npred_test_full = 0","d45b878d":"for train_index, val_index in kf.split(X):\n    X_train, X_val = X.loc[train_index,:], X.loc[val_index,:]\n    y_train, y_val = y[train_index], y[val_index]\n    pred_test, model, evals_result = run_lgb(X_train, y_train, X_val, y_val, test_2) \n    pred_test_full += pred_test","c534abb7":"pred_test_full \/= 5.\npred_test_full = np.expm1(pred_test_full)","88441d3d":"### Feature Importance ###\nfig, ax = plt.subplots(figsize=(12,18))\nlgb.plot_importance(model, max_num_features=20, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","33d3e85e":"# Making a submission file #\nsub_df = pd.DataFrame({\"ID\":test[\"ID\"].values})\nsub_df[\"target\"] = pred_test_full\nsub_df.to_csv(\"lgb_v1.csv\", index=False)","e4f471a2":"sub_df.head()","ebbcf8f9":"So the validation set RMSLE of the folds range from 1.37 to 1.49.","24bd2bb4":"Refer to previous submissions...\n\nhttps:\/\/www.kaggle.com\/dskagglemt\/santander-value-prediction-challenge-v2\n\n","b428bbab":"# Feature Importance & Baseline - Light GBM","1b41d32c":"Now let us build a Light GBM model to get the feature importance.\n\nApart from feature importance, let us also get predictions on the test set using this model and keep them as baseline predictions.\n\nBelow code is a custom helper function for Light GBM."}}