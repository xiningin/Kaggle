{"cell_type":{"424244f2":"code","91a38cdd":"code","c5a410b1":"code","344acfe4":"code","43f4db0d":"code","1f3bd368":"code","f1beb198":"code","3f1f896d":"code","21946aac":"code","fa02659c":"code","e7288725":"code","370f4756":"code","e04d0690":"code","b951bc8f":"code","bf89a528":"code","6d4db99e":"markdown","43ff287b":"markdown","bee39d30":"markdown","32dfd85d":"markdown","1d1bf214":"markdown","b9827f7b":"markdown","84d0e13d":"markdown","4b429152":"markdown","1d68e23c":"markdown","144ab53c":"markdown","8c09fbbe":"markdown","a7f22a01":"markdown"},"source":{"424244f2":"import warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(palette='cubehelix',context='notebook',\n       font='cambria',style='white')\nimport missingno as msn\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\nfrom sklearn.ensemble import StackingClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","91a38cdd":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv').set_index('PassengerId')\ntest = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv').set_index('PassengerId')\n\ncombined = pd.concat([train,test])\nmsn.matrix(combined.drop('Survived', axis=1), figsize=(8,5), fontsize=12)\nplt.show()","c5a410b1":"fig, axes = plt.subplots(ncols=3, figsize=(12,4))\n\nsns.kdeplot(x=train.Survived, hue=train.Pclass, ax=axes[0], fill=True)\nsns.kdeplot(x=train.Survived, hue=train.Sex,ax=axes[1], fill=True)\nsns.kdeplot(x=train.Survived, hue=train.Embarked,ax=axes[2], fill=True)\nsns.despine()","344acfe4":"def alt_age(df)->pd.Series:\n    \n    for sex in df.Sex.unique():\n        for pc in df.Pclass.unique():\n            df.loc[(df.Pclass == pc) & (df.Sex == sex),'Age']=\\\n            df.loc[(df.Pclass == pc) & (df.Sex == sex),'Age'].fillna(\n            df[(df.Pclass == pc) & (df.Sex == sex)].Age.median())\n            \n    return df\n\ncombined = alt_age(combined)\n\nfig, axes = plt.subplots(ncols=2, figsize=(11,4))\n\nsns.histplot(x=train.Age, ax=axes[0],legend='Train',\n             kde=True, bins=20, edgecolor='darkgreen')\nsns.histplot(x=test.Age,ax=axes[1], legend='Test',\n             kde=True, bins=20, edgecolor='darkgreen')\nsns.despine()","43f4db0d":"def alt_fare(df)->pd.Series:\n    \n    for sex in df.Sex.unique():\n        for pc in df.Pclass.unique():\n            df.loc[(df.Pclass == pc) & (df.Sex == sex),'Fare']=\\\n            df.loc[(df.Pclass == pc) & (df.Sex == sex),'Fare'].fillna(\n            df[(df.Pclass == pc) & (df.Sex == sex)].Fare.median())\n            \n    return df\n\ncombined=alt_fare(combined)\n\nfig, axes = plt.subplots(ncols=2, figsize=(11,4), sharey=True)\n\nsns.kdeplot(x=train.Fare, ax=axes[0],label='Train',\n            fill=True,edgecolor='darkgreen')\nsns.kdeplot(x=test.Fare,ax=axes[1], label='Test',\n            fill=True,edgecolor='darkgreen')\nsns.despine()","1f3bd368":"def fam_size(df)->pd.Series:\n    return df.SibSp+df.Parch+1\n\ndef is_mother(series)->pd.Series:\n    mask = series > 0\n    return np.where(mask,1,0)\n\ndef is_alone(series)->pd.Series:\n    mask= series > 0\n    return np.where(mask,1,0)\n\ndef ticket(series)->pd.Series:\n    return [str(i).split()[0] if len(str(i).split())>1 else 'N' \n     for i in series]\n\ndef fill_cabin(series)->pd.Series:\n    series = series.fillna('N')\n    return series.apply(lambda x: x[0][0])\n\ncombined['Famsize'] = fam_size(combined)\ncombined['IsMother'] = is_mother(combined['Parch'])\ncombined['IsAlone'] = is_alone(combined['SibSp'])\ncombined['Ticket'] = ticket(combined['Ticket'])\n#combined['Embarked']=combined['Embarked'].fillna(np.random.choice(['S','C','Q']))\ncombined['Embarked']=combined['Embarked'].fillna('ffill')\ncombined['Cabin'] = fill_cabin(combined['Cabin'])\n\ncombined = combined.drop(['Name','SibSp','Parch'], axis=1)","f1beb198":"encoder = LabelEncoder()\nhot_encoder = OneHotEncoder()\n\nfor col in combined.select_dtypes('object'):\n    if col == 'Cabin':\n        combined = combined.join(pd.get_dummies(combined[col]))\n    else:\n        combined[col] = encoder.fit_transform(combined[col])\n\nscaler = MinMaxScaler()\n\ncombined[['Fare','Age','Famsize','Ticket','Pclass','Embarked']] = scaler.fit_transform(\n    combined[['Fare','Age','Famsize','Ticket','Pclass','Embarked']])\n\ncombined = combined.drop('Cabin',axis=1)","3f1f896d":"a_train = combined[combined.index.isin(train.index)]\na_target = a_train.pop('Survived')\nb_test = combined[combined.index.isin(test.index)]\nb_target = b_test.pop('Survived')","21946aac":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)","fa02659c":"scores = {}\n\nmod = ('DecisionTreeClassifier',DecisionTreeClassifier(random_state=42,\n            max_depth=10,min_samples_split=818, min_samples_leaf=35))\n\nacc_mean,roc_auc_score_mean,f1_mean = [],[],[]\ny_pred = 0\n\nfor fold,(train_index, valid_index) in enumerate(kf.split(a_train,a_target)):\n\n    X_train, X_valid = a_train.iloc[train_index], a_train.iloc[valid_index]\n    y_train, y_valid = a_target.iloc[train_index], a_target.iloc[valid_index]\n\n    model = mod[1]\n    model.fit(X_train, y_train) \n\n    preds = model.predict(X_valid)\n\n    acc = accuracy_score(y_valid, preds)\n    roc = roc_auc_score(y_valid,preds)\n    f1 = f1_score(y_valid,preds)\n\n    acc_mean.append(acc),roc_auc_score_mean.append(roc), f1_mean.append(f1)\n\n    y_pred += model.predict_proba(b_test)[:,1]\n    \ny_pred \/= n_folds\n\nscores['accuracy score'] = np.mean(acc_mean)\nscores['roc auc score'] = np.mean(roc_auc_score_mean)\nscores['f1 score'] = np.mean(f1_mean)\n\npd.DataFrame([scores], index=[mod[0]])","e7288725":"scores = {}\n\nmod = ('CatBoostClassifier',CatBoostClassifier(random_state=42, verbose=False))\n\nacc_mean,roc_auc_score_mean,f1_mean = [],[],[]\ny_pred = 0\n\nfor fold,(train_index, valid_index) in enumerate(kf.split(a_train,a_target)):\n\n    X_train, X_valid = a_train.iloc[train_index], a_train.iloc[valid_index]\n    y_train, y_valid = a_target.iloc[train_index], a_target.iloc[valid_index]\n\n    model = mod[1]\n    model.fit(X_train, y_train) \n\n    preds = model.predict(X_valid)\n\n    acc = accuracy_score(y_valid, preds)\n    roc = roc_auc_score(y_valid,preds)\n    f1 = f1_score(y_valid,preds)\n\n    acc_mean.append(acc),roc_auc_score_mean.append(roc), f1_mean.append(f1)\n\n    y_pred += model.predict_proba(b_test)[:,1]\n    \ny_pred \/= n_folds\n\nscores['accuracy score'] = np.mean(acc_mean)\nscores['roc auc score'] = np.mean(roc_auc_score_mean)\nscores['f1 score'] = np.mean(f1_mean)\n\npd.DataFrame([scores], index=[mod[0]])","370f4756":"scores = {}\n\nmod = ('XGBoost Classifier',XGBClassifier(random_state=42,\n    cv=5,n_estimator=40,verbosity=0,n_jobs=-1,learning_rate=.1))\n\nacc_mean,roc_auc_score_mean,f1_mean = [],[],[]\ny_pred = 0\n\nfor fold,(train_index, valid_index) in enumerate(kf.split(a_train,a_target)):\n\n    X_train, X_valid = a_train.iloc[train_index], a_train.iloc[valid_index]\n    y_train, y_valid = a_target.iloc[train_index], a_target.iloc[valid_index]\n\n    model = mod[1]\n    model.fit(X_train, y_train) \n\n    preds = model.predict(X_valid)\n\n    acc = accuracy_score(y_valid, preds)\n    roc = roc_auc_score(y_valid,preds)\n    f1 = f1_score(y_valid,preds)\n\n    acc_mean.append(acc),roc_auc_score_mean.append(roc), f1_mean.append(f1)\n\n    y_pred += model.predict_proba(b_test)[:,1]\n    \ny_pred \/= n_folds\n\nscores['accuracy score'] = np.mean(acc_mean)\nscores['roc auc score'] = np.mean(roc_auc_score_mean)\nscores['f1 score'] = np.mean(f1_mean)\n\npd.DataFrame([scores], index=[mod[0]])","e04d0690":"scores = {}\n\nmod = ('LGBM Classifier',LGBMClassifier(random_state=42))\n\nacc_mean,roc_auc_score_mean,f1_mean = [],[],[]\ny_pred = 0\n\nfor fold,(train_index, valid_index) in enumerate(kf.split(a_train,a_target)):\n\n    X_train, X_valid = a_train.iloc[train_index], a_train.iloc[valid_index]\n    y_train, y_valid = a_target.iloc[train_index], a_target.iloc[valid_index]\n\n    model = mod[1]\n    model.fit(X_train, y_train) \n\n    preds = model.predict(X_valid)\n\n    acc = accuracy_score(y_valid, preds)\n    roc = roc_auc_score(y_valid,preds)\n    f1 = f1_score(y_valid,preds)\n\n    acc_mean.append(acc),roc_auc_score_mean.append(roc), f1_mean.append(f1)\n\n    y_pred += model.predict_proba(b_test)[:,1]\n    \ny_pred \/= n_folds\n\nscores['accuracy score'] = np.mean(acc_mean)\nscores['roc auc score'] = np.mean(roc_auc_score_mean)\nscores['f1 score'] = np.mean(f1_mean)\n\npd.DataFrame([scores], index=[mod[0]])","b951bc8f":"## final prediction\n\ny_pred = np.where(y_pred>.5,1,0)","bf89a528":"## makes a submission\n\nsubmission = pd.DataFrame({\n    'PassengerId':b_test.index,'Survived': y_pred})\n\nsubmission.to_csv(\n    'submission.csv',index=False)","6d4db99e":"## EDA & Data Pre-Processing\n--------","43ff287b":"## Data Load\n------","bee39d30":"## Submission\n------","32dfd85d":"#### CatBoost Classifier","1d1bf214":"#### XGBoost Classifier","b9827f7b":"#### DecisionTree Classifier","84d0e13d":"#### LGBM Classifier","4b429152":"## Model Selection\n------","1d68e23c":"#### Label Encoding & MinMax Scaling","144ab53c":"#### Defining KFold Parameters","8c09fbbe":"$\\implies$ LGBMClassifier has the best score.","a7f22a01":"#### Filling nans"}}