{"cell_type":{"37c96ddb":"code","7d6a3c6c":"code","90ff1e2e":"code","468a8b39":"code","c16aadbc":"code","7901d0cb":"code","d5d9fd85":"markdown","a6a2dc08":"markdown","6f9af8ac":"markdown","2a224e6e":"markdown"},"source":{"37c96ddb":"#%pip install librosa\nimport librosa\nimport soundfile as sf\nimport scipy.signal as signal\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","7d6a3c6c":"data_dir = '\/kaggle\/input\/rfcx-species-audio-detection\/'\ndf1=pd.read_csv(data_dir+'train_tp.csv')\nshow = df1.iloc[0]\ndf1.head()","90ff1e2e":"data, samplerate = sf.read(data_dir+'train\/'+show.recording_id+'.flac')\ntimes = np.linspace(0,len(data),len(data))\/samplerate\nplt.plot(times,data)\nplt.xlim(show.t_min, show.t_max)\nplt.ylim(-.1,.1)\nplt.ylabel('Time (s)')\nplt.show()","468a8b39":"Pxx, freqs, bins, im = plt.specgram(data, Fs=samplerate)\n\n# add axis labels\nplt.ylabel('Frequency [Hz]')\nplt.xlabel('Time [sec]')\nplt.xlim(44,46)\nplt.ylim(2000,6000)\nplt.show()","c16aadbc":"import IPython.display as ipd\nipd.Audio(data,rate=samplerate)\n","7901d0cb":"from librosa import display as ld\nX = librosa.stft(data)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14, 5))\nld.specshow(Xdb, sr=samplerate, x_axis='time', y_axis='hz')\nplt.colorbar()\nplt.show()","d5d9fd85":"Read the FLAC file associated with the first row of df1. Look at the signal from t_min to t_max.","a6a2dc08":"Look at the spectrogram of the data, using the limits of t and f.","6f9af8ac":"An alternative approach to plotting spectrograph, based on https:\/\/www.kdnuggets.com\/2020\/02\/audio-data-analysis-deep-learning-python-part-1.html","2a224e6e":"We can also play the file and listen for ourselves!"}}