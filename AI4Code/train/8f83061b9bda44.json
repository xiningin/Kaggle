{"cell_type":{"27ae0f2a":"code","d8bca59c":"code","f24847fa":"code","f871b2eb":"code","3f3aea8b":"code","8189652e":"code","2762f0fd":"code","e83d12c0":"code","f13c1483":"code","8707c33e":"code","ce419038":"code","c968d9ac":"code","5efa8e41":"code","eafa75a0":"code","378e3c4c":"code","544845b2":"code","94101b1d":"code","27b7aba4":"code","761c0fad":"code","ef39ee39":"code","63a4110e":"code","79a41bc3":"code","21470bca":"code","4eb48705":"code","c821a9c2":"markdown","ea8db281":"markdown","05d07014":"markdown","565d9387":"markdown","031cc6dc":"markdown","0536845d":"markdown","da475aa4":"markdown","bf78ee19":"markdown","e7130f8d":"markdown","62e26c17":"markdown","4eedae64":"markdown","a27a2f9b":"markdown","7ad0e004":"markdown","54be9d61":"markdown","3da9c053":"markdown","e0ea9006":"markdown","ade3c9de":"markdown","dc06dddf":"markdown","2ead24b2":"markdown","9bc297d0":"markdown","55474971":"markdown","54e2e27a":"markdown","425a710c":"markdown"},"source":{"27ae0f2a":"import os\nimport sys\nprint(sys.version)\nprint('\\nContents of input:\\n %s' % os.listdir('..\/input'))\nprint('\\nContents of snappy:\\n %s' % os.listdir('..\/input\/snappy'))\nprint('\\nContents of snap-memetracker-raw:\\n %s' % os.listdir('..\/input\/snap-memetracker-raw'))\nprint('\\nContents of the working directory:\\n %s' % os.listdir('.'))","d8bca59c":"! cp ..\/input\/snappy\/snap.py snap.py\n! cp ..\/input\/snappy\/_snap.so _snap.so\n! cp ..\/input\/snappy\/setup.py setup.py\n! pwd\n! ls -l","f24847fa":"! python setup.py install","f871b2eb":"import snap","3f3aea8b":"import pandas as pd","8189652e":"go = ''\nvariant = ['']\ncontainer = []\n\nfor line in open('..\/input\/snap-memetracker-raw\/clust-qt08080902w3mfq5.txt\/clust-qt08080902w3mfq5.txt').readlines():\n    if not (line.startswith('\\t\\t') or line.startswith('\\t') or line=='\\n'):\n        # find the header line\n        if 'you can put lipstick on a pig' in line:\n            go = 'on'\n        else:\n            go = 'off'\n    # get quotes of the phrase\n    if go == 'on':\n        # get variant\n        if line.startswith('\\t') and not line.startswith('\\t\\t'):\n            variant[0] = line.split('\\t')[3]\n        # get quote\n        if line.startswith('\\t\\t'):\n            container.append(variant + line.strip().split('\\t'))\n\ndf = pd.DataFrame(container, columns=['Variant','Date', 'Fq', 'Type', 'URL'])\n\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.info()\ndf.sample(n=10)","2762f0fd":"i = 0\n# SNAP hash tables to store URL to NodeId mapping\nurlNode = snap.TStrIntH()\nnodeUrl = snap.TIntStrH()\n\nfor item in df['URL'].unique():\n    urlNode[item] = i\n    nodeUrl[i] = item\n    i += 1","e83d12c0":"nodeUrl[42]","f13c1483":"urlNode['http:\/\/methlabhomes.com\/2008\/11\/foreclosure-lipstick-tricks-that-you-should-know']","8707c33e":"urlNode.IsKey('http:\/\/methlabhomes.com\/2008\/11\/foreclosure-lipstick-tricks-that-you-should-know')","ce419038":"df['NodeId'] = df['URL'].apply(lambda x: urlNode[x])","c968d9ac":"%%time\n\nG1 = snap.TNGraph.New()\n\nfor page in df['URL'].unique():\n    G1.AddNode(urlNode[page])\n# iterators for:\ni = 0 # lines - total\nj = 0 # pages of interest \nk = 0 # outgoing links\nl = 0 # links to other pages of interest\n\ncontainer = []\n\nsource = ''\nmoment = ''\ncopyLinks = 'off'\n\nfor line in open('..\/input\/snap-memetracker-raw\/quotes_2008-09.txt\/quotes_2008-09.txt').readlines():\n    lineSp = line.strip().split('\\t')\n    \n    if lineSp[0] == 'P':\n        if urlNode.IsKey(lineSp[1]):\n            source = lineSp[1]\n            copyLinks = 'on'\n            j += 1\n            \n    if lineSp[0] == 'T':\n        moment = lineSp[1]\n    \n    if line == '\\n':\n        copyLinks = 'off'\n\n    if copyLinks == 'on':\n        if lineSp[0] == 'L':\n            k += 1\n            if urlNode.IsKey(lineSp[1]):\n                try:\n                    G1.AddEdge(urlNode[source], urlNode[lineSp[1]])\n                    container.append([urlNode[source], moment, urlNode[lineSp[1]]])\n                except:\n                    pass\n                l += 1\n\n    i += 1\nprint('''\nTotal lines processed: \\t\\t\\t %d\nPages of interest: \\t\\t\\t %d\nOutgoing links: \\t\\t\\t %d\nLinks to other pages of interest: \\t %d\n''' % (i,j,k,l))","5efa8e41":"dfL = pd.DataFrame(container, columns=['Source','Date', 'Target'])\ndfL.info()\ndfL.sample(n=10)","eafa75a0":"snap.PrintInfo(G1, 'Who points to whom', 'output.txt', False)\nfor line in open('output.txt').readlines():\n    print (line.strip())","378e3c4c":"# Suprematist comment #1\n#snap.DelZeroDegNodes(G1)\nsnap.PrintInfo(G1, 'Who points to whom - trimmed disconnected nodes', 'output.txt', False)\nfor line in open('output.txt').readlines():\n    print (line.strip())","544845b2":"from IPython.display import Image\n\nNIdColorH = snap.TIntStr64H()\n\nfor Node in G1.Nodes():\n    NIdColorH[Node.GetId()] = 'black'\n\nsnap.DrawGViz(G1, snap.gvlNeato, 'f001.png', 'Study 1. Who points to whom', False, NIdColorH )\nImage(filename='f001.png') ","94101b1d":"# Suprematist comments #2 and #3\n#snap.DelSelfEdges(G1)\n#snap.DelZeroDegNodes(G1)\n\nsnap.DrawGViz(G1, snap.gvlNeato, 'f002.png', 'Study 2. Who points to whom - The White Square')\nImage(filename='f002.png') ","27b7aba4":"snap.PrintInfo(G1, 'Who points to whom - trimmed disconnected nodes, removed self-edges', 'output.txt', False)\nfor line in open('output.txt').readlines():\n    print (line.strip())","761c0fad":"MxWcc = snap.GetMxWcc(G1)\nMxScc = snap.GetMxScc(G1)\n\nfor Node in G1.Nodes():\n    if MxWcc.IsNode(Node.GetId()):\n        NIdColorH[Node.GetId()] = 'white'\n        if MxScc.IsNode(Node.GetId()):\n            NIdColorH[Node.GetId()] = 'red'\n    else:\n        NIdColorH[Node.GetId()] = 'black'\n        \nsnap.DrawGViz(G1,\n              snap.gvlNeato, \n              'f003.png', \n              'Study 3. Who points to whom - highlighted maximum connected components\\n'\n              '(red for strong, white for weak ties)',\n              False,\n              NIdColorH)\n\nImage(filename='f003.png') ","ef39ee39":"df['Color'] = df['Type'].map({'B': 'black', 'M': 'yellow'})\ndf.sample(n=10)","63a4110e":"for item in df[['NodeId', 'Color']].values:\n    NIdColorH[item[0]] = item[1]\n\nsnap.DrawGViz(G1, snap.gvlNeato, 'f004.png', 'Study 4. Who points to whom - highlighted mainstream media',False, NIdColorH)\nImage(filename='f004.png') ","79a41bc3":"PRankH = snap.TIntFlt64H()\n\nsnap.GetPageRank(G1, PRankH)\n\nPRankH.SortByDat(False)\n\nl = 0\nfor item in PRankH:\n    if l < 10:\n        print (nodeUrl[item], PRankH[item])\n        l += 1\n        NIdColorH[item] = 'purple'\n    else:\n        NIdColorH[item] = NIdColorH[item]","21470bca":"snap.DrawGViz(G1, snap.gvlNeato, 'f005.png', 'Study 5. Who points to whom - highlighted mainstream media, '\n              'and top 10 identified by Page Rank',False, NIdColorH)\nImage(filename='f005.png')","4eb48705":"df.to_csv('nodes.csv')\ndfL.to_csv('links.csv')\n\nFOut = snap.TFOut('g1.graph')\nG1.Save(FOut)\nFOut.Flush()\n\nFOut = snap.TFOut('NIdColorH.hash')\nNIdColorH.Save(FOut)\nFOut.Flush()","c821a9c2":"<h1>1. Why SNAP? Why here? Why now?<\/h1>\n\nStanford Network Analysis Platform (SNAP) is a general purpose network analysis and graph mining library. <a href=http:\/\/snap.stanford.edu\/snappy\/>Snap.py<\/a> is a Python proxy to numerous SNAP functions implemented in C++.\n\nSince beta version became available for Python 3.6 on Linux, it is a good time to get started here on Kaggle.\n\nOn top of that, we, members of the <a href=http:\/\/ods.ai>Open Data Science<\/a> community, have a tradition of going through Stanford classes after each run of the <a href=http:\/\/mlcourse.ai>Open Machine Learning Course<\/a> as a bonus track for survivors keen to focus on something specific. Prior runs celebrated cs231n (Convolutional Neural Networks for Visual Recognition). \n\nNow we dive into the Graph Theory and enjoy cs224w (Analysis of Networks).\n\nThis is a primer on setting up environment and use of Snap.py for network analysis problems participans may encounter while struggling through the curricula (beware - it is quite broad and that very cross-functional nature makes the exercise tough enough).\n  \n  <br>\n  \n  \n<h1>2. Setting up Snap.py on Kaggle<\/h1>\n\nWe keep installation files downloaded from <a href=http:\/\/snap.stanford.edu\/snappy\/release\/beta\/>Snap.py repository<\/a> in a dataset Snap.py - and they appear in the **\"..\/input\"**. Let's copy them into the working directory and istall the package in a few steps.\n\n<h3>Step 0. Check environment and files<\/h3>","ea8db281":"<h3>Let's look into the raw data and build who-points-to-whom network<\/h3>\n\n>**Format:** files contain the records like this separated by blank lines:\n\n    P       http:\/\/blogs.abcnews.com\/politicalpunch\/2008\/09\/obama-says-mc-1.html\n    T       2008-09-09 22:35:24\n    Q       that's not change\n    Q       you know you can put lipstick on a pig\n    Q       what's the difference between a hockey mom and a pit bull lipstick\n    Q       you can wrap an old fish in a piece of paper called change\n    L       http:\/\/reuters.com\/article\/politicsnews\/idusn2944356420080901?pagenumber=1&virtualbrandchannel=10112\n    L       http:\/\/cbn.com\/cbnnews\/436448.aspx\n    L       http:\/\/voices.washingtonpost.com\/thefix\/2008\/09\/bristol_palin_is_pregnant.html?hpid=topnews\n\n\n>where the first letter of the line encodes: \u00a0 <br><br>\n> P: URL of the document  \nT: time of the post (timestamp)  \nQ: phrase extracted from the text of the document  \nL: hyper-links in the document (links pointing out to other documents on the web)  \n>\n>Note some documents have zero phrases or zero links.\n","05d07014":"<h1>3. Example: network exploration - news propagation on Internet<\/h1>\n<br><br>\nA. Problem understanding<br>\nB. Data at hand<br>\nC. News network and meme flow<br>\n\n<h3>A. Problem understanding<\/h3>\n\nPhony stuff circulating all over the Make News Media has sparked outrage in an alien civiliation that observed humanity since the very good old times and invasion troops landed on Earth. Subsequent dramatic drop in lipstick sales suggests that the root cause somewhat relates to the phrase:\n\n<h1 align=center><i>\"you can put lipstick on a pig<br> but it's still a pig\"<\/i><\/h1>\n<br><br>\nHordes of heavily armed hogs marching city streets only reinforce the first grieve guesses.\n\n![321](https:\/\/vignette.wikia.nocookie.net\/dukenukem\/images\/2\/2d\/Pigcop.png)\n\nYour mission, should you choose to accept it, is to identify key actors in the Make News Media network so that Duke Nukem (a.k.a. The King of the World, a.k.a. The Ultimate Alien Ass Kicker) could take care of them.\n\n<h3>B. Data at hand<\/h3>\n","565d9387":"There are two types of media these pages belong to - blogs encoded as **`B`** and mainstream media labeled as **`M`**. We can color-code them:","031cc6dc":"We are going to use the [data from Memetracker](http:\/\/www.memetracker.org\/data.html) that capture meme (key phrase) propagation through media - blogs and news sites.\n\nFirst, we look into keyword clusters - phrases mutate and the original quote \"you can put lipstick on a pig but it's still a pig\" has multiple variants. Luckily, data comes preprocessed. Let's parse through the text file and pick lines of interest.\n\n> **Data format:** Tab separated file with the following nested structure. Each block of the data has the following structure:\n\n    A:  <ClSz>  <TotFq>  <Root>  <ClId>\n    B:          <QtFq>   <Urls>  <QtStr>  <QtId>\n    C:                   <Tm>    <Fq>     <UrlTy>  <Url>\n \n>**ClSz**: number of different phrases in the cluster (number of B records).  \n**TotFq**: total frequency (number of mentions) of all the phrases in the cluster.  \n**Root**: root phrase of the cluster. Representative phrase from the cluster of phrases.  \n**ClId**: cluster id.  \n**QtFq**: total frequency (number of mentions) of the phrase.  \n**Urls**: number of urls where the phrase appeared (number of C records).  \n**QtStr**: phrase string.  \n**QtId**: phrase id.  \n**Tm**: time when the article\/post **Url**: was published.  \n**Fq**: number of times phrase **QtStr**: was mentioned at the **Url**:.  \n**UrlTy**: type of the url: **`B`**: blog, **`M`**: mainstream media.  \n**Url**: URL of the blog post\/news article.` ","0536845d":"Note the amount of nodes pointing exclusively to themselves - these do not add up much to the model as well. We remove them and get a clearer picture.  \n\nAlso, let's explore the standard output - one with uncolored nodes and reduce Data-Ink ratio.","da475aa4":"<h1>4. Visualization<\/h1>\n\n> <i align=right>\u201cIn 1913, trying desperately to liberate art from the ballast of the representational world, I sought refuge in the form of the square.\u201d<\/i>   \n> <b><i>\u2015 Kasimir Malevich<\/i><\/b>\n\nSnap.py uses two default engines for plotting:  \n* GraphViz  \n* GnuPlot\n\nFormer is a part of standard Python environment on Kaggle, while latter has to be set up separately (enable Internet access in order to do so). All you need is to add a line from the snippet below to your kernel:  \n>\n       ! apt-get -y install gnuplot -qq > \/dev\/null\n\nKeep in mind that visualzation of a graph containing more than 1000 nodes takes some time. If you have to make plots and experiment with layouts for big networks, <a href=https:\/\/gephi.org\/>Gephi<\/a> might be a good tool of choice. ","bf78ee19":"<h3>Let's save the data - we will return and load it later.<\/h3>\n\nNext time we are going to look into statistics of graphs, their meaning, applications, and try to answer the question whether or not news propagate on Internet at random. Stay tuned!","e7130f8d":"<h3>Note the difference in numbers: we see 1092 links in the dataframe dfL, and 969 unique directed links in $G_1$ <\/h3>\n\nThe data structure at hand <a href=http:\/\/snap.stanford.edu\/snappy\/doc\/reference\/graphs.html>TNGraph<\/a> - directed graph (single directed edge between an ordered pair of nodes) does not allow for parallel edges.\n\nTNEANet - is an alternative data structure that supports multiple attributed edges.\n\nPay attention to multiple dangling nodes (those with zero-degree) - pages that contain a quote and have no outgoing links - do not add that much structurally to the model. We can remove them. Or keep those - that might correspond to a narrative like \"these are the links we have established so far\".","62e26c17":"Once we have constructed the first straightforward projection of the data into graph $G_1$, let's look into it's statistics:","4eedae64":"<h3>C. News network and meme flow<\/h3>  \n   \nSo far, we have extracted a timestamped list of links between pages that have mentioned the \"lipstick on a pig\" in any form.","a27a2f9b":"![](https:\/\/habrastorage.org\/webt\/07\/kl\/va\/07klva15cy_pef0gtzooan2x7zi.jpeg)\n<h1 align=center>5. <a href=http:\/\/ods.ai>JOIN US<\/a>!<\/h1>\n<center>The party occupies a dedicated channel #class_cs224w\n<br><br>\n    <b>NOTE:<\/b> refer to <b>CLASS_CS224W<\/b> while filing in the registration form <\/center>\n<br><br><br>\n<br><br><br>\n<br><br><br>","7ad0e004":"![](https:\/\/memegenerator.net\/img\/instances\/24338389\/what-are-you-waiting-for-christmas.jpg)","54be9d61":"<h3> Step 1. Copy files<\/h3>","3da9c053":"Let's peer into statistics once again.","e0ea9006":"Let's establish connections between pages.\n\nWe have timed records of page publication - those with hyperlinks to other pages' URLs - and they correspond to nodes in a graph that is going to be our model, formally:\n\n<h1>Let $G_1$ denote a directed graph, where each web page (URL) is encoded as a node $N_i \\in \\{N_i, .., N_n\\}$ and a hyperlink pointing from a given page $N_i$ to a page $N_j$ results in an edge $E_{ij} = e(i,j)$.<\/h1>","ade3c9de":"SNAP comes with multiple hash types for mapping between data types. Refer to the documentation on <a href=http:\/\/snap.stanford.edu\/snappy\/doc\/reference\/composite.html>composite types<\/a> for the complete list of features.  \n\nThey behave in a way similar to Python dicts:","dc06dddf":"![](https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg)\n\n<h3 align=center>Open Machine Learning Course: Aftermath Christmas Run Bonus Pack<\/h3>\n\n<center>by <a href=https:\/\/www.linkedin.com\/in\/vadymsafronov\/>Vadym Safronov<\/a>\n\n\n<br><br>\n\n~ Introduction~<br>\n\n<\/center>\n\n<h1 align=center>Getting started with Snap.py<br><br>or<br><br>FREE COMPUTATION FOR <a href=http:\/\/web.stanford.edu\/class\/cs224w\/>CS224W<\/a> PROBLEMS<\/h1>\n<center><a href=http:\/\/snap.stanford.edu\/snappy\/index.html>Snap.py<\/a> for Python 3.6 + Kaggle Kernels = fast processing of large networks!<\/center>\n\n\n<hr><br><br><br>\n\n<h3>Outline<\/h3>\n1. Why <a href=https:\/\/www.kaggle.com\/snap>SNAP<\/a>? Why here? Why now?\n1. Setting up Snap.py on Kaggle\n1. Example: network exploration - news propagation on Internet\n1. Visualization\n1. <a href=http:\/\/ods.ai>Join us<\/a>!\n1. Extra mile\n1. Acknowledgements\n\n<br><br><br>\n","2ead24b2":"<h1 align=center>The Winter Has Come<\/h1>","9bc297d0":"\n<h1>6. Extra mile<\/h1>\n  \nSome ideas for you to implement and have fun with the given data:  \n* Remove Suprematist comments from the code above (see cells In[15] and In[17]) and enjoy faster processing of figures and variability of graph statistics between purges - at cost of excluding disconnected nodes and self-edges from the model.  \n* Implement an alternative mapping for the Make News Media network.  \n* Reproduce Figure 1 from <a href=http:\/\/www.memetracker.org\/quotes-kdd09.pdf>Meme-tracking and the Dynamics of the News Cycle<\/a>.\n\n\n\n<h3>NB: You can also install Snap.py with less output using code from the snippet below:<\/h3>\n\n    ! cp ..\/input\/snappy\/snap.py snap.py\n    ! cp ..\/input\/snappy\/_snap.so _snap.so\n    ! cp ..\/input\/snappy\/setup.py setup.py\n     \n    ! python setup.py install > \/dev\/null\n    ! apt-get -y install gnuplot -qq > \/dev\/null\n\n<br><br><br>\n<br><br><br>\n<br><br><br>\n\n\n<h1>7. Acknowledgements<\/h1>\n  \nThanks to all those who made it happen, especially:  \n* Jure Leskovec and the SNAP Group - for making this class and keeping it available\n\n\n\n\n<br><br><br>\n<br><br><br>\n<br><br><br>\n\n\n\nThis text is distributed under the <a href=https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/>Creative Commons CC BY-NC-SA 4.0<\/a> license.","55474971":"<h3>Step 2. Run installation script<\/h3>","54e2e27a":"Let's identify the most important web-pages - we will use  Page Rank (a.k.a. the Google Algorithm - a probability of random walker visiting certain page) and highlight top 10 nodes - those most likely to get a visit from a web surfer(because other well referred pages point to them).","425a710c":"There is a noticeable pattern that catches eye - some nodes form connected components.  \n\nLet's explore those.  \n\nIn order to do so, we shade all nodes that belong elsewhere except for the biggest weakly connected component (a set of nodes that would be connected via some path if we view our graph as undirected) and a strongly connected component (one that considers connectivity through directed edges)."}}