{"cell_type":{"b82cac75":"code","08fbacbc":"code","56655107":"code","c5711d52":"code","9b6d457e":"code","3b69a2e9":"code","3bc81a2d":"code","25bbaf9e":"code","3d7b8f44":"code","fb1b3679":"code","2c58d58b":"code","3ea86693":"code","8081d9c6":"code","4f0ce9e5":"code","51c55132":"code","189456be":"code","f14db7a7":"code","eb416f02":"code","aac70cc2":"code","7762b486":"code","42b7c054":"code","988f33d7":"code","994b59b9":"code","80e5fe80":"code","ba1a21f1":"code","dc765877":"code","c2a7a6b9":"code","bc1355e5":"code","115ccec8":"code","416ef984":"code","c8df6072":"code","c6d4f1f2":"code","3b328dd8":"code","3f7ee26c":"code","567c2537":"code","4ae9804d":"code","98ed96ef":"code","bc955762":"code","8aeaabb2":"code","d83459e6":"code","1ecfc5a3":"code","a2ae2fb5":"code","ad3ac6c9":"code","b4217b9b":"code","254db030":"code","c7ad9d6d":"code","f9385c2c":"code","af922667":"code","16458239":"code","805b0d66":"code","9b4da335":"code","eeba5a5d":"code","2018318a":"code","ad69c30e":"code","a9ab2f9c":"code","aa4b8ffa":"code","a712bf32":"code","9aba2371":"code","a5eca6d6":"code","c3579be1":"code","9a3131fd":"code","24173f60":"code","0ba4362b":"code","a1db43a1":"code","922e420f":"code","e847a236":"code","6f7741d9":"code","81d52f5d":"code","38245236":"code","68203d15":"code","cd30ef18":"code","e6f27462":"code","2c6629c3":"code","e4e077c5":"code","c58bd73b":"code","d6c8e48f":"code","14ccecff":"code","a1bad023":"code","b32ab317":"code","2102f2a6":"code","55afdce8":"code","b25c8508":"code","0be7b112":"code","0dc8302e":"code","11c9ad14":"code","c2c35963":"code","2fabfeaf":"code","772fc4ac":"code","7843e102":"code","ffe341fa":"code","fafe628f":"code","58da09c0":"code","9eb47104":"code","3f0d25a3":"code","c6117294":"markdown","e40e373d":"markdown","23508aa1":"markdown","8f7468f5":"markdown","8060038d":"markdown","667492dd":"markdown","b753e65f":"markdown","b9cf0a75":"markdown","c6e887dd":"markdown","d91306c9":"markdown","a7b78845":"markdown","4877947a":"markdown","cefae0c1":"markdown","33d08dd1":"markdown","8b3db846":"markdown","acd1f55c":"markdown","f6651c0a":"markdown","e9cdb525":"markdown","1703ca91":"markdown","178e496f":"markdown","dad42fc4":"markdown","6edb7d6d":"markdown","b533081a":"markdown","a554c43c":"markdown","dadf65cf":"markdown","212b60ea":"markdown","96d2a41a":"markdown","cacd7383":"markdown","14163a74":"markdown","7098c69d":"markdown","4a7710f8":"markdown","8481161d":"markdown","5abed291":"markdown","d59ae033":"markdown","a4c7fa7b":"markdown","35c47c67":"markdown","302a7946":"markdown","655a7469":"markdown","65f80862":"markdown"},"source":{"b82cac75":"# Import pandas and sklearn\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the data\ndata = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\ndata.shape","08fbacbc":"# To view all columns\npd.set_option('display.max_columns', None)","56655107":"# Preview data\ndata.head()\n","c5711d52":"# Select target\ny = data.Price\ny.isnull().count()","9b6d457e":"# Select features\nmelb_predictors = data.drop(['Price'], axis=1)\nmelb_predictors.shape","3b69a2e9":"# Check the data type of each columns\nmelb_predictors.dtypes","3bc81a2d":"# To keep things simple, we'll use only numerical predictors\n# Remove all object types\nX = melb_predictors.select_dtypes(exclude=['object'])\nX.shape","25bbaf9e":"# Verify the data types of the dataset\nX.dtypes","3d7b8f44":"# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","fb1b3679":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","2c58d58b":"# Get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n\ncols_with_missing","3ea86693":"# long approach\n\ntest_cols_with_missing = []\nfor col in X_train.columns:\n    if X_train[col].isnull().any():\n        test_cols_with_missing.append(col)\n        \ntest_cols_with_missing","8081d9c6":"# Drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)","4f0ce9e5":"reduced_X_train","51c55132":"reduced_X_valid","189456be":"print(\"MAE from Approach 1 (Drop columns with missing values):\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","f14db7a7":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer() # Create an object from SimpleImputer() class\n# fit_transform(X[, y]): Fit to data, then transform it.\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n# transform(X): Impute all missing values in X.\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))","eb416f02":"imputed_X_train","aac70cc2":"imputed_X_valid","7762b486":"# Imputation removed the original column names\n# Following lines replacing 1, 2, 3, ... column names with the actual column names\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns","42b7c054":"# Returning the column names\nimputed_X_train","988f33d7":"imputed_X_valid","994b59b9":"print(\"MAE from Approach 2 (Imputation):\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))","80e5fe80":"# Make copy to avoid changing original data (when imputing)\nX_train_plus = X_train.copy()\nX_valid_plus = X_valid.copy()","ba1a21f1":"cols_with_missing","dc765877":"# Make new columns indicating what will be imputed\n# E.g 'Car_was_missing'\nfor col in cols_with_missing:\n    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()","c2a7a6b9":"X_train_plus","bc1355e5":"X_valid_plus","115ccec8":"# Imputation\n# Ref. approach two for more details\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\nimputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))","416ef984":"imputed_X_train_plus","c8df6072":"# Imputation removed column names; put them back\n# Ref. approach two for more details\nimputed_X_train_plus.columns = X_train_plus.columns\nimputed_X_valid_plus.columns = X_valid_plus.columns","c6d4f1f2":"imputed_X_train_plus","3b328dd8":"print(\"MAE from Approach 3 (An Extension to Imputation):\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))","3f7ee26c":"# Recall original dataset (without removing any columns)\ndata.shape","567c2537":"# Set target dataset\ny = data.Price\ny.shape","4ae9804d":"# Set features\nX = data.drop(['Price'], axis=1)\nX.shape","98ed96ef":"# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)","bc955762":"X_train_full.shape","8aeaabb2":"X_valid_full.shape","d83459e6":"X_valid_full.columns","1ecfc5a3":"X_valid_full.head()","a2ae2fb5":"# Drop columns with missing values (simplest approach)\ncols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \ncols_with_missing","ad3ac6c9":"X_train_full.drop(cols_with_missing, axis=1, inplace=True)\nX_valid_full.drop(cols_with_missing, axis=1, inplace=True)","b4217b9b":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n# Experiment by reducing the number 10 to 5\n# nunique() Count number of distinct elements in specified axis.\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\nlow_cardinality_cols ","254db030":"# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\nnumerical_cols","c7ad9d6d":"# Select low cardinality and numerical columns only (drop other columns)\nmy_cols = low_cardinality_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_train.shape","f9385c2c":"X_train.head()","af922667":"cat_variables = (X_train.dtypes == 'object')\n# Similar to follwoing\n# cat_variables = pd.Series(X_train.dtypes == 'object')\ntype(cat_variables)\n","16458239":"cat_variables","805b0d66":"# Similar to the outcome of a conditional statement\ncat_variables[cat_variables]","9b4da335":"object_cols = list(cat_variables[cat_variables].index)\nobject_cols ","eeba5a5d":"drop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\ndrop_X_train.shape","2018318a":"print(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","ad69c30e":"from sklearn.preprocessing import OrdinalEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()","a9ab2f9c":"object_cols","aa4b8ffa":"# Apply ordinal encoder to each column with categorical data\n# Here we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels\n# We can expect an additional boost in performance if we provide better-informed labels for all ordinal variables.\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\nlabel_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\nlabel_X_train","a712bf32":"print(\"MAE from Approach 2 (Ordinal Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))","9aba2371":"object_cols","a5eca6d6":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\nOH_cols_train","c3579be1":"# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\nOH_cols_train","9a3131fd":"# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\nnum_X_train","24173f60":"# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\nOH_X_train","0ba4362b":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","a1db43a1":"# take the original dataset\ndata.shape","922e420f":"# Define target and feature datasets\ny = data.Price\nX = data.drop(['Price'], axis=1)","e847a236":"# Split data in to training and validation sets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","6f7741d9":"# Select categorical columns with relatively low cardinality\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == 'object']\ncategorical_cols ","81d52f5d":"# Select numerical columns \nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\nnumerical_cols ","38245236":"selected_columns = categorical_cols + numerical_cols\nX_train = X_train_full[selected_columns].copy()\nX_valid = X_valid_full[selected_columns].copy()","68203d15":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder","cd30ef18":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')","e6f27462":"# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])","2c6629c3":"# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols)])","e4e077c5":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)","c58bd73b":"from sklearn.metrics import mean_absolute_error\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])","d6c8e48f":"# fit model\nmy_pipeline.fit(X_train, y_train)","14ccecff":"# get predications\npreds = my_pipeline.predict(X_valid)\npreds","a1bad023":"# Evaluate\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE: ', score)","b32ab317":"# take the original dataset\ndata.shape","2102f2a6":"# Select subset of features (predictors)\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\n\n# Select target\ny = data.Price","55afdce8":"# define a pipeline that uses an imputer to fill in missing values and a random forest model to make predictions.\n# Using a pipeline will make the code remarkably straightforward.\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nmy_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()), ('model', RandomForestRegressor(n_estimators=50, random_state=0))])","b25c8508":"from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\\n\", scores)","0be7b112":"print(\"Average MAE score (across experiments):\")\nprint(scores.mean())","0dc8302e":"# take the original dataset\ndata.shape","11c9ad14":"# Select subset of features (predictors)\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\n\n# Select target\ny = data.Price","c2c35963":"# Separate data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)","2fabfeaf":"# import the scikit-learn API for XGBoost (xgboost.XGBRegressor)\nfrom xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\nmy_model.fit(X_train, y_train)","772fc4ac":"# predict using the model\npredictions = my_model.predict(X_valid)","7843e102":"# Evaluate the model\nfrom sklearn.metrics import mean_absolute_error\n\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","ffe341fa":"# set n_estimator to 500\n# Typical value ranges from 100 to 1000, though this depends a lot on 'learning_rate' parameter\nmy_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train)","fafe628f":"my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             verbose=False)","58da09c0":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","9eb47104":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","3f0d25a3":"## Target leakage","c6117294":"### XGBoost steps\n\n1. First, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble.\n2. These predictions are used to calculate a loss function (like mean squared error, for instance).\n3. Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss. (Side note: The \"gradient\" in \"gradient boosting\" refers to the fact that we'll use gradient descent on the loss function to determine the parameters in this new model.)\n4. Finally, we add the new model to ensemble\n5. repeat!\n\n![](https:\/\/i.imgur.com\/MvCGENh.png)","e40e373d":"# Access and Split datasets","23508aa1":"# Pipelines\n\nPipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step","8f7468f5":"### Select low cardinality and numerical columns only (drop other columns)","8060038d":"## 1. Drop columns with missing values\nSince we are working with both training and validation sets, we are careful to drop the same columns in both DataFrames.","667492dd":"## Target leakage\n\nTarget leakage occurs when your predictors include data that will not be available at the time you make predictions.\n\nTo prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded.\n\nE.g. The raw data shows a strong relationship between \"got_pneumonia\" and \"took_antibiotic_medicine\". However, people take antibiotic medicines AFTER getting pneumonia in order to recover.\n\n\n![](https:\/\/i.imgur.com\/y7hfTYe.png)\n","b753e65f":"While the isnull() method is useful, sometimes we may wish to evaluate whether any value is missing in a Series.\n\nhttps:\/\/chartio.com\/resources\/tutorials\/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\/","b9cf0a75":"Cross-Validation is best for following scenarios\n* For small datasets, where extra computational burden isn't a big deal.\n* For larger datasets, a single validation set is sufficient. \n\nWith Cross-Validation we no longer need to keep track of separate training and validation sets. So, especially for small datasets, it's a good improvement!","c6e887dd":"## 1. Evaluate Drop categorical variables","d91306c9":"# Replacing numerical missing values\n\nMeasure quality of missing value approaches using MAE from Random Forest Model\n1. Drop Columns with Missing Values\n2. Imputation\n3. Extension to Imputation","a7b78845":"## 2. Imputation\nUse SimpleImputer() >> mean to replace missing values with the mean value along each column.","4877947a":"learning_rate: Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in.\n\nIn general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets learning_rate=0.1.","cefae0c1":"## Data preparation","33d08dd1":"## Train-Test Contamination\n\nThis occurs when you aren't careful to distinguish training data from validation data\n\nFor example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.","8b3db846":"### Step 3: Create and Evaluate the Pipeline\n\nUse the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. Pipelines make the life easier in following manner.\n\n* With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)\n    \n* With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)\n","acd1f55c":"n_estimators: specifies how many times to go through the modeling cycle. It is equal to the number of models that we include in the ensemble. This parameter dramatically affects model accuracy and training speed\n\n* Too low a value causes underfitting, which leads to inaccurate predictions on both training data and test data.\n* Too high a value causes overfitting, which causes accurate predictions on training data, but inaccurate predictions on test data","f6651c0a":"### Step 2: Define the model\n\nDefine a random forest model with the familiar RandomForestRegressor class","e9cdb525":"## 3. One-Hot encoding\n\nCreates new columns indicating the presence (or absence) of each possible value in the original data. One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).\n\n![](https:\/\/i.imgur.com\/TW5m0aJ.png)","1703ca91":"We typically want a single measure of model quality to compare alternative models. So we take the average across experiments","178e496f":"### Obtain a list of all of the categorical variables in the training data by verifying dtype == 'object'","dad42fc4":"# XGBoost (extreme gradient boosting)\n\nLearn how to build and optimize models with gradient boosting. This method dominates many Kaggle competitions and achieves state-of-the-art results on a variety of datasets.\n\nWe refer to the random forest method as an \"ensemble method\". Ensemble methods combine the predictions of several models (e.g., several trees of random forests).\n\nNext, we'll learn about another ensemble method called gradient boosting.\n\nIt begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.)","6edb7d6d":"### Select categorical columns with relatively low cardinality ","b533081a":"### Select numerical columns","a554c43c":"early_stopping_rounds: offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving\n\nIt's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n\nSince random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. Setting early_stopping_rounds=5 is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores.\n\nWhen using early_stopping_rounds, you also need to set aside some data for calculating the validation scores - this is done by setting the eval_set parameter.","dadf65cf":"## 3. An Extension to Imputation\nimpute the missing values, while also keeping track of which values were imputed.","212b60ea":"If you specify axis=1 you will be removing columns. If you specify axis=0 you will be removing rows from dataset. This is compulsory.","96d2a41a":"Three steps to construct full pipeline\n1. Define preprocessing steps\n2. Define the model\n3. Create and evaluate pipeline","cacd7383":"### Drop columns with missing values","14163a74":"# Replacing categorical missing values\n\nYou will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first. There are three approaches to prepare categorical variables.\n\n1. Drop categorical variables\n2. Ordinal encoding\n3. One-Hot encoding","7098c69d":"# Cross-Validation\n\nCross-Validation is for better model performance measuring. \n\nIn cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality.\n\nFor example, divide the data into 5 pieces, each 20% of the full dataset. In this case, we say that we have broken the data into 5 \"folds\". Then, we run one experiment for each fold. \n\n* Dark blue represent Validation dataset for each fold\n* White (rest of the data) represent the training dataset for each fold\n\n![](https:\/\/i.imgur.com\/9k60cVA.png)","4a7710f8":"# Data Leakage\n\nData leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.\n\nIn other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate.\n\nThere are two main types of leakage: \n\n1. Target leakage \n2. Train-test contamination","8481161d":"## 3. One Hot Encoding\n\nWe use the OneHotEncoder class from scikit-learn to get one-hot encodings. There are a number of parameters that can be used to customize its behavior.\n\n    handle_unknown='ignore' >> avoid errors when the validation data contains classes that aren't represented in the training data\n    \n    setting sparse=False >> ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n","5abed291":"We obtain the cross-validation scores with the cross_val_score() function from scikit-learn. We set the number of folds with the cv parameter.\n\nScoring parameters: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter \n\nIf you'd like to learn more about hyperparameter optimization, you're encouraged to start with grid search, which is a straightforward method for determining the best combination of parameters for a machine learning model. Thankfully, scikit-learn also contains a built-in function GridSearchCV() that can make your grid search code very efficient!","d59ae033":"n_jobs: On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help","a4c7fa7b":"### Step 1: Define Preprocessing Steps\n\nwe use the ColumnTransformer class to bundle together different preprocessing steps.\n\n* It imputes missing values in numerical data\n* It imputes missing values and applies one-hot encoding to categorical data","35c47c67":"## 2. Evaluate Ordinal Encoding\n\nScikit-learn has a OrdinalEncoder class that can be used to get ordinal encodings. We loop over the categorical variables and apply the ordinal encoder separately to each column.","302a7946":"## 1. Drop categorical variables\n\nThe easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.","655a7469":"### Function for comparing different approaches","65f80862":"## 2. Ordinal encoding\n\nAssign unique integer to each categorical value. Works well with ordinal values such as None(0), low(1), medium(2), High(3)\n\n![](https:\/\/i.imgur.com\/tEogUAr.png)"}}