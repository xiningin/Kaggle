{"cell_type":{"c6637009":"code","c311f7a7":"code","065560a3":"code","fb1ac7ac":"code","5e6798b0":"code","35c7e339":"code","797c6a1e":"code","fc87644c":"code","e311e64b":"code","cb9f038c":"code","3c6e5983":"code","2c737790":"code","94f10d79":"code","10544616":"code","49ac01d6":"code","b2809467":"code","76326699":"code","6a077d69":"code","9e19c841":"code","6c6b53d2":"code","13bd450b":"code","0b50494d":"code","eb3e32da":"code","e9fa9774":"code","c354c009":"code","334b59e8":"code","fd7d7af8":"markdown","3b0301b0":"markdown","8b88c4a7":"markdown","edf2d150":"markdown","6328653d":"markdown","b07c1a76":"markdown","ebeaeef5":"markdown","6ca15f3a":"markdown","f80eb02d":"markdown","a0242bb1":"markdown","b6f23561":"markdown","bdf4dfc2":"markdown","8946ea69":"markdown","70eae991":"markdown","783792fa":"markdown","43ae5477":"markdown","7edcb7c1":"markdown","d1231395":"markdown","4ed399ac":"markdown","43a6db9e":"markdown","5b63a168":"markdown","9086ba48":"markdown","fd67d95d":"markdown","ce69328e":"markdown","85dbffc3":"markdown","4a83676a":"markdown","4bdfd136":"markdown","aba034ff":"markdown","72c862bc":"markdown","6575b217":"markdown","6ca7ad0c":"markdown","ed07b9db":"markdown","c4ef21a3":"markdown","dc38b339":"markdown","62d9c900":"markdown","2ef5a7ff":"markdown","2cee8fa7":"markdown","81b4f594":"markdown","2a436db5":"markdown","f89edd9f":"markdown","67c90726":"markdown","2669b6cc":"markdown","6e404d76":"markdown","8f7d951f":"markdown","7515b146":"markdown","825c92c3":"markdown","a13cfdc8":"markdown","9ff4a15d":"markdown","76f9a220":"markdown","78c83c4f":"markdown","8a19e25c":"markdown","93a27613":"markdown","55a1c288":"markdown","d20e25b0":"markdown","516d6527":"markdown","031b1822":"markdown","d5535a6b":"markdown","897b8244":"markdown","f3bfad8e":"markdown","3f1bb851":"markdown","66c9444b":"markdown","05f1575e":"markdown","81cb2978":"markdown","a2cfffda":"markdown"},"source":{"c6637009":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dropout, Dense, Flatten, BatchNormalization, MaxPooling2D,LeakyReLU\nfrom tensorflow.keras.optimizers import RMSprop,Nadam,Adadelta\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.regularizers import l2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c311f7a7":"tf.test.gpu_device_name()","065560a3":"raw_train = pd.read_csv('..\/input\/Kannada-MNIST\/train.csv')\nraw_test = pd.read_csv('..\/input\/Kannada-MNIST\/test.csv')","fb1ac7ac":"raw_train.iloc[[0,-1],[1,-1]] # First and last values of dataset","5e6798b0":"num = raw_train.label.value_counts()\nsns.barplot(num.index,num)\nnumbers = num.index.values","35c7e339":"num=6\nnumber = raw_train.iloc[num,1:].values.reshape(28,28)\nprint(\"Picture of \"+ str(num) + \"in Kannada style\")\nplt.imshow(number, cmap=plt.get_cmap('gray'))\nplt.show()\n","797c6a1e":"x = raw_train.iloc[:, 1:].values.astype('float32') \/ 255\ny = raw_train.iloc[:, 0] # labels","fc87644c":"x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2, random_state=42) ","e311e64b":"x_train.shape","cb9f038c":"x_train = x_train.reshape(-1, 28, 28,1)\nx_val = x_val.reshape(-1, 28, 28,1)\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)","3c6e5983":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', input_shape=(28, 28, 1)),\n    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(64,  (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(64,  (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.25),    \n    \n    tf.keras.layers.Conv2D(256, (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(256, (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),##\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.25),\n    \n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n \n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])","2c737790":"optimizer = RMSprop(learning_rate=0.002,###########optimizer = RMSprop(learning_rate=0.0025,###########\n    rho=0.9,\n    momentum=0.1,\n    epsilon=1e-07,\n    centered=True,\n    name='RMSprop')\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])","94f10d79":"batch_size = 1024\nnum_classes = 10\nepochs = 40\n","10544616":"# An observation code for our dataset\ndatagen_try = ImageDataGenerator(rotation_range=15,\n                             width_shift_range = 0.15,\n                             height_shift_range = 0.15,\n                             shear_range = 0.15,\n                             zoom_range = 0.4,)\n# fit parameters from data\ndatagen_try.fit(x_train)\n# configure batch size and retrieve one batch of images\nfor x_batch, y_batch in datagen_try.flow(x_train, y_train, batch_size=9):\n\t# create a grid of 3x3 images\n\tfor i in range(0, 9):\n\t\tplt.subplot(330 + 1 + i)\n\t\tplt.imshow(x_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n\t# show the plot\n\tplt.show()\n\tbreak","49ac01d6":"datagen_train = ImageDataGenerator(rotation_range = 10,\n                                   width_shift_range = 0.25,\n                                   height_shift_range = 0.25,\n                                   shear_range = 0.1,\n                                   zoom_range = 0.4,\n                                   horizontal_flip = False)\n\ndatagen_val = ImageDataGenerator() \n\n\nstep_train = x_train.shape[0] \/\/ batch_size\nstep_val = x_val.shape[0] \/\/ batch_size\n\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau( \n    monitor='loss',    # Quantity to be monitored.\n    factor=0.25,       # Factor by which the learning rate will be reduced. new_lr = lr * factor\n    patience=2,        # The number of epochs with no improvement after which learning rate will be reduced.\n    verbose=1,         # 0: quiet - 1: update messages.\n    mode=\"auto\",       # {auto, min, max}. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; \n                       # in the max mode it will be reduced when the quantity monitored has stopped increasing; \n                       # in auto mode, the direction is automatically inferred from the name of the monitored quantity.\n    min_delta=0.0001,  # threshold for measuring the new optimum, to only focus on significant changes.\n    cooldown=0,        # number of epochs to wait before resuming normal operation after learning rate (lr) has been reduced.\n    min_lr=0.00001     # lower bound on the learning rate.\n    )\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=300, restore_best_weights=True)","b2809467":"history = model.fit_generator(datagen_train.flow(x_train, y_train, batch_size=batch_size),\n                              steps_per_epoch=len(x_train)\/\/batch_size,\n                              epochs=epochs,\n                              validation_data=(x_val, y_val),\n                              validation_steps=50,\n                              callbacks=[learning_rate_reduction, es],\n                              verbose=2)","76326699":"tf.keras.utils.plot_model(model, to_file=\"model.png\", show_shapes=True)","6a077d69":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","9e19c841":"model.evaluate(x_val, y_val, verbose=2);","6c6b53d2":"y_predicted = model.predict(x_val)\ny_grand_truth = y_val\ny_predicted = np.argmax(y_predicted,axis=1)\ny_grand_truth = np.argmax(y_grand_truth,axis=1)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_grand_truth, y_predicted)","13bd450b":"f, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm,fmt=\".0f\", annot=True,linewidths=0.1, linecolor=\"purple\", ax=ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Grand Truth\")\nplt.show()","0b50494d":"scores = np.zeros((10,3))\ndef calc_F1(num):\n  TP = cm[num,num]\n  FN = np.sum(cm[num,:])-cm[num,num]\n  FP = np.sum(cm[:,num])-cm[num,num]\n  precision = TP\/(TP+FP)\n  recall = TP\/(TP+FN)\n  F1_score = 2*(recall * precision) \/ (recall + precision)\n  return precision, recall, F1_score\nfor i in range(10):\n   precision, recall, F1_score = calc_F1(i)\n   scores[i,:] = precision, recall, F1_score\nscores_frame = pd.DataFrame(scores,columns=[\"Precision\", \"Recall\", \"F1 Score\"], index=[list(range(0, 10))])","eb3e32da":"f, ax = plt.subplots(figsize = (4,6))\nax.set_title('Number Scores')\nsns.heatmap(scores_frame, annot=True, fmt=\".3f\", linewidths=0.5, cmap=\"PuBu\", cbar=True, ax=ax)\nbottom, top = ax.get_ylim()\nplt.ylabel(\"\")\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","e9fa9774":"raw_dig = pd.read_csv(\"..\/input\/Kannada-MNIST\/Dig-MNIST.csv\")\nraw_dig.head()\nx_dig = raw_dig.iloc[:, 1:].values.astype('float32') \/ 255\ny_dig = raw_dig.iloc[:, 0].values\n\nx_dig = x_dig.reshape(-1,28,28,1)\ny_dig = to_categorical(y_dig)\nmodel.evaluate(x_dig, y_dig, verbose=2)","c354c009":"sample_sub=pd.read_csv('..\/input\/Kannada-MNIST\/sample_submission.csv')\nraw_test_id=raw_test.id\nraw_test=raw_test.drop(\"id\",axis=\"columns\")\nraw_test=raw_test \/ 255\ntest=raw_test.values.reshape(-1,28,28,1)\ntest.shape","334b59e8":"sub=model.predict(test)     ##making prediction\nsub=np.argmax(sub,axis=1) ##changing the prediction intro labels\n\nsample_sub['label']=sub\nsample_sub.to_csv('submission.csv',index=False)","fd7d7af8":"#### **Compile -> Optimizer** <a id=\"255\"><\/a>\n<mark>[Return Contents](#0)\n* To optimize the weight values of the network we should choose an optimizer algorithm. \n* In the first lessons of artificial learning, the gradient descent algorithm is taught as the optimization algorithm used in backpropagation. \n* Over time, the gradient descent algorithm was developed and the algorithms that achieved faster and more accurate results were obtained. \n* Some of them are ADAGRAD, ADAM, ADAMAX, NADAM and RMSPROP. \n* These algorithms use techniques such as adaptive learning rate and momentum to achieve the global minimum.\n* If you want to take a more detailed look at Gradient Descent algorithms, [here](https:\/\/ruder.io\/optimizing-gradient-descent\/) is a very nice overview article written by [Sebastian Ruder](https:\/\/ruder.io\/optimizing-gradient-descent\/).\n\n* __As you see below__ while Stochastic Gradient Descent (SGD) which is a basic gradient descent algorithm cannot escape the saddle point, more advanced algorithms escape the saddle point __at different speeds.__\n","3b0301b0":"### **Types of Padding in Keras** <a id=\"133\"><\/a> \n\n* Valid : No padding\n* Same  : \uc704\uc758 \ubc29\uc815\uc2dd\uc5d0\uc11c $p$ \uac12\uc744 \uc120\ud0dd\ud558\uc5ec \uc785\ub825 \ud589\ub82c\uacfc \uac19\uc740 \ucc28\uc6d0\uc758 \ucd9c\ub825 \ud589\ub82c\uc744 \uc5bb\uc2b5\ub2c8\ub2e4.","8b88c4a7":"* We have training and test set CSV files,\n* In order to evaluate the generalization skill of the model we will split our training set into training and validation sets.\n* After training the data, Kaggle will evaluate the final performance of our data with test set predictions.","edf2d150":"# ** 2. Convolutional Neural Networks** <a id=\"100\"><\/a>\n<mark>[Return Contents](#0)","6328653d":"# ** 3. An Image Classification Application with CNN** <a id=\"200\"><\/a>\n<mark>[Return Contents](#0)\n<hr>","b07c1a76":"# ** 1. Introduction** <a id=\"1\"><\/a>\n    \n\uce78\ub098\ub2e4 MNIST \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc0ac\uc6a9\ud558\ub294 \ud544\uae30 \uc778\uc2dd \ucc4c\ub9b0\uc9c0\uc785\ub2c8\ub2e4. (\ubb3c\ub860 \uc774\ubbf8 \ucc4c\ub9b0\uc9c0\ub294 \ub9c8\uac10\ub418\uc5c8\uc2b5\ub2c8\ub2e4\ub9cc)\n\n\ub450 \uac00\uc9c0 \uc8fc\uc694 \ubd80\ubd84\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.\n\n1. \uae30\ubcf8 \uc774\ub860 \uc9c0\uc2dd; \n\n    * \uc778\uacfc \uad00\uacc4\n\u00a0\u00a0\u00a0 * \uc2dc\uac01\uc801 \ud45c\ud604\n\u00a0\u00a0\u00a0 * CNN\uc5d0 \ub300\ud55c \uc774\uc57c\uae30\n   \n   \n2. \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc73c\ub85c \uc5f0\uc2b5;\n\n    * MNIST \ud544\uae30 \uc22b\uc790 \ub370\uc774\ud130 \uc138\ud2b8\uc640 \ub2e4\ub978 \uce78\ub098\ub2e4 \ud544\uae30 \uc22b\uc790 \ub370\uc774\ud130 \uc138\ud2b8\ub85c \ud544\uae30 \uc778\uc2dd.\n\u00a0\u00a0\u00a0 * \uc801\uc6a9 \ubd80\ubd84\uc5d0\uc11c\ub294 \ud544\uc694\uc5d0 \ub530\ub77c \uc2dc\uac01\uc801 \uc778 \uc138\ubd80 \uc815\ubcf4\uac00 \ub2f4\uae34 \uc774\ub860 \uc815\ubcf4\ub3c4 \uc81c\uacf5\ud588\uc2b5\ub2c8\ub2e4.\n  \n  \n  \n  <font color='green'>\n    \n#### upvote \ub20c\ub7ec\uc8fc\uc138\uc694^^\n\n![rsz_1smile.jpg](attachment:rsz_1smile.jpg)\n","ebeaeef5":"## **Import Modules** <a id=\"210\"><\/a>","6ca15f3a":"## **Understanding the Data** <a id=\"220\"><\/a>\n<mark>[Return Contents](#0)","f80eb02d":"![rsz_convolution-with-multiple-filters2.png](attachment:rsz_convolution-with-multiple-filters2.png)","a0242bb1":"![rsz_1rsz_conv_sample.jpg](attachment:rsz_1rsz_conv_sample.jpg)\n* \uc88c\uce21\uc740 \"\uc785\ub825\"\uc2e0\ud638 (\ub610\ub294 \uc774\ubbf8\uc9c0)\ub85c, \uc911\uac04(\ucee4\ub110)\uc740 \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0\uc11c \"\ud544\ud130\"\ub85c \uc0dd\uac01\ud560 \uc218 \uc788\uace0 \uc774\ub4e4\uc774 \uc138\ubc88\uc9f8\uc778 \ucd9c\ub825 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4 (\ub530\ub77c\uc11c \ucee8\ubcfc\ub8e8\uc158\uc740 \ub450 \uac1c\uc758 \ub9e4\ud2b8\ub9ad\uc2a4\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ud569\ub2c8\ub2e4).\n#### **Steps of convulation operation** <a id=\"111\"><\/a> \n1. \uc67c\ucabd \uc704\uc758 \ud558\uc704 \ud589\ub82c\uc758 \uc694\uc18c\uacf1\uc744 \ud0dd\ud558\uace0 \ud544\ud130\n2. \ubaa8\ub4e0 \ud589\uacfc \uc5f4\uc744 \ud569\ud558\uc5ec \ub2e8\uc77c \uac12\uc744 \uc5bb\uc2b5\ub2c8\ub2e4.\n3. \uacb0\uacfc\ub294 \uc0c8 \ucd9c\ub825 \ud589\ub82c\uc758 \uc0c1\ub2e8 \ubaa8\uc11c\ub9ac \uc140 \uac12\uc785\ub2c8\ub2e4. (output(0,0))\n4. \uadf8\ub7f0 \ub2e4\uc74c \uae30\ubcf8 \uc774\ubbf8\uc9c0\uc758 \"S\"\uc5f4 \uc704\ub85c \ud544\ud130\ub97c \ubc00\uc5b4 \ub123\uace0 \ub3d9\uc77c\ud55c \uc791\uc5c5\uc744 \uc218\ud589\ud558\uc2ed\uc2dc\uc624.  output(0,1) value\uac00 \uc0dd\uae41\ub2c8\ub2e4.\n\n#### **Stride** <a id=\"112\"><\/a>\n* \uc2ac\ub77c\uc774\ub529 \uc591\uc740 stride (S) constant\ub85c \uc120\uc5b8\ub429\ub2c8\ub2e4.  \n* stride content\ub294 \ubcc0\uacbd \ub420 \uc218 \uc788\uc73c\uba70 \uc774\ub294 \ucd9c\ub825 \ucc28\uc6d0\uc5d0 \uc9c1\uc811 \uc601\ud5a5\uc744 \ubbf8\uce69\ub2c8\ub2e4.\n* $ l $ '\ubc88\uc9f8 \ub808\uc774\uc5b4\uc5d0 \ub300\ud55c \ucd9c\ub825 \ub9e4\ud2b8\ub9ad\uc2a4 \ucc28\uc6d0 \uacc4\uc0b0 :\n\\begin{equation}\nn^{l} = \\frac{n^{l-1} - f}{s}+1 \\\\\n\\text{output} =  n^l x n^l\n\\end{equation}\n\uc124\uba85;\n * $l$ - layer number.\n * $n^{l}$ - horizontal\/vertical dimension of output matrix.\n * $n^{l-1}$ -  horizontal\/vertical dimension of input matrix.\n * $f$ - horizontal\/vertical dimension of filter (kernel) matrix.\n * $s$ - stride constant. \ub2e4\uc74c \uc140\uc5d0\uc11c \ucd9c\ub825 \ud589\ub82c\uc744 \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \ucee4\ub110\uc774 \uc624\ub978\ucabd\uc73c\ub85c \uc5bc\ub9c8\ub098 \ub9ce\uc740 \uc5f4\uc744 \uc774\ub3d9\ud560\uc9c0 \uacb0\uc815\ud558\ub294 \ub370 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","b6f23561":"## **Batch Normalization** <a id=\"160\"><\/a>\n ","bdf4dfc2":"### **Confusion Matrix** <a id=\"273\"><\/a>\n<mark>[Return Contents](#0)","8946ea69":"![rsz_images.jpg](attachment:rsz_images.jpg)","70eae991":"### **Why do we use Filters?** <a id=\"114\"><\/a>\n\n* \uc774 \uc9c8\ubb38\uc5d0 \uba3c\uc800 \ub2f5\ud558\uae30 \uc704\ud574\uc11c\ub294 \ub610 \ub2e4\ub978 \uc77c\ubc18\uc801\uc778 \uc9c8\ubb38\uc744\ud574\uc57c\ud569\ub2c8\ub2e4.\n* \uc6b0\ub9ac\uc758 \uc8fc\uc694 \ubaa9\uc801\uc740 \ubb34\uc5c7\uc785\ub2c8\uae4c?\n* \uc774 \ucee4\ub110\uc5d0\uc11c \uc6b0\ub9ac\uc758 \uc8fc\uc694 \ubaa9\uc801\uc740 \ud544\uae30 \ud328\ud134\uc744 \uc778\uc2dd \ud558\ub294 \uac83\uc785\ub2c8\ub2e4.\n* \ub2f9\uc2e0\uc740 \uace0\uc591\uc774 \uc5bc\uad74 \uc0ac\uc9c4\uc744 \ubcfc \ub54c \uc5b4\ub5bb\uac8c \uadf8\uac83\uc774 \uace0\uc591\uc774\uc778\uc9c0 \uc774\ud574\ud569\ub2c8\uae4c? \uc5bc\uad74\uc5d0 \ud655\uc2e4\ud55c \ubb50\uac00 \uc788\uc5b4\uc57c\ud569\ub2c8\uae4c?\n\u00a0\u00a0\u00a0* \ub208 \ubaa8\uc591, \ucf54 \uace1\uc120, \ucf67\uc218\uc5fc \ub4f1\uacfc \uac19\uc740 \ud655\uc2e4\ud55c \ud328\ud134\n\u00a0\u00a0\u00a0* \uc6b0\ub9ac\uc758 \ub1cc\ub294 \uc5bc\uad74\uc758 \uc77c\ubd80\ub97c \uac1c\ubcc4\uc801\uc73c\ub85c \uc778\uc2dd\ud558\uace0 \uacb0\ud569\ud558\uc5ec \ucd5c\uc885 \uacb0\uc815\uc744 \ub0b4\ub9bd\ub2c8\ub2e4. CNN \ud544\ud130\uc5d0 \ub300\ud574\uc11c\ub3c4 \ub3d9\uc77c\ud55c \uc124\uba85\uc774 \ud5c8\uc6a9\ub429\ub2c8\ub2e4.\n* \uac01 \ud544\ud130\uc5d0\ub294 \uc791\uc5c5\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub4e4 \uc911 \uc77c\ubd80\ub294 \uac00\uc7a5\uc790\ub9ac\ub97c \uac10\uc9c0\ud558\uace0 (\uc608\uc640 \uac19\uc774 Sobel \ud544\ud130) \uc678\uacfd\uc120 \uc77c\ubd80\ub9cc \uac10\uc9c0\ud569\ub2c8\ub2e4.\n* \uc9c4\ud654\ud558\ub294 \ub124\ud2b8\uc6cc\ud06c\uc758 \ub525 \ub808\uc774\uc5b4\uc5d0\uc11c \ud544\ud130 \uc791\uc5c5\uc774 \ub354\uc6b1 \ubcf5\uc7a1\ud574\uc9d1\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \ub124\ud2b8\uc6cc\ud06c\uc758 \uccab \ubc88\uc9f8 \ub808\uc774\uc5b4\ub294 __horizontal lines__ \ub9cc \ud0d0\uc9c0 \ud560 \uc218\uc788\ub294 \ubc18\uba74 \ub124\ud2b8\uc6cc\ud06c\uc758 \ub354 \uae4a\uc740 \ub808\uc774\uc5b4\ub294 \ucf54\ub098 \ub208\uc744 \ud0d0\uc9c0 \ud560 \uc218 \uc788\uc73c\uba70 __ \uacc4\uc18d \uc9c4\ud589\uc911\uc778 \ub354 \uae4a\uc740 \ub808\uc774\uc5b4\ub294 \uc0ac\ub78c\uc758 \uc5bc\uad74\uae4c\uc9c0\ub3c4 \ud0d0\uc9c0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 __\n* \uc694\uc57d \ud544\ud130\ub294 \uc774\ubbf8\uc9c0\uc758 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \ud53c\uccd0 \ucd94\ucd9c \ud6c4 \ucd94\ucd9c \ub41c \ud53c\uccd0\uc5d0 \ub530\ub77c \ubd84\ub958\ub97c \uc704\ud574 \uc644\uc804\ud788 \uc5f0\uacb0\ub41c \ub808\uc774\uc5b4\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n","783792fa":"## **Convolution Operation** <a id=\"110\"><\/a>\n","43ae5477":"* \ub525\ub7ec\ub2dd\uc5d0\uc11c CNN (Convolutional Neural Network) \ub610\ub294 ConvNet (Convolutional Neural Network)\uc740 \uc2ec\uce35 \uc2e0\uacbd \ub124\ud2b8\uc6cc\ud06c\uc758 \ud074\ub798\uc2a4\ub85c, \uc2dc\uac01\uc801 \uc774\ubbf8\uc9c0 \ubd84\uc11d\uc5d0 \uac00\uc7a5 \uc77c\ubc18\uc801\uc73c\ub85c \uc801\uc6a9\ub429\ub2c8\ub2e4.\n* \uc774\ubbf8\uc9c0 \ubc0f \ube44\ub514\uc624 \uc778\uc2dd, \ucd94\ucc9c \uc2dc\uc2a4\ud15c, \uc774\ubbf8\uc9c0 \ubd84\ub958, \uc758\ub8cc \uc774\ubbf8\uc9c0 \ubd84\uc11d \ubc0f \uc790\uc5f0\uc5b4 \ucc98\ub9ac \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n*\u201cConvolutional Neural Network\u201d\ub77c\ub294 \uc774\ub984\uc740 \ub124\ud2b8\uc6cc\ud06c\uac00 Convolution\uc774\ub77c\ub294 \uc218\ud559\uc801 \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ucee8\ubcfc\ub8e8\uc158\uc740 \ud2b9\ubcc4\ud55c \uc885\ub958\uc758 \uc120\ud615 \uc5f0\uc0b0\uc785\ub2c8\ub2e4. \ucee8\ubcfc \ub8e8\uc158 \ub124\ud2b8\uc6cc\ud06c\ub294 \ub808\uc774\uc5b4 \uc911 \ud558\ub098 \uc774\uc0c1\uc5d0\uc11c \uc77c\ubc18 \ud589\ub82c \uacf1\uc148 \ub300\uc2e0 \ucee8\ubcfc \ub8e8\uc158\uc744 \uc0ac\uc6a9\ud558\ub294 \ub2e8\uc21c\ud55c \uc2e0\uacbd\ub9dd\uc785\ub2c8\ub2e4.\n* \ucee8\ubcfc\ub8e8\uc158 \uc2e0\uacbd\ub9dd\uc740 \ud558\ub098 \ub610\ub294 \uc5ec\ub7ec \uac1c\uc758 \ucee8\ubcfc \ub8e8\uc158 \uce35\uc73c\ub85c \uad6c\uc131\ub418\uba70 \ud074\ub798\uc2dd \ub2e4\uce35 \ud53c\ub4dc \ud3ec\uc6cc\ub4dc \uc2e0\uacbd\ub9dd\uc5d0\uc11c\uc640 \uac19\uc774 \uc644\uc804\ud788 \uc5f0\uacb0\ub41c \ub274\ub7f0 \ub808\uc774\uc5b4\uac00 \ub4a4 \ub530\ub985\ub2c8\ub2e4.\n* CNN\uc758 \uc228\uaca8\uc9c4 \uacc4\uce35\uc740 \uc77c\ubc18\uc801\uc73c\ub85c \ud68c\uc120 \uacc4\uce35, \ud480\ub9c1 \uacc4\uce35, \uc644\uc804\ud788 \uc5f0\uacb0\ub41c \uacc4\uce35 \ubc0f \uc815\uaddc\ud654 \uacc4\uce35\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.","7edcb7c1":"* If 90% of the data set is cat image and 10% is dog image, your accuracy will be 90% even if you estimate the entire test set as a cat.\n* But in another aspect, the model's success in predicting dogs is 0%.\n* In this context, accuracy may not always give us realistic information about the actual performance of the model.\n* The confusion matrix shows how confused your classification model is for which classes by detailing the relationship between actual class and predicted class.\n* If there is an anomaly something like above mentioned, you can specify the problem with confusion matrix and improve accuracy by various methods like adding more data for a specific class, etc.","d1231395":"## **Drop Out** <a id=\"180\"><\/a>\n<mark>[Return Contents](#0)","4ed399ac":"# A Simple CNN Tutorial\n\n![brain.jpg](attachment:brain.jpg)\n\n1. [__INTRODUCTION__](#1)\n    \n2. [__CONVOLUTIONAL NEURAL NETWORKS__](#100)\n    1. [__Convolution Layer__](#110)\n        * [Steps of Convolution Operation](#111)\n        * [Stride](#112)\n        * [An Example of Convolution Operation](#113)\n        * [Why Do We Use Filters?](#114)       \n    2. [__Padding__](#130)\n        * [Why do we use Padding?](#131)\n        * [Equation of Calculating Output Dimension](#132)\n        * [Types of Padding in Keras](#133)\n    3. [__Pooling Layer__](#150)\n        * [Hyper Parameters of Pooling Operation](#151)\n        * [Why do we use Pooling?](#152)\n        * [Types of Pooling in Keras](#153)\n    4. [__Batch Normalization Layer__](#160)\n    5. [__Drop Out Layer__](#180)\n    \n3. [__APPLICATION WITH CNN__](#200)\n    1. [__Import Modules__](#210)\n    2. [__Understanding the Data__](#220)\n    3. [__Data Preprocessing__](#230)\n        * [Normalizing the Data](#231)\n        * [Train and Test Splitting](#232)\n        * [Reshape Data to Appropriate Sizes ](#233)\n    4. [__Building and Training a CNN Model__](#250)  \n        * [Model - Build](#251)\n        * [Model - Compile](#252)\n            * [Categorical Cross Entropy](#253)\n            * [Optimizer](#255)\n            * [On-the-Fly Data Augmentation](#257)\n        * [Model - Fit](#259)\n    5. [__Evaluation of the Model__](#270) \n        * [Accuracy and Loss Curves](#271) * \n        * [Test Set Accuracy Score](#272)\n        * [Confusion Matrix](#273)\n        * [F1 Score Calculation](#274)\n        * [Evaluate with Another Dataset](#275)\n        * [Submit for Competition](#275)\n6. [__CONCLUSION__](#290)","43a6db9e":"* We have $28$x$28$ dimension handwritten pics.\n* Dataset has been already flattened and has 784-pixel values for each pic.\n* Totaly we have $60000$ pics in training set.","5b63a168":"### **Model - Fit** <a id=\"259\"><\/a>\n<mark>[Return Contents](#0)","9086ba48":"### **Types of Pooling in Keras** <a id=\"153\"><\/a>\n\n    \n* Max Pooling: \uc704 \uadf8\ub9bc\uacfc \uac19\uc774 \uac01 \ubbf8\ub2c8 \uadf8\ub8f9 (\ud480)\uc5d0\uc11c \ucd5c\ub300 \uac12\uc744 \uc120\ud0dd\ud558\ub294 \uc791\uc5c5\n* Average Pooling: \uac01 \ubbf8\ub2c8 \uadf8\ub8f9\uc5d0\uc11c \ud3c9\uade0 \uac12\uc744 \uc120\ud0dd\ud558\ub294 \uc791\uc5c5","fd67d95d":"<a href=\"https:\/\/ibb.co\/sFVGBpY\"><img src=\"https:\/\/i.ibb.co\/xM704KT\/cnn.jpg\" alt=\"cnn\" border=\"0\"><\/a>","ce69328e":"1. \ud2b9\uc815 \ud328\ud134\uc744 \ud3ec\ud568\ud558\ub294 \ud544\ud130 \ub9e4\ud2b8\ub9ad\uc2a4\n2. Conv \ub808\uc774\uc5b4\uc5d0\uc11c\ub294 stride \uc218\ub9cc\ud07c \ud544\ud130\uac00 \uc774\ubbf8\uc9c0 \uc704\ub85c \uc774\ub3d9\ud558\uace0 \ucee8\ubcfc\ub8e8\uc158\uc774 \uc218\ud589\ub429\ub2c8\ub2e4.\n3. 3\ubc88\uc9f8 \uacfc\uc815\uc758 \uacb0\uacfc\ub85c \uc5bb\uc740 \ud589\ub82c\uc744 feature map\uc774\ub77c\uace0\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c \uc774\uc804 \ub808\uc774\uc5b4\uc758 \ub9e4\ud2b8\ub9ad\uc2a4 \uc601\uc5ed\uc5d0\uc11c \ud544\ud130 \ud328\ud134\uc5d0 \uc0ac\uc6a9\ud560 \uc218\uc788\ub294 \ub370\uc774\ud130\uc758 \uc591\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n4. \uc218\uce58 \uc801\uc73c\ub85c \ub192\uc740 \uac12\uc740 \ud328\ud134\ub300\ub85c \ub418\uace0 \uc788\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n5. feature map\uc740 \ub2e4\uc74c CNN \uacc4\uce35\uc5d0 \ub300\ud55c \uc0c8\ub85c\uc6b4 \uadf8\ub9bc\uc73c\ub85c \uac04\uc8fc \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc0c8\ub85c\uc6b4 \ud544\ud130\ub85c\uc774 \uadf8\ub9bc\uc758 \ud328\ud134\uc744 \uac10\uc9c0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 CNN\uc758 \ub450 \ubc88\uc9f8 \uacc4\uce35\uc785\ub2c8\ub2e4. \uc774 \ub85c\uc9c1\uc73c\ub85c \ub808\uc774\uc5b4 \uc218\ub97c \ub298\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","85dbffc3":"### **Submit for Competition** <a id=\"276\"><\/a>\n<mark>[Return Contents](#0)","4a83676a":"## **Pooling** <a id=\"150\"><\/a>\n","4bdfd136":"##### \ud328\ub529\uc740 \uc785\ub825 \ud589\ub82c\uc5d0 \uac12\uc744 \ub300\uce6d \uc801\uc73c\ub85c \ucd94\uac00\ud558\ub294 \uacfc\uc815\uc744 \ub9d0\ud569\ub2c8\ub2e4.\n\n![padding.png](attachment:padding.png)","aba034ff":"### **An Example of Convolution Operation** <a id=\"113\"><\/a>\n\n    \n![conv2.png](attachment:conv2.png)\n\n* \uc704\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774 5x5 \uc774\ubbf8\uc9c0 \ub9e4\ud2b8\ub9ad\uc2a4\uc640 3x3 \ud544\ud130 \ub9e4\ud2b8\ub9ad\uc2a4\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n* stride (S) \ud30c\ub77c\ubbf8\ud130\ub97c 1\ub85c \uc120\ud0dd\ud558\uba74 \ucd9c\ub825 \ub9e4\ud2b8\ub9ad\uc2a4\ub294 3x3 \ub9e4\ud2b8\ub9ad\uc2a4\uac00\ub429\ub2c8\ub2e4.\n* stride (S) \ud30c\ub77c\ubbf8\ud130\ub97c 2\ub85c \uc120\ud0dd\ud558\uba74 \ucd9c\ub825 \ub9e4\ud2b8\ub9ad\uc2a4\ub294 2x2 \ub9e4\ud2b8\ub9ad\uc2a4\uac00\ub429\ub2c8\ub2e4.\n\n\\begin{equation}\nn^{l} = \\frac{5 - 3}{1}+1 = 3\\\\\n\\text{output} =  3 x 3\\\\\n\\text{  }\\\\\nn^{l} = \\frac{5 - 3}{2}+1 = 2\\\\\n\\text{output} =  2 x 2\\\\\n\\end{equation}","72c862bc":"![CNN.jpg](https:\/\/ruder.io\/content\/images\/2016\/09\/saddle_point_evaluation_optimizers.gif)\n![CNN.jpg](https:\/\/ruder.io\/content\/images\/2016\/09\/contours_evaluation_optimizers.gif)","6575b217":"* As mentioned above, [accuracy](#271) gives a general idea about the performance of the model but does not provide any information about the model's trends.\n* In classification algorithms, it is important to analyze the false predictions of the model.\n* There are 2 kinds of false predictions; \n  * __Predicted as \"1\" but Ground Truth is \"0\" (False Positive)__\n  * __Predicted as \"0\" but Ground Truth is \"1\" (False Negative)__\n\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/i.ibb.co\/q99h0R6\/f1.png\" alt=\"f1\" border=\"0\"><\/a>\n* The F1 score creates a success performance metric, taking into account both of these incorrect prediction performances as well as the true positive predictions\n* Since our problem has more than 2 classes, we calculated the F1 score for each class on a one-to-all basis.\n* [Here](https:\/\/towardsdatascience.com\/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1) is a nice article on how to calculate the F1 score in multiple classes, supported by examples.","6ca7ad0c":"### **Model - Compile** <a id=\"252\"><\/a>\n<mark>[Return Contents](#0)\n    \n* On Compile Section we specify the loss function, optimizer algorithm and metric to use for evaluating the model.","ed07b9db":"### **Cross Validation - Training- Validation Set Split** <a id=\"232\"><\/a>\n<mark>[Return Contents](#0)\n    \n* In order to measure the generalization ability of the model, we train the data with the training set and make the model arrangement according to the error value in the validation set. In addition, we determine the final performance of the model with the test set. \n* The reason for using the test set on the final evaluation is the model would have a bias on the validation set because we developed the model according to the validation set performance. So a kind of overfitting on the validation set is formed.\n* This Kernel is prepared on a Kaggle competition dataset. They give us a training set for training the model and a test set without labels for prediction. As we don't have the labels we don't know the final performance of the test set until we submit our predictions.\n* To evaluate the model we need to split our training set into training and validation set.\n    \n* For I prefer to split\n    * Training set - $\\%80$\n    * Validation set - $\\%20$\n","c4ef21a3":"* It is important to know the distribution of data according to the labels they have.\n* This data set is __homogeneously__ distributed as you see below.","dc38b339":"#### **Why do we use Batch Norm?** <a id=\"161\"><\/a>\n\n* \uc778\uacf5 \uc2e0\uacbd\ub9dd\uc5d0\uc11c \uc774\uc804 \ub808\uc774\uc5b4\uc758 \uac00\uc911\uce58 \ub9e4\uac1c \ubcc0\uc218\uac00 \ubcc0\uacbd\ub418\ubbc0\ub85c \ud6c8\ub828 \uc911 \uac01 \ub808\uc774\uc5b4\uc5d0 \ub300\ud55c \uc785\ub825 \ubd84\ud3ec\uac00 \ubcc0\uacbd\ub429\ub2c8\ub2e4. \uc774\ub85c \uc778\ud574 \ub0ae\uc740 \ud559\uc2b5 \uc18d\ub3c4\uc640 \uc2e0\uc911\ud55c \ub9e4\uac1c \ubcc0\uc218 \ucd08\uae30\ud654\uac00 \ud544\uc694\ud558\ubbc0\ub85c \ud6c8\ub828 \uc18d\ub3c4\uac00 \ub290\ub824\uc838 \ube44\uc120\ud615 \ud3ec\ud654 \uc0c1\ud0dc\uc758 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uae30\uac00 \uc5b4\ub835\uc2b5\ub2c8\ub2e4.\n\n* Batch Norm \uc0ac\uc6a9 \uc774\uc720\n\u00a0\u00a0\u00a0\u00a0 1. \ubaa8\ub378\uc758 \ud559\uc2b5 \uc18d\ub3c4\ub97c \ub192\uc774\uae30 \uc704\ud574.\n\u00a0\u00a0\u00a0\u00a0 2. \ud6e8\uc52c \ub192\uc740 \ud559\uc2b5 \uc18d\ub3c4\ub97c \uc0ac\uc6a9\ud558\uace0 initialization\uc5d0 \ub35c \uc8fc\uc758 \ud560 \uc218 \uc788\uac8c\n\u00a0\u00a0\u00a0\u00a0 3. \ub124\ud2b8\uc6cc\ud06c\ub97c \uc815\uaddc\ud654\ud558\ub824\uace0 (side positive effect)\n    ","62d9c900":"## **Padding** <a id=\"130\"><\/a>\n\n    ","2ef5a7ff":"* Let's read csv files","2cee8fa7":"* Drop out is an effective regularization method.\n* During training, some number of layer outputs are randomly ignored or \u201cdropped out.\u201d This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different \u201cview\u201d of the configured layer.\n* __Why do we use Drop Out?__ The answer is to prevent overfitting...\n* It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers such as the long short-term memory network layer.","81b4f594":"![rsz_1form%C3%BCl.jpg](attachment:rsz_1form%C3%BCl.jpg)","2a436db5":"### **Why do we use Pooling?** <a id=\"152\"><\/a>\n\n1. \ud2b9\uc9d5 \ucd94\ucd9c\uc5d0 \ub300\ud55c \uacac\uace0\uc131\uc744 \uc5bb\uae30 \uc704\ud574;\n\u00a0\u00a0\u00a0\u00a0 * \ud480\ub9c1\uc740 \uc120\ud0dd\ud55c \uac12\uacfc \uad00\ub828\ud558\uc5ec \ubd88\ud544\uc694\ud55c \ub370\uc774\ud130\ub97c \uc0ad\uc81c\ud558\uc5ec \ubaa8\ub378\uc774 \uacfc\ub3c4\ud558\uac8c \ud6c8\ub828\ub418\uc9c0 \uc54a\ub3c4\ub85d\ud569\ub2c8\ub2e4.\n2. \uacc4\uc0b0 \uc18d\ub3c4\ub97c \ub192\uc774\uae30 \uc704\ud574;\n\u00a0\u00a0\u00a0\u00a0 * \uacc4\uc0b0 \uc18d\ub3c4\ub97c \ub192\uc774\uae30 \uc704\ud574 \ud45c\ud604\uc758 \ud06c\uae30\ub97c \uc904\uc785\ub2c8\ub2e4.","f89edd9f":"* With this code below you can check if the kernel use GPU or not.","67c90726":"![CNN.png](https:\/\/i.ibb.co\/Qcb9y35\/Types-of-Learning-in-Machine-Learning.jpg)","2669b6cc":"### **Reshape Data to Fit Model** <a id=\"233\"><\/a>\n<mark>[Return Contents](#0)","6e404d76":"## **Evaluation of the Model** <a id=\"270\"><\/a>\n### **Accuracy and Loss Curves** <a id=\"271\"><\/a>\n<mark>[Return Contents](#0)","8f7d951f":"## **Building and Training a CNN Model** <a id=\"250\"><\/a>\n<mark>[Return Contents](#0)\n<hr>\n* On Keras Sequential Networks there is three-stage for training building, compiling and fitting the model.\n    \n#### **Model - Build** <a id=\"251\"><\/a>\n\n    \n* On building stage you specify the architecture of the model mainly.\n* You can decide the [Filter](#110) size and [Padding](#150) type you will use on [Convolution](#110) operations and add [Pooling](#150), Batch Normalization, Dropout, activation function layers with build section.","7515b146":"# **Conclusion** <a id=\"276\"><\/a>\n<mark>[Return Contents](#0)\n<font color='green'>\n* Please do not hesitate to comment and ask questions.\n* If you found it useful, I would appreciate it if you upvote    \n    \n ![rsz_1smile.jpg](attachment:rsz_1smile.jpg)\n \n #### Acknowledgement *\n #### This work is based on works by Benan AKCA\n","825c92c3":"### **Evaluate with Another Dataset** <a id=\"275\"><\/a>\n<mark>[Return Contents](#0)","a13cfdc8":"* \uc704 \uadf8\ub9bc\uc5d0\uc11c\uc758 \ucee8\ubcfc\ub8e8\uc158 \uc791\uc5c5\uacfc \ub3d9\uc77c\ud569\ub2c8\ub2e4. $2$x$2$ pooling size\uac00 \uc788\uace0 stride = 2\uc785\ub2c8\ub2e4. (sliding on matrix by 2 cell)","9ff4a15d":"* __If the data wasn't homogeneously__ distributed what would we do?\n    1. Then we could use data augmentation techniques to generate new data for low quantity labels,\n    2. Or if we have enough data we can discard some high quantity labels","76f9a220":"#### **Some notes about Batch Norm** <a id=\"162\"><\/a>\n    \n* The parameters $\\gamma$ and $\\beta$ are learnable parameters. Therefore, you just initialize them and with backpropagation or other optimization algorithm they will be optimized like weight parameters.\n* Changing the distribution of each layer\u2019s inputs during training, because of the enhancement of parameters of the previous layers calling covariate shifting at the original paper.\n* You can read the original paper written by Sergey Ioffe and Christian Szegedy from [here](https:\/\/arxiv.org\/pdf\/1502.03167.pdf).","78c83c4f":"* On classification, accuracy metric is calculated as below;\n\n\\begin{equation}\nclassification~accuracy = \\frac{correct~predictions}{total~predictions} * 100 \\\\\n\\end{equation}","8a19e25c":"* \ucc98\uc74c \uc0ac\uc9c4\uc774 \uc544\ub798\uc640 \uac19\uc774 \uceec\ub7ec (RGB) \uc778 \uacbd\uc6b0 2 \ucc28\uc6d0 \ubc0f 3 \ucc28\uc6d0\ub3c4 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (6x6x3)\n* \uc774 \uacbd\uc6b0 \ud544\ud130 1\uc744 3 \uac1c\uc758 \ucc44\ub110\uc5d0\uc11c \uc138 \uac1c\uc758 \ub2e4\ub978 \uac12\uc744 \uac16\ub3c4\ub85d \ubcc0\ud658\ud558\uace0 \uacb0\uacfc\ub97c \ud569\uc0b0\ud558\uc5ec \ub2e8\uc77c \uc22b\uc790\ub97c \uc5bb\uc2b5\ub2c8\ub2e4.\n* \uc5ec\uae30\uc11c \uc911\uc694\ud55c \uc694\uc18c\ub294 filter1\uc774 \ubaa8\ub4e0 3 \ucc44\ub110\uc5d0 \ub300\ud574 \ub3d9\uc77c\ud55c \uac12\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4 (\ub2e4\ub978 \uc0c9\uc0c1\uc73c\ub85c \ud45c\uc2dc).\n* \uc704\uc758 \uc608\uc5d0\uc11c\ub294 \ud558\ub098\uc758 \ud544\ud130 \ub9cc \uc0ac\uc6a9\ud558\uc9c0\ub9cc \uc2e4\uc81c \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc5d0\uc11c\ub294 \uc544\ub798\uc640 \uac19\uc774 \ub354 \ub9ce\uc740 \ud544\ud130\uac00 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.\n* \ucd9c\ub825 \ub9e4\ud2b8\ub9ad\uc2a4 \ub108\ube44 (w)\uc640 \ub192\uc774 (h) \ud06c\uae30\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc5d0\uc11c \ub3d9\uc77c\ud558\uc9c0\ub9cc \ub2e4\ub97c \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\n* \ub530\ub77c\uc11c \ucd9c\ub825 \uc774\ubbf8\uc9c0\ub294 $ w $ x $ h $ x $ c $ \ucc28\uc6d0\uc758 \ud589\ub82c\uc785\ub2c8\ub2e4.\n* $ w $ \ubc0f $ h $\ub294 \uc704\uc758 \ubc29\uc815\uc2dd\uc73c\ub85c \uacc4\uc0b0\ub418\uba70 \ud589\ub82c\uc758 \ucc44\ub110 \uce58\uc218\ub294 \ucee8\ubcfc\ub8e8\uc158\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ud544\ud130 \ubc88\ud638\uc640 \uac19\uc2b5\ub2c8\ub2e4. \ucee8\ubc8c\ub8e8\uc158 \uc5f0\uc0b0 \ud6c4 \uac01 \ud544\ud130\uac00 \uc544\ub798\uc640 \uac19\uc774 \ub9e4\ud2b8\ub9ad\uc2a4\uc5d0\uc11c \ub2e4\ub978 \ub808\uc774\uc5b4\ub97c \uc0dd\uc131\ud55c\ub2e4\uace0 \uc0c1\uc0c1\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","93a27613":"## **Data Preprocessing** <a id=\"230\"><\/a>\n### **Normalizing Data** <a id=\"231\"><\/a>\n<mark>[Return Contents](#0)\n<hr>\n* What is normalizing? Normalization means that adjusting values measured on different scales to a notionally common scale.\n* Why should you normalize the data?  With a normalized data weight values reach optimum value faster.\n* On image processing applications generally we normalize data to 0-1 scale with dividing data to 255.\n* Because each pixel in every sample of training set has integer values from 0 to 255.\n* In order to normalize training set data, we need to convert x to float type.","55a1c288":"* You can leave one unknown dimension as -1.\n* Whenever numpy has -1 value on reshape method it will calculate the dimension which denoted with -1 automatically.","d20e25b0":"### **__Why do we use Padding?__** <a id=\"131\"><\/a>\n\n1. \uac00\uc7a5\uc790\ub9ac\uc758 \uc815\ubcf4\ub294 \ud55c \ubc88\ub9cc \ub4a4 \uc5bd\ud788\uace0 \uac00\uc6b4\ub370\uc758 \uc815\ubcf4\ub294 3x3 \ud544\ud130\ub85c 3\ubc88 \ub4a4\uc5bd\ud799\ub2c8\ub2e4. \uc774\ub85c \uc778\ud574 \uac00\uc7a5\uc790\ub9ac\uc5d0 \ub300\ud55c \uc815\ubcf4\uac00 \uc190\uc2e4\ub429\ub2c8\ub2e4. \ud328\ub529\uc73c\ub85c \uc778\ud574 \uc5d0\uc9c0 \uc815\ubcf4\uac00 \uc5ec\ub7ec \ubc88 \ubcf5\uc7a1\ud574\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n2. \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud560 \ub54c\ub9c8\ub2e4 \ucd9c\ub825 \ub9e4\ud2b8\ub9ad\uc2a4 \ud06c\uae30\uac00 \uc791\uc544\uc9d1\ub2c8\ub2e4. \ud328\ub529\uc740 \uc785\ub825 \ubcfc\ub968\uc758 \ud06c\uae30\ub97c \ucd9c\ub825 \ubcfc\ub968\uc5d0 \uc720\uc9c0\ud574\uc57c \ud560 \ub54c CNN \ub808\uc774\uc5b4\ub97c \uc124\uacc4 \ud560 \ub54c \uc8fc\ub85c \uc0ac\uc6a9\ub429\ub2c8\ub2e4 (** \ub3d9\uc77c \ud328\ub529 **).\n    \n### \ud328\ub529 \ud504\ub85c\uc138\uc2a4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd9c\ub825 \ucc28\uc6d0 \ubcc0\uacbd\uc744 \uacc4\uc0b0\ud558\ub294 \ubc29\uc815\uc2dd <a id=\"132\"><\/a>\n\n\\begin{equation}\nn^{l} = \\frac{n^{l-1} + 2p - f}{s}+1 \\\\\n\\text{output} =  n^l x n^l\n\\end{equation}\n","516d6527":"### **Hyper Parameters of Pooling Operation** <a id=\"151\"><\/a>\n1. Pooling Size\n2. Stride","031b1822":"### **F1 Score Calculation** <a id=\"274\"><\/a>\n<mark>[Return Contents](#0)","d5535a6b":"#### **On-the-Fly Data Augmentation** <a id=\"257\"><\/a>\n<mark>[Return Contents](#0)\n* On classification tasks on image datasets data augmentation is a common way to increase the generalization of the model. \n\n\n\n* With the ImageDataGenerator on Keras, we can handle this objective easily.\n* [Here](https:\/\/www.pyimagesearch.com\/2019\/07\/08\/keras-imagedatagenerator-and-data-augmentation\/) is a comprehensive and inspiring article about data augmentation and ImageDataGenerator written by [Adrian Rosebrock](https:\/\/www.pyimagesearch.com\/2019\/07\/08\/keras-imagedatagenerator-and-data-augmentation\/) who is the author of PyImageSearch a very instructive web-site about computer vision.\n* __By changing__ some of the properties of the image from the code below, __you can observe what changes are happening in the dataset__. \n* With this observation, you can roughly specify the range you should choose.","897b8244":"### **Test Set Accuracy Score** <a id=\"272\"><\/a>\n<mark>[Return Contents](#0)","f3bfad8e":"#### **What is Batch Norm?** <a id=\"161\"><\/a>\n \n* Batch Norm\uc740 \ud65c\uc131\ud654\ub97c \uc870\uc815\ud558\uace0 \uc870\uc815\ud558\uc5ec \uc228\uaca8\uc9c4 \ub808\uc774\uc5b4\uc758 \uc785\ub825\uc744 \uc815\uaddc\ud654\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.\n* \uc774 \ubc29\ubc95\uc740 \uac01 \ud6c8\ub828 \ubbf8\ub2c8 \ubc30\uce58\uc5d0\uc11c \ud2b9\uc815 \uce35\uc758 \uac01 \ub274\ub7f0\uc5d0 \ub300\ud574 \uc815\uaddc\ud654\ub97c \uc218\ud589\ud558\ub294 \uac83\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.\n* $1$ \ub808\uc774\uc5b4\uc758 \uc815\uaddc\ud654 \ub41c \ub274\ub7f0\uc740 \ud6c8\ub828 \ub41c $\\gamma$ \ubc0f $\\beta$ \ub9e4\uac1c \ubcc0\uc218\uc5d0 \ub530\ub77c \ubb38\uc81c\uc5d0 \ub300\ud55c \ud2b9\uc815 \ud3c9\uade0 \ubc0f \ubd84\uc0b0\uc744 \uac16\uc2b5\ub2c8\ub2e4.\n* Batch Norm\uc758 \uc54c\uace0\ub9ac\uc998\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\n![batch_norm.png](attachment:batch_norm.png)","3f1bb851":"![pooling.png](attachment:pooling.png)","66c9444b":"> * In order to feed the CNN model we need to reshape our $54000$x$784$ flatten image data to $54000$x$28$x$28$x$1$ dimensions.;","05f1575e":"#### **Compile ->Loss Function ->Categorical Crossentropy** <a id=\"253\"><\/a>\n<mark>[Return Contents](#0)\n    \n* The Categorical Crossentropy Loss Function computes between network predictions and target values for multiclass classification problems.\n\n* The loss is calculated using the following formula;\n\\begin{equation}\nLoss = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K} Y^i_klog(\\hat{Y^i_k})(1-Y^i_k)log(1-\\hat{Y^i_k}) \\\\\n\\end{equation}\n\nwhere $k$ demonstrates class, $i$ demonstrates sample number, $\\hat{Y_c}$ is the predicted value, $Y_c$ is the ground truth value, $m$ is the sample number in a batch and $K$ is the total number of classes.\n\n* Why do we use log? Because cross-entropy function penalize bigger difference more and smaller difference less as mentioned below.\n\n![cross_entropy.png](attachment:cross_entropy.png)","81cb2978":"#### **Image of Handwritten Character** <a id=\"2\"><\/a>\n<mark>[Return Contents](#0)\n<hr>\n    \n * An overview of a picture\n * You can change the $num$ variable to see other numbers.","a2cfffda":"* \ud480\ub9c1\uc774\ub780 \ub9e4\ud2b8\ub9ad\uc2a4\ub97c \ud480\ub85c \ub098\ub204\uace0 \ud574\ub2f9 \ud480\uc744 \ub098\ud0c0\ub0b4\ub294 \uac83\uc73c\ub85c\uc11c \ud558\ub098\uc758 \uac12\ub9cc \ubaa8\ub4e0 \ud480\uc5d0\uc11c \uc120\ud0dd\ud558\ub294 \ud504\ub85c\uc138\uc2a4\ub97c \ub9d0\ud569\ub2c8\ub2e4.\n* \ucee8\ubcfc \ub8e8\uc158 \uc791\uc5c5\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ud480\ub9c1\uc5d0\ub294 \ud544\ud130\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uac00\uc911\uce58\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.\n* \uc18c\uadf8\ub8f9\uc5d0\uc11c \ud558\ub098\uc758 \uac12\ub9cc \uc120\ud0dd\ud558\ub294 \uac83\uc785\ub2c8\ub2e4."}}