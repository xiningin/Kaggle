{"cell_type":{"5264747a":"code","4b170b6d":"code","4d0c04e2":"code","8af18206":"code","e09a5523":"code","5d9feecf":"code","0947500d":"code","fd4a7957":"code","96b7928e":"code","bd5af378":"code","b4d7b897":"code","1eb2dbc4":"code","bf43c5ef":"code","885d343e":"code","8e46d3dd":"code","ca102150":"code","216041b1":"code","212d630f":"code","6a0405cc":"code","90677ff8":"code","6080bce1":"code","53377353":"code","59a3ed56":"code","85fa5af2":"code","313eed2f":"code","fd37229a":"code","8b1ef51b":"code","4dea9109":"code","6ce9eee3":"code","1b2d1c5e":"code","451fe527":"markdown","7c377ae0":"markdown","06fb3836":"markdown","85f92db3":"markdown","42faab38":"markdown"},"source":{"5264747a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b170b6d":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, RNN, SpatialDropout1D, Bidirectional\n\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))","4d0c04e2":"import pandas as pd \n\ntrain = pd.read_csv('\/kaggle\/input\/fake-news\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/fake-news\/test.csv')\n\ntrain = train.set_index('id', drop=True)\ntest = test.set_index('id', drop=True)","8af18206":"train[['title', 'author']] = train[['title', 'author']].fillna(value='missing')\ntrain=train.dropna()\ntrain.isnull().sum()","e09a5523":"length = []\n[length.append(len(str(text))) for text in train['text']]\ntrain['length'] = length\ntrain.head()","5d9feecf":"train = train.drop(train['text'][train['length'] < 50].index, axis = 0)","0947500d":"train.shape","fd4a7957":"length = []\n[length.append(len(str(text))) for text in test['text']]\ntest['length'] = length\ntest.head()","96b7928e":"min(train['length']), max(train['length']), round(sum(train['length'])\/len(train['length']))","bd5af378":"min(test['length']), max(test['length']), round(sum(test['length'])\/len(test['length']))","b4d7b897":"max_features = 4500","1eb2dbc4":"tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ', oov_token='<OOV>')\ntokenizer.fit_on_texts(texts = train['text'])\nX = tokenizer.texts_to_sequences(texts = train['text'])\nX = pad_sequences(sequences = X, maxlen = max_features, padding = 'post')\nX","bf43c5ef":"print(X.shape)\ny = train['label'].values\nprint(y.shape)","885d343e":"X_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size = 0.2, random_state = 101)","8e46d3dd":"y_train","ca102150":"X_train","216041b1":"X_test","212d630f":"y_test","6a0405cc":"\nmodel = Sequential([\n                    Embedding(input_dim=max_features, output_dim=120),\n                    Bidirectional(LSTM(units=120, \n                         activation='tanh',\n                         recurrent_activation='sigmoid',\n                         unroll=False,\n                         use_bias=True,\n                         dropout = 0.2, \n                         recurrent_dropout = 0\n                         )),\n                    # Dropout(rate = 0.5),\n                    Dense(units = 120,  activation = 'relu'),\n                    # Dropout(rate = 0.5),\n                    Dense(units = len(set(y)),  activation = 'sigmoid', name = 'output_layer')\n])\n\n\nmodel.summary()\n","90677ff8":"model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","6080bce1":"with tf.device('\/device:GPU:0'):\n  model_fit = model.fit(\n      X_train, y_train, epochs = 1, verbose=1)","53377353":"from sklearn.metrics import accuracy_score\nprediction = model.predict_classes(\n    X_test\n)\naccuracy_score(y_test, prediction)","59a3ed56":"test[['title', 'author']] = test[['title', 'author']].fillna(value='missing')\ntest = test.fillna(' ')","85fa5af2":"test.shape","313eed2f":"test_sequences = tokenizer.texts_to_sequences(test['text'])\ntest_sequences = pad_sequences(sequences = test_sequences, maxlen = max_features, padding = 'post')\ntest_sequences","fd37229a":"type(test['text'])","8b1ef51b":"test_prediction = model.predict_classes(\n    test_sequences\n)","4dea9109":"submission = pd.DataFrame({'id':test.index, 'label':test_prediction})\nsubmission.shape","6ce9eee3":"submission.head()","1b2d1c5e":"submission.to_csv('submission.csv', index=False)","451fe527":"# Data Preparation","7c377ae0":"# Submission","06fb3836":"# Testing model on test data","85f92db3":"# Building Sequential Model","42faab38":"# Removing Outliers"}}