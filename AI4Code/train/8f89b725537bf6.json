{"cell_type":{"34718505":"code","e88e60e9":"code","8dbb7f7f":"code","58ba561c":"code","2e22f0bd":"code","a6302f7c":"code","77dab687":"code","a6e4938f":"code","892f7c81":"code","30ca5dc9":"code","52f23517":"code","e073611f":"code","062e01ee":"code","b8238a53":"code","1e083084":"code","ed6c5bda":"code","75171061":"code","20e1694b":"code","fa6dfa4a":"code","ff63cbdd":"code","8757f748":"code","baf04985":"code","a682654b":"code","c453222c":"code","6dd9e185":"code","9265886d":"code","a711e2e8":"code","ef7633f4":"code","f04242a7":"code","b5327a5c":"code","71aec5f4":"code","897c01cd":"code","9aaedeac":"code","3e9487d1":"code","4f1e3237":"code","f4379502":"code","d5b10051":"markdown","2c820341":"markdown","5b0f8961":"markdown","03bd0205":"markdown","51e9024e":"markdown","963e96a4":"markdown","55b629db":"markdown","8b8db711":"markdown","db4316da":"markdown","50154fa7":"markdown","8cf33c26":"markdown","327cf954":"markdown","e9cc4e2a":"markdown","58867870":"markdown","65c7d160":"markdown","80f93d9e":"markdown","a213a24c":"markdown","befbf9d9":"markdown","e4443c6a":"markdown","cefde75e":"markdown","eb8395dc":"markdown","09f7b1d0":"markdown","f659f07b":"markdown","138af476":"markdown","aa32f2f2":"markdown","209a5a60":"markdown","62636997":"markdown","70b8e5cf":"markdown","5f571c6d":"markdown","82692367":"markdown","f6f90fce":"markdown","02434c93":"markdown","7975eeb6":"markdown","05ac5a77":"markdown"},"source":{"34718505":"import numpy as np \nimport pandas as pd \nimport os\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestRegressor\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n","e88e60e9":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","8dbb7f7f":"rain = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\nrain.head(10)","58ba561c":"print(f'The number of rows are {rain.shape[0] } and the number of columns are {rain.shape[1]}')","2e22f0bd":"rain.info()","a6302f7c":"categorical_col, contin_val=[],[]\n\nfor i in rain.columns:\n    \n    if rain[i].dtype == 'object':\n        categorical_col.append(i)\n    else:\n        contin_val.append(i)\n        \nprint(categorical_col)\nprint(contin_val)","77dab687":"rain.nunique()\n","a6e4938f":"rain.isnull().sum()","892f7c81":"msno.matrix(rain)\n","30ca5dc9":"msno.bar(rain, sort='ascending')\n","52f23517":"msno.heatmap(rain)\n","e073611f":"plt.figure(figsize=(17,15))\nax = sns.heatmap(rain.corr(), square=True, annot=True, fmt='.2f')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)          \nplt.show()\n\n","062e01ee":"rain['RainTomorrow'] = rain['RainTomorrow'].map({'Yes': 1, 'No': 0})\nrain['RainToday'] = rain['RainToday'].map({'Yes': 1, 'No': 0})\n\nprint(rain.RainToday)\nprint(rain.RainTomorrow)\n","b8238a53":"#Checking percentage of missing data in every column\n\n(rain.isnull().sum()\/len(rain))*100\n","1e083084":"#Filling the missing values for continuous variables with mean\nrain['MinTemp']=rain['MinTemp'].fillna(rain['MinTemp'].mean())\nrain['MaxTemp']=rain['MinTemp'].fillna(rain['MaxTemp'].mean())\nrain['Rainfall']=rain['Rainfall'].fillna(rain['Rainfall'].mean())\nrain['Evaporation']=rain['Evaporation'].fillna(rain['Evaporation'].mean())\nrain['Sunshine']=rain['Sunshine'].fillna(rain['Sunshine'].mean())\nrain['WindGustSpeed']=rain['WindGustSpeed'].fillna(rain['WindGustSpeed'].mean())\nrain['WindSpeed9am']=rain['WindSpeed9am'].fillna(rain['WindSpeed9am'].mean())\nrain['WindSpeed3pm']=rain['WindSpeed3pm'].fillna(rain['WindSpeed3pm'].mean())\nrain['Humidity9am']=rain['Humidity9am'].fillna(rain['Humidity9am'].mean())\nrain['Humidity3pm']=rain['Humidity3pm'].fillna(rain['Humidity3pm'].mean())\nrain['Pressure9am']=rain['Pressure9am'].fillna(rain['Pressure9am'].mean())\nrain['Pressure3pm']=rain['Pressure3pm'].fillna(rain['Pressure3pm'].mean())\nrain['Cloud9am']=rain['Cloud9am'].fillna(rain['Cloud9am'].mean())\nrain['Cloud3pm']=rain['Cloud3pm'].fillna(rain['Cloud3pm'].mean())\nrain['Temp9am']=rain['Temp9am'].fillna(rain['Temp9am'].mean())\nrain['Temp3pm']=rain['Temp3pm'].fillna(rain['Temp3pm'].mean())\n\n","ed6c5bda":"#Filling the missing values for continuous variables with mode\n\nrain['RainToday']=rain['RainToday'].fillna(rain['RainToday'].mode()[0])\nrain['RainTomorrow']=rain['RainTomorrow'].fillna(rain['RainTomorrow'].mode()[0])\n","75171061":"#Filling the missing values for continuous variables with mode\nrain['WindDir9am'] = rain['WindDir9am'].fillna(rain['WindDir9am'].mode()[0])\nrain['WindGustDir'] = rain['WindGustDir'].fillna(rain['WindGustDir'].mode()[0])\nrain['WindDir3pm'] = rain['WindDir3pm'].fillna(rain['WindDir3pm'].mode()[0])","20e1694b":"#Checking percentage of missing data in every column\n\n(rain.isnull().sum()\/len(rain))*100\n","fa6dfa4a":"fig, ax =plt.subplots(1,2)\nprint(rain.RainToday.value_counts())\nprint(rain.RainTomorrow.value_counts())\n\nplt.figure(figsize=(20,20))\nsns.countplot(data=rain,x='RainToday',ax=ax[0])\nsns.countplot(data=rain,x='RainTomorrow',ax=ax[1])","ff63cbdd":"fig, ax =plt.subplots(3,1)\nplt.figure(figsize=(10,10))\n\nsns.countplot(data=rain,x='WindDir9am',ax=ax[0])\nsns.countplot(data=rain,x='WindDir3pm',ax=ax[1])\nsns.countplot(data=rain,x='WindGustDir',ax=ax[2])\nfig.tight_layout()\n","8757f748":"#Dropping date column\nrain=rain.iloc[:,1:]\nrain","baf04985":"le = preprocessing.LabelEncoder()\nrain['Location'] = le.fit_transform(rain['Location'])\nrain['WindDir9am'] = le.fit_transform(rain['WindDir9am'])\nrain['WindDir3pm'] = le.fit_transform(rain['WindDir3pm'])\nrain['WindGustDir'] = le.fit_transform(rain['WindGustDir'])\n\n","a682654b":"rain.head(5)","c453222c":"fig, ax =plt.subplots(2,1)\nplt.figure(figsize=(10,10))\nsns.boxplot(rain['Humidity3pm'],orient='v',color='c',ax=ax[0])\nsns.boxplot(rain['Humidity9am'],orient='v',color='c',ax=ax[1])\nfig.tight_layout()\n","6dd9e185":"fig, ax =plt.subplots(2,1)\nplt.figure(figsize=(10,10))\nsns.boxplot(rain['Pressure3pm'],orient='v',color='c',ax=ax[0])\nsns.boxplot(rain['Pressure9am'],orient='v',color='c',ax=ax[1])\nfig.tight_layout()\n","9265886d":"\nsns.violinplot(x='RainToday',y='MaxTemp',data=rain,hue='RainTomorrow')\n","a711e2e8":"sns.violinplot(x='RainToday',y='MinTemp',data=rain,hue='RainTomorrow')\n","ef7633f4":"print('Shape of DataFrame Before Removing Outliers', rain.shape )\nrain=rain[(np.abs(stats.zscore(rain)) < 3).all(axis=1)]\nprint('Shape of DataFrame After Removing Outliers', rain.shape )\n","f04242a7":"rain=rain.drop(['Temp3pm','Temp9am','Humidity9am'],axis=1)\nrain.columns","b5327a5c":"x_train, x_test, y_train, y_test = train_test_split(rain.iloc[:,:-1], rain.iloc[:,-1], test_size=0.2, random_state=42)\n","71aec5f4":"os = SMOTE()\nx_train, y_train = os.fit_resample(x_train, y_train)\ncount = Counter(y_train)\nprint(count)","897c01cd":"model = LogisticRegression(max_iter=500)\nmodel.fit(x_train, y_train)\npredicted=model.predict(x_test)\n\nconf = confusion_matrix(y_test, predicted)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"F1 score for logistic regression is :\",f1_score(y_test, predicted,)*100, \"%\")\n","9aaedeac":"xgbc = XGBClassifier(objective='binary:logistic')\nxgbc.fit(x_train,y_train)\npredicted = xgbc.predict(x_test)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"F1 score for XGBoost is :\",f1_score(y_test, predicted,)*100, \"%\")\n","3e9487d1":"model = GaussianNB()\nmodel.fit(x_train, y_train)\n  \npredicted = model.predict(x_test)\n  \nprint(\"The accuracy of Gaussian Naive Bayes model is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"F1 score for Gaussian Naive Bayes is :\",f1_score(y_test, predicted,)*100, \"%\")\n","4f1e3237":"model = BernoulliNB()\nmodel.fit(x_train, y_train)\n  \npredicted = model.predict(x_test)\n  \nprint(\"The accuracy of Gaussian Naive Bayes model is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"F1 score for Bernoulli Naive Bayes is :\",f1_score(y_test, predicted,)*100, \"%\")","f4379502":"model = RandomForestRegressor(n_estimators = 100, random_state = 0)  \nmodel.fit(x_train, y_train)  \npredicted = model.predict(x_test)\nprint(\"The accuracy of Random Forest is : \", accuracy_score(y_test, predicted.round())*100, \"%\")\n","d5b10051":"# **1.3 Shape of DataFrame**","2c820341":"**3.2 XGBoost**","5b0f8961":"# **1.8 Changing yes and no to 1 and 0 in some columns**","03bd0205":"**2.6 Removing the outliers**","51e9024e":"**3.3 Gaussian Naive Bayes**","963e96a4":"**2.9 Balancing the data using SMOTE**","55b629db":"# **1.4 Describing the attributes**","8b8db711":"**2.4 Boxplots**","db4316da":"# **1.6 Checking Null values**","50154fa7":"**All the missing values have been removed now.**","8cf33c26":"**3.1 Logistic Regression**","327cf954":"# **3. Training The Models**","e9cc4e2a":"**2.7 Dropping highly correlated columns**","58867870":"* At 9 am, it is highest for direction N.\n* At 3 pm, it is highest for direction SE.\n","65c7d160":"**Violin Plot**","80f93d9e":"**2.1 Count of rain today and tomorrow**","a213a24c":"# **2. Data Visualization**","befbf9d9":"**6 columns are of type 'object' and remaining of 'float'**","e4443c6a":"# **1.1 Reading the dataset**","cefde75e":"# **1.2 Creating DataFrame**","eb8395dc":"**Encoding the categorical variables**","09f7b1d0":"# **1.5 Finding all the categorical and continuous values**","f659f07b":"# **1.6 Unique values**","138af476":"****2.5 Bivariate Analysis****","aa32f2f2":"**3.4 Bernoulli Naive Bayes**","209a5a60":"*  MinTemp and Temp9am highly correlated.\n*  MinTemp and Temp3pm highly correlated.\n*  MaxTemp and Temp9am highly correlated.\n*  MaxTemp and Temp3pm highly correlated.\n\n*  Temp3pm and Temp9am highly correlated.\n*  Humidity9am and Humidity3pm highly correlated.","62636997":"The above graphs show that the number of missing values are high in: Sunshine, Evaporation, Cloud3pm and Cloud9am.","70b8e5cf":"**2.8 Train test split**","5f571c6d":"# **1.7 Visualizing the missing values**","82692367":"**2.3 HeatMap**","f6f90fce":"# **1.9 Dealing with the missing values**","02434c93":"# **1. Importing the modules**","7975eeb6":"**3.5 RandomForest**","05ac5a77":"**2.2 Direction of wind at 9 am, 3 pm.**"}}