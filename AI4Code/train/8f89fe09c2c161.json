{"cell_type":{"ba2db681":"code","b25139f3":"code","80eb36b4":"code","d250f04f":"code","e80a8da2":"code","e83ef286":"code","d49e8b52":"code","409ad31e":"code","cbcf1b43":"code","a30ebd4a":"code","43c1319d":"code","e500dc8f":"code","4d37ede7":"code","de740e3c":"code","a6dd5841":"code","95945016":"code","90200372":"code","d48b08ce":"code","97bfe29f":"code","a9d1ae7b":"code","a336a54a":"code","dc0d08a3":"code","d266867d":"code","05ffd204":"code","ad3f4399":"code","85ede6a2":"code","3f9e0733":"code","3d23d671":"code","3b57aeb7":"code","baf06496":"code","a270ac1b":"code","b7926943":"code","970ef2e5":"markdown","5b6f1dd3":"markdown","cd3aa3ee":"markdown","f3821f3c":"markdown","bb658810":"markdown","87cc5f9b":"markdown","343fb8d8":"markdown","730ca436":"markdown","1188e76a":"markdown","65418046":"markdown","a26298e2":"markdown","0a61d94c":"markdown","865b1440":"markdown","3c0a7f38":"markdown","05180150":"markdown","a06c901e":"markdown","d5dd6636":"markdown","869abcaf":"markdown","93f1b7b5":"markdown","95042976":"markdown","809ea488":"markdown","f2968e2a":"markdown"},"source":{"ba2db681":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b25139f3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(0)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom tensorflow import keras\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.activations import selu","80eb36b4":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","d250f04f":"print(df_train.shape)\nprint(df_test.shape)","e80a8da2":"df_train.head()","e83ef286":"df_train.columns","d49e8b52":"df_train['label'].unique()","409ad31e":"df_train.describe()","cbcf1b43":"y_train = df_train['label']\nx_train = df_train.drop(['label'],axis=1)","a30ebd4a":"g = sns.countplot(x=y_train)\ng.set_ylabel('Frequency of occurence')\ng.set_xlabel('Number')\ng.set_title('Frequency of each digit in the training set')","43c1319d":"#Using the general format of testing for NULL values\nx_train.isnull().sum()","e500dc8f":"#Checking presence of NULL values in the training set:\nnan_cols = [i for i in x_train.columns if x_train[i].isnull().any()]\nnan_cols","4d37ede7":"#Checking presence of NULL values in the test set\nnan_test = [i for i in df_test.columns if df_test[i].isnull().any()]\nnan_test","de740e3c":"#Divide by float to preserve fractional component\n\nx_train = x_train\/255.0 \ndf_test = df_test\/255.0","a6dd5841":"x_train = x_train.values.reshape(-1,28,28,1)\ndf_test = df_test.values.reshape(-1,28,28,1)","95945016":"print(x_train.shape)\nprint(df_test.shape)","90200372":"n_rows = 2\nn_cols = 2 \nc = 0\nfig, ax = plt.subplots(n_rows,n_cols,sharex=True,sharey=True)\nfor i in range(0,n_rows):\n    for j in range (0,n_cols):\n        ax[i,j].imshow(x_train[c][:,:,0])\n        ax[i,j].set_title(y_train[c])\n        c=c+1","d48b08ce":"#We have 10 classes (0-9)\ny_train = to_categorical(y_train,num_classes=10) #Will convert values like 1 to [0 1 0 0 0 0 0 0 0 0 ]","97bfe29f":"rand = 0","a9d1ae7b":"#Split the training into training and validation dataset for our CNN\n\nx_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=rand)","a336a54a":"model = Sequential()\nmodel.add(Conv2D(filters=32,kernel_size=(5,5),padding='Same',activation='selu',kernel_initializer='lecun_normal'))\nmodel.add(Conv2D(filters=32,kernel_size=(5,5),padding='Same',activation='selu',kernel_initializer='lecun_normal'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding='Same',activation='selu',kernel_initializer='lecun_normal'))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding='Same',activation='selu',kernel_initializer='lecun_normal'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dense(10, activation = \"softmax\"))","dc0d08a3":"model.compile(optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","d266867d":"img_gen = ImageDataGenerator(\n    featurewise_center=False, \n    samplewise_center=False,\n    featurewise_std_normalization=False, \n    samplewise_std_normalization=False,\n    rotation_range=10, \n    width_shift_range=0.1,\n    height_shift_range=0.1, \n    zoom_range=0.1,\n    horizontal_flip=False,\n    vertical_flip=False,\n)\n\nimg_gen.fit(x_train)","05ffd204":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=0, mode='auto',min_delta=0.0001, cooldown=0, min_lr=0.00001)\neas_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',baseline=None, restore_best_weights=True)","ad3f4399":"n_epochs = 30\nhistory = model.fit_generator(img_gen.flow(x_train, y_train),epochs=n_epochs,validation_data=(x_val, y_val),callbacks=[reduce_lr])","85ede6a2":"#Final optimisations of the model:\npd.DataFrame(history.history).plot(figsize=(10,10))\nplt.grid(True)\nplt.show()","3f9e0733":"yp = model.predict(x_val)","3d23d671":"yp.shape","3b57aeb7":"y_class = np.argmax(yp,axis = 1) \ny = np.argmax(y_val,axis = 1) \nplt.figure(figsize=(10,10))\nconfusion_mtx = confusion_matrix(y, y_class) \nsns.heatmap(confusion_mtx,annot=True,cmap='Blues')","baf06496":"error = (y_class - y)!=0\ny_class_error = y_class[error]\ny_error = y[error]\nx_val_error = x_val[error]\nn_rows = 2\nn_cols = 2\nc=0\nfig, ax = plt.subplots(n_rows,n_cols,sharex=True,sharey=True,figsize=(10,10))\nfor i in range(0,n_rows):\n    for j in range(0,n_cols):\n        ax[i,j].imshow((x_val_error[c]).reshape((28,28)))\n        ax[i,j].set_title((\"Predicted label :{}\\nTrue label :{}\".format(y_class_error[c],y_error[c])))\n        c=c+1\n    ","a270ac1b":"# predict results\nresults = model.predict(df_test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","b7926943":"history = model.fit_generator(img_gen.flow(x_train, y_train),epochs=10,validation_data=(x_val, y_val),callbacks=[reduce_lr])","970ef2e5":"# Confusion matrix","5b6f1dd3":"# One hot encoding ","cd3aa3ee":"We cannot incorporate horizontal and vertical flips as for cases with 6 and 9, it may hurt the dataset's training.\n\nThis above function will introduce the following operations:\n\n* Randomly rotate some images by 10 degrees\n* Randomly zoom 10% training images\n* Randomly shift training images 10% horizontally and vertically","f3821f3c":"# Importing data","bb658810":"# Checking for errors","87cc5f9b":"<h2>Loading data<\/h2>","343fb8d8":"# Viewing example images","730ca436":"We will encode the labels in the dataset to one-hot vectors","1188e76a":"We see that we have too many columns to comfortably display the NULL value sum in each column, therefore we will use list comprehensions","65418046":"Here, we see that we are able to get most of the predictions right except a few of them still left as wrongly labelled","a26298e2":"We proceed to reshape the data in a correct format for our convolutional network to process: (Batch size, height, width, channels)\n\nHere the channel would be 1 as we are dealing with grayscale images, unlike RGB where we would have it as 3","0a61d94c":"We can visualize these images using matplotlib","865b1440":"# Reshaping data","3c0a7f38":"We see that the pixels are divided in the value range of 0-255 which is the norm for grayscale images.","05180150":"We will introduce horizontal shifts, vertical shifts and zoomed training data in the dataset","a06c901e":"We have a total of 784 pixels (28 x 28) and the label column which corresponds to the digit the image signifies","d5dd6636":"We will first proceed to import the csv training and testing data into their respective dataframes using `read_csv`","869abcaf":"# Normalizing\n\nWe can normalize the dataset to have values in the range (0-1) by diving all values by 255.0","93f1b7b5":"We achieve a very high validation accuracy (0.9883), we can further improve this by introducing data augmentation as well as some learning rate scheduling.","95042976":"Therefore, we see that none of the columns containing any NULL values, therefore we can continue accordingly.","809ea488":"We see that the dataset does not suffer from any form of imbalance. Each number has adequate training cases.","f2968e2a":"# NULL values"}}