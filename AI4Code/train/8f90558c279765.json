{"cell_type":{"96712c1d":"code","2034ae38":"code","ef458157":"code","710d4f4c":"code","38dd89d4":"code","3250c4b7":"code","dff0c23c":"code","e6509921":"code","d8825042":"code","bc8f02b4":"code","0e77abca":"code","b2e76eda":"code","a931b4be":"code","7fb92440":"code","1ab84534":"code","6598bdf0":"code","738a7370":"code","e2b5f366":"code","a9698431":"code","f62dbbb8":"code","4dec7888":"code","6a6efd6f":"code","8fe07e35":"code","50bc5ec0":"code","7d154229":"code","6f913146":"code","aa2b5977":"code","f495eadc":"code","66719abd":"code","ed7dcb05":"code","1bc8d4b2":"code","0552e155":"code","f9531e45":"code","e44d660b":"code","cf7fb77d":"code","a7528480":"code","1a67fe47":"code","8ace9e23":"code","78d003b6":"code","40356c7f":"code","e444a138":"code","51d6b1c9":"code","ab8f848b":"code","ef5622ea":"code","7156e578":"code","5990d142":"code","3387c13f":"code","646230d0":"code","a765d5ca":"code","5face9d9":"code","ef06ab0e":"code","8ae97053":"code","72702bed":"code","d851adc1":"code","8d635271":"code","c54d5c97":"code","2257deb0":"code","44a35f4e":"code","29d2d562":"code","b0ad79b7":"code","430de7f8":"code","8676d84b":"code","bd0465e3":"code","51876685":"markdown","50fccba2":"markdown","292ff950":"markdown","b8ee2d8b":"markdown","8f3586b7":"markdown","852c2065":"markdown","ce887655":"markdown","903b4072":"markdown","82819887":"markdown","17aca424":"markdown","9a605e88":"markdown","b3e43976":"markdown","f76904ee":"markdown","38945976":"markdown","2471bb2c":"markdown","70e56bda":"markdown","5459a89f":"markdown","3e452dec":"markdown","3e225a9e":"markdown","a07d27c4":"markdown"},"source":{"96712c1d":"import pandas as pd\nimport numpy as np","2034ae38":"# example text for model training (SMS messages)\nsimple_train = ['call you tonight', 'Call me a cab', 'Please call me... PLEASE!']","ef458157":"# import and instantiate CountVectorizer (with the default parameters)\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()","710d4f4c":"# learn the 'vocabulary' of the training data (occurs in-place)\nvect.fit(simple_train)","38dd89d4":"# examine the fitted vocabulary\nvect.get_feature_names()","3250c4b7":"# transform training data into a 'document-term matrix'\nsimple_train_dtm = vect.transform(simple_train)\nsimple_train_dtm","dff0c23c":"# convert sparse matrix to a dense matrix\nsimple_train_dtm.toarray()","e6509921":"# examine the vocabulary and document-term matrix together\npd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())","d8825042":"# check the type of the document-term matrix\ntype(simple_train_dtm)","bc8f02b4":"# examine the sparse matrix contents\nprint(simple_train_dtm)","0e77abca":"# example text for model testing\nsimple_test = [\"please don't call me\"]","b2e76eda":"# transform testing data into a document-term matrix (using existing vocabulary)\nsimple_test_dtm = vect.transform(simple_test)\nsimple_test_dtm.toarray()","a931b4be":"# examine the vocabulary and document-term matrix together\npd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())","7fb92440":"# read file into pandas using a relative path\n# sms = pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv', delimiter='\\t', names=['label', 'message'])\nsms = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin-1')\nsms.head()","1ab84534":"sms.dropna(how=\"any\", inplace=True, axis=1)","6598bdf0":"sms.columns = ['label', 'message']\nsms.head()","738a7370":"# examine the shape\nsms.shape","e2b5f366":"# examine the class distribution\nsms.label.value_counts()","a9698431":"# convert label to a numerical variable\nsms['label_num'] = sms.label.map({'ham':0, 'spam':1})\nsms.head()","f62dbbb8":"# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\nX = sms.message\ny = sms.label_num\nprint(X.shape)\nprint(y.shape)","4dec7888":"# split X and y into training and testing sets \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","6a6efd6f":"# instantiate the vectorizer\nvect = CountVectorizer()","8fe07e35":"# learn training data vocabulary, then use it to create a document-term matrix\nvect.fit(X_train)\nX_train_dtm = vect.transform(X_train)","50bc5ec0":"# equivalently: combine fit and transform into a single step\nX_train_dtm = vect.fit_transform(X_train)","7d154229":"# examine the document-term matrix\nX_train_dtm","6f913146":"# transform testing data (using fitted vocabulary) into a document-term matrix\nX_test_dtm = vect.transform(X_test)\nX_test_dtm","aa2b5977":"# import and instantiate a Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","f495eadc":"# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n%time nb.fit(X_train_dtm, y_train)","66719abd":"# make class predictions for X_test_dtm\ny_pred_class = nb.predict(X_test_dtm)","ed7dcb05":"# calculate accuracy of class predictions\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","1bc8d4b2":"# print the confusion matrix\nmetrics.confusion_matrix(y_test, y_pred_class)","0552e155":"X_test.shape","f9531e45":"# print message text for false positives (ham incorrectly classifier)\n# X_test[(y_pred_class==1) & (y_test==0)]\nX_test[y_pred_class > y_test]","e44d660b":"# print message text for false negatives (spam incorrectly classifier)\nX_test[y_pred_class < y_test]","cf7fb77d":"# example of false negative \nX_test[3132]","a7528480":"# calculate predicted probabilities for X_test_dtm (poorly calibrated)\ny_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob","1a67fe47":"# calculate AUC\nmetrics.roc_auc_score(y_test, y_pred_prob)","8ace9e23":"# import an instantiate a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear')","78d003b6":"# train the model using X_train_dtm\n%time logreg.fit(X_train_dtm, y_train)","40356c7f":"# make class predictions for X_test_dtm\ny_pred_class = logreg.predict(X_test_dtm)","e444a138":"# calculate predicted probabilities for X_test_dtm (well calibrated)\ny_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob","51d6b1c9":"# calculate accuracy\nmetrics.accuracy_score(y_test, y_pred_class)","ab8f848b":"# calculate AUC\nmetrics.roc_auc_score(y_test, y_pred_prob)","ef5622ea":"# store the vocabulary of X_train\nX_train_tokens = vect.get_feature_names()\nlen(X_train_tokens)","7156e578":"# examine the first 50 tokens\nprint(X_train_tokens[:50])","5990d142":"# examine the last 50 tokens\nprint(X_train_tokens[-50:])","3387c13f":"# Naive Bayes counts the number of times each token appears in each class\nnb.feature_count_","646230d0":"# rows represent classes, columns represent tokens\nnb.feature_count_.shape","a765d5ca":"# number of times each tokens appears across all HAM meassages\nham_token_count = nb.feature_count_[0, :]\nham_token_count","5face9d9":"# number of times each tokens appears across all SPAM meassages\nspam_token_count = nb.feature_count_[1, :]\nspam_token_count","ef06ab0e":"# create a DataFrame of tokens with their separate ham and spam counts\ntokens = pd.DataFrame({'token':X_train_tokens, 'ham':ham_token_count, \n                       'spam':spam_token_count}).set_index('token')\ntokens.head()","8ae97053":"# examine 5 random DataFrame rows\ntokens.sample(5, random_state=6)","72702bed":"# Naive Bayes counts the number of observations in each class \nnb.class_count_","d851adc1":"# add 1 to ham and spam counts to avoid dividing by 0\ntokens['ham'] = tokens.ham + 1\ntokens['spam'] = tokens.spam + 1\ntokens.sample(5, random_state=6)","8d635271":"# convert the ham and spam counts into frequencies\ntokens['ham'] = tokens.ham \/ nb.class_count_[0]\ntokens['spam'] = tokens.spam \/ nb.class_count_[1]\ntokens.sample(5, random_state=6)","c54d5c97":"# calculate the ratio of spam-to-ham for each token\ntokens['spam_ratio'] = tokens.spam \/ tokens.ham\ntokens.sample(5, random_state=6)","2257deb0":"# examine the DataFrame sorted by spam_ratio\ntokens.sort_values(by='spam_ratio', ascending=False)","44a35f4e":"# look up spam_ratio for a given token\ntokens.loc['dating', 'spam_ratio']","29d2d562":"# show default parameters for CountVectorizer\nvect","b0ad79b7":"# remove English stop words\nvect = CountVectorizer(stop_words='english')","430de7f8":"# include 1-grams and 2-grams\nvect = CountVectorizer(ngram_range=(1, 2))","8676d84b":"# ignore terms that appear in more than 50% of the documents\nvect = CountVectorizer(max_df=0.5)","bd0465e3":"# only keep terms that appear in at least 2 documents\nvect = CountVectorizer(min_df=2)","51876685":"# 3. Vectorizing our dataset","50fccba2":"## Agenda\n\n1. Representing text as numerical data\n2. Reading a text-based dataset into pandas\n3. Vectorizing our dataset\n4. Building and evaluating a model\n5. Comparing models\n6. Examining a model for further insight\n7. Practicing this workflow on another dataset\n8. Tuning the vectorizer (discussion)","292ff950":"# Tutorial: Machine Learning with Text in scikit-learn\n\n***\n### **I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES<\/b><\/font> would be highly appreciated**\n***\n","b8ee2d8b":"From the [scikit-learn documentation](http:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction):\n\n> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n\n> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n\n> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package.","8f3586b7":"- **min_df**: float in range [0.0, 1.0] or int, default=1\n    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n    - If float, the parameter represents a proportion of documents.\n    - If integer, the parameter represents an absolute count.","852c2065":"**Summary:**\n\n- `vect.fit(train)` **learns the vocabulary** of the training data\n\n- `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data\n\n- `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data (and **ignores tokens** it hasn't seen before)","ce887655":"# 6. Examining a model for further insight\n\nWe will examine the our **trained Naive Bayes model** to calculate the approximate **\"spamminess\" of each token**.","903b4072":"- **max_df**: float in range [0.0, 1.0] or int, default=1.0\n    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n    - If float, the parameter represents a proportion of documents.\n    - If integer, the parameter represents an absolute count.","82819887":"- **ngram_range**: tuple (min_n, max_n), default=(1, 1)\n    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n    - All values of n such that min_n <= n <= max_n will be used.","17aca424":"- **Guidelines for tuning CountVectorizer**:\n\n    - Use your knowledge of the problem and the text, and your understanding of the tuning parameters, to help you decide what parameters to tune and how to tune them.\n    - Experiment, and let the data tell you the best approach!","9a605e88":"# 7. Tuning the vectorizer\n\nThus far, we have been using the default parameters of [CountVectorizer:](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html)","b3e43976":"However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:\n\n- **stop_words**: string {'english'}, list, or None (default)\n    - If 'english', a built-in stop word list for English is used.\n    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n    - If None, no stop words will be used.","f76904ee":"From the [scikit-learn documentation](http:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction):\n\n> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n\nWe will use [CountVectorizer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":","38945976":"Before we can calculate the \"spamminess\" of each token, we need to avoid **dividing by zero** and account for the **class imbalance**.","2471bb2c":"# 1. Representing text as numerical data","70e56bda":"# 4. Building and evaluating a model\n\nWe will use [multinomial Naive Bayes](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html):\n\n> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.","5459a89f":"In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning.","3e452dec":"From the [scikit-learn documentation](http:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction):\n\n> In this scheme, features and samples are defined as follows:\n\n> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n\n> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n\n> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.","3e225a9e":"# 2. Reading a text-based dataset into pandas","a07d27c4":"# 5. Caomparing models\n\nWe will compare multinomial Naive Bayes with [logistic regression](http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression):\n\n> Logistic regression, despite its name, is a **linear model for classification** rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function."}}