{"cell_type":{"4556ed85":"code","ee1ac9b9":"code","4cb182c1":"code","dbf2fd2b":"code","133bdb58":"code","d6934e36":"code","27bdcc47":"code","6347dc3e":"code","6b1526d9":"code","1c4e693b":"code","b3299034":"code","0087ca95":"code","1a3b4dca":"code","1273bdac":"code","a7915b97":"code","0de458b2":"code","a55a399d":"code","e8823724":"code","0d32aec3":"code","a150625c":"code","025506eb":"code","fcdb6e72":"code","f5013d1d":"code","2532a66c":"code","3235abca":"code","2c552983":"code","72b44a62":"code","f88801f1":"code","ff469fb9":"code","7f3ffbd4":"code","a66fbd9f":"code","82be5fe2":"code","877a3c88":"code","33a0080c":"code","bf996ade":"code","cb77e6f3":"code","af767079":"code","d66231ac":"code","1ab664c1":"code","cb7cb5a2":"code","5a00f714":"code","06abf387":"code","3fea11ab":"code","d8062eb7":"code","049cf557":"code","34ac4c1b":"code","3dcbaee0":"code","f827a73d":"code","e8b2c9e5":"code","52ef48f2":"code","4b3bf04b":"code","6caa658c":"code","cec1c4f1":"code","d4075de1":"code","4c927847":"code","c2a1d51b":"code","91fe9a9a":"code","c3a2cb64":"code","231c1a46":"code","ee625d4d":"code","c07dd0e0":"code","fcde2791":"code","179da68d":"code","a876c0ad":"code","a4b5ccc6":"code","54c5d8b6":"code","75f08de6":"code","a92c82eb":"code","d994194e":"code","c5376ee4":"code","f987a8a8":"code","13b2e013":"code","3876992a":"code","0c65364e":"code","a2ec9155":"code","7b5b6280":"code","737e7688":"code","cd641fcb":"code","f22b876a":"code","4476338f":"code","eff4c51b":"code","d5e59114":"code","b191a7ab":"code","5a50b82c":"code","c54d0bb2":"code","06606113":"code","0f01248c":"code","485c8a12":"code","c569f85a":"code","cc7f071e":"code","d0997ca5":"code","93922430":"code","319ab3d1":"code","863fb104":"code","190106dc":"code","994a4fb1":"code","145f3c20":"code","ac5c14a9":"code","143887b0":"code","e4a5d806":"code","480a3a30":"code","3b70bde2":"code","f697cb7f":"code","e8a0cd08":"code","2a8ad30f":"code","7ccc262e":"code","b2611ed2":"code","902aaa48":"code","0b65cd06":"code","fa67a6c3":"code","2a8612ed":"code","e9cfe7f3":"code","0c91c6bb":"code","fe4cce50":"code","64c4c695":"code","22c71b5f":"code","9b46c0d9":"code","e3cc1634":"code","e7241b65":"code","c0685ccd":"code","2c026b34":"code","54bc1dbc":"code","d095c954":"code","7c665629":"code","e169ae9d":"code","3fa11544":"code","1403bb76":"code","ffa1fafa":"code","2c9cdf0a":"code","7191abbd":"code","2d0799b1":"code","7429bb38":"code","ea9ec202":"code","2689ab07":"code","3e73bf13":"code","c25b3ae9":"code","f2813282":"code","be0fd4e2":"code","286a03ba":"code","a2f217f1":"code","b5869e5c":"code","be594ef1":"code","f612bb0c":"code","bcb4521e":"code","49d85e72":"code","ea2006af":"code","3e3bff0e":"code","a486d6a4":"code","5026ab4f":"code","4d68ac6d":"code","b5ee2f54":"code","e80dc4b1":"code","e5d3a32b":"code","1dc6f423":"code","e562f878":"code","bf27747c":"code","49611ac8":"code","e12e2db3":"code","a7d6c8b7":"code","596304ab":"code","f8cfa388":"code","a968a395":"code","9788fc87":"code","79904a3b":"markdown","a06389c0":"markdown","68843580":"markdown","a020753e":"markdown","1f2e6968":"markdown","0c5f1928":"markdown","53ad5ed8":"markdown","e833b274":"markdown","a651cc72":"markdown","0d76b48b":"markdown","d9890524":"markdown","d4b48f0b":"markdown","167ebfa1":"markdown","2b015b4b":"markdown","f0d81a05":"markdown"},"source":{"4556ed85":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ee1ac9b9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","4cb182c1":"df_heart=pd.read_csv('..\/input\/heart.csv')","dbf2fd2b":"df_heart.dtypes","133bdb58":"df_heart.head()","d6934e36":"df_heart.describe()","27bdcc47":"df_heart.isnull().sum()","6347dc3e":"sns.countplot(x='age',data=df_heart)","6b1526d9":"df_heart['sex'].value_counts().plot.pie()","1c4e693b":"df_heart['cp'].value_counts().plot.pie()","b3299034":"df_heart['fbs'].value_counts().plot.bar()","0087ca95":"df_heart['exang'].value_counts().plot.bar()","1a3b4dca":"df_heart['restecg'].value_counts().plot.bar()","1273bdac":"df_heart['ca'].value_counts().plot.bar()","a7915b97":"df_heart['thal'].value_counts().plot.bar()","0de458b2":"df_heart['slope'].value_counts().plot.bar()","a55a399d":"df_heart['target'].value_counts().plot.bar()","e8823724":"sns.distplot(df_heart['chol'])","0d32aec3":"sns.distplot(df_heart['thalach'])","a150625c":"sns.distplot(df_heart['oldpeak'])","025506eb":"sns.distplot(df_heart['trestbps'])","fcdb6e72":"sns.countplot(x=\"age\", hue=\"target\", data=df_heart)","f5013d1d":"sns.countplot(x=\"sex\", hue=\"target\", data=df_heart)","2532a66c":"sns.countplot(x=\"cp\", hue=\"target\", data=df_heart)","3235abca":"sns.countplot(x=\"fbs\", hue=\"target\", data=df_heart)","2c552983":"sns.countplot(x=\"restecg\", hue=\"target\", data=df_heart)","72b44a62":"sns.countplot(x=\"exang\", hue=\"target\", data=df_heart)","f88801f1":"sns.countplot(x=\"ca\", hue=\"target\", data=df_heart)","ff469fb9":"sns.countplot(x=\"thal\", hue=\"target\", data=df_heart)","7f3ffbd4":"a = pd.get_dummies(df_heart['cp'], prefix = \"cp\")\nb = pd.get_dummies(df_heart['thal'], prefix = \"thal\")\nc = pd.get_dummies(df_heart['slope'], prefix = \"slope\")\nd = pd.get_dummies(df_heart['ca'], prefix = \"ca\") \ne = pd.get_dummies(df_heart['restecg'], prefix = \"restecg\") ","a66fbd9f":"frames = [df_heart, a, b, c,d,e]\ndf_heart = pd.concat(frames, axis = 1)\ndf_heart.head()","82be5fe2":"df_heart = df_heart.drop(columns = ['cp', 'thal', 'slope','ca','restecg'])\ndf_heart.head()","877a3c88":"from sklearn.model_selection import train_test_split","33a0080c":"# Putting feature variable to X\nX = df_heart.drop(['target'], axis=1)\n\nX.head()","bf996ade":"# Putting response variable to y\ny = df_heart['target']\n\ny.head()","cb77e6f3":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","af767079":"from sklearn.preprocessing import StandardScaler","d66231ac":"scaler = StandardScaler()\n\nX_train[['age','trestbps','chol','thalach','oldpeak']] = scaler.fit_transform(X_train[['age','trestbps','chol','thalach','oldpeak']])\n\nX_train.head()","1ab664c1":"### Checking the disease Rate\ntarget = (sum(df_heart['target'])\/len(df_heart['target'].index))*100\ntarget","cb7cb5a2":"# Let's see the correlation matrix \nimport matplotlib.pyplot as plt\nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(df_heart.corr(),annot = True)\nplt.show()","5a00f714":"import statsmodels.api as sm","06abf387":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","3fea11ab":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","d8062eb7":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)","049cf557":"rfe.support_","34ac4c1b":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","3dcbaee0":"col = X_train.columns[rfe.support_]","f827a73d":"X_train.columns[~rfe.support_]","e8b2c9e5":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","52ef48f2":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","4b3bf04b":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","6caa658c":"y_train_pred_final = pd.DataFrame({'target':y_train.values, 'target_Prob':y_train_pred})\ny_train_pred_final.head()","cec1c4f1":"y_train_pred_final['predicted'] = y_train_pred_final.target_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","d4075de1":"from sklearn import metrics","4c927847":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.target, y_train_pred_final.predicted )\nprint(confusion)","c2a1d51b":"# Predicted     not_have    have_disease\n# Actual\n# no_disease        78      14\n# have_disease     10     110 ","91fe9a9a":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.target, y_train_pred_final.predicted))","c3a2cb64":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","231c1a46":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","ee625d4d":"col = col.drop('ca_0', 1)\ncol","c07dd0e0":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","fcde2791":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","179da68d":"y_train_pred[:10]","a876c0ad":"y_train_pred_final['target_Prob'] = y_train_pred","a4b5ccc6":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.target_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","54c5d8b6":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.target, y_train_pred_final.predicted))","75f08de6":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a92c82eb":"# Let's take a look at the confusion matrix again \nconfusion = metrics.confusion_matrix(y_train_pred_final.target, y_train_pred_final.predicted )\nconfusion","d994194e":"# Predicted     not_have    have\n# Actual\n# not_have        78      14\n# have            10     110 ","c5376ee4":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","f987a8a8":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","13b2e013":"# Let us calculate specificity\nTN \/ float(TN+FP)","3876992a":"# Calculate false postive rate \nprint(FP\/ float(TN+FP))","0c65364e":"# positive predictive value \nprint (TP \/ float(TP+FP))","a2ec9155":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","7b5b6280":"col = col.drop('slope_2', 1)\ncol","737e7688":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","cd641fcb":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","f22b876a":"y_train_pred[:10]","4476338f":"y_train_pred_final['target_Prob'] = y_train_pred","eff4c51b":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.target_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","d5e59114":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.target, y_train_pred_final.predicted))","b191a7ab":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","5a50b82c":"col = col.drop('ca_4', 1)\ncol","c54d0bb2":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","06606113":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","0f01248c":"y_train_pred[:10]","485c8a12":"y_train_pred_final['target_Prob'] = y_train_pred","c569f85a":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.target_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","cc7f071e":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d0997ca5":"col = col.drop('thal_2', 1)\ncol","93922430":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","319ab3d1":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","863fb104":"y_train_pred[:10]","190106dc":"y_train_pred_final['target_Prob'] = y_train_pred","994a4fb1":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.target_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","145f3c20":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.target, y_train_pred_final.predicted))","ac5c14a9":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","143887b0":"col = col.drop('restecg_1', 1)\ncol","e4a5d806":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm7.fit()\nres.summary()","480a3a30":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","3b70bde2":"y_train_pred[:10]","f697cb7f":"y_train_pred_final['target_Prob'] = y_train_pred","e8a0cd08":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.target_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","2a8ad30f":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.target, y_train_pred_final.predicted))","7ccc262e":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b2611ed2":"col = col.drop('cp_3', 1)\ncol","902aaa48":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm8 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm8.fit()\nres.summary()","0b65cd06":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","fa67a6c3":"y_train_pred[:10]","2a8612ed":"y_train_pred_final['target_Prob'] = y_train_pred","e9cfe7f3":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.target_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","0c91c6bb":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.target, y_train_pred_final.predicted))","fe4cce50":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","64c4c695":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","22c71b5f":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.target, y_train_pred_final.target_Prob, drop_intermediate = False )","9b46c0d9":"draw_roc(y_train_pred_final.target, y_train_pred_final.target_Prob)","e3cc1634":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.target_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","e7241b65":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n#TP = confusion[1,1] # true positive \n#TN = confusion[0,0] # true negatives\n#FP = confusion[0,1] # false positives\n#FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.target, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","c0685ccd":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","2c026b34":"y_train_pred_final['final_predicted'] = y_train_pred_final.target_Prob.map( lambda x: 1 if x > 0.57 else 0)\n\ny_train_pred_final.head()","54bc1dbc":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.target, y_train_pred_final.final_predicted)","d095c954":"confusion2 = metrics.confusion_matrix(y_train_pred_final.target, y_train_pred_final.final_predicted )\nconfusion2","7c665629":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","e169ae9d":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","3fa11544":"# Let us calculate specificity\nTN \/ float(TN+FP)","1403bb76":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","ffa1fafa":"# Positive predictive value \nprint (TP \/ float(TP+FP))","2c9cdf0a":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","7191abbd":"confusion = metrics.confusion_matrix(y_train_pred_final.target, y_train_pred_final.predicted )\nconfusion","2d0799b1":"confusion[1,1]\/(confusion[0,1]+confusion[1,1])","7429bb38":"confusion[1,1]\/(confusion[1,0]+confusion[1,1])","ea9ec202":"from sklearn.metrics import precision_score, recall_score","2689ab07":"precision_score(y_train_pred_final.target, y_train_pred_final.predicted)","3e73bf13":"recall_score(y_train_pred_final.target, y_train_pred_final.predicted)","c25b3ae9":"from sklearn.metrics import precision_recall_curve","f2813282":"y_train_pred_final.target, y_train_pred_final.predicted","be0fd4e2":"p, r, thresholds = precision_recall_curve(y_train_pred_final.target, y_train_pred_final.target_Prob)","286a03ba":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","a2f217f1":"X_test[['age','trestbps','chol','thalach','oldpeak']] = scaler.transform(X_test[['age','trestbps','chol','thalach','oldpeak']])","b5869e5c":"X_test = X_test[col]\nX_test.head()","be594ef1":"X_test_sm = sm.add_constant(X_test)","f612bb0c":"y_test_pred = res.predict(X_test_sm)","bcb4521e":"y_test_pred[:10]","49d85e72":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","ea2006af":"# Let's see the head\ny_pred_1.head()","3e3bff0e":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","a486d6a4":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","5026ab4f":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","4d68ac6d":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'target_Prob'})","b5ee2f54":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex_axis(['target','target_Prob'], axis=1)","e80dc4b1":"# Let's see the head of y_pred_final\ny_pred_final.head()","e5d3a32b":"y_pred_final['final_predicted'] = y_pred_final.target_Prob.map(lambda x: 1 if x > 0.60 else 0)","1dc6f423":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.target, y_pred_final.final_predicted)","e562f878":"confusion2 = metrics.confusion_matrix(y_pred_final.target, y_pred_final.final_predicted )\nconfusion2","bf27747c":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","49611ac8":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","e12e2db3":"# Let us calculate specificity\nTN \/ float(TN+FP)","a7d6c8b7":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","596304ab":"# Positive predictive value \nprint (TP \/ float(TP+FP))","f8cfa388":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","a968a395":"fpr, tpr, thresholds = metrics.roc_curve( y_pred_final.target, y_pred_final.target_Prob, drop_intermediate = False )","9788fc87":"draw_roc(y_pred_final.target, y_pred_final.target_Prob)","79904a3b":"## Bi-Variate Analysis","a06389c0":"##### Recall\nTP \/ TP + FN","68843580":"### 1. Train Test Split","a020753e":"**My new to data science. Please my notebook solution and let me know the inputs to improve the solution**","1f2e6968":"### 5. Feature Selection Using RFE","0c5f1928":"### 2. Feature Scaling","53ad5ed8":"### 3. Looking at cormelations","e833b274":"### 4. Model Building","a651cc72":"## Univariate Analysis:","0d76b48b":"### Modeling : Classification","d9890524":"### Precision and recall tradeoff","d4b48f0b":"##### Precision\nTP \/ TP + FP","167ebfa1":"###  Categorical Features","2b015b4b":"## Precision and Recall","f0d81a05":"### Numerical Features:"}}