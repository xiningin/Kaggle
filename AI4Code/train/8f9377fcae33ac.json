{"cell_type":{"3c557e91":"code","7509d073":"code","32760824":"code","14a98f01":"code","9fe296ea":"code","15feb095":"code","9d632273":"code","13420b9b":"code","d7969d6f":"code","0141580b":"code","145e65b8":"code","e17f0ea0":"code","69e0aeb0":"code","7204b827":"code","04751e50":"code","f10fd605":"markdown","82f72c60":"markdown","27bbddbf":"markdown","b7dfa22a":"markdown","4cd081d2":"markdown","bcca8e5e":"markdown","363a9599":"markdown","7fff1305":"markdown","76d9cea1":"markdown","b48c1061":"markdown"},"source":{"3c557e91":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os; os.environ['OMP_NUM_THREADS'] = '4'\nimport sys\nimport tensorflow as tf\nimport numpy as np\nimport shutil\nimport pandas as pd\n\nprint(tf.__version__)\nassert tf.__version__ >= \"1.8\" or tf.__version__ >= \"1.10\"\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# List the CSV columns\nCSV_COLUMNS = ['fare_amount', 'pickup_datetime','pickup_longitude','pickup_latitude',\n               'dropoff_longitude','dropoff_latitude', 'passenger_count', 'key']\n\n#Choose which column is your label\nLABEL_COLUMN = 'fare_amount'\nTRAIN_LINES = 55423856\n#import os\n#print(os.listdir(\"..\/input\"))\nDEBUG=False\n","7509d073":"from contextlib import contextmanager\nimport time\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')","32760824":"#This is just to have a look at the data\nPATH = '..\/input'\ntrain_df = pd.read_csv(f'{PATH}\/train.csv', nrows=100000)\ntrain_df['distance'] = np.sqrt(np.abs(train_df['pickup_longitude']-train_df['dropoff_longitude'])**2 +\n                        np.abs(train_df['pickup_latitude']-train_df['dropoff_latitude'])**2)\ntrain_df.head()\ntrain_df.describe()","14a98f01":"BATCH_SIZE=1 #Filtering works only with size 1 batches!!\ndataset = tf.contrib.data.make_csv_dataset(\n    file_pattern=f'{PATH}\/train.csv',\n    batch_size=BATCH_SIZE,\n    column_names=None,\n    column_defaults=None,\n    label_name='fare_amount',\n    select_columns=[1, 2, 3, 4, 5, 6, 7],\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    header=True,\n    num_epochs=None,\n    shuffle=True,\n    shuffle_buffer_size=10000,\n    shuffle_seed=None,\n    prefetch_buffer_size=1,\n    num_parallel_reads=1,\n#    num_parallel_parser_calls=2,\n    sloppy=False,\n    num_rows_for_inference=100\n)\n\nif DEBUG:\n    next_element = dataset.make_one_shot_iterator().get_next()\n    with tf.Session() as sess:\n        features, label = sess.run(next_element)\n        print(\"Features:\\n\", features, \"\\n\\nLabel:\\n\", label)\n","9fe296ea":"def pd_weekDay(year, month, day):\n    df = pd.DataFrame({'year': year,\n                       'month': month,\n                       'day': day})\n    date_df = pd.to_datetime(df)\n    return date_df.dt.weekday.astype(np.int32)\n\ndef pd_dayofYear(year, month, day):\n    df = pd.DataFrame({'year': year,\n                       'month': month,\n                       'day': day})\n    date_df = pd.to_datetime(df)\n    return date_df.dt.dayofyear.astype(np.int32)\n\nif DEBUG:\n    years=np.array([2018, 2018, 2018])\n    months=np.array([8, 11, 1])\n    days=np.array([20, 6, 8])\n    print(pd_dayofYear(years, months, days))\n","15feb095":"nyccenter = tf.constant([-74.0063889, 40.7141667])\n\ndef outer_product(x, y, x1, y1, x2, y2):\n#    x = tf.Print(x, [x, y, x1, y1, x2, y2])\n    outer_product = (x-x1)*(y2-y1)-(y-y1)*(x2-x1)\n    return outer_product\n\ndef river_side(x, y, river_name='EAS'):\n    '''The function takes coordinates as input and calculates if the point is one side or another of the river.\n        I tried to use the information but I got no benefit.\n    '''\n    if river_name=='EAS': # East River\n        river_line = tf.constant([-74.07, 40.6, -73.84, 40.9])\n    elif river_name=='HUD': # East River\n        river_line = tf.constant([-74.0356, 40.6868, -73.9338, 40.8823])\n    else:\n        raise ValueError( f'Unknown NYC River {river_name}' )\n    tf.reshape(river_line, [1,4])\n    if DEBUG:\n        print(x,y, river_line)\n    side = tf.sign(outer_product(x, y, river_line[0], river_line[1], river_line[2], river_line[3]))\n    return side\n\ndef distance_from_loc(x, y, x1, y1):\n    '''Euclidean Distance between location (x,y) and Tensor of locations (x1,y1)'''\n    return ( tf.sqrt(tf.abs(x1-x)**2 + tf.abs(y1-y)**2) )\n\ndef angle_from_nyc(x, y):\n    x1 = x - nyccenter[0]\n    y1 = y - nyccenter[1]\n    angle = tf.atan2(y1, x1)\n    return angle\n    \nif DEBUG:\n    long=np.array([-73.94, -73.95, -73.96])\n    lat=np.array([40.7613, 40.7613, 40.7613])\n    side = river_side(long, lat, river_name='EAS')\n    with tf.Session() as sess:\n        result = sess.run(side)\n        print(\"Side:\\n\", result)","9d632273":"def tf_isAirport(latitude,longitude,airport_name='JFK'):\n    jfkcoord = tf.constant([-73.8352, -73.7401, 40.6195, 40.6659])\n    ewrcoord = tf.constant([-74.1925, -74.1531, 40.6700, 40.7081])\n    lgucoord = tf.constant([-73.8895, -73.8550, 40.7664, 40.7931])\n    if airport_name=='JFK':\n        coord = jfkcoord\n    elif airport_name=='EWR':\n        coord = ewrcoord\n    elif airport_name=='LGU':\n        coord = lgucoord\n    else:\n        raise ValueError( f'Unknown NYC Airport {airport_name}' )\n        \n    is_airport = \\\n    tf.logical_and(\n        tf.logical_and(\n            tf.greater(longitude, coord[0]), tf.less(longitude, coord[1])\n        ),\n        tf.logical_and(\n            tf.greater(latitude, coord[2]), tf.less(latitude, coord[3])\n        )\n    )\n    return is_airport\n","13420b9b":"def feat_eng_func(features, label=None):\n    print(\"Feature Engineered Label:\", label)\n    #New features based on pickup datetime\n    features['pickup_year'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 0, 4), tf.int32)\n    features['pickup_month'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 5, 2), tf.int32)\n    features['pickup_day'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 8, 2), tf.int32)\n    features['pickup_hour'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 11, 2), tf.int32)\n    #TODO is there an easy way to perform below functions using TF APIs?\n    features['pickup_weekday'] = tf.py_func(pd_weekDay,\n                                            [features['pickup_year'], features['pickup_month'], features['pickup_day']],\n                                            tf.int32,\n                                            stateful=False,\n                                            name='Weekday'\n                                           )\n    #no advantage features['pickup_dayofyear'] = tf.cast(features['pickup_month'] * 31 + features['pickup_day'], tf.int32 ) #not precise, but good enough\n    #Normalize year and add decimals for months. This is because fares increase with time\n#    features['pickup_dense_year'] = (\n#                tf.cast(features['pickup_year'], tf.float32) + \\\n#                tf.cast(features['pickup_month'], tf.float32) \/ tf.constant(12.0, tf.float32) -  \\\n#                 tf.constant(2009.0, tf.float32) ) \/  \\\n#                 tf.constant(6.0, tf.float32) \n    features['night'] = tf.cast(\n        tf.logical_or( tf.greater(features['pickup_hour'], 19),  tf.less(features['pickup_hour'], 7)),\n        tf.float32)\n    #Clip latitudes and longitudes\n    minlat = tf.constant(38.0)\n    maxlat = tf.constant(42.0)\n    minlon = tf.constant(-76.0)\n    maxlon = tf.constant(-72.0)\n    features['pickup_longitude'] = tf.clip_by_value(features['pickup_longitude'], minlon, maxlon)\n    features['pickup_latitude'] = tf.clip_by_value(features['pickup_latitude'], minlat, maxlat)\n    features['dropoff_longitude'] = tf.clip_by_value(features['dropoff_longitude'], minlon, maxlon)\n    features['dropoff_latitude'] = tf.clip_by_value(features['dropoff_latitude'], minlat, maxlat)\n    #Clip (normalize passengers didn't work?)\n    minpass = tf.constant(1.0)\n    maxpass = tf.constant(6.0)\n    features['passenger_count'] = tf.clip_by_value(tf.cast(features['passenger_count'], tf.float32), minpass, maxpass)\n    #Clip fare_amount\n    #TODO normalize or tf.log the fare_amount\n    if label != None:\n        minfare = tf.constant(3.0)\n        maxfare = tf.constant(300.0)\n        label = tf.clip_by_value(label,  minfare, maxfare) \n    #TODO feature for bridge passing\n    #TODO new feature for distance and angle from city center\n    #New features based on pickup and dropoff position\n    features['longitude_dist'] = tf.abs(features['pickup_longitude'] - features['dropoff_longitude'])\n    features['latitude_dist'] = tf.abs(features['pickup_latitude'] - features['dropoff_latitude'])\n    #compute euclidean distance of the trip (multiply by 10 to slightly normalize)\n    features['distance'] = tf.sqrt(features['longitude_dist']**2 + features['latitude_dist']**2)\n#    features['pick_dist_center'] = distance_from_loc(nyccenter[0], nyccenter[1], \n#                                                     features['pickup_longitude'], features['pickup_latitude'])\n#    features['drop_dist_center'] = distance_from_loc(nyccenter[0], nyccenter[1], \n#                                                     features['dropoff_longitude'], features['dropoff_latitude'])\n    features['pick_angle'] = angle_from_nyc(features['pickup_longitude'],features['pickup_latitude'])\n    features['drop_angle'] = angle_from_nyc(features['dropoff_longitude'],features['dropoff_latitude'])\n    features['angle'] = features['pick_angle'] - features['drop_angle']\n#    features['ortdistance'] = features['longitude_dist'] + features['latitude_dist']\n#    long_distance = tf.constant(0.7)\n#    features['is_long_distance'] = tf.less(long_distance, features['distance'])\n#    features['is_JFK_pickup'] = tf_isAirport(features['pickup_latitude'], \n#                                             features['pickup_longitude'],\n#                                             airport_name='JFK')\n#    features['is_JFK_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n#                                             features['dropoff_longitude'],\n#                                             airport_name='JFK')\n#    features['is_EWR_pickup'] = tf_isAirport(features['pickup_latitude'], \n#                                             features['pickup_longitude'],\n#                                             airport_name='EWR')\n#    features['is_EWR_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n#                                             features['dropoff_longitude'],\n#                                             airport_name='EWR')\n#    features['is_LGU_pickup'] = tf_isAirport(features['pickup_latitude'], \n#                                             features['pickup_longitude'],\n#                                             airport_name='LGU')\n#    features['is_LGU_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n#                                             features['dropoff_longitude'],\n#                                             airport_name='LGU')\n#    features['is_NYC_airport'] = tf.logical_or(\n#        tf.logical_or(\n#            tf.logical_or(features['is_JFK_pickup'], features['is_JFK_dropoff']),\n#            tf.logical_or(features['is_EWR_pickup'], features['is_EWR_dropoff'])),\n#        tf.logical_or(features['is_LGU_pickup'], features['is_LGU_dropoff'])\n#    )\n#    BOOL_COLUMNS = ['is_JFK_pickup', 'is_JFK_dropoff', 'is_EWR_pickup', 'is_EWR_dropoff',\n#                   'is_LGU_pickup', 'is_LGU_dropoff', 'is_NYC_airport' ]\n#    for key in BOOL_COLUMNS:\n#        features[key] = tf.cast(features[key], tf.int32)\n#\n#    features['same_side_EAS'] = tf.equal(river_side(features['pickup_longitude'], features['pickup_latitude'], river_name='EAS'),\n#                                         river_side(features['dropoff_longitude'], features['dropoff_latitude'], river_name='EAS'))\n#    features['same_side_HUD'] = tf.equal(river_side(features['pickup_longitude'], features['pickup_latitude'], river_name='HUD'),\n#                                         river_side(features['dropoff_longitude'], features['dropoff_latitude'], river_name='HUD'))\n#\n\n#    features['pickup_minute'] = tf.substr(features['pickup_datetime'], 14, 2)\n#TODO normalize long and lat\n#TODO remove outliers on passenger_count and fare_amount\n#    print(features)\n    if label == None:\n        return features\n    return (features, label)","d7969d6f":"# Create an input function that stores your data into a dataset\ndef read_dataset(filename, mode, batch_size = 512):\n    def _input_fn():    \n        if mode == tf.estimator.ModeKeys.TRAIN:\n            num_epochs = None # indefinitely\n            shuffle = False\n        else:\n            num_epochs = 1 # end-of-input after this\n            shuffle = False\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            label_name=None\n            select_columns=[1, 2, 3, 4, 5, 6]\n        else:\n            label_name ='fare_amount'\n            select_columns = [1, 2, 3, 4, 5, 6, 7]\n\n        # Create list of files that match pattern\n        file_list = tf.gfile.Glob(filename)\n        # Create Dataset from the CSV files\n        dataset = tf.contrib.data.make_csv_dataset(\n            file_pattern=file_list,\n            batch_size=batch_size, #for filtering\n            column_names=None,\n            column_defaults=None,\n            label_name=label_name,\n            select_columns=select_columns,\n            field_delim=',',\n            use_quote_delim=True,\n            na_value='',\n            header=True,\n            num_epochs=num_epochs,\n            shuffle=shuffle,\n            shuffle_buffer_size=128*batch_size,\n            shuffle_seed=None,\n            prefetch_buffer_size=1,\n            num_parallel_reads=1,\n#            num_parallel_parser_calls=3,\n            sloppy=False,\n            num_rows_for_inference=100\n        )\n#This is necessary to split train and eval\n        skip_train_lines = TRAIN_LINES \/\/ batch_size \/\/ 100 * 10 #skip first 10% lines of train data set\n        if mode == tf.estimator.ModeKeys.TRAIN:\n#        dataset = dataset.filter(filter_data)\n            dataset = dataset.skip(skip_train_lines)\n        elif mode == tf.estimator.ModeKeys.EVAL:\n            dataset = dataset.take(skip_train_lines) \n\n        dataset = dataset.map(feat_eng_func)\n#        dataset = dataset.repeat(3)\n#        dataset = dataset.batch(batch_size)\n        return dataset.make_one_shot_iterator().get_next()\n    return _input_fn\n","0141580b":"if DEBUG:\n    train_input_fn = read_dataset(f'{PATH}\/train.csv', tf.estimator.ModeKeys.EVAL, batch_size = 8)\n    with timer('Debugging'):\n        with tf.Session() as sess:\n            features, label = sess.run(train_input_fn())\n            print(\"Features:\\n\", features, \"\\n\\nLabel:\\n\", label)","145e65b8":"NUMERIC_COLUMNS = ['passenger_count', 'pickup_dense_year', \n                   'longitude_dist', 'latitude_dist',\n                   'distance', 'pick_dist_center', 'drop_dist_center', 'angle',\n                   'pick_angle','drop_angle'\n#                   , 'night'\n                  ]\nBOOL_COLUMNS = ['is_JFK_pickup', 'is_JFK_dropoff', 'is_EWR_pickup', 'is_EWR_dropoff',\n                   'is_LGU_pickup', 'is_LGU_dropoff', 'is_NYC_airport'\n                  ]\n# Define your feature columns\ndef create_bucket_feature_cols():\n    hour_buckets = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('pickup_hour'), list(np.linspace(0, 23, 24)))\n    weekday_buckets = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('pickup_weekday'), list(np.linspace(0, 7, 8)))\n    month_buckets = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('pickup_month'), list(np.linspace(0, 11, 12)))\n    year_buckets = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('pickup_year'), list(np.linspace(2009, 2017, 9)))\n    passenger_buckets = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('passenger_count'), list(np.linspace(0, 8, 9)))\n    distance_buckets = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('distance'), list(np.linspace(0, 0.05, 100)))\n    night_buckets = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('night'), list(np.linspace(0, 1, 2)))\n    angle_buckets = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('angle'), list(np.linspace(-3.14, 3.14, 90)))\n    #hour_X_weekday = tf.feature_column.crossed_column([hour_cat, weekday_cat], 500)\n    #days_list = range(367)\n    #yearday = tf.feature_column.categorical_column_with_vocabulary_list('pickup_dayofyear', days_list)\n    NUM_BUCKETS = 27\n    long_list = list(np.linspace(-74.2, -73.7, NUM_BUCKETS))\n    lat_list = list(np.linspace(40.55, 41.0, NUM_BUCKETS))\n    p_lon = tf.feature_column.numeric_column('pickup_longitude')\n    p_lat = tf.feature_column.numeric_column('pickup_latitude')\n    d_lon = tf.feature_column.numeric_column('dropoff_longitude')\n    d_lat = tf.feature_column.numeric_column('dropoff_latitude')\n    buck_p_lon = tf.feature_column.bucketized_column(p_lon, long_list)\n    buck_p_lat = tf.feature_column.bucketized_column(p_lat, lat_list)\n    buck_d_lon = tf.feature_column.bucketized_column(d_lon, long_list)\n    buck_d_lat = tf.feature_column.bucketized_column(d_lat, lat_list)\n########################################################################    \n    return [\n        hour_buckets,\n#        weekday_buckets,\n        month_buckets,\n        year_buckets,\n        passenger_buckets,\n        distance_buckets,\n        angle_buckets,\n        night_buckets,\n        buck_p_lon,\n        buck_p_lat,\n        buck_d_lon,\n        buck_d_lat\n           ]\n\n","e17f0ea0":"if DEBUG:\n    feat = create_bucket_feature_cols()\n    print(\"Number of features=\", len(feat))\n    print(feat)","69e0aeb0":"BATCH_SIZE = 1024\ntrain_input_fn = read_dataset(f'{PATH}\/train.csv', tf.estimator.ModeKeys.TRAIN, batch_size = BATCH_SIZE)\neval_input_fn = read_dataset(f'{PATH}\/train.csv', tf.estimator.ModeKeys.EVAL, batch_size = BATCH_SIZE)\n# Create estimator train and evaluate function\ndef train_and_evaluate(output_dir, num_train_steps):\n#    estimator = tf.estimator.LinearRegressor(model_dir = output_dir, feature_columns = create_feature_cols())\n    runconfig = tf.estimator.RunConfig(model_dir = OUTDIR, keep_checkpoint_max=1, \n                                   save_summary_steps=5000, log_step_count_steps=5000,\n                                   save_checkpoints_steps=10000,\n                                   tf_random_seed = 42\n                                  )\n    estimator = tf.estimator.BoostedTreesRegressor(model_dir = output_dir, \n                                        feature_columns = create_bucket_feature_cols(),\n                                        n_batches_per_layer=128,\n                                        n_trees=400,\n                                        max_depth=6,\n                                        learning_rate=0.05,\n                                        l1_regularization=0.01,\n                                        l2_regularization=0.0,\n                                        tree_complexity=0.0,\n                                        min_node_weight=0.0,\n                                        center_bias = True,\n                                        config=runconfig)\n    train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn, \n                                      max_steps = num_train_steps)\n    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn, \n                                    steps = None, \n                                    start_delay_secs = 0, # start evaluating after N seconds, \n                                    throttle_secs = 60)  # evaluate every N seconds\n    evaluation, result = tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n    return estimator, evaluation, result\n    \n\nOUTDIR = '.\/trained_model'\nshutil.rmtree(OUTDIR, ignore_errors = True)\nwith timer('Train and Evaluate...'):\n    estimator, evaluation, result = train_and_evaluate(OUTDIR, 240000)\nprint(evaluation)","7204b827":"print(evaluation)\nprint(evaluation, file = sys.stderr)\navg_loss = evaluation['average_loss']\npredict_input_fn = read_dataset(f'{PATH}\/test.csv', tf.estimator.ModeKeys.PREDICT, batch_size=1)\npredictions = estimator.predict(predict_input_fn)\n\ntest_df = pd.read_csv(f'{PATH}\/test.csv', nrows=10000)\n#test_df.head()\n\ns = pd.Series()\nfor i, p in enumerate(predictions):\n    if i < 9915:\n        s.at[i] = p['predictions'][0]\n    else:\n        break\ntest_df['fare_amount'] = s\nsub = test_df[['key', 'fare_amount']]\nsub.to_csv(f'DNNregr-{avg_loss:4.4}.csv', index=False, float_format='%.1f')\n#    print(\"Prediction %s: %s\" % (i + 1, p))","04751e50":"if DEBUG:\n    s.describe()\n\n","f10fd605":"# Read the CSV file into a Dataset\nCreate an input function that reads a csv file into a dataset.\nIt is used to create the input functions for training, evaluating and predicting.\nIt also splits training data and evaluation data, even though the method is quite trivial. I tried to split using the filter() function of Dataset class, but it worked only with size=1 batches... and that makes everything very slow.","82f72c60":"Again, it is difficult to debug with Tensorflow so I sometimes run this cell just as a safety check to be sure \nI am transforming the data correctly.","27bbddbf":"# Read the dataset from a csv file\nLet's start using the new API (introduced in core Tensorflow in version 1.8) to read a dataset directly from a CSV file.\nAs it is difficult to debug with Tensorflow  I use below cell just as a safety check to be sure I am reading the data correctly.","b7dfa22a":"Python function to calculate the day of the week. It will be used when mapping the dataset. \nIt'd be more efficient to use tensorflow operators, but it would require much more coding and I am lazy :-) ","4cd081d2":"# Feature Engineering\nHere is the function called by dataset.map(). It does all the feature engineering work.\nA lot of features are commented out because I had no benefit using them. I keep them in the code, may be someone will find them useful.\n\nThe function works also for prediction, when there is no label. \n","bcca8e5e":"# Predicting and creating the submission file","363a9599":"# NYC Taxi Fare with TensorFlow BoostedTrees #\n\nThis is an exercise using TF.data API, that let us process huge datasets without memory costraints. Infact this Kernel uses less than 8Gbytes, but can process the full 55 million records of the *train.csv* file.\nI have created another Kernel that uses a DNNRegressor tensorflow estimator and I wanted to try also a BoostedTree regressor and here it is.","7fff1305":"# Feature Columns\nHere is the functios that returns the feature columns. The BoostedTrees regressor accepts only Indicator and Bucketized feature columns, so you will see a lot of bucketizations.","76d9cea1":"The function is used when mapping the dataset. It creates new boolean features \nstating when coordinates are at one of the NYC airports","b48c1061":"# Training and Evaluating\nThe evaluation is done on the *train.csv* file, but the read_dataset function has logic to differentiate the records that will be read according to the TRAIN or EVAL mode. "}}