{"cell_type":{"5e17ac5b":"code","36fba64a":"code","d9e3625d":"code","67d9aa13":"code","47cc34da":"code","e4bde2ec":"code","03e15204":"code","aa5f52cc":"code","ca78bb0c":"code","7180597b":"code","ca249b25":"code","5cc67ba0":"code","f21f359f":"code","c6c6ba8c":"code","7e7ee677":"code","091cdcbe":"code","55539553":"code","d0c91843":"code","9c7112fc":"code","3f9fd5ee":"code","ff21100c":"code","ecf2bf49":"code","a04b366d":"code","4998da0a":"code","bab1d2d9":"code","42d7da07":"code","a8ff82c1":"code","2ce312d4":"code","e120bba1":"code","760d3e48":"code","e88c7dd3":"code","a1cb863d":"code","bf737025":"code","aea6a3db":"code","76dd5552":"code","a5cf3db2":"code","9605233a":"code","7a48aa34":"code","7659b9ae":"code","f649dce3":"code","f26adc89":"markdown","9176c378":"markdown","8e503fa7":"markdown","908589ed":"markdown","0178d333":"markdown","24d869ac":"markdown","02809fc0":"markdown","ed667138":"markdown","3a557fec":"markdown","fde14f41":"markdown","55cd6473":"markdown","cd945ed2":"markdown","1cb1368b":"markdown","52d8ca65":"markdown","16dbde1e":"markdown","ff7b97fa":"markdown","9ca5c470":"markdown","f991f9ab":"markdown","462406d6":"markdown","ee4d4682":"markdown","64ef6425":"markdown","c9135bd5":"markdown","36844e84":"markdown","6a66d152":"markdown","93224ff0":"markdown","43591de3":"markdown","eae68998":"markdown","b2b6aea7":"markdown","eb2b1b15":"markdown","25bf49ce":"markdown"},"source":{"5e17ac5b":"import numpy as np\nimport pandas as pd\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, precision_score, f1_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\nimport warnings\nwarnings.simplefilter(\"ignore\")","36fba64a":"df = pd.read_csv('..\/input\/toy-dataset\/toy_dataset.csv')\ndf.head()\n","d9e3625d":"df[\"Illness\"].value_counts()","67d9aa13":"df[df[\"Illness\"] == 'No'].count()","47cc34da":"df[df[\"Illness\"] == 'No'].count()","e4bde2ec":"df.info()","03e15204":"df.loc[df['Gender']=='Male','Gender'] = 1\ndf.loc[df['Gender']=='Female','Gender'] = 0\nfrom sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\nlb_results = lb.fit_transform(df['City'])\nnew_df = pd.DataFrame(lb_results, columns=lb.classes_)\ndf = pd.concat([df, new_df], axis = 1)","aa5f52cc":"df.head()","ca78bb0c":"X = df.drop(['Illness','Number', 'City'], axis=1)\ny = df.Illness\n","7180597b":"encoder = LabelEncoder()\ny = pd.Series(encoder.fit_transform(y), name='Illness')","ca249b25":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)","5cc67ba0":"X = pd.concat([X_train, y_train], axis=1)","f21f359f":"not_ill = X[X.Illness== 0]\nill = X[X.Illness== 1]","c6c6ba8c":"ill_upsampled = resample(ill,\n                         replace=True, # sample with replacement\n                         n_samples=len(not_ill), # match number in majority class\n                         random_state=100) # reproducible results","7e7ee677":"upsampled = pd.concat([not_ill, ill_upsampled])\nupsampled.Illness.value_counts()","091cdcbe":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    '''\n    print the accuracy score, classification report and confusion matrix of classifier\n    '''\n    if train:\n        '''\n        training performance\n        '''\n        print(\"Train Result:\\n\")\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n    elif train==False:\n        '''\n        test performance\n        '''\n        print(\"Test Result:\\n\")        \n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test)))) ","55539553":"y_train = upsampled.Illness\nX_train = upsampled.drop('Illness', axis=1)\nclf = LogisticRegression(solver='liblinear').fit(X_train, y_train)","d0c91843":" print_score(clf, X_train, y_train, X_test, y_test, train=True)","9c7112fc":" print_score(clf, X_train, y_train, X_test, y_test, train=False)","3f9fd5ee":"# setting up testing and training sets\nX = df.drop(['Illness','Number', 'City'], axis=1)\ny = df.Illness\nencoder = LabelEncoder()\ny = pd.Series(encoder.fit_transform(y), name='Illness')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)","ff21100c":"sm = SMOTE(random_state=42)\nX_train, y_train = sm.fit_sample(X_train, y_train)","ecf2bf49":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(y_train)\nplt.title('Balanced training data')\nplt.show()","a04b366d":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train, y_train)","4998da0a":"print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)","bab1d2d9":"print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","42d7da07":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n","a8ff82c1":"rf_clf = RandomForestClassifier(random_state=42)\nparams_grid = {\"max_depth\": [3, None],\n               \"min_samples_split\": [2, 3, 5],\n               \"min_samples_leaf\": [1, 3, 5],\n               \"bootstrap\": [True, False],\n               \"criterion\": ['entropy']}","2ce312d4":"grid_search = GridSearchCV(rf_clf, params_grid,\n                           n_jobs=-1, cv=3,\n                           verbose=1, scoring='accuracy')","e120bba1":"grid_search.fit(X_train, y_train)","760d3e48":"grid_search.best_score_","e88c7dd3":"grid_search.best_estimator_.get_params()","a1cb863d":"print_score(grid_search, X_train, y_train, X_test, y_test, train=True)","bf737025":"print_score(grid_search, X_train, y_train, X_test, y_test, train=False)","aea6a3db":"from sklearn.ensemble import ExtraTreesClassifier","76dd5552":"xt_clf = ExtraTreesClassifier(random_state=42, min_samples_leaf=3, min_samples_split=2)","a5cf3db2":"xt_clf.fit(X_train, y_train)","9605233a":"print_score(xt_clf, X_train, y_train, X_test, y_test, train=True)","7a48aa34":"print_score(xt_clf, X_train, y_train, X_test, y_test, train=False)","7659b9ae":"# setting up testing and training sets\nX = df.drop(['Illness','Number', 'City'], axis=1)\ny = df.Illness\nencoder = LabelEncoder()\ny = pd.Series(encoder.fit_transform(y), name='Illness')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\nkf = StratifiedKFold(n_splits=3, random_state=100)\ncross_val_f1_score_lst = []\ncross_val_accuracy_lst = []\ncross_val_recall_lst = []\ncross_val_precision_lst = []\n\nfor train_index_ls, validation_index_ls in kf.split(X_train, y_train):\n    # keeping validation set apart and oversampling in each iteration using smote \n    train, validation = X_train.iloc[train_index_ls], X_train.iloc[validation_index_ls]\n    target_train, target_val = y_train.iloc[train_index_ls], y_train.iloc[validation_index_ls]\n    sm = SMOTE(random_state=100)\n    X_train_res, y_train_res = sm.fit_sample(train, target_train)\n    print (X_train_res.shape, y_train_res.shape)\n    \n    # training the model on oversampled 4 folds of training set\n    rf = RandomForestClassifier(n_estimators=100, random_state=100)\n    rf.fit(X_train_res, y_train_res)\n    # testing on 1 fold of validation set\n    validation_preds = rf.predict(validation)\n    cross_val_recall_lst.append(recall_score(target_val, validation_preds))\n    cross_val_accuracy_lst.append(accuracy_score(target_val, validation_preds))\n    cross_val_precision_lst.append(precision_score(target_val, validation_preds))\n    cross_val_f1_score_lst.append(f1_score(target_val, validation_preds))\nprint ('Cross validated accuracy: {}'.format(np.mean(cross_val_accuracy_lst)))\nprint ('Cross validated recall score: {}'.format(np.mean(cross_val_recall_lst)))\nprint ('Cross validated precision score: {}'.format(np.mean(cross_val_precision_lst)))\nprint ('Cross validated f1_score: {}'.format(np.mean(cross_val_f1_score_lst)))","f649dce3":"from numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\nxgbmodel = XGBClassifier()\nxgbmodel.fit(X_train, y_train)\nfeat_importances = pd.Series(xgbmodel.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(10).plot(kind='barh')","f26adc89":"### Encoding Target Variable using LabelEncoder","9176c378":"## There are other ways to operationalize the model using Google cloud ML Engine and other clouds have the options.","8e503fa7":"# Visualization Variable Importance","908589ed":"### Now we are seperating the data on the basis of majority and minority classes to apply oversampling","0178d333":"### We have tried to build the best model to predict Illness based on Income, Age, Gender and City but  as we have shown earlier in notebook that the data is skewed so we have tried random upsampling and Smote oversampling and we have trained our model on the sampled data but our model is overfit as we have tried different techniques but the result or evaluation metrics not upto the mark we will try doing undersampling the majority class because of the availability issue uploading it till this. Because training the models are taking too much time so it was not possible to use more_splits. ","24d869ac":"### Seperating Feature matrix and target variable","02809fc0":"### DataFrame information","ed667138":"#  To operationalize the model we have to take the below steps:\n## 1. First of all we have to save the best model in pickle format and put it in a database.\n## 2. We have to create a GUI using python Django or Flask framework and there it will ask a person about the details like city, Gender, Income and Age after that when they hit enter button there will be a script which will load the model from pickle format and do the prediction , that predicted output will be shown to user.","3a557fec":"### SMOTE oversampling","fde14f41":"### Encoding Categorical Variable ","55cd6473":"### Count on basis of \"Yes\" and \"No\" Class in Target Variable ","cd945ed2":"### Still we are not getting desired as for class 1 there is only 8% precision and recall is too less,so we are using GridSearch so that we can get best parameter","1cb1368b":"### Now we will combining the majority class and the upsampled minority class ","52d8ca65":"### Below Function will check the training performance and test performance.","16dbde1e":"#### In Below code block output we can check all categorical variable are encoded as numerical value","ff7b97fa":"### Training performance","9ca5c470":"### StratifiedKfold with Random Forest classifier","f991f9ab":"### As resampling doesn't made any big effect we are trying SMOTE oversampling which actually loops through the existing, real minority instance. At each loop iteration, one of the K closest minority class neighbours is chosen and a new minority instance is synthesised somewhere between the minority instance and that neighbour.","462406d6":"### Our Model is overfit as we can see it performing well on training set but on test set it is having issues with the class 1.To Overcome this we will try it with Extra-Trees Ensemble","ee4d4682":"## Importing Libraries","64ef6425":"## Training and Testing Model","c9135bd5":"#### Visualizing Balanced Training data after Smote Sampling ","36844e84":"## Data Preprocessing ","6a66d152":"### concatenate training data together so that we can seperate the data on basis of classes in target column","93224ff0":"### Test Performance","43591de3":"### We are trying to upsample minority class using scikit learn's resample class","eae68998":"### Splitting data into train_set and test_set","b2b6aea7":"### Training Random Forest over Smote Oversampling data set","eb2b1b15":"### Training Logisitic Regression on Oversampled Dataset","25bf49ce":"### Importing Dataset"}}