{"cell_type":{"6b9e4ed1":"code","1e20e1e2":"code","76117f4a":"code","09fae4b4":"code","77726d15":"code","ced6bc63":"code","7d8b0bab":"code","d8fb0c02":"code","03b377cc":"code","7bad0740":"code","7416e897":"code","3ca2022b":"code","0640544c":"code","f45cac68":"code","13504b67":"code","758911a3":"code","5bec74fc":"code","156fa6ed":"code","ee52a136":"code","365a902b":"code","9300c7cc":"code","b7d36581":"code","fb4d972b":"code","6829f14c":"code","e50520dd":"code","255e5cc8":"code","3bbaccff":"code","56bc139f":"code","af44aec3":"code","8fb9bb65":"code","84e08898":"code","c091c761":"code","6d82ba9d":"code","ba954a3d":"code","563cb5da":"code","c6540955":"code","58ae1d0f":"code","46ea541d":"code","0f6c377b":"code","751d4dc7":"code","591419d6":"code","5039614c":"code","133ebcea":"code","b76d24d9":"code","308ee6d7":"code","57380f72":"code","8f57a65e":"code","11de3d6a":"code","8da53f84":"code","865f0448":"code","91377ba5":"code","7b2bae54":"code","864430d2":"code","e440bca1":"code","692ac8cd":"code","a6b7d697":"markdown","5918ae62":"markdown","5a08a103":"markdown","cdff2726":"markdown","b8a04e7e":"markdown","aff3fecc":"markdown","f5a71640":"markdown","f6f94d2c":"markdown","6a4eb669":"markdown","38fee351":"markdown","e5e55e59":"markdown","500a2113":"markdown","30231be4":"markdown","3707ca5b":"markdown","0ab949e9":"markdown","d7f0efda":"markdown","bf5918af":"markdown","08b19879":"markdown","ad495fa2":"markdown","7d5d74a5":"markdown","79db002e":"markdown"},"source":{"6b9e4ed1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e20e1e2":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as stats\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","76117f4a":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")","09fae4b4":"display(train.head())\ndisplay(test.head())\ndisplay(sub.head())","77726d15":"print('size of train: ', train.shape)\nprint('size of test: ', test.shape)\nprint('size of submission: ', sub.shape)","ced6bc63":"print(train.info())\nprint(test.info())","7d8b0bab":"train_original = train.copy()\ntest_original = test.copy()","d8fb0c02":"train.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","03b377cc":"train.columns.values","7bad0740":"features = list(train.columns)\nlist(enumerate(features))","7416e897":"train.describe().style.background_gradient(cmap='bone_r')","3ca2022b":"train['claim'].value_counts()","0640544c":"fig, ax = plt.subplots(figsize=(6, 6))\n\nbars = ax.bar(train[\"claim\"].value_counts().index,\n              train[\"claim\"].value_counts().values,\n              color='darkorange',\n              edgecolor=\"black\",\n              width=0.4)\nax.set_title(\"Claim (target) values distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Claim (target) value\", fontsize=14, labelpad=10)\nax.set_xticks(train[\"claim\"].value_counts().index)\nax.tick_params(axis=\"both\", labelsize=12)","f45cac68":"plt.figure(figsize=(7,6))\nsns.distplot(train['claim']);","13504b67":"fig, ax = plt.subplots(nrows = 2, ncols = 1, figsize=(9,8), sharex = True)\n\nax[0].scatter(train['f10'], train['f15'], ec = 'k', color = 'skyblue')\nax[0].set_ylabel('f15')\nax[0].set_title('Relation b\/w f10 & f15')\n\n\nax[1].scatter(train['f3'], train['f4'], ec = 'k', color = 'skyblue')\nax[1].set_xlabel('f3')\nax[1].set_ylabel('f4')\nax[1].set_title('Relation b\/w f3 & f4')\n\nplt.tight_layout()\nplt.show()","758911a3":"df = pd.concat([train.drop([\"claim\"], axis=1), test], axis=0)\ncolumns = df.columns.values\n\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,130), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i\/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\nplt.show();","5bec74fc":"# print columns with highest number of Null values\ntrain.isnull().sum().sort_values(ascending = False).to_frame().head(100).rename({0:'Counts'}, axis = 1).T.style.background_gradient('crest')","156fa6ed":"fig, ax = plt.subplots(figsize=(28, 15))\n\nbars = ax.bar(train.isnull().sum().index,\n              train.isnull().sum().values,\n              color=\"lightskyblue\",\n              edgecolor=\"black\",\n              width=0.7)\nax.set_title(\"Missing feature values distribution in the train dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=10)\nplt.show()","ee52a136":"# print columns with highest number of Null values\ntest.isnull().sum().sort_values(ascending = False).to_frame().head(100).rename({0:'Counts'}, axis = 1).T.style.background_gradient('crest')","365a902b":"fig, ax = plt.subplots(figsize=(25, 10))\n\nbars = ax.bar(train.isna().sum().index,\n              train.isna().sum().values,\n              color=\"lightskyblue\",\n              edgecolor=\"black\",\n              width=0.7)\nax.set_title(\"Missing feature values distribution in the train dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=10)\nplt.show()","9300c7cc":"imputer = SimpleImputer()\n\ntrain_imputed = pd.DataFrame(imputer.fit_transform(train), columns=train.columns)\ntest_imputed = pd.DataFrame(imputer.fit_transform(test), columns=test.columns)","b7d36581":"# Checking the final dataset for missing values again.\npd.DataFrame(train, columns=train_original.columns).isnull().sum().sort_values(ascending = False).to_frame().reset_index().rename({0:'Count'}, axis =1).head(70).T.style.background_gradient('magma_r')","fb4d972b":"# Checking the final dataset for missing values again.\ntest_imputed.isnull().sum().sort_values(ascending = False).to_frame().reset_index().rename({0:'Count'}, axis =1).head(70).T.style.background_gradient('magma_r')","6829f14c":"train.corr()[['claim']].T.style.background_gradient('copper_r')","e50520dd":"corr_feat = train.corr()\nplt.figure(figsize=(24,8))\ncorr_feat[\"claim\"][:-1].plot(kind=\"bar\", grid=True, color='#FF9505')\nplt.title(\"Features correlation\")","255e5cc8":"# correlation matrix Using pandas\ncorr = train.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(1)","3bbaccff":"plt.figure(figsize=(25,20))\nax = sns.heatmap(corr, vmin=-1, vmax=1, center=0,cmap=sns.diverging_palette(20, 200, n=200),square=True)\n\nax.set_xticklabels(ax.get_xticklabels(), horizontalalignment='right');","56bc139f":"df_plot = ((train_imputed - train_imputed.min())\/(train_imputed.max() - train_imputed.min()))\n\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\nsns.boxplot(data = df_plot.iloc[:, 1:30], ax = ax[0])\nsns.boxplot(data = df_plot.iloc[:, 30:60], ax = ax[1])\nsns.boxplot(data = df_plot.iloc[:, 60:90], ax = ax[2])\nsns.boxplot(data = df_plot.iloc[:, 90:120], ax = ax[3])","af44aec3":"def get_dist(df, col):\n  plt.figure(figsize=(18,6))\n\n  plt.subplot(1,3,1)\n  plt.hist(df[col], bins=30)\n  plt.title('Hist')\n\n  plt.subplot(1,3,2)\n  stats.probplot(df[col], dist='norm', plot=plt)\n  plt.ylabel('quantiles')\n\n  plt.subplot(1,3,3)\n  sns.boxplot(y=df[col])\n  plt.title('Boxplot')\n\n  plt.show()","8fb9bb65":"get_dist(train, 'f9')","84e08898":"get_dist(train, 'f45')","c091c761":"def find_normal_outliers(df, col):\n  upper_limit = train[col].mean() + 3*train[col].std()\n  lower_limit = train[col].mean() - 3*train[col].std()\n\n  return lower_limit, upper_limit","6d82ba9d":"find_normal_outliers(train, 'f1')","ba954a3d":"def find_skewed_outliers(df, col, distance):\n\n  IQR = train[col].quantile(0.75) - train[col].quantile(0.25)\n\n  upper_limit = train[col].quantile(0.75) + (IQR*distance)\n  lower_limit = train[col].quantile(0.25) - (IQR*distance)\n\n\n  return lower_limit, upper_limit","563cb5da":"find_skewed_outliers(train, 'f9', 1.5)","c6540955":"find_skewed_outliers(train, 'f110', 3)","58ae1d0f":"X = train_imputed.drop('claim', axis=1)\ny = train_imputed['claim']","46ea541d":"#### creating dataset split for prediction\nX_train, X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42) # 80-20 split\n\n#### Checking split \nprint('X_train:', X_train.shape)\nprint('y_train:', y_train.shape)\nprint('X_test:', X_test.shape)\nprint('y_test:', y_test.shape)","0f6c377b":"sc = StandardScaler()\nsc.fit(X)\nX_train_sc = pd.DataFrame(sc.transform(X), columns=X.columns)\ntest_sc = pd.DataFrame(sc.transform(test_imputed), columns=test.columns)","751d4dc7":"y_train_lr = train_original['claim']\nX_train_lr = train_original.drop('claim', axis=1)\n\nX_test_lr = test_original","591419d6":"X_train_lr['n_missing'] = X_train_lr.isnull().sum(axis=1)\nX_test_lr['n_missing'] = X_test_lr.isnull().sum(axis=1)","5039614c":"columns_for_pred  = X_train_lr.columns\ncolumns_for_pred","133ebcea":"IMPUTER = SimpleImputer()\n\nX_train_lr =  IMPUTER.fit_transform(X_train_lr)\nX_test_lr =  IMPUTER.fit_transform(X_test_lr)","b76d24d9":"scaler = StandardScaler()\nscaler.fit(X_train_lr)\nX_train_lr = pd.DataFrame(scaler.transform(X_train_lr), columns=columns_for_pred)\nX_test_lr = pd.DataFrame(scaler.transform(X_test_lr), columns=columns_for_pred)","308ee6d7":"lr = LogisticRegressionCV(random_state=0)\nlr.fit(X = X_train_lr, y = y_train_lr)","57380f72":"lr.get_params()","8f57a65e":"predict_lr = lr.predict(X_train_lr)\npredict_proba = lr.predict_proba(X_train_lr)[:,1]","11de3d6a":"m2_cm = confusion_matrix(y, predict_lr)\nm2_acc_score = accuracy_score(y_train_lr, predict_lr)\nprint(\"Confusion Matrix\")\nprint(m2_cm)\nsns.heatmap(m2_cm, annot=True, cmap='CMRmap_r')\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\", round(m2_acc_score*100,2),'\\n')\nprint(classification_report(y_train_lr, predict_lr))","8da53f84":"predict_lr_test = lr.predict_proba(X_test_lr)","865f0448":"LGBM =  LGBMClassifier(objective= 'binary', learning_rate = 0.05, max_depth = 3, n_estimators=500)\nLGBM.fit(X_train, y_train)","91377ba5":"predict_LGB = LGBM.predict(X_test)","7b2bae54":"m2_cm = confusion_matrix(y_test, predict_LGB)\nm2_acc_score = accuracy_score(y_test, predict_LGB)\nprint(\"Confusion Matrix\")\nprint(m2_cm)\nsns.heatmap(m2_cm, annot=True,cmap='CMRmap_r')\nprint(\"\\n\")\nprint(\"Accuracy of LGBM:\",round(m2_acc_score*100,2),'\\n')\nprint(classification_report(y_test, predict_LGB))","864430d2":"preds = LGBM.predict(test_imputed)","e440bca1":"submission = pd.DataFrame({'id': pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv', usecols=['id'])['id'], 'claim': predict_lr_test[:,1]})\nsubmission.to_csv('submission-lr.csv', index = 0)","692ac8cd":"submission.head()","a6b7d697":"- for more information refer: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html","5918ae62":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 3.2) Data Visualization <\/h1>","5a08a103":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3) EDA(Exploratory Data Analysis) <\/h1>","cdff2726":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 4.2) Correlation b\/n features <\/h1>","b8a04e7e":"#### **Normal Distribution (Outliers)**","aff3fecc":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:30px;color: #c1531f  \">Describe Data<\/li>","f5a71640":"#### **Right Skewed (Outliers)**","f6f94d2c":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:30px;color: #c1531f  \">Impute Missing Values(SimpleImputer) with median as the strategy<\/li>","6a4eb669":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2) Read Data <\/h1>","38fee351":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 4.3) Outliers <\/h1>","e5e55e59":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:30px;color: #c1531f  \">Outlier Detection<\/li>","500a2113":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:30px;color: #c1531f  \">Submission<\/li>","30231be4":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 5) Model Building and Evaluation <\/h1>","3707ca5b":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 1) Load Required Libraries <\/h1>","0ab949e9":"predictions = pd.DataFrame()\npredictions[\"id\"] = test_original[\"id\"]\npredictions[\"claim\"] = predict_lr_test\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","d7f0efda":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4) Feature Engineering <\/h1>","bf5918af":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:30px;color: #c1531f  \">LGBM<\/li>","08b19879":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 3.1) Target Variable <\/h1>","ad495fa2":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:30px;color: #c1531f  \">Logistic Regressioncv<\/li>","7d5d74a5":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 4.1) Missing Values <\/h1>","79db002e":"#### **Heavily Right Skewed (Outliers)**"}}