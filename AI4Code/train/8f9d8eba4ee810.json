{"cell_type":{"0f676073":"code","088f630f":"code","4616bcfe":"code","2a6cc6e2":"code","df83a55d":"code","ee5db86d":"code","6bcc3bae":"code","dbde0f5b":"code","50063be0":"code","553d7c81":"code","527c9004":"code","051c66e1":"code","d1e1eae9":"code","dd449e70":"code","aa3dc9c2":"code","f74414f1":"code","d5575dde":"markdown","5eb5ed64":"markdown","5272a656":"markdown","1d0aab86":"markdown","e8773505":"markdown","469cc9c1":"markdown","e7db140d":"markdown","ce16b4b7":"markdown","5856dcbf":"markdown","e076401b":"markdown","9b9f1f5e":"markdown","f9a3c40e":"markdown"},"source":{"0f676073":"import numpy as np \nimport pandas as pd \nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\nimport torch\nfrom datasets import Dataset\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","088f630f":"list_dic = []\nwith open('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json') as json_file:\n    for the_file in json_file:\n        list_dic.append(json.loads(the_file))","4616bcfe":"len(list_dic)","2a6cc6e2":"data = pd.DataFrame(list_dic)","df83a55d":"data.head(3)","ee5db86d":"plt.figure(figsize=(13, 7))\nplt.title('Amount of News Based On Category')\nsns.countplot(data=data, x='category')\nplt.xticks(rotation=90)\nplt.tight_layout()","6bcc3bae":"date = data.date.value_counts().index.sort_values()\nnews_on_category = {}\ncategory = data.category.value_counts().index[:3]\n\nprogress_bar = tqdm(range(len(date) * len(category)))\n\nfor c in category:\n    the_list = []\n    for d in date:\n        val = data[(data['category'] == c) & (data['date'] == d)]\n        the_list.append(len(val))\n        progress_bar.update(1)\n    news_on_category[c] = the_list\n    \ndf_the_list = pd.DataFrame(news_on_category, index=date)","dbde0f5b":"plt.figure(figsize=(15, 7))\nsns.lineplot(data=df_the_list)","50063be0":"labels = data['category']\ntext = [data.headline[i] + '. ' + data.short_description[i] for i in range(len(data))]\n\ndataset = pd.DataFrame({'text':text, 'labels':labels})","553d7c81":"tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\ndef tokenize_function(inp):\n    return tokenizer(inp['text'], padding='max_length', truncation=True)\n\ndef make_dataloader(labs, size_train, size_test, batch_size):\n    train_per_label = int(size_train \/ len(labs))\n    test_per_label = int(size_test \/ len(labs))\n    size = train_per_label + test_per_label\n    \n    train_dataset = pd.DataFrame()\n    test_dataset = pd.DataFrame()\n    \n    for l in labs:\n        add = dataset[dataset.labels == l].sample(size)\n        train_add = add.head(train_per_label)\n        test_add = add.tail(test_per_label)\n        train_dataset = pd.concat([train_dataset, train_add])\n        test_dataset = pd.concat([test_dataset, test_add])\n        \n    train_dataset = train_dataset.sample(frac=1)\n    test_dataset = test_dataset.sample(frac=1)\n    \n    label_encoder = LabelEncoder().fit(train_dataset['labels'])\n    train_dataset['labels'] = label_encoder.transform(train_dataset['labels'])\n    test_dataset['labels'] = label_encoder.transform(test_dataset['labels'])\n    \n    train_dataset = Dataset.from_pandas(train_dataset)\n    test_dataset = Dataset.from_pandas(test_dataset)\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n\n    tokenized_train = tokenized_train.remove_columns(['__index_level_0__', 'text'])\n    tokenized_test = tokenized_test.remove_columns(['__index_level_0__', 'text'])\n\n    tokenized_train.set_format('torch')\n    tokenized_test.set_format('torch')\n\n    train_dataloader = DataLoader(tokenized_train, batch_size=batch_size)\n    test_dataloader = DataLoader(tokenized_test, batch_size=batch_size)\n    \n    return train_dataloader, test_dataloader, label_encoder","527c9004":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\ndef build_a_model(model, optimizer, num_epochs, the_dataloader, the_scheduler, num_training_steps):\n    num_training_steps = len(the_dataloader) * num_epochs\n    progress_bar = tqdm(range(num_training_steps))\n    model = model.to(device)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch in the_dataloader:\n            batch = {k:v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            the_scheduler.step()\n            optimizer.zero_grad()\n            progress_bar.update(1)\n    return model","051c66e1":"labels_used = ['POLITICS', 'ENTERTAINMENT', 'WELLNESS', 'TRAVEL', 'SPORTS', 'STYLE & BEAUTY']\ntrain_dataloader, test_dataloader, encoder = make_dataloader(labels_used, 6000, 600, 10)","d1e1eae9":"category = list(encoder.classes_)\nclasses = [i for i in range(len(category))]\nencoding = pd.DataFrame({'category':category, 'classes':classes})\nencoding.to_pickle('.\/encoder.pkl')","dd449e70":"model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=len(labels_used))\noptimizer = AdamW(model.parameters(), lr=5e-5)\nnum_epochs = 5\nnum_training_steps = len(train_dataloader) * num_epochs\nlr_scheduler = get_scheduler('linear', optimizer=optimizer, num_warmup_steps=0, \n                             num_training_steps=num_training_steps)\n\nmodel = build_a_model(model, optimizer, num_epochs, train_dataloader, lr_scheduler, num_training_steps)","aa3dc9c2":"progress_bar = tqdm(range(len(test_dataloader)))\ntargets = []\npredictions = []\n\nmodel.eval()\nfor batch in test_dataloader:\n    labels = batch['labels'].cpu().numpy()\n    batch.pop('labels', None)\n    batch = {k:v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n    for t, p in zip(labels, preds):\n        targets.append(t)\n        predictions.append(p)\n    progress_bar.update(1)\n    \nvalid_score = accuracy_score(targets, predictions)\nprint('accuracy of valid is {:.3f}'.format(valid_score))","f74414f1":"torch.save(model.state_dict(), '.\/model.pt')","d5575dde":"# Save the Model","5eb5ed64":"# Importing Libraries\n\nImport all the necessary libraries. In this project we used a pretrained bert model and train them using pyTorch. So we also imported transformers and torch libraries in order to do that. ","5272a656":"Then what if we looked at the trend of these top 3 news category.","1d0aab86":"# Importing Data","e8773505":"# Building A Model","469cc9c1":"We can see from above, that news about politics, entertainment and wellness are dominating. To be honest, if politics and entertainement are two genres of news that are the most dominant, it doesn't surprise me at all but wellnes? Come on... Anyway it just proved that people are struggling these days huh. ","e7db140d":"From the above, we can see that the the data that we have isn't complete. It can't be a good representation of news that circling around us everyday. The news about wellness are more than news about politics and enterteinment before one point of time then suddenly it became zero until the end of the trend. Something peculiar that can be a proof that this data is not a good representation. \n\nAnyway, we are here not to analyze the trend but to analyze the content to later predict what the category the news would go into. ","ce16b4b7":"# Evaluate","5856dcbf":"# Exploratory Data Analysis","e076401b":"The accuracy of the model is 91.7% which is good. This can only be achieved after we minimized the number of categories that we use.","9b9f1f5e":"Now, we will see how many data from each categories that we have.","f9a3c40e":"We use **Bert-Base-Cased** as our pre-trained model here. It is reliable and quite light to be trained too.\n\nThe steps that we will go through to train the model is :\n\n**1**. Preparing to dataframe with only text and labels. Text here is headline added with short description.\n\n**2**. Selecting labels.\n\n**3**. Selecting training data and test data from the dataframe based on the labels.\n\n**4**. Tokenize the training data and test data\n\n**5**. Make train dataloader and test dataloader\n\n**6**. Load pre-trained model, optimizer and learning rate scheduler if you need.\n\n**7**. Train the model.\n\n**8**. Evaluate the model.\n\n**9**. Save the model."}}