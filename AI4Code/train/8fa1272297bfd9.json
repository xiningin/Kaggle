{"cell_type":{"3a5a28bf":"code","379f71fc":"code","91db61a7":"code","7798d833":"code","4e92e812":"code","7162ccdf":"code","87a36da9":"code","bad0635d":"code","424190bd":"code","e638bdaf":"markdown","9b979712":"markdown","38dec304":"markdown","fdbeb8c6":"markdown","8350a331":"markdown","1884018a":"markdown","66e296d1":"markdown"},"source":{"3a5a28bf":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nimport tokenizers\nfrom IPython.core.display import display, HTML\nprint('TensorFlow',tf.__version__)","379f71fc":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\ntrain.head()","91db61a7":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\nsentiment_tar = {'positive': 1, 'negative': 2, 'neutral': 0}\nDO_QUES_ANS = False","7798d833":"ct = train.shape[0]\n\n# INPUTS TO ROBERTA\nids = np.ones((ct,MAX_LEN),dtype='int32')\natt = np.zeros((ct,MAX_LEN),dtype='int32')\ntok = np.zeros((ct,MAX_LEN),dtype='int32')\n# QUESTION ANSWER TARGETS\ntar1 = np.zeros((ct,MAX_LEN),dtype='int32')\ntar2 = np.zeros((ct,MAX_LEN),dtype='int32')\n# SEGMENTATION TARGETS\ntar3 = np.zeros((ct,MAX_LEN),dtype='int32')\n# SENTIMENT TARGETS\ntar4 = np.zeros((ct),dtype='int32')\n# CHAR CENTERS\ncha = np.zeros((ct,MAX_LEN),dtype='float32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND TEXT \/ SELECTED_TEXT OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1)\n            \n    # FIND OFFSETS, CHAR CENTERS\n    off = []; ii=0; ct = 0\n    for i,t in enumerate(enc.ids):\n        w = tokenizer.decode([t])\n        off.append((ii,ii+len(w)))\n        ii += len(w)\n        cha[k,i] = ct + len(w)\/2.\n        ct += len(w)\n        \n    # FIND SELECTED TEXT TOKENS\n    tks = []\n    for i,(a,b) in enumerate(off):\n        sm = np.sum(chars[a:b])\n        if sm>0: tks.append(i)\n        \n    # CREATE ROBERTA INPUTS\n    stok = sentiment_id[train.loc[k,'sentiment']]\n    ids[k,:len(enc.ids)+2] = [0] + enc.ids + [2]\n    att[k,:len(enc.ids)+2] = 1\n    if DO_QUES_ANS: # USE THIS FOR QUESTION ANSWER \n        ids[k,len(enc.ids)+2:len(enc.ids)+5] = [2] + [stok] + [2]\n        att[k,len(enc.ids)+2:len(enc.ids)+5] = 1\n        \n    # CREATE ROBERTA TARGETS\n    if len(tks)>0:\n        tar1[k,tks[0]+1] = 1\n        tar2[k,tks[-1]+1] = 1\n    for j in range(len(tks)):\n        tar3[k,tks[j]+1] = 1\n    tar4[k] = sentiment_tar[train.loc[k,'sentiment']]","4e92e812":"def build_model():\n    q_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    q_type = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    q_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(q_id,attention_mask=q_mask,token_type_ids=q_type)\n    \n    x1 = tf.keras.layers.Dropout(0.2)(x[0])\n    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n    x1 = tf.keras.layers.Dense(3,'softmax')(x1)\n    \n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_type], outputs=x1)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, \n        optimizer=optimizer, metrics=['accuracy'])\n    \n    return model","7162ccdf":"# SIMPLE VALIDATION SET\nidxT = np.arange(0,4*train.shape[0]\/\/5)\nidxV = np.arange(4*train.shape[0]\/\/5,train.shape[0])","87a36da9":"model = build_model()\n    \nsv = tf.keras.callbacks.ModelCheckpoint(\n        'cam.h5', monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n    \nmodel.fit([ids[idxT,],att[idxT,],tok[idxT,]], tar4[idxT], \n          validation_data = ([ids[idxV,],att[idxV,],tok[idxV,]], tar4[idxV]),\n          epochs=2, batch_size=32, verbose=1, callbacks=[sv])\n\nmodel.load_weights('cam.h5')","bad0635d":"# NEW MODEL FROM OLD TO EXTRACT ACTIVATION MAPS\nall_layer_weights = model.layers[-1].get_weights()[0]\ncam_model = tf.keras.Model(inputs=model.input, \n        outputs=(model.layers[-4].output, model.layers[-1].output)) \npr = {0:'NEUTRAL',1:'POSITIVE',2:'NEGATIVE'}","424190bd":"for kk in range(100):\n    k = np.random.choice( np.arange(4*train.shape[0]\/\/5,train.shape[0]))\n    if train.loc[k,'sentiment']=='neutral': continue #boring\n    if len(train.loc[k,'text'])>95: continue #too wide for display\n    \n    # EXTRACT INFLUENCIAL TEXT\n    last_conv_output,_, pred_vec = cam_model.predict([ids[k:k+1,:],att[k:k+1,:],tok[k:k+1,:]])\n    last_conv_output = np.squeeze(last_conv_output)\n    pred = np.argmax(pred_vec)\n    layer_weights = all_layer_weights[:, pred]\n    final_output = np.dot(last_conv_output, layer_weights)\n    if pr[pred]!=train.loc[k,'sentiment'].upper(): continue #skip misclassified\n    \n    # PLOT INFLUENCE VALUE\n    print()\n    plt.figure(figsize=(20,3))\n    idx = np.sum(att[k,])\n    v = np.argsort(final_output[:idx-1])\n    mx = final_output[v[-1]]; x = max(-10,-len(v))\n    mn = final_output[v[x]]\n    plt.plot(cha[k,:idx-2],final_output[1:idx-1],'o-')\n    plt.plot([1,95],[mn,mn],':')\n    plt.xlim((0,95))\n    plt.yticks([]); plt.xticks([])\n    plt.title('Train row %i. Predict %s.   True label is %s'%(k,pr[pred],train.loc[k,'sentiment']),size=16)\n    plt.show()\n    \n    # DISPLAY ACTIVATION TEXT\n    html = ''\n    for j in range(1,idx):\n        x = (final_output[j]-mn)\/(mx-mn)\n        html += \"<span style='background:{};font-family:monospace'>\".format('rgba(255,255,0,%f)'%x)\n        html += tokenizer.decode( [ids[k,j]] )\n        html += \"<\/span>\"\n    html += \" (predict)\"\n    display(HTML(html))\n\n    # DISPLAY TRUE SELECTED TEXT\n    text1 = \" \".join(train.loc[k,'text'].lower().split()) \n    text2 = \" \".join(train.loc[k,'selected_text'].lower().split())\n    sp = text1.split(text2)\n    html = \"<span style='font-family:monospace'>\"+sp[0]+\"<\/span>\"\n    for j in range(1,len(sp)):\n        html += \"<span style='background:yellow;font-family:monospace'>\"+text2+'<\/span>'\n        html += \"<span style='font-family:monospace'>\"+sp[j]+\"<\/span>\"\n    html += \" (true)\"\n    display(HTML(html))\n    print()","e638bdaf":"# Load Data","9b979712":"# Train Model\nWe train on 80% train.csv and validate on 20% train.csv. Our model achieves 80% validation accuracy predicting `sentiment` from Tweet `text`.","38dec304":"# TensorFlow roBERTa - Unsupervised Text Selection\nWow, this notebook does not use `selected_text`. This notebook only uses the columns `text` and `sentiment`. We train a roBERTa model to predict sentiment (pos, neg, neu) from Tweet `text` achieving 80% accuracy. We then display what part of the text was influencial in deciding whether the text was postive, negative, or neutral. This is **unsupervised** learning because we are learning the `selected_text` without using the `selected_text`.\n\nThis unsupervised DL notebook is inspired by Nick's awesome unsupervised ML notebook [here][1] and it's similar to my previous unsupervised DL notebook for image recognition [here][2]\n\n[1]: https:\/\/www.kaggle.com\/nkoprowicz\/a-simple-solution-using-only-word-counts\n[2]: https:\/\/www.kaggle.com\/cdeotte\/unsupervised-masks-cv-0-60","fdbeb8c6":"# roBERTa Model\nWe use pretrained roBERTa base model and add a classification head.","8350a331":"# roBERTa Tokenization\nIn this section, we create a variety of roBERTa inputs and targets. In this notebook, we only need tokenized `text` and target `sentiment` (`tar4`). However, for your assistance with future models, we also make roBERTa inputs for a question and answer supervised model (use `DO_QUES_ANS = True`) with accompanying start token (`tar1`) and end token (`tar2`) targets. Feel free to use this for a TensorFlow QA solution for this competition.\n\nBelow tokenization code logic is inspired by Abhishek's PyTorch notebook [here][1]\n\n[1]: https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds","1884018a":"# CAM Extraction Model\nAn example of CAM extraction for image recognition is [here][1]. The basic idea is as follows. When our model classifies a text, it will activate one of three classification outputs (pos, neg, neu). We trace that output backwards to see which words contributed the most to the decision.\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/unsupervised-masks-cv-0-60","66e296d1":"# Display Influential Subtext\nThe code to highlight the text with different colors is from notebook [here][1]. The plot above each sentence indicates the strength of influence of the words below. The x axis is the sentence and the y axis is the strenth of influence in determining the sentiment prediction.\n\n[1]: https:\/\/www.kaggle.com\/jeinsong\/html-text-segment-visualization"}}