{"cell_type":{"b3234696":"code","a5845aaf":"code","4229c5fd":"code","31909bd2":"code","860fc45f":"code","cc89b2a1":"code","bcfb45d7":"code","03e4bee1":"code","9b35f943":"code","ac6c9cde":"code","af1656f5":"code","f4416c6f":"code","83645f8e":"code","c4695ea6":"code","e26a90c9":"code","928e8017":"code","057c3c87":"code","39aa213a":"code","6830d1dd":"code","11ae206d":"code","de8d1bf9":"code","780a1497":"code","201edb70":"code","a13f09e7":"code","db338905":"code","79e6beb4":"code","07b2a705":"code","1ce69e5a":"code","1d271d83":"code","c5437a58":"code","4da01ff2":"code","cf7e2e19":"code","040081ec":"code","35582ecd":"code","28f512de":"code","7bc6ef2a":"code","d6a41dc1":"code","efe36097":"code","203fae8b":"code","16d193f1":"code","fcdab436":"code","6d1d592e":"code","cc99adc9":"code","0785bc3c":"code","8bbd382e":"markdown","78b3029b":"markdown","39e15f21":"markdown","720d41f7":"markdown","f5e7246d":"markdown","3deb4635":"markdown","d0b6b0e5":"markdown","e5894777":"markdown","b30b8802":"markdown","59235348":"markdown","b9b27133":"markdown","f418bdb2":"markdown","1e2b22d9":"markdown","b1ac0d2e":"markdown","eae0c206":"markdown","c297d437":"markdown","19f5f91a":"markdown","bce64234":"markdown","6db184fc":"markdown","08eb8348":"markdown","e779baa3":"markdown","74220ad6":"markdown","ceea7935":"markdown","e5b45d9f":"markdown","a1db60f1":"markdown","f4074b03":"markdown"},"source":{"b3234696":"import numpy as np\nimport pandas as pd\n\nimport urllib.request\nfrom PIL import Image\n\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n","a5845aaf":"### Para que funcione necesitas bajarte los archivos de datos de Kaggle \ndf = pd.read_csv(\"diamonds_train.csv\", index_col=0)","4229c5fd":"df.describe()","31909bd2":"df[\"x\/y\"] = df[\"x\"]\/df[\"y\"]\ndf.head()","860fc45f":"print(df.loc[(df['x'] == 0) | (df['y'] == 0) | (df['z'] == 0) | (df[\"x\/y\"] == 0)] )\nprint('len :',len(df.loc[(df['x'] == 0) | (df['y'] == 0) | (df['z'] == 0) | (df[\"x\/y\"] == 0)]))","cc89b2a1":"df = df[(df['x'] != 0)]\ndf = df[(df['z'] != 0)]  #| (df['y'] != 0) | (df['z'] != 0) | (df[\"x\/y\"] != 0)]\ndf.describe()","bcfb45d7":"#df['volume'] = df['x']*df['y']*df['z']\n#df.drop(['x','y','z'], axis=1, inplace= True)","03e4bee1":"df.head(20)","9b35f943":"print(df[\"cut\"].unique())\nprint(df[\"color\"].unique())\nprint(df[\"clarity\"].unique())","ac6c9cde":"df.columns","af1656f5":"cut_labels = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\ncolor_labels = ['J','I','H','G','F','E','D']\nclarity_labels =['I1','SI2','SI1','VS2','VS1','VVS2','VVS1', 'IF']\n\noe = preprocessing.OrdinalEncoder(categories=[cut_labels, color_labels, clarity_labels])","f4416c6f":"# \u00bfqu\u00e9 opin\u00e1is?\n#S\u00ed, habr\u00eda que hacerlo","83645f8e":"X_encoded = oe.fit_transform(df[[\"cut\", \"color\", \"clarity\"]])\ndf[[\"cut\", \"color\", \"clarity\"]] = X_encoded\ndf.head()","c4695ea6":"X = df.drop([\"price\"],1)\nX.shape","e26a90c9":"#PredictorScaler=MinMaxScaler()\n#PredictorScalerFit=PredictorScaler.fit(X)\n#X=PredictorScalerFit.transform(X)","928e8017":"#skY=df['price'].skew()\n#skY\ny = df[\"price\"]\ny.shape","057c3c87":"#data_score = pd.DataFrame({'price':np.log(df['price'])})","39aa213a":"#sk = data_score['price'].skew()\n#sk","6830d1dd":"#y = np.log(y)","11ae206d":"# \u00bfqu\u00e9 opin\u00e1is?\n# s\u00ed, el conjunto para predecir `diamonds_test.csv` tambi\u00e9n tendr\u00e1 que tener 3 columnas","de8d1bf9":"# el test_size 0.20 y el random_state puede cambiar\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)","780a1497":"X_train","201edb70":"pipe = Pipeline(steps=[\n    ('Reg', CatBoostRegressor())\n])\n\nparams = {\n    'Reg': [CatBoostRegressor()],\n    'Reg__iterations': [1300],\n    'Reg__learning_rate':[0.08],\n    'Reg__depth':[8],\n    'Reg__l2_leaf_reg':[3]\n}\n\n\nmodel = GridSearchCV(estimator = pipe,\n                  param_grid = params,\n                  cv = 10,\n                  verbose=1,\n                  n_jobs=-1)","a13f09e7":"#model = CatBoostRegressor(n_estimators=100, random_state=42)","db338905":"model.fit(X_train, y_train)","79e6beb4":"print(model.best_estimator_)\nprint(model.best_score_)\nprint(model.best_params_)","07b2a705":"predictions = model.predict(X_test)\nprint(predictions)","1ce69e5a":"np.sqrt(mean_squared_error(y_test, predictions))","1d271d83":"X_pred = pd.read_csv(\"..\/diamonds_test.csv\", index_col=0)\nX_pred.head()","c5437a58":"X_pred[\"x\/y\"] = X_pred[\"x\"]\/X_pred[\"y\"]","4da01ff2":"cut_labels = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\ncolor_labels = ['J','I','H','G','F','E','D']\nclarity_labels =['I1','SI2','SI1','VS2','VS1','VVS2','VVS1', 'IF']\n\noe = preprocessing.OrdinalEncoder(categories=[cut_labels, color_labels, clarity_labels])","cf7e2e19":"X_encoded_pred = oe.fit_transform(X_pred[[\"cut\", \"color\", \"clarity\"]])\nX_pred[[\"cut\", \"color\", \"clarity\"]] = X_encoded_pred\nX_pred.head()","040081ec":"#X_pred['volume'] = X_pred['x']*X_pred['y']*X_pred['z']\n#X_pred.drop(['x','y','z'], axis=1, inplace= True)","35582ecd":"#PredictorScaler=MinMaxScaler()\n#PredictorScalerFit=PredictorScaler.fit(X_pred)\n#X_pred=PredictorScalerFit.transform(X_pred)","28f512de":"predictions_submit = model.predict(X_pred)\npredictions_submit","7bc6ef2a":"sample = pd.read_csv(\"..\/sample_submission.csv\")","d6a41dc1":"sample.head()","efe36097":"sample.shape","203fae8b":"submission = pd.DataFrame({\"id\": range(len(predictions_submit)), \"price\": predictions_submit})","16d193f1":"submission.head()","fcdab436":"submission.shape","6d1d592e":"def chequeator(df_to_submit):\n    \"\"\"\n    Esta funci\u00f3n se asegura de que tu submission tenga la forma requerida por Kaggle.\n    \n    Si es as\u00ed, se guardar\u00e1 el dataframe en un `csv` y estar\u00e1 listo para subir a Kaggle.\n    \n    Si no, LEE EL MENSAJE Y HAZLE CASO.\n    \n    Si a\u00fan no:\n    - apaga tu ordenador, \n    - date una vuelta, \n    - enciendelo otra vez, \n    - abre este notebook y \n    - leelo todo de nuevo. \n    Todos nos merecemos una segunda oportunidad. Tambi\u00e9n t\u00fa.\n    \"\"\"\n    if df_to_submit.shape == sample.shape:\n        if df_to_submit.columns.all() == sample.columns.all():\n            if df_to_submit.id.all() == sample.id.all():\n                print(\"You're ready to submit!\")\n                submission.to_csv(\"submission3.csv\", index = False) #muy importante el index = False\n                urllib.request.urlretrieve(\"https:\/\/i.kym-cdn.com\/photos\/images\/facebook\/000\/747\/556\/27a.jpg\", \"gfg.png\")     \n                img = Image.open(\"gfg.png\")\n                img.show()   \n            else:\n                print(\"Check the ids and try again\")\n        else:\n            print(\"Check the names of the columns and try again\")\n    else:\n        print(\"Check the number of rows and\/or columns and try again\")\n        print(\"\\nMensaje secreto de Clara: No me puedo creer que despu\u00e9s de todo este notebook hayas hecho alg\u00fan cambio en las filas de `diamonds_test.csv`. Lloro.\")\n","cc99adc9":"help(chequeator)","0785bc3c":"chequeator(submission)","8bbd382e":"### 1. Definir X e y","78b3029b":"Definici\u00f3n de **modelo que est\u00e1 listo**. \n\n_Tras hacer suficientes pruebas, analizar los datos, hacer feature engineering, probar diferentes modelos con diferentes par\u00e1metros, es con este con el que observo mejores m\u00e9tricas y menos overfitting. \u00a1Cuidado con el overfitting aqu\u00ed! Si vuestro modelo aprende muy bien de estos datos pero hay overfitting cuando le pasemos los datos desconocidos de `diamonds_test.csv` nos arriesgamos a que digamos, no salga lo esperado._","39e15f21":"### 2. Dividir X_train, X_test, y_train, y_test","720d41f7":"![image](competi.png)","f5e7246d":"## Una vez listo el modelo, toca predecir con el dataset de predicci\u00f3n ","3deb4635":"**Importante:**\n\n   - Si quitas columnas o creas columnas nuevas a partir de otras, o cualquier modificaci\u00f3n column-wise tendr\u00e1s que aplicarlo al dataset de `diamonds_test.csv` de cara a hacer la predicci\u00f3n.\n   - Si por lo contrario, decides por ejemplo, quitar los outliers o hacer un `dropna()`, o cualquier modificaci\u00f3n row-wise eso NO PODR\u00c1S (ni debes) aplicarlo al dataset de `diamonds_test.csv` de cara a hacer la predicci\u00f3n. \u00bfPor qu\u00e9? Porque si el conjunto de test tiene 50 observaciones (filas) la predicci\u00f3n se espera que tenga 50 filas.","d0b6b0e5":"### 4. Entrenar el modelo con X_train, y_train","e5894777":"**\u00a1PERO! Para subir a Kaggle la predicci\u00f3n, \u00e9sta tendr\u00e1 que tener una forma espec\u00edfica y no valdr\u00e1 otra.**\n\nEn este caso, la **MISMA** forma que `sample_submission.csv`. ","b30b8802":"### 6. Sacar m\u00e9tricas, valorar el modelo \n\nRecuerdo que en la competici\u00f3n se va a evaluar con la m\u00e9trica de RMSE.","59235348":"Porque:\n    - SI EL ARRAY CON EL QUE HICISTEIS `.fit()` ERA DE 3 COLUMNAS, PARA `.predict()` DEBEN SER LAS MISMAS","b9b27133":"-----------------------------------------------------------------------------------------------------------------","f418bdb2":"-----------------------------------------------------------------","1e2b22d9":"### 3. AHORA puedo hacer la predicci\u00f3n que ser\u00e1 lo que subir\u00e1s a Kaggle. ","b1ac0d2e":"**\u00bfQu\u00e9 es lo que subir\u00e1s a Kaggle?**","eae0c206":"Siempre hay tiempo para una historia:\nhttps:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostregressor.html","c297d437":"**\u00bfDe d\u00f3nde saco `sample_submission.csv`?**","19f5f91a":"#### \u00bfY si despu\u00e9s del an\u00e1lisis exploratorio de mis datos llego a la conclusi\u00f3n de que puedo predecir el precio solo con las columnas `x`, `y` y `z`, tambi\u00e9n aplica al test?","bce64234":"#### \u00bfY si lo que hago es estandarizar los datos o hacer un encoding, tambi\u00e9n se lo tengo que hacer al test antes de predecir?","6db184fc":"### 5. P\u00e1sale el CHEQUEATOR para comprobar que efectivamente est\u00e1 listo para subir a Kaggle.","08eb8348":"### Pasos que si o si deb\u00e9is realizar para poder participar en la competici\u00f3n:","e779baa3":"### 4. Mete tus predicciones en un dataframe. \n\nEn este caso, la **MISMA** forma que `sample_submission.csv`. ","74220ad6":"### 2. Carga los datos de `diamonds_test.csv` para predecir.\n\n**\u00bfDe d\u00f3nde saco `diamonds_test.csv`?**","ceea7935":"### 5. Predecir con el modelo ya entrenado con X_test","e5b45d9f":"### 3. Asignar el modelo (vac\u00edo) a una variable\nAqu\u00ed meter\u00edais los par\u00e1metros. \n\n**Consejo**: Usa GridSearch y vu\u00e9lvete loca o loco probando modelos e hiperpar\u00e1metros.","a1db60f1":"### 1. Entrena dicho modelo con TODOS tus datos de train, esto es con `diamonds_train.csv` al completo.\n\n**CON LAS TRANSFORMACIONES QUE LE HAYAS REALIZADO A `X` INCLU\u00cdDAS.**\n\nV\u00e9ase:\n- Estandarizaci\u00f3n\/Normalizaci\u00f3n\n- Eliminaci\u00f3n de Outliers\n- Eliminaci\u00f3n de columnas\n- Creaci\u00f3n de columnas nuevas\n- Gesti\u00f3n de valores nulos\n- Y un largo etc\u00e9tera de t\u00e9cnicas que como Data Scientist hayas considerado las mejores para tu dataset.","f4074b03":"#### Aqu\u00ed encontrar\u00e1s todo lo que necesitas saber: https:\/\/www.kaggle.com\/t\/ab8726f0cfc84544abbae69a6be88071"}}