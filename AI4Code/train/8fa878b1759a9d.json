{"cell_type":{"813f0eb8":"code","c7e57aba":"code","4a6ea36b":"code","0da73729":"code","637a8e85":"code","0820fcf4":"code","93f1fb46":"code","11dfd332":"code","04c95bc6":"code","501b154c":"code","ee68c938":"code","4352439a":"code","1e0732db":"code","009773a4":"code","f4ad20d2":"code","583402da":"markdown","7cc1da94":"markdown","a4049885":"markdown","f7bca484":"markdown","2e78bc76":"markdown","3b51a65c":"markdown","a66d64a5":"markdown","1681fa7c":"markdown","105d20c7":"markdown"},"source":{"813f0eb8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7e57aba":"train_df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv',sep=',')\ntest_df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv', sep = ',')","4a6ea36b":"train_df.head()","0da73729":"test_df.head()","637a8e85":"img=train_df.iloc[0:1,1:]\nimg=np.array(img)\nimg=np.reshape(img,(28,28,1))\nplt.imshow(img,cmap=plt.cm.binary)\nplt.colorbar()\nplt.grid(False)\nplt.show()","0820fcf4":"rows = 10\ncolumns = 10\nfig,ax = plt.subplots(rows,columns,figsize = (16,16))\nax = ax.ravel()\nn_train = len(train_df)\nfor i in range(0,10*10):\n    index = np.random.randint(0, n_train)\n    img = train_df.iloc[index,1:]\n    img = np.array(img)\n    ax[i].imshow(img.reshape(28,28,1))\n    ","93f1fb46":"train_data = np.array(train_df, dtype = 'float32')\ntest_data = np.array(test_df, dtype='float32')","11dfd332":"x_train = train_data[:,1:]\/255\n\ny_train = train_data[:,0]\n\nx_test= test_data[:,1:]\/255\n\ny_test=test_data[:,0]","04c95bc6":"x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)\n","501b154c":"image_rows = 28\nimage_cols = 28\nbatch_size = 4096\nimage_shape = (image_rows,image_cols,1)","ee68c938":"x_train = x_train.reshape(x_train.shape[0],*image_shape)\nx_test = x_test.reshape(x_test.shape[0],*image_shape)\nx_validate = x_validate.reshape(x_validate.shape[0],*image_shape)","4352439a":"model = tf.keras.Sequential([\n    layers.Conv2D(32,(3,3),padding='same',activation='relu',input_shape=(28,28,1)),\n    layers.MaxPooling2D((2, 2), strides=2),\n     layers.Conv2D(64,(3,3),padding='same',activation='relu'),\n    layers.MaxPooling2D((2, 2), strides=2),\n    layers.Flatten(),\n    layers.Dense(128,activation = 'relu'),\n    layers.Dense(10, activation = 'softmax')\n])","1e0732db":"model.compile(loss ='sparse_categorical_crossentropy', optimizer='adam',metrics =['accuracy'])","009773a4":"history = model.fit(\n    x_train,\n    y_train,\n    batch_size=4096,\n    epochs=75,\n    verbose=1,\n    validation_data=(x_validate,y_validate),\n)","f4ad20d2":"score = model.evaluate(x_test, y_test)\n# checking the test loss and test accuracy\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","583402da":"# **Importing the Data**","7cc1da94":"we have created 2 arrays of train,test data","a4049885":"# **Exploring the data**","f7bca484":"# **Preprocessing the data**","2e78bc76":"first we took all our features into x_train and x_test in training and test data respectively and then we normalized the data by dividing with 255 so our model works better.and we have taken aou target coloumns in y_train,y_test of train and test data respectively","3b51a65c":"# **Visualizing The Data**","a66d64a5":"# **Model building**","1681fa7c":"now we are preparing the validation data from train data with validation size 20% and train size 80% of initial train size","105d20c7":"# **PROBLEM STATEMENT**\n This is a Classification problem where our deep learning model will try to predict in [T-shirt(Top),Trouser,Pullover,Dress,Coat,sandals,shirt,sneaker,bag,ankle boots]\n\nThis notebook is for beginners who are aiming to learn deep learning"}}