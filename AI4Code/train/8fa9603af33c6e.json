{"cell_type":{"c29f1887":"code","6a05a48d":"code","5435daec":"code","2f0c1e9f":"code","10f9cfb7":"code","86e21978":"code","b8954727":"code","14e2bb94":"code","f0b6cce3":"code","0186ad83":"code","efe95064":"code","c6f3b63d":"code","2aa01cea":"code","cea2e82b":"markdown","6423913c":"markdown","50efe5a6":"markdown","aa064853":"markdown","21e9b756":"markdown","cbcbe861":"markdown","054656c6":"markdown","82c83289":"markdown","8b06b5ac":"markdown","4d693239":"markdown"},"source":{"c29f1887":"# Import all the necessary libraries\n\nimport tensorflow as tf\n\nimport os\nimport math\nimport numpy as np\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import array_to_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\nfrom IPython.display import display","6a05a48d":"crop_size = 300\nupscale_factor = 3\ninput_size = crop_size \/\/ upscale_factor\nbatch_size = 8\nroot_dir = '..\/input\/chest-x-ray\/data'\n\ntrain_ds = image_dataset_from_directory(\n    root_dir,\n    batch_size=batch_size,\n    image_size=(crop_size, crop_size),\n    validation_split=0.2,\n    subset=\"training\",\n    seed=1337,\n    label_mode=None,\n)\n\nvalid_ds = image_dataset_from_directory(\n    root_dir,\n    batch_size=batch_size,\n    image_size=(crop_size, crop_size),\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=1337,\n    label_mode=None,\n)","5435daec":"def scaling(input_image):\n    input_image = input_image \/ 255.0\n    return input_image\n\n\n# Scale from (0, 255) to (0, 1)\ntrain_ds = train_ds.map(scaling)\nvalid_ds = valid_ds.map(scaling)","2f0c1e9f":"for batch in train_ds.take(1):\n    for img in batch:\n        display(array_to_img(img))","10f9cfb7":"test_path = os.path.join(root_dir, \"test\")\n\ntest_img_paths = sorted(\n    [\n        os.path.join(test_path, fname)\n        for fname in os.listdir(test_path)\n        if fname.endswith(\".jpeg\")\n    ]\n)\n","86e21978":"# Use TF Ops to process.\ndef process_input(input, input_size, upscale_factor):\n    input = tf.image.rgb_to_yuv(input)\n    last_dimension_axis = len(input.shape) - 1\n    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n    return tf.image.resize(y, [input_size, input_size], method=\"area\")\n\n\ndef process_target(input):\n    input = tf.image.rgb_to_yuv(input)\n    last_dimension_axis = len(input.shape) - 1\n    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n    return y\n\n\ntrain_ds = train_ds.map(\n    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n)\ntrain_ds = train_ds.prefetch(buffer_size=32)\n\nvalid_ds = valid_ds.map(\n    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n)\nvalid_ds = valid_ds.prefetch(buffer_size=32)","b8954727":"for batch in train_ds.take(1):\n    for img in batch[0]:\n        display(array_to_img(img))\n    for img in batch[1]:\n        display(array_to_img(img))","14e2bb94":"def get_model(upscale_factor=3, channels=1):\n    conv_args = {\n        \"activation\": \"relu\",\n        \"kernel_initializer\": \"Orthogonal\",\n        \"padding\": \"same\",\n    }\n    inputs = keras.Input(shape=(None, None, channels))\n    x = layers.Conv2D(64, 5, **conv_args)(inputs)\n    x = layers.Conv2D(64, 3, **conv_args)(x)\n    x = layers.Conv2D(32, 3, **conv_args)(x)\n    x = layers.Conv2D(channels * (upscale_factor ** 2), 3, **conv_args)(x)\n    outputs = tf.nn.depth_to_space(x, upscale_factor)\n\n    return keras.Model(inputs, outputs)","f0b6cce3":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\nfrom mpl_toolkits.axes_grid1.inset_locator import mark_inset\nimport PIL\n\n\ndef plot_results(img, prefix, title):\n    \"\"\"Plot the result with zoom-in area.\"\"\"\n    img_array = img_to_array(img)\n    img_array = img_array.astype(\"float32\") \/ 255.0\n\n    # Create a new figure with a default 111 subplot.\n    fig, ax = plt.subplots()\n    im = ax.imshow(img_array[::-1], origin=\"lower\")\n\n    plt.title(title)\n    plt.show()\n\n\ndef get_lowres_image(img, upscale_factor):\n    \"\"\"Return low-resolution image to use as model input.\"\"\"\n    return img.resize(\n        (img.size[0] \/\/ upscale_factor, img.size[1] \/\/ upscale_factor),\n        PIL.Image.BICUBIC,\n    )\n\n\ndef upscale_image(model, img):\n    \"\"\"Predict the result based on input image and restore the image as RGB.\"\"\"\n    ycbcr = img.convert(\"YCbCr\")\n    y, cb, cr = ycbcr.split()\n    y = img_to_array(y)\n    y = y.astype(\"float32\") \/ 255.0\n\n    input = np.expand_dims(y, axis=0)\n    out = model.predict(input)\n\n    out_img_y = out[0]\n    out_img_y *= 255.0\n\n    # Restore the image in RGB color space.\n    out_img_y = out_img_y.clip(0, 255)\n    out_img_y = out_img_y.reshape((np.shape(out_img_y)[0], np.shape(out_img_y)[1]))\n    out_img_y = PIL.Image.fromarray(np.uint8(out_img_y), mode=\"L\")\n    out_img_cb = cb.resize(out_img_y.size, PIL.Image.BICUBIC)\n    out_img_cr = cr.resize(out_img_y.size, PIL.Image.BICUBIC)\n    out_img = PIL.Image.merge(\"YCbCr\", (out_img_y, out_img_cb, out_img_cr)).convert(\n        \"RGB\"\n    )\n    return out_img","0186ad83":"class ESPCNCallback(keras.callbacks.Callback):\n    def __init__(self):\n        super(ESPCNCallback, self).__init__()\n        self.test_img = get_lowres_image(load_img(test_img_paths[0]), upscale_factor)\n\n    # Store PSNR value in each epoch.\n    def on_epoch_begin(self, epoch, logs=None):\n        self.psnr = []\n\n    def on_epoch_end(self, epoch, logs=None):\n        print(\"Mean PSNR for epoch: %.2f\" % (np.mean(self.psnr)))\n        if epoch % 20 == 0:\n            prediction = upscale_image(self.model, self.test_img)\n            plot_results(prediction, \"epoch-\" + str(epoch), \"prediction\")\n\n    def on_test_batch_end(self, batch, logs=None):\n        self.psnr.append(10 * math.log10(1 \/ logs[\"loss\"]))","efe95064":"early_stopping_callback = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10)\n\ncheckpoint_filepath = \"\/tmp\/checkpoint\"\n\nmodel_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor=\"loss\",\n    mode=\"min\",\n    save_best_only=True,\n)\n\nmodel = get_model(upscale_factor=upscale_factor, channels=1)\nmodel.summary()\n\ncallbacks = [ESPCNCallback(), early_stopping_callback, model_checkpoint_callback]\nloss_fn = keras.losses.MeanSquaredError()\noptimizer = keras.optimizers.Adam(learning_rate=0.001)","c6f3b63d":"epochs = 100\n\nmodel.compile(\n    optimizer=optimizer, loss=loss_fn,\n)\n\nmodel.fit(\n    train_ds, epochs=epochs, callbacks=callbacks, validation_data=valid_ds, verbose=2\n)\n\n# The model weights (that are considered the best) are loaded into the model.\nmodel.load_weights(checkpoint_filepath)","2aa01cea":"total_bicubic_psnr = 0.0\ntotal_test_psnr = 0.0\n\nfor index, test_img_path in enumerate(test_img_paths[35:45]):\n    img = load_img(test_img_path)\n    lowres_input = get_lowres_image(img, upscale_factor)\n    w = lowres_input.size[0] * upscale_factor\n    h = lowres_input.size[1] * upscale_factor\n    highres_img = img.resize((w, h))\n    prediction = upscale_image(model, lowres_input)\n    lowres_img = lowres_input.resize((w, h))\n    lowres_img_arr = img_to_array(lowres_img)\n    highres_img_arr = img_to_array(highres_img)\n    predict_img_arr = img_to_array(prediction)\n    bicubic_psnr = tf.image.psnr(lowres_img_arr, highres_img_arr, max_val=255)\n    test_psnr = tf.image.psnr(predict_img_arr, highres_img_arr, max_val=255)\n\n    total_bicubic_psnr += bicubic_psnr\n    total_test_psnr += test_psnr\n\n    print(\n        \"PSNR of low resolution image and high resolution image is %.4f\" % bicubic_psnr\n    )\n    print(\"PSNR of predict and high resolution is %.4f\" % test_psnr)\n    plot_results(lowres_img, index, \"lowres\")\n    plot_results(highres_img, index, \"highres\")\n    plot_results(prediction, index, \"prediction\")\n\nprint(\"Avg. PSNR of lowres images is %.4f\" % (total_bicubic_psnr \/ 10))\nprint(\"Avg. PSNR of reconstructions is %.4f\" % (total_test_psnr \/ 10))","cea2e82b":"### Here we have defined callbacks to monitor training\n\n1. The ESPCNCallback object will compute and display the PSNR metric. \n2. This is the main metric we use to evaluate super-resolution performance.","6423913c":"### Here we define functions to process the input images.\n\n1. First, we convert our images from the RGB color space to the YUV colour space.\n2. We take input data (low-resolution images), we crop the image, retrieve the y channel (luninance), and resize it. We only consider the luminance channel in the YUV color space because humans are more sensitive to luminance change.\n3. For the target data (high-resolution images), we just crop the image and retrieve the y channel.","50efe5a6":"### Normalize the pixels for training and validation images","aa064853":"### Now we run the model prediction and plot the results","21e9b756":"1. Here I am trying to implement the [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](http:\/\/https:\/\/arxiv.org\/pdf\/1609.05158.pdf) paper on medical chest X-ray images.\n\n2. Here authors have proposed sub-pixel convolution layer which learns an array of upscaling filters to upscale the final Low Resolution feature maps into the High Resolution output. This effectively helps to eliminate bicubic filter in the Super Resolution pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall Super Resolution operation. ","cbcbe861":"### Here we have prepared a dataset of test image paths that we will use for visual evaluation at the end of this example.","054656c6":"### Here we have defined some utility functions to monitor our results\n\n1. plot_results to plot an save an image.\n2. get_lowres_image to convert an image to its low-resolution version.\n3. upscale_image to turn a low-resolution image to a high-resolution version reconstructed by the model. In this function, we use the y channel from the YUV color space as input to the model and then combine the output with the other channels to obtain an RGB image.","82c83289":"### Define image data generators to load and preprocess the images","8b06b5ac":"### Visualize input images after processing them. And we will also look into target images","4d693239":"### Define Our CNN Model Architecture"}}