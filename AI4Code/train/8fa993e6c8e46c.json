{"cell_type":{"7b4fe843":"code","dc106fc4":"code","e7e5f5f9":"code","de64a78b":"code","07445189":"code","9b7a6f61":"code","b0e333c0":"code","268773d5":"code","6b00dd5a":"code","5e017bc2":"code","481570bb":"code","736cb744":"code","d4afc578":"code","9fa08363":"code","25d72637":"code","8b583e97":"code","2ce1bffb":"code","2e0f39ef":"code","50581f80":"code","2c92ea41":"code","c59a4333":"code","068259d3":"code","d825dc98":"code","a1428c45":"code","994b6d4b":"code","5edc3984":"code","ed0e8bf6":"code","44e3a87e":"code","2927c24b":"code","9969e6a9":"code","f67bc736":"code","fe32c8eb":"code","d96484a9":"code","4b5bc200":"code","10c5d063":"code","37d5da4b":"code","d877ed14":"code","be06281c":"code","97526f72":"markdown","7dc609ed":"markdown","d4d2eeb1":"markdown","f8eea735":"markdown","8acddc22":"markdown","19a2b2cc":"markdown","e29380bb":"markdown","bb320ff0":"markdown","7129b7d5":"markdown","e5ccee35":"markdown","e8fff0aa":"markdown","b13e2457":"markdown","4e4e87f5":"markdown","3670ebd0":"markdown","856351ca":"markdown","5d1f8def":"markdown","dcd3fa6b":"markdown"},"source":{"7b4fe843":"### Here are the packages used in this notebook\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport datetime\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns             # https:\/\/seaborn.pydata.org\/index.html\nfrom wordcloud import WordCloud   # https:\/\/pypi.org\/project\/wordcloud\/\n\nimport pandasql                    # https:\/\/github.com\/yhat\/pandasql\nfrom pandasql import sqldf\npysqldf = lambda q: sqldf(q, globals())\n\nfrom sklearn.preprocessing import MultiLabelBinarizer   #https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MultiLabelBinarizer.html\nimport ast                          # https:\/\/docs.python.org\/3\/library\/ast.html\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.linear_model import LinearRegression,Lasso\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split,GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.feature_selection import SelectKBest,chi2\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom xgboost import plot_importance\nfrom types import FunctionType\nfrom fastai.imports import *\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","dc106fc4":"# First let's read the files in\n# I like to do all my transformations on one file to prevent issues so I will create a file with both and tag as train\/test\n# Since I am doing log and normalize transformations across both files, results may vary if done only on the train file.\n# release_date is in a horrible format - parse it on the way in \n\ntrain = pd.read_csv(\"..\/input\/train.csv\",parse_dates=[\"release_date\"])\ntrain['file']='train'\ntrain = train.set_index('id')\nprint('train')\nprint(train.head(2))\n\ntest = pd.read_csv(\"..\/input\/test.csv\",parse_dates=[\"release_date\"])\ntest['file']='test'\ntest = test.set_index('id')\nprint('test')\nprint(test.head(2))\n\nboth = pd.concat([train, test], sort=False)\n\nprint('both')\nprint(both.head(2))\n      \nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nprint('train data types')  \ntrain.dtypes\nprint('test data types')  \ntest.dtypes\nprint('both data types')  \nboth.dtypes\n","e7e5f5f9":"both.info()","de64a78b":"both.describe(include='all')","07445189":"# Show me the nulls across both files - what am I dealing with here?\nprint(both.isnull().sum())","9b7a6f61":"# Unsure if we are concerned WHICH collection, but we will assume that they wouldn't make another in the series if the first one bombed\n# Using pandasql, one of my favorites though perforance can suffer. Dataset is small so let's give it a go\nbelongs_to_q = \"\"\"\nselect \ncount(*) as count, substr(belongs_to_collection, instr(belongs_to_collection,'name'), (instr(belongs_to_collection,'name')+40)) \nfrom both \ngroup by substr(belongs_to_collection, instr(belongs_to_collection,'name'), (instr(belongs_to_collection,'name')+40))\norder by count(*) desc \n\"\"\"\nbelongs_to_o = pysqldf(belongs_to_q)\nprint(belongs_to_o)\n\n# We see James Bond is the big leader. \n# I do see many collections of 1, meaning others in the collection are either spelled differently or not in this dataset.\n# I would error on the side of just one-hotting the existence of a collection","b0e333c0":"# Since we are only interested in the existence of a name, let's one-hot it into a new column part_of_a_collection\nboth['part_of_a_collection'] = np.where(both['belongs_to_collection'].isna(), 0, 1)\n\ncollection_counts_q = \"\"\"\nselect \npart_of_a_collection, count(*) as count\nfrom both\ngroup by part_of_a_collection\n\"\"\"\ncollection_counts_o = pysqldf(collection_counts_q)\nprint(collection_counts_o)","268773d5":"# What is the distribution of the budget?\n\n# Viz with seaborn. It's visually pleasing\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\n# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)\nsns.despine(left=True)\n\n# Plot a simple histogram with binsize determined automatically\nsns.distplot(both['budget'],kde=False, color=\"b\", ax=axes[0, 0])\n\n# Plot a kernel density estimate and rug plot\nsns.distplot(both['budget'], hist=False, rug=True, color=\"r\", ax=axes[0, 1])\n\n# Plot a filled kernel density estimate\nsns.distplot(both['budget'], hist=False, color=\"g\", kde_kws={\"shade\": True}, ax=axes[1, 0])\n\n# Plot a historgram and kernel density estimate\nsns.distplot(both['budget'], color=\"m\", ax=axes[1, 1])\n\nplt.setp(axes, yticks=[])\nplt.tight_layout()\n","6b00dd5a":"# budget is very skewed. Makes sense. \n# I don't want to remove the outliers because those are the blockbusters (isn't that what we want to really know?)\n# The evaluation also logs to minimize the effect of the blockbusters anyway\n# How many of those budgets are zero (we know they aren't really zero...and are the same missing data)\n\nbudgets0_q = \"\"\"\nselect \n status, count(*) as count\nfrom both\nwhere budget = 0\ngroup by status\n\"\"\"\nbudgets0_o = pysqldf(budgets0_q)\nprint(budgets0_o)\n\n# I though that the 0 values might correspond to films not yet released, that this isn't the case. This is a difficulty for sure","5e017bc2":"# I will try a Sigmoid function to help\ndef sigmoid(x):\n    e = np.exp(1)\n    y = 1\/(1+e**(-x))\n    return y\nsigmoid_results = sigmoid(both['budget'])\nsigmoid_results.describe()\n\n# That doesn't look good","481570bb":"# try log function\nbudget_log = np.log(both['budget'])\nbudget_log.describe\n\n# divide by zero...nope","736cb744":"# Next up, Log + 1\nbudget_log1 = np.log(both['budget'] + 1)\nbudget_log1.describe()\n\n# better - include a normalize\ndef normalize(column):\n    upper = column.max()\n    lower = column.min()\n    y = (column - lower)\/(upper-lower)\n    return y\n\nbudget_log1_normalized = normalize(np.log(both['budget'] + 1))\nbudget_log1_normalized.describe()\n\n# looks good\n# set a new column as budget_log1_norm\nboth['budget_log1_norm'] = normalize(np.log(both['budget'] + 1))","d4afc578":"genres_q = \"\"\"\nselect \ncount(*) as count, substr(genres, instr(genres,'name'), (instr(genres,'name')+40)) \nfrom both \ngroup by substr(genres, instr(genres,'name'), (instr(genres,'name')+40))\norder by count(*) desc \n\"\"\"\ngenres_o = pysqldf(genres_q)\nprint(genres_o)\n\n# I see that a movie can have multiple genres.\n# Do I simply 1-hot out the results or dig deeper into the combinations?","9fa08363":"# While I would like to explore the combinations, let's start simply with 1-hots.\n# Compliments to https:\/\/www.kaggle.com\/liviuasnash\/predict-movies-step-by-step\/notebook\nboth['genres'] = both['genres'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\nboth['genres']","25d72637":"# The EDA and feature engineering for the dict columns seems similar. Create a few basic common utilities\ndef print_few_values(col_name):\n    print(\"Sample values for\", col_name)\n    both[col_name].head(5).apply(lambda x: print(x))\n    \ndef dictionary_sizes(col_name):\n    return (both[col_name].apply(lambda x: len(x)).value_counts())\n\ndef print_dictionary_sizes(col_name):\n    print(\"\\n===================================================\")\n    print(\"Distribution of sizes for\", col_name)\n    print(dictionary_sizes(col_name))\n    \n# returns a list of tuples of names for a given row of a column\ndef dict_name_list(d, name=\"name\"):\n    return ([i[name] for i in d] if d != {} else [])\n\n# returns a list of tuples of the (id,name) pairs for a given row of a column\ndef dict_id_name_list(d, name=\"name\"):\n    return ([(i[\"id\"],i[name]) for i in d] if d != {} else [])\n\n# returns a list of names for a given column\ndef col_name_list(col_name, name=\"name\"):\n    # Get a list of lists of names\n    name_list_list = list(both[col_name].apply(lambda x: dict_name_list(x, name)).values)\n    # Merge into 1 list\n    return ([i for name_list in name_list_list for i in name_list])\n\n# returns a list of tuples of the (id,name) pairs for a given column\ndef col_id_name_list(col_name, name=\"name\"):\n    # Get a list of lists of (id,name) tuples\n    tuple_list_list = list(both[col_name].apply(lambda x: dict_id_name_list(x, name)).values)\n    # Merge into 1 list\n    return ([i for tuple_list in tuple_list_list for i in tuple_list])\n\ndef get_names_counter(col_name, name=\"name\"):\n    name_list = col_name_list(col_name, name)\n    return (collections.Counter(name_list))\n    \ndef print_top_names(col_name, name=\"name\"):\n    print(\"\\n===================================================\")\n    print(\"Top {0}s for {1}\".format(name, col_name))\n    c = get_names_counter(col_name, name)\n    print(c.most_common(20))\n    \ndef EDA_dict(col_name):\n    print_few_values(col_name)\n    print_dictionary_sizes(col_name)\n    print_top_names(col_name)\n    \ndef add_dict_size_column(col_name):\n    both[col_name + \"_size\"] = both[col_name].apply(lambda x: len(x) if x != {} else 0)\n\ndef add_dict_id_column(col_name):\n    c = col_name + \"_id\"\n    both[c] = both[col_name].apply(lambda x: x[0][\"id\"] if x != {} else 0)\n    both[c] = both[c].astype(\"category\")\n\n# for each of the top values in the dictionary, add an column indicating if the row belongs to it\ndef add_dict_indicator_columns(col_name):\n    c = get_names_counter(col_name)\n    top_names = [x[0] for x in c.most_common(20)]\n    for name in top_names:\n        both[col_name + \"_\" + name] = both[col_name].apply(lambda x: name in dict_name_list(x))\n        \ndef drop_column(col_name):\n    both.drop([col_name], axis=1, inplace=True)\n    \ndef feature_engineer_dict(col_name):\n    add_dict_size_column(col_name)\n    max_size = dictionary_sizes(col_name).index.max()\n    if max_size == 1:\n        add_dict_id_column(col_name)\n    else:\n        add_dict_indicator_columns(col_name)\n    drop_column(col_name)\n    \ndef encode_column(col_name):\n    lbl = LabelEncoder()\n    lbl.fit(list(both[col_name].values)) \n    both[col_name] = lbl.transform(list(both[col_name].values))\n    \ncol_name=\"genres\"\nEDA_dict(col_name)\n\nfeature_engineer_dict(col_name)","8b583e97":"both.columns","2ce1bffb":"# get rid of the sames in science fiction and tv movie...not helpful\nboth.columns = ['belongs_to_collection', 'budget', 'homepage', 'imdb_id',\n       'original_language', 'original_title', 'overview', 'popularity',\n       'poster_path', 'production_companies', 'production_countries',\n       'release_date', 'runtime', 'spoken_languages', 'status', 'tagline',\n       'title', 'Keywords', 'cast', 'crew', 'revenue', 'file',\n       'part_of_a_collection', 'budget_log1_norm', 'genres_size',\n       'genres_Drama', 'genres_Comedy', 'genres_Thriller', 'genres_Action',\n       'genres_Romance', 'genres_Adventure', 'genres_Crime',\n       'genres_Science_Fiction', 'genres_Horror', 'genres_Family',\n       'genres_Fantasy', 'genres_Mystery', 'genres_Animation',\n       'genres_History', 'genres_Music', 'genres_War', 'genres_Documentary',\n       'genres_Western', 'genres_Foreign', 'genres_TV_Movie']","2e0f39ef":"# Convert T\/F to 1\/0 for future matching\nboth.genres_Drama = both.genres_Drama.astype(int)\nboth.genres_Comedy = both.genres_Comedy.astype(int)\nboth.genres_Thriller = both.genres_Thriller.astype(int)\nboth.genres_Action = both.genres_Action.astype(int)\nboth.genres_Romance = both.genres_Romance.astype(int)\nboth.genres_Adventure = both.genres_Adventure.astype(int)\nboth.genres_Crime = both.genres_Crime.astype(int)\nboth.genres_Science_Fiction = both.genres_Science_Fiction.astype(int)\nboth.genres_Horror = both.genres_Horror.astype(int)\nboth.genres_Family = both.genres_Family.astype(int)\nboth.genres_Fantasy = both.genres_Fantasy.astype(int)\nboth.genres_Mystery = both.genres_Mystery.astype(int)\nboth.genres_Animation = both.genres_Animation.astype(int)\nboth.genres_History = both.genres_History.astype(int)\nboth.genres_Music = both.genres_Music.astype(int)\nboth.genres_War = both.genres_War.astype(int)\nboth.genres_Documentary = both.genres_Documentary.astype(int)\nboth.genres_Western = both.genres_Western.astype(int)\nboth.genres_Foreign = both.genres_Foreign.astype(int)\nboth.genres_TV_Movie = both.genres_TV_Movie.astype(int)\nboth.head(10)","50581f80":"# 20 top revenue genre combos\ng_rev = \"\"\"\nwith list as \n(select avg(revenue) as avg_revenue\n,    genres_Drama, genres_Comedy, genres_Thriller, genres_Action,\n       genres_Romance, genres_Adventure, genres_Crime,\n       genres_Science_Fiction, genres_Horror, genres_Family,\n       genres_Fantasy, genres_Mystery, genres_Animation,\n       genres_History, genres_Music, genres_War, genres_Documentary,\n       genres_Western, genres_Foreign, genres_TV_Movie\n       from both \n       group by\n       genres_Drama, genres_Comedy, genres_Thriller, genres_Action,\n       genres_Romance, genres_Adventure, genres_Crime,\n       genres_Science_Fiction, genres_Horror, genres_Family,\n       genres_Fantasy, genres_Mystery, genres_Animation,\n       genres_History, genres_Music, genres_War, genres_Documentary,\n       genres_Western, genres_Foreign, genres_TV_Movie\n  )\n  select row_number()  over (\n       order by avg_revenue asc) as row_number\n  , genres_Drama, genres_Comedy, genres_Thriller, genres_Action,\n       genres_Romance, genres_Adventure, genres_Crime,\n       genres_Science_Fiction, genres_Horror, genres_Family,\n       genres_Fantasy, genres_Mystery, genres_Animation,\n       genres_History, genres_Music, genres_War, genres_Documentary,\n       genres_Western, genres_Foreign, genres_TV_Movie\n       from list\n       \n       \n\"\"\"\ngenre_revenue = pysqldf(g_rev)\ngenre_revenue.genres_Drama = genre_revenue.genres_Drama.astype(int)\ngenre_revenue.genres_Comedy = genre_revenue.genres_Comedy.astype(int)\ngenre_revenue.genres_Thriller = genre_revenue.genres_Thriller.astype(int)\ngenre_revenue.genres_Action = genre_revenue.genres_Action.astype(int)\ngenre_revenue.genres_Romance = genre_revenue.genres_Romance.astype(int)\ngenre_revenue.genres_Adventure = genre_revenue.genres_Adventure.astype(int)\ngenre_revenue.genres_Crime = genre_revenue.genres_Crime.astype(int)\ngenre_revenue.genres_Science_Fiction = genre_revenue.genres_Science_Fiction.astype(int)\ngenre_revenue.genres_Horror = genre_revenue.genres_Horror.astype(int)\ngenre_revenue.genres_Family = genre_revenue.genres_Family.astype(int)\ngenre_revenue.genres_Fantasy = genre_revenue.genres_Fantasy.astype(int)\ngenre_revenue.genres_Mystery = genre_revenue.genres_Mystery.astype(int)\ngenre_revenue.genres_Animation = genre_revenue.genres_Animation.astype(int)\ngenre_revenue.genres_History = genre_revenue.genres_History.astype(int)\ngenre_revenue.genres_Music = genre_revenue.genres_Music.astype(int)\ngenre_revenue.genres_War = genre_revenue.genres_War.astype(int)\ngenre_revenue.genres_Documentary = genre_revenue.genres_Documentary.astype(int)\ngenre_revenue.genres_Western = genre_revenue.genres_Western.astype(int)\ngenre_revenue.genres_Foreign = genre_revenue.genres_Foreign.astype(int)\ngenre_revenue.genres_TV_Movie = genre_revenue.genres_TV_Movie.astype(int)\ngenre_revenue.tail(20)","2c92ea41":"olang_q = \"\"\"\nselect \ncount(*) as count, original_language\nfrom both \ngroup by original_language\norder by count(*) desc \n\"\"\"\nolang_o = pysqldf(olang_q)\nprint(olang_o)\n\n# overwhelmingly English. I'm not sure if this is enough variation to be useful.","c59a4333":"both['is_English'] = 0 \nboth.loc[ both['original_language'] == \"en\" ,\"is_English\"] = 1","068259d3":"# What is the distribution of the popularity?\n\n# Viz with seaborn. It's visually pleasing\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\n# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)\nsns.despine(left=True)\n\n# Plot a simple histogram with binsize determined automatically\nsns.distplot(both['popularity'],kde=False, color=\"b\", ax=axes[0, 0])\n\n# Plot a kernel density estimate and rug plot\nsns.distplot(both['popularity'], hist=False, rug=True, color=\"r\", ax=axes[0, 1])\n\n# Plot a filled kernel density estimate\nsns.distplot(both['popularity'], hist=False, color=\"g\", kde_kws={\"shade\": True}, ax=axes[1, 0])\n\n# Plot a historgram and kernel density estimate\nsns.distplot(both['popularity'], color=\"m\", ax=axes[1, 1])\n\nplt.setp(axes, yticks=[])\nplt.tight_layout()","d825dc98":"# commenting out for efficiency\n\n# popularity in relation to revenue\n#pop_rev = sns.swarmplot(x='popularity', y='revenue', data=both)\n#pop_rev\n\n# More popular = More revenue? Shocker!\n# It is skewed so let's log1 normalize it to a new column popularity_log1_norm\n\nboth['popularity_log1_norm'] = normalize(np.log(both['popularity'] + 1))\n\n","a1428c45":"# 1 test row without a date - that will probably need to be fixed or removed\n# 3829\n# Thinking about it, do not remove.  Then the shapes will not align. \n# Looking up the information:\n# https:\/\/www.imdb.com\/title\/tt0210130\/?ref_=nv_sr_1?ref_=nv_sr_1\n# 20 March 2001\n\n#both['release_date']=both['release_date'].fillna('3\/20\/2001')\n\n# I want to break the date parts out..... \nboth['release_date']\nboth['release_date'] = pd.to_datetime(both['release_date'], format='%m\/%d\/%y')\nboth['release_year'] = both.release_date.dt.year.fillna(2001)\nboth['release_month'] = both.release_date.dt.month.fillna(3)\nboth['release_day'] = both.release_date.dt.day.fillna(20)\nboth['release_day_of_week'] = both.release_date.dt.dayofweek.fillna(1)\nboth['release_quarter'] = both.release_date.dt.quarter.fillna(1) ","994b6d4b":"# This is a check to be sure I don't have any null date fields\n\nprint(both['release_quarter'].isnull().sum())\n","5edc3984":"# Runtime and Revenue?\n# Replace NaN with zeroes\n\nboth['runtime'] = both['runtime'].fillna\n\n\n","ed0e8bf6":"both.columns","44e3a87e":"# Create a dataframe with only the columns I want and move revenue to the end\n\nboth_final = both[['file', \n       'part_of_a_collection', 'budget_log1_norm', 'genres_size',\n       'genres_Drama', 'genres_Comedy', 'genres_Thriller', 'genres_Action',\n       'genres_Romance', 'genres_Adventure', 'genres_Crime',\n       'genres_Science_Fiction', 'genres_Horror', 'genres_Family',\n       'genres_Fantasy', 'genres_Mystery', 'genres_Animation',\n       'genres_History', 'genres_Music', 'genres_War', 'genres_Documentary',\n       'genres_Western', 'genres_Foreign', 'genres_TV_Movie',\n       'popularity_log1_norm', 'release_year', 'release_month', 'release_day',\n       'release_day_of_week', 'release_quarter','is_English','revenue']]\n\n\n\nboth_final","2927c24b":"# Time to put split the data into train and test files\n# Then drop the file column, we don't need that anymore\n\ntrain_final_1 = both_final[(both_final['file'] == \"train\")]\ntrain_final_1 = train_final_1.drop(columns=['file'])\n\ntest_final_1 = both_final[(both_final['file'] == \"test\")]\ntest_final_1 = test_final_1.drop(columns=['file'])\n\ny_train = train_final_1[['revenue']]\ny_train","9969e6a9":"# Do I have nulls in there??\n# Yes, test revenue\ntest_final_1.sum(), test_final_1.min(), test_final_1.max()\ntest_final_1['revenue'] = 1000\ntest_final_1.sum(), test_final_1.min(), test_final_1.max()","f67bc736":"y_train = y_train.values","fe32c8eb":"# 1.) Remove table meta data, column names etc. \u2192 Just use values for prediction.\nX_train = train_final_1.values\n#y_train = y_train.values\n\nX_test  = test_final_1.values\n\n# Update:) Scale\n\nX_scaler = StandardScaler()\nX_train  = X_scaler.fit_transform(X_train)\nX_test   = X_scaler.transform(X_test)\ny_train  = np.log(y_train)\ny_scaler = MinMaxScaler((0,1))","d96484a9":"# Do I have nulls in there??\n\nX_train.sum(), X_train.min(), X_train.max()\ny_train.sum(), y_train.min(), y_train.max()\nX_test.sum(), X_test.min(), X_test.max()\n","4b5bc200":"y_train  = y_scaler.fit_transform(y_train).ravel() # transform and convert column-vector y to a 1d array\n\n# Update:) Create Validation Split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=56)","10c5d063":"# The data is prepped, now to fit and predict.\n# Nice kernel with more details : https:\/\/www.kaggle.com\/alexandermelde\/code-template-for-simple-regression-prediction\/data\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import SGDRegressor\nimport lightgbm as lgb\n\nkr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), cv=5,\n                  param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n                              \"gamma\": np.logspace(-2, 2, 5)})\n\nsvr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n                               \"gamma\": np.logspace(-2, 2, 5)})\n\ntr = tree.DecisionTreeRegressor()\n\nest = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1,\n     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n\nsgd1 = SGDRegressor(alpha=0.0001, fit_intercept=False, max_iter=100, early_stopping=True, n_iter_no_change=5, epsilon=0.1,\n                   loss='squared_loss')\n\nLGB = lgb.LGBMRegressor(n_estimators=10000, \n                             objective='regression', \n                             metric='rmse',\n                             max_depth = 5,\n                             num_leaves=30, \n                             min_child_samples=100,\n                             learning_rate=0.01,\n                             boosting = 'gbdt',\n                             min_data_in_leaf= 10,\n                             feature_fraction = 0.9,\n                             bagging_freq = 1,\n                             bagging_fraction = 0.9,\n                             importance_type='gain',\n                             lambda_l1 = 0.2,\n                             bagging_seed=2729, \n                             subsample=.8, \n                             colsample_bytree=.9,\n                             use_best_model=True)\n\n# 2.) Calculate the coefficients of the linear regression \/ \"Train\"\n#reg     = KNeighborsRegressor().fit(X_train, y_train)\nreg     = LGB.fit(X_train, y_train)\n\n# 3.) Define functions to calculate a score\ndef score_function(y_true, y_pred):\n    # see https:\/\/www.kaggle.com\/c\/tmdb-box-office-prediction\/overview\/evaluation\n    # we use Root Mean squared logarithmic error (RMSLE) regression loss\n    assert len(y_true) == len(y_pred)\n    return np.sqrt(np.mean((np.log1p(y_pred) - np.log1p(y_true))**2))\n\ndef score_function2(y_true, y_pred):\n    # alternative implementation\n    y_pred = np.where(y_pred>0, y_pred, 0)\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n    \n# 4.) Apply the regression model on the prepared train, validation and test set and invert the logarithmic scaling\ny_train_pred  = np.exp(y_scaler.inverse_transform(np.reshape(reg.predict(X_train), (-1,1))))\ny_val_pred    = np.exp(y_scaler.inverse_transform(np.reshape(reg.predict(X_val), (-1,1))))\ny_test_pred   = np.exp(y_scaler.inverse_transform(np.reshape(reg.predict(X_test), (-1,1))))\n                   \n# 5.) Print the RMLS error on training, validation and test set. it should be as low as possible\nprint(\"RMLS Error on Training Dataset:\\t\", score_function(y_train , y_train_pred), score_function2(y_train , y_train_pred))\nprint(\"RMLS Error on Validation Dataset:\\t\", score_function(y_val , y_val_pred), score_function2(y_val , y_val_pred))\nprint(\"RMLS Error on Provided Test Dataset:\\t mystery!\")","37d5da4b":"# Make sure my shapes are in order\n# If they are not, go back through your code. Did you remove any rows during prep?\n\ntest.shape\ny_test_pred.shape\ntest.index","d877ed14":"test.columns","be06281c":"# 1.) Add the predicted values to the original test data\ndf_test = test.assign(revenue=y_test_pred)\n\n# 2.) Extract a table of ids and their revenue predictions\n\noutput = df_test.loc[:, lambda df_test: [ 'revenue']]\noutput\n\n# 3.) save that table to a csv file\noutput.to_csv(\"submission1c.csv\")\n\n# 4.) take a look\npd.read_csv(\"submission1c.csv\").head(5)","97526f72":"# #1 Simple Regression Model ","7dc609ed":"2.  #### examine budget**  int64 non-null","d4d2eeb1":"#### Before I start addressing the data, what columns do I THINK will be important?\n\nWhat does my gut tell me?\n- belongs_to_collection: I think so given the Avengers phenomenon\n- budget: definitely though you do have your occasional outliers\n- genre: definitely\n- original_language: I would think a common language would appeal to the greater audience\n- release_date: Definitely given inflation and trends in budgets\n- runtime: better check that one, I'm not paying 10 dollars to see a 45 minute movie\n- status: well, it does have to be released, doesn't it?\n- title: I think so, but not positive on that one\n- cast: do I know anyone on the cast?\n- popularity: I know about movies from word of mouth or social media\n- overview:","f8eea735":"5.  #### examine popularity","8acddc22":"svr   \nRMLS Error on Training Dataset:\t 14.986179084900266 14.976100987536602\nRMLS Error on Validation Dataset:\t 15.1306913168581 15.121250850703301\nRMLS Error on Provided Test Dataset:\t mystery!  3.07075\n\ntree \nRMLS Error on Training Dataset:\t 15.626489093924114 15.608505921209446\nRMLS Error on Validation Dataset:\t 15.781387350831444 15.764270359722536\nRMLS Error on Provided Test Dataset:\t mystery!\n\nGRradientBoostingRegression\nRMLS Error on Training Dataset:\t 15.624023710355614 15.606106913849564\nRMLS Error on Validation Dataset:\t 15.77976526199863 15.76267975857145\n\nSDG Regressor\nRMLS Error on Training Dataset:\t 1.6505243240191463 1.6255415812775773\nRMLS Error on Validation Dataset:\t 1.3693205074018826 1.340932778473505\nRMLS Error on Provided Test Dataset:\t mystery!\nRMLS Error on Training Dataset:\t 2.249698339443753 2.230180102897242\nRMLS Error on Validation Dataset:\t 2.049057861691203 2.0282387315895387\nRMLS Error on Provided Test Dataset:\t 14.97801   Overfitting\n\nswinging pedulum\nRMLS Error on Training Dataset:\t 93.5929418826801 93.59325525330341\nRMLS Error on Validation Dataset:\t 94.48388810814194 94.48491750861014\nRMLS Error on Provided Test Dataset:\t mystery!\n\nRMLS Error on Training Dataset:\t 11.025184846538503 11.021722849659747\nRMLS Error on Validation Dataset:\t 10.938330718432164 10.932387972056876\nRMLS Error on Provided Test Dataset:\t 99.56452     whoah!!!!!!\n\nlgb \nRMLS Error on Training Dataset:\t 15.623141909969396 15.605248036469964\nRMLS Error on Validation Dataset:\t 15.779742496373743 15.762788777313922\nRMLS Error on Provided Test Dataset:\t mystery!","19a2b2cc":"### Start Simply using just the data provided \n- https:\/\/www.kaggle.com\/c\/tmdb-box-office-prediction\/data","e29380bb":"3.  #### examine genre**  object json","bb320ff0":"7.  #### examine runtime","7129b7d5":"6.  #### examine release_date","e5ccee35":"### TMDB Box Office Prediction    \n\n###### It is your job to predict the international box office revenue for each movie. For each id in the test set, you must predict the value of the revenue variable.\n\n###### Submissions are evaluated on Root-Mean-Squared-Logarithmic-Error (RMSLE) between the predicted value and the actual revenue. Logs are taken to not overweight blockbuster revenue movies.[\u00b6](http:\/\/localhost:8888\/notebooks\/Kaggle\/MoviePredictions\/Kaggle%20Movie%20Revenue%20Predictor.ipynb#Submissions-are-evaluated-on-Root-Mean-Squared-Logarithmic-Error-(RMSLE)-between-the-predicted-value-and-the-actual-revenue.-Logs-are-taken-to-not-overweight-blockbuster-revenue-movies.)","e8fff0aa":"In this notebook I will convey what I am seeing and thinking. An important part of data science is Subject Matter Expertise. I am definitely NOT a movie industry SME. I will rely on my gut and reading about the industry online. You will see more of my thought process than code.","b13e2457":"4.  #### examine original_language","4e4e87f5":"1.  #### examine belongs_to_collection  (object) , we are interested in 'name'","3670ebd0":"KneighborsRegressor\nRMLS Error on Training Dataset:\t 15.547592983884789 15.539551960636024\nRMLS Error on Validation Dataset:\t 15.70143937791011 15.694826211333938\nRMLS Error on Provided Test Dataset:\t mystery!\n\nKernelRidge\nRMLS Error on Training Dataset:\t 15.497063904422337 15.485354018649339\nRMLS Error on Validation Dataset:\t 15.584945042892722 15.574921228069943\nRMLS Error on Provided Test Dataset:\t mystery!\n\n","856351ca":"Result is 2.66 on the leaderboard... eh","5d1f8def":"#### List of the columns and their tranformations before modelling and effect on rows\n\n - id                          : drop\n - belongs_to_collection       : 1-hot to 'part_of_collection'\n - budget                      : log1 normalized to 'budget_log1_norm'\n - genres                      : new genre_* columns added as 1-hot\n - homepage                    : drop\n - imdb_id                     : drop\n - original_language           : drop for now\n - original_title              : drop for now\n - overview                    : NLP - drop by now but I do have some wordCloud logic below\n - popularity                  : log1 normalized to 'popularity_log1_norm'\n - poster_path                 : drop\n - production_companies        : drop for now\n - production_countries        : drop for now\n - release_date                : fix the layout and add new breakdown columns\n - runtime                     :\n - spoken_languages            : drop for now\n - status                      : only want released movies\n - tagline                     : drop\n - title                       : drop for now\n - Keywords                    : drop for now\n - cast                        : drop\n - crew                        : drop\n - revenue                     : This is the target\n - file                        : drop after split back to train and test","dcd3fa6b":"# Examine the Data"}}