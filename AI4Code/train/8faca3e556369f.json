{"cell_type":{"3cfd10a0":"code","da40513e":"code","d990a35f":"code","ea6a9874":"code","2dc11bf0":"code","4b43abc2":"code","8c090dde":"code","c20a8b47":"code","2e7dda80":"code","7ff2ab08":"code","1bc39e07":"code","18c33f8f":"code","033e0b00":"code","4e339c1f":"code","74fb1b2a":"code","ca3f30ca":"code","b49fbb20":"code","01855381":"code","f28a476d":"code","5faa05d0":"code","0a08068d":"code","946670d3":"code","767ef785":"code","d8564c34":"code","b2b9fd00":"code","cf4913fa":"code","43d5f727":"code","31ab323f":"code","eab5aa58":"code","28c6e3aa":"code","b46b93f8":"code","328953c5":"code","7b1fb258":"code","b6bca60a":"code","11a90acb":"code","f31ef717":"code","72df2d64":"code","1e647352":"markdown","b64a2f61":"markdown"},"source":{"3cfd10a0":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da40513e":"import pandas as pd\nimport os\nos.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data')","d990a35f":"import json\nwith open('..\/input\/coleridgeinitiative-show-us-the-data\/train\/f8b03c87-9d1a-4f20-b76b-cb6c69d447b2.json') as f:\n    sample = json.load(f)","ea6a9874":"sample[:2]","2dc11bf0":"for s in sample:\n    print(s['section_title'])","4b43abc2":"import pandas as pd\ndf_train=pd.read_csv('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\ndf_train.head()","8c090dde":"df_train.shape","c20a8b47":"for col in df_train.columns:\n    print(f\"{col}: {len(df_train[col].unique())}\")","2e7dda80":"df_test=pd.read_csv('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ndf_test","7ff2ab08":"df_input = pd.DataFrame(columns=['id','section_title','text','data_label'])\nids=df_train['Id'].values\nimport warnings\nwarnings.filterwarnings(\"ignore\", 'This pattern has match groups')\nlen(ids)","1bc39e07":"for id in ids:\n    df=pd.read_json ('..\/input\/coleridgeinitiative-show-us-the-data\/train\/{}.json'.format(id))\n    \n    for data_label in df_train[df_train['Id']==id]['dataset_label'].values:\n        new_df=df[df['text'].str.contains(data_label)].copy(deep=True)\n        new_df.loc[:,['data_label']] = data_label\n        new_df.loc[:,['id']] = id\n        new_df.reset_index(inplace=True,drop=True)\n        df_input=pd.concat([df_input, new_df], ignore_index=True)\n        df_input.reset_index(inplace=True,drop=True)","18c33f8f":"df","033e0b00":"df_input.head()","4e339c1f":"df_input.isnull().sum()","74fb1b2a":"from wordcloud import WordCloud, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport pickle\nimport pyLDAvis.sklearn\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ca3f30ca":"words =list( df_input['data_label'].values)\nwords=[word.split() for word in words]","b49fbb20":"allwords = []\nfor wordlist in words:\n    allwords += wordlist\n#print(allwords)","01855381":"mostcommon = FreqDist(allwords).most_common(100)\n#Taking most 100 frequent words occuring\n\nwordcloud = WordCloud(width=1600, height=800, background_color='white').generate(str(mostcommon))\n\nfig = plt.figure(figsize=(30,10), facecolor='grey')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Top 100 Most Common Words in Data Label', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()","f28a476d":"mostcommon_small = FreqDist(allwords).most_common(25)\nx, y = zip(*mostcommon_small)\nplt.figure(figsize=(50,30))\nplt.margins(0.02)\nplt.bar(x, y)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.title('Freq of 25 Most Common Words in Data-Label', fontsize=60)\nplt.show()","5faa05d0":"words =list( df_input['section_title'].values)\n\nstopwords=['ourselves', 'hers','the', 'between', 'yourself', 'but', 'again','of', 'there', 'about', 'once', \n           'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', \n           'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', \n           'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', \n           'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', \n           'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', \n           'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', \n           'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', \n           'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', \n           'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', \n           'was', 'here', 'than']\n\nsplit_words=[]\nfor word in words:\n    lo_w=[]\n    list_of_words=str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\nfor wordlist in split_words:\n    allwords += wordlist\n    \nmostcommon = FreqDist(allwords).most_common(100)\nwordcloud = WordCloud(width=1600, height=800, background_color='white').generate(str(mostcommon))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Top 100 Most Common Words in Section Title', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()","0a08068d":"mostcommon_small = FreqDist(allwords).most_common(25)\nx, y = zip(*mostcommon_small)\nplt.figure(figsize=(50,30))\nplt.margins(0.02)\nplt.bar(x, y)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.title('Freq of 25 Most Common Words in Section-Title', fontsize=60)\nplt.show()","946670d3":"df_test_input = pd.DataFrame(columns=['id','section_title','text'])\n\nids=df_test['Id'].values\nimport warnings\nwarnings.filterwarnings(\"ignore\", 'This pattern has match groups')\n#len(ids)","767ef785":"for id in ids:\n    df=pd.read_json ('..\/input\/coleridgeinitiative-show-us-the-data\/test\/{}.json'.format(id))\n    \n    df.loc[:,['id']] = id\n    df.reset_index(inplace=True,drop=True)\n    df_test_input=pd.concat([df_test_input, df], ignore_index=True)\n    df_test_input.reset_index(inplace=True,drop=True)","d8564c34":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","b2b9fd00":"submission_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv', index_col=0)\nsubmission_df","cf4913fa":"df_test_input","43d5f727":"df_test_input.text.str.len()","31ab323f":"df_test_input['length'] = df_test_input.text.str.len()","eab5aa58":"df_test_input","28c6e3aa":"df_test_input =df_test_input[df_test_input.length > 0]\ndf_test_input","b46b93f8":"submission_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv', index_col=0)\nsubmission_df.head()","328953c5":"datasets_titles = [str(x).lower() for x in df_input['data_label'].unique()]\ndatasets_titles[0]","7b1fb258":"import re\nlabels = []","b6bca60a":"for index in submission_df.index:\n    publication_text = df_test_input[df_test_input['id'] == index].text.str.cat(sep='\\n').lower()\n    \n    label = []\n    for dataset_title in datasets_titles:\n        if dataset_title in publication_text:\n            label.append(clean_text(dataset_title))\n    labels.append('|'.join(label))","11a90acb":"labels","f31ef717":"submission_df['PredictionString'] = labels","72df2d64":"submission_df.to_csv('submission.csv')\n\nsubmission_df","1e647352":"Data provided to us in this competition comprises of following 4 items:\n\n* train.csv - This file comprises of publication-id and the associated data title and labels along with the cleansed label.\n* train folder - This folder provides json file for each of the id's present in the above csv file, with each of the json file describing section-details of the publication and the associated text with it.\n* sample_submission.csv - This file comprises of id, for which we must predict the data-set title.\n* test folder - This folder provides json file for each of the id's present in test.csv file and the json file describes the section details of the publication and the associated text.","b64a2f61":"Competition Objective\nOne liner - We are required to build an algorithm that can find our what are the datasets that a publications uses.\n\nDescription - In this competition, we need to develop an algorithm to automate the discovery of how scientific data are referenced in publications. We have with us the full text of scientific publications from numerous research areas, we'll identify data sets that the publications' authors used in their work.\n\nWe have a labelled dataset (train set) that we'll use to develop our algorithm. The unlabelled dataset (test set) will be used for evaluation of the algorithm."}}