{"cell_type":{"374f16b8":"code","e67a1f1c":"code","e5c5921e":"code","6fc3b062":"code","dd7cfe21":"code","0965f863":"code","0331f040":"code","af4d6dec":"code","7b9738bb":"code","9b699bc9":"code","61a22da7":"code","3ede55db":"code","603e83a2":"code","f918ec95":"code","53e52ab8":"code","853e0058":"markdown","070affd4":"markdown","7f22dcfa":"markdown","a880b409":"markdown","3cac1831":"markdown","415839ac":"markdown"},"source":{"374f16b8":"!# Install scispaCy for tokenization, sentence splitting and bio-entity recognition\n!pip install scispacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","e67a1f1c":"import spacy\nfrom spacy.tokenizer import Tokenizer\nimport unittest\n\nclass SCISPACY:\n    \"\"\"This class wraps scispaCy. Tokenization, sentence splitting and bio-entity recognition are performed. The CTD chemicals vocabulary is used to recognize the Chemical entities. The list\n    of predefined coronavirus Diseases is used to recognize the Disease entities.\n    \"\"\"\n\n    #list of chemicals with their MESH ID in the CTD vocabulary\n    ctd_chemical_dictionary = {}\n    \n    #list of Coronavirus synonyms with their MESH ID\n    coronavirus_dictionary = {'covid-19': 'C000657245',\n                              'covid 19': 'C000657245',\n                              'coronavirus': 'C000657245',\n                              'corona virus': 'C000657245',\n                              'corona-virus': 'C000657245',\n                              '2019-ncov': 'C000657245',\n                              '2019 ncov': 'C000657245',\n                              '2019ncov': 'C000657245',\n                              'sars-cov': 'C000657245',\n                              'sars cov': 'C000657245',\n                              'mers-cov': 'C000657245',\n                              'mers': 'C000657245',\n                              'mers-coronavirus': 'C000657245',\n                              'mers coronavirus': 'C000657245',\n                              'mers cov': 'C000657245',\n                              'severe acute respiratory syndrome coronavirus': 'C000657245',\n                              'severe acute respiratory syndrome': 'C000657245',\n                              'severe acute respiratory syndrome-associated coronavirus': 'C000657245',\n                              'severe acute respiratory syndrome associated coronavirus': 'C000657245',\n                              'middle east respiratory syndrome coronavirus': 'C000657245',\n                              'middle east respiratory syndrome': 'C000657245',\n                              'middle east respiratory coronavirus': 'C000657245',\n                              '2019-ncov disease': 'C000657245',\n                              '2019 ncov disease': 'C000657245',\n                              '2019ncov disease': 'C000657245',\n                              '2019-ncov infection': 'C000657245',\n                              '2019 ncov infection': 'C000657245',\n                              '2019ncov infection': 'C000657245',\n                              '2019 novel coronavirus disease': 'C000657245',\n                              '2019 novel coronavirus infection': 'C000657245',\n                              'coronavirus disease-19': 'C000657245',\n                              'coronavirus disease 2019': 'C000657245',\n                              'hcov': 'C000657245',\n                              'cov': 'C000657245'}\n\n    \n    def __init__(self, custom_tokenizer):\n\n        print(\"Init scispaCy\")\n\n        print(spacy.__version__)\n\n        # load the scispaCy model\n        self.nlp = spacy.load(\"en_core_sci_lg\")\n\n        # create a custom tokenizer that does not split multi-words entities like mers-cov\n        if (custom_tokenizer == True):\n            self.nlp.tokenizer = self.create_custom_tokenizer(self.nlp)\n            self.nlp.add_pipe(self.prevent_sentence_boundary_detection, name='prevent-sbd', before='parser')\n\n        # load the CTD vocabulary\n        input_file = open(\"\/kaggle\/input\/ctd-chemicals\/CTD\/CTD_chemicals.tsv\", encoding='utf-8')\n        # perform file operations\n        for line in input_file:\n            if not line.startswith(\"#\"):\n                fields = line.rstrip(\"\\n\").split(\"\\t\")\n                chemical_name = fields[0]\n                chemical_id = fields[1][5:]\n                if len(chemical_name) > 4:\n                    self.ctd_chemical_dictionary[chemical_name.lower()] = chemical_id\n                synonyms = fields[7].split(\"|\")\n                for synonym in synonyms:\n                    if len(synonym) > 4:\n                        self.ctd_chemical_dictionary[synonym.lower()] = chemical_id\n\n        #print(\"Loaded \" + str(len(self.ctd_chemical_dictionary)) + \" chemicals from CTD.\")\n\n\n    def create_custom_tokenizer(self, nlp):\n        \"\"\"This tokenizer does not split multi-words entities like mers-cov\"\"\"\n\n        my_prefix = r'[\\-]'\n\n        all_prefixes_re = spacy.util.compile_prefix_regex(tuple(list(nlp.Defaults.prefixes) + [my_prefix]))\n\n        # Handle ( that doesn't have proper spacing around it\n        custom_infixes = ['\\.\\.\\.+', '(?<=[0-9])-(?=[0-9])', '[!&:,()]']\n        infix_re = spacy.util.compile_infix_regex(tuple(list(nlp.Defaults.infixes) + custom_infixes))\n\n        suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n\n        return Tokenizer(nlp.vocab, nlp.Defaults.tokenizer_exceptions,\n                         prefix_search=all_prefixes_re.search,\n                         infix_finditer=infix_re.finditer, suffix_search=suffix_re.search,\n                         token_match=None)\n\n\n    def custom_boundary(self, docx):\n\n        is_print = False\n        for token in docx[:-2]:\n            if (token.text[-1:] == \".\" and (docx[token.i + 1].text[0:1].isupper() or not docx[token.i + 1].is_alpha or (not docx[token.i + 1].is_lower and not docx[token.i + 1].is_upper))) or \\\n                    token.text == \";\":\n                docx[token.i + 1].is_sent_start = True\n\n        return docx\n\n\n    def prevent_sentence_boundary_detection(self, docx):\n\n        for token in docx:\n            # This will entirely disable spaCy's sentence detection\n            token.is_sent_start = False\n\n        return self.custom_boundary(docx)\n\n    \n    def tokenize(self, text):\n\n        annotatedDocument = [] \n        doc = self.nlp(text)\n        token_index = 0\n        sentence = []\n        for token in doc:\n            if token.is_sent_start == True and len(sentence) != 0:\n                annotatedDocument.append(sentence)\n                sentence = []\n\n            row = [token_index, token.idx, token.idx + len(token.text), token.text]\n            sentence.append(row)\n            token_index = token_index + 1\n\n        annotatedDocument.append(sentence)\n\n        return annotatedDocument\n\n\n    def analyze_covid19(self, text):\n        \"\"\"Tokenize the COVID-19 dataset and recognize the Chemical and Disease entities.\n        scispaCy is not able to distinguish between Chemicals and coronavirus Diseases. To do that we match \n        the entities recognized by scispaCy with the chemicals in the CTD chemical vocabulary and with the diseases in the predefined list of coronavirus diseases.\n        \"\"\"\n\n        recognized_entities = [] \n        doc = self.nlp(text)\n        token_index = 0\n        sentence = []\n        entity = []\n        for token in doc:\n\n            entity_iob = token.ent_iob_\n\n            if entity_iob == \"B\":\n                if entity != []:\n                    mesh_id = None\n                    entity_type = None\n                    if entity[2].lower() in self.ctd_chemical_dictionary:\n                        mesh_id = self.ctd_chemical_dictionary[entity[2].lower()]\n                        entity_type = \"Chemical\"\n                    elif entity[2].lower() in self.coronavirus_dictionary:\n                        mesh_id = self.coronavirus_dictionary[entity[2].lower()]\n                        entity_type = \"Disease\"\n                    if mesh_id != None:\n                        entity[3] = entity_type\n                        entity[4] = mesh_id\n                        recognized_entities.append(entity.copy())\n                entity = [token.idx, token.idx + len(token.text), token.text, None, None]\n            elif entity_iob == \"I\":\n                entity = [entity[0], token.idx + len(token.text), entity[2] + \" \" + token.text, None, None]\n            else:\n                if entity != []:\n                    mesh_id = None\n                    entity_type = None\n                    if entity[2].lower() in self.ctd_chemical_dictionary:\n                        mesh_id = self.ctd_chemical_dictionary[entity[2].lower()]\n                        entity_type = \"Chemical\"\n                    elif entity[2].lower() in self.coronavirus_dictionary:\n                        mesh_id = self.coronavirus_dictionary[entity[2].lower()]\n                        entity_type = \"Disease\"\n                    if mesh_id != None:\n                        entity[3] = entity_type\n                        entity[4] = mesh_id\n                        recognized_entities.append(entity.copy())\n                    entity = []\n\n        return recognized_entities\n\nif __name__ == '__main__':\n    custom_tokenizer = False\n    obj = SCISPACY(custom_tokenizer)\n    print(\"Check if scispaCy has been installed correctly!\")\n    text = \"Abatacept dose-dependently reduces T-cell proliferation, serum concentrations of acute-phase reactants, and other markers of inflammation, including the production of rheumatoid factor by B cells.\"\n    print(obj.tokenize(text))\n","e5c5921e":"!# Create a running directory to store the processed publications \n!mkdir .\/CORD-19-Running-Dataset\/","6fc3b062":"import json\nimport os\n\ndef annotate_covid_19_dataset():\n    \"\"\"Annotates the COVID-19 dataset with scispaCy. \n    \"\"\"\n    \n    title_counter = 0\n    abstract_counter = 0\n    custom_tokenizer = False\n    annotator = SCISPACY(custom_tokenizer)\n    # the output file\n    output_file = open(\".\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.txt\", 'w', encoding='utf-8')\n    # the input dataset\n    covid_19_dataset = '\/kaggle\/input\/CORD-19-research-challenge\/'\n    for root, dirs, files in os.walk(covid_19_dataset):\n\n        for file_name in files:\n            if '.json' in file_name:\n\n                # Remove this filter if you want to analyze the whole dataset. Analyzing COVID-19 with scispaCy might take several hours.\n                if file_name != '426da8c3fb9c6792b5d26214d55471099877e337.json':\n                    continue\n\n                chemical_entity_found = False\n                disease_entity_found = False\n                abstract = \"\"\n                with open(os.path.join(root, file_name)) as json_file:\n                    data = json.load(json_file)\n                    title = data['metadata']['title'].strip()\n                    if not title.endswith(\".\"):\n                        title = title + \".\"\n                        #print(\"Title\", title)\n                    title_len = len(title.split(\" \"))\n\n                    abstract = \"\"\n                    if len(data['abstract']) > 0:\n                        abstract_object = data['abstract']\n                        for text_i in abstract_object:\n                            if abstract == \"\":\n                                abstract = text_i['text'].strip()\n                            else:\n                                abstract = abstract + \" \" + text_i['text'].strip()\n                        if not abstract.endswith(\".\"):\n                            abstract = abstract + \".\"\n\n                    for text_i in data['body_text']:\n                        text = text_i['text'].strip()\n                        if not abstract.endswith(\".\") and text[0:1].isupper():\n                            abstract = abstract + \". \" + text\n                        else:\n                            abstract = abstract + \" \" + text\n                            \n                    #maximum document length that we can analyze with our pipeline\n                    abstract = abstract[:900000]\n\n                    abstract_len = len(abstract.split(\" \"))\n\n                    #remove really short document\n                    if title_len < 5 and abstract_len < 50:\n                        continue\n\n                    document = title + \"\\n\" + abstract\n\n                    entities = annotator.analyze_covid19(document)\n\n                    for entity in entities:\n                        if entity[3] == 'Chemical':\n                            chemical_entity_found = True\n                        if entity[3] == 'Disease':\n                            disease_entity_found = True\n\n                    #write documents that contain at least one entity of type Chemical and one entity of type Disease\n                    #these are the documents that can contain a Chemical-Disease relation.\n                    if chemical_entity_found == True and disease_entity_found == True:\n\n                        # print(entities)\n                        output_file.write(file_name[:-5] + \"|t|\" + title + \"\\n\")\n                        output_file.write(file_name[:-5] + \"|a|\" + abstract + \"\\n\")\n                        for entity in entities:\n                            output_file.write(file_name[:-5] + \"\\t\" + str(entity[0]) + \"\\t\" + str(entity[1]) + \"\\t\" + entity[2] + \"\\t\" +\n                                entity[3] + \"\\t\" + entity[4] + \"\\n\")\n                        output_file.write(\"\\n\")\n\n    output_file.close()\n    \nannotate_covid_19_dataset()\n\nprint(\"Annotated publication saved in: .\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.txt\")\n\n\n\n","dd7cfe21":"!# See the annotated publication in PubTator format. It contains the full text publication and the Chemical and coronavirus Diseases recognized by scispaCy\n!cat .\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.txt","0965f863":"import re\n\nclass Pubtator2Semeval:\n    \"\"\"This class converts the PubTator file in input into an output file in a format similar to the Semeval 2010 task 8 format.\n     \n    e.g.,\n    \n    input: PubTator file\n    \n    output:\n    .......................\n    4e9acd476a5b8224baf48582bd4f4c095feafe27        CID:0   D008139 C000657245      loperamide      MERS-CoV        12      22      Other medications , such as myc\n    ophenolic acid , chloroquine , chlorpromazine , loperamide and lopinavir , have shown an inhibitory effect on MERS-CoV replication in vitro ;   4983    4993  5044    5052\n    .......................\n     \n    Fields:\n    1. the document id\n    2. a placeholder for the label predicted by the classifier\n    3. chemical entity MESH id\n    4. disease entity MESH id\n    5. chemical name\n    6. disease name\n    7. chemical token position in the sentence\n    8. disease token position in the sentence\n    9. sentence containing the candidate pairs\n    10-11. first-end character offset of the chemical entity in the whole document\n    12-13. first-end character offset of the disease entity in the whole document\n     \n    \"\"\"\n\n    annotator = None\n\n    def __init__(self, custom_tokenizer):\n        print(\"Init\")\n        self.annotator = SCISPACY(custom_tokenizer)\n\n\n    def run(self, input_file_name, output_file_name):\n\n        positive_examples = 0\n        negative_examples = 0\n        title_counter = 0\n        abstract_counter = 0\n        relation_counter = 0\n\n        # file in and file out\n        input_file = open(input_file_name, encoding='utf-8')\n        output_file = open(output_file_name, 'w', encoding='utf-8')\n\n        entities = []\n        relations = {}\n        title = None\n        abstract = None\n\n        # perform file operations\n        for line in input_file:\n\n            line = line.rstrip(\"\\n\")\n            #print(line)\n\n            # get the title\n            if re.match(\"^[0-9a-z]+\\|t\\|\", line):\n                title_counter = title_counter + 1\n                entities = []\n                relations = {}\n                title = line\n\n            # get the abstract\n            elif re.match(\"^[0-9a-z]+\\|a\\|\", line):\n                abstract_counter = abstract_counter + 1\n                abstract = line.replace(\"(ABSTRACT TRUNCATED AT 400 WORDS)\", \"\").replace(\n                    \"(ABSTRACT TRUNCATED AT 250 WORDS)\", \"\").replace(\"\u2002\", \"_\").replace(\"\u2009\", \"_\").replace(\"\u2005\",\n                                                                                                         \"_\").replace(\n                    \"\u200a\", \"_\")\n\n            # get the list of entities\n            elif re.match(\"^[0-9a-z]+\\t[0-9]+\\t[0-9]*\\t.+\\t.+\\t[DC0-9|\\-]+\", line):\n                match = re.match(\"^([0-9a-z])+\\t([0-9])+\\t([0-9]*)\\t(.+)\\t(.+)\\t([DC0-9|\\-]+)\", line)\n                e_id = match.group(6)\n                if e_id != '-1':\n                    entity = line\n                    entities.append(entity)\n\n                #print(line)\n\n            # get the list of relations among entities\n            elif re.match(\"^[0-9a-z]+\\tCID\\t[DC0-9|\\-]+\\t[DC0-9|]+\", line):\n                match = re.match(\"^([0-9a-z]+)\\t(CID)\\t([DC0-9|\\-]+)\\t([DC0-9|]+)\",\n                                     line)\n\n                group = match.group(2)\n                e1Id = match.group(3)\n                e2Id = match.group(4)\n\n                if e1Id + \" \" + e2Id in relations:\n                    labels = relations[e1Id + \" \" + e2Id]\n                    labels.append(group)\n                else:\n                    labels = []\n                    labels.append(group)\n                    relations[e1Id + \" \" + e2Id] = labels\n                relation_counter = relation_counter + 1\n\n            else:\n\n                document = re.match(\"^[0-9a-z]+\\|t\\|(.*)($)\", title).group(1) + \\\n                           \" \" + \\\n                           re.match(\"^[0-9a-z]+\\|a\\|(.*)($)\", abstract).group(1)\n\n                tokenized_document = self.annotator.tokenize(document)\n               \n                #generating the canditate Chemical-Disease pairs\n                for e1 in entities:\n                    e1_match = re.match(\"^([0-9a-z]+)\\t([0-9]+)\\t([0-9]+)\\t(.+)\\t(.+)\\t([DC0-9|]+)\", e1)\n                    e1_document_id = e1_match.group(1)\n\n                    e1_start = int(e1_match.group(2))\n                    e1_end = int(e1_match.group(3))\n                    e1_name = re.sub('[ ]', '#', e1_match.group(4))\n                    e1_type = e1_match.group(5)\n                    e1_id = e1_match.group(6)\n\n                    for e2 in entities:\n                        e2_match = re.match(\"^([0-9a-z]+)\\t([0-9]+)\\t([0-9]+)\\t(.+)\\t(.+)\\t([DC0-9|]+)\", e2)\n                        e2_start = int(e2_match.group(2))\n                        e2_end = int(e2_match.group(3))\n                        e2_name = re.sub('[ ]', '#', e2_match.group(4))\n                        e2_type = e2_match.group(5)\n                        e2_ids = e2_match.group(6)\n\n                        for e2_id in e2_ids.split(\"|\"):\n\n                            if \"Chemical\" in e1_type and \"Disease\" in e2_type:\n\n                                sentence, e1_token_position, e2_token_position = self.get_sentence(e1_document_id,\n                                    tokenized_document, e1_start, e1_end, e2_start, e2_end, e1_name,\n                                    e2_name)\n\n                                #remove really long senetences (i.e., sentences longer than 218 words, which is the maximum \n                                #number of words for a sentence in the dataset used for system training)\n                                if len(sentence) != 0 and len(sentence.split(\" \")) <= 218:\n\n                                    if (e1_id + \" \" + e2_id) in relations:\n\n                                        labels = relations[e1_id + \" \" + e2_id]\n                                        if len(labels) == 1: #no multilabel relations\n                                            for label in labels:\n                                                output_file.write(e1_document_id +\n                                                                \"\\t\" + \"CID:1\" +\n                                                                \"\\t\" + e1_id + \"\\t\" + e2_id +\n                                                                \"\\t\" + e1_name + \"\\t\" + e2_name +\n                                                                \"\\t\" + str(e1_token_position) +\n                                                                \"\\t\" + str(e2_token_position) +\n                                                                \"\\t\" + sentence + \"\\n\")\n                                                positive_examples = positive_examples + 1\n                                    else:\n                                        output_file.write(e1_document_id +\n                                                        \"\\t\" + \"CID:0\" +\n                                                        \"\\t\" + e1_id + \"\\t\" + e2_id +\n                                                        \"\\t\" + e1_name + \"\\t\" + e2_name +\n                                                        \"\\t\" + str(e1_token_position) +\n                                                        \"\\t\" + str(e2_token_position) +\n                                                        \"\\t\" + sentence + \"\\n\")\n                                        negative_examples = negative_examples + 1\n\n        input_file.close()\n        output_file.close()\n\n        \n    def get_sentence(self, document_id, text, e1_start, e1_end, e2_start, e2_end, e1_name, e2_name):\n        \"\"\"Get the sentence of the document that contains the candidate entity pair.\n\n        \"\"\"\n\n        #print(text)\n        out_sentence = \"\"\n\n        e1_position_id = None\n        e2_position_id = None\n\n        e1_in_sentence = False\n        e2_in_sentence = False\n\n        if e1_start == e2_start:\n            return out_sentence, e1_position_id, e2_position_id\n\n        for in_sentence in text:\n\n            out_sentence = \"\"\n\n            id = 0\n            for token in in_sentence:\n\n                start = token[1]\n                end = token[2]\n                form = token[3]\n\n                if e1_name in form and e1_start >= start and e1_end <= end:\n                    e1_in_sentence = True\n                    e1_position_id = id\n                    id = id + 1\n                    out_sentence = out_sentence + \" \" + form\n                elif form in e1_name and start >= e1_start and end <= e1_end:\n                    e1_in_sentence = True\n                    if not out_sentence.endswith(e1_name):\n                        e1_position_id = id\n                        id = id + 1\n                        out_sentence = out_sentence + \" \" + e1_name\n\n                elif e2_name in form and e2_start >= start and e2_end <= end:\n                    e2_in_sentence = True\n                    e2_position_id = id\n                    id = id + 1\n                    out_sentence = out_sentence + \" \" + form\n                elif form in e2_name and start >= e2_start and end <= e2_end:\n                    e2_in_sentence = True\n                    if not out_sentence.endswith(e2_name):\n                        e2_position_id = id\n                        id = id + 1\n                        out_sentence = out_sentence + \" \" + e2_name\n\n                else:\n                    out_sentence = out_sentence + \" \" + form\n                    id = id + 1\n\n            if e1_in_sentence == True and e2_in_sentence == True:\n                break\n\n            out_sentence = \"\"\n            e1_position_id = None\n            e2_position_id = None\n            e1_in_sentence = False\n            e2_in_sentence = False\n\n\n        return out_sentence.lstrip(), e1_position_id, e2_position_id\n\n\ndef contains_any(str, set):\n    \"\"\"Check whether 'str' contains ANY of the chars in 'set'\"\"\"\n    return 1 in [c in str for c in set]\n\n\nif __name__ == '__main__':\n    print(\"This only executes when is executed rather than imported\")\n    pubtator2semeval = Pubtator2Semeval(True)\n    pubtator2semeval.run(\".\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.txt\",\n                         \".\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.semeval.txt\")\n    print(\"Semeval dataset saved in: .\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.semeval.txt\")","0331f040":"!# See the candidate entity pairs to be annotated by BioBERT\n!cat .\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.semeval.txt","af4d6dec":"\"\"\"\nThis script converts the file in input that is in Semeval 2010 task 8 format into a file compatible with BioBERT.\n\ne.g.,\n\nInput:\n4e9acd476a5b8224baf48582bd4f4c095feafe27        CID:0   D008139 C000657245      loperamide      MERS-CoV        12      22      Other medications , such as mycophenolic acid , \nchloroquine , chlorpromazine , loperamide and lopinavir , have shown an inhibitory effect on MERS-CoV replication in vitro ;   4983    4993  5044    5052\n\nOutput:\nOther medications , such as myc ophenolic acid , chloroquine , chlorpromazine , @GENE$ and lopinavir , have shown an inhibitory effect on @DISEASE$ replication in vitro ;\n\nNote: we use @GENE$ instead of @CHEMICAL$ to mark chemicals for our convenience, since our scripts can work on multiple datasets with entities of the different type.\n\n\"\"\"\nfrom __future__ import print_function\nimport numpy as np\nimport re\n\nimport gzip\nimport os\nimport sys\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nif (sys.version_info > (3, 0)):\n    import pickle as pkl\nelse: #Python 2.7 imports\n    import cPickle as pkl\n\nimport networkx as nx\nimport random\n\n\nnp.set_printoptions(suppress=True)\n\n# = [folder_training+'.\/CDR_TrainingDevelopmentSet.PubTator.semeval', folder_training+'.\/CDR_TestSet.PubTator.semeval']\nfiles = ['', '.\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.semeval.txt']\n\n\ndef createTrainingDataSet(fileIn, fileOut):\n\n    output_file = open(fileOut, 'w', encoding='utf-8')\n\n    for line in open(fileIn, encoding='utf-8'):\n\n        splits = line.strip().split('\\t')\n\n        label = splits[1]\n        new_label = None\n        if label == \"CID:0\":\n            new_label = \"0\"\n        else:\n            new_label = \"1\"\n\n        entity1_pos = int(splits[6])\n        entity2_pos = int(splits[7])\n\n        example = \"\"\n        if entity1_pos == entity2_pos:\n            example = \"@GENE$ @DISEASE$\\t0\"\n\n        else:\n            sentence = splits[8].split(\" \")\n            sentence[entity1_pos] = \"@GENE$\"\n            sentence[entity2_pos] = \"@DISEASE$\"\n\n            example = ' '.join(sentence) + \"\\t\" + new_label\n\n        output_file.write(example + \"\\n\")\n\n    output_file.close()\n\n\n\ndef createTestDataSet(fileIn, fileOut):\n    \"\"\"\n    Creates the dataset to be annotaed in the BioBERT format\n\n    :param data_file_in: the dataset in Semeval format\n    :param file_out: the dataset in BioEBRT format\n\n    \"\"\"\n\n    output_file = open(fileOut, 'w', encoding='utf-8')\n    output_file.write(\"index\tsentence\tlabel\\n\")\n\n    counter = 0\n    for line in open(fileIn, encoding='utf-8'):\n\n        splits = line.strip().split('\\t')\n\n        label = splits[1]\n        new_label = None\n        if label == \"CID:0\":\n            new_label = \"0\"\n        else:\n            new_label = \"1\"\n\n        docId = splits[0]\n        entity1_pos = int(splits[6])\n        entity2_pos = int(splits[7])\n\n        example = \"\"\n        if entity1_pos == entity2_pos:\n            example = str(counter) + \"\\t\" + \"@GENE$ @DISEASE$\\t0\"\n\n        else:\n\n            sentence = splits[8].split(\" \")\n            sentence[entity1_pos] = \"@GENE$\"\n            sentence[entity2_pos] = \"@DISEASE$\"\n\n            example = str(counter) + \"\\t\" +  ' '.join(sentence) + \"\\t\" + new_label\n\n        output_file.write(example + \"\\n\")\n\n        counter = counter + 1\n\n    output_file.close()\n\n#createTrainingDataSet(files[0], files[0] + \".BioBERT.txt\")\ncreateTestDataSet(files[1], files[1] + \".BioBERT.txt\")\nprint(\"BioBERT dataset saved in: \" + files[1] + \".BioBERT.txt\")\n\n","7b9738bb":"!# See the candidate entity pairs to be annotated in the format required by BioBERT \n!cat .\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.semeval.txt.BioBERT.txt","9b699bc9":"!# Download BioBERT\n!git clone https:\/\/github.com\/dmis-lab\/biobert.git\n!# Install BioBERT\n!cd .\/biobert; pip install -r requirements.txt\n!# Install CUDA required by BioBERT\n!conda install -y cudatoolkit=9.0\n!# Install the BioBERT models created training BioBERT on the BioCreative V Chemical Disease Relation (CDR) dataset\n!cp -r \/kaggle\/input\/biocreativev-cdr-biobert-model\/BiocreativeV_CDR_BioBERT_Model\/* .\/biobert\/\n!cp -r \/kaggle\/input\/biobert-v10-pubmed-pmc-pretrained-model\/biobert_v1.0_pubmed_pmc_pretrained_model\/WEIGHTS .\/biobert\/\n!# Copy the dataset to be annotated in the test.tsv file required by BioBERT\n!#cp \/kaggle\/input\/covid19annotateddataset\/COVID-19-Annotated-Dataset\/COVID-19-Dataset.PubTator.semeval.BioBERT.txt .\/CORD-19-Running-Dataset\/test.tsv\n!cp .\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.semeval.txt.BioBERT.txt .\/CORD-19-Running-Dataset\/test.tsv\n!# Annotate the data set to be annotated by BioBERT\n!chmod 750 .\/biobert\/run_re.sh\n!.\/biobert\/run_re.sh","61a22da7":"!#See the BioBERT annotation\n!cat \/kaggle\/working\/biobert\/RE_output\/test_results.tsv","3ede55db":"def get_predicted_labels(data_file_in, file_out):\n    \"\"\"\n    Reads the file containing the predictions made by BioBERT and returns the predicted class labels, i.e., CID:0 (negative example), CID:1(positive example).\n\n    :param data_file_in: the predictions made by BioBERT\n    :param file_out: the labels produced by BioBERT\n\n    \"\"\"\n\n    lines = []\n\n    file_out = open(file_out, 'w', encoding=\"utf8\")\n\n    for line in open(data_file_in, encoding=\"utf8\"):\n        # lines.append(line.strip())\n\n        line = line.strip()\n\n        if line != \"\":\n\n            predCPR0 = float(line.split(\"\\t\")[0])\n            predCPR1 = float(line.split(\"\\t\")[1])\n            if predCPR0 > predCPR1:\n                label = \"CID:0\"\n            else:\n                label = \"CID:1\"\n            file_out.write(label + \"\\n\")\n\n    file_out.close()\n\nget_predicted_labels(\".\/biobert\/RE_output\/test_results.tsv\", '.\/CORD-19-Running-Dataset\/test_results.tsv.biobert.txt')\nprint(\"File saved in: .\/CORD-19-Running-Dataset\/test_results.tsv.biobert.tx\")\n\n","603e83a2":"!#See the labels assigned by BioBERT to the candidate entity pairs\n!cat .\/CORD-19-Running-Dataset\/test_results.tsv.biobert.txt","f918ec95":"def read_predictions(data_file_in):\n    \"\"\"\n    Reads the labels assigned by BioBERT to the candidate entity pairs\n    \n    :param data_file_in: the predicted labels \n\n    :return: the list of predicted labels \n    \"\"\"\n\n    classes = []\n    prob = []\n\n    for line in open(data_file_in, encoding=\"utf8\"):\n        line = line.strip()\n        classes.append(line)\n        #prob.append(line.split(\"\\t\")[1])\n\n    return classes, prob\n\n\ndef predictions2Pubtator(pubtator_data_file_in, semeval_data_file_in, predictions_file_in, annotated_data_file_out):\n    \"\"\"\n    Reads the Chemical-Disease relations produced by BioBERT and creates a PubTator file containing the predicted relations\n    \n    :param pubtator_data_file_in: the data set to be annotated\n    :param semeval_data_file_in: the file containing the examples in input to BioBERT\n    :param predictions_file_in: the file containing the predicted relations produced by BioBERT\n    :param annotated_dataset_file_out: the dataset in input including the relazions produced by BioBERT\n\n    :return:\n    \"\"\"\n\n    predictions, probabilities = read_predictions(predictions_file_in)\n    file_out = open(annotated_data_file_out, 'w', encoding=\"utf8\")\n    line_counter = 0\n    docid_label_map = {}\n    print(semeval_data_file_in)\n    visited = {}\n    \n    for line in open(semeval_data_file_in, encoding=\"utf8\"):\n        splits = line.strip().split('\\t')\n        doc_id = splits[0]\n\n        label = predictions[line_counter]\n        e1_id = splits[2]\n        e2_id = splits[3]\n\n        if label != \"CID:0\":\n            label = 'CID'\n            value = doc_id + \"\\t\" + label + \"\\t\" + e1_id + \"\\t\" + e2_id\n\n            if value not in visited:\n                visited[value] = 1\n\n                if doc_id in docid_label_map:\n                    entry = docid_label_map[doc_id]\n                    entry.append(value)\n                else:\n                    entry = []\n                    entry.append(value)\n                    docid_label_map[doc_id] = entry\n\n        line_counter = line_counter + 1\n\n    doc_id = \"\"\n    prev_doc_id = \"\"\n    print(pubtator_data_file_in)\n    string_buffer = \"\"\n    for line in open(pubtator_data_file_in, encoding=\"utf8\"):\n        line = line.strip()\n        if line != \"\":\n            splits = line.split('\\t')\n            if '|t|' in line:\n                if string_buffer != \"\":\n                    if doc_id in docid_label_map:\n                        entry = docid_label_map[doc_id]\n                        for rel in entry:\n                            string_buffer = string_buffer + rel + \"\\n\"\n                    file_out.write(\"%s\\n\" % string_buffer)\n                string_buffer = line + \"\\n\"\n\n            elif not(len(line.split(\"\\t\")) == 4 and 'CID' in line):\n                doc_id = splits[0]\n                string_buffer = string_buffer + line + \"\\n\"\n                \n    if string_buffer != \"\":\n        if doc_id in docid_label_map:\n            entry = docid_label_map[doc_id]\n            for rel in entry:\n                string_buffer = string_buffer + rel + \"\\n\"\n        file_out.write(\"%s\\n\" % string_buffer)\n\n    file_out.close()\n\n\n#the dataset in Semeval format\ndevelopment_semeval_data_in = '.\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.semeval.txt'\n#the dataset in PubTator format\ndevelopment_pubtator_data_in = '.\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.txt'\n# the predictons made by BioBERT\nprediction_data_in = '.\/CORD-19-Running-Dataset\/\/test_results.tsv.biobert.txt'\n# An intermediate format that can use for debugging.\nannotated_development_semeval_data_out = development_semeval_data_in + \".annotated.txt.index.txt\"\n# Our final annotated dataset in PubTator format.\nannotated_development_pubtator_data_out = development_pubtator_data_in + '.annotated.txt'\n\npredictions2Pubtator(development_pubtator_data_in, development_semeval_data_in, prediction_data_in, annotated_development_pubtator_data_out)\n\nprint(\"File saved in: \" + annotated_development_pubtator_data_out)","53e52ab8":"!# See the annotated publication that includes the Chemical\/Disease entity mentions and entity concepts annotations, full-text retrieved from CORD-19, \n!# and the relation annotations among the recognized entity concepts in the same file. \n!# In the current publication there is a relation (marked by the label CID) between ribavirin (D012254) and SarsCov (C000657245)\n!cat .\/CORD-19-Running-Dataset\/CORD-19-Dataset.PubTator.txt.annotated.txt","853e0058":"# Abstract\n\n\n***Background:*** Some drugs that might treat COVID-19 have been studied, and dozens of publications reporting new chemicals that interact with COVID-19 are being published every day. Since scientific publications have no pre-defined format or organization, the retrieval of these chemicals can only be conducted on the basis of their co-occurrence with coronavirus diseases within a publication. Unfortunately, co-occurring chemicals and coronavirus diseases can be mentioned together without any relation between them. To clarify this concept, Example 1 below reports an example of explicitly stated relation between chemical\u3008type I interferon\u3009and MERS-CoV. On the other hand, in Example 2 chemical\u3008cysteine\u3009and cutaneous SARS-CoV co-occur, but without any relation between them. \n\n* Example 1: In vitro, <font color='orange'>MERS-CoV<\/font> is highly sensitive to <font color='green'>type I interferon<\/font>.\n* Example 2: Next, we replaced the corresponding <font color='green'>cysteine<\/font> in <font color='orange'>SARS-CoV<\/font> Nsp9 by alanine and serine.\n\nThis forces researchers to analyze a large amount of documents to find the actual relation of interest.\n\n<br>\n\n***Results:*** To provide researchers with formats that can be more easily queried and analyzed, bio-entity relations in the publications must first be annotated. We used state-of-the-art text-mining tools to automatically extract thousands of Chemical-COVID-19 relations from 29,322 publications in the COVID-19 Open Research Dataset (CORD-19).\n\nFirstly, the bio-entities in the publications were recognized by [scispaCy][1]. Then, from these entities we removed the entities that are neither Chemicals nor coronavirus Diseases. As regards Chemicals, we filtered out the entities that do not appear in the [CTD][2] chemical vocabulary. Unwanted Diseases were discarded considering whether the entities belong to a predefined list of coronavirus Diseases or not. The entity annotation includes both the mentions text spans (e.g., clarithromycin) and the normalized concepts identifiers in MESH (e.g., D017291). At the end of this step, 406,787 Chemical mentions (8,886 concepts) and 88,943 coronavirus Disease mentions (1 concept) have been recognized.\n\nAfter that, the relations between the extracted entities were annotated by [BioBERT][3] trained on the BioCreative V Chemical Disease Relation ([CDR][4]) dataset. This dataset consists of 1,500 PubMed articles with 1,279 annotated chemicals, 1,188 diseases and 3,116 chemical-disease interactions. For the BioBERT model development, we fine-tuned BioBERT on the BioCreative training set. Then, to check the ability of the model to extract the correct relations, the BioBERT model was evaluated on the BioCreative test set. We obtained an F1-Score of 59.25 (Pr:57.20; Re: 61.44). Since the promising results, the model was finally used to automatically annotate 287 potential Chemical-COVID-19 relations from CORD-19.\n\nGiven the dependency of relations on concepts, our test collection contains the Chemical\/Disease entity mentions and entity concepts annotations, full-text publications retrieved from CORD-19, and the relation annotations among the recognized entity concepts in the same set of articles. Figure 1 shows an example of annotated publication in our annotated dataset. **We made the dataset and all the intermediate data produced in this work available to the research community at this** [link][7].\n\n<br>\n\n![Example of annotated publication](http:\/\/hlt-services4.fbk.eu\/kaggle\/example.PubTator.png)\n\nFigure 1: *In article\u3008ID:426da8c3fb9c6792b5d26214d55471099877e337\u3009there are 72 Chemical mentions (10 concepts) and 19 Disease mentions (1 concept). For example, Chemical\u3008Ribavirin\u3009 has 52 mentions formed by the set {Ribavirin, ribavirin} but one concept (ID: D012254). Disease \u3008coronavirus\u3009 has 19 mentions formed by the set {coronavirus, SARS-CoV} but one concept (ID: C000657245). Then, the article contains one relation between Chemical\u3008ID: D012254\u3009and Disease\u3008ID: C000657245\u3009.* \n\n<br>\n\n***Discussion and Conclusions:*** We presented an annotated dataset of relations between Chemicals and COVID-19 entities. The thousands of relations extracted from the literature and included in our annotated dataset will make it easier for researchers to harvest data from publications.\n\nThe application of machine learning for Chemical-COVID-19 relation extraction from text is severely limited by the lack of annotated resources. To overcome this lack of resources for COVID-19, we have exploited the annotation of the BioCreative V Chemical-Disease Relation dataset to train an effectively model for relation extraction. A manual assessment of the quality of the extracted relations would be too expensive to perform. For this reason, we used the results obtained by our model on the BioCreative dataset as a good estimator of the quality of the relations extracted from CORD-19. This approximation is possible because both the relations in the Biocreative dataset and the relations extracted from CORD-19 belong to the same kind of relations of chemicals treating diseases. To have a more accurate assessment, we intend to form a committee of domain experts able to assess the quality of a sample check of the extracted relations from CORD-19. This will also help us gain a better understanding of errors affecting the current annotation and improve the implemented annotation pipeline. \n\nAnother point we are thinking of for a future release of our annotated dataset, is related to the annotation of the entity mention relations. The current version of the annotated dataset only contains the relations among the entity concepts in relationship at document-level. However, we think it might be of interest for researchers to have text evidence that explicitly describes the relation between two related entity mentions. Figure 2 highlights how the current concept-level annotation of the dataset might be enriched with the mention-level annotation.\n\nFinally, to quickly and easily browse publications through searching by chemicals related to COVID-19, we are in the process of developing a web application for publication retrieval (Figure 2). **A very preliminary version of such an application is available at this** [address][6].\n\n<br>\n\n![Example of annotated publication](http:\/\/hlt-services4.fbk.eu\/kaggle\/covid19demoonline_2.png)\n\nFigure 2: *At document-level there is a relation between chloroquine and SARS-CoV concepts. At mention-level the text:\u3008This suggests that previous similar favourable results obtained in cellulo only with chloroquine for highly pathogenic viruses, such as the SARS-CoV\u3009supports a relation between two specific mentions of chloroquine and SARS-CoV concepts.*\n\n<br>\n\n[1]: https:\/\/allenai.github.io\/scispacy\/\n[2]: http:\/\/ctdbase.org\/reports\/CTD_chemicals.tsv.gz\n[3]: https:\/\/github.com\/dmis-lab\/biobert\n[4]: https:\/\/biocreative.bioinformatics.udel.edu\/tasks\/biocreative-v\/track-3-cdr\/\n[5]: https:\/\/academic.oup.com\/database\/article\/doi\/10.1093\/database\/baw071\/2630422\n[6]: http:\/\/hlt-services4.fbk.eu\/covid19demo\/\n[7]: www.kaggle.com\/dataset\/02a02946d7730a0519924d5717f05edaf3cd29fea311e43c7a8d6498676a63bf\n\nI the rest of this document we report the main steps to produce our annotated dataset starting from the publications in CORD-19. Since the whole processing could take several hours, we will deal with the annotation of a single publication to let researchers try executing our code and check immediately for the expected output. However, commenting a single line of code allows us to annotate the whole dataset as described in Section *Bio-entity Recognition*. Then, all the intermediate files produced by our code available have been made available at this [address][7]. To run our code the Accelerator option available from the kaggle interface has to be set on GPU.\n\nSection **Installing sciSPACY** describes how to install scispaCy and how we customized it. Section **Bio-Entity Recognition** reports the code that can be executed to annotate the Chemical and COVID-19 entities. Section **Relation Extraction** shows how to install BioBERT and how to run the model trained on the Biocreative CDR dataset to annotate the relations among the exctracted entities. Finally, Section **Post-Processing** reports the code to trasform the predictions produced by BioBERT into the relation annotations in our dataset. \n","070affd4":"# Post-Processing\n\n![post processing](http:\/\/hlt-services4.fbk.eu\/kaggle\/post_processing.png)\n\n\nAfter the relations have been extracted, the relation annotations produced by BioBERT have to be included into the PubTator file that contains the full-text publication and recognized entities.    ","7f22dcfa":"# Bio-entity Recognition\n\n![entity recognition](http:\/\/hlt-services4.fbk.eu\/kaggle\/entity_recognition.png)\n\nBefore relation extraction, Chemicals and Coronavirus Diseases have to been recognized. We used the pipeline based on scispaCy implemented above to recognize the bio-entities in the COVID-19 dataset. Since this processing could take several hours to demonstrate the use of the implemented code, one single publication taken from the COVID-19 dataset is analyzed. To process the whole COVID-19 dataset it is sufficient to remove the filter in the annotate_covid_19_dataset function defined below.\n\n* Input: The COVID-19 dataset directory containing the json file of its publications.\n* Output: a single file in PubTator format containing the full text publications and the recognized Chemical and Disease entities.","a880b409":"# *Bio-entity Relation extraction*\n\n![relation extraction](http:\/\/hlt-services4.fbk.eu\/kaggle\/relation_extraction.png)\n\nWe formulate the relation extraction task as a binary classification problem, in which examples are generated from sentences as follows. We generate examples for all the sentences containing at least two entities of type Chemical and Disease. Thus the number of examples generated for each sentence is given by the combinations of distinct entities selected two at a time. Then, we use BioBERT to extract the relations among the candidate entity pairs. BioBERT requires the input dataset to be in a particular format. First, we transform the PubTator file annotated in the step above into the Semeval 2010 task 8 format. This file contains the candidate entities pairs and other usefull information that will allow us to trace the corresponding document in PubTator once had the annotation of BioBERT. Then, we use this file to produce the file in the format required by BioBERT. Finally we run the script provided with the BioBERT distribution to extract the relations among the candidate entity pairs.\n\n* Input: The PubTator file containing the full-text publications and the recognized Chemical and Disease entities.\n* Output:\n* The candidate entity pairs in Semeval 2010 task 8 format.\n* The candidate entity pairs in the format required by BioBERT\n* The file containing the prediction done by BioBERT","3cac1831":"The relations among the recognized bio-entities are annotated by BioBERT. The used BioBERT models were created training BioBERT on the BioCreative V Chemical Disease Relation (CDR) dataset. ","415839ac":"# Installing scispaCy\n\n![installation](http:\/\/hlt-services4.fbk.eu\/kaggle\/installation.png)\n\nscispaCy is used to tokenize the COVID-19 dataset and to recognize the Chemicals and Coronavirus Diseases in the publications. The Chemical entities were annotated matching the entities recognized by scispaCy with the entities in the CTD chemical vocabulary. The Diseases entities were annotated matching the entities with the entities contained in a predefined list of coronavirus Diseases. In the followng we describe how to install scispaCy and how we used scispaCy to implement our entity recognizer. "}}