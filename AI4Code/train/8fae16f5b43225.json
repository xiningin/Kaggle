{"cell_type":{"9893bd45":"code","e8fc8a8c":"code","20714344":"code","29d8c1ec":"code","50d48d43":"code","d1e5cade":"code","7dd42e27":"code","340719df":"code","28ca68fb":"code","6d7e5776":"code","7b16bf6e":"code","9c395b0f":"code","6605e9ef":"code","c74df77d":"code","e51aafd8":"code","b45a8930":"code","2674d252":"code","762bc180":"code","2fcb53e5":"code","3aeace9c":"code","a898bbb9":"code","86e0b90b":"code","250ef4f2":"code","e296740e":"code","7e94c314":"code","15a86d0a":"code","7b77ebb9":"code","6e622243":"code","8285e427":"code","5235f299":"code","8a36f7f8":"code","f411d472":"code","b23784aa":"code","debdc8a7":"code","0f6e0ae6":"code","dcfbd114":"code","dded5a3b":"code","ef7425a8":"code","2610b8f9":"code","edc3b391":"markdown"},"source":{"9893bd45":"!pip install iterative-stratification","e8fc8a8c":"# ==================\n# Library\n# ==================\nimport warnings\nwarnings.simplefilter('ignore')\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport sys\nimport pickle\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.nn import LayerNorm\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\nfrom torch.optim import lr_scheduler\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.nn import TransformerEncoder\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","20714344":"# ==========================\n# Constant\n# ==========================\nTRAIN_PATH = \"..\/input\/data-science-spring-osaka-hard-mode\/train.csv\"\nTEST_PATH = \"..\/input\/data-science-spring-osaka-hard-mode\/test_hard.csv\"\nACTION_PATH = \"..\/input\/data-science-spring-osaka-hard-mode\/actions.csv\"\nACTION_HARD_PATH = \"..\/input\/data-science-spring-osaka-hard-mode\/actions_hard.csv\"\nSUB_PATH =\"..\/input\/data-science-spring-osaka-hard-mode\/sample_submission.csv\"","29d8c1ec":"# =========================\n# main\n# =========================\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)","50d48d43":"train.head()","d1e5cade":"test.head()","7dd42e27":"train_all = pd.DataFrame()\ntest_all = pd.DataFrame()\nfor i in tqdm(train[\"file_path\"]):\n    train_ = pd.read_csv(f\"..\/input\/data-science-spring-osaka-hard-mode\/train\/{i}\")\n    train_[\"file\"] = i\n    train_all = pd.concat([train_all,train_]).reset_index(drop=True)\n    \nfor i in tqdm(test[\"file_path\"]):\n    test_ = pd.read_csv(f\"..\/input\/data-science-spring-osaka-hard-mode\/test_hard\/{i}\")\n    test_[\"file\"] = i\n    test_all = pd.concat([test_all,test_]).reset_index(drop=True)","340719df":"len_train = len(train_all)\ndf_all = pd.concat([train_all, test_all]).reset_index(drop=True)","28ca68fb":"sc = StandardScaler()\ncols = df_all.columns[1:-1]","6d7e5776":"df_all[cols] = sc.fit_transform(df_all[cols].values)","7b16bf6e":"train_all = df_all.iloc[:len_train].reset_index(drop=True)\ntest_all = df_all.iloc[len_train:].reset_index(drop=True)","9c395b0f":"train_all[\"file\"].value_counts()","6605e9ef":"test_all[\"file\"].value_counts()","c74df77d":"train_all[\"file\"].nunique()","e51aafd8":"train_seq = np.zeros([len(train),596,len(cols)])\ntest_seq = np.zeros([len(test),596,len(cols)])","b45a8930":"for n,f in enumerate(train[\"file_path\"]):\n    train_value = train_all[train_all.file == f].iloc[:,1:-1].values\n    train_seq[n,-len(train_value):,:] = train_value","2674d252":"for n,f in enumerate(test[\"file_path\"]):\n    if f == '\/test_hard\/0161.csv':\n        test_value = test_all[test_all.file == f].iloc[224:,1:-1].values\n    else:\n        test_value = test_all[test_all.file == f].iloc[:,1:-1].values\n    test_seq[n,-len(test_value):,:] = test_value","762bc180":"# ==========================\n# Settings\n# ==========================\nex = \"005\"\nSEED = 0\nN_SPLITS = 5\nSHUFFLE = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 32\nEPOCH = 10\n\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u5185\u306b\u5b58\u5728\u3059\u308b\u3001straight,hook,upper(&bodyupper),sway,jab,jab\u3068\u306e\u30b3\u30f3\u30d3\u30cd\u30fc\u30b7\u30e7\u30f3\u306e9\u30a2\u30af\u30b7\u30e7\u30f3\nLABEL_NUM=9","2fcb53e5":"# ==========================\n# Function\n# ==========================\n# ====================\n# Function\n# ====================\ndef process_data(data_seq):\n    # attention\u306emask\n    mask = data_seq[:,0] == 0\n    return {\n        'input_data_seq': data_seq,\n        \"mask\":mask\n    }\n\nclass DSPO_Dataset(Dataset):\n    \n    def __init__(self, data_seq, train = True, y = None):\n        self.data_seq = data_seq\n        self.train = train\n        self.y = y\n    \n    def __len__(self):\n        return len(self.data_seq)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.data_seq[item],\n            \n        )\n\n        # Return the processed data where the lists are converted to `torch.tensor`s\n        if self.train : \n            return {\n              'input_data_seq': torch.tensor(data[\"input_data_seq\"], dtype=torch.float32),\n              'mask': torch.tensor(data[\"mask\"], dtype=torch.bool),  \n              \"y\":torch.tensor(self.y[item], dtype=torch.float32)\n               }\n        else:\n            return {\n              'input_data_seq': torch.tensor(data[\"input_data_seq\"], dtype=torch.float32),\n              'mask': torch.tensor(data[\"mask\"], dtype=torch.bool), \n               }\n        \nclass Transformer_model(nn.Module):\n    def __init__(\n        self, dropout=0.2, con_size = 20, linear_emb1 = 240, dim_feedforward = 720, linear_emb2 = 100, nhead=4):\n        super(Transformer_model, self).__init__()\n        self.linear1 = nn.Sequential(\n            nn.Linear(con_size , linear_emb1),\n            nn.LayerNorm(linear_emb1 ),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        self.transformer_encoder = TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=linear_emb1, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout),\n                                                      num_layers=2)\n        \n        # dense\n        self.linear2 = nn.Sequential(\n            nn.Linear(linear_emb1, linear_emb2),\n            nn.LayerNorm(linear_emb2),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.linear_final = nn.Linear(linear_emb2,LABEL_NUM)\n\n    def forward(self, data_seq, mask):\n        data_seq = self.linear1(data_seq)\n        # pytorch\u306etransformer encoder\u306einput\u306eshape\u306b\u6c17\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\n        data_seq = data_seq.permute(1, 0, 2).contiguous()\n        output = self.transformer_encoder(data_seq, src_key_padding_mask=mask)\n        output = output.permute(1, 0, 2).contiguous()\n        output = torch.mean(output, 1)\n        output = self.linear2(output)\n        output = self.linear_final(output)\n        return output\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef sigmoid(value):\n    return 1 \/ (1 + np.exp(-value))","3aeace9c":"# ==========================\n# Main\n# ==========================\naction = pd.read_csv(ACTION_PATH)\naction_hard = pd.read_csv(ACTION_HARD_PATH)\nsub = pd.read_csv(SUB_PATH)","a898bbb9":"train[\"action_seq\"] = train[\"action_seq\"].str.replace('bodyupper', 'upper')","86e0b90b":"train[\"action_seq_str\"] = train[\"action_seq\"].str.split('-').tolist()","250ef4f2":"train","e296740e":"for i,l in enumerate(train[\"action_seq_str\"]):\n    train.iat[i, 2].append(l[0]+l[1])\n    train.iat[i, 2].pop(0)","7e94c314":"train","15a86d0a":"train[\"action_seq_str\"].value_counts()","7b77ebb9":"from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\ntarget = mlb.fit_transform(train[\"action_seq_str\"].values)\ntarget","6e622243":"mlb.classes_","8285e427":"y_oof = np.empty([len(train),LABEL_NUM])\ntest_preds = np.empty([len(test),LABEL_NUM])\ntest_ = DSPO_Dataset(test_seq, train = False, y = None)\ntest_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n#kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=SHUFFLE,random_state=SEED)\n#kf = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE,random_state=SEED)\nkf = MultilabelStratifiedKFold(n_splits=N_SPLITS, shuffle=SHUFFLE, random_state=SEED)\nseed_everything(SEED)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_seq,y = target)):\n    x_train_seq = train_seq[train_idx]\n    y_train = target[train_idx]\n    x_val_seq = train_seq[valid_idx]\n    y_val = target[valid_idx]\n    train_ = DSPO_Dataset(x_train_seq, train = True, y = y_train)\n    val_ = DSPO_Dataset(x_val_seq,train = True, y = y_val)\n    train_loader = DataLoader(dataset=train_, batch_size=BATCH_SIZE, shuffle = True , num_workers=2)\n    val_loader = DataLoader(dataset=val_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n    model = Transformer_model()\n    model = model.to(device)\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters,\n                      lr=1e-3,\n                      weight_decay=0.1,\n                      )\n    num_train_optimization_steps = int(len(train_loader) * EPOCH)\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=5,\n                                                num_training_steps=num_train_optimization_steps)\n    \n    criterion = nn.BCEWithLogitsLoss()\n    best_val = None\n    for epoch in tqdm(range(EPOCH)):\n        model.train() \n        train_losses_batch = []\n        val_losses_batch = []\n        epoch_loss = 0\n\n        # ==========================\n        # train\n        # ==========================\n        for d in train_loader:\n\n            # =========================\n            # data loader\n            # =========================\n\n            input_data_seq = d['input_data_seq']\n            mask = d[\"mask\"]\n            y = d[\"y\"]\n            input_data_seq = input_data_seq.to(device)\n            mask = mask.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n\n            output = model(input_data_seq,mask)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_losses_batch.append(loss.item())\n\n        train_loss = np.mean(train_losses_batch)\n        \n        # ==========================\n        # eval\n        # ==========================\n        model.eval()  # switch model to the evaluation mode\n        val_preds = np.ndarray((0,LABEL_NUM))\n        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n            # Predicting on validation set\n            for d in val_loader:\n                # =========================\n                # data loader\n                # =========================\n                input_data_seq = d['input_data_seq']\n                mask = d[\"mask\"]\n                y = d[\"y\"]\n                input_data_seq = input_data_seq.to(device)\n                mask = mask.to(device)\n                y = y.to(device)\n                output = model(input_data_seq,mask)\n\n                loss = criterion(output, y)\n                val_preds = np.concatenate([val_preds, output.detach().cpu().numpy()], axis=0)\n                val_losses_batch.append(loss.item())\n\n\n        val_loss = np.mean(val_losses_batch)\n        #acc = accuracy_score(np.argmax(y_val,axis=1), np.argmax(val_preds,axis=1))\n        acc = accuracy_score(y_val, np.round(sigmoid(val_preds)))\n        print(epoch, \"train loss:\", train_loss, \"val loss:\",val_loss, \"val acc:\",acc)\n        \n        if not best_val:\n            best_val = val_loss  # So any validation roc_auc we have is the best one for now\n            best_acc = acc\n            torch.save(model.state_dict(), f\"ex{ex}_{fold}.pth\")  # Saving the model\n            y_oof[valid_idx,:] = val_preds\n            continue\n\n        if val_loss <= best_val:\n            best_epoch = epoch\n            best_val = val_loss  # So any validation roc_auc we have is the best one for now\n            best_acc = acc\n            torch.save(model.state_dict(), f\"ex{ex}_{fold}.pth\")  # Saving the model\n            y_oof[valid_idx,:] = val_preds\n            \n    print(f\"{fold}_best_poch:{best_epoch},best_val:{best_val}, best_acc:{best_acc}\")\n    \n    # ===================================\n    # test\n    # ===================================\n    model = Transformer_model()\n    model.load_state_dict(torch.load(f\"ex{ex}_{fold}.pth\"))\n    model.to(device)\n    model.eval()\n    test_preds_ = np.ndarray((0,LABEL_NUM))\n    with torch.no_grad():  # Do not calculate gradient since we are only predicting\n        # Predicting on test set\n        for d in test_loader:\n            # =========================\n            # data loader\n            # =========================\n            input_data_seq = d['input_data_seq']\n            mask = d[\"mask\"]\n            input_data_seq = input_data_seq.to(device)\n            mask = mask.to(device)\n            output = model(input_data_seq,mask)\n\n            test_preds_ = np.concatenate([test_preds_, output.detach().cpu().numpy()], axis=0)\n\n    #torch.save(best_model.state_dict(), f\"..\/ex\/ex{ex}\/ex{ex}_{b}_{fold}.pth\")  # Saving the model\n    test_preds += test_preds_ \/  N_SPLITS\n#acc = accuracy_score(train[\"action_seq_num\"].values,np.argmax(y_oof,axis=1))\nacc = accuracy_score(target,np.round(sigmoid(y_oof)))\nprint(f\"cv:{acc}\")\nnp.save(f\"ex{ex}_test_pred.npy\",test_preds)\nnp.save(f\"ex{ex}_oof.npy\",y_oof)","5235f299":"test_preds_list = np.round(sigmoid(test_preds))","8a36f7f8":"test_preds_list=np.nan_to_num(test_preds_list)","f411d472":"test[\"action_list\"] = mlb.inverse_transform(test_preds_list)","b23784aa":"test[\"action_list\"].value_counts()","debdc8a7":"test","0f6e0ae6":"action_hard","dcfbd114":"test[\"action_seq\"]=''\nfor i,l in enumerate(test[\"action_list\"]):\n    if len(l)>9: #[secret]\u304c\u5206\u304b\u3089\u306a\u3044\u306e\u3067\u4e00\u65e6\u610f\u5473\u306e\u306a\u3044if\u6587 \u95be\u5024\u3092\u4e0b\u3052\u308b\u3068[secret]\u5019\u88dc\u304c\u51fa\u3066\u304f\u308b\n        test.iat[i, 2]='[secret]'\n    else:\n        if (('sway' in l) and ('upper' in l)) or (('jabsway' in l) and ('upper' in l)):\n            if 'jabstraight' in l:\n                test.iat[i, 2]='jab-straight-upper-ducking'\n            else:\n                test.iat[i, 2]='stepinjab-backstep-sway-upper'\n                \n        elif 'jabstraight' in l:\n            if 'upper' in l:\n                test.iat[i, 2]='jab-straight-upper-ducking'\n            elif 'jabjab' in l:\n                test.iat[i, 2]='jab-jab-jab-straight'\n            elif 'hook' in l:\n                test.iat[i, 2]='jab-straight-hook-bodyhook'\n            else:\n                test.iat[i, 2]='stepinjab-straight-backstep-straight'  \n\n        elif ('hook' in l) or ('jabhook' in l):\n            test.iat[i, 2]='jab-straight-hook-bodyhook'\n        elif 'upper' in l:\n            if 'straight' in l:\n                test.iat[i, 2]='jab-straight-upper-ducking'\n            else:\n                test.iat[i, 2]='jab-upper-ducking'\n\n        else:\n            if (l == ('jab', 'straight')) or l==('jab', 'jabjab', 'straight'):\n                test.iat[i, 2]='jab-jab-jab-straight'\n            else:\n                test.iat[i, 2]='jab-jab-jab-jab'","dded5a3b":"pd.set_option('display.max_rows', 200)\ntest[test['action_seq'] == 'jab-jab-jab-jab']","ef7425a8":"test[\"action_seq\"].value_counts()","2610b8f9":"test[['file_path','action_seq']].to_csv(f\"ex{ex}.csv\",index=False)","edc3b391":"### Transformer"}}