{"cell_type":{"f93144e1":"code","91451f9d":"code","ad743f23":"code","dff60f5c":"code","30e0d5f5":"code","3a07a48c":"code","66469ae4":"code","c0c983dc":"code","c01eb430":"code","37ed5455":"code","8dbda741":"code","3b3c5888":"code","a7ff3d22":"code","78cdc341":"code","7685d199":"code","f829d4ad":"code","8a865009":"code","caad33de":"code","b5ccd867":"code","968abbc7":"code","281a584b":"code","6bb71e2c":"code","2e913076":"code","b1d6e298":"code","c4110d10":"code","f71b36ae":"code","db4ae701":"code","e2777596":"code","dc6fdeb5":"code","87f6bfe5":"code","6e82770a":"code","496d2ad7":"code","2eda25c3":"code","a56ff2de":"code","c9b3eba3":"code","d460207e":"code","d20ec014":"code","e41c4c53":"code","6a361248":"code","e52dcc6e":"markdown","978b2b22":"markdown","06bbdc47":"markdown","dc5e6d41":"markdown","bf29cc28":"markdown","cd214170":"markdown","e8cc9f14":"markdown","da9e2004":"markdown","b7a06bb0":"markdown","70c41ea1":"markdown","00e38897":"markdown","7be721b8":"markdown","3cbea159":"markdown","ca7a3ba5":"markdown","a919a021":"markdown","a2fc5f8f":"markdown","e475b1c5":"markdown","b8672f42":"markdown","a42e3fe8":"markdown"},"source":{"f93144e1":"# Importing Libraries\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import permutations \n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import cross_val_score","91451f9d":"# Getting our data\n\ndata = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","ad743f23":"# Renaming the columns for better understanding of features (always a good thing to do)\n\ndata.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'serum_cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate',\n       'exercise_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']","dff60f5c":"# Lets see how our features correlate with the target variable \n\nx = data.corr()\npd.DataFrame(x['target']).sort_values(by='target',ascending = False).style.background_gradient(cmap = 'copper')","30e0d5f5":"# NOTE: This step is not necessary, I am performing it to avoid renaming of columns after One Hot Encoding. Also,\n# here we will leave two class variables as they are already one hot encoded\n\n# Let's map these class values to something meaningful information given in dataset description\n\n\ndata.chest_pain_type = data.chest_pain_type.map({1:'angina pectoris', 2:'atypical angina', 3:'non-anginal pain', 4:'SMI', 0:'absent'})\n\ndata.st_slope = data.st_slope.map({1:'upsloping', 2:'horizontal', 3:'downsloping', 0:'absent'})\n\ndata.thalassemia = data.thalassemia.map({1:'normal', 2:'fixed defect', 3:'reversable defect', 0:'absent'})","3a07a48c":"data.head()","66469ae4":"# Seperating out predictors(X) and target(Y)\n\nX = data.iloc[:, 0:13]\n\nY = data.iloc[:, -1]","c0c983dc":"X.head()","c01eb430":"# Encoding the categorical variables using get_dummies() \ncategorical_columns = ['chest_pain_type', 'thalassemia', 'st_slope']\n\nfor column in categorical_columns:\n    dummies = pd.get_dummies(X[column], drop_first = True)\n    X[dummies.columns] = dummies\n    X.drop(column, axis =1, inplace = True)","37ed5455":"X.head()","8dbda741":"# Let us again look at the correlation against our target variable\n\ntemp = X.copy()\ntemp['target'] = Y\n\nd = temp.corr()\npd.DataFrame(d['target']).sort_values(by='target',ascending = False).style.background_gradient(cmap = 'copper')","3b3c5888":"# Splitting the data into test and train \n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nprint(\"X-Train:\",X_train.shape)\nprint(\"X-Test:\",X_test.shape)\nprint(\"Y-Train:\",y_train.shape)\nprint(\"Y-Test:\",y_test.shape)","a7ff3d22":"# Scaling the continous data\n\nnum_columns =  ['resting_blood_pressure','serum_cholesterol', 'age', 'max_heart_rate', 'st_depression']\n\nscaler = StandardScaler()\n\nscaler.fit(X_train[num_columns])\n\nX_train[num_columns] = scaler.transform(X_train[num_columns])\n\nX_test[num_columns] = scaler.transform(X_test[num_columns])","78cdc341":"X_train.head()","7685d199":"# Creating a function to plot correlation matrix and roc_auc_curve\n\ndef show_metrics(model):\n    fig = plt.figure(figsize=(25, 10))\n\n    # Confusion matrix\n    fig.add_subplot(121)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"size\": 16}, fmt='g')\n\n    # ROC Curve\n    fig.add_subplot(122)\n    \n    \n    auc_roc = roc_auc_score(y_test, model.predict(X_test))\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n\n    plt.plot(fpr, tpr, color='darkorange', lw=2, marker='o', label='Trained Model (area = {0:0.3f})'.format(auc_roc))\n    plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--', label= 'No Skill (area = 0.500)')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()","f829d4ad":"# creating our model instance\nlog_reg = LogisticRegression()\n\n# fitting the model\nlog_reg.fit(X_train, y_train)\n\n# predicting the target vectors\ny_pred=log_reg.predict(X_test)","8a865009":"# calling our show_metrics function\n\nshow_metrics(log_reg)","caad33de":"# let's look at our accuracy\n\naccuracy = accuracy_score(y_pred, y_test)\n\nprint(f\"The accuracy on test set using Logistic Regression is: {np.round(accuracy, 3)*100.0}%\")","b5ccd867":"# Getting precision, recall and f1-score via classification report\n\nprint(classification_report(y_test, y_pred))","968abbc7":"from sklearn.neighbors import KNeighborsClassifier\n\n# creating a list of K's for performing KNN\nmy_list = list(range(0,30))\n\n# filtering out only the odd K values\nneighbors = list(filter(lambda x: x % 2 != 0, my_list))\n\n# list to hold the cv scores\ncv_scores = []\n\n# perform 10-fold cross validation with default weights\nfor k in neighbors:\n  Knn = KNeighborsClassifier(n_neighbors = k, algorithm = 'brute')\n  scores = cross_val_score(Knn, X_train, y_train, cv=10, scoring='accuracy', n_jobs = -1)\n  cv_scores.append(scores.mean())\n\n# finding the optimal k\noptimal_k = neighbors[cv_scores.index(max(cv_scores))]\nprint(\"The optimal K value is with default weight parameter: \", optimal_k)","281a584b":"# plotting accuracy vs K\nplt.plot(neighbors, cv_scores)\nplt.xlabel(\"Number of Neighbors K\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs K Plot for normal \")\nplt.grid()\nplt.show()\n\nprint(\"Accuracy scores for each K value is : \", np.round(cv_scores, 3))","6bb71e2c":"# Finding the accuracy of KNN with optimal K\n\nfrom sklearn.metrics import accuracy_score\n\n# create instance of classifier\nknn_optimal = KNeighborsClassifier(n_neighbors = optimal_k, algorithm = 'kd_tree', \n                                   n_jobs = -1)\n\n# fit the model\nknn_optimal.fit(X_train, y_train)\n\n# predict on test vector\ny_pred = knn_optimal.predict(X_test)\n\n# evaluate accuracy score\naccuracy = accuracy_score(y_test, y_pred)*100\nprint(f\"The accuracy on test set using KNN for optimal K = {optimal_k} is {np.round(accuracy, 3)}%\")","2e913076":"# calling our show_metrics function\n\nshow_metrics(knn_optimal)","b1d6e298":"# Creating an instance of the classifier\nsvm = SVC()\n\n# training on train data\nsvm.fit(X_train, y_train)\n\n# predicting on test data\ny_pred = svm.predict(X_test)\n\n# let's look at our accuracy\naccuracy = accuracy_score(y_pred, y_test)\n\nprint(f\"The accuracy on test set using SVC is: {np.round(accuracy, 3)*100.0}%\")","c4110d10":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(random_grid)","f71b36ae":"# Use the random grid to search for best hyperparameters\n\n# First create the base model to tune\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train, y_train)","db4ae701":"# Lets look at the best parameters\n\nrf_random.best_params_","e2777596":"# Creating an instance for the classifier\nrf_best = RandomForestClassifier(**rf_random.best_params_)\n\n# fitting the model\nrf_best.fit(X_train, y_train)\n\n# predict the labels\ny_pred = rf_best.predict(X_test)","dc6fdeb5":"accuracy = accuracy_score(y_pred, y_test)\n\nprint(f\"The accuracy on test set using RandomForest is: {np.round(accuracy, 3)*100.0}%\")","87f6bfe5":"# calling our show_metrics function\n\nshow_metrics(rf_best)","6e82770a":"# creating a list of our models\nensembles = [log_reg, knn_optimal, rf_best, svm]\n\n# Train each of the model\nfor estimator in ensembles:\n    print(\"Training the\", estimator)\n    estimator.fit(X_train,y_train)","496d2ad7":"# Find the scores of each estimator\nscores = [estimator.score(X_test, y_test) for estimator in ensembles]\n\nscores","2eda25c3":"# Lets define our estimators in a list\n\nnamed_estimators = [\n    (\"log_reg\",log_reg),\n    ('random_forest', rf_best),\n    ('svm',svm),\n    ('knn', knn_optimal),\n]","a56ff2de":"# Creating an instance for our Voting classifier\n\nvoting_clf = VotingClassifier(named_estimators)","c9b3eba3":"# Fit the classifier\n\nvoting_clf.fit(X_train,y_train)","d460207e":"# Let's look at our accuracy\nacc = voting_clf.score(X_test,y_test)\n\nprint(f\"The accuracy on test set using voting classifier is {np.round(acc, 3)*100}%\")","d20ec014":"# to generate permutations of length three\nperm = permutations(named_estimators, 3) \n\n# to store the acc and classifiers\nbest_perm = []\n\n# to store best classifier\nbest=[]\n\n# Traverse through the obtained permutations\nfor i in list(perm):\n    \n    # fit the classifier\n    voting_clf = VotingClassifier(i)\n    voting_clf.fit(X_train,y_train)\n    \n    # obtain accracy score and append it to the list\n    acc = voting_clf.score(X_test,y_test)\n    best_perm.append([acc,voting_clf])\n    \n\n# find out the maximum accuracy\nmaximum = max(best_perm, key = lambda x:x[0])\n              \n# there can be multiple permutations for which we get \n# best score so find all of them and append to best\nfor i in range(len(best_perm)):\n    if maximum[0]==best_perm[i][0]:\n        best.append(best_perm[i][1])","e41c4c53":"# Using the best score permutations\n\nacc_scores = []\n\nfor i in range(len(best)):\n    \n    voting_clf = best[i]\n    \n    # fit the classifier\n    voting_clf.fit(X_train,y_train)\n    \n    # Let's look at our accuracy\n    acc_scores.append(voting_clf.score(X_test,y_test))","6a361248":"print(f\"The accuracy on test set using voting classifier is {np.round(max(acc_scores), 3)*100}%\")","e52dcc6e":"# Modelling","978b2b22":"**We got our optimal value of K as 27 so now let's look at how our accuracy varied across all K's we tried during KFoldCV. This is sometimes important because if our Hyperparameter is giving same accuracy for a wide range of values so it is better to do early stopping and use the lower value which will avoid Underfitting or Overfitting.** \n\n**In our case, KNN would underfit for large values of K**","06bbdc47":"## Support Vector Machine","dc5e6d41":"**Note that I have passed drop_first as True in get_dummies function to avoid dummy variable trap. It is not very necessary in models where you apply L1 or L2 regularization as it will balance any collinearity caused by dummy columns.**\n","bf29cc28":"**Woah, 90% on the first try!**\n\n**Logistic Regression seems to work very well as it is easily able to draw hyperplane and the reason for that can be - Patients with higher ages usually have the problem of high_blood_pressure thus which make it easy to seperate it from the low age patients. (This Speculation might be wrong!)**","cd214170":"# THANKS FOR READING!","e8cc9f14":"### Logistic Regression","da9e2004":"**No, Improvement at all!**\n\n**Lets generate all permutations of the names_estimators list of length 3 and then check if the do any better.**","b7a06bb0":"### **~85 is not very good but isn't bad either!**","70c41ea1":"**SVM did a good job predicting the labels, scoring ~89%**","00e38897":"**As we saw in EDA that chest_pain_type and max_heart_rate have decent positive correlation with our target. Now these numbers say the same thing!** ","7be721b8":"### Ensemble Models - Voting Classifier\n\n**It is always good to use esemble models beacuse they are very effective. Here, I will be using Voting Classifier which does exaclty what it sounds like. It takes mulitple models and combine them together and do voting on the prediction and picks the best for us.**\n\n**Let's combine all the models together!**","3cbea159":"### **This is an extension to my previous kernel on this data. In the previous kernel, I did Rigorous EDA which you can find [here](https:\/\/www.kaggle.com\/nishkarshtripathi\/unleashing-the-power-of-eda).**\n\n### **In this kernel I will go through the preprocessing and prediction part. So, let's start!**","ca7a3ba5":"**Looks like we got our best K. Now, lets fit our training data using this K and find out the accuracy on test.**","a919a021":"### Still No Improvement - Is this the best on this data?\n\n> #### **TASKS FOR YOU :**\n\n**(I) Try some feature engineering or some more models and let me know in comments whether you can achieve higher than this.**<br><br>\n**(II) Provide suggestion, improvements and Criticism! (if any) :)**<br><br>\n**(III) Please do Upvote if you want to see more content like this!**","a2fc5f8f":"### **We have far better correlations after Scaling and One hot encoding.**","e475b1c5":"### KNearestNeighbors","b8672f42":"# Preprocessing","a42e3fe8":"### Ensemble Models - Random Forest Classifier\n\n**Trees are very easy to overfit so we will first do a RandomSearchCV to find the best parameters.**"}}