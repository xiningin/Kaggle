{"cell_type":{"0536089b":"code","6d4f2ce9":"code","6151f480":"code","a5ec0ea0":"code","73669cf6":"code","290a0ffe":"code","45237ead":"code","9cbba24c":"code","893f9c55":"code","5761ad3d":"code","0175626f":"code","c5ac44cc":"code","6b9ce233":"code","faa8a6cd":"code","9c9e9ac4":"code","01e0496b":"code","07d59529":"code","cb3842a4":"code","a433fa01":"code","8201ba51":"code","1019cfcb":"code","fcc77309":"markdown","fcb254d4":"markdown","b3309933":"markdown","6ffcbb7e":"markdown","a835f6d9":"markdown","05c88f9c":"markdown","95b285b4":"markdown","fb76f84c":"markdown","3528bb3b":"markdown","5c6108a1":"markdown","18f7281b":"markdown","c53061ad":"markdown","c6626d0f":"markdown","8b94490c":"markdown","afd10209":"markdown","558e60ca":"markdown","7d247422":"markdown","bcaf9d5a":"markdown","6f4c8759":"markdown","39721336":"markdown","07c9b65b":"markdown","f4cbb042":"markdown","cae2055f":"markdown","d85d3b71":"markdown"},"source":{"0536089b":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport plotly.express as px\nfrom plotly import graph_objects as go\n\ndataset = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ndataset.head()\n\n\ndef plot_training(history):\n    # Creating the plotly figure object\n    fig = go.Figure()\n    # Adding accuracy line\n    fig.add_trace(\n        go.Scatter(\n            x=np.arange(len(history.history[\"loss\"])),\n            y=history.history[\"loss\"],\n            text=np.arange(len(history.history[\"loss\"])),\n            mode=\"lines\",\n            name=\"loss\",\n        )\n    )\n    # Adding val_accuracy line\n    fig.add_trace(\n        go.Scatter(\n            x=np.arange(len(history.history[\"val_loss\"])),\n            text=np.arange(len(history.history[\"val_loss\"])),\n            y=history.history[\"val_loss\"],\n            mode=\"lines\",\n            name=\"val_loss\",\n        )\n    )\n    # Formating the graph\n    fig.update_layout(title=\"NN training curve plot.\", xaxis_title=\"Epochs\", yaxis_title=\"Score\", title_font_size=24)\n    # Showing the image\n    fig.show()\n","6d4f2ce9":"print(\"Dataset shape\", dataset.shape)\ndataset.isna().sum()","6151f480":"print(\"How many unique values?\", len(dataset.id.unique()))\ndataset.id.value_counts()[:5]","a5ec0ea0":"print(\"How many missing values?\", dataset.url_legal.isna().sum())\ndataset.url_legal.sample(3)","73669cf6":"print(\"How many missing values?\", dataset.license.isna().sum())\ndataset.license.value_counts()","290a0ffe":"print(\"How many missing values?\", dataset.excerpt.isna().sum())\nprint(\"How many unique values?\", len(dataset.excerpt.unique()))\ndataset[\"excerpt_len\"] = dataset.excerpt.apply(len)\nprint(\"How big or how small is the texts?\")\nfig = px.box(dataset, y=\"excerpt_len\", width=300, title=\"Boxpot excerpt_len column.\")\nfig.show()","45237ead":"dataset.excerpt.iloc[0]","9cbba24c":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\ndef wordcloud(df, column_name, title):\n    all_words = \" \".join([text for text in df[column_name]])\n    wordcloud = WordCloud(\n        width=800, height=500, max_font_size=80, collocations=False\n    ).generate(all_words)\n    plt.figure(figsize=(24, 12))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n\n\nwordcloud(dataset, \"excerpt\", \"Wordcloud for excerpt column.\")","893f9c55":"dataset.target.describe()","5761ad3d":"fig = px.histogram(dataset, x=\"target\", width=600, title=\"Boxpot target column.\")\nfig.show()","0175626f":"dataset.standard_error.describe()","c5ac44cc":"fig = px.box(\n    dataset, y=\"standard_error\", width=300, title=\"Boxpot standard_error column.\"\n)\nfig.show()","6b9ce233":"with sns.axes_style(\"white\"):\n    table = dataset.corr()\n    mask = np.zeros_like(table)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(5, 5))\n    sns.heatmap(\n        table,\n        cmap=\"Blues\",\n        mask=mask,\n        vmax=0.3,\n        linewidths=0.5,\n        annot=True,\n        annot_kws={\"size\": 15},\n    )","faa8a6cd":"import string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem import SnowballStemmer\n\n\nstemmer = nltk.SnowballStemmer(\"english\")\nstopwords = list(stopwords.words(\"english\"))\npunctuation = [word for word in string.punctuation]\npunctuation += [\"...\", \"  \", \"\\n\"]\n\n\ndef remove_punctuation(serie, stopwords):\n    aux = list()\n    for el in serie:\n        for word in stopwords:\n            el = el.replace(word, \" \")\n        aux.append(el)\n    return aux\n\n\ndef remove_stopwords(serie, stopwords):\n    tokenizer = nltk.WordPunctTokenizer()\n\n    result_serie = list()\n    for row in serie:\n        aux = list()\n        text_row = tokenizer.tokenize(row.lower())\n        for word in text_row:\n            if word not in stopwords:  # stopwords\n                aux.append(stemmer.stem(word))\n        result_serie.append(\" \".join(aux))\n    return result_serie\n\n\ndataset[\"excerpt_cleaned\"] = dataset.excerpt.str.lower()\ndataset[\"excerpt_cleaned\"] = remove_stopwords(dataset.excerpt_cleaned, punctuation)\ndataset[\"excerpt_cleaned\"] = remove_stopwords(dataset.excerpt_cleaned, stopwords)\ndataset[\"excerpt_cleaned\"]","9c9e9ac4":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nvectorize = TfidfVectorizer(min_df=50,  max_features=None, \n            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n            stop_words = 'english')\n\nohe = OneHotEncoder()\nmms_X = StandardScaler()\nmms_y = MinMaxScaler()\n\ndataset = dataset[['license','excerpt_len','excerpt_cleaned', 'target']]\ndataset['license'] = dataset.license.fillna(\"Not Known\")\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset.drop('target',axis=1), dataset.target , test_size=0.10, random_state=42\n)\n\ndef feature_transformation(X_raw, y_raw,  vectorize, ohe, mms_X, mms_y, fit=True):\n    if fit:\n        ohe.fit(X_raw.license.values.reshape(-1, 1))\n        mms_X.fit(X_raw.excerpt_len.values.reshape(-1, 1))\n        vectorize.fit(X_raw[\"excerpt_cleaned\"])\n        mms_y.fit(y_raw.values.reshape(-1,1))\n            \n    X = np.concatenate(\n        (\n            ohe.transform(X_raw.license.values.reshape(-1, 1)).todense(),\n            mms_X.transform(X_raw.excerpt_len.values.reshape(-1, 1)),\n            vectorize.transform(X_raw[\"excerpt_cleaned\"]).todense(),\n        ),\n        axis=1,\n    )\n#     y = mms_y.transform(y_raw.values.reshape(-1,1))\n    y = y_raw.values.reshape(-1,1)\n    return X,y\n\nX_train, y_train = feature_transformation(X_train, y_train, vectorize, ohe, mms_X, mms_y,  fit=True)\nX_test, y_test = feature_transformation(X_test, y_test,vectorize, ohe, mms_X, mms_y,  fit=False)\n\nprint(\"How many features (bag of words): \", len(vectorize.get_feature_names()))\nprint(\"Dataset shape: \", X_train.shape)","01e0496b":"\naux = pd.DataFrame(y_train)\naux.columns = ['Y_value']\n\nfig = px.histogram(aux, x=\"Y_value\", nbins=50)\nfig.show()","07d59529":"from sklearn.metrics import r2_score\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (Dense, Dropout,Conv1D,GlobalMaxPooling1D, Embedding, BatchNormalization)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import RMSprop,Adam\n\nINPUT_SIZE = X_train.shape[1]\ncallback = EarlyStopping(monitor=\"loss\", patience=5)\n\ndef plot_history(history):\n    acc = history.history['mae']\n    val_acc = history.history['val_mae']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training mae')\n    plt.plot(x, val_acc, 'r', label='Validation mae')\n    plt.title('Training and validation mae')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    \ndef baseline_model(neurons=1000, hidden_lenght=10, learning_rate=0.0001, dropout=0.2):\n\n    model = Sequential()\n    model.add(Dense(neurons, input_dim=INPUT_SIZE, activation='relu'))\n    model.add(Dropout(dropout))\n    \n    for _ in range(1,hidden_lenght + 1):\n        model.add(Dense(neurons, activation='relu'))\n        model.add(Dropout(dropout))\n        \n    model.add(Dense(1))\n    \n    optimizer = Adam(lr=learning_rate)\n    model.compile(loss='mean_squared_error',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n\n    return model","cb3842a4":"model = baseline_model(\n    neurons=1000,\n    hidden_lenght=5,\n    learning_rate=0.0001,\n    dropout=0.4\n)\nhistory = model.fit(\n    X_train, y_train, batch_size=5,\n    validation_data=(X_test, y_test),\n    epochs=100,\n    verbose=1,\n    callbacks=[callback],\n)\nprint('R2 score:',r2_score(y_test, model.predict(X_test)))\nplot_training(history)","a433fa01":"submission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission[\"excerpt_cleaned\"] = submission.excerpt.str.lower()\nsubmission[\"excerpt_cleaned\"] = remove_stopwords(\n    submission.excerpt_cleaned, punctuation\n)\nsubmission[\"excerpt_cleaned\"] = remove_stopwords(submission.excerpt_cleaned, stopwords)\nsubmission[\"excerpt_len\"] = submission.excerpt.apply(len)","8201ba51":"X_sub = np.concatenate(\n    (\n        ohe.transform(\n            submission.license.fillna(\"Not Known\").values.reshape(-1, 1)\n        ).todense(),\n        mms_X.transform(submission.excerpt_len.values.reshape(-1, 1)),\n        vectorize.transform(submission[\"excerpt_cleaned\"]).todense(),\n\n    ),\n    axis=1,\n)","1019cfcb":"# submission[\"target\"] = mms_y.inverse_transform(model.predict(X_sub))\nsubmission[\"target\"] = model.predict(X_sub)\nsubmission[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","fcc77309":"The smaller text has 669 chars, and the bigger has 1341 chars.\n\nJust an example...","fcb254d4":"# Submission\n\nLoading the submision dataset.","b3309933":"# EDA\n\n## First part\n\nStarting the first exploratory data analyses using the columns present in the dataset. \n\n### Files\n- train.csv - the training set\n- test.csv - the test set\n- sample_submission.csv - a sample submission file in the correct format\n### Columns\n- id - unique ID for excerpt\n- url_legal - URL of source - this is blank in the test set.\n- license - license of source material - this is blank in the test set.\n- excerpt - text to predict reading ease of\n- target - reading ease\n- standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\n[Source](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/data)","6ffcbb7e":"### There is relation between the text size and dificulty to read? ","a835f6d9":"# NLP\n\n- 1 - Remove stopwords and ponctuation.\n- 2 - TFDIF (word frequency).\n- 3 - Regression.\n- 4 - XAI - SHAP.","05c88f9c":"### url_legal\nLooks like the link that you can find the text...","95b285b4":"Only one ID per line.","fb76f84c":"### excerpt","3528bb3b":"## id\nJust an ID with a hash with a lenght of 8 chars.","5c6108a1":"### standard_error","18f7281b":"Formating the submission dataset.","c53061ad":"In the Data descriptions says the licensing information is provided for the public test set (because the associated excerpts are available for display \/ use), the hidden private test set includes only blank license \/ legal information.\n","c6626d0f":"### Remove stowords and ponctuation","8b94490c":"## The Kernel\n\nThis kernel is an approach to solve the competition problem [CommonLit Readability Prize](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize).\n\nThe problem could be represented by the following paragraph: \n\n\"Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.\"","afd10209":"### TFDIF (word frequency) and feature transformation","558e60ca":"Traning the model","7d247422":"### target\nReading ease information.","bcaf9d5a":"### Regression\n\n\nRegression using neural network and target scalded with values between 0~1.","6f4c8759":"##  Y target transformed","39721336":"There is a considerable inverse correlation between target and excerpt lenght!","07c9b65b":"Continuous values that could be negative, zero or positive. A perfect gaussian distribution.","f4cbb042":"### wordcloud (without process)\nJust a word cloud plot to identify the most common words in the dataset.","cae2055f":"Prediction the new dataset.","d85d3b71":"### license"}}