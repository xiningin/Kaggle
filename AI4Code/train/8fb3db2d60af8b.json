{"cell_type":{"4c12d8c9":"code","8a3b2195":"code","b0f310f1":"code","a0086158":"code","f16510f3":"code","97799626":"code","c7fe473f":"code","483beea2":"code","4dd77d10":"code","31cd2a1a":"code","8b795253":"code","e2363dec":"code","a6b520b7":"code","20f0d2d1":"code","cf833eb7":"code","7416d3fe":"markdown"},"source":{"4c12d8c9":"import numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport math\nfrom torch import nn as nn\nimport torch.nn.functional as F\nimport matplotlib.gridspec as gridspec\nfrom sklearn.metrics import accuracy_score","8a3b2195":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ny = train['label']\ntrain.drop('label', axis=1, inplace=True)","b0f310f1":"(train.isnull().any()).any() # there is no Nan values","a0086158":"def show_pic(position=False):\n    fig = plt.figure()\n    if not position: position = np.random.randint(0, len(train)-1)\n    fig.suptitle(f\"This is {y[position]}\")\n    plt.imshow(train.loc[position,:].values.reshape(28,28), cmap='binary')\n\nshow_pic()","f16510f3":"X_train, X_val, y_train, y_val = train_test_split(train, y, test_size=0.15, random_state=2, shuffle=True)\nX_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","97799626":"X_test","c7fe473f":"inp = torch.from_numpy(train.loc[0,:].values).float().reshape(28, 28) # (n_samples, channels, height, width)\nconv2d = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0)\nmp2d = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\nout1 = conv2d(inp.reshape(1,1,28,28))\nout2 = nn.Sequential(conv2d, nn.ReLU(), mp2d)(inp.reshape(1,1,28,28))\nout1.shape, out2.shape","483beea2":"# Plot figure with subplots of different sizes\nfig = plt.figure(1)\nfig.suptitle('After 2D convolution neural network layer', fontsize=16, y=1.1)\n# set up subplot grid\nOy = 4\nOx = 13\ngridspec.GridSpec(Oy, Ox) # Oy, Ox\n\n# original subplot\nplt.subplot2grid((Oy, Ox), (0,0), colspan=Oy, rowspan=Oy)\nplt.title('original')\nplt.imshow(inp, cmap='binary')\n\nj = 0\nk = 0\nfor i in range(len(out1[0])):\n    if not j%Oy:\n        j = 0\n        k += 1\n    plt.subplot2grid((Oy, Ox), (j,Oy+k))\n    plt.title(f'filter \u2116 {i}')\n    plt.imshow(out1[0][i].detach().numpy(), cmap='binary')\n    j+=1\n        \n\nfig.set_size_inches(w=17,h=5)\nfig.tight_layout()","4dd77d10":"# Plot figure with subplots of different sizes\nfig = plt.figure(1)\nfig.suptitle('After 2D convolution and max pooling neural network layers', fontsize=16, y=1.1)\n# set up subplot grid\nOy = 4\nOx = 13\ngridspec.GridSpec(Oy, Ox) # Oy, Ox\n\n# original subplot\nplt.subplot2grid((Oy, Ox), (0,0), colspan=Oy, rowspan=Oy)\nplt.title('original')\nplt.imshow(inp, cmap='binary')\n\nj = 0\nk = 0\nfor i in range(len(out2[0])):\n    if not j%Oy:\n        j = 0\n        k += 1\n    plt.subplot2grid((Oy, Ox), (j,Oy+k))\n    plt.title(f'filter \u2116 {i}')\n    plt.imshow(out2[0][i].detach().numpy(), cmap='binary')\n    j+=1\n        \n\nfig.set_size_inches(w=17,h=5)\nfig.tight_layout()","31cd2a1a":"class Net(nn.Module):\n    def __init__(self, n_chanels):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_chanels, kernel_size=3, stride=1, padding=0) # Wout - 1 = (Win - kernel_size + 2*padding) \/ stride\n        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=n_chanels, out_channels=n_chanels*2, kernel_size=3, stride=1, padding=0)\n        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n        self.fc1 = nn.Linear(n_chanels*2*7*7, 1024)\n        self.bn = nn.BatchNorm1d(1024)\n        self.fc2 = nn.Linear(1024, 10)\n    def forward(self, x):\n        x = x.reshape(len(x),1,28,28) # (n_samples, n_channels, height, width)\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.mp1(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.mp2(x).reshape(x.size(0), -1)\n        x = F.dropout(x)\n        x = self.fc1(x)\n        x = self.bn(x)\n        x = self.fc2(x)\n        return x","8b795253":"torch.manual_seed(2) # for reproducibility\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nnum_epochs = 10\nnum_classes = 10\nbatch_size = 128\nstart_learning_rate = 1e-1\nn_chanels = 64\nEPOCHS = 50\n\n\n    \ninputs = torch.from_numpy(X_train.values).float()\ntargets = torch.from_numpy(y_train.values)\n\ntrain_ds = TensorDataset(inputs, targets)\nvalid_ds = TensorDataset(torch.from_numpy(X_val.values).float(), torch.from_numpy(y_val.values))\nfull_ds = TensorDataset(torch.from_numpy(train.values).float(), torch.from_numpy(y.values))\n\ntrain_dl = DataLoader(dataset = train_ds, batch_size = batch_size, shuffle=True, num_workers=0)\nvalid_dl = DataLoader(dataset = valid_ds, batch_size = batch_size, shuffle=True, num_workers=0)\nfull_dl = DataLoader(dataset = full_ds, batch_size = batch_size, shuffle=True, num_workers=0)\n\n\nnet = Net(n_chanels=n_chanels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr = start_learning_rate)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, eps=1e-11, verbose=True)\n\n\n\n\n\ntrain_history = [] # list of all mean errors train dataset by epochs \nval_history = [] # list of all mean errors validation dataset by epochs\nfor epoch in range(1, EPOCHS+1):\n    net.train() # torch train mode (for gradient calculation)\n    epoch_losses = 0 # sum of all errors by epoch for subsequent mean calculation\n    for batch in train_dl:\n        x,y_t = batch\n        output = net(x.to(device))\n        loss = criterion(output, y_t.to(device))\n#         print(torch.argmax(output, dim=1))\n        optimizer.zero_grad() # resetting the gradient for subsequent gradient calculation\n        loss.backward() # gradient calculation by backpropagation\n        optimizer.step() # updates the parameters of neural network\n        epoch_losses += loss.item() # sum of errors in current batch\n    train_history.append(epoch_losses\/len(train_dl))\n    \n    net.eval() # torch evaluation mode (for no gradient calculation)\n    with torch.no_grad():\n        epoch_losses = 0\n        for batch in valid_dl:\n            x,y_v = batch\n            output_v = net(x.to(device))\n            loss = criterion(output_v, y_v.to(device))\n            epoch_losses += loss.item()\n        val_history.append(epoch_losses\/len(valid_dl))\n    \n    scheduler.step(val_history[-1])\n    print(f'Epoch {epoch}\/{EPOCHS}:'.ljust(12, ' '), \n          f'loss: {train_history[-1]}'.ljust(30, ' '), \n          f'val_loss: {val_history[-1]}'.ljust(30, ' '), \n          f'--> Accuracy on validation set: {round(accuracy_score(y_v.cpu(), torch.argmax(output_v, dim=1).cpu())*100, 4)} %'.ljust(30, ' '), \n          sep=' ', end = '\\n')","e2363dec":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(22, 4))\nfig.suptitle(f'EPOCHS={EPOCHS}, n_chanels={n_chanels}, start_learning_rate={start_learning_rate}, batch_size={batch_size}', fontsize=16, y=1.02)\nax[0].plot(train_history, alpha=0.8, label='Training loss')\nax[0].plot(val_history, alpha=0.8, label='Validation loss')\nax[0].set_title('Losses')\nax[1].hist((y_t-torch.argmax(output.cpu(), dim=1)).detach().numpy(), alpha=0.8)\nax[1].hist((y_v-torch.argmax(output_v.cpu(), dim=1)).detach().numpy(), alpha=0.8)\nax[1].set_title('Distribution of errors');","a6b520b7":"net.eval()\nwith torch.no_grad():\n    y_test_pred = torch.argmax(net(torch.from_numpy(X_test.values).float().to(device)), dim=1)\n    \ndef show_pic_from_test(position=False):\n    fig = plt.figure()\n    if not position: position = np.random.randint(0, len(X_test)-1)\n    fig.suptitle(f'I predicted meaning of that strange picture, and I think it is \"{y_test_pred[position]}\"')\n    plt.imshow(X_test.loc[position,:].values.reshape(28,28), cmap='binary')","20f0d2d1":"show_pic_from_test()","cf833eb7":"output = pd.DataFrame({'ImageId': pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv').ImageId, \n                       'Label': y_test_pred.cpu()})\noutput.to_csv('submission_torch.csv', index=False)\noutput","7416d3fe":"# How do neural network layers work?"}}