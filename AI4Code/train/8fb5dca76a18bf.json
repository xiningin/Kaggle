{"cell_type":{"e9a52715":"code","f45927c5":"code","d04ebd0d":"code","2ee56d74":"code","acf6186a":"code","b9440a82":"code","4fcb4cd7":"code","9d746c89":"code","e4d5c03e":"code","1a80662b":"code","4c70d7e1":"code","f6d0b87e":"markdown","9e78e758":"markdown","6432bf97":"markdown","fdfa665b":"markdown","bdbb565b":"markdown","f311ca65":"markdown","a8a62842":"markdown","3339a98e":"markdown","1cc1f115":"markdown"},"source":{"e9a52715":"# For file manipulation\nimport os\n\n# For data manipulation\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# For our CNN model\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D","f45927c5":"# Loading the train and test data\nx_train = np.load('..\/input\/reducing-image-sizes-to-32x32\/X_train.npy')\nx_test = np.load('..\/input\/reducing-image-sizes-to-32x32\/X_test.npy')\ny_train = np.load('..\/input\/reducing-image-sizes-to-32x32\/y_train.npy')","d04ebd0d":"# Preprocessing the image data\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255.\nx_test \/= 255.","2ee56d74":"# Defining the required variables\nbatch_size = 64\nnum_classes = 14\nepochs = 30\nval_split = 0.1\ninput_shape=x_train.shape[1:]","acf6186a":"def baseline_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    #model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(Conv2D(64, (3, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    #model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    \n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(num_classes))\n    model.add(Activation('softmax'))\n    \n    return model","b9440a82":"model = baseline_model()","4fcb4cd7":"# Compiling the model\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)","9d746c89":"# Training the model\nhist = model.fit(\n    x_train, \n    y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=val_split,\n    shuffle=True\n)","e4d5c03e":"history = hist.history\n\nfig, ax = plt.subplots(2)\n\nax[0].plot(history['acc'])\nax[0].plot(history['val_acc'])\nax[0].legend(['training accuracy', 'validation accuracy'])\n\nax[1].plot(history['loss'])\nax[1].plot(history['val_loss'])\nax[1].legend(['training loss', 'validation loss'])\n\nfor axs in ax.flat:\n    axs.label_outer()","1a80662b":"y_test = model.predict(x_test)\n\nsubmission_df = pd.read_csv('..\/input\/iwildcam-2019-fgvc6\/sample_submission.csv')\nsubmission_df['Predicted'] = y_test.argmax(axis=1)\nprint(submission_df.shape)\nsubmission_df.head()","4c70d7e1":"submission_df.to_csv('submission.csv',index=False)\n# history_df.to_csv('history.csv', index=False)\n\n# with open('history.json', 'w') as f:\n#     json.dump(hist.history, f)","f6d0b87e":"### 2. Import dataset\n\nSince, the data is too large and will take a lot of time I used data from another awesome [kernel](https:\/\/www.kaggle.com\/xhlulu\/reducing-image-sizes-to-32x32) by [xhlulu](https:\/\/www.kaggle.com\/xhlulu). You can simply click on the \"Add Dataset\" on the top to add the data to your kernel.","9e78e758":"### 5. Make predictions and submission","6432bf97":"### 3. Create and train model","fdfa665b":"In this kernel we will create a CNN using keras to categorize the images of wild animals. These images are captured using WildCams. These WildCams collect images in large quantities which then are used by biologists to monitor the biodiversity and population density of animals.","bdbb565b":"We will use Keras to create a CNN and then we will train it on our training data. There are many good architectures out there that we can use. These architectures can give you much better accuracy on both train and validation set. Here, we will use a simple architecture.","f311ca65":"### 4. Analyse the results","a8a62842":"### 1. Import libraries","3339a98e":"# iWildCam-2019\n### Categorize animals in wild","1cc1f115":"We will follow the steps below in this kernel:\n1. Import libraries\n2. Import dataset\n3. Create and train model\n4. Analyse the results\n5. Make predictions and submission\n\nLet's get started"}}