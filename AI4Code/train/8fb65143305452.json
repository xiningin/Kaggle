{"cell_type":{"99e730f3":"code","661111b8":"code","c5ec9be5":"code","7a675268":"code","7cedfed0":"code","f11e1fe9":"code","16bfea2d":"code","edbcee72":"code","9473a02a":"code","0ec3f5e7":"code","a810d0bb":"code","377cc29b":"code","74ac5d80":"code","42fa6e16":"code","dec5734a":"code","ea1fd6f7":"code","7067c7a8":"code","9c6b5a38":"code","006a4031":"code","49a48544":"code","92598362":"code","3c1cd3c4":"code","2a67dc4e":"code","43977c0f":"code","9cc2d462":"code","8f722091":"code","8307cf96":"code","14d01e7f":"code","f1b74c31":"code","ae116c1e":"code","07beaef2":"code","1d292995":"code","2cfa84d0":"code","9cbeb085":"code","5a4ea056":"code","d32940df":"code","e59c5380":"code","64b91bdf":"code","13be64ed":"code","66102728":"code","d8b57e0b":"code","158249f7":"code","27c723ad":"code","1028e4e7":"code","4aa7e69e":"code","da3f7b0f":"code","510d4974":"code","ef69313b":"code","e200e9d5":"code","783c3bb7":"code","6f81a64a":"markdown","b884e3be":"markdown","925e184d":"markdown","f210a951":"markdown","01d8bf97":"markdown","bf52d34a":"markdown","3ec35e79":"markdown","a29bedbe":"markdown","49e63ac7":"markdown","fb8a8546":"markdown","8f23e235":"markdown","d597d8cf":"markdown","0b0eaf09":"markdown","98a3722d":"markdown","a9c93f14":"markdown","a953e322":"markdown","81f0bd9d":"markdown","daf48a61":"markdown","2a0ea422":"markdown","9bd227f2":"markdown","cf54072b":"markdown","1e9a0bdc":"markdown","f49267a2":"markdown","be5fbc81":"markdown","03b4359b":"markdown","06bd4a8d":"markdown","35541b4e":"markdown","5ec0db83":"markdown","0d6fc2b1":"markdown","30a6fbd1":"markdown","f6e3ac74":"markdown","31110014":"markdown"},"source":{"99e730f3":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","661111b8":"train=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")","c5ec9be5":"train.shape,test.shape","7a675268":"train.head()","7cedfed0":"train.dtypes.value_counts()","f11e1fe9":"train.select_dtypes(include=['object']).head()","16bfea2d":"test['Target']=0\n\ntrain['is_train']=1\n\ntest['is_train']=0\n#del train2\n\ntrain2=train.copy()\ntrain2=train2.append(test,ignore_index=True)\n","edbcee72":"train2['is_train'].value_counts()","9473a02a":"train2.columns[train2.isnull().any()]","0ec3f5e7":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","a810d0bb":"# Missing values statistics\nmissing_values = missing_values_table(train2)\nmissing_values.head(20)","377cc29b":"del train2['rez_esc']\ndel train2['v18q1']\ndel train2['v2a1']\n","74ac5d80":"\ntrain2['meaneduc'].fillna((train2['meaneduc'].mean()), inplace=True)\ntrain2['SQBmeaned'].fillna((train2['SQBmeaned'].mean()), inplace=True)","42fa6e16":"dependency=pd.DataFrame()\ndependency['dependency']=train2['dependency'].loc[train2['dependency']!=('yes')]\ndependency['dependency']=dependency['dependency'].loc[dependency['dependency']!=('no')]\n","dec5734a":"dependency['dependency'].astype('float64').mean()","ea1fd6f7":"train2.loc[train2['dependency']=='yes','dependency'] = 1.59\ntrain2.loc[train2['dependency']=='no','dependency'] = 0","7067c7a8":"train2['dependency']=train2['dependency'].astype('float64')","9c6b5a38":"dependency['edjefe']=train2['edjefe'].loc[train2['edjefe']!=('yes')]\ndependency['edjefe']=dependency['edjefe'].loc[dependency['edjefe']!=('no')]\ndependency['edjefe'].astype('float64').mean()\n","006a4031":"train2.loc[train2['edjefe']=='yes','edjefe'] = 8.54\ntrain2.loc[train2['edjefe']=='no','edjefe'] = 0","49a48544":"train2['edjefe']=train2['edjefe'].astype('float64')","92598362":"dependency['edjefa']=train2['edjefa'].loc[train2['edjefa']!=('yes')]\ndependency['edjefa']=dependency['edjefa'].loc[dependency['edjefa']!=('no')]\ndependency['edjefa'].astype('float64').mean()","3c1cd3c4":"train2.loc[train2['edjefa']=='yes','edjefa'] = 8.47\ntrain2.loc[train2['edjefa']=='no','edjefa'] = 0","2a67dc4e":"train2['edjefa']=train2['edjefa'].astype('float64')","43977c0f":"train2.select_dtypes(include=['object']).head()","9cc2d462":"train2.dtypes.value_counts()","8f722091":"train=train2.loc[train2['is_train']==1]\ntest=train2.loc[train2['is_train']==0]\n","8307cf96":"train.shape,test.shape","14d01e7f":"train['Target'].astype('int64').plot.hist()","f1b74c31":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\ntrain['Target'].value_counts(normalize=True).plot(ax=ax, kind='bar')","ae116c1e":"train.describe()","07beaef2":"from sklearn.ensemble import  RandomForestClassifier\nfrom sklearn.metrics import  f1_score\n\ndef f1_macro(y_true, y_pred): return f1_score(y_true, y_pred, average='macro')","1d292995":"def print_score(m):\n    res = [f1_macro(m.predict(X_train), y_train), f1_macro(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","2cfa84d0":"from sklearn.model_selection import train_test_split\ndf=train.copy()\ny=df['Target']\ndel df['Target']\ndel df['Id']\ndel df['idhogar']\nX_train, X_valid, y_train, y_valid = train_test_split(\n df, y, test_size=0.33, random_state=42)","9cbeb085":"m =RandomForestClassifier(random_state=2,n_jobs=-1,criterion=\"gini\" )","5a4ea056":"\n%time m.fit(X_train, y_train)\nprint_score(m)","d32940df":"feature_importances = pd.DataFrame(m.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)","e59c5380":"#Which are the top 10 factors influencing the vulnerability of a family\nfeature_importances.head(10)","64b91bdf":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"ticks\", color_codes=True)\n","13be64ed":"grid = sns.FacetGrid(train, col='Target', margin_titles=True)\ngrid.map(plt.hist,'meaneduc',normed=True, bins=np.linspace(0, 40, 15));","66102728":"grid = sns.FacetGrid(train, col='Target', margin_titles=True)\ngrid.map(plt.hist,'SQBmeaned',normed=True, bins=np.linspace(0, 40, 15));","d8b57e0b":"grid = sns.FacetGrid(train, col='Target', margin_titles=True)\ngrid.map(plt.hist,'dependency',normed=True, bins=np.linspace(0, 40, 15));","158249f7":"train2=train.copy()\n\n#First lets try to get a mobiles used per person which should be highly predictive of financial well being\ntrain2['Tot_persons']=train2['overcrowding']*train2['bedrooms']\ntrain2['mob_perperson']=train2['qmobilephone']\/train2['Tot_persons'] #This has a higher correlation than individual variables it uses\n#Can we merge both the Education and overcrowding together?\n\ntrain['Edu_crwd_ratio']=train['meaneduc']\/train['overcrowding']","27c723ad":"\ndata = train2[['Target','Edu_crwd_ratio','Tot_persons','mob_perperson', 'dependency', 'SQBmeaned', 'meaneduc','qmobilephone','overcrowding','SQBhogar_nin','edjefe','escolari','SQBovercrowding']]\ndata_corrs = data.corr()\ndata_corrs","1028e4e7":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","4aa7e69e":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['dependency', 'SQBmeaned', 'meaneduc']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(train.loc[train['Target'] == 1, source], label = 'target == 1')\n    # plot loans that were not repaid\n    sns.kdeplot(train.loc[train['Target'] == 2, source], label = 'target == 2')\n    \n    sns.kdeplot(train.loc[train['Target'] == 3, source], label = 'target == 3')\n    \n    sns.kdeplot(train.loc[train['Target'] == 4, source], label = 'target == 4')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","da3f7b0f":"test.head()","510d4974":"del test['Target']\ntest_df=test.copy()\ndel test_df['Id']\ndel test_df['idhogar']\nTarget=m.predict(test_df)","ef69313b":"test.tail()","e200e9d5":"pd.options.mode.chained_assignment = None\ntest['Target'] = Target\n","783c3bb7":"test[['Id', 'Target']].to_csv('submission.csv', index= False)","6f81a64a":"Before we do that Lets once quicky Glance at the dataset that we have to spot any anomalies","b884e3be":"**Factor-2**-Family size(or number of members in a household who are dependent and are not capable of earning bread for themselves).This looks intuitive as if a family has more members to be taken care compared to members who are earning and providing lower is the possibility of them becoming prosperous.","925e184d":"We could not find anything offbeat per se,but even if we are missing something we can always return back here.Let's see what the Random forest trees have to say about this","f210a951":"**2.SQBmeaned**-square of the mean years of education of adults (>=18) in the household\n\nThis variable is also related to education but only for adults(>=18 years)","01d8bf97":"**1. mean education**-average years of education for adults (18+)\nThis kind of makes sense education of adults in household governs their employability  which in turn governs their income\n\nLet us have a look at how mean education is distributed with respect to the poverty levels","bf52d34a":"We observe that that for non vulnerable(Target=4) there are higher proportion of people in higher mean education bracket(i.e greater than 10) while that is lower for other outcomes","3ec35e79":"**Feature Engineering:**Lets try and create some logical ratios","a29bedbe":"Lets read the training and test data both as pandas dataframe","49e63ac7":"We should replace all non numeric values in dependency\/edjefe\/edjefa as:\n\nif no=0\nif yes=median\n\n\n","fb8a8546":"We have id-which is the ID per row and idhogar which is id per household more over we have\ndependency ,edjefe and edjefa.Now lets look at the three variables one by one","8f23e235":"**Lets start with EDA of the variables**","d597d8cf":"Lets look at the variables which random forest has deemed to be important","0b0eaf09":"**Factor-1**-Education,There are many variables here which indicate the importance of education  in impacting the poverty vulnerability of households.There is ~34% correlation between mean education and the poverty level of a household","98a3722d":"**We start with Target**\n\n1 = extreme poverty \n2 = moderate poverty \n3 = vulnerable households \n4 = non vulnerable households","a9c93f14":"**3.dependency**-Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64)\n\nThis should indicate that does having more individuals in your family who are not bread earners but dependent impact the poverty levels.","a953e322":"First lets look at the five object data types that exist","81f0bd9d":"**dependency**-Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64","daf48a61":"**How many missing do we have across all the fields?**","2a0ea422":"**`What we observe is ~60% of households have been classified as non vulnerable while ~5% are close to extreme poverty**","9bd227f2":"Lets find out which variables does random forest feel are important ","cf54072b":"**Lets try and submit the predictions of the Random forest we created**","1e9a0bdc":"**What are the major factors that emerge by looking at top 10 important variables?**","f49267a2":"For  rez_esc,v18q1 and v2a1 we have greater than 70% missing.We can drop there three.For meanedu and SQBmeaned we replace them my the means of their respective columns","be5fbc81":"Lets try this plot in a better way so that we are able to find the percentage split","03b4359b":"Lets replace yes\/no from the categorical(object) variables we identified previously (dependency ,edjefe and edjefa)","06bd4a8d":"**Costa Rican Household Poverty Level Prediction**\n**Can you identify which households have the highest need for social welfare assistance?**\n\nWe start with looking at what are the files available.We have:\n*  train.csv\n* test.csv \n* sample_submission.csv","35541b4e":"**Exploratory Data Analysis**\n\nFind what data types exist in training data","5ec0db83":"We should try and look and some of the most important factors which can help us identify the vulnerabilty of households.Going through ~140 variables may not be the most optimized use of our time .Lets use the amazingly used and abused  random forest for our advantage","0d6fc2b1":"**One good way to quantify these relationships will be a correlation matrix**","30a6fbd1":"Lets combine the train and test set so that we could clean up the variables","f6e3ac74":"It does not look that profound but non vulnerable families seem to have lower number of dependents ","31110014":"Now Lets seperate train and test so that we go forward with EDA"}}