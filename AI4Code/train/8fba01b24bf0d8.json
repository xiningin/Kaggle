{"cell_type":{"3173d022":"code","35ca3528":"code","011f6733":"code","b487a292":"code","333bbbe2":"code","894eb5c6":"code","5bf21ad5":"code","c71ebb7b":"code","f2a19a24":"code","7f7c42f1":"code","25938f46":"code","0e5c71e9":"code","3be6c72c":"code","bec53073":"markdown","b350f1bc":"markdown","e527e30d":"markdown","4ccc3c81":"markdown","7caca185":"markdown","2e2c09ef":"markdown","cb60ef66":"markdown","5baed585":"markdown","40880c4b":"markdown","7f36a237":"markdown","f1b1a885":"markdown","c409e2ea":"markdown","3b8101b5":"markdown"},"source":{"3173d022":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35ca3528":"#lets import tensorflow\nimport tensorflow as tf\nprint(tf.__version__)","011f6733":"import tensorflow_datasets as tfds\nimdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)","b487a292":"import numpy as np\n\ntrain_data, test_data = imdb['train'], imdb['test']\n\ntraining_sentences = []\ntraining_labels = []\n\ntesting_sentences = []\ntesting_labels = []","333bbbe2":"for s,l in train_data:\n  training_sentences.append(s.numpy().decode('utf8'))\n  training_labels.append(l.numpy())\n  \nfor s,l in test_data:\n  testing_sentences.append(s.numpy().decode('utf8'))\n  testing_labels.append(l.numpy())\n  \ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)","894eb5c6":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\noov_tok = \"<OOV>\"","5bf21ad5":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n#lets tokenize each word from data\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(training_sentences)\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n#pad function is use to ensure every batch has same length\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)","c71ebb7b":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\nprint(decode_review(padded[3]))\nprint(training_sentences[3])","f2a19a24":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","7f7c42f1":"num_epochs = 5\nmodel.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))","25938f46":"model_weights = model.layers[0]\nweights = model_weights.get_weights()[0]\nprint(weights.shape) # shape: (vocab_size, embedding_dim)","0e5c71e9":"import io\n\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\nfor word_num in range(1, vocab_size):\n  word = reverse_word_index[word_num]\n  embeddings = weights[word_num]\n  out_m.write(word + \"\\n\")\n  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","3be6c72c":"sentence = \"I really think shifat is honest.\"\nsequence = tokenizer.texts_to_sequences([sentence])\nprint(sequence)","bec53073":"## lets get the weights and dimension","b350f1bc":"# training model","e527e30d":"# lets build keras model","4ccc3c81":"## lets put train and test data into blank lists we created above.. the data are being putted in list because we have to supply continuous words from sentences which are embedded, tokenized.","7caca185":"## lets import imdb dataset from tensorflow","2e2c09ef":"# now the fun part","cb60ef66":"# we can save the vectors and save as tsv. Later you can use the files to plot a 3-D binary graph here: https:\/\/projector.tensorflow.org\/","5baed585":"## Now we have to build our tokenizer using tensorflow.keras tokenizer function","40880c4b":"# Lets check our model with a custom sentence","7f36a237":"## lets see how is looking our data after tokenization and pading","f1b1a885":"## lets define soem parameters","c409e2ea":"## lets split dataset into train and test. Also we have to create blank lists to contain data and labels","3b8101b5":"# we will learn NLP using embedding, vectors, tokenization techniques which are basic and mandatory for text type NLP projects. I will try to explain in a simple ways."}}