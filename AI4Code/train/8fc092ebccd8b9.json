{"cell_type":{"d140ceb2":"code","2d34c124":"code","0c7686ee":"code","ab3f98ff":"code","21c54ea1":"code","4f9a7820":"code","d31fad20":"code","343dfe61":"code","17402b3c":"code","602abba2":"code","3432755b":"code","bd54b0b6":"code","3614db7f":"code","03d988c9":"code","61a9e88a":"code","e4fe7b85":"code","49f114a3":"code","8f75cc50":"code","4180ecfb":"code","3ad58d98":"markdown","8b2b6234":"markdown","74079bda":"markdown","773d61a1":"markdown","18b223bb":"markdown","ac68773d":"markdown","3c352abf":"markdown","277529d2":"markdown","9b590860":"markdown","5a5cf409":"markdown","8223eba5":"markdown","6d3b3a02":"markdown","e2eb22d3":"markdown","a7fb667c":"markdown"},"source":{"d140ceb2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression","2d34c124":"df1= pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf2= pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","0c7686ee":"print(df1.head())\nprint(df2.head())\nprint(df1.dtypes)\nprint(df2.dtypes)","ab3f98ff":"# summary of numerical variable\nprint(df1.describe())\nprint(df2.describe())\n# summary of categorial variable\nprint(df1.describe(include=\"object\"))\nprint(df2.describe(include=\"object\"))\n\nprint(df1.info())\nprint(df2.info())","21c54ea1":"# checking null values\nprint(df1.isnull().sum())\nprint(df2.isnull().sum())","4f9a7820":"# age_mean, fair mean\nmean_age1 = df1[\"Age\"].mean()\nmean_age2 = df2[\"Age\"].mean()\nmean_fare = df2[\"Fare\"].mean()\n","d31fad20":"#remove null values\ndf1[\"Age\"].replace(np.nan, mean_age1, inplace=True)\ndf2[\"Age\"].replace(np.nan, mean_age1, inplace=True)\ndf2[\"Fare\"].replace(np.nan, mean_fare, inplace=True)","343dfe61":"# drop cabin column\ndf1.drop(columns=[\"Cabin\", \"Name\"], axis=1, inplace=True)\ndf2.drop(columns=[\"Cabin\", \"Name\"], axis=1, inplace=True)","17402b3c":"#print(df1[\"Embarked\"].value_counts())\n#drop 2 rows of missing Embarked values and reset index\ndf1.dropna(subset=[\"Embarked\"], axis=0, inplace=True)\ndf1.reset_index(drop=True, inplace=True)","602abba2":"# correlation\ncorrelation= df1.corr()\nprint(correlation)","3432755b":"# again checking null values\nprint(df1.isnull().sum())\nprint(df2.isnull().sum())\n\nprint(df1.info())","bd54b0b6":"# plot\na = sns.countplot(df1[\"Survived\"])\nplt.show(a)\n\nb = sns.countplot(x= \"Survived\",data=df1, hue=\"Sex\")\nplt.show(b)\n\nc= sns.countplot(x=\"Survived\", data=df1, hue=\"Embarked\")\nplt.show(c)\n\nd= sns.countplot(x=\"Survived\", data=df1, hue=\"Pclass\")\nplt.show(d)\n\ne= sns.boxplot(x=\"SibSp\", y= \"Survived\", data=df1)\nplt.show(e)\ne= sns.boxplot(x=\"Parch\", y= \"Survived\", data=df1)\nplt.show(e)\na= sns.distplot(df1[\"Age\"])\nplt.show(a)","3614db7f":"# change categorical to int for df1 sex, embarked and ans will be same if we donot choose drop first\nembarked1=pd.get_dummies(df1[\"Embarked\"], drop_first=True)\nsex1=pd.get_dummies(df1[\"Sex\"], drop_first=True)\n\ndf1=pd.concat([df1, embarked1, sex1],axis=1)\n# correlation of between passengerid, age and survived is small so dropping\ndf1.drop(columns=[\"Sex\", \"Embarked\", \"Ticket\"], axis=1, inplace=True)\n\n# change categorical to int for df2\nembarked1=pd.get_dummies(df2[\"Embarked\"], drop_first=True)\nsex1=pd.get_dummies(df2[\"Sex\"], drop_first=True)\n\ndf2=pd.concat([df2, embarked1, sex1],axis=1)\ndf2.drop(columns=[\"Sex\", \"Embarked\", \"Ticket\"], axis=1, inplace=True)","03d988c9":"# checking data\nprint(df1.head())\nprint(df2.head())\nprint(df1.corr())","61a9e88a":"train_x= df1.drop(columns=[\"Survived\"], axis=1)\ntrain_y= df1[\"Survived\"]\n\ntest_x= df2.copy()\n\nlogistic = LogisticRegression()\nlogistic.fit(train_x, train_y)\n# logistic.coef_\n# logistic.intercept_\nprediction_y= logistic.predict(test_x)\n#print(prediction_y)","e4fe7b85":"# R^2 value\nprint(logistic.score(train_x, train_y))","49f114a3":"from sklearn.neighbors import KNeighborsClassifier\n\n#sorting k nearest neighbours classifier\nKNN_classifier = KNeighborsClassifier(n_neighbors=3)\nKNN_classifier.fit(train_x, train_y)\nprediction_y = KNN_classifier.predict(test_x)\nprint(KNN_classifier.score(train_x, train_y))","8f75cc50":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(train_x, train_y)\nprediction_y = model.predict(test_x)\nprint(model.score(train_x, train_y))","4180ecfb":"submission= pd.DataFrame({\"PassengerId\": df2[\"PassengerId\"], \"Survived\": prediction_y})\nsubmission.to_csv(\"Titanic_data_solution.csv \", index=False)\nprint(\"Your submission was successfully saved!\")","3ad58d98":"## 3.2 Importing data","8b2b6234":"## 6.2 Method 2 - KNN","74079bda":"## 3.1 Required packages","773d61a1":"## Titanic - Machine Learning from Disaster\n### Author: Kavita Khandelwal\n### Date: 09\/22\/2021\n\n\n## 1. Introduction\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\n## 2. Ask\n## 2.1 Business Task\nIn this challenge, we need to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","18b223bb":"## 6.1 Method 1 - Logistic regression","ac68773d":"## 6.3 Method 3 - Random forest","3c352abf":"## 4. Process\n##  4.1 Data cleaning","277529d2":"## 5. Data analysis","9b590860":"## 3.3 Getting to know data","5a5cf409":"## 6. Classification Methods\n* Removing insignificant variables","8223eba5":"## 3. Prepare\nThis is a Titanic-Disaster historical dataset. The data has been split into two groups:\n* training set (train.csv)\n* test set (test.csv)\nYou can download the training dataset [here](https:\/\/www.kaggle.com\/c\/titanic\/data?select=train.csv) and test dataset [here](https:\/\/www.kaggle.com\/c\/titanic\/data?select=test.csv). \n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n","6d3b3a02":"## 5.2 Visualization","e2eb22d3":"## 7. Submission","a7fb667c":"## 5.1 Correlation"}}