{"cell_type":{"2c1465e7":"code","fab6e368":"code","ee2459ac":"code","403430e5":"code","43269cd6":"code","ae8b67f7":"code","e79e1ec4":"code","75dd9bfc":"code","3aa301ae":"code","726e6609":"code","22b11b86":"code","be1d30d6":"code","ee86691b":"code","72759196":"code","939879ff":"code","77110223":"code","474df1de":"code","03a1edec":"code","1539abfa":"code","6cce80ce":"code","e5370fe1":"code","48e41a97":"code","e647d488":"code","893a0b68":"code","eac1adfc":"code","49cd4211":"code","478c4764":"code","e0322c6e":"code","eb73d621":"code","a78610ab":"code","8ecd600f":"code","ae9f2cb0":"code","565509ff":"code","f20e07a0":"code","e0f08136":"code","cd316722":"code","f8684cd6":"code","072a4baa":"code","26853e11":"code","ca8ac2fb":"code","cc3a5dfe":"code","f37ca3be":"code","0950ef67":"code","847fe20b":"code","8cc6405b":"code","25b22a7e":"code","eb1059ef":"code","fcf77846":"code","b12bb9cf":"code","6ff3982d":"code","e3a2e7b8":"code","bf9fa5a6":"code","d3b661ea":"code","ecab3243":"code","ec511827":"code","1244f086":"code","73d22740":"code","c322db9b":"code","5fc7f574":"code","a1669b8a":"code","170ee8db":"markdown","4df248cb":"markdown","068a88b1":"markdown","27102b9a":"markdown","edd9f22e":"markdown","0999178c":"markdown","a6eafc5f":"markdown","f390a5af":"markdown","5cfa2848":"markdown","40f25c16":"markdown","eadded07":"markdown","51eeaa45":"markdown","eaf972bd":"markdown","a8aa4cbe":"markdown","8a736b2b":"markdown","f5da649c":"markdown","611db1ac":"markdown","629d9ba7":"markdown","d4d29cd9":"markdown","9ca7c515":"markdown","578d78d1":"markdown","12c17113":"markdown","70c1d48c":"markdown","fbdcba6e":"markdown","42b067c3":"markdown","869fe11e":"markdown","9540905f":"markdown","6612bb0b":"markdown","10520a31":"markdown","d74a7576":"markdown"},"source":{"2c1465e7":"#!pip install --upgrade pip\n!pip install transformers\n!pip install torch\n!pip install pyshorteners\n#!pip install xlrd\n#!pip install wikipedia\n!pip install vaderSentiment","fab6e368":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\n\nimport urllib.request\nimport re\n    \n    \nimport datetime\n\n#UrlLib for http handlings\nfrom   bs4 import BeautifulSoup\nimport bs4 as BeautifulSoup\nimport urllib.request\nfrom urllib.request import urlopen \nfrom socket import timeout\n\n\n#WordCloud - not used in this version.\n#from os import path\n#from PIL import Image\n#from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n#For the NLP  \nimport nltk\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n#For the Tf \nimport tensorflow as tf\nimport torch\nimport json \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, pipeline\nfrom transformers import TransfoXLTokenizer, TransfoXLModel\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport torch\n#Wikipedia\n#import wikipedia\n\n#TinyUrl\nimport pyshorteners\n\nimport gc\n\nimport time","ee2459ac":"start_time = time.time()","403430e5":"links = pd.read_excel('..\/input\/newsletterdata\/NewsLetter_Links.xlsx',encoding='Latin\u20131')\n#links    = pd.read_csv(\"..\/input\/SRWPwAI.csv\",encoding='utf8' )\nlinks['to_summarize'] = 0\nlinks['article_text'] = \"\"\nlinks","43269cd6":"#Sorting by date\nlinks=links.sort_values(by=\"Title\")","ae8b67f7":"links","e79e1ec4":"links = links[links['Url'].notna()]\n\nlinks.shape","75dd9bfc":"# Resetting the dataframe index\nlinks.reset_index(inplace=True)","3aa301ae":"# Dropping the Index and creating a column called \"number\" as index\nlinks.drop(columns=\"index\",inplace=True)\nlinks.rename(columns={\"level_0\":\"number\"},inplace=True)","726e6609":"links.dropna(inplace=True)\nlinks.drop_duplicates(subset =\"Url\",keep = False, inplace = True) \nlinks.shape","22b11b86":"#final version of the Dataframe\nlinks","be1d30d6":"# Resetting the dataframe index\nlinks.reset_index(inplace=True)\n\n# Dropping the Index and creating a column called \"number\" as index\nlinks.drop(columns=\"index\",inplace=True)\nlinks.rename(columns={\"level_0\":\"number\"},inplace=True)","ee86691b":"links.shape","72759196":"links = links[links['Url'].notna()]\nlinks.shape","939879ff":"links = links[links['Title'].notna()]\nlinks.shape","77110223":"links.dropna(inplace=True)\n#links.drop_duplicates(subset =\"MediaURL\",keep = False, inplace = True) \nlinks.drop_duplicates(subset =\"Url\",keep = False, inplace = True) \nlinks.shape","474df1de":"# Resetting the dataframe index\nlinks.reset_index(inplace=True)","03a1edec":"user_agent = \"Mozilla\/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/70.0.3538.77 Safari\/537.36\"\nmy_headers = {'User-Agent': user_agent, 'Accept': 'text\/html,application\/xhtml+xml,application\/xml;q=0.9,*\/*;q=0.8'}\n\nvalues = {'name': 'Michael Foord',\n          'location': 'Northampton',\n          'language': 'Sweden' }","1539abfa":"# import module sys to get the type of exception\nimport sys\nimport urllib.request\nfrom urllib.request import urlopen \nfrom socket import timeout\nfrom urllib.request import build_opener, HTTPCookieProcessor, Request\n\nn = int(links[\"Url\"].count())\n#n =3 # to keep this short I reduce the number of articles to 3 - remove this limitation if you want to summarize everything.\n\nprint(\"Checking \" + str(n)+\" article's links for errors accessing the source websites...\")\nprint()\ni=0\n\nfor i in range(n):\n    \n    print(\"Checking access to article number: \"+ str(i) + \" - \" + str(links[\"Title\"][i]) + \".\")\n    req = urllib.request.Request(links[\"Url\"][i], headers=my_headers)\n    try: \n        urllib.request.urlopen(req,timeout=1000)\n        print(\"Access OK\")\n        links['to_summarize'][i] = 1\n    except Exception as e:\n        print(\"Oops!\", e.__class__, \"occurred.\")\n        print()\nprint(\"All the Links were verified.\")","6cce80ce":"links","e5370fe1":"import random\nfrom random import seed\nfrom random import sample\nfrom numpy.random import shuffle\n# seed random number generator\nseed(1)\n# prepare a sequence\nsequence = [i for i in range(n)]\n#print(sequence)\nshuffle(sequence)\n# select a subset without replacement\nsubset = sample(sequence, 1)\n#print(subset)\n\n\nnumberList = subset\nprint(\"random article to be tested: \", random.choice(numberList))\n\nurl_test = random.choice(numberList)\n#url_test","48e41a97":"d = links\ni = url_test","e647d488":"import pyshorteners\n\ns = pyshorteners.Shortener()\nprint(s.tinyurl.short(links[\"Url\"][url_test]))","893a0b68":"def text_extract(d,i):\n    \n    # Data collection from the links using web scraping(using Urllib library)\n    links_url = d['Url'].tolist()\n    links_url = links_url[i]\n    #links_url = test_url\n\n    req = urllib.request.Request(links[\"Url\"][i], headers=my_headers)\n    text = urllib.request.urlopen(req, timeout=100)\n    \n    summary = ''\n    link_summary = text.read()\n    \n    \n    # Parsing the URL content \n    link_parsed = BeautifulSoup.BeautifulSoup(link_summary,'html.parser')\n    \n    # Returning <p> tags\n    paragraphs = link_parsed.find_all('p')\n    \n    # To get the content within all paragrphs loop through it\n    link_content = ''\n    for p in paragraphs:  \n        link_content += p.text\n    \n    # Removing Square Brackets and Extra Spaces\n    link_content = re.sub(r'\\[[0-9]*\\]', ' ', link_content)\n    link_content = re.sub(r'\\s+', ' ', link_content)\n    \n    # Removing special characters and digits\n    formatted_article_text = re.sub('[^a-zA-Z]', ' ', link_content )\n    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n    \n    text = formatted_article_text\n    #text_temp = text.read()\n    global webtext\n    preprocess_text = text.strip().replace(\"\\n\",\"\")\n    #webtext = preprocess_text[0:512]\n    webtext = preprocess_text\n    links['article_text'][i] = webtext\n    return webtext","eac1adfc":"device    = torch.device('cpu')\nmodel     = T5ForConditionalGeneration.from_pretrained('t5-large')\ntokenizer = T5Tokenizer.from_pretrained('t5-large')\n\n#tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n#model = TFTransfoXLModel.from_pretrained('transfo-xl-wt103')\n\n\ndef summarization_infer(text, max=512):\n  preprocess_text = text.replace(\"\\n\", \" \").strip()\n  t5_prepared_Text = preprocess_text\n  tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\")\n\n  summary_ids = model.generate(tokenized_text, min_length=100, max_length=max, top_k=100, top_p=0.8, early_stopping=False, maxfeatures=100, num_beams=3,no_repeat_ngram_size=2) #top-k top-p sampling strategy\n  \n  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n  #end_time = time.time()\n  #print (f'Time taken : {end_time-start_time}')\n  return output\n\ndef translation_infer(text, max=50):\n  preprocess_text = text.replace(\"\\n\", \" \").strip()\n  t5_prepared_Text = \"translate English to German: \"+preprocess_text\n  tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\")\n\n  translation_ids = model.generate(tokenized_text, min_length=10, max_length=50, early_stopping=True, num_beams=2)\n  output = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n  #end_time = time.time()\n  #print (f'Time taken : {end_time-start_time}')\n  return output\n\ndef grammatical_acceptibility_infer(text):\n  preprocess_text = text.replace(\"\\n\", \" \").strip()\n  t5_prepared_Text = \"cola sentence: \"+preprocess_text\n  tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\")\n\n  grammar_ids = model.generate(tokenized_text, min_length=1, max_length=3)\n  output = tokenizer.decode(grammar_ids[0], skip_special_tokens=True)\n  #end_time = time.time()\n  #print (f'Time taken : {end_time-start_time}')\n  return output.capitalize()\n\ndef summarization_XL(text,max_len):\n  #tokenizer = TransfoXLTokenizer.from_pretrained('t5-large')\n  model     = T5ForConditionalGeneration.from_pretrained('t5-large')\n\n\n  #tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n  tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n  #model = TFTransfoXLModel.from_pretrained('transfo-xl-wt103')\n    \n  sequence_a = text\n  encoded_sequence_a = tokenizer.encode(sequence_a)\n\n  # Continuation of the previous script\n  sequence_a_dict = tokenizer.encode_plus(sequence_a,  pad_to_max_length=True)\n\n  sequence_a_dict['input_ids'] \n  sequence_a_dict['attention_mask']   \n    \n  input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]  # Batch size 1\n  output = model(input_ids,attention_mask)\n  last_hidden_states, mems = output[:2]\n  return output","49cd4211":"def reduce_mem_usage(d, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = d.memory_usage().sum() \/ 1024**2\n    for col in d.columns:\n        col_type = d[col].dtypes\n        if col_type in numerics:\n            c_min = d[col].min()\n            c_max = d[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    d[col] = d[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    d[col] = d[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    d[col] = d[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    d[col] = d[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    d[col] = d[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    d[col] = d[col].astype(np.float32)\n                else:\n                    d[col] = d[col].astype(np.float64)\n    import gc\n    gc.collect()\n    end_mem = d.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))","478c4764":"reduce_mem_usage(d, verbose=True)","e0322c6e":"#summarization_pipeline = pipeline(task='summarization', model=\"t5-large\")","eb73d621":"def clean_summarized_text(output_text):\n    \n    s = str(output_text)\n    s = s.replace(\"[{'summary_text': '\", '')\n    s = s.replace(\"'}]\", '.')\n    s = s.replace(\"na en a-ena re-a-a n aen .a oa\", '')\n    s = s.replace(\"aa na as re n a\", '')\n    s = s.replace(\"en re-a\", '')\n    s = s.replace(\"a<extra_id_27>\", '')\n    s = s.replace(\"The a aa . \", '')\n    s = s.replace(\" .\", '.')\n    s = s.replace(\"  .\", '.')\n    s = s.replace(\"  .  \", '.')\n    \n    s = s.replace(\"\\n\\n\", '')\n    s = s.replace(\" + iWork\", '')\n    \n    s = s.replace(\"[{'generated_text':\", '')\n     \n    \n    \n    \n    \n    clean_output = s.capitalize()\n    return clean_output","a78610ab":"i=0\nn = int(links[\"Url\"].count())\n#n = 2\nprint(\"Save \"+str(n) +\" articles to the dataframe.\")\nprint(\"____________________________________________________________\")\nfor i in range(n):\n            \n    \n    #published = links[\"added\"][i]\n    to_summarize = links[\"to_summarize\"][i]\n    if to_summarize == 1:\n            print(\" \")\n            print(str(i)+ \") \" + links[\"Title\"][i] + \".\")\n            print(\" \")\n            text_extract(d,i)\n            \n            gc.collect()\n            i=i+1","8ecd600f":"links","ae9f2cb0":"def summarization_links_t5(d,i):\n    t5_model     = T5ForConditionalGeneration.from_pretrained('t5-large')\n    t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')    \n    \n# Data collection from the links using web scraping(using Urllib library)\n    links_url = d['Url'].tolist()\n    links_url = links_url[i]\n    #links_url = test_url\n    \n    req = urllib.request.Request(links[\"Url\"][i], headers=my_headers)\n    text = urllib.request.urlopen(req)\n    \n    summary = ''\n    link_summary = text.read()\n    #link_summary = link_summary[0:512]\n    \n    \n    \n    # Parsing the URL content \n    link_parsed = BeautifulSoup.BeautifulSoup(link_summary,'lxml')\n    \n    # Returning <p> tags\n    paragraphs = link_parsed.find_all('p')\n    \n    # To get the content within all paragrphs loop through it\n    link_content = ''\n    for p in paragraphs:  \n        link_content += p.text\n    \n    # Removing Square Brackets and Extra Spaces\n    link_content = re.sub(r'\\[[0-9]*\\]', ' ', link_content)\n    link_content = re.sub(r'\\s+', ' ', link_content)\n    \n    # Removing special characters and digits\n    formatted_article_text = re.sub('[^a-zA-Z]', ' ', link_content )\n    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n    \n    text = formatted_article_text\n    #text_temp = text.read()\n\n    preprocess_text = text.strip().replace(\"\\n\",\"\")\n    t5_prepared_Text = \"summarize: \"+preprocess_text\n    #print (\"original text preprocessed: \\n\", preprocess_text)\n\n    tokenized_text = t5_tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n\n\n    # summmarize \n    summary_ids = t5_model.generate(tokenized_text,\n                                    num_beams=3,\n                                    no_repeat_ngram_size=2,\n                                    min_length=100,\n                                    max_length=512,\n                                    early_stopping=False,\n                                    maxfeatures=10)\n\n    global t5_output\n    t5_output = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    \n    #output = str(d['Title'][i]) + \" - \" +output \n\n    #print (\"\\n\\nSummarized text: \\n\",output)\n    \n    return t5_output.capitalize()","565509ff":"links.head()","f20e07a0":"def summarization_links_t5_local(i):\n    \n    model     = T5ForConditionalGeneration.from_pretrained('t5-large')\n    tokenizer = T5Tokenizer.from_pretrained('t5-large')    \n\n    \n    \n    # Parsing the URL content \n    link_parsed = str(links['article_text'][i])\n    \n    # Returning <p> tags\n    paragraphs = link_parsed.find_all('p')\n    \n    # To get the content within all paragrphs loop through it\n    link_content = ''\n    for p in paragraphs:  \n        link_content += p.text\n    \n    # Removing Square Brackets and Extra Spaces\n    link_content = re.sub(r'\\[[0-9]*\\]', ' ', link_content)\n    link_content = re.sub(r'\\s+', ' ', link_content)\n    \n    # Removing special characters and digits\n    formatted_article_text = re.sub('[^a-zA-Z]', ' ', link_content )\n    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n    \n    text = formatted_article_text\n    #text_temp = text.read()\n\n    preprocess_text = text.strip().replace(\"\\n\",\"\")\n    t5_prepared_Text = \"summarize: \"+preprocess_text\n    #print (\"original text preprocessed: \\n\", preprocess_text)\n\n    tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n\n\n    # summmarize \n    summary_ids = model.generate(tokenized_text,\n                                    num_beams=3,\n                                    no_repeat_ngram_size=2,\n                                    min_length=100,\n                                    max_length=512,\n                                    early_stopping=False,\n                                    maxfeatures=10)\n\n    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    \n    #output = str(d['Title'][i]) + \" - \" +output \n\n    #print (\"\\n\\nSummarized text: \\n\",output)\n    return output.capitalize()","e0f08136":"#summarization_links_t5_local(1)","cd316722":"#text_extract(links,url_test)","f8684cd6":"from transformers import pipeline","072a4baa":"text = str(links[\"article_text\"][0])\nsummarization_pipeline = pipeline(task='summarization', model=\"t5-large\") \noutput = summarization_pipeline(text, min_length=1, max_length=500, top_k=10, top_p=0.8)\ns = str(output)\nclean_summarized_text(s)","26853e11":"#summarization_links_t5(links,url_test)","ca8ac2fb":"text = str(links[\"article_text\"][0])\n#summarization_infer(text, max=512)","cc3a5dfe":"#grammatical_acceptibility_infer(sentiment_analyzer_scores(summarization_links_t5(d,i)))","f37ca3be":"# Extra Text Generation using Transformers based on the article's title","0950ef67":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()","847fe20b":"def sentiment_analyzer_scores(sentence):\n    score = analyser.polarity_scores(sentence)\n    print(\"{:-<40} {}\".format(sentence, str(score)))","8cc6405b":"#sentiment_analyzer_scores(summarization_links_t5(d,i))","25b22a7e":"links","eb1059ef":"def check_sentiment_score(i):\n    analyzer = SentimentIntensityAnalyzer()\n    links['sentiment_score'] = pd.DataFrame(links.article_text.apply(analyzer.polarity_scores).tolist())['compound']\n    sentiment_score = pd.cut(links['sentiment_score'], [-np.inf, -0.35, 0.35, np.inf], labels=['negative', 'neutral', 'positive'])\n    links['article_sentiment'] = str(sentiment_score[i])\n    return sentiment_score[i]","fcf77846":"#i=0\n#sentiment = str(check_sentiment_score(i))\n#sentiment","b12bb9cf":"#links","6ff3982d":"#url_test = 188\n#knowledge_base = str(text_extract(links,url_test))\n#knowledge_base\n\n#webtext = preprocess_text[0:512]","e3a2e7b8":"def question_answer(knowledge_base):\n    qa_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\n    qa_model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\n\n    \n    \n    \n    #knowledge_base = str(text_extract(links,i))\n    text = knowledge_base[0:512]\n    questions = [\n        \"What is this article about?\",\n        \"Why is this article interesting?\"\n    ]\n\n    for question in questions:\n        inputs = qa_tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].tolist()[0]\n\n        text_tokens = qa_tokenizer.convert_ids_to_tokens(input_ids)\n        answer_start_scores, answer_end_scores = qa_model(**inputs)\n\n        answer_start = torch.argmax(\n            answer_start_scores\n        )  # Get the most likely beginning of answer with the argmax of the score\n        answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n\n        answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n        print(f\"Question: {question}\")\n        print(f\"Answer: {answer.capitalize()}\\n\")","bf9fa5a6":"#question_answer(i)","d3b661ea":"reduce_mem_usage(d, verbose=True)","ecab3243":"summarization_pipeline = pipeline(task='summarization', model=\"t5-large\")","ec511827":"i","1244f086":"def new_summarizer(i):\n    output =\"\"\n    text = str(links[\"article_text\"][i])\n    output = summarization_pipeline(text, num_beams=3, no_repeat_ngram_size=2, min_length=100, max_length=512, early_stopping=False,maxfeatures=10)\n    s = str(output)\n    summary = str(s)\n    return summary","73d22740":"#new_summarizer(i)","c322db9b":"#summary = str(summarization_links_t5(d,i))\n#summary","5fc7f574":"i=0\nsummary =\"\"\nn = int(links[\"Url\"].count())\n#n = 10\nprint(\"Building the summary of \"+str(n) +\" articles to the newsletter.\")\nprint(\"____________________________________________________________\")\nfor i in range(n):\n            \n    \n    #published = links[\"added\"][i]\n    to_summarize = links[\"to_summarize\"][i]\n    if to_summarize == 1:\n            print(\" \")\n            print(links[\"Title\"][i] + \".\")\n            print(\" \")\n            summary = str(summarization_links_t5(d,i))\n            print(summary)\n            #sentiment = str(check_sentiment_score(i))\n            print (\"\")\n            #print(\"Sentiment score of the article: \" +str(sentiment))        \n            question_answer(summary)\n            print(\" \")\n            s = pyshorteners.Shortener()\n            print(\"Link: \"+ str(links[\"Url\"][i]))\n            print(\"____________________________________________________________\")\n            gc.collect()\n            i=i+1\n            \nprint(\"All articles were summarized! Well done!\")","a1669b8a":"end_time = time.time()\nprint (f'Time taken to run this script: {end_time-start_time}')","170ee8db":"Before we go to the function that summarize all the articles.. just let me clean the memory...","4df248cb":"def generate_text(input_text,i,length):\n    from transformers import AutoModelWithLMHead, AutoTokenizer\n\n    model = AutoModelWithLMHead.from_pretrained(\"xlnet-large-cased\")\n    tokenizer = AutoTokenizer.from_pretrained(\"xlnet-large-cased\")\n\n    # Padding text helps XLNet with short prompts - proposed by Aman Rusia in https:\/\/github.com\/rusiaaman\/XLNet-gen#methodology\n    PADDING_TEXT = \"<eod> <\/s> <eos>\"\n    \n    inputs = tokenizer.encode(input_text, add_special_tokens=True, return_tensors=\"pt\")\n\n    prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=False, clean_up_tokenization_spaces=True))\n    outputs = model.generate(inputs, max_length=length, do_sample=True, top_p=0.1, top_k=20)\n    generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n\n    return generated\n#End of Text Generation","068a88b1":"import tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# add the EOS token as PAD token to avoid warnings\nmodel = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)","27102b9a":"#Sentiment Analysis\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n    #note: depending on how you installed (e.g., using source code download versus pip install), you may need to import like this:\n    #from vaderSentiment import SentimentIntensityAnalyzer\n\n# --- examples -------\nsentences = [str(sentiment_analyzer_scores(summarization_links_t5(d,i)))]\n\nanalyzer = SentimentIntensityAnalyzer()\nfor sentence in sentences:\n    vs = analyzer.polarity_scores(sentence)\n    print(\"{:-<65} {}\".format(sentence, str(vs)))\n       \n    \n#End of Sentiment Analysis","edd9f22e":"T5 is a new transformer model from Google that is trained in an end-to-end manner with text as input and modified text as output. You can read more about it here.\nIt achieves state-of-the-art results on multiple NLP tasks like summarization, question answering, machine translation etc using a text-to-text transformer trained on a large text corpus. More details here: [Text-to-Text Transfer Transformer](https:\/\/github.com\/google-research\/text-to-text-transfer-transformer)\nToday we will see how we can use huggingface\u2019s transformers library to summarize any given text. T5 is an abstractive summarization algorithm. It means that it will rewrite sentences when necessary than just picking up sentences directly from the original text.\n\nThe key point to note in the code above is that we prepend our text with \u201csummarize: \u201d before passing it to the T5 summarizer.\nIf you look in the paper, all we need to do for other tasks is to prepend our text with their corresponding string eg: \u201ctranslate English to German: \u201d for translation task.\n\n\n@article{2019t5,\n  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {arXiv e-prints},\n  year = {2019},\n  archivePrefix = {arXiv},\n  eprint = {1910.10683},\n}","0999178c":"# Summarizing using T5","a6eafc5f":"# Website error handling","f390a5af":"# Article Summarization","5cfa2848":"Work in Progress!!!! Some improvements will come..\n\nJair Ribeiro","40f25c16":"# encode context the generation is conditioned on\ninput_ids = tokenizer.encode(prompt_text, return_tensors='tf')\n\n# generate text until the output length (which includes the context length) reaches 50\ngreedy_output = model.generate(input_ids, max_length=100)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(greedy_output[0], skip_special_tokens=True))","eadded07":"# Create a randowm value for test","51eeaa45":"# Main Class","eaf972bd":"Here I am applying some data transformation to uniform the dataset. Working on Date format and Index.","a8aa4cbe":"# Using Tf for summarizing the newsletter\n\nAlso, I will be experimenting with a more advanced technique of summarization using a Transfer Learning model called T5:\n\nThe T5 model is inpired on the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https:\/\/arxiv.org\/pdf\/1910.10683.pdf) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu in Here the abstract:\n\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP) on several language understanding tasks. \n\nBy combining the insights from the \u201cColossal Clean Crawled Corpus\u201d or C4, the T5 model achieved state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and of course, text summarization.","8a736b2b":"# Using tinyUrl","f5da649c":"# Extractive Question Answering\nExtractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the run_squad.py.","611db1ac":"# Model Checkpoints\nWe have released the following checkpoints for pre-trained models described in our paper:\n\nT5-Small (60 million parameters): [gs:\/\/t5-data\/pretrained_models\/small](https:\/\/console.cloud.google.com\/storage\/browser\/t5-data\/pretrained_models\/small\/)\nT5-Base (220 million parameters): [gs:\/\/t5-data\/pretrained_models\/base](https:\/\/console.cloud.google.com\/storage\/browser\/t5-data\/pretrained_models\/base\/)\nT5-Large (770 million parameters): [gs:\/\/t5-data\/pretrained_models\/large](https:\/\/console.cloud.google.com\/storage\/browser\/t5-data\/pretrained_models\/large\/)\nT5-3B (3 billion parameters): [gs:\/\/t5-data\/pretrained_models\/3B](https:\/\/console.cloud.google.com\/storage\/browser\/t5-data\/pretrained_models\/3B\/)\nT5-11B (11 billion parameters): [gs:\/\/t5-data\/pretrained_models\/11B](https:\/\/console.cloud.google.com\/storage\/browser\/t5-data\/pretrained_models\/11B\/)\n\nSee [here](https:\/\/github.com\/google-research\/text-to-text-transfer-transformer\/blob\/master\/released_checkpoints.md) for a list of additional experimental pre-trained model checkpoints.","629d9ba7":"# Experimental method","d4d29cd9":"# Sentiment Scoring on the summarized text using Transformers","9ca7c515":"To simplify the copy\/paste from the summary to my newsletter I wanted to uniform all the links using tinyUrl so I've build a function to do it here:","578d78d1":"Save the article text to the column article_text on the dataframe","12c17113":"**Reset Index**","70c1d48c":"prompt_text = links[\"Title\"][url_test]\nprompt_text","fbdcba6e":"input_text = str(text_extract(links,url_test)) \n#input_text","42b067c3":"length = 200","869fe11e":"# Website error handling","9540905f":"Standard Method","6612bb0b":"# set seed to reproduce results. Feel free to change the seed though to get different results\ntf.random.set_seed(1)\n\n# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\nsample_outputs = model.generate(\n    input_ids,\n    do_sample=True, \n    max_length=250, \n    top_k=50, \n    top_p=0.95, \n    num_return_sequences=1\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","10520a31":"#Test of the AI Text Generation\nprint(\"Testing Text generation :\")\n\ntext_generator = pipeline(\"text-generation\")\nnew_text = str(text_generator(prompt_text, max_length=150))\nclean_summarized_text(new_text)","d74a7576":"#  Summarization"}}