{"cell_type":{"91a9a82e":"code","dbeecae1":"code","a8485e2e":"code","a454e672":"code","6204aa64":"code","bb8096de":"code","6faa52f3":"code","aba7e688":"code","29dd97ac":"code","7221ae79":"code","44e681e1":"code","0307a534":"code","9a90e760":"code","b8699fed":"code","40d7a273":"code","25301da7":"code","75d026e7":"code","593ecf7c":"code","498846ce":"code","20ca89ed":"code","af87a97f":"code","18cb6438":"code","4a9d8364":"code","864776a0":"code","37cb76ba":"code","aa995f84":"code","5e332042":"code","81807170":"code","e4dbc8bc":"code","51c798d5":"code","70acf827":"code","d2df12dc":"code","0d58d7fa":"code","6663b7ea":"code","e1ac9b98":"code","d2ee0528":"code","b4931091":"code","081fb533":"code","185436ab":"code","1f46489b":"code","e3458c14":"code","b165424c":"code","932e9b33":"code","a9dfa4cf":"code","7704ef6b":"code","7508376a":"code","6262dbfb":"code","4a0dec32":"markdown","67ddb9d0":"markdown","41ad1deb":"markdown","929b41f1":"markdown","c86105cf":"markdown","b5a577d5":"markdown","baa0aa03":"markdown","05f8b21e":"markdown","db7f42d4":"markdown","abd20f5a":"markdown","25a166ac":"markdown","e94d9754":"markdown","1e520ae5":"markdown","3155fc7b":"markdown","48beee27":"markdown","191d1db3":"markdown","9c35ceb4":"markdown","9495367b":"markdown","aaedb313":"markdown","9c9c7da1":"markdown","40596bb7":"markdown","47c9c5cf":"markdown","8ae9b443":"markdown","4604a24a":"markdown","fa410024":"markdown","29ad2cf5":"markdown","e77c25b8":"markdown","bcf8e490":"markdown","3f1f77c4":"markdown","9a41307f":"markdown","ec1b8ca9":"markdown","e44c1988":"markdown","97af1d2f":"markdown","dadeaec2":"markdown","52474101":"markdown","3c3685cf":"markdown"},"source":{"91a9a82e":"#load libraries for data manipulation and visualization\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n# text\/file processing libraries\nimport string\nimport re\nimport sys\nfrom nltk.corpus import stopwords\nfrom itertools import chain\n# warnings\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","dbeecae1":"# load the train and test data sets\ntrain_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint('Number of Training Samples = {}'.format(train_df.shape[0]))\nprint('Number of Test Samples = {}\\n'.format(test_df.shape[0]))\nprint('Training X Shape = {}'.format(train_df.shape))\nprint('Training y Shape = {}\\n'.format(train_df['target'].shape[0]))\nprint('Test X Shape = {}'.format(test_df.shape))\n\nprint('Test y Shape = {}\\n'.format(test_df.shape[0]))\nprint('Index of Train Set:\\n', train_df.columns)\nprint('Index of Test Set:\\n', test_df.columns)","a8485e2e":"# class distribution of train set\npl = sb.countplot(train_df['target'])","a454e672":"# display sample train data\ntrain_df.sample(5)","6204aa64":"# sample train disaster tweet\ntrain_df.loc[1241]['text']","bb8096de":"# sample train non disaster tweet\ntrain_df.loc[2301]['text']","6faa52f3":"train_df.groupby(['text']).nunique().sort_values(by='target', ascending=False)[0:18]","aba7e688":"df_mislabeled = train_df.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\ndf_mislabeled_all = df_mislabeled.index.tolist()\nprint(f'Number of repeated tweets(after preprocessing): {len(df_mislabeled_all)}')","29dd97ac":"train_df['target_relabeled'] = train_df['target'].copy() \n\ntarget_1_list = [   \n   \n    \"CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring\",\n    \".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4\",\n    \"Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE\",\n    \"RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG\",\n    \"Caution: breathing may be hazardous to your health.\" ]\n    \nfor mislabeled_sample in df_mislabeled_all:\n    if mislabeled_sample in target_1_list:\n        train_df.loc[train_df['text'] == mislabeled_sample, 'target_relabeled'] = 1\n    else:\n        train_df.loc[train_df['text'] == mislabeled_sample, 'target_relabeled'] = 0\n\nfilter_mislabel = (train_df['target'] != train_df['target_relabeled'])\nprint(f'Number of relabeled: {len(train_df[filter_mislabel])}')\ntrain_df[filter_mislabel][:12]  ","7221ae79":"train_df['text'].sample(20).tolist()","44e681e1":"def html_references(tweets):\n    texts = tweets\n    # remove url - references to websites\n    url_remove  = r'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    texts  = re.sub(url_remove, '', texts)\n    # remove common html entity references in utf-8 as '&lt;', '&gt;', '&amp;'\n    entities_remove = r'&amp;|&gt;|&lt'\n    texts = re.sub(entities_remove, \"\", texts)\n    # split into words by white space\n    words = texts.split()\n    #convert to lower case\n    words = [word.lower() for word in words]\n    return \" \".join(words)\ntrain_df['clean_text'] = train_df['text'].apply(lambda x : html_references(x))\ntest_df['clean_text'] = test_df['text'].apply(lambda x : html_references(x))","0307a534":"def decontraction(tweet):\n    # specific\n    tweet = re.sub(r\"won\\'t\", \" will not\", tweet)\n    tweet = re.sub(r\"won\\'t've\", \" will not have\", tweet)\n    tweet = re.sub(r\"can\\'t\", \" can not\", tweet)\n    tweet = re.sub(r\"don\\'t\", \" do not\", tweet)\n    \n    tweet = re.sub(r\"can\\'t've\", \" can not have\", tweet)\n    tweet = re.sub(r\"ma\\'am\", \" madam\", tweet)\n    tweet = re.sub(r\"let\\'s\", \" let us\", tweet)\n    tweet = re.sub(r\"ain\\'t\", \" am not\", tweet)\n    tweet = re.sub(r\"shan\\'t\", \" shall not\", tweet)\n    tweet = re.sub(r\"sha\\n't\", \" shall not\", tweet)\n    tweet = re.sub(r\"o\\'clock\", \" of the clock\", tweet)\n    tweet = re.sub(r\"y\\'all\", \" you all\", tweet)\n    # general\n    tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n    tweet = re.sub(r\"n\\'t've\", \" not have\", tweet)\n    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n    tweet = re.sub(r\"\\'s\", \" is\", tweet)\n    tweet = re.sub(r\"\\'d\", \" would\", tweet)\n    tweet = re.sub(r\"\\'d've\", \" would have\", tweet)\n    tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n    tweet = re.sub(r\"\\'ll've\", \" will have\", tweet)\n    tweet = re.sub(r\"\\'t\", \" not\", tweet)\n    tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n    tweet = re.sub(r\"\\'m\", \" am\", tweet)\n    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n    return tweet \ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : decontraction(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : decontraction(x))","9a90e760":"# print puntuation characters\nstring.punctuation","b8699fed":"# print printable characters\nstring.printable","40d7a273":"def filter_punctuations_etc(tweets):\n    words = tweets.split()\n    # prepare regex for char filtering\n    re_punc = re.compile( '[%s]' % re.escape(string.punctuation))\n    # remove punctuation from each word\n    words = [re_punc.sub('', w) for w in words]\n    # filter out non-printable characters\n    re_print = re.compile( '[^%s]' % re.escape(string.printable))\n    words = [re_print.sub(' ', w) for w in words]\n    return \" \".join(words)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : filter_punctuations_etc(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : filter_punctuations_etc(x))","25301da7":"def separate_alphanumeric(tweets):\n    words = tweets\n    # separate alphanumeric\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : separate_alphanumeric(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : separate_alphanumeric(x))","75d026e7":"def cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] # take max of 2 consecutive letters\ndef unique_char(rep, tweets):\n    substitute = re.sub(r'(\\w)\\1+', rep, tweets)\n    return substitute\ntrain_df['clean_text'] = (train_df['clean_text'].astype('str').apply(lambda x : unique_char(cont_rep_char, x)))\ntest_df['clean_text'] = (test_df['clean_text'].astype('str').apply(lambda x : unique_char(cont_rep_char, x)))","593ecf7c":"!pip install pyspellchecker","498846ce":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n#train_df['clean_text'] = train_df['clean_text'].apply(lambda x : correct_spellings(x))\n#test_df['clean_text'] = test_df['clean_text'].apply(lambda x : correct_spellings(x))","20ca89ed":"!pip install wordninja","af87a97f":"import wordninja # !pip install wordninja\ndef split_attached_words(tweet):\n    words = wordninja.split(tweet)\n    return\" \".join(words)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : split_attached_words(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : split_attached_words(x))","18cb6438":"def stopwords_shortwords(tweet):\n    # filter out stop words\n    words = tweet.split()\n    stop_words = set(stopwords.words( 'english' ))\n    words = [w for w in words if not w in stop_words]\n    # filter out short tokens\n    for word in words:\n        if word.isalpha():\n            words = [word for word in words if len(word) > 1 ]\n        else:\n            words = [word for word in words]\n    return\" \".join(words)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : stopwords_shortwords(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : stopwords_shortwords(x))","4a9d8364":"from sklearn.model_selection import train_test_split\n# split train set into train\/validate \ntrain_df2, validate_df = train_test_split(train_df, test_size=0.075, random_state=0)\ntrain_df2 = train_df2.reset_index(drop=True)\nvalidate_df = validate_df.reset_index(drop=True)","864776a0":"# train and test sets\nall_df=pd.concat([train_df,test_df])\nX_all = all_df['clean_text']\n# training set\nX_train = train_df2['clean_text']\ny_train = train_df2['target_relabeled'].astype(int)\n# validation set\nX_validate= validate_df['clean_text']\ny_validate = validate_df['target_relabeled'].astype(int)\n# test set\nX_test = test_df['clean_text']","37cb76ba":"from tensorflow.keras.preprocessing.text import Tokenizer\n# create a tokenizer for encoding texts as digits\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n# create the tokenizer - mapping data to integer values\ntokenizer = create_tokenizer(X_all)\nword_index=tokenizer.word_index\nmax_words = len(word_index) + 1\nprint( 'unique words are : %d' % max_words)","aa995f84":"# tweet with maximum length\nmax_length = max([len(s.split()) for s in X_all])\nprint( ' Maximum length: %d ' % max_length)","5e332042":"# integer encode and pad tweets\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ndef encode_data(tokenizer, max_length, data):\n    # integer encode\n    encoded = tokenizer.texts_to_sequences(data)\n    # pad sequences\n    padded = pad_sequences(encoded, maxlen=max_length, padding= 'post' )\n    return padded\nXtrain = encode_data(tokenizer, max_length, X_train)\nXvalidate = encode_data(tokenizer, max_length, X_validate)\nXtest = encode_data(tokenizer, max_length, X_test)","81807170":"sb.heatmap(Xtrain==0, vmin=0, cbar=False)\nplt.show()","e4dbc8bc":"# parsing the GloVe word-embeddings file\nimport os\nglove_dir = '..\/input\/glove6b'\nembeddings_index = {}\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","51c798d5":"# preparing the GloVe word-embeddings matrix\nembedding_dim = 100\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","70acf827":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding\nimport  tensorflow.keras.optimizers as optimizers\nfrom tensorflow.keras.layers import LSTM, Bidirectional\nmodel = Sequential()\nmodel.add(Embedding(max_words, 100, input_length=max_length))\n# lstm layer\nmodel.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))   \nmodel.add(Bidirectional(LSTM(64,  dropout=0.2, recurrent_dropout=0.2,))) \n# densely connected classifier\nmodel.add(Dense(64, activation= 'relu' ))\nmodel.add(Dense(1, activation='sigmoid'))\n# summarize\nmodel.summary()","d2df12dc":"# load pretrained word embeddings into the Embedding layer\nmodel.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = True","0d58d7fa":"# compile\nmodel.compile(loss= 'binary_crossentropy',  optimizer=optimizers.Adam(lr=.0001), metrics=[ 'accuracy' ])","6663b7ea":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\ncallbacks = [\n    EarlyStopping(patience=3, verbose=1),\n    ReduceLROnPlateau(factor=0.25, patience=2, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('model_lstm.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]","e1ac9b98":"# fit network\nmodel.fit(Xtrain, y_train, epochs=10, callbacks=callbacks, validation_data=(Xvalidate,y_validate))","d2ee0528":"sample_submission=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\ny_pre=model.predict(Xtest)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_submission['id'].values.tolist(),'target':y_pre})\nsub.to_csv('tweet5a.csv',index=False)","b4931091":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","081fb533":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tokenization\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nimport  tensorflow.keras.optimizers as optimizers","185436ab":"# create BERT embedding layer\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","1f46489b":"# create BERT tokenizer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","e3458c14":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","b165424c":"# encoding input data\ntrain_input = bert_encode(train_df.text.values, tokenizer, max_len=128)\ntest_input = bert_encode(test_df.text.values, tokenizer, max_len=128)\ntrain_labels = train_df.target_relabeled.values","932e9b33":"# define create model function\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n   \n    return model","a9dfa4cf":"# create model\nmodel = build_model(bert_layer, max_len=128)\nmodel.summary()","7704ef6b":"from tensorflow.keras.callbacks import ModelCheckpoint\ncallbacks2 = [\n    ModelCheckpoint('model_bert.h5', monitor='val_loss', save_best_only=True)\n]","7508376a":"model.fit(train_input, train_labels, validation_split=0.075, epochs=3, \n          callbacks=callbacks2, batch_size=32)","6262dbfb":"sample_submission=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\ny_pre2=model.predict(test_input)\ny_pre2=np.round(y_pre2).astype(int).reshape(3263)\nsub2=pd.DataFrame({'id':sample_submission['id'].values.tolist(),'target':y_pre2})\nsub2.to_csv('tweet5b.csv',index=False)","4a0dec32":"# Introduction\nThe challenge in this kernel is to build a model to predict which tweets are disaster tweets and which ones are not given a train set of tweets with their labels. The model will be evaluated with the given test set.","67ddb9d0":"#### Spell checking\nCheck spellings and make corrections where possible, this a computational expensive exercise.","41ad1deb":"#### GloVe Embedding\nCreate an embedding matrix with GloVe","929b41f1":"## 2.0 Data cleaning\nCleaning text means converting it to a list of words or tokens, different cleaning task will be performed on the dataset.","c86105cf":"#### Pre-processing\nPrepare data for processing","b5a577d5":"Transform preprocessed text into padded sequences of word ids to get a feature matrix","baa0aa03":"#### Tokenization","05f8b21e":"#### Remove punctuations and unprintable characters\nRemove punctuations will remove the characters specified by string.punctuation while the inverse of string.printable will remove non ascii characters.","db7f42d4":"The keyword 'demolish' in the above tweet may literally mean disaster but reading the text indicates no disaster.","abd20f5a":"#### Preprocessing","25a166ac":"## 1.0 Data exploration\nLoad and explore data","e94d9754":"\"There are **18** unique tweets in training set which are labeled differently in their duplicates. Those tweets are probably labeled by different people and they interpreted the meaning differently because some of them are not very clear. Tweets with two unique `target` values are relabeled since they can affect the training score.\"","1e520ae5":"The keywords 'buildings on fire' in the above tweet are in the text. Though, the keywords do not appear in that order in the text.","3155fc7b":"## 4.0 BERT Sentiment Analysis\nBERT(Bidirectional Encoded Representation from Transformers) is a language model that can recognize context and semantics in a sentence. It takes input text transformed into 3 vectors ids, masks and segments. It can be used in a variety of NLP including sentiment analysis and is known to perform well with minimal or no text preprocessing. This is because the tokenizer needs to capture the context of each sentence which can be lost in a text cleaning exercise. This kernel will process text for BERT without text cleaning.","48beee27":"## 3.0 GloVe Sentiment Analysis\nPrepare data for a word embedding sentiment analysis model. A word embedding is a context based dense vector representation of texts. There are 2 approaches of using word embeddings, the 1st approach is learn word embedding for a specific task or to be reused in another project,  the 2nd approach is the use of pretrained word embeddings like Word2Vec, GloVe, BERT etc.","191d1db3":"#### Separate alphanumeric characters","9c35ceb4":"Non disaster tweets represented with 0 are more than disaster tweets represented with 1","9495367b":"The dataset for this kernel is the kaggle Real or Not? NLP with  Disater Tweets","aaedb313":"The puntuation characters will be removed with non english and unicode characters not in string.printable","9c9c7da1":"#### Modeling","40596bb7":"A pretrained BERT model can be used where the BERT layers are frozen during training preserving the model parameters or the prefered method where the BERT  layers are finetuned and trained on the new data . Running BERT the first time can be a hassle, gained insight on implementation from [disaster nlp: keras bert using tfhub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub) ","47c9c5cf":"## If you like this kernel, please upvote, corrections are welcome.","8ae9b443":"####  GloVe Embedding and LSTM","4604a24a":"#### Split attached words\nSplit attached words especially common with twitter hashtags like \"caraccidentlawyer\" into \"car\", \"accident\", and \"lawyer\". Some desirable words may be split, but the gain may be more than the loss.","fa410024":"There is need to remove or filter out characters that may not be relevant in predicting disaster or non disaster tweets such as: punctuations, contractions, stop words, short words, urls, html tags, emojis, mentions, hashtags, and bad spellings ","29ad2cf5":"#### Remove html links and entity references","e77c25b8":"An analysis of the sample disaster and non disaster tweet indicate that the text column is the most important as it contains the  keywords and the context of the text is important in determining a disaster and non disaster tweet.","bcf8e490":"#### Remove apostrophes\/contractions","3f1f77c4":"Define function to encode input as 3 matrices of tokens or ids, masks and segments","9a41307f":"#### Mislabelled tweets\nThis idea was adapted from  [disaster nlp: keras bert using tfhub](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)","ec1b8ca9":"#### Exploring text\nExplore text column for data cleaning","e44c1988":"Tokenizing the text","97af1d2f":"Separating texts and targets for modeling","dadeaec2":"#### Remove stopwords and short words\nStopwords are common words that may not add to keywords. Stopwords and single letter words will be removed to reduce vocabularity and sparsity  of a bag of words model.","52474101":"#### Change repetitive characters\nChange repetitive characters e.g.goooooooaaaal to gooaal, so the spell checker can try correcting it. An english word cannot have more than 2 consecutive same letter.","3c3685cf":"####  Sample data\nExplore sample disaster and non disaster tweet"}}