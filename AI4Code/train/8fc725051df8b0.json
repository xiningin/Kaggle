{"cell_type":{"84e526f9":"code","800f0a54":"code","c777a168":"code","f84b0cac":"code","76974ca2":"code","3f5491cd":"code","6b202e81":"code","a011d27c":"code","2cd2a1be":"code","f23db35d":"code","53410118":"code","e34d12eb":"code","333016bc":"code","0dd6102d":"code","fb8d1aed":"code","313b7652":"code","35ab5ee1":"code","276cde2b":"code","e098d631":"code","d5964cfa":"code","a61b9692":"code","3b20f236":"code","8a04d432":"code","f135696e":"code","941a0721":"code","29f504e6":"code","e83888f7":"code","0cde17a3":"code","fd3c4b41":"code","c09f97ed":"code","71fdb93f":"code","1240a4d3":"code","29cefdb6":"code","dd30162e":"code","d958b633":"code","967c75b4":"code","4faaf8a7":"code","8c679d5c":"code","0f0c756d":"code","1b65783f":"code","d3a9d51b":"code","54a707d1":"code","11c0061e":"code","63e4cde6":"code","5efa3cca":"code","94962161":"code","3860fa32":"code","3458bc4a":"code","7bc4646d":"code","8d2e9e22":"code","a4402564":"code","c0cd3e97":"code","99c14c94":"code","44c3fc15":"code","b559d3f4":"code","1a855b3d":"code","582a1dc5":"code","3d6f9931":"code","c00e4e57":"code","871ffadc":"code","da456678":"code","f7b21d43":"code","0da39063":"code","44e6a6d9":"code","6b636be9":"code","d4db0792":"code","5c139487":"code","fedf8222":"code","90bc15d9":"code","78947f96":"code","6cf8bb0c":"code","1c058322":"code","62fb6284":"code","022a140a":"code","2db0f95e":"code","ba055f8f":"code","cd3eb777":"code","11454523":"code","8c2d2723":"code","8bdfa51e":"code","811ae17d":"markdown","ed1f8b01":"markdown"},"source":{"84e526f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","800f0a54":"news = pd.read_json(\"\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json\",lines=True)","c777a168":"news.shape","f84b0cac":"news.columns","76974ca2":"news.head()","3f5491cd":"pd.options.display.max_rows\npd.set_option('display.max_colwidth', -1)","6b202e81":"news.headline","a011d27c":"news.short_description","2cd2a1be":"news[\"category\"].value_counts()","f23db35d":"news.shape","53410118":"short = news.sample(frac = 0.1,random_state = 1)","e34d12eb":"short.shape","333016bc":"short[\"category\"].value_counts()","0dd6102d":"short.columns","fb8d1aed":"!nvidia-smi","313b7652":"!pip install transformers\n!pip install pytorch_lightning","35ab5ee1":"import argparse\nimport glob\nimport os\nimport json\nimport time\nimport logging\nimport random\nimport re\nfrom itertools import chain\nfrom string import punctuation\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\n\n\nfrom transformers import (\n    AdamW,\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    get_linear_schedule_with_warmup\n)","276cde2b":"def set_seed(seed):\n  random.seed(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","e098d631":"class T5FineTuner(pl.LightningModule):\n  def __init__(self, hparams):\n    super(T5FineTuner, self).__init__()\n    self.hparams = hparams\n    \n    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n  \n  def is_logger(self):\n    return self.trainer.proc_rank <= 0\n  \n  def forward(\n      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n  ):\n    return self.model(\n        input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n        lm_labels=lm_labels,\n    )\n\n  def _step(self, batch):\n    lm_labels = batch[\"target_ids\"]\n    lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n\n    outputs = self(\n        input_ids=batch[\"source_ids\"],\n        attention_mask=batch[\"source_mask\"],\n        lm_labels=lm_labels,\n        decoder_attention_mask=batch['target_mask']\n    )\n\n    loss = outputs[0]\n\n    return loss\n\n  def training_step(self, batch, batch_idx):\n    loss = self._step(batch)\n\n    tensorboard_logs = {\"train_loss\": loss}\n    return {\"loss\": loss, \"log\": tensorboard_logs}\n  \n  def training_epoch_end(self, outputs):\n    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n    return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n  def validation_step(self, batch, batch_idx):\n    loss = self._step(batch)\n    return {\"val_loss\": loss}\n  \n  def validation_epoch_end(self, outputs):\n    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n    tensorboard_logs = {\"val_loss\": avg_loss}\n    return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n  def configure_optimizers(self):\n    \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n    model = self.model\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": self.hparams.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n    self.opt = optimizer\n    return [optimizer]\n  \n  def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n    if self.trainer.use_tpu:\n      xm.optimizer_step(optimizer)\n    else:\n      optimizer.step()\n    optimizer.zero_grad()\n    self.lr_scheduler.step()\n  \n  def get_tqdm_dict(self):\n    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n\n    return tqdm_dict\n\n  def train_dataloader(self):\n    train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n    t_total = (\n        (len(dataloader.dataset) \/\/ (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n        \/\/ self.hparams.gradient_accumulation_steps\n        * float(self.hparams.num_train_epochs)\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n    )\n    self.lr_scheduler = scheduler\n    return dataloader\n\n  def val_dataloader(self):\n    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)","d5964cfa":"logger = logging.getLogger(__name__)\n\nclass LoggingCallback(pl.Callback):\n  def on_validation_end(self, trainer, pl_module):\n    logger.info(\"***** Validation results *****\")\n    if pl_module.is_logger():\n      metrics = trainer.callback_metrics\n      # Log results\n      for key in sorted(metrics):\n        if key not in [\"log\", \"progress_bar\"]:\n          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n\n  def on_test_end(self, trainer, pl_module):\n    logger.info(\"***** Test results *****\")\n\n    if pl_module.is_logger():\n      metrics = trainer.callback_metrics\n\n      # Log and save results to file\n      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n      with open(output_test_results_file, \"w\") as writer:\n        for key in sorted(metrics):\n          if key not in [\"log\", \"progress_bar\"]:\n            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))","a61b9692":"args_dict = dict(\n    data_dir=\"\", # path for data files\n    output_dir=\"\", # path to save the checkpoints\n    model_name_or_path='t5-base',\n    tokenizer_name_or_path='t5-base',\n    max_seq_length=512,\n    learning_rate=3e-4,\n    weight_decay=0.0,\n    adam_epsilon=1e-8,\n    warmup_steps=0,\n    train_batch_size=8,\n    eval_batch_size=8,\n    num_train_epochs=2,\n    gradient_accumulation_steps=16,\n    n_gpu=1,\n    early_stop_callback=False,\n    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n    opt_level='O1', # you can find out more on optimisation levels here https:\/\/nvidia.github.io\/apex\/amp.html#opt-levels-and-properties\n    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n    seed=42,\n)","3b20f236":"train = news.sample(frac = 0.1,random_state = 1)\nval = news.sample(frac = 0.01,random_state = 41)","8a04d432":"train.shape,val.shape","f135696e":"train.columns","941a0721":"drop_list = ['authors', 'link', 'date']\ntrain.drop(drop_list,axis = 1,inplace = True)\nval.drop(drop_list,axis = 1,inplace = True)","29f504e6":"!pwd","e83888f7":"!mkdir data\ntrain.to_csv(\"\/kaggle\/working\/data\/train.csv\",index = False)\nval.to_csv(\"\/kaggle\/working\/data\/val.csv\",index = False)","0cde17a3":"tokenizer = T5Tokenizer.from_pretrained('t5-base')","fd3c4b41":"train.columns","c09f97ed":"class Topicmodel(Dataset):\n    def __init__(self, tokenizer, data_dir, type_path, max_len=256):\n        self.path = os.path.join(data_dir, type_path + '.csv')\n\n        self.source_column_1 = 'headline'\n        self.source_column_2 = 'short_description'\n        self.target_column = 'category'\n        self.data = pd.read_csv(self.path)\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.inputs = []\n        self.targets = []\n\n        self._build()\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, index):\n        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n        target_ids = self.targets[index][\"input_ids\"].squeeze()\n\n        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n\n        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n\n    def _build(self):\n        for idx in range(len(self.data)):\n            input_1,input_2,target = self.data.loc[idx, self.source_column_1],self.data.loc[idx, self.source_column_2],self.data.loc[idx, self.target_column]\n\n            input_ = \"Text : \"+ str(input_1) + \".\"+ str(input_2)  +\"<\/s>\"\n            target = \"Topic : \" +str(target) + \" <\/s>\"\n\n            # tokenize inputs\n            tokenized_inputs = self.tokenizer.batch_encode_plus(\n                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n            )\n            # tokenize targets\n            tokenized_targets = self.tokenizer.batch_encode_plus(\n                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n            )\n\n            self.inputs.append(tokenized_inputs)\n            self.targets.append(tokenized_targets)","71fdb93f":"!pwd","1240a4d3":"dataset = Topicmodel(tokenizer, '\/kaggle\/working\/data\/', 'val', 312)\nprint(\"Val dataset: \",len(dataset))","29cefdb6":"data = dataset[61]\nprint(tokenizer.decode(data['source_ids']))\nprint(tokenizer.decode(data['target_ids']))","dd30162e":"if not os.path.exists('t5_data'):\n    os.makedirs('t5_data')","d958b633":"!ls","967c75b4":"!ls t5_data","4faaf8a7":"args_dict.update({'data_dir': '\/kaggle\/working\/data\/', 'output_dir': 't5_data', 'num_train_epochs':1,'max_seq_length':312})\nargs = argparse.Namespace(**args_dict)\nprint(args_dict)","8c679d5c":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n)\n\ntrain_params = dict(\n    accumulate_grad_batches=args.gradient_accumulation_steps,\n    gpus=args.n_gpu,\n    max_epochs=args.num_train_epochs,\n    early_stop_callback=False,\n    precision= 16 if args.fp_16 else 32,\n    amp_level=args.opt_level,\n    gradient_clip_val=args.max_grad_norm,\n    checkpoint_callback=checkpoint_callback,\n    callbacks=[LoggingCallback()],\n)\n","0f0c756d":"def get_dataset(tokenizer, type_path, args):\n  return Topicmodel(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)","1b65783f":"print (\"Initialize model\")\nmodel = T5FineTuner(args)\n\ntrainer = pl.Trainer(**train_params)\n\n","d3a9d51b":"print (\" Training model\")\ntrainer.fit(model)","54a707d1":"print (\"training finished\")\n\nprint (\"Saving model\")\nmodel.model.save_pretrained('t5_data')\n\nprint (\"Saved model\")","11c0061e":"import textwrap\nfrom tqdm.auto import tqdm\nfrom sklearn import metrics","63e4cde6":"!pwd","5efa3cca":"dataset =  Topicmodel(tokenizer, data_dir='\/kaggle\/working\/data\/', type_path='val')\nloader = DataLoader(dataset, batch_size=32, num_workers=4)","94962161":"outputs = []\ntargets = []\nfor batch in tqdm(loader):\n  outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n                              attention_mask=batch['source_mask'].cuda(), \n                              max_length=20)\n\n  dec = [tokenizer.decode(ids) for ids in outs]\n  target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n  \n  outputs.extend(dec)\n  targets.extend(target)","3860fa32":"outputs","3458bc4a":"metrics.accuracy_score(targets, outputs)","7bc4646d":"def topic(string):\n    text = \"Text : \" + string + \"<\/s>\"\n    encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n#     input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n    outs = model.model.generate(input_ids=encoding[\"input_ids\"].cuda(), \n                              attention_mask=encoding[\"attention_mask\"].cuda(), \n                              max_length=20)\n    print (\"\\nOriginal Text ::\")\n    print (string)\n    print (\"Topic :: \")\n    string_final = [tokenizer.decode(ids) for ids in outs]    \n    return(\" \".join(string_final))\n\n\n    ","8d2e9e22":"str1 = \"sachin tendulkar\"\ntopic(str1)","a4402564":"str1 = \"Donald Trump\"\ntopic(str1)","c0cd3e97":"str1 = \"pharma\"\ntopic(str1)","99c14c94":"# Merge both Title and description\n# !wget http:\/\/blob.thijs.ai\/wiki-summary-dataset\/raw.tar.gz","44c3fc15":"# !ls","b559d3f4":"# !unzip raw.tar.gz","1a855b3d":"# !ls","582a1dc5":"# !tar --gunzip --extract --verbose --file=raw.tar.gz","3d6f9931":"# df = pd.read_csv('raw.txt', sep=\"|\", header=None, names=['Topic','Nan','Nan2','Text'])","c00e4e57":"# df.shape","871ffadc":"# df.head()","da456678":"# drop_list = [\"Nan\",\"Nan2\"]\n# df.drop(drop_list,inplace = True,axis = 1 )","f7b21d43":"# new_df = df.sample(frac=0.01, replace=True, random_state=1)","0da39063":"# new_df.shape","44e6a6d9":"# new_df.tail(10)","6b636be9":"# df = pd.read_csv(\"\/kaggle\/input\/medium-stories\/Medium_Clean.csv\")","d4db0792":"# print(df.columns)","5c139487":"# drop_list = ['Image', 'Author', 'Publication','Year', 'Month', 'Day', 'Reading_Time']\n# df.drop(drop_list,axis = 1,inplace = True)","fedf8222":"# drop_list = ['Unnamed: 0','Claps', 'url', 'Author_url']\n\n\n\n\n# df.drop(drop_list,axis = 1,inplace = True)","90bc15d9":"# df.columns","78947f96":"# df.shape","6cf8bb0c":"# df.isna().sum()","1c058322":"# df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n# df.shape","62fb6284":"# listl = ['Tag_ai', 'Tag_android', 'Tag_apple','Tag_architecture', 'Tag_art', 'Tag_artificial-intelligence','Tag_big-data', 'Tag_bitcoin', 'Tag_blacklivesmatter', 'Tag_blockchain',\n#        'Tag_blog', 'Tag_blogging', 'Tag_books', 'Tag_branding', 'Tag_business',\n#        'Tag_college', 'Tag_computer-science', 'Tag_creativity',\n#        'Tag_cryptocurrency', 'Tag_culture', 'Tag_data', 'Tag_data-science',\n#        'Tag_data-visualization', 'Tag_deep-learning', 'Tag_design', 'Tag_dogs',\n#        'Tag_donald-trump', 'Tag_economics', 'Tag_education', 'Tag_energy',\n#        'Tag_entrepreneurship', 'Tag_environment', 'Tag_ethereum',\n#        'Tag_feminism', 'Tag_fiction', 'Tag_food', 'Tag_football', 'Tag_google',\n#        'Tag_government', 'Tag_happiness', 'Tag_health', 'Tag_history',\n#        'Tag_humor', 'Tag_inspiration', 'Tag_investing', 'Tag_ios',\n#        'Tag_javascript', 'Tag_jobs', 'Tag_journalism', 'Tag_leadership',\n#        'Tag_life', 'Tag_life-lessons', 'Tag_love', 'Tag_machine-learning',\n#        'Tag_marketing', 'Tag_medium', 'Tag_mobile', 'Tag_motivation',\n#        'Tag_movies', 'Tag_music', 'Tag_nba', 'Tag_news', 'Tag_nutrition',\n#        'Tag_parenting', 'Tag_personal-development', 'Tag_photography',\n#        'Tag_poem', 'Tag_poetry', 'Tag_politics', 'Tag_product-design',\n#        'Tag_productivity', 'Tag_programming', 'Tag_psychology', 'Tag_python',\n#        'Tag_racism', 'Tag_react', 'Tag_relationships', 'Tag_science',\n#        'Tag_self-improvement', 'Tag_social-media', 'Tag_software-engineering',\n#        'Tag_sports', 'Tag_startup', 'Tag_tech', 'Tag_technology', 'Tag_travel',\n#        'Tag_trump', 'Tag_ux', 'Tag_venture-capital', 'Tag_web-design',\n#        'Tag_web-development', 'Tag_women', 'Tag_wordpress', 'Tag_work',\n#        'Tag_writing']","022a140a":"# listl[0]","2db0f95e":"# len(df)\n\n\n\n\n","ba055f8f":"# df[\"Tags\"] = \" \"","cd3eb777":"# df.reset_index(inplace = True)","11454523":"# def tag(df):\n#     for i in range(10000):\n#         for j in listl:\n#             if df[j][i] == int(1):\n#                 df[\"Tags\"][i] = str(df[\"Tags\"][i]) + \" \" +str(j)\n","8c2d2723":"# %%timeit\n# tag(df)","8bdfa51e":"# df[\"Tags\"]","811ae17d":"# Topic Modelling with T5","ed1f8b01":"# Model"}}