{"cell_type":{"2fe07a85":"code","5a38db05":"code","9877f01f":"code","4d4565a8":"code","713207f3":"code","734dedf9":"code","7a5d9459":"code","2526e364":"code","e4b455dd":"markdown","c2e6db19":"markdown","6ec044b8":"markdown","7507a6e1":"markdown","249fe8fd":"markdown","df86541f":"markdown","c64190b6":"markdown","5bf63722":"markdown","2a8ab028":"markdown","f88a1c70":"markdown","47b149ed":"markdown","b3cac2c7":"markdown","78d5b0b2":"markdown","6f528ceb":"markdown","22cffaa0":"markdown","bdb0d6dc":"markdown","4c4fe355":"markdown","38f1740b":"markdown","f0a76f92":"markdown","ae867c72":"markdown"},"source":{"2fe07a85":"!nvidia-smi -L","5a38db05":"!pip install tensorflow==1.15.2\n!pip install -q unidecode tensorboardX\n!git clone -q https:\/\/github.com\/NVIDIA\/tacotron2\n%cd tacotron2\n!pip install inflect\n!git submodule init\n!git submodule update\n!apt-get install pv\n!apt-get install jq\n!pip install gdown\n\n%matplotlib inline\nimport os\nimport time\nimport argparse\nimport math\nfrom distutils.dir_util import copy_tree\nfrom numpy import finfo\n\nimport torch\nfrom distributed import apply_gradient_allreduce\nimport torch.distributed as dist\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import DataLoader\n\nfrom model import Tacotron2\nfrom data_utils import TextMelLoader, TextMelCollate\nfrom loss_function import Tacotron2Loss\nfrom logger import Tacotron2Logger\nfrom hparams import create_hparams\n \nimport random\nimport numpy as np\n\nimport layers\nfrom utils import load_wav_to_torch, load_filepaths_and_text\nfrom text import text_to_sequence\nfrom math import e\nfrom tqdm.notebook import tqdm # Modern Notebook TQDM\nfrom distutils.dir_util import copy_tree\nimport matplotlib.pylab as plt\n\ndef create_mels():\n    print(\"Generating Mels\")\n    stft = layers.TacotronSTFT(\n                hparams.filter_length, hparams.hop_length, hparams.win_length,\n                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n                hparams.mel_fmax)\n    def save_mel(filename):\n        audio, sampling_rate = load_wav_to_torch(filename)\n        if sampling_rate != stft.sampling_rate:\n            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename, \n                sampling_rate, stft.sampling_rate))\n        audio_norm = audio \/ hparams.max_wav_value\n        audio_norm = audio_norm.unsqueeze(0)\n        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n        melspec = stft.mel_spectrogram(audio_norm)\n        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n        np.save(filename.replace('.wav', ''), melspec)\n\n    import glob\n    wavs = glob.glob('\/wavs\/*.wav')\n    for i in tqdm(wavs):\n        save_mel(i)\n\n\ndef reduce_tensor(tensor, n_gpus):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt \/= n_gpus\n    return rt\n\n\ndef init_distributed(hparams, n_gpus, rank, group_name):\n    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n    print(\"Initializing Distributed\")\n\n    # Set cuda device so everything is done on the right GPU.\n    torch.cuda.set_device(rank % torch.cuda.device_count())\n\n    # Initialize distributed communication\n    dist.init_process_group(\n        backend=hparams.dist_backend, init_method=hparams.dist_url,\n        world_size=n_gpus, rank=rank, group_name=group_name)\n\n    print(\"Done initializing distributed\")\n\n\ndef prepare_dataloaders(hparams):\n    # Get data, data loaders and collate function ready\n    trainset = TextMelLoader(hparams.training_files, hparams)\n    valset = TextMelLoader(hparams.validation_files, hparams)\n    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n\n    if hparams.distributed_run:\n        train_sampler = DistributedSampler(trainset)\n        shuffle = False\n    else:\n        train_sampler = None\n        shuffle = True\n\n    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n                              sampler=train_sampler,\n                              batch_size=hparams.batch_size, pin_memory=False,\n                              drop_last=True, collate_fn=collate_fn)\n    return train_loader, valset, collate_fn\n\n\ndef prepare_directories_and_logger(output_directory, log_directory, rank):\n    if rank == 0:\n        if not os.path.isdir(output_directory):\n            os.makedirs(output_directory)\n            os.chmod(output_directory, 0o775)\n        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n    else:\n        logger = None\n    return logger\n\n\ndef load_model(hparams):\n    model = Tacotron2(hparams).cuda()\n    if hparams.fp16_run:\n        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n\n    if hparams.distributed_run:\n        model = apply_gradient_allreduce(model)\n\n    return model\n\n\ndef warm_start_model(checkpoint_path, model, ignore_layers):\n    assert os.path.isfile(checkpoint_path)\n    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n    model_dict = checkpoint_dict['state_dict']\n    if len(ignore_layers) > 0:\n        model_dict = {k: v for k, v in model_dict.items()\n                      if k not in ignore_layers}\n        dummy_dict = model.state_dict()\n        dummy_dict.update(model_dict)\n        model_dict = dummy_dict\n    model.load_state_dict(model_dict)\n    return model\n\n\ndef load_checkpoint(checkpoint_path, model, optimizer):\n    assert os.path.isfile(checkpoint_path)\n    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n    model.load_state_dict(checkpoint_dict['state_dict'])\n    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n    learning_rate = checkpoint_dict['learning_rate']\n    iteration = checkpoint_dict['iteration']\n    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n        checkpoint_path, iteration))\n    return model, optimizer, learning_rate, iteration\n\n\ndef save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n        iteration, filepath))\n    try:\n        torch.save({'iteration': iteration,\n                'state_dict': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n                'learning_rate': learning_rate}, filepath)\n    except KeyboardInterrupt:\n        print(\"interrupt received while saving, waiting for save to complete.\")\n        torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n    print(\"Model Saved\")\n\ndef plot_alignment(alignment, info=None):\n    %matplotlib inline\n    fig, ax = plt.subplots(figsize=(int(alignment_graph_width\/100), int(alignment_graph_height\/100)))\n    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n                   interpolation='none')\n    ax.autoscale(enable=True, axis=\"y\", tight=True)\n    fig.colorbar(im, ax=ax)\n    xlabel = 'Decoder timestep'\n    if info is not None:\n        xlabel += '\\n\\n' + info\n    plt.xlabel(xlabel)\n    plt.ylabel('Encoder timestep')\n    plt.tight_layout()\n    fig.canvas.draw()\n    plt.show()\n\ndef validate(model, criterion, valset, iteration, batch_size, n_gpus,\n             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n    \"\"\"Handles all the validation scoring and printing\"\"\"\n    model.eval()\n    with torch.no_grad():\n        val_sampler = DistributedSampler(valset) if distributed_run else None\n        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n                                shuffle=False, batch_size=batch_size,\n                                pin_memory=False, collate_fn=collate_fn)\n\n        val_loss = 0.0\n        for i, batch in enumerate(val_loader):\n            x, y = model.parse_batch(batch)\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            if distributed_run:\n                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n            else:\n                reduced_val_loss = loss.item()\n            val_loss += reduced_val_loss\n        val_loss = val_loss \/ (i + 1)\n\n    model.train()\n    if rank == 0:\n        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)\/60, learning_rate))\n        logger.log_validation(val_loss, model, y, y_pred, iteration)\n        if hparams.show_alignments:\n            %matplotlib inline\n            _, mel_outputs, gate_outputs, alignments = y_pred\n            idx = random.randint(0, alignments.size(0) - 1)\n            plot_alignment(alignments[idx].data.cpu().numpy().T)\n\ndef train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n          rank, group_name, hparams, log_directory2):\n    \"\"\"Training and validation logging results to tensorboard and stdout\n\n    Params\n    ------\n    output_directory (string): directory to save checkpoints\n    log_directory (string) directory to save tensorboard logs\n    checkpoint_path(string): checkpoint path\n    n_gpus (int): number of gpus\n    rank (int): rank of current gpu\n    hparams (object): comma separated list of \"name=value\" pairs.\n    \"\"\"\n    if hparams.distributed_run:\n        init_distributed(hparams, n_gpus, rank, group_name)\n\n    torch.manual_seed(hparams.seed)\n    torch.cuda.manual_seed(hparams.seed)\n\n    model = load_model(hparams)\n    learning_rate = hparams.learning_rate\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n                                 weight_decay=hparams.weight_decay)\n\n    if hparams.fp16_run:\n        from apex import amp\n        model, optimizer = amp.initialize(\n            model, optimizer, opt_level='O2')\n\n    if hparams.distributed_run:\n        model = apply_gradient_allreduce(model)\n\n    criterion = Tacotron2Loss()\n\n    logger = prepare_directories_and_logger(\n        output_directory, log_directory, rank)\n\n    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n\n    # Load checkpoint if one exists\n    iteration = 0\n    epoch_offset = 0\n    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n        if warm_start:\n            model = warm_start_model(\n                checkpoint_path, model, hparams.ignore_layers)\n        else:\n            model, optimizer, _learning_rate, iteration = load_checkpoint(\n                checkpoint_path, model, optimizer)\n            if hparams.use_saved_learning_rate:\n                learning_rate = _learning_rate\n            iteration += 1  # next iteration is iteration + 1\n            epoch_offset = max(0, int(iteration \/ len(train_loader)))\n    else:\n      os.path.isfile(\"pretrained_model\")\n#       %cd \/kaggle\/working\n#       !\/output\/kaggle\/working\/tacotron2\/megadown.sh https:\/\/mega.nz\/#!WXY3RILA!KyoGHtfB_sdhmLFoykG2lKWhh0GFdwMkk7OwAjpQHRo --o pretrained_model\n      !gdown https:\/\/drive.google.com\/uc?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA -O pretrained_model\n#       %cd tacotron2\n      model = warm_start_model(\"pretrained_model\", model, hparams.ignore_layers)\n      # download LJSpeech pretrained model if no checkpoint already exists\n    \n    start_eposh = time.perf_counter()\n    learning_rate = 0.0\n    model.train()\n    is_overflow = False\n    # ================ MAIN TRAINNIG LOOP! ===================\n    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n        start_eposh = time.perf_counter() # eposh is russian, not a typo\n        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n            start = time.perf_counter()\n            if iteration < hparams.decay_start: learning_rate = hparams.A_\n            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted\/hparams.B_))) + hparams.C_\n            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = learning_rate\n\n            model.zero_grad()\n            x, y = model.parse_batch(batch)\n            y_pred = model(x)\n\n            loss = criterion(y_pred, y)\n            if hparams.distributed_run:\n                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n            else:\n                reduced_loss = loss.item()\n            if hparams.fp16_run:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            if hparams.fp16_run:\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n                is_overflow = math.isnan(grad_norm)\n            else:\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), hparams.grad_clip_thresh)\n\n            optimizer.step()\n\n            if not is_overflow and rank == 0:\n                duration = time.perf_counter() - start\n                logger.log_training(\n                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n\n            iteration += 1\n        validate(model, criterion, valset, iteration,\n                 hparams.batch_size, n_gpus, collate_fn, logger,\n                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n        save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n        if log_directory2 != None:\n            copy_tree(log_directory, log_directory2)\ndef check_dataset(hparams):\n    from utils import load_wav_to_torch, load_filepaths_and_text\n    import os\n    import numpy as np\n    def check_arr(filelist_arr):\n        for i, file in enumerate(filelist_arr):\n            if len(file) > 2:\n                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n            if hparams.load_mel_from_disk and '.wav' in file[0]:\n                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n            else:\n                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n            if (not os.path.exists(file[0])):\n                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n            if len(file[1]) < 3:\n                print(\"|\".join(file), \"\\n[info] has no\/very little text.\")\n            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n            mel_length = 1\n            if hparams.load_mel_from_disk and '.npy' in file[0]:\n                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n                mel_length = melspec.shape[1]\n            if mel_length == 0:\n                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n    print(\"Checking Training Files\")\n    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n    check_arr(audiopaths_and_text)\n    print(\"Checking Validation Files\")\n    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n    check_arr(audiopaths_and_text)\n    print(\"Finished Checking\")\n\nwarm_start=False\nn_gpus=1\nrank=0\ngroup_name=None\n\n# ---- DEFAULT PARAMETERS DEFINED HERE ----\nhparams = create_hparams()\nmodel_filename = 'current_model'\nhparams.training_files = \"filelists\/clipper_train_filelist.txt\"\nhparams.validation_files = \"filelists\/clipper_val_filelist.txt\"\nhparams.p_attention_dropout=0.1\nhparams.p_decoder_dropout=0.1\nhparams.decay_start = 15000\nhparams.A_ = 5e-4\nhparams.B_ = 8000\nhparams.C_ = 0\nhparams.min_learning_rate = 1e-5\ngenerate_mels = True\nhparams.show_alignments = True\nalignment_graph_height = 600\nalignment_graph_width = 1000\nhparams.batch_size = 32\nhparams.load_mel_from_disk = True\nhparams.ignore_layers = []\nhparams.epochs = 10000\ntorch.backends.cudnn.enabled = hparams.cudnn_enabled\ntorch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n\ndata_path = 'wavs'\n!mkdir {data_path}","9877f01f":"# Variables\ndataset_name = \"none\"\ntraining_list_name = \"none\"\n\n# Inputs\nwhile dataset_name == \"none\":\n    dataset_name = input(\"What is the name of your dataset?: \")\n\nwhile training_list_name == \"none\":\n    training_list_name = input(\"What is the name of your training transcription file (add file extention: eg .txt)?: \")\n\n# Training list\nwith open(f\"..\/..\/input\/{dataset_name}\/{training_list_name}\") as f:\n    with open(f\"filelists\/{training_list_name}\", \"w\") as f1:\n        for line in f:\n            f1.write(f'\/'+line)\n\n# User Completion\nprint(\"Done, text file has been copied in the right location.\")","4d4565a8":"model_filename = \"none\"\nwhile model_filename == \"none\":\n    model_filename = input(\"What is your desired model name?: \")\n\nTraining_file = f\"filelists\/{training_list_name}\"\n    \nhparams.training_files = Training_file\nhparams.validation_files = Training_file\n\n# hparams to Tune\nhparams.p_attention_dropout=0.1\nhparams.p_decoder_dropout=0.1\n\n# Learning Rate\nhparams.decay_start = 15000         # wait till decay_start to start decaying learning rate\nhparams.A_ = 5e-4                   # Start\/Max Learning Rate\nhparams.B_ = 8000                   # Decay Rate\nhparams.C_ = 0                      # Shift learning rate equation by this value\nhparams.min_learning_rate = 1e-5    # Min Learning Rate\n\n# Quality of Life\ngenerate_mels = True\nhparams.show_alignments = True\nalignment_graph_height = 600\nalignment_graph_width = 1000\n\n#Your batch size, lower if you don't have enough ram.\nhparams.batch_size = 14 #18\nhparams.load_mel_from_disk = True\nhparams.ignore_layers = [] # Layers to reset (None by default, other than foreign languages this param can be ignored)\n\nhparams.epochs = -1\nwhile hparams.epochs <= -1:\n    hparams.epochs = int(input(\"Your total epochs to train to (recommended: 250): \"))\n\ntorch.backends.cudnn.enabled = hparams.cudnn_enabled\ntorch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n\noutput_directory = '..\/outdir'\nlog_directory = '\/logs' # Location to save Log files locally\nlog_directory2 = '..\/outdir\/logs' # Location to copy log files (done at the end of each epoch to cut down on I\/O)\ncheckpoint_path = output_directory+(r'\/')+model_filename\n\n# User Completion\nprint(\"Done, Settings applied\")","713207f3":"os.system(f'cp -a ..\/..\/input\/{dataset_name}\/wavs \/')\nprint(\"Files have successfully been copied over\")","734dedf9":"print(\"Generating mels\")\nif generate_mels:\n    create_mels()\n\nprint(\"Checking for missing files\")\n# ---- Replace .wav with .npy in filelists ----\n!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}; sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n\ncheck_dataset(hparams)","7a5d9459":"CONTINUEQ = input(\"Are you continuing to train your model? [Y\/N]: \").upper()\nwhile CONTINUEQ not in (\"Y\",\"N\"):\n    CONTINUEQ = input(\"Please only enter Y or N\\nAre you continuing to train your model? [Y\/N]: \")\n\nif CONTINUEQ == \"Y\":\n    os.system('mkdir ..\/outdir')\n    print(\"NOT THE MODEL FILENAME ITSELF, ONLY THE NAME YOU HAD GIVEN IT WHEN IMPORTING AS DATA INTO KAGGLE\")\n    MODELDATA_FILENAME = input(\"What is the name of the name you had assigned your TACOTRON2 MODEL within your KAGGLE ACCOUNT?: \")\n    os.system(f'cp -r ..\/..\/input\/{MODELDATA_FILENAME}\/{model_filename} ..\/outdir\/')\nelse:\n    print(\"Ok\")","2526e364":"print(\"FP16 Run:\", hparams.fp16_run)\nprint(\"Dynamic Loss Scaling:\", hparams.dynamic_loss_scaling)\nprint(\"Distributed Run:\", hparams.distributed_run)\nprint(\"cuDNN Enabled:\", hparams.cudnn_enabled)\nprint(\"cuDNN Benchmark:\", hparams.cudnn_benchmark)\ntrain(output_directory, log_directory, checkpoint_path, warm_start, n_gpus, rank, group_name, hparams, log_directory2)","e4b455dd":"![image.png](attachment:7966d94d-378f-4cd6-b7da-d1cb3460bf37.png)","c2e6db19":"# 6 Train the model","6ec044b8":"# **Good training will look like this:**","7507a6e1":"# 3 Setting Model Parameters\nEnter your model name and your EPOCH limit to train toward","249fe8fd":"# (Optional) Check GPU type","df86541f":"---","c64190b6":"\n\n# **Tacotron 2 Training (Kaggle Notebook)**\n---\nTacotron 2 | **Created by ColdFir3#9543 (Michael)**\n\n**MAKE SURE 'GPU' HAS BEEN SELECTED AS THE ACCELERATOR IN THE NOTEBOOK SETTINGS**\n\n*UPDATED 28\/09\/21 VERSION 13: Fixed the issue with not being able to load in previous models to continue training*\n\n*UPDATED 30\/09\/21 VERSION 14: Made instructions a little more clearer to understand and made the code a little easier to use*\n\n*UPDATED 09\/10\/21 VERSION 15: Fixed bugs*\n\n---\n# TRAINING INSTRUCTIONS\n\n* **Make sure to make your own version of this notebook for each new model**\n\n* Transcription file should look be in this format for each wav (WITH PUNCTUATION): ```wavs\/1.wav|[Text here].```\n\n* Dataset WAVS format should be: **22050HZ and 16 bit PCM**\n\n* **ONLY IMPORT YOUR DATASET ONCE** *(should consist of a folder called 'wavs' with all audio clips and another file for your transcriptions)*\n\nExample: \n\n```\nKaggle Dataset\/\n          \u251c\u2500\u2500wavs\/\n          \u2502    \u251c\u2500\u25001.wav\n          \u2502    \u251c\u2500\u25002.wav\n          \u2502    \u251c\u2500\u25003.wav\n          \u2502    \u2514\u2500\u2500etc\n          \u2514\u2500\u2500transcription.txt\n```\n* **MAKE SURE TO RUN ALL AND ENTER REQUIRED DATA**\n\n* ***(VERY IMPORTANT)*** Once your model has been trained **DO NOT FORGET TO SAVE VERSION AND GO TO 'ADVANCED' AND CHECK 'ALWAYS SAVE OUTPUT'** so you do NOT LOSE progress\n---\n# CONTINUING TO TRAIN?\n* If returning back to continue training your model **MAKE SURE TO RUN ALL AND ENTER REQUIRED DATA**\n\n* Import your Model as a \"dataset\" **SEPERATELY** from the training dataset\n\n* **DON'T MAKE ANY OF THE SETTINGS DIFFERENT FROM YOUR PREVIOUS TRAINING**\n\n* Recommended to name your \"input\" for your AI Model to be the same name as the model itself **(WILL SAVE LOTS OF TROUBLE AT STEP 5.5)**\n\n* ***(VERY IMPORTANT)*** Once your model has been trained **DO NOT FORGET TO SAVE VERSION USING 'QUICK SAVE' AS THE OPTION AND GO TO 'ADVANCED' AND CHECK 'ALWAYS SAVE OUTPUT'** so you do NOT LOSE progress\n\n---","5bf63722":"# 1 Run the below block of code to download and install Tacotron 2 and dependancies","2a8ab028":"# 7 Download Model\n\n**DOUBLE CLICK THIS BOX AND CHANGE 'EDIT' to the name of your model, then click outside the box and click 'Download File'**\n\n<a href=\"..\/outdir\/EDIT\"> Download File <\/a>","f88a1c70":"# 2 Create file lists from train and validation sets\nEnter the name of your dataset and the name of your training transcription","47b149ed":"---","b3cac2c7":"# 4 Move files from Dataset to working directory","78d5b0b2":"# **(CONTINUING TO TRAIN?)** 5.5 Transfer Model into the working directory","6f528ceb":"---","22cffaa0":"---","bdb0d6dc":"---","4c4fe355":"---","38f1740b":"---","f0a76f92":"# 5 Convert WAV files into Mel-Spectograms and Checks missing files","ae867c72":"---"}}