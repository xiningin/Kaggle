{"cell_type":{"647d1b37":"code","69853fff":"code","b7dd15ca":"code","b202d2db":"code","f224e5c9":"code","ad210180":"code","205a5701":"code","61383b5d":"code","f69c28bd":"code","ecaf7e6e":"code","38bff38a":"code","29b8157c":"code","a47ef175":"code","5443fcba":"code","b6872397":"code","b9e8ad03":"code","bfc8bc88":"code","48dd5518":"code","ddceebfb":"code","6c520697":"code","c29ee296":"code","8e943e2f":"code","aad2a433":"code","4672db99":"code","bf076a40":"code","4efd3beb":"code","14ddd66e":"code","b96efd0e":"code","1d2401a9":"code","67a4a862":"code","36212d5b":"code","456f798d":"code","ce860df4":"code","744d8ea3":"code","204095f9":"code","dbceae6f":"code","ba7cc745":"code","0eccee9f":"code","bb8626f6":"code","8309f97c":"markdown","69e8bfd6":"markdown","ad04b176":"markdown","d7f432c9":"markdown","01edd6d5":"markdown","412a797b":"markdown","fd0184c7":"markdown","ad5a78d2":"markdown","0b6e866a":"markdown","628b800b":"markdown","e0421e84":"markdown","a4858350":"markdown","92ec9e3b":"markdown","001d2227":"markdown","8a963d43":"markdown","039e1492":"markdown","723d84b4":"markdown","3e872bd4":"markdown","ae95aa04":"markdown","b5645a3f":"markdown","f39686ed":"markdown","928aa45a":"markdown","2877d6ce":"markdown","d4c1ab00":"markdown","bdea1306":"markdown","24a84f90":"markdown","b22d167e":"markdown","bbc056d1":"markdown","ee04a3c1":"markdown","42e5b28c":"markdown","c067f890":"markdown"},"source":{"647d1b37":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nPATH = '..\/input\/store-sales-time-series-forecasting\/'","69853fff":"train = pd.read_csv(PATH + 'train.csv', dtype={'store_nbr': 'category'}, \n                    usecols=['store_nbr', 'family', 'date', 'sales', 'onpromotion'])\ntest = pd.read_csv(PATH + 'test.csv', dtype={'store_nbr': 'category'},\n                   usecols=['store_nbr', 'family', 'date', 'onpromotion'])\n\n# Check for missing values\nprint('Missing values in train:', train.isna().sum().sum())\nprint('Missing values in test:', test.isna().sum().sum())\n\n# There are some missing dates in training set ['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'] \n\ntrain['date'] = pd.to_datetime(train['date'])\ntest['date'] = pd.to_datetime(test['date'])","b7dd15ca":"train.head(20)","b202d2db":"temp = train.set_index('date').groupby('store_nbr').resample('D').sales.sum().reset_index()\npx.line(temp, x='date', y='sales', color='store_nbr',\n        title='Daily total sales of the stores')","f224e5c9":"print(train.shape)\ntrain = train[~((train.store_nbr == '52') & (train.date < \"2017-04-20\"))]\ntrain = train[~((train.store_nbr == '22') & (train.date < \"2015-10-09\"))]\ntrain = train[~((train.store_nbr == '42') & (train.date < \"2015-08-21\"))]\ntrain = train[~((train.store_nbr == '21') & (train.date < \"2015-07-24\"))]\ntrain = train[~((train.store_nbr == '29') & (train.date < \"2015-03-20\"))]\ntrain = train[~((train.store_nbr == '20') & (train.date < \"2015-02-13\"))]\ntrain = train[~((train.store_nbr == '53') & (train.date < \"2014-05-29\"))]\ntrain = train[~((train.store_nbr == '36') & (train.date < \"2013-05-09\"))]\nprint(train.shape)","ad210180":"train.corr('spearman').sales.loc['onpromotion']","205a5701":"transactions = pd.read_csv(PATH + 'transactions.csv', dtype={'store_nbr': 'category'})\ntransactions.head()","61383b5d":"print('Missing values in transactions:', train.isna().sum().sum())\n\n# Similar to training set, we have missing dates ['2013-12-25', '2014-12-25', '2015-12-25', \n#                                                 '2016-01-01', '2016-01-03', '2016-12-25']\n\ntransactions['date'] = pd.to_datetime(transactions['date'])\n\n# Proof that transactions are highly correlated with sales\ntemp = pd.merge(train.groupby(['date', 'store_nbr']).sales.sum().reset_index(),\n                transactions, how='left')\nprint(temp.corr(\"spearman\").sales.loc[\"transactions\"])\n\n# Now we can proof that stores on holidays make more money than on working days\ntemp = transactions.copy()\ntemp['year'] = temp.date.dt.year\ntemp['day_of_week'] = temp.date.dt.dayofweek + 1\ntemp = temp.groupby(['year', 'day_of_week']).transactions.mean().reset_index()\n\npx.line(temp, x='day_of_week', y='transactions', color='year', title='Transactions')","f69c28bd":"def strip_spaces(a_str_with_spaces):\n    return a_str_with_spaces.replace(' ', '')\n\nholidays = pd.read_csv(PATH + 'holidays_events.csv', index_col='date',\n                       parse_dates=['date'], infer_datetime_format=True,\n                       converters={'locale_name': strip_spaces})  # removes spaces from locale_name\n\nholidays.head()","ecaf7e6e":"# By printing unique labels, we can check data on misspells, and get better data understanding\nprint('Holidays types:', holidays['type'].unique())\nprint('Holidays region types:', holidays['locale'].unique()) \nprint('Holidays locale names:', holidays['locale_name'].unique())  ","38bff38a":"# What about missing values\nholidays.isna().sum()  ","29b8157c":"# Calendar\nholidays_rdy = pd.DataFrame(index=pd.date_range('2013-01-01', '2017-08-31'))\nholidays_rdy['day_of_week'] = holidays_rdy.index.dayofweek + 1  # Monday = 1, Sunday = 7\nholidays_rdy['work_day'] = True\nholidays_rdy.loc[holidays_rdy['day_of_week'] > 5, 'work_day'] = False  # False for saturdays and sundays \n\n# Fixing index duplicates in holidays dataset\nduplicates = holidays[holidays.index.duplicated(keep=False)]\nprint(duplicates['locale_name'])\n\n# This was done manually\nduplicates = [('2012-06-25', 'Latacunga Machala'), ('2012-07-03', 'ElCarmen'),\n              ('2012-12-22', 'Ecuador'), ('2012-12-24', 'Ecuador'),\n              ('2012-12-31', 'Ecuador'), ('2013-05-12', 'Ecuador'),\n              ('2013-06-25', 'Machala Latacunga'), ('2013-07-03', 'SantoDomingo'),\n              ('2013-12-22', 'Salinas'), ('2014-06-25', 'Machala Imbabura Ecuador'),\n              ('2014-07-03', 'SantoDomingo'), ('2014-12-22', 'Ecuador'),\n              ('2014-12-26', 'Ecuador'), ('2015-06-25', 'Imbabura Latacunga'),\n              ('2015-07-03', 'SantoDomingo'), ('2015-12-22', 'Salinas'),\n              ('2016-04-21', 'Ecuador'), ('2016-05-01', 'Ecuador'),\n              ('2016-05-07', 'Ecuador'), ('2016-05-08', 'Ecuador'),\n              ('2016-05-12', 'Ecuador'), ('2016-06-25', 'Imbabura Latacunga'),\n              ('2016-07-03', 'SantoDomingo'), ('2016-07-24', 'Guayaquil'),\n              ('2016-11-12', 'Ecuador'), ('2016-12-22', 'Salinas'),\n              ('2017-04-14', 'Ecuador'), ('2017-06-25', 'Latacunga Machala'),\n              ('2017-07-03', 'SantoDomingo'), ('2017-12-08', 'Quito'),\n              ('2017-12-22', 'Ecuador')]\n# No holidays was transferred in duplicates\n\nholidays = holidays.groupby(holidays.index).first() # we left only first, but we need others too\nfor date, locale_name in duplicates:\n    holidays.loc[date, 'locale_name'] = holidays.loc[date, 'locale_name'] + ' ' + locale_name","a47ef175":"# Apply holidays to calendar\nholidays_rdy = holidays_rdy.merge(holidays, how='left', left_index=True, right_index=True)\n\n# type column: 'Work Day'\nholidays_rdy.loc[holidays_rdy['type'] == 'Work Day', 'work_day'] = True\n\n# type column: 'Holiday', 'Transfer', 'Additional', 'Bridge'\nholidays_rdy.loc[(holidays_rdy['type'] == 'Holiday') &\n                 (holidays_rdy['locale_name'].str.contains('Ecuador', na=False)),\n                 'work_day'] = False\nholidays_rdy.loc[(holidays_rdy['type'] == 'Transfer') & \n                 (holidays_rdy['locale_name'].str.contains('Ecuador', na=False)),  \n                 'work_day'] = False\nholidays_rdy.loc[(holidays_rdy['type'] == 'Additional') & \n                 (holidays_rdy['locale_name'].str.contains('Ecuador', na=False)),\n                 'work_day'] = False\nholidays_rdy.loc[(holidays_rdy['type'] == 'Bridge') & \n                 (holidays_rdy['locale_name'].str.contains('Ecuador', na=False)),   \n                 'work_day'] = False\n\nholidays_rdy.drop(['locale'], axis=1, inplace=True)\n\n# transferred column\nholidays_rdy.loc[holidays_rdy['transferred'] == True, 'work_day'] = True","5443fcba":"# First let's look at events\nevents = holidays_rdy[holidays_rdy['type']=='Event']\nevents","b6872397":"# I do it for simplicity\nholidays_rdy.loc[holidays_rdy['description'].str.contains('Terremoto', na=False),\n                 'description'] = 'Earthquake'\nholidays_rdy.loc[holidays_rdy['description'].str.contains('futbol', na=False), \n                 'description'] = 'Football'\nevents = holidays_rdy[holidays_rdy['type']=='Event']\n\n# Check for misspells\nprint(events['description'].unique())\n\n# Print mean sales \nsales = train.groupby(['date']).sales.sum()\nevents = events.merge(sales, how='left', left_index=True, right_index=True)\nprint(events.groupby(['description']).sales.mean())\nprint('All sales mean:', sales.mean())","b9e8ad03":"# descriptions \ndescriptions = pd.get_dummies(holidays_rdy['description'])[['Earthquake', 'Cyber Monday', 'Black Friday']]\nholidays_rdy = holidays_rdy.merge(descriptions, how='left', left_index=True, right_index=True)\n\n# Fill NaNs\nholidays_rdy['locale_name'].fillna('Ecuador', inplace=True)\n\n# Get rid of useless columns\nholidays_rdy.drop(['type', 'description', 'transferred'], axis=1, inplace=True)","bfc8bc88":"# If you want to merge two dataframes, they should have same indexes, later we will need it\nholidays_rdy['date'] = holidays_rdy.index\nholidays_rdy['date'] = pd.to_datetime(holidays_rdy['date'])\nholidays_rdy['date'] = holidays_rdy['date'].dt.to_period('D')\nholidays_rdy = holidays_rdy.set_index(['date'])\n\nholidays_rdy = pd.get_dummies(holidays_rdy, columns=['day_of_week'])\n\nholidays_rdy.head()","48dd5518":"oil = pd.read_csv(PATH + 'oil.csv')\noil['date'] = pd.to_datetime(oil['date'])\noil.head()","ddceebfb":"# Resample\noil = oil.set_index('date')['dcoilwtico'].resample(\n    'D').sum().reset_index()  # add missing dates and fill NaNs with 0 \n\n# Interpolate\noil['dcoilwtico'] = np.where(oil['dcoilwtico']==0, np.nan, oil['dcoilwtico'])  # replace 0 with NaN\noil['dcoilwtico_interpolated'] = oil.dcoilwtico.interpolate()  # fill NaN values using an interpolation method\n\noil.head(10)","6c520697":"temp = oil.melt(id_vars=['date'], var_name='Legend') \npx.line(temp.sort_values(['Legend', 'date'], ascending=[False, True]), x='date',\n        y='value', color='Legend', title='Daily Oil Price')","c29ee296":"oil_rdy = oil.loc[:, ['date', 'dcoilwtico_interpolated']]\noil_rdy.iloc[0, 1] = 93.1\n\nassert oil_rdy.isna().sum().sum() == 0\n\noil_rdy['date'] = pd.to_datetime(oil_rdy['date'])\noil_rdy['date'] = oil_rdy['date'].dt.to_period('D')\noil_rdy = oil_rdy.set_index(['date'])\noil_rdy","8e943e2f":"import matplotlib.pyplot as plt\n\ndef plot_sales_and_oil_dependency():\n    a = pd.merge(train.groupby([\"date\", \"family\"]).sales.sum().reset_index(),\n                 oil.drop(\"dcoilwtico\", axis=1), how=\"left\")\n    c = a.groupby(\"family\").corr(\"spearman\").reset_index()\n    c = c[c.level_1 == \"dcoilwtico_interpolated\"][[\"family\", \"sales\"]].sort_values(\"sales\")\n    \n    fig, axes = plt.subplots(7, 5, figsize=(20, 20))\n    for i, fam in enumerate(c.family):\n        a[a.family == fam].plot.scatter(x=\"dcoilwtico_interpolated\", y=\"sales\", ax=axes[i \/\/ 5, i % 5])\n        axes[i \/\/ 5, i % 5].set_title(fam + \"\\n Correlation:\" + str(c[c.family == fam].sales.iloc[0])[:6],\n                                 fontsize=12)\n        axes[i \/\/ 5, i % 5].axvline(x=70, color='r', linestyle='--')\n\n    plt.tight_layout(pad=5)\n    plt.suptitle(\"Daily Oil Product & Total Family Sales \\n\", fontsize=20)\n    plt.show()\n\nplot_sales_and_oil_dependency()","aad2a433":"oil_rdy['rolling_mean_7'] = oil_rdy['dcoilwtico_interpolated'].rolling(7).mean()\noil_rdy.fillna(93.1, inplace=True)\noil_rdy","4672db99":"from statsmodels.graphics.tsaplots import plot_pacf\n\n_ = plot_pacf(oil_rdy.rolling_mean_7, lags=12, method='ywm')  # 1 lag","bf076a40":"for i in range(1, 2) :\n    oil_rdy[f'oil_lag_{i}'] = oil_rdy.rolling_mean_7.shift(i)\noil_rdy.fillna(93.1, inplace=True)","4efd3beb":"stores = pd.read_csv(PATH + 'stores.csv', index_col='store_nbr',\n                     converters={'city': strip_spaces, 'state': strip_spaces})  # removes spaces\n\nstores.head()","14ddd66e":"# Let's look at the unique labels\nprint('Cities:\\n', stores['city'].unique())  \nprint('States:\\n', stores['state'].unique())  \nprint('Store types:\\n', stores['type'].unique())  # no type information was provided in data description\nprint('Clusters:\\n', sorted(list(stores['cluster'].unique())))\n\n\n# Do not forget about missing values\nprint('Missing values:', stores.isna().sum().sum())","b96efd0e":"stores_rdy = stores.loc[:, ['city', 'state']]\nstores_rdy.head()","1d2401a9":"train['date'] = train['date'].dt.to_period('D')\ntrain_rdy = train.set_index(['store_nbr', 'family', 'date']).sort_index()\ndisplay(train_rdy)\n\ntest['date'] = test['date'].dt.to_period('D')\ntest_rdy = test.set_index(['store_nbr', 'family', 'date']).sort_index()\ndisplay(test_rdy)","67a4a862":"sdate = '2017-03-25' # start and end of training date\nedate = '2017-08-15'\n\ny_arr = []\nonpromotion_arr = []\ntest_onpromotion_arr = []\n\nfor nbr in stores.index:\n    # y_arr\n    temp = train_rdy.loc[str(nbr), 'sales']\n    y_arr.append(temp.unstack(['family']).loc[sdate:edate])\n    \n    # onpromotion_arr\n    onpromotion = train_rdy.loc[str(nbr), 'onpromotion']\n    onpromotion = onpromotion.unstack(['family']).loc[sdate:edate].sum(axis=1)\n    onpromotion.name = 'onpromotion'\n    onpromotion_arr.append(onpromotion)\n    \n    # test_onpromotion_arr\n    test_onpromotion = test_rdy.loc[str(nbr), 'onpromotion']\n    test_onpromotion = test_onpromotion.unstack(['family']).sum(axis=1)\n    test_onpromotion.name = 'onpromotion'\n    test_onpromotion_arr.append(test_onpromotion)\n    \n    # sum of all products that are on promotion in the currect store\n    \n    \ny_arr[1] # sales of store_nbr 2","36212d5b":"from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n\nfourier = CalendarFourier(freq='W', order=4)\nX_arr = []\nX_test_arr = []\nstore_index = 1\n\nfor y, onpromotion, test_onpromotion in zip(y_arr, onpromotion_arr, test_onpromotion_arr): \n    dp = DeterministicProcess(index=y.index,\n                              constant=False,\n                              order=1,\n                              seasonal=False,\n                              additional_terms=[fourier],\n                              drop=True)\n    X = dp.in_sample()\n    X_test = dp.out_of_sample(steps=16)\n    \n    # On promotion\n    X = X.merge(onpromotion, how='left', left_index=True, right_index=True)\n    X_test = X_test.merge(test_onpromotion, how='left', left_index=True, right_index=True)\n    \n    # Holidays\n    X = X.merge(holidays_rdy, how='left', left_index=True, right_index=True)\n    X_test = X_test.merge(holidays_rdy, how='left', left_index=True, right_index=True)\n    \n    store_state = stores.loc[store_index, 'state']\n    store_city = stores.loc[store_index, 'city']\n    \n    # Apply local holidays\n    for j in X.index: \n        if X.loc[j, 'locale_name'].find(store_state) != -1 or X.loc[j, 'locale_name'].find(store_city) != -1:\n            X.loc[j, 'work_day'] = False\n    \n    for j in X_test.index: \n        if X_test.loc[j, 'locale_name'].find(store_state) != -1 or X_test.loc[j, 'locale_name'].find(store_city) != -1:\n            X_test.loc[j, 'work_day'] = False\n    \n    X.drop(['locale_name'], axis=1, inplace=True)  \n    X_test.drop(['locale_name'], axis=1, inplace=True)  \n    \n    # Oil\n    X = X.merge(oil_rdy, how='left', left_index=True, right_index=True)\n    X_test = X_test.merge(oil_rdy, how='left', left_index=True, right_index=True)\n    \n    X_arr.append(X)\n    X_test_arr.append(X_test)\n    \n    store_index += 1\n    \nX_arr[0]","456f798d":"X_test_arr[0]","ce860df4":"from sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\n\nridge = make_pipeline(RobustScaler(),\n                      Ridge(alpha=31.0))\n\nt_errors = []\nv_errors = []\n\n# Collect errors for each store\nfor X, y in zip(X_arr, y_arr):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3,\n                                                      random_state=1, shuffle=False)\n    model = ridge.fit(X_train, y_train)\n    train_pred = pd.DataFrame(model.predict(X_train), index=X_train.index,\n                              columns=y_train.columns).clip(0.0)\n    val_pred = pd.DataFrame(model.predict(X_val), index=X_val.index,\n                            columns=y_val.columns).clip(0.0)\n\n    y_train = y_train.stack(['family']).reset_index()\n    y_train['pred'] = train_pred.stack(['family']).reset_index().loc[:, 0]\n\n    y_val = y_val.stack(['family']).reset_index()\n    y_val['pred'] = val_pred.stack(['family']).reset_index().loc[:, 0]\n\n    t_errors.append(y_train.groupby('family').apply(\n        lambda r: mean_squared_log_error(r.loc[:, 0], r['pred'])))\n    v_errors.append(y_val.groupby('family').apply(\n        lambda r: mean_squared_log_error(r.loc[:, 0], r['pred'])))","744d8ea3":"# Sum of mean squared log error from validation dataset\nsum(v_errors).sort_values(ascending=False)","204095f9":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n    \nfrom joblib import Parallel, delayed    \nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor, VotingRegressor\n\nclass CustomRegressor:\n    def __init__(self, n_jobs: int = -1, seed: int = 1):\n        self.n_jobs = n_jobs\n        self.seed = seed\n        self._estimators = None\n\n    def _get_model(self, x_, y_):\n        if y_.name == 'SCHOOL AND OFFICE SUPPLIES':\n            etr = ExtraTreesRegressor(n_estimators=500, n_jobs=self.n_jobs,\n                                      random_state=self.seed)\n            rfr = RandomForestRegressor(n_estimators=500, n_jobs=self.n_jobs,\n                                        random_state=self.seed)\n            br1 = BaggingRegressor(base_estimator=etr, n_estimators=10,\n                                   n_jobs=self.n_jobs, random_state=self.seed)\n            br2 = BaggingRegressor(base_estimator=rfr, n_estimators=10,\n                                   n_jobs=self.n_jobs, random_state=self.seed)\n            model = VotingRegressor([('ExtraTrees', br1), ('RandomForest', br2)])\n        else:\n            ridge = make_pipeline(RobustScaler(),\n                                  Ridge(alpha=31.0, random_state=self.seed))\n            svr = make_pipeline(RobustScaler(),\n                                SVR(C=1.68, epsilon=0.09, gamma=0.07))\n\n            model = VotingRegressor([('ridge', ridge), ('svr', svr)])\n\n        model.fit(x_, y_)\n        return model\n\n    def fit(self, x_, y_):\n        self._estimators = Parallel(n_jobs=self.n_jobs, verbose=0) \\\n            (delayed(self._get_model)(x_, y_.iloc[:, i]) for i in range(y_.shape[1]))\n\n    def predict(self, x_):\n        y_pred = Parallel(n_jobs=self.n_jobs, verbose=0) \\\n            (delayed(e.predict)(x_) for e in self._estimators)\n\n        return np.stack(y_pred, axis=1)","dbceae6f":"# Get fitted models\nmodels = []\nfor X, y in zip(X_arr, y_arr):\n    model = CustomRegressor()\n    model.fit(X, y)\n    models.append(model)","ba7cc745":"# Get predictions\nresults = []\nfor X_test, model, y in zip(X_test_arr, models, y_arr):\n    y_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns).clip(0.0)\n    results.append(y_pred.stack(['family']))\n    \nresults[0]","0eccee9f":"# Get correct dates for submission\ndates = ['2017-08-16', '2017-08-17', '2017-08-18', '2017-08-19', '2017-08-20', '2017-08-21',\n         '2017-08-22', '2017-08-23', '2017-08-24', '2017-08-25', '2017-08-26', '2017-08-27',\n         '2017-08-28', '2017-08-29', '2017-08-30', '2017-08-31']\n\n# Get correct order for submission\norder = list(range(1, len(results) + 1))\nstr_map = map(str, order)\ncorrect_order_str = sorted(list(str_map))\nint_minus_one = lambda element: int(element) - 1\ncorrect_order_int = list(map(int_minus_one, correct_order_str))\n\n# Create and fill list with predictions in the correct order\ndata = []\nfor date in dates:\n    for i in correct_order_int:\n        data += results[i].loc[date].to_list()\n\n# Create dataframe from the list\nresult = pd.DataFrame(data, columns = ['sales'])\n\n# We can use sample_submission.csv to make submission\nsubmission = pd.read_csv(PATH + 'sample_submission.csv')\nsubmission['sales'] = result['sales']\nsubmission","bb8626f6":"# Save submission\nsubmission.to_csv('submission.csv', index = False)","8309f97c":"The first thing I do, is reading dataset [notes](https:\/\/www.kaggle.com\/c\/store-sales-time-series-forecasting\/data), provided by author.    \nWe have 7 (actually 6) csv files to work with, each of them we should analyse individually.  \nOur target: predict sales for the thousands of product families sold at Favorita stores.   \nThere are 54 stores and my idea is to train the model on each store's data separately. \n\nNow let's get closer look at each table.\n\n## Train and Test datasets","69e8bfd6":"Without any digging, we can see missing prices and dates, which are better to fill in.  ","ad04b176":"## X","d7f432c9":"Here we can see that **SCHOOL AND OFFICE SUPPLIES** error is much higher than others.  \nWe need to create custom regressor, and specify different models to deal with this problem.","01edd6d5":"## Transactions","412a797b":"# Data Analysis (Understanding the datset and the task)","fd0184c7":"But what if oil prices don't influence sales? Why do we need an oil dataset?  \nHere you are.  \nFor some columns we can see strong correlation.","ad5a78d2":"Add **rolling mean** and **lags**  ","0b6e866a":"In our train and test datasets we have one interesting feature - **onpromotion**, which means total number of items in a product family that were being promoted at a giving date.   \nSounds like it should influence well on sales.","628b800b":"## Stores","e0421e84":"We already know about **type** and **transferred** columns from dataset description. And now we understand what **locale** and **locale_name** are.\n\n**Now**: we need to create a full calendar and specify working days and not working days.","a4858350":"We should connect stores and holidays by location, because holidays can be local or regional (in one city, or in the whole state).","92ec9e3b":"**Warning**: it will take a while","001d2227":"A lot of zeros, it can means that store was not working yet, or we just do not have infromation.  \nWe should plot train dataset, and watch closely.   ","8a963d43":"First of all, let's try Ridge Regressor and see at the results","039e1492":"I want to split training and testing datasets to train models for each store separately.  \nBecause of the local holidays.  \nYeah, maybe it's not the best idea, but lets try.  ","723d84b4":"All events are national, no events were transferred.  \nWe should set one label for all football events, same for earthquake.  ","3e872bd4":"Some of the stores don't have sales information until 2014, 2015, or 2017.   \nLet's fix it.  ","ae95aa04":"Not bad result, but how can we use it? I have one idea.","b5645a3f":"# Modelling","f39686ed":"# Preprocessing","928aa45a":"After the visual analysis, it is obvious that we should set holidays correctly to get better results.  \nThis is all for transactions, we do not need them for training models.  ","2877d6ce":"**type column: 'Event'**  \nThere are multiple events in dataset: **Mother's day**, **Footbal** championship, **Black Friday**, **Cyber Monday**, Manabi **Earthquake** (about a month long)  \nWe should understand how does it affect our sales.","d4c1ab00":"## y","bdea1306":"# Conclusion\nIn conclusion I want to say thank you to Kaggle comunity, you guys doing very cool stuff.  \nAll links to resources I used, you can find on the top of the notebook.\n\n**If you find any errors in this notebook, pls let me know.**","24a84f90":"**Didn't find good, noob friendly, notebook, so I decided to do it by myself with information I learned from other notebooks.**\n\nReferences:  \nhttps:\/\/www.kaggle.com\/hiro5299834\/store-sales-ridge-voting-bagging-et-bagging-rf  \nhttps:\/\/www.kaggle.com\/andrej0marinchenko\/hyperparamaters#DeterministicProcess  \nhttps:\/\/www.kaggle.com\/ekrembayar\/store-sales-ts-forecasting-a-comprehensive-guide  ","b22d167e":"I took already working well custom regressor, and made parameters tuning for Ridge and SVR.","bbc056d1":"## Holidays Events","ee04a3c1":"# Submission\nTo create submission we need concatenate all predictions in one dataframe.  \n**Note:** originaly data was sorted by store_nbr as string, so it looked like this 1, 10, 11, 12, ... 2, 20, 21, 22, ...  \nTo concatenate predictions correctly \n","42e5b28c":"Imprecise method because we do not have enough data, but **Earthquake** and **Cyber Monday** definitely should be considered during training, + **Black Friday**.  \nSales are not depends much on **Football** and **Mother's day**.","c067f890":"## Oil"}}