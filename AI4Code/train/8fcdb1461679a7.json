{"cell_type":{"9f3cf970":"code","6e9949dc":"code","32e724f6":"code","32b2ea41":"code","4136f87e":"code","85bdd59a":"code","bd65772e":"code","63c8173b":"code","4b90b2a2":"code","d54f307f":"code","54fd4a8c":"code","acbb371e":"code","06db1cbd":"code","2b9143f0":"code","36520d42":"code","95de61f5":"code","39089751":"code","92cda9c7":"code","caaf26aa":"code","a732917b":"code","357db2db":"code","ddfaaee9":"code","be35e6c4":"code","fe359bf9":"code","0214eaa6":"code","9c5a39f2":"code","9aea5125":"code","318a1fe7":"code","e54f6f88":"code","5cd567fa":"code","b22c2000":"code","73743a62":"code","50b36409":"code","ac70944a":"code","ef574f6e":"code","98c8ba1f":"code","66184dd3":"code","06136d4e":"code","7da3d9f9":"code","37c8040a":"code","3bb7d168":"code","2144896c":"code","02577738":"code","be0b91dd":"code","620a1e5e":"code","ac5bc7d7":"code","0dabf0e0":"code","1a3e6d8b":"code","904c8329":"code","8e5a8880":"code","3a27883e":"code","15bf197c":"code","197d9a7b":"code","936d3f2e":"code","94f6ffe3":"code","9b6133f5":"markdown","5e49996c":"markdown","3d26e2bf":"markdown","50297454":"markdown","88249dc0":"markdown","3fcdcc56":"markdown","102999bd":"markdown","ccf989d4":"markdown","9c9e04a4":"markdown","e28eb03e":"markdown","b02d470a":"markdown","c94cac35":"markdown","b9389495":"markdown","88ba8d34":"markdown","c2f8ac33":"markdown","4e153309":"markdown","3d78d6e9":"markdown","da9e1ef2":"markdown","ca14ca45":"markdown","c64970e9":"markdown","d84fd9a6":"markdown","b543e3aa":"markdown","fd1180ec":"markdown","001a2216":"markdown","5dc456f7":"markdown","f18b7201":"markdown","d69b6d80":"markdown","d7169760":"markdown","ae110554":"markdown","eb41e5e4":"markdown","3a28306a":"markdown","cbc7fe37":"markdown","4c682ec5":"markdown","89b0e312":"markdown","bf4d4680":"markdown","3faf282a":"markdown","bfc72dba":"markdown","0c4ac312":"markdown","73327509":"markdown"},"source":{"9f3cf970":"import warnings\nwarnings.simplefilter(\"ignore\")","6e9949dc":"import os\nimport pandas as pa\nfrom datetime import datetime as dt\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns","32e724f6":"PLOTS_PATH = \"..\/input\/bike-sharing-demand\/plots\/\"\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(PLOTS_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving plot\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n    print(\"plot saved\", fig_id)","32b2ea41":"DATA_DIR_PATH = \"..\/input\/bike-sharing-demand\/\"\nTRAIN_FILE = \"train.csv\"\nTEST_FILE = \"test.csv\"\n\ndef load_csv_data(file_path, file_name):\n    return pa.read_csv(os.path.join( file_path, file_name) )\n\ntrain_data = load_csv_data(DATA_DIR_PATH,TRAIN_FILE)","4136f87e":"# Copy dataset for analysis and manupulations.\nanalysis_data_set = train_data.copy()","85bdd59a":"# get the metadata of the data.\nanalysis_data_set.info()","bd65772e":"analysis_data_set.head()","63c8173b":"# \"weekday\",\"month\",\"hour\",\"year\" extraction\ndef transform(X):\n    c_datetime = X[\"datetime\"].unique()\n    formatted_dt = [dt.strptime(x, \"%Y-%m-%d %H:%M:%S\") for x in c_datetime]\n    X[\"weekday\"] = extract_part_from_date(formatted_dt,\"%w\")\n    X[\"month\"] = extract_part_from_date(formatted_dt,\"%m\")\n    X[\"hour\"] = extract_part_from_date(formatted_dt,\"%H\")\n    X[\"year\"] = extract_part_from_date(formatted_dt,\"%Y\")\n\n    int_features = [\"weekday\",\"month\",\"hour\",\"year\"]\n    for feature in int_features:\n        X[feature] = X[feature].astype(np.int64)\n\n    return X\n\ndef extract_part_from_date(dates, code_str):\n    return [date.strftime(code_str) for date in dates]\n\nanalysis_data_set = transform(analysis_data_set)    \nanalysis_data_set = analysis_data_set.drop([\"datetime\"], axis=1)\n\n# print first 6 instances of the dataset\nanalysis_data_set.head(6)","4b90b2a2":"import missingno as msno\nmsno.matrix(analysis_data_set,figsize=(12,5))","d54f307f":"corr_matrix=analysis_data_set.corr()\ncorr_matrix[\"count\"].sort_values(ascending=False)","54fd4a8c":"cmap = sns.cubehelix_palette(light=1, as_cmap=True)\nsns.heatmap(corr_matrix, cmap=cmap)","acbb371e":"features = [\"temp\",\"season\",\"windspeed\",\"humidity\",\"weather\",\"count\"]\ng = sns.pairplot(analysis_data_set,\n                 vars= features,\n                 palette = 'husl',\n                 height=1.4,\n                 aspect = 1.5,\n                 kind='reg',\n                 diag_kind = 'kde',\n                 diag_kws=dict(shade=True),\n                 plot_kws=dict(scatter_kws={'s':.2}),\n                 \n            )\ng.set(xticklabels=[])","06db1cbd":"# \"temp\",\"season\",\"windspeed\",\"humidity\",\"weather\",\"count\"\ndef bxplot(data):\n    fig, axes =  plt.subplots(nrows=3,ncols=2)\n    fig.set_size_inches(12,10)\n    sns.boxplot(y=data['count'],ax=axes[0][0])\n    sns.boxplot(x=data['temp'],y=data['count'],ax=axes[0][1])\n    sns.boxplot(x=data['season'],y=data['count'],ax=axes[1][0])\n    sns.boxplot(x=data['windspeed'],y=data['count'],ax=axes[1][1])\n    sns.boxplot(x=data['humidity'],y=data['count'],ax=axes[2][0])\n    sns.boxplot(x=data['weather'],y=data['count'],ax=axes[2][1])\n    ","2b9143f0":"bxplot(analysis_data_set)","36520d42":"from scipy.stats import zscore\nanalysis_data_set = analysis_data_set[(np.abs(zscore(analysis_data_set)) < 3).all(axis=1)]","95de61f5":"bxplot(analysis_data_set)","39089751":"# hour \nsns.catplot(x='hour',y='count', hue=\"season\", data=analysis_data_set, kind=\"point\", aspect=2)","92cda9c7":"# day\nsns.catplot(x='hour',y='count',hue=\"weekday\",data=analysis_data_set, kind=\"point\", aspect=2)","caaf26aa":"# month wise rent\nsns.barplot(x=\"month\", y=\"count\", data=analysis_data_set, capsize=0.2)","a732917b":"# train and test split \nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nstratified = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_i, validation_i in stratified.split(train_data, train_data['season']):\n    strat_train_set = train_data.loc[train_i]\n    strat_validation_set = train_data.loc[validation_i]","357db2db":"from sklearn.model_selection import train_test_split\ntrain, validation = train_test_split(train_data, test_size=0.2, random_state=42)","ddfaaee9":"train_X = strat_train_set.drop([\"count\"],axis=1).copy()\ntrain_Y = strat_train_set[\"count\"].copy()","be35e6c4":"train_X.info()","fe359bf9":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DatePreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return None\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        c_datetime = X[\"datetime\"].unique()\n        formatted_dt = [dt.strptime(x, \"%Y-%m-%d %H:%M:%S\") for x in c_datetime]\n        X[\"weekday\"] = self.extract_part_from_date(formatted_dt,\"%w\")\n        X[\"month\"] = self.extract_part_from_date(formatted_dt,\"%m\")\n        X[\"hour\"] = self.extract_part_from_date(formatted_dt,\"%H\")\n        X[\"year\"] = self.extract_part_from_date(formatted_dt,\"%Y\")\n        \n        int_features = [\"weekday\",\"month\",\"hour\",\"year\"]\n        for feature in int_features:\n            X[feature] = X[feature].astype(np.int64)\n        return X\n    \n    def extract_part_from_date(self, dates, code_str):\n        return [date.strftime(code_str) for date in dates]","0214eaa6":"class DropColumns(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, columns):\n        self.columns = columns\n    \n    def fit(self,X):\n        return self\n    \n    def transform(self, X):\n        X = X.drop(self.columns, axis=1)\n        return X ","9c5a39f2":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\ndrop_cols = ['datetime','registered','casual','atemp']\nnum_pipline = Pipeline([\n        (\"date_processor\", DatePreprocessor()),\n        (\"drop_cols\", DropColumns(columns = drop_cols) )\n])","9aea5125":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nnum_attribs = list(train_X)\n\ncat_attribs = [\"season\",\"weather\"]\n\ncat_pipeline = ColumnTransformer([\n    (\"num_pipline\", num_pipline, num_attribs), # Handling Text and Categorical Attributes\n    (\"cat_pipline\", OneHotEncoder(sparse=False, categories='auto'), cat_attribs) \n])\n\ntrain_prepared = cat_pipeline.fit_transform(train_X)","318a1fe7":"# train_prepared = train_prepared[(np.abs(zscore(train_prepared)) < 3).all(axis=1)]","e54f6f88":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(train_prepared,train_Y)","5cd567fa":"validation_x = validation.drop(['count'],axis=1)\nvalidation_y = validation['count'].copy()\n\nnum_attribs = list(validation_x)\ncat_attribs = [\"season\",\"weather\"]\ndrop_cols = ['datetime','registered','casual','atemp','holiday']\nvalidation_x_prepared = cat_pipeline.transform(validation_x)\nrfr_prediction = rfr.predict(validation_x_prepared)","b22c2000":"rfr_prediction[:2], validation_y[:2]","73743a62":"from sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(validation_y,rfr_prediction)\nlin_rmse = np.sqrt(mse)\nlin_rmse","50b36409":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(rfr, validation_x_prepared, validation_y, cv=10, scoring=\"neg_mean_squared_error\")\nrmse_scores = np.sqrt(-scores)","ac70944a":"scores","ef574f6e":"scores.mean()","98c8ba1f":"scores.std()","66184dd3":"from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodels=[RandomForestRegressor(),AdaBoostRegressor(),BaggingRegressor(),SVR(),KNeighborsRegressor()]\nmodel_names=['RandomForestRegressor','AdaBoostRegressor','BaggingRegressor','SVR','KNeighborsRegressor']","06136d4e":"validation_x = strat_validation_set.drop(['count'],axis=1)\nvalidation_y = strat_validation_set['count'].copy()\nnum_attribs = list(validation_x)\ncat_attribs = ['season','weather']\ndrop_cols = ['datetime','registered','casual','atemp','holiday']\n\nrmses=[]\nstds = []\nd={}\ntrained_models = []\nfor model in models:\n    model.fit(train_prepared,train_Y)\n    trained_models.append(model)\n    validation_x_prepared = cat_pipeline.transform(validation_x)\n    prediction = model.predict(validation_x_prepared)\n    # validation\n    scores = cross_val_score(model, validation_x_prepared, validation_y, cv=10, scoring=\"neg_mean_squared_error\")\n    rmse_scores = np.sqrt(-scores)\n    rmses.append(rmse_scores.mean())\n    stds.append(rmse_scores.std())\n    \nd = {\"Models\": model_names, \"mean rsme score\": rmses, \"Standard deviation\": stds}\ndf = pa.DataFrame(d)\ndf","7da3d9f9":"df.loc[df['mean rsme score'].idxmin()]","37c8040a":"model = trained_models[0]\nmodel","3bb7d168":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3,10,20,30]}\n]\n\nrfr = RandomForestRegressor(random_state=42)\ngrid_search = GridSearchCV(rfr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(train_prepared,train_Y)","2144896c":"grid_search.best_params_","02577738":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","be0b91dd":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n    'n_estimators': randint(1,200)\n}\n\nrfr = RandomForestRegressor(random_state=42)\nrandom_search = RandomizedSearchCV(rfr, param_distributions=param_distribs,cv=10, scoring='neg_mean_squared_error')\nrandom_search.fit(train_prepared,train_Y)","620a1e5e":"random_search.best_estimator_","ac5bc7d7":"random_search.best_params_","0dabf0e0":"cvres = random_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'],cvres['params']):\n    print(np.sqrt(-mean_score), params)","1a3e6d8b":"test_data = load_csv_data(DATA_DIR_PATH,TEST_FILE)\ntest_data.columns\ntest_data.head()","904c8329":"validation_x.head()","8e5a8880":"final_model = random_search.best_estimator_\n\nX_test = test_data.copy()\nX_test['casual'] = 0\nX_test['registered'] = 0\n\nnum_attribs = list(X_test)\n\nX_prepared = cat_pipeline.transform(X_test)\nprediction = final_model.predict(X_prepared)","3a27883e":"prediction","15bf197c":"len(prediction),len(X_test)","197d9a7b":"list(prediction[:10])","936d3f2e":"out_dict = {\"date\":test_data.datetime, \"count\" : prediction }\nout_dict_df = pa.DataFrame(out_dict)\nout_dict_df.head()","94f6ffe3":"# out_dict_df.to_csv('..\/input\/bike-sharing-demand\/bike-sharing-demand-estimation.csv',index=False)","9b6133f5":"# Parameter Tuning","5e49996c":"##  RMSE","3d26e2bf":"### Inference :\n\n* Across all the season the rental timing seems to be similar, yet the number of rentals varies by season. \n* The green season has almost double the count of blue during evening ( 17:00 - 19:00 )\n* During morning hours ( 7:00 - 9:00 ) the count variance small comparied to evening ","50297454":"## save to csv","88249dc0":"From the above results we can see with 'n_estimators': 30 the msre is 43.468935429069326. We could try increasing the n_estimators in multiples of 10 and check.","3fcdcc56":"# Setup","102999bd":"### Date extraction transformer","ccf989d4":"### Discover and visualize the data to gain insights","9c9e04a4":"### ** Please share your valuable comments and suggestions. Thank you! **","e28eb03e":"### Inference:\n* _humidity, weather_ have strong negative relation \n* and _temp, season, windspeed_ have strong positive relation the target (_count_). \n* Other features such as  _weekday, workingday, year, month_ have considered corelation value.   \n* _atemp_ is highly correlated with _temp_, as how _registered, casual_ are correlated, so we can ignore _atemp, registered, casual_ features for our model.  ","b02d470a":"# Data Analysis\nlets have a look at the data and figure out what can we infer from it.","c94cac35":"### Outliers\n\nIdentify outliers in target and in the features that have strong corelation with the target.","b9389495":"### Remove outliers","88ba8d34":"### Transformer to drop columns","c2f8ac33":"### Finding missing values","4e153309":"# Get the data","3d78d6e9":"# Prepare the Data for Machine Learning Algorithms","da9e1ef2":"## Transformation Pipelines\n\nLets create custome transformation classes for \n* Date extraction\n* Columns drop\n* Outliers removals","ca14ca45":"# Evaluate Your System on the Test Set","c64970e9":"### Random Forest Regressor performs better\nLets Fine-Tune this Model. We could find best parameters for our models using GridSearchCV or RandomizedSearchCV. Lets try both.","d84fd9a6":"Addtional informations on the data.","b543e3aa":"### Pipeline class to help with sequences of transformations\n\nPlace our custome transformation classes in the pipeline ","fd1180ec":"RandomForestRegressor has performed better on our data than the other algorithms. So lets proceed with RandomForestRegressor with little fine tuning. ","001a2216":"### Train and Test split \n\nStratified sampling. Spliting data for test and train set by stratifying based on season i.e., equal number of intances are fetched from each seasons.","5dc456f7":"Remove values on all features which are above 3 Standard deviations","f18b7201":"# Select and Train a model","d69b6d80":"With RandomizedSearchCV we have identified a precise n_estimators.","d7169760":"### Correlation\n\nlets find the pairwise correlation of all features in the our dataset. Non-numeric features are ignored automatically by the `.corr()` function in pandas.","ae110554":"With all the analysis, observations and inference we have a better idea on the features and the manupulations to be carried out on the data before we feed it into out machine learning algorithms for training and predictions. ","eb41e5e4":"#### We are adding two colums for the sake of transformers since we had \"casual,registered\" columns in train set we expect text set also to have the same number of columns. ","3a28306a":"# Better Evaluation Using Cross-Validation","cbc7fe37":"_datetime_ feature is datetime formatted. Parse the required information for our model and drop the feature.\n\nExtract _\"weekday\",\"month\",\"hour\",\"year\"_ from _datetime_ object and add them as seperate features in our dataset. Then drop the _datatime_ feature. We must do the same process for validation and test instances.","4c682ec5":"### Inference :\n\n* The count is high during weekdays and low during weekends\n* On weekdays, morning and evening the count is at the peak. We may assume that is due to office goers.\n* Whereas on the weekends the count is flat and spread across the midday. Could be leisure rental.","89b0e312":"#### Inference :\n* Count is lower on the year start and gradually increases during the mid year.\n* Then the Count starts decreasing smoothly and reaches little more than the average count during month end.  ","bf4d4680":"#### Find specified parameter values for our model using GridSearchCV","3faf282a":"#### Now finding specified parameter values for our model using RandomizedSearchCV","bfc72dba":"We can pick a model which has lower mean rsme score and it good to consider the standard deviation as well.","0c4ac312":"Previously we used RandomForestRegressor, now lets try various other algorithms and find out which model performs better on our data. Then we pick that model and do some fine tuning.  ","73327509":"### Observation:\n\n* We have 12 columns. 11 Features and 1 Target. \n* Data types : 3 Float, 8 int and 1 Object.\n\nWe need take care of Object datatype. i.e., _datetime_ feature. Because most of the model training algorithims accept features as numeric datatypes. \n\nTo get a better understanding of the data. Lets print the first 5 rows of the data table. "}}