{"cell_type":{"9afc5048":"code","b0c21a76":"code","28dc3e82":"code","d1efa53f":"code","468393e1":"code","1c3df492":"code","324d7950":"code","f34dccfa":"code","1b62f611":"code","8be95bef":"code","8bef44b3":"code","6ab0eb75":"code","cd2baf51":"code","28a29ef1":"code","de3e7e5d":"code","23d0d87a":"code","0f3fe457":"code","435ac011":"code","9020ac32":"code","a299aef3":"code","9359af6c":"code","e987dd86":"code","1411371d":"code","75e2d2d0":"code","30a383eb":"code","26998450":"code","0217bd5b":"code","584651c8":"code","4ee772ff":"code","94d5628c":"code","ebfa9af1":"code","4c378df6":"code","205ce741":"code","47a65470":"code","8429aba2":"code","c9e5b050":"code","b626ddeb":"code","b6467e8c":"code","32662351":"code","e4ed3fbf":"code","c27fcfea":"code","3bc51f86":"code","7137df72":"code","cebd0ca9":"code","3ca14572":"code","57cef06d":"code","50772102":"code","3cfc1944":"code","5c66ea5c":"code","3a38ae4a":"code","49b2286f":"code","1764dd06":"code","b0f1c5fa":"code","bed6372f":"code","1aff0539":"code","2063ec0c":"code","2c4277cd":"code","e9a9084d":"code","13ad2672":"code","68d72d0e":"code","9e55d022":"code","fd691a8b":"code","5c2782a7":"code","5e67ff5a":"code","bc5ac44a":"code","0e6dd05c":"code","979c95f1":"code","26d9ce49":"code","7db8cb64":"code","b834240b":"code","059bab71":"code","b0b23bfe":"code","afb48228":"code","d43b18bf":"code","0713c5cb":"code","052c198a":"code","27034afe":"code","c2932c45":"code","40a877b6":"code","e78764e8":"code","07c9248b":"code","c2379a8d":"code","e1b6b3d0":"code","52ed6246":"code","ab6a4f4f":"code","c59e9b53":"code","b0f05391":"code","c6da6757":"code","789fc56b":"code","c297b2be":"code","0b220bc8":"code","4c4bc294":"code","b933e8b4":"code","71927702":"code","8c8be397":"code","849ec5cf":"code","94bbacc9":"code","d6d4573a":"code","b79c1e9b":"code","8be7a770":"code","fb36caa9":"code","8bb4f54e":"code","19181738":"code","9b134c87":"code","e60b2c6e":"code","01e81eb2":"code","11a7429b":"code","9d6e59d4":"code","7a446846":"code","d3aef0d4":"code","268a8b05":"code","dabe3e1e":"code","a7206cbe":"code","53f39000":"code","c27478ec":"code","c06ee9b9":"code","191ac4a7":"code","0e4a2aa4":"code","58be481e":"code","9ca9b616":"code","9de048da":"code","2c0a6e6f":"code","5fc89c53":"code","3e7c6323":"code","8a7689a4":"code","b53409be":"code","9c89b25f":"code","56ebab20":"code","5c9fa723":"code","865671cf":"code","3667cf9a":"code","0318bf63":"code","7360bb69":"code","3f7414c8":"code","f75b5a8c":"code","b86ad2fc":"code","e2d4203f":"code","dcbae2b7":"code","402585d3":"code","d3fd3e9a":"code","3ad35084":"code","d386ed6e":"code","8b549f17":"code","4918edcc":"code","0c53a789":"code","6ee64e19":"code","6a329eb2":"code","5caeb707":"code","6dd2c936":"code","3239b711":"code","b9ab51b4":"code","6b431b10":"code","6d6582e9":"code","26e60309":"code","860176ab":"code","9122caee":"code","7fe46b7c":"code","e352340d":"code","e11c5b2a":"code","23127008":"code","591425b8":"code","bb1118e7":"code","b550e366":"code","a98d143e":"code","fe23a530":"code","125d096a":"code","dcf20adb":"code","45d65a22":"code","1a226d65":"code","26c803ef":"code","ee8ba95d":"code","a67f4139":"code","7a87641f":"code","91d77e8c":"code","a8738f9b":"code","ecc4a496":"code","4c22eb8e":"code","981d2fca":"code","180c8410":"code","86cfd568":"code","74032471":"code","73e2f16e":"code","0e188642":"code","81941151":"code","0b6587a5":"code","0bfe719a":"code","9b727c25":"code","8d2f680f":"code","87831e09":"code","ce5cffdf":"code","cf8f3921":"code","65868388":"code","07718a9d":"code","d4a9af84":"code","fea33bf2":"code","41407983":"code","346075b5":"code","3b436c84":"code","bbf82fe4":"code","9595ae3a":"code","4f3ad51d":"code","2af9b72d":"code","e44e487d":"code","0bd34fbc":"code","79b8143b":"code","4d642e3f":"code","fb3c4325":"code","607bf665":"code","7deb00f4":"code","d6a9881a":"code","e3606411":"code","a13d6f0a":"code","9a39207b":"code","8f4c2ee9":"code","ad2adaec":"code","f62e0772":"code","5b5e6917":"code","04053dcd":"code","06f76eb8":"code","d2d103fc":"code","cc0311d1":"code","973fe38a":"code","bb80267a":"code","8026f572":"code","e3973bc0":"code","3d7dac20":"code","2d014f27":"code","47ab3b0c":"code","164a8285":"code","007b719f":"code","e6f27d0b":"code","8d293ed4":"code","932c955e":"code","2cc365f3":"code","401d04c7":"code","9b7309e5":"code","75d673ae":"code","33153585":"code","86acdb6b":"code","359a9bc5":"code","90c18d78":"code","03c72fd6":"code","2515f767":"code","1536ed52":"code","fa48c032":"code","5fffc045":"code","00424bb5":"code","b265dabe":"code","951049b3":"code","22724b24":"code","1bf0b714":"code","510a8f57":"code","5b260770":"code","dfa7c8fc":"code","0b4c6fc9":"code","64e43523":"code","d68ad566":"code","04b56028":"code","61cc50b1":"code","b0782693":"code","3d8e5890":"code","f0b76edd":"code","30bc09ed":"code","31cb5545":"code","462df32d":"code","f1e570c4":"code","25504599":"code","b4bf918c":"code","8822e75d":"code","e0cb2cce":"code","cc74bbb1":"code","93468c06":"code","8abbcd01":"code","44bf2c94":"code","dd3702a7":"code","720d557c":"code","bc046cc3":"code","ab56d012":"code","0e054ea1":"code","86d30483":"code","f037108c":"code","3c3e13fb":"code","5da6988f":"code","012c0967":"code","b0acdde4":"code","c3ca4cbe":"code","01df8caf":"code","96710156":"code","7dd51ba4":"code","03ffd89f":"code","ebe2cf9d":"code","150659b2":"code","7cab5ed2":"code","15071353":"code","15f5152b":"code","b9786bc4":"code","d6f4bec3":"code","121e1a24":"code","66841736":"code","88369ae5":"markdown","6d3d33c0":"markdown","2700e142":"markdown","8f24de25":"markdown","8d538f23":"markdown","954f644d":"markdown","aa1b16f4":"markdown","311c16c0":"markdown","e6ac77e4":"markdown","ae71fd75":"markdown","acf9c898":"markdown","4915c9aa":"markdown","11e5a1b7":"markdown","31cb345b":"markdown","0f0ea643":"markdown","27f052b0":"markdown","a6b0d86b":"markdown","2e47c9b1":"markdown","277a54a3":"markdown","7aae5f01":"markdown","bf26eaa1":"markdown","4ef6152e":"markdown","42f86132":"markdown","490a5965":"markdown","3847fade":"markdown","aab3abd0":"markdown","1f397544":"markdown","384562cb":"markdown","b10ad5e7":"markdown","2c81de91":"markdown","feba3692":"markdown","cd5f1d84":"markdown","5222f08b":"markdown","329a84de":"markdown","4311c88d":"markdown","1a92c73d":"markdown","301ae112":"markdown","2c89745f":"markdown","1aeaf3e6":"markdown","ac3db874":"markdown","fd3a1b27":"markdown","e56d7c51":"markdown","43ac6c9f":"markdown","8410343b":"markdown","41f0d629":"markdown","18a188b7":"markdown","655af97f":"markdown","5dd772ec":"markdown","bc355d18":"markdown","45e21d4d":"markdown","11559595":"markdown","4749e7a2":"markdown","f8d3d758":"markdown","e4371f73":"markdown","fcadf546":"markdown","bb504f7e":"markdown","66a479a1":"markdown","d8871bd7":"markdown","eed100f7":"markdown","d1614195":"markdown","abde0e31":"markdown","efcae930":"markdown","7f3d3415":"markdown","10fd4fc0":"markdown","222de00a":"markdown","d8fd0f04":"markdown","d626b510":"markdown","8f6e62f5":"markdown","3372a062":"markdown","a090791b":"markdown","d362ecd4":"markdown","0971d0a2":"markdown","9095be21":"markdown","a3d72eb9":"markdown","886239d9":"markdown","9409fd94":"markdown","dbaa4419":"markdown","26edadcb":"markdown","738c46b3":"markdown","dc8f6efe":"markdown","7d41ecce":"markdown","702599b8":"markdown","14ba4033":"markdown","e8f987cb":"markdown","e3d2ccec":"markdown","aeee86e0":"markdown","2438979e":"markdown","5a1bb617":"markdown","05e00b21":"markdown","3012c737":"markdown","49e2b6ae":"markdown","fa5d9d00":"markdown","142c1b46":"markdown","c0c6e411":"markdown","af72174d":"markdown","17cd2b7c":"markdown","d08ade1b":"markdown","b7a9b0ff":"markdown","66caa266":"markdown","b0655750":"markdown","d94f65f1":"markdown","91cc5e4f":"markdown","58476a6b":"markdown","a8d0089d":"markdown","43ef00a8":"markdown","79eb2242":"markdown","b1dfb817":"markdown","5833e898":"markdown","4ac79ac8":"markdown","e2b7309b":"markdown","e72491d0":"markdown","641e5a94":"markdown","a7c2141b":"markdown","638d19af":"markdown","5d72c78f":"markdown","28a374d2":"markdown","3ace8429":"markdown","9baaa6f0":"markdown","75ba7296":"markdown","b71cc50e":"markdown","f364c183":"markdown","a2da1941":"markdown","083d3e71":"markdown","333735f0":"markdown","c7e3fb4e":"markdown","4ff1f069":"markdown","5b3ccf19":"markdown","edb32116":"markdown","4fdb3ec1":"markdown","504851db":"markdown","a7e20753":"markdown","2d07740d":"markdown","6fde9c0a":"markdown","75a3f6c1":"markdown","7665205b":"markdown","fc88067f":"markdown","ebb35b55":"markdown","7fa78d88":"markdown","62dcb3a6":"markdown","c99d31ce":"markdown","65aa7776":"markdown","bfdd7b14":"markdown","329212cb":"markdown","15f6b2ed":"markdown","c732fd45":"markdown","53d8a207":"markdown","4838ca7a":"markdown","6ee142ec":"markdown","392dbc64":"markdown","7750b2a7":"markdown","d984cf58":"markdown","41371bdc":"markdown","11f7f627":"markdown","e14a9933":"markdown","041b0da2":"markdown","e3fdc04a":"markdown","d64ba184":"markdown","6dfa8c39":"markdown","429efcb2":"markdown","acf206ae":"markdown","8384ae73":"markdown","03bf43ac":"markdown","15f7faf7":"markdown","ff5328b4":"markdown","b39cff66":"markdown","57d54fd0":"markdown","12134c75":"markdown","babf3443":"markdown","afe43d58":"markdown","63d9c486":"markdown","396692f4":"markdown","c920bab6":"markdown","0a20ee08":"markdown","cbb3fa40":"markdown","22ac1243":"markdown","7f170175":"markdown","b4619a8a":"markdown","3d64e32d":"markdown","18a0340f":"markdown","92529265":"markdown","73d4721e":"markdown","1cdb6c18":"markdown","52f9170f":"markdown","b3162290":"markdown","43827b7e":"markdown","0e78f5ed":"markdown","4763f402":"markdown","abfa764c":"markdown","8783c026":"markdown","604c9f15":"markdown","3b021bda":"markdown","010fc73a":"markdown","9ebeb0b7":"markdown","dd4b70c9":"markdown","38ef7e40":"markdown","745c88a4":"markdown","8eaef19b":"markdown","f8174f3b":"markdown","35826eb9":"markdown","89e1f722":"markdown","d6cf9b59":"markdown","a2901106":"markdown","74e06ae4":"markdown","3aed7808":"markdown","823bc64b":"markdown","37139fdd":"markdown","3dba3479":"markdown","4a9e6e99":"markdown","9e3ff545":"markdown","ab1e1ad4":"markdown","fa1095b4":"markdown","fa4341d3":"markdown","642975c7":"markdown","435e9f59":"markdown","c73297f5":"markdown","f532d957":"markdown","d45fecd4":"markdown","a32427a7":"markdown"},"source":{"9afc5048":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b0c21a76":"# Importing libraries for imputation and model building\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import BayesianRidge\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","28dc3e82":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d1efa53f":"# Setting option to display all the rows and columns in dataset\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_columns', None)","468393e1":"# Setting plot style to ggplot\nplt.style.use('ggplot')","1c3df492":"df = pd.read_csv(\"telecom_churn_data.csv\")","324d7950":"df.shape","f34dccfa":"df.head()","1b62f611":"df.describe()","8be95bef":"df.info(verbose=True, show_counts=True)","8bef44b3":"date_var = df.select_dtypes(include='object').columns","6ab0eb75":"for col in date_var:\n    df[col] = pd.to_datetime(df[col])","cd2baf51":"df[date_var].info()","28a29ef1":"# Getting total number of NULL values and percentage of the columns\n# null_columns = df.columns[df.isna().any()]\nnull_value_count = df[df.columns[df.isna().any()]].isna().sum().sort_values(ascending=False)\nnull_percentage = (df[df.columns[df.isna().any()]].isna().sum() * 100 \/ df.shape[0]).sort_values(ascending=False)","de3e7e5d":"null_data = pd.concat([null_value_count, null_percentage], axis=1, keys=['Count', 'Percentage'])","23d0d87a":"# Columns with NULL values and % of NULLs are populated in descending order\nnull_data","0f3fe457":"round(df.isna().sum()\/df.shape[0],2)","435ac011":"df_6 = df[['date_of_last_rech_data_6', 'total_rech_data_6', 'max_rech_data_6', 'count_rech_2g_6', \n    'count_rech_3g_6', 'av_rech_amt_data_6', 'arpu_3g_6', 'arpu_2g_6', 'night_pck_user_6', 'fb_user_6']]","9020ac32":"df_6[df_6['night_pck_user_6'].isna()].head(5)","a299aef3":"df_7 = df[['date_of_last_rech_data_7', 'total_rech_data_7', 'max_rech_data_7', 'count_rech_2g_7', \n    'count_rech_3g_7', 'av_rech_amt_data_7', 'arpu_3g_7', 'night_pck_user_7', 'arpu_2g_7', 'fb_user_7']]","9359af6c":"df_7[df_7['total_rech_data_7'].isna()].head()","e987dd86":"df_8 = df[['date_of_last_rech_data_8', 'total_rech_data_8', 'max_rech_data_8', 'count_rech_2g_8', \n    'count_rech_3g_8', 'av_rech_amt_data_8', 'arpu_3g_8', 'arpu_2g_8', 'night_pck_user_8', 'fb_user_8']]","1411371d":"df_8[df_8['fb_user_8'].isna()].head()","75e2d2d0":"df_9 = df[['date_of_last_rech_data_9', 'total_rech_data_9', 'max_rech_data_9', 'count_rech_2g_9', \n    'count_rech_3g_9', 'av_rech_amt_data_9', 'arpu_3g_9', 'arpu_2g_9', 'night_pck_user_9', 'fb_user_9']]","30a383eb":"df_9[df_9['av_rech_amt_data_9'].isna()].head()","26998450":"meaningful_missing_columns = ['total_rech_data_6', 'max_rech_data_6', 'count_rech_2g_6', \n    'count_rech_3g_6', 'av_rech_amt_data_6', 'arpu_3g_6', 'arpu_2g_6', 'night_pck_user_6', 'fb_user_6', 'total_rech_data_7', 'max_rech_data_7', 'count_rech_2g_7', \n    'count_rech_3g_7', 'av_rech_amt_data_7', 'arpu_3g_7', 'arpu_2g_7', 'night_pck_user_7', 'fb_user_7', 'total_rech_data_8', 'max_rech_data_8', 'count_rech_2g_8', \n    'count_rech_3g_8', 'av_rech_amt_data_8', 'arpu_3g_8', 'arpu_2g_8', 'night_pck_user_8', 'fb_user_8', 'total_rech_data_9', 'max_rech_data_9', 'count_rech_2g_9', \n    'count_rech_3g_9', 'av_rech_amt_data_9', 'arpu_3g_9', 'arpu_2g_9', 'night_pck_user_9', 'fb_user_9']","0217bd5b":"df[meaningful_missing_columns] = df[meaningful_missing_columns].apply(lambda x: x.fillna(0))","584651c8":"# Getting total number of NULL values and percentage of the columns\n# null_columns = df.columns[df.isna().any()]\nnull_value_count = df[df.columns[df.isna().any()]].isna().sum().sort_values(ascending=False)\nnull_percentage = (df[df.columns[df.isna().any()]].isna().sum() * 100 \/ df.shape[0]).sort_values(ascending=False)","4ee772ff":"null_data = pd.concat([null_value_count, null_percentage], axis=1, keys=['Count', 'Percentage'])\nnull_data","94d5628c":"null_data[ null_data['Percentage'] > 40].index","ebfa9af1":"# Dropping these columns from the dataframe `df`\ndf.drop(columns=null_data[ null_data['Percentage'] > 40].index, inplace=True)","4c378df6":"df.head()","205ce741":"cols_to_be_dropped = ['mobile_number', 'circle_id', 'last_date_of_month_6', 'last_date_of_month_7', \n                      'last_date_of_month_8', 'last_date_of_month_9', \n                      'date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8', 'date_of_last_rech_9']","47a65470":"df.shape","8429aba2":"df.drop(columns = cols_to_be_dropped, inplace=True)","c9e5b050":"df.shape","b626ddeb":"# Getting total number of NULL values and percentage of the columns\n# null_columns = df.columns[df.isna().any()]\nnull_value_count = df[df.columns[df.isna().any()]].isna().sum().sort_values(ascending=False)\nnull_percentage = (df[df.columns[df.isna().any()]].isna().sum() * 100 \/ df.shape[0]).sort_values(ascending=False)\n\nnull_data = pd.concat([null_value_count, null_percentage], axis=1, keys=['Count', 'Percentage'])\nnull_data","b6467e8c":"df[null_data.index].info(verbose=True, show_counts=True)","32662351":"df[list(null_data.index)].head()","e4ed3fbf":"# Check if the related columns don't have NULL values as we will be using the calculation based on this to \n# filter the entire dataset\ndf[['total_rech_amt_6', 'total_rech_amt_7', 'total_rech_data_6', 'total_rech_data_7', \n    'av_rech_amt_data_6', 'av_rech_amt_data_7']].isna().sum()","c27fcfea":"# Calculate total recharge data amount for months 6 and 7\ndf['total_rech_amt_data_6'] = df['total_rech_data_6'] * df['av_rech_amt_data_6']\ndf['total_rech_amt_data_7'] = df['total_rech_data_7'] * df['av_rech_amt_data_7']","3bc51f86":"# Calculate total recharge amount for both mobile and data\ndf['total_rech_amt_6_combined'] = df['total_rech_amt_6'] + df['total_rech_amt_data_6']\ndf['total_rech_amt_7_combined'] = df['total_rech_amt_7'] + df['total_rech_amt_data_7']","7137df72":"# Calculate average recharge done by the customer in June and July (6 & 7)\ndf['av_rech_amt_6_7'] = (df['total_rech_amt_6_combined'] + df['total_rech_amt_7_combined']) \/ 2","cebd0ca9":"# Calculate 70th percentile average recharge amount\npercentile_70_value = df['av_rech_amt_6_7'].quantile(q=0.7)\npercentile_70_value","3ca14572":"# Retain only those customers who have recharged their mobiles with >= 70th percentile\ndf[df['av_rech_amt_6_7']>= percentile_70_value].shape","57cef06d":"df_hv = df[df['av_rech_amt_6_7']>= percentile_70_value].copy()","50772102":"df_hv.shape","3cfc1944":"# Check if the related columns don't have NULL values as we will be deriving the target variable based on this\ndf_hv[['total_ic_mou_9', 'total_og_mou_9', 'vol_2g_mb_9', 'vol_3g_mb_9']].isna().sum()","5c66ea5c":"# Calculate total incoming and outgoing minutes of usage\ndf_hv['total_ic_og_mou_9'] = df_hv['total_ic_mou_9'] + df_hv['total_og_mou_9']","3a38ae4a":"# Calculate 2g and 3g data consumption\ndf_hv['vol_2g_3g_mb_9'] = df_hv['vol_2g_mb_9'] + df_hv['vol_3g_mb_9']","49b2286f":"# Initializing Churn variable with default value of 0\ndf_hv['churn'] = 0","1764dd06":"# Updating Churn to 1 where there are no calls or internet in the month of September indicating they have churned\ndf_hv.loc[(df_hv['total_ic_og_mou_9']==0) & (df_hv['vol_2g_3g_mb_9']==0), 'churn'] = 1","b0f1c5fa":"# Check churn percentage\nround(df_hv['churn'].value_counts(normalize=True)*100, 2)","bed6372f":"# Getting list of 9th month columns\ncolumns_month_9 = list(df_hv.columns[df_hv.columns.str.endswith(pat='_9')])\ncolumns_month_9","1aff0539":"# As we have used 9th month columns to derive the target variable \"churn\", we will go ahead and drop all 9th month columns\n# from the entire dataset\nhv = df_hv.loc[:, ~df_hv.columns.isin(columns_month_9)].copy()","2063ec0c":"hv.shape","2c4277cd":"hv_columns = list(hv.columns)\nhv_columns","e9a9084d":"# Creating a dictionary to hold the unique values of each column\nunique_dict = dict()\nfor each_col in hv_columns:\n    unique_dict[each_col] = hv[each_col].nunique()","13ad2672":"# Creating a pandas series to hold the unique values from the dictionary\nunique_df = pd.Series(data=unique_dict).sort_values(ascending=True)\nunique_df","68d72d0e":"# We need to filter the columns which have only 1 unique value and remove them from \"hv\" dataframe as they\nremove_columns_unique = unique_df[unique_df==1].index\nremove_columns_unique","9e55d022":"# Checking shape before column removal\nhv.shape","fd691a8b":"# Remove the columns from the dataframe \"hv\"\nhv.drop(columns=remove_columns_unique, inplace=True)","5c2782a7":"# Checking shape post column removal\nhv.shape","5e67ff5a":"# Getting total number of NULL values and percentage of the columns\n\nnull_value_count = hv[hv.columns[hv.isna().any()]].isna().sum().sort_values(ascending=False)\nnull_percentage = (hv[hv.columns[hv.isna().any()]].isna().sum() * 100 \/ hv.shape[0]).sort_values(ascending=False)\n\nnull_data = pd.concat([null_value_count, null_percentage], axis=1, keys=['Count', 'Percentage'])\nnull_data","bc5ac44a":"null_data.shape","0e6dd05c":"# Checking the data of NULLable columns\nhv[null_data.index].head(10)","979c95f1":"# Check the data type of NULLable columns\nhv[null_data.index].dtypes.unique()","26d9ce49":"# Getting the list of NULLable columns\nnull_columns = list(null_data.index)","7db8cb64":"# Median values that will be used for imputation\nfor each_col in null_columns:\n    print(each_col, \" - \", hv[each_col].median())","b834240b":"# Loop over the NULLable columns and impute by median\nfor each_col in null_columns:\n    hv[each_col].fillna(hv[each_col].median(), inplace=True)","059bab71":"null_value_count = hv[hv.columns[hv.isna().any()]].isna().sum().sort_values(ascending=False)\nnull_percentage = (hv[hv.columns[hv.isna().any()]].isna().sum() * 100 \/ hv.shape[0]).sort_values(ascending=False)\n\nnull_data = pd.concat([null_value_count, null_percentage], axis=1, keys=['Count', 'Percentage'])\nnull_data","b0b23bfe":"# Create \"aon_years\" column to see the age of customer in years\nhv['aon_years'] = round(hv['aon']\/365,0)\nhv['aon_years'].head()","afb48228":"# Drop \"aon\" column\nhv.drop(\"aon\", axis=1, inplace=True)","d43b18bf":"# Average On Network Minutes of Usage for 6 & 7\nhv['avg_onnet_mou_6_7'] = (hv['onnet_mou_6'] + hv['onnet_mou_7']) \/ 2","0713c5cb":"# Drop \"onnet_mou_6\" and \"onnet_mou_7\" columns\nhv.drop([\"onnet_mou_6\", \"onnet_mou_7\"], axis=1, inplace=True)","052c198a":"# Average Off Network Minutes of Usage for 6 & 7\nhv['avg_offnet_mou_6_7'] = (hv['offnet_mou_6'] + hv['offnet_mou_7']) \/ 2","27034afe":"# Drop \"offnet_mou_6\" and \"offnet_mou_7\" columns\nhv.drop([\"offnet_mou_6\", \"offnet_mou_7\"], axis=1, inplace=True)","c2932c45":"count_rech_2g_6_7 = hv['count_rech_2g_6'] + hv['count_rech_2g_7']\ncount_rech_3g_6_7 = hv['count_rech_3g_6'] + hv['count_rech_3g_7']","40a877b6":"# Avg count of total recharge (2G + 3G) for 6 & 7\nhv['avg_count_rech_2g3g_6_7'] = (count_rech_2g_6_7 + count_rech_3g_6_7) \/ 2","e78764e8":"# Univariate analyis - numerical variables with outlier handling\ndef fn_uni_num(dt, column):\n    \n    plt.figure(figsize = [20,8])\n\n    plt.subplot(1,2,1)\n    pltname = column + ' - without handling outliers'\n    plt.title(pltname)\n    sns.histplot(data=dt, x=column, color='blue')\n    plt.xticks(rotation = 45)\n    \n    Q1 = hv[column].quantile(0.25)\n    Q3 = hv[column].quantile(0.75)\n    IQR = Q3 - Q1\n    Min_value = (Q1 - 1.5 * IQR)\n    Max_value = (Q3 + 1.5 * IQR)\n    print(\"Min value before which outlier exist: {}\".format(Min_value))\n    print(\"Max value after which outlier exist: {}\".format(Max_value))\n    \n    plt.subplot(1,2,2)\n    pltname = column + ' - with handling outliers'\n    plt.title(pltname)\n    sns.histplot(data=hv, x=dt[dt[column] <= Max_value][column], color='green')\n    plt.xticks(rotation = 45)\n\n    plt.tight_layout(pad = 4)\n    plt.show()","07c9248b":"sns.countplot(x = 'aon_years', data = hv)\nplt.show()","c2379a8d":"sns.countplot(x = 'churn', data = hv)\nplt.show()","e1b6b3d0":"fn_uni_num(dt=hv, column='avg_onnet_mou_6_7')","52ed6246":"fn_uni_num(dt=hv, column='avg_offnet_mou_6_7')","ab6a4f4f":"fn_uni_num(dt=hv, column='avg_count_rech_2g3g_6_7')","c59e9b53":"def fn_bi_boxplot(dt,categorical,continuous):\n    plt.figure(figsize = [10,6])\n    \n    Q1 = hv[continuous].quantile(0.25)\n    Q3 = hv[continuous].quantile(0.75)\n    IQR = Q3 - Q1\n    Min_value = (Q1 - 1.5 * IQR)\n    Max_value = (Q3 + 1.5 * IQR)\n    print(\"Min value before which outlier exist: {}\".format(Min_value))\n    print(\"Max value after which outlier exist: {}\".format(Max_value))\n    \n    pltname = f\"{categorical} v\/s {continuous} - with handling outliers\"\n    plt.title(pltname)\n    sns.boxplot(data=dt, x=categorical, y=dt[dt[continuous] <= Max_value][continuous], color='green')\n    plt.xticks(rotation = 45)\n\n    plt.tight_layout(pad = 4)\n    plt.show()","b0f05391":"fn_bi_boxplot(dt=hv,categorical='churn',continuous='arpu_6')","c6da6757":"fn_bi_boxplot(dt=hv,categorical='churn',continuous='avg_count_rech_2g3g_6_7')","789fc56b":"fn_bi_boxplot(dt=hv,categorical='churn',continuous='last_day_rch_amt_8')","c297b2be":"fn_bi_boxplot(dt=hv,categorical='churn',continuous='onnet_mou_8')","0b220bc8":"plt.figure(figsize = [10,4])\npalt = sns.color_palette(\"bright\")\nsns.countplot(x = 'aon_years', data = hv, palette = palt, hue = 'churn')\nplt.xticks(rotation = 90)\nplt.tight_layout(pad = 4)\nplt.show()","4c4bc294":"corr = hv.corr()\n\nplt.figure(figsize=[20,10])\n\nk = 20 # number of variables for the heatmap\ncols = corr.nlargest(k,'churn')['churn'].index\ncorrmatrix = np.corrcoef(hv[cols].values.T)\nsns.set(font_scale=1)\nhm = sns.heatmap(corrmatrix, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':10},\n                yticklabels=cols.values, xticklabels=cols.values, cmap='RdYlGn')\nplt.show()","b933e8b4":"# Plots to check outliers\ndef fn_checkoutliers(df, col):\n    plt.figure(figsize=(16, 4))\n\n    # histogram\n    plt.subplot(1, 2, 1)\n    sns.histplot(df[col], bins=40)\n    plt.title('Histogram')\n\n    # boxplot\n    plt.subplot(1, 2, 2)\n    sns.boxplot(y=df[col])\n    plt.title('Boxplot')\n\n    plt.show()","71927702":"# Checking few columns before outlier capping\ncols_check = ['arpu_6',\n              'arpu_7',\n              'arpu_8',\n              'onnet_mou_8',\n              'offnet_mou_8',\n              'roam_ic_mou_6',\n              'roam_ic_mou_7',\n              'roam_ic_mou_8',\n              'roam_og_mou_6',\n              'roam_og_mou_7']","8c8be397":"for each_col in cols_check:\n    fn_checkoutliers(df=hv, col=each_col)","849ec5cf":"hv['arpu_6'].describe()","94bbacc9":"# Getting list of columns\ncol_list = list(hv.columns)","d6d4573a":"# remove churn from column list\ncol_list.remove('churn')","b79c1e9b":"len(col_list)","8be7a770":"# Applying outlier capping for all applicable columns\nfor each_col in col_list:\n    lb = hv[each_col].quantile(0.01)\n    ub = hv[each_col].quantile(0.99)\n    hv[each_col] = np.clip(hv[each_col], lb, ub)","fb36caa9":"hv['arpu_6'].describe()","8bb4f54e":"# Checking post outlier capping\nfor each_col in cols_check:\n    fn_checkoutliers(df=hv, col=each_col)","19181738":"# Add feature variables to X\nX = hv.drop('churn', axis=1)\n# Add response variable to y\ny = hv['churn']","9b134c87":"from sklearn.model_selection import train_test_split","e60b2c6e":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","01e81eb2":"hv.info(verbose=True, show_counts=True)","11a7429b":"col_scale = list(hv.columns)\ncol_scale.remove('churn')\ncol_scale.remove('night_pck_user_6')\ncol_scale.remove('night_pck_user_7')\ncol_scale.remove('night_pck_user_8')\ncol_scale.remove('fb_user_6')\ncol_scale.remove('fb_user_8')","9d6e59d4":"len(col_scale)","7a446846":"# Import sklearn library for MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Instantiate an object of MinMaxScaler\nscaler = MinMaxScaler()\n\n# Perform fit and transform on the X_train dataset\nX_train[col_scale] = scaler.fit_transform(X_train[col_scale])\n\n# Perform only transform on the X_test dataset\nX_test[col_scale] = scaler.transform(X_test[col_scale])","d3aef0d4":"from imblearn.over_sampling import SMOTE","268a8b05":"smote = SMOTE(random_state=100)\nX_train_smote, y_train_smote = smote.fit_resample(X_train,y_train)","dabe3e1e":"# Class imbalance before SMOTE\ny_train.value_counts(normalize=True)","a7206cbe":"# Class imbalance after SMOTE\ny_train_smote.value_counts(normalize=True)","53f39000":"# Import library from statsmodels\nimport statsmodels.api as sm","c27478ec":"logm1 = sm.GLM(y_train_smote, (sm.add_constant(X_train_smote)),  family=sm.families.Binomial())","c06ee9b9":"logm1.fit().summary()","191ac4a7":"# Importing LogisticRegression from sklearn\nfrom sklearn.linear_model import LogisticRegression","0e4a2aa4":"logreg = LogisticRegression(random_state=100)","58be481e":"# Importing RFE from sklearn\nfrom sklearn.feature_selection import RFE","9ca9b616":"# Selection of 20 features by RFE\nrfe = RFE(logreg, 20)\nrfe = rfe.fit(X_train_smote, y_train_smote)","9de048da":"# Looking at the feature names selected by RFE\nlist(zip(X_train_smote.columns, rfe.support_, rfe.ranking_))","2c0a6e6f":"# Get only the columns selected by RFE\ncol = X_train_smote.columns[ rfe.support_ ]","5fc89c53":"col","3e7c6323":"# Columns not selected by RFE\nX_train_smote.columns[~rfe.support_]","8a7689a4":"X_train_sm = sm.add_constant(X_train_smote[col])","b53409be":"# Second model\nlogm2 = sm.GLM(y_train_smote, X_train_sm, family = sm.families.Binomial())","9c89b25f":"# Fitting the model\nres = logm2.fit()","56ebab20":"# Checking the summary of model statistics\nres.summary()","5c9fa723":"# import variance_inflation_factor from statsmodels\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","865671cf":"# Create a dataframe that contains the names of all the feature variables and their corresponding VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_smote[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_smote[col].values, i) for i in range(X_train_smote[col].shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif = vif.sort_values(by = 'VIF', ascending=False)\nvif","3667cf9a":"col = col.drop('total_og_mou_8', 1)\ncol = col.drop('total_ic_mou_8', 1)\ncol = col.drop('std_og_mou_8', 1)\ncol = col.drop('loc_ic_mou_8', 1)\ncol = col.drop('offnet_mou_8', 1)\ncol = col.drop('onnet_mou_8', 1)\ncol = col.drop('std_og_t2m_mou_8', 1)\ncol = col.drop('std_og_t2t_mou_7', 1)\nlen(col)","0318bf63":"X_train_sm = sm.add_constant(X_train_smote[col])\nlogm3 = sm.GLM(y_train_smote, X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","7360bb69":"# Create a dataframe that contains the names of all the feature variables and their corresponding VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_smote[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_smote[col].values, i) for i in range(X_train_smote[col].shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif = vif.sort_values(by = 'VIF', ascending=False)\nvif","3f7414c8":"col = col.drop('avg_onnet_mou_6_7', 1)\nlen(col)","f75b5a8c":"X_train_sm = sm.add_constant(X_train_smote[col])\nlogm4 = sm.GLM(y_train_smote, X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","b86ad2fc":"# Create a dataframe that contains the names of all the feature variables and their corresponding VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_smote[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_smote[col].values, i) for i in range(X_train_smote[col].shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif = vif.sort_values(by = 'VIF', ascending=False)\nvif","e2d4203f":"X_train_sm.shape","dcbae2b7":"# Predicting probabilities on the Train set\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","402585d3":"y_train_pred_final = pd.DataFrame({\"Converted\": y_train_smote.values, \"Conversion_Prob\": y_train_pred})\ny_train_pred_final.head()","d3fd3e9a":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","3ad35084":"# For evaluation metrics, import metrics from sklearn\nfrom sklearn import metrics","d386ed6e":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","8b549f17":"# Confusion Matrix\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","4918edcc":"# Evaluation of other metrics than accuracy\n\nTP = confusion[1,1] # True Positive\nTN = confusion[0,0] # True Negative\nFP = confusion[0,1] # False Positive\nFN = confusion[1,0] # False Negative","0c53a789":"# Calculating Sensitivity of the model\nTP \/ (TP + FN)","6ee64e19":"# Calculating Specificity of the model\nTN \/ (TN + FP)","6a329eb2":"# Calculating False Positive rate (i.e. Predicting Churn when the Customer has not Churned)\nprint(FP \/ float(TN + FP))","5caeb707":"# Calculating Positive Predictive rate (i.e. Rate of identifying Actual Churn as Churn) (also called \"Precision\")\nprint(TP \/ float(TP + FP))","6dd2c936":"# Calculating Negative Predictive rate (i.e. Rate of identifying Actual Non-Churn as Non-Churn)\nprint(TN \/ float(TN + FN))","3239b711":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","b9ab51b4":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob, drop_intermediate = False )","6b431b10":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","6d6582e9":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","26e60309":"y_train_pred_final.head()","860176ab":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","9122caee":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","7fe46b7c":"# Let's create columns with different probability cutoffs \nnumbers = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","e352340d":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","e11c5b2a":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","23127008":"y_train_pred_final['Final_Predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.54 else 0)\n\ny_train_pred_final.head()","591425b8":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted)","bb1118e7":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted )\nconfusion2","b550e366":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","a98d143e":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","fe23a530":"# Let us calculate specificity\nTN \/ float(TN+FP)","125d096a":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","dcf20adb":"# Positive predictive value \nprint (TP \/ float(TP+FP))","45d65a22":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","1a226d65":"# Creating a confusion matrix\nconf = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted )\nconf","26c803ef":"# Precision\nconfusion[1,1]\/(confusion[0,1]+confusion[1,1])","ee8ba95d":"# Recall\nconfusion[1,1]\/(confusion[1,0]+confusion[1,1])","a67f4139":"# Import precision_recall_curve from sklearn\nfrom sklearn.metrics import precision_recall_curve","7a87641f":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","91d77e8c":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","a8738f9b":"X_test = X_test[col]\nX_test.head()","ecc4a496":"# Adding intercept to the test set\nX_test_sm = sm.add_constant(X_test)","4c22eb8e":"y_test_pred = res.predict(X_test_sm)","981d2fca":"y_test_pred[:10]","180c8410":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","86cfd568":"# Let's see the head\ny_pred_1.head()","74032471":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","73e2f16e":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","0e188642":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","81941151":"y_pred_final.head()","0b6587a5":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Conversion_Prob'})\n\ny_pred_final.head()","0bfe719a":"# As optimal cut-off point is 0.54 (where \"Sensitivity\" and \"Specificity\" intersect)\ny_pred_final['Final_Predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.54 else 0)","9b727c25":"y_pred_final.head()","8d2f680f":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.churn, y_pred_final.Final_Predicted)","87831e09":"confusion2 = metrics.confusion_matrix(y_pred_final.churn, y_pred_final.Final_Predicted )\nconfusion2","ce5cffdf":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","cf8f3921":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","65868388":"# Let us calculate specificity\nTN \/ float(TN+FP)","07718a9d":"fpr, tpr, thresholds = metrics.roc_curve( y_pred_final.churn, y_pred_final.Conversion_Prob, drop_intermediate = False )","d4a9af84":"draw_roc(y_pred_final.churn, y_pred_final.Conversion_Prob)","fea33bf2":"# Looking at the results of final model\nprint(res.summary())","41407983":"# Add feature variables to X\nX = hv.drop('churn', axis=1)\n# Add response variable to y\ny = hv['churn']","346075b5":"from sklearn.model_selection import train_test_split","3b436c84":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","bbf82fe4":"hv.info(verbose=True, show_counts=True)","9595ae3a":"col_scale = list(hv.columns)\ncol_scale.remove('churn')\ncol_scale.remove('night_pck_user_6')\ncol_scale.remove('night_pck_user_7')\ncol_scale.remove('night_pck_user_8')\ncol_scale.remove('fb_user_6')\ncol_scale.remove('fb_user_8')","4f3ad51d":"len(col_scale)","2af9b72d":"# Import sklearn library for MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Instantiate an object of MinMaxScaler\nscaler = MinMaxScaler()\n\n# Perform fit and transform on the X_train dataset\nX_train[col_scale] = scaler.fit_transform(X_train[col_scale])\n\n# Perform only transform on the X_test dataset\nX_test[col_scale] = scaler.transform(X_test[col_scale])","e44e487d":"from imblearn.over_sampling import SMOTE","0bd34fbc":"smote = SMOTE(random_state=100)\nX_train_smote, y_train_smote = smote.fit_resample(X_train,y_train)","79b8143b":"# Class imbalance before SMOTE\ny_train.value_counts(normalize=True)","4d642e3f":"# Class imbalance after SMOTE\ny_train_smote.value_counts(normalize=True)","fb3c4325":"from sklearn.decomposition import PCA","607bf665":"# Initializing PCA\npca = PCA(random_state=100)","7deb00f4":"# Fitting PCA on the train dataset\npca.fit(X_train_smote)","d6a9881a":"pca.components_[:2, :2]","e3606411":"# Looking at the explained variance ratio for each component\npca.explained_variance_ratio_","a13d6f0a":"# Make a Scree Plot for the explained variance\nvar_cum = np.cumsum(pca.explained_variance_ratio_)\nvar_cum","9a39207b":"# Plot the Scree Plot\nplt.plot(var_cum)\nplt.show()","8f4c2ee9":"# Initializing PCA to capture 90% variance\npca = PCA(0.90, random_state=100)","ad2adaec":"train_pca = pca.fit_transform(X_train_smote)","f62e0772":"train_pca.shape","5b5e6917":"corrmat = np.corrcoef(train_pca.transpose())","04053dcd":"corrmat.shape","06f76eb8":"plt.figure(figsize=[15,10])\nsns.heatmap(corrmat, annot=True, cmap='RdYlGn')\nplt.show()","d2d103fc":"test_pca = pca.transform(X_test)","cc0311d1":"# Initializing logistic regression model for PCA\nlearner_pca = LogisticRegression(random_state=100)","973fe38a":"model_pca = learner_pca.fit(train_pca, y_train_smote)","bb80267a":"# Predicting on the Train set\ny_train_pred = model_pca.predict_proba(train_pca)[:,1]","8026f572":"y_train_pred_final = pd.DataFrame({\"Converted\": y_train_smote.values, \"Conversion_Prob\": y_train_pred})\ny_train_pred_final.head()","e3973bc0":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","3d7dac20":"# For evaluation metrics, import metrics from sklearn\nfrom sklearn import metrics","2d014f27":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","47ab3b0c":"# Confusion Matrix\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","164a8285":"# Evaluation of other metrics than accuracy\n\nTP = confusion[1,1] # True Positive\nTN = confusion[0,0] # True Negative\nFP = confusion[0,1] # False Positive\nFN = confusion[1,0] # False Negative","007b719f":"# Calculating Sensitivity of the model\nTP \/ (TP + FN)","e6f27d0b":"# Calculating Specificity of the model\nTN \/ (TN + FP)","8d293ed4":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob, drop_intermediate = False )","932c955e":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","2cc365f3":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","401d04c7":"y_train_pred_final.head()","9b7309e5":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","75d673ae":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","33153585":"# Let's create columns with different probability cutoffs \nnumbers = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","86acdb6b":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","359a9bc5":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","90c18d78":"y_train_pred_final['Final_Predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.53 else 0)\n\ny_train_pred_final.head()","03c72fd6":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted)","2515f767":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted )\nconfusion2","1536ed52":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","fa48c032":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","5fffc045":"# Let us calculate specificity\nTN \/ float(TN+FP)","00424bb5":"# Predicting on the test set\ny_test_pred = model_pca.predict_proba(test_pca)[:,1]","b265dabe":"y_test_pred_final = pd.DataFrame({\"Converted\": y_test, \"Conversion_Prob\": y_test_pred})\ny_test_pred_final.head()","951049b3":"# As optimal cut-off point is 0.53 (where \"Sensitivity\" and \"Specificity\" intersect)\ny_test_pred_final['Final_Predicted'] = y_test_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.53 else 0)","22724b24":"y_test_pred_final.head()","1bf0b714":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_test_pred_final.Converted, y_test_pred_final.Final_Predicted)","510a8f57":"confusion2 = metrics.confusion_matrix(y_test_pred_final.Converted, y_test_pred_final.Final_Predicted)\nconfusion2","5b260770":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","dfa7c8fc":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","0b4c6fc9":"# Let us calculate specificity\nTN \/ float(TN+FP)","64e43523":"fpr, tpr, thresholds = metrics.roc_curve( y_test_pred_final.Converted, y_test_pred_final.Conversion_Prob, drop_intermediate = False )","d68ad566":"draw_roc(y_test_pred_final.Converted, y_test_pred_final.Conversion_Prob)","04b56028":"train_pca.shape","61cc50b1":"test_pca.shape","b0782693":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier","3d8e5890":"# Starting a default Decision Tree model with max depth of 3\ndt = DecisionTreeClassifier(max_depth=3, random_state=100, class_weight='balanced')","f0b76edd":"dt.fit(train_pca, y_train_smote)","30bc09ed":"# Train and test predictions\ny_train_pred = dt.predict(train_pca)\ny_test_pred = dt.predict(test_pca)","31cb5545":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n# classification report\nprint(classification_report(y_test, y_test_pred))\n# confusion matrix\nprint(\"*\"*50)\nprint(confusion_matrix(y_test,y_test_pred))\nprint(\"*\"*50)\n# accuracy of the decision tree\nprint('Decision Tree - Test Accuracy :',accuracy_score(y_test,y_test_pred))","462df32d":"from sklearn.model_selection import GridSearchCV","f1e570c4":"dt = DecisionTreeClassifier(random_state=100, class_weight='balanced')\n\nparams = {\n            'max_depth' : [2,3,5,10,20,30,40,50]\n        }\n\ngrid_search = GridSearchCV(estimator=dt,\n                           param_grid=params, \n                           cv=5, \n                           n_jobs=-1,\n                           verbose=1,\n                           scoring='accuracy')\n\ngrid_search.fit(train_pca, y_train_smote)","25504599":"print(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)","b4bf918c":"dt = DecisionTreeClassifier(random_state=100, class_weight='balanced')\n\nparams = {\n            'min_samples_leaf' : [5,10,20,50,100,150,200]\n        }\n\ngrid_search = GridSearchCV(estimator=dt,\n                           param_grid=params, \n                           cv=5, \n                           n_jobs=-1,\n                           verbose=1,\n                           scoring='accuracy')\n\ngrid_search.fit(train_pca, y_train_smote)","8822e75d":"print(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)","e0cb2cce":"dt = DecisionTreeClassifier(random_state=100, class_weight='balanced')\n\nparams = {\n            'min_samples_split' : [5,10,20,50,100,150,200]\n        }\n\ngrid_search = GridSearchCV(estimator=dt,\n                           param_grid=params, \n                           cv=5, \n                           n_jobs=-1,\n                           verbose=1,\n                           scoring='accuracy')\n\ngrid_search.fit(train_pca, y_train_smote)","cc74bbb1":"print(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)","93468c06":"dt = DecisionTreeClassifier(random_state=100, class_weight='balanced')\n\nparams = {\n            'max_depth' : [26,28,30,32,34], # Values before and after 30\n            'min_samples_leaf' : [1,3,5,7,9], # Values before and after 5\n            'min_samples_split' : [1,3,5,7,9] # Values before and after 5\n#             'criterion' : ['gini','entropy']\n        }\n\ngrid_search = GridSearchCV(estimator=dt,\n                           param_grid=params, \n                           cv=5, \n                           n_jobs=-1,\n                           verbose=1,\n                           scoring='accuracy')\n\ngrid_search.fit(train_pca, y_train_smote)","8abbcd01":"print(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)","44bf2c94":"# Keeping min_samples_leaf=5\ndt = DecisionTreeClassifier(max_depth=28, min_samples_leaf=1, min_samples_split=3, \n                            random_state=100, class_weight='balanced')\n\ndt.fit(train_pca, y_train_smote)","dd3702a7":"# Predicting on the Train set\ny_train_pred = dt.predict(train_pca)\ny_train_pred_proba = dt.predict_proba(train_pca)[:,1]","720d557c":"# Model evaluation metrics on train\nprint (\"Accuracy :\",metrics.roc_auc_score(y_train_smote, y_train_pred))\nprint (\"Recall\/Sensitivity :\",metrics.recall_score(y_train_smote, y_train_pred))\nprint (\"AUC Score (Train):\",metrics.roc_auc_score(y_train_smote, y_train_pred_proba))","bc046cc3":"# Predicting on the test set\ny_test_pred = dt.predict(test_pca)\ny_test_pred_proba = dt.predict_proba(test_pca)[:,1]","ab56d012":"# Model evaluation metrics on train\nprint (\"Accuracy :\",metrics.roc_auc_score(y_test, y_test_pred))\nprint (\"Recall\/Sensitivity :\",metrics.recall_score(y_test, y_test_pred))\nprint (\"AUC Score (Test):\",metrics.roc_auc_score(y_test, y_test_pred_proba))","0e054ea1":"from sklearn.ensemble import RandomForestClassifier","86d30483":"# Starting default model with depth of 3 and n_estimators of 10\nrf = RandomForestClassifier(random_state=100, max_depth=3, \n                            n_estimators=10, oob_score=True,\n                            class_weight='balanced', n_jobs=-1)","f037108c":"rf.fit(train_pca, y_train_smote)","3c3e13fb":"rf.oob_score_","5da6988f":"# Train and test predictions\ny_train_pred = rf.predict(train_pca)\ny_test_pred = rf.predict(test_pca)","012c0967":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n# classification report\nprint(classification_report(y_test, y_test_pred))\n# confusion matrix\nprint(\"*\"*50)\nprint(confusion_matrix(y_test,y_test_pred))\nprint(\"*\"*50)\n# accuracy of the decision tree\nprint('Decision Tree - Test Accuracy :',accuracy_score(y_test,y_test_pred))","b0acdde4":"classifier_rf = RandomForestClassifier(random_state=100, oob_score=True,\n                            class_weight='balanced', n_jobs=-1)\n\n# Create the parameter grid based on the results of random search \nparams = {\n    'n_estimators': [10, 30, 50, 100, 200]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=classifier_rf, param_grid=params, \n                          cv=5, n_jobs=-1, verbose=1, return_train_score=True,\n                           scoring = \"accuracy\")\n\ngrid_search.fit(train_pca, y_train_smote)","c3ca4cbe":"print(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)","01df8caf":"classifier_rf = RandomForestClassifier(random_state=100, oob_score=True,\n                            class_weight='balanced', n_jobs=-1)\n\n# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [1, 2, 5, 10, 20]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=classifier_rf, param_grid=params, \n                          cv=5, n_jobs=-1, verbose=1, return_train_score=True,\n                           scoring = \"accuracy\")\n\ngrid_search.fit(train_pca, y_train_smote)","96710156":"print(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)","7dd51ba4":"classifier_rf = RandomForestClassifier(random_state=100, oob_score=True,\n                            class_weight='balanced', n_jobs=-1)\n\n# Create the parameter grid based on the results of random search \nparams = {\n    'min_samples_leaf': [50, 100, 150, 200, 300]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=classifier_rf, param_grid=params, \n                          cv=4, n_jobs=-1, verbose=1, return_train_score=True,\n                           scoring = \"accuracy\")\n\ngrid_search.fit(train_pca, y_train_smote)","03ffd89f":"print(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)","ebe2cf9d":"classifier_rf = RandomForestClassifier(random_state=100, oob_score=True,\n                            class_weight='balanced', n_jobs=-1)\n\n# Create the parameter grid based on the results of random search \nparams = {\n    'min_samples_split': [50, 100, 150, 200, 300]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=classifier_rf, param_grid=params, \n                          cv=4, n_jobs=-1, verbose=1, return_train_score=True,\n                           scoring = \"accuracy\")\n\ngrid_search.fit(train_pca, y_train_smote)","150659b2":"print(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)","7cab5ed2":"classifier_rf = RandomForestClassifier(random_state=100, oob_score=True,\n                            class_weight='balanced', n_jobs=-1)\n\n# Create the parameter grid based on the results of random search \nparams = {\n    'n_estimators': [200,300],\n    'max_depth': [20,25],\n    'min_samples_leaf': [50,100],\n    'min_samples_split': [50,100]\n}\n\n# Instantiate the grid search model (using cv=3) as it takes more time\ngrid_search = GridSearchCV(estimator=classifier_rf, param_grid=params, \n                          cv=3, n_jobs=-1, verbose=1, return_train_score=True,\n                           scoring = \"accuracy\")\n\ngrid_search.fit(train_pca, y_train_smote)","15071353":"print(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)","15f5152b":"rf = RandomForestClassifier(class_weight='balanced', max_depth=25,\n                       min_samples_leaf=50, min_samples_split=50,\n                       n_estimators=300, n_jobs=-1, oob_score=True,\n                       random_state=100)\n\nrf.fit(train_pca, y_train_smote)","b9786bc4":"# Predicting on the Train set\ny_train_pred = rf.predict(train_pca)\ny_train_pred_proba = rf.predict_proba(train_pca)[:,1]","d6f4bec3":"# Model evaluation metrics on train\nprint (\"Accuracy :\",metrics.roc_auc_score(y_train_smote, y_train_pred))\nprint (\"Recall\/Sensitivity :\",metrics.recall_score(y_train_smote, y_train_pred))\nprint (\"AUC Score (Train):\",metrics.roc_auc_score(y_train_smote, y_train_pred_proba))","121e1a24":"# Predicting on the test set\ny_test_pred = rf.predict(test_pca)\ny_test_pred_proba = rf.predict_proba(test_pca)[:,1]","66841736":"# Model evaluation metrics on train\nprint (\"Accuracy :\",metrics.roc_auc_score(y_test, y_test_pred))\nprint (\"Recall\/Sensitivity :\",metrics.recall_score(y_test, y_test_pred))\nprint (\"AUC Score (Test):\",metrics.roc_auc_score(y_test, y_test_pred_proba))","88369ae5":"### Tuning n_estimators","6d3d33c0":"We need to handle the NULL values with the following steps:\n1. Identify missing NULL percentages in the columns (already done above)\n2. Identify if the missing values are meaningful\/actual missing data\n    - Meaningful missing - The associated data is NOT present in the other columns (for e.g. Date & Amount of recharge is missing, hence Count of Recharge will also have NA. Basically, no recharge was made)\n    - Actual missing - The associated data is present in the other columns (for e.g. Date & Amount of recharge have valid values, but Count of Recharge has NA. In this case, recharge was made but the count of recharge is missing)\n3. Impute Meaningful missing values with 0\n4. Check NULL percentage of columns again\n    - For higher NULL % (above 40%), drop the columns\n    - Impute Actual missing values with Median for continous columns and mode for categorical columns","2700e142":"### Model Performance - Logistic Regression with PCA","8f24de25":"### aon_years","8d538f23":"Random Forest classification model is performing much better than Decision Trees.\nHowever, the sensitivity\/recall score for Test is only 72% which is still less than 81.2% for Logistic Regression with PCA","954f644d":"## Checking columns which have NULL values","aa1b16f4":"Few Observations:\n- std_og_t2m_mou_g has 75% correlation with std_og_mou_6\n- std_og_t2t_mou_g has 74% correlation with std_og_mou_6\n- total_og_mou_6 has 83% correlation with std_og_mou_6\n- std_og_mou_7 has 75% correlation with std_og_mou_6\n- std_og_t2t_mou_g has 74% correlation with avg_onnet_mou_6_7","311c16c0":"## Handling class imbalance using SMOTE","e6ac77e4":"## Plotting ROC curve","ae71fd75":"Since 0.5 is an arbitrary cut-off chosen by us, we want to find the Optimal cut-off point where sensitivity and specificity are balanced","acf9c898":"## Random Forest classification with PCA","4915c9aa":"Median last day recharge amount for month 8 (action phase) is close to zero which indicates no recharge for churned customers. So, we can clearly see that they make less purchases in month 8 and they churn at month 9","11e5a1b7":"| Actual\/Predicted | Not Converted | Converted |\n| --- | --- | --- |\n| Not Converted | 15425   | 3884 |\n| Converted | 3753     | 15556 |","31cb345b":"## Logistic Regression with PCA","0f0ea643":"## Model 3","27f052b0":"### Imputing meaningful missing columns with zero","a6b0d86b":"- AUC score of train model is 0.90\n- AUC score of test model is 0.88","2e47c9b1":"### Model Evaluation on test","277a54a3":"**Month 7 - July**","7aae5f01":"## Base Model","bf26eaa1":"### Check if any NULL values are left","4ef6152e":"| Actual\/Predicted | Not Converted | Converted |\n| --- | --- | --- |\n| Not Converted | 14801   | 4508 |\n| Converted | 3280     | 16029 |","42f86132":"As the age of customer increases in the network, their churn decreases.\nSo, if we retain the customer for a longer period, they would churn less.","490a5965":"### Tuning min_samples_leaf","3847fade":"| Train\/Test | Accuracy | Sensitivity | Specificity |\n| --- | --- | --- | --- |\n| Train | 0.8022  | 0.8056 | 0.7988 | \n| Test  | 0.7988  | 0.7813 | 0.8003 |","aab3abd0":"#### avg_count_rech_2g3g_6_7","1f397544":"### Default Model","384562cb":"## Plot test ROC curve","b10ad5e7":"### Tuning min_samples_leaf","2c81de91":"| Important Features | Column Information |\n| --- | --- |\n| arpu_7    |             Average revenue per use for month 7  |\n| std_og_mou_7    |       STD outgoing calls minutes of usage for month 7  |\n| loc_ic_mou_7     |      Local incoming calls minutes of usage for month 7  |\n| std_ic_mou_8   |        STD incoming calls minutes of usage for month 8 |\n| spl_ic_mou_8    |       Special incoming calls minutes of usage for month 8  |\n| isd_ic_mou_8   |        ISD incoming calls minutes of usage for month 8  |\n| total_rech_num_8    |   Total number of recharges done for month 8  |\n| last_day_rch_amt_8  |   Total amount recharged on the last date of month 8  |\n| max_rech_data_8    |    Maximum data recharge for month 8  |\n| vol_2g_mb_8    |        Mobile internet usage volume (in MB) for 2G in month 8  |\n| sep_vbc_3g      |       Volume based cost for september (month 8)  |","feba3692":"'min_samples_leaf': 5 is the best parameter","cd5f1d84":"In churn prediction, we assume that there are three phases of customer lifecycle :\n- *The \u2018good\u2019 phase*: In this phase, the customer is happy with the service and behaves as usual. Corresponds to months 6 & 7\n- *The \u2018action\u2019 phase*: The customer experience starts to sore in this phase. Corresponds to month 8\n- *The \u2018churn\u2019 phase*: In this phase, the customer is said to have churned. We will define churn based on this phase. Corresponds to month 9. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available for prediction. Thus, after tagging churn as 1\/0 based on this phase, we discard all data corresponding to this phase.","5222f08b":"## Model 4","329a84de":"## Making Predictions on the test set","4311c88d":"Even with Precision and Recall, we get the optimal cut-off at 0.54","1a92c73d":"# Importing required libraries","301ae112":"| Train\/Test | Accuracy | Sensitivity | Specificity |\n| --- | --- | --- | --- |\n| Train | 0.8348  | 0.8361 | 0.8334 | \n| Test  | 0.8386  | 0.8120 | 0.8411 |","2c89745f":"#### arpu_6 vs churn","1aeaf3e6":"| Train\/Test | Accuracy | Sensitivity | Specificity |\n| --- | --- | --- | --- |\n| Train | 0.8348  | 0.8361 | 0.8334 | \n| Test  | 0.8386  | 0.8120 | 0.8411 |","ac3db874":"### 0.53 is the closest optimal cut-off point based on the intersection values in above curve","fd3a1b27":"It's between 40-80 features that explain 95% of the variance. To get the right number, we will try a more unsupervised approach to PCA","e56d7c51":"We will select 20 features using Recursive Feature elimination technique","43ac6c9f":"## Model 2","8410343b":"### 1st run of hyperparameter tuning","41f0d629":"We can see that the optimum cut-off point lies between 0.5 and 0.6. To find the exact cut-off, we will check various cut-off points between 0.5 and 0.6","18a188b7":"We can see that the maximum NULL value is 7.74% and all these mismatches are `Actual missing` data.\nFor these values, we will need to perform Median imputation to impute the data","655af97f":"First we will look at all `mobile data` related fields which are NULL. We will check the values of these columns together and see if they are meaningful missing. If the values are NA across all columns this would mean that they are meaningful missing values and that the customer is not using data (2g\/3g). We can impute these with zero in that case.\n\n- To understand better, if there is no date of last recharge, total recharge amount must be NA. Similarly, there will be no max recharge amount as well as count of recharge for both 2g and 3g.\n- Since there is no 2g and 3g, we will not have any revenue per user. With the data, we can also see that whenever night packs are NA, corresponding 3g and 2g fields are NA as well. And no data also means no access to facebook.\n- With this, all of these fields can be imputed with zero.","5dd772ec":"# Data Preparation","bc355d18":"We will be dropping \"avg_onnet_mou_6_7\" which has a p-value of 0.144. We need to remove p-values above 0.05","45e21d4d":"## Deriving Churn (target variable)","11559595":"## Model Performance - Logistic Regression without PCA (Interpretable model)","4749e7a2":"# Interpretable models without PCA","f8d3d758":"#### aon_years","e4371f73":"## Metrics Evaluation","fcadf546":"| Train\/Test | Accuracy | Sensitivity | AUC |\n| --- | --- | --- | --- |\n| Train | 0.9914  | 0.9964 | 0.9981 | \n| Test  | 0.6841  | 0.5080 | 0.6762 |","bb504f7e":"All p-values are less than 0.05 and all VIF values are less than 5. We will use this model for evaluation","66a479a1":"We have the Area Under the Curve as 0.88 which is a good model","d8871bd7":"Test evaluation","eed100f7":"## Important indicators of customer churn","d1614195":"| Train\/Test | Accuracy | Sensitivity | AUC |\n| --- | --- | --- | --- |\n| Train | 0.9914  | 0.9964 | 0.9981 | \n| Test  | 0.6841  | 0.5080 | 0.6762 |","abde0e31":"Class imbalance is not present and has been handled by SMOTE","efcae930":"### Check VIFs","7f3d3415":"### Model evaluation on train","10fd4fc0":"## Checking the data types and nullability of columns","222de00a":"## Create a dataframe with y_train (target value) and y_train_pred (predicted probability)","d8fd0f04":"## Finding the Optimal cut-off point","d626b510":"We can see that the optimum cut-off point lies between 0.5 and 0.6. To find the exact cut-off, we will check various cut-off points between 0.5 and 0.6","8f6e62f5":"We will drop the columns which are not required for analysis. This will constitute information which cannot be aggregated. In our case, we will remove\n- mobile_number (this is a primary identifier)\n- circle_id (another id field which indicates telephone circle area)\n- date fields (last_date_of_month_6, last_date_of_month_7, last_date_of_month_8, last_date_of_month_9)\n- date fields (date_of_last_rech_6, date_of_last_rech_7, date_of_last_rech_8, date_of_last_rech_9)","3372a062":"There is a heavy class imbalance for the target variable `churn`. We will need to handle this later on.\n- Churned customers make up only 8.14% of the data\n- Non-churned customers make up the remaining 91.86% of the data","a090791b":"'max_depth': 30 is the best parameter","d362ecd4":"#### aon_years V\/s churn","0971d0a2":"We are left with 216 columns","9095be21":"We will follow the below steps as part of Data Preparation:\n1. Derive new features (at least 3) that could be important indicators of churn\n2. Exploratory Data Analysis\n3. Outlier treatment\n4. Train, test & split\n5. Scaling","a3d72eb9":"## Train Test Split","886239d9":"Random Forest classification model is performing much better than Decision Trees.\nHowever, the sensitivity\/recall score for Test is only 72% which is still less than 81.2% for Logistic Regression with PCA","9409fd94":"## 0.54 is the closest optimal cut-off point based on the intersection values in above curve","dbaa4419":"### Tuning max_depth","26edadcb":"'min_samples_split': 5 is the best parameter","738c46b3":"### Tuning max_depth","dc8f6efe":"| Actual\/Predicted | Not Converted | Converted |\n| --- | --- | --- |\n| Not Converted | 6604   | 1647 |\n| Converted | 164     | 586 |","7d41ecce":"| Actual\/Predicted | Not Converted | Converted |\n| --- | --- | --- |\n| Not Converted | 14801   | 4508 |\n| Converted | 3280     | 16029 |","702599b8":"## EDA","14ba4033":"Class imbalance is not present and has been handled by SMOTE","e8f987cb":"### Model Performance - Random Forest with PCA","e3d2ccec":"# Data Understanding, Exploration & Cleaning","aeee86e0":"### Check NULL values again","2438979e":"We can see that the values are missing for all rows together and hence they are meaningful missing values","5a1bb617":"Test evaluation","05e00b21":"**Month 6 - June**","3012c737":"## Checking first few rows","49e2b6ae":"### Components from PCA","fa5d9d00":"### Create new column \"Predicted\" with 1 if 'Conversion_Prob' > 0.5 else 0","142c1b46":"| Actual\/Predicted | Not Converted | Converted |\n| --- | --- | --- |\n| Not Converted | 6940   | 1311 |\n| Converted | 141     | 609 |","c0c6e411":"## Imputation using median","af72174d":"## Derive new features","17cd2b7c":"### Check VIFs","d08ade1b":"### Model evaluation on train","b7a9b0ff":"## Train Test Split","66caa266":"# Non-interpretable models using PCA","b0655750":"### Handling date columns","d94f65f1":"## Precision and Recall","91cc5e4f":"As there are many features in the dataset, we will only look at the top 20 correlated features with churn","58476a6b":"### Metrics Evaluation on test","a8d0089d":"### Tuning min_samples_split","43ef00a8":"## Applying PCA on the dataset","79eb2242":"## Check the NULLable columns left","b1dfb817":"## Model Performance","5833e898":"## Handling class imbalance using SMOTE","4ac79ac8":"## Model Evaluation","e2b7309b":"#### avg_count_rech_2g3g_6_7 vs churn","e72491d0":"There are still **81** columns which have NULL values that needs to be handled","641e5a94":"## Recommendations to manage Customer churn","a7c2141b":"We have **156** features to start with building the base model","638d19af":"We will create a **Logistic Regression** model without PCA to get interpretable results and actionable recommendations to the company on how the churn rate can be reduced","5d72c78f":"### 1st run of hyperparameter tuning","28a374d2":"## Create new column \"Predicted\" with 1 if 'Conversion_Prob' > 0.5 else 0","3ace8429":"None of the 9th month columns are visible in above list and so we have successfully removed them","9baaa6f0":"We can see that the values are missing for all rows together and hence they are meaningful missing values","75ba7296":"We are expecting PCA to return components that are NOT multi-collinear. The heatmap will confirm this.","b71cc50e":"## Comparing performance of non-interpretable models","f364c183":"### Applying the transformation on the test set","a2da1941":"## Precision and Recall trade-off","083d3e71":"We can see that the values are missing for all rows together and hence they are meaningful missing values","333735f0":"There are no NULL values left in the data after median imputation","c7e3fb4e":"### Drop columns with NULL values over 40%","4ff1f069":"- AUC score of train model is 0.87\n- AUC score of test model is 0.86","5b3ccf19":"#### avg_offnet_mou_6_7","edb32116":"We will follow the below steps to derive churn:\n1. Calculate total incoming and outgoing minutes of usage\n2. Calculate 2g and 3g data consumption\n3. Create churn variable: those who have not used either calls or internet in the month of September are customers who have churned\n4. Check Churn percentage\n5. Delete all columns that belong to the churn month 9","4fdb3ec1":"We are retaining 30001 high value customers in the dataframe `df_hv`","504851db":"We can see that the values are missing for all rows together and hence they are meaningful missing values","a7e20753":"All the columns having NULL values are continuous in nature. Before we proceed further for imputation, we need to tackle couple of steps\n1. Filter the data for High value customers\n    - As we see in the problem statement, we need to predict the churn only for high-value customers i.e., the top 20% customers who contribute to 80% of the revenue\n    - This will also help in reducing the processing power as we will deal with less data than that entire dataset (~100K)\n2. We need to create a target variable to predict the churn for the 9th month\n    - We will create a churn variable as a target variable\n3. We will then proceed with median imputation of NULL values","2d07740d":"### 3. Model Performance - Random Forest with PCA","6fde9c0a":"## Important indicators of customer churn (identified by Logistic Regression model without PCA)","75a3f6c1":"### Model evaluation on test","7665205b":"| Train\/Test | Accuracy | Sensitivity | AUC |\n| --- | --- | --- | --- |\n| Train | 0.8975  | 0.8940 | 0.9640 | \n| Test  | 0.8045  | 0.7213 | 0.8840 |","fc88067f":"### Plotting heatmap of the correlation matrix","ebb35b55":"**Observations**\n- Looking at all three models above, we can clearly see that **Logistic Regression with PCA** is the clear choice of the ***best model*** among them to predict Customers who will churn.\n- This is because it has the highest Sensitivity score of 81.2% with test data.\n- We are using **Sensitivity as the evaluation metric** as it measures the proportion of actual positives that were correctly identified (i.e., Basically % of churn we correctly identify out of the actual churn)","7fa78d88":"An ROC (Receiver Operating Characteristic) curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","62dcb3a6":"Steps for filtering high value customers:\n* We will only look at the data for the first two months i.e., The \u2018good\u2019 phase: In this phase, the customer is happy with the service and behaves as usual.\n* Calculate total recharge amount (including both mobile and data recharge) for months 6 and 7\n    - Calculate total data recharge amount = Total data recharge * Average recharge amount (6 and 7)\n    - Calculate total recharge amount = total recharge amount mobile + total recharge amount data\n* Calculate average recharge done by the customer in June and July (6 & 7)\n    - Average recharge amount = (total recharge amount 6 + total recharge amount 7) \/ 2\n* Find out 70th percentile average recharge amount\n* Retain only those customers who have recharged their mobiles with >= 70th percentile","c99d31ce":"### Tuning min_samples_split","65aa7776":"### Create a dataframe with y_train (target value) and y_train_pred (predicted probability)","bfdd7b14":"## Feature Selection using RFE","329212cb":"- We will perform outlier treatment using **capping** techniques where values above or below the 99th or 1st quantile are considered outliers.\n- This method does not remove the data but may distort the distribution of the variablesAll the columns have skew and outliers. We will check for a few of those columns to compare pre and post outlier capping values","15f6b2ed":"### 1. Model Performance - Logistic Regression with PCA","c732fd45":"Recharge count of 2g and 3g is highest near zero data recharge and the trend follows the decreasing pattern of more data recharges in 6 & 7","53d8a207":"### Identifying meaningful missing columns","4838ca7a":"| Actual\/Predicted | Not Converted | Converted |\n| --- | --- | --- |\n| Not Converted | 15783         | 3526 |\n| Converted | 2913       | 16396 |","6ee142ec":"- AUC score of train model is 0.90\n- AUC score of test model is 0.88\n> The AUC scores are good for both and test and so we have built a good model","392dbc64":"We have the Area Under the Curve as 0.87 which is a good model. So, we will go ahead and find the Optimal Cut-off Point based on Sensivitivity V\/S Specificity trade-off","7750b2a7":"We have more data for customers with age of network in 1st, 2nd and 3rd years and the data decreases for customers with more time on network","d984cf58":"### Model Performance - Decision Tree with PCA","41371bdc":"### Creating correlation matrix and heatmap to understand the relationship b\/w features","11f7f627":"| Important Features | Coefficients | Column Information |\n| --- | --- | --- |\n| arpu_7    |              2.2679  |   Average revenue per use for month 7  |\n| std_og_mou_7    |        0.6073  |   STD outgoing calls minutes of usage for month 7  |\n| loc_ic_mou_7     |      -3.2994  |   Local incoming calls minutes of usage for month 7  |\n| std_ic_mou_8   |        -2.7490  |   STD incoming calls minutes of usage for month 8 |\n| spl_ic_mou_8    |       -3.7581  |   Special incoming calls minutes of usage for month 8  |\n| isd_ic_mou_8   |        -0.6990  |   ISD incoming calls minutes of usage for month 8  |\n| total_rech_num_8    |   -6.9914  |   Total number of recharges done for month 8  |\n| last_day_rch_amt_8  |   -4.2487  |   Total amount recharged on the last date of month 8  |\n| max_rech_data_8    |    -2.4759  |   Maximum data recharge for month 8  |\n| vol_2g_mb_8    |        -3.5718  |   Mobile internet usage volume (in MB) for 2G in month 8  |\n| sep_vbc_3g      |       -7.3101  |   Volume based cost for september (month 8)  |","e14a9933":"## Recommendations to manage Customer churn","041b0da2":"We wil be dropping columns with VIF over 10 first","e3fdc04a":"#### last_day_rch_amt_8 vs churn","d64ba184":"### Default Model","6dfa8c39":"We have the Area Under the Curve as 0.86 which is a good model","429efcb2":"- The model has very poor test Accuracy 68% and Sensitivity of 50.8%. This happens as Decision Tree is very prone to overfitting (sometimes even after pruning).\n- To resolve the issue, we can go ahead and build a Random Forest model which reduces the overfitting problem in Decision Trees.","acf206ae":"## Decision Tree classification with PCA","8384ae73":"### Bivariate analysis","03bf43ac":"The model has chosen 45 components to explain 90% variance","15f7faf7":"### avg_offnet_mou_6_7","ff5328b4":"# Telecom Churn Case Study","b39cff66":"### Final tuned model","57d54fd0":"We will derive the following features\n1. Tenure of customer based on aon in years (Age on Network)\n2. As 6 & 7 months are good periods, we can derive few features to compare against how they performed against 8th month.\n    - Average On Network Minutes of Usage for 6 & 7\n    - Average Off Network Minutes of Usage for 6 & 7\n3. Avg count of total recharge (2G + 3G) for 6 & 7","12134c75":"- The model has very poor test Accuracy 68% and Sensitivity of 50.8%. This happens as Decision Tree is very prone to overfitting (sometimes even after pruning).\n- To resolve the issue, we can go ahead and build a Random Forest model which reduces the overfitting problem in Decision Trees.","babf3443":"**Month 8 - August**","afe43d58":"## Checking the summary statistics of the dataset","63d9c486":"## Understanding the business objective and the data\nThe dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively. \n\n\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.","396692f4":"### avg_count_rech_2g3g_6_7","c920bab6":"| Actual\/Predicted | Not Converted | Converted |\n| --- | --- | --- |\n| Not Converted | 16094   | 3215 |\n| Converted | 3163     | 16146 |","0a20ee08":"## Checking size of the dataset","cbb3fa40":"## Business problem overview\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\n\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n\nIn this project, we will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.","22ac1243":"**Month 9 - September**","7f170175":"Data is more for less minutes of on network usage at 0 and steadily decreases with more minutes of usage","b4619a8a":"# Importing dataset","3d64e32d":"#### avg_onnet_mou_6_7","18a0340f":"### Model evaluation on test","92529265":"### Check NULL values again","73d4721e":"- AUC score of train model is 0.87\n- AUC score of test model is 0.86\n> The AUC scores are good for both and test and so we have built a good model","1cdb6c18":"### Initial Metrics Evaluation on train","52f9170f":"### Merics Evaluation on train - optimal","b3162290":"The **sensitivity** score of Logistic Regression with PCA has a much better score compared to Logistic Regression without PCA","43827b7e":"### Drop columns not necessary for analysis","0e78f5ed":"| Train\/Test | Accuracy | Sensitivity | AUC |\n| --- | --- | --- | --- |\n| Train | 0.8975  | 0.8940 | 0.9640 | \n| Test  | 0.8045  | 0.7213 | 0.8840 |","4763f402":"### Univariate analysis","abfa764c":"### 2. Model Performance - Decision Tree with PCA","8783c026":"## Dropping unimportant columns","604c9f15":"### avg_onnet_mou_6_7","3b021bda":"### Finding the Optimal cut-off point","010fc73a":"As all the values are in red (except the diagonals), we can confirm that the PCA components do not exhibit multi collinearity","9ebeb0b7":"For this telecom company, it's more important to identify churners than non-churners. This is because to reduce churn, customer retention is the most important criterion than anything else. <br>\nSo, we will have to choose **Sensitivity\/Recall** as our evaluation metric as it measures the proportion of actual positives that were correctly identified. <br>\nThe important predictor attributes of churn are listed above. <br>\n\n***Recommendations:***\n1. The number of recharges, amount of recharge and the last day of recharge are important indicators of customer usage. If any of them decrease, it could be a sign that the customer is trying to wait the remaining period of validity and then try to switch providers. Recharge packs with discount can be offered during this period to retain customer.\n2. Internet usage is also an important variable as it indicates if the customer is actively using the mobile for operations such as social networking, mobile banking, bill payments etc., and any reduction\/non-usage indicates higher possibility of churn. Data Packs can be offered as a bundle\/reduced prices\/free for a month to attract customer to be in the network. \n3. Incoming and Outgoing calls in the last good month 7 and action phase month 8 also need to be taken into consideration\n4. STD, ISD & Special incoming calls for month 8 need to monitored for usage consistently and see if we are able to identify any drop in the usage pattern\n5. Depending on the volume based cost for 8th month, we can observe if the customer is making a commitment to stay further on the network through month 9\n6. Average revenue per user should also be monitored for reduction in consumption which would indicate potential customer churn","dd4b70c9":"#### onnet_mou_8 vs churn","38ef7e40":"We have the Area Under the Curve as 0.90 which is a good model. So, we will go ahead and find the Optimal Cut-off Point based on Sensivitivity V\/S Specificity trade-off","745c88a4":"We will be dropping the columns if the variance in the column is very less. We do this because they provides little scope for the model to learn.","8eaef19b":"## Scaling (Min Max Scaler)","f8174f3b":"There are various ways to define churn, such as:\n\n> Revenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as \u2018customers who have generated less than INR 4 per month in total\/average\/median revenue\u2019.\n\n> Usage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\n\nFor this problem statement, we will use **usage-based definition** to define churn","35826eb9":"## Filter high value customers","89e1f722":"Data is more for less minutes of off network usage at 0 and steadily decreases with more minutes of usage","d6cf9b59":"Median Average data recharge count for months 6&7 is close to zero which indicates no recharge for churned customers.\nFor non-churned customers, we can see that they made at least one recharge","a2901106":"We can see a heavy class imbalance for churn class 1 (People who have moved out of the network). This needs to be handled later on","74e06ae4":"### Model Evaluation on train","3aed7808":"## Outlier treatment","823bc64b":"Since 0.5 is an arbitrary cut-off chosen by us, we want to find the Optimal cut-off point where sensitivity and specificity are balanced","37139fdd":"## Scaling (Min Max Scaler)","3dba3479":"### Plot test ROC curve","4a9e6e99":"#### churn","9e3ff545":"### Final tuned model","ab1e1ad4":"# Conclusion","fa1095b4":"All the NULLable columns are continuous in nature and we need to impute the data with median","fa4341d3":"### Plotting ROC curve","642975c7":"### Check VIFs","435e9f59":"| Train\/Test | Accuracy | Sensitivity | Specificity |\n| --- | --- | --- | --- |\n| Train | 0.8022  | 0.8056 | 0.7988 | \n| Test  | 0.7988  | 0.7813 | 0.8003 |","c73297f5":"For this telecom company, it's more important to identify churners than non-churners. This is because to reduce churn, customer retention is the most important criterion than anything else. <br>\nSo, we will have to choose **Sensitivity\/Recall** as our evaluation metric as it measures the proportion of actual positives that were correctly identified. <br>\nThe following important predictor attributes are indicators of churn. Keep note of the following:\n1. The number of recharges, amount of recharge and the last day of recharge are important indicators of customer usage. If any of them decrease, it could be a sign that the customer is trying to wait the remaining period of validity and then try to switch providers. Recharge packs with discount can be offered during this period to retain customer.\n2. Internet usage is also an important variable as it indicates if the customer is actively using the mobile for operations such as social networking, mobile banking, bill payments etc., and any reduction\/non-usage indicates higher possibility of churn. Data Packs can be offered as a bundle\/reduced prices\/free for a month to attract customer to be in the network. \n3. Incoming and Outgoing calls in the last good month 7 and action phase month 8 also need to be taken into consideration\n4. STD, ISD & Special incoming calls for month 8 need to monitored for usage consistently and see if we are able to identify any drop in the usage pattern\n5. Depending on the volume based cost for 8th month, we can observe if the customer is making a commitment to stay further on the network through month 9\n6. Average revenue per user should also be monitored for reduction in consumption which would indicate potential customer churn","f532d957":"Median on network minutes of usage for month 8 (action phase) is very less for churned customers compared to non-churned customers. This clearly shows their usage also decreasing over the network before they churn.","d45fecd4":"Median average revenue per user for month 6 is higher for churned customers compared to non-churned customers","a32427a7":"## Handling NULL values"}}