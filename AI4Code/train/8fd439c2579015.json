{"cell_type":{"96fa8fb2":"code","ae3aa870":"code","14624dee":"code","a64984ab":"code","3afd35ff":"code","d9053233":"code","fa98d75b":"code","ede48727":"code","91faa900":"code","21dc00f4":"code","181732e8":"code","90c82f95":"code","139678a4":"code","7a87d8ff":"code","5fc1b98b":"code","2e4880fa":"code","8c0ef4b5":"code","e0c9d376":"code","687d11ac":"code","aa16fd13":"code","31602bb8":"code","ea2daef7":"code","584e0d92":"code","6c27787d":"code","8758d6f0":"code","764261e6":"code","bf649678":"code","803f04fb":"code","fbaad05b":"code","24f31c24":"markdown","4b564a3f":"markdown","b97ad479":"markdown","85853887":"markdown","e3e9235d":"markdown","816713a2":"markdown","d1148033":"markdown","47e047b2":"markdown","714b1900":"markdown","5d1615cb":"markdown","8ca92b57":"markdown","98995afb":"markdown","f44ef7db":"markdown","2e2a18b3":"markdown","767dd99f":"markdown","0f361fcb":"markdown","2269f3c3":"markdown","5b8fab05":"markdown","a0dff954":"markdown","77904e2a":"markdown","95db4983":"markdown","1cd1ba7e":"markdown"},"source":{"96fa8fb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae3aa870":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","14624dee":"train.isnull().sum()","a64984ab":"df = train.drop('location', axis=1)\ndf = df.drop('keyword', axis=1)\ndf = df.drop('id', axis=1)","3afd35ff":"df.head()","d9053233":"df_test = test.drop('location', axis=1)\ndf_test = df_test.drop('keyword', axis=1)\ndf_test = df_test.drop('id', axis=1)","fa98d75b":"df_test.head()","ede48727":"len(df)","91faa900":"blanks = []\n\nfor i,tx,tg in df.itertuples():\n    if type(tx) == str:\n        if tx.isspace():\n            blanks.append(i)\nprint(len(blanks), \"blanks\", blanks)","21dc00f4":"df.drop(blanks, inplace=True)\n\nlen(df)","181732e8":"blanks = []\n\nfor i,tx in df_test.itertuples():\n    if type(tx) == str:\n        if tx.isspace():\n            blanks.append(i)\nprint(len(blanks), \"blanks\", blanks)","90c82f95":"df_test.drop(blanks, inplace=True)\n\nlen(df_test)","139678a4":"df['target'].value_counts()","7a87d8ff":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5fc1b98b":"sns.countplot(df['target'])","2e4880fa":"from sklearn.model_selection import train_test_split\nX = df['text']\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","8c0ef4b5":"import string\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load('en')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Creating our tokenizer function\ndef spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = parser(sentence)\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens","e0c9d376":"from sklearn.base import TransformerMixin\n# Custom transformer using spaCy\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    return text.strip().lower()","687d11ac":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics","aa16fd13":"model = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01]\ngamma = ['scale']\n# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nclassifier = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ntext_clf_model = Pipeline([(\"cleaner\", predictors()),\n                           ('tfidf', TfidfVectorizer(tokenizer = spacy_tokenizer)),\n                           ('clf',classifier)])\n#                              ('clf', GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)),\n#                             ])\ntext_clf_model.fit(X_train, y_train)\npredictions = text_clf_model.predict(X_test)","31602bb8":"sns.heatmap(metrics.confusion_matrix(y_test,predictions),annot=True,cmap='Blues', fmt='g')","ea2daef7":"print(\"Classification Report:\")\nprint(metrics.classification_report(y_test,predictions))\nprint(\"\\n\")\nprint(f\"Accuracy score is {metrics.accuracy_score(y_test,predictions)}\")\n   ","584e0d92":"X_predict = df_test['text']","6c27787d":"df_test_predictions = text_clf_model.predict(X_predict)","8758d6f0":"sample_predicted_values = pd.DataFrame(df_test_predictions, columns=['predicted_target'])\n","764261e6":"sample = sample.join(sample_predicted_values)","bf649678":"sample = sample.drop('target', axis=1)","803f04fb":"sample","fbaad05b":"sample.to_csv('Submissions2.csv', index=False)","24f31c24":"I next made a pipeline object performing the activities like Cleaning the text, TfIdf vectorizing, build the model using SVC, do Grid Search for finding the best parameters for the model and then finally fit the model on the training set. After the model has been fit I will do the predictions on the X_test set.","4b564a3f":"I started the data preprocessing by splitting the df dataframe into training set set and test set. ","b97ad479":"Next I made a countplot on the target values of the dataframe df.","85853887":"Now I made a count of how many different types of target values that is present in the dataframe df. It seems the dataframe df has **4342 tweets** which don't actually signify a disaster and **3271 tweets** which signify some kind of disaster.","e3e9235d":"I first imported the necessary libraries as shown below.","816713a2":"Now I will do a predict on the df_test based on our model and save the predictions on a CSV file.","d1148033":"# Building the model","47e047b2":"From the above plot it is clear there is higher number of tweets with a target value of 0. ","714b1900":"I repeated the same steps for the test dataset and created a new df_test dataframe out of it and displayed the first five rows of the dataframe.","5d1615cb":"Now I checked the number of rows present in the df dataframe. It can be seen that the df dataframe has 7613 rows.","8ca92b57":"I start by reading the input data. The train data set would be the one on which I will train our model. The test dataset would be the one on which I will do my predictions. Finally the sample dataset will be used for storing my final predicted target values against the id number.","98995afb":"# Exploring the data","f44ef7db":"I then created a new dataframe by droping the columns location, keyword and id columns from the train dataset.","2e2a18b3":"Sometimes the texts column may not have null values but could be represented as a blank string. Such texts won't be of any use in our prediction. Hence, I will next filter out such texts if any. ","767dd99f":"Since, there were no blank spaces the length of the dataframe df remained the same even after executing the above code.","0f361fcb":"Next I repeated the same for the df_test dataframe.","2269f3c3":"# DATA PREPROCESSING","5b8fab05":"Next I imported the Spacy library for text preprocessing. This would include the activities like tokenizing, lemmatizing, NER, lower case conversion, stop words removal, punctuation removal, etc as shown below.","a0dff954":"Next I checked for null values in all the columns of the train dataset. It can be seen from the below result that the columns **keyword** has **61 null values** and **location** has **2533 null values**. Probably it would be better if we drop these columns for now.","77904e2a":"From the above results it seems that the text column under dataframe has no blank tweets. But, if there were any the below piece of code will filter them out.","95db4983":"I next printed down the Confusion matrix, classification report and accuracy score. It seems my model gave an accuracy 0.80 which is not bad as a start.","1cd1ba7e":"Next I displayed the first 5 rows of the new dataframe df. It can be seen now the dataframe df has two columns **text** and **target**."}}