{"cell_type":{"21f1c765":"code","c458ca81":"code","97e964f5":"code","45d5d028":"code","e9836517":"code","eab9f93d":"code","5d8a13fd":"code","1e305ee1":"code","99837d25":"code","80cb569e":"code","b1f11ebd":"code","97d17a99":"code","ca366f47":"code","393a42e0":"code","966b37fb":"code","530ee949":"code","f332d023":"code","d40528fe":"code","c593f715":"code","787699ab":"code","7e6b9e04":"code","35345311":"code","1d8ac805":"code","3914a098":"code","61586638":"code","67b3900b":"code","bf8cdda2":"code","ef7a10ce":"markdown","5e4a676f":"markdown","fcfa42db":"markdown","c31b5ec9":"markdown","bf5e17b6":"markdown","7691fdd0":"markdown","4f4c5e25":"markdown","40501b03":"markdown","73f920f7":"markdown","d8b62e01":"markdown","db91ead8":"markdown","91270b25":"markdown","1078b871":"markdown","0af3151f":"markdown","f22e67f5":"markdown","3565894f":"markdown","0edacad7":"markdown","ac9c3983":"markdown","a2f89872":"markdown","53784675":"markdown","0ded4ed5":"markdown","a5d75639":"markdown","51c5c4fc":"markdown","e0dbd476":"markdown","8a6196cb":"markdown","4f344f4b":"markdown","dbb87fd4":"markdown","43413aef":"markdown","945a7096":"markdown","82c79f66":"markdown"},"source":{"21f1c765":"import pandas as pd\nimport numpy as np\nimport datetime\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import r2_score\nimport warnings\nwarnings.warn = False\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c458ca81":"gender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\nprint(\"Training set = \",train.shape)\nprint(\"Testing set = \",test.shape)\nprint(\"Sum of Missing Values (Train\/Test)= \",train.isna().sum().sum(),\"(\",test.isna().sum().sum(),\")\")\n","97e964f5":"print(\"Survival Rate (in Training Data) =\",round(train.Survived.sum()\/train.shape[0]*100,2),\"%\")","45d5d028":"train.describe(include='all')","e9836517":"print(\"Missing Values in Training Dataset:\\n\",round(train.isna().sum()\/train.shape[0]*100,2))\nprint(\"Missing Values in Testing Dataset:\\n\",round(test.isna().sum()\/test.shape[0]*100,2))","eab9f93d":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score,mean_squared_error\n\n\ndef get_accuracy_score(data,col):\n  X_train, X_test, y_train, y_test = train_test_split(data[col], data['Age'], test_size=0.3, random_state=7)\n\n  lr1=LinearRegression()\n  lr1.fit(X_train,y_train)\n  y_pred = lr1.predict(X_test)\n  return mean_squared_error(y_test, y_pred)","5d8a13fd":"def get_missing_age(data):\n  index = np.where(data.Age.isnull())\n  \n  # Find Prediction based on Present Elements\n  data1=data[~data.Age.isnull()]\n  acc_score0 = get_accuracy_score(data1,['SibSp','Parch','Fare'])\n  acc_score1 = get_accuracy_score(data1,['SibSp','Parch'])\n  acc_score2 = get_accuracy_score(data1,['SibSp','Fare'])\n  acc_score3 = get_accuracy_score(data1,['Parch','Fare'])\n  min_score = min(acc_score0,acc_score1,acc_score2,acc_score3) # Error to be minimized\n\n  # Fit Model by Best Feature Selection\n  data2 = data[data.Age.isnull()] \n  if min_score == acc_score0:\n    col = ['SibSp','Parch','Fare']  \n  else:\n    if min_score == acc_score1:\n      col = ['SibSp','Parch']\n    else:\n      if min_score == acc_score2:\n        col = ['SibSp','Fare']                \n      else:\n        col = ['Parch','Fare']        \n  X_train, y_train, X_test = data1[col], data1['Age'], data2[col]    \n\n  # Do Prediction on Absent Elements\n  lr = LinearRegression()\n  lr.fit(X_train,y_train)\n  \n\n  data2 = data2.drop(columns=['Age'],axis=1)\n  data2['Age']=[max(0,min(100,i)) for i in lr.predict(X_test)]\n  \n  temp=data1['Age']\n  data1 = data1.drop(columns=['Age'],axis=1)\n  data1['Age']=temp\n\n  return data1.append(data2)","1e305ee1":"def impute_fare(dataset):\n  if dataset.Fare.isnull().sum()<=0:\n    return dataset\n  else:\n    ok_set = dataset[~dataset.Fare.isnull()]\n    to_set = dataset[dataset.Fare.isnull()]\n\n    l=list()\n    for i in range(to_set.shape[0]):  \n      l.append(ok_set.Fare[(ok_set.Pclass==to_set.iloc[i]['Pclass']) & (ok_set.Parch==to_set.iloc[i]['Parch'])].mean())\n\n    # Structural Re-format (Faster compared to For Loop)\n    temp = ok_set['Fare']\n    ok_set = ok_set.drop(columns='Fare',axis=1)\n    ok_set['Fare'] = temp\n    \n    to_set=to_set.drop(columns='Fare',axis=1)\n    to_set['Fare']=l\n\n    return ok_set.append(to_set)","99837d25":"def impute_cabin(data):\n  if data.Cabin.isnull().sum()<=0:\n    return data\n  else:\n    ok_set = data[~data.Cabin.isnull()]\n    to_set = data[data.Cabin.isnull()]\n\n    #l=list()\n    #for i in range(ok_set.shape[0]):\n    # l.append(ok_set.iloc[i]['Cabin'][0])\n\n    l = [ok_set.iloc[i]['Cabin'][0] for i in range(ok_set.shape[0]) ] # Cannot put in next assignment, drop used\n    ok_set = ok_set.drop(columns=['Cabin'],axis=1)\n    ok_set['Cabin'] = l\n\n    to_set = to_set.drop(columns='Cabin',axis=1)\n    to_set['Cabin'] = 'U'\n\n    return ok_set.append(to_set)","80cb569e":"def impute_embarked(data):\n  data.Embarked.replace('S',1,inplace=True)\n  data.Embarked.replace('C',2,inplace=True)\n  data.Embarked.replace('Q',3,inplace=True)\n  if data.Embarked.isnull().sum()<=0:\n    return data\n  else:\n    ok_data = data[~data.Embarked.isnull()]\n    to_data = data[data.Embarked.isnull()]\n\n    lr = LinearRegression() # Using Regression, the mode of Embarked is captured\n    lr.fit(ok_data[['Pclass','Fare','Parch']],ok_data['Embarked'])\n    \n    temp = ok_data['Embarked']\n    ok_data = ok_data.drop(columns='Embarked',axis=1)\n    ok_data['Embarked'] = temp\n    \n    to_data = to_data.drop(columns='Embarked',axis=1)\n    temp = [int(i) for i in lr.predict(to_data[['Pclass','Fare','Parch']])]\n    to_data['Embarked'] = temp\n\n    return ok_data.append(to_data)","b1f11ebd":"# Impute Missing Value \n# ~ Fare\ntest = impute_fare(test)\n\n# ~ Age\ntrain = get_missing_age(train)\ntest = get_missing_age(test)\n\n# ~ Cabin\ntrain=impute_cabin(train)\ntest=impute_cabin(test)\n\n# ~ Embarked\ntrain = impute_embarked(train)\ntest = impute_embarked(test)","97d17a99":"# Age Grouping\nage_group = 3\nmax_age = max(train.Age)\nmin_age = min(train.Age)\n\ntrain['Age_Group'] = [int(i) for i in round((train.Age-min_age)\/((max_age-min_age)\/age_group),0)+1]\ntest['Age_Group']  = [int(i) for i in round((test.Age-min_age)\/((max_age-min_age)\/age_group),0)+1]\ntrain = train.drop(columns='Age')\ntest = test.drop(columns='Age')","ca366f47":"# Fare: Based on Pclass and Parch\n# Depending on Passenger class and number of family members, Fare is provided. \n# Thus, Fare is more like a dependent feature (not correlated) when compared to other features.\npd.pivot_table(train, values='Fare', index=['Pclass'],columns=['Parch'], aggfunc=np.mean)","393a42e0":"def get_feature_count(data,col_names):\n  df_all=pd.DataFrame()\n  for i in col_names:\n    u = data[i].unique()\n    temp=pd.DataFrame()\n    for j in u:\n      m = (data[i]==j).sum()\n      temp = temp.append([[j,m]])\n    temp['col_name'] = i    \n    df_all = df_all.append(temp)\n\n  df_all.columns = ['X','Y','Feature']\n  return df_all","966b37fb":"df = get_feature_count(train,['Pclass','Sex','Cabin','Embarked','Age_Group','SibSp','Parch'])\n\nfig=px.bar(data_frame=df, x='X',y='Y',color='Y',facet_col='Feature',facet_col_wrap=7,width=1000,height=350)\nfig.update_xaxes(matches=None)\nfig.update_yaxes(matches=None)\nfig.show()","530ee949":"# How people paid Fair based on Pclass\nfig=px.histogram(train,x='Fare',color='Pclass',height=300)\nfig.show()","f332d023":"# Who survided and who didn't get to?\ntemp_table = train\ntemp_table['Survive_Copy'] = 1\n\ncols=['Pclass','Sex','SibSp','Parch','Cabin','Embarked','Age_Group']\nfor i in cols:\n  print(\"\\nSurvival per\",i,\"\\n\",'_'*70,\"\\n\",\n        pd.pivot_table(temp_table, values='Survive_Copy', index=['Survived'],columns=[i], aggfunc=np.sum))","d40528fe":"# Survivor Selection \n\ncols=['Pclass','Sex','SibSp','Parch','Cabin','Embarked','Age_Group']\nfor i in cols:\n  for j in cols:\n    if i==j:\n      # do nothing\n      d=0\n    else:\n      print(\"\\n\",i,\"vs\",j,\"\\n\",'_'*70,\"\\n\",\n            pd.pivot_table(train, values='Survived', index=[i],columns=[j], aggfunc=np.sum))","c593f715":"result = pd.DataFrame()\n\n# Rearrange Columns (Before Modeling)\nX_train_ID = train[['PassengerId']]\nX_train = train[['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked', 'Fare', 'Age_Group']]\ny_train = train[['Survived']]\nX_test_ID = test[['PassengerId']]\nX_test  = test[['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked', 'Fare', 'Age_Group']]\nprint(\"Original Dimension\\n\",X_train.shape,y_train.shape,X_test.shape)\n\nresult['PassengerId']=X_test_ID.PassengerId\n\n# One hot Encoding for Sex, Cabin, Embarked\nX_train = pd.get_dummies(X_train, columns=['Sex','Cabin','Embarked'])\nX_test  = pd.get_dummies(X_test , columns=['Sex','Cabin','Embarked'])\nprint(\"Post OHE Dimension\\n\",X_train.shape,X_test.shape)\n\n\n# Post OHE: Remove irrelevant column (remove multi-collinearity) & rename column (for consistency)\nX_train = X_train.drop(columns=['Sex_female','Cabin_U','Cabin_T','Embarked_1.0'],axis=1)\nX_test  = X_test.drop(columns=['Sex_female','Cabin_U','Embarked_1'],axis=1)\nX_train.columns = X_test.columns\nprint(\"Post Dimension Process\\n\",X_train.shape,X_test.shape)\n\nprint(\"List of columns:\\n\",*X_train.columns)","787699ab":"from sklearn.model_selection import train_test_split\n\ntemp_train = X_train\ncols=temp_train.columns\ntemp_train['target'] = y_train\n\na_train, a_test, b_train, b_test = train_test_split(temp_train[cols],temp_train['target'], test_size=0.3, random_state=7)\nprint(\"Re-Split dimenstions\\n\",a_train.shape,a_test.shape,b_train.shape,b_test.shape)","7e6b9e04":"from sklearn.metrics import accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='lbfgs',max_iter=1000,tol=0.001,random_state=7)\nlr.fit(a_train,b_train)\nprint(\"Accuracy on Training Set=\",round(accuracy_score(b_test,lr.predict(a_test))*100,2))\n\nresult['Log_Reg'] = lr.predict(X_test)","35345311":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion='gini',max_features=.75,max_depth=3,random_state=7)\ndt.fit(a_train,b_train)\nprint(\"Accuracy on Training Set=\",round(accuracy_score(b_test,dt.predict(a_test))*100,2))\n\nresult['DT'] = dt.predict(X_test)","1d8ac805":"from sklearn.linear_model import Perceptron\np = Perceptron(max_iter=50,shuffle=True, tol=0.001,random_state=7)\np.fit(a_train,b_train)\nprint(\"Accuracy on Training Set=\",round(accuracy_score(b_test,p.predict(a_test))*100,2))\n\nresult['Perceptron'] = p.predict(X_test)","3914a098":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(10,5), max_iter=1000,\n                    n_iter_no_change=5,learning_rate='constant',shuffle=True,\n                    validation_fraction=0.1,tol=0.001, random_state=7)\nmlp.fit(a_train,b_train)\nprint(\"Accuracy on Training Set=\",round(accuracy_score(b_test,mlp.predict(a_test))*100,2))\n\nresult['MLP'] = mlp.predict(X_test)","61586638":"print(\"Survival Rate from Logistic Regression    =\",round(result.Log_Reg.sum()\/result.shape[0]*100,2)   ,\"%\")\nprint(\"Survival Rate from Decision Tree          =\",round(result.DT.sum()\/result.shape[0]*100,2)        ,\"%\")\nprint(\"Survival Rate from Perceptron             =\",round(result.Perceptron.sum()\/result.shape[0]*100,2),\"%\")\nprint(\"Survival Rate from Multi-layer Perceptron =\",round(result.MLP.sum()\/result.shape[0]*100,2)       ,\"%\")","67b3900b":"result['Survived'] = [int(result.loc[i][[1,2,4]].mode()) for i in range(result.shape[0]) ]                  \nprint(\"Survival Response (from each method)\\n\",result.sum()[1:])    ","bf8cdda2":"submission = result[['PassengerId','Survived']]\nsubmission.to_csv(\"submission.csv\", index=False)","ef7a10ce":"Fare is a function of Passenger Class and Number of Family Members. However, certain differences are noticable, which arises due to cost of space. For example: A room with capacity of 4 people will have optimal fare when no less or no more than 4 people are allocated. ","5e4a676f":"### Age Grouping\nGrouped into 3 groups (1=Young, 2=Mid Age, 3=Old).","fcfa42db":"## Feature Engineering - II","c31b5ec9":"> #### Impute Missing Value - Age","bf5e17b6":"### Overall Fare \nCannot be grouped, as Fare is dependent on Pclass and Parch Feature (look at pivot table below)","7691fdd0":"#### Logistic Regression","4f4c5e25":"##### Train-Test split of Training Set\n- To check accuracy score","40501b03":"## Ratio Analysis\n\n","73f920f7":"## Feature Engineering - I\n#### Regression - To impute missing value in Age","d8b62e01":"#### Impute Missing Value - Fare","db91ead8":"**Observation**:\n- Pclass: Most passenger are from class 3, which is more than the number of passengers from Class 1 and 2 combined. \n- More male passengers have boarded the Titanic compared to number of female passengers.\n- Very limited information on Cabin allocated to passengers is available. \n- Embarked: Most passenger have board Titanic from berth S, which is more than the number of passengers who have board from berth C and Q combined. \n- Age Group: Mostly middle aged people are on Titanic, which is higher than the number of people from other age groups (young and old) combined.\n- SibSp and Parch: Most passengers are without any sibling or relatives on board. ","91270b25":"#### Univariate Analysis","1078b871":"### Impute Missing Value - Cabin","0af3151f":" ## Get Library","f22e67f5":"#### Multi-Layer Perceptron","3565894f":"## Initial Assesment of Data\n- Descriptive Statistics\n- Missing Values","0edacad7":"#### Perceptron","ac9c3983":"#### Decision Tree ","a2f89872":"#### Bivariate Analysis - Using Pivot Table\nWho survived Titanic?","53784675":"**Taking Maximum Vote from all Classifier Model**","0ded4ed5":"## Model Building","a5d75639":"### Impute Missing Value - Embarked","51c5c4fc":"#### Tri-Variate Analysis - Using Pivot Table","e0dbd476":"Findings which contradicts previous results are reported:\n\n- Irrespective of feature Cabin and Embarked, First class passenger were given more priority to be saved. \n\n- Though women with chhildren were given higher priority to be saved, \nit is observed that people having fewer children had higher chance of survival\n\n- Most survivors from first class were from Cabin A-E, which previously could not be identified. \n\n*Please Note:* NaN in above table implies no possible combination\n\n\n\n","8a6196cb":"## Visualize Data","4f344f4b":"- Most passengers from Pclass = 1 is saved, where as most people from Pclass = 3 could not be saved.\n- More women were saved compared to men\n- Contrary to total head count, ratio suggests that people with sibling or kids were given priority, besides women. \n\n  *Note: Family of more than one children is also seen in SibSp feature column.*\n- Cabin findings is not conclusive, thus no comments\n- Number of people who embarked at C have higher survival rate compared to S and Q. \n- Young peple seem to have a higher survival rate compared to older people.\n\n\nTo gain a more granular look at data, multivariate analysis is performed on 3 variables at a time.","dbb87fd4":"Features - Age, Cabin, Embarked, Fare have missing values and engineered as follows,\n- Age: Missing values can be obtained from other features such as SibSp, Parch, Fare through specific value, if-else condition or linear regression (used here)\n- Cabin: Missing values cannot be obtained from other features. Therefore, missing values can be \n  - removed (due to high percentage of missing values), or \n  - imputed as a seperate category (done here)\n- Embarked: Missing values can be obtained from other features such as Pclass, Fare, Parch through linear regression, similar to Age imputation.\n- Fare: Missing values can be obtained from other features such as Pclass, Parch through specific value such as mean through if-else condition.\n","43413aef":"Assuming training and testing dataset samples represent total population adequately, the quality of solution obtained can be analyzed. \n\nThe total survival rate in training dataset is around 38.38 %. Similar percentage can be expected from Testing dataset.\n- Logistic regression model with an accuracy of 80.6% predicts survival of passenger, which is 38.76% (very close to 38.38%)\n\n- Decision Tree Classifier with an accuracy of 82.46% predicts survival of passenger, which is 36.36% (close to 38.38%)\n\n- Perceptron model with an accuracy of 70.52% predicts survival of passenger, which is 47.85% (far from 38.38%)\n\n- Multi-layered Perceptron (MLP) model with an accuracy of 82.09% predicts survival of passenger, which is 36.36% (close to 38.38%). Thus, MLP is an improvement over Perceptron.\n\nObserving the results closely, it can seen that Logistic Regression and Multi-layer Perceptron captures non-survival adequately where Perceptron model do not.\n\n*Please note*: Using more models (different) can have better results through aggregation but is computationally expensive. ","945a7096":"## Get Data ","82c79f66":"# <font color='red'>Who survived the Titanic<\/font>\n\n---\n\n\nTitanic is well known to all. It's the ship that sank into Atlantic ocean travelling with more than a thousand passengers. \nIn this notebook, we study the data regarding persons who have survived the accident. \nBased on the study, we will try to predict who else have survived i.e. people we do not know have survived or not.\n\n---\n\n\n\n<font color='blue'>**Table of Contents** for the analysis is quite simple:<\/font>\n- Get Data and necessary Libraries\n- Perform Initial Assesment on Data\n- Feature Engineering \n  - Missing Value Imputation <font color='green'>**(Using Linear Regression)**<\/font>\n  - Grouping of Data\n  - One-Hot Encoding\n- Data Vizualization\n  - Univariate Analysis *using Column Chart*\n  - Bivariate and Trivariate Analysis <font color='green'>**(using Pivot Table)**<\/font>\n- Modeling\n  - Prepare dataset (train-test split)\n  - Build Basic Classification Models <font color='green'>(Logistics Regression, Perceptron, Multilayer Perceptron)<\/font>\n- Predict Test Data\n  - **Ratio Analysis**"}}