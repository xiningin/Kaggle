{"cell_type":{"5b4cbfc5":"code","ccdad5b2":"code","c8218d18":"code","c023166a":"code","b12a5afd":"code","a535f56e":"code","4609eb59":"code","e6bb8d7f":"code","6e107af1":"code","b2a8be8c":"code","48ad0f4c":"code","31285390":"code","340ffd4b":"code","bd324bac":"code","9d9730d3":"code","9a107f5d":"code","022bbe52":"code","97581f1d":"code","8ced261f":"code","2ce30ccd":"code","00a767e6":"code","801c9d00":"code","da0644ef":"code","156262ec":"code","a964f186":"code","48b9e4b2":"code","50a0a417":"code","17c804e7":"code","13acfaa2":"code","b5932f3d":"code","c75218a7":"code","b3c82b17":"code","9fc4dc3d":"code","3a6fd62a":"code","48a31fd3":"code","69e6c84f":"code","35f9e123":"code","e80ac41c":"code","bf42b216":"code","837a1669":"code","14ad5b6d":"markdown","a1bcc229":"markdown","f36d6ff2":"markdown","e0f94bf5":"markdown","a9ed7dfc":"markdown","42080805":"markdown","50226503":"markdown","7c2b1da5":"markdown","5dd7504d":"markdown","7e270f15":"markdown"},"source":{"5b4cbfc5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ccdad5b2":"# importing datasets\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","c8218d18":"df = pd.read_csv(\"..\/input\/car-evaluation-data-set\/car_evaluation.csv\",header=0)\ndf.describe()","c023166a":"columns = [\"Buy_price\",\"mcost\",\"ndoors\",\"nop\",\"lug_boot\",\"safety\",\"decision\"]\ndf.columns = columns","b12a5afd":"df.head()\n\nX_list=[\"Buy_price\",\"mcost\",\"ndoors\",\"nop\",\"lug_boot\",\"safety\"]\n\nX=df[X_list]\ny=df.decision\ny.value_counts()\n","a535f56e":"df.shape","4609eb59":"X.Buy_price.unique()","e6bb8d7f":"X.mcost.unique()","6e107af1":"X.ndoors.unique()","b2a8be8c":"X.nop.unique()","48ad0f4c":"X.lug_boot.unique()","31285390":"X.safety.unique()","340ffd4b":"y.unique()","bd324bac":"def grp_brplt(col1):\n    \n    df1 = df.groupby(['decision',col1]).size().to_frame('total').reset_index()\n    \n    plt.figure(figsize=(10,8))\n    ax=plt.subplot()\n    ax = sns.barplot(data=df1, x=df1[col1], y=df1[\"total\"], hue=df1[\"decision\"])\n    \n    for p in ax.patches:\n        ax.annotate(format(p.get_height(), '.1f'), \n                       (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                       ha = 'center', va = 'center', \n                       xytext = (0, 9), \n                       textcoords = 'offset points')\n\n    ax.set_title('Distribution of ' +col1+ ' per target variable', fontsize=20)\n    ax.legend(loc='center right', bbox_to_anchor=(1.25, 0.5), ncol=1, title='decision')\n    return ax","9d9730d3":"grp_brplt(\"Buy_price\")","9a107f5d":"grp_brplt(\"mcost\")","022bbe52":"grp_brplt(\"ndoors\")","97581f1d":"grp_brplt(\"nop\")","8ced261f":"grp_brplt(\"lug_boot\")","2ce30ccd":"grp_brplt(\"safety\")","00a767e6":"# importing necessary package for encoding our categorial features\nimport category_encoders as ce\n\nencoder_X = ce.OrdinalEncoder(cols=[\"Buy_price\",\"mcost\",\"ndoors\",\"nop\",\"lug_boot\",\"safety\"])\nX= encoder_X.fit_transform(X)\n\nencoder_Y = ce.OrdinalEncoder()\ny=np.ravel(encoder_Y.fit_transform(y))","801c9d00":"from sklearn.model_selection import train_test_split,GridSearchCV\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)","da0644ef":"from sklearn import tree,svm,naive_bayes,neighbors,ensemble\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report","156262ec":"cl1 = naive_bayes.BernoulliNB()\ncl2 = naive_bayes.GaussianNB()","a964f186":"#decision tree\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [3, 6, 9, 12],\n    'max_features': [2, 3],\n    'min_samples_leaf': [2, 3, 4, 5]\n}\ndt_gs = GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param_grid)\ndt_gs.fit(X_train, y_train)\nprint(\"####### DECISION TREE #######\")\nprint(dt_gs.best_params_)\n\n#output\n####### DECISION TREE #######\n#{'criterion': 'entropy', 'max_depth': 12, 'max_features': 3, 'min_samples_leaf': 2}\n\n# random forest\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\nrf_gs = GridSearchCV(ensemble.RandomForestClassifier(), param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\nrf_gs.fit(X_train, y_train)\nprint(\" ####### RANDOM FOREST #######\")\nprint(rf_gs.best_params_)\n\n# output\n ####### RANDOM FOREST #######\n# {'bootstrap': True, 'max_depth': 80, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 100}\n\n# support vector machines\n\nparam_grid2 = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf','linear'],\n              'decision_function_shape':['ovo','ovr']}\nsvc_gs = GridSearchCV(svm.SVC(), param_grid2, refit = True, verbose = 3)\nsvc_gs.fit(X_train, y_train)\nprint(\" ####### SVM #######\")\nprint(svc_gs.best_params_)\n\n#output\n####### SVM #######\n#{'C': 100, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'kernel': 'rbf'}\n\n# knn\n\nk_range = list(range(1, 31))\nprint(k_range)\nparam_grid = dict(n_neighbors=k_range)\nprint(param_grid)\nknn_gs = GridSearchCV(estimator=neighbors.KNeighborsClassifier(),param_grid=param_grid,cv=5,return_train_score=True) # Turn on cv train scores\nknn_gs.fit(X_train, y_train)\nprint(\"####### KNN ######\")\nprint(knn_gs.best_params_)\n\n#output\n# 6 neighbours \n","48b9e4b2":"import pandas as pd\nrow_num = 0\ncompiler_compare = pd.DataFrame()","50a0a417":"y_pred = dt_gs.predict(X_train)\ncas = accuracy_score(y_train,y_pred)\nprint(\"Accuracy score for training data in decision tree is : {}\".format(cas))\nprint(\"Confusion matrix for decision tree is : \")\nprint(confusion_matrix(y_pred,y_train))\ncompiler_compare.loc[row_num, 'Name'] = 'Decision Tree GS'\ncompiler_compare.loc[row_num, 'Train Accuracy Score'] = cas*100  ","17c804e7":"y_pred = dt_gs.predict(X_test)\ncas = accuracy_score(y_test,y_pred)\nprint(\"Accuracy score for training data in decision tree is : {}\".format(cas))\nprint(\"Confusion matrix for decision tree is : \")\nprint(confusion_matrix(y_pred,y_test))\nprint(\"Classification report for decision tree is : \")\nprint(classification_report(y_pred,y_test))\ncompiler_compare.loc[row_num, 'Test Accuracy Score'] = cas*100 \nrow_num+=1","13acfaa2":"y_pred = rf_gs.predict(X_train)\ncas = accuracy_score(y_train,y_pred)\nprint(\"Accuracy score for training data in Random Forest is : {}\".format(cas))\nprint(\"Confusion matrix for Random Forest is : \")\nprint(confusion_matrix(y_pred,y_train))\ncompiler_compare.loc[row_num, 'Name'] = 'Random Forest'\ncompiler_compare.loc[row_num, 'Train Accuracy Score'] = cas*100  ","b5932f3d":"y_pred = rf_gs.predict(X_test)\ncas = accuracy_score(y_test,y_pred)\nprint(\"Accuracy score for training data in Random Forest is : {}\".format(cas))\nprint(\"Confusion matrix for Random Forest is : \")\nprint(confusion_matrix(y_pred,y_test))\nprint(\"Classification report for Random Forest is : \")\nprint(classification_report(y_pred,y_test))\ncompiler_compare.loc[row_num, 'Test Accuracy Score'] = cas*100 \nrow_num+=1","c75218a7":"y_pred = svc_gs.predict(X_train)\ncas = accuracy_score(y_train,y_pred)\nprint(\"Accuracy score for training data in SVM is : {}\".format(cas))\nprint(\"Confusion matrix for SVM is : \")\nprint(confusion_matrix(y_pred,y_train))\ncompiler_compare.loc[row_num, 'Name'] = 'SVM'\ncompiler_compare.loc[row_num, 'Train Accuracy Score'] = cas*100  ","b3c82b17":"y_pred = svc_gs.predict(X_test)\ncas = accuracy_score(y_test,y_pred)\nprint(\"Accuracy score for training data in SVM is : {}\".format(cas))\nprint(\"Confusion matrix for SVM is : \")\nprint(confusion_matrix(y_pred,y_test))\nprint(\"Classification report for SVM is : \")\nprint(classification_report(y_pred,y_test))\ncompiler_compare.loc[row_num, 'Test Accuracy Score'] = cas*100 \nrow_num+=1","9fc4dc3d":"y_pred = knn_gs.predict(X_train)\ncas = accuracy_score(y_train,y_pred)\nprint(\"Accuracy score for training data in KNN is : {}\".format(cas))\nprint(\"Confusion matrix for KNN is : \")\nprint(confusion_matrix(y_pred,y_train))\ncompiler_compare.loc[row_num, 'Name'] = 'KNN'\ncompiler_compare.loc[row_num, 'Train Accuracy Score'] = cas*100  ","3a6fd62a":"y_pred = knn_gs.predict(X_test)\ncas = accuracy_score(y_test,y_pred)\nprint(\"Accuracy score for training data in KNN is : {}\".format(cas))\nprint(\"Confusion matrix for KNN is : \")\nprint(confusion_matrix(y_pred,y_test))\nprint(\"Classification report for KNN is : \")\nprint(classification_report(y_pred,y_test))\ncompiler_compare.loc[row_num, 'Test Accuracy Score'] = cas*100 \nrow_num+=1","48a31fd3":"cl1.fit(X_train,y_train)\ny_pred = cl1.predict(X_train)\ncas = accuracy_score(y_train,y_pred)\nprint(\"Accuracy score for training data in Bernoulli NB is : {}\".format(cas))\nprint(\"Confusion matrix for Bernoulli NB is : \")\nprint(confusion_matrix(y_pred,y_train))\ncompiler_compare.loc[row_num, 'Name'] = 'BernoulliNB'\ncompiler_compare.loc[row_num, 'Train Accuracy Score'] = cas*100","69e6c84f":"y_pred = cl1.predict(X_test)\ncas = accuracy_score(y_test,y_pred)\nprint(\"Accuracy score for training data in Bernoulli NB is : {}\".format(cas))\nprint(\"Confusion matrix for Bernoulli NB is : \")\nprint(confusion_matrix(y_pred,y_test))\nprint(\"Classification report for Bernoulli NB is : \")\nprint(classification_report(y_pred,y_test))\ncompiler_compare.loc[row_num, 'Test Accuracy Score'] = cas*100 \nrow_num+=1","35f9e123":"cl2.fit(X_train,y_train)\ny_pred = cl2.predict(X_train)\ncas = accuracy_score(y_train,y_pred)\nprint(\"Accuracy score for training data in Gaussian NB is : {}\".format(cas))\nprint(\"Confusion matrix for Gaussian NB is : \")\nprint(confusion_matrix(y_pred,y_train))\ncompiler_compare.loc[row_num, 'Name'] = 'GaussianNB'\ncompiler_compare.loc[row_num, 'Train Accuracy Score'] = cas*100 ","e80ac41c":"y_pred = cl2.predict(X_test)\ncas = accuracy_score(y_test,y_pred)\nprint(\"Accuracy score for training data in Gaussian NB is : {}\".format(cas))\nprint(\"Confusion matrix for Gaussian NB is : \")\nprint(confusion_matrix(y_pred,y_test))\nprint(\"Classification report for Bernoulli NB is : \")\nprint(classification_report(y_pred,y_test))\ncompiler_compare.loc[row_num, 'Test Accuracy Score'] = cas*100  \nrow_num+=1","bf42b216":"compiler_compare","837a1669":"import seaborn as sns\nsns.set_style(\"dark\")\nax = compiler_compare.plot.barh(x='Name', rot=0, figsize=(10, 10))\nplt.yticks(size =16)\nplt.legend(loc='lower left')\nplt.ylabel(\"Compiler\" , size =20)\nplt.title(\"Compiler Accuracy Comparision\", size = 20)\nfor index, value in enumerate(compiler_compare['Test Accuracy Score']):\n    plt.text(value+10, index+.12, str(value),size=16)\nfor index, value in enumerate(compiler_compare['Train Accuracy Score']):\n    plt.text(value+10, index-.17, str(value),size=16)","14ad5b6d":"## Check column values","a1bcc229":"## Decision Tree","f36d6ff2":"##### Therefore we know that we cannot use bernoulli when target with multiple classifable classes.","e0f94bf5":"## BernoulliNB","a9ed7dfc":"## GaussianNB","42080805":"## Random Forest","50226503":"## SVM","7c2b1da5":"# Evaluation of Car dataset with Various Classifiers and Grid Search to Maximize Accuracy","5dd7504d":"## Accuracy Table","7e270f15":"## KNN"}}