{"cell_type":{"e8d14dca":"code","bbbe2117":"code","8f68b098":"code","3593c2c8":"code","1962f897":"code","c99ad18c":"code","01eb4864":"code","ab01f3c6":"code","5a267af5":"code","5975881a":"code","3ad068f4":"code","d8ffef3f":"code","22c6a81a":"code","e194469d":"code","f471190f":"code","d895ca61":"code","e705f65b":"code","6a2e1b56":"code","38da5130":"code","b2e4e428":"code","1dd41486":"code","75f1fa13":"code","7b0252ac":"code","c486f475":"code","94a6d486":"code","7ca844fd":"code","11196207":"code","f4665ff5":"code","5a0b8f6e":"code","345487e6":"code","77646b9e":"code","9496ba91":"code","5ae71fbc":"markdown"},"source":{"e8d14dca":"pip install -q -U tensorflow-addons","bbbe2117":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow.keras as keras\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom IPython.display import display\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import classification_report, confusion_matrix\nplt.rcParams['font.size'] = 12\nplt.rcParams['lines.color'] ='#000000'\nnp.random.seed(42)","8f68b098":"train_path = '..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv'\ntest_path = '..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv'","3593c2c8":"# import train and test data\ntrain = pd.read_csv(train_path)\ndisplay(train.head())\ntest = pd.read_csv(test_path)\ndisplay(test.head())","1962f897":"print('shape of training data:', train.shape) # (nrow, ncolumn)\nprint('shape of testing data:', test.shape)\n\n''' na count over all rows and columns\nfirst sum = sums over rows, last sum = sums over columns\n'''\nprint('na count train:', train.isna().sum().sum())\nprint('na count test:', test.isna().sum().sum())","c99ad18c":"# plot label distribution\nfig = plt.figure(figsize=(10,5))\nax = sns.countplot(x=train['label'], palette='Accent')\nplt.title('Target distribution on train set')\nplt.show()\n\nfig = plt.figure(figsize=(10,5))\nax = sns.countplot(x=test['label'], palette='Accent')\nplt.title('Target distribution on test set')\nplt.show()","01eb4864":"X = train.drop('label', axis=1) # get feature columns\ny = keras.utils.to_categorical(train['label']) # encode target\n\nX_test = test.drop('label', axis=1)\ny_test = keras.utils.to_categorical(test['label'])","ab01f3c6":"# reshape square black and white images\ndef reshape_bw(X):\n    samples, width, channel = X.shape[0], int(np.sqrt(X.shape[1])) , 1\n    return X.values.reshape(samples, width, width, channel)","5a267af5":"X = reshape_bw(X)\nX_test = reshape_bw(X_test)\nwidth, height, channel = X.shape[1], X.shape[2], X.shape[3]","5975881a":"# plot a square black and white image\ndef plot_image(X, width):\n    _ = plt.imshow(X.reshape(width, width), cmap='gray')\n    plt.show()\n\n# plot sample image\nprint('label:', y[0].argmax())    \nplot_image(X[0], width)\nprint('label:', y_test[0].argmax())    \nplot_image(X_test[0], width)","3ad068f4":"# split data into train and validation set with same target distribution as before spliting\nX_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)","d8ffef3f":"# create data augmentor and fit it on train set\n# feature = pixel\ngenerator = keras.preprocessing.image.ImageDataGenerator(\n    featurewise_center=True, # make pixel intensity mean = 0\n    featurewise_std_normalization=True, # make pixel intensity s.d. = 1\n    height_shift_range=0.2, # shift image vertically (up or down) by 0.2 * 28 pixel\n    width_shift_range=0.2, # shift image horizontally (left or right) by 0.2 * 28 pixel\n    fill_mode='nearest'\n)\ngenerator.fit(X_train)","22c6a81a":"def plot_learning_curve(history):\n    ephs = np.arange(0, len(history.history['loss'])) + 1\n    fig = plt.figure(figsize=(12, 5))\n    plt.subplot(1,2,1)\n    plt.plot(ephs, history.history['accuracy'], '--')\n    plt.plot(ephs, history.history['val_accuracy'], '--')\n    plt.legend(['accuracy', 'val_accuracy'])\n    plt.subplot(1,2,2)\n    plt.plot(ephs, history.history['loss'], '--')\n    plt.plot(ephs, history.history['val_loss'], '--')\n    plt.legend(['loss', 'val_loss'])\n    plt.show()","e194469d":"def evaluate(model):\n    loss_t, acc_t, pre_t, rec_t, f1_t = model.evaluate(generator.flow(X_train, y_train, batch_size=32))\n    loss, acc, pre, rec, f1 = model.evaluate(generator.flow(X_test, y_test, batch_size=32))\n    print()\n    print(f\"train loss: {loss_t}\")\n    print(f\"train accuracy: {round(acc_t*100,2)} %\")\n    print(f\"train precision: {pre_t}\")\n    print(f\"train recall: {rec_t}\")\n    print(f\"train f1: {f1_t}\")\n    print(f\"test loss: {loss}\")\n    print(f\"test accuracy: {round(acc*100,2)} %\")\n    print(f\"test precision: {pre}\")\n    print(f\"test recall: {rec}\")\n    print(f\"test f1: {f1}\")\n    predictions = np.argmax(model.predict(X_test), axis=-1)\n    classes = [str(i) for i in range(y_test.shape[1]) if i!= 9]\n    print(classification_report(np.argmax(y_test, axis=-1), predictions, target_names=classes))","f471190f":"def get_callbacks(file_path):\n    return [\n          keras.callbacks.ReduceLROnPlateau(monitor='val_f1_score', mode='max', factor=0.3, patience=2, min_lr=0.00001),\n          keras.callbacks.ModelCheckpoint(file_path, monitor='val_f1_score', mode='max', save_best_only=True),\n          keras.callbacks.EarlyStopping(monitor='val_f1_score', mode='max', restore_best_weights=True, min_delta=1e-8, patience=3)\n  ]","d895ca61":"keras.backend.clear_session()\nbaseline_model = keras.models.Sequential()\nbaseline_model.add(keras.layers.Input(shape=X.shape[1:]))\nbaseline_model.add(keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\nbaseline_model.add(keras.layers.MaxPool2D((2, 2)))\nbaseline_model.add(keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\nbaseline_model.add(keras.layers.MaxPool2D((2, 2)))\nbaseline_model.add(keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\nbaseline_model.add(keras.layers.MaxPool2D((2, 2)))\nbaseline_model.add(keras.layers.Flatten())\nbaseline_model.add(keras.layers.Dense(y.shape[1], activation='softmax'))\n\n# compile model\nbaseline_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.CategoricalCrossentropy(),\n    metrics=[\n        'accuracy', \n        keras.metrics.Precision(), \n        keras.metrics.Recall(), \n        tfa.metrics.F1Score(num_classes=y.shape[1], average='weighted')\n    ]\n)","e705f65b":"baseline_path = 'baseline.h5'\nhistory_baseline = baseline_model.fit(\n    generator.flow(X_train, y_train, batch_size=32),\n    epochs=20,\n    validation_data=generator.flow(X_val, y_val, batch_size=32),\n    callbacks=get_callbacks(baseline_path),\n    verbose=2\n)","6a2e1b56":"plot_learning_curve(history_baseline)","38da5130":"best_baseline = keras.models.load_model(baseline_path)\nevaluate(best_baseline)","b2e4e428":"keras.backend.clear_session()\ndrop_model = keras.models.Sequential()\ndrop_model.add(keras.layers.Input(shape=X.shape[1:]))\ndrop_model.add(keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\ndrop_model.add(keras.layers.MaxPool2D((2, 2)))\ndrop_model.add(keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\ndrop_model.add(keras.layers.Dropout(0.1))\ndrop_model.add(keras.layers.MaxPool2D((2, 2)))\ndrop_model.add(keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\ndrop_model.add(keras.layers.Dropout(0.2))\ndrop_model.add(keras.layers.MaxPool2D((2, 2)))\ndrop_model.add(keras.layers.Flatten())\ndrop_model.add(keras.layers.Dense(128, activation='relu'))\ndrop_model.add(keras.layers.Dropout(0.2))\ndrop_model.add(keras.layers.Dense(y.shape[1], activation='softmax'))\n\n# compile model\ndrop_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.CategoricalCrossentropy(),\n    metrics=[\n        'accuracy', \n        keras.metrics.Precision(), \n        keras.metrics.Recall(), \n        tfa.metrics.F1Score(num_classes=y.shape[1], average='weighted')\n    ]\n)","1dd41486":"dropout_path = 'dropout.h5'\nhistory_dropout = drop_model.fit(\n    generator.flow(X_train, y_train, batch_size=32),\n    epochs=20,\n    validation_data=generator.flow(X_val, y_val, batch_size=32),\n    callbacks=get_callbacks(dropout_path),\n    verbose=2\n)","75f1fa13":"plot_learning_curve(history_dropout)","7b0252ac":"best_dropout = keras.models.load_model(dropout_path)\nevaluate(best_dropout)","c486f475":"keras.backend.clear_session()\ndrop_model2 = keras.models.Sequential()\ndrop_model2.add(keras.layers.Input(shape=X.shape[1:]))\ndrop_model2.add(keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\ndrop_model2.add(keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\ndrop_model2.add(keras.layers.Dropout(0.1))\ndrop_model2.add(keras.layers.MaxPool2D((2, 2)))\ndrop_model2.add(keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\ndrop_model2.add(keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\ndrop_model2.add(keras.layers.Dropout(0.2))\ndrop_model2.add(keras.layers.MaxPool2D((2, 2)))\ndrop_model2.add(keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\ndrop_model2.add(keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\ndrop_model2.add(keras.layers.Dropout(0.3))\ndrop_model2.add(keras.layers.MaxPool2D((2, 2)))\ndrop_model2.add(keras.layers.Flatten())\ndrop_model2.add(keras.layers.Dense(128, activation='relu'))\ndrop_model2.add(keras.layers.Dropout(0.2))\ndrop_model2.add(keras.layers.Dense(y.shape[1], activation='softmax'))\n\n# compile model\ndrop_model2.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.CategoricalCrossentropy(),\n    metrics=[\n        'accuracy', \n        keras.metrics.Precision(), \n        keras.metrics.Recall(), \n        tfa.metrics.F1Score(num_classes=y.shape[1], average='weighted')\n    ]\n)","94a6d486":"dropout_path2 = 'dropout2.h5'\nhistory_dropout2 = drop_model2.fit(\n    generator.flow(X_train, y_train, batch_size=32),\n    epochs=20,\n    validation_data=generator.flow(X_val, y_val, batch_size=32),\n    callbacks=get_callbacks(dropout_path2),\n    verbose=2\n)","7ca844fd":"plot_learning_curve(history_dropout2)","11196207":"best_dropout2 = keras.models.load_model(dropout_path2)\nevaluate(best_dropout2)","f4665ff5":"keras.backend.clear_session()\ndrop_model3 = keras.models.Sequential()\ndrop_model3.add(keras.layers.Input(shape=X.shape[1:]))\ndrop_model3.add(keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu'))\ndrop_model3.add(keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu'))\ndrop_model3.add(keras.layers.Dropout(0.1))\ndrop_model3.add(keras.layers.MaxPool2D((2, 2)))\ndrop_model3.add(keras.layers.Conv2D(64, (5, 5), padding='same', activation='relu'))\ndrop_model3.add(keras.layers.Conv2D(64, (5, 5), padding='same', activation='relu'))\ndrop_model3.add(keras.layers.Dropout(0.2))\ndrop_model3.add(keras.layers.MaxPool2D((2, 2)))\ndrop_model3.add(keras.layers.Conv2D(128, (5, 5), padding='same', activation='relu'))\ndrop_model3.add(keras.layers.Conv2D(128, (5, 5), padding='same', activation='relu'))\ndrop_model3.add(keras.layers.Dropout(0.3))\ndrop_model3.add(keras.layers.MaxPool2D((2, 2)))\ndrop_model3.add(keras.layers.Flatten())\ndrop_model3.add(keras.layers.Dense(128, activation='relu'))\ndrop_model3.add(keras.layers.Dropout(0.2))\ndrop_model3.add(keras.layers.Dense(y.shape[1], activation='softmax'))\n\n# compile model\ndrop_model3.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.CategoricalCrossentropy(),\n    metrics=[\n        'accuracy', \n        keras.metrics.Precision(), \n        keras.metrics.Recall(), \n        tfa.metrics.F1Score(num_classes=y.shape[1], average='weighted')\n    ]\n)","5a0b8f6e":"dropout_path3 = 'dropout3.h5'\nhistory_dropout3 = drop_model3.fit(\n    generator.flow(X_train, y_train, batch_size=32),\n    epochs=20,\n    validation_data=generator.flow(X_val, y_val, batch_size=32),\n    callbacks=get_callbacks(dropout_path3),\n    verbose=2\n)","345487e6":"plot_learning_curve(history_dropout3)","77646b9e":"best_dropout3 = keras.models.load_model(dropout_path3)\nevaluate(best_dropout3)","9496ba91":"# cr. https:\/\/github.com\/bnsreenu\/python_for_microscopists\/blob\/master\/152-visualizing_conv_layer_outputs.py\nlayer = best_dropout3.layers\nidx = 0\nfilters, biases = best_dropout3.layers[idx].get_weights()  \n\nmain_fig = plt.figure(figsize=(12, 8))\ncolumns = 8\nrows = 4\nn_filters = columns * rows\nfor i in range(1, n_filters +1):\n    f = filters[:, :, :, i-1]\n    fig = plt.subplot(rows, columns, i)\n    fig.set_xticks([]) \n    fig.set_yticks([])\n    plt.imshow(f[:, :, 0], cmap='gray')\nplt.show()    \n\nconv_layer_index = [0, 1, 4, 5, 8, 9]\nconv_layer_index = [0]\noutputs = [best_dropout3.layers[i].output for i in conv_layer_index]\nmodel_short = keras.models.Model(inputs=best_dropout3.inputs, outputs=[outputs])\n\nfeature_output = model_short.predict(X_test[0:1])\n\n\ncolumns = 8\nrows = 4\nfor ftr in feature_output:\n    fig=plt.figure(figsize=(12, 8))\n    for i in range(1, columns*rows +1):\n        fig =plt.subplot(rows, columns, i)\n        fig.set_xticks([])\n        fig.set_yticks([])\n        plt.imshow(ftr[0, :, :, i-1], cmap='gray')\n    plt.show()","5ae71fbc":"we could notice that there are no example in both train set and test set with label 9"}}