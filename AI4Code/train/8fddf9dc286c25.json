{"cell_type":{"89a9eb42":"code","c1a8ef3e":"code","55623087":"code","9fb2a433":"code","57c20374":"code","8d97409e":"code","a128c3de":"code","0a6d4457":"code","41af5121":"code","a8910d69":"code","cf7bc114":"code","757cf561":"code","5b2e6e32":"code","6635f554":"code","b8b264d9":"code","4a6c3336":"code","1813a948":"code","a1a1e161":"code","89882abf":"code","0f07829c":"code","228f33a5":"code","d9c09dc0":"code","da54d14a":"code","9ccbfe72":"code","55debe6c":"code","ce3e7201":"code","b9c21bdb":"code","a9cbc9c6":"code","6d23dfaa":"code","00ebc83d":"code","3d34db6b":"code","fdf50345":"code","fb873b93":"code","d99270ed":"code","0717f88d":"code","de1925e2":"code","bb8b1c24":"code","9fc11d47":"code","6ea0b4cc":"code","533e216e":"code","61d2f4bd":"code","bc783702":"code","cbf3e969":"code","81fc714a":"code","cb23e051":"code","33b5ae49":"code","44d8fa84":"code","6e4795f7":"code","0b972213":"code","e573ab1a":"code","4040b47e":"code","a01923a6":"code","7a77259c":"code","8293bd60":"code","b3943ea1":"code","784c672a":"code","4071da24":"code","bad67762":"code","90ea3f46":"code","2154eaf6":"code","21813425":"code","ec78a9fa":"code","25c79d5b":"code","1865b198":"code","311bcbfb":"code","b61bb544":"code","a48f47d3":"code","b5a66cfe":"code","4f0658cf":"code","23ec5b9d":"code","b22997bc":"code","8b9c7e2d":"code","36be487f":"markdown","8262ce3e":"markdown","0df77c05":"markdown","c02d5b95":"markdown","e2396f4a":"markdown","c5108622":"markdown","cd4534ec":"markdown","7a694a13":"markdown","251afe7b":"markdown","8069a368":"markdown","0ca249c9":"markdown","6574c03c":"markdown","18dcc80c":"markdown"},"source":{"89a9eb42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n#Import libraries\n\n#Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nimport seaborn as sns \nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom datetime import datetime\nimport statsmodels.formula.api as sm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1a8ef3e":"train_data=pd.read_csv(r'..\/input\/bike-sharing-demand\/train.csv')\ntest_data=pd.read_csv(r'..\/input\/bike-sharing-demand\/test.csv')\ndf=train_data.copy()\ntest_df=test_data.copy()\ndf.head()","55623087":"#Describe dataset\ntrain_data.describe() ","9fb2a433":"#check Null values\ntrain_data.isnull().values.any()","57c20374":"# Data Exploration\nsns.barplot(x='season', y='count', data=train_data)","8d97409e":"sns.barplot(x='weather', y='count', data=train_data)","a128c3de":"train_data[['count', 'holiday']].groupby(['holiday'], as_index = True).mean().sort_values(by = 'count')","0a6d4457":"train_data[['count', 'season']].groupby(['season'], as_index = True).mean().sort_values(by = 'count')","41af5121":"#we have a datetime object here, so it's better to break them into hour, day, month, year and make them a separate column.\ntrain_data[\"hour\"] = [t.hour for t in pd.DatetimeIndex(train_data.datetime)]\ntrain_data[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(train_data.datetime)]\ntrain_data[\"month\"] = [t.month for t in pd.DatetimeIndex(train_data.datetime)]\ntrain_data['year'] = [t.year for t in pd.DatetimeIndex(train_data.datetime)]","a8910d69":"#Box plot\nfig, axes = plt.subplots(nrows=3,ncols=2)\nfig.set_size_inches(15, 15)\nsns.boxplot(data=train_data,y=\"count\",orient=\"v\",ax=axes[0][0])\nsns.boxplot(data=train_data,y=\"count\",x=\"month\",orient=\"v\",ax=axes[0][1])\nsns.boxplot(data=train_data,y=\"count\",x=\"weather\",orient=\"v\",ax=axes[1][0])\nsns.boxplot(data=train_data,y=\"count\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\nsns.boxplot(data=train_data,y=\"count\",x=\"hour\",orient=\"v\",ax=axes[2][0])\nsns.boxplot(data=train_data,y=\"count\",x=\"temp\",orient=\"v\",ax=axes[2][1])\n\naxes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\naxes[0][1].set(xlabel='Month', ylabel='Count',title=\"Box Plot On Count Across Months\")\naxes[1][0].set(xlabel='Weather Situation', ylabel='Count',title=\"Box Plot On Count Across Weather Situations\")\naxes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Box Plot On Count Across Working Day\")\naxes[2][0].set(xlabel='Hour Of The Day', ylabel='Count',title=\"Box Plot On Count Across Hour Of The Day\")\naxes[2][1].set(xlabel='Temperature', ylabel='Count',title=\"Box Plot On Count Across Temperature\")","cf7bc114":"# Dropping datetime column becuase we already break them and created new columns\ntrain_data.drop('datetime',axis=1,inplace=True) \n ","757cf561":"# correlation by pairplot\nsns.pairplot(train_data)","5b2e6e32":"# Dropping holiday column as it is highly correlated to\u2018workingday\u2019 column\ntrain_data.drop('holiday',axis=1,inplace=True) \n ","6635f554":"# Dropping atemp column as it is highly correlated to \u2018temp\u2019 column\ntrain_data.drop('atemp',axis=1) ","b8b264d9":"# there are just 2 different years 2011,2012 so using map(), I converted 2011 and 2012 to 0 and 1 respectively.\ntrain_data['year'] = train_data['year'].map({2011:0, 2012:1})\n","4a6c3336":"# finding the correlation between the columns 'casual','registred','count'\n\nplt.scatter(x = train_data['casual'] + train_data['registered'], y = train_data['count'])\nplt.show()","1813a948":"# Dropping the column registred and casual\ntrain_data = train_data.drop(['registered', 'casual'],axis=1)","a1a1e161":"X, y = train_data.iloc[:, :], train_data['count']","89882abf":"X = X.drop('count',axis=1)","0f07829c":"#Normalize the train set\n#def norm_func(i):\n    #x = (i-i.min())\t\/ (i.max()-i.min())\n    #return (x)\nfrom sklearn.preprocessing import StandardScaler\nscl= StandardScaler()","228f33a5":"#X = scl.fit_transform(X)\n#y = scl.fit_transform(y)","d9c09dc0":"from sklearn.model_selection import  train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","da54d14a":"\nX_train = scl.fit_transform(X_train)\nX_test = scl.transform(X_test)","9ccbfe72":"print(X_train.shape)\nprint(X_test.shape)","55debe6c":"print(y_train.shape)\nprint(y_test.shape)","ce3e7201":"from sklearn.linear_model import Ridge\nreg5 = Ridge(alpha=0.05, normalize=True)\nreg5.fit(X_train,y_train)\nreg5.score(X_train,y_train)","b9c21bdb":"Ridge = reg5.predict(X_test)\nRidge","a9cbc9c6":"print(reg5.intercept_)\nprint(reg5.coef_)","6d23dfaa":"sns.regplot(y_test,Ridge)\nplt.title('Residual Analysis - Ridge_Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","00ebc83d":"from sklearn import metrics\nprint(\"MAE:\", metrics.mean_absolute_error(y_test,Ridge))\nprint('MSE:', metrics.mean_squared_error(y_test, Ridge))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, Ridge)))","3d34db6b":"# grid search hyperparameters for ridge regression\nfrom numpy import arange\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Ridge\n","fdf50345":"# define model\nmodel = Ridge()\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_squared_error', cv=cv)\n# perform the search\nresults = search.fit(X_train, y_train)\n# summarize\nprint('MSE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)","fb873b93":"from sklearn.linear_model import Lasso\nreg6 = Lasso(alpha=0.3, normalize=True)\nreg6.fit(X_train,y_train)\nreg6.score(X_train,y_train)","d99270ed":"Lasso = reg6.predict(X_test)\nLasso","0717f88d":"print(reg6.intercept_)\nprint(reg6.coef_)","de1925e2":"sns.regplot(y_test,Lasso)\nplt.title('Residual Analysis - Lasso Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","bb8b1c24":"print(\"MAE:\", metrics.mean_absolute_error(y_test,Lasso))\nprint('MSE:', metrics.mean_squared_error(y_test, Lasso))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, Lasso)))","9fc11d47":"# 10 fold CV\nfrom sklearn.linear_model import LassoCV\n## define model evaluation method\ncv = RepeatedKFold(n_splits=10, random_state=1)\n# define model\nmodel = LassoCV(alphas=arange(0, 1, 0.01), cv=cv)\n# fit model\nmodel.fit(X_train, y_train)\n# summarize chosen configuration\nprint('alpha: %f' % model.alpha_)","6ea0b4cc":"from sklearn.tree import DecisionTreeRegressor\nreg4 = DecisionTreeRegressor()\nreg4.fit(X_train,y_train)\nreg4.score(X_train,y_train)","533e216e":"Dec_Tree = reg4.predict(X_test)\nDec_Tree","61d2f4bd":"sns.regplot(y_test,Dec_Tree)\nplt.title('Residual Analysis - Decision Tree Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","bc783702":"print(\"MAE:\", metrics.mean_absolute_error(y_test,Dec_Tree ))\nprint('MSE:', metrics.mean_squared_error(y_test, Dec_Tree))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, Dec_Tree)))","cbf3e969":"# 10 Fold\nparameters = {'max_depth':range(3,20)}\nclf = GridSearchCV(reg4, parameters,scoring='neg_mean_squared_error', cv=10)\nclf.fit(X=X_train, y=y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)","81fc714a":"# Pruning the Tree\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Minimum observations at the internal node approach\nregtree2 = DecisionTreeRegressor(min_samples_split = 3)\nregtree2.fit(X_train, y_train)","cb23e051":"# Prediction\ntest_pred2 = regtree2.predict(X_test)\ntrain_pred2 = regtree2.predict(X_train)","33b5ae49":"# Error on test dataset\nmean_squared_error(y_test, test_pred2)\nr2_score(y_test, test_pred2)","44d8fa84":"# Error on train dataset\nmean_squared_error(y_train, train_pred2)\nr2_score(y_train, train_pred2)","6e4795f7":"## Minimum observations at the leaf node approach\nregtree3 = DecisionTreeRegressor(min_samples_leaf = 3)\nregtree3.fit(X_train, y_train)\n\n# Prediction\ntest_pred3 = regtree3.predict(X_test)\ntrain_pred3 = regtree3.predict(X_train)\n\n# measure of error on test dataset\nmean_squared_error(y_test, test_pred3)\nr2_score(y_test, test_pred3)\n\n# measure of error on train dataset\nmean_squared_error(y_train, train_pred3)\nr2_score(y_train, train_pred3)","0b972213":"# 10 Fold DT pruning with leaf node apporoach\nparameters = {'max_depth':range(3,20)}\nclf = GridSearchCV(regtree3, parameters,scoring='neg_mean_squared_error' ,cv=10)\nclf.fit(X=X_train, y=y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)","e573ab1a":"# 10 Fold DT pruning internal node approach\nparameters = {'max_depth':range(3,20)}\nclf = GridSearchCV(regtree2, parameters,scoring='neg_mean_squared_error', cv=10)\nclf.fit(X=X_train, y=y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)","4040b47e":"\nfrom sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor(n_estimators = 400, criterion='mse',random_state=1, n_jobs=-1)\nforest.fit(X_train, y_train)\ny_train_pred = forest.predict(X_train)\ny_test_pred = forest.predict(X_test)\n","a01923a6":"from sklearn.metrics import mean_squared_error, r2_score\n#Root_Mean_Square_Log_Error(RMSE) is accuracy criteria for this problem\nprint('RMSLE train: %.3f' % np.sqrt(mean_squared_error(np.log(y_train + 1), np.log(y_train_pred + 1))))\nprint('RMSLE test: %.3f' % np.sqrt(mean_squared_error(np.log(y_test + 1), np.log(y_test_pred + 1))))\nprint('R2 train: %.3f' % r2_score(y_train, y_train_pred))\nprint('R2 test: %.3f' % r2_score(y_test, y_test_pred))","7a77259c":"#model = RandomForestClassifier()\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std\n# evaluate the model\nmodel = RandomForestRegressor()\n# evaluate the model\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(forest, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\nprint('MSE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","8293bd60":"sns.regplot(y_test,y_test_pred)\nplt.title('Residual Analysis - Random Forest Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","b3943ea1":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import GradientBoostingRegressor","784c672a":"model1 = GradientBoostingRegressor()","4071da24":"# fit the model on the whole dataset\nmodel1.fit(X_train, y_train)","bad67762":"# define the evaluation procedure\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate the model\nn_scores = cross_val_score(model1, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# report performance\nprint('MSE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","90ea3f46":"test_data.head()","2154eaf6":"test_data.isnull().values.any()  # checking missing entries","21813425":"test_data[\"hour\"] = [t.hour for t in pd.DatetimeIndex(test_data.datetime)]\ntest_data[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(test_data.datetime)]\ntest_data[\"month\"] = [t.month for t in pd.DatetimeIndex(test_data.datetime)]\ntest_data['year'] = [t.year for t in pd.DatetimeIndex(test_data.datetime)]\ntest_data['year'] = test_data['year'].map({2011:0, 2012:1})","ec78a9fa":"test_data =test_data.drop('atemp',axis=1) \n","25c79d5b":"X_test=test_data.iloc[:,1:]","1865b198":"X_test = scl.transform(X_test)","311bcbfb":"y_test=forest.predict(X_test) # Random Forest ","b61bb544":"y_test","a48f47d3":"y_test = pd.DataFrame(y_test)","b5a66cfe":"df_final = test_data\n","4f0658cf":"df_final['count'] = np.round(y_test)","23ec5b9d":"df_final = df_final.drop(['season', 'workingday','weather', 'holiday',\n                            'temp', 'humidity', 'windspeed', 'hour', 'day', 'month', 'year'], axis=1)","b22997bc":"df_final.head()","8b9c7e2d":"df_final.to_csv('submission.csv', index=False)\n","36be487f":"# # E. Regression Tree","8262ce3e":"# # B. Data Preprocessing","0df77c05":"similarly converting datetime to hour , month and year ","c02d5b95":"# # G. Random Forest","e2396f4a":"# #B. Applying Machine Learning Models","c5108622":"# # H. Gradient Boosting","cd4534ec":"# **Read Training and Test Data**","7a694a13":"# F.Decision Tree with Pruning","251afe7b":"# # I. Optimal Model","8069a368":"# # C. Ridge Regression","0ca249c9":"# # D. Lasso Regression\n","6574c03c":"The best result given by Random Forest across A-H.","18dcc80c":"*Random Forest with 10 fold cross validation was giving the least error and high accuracy across all the models from A-H\n\nSimilar apporach within train and validation can be apply to the given test data*"}}