{"cell_type":{"3629baf2":"code","be3c1154":"code","6db96980":"code","2aeb2727":"code","d71acd04":"code","22335e69":"code","892ab391":"code","8de897c9":"code","085c588f":"code","fd54650a":"code","5fe78db6":"code","9b034fea":"code","6b79ff3c":"code","7bb07c2e":"code","29a0045d":"code","b0b0e40b":"code","e4c7b9d3":"code","15e2e809":"code","d1b3ed21":"code","3f483ece":"code","51815238":"code","e5466bf8":"code","3d10a481":"code","85d4d9d6":"code","d1372885":"code","207dad1e":"code","03f8c227":"code","1e2d951a":"code","c1abea10":"code","062b1a55":"code","2f358bb5":"code","b83b52df":"code","1008329a":"markdown","641091b7":"markdown","ee4fc8dc":"markdown","453157fc":"markdown","b82776e8":"markdown","50aca706":"markdown","a187e3e5":"markdown","adac3026":"markdown","5d4dbf50":"markdown","347a378e":"markdown","2af09838":"markdown","4f656a25":"markdown","86e8457b":"markdown","0f3e98a3":"markdown","03209ad4":"markdown","7d7279a1":"markdown","c87f1766":"markdown","becd66dc":"markdown"},"source":{"3629baf2":"import matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.style.use('dark_background')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","be3c1154":"dataset = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')","6db96980":"dataset.head()","2aeb2727":"dataset.shape","d71acd04":"dataset.describe().T.style.bar(\n    subset=['mean'],\n    color='#606ff2').background_gradient(\n    subset=['std'], cmap='PuBu').background_gradient(subset=['50%'], cmap='PuBu')","22335e69":"dataset.info()","892ab391":"dataset.isnull().values.any()","8de897c9":"corr_data = dataset.drop(['id','date','price'], axis = 1)\nplt.figure(figsize=(20, 17))\nmatrix = np.triu(corr_data.corr())\nsns.heatmap(corr_data.corr(), annot=True,\n            linewidth=.8, mask=matrix, cmap=\"rocket\");","085c588f":"plot_data = dataset.drop(['id', 'date','price'], axis=1)","fd54650a":"fig = plt.figure(figsize=(20, 20))\nfor i in tqdm_notebook(range(len(plot_data.columns)), desc = 'Generating Frequency Plots'):\n    fig.add_subplot(np.ceil(len(plot_data.columns)\/5), 5, i+1)\n    plot_data.iloc[:, i].hist(bins=20)\n    plt.title(plot_data.columns[i])\n    fig.tight_layout(pad=3.0)\nplt.show();","5fe78db6":"plot_data = dataset.drop(['id', 'date'], axis=1)","9b034fea":"def distributionPlot(dataset):\n    \"\"\"\n    This function will create distribution plot for the dataset provided.\n    \"\"\"\n    fig = plt.figure(figsize=(20, 20))\n    for i in tqdm_notebook(range(len(dataset.columns)), desc = 'Generating Distribution Plots'):\n        fig.add_subplot(np.ceil(len(dataset.columns)\/5), 5, i+1)\n        sns.distplot(\n            dataset.iloc[:, i], color=\"lightcoral\", rug=True)\n        fig.tight_layout(pad=3.0)","6b79ff3c":"distributionPlot(plot_data)","7bb07c2e":"def pieChartPlotter(dataset, columnName):\n    \"\"\"\n    This function will take dataset and column as input and plot pie chart of the distribution within that column.\n    \"\"\"\n    values = dataset[columnName].value_counts()\n    labels = dataset[columnName].unique()\n    pie, ax = plt.subplots(figsize=[10, 6])\n    patches, texts, autotexts = ax.pie(\n        values, labels=labels, autopct='%1.2f%%', \n        shadow=True, pctdistance=.5, \n        explode=[0.06]*dataset[columnName].unique()\n    )\n    plt.legend(patches, labels, loc=\"best\")\n    plt.title(columnName, color='white', fontsize=14)\n    plt.setp(texts, color='white', fontsize=20)\n    plt.setp(autotexts, size=10, color='white')\n    autotexts[1].set_color('white')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()","29a0045d":"pieChartPlotter(dataset,'waterfront')\npieChartPlotter(dataset, 'floors')","b0b0e40b":"def countPlotter(dataset):\n    \"\"\"\n    This function will take dataset and will create a countplot for each column in the dataset with price column\n    \"\"\"\n    fig = plt.figure(figsize=(20, 20))\n    for i in tqdm_notebook(range(len(dataset.columns)), desc = 'Generating Count Plots'):\n        if not dataset.columns[i] == 'price':\n            fig.add_subplot(np.ceil(len(dataset.columns)\/2), 2, i)\n            sns.countplot(dataset[dataset.columns[i]],\n                          order=dataset[dataset.columns[i]].value_counts().index)\n\n            fig.tight_layout(pad=3.0)\n","e4c7b9d3":"#Dividing data for plotting\nplot_data = dataset.drop(\n    ['id', 'date', 'sqft_living15', 'sqft_lot15', 'lat', 'long', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode'], axis=1)","15e2e809":"countPlotter(plot_data)","d1b3ed21":"def groupBarPlotter(dataset):\n    \"\"\"\n    This function will create bar plot for each column in the dataset with price column\n    \"\"\"\n    fig = plt.figure(figsize=(20, 20))\n    for i in tqdm_notebook(range(len(dataset.columns)), desc = 'Generating Group Plots'):\n        if not dataset.columns[i] == 'price':\n            groups = dataset.groupby(dataset.columns[i])['price'].mean()\n            fig.add_subplot(np.ceil(len(dataset.columns)\/2), 2, i)\n            plt.xlabel('price')\n            groups.plot.barh()\n            fig.tight_layout(pad=3.0)","3f483ece":"groupBarPlotter(plot_data)","51815238":"house = dataset.drop(['id', 'date','price'], axis=1)","e5466bf8":"X = house.values\ny = dataset.iloc[:, 2:3].values\ncolnames = house.columns","3d10a481":"X","85d4d9d6":"y","d1372885":"X.shape","207dad1e":"y.shape","03f8c227":"ranks = {}\ndef ranking(coefficients, columnNames, order=1):\n    \"\"\"\n    This function will take coefficients of different models and will scale them and return a dictionary.\n    \"\"\"\n    minmax = MinMaxScaler()\n    if np.array(coefficients).ndim == 1:\n        coefficients = np.array(coefficients).reshape(1, -1)\n    coefficients = minmax.fit_transform(order*np.array(coefficients).T).T[0]\n    coefficients = map(lambda x: round(x, 2), coefficients)\n    return dict(zip(columnNames, coefficients))\n","1e2d951a":"ranks = {}\ndef featureRanker(X, y, ranking, colnames):\n    \"\"\"\n    This function will return the ranks dictionary with coefficients of different models. \n    This function will take independent variables and dependent variable as an input. \n    Along with that a ranking function which will generate ranks.\n    \"\"\"\n    params = {}\n    models = {\n        LinearRegression(normalize= True):  'lr',\n        Ridge(alpha= 7):  'Ridge',\n        Lasso(alpha= .05):  'Lasso',\n        ElasticNet(alpha= 0.0005, l1_ratio= .9, random_state= 0):  'Elastic',\n        RandomForestRegressor(n_jobs= -1,\n                               n_estimators= 100, random_state= 0):  'RF',\n        GradientBoostingRegressor(n_estimators= 100, random_state= 0):  'GBR',\n        XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n                      colsample_bytree=1, max_depth=70, random_state = 0):  'XGBR',\n        LGBMRegressor(n_jobs= -1,\n                       n_estimators= 100, random_state= 0):  'LGBM',\n    }\n    for i, model in enumerate(tqdm_notebook(models, desc = 'Training models')):\n        estimator = model\n        estimator.fit(X, y)\n        if models[model] == 'lr':\n            rfe = RFE(estimator, n_features_to_select=1)\n            rfe.fit(X, y)\n            ranks[\"RFE\"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)\n        if not hasattr(estimator, 'coef_'):\n            if not hasattr(estimator, 'dual_coef_'):\n                ranks[models[model]] = ranking(\n                    estimator.feature_importances_, colnames)\n            elif hasattr(estimator, 'dual_coef_'):\n                ranks[models[model]] = ranking(\n                    np.abs(svr.dual_coef_), colnames)\n        elif hasattr(estimator, 'coef_'):\n            ranks[models[model]] = ranking(\n                np.abs(estimator.coef_), colnames)\n    return ranks\n","c1abea10":"ranks = featureRanker(X, y, ranking, colnames)","062b1a55":"r = {}\nfor name in colnames:\n    r[name] = round(np.mean([ranks[method][name]\n                             for method in ranks.keys()]), 2)\n\nmethods = sorted(ranks.keys())\nranks[\"Mean\"] = r\nmethods.append(\"Mean\")\n\nprint(\"\\t%s\" % \"\\t\".join(methods))\nfor name in colnames:\n    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str,\n                                          [ranks[method][name] for method in methods]))))","2f358bb5":"'''Put the mean scores into a Pandas dataframe'''\n\nmeanplot = pd.DataFrame(list(r.items()), columns=['Feature', 'Mean Ranking'])\n\n'''Sorting the dataframe'''\nmeanplot = meanplot.sort_values('Mean Ranking', ascending=False)","b83b52df":"sns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data=meanplot,\n               kind=\"bar\", size=4, aspect=1.9, palette='coolwarm');","1008329a":"This aim of this notebook isto provide an explanation and application of different feature ranking methods, with linear models, support vector machine. ensemble learning and last but not least gradient boosting.\n\nThe contents of this notebook are as follows:\n1. Loading of Dataset : Here dataset will be loaded.\n1. Exploratory Data Analysis : Here I will correlation among the different columns, will visualise all the columns with different plots namely distribution plots, countplots and several pie charts.\n1. Data Preprocessing: Here I will will detect and correct skewness and outliers in the dataset\n1. Feature Selection: Here I will do feature selection with several methods.\n1. Feature Ranking Matrix : Matrix of all the features along with the respective model scores which we can use in our ranking will be created and will be plotted\n","641091b7":"## group Plots","ee4fc8dc":"# Feature Selection ","453157fc":"Here we can see different parameters in comparison with price.","b82776e8":"# Loading Dataset","50aca706":"## Visualising Numerical Data","a187e3e5":"## Pie Charts","adac3026":"## Finding Correlation among the variables","5d4dbf50":"# Exploratory Data Analysis\n","347a378e":"# I hope you guys like this notebook. I will be more than happy to hear your feedbacks and Please consider upvoting if you like my work.","2af09838":"## Count Plots","4f656a25":"## Distribution Plots","86e8457b":"# Creating the Feature Ranking Matrix\n\nWe combine the scores from the various methods above and output it in a matrix form :","0f3e98a3":"# Data Preprocessing","03209ad4":"## Frequency Plots","7d7279a1":"Now, with the matrix above, the numbers and layout does not seem very easy or pleasant to the eye. So lets just visualise them.","c87f1766":"# Introduction","becd66dc":"As you can see the top 3 features are : 'lat', 'waterfront', 'grade'\nand bottom 3 features are: 'yr_renovated' , 'sqft_lot15', ;sqft_lot'"}}