{"cell_type":{"b03eca89":"code","fab8685a":"code","c77abfa8":"code","39018742":"code","c093b810":"code","2d16a3ee":"code","e81c7379":"code","f52fad57":"code","56b7b805":"code","d7f06cae":"code","5707b6d7":"code","e357ece2":"code","429fabc2":"code","1758bfcb":"code","56096f61":"code","ba1e5960":"code","2a4ddd17":"code","9955555c":"code","12eb30af":"code","4cde8b55":"code","09f9164a":"code","9ecbd072":"code","ee6c3e4d":"code","9b2eb272":"code","106526a6":"code","a23087cb":"code","8a544666":"code","5b390bf3":"code","bb7bd114":"code","bdc1498b":"code","162ebf33":"code","dbed1838":"code","876c1b60":"code","a9219dfa":"code","6c0fb2a8":"code","a7bed346":"code","14371d93":"code","fbf53f3d":"code","add18751":"code","a6e81d9e":"code","90abf65d":"code","10adaffc":"code","66440240":"code","67e6c0f2":"code","0acfd0fd":"code","cc244298":"code","c0210364":"code","95d519f5":"code","b238f0b4":"code","010656d5":"code","58ccfc83":"code","1463c457":"code","f69c1c40":"code","f32e2643":"code","1d13d650":"code","d359300a":"code","8c350b49":"code","fb0aebd9":"code","3ae5e3f7":"code","fc73bb11":"code","a23a3568":"code","de43c9cf":"code","c644c58a":"code","5585708b":"code","1e2c8c77":"code","a988ac06":"code","4293cebe":"code","2f691b1f":"code","ae9c0d08":"code","4fe863b8":"code","72ad56fb":"code","04f31383":"code","04cbd731":"code","e2b8805e":"code","e451e024":"code","c676aa8c":"code","21065db3":"code","0320d219":"code","ea3b977c":"code","cbbd4b49":"code","8f6061df":"code","f160370a":"code","58848bd3":"code","8abadade":"code","1418bee6":"code","5e043af5":"code","ddba0a00":"code","6bef6851":"code","41e44c65":"code","d9454d98":"code","706be1b0":"code","852b5a0a":"code","f91457f9":"code","00ab52b3":"code","7daef43b":"code","363e851b":"code","1aafeaf6":"code","c0d4dda2":"code","ec67a71e":"code","a12efacc":"code","98559bd9":"markdown","7a801a07":"markdown","a7ac8105":"markdown","6fc1f5e5":"markdown","8bcf6982":"markdown","ef940a6b":"markdown","ac380ce2":"markdown","28e7e085":"markdown","2de08d53":"markdown","dc3ffe1c":"markdown","a0857fd7":"markdown","3e7eb1f5":"markdown","19f96cd4":"markdown","263b38c1":"markdown","f6e79808":"markdown","303500d4":"markdown","85db4a8c":"markdown","f932219d":"markdown","32f48ddc":"markdown","2423dae8":"markdown","f3ebf4cb":"markdown","74dc5946":"markdown","9cdaabce":"markdown","d47e6710":"markdown","864e7386":"markdown","4486e368":"markdown","73fd3cc2":"markdown","6a46afc3":"markdown","41db12e4":"markdown","845ab6ea":"markdown","3dd0e712":"markdown","c499e01f":"markdown","70928568":"markdown","a937a5bc":"markdown","8d78cc61":"markdown","6860386a":"markdown","bbb84cb7":"markdown","829e4eaf":"markdown"},"source":{"b03eca89":"import matplotlib.pyplot as plt\n\nimport pandas as pd\n\nimport math\n\nfrom keras import backend\nfrom keras import Input\n\nfrom keras.layers import concatenate, Dense, BatchNormalization\nfrom keras.models import Model, Sequential\n\n# for generating a png image of our model\nfrom keras.utils import plot_model\nfrom IPython.display import Image","fab8685a":"# disable the `SettingWithCopy` because we are pretty sure we know what we are doing\npd.set_option('mode.chained_assignment', None) ","c77abfa8":"# when plotting, smooth out the points by some factor (0.5 = rough, 0.99 = smooth)\n# method taken from `Deep Learning with Python` by Fran\u00e7ois Chollet\n\ndef smooth_curve(points, factor=0.75):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points","39018742":"# Plot model history more easily\n\ndef set_plot_history_data(ax, history, which_graph):\n\n    if which_graph == 'acc':\n        train = smooth_curve(history.history['acc'])\n        valid = smooth_curve(history.history['val_acc'])\n\n    if which_graph == 'loss':\n        train = smooth_curve(history.history['loss'])\n        valid = smooth_curve(history.history['val_loss'])\n\n    plt.xkcd() # make plots look like xkcd\n        \n    epochs = range(1, len(train) + 1)\n        \n    trim = 5 # remove first 5 epochs\n    # when graphing loss the first few epochs may skew the (loss) graph\n    \n    ax.plot(epochs[trim:], train[trim:], 'dodgerblue', label=('Training'))\n    ax.plot(epochs[trim:], train[trim:], 'dodgerblue', linewidth=15, alpha=0.1)\n    \n    ax.plot(epochs[trim:], valid[trim:], 'g', label=('Validation'))\n    ax.plot(epochs[trim:], valid[trim:], 'g', linewidth=15, alpha=0.1)\n\n    \ndef get_max_validation_accuracy(history):\n    validation = smooth_curve(history.history['val_acc'])\n    ymax = max(validation)\n    return 'Max validation accuracy \u2248 ' + str(round(ymax, 3)*100) + '%'","c093b810":"def plot_history(history):    \n    \n    fig, (ax1, ax2) = plt.subplots(nrows=2,\n                                   ncols=1,\n                                   figsize=(10, 6),\n                                   sharex=True,\n                                   gridspec_kw = {'height_ratios':[5, 2]})\n\n    set_plot_history_data(ax1, history, 'acc')\n    \n    set_plot_history_data(ax2, history, 'loss')\n    \n    # Accuracy graph\n    ax1.set_ylabel('Accuracy')\n    ax1.set_ylim(bottom=0.5, top=1)\n    ax1.legend(loc=\"lower right\")\n    ax1.spines['top'].set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    ax1.xaxis.set_ticks_position('none')\n    ax1.spines['bottom'].set_visible(False)\n    \n    # max accuracty text\n    plt.text(0.97,\n             0.97,\n             get_max_validation_accuracy(history),\n             horizontalalignment='right',\n             verticalalignment='top',\n             transform=ax1.transAxes,\n             fontsize=12)\n\n    # Loss graph\n    ax2.set_ylabel('Loss')\n    ax2.set_yticks([])\n    ax2.plot(legend=False)\n    ax2.set_xlabel('Epochs')\n    ax2.spines['top'].set_visible(False)\n    ax2.spines['right'].set_visible(False)\n\n    plt.tight_layout()","2d16a3ee":"# labeled data\ntitanic_path = '..\/input\/train.csv'\n\n# unlabeled data -- we'll need to predict this data\ntitanic_unlabeled = '..\/input\/test.csv'\n\ndf = pd.read_csv(titanic_path, quotechar='\"')\n\npredict = pd.read_csv(titanic_unlabeled, quotechar='\"')","e81c7379":"df.head(5)","f52fad57":"labeled_rows = len(df)\nlabeled_rows","56b7b805":"predict.insert(1, 'Survived', '?');","d7f06cae":"predict.head(5)","5707b6d7":"frames = [df, predict]\ntogether = pd.concat(frames)","e357ece2":"together[888:894]","429fabc2":"preferredOrder = ['PassengerId', 'Sex', 'Pclass', 'Cabin', 'Embarked', 'SibSp', 'Parch', 'Age', 'Fare', 'Survived']\ntogether = together[preferredOrder]","1758bfcb":"together.head(5)","56096f61":"together['isalone'] = '?'\n\ndef set_is_alone(row):\n    if row.SibSp >= 1 or row.Parch >= 1:\n        return '0'\n    else: \n        return '1'\n\ntogether['isalone'] = together.apply(set_is_alone, axis=1)","ba1e5960":"together['ischild'] = '?'\n\ndef set_is_child(row):\n    if row.Age < 18:\n        return '1'\n    else:\n        return '0'\n\ntogether['ischild'] = together.apply(set_is_child, axis=1)","2a4ddd17":"# confirm we did what we wanted\ntogether[['SibSp', 'Parch', 'isalone', 'Age', 'ischild']].head(10)","9955555c":"together.head(5)","12eb30af":"together.shape","4cde8b55":"# first four columns\ncategorical_data = preferredOrder[1:5] # ignore `PassengerId`\n\n# and the two we created\ncategorical_data.append('ischild')\ncategorical_data.append('isalone')\n\ncategorical_data","09f9164a":"together['Cabin'].values[:20]","9ecbd072":"def cleanCabin(el):\n    if isinstance(el, str):\n        return el[:1]\n    else:\n        return '0'\ntogether['Cabin'] = together['Cabin'].apply(cleanCabin)\ntogether['Cabin'].values[:20]","ee6c3e4d":"def convert_to_natural_number(x, temp_dict):\n    if x in temp_dict:\n        return temp_dict[x]\n    else:\n        temp_dict[x] = temp_dict['curr_count']\n        temp_dict['curr_count'] += 1\n        return temp_dict[x]\n","9b2eb272":"def categorical_column_to_number(col):\n    temp_dict = temp_dict = {'curr_count': 0}\n    together[col] = together[col].apply(convert_to_natural_number, args=(temp_dict,))","106526a6":"together[categorical_data].head(10)","a23087cb":"for col in categorical_data:\n    categorical_column_to_number(col)","8a544666":"together[categorical_data].head(10)","5b390bf3":"together.shape","bb7bd114":"newDF = pd.DataFrame()","bdc1498b":"for col in categorical_data:\n    one_hot = pd.get_dummies(together[col])\n    one_hot = one_hot.add_prefix(col)\n    for new_name in list(one_hot):\n      newDF[new_name] = one_hot[new_name]","162ebf33":"newDF.shape","dbed1838":"newDF.head(5)","876c1b60":"newDF.shape","a9219dfa":"print(list(newDF))","6c0fb2a8":"print('Column   \\ttotal entrties')\nprint('-------------------------------')\nfor col in list(newDF):\n    total = newDF[col].sum()\n    print(col,':    ', '\\t', total, sep='', end='')\n    if (total < 10):\n        newDF = newDF.drop([col], axis=1)\n        print('\\t<-- dropped', end='')\n    print()","a7bed346":"newDF.head(5)","14371d93":"one_hot_columns = list(newDF)\nprint(one_hot_columns)","fbf53f3d":"result = pd.concat([newDF, together], axis=1, join_axes=[newDF.index])","add18751":"result = result.drop(categorical_data, axis=1)","a6e81d9e":"result[one_hot_columns].head(5)","90abf65d":"numerical_data = preferredOrder[5:9]\nnumerical_data","10adaffc":"def normalize(x, colMax, colMean):\n    if math.isnan(x):\n        return 0\n        # I have seen this approach instead but don't think it yields better results\n        # return colMean\n    if isinstance(x, float):\n        return x \/ colMax\n    elif isinstance(x, int):\n        return float(x) \/ colMax\n    else:\n        return 0","66440240":"def applyNormalize(col):\n    column_max = result[col].max()\n    column_mean = result[col].mean()\n    result[col] = result[col].apply(normalize, args=(column_max, column_mean))","67e6c0f2":"for col in numerical_data:\n    applyNormalize(col)","0acfd0fd":"result[numerical_data].head(3)","cc244298":"result[numerical_data].describe()","c0210364":"result[one_hot_columns].head(5)","95d519f5":"result[numerical_data].head(5)","b238f0b4":"rows_to_predict = result[labeled_rows:]","010656d5":"rows_to_predict.head(5)","58ccfc83":"result = result[:labeled_rows].sample(frac=1)","1463c457":"result.shape","f69c1c40":"x_cat_all = result[one_hot_columns]\nx_num_all = result[numerical_data]\n\ny_data_all = result['Survived']","f32e2643":"# Here is how you can manually split the data into training & validation\n# later we will use Keras's native method (making this process very simple)\n\n# 80% for training, the rest for validation\ncutoff = round(0.8 * len(x_cat_all)) \n\n# training\nx_cat = x_cat_all[:cutoff]\nx_num = x_num_all[:cutoff]\ny_train = y_data_all[:cutoff]\n\n# validation\nx_cat_val = x_cat_all[cutoff:]\nx_num_val = x_num_all[cutoff:]\ny_validation = y_data_all[cutoff:]","1d13d650":"print('left input:', len(one_hot_columns))\nprint('right input:', len(numerical_data))","d359300a":"x_cat.shape","8c350b49":"x_num.shape","fb0aebd9":"# backend.clear_session()\n\n# categorical branch -- 'left'\nleft_in = Input(shape=(19,))\n\nleft1  = Dense(64, activation='relu')(left_in)\nleft1n = BatchNormalization()(left1)\nleft2  = Dense(32, activation='relu')(left1n)\nleft2n = BatchNormalization()(left2)\nleft3  = Dense(16, activation='relu')(left2n)\nleft3n = BatchNormalization()(left3)\nleft4  = Dense(8,  activation='relu')(left3n)\nleft4n = BatchNormalization()(left4)\nleft5  = Dense(4,  activation='relu')(left4n)\n\nleft_out  = Dense(1, activation='sigmoid')(left5)\n\n# numerical branch -- 'right'\nright_in = Input(shape=(4,))\n\nright1  = Dense(64, activation='relu')(right_in)\nright1a = BatchNormalization()(right1)\nright2  = Dense(32, activation='relu')(right1a)\nright2a = BatchNormalization()(right2)\nright3  = Dense(16, activation='relu')(right2a)\nright3a = BatchNormalization()(right3)\nright4  = Dense(8,  activation='relu')(right3a)\nright4a = BatchNormalization()(right4)\nright5  = Dense(4,  activation='relu')(right4a)\n\nright_out = Dense(1, activation='sigmoid')(right5)\n\n# merge two branches\nmerge_in = concatenate([left_out, right_out])\n\ndense1  = Dense(16, activation='relu')(merge_in)\ndense2  = Dense(8,  activation='relu')(dense1)\ndense2a = BatchNormalization()(dense2)\ndense3  = Dense(8,  activation='relu')(dense2a)\n\noutput  = Dense(1, activation='sigmoid')(dense3)\n\nmodel_new = Model(inputs=[left_in, right_in], outputs=output)\n\nmodel_new.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n# model_new.summary()","3ae5e3f7":"plot_model(model_new, show_shapes=True, to_file='model.png', show_layer_names=False)\nImage('model.png', width=200)","fc73bb11":"history_final = model_new.fit([x_cat, x_num],\n                              y_train,\n                              epochs=150,\n                              verbose=0,\n                              batch_size=64,\n                              validation_data=([x_cat_val, x_num_val], y_validation))","a23a3568":"plot_history(history_final)","de43c9cf":"# model_new.save('model_new.h5')","c644c58a":"x_data = result[one_hot_columns]\ny_data = result['Survived']","5585708b":"print('Number of inputs:', len(one_hot_columns))","1e2c8c77":"result[one_hot_columns].shape","a988ac06":"# backend.clear_session()\n\nmodel_cat = Sequential()\n\nmodel_cat.add(Dense(64, activation='relu', input_shape=(19,)))\nmodel_cat.add(Dense(32, activation='relu'))\nmodel_cat.add(BatchNormalization())\nmodel_cat.add(Dense(16, activation='relu'))\nmodel_cat.add(BatchNormalization())\nmodel_cat.add(Dense(8, activation='relu'))\nmodel_cat.add(BatchNormalization())\nmodel_cat.add(Dense(4, activation='relu'))\nmodel_cat.add(BatchNormalization())\n# model_cat.add(Dense(8, activation='relu'))\n# model_cat.add(Dense(4, activation='relu'))\n# model_cat.add(Dense(8, activation='relu'))\n# model_cat.add(Dense(32, activation='relu'))\nmodel_cat.add(Dense(1, activation='sigmoid'))\n\nmodel_cat.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n# model_cat.summary()","4293cebe":"history_cat = model_cat.fit(x_data,\n                            y_data,\n                            validation_split=0.2,\n                            epochs=150,\n                            batch_size=64,\n                            verbose=0)","2f691b1f":"plot_history(history_cat)","ae9c0d08":"result[numerical_data].head(5)","4fe863b8":"x_data = result[numerical_data]\ny_data = result['Survived']","72ad56fb":"# backend.clear_session()\n\nfrom keras.layers import Dropout\n\nmodel_num = Sequential()\n\nmodel_num.add(Dense(64, activation='relu', input_shape=(4,)))\nmodel_num.add(Dropout(0.1))\nmodel_num.add(Dense(32, activation='relu'))\nmodel_num.add(Dropout(0.1))\nmodel_num.add(Dense(16, activation='relu'))\nmodel_num.add(Dropout(0.1))\nmodel_num.add(Dense(8, activation='relu'))\nmodel_num.add(Dense(4, activation='relu'))\n# model_num.add(Dense(8, activation='relu'))\n# model_num.add(Dense(4, activation='relu'))\n# model_num.add(Dense(32, activation='relu'))\nmodel_num.add(Dense(1, activation='sigmoid'))\n\nmodel_num.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n# model_num.summary()","04f31383":"history_num = model_num.fit(x_data,\n                            y_data,\n                            epochs=300,\n                            batch_size=len(x_data),\n                            verbose=0,\n                            validation_split=0.2)","04cbd731":"plot_history(history_num)","e2b8805e":"# one option is to freeze the first two models\n# model_cat.trainable = False\n# model_num.trainable = False\n# but we will not\nmodel_cat.trainable = True\nmodel_num.trainable = True","e451e024":"merge = concatenate([model_num.output, model_cat.output])\n\n# d1 =  Dense(8, activation='relu')(merge)\n# # d1n = BatchNormalization()(d1)\n# d2 =  Dense(16, activation='relu')(d1)\n# # d2n = BatchNormalization()(d2)\n# d3 =  Dense(8, activation='relu')(d2)\n# # d3n = BatchNormalization()(d2)\n# d4 =  Dense(4, activation='relu')(d3)\n\noutput = Dense(1, activation='sigmoid')(merge)\n\njoint_model = Model(inputs=[model_num.input, model_cat.input], outputs=output)\n\njoint_model.compile(optimizer='rmsprop',\n                    loss='binary_crossentropy',\n                    metrics=['accuracy'])\n\n# joint_model.summary()","c676aa8c":"plot_model(joint_model, show_shapes=True, to_file='model2.png', show_layer_names=False)\nImage('model2.png', width=200)","21065db3":"from keras.callbacks import ModelCheckpoint","0320d219":"filepath=\"{val_acc:.2f}-accuracy.h5\"\ncheckpoint = ModelCheckpoint(filepath,\n                             monitor='val_acc',\n                             verbose=0,\n                             save_best_only=True,\n                             mode='max')\n\ncallbacks_list = [checkpoint]","ea3b977c":"history_final = joint_model.fit([x_num, x_cat],\n                                y_train,\n                                epochs=300,\n                                verbose=0,\n                                batch_size=len(x_num),\n                                callbacks=callbacks_list,\n                                validation_split=0.2)","cbbd4b49":"plot_history(history_final)","8f6061df":"# joint_model.save('joint_model.h5')","f160370a":"from keras.models import load_model","58848bd3":"# you would load the best saved model here (notice the custom callback that saves the best-performing model) \n# loaded_model = load_model('0.84-accuracy.h5')\n# but for Kaggle kernel we'll just load the model as it is after the last epoch\nloaded_model = joint_model","8abadade":"rows_to_predict.head(5)","1418bee6":"rows_to_predict.shape","5e043af5":"passenger_ids_to_predict = rows_to_predict['PassengerId']","ddba0a00":"prediction = loaded_model.predict([rows_to_predict[numerical_data], rows_to_predict[one_hot_columns]])","6bef6851":"prediction.shape","41e44c65":"prediction[:10]","d9454d98":"prediction = (prediction > 0.5).astype(int).reshape(-1)","706be1b0":"prediction[:10]","852b5a0a":"submission = pd.DataFrame({\"PassengerId\": passenger_ids_to_predict, \"Survived\": prediction})\nsubmission.to_csv('submission.csv', index=False)","f91457f9":"from keras.utils import to_categorical","00ab52b3":"for col in categorical_data:\n    print(col)\n    lol = (to_categorical(together[col]))\n    print(lol[:5])","7daef43b":"# from IPython.display import SVG\n# from keras.utils.vis_utils import model_to_dot\n\n# SVG(model_to_dot(model_new).create(prog='dot', format='svg'))","363e851b":"def plot_this(training, validation, label):\n    \n    epochs = range(1, len(training) + 1)\n    \n    plt.clf() # clear out old\n    \n    plt.xkcd() # make look like xkcd\n    \n    # plt.figure(figsize=(8, 3)) # make wider\n    \n    trim = 10 # remove first 10 data points\n\n    plt.plot(epochs[trim:], training[trim:], 'b', label=('Training '+label))\n    plt.plot(epochs[trim:], validation[trim:], 'g', label=('Validation '+label))\n    plt.title('Model ' + label)\n    plt.xlabel('Epochs')\n    plt.ylabel(label)\n    \n    if label == 'Loss':\n        plt.yticks([])\n        \n    if label == 'Accuracy':\n        plt.ylim(ymin=0.5)\n        plt.ylim(ymax=1)\n        ymax = max(validation[trim:])\n        best = 'Max validation accuracy \u2248 ' + str(round(ymax, 3)*100) + '%'\n        plt.text(0, 0.35, best, fontsize=12)\n        \n    plt.legend()\n\n    return plt","1aafeaf6":"def plot_history_old(history):\n\n    label1 = 'Loss'\n    train1 = smooth_curve(history.history['loss'])\n    valid1 = smooth_curve(history.history['val_loss'])\n\n    plot_this(train1, valid1, label1).show()\n    \n    label2 = 'Accuracy'\n    train2 = smooth_curve(history.history['acc'])\n    valid2 = smooth_curve(history.history['val_acc'])\n    \n    plot_this(train2, valid2, label2).show()","c0d4dda2":"plot_history_old(history_final)","ec67a71e":"import seaborn as sns","a12efacc":"facet = sns.FacetGrid(df, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, df['Age'].max()))\nfacet.add_legend()","98559bd9":"## Build two-input model","7a801a07":"`Cabin` values need to be placed into categories.","a7ac8105":"## Merge the two models","6fc1f5e5":"Can be as high as 82%","8bcf6982":"Worked example for the Titanic Kaggle competition: https:\/\/www.kaggle.com\/c\/titanic","ef940a6b":"## Prepare categorical data","ac380ce2":"## Create two new categories: alone & child","28e7e085":"## Train numerical","2de08d53":"# Archive","dc3ffe1c":"## Archive: another way to plot\nsimpler code but less pretty","a0857fd7":"## Imports & methods for plotting graphs","3e7eb1f5":"## Archive: another way to visually see your model\n\nThis method does not create a .png file and will render directly to Jupyter (but result will not be as detailed or pretty)\n\n*note*: you'll need to install `graphviz` externally for this diagram to be generated","19f96cd4":"**Randomize the order of all the rows**\n\n_this is important because the original data may not be (and in this case is not) random_","263b38c1":"Can be as high as 77%","f6e79808":"## Archive: plot with seaborn","303500d4":"## Archive: Keras one-hot method","85db4a8c":"0. Import libraries & create helper functions\n1. Import & look at data\n2. Prepare categorical data\n3. Prepare numerical data\n4. Create model\n5. Train model\n6. ???\n7. Profit","f932219d":"Keras has `to_categorical` method for **one hot encoding** of data, but it requires natural number input.\n\nWe will create our own conversion","32f48ddc":"Prepare checkpoint method for storing the best model","2423dae8":"Some columns have too-few elements and we'll drop them - you can't learn from too-few examples (they may be flukes and not statistically representative)","f3ebf4cb":"We will use 6 categories. The four given:\n - Sex\n - Pclass\n - Cabin\n - Embarked\n \nas well as the two we created\n - is_child\n - is_alone","74dc5946":"## Let's begin!","9cdaabce":"## Predict unlabeled data","d47e6710":"## Prepare numerical data","864e7386":"Now we are ready to convert data to _one hot encoding_","4486e368":"Split validation & training data","73fd3cc2":"## Split data for training","6a46afc3":"Can be as high as 87%","41db12e4":"## Build numerical","845ab6ea":"# Titanic Kaggle","3dd0e712":"Keep only desired rows and rearrange for easier view:","c499e01f":"We will use these 4 as numbers:\n - SibSp (# of **sib**lings \/ # of **sp**ouses)\n - Parch (# of **par**ent \/ # of **ch**ildren)\n - Age\n - Fare","70928568":"Can be as high as 83% accurate","a937a5bc":"Note that unlike in the previous example, we are using Keras's method \n```\nvalidation_split=0.2\n```\ninstead of explicit validation\n```\nvalidation_data=(x_validation, y_validation)\n```","8d78cc61":"## Train categorical","6860386a":"# Alternative approach\nBuild two models, train them, and then build a third to fuse the two","bbb84cb7":"## Built categorical model","829e4eaf":"Notice that max is now 1"}}