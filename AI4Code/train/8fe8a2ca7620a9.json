{"cell_type":{"2e507be9":"code","0e3bdc69":"code","6718f974":"code","646ff5d9":"code","6c0f171c":"code","f8f65019":"code","8e66a8ab":"code","48de129c":"code","056a943d":"code","481b1697":"code","2773dda9":"code","73602f40":"code","befe1dec":"code","ebe5a060":"markdown"},"source":{"2e507be9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom math import sqrt\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e3bdc69":"train_data = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest_data = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\n\nfeature_columns = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n     'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nlen_features = len(feature_columns)\n\ntarget_cloumn = 'target'\nX_data = train_data[feature_columns]\ny_data = train_data[target_cloumn]\nX_test = test_data[feature_columns]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_data, y_data, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nnumerical_cols = [col for col in X_train.columns if X_train[col].dtype != 'object']\ncategorical_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\nhigh_cardinality_cols = [col for col in categorical_cols  if X_train[col].nunique() >= 10]\nlow_cardinality_cols = [col for col in categorical_cols  if X_train[col].nunique() < 10]\n","6718f974":"len(X_valid)","646ff5d9":"print(high_cardinality_cols)\nlow_cardinality_cols","6c0f171c":"i=0\na = [col for col in X_train.columns if X_train[col].isnull().any()]\nprint(a)","f8f65019":"y=[]\nx=[]\nz=[]\nnumerical_transformer = SimpleImputer(strategy='median')\n\ncategorical_transformer1 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\ncategorical_transformer2 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordi', OrdinalEncoder())\n])\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer1, low_cardinality_cols),\n        ('ordi', categorical_transformer2, high_cardinality_cols),\n    ])\n\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n\n","8e66a8ab":"#reduced_X_train = X_train.drop(high_cardinality_cols, axis=1)\n#reduced_X_valid = X_valid.drop(high_cardinality_cols, axis=1)\nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[high_cardinality_cols] = ordinal_encoder.fit_transform(X_train[high_cardinality_cols])\nlabel_X_valid[high_cardinality_cols] = ordinal_encoder.transform(X_valid[high_cardinality_cols])\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(label_X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(label_X_valid[low_cardinality_cols]))\n\nOH_cols_train.index = label_X_train.index\nOH_cols_valid.index = label_X_valid.index\n\nnum_X_train = label_X_train.drop(low_cardinality_cols, axis=1)\nnum_X_valid = label_X_valid.drop(low_cardinality_cols, axis=1)\n\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n\n","48de129c":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(OH_X_train, y_train)","056a943d":"\nvalid_preds = my_model.predict(OH_X_valid)\nprint(valid_preds)\n","481b1697":"\nscore = mean_squared_error(y_valid, valid_preds)\nmae = mean_absolute_error(y_valid, valid_preds)\nprint(mae)\nprint('MSE:', score)\nrscore = sqrt(score)\nprint('RMSE:', rscore)","2773dda9":"output = pd.DataFrame({'id': test_data.id ,'target': preds})\noutput.to_csv('submission_1.csv', index=False)","73602f40":"XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.05, max_delta_step=0, max_depth=6,\n             min_child_weight=1, missing=nan, monotone_constraints='()',\n             n_estimators=1000, n_jobs=4, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nMSE: 0.5219331100061471\nRMSE: 0.7224493823141848\n    \n#drop high cardinality columns","befe1dec":"model = RandomForestRegressor(n_estimators=150,random_state=0)\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', model)])\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\npreds = my_pipeline.predict(X_valid)\n\n    # Evaluate the model\nscore = mean_squared_error(y_valid, preds)\nprint('MSE:', score)\nrscore = sqrt(score)\nprint('RMSE:', rscore)\ny.append(rscore)\nz.append(score)\n\n#7.36\n#drop cat9 col..with high cardinality\npreds = my_pipeline.predict(X_test)","ebe5a060":"**** dropping high cardinality cols ****"}}