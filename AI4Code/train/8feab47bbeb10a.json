{"cell_type":{"479dd457":"code","f2af1a4f":"code","1c342116":"code","f312e32a":"code","ce29274a":"code","aa1df739":"code","9861c59c":"code","6f3c4abc":"code","b69caee2":"code","65105029":"code","3b7dfa7d":"code","4d6fbba5":"code","5fd882ac":"code","675846b9":"code","8fab0aa1":"code","685f40bf":"code","fa723656":"code","6a0d7859":"code","56638243":"code","c61c428c":"code","fef79d6b":"code","bf25b690":"code","2c83e50d":"code","03cf3932":"code","51198be4":"code","618d7ad8":"code","c24b4e69":"code","d18699f3":"code","c085c20b":"code","3acd38b2":"code","99fe4b6c":"markdown","d6a06795":"markdown","75dc746b":"markdown","8297e9ee":"markdown"},"source":{"479dd457":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2af1a4f":"#Importing modules and libraries for data visualizations, algorithm, data splitting and testing, accuracy score\nimport seaborn as sns\nimport matplotlib as plt\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import train_test_split \n\nfrom sklearn.preprocessing import LabelEncoder","1c342116":"train=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain.head(10)","f312e32a":"#counting missing values of the train set\ntrain.isnull().sum()","ce29274a":"#counting missing values of the test set\ntest.isnull().sum()","aa1df739":"#creating a function for data visualisations using seaborn\ndef categorical_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', verbose=True):\n    if x == None:\n        column_interested = y\n    else:\n        column_interested = x\n    series = dataframe[column_interested]\n    print(series.describe())\n    print('mode: ', series.mode())\n    if verbose:\n        print('='*80)\n        print(series.value_counts())\n\n    sns.countplot(x=x, y=y, hue=hue, data=dataframe, palette=palette)","9861c59c":"#cheking general numbers: how many passengers survived and died\n#we can see that there are more passengers who died\nc_palette = ['tab:blue', 'tab:red']\ncategorical_summarized(train, y = 'Survived', palette=c_palette)","6f3c4abc":"#the same thing, but using Sex - how many men and women survived. \n#Female survival chances were much higher than male ones.\ncategorical_summarized(train, y=\"Sex\", hue = 'Survived', palette=c_palette)","b69caee2":"#distribution of dead\/survived passengers depending on their embarkation\ncategorical_summarized(train, y= \"Embarked\", hue = 'Survived', palette=c_palette)","65105029":"#The same numbers distributed by Pclass. Predictably, the higher class - the higher survival chances\ncategorical_summarized(train, x=\"Pclass\", hue = 'Survived', palette=c_palette)","3b7dfa7d":"#Correlation map. We can see that the highest correlation with Survived label have Pclass and Fare\n#those two features have pretty high correlation.\nsns.heatmap((train.loc[:,[\"Age\", \"SibSp\", 'Parch', 'Fare', 'Pclass', 'Survived']]).corr(), annot=True)","4d6fbba5":"\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data = train, palette = \"Set1\")","5fd882ac":"#concatenating data sets\nall_data = train.append(test)","675846b9":"#Filling missing Age values, using created Title feature\nimport re\nall_data['Title'] = all_data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\nall_data.replace({'Title': mapping}, inplace=True)\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\nfor title in titles:\n    age_to_impute = all_data.groupby('Title')['Age'].median()[titles.index(title)]\n    all_data.loc[(all_data['Age'].isnull()) & (all_data['Title'] == title), 'Age'] = age_to_impute\n    \n# Substituting Age values in train and test:\ntrain['Age'] = all_data['Age'][:891]\ntest['Age'] = all_data['Age'][891:]\n\nsns.countplot(x='Title', data=all_data);\n#all_data.drop('Title', axis = 1, inplace = True)\ntrain['Title'] = all_data['Title'][:891]\ntest['Title'] = all_data['Title'][891:]\n\ntrain['Title'].replace(['Mr', 'Mrs', 'Miss', 'Master', 'Rev', 'Dr'],[1,2,3,4,5,6],inplace=True)\ntest['Title'].replace(['Mr', 'Mrs', 'Miss', 'Master', 'Rev', 'Dr'],[1,2,3,4,5,6],inplace=True)","8fab0aa1":"all_data['Family_Size'] = all_data['Parch'] + all_data['SibSp']\n\n# Substituting Age values in train and test data frames:\ntrain['Family_Size'] = all_data['Family_Size'][:891]\ntest['Family_Size'] = all_data['Family_Size'][891:]","685f40bf":"all_data['Last_Name'] = all_data['Name'].apply(lambda x: str.split(x, \",\")[0])\nall_data['Fare'].fillna(all_data['Fare'].mean(), inplace=True)\n\nDEFAULT_SURVIVAL_VALUE = 0.5\nall_data['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in all_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 0\n\nprint(\"Number of passengers with family survival information:\", \n      all_data.loc[all_data['Family_Survival']!=0.5].shape[0])","fa723656":"for _, grp_df in all_data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \nprint(\"Number of passenger with family\/group survival information: \" \n      +str(all_data[all_data['Family_Survival']!=0.5].shape[0]))\n\n# # Family_Survival in train and test:\ntrain['Family_Survival'] = all_data['Family_Survival'][:891]\ntest['Family_Survival'] = all_data['Family_Survival'][891:]","6a0d7859":"#counting for how many people every ticket was usedm another grouping\nall_data['Ticket_Frequency'] = all_data.groupby('Ticket')['Ticket'].transform('count')\ntrain['Ticket_Frequency'] = all_data['Ticket_Frequency'][:891]\ntest['Ticket_Frequency'] = all_data['Ticket_Frequency'][891:]","56638243":"#Fare per one\nall_data[\"Fare\"] = all_data[\"Fare\"].fillna(test[\"Fare\"].median())\nall_data[\"Fare_per_one\"]=all_data['Fare']\/all_data['Ticket_Frequency']\n\n# Making 16 Bins for Fare\nall_data['FareBin'] = pd.qcut(all_data['Fare_per_one'], 16)\n\nlabel = LabelEncoder()\nall_data['FareBin_Code'] = label.fit_transform(all_data['FareBin'])\n\ntrain['FareBin_Code'] = all_data['FareBin_Code'][:891]\ntest['FareBin_Code'] = all_data['FareBin_Code'][891:]\n\ntrain.drop(['Fare'], 1, inplace=True)\ntest.drop(['Fare'], 1, inplace=True)","c61c428c":"#Making 4 bins for Age\nall_data['AgeBin'] = pd.qcut(all_data['Age'], 6)\n\nlabel = LabelEncoder()\nall_data['AgeBin_Code'] = label.fit_transform(all_data['AgeBin'])\n\ntrain['AgeBin_Code'] = all_data['AgeBin_Code'][:891]\ntest['AgeBin_Code'] = all_data['AgeBin_Code'][891:]\n\ntrain.drop(['Age'], 1, inplace=True)\ntest.drop(['Age'], 1, inplace=True)","fef79d6b":"sns.set(rc={'figure.figsize':(13,8)})\nsns.heatmap((train.loc[:,['SibSp', 'Parch', 'Family_Size','FareBin_Code', \n            'AgeBin_Code','Pclass', 'Ticket_Frequency', 'Family_Survival','Survived']]).corr(), annot=True)","bf25b690":"#replacing 'malee' and 'female' with 0 and 1 for the algoritm\ntrain['Sex'].replace(['male','female'],[0,1],inplace=True)\ntest['Sex'].replace(['male','female'],[0,1],inplace=True)\n\n#dropping unnecessary features\ntrain.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin', \n               'Embarked'], axis = 1, inplace = True)\ntest.drop(['Name','PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin', \n              'Embarked'], axis = 1, inplace = True)","2c83e50d":"train.head()","03cf3932":"y = train['Survived']\ntrain_df = train.drop('Survived', 1)\ntest_df = test.copy()","51198be4":"#test\/train splitting\nmy_train, my_test, my_y, my_res = train_test_split(train_df, y, test_size=0.1, random_state=42)","618d7ad8":"#creating trial RFC model\nmodel_try = RandomForestClassifier(n_estimators=300, max_depth=5, random_state=42)\nmodel_try.fit(my_train, my_y)\npreds = model_try.predict(my_test)\nprint(accuracy_score(my_res, preds))","c24b4e69":"#feature importance of the algorithm\npd.Series(model_try.feature_importances_, index = my_train.columns).nlargest(12).plot(kind = 'barh',\n                            figsize = (10, 10),title = 'Feature importance from Random Forest').invert_yaxis()","d18699f3":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","c085c20b":"#creating the model, predictions and output\nmodel = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\nmodel.fit(train_df, y)\npredictions = model.predict(test_df)\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(predictions)","3acd38b2":"#feature importance iss a bit different from the above model on train data\npd.Series(model.feature_importances_, index = train_df.columns).nlargest(30).plot(kind = 'barh',\n                            figsize = (10, 10),title = 'Feature importance from Random Forest').invert_yaxis()","99fe4b6c":"Let's have a look at new correlation map. \nAs was said, SibSp, Parch, Family Size are highly correlated. It could be expected 1.0 correlation between Family_size and Ticket_Frequency, but it is only 0.82","d6a06795":"Violinplots to look at the distribution using Pclass and Age.\nThere were more dead people at the age 40-60 in the first class and more younger people survived (~25-45) and we can see some older outliers here. Very few kids survived and very few of them died, it seems, that there no many kids in the first class.\nIn the second class, there are much more dead people in younger category - 20-35 and a bit less of survived adults and kids (no kids till ~10 years old died)\nIn the third class, there are some kids who died, those ones whose families were too big. In general, the youngest people died in this group. My guess - young single men who were going for a better life and job.","75dc746b":"**Family_survival.**\nThis feature wasn't written by me. I googled the code having an idea what I need to do, thanks to SQL insights about grouping. The original work is here: https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever \nWe will get family survival numbers: 0, 0.5, 1.","8297e9ee":"So, Titanic. Let me say, that this is both great and tricky problem for beginners to learn along the way. I am a beginner ML engineer, thus can verify it. I have more than 160 saves of this work, totally changed the code 3 times, starting with basic features choice to adding more features and ending up at reducing features. \n\nIn this notebook I will show a bit of basic EDA with data visualizations, not going to deep into feature engineering (the main explanation of the whole process is below) and train the model with Random Forest Classifier. \nLet's dive into it!\n\n**1. PassengerID**\n\nTotally useless feature for modeling, it will be dropped by default.\n\n**2. Survived**\n\nThat is our label, it will be used later for feature correlations. \n\n**3. Pclass**\n\nCategorical feature, that can be used in a model with OneHotEncoder or without it. I am writing about RF only now. In the previous versions I used this feature with get_dummies, thus had three more features in the feature list. It demonstrates class of passengers' cabin. Very important one to understand  the difference of surviva rate. Well, that is too obvious. \n\n**4. Name**\n\nIn the first versions I used classical beginner approach - getting titles from names to understand which men could survive. It turned out that it was helpful, basically, I got another Sex features that gets weights of dying men. The result was 79,9% accuracy (mean the public score). The second way was adding Surnames. Feature list after get_dummies was too big (over 900 positions), but it didn't help at all, the result was not better than 78% accuracy. in the last version feature \"Titles\" was used as well.\n\n**5. Sex**\n\nObvious as well. \"Women and children first\". i will provide some code to see that men had less chances to survive, of course.\n\n**6. Age**\n\nThe way of processing this feature is the next one:\n1) using mean values to fill mising values\n2) using median (that really improved the score)\n3) adding bins (another improvement of the score)\n4) creating a function to fill missing values using Sex and Pclass, using median values of these groups\n5) creating a function to fill missing values using Title feature and median. \n\n**7. SibSp and Parch**\n\nThe quantity of siblings, spouses, parents, children. Used it for creating features like FamilySize or IsAlone to get chances of survival depending on having a family or not. It happened that using all of them at once wasn't helpful, just a noise, no improvement to the model. I experimented with using only SibSp and  Parch, adding FamilySize or IsAlone to them, it didn't help. The answer was revealed later - if you have a lot of highly correlated features between themselves, it's better to use one-two of them. In this version there is Family_Size feature.\n\n**8.Ticket and Fare**\n\nUsing this feature wasn't very helpful in terms of get_dummies. A lot of feature, some of them had some values in feature_importance table (because there were big families who used one ticket for all members of the family, it also included not only family members, but also nannys or servants etc). It had more helpful use in pair with **Fare** feature, because it was revealed that big families  in the third class have payed pretty big money that could be equal to second class prices. That is why I decided those were prices for the whole ticket and not a fare per person. Divided Fare\/Ticket_Frequency and got Fare_per_one. \n\n**9. Cabin, Embarked**\n\nSpent a lot of time on dealing with them - creating Decks, grouping those decks in different combinations, but it seems, the prefiction power of these features is not very high. \n\nAbout algorithms: used Random Forest, XGBoost, SVM, KNN. My personal favourite for this task is Random Forest, it did the best job, thus I lest only the code for using this particular algorithm. "}}