{"cell_type":{"0cc71c2a":"code","8266b533":"code","635bdc33":"code","f246ebe1":"code","4259c1de":"code","408ab0c9":"code","2a325948":"code","b77754f9":"code","78f6d5e5":"code","d97fe4c2":"code","b616b3d2":"code","49878fd5":"code","2a32bcd4":"code","bd072169":"code","9c238ac6":"code","dd2892d4":"code","e14ed05b":"code","0d8200f7":"code","05b57699":"code","4af6d0c4":"code","191d0f69":"code","eb5c3722":"code","c1954c1f":"code","3da04aa3":"code","3e82bde5":"markdown","ba241186":"markdown","bae6f3ce":"markdown","7ccb9a6b":"markdown","eaab8886":"markdown"},"source":{"0cc71c2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8266b533":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans","635bdc33":"data = pd.read_csv('..\/input\/wine-dataset-for-clustering\/wine-clustering.csv')","f246ebe1":"data.head()","4259c1de":"data.describe()","408ab0c9":"data.info()","2a325948":"data.isnull().sum().sort_values(ascending=False)","b77754f9":"sns.set(font_scale=1.3, rc={'figure.figsize':(20,20)})\nax=data.hist(bins=20,color='blue' )","78f6d5e5":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ndata_scaled=data.copy()\ndata_scaled[data_scaled.columns]=scaler.fit_transform(data_scaled)","d97fe4c2":"data_scaled.describe()","b616b3d2":"corr = data_scaled.corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr, linewidths=.5, cmap='viridis')","49878fd5":"pca = PCA()\n\npca_data = pca.fit_transform(data_scaled)","2a32bcd4":"pca.get_covariance()","bd072169":"explained_variance=pca.explained_variance_ratio_\nexplained_variance","9c238ac6":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(8, 6))\n\n    plt.bar(range(13), explained_variance, alpha=0.5, align='center',\n            label='Explained Variance representation')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","dd2892d4":"#Plotting the Cumulative Summation of the Explained Variance\nplt.figure(figsize = (8,5))\nplt.plot(np.cumsum(pca.explained_variance_ratio_) * 100)\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)')\nplt.title('derma - Explained Variance')\n\n# We want to preserve 95% of the variance so plot a horizontal line at 95% variance\nplt.axhline(y=85, color='r', linestyle='--')\nplt.show()","e14ed05b":"pca2 = PCA(2)\n\npca_data2 = pca2.fit_transform(data_scaled)\n\nprint ('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(pca2.explained_variance_ratio_)))","0d8200f7":"\nplt.figure(figsize=(10,6))\nplt.scatter(x=pca_data2[:, 0], y=pca_data2[:, 1], color='blue',lw=0.1)\nplt.xlabel('PC 1')\nplt.ylabel('PC 2')\nplt.title('Data')\nplt.show()","05b57699":"SSE_scaled = []\nfrom sklearn.cluster import KMeans","4af6d0c4":"for cluster in range(1,20):\n    kmeans = KMeans( n_clusters = cluster)\n    kmeans.fit(data_scaled)\n    SSE_scaled.append(kmeans.inertia_)","191d0f69":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,6))\nframe_scaled = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE_scaled})\nplt.plot(frame_scaled['Cluster'], frame_scaled['SSE'], marker='o')\nplt.xlabel(\"Clusters\")\nplt.ylabel(\"sq. sum of error\")","eb5c3722":"kmeans = KMeans(n_clusters = 3,init='k-means++',random_state=9)\nkmeans.fit(data_scaled)\npred = kmeans.predict(data_scaled)","c1954c1f":"frame = pd.DataFrame(data_scaled)\ndata['cluster'] = pred\n\ndata['cluster'].value_counts()","3da04aa3":"aux=data.columns.tolist()\naux[0:len(aux)-1]\n\nfor cluster in aux[0:len(aux)-1]:\n    grid= sns.FacetGrid(data, col='cluster')\n    grid.map(plt.hist, cluster,color='red')","3e82bde5":"**Applying clustering on main dataset**","ba241186":"### using  k-means","bae6f3ce":"## using PCA","7ccb9a6b":"2 pc explain about 55% of the variance","eaab8886":"## pca for 2 PC"}}