{"cell_type":{"fbc3f068":"code","35deacd2":"code","af4be608":"code","e8008e39":"code","086fafd1":"code","50ebc67e":"code","54e3af0c":"code","4870fade":"code","ba572fbd":"code","00aed651":"code","6c2563b8":"code","6f1d8a95":"code","0436f069":"code","e2897788":"code","7554d3b3":"code","6c85106d":"code","9c36d0f6":"code","4feb2b45":"code","7b7b434c":"code","2df74c45":"code","5a7f4259":"code","ac195399":"code","69400c81":"code","d06ebd6a":"code","86a19669":"code","e7c9aea1":"code","44b99a34":"code","7bb86415":"code","01c9bd38":"code","40fbef6d":"code","11c94443":"code","83b317ec":"code","9cb56a04":"code","5306ab3d":"code","0b38b2bb":"code","9a635df9":"code","2b99d82d":"code","5c090933":"code","b2dd7b39":"code","b7ce1299":"code","20ce6221":"code","8ebad13f":"code","f3b29bcf":"code","913f497f":"code","93209efc":"code","52e9d2a8":"code","7eb300ce":"code","0ae3ecef":"code","64f89944":"code","82e1902f":"code","a402b1a5":"code","2e9ee8ef":"code","b89493b8":"code","f1414e37":"code","603e7485":"code","9dbe6f75":"code","7430b665":"code","5105d4f8":"code","b3558c66":"code","805b55f3":"code","fc894925":"code","d5718204":"code","d8a4439e":"code","5556cca2":"code","301f2c13":"code","3b0d78e1":"code","aed7b14d":"code","64861504":"code","22ced0c7":"code","b790541a":"code","f7e03671":"code","0f109f13":"code","aac97006":"code","75048fcd":"code","fd94c901":"code","bc72066a":"code","866da70d":"code","73feefcf":"code","295e5ff5":"code","511c9fbf":"code","9951694d":"code","ae49a1a6":"code","5a618483":"code","dc42785a":"code","44c32f5b":"code","958cb5f9":"code","eeb7bfbe":"code","1523c58c":"code","2b26c6c2":"code","5786a057":"code","36794cfd":"code","41757839":"code","e53b3bf1":"code","6a5a68ae":"code","1f045cd2":"code","5fb52ab7":"code","9383d2f7":"code","48955280":"code","f14fec9c":"code","f4e6c71f":"code","ec5e0309":"code","24c34a68":"code","ed762442":"code","bb35b9cd":"code","f0a286a4":"code","0685fb44":"code","3f5b83d2":"code","4d2148c1":"code","499559ec":"code","5fda5164":"code","c70df510":"code","3e05dad4":"code","7002fde4":"code","d0223717":"code","5c9b9cda":"code","088a7281":"code","538e9c95":"code","deb8ca48":"code","d64e1197":"code","b428ccf7":"code","e25de53c":"code","e32b2cd3":"code","2cf6ce32":"code","af49bf42":"code","28ad8917":"code","6ec53f24":"code","456c5811":"code","567ae39b":"code","6933faac":"code","3695926a":"code","b1a019d2":"code","a2f41610":"code","569d1267":"code","98c45fe0":"code","4f866c28":"code","52437e15":"code","f146d645":"code","1f37136c":"code","1b4cc12d":"code","bf043b7e":"code","134065f6":"code","50013110":"code","23a55ea8":"code","a265f2ef":"code","90678bb3":"code","72866293":"code","61d706dc":"code","50bca887":"code","32b3a095":"code","10007456":"code","8aaf35d9":"code","be82620c":"code","f9112fef":"code","de9c9c68":"code","dea830ec":"code","11b5520c":"code","8d024f97":"code","0e2ff890":"code","bd85da28":"code","12f08698":"code","1ab6fdb3":"code","d73f70aa":"code","7365ae5c":"code","e2b55321":"code","43caef1b":"code","8f468ed8":"code","de017c31":"code","c6328f5c":"code","078751d3":"code","6d406598":"code","70977df7":"code","d1ab39bd":"code","75a26ce3":"code","5a64e95e":"code","48ceb4bf":"code","553bdf39":"code","55e1dfe7":"code","7632b785":"code","a54cd412":"code","db0b31a4":"code","b239da06":"code","e64a1578":"code","a37fc587":"code","c43f3529":"code","e5dbf5be":"code","9ad705b3":"code","a1172807":"code","2a1abb48":"markdown","8b598115":"markdown","71fcc318":"markdown","6d142b2d":"markdown","3a8c0090":"markdown","2be34f26":"markdown","7cde329e":"markdown","fbdfc6f4":"markdown","af25ca22":"markdown","d3263b08":"markdown","c4ffe2ec":"markdown","55ea5526":"markdown","67007a2c":"markdown","d292866a":"markdown","5b1c018c":"markdown","fd795148":"markdown","f8719bbf":"markdown","0b7e462c":"markdown","f9b14d34":"markdown","959c572e":"markdown","543f1954":"markdown","861ef77f":"markdown","0a4f9b99":"markdown","055703ca":"markdown","397f094e":"markdown","e09e9a2b":"markdown","50ec3675":"markdown","08aa282f":"markdown","31bb8a71":"markdown","4b302c68":"markdown","cb2103c8":"markdown","148e1717":"markdown","66b0d587":"markdown","f3acefa9":"markdown","d71f0dd1":"markdown","bf215b2c":"markdown","d5e1990d":"markdown","d1ea3ad4":"markdown","00168d8f":"markdown","73750315":"markdown","012501c9":"markdown","42aa921d":"markdown","258a16a4":"markdown","d7e22fe4":"markdown","c6f27fc8":"markdown","aa5a751e":"markdown","5f6568e9":"markdown","50728392":"markdown","3ff9f4bc":"markdown","c240c3fc":"markdown","c49be7ca":"markdown","eb4f5e1b":"markdown","09937234":"markdown"},"source":{"fbc3f068":"# import the basic libraries we will use in this kernel\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle #importa objetos (hay una celda que ejecuta en dos horas, para no tener que esperar cada vez)\n\nimport time\nimport datetime\nfrom datetime import datetime\nimport calendar\n\nfrom sklearn import metrics\nfrom math import sqrt\nimport gc\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport itertools\nimport warnings\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages","35deacd2":"# Resample the sales by this parameter\nPERIOD = \"M\" #agrupaci\u00f3n mensual\n\nSHOPS = [8, 14, 37, 41, 59] #seleccionamos estas tiendas para no colapsar la memoria\n\n# this is help us change faster between Kaggle and local machine\nLOCAL = False\n\nif LOCAL:   #permite cambiar entre entorno kaggle y local\n    PATH = os.getcwd()\n    FULL_DF_PATH = PATH\n    GB_DF_PATH = PATH\n    OUTPUT_PATH = PATH\nelse:\n    PATH = '..\/input\/competitive-data-science-predict-future-sales\/'\n    FULL_DF_PATH = \"..\/input\/full-df-only-test-all-features\/\"\n    GB_DF_PATH = \"..\/input\/group-by-df\/\"","af4be608":"# prints the local files\ndef print_files():\n    \n    '''\n    Prints the files that are in the current working directory.\n    '''\n    \n    cwd = \"..\/input\/competitive-data-science-predict-future-sales\/\"\n    \n    for f, ff, fff in os.walk(cwd):\n        for file in fff:\n            if file.split(\".\")[1] in [\"pkl\", \"csv\"]:\n                print(file)","e8008e39":"print_files()","086fafd1":"# reduces the memory of a dataframe\ndef reduce_mem_usage(df, verbose = True):\n    \n    '''\n    Reduces the space that a DataFrame occupies in memory.\n\n    This function iterates over all columns in a df and downcasts them to lower type to save memory.\n    '''\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n              .format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))","50ebc67e":"# load all the df we have\nshops_df = pd.read_csv(os.path.join(PATH, \"shops.csv\"))\nitems_df = pd.read_csv(os.path.join(PATH, \"items.csv\"))\nitems_category_df = pd.read_csv(os.path.join(PATH, \"item_categories.csv\"))\nsales_df = pd.read_csv(os.path.join(PATH, \"sales_train.csv\"))\ntest_df = pd.read_csv(os.path.join(PATH, \"test.csv\"))","54e3af0c":"# we have seen in our EDA that we have some duplicate shops, let's correct them.\nshops_df.loc[shops_df.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops_df['city'] = shops_df['shop_name'].str.split(' ').map(lambda x: x[0])\nshops_df.loc[shops_df.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops_df['city_code'] = LabelEncoder().fit_transform(shops_df['city'])\nshops_df.head()","4870fade":"shops_df[shops_df[\"shop_id\"].isin([0, 57])]","ba572fbd":"shops_df[shops_df[\"shop_id\"].isin([1,58])]","00aed651":"shops_df[shops_df[\"shop_id\"].isin([10,11])]","6c2563b8":"#Limpieza de datos manual:\n\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nsales_df.loc[sales_df.shop_id == 0, 'shop_id'] = 57 #train dataset\ntest_df.loc[test_df.shop_id == 0, 'shop_id'] = 57 #test dataset\n\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales_df.loc[sales_df.shop_id == 1, 'shop_id'] = 58\ntest_df.loc[test_df.shop_id == 1, 'shop_id'] = 58\n\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales_df.loc[sales_df.shop_id == 10, 'shop_id'] = 11\ntest_df.loc[test_df.shop_id == 10, 'shop_id'] = 11","6f1d8a95":"#generaci\u00f3n de variables: categor\u00eda de items --> LabelEncoder\n#                         subcategor\u00eda --> LabelEncoder\n\nitems_category_df['split'] = items_category_df['item_category_name'].str.split('-')\nitems_category_df['type'] = items_category_df['split'].map(lambda x: x[0].strip())\nitems_category_df['type_code'] = LabelEncoder().fit_transform(items_category_df['type'])\n\n# if subtype is nan then type\nitems_category_df['subtype'] = items_category_df['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitems_category_df['subtype_code'] = LabelEncoder().fit_transform(items_category_df['subtype'])\n\nitems_category_df.head()","0436f069":"sales_df.head()","e2897788":"# we have negative prices and some outlier\n# let's replace the data with the mean value and also filter all the outliers\nmean = sales_df[(sales_df[\"shop_id\"] == 32) & (sales_df[\"item_id\"] == 2973) & (sales_df[\"date_block_num\"] == 4) & (sales_df[\"item_price\"] > 0)][\"item_price\"].mean()\nsales_df.loc[sales_df.item_price < 0, 'item_price'] = mean\n\nsales_df = sales_df[sales_df[\"item_price\"] < np.percentile(sales_df[\"item_price\"], q = 100)]\nsales_df = sales_df[sales_df[\"item_cnt_day\"] < np.percentile(sales_df[\"item_cnt_day\"], q = 100)]","7554d3b3":"sales_df.info()","6c85106d":"type(sales_df[\"date\"].iloc[0])","9c36d0f6":"# convert to datetime the date column\n# specify the format since otherwise it might give some problems\nsales_df[\"date\"] = pd.to_datetime(sales_df[\"date\"], format = \"%d.%m.%Y\")","4feb2b45":"# max date in sales is 31.10.2015.\n# In the Kaggle competition we are asked to predict the sales for the next month\n# this means the sales of November\nmin_date = sales_df[\"date\"].min()\nmax_date_sales = sales_df[\"date\"].max()","7b7b434c":"max_date_sales","2df74c45":"max_date_test = datetime(2015, 11, 30) #genera nueva fecha m\u00e1xima, sobre la que hacer la predicci\u00f3n","5a7f4259":"#genera un arrange de fechas nuevas: empieza el primer d\u00eda de ventas\ndate_range = pd.date_range(min_date, max_date_test, freq = \"D\")\ndate_range","ac195399":"len(date_range)","69400c81":"shops = sorted(list(shops_df[\"shop_id\"].unique()))\n\n# only items present in test\nitems = sorted(list(items_df[\"item_id\"].unique()))\n\n#creamos un producto cartesiano para eficientar el uso de memoria\ncartesian_product = pd.MultiIndex.from_product([date_range, shops, items], names=[\"date\", \"shop_id\", \"item_id\"])\nlen(cartesian_product) ","d06ebd6a":"date_range = pd.date_range(min_date, max_date_test, freq = \"W\")\ndate_range","86a19669":"len(date_range)","e7c9aea1":"shops = sorted(list(shops_df[\"shop_id\"].unique()))\n\n# only items present in test\nitems = sorted(list(items_df[items_df[\"item_id\"].isin(test_df[\"item_id\"].unique())][\"item_id\"].unique()))\n\ncartesian_product = pd.MultiIndex.from_product([date_range, shops, items], names=[\"date\", \"shop_id\", \"item_id\"])\nlen(cartesian_product)","44b99a34":"date_range = pd.date_range(min_date, max_date_sales, freq = PERIOD)\ndate_range","7bb86415":"len(date_range)","01c9bd38":"# only items present in test\nitems = sorted(list(test_df[\"item_id\"].unique()))\n\n#producto cartesiano genera todas las variedades de fecha, tiendas y id de producto\ncartesian_product = pd.MultiIndex.from_product([date_range, SHOPS, items], names = [\"date\", \"shop_id\", \"item_id\"])\nlen(cartesian_product)","40fbef6d":"len(items)","11c94443":"items_df.shape","83b317ec":"gc.collect()","9cb56a04":"groupby_temporal = sales_df.groupby([\"date_block_num\",\"shop_id\"])","5306ab3d":"groupby_temporal.get_group((0,2))","0b38b2bb":"sales_df[(sales_df[\"date_block_num\"] == 0) & (sales_df[\"shop_id\"] == 2)]","9a635df9":"# st = time.time()\n\n# # set index\n# sales_df[\"revenue\"] = sales_df[\"item_cnt_day\"]*sales_df[\"item_price\"]\n# gb_df = sales_df.set_index(\"date\")\n\n# # groupby shop_id and item_id\n# gb_df = gb_df.groupby([\"shop_id\", \"item_id\"])\n\n# # resample the sales to a weekly basis\n# gb_df = gb_df.resample(PERIOD).agg({'item_cnt_day': np.sum, \"item_price\": np.mean, \"revenue\":np.sum})\n\n# # convert to dataframe and save the full dataframe\n# gb_df.reset_index(inplace = True)\n\n# # save the groupby dataframe\n# gb_df.to_pickle(\"GROUP_BY_DF.pkl\")\n\n# et = time.time()\n\n# print(\"Total time in minutes to preprocess took {}\".format((et - st)\/60))\n","2b99d82d":"# read the groupby dataframe\ngb_df = pd.read_pickle(os.path.join(GB_DF_PATH, \"GROUP_BY_DF.pkl\"))\n# gb_df = pd.read_pickle(\"GROUP_BY_DF.pkl\")","5c090933":"gb_df.head()","b2dd7b39":"gb_df.isnull().sum()","b7ce1299":"gb_df.fillna(0, inplace = True)","20ce6221":"full_df = pd.DataFrame(index = cartesian_product).reset_index()\nfull_df.head()","8ebad13f":"full_df = pd.merge(full_df, gb_df, on = ['date','shop_id', \"item_id\"], how = 'left')\n\nfull_df[\"item_cnt_day\"].sum()","f3b29bcf":"full_df.shape","913f497f":"full_df[\"shop_id\"].value_counts()","93209efc":"full_df.head()","52e9d2a8":"# add shops_df information\nfull_df = pd.merge(full_df, shops_df, on = \"shop_id\")\nfull_df.head()","7eb300ce":"# add items_df information\nfull_df = pd.merge(full_df, items_df, on = \"item_id\")\nfull_df.head()","0ae3ecef":"# add items_category_df information\nfull_df = pd.merge(full_df, items_category_df, on = \"item_category_id\")\nfull_df.head()","64f89944":"full_df.isnull().sum()","82e1902f":"full_df.fillna(0, inplace = True)","a402b1a5":"# We will clip the value in this line.\n# This means that the values greater than 20, will become 20 and lesser than 20\nfull_df[\"item_cnt_day\"] = np.clip(full_df[\"item_cnt_day\"], 0, 20)","2e9ee8ef":"full_df.head().T","b89493b8":"# definici\u00f3n de clase:\nclass Persona:\n    \n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n        print(\"Hemos llegado hasta aqu\u00ed para ver que el __init__ siempre se ejecuta\")\n        \n    def presentar(self, tipo_presentaci\u00f3n):\n        if tipo_presentaci\u00f3n == \"Formal\":\n            return \"Hola, mi nombre es {} y tengo {} a\u00f1os\".format(self.name, self.age)\n        elif tipo_presentaci\u00f3n == \"Informal\":\n            return \"Qu\u00e9 pasa t\u00edo?\"","f1414e37":"Carlos = Persona(\"Carlos\", 25)","603e7485":"Carlos.presentar(tipo_presentaci\u00f3n = \"Formal\")","9dbe6f75":"class FeatureGenerator(object):\n    \n    '''\n    This is a helper class that takes a df and a list of features and creates sum, mean, \n    lag features and variation (change over month) features.\n    \n    '''\n    \n    def __init__(self, full_df,  gb_list):\n        \n        '''\n        Constructor of the class.\n        gb_list is a list of columns that must be in full_df.\n        '''\n        \n        self.full_df = full_df\n        self.gb_list = gb_list\n        # joins the gb_list, this way we can dinamically create new columns\n        # [\"date, \"shop_id] --> date_shop_id\n        self.objective_column_name = \"_\".join(gb_list)\n\n    @staticmethod\n    def reduce_mem_usage(df, verbose = True):\n        \n        '''\n        Reduces the space that a DataFrame occupies in memory.\n        This is a static method of FeatureGenerator class (we can use it outside the class).\n        \n        This function iterates over all columns in a df and downcasts them to lower type to save memory.\n        '''\n        \n        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n        start_mem = df.memory_usage().sum() \/ 1024**2    \n        for col in df.columns:\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n        end_mem = df.memory_usage().sum() \/ 1024**2\n        if verbose:\n            print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n                  .format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n            \n    def generate_gb_df(self):\n        \n        '''\n        This function thakes the full_df and creates a groupby df based on the gb_list.\n        It creates 2 columns: \n            1. A sum column for every date and gb_list\n            2. Mean columns for every_date and gb_list\n            \n        The resulting df (gb_df_) is assigned back to the FeatureGenerator class as an attribute.\n        '''\n\n        def my_agg(full_df_, args):\n            \n            '''\n            This function is used to perform multiple operations over a groupby df and returns a df\n            without multiindex.\n            '''\n            \n            names = {\n                '{}_sum'.format(args):  full_df_['item_cnt_day'].sum(),\n                '{}_mean'.format(args): full_df_['item_cnt_day'].mean()\n            }\n\n            return pd.Series(names, index = [key for key in names.keys()])\n        \n        # the args is used to pass additional argument to the apply function\n        gb_df_ = self.full_df.groupby(self.gb_list).apply(my_agg, args = (self.objective_column_name)).reset_index()\n\n        self.gb_df_ = gb_df_\n\n        \n    def return_gb_df(self):  \n        \n        '''\n        This function takes the gb_df_ created in the previous step (generate_gb_df) and creates additional features.\n        We create 4 lag features (values from the past).\n        And 6 variation features: 3 with absolute values and 3 with porcentual change.\n        '''\n        \n        def generate_shift_features(self, suffix):\n            \n            '''\n            This function is a helper function that takes the gb_df_ and a suffix (sum or mean) and creates the\n            additional features.\n            '''\n\n            # dinamically creates the features\n            # date_shop_id --> date_shop_id_sum if suffix is sum\n            # date_shop_id --> date_shop_id_mean if suffix is mean\n            name_ = self.objective_column_name + \"_\" + suffix\n\n            self.gb_df_['{}_shift_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1))\n            \n            self.gb_df_['{}_shift_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(2))\n            \n            self.gb_df_['{}_shift_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(3))\n            \n            self.gb_df_['{}_shift_6'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(6))\n            \n            # notice taht in var_3 we use shift(4), we do this because we want to capture the variation of 3 months\n            # and not the variation of month - 3\n\n            self.gb_df_['{}_var_1'.format(name_)] = self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1) - x.shift(2))\n            self.gb_df_['{}_var_2'.format(name_)] = self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1) - x.shift(3))\n            self.gb_df_['{}_var_3'.format(name_)] = self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1) - x.shift(4))\n\n            self.gb_df_['{}_var_pct_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(2))\/x.shift(2))\n            \n            self.gb_df_['{}_var_pct_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(3))\/x.shift(3))\n            \n            self.gb_df_['{}_var_pct_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(4))\/x.shift(4))\n            \n            self.gb_df_.fillna(-1, inplace = True)\n\n            self.gb_df_.replace([np.inf, -np.inf], -1, inplace = True)\n        \n        # call the generate_shift_featues function with different suffix (sum and mean)\n        generate_shift_features(self, suffix = \"sum\")\n        generate_shift_features(self, suffix = \"mean\")\n        \n        FeatureGenerator.reduce_mem_usage(self.gb_df_)\n    \n        return self.gb_df_\n        ","7430b665":"st = time.time()\n\ngb_list = [\"date\", \"shop_id\"]\n\nfe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nshop_sales_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)\/60","5105d4f8":"shop_sales_features.shape","b3558c66":"shop_sales_features[shop_sales_features[\"shop_id\"] == 8].T","805b55f3":"st = time.time()\n\ngb_list = [\"date\", \"item_id\"]\n\nfe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nitem_sales_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)\/60","fc894925":"item_sales_features.shape","d5718204":"item_sales_features[item_sales_features[\"item_id\"]== 30].head(20).T","d8a4439e":"item_sales_features","5556cca2":"item_sales_features[item_sales_features[\"item_id\"] == 30].head(20).T","301f2c13":"st = time.time()\n\ngb_list = [\"date\", \"item_category_id\"]\n\nfe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nmonth_item_category_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)\/60","3b0d78e1":"month_item_category_features.shape","aed7b14d":"month_item_category_features[month_item_category_features[\"item_category_id\"] == 2].T","64861504":"st = time.time()\n\ngb_list = [\"date\", \"type_code\"]\n\nfe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nmonth_type_code_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)\/60","22ced0c7":"month_type_code_features.shape","b790541a":"month_type_code_features[month_type_code_features[\"type_code\"] == 1].T","f7e03671":"full_df[\"year\"] = full_df[\"date\"].dt.year\nfull_df[\"month\"] = full_df[\"date\"].dt.month\nfull_df[\"days_in_month\"] = full_df[\"date\"].dt.days_in_month\nfull_df[\"quarter_start\"] = full_df[\"date\"].dt.is_quarter_start\nfull_df[\"quarter_end\"] = full_df[\"date\"].dt.is_quarter_end","0f109f13":"full_df.head().T","aac97006":"holidays_next_month = {\n    12:8,\n    1:1,\n    2:1,\n    3:0,\n    4:2,\n    5:1,\n    6:0,\n    7:0,\n    8:0,\n    9:0,\n    10:1,\n    11:0\n}\n\nholidays_this_month = {\n    1:8,\n    2:1,\n    3:1,\n    4:0,\n    5:2,\n    6:1,\n    7:0,\n    8:0,\n    9:0,\n    10:0,\n    11:1,\n    12:0\n}\n\nfull_df[\"holidays_next_month\"] = full_df[\"month\"].map(holidays_next_month)\nfull_df[\"holidays_this_month\"] = full_df[\"month\"].map(holidays_this_month)","75048fcd":"def extract_number_weekends(test_month):\n    saturdays = len([1 for i in calendar.monthcalendar(test_month.year, test_month.month) if i[5] != 0])\n    sundays = len([1 for i in calendar.monthcalendar(test_month.year, test_month.month) if i[6] != 0])\n    return saturdays + sundays\n\nfull_df[\"total_weekend_days\"] = full_df[\"date\"].apply(extract_number_weekends)\n\ndate_diff_df = full_df[full_df[\"item_cnt_day\"] > 0][[\"shop_id\", \"item_id\", \"date\", \"item_cnt_day\"]].groupby([\"shop_id\", \"item_id\"])\\\n[\"date\"].diff().apply(lambda timedelta_: timedelta_.days).to_frame()\n\ndate_diff_df.columns = [\"date_diff_sales\"]\n\nfull_df = pd.merge(full_df, date_diff_df, how = \"left\", left_index=True, right_index=True)\n\nfull_df.fillna(-1, inplace = True)","fd94c901":"full_df.head().T","bc72066a":"city_population = {\\\n'\u042f\u043a\u0443\u0442\u0441\u043a':307911, \n'\u0410\u0434\u044b\u0433\u0435\u044f':141970,\n'\u0411\u0430\u043b\u0430\u0448\u0438\u0445\u0430':450771, \n'\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439':326055, \n'\u0412\u043e\u043b\u043e\u0433\u0434\u0430':313012, \n'\u0412\u043e\u0440\u043e\u043d\u0435\u0436':1047549,\n'\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f':1228680, \n'\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439':107560, \n'\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d':1228680, \n'\u041a\u0430\u0437\u0430\u043d\u044c':1257391, \n'\u041a\u0430\u043b\u0443\u0433\u0430':341892,\n'\u041a\u043e\u043b\u043e\u043c\u043d\u0430':140129,\n'\u041a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a':1083865, \n'\u041a\u0443\u0440\u0441\u043a':452976, \n'\u041c\u043e\u0441\u043a\u0432\u0430':12678079,\n'\u041c\u044b\u0442\u0438\u0449\u0438':205397, \n'\u041d.\u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434':1252236,\n'\u041d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a':1602915 , \n'\u041e\u043c\u0441\u043a':1178391, \n'\u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443':1125299, \n'\u0421\u041f\u0431':5398064, \n'\u0421\u0430\u043c\u0430\u0440\u0430':1156659,\n'\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434':104579, \n'\u0421\u0443\u0440\u0433\u0443\u0442':373940, \n'\u0422\u043e\u043c\u0441\u043a':572740, \n'\u0422\u044e\u043c\u0435\u043d\u044c':744554, \n'\u0423\u0444\u0430':1115560, \n'\u0425\u0438\u043c\u043a\u0438':244668,\n'\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439':1228680, \n'\u0427\u0435\u0445\u043e\u0432':70548, \n'\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c':608353\n}\n\ncity_income = {\\\n'\u042f\u043a\u0443\u0442\u0441\u043a':70969, \n'\u0410\u0434\u044b\u0433\u0435\u044f':28842,\n'\u0411\u0430\u043b\u0430\u0448\u0438\u0445\u0430':54122, \n'\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439':31666, \n'\u0412\u043e\u043b\u043e\u0433\u0434\u0430':38201, \n'\u0412\u043e\u0440\u043e\u043d\u0435\u0436':32504,\n'\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f':46158, \n'\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439':54122, \n'\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d':46158, \n'\u041a\u0430\u0437\u0430\u043d\u044c':36139, \n'\u041a\u0430\u043b\u0443\u0433\u0430':39776,\n'\u041a\u043e\u043b\u043e\u043c\u043d\u0430':54122,\n'\u041a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a':48831, \n'\u041a\u0443\u0440\u0441\u043a':31391, \n'\u041c\u043e\u0441\u043a\u0432\u0430':91368,\n'\u041c\u044b\u0442\u0438\u0449\u0438':54122, \n'\u041d.\u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434':31210,\n'\u041d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a':37014 , \n'\u041e\u043c\u0441\u043a':34294, \n'\u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443':32067, \n'\u0421\u041f\u0431':61536, \n'\u0421\u0430\u043c\u0430\u0440\u0430':35218,\n'\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434':54122, \n'\u0421\u0443\u0440\u0433\u0443\u0442':73780, \n'\u0422\u043e\u043c\u0441\u043a':43235, \n'\u0422\u044e\u043c\u0435\u043d\u044c':72227, \n'\u0423\u0444\u0430':35257, \n'\u0425\u0438\u043c\u043a\u0438':54122,\n'\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439':46158, \n'\u0427\u0435\u0445\u043e\u0432':54122, \n'\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c':34675\n}\n\nfull_df[\"city_population\"] = full_df[\"city\"].map(city_population)\n\nfull_df[\"city_income\"] = full_df[\"city\"].map(city_income)\n\nfull_df[\"price_over_income\"] = full_df[\"item_price\"]\/full_df[\"city_income\"]","866da70d":"full_df.head().T","73feefcf":"full_df.shape","295e5ff5":"full_df = pd.merge(full_df, shop_sales_features, on = [\"date\", \"shop_id\"], how = \"left\")\n\n\nfull_df = pd.merge(full_df, item_sales_features, on = [\"date\", \"item_id\"], how = \"left\")\n\n\nfull_df = pd.merge(full_df, month_item_category_features, on = [\"date\", \"item_category_id\"], how = \"left\")\n\n\nfull_df = pd.merge(full_df, month_type_code_features, on = [\"date\", \"type_code\"], how = \"left\")","511c9fbf":"full_df.shape","9951694d":"columnas = list(full_df.columns)\ncolumnas","ae49a1a6":"# delete dfs with features\ndel shop_sales_features, item_sales_features, month_item_category_features, month_type_code_features","5a618483":"# delete all the previous df\ndel shops_df, items_df, items_category_df, sales_df, test_df, cartesian_product, gb_df\ngc.collect()","dc42785a":"full_df.rename(columns = {\"item_cnt_day\":\"sales\"}, inplace = True)","44c32f5b":"st = time.time()\n\nfull_df.to_pickle(\"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\")\n\net = time.time()\n\n(et - st)\/60","958cb5f9":"# full_df = pd.read_pickle(os.path.join(FULL_DF_PATH, \"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\"))\nfull_df = pd.read_pickle(\"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\")","eeb7bfbe":"full_df = full_df[full_df[\"shop_id\"].isin(SHOPS)]","1523c58c":"full_df.head()","2b26c6c2":"# delete all the columns where lags features are - 1 (shift(6))\nfull_df = full_df[full_df[\"date\"] > np.datetime64(\"2013-06-30\")]","5786a057":"cols_to_drop = [\n\n'revenue', #lo hemos calculado sobre el mes anterior, ya tenemos variables que hacen referencia a mes anterior, para la predicci\u00f3n estar\u00edamos dando el valor de ingreso del mes en curso, no tiene sentido\n'shop_name',\n'city',\n'item_name',\n'item_category_name',\n'split',\n'type',\n'subtype',\n    \n'date_item_category_id_sum',#eliminamos variables que contengan info del mes en curso\n'date_item_category_id_mean',\n\n'date_type_code_sum',\n'date_type_code_mean'\n    \n]","36794cfd":"full_df.drop(cols_to_drop, inplace = True, axis = 1)","41757839":"# split the data into train, validation and test dataset\ntrain_index = sorted(list(full_df[\"date\"].unique()))[:-2] #train con todos los meses excepto los dos \u00faltimos\n\nvalida_index = [sorted(list(full_df[\"date\"].unique()))[-2]] #septiembre\n\ntest_index = [sorted(list(full_df[\"date\"].unique()))[-1]] #octubre:predecir para evaluar","e53b3bf1":"X_train = full_df[full_df[\"date\"].isin(train_index)].drop(['sales', \"date\"], axis=1)\nY_train = full_df[full_df[\"date\"].isin(train_index)]['sales']\n\nX_valida = full_df[full_df[\"date\"].isin(valida_index)].drop(['sales', \"date\"], axis=1)\nY_valida = full_df[full_df[\"date\"].isin(valida_index)]['sales']\n\nX_test = full_df[full_df[\"date\"].isin(test_index)].drop(['sales', \"date\"], axis = 1)\nY_test = full_df[full_df[\"date\"].isin(test_index)]['sales']","6a5a68ae":"gc.collect()","1f045cd2":"st = time.time()\n\nmodel = XGBRegressor(seed = 175)\n\nmodel_name = str(model).split(\"(\")[0]\n\nday = str(datetime.now()).split()[0].replace(\"-\", \"_\")\nhour = str(datetime.now()).split()[1].replace(\":\", \"_\").split(\".\")[0]\nt = str(day) + \"_\" + str(hour)\n\nmodel.fit(X_train, Y_train, eval_metric = \"rmse\", \n    eval_set = [(X_train, Y_train), (X_valida, Y_valida)], \n    verbose = True, \n    early_stopping_rounds = 10)\n\net = time.time()\n\nprint(\"Training took {} minutes!\".format((et - st)\/60))","5fb52ab7":"pickle.dump(model, open(\"{}_{}.dat\".format(model_name, t), \"wb\")) #guarda en local","9383d2f7":"print(\"{}_{}.dat\".format(model_name, t))","48955280":"model = pickle.load(open(\"{}_{}.dat\".format(model_name, t), \"rb\")) #recupera\/carga desde local","f14fec9c":"importance = model.get_booster().get_score(importance_type = \"gain\")\n\nimportance = {k: v for k, v in sorted(importance.items(), key = lambda item: item[1])}","f4e6c71f":"fig, ax = plt.subplots(figsize=(15, 30))\nplot_importance(model, importance_type = \"weight\", ax = ax)\nplt.savefig(\"{}_{}_plot_importance.png\".format(model_name, t))","ec5e0309":"Y_valida_pred = model.predict(X_valida)","24c34a68":"metrics.r2_score(Y_valida, Y_valida_pred)","ed762442":"rmse_valida = sqrt(metrics.mean_squared_error(Y_valida, Y_valida_pred))\nrmse_valida","bb35b9cd":"Y_test_predict = model.predict(X_test)","f0a286a4":"Y_test_predict.sum()","0685fb44":"Y_test_predict.max()","3f5b83d2":"Y_test.head()","4d2148c1":"Y_test.sum()","499559ec":"Y_test.max()","5fda5164":"rmse_test = sqrt(metrics.mean_squared_error(Y_test, Y_test_predict))\nrmse_test","c70df510":"perfect_rmse = sqrt(metrics.mean_squared_error(Y_test, Y_test))\nperfect_rmse","3e05dad4":"full_df1 = pd.read_pickle(\"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\")","7002fde4":"full_df1 = full_df1[full_df1[\"shop_id\"].isin(SHOPS)]","d0223717":"full_df1[\"shop_id\"].unique()","5c9b9cda":"full_df1[\"city_code\"].unique()","088a7281":"del full_df","538e9c95":"full_df1.rename(columns = {\"sales\":\"item_cnt_day\"}, inplace = True)","deb8ca48":"columnas = list(full_df1.columns)\ncolumnas","d64e1197":"st = time.time()\n\ngb_list = [\"shop_id\", \"item_id\", \"revenue\"]\n\nfe_generator = FeatureGenerator(full_df = full_df1, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nshop_id_item_id_revenue_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)\/60","b428ccf7":"shop_id_item_id_revenue_features[shop_id_item_id_revenue_features[\"shop_id\"]== 8].T","e25de53c":"shop_id_item_id_revenue_features.shape","e32b2cd3":"st = time.time()\n\ngb_list = [\"shop_id\", \"item_id\", \"item_price\"]\n\nfe_generator = FeatureGenerator(full_df = full_df1, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nshop_id_item_id_price_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)\/60","2cf6ce32":"shop_id_item_id_price_features[shop_id_item_id_price_features[\"item_id\"] == 30 ].T","af49bf42":"shop_id_item_id_price_features.shape","28ad8917":"st = time.time()\n\ngb_list = [\"price_over_income\", \"revenue\", \"city_code\"]\n\nfe_generator = FeatureGenerator(full_df = full_df1, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nprice_over_income_revenue_city_code = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)\/60","6ec53f24":"price_over_income_revenue_city_code[\"city_code\"].unique()","456c5811":"price_over_income_revenue_city_shape.shape","567ae39b":"st = time.time()\n\ngb_list = [\"item_id\", \"price_over_income\"]\n\nfe_generator = FeatureGenerator(full_df = full_df1, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nitem_id_price_over_income = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)\/60","6933faac":"item_id_price_over_income[item_id_price_over_income[\"item_id\"] == 30].T","3695926a":"item_id_price_over_income.shape","b1a019d2":"monthly_sales = full_df1[['date', 'shop_id','item_id', 'revenue']]\nmonthly_sales.head()","a2f41610":"reduce_mem_usage(monthly_sales, verbose = True)","569d1267":"monthly_sales.head()","98c45fe0":"monthly_sales.info(verbose=True)","4f866c28":"monthly_sales[\"MA1Q\"] = monthly_sales[\"revenue\"].rolling(window=3).mean()\nmonthly_sales[\"MA2Q\"] = monthly_sales[\"revenue\"].rolling(window=6).mean()\nmonthly_sales[\"MA3Q\"] = monthly_sales[\"revenue\"].rolling(window=9).mean()\nmonthly_sales[\"MA1Y\"] = monthly_sales[\"revenue\"].rolling(window=12).mean()","52437e15":"#shift\nmonthly_sales[\"MA1Q\"] = monthly_sales[\"MA1Q\"].shift(1)\nmonthly_sales[\"MA2Q\"] = monthly_sales[\"MA2Q\"].shift(1)\nmonthly_sales[\"MA3Q\"] = monthly_sales[\"MA3Q\"].shift(1)\nmonthly_sales[\"MA1Y\"] = monthly_sales[\"MA1Y\"].shift(1)","f146d645":"monthly_sales.head(20)","1f37136c":"monthly_sales.fillna(0, inplace=True)","1b4cc12d":"monthly_sales.drop(['revenue'], axis=1, inplace=True)","bf043b7e":"full_df1.shape","134065f6":"shop_id_item_id_revenue_features.shape","50013110":"full_df1 = pd.merge(full_df1, shop_id_item_id_revenue_features, on = [\"shop_id\", \"item_id\", \"revenue\"], how = \"left\")\n","23a55ea8":"full_df1.shape","a265f2ef":"full_df1 = pd.merge(full_df1, shop_id_item_id_price_features, on = [\"shop_id\", \"item_id\", \"item_price\"], how = \"left\")\n","90678bb3":"full_df1.shape","72866293":"full_df1 = pd.merge(full_df1, price_over_income_revenue_city_code, on = [\"price_over_income\", \"revenue\", \"city_code\"], how = \"left\")","61d706dc":"full_df1.shape","50bca887":"full_df1 = pd.merge(full_df1, item_id_price_over_income, on= [\"item_id\", \"price_over_income\"], how = \"left\")","32b3a095":"full_df1.shape","10007456":"full_df1 = pd.merge(full_df1, monthly_sales, on = [\"shop_id\", \"item_id\", \"date\"], how = \"left\")","8aaf35d9":"full_df1.shape","be82620c":"# delete dfs with features\ndel shop_id_item_id_revenue_features, shop_id_item_id_price_features, price_over_income_revenue_city_code, item_id_price_over_income, monthly_sales\ngc.collect()","f9112fef":"full_df1.rename(columns = {\"item_cnt_day\":\"sales\"}, inplace = True)","de9c9c68":"full_df1\n","dea830ec":"st = time.time()\n\nfull_df1.to_pickle(\"FULL_DF1_ONLY_TEST_ALL_FEATURES.pkl\")\n\net = time.time()\n\n(et - st)\/60","11b5520c":"full_df1 = pd.read_pickle(\"FULL_DF1_ONLY_TEST_ALL_FEATURES.pkl\")","8d024f97":"full_df1 = full_df1[full_df1[\"shop_id\"].isin(SHOPS)]","0e2ff890":"full_df1 = full_df1[full_df1[\"date\"] > np.datetime64(\"2013-06-30\")]","bd85da28":"full_df1","12f08698":"columnas = list(full_df1.columns)\ncolumnas","1ab6fdb3":"cols_to_drop = [\n\n'revenue',\n'shop_name',\n\"city\",\n'item_name',\n'item_category_name',\n'split',\n'type',\n'subtype',\n    \n'date_shop_id_sum',\n'date_shop_id_mean',\n\n\"date_item_id_sum\",\n\"date_item_id_mean\",\n\n\"date_item_category_id_sum\",\n\"date_item_category_id_mean\",\n\n\"date_type_code_sum\",\n\"date_type_code_mean\",\n\n'shop_id_item_id_revenue_sum',\n'shop_id_item_id_revenue_mean',\n    \n'shop_id_item_id_item_price_sum',\n'shop_id_item_id_item_price_mean',\n    \n'price_over_income_revenue_city_code_sum',\n'price_over_income_revenue_city_code_mean',\n    \n'item_id_price_over_income_sum',\n'item_id_price_over_income_mean',\n    \n# 'MA1Q',\n# 'MA2Q',\n# 'MA3Q',\n# 'MA1Y'\n    \n]","d73f70aa":"full_df1.drop(cols_to_drop, inplace = True, axis = 1)","7365ae5c":"list(full_df1.columns)","e2b55321":"train_index = sorted(list(full_df1[\"date\"].unique()))[:-2]\n\nvalida_index = [sorted(list(full_df1[\"date\"].unique()))[-2]]\n\ntest_index = [sorted(list(full_df1[\"date\"].unique()))[-1]]","43caef1b":"X_train = full_df1[full_df1[\"date\"].isin(train_index)].drop(['sales', \"date\"], axis=1) \nY_train = full_df1[full_df1[\"date\"].isin(train_index)]['sales']\n\nX_valida = full_df1[full_df1[\"date\"].isin(valida_index)].drop(['sales', \"date\"], axis=1)\nY_valida = full_df1[full_df1[\"date\"].isin(valida_index)]['sales']\n\nX_test = full_df1[full_df1[\"date\"].isin(test_index)].drop(['sales', \"date\"], axis = 1)\nY_test = full_df1[full_df1[\"date\"].isin(test_index)]['sales']","8f468ed8":"st = time.time()\n\nmodel = XGBRegressor(seed = 175)\n\nmodel_name = str(model).split(\"(\")[0]\n\nday = str(datetime.now()).split()[0].replace(\"-\", \"_\")\nhour = str(datetime.now()).split()[1].replace(\":\", \"_\").split(\".\")[0]\nt = str(day) + \"_\" + str(hour)\n\nmodel.fit(X_train, Y_train, eval_metric = \"rmse\", \n    eval_set = [(X_train, Y_train), (X_valida, Y_valida)], \n    verbose = True, \n    early_stopping_rounds = 10)\n\net = time.time()\n\nprint(\"Training took {} minutes!\".format((et - st)\/60))","de017c31":"pickle.dump(model, open(\"{}_{}.dat\".format(model_name, t), \"wb\"))","c6328f5c":"print(\"{}_{}.dat\".format(model_name, t))","078751d3":"model = pickle.load(open(\"{}_{}.dat\".format(model_name, t), \"rb\"))","6d406598":"importance = model.get_booster().get_score(importance_type = \"gain\")\n\nimportance = {k: v for k, v in sorted(importance.items(), key = lambda item: item[1])}","70977df7":"print(model.feature_importances_)","d1ab39bd":"fig, ax = plt.subplots(figsize=(15, 30))\nplot_importance(model, importance_type = \"gain\", ax = ax)\nplt.savefig(\"{}_{}_plot_importance.png\".format(model_name, t))","75a26ce3":"Y_valida_pred = model.predict(X_valida)","5a64e95e":"metrics.r2_score(Y_valida, Y_valida_pred)","48ceb4bf":"rmse_valida = sqrt(metrics.mean_squared_error(Y_valida, Y_valida_pred))\nrmse_valida","553bdf39":"Y_test_predict = model.predict(X_test)","55e1dfe7":"Y_test_predict.sum()","7632b785":"Y_test_predict.max()","a54cd412":"Y_test.head()","db0b31a4":"Y_test.sum()","b239da06":"Y_test.max()","e64a1578":"rmse_test = sqrt(metrics.mean_squared_error(Y_test, Y_test_predict))\nrmse_test","a37fc587":"perfect_rmse = sqrt(metrics.mean_squared_error(Y_test, Y_test))\nperfect_rmse","c43f3529":"full_df1.drop(\"price_over_income\", axis=1, inplace=True)","e5dbf5be":"train_index = sorted(list(full_df1[\"date\"].unique()))[:-2]\n\nvalida_index = [sorted(list(full_df1[\"date\"].unique()))[-2]]\n\ntest_index = [sorted(list(full_df1[\"date\"].unique()))[-1]]","9ad705b3":"X_train = full_df1[full_df1[\"date\"].isin(train_index)].drop(['sales', \"date\"], axis=1) \nY_train = full_df1[full_df1[\"date\"].isin(train_index)]['sales']\n\nX_valida = full_df1[full_df1[\"date\"].isin(valida_index)].drop(['sales', \"date\"], axis=1)\nY_valida = full_df1[full_df1[\"date\"].isin(valida_index)]['sales']\n\nX_test = full_df1[full_df1[\"date\"].isin(test_index)].drop(['sales', \"date\"], axis = 1)\nY_test = full_df1[full_df1[\"date\"].isin(test_index)]['sales']","a1172807":"st = time.time()\n\nmodel = XGBRegressor(seed = 175)\n\nmodel_name = str(model).split(\"(\")[0]\n\nday = str(datetime.now()).split()[0].replace(\"-\", \"_\")\nhour = str(datetime.now()).split()[1].replace(\":\", \"_\").split(\".\")[0]\nt = str(day) + \"_\" + str(hour)\n\nmodel.fit(X_train, Y_train, eval_metric = \"rmse\", \n    eval_set = [(X_train, Y_train), (X_valida, Y_valida)], \n    verbose = True, \n    early_stopping_rounds = 10)\n\net = time.time()\n\nprint(\"Training took {} minutes!\".format((et - st)\/60))","2a1abb48":"The idea of this section is very simple. We have seen in our EDA part that there are a lot of missing values.\nOur model will benefit a lot if we can supply it a training data, with the missing values being zero. This way, it can learn from more amount of data.\n\nIn order to do so, we must perform a cartesian operation over dates x shops x items_id to generate all the possible combinations of months x shops and x items sales.\n\nIn this kernel we will only generate this type of features for the items that are present in TEST only.\n\nThis will reduce the amount of calculations required. If you have enough memory, we can do this for all possible combinations.","8b598115":"Podemos generar nuevas variables:\n    - D\u00eda de la semana\n    - LabelEncoder \n    - Medias m\u00f3viles ventas por producto y ciudad\n    - Medias m\u00f3viles por d\u00eda y semana\n    - Medias m\u00f3viles por tienda y ciudad\n    - ....","71fcc318":"<a id = \"new_feature_1b\"><\/a>\n# Additional feature 1b: shop_id, item_id & item_price feature\n[Go back to the table of contents](#table_of_contents)","6d142b2d":"<a id = \"imports\"><\/a>\n# Import of libraries\n[Go back to the table of contents](#table_of_contents)","3a8c0090":"<a id = \"remove_outliers\"><\/a>\n## Remove the huge price and item sales outliers\n[Go back to the table of contents](#table_of_contents)","2be34f26":"<a id = \"generate_full_df_with_all_records\"><\/a>\n# Generate a full df with all data and records\n[Go back to the table of contents](#table_of_contents)","7cde329e":"<a id = \"preprocessing_before_fe\"><\/a>\n# Preprocessing before features generation\n[Go back to the table of contents](#table_of_contents)","fbdfc6f4":"<a id = \"bonus_track\"><\/a>\n# Bonus track: Moving averages of monthly sales for each quarter backwards (1 year far)\n[Go back to the table of contents](#table_of_contents)","af25ca22":"<a id = \"join_dfs\"><\/a>\n# Join the full_df with gb_df\n[Go back to the table of contents](#table_of_contents)","d3263b08":"Now that we have the sales_df resampled by months, and we have created a cartesian product (all possible combinations of months, shop_id and item_id), let's merge the df.","c4ffe2ec":"<a id = \"global_variables\"><\/a>\n# Global variables\n[Go back to the table of contents](#table_of_contents)","55ea5526":"Initially, we recover the full dataframe from the saved pickle before deleting all the variables related to the current month and all non numeric values as to generate three new atributes. We will also delete the previous full_df with the aim of saving memory.","67007a2c":"<a id = \"new_model\"><\/a>\n# Model training\n[Go back to the table of contents](#table_of_contents)","d292866a":"Our model will benefit a lot if we can train it with the highest granularity (daily sales).\n\nHowever, as we can see doing this on a local machine is almost impossible since we have more than 1.4 BILLION rows.\nIf we add 10 featrues (columns) this means that our total DataFrame will have more than 10.4 BILLIONS instances.","5b1c018c":"In order to replicate the Kaggle competition, we will create a smaller DataFrame with only selected shops and train the model on a Monthly basis.\n\n\nWe will use only 5 shops since generating a lot of features will consume a lot of memory and we won't be able to train on Kaggle. If you have a more powerful machine, you can run the script with all shops.","fd795148":"<a id = \"feature_7\"><\/a>\n## City population and mean_income per city\n[Go back to the table of contents](#table_of_contents)","f8719bbf":"<a id = \"feature_1\"><\/a>\n## Date and shop_id features\n[Go back to the table of contents](#table_of_contents)","0b7e462c":"<a id = \"join_dfs_with_features\"><\/a>\n# Join full sales df with all the features generated\n[Go back to the table of contents](#table_of_contents)","f9b14d34":"If we try to build our model with all the shops and item_id on a weekly basis we have a total of 46 million rows.\nThis makes the modeling part on a local machine very difficult.","959c572e":"Elimina variables originales, trabaja sobre atributos secundarios de variaciones calculados a partir de variables iniciales.","543f1954":"La celda anterior indica que el n\u00famero m\u00e1ximo de productos que se van a vender en la predicci\u00f3n ser\u00e1n 20, lo pone en el enunciado de la prueba.","861ef77f":"<a id = \"feature_5\"><\/a>\n## Datetime features\n[Go back to the table of contents](#table_of_contents)","0a4f9b99":"<a id = \"to_do\"><\/a>\n# To do\n[Go back to the table of contents](#table_of_contents)","055703ca":"Might also be interesting to check wether the differences in the price of the items sold and and number of items sold help to make a better prediction.","397f094e":"0.87 million rows, we CAN work with this on a local machine.\n\nWe have created monthly date_range, if we want to join this with our sales data, we must \"resample\" our data to a monthly date_range aswell.","e09e9a2b":"<a id = \"feature_importance_2\"><\/a>\n# Feature importance of new model\n[Go back to the table of contents](#table_of_contents)","50ec3675":"<a id = \"add_new_csvs\"><\/a>\n# Add additional features to our full sales df\n[Go back to the table of contents](#table_of_contents)","08aa282f":"<a id = \"new_feature_3\"><\/a>\n# Additional feature 3: item_id & price_over_income\n[Go back to the table of contents](#table_of_contents)","31bb8a71":"<a id = \"generate_item_category_features\"><\/a>\n## Generate item_category_features\n[Go back to the table of contents](#table_of_contents)","4b302c68":"<a id = \"generate_gb_df\"><\/a>\n# Create a groupby df with all the sales for shop_id and item_id grouped by months\n[Go back to the table of contents](#table_of_contents)","cb2103c8":"# Run 2: GO","148e1717":"<a id = \"feature_4\"><\/a>\n## Date and type_code features\n[Go back to the table of contents](#table_of_contents)","66b0d587":"<a id = \"feature_importance_1\"><\/a>\n# Feature importance\n[Go back to the table of contents](#table_of_contents)","f3acefa9":"As we can see, we have some duplicate shop names, let's manually clean them.","d71f0dd1":"Groupby genera dataframes m\u00e1s peque\u00f1os que permite realizar filtros. Adem\u00e1s, se aplican operaciones sobre esos filtros mediante aggfunc.","bf215b2c":"<a id = \"feature_3\"><\/a>\n## Date and item_category features\n[Go back to the table of contents](#table_of_contents)","d5e1990d":"\n\n<a id = \"new_feature_2\"><\/a>\n# Additional feature 2: Price_over_income, revenue & city_code feature\n[Go back to the table of contents](#table_of_contents)","d1ea3ad4":"<a id = \"table_of_contents\"><\/a>\n# Table of contents\n\n[Import of libraries](#imports)\n\n[Global variables](#global_variables)\n\n[Helper functions](#helper_functions)\n\n[Preprocessing before features generation](#preprocessing_before_fe)\n\n-->[Correct the shop names and id](#correct_shop_names_id)\n\n-->[Generate item_category_features](#generate_item_category_features)\n\n-->[Remove the huge price and item sales outliers](#remove_outliers)\n\n[Generate a full df with all data and records](#generate_full_df_with_all_records)\n\n[Create a groupby df with all the sales for shop_id and item_id grouped by months](#generate_gb_df)\n\n[Join the full_df with gb_df](#join_dfs)\n\n[Add additional features to our full sales df](#add_new_csvs)\n\n[FeatureGenerator class](#fe_generator_class)\n\n[Generate additional features as, mean and total sales for shop_id , item_id, city ... for every month](#create_new_features)\n\n-->[Date and shop_id features](#feature_1)\n\n-->[Date and item_id features](#feature_2)\n\n-->[Date and item_category features](#feature_3)\n\n-->[Date and type_code features](#feature_4)\n\n-->[Datetime features](#feature_5)\n\n-->[Adding holiday and number of weekends data](#feature_6)\n\n-->[City population and mean_income per city](#feature_7)\n\n[Join full sales df with all the features generated](#join_dfs_with_features)\n\n[Basic model train](#basic_model)\n\n[Feature importance](#feature_importance_1)\n\n[Predict and model evaluation](#predict_and_model_evaluation_1)\n\n[To do](#to_do)\n\n-->[Additional feature 1a](#new_feature_1a)\n\n-->[Additional feature 1b](#new_feature_1b)\n\n-->[Additional feature 2](#new_feature_2)\n\n-->[Additional feature 3](#new_feature_3)\n\n-->[Bonus track](#bonus_track)\n\n-->[Join df's with new features](#join_dfs_with_new_features)\n\n-->[Model training](#new_model)\n\n-->[Feature importance of new model](#feature_importance_2)\n\n-->[Predict and model evaluation of new model](#predict_and_model_evaluation_2)","00168d8f":"<a id = \"feature_6\"><\/a>\n## Adding holiday and number of weekends data\n[Go back to the table of contents](#table_of_contents)","73750315":"<a id = \"feature_2\"><\/a>\n## Date and item_id features\n[Go back to the table of contents](#table_of_contents)","012501c9":"<a id = \"correct_shop_names_id\"><\/a>\n## Correct the shop names and id\n[Go back to the table of contents](#table_of_contents)","42aa921d":"<a id = \"fe_generator_class\"><\/a>\n# FeatureGenerator class\n[Go back to the table of contents](#table_of_contents)","258a16a4":"<a id = \"predict_and_model_evaluation_1\"><\/a>\n# Predict and model evaluation\n[Go back to the table of contents](#table_of_contents)","d7e22fe4":"<a id = \"join_dfs_with_new_features\"><\/a>\n# Join df's with new features & df preparation for modelling\n[Go back to the table of contents](#table_of_contents)","c6f27fc8":"Let's create a weekly range and see how many rows it will produce.","aa5a751e":"Eliminaremos algunas variables para ver c\u00f3mo var\u00eda el error en la predicci\u00f3n.","5f6568e9":"We will be working with a DataFrame resampled by Months. We must resample the sales_df.","50728392":"<a id = \"basic_model\"><\/a>\n# Basic model train\n[Go back to the table of contents](#table_of_contents)","3ff9f4bc":"As the first feature, we will add the combination of shop, item and the revenue obtained by each shop for each item.","c240c3fc":"<a id = \"create_new_features\"><\/a>\n# Generate additional features as, mean and total sales for shop_id , item_id, city ... for every month\n[Go back to the table of contents](#table_of_contents)","c49be7ca":"<a id = \"helper_functions\"><\/a>\n# Helper functions\n[Go back to the table of contents](#table_of_contents)","eb4f5e1b":"<a id = \"predict_and_model_evaluation_2\"><\/a>\n# Predict and model evaluation of new model\n[Go back to the table of contents](#table_of_contents)","09937234":"<a id = \"new_feature_1a\"><\/a>\n# Additional feature 1a: shop_id, item_id & revenue feature\n[Go back to the table of contents](#table_of_contents)"}}