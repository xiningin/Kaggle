{"cell_type":{"2f4d37e8":"code","f2460a9a":"code","d2949c84":"code","87cc6be7":"code","701e50bc":"code","e55af5d6":"code","a28a570e":"code","ef6cab89":"code","af26d3ac":"code","0dafb3da":"code","6b5a3d73":"code","71144bbc":"code","8bbf61e7":"code","6f7f7a98":"code","324dcdaa":"code","0fe449e9":"code","6f4dc1e5":"code","649f9279":"code","ca2e161b":"code","7810fb25":"code","b856e6ad":"code","4f30fc60":"code","b322d1a3":"code","343a165f":"code","1708c418":"code","7e0b106f":"code","01240d0e":"code","a376c5b1":"code","283b7342":"code","4bdc93d0":"code","abca6fb2":"code","5194af68":"code","3f8efc7a":"code","d3861756":"code","474dea5c":"code","f45378cc":"code","ec71a2b9":"code","c5c4889d":"code","794c4912":"code","decdbd3c":"code","a5a8a624":"code","ea635a1f":"code","c9a8955b":"code","a8f8f804":"code","fe55bad2":"code","6781baa0":"code","40a04270":"code","c7e0f14b":"code","ddcf32d1":"code","618c8479":"code","cb55916d":"code","b2a7b075":"code","643b9a7e":"code","5b2102ca":"code","bd3ef3d7":"code","8f522fe1":"code","d9313000":"code","07fd38f4":"code","59a76443":"code","14fdb7cf":"code","4ce40b66":"code","dfcbd324":"code","5412fa06":"markdown","c5df606b":"markdown","9f51da92":"markdown","643cb077":"markdown","37e66e05":"markdown","9407b1c7":"markdown","2bd35dd1":"markdown","b342a93c":"markdown","2568d202":"markdown","b0f227a9":"markdown","43f06052":"markdown","d7a1decf":"markdown","14985cae":"markdown","95cbad64":"markdown","0c1d550b":"markdown","0e2bf3e8":"markdown","b75b522f":"markdown","d657b370":"markdown","67cca0ad":"markdown","9a552cd8":"markdown","355abe92":"markdown","54f80f27":"markdown","3cef0527":"markdown","44630ab3":"markdown","2196b52a":"markdown","4ca1b12e":"markdown","4aedc4da":"markdown","27af41ec":"markdown","4470b9f7":"markdown","a7bc1c4b":"markdown","bd7d797c":"markdown","0a5cff23":"markdown","4d12b5ee":"markdown","1193a49f":"markdown","685d7d07":"markdown","b7d27fea":"markdown","eb10f1b6":"markdown"},"source":{"2f4d37e8":"#from plotly.offline import init_notebook_mode, iplot\n#import plotly.graph_objs as go\n#import plotly.plotly as py\n#from plotly import tools\n#from datetime import date\nimport pandas as pd\nimport numpy as np \n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import random \n#import warnings\n#import operator\n#warnings.filterwarnings(\"ignore\")\n#init_notebook_mode(connected=True)\n","f2460a9a":"test = pd.read_csv(\"..\/input\/test.csv\")\npoverty_train = pd.read_csv(\"..\/input\/train.csv\")\ndisplay(poverty_train.head(), poverty_train.shape)","d2949c84":"display(test.head(), test.shape, test[[\"Id\", \"idhogar\", \"parentesco1\"]].head(10))","87cc6be7":"print (\"Top Columns having missing values in the train set.\")\nmissmap = poverty_train.isnull().sum().to_frame().sort_values(0, ascending = False) \/ len(poverty_train)\nmissmap.head()","701e50bc":"print (\"Top Columns having missing values in the test set.\")\nmissmap = test.isnull().sum().to_frame().sort_values(0, ascending = False) \/ len(test)\nmissmap.head()","e55af5d6":"poverty_train[poverty_train.meaneduc.isnull()]","a28a570e":"value1 = poverty_train[\"meaneduc\"].mean()\nvalue2 = value1*value1\ndisplay(value1, value2)\n","ef6cab89":"poverty_train[\"meaneduc\"].fillna(value1, inplace = True)\npoverty_train[\"SQBmeaned\"].fillna(value2, inplace = True)\n\nprint (\"Top Columns having missing values\")\nmissmap = poverty_train.isnull().sum().to_frame().sort_values(0, ascending = False) \/ len(poverty_train)\nmissmap.head()","af26d3ac":"poverty_train.describe()","0dafb3da":"display(poverty_train.loc[( poverty_train[\"tipovivi1\" ] == 1, \"v2a1\")].isna().sum(), \n\n        len(poverty_train.loc[( poverty_train[\"tipovivi1\" ] == 1, \"v2a1\")]))","6b5a3d73":"poverty_train.loc[(poverty_train[\"tipovivi1\" ] == 1, \"v2a1\")] = 0\n\nprint (\"Top Columns having missing values\")\nmissmap = poverty_train.isnull().sum().to_frame().sort_values(0, ascending = False) \/ len(poverty_train)\nmissmap.head()","71144bbc":"display(poverty_train.loc[( poverty_train[\"tipovivi2\" ] == 1, \"v2a1\")].isna().sum(), \n        len(poverty_train.loc[( poverty_train[\"tipovivi2\" ] == 1, \"v2a1\")]))","8bbf61e7":"display(poverty_train.loc[( poverty_train[\"tipovivi3\" ] == 1, \"v2a1\")].isna().sum(), \n        len(poverty_train.loc[( poverty_train[\"tipovivi3\" ] == 1, \"v2a1\")]))","6f7f7a98":"display(poverty_train.loc[( poverty_train[\"tipovivi4\" ] == 1, \"v2a1\")].isna().sum(), \n        len(poverty_train.loc[( poverty_train[\"tipovivi4\" ] == 1, \"v2a1\")]))","324dcdaa":"display(poverty_train.loc[( poverty_train[\"tipovivi5\" ] == 1, \"v2a1\")].isna().sum(), \n        len(poverty_train.loc[( poverty_train[\"tipovivi5\" ] == 1, \"v2a1\")]))","0fe449e9":"poverty_train[\"v2a1\"].fillna(0, inplace = True)\n\nprint (\"Top Columns having missing values (%)\")\nmissmap = poverty_train.isnull().sum().to_frame().sort_values(0, ascending = False) \/ len(poverty_train)\nmissmap.head()","6f4dc1e5":"poverty_train[\"v18q1\"].describe()","649f9279":"poverty_train[\"v18q\"].describe()","ca2e161b":"display(poverty_train.loc[( poverty_train[\"v18q\" ] == 0, \"v18q1\")].isna().sum(),\n        len(poverty_train[poverty_train[\"v18q\"] == 0]))","7810fb25":"poverty_train.loc[(poverty_train[\"v18q\" ] == 0, \"v18q1\")] = 0\n\nprint (\"Top Columns having missing values (%)\")\nmissmap = poverty_train.isnull().sum().to_frame().sort_values(0, ascending = False) \/ len(poverty_train)\nmissmap.head()","b856e6ad":"poverty_train[\"rez_esc\"].describe()","4f30fc60":"poverty_train[\"rez_esc\"].median()","b322d1a3":"poverty_train['rez_esc'] = poverty_train.apply(\n    lambda row: 0 if (row['instlevel1'] == 1) else row['rez_esc'],\n    axis=1\n)\n\npoverty_train['rez_esc'] = poverty_train.apply(\n    lambda row: poverty_train[\"rez_esc\"].median() if np.isnan(row[\"rez_esc\"]) else row['rez_esc'],\n    axis=1\n)\n\nprint (\"Top Columns having missing values (%)\")\nmissmap = poverty_train.isnull().sum().to_frame().sort_values(0, ascending = False) \/ len(poverty_train)\nmissmap.head()\n","343a165f":"def plot_value_counts(series, title=None):\n    '''\n    Plot distribution of values counts in a pd.Series\n    '''\n    _ = plt.figure(figsize=(12,6))\n    z = series.value_counts()\n    sns.barplot(x=z, y=z.index)\n    _ = plt.title(title)\n    \nplot_value_counts(poverty_train['edjefe'], 'Value counts of edjefe')\nplot_value_counts(poverty_train['edjefa'], 'Value counts of edjefa')\nplot_value_counts(poverty_train['dependency'], 'Value counts of dependency')","1708c418":"poverty_train[\"dependency\"].replace('yes', 1, inplace = True)\npoverty_train[\"dependency\"].replace('no', 0, inplace = True)\npoverty_train[\"edjefa\"].replace('yes', 1, inplace = True)\npoverty_train[\"edjefa\"].replace('no', 0, inplace = True)\npoverty_train[\"edjefe\"].replace('yes', 1, inplace = True)\npoverty_train[\"edjefe\"].replace('no', 0, inplace = True)\n\n#check if our solution worked\ndisplay(poverty_train[\"dependency\"].value_counts(),\n        poverty_train[\"edjefa\"].value_counts(),\n        poverty_train[\"edjefe\"].value_counts())","7e0b106f":"poverty_train[\"edjefe\"] = poverty_train[\"edjefe\"].astype(float)\npoverty_train[\"edjefa\"] = poverty_train[\"edjefa\"].astype(float)\npoverty_train[\"dependency\"] = poverty_train[\"dependency\"].astype(float)","01240d0e":"d={}\nweird=[]\nfor row in poverty_train.iterrows():\n    idhogar=row[1]['idhogar']\n    target=row[1]['Target']\n    if idhogar in d:\n        if d[idhogar]!=target:\n            weird.append(idhogar)\n    else:\n        d[idhogar]=target\n\nlen(set(weird))","a376c5b1":"for i in set(weird):\n    hhold=poverty_train[poverty_train['idhogar']==i][['idhogar', 'parentesco1', 'Target']]\n    target=hhold[hhold['parentesco1']==1]['Target'].tolist()[0]\n    for row in hhold.iterrows():\n        idx=row[0]\n        if row[1]['parentesco1']!=1:\n            poverty_train.at[idx, 'Target']=target\n","283b7342":"poverty_train[poverty_train['idhogar']==weird[1]][['idhogar','parentesco1', 'Target']]","4bdc93d0":"#poverty_train.hist(bins=30, figsize=(20,15))\n#plt.title( \"Histogram Plots\")\n#plt.show()","abca6fb2":"#poverty_train[[\"meaneduc\", \"v2a1\", \"age\", \"agesq\", \"SQBmeaned\",\n              #\"SQBovercrowding\", \"overcrowding\", \"SQBage\",\"escolari\", \"SQBescolari\",\n               #\"SQBdependency\", \"SQBhogar_total\" ]].hist(bins=30, figsize=(20,15))\n#plt.title( \"Histogram Plots\")\n#plt.show()","5194af68":"poverty_train[[\"v18q1\", \"v2a1\", \"rez_esc\"]].describe()","3f8efc7a":"very_poor = poverty_train[poverty_train[\"Target\"] == 1]\npoor = poverty_train[poverty_train[\"Target\"] == 2]\nvulnerable = poverty_train[poverty_train[\"Target\"] == 3]\nsafe = poverty_train[poverty_train[\"Target\"] == 4]","d3861756":"display(very_poor.shape,\n       poor.shape,\n       vulnerable.shape,\n       safe.shape)","474dea5c":"display(\n    very_poor[[\"v18q1\", \"v2a1\", \"rez_esc\"]].describe(),\n    poor[[\"v18q1\", \"v2a1\", \"rez_esc\"]].describe(),\n    vulnerable[[\"v18q1\", \"v2a1\", \"rez_esc\"]].describe(),\n    safe[[\"v18q1\", \"v2a1\", \"rez_esc\"]].describe())","f45378cc":"very_poor[[\"Target\", \"rez_esc\", \"meaneduc\", \"v2a1\", \"age\",\"overcrowding\",\n           \"rooms\", \"v18q1\"]].hist(bins=25, figsize=(20,15))\nplt.title( \"Histogram Plots\")\nplt.show()","ec71a2b9":"safe[[\"Target\", \"rez_esc\", \"meaneduc\", \"v2a1\", \"age\",\"overcrowding\",\n           \"rooms\", \"v18q1\"]].hist(bins=25, figsize=(20,15))\nplt.title( \"Histogram Plots\")\nplt.show()","c5c4889d":"plt.figure(figsize=(12,7))\nplt.title(\"Distributions of Age\")\nsns.kdeplot(very_poor[\"age\"], label = \"Very Poor Group\", shade = True)\nsns.kdeplot(poor[\"age\"], label = \"Poor Group\", shade = True)\nsns.kdeplot(vulnerable[\"age\"], label = \"Vulnerable Group\", shade = True)\nsns.kdeplot(safe[\"age\"], label=\"Safe Group\", shade = True)\nplt.legend();","794c4912":"import numpy as np\nimport scipy as sp\nimport scipy.stats\n\ndef mean_confidence_interval(data, confidence=0.95):\n    a = 1.0*np.array(data)\n    n = len(a)\n    m, se = np.mean(a), scipy.stats.sem(a)\n    h = se * sp.stats.t._ppf((1+confidence)\/2., n-1)\n    return m, m-h, m+h","decdbd3c":"display(mean_confidence_interval(very_poor[\"age\"], confidence = 0.95),\nmean_confidence_interval(safe[\"age\"], confidence = 0.95))","a5a8a624":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n    # do something advanced above...\n    \n    # Drop SQB variables, as they are just squres of other vars \n    #df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df\n\ndef convert_OHE2LE(df):\n    print_check = True\n    \n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        if print_check:\n            sum_ohe = df[cols_s_].sum(axis=1).unique()\n            if sum_ohe.shape[0]>1:\n                print(s_)\n                print(df[cols_s_].sum(axis=1).value_counts())\n                #print(df[list(cols_s_+['Id'])].loc[df[cols_s_].sum(axis=1) == 0])\n        tmp_cat = df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df","ea635a1f":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n    '''\n    The function does not return, but transforms the input pd.DataFrame\n    \n    Encodes the Costa Rican Household Poverty Level data \n    following studies in https:\/\/www.kaggle.com\/mlisovyi\/categorical-variables-in-the-data\n    and the insight from https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403#359631\n    \n    The following columns get transformed: edjefe, edjefa, dependency, idhogar\n    The user most likely will simply drop idhogar completely (after calculating houshold-level aggregates)\n    '''\n    \n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])","c9a8955b":"def process_df(df_):\n    # fix categorical features\n    encode_data(df_)\n    #fill in missing values based on https:\/\/www.kaggle.com\/mlisovyi\/missing-values-in-the-data\n    for f_ in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n        df_[f_] = df_[f_].fillna(0)\n    df_['rez_esc'] = df_['rez_esc'].fillna(0)\n    # do feature engineering and drop useless columns\n    return do_features(df_)\n\n#train = process_df(train)\ntest = process_df(test)\n","a8f8f804":"def train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_","fe55bad2":"train = do_features(poverty_train)","6781baa0":"#train, test = train_test_apply_func(train, test, convert_OHE2LE)","40a04270":"X = train.query('parentesco1==1')\n#X = train\n\n# pull out the target variable\ny = X['Target'] - 1\nX = X.drop(['Target'], axis=1)","c7e0f14b":"cols_2_drop = ['agg18_estadocivil1_MEAN', 'agg18_estadocivil3_COUNT', 'agg18_estadocivil4_COUNT', \n               'agg18_estadocivil5_COUNT', 'agg18_estadocivil6_COUNT', 'agg18_estadocivil7_COUNT', \n               'agg18_instlevel1_COUNT', 'agg18_instlevel2_COUNT', 'agg18_instlevel3_COUNT', \n               'agg18_instlevel4_COUNT', 'agg18_instlevel5_COUNT', 'agg18_instlevel6_COUNT', \n               'agg18_instlevel7_COUNT', 'agg18_instlevel8_COUNT', 'agg18_instlevel9_COUNT', \n               'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_COUNT', \n               'agg18_parentesco11_MEAN', 'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN', \n               'agg18_parentesco1_COUNT', 'agg18_parentesco2_COUNT', 'agg18_parentesco3_COUNT', \n               'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN', 'agg18_parentesco5_COUNT', \n               'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_COUNT', \n               'agg18_parentesco7_MEAN', 'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN', \n               'agg18_parentesco9_COUNT', 'fe_people_weird_stat', 'hacapo', 'hacdor', 'mobilephone',\n               'parentesco1', 'rez_esc', 'v14a', 'v18q', # here\n                'agg18_age_MIN', 'agg18_age_MAX' , 'agg18_age_MEAN', 'agg18_escolari_MIN',\n                'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_dis_MEAN', \n                'agg18_estadocivil1_COUNT', 'agg18_estadocivil2_MEAN', 'agg18_estadocivil2_COUNT',\n                'agg18_estadocivil3_MEAN', 'agg18_estadocivil4_MEAN', 'agg18_estadocivil5_MEAN',\n                'agg18_estadocivil6_MEAN','agg18_estadocivil7_MEAN','agg18_parentesco1_MEAN',\n                'agg18_parentesco2_MEAN','agg18_parentesco3_MEAN','agg18_parentesco5_MEAN','agg18_parentesco9_MEAN',\n                'agg18_instlevel1_MEAN','agg18_instlevel2_MEAN','agg18_instlevel3_MEAN','agg18_instlevel4_MEAN',\n                'agg18_instlevel5_MEAN','agg18_instlevel6_MEAN','agg18_instlevel7_MEAN','agg18_instlevel8_MEAN',\n                'agg18_instlevel9_MEAN']\n\nX.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)\ntest.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)","ddcf32d1":"#use the following to test model generalization\n#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=314, stratify=y)","618c8479":"#display(X_train.shape,test.shape)","cb55916d":"#num_cols = poverty_train.columns[poverty_train.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n#num_cols = train.select_dtypes(include=[np.float32, np.int])\n#num_cols.head()\n","b2a7b075":"#cols_to_norm = ['v2a1','meaneduc', \"overcrowding\", \"SQBovercrowding\", \"SBQdependency\", \n                #\"SQBmeaned\", \"age\", \"SQBescolari\", \"escolari\",\n                #\"SQBage\", \"SQBhogar_total\", \"SQBedjefe\", \"SQBhogar_nin\",\n                #\"agesq\"]\n#train[cols_to_norm] = survey_data[cols_to_norm].apply(lambda x: (x - x.min()) \/ (x.max() - x.min()))","643b9a7e":"#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()\n#train[num_cols] = scaler.fit_transform(train[num_cols])","5b2102ca":"#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()\n\n#X_train = X\n#y_train = train[\"Target\"].copy()\n#X_train_scaled = scaler.fit_transform(X_train)","bd3ef3d7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import expon, reciprocal\n\nparam_distribs = { \n        'penalty': ['l1', 'l2'],\n        'C': [0.001, 0.01, 0.1, 0.25, 0.50, 0.75, 1, 5,\n              10, 500, 750, 1000, 1250, 1500, 2000, 10000],\n        #'multi_class': ['ovr', 'multinomial'],\n        'fit_intercept': [True, False],\n        'class_weight': [None, 'balanced']\n        \n    }\n\nlogit = LogisticRegression(random_state=42, solver = \"liblinear\", multi_class = 'ovr',\n                             n_jobs = 4, max_iter = 200)\nrnd_search = GridSearchCV(logit, param_grid=param_distribs,\n                                cv=5, scoring='f1_macro',\n                                verbose=2, n_jobs=4)\nrnd_search.fit(X, y)","8f522fe1":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(mean_score, params)","d9313000":"print(rnd_search.best_score_, rnd_search.best_estimator_, rnd_search.best_params_)","07fd38f4":"#to test model generalization\n#from sklearn.metrics import f1_score\n#y_pred = rnd_search.predict(X_test)\n#f1_score(y_test, y_pred, average='macro')","59a76443":"#fit the entire model with selected parameters.\n","14fdb7cf":"y_subm = pd.read_csv('..\/input\/sample_submission.csv')\ny_subm['Target'] = rnd_search.predict(test) + 1\ny_subm.to_csv('submission.csv', index=False)","4ce40b66":"#from sklearn.model_selection import RandomizedSearchCV\n#from scipy.stats import expon, reciprocal\n#from sklearn.svm import SVC\n\n# see https:\/\/docs.scipy.org\/doc\/scipy\/reference\/stats.html\n# for `expon()` and `reciprocal()` documentation and more probability distribution functions.\n\n# Note: gamma is ignored when kernel is \"linear\"\n#param_distribs = {\n        #'kernel': ['linear', 'rbf', 'poly'],\n        #'C': reciprocal(20, 200000),\n        #'gamma': expon(scale=1.0),\n#}\n\n#svm = SVC()\n#rnd_search = RandomizedSearchCV(svm, param_distributions=param_distribs,\n                                #n_iter=50, cv=5, scoring='neg_mean_squared_error',\n                                #averbose=2, n_jobs=4, random_state=42)\n#rnd_search.fit(X_train_scaled, y_train)","dfcbd324":"#cvres = rnd_search.cv_results_\n#for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    #print(mean_score, params)","5412fa06":"This drops the number of NaN's in the monthly rent payment considerably. Let's keep going.","c5df606b":"Okay, we need to fix this. His other code cell does the job.","9f51da92":"One thing we can do quickly is impute the mean education level with the overall mean in the data set (meaneduc). Only 5 values are missing, and we are using a mean, so this is not a bad method. We can then square those values and fill in the SQBmeaned, so that the rows are consistent with each other.\n\nNB: imputing with means is not appropriate when a large chunk of your data is missing, or you have a large amount amount of skew in the data. Since this is not the case with only 5 values missing, we can proceed.","643cb077":"Success.","37e66e05":"Unsurprisingly, the mean monthly payment for each group increases as the group acquires more wealth. Strangely, the median housing payment is zero for most houses. Is this an error? Not really. Nearly half this data set actually owns a paid off house, as we determined earlier, and so we imputed a 0 for the missing valules in the monthly payment, whenever a house was owned. This might seem strange, but makes sense in that the mean monthly payment is going up for each demographic as they move out of poverty.\n\n\"v18q1\" indicates the number of tablets at home, which has a higher max value for those who are not vulnerable to poverty. The median is 1, for each group, but so is the minimum. Which means that we dont have anyone who put down a zero for tablets at home. This may be unlikely, as we are talking about families in poverty, and not everyone can have tablet. Perhaps the missing values are actually 0s?\n\n\"rez_esc\" is the number of years behind in schooling. Unsurprisingly, this number has a higher mean in poor groups, but is lower in the wealthier group. This likely will not be encoded or anything, as having a mean number of years behind in school actually makes sense inuitively.","9407b1c7":"The means and medians are pretty close to 0. It is probably \"safe\" to simply fill these all in with a 0, but we'll take a look at the dataset documentation first. We can find the \"instlevel\" variable which has the following description:\n\ninstlevel1, =1 no level of education\n\ninstlevel2, =1 incomplete primary\n\ninstlevel3, =1 complete primary\n\ninstlevel4, =1 incomplete academic secondary level\n\ninstlevel5, =1 complete academic secondary level\n\ninstlevel6, =1 incomplete technical secondary level\n\ninstlevel7, =1 complete technical secondary level\n\ninstlevel8, =1 undergraduate and higher education\n\ninstlevel9, =1 postgraduate higher education\n\n\nIt is probably safe to assume that anyone with a complete high school education (or higher) is not behind in school). We can also say that someone with no level of education is the maximum years behind in school. For this data set, that is listed as a 5. Let's look at the number of NaNs in this area. If there are any to replace, we will replace them with our assumed values.  For the rest, we won't really be able to know what the proper treatment is, so in that case, we can replace them with the median (which is 0). If spending more time imputing this helps our model we can come back to it.","2bd35dd1":"\n\nWe'll take a moment and discuss three other variables. \"edjefe\" and \"edjefa\" and \"dependency\", which is related to our job of imputation. We use a snippet of code from the kernal https:\/\/www.kaggle.com\/mlisovyi\/categorical-variables-encoding-function to make a plot.","b342a93c":"Let's look at some of these distributions a little closer, between the very poor group and the non-vulnerable group.","2568d202":"Some of these are pretty obviously continuous variables, but it's not clear what escolari is. The documentation  (https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/data) says that \"escolari\" is the number of years in school. We can probably treat this as continuous. It might be useful to aggregate this value later to create a house-hold level feature.\n\nUnderstanding of some of these columns can help us engineer more features later. For example, if we see that some columns are interesting or may be important, we can make note of these columns, then use them to define *interactions*. Essentially, multiplying columns with other columns. That may be useful, but for now, let's explore the data a little more.","b0f227a9":"Looks fixed. Let's take a minute and look at some histograms.","43f06052":"\nWe'll divide the data up based on the level of poorness. A value of 1 is the most poor, while a value of 4 is a \"safe\" level of wealth. Ie, the household is not vulnerable to poverty.","d7a1decf":"\nOne thing that is important to note about this data set, is that we are interested in predicting the Target variable, which is a classification of 1 to 4. Values of 1 indicate that a person, or household, is subjected to \n\"extreme poverty\", while values of 4 indicate that person or household is \"non-vulnerable\". This is useful to know, and may help in explaining some of these features. ","14985cae":"Okay, so we can replace all of these with a 0, as this subset of people actually own their house. Note that the number of NaN's is the same as the number of people in this subset, so it's fine to continue as follows:","95cbad64":"**Preparing data for the model**\n\nNow that our missing values are filled in,  we can process our data and get it ready to be fit into a model.  Let's make a copy of the data for now and have another look at the columns. We want to see which columns should be standardized or encoded and which can be left along.","0c1d550b":"This is sort of hard to see, but we can just get an idea of what distributions might be worth deeper inspection. \"Target\" is naturally of interest, but so would \"meaneduc\", \"v2a1\", \"age\", \"agesq\", \"SQBmeaned\", \"SQBovercrowding\", \"overcrowding\", \"rooms\", \"SQBage\", and some others. We look at these plots a little closer below. We want to make a note of what features are continuous and which ones are categorical, which will help us process the data later. Let's take a closer look at all the data that looks continuous. The notation \"SQB\" in front of some of these variables means that it is actually the *square* of another variable. For example, \"SQBage\" is the square of the age. It seems that the competition hosts have done some feature engineering for us already.","0e2bf3e8":"Now, we want to spend some time exploring the data to get a better understanding of what we are working with. Understanding the data a little better will give us some insight on how to impute what's missing. Let's have a look. We want to come up with a reasonable imputation of the next feature with the smallest number of NaN's. In this case, it will be the monthly rent payment, \"v2a1\".","b75b522f":"Now, according to this kernal https:\/\/www.kaggle.com\/katacs\/data-cleaning-and-random-forest we need to adjust one other thing. There are apparently discrepancies between the Target values for individuals and their households. We'll use some of katacs' code and take a look.","d657b370":"\nHere is the test data. \"Id\" is the unique ID attached to a person. \"idhogar\" is the ID attached to a house (multiple people can live in a house). And \"parentesco1\" is a binary code indicating whether a person is the head of the household or not. We mostly care about predictions on the head of the household, as per the competition rules of evaluating only *those* predictions.","67cca0ad":"We're almost done imputing! We finally take a look at one of the last columns, which is \"rez_esc\". This feature denotes the number of years behind in schooling. Let's have a look here as well","9a552cd8":"From the documentation, and the discussion here: https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403\n\nwe can see that there might be a way of dealing with \"v2a1\". The monthly rent payment is related to the following columns \n\n\"tipovivi1, =1 own and fully paid house\n\ntipovivi2, \"=1 own,  paying in installments\"\n\ntipovivi3, =1 rented\n\ntipovivi4, =1 precarious\n\ntipovivi5, \"=1 other(assigned,  borrowed)\"\n\nwhich are from the dataset documentation. We will filter by these and impute the rent payment accordingly. We will assume that if tipovivi1 is 1 for a particular person, then their rent payment is 0, as they own the house and it's paid off.","355abe92":"These are clean of NaNs. What about tipovivi4? This is a group of people who have \"precarious\" housing. tipovivi5 is a group of people that have assigned, or borrowed housing.","54f80f27":"The above output gives first the mean age, and the 95% confidence interval between the very_poor, and safe groups respectively. Note that the confidence intervals are actually disjoint, which implies there is a statistically significant difference in age between the two groups. The safe group is, on average, older. Perhaps age will be an interesting feature to use in our model (when we build it!).\n\nHere are some ideas for feature interactions, which we can test later. \n\n$age \\times meaneduc$ (age times the mean education level in the house)\n\n$SQBage \\times meaneduc$ (Square of the age times the mean education level)\n\n$age \\times escolari$ (age times the years of schooling)","3cef0527":"Looks like a binary indicator variable. The documentation doesn't say this, but it is probably safe to assume that a \"1\" means \"owns a tablet\" and a \"0\" means \"doesn't own a tablet\". Let's see if these line up with our NaNs","44630ab3":"Right away, we can see a difference between the distributions in the age of the two groups, and in their monthly payments. The safer crowd has a much more uniform age, while the very poor group seems to be on the young side. Also, the safe group has a more skewed distribution of monthly payments. (Note that the safe group's histogram of monthly payments (v2a1) is actually an order of magnitude higher on the x-axis than the very poor group. This is because there is more skew in that distribution, there are some people who pay a lot of money for rent, yet aren't in danger of poverty. As expected, this means the safe group has quite a bit more money to spend. However, the majority of people in both demographics actually own their own house).\n\nAge seems interesting. Let's take a closer look","2196b52a":"We're done imputing the NaNs. Here is a short pipeline to do it fast.","4ca1b12e":"\nThe median number of tablets in each house is 1, but also note that the *minimum* number of tablets is also 1. Is it really possible that every family in this data set has at least one tablet at home? Note that approximately 77% of the data for this feature is actually missing as well.  We have a feature related to this value though. It's \"v18q\" which is a binary value for \"owns a tablet\". Let's see if the the ownership of tablets lines up with the missing values. First, lets take a look at it","4aedc4da":"\nSo we have the percentage of data missing in each feature for the train and test sets. Lucky for us, the train and test sets have missing \nvalues in the same features.\n\nAccording to the documentation for this data set, \"rez_esc\" refers to \"years behind in school\", \"v18q1\" refers to the number of tablets in the household, and \"v2a1\" refers to the monthly rent payment. \"meaneduc\" is average number of years education, and \"SQBmeaned\" is the square of the education levels. This is available in the documentation. These are the only columns that are missing values.\n\nWe could drop these values, or we could impute them. For our purposes, it can be better to impute these data points. It is good practice, in general, to impute the values in the columns with the smallest number of missing variables, and work your way up to the columns with larger amounts of missing variables. This is especially true if you are using an algorithm to impute missing data, as the imputation of a variable will depend on what's already been imputed.\n\nTo this end, we'll start with \"meaneduc\" and \"SQBmeaned\". Let's take a quick look at the rows that are missing values in \"educ\". ","27af41ec":"First we will load in the data and take a quick look at the head.","4470b9f7":"\nYup, the number of missing values correspond exactly to the number of 0's in the tablet ownership feature. We can simply replace the NaNs with a 0 and see if we imputed everything","a7bc1c4b":"We want to see what percentage of data is missing. This counts the number of missing values and divides by the length of the training set so that we get a percentage. Note that the first three columns here are missing quite a bit of values. A rule of thumb is to drop data with more than a third of the data missing, but since we don't know too much of the data yet, we can't (shouldn't) do anything yet.","bd7d797c":"\nNote that the exact same rows are also missing values in SQBmeaned. ","0a5cff23":"Now we fit a regularized Logistic Regression to use as a baseline model.","4d12b5ee":"We're almost there. Let's look at \"v18a1\" next. According to the documentation, this variable is related to the number of tablets that belong to each household. What do we know about this value?","1193a49f":"Note that the values in the three columns are mostly categorical, with some random yes's or no's. From the discussion here: https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403#359631\n\nwe can see the following quote from the competition host: \"Also a clarification on edjefe and edjefa. These variables are just the interaction of escolari (years of education) head of husehold and gender. Some labels were generated whenever continuous variables have 1 or 0. The rule is to have yes being 1 yes=1 and no=0\". \n\nAlso: \"The dependency variable is one of the variables created from the data. the formula is:\n\ndependency=(number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64)\"\n\n\nAccording to the same discussion, this mapping of $yes \\mapsto 1$, and $no \\mapsto 0$ applies to all three columns. So, we'll go ahead and apply this mapping.","685d7d07":"\nFor the moment, we are going to do a quick fix. We'll just assume that the rest of these groups don't pay anything and just fill with \nzeroes.","b7d27fea":"Now let's have a look at the number of tablets, the monthly rent, and average years behind in school between these groups. But first, a look at the shape of these different groups.","eb10f1b6":"We'll prepare the data in a bunch of different ways to try and compare models with different data preparations. It was shown in some discussions and other kernals that the majority of the target class is \"4\". So we will try to balance that with sampling as well.\n\nWe'll use some slight variations on the feature engineering made in this kernal: https:\/\/www.kaggle.com\/mlisovyi\/feature-engineering-lighgbm-with-f1-macro\n\nWe actually didn't use a lot of this stuff, but some of the pipelines were useful. If you liked this part of the notebook, make sure to go give them an upvoat as well.\n\n"}}