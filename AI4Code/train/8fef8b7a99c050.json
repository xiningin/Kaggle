{"cell_type":{"084b9ae1":"code","e1122a93":"code","b3d87ee8":"code","d0e70e06":"code","23ace4ba":"code","8f58d7b5":"code","d5ff0ae8":"code","51437977":"code","e2221dba":"code","4c6d978d":"code","732b82ff":"code","8b8a49e4":"code","72080d65":"code","5345c5d0":"code","e1c2f1aa":"code","8799e85d":"code","61d8b358":"code","2245c555":"code","d936b8ed":"code","05c8104a":"code","9c9e1508":"code","a32f443d":"code","ce68cf51":"code","c0048f33":"code","7c5ea5e6":"markdown","207129be":"markdown","f347a4f0":"markdown","b8f90cae":"markdown","ef320eaf":"markdown","96b61511":"markdown","eb69496b":"markdown","bb4b1610":"markdown","514f2d11":"markdown","9228b2d6":"markdown","6eeb699a":"markdown","be2c17f8":"markdown","55b395a6":"markdown","1c5e3ba6":"markdown","0640ac28":"markdown","a7fe168f":"markdown","06b625c9":"markdown","2ab83998":"markdown","c52c5f0c":"markdown","1fac4375":"markdown","7ee36173":"markdown","98fc4850":"markdown","92b9f9e9":"markdown","0b215bb4":"markdown","0f1b1cda":"markdown","283c5dcf":"markdown","d1c9e4b3":"markdown","c61a7cc0":"markdown","bbbb3906":"markdown","b6bf6047":"markdown","5b72b498":"markdown","898eb53c":"markdown","4fe25ce3":"markdown","e0568f03":"markdown"},"source":{"084b9ae1":"# Data manipulation:\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n# ML packages:\nfrom sklearn.model_selection import cross_validate, KFold\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, matthews_corrcoef, make_scorer\nfrom sklearn.model_selection import train_test_split\n\n# Auxilliary:\nimport os\nfrom multiprocessing import cpu_count","e1122a93":"# Deinfe integer encoding for the 6 classes:\nactivity_to_code = {'dws': 1, 'ups': 2, 'sit': 3, 'std': 4, 'wlk': 5, 'jog': 6}\ncode_to_activity = {v:k for k,v in activity_to_code.items()}","b3d87ee8":"def load_raw_data(data_directory_path):\n    \"\"\"\n    Given a path to the motionsense-dataset directory, loop through the different CSVs and concatenate them \n    to one pandas DataFrame. Join with demographic data, and return the DataFrame.\n    \"\"\"\n    subjects_data_directory_path = os.path.join(dataset_path, \"data_subjects_info.csv\")\n    \n    # Load demographic data of subjects:\n    subject_data = pd.read_csv(subjects_data_directory_path).rename(columns={'code':'subject'}) # rename for clarity\n    subject_data['subject'] = subject_data.subject.astype(str)\n\n    # Load data from sensor:\n    motion_data_directory_path = os.path.join(dataset_path, r\"A_DeviceMotion_data\/A_DeviceMotion_data\")\n    dirs = os.listdir(motion_data_directory_path)\n    dfs = []\n    for d in dirs:\n        activity_name, experiment_id = d.split(\"_\")\n        for subject in os.listdir(os.path.join(motion_data_directory_path, d)):\n            filepath = os.path.join(os.path.join(motion_data_directory_path, d), subject)\n            df = pd.read_csv(filepath, index_col=0)\n            df['subject'] = subject.split(\".\")[0].split(\"_\")[1] # keep only the subject's numerical i.d.\n            df['activity'] = activity_to_code[activity_name]\n            df['experiment_id'] = int(experiment_id)\n            df['experiment_step'] =  np.arange(0, len(df)) # assign a numerical step number for every measurement in the experiment\n            dfs.append(df)\n\n    motion_data = pd.concat(dfs)\n    \n    # Join demographic data to final dataframe:\n    final_df = motion_data.merge(subject_data, on=['subject'])\n    \n    return final_df, subject_data","d0e70e06":"dataset_path = r\"..\/input\/motionsense-dataset\" # Assuming here dataset is in cwd.\nraw_df, subject_data = load_raw_data(dataset_path)","23ace4ba":"print(\"raw data shape: \", raw_df.shape)\nprint(\"\\nraw data columns:\\n\", raw_df.columns)","8f58d7b5":"raw_df.isnull().sum(axis=0)\n# There are no missing values","d5ff0ae8":"MOTION_SENSOR_COLUMNS = ['attitude.roll', 'attitude.pitch', 'attitude.yaw', 'gravity.x', 'gravity.y', 'gravity.z', \n                         'rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y',\n                        'userAcceleration.z']\nDEMOGRAPHIC_FEATURES = ['weight', 'height', 'age', 'gender']","51437977":"raw_df[MOTION_SENSOR_COLUMNS].describe(percentiles=[0.001,0.01,0.25,0.5,0.75,0.95,0.99, 0.999]).round(3)","e2221dba":"# Plot histograms of the motion features:\n\nsamp = raw_df.sample(10**5) # plotting a sample, for run-time considerations\nfig, ax = plt.subplots(3, 4, sharex='col', sharey='row', figsize=(10, 8))\n\nm=0\nfor i in range(3):\n    for j in range(4):\n        colname = MOTION_SENSOR_COLUMNS[m]\n        samp[colname].plot(kind='hist', ax=ax[i,j], bins=20, title=colname, density=True, xlim=(-4,4))\n        m += 1\nplt.show()","4c6d978d":"raw_df[['weight','height','age']].describe().round(2)","732b82ff":"# Plot class distribution:\nactiviry_counts = raw_df.activity.apply(lambda x: code_to_activity[x] ).value_counts()\nactiviry_counts.plot(kind='bar', title='Activity Class Distibution')\nplt.show()","8b8a49e4":"def get_processed_df(window_size, summary_statistics):\n    \"\"\"\n    Group measurements per subject per experiment to windows of requested size, and calculate a set of summary_statistics \n    withing that window \n    window_size: int, size of window.\n    summary_statistics: list, either of string representations of aggregation functions or aggregation function objects\n    \"\"\"\n    grouped_df = raw_df.groupby(['subject', 'experiment_id'])\n    processed_dfs = []\n    for name, group in grouped_df: # iterate over data from consecutive meaurements, per subject per experiment\n        subject, experiment = name\n        nbins = int(len(group) \/ window_size) # num of bins depends on length of epxeriment\n        bin_edges = pd.cut(group.experiment_step, bins=nbins) \n        activity = group.activity.values[0] # all activites in this df are the same\n        agg_per_bin = group[MOTION_SENSOR_COLUMNS].groupby(bin_edges).agg(summary_statistics)\n\n        # fix colum names:\n        agg_per_bin = agg_per_bin.reset_index()\n        cols = list(agg_per_bin.columns)\n        fixed_cols = [str(c[0])+\"_\"+str(c[1]) for c in cols]\n        agg_per_bin.columns = fixed_cols\n\n        # Add the constant features (constant per sbject and experiment):\n        agg_per_bin['experiment_id'] = experiment\n        agg_per_bin['activity'] = activity # this will be the label\n        agg_per_bin['subject'] = subject\n        agg_per_bin = agg_per_bin.merge(subject_data, on='subject')\n        \n        processed_dfs.append(agg_per_bin)\n    \n    processed_data = pd.concat(processed_dfs)\n    return processed_data","72080d65":"summary_statistics = ['mean', 'median', 'max', 'min', 'std', 'skew']\nprocessed_data = get_processed_df(window_size=100, summary_statistics=summary_statistics)","5345c5d0":"print(processed_data.columns)","e1c2f1aa":"features_to_plot = [\"attitude.roll_mean\", \"userAcceleration.y_skew\", \"gravity.y_median\"] # arbitrary choice\nfor feature in features_to_plot:\n    processed_data.groupby('activity')[feature].plot(kind='kde',legend=True,\n                                                    title=\"'%s' density per class\" % feature)\n    plt.show()\n","8799e85d":"motion_sensor_features =  [f for f in processed_data.columns if any([x in f for x in MOTION_SENSOR_COLUMNS])]\nfeature_names = motion_sensor_features + DEMOGRAPHIC_FEATURES","61d8b358":"X = processed_data[feature_names]\ny = processed_data['activity']\n\nprint(\"feature matrix shape: \",X.shape)\nprint(\"target vector matrix shape: \",y.shape)","2245c555":"xgb_clf = XGBClassifier(n_jobs=-1)\nlr = LogisticRegression(n_jobs=-1)","d936b8ed":"mcc_scorer = make_scorer(matthews_corrcoef) # defining a custom scorer (requried for computing MCC + accuracy in one go)\nkfold_splitter = KFold(n_splits=5, shuffle=True)\nnamed_classifiers = [(xgb_clf, \"xgb_classifier\"), (lr, \"logistic_regression\")]","05c8104a":"cv_results = {}\nfor model, name in named_classifiers:\n    print(\"fitting %s...\" % name)\n    cv_results[name] = \\\n        cross_validate(model, X, y, \n                        cv=kfold_splitter, scoring={\"mcc\":mcc_scorer, \"accuracy\":\"accuracy\"}, \n                       n_jobs=cpu_count()-1, verbose=3, return_estimator=True) # use all cores but one\n    print(\"Done\")\n\n","9c9e1508":"test_metrics = {}\nfor model, results in cv_results.items():\n    test_metrics[model] = {}\n    for metric, values in results.items():\n        if \"test\" in metric: # skip timing metrics\n            test_metrics[model].update({\n                metric: np.mean(values),\n                metric + \"_std\": np.std(values)\n            })\ntest_metrics = pd.DataFrame(test_metrics)\n\nprint(test_metrics)","a32f443d":"trained_xgb_clf = cv_results[\"xgb_classifier\"][\"estimator\"][0] # 0 is abritrary, the estimator from the 1st fold\n\n# get feature importances:\nfi = pd.DataFrame(zip(X.columns, trained_xgb_clf.feature_importances_), columns=['feature','importance'])\nfi.sort_values(by='importance', ascending=False, inplace=True)\nfi.set_index('feature', inplace=True)","ce68cf51":"fi.head(20).plot(kind='barh', figsize=(10,10), title=\"Most important features\")\nplt.show()","c0048f33":"excluded_features = fi[fi.importance == 0]\nprint(excluded_features)","7c5ea5e6":"Plot the 20 most important features:","207129be":"<font color=blue>It's evident that the features are distributed differently between classes.<\/font>","f347a4f0":"create a list of all the features to train on (sensor summary statistics & demographic features):","b8f90cae":"Create feature matrix **X** and labels vecotr **y**:","ef320eaf":"We'll choose the xgboost implementation of **gradient boosted trees clasifier**, as a good off-the-shelf classifier with relative robustness to redundant features and fast training time, as well as explainable results. We will also compare it's perofrmance to multiclass **logistic-regression**.","96b61511":"<font color=blue>Very few features were not used for splits by the model (in my run they were interestingly all \"gravity\" features, but this may slightly change accross runs).<\/font>","eb69496b":"# 1. Import relevant packges \/ functions:","bb4b1610":"<font color=blue>classes are not balances, with (walk, sit, stand) ~2X larger than (upstairs, jog, downstairs)<\/font>","514f2d11":"### Parse and report cv results - keep only the evaluation metrics (cross_validate also reports timing metrics):","9228b2d6":"<font color=blue>\nWe observe that most raw features exhibit bell-shaped distributions around 0, while others (like attitute.pitch) are highly skewed<\/font>","6eeb699a":"### Examine \"demographics\" distibution:","be2c17f8":"# Additional directions:","55b395a6":"### Inspect target-class distribution:","1c5e3ba6":"### Extract feature importance from one of the trained xgb_classifiers:","0640ac28":"* Experiment with different window sizes. current windows requires 2 seconds of data, which may be a lot for some applications\n* Consider other metrics such as multi-class AUC (I didn't add it here because cross_validate doesn't support it, and I chose to go along woth the parallelized implementation othar than iterating over CV folds)\n* Investigate perfomrance metrics per class and per person in the test set - could be interesting.\n* Deal with anbormal values in the raw data - this requires some more info about their scales (consider some transformation such as log-scale for the features with abnormal values)\n* Conduct meta-parameter optimization, and switch from using the vanilla model.\n* Consider additional models, including RNN architectures (could work on the raw data, or on smaller feature-windows)\n* Use bootstrapping to better estimate the variability of the test metrics\n* Add more summary statistics as features, for example I would try adding more percentiles of each raw feature in the window in order to better convey it's distribution to the model (currently we use only the 0.5 percentile - the median).\n","a7fe168f":"# Inspect feature importances","06b625c9":"<font color=blue>\nWe can see that some of the feature exhibit some extremely large values (relatively to their distribution) - for \n    example rotationRate.y. This appears to happen in both sides of the distribution (extremely small values & extremely large ones), and thus less likely to be measurements errors.\n<\/font>","2ab83998":"### Train and evaluate using k-ofld CV, to estimate MCC (and is standard deviation):","c52c5f0c":"# Extract feature matrix & labels for model training:","1fac4375":"<font color=blue>No suspicious values such as negative height or impossible age.<\/font>","7ee36173":"# Calculate features as summary statistics over fixed window:\n\nWe'll split consecutive measurements (within subject, within experiment) to windows of a fixed size (for example, a window size of 50 corresponds to 1 second of measurements as the measure frequency is 50htz), and calculate a set of summary statistics for the measurements in the window.\nAdditionaly, we'll add the demographic features (age, weight, etc.) per subject.","98fc4850":"<font color=blue>We can see that all 6 statistics (mean, median, min, max, std, skew) are represented in the top-20 features.<\/font>","92b9f9e9":"### Examine feature behaviour accross classes:\n\nWe'll look at the difference in distribution of some of the features accross classes\"","0b215bb4":"### Check how many features (and which ones) were excluded form model:","0f1b1cda":"Define a scorrer and a CV splitter:\nThe CV splitter has to explicitly be defined so that we can demand that data be shuffeld before splitting. This is required because (X,y) are sorted by subject\/experiment, so unshuffled splits will produce biased estimators.","283c5dcf":"<font color=blue>We can see that the gradient boosted trees model achieves high MCC and accuracy,\nwith very small standard deviations. logistic_regression doesn't perform as well.<\/font>\n\n\n","d1c9e4b3":"# 3. EDA:","c61a7cc0":"Train & evaluate both classifeirs <font color=red>(will take a few minutes on the full dataset):<\/font>\n","bbbb3906":"# 2. Load raw data:\n<font color=red>Note:<\/font>make sure the data is in the current working directory, or change `dataset_path` accordingly.","b6bf6047":"### Check for missing values:","5b72b498":"### Load all data from all subjects \/ experiments, as well as their demographic data:","898eb53c":"We'll choose the following summary statistics, as they seem like a reasonable approximation for the disribution of a feature within the window. \nI'd expect these values to exibit different mean, std, etc. when a person switches between activities.\nAdditionaly, such measures could vary between different gender\/height\/weight\/age groups, so we keep the demographoc features as well.\n<font color=red>Following cell takes ~1m to run (on a local i7 processor):<\/font>\n","4fe25ce3":"### Inspect the distribution of the raw data:","e0568f03":"<font color=blue>There are no missing values in the data.<\/font>"}}