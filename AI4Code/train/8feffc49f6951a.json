{"cell_type":{"84c7f8b6":"code","e13ee653":"code","314fc471":"code","b86445c3":"code","6f207d44":"code","6bedfc76":"code","0beafb18":"code","3ff0c690":"code","39046b06":"code","4144675b":"code","5181b7d3":"code","957563bb":"code","48339638":"code","248a1b78":"code","5f3e26da":"code","bc928eac":"code","8c1c60d1":"code","70c14973":"code","fb9a153c":"code","e6b132da":"code","2bd0db33":"code","2060e77a":"code","c1dcbcc5":"code","19635e6c":"code","0b5063b1":"code","6748edcc":"code","dcd8dba8":"code","9f18e923":"code","cbf790aa":"code","194e5854":"code","836507eb":"code","b75d781f":"code","c8a879b4":"code","32c3916f":"code","77cac284":"code","94e261a0":"code","4d2bced9":"code","97c86e43":"code","48ca4c24":"code","2f76a5cd":"code","21ca213c":"code","7e279df7":"code","1b6bb9c3":"code","4f2030ca":"code","386f6dea":"code","79e5c6cf":"code","b30fad95":"code","af1681d4":"code","79635ccd":"code","efbf1ab1":"code","07718894":"code","3d414f26":"code","a579a40a":"code","569519bf":"code","5130dd46":"code","46c8a45a":"markdown","095ee54b":"markdown","d515f690":"markdown","e2938d55":"markdown","dbd25a20":"markdown","717007e4":"markdown","89860a27":"markdown","22a68337":"markdown","1c3061d2":"markdown","c0763690":"markdown","05d18d78":"markdown","a004f153":"markdown"},"source":{"84c7f8b6":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\ndf.head()","e13ee653":"df = df.drop_duplicates(subset = ['review'], keep = 'first')\ndf.head()","314fc471":"sentiment_plot = df['sentiment'].value_counts().plot(kind = 'bar')\nfig = sentiment_plot.get_figure()\nfig.savefig(\"class_distribution.png\")","b86445c3":"len(df)","6f207d44":"seqlen = df['review'].apply(lambda x: len(x.split()))\nseqlen","6bedfc76":"import seaborn as sns\nimport matplotlib.pyplot as plt","0beafb18":"sns.set_style('darkgrid')\nplt.figure(figsize = (16, 10))\nseqlen_plot = sns.distplot(seqlen)\n\nfig = seqlen_plot.get_figure()\nfig.savefig(\"seqlen_plot.png\")","3ff0c690":"# Choosing our max_length of sequence as 512 to capture maximum amount of data\nSEQ_LEN = 512","39046b06":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')","4144675b":"tokens = tokenizer.encode_plus(\"hello world\", max_length = SEQ_LEN, \n                               truncation = True, padding = 'max_length', \n                               add_special_tokens = True, return_token_type_ids = False, \n                               return_attention_mask = True, return_tensors = 'tf')\ntokens","5181b7d3":"import numpy as np\n\nXids = np.zeros((len(df), SEQ_LEN))\nXmask = np.zeros((len(df), SEQ_LEN))\n\nXids.shape, Xmask.shape","957563bb":"for i, sequence in enumerate(df['review']):\n    tokens = tokenizer.encode_plus(sequence, max_length = SEQ_LEN, \n                               truncation = True, padding = 'max_length', \n                               add_special_tokens = True, return_token_type_ids = False, \n                               return_attention_mask = True, return_tensors = 'tf')\n    Xids[i, :], Xmask[i, :] = tokens['input_ids'], tokens['attention_mask']","48339638":"Xids","248a1b78":"Xmask","5f3e26da":"class_map = {\n    \"positive\": 0,\n    \"negative\": 1\n}\ndf['Sentiment'] = df['sentiment'].map(class_map)\narr = df['Sentiment'].values\narr, arr.shape","bc928eac":"labels = np.zeros((len(df), arr.max() + 1))\nlabels.shape","8c1c60d1":"labels[np.arange(len(df)), arr] = 1","70c14973":"labels","fb9a153c":"import numpy as np\nwith open('..\/working\/movie-xids.npy', 'wb') as f:\n    np.save(f, Xids)\nwith open('..\/working\/movie-xmask.npy', 'wb') as f:\n    np.save(f, Xmask)\nwith open('..\/working\/movie-labels.npy', 'wb') as f:\n    np.save(f, labels)","e6b132da":"del Xids, Xmask, labels","2bd0db33":"import numpy as np\n\nwith open('..\/working\/movie-xids.npy', 'rb') as f:\n    xids = np.load(f, allow_pickle = True)\nwith open('..\/working\/movie-xmask.npy', 'rb') as f:\n    xmask = np.load(f, allow_pickle = True)\nwith open('..\/working\/movie-labels.npy', 'rb') as f:\n    labels = np.load(f, allow_pickle = True)","2060e77a":"labels","c1dcbcc5":"import tensorflow as tf\ntf.config.experimental.list_physical_devices('GPU')","19635e6c":"dataset = tf.data.Dataset.from_tensor_slices((xids, xmask, labels))\ndataset.take(1)","0b5063b1":"def map_fn(input_ids, masks, labels):\n    return {'input_ids': input_ids, 'attention_mask': masks}, labels","6748edcc":"dataset = dataset.map(map_fn)\ndataset.take(1)","dcd8dba8":"batch_size = 16\ndataset = dataset.shuffle(100000).batch(batch_size, drop_remainder = True)\ndataset.take(1)","9f18e923":"DS_LEN = len(list(dataset))\nDS_LEN","cbf790aa":"split = 0.9","194e5854":"# Here we'll be dividing the train and test in 9:1\ntrain = dataset.take(round(DS_LEN * split))\ntest_ds = dataset.skip(round(DS_LEN * split))","836507eb":"# We'll further be dividing the train set in train and validation set in 9:1\ntrain_size = len(list(train))\ntrain_ds = train.take(round(train_size * split))\nval_ds = train.skip(round(train_size * split))","b75d781f":"test_ds.take(1)","c8a879b4":"train_ds.take(1)","32c3916f":"val_ds.take(1)","77cac284":"from transformers import TFAutoModel\nbert = TFAutoModel.from_pretrained('bert-base-cased')\nbert.summary()","94e261a0":"# Two inputs\ninput_ids = tf.keras.layers.Input(shape = (SEQ_LEN,), name = 'input_ids', dtype = 'int32')\nmask = tf.keras.layers.Input(shape = (SEQ_LEN,), name = 'attention_mask', dtype = 'int32')\n\n#Transformer\nembeddings = bert.bert(input_ids, attention_mask = mask)[0]\n\n#classifier head\n# x = tf.keras.layers.Dense(1024, activation = 'relu')(embeddings)\n# y = tf.keras.layers.Dense(2, activation = 'softmax', name = 'outputs')(x)\n\n# X = tf.keras.layers.LSTM(64)(embeddings)\n# X = tf.keras.layers.BatchNormalization()(X)\n# X = tf.keras.layers.Dense(64, activation='relu')(X)\n# X = tf.keras.layers.Dropout(0.1)(X)\n# y = tf.keras.layers.Dense(2, activation = 'softmax', name = 'outputs')(X)\n\nX = tf.keras.layers.Dropout(0.1)(embeddings)\nX = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(768))(X)\ny = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)\n\nmodel = tf.keras.Model(inputs = [input_ids, mask], outputs = y)\nmodel.summary()","4d2bced9":"model.layers[2].trainable = False\nmodel.summary()","97c86e43":"tf.keras.utils.plot_model(model, show_shapes=True)","48ca4c24":"optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, decay = 1e-6)\nloss = tf.keras.losses.CategoricalCrossentropy()\nacc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n\nmodel.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])","2f76a5cd":"history = model.fit(\n    train_ds,\n    validation_data = val_ds,\n    epochs = 3\n)","21ca213c":"model.save('..\/working\/sentiment_model_imdb')","7e279df7":"history.history","1b6bb9c3":"train_loss, train_accuracy = model.evaluate(train_ds)\nprint(\"train_loss: \", train_loss)\nprint(\"train_accuracy: \", train_accuracy)","4f2030ca":"val_loss, val_accuracy = model.evaluate(val_ds)\nprint(\"val_loss: \", val_loss)\nprint(\"val_accuracy: \", val_accuracy)","386f6dea":"test_loss, test_accuracy = model.evaluate(test_ds)\nprint(\"test_loss: \", test_loss)\nprint(\"test_accuracy: \", test_accuracy)","79e5c6cf":"import tensorflow as tf\nfrom transformers import AutoTokenizer\nimport numpy as np\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\nmodel = tf.keras.models.load_model('sentiment_model_imdb')\n\ncode_to_category = {\n    0: \"positive\",\n    1: \"negative\"\n}\ndef pred_text(model, text, max_length = 512):\n    tokens = tokenizer.encode_plus(text, max_length = max_length, \n                               truncation = True, padding = 'max_length', \n                               add_special_tokens = True, return_token_type_ids = False, \n                               return_attention_mask = True, return_tensors = 'tf')\n    probs = model.predict({\n        'input_ids': tf.cast(tokens['input_ids'], tf.float64),\n        'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)\n    })\n    \n    pred = np.argmax(probs[0])\n    \n    prediction = code_to_category[pred]\n    confidence = probs[0][pred]\n    \n    return prediction, confidence","b30fad95":"text = \"This movie was amazingly brilliant.\"\npred_text(model, text)","af1681d4":"text = \"This movie was amazingly awful.\"\npred_text(model, text)","79635ccd":"text = \"Maybe they should try to get a better cast next time.\"\npred_text(model, text)","efbf1ab1":"text = \"Movie sucks!!\"\npred_text(model, text)","07718894":"text = \"Movie rocks!!\"\npred_text(model, text)","3d414f26":"text = \"Only the first half of the movie was enjoyable\"\npred_text(model, text)","a579a40a":"text = \"They could have spent the money better helping people\"\npred_text(model, text)","569519bf":"import shutil\nshutil.make_archive('..\/working\/sentiment_model_imdb', 'zip', '..\/working\/sentiment_model_imdb')","5130dd46":"from IPython.display import FileLink\nFileLink(r'sentiment_model_imdb.zip')","46c8a45a":"Save the input_ids, mask and labels for reusability","095ee54b":"# Step 2: Create Data Pipiline for Model Input","d515f690":"Explore the Distribution of the classes in the dataset","e2938d55":"Download Model","dbd25a20":"# Load and Test Model on Examples","717007e4":"Explore the distribution of the length of reviews","89860a27":"## Evaluate the Model:","22a68337":"Now let's get to tokenizing our inputs","1c3061d2":"# Step 3: Build and Train our Model","c0763690":"## Start Training","05d18d78":"# Step 1: Load the IMDB Dataset of 50K Movie Reviews","a004f153":"Drop any duplicate entries from the dataset, keeping only the first entry"}}