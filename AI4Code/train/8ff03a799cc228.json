{"cell_type":{"fda238f0":"code","cee10899":"code","787e8c20":"code","495fc92c":"code","a5f027e1":"code","e2d81051":"code","5204de98":"code","4750d412":"code","196c6cdb":"code","89772d9e":"code","1a2bce27":"code","09607df8":"code","79d4e182":"code","83422152":"code","d418f730":"code","779f49f8":"code","27effe72":"code","f8c1fd80":"code","fa9c42a3":"code","d480333b":"code","28717790":"code","d9c9365c":"code","148fb2ef":"code","5499bf3a":"code","b5df73a6":"code","4da3c398":"code","034c25a3":"code","4d76c6d8":"code","e22ac99c":"code","80b17759":"code","d904243e":"code","db661adc":"code","6cacbacb":"code","cb686da2":"code","ede21879":"code","7201b59c":"code","77fee1c5":"code","1d6bf60c":"code","7c032d5f":"code","336e8558":"code","5046a465":"code","45b310f0":"code","2c46dbee":"code","fb928abd":"code","86a2f316":"code","86e80236":"code","875b73f0":"code","b847e8b3":"code","c47dc291":"code","b5241903":"code","9074e319":"code","388e5fc6":"code","8091e240":"code","cb5b656a":"code","d37c81c6":"markdown","43d0e04a":"markdown","bf571750":"markdown","1ce37ffe":"markdown","d5f6ace6":"markdown","ea72a310":"markdown","822a9c41":"markdown","e02b0a18":"markdown","7bd1c148":"markdown","666696b2":"markdown","f9d12e30":"markdown","92da7dda":"markdown","0ad94ff0":"markdown","7fadc762":"markdown","004dc375":"markdown","5b548703":"markdown","f553faec":"markdown","7aa2bbaa":"markdown","b870b3de":"markdown","63d03921":"markdown","d43e5cfc":"markdown","766a88d2":"markdown","a9cb16db":"markdown","1368afc5":"markdown","31c317ed":"markdown","58799a1e":"markdown","d8ead94e":"markdown","c2d5dd46":"markdown","208497e3":"markdown","9abaa38b":"markdown","691eecdb":"markdown","689d7822":"markdown","b148d63b":"markdown","9a1bde82":"markdown","b7cafa75":"markdown","24b95548":"markdown","8faddcde":"markdown","8e477090":"markdown","fb856f3e":"markdown","1429c575":"markdown","5fa09a10":"markdown","02a02787":"markdown","ed8e309e":"markdown"},"source":{"fda238f0":"#Regular explanatory data analysis and plotting libraries.\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('fivethirtyeight')\n\n#models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#model evaluators\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score,recall_score,f1_score,plot_roc_curve","cee10899":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf.head() #first five rows","787e8c20":"#lets count the total number of values in the target colums.\ndf['target'].value_counts()","495fc92c":"#Plot our target column\ndf['target'].value_counts().plot(kind='bar',color=[\"teal\", \"indigo\"]);","a5f027e1":"#Finding if there is any missing values in the dataset.\ndf.isna().sum()","e2d81051":"df.info()","5204de98":"df.describe()","4750d412":"df['sex'].value_counts()","196c6cdb":"# comparing target column with sex column.\npd.crosstab(df['target'],df['sex'])","89772d9e":"pd.crosstab(df['target'],df['sex']).plot(kind='bar',color=['darkgreen','cornflowerblue'])\nplt.title('Heart disease frequency for sex')\nplt.xlabel('0=No Heart Disease, 1=Heart Disease')\nplt.ylabel('Number')\nplt.legend(['Female','Male']);","1a2bce27":"fig,ax = plt.subplots(nrows=1,\n                     ncols=1,\n                     figsize=(12,8))\n\n#positive examples\nax.scatter(df['age'][df['target']==1],\n          df['thalach'][df['target']==1],\n          c = 'darkgreen')\n\n#negative examples\nax.scatter(df['age'][df['target']==0],\n          df['thalach'][df['target']==0],\n          c = 'red')\n\nax.set(title='Heart disease in function of Age and Max Heart Rate',\n      xlabel='Age',\n      ylabel = 'Max Heart Rate')\nax.legend(['Disease','No disease']);","09607df8":"#age distribution\ndf['age'].plot(kind='hist')","79d4e182":"pd.crosstab(df['cp'],df['target'])","83422152":"pd.crosstab(df['cp'],df['target']).plot(kind='bar',\n                                       figsize=(12,8),\n                                       color=['springgreen','purple'])\nplt.title('Heart Disease frequency Per chest pain')\nplt.xlabel('Chest Pain Type')\nplt.ylabel('Frequency')\nplt.legend(['No Heart Disease','Heart Disease']);","d418f730":"df_corr = df.corr()\ndf_corr","779f49f8":"corr_matrix = df.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(corr_matrix,\n           annot=True,\n           linewidths=0.5,\n           fmt='.2f',\n           cmap='YlGnBu');","27effe72":"# droping target variable\nX = df.drop('target',axis=1)\n\n# only target column\ny = df['target']","f8c1fd80":"X.head()","fa9c42a3":"y.head()","d480333b":"#for reproducible code\nnp.random.seed(45)\n\n#spliting our data into training and testing set.\nX_train,X_test,y_train,y_test = train_test_split(X, #independent variable\n                                                 y, #dependent variable\n                                                 test_size=0.2) #percentage of data used for testing","28717790":"#lets look at the shape of our training and testing data\nX_train.shape, X_test.shape, y_train.shape,y_test.shape","d9c9365c":"# Using logistic regression\nlog = LogisticRegression(max_iter=1000).fit(X_train,y_train)\nlog_score = log.score(X_test,y_test)\nprint('The accuracy score of Logistic regression is: {:.2f}%'.format(log_score*100))\n\n#Using KNeighbor\nknn = KNeighborsClassifier().fit(X_train,y_train)\nknn_score = knn.score(X_test,y_test)\nprint('The accuracy of K-Nearest Neighbors is: {:.2f}%'.format(knn_score*100))\n\n#Using Random Forest \nclf = RandomForestClassifier().fit(X_train,y_train)\nclf_score = clf.score(X_test,y_test)\nprint('The accuracy of Random Forest is: {:.2f}%'.format(clf_score*100))","148fb2ef":"model_score={log_score,knn_score,clf_score}\nmodel_comparison = pd.DataFrame(model_score,index=['Logistic Regression','K-Nearest Neighbors','Random Forest'])\nmodel_comparison.plot(kind='barh')\nplt.ylabel('Algorithms')\nplt.xlabel('Accuracy Score')\nplt.title('Accuracy Comparison between different Models')\nplt.legend('Accuracy');","5499bf3a":"# Logistic Regression hyperperimeters\nlog_grid = {\"C\": np.logspace(-4, 4, 20),\n            \"solver\": [\"liblinear\"]}\n\n# Random Forest hyperperimeter\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","b5df73a6":"np.random.seed(20)\n\n#setup random hyperparameter search.\nlog_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_grid,\n                                cv=5,\n                                n_iter=20,#try 20 different combinations of hyperparameters\n                                verbose=True)\n\n#fitting the model\nlog_reg.fit(X_train,y_train)","4da3c398":"#checking the best parameters for logistic regression\nlog_reg.best_params_","034c25a3":"print('The accuacy of Logistic regression using RandomizedSearchCV is: {:.2f}%'.format(log_reg.score(X_test,y_test)*100))","4d76c6d8":"np.random.seed(20)\n\n#setup random hyperparameter search.\nrand = RandomizedSearchCV(RandomForestClassifier(),\n                                param_distributions=rf_grid,\n                                cv=5,\n                                n_iter=20,#try 20 different combinations of hyperparameters\n                                verbose=True)\n\n#fitting the model\nrand.fit(X_train,y_train)","e22ac99c":"#checking the best parameters for random forest\nrand.best_params_","80b17759":"print('The accuracy of Random forest using RandomizedSearchCV is: {:.2f}%'.format(rand.score(X_test,y_test)*100))","d904243e":"log_search = GridSearchCV(LogisticRegression(),\n                          param_grid=log_grid,\n                          cv=5,\n                          verbose=True)\n#fitting the model\nlog_search.fit(X_train,y_train)","db661adc":"#checking the best hyperperimeter.\nlog_search.best_params_","6cacbacb":"print('The accuacy of Logistic regression using GridSearchCV is: {:.2f}%'.format(log_search.score(X_test,y_test)*100))","cb686da2":"#making predictions on test data\ny_preds = log_search.predict(X_test)\ny_preds","ede21879":"y_test.values","7201b59c":"from sklearn.metrics import plot_roc_curve\n#plotting the curve\nplot_roc_curve(log_search,X_test,y_test);","77fee1c5":"#displaying confusion matrix.\nprint(confusion_matrix(y_test,y_preds))","1d6bf60c":"#plotting confusion matrix.\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(log_search,X_test,y_test);","7c032d5f":"# classification report\nprint(classification_report(y_test, y_preds))","336e8558":"# our best hyperparameter\nlog_search.best_params_","5046a465":"from sklearn.model_selection import cross_val_score\n\n#lets use the best model with best hyperpararmeters.\nclf = LogisticRegression(C=0.615848211066026,\n                        solver='liblinear')","45b310f0":"#cross validation accuracy score\ncross_val_accuracy = cross_val_score(clf,X,y,cv=5,scoring='accuracy')\ncross_val_accuracy","2c46dbee":"#Let's find the average of the above 5 values.\ncross_val_accuracy = np.mean(cross_val_accuracy)\nprint('Cross validation accuracy score is: {:.2f}'.format(cross_val_accuracy))","fb928abd":"#cross validation precision score.\ncross_val_precision = cross_val_score(clf,X,y,cv=5,scoring='precision')\ncross_val_precision","86a2f316":"#Let's find the average of the above 5 values.\ncross_val_precision = np.mean(cross_val_precision)\nprint('Cross validation Precision score is: {:.2f}'.format(cross_val_precision))","86e80236":"#cross validation recall score.\ncross_val_recall = cross_val_score(clf,X,y,cv=5,scoring='recall')\ncross_val_recall","875b73f0":"#Let's find the average of the above 5 values.\ncross_val_recall = np.mean(cross_val_recall)\nprint('Cross validation recall score is: {:.2f}'.format(cross_val_recall))","b847e8b3":"#cross validation recall score.\ncross_val_f1 = cross_val_score(clf,X,y,cv=5,scoring='f1')\ncross_val_f1","c47dc291":"#Let's find the average of the above 5 values.\ncross_val_f1 = np.mean(cross_val_f1)\nprint('Cross validation f1 score is: {:.2f}'.format(cross_val_f1))","b5241903":"# Visualizing cross-validated metrics\ncross_val_metrics = pd.DataFrame({\"Accuracy\": cross_val_accuracy,\n                            \"Precision\": cross_val_precision,\n                            \"Recall\": cross_val_recall,\n                            \"F1\": cross_val_f1},\n                          index=[0])\ncross_val_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False);","9074e319":"clf.fit(X_train,y_train)\nfeatures_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeatures_dict","388e5fc6":"# Visualize feature importance\nfeatures_df = pd.DataFrame(features_dict, index=[0])\nfeatures_df.T.plot.bar(title=\"Feature Importance\", legend=False);","8091e240":"pd.crosstab(df[\"sex\"], df[\"target\"])","cb5b656a":"# Contrast slope (positive coefficient) with target\npd.crosstab(df[\"slope\"], df[\"target\"])","d37c81c6":"df.info() shows a quick insight to the number of missing values we have and what type of data we are working with.\n\nIn our case, there are no missing values and all of our columns are numerical in nature.","43d0e04a":"## Tuning a model with GridSearchCV\n\nThe difference between RandomizedSearchCV and GridSearchCV is where RandomizedSearchCV searches over a grid of hyperparameters performing n_iter combinations, GridSearchCV will test every single possible combination.\n\n* RandomizedSearchCV - tries n_iter combinations of hyperparameters and saves the best.\n* GridSearchCV - tries every single combination of hyperparameters and saves the best.","bf571750":"X values.","1ce37ffe":"From our data dictionary:\n1. cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n    \nIt's interesting the atypical agina (value 1) states it's not related to the heart but seems to have a higher ratio of participants with heart disease than not.\n\n\n## Correlation between independent variables\n\nNow compare all our independent variable because it gives the clear idea about which independent variable may or maynot have the impact on target variable. we can do this by using df.corr() function which gives the overalll result in a big table.","d5f6ace6":"We have make our hyperparameter dictionary. Now its time to tune our logistic regression model.\n\n### Tuning Logistic Regression","ea72a310":"According to the model, there's a positive correlation of 0.70, not as strong as sex and target but still more than 0.\n\nThis positive correlation means our model is picking up the pattern that as slope increases, so does the target value.","822a9c41":"Let's go more in depth to the problem we are solving. \n\n* [Hyperparameter Tuning](https:\/\/www.oreilly.com\/library\/view\/evaluating-machine-learning\/9781492048756\/ch04.html): A parameter whose value is set before the learning process begins.Changing these values may increase or decrease model performance.\n* [Feature Importance](https:\/\/machinelearningmastery.com\/calculate-feature-importance-with-python\/#:~:text=Feature%20importance%20refers%20to%20a,feature%20when%20making%20a%20prediction.): if we are dealing with the large dataset we must give importance to those who plays a significant role in the problem.\n* [Confusion Matrix](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html): Compares the predicted values with the true values in a tabular way.\n* [Cross Validation](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html):  Splits dataset into multiple parts and train and tests the model on each part and evaluates performance as an average. \n* [Precision](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_score.html): Proportion of true positives over total number of samples.\n* [Recall](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html): Proportion of true positives over total number of true positives and false negatives.\n* [F1 score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html): Combines precision and recall into one metric\n* [Classification Report](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html): Returns some of the main classification metrics such as precision, recall and f1-score.\n* [ROC curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html): Plot of true positive rate versus false positive rate.\n* [AUC curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.auc.html): The area underneath the ROC curve.\n\n## Hyperparameter tuning and cross validation\n\nTo test different hyperparameters, we could use a validation set but since we don't have much data, we'll use cross-validation.\nThe most common type of cross-validation is k-fold. It involves splitting  data into k-fold's and then testing a model on each. For example, let's say we had 5 folds (k = 5). This what it might look like.\n\n<img src='https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png'\/>\n\n","e02b0a18":"## Evaluating a model beyond Accuracy.","7bd1c148":"As best model always scores 1. our model achieve the score of 0.93. Till now we have done the great job.","666696b2":"* Precision - Indicates the proportion of positive identifications (model predicted class 1) which were actually correct. A model which produces no false positives has a precision of 1.0.\n* Recall - Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.\n* F1 score - A combination of precision and recall. A perfect model achieves an F1 score of 1.0.\n* Support - The number of samples each metric was calculated on.\n* Accuracy - The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0.\n* Macro avg - Short for macro average, the average precision, recall and F1 score between classes. Macro avg doesn\u2019t class imbalance into effort, so if you do have class imbalances, pay attention to this metric.\n* Weighted avg - Short for weighted average, the weighted average precision, recall and F1 score between classes. Weighted means each metric is calculated with respect to how many samples there are in each class. This metric will favour the majority class (e.g. will give a high value when one class out performs another due to having more samples).\n\n\nLet's see the same thing in action using cross validation.\n\n\n<img src='https:\/\/img.devrant.com\/devrant\/rant\/r_1393414_v3ymZ.jpg'\/>","f9d12e30":"\nIn this case, we get the same results as before since our grid only has a maximum of 20 different hyperparameter combinations.\n\nNote: If there are a large amount of hyperparameters combinations in your grid, GridSearchCV may take a long time to try them all out. This is why it's a good idea to start with RandomizedSearchCV, try a certain amount of combinations and then use GridSearchCV to refine them.","92da7dda":"It is a normal distribution also slightly a right skewed.\n\n## Heart Disease frequency by chest pain type\n\nLet's use the same process as used before.","0ad94ff0":"Now we have the tuned model now lets try with matrix.\n\n* ROC curve and AUC score - plot_roc_curve()\n* Confusion matrix - confusion_matrix()\n* Classification report - classification_report()\n* Precision - precision_score()\n* Recall - recall_score()\n* F1-score - f1_score()","7fadc762":"We can see, when sex is 0 (female), there are almost 3 times as many (72 vs. 24) people with heart disease (target = 1) than without.\n\nAnd then as sex increases to 1 (male), the ratio goes down to almost 1 to 1 (114 vs. 93) of people who have heart disease and who don't.\n\nWhat does this mean?\n\nIt means the model has found a pattern which reflects the data. Looking at these figures and this specific dataset, it seems if the patient is female, they're more likely to have heart disease.\n\nHow about a positive correlation?","004dc375":"## Exploratory Data Analysis(Data Exploration or EDA)\n\none we imported our data and obtained in tabular form now we need to explore the data. The main thing we need to do is become more and more familiar with the data. Compare to the different columns, compare those columns (features) with the target variables(labels).","5b548703":"It seems the younger someone is, the higher their max heart rate (dots are higher on the left of the graph) and the older someone is, the more green dots there are. But this may be because there are more dots all together on the right side of the graph (older participants).\n\nBoth of these are observational of course, but this is what we're trying to do, build an understanding of the data.","f553faec":"## Load Data\nThere are many different formats of data and there are lots of tools to visualize those data. In these notebook we are dealing with the data with comma separated values(.csv) format data. Pandas have the inbuilt functions to load and visualize the data in a dataframe.","7aa2bbaa":"### Confusion matrix\n\nThe confusion matrix is to see where our model predicts the right decision and where it predicts the wrong.","b870b3de":"1 = Heart disease,  \n0 = No heart disease\n\n1 = Male,  \n0 = Female","63d03921":"### Spliting our data into training and testing\n\nwe will use the scikit learn to split our data into train and test set. Before doing that we should remember not to train the model in a whole data.\nif we use our all data to train a model then how do we know that our model is performing well in unseen data.\nSo before modelling remember that we should use our training set to train our model and test set to test our model.\n","d43e5cfc":"We have 207 males and 96 females in our dataset.","766a88d2":"We have used 80% of our data to train and 20% to test.","a9cb16db":"## What we are gonna do throughout the notebook?\n\nWe have the dataset of heart disease UCI using this we are going to predict whether the patient have the heart disease or not. We will approach the problem step by step using this cycle.\n\n<img src='https:\/\/miro.medium.com\/max\/6608\/1*Gf0bWgr2wst9A1XR5gakLg.png'\/>\n\nFig: 6 step machine learning modelling framework.\n\nMainly the topics that are involved in this notebook are:\n\n* Explanatory Data Analysis: The process of going through the data set and finding more of it.\n* Model Training: Creating the model using the train data set to predict the correct outcome using test data set.\n* Model Evaluation: Evaluating a model using problem specific evaluation matrices.\n* Model Comparision: Comparing the several different model to predict the best one.\n* Model fine-tuning: Once we get the best model for our problem, how can we tune\/improve it?\n* Feature Importance: We are working to predict whether someone has heart disease or not? so we need to find that are there anything that are more important for prediction?\n* Cross Validation: We know that cross validation is the one of best choice to predict from the unseen data because the model can be trained with the many folds during the training. It is way better choice than a random selection.\n* Reporting what we've found: Presenting our work to others who are not familiar with these technical lines.\n\nWe will dive into them one by one. We are using following libraries:\n### Data Analysis\n* Numpy\n* Pandas\n* Matplotlib\n* Seaborn\n\n### Machine Learning and modelling\n* Scikit-learn\n\nSo to understand this notebook well you need to better familiar with the above mentioned Python Libraries. At the end of the notebook we are going to successfully predict the heart disease patient using the given features in the data set. We we also know that which colum matter the most to predict the disease.\n\n\n<img src = 'https:\/\/inteng-storage.s3.amazonaws.com\/img\/iea\/Xy6xeK3Wwr\/sizes\/heart-attack-ai-oxford_resize_md.jpg'\/>\n\n\n\n\n\n## 1.Problem Definition\n\nWe know that we are prediction whether the patient have heart disease or not. This is based on the binary class classification because it is only based of 'True' or 'False'. We're going to be using a number of differnet features (pieces of information) about a person. Our main goal is:\n\n`Given clinical parameters about a patient, can we predict whether or not they have heart disease?`\n\n\n## 2.Data\n\nThe original database contains 76 attributes, but here only 14 attributes will be used. Attributes (also called features) are the variables what we'll use to predict our target variable. Attributes and features are also referred to as independent variables and a target variable can be referred to as a dependent variable.\n\n**We use the independent variables to predict our dependent variable.**\n\nOr in our case, the independent variables are a patients different medical attributes and the dependent variable is whether or not they have heart disease.\n\n## 3.Evaluation\n\nAs machine learning is all about experimentation, we no need to worry about our first failure. Those who want to be successful in the very first attempt i think they don't have the long lasting career. So, we have to evaluate our project until we get the more nearly predicting output. Patence is the most important key in those moments.\n\n'If we approach near to 95% accuracy we will be near to the future prediction. So, stay calm and tune your model to get the more better accuracy.'\n\n\n## 4.Features\n\nFeatures plays the most important role to predict something that we don't know. So a good features always plays a better role for prediction.\n\nTo understand the features in a better way let's create a data dictionary.\n\n### Data Dictionary \n\nThis data dictionary describes about the data that we are dealing with. We don't describes those data that i am not going to use during the modelling process.\n\nThe following are the features that we are going to use to predict target variable(heart disease or no heart disease):\n1. age - age in years\n2. sex - (1=male,0=female)\n3. cp = chest pain\n    * Typical angina: chest pain related decrease blood supply to the heart\n    * Atypical angina: chest pain not related to heart\n    * Non-anginal pain: typically esophageal spasms (non heart related)\n    * Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n    * anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest\n    * looks at stress of heart during excercise\n    * unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)\n\n\n## Preparing Tools\n\nPython doesnot provide all the support to solve these types of problems. So, before starting the project we have to import the necessarily library to do such type of tasks. I am going to import the necessary library in the beginning and i will import the other libraries when the problem arieses and python can not tackle it.\n\n* [Pandas](https:\/\/pandas.pydata.org\/) for data analysis\n* [Numpy](https:\/\/numpy.org\/) for numerical operations\n* [Matplotlib](https:\/\/matplotlib.org\/) \/ [Seaborn](https:\/\/seaborn.pydata.org\/) for plotting and visualizations\n* [Scikit-learn](https:\/\/scikit-learn.org\/stable\/) for machine learning","1368afc5":"### ROC Curve and AUC score","31c317ed":"Looking at this it might not make much sense. But these values are how much each feature contributes to how a model makes a decision on whether patterns in a sample of patients health data leans more towards having heart disease or not.","58799a1e":"Boom!! we are using 242 samples for training purpose and 61 samples for testing purpose.\n\n\n### Choosing the right model.\n\nI have my data prepared and everything is going fine upto now. \nNow its time to select the right model for the problem. I am gonna tackle this problem by using the following algorithms: \n1. Logistic Regression\n2. K-Nearest Neighbors\n3. Random Forest\n\n### Why I am using these?\n\n<img src='https:\/\/scikit-learn.org\/stable\/_static\/ml_map.png'>\n\nfig: Scikit-learn workflow\n\n\n\nBy looking at the above picture we can clearly see that we have less data in our database. So these could be the best choices at that condition.\n\nSince our dataset is relatively small, we can experiment to find algorithm performs best.\n\nAll of the algorithms in the Scikit-Learn library use the same functions, for training a model, model.fit(X_train, y_train) and for scoring a model model.score(X_test, y_test). score() returns the ratio of correct predictions (1.0 = 100% correct).","d8ead94e":"Another way to get some quick insights from data is df.describe() function.","c2d5dd46":"## Feature Importance\n\n\"which features contributing most to the outcomes of the model?\"\n","208497e3":"In the beginning when we use Logistic regression and Random forest  without cross validation, we got the accuracy score of 83.61% and 85.25% respectively. After tuning them we got the better result as 90.16% for both. This is the reason beside doing hyperparameter tuning.","9abaa38b":"From the above table we can see that, there are about 100 women and 72 of them are positive value which is about 75% of the total women. on the other hand there are about 200 men out of them about 100 of them have the positice value which is about 50% of the total men.\n\nAveraging these two values, we can assume, based on no other parameters, if there's a person, there's a 62.5% chance they have heart disease.","691eecdb":"Let's find some matrix using cross validation.","689d7822":"Now its time to do the same with Random Forest.\n### Tuning Random Forest","b148d63b":"value_counts() allows you to show how many times each of the values of a categorical column appear.","9a1bde82":"Y values.","b7cafa75":"## Tuning models with with RandomizedSearchCV\n\nwe have seen that k-Nearest Neighbour have less accuracy as compared to Logistic regression and random forest. So let ignore that Knn for now and start with remaining two.\n\nLet's create a hyperparameter dictionary  for each and test them out.\n","24b95548":"# Predicting Heart Disease Using Machine Learning\n\nThis notebook is based on predicting heart disease with some foundation machine learning and data science concepts. This is the classification problem.\n\nIt is intended to be an end-to-end example of what a data science and machine learning proof of concept might look like.\n\n\n## What is Classification? \n\nCategorizing the given data into the classes wheather the date is structured or unstructured. Classification with two outcomes is binary classification, Classification with more than two outcomes is Multi class Classification and the problem pattern belongs to more than one class is Multi label Classification. \n\nIn this notebook we are dealing with the binary classification Problem.","8faddcde":"### Model Comparison\n\nFrom the above randomized selected data we can see that Logistic regression and Random Forest have better accuracy compared to K-Nearnest Neighbors.","8e477090":"Here we notice that some are negative and some are positive.\n\nThe larger the value (bigger bar), the more the feature contributes to the models decision.\n\nIf the value is negative, it means there's a negative correlation. And vice versa for positive values.\n\nFor example, the sex attribute has a negative value of -1.3, which means as the value for sex increases, the target value decreases.\n\nWe can see this by comparing the sex column to the target column.","fb856f3e":"Much better. A higher positive value means a potential positive correlation (increase) and a higher negative value means a potential negative correlation (decrease). This much of data analysis gives us a indepth knowledge about heart disease data.\n\nNow it's time to model.\n\n\n## 5.Modeling\n\nWe have get the idea from the data. Now its time to built a model using machine learning. We will use the catagorical features to predict the label. In our case we have 13 features and 1 label. so to do this we have to split the data into two parts. One is for features and other for labels.","1429c575":"## Making Crosstab Visuals\n\nLet's Compare the target and sex columns in a visual way.","5fa09a10":"## Age vs Max Heart Rate for Heart Disease\n\nLet's first combine a couple of independent variable like age, thalach and compared with our target valiable.","02a02787":"We have got our cross validated metrics, Now visualize them to compare how they perform.","ed8e309e":"### Classification report\n\nA classification report give us information of the precision and recall of our model for each class."}}