{"cell_type":{"b63eade4":"code","d0324403":"code","8fab7458":"code","ec483d5b":"code","6788e3d2":"code","fbfebee9":"code","132408e4":"code","25f7b5d0":"markdown","2e194ea9":"markdown","c48c4c68":"markdown","7bcd8ce3":"markdown","0a4da04f":"markdown","a190fcaa":"markdown","ec6fd4ab":"markdown","bedbed33":"markdown"},"source":{"b63eade4":"import numpy as np\nimport pandas as pd","d0324403":"def map_per_image(label, predictions):\n    \"\"\"Computes the precision score of one image.\n\n    Parameters\n    ----------\n    label : string\n            The true label of the image\n    predictions : list\n            A list of predicted elements (order does matter, 5 predictions allowed per image)\n\n    Returns\n    -------\n    score : double\n    \"\"\"    \n    try:\n        return 1 \/ (predictions[:5].index(label) + 1)\n    except ValueError:\n        return 0.0\n\ndef map_per_set(labels, predictions):\n    \"\"\"Computes the average over multiple images.\n\n    Parameters\n    ----------\n    labels : list\n             A list of the true labels. (Only one true label per images allowed!)\n    predictions : list of list\n             A list of predicted elements (order does matter, 5 predictions allowed per image)\n\n    Returns\n    -------\n    score : double\n    \"\"\"\n    return np.mean([map_per_image(l, p) for l,p in zip(labels, predictions)])\n","8fab7458":"#                   (true, [predictions])\nassert map_per_image('x', []) == 0.0\nassert map_per_image('x', ['y']) == 0.0\nassert map_per_image('x', ['x']) == 1.0\nassert map_per_image('x', ['x', 'y', 'z']) == 1.0\nassert map_per_image('x', ['y', 'x']) == 0.5\nassert map_per_image('x', ['y', 'x', 'x']) == 0.5\nassert map_per_image('x', ['y', 'z']) == 0.0\nassert map_per_image('x', ['y', 'z', 'x']) == 1\/3\nassert map_per_image('x', ['y', 'z', 'a', 'b', 'c']) == 0.0\nassert map_per_image('x', ['x', 'z', 'a', 'b', 'c']) == 1.0\nassert map_per_image('x', ['y', 'z', 'a', 'b', 'x']) == 1\/5\nassert map_per_image('x', ['y', 'z', 'a', 'b', 'c', 'x']) == 0.0\n\nassert map_per_set(['x'], [['x', 'y']]) == 1.0\nassert map_per_set(['x', 'z'], [['x', 'y'], ['x', 'y']]) == 1\/2\nassert map_per_set(['x', 'z'], [['x', 'y'], ['x', 'y', 'z']]) == 2\/3\nassert map_per_set(['x', 'z', 'k'], [['x', 'y'], ['x', 'y', 'z'], ['a', 'b', 'c', 'd', 'e']]) == 4\/9","ec483d5b":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntrain_df.head()","6788e3d2":"labels = train_df['Id'].values\nlabels","fbfebee9":"# 5 most common Id\n# sample_pred = train_df['Id'].value_counts().nlargest(5).index.tolist()\nsample_pred = ['new_whale', 'w_23a388d', 'w_9b5109b', 'w_9c506f6', 'w_0369a5c']\npredictions = [sample_pred for i in range(len(labels))]\nsample_pred","132408e4":"map_per_set(labels, predictions)","25f7b5d0":"## Precision @k\nPrecision at cutoff `k`, `P(k)`, is simply the precision calculated by considering only the subset of your predictions from rank 1 through `k`.\n\nFor example:\n\n| true  | predicted   | k  | P(k) |\n|:-:|:-:|:-:|:-:|\n| [x]  | [x, ?, ?, ?, ?]   | 1  | 1.0  |\n| [x]  | [?, x, ?, ?, ?]   | 1  | 0.0  |\n| [x]  | [?, x, ?, ?, ?]   | 2  | $$1\\over2$$  |\n| [x]  | [?, ?, x, ?, ?]   | 2  | 0.0  |\n| [x]  | [?, ?, x, ?, ?]   | 3  | $$1\\over3$$  |\n\nwhere `x` is the correct and `?` is incorrect prediction. ","2e194ea9":"## Precision @5 per image\nI think the evaluation metric in the competition's description is a bit confusing. According to @inversion's [answer](https:\/\/www.kaggle.com\/c\/humpback-whale-identification\/discussion\/73303#431164) in [this discussion](https:\/\/www.kaggle.com\/c\/humpback-whale-identification\/discussion\/73303):\n> the calculation would stop after the first occurrence of the correct whale, so `P(1) = 1`. So, a prediction that is `correct` `incorrect` `incorrect` `incorrect` `incorrect` also scores `1`.\n\nSo we don't have to sum up to 5, only up to the first correct answer. In this competition there is only one correct (`TP`) answer per image, so the possible precision scores per image are either `0` or `P(k)=1\/k`.\n\n| true  | predicted   | k  | Image score |\n|:-:|:-:|:-:|:-:|:-:|\n| [x]  | [x, ?, ?, ?, ?]   | 1  | 1.0  |\n| [x]  | [?, x, ?, ?, ?]   | 2  | 0 + 1\/2 = 0.5 |\n| [x]  | [?, ?, x, ?, ?]   | 3  | 0\/1 + 0\/2 + 1\/3  = 0.33 |\n| [x]  | [?, ?, ?, x, ?]   | 4  | 0\/1 + 0\/2 + 0\/3 + 1\/4  = 0.25 |\n| [x]  | [?, ?, ?, ?, x]   | 5  | 0\/1 + 0\/2 + 0\/3 + 0\/4 + 1\/5  = 0.2 |\n| [x]  | [?, ?, ?, ?, ?]   | 5  | 0\/1 + 0\/2 + 0\/3 + 0\/4 + 0\/5  = 0.0 |\n\nwhere `x` is the correct and `?` is incorrect prediction. \n\n","c48c4c68":"# Mean Average Precision (MAP)\nSubmissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):\n\n$$MAP@5 = {1 \\over U} \\sum_{u=1}^{U} \\sum_{k=1}^{min(n,5)}P(k)$$\n\nwhere `U` is the number of images, `P(k)` is the precision at cutoff `k` and `n` is the number of predictions per image.\n","7bcd8ce3":"# Implementation","0a4da04f":"## Leaderboard score\nThe final score is simply the average over the scores of the images.","a190fcaa":"## Precision\n\n\nPrecision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.\n\nIn a classification task, the precision for a class is the number of true positives (i.e. the number of items correctly labeled as belonging to the positive class) divided by the total number of elements labeled as belonging to the positive class (i.e. the sum of true positives and false positives, which are items incorrectly labeled as belonging to the class).\n\n\n$$ P = { \\#\\ of\\ correct\\ predictions\\over \\#\\ of\\ all\\ predictions  } = {TP \\over (TP + FP)}$$\n","ec6fd4ab":"**Thanks for reading.** ","bedbed33":"In this notebook I'll try to implement the scoring metric. I am still not 100% sure about my interpretation, so please leave me a comment if you find something wrong.\n\n-------------------------------"}}