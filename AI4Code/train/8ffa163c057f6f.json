{"cell_type":{"43aec625":"code","bc10d20f":"code","3f0c370b":"code","8308e164":"code","9b5cf21f":"code","dcdb0819":"code","e4097eb9":"code","a39871bd":"code","c2e51371":"code","87b74971":"code","bab95293":"code","aec57023":"code","7a0caf99":"code","5c701610":"code","8058cc3b":"code","2f55086a":"code","5c03ae7b":"code","c3779003":"code","465576dc":"code","679fbeba":"code","b6f03d68":"markdown","1983e646":"markdown","4881abb2":"markdown","8469e958":"markdown","b50f1520":"markdown","9309f7ac":"markdown","2227d49f":"markdown","dfaf2285":"markdown","81835712":"markdown","dae88ed3":"markdown","38458c65":"markdown","f5314933":"markdown","2a230984":"markdown","d62f0f7c":"markdown"},"source":{"43aec625":"from keras.models import Model\nfrom keras.layers import Embedding, Dense, Input, RepeatVector, concatenate, Dropout\nfrom keras.layers.recurrent import LSTM\nfrom keras.preprocessing.sequence import pad_sequences\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python.ops.rnn import dynamic_rnn\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras","bc10d20f":"NUM_ALGORITHME = 1\n\nRNN_SIZE = 128\nBATCH_SIZE = 64\nEPOCHS = 1\nKEEP_PROBABILITY = 0.5\nOPTIMIZER_TYPE = 'adam'\nLEARNING_RATE = 0.001\nEMBEDDING_SIZE = 100\ndata_dir_path = '..\/input'\ndata_file = '\/text-sumarization\/articles3_token_stop.csv'\nglove_file = '\/glove-global-vectors-for-word-representation\/glove.6B.' + str(EMBEDDING_SIZE) + 'd.txt'","3f0c370b":"VERBOSE = 1\nNUM_LAYERS = 3\nMAX_INPUT_SEQ_LENGTH = 500\nMAX_TARGET_SEQ_LENGTH = 50\nMAX_INPUT_VOCAB_SIZE = 5000\nMAX_TARGET_VOCAB_SIZE = 2000\nNUM_SAMPLES = 10000\nMAX_DECODER_SEQ_LENGTH = 4","8308e164":"def def_keras_optimizer():\n    if OPTIMIZER_TYPE == 'sgd':\n        # default LEARNING_RATE = 0.01\n        keras_optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, momentum=0.0, decay=0.0, nesterov=False)\n    elif OPTIMIZER_TYPE == 'rmsprop':\n        # default LEARNING_RATE = 0.001\n        keras_optimizer = keras.optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=None, decay=0.0)\n    elif OPTIMIZER_TYPE == 'adagrad':\n        # default LEARNING_RATE = 0.01\n        keras_optimizer = keras.optimizers.Adagrad(lr=LEARNING_RATE, epsilon=None, decay=0.0)\n    elif OPTIMIZER_TYPE == 'adadelta':\n        # default LEARNING_RATE = 1.0\n        keras_optimizer = keras.optimizers.Adadelta(lr=LEARNING_RATE, rho=0.95, epsilon=None, decay=0.0)\n    else:   # OPTIMIZER_TYPE == 'adam':\n        # default LEARNING_RATE = 0.001\n        keras_optimizer = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n                                                amsgrad=False)\n\n    return keras_optimizer\n\n\ndef def_tf_optimizer(lr):\n    if OPTIMIZER_TYPE == 'sgd':\n        # default LEARNING_RATE = 0.01\n        tf_optimizer = tf.train.GradientDescentOptimizer(lr)\n    elif OPTIMIZER_TYPE == 'rmsprop':\n        # default LEARNING_RATE = 0.001\n        tf_optimizer = tf.train.RMSPropOptimizer(lr)\n    elif OPTIMIZER_TYPE == 'adagrad':\n        # default LEARNING_RATE = 0.01\n        tf_optimizer = tf.train.AdagradOptimizer(lr)\n    elif OPTIMIZER_TYPE == 'adadelta':\n        # default LEARNING_RATE = 1.0\n        tf_optimizer = tf.train.AdadeltaOptimizer(lr)\n    else:   # OPTIMIZER_TYPE == 'adam':\n        # default LEARNING_RATE = 0.001\n        tf_optimizer = tf.train.AdamOptimizer(lr)\n\n    return tf_optimizer","9b5cf21f":"def preprocess_data_char(inputs, targets):\n    input_texts = []\n    target_texts = []\n    characters = set()\n\n    for line in inputs[:NUM_SAMPLES]:\n        input_texts.append(line)\n        for char in line:\n            if char not in characters:\n                characters.add(char)\n    for line in targets[:NUM_SAMPLES]:\n        line = '\\t' + line + '\\n'\n        target_texts.append(line)\n        for char in line:\n            if char not in characters:\n                characters.add(char)\n\n    characters = sorted(list(characters))\n    num_tokens = len(characters)\n    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n\n    char2idx = dict([(char, i) for i, char in enumerate(characters)])\n    idx2char = dict((i, char) for char, i in char2idx.items())\n\n    encoder_input_data = np.zeros(\n        (len(input_texts), max_encoder_seq_length, num_tokens), dtype='float32')\n    decoder_input_data = np.zeros(\n        (len(input_texts), max_decoder_seq_length, num_tokens), dtype='float32')\n    decoder_target_data = np.zeros(\n        (len(input_texts), max_decoder_seq_length, num_tokens), dtype='float32')\n\n    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n        for t, char in enumerate(input_text):\n            encoder_input_data[i, t, char2idx[char]] = 1.\n        for t, char in enumerate(target_text):\n            decoder_input_data[i, t, char2idx[char]] = 1.\n            if t > 0:\n                decoder_target_data[i, t - 1, char2idx[char]] = 1.\n    config = dict()\n    config['num_tokens'] = num_tokens\n    config['encoder_input_data'] = encoder_input_data\n    config['decoder_input_data'] = decoder_input_data\n    config['decoder_target_data'] = decoder_target_data\n    config['char2idx'] = char2idx\n    config['idx2char'] = idx2char\n    config['max_encoder_seq_length'] = max_encoder_seq_length\n    config['max_decoder_seq_length'] = max_decoder_seq_length\n\n    return config","dcdb0819":"class Seq2SeqChar(object):\n    def __init__(self, config):\n        self.num_tokens = config['num_tokens']\n        self.encoder_input_data = config['encoder_input_data']\n        self.decoder_input_data = config['decoder_input_data']\n        self.decoder_target_data = config['decoder_target_data']\n        self.char2idx = config['char2idx']\n        self.idx2char = config['idx2char']\n        self.max_encoder_seq_length = config['max_encoder_seq_length']\n        self.max_decoder_seq_length = config['max_decoder_seq_length']\n\n        encoder_inputs = Input(shape=(None, self.num_tokens))\n        encoder = LSTM(RNN_SIZE, return_state=True)\n        encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n        encoder_states = [state_h, state_c]\n        decoder_inputs = Input(shape=(None, self.num_tokens))\n        decoder_lstm = LSTM(RNN_SIZE, return_sequences=True, return_state=True)\n        decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                                             initial_state=encoder_states)\n        decoder_dense = Dense(self.num_tokens, activation='softmax')\n        decoder_outputs = decoder_dense(decoder_outputs)\n\n        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n        optimizer = def_keras_optimizer()\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n        model.fit([self.encoder_input_data, self.decoder_input_data], self.decoder_target_data, batch_size=BATCH_SIZE,\n                  epochs=EPOCHS, validation_split=0.2)\n        encoder_model = Model(encoder_inputs, encoder_states)\n\n        decoder_state_input_h = Input(shape=(RNN_SIZE,))\n        decoder_state_input_c = Input(shape=(RNN_SIZE,))\n        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n        decoder_outputs, state_h, state_c = decoder_lstm(\n            decoder_inputs, initial_state=decoder_states_inputs)\n        decoder_states = [state_h, state_c]\n        decoder_outputs = decoder_dense(decoder_outputs)\n        decoder_model = Model(\n            [decoder_inputs] + decoder_states_inputs,\n            [decoder_outputs] + decoder_states)\n\n        self.encoder_model = encoder_model\n        self.decoder_model = decoder_model\n\n    def summarize(self, input_seq):\n        # Encode the input as state vectors.\n        encoder_input = np.zeros((1, self.max_encoder_seq_length, self.num_tokens), dtype='float32')\n        for i, char in enumerate(input_seq):\n            encoder_input[0, i, self.char2idx[char]] = 1\n\n        states_value = self.encoder_model.predict(encoder_input)\n\n        target_seq = np.zeros((1, 1, self.num_tokens))\n        target_seq[0, 0, self.char2idx['\\t']] = 1.\n        stop_condition = False\n        decoded_sentence = ''\n        while not stop_condition:\n            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n\n            sampled_token_index = int(np.argmax(output_tokens[0, -1, :]))\n            sampled_char = self.idx2char[sampled_token_index]\n            decoded_sentence += sampled_char\n\n            if (sampled_char == '\\n' or\n                    len(decoded_sentence) > self.max_decoder_seq_length):\n                stop_condition = True\n\n            target_seq = np.zeros((1, 1, self.num_tokens))\n            target_seq[0, 0, sampled_token_index] = 1.\n\n            states_value = [h, c]\n\n        return decoded_sentence","e4097eb9":"def main_seq2seq_char():\n    df = pd.read_csv(data_dir_path + data_file)\n    targets = df['titre']\n    inputs = df['article']\n    config = preprocess_data_char(inputs, targets)\n    summarize = Seq2SeqChar(config)\n    for i in np.random.permutation(np.arange(len(inputs)))[0:20]:\n        x = inputs[i]\n        decoded_sentence = summarize.summarize(x)\n        print('-')\n        print('Input sentence:', x)\n        print('Decoded sentence:', decoded_sentence)\n\n\ndef get_accuracy(target, logits):\n    max_seq = max(target.shape[1], logits.shape[1])\n    if max_seq - target.shape[1]:\n        target = np.pad(\n            target,\n            [(0, 0), (0, max_seq - target.shape[1])],\n            'constant')\n    if max_seq - logits.shape[1]:\n        logits = np.pad(\n            logits,\n            [(0, 0), (0, max_seq - logits.shape[1])],\n            'constant')\n\n    return np.mean(np.equal(target, logits))","a39871bd":"def preprocess_data(input_texts, target_texts):\n    vocab = set()\n    vocab.add('<PAD>')\n    vocab.add('END')\n    vocab.add('<UNK>')\n    vocab.add('START')\n\n    for input_text in input_texts:\n        for word in input_text:\n            if word not in vocab:\n                vocab.add(word)\n    for target_text in target_texts:\n        for word in target_text:\n            if word not in vocab:\n                vocab.add(word)\n\n    vocab = sorted(list(vocab))\n    word2idx = dict([(word, i) for i, word in enumerate(vocab)])\n    idx2word = dict((i, word) for word, i in word2idx.items())\n\n    source_text_id = []\n    target_text_id = []\n\n    for i in range(len(input_texts)):\n        source_sentence = input_texts[i]\n        target_sentence = target_texts[i]\n\n        source_token_id = []\n        target_token_id = []\n\n        for index, token in enumerate(source_sentence):\n            if token != \"\":\n                source_token_id.append(word2idx[token])\n\n        for index, token in enumerate(target_sentence):\n            if token != \"\":\n                target_token_id.append(word2idx[token])\n\n        target_token_id.append(word2idx['END'])\n\n        source_text_id.append(source_token_id)\n        target_text_id.append(target_token_id)\n\n    config = dict()\n    config['word2idx'] = word2idx\n    config['idx2word'] = idx2word\n    config['source_text_id'] = source_text_id\n    config['target_text_id'] = target_text_id\n\n    return config","c2e51371":"class TfSeq2Seq(object):\n\n    def __init__(self, config):\n        self.word2idx = config['word2idx']\n        self.idx2word = config['idx2word']\n        self.source_text_id = config['source_text_id']\n        self.target_text_id = config['target_text_id']\n\n    def encoding_layer(self, input_data, keep_prob):\n        embed = tf.contrib.layers.embed_sequence(input_data,\n                                                 vocab_size=len(self.word2idx),\n                                                 embed_dim=EMBEDDING_SIZE)\n\n        stacked_cells = tf.contrib.rnn.MultiRNNCell(\n            [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(RNN_SIZE), keep_prob) for _ in\n             range(NUM_LAYERS)])\n\n        outputs, state = dynamic_rnn(stacked_cells, embed, dtype=tf.float32)\n        return outputs, state\n\n    def process_decoder_input(self, targets):\n        go_id = self.word2idx['START']\n\n        after_slice = tf.strided_slice(targets, [0, 0], [BATCH_SIZE, -1], [1, 1])\n        after_concat = tf.concat([tf.fill([BATCH_SIZE, 1], go_id), after_slice], 1)\n\n        return after_concat\n\n    def decoding_layer(self, dec_input, encoder_state, keep_prob, target_sequence_length, max_target_len):\n        target_vocab_size = len(self.word2idx)\n        dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, EMBEDDING_SIZE]))\n        dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n\n        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(RNN_SIZE) for _ in range(NUM_LAYERS)])\n\n        with tf.variable_scope(\"decode\"):\n            output_layer = tf.layers.Dense(target_vocab_size)\n            dec_cell_train = tf.contrib.rnn.DropoutWrapper(cells, output_keep_prob=keep_prob)\n\n            helper_train = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_sequence_length)\n\n            decoder_train = tf.contrib.seq2seq.BasicDecoder(dec_cell_train, helper_train, encoder_state,\n                                                            output_layer)\n\n            train_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_train, impute_finished=True,\n                                                                   maximum_iterations=max_target_len)\n\n        with tf.variable_scope(\"decode\", reuse=True):\n            start_sequence_id = self.word2idx['START']\n            end_sequence_id = self.word2idx['END']\n\n            dec_cell_infer = tf.contrib.rnn.DropoutWrapper(cells, output_keep_prob=keep_prob)\n\n            helper_infer = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, tf.fill([BATCH_SIZE],\n                                                                                            start_sequence_id),\n                                                                    end_sequence_id)\n\n            decoder_infer = tf.contrib.seq2seq.BasicDecoder(dec_cell_infer, helper_infer, encoder_state, output_layer)\n\n            infer_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_infer, impute_finished=True,\n                                                                   maximum_iterations=max_target_len)\n\n        return train_output, infer_output\n\n    def seq2seq_model(self, input_data, targets, keep_prob, target_sequence_length, max_target_len):\n\n        enc_outputs, enc_states = self.encoding_layer(input_data, keep_prob)\n\n        dec_input = self.process_decoder_input(targets)\n\n        train_output, infer_output = self.decoding_layer(dec_input, enc_states, keep_prob, target_sequence_length,\n                                                         max_target_len)\n\n        return train_output, infer_output\n\n    def get_batches(self, sources, targets):\n        for batch_i in range(0, len(sources) \/\/ BATCH_SIZE):\n            start_i = batch_i * BATCH_SIZE\n\n            sources_batch = sources[start_i:start_i + BATCH_SIZE]\n            targets_batch = targets[start_i:start_i + BATCH_SIZE]\n\n            pad_int = self.word2idx['<PAD>']\n            max_sentence_source = max([len(sentence) for sentence in sources_batch])\n            max_sentence_target = max([len(sentence) for sentence in targets_batch])\n            pad_sources_batch = np.array([sentence + [pad_int] * (max_sentence_source - len(sentence)) for sentence in\n                                          sources_batch])\n            pad_targets_batch = np.array([sentence + [pad_int] * (max_sentence_target - len(sentence)) for sentence in\n                                          targets_batch])\n\n            pad_targets_lengths = []\n            for target in pad_targets_batch:\n                pad_targets_lengths.append(len(target))\n\n            pad_source_lengths = []\n            for source in pad_sources_batch:\n                pad_source_lengths.append(len(source))\n\n            yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n\n    def fit(self):\n        train_graph = tf.Graph()\n        with train_graph.as_default():\n            inputs = tf.placeholder(tf.int32, [None, None], name='input')\n            input_data = tf.reverse(inputs, [-1])\n            targets = tf.placeholder(tf.int32, [None, None], name='targets')\n            target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n            max_target_len = tf.reduce_max(target_sequence_length)\n            lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n            keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n            train_logits, inference_logits = self.seq2seq_model(input_data, targets, keep_prob, target_sequence_length,\n                                                                max_target_len)\n            training_logits = tf.identity(train_logits.rnn_output, name='logits')\n            inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n            masks = tf.sequence_mask(target_sequence_length, max_target_len, dtype=tf.float32, name='masks')\n\n            with tf.name_scope(\"optimization\"):\n\n                cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n                # optimizer = tf.train.AdamOptimizer(lr_rate)\n                optimizer = def_tf_optimizer(lr_rate)\n                gradients = optimizer.compute_gradients(cost)\n                capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not\n                                    None]\n                train_op = optimizer.apply_gradients(capped_gradients)\n\n            train_source = self.source_text_id[BATCH_SIZE:]\n            train_target = self.target_text_id[BATCH_SIZE:]\n            valid_source = self.source_text_id[:BATCH_SIZE]\n            valid_target = self.target_text_id[:BATCH_SIZE]\n\n            (valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths) = \\\n                next(self.get_batches(valid_source, valid_target))\n\n            sess = tf.Session(graph=train_graph)\n            sess.run(tf.global_variables_initializer())\n\n            for epoch_i in range(EPOCHS):\n                for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n                        self.get_batches(train_source, train_target)):\n                    _, loss = sess.run([train_op, cost], {inputs: source_batch, targets: target_batch,\n                                                          lr_rate: LEARNING_RATE,\n                                                          target_sequence_length: targets_lengths,\n                                                          keep_prob: KEEP_PROBABILITY})\n\n                    if batch_i > 0:\n                        if batch_i % 5 == 0:\n                            batch_train_logits = sess.run(\n                             inference_logits, {input_data: source_batch, target_sequence_length: targets_lengths,\n                                                keep_prob: 1.0})\n\n                            batch_valid_logits = sess.run(\n                                inference_logits, {inputs: valid_sources_batch,\n                                                   target_sequence_length:  valid_targets_lengths,\n                                                   keep_prob: 1.0})\n\n                            train_acc = get_accuracy(target_batch, batch_train_logits)\n                            valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n\n                            print('Epoch {:>3} Batch {:>4}\/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}'\n                                  ', Loss: {:>6.4f}'.format(epoch_i, batch_i, len(self.source_text_id) \/\/ BATCH_SIZE,\n                                                            train_acc, valid_acc, loss))\n        return target_sequence_length, keep_prob, inputs, sess, inference_logits\n\n    def summarize(self, inputs, text, sess, inference_logits, target_sequence_length, keep_prob):\n        translate_sentence = []\n        for word in text:\n            if word in self.word2idx:\n                translate_sentence.append(self.word2idx[word])\n            else:\n                translate_sentence.append(self.word2idx['<UNK>'])\n\n        translate_logits = sess.run(inference_logits, {inputs: [translate_sentence] * BATCH_SIZE,\n                                                       target_sequence_length: [len(\n                                                           translate_sentence) * 2] * BATCH_SIZE,\n                                                       keep_prob: 1.0})[0]\n\n        return \" \".join([self.idx2word[i] for i in translate_logits])","87b74971":"def main_tf_seq2seq():\n    train = pd.read_csv(data_dir_path + data_file)\n    resumes = []\n    articles = []\n    for resume in train[train.columns[0]].values:\n        resumes.append(resume.split(' '))\n    for article in train[train.columns[1]].values:\n        articles.append(article.split(' '))\n\n    config = preprocess_data(articles[:NUM_SAMPLES], resumes[:NUM_SAMPLES])\n    summarizer = TfSeq2Seq(config)\n    target_sequence_length, keep_prob, inputs, sess, inference_logits = summarizer.fit()\n    for i in np.random.permutation(np.arange(len(articles)))[0:20]:\n        x = articles[i]\n        headline = summarizer.summarize(inputs, x, sess, inference_logits, target_sequence_length, keep_prob)\n        print('Article: ', articles)\n        print('Generated Headline: ', headline)\n        print('Original Article: ', x)","bab95293":"def fit_text(x, y, input_seq_max_length=None, target_seq_max_length=None):\n    if input_seq_max_length is None:\n        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n    if target_seq_max_length is None:\n        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n    input_counter = Counter()\n    target_counter = Counter()\n    max_input_seq_length = 0\n    max_target_seq_length = 0\n\n    for line in x:\n        text = [word for word in line.split(' ')]\n        for i, word in enumerate(text):\n            if word == '':\n                del text[i]\n        seq_length = len(text)\n        if seq_length > input_seq_max_length:\n            text = text[0:input_seq_max_length]\n            seq_length = len(text)\n        for word in text:\n            input_counter[word] += 1\n        max_input_seq_length = max(max_input_seq_length, seq_length)\n\n    for i, line in enumerate(y):\n\n        line2 = 'START ' + str(line) + ' END'\n        text = [word for word in line2.split(' ')]\n        for j, word in enumerate(text):\n            if word == '':\n                del text[j]\n        seq_length = len(text)\n        if seq_length > target_seq_max_length:\n            text = text[0:target_seq_max_length]\n            seq_length = len(text)\n        for word in text:\n            target_counter[word] += 1\n            max_target_seq_length = max(max_target_seq_length, seq_length)\n\n    input_word2idx = dict()\n    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n        input_word2idx[word[0]] = idx + 2\n    input_word2idx['PAD'] = 0\n    input_word2idx['UNK'] = 1\n    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n\n    target_word2idx = dict()\n    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n        target_word2idx[word[0]] = idx + 1\n    target_word2idx['UNK'] = 0\n\n    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n\n    num_input_tokens = len(input_word2idx)\n    num_target_tokens = len(target_word2idx)\n\n    config = dict()\n    config['input_word2idx'] = input_word2idx\n    config['input_idx2word'] = input_idx2word\n    config['target_word2idx'] = target_word2idx\n    config['target_idx2word'] = target_idx2word\n    config['num_input_tokens'] = num_input_tokens\n    config['num_target_tokens'] = num_target_tokens\n    config['max_input_seq_length'] = max_input_seq_length\n    config['max_target_seq_length'] = max_target_seq_length\n\n    return config","aec57023":"def transform_input_text(texts, input_word2idx, max_input_seq_length):\n    temp = []\n    for line in texts:\n        x = []\n        for word in line.lower().split(' '):\n            wid = 1\n            if word in input_word2idx:\n                wid = input_word2idx[word]\n            x.append(wid)\n            if len(x) >= max_input_seq_length:\n                break\n        temp.append(x)\n    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n\n    print(temp.shape)\n    return temp","7a0caf99":"def transform_target_encoding(texts, max_target_seq_length):\n    temp = []\n    for line in texts:\n        x = []\n        line2 = 'START ' + line.lower() + ' END'\n        for word in line2.split(' '):\n            x.append(word)\n            if len(x) >= max_target_seq_length:\n                break\n        temp.append(x)\n\n    temp = np.array(temp)\n    print(temp.shape)\n    return temp","5c701610":"class RecursiveRNN(object):\n\n    def __init__(self, config):\n        self.num_input_tokens = config['num_input_tokens']\n        self.max_input_seq_length = config['max_input_seq_length']\n        self.num_target_tokens = config['num_target_tokens']\n        self.max_target_seq_length = config['max_target_seq_length']\n        self.input_word2idx = config['input_word2idx']\n        self.input_idx2word = config['input_idx2word']\n        self.target_word2idx = config['target_word2idx']\n        self.target_idx2word = config['target_idx2word']\n        self.config = config\n\n        if NUM_ALGORITHME == 3:\n            inputs1 = Input(shape=(self.max_input_seq_length,))\n            am1 = Embedding(self.num_input_tokens, 128)(inputs1)\n            am2 = LSTM(128)(am1)\n\n            inputs2 = Input(shape=(self.max_target_seq_length,))\n            sm1 = Embedding(self.num_target_tokens, 128)(inputs2)\n            sm2 = LSTM(128)(sm1)\n\n            decoder1 = concatenate([am2, sm2])\n            outputs = Dense(self.num_target_tokens, activation='softmax')(decoder1)\n        elif NUM_ALGORITHME == 4:\n            # article input model\n            inputs1 = Input(shape=(self.max_input_seq_length,))\n            article1 = Embedding(self.num_input_tokens, 128)(inputs1)\n            article2 = Dropout(0.3)(article1)\n\n            # summary input model\n            inputs2 = Input(shape=(min(self.num_target_tokens, MAX_DECODER_SEQ_LENGTH),))\n            summ1 = Embedding(self.num_target_tokens, 128)(inputs2)\n            summ2 = Dropout(0.3)(summ1)\n            summ3 = LSTM(128)(summ2)\n            summ4 = RepeatVector(self.max_input_seq_length)(summ3)\n\n            # decoder model\n            decoder1 = concatenate([article2, summ4])\n            decoder2 = LSTM(128)(decoder1)\n            outputs = Dense(self.num_target_tokens, activation='softmax')(decoder2)\n        else:\n            # article input model\n            inputs1 = Input(shape=(self.max_input_seq_length,))\n            article1 = Embedding(self.num_input_tokens, 128)(inputs1)\n            article2 = LSTM(128)(article1)\n            article3 = RepeatVector(128)(article2)\n            # summary input model\n            inputs2 = Input(shape=(self.max_target_seq_length,))\n            summ1 = Embedding(self.num_target_tokens, 128)(inputs2)\n            summ2 = LSTM(128)(summ1)\n            summ3 = RepeatVector(128)(summ2)\n            # decoder model\n            decoder1 = concatenate([article3, summ3])\n            decoder2 = LSTM(128)(decoder1)\n            outputs = Dense(self.num_target_tokens, activation='softmax')(decoder2)\n\n        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n        optimizer = def_keras_optimizer()\n        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        self.model = model\n\n    def generate_batch(self, x_samples, y_samples, batch_size):\n        encoder_input_data_batch = []\n        decoder_input_data_batch = []\n        decoder_target_data_batch = []\n        line_idx = 0\n        while True:\n            for recordIdx in range(0, len(x_samples)):\n                target_words = y_samples[recordIdx]\n                x = x_samples[recordIdx]\n                decoder_input_line = []\n\n                for idx in range(0, len(target_words) - 1):\n                    w2idx = 0  # default [UNK]\n                    w = target_words[idx]\n                    if w in self.target_word2idx:\n                        w2idx = self.target_word2idx[w]\n                    decoder_input_line = decoder_input_line + [w2idx]\n                    decoder_target_label = np.zeros(self.num_target_tokens)\n                    w2idx_next = 0\n                    if target_words[idx + 1] in self.target_word2idx:\n                        w2idx_next = self.target_word2idx[target_words[idx + 1]]\n                    if w2idx_next != 0:\n                        decoder_target_label[w2idx_next] = 1\n                    decoder_input_data_batch.append(decoder_input_line)\n                    encoder_input_data_batch.append(x)\n                    decoder_target_data_batch.append(decoder_target_label)\n\n                    line_idx += 1\n                    if line_idx >= batch_size:\n                        if NUM_ALGORITHME != 4:\n                            yield [pad_sequences(encoder_input_data_batch, self.max_input_seq_length),\n                                   pad_sequences(decoder_input_data_batch,\n                                                 self.max_target_seq_length)], np.array(decoder_target_data_batch)\n                        else:\n                            yield [pad_sequences(encoder_input_data_batch, self.max_input_seq_length), pad_sequences(\n                                decoder_input_data_batch, min(self.num_target_tokens, MAX_DECODER_SEQ_LENGTH))], \\\n                                  np.array(decoder_target_data_batch)\n                        line_idx = 0\n                        encoder_input_data_batch = []\n                        decoder_input_data_batch = []\n                        decoder_target_data_batch = []\n\n    def fit(self, x_train, y_train, x_test, y_test, epochs, batch_size):\n\n        y_train = transform_target_encoding(y_train, self.max_target_seq_length)\n        y_test = transform_target_encoding(y_test, self.max_target_seq_length)\n\n        x_train = transform_input_text(x_train, self.input_word2idx, self.max_input_seq_length)\n        x_test = transform_input_text(x_test, self.input_word2idx, self.max_input_seq_length)\n\n        train_gen = self.generate_batch(x_train, y_train, batch_size)\n        test_gen = self.generate_batch(x_test, y_test, batch_size)\n\n        total_training_samples = sum([len(target_text) - 1 for target_text in y_train])\n        total_testing_samples = sum([len(target_text) - 1 for target_text in y_test])\n        train_num_batches = total_training_samples \/\/ batch_size\n        test_num_batches = total_testing_samples \/\/ batch_size\n\n        self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches, epochs=epochs, verbose=VERBOSE,\n                                 validation_data=test_gen, validation_steps=test_num_batches)\n\n    def summarize(self, input_text):\n        input_seq = []\n        input_wids = []\n        for word in input_text.lower().split(' '):\n            idx = 1  # default [UNK]\n            if word in self.input_word2idx:\n                idx = self.input_word2idx[word]\n            input_wids.append(idx)\n        input_seq.append(input_wids)\n        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n        start_token = self.target_word2idx['START']\n        wid_list = [start_token]\n        if NUM_ALGORITHME != 4:\n            sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n        else:\n            sum_input_seq = pad_sequences([wid_list], min(self.num_target_tokens, MAX_DECODER_SEQ_LENGTH))\n        terminated = False\n\n        target_text = ''\n\n        while not terminated:\n            output_tokens = self.model.predict([input_seq, sum_input_seq])\n            sample_token_idx = np.argmax(output_tokens[0, :])\n            sample_word = self.target_idx2word[sample_token_idx]\n            wid_list = wid_list + [sample_token_idx]\n\n            if sample_word != 'START' and sample_word != 'END':\n                target_text += ' ' + sample_word\n\n            if sample_word == 'END' or len(wid_list) >= self.max_target_seq_length:\n                terminated = True\n            else:\n                if NUM_ALGORITHME != 4:\n                    sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n                else:\n                    sum_input_seq = pad_sequences([wid_list],  min(self.num_target_tokens, MAX_DECODER_SEQ_LENGTH))\n        return target_text.strip()","8058cc3b":"def main_rnn():\n    print('loading csv file ...')\n    df = pd.read_csv(data_dir_path + data_file)\n\n    print('extract configuration from input texts ...')\n    y = df['titre']\n    x = df['article']\n    config = fit_text(x, y)\n\n    print('configuration extracted from input texts ...')\n\n    summarizer = RecursiveRNN(config)\n\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n    print('demo size: ', len(x_train))\n    print('testing size: ', len(x_test))\n\n    print('start fitting ...')\n    summarizer.fit(x_train, y_train, x_test, y_test, epochs=EPOCHS, batch_size=BATCH_SIZE)\n\n    print('start predicting ...')\n    for i in np.random.permutation(np.arange(len(x)))[0:20]:\n        x = x[i]\n        actual_headline = y[i]\n        headline = summarizer.summarize(x)\n        # print('Article: ', x)\n        print('Generated Headline: ', headline)\n        print('Original Headline: ', actual_headline)","2f55086a":"def load_glove():\n    with open(data_dir_path + glove_file, 'r') as f:\n        word2vector = {}\n        for line in f:\n            line_ = line.strip()    # Remove white space\n            words_vec = line_.split()\n            word2vector[words_vec[0]] = np.array(words_vec[1:], dtype=float)\n    return word2vector","5c03ae7b":"def transform_input_text_glove(texts, max_input_seq_length, unknown_emb, word2em):\n    temp = []\n    for line in texts:\n        x = np.zeros(shape=(max_input_seq_length, EMBEDDING_SIZE))\n        for idx, word in enumerate(line.lower().split(' ')):\n            if idx >= max_input_seq_length:\n                break\n            emb = unknown_emb\n            if word in word2em:\n                emb = word2em[word]\n            x[idx, :] = emb\n        temp.append(x)\n    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n\n    print(temp.shape)\n    return temp","c3779003":"class Seq2SeqSummarizer(object):\n\n    def __init__(self, config):\n        self.num_input_tokens = config['num_input_tokens']\n        self.max_input_seq_length = config['max_input_seq_length']\n        self.num_target_tokens = config['num_target_tokens']\n        self.max_target_seq_length = config['max_target_seq_length']\n        self.input_word2idx = config['input_word2idx']\n        self.input_idx2word = config['input_idx2word']\n        self.target_word2idx = config['target_word2idx']\n        self.target_idx2word = config['target_idx2word']\n        self.config = config\n\n        self.word2em = dict()\n        if 'unknown_emb' in config:\n            self.unknown_emb = config['unknown_emb']\n        else:\n            self.unknown_emb = np.random.rand(1, EMBEDDING_SIZE)\n            config['unknown_emb'] = self.unknown_emb\n\n        self.config = config\n\n        if NUM_ALGORITHME == 6:\n            encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n            encoder_embedding = Embedding(input_dim=self.num_input_tokens, output_dim=RNN_SIZE,\n                                          input_length=self.max_input_seq_length, name='encoder_embedding')\n            encoder_lstm = LSTM(units=RNN_SIZE, return_state=True, name='encoder_lstm')\n            encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n            encoder_states = [encoder_state_h, encoder_state_c]\n\n            decoder_inputs = Input(shape=(None, self.num_target_tokens), name='decoder_inputs')\n            decoder_lstm = LSTM(units=RNN_SIZE, return_state=True, return_sequences=True, name='decoder_lstm')\n            decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n                                                                             initial_state=encoder_states)\n            decoder_dense = Dense(units=self.num_target_tokens, activation='softmax', name='decoder_dense')\n            decoder_outputs = decoder_dense(decoder_outputs)\n        else:\n            encoder_inputs = Input(shape=(None, EMBEDDING_SIZE), name='encoder_inputs')\n            encoder_lstm = LSTM(units=RNN_SIZE, return_state=True, name='encoder_lstm')\n            encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n            encoder_states = [encoder_state_h, encoder_state_c]\n\n            decoder_inputs = Input(shape=(None, self.num_target_tokens), name='decoder_inputs')\n            decoder_lstm = LSTM(units=RNN_SIZE, return_state=True, return_sequences=True, name='decoder_lstm')\n            decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n                                                                             initial_state=encoder_states)\n            decoder_dense = Dense(units=self.num_target_tokens, activation='softmax', name='decoder_dense')\n            decoder_outputs = decoder_dense(decoder_outputs)\n\n        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n        optimizer = def_keras_optimizer()\n        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n        self.model = model\n\n        self.encoder_model = Model(encoder_inputs, encoder_states)\n\n        decoder_state_inputs = [Input(shape=(RNN_SIZE,)), Input(shape=(RNN_SIZE,))]\n        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n        decoder_states = [state_h, state_c]\n        decoder_outputs = decoder_dense(decoder_outputs)\n        self.decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n\n    def load_glove(self):\n        self.word2em = load_glove()\n\n    def generate_batch(self, x_samples, y_samples, batch_size):\n        num_batches = len(x_samples) \/\/ batch_size\n        while True:\n            for batchIdx in range(0, num_batches):\n                start = batchIdx * batch_size\n                end = (batchIdx + 1) * batch_size\n                encoder_input_data_batch = pad_sequences(x_samples[start:end], self.max_input_seq_length)\n                decoder_target_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length,\n                                                            self.num_target_tokens))\n                decoder_input_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length,\n                                                           self.num_target_tokens))\n                for lineIdx, target_words in enumerate(y_samples[start:end]):\n                    for idx, w in enumerate(target_words):\n                        w2idx = 0  # default [UNK]\n                        if w in self.target_word2idx:\n                            w2idx = self.target_word2idx[w]\n                        if w2idx != 0:\n                            decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n                            if idx > 0:\n                                decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n                yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n\n    def fit(self, x_train, y_train, x_test, y_test, epochs, batch_size):\n\n        y_train = transform_target_encoding(y_train, self.max_target_seq_length)\n        y_test = transform_target_encoding(y_test, self.max_target_seq_length)\n\n        if NUM_ALGORITHME == 6:\n            x_train = transform_input_text(x_train, self.input_word2idx, self.max_input_seq_length)\n            x_test = transform_input_text(x_test, self.input_word2idx, self.max_input_seq_length)\n        else:\n            x_train = transform_input_text_glove(x_train, self.max_input_seq_length, self.unknown_emb, self.word2em)\n            x_test = transform_input_text_glove(x_test, self.max_input_seq_length, self.unknown_emb, self.word2em)\n\n        train_gen = self.generate_batch(x_train, y_train, batch_size)\n        test_gen = self.generate_batch(x_test, y_test, batch_size)\n\n        train_num_batches = len(x_train) \/\/ batch_size\n        test_num_batches = len(x_test) \/\/ batch_size\n\n        history = self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n                                           epochs=epochs,\n                                           verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches)\n        return history\n\n    def summarize(self, input_text):\n        input_seq = []\n        input_wids = []\n        for word in input_text.lower().split(' '):\n            idx = 1  # default [UNK]\n            if word in self.input_word2idx:\n                idx = self.input_word2idx[word]\n            input_wids.append(idx)\n        input_seq.append(input_wids)\n        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n        states_value = self.encoder_model.predict(input_seq)\n        target_seq = np.zeros((1, 1, self.num_target_tokens))\n        target_seq[0, 0, self.target_word2idx['START']] = 1\n        target_text = ''\n        target_text_len = 0\n        terminated = False\n        while not terminated:\n            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n\n            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n            sample_word = self.target_idx2word[sample_token_idx]\n            target_text_len += 1\n\n            if sample_word != 'START' and sample_word != 'END':\n                target_text += ' ' + sample_word\n\n            if sample_word == 'END' or target_text_len >= self.max_target_seq_length:\n                terminated = True\n\n            target_seq = np.zeros((1, 1, self.num_target_tokens))\n            target_seq[0, 0, sample_token_idx] = 1\n\n            states_value = [h, c]\n        return target_text.strip()\n\n    def summarize_glove(self, input_text):\n        input_seq = np.zeros(shape=(1, self.max_input_seq_length, EMBEDDING_SIZE))\n        for idx, word in enumerate(input_text.lower().split(' ')):\n            if idx >= self.max_input_seq_length:\n                break\n            emb = self.unknown_emb  # default [UNK]\n            if word in self.word2em:\n                emb = self.word2em[word]\n            input_seq[0, idx, :] = emb\n        states_value = self.encoder_model.predict(input_seq)\n        target_seq = np.zeros((1, 1, self.num_target_tokens))\n        target_seq[0, 0, self.target_word2idx['START']] = 1\n        target_text = ''\n        target_text_len = 0\n        terminated = False\n        while not terminated:\n            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n\n            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n            sample_word = self.target_idx2word[sample_token_idx]\n            target_text_len += 1\n\n            if sample_word != 'START' and sample_word != 'END':\n                target_text += ' ' + sample_word\n\n            if sample_word == 'END' or target_text_len >= self.max_target_seq_length:\n                terminated = True\n\n            target_seq = np.zeros((1, 1, self.num_target_tokens))\n            target_seq[0, 0, sample_token_idx] = 1\n\n            states_value = [h, c]\n        return target_text.strip()","465576dc":"def main_seq2seq():\n\n    print('loading csv file ...')\n    df = pd.read_csv(data_dir_path + data_file)\n\n    print('extract configuration from input texts ...')\n    y = df['titre']\n    x = df['article']\n    config = fit_text(x, y)\n\n    print('configuration extracted from input texts ...')\n\n    summarizer = Seq2SeqSummarizer(config)\n    summarizer.load_glove()\n\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n    print('training size: ', len(x_train))\n    print('testing size: ', len(x_test))\n\n    print('start fitting ...')\n    summarizer.fit(x_train, y_train, x_test, y_test, epochs=EPOCHS, batch_size=BATCH_SIZE)\n\n    for i in np.random.permutation(np.arange(len(x)))[0:20]:\n        _x = x[i]\n        actual_headline = y[i]\n        if NUM_ALGORITHME == 6:\n            headline = summarizer.summarize(_x)\n        else:\n            headline = summarizer.summarize_glove(_x)\n        print('Generated Headline: ', headline)\n        print('Original Headline: ', actual_headline)","679fbeba":"if NUM_ALGORITHME == 1:\n    main_seq2seq_char()\nelif NUM_ALGORITHME == 2:\n    main_tf_seq2seq()\nelif 2 < NUM_ALGORITHME < 6:\n    main_rnn()\nelse:\n    main_seq2seq()\n    ","b6f03d68":"Chargement de Glove pour le Word Embedding","1983e646":"Permet de mettre les donn\u00e9es d'entr\u00e9e et de sortie sous forme de hot vecteur pour chaque caract\u00e8res. Ces hots vecteurs sont form\u00e9es \u00e0 partir d'un dictionnaire d'indexation des charact\u00e8res. ","4881abb2":"# Algorithme 1","8469e958":"Credits : \n* [https:\/\/blog.keras.io\/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html](https:\/\/blog.keras.io\/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n* [https:\/\/github.com\/jamesrequa\/Language-Translator-RNN](https:\/\/github.com\/jamesrequa\/Language-Translator-RNN)\n* [https:\/\/machinelearningmastery.com\/encoder-decoder-models-text-summarization-keras\/](https:\/\/machinelearningmastery.com\/encoder-decoder-models-text-summarization-keras\/)\n* [https:\/\/github.com\/chen0040\/keras-text-summarization](https:\/\/github.com\/chen0040\/keras-text-summarization)\n[https:\/\/github.com\/tensorflow\/nmt](https:\/\/github.com\/tensorflow\/nmt)","b50f1520":"Importation des librairies et d\u00e9finition des param\u00e8tres principaux :","9309f7ac":"# Le But de ce Notebook est de cr\u00e9er des headers pour des extraits d'articles de journaux.\n\nLes donn\u00e9es d'entr\u00e9es sont des articles de journaux de taille r\u00e9duite \u00e0 deux phrases. Les donn\u00e9es de sortie sont des titres d'articles. Toutes les donn\u00e9es ont \u00e9t\u00e9 pr\u00e9trait\u00e9es et n'ont plus de stop words (\"the\", \"a\", etc), ni d'abr\u00e9viation (\"isn't devient\" is \"not\") et chaque token est s\u00e9par\u00e9 d'un espace.\n\nCe code regroupe 2 m\u00e9thodes de r\u00e9sum\u00e9 de texte : \n\n**1.** La m\u00e9thode **seq2seq** classique qui consiste \u00e0 construire un **model d'entrainement** \u00e0 partir d'un encodeur et d'un decodeur. L'encodeur re\u00e7oit l'article sous forme de hot vecteur, d'embedding ou d'indexes faisant r\u00e9f\u00e9rence \u00e0 un dictionnaire, Ces \u00e9tats internes (h,c) sont ensuite envoy\u00e9s \u00e0 un d\u00e9codeur qui prend \u00e9galement en entr\u00e9e le header sous la m\u00eame forme que l'article. Le headeur \u00e0 t+1 est renvoy\u00e9 en sortie du d\u00e9codeur. Le Mod\u00e8le final prend en entr\u00e9es l'article et le header, et en sortie le header \u00e0 t+1.\n\n![](https:\/\/blog.keras.io\/img\/seq2seq\/seq2seq-inference.png)\n\n\nUn **mod\u00e8le d'inf\u00e9rence** est necessaire pour la pr\u00e9diction des nouvelles valeurs. Il permet d'actualiser les donn\u00e9es d'entr\u00e9es du d\u00e9codeur pendant la pr\u00e9diction.\n\n\n\n![Mod\u00e8le d'inf\u00e9rence](https:\/\/raw.githubusercontent.com\/tensorflow\/nmt\/master\/nmt\/g3doc\/img\/greedy_dec.jpg)\n\n\nIci tous les encodeurs et les decodeurs sont des LSTM.\n\nCette m\u00e9thode regroupe 3 classes diff\u00e9rentes : \n* La classe **Seq2SeqChar** qui consiste \u00e0 une pr\u00e9diction charact\u00e8re pas charact\u00e8re (et non pas mot par mot). Les donn\u00e9es sont entr\u00e9es sous forme de hot vecteur.\n* La classe **TfSeq2Seq** qui utilise la librairy Tensorflow (tous les autres sont sous Keras). Les donn\u00e9es sont index\u00e9es par rapport \u00e0 un dictionnaire.\n* La classe **Seq2SeqSummarizer** qui r\u00e9unit une m\u00e9thode seq2seq bas\u00e9e sur des donn\u00e9es index\u00e9es, et une m\u00e9thode seq2seq bas\u00e9e sur des vecteurs Glove.\n\n\n\n**2.** La m\u00e9thode **Recursif Neural Network** consiste \u00e9galement en un encodeur suivi d'un d\u00e9codeur mais sans passage d'\u00e9tat interne entre les deux. La sortie de l'encodeur et l'entr\u00e9e du d\u00e9codeur sont utilis\u00e9s comme contexte pour pr\u00e9dire la sortie du decodeur.\n\nCette m\u00e9thode est d\u00e9finie dans la class **RecursiveRNN** qui regroupe trois m\u00e9thodes diff\u00e9rentes : \n* La m\u00e9thode **RNN1** qui consiste en deux mod\u00e8les qui mergent leurs outputs pour servir de contexte pour la pr\u00e9diction du mot suivant. Le mot pr\u00e9dit est ensuite r\u00e9inject\u00e9 dans l'input d'un des deux mod\u00e8le.\n\n![](https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/09\/Alternate-2-Recursive-Text-Summarization-Model-A.png)\n\n\n* Les m\u00e9thodes **RNN2** et **RNN3** consistent \u00e9galement en deux mod\u00e8les mais cette fois ci l'output du premier mod\u00e8les est inject\u00e9 comme contexte dans le deuxi\u00e8me mod\u00e8le. **RNN2**  et **RNN3** diff\u00e8rent de par leur couches internes.\n\n![](http:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/09\/Alternate-3-Recursive-Text-Summarization-Model-B.png)\n\n","2227d49f":"**D\u00e9finition de la classe RecursiveRNN avec 6 fonctions :**\n* **init(self, config)**: Constuit deux inputs pour le decoder, un pour les donn\u00e9es d'entr\u00e9e (article) compos\u00e9 de troix couches (Input, Embedding, Dropout), et un pour les donn\u00e9es de sortie (r\u00e9sum\u00e9s) compos\u00e9 de cinq couches (Input, Embedding, Dropout, LSTM, RepeatVector). Le decodeur qui va servir d'output au model est constitut\u00e9 d'une couche input qui est une concat\u00e9nation des deux inputs pr\u00e9c\u00e9dement cr\u00e9\u00e9s, d'une couche LSTM et d'une couche Dense (couche o\u00f9 tous les neuronnes sont connect\u00e9s). Enfin un model est construit \u00e0 partir des deux inputs et de l'output (decodeur) puis il est compil\u00e9.\n* **generate_batch(self, x_samples, y_samples, batch_size)**: Traite les donn\u00e9es d'entr\u00e9e et de sortie afin d'indexer les mots des donn\u00e9es de sortie et de cr\u00e9er un hot vecteur pour les donn\u00e9es de sortie \u00e0 t+1, mais aussi \u00e0 faire un padding pour que toutes les donn\u00e9es correspondent au batch_size.\n* **fit(self, x_train, y_train, x_test, y_test, epochs=None, model_dir_path=None, batch_size=None)**: s\u00e9pare les donn\u00e9es en donn\u00e9es d'entrainement et en donn\u00e9es de test et entraine le mod\u00e8le avec ces donn\u00e9es. Utilise les fonctions transform_input_text, split_target_text et generate_batch.\n* **summarize(self, input_text)**: indexe le texte d'entr\u00e9e et pr\u00e9dit chaque mot du texte de sortie \u00e0 partir du texte d'entr\u00e9e et des pr\u00e9c\u00e9dents mots du texte de sortie (\"START\" pour le premier mot). S'arr\u00eate lorsque que le mot pr\u00e9dit est \"END\" ou que la taille maximale de phrase est atteinte.\n","dfaf2285":"# Algorithme 2","81835712":"**Pr\u00e9paration des donn\u00e9es :**\n* Cr\u00e9ation de dictionnaire de donn\u00e9es pour les donn\u00e9es d'entr\u00e9e et de sortie\n* Ajout de token \"START\" et \"END\" au d\u00e9but et la fin des donn\u00e9es de sortie\nTaille maximale des donn\u00e9es d'entr\u00e9e et de sortie\n* Nombre de donn\u00e9es en entr\u00e9e et en sortie\n","dae88ed3":"Pr\u00e9traite les donn\u00e9es de sortie (r\u00e9sum\u00e9s). Ajoute les token \"START\" et \"END\" au d\u00e9but et \u00e0 la fin des phrases.","38458c65":"# Algoritmes 6 et 7","f5314933":"# Algorithmes 3,4 et 5","2a230984":"# Hypperparam\u00e8tres\n\n**NUM_ALGORITHME** : Le num\u00e9ro d'algorithme \u00e0 \u00e9x\u00e9cuter. \n\n**LEARNING_RATE** : comprit entre 0 et 1\n\n**OPTIMIZER_TYPE** : possibilit\u00e9es : sgd, rmsprop, adagrad, dadelta, adam\n\n**Param\u00e8tres par d\u00e9faut** : \n\n![image.png](attachment:image.png)\n","d62f0f7c":"Pr\u00e9traite les donn\u00e9es d'entr\u00e9e (articles). Cr\u00e9er une liste de listes d'indexes en fonction des dictionnaires word2idx et idx2word, pour toutes les phrases des donn\u00e9es d'entr\u00e9e. Toutes les listes sont mises \u00e0 la m\u00eame taille"}}