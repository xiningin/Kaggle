{"cell_type":{"b46ad870":"code","6d5a6cae":"code","e42206fd":"code","2777eb51":"code","22abca24":"code","ac51669a":"code","6c433807":"code","bb71a877":"code","18b524f4":"code","b6cf9a4b":"code","77a35c16":"code","dd0397c7":"code","05b63de0":"code","186ee6a2":"code","bfa53c7d":"code","9288532b":"code","36060ac0":"code","f2227451":"markdown","75ad8c55":"markdown","21089b12":"markdown","1498b04f":"markdown","a731cd8d":"markdown","da18bacf":"markdown","4679687a":"markdown","535e1528":"markdown","14cdc8b1":"markdown","55e7c716":"markdown"},"source":{"b46ad870":"import matplotlib.pyplot as plt \nimport numpy as np \nimport random \nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader,random_split\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim","6d5a6cae":"! echo \"{\\\"username\\\":\\\"usenameee\\\",\\\"key\\\":\\\"keyyyyy\\\"}\" > kaggle.json","e42206fd":"# and upload kaggle.json account file\n!cp kaggle.json ~\/.kaggle\/\n!kaggle datasets download -d ifigotin\/imagenetmini-1000\n!mkdir tmp\n!unzip imagenetmini-1000.zip -d tmp\n! mv tmp\/imagenet-mini imagenet\/\n! rmdir tmp","2777eb51":"IMSIZE = 256\nIMCHANNELS = 3\n\ntrain_transform = transforms.Compose([                      \n      transforms.Resize((IMSIZE,IMSIZE)),\n      transforms.ToTensor(),\n])\n\ntest_transform = transforms.Compose([\n          transforms.Resize((IMSIZE,IMSIZE)),\n          transforms.ToTensor(),\n])\n\n\ndata_dir = 'imagenet'\n\n\n\ntrain_dataset = torchvision.datasets.ImageFolder(data_dir+'\/train', transform=train_transform)\ntest_dataset  = torchvision.datasets.ImageFolder(data_dir+'\/val', transform=test_transform)","22abca24":"batch_size=50\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)","ac51669a":"a = next(iter(test_loader))\nplt.imshow(a[0][0,0])\nprint(a[0].shape)","6c433807":"# encoding size is given as a parameter\n\nclass Encoder(nn.Module):\n    \n    def __init__(self, encoded_space_dim):\n        super().__init__()\n        \n        self.encoder_cnn = nn.Sequential(\n            nn.Conv2d(in_channels=IMCHANNELS, out_channels=8, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(8, 16, 4, stride=2, padding=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True),\n            nn.Conv2d(16, 32, 4, stride=2, padding=1), # after, \n            nn.ReLU(True),\n            nn.Conv2d(32, 40, 4, stride=2, padding=1), # after shape=(40, 16, 16)\n            nn.ReLU(True)\n        )\n        \n        self.flatten = nn.Flatten(start_dim=1)\n        self.encoder_lin = nn.Sequential(\n            nn.Linear(40*16*16, 128),\n            nn.ReLU(True),\n            nn.Linear(128, encoded_space_dim)\n        ) # the image encoding is the output of these layers\n        \n    def forward(self, x):\n        x = self.encoder_cnn(x)\n        x = self.flatten(x)\n        x = self.encoder_lin(x)\n        return x\nclass Decoder(nn.Module):\n    \n    def __init__(self, encoded_space_dim):\n        super().__init__()\n        # Do the reverse action\n        self.decoder_lin = nn.Sequential(\n            nn.Linear(encoded_space_dim, 128),\n            nn.ReLU(True),\n            nn.Linear(128, 40*16*16),\n            nn.ReLU(True)\n        )\n\n        self.unflatten = nn.Unflatten(dim=1, \n        unflattened_size=(40, 16, 16))\n\n        self.decoder_conv = nn.Sequential(\n            nn.ConvTranspose2d(40, 32, 4, \n            stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 16, 4, \n            stride=2, padding=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(16, 8, 4, stride=2, \n            padding=1),\n            nn.BatchNorm2d(8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(8, IMCHANNELS, 4, stride=2, \n            padding=1)\n        )\n        \n    def forward(self, x):\n        x = self.decoder_lin(x)\n        x = self.unflatten(x)\n        x = self.decoder_conv(x)\n        x = torch.sigmoid(x) # =normalize to [0,1]\n        return x","bb71a877":"torch.manual_seed(16)\n\nloss_fn = torch.nn.MSELoss()\nlr= 0.001\n\nd = 10\n\nencoder = Encoder(encoded_space_dim=d)\ndecoder = Decoder(encoded_space_dim=d)\nparams_to_optimize = [\n    {'params': encoder.parameters()},\n    {'params': decoder.parameters()}\n] # optimize both nets\n\noptim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n\n# Check if the GPU is available\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f'Selected device: {device}')\n\n# Move both the encoder and the decoder to the selected device\nencoder.to(device)\ndecoder.to(device)","18b524f4":"### Training function\ndef train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer,noise_factor=0.3, noise_training=False, max_samples=100):\n    # Set train mode for both the encoder and the decoder\n    encoder.train()\n    decoder.train()\n    train_loss = []\n    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n    sampled = 0\n    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n        if sampled>max_samples:\n          break\n        sampled += batch_size\n        # Move tensor to the proper device\n        image_batch = image_batch.to(device)\n        encode_images = image_batch\n        if noise_training:\n          encode_images += torch.rand_like(image_batch)\/3 # limit noise to <1\/3\n        # Encode data\n        encoded_data = encoder(encode_images)\n        # Decode data\n        decoded_data = decoder(encoded_data)\n        # Evaluate loss\n        loss = loss_fn(decoded_data, image_batch)\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # Print batch loss\n        train_loss.append(loss.detach().cpu().numpy())\n\n    return np.mean(train_loss)","b6cf9a4b":"### Testing function\ndef test_epoch(encoder, decoder, device, dataloader, loss_fn):\n    # Set evaluation mode for encoder and decoder\n    encoder.eval()\n    decoder.eval()\n    with torch.no_grad(): # No need to track the gradients\n        # Define the lists to store the outputs for each batch\n        conc_out = []\n        conc_label = []\n        for image_batch, _ in dataloader:\n            # Move tensor to the proper device\n            image_batch = image_batch.to(device)\n            # Encode data\n            encoded_data = encoder(image_batch)\n            # Decode data\n            decoded_data = decoder(encoded_data)\n            # Append the network output and the original image to the lists\n            conc_out.append(decoded_data.cpu())\n            conc_label.append(image_batch.cpu())\n        # Create a single tensor with all the values in the lists\n        conc_out = torch.cat(conc_out)\n        conc_label = torch.cat(conc_label) \n        # Evaluate global loss\n        val_loss = loss_fn(conc_out, conc_label)\n    return val_loss.data","77a35c16":"def plot_ae_outputs(encoder,decoder,n=5, use_train=True):\n    plt.figure(figsize=(10,4.5))\n    for i in range(n):\n      ax = plt.subplot(2,n,i+1)\n      if use_train:\n        img = train_dataset[i][0].unsqueeze(0).to(device)\n      else:\n        img = test_dataset[i][0].unsqueeze(0).to(device)\n      encoder.eval()\n      decoder.eval()\n      with torch.no_grad():\n         rec_img  = decoder(encoder(img))\n      \n      rec_img = rec_img.squeeze().permute(1,2,0).cpu().numpy()\n\n      img = img.squeeze().permute(1,2,0).cpu().numpy()\n      plt.imshow(img, cmap='gist_gray')\n      ax.get_xaxis().set_visible(False)\n      ax.get_yaxis().set_visible(False)  \n      if i == n\/\/2:\n        ax.set_title('Original images')\n      ax = plt.subplot(2, n, i + 1 + n)\n      plt.imshow(rec_img, cmap='gist_gray')  \n      ax.get_xaxis().set_visible(False)\n      ax.get_yaxis().set_visible(False)  \n      if i == n\/\/2:\n         ax.set_title('Reconstructed images')\n    plt.show()","dd0397c7":"num_epochs = 200\nfor epoch in range(num_epochs):\n\n   train_loss = train_epoch(encoder,decoder,device,train_loader,loss_fn,optim)\n   print('\\n EPOCH {}\/{} \\t train loss {:.3f} \\t'.format(epoch + 1, num_epochs,train_loss))\n   plot_ae_outputs(encoder,decoder,n=5)","05b63de0":"# loop process\nNSAMPLES = 2\nshift = 30 # to see other samples\n\nloop_data_array = []\nfor i in range(NSAMPLES): # for some reason cant use [:NSAMPLES]\n    img = train_dataset[shift+i][0].unsqueeze(0)\n    loop_data_array.append(img)\nloop_data = torch.cat(loop_data_array, dim=0)\nloop_data = loop_data.to(device)\n\nprint(loop_data.shape)\n\nplt.imshow(loop_data.cpu()[0,0])","186ee6a2":"with torch.no_grad():\n  decoded = decoder(encoder(loop_data))\n  plt.imshow(decoded.cpu()[0,0])","bfa53c7d":"# set noise squares in images\n# shape: (nimages, nchannels, im_w, im_h)\n# doesnt change original data\ndef noise(imgs, im_w=28, im_h=28, square_width = 7, rand=False):\n  images = imgs.clone()\n  midw = im_w\/\/2\n  midh = im_h\/\/2\n  half = square_width\/\/2\n  \n  st_row = midh-half\n  st_col = midw-half\n\n  end_row = midh+half+1\n  end_col = midw+half+1\n\n  ns = None\n  if rand:\n    ns = torch.rand_like(images[:,:, st_row:end_row, st_col:end_col]) # second : to deal with channel\n  else:\n    ns = torch.zeros_like(images[:,:, st_row:end_row, st_col:end_col]) # second : to deal with channel\n    \n  images[:,:,st_row:end_row, st_col:end_col] = ns\n\n  return images\n","9288532b":"use_blank = True # TEST - see on blank & full noise image\nNBLANKS, NNOISE = 1, 1\nNEXTRA = NBLANKS + NNOISE # other than NSAMPLES\n\nencoder.train() # eval messed it up\ndecoder.train()\n\nNROUNDS = 20 # num of stacked AEs\nuse_loop_data = loop_data.clone()\ncurr_images = noise(use_loop_data, square_width=13, rand=True)\nimg_shape = curr_images.shape[1:]\n\nif use_blank:\n  shape_blanks = (NBLANKS,img_shape[0],img_shape[1],img_shape[2])\n  shape_noise = (NNOISE,img_shape[0],img_shape[1],img_shape[2])\n\n  blanks = torch.zeros(shape_blanks).to(device)\n  noises = torch.rand(shape_noise).to(device)\n\n  curr_images = torch.cat([curr_images, blanks, noises])\n  # no original images for hand-crafted samples\n  fill = torch.cat([blanks, noises])\n  use_loop_data = torch.cat([use_loop_data, fill])\n\n\n\nhisto = [use_loop_data.unsqueeze(0).cpu().permute(0,1,3,4,2),curr_images.unsqueeze(0).cpu().permute(0,1,3,4,2)]\n\n# Run model again & again, each time on previous output\nwith torch.no_grad():\n  for i in range(NROUNDS):\n    curr_images = decoder(encoder(curr_images))\n    add = curr_images.unsqueeze(0).cpu().permute(0,1,3,4,2)\n    histo.append(add)\n\nhisto = torch.cat(histo, dim=0)\nhisto = histo.squeeze() # remove channel dim\nprint(histo.shape)\n\nplt.imshow(histo.cpu()[0,-1])\n","36060ac0":"fig = plt.figure(figsize=(150, 30))\ncolumns = len(histo)\nrows = NSAMPLES+NEXTRA\n\nfor im_round in range(columns):\n  for im_sampid in range(rows):\n    img = histo[im_round, im_sampid]\n    seq_num = im_sampid*columns + im_round # zero fixed\n    fig.add_subplot(rows, columns, seq_num+1)\n    plt.axis('off')\n    plt.imshow(img)\n\nfig.suptitle('Iterative Feeding: corrupted train samples (rows) through iterations (columns)\\n Col. 1 = original samples, Col. 2 corrupted samples', fontsize=70)\nplt.show()","f2227451":"# Create model, loss and optimizer","75ad8c55":"# Goals\nSeeing the process of repeated feeding of an image into a fully trained autoencoder.\\\nGeneralization isn't a concern here.","21089b12":"# Training routines","1498b04f":"# Results - associative memory","a731cd8d":"# Train","da18bacf":"# Fetch dataset\nDownload the imagenet dataset through kaggle.\\\nKaggle json file should be uploaded to the working directory.\\","4679687a":"Define datasets.\\\nThe images are already normalized to the range [0,1].","535e1528":"# Create encoder and decoder models\nBoth are fully convolutional except for the autoencoder bottleneck.","14cdc8b1":"Visualize outputs: image (i,j) represents sample i after j iterations.","55e7c716":"Results of encoding and decoding a train image."}}