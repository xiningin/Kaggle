{"cell_type":{"07a0d927":"code","4e86bb7a":"code","3d261849":"code","36874790":"code","780b51a2":"code","144fa20b":"code","626688a2":"code","a7e59544":"code","18f50ba0":"code","bbf4dbdd":"code","bd93e18b":"code","43fdd573":"code","244095e9":"code","622b5ef2":"code","e46a0218":"code","7fa970b7":"code","85c99773":"code","210245c7":"code","1340146b":"code","a7d68e1b":"code","297062a3":"code","ae2e130a":"code","1c7d2e13":"code","778489cb":"code","da8dea02":"code","57fd0934":"code","50df98c5":"code","bc1dec4f":"code","1801b544":"code","0a02c16c":"code","9757142a":"code","6fd79598":"code","a814fbc5":"code","1b606d35":"code","9e071cc1":"code","b30020b2":"code","4c800c19":"code","bde47406":"code","6987378e":"code","513860f4":"code","b4b88e48":"markdown","1441b790":"markdown","03c64e92":"markdown","ef40c0b6":"markdown","9aa59322":"markdown","371dbf8a":"markdown"},"source":{"07a0d927":"!pip install pyunpack\n!pip install patool\n!pip install py7zr\n!pip install pathlib","4e86bb7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d261849":"!pwd\nos.getcwd()","36874790":"from pathlib import Path\nimport random\nfrom importlib import reload\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport IPython.display as ipd\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nimport librosa, librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.signal import butter, lfilter\nimport seaborn as sns\nimport soundfile as sf\nfrom tqdm import tqdm_notebook as tqdm\n\nimport librosa\nfrom scipy.io import wavfile\nimport numpy as np\n\nimport glob","780b51a2":"from py7zr import unpack_7zarchive\nimport shutil\n\nshutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\nshutil.unpack_archive('\/kaggle\/input\/tensorflow-speech-recognition-challenge\/train.7z', '\/kaggle\/working\/tensorflow-speech-recognition-challenge\/')","144fa20b":"path_base = Path('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train')\npaths_train = list(path_base.glob('**'))\npaths_train","626688a2":"print([str(path).split('\/')[-1] for path in paths_train] )","a7e59544":"os.listdir('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/audio')","18f50ba0":"trial_path = '\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/audio'\ncnt = 0\nfor dirnames, _, filenames in os.walk(trial_path):\n    cnt += len(filenames)","bbf4dbdd":"cnt","bd93e18b":"path_of_audios = '\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/audio\/house'\nhouse_audio = os.listdir(path_of_audios) #List of all file names inside house folder\n\n\nfor i in range(300,400):\n    print(path_of_audios+'\/'+house_audio[i])\n    \nsr = 16000 #hz (sample rate)","43fdd573":"idx = 7\nsamples, sample_rate = librosa.load(path_of_audios+'\/'+house_audio[idx], sr = sr)\nsamples.shape","244095e9":"ipd.Audio(samples, rate = sr)","622b5ef2":"ipd.Audio(path_of_audios+'\/'+house_audio[idx])","e46a0218":"fig = plt.figure(figsize=(20,13))\nx_values = np.linspace(0,sr\/len(samples),sr)\ny_values = samples\nax = plt.gca()\nax.set_title('Raw wave of '+path_of_audios+'\/'+house_audio[idx])\nax.set_xlabel('Time')\nax.set_ylabel('Amplitude')\nplt.plot(x_values, y_values)","7fa970b7":"16*3","85c99773":"resamples = librosa.resample(samples, sr, 8000)\nipd.Audio(resamples, rate=8000)","210245c7":"# sample rate is how many complete cycles we have in one second","1340146b":"labels=[ 'dog', 'one', 'down', 'right', 'cat', 'bed', 'up', 'eight', 'marvin', 'six', 'nine', 'four',\n        'five', 'yes', 'three', 'wow', 'sheila', 'zero', 'seven', 'happy', 'go', 'bird', 'two', 'stop', 'off', 'tree',  'house', 'on', 'left', 'no']","a7d68e1b":"len(labels)","297062a3":"# 1- resample all files\n# 2- Keep only the files that belong to our labels.\n# x is the features (It is the amplitude in the time domain). (all_wave)\n# y is the labels. (all_label)\n\nall_wave = []\nall_label = []\npath = '\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/audio\/'\nfor label in labels:\n    waves = [file for file in os.listdir(path+label) if file.endswith('.wav')] \n    for wav in waves:\n        samples,sr = librosa.load(path+label+'\/'+wav, sr=16_000)\n        resamples = librosa.resample(samples, sr, 8_000)\n        \n        if len(resamples)==8_000:\n            all_wave.append(resamples)\n            all_label.append(label)","ae2e130a":"print(len(all_wave))","1c7d2e13":"for i in range(10):\n    print(all_wave[i].shape)","778489cb":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny=le.fit_transform(all_label)\nclasses= list(le.classes_)","da8dea02":"from keras.utils import np_utils\ny = np_utils.to_categorical(y, num_classes = len(labels))","57fd0934":"y.shape","50df98c5":"all_wave = np.array(all_wave).reshape(-1,8000,1)","bc1dec4f":"all_wave.shape","1801b544":"from sklearn.model_selection import train_test_split\n\nx_tr, x_val, y_tr , y_val = train_test_split(all_wave,y, stratify = y, test_size = 0.2, shuffle =True)","0a02c16c":"from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K","9757142a":"inputs = Input(shape=(8000,1))\n\n# First Conv1D layer\nconv = Conv1D(8, 12, padding=\"valid\", activation='relu', strides=1)(inputs)\nconv = MaxPooling1D(3)(conv)\nconv = Dropout(0.3)(conv)\n\n# Second Conv1D layer\nconv = Conv1D(16, 11, padding=\"valid\", activation='relu', strides=1)(conv)\nconv = MaxPooling1D(3)(conv)\nconv = Dropout(0.3)(conv)\n\n# Third Conv1D layer\nconv = Conv1D(32, 9, padding=\"valid\", activation='relu', strides=1)(conv)\nconv = MaxPooling1D(pool_size=3)(conv)\nconv = Dropout(0.3)(conv)\n\n# Fourth Conv1D layer\nconv = Conv1D(64,7, padding=\"valid\", activation='relu', strides=1)(conv)\nconv = MaxPooling1D(3)(conv)\nconv = Dropout(0.3)(conv)\n\nconv = Flatten()(conv)\n\n#Dense Layer1\nconv = Dense(128, activation = 'relu')(conv)\nconv = Dropout(0.3)(conv)\n\n#Dense Layer2\nconv = Dense(128, activation = 'relu')(conv)\nconv = Dropout(0.3)(conv)\n\n# The output Function of the last layer is not ReLU, it's SofMax\n#len(labels) is 10\noutputs = Dense(len(labels), activation='softmax')(conv)\n\nmodel = Model(inputs, outputs)\nmodel.summary()","6fd79598":"model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n\nes = EarlyStopping(monitor = 'val_loss', mode='min', verbose=1, patience = 10)\n\nmc = ModelCheckpoint('kaggle\/working\/best_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, \n                     mode='max')\nhistory = model.fit(x_tr, y_tr, callbacks=[es,mc],validation_data=(x_val,y_val),\n          epochs=100, batch_size = 32)","a814fbc5":"# model.save('kaggle\/working\/')\n\nmodel.save(\"my_h5_model.h5\")","1b606d35":"from keras.models import load_model\n\nmodel = load_model('my_h5_model.h5')\nmodel.summary()","9e071cc1":"def predict(audio):\n    prob = model.predict(audio.reshape(-1,8_000,1)) #prob is a vector of 10 values\n    index = np.argmax(prob[0])\n    return classes[index]","b30020b2":"samples = x_tr[24564].flatten()\nprint(predict(samples))\nipd.Audio(samples,rate=8_000)","4c800c19":"import sounddevice as sd\nimport soundfile as sf\n\nsamplerate = 16000  \nduration = 1 # seconds\nfilename = 'yes.wav'\nprint(\"start\")\nmydata = sd.rec(int(samplerate * duration), samplerate=samplerate,channels=1, blocking=True)\nprint(\"end\")\nsd.wait()\nsf.write(filename, mydata, samplerate)","bde47406":"#reading the voice commands\nsamples, sample_rate = librosa.load(filepath + '\/' + 'stop.wav', sr = 16000)\nsamples = librosa.resample(samples, sample_rate, 8000)\nipd.Audio(samples,rate=8000)     ","6987378e":"predict(samples)","513860f4":"Why do we need a validation dataset?\nHow many epochs did you need to train your model?\nWhat was your stopping criteria to stop training?\nWhich libraries did you use for building your model and converting the audio files?\nWhat was the proportion you used to split your data into train and validation datasets?\nHow many parameters do you have in your model?\nWhat is your optimizer?\n\nWhy did you use Conv1D not Conv2D?\nWhat is the sample rate that you used to train your model, and why?\nWhat is your evaluation metric?\nWhat is the spectrogram representation?\nWhat is the sampling rate?","b4b88e48":"## preparing the output","1441b790":"samples","03c64e92":"## Splitting Into Train and Validation Datasets","ef40c0b6":"## Reshaping the Input","9aa59322":"# Building The Model","371dbf8a":"## Keras Model"}}