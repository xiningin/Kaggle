{"cell_type":{"41bf6c6d":"code","3c453f5f":"code","baffea90":"code","67e5735f":"code","36491bac":"code","e0fb830f":"code","0c5ecc90":"code","66e55f0e":"code","cea8a1d6":"code","459b3545":"code","d905409d":"code","d8e84c22":"code","d8f30c5e":"code","7ac280ee":"markdown","8d4f4efc":"markdown","33605cef":"markdown","219b99b9":"markdown","d8d7a67d":"markdown","11df7001":"markdown","9017c119":"markdown","f6ee344c":"markdown","8da2ec2a":"markdown","3852b049":"markdown"},"source":{"41bf6c6d":"from argparse import ArgumentParser, Namespace\nfrom datetime import datetime\nfrom os import cpu_count\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Union\nimport gc\nimport pickle\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom torch.distributions.beta import Beta\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_constant_schedule_with_warmup,\n)\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn","3c453f5f":"args = Namespace(\n    data=Path('\/kaggle\/input\/commonlitreadabilityprize'),\n    models=Path('models'),\n    infer_path=Path('\/kaggle\/input\/7-entrenamiento-transformer'), # CAMBIAR POR EL PATH AL DATASET RESULTADO DE ENTRENAMIENTO\n    seed=2021,\n    kfold_seed=2021,\n    bs=8, \n    n_folds=5,\n    seq_len=200,\n    model_name='roberta-base',\n    trf_do=0.,\n    lr=1e-5,\n    epochs=1,\n    val_steps=100,\n    mode='infer', # train\/infer\n)","baffea90":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","67e5735f":"class ReadabilityDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=200):\n        super().__init__()\n        self.df = df\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.labeled = 'target' in df\n        \n        if self.labeled:\n            self.target = torch.tensor(df.target.values, dtype=torch.float)\n            self.stderr = torch.tensor(df.standard_error.values, dtype=torch.float)\n            #self.bin = torch.tensor(df.bin.values, dtype=torch.long)\n        \n        texts = list(df.excerpt.values)\n        self.tokens = tokenizer(\n            texts, \n            max_length=max_len, \n            truncation=True, \n            padding='max_length', \n            return_tensors='pt', \n            add_special_tokens=True\n        )\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        ids = self.tokens['input_ids'][idx].clone()\n        mask = self.tokens['attention_mask'][idx].clone()\n\n        if self.labeled:\n            target = self.target[idx]\n\n        return (ids, mask, target) if self.labeled else (ids, mask)","36491bac":"class ReadabilityModel(nn.Module):\n\n    def __init__(self, args, body):\n        super().__init__()\n\n        self.args = args\n        self.body = body\n        \n        self.out_features = body(torch.zeros(1, 1, dtype=torch.long)).hidden_states[-1].shape[-1]\n        self.head = nn.Linear(self.out_features, 1) # 768 base, 1024 large\n        self.head.bias.data.fill_(-0.9593187699947071)\n\n    def forward(self, ids, mask):\n        x = self.body(input_ids=ids, attention_mask=mask)\n        # \n        x = x['hidden_states']  # 13, bs, max_len, d_model (ej: 13, 8, 200, 768)\n        x = x[-1]               # 8, 200, 768\n        x = torch.mean(x, 1)    # 8, 768\n        x = self.head(x)        # 8, 1\n        x = x.squeeze(-1)       # 8\n        return x","e0fb830f":"def loss_fn(preds, targs):\n    return torch.sqrt(nn.MSELoss()(preds, targs))","0c5ecc90":"class Checkpointer:\n    def __init__(self, base_name, path):\n        self.best_loss = None\n        self.best_path = None\n        self.base_name = base_name\n        self.path = path\n\n    def on_validation_end(self, model, epoch, loss) -> None:\n        if (self.best_loss is None) or (loss < self.best_loss):\n            #print(f\"Loss {loss} better than {self.best_loss}\")\n            self.path.mkdir(exist_ok=True, parents=True)\n            stem = f'{self.base_name}_epoch_{epoch}_loss_{loss:.6f}'\n            out_p = self.path \/ f'{stem}.pt'\n            with out_p.open('wb') as f:\n                torch.save(model.state_dict(), f)\n            if self.best_path is not None:\n                self.best_path.unlink()\n            self.best_loss = loss\n            self.best_path = out_p","66e55f0e":"def validate(model, valid_dl):\n    is_training = model.training\n    model.eval()\n    preds = []\n    targs = []\n    with torch.no_grad():\n        for batch in tqdm(valid_dl, leave=False):\n            ids, mask, targ = ( t.cuda() for t in batch )\n            pred = model(ids, mask)\n            targs.append(targ)\n            preds.append(pred)\n        preds = torch.cat(preds)\n        targs = torch.cat(targs)\n        #print(preds.shape, targs.shape)\n        loss = loss_fn(preds, targs).cpu()\n    if is_training:\n        model.train()\n    return loss","cea8a1d6":"def predict(model, test_dl):\n    is_training = model.training\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch in tqdm(test_dl, leave=False):\n            if len(batch) == 3:\n                ids, mask, _ = ( t.cuda() for t in batch )\n            else:\n                ids, mask = ( t.cuda() for t in batch )\n            pred = model(ids, mask).detach().cpu()\n            preds.append(pred)\n        preds = torch.cat(preds)\n    if is_training:\n        model.train()\n    return preds","459b3545":"def train_fold(fold, df, train_idx, valid_idx, ts):\n    print(f\"Training fold {fold}\")\n\n    # Model\n    model_config = AutoConfig.from_pretrained(f'cfgs\/{args.model_name}', add_pooling_layer=False)\n    model_config.output_hidden_states = True\n    model_config.hidden_dropout_prob = args.trf_do\n    model_config.attention_probs_dropout_prob = args.trf_do\n    body = AutoModel.from_pretrained(args.model_name, config=model_config, add_pooling_layer=False)\n    model = ReadabilityModel(args, body)\n\n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(f'toks\/{args.model_name}', config=model_config)\n\n    # Datasets\n    train_ds = ReadabilityDataset(df.iloc[train_idx], tokenizer, max_len=args.seq_len)\n    valid_ds = ReadabilityDataset(df.iloc[valid_idx], tokenizer, max_len=args.seq_len)\n\n    # Dataloader\n    train_dl = DataLoader(\n        dataset=train_ds,\n        batch_size=args.bs,\n        num_workers=0, #cpu_count() \/\/ 4,\n        drop_last=True,\n        shuffle=True\n    )\n    \n    valid_dl = DataLoader(\n        dataset=valid_ds,\n        batch_size=args.bs * 8,\n        num_workers=0, #cpu_count() \/\/ 4,\n    )\n    \n    # Optimizer\n    optimizer = torch.optim.Adam(model.parameters(), args.lr)\n    model.train().cuda()\n\n    model_p = args.models \/ ts\n    prefix = f'{args.model_name}_fold_{fold}'\n    checkpointer = Checkpointer(prefix, model_p)    \n    num_training_steps = len(train_dl) * args.epochs\n    pbar = tqdm(range(num_training_steps))\n\n    step = 0\n    val_loss = None\n\n    for epoch in range(args.epochs):\n        for nb, batch in enumerate(train_dl):\n            \n            optimizer.zero_grad()\n\n            ids, mask, targ = ( t.cuda() for t in batch )\n\n            pred = model(ids, mask)\n            loss = loss_fn(pred, targ)\n                \n            loss.backward()\n            optimizer.step()\n\n            # Valida cada n pasos\n            step += 1\n            if step % args.val_steps == 0 or step == num_training_steps:\n                val_loss = validate(model, valid_dl)\n\n                # Checkpoint\n                checkpointer.on_validation_end(model, epoch, val_loss.item())\n                \n            # update tqdm\n            postfix = {\n                'loss': loss.item(),\n                'val_loss': val_loss.item() if val_loss else '-',\n                'best': checkpointer.best_loss,\n            }\n\n            pbar.set_postfix(postfix)\n            pbar.update(1)\n\n    # Guarda args\n    with (model_p \/ f'{prefix}.args').open('wb') as f:\n        pickle.dump(args, f)\n\n    # Predice con el mejor checkpoint\n    model.load_state_dict(torch.load(checkpointer.best_path))\n    model.eval()\n    df.loc[valid_idx, 'pred'] = predict(model, valid_dl)\n","d905409d":"def train():\n    args.models.mkdir(exist_ok=True)\n    seed_everything(args.seed)\n\n    # Crea dirs para la config del modelo y datos del tokenizador\n    for d in ['cfgs', 'toks']:\n        Path(d).mkdir(exist_ok=True)\n        \n    # Descarga config del modelo + tokenizer para la inferencia (no tendremos internet)\n    model_config = AutoConfig.from_pretrained(args.model_name, add_pooling_layer=False)\n    model_config.save_pretrained(f'cfgs\/{args.model_name}')\n    tok = AutoTokenizer.from_pretrained(args.model_name)    \n    tok.save_pretrained(f'toks\/{args.model_name}')\n\n    # Divide conjunto de datos en train\/val\n    df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\n    df['pred'] = pd.NA\n    ts = datetime.strftime(datetime.now(), '%Y%m%d_%H%M%S')\n    cv = KFold(n_splits=args.n_folds, shuffle=True, random_state=args.kfold_seed)\n\n    for fold, (train_idx, valid_idx) in enumerate(cv.split(df)):\n        train_fold(fold, df, train_idx, valid_idx, ts)\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    # Validaci\u00f3n cruzada (CV) de todo el dataset\n    cv = mean_squared_error(df.target, df.pred, squared=False)\n    print(f\"cv={cv}\")\n    df.to_csv(args.models \/ ts \/ f'{args.model_name}_oof_preds_{cv:.6f}.csv', index=False)\n","d8e84c22":"def infer():\n    # Carga config del modelo, modelo y tokenizador\n    model_config = AutoConfig.from_pretrained(args.infer_path \/ 'cfgs' \/ args.model_name, add_pooling_layer=False)\n    model_config.output_hidden_states = True\n    model_config.hidden_dropout_prob = args.trf_do\n    model_config.attention_probs_dropout_prob = args.trf_do\n    body = AutoModel.from_config(model_config, add_pooling_layer=False)\n    model = ReadabilityModel(args, body)\n    model.cuda()\n    tokenizer = AutoTokenizer.from_pretrained(args.infer_path \/ 'toks' \/ args.model_name, config=model_config)\n\n    # Carga datos de test\n    test_df = pd.read_csv(args.data \/ 'test.csv')\n\n    # Dataset y dataloader\n    test_ds = ReadabilityDataset(test_df, tokenizer, max_len=args.seq_len)\n    test_dl = DataLoader(dataset=test_ds, batch_size=args.bs * 8, num_workers=0)\n\n    # Acumula las predicci\u00f3nes de los 5 folds aqu\u00ed\n    preds_l = []\n\n    for model_p in (args.infer_path \/ args.models).iterdir():\n        for pt_p in model_p.glob('*.pt'):\n            print(f\"Infer {str(pt_p)}\")\n            # Carga los params del .pt\n            model.load_state_dict(torch.load(pt_p))\n            \n            preds = predict(model, test_dl)\n            preds_l.append(preds)\n\n    all_preds = torch.stack(preds_l).mean(dim=0)\n    \n    test_df['target'] = all_preds\n    sub_df = test_df[['id', 'target']]\n\n    sub_df.to_csv('submission.csv', index=False)","d8f30c5e":"if args.mode == 'train':\n    train()\nelif args.mode == 'infer':\n    infer()","7ac280ee":"# Funci\u00f3n de p\u00e9rdidas","8d4f4efc":"# Datos","33605cef":"# Entrenamiento de 1 fold","219b99b9":"# Inferencia","d8d7a67d":"# Definici\u00f3n del modelo\nDimensiones:\n* `bs`: batch size\n* `max_len`: longitud de la secuencia\n* `d_model`: dimensi\u00f3n del transformer (768 roberta-base, 1024 roberta-large)","11df7001":"# Funciones auxiliares","9017c119":"# Guarda el mejor modelo","f6ee344c":"# Predicci\u00f3n","8da2ec2a":"# Entrenamiento K folds","3852b049":"# Validaci\u00f3n"}}