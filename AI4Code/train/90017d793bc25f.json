{"cell_type":{"0e58402e":"code","f17ce6fa":"code","9620c4db":"code","5cd4fc27":"code","c19029f2":"code","52929e19":"code","ebef2b44":"code","66f806f2":"code","87eb148e":"code","645e5b4a":"code","6b5098b3":"code","2f9bb5e5":"code","77f6e606":"code","dda5d9c4":"code","6a168398":"code","470d45d8":"code","7284ffd2":"code","f067a028":"code","3d30ffec":"code","140de70d":"markdown","338d7ce7":"markdown","196c07fb":"markdown","440e1fc1":"markdown","2107fe84":"markdown","abaffcb0":"markdown","51ff830b":"markdown","ab99dc1a":"markdown","c0654bd8":"markdown","7336f520":"markdown","ddad3006":"markdown","d2949d3f":"markdown","e918f817":"markdown","6aacd132":"markdown","76d02499":"markdown","62ea316a":"markdown","2545418d":"markdown","8e20716e":"markdown","917d68dc":"markdown","c300c36d":"markdown","3d85964a":"markdown","4455c993":"markdown","e541b07e":"markdown","8f31e681":"markdown","256992b6":"markdown","c4c42c19":"markdown","181bc35a":"markdown","45ced894":"markdown","885d31b4":"markdown","8769b84e":"markdown","a071b14f":"markdown","f01dbe09":"markdown","17bfc522":"markdown","e97074ba":"markdown","4597ba60":"markdown"},"source":{"0e58402e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndjia= pd.read_csv('..\/input\/stocknews\/upload_DJIA_table.csv',parse_dates=['Date'], index_col='Date')\ndjia.head()\n","f17ce6fa":"ts=djia['Open']\nplt.plot(ts)\nplt.show()","9620c4db":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    # Determining Rolling Statistics\n    rolmean=timeseries.rolling(window=12).mean()\n    rolstd=timeseries.rolling(window=12).std()\n    \n    #Plot Rolling Statistics\n    orig=plt.plot(timeseries,color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Plot Dickey-Fuller Test\n    \n    print('Results of Dickey-Fuller Test:')\n    dftest=adfuller(timeseries , autolag='AIC')\n    dfoutput=pd.Series(dftest[0:4],index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    \n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key]=value\n    print (dfoutput)\n    ","5cd4fc27":"test_stationarity(ts)","c19029f2":"#taking log transformation\n\nts_log=np.log(ts)\n\nplt.plot(ts_log)","52929e19":"import matplotlib.gridspec as gridspec\n\nts_log = np.log(ts)\nfig = plt.figure(constrained_layout = True)\ngs_1 = gridspec.GridSpec(2, 3, figure = fig)\nax_1 = fig.add_subplot(gs_1[0, :])\nax_1.plot(ts_log)\nax_1.set_xlabel('Year')\nax_1.set_ylabel('Data')\nplt.title('Logarithmic time series')\n\nax_2 = fig.add_subplot(gs_1[1, :])\nax_2.plot(ts)\nax_1.set_xlabel('Year')\nax_1.set_ylabel('Data')\nplt.title('Original time series')\nplt.show()","ebef2b44":"mov_avg = ts_log.rolling(window=12).mean()\nplt.plot(ts_log)\nplt.plot(mov_avg, color='red')","66f806f2":"from sklearn import datasets, linear_model\n\nts_wi = ts_log.reset_index()\ndf_values = ts_wi.values\ntrain_y = df_values[:,1]\ntrain_y = train_y[:, np.newaxis]\ntrain_x = ts_wi.index\ntrain_x = train_x[:, np.newaxis]\nregr = linear_model.LinearRegression()\nregr.fit(train_x, train_y)\npred = regr.predict(train_x)\nplt.plot(ts_wi.Date, pred)\nplt.plot(ts_log)","87eb148e":"ts_log_moving_avg_diff= ts_log-mov_avg\nts_log_moving_avg_diff.dropna(inplace=True)\ntest_stationarity(ts_log_moving_avg_diff)\n","645e5b4a":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_log,freq=4, model='additive')\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(ts_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","6b5098b3":"#ts_decompose = residual\nts_log_diff = ts_log - ts_log.shift(1)\nts_decompose = ts_log_diff\nts_decompose.dropna(inplace=True)\ntest_stationarity(ts_decompose)","2f9bb5e5":"#ACF and PACF plots:\nfrom statsmodels.tsa.stattools import acf, pacf\n\nlag_acf = acf(ts_log_diff, nlags=20)\nlag_pacf = pacf(ts_log_diff, nlags=20, method='ols')\n\n#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","77f6e606":"from statsmodels.tsa.arima_model import ARIMA","dda5d9c4":"model = ARIMA(ts_log, order=(2, 1, 0))  \nresults_AR = model.fit(disp=-1)  \nplt.plot(ts_log_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_diff)**2))","6a168398":"model = ARIMA(ts_log, order=(0, 1, 2))  \nresults_MA = model.fit(disp=-1)  \nplt.plot(ts_log_diff)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_MA.fittedvalues-ts_log_diff)**2))","470d45d8":"predictions_ARIMA_diff = pd.Series(results_AR.fittedvalues, copy=True)\nprint (predictions_ARIMA_diff.head())","7284ffd2":"predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint (predictions_ARIMA_diff_cumsum.head())","f067a028":"predictions_ARIMA_log = pd.Series(ts_log.iloc[0], index=ts_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA_log.head()","3d30ffec":"predictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(ts)\nplt.plot(predictions_ARIMA)\nplt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)\/len(ts)))","140de70d":"# Forecasting the time series","338d7ce7":"# Using ARIMA to forecast the data","196c07fb":"# Estimating and Eliminating Trend","440e1fc1":"We can see that RSS for AR model is slightly better","2107fe84":"Since we have already done moving averages, let us use it (smoothing) to eliminate the trends","abaffcb0":"**It is clearly evident that there is an overall increasing trend in the data along with some seasonal variations. However, it might not always be possible to make such visual inferences (we\u2019ll see such cases later). So, more formally, we can check stationarity using the following:**","51ff830b":"# Taking it back to original scale","ab99dc1a":"# Dickey Fuller Test","c0654bd8":"**We\u2019ll be using the rolling statistics plots along with Dickey-Fuller test results a lot so I have defined a function which takes a TS as input and generated them for us. Please note that I\u2019ve plotted standard deviation instead of variance to keep the unit similar to mean.**","7336f520":"**This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the \u2018Test Statistic\u2019 is less than the \u2018Critical Value\u2019, we can reject the null hypothesis and say that the series is stationary.**","ddad3006":"**We can plot the moving average or moving variance and see if it varies with time. By moving average\/variance I mean that at any instant \u2018t\u2019, we\u2019ll take the average\/variance of the last year, i.e. last 12 months. But again this is more of a visual technique.**","d2949d3f":"# Plotting Rolling Statistics","e918f817":"**Here we use transformation. Using transformation such as log or square root function can penalize higher values more than smaller values**","6aacd132":"# MA Model","76d02499":"**Eliminating Trend**","62ea316a":"**Seasonal decomposing is the fastest way to remove trend and seasonality components from a time serie to becoming it stationary.**","2545418d":"1. A strictly stationary series with no dependence among the values. This is the easy case wherein we can model the residuals as white noise. But this is very rare.\n2. A series with significant dependence among values. In this case we need to use some statistical models like ARIMA to forecast the data.","8e20716e":"**Though the variation in standard deviation is small, mean is clearly increasing with time and this is not a stationary series. Also, the test statistic is way more than the critical values.**","917d68dc":"**Therefore to get a stationary TS we must identify and eliminate Trend and Seasonality**","c300c36d":"Since the series is now stationary, we can perform forecasting on the data","3d85964a":"**Linear Regression**","4455c993":"# AR Model","e541b07e":"**To make a TS stationary, we must first understand what makes it non-stationary.There are 2 reasons**\n\nTrend and Seasonality","8f31e681":"**Eliminating trends are absolutely necessary as TS are time dependent and developing a regression model requires stationarity.**","256992b6":"1. p \u2013 The lag value where the PACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case p=2.\n2. q \u2013 The lag value where the ACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case q=2.","c4c42c19":"**Moving Average**","181bc35a":"Now, lets make 2 different ARIMA models considering individual as well as combined effects. I will also print the RSS for each. Please note that here RSS is for the values of residuals and not actual series.","45ced894":"First of all we define time series(TS). A TS is a collection of data points collected at specific time intervals i.e they are time varying data.\nA time series is said to be stationary if its statistical properties such as mean, variance remain constant over time.","885d31b4":"# Time Series Stationarity","8769b84e":"**Decomposing**","a071b14f":"In this plot, the two dotted lines on either sides of 0 are the confidence interevals. These can be used to determine the \u2018p\u2019 and \u2018q\u2019 values as:","f01dbe09":"There are 3 ways to eliminate trends\n1. Aggregation - taking average of a time period\n2. Smoothing - taking rolling averages\n3. Polynomial fitting - taking a regression model","17bfc522":"**What is time series stationarity?**\n\n","e97074ba":"# Making the time series stationary","4597ba60":"In this approach, we take average of \u2018k\u2019 consecutive values depending on the frequency of time series. Here we can take the average over the past 1 year, i.e. last 12 values. Pandas has specific functions defined for determining rolling statistics"}}