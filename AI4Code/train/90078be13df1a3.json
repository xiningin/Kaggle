{"cell_type":{"7501324d":"code","2448e181":"code","55e004f1":"code","391b5335":"code","e5b98ef6":"code","b7849f33":"code","4bc6d130":"code","e7ab170e":"code","6244c000":"code","efe2efe0":"code","dec6e200":"code","ca29b2c4":"code","18d1ae42":"code","a8fc2c46":"code","ef0d1901":"code","cacfcaaf":"code","d96101cb":"code","3db42926":"code","56e33cc4":"code","4dea35aa":"code","f8852d32":"code","489cb3d4":"code","da491266":"code","245fac37":"code","ae4fb9ca":"code","b6fb7849":"code","cbde1608":"code","709afaca":"code","7eace8c9":"code","da88e046":"code","9baecaba":"code","0ac9365a":"code","09c541c7":"code","f9e49d41":"code","308bac03":"code","b412be28":"code","f352b6e7":"code","5ff2df70":"code","a162b716":"code","1afbea29":"code","335abbc3":"code","b00f63d2":"code","2afb6ee9":"code","8ca783d3":"code","10ac8b6a":"code","79940cc0":"code","fa4bfc0c":"code","d1738b47":"code","62c41c0a":"code","101a54b8":"code","3100dbdb":"code","af777d43":"code","ac52947d":"code","bf07f0c3":"code","c4499ace":"code","d7050fd5":"code","3118d20e":"code","072058c9":"code","541400f9":"code","0c66c7e2":"code","1d4d6152":"code","515b15d2":"code","5eb6dd88":"code","c36a9f3c":"markdown","bb8dd1b5":"markdown","94db71f3":"markdown","45877972":"markdown","f2d28b1c":"markdown","90cf5b5b":"markdown","c54933b3":"markdown","7896d42a":"markdown","34a090f8":"markdown","ab9d8d8a":"markdown","82d0fdea":"markdown","730684d3":"markdown","2d6993cd":"markdown","a63838b9":"markdown","6fdc7909":"markdown","ed101ad7":"markdown","e87672ed":"markdown","d10f1c7c":"markdown","d8406f19":"markdown","d9d6610e":"markdown","21669554":"markdown","c56b1b19":"markdown","4e33f69d":"markdown","31fb5585":"markdown","8ab2e2d2":"markdown","fd22bee8":"markdown","c52e8fe4":"markdown","917b0ab6":"markdown","804e84c4":"markdown","dc19d5b2":"markdown","f4cabb12":"markdown","52ecf40c":"markdown","ace8263f":"markdown","54a15723":"markdown","2ed1781f":"markdown","4743e288":"markdown","5ca2407d":"markdown"},"source":{"7501324d":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2448e181":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","55e004f1":"data = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/ford.csv')","391b5335":"data.drop_duplicates(inplace=True)","e5b98ef6":"data","b7849f33":"data.info()","4bc6d130":"data.nunique()","e7ab170e":"data.iloc[0]","6244c000":"data.model.unique()","efe2efe0":"data.model = data.model.str.strip().str.lower().str.replace(' ', '')","dec6e200":"data.transmission.unique()","ca29b2c4":"data.transmission = data.transmission.str.lower()","18d1ae42":"data.fuelType.unique()","a8fc2c46":"data.fuelType = data.fuelType.str.lower()","ef0d1901":"data.year.unique()","cacfcaaf":"data.year = 2021 - data.year","d96101cb":"data = data[~(data['year'] == -39)]","3db42926":"data.year.unique()","56e33cc4":"data['year'].value_counts().plot.barh(figsize=(5, 5))","4dea35aa":"data.model.value_counts()","f8852d32":"data.groupby('model')['price'].agg(['count', 'mean']).sort_values('count', ascending=True)","489cb3d4":"data.fuelType.value_counts()","da491266":"data.groupby('fuelType')['price'].agg(['count', 'mean'])","245fac37":"data[data['fuelType'] == 'electric']","ae4fb9ca":"data[data['fuelType'] == 'other']","b6fb7849":"data['fuelType'].replace('other', 'electric', inplace=True)","cbde1608":"data.fuelType.value_counts()","709afaca":"data['fuelType'].value_counts().plot.barh(figsize=(5, 5))","7eace8c9":"data.price.hist(bins=60)","da88e046":"plt.figure(figsize=(10,15))\nsns.boxenplot(x=\"price\", y=\"model\",\n              color=\"b\",\n              scale=\"linear\", data=data)","9baecaba":"print('Mustang models:', len(data[data['model'] == 'mustang']))\nprint('Percentage of mustang models:', round(len(data[data['model'] =='mustang'])*100\/len(data), 2), '%')","0ac9365a":"data_mean, data_std = np.mean(data.price), np.std(data.price)\ncut_off = data_std * 3\nlower, upper = data_mean - cut_off, data_mean + cut_off","09c541c7":"print('Outliers:', len(data[~data['price'].between(lower, upper)]))\nprint('Percentage of emissions:', round(len(data[~data['price'].between(lower, upper)])*100\/len(data), 2), '%')","f9e49d41":"data = data[data['price'].between(lower, upper)]","308bac03":"data.transmission.value_counts()","b412be28":"data.info()","f352b6e7":"data = pd.get_dummies(data)","5ff2df70":"data","a162b716":"pd.DataFrame(data.corr()['price'].abs()).sort_values('price').T","1afbea29":"X = data.drop(['price'], axis=1)\ny = data.price\nmodel_OLS = sm.OLS(y, X).fit()\nmodel_OLS.summary()","335abbc3":"print('R-squared', model_OLS.rsquared)\nprint('Adj. R-squared', model_OLS.rsquared_adj)","b00f63d2":"print(dir(model_OLS))","2afb6ee9":"pd.DataFrame(model_OLS.pvalues, columns = ['pvalues']).head(7)","8ca783d3":"p_value = pd.DataFrame(model_OLS.pvalues, columns = ['pvalues'])","10ac8b6a":"p_value = p_value.style.format({'pvalues': '{:.7f}'})","79940cc0":"p_value","fa4bfc0c":"X = data.drop(['price', 'model_transittourneo'], axis=1)\ny = data.price\nmodel_OLS = sm.OLS(y, X).fit()\nprint(model_OLS.summary())","d1738b47":"X = data.drop(['price', 'model_transittourneo', 'model_grandc-max', 'model_ecosport', 'tax', 'model_ranger'], axis=1)\ny = data.price\nmodel_OLS = sm.OLS(y, X).fit()\nprint(model_OLS.summary())","62c41c0a":"X.head()","101a54b8":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","3100dbdb":"models = []\n\nmodels.append(('LR', LinearRegression()))\nmodels.append(('R', Ridge()))\nmodels.append(('L', Lasso()))\nmodels.append(('BR', BayesianRidge(n_iter=1000)))\nmodels.append(('KNR', KNeighborsRegressor()))\nmodels.append(('DTR', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('ABR', AdaBoostRegressor(n_estimators=300)))\nmodels.append(('BR', BaggingRegressor(n_estimators=300)))\nmodels.append(('ETR', ExtraTreesRegressor(n_estimators=300)))\nmodels.append(('GBR', GradientBoostingRegressor(n_estimators=300)))\nmodels.append(('RFR', RandomForestRegressor(n_estimators=300)))\nmodels.append(('XGB', XGBRegressor(n_estimators=300)))\nfor name, model in models:\n    model.fit(x_train, y_train)\n    m_predict = model.predict(x_test)\n    m_score = r2_score(y_test, m_predict)\n    print(\"%s: r2_score = %.3f\" % (name, m_score))","af777d43":"XGBR = XGBRegressor(n_estimators = 300, eta=0.07, gamma=0.1, \n                    max_depth=8, min_child_weight=6, \n                    colsample_bytree=0.6, subsample=0.9)\nXGBR.fit(x_train, y_train)\ny_pred = XGBR.predict(x_test)","ac52947d":"r2 = round(r2_score(y_test, y_pred), 3)\nrmse = round(np.sqrt(mean_squared_error(y_test, y_pred, squared=False)), 3)\nprint('R2 score:', r2)\nprint('RMSE:', rmse)","bf07f0c3":"feat_import = pd.DataFrame(XGBR.feature_importances_, columns = ['value'])","c4499ace":"features = pd.DataFrame(x_train.columns, columns = ['features'])","d7050fd5":"feat_import = pd.concat([features, feat_import], axis=1)","3118d20e":"feat_import.sort_values('value', ascending=False, inplace=True)","072058c9":"feat_import","541400f9":"feat_import.plot.barh(x ='features', figsize=(13, 9))","0c66c7e2":"print(list(feat_import.features))","1d4d6152":"X = data.drop(['price', 'model_transittourneo', 'model_grandc-max', 'model_ecosport', 'tax', 'model_ranger', 'model_c-max',\n 'transmission_manual',\n 'model_grandtourneoconnect',\n 'model_tourneocustom',\n 'fuelType_hybrid',\n 'model_fusion',\n 'model_tourneoconnect',\n 'transmission_automatic',\n 'transmission_semi-auto',\n 'model_mustang',\n 'model_escort',\n 'fuelType_electric',\n 'model_streetka'], axis=1)\ny = data.price\nmodel_OLS_2 = sm.OLS(y, X).fit()\nprint(model_OLS_2.summary())","515b15d2":"print('R-squared', model_OLS.rsquared)\nprint('Adj. R-squared', model_OLS.rsquared_adj)","5eb6dd88":"print('R-squared', model_OLS_2.rsquared)\nprint('Adj. R-squared', model_OLS_2.rsquared_adj)","c36a9f3c":"**Removing `model_transittourneo`  -  0.7243908.**","bb8dd1b5":"**Take `p_value`, formalize it into a dataframe and replace exponential numbers with prime ones. The value of `p_value` tells us how much the variable affects the target variable. The closer to 1, the worse. We will now delete variables and build the model anew to reduce the value of `p_value` for all signs to 0**","94db71f3":"**Most of it is `manual` transmission**","45877972":"# Training the model","f2d28b1c":"**<li>Up to `XGBRegressor` the result is `R-squared` 0.849**\n**<li>after learning on the `XGBRegressor` algorithm and removing weak features - `R-squared` 0.979**","90cf5b5b":"**It was 0.932, it became 0.941. Let's see which signs affected more**","c54933b3":"**Let's apply the `get_dummies` method to the features `model`, `transmission`, `FuelType`**","7896d42a":"## year","34a090f8":"## model","ab9d8d8a":"**`Fiesta`, `focus`, and `kuga` models represent 75% of all cars in the dataset. I wanted to create a separate category for unpopular models - `other`, but grouping by models showed that they differ greatly in price. And it's easier to split each model into 1\/0 columns in order to 'feed' it to the algorithm - we don't have to count manually )**","82d0fdea":"**Let's add an element from the `other` category to `electric` - they have the same `model`, `transmission`, `tax`, `mpg` and `engineSize`**","730684d3":"## XGBRegressor","2d6993cd":"# Importing Necessary Libraries","a63838b9":"# Data preparation and analysis","6fdc7909":"**For training, we will take not all brands of cars, but one. For example, Ford.**","ed101ad7":"## Correlation","e87672ed":"## fuelType","d10f1c7c":"**We will change the value of the variable to the age of the car, we will take 2021 as the countdown**","d8406f19":"**Let's take those signs where the value is less than 0.01**","d9d6610e":"## Learning by algorithms","21669554":"## transmission","c56b1b19":"**The graph shows that all `mustang` models fall into these outliers, if we remove them, the algorithm will not predict their cost.**","4e33f69d":"**Let's run several algorithms, the best algorithm is set up and run again**","31fb5585":"**We extracted these values using `model_OLS.name`, other `name` can be viewed using `dir(model_OLS)`**","8ab2e2d2":"**We removed `model_grand c-max`, `model_ecosport`, `tax`, `model_ranger` until the p-value was close to 0 in all variables**","fd22bee8":"## price","c52e8fe4":"**Pay attention to `R-squared` and `Adj. R-squared` - they show how well the independent variables predict the target variable price. The maximum value is 1.**","917b0ab6":"**I don't like outliers after 30000, let's see what they include**","804e84c4":"## Removing spaces, lowercase letters","dc19d5b2":"**Let's create a dataframe with the attributes and their values, sort them in descending order. `Model_mustang`, `model_escort`, `fuelType_electric`, `model_streetka` didn't bring anything to our training.**  \n  \n**Let's remove the signs with a small value and perform a regression analysis again**    ","f4cabb12":"**Let's build a correlation matrix, transpose it so that it is convenient to read. Variables with low correlation can be deleted now, but let's leave them to delete them at the stage of regression analysis.**","52ecf40c":"# Reading the Dataset","ace8263f":"**<li>model, transmission, fuelType -categorical (text) variables  >>>>   get_dummies**\n**<li>year, price, mileage, tax, mpg, engineSize - continuous variables    >>>>   no changes**","54a15723":"**`XGBRegressor` showed 0.932, so we will work with it**","2ed1781f":"## Regression analysis","4743e288":"**We removed outliers, applied the `get_dummies` method to text (categorical) variables, and replaced the `year`.**  \n  \n**In total, we have 17634 elements and 36 variables.**","5ca2407d":"**There are very few mustangs in the dataset - 99.68% of other Ford models. A third of the outliers are mustangs, the rest are other models. If we leave the mustangs, they will correlate very strongly with the price, overshadowing other signs in our training. So let's remove all outliers. The price distribution is close to the normal distribution, so we use 3 standard deviations to remove outliers.**"}}