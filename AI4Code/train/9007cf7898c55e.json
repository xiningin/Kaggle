{"cell_type":{"4eabc047":"code","416d5bec":"code","cde83276":"code","d6a07b44":"code","77b8e42a":"code","097dec5d":"code","5730280e":"code","ae032c2f":"code","dbc3de18":"code","e39ac40d":"code","dff6a90d":"code","633818f3":"code","9d2e3ce0":"code","b5abf689":"code","076d0811":"code","0c8c8806":"code","cc93e449":"code","6950f59e":"code","f334a865":"code","26dfc4ec":"code","62372abe":"code","c2f41fd8":"code","2a45786a":"code","eddbaeef":"code","1462505c":"code","e88f0a1c":"code","9e1978fd":"code","5c645077":"code","16b7496b":"code","1b092c91":"code","2afc8360":"code","235a2410":"code","a583dda7":"code","f8718b33":"code","fa7afb85":"code","c1fb382e":"code","3a57671f":"code","3ec2095c":"code","8837b5c6":"code","43180274":"code","d06ad6f3":"code","a141ba45":"code","efc0af2f":"code","4056a51b":"code","f779c701":"code","2169d6ce":"code","f80d1aa5":"code","dc4ac678":"code","a90add02":"code","fac49d51":"code","ab92cf9d":"code","36b663eb":"code","65a1e053":"code","b1738afb":"code","b095a91b":"code","7d5e8199":"code","63297a09":"code","d6707c06":"code","94f0e3f3":"code","c35ebe40":"code","6faab549":"code","81e301fc":"code","06484339":"code","38aaa4c7":"code","d4e7a081":"code","434efb36":"code","90135c68":"code","1966fc47":"code","1f9d27b5":"code","c0be9f8e":"code","e03f8342":"code","901613eb":"code","b85b91d9":"code","8e244027":"code","62fb461a":"code","48bacf01":"code","a83b243e":"code","ca8350b9":"code","4f8927fa":"code","d34f4ee0":"code","397576e0":"code","cde3458f":"code","266afe1f":"code","44d369ee":"code","c298857d":"code","36cfc7ce":"code","b82cc0a7":"code","799171b3":"code","fb62be5f":"code","e9f211ec":"code","6bcc80c4":"code","c79b6d4a":"code","029c6cc7":"code","5c73a837":"code","61000f4e":"code","c96793d5":"code","058dc5d1":"code","b0240487":"code","cf4a5c3f":"code","12cbedf6":"code","7123fb74":"code","0767c002":"code","7b35f097":"code","8eb59cab":"code","1ce49fc1":"code","b54d71fb":"code","1edb83f4":"code","ed8c7229":"code","684de50d":"code","4ac4c371":"code","98d3bfbe":"markdown","4a512880":"markdown","cc459b97":"markdown","4e18f6aa":"markdown","ccea35af":"markdown","e0a494b3":"markdown","4f5ab4f2":"markdown","53ad787c":"markdown","4541c8c2":"markdown","a262823b":"markdown","2e315791":"markdown","9308e06e":"markdown","75aec8a8":"markdown","025baab2":"markdown","f50a2185":"markdown","459eb229":"markdown","09644648":"markdown","8a90943f":"markdown","a426b2a1":"markdown","f9bbb636":"markdown","2b7da09a":"markdown","f2584ce7":"markdown","024a9885":"markdown","b4e4b65a":"markdown","21ce8ea2":"markdown","406eda2a":"markdown","35f9df95":"markdown","5b23a81e":"markdown","9cef59ab":"markdown","013a076b":"markdown","7f76e17a":"markdown","2b4c821b":"markdown","ff98c648":"markdown","9d98000f":"markdown","dc1776c3":"markdown","435cdd57":"markdown","d6d73bd5":"markdown","dc826caa":"markdown","da25649c":"markdown","dcb1d5f6":"markdown","f5260057":"markdown","daf0a0eb":"markdown","cff0d1c2":"markdown","bfbfc868":"markdown","a099fa27":"markdown","5f3fbb82":"markdown","935293c3":"markdown","c27f5338":"markdown","d54ae567":"markdown","8e932eed":"markdown","5bc7ea02":"markdown","9237075e":"markdown","d4a30ac1":"markdown","b46df649":"markdown","b5e02242":"markdown","f3a5e24c":"markdown","4ea872c0":"markdown","287febd8":"markdown","44ec2b36":"markdown","61be9d29":"markdown","af81d930":"markdown","b3c1e7e1":"markdown","5945b0ce":"markdown","680c0ab3":"markdown","0222018c":"markdown"},"source":{"4eabc047":"import os\n!pip install numba==0.50 \nimport numpy as np \nnp.random.seed(123)\n\nimport pandas as pd \npd.set_option(\"display.max_rows\", 1000)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.ensemble import IsolationForest as ISOF\nimport umap\nimport statsmodels.api as sm\n\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error, r2_score, max_error\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import KernelDensity as kde\n\nimport keras as ks\nimport tensorflow as tf\n\nfrom scipy import interpolate\n\n!pip install trimap\nimport trimap\n\n!pip install pacmap==0.4\nimport pacmap\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","416d5bec":"Data = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\nData.head(n=5)","cde83276":"parCo = go.Figure(data=\n    go.Parcoords(\n        line = dict(color = Data['quality'],\n                    colorscale = 'Tealrose',\n                    showscale = True,\n                    cmin = 3,\n                    cmax = 8,\n                    reversescale = True),\n        dimensions = list([\n            dict(label = \"fixed acidity\", \n                 values = Data['fixed acidity']),\n            dict(label = 'volat. acidity', \n                 values = Data['volatile acidity']),\n            dict(label = 'citric acid', \n                 values = Data['citric acid']),\n            dict(label = 'res. sugar', \n                 values = Data['residual sugar']),\n            dict(label = 'chlorides', \n                 values = Data['chlorides']),\n            dict(label = 'free sulf. diox.', \n                 values = Data['free sulfur dioxide']),\n            dict(label = 'tot. sulf. diox.', \n                 values = Data['total sulfur dioxide']),\n            dict(label = 'density', \n                 values = Data['density']),\n            dict(label = 'pH', \n                 values = Data['pH']),\n            dict(label = 'sulphates', \n                 values = Data['sulphates']),\n            dict(label = 'alcohol', \n                 values = Data['alcohol']),\n            dict(label = 'quality', \n                 values = Data['quality']),\n        ])\n    )\n)\nparCo.show()","d6a07b44":"def Normalization(DF, cols):\n    DF=DF.copy()\n    for c in cols:\n        DF[f\"{c}\"] = ((DF[\"{}\".format(c)]-DF[\"{0}\".format(c)].mean()) \/ DF[\"{}\".format(c)].std())\n        \n    return DF","77b8e42a":"Data = Normalization(DF=Data, cols=Data.drop(columns=[\"quality\"]).columns)","097dec5d":"pca = PCA(n_components=Data.shape[1]-1, \n          random_state=1\n          )\n\ntransformed = pca.fit_transform(Data.drop(columns=[\"quality\"]))","5730280e":"Reduced = pd.DataFrame(transformed)","ae032c2f":"def scatterPlot(x,y,df,color,colorName=\"quality\", a=0.65, main_title=\"\"):\n    df=df.copy()\n    df[f\"{colorName}\"]=color\n    #plt.figure(figsize=(12,9))\n\n    s=sns.JointGrid(\n        x=x, \n        y=y, \n        data=df, \n        hue=f\"{colorName}\",\n        palette=\"viridis\",\n        height=10,\n        ratio=5\n        )\n    s.plot_joint(sns.scatterplot, s=100, alpha=a)\n    s.plot_marginals(sns.kdeplot, cut=0)\n    s.fig.suptitle(main_title, fontsize=15)\n    \n    ax = plt.gca()\n    #ax.set_title(main_title)","dbc3de18":"scatterPlot(x=0,y=1,df=Reduced,color=Data.quality, main_title=\"PCA first 2 Dimensions\")","e39ac40d":"def barPlot(y, threshold):\n    fig, axs = plt.subplots(1,2, figsize=(20,6))\n    #plt.figure(figsize=(12,9))\n    heights1=y\n    heights2=y.cumsum()\n    colors=heights2 >= threshold\n    \n    axs[0].bar(height=heights1, x=range(1,len(y)+1), width=0.8, color=[\"black\" if x == True else \"maroon\" for x in colors])\n    axs[0].set_xticks(ticks=range(1,len(y)+1))\n    axs[0].set_yticks(ticks=np.array(list(range(0,11)))\/10)\n    \n    axs[1].bar(height=heights2, x=range(1,len(y)+1), width=0.8, color=[\"black\" if x == True else \"maroon\" for x in colors])\n    axs[1].axhline(y=threshold, xmin=0, xmax=3, c=\"black\")\n    axs[1].set_xticks(ticks=range(1,len(y)+1))\n    axs[1].set_yticks(ticks=np.array(list(range(0,11)))\/10)\n    axs[1].set_title(\"Cumul. Variance with \" + str(sum(colors == False)+1) + \" Components\")\n    plt.show()","dff6a90d":"barPlot(y=pca.explained_variance_ratio_, threshold=0.9)","633818f3":"def total_variance(DF, subDF):\n    variance, subVariance = 0, 0\n    \n    for c in DF.drop(columns=[\"quality\"]).columns:\n        variance = variance + np.var(DF[\"{0}\".format(c)])\n        \n    for s in range(len(subDF.columns)):\n        subVariance = subVariance + np.var(subDF.iloc[:,s])    \n        \n    result = \"Variance left in reduced dimensions: \" + str(round(subVariance \/ variance * 100, 1)) + \" %\"\n    return result","9d2e3ce0":"total_variance(Data, Reduced.iloc[:,[0,1]])","b5abf689":"def Comparison(DF, subDF):\n    \"\"\"this function fits a svr with the reduced data as predictors to each original column iteratively with a loop. \\n\n regression target = original column \\n predictors = all columns for the reduced dataset\"\"\"\n    \n    DF=DF.copy()\n    subDF=subDF.copy()\n    subDF.columns=[str(i) for i in subDF.columns]\n    \n    r2=[]\n    mae=[]\n    maxErr=[]\n    model = GBR()\n    \n    for i in range(DF.shape[1]):\n        model.fit(X=subDF, y=DF.iloc[:,i])\n        pred = model.predict(X=subDF)\n        #r2.append(model.score(X=subDF, y=DF.iloc[:,i]))\n        r2.append(r2_score(y_true=DF.iloc[:,i], y_pred=pred))\n        mae.append(mean_absolute_error(y_true=DF.iloc[:,i], y_pred=pred))\n        maxErr.append(max_error(y_true=DF.iloc[:,i], y_pred=pred))\n        \n        \n    return r2, mae, maxErr","076d0811":"pcar2, pcamae, pcamaxerr=Comparison(Data, Reduced.iloc[:,[0,1]])","0c8c8806":"tSNE = TSNE(\n    n_components=2, \n    perplexity=60,\n    init = 'random'\n    )\n\ntransformed = tSNE.fit_transform(Data.drop(columns=[\"quality\"]))","cc93e449":"Reduced = pd.DataFrame(transformed)","6950f59e":"scatterPlot(x=0,y=1,df=Reduced,color=Data.quality, main_title=\"t-SNE Reduced to 2 Dimensions\")","f334a865":"tSNEr2, tSNEmae, tSNEmaxerr=Comparison(Data, Reduced.iloc[:,[0,1]])","26dfc4ec":"UMP = umap.UMAP(\n    n_neighbors=20, \n    min_dist=0.5,\n    n_components=2,\n    spread=3\n    )\n\ntransformed = UMP.fit_transform(Data.drop(columns=[\"quality\"]))","62372abe":"Reduced = pd.DataFrame(transformed)","c2f41fd8":"scatterPlot(x=0,y=1,df=Reduced,color=Data.quality, main_title=\"UMAP Reduced to 2 Dimensions\")","2a45786a":"UMPr2, UMPmae, UMPmaxerr=Comparison(Data, Reduced.iloc[:,[0,1]])","eddbaeef":"TriMap = trimap.TRIMAP(\n    n_inliers=10, \n    n_outliers=50, \n    n_random=5, \n    apply_pca=False,\n    verbose=False\n    )\n\ntransformed = TriMap.fit_transform(Data.drop(columns=[\"quality\"]).values)","1462505c":"Reduced = pd.DataFrame(transformed)","e88f0a1c":"scatterPlot(x=0,y=1,df=Reduced,color=Data.quality, main_title=\"TriMap 2 Dimensions\")","9e1978fd":"TriMapr2, TriMapmae, TriMapmaxerr=Comparison(Data, Reduced.iloc[:,[0,1]])","5c645077":"PaCMP = pacmap.PaCMAP(\n    n_dims=2, \n    n_neighbors=5, \n    MN_ratio=0.5, \n    FP_ratio=2.0\n    ) \n\ntransformed = PaCMP.fit_transform(Data.drop(columns=[\"quality\"]).values)","16b7496b":"Reduced = pd.DataFrame(transformed)","1b092c91":"scatterPlot(x=0,y=1,df=Reduced,color=Data.quality, main_title=\"PaCMAP 2 Dimensions\")","2afc8360":"PaCMAPr2, PaCMAPmae, PaCMAPmaxerr=Comparison(Data, Reduced.iloc[:,[0,1]])","235a2410":"np.random.seed(123)\ntf.random.set_seed(123)\n\ncomp=2\n  \nINPUT = ks.layers.Input(shape=(Data.drop(columns=[\"quality\"]).shape[1]), name=\"EncoderInput\")\n\n#ENCO = ks.layers.BatchNormalization()(INPUT)\nENCO = ks.layers.Dense(32, activation='relu')(INPUT)\n\nENCO = ks.layers.Dropout(0.1)(ENCO)\nENCO = ks.layers.BatchNormalization()(ENCO)\nENCO = ks.layers.Dense(16, activation='relu')(ENCO)\n\nENCO = ks.layers.Dropout(0.1)(ENCO)\nENCO = ks.layers.BatchNormalization()(ENCO)\nENCO = ks.layers.Dense(comp, activation='linear', name=\"compresseion\")(ENCO)\n\nENC = ks.Model(inputs=INPUT, outputs=ENCO)\n\nDECO = ks.layers.BatchNormalization()(ENCO)\nDECO = ks.layers.Dense(16, activation='relu')(DECO)\n\nDECO = ks.layers.Dropout(0.1)(DECO)\nDECO = ks.layers.BatchNormalization()(DECO)\nDECO = ks.layers.Dense(32, activation='relu')(DECO)\n\nDECO = ks.layers.Dropout(0.1)(DECO)\nDECO = ks.layers.BatchNormalization()(DECO)\nDECO = ks.layers.Dense(Data.drop(columns=[\"quality\"]).shape[1], activation='linear')(DECO)\n\nAE = ks.Model(inputs=INPUT, outputs=DECO)","a583dda7":"ks.utils.plot_model(\n    AE, \n    show_shapes=True, \n    show_layer_names=True\n    )","f8718b33":"def rmse(y_pred, y_true):\n    y_pred = tf.cast(y_pred, dtype=\"float32\")\n    y_true = tf.cast(y_true, dtype=\"float32\")\n    r = tf.sqrt(tf.keras.backend.mean(tf.square(y_pred - y_true)))\n    return r","fa7afb85":"stop = ks.callbacks.EarlyStopping(\n    monitor='mae', \n    min_delta=0.000001, \n    patience=100, \n    mode='max'\n    )\n\nlrReducer = ks.callbacks.ReduceLROnPlateau(    \n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=10,\n    verbose=0,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0.000001,\n    )","c1fb382e":"eps=1500\nbs=1600\nleRa=0.1\ndec = 0.0000\n\n\"Learning rate and decay ok?: \" + str(leRa - dec * eps > 0)","3a57671f":"optimizer = ks.optimizers.Adamax(lr=leRa, decay=dec)\nAE.compile(optimizer = optimizer, loss = rmse, metrics = [\"mae\"])","3ec2095c":"Reduced = ENC.predict(\n   x=Data.drop(columns=[\"quality\"]), \n   workers = 1, \n   use_multiprocessing = True\n)\n\nReduced = pd.DataFrame(Reduced)","8837b5c6":"scatterPlot(x=0,y=1,df=Reduced,color=Data.quality, main_title=\"Aotoencoder Compressed to \" + str(comp) + \" Dimensions (Untrained)\")","43180274":"history = ks.callbacks.History()\n\nAE.fit(\n    x=Data.drop(columns=[\"quality\"]), \n    y=Data.drop(columns=[\"quality\"]), \n    epochs = eps, \n    batch_size = bs, \n    shuffle = False,\n    callbacks=[history, lrReducer],\n    verbose=0\n    )\n\nprint(AE.evaluate(Data.drop(columns=[\"quality\"]), Data.drop(columns=[\"quality\"])))\n\n#print(history.history)","d06ad6f3":"Reduced = ENC.predict(\n   x=Data.drop(columns=[\"quality\"]), \n   workers = 1, \n   use_multiprocessing = True\n)\n\nReduced = pd.DataFrame(Reduced)","a141ba45":"scatterPlot(x=0,y=1,df=Reduced,color=Data.quality, main_title=\"Aotoencoder Compressed to \" + str(comp) + \" Dimensions\")","efc0af2f":"autoEncr2, autoEncmae, autoEncmaxerr=Comparison(Data, Reduced.iloc[:,[0,1]])","4056a51b":"def compare(dfList, method_names, title=\"Comparison R\u00b2 Score\"):\n\n    fig, ax = plt.subplots(figsize=(16,9))\n    ax.set_title(title)\n\n    custom_lines = [\n        plt.Line2D([0], [0], color=\"#20A387FF\", lw=3),\n        plt.Line2D([0], [0], color=\"#440154FF\", lw=3)\n        ]\n\n    ax.legend(custom_lines, ['Median', 'Mean'])\n\n    violin_parts = ax.violinplot(\n        dfList, \n        showmeans =True, \n        vert=True, \n        widths=0.75, \n        showmedians=True, \n        showextrema=False\n        )\n\n    plt.xticks(range(1,len(method_names)+1), method_names)\n\n    for pc in violin_parts['bodies']:\n        pc.set_facecolor('#3CBB75FF')\n        pc.set_edgecolor('#404788FF')\n        pc.set_linewidth(2)\n        pc.set_alpha(0.3)\n\n    for partname in ('cmedians', 'cmeans'):\n        vp = violin_parts[partname]\n        if partname == 'cmedians':\n            vp.set_edgecolor('#20A387FF')\n            vp.set_linewidth(3)\n        else:\n            vp.set_edgecolor('#440154FF')\n            vp.set_linewidth(3)\n\n    plt.show()","f779c701":"mNames=['PCA', 'tSNE', 'UMAP', 'TriMap', 'PaCMAP', 'Auto-Encoder']\n\ncomDF=[pcar2, tSNEr2, UMPr2, TriMapr2, PaCMAPr2, autoEncr2]\ncompare(comDF, mNames)","2169d6ce":"#comDF=[pcamae, tSNEmae, UMPmae, TriMapmae, PaCMAPmae, autoEncmae]\n#compare(comDF, mNames, title=\"Comparison MAE Score\")","f80d1aa5":"comDF=[pcamaxerr, tSNEmaxerr, UMPmaxerr, TriMapmaxerr, PaCMAPmaxerr, autoEncmaxerr]\ncompare(comDF, mNames, title=\"Comparison Maximum Error Score\")","dc4ac678":"with open(\"..\/input\/modelnet40-princeton-3d-object-dataset\/ModelNet40\/airplane\/train\/airplane_0200.off\") as raw:\n\n    df = []\n    try:\n        for line in raw:\n            row = raw.readline().strip().split(\" \")\n            if \"OFF\" not in row:\n                df.append([float(i) for i in row])\n    except Exception as e:\n        print(line)\n               \ndf=pd.DataFrame(df)\ndf=df.loc[pd.isna(df[3]),[0,1,2]]\ndf=df.loc[1:,:]\n\ndf.columns = [\"x\", \"y\", \"z\"]\n\ndef coloring(row):\n    \n    if (row[\"z\"] >= 137) & (row[\"y\"] >= 1136) & ((row[\"x\"] >= 1334.27) | (row[\"x\"] <= 1041.04)):\n        c = \"red\"\n    elif (row[\"y\"] <= 1098.2) & ((row[\"x\"] >= 1400) | (row[\"x\"] <= 905)):\n        c = \"green\"\n    elif (row[\"z\"] > 80) & ((row[\"x\"] < 1400) | (row[\"x\"] > 905)):\n        c = \"purple\"\n    else:\n        c = \"black\"\n    return c\n\ndf[\"component\"] = df.apply(coloring, axis=\"columns\")\n\nx,y,z,component=df[\"x\"],df[\"y\"],df[\"z\"],df[\"component\"]\n\ndfNormal = Normalization(df, [\"x\", \"y\", \"z\"])","a90add02":"fig = go.Figure(\n    data=[\n        go.Scatter3d(\n            x=x, \n            y=y, \n            z=z,\n            mode='markers',\n            marker=dict(\n                size=5,\n                opacity=0.9,\n                line =dict(color= \"white\"),\n                color=component\n            )\n        )])\nfig.update_layout(height=800, width=800, title_text=\"Airplane\", template=\"simple_white\")\nfig.show()","fac49d51":"methods={\n    \"PCA\": PCA(2),\n    \"tSNE\": TSNE(n_components=2, perplexity=250),\n    \"UMAP\": umap.UMAP(n_components=2, n_neighbors=350),\n    \"TriMap\": trimap.TRIMAP(verbose=False, n_inliers=20, n_outliers=50, n_random=10),\n    \"PaCMAP\": pacmap.PaCMAP(n_dims=2, n_neighbors=50) \n}","ab92cf9d":"fig, p = plt.subplots(nrows=3, ncols=2, figsize=(20*1.5,30*1.5))\n\nc=0\nr=0\nk=0\n\nfor c in range(2):\n    for r in range(3):\n        if k < 5:\n            transformed=list(methods.values())[k].fit_transform(dfNormal[[\"x\", \"y\", \"z\"]].values)\n            Reduced = pd.DataFrame(transformed)\n            p[r, c].scatter(\n                x=Reduced[0],\n                y=Reduced[1],\n                c=component\n                )\n            p[r, c].set_title(list(methods.keys())[k], fontsize=20)\n            \n            globals()[f\"{list(methods.keys())[k]}r2\"], globals()[f\"{list(methods.keys())[k]}mae\"], globals()[f\"{list(methods.keys())[k]}maxErr\"] \\\n            =Comparison(df[[\"x\", \"y\", \"z\"]], Reduced.iloc[:,[0,1]])\n            \n            k+=1\n        else:\n            np.random.seed(123)\n            tf.random.set_seed(123)\n            \n            comp=2\n  \n            INPUT = ks.layers.Input(shape=(dfNormal[[\"x\", \"y\", \"z\"]].shape[1]), name=\"EncoderInput\")\n\n            ENCO = ks.layers.Dense(8, activation='relu')(INPUT)\n            ENCO = ks.layers.Dropout(0.1)(ENCO)\n            ENCO = ks.layers.BatchNormalization()(ENCO)\n            ENCO = ks.layers.Dense(comp, activation='linear', name=\"compresseion\")(ENCO)\n\n            ENC3D = ks.Model(inputs=INPUT, outputs=ENCO)\n\n            DECO = ks.layers.Dropout(0.1)(ENCO)\n            DECO = ks.layers.BatchNormalization()(DECO)\n            DECO = ks.layers.Dense(8, activation='relu')(DECO)\n            DECO = ks.layers.Dropout(0.1)(DECO)\n            DECO = ks.layers.BatchNormalization()(DECO)\n            DECO = ks.layers.Dense(df[[\"x\", \"y\", \"z\"]].shape[1], activation='linear')(DECO)\n\n            AE3D = ks.Model(inputs=INPUT, outputs=DECO)\n            \n            optimizer3D = ks.optimizers.Adamax(lr=0.1)\n            AE3D.compile(optimizer = optimizer3D, loss = \"mae\", metrics = [\"mae\"])\n            \n            AE3D.fit(\n                x=dfNormal[[\"x\", \"y\", \"z\"]], \n                y=df[[\"x\", \"y\", \"z\"]], \n                epochs = 2000, \n                batch_size = df.shape[0], \n                shuffle = False,\n                callbacks=[lrReducer],\n                verbose=0\n            )\n\n            Reduced=ENC3D.predict(\n               x=dfNormal[[\"x\", \"y\", \"z\"]], \n               workers = 1, \n               use_multiprocessing = True\n            )\n\n            Reduced = pd.DataFrame(Reduced)\n                  \n            p[r, c].scatter(\n                x=Reduced[0],\n                y=Reduced[1],\n                c=component\n                )\n            p[r, c].set_title(\"Autoencoder\", fontsize=20)\n            \n            Autoencoderr2, Autoencodermae, AutoencodermaxErr \\\n            =Comparison(df[[\"x\", \"y\", \"z\"]], Reduced.iloc[:,[0,1]])\nplt.show()","36b663eb":"comDF=[PCAr2, tSNEr2, UMAPr2, TriMapr2, PaCMAPr2, Autoencoderr2]\ncompare(comDF, mNames)","65a1e053":"comDF=[PCAmaxErr, tSNEmaxErr, UMAPmaxErr, TriMapmaxErr, PaCMAPmaxErr, AutoencodermaxErr]\ncompare(comDF, mNames, title=\"Comparison Maximum Error\")","b1738afb":"Data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nData[Data.columns[1:]]  = Data[Data.columns[1:]] \/ 255 -0.5\nData = Data.sample(frac=0.3, random_state=123)","b095a91b":"methods={\n    \"PCA\": PCA(2),\n    \"tSNE\": TSNE(n_components=2, perplexity=25),\n    \"UMAP\": umap.UMAP(n_components=2, n_neighbors=35),\n    \"TriMap\": trimap.TRIMAP(verbose=False, n_inliers=25, n_outliers=50, n_random=10),\n    \"PaCMAP\": pacmap.PaCMAP(n_dims=2, n_neighbors=10) \n}","7d5e8199":"fig, p = plt.subplots(nrows=3, ncols=2, figsize=(20*1.5,30*1.5))\n\nc=0\nr=0\nk=0\n\nfor c in range(2):\n    for r in range(3):\n        if k < 5:\n            transformed=list(methods.values())[k].fit_transform(Data[Data.columns[1:]].values)\n            Reduced = pd.DataFrame(transformed)\n            p[r, c].scatter(\n                x=Reduced[0],\n                y=Reduced[1],\n                c=Data[\"label\"],\n                cmap=\"Paired\"\n                )\n            p[r, c].set_title(list(methods.keys())[k], fontsize=20)\n            \n            globals()[f\"{list(methods.keys())[k]}r2\"], globals()[f\"{list(methods.keys())[k]}mae\"], globals()[f\"{list(methods.keys())[k]}maxErr\"] \\\n            =Comparison(Data[Data.columns[1:]], Reduced.iloc[:,[0,1]])\n            \n            k+=1\n        else:\n            np.random.seed(123)\n            tf.random.set_seed(123)\n            \n            comp=2\n  \n            INPUT = ks.layers.Input(shape=(Data[Data.columns[1:]].shape[1]), name=\"EncoderInput\")\n\n            ENCO = ks.layers.Dense(128, activation='relu')(INPUT)\n            ENCO = ks.layers.Dropout(0.1)(ENCO)\n            ENCO = ks.layers.BatchNormalization()(ENCO)\n            ENCO = ks.layers.Dense(comp, activation='linear', name=\"compresseion\")(ENCO)\n\n            ENC784D = ks.Model(inputs=INPUT, outputs=ENCO)\n\n            DECO = ks.layers.Dropout(0.1)(ENCO)\n            DECO = ks.layers.BatchNormalization()(DECO)\n            DECO = ks.layers.Dense(128, activation='relu')(DECO)\n            DECO = ks.layers.Dropout(0.1)(DECO)\n            DECO = ks.layers.BatchNormalization()(DECO)\n            DECO = ks.layers.Dense(Data[Data.columns[1:]].shape[1], activation='linear')(DECO)\n\n            AE784D = ks.Model(inputs=INPUT, outputs=DECO)\n            \n            optimizer784D = ks.optimizers.Adamax(lr=0.1)\n            \n            AE784D.compile(\n                optimizer = optimizer784D, \n                loss = \"mae\", \n                metrics = [\"mae\"]\n            )\n            \n            AE784D.fit(\n                x=Data[Data.columns[1:]], \n                y=Data[Data.columns[1:]], \n                epochs = 15000, \n                batch_size = 8192, \n                shuffle = False,\n                callbacks=[lrReducer],\n                verbose=0\n            )\n\n            Reduced=ENC784D.predict(\n               x=Data[Data.columns[1:]], \n               workers = 1, \n               use_multiprocessing = True\n            )\n\n            Reduced = pd.DataFrame(Reduced)\n                  \n            p[r, c].scatter(\n                x=Reduced[0],\n                y=Reduced[1],\n                c=Data[\"label\"],\n                cmap=\"Paired\"\n                )\n            p[r, c].set_title(\"Autoencoder\", fontsize=20)\n            Handles, Labels = p[r, c].get_legend_handles_labels()\n            \n            Autoencoderr2, Autoencodermae, AutoencodermaxErr \\\n            =Comparison(Data[Data.columns[1:]], Reduced.iloc[:,[0,1]])\n            \nplt.show()","63297a09":"comDF=[PCAr2, tSNEr2, UMAPr2, TriMapr2, PaCMAPr2, Autoencoderr2]\ncompare(comDF, mNames)","d6707c06":"comDF=[PCAmaxErr, tSNEmaxErr, UMAPmaxErr, TriMapmaxErr, PaCMAPmaxErr, AutoencodermaxErr]\ncompare(comDF, mNames, title=\"Comparison Maximum Error\")","94f0e3f3":"Data = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\nvodka =  {Data.columns[key]: value for key, value in enumerate([0,0,0,0,0,0,0,0.916,6,0,40,8])}\nData = Data.append(vodka, ignore_index=True)\nData.tail(n=2)","c35ebe40":"kf = KFold(n_splits=2,random_state=123, shuffle=True)\nfor train_index, test_index in kf.split(Data):\n    train = Normalization(DF=Data.loc[train_index], cols=Data.drop(columns=[\"quality\"]).columns)\n    test = Normalization(DF=Data.loc[test_index], cols=Data.drop(columns=[\"quality\"]).columns)\n    \ntest.alcohol.max()#So, Vodka is in the test set","6faab549":"parCoOut = go.Figure(data=\n    go.Parcoords(\n        line = dict(color = Data['quality'],\n                   colorscale = 'Tealrose',\n                   showscale = True,\n                   cmin = 3,\n                   cmax = 8),\n        dimensions = list([\n            dict(label = \"fixed acidity\", \n                 values = Data['fixed acidity']),\n            dict(label = 'volat. acidity', \n                 values = Data['volatile acidity']),\n            dict(label = 'citric acid', \n                 values = Data['citric acid']),\n            dict(label = 'res. sugar', \n                 values = Data['residual sugar']),\n            dict(label = 'chlorides', \n                 values = Data['chlorides']),\n            dict(label = 'free sulf. diox.', \n                 values = Data['free sulfur dioxide']),\n            dict(label = 'tot. sulf. diox.', \n                 values = Data['total sulfur dioxide']),\n            dict(label = 'density', \n                 values = Data['density']),\n            dict(constraintrange = [5.9,6],\n                 label = 'pH', \n                 values = Data['pH']),\n            dict(label = 'sulphates', \n                 values = Data['sulphates']),\n            dict(label = 'alcohol', \n                 values = Data['alcohol']),\n            dict(label = 'quality', \n                 values = Data['quality']),\n        ])\n    )\n)\nparCoOut.show()","81e301fc":"pca.fit(train.drop(columns=[\"quality\"]))\ntransformed = pca.transform(test.drop(columns=[\"quality\"]))\nReduced = pd.DataFrame(transformed)","06484339":"scatterPlot(\n    x=0,\n    y=1,\n    df=Reduced,color=test.alcohol.map(lambda x: \"yes\" if x > 10 else \"no\").values,\n    colorName=\"outlier\",\n    main_title=\"PCA Outlier Yes or No\",\n    a=0.8\n    )","38aaa4c7":"UMP.fit(train.drop(columns=[\"quality\"]))\ntransformed = UMP.transform(test.drop(columns=[\"quality\"]))\nReduced = (pd.DataFrame(transformed))","d4e7a081":"scatterPlot(\n    x=0,\n    y=1,\n    df=Reduced,\n    color=test.alcohol.map(lambda x: \"yes\" if x > 10 else \"no\").values,\n    colorName=\"outlier\",\n    main_title=\"UMAP Outlier Yes or No\",\n    a=0.8\n    )","434efb36":"optimizer = ks.optimizers.Adam(lr=leRa, decay=dec)\nAE.compile(optimizer = optimizer, loss = rmse)\n\nhistory = ks.callbacks.History()\n\nAE.fit(\n    x=train.drop(columns=[\"quality\"]), \n    y=train.drop(columns=[\"quality\"]), \n    epochs = eps, \n    batch_size = bs, \n    shuffle = False,\n    callbacks=[history],\n    verbose=0\n    )\n\nReduced = ENC.predict(\n   x=test.drop(columns=[\"quality\"]), \n   workers = 1, \n   use_multiprocessing = True\n)\n\nReduced = pd.DataFrame(Reduced)","90135c68":"scatterPlot(\n    x=0,\n    y=1,\n    df=Reduced,\n    color=test.alcohol.map(lambda x: \"yes\" if x > 10 else \"no\").values,\n    colorName=\"outlier\",\n    main_title=\"Autoencoder Outlier Yes or No\",\n    a=0.8\n    )","1966fc47":"isoForest=ISOF(random_state=0)\nisoForest.fit(train.drop(columns=[\"quality\"]))\ndFunc = isoForest.decision_function(test.drop(columns=[\"quality\"]))","1f9d27b5":"scatterPlot(\n    x=\"density\",\n    y=\"pH\",\n    df=test.loc[:,[\"density\",\"pH\"]],\n    color=[\"yes\" if i < -0.15 else \"No\" for i in dFunc],\n    colorName=\"outlier\",\n    main_title=\"Isolation Forest Outlier Yes or No\",\n    a=0.8\n    )","c0be9f8e":"cluster = DBSCAN(eps=2.5, min_samples=50)\ncluster.fit(train.drop(columns=[\"quality\"]))\nclustLabs = cluster.labels_","e03f8342":"scatterPlot(\n    x=\"density\",\n    y=\"pH\",\n    df=test.loc[:,[\"density\",\"pH\"]],\n    color=[\"yes\" if i ==-1 else \"No\" for i in clustLabs],\n    colorName=\"outlier\",\n    main_title=\"DBSCAN Outlier Yes or No\",\n    a=0.8\n    )","901613eb":"cluster = DBSCAN(eps=2, min_samples=20)\ncluster.fit(Reduced)\nclustLabs = cluster.labels_","b85b91d9":"scatterPlot(\n    x=\"density\",\n    y=\"pH\",\n    df=test.loc[:,[\"density\",\"pH\"]],\n    color=[\"yes\" if i ==-1 else \"No\" for i in clustLabs],\n    colorName=\"outlier\",\n    main_title=\"DBSCAN Outlier Yes or No\",\n    a=0.8\n    )","8e244027":"train = pd.read_csv(\"..\/input\/nab\/artificialNoAnomaly\/artificialNoAnomaly\/art_daily_small_noise.csv\")\ntrain =  Normalization(train, [\"value\"])\ntrain[\"timestamp\"] = train.timestamp.astype(\"datetime64\")\n\ntest1 = pd.read_csv(\"..\/input\/nab\/artificialWithAnomaly\/artificialWithAnomaly\/art_daily_jumpsdown.csv\")\ntest1 =  Normalization(test1, [\"value\"])\ntest1[\"timestamp\"] = test1.timestamp.astype(\"datetime64\")\n\ntest2 = pd.read_csv(\"..\/input\/nab\/artificialWithAnomaly\/artificialWithAnomaly\/art_daily_jumpsup.csv\")\ntest2 =  Normalization(test2, [\"value\"])\ntest2[\"timestamp\"] = test2.timestamp.astype(\"datetime64\")","62fb461a":"def FeatEng(DF):\n    \n    DF=DF.copy()\n    \n    DF[\"weekday\"] = DF.timestamp.dt.weekday\n    DF[\"hour\"] = DF.timestamp.dt.hour\n    DF[\"minute\"] = DF.timestamp.dt.minute\n    DF[\"second\"] = DF.timestamp.dt.second\n    \n    m=DF.value.mean()\n    mx=m+DF.value.std()\n    mi=m-DF.value.std()\n\n    DF[\"mean20\"] = DF.value.rolling(window=20).mean().fillna(m)\n    DF[\"mean600\"] = DF.value.rolling(window=600).mean()#.fillna(m)\n    DF[\"max\"] = DF.value.rolling(window=10).max().fillna(mx)\n    DF[\"min\"] = DF.value.rolling(window=10).min().fillna(mi)\n\n    DF[\"lag1\"] = DF.value.shift(periods=1, fill_value=m)\n    DF[\"lag20\"] = DF.value.shift(periods=20, fill_value=m)\n    DF[\"lag30\"] = DF.value.shift(periods=30, fill_value=m)\n    DF[\"lag100\"] = DF.value.shift(periods=100, fill_value=m)\n    DF[\"lag200\"] = DF.value.shift(periods=200, fill_value=m)\n    DF[\"lag300\"] = DF.value.shift(periods=300, fill_value=m)\n    \n    DF.dropna(inplace=True)\n\n    return DF","48bacf01":"train = FeatEng(train)\ntest1 = FeatEng(test1)\ntest2 = FeatEng(test2)\n\ntime = test1.timestamp","a83b243e":"pltDf = pd.concat([train[[\"timestamp\", \"value\"]], test1[[\"timestamp\", \"value\"]], test2[[\"timestamp\", \"value\"]]]).reset_index(drop=True)\n\nl = train.shape[0]*[\"train\"]\nl.extend(train.shape[0]*[\"test1\"])\nl.extend(train.shape[0]*[\"test2\"])\n\npltDf[\"sample\"] = l\n\nplot = px.line(\n    pltDf,\n    x=\"timestamp\",\n    y=\"value\",\n    color = \"sample\",\n    color_discrete_sequence=px.colors.qualitative.G10,\n    template=\"simple_white\"\n    )\n\nplot.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"grey\", \n    opacity=0.25, \n    line_width=0\n    )\n\nplot","ca8350b9":"pca = PCA(n_components=train.shape[1]-1, \n          random_state=1\n          )\n\npca.fit(train.drop(columns=[\"timestamp\"]))\ntransformed1 = pca.transform(test1.drop(columns=[\"timestamp\"]))\ntransformed2 = pca.transform(test2.drop(columns=[\"timestamp\"]))\nReduced_PCA1 = pd.DataFrame(transformed1)\nReduced_PCA2 = pd.DataFrame(transformed2)","4f8927fa":"barPlot(y=pca.explained_variance_ratio_, threshold=0.9)","d34f4ee0":"scatterPlot(x=0,\n            y=1,\n            df=Reduced_PCA1,\n            color=time.map(lambda x: \"yes\" if (x > pd.to_datetime(\"2014-04-11 08:00\")) & (x <= pd.to_datetime(\"2014-04-11 20:50\")) else \"no\").values,\n            colorName=\"outlier\",\n            main_title=\"Test1: PCA Time Series reduced to 2 Dimernsions\",\n            a=0.8\n            )","397576e0":"scatterPlot(x=0,\n            y=1,\n            df=Reduced_PCA2,\n            color=time.map(lambda x: \"yes\" if (x > pd.to_datetime(\"2014-04-11 08:00\")) & (x <= pd.to_datetime(\"2014-04-11 20:50\")) else \"no\").values,\n            colorName=\"outlier\",\n            main_title=\"Test2: PCA Time Series reduced to 2 Dimernsions\",\n            a=0.8\n            )","cde3458f":"UMP = umap.UMAP(\n    n_neighbors=20, \n    min_dist=0.5,\n    n_components=2,\n    spread=3\n    )\n\nUMP.fit(train.drop(columns=[\"timestamp\"]))\n\ntransformed1 = UMP.transform(test1.drop(columns=[\"timestamp\"]))\ntransformed2 = UMP.transform(test2.drop(columns=[\"timestamp\"]))\nReduced1 = (pd.DataFrame(transformed1))\nReduced2 = (pd.DataFrame(transformed2))","266afe1f":"scatterPlot(x=0,\n            y=1,\n            df=Reduced1,\n            color=time.map(lambda x: \"yes\" if (x > pd.to_datetime(\"2014-04-11 08:00\")) & (x <= pd.to_datetime(\"2014-04-11 20:50\")) else \"no\").values,\n            colorName=\"outlier\",\n            main_title=\"Test1: UMAP Time Series reduced to 2 Dimernsions\",\n            a=0.8\n            )","44d369ee":"scatterPlot(x=0,\n            y=1,\n            df=Reduced2,\n            color=time.map(lambda x: \"yes\" if (x > pd.to_datetime(\"2014-04-11 08:00\")) & (x <= pd.to_datetime(\"2014-04-11 20:50\")) else \"no\").values,\n            colorName=\"outlier\",\n            main_title=\"Test2: UMAP Time Series reduced to 2 Dimernsions\",\n            a=0.8\n            )","c298857d":"isoForest=ISOF(\n    random_state=0, \n    n_estimators=200, \n    max_features=0.5\n    )\n\nisoForest.fit(train.drop(columns=[\"timestamp\"]))\ndFunc1 = isoForest.decision_function(test1.drop(columns=[\"timestamp\"]))\n\nisoForest=ISOF(\n    random_state=0, \n    n_estimators=200, \n    max_features=0.1\n    )\n\nisoForest.fit(train.drop(columns=[\"timestamp\"]))\n\ndFunc2 = isoForest.decision_function(test2.drop(columns=[\"timestamp\"]))","36cfc7ce":"cluster = DBSCAN(eps=3.5, min_samples=25)\ncluster.fit(test1.drop(columns=[\"timestamp\"]))\nclustLabs1 = cluster.labels_\ncluster.fit(test1.drop(columns=[\"timestamp\"]))\nclustLabs2 = cluster.labels_","b82cc0a7":"test1[\"PCA\"] = Reduced_PCA1[0].values\ntest1[\"iso\"] = [\"yes\" if i < -0.115 else \"no\" for i in dFunc1]\ntest1[\"umap\"] = Reduced1[0].values\ntest1[\"DBSCAN\"] = [\"yes\" if i == -1 else \"no\" for i in clustLabs1]\n\ntest2[\"PCA\"] = Reduced_PCA2[0].values\ntest2[\"iso\"] = [\"yes\" if i < -0.08 else \"no\" for i in dFunc2]\ntest2[\"umap\"] = Reduced2[0].values\ntest2[\"DBSCAN\"] = [\"yes\" if i == -1 else \"no\" for i in clustLabs2]","799171b3":"plotP1 = px.scatter(\n    test1,\n    color=\"PCA\",\n    x=\"timestamp\",\n    y=\"value\",\n    title=\"PCA test1\",\n    marginal_y=\"histogram\",\n    template=\"simple_white\"\n    )\n\nplotP1.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"grey\", \n    opacity=0.25, \n    line_width=0\n    )\n\nplotP1","fb62be5f":"plotP2 = px.scatter(\n    test2,\n    color=\"PCA\",\n    x=\"timestamp\",\n    y=\"value\",\n    title=\"PCA test2\",\n    marginal_y=\"histogram\",\n    template=\"simple_white\"\n    )\n\nplotP2.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"grey\", \n    opacity=0.25, \n    line_width=0\n    )\n\nplotP2","e9f211ec":"plotU1 = px.scatter(\n    test1,\n    color=\"umap\",\n    x=\"timestamp\",\n    y=\"value\",\n    title=\"UMAP test1\",\n    marginal_y=\"histogram\",\n    template=\"simple_white\"\n    )\n\nplotU1.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"grey\", \n    opacity=0.25, \n    line_width=0\n    )\n\nplotU1","6bcc80c4":"plotU2 = px.scatter(\n    test2,\n    color=\"umap\",\n    x=\"timestamp\",\n    y=\"value\",\n    title=\"UMAP test2\",\n    marginal_y=\"histogram\",\n    template=\"simple_white\"\n    )\n\nplotU2.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"grey\", \n    opacity=0.25, \n    line_width=0\n    )\n\nplotU2","c79b6d4a":"plotI1 = px.scatter(\n    test1,\n    color=\"iso\",\n    x=\"timestamp\",\n    y=\"value\",\n    title=\"Isolation Forest test 1\",\n    marginal_y=\"histogram\",\n    template=\"simple_white\",\n    color_discrete_sequence=[\"green\", \"red\"]\n    )\n\nplotI1.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"grey\", \n    opacity=0.25, \n    line_width=0\n    )\n\nplotI1","029c6cc7":"plotI2 = px.scatter(\n    test2,\n    color=\"iso\",\n    x=\"timestamp\",\n    y=\"value\",\n    title=\"Isolation Forest test 2\",\n    marginal_y=\"histogram\",\n    template=\"simple_white\",\n    color_discrete_sequence=[\"green\", \"red\"]\n    )\n\nplotI2.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"grey\", \n    opacity=0.25, \n    line_width=0\n    )\n\nplotI2","5c73a837":"plotD1 = px.scatter(\n    test1,\n    color=\"DBSCAN\",\n    x=\"timestamp\",\n    y=\"value\",\n    title=\"DBSCAN test 1\",\n    marginal_y=\"histogram\",\n    template=\"simple_white\",\n    color_discrete_sequence=[\"green\", \"red\"]\n    )\n\nplotD1.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"grey\", \n    opacity=0.25, \n    line_width=0\n    )\n\nplotD1","61000f4e":"plotD2 = px.scatter(\n    test2,\n    color=\"DBSCAN\",\n    x=\"timestamp\",\n    y=\"value\",\n    title=\"DBSCAN test 2\",\n    marginal_y=\"histogram\",\n    template=\"simple_white\",\n    color_discrete_sequence=[\"green\", \"red\"]\n    )\n\nplotD2.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"grey\", \n    opacity=0.25, \n    line_width=0\n    )\n\nplotD2","c96793d5":"train = train.reset_index(drop=True).loc[3:,:]\ntest1 = test1.reset_index(drop=True).loc[3:,:]\ntest2 = test2.reset_index(drop=True).loc[3:,:]\ntrain.shape","058dc5d1":"test1.drop(columns=[\"iso\", \"umap\", \"PCA\", \"DBSCAN\"], inplace=True)\ntest2.drop(columns=[\"iso\", \"umap\", \"PCA\", \"DBSCAN\"], inplace=True)\ndropCols = [\"timestamp\"]\n#dropCols.extend([i for i in train.columns if \"lag\" in i])\n#dropCols.extend([i for i in train.columns if \"mean\" in i])\n#dropCols.extend([\"hour\", \"weekday\", \"minute\", \"second\"])\n\ny = train[\"value\"].values.reshape((-1, 10, 1))\n\ntrain = train.drop(columns=dropCols).to_numpy()\ntrain = train.reshape((-1, 10, train.shape[1]))# samples, timesteps, features\n\ntime = test1.timestamp\ntest1 = test1.drop(columns=dropCols).to_numpy()\ntest1 = test1.reshape((-1, 10, test1.shape[1]))\n\ntest2 = test2.drop(columns=dropCols).to_numpy()\ntest2 = test2.reshape((-1, 10, test2.shape[1]))","b0240487":"np.random.seed(123)\ntf.random.set_seed(12)\n\ncomp=2\n  \nINPUT = ks.layers.Input(shape=(train.shape[1], train.shape[2]), name=\"EncoderInput\")\n\nENCO = ks.layers.LSTM(\n    32, \n    activation='relu', \n    return_sequences = True,\n    stateful=False,\n    dropout=0.2)(INPUT)\n\nENCO = ks.layers.LSTM(\n    comp, \n    activation = \"relu\", \n    return_sequences = True, \n    stateful=False,\n    name = \"compression\")(ENCO)\n\nENC = ks.Model(inputs=INPUT, outputs=ENCO)\n\nDECO = ks.layers.Dense(1)(ENCO)\n\nAE = ks.Model(inputs=INPUT, outputs=DECO)\n","cf4a5c3f":"ks.utils.plot_model(\n    AE, \n    show_shapes=True, \n    show_layer_names=True\n    )","12cbedf6":"lrReducer = ks.callbacks.ReduceLROnPlateau(    \n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=10,\n    verbose=0,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0.000001,\n    )","7123fb74":"leRa=0.01\ndec=0.00000\neps=2500\nprint(\"Learningrate ok: \" + str(leRa - dec * 200 >= 0))\nbs=train.shape[0]*10\n\noptimizer = ks.optimizers.Adamax(lr=leRa, decay=dec, clipvalue=10)\nAE.compile(optimizer = optimizer, loss = 'mae')\n\nhistory = ks.callbacks.History()\n\nAE.fit(\n    x=train, \n    y=y, \n    epochs = eps, \n    batch_size = bs, \n    shuffle = False,\n    callbacks=[history, lrReducer],\n    verbose=0\n    )\n\nprint(AE.evaluate(train, y))\n\nEncoded1 = AE.predict(\n   x=test1, \n   workers = 1, \n   use_multiprocessing = True\n)\n\nEncoded2 = AE.predict(\n   x=test2, \n   workers = 1, \n   use_multiprocessing = True\n)\n\nEncoded1 = pd.DataFrame(np.reshape(Encoded1, (3430, y.shape[2])))\nEncoded2 = pd.DataFrame(np.reshape(Encoded2, (3430, y.shape[2])))\n\nReducedTRAIN = ENC.predict(\n   x=train, \n   workers = 1, \n   use_multiprocessing = True\n)\n\nReducedTRAIN = pd.DataFrame(np.reshape(ReducedTRAIN, (3430, comp)))\n\nReduced1 = ENC.predict(\n   x=test1, \n   workers = 1, \n   use_multiprocessing = True\n)\n\nReduced2 = ENC.predict(\n   x=test2, \n   workers = 1, \n   use_multiprocessing = True\n)\n\nReduced1 = pd.DataFrame(np.reshape(Reduced1, (3430, comp)))\nReduced2 = pd.DataFrame(np.reshape(Reduced2, (3430, comp)))","0767c002":"def reshapeAdd(DF, t, tr, reduced, encoded, reducedtrain):\n    \"\"\"DF: Data Frame, t: time, tr: train, reduced: Reduced#, encoded: Encoded#\"\"\"\n    \n    DF=DF.copy()\n    DF = pd.DataFrame(DF.reshape((-1, tr.shape[2])))\n\n    DF[\"timestamp\"] = t.values\n    DF[\"transformed1\"] = reduced[0].values\n    DF[\"transformed2\"] = reduced[1].values\n    DF[\"trained1\"] = reducedtrain[0].values\n    DF[\"trained2\"] = reducedtrain[1].values\n    DF[\"Encoded\"] = encoded[0]\n    DF.rename(columns={0: \"value\"}, inplace=True)\n    \n    return DF","7b35f097":"test1 = reshapeAdd(test1, time, train, Reduced1, Encoded1, ReducedTRAIN)\ntest2 = reshapeAdd(test2, time, train, Reduced2, Encoded2, ReducedTRAIN)","8eb59cab":"scatterPlot(\n    x=0,\n    y=1,\n    df=Reduced1,\n    color=time.map(lambda x: \"yes\" if (x > pd.to_datetime(\"2014-04-11 08:00\")) & (x <= pd.to_datetime(\"2014-04-11 20:50\")) else \"no\").values,\n    colorName=\"outlier\",\n    main_title=\"Test1: Autoencoder Compressed to 2 Dimensions\",\n    a=0.8\n    )","1ce49fc1":"scatterPlot(\n    x=0,\n    y=1,\n    df=Reduced2,\n    color=time.map(lambda x: \"yes\" if (x > pd.to_datetime(\"2014-04-11 08:00\")) & (x <= pd.to_datetime(\"2014-04-11 20:50\")) else \"no\").values,\n    colorName=\"outlier\",\n    main_title=\"Test2: Autoencoder Compressed to 2 Dimensions\",\n    a=0.8\n    )","b54d71fb":"scatterPlot(\n    x=0,\n    y=1,\n    df=ReducedTRAIN,\n    color=time.map(lambda x: 1 if (x > pd.to_datetime(\"2014-04-11 08:00\")) & (x <= pd.to_datetime(\"2014-04-11 20:50\")) else 0).values,\n    colorName=\"outlier area\",\n    main_title=\"Train: Autoencoder Compressed to 2 Dimensions\",\n    a=0.8\n    )","1edb83f4":"def vizDF(DF, t, num):\n    \n    DF=DF.copy()\n    vDF = DF[[f\"transformed{num}\", \"timestamp\", f\"trained{num}\"]]\n    \n    vDF[\"outlier\"] = t.map(lambda x: \"yes\" if (x > pd.to_datetime(\"2014-04-11 08:00\")) & \n                           (x <= pd.to_datetime(\"2014-04-11 20:50\")) else \"no\").values\n    \n    vDF[\"type\"] = f\"dim{num}\"\n    vDF.rename(columns={f\"transformed{num}\":\"transformed\", f\"trained{num}\": \"trained\"},inplace = True)\n    \n    return vDF\n\nvDF1 = pd.concat([vizDF(test1, time, 1), vizDF(test1, time, 2)])\nvDF1[\"testNo\"] = \"test1\"\nvDF2 = pd.concat([vizDF(test2, time, 1), vizDF(test2, time, 2)])\nvDF2[\"testNo\"] = \"test2\"\nvDF = pd.concat([vDF1, vDF2])\nvDF.reset_index(drop=True, inplace=True)\n\nfig, p = plt.subplots(nrows=2, ncols=2, figsize=(20,20))\n\nbin_array=np.array([[[10, 10, 10],[10, 10, 10]],[[10, 10, 10],[10, 10, 10]]])\n\nfor c in range(2):\n    for r in range(2):\n        \n        p[r, c].hist(\n            vDF.transformed.loc[(vDF.type == f\"dim{c+1}\") & (vDF.outlier == \"no\") & (vDF.testNo == f\"test{r+1}\")], \n            bins = bin_array[r,c,0], \n            density=True, \n            histtype='bar', \n            color=\"green\", \n            label='no outlier', \n            edgeColor = 'white',\n            alpha=0.9\n            )\n\n        p[r, c].hist(\n            vDF.transformed.loc[(vDF.type == f\"dim{c+1}\") & (vDF.outlier == \"yes\") & (vDF.testNo == f\"test{r+1}\")], \n            bins = bin_array[r,c,1], \n            density=True, \n            histtype='bar', \n            color=\"red\", \n            label='outlier', \n            alpha=0.6\n            )\n\n        p[r, c].hist(\n            vDF.trained.loc[(vDF.type == f\"dim{c+1}\") & (vDF.testNo == f\"test{r+1}\")], \n            bins = bin_array[r,c,2], \n            density=True, \n            histtype='step', \n            color=\"black\", \n            linestyle='dashed',\n            rwidth=3,\n            label='trained', \n            alpha=1\n            )\n\n        p[r, c].legend(loc='upper right')\n        p[r, c].set_xlabel(f\"Dimension {c}\")\n\n        if c == 0:\n            p[r, c].set_ylabel('frequency')\n\n        if r == 0:\n            p[r, c].set_title('Histogram')\n\nplt.show()","ed8c7229":"k = kde(\n    bandwidth=0.75, \n    kernel='gaussian'\n    )\n\nfor i in range(1,3):\n    for j in range(1,3):\n        k.fit(globals()[f\"test{i}\"][f\"trained{j}\"].values.reshape(-1, 1))   \n        globals()[f\"test{i}\"][\"probaDim{}\".format(j)] = np.exp(k.score_samples(globals()[f\"test{i}\"][f\"transformed{j}\"].values.reshape(-1, 1)))","684de50d":"test1[\"proba\"] = 1-(test1.probaDim1 * 0.5 + test1.probaDim2 * 0.5)\ntest2[\"proba\"] = 1-(test2.probaDim1 * 0.5 + test2.probaDim2 * 0.5)","4ac4c371":"fig = make_subplots(rows=2, cols=1)\n\nfig.add_trace(\n    go.Scatter(\n        x=test1.timestamp, \n        y=test1.Encoded, \n        mode='markers',\n        showlegend=False,\n        hovertemplate =\n        '<br>%{x}'+\n        '<br><b>Encoded<\/b>: %{y}'+\n        '<br><b>Probability<\/b>:%{text}',\n        text = ['{}'.format(i) for i in test1.proba],\n        name='test1',\n         marker=dict(\n            size=6,\n            color=test1.proba, \n            colorscale='Tealrose', \n            showscale=True\n            )\n        ),\n    row=1, col=1\n)\n\nfig.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"#DCDCDC\", \n    opacity=0.25, \n    line_width=0\n    )\n\nfig.add_trace(\n    go.Scatter(\n        x=test2.timestamp, \n        y=test2.Encoded, \n        mode='markers',\n        showlegend=False,\n        hovertemplate =\n        '<br>%{x}'+\n        '<br><b>Encoded<\/b>: %{y}'+\n        '<br><b>Probability<\/b>:%{text}',\n        text = ['{}'.format(i) for i in test2.proba],\n        name='test2',\n        marker=dict(\n            size=6,\n            color=test2.proba,\n            colorscale='Tealrose'\n            )\n        ),\n    row=2, col=1\n)\n\nfig.add_vrect(\n    x0=\"2014-04-11 08:00\", \n    x1=\"2014-04-11 20:50\", \n    annotation_text=\"outlier area\", \n    annotation_position=\"top left\",\n    fillcolor=\"#DCDCDC\", \n    opacity=0.25, \n    line_width=0\n    )\n\nfig.update_layout(height=800, width=800, title_text=\"LSTM Outlier Detection\", template=\"simple_white\")\nfig.show()","98d3bfbe":"So this is the structure of the autoencoder.","4a512880":"<font size=\"+2\" color=\"grey\"><b>1.1 PCA (Principal Component Analysis) <\/b><\/font><br><a id=\"1.1\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nThis method reframes the data into uncorrelated principal components. These components are linear representations of the original data. The total variance (What is equated here with information content) of all components is exactly equal to the total variance of the entire data set. The first component contains the most variance. With each component, the variance decreases, so that a large part of the total variance is in the first components. We will find this out later in this story.","cc459b97":"We put the vodka in the testset.","4e18f6aa":"<font size=\"+2\" color=\"grey\"><b>1.7 Comparison <\/b><\/font><br><a id=\"1.7\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nNow comes the comparison, where we simply try to explain each variable in the data set with the two dimensions. The distribution of the RSquares is shown. \n\n> Note that the coefficient of determination R squared measures linear correlations, but between the target (here, the respective feature of the data set) and the model output. Even though the relationships between the reduced dimensions of the methods and the original data set need not be linear.","ccea35af":"Based on the R-Square t-SNE, PaCMAP and TriMap take the most information in two dimensions. This is closely followed by UMAP and Autoencoders. PCA brings the lowest explanatory power. Probably because the observed correlations are not linear.\n\nBased on the maximum error, PaCMAP and TriMap show the most robust results followed by Autoencoders and UMAP. t-SNE and PCA bring up the rear.","e0a494b3":"An interesting question would be which method provides the most information in the two dimensions. One could take the two dimensions and try to explain the original data set, dimension by dimension, with them. The better this works, the more information should be contained. But be careful, the correlations between the reduced dimensions and the original data set are not necessarily linear. Thus we have to take a method that takes this non-linearity into account. Here I chose a Gradient Boosting Regressor with default parameters, because it adapts very well also non-linear correlations. To assess how good the explanatory power is, the R-square score, and the maximum error are used.","4f5ab4f2":"<font size=\"+3\" color=\"grey\"><b>1. Cross Section Data <\/b><\/font><br><a id=\"1\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nFor the first part, which deals with simple outliers, we take the data set [Red Wine Quality](https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009) from UCI Machine Learning.\n\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/4458\/6836\/30587db9a40233164f65a4a3f148f40d\/dataset-cover.jpg?t=2017-11-12-14-28-34)","53ad787c":"<font size=\"+2\" color=\"grey\"><b>1.2 t-SNE (t-Distributed Stochastic Neighbor Embedding) <\/b><\/font><br><a id=\"1.2\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nIn this method, a gradient descent is used to project high-dimensional clusters onto low-dimensional space and, by shifting the individual observations, to recover the relative distance between two points that were close to each other in high-dimensional space. This, of course, means that the method tends to focus on local structures. An important difference to PCA is that the resulting components are not necessarily linear to the original data. Most important parameters are:\n\n* perplexity (controls focus on local or global structure)\n* no. of components","4541c8c2":"UMAP tends to pull the outlier in.","a262823b":"<font size=\"+2\" color=\"grey\"><b>4.5 Isolation Forest <\/b><\/font><br><a id=\"4.5\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nWhat? Why not t-SNE? And what is an isolation forest?\n\nAs explained above, t-SNE and TriMap do not create a learned hypothesis, which means that it cannot be transferred to unseen data. There are actually approaches that try to solve the problem. E.g. the hypothesis is simply learned in a supervised manner or each unseen observation point is added to the learning data set one by one to know where it belongs. However, all these approaches are fraught with not insignificant problems. Therefore I decided to bring the so called isolation forest into the game.\n\nThe isolation forest separates all observations of a dataset - from each other. Thereby it is assumed that the observations, which are particularly easy to isolate, could be outliers. The shorter the path from the root to the leaf, the more likely an outlier. Thus, a cutoff criterion is needed here to determine what is an outlier.","2e315791":"<font size=\"+2\" color=\"grey\"><b>5.4 DBSCAN <\/b><\/font><br><a id=\"5.4\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>","9308e06e":"It is important in feature engineering not to include transient data such as time or year. These do not repeat when they have expired.","75aec8a8":"<font size=\"+2\" color=\"grey\"><b>5.5 Time Series Results <\/b><\/font><br><a id=\"5.5\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>","025baab2":"Unfortunately, if we reduce the dimensions to two, it doesn't work so well either.","f50a2185":"<font size=\"+3\" color=\"grey\"><b>3. The MNIST Dataset <\/b><\/font><br><a id=\"3\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\n*MNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.*\n(extract from the Digit Recognizer competition on kaggle)\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3004\/logos\/header.png?t=2018-11-14-20-12-43)","459eb229":"<font size=\"+2\" color=\"grey\"><b>4. Outlier Detection <\/b><\/font><br><a id=\"4\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nLets go back to the wine dataset again to deal with outliers. We'll start simple for now, with vodka. Since there are only wines in the data set, vodka should be easy to find.","09644648":"First, we look at whether there are differences between the outliers and regular patterns in the reduced two-dimensional space. \nFirst for test1:","8a90943f":"You can see that already with 7 components more than 90% of the variance is reached.","a426b2a1":"<font size=\"+3\" color=\"grey\"><b>5. Time Series Outliers <\/b><\/font><br><a id=\"5\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nTime series are a special case, as they are often non-stationary and often only a time series is available without further explanatory variables. I decided here to use a rather difficult dataset, the [Numenta Anomaly Benchmark](https:\/\/www.kaggle.com\/boltzmannbrain\/nab) dataset provided by [boltzmannbrain](https:\/\/www.kaggle.com\/boltzmannbrain).\n\n![](https:\/\/numenta.com\/wp-content\/uploads\/2019\/03\/heartbeat@2x-1024x341.png)","f9bbb636":"Hello, I am Frank and I would like to tell you a little story about outliers, dimensions and methods. \n\nIn the following elaboration I will explain and apply different methods. I always explain intuitively, that means I don't use mathematics - although a little understanding will be necessary. \n\nFirst, I have selected a dataset about wine ([Red Wine Quality](https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009)), the [ModelNet40 - Princeton 3D Object Dataset](https:\/\/www.kaggle.com\/balraj98\/modelnet40-princeton-3d-object-dataset\/metadata) and the [MNIST](https:\/\/www.kaggle.com\/c\/digit-recognizer) Dataset, which I will use to explain the methods PCA, UMAP, t-SNE, TriMap, PaCMAP Isolation Forests, Autoencoders and DBCSAN. It is about dimension reductions, plotting nice graphics and outlier detection. \n\nThe second part is about outliers in time series. Here i have used the [Numenta Anomaly Benchmark (NAB)](https:\/\/www.kaggle.com\/boltzmannbrain\/nab) dataset. You will see that this is quite different from working with cross-sectional data. The methods mentioned above will be used (if possible). \n\nFor one or the other it may be clear from the beginning that some things done here do not work well. But why do I do it anyway? Because it is fun to see how the methods behave in different situations \ud83e\udd17\n\nI hope you enjoy reading!","2b7da09a":"How much variance is contained in these two dimensions. Well, we could just look at the barplots and see the result. However, I promised to come back to the calculation of the variance. The variance of the original data set is, as we know, exactly the variance of all components. Thus, we only need to sum up.","f2584ce7":"One needs just two components co capture more than 90% of the variance of the data. No surprise, it originally consists of only one time series.","024a9885":"\nFirst we have to reshape the dataset into the structure [samples, timesteps, features]","b4e4b65a":"<font size=\"+2\" color=\"grey\"><b>1.3 UMAP (Uniform Manifold Approximation and Projection) <\/b><\/font><br><a id=\"1.3\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nThis method works basically similar to t-SNE. Here, too, the clusters of the high-dimensional space are tried to be restored in the low-dimensional space. However, a graph is constructed, which is to be regarded as a learned hypothesis. This leads to the fact that this hypothesis can be applied also to unseen observations. Overall, UMAP is faster than t-SNE and minimally better at reconstructing global structures. Most important parameters are:\n\n* no. of neighbors (controls focus on local or global structure)\n* minimum distance\n* no. of components","21ce8ea2":"As can be seen, the green non-outliers and the black line (trained) correspond with each other. Whereby the outliers (red) are found in other places. \n\nThe distribution density of the learned hypothesis is close to zero for the outliers. Therefore, we use a kernel density estimator that first gives the probabilities of an observation occurring based on the learned hypothesis. It should be noted that the probability for a given observation is zero. However, if we define that each observation is x = x + e, where e is infinitesimally small, we can consider the value of the kernel density estimator at x as the probability of x occurring. \n\nThus, we are also able to determine the probability of whether or not the observation is an outlier.\n\nTo be honest, however, the method could also have been applied to the original time series. However, this procedure should show that the result of the LSTM is quite suitable for outlier detection in time series. ","406eda2a":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"><p style=\"font-size : 30px\"><font color=\"darkgrey\">Content<font\/><\/p><\/h3>\n\n1. [<font color=\"darkgrey\">Cross Section Data<font\/>](#1)   \n    - 1.1 [<font color=\"darkgrey\">PCA (Principal Component Analysis)<font\/>](#1.1)\n    - 1.2 [<font color=\"darkgrey\"> t-SNE (t-Distributed Stochastic Neighbor Embedding)<font\/>](#1.2)\n    - 1.3 [<font color=\"darkgrey\"> UMAP (Uniform Manifold Approximation and Projection)<font\/>](#1.3)\n    - 1.4 [<font color=\"darkgrey\"> TriMap (Triplets Manifold Approximation and Projection)<font\/>](#1.4)\n    - 1.5 [<font color=\"darkgrey\"> PaCMAP (Pairwise Controlled Manifold Approximation Projection)<font\/>](#1.5)\n    - 1.6 [<font color=\"darkgrey\"> Autoencoder<font\/>](#1.6)\n    - 1.7 [<font color=\"darkgrey\"> Comparison<font\/>](#1.7)\n2. [<font color=\"darkgrey\">A somewhat Simpler Dataset<font\/>](#2)\n    - 2.1 [<font color=\"darkgrey\">Reduction<font\/>](#2.1)\n    - 2.2 [<font color=\"darkgrey\"> Comparison<font\/>](#2.2)\n3. [<font color=\"darkgrey\">The MNIST Dataset<font\/>](#3)\n    - 3.1 [<font color=\"darkgrey\">Reduction<font\/>](#3.1)\n    - 3.2 [<font color=\"darkgrey\"> Comparison<font\/>](#3.2)    \n4. [<font color=\"darkgrey\">Outlier Detection<font\/>](#4)\n    - 4.1 [<font color=\"darkgrey\">Visual Analysis<font\/>](#4.1)\n    - 4.2 [<font color=\"darkgrey\"> PCA<font\/>](#4.2)\n    - 4.3 [<font color=\"darkgrey\"> UMAP<font\/>](#4.3)\n    - 4.4 [<font color=\"darkgrey\"> Autoencoder<font\/>](#4.4)\n    - 4.5 [<font color=\"darkgrey\"> Isolation Forest<font\/>](#4.5)\n    - 4.6 [<font color=\"darkgrey\"> DBSCAN (Density Based Spatial Clustering of Applications with Noise)<font\/>](#4.6)\n5. [<font color=\"darkgrey\">Time Series Outliers<font\/>](#5)\n    - 5.1 [<font color=\"darkgrey\">Visual Analysis<font\/>](#5.1)\n    - 5.2 [<font color=\"darkgrey\"> PCA<font\/>](#5.2)\n    - 5.3 [<font color=\"darkgrey\"> UMAP<font\/>](#5.3)\n    - 5.4 [<font color=\"darkgrey\">Isolation Forest<font\/>](#5.4)\n    - 5.5 [<font color=\"darkgrey\"> DBSCAN<font\/>](#5.5)\n    - 5.6 [<font color=\"darkgrey\"> Autoencoder LSTM<font\/>](#5.6)\n    - 5.7 [<font color=\"darkgrey\"> KDE (Kernel Density Estimator)<font\/>](#5.7)\n6. [<font color=\"darkgrey\">Conclusion<font\/>](#6)","35f9df95":"You can see that the individual digits have been adequately separated from the methods. PaCMAP in particular, but also UMAP, provide a balanced performance.","5b23a81e":"The comparison by explanatory power also shows that PCA provides good results but PaCMAP and t-SNE are more stable. based on the maximum exxor, Autoencoders and t-SNE lead the ranking.","9cef59ab":"<font size=\"+2\" color=\"grey\"><b>1.6 Autoencoder <\/b><\/font><br><a id=\"1.6\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nAn autoencoder is a neural network that takes as input a data set and usually predicts it as a target. The autoencoder thus learns its own input. However, the input is compressed and reduced to a low-dimensional space (bottleneck). This is done by the encoder. Afterwards, the original data set is restored from the low-dimensional vectors via the decoder. The nice thing about this is that you can use metrics to check the quality of the autoencoder. If the autoencoder does a very good job of recreating the original dataset, much of the total information contained in the dataset should exist in the bottleneck.","013a076b":"So what does the reconstructed time series look like? \n\nIn the following we plot the two time series replicated by the autoencoder test1 and test2 - also to show that the LSTM autoencoder worked very well. In addition, we color each observation with the probability whether it is an outlier. ","7f76e17a":"Why does PCA look so perfect now? Well, the answer lies in the method. PCA lays a linear hyperplane through the multidimensional space and then sorts the residuals by variance in descending order. Thus, all we have is a linear projection on from 3 to 2 dimensions. Is this good? In this case, yes, because a 3D plane is composed of linear polygons. PCA is the only method that necessarily projects linearly. All other methods are not necessarily linear. The other methods do not have this additional information in this case. Therefore PCA wins here. ","2b4c821b":"So this is the structure of the autoencoder.","ff98c648":"<font size=\"+2\" color=\"grey\"><b>2.1 Reduction <\/b><\/font><br><a id=\"2.1\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nNow we convert the 3D into a 2D structure.","9d98000f":"The better the wine the higher the alcohol percentage \ud83c\udf77\n\nThe following methods are either based on the optimization of variances (PCA) or use distances (PCA, t-SNE, UMAP, TriMap, PaCMAP, DBSCAN) or they are neural networks that use weights. All of them have in common that the data should be normalized before. We will not go into the different methods of normalization for the time being. \n\nWhy should distance-based methods be normalized? Well, suppose the scale of one variable (a) goes from 0 to 1 and that of another (b) goes from - 1000 to 1000. If we were to compute distance en between the observations of a and b, this would of course be determined almost exclusively by b. The same applies to the calculation of variances.\n\nNeural networks, on the other hand, simply have difficulties with backpropagation when the scales are very different. If the inputs are of different scales, the weights of some inputs will be updated much faster than other ones. ","dc1776c3":"I have found that large initial learning rates and large batch sizes lead to significantly better results.","435cdd57":"<font size=\"+2\" color=\"grey\"><b>1.4 TriMap (Triplets Manifold Approximation and Projection) <\/b><\/font><br><a id=\"1.4\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nThe algorithm attempts to compute an embedding that preserves the order of the\ndistances within a subset of triplets (set of three observations). This subset usually consists of a k-nearest neighbor and another\nobservation with respect to a focused observation, and random triplets\nwhich consist of two nonneighbors. This method focuses on the area of local and global structure. Most important parameters are:\n\n* n_inliers (Number of nearest neighbors for forming the nearest neighbor triplets)\n* n_outliers (Number of outliers for forming the nearest neighbor triplets)\n* n_random (Number of random triplets per point)","d6d73bd5":"<font size=\"+2\" color=\"grey\"><b>5.2 UMAP <\/b><\/font><br><a id=\"5.2\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>","dc826caa":" <font size=\"+2\" color=\"grey\"><b>4.1 Visual Analysis <\/b><\/font><br><a id=\"4.1\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nFirst, let's take a visual look. The vodka is marked in red here. By the way, the marking can be removed by double-clicking on the corresponding pH axis.","da25649c":"I think that worked, right?","dcb1d5f6":"<font size=\"+2\" color=\"grey\"><b>2.2 Comparison <\/b><\/font><br><a id=\"2.2\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>","f5260057":"<font size=\"+3\" color=\"grey\"><b>2. A somewhat Simpler Dataset <\/b><\/font><br><a id=\"2\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nThe [ModelNet40 - Princeton 3D Object Dataset](https:\/\/www.kaggle.com\/balraj98\/modelnet40-princeton-3d-object-dataset) dataset is from [Balraj Ashwath](https:\/\/www.kaggle.com\/balraj98) and shows 3D representations of objects. I have chosen here a modern airplane. \n\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/943894\/1599485\/e05e0b42b5ec14d76119be1c71cf7a66\/data-original.png?t=2020-10-28-11-36-59)","daf0a0eb":"<font size=\"+2\" color=\"grey\"><b>5.3 Isolation Forest <\/b><\/font><br><a id=\"5.3\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nIn the isolation forest we train two different variants with different randomized feature selections.","cff0d1c2":"For the isolation forest it was very easy to find the cutoff. This is done by simply coloring the time series as a scatterplot with all possible path depths (given by the decision function). For DBSCAN it was also easy. But here we had to experiment a little with the sphere size and the number of neighbors.\n\nFor UMAP and PCA, no meaningful cutoff has been found by which outliers can be separated from the regular observations. Therefore, so that you can get an idea, I have colored the time series here with the value of the reduced dimension. ","bfbfc868":"This special autoencoder uses all features, but learns only the time series, using two dimensions as encoder. It is therefore not a classic autoencoder. However, this variant works better for the reconstruction of the time series.","a099fa27":"<font size=\"+2\" color=\"grey\"><b>5.1 PCA <\/b><\/font><br><a id=\"5.1\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>","5f3fbb82":"Lets train the autoencoder.","935293c3":"Let's take a look at the variance of the components individually as well as cumulatively.","c27f5338":"First, let's take a look at the training dataset and the two test datasets","d54ae567":"I wonder what low-dimensional space would look like if we just used the untrained encoder. Let's just give it a try.","8e932eed":"It is clearly visible that the outliers exist in certain areas. So let's see if observations also exist in these areas for the learned two dimensions. Because if not, then we could exploit this circumstance....","5bc7ea02":"<font size=\"+2\" color=\"grey\"><b>4.2 PCA <\/b><\/font><br><a id=\"4.2\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>","9237075e":"<font size=\"+2\" color=\"grey\"><b>5.7 KDE (Kernel Density Estimator) <\/b><\/font><br><a id=\"5.7\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>","d4a30ac1":"Each color is a number:","b46df649":"Let's see if the original image has changed. Note, we have set a seed.","b5e02242":"<font size=\"+2\" color=\"grey\"><b>4.4 Autoencoder <\/b><\/font><br><a id=\"4.4\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>","f3a5e24c":"<font size=\"+2\" color=\"grey\"><b>3.1 Reduction <\/b><\/font><br><a id=\"3.1\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nNow we convert the 784D into a 2D structure.","4ea872c0":"<font size=\"+2\" color=\"grey\"><b>1.5 PaCMAP (Pairwise Controlled Manifold Approximation Projection) <\/b><\/font><br><a id=\"1.5\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nThe Pairwise Controlled Manifold Approximation Projection (PaCMAP) preserves both local and global structure. To achieve this, the method uses a well defined foss function as well as a special graph optimization technique. Most important parameters:\n\n* n_neighbors (number of neighbors considered in the k-Nearest Neighbor graph)\n* MN_ratio (the ratio of the number of mid-near pairs to the number of neighbors)\n* FP_ratio (the ratio of the number of further pairs to the number of neighbors)","287febd8":"<font size=\"+3\" color=\"grey\"><b>6. Conclusion <\/b><\/font><br><a id=\"6\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nIt has been shown that initially **t-SNE** and TriMap carried the most information in the wine dataset. **UMAP** showed the most robust results, as R squared, the mean absolute error but also the maximum error were well balanced. **PaCMAP** and Autoencoder also did well, with **PCA** taking the least information within two dimensions. \n\nWhen projecting the aircraft, it becomes clear which methods represent more local or more global structures. **PCA** and **Autoencoders** show a strong focus on the global structure. **UMAP** and **t-SNE**, on the other hand, show a focus on locae structures. **TriMap** and **PaCMAP** show that the methods try to represent both local and global structures. In many cases, **PCA** is no longer even mentioned when comparing dimension reduction methods. Here, however, **PCA** has impressively proven that it can sometimes even outperform the other methods. \n\n**PCA** showed a very robust result for the Vodka otlier. For time series it was not able to deliver a convincing result.\n\nAs method for outlier analsye, **UMAP** was found to be unsuitable for both the cross section and time series datasets. \n\n**DBSCAN**, as the only clustering algorithm used here, showed no usable results in the cross section data (wine dataset). For the time series, on the other hand, the results were very satisfactory.\n\nThe **Isolation Forest** produced very convincing results for both the cross section (wine dataset) and the time series. However, one has to determine the cut-off criterion that fits.\n\n**Autoencoders** were also able to separate outliers easily in the cross section. For the time series, the special LSTM autoencoder showed that the replication of the time series was straightforward. The two-dimensional bottleneck separated the outliers. However, we had to apply kernel density estimation again to separate the outliers from the other observations. \n\nto be continued...","44ec2b36":"Then for test2:","61be9d29":"<font size=\"+2\" color=\"grey\"><b>4.6 DBSCAN (Density Based Spatial Clustering of Applications with Noise) <\/b><\/font><br><a id=\"4.6\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nThis clustering algorithm places a sphere around the observations and determines which of the observations overlap. When enough observations have been concatenated in this way, they are defined into a cluster. Observations that do not overlap are not included. Therefore, this algorithm is suitable for finding outliers. Important patrameters are therefore:\n\n* the size of the spheres \n* the minimum number of neighboring observations for clustering","af81d930":"<font size=\"+2\" color=\"grey\"><b>5.6 Autoencoder LSTM <\/b><\/font><br><a id=\"5.6\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>\n\nOne idea is of course to use an autoencoder for time series. LSTM is a natural choice for this. \n\nA LSTM is a neural net, which sequentially loops after each observation. This means that the observations are equipped with a forgetting structure, similar to exponential smoothing. First, an observation is passed through the neural net. From the second observation on, the weighting of the first observation is still there. Therefore, it is first decided which part of the so-called cell state should be forgotten. Then it is seen which part of the new observation should be added to the cell state before the process is repeated. One can say that the cell state is the long term memory and the hidden state is the short term memory, as in simple RNNs.","b3c1e7e1":"...In fact, at the points where the outliers appear, no observations exist in the learned hypothesis. That is, we can simply compare the learned hypothesis with the test data (test1 and test2). How do we do this? Let's take a look at the distributions of the individual dimensions. We compare the distributions of the learned hypothesis with the test1 and test2 data. ","5945b0ce":"<font size=\"+2\" color=\"grey\"><b>3.2 Comparison <\/b><\/font><br><a id=\"3.2\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>","680c0ab3":"To get a feeling for the data set, let's take a look at it first.","0222018c":"<font size=\"+2\" color=\"grey\"><b>4.3 UMAP <\/b><\/font><br><a id=\"4.3\"><\/a>\n<a href=\"#top\" class=\"btn-xs btn-danger\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go back to the TOP<\/a>"}}