{"cell_type":{"3e8dc126":"code","8689fdae":"code","5e6bf669":"code","640c1fa4":"code","a3272835":"code","991d8495":"code","69089190":"code","66980836":"code","c62ffff8":"code","1b708314":"code","97610485":"code","1aaac458":"code","44c3f587":"code","60df89e1":"code","15b8f54a":"code","2ec9d0d0":"code","64c3d8ae":"code","dfa542c0":"code","0c9fe045":"code","2e50d11c":"code","8c10db94":"markdown","e6bf1eef":"markdown","7520fdd2":"markdown","1236dcd5":"markdown","2c12a23f":"markdown","465af2f1":"markdown","5262f3da":"markdown","b4b6ad60":"markdown","f9cd000e":"markdown","cf0f4ee0":"markdown","09eb2779":"markdown","c45940c2":"markdown","0faf1e4c":"markdown","5feafbf6":"markdown","dc5435bf":"markdown","c8333e71":"markdown"},"source":{"3e8dc126":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8689fdae":"import nltk\n \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score,cross_validate\nfrom sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\nfrom nltk.tokenize import word_tokenize\n ","5e6bf669":"data=pd.read_excel('..\/input\/trke-twitter-duygu-analizi\/TrkceTwit.xlsx')  \ndata.head()","640c1fa4":"data['Duygu'].value_counts()","a3272835":"with open(\"..\/input\/trke-stopword-kelimeler\/StopWordTurkce.txt\", \"r\") as dosya:\n    stop = dosya.read() ","991d8495":"def text_preprocess(text):\n    \n    text = [word for word in text.split() if word not in stop]\n    return \" \".join(text)","69089190":"data['stop'] = data['Tweets'].apply(text_preprocess)\ndata.head(10)","66980836":"from TurkishStemmer import TurkishStemmer\n\nkokbul = TurkishStemmer()","c62ffff8":"def stemming(text):\n    words=word_tokenize(text)\n    stems=[]\n\n    for w in words:\n        stems.append(kokbul.stem(w))\n        \n    return ' '.join(stems)\n","1b708314":"data['stemm'] = data['stop'].apply(stemming)\ndata.head()","97610485":"x = data['stemm']\ny = data['Duygu']","1aaac458":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state = 42)","44c3f587":"from sklearn.feature_extraction.text import TfidfVectorizer","60df89e1":"vectorizer = TfidfVectorizer()\n\nvectorizer.fit(x_train)\ntraining_features = vectorizer.transform(x_train)    \ntest_features = vectorizer.transform(x_test)","15b8f54a":"from sklearn.svm import SVC\n# Destek Vekt\u00f6r Makineleri (Support Vector Machine) algoritmas\u0131n\u0131n bir\u00e7ok hiper parametresi vard\u0131r.\n# bunu ara\u015ft\u0131rmac\u0131 manuel olarak belirledi\u011fi gibi parametreleri liste halinde sunarak ''GridSearchCV\" arama \u0131zgaras\u0131nda\n#e\u011fitim verisi do\u011fruluk skoruna g\u00f6re tespit edebilir.\nparametreler = {'C': [1, 10, 100, 1000], 'kernel': ['linear','rbf'],\n              'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n}\n# hiper parametrelerin  do\u011fruluk skorlar\u0131n\u0131 olu\u015ftur\nacc_scorer = make_scorer(accuracy_score)\n# her parametrenin do\u011frulluk skorunu tek tek hesaplayan \"GridSearchCV\" \u0131zgaras\u0131n\u0131 olu\u015ftur. \n#bunun i\u00e7in egitim verisine 10 kez \u00e7apraz ge\u00e7erlilik s\u0131namas\u0131 yap\ngrid_obj = GridSearchCV(SVC(), parametreler, cv=10, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(training_features, y_train)\n# bulunan en iyi parametre k\u00fcmesini modele ata ve modeli olu\u015ftur.\nmodel = grid_obj.best_estimator_\n","2ec9d0d0":"accuracy = grid_obj.best_score_    # En iyi modelin do\u011frulu\u011fu hesaplan\u0131r.\naccuracy","64c3d8ae":"model.fit(training_features, y_train) # a\u015fa\u011f\u0131da, olu\u015fturdu\u011fumuz modelin hiper parametreleri g\u00f6sterilmektedir","dfa542c0":"y_pred = model.predict(test_features)","0c9fe045":"print(\"Accuracy_score on the dataset:{:.2f}\".format(accuracy_score(y_test, y_pred)))","2e50d11c":"from sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False\n            )\nplt.xlabel('true label')\nplt.ylabel('predicted label'","8c10db94":"bu ad\u0131mda ise verimizi test ve e\u011fitim seti olarak ikiye ay\u0131rac\u011f\u0131z.\n","e6bf1eef":"en iyi parametreleri bulunan modelin do\u011fruluk oran\u0131 bulundu. art\u0131k e\u011fitim verisini bu modele g\u00f6re i\u015fleyebilriz \n","7520fdd2":"g\u00f6r\u00fcld\u00fc\u011f\u00fc gibi ya\u011fmasa da g\u00fcrledi. En az\u0131ndan bu alanda ne kadar \u00f6nemli bir bo\u015flu\u011fun oldu\u011funu g\u00f6rm\u00fc\u015f olduk.\nArt\u0131k ham veriyi belirli bir \u00f6n i\u015flemeden ge\u00e7irerek analize haz\u0131r hale getirdik, \u015fimdi verimizi ba\u011f\u0131ml\u0131 ve ba\u011f\u0131ms\u0131z de\u011fi\u015fkenlere ay\u0131rarak s\u00fcreci ba\u015flatal\u0131m\n","1236dcd5":"g\u00f6r\u00fcld\u00fc\u011f\u00fc gibi modelimizin test verilerini tahmin g\u00fc\u00e7\u00fc \u00e7ok istinilen seviye de olmasada idare eder. bu oran\u0131 b\u00fcy\u00fctmek i\u00e7in bir\u00e7ok d\u00fczenleme yap\u0131labalinir. \u00f6rne\u011fin veri say\u0131s\u0131n\u0131 artt\u0131rmak bunlar\u0131n en \u00f6nemlisidir.\n\n\u015fimdi modelimizin atad\u0131\u011f\u0131 kategoriyle test verisinin (ger\u00e7ek) kategorileri arsas\u0131ndaki do\u011fru ve yanl\u0131\u015f e\u015fle\u015ftirmeleri ortaya koyacak olan kar\u015f\u0131la\u015ft\u0131rma matrisini inceleyelim.\n","2c12a23f":"Art\u0131k gerek test gerekse da e\u011fitim setindeki ba\u011f\u0131ms\u0131z de\u011fi\u015fken olan 'x'in i\u00e7indeki metinleri say\u0131salla\u015ft\u0131rabiliriz ","465af2f1":"Burada kelimeleri algoritman\u0131n anlayaca\u011f\u0131 dile \u00e7evirmek yani asy\u0131salla\u015ft\u0131rmak i\u00e7in en \u00e7ok kullan\u0131lan y\u00f6ntemlerden biri olan TF-IDF (Term Frequency - Inverse Document Frequency ) terim a\u011f\u0131rl\u0131kland\u0131rma y\u00f6ntemini kullanaca\u011f\u0131z. TF-TDF, verideki bir kelimenin frekans\u0131n\u0131, o kelimeyi i\u00e7eren toplam dok\u00fcman say\u0131s\u0131 ve t\u00fcm dok\u00fcmanlar\u0131n say\u0131s\u0131na dayal\u0131 olarak hesaplayan a\u011f\u0131rl\u0131kland\u0131rma y\u00f6ntemidir. B\u00f6ylece verideki \u00f6nemsiz de\u011ferler elenerek \u00f6nemli \u00f6znitelikler tespit edilir ve s\u0131n\u0131fland\u0131rma i\u015flemi s\u0131ras\u0131nda performans art\u0131\u015f\u0131 sa\u011flan\u0131r.","5262f3da":"Verimizdeki kategorilerin ka\u00e7 adet oldu\u011funu \u015fu \u015fekilde bulabiliriz.\n","b4b6ad60":"\u015eimdi de test verisindeki x de\u011fi\u015fkenini kullanarak model \u00fczerinden y de\u011fi\u015fkenini tahmin edelim. ard\u0131ndan modelin buldu\u011fu tahmini veri(y-pred )ile ger\u00e7ek test (y-test) verisi aras\u0131ndaki benzerlik oran\u0131n\u0131 bulaca\u011f\u0131z","f9cd000e":"Confusion matrixin k\u00f6\u015fegenleri \u00fczerindeki say\u0131lar do\u011frue\u015flemeyi, di\u011fer say\u0131lar yanl\u0131\u015f e\u015flemeyi g\u00f6sterir. \u0130deal bir modelde matrisin diagonel olmas\u0131 istenir. yani k\u00f6\u015fegen d\u0131\u015f\u0131ndaki say\u0131lar\u0131n s\u0131f\u0131r olmas\u0131 m\u00fckemmel modelleme oldu\u011funa i\u015farettir. Buna g\u00f6re n\u00f6tr ifadelerin 113'\u00fc, negatiflerin 228'i ve pozitiflerin 197'si do\u011fru atanm\u0131\u015ft\u0131r. modelin tahmin g\u00fcc\u00fc % 64 olarak bulundu.  ","cf0f4ee0":"yine burada t\u00fcm kelimeleri k\u00f6k\u00fcne indirgemeyi sa\u011flayacak bir fonksiyon yaz\u0131p, bunuda 'stop' s\u00fctununa uygulad\u0131\u011f\u0131m\u0131zda boyut indirgeme sa\u011flanacakt\u0131r","09eb2779":"boyutu \u00e7ok b\u00fcy\u00fck olan Vekt\u00f6r uzay modelimiz analize haz\u0131r s\u0131n\u0131flac\u0131s\u0131n\u0131 beklemektedir. \u015fimdi Destek Vekt\u00f6r Makineleri (Support Vector Machine) algoritmas\u0131n\u0131n \u00e7a\u011f\u0131ral\u0131m","c45940c2":"yukar\u0131da da g\u00f6r\u00fcld\u00fc\u011f\u00fc gibi bir anlam ifade etmeyen kelime gruplar\u0131ndan kurtulduk. B\u00f6ylece vekt\u00f6r uzay\u0131n\u0131n hacmi b\u00fcy\u00fck bir y\u00fckten kurtuldu.\n\nyinede tam bir yap\u0131land\u0131rma i\u00e7in bir ad\u0131m kald\u0131. o da kelimeleri \u00e7ekim ve yap\u0131m ekleri nedeniyle olu\u015fan varyosyonlardan kurtararak (sevmek=sevilmek,sevgi,sevin\u00e7 vb... ) k\u00f6ke ayr\u0131ma ya da di\u011fer ad\u0131yla Steeming i\u015flemine tabii tutmam\u0131z gerekecek. Maalesef \u0130ngilizce i\u00e7in ba\u015far\u0131l\u0131 \u00e7al\u0131\u015fmalar\u0131n oldu\u011fu k\u00fct\u00fcphaneler mevcutken T\u00fcrk\u00e7e i\u00e7in en de\u011ferli \u00e7al\u0131\u015fma TurkishStemmer ya da Zemberek k\u00fct\u00fcphaneleridir. Onlar da \u00e7ok ba\u015far\u0131l\u0131 olmasa da mevcut \u015fartlarda yeterli say\u0131labilir. \n","0faf1e4c":"bu fonksiyonu verimizdeki metin (Tweet) s\u00fctununa uygulay\u0131p yeni bir s\u00fctun olu\u015fturup farklara bakal\u0131m.","5feafbf6":"\u015fimdi verimizdeki gereksiz kelimeleri atmak i\u00e7in bir fonksiyon yazal\u0131m ","dc5435bf":"\n\n\nBuna g\u00f6re verimizin 1158 adedi n\u00f6tr, 1572 negatif ve 1472 pozitif olarak etiketlenmi\u015ftir.\n\nUygulamaya ge\u00e7meden \u00f6nce verinin analize haz\u0131r hale getirilmesi b\u00fcy\u00fck \u00f6nem arz eder. Bu s\u00fcrece veri \u00f6n i\u015fleme (Data Preprocessing) denir. Bunun i\u00e7in \u015fu ad\u0131mlar kullan\u0131l\u0131r. Uygulamada \u00e7ok olsa da kendimizin olu\u015fturdu\u011fu durak kelimeleri(ama, ve, ile, hi\u00e7 vb...) dosyas\u0131n\u0131 a\u00e7\u0131p 'stop' adl\u0131 dosyaya atal\u0131m\n","c8333e71":"T\u00fcrk\u00e7e Twitter verileriyle duygu analiz nas\u0131l yap\u0131l\u0131r? Bu \u00e7al\u0131\u015fmada tamamen bizim taraf\u0131m\u0131zdan elde edilen 4202 adet ileti etiketlenerek analiz edilecektir. Duygu ifadeleri metnin anlam\u0131ndan olu\u015fturulmu\u015ftur. Buna g\u00f6re, n\u00f6tr ifadeler 0 ile, negatifler 1 ile ve pozitifler 2 ile etiketlenmi\u015ftir. Veri dosyam\u0131z\u0131 \u00e7a\u011f\u0131ral\u0131m ve inceleyelim.\n"}}