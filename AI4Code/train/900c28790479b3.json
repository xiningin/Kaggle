{"cell_type":{"8ffda404":"code","eb9e599f":"code","55787365":"code","ed2013b9":"code","19534bc0":"code","3a9379f9":"code","8e222dd5":"code","966091cf":"code","03e78dc7":"code","33737e94":"code","6873b23f":"code","179b3ae2":"code","7e5139e9":"code","2b9fbc2a":"code","c0d6719b":"code","882107c8":"code","82ac1bac":"code","950a3917":"code","0009507e":"code","e303c8a9":"code","ba9127e4":"code","9fe7d42e":"code","9649933d":"code","4ed7ae2e":"code","782721db":"code","eed6b28c":"code","c69860dd":"code","1e89fc9c":"code","52074443":"code","a1d216c1":"code","56defb4e":"code","dbad3213":"code","a1b4b508":"code","192c0714":"code","b843131f":"code","19d36e4b":"code","5dee91c3":"code","609a3d6f":"code","4c642f25":"code","94d272d5":"code","91d42635":"markdown","4c35b85b":"markdown","de8dd8b5":"markdown","5060f76e":"markdown","0c000687":"markdown","e36a5c49":"markdown","3f579f98":"markdown","c6cdd8ad":"markdown","b0cb9889":"markdown","913ea189":"markdown","0f774587":"markdown","8750f669":"markdown","a66d5b4c":"markdown","a7cce8c2":"markdown"},"source":{"8ffda404":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score\nfrom sklearn.utils import resample ## Used for sampling the data","eb9e599f":"cc = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\n","55787365":"cc.head()","ed2013b9":"cc.shape","19534bc0":"cc.info()","3a9379f9":"cc.Class.value_counts()","8e222dd5":"Y = cc['Class']","966091cf":"Y.count()","03e78dc7":"X = cc.drop(['Class'], axis = 1)\nX.head()","33737e94":"Y.value_counts()","6873b23f":"## Preparing the Training and test datasets\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.20, random_state = 30)","179b3ae2":"X_train.shape","7e5139e9":"Y_train.shape","2b9fbc2a":"X_test.shape","c0d6719b":"Y_train.shape","882107c8":"## Logistic Regression\nlr_model = LogisticRegression(solver='liblinear').fit(X_train,Y_train)","82ac1bac":"lr_pred = lr_model.predict(X_test)","950a3917":"print(\"Logistic Regression Metrics:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(Y_test, lr_pred))\nprint(\"F1 Score:\", f1_score(Y_test,lr_pred))\nprint(\"Recall Score:\",recall_score(Y_test, lr_pred))","0009507e":"## Random Forest Classifier \n\nrf = RandomForestClassifier(n_estimators=10)","e303c8a9":"rf_model = rf.fit(X_train, Y_train)","ba9127e4":"rf_pred = rf_model.predict(X_test)","9fe7d42e":"print(\"Random Forest Metrics:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(Y_test, rf_pred))\nprint(\"Recall Score:\", recall_score(Y_test, rf_pred))\nprint(\"F1 Score:\", f1_score(Y_test, rf_pred))","9649933d":"# concatenate our training data back together\n\nX = pd.concat([X_train, Y_train], axis=1)\nX.head()","4ed7ae2e":"not_fraud = X[X.Class==0]\nfraud = X[X.Class==1]\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nupsampled.Class.value_counts()","782721db":"y_train = upsampled.Class\nX_train = upsampled.drop('Class', axis = 1)","eed6b28c":"X_train.shape","c69860dd":"y_train.shape","1e89fc9c":"## Logistic Regression\nlr_model2 = LogisticRegression(solver='liblinear').fit(X_train,y_train)","52074443":"lr_pred2 = lr_model2.predict(X_test)","a1d216c1":"print(\"Logistic Regression Metrics after Oversampling minority class:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(Y_test, lr_pred2))\nprint(\"F1 Score:\", f1_score(Y_test,lr_pred2))\nprint(\"Recall Score:\",recall_score(Y_test, lr_pred2))","56defb4e":"## Random Forest Classifier \n\nrf = RandomForestClassifier(n_estimators=10)","dbad3213":"rf_model2 = rf.fit(X_train, y_train)","a1b4b508":"rf_pred2 = rf_model2.predict(X_test)","192c0714":"print(\"Random Forest Metrics after Oversampling minority class:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(Y_test, rf_pred2))\nprint(\"Recall Score:\", recall_score(Y_test, rf_pred2))\nprint(\"F1 Score\", f1_score(Y_test, rf_pred2))","b843131f":"from imblearn.over_sampling import SMOTE","19d36e4b":"# Separate input features and target\ny = cc.Class\nX = cc.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=27)\n\nsm = SMOTE(random_state=27, ratio=1.0)\nX_train, y_train = sm.fit_sample(X_train, y_train)","5dee91c3":"lr_pred_smote = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nsmote_pred = lr_pred_smote.predict(X_test)\n","609a3d6f":"print(\"Logistic Regression Metrics after SMOTE:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(y_test, smote_pred))\nprint(\"F1 Score:\", f1_score(y_test,smote_pred))\nprint(\"Recall Score:\",recall_score(y_test, smote_pred))","4c642f25":"rf_pred_smote = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\nrf_smote_pred = rf_pred_smote.predict(X_test)\n","94d272d5":"print(\"Random Forest Metrics after SMOTE:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(y_test, rf_smote_pred))\nprint(\"Recall Score:\", recall_score(y_test, rf_smote_pred))\nprint(\"F1 Score\", f1_score(y_test, rf_smote_pred))","91d42635":"**Oversampling minorty class** ","4c35b85b":"**We can see the Fraud and Non Fraud rows are same in count. Now we will run classifier algorithm and check whether the metrics parameter changed or not.**","de8dd8b5":"**Let us run Logistic regression and evaluate the performance metrics**","5060f76e":"**3. Generation Synthetic Samples - SMOTE (Synthetic Minority Oversampling Technique)**\n\nA technique similar to upsampling is to create synthetic samples. Here we will use imblearn\u2019s SMOTE or Synthetic Minority Oversampling Technique. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data.\n\n* Works by creating synthetic samples from the minor class (no-subscription) instead of creating copies.\n* Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observations.","0c000687":"**2. Resampling Techniques - Oversampling minorty class**\n\nIn Oversampling minority class we will increasing the fraud rows to such an extent that it will be 1:1 ratio with non fraud rows so as to attain equal representation of both the classes.\n\nNon Fradulent rows = 1660\n\nIncreasing the Fradulent rows to 1660 to have equal ratio between both.\n\nLet us learn by solving one example. We will use the **Credit Card Fraud Detection Dataset** available on Kaggle for our operations.\n\n\n","e36a5c49":"**Running Random Forest**","3f579f98":"**If we observe here, after Oversampling of minority class the accuracy has reduced but the Recall has significantly increased which serve some of our purpose of classifications model. The percentage of FN( False Negative) has reduced a lot.**\n\n**Recall is define as TP \/ (TP + FN)**","c6cdd8ad":"**Dealing with Imbalanced Datasets**\n\nImbalanced class problem are very common in classification model where one class count of response variable is very less in comparision to other class. Such as in banking Fraud detection, health care medicial diagnosis of rare disease etc where the fraud counts are very less in comparision to non fraud rows. It has been observed that positive cases of being deafult or fraud is approximately to 2-3% of the total data. So in such scenario sometimes machine learning algorithm fails to learn the underlying pattern and could not correctly identify the cases where real default occurs.\n\nSo to deal with this kind of problem is to oversample the minority class of response variable and make it as 50:50(class=0 :class =1) or 60:40(class=0 :class =1)\n\nWe have many ways to deal this , few techniques are as below:","b0cb9889":"**1. Resampling techniques - Undersampling majority class**\n\nLet us consider a fraud detection dataset where we have\nTotal Observation = 2000\nNon Fradulent rows = 1660\nFradulent rows = 40\n\nSo here we can see the fradulent rows are only 2% of the total dataset.\n\nUndersampling majority class is a technique where we will take some 10% or 15% from samples without replacement from Non Fraud instances and combining them with the Fradulent rows.\n\n 10% of 2000 = 200\n \n Total Observation = 40 + 200 = 240\n Fraudulent rows% = 40\/240 = 16.6%\n \n Now we have significant increase in the Fradulent data set count. \n \n **Disadvantages:**\n \n*  Due to less number of data we will have bias problem, as a result machine learning algorithm will fail to learn    many underlying pattern and cannot able to predict for new data.\n\n*  Many useful data will be missed.\n ","913ea189":"**We can see the Recall score has increased significantly after the SMOTE**","0f774587":"**CONCLUSION**\n\nWe explored 3 different methods for dealing with imbalanced datasets:\n\n1. Undersampling majority class\n2. Oversampling minorty class\n3. Generation Synthetic Samples - SMOTE\n\nThere are lot of methods to deal with Imbalanced dataset. We have to choose whoich best suits your problem.\n\n","8750f669":"**Running Logistic regression**","a66d5b4c":"** Let us run Random Forest and evaluate the performance Metrics**","a7cce8c2":"**we will apply resample function from sklearn**"}}