{"cell_type":{"14c466ab":"code","7eea5136":"code","e656d141":"code","760627e2":"code","ef891853":"code","8503093c":"code","c71c2a59":"code","2df90fda":"code","9ba7773e":"code","0e707327":"code","2f7b3752":"code","c79fa516":"code","5b181ce0":"code","8fb0f3fa":"code","4e31b7c7":"code","a84279a4":"code","101405a4":"code","df494514":"code","28232d70":"code","684388de":"code","fbd260e9":"code","09edb13b":"code","9a0d6137":"code","e63971d6":"code","e9ad0294":"code","b05ffcf0":"code","fab977bf":"code","2c9d0684":"code","618b550a":"code","6fe886f5":"code","5abedd86":"code","1b6f9866":"code","ac9e0c9f":"code","5c27e6ef":"code","1c2f2661":"code","11479351":"code","b6849381":"code","62262b2b":"code","5162b5d8":"code","d2c8d830":"code","2da59230":"code","aeeeeac8":"code","c715e8b0":"code","5000fff1":"code","8d7662c8":"code","49ca2010":"code","185b99d8":"code","3fd18929":"code","d1562b8c":"code","5fa80da2":"code","b7833ba7":"code","e75524b2":"code","6432ece5":"code","2713c5f3":"code","94393491":"code","c4624eea":"code","5d51b024":"code","67b50e35":"code","5acbb614":"code","de151e88":"code","742f8322":"code","3c319e6a":"code","27f81e86":"code","afd681aa":"code","e581679b":"code","fb359f77":"code","c6ce6bab":"code","dc8647a1":"code","50e1bc91":"code","23288fd1":"code","68df1df4":"code","68071967":"code","11a6e911":"code","06c383d8":"markdown","6d26fc9b":"markdown","b7b2f5d5":"markdown","9849e408":"markdown","89bdc28b":"markdown","ca0848f4":"markdown","803c8b0f":"markdown","e9260b90":"markdown","4cec6f1f":"markdown","8c305528":"markdown","9e0d517c":"markdown","255c94f6":"markdown","04b676c4":"markdown","6f48d00c":"markdown","9bda9e6e":"markdown","ffe66afe":"markdown","17517fd8":"markdown","bf3d26c7":"markdown","0d5d9f7f":"markdown","5a6817ec":"markdown","af3c8bf1":"markdown","22b07f8c":"markdown","50df8ad0":"markdown","0486f2b9":"markdown","f6d96f35":"markdown","9a9745cd":"markdown","ca0121d6":"markdown","4be41b7a":"markdown","f5487133":"markdown","93f65adf":"markdown","ad436c68":"markdown","ed1abc3e":"markdown","e9eef5a6":"markdown","6b67f840":"markdown","ebb7b59e":"markdown","4756b7ba":"markdown","38faf876":"markdown","06854ea2":"markdown","1f63c121":"markdown","8dfaf6f0":"markdown","ca0c6801":"markdown","e9c1e128":"markdown","0ae58939":"markdown","31d778e3":"markdown","595fadcb":"markdown","183536ae":"markdown","05a91ee5":"markdown","1327dbcb":"markdown","416e55b0":"markdown","a68618df":"markdown","9436d221":"markdown","dff73f2a":"markdown","e72d8210":"markdown","81653b46":"markdown"},"source":{"14c466ab":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","7eea5136":"#Check the number of columns and their names\ntrain_cols = train.columns\ntest_cols = test.columns\n\nprint(\"Train columns: {}\".format(len(train_cols)))\nprint(\"Test columns: {}\".format(len(test_cols)))\n\nprint(\"Is in train, not in test: {}\".format(set(train_cols) - set(test_cols)))\nprint(\"Is in test, not in train: {}\".format(set(test_cols) - set(train_cols)))","e656d141":"#Getting general dataframe data\ntrain.info()","760627e2":"#Check first and last PassengerId in train\/test set\nprint(train[\"PassengerId\"].min())\nprint(train[\"PassengerId\"].max())\nprint(test[\"PassengerId\"].min())\nprint(test[\"PassengerId\"].max())","ef891853":"all_data_df = train.append(test, ignore_index=True)\nall_data_df[\"is_train\"] = all_data_df[\"PassengerId\"].apply(lambda x: True if x <= 891 else False)","8503093c":"from scipy.stats import kendalltau, pearsonr, spearmanr\nimport scipy.stats as stats\nfrom sklearn.feature_selection import chi2\nfrom sklearn.metrics import mutual_info_score\nfrom sklearn.metrics import adjusted_mutual_info_score\n\n#Features overview function\n\ndef kendall_pval(x,y):\n    return kendalltau(x,y)[1]\n\ndef pearsonr_pval(x,y):\n    return pearsonr(x,y)[1]\n\ndef spearmanr_pval(x,y):\n    return spearmanr(x,y)[1]\n\ndef generate_features_overview(df):\n    df_info = pd.DataFrame()\n    df_info[\"type\"] = df.dtypes\n    df_info[\"missing_count\"] = df.isna().sum()\n    df_info[\"missing_perc\"] = (df_info[\"missing_count\"] \/ len(df) * 100).astype(int)\n    df_info[\"unique\"] = df.nunique()\n    df_info[\"top\"] = df.mode().head(1).T\n    df_info[\"freq\"] = df[df==df_info[\"top\"]].count()\n    df_info[\"freq_perc\"] = (df_info[\"freq\"] \/ len(df) * 100).astype(int)\n    \n    temp_df = df.apply(lambda x : pd.factorize(x)[0] if x.dtypes == \"object\" else x)\n\n    temp_df.fillna(-1, inplace=True)\n\n    df_info[\"var\"] = temp_df[temp_df[\"is_train\"] == True].var()\n    df_info[\"skew\"] = temp_df[temp_df[\"is_train\"] == True].skew()\n    \n    \n    df_info[\"corr\"] = df[df[\"is_train\"] == True].corr(\"pearson\")[target_feature]\n    df_info[\"corr_p_value\"] = df[df[\"is_train\"] == True].corr(method=pearsonr_pval)[target_feature]\n\n    for feature in df.loc[:, df.dtypes == \"object\"].columns:\n        dummies_df = pd.get_dummies(df[feature].fillna(-1),prefix=feature)\n        dummies_df[target_feature] = df[target_feature]\n        \n        \n        ma = dummies_df.corr(\"pearson\")[target_feature][:-1].max()\n        mi = dummies_df.corr(\"pearson\")[target_feature][:-1].min()\n        \n        if abs(ma) > abs(mi):\n            df_info.loc[feature, \"corr\"] = ma\n        else:\n            df_info.loc[feature, \"corr\"] = mi\n\n        \n    df_info = pd.concat([df_info, df.describe().T], axis=1)\n    return df_info\n\n#Correlation function\n\ndef show_corr_heatmap(df, method, width=10):\n    \n    if method == \"MI\":\n        corr = MI_correlations(df)\n    else:\n        corr = df.corr(method)\n        \n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(width, width))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt=\".2f\")\n    \n    return corr\n\n\ndef MI_correlations(df):\n    corrs = {}\n    for col_init in df.columns:\n        corrs[col_init] = {}\n        for col_corr in df.columns:\n            if col_init != col_corr:\n                corrs[col_init][col_corr] = calc_MI(df[col_init], df[col_corr])\n\n    return pd.DataFrame(corrs)\n\ndef calc_MI(col_init, col_corr):\n    \n    if col_init.dtype == np.object:\n        col_init = col_init.astype('category').cat.codes\n    elif col_init.dtype.name == \"category\":\n        col_init = col_init.cat.codes\n        \n    if col_corr.dtype == np.object:\n        col_corr = col_corr.astype('category').cat.codes\n    elif col_corr.dtype.name == \"category\":\n        col_corr = col_corr.cat.codes\n\n    mi = mutual_info_score(col_init, col_corr)\n\n    return mi","c71c2a59":"all_data_df.head(10)","2df90fda":"#Features categorization\nid_feature = \"PassengerId\"\n\ntarget_feature = \"Survived\"\n\nnumerical_features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n\ncategorical_features = [\"Pclass\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]","9ba7773e":"#Target feature balance\nall_data_df[\"Survived\"].value_counts(normalize=True)","0e707327":"#Converting Pclass to categorical feature\nall_data_df[\"Pclass\"] = all_data_df[\"Pclass\"].astype(str)","2f7b3752":"#I am using custom function to overview the features with lots of additional data\n\ninfo_df = generate_features_overview(all_data_df)\ninfo_df","c79fa516":"info_df.loc[[\"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\"]]","5b181ce0":"#Show first 20 names in a dataframe\nall_data_df[\"Name\"].head(20)","8fb0f3fa":"#Extract titles\nall_data_df[\"Title\"] = all_data_df[\"Name\"].str.extract(r\".+,\\s([a-zA-z\\s]*)\")\ncounts = all_data_df[\"Title\"].value_counts()\ncounts","4e31b7c7":"#Create new title group \"special\" for all titles which occure less than 10 times\nall_data_df[\"Title\"] = all_data_df[\"Title\"].apply(lambda x: x if x in counts[:4] else \"special\")\nall_data_df[\"Title\"].value_counts()","a84279a4":"#Remove \"Name\" feature since we can't use it in the model due to high variability\nall_data_df.drop(\"Name\", axis=1, inplace=True)","101405a4":"plt.subplots(1,1,figsize=(10,5))\nsns.barplot(x=\"Title\", y=\"Survived\", hue=\"Sex\", data=all_data_df)","df494514":"sns.barplot(x=\"Sex\", y=\"Survived\", data=all_data_df)","28232d70":"all_data_df[\"IsFemale\"] = all_data_df[\"Sex\"] == \"female\"","684388de":"sns.distplot(all_data_df['Age'], bins=10)","fbd260e9":"age_ranges = pd.qcut(all_data_df[\"Age\"], 10)\nplt.subplots(1,1,figsize=(15,5))\nsns.barplot(x=age_ranges.values, y=\"Survived\", hue=\"Sex\", data=all_data_df)","09edb13b":"all_data_df[\"IsAgeMissing\"] = all_data_df[\"Age\"].isna()\nsns.barplot(x=\"IsAgeMissing\", y=\"Survived\", hue=\"Sex\", data=all_data_df)\nprint(\"Missing ages count: {}\".format(len(all_data_df[all_data_df[\"IsAgeMissing\"] == True])))","9a0d6137":"all_data_df[\"Age\"].fillna(all_data_df[\"Age\"].median()+0.5, inplace=True)","e63971d6":"#Function to select estimated values\ndef isAgeEstimated(row):\n    \n    decimal_part = row % 1\n    whole_part = row \/\/ 1\n    \n    if (decimal_part > 0) & (whole_part > 0):\n        return True\n    else:\n        return False\n    \nall_data_df[\"IsAgeEstimated\"] = all_data_df[\"Age\"].apply(isAgeEstimated)\nsns.barplot(x=\"IsAgeEstimated\", y=\"Survived\", hue=\"Sex\", data=all_data_df)\nprint(\"Estimated ages count: {}\".format(len(all_data_df[all_data_df[\"IsAgeEstimated\"] == True])))","e9ad0294":"fig, ax = plt.subplots(2,2,figsize=(10,10))\n\nax[0,0].set(ylim=(0, 1100))\nax[0,1].set(ylim=(0, 1100))\nax[1,0].set(ylim=(0, 1))\nax[1,1].set(ylim=(0, 1))\n\nsns.countplot(x=\"SibSp\", data=all_data_df, ax=ax[0,0])\nsns.countplot(x=\"Parch\", data=all_data_df, ax=ax[0,1])\n\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=all_data_df, ax=ax[1,0])\nsns.barplot(x=\"Parch\", y=\"Survived\", data=all_data_df, ax=ax[1,1])","b05ffcf0":"all_data_df[\"NumberOfRelatives\"] = all_data_df[\"Parch\"] + all_data_df[\"SibSp\"]\nfig, ax = plt.subplots(1,2,figsize=(10,5))\nsns.countplot(x=\"NumberOfRelatives\", hue=\"Sex\", data=all_data_df, ax=ax[0])\nsns.barplot(x=\"NumberOfRelatives\", y=\"Survived\", hue=\"Sex\", data=all_data_df, ax=ax[1])","fab977bf":"info_df.loc[[\"Pclass\", \"Fare\", \"Cabin\"]]","2c9d0684":"sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=all_data_df)","618b550a":"sns.distplot(all_data_df['Fare'], bins=10)","6fe886f5":"fare_ranges = pd.qcut(all_data_df[\"Fare\"], 10)\nfig, ax = plt.subplots(2,1,figsize=(20,10))\n\nsns.countplot(x=fare_ranges.values, data=all_data_df, ax=ax[0])\nsns.barplot(x=fare_ranges.values, y=\"Survived\", data=all_data_df, ax=ax[1])","5abedd86":"all_data_df[\"Fare\"].fillna(all_data_df[\"Fare\"].mean(), inplace=True)","1b6f9866":"print(\"Skewness before: {}\".format(all_data_df[\"Fare\"].skew()))\nall_data_df[\"Fare\"] = np.log1p(all_data_df[\"Fare\"])\nprint(\"Skewness after: {}\".format(all_data_df[\"Fare\"].skew()))","ac9e0c9f":"#Fill missing values\nall_data_df[\"Cabin\"].fillna(\"No_cabin\", inplace=True)\n#Show top 20 cabins in a dataframe\nall_data_df[\"Cabin\"].value_counts()[:20]","5c27e6ef":"all_data_df[\"Deck\"] = all_data_df[\"Cabin\"].str[0]","1c2f2661":"all_data_df[\"Deck\"].value_counts()","11479351":"#Remove \"Cabin\" feature since we can't use it in the model due to high variability\nall_data_df.drop(\"Cabin\", axis=1, inplace=True)","b6849381":"fig, ax = plt.subplots(2,1,figsize=(20,10))\n\nsns.countplot(x=\"Deck\", data=all_data_df.sort_values(\"Deck\"), ax=ax[0])\nsns.barplot(x=\"Deck\", y=\"Survived\", data=all_data_df.sort_values(\"Deck\"), ax=ax[1])","62262b2b":"all_data_df[\"HasCabin\"] = all_data_df[\"Deck\"] != \"N\"","5162b5d8":"fig, ax = plt.subplots(1,2,figsize=(10,5))\n\nsns.countplot(x=\"HasCabin\", hue=\"Sex\", data=all_data_df, ax=ax[0])\nsns.barplot(x=\"HasCabin\", y=\"Survived\", hue=\"Sex\", data=all_data_df, ax=ax[1])","d2c8d830":"info_df.loc[[\"Embarked\", \"Ticket\"]]","2da59230":"#Fill 2 missing values\nall_data_df[\"Embarked\"].fillna(all_data_df[\"Embarked\"].mode(), inplace=True)","aeeeeac8":"fig, ax = plt.subplots(1,2,figsize=(10,5))\n\nsns.countplot(x=\"Embarked\", data=all_data_df, ax=ax[0])\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=all_data_df, ax=ax[1])","c715e8b0":"fig, ax = plt.subplots(1,2,figsize=(10,5))\n\nax[0].set(ylim=(0, 650))\nax[1].set(ylim=(0, 650))\n\nsns.countplot(x=\"Embarked\", hue=\"Sex\", data=all_data_df, ax=ax[0])\nsns.countplot(x=\"Embarked\", hue=\"Pclass\", data=all_data_df, ax=ax[1])","5000fff1":"all_data_df.drop(\"Embarked\", axis=1, inplace=True)","8d7662c8":"#Show top 20 tickets in a dataframe\nall_data_df[\"Ticket\"].value_counts()[:20]","49ca2010":"#Remove \"Ticket\" feature since we can't use it in the model due to high variability\nall_data_df.drop(\"Ticket\", axis=1, inplace=True)","185b99d8":"cleaned_df = all_data_df.copy()\ncleaned_df.info()","3fd18929":"cleaned_df.drop(\"Sex\", axis=1, inplace=True)","d1562b8c":"#Encoding functions\n\ndef create_dummies(df,features):\n    df_temp = df.copy()\n    for col in features:\n        dummies = pd.get_dummies(df_temp[col],prefix=col)\n        df_temp = pd.concat([df_temp,dummies],axis=1)\n        df_temp = df_temp.drop(col, axis=1)\n    return df_temp","5fa80da2":"#Get a list of categorical columns\ncategorical_columns = list(cleaned_df.columns[cleaned_df.dtypes == \"object\"])\n\n#Encode columns\ncleaned_df = create_dummies(cleaned_df, categorical_columns)","b7833ba7":"#Split train and test set\ntrain = cleaned_df[cleaned_df[\"is_train\"] == True]\ntest = cleaned_df[cleaned_df[\"is_train\"] != True]\n\n#Drop is_train column\ntrain.drop(\"is_train\", axis=1, inplace=True)\ntest.drop(\"is_train\", axis=1, inplace=True)","e75524b2":"#Select all train features\nall_features = list(train.columns)\n\n#Remove ID feature\nall_features.remove(id_feature)","6432ece5":"manually_selected_features = list(train[all_features].corr()[target_feature].sort_values(ascending=False).index)\ncorr_df = show_corr_heatmap(train[manually_selected_features], \"pearson\", 30)","2713c5f3":"#Remove low target correlated independent features - correlation index < abs(0.05)\nmanually_selected_features.remove(\"Deck_A\")\nmanually_selected_features.remove(\"Title_special\")\nmanually_selected_features.remove(\"NumberOfRelatives\")\nmanually_selected_features.remove(\"Deck_G\")\nmanually_selected_features.remove(\"Deck_T\")\nmanually_selected_features.remove(\"SibSp\")","94393491":"#Remove high mutually correlated independent features - correlation index > abs(0.70)\nmanually_selected_features.remove(\"IsFemale\")\nmanually_selected_features.remove(\"IsAgeMissing\")\nmanually_selected_features.remove(\"Deck_N\")","c4624eea":"corr_df = show_corr_heatmap(train[manually_selected_features], \"pearson\", 30)","5d51b024":"#Remove target feature since we are prediting it\nmanually_selected_features.remove(target_feature)","67b50e35":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\nfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n\ndef get_kBest_features(df, target_feature, ID_feature, function, num_of_features=-1):\n    \n    all_X = df.drop([target_feature, ID_feature], axis=1)\n    all_y = df[target_feature]\n    \n    # Select all features if number is not passed\n    if num_of_features == -1:\n        num_of_features = len(all_X.columns)\n    \n    # Create the model and fit it with data\n    kBest = SelectKBest(score_func=function,k=num_of_features)\n    kBest.fit(all_X, all_y)\n    \n    # Get columns to keep and create new dataframe with those only\n    cols = kBest.get_support(indices=True)\n        \n    # Create a dataframe of feature names and scores\n    names = all_X.columns.values[kBest.get_support()]\n    scores = kBest.scores_\n    names_scores = list(zip(names, scores))\n    feature_scores_df = pd.DataFrame(data = names_scores, columns=['feature', 'score'])\n    \n    #Sort the dataframe for better visualization\n    feature_scores_df_sorted = feature_scores_df.sort_values(['score', 'feature'], ascending = [False, True])\n\n    return feature_scores_df_sorted","5acbb614":"from sklearn.feature_selection import chi2, f_regression, mutual_info_regression, f_classif, mutual_info_classif\n\nmethods = [mutual_info_classif]\n\n#Score calculation\nfor method in methods:\n    best_features_kBest_df = get_kBest_features(train, target_feature, id_feature, method, 10)\n    print(best_features_kBest_df)\n    \n    #Create a list for easy modeling\n    best_features_kBest = list(best_features_kBest_df[\"feature\"])","de151e88":"from sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\n\ndef select_features_RFECV(df, target_feature, ID_feature):\n    df.dropna(axis=0, inplace=True)\n    \n    df = df.select_dtypes([np.number])\n    \n    all_X = df.drop([target_feature, ID_feature], axis=1)\n    all_y = df[target_feature]\n    \n    clf = LogisticRegression(max_iter=1000)\n    selector = RFECV(clf, cv=5, scoring='accuracy')\n    selector.fit(all_X, all_y)\n\n    \n    optimized_columns = all_X.columns[selector.support_]\n    \n    return optimized_columns","742f8322":"best_features_RFECV = list(select_features_RFECV(train, target_feature, id_feature))\nprint(best_features_RFECV)","3c319e6a":"import contextlib\nimport time\n\n@contextlib.contextmanager\ndef timer():\n    start = time.time()\n    \n    yield\n\n    end = time.time()\n    runtime = 'Runtime: {:.2f}s \\n'.format(end - start)\n    print(runtime)","27f81e86":"#Classification imports\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\ndef select_model(df, features, target, typ):\n    \n    all_X = df[features]\n    all_y = df[target]\n    \n    #CLASSIFICATION MODELS\n    models = [\n        {\n            \"name\": \"LogisticRegression\",\n            \"estimator\": LogisticRegression(),\n            \"hyperparameters\":\n                {\n                    \"max_iter\": [1000],\n                    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"],\n                    \"class_weight\": [None, \"balanced\"]\n                }\n        },\n        {\n            \"name\": \"LinearDiscriminantAnalysis\",\n            \"estimator\": LinearDiscriminantAnalysis(),\n            \"hyperparameters\":\n                {}\n        },\n        {\n            \"name\": \"KNeighborsClassifier\",\n            \"estimator\": KNeighborsClassifier(),\n            \"hyperparameters\":\n                {\n                    \"n_neighbors\": range(1,20,2),\n                    \"weights\": [\"distance\", \"uniform\"],\n                    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n                    \"p\": [1,2]\n                }\n        },   \n        {\n            \"name\": \"ExtraTreesClassifier\",\n            \"estimator\": ExtraTreesClassifier(),\n            \"hyperparameters\":\n                {\n                    \"max_depth\": [None],\n                    \"max_features\": [1, 3, 10],\n                    \"min_samples_split\": [2, 3, 10],\n                    \"min_samples_leaf\": [1, 3, 10],\n                    \"bootstrap\": [False],\n                    \"n_estimators\" :[10, 30],\n                    \"criterion\": [\"gini\"]\n                }\n        },\n        {\n            \"name\": \"AdaBoostClassifier\",\n            \"estimator\": AdaBoostClassifier(DecisionTreeClassifier(), random_state=1),\n            \"hyperparameters\":\n                {\n                    \"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n                    \"base_estimator__splitter\" :   [\"best\", \"random\"],\n                    \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n                    \"n_estimators\" :[1,2],\n                    \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]\n                }\n        },\n        {\n            \"name\": \"XGBClassifier\",\n            \"estimator\": XGBClassifier(),\n            \"hyperparameters\":\n                {\n                    \"n_estimators\":[280],\n                    \"max_depth\":[4,6,8],\n                    \"min_child_weight\":[1,2,3], \n                    \"learning_rate\":[0.3],\n                    \"subsample\":[0.8],\n                    \"base_score\":[0.5]\n                }\n        }\n    ]\n    \n    models_df = pd.DataFrame()\n\n    for model in models:\n        with timer():\n            print(\"Model: \", model[\"name\"])\n            if typ == \"grid\":\n                search = GridSearchCV(model[\"estimator\"], param_grid=model[\"hyperparameters\"], cv=5, scoring='accuracy')\n            elif typ == \"random\":\n                search = RandomizedSearchCV(model[\"estimator\"], param_distributions=model[\"hyperparameters\"], n_iter = 1, cv=5, scoring='accuracy')\n\n            search.fit(all_X, all_y)\n            model[\"best_params\"] = search.best_params_\n            model[\"best_score\"] = search.best_score_\n            model[\"best_model\"] = search.best_estimator_\n            model[\"mean_list\"] = search.cv_results_[\"mean_test_score\"]\n            model[\"std_list\"] = search.cv_results_[\"std_test_score\"]\n            model[\"mean\"] = search.cv_results_[\"mean_test_score\"].mean()\n            model[\"std\"] = search.cv_results_[\"mean_test_score\"].std()\n\n            print(\"Mean score: {:.4f} (+\/- {:.4f})\".format(model[\"mean\"], model[\"std\"]))\n            print(\"Best hyperparameters: \", model[\"best_params\"])\n    \n            models_df = models_df.append(model, ignore_index=True)\n\n    return models_df, models","afd681aa":"result_df_man, result_man = select_model(train, manually_selected_features, target_feature, \"grid\")","e581679b":"g = sns.barplot(\"mean\", \"name\", data = result_df_man, **{'xerr': result_df_man[\"std\"]})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","fb359f77":"result_df_kbest, result_kbest = select_model(train, best_features_kBest, target_feature, \"grid\")","c6ce6bab":"g = sns.barplot(\"mean\", \"name\", data = result_df_kbest, **{'xerr': result_df_kbest[\"std\"]})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","dc8647a1":"result_df_RFECV, result_RFECV = select_model(train, best_features_RFECV, target_feature, \"grid\")","50e1bc91":"g = sns.barplot(\"mean\", \"name\", data = result_df_RFECV, **{'xerr': result_df_RFECV[\"std\"]})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","23288fd1":"best_rf_model = result_RFECV[0][\"best_model\"]\n\nbest_rf_model.fit(train[best_features_RFECV], train[target_feature])\npredictions = best_rf_model.predict(test[best_features_RFECV])","68df1df4":"#Cast predictions from float to integer\ntest[target_feature] = predictions.astype(int)\n\n#Check predictions\ntest[target_feature].value_counts(normalize=True)","68071967":"def save_submission_file(data, filename=\"submission.csv\"):\n    test_ids = data[id_feature]\n    predictions = data[target_feature]\n    \n    submission_df = {id_feature: test_ids,\n                 target_feature: predictions}\n    \n    submission = pd.DataFrame(submission_df)\n    submission.to_csv(filename,index=False)","11a6e911":"save_submission_file(test)","06c383d8":"Modeling functions","6d26fc9b":"We can see that women are much more likely to survive - the survival rate for \"special\" title is 100%.\nMaster is a title which includes only young boys, which indicates that childen also have a better odds.","b7b2f5d5":"There seems to be a moderate drop in survival rate when the age is missing. Now lets fill the missing values with median + 0.5 so that we can tell the values were estimated. Then we take care of all estimated values.","9849e408":"<a id=\"section-4.1\"><\/a>\n## 4.1 Training and evaluating ##","89bdc28b":"Calculate Mutual information (MI) score for top 10 features.","ca0848f4":"Now let's generate correlation heatmap to get a better understanding in which feature to select for model training:\n\n1. Select good target correlated independent features\n2. Remove high mutually correlated independent features","803c8b0f":"<a id=\"section-2.4\"><\/a>\n## 2.4 Passenger social data ##\n\nThere were rich and poor passengers on board. Intuition tells ur that higher class passengers must have a higher chance to survive.\nBut let's look at the data.","e9260b90":"There are 186 unique values in Cabin column and 1014 missing values. As I already said before, the missing values should be filled with \"No_cabin\". But the data will still have high variability, so we should check cabin names for potential grouping.","4cec6f1f":"Split the dataframe into train and test set. Drop \"is_train\" column.","8c305528":"Predictions sanity check.","9e0d517c":"<a id=\"section-2.5\"><\/a>\n## 2.5 Passenger boarding data ##\n\nLast but not least I will take a look at the bording data.","255c94f6":"The survival rate drops even further. This could be related to the passenger class, but we will consider this later.\n\nFirst we need to finish with biological data, there are relatives features remaining.","04b676c4":"Now let's tackle the \"age\" feature, which can tell us more than just the passenger age:\n- age formated xx.5 is an estimated age\n- missing age values can indicate lower class passenger for which the age was not even estimated","6f48d00c":"<a id=\"section-2.3\"><\/a>\n## 2.3 Passenger biological data ##\n\nFirst I will compare biological passenger features.","9bda9e6e":"<a id=\"section-2.2\"><\/a>\n## 2.2 Features overview ##","ffe66afe":"Models need numerical data, therefore categorical features should be converted. I am using One Hot Encoding (dummy features).","17517fd8":"The age is slightly right skewed, but that should not be a problem. From the chart above we can also see that young boys (0-14 years) have by far the highest survival rate in the male category. However young girls (0-14 years) survival rate is the lowest in the women category.\n\nNow let's take care of the \"Age\" column. First we will create new columns \"IsAgeMissing\" and \"IsAgeEstimated\" to find potential correlation to survival rate.","bf3d26c7":"The chart confirms that females were much more likely to survive. Now let's convert the feature to binary to prevent overfitting.","0d5d9f7f":"<a id=\"section-4.2\"><\/a>\n## 4.2 Predicting ##","5a6817ec":"Add new column for easy distinction between train and test set.","af3c8bf1":"- Name has very high variability, therefore it needs some engineering to become useful\n- Sex column should be converted to binary column \"is_female\", it will help to prevent overfitting. It is also the feature with the best correlation to the survival feature.\n- Age has 20% of missing values that we need to fill. According to the documentation all ages below 1 are fractional and the format xx.5 shows that age is estimated\n- SibSp and Parch have high skewness we need to take care of","22b07f8c":"<a id=\"section-2\"><\/a>\n# 2. DATA EXPLORATION AND CLEANING #","50df8ad0":"We have 16 features with no missing values (except for the features we are predicting). First let's drop the Sex feature since we created its duplicate - IsFemale, which gives us the same information","0486f2b9":"<a id=\"section-3.3\"><\/a>\n## 3.3 Feature selection with RFECV ##\n\nSearching for best features using RFECV and LogisticRegression.","f6d96f35":"Most of the passengers which travel alone are men, they also have the lowest probability to survive. If they have any relatives, their odds increase.\nOn the other hand women survival rate stays pretty much the same if they are alone or not.\n\nIf a passenger has 4 or more relatives their survival rate decrease significantly. But that could be the coincidence, since there are very few cases.","9a9745cd":"We got the best accuracy from Logistic Regression, LinearDiscriminantAnalysis and XGBClassifier.\n\nIf you remember, RFECV features were selected from LogisticRegression model. That is why I will use it as my best model to make predictions.","ca0121d6":"Name column has a nice structure:\nSurname, Title. Name\n\nTitle is the value with the lowest variability so let's extract it using regex.","4be41b7a":"### Merging train and test data ###\nTrain and test data merge for easier cleaning and engineering.","f5487133":"<a id=\"section-3.1\"><\/a>\n## 3.1 Feature selection with correlation heatmap ##","93f65adf":"<a id=\"section-3.2\"><\/a>\n## 3.2 Feature selection with SelectKBest ##\n\nSearching for best features using SelectKBest and mutual information classification method.","ad436c68":"XGBClassifier is a clear winner with the highest mean accuracy and relatively low standard deviation.\n\nNow let't try the features selected by kBest algorithm (mutual information score).","ed1abc3e":"<a id=\"section-2.1\"><\/a>\n## 2.1 Features categorization ##\n\nThere are 12 total features (is_train excluded). Let's categorize them.","e9eef5a6":"As expected, higher class passengers had a higher survival rate.\nThe first class males odds were cca double from the other 2 categories. On the other hand, females survival rate is significantly lower only in the bottom class.","6b67f840":"Fare is highly right skewed, so we need to transform it using numpy's log1p function. Looking at the survival rate chart, results are expected. The more you paid for the ticket, the better your chances.\nWe have 1 missing value should be filled with mean.","ebb7b59e":"Men vs women percentage was the highest at Southampton which resulted in the lowest survival rate (as we already know women have much better odds). The highest survival rate of passengers boarding in Cherbourg (France) can be explained by the highest percentage of first classs passengers.\n\nTo sum up, Embarked column seems to be irrelavant for the model and should be removed.\n\nFinally let's take a look at the Ticket column.","4756b7ba":"The variability between scores is much lower and there is no clear winner. Let's finish with RFECV features modeling.","38faf876":"Ticket column has high variability, therefore it is useless as it is. I can't even find any meaningful grouping possibility, so I will exclude it from the modeling.","06854ea2":"As we can see, the predicted survival ratio is pretty close to the train data ratio. We can now proceed to creating submission file and publishing the results.","1f63c121":"I use GridSearchCV to select the best hyperparameters for my models. I am also using \"accuracy\" as my evaluating metric - it calculates the percentage of correct predictions.\n\nFirst I will model manually selected features, which are saved in \"manually_selected_features\" list.","8dfaf6f0":"Looks like Cabin name is in the following format: Character - Number. According to google, characters represent the deck level.\nLet's extract the decks and look for correlation to the Survival rate.","ca0c6801":"<a id=\"section-5\"><\/a>\n# 5. CREATING SUBMISSION FILE #","e9c1e128":"- More than half of the passengers (54%) were bottom class\n- Fare has extremely high skewness that we need to take care of\n- According to quick google search there were 371 cabins accomodating max 550 passengers, therefore \"Cabin\" missing values are most likely not missing values. They should be filled with \"No_cabin\"","0ae58939":"<a id=\"section-4\"><\/a>\n# 4. MODELING #","31d778e3":"Highlights:\n- 20% of missing values at the \"Age\" feature\n- 77% of missing values at the \"Cabin\" feature\n- High skewness of \"Fare\" feature\n- High variability at \"Name\", \"Ticket\" and \"Cabin\"\n- Good correlation of \"Pclass\", \"Sex\" and \"Cabin\" to target feature","595fadcb":"There seems to be no correlation between cabin deck and survival rate. The only what that stands out is \"N\" deck (No_cabin), so we should create new feature \"HasCabin\" and check it with the chart.","183536ae":"<a id=\"section-1\"><\/a>\n# 1. DATA IMPORT AND MERGE #","05a91ee5":"- Titanic boarded its passengers in three cities: Southampton (England) => Cherbourg (France) => Queenstown (Ireland), we have 2 missing values to fill with the mode\n- Ticket code variability is very high, therefore we need some engineering to make it useful","1327dbcb":"Highlights:\n- All_data_df dataframe (1309 passengers) is a sample of all Titanic passengers - 2224\n- Target feature is a binary value, meaning that classification model will have to be used.\n- Target feature balance should be considered for optimal model.\n- Pclass is presented as numerical feature, however it is categorical one (according to data presentation)","416e55b0":"Most of the passengers boarded at the begining of the voyage at Southampton. However the highest survival rate had the passengers which boarded at the first stop in France, which looks very strange. Let's find out why.","a68618df":"Passengers which had the cabin had more than double chance to survive. This must be related to passenger class and fare, but we will consider that later.","9436d221":"Table of contents:\n- [1. DATA IMPORT AND MERGE](#section-1)\n\n\n- [2. DATA EXPLORATION AND CLEANING](#section-2)\n    - [Features categorization](#section-2.1)\n    - [Features overview](#section-2.2)\n    - [Passenger biological data](#section-2.3)\n    - [Passenger social data](#section-2.4)\n    - [Passenger boarding data](#section-2.5)\n\n\n- [3. FEATURE SELECTION](#section-3)\n    - [Feature selection with correlation heatmap](#section-3.1)\n    - [Feature selection with SelectKBest](#section-3.2)\n    - [Feature selection with RFECV](#section-3.3)\n\n\n- [4. MODELING](#section-4)\n    - [Training and evaluating](#section-4.1)\n    - [Predicting](#section-4.2)\n\n\n- [5. CREATING SUBMISSION FILE](#section-5)","dff73f2a":"<a id=\"section-3\"><\/a>\n# 3. FEATURE SELECTION #","e72d8210":"Majority of passengers are traveling without sibling\/spouse and parent\/child. Furthermore the survival rate in this category is relatively low. Both features are highly right skewed so we will have to take care of that later.\n\nNow let's create new feature \"NumberOfRelavites\" to join both columns and make another comparison with Survival column.","81653b46":"Target feature is not in balance - there is much more likely to die (61,6%) than to survive (38,4%)."}}