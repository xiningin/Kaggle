{"cell_type":{"9e9083f1":"code","526cbdd9":"code","c78d0a92":"code","0ac05cc9":"code","1852a8cb":"code","f3f5d012":"code","277c7d48":"code","a03dec9b":"code","a63e03bd":"code","cef5e67a":"code","5449d834":"code","9e24de72":"code","3cbcb58d":"code","6f601166":"code","af24b749":"code","80fc3cfe":"code","54a13fc8":"code","e09a2c9e":"code","4e230090":"code","d739b700":"code","139434d8":"code","1cecab2c":"markdown","e8d3d348":"markdown","fdb0331a":"markdown","9ac160f6":"markdown","a5f5def9":"markdown","47d0f8c6":"markdown","f0baf650":"markdown","94410652":"markdown","dc5019e2":"markdown","84b05aed":"markdown","935b2d18":"markdown","191ef0d1":"markdown","573f7907":"markdown","d0332da3":"markdown","6c2e348d":"markdown","8dc55298":"markdown"},"source":{"9e9083f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","526cbdd9":"!pip -qq install split-folders tqdm","c78d0a92":"from __future__ import print_function\nfrom __future__ import division\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nimport random\nimport splitfolders\nimport shutil\n\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)","0ac05cc9":"data = '..\/input\/hymenoptera-data\/hymenoptera_data'\nnum_classes = 2\nnum_epochs = 30\n\n# When feature)Extract = False, we finetune the complete model, else we only update the reshaped layer params.\n# try this with feature_extract = False\nfeature_extract = True \n\nbatch_size = 8","1852a8cb":"seed = 42\nprint(f'setting everything to seed {seed}')\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","f3f5d012":"# Data Augmentation and Normalization for training\n# For Validation, we will only normalize the data\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}\n\n# Create training and validation datasets\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data, x), data_transforms[x]) for x in ['train', 'val']}\n# Dataloaders are created from the datasets\ndataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')","277c7d48":"image,label = image_datasets['train'][15]\nplt.imshow(image.permute(2,1,0))\nplt.show()\nprint(label, image_datasets['train'].classes[label])","a03dec9b":"image,label = image_datasets['train'][50]\nplt.imshow(image.permute(2,1,0))\nplt.show()\nprint(label, image_datasets['train'].classes[label])","a63e03bd":"image,label = image_datasets['val'][50]\nplt.imshow(image.permute(2,1,0))\nplt.show()\nprint(label, image_datasets['val'].classes[label])","cef5e67a":"labels = image_datasets['train'].targets\n_,labels_count = np.unique(labels, return_counts=True)\nlabels_count","5449d834":"class_names = image_datasets['train'].classes\n\nplt.figure(figsize=(8, 4))\nplt.title('Data distribution')\nplt.bar(class_names, labels_count, width=.5, color = ['C0', 'C1'])\nplt.show()","9e24de72":"# Feature extracting = True -> requires_grad = False\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","3cbcb58d":"def initialize_model(num_classes, feature_extract, use_pretrained=True):\n    model = None\n    input_size = 0\n    \n    model = models.resnet101(pretrained=use_pretrained)\n    set_parameter_requires_grad(model, feature_extract)\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, num_classes)\n    input_size = 224\n    \n    return model","6f601166":"model = initialize_model(num_classes, feature_extract)\n\nmodel = model.to(device)","af24b749":"# Gather the parameters to be optimized\/updated in this run. If we are\n# finetuning we will be updating all parameters. However, if we are\n# doing feature extract method, we will only update the parameters\n# that we have just initialized, i.e. the parameters with requires_grad\n# is True.\nparams_to_update = model.parameters()\nif feature_extract:\n    params_to_update = []\n    for name, param in model.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(name)\nelse:\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            print(name)\n\n# The parameters are optimizing\noptimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)","80fc3cfe":"# Loss function\ncriterion = nn.CrossEntropyLoss()","54a13fc8":"def train_model(model, dataloaders, optimizer, criterion=criterion, num_epochs=num_epochs, is_inception=False):\n    start = time.time()\n    val_acc_history = []\n    \n    best_model_weights = copy.deepcopy(model.state_dict())\n    best_acc = 0\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-'*10)\n        \n        # Each epoch's training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()   # Do this if model is in training phase\n            else:\n                model.eval()    # Do this if model is in validation phase\n                \n            running_loss = 0\n            running_corrects = 0\n            \n            # Iteration over the data\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # Parameter gradients are initialized to 0\n                optimizer.zero_grad()\n                \n                # Forward Pass\n                # Getting model outputs and calculating loss\n                with torch.set_grad_enabled(phase == 'train'):\n                    if is_inception and phase == 'train':      # Special case of inception because InceptionV3 has auxillary outputs as well. \n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n                        \n                    _, preds = torch.max(outputs, 1)\n                    \n                    # Backward pass and Optimization in training phase \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                # Stats\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds==labels.data)\n                \n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n            # Deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_weights = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n                \n    time_elapsed = time.time()\n    \n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    \n    # Best model weights are loaded here\n    model.load_state_dict(best_model_weights)\n    return model, val_acc_history","e09a2c9e":"model, history = train_model(model, dataloaders_dict, optimizer, criterion, num_epochs, False)","4e230090":"model_nft = initialize_model(num_classes, feature_extract=False)\n\nmodel_nft = model_nft.to(device)","d739b700":"feature_extract = False\nparams_to_update = model_nft.parameters()\nif feature_extract:\n    params_to_update = []\n    for name, param in model_nft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(name)\nelse:\n    for name, param in model_nft.named_parameters():\n        if param.requires_grad:\n            print(name)\n            \noptimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)","139434d8":"model_nft, history_nft = train_model(model_nft, dataloaders_dict, optimizer, criterion, num_epochs, False)","1cecab2c":"## Initializng model with feature_extract = False","e8d3d348":"## Load and Transforming the data","fdb0331a":"## Seed everything","9ac160f6":"## Optimizing parameters of the model\n\nDepends on feature_extract == {True or False}","a5f5def9":"## Simple Visualization","47d0f8c6":"## Data Distribution","f0baf650":"# Hope you like this notebook!\n## Please Upvote","94410652":"## Training the model with feature_extract = False","dc5019e2":"# Binary Classification using PyTorch: Resnet-101 illustrating Transfer Learning baseline ","84b05aed":"## Basic setting configuration","935b2d18":"Model here has been initialized and sent to GPU","191ef0d1":"When feature extract is false, the complete model will be fine-tuned.","573f7907":"## Parameter optimization check","d0332da3":"## Model training phase","6c2e348d":"## Importing libraries and modules","8dc55298":"## Model Initialization"}}