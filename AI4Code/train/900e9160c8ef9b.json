{"cell_type":{"1c470b9f":"code","dcc01b9d":"code","1fc009a4":"code","5ec4ce1d":"code","391d5b51":"code","0eae482c":"code","3787ce5e":"code","bdfbbd4c":"code","06c1c988":"code","2ed72d2e":"code","a26ce529":"code","36bc2d04":"code","d483b4f1":"code","008806b2":"code","8d5be7e5":"code","414a174b":"code","1a8d68bf":"code","ca154f2d":"code","1caf56c4":"code","44ece046":"code","f77825bd":"code","f077f1f3":"code","000b1762":"code","12166f5f":"code","b3e3e03f":"code","406403df":"markdown","9ae8470f":"markdown","8955aa50":"markdown","ff14f3ec":"markdown","feba347c":"markdown","530cb4b0":"markdown","8d393fbe":"markdown","96a6abbc":"markdown","dc10958f":"markdown","133356a8":"markdown","a4eb87aa":"markdown","03278c93":"markdown","2b6b64ca":"markdown","898ce397":"markdown","530e0621":"markdown","dbf868c8":"markdown","4a470448":"markdown","c972e740":"markdown","f7a77d1f":"markdown","30d20370":"markdown","5d1de35c":"markdown","121b73c3":"markdown","1664a47e":"markdown","65b40e6b":"markdown","373a98f7":"markdown","b5208c69":"markdown","2418274f":"markdown","e86a78b1":"markdown","a3c43d38":"markdown","95593976":"markdown","7e6a3e49":"markdown","e0e2c687":"markdown","466ac82c":"markdown","3244f2d0":"markdown","3264cc6c":"markdown","f7d54450":"markdown","73d3512c":"markdown","53050832":"markdown","95e33b2d":"markdown","63b245b1":"markdown"},"source":{"1c470b9f":"# import utilities\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport string\nimport time\nimport math\nimport re\n\n# import visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline\n\n# import PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as f\n\nimport os\nprint(os.listdir(\"..\/input\"))","dcc01b9d":"# Load data\n# Load kernel versions\nkernel_versions = pd.read_csv('..\/input\/KernelVersions.csv')\n# Load kernels (to retreive TotalVotes)\nkernels = pd.read_csv('..\/input\/Kernels.csv')","1fc009a4":"# Merge kernels and versions to retreive kernel title and total votes for kernel\nkernels_trc = kernels[['CurrentKernelVersionId', 'TotalVotes']].rename(columns={'CurrentKernelVersionId' : 'Id'})\nkernel_version_trc = kernel_versions[['Id', 'Title']]\nkernels_titles_votes = kernels_trc.merge(kernel_version_trc)\n\n# Sort titles by the number of votes\nkernels_titles_votes = kernels_titles_votes.sort_values(by=['TotalVotes'])\n\n# Retreive the list of popular kernel titles (at leat 1 vote)\npopular_kernel_titles = kernels_titles_votes[kernels_titles_votes['TotalVotes'] > 0]['Title'].unique().tolist()\n\n# Print out some examples\npopular_kernel_titles[:10]","5ec4ce1d":"print('Total number of kernels is {}.'.format(len(kernels)))\nprint('Total number of kernels with at leat 1 upvote is {}.'.format(len(popular_kernel_titles)))","391d5b51":"# Lowercase, remove punctuation and numbers from kernel titles\ndef clean_title(title):\n    '''\n    Function to lowercase, remove punctuation and numbers from kernel titles\n    '''\n    # lowercase\n    title = str(title).lower()\n    # replace punctuation into spaces\n    title = re.sub(r\"[,.;@#?!&$%<>-_*\/\\()~='+:`]+\\ *\", \" \", title)\n    title = re.sub('-', ' ', title)\n    title = re.sub(\"''\", ' ', title)\n    # replace numbers into spaces\n    title = re.sub(r\"[0123456789]+\\ *\", \" \", title)\n    #remove duplicated spaces\n    title = re.sub(' +', ' ', title)\n    \n    return title.strip()","0eae482c":"# Extract words from kernel titles\nend_of_sentence = '.' # symbol to denote the end of the sentence\ndef extract_words(title):\n    '''\n    Function which transforms kernel title into a list of words ending with 'end_of_sentence' word.\n    '''\n    title = clean_title(title)\n    words = title.split(' ')\n\n    return words","3787ce5e":"def create_vocabulary(titles):\n    '''\n    Function to create a vocabulary out of a list of titles\n    '''\n    vocab = set()\n    \n    for title in titles:\n        if (clean_title(title) != ''):\n            words = extract_words(title)\n            vocab.update(words)\n        \n    word_list = list(vocab)\n    word_list.append(end_of_sentence)\n    vocabulary = {word_list[n]:n for n in range(0,len(word_list))}\n    \n    return vocabulary","bdfbbd4c":"# create vocabulary out of pipular kernel titles\nvocab = create_vocabulary(popular_kernel_titles)\nvocab_size = len(vocab)","06c1c988":"print('There are {} words in vocabulary.'.format(vocab_size))","2ed72d2e":"# Translate word to an index from vocabulary\ndef wordToIndex(word):\n    if (word != end_of_sentence):\n        word = clean_title(word)\n    return vocab[word]\n\n# Translate word to 1-hot tensor\ndef wordToTensor(word):\n    tensor = torch.zeros(1, 1, vocab_size)\n    tensor[0][0][wordToIndex(word)] = 1\n    return tensor\n\n# Turn a title into a <title_length x 1 x vocab_size>,\n# or an array of one-hot vectors\ndef titleToTensor(title):\n    words = extract_words(title)\n    tensor = torch.zeros(len(words) + 1, 1, vocab_size)\n    for index in range(len(words)):\n        tensor[index][0][wordToIndex(words[index])] = 1\n    \n    tensor[len(words)][0][vocab[end_of_sentence]] = 1\n    return tensor\n\n# Turn a sequence of words from title into tensor <sequence_length x 1 x vocab_size>\ndef sequenceToTensor(sequence):\n    tensor = torch.zeros(len(sequence), 1, vocab_size)\n    for index in range(len(sequence)):\n        tensor[index][0][wordToIndex(sequence[index])] = 1\n    return tensor","a26ce529":"# Demonstrate functions above\nprint('Index of \"love\" in vocabulary is {}'.format(wordToIndex('love')))\nprint('Index of \"LoVE\" in vocabulary is {}'.format(wordToIndex('LoVE')))\n\nprint('Tensor representation of \"Fiddling With Python\" is:')\ntitle_tensor = titleToTensor('Fiddling With Python')\nprint(title_tensor)\n\nprint('Dimensions of title tensor: {}'.format(title_tensor.size()))","36bc2d04":"# Generate sequences out of titles:\n\n# Define sequence length\nsequence_length = 3\n\n# Generate sequences\ndef generate_sequences(titles):\n    sequences = []\n    targets = []\n    # Loop for all selected titles\n    for title in titles:\n        # Run through each title\n        if clean_title(title) != '' and clean_title(title) != ' ':\n            words = extract_words(title)\n            words.append(end_of_sentence)\n\n            for i in range(0, len(words) - sequence_length):\n                sequence = words[i:i + sequence_length]\n                target = words[i + sequence_length:i + sequence_length + 1]\n\n                sequence_tensor = sequenceToTensor(sequence)\n                target_tensor = sequenceToTensor(target)\n\n                sequences.append(sequence_tensor)\n                targets.append( target_tensor)\n            \n    return sequences, targets","d483b4f1":"sequences, targets = generate_sequences(popular_kernel_titles[:5000])","008806b2":"# Create LSTM\nclass SimpleLSTM(nn.Module):\n    '''\n    Simple LSTM model to generate kernel titles.\n    Arguments:\n        - input_size - should be equal to the vocabulary size\n        - output_size - should be equal to the vocabulary size\n        - hidden_size - hyperparameter, size of the hidden state of LSTM.\n    '''\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleLSTM, self).__init__()\n\n        self.hidden_size = hidden_size\n\n        self.lstm = nn.LSTM(input_size, hidden_size)\n        self.linear = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        output, hidden = self.lstm(input.view(1, 1, -1), hidden)\n        \n        output = self.linear(output[-1].view(1, -1))\n        \n        output = self.softmax(output)\n        return output, hidden\n\n    # the initialization of the hidden state\n    # device is cpu or cuda\n    # I suggest using cude to speedup the computation\n    def initHidden(self, device):\n        return (torch.zeros(1, 1, n_hidden).to(device), torch.zeros(1, 1, n_hidden).to(device))\n\n# Initialize LSTM\nn_hidden = 128\nrnn = SimpleLSTM(vocab_size, n_hidden, vocab_size) # inputs and outputs of RNN are tensors representing words from the vocabulary","8d5be7e5":"# Define a function which converts output into word\ndef wordFromOutput(output):\n    '''\n    Functions returns an index from the vocabulary and the corresponding word\n    '''\n    top_n, top_i = output.topk(1)\n    category_i = top_i[0].item()\n    return [k for (k, v) in vocab.items() if v == category_i], category_i","414a174b":"# test forward pass of the model\ninput = titleToTensor('Fiddling with')\nhidden = (torch.zeros(1, 1, n_hidden), torch.zeros(1, 1, n_hidden))\n\noutput, next_hidden = rnn(input[0], hidden)\n\nprint(output)\nwordFromOutput(output)","1a8d68bf":"# Define a function to convert tensor into index in vocabulary\ndef indexFromTensor(target):\n    '''\n    Function returns tensor containing target index given tensor representing target word\n    '''\n    top_n, top_i = target.topk(1)\n    target_index = top_i[0].item()\n    \n    target_index_tensor = torch.zeros((1), dtype = torch.long)\n    target_index_tensor[0] = target_index\n    return target_index_tensor","ca154f2d":"learning_rate = 0.005 \ncriterion = nn.NLLLoss()","1caf56c4":"# device to use\n# don't forget to turn on GPU on kernel's settings\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\ndevice","44ece046":"# Define training procedure\ndef train(sequence, target, device):\n    # Move tensors to device\n    hidden = rnn.initHidden(device)\n    sequence = sequence.to(device)\n    target = target.to(device)\n\n    rnn.zero_grad()\n\n    # Forward step\n    for i in range(sequence.size()[0]):\n        output, hidden = rnn(sequence[i], hidden)\n        \n    output, hidden = rnn(sequence[i], hidden)\n    \n    loss = criterion(output, indexFromTensor(target).to(device))\n    loss.backward()\n\n    # Add parameters' gradients to their values, multiplied by learning rate\n    for p in rnn.parameters():\n        p.data.add_(-learning_rate, p.grad.data)\n\n    return output, loss.item()","f77825bd":"# Set up the number of iterations, printing and plotting options\nn_iters = 1100000\nprint_every = 1000\nplot_every = 1000\n\n# Keep track of losses for plotting\ncurrent_loss = 0\nall_losses = []\n\nrnn = rnn.to(device)\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n# shuffle indices\nindices = np.random.permutation(len(sequences))\n\nstart = time.time()\n\n# run training procedure\nfor iter in range(1, n_iters + 1):\n    \n    # Pick index\n    index = indices[iter % len(sequences)]\n    \n    # Run one training step\n    output, loss = train(sequences[index], targets[index][0].long(), device)\n    current_loss += loss\n\n    # Print iter number and loss\n    if iter % print_every == 0:\n        guess, guess_i = wordFromOutput(output)\n        print('%d %d%% (%s) Loss: %.4f' % (iter, iter \/ n_iters * 100, timeSince(start), loss))\n\n    # Add current loss avg to list of losses\n    if iter % plot_every == 0:\n        all_losses.append(current_loss \/ plot_every)\n        current_loss = 0","f077f1f3":"# plot training loss\nplt.figure()\nplt.plot(all_losses)","000b1762":"# Sample title from the trained model\ndef sample():   \n    num_words = 10\n    \n    # Initialize input step and hidden state\n    input = torch.zeros(1, 1, vocab_size)\n    hidden = (torch.zeros(1, 1, n_hidden).to(device), torch.zeros(1, 1, n_hidden).to(device))\n    \n    i = 0\n    output_word = None\n    sentence = []\n    # Sample words from the model\n    while output_word != '.' and i < num_words:\n          \n        input = input.to(device)\n        output, next_hidden = rnn(input[0], hidden)\n        \n        y = output.clone()\n        y = y.to(device)\n        # use probabilities from the output to choose the next word\n        idx = np.random.choice(range(vocab_size), p = f.softmax(y, dim=1).detach().cpu().numpy().ravel())\n        \n        output_word = [k for (k, v) in vocab.items() if v == idx][0]\n        sentence.append(output_word)\n         \n        hidden = next_hidden\n        input = wordToTensor(output_word)\n        i = i+1\n        \n    return sentence","12166f5f":"# sample 15 titles and print\nfor i in range(15):\n    sampled_title = sample()\n    title = ' '.join(sampled_title)\n    print(title)\n    print(\"\\n\")","b3e3e03f":"# save the model for later\ncheckpoint = {'input_size': vocab_size,\n          'output_size': vocab_size,\n          'hidden_size': n_hidden,\n          'state_dict': rnn.state_dict()}\n\ntorch.save(checkpoint, 'rnn.pth')","406403df":"Introduce a function to clean kernel titles:","9ae8470f":"Plot training statistics:","8955aa50":"## Build the Model","ff14f3ec":"## Make a List of Popular Kernel Titles","feba347c":"Define training procedure:","530cb4b0":"When I first found out about [sequence models](https:\/\/medium.com\/machine-learning-bites\/deeplearning-series-sequence-models-7855babeb586), I was amazed with how easily they can be applied to a wide range of problems: text classification, text generation, music generation, machine translation and others. I shared some great resources on sequence models and LSTM in `\"References and Further Reading\"` section below. You might want to explore them before going through the rest of this kernel, because in this kernel I would like to focus on step-by-step process of creation a model and not on sequence models theory.\n\nI got an idea to use [Meta Kaggle](https:\/\/www.kaggle.com\/kaggle\/meta-kaggle) dataset to train a model for generation of new kernel titles. This could help to capture some trends for Kaggle kernels and give an inspiration. In this kernel:\n* I loaded and preprocessed Kaggle data on kernels.\n* Implemented and trained a sequence model for generation of new Kaggle titles.","8d393fbe":"Next step is to make a list of most popular kernel titles, which will be then converted into word sequences and passed to the model. It comes out that kernel titles are __extremely untidy__: misspelled words, foreign words, special symbols or just have poor names like 'kernel678hggy'.\n\nThat is why:\n* __I will drop kernels without votes from the analysis__. I will assume that kernels, which have votes are of better quality and have more meaningful titles.\n* I will sort kernels by the total number of votes and __take only the most voted ones__.","96a6abbc":"Introduce a function, which converts a tensor representing a word into an index from the vocabulary:","dc10958f":"Try the model with a sigle forward pass:","133356a8":"# Kaggle Titles Generation with PyTorch LSTM","a4eb87aa":"## Train the Model","03278c93":"## Conclusion\n","2b6b64ca":"Save the model weights for later:","898ce397":"Create the vocabulary out of kernel titles:","530e0621":"## Further Improvement\nThough I managed to get some interesting results, there is a lot what should be done here:\n* __Better data cleaning__: a lot of titles should be removed from the analysis as they are not in English or they're just can't be used (for example 'kernel123').\n* __Auto-correction of misspelled words__: titles can be preprocessed with automatic correction of misspelled words (for example, consider [PySpell package](https:\/\/facelessuser.github.io\/pyspelling\/)). I could add this to this kernel, but this would run too long for Kaggle. But this is still an option since data preprocessing happens just one time before training and preprocessed data could be saved for later.\n* __Hyperparameter tuning__: I suppose that learning rate and sequence length can be tuned to achieve even better results.\n* __Use [word embeddings](https:\/\/hackernoon.com\/word-embeddings-in-nlp-and-its-applications-fab15eaf7430) instead of one-hot encoding__ for words.","dbf868c8":"Set up the device for training:\n\n_Don't forget to enable GPU in the kernel settings!_","4a470448":"Set up learning rate and loss function:","c972e740":"Train the model:","f7a77d1f":"Demonstrate the utility functions:","30d20370":"In this section we will create a training set for our future model:\n* __Introduce functions which encode each word into tensor__ using the vocabulary created above. I will use one-hot encoding of words: each word will be represented as a tensor with zeros and ones with all zeros and one in the position which respects to the index of the word in the vocabulary.\n* __Generate sequences out of kernel titles.__ The length of the sequence is a hyperparameter. I chose sequence length equal to 3. So we will give the model a tensor containing encoding for 3 words and a prediction target, which contains the index of the 4th consequent word.","5d1de35c":"Function to create a vocabulary out of a list of titles:","121b73c3":"## Preprocess Kernel Titles and Create Vocabulary","1664a47e":"We should see that __training loss is decreasing__ over the number of epochs.","65b40e6b":"Generate word sequences:","373a98f7":"Introduce a symbol for the end of title and a word extraction function:","b5208c69":"Now the dataset and the model are ready and we can start training:","2418274f":"Print out some statistics:","e86a78b1":"## Load Data","a3c43d38":"I decided to try a __word based model__. That's why in the next step I will need to __create a vocabulary__, which then should be used to encode word sequences. \n\nTo create the vocabulary we have to do the following steps:\n* Clean each title to remove punctuation and lowercase all the words.\n* Split each title to words and add each word to the vocabulary.\n* Introduce a symbol, which denotes the end of the title (I chose `.`, but you can change it) and add it to the vocabulary.","95593976":"## Sample Kernel Titles from the Model","7e6a3e49":"## Create the Training Set","e0e2c687":"Sample 15 titles from the model:","466ac82c":"## References and Further Reading\nI also want to share some great resources on RNNs and LSTM:\n1. [The Unreasonable Effectiveness of Recurrent Neural Networks](https:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/): Article about RNNs and their use cases.\n2. [Long Short-Term Memory: From Zero to Hero with PyTorch](https:\/\/blog.floydhub.com\/long-short-term-memory-from-zero-to-hero-with-pytorch\/): PyTorch implementation of LSTMs.\n3. [LSTMs for Time Series in PyTorch](https:\/\/www.jessicayung.com\/lstms-for-time-series-in-pytorch\/): another LSTM implementation.\n4. [Exercise: Sampling from an RNN](https:\/\/pytorch-nlp-tutorial-ny2018.readthedocs.io\/en\/latest\/day2\/sampling.html): examples of sampling from RNN.","3244f2d0":"![image](https:\/\/github.com\/Lexie88rus\/sequence-models\/raw\/master\/assets\/antique-book-book-bindings-2355408.jpg)","3264cc6c":"In this kernel:\n* I loaded and did a simple preprocessing of real text data.\n* I created a sequence word based model, which can be used to generate new kernel titles.\n\nYou can see that the model doesn't generate something that makes sence, but there are still some funny results like these:\n* \"wealth bowl datamining\"\n* \"supplement approved databases\"\n* \"plane ignore population competition\"\n* \"projecting superfood prescribing survey\"\n* \"mediation cta aluminum kernals reviews\"\n\nThis happens when models crush into a __real life data__. They contain abbreviations, nicknames, words in different languages, misspelled words and a lot more. Of course, these results can be improved by __better data preprocessing__. I decsribed actions to improve the results in `\"Further Improvement\"` section below.","f7d54450":"The next step is building a simple __LSTM model__ with:\n* __Input and output sizes__ of the model should be equal to the size of the vocabulary, because we are trying to predict the next word for a sequence;\n* __LSTM block__ with 128 hidden units;\n* One __linear layer__ to translate from hidden size into the output size;\n* __Softmax activation__.","73d3512c":"At first I need to load the data. I will load `Kernels` and `KernelVersions` tables, which contain information on all kernels, total number of votes per kernel (later I will explain why we need this) and kernel titles.","53050832":"Print out some statistics:","95e33b2d":"Now we can use our trained model to generate new kernel titles! All we need to do is to write a simple sampling procedure:\n* Introduce the maximum number of words in the title (10 for example);\n* Pass zero tensors to the model as the initial word and hidden state;\n* Repeat following steps until the end of the title symbol is sampled or the number of maximum words in title exceeded:\n\n    * Use the probabilities from the output of the model to get the next word for a sequence;\n    * Pass sampled word as a next input for the model.","63b245b1":"## Introduction"}}