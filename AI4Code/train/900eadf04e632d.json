{"cell_type":{"144d1127":"code","e1862831":"code","4bda97be":"code","272d6732":"code","a850fe49":"code","c17c6fb5":"code","423e2f3f":"code","d59b6378":"code","619eea64":"code","1183d9a5":"code","8184dad7":"code","15251cd3":"code","df0a376a":"code","b31ca61f":"code","4451e3c3":"code","15fa8632":"code","8700d8fb":"code","e086b2ab":"code","7db46e25":"code","1abadec4":"code","9c91c640":"code","ce5ad788":"code","28c4d9bf":"markdown","314d6f9b":"markdown","2915ffb9":"markdown","0dd91fee":"markdown","607032d4":"markdown"},"source":{"144d1127":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV,cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\nfrom sklearn.metrics import classification_report","e1862831":"df = pd.read_csv('..\/input\/parkinsons-data-set\/parkinsons.data')","4bda97be":"df.head()","272d6732":"df.info()","a850fe49":"df.describe()","c17c6fb5":"df.isnull().any()","423e2f3f":"df['status'].value_counts()","d59b6378":"sns.countplot(df['status'])","619eea64":"X = df.drop(['name'], 1)\nX = np.array(X.drop(['status'], 1))\ny = np.array(df['status'])","1183d9a5":"parameters = {'criterion': ['gini', 'entropy'], 'max_depth': range(1,30), \n              'min_samples_leaf': range(5,50,5), 'min_samples_split': range(20,200,20)}","8184dad7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","15251cd3":"clf = tree.DecisionTreeClassifier()","df0a376a":"grid = GridSearchCV(clf, param_grid=parameters, cv=5)","b31ca61f":"grid.fit(X_train, y_train)\ngrid.best_params_","4451e3c3":"best_clf = grid.best_estimator_","15fa8632":"best_clf.score(X_test, y_test)","8700d8fb":"knn = KNeighborsClassifier(n_neighbors=5)\n#for kNN we need to scale the functions\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_holdout_scaled = scaler.transform(X_test)\nknn.fit(X_train_scaled, y_train)","e086b2ab":"from sklearn.metrics import accuracy_score","7db46e25":"knn_pred = knn.predict(X_holdout_scaled)\naccuracy_score(y_test, knn_pred)","1abadec4":"from sklearn.pipeline import Pipeline\n\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n\nknn_params = {'knn__n_neighbors': range(1, 10)}\n\nknn_grid = GridSearchCV(knn_pipe, knn_params,cv=5, n_jobs=-1, verbose=True)\n\nknn_grid.fit(X_train_scaled, y_train)\n\nknn_grid.best_params_, knn_grid.best_score_","9c91c640":"accuracy_score(y_test, knn_grid.predict(X_holdout_scaled))","ce5ad788":"from sklearn.linear_model import LogisticRegressionCV\n\n\nclf = LogisticRegressionCV(cv=5)\n\n\nclf.fit(X_train, y_train)\nclf.score(X_test,y_test)\n","28c4d9bf":"**K-NN** gave us the best result with 1 nearest neighbour, exactly 0.9385 accuracy_score between y_predicted and y_test","314d6f9b":"So that the tree does not retrain, we will cut it\n\nWe also use **cross validation** to improve the model.","2915ffb9":"Now let's adjust the number of neighbors k for k-NN:","0dd91fee":"Now, let's cross-validate the tree parameters. We'll tweak the maximum depth and maximum number of features to use on each split. Here's the gist of how GridSearchCV works: For each unique pair of max_depth and max_features, compute the model's performance with 5-fold cross validation, and then choose the best combination of parameters.","607032d4":"Let's list the best parameters and the corresponding average cross-validation accuracy."}}