{"cell_type":{"5f788263":"code","b795200f":"code","496ceec3":"code","82990a1f":"code","1b34d1b7":"code","bcba6f1c":"code","ff0cb721":"code","3e4176c5":"code","45ff573a":"code","7c4d31bc":"code","da9df3da":"code","51a67a98":"code","b9e83a92":"code","f974be96":"code","3c41635a":"code","44be4636":"code","058360dd":"code","4e2c112d":"code","aa6cdc02":"code","669fa23e":"code","835a9987":"code","1e67d96c":"code","dab2a1ac":"code","9d5d63a4":"code","819fa84d":"code","8942c2b1":"code","4f212fd5":"code","8b1d2ad2":"code","84fa8a4f":"code","fff2d762":"code","4df5a3df":"code","c7edb094":"code","18269379":"code","e3abcab3":"code","dadb2213":"code","e2f63660":"code","ac640d53":"code","950a1571":"code","05c88fc0":"code","f9f36a0f":"code","6b07901e":"code","99aa4ef8":"code","3f04bde7":"code","79bee0d6":"code","6ea60cce":"code","90917782":"code","d04e1f64":"code","95c6a43f":"code","e747fc1e":"markdown","5637d58c":"markdown","8180012b":"markdown","1c3e5a91":"markdown","4951544e":"markdown","5c571988":"markdown","a0309833":"markdown","a42b32a4":"markdown","e1da7a48":"markdown","04972081":"markdown","559942d4":"markdown","edf16b5a":"markdown","a3166eae":"markdown","7bfa5bb6":"markdown"},"source":{"5f788263":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nimport cv2","b795200f":"def readTextFile(path):\n    with open(path) as f:\n        captions = f.read()\n    return captions","496ceec3":"captions = readTextFile(\"..\/input\/flicker8k-image-captioning\/Flickr8k_text\/Flickr8k.token.txt\")\ncaptions = captions.split('\\n')[:-1]","82990a1f":"first,second  = captions[10].split('\\t')\nprint(first)\nprint(second)","1b34d1b7":"descriptions = {}\nfor x in captions:\n    first,second = x.split('\\t')\n    image_name = first.split(\".\")[0]\n    # if the image id is already present or not\n    if descriptions.get(image_name) is None:\n        descriptions[image_name] = []\n    descriptions[image_name].append(second)","bcba6f1c":"descriptions[\"1002674143_1b742ab4b8\"]","ff0cb721":"img_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_Dataset\/Flicker8k_Dataset\/\"\nimg = cv2.imread(img_path + \"1002674143_1b742ab4b8.jpg\")\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","3e4176c5":"def clean_text(sentence):\n    sentence = sentence.lower()\n    sentence = re.sub(\"[^a-z]\", \" \", sentence)\n    sentence = sentence.split()\n    sentence = [s for s in sentence if len(s)>1]\n    sentence = \" \".join(sentence)\n    return sentence","45ff573a":"clean_text(\"A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it . #12\")","7c4d31bc":"for key,caption_list in descriptions.items():\n    for i in range(len(caption_list)):\n        caption_list[i] = clean_text(caption_list[i])","da9df3da":"descriptions[\"1000268201_693b08cb0e\"]","51a67a98":"vocab = set()\nfor key in descriptions.keys():\n    [vocab.update(sentence.split()) for sentence in descriptions[key]]\n    \nprint(\"Vocab Size : %d\"% len(vocab))","b9e83a92":"total_words = []\nfor key in descriptions.keys():\n    [total_words.append(i) for des in descriptions[key] for i in des.split()]\nprint(\"Total Words\" , len(total_words))","f974be96":"\nimport collections\n\ncounter = collections.Counter(total_words)\nfreq_cnt = dict(counter)\nprint(len(freq_cnt.keys()))","3c41635a":"# Sort this dictionary according to the freq count\nsorted_freq_cnt = sorted(freq_cnt.items(),reverse=True,key=lambda x:x[1])\n\n# Filter\nthreshold = 10\nsorted_freq_cnt  = [x for x in sorted_freq_cnt if x[1]>threshold]\ntotal_words = [x[0] for x in sorted_freq_cnt]\n\n\nprint(len(total_words))","44be4636":"train_file_data = readTextFile(\"..\/input\/flicker8k-image-captioning\/Flickr8k_text\/Flickr_8k.trainImages.txt\")\ntest_file_data = readTextFile(\"..\/input\/flicker8k-image-captioning\/Flickr8k_text\/Flickr_8k.testImages.txt\")","058360dd":"train = [row.split(\".\")[0] for row in train_file_data.split(\"\\n\")[:-1]]\ntest = [row.split(\".\")[0] for row in test_file_data.split(\"\\n\")[:-1]]","4e2c112d":"train[:5]","aa6cdc02":"# Prepare Description for the Training Data\n# Tweak - Add <s> and <e> token to our training data\n\ntrain_descriptions = {}\nfor img_id in train:\n    train_descriptions[img_id] = []\n    for cap in descriptions[img_id]:\n        cap_to_append = \"startseq \" + cap + \" endseq\"\n        train_descriptions[img_id].append(cap_to_append)","669fa23e":"train_descriptions[\"1000268201_693b08cb0e\"]","835a9987":"from tensorflow.keras.applications.xception import Xception,preprocess_input,decode_predictions\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.layers import Input,Dense,Dropout,Embedding,LSTM\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical","1e67d96c":"model = Xception(weights='imagenet', input_shape=(299,299,3))\nmodel.summary()","dab2a1ac":"model_new = Model(model.input,model.layers[-2].output)","9d5d63a4":"def preprocess_img(img):\n    img = image.load_img(img,target_size=(299,299))\n    img = image.img_to_array(img)\n    img = np.expand_dims(img,axis=0)\n    # Normalisation\n    img = preprocess_input(img)\n    return img\n\ndef encode_img(img):\n    img = preprocess_img(img)\n    feature_vector = model_new.predict(img)\n    feature_vector = feature_vector.reshape((-1))\n    return feature_vector","819fa84d":"encode_img(img_path + \"1000268201_693b08cb0e.jpg\")","8942c2b1":"encoding_train = {}\nfor img_id in train:\n    PATH = img_path + img_id + \".jpg\"\n    encoding_train[img_id] = encode_img(PATH)","4f212fd5":"encoding_test = {}\nfor img_id in test:\n    PATH = img_path + img_id + \".jpg\"\n    encoding_test[img_id] = encode_img(PATH)","8b1d2ad2":"import pickle\n\nwith open(\"encoded_train_features.pkl\",\"wb\") as f:\n    pickle.dump(encoding_train,f)\n\nwith open(\"encoded_test_features.pkl\",\"wb\") as f:\n    pickle.dump(encoding_test,f)","84fa8a4f":"# Vocab\nlen(total_words)","fff2d762":"word_to_idx = {}\nidx_to_word = {}\nfor i,word in enumerate(total_words):\n    word_to_idx[word] = i+1\n    idx_to_word[i+1] = word","4df5a3df":"# Two special words\nidx_to_word[1846] = 'startseq'\nword_to_idx['startseq'] = 1846\n\nidx_to_word[1847] = 'endseq'\nword_to_idx['endseq'] = 1847\nvocab_size = len(word_to_idx) + 1\nprint(\"Vocab Size\",vocab_size)","c7edb094":"max_len = 0 \nfor key in train_descriptions.keys():\n    for cap in train_descriptions[key]:\n        max_len = max(max_len,len(cap.split()))\n        \nprint(max_len)","18269379":"from tensorflow.keras.utils import to_categorical\ndef data_generator(train_descriptions,encoding_train,word_to_idx,max_len,batch_size):\n    X1,X2, y = [],[],[]\n    \n    n =0\n    while True:\n        for key,desc_list in train_descriptions.items():\n            n += 1\n            \n            photo = encoding_train[key]\n            for desc in desc_list:\n                \n                seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]\n                for i in range(1,len(seq)):\n                    xi = seq[0:i]\n                    yi = seq[i]\n                    \n                    #0 denote padding word\n                    xi = pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n                    yi = to_categorical([yi],num_classes=vocab_size)[0]\n                    \n                    X1.append(photo)\n                    X2.append(xi)\n                    y.append(yi)\n                    \n                if n==batch_size:\n                    yield ([np.array(X1),np.array(X2)],np.array(y))\n                    X1,X2,y = [],[],[]\n                    n = 0","e3abcab3":"f = open(\"..\/input\/glove6b50dtxt\/glove.6B.50d.txt\",encoding='utf8')","dadb2213":"embedding_index = {}\nfor line in f :\n    values = line.split()\n    word = values[0]\n    word_embedding = np.array(values[1:],dtype = 'float')\n    embedding_index[word] = word_embedding","e2f63660":"embedding_index['machine']","ac640d53":"def get_embedding_matrix():\n    emb_dim = 50\n    matrix = np.zeros((vocab_size,emb_dim))\n    for word,idx in word_to_idx.items():\n        embedding_vector = embedding_index.get(word)\n        \n        if embedding_vector is not None:\n            matrix[idx] = embedding_vector\n    return matrix","950a1571":"embedding_matrix = get_embedding_matrix()\nembedding_matrix.shape","05c88fc0":"input_img_features = Input(shape=(2048,))\ninp_img1 = Dropout(0.3)(input_img_features)\ninp_img2 = Dense(256,activation='relu')(inp_img1)\n\n#Captions as Input\n\ninput_captions = Input(shape=(max_len,))\ninp_cap1 = Embedding(input_dim = vocab_size,output_dim=50,mask_zero=True)(input_captions)\ninp_cap2 = Dropout(0.3)(inp_cap1)\ninp_cap3 = LSTM(256)(inp_cap2)","f9f36a0f":"from tensorflow.keras.layers import Add\ndecoder1 = Add()([inp_img2,inp_cap3])\ndecoder2 = Dense(256,activation='relu')(decoder1)\noutputs = Dense(vocab_size,activation='softmax')(decoder2)\nmodel = Model(inputs=[input_img_features,input_captions],outputs=outputs)","6b07901e":"model.summary()","99aa4ef8":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","3f04bde7":"\nmodel.compile(loss='categorical_crossentropy',optimizer='adam')","79bee0d6":"epochs = 20\nbatch_size = 3\nsteps = len(train_descriptions)\/\/ batch_size\n#number_pics_per_batch","6ea60cce":"generator = data_generator(train_descriptions,encoding_train,word_to_idx,\n                                   max_len,batch_size)\nmodel.fit_generator(generator,epochs=20,steps_per_epoch=steps)","90917782":"model.save('model.h5')","d04e1f64":"def predict_caption(img):\n    in_text = 'startseq'\n    for i in range(max_len):\n        sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n        sequence = pad_sequences([sequence],maxlen=max_len,padding='post')\n        \n        ypred = model.predict([img,sequence])\n        ypred = ypred.argmax()\n        word = idx_to_word[ypred]\n        in_text += (' ' +  word)\n        if word == \"endseq\":\n            break\n    \n    final_caption = in_text.split()[1:-1]\n    final_caption = \" \".join(final_caption)\n    return final_caption","95c6a43f":"plt.style.use(\"seaborn\")\nfor i in range(15):\n    idx = np.random.randint(0,1000)\n    all_img_names = list(encoding_test.keys())\n    img_name = all_img_names[idx]\n    photo_2048 = encoding_test[img_name].reshape((1,2048))\n    \n    i = plt.imread(\"..\/input\/flicker8k-image-captioning\/Flickr8k_Dataset\/Flicker8k_Dataset\/\"+img_name+\".jpg\")\n    \n    caption = predict_caption(photo_2048)\n    #print(caption)\n    \n    plt.title(caption)\n    plt.imshow(i)\n    plt.axis(\"off\")\n    plt.show()","e747fc1e":"**Model Architecture**","5637d58c":"**Data Cleaning**","8180012b":"**Data PreProcessing For Captions**","1c3e5a91":"**Vocabulary**","4951544e":"**Predictions**","5c571988":"**Reading Text Captions**","a0309833":"**Word Embeddings**","a42b32a4":"**Total No of words across all the sentences**","e1da7a48":"**Transfer learning**","04972081":"**Filter Words from the Vocab according to certain threshold frequncy**","559942d4":"**Training Model**","edf16b5a":"**Prepare Train and Test Data**","a3166eae":"**Data Loader (Generator)**","7bfa5bb6":"**Dictionary to Map each image with the list of captions it has**"}}