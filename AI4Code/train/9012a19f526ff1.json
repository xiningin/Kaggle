{"cell_type":{"1530bd23":"code","17fe1b70":"code","009a793b":"code","ab1882bc":"code","925324ac":"code","519b0cb5":"code","010ac975":"code","3c930af4":"code","0dea4c6c":"code","9cd3b03e":"code","c9fd9a7a":"code","287ed1ce":"code","9fc6597a":"code","cf3beee6":"code","2006ee81":"code","8eee7e10":"code","2fc66afc":"code","85babc3c":"code","79ad2e9c":"code","9e4bc6f8":"markdown","c52315c8":"markdown","bd2d404e":"markdown","a65fb152":"markdown","c3e4ee34":"markdown","2019bba6":"markdown","9657ab5b":"markdown","fdb98e45":"markdown","b8563e83":"markdown","8109b184":"markdown","72cdd88d":"markdown","11c1e940":"markdown","da3dec03":"markdown","86d825ae":"markdown","70cb4234":"markdown","b8e01872":"markdown","1b2048b1":"markdown","ce24dabb":"markdown","d52c0892":"markdown"},"source":{"1530bd23":"import pandas as pd\nfrom sklearn import tree\nimport pydotplus\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\nimport matplotlib.image as pltimg\nimport numpy as np","17fe1b70":"# setting the display option to maximum\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', -1)","009a793b":"train_data = pd.read_csv('Kaggle-digit-train.csv')\n\ntest_data = pd.read_csv('Kaggle-digit-test.csv')","ab1882bc":"# Check the data structure\nprint(train_data.info())\n\n# checking for missing and null values in the given dataset\nprint(train_data.isnull().sum())","925324ac":"# Identifying the counts of distinct labels\ntrain_data['label'].value_counts()","519b0cb5":"# Splitting the Features and Target data for training and testing datasets\nfeatures_train = train_data.drop(['label'],axis = 1)\ntarget_train = pd.DataFrame(train_data['label'])\nfeatures_test = test_data.drop(['label'],axis = 1)","010ac975":"# Plotting histogram for target variable in train_data to see the data distribution.\nplt.hist(target_train)","3c930af4":"# Using train_data train the model with default criterion \"gini\"\ndt = DecisionTreeClassifier()","0dea4c6c":"# importing library to perform k-fold cross validation\nfrom sklearn.model_selection import cross_val_score\n\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\naccuracies_dt = cross_val_score(estimator = dt, X = features_train, y = target_train, cv = 3) \nprint('The mean values of the accuracies is: ',accuracies_dt.mean()) \nprint('-----------------------------------')\nprint('The mean deviation of the accuracies is: ',accuracies_dt.std())","9cd3b03e":"# Using train_data train the model with with criterion \"entropy\"\ndt2 = DecisionTreeClassifier(criterion = \"entropy\")","c9fd9a7a":"# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\naccuracies_dt2 = cross_val_score(estimator = dt2, X = features_train, y = target_train, cv = 3) \nprint('The mean values of the accuracies is: ',accuracies_dt2.mean()) \nprint('-----------------------------------')\nprint('The mean deviation of the accuracies is: ',accuracies_dt2.std())","287ed1ce":"# Using train_data to train the model with criterion \"entropy\", max_depth = 10, max_leaf_nodes = 20, random_state = 42\ndt3 = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 10, max_leaf_nodes = 20, random_state = 42)","9fc6597a":"# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\naccuracies_dt3 = cross_val_score(estimator = dt2, X = features_train, y = target_train, cv = 3) \nprint('The mean values of the accuracies is: ',accuracies_dt3.mean()) \nprint('-----------------------------------')\nprint('The mean deviation of the accuracies is: ',accuracies_dt3.std())","cf3beee6":"dt2.fit(features_train,target_train)\ntest_data_pred = dt2.predict(features_test)\npd.DataFrame(test_data_pred).value_counts()","2006ee81":"# Plotting histogram for predicted data to see the data distribution.\nplt.hist(test_data_pred)","8eee7e10":"# importing library to generate Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\n \n#Calling the Class\nmnb = MultinomialNB(alpha = 0.02)\n\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\naccuracies_mnb = cross_val_score(estimator = mnb, X = features_train, y = target_train, cv = 3) \nprint('The mean values of the accuracies is: ',accuracies_mnb.mean()) \nprint('-----------------------------------')\nprint('The mean deviation of the accuracies is: ',accuracies_mnb.std())","2fc66afc":"mnb.fit(features_train,target_train)\ntest_data_pred2 = mnb.predict(features_test)\npd.DataFrame(test_data_pred2).value_counts()","85babc3c":"# Plotting histogram for predicted data to see the data distribution.\nplt.hist(test_data_pred2)","79ad2e9c":"#tree.plot_tree(dt2,feature_names = features_test.columns, class_names=['0','1','2','3','4','5','6','7','8','9'], filled = True);","9e4bc6f8":"# Naive Bayes","c52315c8":"##### Applying Cross validation to Multinomial Naive Bayes model with 3 folds\n\nWe have used Multinomial Naive Bayes model because our data is large and uniformly distributed; and has multiple classes in the target variable.","bd2d404e":"As we can see our predicted result plot is uniformly distributed and closely resemble to the plot obtained with train data.","a65fb152":"The mean value for the accuracies is ~86% with a mean deviation of ~0.2%. That means our model is accurate for ~86% of time.","c3e4ee34":"##### Applying Cross validation to first decision tree model using criterion entropy with 3 folds","2019bba6":"## Let's build Decision Tree Models","9657ab5b":"The mean value for the accuracies is ~85% with a mean deviation of ~0.06%. That means our model is accurate for ~85% of time.","fdb98e45":"Based on above two classifications we can say that decision tree classifier creates better model because of more accuracy than the Naive Bayes model in solving the classification problem of handwritten digits from 0 to 9.","b8563e83":"The mean value for the accuracies is ~82% with a mean deviation of ~0.2%. That means our model is accurate for ~82% of time.","8109b184":"##### Fitting and predicting the data using Multinomial Naive Bayes model ","72cdd88d":"##### Fitting and predicting the data using Decision Tree model (dt2) with highest accuracy","11c1e940":"As we can see our target data is near to uniform distribution, thus the data is not normally distributed.","da3dec03":"As we can see that there are no null or missing values in the dataset.","86d825ae":"The mean value for the accuracies is ~86% with a mean deviation of ~0.2%. That means our model is accurate for ~86% of time.","70cb4234":"# Importing Libraries","b8e01872":"# Loading Train and Test data","1b2048b1":"##### Applying Cross validation to first decision tree model using criterion entropy , max_depth = 10, max_leaf_nodes = 20, random_state = 42 with 3 folds","ce24dabb":"##### Applying Cross validation to first decision tree model using default hyperparameters with 3 folds","d52c0892":"As we can see our predicted result plot is uniformly distributed and closely resemble to the plot obtained with train data."}}