{"cell_type":{"bd46252d":"code","4b920c9c":"code","5f854156":"code","a16e1e6f":"code","84ca35b2":"code","59055139":"code","779e8e9c":"code","ba1bff92":"code","4d06aa72":"code","c492344d":"code","b9146fa7":"code","aa77b1b4":"code","566d8b99":"code","65cc4521":"code","96019e28":"code","654fdc62":"code","ebe15d86":"code","737e1c7a":"code","d8a582a6":"code","8f9ddc1a":"code","09480471":"code","6ad7c57f":"code","b31d7f17":"code","c8a05773":"code","a11aba63":"code","d80cd876":"code","63c72110":"code","7c7d3507":"code","e1acbac9":"code","3e3195a6":"markdown","68f7284b":"markdown","e099b21c":"markdown","ba2829a4":"markdown","637cf25b":"markdown","428a3864":"markdown","bef3baae":"markdown","69401d98":"markdown","93f55382":"markdown","828f5caf":"markdown","fc4b708a":"markdown","dd25eae2":"markdown","241d02af":"markdown","574cdbe3":"markdown","bea87297":"markdown","7f330d69":"markdown","29d50358":"markdown","ed83f12b":"markdown"},"source":{"bd46252d":"# import libraries\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nfrom scipy.stats import norm ","4b920c9c":"forest_data = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\nforest_test = pd.read_csv(\"..\/input\/learn-together\/test.csv\")","5f854156":"forest_data.info()","a16e1e6f":"forest_data.describe()","84ca35b2":"forest_data.head()","59055139":"forest_data.columns","779e8e9c":"forest_data.dtypes","ba1bff92":"forest_data.shape","4d06aa72":"# yet I didn't use any","c492344d":"#corrMatt = X_train[[\"\",\"\",\"\",\"\",\"\",\"\",\"\"]].corr()\ncorrmat = forest_data[[ 'Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n       'Cover_Type']].corr()\n\nf, ax = plt.subplots(figsize =(11, 10)) \nsns.heatmap(corrmat, annot=True, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1)","b9146fa7":"# Elevation in meters\nsns.distplot(forest_data.Elevation, color=\"b\")","aa77b1b4":"f, axes = plt.subplots(3, 1, figsize=(15, 15), sharex=True, sharey=True)\nsns.scatterplot(y='Hillshade_9am', x='Elevation', \n                 data=forest_data, ax=axes[0])\nsns.scatterplot(y='Hillshade_Noon', x='Elevation', \n                 data=forest_data, ax=axes[1])\nsns.scatterplot(y='Hillshade_3pm', x='Elevation', \n                 data=forest_data, ax=axes[2])","566d8b99":"f, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n\nsns.distplot(forest_data.Hillshade_9am, color=\"y\", ax=axes[0])\nsns.distplot(forest_data.Hillshade_Noon, color=\"b\", ax=axes[1])\nsns.distplot(forest_data.Hillshade_3pm, color=\"g\", ax=axes[2])\n","65cc4521":"f, axes = plt.subplots(2, 1, figsize=(15, 15), sharex=True, sharey=False)\n\nsns.distplot(forest_data.Slope, color=\"y\", ax=axes[0])\nsns.distplot(forest_data.Aspect, color=\"b\", ax=axes[1])\n","96019e28":"f, axes = plt.subplots(2, 1, figsize=(15, 15), sharex=True, sharey=False)\nsns.scatterplot(y='Slope', x='Aspect', \n                 data=forest_data, ax=axes[0])\nsns.scatterplot(y='Aspect', x='Slope', \n                 data=forest_data, ax=axes[1])","654fdc62":"forest_train = forest_data.drop([\"Id\"], axis = 1)\n\nforest_test_id = forest_test[\"Id\"]\nforest_test = forest_test.drop([\"Id\"], axis = 1)","ebe15d86":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# Split X and y in train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(forest_train.drop(['Cover_Type'], axis=1), forest_train['Cover_Type'], test_size=0.2, random_state = 50)\n\n# Define model\nforest_model = RandomForestClassifier(n_estimators=100, random_state=50)\n# Fit the model to train data\nforest_model.fit(X_train, y_train)","737e1c7a":"# Check the model accuracy\nfrom sklearn.metrics import classification_report, accuracy_score\nforest_model.score(X_train, y_train)","d8a582a6":"# Make prediction\nforest_preds = forest_model.predict(X_val)\n\naccuracy_score(y_val, forest_preds)","8f9ddc1a":"# Select features\nfeatures = ['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\n\nforest_data_reduced = forest_data[features]","09480471":"X = forest_data_reduced.copy()\ny = forest_data['Cover_Type']","6ad7c57f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# Split X and y in train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state = 2)\n\n# Define model\nforest_model = RandomForestClassifier(n_estimators=100, random_state=2)\n# Fit the model to train data\nforest_model.fit(X_train, y_train)","b31d7f17":"# Check the model accuracy\nfrom sklearn.metrics import classification_report, accuracy_score\nforest_model.score(X_train, y_train)","c8a05773":"# Make prediction\nforest_preds = forest_model.predict(X_val)\naccuracy_score(y_val, forest_preds)","a11aba63":"# Define a function to calculate accuracy_score\ndef acc_calculate(max_leaf_nodes, X_train, X_val, y_train, y_val):\n    model = RandomForestClassifier(n_estimators=100,max_leaf_nodes=max_leaf_nodes, random_state=50)\n    model.fit(X_train, y_train)\n    val_preds = model.predict(X_val)\n    acc = accuracy_score(y_val, val_preds)\n    return(acc)","d80cd876":"# compare accuracy_score with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000, 10000]:\n    my_acc = acc_calculate(max_leaf_nodes, X_train, X_val, y_train, y_val)\n    print(\"Max leaf nodes: %d  \\t\\t accuracy_score:  %f\" %(max_leaf_nodes, my_acc))","63c72110":"# Run the best model to be used for prediction\nX_train, X_val, y_train, y_val = train_test_split(forest_train.drop(['Cover_Type'], axis=1), forest_train['Cover_Type'], test_size=0.2, random_state = 50)\n\n# Define model\nforest_model = RandomForestClassifier(n_estimators=100, random_state=50)\n# Fit the model to train data\nforest_model.fit(X_train, y_train)","7c7d3507":"test_preds = forest_model.predict(forest_test)","e1acbac9":"# To submit on kaggle\noutput = pd.DataFrame({'Id': forest_test_id,\n                       'Cover_Type': test_preds})\noutput.to_csv('submission.csv', index=False)","3e3195a6":"## Prediction\nUse test data to make prediction.","68f7284b":"Reviewing these the plot, we can identify that there are relations between ( I only considered features with corr > 0.5)\n\n\n*   `Elevtion`  and   `Horizontal_Distance_To_Roadways`\n*   `Aspect` and `Hillshade_3pm`\n\n\n*   `Horizontal_Distance_To_Hydrology` and `Vertical_Distance_To_Hydrology`\n*   `Hillshade_noon` and `Hillshade_3pm`\n\n\n*   `Horizontal_Distance_To_Fire_Points` and `Horizontal_Distance_To_Roadways`\n\n\n\n\n\n","e099b21c":"# Classify Forest Types\nOne of Kaggle competitions. The task is to predict types of trees based on geographic features. The dataset was provided by Jock A. Blackard and Colorado State University.","ba2829a4":"Seperate features and label (Cover_Type) and save them into `X` and `y`.","637cf25b":"Investigave different aspect of data, like data type, missing data, data distribution and etc.\n<br>Using `.info()` we can see that there is no missing data and all data types are `int64`, so we don't need further process to deal with these two.","428a3864":"Now using histogram we investigate distribution of trees based on different features to see what pattern we can find.\n<br> `Soil_Types` and `Wilderness_Area` are categorical variables and shows us absence or presence of each soil type or wilderness area type.","bef3baae":"## References\n*   This [Note Book](https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\/notebook#Distances-analysis) inspired me how to explore each feature more precisely.\n*   This [Course](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning) is a great source to learn about basics of machine learning.","69401d98":"Investigate change in other features with `Elevetion` .","93f55382":"###Correlation Analysis\nThis step is required to investigate possible relation and dependency between different features.","828f5caf":"## Data Preprocessing","fc4b708a":"#### Hillshade\n`Hillshade`: 0 to 255 index, obtains the hypothetical illumination of a surface by determining illumination values for each cell.\n<br>Most of the areas have high sunlight in 9 am and noon (in expected elevation with forest cover). This should be obvious in Hillshade histograms. \n<br> The plot related to 3 pm is distributed normaly but the two other is more pushed toward higher values.","dd25eae2":"# Data Processing\nSimple analysis on data using Random Forest.","241d02af":"#### Elevation","574cdbe3":"#### Aspect and Slope\n`Aspect`: indicates the direction in which the cell's slope faces, measures counterclockwise in degrees from 0 (due north) to 360.","bea87297":"These two show that most of forest cover are concentrated in areas with 0~60 degree slope and are facing toward ( `Aspect` ): North, North-West, West and North-East.","7f330d69":"It shows that must of the forest cover are in elevation between 2050 ~ 3450 m.","29d50358":"Try different numbers of leaf for Random Forest model to find out which gives us better accuracy.","ed83f12b":"### Interactive Visualization"}}