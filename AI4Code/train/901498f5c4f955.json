{"cell_type":{"f7b44d28":"code","ff44a59d":"code","f963437d":"code","b490d2eb":"code","981ff643":"code","71a6a5b5":"code","ca3ce96f":"code","b74ab7e6":"code","ebc63375":"code","073b05d9":"code","7c96ddd8":"code","991d2d3e":"code","d4c939bb":"code","979cc08d":"code","c1e0580e":"code","e83e5703":"code","fc6a0a4d":"code","8f4b02a4":"code","66579b86":"code","349a22de":"code","b078c313":"code","0c160c59":"code","91442bdf":"code","b7df78c4":"code","a6149251":"code","03f96ad7":"code","322ab2b3":"code","b9557deb":"code","251ce4b8":"code","fcca8d6d":"code","f5e4f5e5":"code","4176d51e":"code","2f92eb71":"code","0cb40902":"code","8041342b":"code","250138e7":"code","f8d8f67e":"code","e201cf30":"code","74478580":"code","c476432d":"code","4f1f8787":"code","2a382b4f":"code","6979d482":"code","771d32d6":"code","914931dd":"code","e39015c7":"code","371ca670":"code","44e832f8":"code","608c036b":"code","7ef2a224":"code","e031ec9e":"code","55206cd8":"code","4450f7b1":"code","36e7d72c":"code","59e59e34":"code","eae67048":"code","5f2fc8ed":"code","91ab1db8":"code","02f1b545":"code","8edb5314":"code","fe7b1562":"code","8fb79cb1":"code","865b79cb":"code","9dab3598":"code","45cc4a5d":"code","f5e37fdd":"code","40f3094c":"code","1c36052e":"code","20a3e01a":"code","43aa896f":"code","0d446e0e":"code","4f93c8a7":"code","f02f6746":"code","54dcf83f":"code","f2a585b7":"code","43a598f7":"code","570f8a02":"code","c137fde8":"code","267b039c":"code","aadf225f":"code","7d5d5980":"code","42ccdcd0":"code","d09a6dad":"code","e4642692":"code","37885850":"code","b534cf3d":"code","4f30cb39":"code","de2f9908":"code","b0cc20a6":"code","49672fe0":"code","6352bbaf":"code","fbc4377b":"code","9e201cd2":"code","d4c8917b":"code","852ccf9e":"code","fe9cea71":"code","c111770f":"code","e81ef849":"code","9471914b":"code","c84679be":"code","f8dbb4ce":"code","9f647def":"code","86910dfa":"code","60106fdd":"code","73ed97f3":"code","71b7d6d7":"code","8f46611a":"code","402904ef":"code","1f2f60dd":"code","755430dd":"code","065aa4c2":"code","7d260416":"code","764ae7ba":"code","f4e5ce3b":"code","458abf2c":"code","a9c78125":"code","8be3daeb":"code","ba469546":"code","12024bc8":"code","e96377bc":"code","a4d3e776":"code","631b4278":"code","da13186c":"code","05813ab8":"code","fe6868e5":"code","49c063bd":"code","ddb24c46":"code","94a6188e":"code","2c6aae71":"code","faf7786e":"code","f7e964d6":"code","469cd8a2":"code","65d7702d":"code","9e1dfa33":"code","83e1290d":"code","de590e98":"code","54d1c4c1":"code","234d4773":"code","89e0c7b7":"code","7ce01320":"code","48b4e566":"code","f741aec9":"code","dc78260a":"code","a869818d":"code","fce47a74":"code","32dff198":"markdown","167194ab":"markdown","666c748a":"markdown","eed66127":"markdown","afd450f9":"markdown","5a66ed64":"markdown","3da05b44":"markdown","cc7c5807":"markdown","617178fd":"markdown","d85581e1":"markdown","c14242e2":"markdown","f8b3ca8c":"markdown","db1a5457":"markdown","7a183713":"markdown","506a2804":"markdown","edfa1090":"markdown","e14b13ec":"markdown","b7ff6f42":"markdown","d4bb7840":"markdown","2b17b68a":"markdown","d6bb279c":"markdown","7ac2f654":"markdown","89e8cad7":"markdown","2df8ade9":"markdown","4faeba4b":"markdown","9896a410":"markdown","ea6ba481":"markdown","7688fe1f":"markdown","4a21ba56":"markdown","c966153c":"markdown","608d4670":"markdown","e4db2ab5":"markdown","3e5bd431":"markdown","21d5276a":"markdown","6e382698":"markdown","e1b764ee":"markdown","0497e8fd":"markdown","926dff40":"markdown"},"source":{"f7b44d28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport os \nimport pandas as pd \nimport seaborn as sns","ff44a59d":"df = pd.read_csv('\/kaggle\/input\/add-data\/Ascendeum_Dataset2.csv')","f963437d":"df.head()","b490d2eb":"df[df['advertiser_id']==84].total_revenue","981ff643":"df.count()","71a6a5b5":"df.describe()","ca3ce96f":"df.info()","b74ab7e6":"df.drop(['order_id' , 'line_item_type_id'], axis = 1, inplace=True)","ebc63375":"df.head()","073b05d9":"corr = df.corr()\nplt.figure(figsize=(15,12))\nsns.heatmap(data=corr,vmin=0, vmax=1,  square=True)\nplt.show()","7c96ddd8":"#we can remove integration type as it has only one value and revenue share percent as that we have already used and \n#is only one single value as well\ndf.drop(['integration_type_id'], axis = 1, inplace=True)","991d2d3e":"corr = df.corr()\nplt.figure(figsize=(15,12))\nsns.heatmap(data=corr,vmin=0, vmax=1,  square=True)\nplt.show()","d4c939bb":"df['total_revenue'].sum()","979cc08d":"#calculating CPM\n#calculating the value that the Advertisers Bid for the month of June\n# CPM(the value which was the winning bid value) = \n#((revenue of the publisher*100)\/revenue_share_percentage)\/measurable_impressions)*1000\n\ndef division_1(a, b):\n    return a \/ b if b else 0\n\ndf['CPM'] = df.apply(lambda x: division_1(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\n","c1e0580e":"df[df['CPM']>0].groupby(['site_id']).agg({'site_id':'count'})","e83e5703":"import pandasql\nimport seaborn as sns\n","fc6a0a4d":"sub_data1= pandasql.sqldf(\"SELECT avg(CPM) as CPM_AVG ,date FROM df group by date  ;\", globals())\nsub_data1.head()","8f4b02a4":"sub_data1['CPM_AVG'].mean()","66579b86":"df['CPM'].mean()","349a22de":"sub_data1","b078c313":"df1=pd.merge(df, sub_data, on='date', how='left')","0c160c59":"df1.head()","91442bdf":"df1.plot(x='date', y='CPM_AVG')\n","b7df78c4":"df.plot(x='date', y='total_revenue')\n","a6149251":"sns.barplot(x = 'date', y = 'CPM_AVG', data = df1)\nplt.show()","03f96ad7":"rider = df1[['CPM_AVG']]","322ab2b3":"rider.rolling(6).mean().plot(figsize=(20,10), linewidth=5, fontsize=20)\nplt.show()","b9557deb":"#season check","251ce4b8":"rider.diff(periods=4).plot(figsize=(20,10), linewidth=5, fontsize=20)\nplt.show()","fcca8d6d":"pd.plotting.lag_plot(df1['CPM_AVG'])\nplt.show()","f5e4f5e5":"df1 = df1.set_index('date')","4176d51e":"sub_data1.head()","2f92eb71":"data=sub_data1.copy()","0cb40902":"import seaborn as sns\nfig = plt.subplots(figsize=(12, 2))\nax = sns.boxplot(x=data['CPM_AVG'],whis=1.5)","8041342b":"fig = data.CPM_AVG.hist(figsize = (12,4))","250138e7":"data['date'] = pd.to_datetime(data['date'])","f8d8f67e":"data.dtypes","e201cf30":"data.set_index(data['date'],inplace=True)","74478580":"data.drop('date',inplace=True,axis=1)","c476432d":"data.head()","4f1f8787":"from pylab import rcParams\nimport statsmodels.api as sm\nrcParams['figure.figsize'] = 12, 8\ndecomposition = sm.tsa.seasonal_decompose(data.CPM_AVG, model='additive') # additive seasonal index\nfig = decomposition.plot()\nplt.show()","2a382b4f":"decomposition = sm.tsa.seasonal_decompose(data.CPM_AVG, model='multiplicative') # multiplicative seasonal index\nfig = decomposition.plot()\nplt.show()","6979d482":"train_len = 21\ntrain = data[0:train_len] # first 21 days as training set\ntest = data[train_len:] # last 9 days as out-of-time test set","771d32d6":"y_hat_naive = test.copy()\ny_hat_naive['naive_forecast'] = train['CPM_AVG'][train_len-1]\n","914931dd":"plt.figure(figsize=(12,4))\nplt.plot(train['CPM_AVG'], label='Train')\nplt.plot(test['CPM_AVG'], label='Test')\nplt.plot(y_hat_naive['naive_forecast'], label='Naive forecast')\nplt.legend(loc='best')\nplt.title('Naive Method')\nplt.show()","e39015c7":"from sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_naive['naive_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_naive['naive_forecast'])\/test['CPM_AVG'])*100,2)\n\nresults = pd.DataFrame({'Method':['Naive method'], 'MAPE': [mape], 'RMSE': [rmse]})\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","371ca670":"y_hat_avg = test.copy()\ny_hat_avg['avg_forecast'] = train['CPM_AVG'].mean()","44e832f8":"plt.figure(figsize=(12,4))\nplt.plot(train['CPM_AVG'], label='Train')\nplt.plot(test['CPM_AVG'], label='Test')\nplt.plot(y_hat_avg['avg_forecast'], label='Simple average forecast')\nplt.legend(loc='best')\nplt.title('Simple Average Method')\nplt.show()","608c036b":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_avg['avg_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_avg['avg_forecast'])\/test['CPM_AVG'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple average method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","7ef2a224":"y_hat_sma = data.copy()\nma_window = 4\ny_hat_sma['sma_forecast'] = data['CPM_AVG'].rolling(ma_window).mean()\ny_hat_sma['sma_forecast'][train_len:] = y_hat_sma['sma_forecast'][train_len-1]","e031ec9e":" y_hat_sma['sma_forecast']","55206cd8":"plt.figure(figsize=(12,4))\nplt.plot(train['CPM_AVG'], label='Train')\nplt.plot(test['CPM_AVG'], label='Test')\nplt.plot(y_hat_sma['sma_forecast'], label='Simple moving average forecast')\nplt.legend(loc='best')\nplt.title('Simple Moving Average Method')\nplt.show()","4450f7b1":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_sma['sma_forecast'][train_len:])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_sma['sma_forecast'][train_len:])\/test['CPM_AVG'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple moving average forecast'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","36e7d72c":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\nmodel = SimpleExpSmoothing(data['CPM_AVG'])\nmodel_fit = model.fit(smoothing_level=0.3,optimized=True)\n#model_fit.params\n#y_hat_ses = test.copy()\n","59e59e34":"model_fit.forecast(7)","eae67048":"plt.figure(figsize=(18,14))\nplt.plot(train['CPM_AVG'], label='Train')\nplt.plot(test['CPM_AVG'], label='Test')\nplt.plot(model_fit.forecast(7), label='Simple exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Simple Exponential Smoothing Method')\nplt.show()","5f2fc8ed":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_ses['ses_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_ses['ses_forecast'])\/test['CPM_AVG'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple exponential smoothing forecast'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults","91ab1db8":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\nmodel = ExponentialSmoothing(np.asarray(train['CPM_AVG']) ,seasonal_periods=3 ,trend='additive', seasonal=None)\nmodel_fit = model.fit(smoothing_level=0.2, smoothing_slope=0.01, optimized=False)\nprint(model_fit.params)\ny_hat_holt = test.copy()\ny_hat_holt['holt_forecast'] = model_fit.forecast(len(test))","02f1b545":"plt.figure(figsize=(12,4))\nplt.plot( train['CPM_AVG'], label='Train')\nplt.plot(test['CPM_AVG'], label='Test')\nplt.plot(y_hat_holt['holt_forecast'], label='Holt\\'s exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Holt\\'s Exponential Smoothing Method')\nplt.show()","8edb5314":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_holt['holt_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_holt['holt_forecast'])\/test['CPM_AVG'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt\\'s exponential smoothing method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","fe7b1562":"d=data.copy()","8fb79cb1":"y_hat_hwa = test.copy()\nmodel = ExponentialSmoothing(np.asarray(data['CPM_AVG']) ,seasonal_periods=3 ,trend='add', seasonal='add')\nmodel_fit = model.fit(optimized=True)\n#print(model_fit.params)\n#y_hat_hwa['hw_forecast'] = model_fit.forecast(len(test))","865b79cb":"pd.DataFrame(model_fit.forecast(31))[0]","9dab3598":"future_dates=[d.index[-1]+ DateOffset(days=x)for x in range(32)]\nfuture_dates=pd.DataFrame(index=future_dates[1:],columns=d.columns)\n","45cc4a5d":"future_dates['CPM_AVG']=model_fit.forecast(31)","f5e37fdd":"final_data=pd.concat([d,future_dates])","40f3094c":"final_data","1c36052e":"final_df=final_data.reset_index()","20a3e01a":"final_df.columns","43aa896f":"plt.figure(figsize=(12,4))\nplt.plot( final_df[final_df['index']<='2019-06-30'].CPM_AVG, label='June Data')\nplt.plot( final_df[final_df['index']>='2019-06-30'].CPM_AVG, label='June Data')\nplt.plot( final_df[final_df['index']>='2019-06-30'].CPM_AVG-final_df[final_df['index']>='2019-06-30'].CPM_AVG*.20, label=' Data')\nplt.plot( final_df[final_df['index']>='2019-06-30'].CPM_AVG+final_df[final_df['index']>='2019-06-30'].CPM_AVG*.20, label='June Data')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Additive Method')\nplt.show()","0d446e0e":"plt.figure(figsize=(12,4))\nplt.plot( train['CPM_AVG'], label='Train')\nplt.plot(test['CPM_AVG'], label='Test')\nplt.plot(y_hat_hwa['hw_forecast'], label='Holt Winters\\'s additive forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Additive Method')\nplt.show()","4f93c8a7":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_hwa['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_hwa['hw_forecast'])\/test['CPM_AVG'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' additive method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","f02f6746":"y_hat_hwm = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train['CPM_AVG']) ,seasonal_periods=3 ,trend='add', seasonal='mul')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\ny_hat_hwm['hw_forecast'] = model_fit.forecast(len(test))","54dcf83f":"plt.figure(figsize=(12,4))\nplt.plot( train['CPM_AVG'], label='Train')\nplt.plot(test['CPM_AVG'], label='Test')\nplt.plot(y_hat_hwm['hw_forecast'], label='Holt Winters\\'s mulitplicative forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Mulitplicative Method')\nplt.show()","f2a585b7":"RMSE","43a598f7":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_hwm['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_hwm['hw_forecast'])\/test['CPM_AVG'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' multiplicative method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","570f8a02":"# Plot the axis\ndata.plot(figsize=(12,4))\nplt.legend(loc='best')\nplt.title(\"Time Vs CPM\")\nplt.show()","c137fde8":"train_len = 21\ntrain = data[0:train_len] # first 21 days as training set\ntest = data[train_len:] # last 9 days as out-of-time test set","267b039c":"future_dates=[test.index[-1]+ DateOffset(days=x)for x in range(30)]\nfuture_dates=pd.DataFrame(index=future_dates[1:],columns=test.columns)\n","aadf225f":"test=future_datest_df.append(future_datest_df)","7d5d5980":"test.tail()","42ccdcd0":"# Naive Method\ny_hat_naive = test.copy()\ny_hat_naive['forecast'] = train['CPM_AVG'][train_len-1]","d09a6dad":"# Plot the forecast\nplt.figure(figsize=(12,8))\nplt.plot(train['CPM_AVG'], label= 'Train')\nplt.plot(test['CPM_AVG'], label= 'Test')\nplt.plot(y_hat_naive['forecast'], label= 'Naive Forecast')\nplt.legend(loc='best')\nplt.title(\"Naive Method\")\nplt.show()","e4642692":"# Calculate RMSE and MAPE to evaluate forecast\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_naive['forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_naive['forecast'])\/test['CPM_AVG'])*100, 2)\n                \nresults = pd.DataFrame({'method':['Naive Method'], 'RMSE':[rmse], 'MAPE':[mape]})\nresults = results[['method','RMSE', \"MAPE\"]]  \nresults","37885850":"\nmodel = ExponentialSmoothing(np.array(train['CPM_AVG']), seasonal_periods=3, trend = 'additive', seasonal='additive')\nmodel_fit = model.fit(optimized =True)\nprint(model_fit.params)\ny_hat_holts = test.copy()\ny_hat_holts['forecast'] = model_fit.forecast(len(test))","b534cf3d":"# Plot the forecast\nplt.figure(figsize=(12,8))\nplt.plot(train['CPM_AVG'], label= 'Train')\nplt.plot(test['CPM_AVG'], label= 'Test')\nplt.plot(y_hat_holts['forecast'], label= 'Exponential Smoothing Method Forecast')\nplt.legend(loc='best')\nplt.title(\" Exponential Smoothing Method Forecast- Level ,Trend & Seasonality\")\nplt.show()","4f30cb39":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_holts['forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_holts['forecast'])\/test['CPM_AVG'])*100, 2)\nTempresults = pd.DataFrame({'method':['Exponential Smoothing  Method Level ,Trend & Sesnality'], 'RMSE':[rmse], 'MAPE':[mape]})\nTempresults = Tempresults[['method','RMSE', \"MAPE\"]]  \nTempresults","de2f9908":"# Auto Regressive Models\n# 1. Stationary & Non stationary time series\n","b0cc20a6":"# Plot the axis\ndata.plot(figsize=(12,4))\nplt.legend(loc='best')\nplt.title(\"Time Vs CPM AVG\")\nplt.show()","49672fe0":"# Augmented -Dickey Fuller (ADF) test\nfrom statsmodels.tsa.stattools import adfuller","6352bbaf":"adf_test = adfuller(data['CPM_AVG'])\n\nprint(\"Statistics\", adf_test[0])\nprint(\"Critical Value @ 0.05%: \",adf_test[4]['5%'])\nprint(\"p value \",adf_test[1])","fbc4377b":"adf_test","9e201cd2":"# KPSS test \nfrom statsmodels.tsa.stattools import kpss","d4c8917b":"kpss_test = kpss(data['CPM_AVG'])\n\nprint(\"Statistics\", kpss_test[0])\nprint(\"Critical Value @ 0.05%: \",kpss_test[3]['5%'])\nprint(\"p value \",kpss_test[1])","852ccf9e":"# Non Stationary to Stationary Series\n# 1. Differencing \n#  2. Transforming - BOx Cox Transformation from the family of power transformer indexed by parameter lambda\n # It helps in making variance constant\n\n #   formulat of box cox transformation below","fe9cea71":"#Box COx Transformation\nfrom scipy.stats import boxcox","c111770f":"boxcox_data = pd.Series(boxcox(data['CPM_AVG'], lmbda =0), index = data.index)","e81ef849":"#data.plot(figsize=(12,4),label=\"bebore\")\nboxcox_data.plot(figsize=(12,4),label=\"before\")\nplt.legend(loc='best')\nplt.title(\"Aster Box Cox Transformation\")\nplt.show()","9471914b":"# Differencing\ndata_boxcox_diff = pd.Series(boxcox_data - boxcox_data.shift()-7, index= data.index)\ndata_boxcox_diff.dropna(inplace= True)","c84679be":"data_boxcox_diff.plot(figsize=(12,4), label = \"After Defrentiating\")\nplt.legend(loc='best')\nplt.title(\"After Differntiaing \")\nplt.show()","f8dbb4ce":"## ADF TEst\n\nadf_test = adfuller(data_boxcox_diff)\n\nprint(\"Statistics\", adf_test[0])\nprint(\"Critical Value @ 0.05%: \",adf_test[4]['5%'])\nprint(\"p value \",adf_test[1])","9f647def":"# KPSS test\n\nkpss_test = kpss(data_boxcox_diff)\n\nprint(\"Statistics\", kpss_test[0])\nprint(\"Critical Value @ 0.05%: \",kpss_test[3]['5%'])\nprint(\"p value \",kpss_test[1])","86910dfa":"# Autocorrelation Function plot --> direct and indrect relationship\nfrom statsmodels.graphics.tsaplots import plot_acf","60106fdd":"plt.figure(figsize=(12,6))\nacf = plot_acf(data_boxcox_diff, ax =plt.gca(), lags= 3)\nplt.show()","73ed97f3":"# Partial Autocorrelation Function PACF --> only direct relationship \nfrom statsmodels.graphics.tsaplots import plot_pacf","71b7d6d7":"plt.figure(figsize=(12,6))\npacf = plot_pacf(data_boxcox_diff, ax =plt.gca(), lags= 3)\nplt.show()","8f46611a":"# Simple Auto Regressive model(AR)\n# The simple autoregressive model forecasts the dependent variable (future observation)\n#when one or more independent variables are known (past observations). \n#This model has a parameter \u2018p\u2019 called lag order. \n#Lag order is the maximum number of lags used to build \u2018p\u2019 number of past data points to predict future data points. ","402904ef":"# Train test split\ntrain_data_boxcox_diff = data_boxcox_diff[:train_len-1]\ntest_data_boxcox_diff = data_boxcox_diff[train_len-1:]\n\ntrain_boxcox_data = boxcox_data[:train_len]\ntest_boxcox_data = boxcox_data[train_len:]","1f2f60dd":"# Auto Regression Method \nfrom statsmodels.tsa.arima_model import ARIMA","755430dd":"model = ARIMA(train_data_boxcox_diff, order=(1,0,0))\nmodel_fit = model.fit()\nprint(model_fit.params)","065aa4c2":"# Recover original time series\ny_hat_ar = data_boxcox_diff.copy()\ny_hat_ar['ar_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_ar['ar_forecast_boxcox'] = y_hat_ar['ar_forecast_boxcox_diff'].cumsum()\ny_hat_ar['ar_forecast_boxcox']  = y_hat_ar['ar_forecast_boxcox'].add(boxcox_data[0])\n\ny_hat_ar['ar_forecast'] = np.exp(y_hat_ar['ar_forecast_boxcox'])","7d260416":"# Plot the forecast\nplt.figure(figsize=(12,8))\nplt.plot(train['CPM_AVG'], label= 'Train')\nplt.plot(test['CPM_AVG'], label= 'Test')\nplt.plot(y_hat_ar['ar_forecast'][test.index.min():], label= ' Auto regressive method')\nplt.legend(loc='best')\nplt.title(\"Auto Regressive method\")\nplt.show()","764ae7ba":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_ar['ar_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_ar['ar_forecast'][test.index.min():])\/test['CPM_AVG'])*100, 2)\nTempresults = pd.DataFrame({'method':['AutoRegressive Method'], 'RMSE':[rmse], 'MAPE':[mape]})\nTempresults = Tempresults[['method','RMSE', \"MAPE\"]]  \nTempresults","f4e5ce3b":"# Moving Average Model (MA)\n#The Moving Average Model models the future forecasts using past forecast errors in a regression-like model. \n#This model has a parameter \u2018q\u2019 called window size, over which linear combination of errors are calculated.","458abf2c":"model = ARIMA(train_data_boxcox_diff, order=(0,0,1))\nmodel_fit = model.fit()\nprint(model_fit.params)","a9c78125":"# Recover original time series\ny_hat_ma = data_boxcox_diff.copy()\ny_hat_ma['ma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_ma['ma_forecast_boxcox'] = y_hat_ma['ma_forecast_boxcox_diff'].cumsum()\ny_hat_ma['ma_forecast_boxcox']  = y_hat_ma['ma_forecast_boxcox'].add(boxcox_data[0])\n\ny_hat_ma['ma_forecast'] = np.exp(y_hat_ma['ma_forecast_boxcox'])","8be3daeb":"# Plot the forecast\nplt.figure(figsize=(12,8))\nplt.plot(train['CPM_AVG'], label= 'Train')\nplt.plot(test['CPM_AVG'], label= 'Test')\nplt.plot(y_hat_ma['ma_forecast'][test.index.min():], label= ' Moving Average method')\nplt.legend(loc='best')\nplt.title(\"Moving Average method\")\nplt.show()","ba469546":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_ma['ma_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_ma['ma_forecast'][test.index.min():])\/test['CPM_AVG'])*100, 2)\nTempresults = pd.DataFrame({'method':['Moving Average Method'], 'RMSE':[rmse], 'MAPE':[mape]})\nTempresults = Tempresults[['method','RMSE', \"MAPE\"]]  \nTempresults","12024bc8":"model = ARIMA(train_data_boxcox_diff, order=(1,0,1))\nmodel_fit = model.fit()\nprint(model_fit.params)","e96377bc":"# Recover original time series\ny_hat_arma = data_boxcox_diff.copy()\ny_hat_arma['arma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_arma['arma_forecast_boxcox'] = y_hat_arma['arma_forecast_boxcox_diff'].cumsum()\ny_hat_arma['arma_forecast_boxcox']  = y_hat_arma['arma_forecast_boxcox'].add(boxcox_data[0])\n\ny_hat_arma['arma_forecast'] = np.exp(y_hat_arma['arma_forecast_boxcox'])","a4d3e776":"# Plot the forecast\nplt.figure(figsize=(12,8))\nplt.plot(train['CPM_AVG'], label= 'Train')\nplt.plot(test['CPM_AVG'], label= 'Test')\nplt.plot(y_hat_arma['arma_forecast'][test.index.min():], label= ' Auto Regressive -Moving Average(ARMA) method')\nplt.legend(loc='best')\nplt.title(\"Auto Regressive Moving Average(ARMA) method\")\nplt.show()","631b4278":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_arma['arma_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_arma['arma_forecast'][test.index.min():])\/test['CPM_AVG'])*100, 2)\nTempresults = pd.DataFrame({'method':['Auto Regressive Moving Average Method'], 'RMSE':[rmse], 'MAPE':[mape]})\nTempresults = Tempresults[['method','RMSE', \"MAPE\"]]  \nTempresults","da13186c":"# ARIMA \n\n#Steps of ARIMA model\n\n#Original time series is differenced to make it stationary\n#Differenced series is modeled as a linear regression of\n#One or more past observations\n#Past forecast errors\n#ARIMA model has three parameters\n#p: Highest lag included in the regression model\n#d: Degree of differencing to make the series stationary\n#q: Number of past error terms included in the regression model\n#Here the new parameter introduced is the \u2018I\u2019 part called integrated. It removes the trend non-stationarity and later integrates the trend to the original series.","05813ab8":"model = ARIMA(train_boxcox_data, order=(1,1,1))\nmodel_fit = model.fit()\nprint(model_fit.params)","fe6868e5":"# Recover original time series\ny_hat_arima = train_data_boxcox_diff.copy()\ny_hat_arima['arima_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_arima['arima_forecast_boxcox'] = y_hat_arima['arima_forecast_boxcox_diff'].cumsum()\ny_hat_arima['arima_forecast_boxcox']  = y_hat_arima['arima_forecast_boxcox'].add(boxcox_data[0])\n\ny_hat_arima['arima_forecast'] = np.exp(y_hat_arima['arima_forecast_boxcox'])","49c063bd":"# Plot the forecast\nplt.figure(figsize=(12,8))\nplt.plot(train['CPM_AVG'], label= 'Train')\nplt.plot(test['CPM_AVG'], label= 'Test')\nplt.plot(y_hat_arima['arima_forecast'][test.index.min():], label= ' Auto Regressive Integrated Moving Average(ARMA) method')\nplt.legend(loc='best')\nplt.title(\"Auto Regressive Integrated Moving Average(ARIMA) method\")\nplt.show()","ddb24c46":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_arima['arima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_arima['arima_forecast'][test.index.min():])\/test['CPM_AVG'])*100, 2)\nTempresults = pd.DataFrame({'method':['Auto Regressive Integrated Integrated Moving Average Method'], 'RMSE':[rmse], 'MAPE':[mape]})\nTempresults = Tempresults[['method','RMSE', \"MAPE\"]]  \nTempresults","94a6188e":"# SARIMA - Seasonal Auto Regressive Integrated Moving Average\n#SARIMA brings all the features of an ARIMA model with an extra feature, seasonality. \n \n#The non-seasonal elements of SARIMA\n\n#Time series is differenced to make it stationary.\n#Models future observation as linear regression of past observations and past forecast errors.\n#The seasonal elements of SARIMA\n\n#Perform seasonal differencing on time series.\n#Model future seasonality as linear regression of past observations of seasonality and past forecast errors of seasonality.","2c6aae71":"from statsmodels.tsa.statespace.sarimax import SARIMAX","faf7786e":"model = SARIMAX(train_boxcox_data, order=(1,1,1), seasonal_order=(1,1,1,12))\nmodel_fit = model.fit()\nprint(model_fit.params)","f7e964d6":"# Recover original time series\ny_hat_sarima = train_data_boxcox_diff.copy()\ny_hat_sarima['sarima_forecast_boxcox'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\n#y_hat_sarima['sarima_forecast_boxcox'] = y_hat_sarima['sarima_forecast_boxcox'].cumsum()\n#y_hat_sarima['sarima_forecast_boxcox']  = y_hat_sarima['sarima_forecast_boxcox'].add(boxcox_data[0])\n\ny_hat_sarima['sarima_forecast'] = np.exp(y_hat_sarima['sarima_forecast_boxcox'])","469cd8a2":"# Plot the forecast\nplt.figure(figsize=(12,8))\nplt.plot(train['CPM_AVG'], label= 'Train')\nplt.plot(test['CPM_AVG'], label= 'Test')\nplt.plot(y_hat_sarima['sarima_forecast'][test.index.min():], label= ' Seasonal Auto Regressive Integrated Moving Average(ARMA) method')\nplt.legend(loc='best')\nplt.title(\" Seasonal Auto Regressive Integrated Moving Average(ARIMA) method\")\nplt.show()","65d7702d":"rmse = np.sqrt(mean_squared_error(test['CPM_AVG'], y_hat_sarima['sarima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['CPM_AVG']-y_hat_sarima['sarima_forecast'][test.index.min():])\/test['CPM_AVG'])*100, 2)\nTempresults = pd.DataFrame({'method':['Seasonal Auto Regressive Integrated Integrated Moving Average Method'], 'RMSE':[rmse], 'MAPE':[mape]})\nTempresults = Tempresults[['method','RMSE', \"MAPE\"]]  \nTempresults","9e1dfa33":"\nfrom pandas.tseries.offsets import DateOffset\n","83e1290d":"from pandas.tseries.offsets import DateOffset\nts = pd.Timestamp('2017-01-01 09:10:11')\nts + DateOffset(days=3)\n","de590e98":"data.tail()","54d1c4c1":"future_dates=[data.index[-1]+ DateOffset(days=x)for x in range(30)]","234d4773":"future_datest_df=pd.DataFrame(index=future_dates[1:],columns=data.columns)\n","89e0c7b7":"\nfuture_df=pd.concat([data,future_datest_df])\n","7ce01320":"future_df.tail()","48b4e566":"\nfuture_df['forecast'] = results.predict(start = 104, end = 120, dynamic= True)  \nfuture_df[['Sales', 'forecast']].plot(figsize=(12, 8))","f741aec9":"future_datest_df.tail()","dc78260a":"final_df[final_df['index']>='2019-07-01'].sum()","a869818d":"final_df[final_df['index']>='2019-07-01'].mean()","fce47a74":"final_df","32dff198":"Plotting","167194ab":"# Holt's method with trend","666c748a":"*  Publisher \u2013person who owns and publishescontent on the website\n    \n*  Inventory\u2013all the users that visit the website * all the ad slots present in the website for the observation period\n    \n*  Impressions -showing an ad to a user constitutes one impression. If the ad slotis present but an ad is not shown, it falls as \u201cunfilled impression\u201d. Inventory is the sum of impressions + unfilled impressions.\n    \n*  CPM \u2013cost per Mille. This is one of the most important ways to measure performance. It is. Calculated as revenue\/impressions * 1000. 'bids' and 'price' are measured in terms of CPM","eed66127":"# Data analysis and Preprocessing","afd450f9":"# INTRODUCTION","5a66ed64":"# Simple moving average forecast method","3da05b44":"# Naive method","cc7c5807":"# Box plot and interquartile range","617178fd":"1.What is the potential revenuerangeour publisher can make in July? \n\n2.What is the reserve prices that he\/she can set ? ","d85581e1":"### Plot train, test and forecast","c14242e2":"# PROBLEM STATEMENT","f8b3ca8c":"### Plot train, test and forecast","db1a5457":"\n### Multiplicative seasonal decomposition[](http:\/\/)","7a183713":"# Time series Decomposition","506a2804":"Dropping Can be ignored Items","edfa1090":"RMSE","e14b13ec":"# QUESTIONS","b7ff6f42":"# Simple average method","d4bb7840":"The dataset provided to you has data for several websites owned by the same company and they are asking for yourhelp for what should be their approach to set reserve prices and what is the range for reserve prices they should be setting for July. The data is only of the actual revenue generation and not at bid level. The dataset has the following columns:\n\n1.Date\n\n2.site_id : each id denotes a different website\n\n3.ad_type_id : each id denotes a different ad_type. These can be display ads , video ads, text adsetc\n\n4.geo_id : each id denotes a different country. our maximum traffic is from english speaking countries\n\n5.device_category_id : each id denoted a different device_category like desktop , mobile, tablet\n\n6.advertiser_id: each id denotes a different bidder in the auction \n\n7.order_id : can be ignored\n\n8.line_item_type_id : can be ignored\n\n9.os_id : each id denotes a different operating system for mobile device category only (android , ios etc) . for all other device categories, os_id will correspond to not_mobile\n\n10.integration_type_id : it describes how the demand partner is setup within a publisher's ecosystem -can be adserver (running through the publisher adserver) or hardcoded\n\n11.monetization_channel_id : it describes the mode through which demand partner integrates with aparticular publisher -it can be header bidding (running via prebid.js), dynamic allocation, exchange bidding, direct etc\n\n12.ad_unit_id -each id denotes a different ad unit (one page can have more than one ad units)\n\n13.total_impressions -measurement column measuring the impressions for the particular set of dimensions\n\n14.total_revenue -measurement column measuring the revenue for the particular set of dimensions\n\n15.viewable_impressions-Number of impressions on the site that were viewable out of all measurable impressions. A display ad is counted as viewable if at least 50% of its area was displayed on screen for at least one second \n\n16.measurable_impressions -Impressions that were measurable by Active View out of the total number of eligible impressions. This value should generally be close to 100%. For example, an impression that is rendering in a cross-domain iframe may not be measurable.\n\n17.Revenue_share_percent -not every advertiser gives all the revenue to the publisher. They charge a certain share for the services they provide. This captures the fraction of revenue that will actually reach the publishers pocket.","2b17b68a":"Plot train, test and forecast","d6bb279c":"# DATASET:","7ac2f654":"# Important Terms","89e8cad7":"Split time series data into training and test set","2df8ade9":"Simple exponential smoothing","4faeba4b":"2.What is the reserve prices that he\/she can set ? ","9896a410":"# Holt Winter's multiplicative method with trend and seasonality","ea6ba481":"# Histogram","7688fe1f":"# Simple time series methods","4a21ba56":"60% of the digital ad inventory is sold by publishers in Real Timefirst priceAuctions. \n\nOnce a user lands on a webpage, bidders (advertisers)bid fordifferent ad slots on the page and the one with the highest winning bid displays their ad in the ad space and pays the amount he bid. This process encourages bid shading \u2013biddinglesser than the perceived valueof the ad space to maximize utilization for self whilemaintaininga particular win rate at lowest prices.\n\nHence, for publishers, it becomes important to value their inventory (all the users that visit their website * all the adslots they have on their websites) correctly so that a reserve price, or a minimum price can be set up for the auctions.The minimum price ","c966153c":"Plot train, test and forecast","608d4670":"RSME","e4db2ab5":"1.What is the potential revenuerangeour publisher can make in July? \n\n","3e5bd431":"# Exponential smoothing methods","21d5276a":"RMSE","6e382698":"In a first price auction, the highest bidder wins and pays the price they bid if it exceeds the reserve price. The optimal strategy of a bidder is to shade their bids (bid less than their true value of the inventory). However, bidder needs to win a certain amount to achieve their goals. This suggests they need to shade as much possible while maintaining a certain win rate.\n\nA bidder perceives a certain value out of every impression they win. Each bidder would like to maintain the value they derived out of this set of websites (given in the dataset) in June with a maximum deviation of 20%.\n\nSetting a reserve price induces this by causing bidders to lose at lower bids which encourages higher biddingand more publisher revenue. However, since most of these takes place through automated systems, theremight be an unknown delay in setting reserve prices & reducing win rate of bidder & bidder changing their bid shading algorithm & increased publisher revenue. ","e1b764ee":"Exponential Smooting Forecasting Method - Level , Trend & Seasonality","0497e8fd":"# Calculate RMSE and MAPE","926dff40":"# HOlt Winters' additive method with trend and seasonality"}}