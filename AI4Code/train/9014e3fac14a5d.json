{"cell_type":{"8527747e":"code","82f72af1":"code","023cbc89":"code","911a99c9":"code","9564461a":"code","b2f46fcc":"code","e6eb2e82":"code","b7b35200":"code","04fe6034":"code","f8c19c5a":"code","f4d0e3df":"code","5c5bc584":"code","3c1dc936":"code","695f5b4f":"code","c2606a58":"code","3b9a2b13":"code","0dbe4d1a":"code","35c12a27":"code","53e41ca7":"code","05b0f427":"code","90ce5ede":"code","19167bc6":"code","1bf53335":"code","e8049daf":"code","581b8a88":"code","0ef04d56":"code","6bff6a68":"code","66650f31":"code","acf6f23c":"code","dc0f5820":"code","c309c6ce":"markdown","0579d82d":"markdown","2aef72cd":"markdown","4770f1bf":"markdown","729cee69":"markdown","7f794384":"markdown","b09267e0":"markdown","ce8a0327":"markdown","b0fb58e4":"markdown","63450149":"markdown","3516f416":"markdown","945da61f":"markdown","a8501807":"markdown","7c53d2ee":"markdown","094df59b":"markdown","6542d124":"markdown","9ca305e4":"markdown","b4360184":"markdown","1fc23de4":"markdown","e69d4c72":"markdown","7b705f9a":"markdown","93a4077e":"markdown","4cf1ef39":"markdown","b8a1ea5b":"markdown"},"source":{"8527747e":"!pip install praat-textgrids","82f72af1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory","023cbc89":"import textgrids\n\nFRAME_DURATION = 30 # 30 msec\nOVERLAP_RATE = 0.0 # frames don't overlap\n\ndef readFile(path):\n    '''\n    Read the file and return the list of SPEECH\/NONSPEECH labels for each frame\n    '''\n        \n    labeled_list  = []\n    grid = textgrids.TextGrid(path)\n\n    for interval in grid['silences']:\n        label = int(interval.text)\n\n        dur = interval.dur\n        dur_msec = dur * 1000 # sec -> msec\n        num_frames = int(round(dur_msec \/30)) # the audio is divided into 30 msec frames\n        print(dur_msec)\n        for i in range(num_frames):\n            \n            labeled_list.append(label)\n\n    return labeled_list","911a99c9":"!pip install librosa\nimport librosa\n\nroot ='\/Female\/TIMIT\/SA2'\nannotation_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Annotation\/Female\/TMIT\/SI2220.TextGrid\"\naudio_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Audio\/Female\/TMIT\/SI2220.wav\"\n\n# read annotaion\nlabel_list = readFile(annotation_path)\n\n# read wav file\ndata, fs = librosa.load(audio_path)","9564461a":"# define time axis\nNs = len(data)  # number of sample\nTs = 1 \/ fs  # sampling period\nt = np.arange(Ns) * 1000 * Ts  # time axis\n\n\nshift = 1 - OVERLAP_RATE\nframe_length = int(np.floor(FRAME_DURATION * fs \/ 1000)) # frame length in sample\nframe_shift = round(frame_length * shift)# frame shift in sample\n","b2f46fcc":"import matplotlib.pyplot as plt\n\nfigure = plt.Figure(figsize=(10, 7), dpi=85)\nplt.plot(t, data)\n\nfor i, frame_labeled in enumerate(label_list):\n    idx = i * frame_shift\n    if (frame_labeled == 1):\n        plt.axvspan(xmin= t[idx], xmax=t[idx + frame_length-1], ymin=-1000, ymax=1000, alpha=0.4, zorder=-100, facecolor='g', label='Speech')\n\nplt.title(\"Ground truth labels\")\nplt.legend(['Signal', 'Speech'])\nplt.show()","e6eb2e82":"print(len(label_list))\nprint(len(t))\nprint(len(data))","b7b35200":"!pip install python_speech_features","04fe6034":"import os # Working with directories\nimport python_speech_features # For exctracting features for deep learning\nfrom tqdm import tqdm # Progress meter\nfrom sklearn import model_selection, preprocessing, metrics # Preparation data\nfrom tensorflow.keras import models, layers","f8c19c5a":"# Function for reading labels from .TextGrig file:\ndef readLabels(path, sample_rate):\n    '''\n    Read the file and return the list of SPEECH\/NONSPEECH labels for each frame\n    '''\n        \n    labeled_list  = []\n    grid = textgrids.TextGrid(path)\n\n    for interval in grid['silences']:\n        label = int(interval.text)\n\n        dur = interval.dur\n        dur_samples = int(np.round(dur * sample_rate)) # sec -> num of samples\n        \n        for i in range(dur_samples):\n            labeled_list.append(label)\n\n    return labeled_list","f4d0e3df":"# Function for getting all files in directories and sub-directories with definite extension:\ndef getFiles(path, extension):\n    list_paths = list()\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if(file.endswith(extension)):\n                list_paths.append(os.path.join(root, file))\n    return list_paths","5c5bc584":"annotation_path = '\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Annotation\/'\nannotation_extension = '.TextGrid'\naudio_path = '\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Audio\/'\naudio_extension = '.wav'\n\nannotation_files = getFiles(path=annotation_path, extension=annotation_extension)\naudio_files = getFiles(path=audio_path, extension=audio_extension)\nannotation_files = sorted(annotation_files)\naudio_files = sorted(audio_files)\n\nprint('Number of files:', len(audio_files))","3c1dc936":"# Set params for model:\npreemphasis_coef = 0.97 # Coefficient for pre-processing filter\nframe_length = 0.025 # Window length in sec\nframe_step = 0.01 # Length of step in sec\nnum_nfft = 512 # Point for FFT\nnum_features = 32 # Number of Mel filters\nn_frames = 32 # Number of frames for uniting in image","695f5b4f":"# Extraction features for each file:\ndataset = list()\nfor i in tqdm(range(len(audio_files))):\n    sig, sample_rate = librosa.load(audio_files[i])\n    markers = readLabels(path=annotation_files[i], sample_rate=sample_rate)\n    \n    # Extract logfbank features:\n    features_logfbank = python_speech_features.base.logfbank(signal=sig, samplerate=sample_rate, winlen=frame_length, winstep=frame_step, nfilt=num_features, \n                                                                        nfft=num_nfft, lowfreq=0, highfreq=None, preemph=preemphasis_coef)\n    \n    # Reshape labels for each group of features:\n    markers_of_frames = python_speech_features.sigproc.framesig(sig=markers, frame_len=frame_length * sample_rate, frame_step=frame_step * sample_rate, \n                                                                winfunc=np.ones)\n    \n    # For every frame calc label:\n    marker_per_frame = np.zeros(markers_of_frames.shape[0])\n    marker_per_frame = np.array([1 if np.sum(markers_of_frames[j], axis=0) > markers_of_frames.shape[0] \/ 2 else 0 for j in range(markers_of_frames.shape[0])])\n    \n    spectrogram_image = np.zeros((n_frames, n_frames))\n    for j in range(int(np.floor(features_logfbank.shape[0] \/ n_frames))):\n        spectrogram_image = features_logfbank[j * n_frames:(j + 1) * n_frames]\n        label_spectrogram_image = 1 if np.sum(marker_per_frame[j * n_frames:(j + 1) * n_frames]) > n_frames \/ 2 else 0\n        dataset.append((label_spectrogram_image, spectrogram_image))","c2606a58":"# Split dataset on train and test:\nX = list()\ny = list()\nfor i in range(len(dataset)):\n    X.append(dataset[i][1])\n    y.append(dataset[i][0])\n    \nX = np.array(X)\ny = np.array(y)\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, shuffle=True, random_state=1)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n# Reshaping for scaling:\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n    \n# Scale data:\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# And reshape back:\nX_train = X_train.reshape(X_train.shape[0], n_frames, n_frames)\nX_test = X_test.reshape(X_test.shape[0], n_frames, n_frames)","3b9a2b13":"# Encoding label:\ny_train = pd.get_dummies(y_train)\ny_test = pd.get_dummies(y_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","0dbe4d1a":"# Reshape data for convolution layer:\nstride = int(15)\n\nX_train_reshaped = X_train[:int(np.floor(X_train.shape[0] \/ stride) * stride)]\nX_test_reshaped = X_test[:int(np.floor(X_test.shape[0] \/ stride) * stride)]\n\ny_train_reshaped = y_train[:int(np.floor(X_train.shape[0] \/ stride) * stride)]\ny_test_reshaped = y_test[:int(np.floor(X_test.shape[0] \/ stride) * stride)]\n\nX_train_reshaped = X_train_reshaped.reshape((int(X_train_reshaped.shape[0] \/ stride), stride, n_frames, n_frames, 1))\nX_test_reshaped = X_test_reshaped.reshape((int(X_test_reshaped.shape[0] \/ stride), stride, n_frames, n_frames, 1))\n\ny_train_reshaped = y_train_reshaped.reshape((int(y_train_reshaped.shape[0] \/ stride), stride, y_train[-1].shape[0]))\ny_test_reshaped = y_test_reshaped.reshape((int(y_test_reshaped.shape[0] \/ stride), stride, y_test[-1].shape[0]))","35c12a27":"model = models.Sequential()\nmodel.add(layers.TimeDistributed(layers.Conv2D(64, (5, 5), activation='elu'), input_shape=(stride, n_frames, n_frames, 1)))\nmodel.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\nmodel.add(layers.TimeDistributed(layers.Conv2D(128, (3, 3), activation='elu')))\nmodel.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\nmodel.add(layers.TimeDistributed(layers.Conv2D(128, (3, 3), activation='elu')))\nmodel.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\nmodel.add(layers.TimeDistributed(layers.Flatten()))\nmodel.add(layers.TimeDistributed(layers.Dense(64, activation='elu')))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.TimeDistributed(layers.Dense(y_train[-1].shape[0], activation='softmax')))\nmodel.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])","53e41ca7":"from tensorflow.keras.utils import plot_model\n\nplot_model(model, 'model.png', show_shapes=True)","05b0f427":"epochs = 25\nbatch_size = 32\ncallbacks = None\n\nmodel.fit(X_train_reshaped, y_train_reshaped,\n          validation_data=(X_test_reshaped, y_test_reshaped),\n          epochs=epochs, batch_size=batch_size, callbacks=callbacks)","90ce5ede":"annotation_file = \"..\/input\/speech-activity-detection-datasets\/Data\/Annotation\/Female\/TMIT\/SA1.TextGrid\"\naudio_file = \"..\/input\/speech-activity-detection-datasets\/Data\/Audio\/Female\/TMIT\/SA1.wav\"\n\ndataset_valid = list()\n\n# Load samples:\ninput_signal, fs = librosa.load(audio_file)\n\n# Load labels:\ntruth_labels = readLabels(path=annotation_file, sample_rate=fs)\n\n# Extract logfbank features:\nfeatures_logfbank_valid = python_speech_features.base.logfbank(signal=input_signal, samplerate=fs, winlen=frame_length, winstep=frame_step, nfilt=num_features, \n                                                               nfft=num_nfft, lowfreq=0, highfreq=None, preemph=preemphasis_coef)\n\n# Reshape labels for each group of features:\nmarker_per_frames_truth = python_speech_features.sigproc.framesig(sig=truth_labels, frame_len=frame_length * fs, frame_step=frame_step * fs, \n                                                                 winfunc=np.ones)\n\n# For every frame calc label:\nmarker_per_frame_truth = np.zeros(marker_per_frames_truth.shape[0])\nmarker_per_frame_truth = np.array([1 if np.sum(marker_per_frames_truth[j], axis=0) > marker_per_frames_truth.shape[0] \/ 2 else 0 \n                                   for j in range(marker_per_frames_truth.shape[0])])\n\nspectrogram_image_valid = np.zeros((n_frames, n_frames))\nfor j in range(int(np.floor(features_logfbank_valid.shape[0] \/ n_frames))):\n    spectrogram_image_valid = features_logfbank_valid[j * n_frames:(j + 1) * n_frames]\n    label_spectrogram_image_valid = 1 if np.sum(marker_per_frame_truth[j * n_frames:(j + 1) * n_frames]) > n_frames \/ 2 else 0\n    dataset_valid.append((label_spectrogram_image_valid, spectrogram_image_valid))","19167bc6":"if stride - len(dataset_valid) > 0:\n    for i in range(stride - len(dataset_valid)):\n        dataset_valid.append((0, np.zeros((n_frames, n_frames))))","1bf53335":"# Split dataset on train and test:\nX_valid = list()\ny_valid = list()\nfor i in range(len(dataset_valid)):\n    X_valid.append(dataset_valid[i][1])\n    y_valid.append(dataset_valid[i][0])\n    \nX_valid = np.array(X_valid)\ny_valid = np.array(y_valid)\n\n# Reshaping for scaling:\nX_valid = X_valid.reshape(X_valid.shape[0], X_valid.shape[1] * X_valid.shape[2])\n    \n# Scale data:\nX_valid = scaler.transform(X_valid)\n\n# And reshape back:\nX_valid = X_valid.reshape(X_valid.shape[0], n_frames, n_frames)","e8049daf":"# Encoding label:\ny_truth = y_valid\ny_valid = pd.get_dummies(y_valid)\ny_valid = np.array(y_valid)","581b8a88":"# Reshape data for convolution layer:\nX_valid_reshaped = X_valid[:int(np.floor(X_valid.shape[0] \/ stride) * stride)]\ny_valid_reshaped = y_valid[:int(np.floor(X_valid.shape[0] \/ stride) * stride)]\n\nX_valid_reshaped = X_valid_reshaped.reshape((int(X_valid_reshaped.shape[0] \/ stride), stride, n_frames, n_frames, 1))\ny_valid_reshaped = y_valid_reshaped.reshape((int(y_valid_reshaped.shape[0] \/ stride), stride, y_valid[-1].shape[0]))","0ef04d56":"prediction = model.predict(X_valid_reshaped)\n\npredicted_label = np.zeros(prediction.shape[1])\npredicted_proba = np.zeros(prediction.shape[1])\nind = 0\nfor i in range(prediction.shape[1]):\n    if prediction[0][i][0] >= prediction[0][i][1]:\n        predicted_label[ind] = 0\n        predicted_proba[ind] = prediction[0][i][0]\n    else:\n        predicted_label[ind] = 1\n        predicted_proba[ind] = prediction[0][i][1]\n    ind = ind + 1\n        \npredicted_label","6bff6a68":"print(predicted_label, y_truth)","66650f31":"# Plot AUC-ROC Curve:\npoint_start = [0, 1]\npoint_end = [0, 1]\nfpr, tpr, _ = metrics.roc_curve(y_truth, predicted_proba, pos_label=1)\nroc_display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr).plot(marker='.')\nplt.plot(point_start, point_end, '--')","acf6f23c":"predicted_label_widely = np.zeros(predicted_label.shape[0] * n_frames)\nind_start = 0\nind_stop = n_frames\nshift_step = n_frames\nfor i in range(predicted_label.shape[0]):\n    predicted_label_widely[ind_start:ind_stop] = predicted_label[i]\n    ind_start = ind_start + shift_step\n    ind_stop = ind_stop + shift_step\n\nlabel_timeseries = np.zeros(input_signal.shape[0])\nbegin = int(0)\nend = int(frame_length * fs)\nshift_step = int(frame_step * fs)\nfor i in range(predicted_label_widely.shape[0]):\n    label_timeseries[begin:end] = predicted_label_widely[i]\n    begin = begin + shift_step\n    end = end + shift_step","dc0f5820":"# define time axis\nNs = len(input_signal)  # number of sample\nTs = 1 \/ fs  # sampling period\nt = np.arange(Ns) * Ts  # time axis in seconds\nnorm_coef = 1.1 * np.max(input_signal)\n\nedge_ind = np.min([input_signal.shape[0], len(truth_labels)])\n\nplt.figure(figsize=(20, 9), dpi=85)\nplt.plot(t[:edge_ind], input_signal[:edge_ind])\nplt.plot(t[:edge_ind], np.array(truth_labels[:edge_ind]) * norm_coef)\nplt.plot(t[:edge_ind], label_timeseries[:edge_ind] * norm_coef)\n\nplt.title(\"Ground truth labels\")\nplt.legend(['Signal', 'Speech', 'Predicted'])\nplt.show()","c309c6ce":"![image.png](attachment:0bcd572e-6e05-497c-bc62-672dd3fdeaa3.png)","0579d82d":"## Preparation:","2aef72cd":"We will use model shown in the picture below","4770f1bf":"## Build and training model:","729cee69":"### Load all files:","7f794384":"### Load data from kaggle","b09267e0":"Algorithm based on a article \"A HYBRID CNN-BILSTM VOICE ACTIVITY DETECTOR\"\n\nIn preprocessing part we need extract Mel filter bank energies from signal as a features. Features are extracted every 10 ms using a 25 ms window. We will use 32 Mel log energies and the log energy of frame. And after extracting, we need form sequences of 32 \u00d7 32 spectrogram imagesas input features.","ce8a0327":"Let's will get prediction on choosen audio file and will compare with truth labels.","b0fb58e4":"It's important for this kind of tasks to perform short time analysis on the signal, so it needs to assign the lables (SPEECH\/NONSPEECH) to very little portions of the signal. I decide to split the data into portion of 30 milliseconds.","63450149":"Let's redefine funtion that has already been implemented above","3516f416":"### Preprocessing input data (extraction features):","945da61f":"## Plot signal","a8501807":"### Install library","7c53d2ee":"# Voice Activity Detection:","094df59b":"## Load a file","6542d124":"# Example of usage ","9ca305e4":"### Define the function extracting the ground truth labels","b4360184":"## Processing files:","1fc23de4":"### Preparing the variable","e69d4c72":"### Build model:","7b705f9a":"### Preparation data:","93a4077e":"The green parts indicates the frames where an human speech is detected.","4cf1ef39":"### Libraries:","b8a1ea5b":"## Validation model:"}}