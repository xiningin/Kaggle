{"cell_type":{"9932a499":"code","c1e18c1b":"code","11aca4cd":"code","7a72b91d":"code","219e32d8":"code","39da442f":"code","899d094b":"code","5bb079ee":"code","51ad5bc9":"code","06e8cfea":"code","e236e11e":"code","d6d9ffab":"code","828200a1":"code","bf9b416b":"code","ac03311b":"code","4601894a":"code","0b5f66cb":"code","10a3f7e6":"code","bc1eab5e":"code","d90efac8":"code","db655e4a":"code","bed1d045":"code","7ce6c244":"code","9cb5452e":"code","d0fe1453":"code","2376ac6c":"markdown","d17ce6ba":"markdown","4a2da53a":"markdown","1cffa901":"markdown","b079a6d4":"markdown"},"source":{"9932a499":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMRegressor","c1e18c1b":"train = pd.read_csv(\"..\/input\/covid19-japan-time-series-transition\/step1\/train.csv\")\ntrain.head()","11aca4cd":"train.isnull().sum() \/ train.shape[0]","7a72b91d":"\n'''\nDefects occur evenly in each prefecture.\n\n'''\n\n\nprefecture = np.random.choice(train.prefecture.unique(), 9)\n\nfor p in prefecture:\n    x = train[train.prefecture == p].isnull().sum() \n    print(\"-\"*35)\n    print(f\"prefecture = {p}\")\n    print(\"-\"*35)\n    print(x)\n","219e32d8":"train = train.fillna(0)\ntrain.isnull().sum()","39da442f":"\ndef viz_transition(df):\n    x = df.groupby(\"date\").mean()\n    \n    plt.figure(figsize=(15, 6))\n    x.plot()\n    plt.title(f\"ALL\")\n    plt.xticks(rotation=45)\n    plt.grid()\n    plt.show()\n    \ndef viz_transition_prefecture(df):\n    prefecture = np.random.choice(df.prefecture, 9)\n    \n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    ax = axes.ravel()\n    \n    for i in range(9):\n        x = df[df.prefecture == prefecture[i]].groupby(\"date\").mean()\n        x.plot(ax=ax[i])\n        ax[i].set_title(f\"prefecture={prefecture[i]}\")\n    plt.tight_layout()\n    \n    ","899d094b":"viz_transition(train)","5bb079ee":"viz_transition_prefecture(train)","51ad5bc9":"train.corr().loc[:, [\"newly_confirmed\"]].style.background_gradient(cmap=\"coolwarm\")","06e8cfea":"\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nsns.histplot(train.newly_confirmed, bins=100)\nplt.xticks(range(0, 8000, 1000))\nplt.subplot(122)\nsns.boxplot(train.newly_confirmed)\n\nplt.show()","e236e11e":"\npref_newly = train.groupby(\"prefecture\").agg({\"newly_confirmed\": \"mean\"})\npref_newly = pref_newly.rename(columns={\"newly_confirmed\": \"pref_newly\"}).sort_values(\"pref_newly\", ascending=False)\n\n\nplt.figure(figsize=(15, 6))\nsns.barplot(data=pref_newly, x=pref_newly.index, y=pref_newly.pref_newly)\nplt.xticks(rotation=90)\nplt.title(\"Prefecture vs newly confirmed\")\nplt.show()","d6d9ffab":"fig, axes  = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.ravel()\n\ntrain[train.prefecture == \"Tokyo\"].set_index(\"date\").plot(ax=ax[0])\nax[0].set_title(\"Tokyo\")\ntrain[train.prefecture == \"Tottori\"].set_index(\"date\").plot(ax=ax[1])\nax[1].set_title(\"Tottori\")\nfig.autofmt_xdate(rotation= 40)","828200a1":"# datetime \ntrain.date = pd.to_datetime(train.date)\n\ntrain[\"year\"] = train.date.dt.year \ntrain[\"month\"] = train.date.dt.month \ntrain[\"week\"] = train.date.dt.dayofweek \ntrain[\"quarter\"] = train.date.dt.quarter \n\n'''\nHere, we want to predict tomorrow based on the information on the number of newly infected people announced on the day, \nso we will move one series and use it as prediction data.\n'''\n\ntrain = train.rename(columns={\"newly_confirmed\": \"newly_confirmed_lag\"})\n# train = train.set_index(\"date\")\ntrain[\"newly_confirmed\"] = train.groupby(\"prefecture\")[\"newly_confirmed_lag\"].shift(-1).fillna(0)\n\n# lag \nuse_col = [\"death\", \"newly_confirmed_lag\", \"requiring_inpatient\", \"released_from_treatment\", \"tobe_confirmed\", \"severe\"]\nfor col in use_col:\n    train[col+\"_cumsum\"] = train.groupby(\"prefecture\")[col].cumsum().fillna(0)\n    train[col+\"lag_1\"] = train.groupby(\"prefecture\")[col].shift(1).fillna(0)\n    train[col+\"lag_7\"] = train.groupby(\"prefecture\")[col].shift(7).fillna(0)\n    train[col+\"lag_30\"] = train.groupby(\"prefecture\")[col].shift(30).fillna(0)\n    train[col+\"avg_7\"] = train.groupby(\"prefecture\")[col].rolling(window=7).mean().reset_index(drop=True).fillna(0)\n    \ntrain.head()","bf9b416b":"\n# group mean -> merge \n\npref = train.groupby(\"prefecture\").mean().loc[:, use_col]\npref.columns = [c+str(\"_pref\") for c in use_col]\ntrain = pd.merge(train, pref, how=\"left\", left_on=\"prefecture\", right_index=True)\n\ndate = train.groupby(\"date\").mean().loc[:, use_col]\ndate.columns = [c + str(\"_date\") for c in use_col]\ntrain = pd.merge(train, date, how=\"left\", left_on=\"date\", right_index=True)\n\npref_date = train.groupby([\"prefecture\", \"date\"]).mean().loc[:, use_col]\npref_date.columns = [c + str(\"_date_pref\") for c in use_col]\ntrain = pd.merge(train, pref_date, how=\"left\", left_on=[\"prefecture\", \"date\"], right_index=True)\n\ntrain.head()","ac03311b":"\n'''\ndrop out 2020 year \n'''\n\ndf = train[train.date >= \"2021-01-01\"]\ndf.shape ","4601894a":"df.corr().loc[:, [\"newly_confirmed\"]].style.background_gradient(cmap=\"coolwarm\")","0b5f66cb":"la = LabelEncoder()\ndf[\"prefecture\"] = la.fit_transform(df.prefecture)\n\ntrain, val, test = df[df.date < \"2021-09-01\"], df[(df.date >= \"2021-09-01\") & (df.date <= \"2021-09-30\")], df[df.date == \"2021-10-01\"]\ntrain.drop(\"date\", axis=1, inplace=True)\nval.drop(\"date\", axis=1, inplace=True)\ntest.drop(\"date\", axis=1, inplace=True)\n\nprint(train.shape[0], val.shape[0], test.shape[0])","10a3f7e6":"train.dtypes ","bc1eab5e":"\n'''\npredict this. \n\nIn other words, we will predict tomorrow from the data of October 1, 2021.\n'''\n\ntest[[\"newly_confirmed\"]]","d90efac8":"\n'''\nMake predictions with a simple regression model\n'''\n\n\nparams = {'objective': 'regression',\n          'learning_rate': 0.25,\n          \"boosting_type\": \"gbdt\",\n          'min_data_in_leaf':600,\n          'max_bin': 196,\n          #'device':'gpu',\n          'feature_fraction':0.4,\n          'lambda_l1':36, 'lambda_l2':80,\n          'max_depth':16,\n          'num_leaves':1000,\n          \"metric\": 'mae',\n          'n_jobs': -1\n         }\n\ndef mae(pred, corr):\n    return np.mean(np.abs(pred - corr))\n\ndef train_fn(train, val, test):\n    x_train, y_train = train.drop(\"newly_confirmed\", axis=1), train[[\"newly_confirmed\"]]\n    x_val, y_val = val.drop(\"newly_confirmed\", axis=1), val[[\"newly_confirmed\"]]\n    x_test = test.drop(\"newly_confirmed\", axis=1)\n    \n    model = LGBMRegressor(**params, random_state=42, n_estimators=2000)\n    model.fit(x_train, \n             y_train, \n             eval_set=[(x_train, y_train), (x_val, y_val)], \n             verbose=1000, \n             early_stopping_rounds=30)\n    \n    pred_v = model.predict(x_val).flatten()\n    pred_t = model.predict(x_test).flatten()\n    \n    print(f\"MAE: {mae(pred_v, y_val.values.ravel())}\")\n    return pred_v, pred_t, model \n    ","db655e4a":"pred_val, pred_test, model = train_fn(train, val, test)","bed1d045":"\n'''\nVisualize the forecast surface\n\nFor more accurate predictions, it is better to use validation data as well.\n'''\n\ndef viz_predict(pred_val):\n    x = val.copy()\n    x[\"predict\"] = pred_val \n    \n    xx = x.groupby(\"prefecture\").mean().loc[:, [\"newly_confirmed\", \"predict\"]]\n    xx.plot()\n    \ndef viz_predict_pref(pred_val):\n    x = val.copy()\n    x[\"predict\"] = pred_val \n    pref = np.random.choice(x.prefecture, 9)\n    \n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    ax = axes.ravel()\n    \n    for i in range(9):\n        xx = x[x.prefecture == pref[i]]\n        xx[[\"newly_confirmed\", \"predict\"]].plot(ax=ax[i])\n    plt.tight_layout()\n    \n    ","7ce6c244":"viz_predict(pred_val)","9cb5452e":"viz_predict_pref(pred_val)","d0fe1453":"sub = pd.read_csv(\"..\/input\/covid19-japan-time-series-transition\/step1\/sample_submission.csv\")\nsub[\"newly_confirmed\"] = pred_test \nsub.to_csv(\"submission.csv\", index=False)","2376ac6c":"# Add features ","d17ce6ba":"# Evaluate ","4a2da53a":"# Time split ","1cffa901":"# Submission ","b079a6d4":"# Training "}}