{"cell_type":{"f0569f97":"code","ba16503c":"code","3005f07e":"code","e723e214":"code","4b0b96f3":"code","54439829":"code","be727478":"code","b4fde631":"code","4cb75bfb":"code","5fe6dccc":"code","87323822":"code","7282b851":"code","46451011":"code","80895a34":"code","5e0a0a92":"code","9f6a78d4":"code","a1c29f82":"code","c23b3514":"code","11032458":"code","4e80e5b6":"code","e183fbbd":"code","52a7fffd":"code","161ee796":"code","986662fa":"code","098a373d":"code","3aa2a9f9":"code","512ba1cd":"code","d4088958":"code","cb7046ce":"code","0b03ea06":"markdown","da161584":"markdown","f70fe4fb":"markdown","c7122f34":"markdown","ec0c077e":"markdown","6a55e059":"markdown","c5115fa1":"markdown","5c139691":"markdown","107fc630":"markdown","698af7da":"markdown","42cef69d":"markdown","66d88b1a":"markdown","a16bdf54":"markdown","793e8026":"markdown","3a6461a3":"markdown","bc0f2621":"markdown","6f535410":"markdown","ea75df83":"markdown","a384710a":"markdown","6ae76f7a":"markdown","5b01d29f":"markdown","08b1e875":"markdown","d43212c6":"markdown","0c78136f":"markdown","9ddc5ef2":"markdown","f3d833ce":"markdown","05658644":"markdown","60959a9c":"markdown"},"source":{"f0569f97":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.api.types import CategoricalDtype\n#from plotnine import *\n","ba16503c":"discursos = pd.read_csv(\"..\/input\/discursos-impeachment.csv\",\n            header=0,\n            names=['deputado','partido', 'estado', 'voto', 'genero', 'fala'])\n\n\ndiscursos.deputado = discursos.deputado.apply(lambda v: v.lower()).astype('category')\ndiscursos.partido = discursos.partido.astype(CategoricalDtype(discursos.partido.value_counts().index.tolist(), ordered=True))\ndiscursos.estado = discursos.estado.astype(CategoricalDtype(discursos.estado.value_counts().index.tolist(), ordered=True))\ndiscursos.voto = discursos.voto.apply(lambda v: v.lower())\ndiscursos.voto = discursos.voto.astype(CategoricalDtype(list(set(discursos.voto)), ordered=True))\ndiscursos.genero = discursos.genero.apply(lambda v: v.upper()).astype('category')\ndiscursos.fala = discursos.fala.apply(lambda v: str(v).lower())\n\n","3005f07e":"discursos.head()","e723e214":"discursos.tail()","4b0b96f3":"discursos.describe()","54439829":"corpus = ' '.join(discursos.fala.tolist())\nlen(corpus.split(' ')), len(corpus)","be727478":"unig_corpus = [w for w in corpus.split(' ') if w.strip() != ''] ","b4fde631":"def build_neighbor_dict(corpus):\n    result = dict([(w,dict()) for w in list(set(corpus))])\n    for i in range(0,len(corpus)-1):\n        word = corpus[i]\n        next_word = corpus[i+1]\n        if next_word not in result[word].keys():\n            result[word][next_word] = 0\n        result[word][next_word] += 1\n    \n    for word, counts in result.items():\n        total = sum(counts.values())\n        result[word] = dict(list(map(lambda t: (t[0], t[1]\/total), counts.items())))\n    \n    return result \nneigh_dict = build_neighbor_dict(unig_corpus)","4cb75bfb":"def sentence_gen(neigh_dict, first_word_list, stop_condition):\n    current_word = np.random.choice(first_word_list)\n    sentence = [current_word.capitalize()]\n    word_count = 0\n    while 1:\n        current_probs = neigh_dict[current_word]\n        chosen_word = np.random.choice(np.array(list(current_probs.keys())), p = np.array(list(current_probs.values())))\n        sentence.append(chosen_word)        \n        if stop_condition(sentence, word_count): #Stops when there is at least min_word_count and the current word ends in a '.'.\n            return ' '.join(sentence)\n        else:\n            current_word = chosen_word\n            word_count+=1\n        \n","5fe6dccc":"#Feij\u00e3o-com-arroz\nfor i in range(0,5):\n    print(i,\":\",\n          sentence_gen(neigh_dict, \n             first_word_list = list(set(unig_corpus)),\n             stop_condition = lambda _, word_count: word_count == 20), '\\n') ","87323822":"for i in range(0,5):\n    print(i,\":\",\n          sentence_gen(neigh_dict, \n             first_word_list = list(set(discursos.fala.apply(lambda s: s.split(\" \")[0]))),\n             stop_condition = lambda _, word_count: word_count == 20), '\\n') \n","7282b851":"#Mais polimento - Primeira palavra e condi\u00e7\u00e3o de parada simples (ponto final)\nfor i in range(0,5):\n    print(i,\":\",\n          sentence_gen(neigh_dict, \n             first_word_list = discursos.fala.apply(lambda s: s.split(\" \")[0]),\n             stop_condition = lambda sentence, word_count: \n                       sentence[-1][-1] in ['.','!','?'] and word_count > 20),'\\n')","46451011":"#Polishing the most of that pile of turd\nfor i in range(0,5):\n    print(i,\":\",\n          sentence_gen(neigh_dict, \n             first_word_list = discursos.fala.apply(lambda s: s.split(\" \")[0]),\n             stop_condition = lambda sentence, word_count: \n                       ('sim' in ' '.join(sentence) or 'n\u00e3o' in ' '.join(sentence)) and \n                       sentence[-1][-1] in ['.','!','?'] and \n                       word_count > np.random.normal(53,32))\n          ,'\\n')","80895a34":"def ngramify(corpus, n):\n    return list(['_'.join(corpus[i : i+n]) for i in range(len(corpus)-n+1)])","5e0a0a92":"n = 2\nsentences = [[\"$\"]*(n-1) + sentence.split(' ') for sentence in discursos.fala.tolist()]\nsentences = [ [w for w in sentence if w.strip() != ''] for sentence in sentences]\nngrams = []\nfor sentence in sentences:\n    ngrams +=ngramify(sentence, n)\ndel(sentences)","9f6a78d4":"ngrams[:5]","a1c29f82":"ngram_neigh_dict = build_neighbor_dict(ngrams)","c23b3514":"def ngram_sentence_gen(neigh_dict, n, first_word_list, stop_condition):    \n    first_word = ''\n    while len(first_word.strip()) == 0:\n        first_word = np.random.choice(first_word_list)\n    sentence = [first_word.capitalize()]\n    if n > 1:\n        current_ngram = '_'.join([\"$\"]*(n-1)) + \"_\" + first_word\n    else:\n        current_ngram = first_word\n    word_count = 0\n    while 1:         \n        next_ngrams = np.array(list(neigh_dict[current_ngram].keys()))\n        next_probs = np.array(list(neigh_dict[current_ngram].values()))\n        if next_ngrams.shape[0] == 0: return ' '.join(sentence)        \n        chosen_ngram = np.random.choice(next_ngrams, p = next_probs)\n        sentence.append(chosen_ngram.split('_')[-1])                \n        if stop_condition(sentence, word_count):\n            return ' '.join(sentence)\n        else:\n            current_ngram = chosen_ngram\n            word_count+=1\n        \n","11032458":"#Polishing the most of that pile of turd\nfor i in range(0,5):\n    print(i,\":\",\n          ngram_sentence_gen(ngram_neigh_dict, n, \n             first_word_list = list(set(discursos.fala.apply(lambda s: s.split(\" \")[0]))),\n             stop_condition = lambda sentence, word_count: \n                       ('sim' in ' '.join(sentence) or 'n\u00e3o' in ' '.join(sentence)) and \n                       sentence[-1][-1] in ['.','!','?'] and \n                       word_count > np.random.normal(53,32))\n          ,'\\n')","4e80e5b6":"generated_sentences = [\n    ngram_sentence_gen(ngram_neigh_dict, n, \n             first_word_list = list(set(discursos.fala.apply(lambda s: s.split(\" \")[0]))),\n             stop_condition = lambda sentence, word_count: \n                       ('sim' in ' '.join(sentence) or 'n\u00e3o' in ' '.join(sentence)) and \n                       sentence[-1][-1] in ['.','!','?'] and \n                       word_count > np.random.normal(53,32)) \n    for i in range(0, 100)]","e183fbbd":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndiscursos.fala.tolist()\n\n\ndef build_count_model(sentences, generated_sentences):\n        model = CountVectorizer()\n        tfidf = TfidfTransformer()\n        return tfidf.fit_transform(model.fit_transform(sentences + generated_sentences))\n        \nmatrix = build_count_model(discursos.fala.tolist(), generated_sentences)\ncorpus_vecs = matrix[:-100,]\ngenerated_vecs = matrix[-100:,]\n\nsim_matrix = cosine_similarity(corpus_vecs, generated_vecs)\nmax_sims = np.amax(sim_matrix, 0)\n\n","52a7fffd":"\nimport matplotlib.pyplot as plt\nplt.boxplot(max_sims)\nplt.show()\n","161ee796":"def generate_for_n(n, generated_count):\n    sentences = [[\"$\"]*(n-1) + sentence.split(' ') for sentence in discursos.fala.tolist()]\n    sentences = [ [w for w in sentence if w.strip() != ''] for sentence in sentences]\n    ngrams = []\n    for sentence in sentences:\n        ngrams +=ngramify(sentence, n)\n    ngram_neigh_dict = build_neighbor_dict(ngrams)\n    fwords = discursos.fala.apply(lambda s: s.split(\" \")[0])\n    generated_sentences = [\n        ngram_sentence_gen(ngram_neigh_dict, n, \n                 first_word_list = list(set(fwords)),\n                 stop_condition = lambda sentence, word_count: \n                           ('sim' in ' '.join(sentence) or 'n\u00e3o' in ' '.join(sentence)) and \n                           sentence[-1][-1] in ['.','!','?'] and \n                           word_count > np.random.normal(53,32)) \n    for i in range(0, generated_count)]\n    matrix = build_count_model(discursos.fala.tolist(), generated_sentences)\n    corpus_vecs = matrix[:-generated_count,]\n    generated_vecs = matrix[-generated_count:,]\n\n    sim_matrix = cosine_similarity(corpus_vecs, generated_vecs)\n    max_sims = np.amax(sim_matrix, 0)\n    \n    return max_sims, generated_sentences","986662fa":"sims_vecs = pd.DataFrame()\nsentences = []\nfor n in range(1,21):    \n    max_sims, sents = generate_for_n(n, 100)\n    sentences.append(sents)\n    sims_vecs[str(n)] = (max_sims)\n","098a373d":"plt.boxplot(sims_vecs.values)\nplt.show()","3aa2a9f9":"print(\"2-grams\")\nfor i,sent in enumerate(sentences[1][:5]):\n    print(i, \":\", sent)","512ba1cd":"print(\"3-grams\")\nfor i,sent in enumerate(sentences[2][:5]):\n    print(i, \":\", sent)","d4088958":"print(\"10-grams\")\nfor i,sent in enumerate(sentences[9][:5]):\n    print(i, \":\", sent)","cb7046ce":"print(\"20-grams\")\nfor i,sent in enumerate(sentences[19][:5]):\n    print(i, \":\", sent)","0b03ea06":"Then, we use these words to build our graph, using a dict of neighbors. This dict describes how words connect with each other in our graph. In other words, it shows the probabilities of a word being followed by another.","da161584":"Bellow, we can see some of the generated ngrams (n=3).","f70fe4fb":"Sound much better. With some good luck, some sentences even show some real sense! However, we can still do something for this. Lets estbilish that the sentence must have a vote (`sim`(yes) or `n\u00e3o`(no)), and we will choose the ammount of words with a normal distribution based on the mean (`53`) and SD (`32`) of the corpus sentences word count.","c7122f34":"In the function below, we introduce a way to transform strings into ngrams.","ec0c077e":"The sentences are somewhat readable but dont make much sense. The sentences end suddenly, and some of the first words does not makes much sense. We can try to change the first word list in order to only include only words that are first words in the corpus as well.","6a55e059":"However, our sentence generation should be ajusted in order to generate our words. We must consider only the last word of each ngram to build the sentence itself, as all the previous words are just context for it. Otherwise, our generation process stays the same.","c5115fa1":"In this series of notebooks, we aim to show diferent ways that text can be manipulated with machine learning techiniques. In this notebook, we are using a dataset composed of arround 500 sentences, transcribed from congressman vote speeches during the 2016 impeachment process in Brazil.  \n","5c139691":"Below we transform this whole ngram-based sentence generation process in a single function. The objective is to ffind the higest n that still gives distinct sentences from the ones in the corpus. The n-gram generator is also valid for unigrams (single word with no context), so we are going to test for these as well.\n","107fc630":"## Markov Chains\n\nMarkov chains are graphs that describe probabilities for event sequences. Each node is an event, in our case a word, with directed connections with other nodes, where this connection describes the probability of the next node\/event happens after the current node. These probabilities are discovered from a set of data. All connections going out from an node must sum up to 1.\n\nHere we are going to build a markov chain from zero. However, there are python packages cabable of building markov chains from texts, as the `markovify` package.","698af7da":"A little better, but the suddenly ending senteces  still are very annoying. Here we estabilish that our generator should only stop if the last word is ends in a period and the sentence has more than a certain ammount of words (20)","42cef69d":"The boxplot above show us the distribution of similarity metrics for 1-grams, 2-grams, ..., 20-grams. We see that the distributions converge quickly to values close to 1.0. 2 and 3-grams seems the best choice in order to generate new sentences, as they have a respectable similarity, but are not too close of being identical. In order to show this, we generate some 2, 3, 10 and 20 grams sentences bellow.","66d88b1a":"To build our markov model dict, the process is the same as with the single word model. So, we will use the same neighbor dictionary as before.","a16bdf54":"As for the generating parameters, we'll use the same parameters as the most complete one in the single word generation: the first word comes from the first word list previously described, the stop conditions are:\n* at least some voting definition (`sim` or `n\u00e3o`),\n* the sentence must be of a size generated from the normal distribution of the corpus sentence sizes, \n* and it must end in a sentence end marker.","793e8026":"To preprocess our data we need to define the word generation method, and the basic building block to our sentences.\n\nFor instance, if we characters are to be used, our model will have to learn the whole portuguese grammar from scratch, giving us a much more complete learning, however it is more costly and needs huge ammounts of data.\n\nElse, if we use words, our model does not need to learn the whole grammar, only the relationship between words. It will stiffen our model, but we will need less data and less computing power.\n\nWe can use as model, very simple algorithms, as Markov Chains, up to advanced sequence-aware neural networks as LSTMs. More adanced models will give us better results, but in exchange of bigger ammounts of data and computational costs.\n\nIn this notebook we are going to use a simpler model: an Markov Chain. It does not require huge amounts of data, and it is very fast to compute. It matches well our corpus, that only has 27.000 words. It is a simple model that can't accomplish complex associations, but we'll try to extract the most of it.\n\nIn order to create phrases that follows more closely the portuguese grammar, we are using words with punctuation marks as the building block of our model.","3a6461a3":"Not that much of an improvment from the previous version, but here at least the sentences reflects better the (very) talkative nature of Brazilian congressman during the impeachment votation. \n\nWe can still improve these results, but we have to change more fundamental stuff. We need to introduce context to our words.\n","bc0f2621":"## Markov Chains (n-grams)","6f535410":"## Introdu\u00e7\u00e3o","ea75df83":"We see that the data appears to have been loaded successfuly. We also see some facts about Brazilian politics. There are 25 parties in congress, with the biggest being the PMDB, with 66 congressman in an universe of 513 parliamentarians, which hints on the very fragmented nature of Brazilian politics. We also see that the lower house is composed mainly by men (462 of 513).","a384710a":"Passing a first word list and a stop condition as parameters makes our function a little more complicated, but allows us a degree  of flexibility that can greatly improve our model. Bellow we start with a pretty simple generation. We get our first word randomly from our corpus, and then, we generate a fixated ammount of words (20).","6ae76f7a":"Bellow we load some data to verify if it have any inconsistencies, and for we have some taste of it","5b01d29f":"## Preprocessing","08b1e875":"The boxplot above shows that the similarities between our sentences are between approx. 0.7 and 0.4, centered in between 0.5 and 0.6. The values can change slightly in diferent executions, due to the stochastic nature of the solution. These values mean that we have not-so-similar sentences, but similar enough so we can believe that they are in the same context, which is a good result.","d43212c6":"We see that our results are much more realistic now, and that they get close from real life sentences. Perhaps TOO close. We should check if we are generating real distinct sentences or just reenacting the sentences from the original corpus.\n\nSentence similarity is a big field. We could go deep here and explore more advanced word models. However, our corpus is quite small (only around 27k words and much less sentences), so simpler models that do not require extensive training could be a better fit here. \n\nWe will compare 100 generated sentences with all sentences in the corpus, using the cosine similarity of TFIDF(link) vectors generated with all these sentences together. Then we should get the highest similarity for each generated sentence and see their distribution. Various values too close to 1.0 means that the sentences are too similar and are reproducting their training set. In the other hand, values too close to 0.0 should not happen as we should be somewhat reproducting the words and structure of this corpus.","0c78136f":"It is often said that you can tell a person nature from its peers. This is no different for words, as they are closely linked to the words arround them, or their *context*. Giving some context to our sentence generator can improve its performance. What we end up doing here, in a nutshell, is anwsering the following question: \"Given these (word1,...wordn) previous words, what is the probability that this node is the next word in the sequence? In order to do so, we go the old-fashioned way, using n-grams.","9ddc5ef2":"Initialy we load all columns from the dataset, although is very likely that we wont use the majority of them. We load the discoruses themselves (`discursos.fala`) as string, normalizing everyone to lower case, in order to avoid distinctions between upper-cased and lower-cased words.\n\nAs for the other fields, they are all loaded as categorical, as they describe things as congressman names (`discursos.deputado`), their party (`discursos.partido`), which state they are from (`discursos.estado`), their vote(`discursos.voto`), which is normalized to lower case to better consistency and their gender (`discursos.genero`).","f3d833ce":"Bellow, we have a function to generate our sentences. It walks through our markov chain (`neigh_dict`), generating the sentence as it goes, with its first word being chosen from a list of first words (`first_word_list`), until a certain stop condition is met (`stop_condition`). The next word is chosen randomly, based on the probabilities of it happening based in the previous word.","05658644":"In order to not have n-grams that include context from distinct sentences, we have to isolate them in our corpus. We also want to keep using single words as the starting point for our sentences, so we have to put some mock symbol($) in the start of each sentence.","60959a9c":"First of all, we separate our corpus into the building blocks for the graph nodes: single words."}}