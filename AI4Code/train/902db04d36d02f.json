{"cell_type":{"03d2ac5e":"code","93022834":"code","59a963a3":"code","6ca6bfd0":"code","e906f6d5":"code","b2336d91":"code","9b481a34":"code","e5bac7b1":"code","d3b52892":"code","9921ace4":"code","26b0429f":"code","355b83d4":"code","fae621b0":"code","de01fc5d":"code","ecfca033":"code","387a8a14":"code","f4141a94":"code","f9a59189":"code","46574a40":"code","24affefd":"code","07788d6b":"code","fe79132f":"code","efe8d95b":"code","36f0017f":"markdown","d0b77ebd":"markdown","c3e1311d":"markdown","97009669":"markdown","461e7b6b":"markdown","4f9c2ce7":"markdown","f121c760":"markdown","29571ad9":"markdown","2ea23fcb":"markdown","ad2690c9":"markdown","bbb7ec95":"markdown","b3d1f8c7":"markdown","c587e144":"markdown","a5461247":"markdown","435d33ca":"markdown","70157330":"markdown","94690d4a":"markdown","025f5ebd":"markdown"},"source":{"03d2ac5e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n%matplotlib inline","93022834":"from sklearn.model_selection import  train_test_split\nfrom keras.datasets import mnist\nfrom keras import models\nfrom keras import layers\nfrom keras.utils import to_categorical","59a963a3":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","6ca6bfd0":"y_train = train['label']\nX_train = train.drop(labels = ['label'], axis = 1)\ndel train","e906f6d5":"X_train.shape, y_train.shape","b2336d91":"test.shape","9b481a34":"X_train = X_train\/255.0\nX_train = X_train.values.reshape(-1, 28, 28, 1)\n\ntest = test\/255.0\ntest = test.values.reshape(-1, 28, 28, 1)","e5bac7b1":"y_train[9]","d3b52892":"g = sns.countplot(y_train)","9921ace4":"y_train = to_categorical(y_train, num_classes = 10)","26b0429f":"y_train[9]","355b83d4":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 2)","fae621b0":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (28, 28, 1)))\nmodel.add(layers.MaxPooling2D(2, 2))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D(2, 2))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation = 'relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(10, activation = 'softmax'))","de01fc5d":"model.summary()","ecfca033":"model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])","387a8a14":"history = model.fit(X_train, y_train, epochs = 20, batch_size = 128, validation_data = (X_val, y_val), verbose = 2)","f4141a94":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nacc = history.history['acc']\nval_acc = history.history['val_acc']","f9a59189":"epochs = range(1, 21)\n\nplt.plot(epochs, loss, 'ko', label = 'Training Loss')\nplt.plot(epochs, val_loss, 'k', label = 'Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()","46574a40":"plt.plot(epochs, acc, 'yo', label = 'Training Accuracy')\nplt.plot(epochs, val_acc, 'y', label = 'Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()","24affefd":"results = model.predict(test)","07788d6b":"results = np.argmax(results, axis = 1)\nresults = pd.Series(results, name = 'Label')","fe79132f":"results","efe8d95b":"submission = pd.concat([pd.Series(range(1, 28001), name = 'ImageId'), results], axis = 1)\nsubmission.to_csv(\"MNIST_Dataset_Submissions.csv\", index = False)","36f0017f":"**Digit Classification on MNIST Dataset using Keras**","d0b77ebd":"**Plotting** the training vs validation loss and accuracy, and checking how our model performed.","c3e1311d":"Makng a *submission.csv* file containing our results.","97009669":"Since this is a multiclass classification problem, we use loss as *categorial_crossentropy*.\n\nNow, **training the model**.","461e7b6b":"Presently *y_train* contains the actual digit (from 0 through 9). We need to transform it into a more computer friendly form. We do this by **One-Hot Encoding**.","4f9c2ce7":"Presently, *results* contains One-Hot Vectors. We convert them back to digits by picking out the maximum index of the vector.","f121c760":"Now when that's underway, let us **import the data**.","29571ad9":"This is what the model actually looks like.","2ea23fcb":"Let's take a look at what our *results* look like.","ad2690c9":"In this example, I've trained a Convolution Neural Network from scratch on the MNIST Dataset\nThis notebook is inspired by the book **Deep Learning with Python** written by Keras author Francois Chollet.\n\nWe'll be using Keras, a simple-to-use deep learning library in Python. Let us start by importing the required libraries.","bbb7ec95":"As you can see, our accuracy on validation set is above 99%. Let's see how well this model can do on test set.","b3d1f8c7":"**Building the Model. **\n\nOur model consists of 3 Pairs of Conv2D-MaxPool layers and 2 Dense layers. Dropout is also used to fight overfitting.","c587e144":"**Preprocessing the data**\n\nAt present, each data value contains a pixel of range 0 to 255. We normalize them by so that each pixel value is between 0 and 1.\n\nNext, we reshape them so that the linear array of (Size, 784) can be made to a shape of (Size, 28, 28, 1). This is the shape that will be required by our ConvNet. The general form is (Size, height, width, no. of channels).\n\n*Note: This operation is applied to both training and testing data.*","a5461247":"This is my first public kernel on Kaggle. Hope you enjoyed it. \n\nAny comments and suggestions are welcome :)\n\nCheckout this repository for a slightly modified version of the above kernel: https:\/\/github.com\/raahatg21\/Digit-Recognition-MNIST-Dataset-with-Keras\/blob\/master\/MNIST_9914.ipynb\n\nThanks for reading\n\n*Raahat Gupta*","435d33ca":"Now let's come to the labels. First, take a look at them before we do anything.","70157330":"We've created two dataframes, *train* and *test* to store the training and testing data respectively. Now let's extract the features and labels out of the training data. \n\n*Note: the 'test' dataframe is left unchanged here.* ","94690d4a":"See, *y_train[9]* now stores [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] instead of just 3. That is, an array of 10, where all are 0 expect the label, which is 1.\n\nNow, let's crave the validaton set out of out training data. This is the data that our model evaluates on after each cycle of training. We use the Scikit Lean function ***train_test_split*** for this.","025f5ebd":"Thus, *X_train* and *y_train* contain training data and labels respectively, and *test* contains test data."}}