{"cell_type":{"80f85dd5":"code","2d06e65b":"code","b43b00b6":"code","7dda6092":"code","21b12230":"code","81ad2927":"code","5d6526cb":"code","0ffdefe0":"code","5e77ef86":"code","349759c5":"code","ecf3dd00":"code","e558f67a":"code","b04add9b":"code","93f82a3e":"code","2b681197":"code","ddda422a":"code","c742b01f":"code","80351f08":"code","8ea5b9dc":"code","176c4a50":"code","67e7b255":"code","cb0b4c15":"code","a05805dd":"code","a29e36de":"code","70488948":"code","5f3101c4":"code","9c3fec83":"code","3ca6bbbd":"code","4af8784e":"code","c13cb554":"code","7c55f516":"code","759d7a5d":"code","c9dad69e":"code","dd1528c3":"code","9fed085e":"code","17852e39":"code","c29dcfb7":"code","dacbbfea":"code","4e20cd03":"code","1a5c4a71":"markdown","ce03f861":"markdown","8ab939c8":"markdown","99454167":"markdown","018c4494":"markdown","958168ac":"markdown","0b602916":"markdown","a1c86b11":"markdown","81ea2cea":"markdown"},"source":{"80f85dd5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as implt\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer,TFBertModel,RobertaTokenizer,TFRobertaModel,XLNetTokenizer,TFXLNetModel\nfrom tensorflow.keras.layers import Dense,Dropout,Input,Conv1D,BatchNormalization\n%matplotlib inline","2d06e65b":"print(tf.__version__)","b43b00b6":"gpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\n    tf.config.experimental.set_memory_growth(device, True)","7dda6092":"import os\nimport random as rn\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(42)\nrn.seed(42)\ntf.random.set_seed(42)","21b12230":"df=pd.read_csv('..\/input\/google-quest-challenge\/train.csv')","81ad2927":"pd.options.display.max_columns=None","5d6526cb":"df.head()","0ffdefe0":"df.isna().sum()","5e77ef86":"df.info()","349759c5":"df.shape","ecf3dd00":"df.columns","e558f67a":"PRE_NAME='bert-base-uncased'\n#PRE_NAME='bert-large-uncased'\n#PRE_NAME='roberta-base'\n#PRE_NAME='roberta-large'\n#PRE_NAME='xlnet-base-cased'\n#PRE_NAME='xlnet-large-cased'\nHEADS=1","b04add9b":"if PRE_NAME.startswith('bert'):\n    tokenizer=BertTokenizer.from_pretrained(PRE_NAME)\n    print(f\"the input format is \\n\")\n    print(tokenizer('Example where we will have first sentence here','Example where we will have second sentence here',\n                   max_length=40,padding='max_length',truncation='longest_first'))\n    print(\"\\n the tokens are\\n\")\n    print(tokenizer.convert_ids_to_tokens(tokenizer('Example where we will have first sentence here','Example where we will have second sentence here',\n                   max_length=40,padding='max_length',truncation='longest_first')['input_ids']))\nelif PRE_NAME.startswith('roberta'):\n    tokenizer=RobertaTokenizer.from_pretrained(PRE_NAME)\n    print(tokenizer(' Example where we will have first sentence here',' Example where we will have second sentence here',\n                   max_length=40,padding='max_length',truncation='longest_first'))\n    print(\"\\n the tokens are\\n\")\n    print(tokenizer.convert_ids_to_tokens(tokenizer(' Example where we will have first sentence here',' Example where we will have second sentence here',\n                   max_length=40,padding='max_length',truncation='longest_first')['input_ids']))\nelse:\n    tokenizer=XLNetTokenizer.from_pretrained(PRE_NAME)\n    print(tokenizer('Example where we will have first sentence here','Example where we will have second sentence here',\n                   max_length=40,padding='max_length',truncation='longest_first'))\n    print(\"\\n the tokens are\\n\")\n    print(tokenizer.convert_ids_to_tokens(tokenizer('Example where we will have first sentence here','Example where we will have second sentence here',\n                   max_length=40,padding='max_length',truncation='longest_first')['input_ids']))","93f82a3e":"cols=['question_title', 'question_body','answer']\nfor col in cols:\n    lens=[]\n    for i in tqdm(range(df.shape[0])):\n        lens.append(len(tokenizer.tokenize(df.loc[i,col])))\n    print(f\"{col} has len distribution\\n\")\n    print(pd.Series(lens).describe())\n    print('\\n##############\\n')","2b681197":"plt.hist(df['category'])\nplt.xticks(rotation=30)\nplt.show()","ddda422a":"cols=['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\nplt.subplots(6,5,figsize=(40,50))\nfor i,col in enumerate(cols):\n    plt.subplot(6,5,i+1)\n    plt.hist(df[col])\n    plt.title(col)\nplt.show()","c742b01f":"kfold=KFold(n_splits=5)\ndf['kfold']=-1\nfor i,(train,test) in enumerate(kfold.split(df[['question_title', 'question_body','answer']],df[cols])):\n    df.loc[test,'kfold']=i","80351f08":"df['kfold'].value_counts()","8ea5b9dc":"MAX_LENGTH=512\n#h1=[['question_title', 'question_body'],['question_title','answer']]\n#h2=[]\n####################\nh1=[['question_title', 'question_body'],['answer']]\nh2=[]\n####################\n#h1=[['question_title', 'question_body'],['question_body','answer']]\n#h2=[]\n#####################\n#h1=[['question_body'],['question_title','answer']]\n#h2=[]\n#######################\n#h1=[['question_title'],['question_body','answer']]\n#h2=[]\n#####################\n#h1=[['question_title'],[ 'question_body']]\n#h2=[['question_title'],['answer']]\n#######################\n#h1=[['question_title'],[ 'question_body']]\n#h2=[['question_body'],['answer']]","176c4a50":"def get_bert_single_head_inputs(tokenizer,s1,s2,max_length):\n    s1=tokenizer.tokenize(s1)\n    s2=tokenizer.tokenize(s2)\n    s_len=max_length-3\n    s1_len=len(s1)\n    s2_len=len(s2)\n    if s1_len+s2_len==s_len:\n        total_tokens=['[CLS]']+s1+['[SEP]']+s2+['[SEP]']\n    elif s1_len+s2_len<s_len:\n        total_tokens=['[CLS]']+s1+['[SEP]']+s2+['[SEP]']\n    else:\n        if s_len%2==0: #even length so, lets divide equally\n            req_s1_len=s_len\/\/2\n            req_s2_len=s_len\/\/2\n        else:\n            req_s1_len=s_len\/\/2\n            req_s2_len=(s_len\/\/2)+1\n        if s1_len<=req_s1_len and s2_len>req_s2_len:\n            # s1 is shorter but s2 is longer\n            s2=s2[:s_len-s1_len]\n            total_tokens=['[CLS]']+s1+['[SEP]']+s2+['[SEP]']\n        elif s1_len>req_s1_len and s2_len<=req_s2_len:\n            # s1 is longer but s2 is shorter\n            s1=s1[:s_len-s2_len]\n            total_tokens=['[CLS]']+s1+['[SEP]']+s2+['[SEP]']\n        elif s1_len>req_s1_len and s2_len>req_s2_len:\n            # both are longer\n            s1=s1[:req_s1_len]\n            s2=s2[:req_s2_len]\n            total_tokens=['[CLS]']+s1+['[SEP]']+s2+['[SEP]']\n    total_tokens=total_tokens+['[PAD]']*(max_length-len(total_tokens))\n    input_ids=tokenizer.convert_tokens_to_ids(total_tokens)\n    attention_mask=np.char.not_equal(total_tokens,'[PAD]').astype('int32')\n    token_type_ids=[]\n    seq_num=0\n    for tok in total_tokens:\n        if tok=='[SEP]':\n            token_type_ids.append(seq_num)\n            seq_num=1-seq_num\n        else:\n            token_type_ids.append(seq_num)\n    return input_ids,attention_mask,token_type_ids","67e7b255":"def get_roberta_single_head_inputs(tokenizer,s1,s2,max_length):\n    s1=tokenizer.tokenize(s1)\n    s2=tokenizer.tokenize(s2)\n    s_len=max_length-4\n    s1_len=len(s1)\n    s2_len=len(s2)\n    if s1_len+s2_len==s_len:\n        total_tokens=['<s>']+s1+['<\/s>']+['<\/s>']+s2+['<\/s>']\n    elif s1_len+s2_len<s_len:\n        total_tokens=['<s>']+s1+['<\/s>']+['<\/s>']+s2+['<\/s>']\n    else:\n        if s_len%2==0: #even length so, lets divide equally\n            req_s1_len=s_len\/\/2\n            req_s2_len=s_len\/\/2\n        else:\n            req_s1_len=s_len\/\/2\n            req_s2_len=(s_len\/\/2)+1\n        if s1_len<=req_s1_len and s2_len>req_s2_len:\n            # s1 is shorter but s2 is longer\n            s2=s2[:s_len-s1_len]\n            total_tokens=['<s>']+s1+['<\/s>']+['<\/s>']+s2+['<\/s>']\n        elif s1_len>req_s1_len and s2_len<=req_s2_len:\n            # s1 is longer but s2 is shorter\n            s1=s1[:s_len-s2_len]\n            total_tokens=['<s>']+s1+['<\/s>']+['<\/s>']+s2+['<\/s>']\n        elif s1_len>req_s1_len and s2_len>req_s2_len:\n            # both are longer\n            s1=s1[:req_s1_len]\n            s2=s2[:req_s2_len]\n            total_tokens=['<s>']+s1+['<\/s>']+['<\/s>']+s2+['<\/s>']\n    total_tokens=total_tokens+['<pad>']*(max_length-len(total_tokens))\n    input_ids=tokenizer.convert_tokens_to_ids(total_tokens)\n    attention_mask=np.char.not_equal(total_tokens,'<pad>').astype('int32')\n    return input_ids,attention_mask","cb0b4c15":"def get_xlnet_single_head_inputs(tokenizer,s1,s2,max_length):\n    s1=tokenizer.tokenize(s1)\n    s2=tokenizer.tokenize(s2)\n    s_len=max_length-3\n    s1_len=len(s1)\n    s2_len=len(s2)\n    if s1_len+s2_len==s_len:\n        total_tokens=s1+['<sep>']+s2+['<sep>']+['<cls>']\n    elif s1_len+s2_len<s_len:\n        total_tokens=s1+['<sep>']+s2+['<sep>']+['<cls>']\n    else:\n        if s_len%2==0: #even length so, lets divide equally\n            req_s1_len=s_len\/\/2\n            req_s2_len=s_len\/\/2\n        else:\n            req_s1_len=s_len\/\/2\n            req_s2_len=(s_len\/\/2)+1\n        if s1_len<=req_s1_len and s2_len>req_s2_len:\n            # s1 is shorter but s2 is longer\n            s2=s2[:s_len-s1_len]\n            total_tokens=s1+['<sep>']+s2+['<sep>']+['<cls>']\n        elif s1_len>req_s1_len and s2_len<=req_s2_len:\n            # s1 is longer but s2 is shorter\n            s1=s1[:s_len-s2_len]\n            total_tokens=s1+['<sep>']+s2+['<sep>']+['<cls>']\n        elif s1_len>req_s1_len and s2_len>req_s2_len:\n            # both are longer\n            s1=s1[:req_s1_len]\n            s2=s2[:req_s2_len]\n            total_tokens=s1+['<sep>']+s2+['<sep>']+['<cls>']\n    token_type_ids=[]\n    seq_num=0\n    for tok in total_tokens:\n        if tok=='<sep>':\n            token_type_ids.append(seq_num)\n            seq_num=1-seq_num\n        elif tok=='<cls>':\n            token_type_ids.append(2)\n        else:\n            token_type_ids.append(seq_num)\n    token_type_ids=[3]*(max_length-len(total_tokens))+token_type_ids\n    total_tokens=['<pad>']*(max_length-len(total_tokens))+total_tokens\n    input_ids=tokenizer.convert_tokens_to_ids(total_tokens)\n    attention_mask=np.char.not_equal(total_tokens,'<pad>').astype('int32')\n    return input_ids,attention_mask,token_type_ids","a05805dd":"def get_inputs(data,h1,h2,inference=False):\n    columns=data.columns.tolist()\n    y_columns=['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n    if len(h2)==0:# single head model\n        INPUT_IDS=np.empty((data.shape[0],MAX_LENGTH),dtype=np.int32)\n        ATTENTION_MASK=np.empty((data.shape[0],MAX_LENGTH),dtype=np.int32)\n        if not inference:\n            Y=np.empty((data.shape[0],30))\n        if not PRE_NAME.startswith('roberta'):\n            TOKEN_TYPE_IDS=np.empty((data.shape[0],MAX_LENGTH),dtype=np.int32)\n        for i in range(data.shape[0]):\n            t1,t2=h1\n            s1=' '.join(data.loc[i,t1].values)\n            s2=' '.join(data.loc[i,t2].values)\n            if PRE_NAME.startswith('bert'):\n                INPUT_IDS[i,],ATTENTION_MASK[i,],TOKEN_TYPE_IDS[i,]=get_bert_single_head_inputs(tokenizer,s1,s2,MAX_LENGTH)\n            elif PRE_NAME.startswith('xlnet'):\n                INPUT_IDS[i,],ATTENTION_MASK[i,],TOKEN_TYPE_IDS[i,]=get_xlnet_single_head_inputs(tokenizer,s1,s2,MAX_LENGTH)\n            else:\n                INPUT_IDS[i,],ATTENTION_MASK[i,]=get_roberta_single_head_inputs(tokenizer,' '+s1,' '+s2,MAX_LENGTH)\n            if not inference:\n                Y[i,]=data.loc[i,y_columns]\n        if not PRE_NAME.startswith('roberta') and not inference:\n            return INPUT_IDS,ATTENTION_MASK,TOKEN_TYPE_IDS,Y\n        if not PRE_NAME.startswith('roberta') and inference:\n            return INPUT_IDS,ATTENTION_MASK,TOKEN_TYPE_IDS\n        else:\n            if not inference:\n                return INPUT_IDS,ATTENTION_MASK,Y\n            else:\n                return INPUT_IDS,ATTENTION_MASK\n    else:\n        INPUT_IDS1=np.empty((data.shape[0],MAX_LENGTH),dtype=np.int32)\n        ATTENTION_MASK1=np.empty((data.shape[0],MAX_LENGTH),dtype=np.int32)\n        INPUT_IDS2=np.empty((data.shape[0],MAX_LENGTH),dtype=np.int32)\n        ATTENTION_MASK2=np.empty((data.shape[0],MAX_LENGTH),dtype=np.int32)\n        if not inference:\n            Y=np.empty((data.shape[0],30))\n        if not PRE_NAME.startswith('roberta'):\n            TOKEN_TYPE_IDS1=np.empty((data.shape[0],MAX_LENGTH),dtype=np.int32)\n            TOKEN_TYPE_IDS2=np.empty((data.shape[0],MAX_LENGTH),dtype=np.int32)\n        for i in range(data.shape[0]):\n            t1,t2=h1\n            s1=' '.join(data.loc[i,t1].values)\n            s2=' '.join(data.loc[i,t2].values)\n            if PRE_NAME.startswith('bert'):\n                INPUT_IDS1[i,],ATTENTION_MASK1[i,],TOKEN_TYPE_IDS1[i,]=get_bert_single_head_inputs(tokenizer,s1,s2,MAX_LENGTH)\n            elif PRE_NAME.startswith('xlnet'):\n                INPUT_IDS1[i,],ATTENTION_MASK1[i,],TOKEN_TYPE_IDS1[i,]=get_xlnet_single_head_inputs(tokenizer,s1,s2,MAX_LENGTH)\n            else:\n                INPUT_IDS1[i,],ATTENTION_MASK1[i,]=get_roberta_single_head_inputs(tokenizer,' '+s1,' '+s2,MAX_LENGTH)\n            t1,t2=h2\n            s1=' '.join(data.loc[i,t1].values)\n            s2=' '.join(data.loc[i,t2].values)\n            if PRE_NAME.startswith('bert'):\n                INPUT_IDS2[i,],ATTENTION_MASK2[i,],TOKEN_TYPE_IDS2[i,]=get_bert_single_head_inputs(tokenizer,s1,s2,MAX_LENGTH)\n            elif PRE_NAME.startswith('xlnet'):\n                INPUT_IDS2[i,],ATTENTION_MASK2[i,],TOKEN_TYPE_IDS2[i,]=get_xlnet_single_head_inputs(tokenizer,s1,s2,MAX_LENGTH)\n            else:\n                INPUT_IDS2[i,],ATTENTION_MASK2[i,]=get_roberta_single_head_inputs(tokenizer,' '+s1,' '+s2,MAX_LENGTH)\n            if not inference:\n                Y[i,]=data.loc[i,y_columns]\n        if not PRE_NAME.startswith('roberta') and not inference:\n            return INPUT_IDS1,ATTENTION_MASK1,TOKEN_TYPE_IDS1,INPUT_IDS2,ATTENTION_MASK2,TOKEN_TYPE_IDS2,Y\n        if not PRE_NAME.startswith('roberta') and inference:\n            return INPUT_IDS1,ATTENTION_MASK1,TOKEN_TYPE_IDS1,INPUT_IDS2,ATTENTION_MASK2,TOKEN_TYPE_IDS2\n        else:\n            if not inference:\n                return INPUT_IDS1,ATTENTION_MASK1,INPUT_IDS2,ATTENTION_MASK2,Y\n            else:\n                return INPUT_IDS1,ATTENTION_MASK1,INPUT_IDS2,ATTENTION_MASK2","a29e36de":"def get_final_model_inputs(data,h1,h2,inference=False):\n    if HEADS==1:\n        if not PRE_NAME.startswith('roberta'):\n            if not inference:\n                INPUT_IDS,ATTENTION_MASK,TOKEN_TYPE_IDS,Y=get_inputs(data,h1,h2,inference)\n            else:\n                INPUT_IDS,ATTENTION_MASK,TOKEN_TYPE_IDS=get_inputs(data,h1,h2,inference)\n            model_inputs={'input_ids':INPUT_IDS,\n                         'attention_mask':ATTENTION_MASK,\n                         'token_type_ids':TOKEN_TYPE_IDS}\n            if not inference:\n                model_outputs=Y\n        else:\n            if not inference:\n                INPUT_IDS,ATTENTION_MASK,Y=get_inputs(data,h1,h2,inference)\n            else:\n                INPUT_IDS,ATTENTION_MASK=get_inputs(data,h1,h2,inference)\n            model_inputs={'input_ids':INPUT_IDS,\n                         'attention_mask':ATTENTION_MASK\n                         }\n            if not inference:\n                model_outputs=Y\n    else:\n        if not PRE_NAME.startswith('roberta'):\n            if not inference:\n                INPUT_IDS1,ATTENTION_MASK1,TOKEN_TYPE_IDS1,INPUT_IDS2,ATTENTION_MASK2,TOKEN_TYPE_IDS2,Y=get_inputs(data,h1,h2,inference)\n            else:\n                INPUT_IDS1,ATTENTION_MASK1,TOKEN_TYPE_IDS1,INPUT_IDS2,ATTENTION_MASK2,TOKEN_TYPE_IDS2=get_inputs(data,h1,h2,inference)\n            model_inputs=[{'input_ids1':INPUT_IDS1,\n                         'attention_mask1':ATTENTION_MASK1,\n                         'token_type_ids1':TOKEN_TYPE_IDS1},\n                          {'input_ids2':INPUT_IDS2,\n                         'attention_mask2':ATTENTION_MASK2,\n                         'token_type_ids2':TOKEN_TYPE_IDS2}]\n            if not inference:\n                model_outputs=Y\n        else:\n            if not inference:\n                INPUT_IDS1,ATTENTION_MASK1,INPUT_IDS2,ATTENTION_MASK2,Y=get_inputs(data,h1,h2,inference)\n            else:\n                INPUT_IDS1,ATTENTION_MASK1,INPUT_IDS2,ATTENTION_MASK2=get_inputs(data,h1,h2,inference)\n            model_inputs=[{'input_ids1':INPUT_IDS1,\n                         'attention_mask1':ATTENTION_MASK1},\n                          {'input_ids2':INPUT_IDS2,\n                         'attention_mask2':ATTENTION_MASK2}]\n            if not inference:\n                model_outputs=Y\n    if not inference:\n        return model_inputs,model_outputs\n    else:\n        return model_inputs","70488948":"PRE_NAME='bert-base-uncased'\n#PRE_NAME='bert-large-uncased'\n#PRE_NAME='roberta-base'\n#PRE_NAME='roberta-large'\n#PRE_NAME='xlnet-base-cased'\n#PRE_NAME='xlnet-large-cased'\nHEADS=1\nMAX_LENGTH=512","5f3101c4":"def SINGLE_MODEL(sequence=False,final_activation=True,hidden_states=True,hidden_number=4):\n    tf.keras.backend.clear_session()\n    if PRE_NAME.startswith('bert'):\n        ins1=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins2=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins3=Input((MAX_LENGTH,),dtype=tf.int32)\n        pre_model=TFBertModel.from_pretrained(PRE_NAME,output_hidden_states=hidden_states,return_dict=True)\n        pre_layers=pre_model({'input_ids':ins1,'attention_mask':ins2,'token_type_ids':ins3})\n    elif PRE_NAME.startswith('xlnet'):\n        ins1=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins2=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins3=Input((MAX_LENGTH,),dtype=tf.int32)\n        pre_model=TFXLNetModel.from_pretrained(PRE_NAME,output_hidden_states=hidden_states,return_dict=True)\n        pre_layers=pre_model({'input_ids':ins1,'attention_mask':ins2,'token_type_ids':ins3})\n    else:\n        ins1=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins2=Input((MAX_LENGTH,),dtype=tf.int32)\n        pre_model=TFRobertaModel.from_pretrained(PRE_NAME,output_hidden_states=hidden_states,return_dict=True)\n        pre_layers=pre_model({'input_ids':ins1,'attention_mask':ins2})\n    if sequence:\n        x=Conv1D(1,1)(pre_layers[0])\n        x=tf.squeeze(x,axis=-1)\n        x=BatchNormalization()(x)\n        x=Dropout(0.1)(x)\n        x=tf.keras.layers.ReLU()(x)\n    elif hidden_states:\n        if not PRE_NAME.startswith('xlnet'):\n            x=tf.stack([layer[:,0,:] for layer in pre_layers[2][-hidden_number:]],axis=-1)\n            x=tf.keras.layers.Flatten()(x)\n            x=Dense(768*hidden_number,activation='tanh')(x)\n        else:\n            x=tf.stack([layer[:,-1,:] for layer in pre_layers[2][-hidden_number:]],axis=-1)\n            x=tf.keras.layers.Flatten()(x)\n            x=Dense(768*hidden_number,activation='tanh')(x)\n    else:\n        if not PRE_NAME.startswith('xlnet'):\n            x=pre_layers[1]\n        else:\n            x=pre_layers[0][:,-1,:]\n        x=BatchNormalization()(x)\n        x=Dropout(0.1)(x)\n    if final_activation:\n        outs=Dense(30,activation='sigmoid')(x)\n    else:\n        outs=Dense(30)(x)\n    if not PRE_NAME.startswith('roberta'):\n        model=tf.keras.models.Model(inputs={'input_ids':ins1,'attention_mask':ins2,'token_type_ids':ins3},outputs=outs)\n    else:\n        model=tf.keras.models.Model(inputs={'input_ids':ins1,'attention_mask':ins2},outputs=outs)\n    return model","9c3fec83":"def DOUBLE_MODEL(sequence=False,final_activation=True,hidden_states=True):\n    tf.keras.backend.clear_session()\n    if PRE_NAME.startswith('bert'):\n        ins1=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins2=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins3=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins4=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins5=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins6=Input((MAX_LENGTH,),dtype=tf.int32)\n        pre_model=TFBertModel.from_pretrained(PRE_NAME,output_hidden_states=hidden_states,return_dict=True)\n        pre_layers1=pre_model({'input_ids':ins1,'attention_mask':ins2,'token_type_ids':ins3})\n        pre_layers2=pre_model({'input_ids':ins4,'attention_mask':ins5,'token_type_ids':ins6})\n    elif PRE_NAME.startswith('xlnet'):\n        ins1=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins2=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins3=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins4=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins5=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins6=Input((MAX_LENGTH,),dtype=tf.int32)\n        pre_model=TFXLNetModel.from_pretrained(PRE_NAME,output_hidden_states=hidden_states,return_dict=True)\n        pre_layers1=pre_model({'input_ids':ins1,'attention_mask':ins2,'token_type_ids':ins3})\n        pre_layers2=pre_model({'input_ids':ins4,'attention_mask':ins5,'token_type_ids':ins6})\n    else:\n        ins1=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins2=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins3=Input((MAX_LENGTH,),dtype=tf.int32)\n        ins4=Input((MAX_LENGTH,),dtype=tf.int32)\n        pre_model=TFRobertaModel.from_pretrained(PRE_NAME,output_hidden_states=hidden_states,return_dict=True)\n        pre_layers1=pre_model({'input_ids':ins1,'attention_mask':ins2})\n        pre_layers2=pre_model({'input_ids':ins3,'attention_mask':ins4})\n    if sequence:\n        x1=Conv1D(1,1)(pre_layers1[0])\n        x1=tf.squeeze(x1,axis=-1)\n        x1=BatchNormalization()(x1)\n        x1=Dropout(0.1)(x1)\n        x1=tf.keras.layers.ReLU()(x1)\n        x2=Conv1D(1,1)(pre_layers2[0])\n        x2=tf.squeeze(x2,axis=-1)\n        x2=BatchNormalization()(x2)\n        x2=Dropout(0.1)(x2)\n        x2=tf.keras.layers.ReLU()(x2)\n        x=tf.keras.layers.Concatenate()([x1,x2])\n    elif hidden_states:\n        if not PRE_NAME.startswith('xlnet'):\n            x1=tf.stack([layer[:,0,:] for layer in pre_layers1[2]],axis=-1)\n            x1=tf.squeeze(Conv1D(1,1)(x1),axis=-1)\n            x1=BatchNormalization()(x1)\n            x1=Dropout(0.1)(x1)\n            x1=tf.keras.layers.ReLU()(x1)\n            x2=tf.stack([layer[:,0,:] for layer in pre_layers2[2]],axis=-1)\n            x2=tf.squeeze(Conv1D(1,1)(x2),axis=-1)\n            x2=BatchNormalization()(x2)\n            x2=Dropout(0.1)(x2)\n            x2=tf.keras.layers.ReLU()(x2)\n        else:\n            x1=tf.stack([layer[:,-1,:] for layer in pre_layers1[2]],axis=-1)\n            x1=tf.squeeze(Conv1D(1,1)(x1),axis=-1)\n            x1=BatchNormalization()(x1)\n            x1=Dropout(0.1)(x1)\n            x1=tf.keras.layers.ReLU()(x1)\n            x2=tf.stack([layer[:,-1,:] for layer in pre_layers2[2]],axis=-1)\n            x2=tf.squeeze(Conv1D(1,1)(x2),axis=-1)\n            x2=BatchNormalization()(x2)\n            x2=Dropout(0.1)(x2)\n            x2=tf.keras.layers.ReLU()(x2)\n        x=tf.keras.layers.Concatenate()([x1,x2])\n    else:\n        if not PRE_NAME.startswith('xlnet'):\n            x1=pre_layers1[1]\n            x1=BatchNormalization()(x1)\n            x1=Dropout(0.1)(x1)\n            x2=pre_layers2[1]\n            x2=BatchNormalization()(x2)\n            x2=Dropout(0.1)(x2)\n        else:\n            x1=pre_layers1[0][:,-1,:]\n            x1=BatchNormalization()(x1)\n            x1=Dropout(0.1)(x1)\n            x2=pre_layers2[0][:,-1,:]\n            x2=BatchNormalization()(x2)\n            x2=Dropout(0.1)(x2)\n        x=tf.keras.layers.Concatenate()([x1,x2])\n    if final_activation:\n        outs=Dense(30,activation='sigmoid')(x)\n    else:\n        outs=Dense(30)(x)\n    if not PRE_NAME.startswith('roberta'):\n        model=tf.keras.models.Model(inputs=[{'input_ids1':ins1,'attention_mask1':ins2,'token_type_ids1':ins3},\n                                            {'input_ids2':ins4,'attention_mask2':ins5,'token_type_ids2':ins6}]\n                                    ,outputs=outs)\n    else:\n        model=tf.keras.models.Model(inputs=[{'input_ids1':ins1,'attention_mask1':ins2},\n                                            {'input_ids2':ins3,'attention_mask2':ins4}]\n                                    ,outputs=outs)\n    \n    return model","3ca6bbbd":"import tensorflow.keras.backend as K","4af8784e":"def custom_loss(y_true,y_pred):\n    y_true=tf.cast(y_true,dtype=y_pred.dtype)\n    bce=K.mean(-1*((y_true*K.log(y_pred+1e-5))+((1-y_true)*K.log(1-y_pred+1e-5))),axis=-1)\n    rmsle=K.sqrt(K.mean(K.square(K.log(y_true+1)-K.log(y_pred+1))))\n    total_loss=bce+rmsle\n    return K.mean(total_loss)","c13cb554":"lr=3e-5\nloss='binary_crossentropy'\nname='bert_4_hiddens_type2_bce'\nbatch_size=8","7c55f516":"batch_size","759d7a5d":"for i in range(5):\n    train=df[df['kfold']!=i].drop('kfold',axis=1).reset_index(drop=True)\n    test=df[df['kfold']==i].drop('kfold',axis=1).reset_index(drop=True)\n    model_inputs,model_outputs=get_final_model_inputs(train,h1,h2,inference=False)\n    print(\"\\nsome samples of training data\\n\")\n    print(model_inputs,model_outputs)\n    valid_inputs,valid_outputs=get_final_model_inputs(test,h1,h2,inference=False)\n    print(\"\\nsome samples of testing data\\n\")\n    print(valid_inputs,valid_outputs)\n    print('\\nsummary of the model\\n')\n    model=SINGLE_MODEL(sequence=False,final_activation=True,hidden_states=True,hidden_number=4)\n    model.summary()\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                 loss=loss)\n    early=tf.keras.callbacks.EarlyStopping(patience=3,monitor='val_loss',mode='min',verbose=1,restore_best_weights=True)\n    saver=tf.keras.callbacks.ModelCheckpoint(monitor='val_loss',mode='min',save_best_only=True,\n                                            save_weights_only=True,save_freq='epoch',filepath=f\"{name}_fold_{i}.h5\",verbose=1)\n    model.fit(model_inputs,model_outputs,epochs=20,batch_size=batch_size,\n              validation_data=(valid_inputs,valid_outputs),\n             callbacks=[early,saver])\n    print(f\"\\nloss for fold {i} we got {model.evaluate(valid_inputs,valid_outputs)}\\n\")\n    del model\n    import gc\n    gc.collect()","c9dad69e":"#when using single cls token as output\n# for 1st H1,H2 bert gave 0.35417 for bce loss and 0.1 dropout\n# roberta,xlnet gave around 0.35 only\n#for remaining H1,H2 score is aroung 0.34...  and less than bert score in 1st H1,H2only\n##################################################################","dd1528c3":"# after running bert base. the bench mark decided is 0.36262","9fed085e":"#for h1=[['question_title', 'question_body'],['question_title','answer']]\n#h2=[]\n#using 4 hidden states\n# bert,0.1,3e-5->0.35909\n# roberta,0.1,3e-5-> 0.35331\n# roberta,0.1,1e-5-> 0.35096\n# sequence\n# bert,0.1,3e-5->0.28393","17852e39":"#for h1=[['question_title', 'question_body'],['answer']]\n#h2=[]\n# using 4 hidden states\n#bert,3e-5,0.1->0.36262\n#roberta,3e-5,0.1->0.35200\n#xlnet,3e-5,0.1->0.35375","c29dcfb7":"#h1=[['question_title', 'question_body'],['question_body','answer']]\n#h2=[]\n#bert with 4 hidden states-> 0.33143","dacbbfea":"#h1=[['question_body'],['question_title','answer']]\n#h2=[]\n#using 4 hidden states\n#bert,3e-5,0.1->0.35709","4e20cd03":"#h1=[['question_title'],['question_body','answer']]\n#h2=[]\n#using 4 hidden states\n#bert,3e-5,0.1->0.33911","1a5c4a71":"# bert related input","ce03f861":"# roberta related input","8ab939c8":"# single head model","99454167":"# xlnet related input","018c4494":"# input types for models\n \n title&q,title&a\n \n title&q,a\n \n title,q&  title,a\n \n title,q&  q,a ","958168ac":"# models","0b602916":"# models\nbert,roberta,xlnet","a1c86b11":"# double head model","81ea2cea":"# import libraries"}}