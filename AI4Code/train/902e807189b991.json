{"cell_type":{"a336879f":"code","d4564934":"code","6e83f710":"code","96d5e8f5":"code","94e35be5":"code","aef01068":"code","054b3cab":"code","c0a40a84":"code","db7bfefc":"code","fd017c68":"code","1370c008":"code","98c2b60c":"code","a3027b25":"code","e04f37cd":"code","8d24d3ae":"code","f540327d":"code","6606e75d":"markdown","4a935a02":"markdown","522ffca7":"markdown","dfe274d5":"markdown","29e210df":"markdown","bfec16a3":"markdown","221f5e3b":"markdown","f71fcf29":"markdown","ccec46d9":"markdown","69ff1633":"markdown","2aee1c6e":"markdown","abccfc18":"markdown","e6607c13":"markdown","30c15ec5":"markdown","88859240":"markdown","354a61b7":"markdown","5653e4a1":"markdown","928328a5":"markdown","42f6681c":"markdown","1d203da4":"markdown","933be595":"markdown","238707b4":"markdown","7a9ed0b3":"markdown","671f884f":"markdown","6c45f328":"markdown","e377bad3":"markdown"},"source":{"a336879f":"import os\nimport pandas as pd\n\nPATH_TO_DATA = '..\/input'\n\ndf_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                             'train_features.csv'), \n                                    index_col='match_id_hash')\n\ndf_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                            'train_targets.csv'), \n                                   index_col='match_id_hash')\n\ndf_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                            'test_features.csv'), \n                                   index_col='match_id_hash')","d4564934":"X = df_train_features\ny = df_train_targets['radiant_win'].astype('int') #models prefer numbers instead of True\/False labels\ntest = df_test_features","6e83f710":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.25, random_state=17, \n                                                      shuffle=True, stratify=y)","96d5e8f5":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","94e35be5":"#we will use Sequential model, where we can add layers easily\nfrom keras.models import Sequential\n\n#now we import different layer types to use in our model\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization\n\n#we will have to optimize the model, so\nfrom keras import optimizers\n\n#tools to control overfitting\nfrom keras.callbacks import EarlyStopping","aef01068":"#is keras on gpu?\nfrom keras import backend as K\nK.tensorflow_backend._get_available_gpus()","054b3cab":"def auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc","c0a40a84":"model = Sequential() #we define the model\n\n#first layer\nmodel.add(Dense(512,input_dim=X_train.shape[1])) #basicly,245x512 matrix of weights\nmodel.add(BatchNormalization()) #makes NN learning easier\nmodel.add(Activation('relu')) #applying non-linear transformation\nmodel.add(Dropout(0.2)) #say no to overfitting\n\n#second layer\nmodel.add(Dense(256)) #here we don't have to specify its input_dim, it knows it from previous layer automaticly\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\n#third layer\nmodel.add(Dense(256)) #here we don't have to specify its input_dim, it knows it from previous layer automaticly\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\n#you can experiment with the number of layers as well...\n\n#now the final layer to convert everything to one number(probability)\nmodel.add(Dense(1, activation='sigmoid'))","db7bfefc":"model.summary()","fd017c68":"#Adam optimizer, there others as well, like RMSProp\nadam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n\n#EarlyStop if things does not improve for some time\nearlystop = EarlyStopping(monitor=\"val_auc\", patience=20, verbose=1, mode='max')","1370c008":"#Telling the model what to do...\nimport tensorflow as tf\nmodel.compile(optimizer=adam,loss='binary_crossentropy',metrics = [auc])  ","98c2b60c":"history = model.fit(X_train, y_train, validation_data = (X_valid,y_valid),\\\n                       epochs=20,batch_size=64,verbose=1,callbacks=[earlystop])","a3027b25":"# Plot the loss and auc curves for training and validation \nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['auc'], color='b', label=\"Training auc\")\nax[1].plot(history.history['val_auc'], color='r',label=\"Validation auc\")\nlegend = ax[1].legend(loc='best', shadow=True)","e04f37cd":"#simply as it\npredictions = model.predict(test)[:,0]","8d24d3ae":"predictions","f540327d":"df_submission = pd.DataFrame({'radiant_win_prob': predictions}, \n                                 index=df_test_features.index)\nimport datetime\nsubmission_filename = 'submission_{}.csv'.format(\n    datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\ndf_submission.to_csv(submission_filename)\nprint('Submission saved to {}'.format(submission_filename))","6606e75d":"First, let's import our data. \n\nPS: we will be importing required packadges on the fly..","4a935a02":"## Prediction and Submition","522ffca7":"## 2) Prepairing data for NN\n\nNothing special in preparing data for NN, we can easily feed NN with our tabular data with numeric values. ","dfe274d5":"Hmm, validation loss looks funny at least).  We need to investigate more the learning curves and adjust the model properly. Andrew Ng explains learning curves in this [video](https:\/\/www.youtube.com\/watch?v=ISBGFY-gBug)","29e210df":"Keras NN model can be evaluated on lots of metrics with just passing the metric name, but it is not the case for ROC_AUC score, so we will define our own auc metric function:","bfec16a3":"## 1) What is a Neural Net?\n\nJust kidding, everyone knows it)\n\nHere is the definition from Wikipedia:\n\nA neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.\n\nBasicly, this picture shows the whole philosophy of Neural Nets\n![](https:\/\/www.neuraldesigner.com\/images\/deep_neural_network_big.png)\n\nSo we take some inputs, combine them with a some weights, apply some kind of non-linear transformation, then pass the data to next layer and so on...\n\nSome usefull resources:\n* [video by 3Blue1Brown](https:\/\/www.youtube.com\/watch?v=aircAruvnKk)\n* [www.deeplearning.ai](https:\/\/www.deeplearning.ai\/)\n* [cs231n Stanford Course](http:\/\/cs231n.github.io\/)\n* and many more...","221f5e3b":"Let's confirm dimensions visualy:","f71fcf29":"Looks realistic)","ccec46d9":"### Submition","69ff1633":"Let's use some short notations:","2aee1c6e":"Let's see them","abccfc18":"We will run the model using \"history\", so we will be able to see the performance by time","e6607c13":"Surely, you can play with number of epochs, batch size, callbacks and other params","30c15ec5":"That's all! Our data is ready)","88859240":"### Finally, it's time to define the model itself:","354a61b7":"Now, we will define optimizer and a callback:","5653e4a1":"## In this tutorial:\n* What is a Neural Net?\n* Prepairing data for NN\n* Defining NN model (introducing keras..)\n* Evaluation and Results\n* Submition","928328a5":"## 3) Defining NN model\n\nIn this tutorial we will implement Neural Network using Keras.","42f6681c":"That's all! Our model is ready! Lets see it once again:","1d203da4":"## 4) Evaluation and Results","933be595":"Now, after our model has been trained lets see its performance","238707b4":"We can run keras model on GPU to accelerate computations. To do so, firstly, we have to switch on GPU (it is in the settings, on the right side of the kaggle kernel) and check whether we are on GPU:","7a9ed0b3":"First, let's import keras packages...","671f884f":"**Keras** is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n\nMore info about keras is [here](https:\/\/keras.io\/)","6c45f328":"We will need to evaluate the NN model on a validation set to see the performance. So, let's divide our train data (X) to train and validation sets:","e377bad3":"#### Hope this tutorial was useful, feel free fork, experiment and upvote."}}