{"cell_type":{"3b1f93ca":"code","c4ec5654":"code","39727f75":"code","a63752ff":"code","e49b8ab6":"code","031dad19":"code","8633367d":"code","01067e7c":"code","d4582780":"code","129a5573":"code","35554f84":"code","1395cdd5":"code","4ba32543":"markdown","19b1ada1":"markdown","d9d3e29a":"markdown","9efc5dca":"markdown"},"source":{"3b1f93ca":"import re\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer","c4ec5654":"# parameters\nFILTER_STEM = True\nTRAIN_PORTION = 0.8\nRANDOM_STATE = 7","39727f75":"dataset_path = os.path.join(\"..\",\"input\" ,os.listdir(\"..\/input\")[0])\ndf = pd.read_csv(dataset_path, encoding=\"ISO-8859-1\", names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])","a63752ff":"%%time\ndecode_map = {0: -1, 2: 0, 4: 1}\ndf.target = df.target.apply(lambda x: decode_map[x])","e49b8ab6":"df.target.value_counts()","031dad19":"%%time\nstop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\ndef filter_stopwords(text):\n    text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", ' ', str(text).lower()).strip()\n    if FILTER_STEM:\n        return \" \".join([stemmer.stem(token) for token in text.split() if token not in stop_words])\n    else:\n        return \" \".join([token for token in text.split() if token not in stop_words])\ndf.text = df.text.apply(filter_stopwords)","8633367d":"%%time\nvectorizer = TfidfVectorizer()\nword_frequency = vectorizer.fit_transform(df.text)","01067e7c":"# for not stem\nlen(vectorizer.get_feature_names())","d4582780":"len(vectorizer.get_feature_names())","129a5573":"sample_index = np.random.random(df.shape[0])\nX_train, X_test = word_frequency[sample_index <= TRAIN_PORTION, :], word_frequency[sample_index > TRAIN_PORTION, :]\nY_train, Y_test = df.target[sample_index <= TRAIN_PORTION], df.target[sample_index > TRAIN_PORTION]\nprint(f\"shape of training set: X={X_train.shape}, Y={Y_train.shape}\")\nprint(f\"shape of test set: X={X_test.shape}, Y={Y_test.shape}\")","35554f84":"%%time\nclf = LogisticRegression(random_state=0, solver='saga', multi_class='multinomial').fit(X_train, Y_train)","1395cdd5":"Y_predit = clf.predict(X_test)\nsum(Y_predit == Y_test) \/ len(Y_test)","4ba32543":"It seems that there are no natural label, so it's a binary classification problem","19b1ada1":"## parameters","d9d3e29a":"preprocessing for label, we mark -1 as negative, 0 as natural and 1 as positive","9efc5dca":"split train and test data"}}