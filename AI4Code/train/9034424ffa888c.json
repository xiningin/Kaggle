{"cell_type":{"500a7ebb":"code","d685ce3a":"code","d9630bae":"code","751046b1":"code","479103c7":"code","a716bc2e":"code","f7ed94a4":"code","59a16f9f":"code","d29ad763":"code","bb823419":"code","8a4900df":"code","1c3746aa":"code","07ae2fbe":"code","24020ad7":"code","c6015695":"code","7e25bf02":"code","d79435c5":"code","5265368f":"code","0fc33a74":"code","2c36762b":"code","33b3c6a1":"code","11347f4c":"code","4ad2c97b":"code","3a14282c":"code","4841558a":"code","5bc0df29":"code","9685ee64":"code","090d5e41":"code","3b4d2c0f":"code","7dea8f36":"code","03799dfa":"code","dbfff92f":"code","aee7bb0c":"code","c1001443":"code","efa986ce":"code","277a9e72":"code","04ac152c":"code","b2b9d80f":"code","120943d9":"code","9191f013":"code","6a37fef9":"code","aff95506":"code","d4ffb055":"code","7b529688":"code","eb557ba8":"code","0e91340d":"code","29226a6b":"code","8c8ea9a5":"code","872c1592":"code","23aed00d":"code","7d8c7daa":"code","f4da2b40":"code","9ce4d269":"code","384d5290":"code","072a3cdc":"markdown"},"source":{"500a7ebb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d685ce3a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('dark_background')\nimport warnings\nwarnings.filterwarnings('ignore')","d9630bae":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","751046b1":"#import dftrain csv files\ndftrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndftrain","479103c7":"dftrain.info()","a716bc2e":"dftrain.describe().T","f7ed94a4":"#import dftest csv files\ndftest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndftest\n","59a16f9f":"dftrain.isna()","d29ad763":"dftrain.isna().sum(axis=0)","bb823419":"#dftrain.dropna(axis=0, inplace=True)","8a4900df":"#dftrain.isna().sum(axis=0)","1c3746aa":"dftest.isna().sum(axis=0)","07ae2fbe":"#dftest.dropna(axis=0, inplace=True)","24020ad7":"dftest.isna().sum(axis=0)","c6015695":"dftrain","7e25bf02":"dftest","d79435c5":"sns.barplot(x=\"SibSp\", y=\"Survived\", data=dftrain,palette=\"ch:.25\")\n\n#I won't be printing individual percent values for all of these.\nprint(\"Percentage of SibSp = 0 who survived:\", dftrain[\"Survived\"][dftrain[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 1 who survived:\", dftrain[\"Survived\"][dftrain[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 2 who survived:\", dftrain[\"Survived\"][dftrain[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)","5265368f":"dftrain","0fc33a74":"dftrain[\"Age\"] = dftrain[\"Age\"].fillna(-0.5)\ndftest[\"Age\"] = dftest[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ndftrain['AgeGroup'] = pd.cut(dftrain[\"Age\"], bins, labels = labels)\ndftest['AgeGroup'] = pd.cut(dftest[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.set(font_scale = 1)\nsns.barplot(x=\"AgeGroup\", y=\"Survived\" , palette=\"ch:.25\",data=dftrain)\nplt.show()","2c36762b":"dftrain[\"CabinBool\"] = (dftrain[\"Cabin\"].notnull().astype('int'))\ndftest[\"CabinBool\"] = (dftest[\"Cabin\"].notnull().astype('int'))\n\n\n","33b3c6a1":"dftrain","11347f4c":"dftest","4ad2c97b":"dftrain[\"Survived\"][dftrain[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100","3a14282c":"dftest","4841558a":"#calculate percentages of CabinBool vs. survived\nprint(\"Percentage of CabinBool = 1 who survived:\", dftrain[\"Survived\"][dftrain[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of CabinBool = 0 who survived:\", dftrain[\"Survived\"][dftrain[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\n","5bc0df29":"#draw a bar plot of CabinBool vs. survival\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=dftrain)\nplt.show()","9685ee64":"#we'll start off by dropping the Cabin feature since not a lot more useful information can be extracted from it.\ndftrain = dftrain.drop(['Cabin'], axis = 1)\ndftest = dftest.drop(['Cabin'], axis = 1)\n","090d5e41":"dftest","3b4d2c0f":"#we can also drop the Ticket feature since it's unlikely to yield any useful information\ndftrain = dftrain.drop(['Ticket'], axis = 1)\ndftest = dftest.drop(['Ticket'], axis = 1)","7dea8f36":"dftest","03799dfa":"#now we need to fill in the missing values in the Embarked feature\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = dftrain[dftrain[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n","dbfff92f":"print(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = dftrain[dftrain[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n","aee7bb0c":"print(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = dftrain[dftrain[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)\n","c1001443":"#replacing the missing values in the Embarked feature with S\ndftrain = dftrain.fillna({\"Embarked\": \"S\"})","efa986ce":"dftrain.info()","277a9e72":"#create a combined group of both datasets\ncombine = [dftrain, dftest]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(dftrain['Title'], dftrain['Sex'])","04ac152c":"#replace various titles with more common names\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ndftrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","b2b9d80f":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ndftrain.head()","120943d9":"# fill missing age with mode age group for each title\nmr_age = dftrain[dftrain[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = dftrain[dftrain[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = dftrain[dftrain[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = dftrain[dftrain[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = dftrain[dftrain[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = dftrain[dftrain[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\n\nfor x in range(len(dftrain[\"AgeGroup\"])):\n    if dftrain[\"AgeGroup\"][x] == \"Unknown\":\n        dftrain[\"AgeGroup\"][x] = age_title_mapping[dftrain[\"Title\"][x]]\n        \nfor x in range(len(dftest[\"AgeGroup\"])):\n    if dftest[\"AgeGroup\"][x] == \"Unknown\":\n        dftest[\"AgeGroup\"][x] = age_title_mapping[dftest[\"Title\"][x]]","9191f013":"#map each Age value to a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ndftrain['AgeGroup'] = dftrain['AgeGroup'].map(age_mapping)\ndftest['AgeGroup'] = dftest['AgeGroup'].map(age_mapping)\n\ndftrain.head()","6a37fef9":"#dropping the Age feature for now, might change\ndftrain = dftrain.drop(['Age'], axis = 1)\ndftest = dftest.drop(['Age'], axis = 1)","aff95506":"dftrain","d4ffb055":"dftest","7b529688":"#drop the name feature since it contains no more useful information.\ndftrain = dftrain.drop(['Name'], axis = 1)\ndftest = dftest.drop(['Name'], axis = 1)\n","eb557ba8":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ndftrain['Sex'] = dftrain['Sex'].map(sex_mapping)\ndftest['Sex'] = dftest['Sex'].map(sex_mapping)\n","0e91340d":"\n\n#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ndftrain['Embarked'] = dftrain['Embarked'].map(embarked_mapping)\ndftest['Embarked'] = dftest['Embarked'].map(embarked_mapping)\ndftrain.head()","29226a6b":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(dftest[\"Fare\"])):\n    if pd.isnull(dftest[\"Fare\"][x]):\n        pclass = dftest[\"Pclass\"][x] #Pclass = 3\n        dftest[\"Fare\"][x] = round(dftrain[dftrain[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        ","8c8ea9a5":"#map Fare values into groups of numerical values\ndftrain['FareBand'] = pd.qcut(dftrain['Fare'], 4, labels = [1, 2, 3, 4])\ndftest['FareBand'] = pd.qcut(dftest['Fare'], 4, labels = [1, 2, 3, 4])","872c1592":"#drop Fare values\ndftrain = dftrain.drop(['Fare'], axis = 1)\ndftest = dftest.drop(['Fare'], axis = 1)","23aed00d":"print(f\"Dataset Terining:\",dftrain)\nprint(\"______________________________________\")\nprint(\"______________________________________\")\nprint(f\"Dataset Testing:\",dftest)","7d8c7daa":"from sklearn.model_selection import train_test_split\n\npredictors = dftrain.drop(['Survived', 'PassengerId'], axis=1)\ntarget = dftrain[\"Survived\"]\nX_train, X_test, Y_train, Y_test  = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","f4da2b40":"X_train","9ce4d269":"\ndef models(X_train,Y_train):\n  \n  #Using Logistic Regression Algorithm to the Training Set\n  from sklearn.linear_model import LogisticRegression\n  log = LogisticRegression(random_state = 0)\n  log.fit(X_train, Y_train)\n  \n  #Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm\n  from sklearn.neighbors import KNeighborsClassifier\n  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n  knn.fit(X_train, Y_train)\n    \n  #Using SVC method of svm class to use Support Vector Machine Algorithm\n  from sklearn.svm import SVC\n  svc_lin = SVC(kernel = 'linear', random_state =0)\n  svc_lin.fit(X_train, Y_train)\n\n  #Using SVC method of svm class to use Kernel SVM Algorithm\n  from sklearn.svm import SVC\n  svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n  svc_rbf.fit(X_train, Y_train)\n\n  #Using GaussianNB method of na\u00efve_bayes class to use Na\u00efve Bayes Algorithm\n  from sklearn.naive_bayes import GaussianNB\n  gauss = GaussianNB()\n  gauss.fit(X_train, Y_train)\n\n  #Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm\n  from sklearn.tree import DecisionTreeClassifier\n  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n  tree.fit(X_train, Y_train)\n\n  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm\n  from sklearn.ensemble import RandomForestClassifier\n  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 10)\n  forest.fit(X_train, Y_train)\n  \n  \n  #print model accuracy on the training data.\n  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train))\n  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train))\n  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train))\n  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train))\n  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train))\n  print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train))\n  print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train))\n  \n  return log, knn, svc_lin, svc_rbf, gauss, tree, forest\n\nmodel = models(X_train,Y_train)","384d5290":"#Show the confusion matrix and accuracy for all of the models on the test data\n#Classification accuracy is the ratio of correct predictions to total predictions made.\nfrom sklearn.metrics import confusion_matrix\nfor i in range(len(model)):\n  cm = confusion_matrix(Y_test, model[i].predict(X_test))\n  TN = cm[0][0]\n  TP = cm[1][1]\n  FN = cm[1][0]\n  FP = cm[0][1]\n  print(cm)\n  print('Model[{}] Testing Accuracy = \"{}!\"'.format(i,  (TP + TN) \/ (TP + TN + FN + FP)))\n  print()# Print a new line\n\n#Show other ways to get the classification accuracy & other metrics \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nfor i in range(len(model)):\n  print('Model ',i)\n  #Check precision, recall, f1-score\n  print( classification_report(Y_test, model[i].predict(X_test)) )\n  #Another way to get the models accuracy on the test data\n  print( accuracy_score(Y_test, model[i].predict(X_test)))\n  print()#Print a new line\n\n    \n#Print Prediction of Random Forest Classifier model\npred = model[6].predict(X_test)\nprint(pred)\n#Print a space\nprint()\n#Print the actual values\nprint(Y_test)\n","072a3cdc":"# Titanic - Machine Learning from Disaster\n \n### We uesd 6 algorithms \n \n* Logistic Regression \n* K Nearest Neighbor \n* Support Vector Machine (Linear) \n* Support Vector Machine (RBF) \n* Gaussian Naive Bayes \n* Decision Tree \n* Random Forest\n\n\n## Data Overview\n\n#### The data has been split into two groups:\n\n* training set (train.csv)\n* test set (test.csv)\n\nDataset link\n\nhttps:\/\/www.kaggle.com\/c\/titanic\/data"}}