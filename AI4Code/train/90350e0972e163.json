{"cell_type":{"fa5f60df":"code","7205e951":"code","f61a727e":"code","45f80cbe":"code","9dbd1520":"code","a66774fb":"code","ecc8327c":"code","05f930d1":"code","b40b8e9b":"code","c8223304":"code","2409d1b1":"code","810c578e":"code","86944c80":"code","497f2226":"code","df7f11ad":"markdown","ed987fa1":"markdown","5fd8b6f5":"markdown","d45d11d9":"markdown"},"source":{"fa5f60df":"%load_ext autoreload\n%autoreload 2\n\n# Basic Library\nimport pandas as pd\nimport pandas.io.sql as psql\nimport numpy as np\nimport numpy.random as rd\nimport gc\nimport multiprocessing as mpa\nimport os\nimport sys\nimport pickle\nfrom collections import defaultdict\nfrom glob import glob\nimport math\nfrom datetime import datetime as dt\nfrom pathlib import Path\nimport scipy.stats as st\nimport re\nfrom scipy.stats.stats import pearsonr\nfrom itertools import combinations\n\n# Matplotlib\nimport matplotlib\nfrom matplotlib import font_manager\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib import rc\n\nfrom matplotlib import animation as ani\nfrom IPython.display import Image\n\nfrom joblib import Parallel, delayed\nimport multiprocessing as mp\n\nplt.rcParams[\"patch.force_edgecolor\"] = True\n#rc('text', usetex=True)\nfrom IPython.display import display # Allows the use of display() for DataFrames\nimport seaborn as sns\nsns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\nsns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\nred = sns.xkcd_rgb[\"light red\"]\ngreen = sns.xkcd_rgb[\"medium green\"]\nblue = sns.xkcd_rgb[\"denim blue\"]\n\npd.set_option(\"display.max_colwidth\", 100)\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = '{:,.5f}'.format\n\n%matplotlib inline\n#%config InlineBackend.figure_format='retina'","7205e951":"# for gaussian process\nfrom scipy.optimize import minimize\nimport celerite\nfrom celerite import terms\nfrom celerite.modeling import Model","f61a727e":"def get_data_for_gp(df_pb, offset = 11):\n    # original code of this part of code is CPMP's following notebook\n    # https:\/\/github.com\/jfpuget\/Kaggle_PLAsTiCC\/blob\/master\/code\/celerite_003.ipynb\n    x_min = df_pb.mjd.min()\n    x_max = df_pb.mjd.max()\n\n    yerr_mean = df_pb.flux_err.mean()\n    x = df_pb.mjd.values\n    y = df_pb.flux.values\n    yerr = df_pb.flux_err\n\n    x = np.concatenate((np.linspace(x_min-250, x_min -200, offset), x, np.linspace(x_max+200, x_max+250, offset),))\n    y = np.concatenate((np.random.randn(offset) * yerr_mean, y, np.random.randn(offset) * yerr_mean))\n    yerr = np.concatenate((yerr_mean * np.ones(offset), yerr, yerr_mean * np.ones(offset) ))\n    return x, y, yerr\n\n\noptimizer_list = [\"L-BFGS-B\",\n                  \"Nelder-Mead\",\n                  \"Powell\",\n                  \"CG\",\n                  \"BFGS\",\n                  \"Newton-CG\",\n                  \"TNC\",\n                  \"COBYLA\",\n                  \"SLSQP\",\n                  \"dogleg\",\n                  \"trust-ncg\",]\n\ndef neg_log_like(params, y, gp):\n    gp.set_parameter_vector(params)\n    return -gp.log_likelihood(y)\n\ndef grad_neg_log_like(params, y, gp):\n    gp.set_parameter_vector(params)\n    return -gp.grad_log_likelihood(y)[1]\n\ndef build_gp_model(x, y, yerr, n_param = 2):\n    log_sigma = 0\n    log_rho = 0\n    eps = 0.001\n    bounds = dict(log_sigma=(-15, 15), log_rho=(-15, 15))\n    kernel = terms.Matern32Term(log_sigma=log_sigma, \n                                log_rho=log_rho, \n                                eps=eps, \n                                bounds=bounds)\n\n    gp = celerite.GP(kernel, mean=0)\n    gp.compute(x, yerr) \n\n    initial_params = gp.get_parameter_vector()\n    bounds = gp.get_parameter_bounds()\n\n    # depend on a combination of optimizer and dataset, minimize function throw exception,\n    # so trying all type of method.\n    for opt in optimizer_list:\n        try:\n            r = minimize(neg_log_like, \n                         initial_params, \n                         jac=grad_neg_log_like, \n                         method=opt, #\"L-BFGS-B\", \n                         bounds=bounds, \n                         args=(y, gp))\n            return gp\n\n        except Exception as e:\n            pass\n    raise Exception(\"[build_gp_model] can`t optimize\")","45f80cbe":"ZIP = False\nif ZIP:\n    train = pd.read_csv('..\/input\/training_set.csv.zip', compression='zip')\nelse:\n    train = pd.read_csv('..\/input\/training_set.csv')\n    \nmeta_train = pd.read_csv('..\/input\/training_set_metadata.csv')\nmeta_train[\"gal\"] = (meta_train['hostgal_specz'] == 0).astype(int)\nclasses = np.sort(meta_train.target.unique())\n########################################\nn_row = 2 # with changing this variable, you can set the number of graphs for each target showing in this notebook.\n########################################\nimport numpy.random as rd\nrd.seed(71)\ndf_list = []\nfor u in np.sort(meta_train.target.unique()):\n    df_list.append(meta_train[meta_train.target==u].sample(frac=1).iloc[:n_row,:]) # with shuffle\ndf_target_short = pd.concat(df_list)","9dbd1520":"def draw_gp_result(object_id_list):\n    for object_id in object_id_list:\n        meta = meta_train[meta_train.object_id==object_id]\n        df = train[train.object_id==object_id]\n\n        for pb in range(6):\n            df_pb = df[df.passband == pb]\n\n            flux_err_mean = df_pb.flux_err.mean()\n            flux_err_std = df_pb.flux_err.std()\n\n            # using flux_err within mean\u00b16std\n            df_pb = df_pb[df_pb.flux_err <= flux_err_mean + 6*flux_err_std]\n\n            x, y, yerr = get_data_for_gp(df_pb)\n            gp = build_gp_model(x, y, yerr)\n\n            # graph drowing\n            colors = [\"r\",\"b\", \"g\", \"k\", \"purple\", \"orange\"]\n            n_xx = 800\n            gp_xx = np.linspace(x.min(), x.max(), n_xx)\n            mu, var = gp.predict(y, gp_xx, return_var=True) \n            # N\/A interpolation\n            mu = np.interp(gp_xx, gp_xx[~np.isnan(mu)], mu[~np.isnan(mu)], )\n            \n            # Draw Graph\n            ax = plt.subplot(111)\n            df_pb.plot.scatter(\"mjd\", \"flux\", c=colors[pb], figsize=(18,4), ax=ax) #cmap=cm.rainbow,\n            ax.errorbar(df_pb.mjd, df_pb.flux, df_pb.flux_err, lw=1, fmt=\"none\", ecolor=\"k\", zorder=-1, label=\"flux_err\")\n            plt.plot(gp_xx, mu, color=\"gray\")\n\n            plt.fill_between(gp_xx, mu+np.sqrt(var), mu-np.sqrt(var), color=\"orange\", alpha=0.3, edgecolor=\"none\")\n            plt.title(f\"oid:{object_id}, passband:{pb}, class:{meta.target.values[0]}\")\n            plt.show()","a66774fb":"for c in classes:\n    print(\"=\"*30, f\" class_{c} \", \"=\"*30)\n    oid_list = df_target_short[df_target_short.target==c].object_id.values\n    draw_gp_result(oid_list)","ecc8327c":"\ndef applyParallel(dfGrouped, func):\n    retLst = Parallel(n_jobs=mp.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n    return pd.concat(retLst)\n\ndef gp_features(df):\n    prefix = \"gp001:\"\n    feature = {}\n    feature[\"object_id\"] = [df.object_id.values[0]]\n\n    mu_interp_list = []\n    for pb in range(6):\n        df_pb = df[df.passband == pb]\n\n        flux_err_mean = df_pb.flux_err.mean()\n        flux_err_std = df_pb.flux_err.std()\n\n        df_pb = df_pb[df_pb.flux_err <= flux_err_mean + 6*flux_err_std]\n        x, y, yerr = get_data_for_gp(df_pb)\n\n        try:\n            gp = build_gp_model(x, y, yerr)\n\n            n_xx = 800\n            dx = (x.max() - x.min()) \/ n_xx\n            gp_xx = np.linspace(x.min(), x.max(), n_xx)\n            mu, var = gp.predict(y, gp_xx, return_var=True) \n            mu = np.interp(gp_xx, gp_xx[~np.isnan(mu)], mu[~np.isnan(mu)], )\n            mu_interp_list.append(mu)    \n\n            feature[f\"{prefix}mean_var_pb{pb}\"] = [np.mean(var)]\n            feature[f\"{prefix}skew_pb{pb}\"] = [st.skew(mu)]\n            feature[f\"{prefix}range_pb{pb}\"] = [mu.max() - mu.min()]\n\n            mu_slope = pd.Series(np.diff(mu) \/ dx).rolling(window=30).mean()\n            feature[f\"{prefix}slope_max_pb{pb}\"] = [mu_slope.max()]\n            feature[f\"{prefix}slope_min_pb{pb}\"] = [mu_slope.min()]\n\n        except Exception as e:\n            print(e)\n            mu_interp_list.append(np.zeros_like(gp_xx))\n\n            feature[f\"{prefix}skew_pb{pb}\"] = [np.nan]\n            feature[f\"{prefix}range_pb{pb}\"] = [np.nan]\n            feature[f\"{prefix}slope_max_pb{pb}\"] = [np.nan]\n            feature[f\"{prefix}slope_min_pb{pb}\"] = [np.nan]\n\n    for c1, c2 in combinations(range(6), 2):\n        ratio = mu_interp_list[c1] \/ mu_interp_list[c2]\n        ratio = ratio[~np.isnan(ratio)]\n        feature[f\"{prefix}ratio_pb{c1}_{c2}\"] = [np.mean(ratio)]\n\n        feature[f\"{prefix}corr_pb{c1}_{c2}\"] = [pearsonr(mu_interp_list[c1], mu_interp_list[c2])[0]]\n\n    return pd.DataFrame(feature)\n\nimport pickle\ndef unpickle(filename):\n    with open(filename, 'rb') as fo:\n        p = pickle.load(fo)\n    return p\n\ndef to_pickle(filename, obj):\n    with open(filename, 'wb') as f:\n        pickle.dump(obj, f, -1)","05f930d1":"# Test one object\noid = 23822\ndf = train[train.object_id==oid]\ndf_gp_feat = gp_features(df)","b40b8e9b":"df_gp_feat","c8223304":"%%time\n# all object ids\ndf_gp_feature = applyParallel(train.groupby(\"object_id\"), gp_features)","2409d1b1":"df_gp_feature.head()","810c578e":"df_gp_feature = df_gp_feature.merge(meta_train, on=\"object_id\", how=\"left\")\ndf_gp_feature_dropna = df_gp_feature.replace(np.inf, np.nan).replace(-np.inf, np.nan).dropna()","86944c80":"upper_limit = 99999999\ndf_gp_feature_dropna[\"gp001:ratio_pb1_3\"] = np.where(np.abs(df_gp_feature_dropna[\"gp001:ratio_pb1_3\"])>upper_limit, \n                                                     np.sign(df_gp_feature_dropna[\"gp001:ratio_pb1_3\"])*upper_limit, \n                                                     df_gp_feature_dropna[\"gp001:ratio_pb1_3\"])","497f2226":"for i, c in enumerate(df_gp_feature_dropna.loc[:,df_gp_feature_dropna.columns.str.startswith(\"gp001:\")]):\n    try:\n        print(c)\n        df_gp_feature_dropna[c] = np.where(np.abs(df_gp_feature_dropna[c]) > 99999999, \n                                                     np.sign(df_gp_feature_dropna[c])*99999999, \n                                                     df_gp_feature_dropna[c])\n        \n        plt.figure(figsize=[20, 4])\n        plt.subplot(1, 2, 1)\n        sns.violinplot(x='target', y=c, data=df_gp_feature_dropna)\n        plt.grid()\n\n        plt.subplot(1, 2, 2)\n        sns.distplot(df_gp_feature_dropna[c], kde=False)\n        plt.yscale('log')\n        plt.legend(['train', 'test'])\n        plt.grid()\n        plt.show();\n    except Exception as e:\n        print(e)","df7f11ad":"# EDA","ed987fa1":"# Visualization\n","5fd8b6f5":"In this kernel, I\u2019m sharing my Gaussian Process Feature Engineering method with celerite used on my 19th place solution ( https:\/\/www.kaggle.com\/c\/PLAsTiCC-2018\/discussion\/75167 ) \n\nGaussian Process is used to interpolate flux curve, then extract features from the light curves. Some of Gaussian Process model tuning refinement from my original code was done with reference @CPMP 's solution as the following.\n\nThanks @CPMP.\nCPMP\u2019s Solution detail: https:\/\/www.kaggle.com\/c\/PLAsTiCC-2018\/discussion\/75050  \nCPMP\u2019s Github repo:  https:\/\/github.com\/jfpuget\/Kaggle_PLAsTiCC  \n\nRemaining issue:  Some of the flux max can not be captured with current implementation of gaussian process curve.\n","d45d11d9":"# Create Features"}}