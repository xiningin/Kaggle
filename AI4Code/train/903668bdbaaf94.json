{"cell_type":{"79c63520":"code","bcc44721":"code","7e486ab9":"code","3485f1bf":"code","f0e5d0f3":"code","23c807d9":"code","b0904756":"code","626972b3":"code","0732fef1":"markdown","6fcc837b":"markdown","62e15fab":"markdown","cec1ecab":"markdown","009bec2f":"markdown","3cf380db":"markdown","b873ec05":"markdown","0e21cc67":"markdown","6bc5a602":"markdown","cfc95f3b":"markdown"},"source":{"79c63520":"import numpy as np\nimport pandas as pd\n\ntry:\n    dtrain = pd.read_parquet('dtrain.parquet')\nexcept:\n    dtrain = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv', index_col='ts_id')\n    dtrain = dtrain.astype({c: np.float32 for c, t in dtrain.dtypes.items() if t == np.float64})\n    dtrain.to_parquet('dtrain.parquet')","bcc44721":"dlabels = dtrain.filter(regex='resp')\ndtrain = dtrain.drop(dlabels.columns, axis=1)\n\nprint(dtrain.columns)\nprint(dlabels.columns)","7e486ab9":"from sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(\n    dtrain,\n    dlabels,\n    test_size=.25,\n    random_state=1,\n    shuffle=False\n)\n\nprint(x_train.shape, x_test.shape)\nprint(x_train.index)","3485f1bf":"from sklearn.model_selection import KFold\n\nfor train_idx, test_idx in KFold().split(x_train):\n    #print(train, test)\n    print(x_train.loc[train_idx, 'date'].index)\n    print(x_train.loc[test_idx, 'date'].index)\n    break","f0e5d0f3":"from sklearn.model_selection import TimeSeriesSplit\n\nfor train_idx, test_idx in TimeSeriesSplit().split(x_train):\n    print(x_train.loc[train_idx, 'date'].unique())\n    print(x_train.loc[test_idx, 'date'].unique())\n    break","23c807d9":"from sklearn.model_selection import GroupKFold\n\nfor train_idx, test_idx in GroupKFold().split(x_train, groups=x_train['date']):\n    print(x_train.loc[train_idx, 'date'].unique())\n    print(x_train.loc[test_idx, 'date'].unique())\n    break","b0904756":"from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# https:\/\/github.com\/getgaurav2\/scikit-learn\/blob\/d4a3af5cc9da3a76f0266932644b884c99724c57\/sklearn\/model_selection\/_split.py#L2243\nclass GroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Provides train\/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_size : int, default=None\n        Maximum size for a single training set.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n                           'b', 'b', 'b', 'b', 'b',\\\n                           'c', 'c', 'c', 'c',\\\n                           'd', 'd', 'd'])\n    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n                  \"TEST GROUP:\", groups[test_idx])\n    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n    TEST GROUP: ['c' 'c' 'c' 'c']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n    TEST: [15, 16, 17]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n    TEST GROUP: ['d' 'd' 'd']\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_size=None\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_size = max_train_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train\/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n        group_test_size = n_groups \/\/ n_folds\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n            for train_group_idx in unique_groups[:group_test_start]:\n                train_array_tmp = group_dict[train_group_idx]\n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n            train_end = train_array.size\n            if self.max_train_size and self.max_train_size < train_end:\n                train_array = train_array[train_end -\n                                          self.max_train_size:train_end]\n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n            yield [int(i) for i in train_array], [int(i) for i in test_array]","626972b3":"for train_idx, test_idx in GroupTimeSeriesSplit().split(x_train, groups=x_train['date']):\n    print(x_train.loc[train_idx, 'date'].unique())\n    print(x_train.loc[test_idx, 'date'].unique())","0732fef1":"The [GroupKFold](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#group-k-fold) iterator does respect groupings: no group will ever be part of two folds. Unfortunately, it is also clear that it mixes up the order completely and thus loses the temporal dimension again. What we need is a a crossover between `GroupKFold` and `TimeSeriesSplit`: `GroupTimesSeriesSplit`.","6fcc837b":"### GroupKFold\n\n![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_0051.png)","62e15fab":"Use of `shuffle=False` is key here; since otherwise we would lose all chronological order.\n\nHowever, this approach is problematic because by constantly verifying on the same data, we also slowly start to overfit on the test set (\"leakage\"). Splitting the test set again into a validation set could solve this: the model's hyper-parameters are tuned and verified on the validation set and once that is completely finished we test it (only once!) on the test set. The problem now becomes that either the test and validation sets become too small to be useful or that so much data is used to validate and test that nog enough data remains to train on.","cec1ecab":"Already in the first split we can see data from day `44` present in both the training and test set. That would mean that we are training on half of the trades of a certain day, just to validate their performance on the other half of the trades of that day. What we of course want is to train on all trades of a particular day, and to validate them on the day that follows! Otherwise again leaking will occur.","009bec2f":"# How to validate a model on chronologically ordered data which also contains groups?\n\nSince it takes quite some time to get a utility (leaderboard) score back for our model, it would be nice to be able to 'locally' calculate an indication of a model's performance; independent of the (time expensive and limited) submission API. This would allow for much better tuning of hyper-parameters or other aspects of the model's training process.\n\nIn this notebook I want to lay out a couple of techniques that can be used to do this. For every step we will see that there is a problem with using it for this particular competition. Fortunately the last chapter provides a solution! If you are not interested in an introduction in test and validation techniques, then skip to the bottom. First up: train and test subsets.","3cf380db":"### GroupTimesSeriesSplit\n\nOK, so this iterator does not exist yet in scikit-learn. However, a request for it has been documented on GitHub over a year ago ([Feature request: Group aware Time-based cross validation #14257](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14257)) and is almost ready for release. Thanks to open source we can take a sneak peek already! \n\n**Do note that this is not fully reviewed yet!!!** This might be the final code that it will make it into `sklearn`'s version `0.24` as a major feature, but there's also a chance of bugs still being present.\n\nI did not write *any* of this but it did take me a good day of research and trying to write it myself. All credits go to [@getgaurav2](https:\/\/github.com\/getgaurav2\/).\n\nHere are some more attempts at grouped cross-validation I encountered in my research:\n- https:\/\/stackoverflow.com\/questions\/51963713\/cross-validation-for-grouped-time-series-panel-data\n- https:\/\/datascience.stackexchange.com\/questions\/77684\/time-series-grouped-cross-validation\n- https:\/\/nander.cc\/writing-custom-cross-validation-methods-grid-search","b873ec05":"### TimeSeriesSplit\n\nNote however, that in the first split shown above we are validating our *chronological* data on the past. We are training on trades starting from `358574` but testing on trades starting from `0`. In other words, our model has been trained using information which wasn't yet available at the time of the validation set. This is clear leakage; we are predicting the past with knowledge from the future. But our aim is to predict data in the future! This problem has already been addressed by scikit-learn in the form of [TimeSeriesSplit](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#time-series-split):\n![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_0101.png)\nBut what is the problem this time? `TimeSeriesSplit` does not respect the groups available in the data. Although not clearly visible in this plot, we can imagine that a group can partially fall in the training set and partially in the test set.","0e21cc67":"Chronological order is maintained and group separation is respected!","6bc5a602":"## Train and Test Subsets\n\nThe first obvious step is to set apart some data which the model never gets to see. After the model has been trained, we use that unseen data to verify our model's predicitions. Scikit-learn's [train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) makes the process of splitting datasets easy for us. Let's load our training data and set aside a test set:","cfc95f3b":"## Cross-validation\n\nIn cross-validation (CV), multiple validation sets are derived from the training set. Every *fold* a new part of the training set is used as the vaildation set, and the data previously used for validation now becomes part of the training set again:\n\n![img](https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png)\n\n(Source: [scikit-learn's User Guide, Ch. 3.1](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#cross-validation-evaluating-estimator-performance).)"}}