{"cell_type":{"c2c08e7d":"code","92f7f1de":"code","15821cd4":"code","750ffe44":"code","89c6f1ff":"code","413903b7":"code","f124c9e3":"code","8c9f2a95":"code","fa8db371":"code","f3d69e2a":"code","4c2cb30a":"code","921f859b":"code","3381415d":"code","6ac642e6":"code","4bb16884":"code","41a3ef4e":"code","8ddc0b62":"code","81cd0d58":"code","4a764ce4":"code","5d43f4f3":"code","772b3bf9":"code","86e915cc":"code","ca79d2f5":"code","9ddbff8b":"code","89598859":"code","9973e315":"code","d1d628c0":"code","d6107773":"markdown","3f800163":"markdown"},"source":{"c2c08e7d":"# IMPORT MODULES\nimport sys\nfrom os.path import join\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.python.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.python.keras.preprocessing.image import load_img, img_to_array\n#from tensorflow.python.keras.applications import ResNet50\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils, to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nfrom keras.applications import ResNet50\n\nimport os\nprint(os.listdir(\"..\/input\"))","92f7f1de":"PATH = \"..\/input\/melanoma\/dermmel\/DermMel\/\"\nprint(os.listdir(PATH))","15821cd4":"# Check content of the dirs\n\nPATHtrain = PATH + 'train_sep\/'\nprint(len(os.listdir(PATHtrain)), \" TRAIN Directories of photos\")\nLabels = os.listdir(PATHtrain)\nsig = 0\nfor label in sorted(Labels):\n    print(label,len(os.listdir(PATHtrain + label +'\/')))\n    sig = sig + len(os.listdir(PATHtrain + label +'\/'))\n\nprint(\"Total TRAIN photos \", sig)\nprint(\"_\"*50)\n\nPATHvalid = PATH + 'valid\/'\nprint(len(os.listdir(PATHvalid)), \" VALID Directories of photos\")\nLabels = os.listdir(PATHvalid)\nsig = 0\nfor label in sorted(Labels):\n    print(label,len(os.listdir(PATHvalid + label +'\/')))\n    sig = sig + len(os.listdir(PATHvalid + label +'\/'))\n\nprint(\"Total Validation photos \", sig)\nprint(\"_\"*50)\n\nPATHtest = PATH + 'test\/'\nprint(len(os.listdir(PATHtest)), \" TEST Directories of photos\")\nLabels = os.listdir(PATHtest)\nsig = 0\nfor label in sorted(Labels):\n    print(label,len(os.listdir(PATHtest + label +'\/')))\n    sig = sig + len(os.listdir(PATHtest + label +'\/'))\n\nprint(\"Total Testing photos \", sig)\nprint(\"_\"*50)","750ffe44":"# Check the photos and their labels \n\nTestNum = 77\ndiag = 'Melanoma'\n\nimage_dir = PATHtrain +'\/'+diag+'\/'\nimg_name = os.listdir(image_dir)[TestNum]\nimg_path = image_dir+str(img_name)\nimg = image.load_img(img_path, target_size=(224, 224))\nimgplot = plt.imshow(img)\nprint(\"TRAIN \",diag,\" photo number \", TestNum)\nplt.show()\n\nimage_dir = PATHvalid +'\/'+diag+'\/'\nimg_name = os.listdir(image_dir)[TestNum]\nimg_path = image_dir+str(img_name)\nimg = image.load_img(img_path, target_size=(224, 224))\nimgplot = plt.imshow(img)\nprint(\"VALID \",diag,\" photo number \", TestNum)\nplt.show()\n\nimage_dir = PATHtest +'\/'+diag+'\/'\nimg_name = os.listdir(image_dir)[TestNum]\nimg_path = image_dir+str(img_name)\nimg = image.load_img(img_path, target_size=(224, 224))\nimgplot = plt.imshow(img)\nprint(\"TEST \",diag,\" photo number \", TestNum)\nplt.show()\n","89c6f1ff":"# Convoluted Base MODEL\n\nconv_base = ResNet50(weights='imagenet',\ninclude_top=False,\ninput_shape=(224, 224, 3))\n\nprint(conv_base.summary())","413903b7":"# MODEL\n\nmodel = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(2, activation='sigmoid'))\n\nprint(model.summary())","f124c9e3":"# Make the conv_base NOT trainable:\n\nfor layer in conv_base.layers[:]:\n   layer.trainable = False\n\nprint('conv_base is now NOT trainable')","8c9f2a95":"for i, layer in enumerate(conv_base.layers):\n   print(i, layer.name, layer.trainable)","fa8db371":"# Compile frozen conv_base + my top layer\n\nmodel.compile(optimizer=optimizers.Adam(),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nprint(\"model compiled\")\nprint(model.summary())","f3d69e2a":"# Prep the Train Valid and Test directories for the generator\n\ntrain_dir = PATHtrain\nvalidation_dir = PATHvalid\ntest_dir = PATHtest\nbatch_size = 20\ntarget_size=(224, 224)\n\n#train_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   rotation_range=40,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                   vertical_flip=True,\n                                   fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,target_size=target_size,batch_size=batch_size)\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,target_size=target_size,batch_size=batch_size)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,target_size=target_size,batch_size=batch_size)","4c2cb30a":"print(train_generator.class_indices)\nprint(validation_generator.class_indices)\nprint(test_generator.class_indices)","921f859b":"# Short training ONLY my top layers \n#... so the conv_base weights will not be destroyed by the random intialization of the new weights\n\nhistory = model.fit_generator(train_generator,\n                              epochs=1,\n                              steps_per_epoch = batch_size \/\/ batch_size,\n                              validation_data = validation_generator,\n                              validation_steps = batch_size \/\/ batch_size)","3381415d":"# Make last block of the conv_base trainable:\n\nfor layer in conv_base.layers[:165]:\n   layer.trainable = False\nfor layer in conv_base.layers[165:]:\n   layer.trainable = True\n\nprint('Last block of the conv_base is now trainable')","6ac642e6":"for i, layer in enumerate(conv_base.layers):\n   print(i, layer.name, layer.trainable)","4bb16884":"# Compile frozen conv_base + UNfrozen top block + my top layer ... SLOW LR\n\nmodel.compile(optimizer=optimizers.Adam(lr=1e-5),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nprint(\"model compiled\")\nprint(model.summary())","41a3ef4e":"# Long training with fine tuning\n\n# history = model.fit_generator(train_generator,\n#                               epochs=50,\n#                               steps_per_epoch = 10682 \/\/ batch_size,\n#                               validation_data = validation_generator,\n#                               validation_steps = 3562 \/\/ batch_size)","8ddc0b62":"# acc = history.history['acc']\n# val_acc = history.history['val_acc']\n# loss = history.history['loss']\n# val_loss = history.history['val_loss']\n# epochs = range(1, len(acc) + 1)\n# plt.plot(epochs, acc, 'bo', label='Training acc')\n# plt.plot(epochs, val_acc, 'r', label='Validation acc')\n# plt.title('Training and validation accuracy')\n# plt.legend()\n# plt.figure()\n# plt.plot(epochs, loss, 'bo', label='Training loss')\n# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n# plt.title('Training and validation loss')\n# plt.legend()\n# plt.show()","81cd0d58":"# test_loss, test_acc = model.evaluate_generator(test_generator, steps= 3561 \/\/ batch_size, verbose=1)\n# print('test acc:', test_acc)","4a764ce4":"# SAVE or LOAD model (Keras - all batteries included: architecture, weights, optimizer, last status in training, etc.)\n# YOU supply this model.h5 file from previous training session(s) - expected as a data source by Kaggle\n\n# SAVE model\n# model.save('MelanomaResNet50FineTune.h5')\n# print(\"MelanomaResNet50FineTune.h5 was saved\")\n\n# LOAD model\n#del model\nfrom keras.models import load_model\n\nmodel = load_model('..\/input\/trained-model-resnet\/MelanomaResNet50FineTune.h5')\nprint(\"modelWeatherV10.h5 was loaded\")","5d43f4f3":"test_loss, test_acc = model.evaluate_generator(test_generator, steps= 3561 \/\/ batch_size, verbose=1)\nprint('test acc:', test_acc)","772b3bf9":"#!pip install tqdm\nfrom tqdm import tqdm\nscores=[]\nlabels = []\nfor b, (x_batch, y_batch) in enumerate(tqdm(test_generator)):\n    #print(x_batch,end=\"########\\n\")\n    #break\n    batch_scores = model.predict(x_batch)\n    scores.append(batch_scores)\n    #print(batch_scores)\n    labels.append(y_batch)\n    \n    if b >= len(test_generator):\n        break\n\nscores = np.concatenate(scores).squeeze()\nlabels = np.concatenate(labels).squeeze()","86e915cc":"print(labels[0])\nprint(scores[0])\nprint(labels[1])\nprint(scores[1])\nprint(labels[2])\nprint(scores[2])\nprint(labels[3])\nprint(scores[3])\n#y=np.concatenate([test_generator.next()[1] for i in range(test_generator.__len__())])\n#true_labels=np.argmax(y, axis=-1)\n#prediction= model.predict(test_generator, verbose=2)\n#prediction=np.argmax(prediction, axis=-1)","ca79d2f5":"adapted_labels=[x[0] for x in labels ]\nadapted_scores=[x[0] for x in scores ]","9ddbff8b":"print(adapted_scores[0])\nadapted_labels=np.array(adapted_labels)\nadapted_scores=np.array(adapted_scores)","89598859":"sorted_idx = np.argsort(adapted_scores)\nadapted_scores = adapted_scores[sorted_idx]\nadapted_labels = adapted_labels[sorted_idx]","9973e315":"from sklearn.metrics import confusion_matrix\n\ntprs, fprs = [], []\n#mydict={}\nfor threshold in tqdm(np.linspace(0, 1, 100)):\n    index = np.searchsorted(adapted_scores, threshold, side='right')\n    \n    y_pred = np.concatenate((\n        np.zeros(index),\n        np.ones(max(0, len(labels) - index))\n    ))\n    tn, fp, fn, tp = confusion_matrix(adapted_labels, y_pred).ravel()\n\n    tpr = tp \/ (tp + fn)\n    fpr = fp \/ (fp + tn)\n    #mydict[threshold]=tpr-fpr\n    tprs.append(tpr)\n    fprs.append(fpr)\n#max_key = max(mydict, key=mydict.get)\n#print(max_key)\n#for key,value in mydict.items():\n#    print(f\"{key}\\t{value}\")","d1d628c0":"from sklearn.metrics import auc\n\nfprs, tprs = np.array(fprs), np.array(tprs)\nidx = np.argsort(fprs)\nplt.plot(fprs[idx], tprs[idx], label='model')\nplt.plot([0, 1], [0, 1], label='random', ls='--')\nplt.title(f'AUC: {auc(fprs[idx], tprs[idx])}')\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.legend()\nplt.tight_layout()\nplt.show()","d6107773":"It\u2019s necessary to freeze the convolution base of the conv base in order to\nbe able to train a randomly initialized classifier on top. For the same reason, it\u2019s only\npossible to fine-tune the top layers of the convolutional base **once the classifier on top\nhas already been trained**. If the classifier isn\u2019t already trained, then the error signal\npropagating through the network during training will be too large, and the representations\npreviously learned by the layers being fine-tuned will be destroyed\n\n**Below, first train with no limit to lr - with conv_base frozen - only  my top layers**\n\n**Then, unfreeze last model conv block , recompile and train all with LOW lr=1e-5**","3f800163":"Classify pigmented skin lesions dermatoscopic images from HAM10k https:\/\/www.nature.com\/articles\/sdata2018161 into 7 diagnosis\n"}}