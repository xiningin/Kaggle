{"cell_type":{"78ed1dc5":"code","74291207":"code","c18326f0":"code","6e4b53cb":"code","ae81094d":"code","f12d5f56":"code","c6542360":"code","17b498c0":"code","0c529aec":"code","c127151c":"code","bb6c1c42":"code","40f17b83":"code","e34e2481":"code","57df6fd3":"code","b8a60559":"code","da62ee1e":"code","9e2239a7":"code","f4d2dcdf":"markdown","6608d5ae":"markdown","79a4e947":"markdown","bd1e56ef":"markdown","4adba1dd":"markdown","94376a8a":"markdown","d7200372":"markdown","9ade0730":"markdown","df09b5b3":"markdown","e8ada048":"markdown","aee8710d":"markdown"},"source":{"78ed1dc5":"!git clone https:\/\/github.com\/Megvii-BaseDetection\/YOLOX -q\n\n%cd YOLOX\n!pip install -U pip && pip install -r requirements.txt\n!pip install -v -e . \n!pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\n!pip install Cython","74291207":"%cd \/kaggle\/working\n\nfrom tqdm import tqdm\ntqdm.pandas()\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport ast\nimport os\nimport json\nimport pandas as pd\nimport torch\nimport importlib\nimport cv2 \n\nfrom shutil import copyfile\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import GroupKFold\nfrom PIL import Image\nfrom string import Template\nfrom IPython.display import display","c18326f0":"def get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_path(row):\n    row['image_path'] = f'{TRAIN_PATH}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row","6e4b53cb":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ndf.head()","ae81094d":"# Taken only annotated photos\ndf[\"num_bbox\"] = df['annotations'].apply(lambda x: str.count(x, 'x'))\ndf_train = df[df[\"num_bbox\"]>0]\ndf_train.head()\n\n\n#Annotations \ndf_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)\n\n#Images resolution\ndf_train[\"width\"] = 1280\ndf_train[\"height\"] = 720\n\n#Path of images\nTRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'\ndf_train = df_train.progress_apply(get_path, axis=1)","f12d5f56":"SELECTED_VIDEO = 2","c6542360":"\n!rm -r \/kaggle\/working\/cut_paste\/\n!mkdir -p \/kaggle\/working\/cut_paste\/video_0\/\n!mkdir -p \/kaggle\/working\/cut_paste\/video_1\/\n!mkdir -p \/kaggle\/working\/cut_paste\/video_2\/\n\nimport cv2\nimport random\nimport torchvision.ops.boxes as bops\nimport copy\ndf_train\n\nPASTE_NUM = 20\nbboxes_list = []\nimg_path = []\nfor i in tqdm(range(len(df_train))):\n    if df_train.iloc[i]['video_id'] == SELECTED_VIDEO:\n        bboxes_list.append(df_train.iloc[i]['bboxes'])\n        img_path.append(df_train.iloc[i]['image_path'])\n        continue\n    bboxes = copy.deepcopy(df_train.iloc[i]['bboxes'])\n    new_bbox_list = copy.deepcopy(bboxes)\n    path = df_train.iloc[i]['image_path']\n    video_id = df_train.iloc[i]['video_id']\n    video_frame = df_train.iloc[i]['video_frame']\n    img = cv2.imread(path)\n    for i in range(PASTE_NUM):\n        bbox_tensor = torch.tensor([[bbox[0] , bbox[1] , bbox[0]+bbox[2] , bbox[1]+bbox[3]] for bbox in new_bbox_list])\n        bbox = random.choice(bboxes)\n        cut_img = img[bbox[1]:bbox[1]+bbox[3] , bbox[0]:bbox[0]+bbox[2]]\n        point = random.randint(0 , 1280 - bbox[2]) ,random.randint(0 , 720 - bbox[3] ) \n        iou = bops.box_iou(bbox_tensor , torch.tensor([[point[0]  , point[1] ,point[0] +  bbox[2] ,point[1]+  bbox[3]]]))\n        if(torch.sum(iou) > 0):\n            continue\n        try:\n            img[point[1]:point[1]+bbox[3]  , point[0]:point[0] + bbox[2]] = cut_img\n            new_bbox_list.append([point[0], point[1] , bbox[2]  , bbox[3]])\n        except:\n            continue\n    bboxes_list.append(new_bbox_list)\n\n    cv2.imwrite(f'\/kaggle\/working\/cut_paste\/video_{video_id}\/{video_frame}.jpg' , img) \n    img_path.append(f'\/kaggle\/working\/cut_paste\/video_{video_id}\/{video_frame}.jpg')","17b498c0":"df_train['bboxes'] = bboxes_list\ndf_train['image_path'] = img_path\ndf_train","0c529aec":"HOME_DIR = '\/kaggle\/working\/' \nDATASET_PATH = f'kaggle_dataset\/images'\n\n!rm -r {HOME_DIR}{DATASET_PATH}\n\n!mkdir {HOME_DIR}kaggle_dataset\n!mkdir {HOME_DIR}{DATASET_PATH}\n!mkdir {HOME_DIR}{DATASET_PATH}\/train2017\n!mkdir {HOME_DIR}{DATASET_PATH}\/val2017\n!mkdir {HOME_DIR}{DATASET_PATH}\/annotations\n\n\nfor i in tqdm(range(len(df_train))):\n    row = df_train.iloc[i]\n    if row.video_id != SELECTED_VIDEO:\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}\/train2017\/{row.image_id}.jpg')\n    else:\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}\/val2017\/{row.image_id}.jpg') \nprint(f'FOLD {SELECTED_VIDEO} Number of training files: {len(os.listdir(f\"{HOME_DIR}{DATASET_PATH}\/train2017\/\"))}')\nprint(f'FOLD {SELECTED_VIDEO} Number of validation files: {len(os.listdir(f\"{HOME_DIR}{DATASET_PATH}\/val2017\/\"))}')","c127151c":"def save_annot_json(json_annotation, filename):\n    with open(filename, 'w') as f:\n        output_json = json.dumps(json_annotation)\n        f.write(output_json)\n\nannotion_id = 0","bb6c1c42":"def dataset2coco(df, dest_path):\n    \n    global annotion_id\n    \n    annotations_json = {\n        \"info\": [],\n        \"licenses\": [],\n        \"categories\": [],\n        \"images\": [],\n        \"annotations\": []\n    }\n    \n    info = {\n        \"year\": \"2021\",\n        \"version\": \"1\",\n        \"description\": \"COTS dataset - COCO format\",\n        \"contributor\": \"\",\n        \"url\": \"https:\/\/kaggle.com\",\n        \"date_created\": \"2021-11-30T15:01:26+00:00\"\n    }\n    annotations_json[\"info\"].append(info)\n    \n    lic = {\n            \"id\": 1,\n            \"url\": \"\",\n            \"name\": \"Unknown\"\n        }\n    annotations_json[\"licenses\"].append(lic)\n\n    classes = {\"id\": 0, \"name\": \"starfish\", \"supercategory\": \"none\"}\n\n    annotations_json[\"categories\"].append(classes)\n\n    \n    for ann_row in df.itertuples():\n            \n        images = {\n            \"id\": ann_row[0],\n            \"license\": 1,\n            \"file_name\": ann_row.image_id + '.jpg',\n            \"height\": ann_row.height,\n            \"width\": ann_row.width,\n            \"date_captured\": \"2021-11-30T15:01:26+00:00\"\n        }\n        \n        annotations_json[\"images\"].append(images)\n        \n        bbox_list = ann_row.bboxes\n        \n        for bbox in bbox_list:\n            b_width = bbox[2]\n            b_height = bbox[3]\n            \n            # some boxes in COTS are outside the image height and width\n            if (bbox[0] + bbox[2] > 1280):\n                b_width = bbox[0] - 1280 \n            if (bbox[1] + bbox[3] > 720):\n                b_height = bbox[1] - 720 \n                \n            image_annotations = {\n                \"id\": annotion_id,\n                \"image_id\": ann_row[0],\n                \"category_id\": 0,\n                \"bbox\": [bbox[0], bbox[1], b_width, b_height],\n                \"area\": bbox[2] * bbox[3],\n                \"segmentation\": [],\n                \"iscrowd\": 0\n            }\n            \n            annotion_id += 1\n            annotations_json[\"annotations\"].append(image_annotations)\n        \n        \n    print(f\"Dataset COTS annotation to COCO json format completed! Files: {len(df)}\")\n    return annotations_json","40f17b83":"\n# Convert COTS dataset to JSON COCO\ntrain_annot_json = dataset2coco(df_train[df_train.video_id != SELECTED_VIDEO], f\"{HOME_DIR}{DATASET_PATH}\/train2017\/\")\nval_annot_json = dataset2coco(df_train[df_train.video_id == SELECTED_VIDEO], f\"{HOME_DIR}{DATASET_PATH}\/val2017\/\")\n\n# Save converted annotations\nsave_annot_json(train_annot_json, f\"{HOME_DIR}{DATASET_PATH}\/annotations\/train.json\")\nsave_annot_json(val_annot_json, f\"{HOME_DIR}{DATASET_PATH}\/annotations\/valid.json\")","e34e2481":"import matplotlib.pyplot as plt\nimport random\nimport numpy as np\nhist_list = []\nfor k in [SELECTED_VIDEO]:\n    train_area_list = []\n    val_area_list = []\n    #print(f'fold {k}')\n    for i in tqdm(range(len(df_train))):\n        bboxes = df_train.iloc[i].bboxes\n        for bbox in bboxes:\n            area = np.sqrt(bbox[2]*bbox[3])\n            if df_train.iloc[i].video_id == k:\n                val_area_list.append(area)\n            else:\n                train_area_list.append(area)\n    #sample_train = random.sample(train_area_list, len(val_area_list))\n    #train = np.array(sample_train)\n    #val = np.array(val_area_list)\n\n    \n    #plt.hist(sample_train,  range=(0, 5000),bins=100);\n    #plt.hist(val_area_list ,  range=(0, 5000),bins=100);\n\n    print(f'train: mean {np.mean(train_area_list)} std {np.std(train_area_list)}')\n    print(f'val: mean {np.mean(val_area_list)} std {np.std(val_area_list)}')\n    print(np.mean(train_area_list) - np.mean(val_area_list) ,  np.std(train_area_list) - np.std(val_area_list))\n    # \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u63cf\u753b\u3059\u308b\n    plt.hist(train_area_list,  range=(0, 200),bins=100);\n    plt.hist(val_area_list ,  range=(0, 200),bins=100);","57df6fd3":"def make_config(fold):\n    config_file_template = f'''\n\n#!\/usr\/bin\/env python3\n# -*- coding:utf-8 -*-\n# Copyright (c) Megvii, Inc. and its affiliates.\n\nimport os\n\nfrom yolox.exp import Exp as MyExp\n\n\nclass Exp(MyExp):\n  def __init__(self):\n    super(Exp, self).__init__()\n    self.depth = 1\n    self.width = 1\n    self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]\n    self.output_dir = \"\/kaggle\/working\"\n\n    # Define yourself dataset path\n    self.data_dir = \"\/kaggle\/working\/kaggle_dataset\/images\"\n    self.train_ann = \"train.json\"\n    self.val_ann = \"valid.json\"\n    self.basic_lr_per_img = 1e-2 \/ 64.0 \n    self.num_classes = 1\n\n    self.max_epoch = $max_epoch\n    self.data_num_workers = 2\n    self.eval_interval = 1\n\n    self.mosaic_prob = 0.5\n    self.mixup_prob = 0.5\n    self.hsv_prob = 1.0\n    self.flip_prob = 0.5\n    self.no_aug_epochs = 2\n\n    self.input_size = (1280, 1280)\n    self.mosaic_scale = (1.0, 1.5)\n    self.mixup_scale = (1.0 , 1.5)\n    self.test_size = (1280, 1280)\n'''\n    return config_file_template","b8a60559":"!wget https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/releases\/download\/0.1.1rc0\/yolox_l.pth","da62ee1e":"%cd \/kaggle\/working\/YOLOX\n!cp .\/tools\/train.py .\/\n!cp  -r \/kaggle\/input\/yolox-cots-cv-patch\/yolox \/kaggle\/working\/YOLOX\/","9e2239a7":"%cd \/kaggle\/working\/YOLOX\n\n#!rm -r \/content\/kaggle_dataset_{SELECTED_FOLD}\/images\/img_resized_cache_train2017.array\nconfig_file_template = make_config(SELECTED_VIDEO)\nPIPELINE_CONFIG_PATH=f'.\/cots_config_video_id_{SELECTED_VIDEO}_epoch_20_yolox_l_size_1280_data_mizumashi_0114.py'\n\npipeline = Template(config_file_template).substitute(max_epoch = 0)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)\n    # .\/yolox\/data\/datasets\/voc_cl asses.py\n\nvoc_cls = '''VOC_CLASSES = (\"starfish\",)'''\nwith open('\/kaggle\/working\/YOLOX\/yolox\/data\/datasets\/voc_classes.py', 'w') as f:\n    f.write(voc_cls)\n\n# .\/yolox\/data\/datasets\/coco_classes.py\n\ncoco_cls = '''COCO_CLASSES = (\"starfish\",)'''\n\nwith open('\/kaggle\/working\/YOLOX\/yolox\/data\/datasets\/coco_classes.py', 'w') as f:\n    f.write(coco_cls)\n\n# check if everything is ok    \n!more \/kaggle\/working\/YOLOX\/yolox\/data\/datasets\/coco_classes.py\n!python train.py \\\n  -f {PIPELINE_CONFIG_PATH} \\\n  -d 1 \\\n  -b 4\\\n  --fp16 \\\n  --cache\\\n  -o \\\n  -c \/kaggle\/working\/yolox_l.pth\\\n  ","f4d2dcdf":"# YOLOX\u306e\u6e96\u5099","6608d5ae":"# \u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u78ba\u8a8d(Val\u306f\u91cd\u8981)","79a4e947":"# \u5b66\u7fd2","bd1e56ef":"# \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9","4adba1dd":"## \u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u304c\u3042\u308b\u90e8\u5206\u306e\u307f\u62bd\u51fa","94376a8a":"# YOLOX\u306e\u5b66\u7fd2\u8a2d\u5b9a","d7200372":"# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u6e96\u5099","9ade0730":"# CV\u51fa\u529b\u7528\u306e\u30d1\u30c3\u30c1\u30b3\u30fc\u30c9\u3092\u79fb\u690d","df09b5b3":"## train.csv\u306e\u8aad\u307f\u51fa\u3057","e8ada048":"## \u30d2\u30c8\u30c7\u306e\u30ab\u30c3\u30c8&\u30da\u30fc\u30b9\u30c8","aee8710d":"## COCO\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u5909\u66f4"}}