{"cell_type":{"c78bc74f":"code","07ebc574":"code","bb6a54d9":"code","f864168c":"code","d13ddffe":"code","48ac23dc":"code","6c9f72b1":"code","f5252c56":"code","30cd3d16":"code","b3106742":"code","ff95f113":"code","01523ca7":"code","72adf86c":"code","00a9f7fc":"code","d794a89e":"code","1bc4ecbc":"code","df296894":"code","a24bc4aa":"code","3e1f1c7e":"code","f3b1fc1c":"code","e13c6c5f":"code","b657d3cd":"code","ded80b65":"code","30670809":"code","4d75311b":"code","d760cd91":"code","ea2c33ba":"code","7b32edf2":"code","14342b79":"code","ddc3ced7":"code","9bd208b6":"code","1fb2fa2b":"code","11884151":"code","a5fa72f3":"code","c38f37b0":"code","58cbaa8a":"code","c85fcd45":"code","471ec107":"code","8a3471e2":"code","052cf896":"code","fddba708":"code","73da613f":"code","e49a3db1":"code","c3c8a8f3":"code","0834dde0":"code","44783e44":"code","4c8895c1":"code","dc480dc4":"code","9673b9a5":"code","9f476074":"code","04f31768":"code","fc8724bf":"code","11783d26":"code","d3a13dae":"code","7905a3e6":"code","ef1a7782":"code","29bd2375":"code","daf46d23":"code","00499abc":"code","5582648c":"code","57460e63":"code","742722cf":"code","ede0e9e9":"code","5e68294d":"code","3fe3fd8d":"code","6cbf572b":"code","c58357ab":"code","6834ca85":"code","b0d7e39e":"code","0b9a207a":"code","dcbf421f":"code","ff2e71ca":"code","fe4619c2":"code","e16e5833":"code","ba623e96":"code","3321535a":"code","c262a34d":"code","e1dbb9a2":"code","5db29e74":"code","a71a1bf4":"code","a399358c":"code","d4305188":"code","27f0ebb6":"code","f83fed8f":"code","8ff6fb95":"code","c264857d":"code","a2b7cb2a":"code","13d04ada":"code","864cfbd1":"code","1b7671a6":"code","eaef5bfc":"code","797115b3":"code","72724eff":"code","eb446897":"code","a03c2a94":"code","e061bee0":"code","7269f279":"code","a929e424":"code","dfdb6878":"code","772e7bf2":"code","7a04e400":"code","88369fee":"code","e482f061":"code","a56ab64e":"code","c3604b0a":"code","92e877b1":"code","4448d436":"code","561d8f68":"code","34d9d92a":"code","7dff4d02":"code","d68c8646":"code","fe82b97b":"code","84dcb115":"markdown","bf770d07":"markdown","338d2f01":"markdown","dc274dc5":"markdown","33e4e6ef":"markdown","2ecc5e80":"markdown","cdaf267c":"markdown","8f2feb10":"markdown","f00249b8":"markdown","0eb3a17c":"markdown","f18cbdf5":"markdown","4409527b":"markdown","261ca226":"markdown","db9b5716":"markdown","ae1bb147":"markdown","1e782904":"markdown","cf96eb54":"markdown","fa1eae21":"markdown","47e8e80f":"markdown","62ecddf0":"markdown","29f58809":"markdown","a1b5213f":"markdown","24b9d4a0":"markdown","c276cf67":"markdown","67f2b096":"markdown","58152b30":"markdown","76bdce7e":"markdown","6bf270ce":"markdown","c0fdcb69":"markdown","7cee5fef":"markdown","8191f874":"markdown","f31565ac":"markdown","80ad9ea5":"markdown","85b1e555":"markdown"},"source":{"c78bc74f":"#Importacion de librerias\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport statistics\nimport seaborn as sns\nimport sklearn as skl\nimport scipy.stats as st\nfrom sklearn.model_selection import cross_validate #importo libreria para cross validation\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\npd.set_option('display.float_format', lambda x: '%.2f' % x)","07ebc574":"#importacion de datasets\nOriginal_Train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nOriginal_Test=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","bb6a54d9":"Original_Train.info() #exploracion inicial","f864168c":"Original_Test.info() #exploracion inicial","d13ddffe":"Original_Train['ExterQual'].value_counts()\nexterqual_nums={'ExterQual': {\"Ex\":5, \"Gd\":4,'TA':3,'Fa':2}}\nOriginal_Train.replace(exterqual_nums,inplace=True)","48ac23dc":"Original_Test['ExterQual'].value_counts()\nexterqual_nums={'ExterQual': {\"Ex\":5, \"Gd\":4,'TA':3,'Fa':2}}\nOriginal_Test.replace(exterqual_nums,inplace=True)","6c9f72b1":"data = [Original_Train, Original_Test] #creacion de gurpo para el loop\nfor dataset in data:\n    dataset['BaseArea'] = dataset['GrLivArea'] + dataset['GarageArea'] #creacion de variable relatives\n    dataset['Qual_Time'] = (dataset['YearBuilt']-dataset['YearRemodAdd']) \n    dataset['Exqual_YearB'] = (dataset['YearBuilt']*dataset['ExterQual'])\nOriginal_Train.head() #visualizacion","f5252c56":"# establecer grupos de variables cuantitatvas y cualitativas\nquantitative_ori = [f for f in Original_Train.columns if Original_Train.dtypes[f] != 'object']\n#remover saleprice y id de las variable cualitativas\nquantitative_ori.remove('SalePrice')\nquantitative_ori.remove('Id')\nqualitative_ori = [f for f in Original_Train.columns if Original_Train.dtypes[f] == 'object']","30cd3d16":"# distribuci\u00f3n de las variables caulitativas\n#Original_melt= pd.melt(Original_Train, value_vars=quantitative_ori) # melt el data set original para obtener las variables y sus valores\n#graph = sns.FacetGrid(Original_melt, col=\"variable\",  col_wrap=2, sharex=False, sharey=False) #establecer los ejes del grafico\n#graph = graph.map(sns.distplot, \"value\") #impirmir el graph y mapear las distribuciones de cada valor\n\n#referencia: https:\/\/www.kaggle.com\/dgawlik\/house-prices-eda","b3106742":"cova=Original_Train.cov()\ncova['ExterQual'].head()","ff95f113":"corr=Original_Train.corr()\ncorr_extq=corr['ExterQual'].sort_values(ascending=False)\ncorr_extq.head()","01523ca7":"saleprice_corr=pd.DataFrame(corr['SalePrice'])\nsaleprice_corr.sort_values('SalePrice',ascending=False, inplace=True)\nsaleprice_corr.reset_index(inplace=True)\nsaleprice_corr.rename(columns={'index':'Variable'}, inplace=True)\nsaleprice_corr.head(10)","72adf86c":"s=corr.abs().unstack()\nso =pd.DataFrame(s.sort_values())\nso","00a9f7fc":"fig, ax = plt.subplots()\nfig.set_size_inches(14, 7)\nax.set_title('Features correlation', )\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, vmin=-1, vmax=1, annot=False, fmt=\".2f\", cmap='RdBu', center=0, ax=ax)","d794a89e":"Original_Esta=Original_Test.describe()\nOriginal_Esta","1bc4ecbc":"#encontrar el total de valores perdidos de cada variable\ntrain_total = Original_Train.isnull().sum().sort_values(ascending=False)\n#expresar el porcentaje de valores perdidos por variable\ntrain_percent_1 = Original_Train.isnull().sum()\/Original_Train.isnull().count()*100\n#redondear los decimales del porcentaje\ntrain_percent_2 = (round(train_percent_1, 1)).sort_values(ascending=False)\n#crear un nuevo dataframe que muestre el total y porcentaje\ntrain_missing_data = pd.DataFrame([train_total, train_percent_2], index=[\"Total\",'Porcentaje']).T\ntrain_missing_data #visualizacion","df296894":"missing_train = Original_Train.isnull().sum()\nmissing_train = missing_train[missing_train > 0]\nmissing_train.sort_values(inplace=True)\nmissing_train.plot.bar()\n#plt.set_title('Missing Values')","a24bc4aa":"#encontrar el total de valores perdidos de cada variable para el set de test\nTest_total = Original_Test.isnull().sum().sort_values(ascending=False)\n#expresar el porcentaje de valores perdidos por variable\nTest_percent_1 = Original_Test.isnull().sum()\/Original_Test.isnull().count()*100\n#redondear los decimales del porcentaje\nTest_percent_2 = (round(Test_percent_1, 1)).sort_values(ascending=False)\n#crear un nuevo dataframe que muestre el total y porcentaje\nTest_missing_data = pd.DataFrame([Test_total, Test_percent_2], index=[\"Total\",'Porcentaje']).T\nTest_missing_data.head(10) #visualizacion","3e1f1c7e":"missing_test = Original_Test.isnull().sum()\nmissing_test = missing_test[missing_test > 0]\nmissing_test.sort_values(inplace=True)\nmissing_test.plot.bar()\n#plt.set_title('Missing Values')","f3b1fc1c":"# crear un nuevo dataframe de set de datos editados\nTrain_Edited=pd.DataFrame(Original_Train.copy())\nTest_Edited=pd.DataFrame(Original_Test.copy())","e13c6c5f":"Test_Edited['BsmtQual'].unique()","b657d3cd":"data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\nfor dataset in data:\n    pool_qual = {\"Ex\": 4, \"Gd\": 3, \"TA\":2, \"Fa\":1} #crear un diccionario de correspondencia\n    dataset['PoolQC'] = dataset['PoolQC'].map(pool_qual)  #cambiar los valores origniales con los del dic\n    dataset['PoolQC']=dataset['PoolQC'].fillna(0) #cambiar los valores nulos por 0\n    dataset['PoolQC']=dataset['PoolQC'].astype(int)\nTest_Edited['PoolQC'].isnull().sum() #contar la cantidad de valores nulos","ded80b65":"Test_Edited['PoolQC'].unique()","30670809":"data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\nfor dataset in data:\n    fence_qual = {\"GdPrv\": 4, \"MnPrv\": 3, \"GdWo\":2, \"MnWw\":1} #crear un diccionario de correspondencia\n    dataset['Fence'] = dataset['Fence'].map(fence_qual)  #cambiar los valores origniales con los del dic\n    dataset['Fence']=dataset['Fence'].fillna(0) #cambiar los valores nulos por 0\n    dataset['Fence']=dataset['Fence'].astype(int)\nTest_Edited['Fence'].isnull().sum() #contar la cantidad de valores nulos","4d75311b":"Test_Edited['Fence'].unique()","d760cd91":"data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\nfor dataset in data:\n    alley_qual = {\"Grvl\": 2, \"Pave\": 1} #crear un diccionario de correspondencia\n    dataset['Alley'] = dataset['Alley'].map(alley_qual)  #cambiar los valores origniales con los del dic\n    dataset['Alley']=dataset['Alley'].fillna(0) #cambiar los valores nulos por 0\n    dataset['Alley']=dataset['Alley'].astype(int)\nTest_Edited['Alley'].isnull().sum() #contar la cantidad de valores nulos","ea2c33ba":"Test_Edited['Alley'].unique()","7b32edf2":"Train_Edited['Shed']=np.where(Original_Train['MiscFeature']=='Shed',1,0)\nTrain_Edited['Gar2']=np.where(Original_Train['MiscFeature']=='Gar2',1,0)\nTrain_Edited['Othr']=np.where(Original_Train['MiscFeature']=='Othr',1,0)\nTrain_Edited['TenC']=np.where(Original_Train['MiscFeature']=='TenC',1,0)\nTrain_Edited\n#data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\n#for dataset in data:\n#    misc_fea = {\"Shed\": 4, \"Gar2\": 3, \"Othr\": 2, \"TenC\": 1} #crear un diccionario de correspondencia\n#    dataset['MiscFeature'] = dataset['MiscFeature'].map(misc_fea)  #cambiar los valores origniales con los del dic\n#    dataset['MiscFeature']=dataset['MiscFeature'].fillna(0) #cambiar los valores nulos por 0\n#    dataset['MiscFeature']=dataset['MiscFeature'].astype(int)\n#Train_Edited['MiscFeature'].isnull().sum() #contar la cantidad de valores nulos","14342b79":"Train_Edited['Shed'].unique()","ddc3ced7":"testcor=Train_Edited.corr()\ntestcor[['Shed', 'Gar2', 'Othr', 'TenC']].loc['SalePrice':].head(1)","9bd208b6":"data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\nfor dataset in data:\n    fire_qua = {\"Ex\": 5, \"Gd\": 4, \"TA\":3, \"Fa\":2, \"Po\":1}  #crear un diccionario de correspondencia\n    dataset['FireplaceQu'] = dataset['FireplaceQu'].map(fire_qua)  #cambiar los valores origniales con los del dic\n    dataset['FireplaceQu']=dataset['FireplaceQu'].fillna(0) #cambiar los valores nulos por 0\n    dataset['FireplaceQu']=dataset['FireplaceQu'].astype(int)\nTrain_Edited['FireplaceQu'].isnull().sum() #contar la cantidad de valores nulos","1fb2fa2b":"Test_Edited['BsmtQual'].unique()","11884151":"Train_Edited","a5fa72f3":"# reemplazar los valores perdidos de LotFrontage con la mediana\nTrain_Edited['LotFrontage'].fillna(Original_Train['LotFrontage'].median(), inplace = True)","c38f37b0":"# reemplazar los valores perdidos de LotFrontage con la mediana\nTest_Edited['LotFrontage'].fillna(Original_Test['LotFrontage'].median(), inplace = True)\nTest_Edited['BsmtFullBath'].fillna(Original_Test['BsmtFullBath'].median(), inplace = True)\nTest_Edited['BsmtHalfBath'].fillna(Original_Test['BsmtHalfBath'].median(), inplace = True)\nTest_Edited['TotalBsmtSF'].fillna(Original_Test['TotalBsmtSF'].median(), inplace = True)\nTest_Edited['GarageCars'].fillna(Original_Test['GarageCars'].median(), inplace = True)\nTest_Edited['GarageArea'].fillna(Original_Test['GarageArea'].median(), inplace = True)\nTest_Edited['BsmtUnfSF'].fillna(Original_Test['BsmtUnfSF'].median(), inplace = True)\nTest_Edited['BsmtFinSF2'].fillna(Original_Test['BsmtFinSF2'].median(), inplace = True)\nTest_Edited['BsmtFinSF1'].fillna(Original_Test['BsmtFinSF1'].median(), inplace = True)\nTest_Edited['BaseArea'].fillna(Original_Test['BaseArea'].median(), inplace = True)","58cbaa8a":"Train_Edited.drop(columns=['Shed', 'Gar2', 'Othr', 'TenC', 'MiscFeature', 'GarageYrBlt', 'MasVnrArea'], axis=1, inplace=True) #eliminar la variable MiscFeature\n#encontrar el total de valores perdidos de cada variable en el nuevo dataframe de train\ntrain_total_edited = Train_Edited.isnull().sum().sort_values(ascending=False)\n#expresar el porcentaje de valores perdidos por variable\ntrain_percent_1_edited = Train_Edited.isnull().sum()\/Train_Edited.isnull().count()*100\n#redondear los decimales del porcentaje\ntrain_percent_2_edited = (round(train_percent_1_edited, 1)).sort_values(ascending=False)\n#crear un nuevo dataframe que muestre el total y porcentaje\ntrain_missing_data_edited = pd.DataFrame([train_total_edited, train_percent_2_edited], index=[\"Total\",'Porcentaje']).T\ntrain_missing_data_edited.head(15)","c85fcd45":"Test_Edited.drop(columns=['MiscFeature', 'GarageYrBlt', 'MasVnrArea'], axis=1, inplace=True) #eliminar la variable MiscFeature\n#encontrar el total de valores perdidos de cada variable en el nuevo dataframe de Test\nTest_total_edited = Test_Edited.isnull().sum().sort_values(ascending=False)\n#expresar el porcentaje de valores perdidos por variable\nTest_percent_1_edited = Test_Edited.isnull().sum()\/Test_Edited.isnull().count()*100\n#redondear los decimales del porcentaje\nTest_percent_2_edited = (round(Test_percent_1_edited, 1)).sort_values(ascending=False)\n#crear un nuevo dataframe que muestre el total y porcentaje\nTest_missing_data_edited = pd.DataFrame([Test_total_edited, Test_percent_2_edited], index=[\"Total\",'Porcentaje']).T\nTest_missing_data_edited.head()","471ec107":"Original_Train['BsmtQual'].unique()","8a3471e2":"# crear un nuevo dataframe \nTrain_pca=Train_Edited.drop(columns=['Id']) #eliminar las columnas Id y SalePrice\nTrain_pca","052cf896":"# establecer grupos de variables cuantitatvas y cualitativas\nquantitative = [f for f in Train_pca.columns if Train_pca.dtypes[f] != 'object']\nqualitative = [f for f in Train_pca.columns if Train_pca.dtypes[f] == 'object']\nquantitative_SP = [f for f in Train_pca.columns if Train_pca.dtypes[f] != 'object']\nquantitative_SP.remove('SalePrice')","fddba708":"#normalizar datos\nTrain_N=skl.preprocessing.StandardScaler().fit(Train_pca[quantitative]).transform(Train_pca[quantitative].astype(float))\nTrain_N=pd.DataFrame(Train_N.copy(), columns=Train_pca[quantitative].columns) # renombrar columanas con los nombres del DF original\nTrain_N #visualizar DF","73da613f":"#correr modelo PCA \nTrain_N_SP=Train_N.copy().drop(columns='SalePrice')\npcs = skl.decomposition.PCA()\npcs.fit(Train_N_SP)\n#obtener estadisticas de los componentes principales\npcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary_df = pcsSummary_df.transpose() #transponer DF\npcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_df.columns)+1)] #establecer el nombre de las columnas\npcsSummary_df.round(4) #precision de 4 decimales\npcsSummary_df.iloc[:,:20] #mostrar unicamente los 20 primeros PC","e49a3db1":"# encontrar los pesos de cada PC\npcsComponents_df = pd.DataFrame(pcs.components_.transpose(), columns=pcsSummary_df.columns, \n                                index=Train_N_SP[quantitative_SP].columns)\npcsComponents_df.iloc[:10,:13] #mostrar los pesos de los 13 primeros componentes","c3c8a8f3":"houses_red_df = Train_N_SP[quantitative_SP].dropna(axis=0)\nhouses_red_df = Train_N_SP[quantitative_SP].reset_index(drop=True)\n\nscores = pd.DataFrame(pcs.fit_transform(skl.preprocessing.scale(houses_red_df.dropna(axis=0))), \n                      columns=[f'PC{i}' for i in range(1, 43)])\nhouses_pca_df = pd.concat([houses_red_df['MSSubClass'].dropna(axis=0), scores[['PC1', 'PC2']]], axis=1)\nax = houses_pca_df.plot.scatter(x='PC1', y='PC2', figsize=(6, 6))\npoints = houses_pca_df[['PC1','PC2','MSSubClass']]\n\ntexts = []\nplt.show()","0834dde0":"#obtener los valores transformados de cada uno de PC para todos los registros del set de datos\ntransform_df = pd.DataFrame(pcs.transform(Train_N_SP[quantitative_SP]), \n                      columns=pcsSummary_df.columns)\ntransform_df_13=transform_df.iloc[:,:13].copy() # DF con 13 componentes principales\ntransform_df_20=transform_df.iloc[:,:20].copy() # DF con 20 componentes principales\ntransform_df_20.head()","44783e44":"# PCA para el set de test\n\n# crear un nuevo dataframe \nTest_pca=Test_Edited.drop(columns=['Id']) #eliminar las columnas Id y SalePrice\nTest_pca\n\n\n#normalizar datos\nTest_N=skl.preprocessing.StandardScaler().fit(Test_pca[quantitative_SP]).transform(Test_pca[quantitative_SP].astype(float))\nTest_N=pd.DataFrame(Test_N.copy(), columns=Test_pca[quantitative_SP].columns) # renombrar columanas con los nombres del DF original\nTest_N #visualizar DF\n\n\n#correr modelo PCA \nTest_N_SP=Test_N.copy()\npcs_test = skl.decomposition.PCA()\npcs_test.fit(Test_N_SP)\n#obtener estadisticas de los componentes principales\npcsSummary_test_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs_test.explained_variance_),\n                           'Proportion of variance': pcs_test.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs_test.explained_variance_ratio_)})\npcsSummary_test_df = pcsSummary_test_df.transpose() #transponer DF\npcsSummary_test_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_test_df.columns)+1)] #establecer el nombre de las columnas\npcsSummary_test_df.round(4) #precision de 4 decimales\npcsSummary_test_df.iloc[:,:20] #mostrar unicamente los 20 primeros PC\n\npcsComponents_test_df = pd.DataFrame(pcs_test.components_.transpose(), columns=pcsSummary_test_df.columns, \n                                index=Test_N_SP[quantitative_SP].columns)\npcsComponents_test_df.iloc[:10,:13] #mostrar los pesos de los 13 primeros componentes\n\n\nhouses_red_df_test = Test_N_SP[quantitative_SP].dropna(axis=0)\nhouses_red_df_test = Test_N_SP[quantitative_SP].reset_index(drop=True)\n\nscores_test = pd.DataFrame(pcs_test.fit_transform(skl.preprocessing.scale(houses_red_df_test.dropna(axis=0))), \n                      columns=[f'PC{i}' for i in range(1, 43)])\nhouses_pca_df_test = pd.concat([houses_red_df_test['MSSubClass'].dropna(axis=0), scores_test[['PC1', 'PC2']]], axis=1)\nax = houses_pca_df_test.plot.scatter(x='PC1', y='PC2', figsize=(6, 6))\npoints = houses_pca_df_test[['PC1','PC2','MSSubClass']]\n\ntexts = []\nplt.show()\n\n#obtener los valores transformados de cada uno de PC para todos los registros del set de datos\ntransform_df_test = pd.DataFrame(pcs_test.transform(Test_N_SP[quantitative_SP]), \n                      columns=pcsSummary_test_df.columns)\ntransform_df_test_13=transform_df_test.iloc[:,:13].copy() # DF con 13 componentes principales\ntransform_df_test_20=transform_df_test.iloc[:,:20].copy() # DF con 20 componentes principales\ntransform_df_test_20.head()","4c8895c1":"PCA_X_train_13 = transform_df_13 #definimos los feautures del set train para los modelos de prediccion\nPCA_Y_train_13 = Train_Edited[\"SalePrice\"] #definimos el target\nPCA_X_test_13 = transform_df_test_13 #definimos sobre que set de datos se desean realizar las prediccione\n\n#random forest model\nPCA_RF_13 = RandomForestClassifier(n_estimators=100)  #llamamos al primer modelo definiendo el numero de estimadores\nPCA_RF_13.fit(PCA_X_train_13, PCA_Y_train_13) #asignamos el set de train\nPCA_RF_EXIST_13 = PCA_RF_13.predict(PCA_X_train_13)\nPCA_Y_prediction_13 = PCA_RF_13.predict(PCA_X_test_13) #asignamos sobre que deseamos calcular el predictor\nPCA_ACC_RF_13 = PCA_RF_13.score(PCA_X_train_13, PCA_Y_train_13) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_RF_13= skl.metrics.mean_absolute_error(PCA_RF_EXIST_13, PCA_Y_train_13)\nPCA_MSE_RF_13= skl.metrics.mean_squared_error(PCA_RF_EXIST_13, PCA_Y_train_13)\nPCA_MSE_RF_13","dc480dc4":"# adaboosreg model\nX_13=PCA_X_train_13\ny_13=PCA_Y_train_13\nPCA_AB_13=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=100)\nPCA_AB_13.fit(X_13,y_13)\nPCA_Y_pred_ADAB_13=PCA_AB_13.predict(PCA_X_test_13)\nPCA_AB_EXIST_13 = PCA_AB_13.predict(PCA_X_train_13)\nPCA_ACC_AB_13= PCA_AB_13.score(PCA_X_train_13, PCA_Y_train_13) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_AB_13= skl.metrics.mean_absolute_error(PCA_AB_EXIST_13, PCA_Y_train_13)\nPCA_MSE_AB_13= skl.metrics.mean_squared_error(PCA_AB_EXIST_13, PCA_Y_train_13)\n#referencia: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html","9673b9a5":"#linear regresion model\nPCA_LR_13=skl.linear_model.LinearRegression()\nPCA_LR_13.fit(X_13, y_13)\nPCA_Y_pred_LR_13= PCA_LR_13.predict(PCA_X_test_13)\nPCA_LR_EXIST_13 = PCA_LR_13.predict(PCA_X_train_13)\nPCA_ACC_LR_13 = PCA_LR_13.score(PCA_X_train_13, PCA_Y_train_13) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_LR_13= skl.metrics.mean_absolute_error(PCA_LR_EXIST_13, PCA_Y_train_13)\nPCA_MSE_LR_13= skl.metrics.mean_squared_error(PCA_LR_EXIST_13, PCA_Y_train_13)","9f476074":"print('Intercept (b0):',PCA_LR_13.intercept_)\n# For retrieving the slope:\nprint('****************')\nprint('(bi):',PCA_LR_13.coef_)\nprint('****************')","04f31768":"PCA_X_train_20 = transform_df_20 #definimos los feautures del set train para los modelos de prediccion\nPCA_Y_train_20 = Train_Edited[\"SalePrice\"] #definimos el target\nPCA_X_test_20 = transform_df_test_20 #definimos sobre que set de datos se desean realizar las prediccione\n\n#random forest model\nPCA_RF_20 = RandomForestClassifier(n_estimators=100)  #llamamos al primer modelo definiendo el numero de estimadores\nPCA_RF_20.fit(PCA_X_train_20, PCA_Y_train_20) #asignamos el set de train\nPCA_RF_EXIST_20 = PCA_RF_20.predict(PCA_X_train_20)\nPCA_Y_prediction_20 = PCA_RF_20.predict(PCA_X_test_20) #asignamos sobre que deseamos calcular el predictor\nPCA_ACC_RF_20 = PCA_RF_20.score(PCA_X_train_20, PCA_Y_train_20) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_RF_20= skl.metrics.mean_absolute_error(PCA_RF_EXIST_20, PCA_Y_train_20)\nPCA_MSE_RF_20= skl.metrics.mean_squared_error(PCA_RF_EXIST_20, PCA_Y_train_20)","fc8724bf":"# adaboosreg model\nX_20=PCA_X_train_20\ny_20=PCA_Y_train_20\nPCA_AB_20=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=100)\nPCA_AB_20.fit(X_20,y_20)\nPCA_Y_pred_ADAB_20=PCA_AB_20.predict(PCA_X_test_20)\nPCA_AB_EXIST_20 = PCA_AB_20.predict(PCA_X_train_20)\nPCA_ACC_AB_20= PCA_AB_20.score(PCA_X_train_20, PCA_Y_train_20) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_AB_20= skl.metrics.mean_absolute_error(PCA_AB_EXIST_20, PCA_Y_train_20)\nPCA_MSE_AB_20= skl.metrics.mean_squared_error(PCA_AB_EXIST_20, PCA_Y_train_20)\n#referencia: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html","11783d26":"#linear regresion model\nPCA_LR_20=skl.linear_model.LinearRegression()\nPCA_LR_20.fit(X_20, y_20)\nPCA_Y_pred_LR_20= PCA_LR_20.predict(PCA_X_test_20)\nPCA_LR_EXIST_20 = PCA_LR_20.predict(PCA_X_train_20)\nPCA_ACC_LR_20 = PCA_LR_20.score(PCA_X_train_20, PCA_Y_train_20) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_LR_20= skl.metrics.mean_absolute_error(PCA_LR_EXIST_20, PCA_Y_train_20)\nPCA_MSE_LR_20= skl.metrics.mean_squared_error(PCA_LR_EXIST_20, PCA_Y_train_20)","d3a13dae":"print('Intercept (b0):',PCA_LR_20.intercept_)\n# For retrieving the slope:\nprint('****************')\nprint('(bi):',PCA_LR_20.coef_)\nprint('****************')","7905a3e6":"#cross validate linear regression PCA\nPCA_LR_CV_SCORE = cross_validate(PCA_LR_13, PCA_X_train_13, PCA_Y_train_13, cv=4,scoring=['neg_mean_absolute_error','neg_mean_squared_error'],return_estimator=True) #llamo a cross validate definiendo feautures del dataframe auxiliar, la variable de salida, el valor de k, y las metricas deseadas\n\n\nPCA_LR_CV_METRICS=pd.DataFrame(PCA_LR_CV_SCORE, columns=['test_neg_mean_squared_error']) #almaceno las metricas obtenidas en un dataframe\nPCA_LR_CV_METRICS.rename(columns={'test_neg_mean_squared_error':'MSE'}, inplace=True) #cambio los nombres predeterminados por nombres mas simples\nPCA_LR_CV_METRICS_2=pd.DataFrame() #creo un nuevo dataframe\n\nfor i in PCA_LR_CV_SCORE['estimator']: \n    PCA_LR_CV_METRICS_2.loc[i,'MAPE']=((i.predict(PCA_X_train_13)-PCA_Y_train_13).abs()\/PCA_Y_train_13).sum()\/PCA_X_train_13['PC1'].count()*100 ##guardo en el dataframe, el MAPE ajustado de cada modelo a meida itera\nPCA_LR_CV_METRICS_2.reset_index(drop=True, inplace=True) #borro el indice para cambiar el codigo del modelo por numeros iniciando en 0\nPCA_LR_CV_METRICS_FINAL=pd.DataFrame(PCA_LR_CV_METRICS_2[['MAPE']].copy()) #genero un dataframe que copie las metricas del pirmer dataframe de metricas \nPCA_LR_CV_METRICS_FINAL[['MSE']]=PCA_LR_CV_METRICS[['MSE']].copy() #a\u00f1ado las columnas del segundo dataframe de metricas al dataframe final\n\nPCA_LR_CV_METRICS_FINAL=abs(PCA_LR_CV_METRICS_FINAL) #cambio todos los valores del dataframe final por sus valores absolutos para evitar negativos\n\nPCA_LR_CV_METRICS_FINAL.rename(index={0: 'Fold_1',1: 'Fold_2',2: 'Fold_3',3: 'Fold_4' }, inplace=True) #renombro los indices segun el numero de fold\nPCA_LR_CV_METRICS_FINAL #visualzo el datagrame final","ef1a7782":"#saving de dataframes en archivos csv excluyendo la columna de index para datos CON y SIN AGE\nPred_RF_PCA_13=pd.DataFrame()\nPred_RF_PCA_13['Id']=Original_Test['Id']\nPred_RF_PCA_13['SalePrice']=PCA_Y_prediction_13\nPred_RF_PCA_13.to_csv('Pred_RF_PCA_13.csv',index=False)\n\nPred_LR_PCA_13=pd.DataFrame()\nPred_LR_PCA_13['Id']=Original_Test['Id']\nPred_LR_PCA_13['SalePrice']=PCA_Y_pred_LR_13\nPred_LR_PCA_13.to_csv('Pred_LR_PCA_13.csv',index=False)\n\nPred_AB_PCA_13=pd.DataFrame()\nPred_AB_PCA_13['Id']=Original_Test['Id']\nPred_AB_PCA_13['SalePrice']=PCA_Y_pred_ADAB_13\nPred_AB_PCA_13.to_csv('Pred_AB_PCA_13.csv',index=False)\n\nPred_RF_PCA_20=pd.DataFrame()\nPred_RF_PCA_20['Id']=Original_Test['Id']\nPred_RF_PCA_20['SalePrice']=PCA_Y_prediction_20\nPred_RF_PCA_20.to_csv('Pred_RF_PCA_20.csv',index=False)\n\nPred_LR_PCA_20=pd.DataFrame()\nPred_LR_PCA_20['Id']=Original_Test['Id']\nPred_LR_PCA_20['SalePrice']=PCA_Y_pred_LR_20\nPred_LR_PCA_20.to_csv('Pred_LR_PCA_20.csv',index=False)\n\nPred_AB_PCA_20=pd.DataFrame()\nPred_AB_PCA_20['Id']=Original_Test['Id']\nPred_AB_PCA_20['SalePrice']=PCA_Y_pred_ADAB_20\nPred_AB_PCA_20.to_csv('Pred_AB_PCA_20.csv',index=False)","29bd2375":"obj_df = Train_Edited.select_dtypes(include=['object']).copy() #analysis de object variables\nobj_df.head()","daf46d23":"corr_2=Train_Edited.corr()","00499abc":"saleprice_corr_2=pd.DataFrame(corr_2['SalePrice'])\nsaleprice_corr_2.sort_values('SalePrice',ascending=False, inplace=True)\nsaleprice_corr_2.reset_index(inplace=True)\nsaleprice_corr_2.rename(columns={'index':'Variable'}, inplace=True)\nsaleprice_corr_2.head(10)","5582648c":"qualitative_model=['MSZoning', 'Neighborhood', 'MasVnrType', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType'] #seleccion de variables qualitativas en base a conocimientos previos\nquantitative_model=['OverallQual','BaseArea','GrLivArea','Exqual_YearB','ExterQual','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF'] #seleccion de variables cuantitativas en funcion de su correlacion con sale price\n","57460e63":"Train_Edited[qualitative_model].info() #analisis de mssing values para variables categoricas en set de entrenamiento","742722cf":"Test_Edited[qualitative_model].info() #analisis de mssing values para variables categoricas en set de prueba","ede0e9e9":"Train_Edited[quantitative_model].info() #analisis de mssing values para variables cuantitativas en set de entrenamiento","5e68294d":"Test_Edited[quantitative_model].info() #analisis de mssing values para variables cuantitativas en set de entrenamiento","3fe3fd8d":"Train_Edited_m1=pd.DataFrame()\nTrain_Edited_m1[qualitative_model]=Train_Edited[qualitative_model].copy()\nTrain_Edited_m1[quantitative_model]=Train_Edited[quantitative_model].copy()\nTrain_Edited_m1['SalePrice']=Train_Edited['SalePrice'].copy()\n\nTest_Edited_m1=pd.DataFrame()\nTest_Edited_m1[qualitative_model]=Test_Edited[qualitative_model].copy()\nTest_Edited_m1[quantitative_model]=Test_Edited[quantitative_model].copy()\n\nTrain_Edited_m1","6cbf572b":"data_m1 = [Train_Edited_m1,Test_Edited_m1] #creacion de gurpo para el loop","c58357ab":"for dataset in data_m1:\n    msz = {\"FV\": 5, \"RL\": 4, \"RH\":3, \"RM\":2, \"C (all)\":1}  #crear un diccionario de correspondencia\n    dataset['MSZoning'] = dataset['MSZoning'].map(msz)  #cambiar los valores origniales con los del dic\n    dataset['MSZoning']=dataset['MSZoning'].fillna(dataset['MSZoning'].median())\n    dataset['MSZoning']=dataset['MSZoning'].astype(int)   ","6834ca85":"for dataset in data_m1:\n    NB = {\"CollgCr\": 25, \"OldTown\": 24, \"Edwards\":23, \"Somerst\":22, \"Gilbert\":21,\"NridgHt\": 20, \"Sawyer\": 19, \"NWAmes\":18, \"SawyerW\":17, \"BrkSide\":16,\"Crawfor\": 15, \"Mitchel\": 14, \"NoRidge\":13, \"Timber\":12, \"IDOTRR\":10,\"ClearCr\": 9, \"StoneBr\":8, \"SWISU\":7, \"Blmngtn\":6,\"MeadowV\": 5, \"BrDale\": 4, \"Veenker\":3, \"NPKVill\":2, \"Blueste\":1}  #crear un diccionario de correspondencia\n    dataset['Neighborhood'] = dataset['Neighborhood'].map(NB)  #cambiar los valores origniales con los del dic\n    dataset['Neighborhood']=dataset['Neighborhood'].fillna(dataset['Neighborhood'].median())\n    dataset['Neighborhood']=dataset['Neighborhood'].astype(int)   ","b0d7e39e":"for dataset in data_m1:\n    MVT = {\"Stone\": 4, \"BrkFace\": 3, \"BrkCmn\":2, \"None\":1}  #crear un diccionario de correspondencia\n    dataset['MasVnrType'] = dataset['MasVnrType'].map(MVT)  #cambiar los valores origniales con los del dic\n    dataset['MasVnrType']=dataset['MasVnrType'].fillna(dataset['MasVnrType'].median())\n    dataset['MasVnrType']=dataset['MasVnrType'].astype(int)   ","0b9a207a":"for dataset in data_m1:\n    BMQ = {\"Ex\": 4, \"Gd\": 3, \"TA\":2, \"Fa\":1}  #crear un diccionario de correspondencia\n    dataset['BsmtQual'] = dataset['BsmtQual'].map(BMQ)  #cambiar los valores origniales con los del dic\n    dataset['BsmtQual']=dataset['BsmtQual'].fillna(dataset['BsmtQual'].median())\n    dataset['BsmtQual']=dataset['BsmtQual'].astype(int)   ","dcbf421f":"for dataset in data_m1:\n    CA = {\"Y\": 1, \"N\": 0}  #crear un diccionario de correspondencia\n    dataset['CentralAir'] = dataset['CentralAir'].map(CA)  #cambiar los valores origniales con los del dic\n    dataset['CentralAir']=dataset['CentralAir'].fillna(dataset['CentralAir'].median())\n    dataset['CentralAir']=dataset['CentralAir'].astype(int)   ","ff2e71ca":"for dataset in data_m1:\n    EL = {\"SBrkr\": 5, \"FuseA\": 4,\"FuseF\":3,\"FuseP\":2,\"Mix\":1}  #crear un diccionario de correspondencia\n    dataset['Electrical'] = dataset['Electrical'].map(EL)  #cambiar los valores origniales con los del dic\n    dataset['Electrical']=dataset['Electrical'].fillna(dataset['Electrical'].median())\n    dataset['Electrical']=dataset['Electrical'].astype(int)   ","fe4619c2":"for dataset in data_m1:\n    KQ = {\"Ex\": 4,\"Gd\":3,\"TA\":2,\"Fa\":1}  #crear un diccionario de correspondencia\n    dataset['KitchenQual'] = dataset['KitchenQual'].map(KQ)  #cambiar los valores origniales con los del dic\n    dataset['KitchenQual']=dataset['KitchenQual'].fillna(dataset['KitchenQual'].median())\n    dataset['KitchenQual']=dataset['KitchenQual'].astype(int)   ","e16e5833":"for dataset in data_m1:\n    KQ = {\"Con\": 9,\"New\":8,\"CWD\":7,\"WD\":6,\"ConLw\": 5,\"COD\":4,\"ConLD\":3,\"ConLI\":2,\"Oth\": 1}  #crear un diccionario de correspondencia\n    dataset['SaleType'] = dataset['SaleType'].map(KQ)  #cambiar los valores origniales con los del dic\n    dataset['SaleType']=dataset['SaleType'].fillna(dataset['SaleType'].median())\n    dataset['SaleType']=dataset['SaleType'].astype(int)  ","ba623e96":"for dataset in data_m1:\n    dataset['OverallQual']=dataset['OverallQual'].fillna(dataset['OverallQual'].median())\n    dataset['OverallQual']=dataset['OverallQual'].astype(int)  ","3321535a":"for dataset in data_m1:\n    dataset['BaseArea']=dataset['BaseArea'].fillna(dataset['BaseArea'].median())\n    dataset['BaseArea']=dataset['BaseArea'].astype(int)  ","c262a34d":"for dataset in data_m1:\n    dataset['GrLivArea']=dataset['GrLivArea'].fillna(dataset['GrLivArea'].median())\n    dataset['GrLivArea']=dataset['GrLivArea'].astype(int)  ","e1dbb9a2":"for dataset in data_m1:\n    dataset['Exqual_YearB']=dataset['Exqual_YearB'].fillna(dataset['Exqual_YearB'].median())\n    dataset['Exqual_YearB']=dataset['Exqual_YearB'].astype(int)  ","5db29e74":"for dataset in data_m1:\n    dataset['ExterQual']=dataset['ExterQual'].fillna(dataset['ExterQual'].median())\n    dataset['ExterQual']=dataset['ExterQual'].astype(int)  ","a71a1bf4":"for dataset in data_m1:\n    dataset['GarageCars']=dataset['GarageCars'].fillna(dataset['GarageCars'].median())\n    dataset['GarageCars']=dataset['GarageCars'].astype(int)  ","a399358c":"for dataset in data_m1:\n    dataset['GarageArea']=dataset['GarageArea'].fillna(dataset['GarageArea'].median())\n    dataset['GarageArea']=dataset['GarageArea'].astype(int)  ","d4305188":"for dataset in data_m1:\n    dataset['TotalBsmtSF']=dataset['TotalBsmtSF'].fillna(dataset['TotalBsmtSF'].median())\n    dataset['TotalBsmtSF']=dataset['TotalBsmtSF'].astype(int)  ","27f0ebb6":"for dataset in data_m1:\n    dataset['1stFlrSF']=dataset['1stFlrSF'].fillna(dataset['1stFlrSF'].median())\n    dataset['1stFlrSF']=dataset['1stFlrSF'].astype(int)  ","f83fed8f":"X_Train_Final_m1=pd.DataFrame()\nX_Train_Final_m1[qualitative_model]=Train_Edited_m1[qualitative_model].copy()\nX_Train_Final_m1[quantitative_model]=Train_Edited_m1[quantitative_model].copy()\nY_Train_Final_m1=Train_Edited_m1['SalePrice']\n\nX_Test_Final_m1=pd.DataFrame()\nX_Test_Final_m1[qualitative_model]=Test_Edited_m1[qualitative_model].copy()\nX_Test_Final_m1[quantitative_model]=Test_Edited_m1[quantitative_model].copy() ","8ff6fb95":"X_Train_Final_m1","c264857d":"X_Test_Final_m1","a2b7cb2a":"Y_Train_Final_m1","13d04ada":"#random forest model\nAP2_RF = RandomForestClassifier(n_estimators=100)  #llamamos al primer modelo definiendo el numero de estimadores\nAP2_RF.fit(X_Train_Final_m1, Y_Train_Final_m1) #asignamos el set de train\nAP2_RF_EXIST= AP2_RF.predict(X_Train_Final_m1)\nAP2_RF_PRED = AP2_RF.predict(X_Test_Final_m1) #asignamos sobre que deseamos calcular el predictor\nAP2_ACC_RF = AP2_RF.score(X_Train_Final_m1, Y_Train_Final_m1) * 100 #sacamos el porcetaje de certeza del modelo\nAP2_MAE_RF= skl.metrics.mean_absolute_error(AP2_RF_EXIST, Y_Train_Final_m1)\nAP2_MSE_RF= skl.metrics.mean_squared_error(AP2_RF_EXIST, Y_Train_Final_m1)","864cfbd1":"# adaboosreg model\nAP2_AB=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=100)\nAP2_AB.fit(X_Train_Final_m1,Y_Train_Final_m1)\nAP2_AB_EXIST= AP2_AB.predict(X_Train_Final_m1)\nAP2_AB_PRED=AP2_AB.predict(X_Test_Final_m1)\nAP2_ACC_AB = AP2_AB.score(X_Train_Final_m1, Y_Train_Final_m1) * 100 #sacamos el porcetaje de certeza del modelo\nAP2_MAE_AB= skl.metrics.mean_absolute_error(AP2_AB_EXIST, Y_Train_Final_m1)\nAP2_MSE_AB= skl.metrics.mean_squared_error(AP2_AB_EXIST, Y_Train_Final_m1)\n#referencia: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html","1b7671a6":"#linear regresion model\nAP2_LR=skl.linear_model.LinearRegression()\nAP2_LR.fit(X_Train_Final_m1,Y_Train_Final_m1)\nAP2_LR_EXIST= AP2_LR.predict(X_Train_Final_m1)\nAP2_LR_PRED= AP2_LR.predict(X_Test_Final_m1)\nAP2_ACC_LR = AP2_LR.score(X_Train_Final_m1, Y_Train_Final_m1) * 100 #sacamos el porcetaje de certeza del modelo\nAP2_MAE_LR= skl.metrics.mean_absolute_error(AP2_LR_EXIST, Y_Train_Final_m1)\nAP2_MSE_LR= skl.metrics.mean_squared_error(AP2_LR_EXIST, Y_Train_Final_m1)","eaef5bfc":"print('Intercept (b0):',AP2_LR.intercept_)\n# For retrieving the slope:\nprint('(bi):',AP2_LR.coef_)\nprint('****************')","797115b3":"#cross validate para linear regression approach 2\nAP2_LR_CV_SCORE = cross_validate(AP2_LR, X_Train_Final_m1, Y_Train_Final_m1, cv=4,scoring=['neg_mean_absolute_error','neg_mean_squared_error'],return_estimator=True) #llamo a cross validate definiendo feautures del dataframe auxiliar, la variable de salida, el valor de k, y las metricas deseadas\n\n\nAP2_LR_CV_METRICS=pd.DataFrame(AP2_LR_CV_SCORE, columns=['test_neg_mean_squared_error']) #almaceno las metricas obtenidas en un dataframe\nAP2_LR_CV_METRICS.rename(columns={'test_neg_mean_squared_error':'MSE'}, inplace=True) #cambio los nombres predeterminados por nombres mas simples\nAP2_LR_CV_METRICS_2=pd.DataFrame() #creo un nuevo dataframe\n\nfor i in AP2_LR_CV_SCORE['estimator']: \n    AP2_LR_CV_METRICS_2.loc[i,'MAPE']=((i.predict(X_Train_Final_m1)-Y_Train_Final_m1).abs()\/Y_Train_Final_m1).sum()\/X_Train_Final_m1['Electrical'].count()*100 ##guardo en el dataframe, el MAPE ajustado de cada modelo a meida itera\nAP2_LR_CV_METRICS_2.reset_index(drop=True, inplace=True) #borro el indice para cambiar el codigo del modelo por numeros iniciando en 0\nAP2_LR_CV_METRICS_FINAL=pd.DataFrame(AP2_LR_CV_METRICS_2[['MAPE']].copy()) #genero un dataframe que copie las metricas del pirmer dataframe de metricas \nAP2_LR_CV_METRICS_FINAL[['MSE']]=AP2_LR_CV_METRICS[['MSE']].copy() #a\u00f1ado las columnas del segundo dataframe de metricas al dataframe final\n\nAP2_LR_CV_METRICS_FINAL=abs(AP2_LR_CV_METRICS_FINAL) #cambio todos los valores del dataframe final por sus valores absolutos para evitar negativos\n\nAP2_LR_CV_METRICS_FINAL.rename(index={0: 'Fold_1',1: 'Fold_2',2: 'Fold_3',3: 'Fold_4' }, inplace=True) #renombro los indices segun el numero de fold\nAP2_LR_CV_METRICS_FINAL #visualzo el datagrame final","72724eff":" #saving de dataframes en archivos csv\nPred_RF_AP2=pd.DataFrame()\nPred_RF_AP2['Id']=Original_Test['Id']\nPred_RF_AP2['SalePrice']=AP2_RF_PRED\nPred_RF_AP2.to_csv('Pred_RF_AP2.csv',index=False)\n\nPred_LR_AP2=pd.DataFrame()\nPred_LR_AP2['Id']=Original_Test['Id']\nPred_LR_AP2['SalePrice']=AP2_LR_PRED\nPred_LR_AP2.to_csv('Pred_LR_AP2.csv',index=False)\n\nPred_AB_AP2=pd.DataFrame()\nPred_AB_AP2['Id']=Original_Test['Id']\nPred_AB_AP2['SalePrice']=AP2_AB_PRED\nPred_AB_AP2.to_csv('Pred_AB_AP2.csv',index=False)","eb446897":"#crear un data frame de train con las variables categoricas codificadas y las variables cuantitativas\nX_Train_Final_m2=pd.DataFrame(X_Train_Final_m1[qualitative_model].copy())\ncolumns_3=Train_pca[quantitative_SP].columns\nfor i in columns_3:\n    X_Train_Final_m2[i]=Train_pca[i].copy()\nX_Train_Final_m2","a03c2a94":"#crear un data frame de test con las variables categoricas codificadas y las variables cuantitativas\nX_Test_Final_m2=pd.DataFrame(X_Test_Final_m1[qualitative_model].copy())\ncolumns_3=Test_pca[quantitative_SP].columns\nfor i in columns_3:\n    X_Test_Final_m2[i]=Test_pca[i].copy()\nX_Test_Final_m2.info()","e061bee0":"# crear una variable con lo valores del outcome del set de train\nY_Train_Final_m2=Y_Train_Final_m1.copy()\n\n# normalizar datos\nX_Train_Final_m2_N=skl.preprocessing.StandardScaler().fit(X_Train_Final_m2).transform(X_Train_Final_m2.astype(float))\nX_Train_Final_m2_N=pd.DataFrame(X_Train_Final_m2_N.copy(), columns=X_Train_Final_m2.columns) # renombrar columanas con los nombres del DF original\n\nX_Test_Final_m2_N=skl.preprocessing.StandardScaler().fit(X_Test_Final_m2).transform(X_Test_Final_m2.astype(float))\nX_Test_Final_m2_N=pd.DataFrame(X_Test_Final_m2_N.copy(), columns=X_Test_Final_m2.columns) # renombrar columanas con los nombres del DF original","7269f279":"#correr modelo PCA \npcs_train_3 = skl.decomposition.PCA()\npcs_train_3.fit(X_Train_Final_m2_N)\n#obtener estadisticas de los componentes principales\npcsSummary_train_3_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs_train_3.explained_variance_),\n                           'Proportion of variance': pcs_train_3.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs_train_3.explained_variance_ratio_)})\npcsSummary_train_3_df = pcsSummary_train_3_df.transpose() #transponer DF\npcsSummary_train_3_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_train_3_df.columns)+1)] #establecer el nombre de las columnas\npcsSummary_train_3_df.round(4) #precision de 4 decimales\npcsSummary_train_3_df.iloc[:,:30] #mostrar unicamente los 20 primeros PC","a929e424":"pcsComponents_train_3_df = pd.DataFrame(pcs_train_3.components_.transpose(), columns=pcsSummary_train_3_df.columns, \n                                index=X_Train_Final_m2_N.columns)\npcsComponents_train_3_df.iloc[:10,:25] #mostrar los pesos de los 13 primeros componentes","dfdb6878":"houses_red_df_train_3 = X_Train_Final_m2_N.dropna(axis=0)\nhouses_red_df_train_3 = X_Train_Final_m2_N.reset_index(drop=True)\nscores_train_3 = pd.DataFrame(pcs_train_3.fit_transform(skl.preprocessing.scale(houses_red_df_train_3.dropna(axis=0))), \n                      columns=[f'PC{i}' for i in range(1, 51)])\n#obtener los valores transformados de cada uno de PC para todos los registros del set de datos\ntransform_df_train_3 = pd.DataFrame(pcs_train_3.transform(X_Train_Final_m2_N), \n                      columns=pcsSummary_train_3_df.columns)\ntransform_df_train_3=transform_df_train_3.iloc[:,:25] #25 componentes principales\ntransform_df_train_3","772e7bf2":"#correr modelo PCA \npcs_test_3 = skl.decomposition.PCA()\npcs_test_3.fit(X_Test_Final_m2_N)\n#obtener estadisticas de los componentes principales\npcsSummary_test_3_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs_test_3.explained_variance_),\n                           'Proportion of variance': pcs_test_3.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs_test_3.explained_variance_ratio_)})\npcsSummary_test_3_df = pcsSummary_test_3_df.transpose() #transponer DF\npcsSummary_test_3_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_test_3_df.columns)+1)] #establecer el nombre de las columnas\npcsSummary_test_3_df.round(4) #precision de 4 decimales\npcsSummary_test_3_df.iloc[:,:20] #mostrar unicamente los 20 primeros PC\npcsComponents_test_3_df = pd.DataFrame(pcs_test_3.components_.transpose(), columns=pcsSummary_test_3_df.columns, \n                                index=X_Test_Final_m2_N.columns)\npcsComponents_test_3_df.iloc[:10,:13] #mostrar los pesos de los 13 primeros componentes\nhouses_red_df_test_3 = X_Test_Final_m2_N.dropna(axis=0)\nhouses_red_df_test_3 = X_Test_Final_m2_N.reset_index(drop=True)\nscores_test_3 = pd.DataFrame(pcs_test_3.fit_transform(skl.preprocessing.scale(houses_red_df_test_3.dropna(axis=0))), \n                      columns=[f'PC{i}' for i in range(1, 51)])\n#obtener los valores transformados de cada uno de PC para todos los registros del set de datos\ntransform_df_test_3 = pd.DataFrame(pcs_test_3.transform(X_Test_Final_m2_N), \n                      columns=pcsSummary_test_3_df.columns)\ntransform_df_test_3=transform_df_test_3.iloc[:,:25] # 25 pc\ntransform_df_test_3","7a04e400":"PCA_X_train_3 = transform_df_train_3 #definimos los feautures del set train para los modelos de prediccion\nPCA_Y_train_3 = Train_Edited[\"SalePrice\"] #definimos el target\nPCA_X_test_3 = transform_df_test_3 #definimos sobre que set de datos se desean realizar las prediccione\n\nPCA_RF_3 = RandomForestClassifier(n_estimators=100)  #llamamos al primer modelo definiendo el numero de estimadores\nPCA_RF_3.fit(PCA_X_train_3, PCA_Y_train_3) #asignamos el set de train\nPCA_RF_EXIST_3 = PCA_RF_3.predict(PCA_X_train_3)\nPCA_Y_prediction_3 = PCA_RF_3.predict(PCA_X_test_3) #asignamos sobre que deseamos calcular el predictor\nPCA_ACC_RF_3 = PCA_RF_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_RF_3= skl.metrics.mean_absolute_error(PCA_RF_EXIST_3, PCA_Y_train_3)\nPCA_MSE_RF_3= skl.metrics.mean_squared_error(PCA_RF_EXIST_3, PCA_Y_train_3)\nPCA_MSE_RF_3","88369fee":"from sklearn.model_selection import train_test_split\n# partici\u00f3n de set de datos para poder obtener el mejor n\nPCA_split=PCA_X_train_3.copy()\nPCA_split['Sale Price']= Train_Edited[\"SalePrice\"]\nPCA_train_split, PCA_test_split = train_test_split(PCA_split,test_size=0.25, random_state=1)\nprint('Training : ', PCA_train_split.shape) #imprimir la dimensionalidad del set de entrenamiento\nprint('Test : ', PCA_test_split.shape) #imprimir dimensionalidad del set de test","e482f061":"X_train_split=PCA_train_split.drop(columns='Sale Price')\ny_train_split=PCA_train_split['Sale Price']\nX_test_split=PCA_test_split.drop(columns='Sale Price')\ny_test_split=PCA_test_split['Sale Price']\nresults=[]\nfor k in range(1,100):\n    PCA_RF_3 =  RandomForestClassifier(n_estimators=k).fit(X_train_split,y_train_split)\n    results.append({\n        'k': k,\n        'MAE': skl.metrics.mean_absolute_error(y_test_split, PCA_RF_3.predict(X_test_split)),\n        'MSE': skl.metrics.mean_squared_error(y_test_split, PCA_RF_3.predict(X_test_split))\n    })\n# Convert results to a pandas data frame\nresults = pd.DataFrame(results)\nresults.sort_values([\"k\",'MAE','MSE'],ascending=True).head(10)","a56ab64e":"print('El mejor valor de n es: \\n', results.sort_values(['MAE','MSE'],ascending=True).iloc[0,0:3])","c3604b0a":"PCA_RF_3 = RandomForestClassifier(n_estimators=91)  #llamamos al primer modelo definiendo el numero de estimadores\nPCA_RF_3.fit(PCA_X_train_3, PCA_Y_train_3) #asignamos el set de train\nPCA_RF_EXIST_3 = PCA_RF_3.predict(PCA_X_train_3)\nPCA_Y_prediction_3 = PCA_RF_3.predict(PCA_X_test_3) #asignamos sobre que deseamos calcular el predictor\nPCA_ACC_RF_3 = PCA_RF_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_RF_3= skl.metrics.mean_absolute_error(PCA_RF_EXIST_3, PCA_Y_train_3)\nPCA_MSE_RF_3= skl.metrics.mean_squared_error(PCA_RF_EXIST_3, PCA_Y_train_3)\nPCA_MSE_RF_3","92e877b1":"# adaboosreg model\nPCA_AB_3=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=100)\nPCA_AB_3.fit(PCA_X_train_3,PCA_Y_train_3)\nPCA_Y_pred_ADAB_3=PCA_AB_3.predict(PCA_X_test_3)\nPCA_AB_EXIST_3 = PCA_AB_3.predict(PCA_X_train_3)\nPCA_ACC_AB_3= PCA_AB_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_AB_3= skl.metrics.mean_absolute_error(PCA_AB_EXIST_3, PCA_Y_train_3)\nPCA_MSE_AB_3= skl.metrics.mean_squared_error(PCA_AB_EXIST_3, PCA_Y_train_3)\n#referencia: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html","4448d436":"results_ab=[]\nfor b in range(1,100):\n    PCA_AB_rf =  skl.ensemble.AdaBoostRegressor(n_estimators=b).fit(X_train_split,y_train_split)\n    results_ab.append({\n        'b': b,\n        'MAE': skl.metrics.mean_absolute_error(y_test_split, PCA_AB_rf.predict(X_test_split)),\n        'MSE': skl.metrics.mean_squared_error(y_test_split, PCA_AB_rf.predict(X_test_split))\n    })\n# Convert results to a pandas data frame\nresults_ab = pd.DataFrame(results_ab)","561d8f68":"results_ab.sort_values(['MAE'],ascending=True).head()","34d9d92a":"# adaboosreg model\nPCA_AB_3=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=84)\nPCA_AB_3.fit(PCA_X_train_3,PCA_Y_train_3)\nPCA_Y_pred_ADAB_3=PCA_AB_3.predict(PCA_X_test_3)\nPCA_AB_EXIST_3 = PCA_AB_3.predict(PCA_X_train_3)\nPCA_ACC_AB_3= PCA_AB_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_AB_3= skl.metrics.mean_absolute_error(PCA_AB_EXIST_3, PCA_Y_train_3)\nPCA_MSE_AB_3= skl.metrics.mean_squared_error(PCA_AB_EXIST_3, PCA_Y_train_3)\n#referencia: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html","7dff4d02":"#linear regresion model\nPCA_LR_3=skl.linear_model.LinearRegression()\nPCA_LR_3.fit(PCA_X_train_3, PCA_Y_train_3)\nPCA_Y_pred_LR_3= PCA_LR_3.predict(PCA_X_test_3)\nPCA_LR_EXIST_3 = PCA_LR_3.predict(PCA_X_train_3)\nPCA_ACC_LR_3 = PCA_LR_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_LR_3= skl.metrics.mean_absolute_error(PCA_LR_EXIST_3, PCA_Y_train_3)\nPCA_MSE_LR_3= skl.metrics.mean_squared_error(PCA_LR_EXIST_3, PCA_Y_train_3)","d68c8646":"#saving de dataframes en archivos csv excluyendo la columna de index para datos CON y SIN AGE\nPred_RF_PCA_3=pd.DataFrame()\nPred_RF_PCA_3['Id']=Original_Test['Id']\nPred_RF_PCA_3['SalePrice']=PCA_Y_prediction_3\nPred_RF_PCA_3.to_csv('Pred_RF_PCA_3.csv',index=False)\n\nPred_LR_PCA_3=pd.DataFrame()\nPred_LR_PCA_3['Id']=Original_Test['Id']\nPred_LR_PCA_3['SalePrice']=PCA_Y_pred_LR_3\nPred_LR_PCA_3.to_csv('Pred_LR_PCA_3.csv',index=False)\n\nPred_AB_PCA_3=pd.DataFrame()\nPred_AB_PCA_3['Id']=Original_Test['Id']\nPred_AB_PCA_3['SalePrice']=PCA_Y_pred_ADAB_3\nPred_AB_PCA_3.to_csv('Pred_AB_PCA_3.csv',index=False)","fe82b97b":"Accuracy_df = pd.DataFrame({'Model': ['Random Forest','Linear Reg','ADA Boost'],\n                        'AP-2 Acc':[AP2_ACC_RF, AP2_ACC_LR, AP2_ACC_AB],\n                        '13 PCA Acc':[PCA_ACC_RF_13,PCA_ACC_LR_13,PCA_ACC_AB_13],\n                        '20 PCA Acc':[PCA_ACC_RF_20,PCA_ACC_LR_20,PCA_ACC_AB_20],\n                        '25 PCA Acc':[PCA_ACC_RF_3,PCA_ACC_LR_3,PCA_ACC_AB_3],\n                        'AP-2 MAE':[AP2_MAE_RF,AP2_MAE_LR,AP2_MAE_AB],\n                        '13 PCA MAE':[PCA_MAE_RF_13,PCA_MAE_LR_13,PCA_MAE_AB_13],  \n                        '20 PCA MAE':[PCA_MAE_RF_20,PCA_MAE_LR_20,PCA_MAE_AB_20],\n                        '25 PCA MAE':[PCA_MAE_RF_3,PCA_MAE_LR_3,PCA_MAE_AB_3],\n                        'AP-2 MSE': [AP2_MSE_RF, AP2_MSE_LR, AP2_MSE_AB],\n                        '13 PCA MSE':[PCA_MSE_RF_13,PCA_MSE_LR_13,PCA_MSE_AB_13],\n                        '20 PCA MSE':[PCA_MSE_RF_20,PCA_MSE_LR_20,PCA_MSE_AB_20],\n                        '25 PCA MSE':[PCA_MSE_RF_3,PCA_MSE_LR_3,PCA_MSE_AB_3],   \n                        })\nAccuracy_df               ","84dcb115":"## Referencias\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html\n\nhttps:\/\/www.pluralsight.com\/guides\/different-ways-create-numpy-arrays\n\nhttps:\/\/www.guru99.com\/numpy-statistical-function.html\n\nhttps:\/\/stackoverflow.com\/questions\/45416684\/python-pandas-replace-multiple-columns-zero-to-nan?rq=1\n\nhttps:\/\/kanoki.org\/2019\/07\/17\/pandas-how-to-replace-values-based-on-conditions\/\n\nhttps:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html\n\nhttps:\/\/www.kaggle.com\/dgawlik\/house-prices-eda\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html\n\nhttps:\/\/www.pluralsight.com\/guides\/different-ways-create-numpy-arrays\n\nhttps:\/\/www.guru99.com\/numpy-statistical-function.html\n\nhttps:\/\/stackoverflow.com\/questions\/45416684\/python-pandas-replace-multiple-columns-zero-to-nan?rq=1\n\nhttps:\/\/kanoki.org\/2019\/07\/17\/pandas-how-to-replace-values-based-on-conditions\/\n\nhttps:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html\n\nhttps:\/\/www.kaggle.com\/dgawlik\/house-prices-eda","bf770d07":"## Guardado de Dataframes","338d2f01":"## Completado de set de entranamiento mediante llenado de nulos y codificacion de variables cualitativas","dc274dc5":"> #### Refinamiento de n:","33e4e6ef":"## Modelos","2ecc5e80":"## Modelos","cdaf267c":"> #### La variable Alley cuenta con el 94% de datos perdidos; sin embargo, es posible corregir en su totalidad este error dado que el hecho de que no existan valores de esta variable para las distintas casas quiere decir que estas no tienen alley. De manera adicional, se realiz\u00f3 una codificaci\u00f3n de la variable categ\u00f3rica.\n","8f2feb10":"## Completado de set de entranamiento y test mediante llenado de nulos de variables cuantitativas","f00249b8":"> ### 13 PC (69 % de variabilidad explicada)","0eb3a17c":"> Se usar\u00e1 el algoritmo predefinido de PCA para reducir la dimensionalidad de los set de datos","f18cbdf5":"> #### La variable MiscFeature cuenta con el 97% de datos perdidos; sin embargo, es posible corregir en su totalidad este error creando dummy variables a partir de sus categor\u00edas\n","4409527b":"# Approach #3","261ca226":"> ### 20 PC (85 % de variabilidad explicada)","db9b5716":"> ### AdaBoost","ae1bb147":"> En base al an\u00e1lisis de correlaci\u00f3n realizado, es posible concluir que ninguna de las categor\u00edas de MiscFeature influye en el precio, por lo que la variable puede ser removida con seguridad","1e782904":"## Adici\u00f3n de las nuevas variables y codificaci\u00f3n de las existentes","cf96eb54":"> #### Refinamiento de n:","fa1eae21":"## Cross Validation for RL (13 PC)","47e8e80f":"## Resultados Generales","62ecddf0":"> #### La variable Fence cuenta con el 81% de datos perdidos; sin embargo, es posible corregir en su totalidad este error dado que el hecho de que no existan valores de esta variable para las distintas casas quiere decir que estas no tienen fence. De manera adicional, se realiz\u00f3 una codificaci\u00f3n de la variable categ\u00f3rica.","29f58809":"> Los valores perdidos de las variables de inter\u00e9s ser\u00e1n reemplazados por la mediana del correspondiente set de datos. Seg\u00fan Schmueli, la mediana es una medida de tendencia central robusta hacia los datos at\u00edpicos, por lo que su uso para asignaci\u00f3n de valores perdidos es recomendado sobre la media de la muestra.","a1b5213f":"> ### Random Forest","24b9d4a0":"# Approach #2","c276cf67":"> #### La variable MiscFeature cuenta con el 47% de datos perdidos; sin embargo, es posible corregir en su totalidad este error dado que el hecho de que no existan valores de esta variable para las distintas casas quiere decir que estas no tienen fire place. De manera adicional, se realiz\u00f3 una codificaci\u00f3n de la variable categ\u00f3rica.\n","67f2b096":"> ### Linear Regression","58152b30":"## Guardado de resultados","76bdce7e":"## Limpieza de Datos y asignaci\u00f3n de valores perdidos\n","6bf270ce":"### Cross Validation for LR","c0fdcb69":"## Estad\u00edsticas set de datos originales","7cee5fef":"## Modelos","8191f874":"## Reducci\u00f3n de Dimensionalidad","f31565ac":"## Guardado de Dataframes","80ad9ea5":"# Universidad San Francisco de Quito\n## IIN-3007 Anal\u00edtica de Datos\n### Paralelo: 1\n### NRC: 1635\n### Semestre: Verano 2019-2020 (201930)\n## Los Pollos Hermanos:\n### Ana Caama\u00f1o, 200229\n### Carlos Herrera, 200619\n### Yuvinne Guerrero, 201420\n### Arturo Romo Leroux, 201690\n\n## Avance Proyecto #2\n\n#### Fecha de entrega: 13-Julio-2020\n\n\n","85b1e555":"> #### La variable PoolQC cuenta con el 98% de datos perdidos; sin embargo, es posible corregir en su totalidad este error dado que el hecho de que no existan valores de esta variable para las distintas casas quiere decir que estas no tienen piscina. De manera adicional, se realiz\u00f3 una codificaci\u00f3n de la variable categ\u00f3rica."}}