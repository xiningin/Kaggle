{"cell_type":{"6e695655":"code","a253271d":"code","919c17e0":"code","2c79b75d":"code","4fc724f7":"code","83cc16a4":"code","f6ecd55b":"code","4db26b08":"code","4c76f86e":"code","2cfd45be":"code","125188fd":"code","3b4922e0":"code","386870d0":"code","e5e0437b":"code","9e7b380a":"code","b69a0546":"code","b42490c2":"code","d171f1b3":"code","06c89ea9":"code","b9ec75fa":"code","49579a64":"code","1dea1d66":"code","a083c6b4":"code","b209f7ad":"code","804ae677":"code","0a6f6576":"code","d7727645":"code","c1be8cca":"code","45da5038":"code","502c8473":"code","6d6503ba":"code","c0e3c4c2":"code","d0e330e7":"code","c7b3e543":"code","51908abf":"code","a2851945":"code","676a1cf3":"code","a794767d":"code","e1c91001":"code","98c688e9":"code","b3507a42":"code","7f0f018b":"code","1a1d0dc4":"code","d67fd080":"code","50e24afb":"code","cd15e0c2":"code","0c768b6c":"code","3334225d":"code","4baeca30":"code","db87faff":"code","8c1ff501":"code","ed99efa7":"code","92367c79":"code","d23e09bc":"code","79208c9d":"markdown","f6132b09":"markdown","7412c8bc":"markdown","78bcab1a":"markdown","9c4414fd":"markdown","d621835e":"markdown","cb715ba1":"markdown","f4482b7c":"markdown","f8a096e7":"markdown","709aa061":"markdown","6c12172f":"markdown","799635ce":"markdown","b698e8d1":"markdown","8f80bebd":"markdown","a51f7c0a":"markdown","fd18507b":"markdown","3c1581e2":"markdown","6ea56b06":"markdown","67697904":"markdown","3103c281":"markdown","60fcc584":"markdown","1a241f14":"markdown","241b6836":"markdown","52e88182":"markdown","1055a3c4":"markdown","e26a9f9b":"markdown","bd2c6d19":"markdown","a35d6211":"markdown","35ec3768":"markdown","ec94a20f":"markdown","1d7a4dc9":"markdown","afef66e3":"markdown","6ac144fe":"markdown","5f106c87":"markdown"},"source":{"6e695655":"# Importing numpy (linear algebra) and pandas (data processing): \nimport numpy as np \nimport pandas as pd \n\n# Imports for plotting:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nimport os\nimport matplotlib.ticker as ticker","a253271d":"# Explore what's in the cat-in-the-dat folder:\nprint(os.listdir(\"..\/input\/cat-in-the-dat\"))","919c17e0":"# Read train, test and sample_submission data:\ntrain_df = pd.read_csv(\"..\/input\/cat-in-the-dat\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/cat-in-the-dat\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/cat-in-the-dat\/sample_submission.csv\")","2c79b75d":"submission.head()","4fc724f7":"# Shape of the train and testdataset:\nprint(train_df.shape)","83cc16a4":"# To display first 5 rows of the train_df:\ntrain_df.head()","f6ecd55b":"# Print the names of all columns in train DataFrame:\nprint(train_df.columns.values)","4db26b08":"# Are there any missing values in train_df?\n# train_df.apply(axis=0, func=lambda x : any(pd.isnull(x)))","4c76f86e":"# Function to describe variables\ndef desc(df):\n    summ = pd.DataFrame(df.dtypes,columns=['Data_Types'])\n    summ = summ.reset_index()\n    summ['Columns'] = summ['index']\n    summ = summ[['Columns','Data_Types']]\n    summ['Missing'] = df.isnull().sum().values    \n    summ['Uniques'] = df.nunique().values\n    return summ\n\n# Function to analyse missing values\ndef nulls_report(df):\n    nulls = df.isnull().sum()\n    nulls = nulls[df.isnull().sum()>0].sort_values(ascending=False)\n    nulls_report = pd.concat([nulls, nulls \/ df.shape[0]], axis=1, keys=['Missing_Values','Missing_Ratio'])\n    return nulls_report","2cfd45be":"# Use desc function to describe test data:\ndesc(train_df)","125188fd":"# Bar chart of frequency of digit occurance in our train dataset:\ntotal = float(len(train_df))\n\nplt.figure(figsize=(16,4))\nax = sns.countplot(x = 'target', data=train_df,  palette = 'rocket_r')\n\n# Make twin axis\nax2=ax.twinx()\nax2.set_ylabel('Frequency [%]')\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height*100\/total),\n           # '{0:.0%}'.format(height\/total),\n            ha=\"center\") \n\n\n# Use a LinearLocator to ensure the correct number of ticks\nax.yaxis.set_major_locator(ticker.LinearLocator(11))\n\n# Fix the Frequency [%] range to 0-100\nax2.set_ylim(0,100)\nax.set_ylim(0,300000)\n\n# And use a MultipleLocator to ensure a tick spacing of 10\nax.yaxis.set_major_locator(ticker.MultipleLocator(25000))\nax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n\n# Turn the grid on ax2 off, otherwise the gridlines will cut through percentages %:\nax.grid(False)\nax2.grid(False)   \n    \nplt.title('Target Distribution')\nplt.show()","3b4922e0":"print(train_df['target'].value_counts())","386870d0":"# Define bin list:\nbin = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']","e5e0437b":"# Bar charts for binary features, split according to the target:\nfor i in bin:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'ocean_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}'.format(height*100\/total),\n                #'{0:.0%}'.format(height\/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,200000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()","9e7b380a":"# Define nom as:\nnom = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']","b69a0546":"# Bar charts for nominal features, split according to the target:\nfor i in nom[0:5]:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'gist_heat_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height*100\/total),\n                #'{0:.0%}'.format(height\/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,100000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()  ","b42490c2":"# Create a crosstab with nom_1 and target:\nprint('Crosstab for numerical target distribution in nom_1:')\n\npd.crosstab([train_df.target], \n            [train_df.nom_1],\n             margins=True).style.background_gradient(cmap='autumn_r')","d171f1b3":"# Create a crosstab with nom_2 and target:\nprint('Crosstab for numerical target distribution in nom_2:')\n\npd.crosstab([train_df.target], \n            [train_df.nom_2],\n             margins=True).style.background_gradient(cmap='autumn_r')","06c89ea9":"# Create a crosstab with nom_3 and target:\nprint('Crosstab for numerical target distribution in nom_3:')\n\npd.crosstab([train_df.target], \n            [train_df.nom_3],\n             margins=True).style.background_gradient(cmap='autumn_r')","b9ec75fa":"ord = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']","49579a64":"# Bar charts for ordinal features, split according to the target:\n\nfor i in ord[0:3]:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'winter_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.1f}%'.format(height*100\/total),\n                #'{0:.0%}'.format(height\/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,150000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()","1dea1d66":"for i in ord[3:5]:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'winter_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                #'{:1.1f}%'.format(height*100\/total),\n                '{0:.0%}'.format(height\/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,35000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()","a083c6b4":"# Number of unique values in ord_5:\nprint('Number of unique values for ord_5: ' + str(train_df['ord_5'].nunique()))","b209f7ad":"print('Unique values of day:',train_df.day.unique())\nprint('Unique values of month:',train_df.month.unique())","804ae677":"cyc = ['day', 'month']\n\n\nfor i in cyc:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'cool_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                #'{:1.1f}%'.format(height*100\/total),\n                '{0:.0%}'.format(height\/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,60000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()      ","0a6f6576":"# Assign output target to the following variable:\ntarget = train_df['target']","d7727645":"# Merge train and test data into tetra_df and drop target and id column:\ntetra_df = train_df.append(test_df, ignore_index = True, sort = 'True')\ntetra_df = tetra_df.drop(['target', 'id'], axis = 1)","c1be8cca":"# Check if merge worked (must have 500,000 entries):\ntetra_df.shape","45da5038":"# Create indexes to separate data later:\ntrain_df_idx = len(train_df)\ntest_df_idx = len(tetra_df) - len(test_df)","502c8473":"# Convert T, F in bin_3 to binary values (0,1):\ntetra_df['bin_3'] = tetra_df['bin_3'].map({'T':1, 'F':0})\n\n# Similarly convert Y, N in bin_4 to binary values:\ntetra_df['bin_4'] = tetra_df['bin_4'].map({'Y':1, 'N':0})","6d6503ba":"# Check the outcome:\ntetra_df[bin].head()","c0e3c4c2":"# One hot encoding for column : nom_0 to nom_4\ntetra_df = pd.get_dummies(tetra_df, columns = nom[0:5],\n                        prefix = nom[0:5], \n                        drop_first = True)","d0e330e7":"# Encoding hex features\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nfeatures_hex = nom[5:]\n\nfor col in features_hex:\n    labelencoder.fit(tetra_df[col])\n    tetra_df[col] = labelencoder.transform(tetra_df[col])","c7b3e543":"#tetra_df[ord].head()","51908abf":"# Convert ord_1 by dictionary mapping as follows:\ntetra_df['ord_1'] = tetra_df['ord_1'].map({\n    'Novice': 0,\n    'Contributor': 1,\n    'Master': 2,\n    'Expert' : 3,\n    'Grandmaster': 4\n})\n\n# Similarly convert ord_2:\ntetra_df['ord_2'] = tetra_df['ord_2'].map({\n    'Freezing': 0,\n    'Cold': 1,\n    'Warm': 2,\n    'Hot' : 3,\n    'Boiling Hot': 4,\n    'Lava Hot' : 5\n})","a2851945":"# Change type of ord_3 to category, create a dictionary alph that orders letters alphabetically:\ntetra_df['ord_3'] = tetra_df['ord_3'].astype('category')\nalph = dict(zip(tetra_df['ord_3'],tetra_df['ord_3'].cat.codes))\n# Map alphord to ord_3 and change type of ord_3 to integer:\ntetra_df['ord_3'] = tetra_df['ord_3'].map(alph)\ntetra_df['ord_3'] = tetra_df['ord_3'].astype(int)\n\n# Similarly change ord_4:\ntetra_df['ord_4'] = tetra_df['ord_4'].astype('category')\nalph1 = dict(zip(tetra_df['ord_4'],tetra_df['ord_4'].cat.codes))\ntetra_df['ord_4'] = tetra_df['ord_4'].map(alph1)\ntetra_df['ord_4'] = tetra_df['ord_4'].astype(int)","676a1cf3":"# Create sorted list of ord_5 values (ordered alphabetically):\nordli = sorted(list(set(tetra_df['ord_5'].values)))\n\n# Create mapping dictionary alph2 for ord_5\nalph2 = dict(zip(ordli, range(len(ordli))))  \n\n# Map alph2 dictionary to ord_5\ntetra_df['ord_5'] = tetra_df['ord_5'].map(alph2)","a794767d":"# Cyclical encoding for day:\ntetra_df['day_sin'] = np.sin(2 * np.pi * tetra_df['day']\/7.0)\ntetra_df['day_cos'] = np.cos(2 * np.pi * tetra_df['day']\/7.0)\n\n# Cyclical encoding for month:\ntetra_df['month_sin'] = np.sin(2 * np.pi * tetra_df['month']\/12.0)\ntetra_df['month_cos'] = np.cos(2 * np.pi * tetra_df['month']\/12.0)","e1c91001":"# Show that Encoded values are now placed on the circle with radius 1 and origing at [0,0]:\nx = tetra_df.day_sin\ny = tetra_df.day_cos\n\ntetra_df.sample(5000).plot.scatter('day_sin','day_cos').set_aspect('equal')\ntetra_df.sample(5000).plot.scatter('month_sin','month_cos').set_aspect('equal')","98c688e9":"tetra_df = tetra_df.drop(['day', 'month'], axis = 1)","b3507a42":"# Print the names of all columns in tetra_df DataFrame:\n print(tetra_df.columns.values)","7f0f018b":"#from sklearn.preprocessing import MinMaxScaler\n#min_max_scaler = MinMaxScaler()\n\n# x returns a numpy array\n#x = tetra_df.values \n\n\n#x_scaled = min_max_scaler.fit_transform(x)\n#tetra_df = pd.DataFrame(x_scaled)","1a1d0dc4":"#tetra_df.describe()","d67fd080":"# Creating training and testing data:\ntraining = tetra_df[ : train_df_idx]\ntesting = tetra_df[test_df_idx :]","50e24afb":"# For splitting data we will be using train_test_split from sklearn:\nfrom sklearn.model_selection import train_test_split","cd15e0c2":"X = training\ny = target","0c768b6c":"# Splitting the training data into test and train, we are testing on 0.20 = 20% of dataset:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=13)","3334225d":"from xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_validate, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler","4baeca30":"xgb = XGBClassifier(objective= 'binary:logistic'\n                    , learning_rate=0.7\n                    , max_depth=3\n                    , n_estimators=250\n                    , scale_pos_weight=2\n                    , random_state=42\n                    , colsample_bytree=0.5\n                    )\n    \nxgb.fit(X_train, y_train)   ","db87faff":"y_predict = xgb.predict(X_test)\nprint(classification_report(y_test,y_predict))","8c1ff501":"# Confusion matrix cm:\ncm = confusion_matrix(y_test,y_predict)\ncm","ed99efa7":"# Quick overview of our confusion matrix:\nsns.heatmap(cm, annot = True, square = True, fmt='g')","92367c79":"prediction = xgb.predict(testing)","d23e09bc":"# Combine ImageID and Label into one DataFrame:\nfinal_result = pd.DataFrame({'target': prediction, 'id': submission.id})\nfinal_result = final_result[['id', 'target']]\n\n# Downloading final_result dataset as digit_output.csv:\nfinal_result.to_csv('cat_output.csv', index = False)","79208c9d":"### Importing Libraries <a class=\"anchor\" id=\"imports\"><\/a>","f6132b09":"Interesting! We don't have much data for June and for Saturdays. ","7412c8bc":"#### **Nominal (nom_) features** <a class=\"anchor\" id=\"nominal_features\"><\/a>","78bcab1a":"Both sin and cos values will be in the range between -1 and 1.","9c4414fd":"### Cyclical features encoding","d621835e":"### Normalize data columns","cb715ba1":"In this competition, we will be predicting the probability [0, 1] of a binary target column.\n\nThe data contains binary features (bin_), nominal features (nom_), ordinal features (ord_) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.\n\nSince the purpose of this competition is to explore various encoding strategies, the data has been simplified in that (1) there are no missing values, and (2) the test set does not contain any \"unseen\" feature values. (Of course, in real-world settings both of these factors are often important to consider!)","f4482b7c":"#### **Cyclical features** <a class=\"anchor\" id=\"cyclical_features\"><\/a>","f8a096e7":"### Table of Content","709aa061":"- [Importing Libraries](#imports)\n- [Exploring the Data](#explore_data)\n   - [Binary features](#binary_features)\n   - [Nominal features](#nominal_features)\n   - [Ordinal features](#ordinal_features)\n   - [Cyclical features](#cyclical_features)\n- [Categorical Feature Encoding](#cat)  \n   - [Binary features encoding](#bin_cat)\n   - [Nominal features encoding](#nom_cat)\n   - [Ordinal features encoding](#ord_cat)\n   - [Cyclical features encoding](#cyc_cat)","6c12172f":"### Nominal features encoding","799635ce":"#### **Ordinal (ord_) features** <a class=\"anchor\" id=\"ordinal_features\"><\/a>","b698e8d1":"We still have columns from nom_5 to nom_9, those hold from 222 to 11,981 categories respectively. Let's have a look at how many categories each of the columns hold:","8f80bebd":"**Checking for missing data (nan)**","a51f7c0a":"### Training the Model","fd18507b":"In our train_df we have 300,000 rows of data with 208,236 (69.41%) rows with the target of 0 and 91,764 (30.59%) rows with the target of 1. ","3c1581e2":"**Target distribution**","6ea56b06":"####\u00a0**Binary (bin_) features** <a class=\"anchor\" id=\"binary_features\"><\/a>","67697904":"Before we start working on feature encoding, we will combine train_df and test_df into one DataFrame called tetra_df and separate target column. This will allow us to make changes to both DataFrames at the same time.","3103c281":"For ord_5 we have 192 unique values, all of them consist of 2 alphabet letters.","60fcc584":"**Names of all columns**","1a241f14":"Since bin_3 and bin_4 contain only two values, we can convert them to a binary columns. Let's assume that: <br>\n => T = True and F = False, <br>\n => Y = Yes and N = No <br>\nWe can just simply replace T by1 in bin_3, F by 0 and Y by 1, N by 0 in bin_4.  \n","241b6836":"This is interesting: there are some similarities in Target Distributions for nom1, nom2 and nom3. To be more at target distribution rounded to the nearest integer and compare the following: \n- Trapezoid, Lion, Russia (24%,10%)\n- Square, Cat, Canada (11%, 6%)\n- Star, Snake, China (11%, 5%)\n- Circle, Dog, Finaland (9%, 3%)\n- Polygon, Axolotl, Costa Rica (8%, 4%)\n- Triangle, Hamster, India (6%,4%)\n\nLet's have a look at the value tables for nom_1, nom_2 and nom_3, just to confirm that Target Distribution is very similar for all three features.","52e88182":"# **Is there a cat in your dat?**","1055a3c4":"### Ordinal features encoding","e26a9f9b":"One of the methods for cyclical features encoding is to perform sine and cosine transformation of the feature by using the following formulas:\n\n$$x_{sin} = sin(\\frac{2*\\pi*x}{max(x)})$$\n\n$$x_{cos} = cos(\\frac{2*\\pi*x}{max(x)})$$\n\nSince both trigonometric functions are periodical, it's not a good idea to use only one of them for encoding. The reason is simple: two different features can be encoded as the same value. <br>\nBy using sin and cos function we will avoid this and assign an unique position on a [unit circle](http:\/\/mathworld.wolfram.com\/UnitCircle.html).","bd2c6d19":"Hours of the day, days of the week, months in a year are all examples of features that are cyclical. In our DataFrame we have days and months, let's have a look at unique values for those features.","a35d6211":"## Categorical Features Encoding <a class=\"anchor\" id=\"cat\"><\/a>","35ec3768":"Columns bin_3 and bin_4 contain T,F and Y,N respectively, isntead of numerical values 0,1.","ec94a20f":"**USE XGBoost CLASSIFIER**","1d7a4dc9":"### Binary features encoding","afef66e3":"### Overview\n\nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.\n\nBecause this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:\n\n- binary features\n- low- and high-cardinality nominal features\n- low- and high-cardinality ordinal features\n- (potentially) cyclical features <br>\n\nThis Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. \n\n![cat](https:\/\/i.kinja-img.com\/gawker-media\/image\/upload\/s--rqCW9nxC--\/c_scale,f_auto,fl_progressive,q_80,w_800\/p4b69sblvgebowkdhnfy.jpg)","6ac144fe":"As we could expect, we have 1-7 values for day and 1-12 values for month feature. ","5f106c87":"### Exploring the Data <a class=\"anchor\" id=\"explore_data\"><\/a>"}}