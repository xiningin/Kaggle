{"cell_type":{"dc5e2981":"code","f8a83afc":"code","bcd2cc50":"code","7aafe9df":"code","39b38ad3":"code","99c03d23":"code","a38cbbf7":"code","27ef4046":"code","3f59471e":"code","6a704a5e":"code","a5ccf345":"code","a70af200":"code","a4dd3cf6":"code","8a502d4f":"code","997f350a":"code","07cb7c0d":"code","f7d66cd5":"code","b97576c1":"markdown","2f337325":"markdown","6e0ad3b1":"markdown","9edf989f":"markdown","d5892b4b":"markdown","e5a50a18":"markdown","e1909e4e":"markdown","828d2495":"markdown","5024994c":"markdown","35976583":"markdown","c3661267":"markdown","ba6879ce":"markdown","28c06313":"markdown","5a798816":"markdown","5a6491ae":"markdown","4789a567":"markdown","16127869":"markdown","1b513f6e":"markdown","408bd551":"markdown","c0102aa7":"markdown","e0ebb2dd":"markdown","e7f4882a":"markdown","24cb2b72":"markdown","8f9004e7":"markdown","6b9a19e3":"markdown","2e10a1d1":"markdown","26ff129e":"markdown","f3779e26":"markdown","2bc551fd":"markdown","db7e9748":"markdown","4e70928d":"markdown","99f25be1":"markdown","cf9e5889":"markdown","48198195":"markdown","23693b5b":"markdown","1350c891":"markdown","0601c92b":"markdown","178e81f5":"markdown","f172ad7a":"markdown","dc3f3194":"markdown"},"source":{"dc5e2981":"import numpy as np \n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns","f8a83afc":"from math import sqrt\nn=300 # Sample size\ndata=np.random.uniform(low=1,high=10,size=n) # Sample\n#data=np.random.normal(loc=0,scale=10,size=n)\nbin_value=int(sqrt(len(data))) #choosing the number of bins to draw histogram\nsns.distplot(data,bins=bin_value);\nplt.xlabel(\"Sample Data\",size=15);\nplt.ylabel(\"Density\",size=15);\nplt.title(\"Histogram\",size=20);\nplt.show()","bcd2cc50":"import scipy.stats as stats\nstats.probplot(data, dist=\"norm\", plot=plt);","7aafe9df":"from scipy.stats import shapiro\nShapiro_statistic_value, Shapiro_p_value = shapiro(data)\nprint(Shapiro_statistic_value, Shapiro_p_value)","39b38ad3":"standardized_data=(data-np.mean(data))\/np.std(data)\nks_statistic,ks_p_value=stats.kstest(standardized_data, 'norm')#Compared with standard normal distribution\nks_statistic,ks_p_value","99c03d23":"# By now, we had known the value of maximum distance. If you are intrested in knowing where it is located.\n# Consider this code\n\nprobs=np.arange(1, n+1)\/n # n - sample size\nstzd_data=np.sort(standardized_data)\ntheoretical_values=stats.norm.ppf(probs)\nest_probs=np.interp(stzd_data,theoretical_values,probs)\nmax_id=np.argmax((probs-est_probs)**2)\nplt.plot(theoretical_values,probs,label='Standard Normal CDF')\nplt.plot(stzd_data,probs,label='Sample EDF')\nplt.axvline(stzd_data[max_id], color=\"red\", linestyle=\"dashed\", alpha=0.4)\nplt.plot([stzd_data[max_id], stzd_data[max_id]], [probs[max_id], est_probs[max_id]], color=\"red\")\nplt.xlabel('X',size=15)\nplt.ylabel('CDF(x)',size=15)\nplt.title('Illustration of the Kolmogorov\u2013Smirnov statistic',size=15)\nplt.legend()\nplt.show()","a38cbbf7":"import statsmodels.api as sm\nLilliefors_statistic,Lilliefors_p_value=sm.stats.diagnostic.lilliefors(data,'norm')\nLilliefors_statistic,Lilliefors_p_value","27ef4046":"stats.kstest(data,'norm',args=(np.mean(data),np.std(data,ddof=1))) \n#One can obsserve the value of test statistic is same for KS test,Lilliefors test, But the p-values are different.","3f59471e":"!pip install scikit-gof","6a704a5e":"from skgof import cvm_test\nCvM_statistic,CvM_pvalue=cvm_test(standardized_data,'norm')\nCvM_statistic,CvM_pvalue","a5ccf345":"AD_result = stats.anderson(data) #The sample data will standardized and compare with N(0,1)\nprint(AD_result)\nprint('-'*20)\nfor i in range(len(AD_result.critical_values)):\n    sl, cv = AD_result.significance_level[i], AD_result.critical_values[i]\n    if AD_result.statistic < AD_result.critical_values[i]:\n        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n    else:\n        print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))","a70af200":"AD_result.statistic,AD_result.significance_level,AD_result.critical_values","a4dd3cf6":"# To conduct AD test with unknown mean and variance use this method \n# statsmodels.stats.diagnostic.normal_ad() the estimates for parameters will obtained from sample","8a502d4f":"import pandas as pd\n\ndf=pd.DataFrame([{'SW (n<5000))':Shapiro_p_value,'KS':ks_p_value, 'L':Lilliefors_p_value, 'CvM':CvM_pvalue}])\nAD_SL=pd.Series(AD_result.significance_level)\nAD_CV=pd.Series(AD_result.critical_values)\nAD_statistic=pd.Series([AD_result.statistic]*len(AD_result.critical_values))\nAD_statistic\ndf_AD=pd.DataFrame({'AD_statistic':AD_statistic,'AD_CV':AD_CV, 'AD_SL':AD_SL})\n\nprint(\"AD Test Results\")\ndf_AD.style.apply(lambda x: [\"background: green\" \n                             if (i>=2 and (x.iloc[0] < x.iloc[1]))\n                             else (\"background: red\" if (i>=2 and (x.iloc[0] >= x.iloc[1])) else \"\")\n                             for i,v in enumerate(x)], axis = 1)\n\n#Sorry if you are reading this in github, the colours are not highlighted(try this in your code editor)","997f350a":"#At alpha=0.01\ndf.style.apply(lambda x: [\"background: green\" if v > 0.01 else \"background: red\" for v in x], axis = 1)","07cb7c0d":"#At alpha=0.05\ndf.style.apply(lambda x: [\"background: green\" if v > 0.05 else \"background: red\" for v in x], axis = 1)","f7d66cd5":"#At alpha=0.1\ndf.style.apply(lambda x: [\"background: green\" if v > 0.1 else \"background: red\" for v in x], axis = 1)","b97576c1":"### Q4. What is the best test for normality?","2f337325":"## Shapiro\u2013Wilk test","6e0ad3b1":"## Kolmogorov-Smirnov test and Lilliefors test","9edf989f":"## Questions","d5892b4b":"In statistics, many statistical tests and procedures are based on specific distributional assumptions. The assumption of normality is particularly common in classical statistical tests. Normality tests are used to determine if a data set is well-modeled by a normal distribution and to compute how likely it is for a random variable underlying the data set to be normally distributed. \n\nHere I just wanted to run few normality tests in **python** and make notes on their advantages and limitations. So that this notebook can be useful as a reference whenever required. My journey for knowing more about the normality tests is begin by asking myself the question \"Why there are so many normality tests?\"","e5a50a18":"- From the histogram we can clearly say the sample is not normally distributed.\n- But based on histogram, we **can't always be sure** to judge that the sample data is normally distributed or not, even when the histogram gives a signal that it is normal. The science of data was not allowing to do so. Click [here](https:\/\/stats.stackexchange.com\/questions\/51718\/assessing-approximate-distribution-of-data-based-on-a-histogram\/51753#51753) to know **why**.\n- One can consider the **KDE**(the line drawn in the plot), as an alternative(which reduces the arbitrariness of histograms) to test the normality for the given sample.\n- KDE also not preferred to test the normality when the sample size is small. \n- In practical situations one can't be come to the conclusion about the normality by just checking the KDE\/histogram. Check [this](https:\/\/stats.stackexchange.com\/questions\/129417\/if-my-histogram-shows-a-bell-shaped-curve-can-i-say-my-data-is-normally-distrib?noredirect=1&lq=1) for more information.\n- But this is the **most important** step to choose an appropriate normality test as it describes how the sample data is distributed.","e1909e4e":"But both the KS and CvM statistics are insensitive when the differences between the curves is most prominent near the beginning or end of the distributions. This is because, by construction, the EDFs converge to 0.0 and 1.0 at the ends and any deviations must be small. To know more about this click [here](https:\/\/stats.stackexchange.com\/questions\/437979\/limitations-of-kolmogorov-smirnov-test).\n\nThe Anderson-Darling (AD) test was developed in the 1950s as a weighted CvM test to overcome this problem.\n\n**Anderson-Darling (AD) test**:\n\nCompared with the Cram\u00e9r\u2013von Mises test, K-S test the Anderson\u2013Darling distance places more weight on observations in the tails of the distribution. If we calculate a weighted version of the Cram\u00e9r-von Mises (with weights inversely proportional to this variance) then we end up with the Anderson-Darling statistic. The K-S test is distribution free in the sense that the critical values do not depend on the specific distribution being tested. The Anderson-Darling test makes use of the specific distribution in calculating critical values. The Anderson-Darling test is an alternative to the chi-square and Kolmogorov-Smirnov goodness-of-fit tests. We can use the Anderson-Darling statistic to compare how well a data set fits different distributions. Kolmogorov-Smirnov test taken the maximum difference between the EDF curves, in Anderson-Darling test will consider all the differences.Overall Anderson-Darling test is more powerful than Kolmogorov-Smirnov test because of the more detailed comparison it does.","828d2495":"## Histogram, KDE","5024994c":"### Q2.Why do we have so many number of normality tests?\n\nBecause the tests are based on **different characteristics of the normal distribution** and the **_power_** of these tests differs depending on the nature of the non normality.\n\nNormality tests differ in the characteristic of the normal distribution they focus in, such as its\nskewness and kurtosis values,its distribution or characteristic function, and the linear relationship existing between a normally distributed variable and the standard normal z. The tests also differ in the level at which they compare the\nempirical distribution with the normal distribution (**compare and summarize vs. summarize and\ncompare**), in the complexity of the test statistic and the nature of its distribution (a standard\ndistribution or specified one).","35976583":"### Q1. Why can't we make decision about the normality by just checking the sample mean,median,mode,variance,skewness, kurtosis?","c3661267":"- This method is not preferred if the sample size is small.\n- **If the sample size is sufficiently large** most statistical significance tests may detect even trivial departures from the null hypothesis (i.e., although there may be some statistically significant effect, **it may be too small to be of any practical significance**); thus, additional investigation of the effect size is typically advisable, like a Q\u2013Q plot. So, if we have sufficiently large sample, we can opt this method **instead of statistical significance tests**. For more information check this [link](https:\/\/stats.stackexchange.com\/questions\/2516\/are-large-data-sets-inappropriate-for-hypothesis-testing) and this [link](https:\/\/stats.stackexchange.com\/questions\/2492\/is-normality-testing-essentially-useless).\n- As it needs the visual inspection, if we don't have enough sample, it is trickier to make conclusions.","ba6879ce":"- Efficient for small samples(n>8) too.\n- You can adjust the Anderson-Darling test statistic under the estimation of parameters from the sample for testing of normality\n- If we observe the the distribution of sample has the heavier tail, this is considered to be best test.","28c06313":"Eventually, I was curious to know the answers for these questions too.\n\n- Why can't we make decision about the normality of our sample by just checking the sample mean,median,mode,variance,skewness, kurtosis?\n- For what criteria we should look while choosing the normality test?\n- What is the best test for normality?","5a798816":"Note: To understand the following concepts clearly, knowing about some inferential statistical terms like test statistic,null distribution, power,p-value,critical values will helps a lot. Here I didn't provided the formulas of the test statistic. To know about the formula for a specific test statistic please refer their wikipedia page.","5a6491ae":"- Initially this test is feasible to apply for only small samples(<=50). But after Royston proposed an alternative method of calculating the coefficients vector by providing an algorithm for calculating values, which extended the sample size to 2,000. This technique is used in several software packages. Rahman and Govindarajulu extended the sample size further up to 5,000.\n\n- In python this test is not recommended to use if the sample size is above 5000. For N > 5000 the $W$ test statistic is accurate but the p-value may not be.\n(The accuracy of the p-value depends on **how close** the assumed distribution is **to the true distribution of the test statistic** under the null hypothesis.)\n\n- This test is not recommended to use if the data set has many identical values.\n- Shapiro\u2013Wilk has the best power under a broad range of useful alternatives for a given significance, when comparing with Anderson\u2013Darling , Kolmogorov\u2013Smirnov, Lilliefors tests.\n- Recommended when we don't have a particular alternative distribution in mind.","4789a567":"All these are discussed above whenever needed.\n\nEx1:\n- If you are not interested in the parameters of the normal and just want to simply determine if the distribution is normal or not S-W test, A-D test, Lilliefors test for testing normality with normal distribution unknown mean and variance is preferred.\n\n- If you are interested to compare with in specific parameters(Ex: In general we expect the error distribution ~ N(0,\u03c3)) one can opt the KS test,AD test.\n\n\nEx2:\n- SW test was preferred for small samples.\n- AD test was preferred if we have enough sample size.\n- The asymptotic power of KS test is 1.\n- If we have sufficiently large sample QQ plot is preferred.\n\nEx3:\n- If we observe the the distribution of sample has the heavier tail, AD best will preferred.\n- If we observe the the distribution of sample was skewed(i.e., The empirical distribution is summarized through its skewness and kurtosis statistics and compared to the skewness and kurtosis of the normal distribution.) tests based on skewness and kurtosis(Jarque\u2013Bera test) are preferred.\n- For the distributions that have slightly or definitely higher kurtosis than the normal, the skewness-kurtosis based tests are more powerful than the other types of test.\n\nEx4:\nNote: The power depends on the way in which the null hypothesis is false(i.e, Depends on how you defined your alternative hypothesis) \n\nSuppose that the hypothesis H0 : X\u223cN(\u03bc, \u03c3) is verified using the SW test.\n\nThree kinds of alternative hypothesis can be considered:\n\na)  X\u223cN(\u03bc,  \u03c3) with \u03bc \u2260 \u03bc0\n\nb) X is not normal with \u03bc = \u03bc0\n\nc) X is not normal with \u03bc \u2260 \u03bc0.\n\nThe null hypothesis is tested against with entirely different distribution also(lets say the x follows the Weibull distribution)\n\nThere are lots of studies available in the internet on the power of the test under different conditions.\n\nIn most of the software packages the the alternative hypothesis is simply X\u2241N(\u03bc,  \u03c3) as like the methods we discussed here.","16127869":"## Q-Q plot","1b513f6e":"# Graphical Methods (Visual checks)","408bd551":"A Q\u2013Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. The use of Q\u2013Q plots to compare two samples of data can be viewed as a **non-parametric approach** to comparing their underlying distributions. A Q\u2013Q plot is generally a more powerful approach to do this than the common technique of comparing histograms of the two samples. Q\u2013Q plots are also commonly used to compare a data set to a theoretical model by plotting their quantiles against each other. For normality test we compare our ordered sample data with the quantiles obtained from the standard normal distribution. Thus, Q\u2013Q plot help us to identify substantive departures from normality. The resulting image look close to a straight line if the data are approximately normally distributed. Deviations from a straight line suggest departures from normality. **The main step in constructing a Q\u2013Q plot is calculating or estimating the quantiles to be plotted.** To know how the quantiles are generated in scipy.stats.probplot() see [this](https:\/\/github.com\/ThisIsVenkatesh\/Normality-Tests\/blob\/master\/Tests\/QQ%20plot.ipynb) notebook.\n\n**Note**: The normal probability plot is a special case of the Q\u2013Q probability plot for a normal distribution.","c0102aa7":"One can get answers for the above questions in this notes, started with running each normality test and answered the above questions at last. I will keep updating this notebook when ever I learn a new normality test and try to make notes on its limitations and advantages.","e0ebb2dd":"**Cram\u00e9r\u2013von Mises test**: To simply describe, In KS test only the maximum distance is considered as test statistic. But, In Cram\u00e9r\u2013von Mises test, the test statistic will be based on all the deviations and it is sum of squares of the deviations. Empirical evidences suggest that the Cram\u00e9r\u2013von Mises test is usually more powerful than the Kolmogorov\u2013Smirnov test for a broad class of alternative hypothesis.","e7f4882a":"- For KS test, the asymptotic power of this test is 1.\n- **Perhaps** the limitation is that the distribution must be fully specified.\n- KS test can be appropriate to test the distribution of errors is normally distributed with mean 0 or not, to make decisions about the process.\n- It tends to be **more sensitive near the center of the distribution than at the tails**. If there are repeated deviations between the EDFs, or the EDFs have (or are adjusted to have) the same mean values, then the EDFs cross each other multiple times and the maximum deviation between the distributions is reduced. The Cramer-von Mises (CvM) test  that measures the sum of square deviations between the EDFs treats this case well.","24cb2b72":"As we know the normal distribution is symmetric and bell shaped. A simple method to test the normality is to observe the distribution of the sample data by the histogram.","8f9004e7":"The Kolmogorov-Smirnov (K-S) test is a non-parametric test based on the empirical distribution function (ECDF).\n\nThere are two standard versions of the Kolmogorov-Smirnov test:\n\n1. The one-sample KS, which tests if a sample of points X1,\u2026,Xn\u2208R fits a specific continuous distribution function F.\n2. The two-sample KS, which tests whether it is reasonable to assume that two sets of samples X1,\u2026,Xn and Y1,\u2026,Ym come from the same continuous distribution.\n\nSo, The Kolmogorov\u2013Smirnov test can be serve as a goodness of fit test(one-sample case). In the special case of testing for normality of the distribution\n\n\nH0 : X \u223c N(\u03bc, \u03c3) with specified \u03bc, \u03c3\n\nH1 : X \u2241 N(\u03bc, \u03c3)\n\nIn general samples are **standardized and compared with a standard normal distribution(i.e., with specified mean(0) and variance(1))**. \n\n**Test statistic**:The Kolmogorov\u2013Smirnov statistic **measures the supremum (greatest) distance** between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution(standard normal distribution in our case for testing normality). If the sample comes from the referenced distribution F(x), then  the test statistic converges to 0 almost.\n\n**The null distribution of this statistic** calculated under the null hypothesis that the sample is drawn from the reference distribution **converges to the Kolmogorov distribution**.\n\nThe goodness-of-fit test or the Kolmogorov\u2013Smirnov test can be constructed by using the critical values of the Kolmogorov distribution. This test is asymptotically valid when n tends to infinity.","6b9a19e3":"Code taken from [here](https:\/\/machinelearningmastery.com\/a-gentle-introduction-to-normality-tests-in-python\/)         ","2e10a1d1":"- One of the reason to create this notebook is most of the analysts are recommended to run more than one test when ever we are looking for normality tests. Lets see the results of different normality tests for our sample data. \n\n- There are few other tests based on sample skewness and kurtosis, entropy etc. I didn't explained them here. I will keep updating this notebook when ever I learn a new normality test and try to make notes on its limitations and advantages.","26ff129e":"**The results of a test for normality should not only report a p-value but they should be accompanied by a careful interpretation of the probability plot and skewness and kurtosis statistics for a complete diagnosis.**","f3779e26":"The simple answer is they don\u2019t completely characterize the data. For more information click [here](https:\/\/stats.stackexchange.com\/questions\/154951\/non-normal-distributions-with-zero-skewness-and-zero-excess-kurtosis)","2bc551fd":"# Final Notes :","db7e9748":"We have seen each of the common normality test is most powerful at some conditions.\n\nInstead of asking for best normality test think of this question \"Why are you testing normality?\" Because What is more important is to consider what it means when these tests of normality reject the null, or fail to reject the null.\n\nWhy is normality important in your application? With that information you can able to choose the appropriate test for your analysis.\n\nCheck [this](https:\/\/stats.stackexchange.com\/questions\/90697\/is-shapiro-wilk-the-best-normality-test-why-might-it-be-better-than-other-tests) and [this](https:\/\/stats.stackexchange.com\/questions\/371386\/a-powerful-test-for-any-distribution?rq=1). Definitely your perception towards the best test for normality will change.","4e70928d":"    1. Parameters of the normal distribution which you want to compare with the sample data.\n    2. Sample size\n    3. Which characteristic of the normal distribution you want to test with\n    4. Power","99f25be1":"# Statistical tests to test the normality","cf9e5889":"### Q3. For what criteria we should look while choosing the normality test?","48198195":"## Cram\u00e9r\u2013von Mises test and Anderson\u2013Darling test.","23693b5b":"The cell in red color means the test reject our null hypothesis.\n\nThe cell in green color means the test fail to reject the null hypothesis.\n\n$_Sorry if you are reading this in github, the colours are not highlighted(try this in your code editor)_$\n\nRemember we have taken n=300, try with different n values and different samples.\n\nHowever, It is completely possible that for p > _alpha_ and the data does not come from a normal population. **Failure to reject could be from the sample size being too small to detect the non-normality**. So, keep this in mind when interpreting the results. Click [here](https:\/\/stats.stackexchange.com\/questions\/114027\/distribution-hypothesis-testing-what-is-the-point-of-doing-it-if-you-cant-ac?rq=1), and [here](https:\/\/stats.stackexchange.com\/questions\/15696\/interpretation-of-shapiro-wilk-test) for deep understanding.\n\nI didn't mentioned about the outliers in the sample data.Outliers will effect any normality tests to some extent. I didn't taken any outliers in the sample.","1350c891":"## Summary of Test Result","0601c92b":"The Shapiro\u2013Wilk test tests the null hypothesis that a sample x1, ..., xn came from a normally distributed population. The Shapiro-Wilk test is for testing **normality with unspecified \u03bc and \u03c3**, i.e., the sample is from a normal distribution with unknown mean \u03bc and unknown SD \u03c3. This test exhibiting high _power_( probability that the test rejects the null hypothesis (H0) when a specific alternative hypothesis (H1) is true.), leading to good results **even with a small number of observations.**\n\nH0 : X \u223c N(\u03bc, \u03c3)\n\nH1 : X \u2241 N(\u03bc, \u03c3)\n\n**Test Statistic**: The test statistic is obtained by dividing the square of an appropriate linear combination of the sample order statistics by the usual symmetric estimate of variance. It may be noted that if one is indeed sampling from a normal population then the numerator, $b^2$, and denominator, $S^2$,of $W$(test Statistic) are both, up to a constant, estimating the **same quantity, namely $\u03c3^2$**.","178e81f5":"## References\n\n[Comparison of Tests for Univariate Normality](http:\/\/interstat.statjournals.net\/YEAR\/2002\/abstracts\/0201001.php)\n\n[An Analysis of Variance Test for Normality](https:\/\/www.jstor.org\/stable\/2333709?seq=1)\n\n[Beware the Kolmogorov-Smirnov test!](https:\/\/asaip.psu.edu\/Articles\/beware-the-kolmogorov-smirnov-test)\n\n[Distribution of the Anderson-Darling Statistic](https:\/\/projecteuclid.org\/euclid.aoms\/1177704850)","f172ad7a":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Graphical-Methods-(Visual-checks)\" data-toc-modified-id=\"Graphical-Methods-(Visual-checks)-1\">Graphical Methods (Visual checks)<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Histogram,-KDE\" data-toc-modified-id=\"Histogram,-KDE-1.1\">Histogram, KDE<\/a><\/span><\/li><li><span><a href=\"#Q-Q-plot\" data-toc-modified-id=\"Q-Q-plot-1.2\">Q-Q plot<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Statistical-tests-to-test-the-normality\" data-toc-modified-id=\"Statistical-tests-to-test-the-normality-2\">Statistical tests to test the normality<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Shapiro\u2013Wilk-test\" data-toc-modified-id=\"Shapiro\u2013Wilk-test-2.1\">Shapiro\u2013Wilk test<\/a><\/span><\/li><li><span><a href=\"#Kolmogorov-Smirnov-test-and-Lilliefors-test\" data-toc-modified-id=\"Kolmogorov-Smirnov-test-and-Lilliefors-test-2.2\">Kolmogorov-Smirnov test and Lilliefors test<\/a><\/span><\/li><li><span><a href=\"#Cram\u00e9r\u2013von-Mises-test-and-Anderson\u2013Darling-test.\" data-toc-modified-id=\"Cram\u00e9r\u2013von-Mises-test-and-Anderson\u2013Darling-test.-2.3\">Cram\u00e9r\u2013von Mises test and Anderson\u2013Darling test.<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Final-Notes-:\" data-toc-modified-id=\"Final-Notes-:-3\">Final Notes :<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Summary-of-Test-Result\" data-toc-modified-id=\"Summary-of-Test-Result-3.1\">Summary of Test Result<\/a><\/span><\/li><li><span><a href=\"#Questions\" data-toc-modified-id=\"Questions-3.2\">Questions<\/a><\/span><\/li><li><span><a href=\"#References\" data-toc-modified-id=\"References-3.3\">References<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","dc3f3194":"**<u>Test with estimated parameters (Special case):<\/u>** \nIf either the form or the parameters of F(x) are determined **from the data $X_i$** the critical values determined in this way are invalid. In such cases, Monte Carlo or other methods may be required, but tables have been prepared for some cases. Details for the **required modifications to the test statistic** and for the critical values **for the normal distribution** and the exponential distribution have been published. The Lilliefors test represents a special case of this **for the normal distribution**.\n\n**Lilliefors test**:\n\nThe Lilliefors test is a normality test based on the Kolmogorov\u2013Smirnov test. It is used to test the null hypothesis that data come from a normally distributed population, when the null hypothesis does not specify _which normal distribution_; i.e., it does not specify the expected value and variance of the distribution.\n\nInstead of comparing the standardized data with the standard normal distribution, we will compare the sample data by the normal distribution with the **estimated mean and estimated variance**. The test statistic will be same that used in Kolmogorov\u2013Smirnov test.\n\nThis is where this test becomes more complicated than the Kolmogorov\u2013Smirnov test. Since the hypothesised CDF has been moved closer to the data by estimation based on those data, the maximum discrepancy has been made smaller than it would have been if the null hypothesis had singled out just one normal distribution. **Thus the \"null distribution\" of the test statistic**, i.e., its probability distribution assuming the null hypothesis is true, is stochastically smaller than the Kolmogorov\u2013Smirnov distribution. This is the **Lilliefors distribution**.\n"}}