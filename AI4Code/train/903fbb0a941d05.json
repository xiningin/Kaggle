{"cell_type":{"e3f8e60a":"code","c50ddf1c":"code","025b8005":"code","feb8be28":"code","7b82b8d9":"code","ee3fd7a8":"code","5fabf4f2":"code","ba41bab1":"code","35df2c87":"code","6ba57471":"code","5cc4303b":"code","080d351b":"code","f6896a1b":"code","7dcc6b84":"code","8c1304ce":"code","c078453f":"code","54a430fd":"code","e8839f3f":"code","f59757e6":"code","f7f4ef22":"code","ab886bbf":"code","6380c3ff":"code","b8882cb8":"code","b48f9a04":"code","0f174e5f":"code","d50e98f1":"code","1c8d33b7":"code","acf8b6e7":"code","602d2040":"code","e7d11057":"code","52a9719f":"code","6fc2eadc":"code","88a6d09f":"code","e06b2723":"code","deddbfd1":"code","14b2a68f":"code","6edaa9fb":"code","ce50276b":"code","80f5b46a":"code","60c22662":"code","3fd483e9":"code","7ec3e2cd":"code","145b5c24":"code","2fd6759b":"code","c05e5eac":"code","be03da43":"code","6e472d7c":"code","fd44afdd":"code","3f2651b6":"code","3eae29f7":"code","b88b074e":"code","4638e677":"code","4026e9c3":"code","613f47fc":"code","142342a1":"code","3d724d99":"code","15c17a5b":"code","e60acf90":"code","20671f58":"code","5f15e190":"code","70450f5f":"code","45515576":"code","50a357bc":"code","afbf140d":"code","93fbdfe7":"code","d12248be":"code","7b91e677":"code","7a797018":"code","bee3367e":"code","001704e9":"code","bd778d01":"code","af4437e4":"code","49ed8035":"code","fb1da5d1":"markdown","9a1d7fe0":"markdown","93b80f84":"markdown","54cbe799":"markdown","0703e3d5":"markdown","acd94e59":"markdown","2f1b51d3":"markdown","20cf01f7":"markdown","ed83f1f3":"markdown","c4abc9e8":"markdown","c0d9a472":"markdown","44951bc8":"markdown","91ddc150":"markdown","7fed83c6":"markdown","fbd5789f":"markdown","fbc56454":"markdown","021f58cf":"markdown","d34c029f":"markdown","3258ec9a":"markdown","0b515a1c":"markdown","97ca5139":"markdown","a4c55fb4":"markdown","c514afaa":"markdown","751338e4":"markdown","e6305a2c":"markdown","f347a780":"markdown","a2c5644e":"markdown","6cee4732":"markdown","26d02a8a":"markdown","c6daf2b3":"markdown","f70409b8":"markdown","40c4cb25":"markdown","94935bd7":"markdown"},"source":{"e3f8e60a":"import pandas as pd # package for high-performance, easy-to-use data structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nfrom numpy import array\nfrom pandas import DataFrame,Series\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib.patches as patches\nfrom matplotlib import cm\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette() # init color object \nimport plotly.offline as py # create embed interactive plots\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go \nimport plotly.offline as offline\noffline.init_notebook_mode()\nimport plotly.tools as tls\nfrom scipy import interp\nimport squarify\nimport re\nfrom mpl_toolkits.basemap import Basemap\nfrom wordcloud import WordCloud\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Print all rows and columns\n#pd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)\nfrom sklearn import preprocessing\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nimport datetime as dt\nimport warnings\nimport string\nimport time\n# stop_words = []\nstop_words = list(set(stopwords.words('english')))\nwarnings.filterwarnings('ignore')\npunctuation = string.punctuation","c50ddf1c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nmy_pipeline = make_pipeline(Imputer(), RandomForestClassifier())\n# plot arrows\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom subprocess import check_output\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","025b8005":"def generate_data_audit(data,file_name):\n    \"\"\"\n    This function process the DataFrame and create a csv file with passed on file_name.\n    \"\"\"\n    d=data.dtypes[data.dtypes!=('object')].index.values\n    data[d]=data[d].astype('float64')\n    mean=DataFrame({'mean':data[d].mean()})\n    std_dev=DataFrame({'std_dev':data[d].std()})\n    missing= DataFrame({'missing':data[d].isnull().sum()})\n    obs=DataFrame({'obs':np.repeat(data[d].shape[0],len(d))},index=d)\n    missing_perc=DataFrame({'missing_perc':data[d].isnull().sum()\/data[d].shape[0]})\n    minimum=DataFrame({'min':data[d].min()})\n    maximum=DataFrame({'max':data[d].max()})\n    unique=DataFrame({'unique':data[d].apply(lambda x:len(x.unique()),axis=0)})\n    q5=DataFrame({'q5':data[d].apply(lambda x:x.dropna().quantile(0.05))})\n    q10=DataFrame({'q10':data[d].apply(lambda x:x.dropna().quantile(0.10))})\n    q25=DataFrame({'q25':data[d].apply(lambda x:x.dropna().quantile(0.25))})\n    q50=DataFrame({'q50':data[d].apply(lambda x:x.dropna().quantile(0.50))})\n    q75=DataFrame({'q75':data[d].apply(lambda x:x.dropna().quantile(0.75))})\n    q85=DataFrame({'q85':data[d].apply(lambda x:x.dropna().quantile(0.85))})\n    q95=DataFrame({'q95':data[d].apply(lambda x:x.dropna().quantile(0.95))})\n    q99=DataFrame({'q99':data[d].apply(lambda x:x.dropna().quantile(0.99))})\n    DQ=pd.concat([mean,std_dev,obs,missing,missing_perc,minimum,maximum,unique,q5,q10,q25,q50,q75,q85,q95,q99],axis=1)\n\n    c=data.dtypes[data.dtypes=='object'].index.values\n    Mean=DataFrame({'mean':np.repeat('Not Applicable',len(c))},index=c)\n    Std_Dev=DataFrame({'std_dev':np.repeat('Not Applicable',len(c))},index=c)\n    Missing=DataFrame({'missing':data[c].isnull().sum()})\n    Obs=DataFrame({'obs':np.repeat(data[d].shape[0],len(c))},index=c)\n    Missing_perc=DataFrame({'missing_perc':data[c].isnull().sum()\/data[c].shape[0]})\n    Minimum=DataFrame({'min':np.repeat('Not Applicable',len(c))},index=c)\n    Maximum=DataFrame({'max':np.repeat('Not Applicable',len(c))},index=c)\n    Unique=DataFrame({'unique':data[c].apply(lambda x:len(x.unique()),axis=0)})\n    Q5=DataFrame({'q5':np.repeat('Not Applicable',len(c))},index=c)\n    Q10=DataFrame({'q10':np.repeat('Not Applicable',len(c))},index=c)\n    Q25=DataFrame({'q25':np.repeat('Not Applicable',len(c))},index=c)\n    Q50=DataFrame({'q50':np.repeat('Not Applicable',len(c))},index=c)\n    Q75=DataFrame({'q75':np.repeat('Not Applicable',len(c))},index=c)\n    Q85=DataFrame({'q85':np.repeat('Not Applicable',len(c))},index=c)\n    Q95=DataFrame({'q95':np.repeat('Not Applicable',len(c))},index=c)\n    Q99=DataFrame({'q99':np.repeat('Not Applicable',len(c))},index=c)\n    dq=pd.concat([Mean,Std_Dev,Obs,Missing,Missing_perc,Minimum,Maximum,Unique,Q5,Q10,Q25,Q50,Q75,Q85,Q95,Q99],axis=1)\n\n    DQ=pd.concat([DQ,dq])\n    DQ.to_csv(file_name)\n    \n","feb8be28":"from nltk.corpus import stopwords #To check the list of stopwords \n\nREPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\ndef text_prepare(text):\n    \"\"\"\n    text: a string\n    return: modified initial string\n    \"\"\"\n    text = text.lower()# lowercase text  \n    text = REPLACE_BY_SPACE_RE.sub(' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text    \n    text = BAD_SYMBOLS_RE.sub('',text)# delete symbols which are in BAD_SYMBOLS_RE from text    \n    temp = [s.strip() for s in text.split() if s not in STOPWORDS]# delete stopwords from text\n    new_text = ''\n    for i in temp:\n        new_text +=i+' '\n    text = new_text\n    return text.strip()","7b82b8d9":"# Read the files in data frames\ndf_es = pd.read_csv(\"\/kaggle\/input\/outcome-value\/essays.csv\/essays.csv\")\ndf_out = pd.read_csv(\"\/kaggle\/input\/outcome-value\/outcomes.csv\/outcomes.csv\")\ndf_proj = pd.read_csv(\"\/kaggle\/input\/outcome-value\/projects.csv\/projects.csv\")","ee3fd7a8":"# Join the data frames based on the project id, leaving out rows which dont have any outcome\/classified yet\ndf = (df_proj.merge(df_es, left_index=True, right_index=True,\n                 how='inner', suffixes=('', '_y'))).merge(df_out, left_index=True, right_index=True,\n                 how='inner', suffixes=('', '_y'))  \ndf.drop(list(df.filter(regex='_y$')), axis=1, inplace=True)","5fabf4f2":"print(df_out.shape)\nprint(df_proj.shape)\nprint(df_es.shape)\nprint(df.shape)\n","ba41bab1":"# To free up some memory use garbage collector and set the initial data frames as null\nimport gc\ndf_es=pd.DataFrame()\ndf_out=pd.DataFrame()\ndf_proj=pd.DataFrame()\ndel [[df_es,df_out,df_proj]]\ngc.collect()\n\n","35df2c87":"# lets create a Directory and see what are the columns which have  only 3 unique values in them\nunique_dict = {}\nfor col in df.columns:\n    if len(df[col].unique())<4:\n        unique_dict[col] = df[col].unique()","6ba57471":"#cretaed the map assuming NAN as 0\nt_f_map ={'t': 1, 'f': 0,np.nan:0}","5cc4303b":"# for all columns apply the map. This will transform categorical to numeric variables. we dont require on hot Encoding here\nfor col in unique_dict.keys():\n    df[col]= df[col].map(t_f_map)","080d351b":"#create a Data Audit Report \ngenerate_data_audit(df,'data_audit.csv')","f6896a1b":"# get the missing data in number\/Percentages\ntotal = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(50)","7dcc6b84":"# with almost 30% blank value in secondary_focus_subject and secondary_focus_area there is no point in keeping this field in prediction model as there is no way to gather information for this.\ndf = df.drop(['secondary_focus_area','secondary_focus_subject'],axis =1)","8c1304ce":"# check the distribution of target class\ntemp = df['is_exciting'].value_counts()\nlabels = temp.index\nsizes = (temp \/ temp.sum())*100\ntrace = go.Pie(labels=labels, values=sizes, hoverinfo='label+percent')\nlayout = go.Layout(title='Project proposal is approved or not')\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","c078453f":"# check the distribution of proposal per state\ntemp = df[\"school_state\"].value_counts()\n#print(\"Total number of states : \",len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp \/ temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of School states in % \",\n    xaxis=dict(\n        title='State Name',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count of project proposals submitted in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='schoolStateNames')","54a430fd":"# Pictorial Display of state importance \ntemp = pd.DataFrame(df[\"school_state\"].value_counts()).reset_index()\ntemp.columns = ['state_code', 'num_proposals']\n\ndata = [dict(\n        type='choropleth',\n        locations= temp['state_code'],\n        locationmode='USA-states',\n        z=temp['num_proposals'].astype(float),\n        text=temp['state_code'],\n        colorscale='Red',\n        marker=dict(line=dict(width=0.7)),\n        colorbar=dict(autotick=False, tickprefix='', title='Number of project proposals'),\n)]\nlayout = dict(title = 'Project Proposals by US States',geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\nfig = dict(data=data, layout=layout)\npy.iplot(fig, validate=False)","e8839f3f":"# Check the proposal Disrtibution per Grade\ntemp = df[\"grade_level\"].value_counts()\nprint(\"Total number of project grade categories : \", len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp \/ temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of project_grade_category (school grade levels) in %\",\n    xaxis=dict(\n        title='school grade levels',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count of project proposals submitted in % ',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='schoolStateNames')","f59757e6":"print (df['primary_focus_area'].unique())\nprint(df['primary_focus_subject'].unique())","f7f4ef22":"temp = df[\"primary_focus_area\"].value_counts()\nprint(\"Total number of project based on focus area : \", len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp \/ temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of primary_focus_area (school grade levels) in %\",\n    xaxis=dict(\n        title='Primary focus area',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count of project proposals submitted in % ',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='focusArea')","ab886bbf":"temp = df[\"primary_focus_subject\"].value_counts()\nprint(\"Total number of project based on focus subject : \", len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp \/ temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of primary_focus_Subject (school grade levels) in %\",\n    xaxis=dict(\n        title='Primary focus Subject',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count of project proposals submitted in % ',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='focusSubject')","6380c3ff":"total_cnt = df[\"primary_focus_area\"].value_counts()\ntotal_exiting = df[\"primary_focus_area\"][df['is_exciting']==1].value_counts()\nimpact  = pd.concat([total_cnt, total_exiting], axis=1, keys=['Total', 'existing'])\nimpact['percentage_sucess'] = (impact['existing']\/impact['Total'])*100\n#impact.head(25)","b8882cb8":"# droping the primary_focus_subject parameter as this parameter is not providing any more insight than simply using primary_focus_area\n# more over the success percentage is simmilar accross Category.\ndf = df.drop(['primary_focus_subject'], axis =1)","b48f9a04":"# Sucess and rejection based on state\ntemp = df[\"school_state\"].value_counts()\n#print(temp.values)\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(df[\"is_exciting\"][df[\"school_state\"]==val] == 1))\n    temp_y0.append(np.sum(df[\"is_exciting\"][df[\"school_state\"]==val] == 0))    \ntrace1 = go.Bar(\n    x = temp.index,\n    y = temp_y1,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = temp_y0, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Popular School states in terms of project acceptance rate and project rejection rate\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","0f174e5f":"# Project Proposals Mean Acceptance Rate by US States\n\ntemp = pd.DataFrame(df.groupby(\"school_state\")[\"is_exciting\"].apply(np.mean)).reset_index()\ntemp.columns = ['state_code', 'num_proposals']\n\ndata = [dict(\n        type='choropleth',\n        locations= temp['state_code'],\n        locationmode='USA-states',\n        z=temp['num_proposals'].astype(float),\n        text=temp['state_code'],\n        colorscale='Red',\n        marker=dict(line=dict(width=0.7)),\n        colorbar=dict(autotick=False, tickprefix='', title='Number of project proposals'),\n)]\nlayout = dict(title = 'Project Proposals Mean Acceptance Rate by US States',geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\nfig = dict(data=data, layout=layout)\npy.iplot(fig, validate=False)","d50e98f1":"df[\"school_state\"].value_counts()","1c8d33b7":"# Check impact of Teacher's Prefix in outcome\ntemp = df[\"teacher_prefix\"].value_counts()\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(df[\"is_exciting\"][df[\"teacher_prefix\"]==val] == 1))\n    temp_y0.append(np.sum(df[\"is_exciting\"][df[\"teacher_prefix\"]==val] == 0))    \ntrace1 = go.Bar(\n    x = temp.index,\n    y = temp_y1,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = temp_y0, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Popular Teacher prefixes in terms of project acceptance rate and project rejection rate\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","acf8b6e7":"# numeric distribution of prefix\ndf['teacher_prefix'].value_counts()\n","602d2040":"# Dropping the location attributes and id as we will be using state as location parameter. \ndf = df.drop(['school_metro','school_zip','school_city','school_longitude','school_ncesid','school_latitude'], axis =1)\n\n                                            ","e7d11057":"#cretaed the map to get Female as 1 and Male as 0. In doctor we might have females, but considering the population we are assuming them to 0\ngender_map ={'Mrs.': 1, \n          'Ms.':1,\n          'Mr.': 0,\n          'Dr.':0,\n          'Mr. & Mrs.':0}","52a9719f":"# update the features with the map\ndf['teacher_prefix'] = df['teacher_prefix'].map(gender_map)","6fc2eadc":"df[\"date_posted\"] = pd.to_datetime(df[\"date_posted\"])\ndf[\"month_created\"] = df[\"date_posted\"].dt.month_name()\ndf[\"year\"] = df[\"date_posted\"].dt.year\ndf[\"weekday_created\"] = df[\"date_posted\"].dt.weekday_name\ndf = df.drop('date_posted',axis =1)","88a6d09f":"temp = df[\"month_created\"].value_counts()\n#print(temp.values)\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(df[\"is_exciting\"][df[\"month_created\"]==val] == 1))\n    temp_y0.append(np.sum(df[\"is_exciting\"][df[\"month_created\"]==val] == 0))\n    \ntrace1 = go.Bar(\n    x = temp.index,\n    y = temp_y1,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = temp_y0, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Project Proposal Submission Month Distribution\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)\n","e06b2723":"temp = df[\"weekday_created\"].value_counts()\n#print(temp.values)\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(df[\"is_exciting\"][df[\"weekday_created\"]==val] == 1))\n    temp_y0.append(np.sum(df[\"is_exciting\"][df[\"weekday_created\"]==val] == 0))\n \ntemp.index = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\ntrace1 = go.Bar(\n    x = temp.index,\n    y = temp_y1,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = temp_y0, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Project Proposal Submission weekday Distribution\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","deddbfd1":"#Teacher_prefix and is_exciting Intervals Correlation\ncols = ['teacher_prefix', 'is_exciting']\ncm = sns.light_palette(\"red\", as_cmap=True)\npd.crosstab(df[cols[0]], df[cols[1]]).style.background_gradient(cmap = cm)","14b2a68f":"#Correlation Matrix\ncorr = df.corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, annot=True, cmap='cubehelix', square=True)\nplt.title('Correlation between different features')\npd.DataFrame(corr).to_csv('corr.csv')\ncorr","6edaa9fb":"df['price'] = df['total_price_excluding_optional_support']+ df['total_price_including_optional_support']","ce50276b":"df = df.drop(['total_price_excluding_optional_support','total_price_including_optional_support'], axis =1)","80f5b46a":"second_analysis_false = {}\nsecond_analysis_true = {}\nfor col in df.columns :\n    if len(df[col].unique())<30:\n        second_analysis_false[col] = df[col][df['is_exciting']==0].value_counts().to_frame()\n        second_analysis_true[col] = df[col][df['is_exciting']==1].value_counts().to_frame()\n    ","60c22662":"with open('second_analysis_fasle.csv', 'w') as f:\n    for key in second_analysis_false.keys():\n        f.write(\"%s,%s\\n\"%(key,second_analysis_false[key]))","3fd483e9":"with open('second_analysis_true.csv', 'w') as f:\n    for key in second_analysis_true.keys():\n        f.write(\"%s,%s\\n\"%(key,second_analysis_true[key]))","7ec3e2cd":"# Word imporatcance cloud\ntemp_data = df.dropna(subset=['short_description'])\n# converting into lowercase\ntemp_data['short_description'] = temp_data['short_description'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntemp_data['short_description'] = temp_data['short_description'].map(text_prepare)\n\nwordcloud = WordCloud(max_font_size=50, width=600, height=300).generate(' '.join(temp_data['short_description'].values))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Word Cloud of short_description\", fontsize=35)\nplt.axis(\"off\")\nplt.show() ","145b5c24":"# quick view on the title parameter's effect on the proposal\ntemp = df[\"title\"].value_counts().head(25)\n#print(temp.values)\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(df[\"is_exciting\"][df[\"title\"]==val] == 1))\n    temp_y0.append(np.sum(df[\"is_exciting\"][df[\"title\"]==val] == 0))    \ntrace1 = go.Bar(\n    x = temp.index,\n    y = temp_y1,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = temp_y0, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Popular project titles in terms of project acceptance rate and project rejection rate\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","2fd6759b":"df['easay_len'] = df['essay'].apply(lambda x: len(str(x))) # Essay length\ndf['need_statement_len'] = df['need_statement'].apply(lambda x: len(str(x))) # Need Statement length\ndf ['short_description_len'] = df['short_description'].apply(lambda x: len(str(x))) # Short description length\ndf['title_len']=df['title'].apply(lambda x: len(str(x))) # title length","c05e5eac":"temp = pd.DataFrame()\ntemp['text'] = df.apply(lambda row: ' '.join([str(row['essay']), \n                                            str(row['need_statement']),\n                                            str(row['short_description']),\n                                            str(row['title'])\n                                            ]), axis=1)","be03da43":"df = df.drop(['essay','need_statement','short_description','title'],axis =1)","6e472d7c":"df['char_count'] = temp['text'].apply(len)\ndf['word_count'] = temp['text'].apply(lambda x: len(x.split()))","fd44afdd":"df['word_density'] = df['char_count'] \/ (df['word_count']+1)","3f2651b6":"df['word_count'] = temp['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))","3eae29f7":"df['upper_case_word_count'] = temp['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))","b88b074e":"df['stopword_count'] = temp['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))\n","4638e677":"df['punctuation_count'] = temp['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) ","4026e9c3":"generate_data_audit(df,'data_audit_before_Cat2Numeric.csv')","613f47fc":"print(df.shape)\ndf['teacher_prefix'].fillna(0, inplace = True)\ndf['fulfillment_labor_materials'].fillna(0, inplace = True)\ndf['fulfillment_labor_materials'] = df['fulfillment_labor_materials'].apply(lambda x:1 if x>17 else 0)\ndf['primary_focus_area'].fillna('Literacy & Language', inplace = True)\ndf['resource_type'].fillna('Other', inplace = True)\ndf['students_reached'].fillna(df['students_reached']. mean(), inplace = True)\ndf['grade_level'].dropna( axis =0,inplace = True)\ncolumn_to_drop = ['non_teacher_referred_count','teacher_referred_count','great_messages_proportion','school_county','school_district','teacher_acctid','schoolid']\ndf = df.drop(column_to_drop, axis =1)\nprint(df.shape)","142342a1":"#columns to apply normailzation\napply_normalization = ['students_reached','price','easay_len','need_statement_len',\n'short_description_len','title_len','char_count','word_count',\n'word_density','upper_case_word_count','stopword_count','punctuation_count'\n]\n\nfor i in apply_normalization:\n    df[i] = (df[i]-df[i].mean())\/(df[i].max() -df[i].min())\n    ","3d724d99":"# Create a CSV to revalidate the data under process after transformations\ngenerate_data_audit(df,'data_audit_before_Cat2Numeric_after Transform.csv')","15c17a5b":"state_map_bin = {'WY' : 'D','MT' : 'D','RI' : 'D','WV' : 'D','DC' : 'D',\n'VT' : 'D','OK' : 'D','MN' : 'D','NM' : 'C','LA' : 'C','SD' : 'C',\n'WA' : 'C','VA' : 'C','KY' : 'C','GA' : 'C','UT' : 'C','MA' : 'C',\n'TX' : 'C','HI' : 'C','IA' : 'C','SC' : 'C','KS' : 'C','FL' : 'C',\n'OH' : 'C','ME' : 'C','NY' : 'B','MD' : 'B','NH' : 'B','IL' : 'B',\n'IN' : 'B','CA' : 'B','MS' : 'B','PA' : 'B','CO' : 'B','MI' : 'B',\n'AR' : 'B','NC' : 'B','OR' : 'B','MO' : 'B','AZ' : 'B','WI' : 'B',\n'NV' : 'B','DE' : 'B','NE' : 'A','TN' : 'A','AL' : 'A','CT' : 'A',\n'NJ' : 'A','ND' : 'A','AK' : 'A','ID' : 'A'\n}\n\ndf['school_state'] = df['school_state'].map(state_map_bin)","e60acf90":"\ncols = ['school_state', 'grade_level','primary_focus_area', 'resource_type','poverty_level','month_created','weekday_created']\ndf_dummies = pd.get_dummies(df, columns =cols, drop_first = True)\ndf_dummies.shape","20671f58":"(df_dummies['year'][df_dummies['year']!= 2014]).value_counts()","5f15e190":"X_test = df_dummies[df_dummies['year']== 2014]\nX_train = df_dummies[df_dummies['year']!= 2014]\ny_train = (X_train['is_exciting'])\ny_train = y_train.astype(int)\ny_test = X_test['is_exciting'].astype(int)\nproject_id = X_test['projectid'] # to get the prediction report\ncl_to_drop = ['is_exciting','year','projectid']\nX_test.drop(cl_to_drop,axis =1, inplace = True)\nX_train.drop(cl_to_drop,axis =1, inplace = True)","70450f5f":"print('  \\n '.join((' X- Test',str(X_test.shape[0]),\n                    'y_test',str(y_test.shape[0]), \n                    'X_train',str(X_train.shape[0]),\n                    'y_train',str(y_train.shape[0])\n                                            )))","45515576":"# Create a Random Forest Model\nrandom_forest = RandomForestClassifier(\n    n_estimators=50,\n    criterion='gini',\n    max_depth=5,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features='auto',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=-1,\n    random_state=0,\n    verbose=0,\n    warm_start=False,\n    class_weight='balanced'\n)","50a357bc":"# reduced the model to 2\nmodels = { 'regr' :random_forest,\n          'logistic': LogisticRegression(random_state=0,max_iter=500)\n          }","afbf140d":"\"\"\"\nFunction to create Precision recal Graph \n\"\"\"\n\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n    plt.figure(figsize=(15, 8))\n    plt.show()\n","93fbdfe7":"# Data Frame to Capture Model evaluation parameter\nmodel_auc = pd.DataFrame(columns= ['Model','Fold','AUC'])","d12248be":"\"\"\"\nThis method create a comparison analysis of different models\n\"\"\"\n\ndef create_model_compare(model):\n    model_name = type(model).__name__\n    print (\"###############################################\")\n    print(\"Create Model for : \", model_name )\n    fig1 = plt.figure(figsize=[12,12])\n    ax1 = fig1.add_subplot(111,aspect = 'equal')\n    ax1.add_patch(\n        patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n        )\n    ax1.add_patch(\n        patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n        )\n\n    tprs = []\n    aucs = []\n    scores = []\n    results = pd.DataFrame(columns=['training_score', 'test_score'])\n    mean_fpr = np.linspace(0,1,100)\n    i = 1\n    cv = StratifiedKFold(n_splits=5, random_state=100,shuffle=True)\n   \n    for (train, test), i in zip(cv.split(X_train, y_train), range(5)):\n        prediction = model.fit(X_train.iloc[train],y_train.iloc[train]).predict_proba(X_train.iloc[test])\n        fpr, tpr, t = metrics.roc_curve(y_train.iloc[test], prediction[:, 1])\n        #plot_precision_and_recall(fpr, tpr, t)\n        tprs.append(interp(mean_fpr, fpr, tpr))\n\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n        i= i+1\n\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n    plt.plot(mean_fpr, mean_tpr, color='blue',\n             label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC')\n    plt.legend(loc=\"lower right\")\n    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n    plt.show()\n    print (\"###############################################\")","7b91e677":"# To store the models\nfinal_model ={}","7a797018":"for name , model in models.items():\n    fit_model = model.fit(X_train, y_train)\n    pred = fit_model.predict(X_test)\n    skfold = StratifiedKFold(n_splits=5, random_state=100,shuffle=True)\n    results_skfold = model_selection.cross_val_score(model, X_train, y_train, cv=skfold)\n    for count,ele in enumerate(results_skfold,1):\n        # Initialise data to lists. \n        data = [{'Model': name, 'Fold': count, 'AUC':ele}] \n        temp = pd.DataFrame(data)\n        model_auc = model_auc.append(temp)\n    \n    score = metrics.accuracy_score(y_test,pred)\n    print(\"{:3s}: {:0.2f}\".format(name,score))\n    print(\"Accuracy: %.2f%%\" % (results_skfold.mean()*100.0))\n    create_model_compare(model)## Run this only at training stage - should comment this out if dont require detail compared.\n    final_model[name] = fit_model\n    ","bee3367e":"# write the matrix to a CSV file for evaluation\nmodel_auc.to_csv('MODEL_FOLD_AUC.csv')","001704e9":"# Choosing the logstic regression model as this is simple and have better prediction\n\nfinal_ml_model = final_model.get('logistic')","bd778d01":"# get the probablity prediction from the model for class 1\ny_pred= final_ml_model.predict_proba(X_test)[:, 1]\ny_pred= pd.Series(y_pred)\nprint (y_pred.shape)","af4437e4":"# create a Data frmae to hold the prediction and project id. The Labvel is the original label from the data\nfinal_prediction_csv= pd.DataFrame(columns = ['is_exciting','projectid','Label'] )\nfinal_prediction_csv['is_exciting'] = np.around(y_pred, decimals=6)\nfinal_prediction_csv['projectid'] = project_id\nfinal_prediction_csv['Label'] = y_test","49ed8035":"# Create a CSV of predictions.\nfinal_prediction_csv.to_csv('final_prediction.csv')","fb1da5d1":"# Transform the catgorical to numerical using one hot encoding","9a1d7fe0":"(df['is_exciting'][df['teacher_prefix'].isnull()]).value_counts()","93b80f84":"# for analysis write Sample data to a CSV\n(df_dummies.head(50)).to_csv('final_data.csv')","54cbe799":"kl = df.groupby(['school_state','is_exciting']).size()\nkl.to_csv('State_analysis.csv')\n\ndf_s['Success_analysis']= (df_s['Success per'] - df_s['Success per'].mean())*100\/(df_s['Success per'].max()-df_s['Success per'].min()) <br>\n\nbins = [-100, -20, 0, 20, 100]<br>\nlabels = ['A','B','C','D']<br>\n\ndf_s['binned'] = pd.cut(df_s['Success_analysis'], bins=bins, labels=labels)<br>","0703e3d5":"# There is not much corelation except total_price_excluding_optional_support and total_price_including_optional_support.\nCreated a normalized price columns from . This is higly correlated feature\n","acd94e59":"To Simplify computation lets mark the nan as 0 for the below columns - \nat_least_1_teacher_referred_donor<br>\nat_least_1_green_donation<br>\nthree_or_more_non_teacher_referred_donors<br>\none_non_teacher_referred_donor_giving_100_plus<br>\ndonation_from_thoughtful_donor<br>","2f1b51d3":"df['is_exciting'][df['teacher_prefix'].isnull()]","20cf01f7":"# clearly there is a class imbalance in the data under analysis","ed83f1f3":"******Column With missing values** **- **\n1. teacher_prefix - 4 Will be replaced with 0 as (df['is_exciting'][df['teacher_prefix'].isnull()]).value_counts() has only 0<br>\n2. fulfillment_labor_materials - 48, Replacing 48 with 0 and create Bin '0'<17.1<'1' <br>\n. students_reached - 146 replacing with mean <br>\n3. great_messages_proportion - 179839 Dropping  because the missing percentage is very high and number of unique values are high. <br> \n4. teacher_referred_count - 94398 Dropping  because the missing percentage is very high and number of unique values are high. <br>\n5. non_teacher_referred_count - 94398  Dropping  because the missing percentage is very high and number of unique values are high.<br>\n6. school_district - 548 Dropping as Anyway considering state as location parameter<br>\n7. primary_focus_area - 39 , Marking as Literacy & Language as this is the higest category by count by a big percentage<br>\n8. resource_type - 45 , Marking this as Other category<br>\n9. grade_level - 9 , We have already a class imbalance in our data set. All the observations has is_exciting as 0. droping this records <br>\n10. dropping school_county as using state as paramter\n\n","c4abc9e8":"Again The importance of CA\/NY are far more than other states from FL onwards.","c0d9a472":"# Put States in Bin based on success rates","44951bc8":"# Text features extraction","91ddc150":"# So its clear CA has the higest number of submission and next based is NY. <br>\nThis seems to be a important variable to consider. Needs to check if we can bin it some categories.","7fed83c6":"\"\"\"print(df_out.shape)\nprint(df_proj.shape)\nprint(df_es.shape)\nprint(df_out.head())\nprint(df_proj.head())\nprint(df_es.head())\"\"\"","fbd5789f":"df['grade_level'].value_counts()","fbc56454":"# Need to check impact of primary_focus_subject and primary_focus_area\n\n","021f58cf":"# A look into the data tell us that lots of column has value 't','f' and nan as unique values. <br>\nWe need to remap t as 1 and f as 0.","d34c029f":"df.groupby(['resource_type','is_exciting']).size()\nresource_type  is_exciting\nBooks          0.0            124706<br>\n               1.0              7793<br>\nOther          0.0             62217<br>\n               1.0              3971<br>\nSupplies       0.0            198651<br>\n               1.0             12475<br>\nTechnology     0.0            193220<br>\n               1.0             12195<br>\nTrips          0.0              3189<br>\n               1.0               223<br>\nVisitors       0.0               595<br>\n               1.0                46<br>\ndtype: int64","3258ec9a":"There is a clear imbalance of gender vs submisition","0b515a1c":"# Create visualization of For  primary_focus_area and primary_focus_subject","97ca5139":"df.groupby(['fulfillment_labor_materials','is_exciting']).size()<br>\n\nfulfillment_labor_materials  is_exciting<br>\n9.0                          0.0             60308<br>\n                             1.0              3787<br>\n17.0                         0.0             87431<br>\n                             1.0              5505<br>\n30.0                         0.0            226703<br>\n                             1.0             14399<br>\n35.0                         0.0            208129<br>\n                             1.0             13016<br>\n\n\n30.0    241102<br>\n35.0    221145<br>\n17.0     92936<br>\n9.0      64095<br>","a4c55fb4":"![![image.png](attachment:image.png)](http:\/\/)\nClearly shows the importance of the parameters fully_funded\nat_least_1_green_donation\ngreat_chat\nthree_or_more_non_teacher_referred_donors\none_non_teacher_referred_donor_giving_100_plus","c514afaa":"print(df['school_metro'].value_counts())\nprint('Number of unique county - ',len(df['school_county'].unique()))\nprint('Number of unique school_metro - ',len(df['school_metro'].unique()))\nprint('Number of unique school_city - ',len(df['school_city'].unique()))\n","751338e4":"df.groupby(['resource_type','is_exciting']).size()","e6305a2c":"df.to_csv('output.csv')","f347a780":"# Text Processing","a2c5644e":"Need to get dummies for this columns. Out of 4 school grade levels, Project proposals submission in school grade levels is higher for **Grades Prek-2** which is approximately **41 %** followed by **Grades 3-5** which has approx. **34 %**.","6cee4732":"# Compared Different Models\n\nmodels = { 'kNN': KNeighborsClassifier(n_neighbors=3),<br>\n          'NB'  : GaussianNB(),<br>\n          'regr' :random_forest,<br>\n          'logistic': LogisticRegression(random_state=0,max_iter=500)<br>\n                   }","26d02a8a":"# Create Test Train split based on year 2014","c6daf2b3":"Quick check<br>\ndf['at_least_1_green_donation'] <br>\ndf.info()<br>\ndf.describe()\n","f70409b8":"missing_data.head(50)","40c4cb25":"# Normalize the data to get  better predictions","94935bd7":"(df['is_exciting'][df['grade_level'].isnull()]).value_counts()"}}