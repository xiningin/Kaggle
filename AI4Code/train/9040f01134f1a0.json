{"cell_type":{"6d4acc98":"code","be086b4b":"code","7c925e25":"code","78f9e74b":"code","29383eac":"code","4ce57dfb":"code","870ca401":"code","2108349d":"code","bfbed34c":"code","9b14de53":"code","b7614945":"code","7f0f5d48":"code","27453fb9":"code","a3b2c056":"code","3c4ed843":"code","d4e2c339":"code","8fa29da6":"code","d0d78013":"code","11432efa":"code","f8109ab2":"code","f812ba8e":"code","403ead7a":"code","6fcd31e9":"code","8e4e3109":"code","2756a8bc":"markdown","44b366da":"markdown","30d1d596":"markdown","13260c3c":"markdown","718dbb92":"markdown","6f9c7b9c":"markdown","9fa5fd84":"markdown","cf103b3a":"markdown","6d7c2f03":"markdown","c5cfaf1f":"markdown","2244e9ec":"markdown","bf75c93d":"markdown","ca972cd3":"markdown","166bc356":"markdown","348d8d8c":"markdown","2f7bd693":"markdown","9ef120ad":"markdown","b19856e6":"markdown","308e2cb7":"markdown","db6ed0a2":"markdown"},"source":{"6d4acc98":"# general purpose libraries\nimport numpy as np\nimport datetime as dt\nimport pandas as pd\nimport os\nimport warnings\nimport pickle\nfrom timeit import default_timer as timer\nfrom collections import OrderedDict\nfrom itertools import chain\n\npd.set_option(\"display.max_columns\", None)\n\n\nimport warnings\nwarnings.filterwarnings('ignore', '.*PySoundFile failed. Trying audioread instead*.', )\n","be086b4b":"# plots and visualisation\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as ply_go\nimport plotly.figure_factory as ply_ff\nimport plotly.colors as ply_colors #.sequential.Oranges as orange_palette\n#print(plotly.colors.named_colorscales() )\n#plotly.colors.sequential.swatches()\n#ply_colors.sequential.Oranges","7c925e25":"# DSP libraries\nfrom scipy import signal\nimport librosa\nimport librosa.display as librosa_display","78f9e74b":"# ML and data modelling libraries\nfrom sklearn.preprocessing   import MinMaxScaler, OneHotEncoder,LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, roc_auc_score,roc_curve, precision_recall_curve,confusion_matrix,precision_score, recall_score,average_precision_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\n\nimport xgboost as xgb","29383eac":"### Setup paths and directories\nwork_dir = '\/kaggle\/' \ndata_dir = work_dir + \"input\/covid19-cough-audio-classification\/\"\n#audio_indir = data_dir+\"Respiratory_Sound_Database\\\\audio_and_txt_files\\\\\"\naudio_outdir = work_dir + \"working\/\"\n#text_outdir = data_dir + \"annotations\\\\\"\n\nmetadata_file = \"metadata_compiled.csv\"","4ce57dfb":"metadata=pd.read_csv(data_dir+metadata_file,sep=\",\")\n#print(metadata.columns)\n\n# convert strings 'True'\/'False' to genuine booleans\ncols_to_boolean = (['respiratory_condition', 'fever_muscle_pain',\n                     'dyspnea_1', 'wheezing_1', 'stridor_1','choking_1', 'congestion_1', 'nothing_1',\n                     'dyspnea_2', 'wheezing_2', 'stridor_2','choking_2', 'congestion_2', 'nothing_2',\n                     'dyspnea_3', 'wheezing_3', 'stridor_3','choking_3', 'congestion_3', 'nothing_3',\n                     'dyspnea_4', 'wheezing_4', 'stridor_4','choking_4', 'congestion_4', 'nothing_4'])\nfor c in cols_to_boolean:\n    metadata.loc[metadata[c].notnull(),c] = metadata.loc[metadata[c].notnull(),c].astype(bool) \n\n# remove entries where either status or age is NA\nprint(\"Metadata df entries before cleaning NAs: {}\".format(metadata.shape[0]))\nmetadata = metadata.loc[~((metadata['status'].isnull() ) | (metadata['age'].isnull()) ),]\nprint(\"Metadata df entries after cleaning NAs: {}\".format(metadata.shape[0]))\n\n","870ca401":"metadata['audio_class'] = 'X' # default, we should have none by the end of this classification process\nmetadata.loc[ (metadata['cough_detected'] >= 0.80) & (metadata['age']>=60) ,'audio_class'] = 'A'\nmetadata.loc[ (metadata['cough_detected'] >= 0.80) & (metadata['age']>=40) & (metadata['age']<60) ,'audio_class'] = 'B'\nmetadata.loc[ (metadata['cough_detected'] >= 0.80) & (metadata['age']< 40) ,'audio_class'] = 'C'\nmetadata.loc[ (metadata['cough_detected'] < 0.80) & (metadata['age']>=60) ,'audio_class'] = 'D'\nmetadata.loc[ (metadata['cough_detected'] < 0.80) & (metadata['age']>=40) & (metadata['age']<60) ,'audio_class'] = 'E'\nmetadata.loc[ (metadata['cough_detected'] < 0.80) & (metadata['age']< 40) ,'audio_class'] = 'F'\n\nprint(\"Entries subdivided in classes. Printing the number of entries for each class:\")\nprint(metadata[['audio_class','uuid']].groupby(['audio_class']).count().rename(columns={'uuid':'N_entries'}) )\n\nprint(\"\\n\\n\\nSplitting count by class and status:\")\nprint(metadata[['audio_class','status','uuid']].groupby(['audio_class','status']).count().rename(columns={'uuid':'N_entries'}) )\n","2108349d":"\ndef import_raw_audio(filename,indir, sr=None ):\n    t, sr = librosa.load(indir+filename, sr=sr, mono=True)\n    duration = t.shape[0]\/sr #in seconds\n    mu_t = t.mean()\n    min_t = t.min()\n    max_t = t.max()\n    #tnorm = (t - mu_t )\n    #tnorm = tnorm \/ (max_t-mu_t)\n    f_token = np.array([filename[:-4]]).reshape(1, -1)\n    tokens = np.array([sr, duration, mu_t, max_t, min_t]).reshape(1,-1)\n    audio_df = pd.DataFrame(data= np.hstack((f_token, tokens)),\n                         columns=['AUDIO_FILE', 'SAMPLING_RATE','DURATION', 'MEAN_SIG', 'MAX_SIG', 'MIN_SIG' ],\n                         )\n    audio_df['SAMPLING_RATE'] = audio_df['SAMPLING_RATE'].astype(float).astype(int)#weird conversion from string to int \n    for i in ['DURATION', 'MEAN_SIG', 'MAX_SIG', 'MIN_SIG' ]:\n        audio_df[i] = audio_df[i].astype(float)\n    \n    return audio_df, t, sr\n    \ndef zero_padding(t, sr, target_duration):\n    \"\"\"do zero-padding to get audio files all of the same duration; \n       this will allow us to have spectrograms all of the same size\"\"\"\n    target_len = target_duration * sr\n    if t.shape[0] > target_len:\n        t = t[0:target_len]\n    elif t.shape[0] < target_len:\n        n_pads = target_len - t.shape[0] \n        t = np.append(t, np.repeat(0,n_pads)  )\n    else:\n        pass\n    return t\n\n\ndef calc_stft_power_spectrum(stft, sr, n_fft):\n    amplitudes = np.abs(stft)**2\n    frequencies = librosa.fft_frequencies(sr, n_fft)\n    psx = amplitudes.mean(axis=-1)\n    return frequencies, np.sqrt(psx)\n\n\ndef calc_power_spectrum_welch(t, sr, n_fft):\n    f, psx = signal.welch(t, sr, window='hann',nfft=n_fft, noverlap=0,axis=-1, scaling='spectrum')\n    return f, np.sqrt(psx)\n\ndef calc_spectral_features(t, sr, n_fft = 512, win_length = None, win_overlap=0.0, n_mfcc=None, rec_width=0):\n    \n    ### Calculate spectrograms:\n    ###   -) Short-time Fourier transform (STFT) for the power spectrum\n    ###   -) Mel-frequency cepstral coefficients (MFCC)\n    ###\n    ### win_overlap: float, [0.0, 1.0] ; if 0.0, windows will be NOT overlapping; 0.9999 means almost completely overlapping windows\n    ### rec_width: float, unused\n    ### \n    ### Output:\n    ###    stft: numpy.ndarray of dimension [n_fft\/2, duration*my_sampling_rate\/n_fft]; \n    ###          the n_fft\/2 rows represent the frequencies of the Fast Fourier Transform in time domain;\n    ###          the columns are the time windows in which the raw signal has been subdivided for the FFT.\n    ###          The output values are complex numbers representing the amplitude of the sine and cosine\n    ###          at that specific frequency for that specific signal window\n    ###\n    ###   mfcc: numpy.array of dimensions [n_mfcc, duration*my_sampling_rate\/n_fft];\n    ###         the n_mfcc rows indicate the different mel frequency bands;\n    ###         the columns are the time windows in which the raw signal has been subdivided for the FFT\n    ###         that is then mapped to the mel-frequncy bins.\n    ###\n    \n    if win_length is None:\n        win_length = n_fft\n    \n    if n_mfcc is None:\n        n_mfcc = n_fft\n    \n    assert (win_overlap>=0)&(win_overlap<1.0), \"Invalid value of win_overlap {} - it must be in range [0.0, 1.0) \".format(win_overlap)\n    hop_length = int(win_length*(1.0-win_overlap))\n    \n    #   stft_db = librosa.amplitude_to_db(  np.abs(librosa.stft(t, n_fft=n_fft, \n    #                                                              hop_length=hop_length, win_length=win_length )))\n    stft = librosa.stft(t, n_fft=n_fft, hop_length=hop_length, win_length=win_length ) \n    mfcc = librosa.feature.mfcc(t, sr=sr, n_mfcc=n_mfcc, dct_type=2)\n    #iirt_db = librosa.amplitude_to_db(  np.abs(librosa.iirt(t, hop_length=hop_length, win_length=win_length )) )\n\n    #R_stft = librosa.segment.recurrence_matrix(stft_db, mode='affinity', self=False, width=rec_width)\n    #R_iirt = librosa.segment.recurrence_matrix(iirt_db, mode='affinity', self=False, width=rec_width)\n\n    return stft,mfcc\n\n\ndef stack_rows_with_pad(list_of_arrays):\n    f1 = lambda x: x.shape[1]\n    max_dim = max(list(map(f1,list_of_arrays)) )\n    #print(\"Original shapes:\")\n    #print([m.shape for m in list_of_arrays])\n    #print(\"Padding shapes:\")\n    #print([(m.shape[0], max_dim-m.shape[1] ) for m in list_of_arrays])\n    #print(\"nan pads:\")\n    #print([np.full([ m.shape[0],max_dim-m.shape[1] ],np.nan) for m in list_of_arrays])\n    padded_arrays = [ np.append(m, np.full([ m.shape[0],max_dim-m.shape[1] ],np.nan), axis=1 ) for m in list_of_arrays]\n    return np.concatenate(padded_arrays, axis=0)\n\n\ndef calc_spectral_properties_welch(t, sr, n_fft, time_window_ms, freq_bins):\n    #######\n    ### Computes a whole bunch of spectral properties, after the reference (see section III.A)\n    ### https:\/\/myresearchspace.uws.ac.uk\/ws\/files\/10993506\/2018_12_15_Monge_Alvarez_et_al_Cough.pdf\n    ###\n    ### It splits the audio signal in smaller chunks. For each chunk computes the Power Spectrum Density\n    ### using the Welch method. It then averages the power for user-defined frequency bands.\n    ### At that point, we have many subsegments of the audio, k, and many average PSD, j\n    ### The spectral properties are calculated averaging and summing over k.\n    ### Output is a dictionary with various spectral properties, each one replicated j times \n    ### (as many as the frequency bands).\n    ###\n    \n    #sanity checks\n    assert len(freq_bins)>1,\"Error, input freq_bins must be a list with the boundaries of the frequency bins\"\n    \n    \n    #define how many ms is long each sample of the audio signal and how many values go in each subsegment\n    n_samples_tot = len(t)\n    if( time_window_ms is None ):\n        time_window_ms = 1000*n_samples_tot\/sr\n    chunk_length = min(n_samples_tot, round(time_window_ms*sr\/1000) )# how many audio samples fit in time_window_ms\n    n_chunks = int(np.ceil(n_samples_tot \/ chunk_length))\n    n_freq_bins = len(freq_bins)-1\n    out_all_freq = np.empty((n_freq_bins,0),float)\n    out_all_psx = np.empty((n_freq_bins,0),float)\n    #print(\"*\"*30+\"\\nLooping over {} chunks (tot t samples:{}, chunk l = {})\".format(n_chunks,n_samples_tot,chunk_length) )\n    for k in range(0,n_chunks,1):\n        tmin = k*chunk_length\n        tmax = min((k+1)*chunk_length, n_samples_tot)\n        tmp_segment = t[tmin:tmax]\n        freqs_welch, psx_welch = calc_power_spectrum_welch(tmp_segment,sr, n_fft)\n        #print(\"k={} n_freq_bins={} --> {} {}\".format(k,n_freq_bins,freqs_welch.shape, psx_welch.shape))\n        chunk_freq = np.empty((1,0),float)\n        chunk_psx = np.empty((1,0),float)\n    \n        for j in range(0, n_freq_bins,1):\n            freqmin = freq_bins[j]\n            freqmax = freq_bins[j+1]\n            freq_mask = (freqs_welch>=freqmin)&(freqs_welch<freqmax)\n            selfreqs = freqs_welch[freq_mask]\n            selpsx = psx_welch[freq_mask]\n            #print(\"{} {} |||  {} {} {}\".format(k,j,chunk_freq.shape,selfreqs.shape, selfreqs.reshape(1,-1).shape,selpsx.shape))\n            if(j==0):\n                chunk_freq = selfreqs.reshape(1,-1)\n                chunk_psx  = selpsx.reshape(1,-1)\n            else:\n                chunk_freq = stack_rows_with_pad([chunk_freq,selfreqs.reshape(1,-1)])  \n                chunk_psx = stack_rows_with_pad([chunk_psx,selpsx.reshape(1,-1)])  \n            #print( \"max chunk {} {} : {}\".format(k,j,np.nanmax(chunk_psx, axis=1) ) )\n        ### end for loop on j\n        \n        # append horizontally (row-wise) different frames\n        out_all_freq = np.append(out_all_freq,chunk_freq, axis=1)\n        out_all_psx = np.append(out_all_psx,chunk_psx, axis=1)\n    ####end for loop on k   \n    \n    #print(\"SHAPES:\")\n    #print(out_all_freq.shape)\n    #print(out_all_psx.shape)\n    \n    #Zero crossing rate\n    zcr = librosa.feature.zero_crossing_rate(t, frame_length=chunk_length, hop_length=chunk_length+1)\n    \n    # spectral centroid\n    psx_sum = np.nansum(out_all_psx, axis=1)\n    spec_centroid = (np.nansum(out_all_freq*out_all_psx, axis=1)\/ psx_sum).reshape(-1,1)\n        \n    #spectral bandwidth\n    spec_bw = np.nansum( ((out_all_freq-spec_centroid)**2)*out_all_psx,axis=1)\/psx_sum\n\n    #spectral crest factor\n    #C = 1.0 \/ (np.nanmax(out_all_freq) - np.nanmin(out_all_freq) +1)\n    #spec_crest = (np.nanmax(out_all_psx)\/(C*psx_sum) ).reshape(-1,1)\n    psx_25 = np.nanquantile(out_all_psx,.25, axis=1)\n    psx_50 =np.nanquantile(out_all_psx,.50, axis=1)\n    psx_75 = np.nanquantile(out_all_psx,.75, axis=1)  \n    psx_max = np.nanmax(out_all_psx, axis=1)\n    #print(\"MAX: {} ; P25: {} ; P50: {} ; P75: {}\".format(psx_max, psx_25, psx_50, psx_75))\n    spec_crest = (psx_max-psx_50) \/ (psx_75 - psx_25)\n    \n    # spectral standard deviation\n    spec_sd = np.nanstd( out_all_psx,axis=1)\n    \n    #spectral skewness\n    n_entries = np.array([ len(row[~np.isnan(row)]) for row in out_all_psx])#.reshape(-1,)\n    skew_factors = [ e*np.sqrt(e-1)\/(e-2) for e in n_entries]\n    spec_mean = np.nanmean(out_all_psx,axis=1).reshape(-1,1)\n    spec_skew = skew_factors*np.nansum((out_all_psx-spec_mean)**3, axis=1)  \/ spec_sd**3\n    \n    return zcr, spec_centroid.reshape(1,-1), spec_bw.reshape(1,-1), spec_crest.reshape(1,-1), spec_mean.reshape(1,-1),spec_sd.reshape(1,-1),spec_skew.reshape(1,-1)\n    \n ","bfbed34c":"my_sampling_rate = int(4096*2)  # Sampling rate, how frequently we want to take a value of the audio curve.\n                                 # the max frequency in the STFT will be (approximately) half of this\n                               \nmy_n_fft = 512 # number of frequency bins to be calculated in the STFT; \n               # if my_window_size is None, this drives also the time-sampling window\n    \nmy_n_mfcc = 26 # number of mel-frequencies used for the MFCC calculation. The original article used\n               # a number of 13 MFCC frequencies, I am trying to add some extra info   \n    \nmy_window_size = None # should not be greater than n_fft\ntarget_duration = 10 # seconds; shorter audios will be zero padded; longer audios will be cut;\n                     # obtained from an earlier dry run over all data and charting the distribution \n                     # of duration of the raw sound samples; 10 sec corresponds to the 97th percentile and \n                     # represent a significant improvement in terms of computing time (x5 faster) \n                     # respect to more conservative choices like 70 seconds (99th percentile)\n\n\niclass = \"A\"   # the class of audio records to be processesed (see previous cells)\n\n### these are the frequency bins used to compute short-term features as per the original article (used as inputs to the cough detection classifier).\n### Note that the code here will compute features for all bins but in the original paper they use only non-contiguous values.\n### For example, we keep the bin [0, 200] Hz but not the [200, 300] Hz; the lowest bin used inthe rest of htis analysis will be [0, 200] Hz,\n### the highest one will be [3800, 3900] Hz\nmy_psd_freqs = [0.0, 200.0, 300.0, 425, 500.0, 650.0, 950.0, 1150.0, 1400.0, 1800.0, 2300.0, 2400.0, 2850.0, 2950.0, 3800.0, 3900, 4000]\npsd_feature_names =['SPEC_CENTROID', 'SPEC_WIDTH', 'SPEC_CREST', 'SPEC_MEAN', 'SPEC_SD', 'SPEC_SKEW']\n\n\nmfcc_feature_names = [ \"MFCC_MEAN_{:02}\".format(i) for i in range(0,my_n_mfcc,1)]\nmfcc_feature_names = mfcc_feature_names + [ \"MFCC_SD_{:02}\".format(i) for i in range(0,my_n_mfcc,1)]\n","9b14de53":"def prepare_data(input_data, audio_datadir, sr, target_duration, \n                 n_fft, n_mfcc, fft_window_size, psd_freq_bins,\n                 mfcc_feature_names, psd_feature_names, \n                 max_audio_samples=None, print_every_n=10):\n    ######################################################\n    ###\n    ### Prepares a dataframe with a collection of properties and sound features \n    ### that can be readily used later in a ML classification process\n    ### \n    ### input_data: pandas data.frame; an extract of the metadata file present in the original dataset\n    ###\n    ### audio_datadir: string; the path to the diretory where the audio files are stored\n    ###\n    ### sr: int; sampling rate\n    ###\n    ### target_duration: int; final length of audio sample, in seconds. All audio files will be formatted \n    ###                  to this duration; longer audios will be cut; shorter audios will be padded with zeros\n    ###\n    ### n_fft: int; number of frequency bins to be considered in the Fast Fourier Transform\n    ###\n    ### n_mfcc: int; number of Mel-freuqencies to be used when computing the MFCC\n    ###\n    ### max_audio_samples: int; maximum number of audio files to be processed. If None, all available UUIDs  \n    ###                    will be processed; otherwise, only the first max_audio_sample UUID will be considered\n    ###\n    ###\n    ### Output: The output of this loop is a big pandas dataframe with as many rows as audio files \n    ###         and as many columns as a series of audio features. \n    ###         The column list includes also the audio UUID and the sample label (the \"status\" column in the metadata file).\n    ###\n    ######################################################\n    \n    # get the full list of uuid to be processed\n    all_uuids = input_data['uuid'].values\n    if max_audio_samples is not None:\n        all_uuids[0:max_audio_samples]\n\n\n    # empty pandas df where to store all features for all UUIDs\n    all_data = pd.DataFrame()\n\n    # init  timer and df containig some metadata of the audio files\n    skipped_uuids = []\n    audio_metadata = pd.DataFrame()\n    t_start = timer()\n\n\n    for idx, uuid in enumerate(all_uuids):\n              \n        tmp_audiofilename = uuid+\".webm\"\n        if not os.path.exists(audio_datadir+tmp_audiofilename):\n            # try to look for a .ogg file\n            tmp_audiofilename = uuid+\".ogg\"\n            if not os.path.exists(audio_datadir+tmp_audiofilename):\n                warn(\"WARNING! Could not find audio file for UUID: {}  . Skipping.\".format(uuid))\n                continue\n\n        if idx % print_every_n ==0:\n            print(\"Processing file #{}: {}\".format(idx,tmp_audiofilename))\n\n        try:\n            tmp_df, tmp_audio, sr = import_raw_audio(tmp_audiofilename, indir=audio_datadir, sr=sr)\n        except FileNotFoundError as e_fnf:\n            print(\"Could not find audio file {}.\\n\\n\\n\".format(tmp_audiofilename))\n            skipped_uuids = skipped_uuids + [uuid]\n            continue #move to next file\n        except Exception as e:\n            print(\"Some other exception occurred\")\n            raise e #rethrow exception\n\n        tmp_audio = zero_padding(tmp_audio, sr=sr, target_duration=target_duration) \n        tmp_df['UUID'] = uuid\n        audio_metadata = audio_metadata.append(tmp_df)\n\n\n        stft , mfcc = calc_spectral_features(tmp_audio, sr, n_fft=n_fft,n_mfcc=n_mfcc, win_length=fft_window_size, win_overlap=0.0)\n\n        ### extract mean and std dev for each mel-frequency in the mfcc\n        mfcc_mean = np.mean(mfcc, axis=1)\n        mfcc_sd = np.std(mfcc,axis=1)\n        mfcc_features = np.append(mfcc_mean,mfcc_sd,axis=0)\n        mfcc_feat_dict = {name:val for name,val in zip(mfcc_feature_names,mfcc_features)}\n\n        ### Power Spectrum Density based short-term features\n        zcr,sc,sb,scf,ssmean, ssd, ssk = calc_spectral_properties_welch(tmp_audio,sr, my_n_fft,None, psd_freq_bins)\n        # consider only every second bin to reduce features; following original article freq bins\n        psd_features = np.array([ (x0, x1, x2, x3,x4,x5) for i, (x0, x1, x2, x3,x4,x5) in enumerate(zip(*sc, *sb,*scf,*ssmean, *ssd, *ssk)) if i%2==0]).transpose() \n\n        #now extract each element of the PSD feature (correspondignto a unique combination of spectral feature and freq bin)\n        n_freq_bins = psd_features.shape[1]\n        psd_features = psd_features.ravel()\n        psd_feature_names_expanded = [ [ \"{f}_{b:02}\".format(f=f,b=b) for b in range(0,n_freq_bins,1) ] for f in psd_feature_names]\n        psd_feature_names_expanded = list(chain.from_iterable(psd_feature_names_expanded))\n        assert len(zcr)==1, \"Zero-Crossing Rate vector has length different from 1: {}\".format(len(zcr))\n        assert len(psd_feature_names_expanded)==len(psd_features), \"Mismatch between number of spectral features ({nf}) and vector with their names ({nn})\".format(nf=len(psd_features) , nn=len(psd_feature_names_expanded)) \n        psd_feat_dict = { name:val for name,val in zip(psd_feature_names_expanded,psd_features)}\n        psd_feat_dict['ZCR'] = zcr[0,0]\n\n        # store all features in a pandas dataframe\n        tmp_df = input_data.loc[ tmp_metadata['uuid']==uuid, ['uuid','audio_class','cough_detected','SNR','age','gender','respiratory_condition','fever_muscle_pain','status'] ]\n        tmp_df.columns = [c.upper() for c in tmp_df.columns]\n        tmp_dict = tmp_df.to_dict(orient='records')\n        #assert len(tmp_dict)==1, \"ERROR! Multiple records for UUID {} : {}\".format(uuid,len(tmp_dict))\n        tmp_dict = OrderedDict(tmp_dict[0] ) \n        tmp_dict.update(mfcc_feat_dict)\n        tmp_dict.update( psd_feat_dict)\n        #tmp_df = pd.DataFrame(tmp_dict, columns=tmp_dict.keys())\n        all_data = all_data.append(pd.DataFrame(tmp_dict,index=[idx]))#, ignore_index=True)\n\n    ### end for loop over raw audio files\n    print(\"\\n{} files processed in {:.1f} seconds\\n\".format(idx+1, timer()-t_start ))\n    return all_data,audio_metadata\n####\n#### end prepare_data\n#### ","b7614945":"# the following looks more complicated than needed because I want first to keep all entries of the smallest group\n# then upsample the difference between that and the target number n_resampling. No pandas function is allowing me to do this.\ndef sample_df_balanced(df, group_col, n, random=42):\n    assert isinstance(group_col,str), \"Input group_col must be a plain string with the column name: {}\".format(type(group_col))\n    #df_count = df[[group_col]].groupby([group_col]).cumcount()+1\n    df['N'] = np.zeros(len(df[group_col]))\n    df_count = df[[group_col,'N']].groupby([group_col]).count().reset_index() #cumcount()+1\n    \n    out_df = pd.DataFrame()\n    for igroup in df[group_col].unique():\n\n        n_orig = df_count.loc[df_count[group_col]==igroup,'N'].values[0]\n        if n_orig < n: # need to upsample\n            delta = max(n - n_orig, 0)\n            tmp_df = df.loc[df[group_col]==igroup, ]\n            delta_df = tmp_df.sample(n=delta,random_state=random,replace=False)\n            out_df = pd.concat([out_df,tmp_df,delta_df])\n        else: #downsample\n            tmp_df = df.loc[df[group_col]==igroup, ].sample(n=n,random_state=random,replace=False)\n            out_df = pd.concat([out_df,tmp_df])\n    ### end for loop over groups\n    return out_df.drop('N',axis=1,inplace=False)\n### end sample_df_balanced\n\n   ","7f0f5d48":"# filter UUID, keeping only those in the desired class\ntmp_metadata = metadata.loc[metadata['audio_class']==iclass,]\n\n# remove entries where the SNR is low (hence the cough audio sound is of poor quality)\n# This cut should be optimised, at this stage I just decide to cut off the worst 10%\ntmp_metadata = tmp_metadata.loc[tmp_metadata['SNR']>=tmp_metadata['SNR'].quantile(0.10),]\n","27453fb9":"print(\"Before resampling, count of entries by STATUS class in the full data.frame:\")\ntmp_metadata_count = tmp_metadata[['uuid','status']].groupby(['status']).count()\nprint(tmp_metadata_count)\nprint(len(tmp_metadata.loc[tmp_metadata['status']=='COVID-19', 'uuid'].unique()))\n\n# every group will have a number of entries equal to the number of records \n# in the smallest group, rounded to the closest ten above\nn_resampling = int(np.ceil(tmp_metadata_count['uuid'].min()\/10)*10)\n\n  \n    \n#tmp_metadata = tmp_metadata.groupby(['status']).sample(n=n_resampling,random_state=42,replace=False)\ntmp_metadata = sample_df_balanced(tmp_metadata, 'status', n_resampling) #tmp_metadata.groupby(['status']).sample(n=n_resampling,random_state=42,replace=False)\nprint(\"\\nAfter resampling, count of entries by STATUS class in the full data.frame:\")\nprint(tmp_metadata[['uuid','status']].groupby(['status']).count())\nprint(len(tmp_metadata.loc[tmp_metadata['status']=='COVID-19', 'uuid'].unique()))\n","a3b2c056":"\nall_data, all_audio_metadata = prepare_data(input_data=tmp_metadata, audio_datadir=data_dir, sr=my_sampling_rate, \n                                            target_duration=target_duration, n_fft=my_n_fft, n_mfcc=my_n_mfcc, \n                                            fft_window_size=my_window_size, psd_freq_bins=my_psd_freqs,\n                                            mfcc_feature_names=mfcc_feature_names, psd_feature_names=psd_feature_names,\n                                            max_audio_samples=None, print_every_n=20) \n\nall_audio_metadata = all_audio_metadata.drop_duplicates(subset='UUID', keep='first') # this avoids spurious duplicates at the following merge\n\nprint(\"Merging dataframe with audio features and df with audio metadata\")\nall_data = pd.merge(all_data,all_audio_metadata,on=['UUID'],how='inner')\nall_data.drop(['AUDIO_FILE'],axis=1,inplace=True)\n\n#print(all_data[['UUID','STATUS']].groupby(['STATUS']).count())\nprint(\"Shape of full dataframe with features and labels: {}\".format(all_data.shape))\nall_data.head(10)\n\n","3c4ed843":"out_data_filename = audio_outdir+\"\/cough-classification-data_Class{}.pkl\".format(iclass )\npd.to_pickle(all_data, out_data_filename)  # save df to file\n#all_data = pd.read_pickle(out_data_filename) # load df from file\n\n### one more check that all covid status are balanced\nall_data[['UUID','STATUS']].groupby(['STATUS']).count().rename(columns={'UUID':'N_UUID'})\n","d4e2c339":"all_uuids = all_data['UUID'].unique()\nuuid_tmp = all_uuids[10] #\"0379c586-c500-483c-83a6-95b63afe6931\"#all_uuids[10]\ntmp_audiofilename = uuid_tmp+\".webm\"\n        \ntmp_df, tmp_audio, sr = import_raw_audio(tmp_audiofilename, indir=data_dir, sr=my_sampling_rate)\ntmp_audiofilename = uuid_tmp+\".webm\"\ntmp_audio = zero_padding(tmp_audio, sr=sr, target_duration=target_duration)                                                         \nstft , mfcc = calc_spectral_features(tmp_audio, sr, n_fft=my_n_fft,n_mfcc=my_n_mfcc,win_overlap=0.0)\nfreqs_welch, psx_welch = calc_power_spectrum_welch(tmp_audio,sr, my_n_fft)\n        \nprint(mfcc.shape)\ntime_stamps = np.arange(0,target_duration, 1\/my_sampling_rate)\n\n# plot raw signal\nline_data = ply_go.Scatter(x=time_stamps, \n                           y=tmp_audio,\n                           name=\"Audio signal\", showlegend=False)\nfig = ply_go.Figure(data=[line_data])#, layout=my_layout)\nfig.update_layout(title={'text': \"Raw audio (UUID:{})\".format(uuid_tmp)}, \n                  xaxis={\"title\":{\"text\":\"Time [s]\"}}, yaxis={\"title\":{\"text\":\"Amplitude\"}})\nfig.show()\n\n# plot STFT\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18,18)) # tight_layout=False,constrained_layout=True\nfig.tight_layout()\nimg0 = librosa.display.specshow(np.abs(stft), sr=sr, y_axis='log', x_axis='time', ax=ax)\n\n#img0 = librosa_display.specshow(x, y_axis='log', x_axis='time',\n#                               sr=my_sampling_rate, ax=ax)\nax.set_title('Log-Frequency power spectrogram', size=18)\nfig.colorbar(img0, format=\"%+2.f\")\nfig.show()\n\n\n# plot MFCC\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18,18)) # tight_layout=False,constrained_layout=True\nfig.tight_layout()\nimg0 = librosa.display.specshow(np.abs(mfcc), sr=sr, y_axis='log', x_axis='time', ax=ax)\n\n#img0 = librosa_display.specshow(x, y_axis='log', x_axis='time',\n#                               sr=my_sampling_rate, ax=ax)\nax.set_title('MFCC spectrogram', size=18)\nfig.colorbar(img0, format=\"%+2.f\")\nfig.show()\n\n# plot power spectrum\nline_data = ply_go.Scatter(x=freqs_welch,#np.arange(0,target_duration,my_n_fft\/(my_sampling_rate)), \n                           y=psx_welch,\n                           name=\"Power Spectrum density\", showlegend=False)\nfig = ply_go.Figure(data=[line_data])#, layout=my_layout)\nfig.update_layout(title={'text': \"Power Spectrum Density (UUID:{})\".format(uuid_tmp)}, \n                  xaxis={\"title\":{\"text\":\"Frequency [Hz]\"}}, yaxis={\"title\":{\"text\":\"Average Power\"}})\nfig.show()\n\n\n","8fa29da6":"sampled_data = all_data.copy()\nall_uuids = sampled_data['UUID'].values\n\n#select X features to be used in ML classification\n#train_features = (['RESPIRATORY_CONDITION', 'FEVER_MUSCLE_PAIN','MEAN_SIG','MAX_SIG','MIN_SIG','ZCR']+\n#                  [ f for f in sampled_data.columns.values if f.startswith('MFCC_')] + \n#                  [ f for f in sampled_data.columns.values if f.startswith('SPEC_')])\n\nmax_freq_features = 99\ntrain_features = (['RESPIRATORY_CONDITION', 'FEVER_MUSCLE_PAIN','MEAN_SIG','MAX_SIG','MIN_SIG','ZCR']+\n                  [f2 for f2 in [ f1 for f1 in sampled_data.columns.values  if f1.startswith('MFCC_')] if int(f2[-2:])<max_freq_features]+\n                  [f2 for f2 in [ f1 for f1 in sampled_data.columns.values  if f1.startswith('SPEC_')] if int(f2[-2:])<max_freq_features])\n\ny_label = 'STATUS'\n\n### this is used if one wants to reduce the number of classes\/status; \n#sampled_data.loc[sampled_data['STATUS']=='symptomatic','STATUS'] = 'NOCOVID' \n#sampled_data.loc[sampled_data['STATUS']=='healthy','STATUS']     = 'NOCOVID' \n\n\n#print(train_features)\nprint(\"Number of training X features: {}\".format(len(train_features)) )\nX_train, X_test, y_train, y_test, uuid_train, uuid_test = train_test_split(sampled_data[train_features].values, sampled_data[[y_label]].values, all_uuids,\n                                                                            test_size=0.3,random_state=612, stratify=sampled_data[y_label])\nprint(\"Shapes of train X and y datasets: X->{}    y->{}\".format(X_train.shape ,y_train.shape))\nprint(\"Shapes of test  X and y datasets: X->{}    y->{}\".format(X_test.shape, y_test.shape))\n\n\nprint(\"\\n\\nTRAIN DATASET - Count of entries by STATUS:\\nhealthy={} \\tsymptomatic={} \\tcovid={}\\n\\n\".format(y_train[y_train==\"healthy\"].shape[0], y_train[y_train==\"symptomatic\"].shape[0], y_train[y_train==\"COVID-19\"].shape[0]))","d0d78013":"scaler = MinMaxScaler(feature_range=(-1,1))\nX_train_norm = scaler.fit_transform(X_train)\nX_test_norm = scaler.transform(X_test)\n\nlabenc = LabelEncoder()\ny_train_enc = labenc.fit_transform(y_train.ravel())\ny_enc_labels = labenc.classes_\nprint(y_enc_labels)\ny_test_enc = labenc.transform(y_test.ravel())","11432efa":"def roc_df(ytrue, ypred):   \n    falseposrate, trueposrate, thresholds = metrics.roc_curve(ytrue, ypred)\n    roc_df = pd.DataFrame()\n    roc_df['FalsePosRate'] = falseposrate\n    roc_df['TruePosRate'] = trueposrate\n    roc_df['Thresholds'] = thresholds\n    return roc_df\n\ndef prc_df(ytrue, ypred):   \n    precision, recall, thresholds = metrics.precision_recall_curve(ytrue, ypred)\n    prc_df = pd.DataFrame()\n    prc_df['Precision'] = precision[:-1]\n    prc_df['Recall'] = recall[:-1]\n    prc_df['Thresholds'] = thresholds\n    return prc_df\n\ndef plot_prc(recall, precision, ax,\n             prc_score=None, xrange=[-0.05,1.05],yrange=[-0.05,1.05]):\n    \n    if prc_score is not None:\n        prc_label='PRC Avg Score = {:.4}'.format(prc_score)\n    else:\n        prc_label=None\n    ax.plot(recall,precision,  'b', label=prc_label)\n    ax.plot([0,1],[1,0],'r--')\n    ax.set_title('Precision-Recall curve', fontsize=28)\n    ax.set_xlabel('Recall', fontsize=24)\n    ax.set_ylabel('Precision', fontsize=24)\n    ax.tick_params(axis='both', which='major', labelsize=18)\n    ax.set_xlim(xrange)\n    ax.set_ylim(yrange)\n    ax.legend(loc='lower left',fontsize=24)\n    ax.grid()\n    return ax      \n\ndef score_eval(ytrue, ypreds, model_name=\"\", ylabels=None):\n    tmp_acc = accuracy_score(ytrue, ypreds)\n    tmp_precision = precision_score(ytrue, ypreds, average='macro')\n    tmp_recall = recall_score(ytrue, ypreds, average='macro')\n    tmp_cm = confusion_matrix(ytrue, ypreds)\n    print(\"{mn} accuracy \/ precision \/ recall: {a:.3f} \/ {p:.3f} \/ {r:.3f}\".format(a=tmp_acc, p=tmp_precision, r=tmp_recall, mn=model_name) )\n    print(\"\\n\\n\")\n    print(classification_report(ytrue, ypreds, target_names=ylabels) )\n    return tmp_acc,tmp_precision, tmp_recall, tmp_cm","f8109ab2":"### SETUP LOGISTIC REGRESSION (MULTICLASS)\nfrom sklearn.linear_model import LogisticRegression\nlogit_params = dict(multi_class='multinomial', penalty='l2', C=0.20, solver='newton-cg', random_state=991)\n\nlogit_class = LogisticRegression(**logit_params)\nlogit_model = logit_class.fit(X_train_norm, y_train_enc)\nlogit_test  = logit_model.predict(X_test_norm )\nprint(y_enc_labels)\nlogit_acc, logit_precision, logit_recall, logit_cm = score_eval(y_test_enc, logit_test, \"Logit_multi\", ylabels=y_enc_labels)\n\nprint(logit_cm)\n\nlogit_coeffs = pd.concat([pd.DataFrame(train_features),pd.DataFrame(np.transpose(logit_model.coef_))], axis = 1,ignore_index=True)\nlogit_coeffs.columns = np.append(['XVAR'], y_enc_labels,axis=0)\nlogit_coeffs.sort_values('COVID-19')\n#logit_coeffs.sort_values('healthy')","f812ba8e":"#%% SETUP XGBOOST\n\n# cast train and test sample to a XGBoost DMatrix data container -- NOT NEEDED if using sklearn API !\n#dtrain = xgb.DMatrix(data=X_train_norm, label=y_train,feature_names=train_features)\n#dtest = xgb.DMatrix(data=X_test_norm, label=y_test,feature_names=train_features)\n\n#\n# define XGBoost classification model\n# xgb_params = {'max_depth': 3,   # max depth of a tree\n#               'n_estimators': 250,\n#               'learning_rate': 0.1,   # learning rate; smaller eta make convergence more accurate but slower\n#               'min_split_loss': 0.05, #gamma parameter in xgboost; the larger gamma, the more conservative the algo is in adding one extra leaf to the tree\n#               'reg_lambda':5.0,   # disable L2 reg only if features are all reasonably independent\n#               'reg_alpha':5.0,    #  L1 reg,tring to prune unnecessary features\n#               'objective': 'multi:softmax',\n#               'num_class': 3,     # number of classes to classify in the dataset\n#               'use_label_encoder':False,\n#               'subsample': 0.5,  #use only a fraction of the training set to grow the tree; if =1.0, subsampling is disabled\n#               #'colsample_bytree':0.50,\n#               'random_state':9443,\n#               'verbosity':0  #0: silent --> 3: very verbose\n#               }\n\n# define XGBoost classification model\nxgb_params = {'max_depth': 3,   # max depth of a tree\n              'n_estimators': 20,\n              'learning_rate': 0.2,   # learning rate; smaller eta make convergence more accurate but slower\n              #'min_split_loss': 0, #gamma parameter in xgboost; the larger gamma, the more conservative the algo is in adding one extra leaf to the tree\n              'reg_lambda':10.0,   # disable L2 reg only if features are all reasonably independent\n              'reg_alpha':0.0,    #  L1 reg,tring to prune unnecessary features\n              'objective': 'multi:softmax',\n              'num_class': 3,     # number of classes to classify in the dataset\n              'use_label_encoder':False,\n              'subsample': 1,  #use only a fraction of the training set to grow the tree; if =1.0, subsampling is disabled\n              #'colsample_bytree':0.50,\n              'random_state':9443,\n              'verbosity':0  #0: silent --> 3: very verbose\n              }\n\n#evallist = [(dtest, 'eval'), (dtrain, 'train')]\nxgb_class = xgb.XGBClassifier(**xgb_params)\n\n# fit the model\nxgb_model = xgb_class.fit(X_train_norm, y_train_enc, \n                          eval_metric=['mlogloss'], \n                          eval_set=[(X_train_norm, y_train_enc), (X_test_norm, y_test_enc)],\n                          verbose=False)\n\n\n#%% run evaluation\nxgb_train = xgb_model.predict(X_train_norm )\nxgb_test = xgb_model.predict(X_test_norm )\n#xgb_test= labenc.inverse_transform(xgb_test)\n","403ead7a":"### plot global feature importance as calculated by xgboost\nxgb_importance = xgb_class.feature_importances_\nsorted_indices = np.flip(xgb_importance.argsort())\nxgb_importance = xgb_importance[sorted_indices]\nimportance_labels = np.array(train_features)[sorted_indices]\nplt.barh(importance_labels[0:10], xgb_importance[0:10])\n\n\n### accuracy and valuation metrics calculated on the training sample\nxgb_acc, xgb_precision, xgb_recall , xgb_cm = score_eval(y_train_enc, xgb_train, \"XGBoost TRAIN\", ylabels=y_enc_labels)\n\nprint(\"\\n\\nConfusion matrix - in-sample training dataset:\")\nprint(xgb_cm)\n\n# Applying k-fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\nn_folds = 5\naccuracies = cross_val_score(estimator = xgb_model, X = X_train, y = y_train_enc, cv = n_folds)\nprint(\"\\n\\nEvaluating XGBoost model using training set and {}-fold cross validation: \\nAverage Accuracy {:.3f} +\/- {:.3f}\".format(n_folds,accuracies.mean(), accuracies.std() ) )\nprint(\"Fold accuracies: {}\\n\\n\".format(accuracies))\n\n\n### extract loss values for both training and test datasets asa function of iteration (i.e., estimator added to the BDT)\nxgb_train_results = xgb_model.evals_result()\nxgb_train_loss = xgb_train_results['validation_0']['mlogloss']\nxgb_test_loss  = xgb_train_results['validation_1']['mlogloss']\niters = len(xgb_train_loss)\nx_iters = list(range(0, iters))\n\nline_data = [ply_go.Scatter(x=x_iters, \n                            y=xgb_train_loss,\n                            name=\"TRAIN\"),\n             ply_go.Scatter(x=x_iters, \n                            y=xgb_test_loss,\n                            name=\"TEST\")]\nfig = ply_go.Figure(data=line_data)#, layout=my_layout)\nfig.update_layout(title={'text': \"Evaluation Log-Loss of XGBoost classifier\"}, \n                  xaxis={\"title\":{\"text\":\"Iteration\"}}, yaxis={\"title\":{\"text\":\"Multiclass Log-Loss\"}})\nfig.show()","6fcd31e9":"\nprint(\"\\n\\n\\nVALIDATION USING TEST SAMPLE:\")\nxgb_acc, xgb_precision, xgb_recall , xgb_cm = score_eval(y_test_enc, xgb_test, \"XGBoost\", ylabels=y_enc_labels)\n\nprint(\"\\n\\nConfusion matrix:\")\nprint(xgb_cm)\n","8e4e3109":"logit_test_dec = labenc.inverse_transform(logit_test)\nxgb_test_dec = labenc.inverse_transform(xgb_test)\nlogit_data = pd.DataFrame({'UUID':uuid_test, 'LOGIT_STATUS':logit_test_dec})\nxgb_data = pd.DataFrame({'UUID':uuid_test, 'XGB_STATUS':xgb_test_dec})\npred_data = pd.merge(sampled_data, logit_data,on='UUID', how='left')\npred_data = pd.merge(pred_data, xgb_data,on='UUID', how='left')\n#pred_data[['UUID','STATUS','LOGIT_STATUS','XGB_STATUS']].head()\npred_data.loc[(pred_data['STATUS']==pred_data['LOGIT_STATUS']) & (pred_data['STATUS']!=pred_data['XGB_STATUS']) &(~pd.isnull(pred_data['XGB_STATUS'])),]","2756a8bc":"### Normalise datasets\n\nIn order to avoid data leakage, we fit the scaler using only the training set. We also encode the target labels, XGBoost likes it better that way.","44b366da":"#### Save dataframe for faster pricessing later ","30d1d596":"### Helper functions","13260c3c":"### Setup a logistic regression (multiclass)\n\nAlthough we expect the logistic regression to be inferior to the Boosted Decision Tree, it is always good to have a benchmark using a simpler method.","718dbb92":"### Visualise sound and spectrograms for a given UUID","6f9c7b9c":"## General strategy\n\n1. Import metadata and select valid audio recordings\n    1. Clean up entries: remove records with\n        * status is NA\n        * age is NA\n        \n        I will keep all ages but I have some doubts about entries with age > 80 (possible that an 80 yo with COVID bothers to connect to Internet and record their cough????)\n    1. Divide entries in classes\n        1. CoughDetection > 0.80 && age >= 60\n        1. CoughDetection > 0.80 &&  40 <= age < 60        \n        1. CoughDetection > 0.80 &&  age < 40\n        1. CoughDetection < 0.80 && age >= 60\n        1. CoughDetection < 0.80 &&  40 <= age < 60        \n        1. CoughDetection < 0.80 &&  age < 40\n        \n        \n2. For each class:\n    1. Apply more filters, like a selection on the SNR (needed?)\n    1. get a list of all the remaining audio filenames from the UUID\n    1. for each UUID\n        - import the corresponding audio file\n        - pass it to the audio feature extraction (librosa)\n        - extract spectral features as per original paper: https:\/\/www.nature.com\/articles\/s41597-021-00937-4\/tables\/3\n        - store features in a dataframe, together with the metadata corresponding to that UUID (for a single UUID --> one row and many columns with all the features\n    1. once the features for every UUID in the same class have been extracted, feed them into a ML classification\n        - split test\/train\n        - do this for two algos: LogRegression with regularisation & XGBoost\n        - add prediction to dataframe\n    1. return a dataframe with all the UUID in that class, the true status and the estimated status\n    \n    \n3. TO-DO: Aggregate the output df from all the classes and compute total accuracy metrics \n    ","9fa5fd84":"### DSR parameters","cf103b3a":"### Define different classes based on combination of age and cough detection score\n\ncough_detected is a score [0, 1] produced by a ML run by the authors. Values closer to 1 indicate a more likely presence of an actual cough sound in the recording. Values close to zero inidcate that the audio recording is likely to be spurious and not contain an actual cough. ","6d7c2f03":"### Balance classes in the dataframe\n\nThe dataset is imbalanced (about 10% of entries are labelled as 'COVID') but at least we have quite a number of records. We rebalance the three STATUS classes in the dataset by reducing the number of the overwhelming 'healthy' class.","c5cfaf1f":"OK, now we do the proper audio signal processing over all files in the same class.","2244e9ec":"### merge predictions back to original dataframe\n\nAfter merging, select and print entries where the XGBoost does a bad job (logit regression is right but XGBoost is wrong)","bf75c93d":"## Train a COVID identification model\n\nFor each audio_class, train a separate model that labels the status based on key metadata, as well as audio properties of the recording.","ca972cd3":"### Diagnostics of XGBoost training\n\nWe are gonna plot the top 10 most important features as welll the cross-validated accuracy and the evolution of the loss function as we add more estimators to the BDT","166bc356":"### Train a BDT classifier using XGBoost","348d8d8c":"### Helper functions used to plot and evaluate model accuracy","2f7bd693":"## Import metadata and select valid audio recordings","9ef120ad":"## Conclusions and comments\n\nThe performances of the classifier proposed here are quite unsatisfactory. The main issue is a massive overfitting problem that limits the generalisation ability of the model.\nI am not sure that a different ML algorithm (e.g., a CNN operating over the full MFCC spectrogram) could do much better.\n\n\nI might rather go back to the code and simplify the class strucutre, perhaps ditching it completely and using a larger and more heterogeneous dataset.","b19856e6":"### DSR loop over all audio files\n\nThe output of this loop is a big pandas dataframe with as many rows as audio files and a series of audio features  in the columns. The column list includes also the audio UUID and the sample label (the \"status\" column in the metadata file)","308e2cb7":"## Dataset info\n\nDataset sourced from Kaggle:  https:\/\/www.kaggle.com\/andrewmvd\/covid19-cough-audio-classification\n\nOriginal data and info:       https:\/\/www.nature.com\/articles\/s41597-021-00937-4\n","db6ed0a2":"### Split train-test\n\nUse argument 'stratify' to preserve small classes, due to imbalance in dataset"}}