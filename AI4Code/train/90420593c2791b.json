{"cell_type":{"6ef8c307":"code","28d69419":"code","bd488be1":"code","aaae33cd":"code","477814fb":"code","21093023":"code","ea032cec":"code","f5bb2c0b":"code","327c6492":"code","586e9c85":"code","7994dc8b":"code","98ce5c34":"code","54db2bd8":"code","ae78efc7":"code","1a122456":"code","507860c4":"code","678b8144":"code","5280e330":"code","6cf0ac80":"code","f9547309":"markdown","04fa5657":"markdown","34acc7b0":"markdown","9b38b45d":"markdown","df21625e":"markdown","3a9ab4ec":"markdown","eabd830b":"markdown","0cd851ee":"markdown","7efaea87":"markdown","789541ea":"markdown","faf0a1a4":"markdown","95574070":"markdown"},"source":{"6ef8c307":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, warnings,json\nwarnings.filterwarnings('ignore')\n\n# Plot\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# TensorFlow & Transformers\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, Adamax\n\n# Transformer Model\nfrom transformers import BertTokenizer, TFBertModel\nfrom transformers import TFAutoModel, AutoTokenizer\n\n#SK Learn\nfrom sklearn.model_selection import train_test_split\n\n# Garbage Collector\nimport gc","28d69419":"train_data = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/sample_submission.csv\")","bd488be1":"def Utilize_TPUs():  \n    \"\"\"\n    Initialize training strategy using TPU if available else using default strategy for CPU and  single GPU\n    \n    After the TPU is initialized, you can also use manual device placement to place the computation on a single TPU device.\n\n    \"\"\"\n    try:\n        \n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        REPLICAS = strategy.num_replicas_in_sync\n        print(\"Connected to TPU Successfully:\\n TPUs Initialised with Replicas:\",REPLICAS)\n        \n        return strategy\n    \n    except ValueError:\n        \n        print(\"Connection to TPU Falied\")\n        print(\"Using default strategy for CPU and single GPU\")\n        strategy = tf.distribute.get_strategy()\n        \n        return strategy\n    \nstrategy=Utilize_TPUs()","aaae33cd":"the_chosen_one=\"jplu\/tf-xlm-roberta-base\"\nmax_len =80\nbatch_size = 16 * strategy.num_replicas_in_sync\n\nAUTO = tf.data.experimental.AUTOTUNE\nepochs = 30\nn_steps = len(train_data) \/\/ batch_size","477814fb":"def model_baseline(strategy,transformer):\n    with strategy.scope():\n        transformer_encoder = TFAutoModel.from_pretrained(transformer)\n        input_layer = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        sequence_output = transformer_encoder(input_layer)[0]\n        cls_token = sequence_output[:, 0, :]\n        output_layer = Dense(3, activation='softmax')(cls_token)\n        model = Model(inputs=input_layer, outputs=output_layer)\n        model.compile(\n            Adamax(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        return model\n    \nmodel=model_baseline(strategy,the_chosen_one)","21093023":"model.summary()","ea032cec":"train_data.head()","f5bb2c0b":"# Pie Chart\ndf = pd.DataFrame({\"count\": train_data.language.value_counts() })\nfig = px.pie(df, values='count', names=df.index, title='Language Count %',\n             labels={'index':'lang'}, color_discrete_sequence=px.colors.diverging.Earth)\nfig.update_traces(textinfo='percent')\nfig.show()","327c6492":"# Bar Plot - Label Count per Language \nfig, ax = plt.subplots(figsize=(20,10))\ntrain_data.groupby(['language','label']).count()['premise'].unstack().plot(ax=ax,kind='bar', cmap='cividis')\nplt.grid(color='gray',linestyle='--',linewidth=0.2)\nax.set_facecolor('#d8dcd6')\nplt.title(\"Label Count per Language\", fontsize='18')\nplt.xticks(rotation=45)","586e9c85":"tokenizer = AutoTokenizer.from_pretrained(the_chosen_one)","7994dc8b":"train_df = train_data[['premise', 'hypothesis']].values.tolist()","98ce5c34":"test_df = test_data[['premise', 'hypothesis']].values.tolist()","54db2bd8":"train_encoded=tokenizer.batch_encode_plus(train_df,pad_to_max_length=True,max_length=max_len)","ae78efc7":"test_encoded=tokenizer.batch_encode_plus(test_df,pad_to_max_length=True,max_length=max_len)","1a122456":"x_train, x_valid, y_train, y_valid = train_test_split(train_encoded['input_ids'], train_data.label.values, test_size=0.1)\n\nx_test = test_encoded['input_ids']","507860c4":"train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\n\nvalid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size).cache().prefetch(AUTO))\n\ntest_dataset = (tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size))","678b8144":"model.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=epochs)","5280e330":"predictions = model.predict(test_dataset, verbose=1)\nsample_sub['prediction'] = predictions.argmax(axis=1)","6cf0ac80":"sample_sub.to_csv(\"submission.csv\",index= False)","f9547309":"# Initialise TPU","04fa5657":"# Loading Data Into tf.Data.Dataset ","34acc7b0":"Submission for NLI (Natural Language Inferencing) problem based on the [Contradictory, My Dear Watson](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/overview) dataset. \n\nThis notebook is inspired from [here](https:\/\/www.kaggle.com\/tkrsh09\/nlp-starter-complete-tpu-bert-guide-keras) but with slight tweaks & changes. ","9b38b45d":"# Defining Parameters","df21625e":"# AutoModels\n* In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the from_pretrained method.\n\n* AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name\/path to the pretrained weights\/config\/vocabulary:\n\n* Instantiating one of AutoModel, AutoConfig and AutoTokenizer will directly create a class of the relevant architecture (ex: model = AutoModel.from_pretrained('bert-base-cased') will create a instance of BertModel).","3a9ab4ec":"# Training Base Model","eabd830b":"# Validation Split ","0cd851ee":"# Libraries & Data \n","7efaea87":"# EDA & Data Preprocessing ","789541ea":"# Encoding Data \n\nNumberically representing  text data such that It can be feed to the model ","faf0a1a4":"# Define Build & Compile Model","95574070":"# Saving"}}