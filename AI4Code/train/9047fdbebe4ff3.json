{"cell_type":{"ecbb3cf4":"code","796c8b2c":"code","e91b6447":"code","379a39ab":"code","2e2813be":"code","b1626db3":"code","4fa83614":"code","1a9d28b6":"markdown","04516f4b":"markdown","8985cc52":"markdown","9e58827f":"markdown","f6f18e25":"markdown"},"source":{"ecbb3cf4":"# import library (pustaka) yang dibutuhkan\nimport cv2 #pustaka pengolah image dan video\nimport numpy as np #pustaka pengolah array\nimport matplotlib.pyplot as plt #pustaka untuk visualisasi\n\n# install library tambahan (download)\n!pip install imutils\nimport imutils\n\nfrom scipy.spatial import distance as dist #pustaka u\/ menghitung jarak\nfrom collections import OrderedDict #pustaka u\/ sorting\nimport time","796c8b2c":"class CentroidTracker():\n    def __init__(self, maxDisappeared=50):\n        # initialize the next unique object ID along with two ordered\n        # dictionaries used to keep track of mapping a given object\n        # ID to its centroid and number of consecutive frames it has\n        # been marked as \"disappeared\", respectively\n        self.nextObjectID = 0\n        self.objects = OrderedDict()\n        self.disappeared = OrderedDict()\n    \n        # store the number of maximum consecutive frames a given\n        # object is allowed to be marked as \"disappeared\" until we\n        # need to deregister the object from tracking\n        self.maxDisappeared = maxDisappeared\n\n    def register(self, centroid):\n        # when registering an object we use the next available object\n        # ID to store the centroid\n        self.objects[self.nextObjectID] = centroid\n        self.disappeared[self.nextObjectID] = 0\n        self.nextObjectID += 1\n\n    def deregister(self, objectID):\n        # to deregister an object ID we delete the object ID from\n        # both of our respective dictionaries\n        del self.objects[objectID]\n        del self.disappeared[objectID]\n\n    def update(self, rects):\n        # check to see if the list of input bounding box rectangles\n        # is empty\n        if len(rects) == 0:\n            # loop over any existing tracked objects and mark them\n            # as disappeared\n            for objectID in list(self.disappeared.keys()):\n                self.disappeared[objectID] += 1\n\n                # if we have reached a maximum number of consecutive\n                # frames where a given object has been marked as\n                # missing, deregister it\n                if self.disappeared[objectID] > self.maxDisappeared:\n                    self.deregister(objectID)\n\n            # return early as there are no centroids or tracking info\n            # to update\n            return self.objects\n\n        # initialize an array of input centroids for the current frame\n        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n\n        # loop over the bounding box rectangles\n        for (i, (startX, startY, endX, endY)) in enumerate(rects):\n            # use the bounding box coordinates to derive the centroid\n            cX = int(startX - (endX\/2))\n            cY = int(startY - (endY\/2))\n            inputCentroids[i] = (cX, cY)\n            #cX = int((startX + endX) \/ 2.0)\n            #cY = int((startY + endY) \/ 2.0)\n            \n        # if we are currently not tracking any objects take the input\n        # centroids and register each of them\n        if len(self.objects) == 0:\n            for i in range(0, len(inputCentroids)):\n                self.register(inputCentroids[i])\n\n        # otherwise, are are currently tracking objects so we need to\n        # try to match the input centroids to existing object\n        # centroids\n        else:\n            # grab the set of object IDs and corresponding centroids\n            objectIDs = list(self.objects.keys())\n            objectCentroids = list(self.objects.values())\n\n            # compute the distance between each pair of object\n            # centroids and input centroids, respectively -- our\n            # goal will be to match an input centroid to an existing\n            # object centroid\n            D = dist.cdist(np.array(objectCentroids), inputCentroids)\n\n            # in order to perform this matching we must (1) find the\n            # smallest value in each row and then (2) sort the row\n            # indexes based on their minimum values so that the row\n            # with the smallest value as at the *front* of the index\n            # list\n            rows = D.min(axis=1).argsort()\n\n            # next, we perform a similar process on the columns by\n            # finding the smallest value in each column and then\n            # sorting using the previously computed row index list\n            cols = D.argmin(axis=1)[rows]\n\n            # in order to determine if we need to update, register,\n            # or deregister an object we need to keep track of which\n            # of the rows and column indexes we have already examined\n            usedRows = set()\n            usedCols = set()\n\n            # loop over the combination of the (row, column) index\n            # tuples\n            for (row, col) in zip(rows, cols):\n                # if we have already examined either the row or\n                # column value before, ignore it\n                # val\n                if row in usedRows or col in usedCols:\n                    continue\n\n                # otherwise, grab the object ID for the current row,\n                # set its new centroid, and reset the disappeared\n                # counter\n                objectID = objectIDs[row]\n                self.objects[objectID] = inputCentroids[col]\n                self.disappeared[objectID] = 0\n\n                # indicate that we have examined each of the row and\n                # column indexes, respectively\n                usedRows.add(row)\n                usedCols.add(col)\n\n            # compute both the row and column index we have NOT yet\n            # examined\n            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n\n            # in the event that the number of object centroids is\n            # equal or greater than the number of input centroids\n            # we need to check and see if some of these objects have\n            # potentially disappeared\n            if D.shape[0] >= D.shape[1]:\n                # loop over the unused row indexes\n                for row in unusedRows:\n                    # grab the object ID for the corresponding row\n                    # index and increment the disappeared counter\n                    objectID = objectIDs[row]\n                    self.disappeared[objectID] += 1\n\n                    # check to see if the number of consecutive\n                    # frames the object has been marked \"disappeared\"\n                    # for warrants deregistering the object\n                    if self.disappeared[objectID] > self.maxDisappeared:\n                        self.deregister(objectID)\n            \n            # otherwise, if the number of input centroids is greater\n            # than the number of existing object centroids we need to\n            # register each new input centroid as a trackable object\n            else:\n                for col in unusedCols:\n                    self.register(inputCentroids[col])\n\n        # return the set of trackable objects\n        return self.objects","e91b6447":"# kelas utills atau utilitas\nclass utills:\n\n    # Fungsi u\/ menentukan bottom center dari semua bounding boxes object--\n    # --dan kemudian akan digunakan u\/ melakukan transformasi dari prespective-view ke bird-view\n    def get_transformed_points(boxes, prespective_transform):\n        \n        # initialize rects dan bottom_points yg berguna untuk menyimpan array bottom center--\n        # --dari bounding box object\n        rects = []\n        bottom_points = []\n        \n        for box in boxes:\n            pnts = np.array([[[int(box[0]+(box[2]*0.5)),int(box[1]+box[3])]]] , dtype=\"float32\")\n            bd_pnt = cv2.perspectiveTransform(pnts, prespective_transform)[0][0]\n            pnt = [int(bd_pnt[0]), int(bd_pnt[1])]\n            pnt_bird = [int(bd_pnt[0]), int(bd_pnt[1]), 0, 0]\n            \n            bottom_points.append(pnt)\n            rects.append(np.array(pnt_bird))\n\n        return bottom_points, rects\n    \n    # Fungsi u\/ menghitung jarak antar dua point object (orang).\n    # distance_w, distance_h mempresentasikan pixel-to-metric ratio atau--\n    # --besar nilai pixel untuk jarak 180 cm dalam frame video.\n    def cal_dis(p1, p2, distance_w, distance_h):\n\n        h = abs(p2[1]-p1[1])\n        w = abs(p2[0]-p1[0])\n\n        dis_w = float((w\/distance_w)*180)\n        dis_h = float((h\/distance_h)*180)\n\n        return int(np.sqrt(((dis_h)**2) + ((dis_w)**2)))\n\n    # Fungsi u\/ menghitung jarak antar semua titik object dan--\n    # --menghiutng closeness ratio (rasio kedekatan).\n    def get_distances(boxes1, bottom_points, distance_w, distance_h):\n\n        distance_mat = []\n        bxs = []\n\n        for i in range(len(bottom_points)):\n            for j in range(len(bottom_points)):\n                if i != j:\n                    dist = utills.cal_dis(bottom_points[i], bottom_points[j], distance_w, distance_h)\n                    #dist = int((dis*180)\/distance)\n                    if dist <= 150:\n                        closeness = 0\n                        distance_mat.append([bottom_points[i], bottom_points[j], closeness])\n                        bxs.append([boxes1[i], boxes1[j], closeness])\n#                     elif dist > 150 and dist <=180:\n#                         closeness = 1\n#                         distance_mat.append([bottom_points[i], bottom_points[j], closeness])\n#                         bxs.append([boxes1[i], boxes1[j], closeness])       \n                    else:\n                        closeness = 2\n                        distance_mat.append([bottom_points[i], bottom_points[j], closeness])\n                        bxs.append([boxes1[i], boxes1[j], closeness])\n\n        return distance_mat, bxs\n\n    # Function gives scale for birds eye view  \n    # Fungsi memberikan skala untuk transformasi bird-view\n    # Skala yg digunakan w:480, h:1180 (video=1080 + pad=100) \n    def get_scale(W, H):\n        \n        dis_w = 387\n        dis_h = 580\n        \n        return float(dis_w\/W),float(dis_h\/H)\n\n    # Fungsi u\/ menghitung jumlah objek (orang) yg melakukan pelanggaran\n    def get_count(distances_mat):\n\n        r = []\n        g = []\n        #y = []\n\n        for i in range(len(distances_mat)):\n\n            if distances_mat[i][2] == 0:\n                if (distances_mat[i][0] not in r) and (distances_mat[i][0] not in g):\n                    r.append(distances_mat[i][0])\n                if (distances_mat[i][1] not in r) and (distances_mat[i][1] not in g):\n                    r.append(distances_mat[i][1])\n\n#         for i in range(len(distances_mat)):\n\n#             if distances_mat[i][2] == 1:\n#                 if (distances_mat[i][0] not in r) and (distances_mat[i][0] not in g) and (distances_mat[i][0] not in y):\n#                     y.append(distances_mat[i][0])\n#                 if (distances_mat[i][1] not in r) and (distances_mat[i][1] not in g) and (distances_mat[i][1] not in y):\n#                     y.append(distances_mat[i][1])\n\n        for i in range(len(distances_mat)):\n\n            if distances_mat[i][2] == 2:\n                if (distances_mat[i][0] not in r) and (distances_mat[i][0] not in g):\n                    g.append(distances_mat[i][0])\n                if (distances_mat[i][1] not in r) and (distances_mat[i][1] not in g):\n                    g.append(distances_mat[i][1])\n\n        #return (len(r),len(y),len(g))\n        return (len(r), len(g))","379a39ab":"class plot:\n\n    # Fungsi u\/ melakukan transformasi bird-view\n    def bird_eye_view(frame, distances_mat, bottom_points, scale_w, scale_h, risk_count, objects):\n        h = frame.shape[0]\n        w = frame.shape[1]\n\n        red = (0, 0, 255)\n        green = (0, 255, 0)\n        #yellow = (0, 255, 255)\n        white = (200, 200, 200)\n        black = (0,0,0)\n        \n\n        \n        blank_image = np.zeros((int(h * scale_h), int(w * scale_w), 3), np.uint8)\n        blank_image[:] = white\n        red_image = blank_image.copy()\n        \n        warped_pts = []\n        r = []\n        g = []\n\n        for i in range(len(distances_mat)):\n            if distances_mat[i][2] == 0:\n                if (distances_mat[i][0] not in r) and (distances_mat[i][0] not in g):\n                    r.append(distances_mat[i][0])\n                if (distances_mat[i][1] not in r) and (distances_mat[i][1] not in g):\n                    r.append(distances_mat[i][1])\n\n                blank_image = cv2.line(blank_image, (int(distances_mat[i][0][0] * scale_w), \n                                                     int(distances_mat[i][0][1] * scale_h)), \n                                       (int(distances_mat[i][1][0] * scale_w), \n                                        int(distances_mat[i][1][1]* scale_h)), red, 2)\n\n        for i in range(len(distances_mat)):\n            if distances_mat[i][2] == 2:\n                if (distances_mat[i][0] not in r) and (distances_mat[i][0] not in g):\n                    g.append(distances_mat[i][0])\n                if (distances_mat[i][1] not in r) and (distances_mat[i][1] not in g):\n                    g.append(distances_mat[i][1])\n                    \n        for i in bottom_points:\n            blank_image = cv2.circle(blank_image, (int(i[0]  * scale_w), int(i[1] * scale_h)), 5, green, 10)\n        for i in r:\n            blank_image = cv2.circle(blank_image, (int(i[0]  * scale_w), int(i[1] * scale_h)), 5, red, 10)\n            red_image = cv2.circle(red_image, (int(i[0]  * scale_w), int(i[1] * scale_h)), 10, red, 10)\n            \n        # Tampilkan object ID pada setiap objek yg terdeteksi-\n        # --pada frame bird-view\n        for (objectID, centroid) in objects.items():\n            text = \"ID {}\".format(objectID)\n            cv2.putText(blank_image, text, (int(centroid[0] * scale_w) - 10, int(centroid[1] * scale_h) - 15),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n        \n        \n        return blank_image, red_image\n    \n    \n    # Fungsi u\/ drawing bounding boxes pada frame perspective view--\n    # --dan drawing lines antar objek yg melakukan pelanggaran\n    def social_distancing_view(frame, distances_mat, boxes, risk_count, bird_view):\n\n        red = (0, 0, 255)\n        green = (0, 255, 0)\n        #yellow = (0, 255, 255)\n\n        for i in range(len(boxes)):\n            x,y,w,h = boxes[i][:]\n            frame = cv2.rectangle(frame,(x,y),(x+w,y+h),green,2)\n\n        for i in range(len(distances_mat)):\n            per1 = distances_mat[i][0]\n            per2 = distances_mat[i][1]\n            closeness = distances_mat[i][2]\n\n            if closeness == 0:\n                x,y,w,h = per1[:]\n                frame = cv2.rectangle(frame,(x,y),(x+w,y+h),red,2)\n\n                x1,y1,w1,h1 = per2[:]\n                frame = cv2.rectangle(frame,(x1,y1),(x1+w1,y1+h1),red,2)\n\n                frame = cv2.line(frame, (int(x+w\/2), int(y+h\/2)), (int(x1+w1\/2), int(y1+h1\/2)),red, 2)\n                \n            # buat pad (padding) pada sisi bawah frame prespective-view\n            # dengan h=100, dan w=frame.shape[1]\n            pad = np.full((100,frame.shape[1],3), [250, 250, 250], dtype=np.uint8)\n\n            # draw text pada padding\n            cv2.putText(pad, \"Jumlah Orang Terdeteksi : \" + str(risk_count[0] + risk_count[1]) + \" Orang\", (100, 40),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 0), 2)\n            cv2.putText(pad, \"Jumlah Pelanggaran Social Distancing : \" + str(risk_count[0]) + \" Orang\", (100, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n    \n        # gabungkan pad dengan frame prespective-view\n        # dan kemudian gabungkan dengan bird-view\n        frame = np.vstack((frame,pad))\n        frame = np.hstack((frame, bird_view))\n    \n        return frame","2e2813be":"# Fungsi u\/ melakukan processing social distancing detector\ndef calculate_social_distancing(vid_path, net, ln1, points):\n    \n    # initialize count dan video capture\n    count = 0\n    vs = cv2.VideoCapture(vid_path)    \n\n    # Ambil video height, width dan fps\n    height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n    fps = int(vs.get(cv2.CAP_PROP_FPS))\n    \n    # Tentukan skala untuk bird-view\n    scale_w, scale_h = utills.get_scale(width, height)\n    \n    # initialize penyimpanan video output hasil processing\n    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n    output_movie = cv2.VideoWriter(\"perspective_view.avi\", fourcc, fps, (1464, 720), True)\n    #bird_movie = cv2.VideoWriter(\"bird_eye_view.avi\", fourcc, fps, (400, 600), True)\n    \n    #points = []\n    \n    # variable berisi nilai red\n    RED_IMAGE = []\n    \n    global image\n    \n    # mulai processing dengan loop pada video capture\n    while True:\n        \n        # ambil grab dan frame\n        (grabbed, frame) = vs.read()\n\n        # berhenti saat nilai grab = false\n        if not grabbed:\n            print(\"[INFO] Processing done...\")\n            break\n        \n        # ambil H dan W dari frame vs\n        (H, W) = frame.shape[:2]\n          \n        # initialize src yg berisi 4 titik tranformasi, dan--\n        # --dst yang berisi 4 titik ukuran vs sebenarnya\n        # prespective_transform berisi matriks u\/ transformasi prespective ke bird-view\n        src = np.float32(np.array(points[:4]))\n        dst = np.float32([[0, H], [W, H], [W, 0], [0, 0]])\n        prespective_transform = cv2.getPerspectiveTransform(src, dst)\n        \n        # invers matriks u\/ transformasi bird ke prespective-view\n        inv_trans = np.linalg.pinv(prespective_transform)\n        \n        # gunakan 3 titik setelahnya u\/ pixel-to-metric ratio dalam variable pts\n        # warped_pt berisi transformasi 3 titik pada bird-view\n        pts = np.float32(np.array([points[4:7]]))\n        warped_pt = cv2.perspectiveTransform(pts, prespective_transform)[0]\n        \n        # initialize distance_w, dan distance_h yg masing2 berisi jarak 180 cm dalam satuan pixel\n        distance_w = np.sqrt((warped_pt[0][0] - warped_pt[1][0]) ** 2 + (warped_pt[0][1] - warped_pt[1][1]) ** 2)\n        distance_h = np.sqrt((warped_pt[0][0] - warped_pt[2][0]) ** 2 + (warped_pt[0][1] - warped_pt[2][1]) ** 2)\n        \n        # draw 4 titik transformasi pada frame video prespective-view\n        pnts = np.array(points[:4], np.int32)\n        cv2.polylines(frame, [pnts], True, (70, 70, 70), thickness=2)\n    \n        # Memproses deteksi dengan pre-trained model YOLO-v3\n        blob = cv2.dnn.blobFromImage(frame, 1 \/ 255.0, (416, 416), swapRB=True, crop=False)\n        net.setInput(blob)\n        start = time.time()\n        layerOutputs = net.forward(ln1)\n        end = time.time()\n        \n        boxes = []\n        confidences = []\n        classIDs = []   \n        rects = []\n    \n        for output in layerOutputs:\n            for detection in output:\n                scores = detection[5:]\n                classID = np.argmax(scores)\n                confidence = scores[classID]\n                \n                # deteksi (hanya) orang pada frame\n                # YOLO menggunakan dataset COCO dimana index human dalam--\n                # --dataset berada pada index 0\n                if classID == 0:\n\n                    if confidence > confid:\n\n                        box = detection[0:4] * np.array([W, H, W, H])\n                        (centerX, centerY, width, height) = box.astype(\"int\")\n                                    \n                        x = int(centerX - (width \/ 2))\n                        y = int(centerY - (height \/ 2))\n                        \n                        boxes.append([x, y, int(width), int(height)])\n                        confidences.append(float(confidence))\n                        classIDs.append(classID)\n            \n        idxs = cv2.dnn.NMSBoxes(boxes, confidences, confid, thresh)\n        font = cv2.FONT_HERSHEY_PLAIN\n        boxes1 = []\n        \n        for i in range(len(boxes)):\n            if i in idxs:\n                boxes1.append(boxes[i])\n                x,y,w,h = boxes[i]\n                \n        if len(boxes1) == 0:\n            count = count + 1\n            continue\n            \n        # initialize bottom-center dari setiap bounding-box yg terdeteksi, dan--\n        # initialize rects yg berisi bottom-center untuk digunakan pada generating ID's\n        person_points, rects = utills.get_transformed_points(boxes1, prespective_transform)\n        \n        # initialize objects yg berisi ID's dan centroid dari object yg terdeteksi\n        objects = ct.update(rects)\n        \n#         for (objectID, centroid) in objects.items():\n#             text = \"ID {}\".format(objectID)\n#             centroid = np.array([[[centroid[0], centroid[1]]]] , dtype=\"float32\")\n#             normal = cv2.perspectiveTransform(centroid, inv_trans)\n#             cv2.putText(frame, text, (int(normal[0][0][0]) - 10, int(normal[0][0][1]) + 20),\n#                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n# #             cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n# #                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n        \n        frame1 = np.copy(frame)\n        \n        # Hitung dan pelanggaran jarak antar objek (manusia) pada transformasi bird-view\n        \n        distances_mat, bxs_mat = utills.get_distances(boxes1, person_points, distance_w, distance_h)\n        risk_count = utills.get_count(distances_mat)\n    \n        # Draw bird eye view and frame with bouding boxes around humans according to risk factor \n        # Hasilkan video output transformasi bird-view, dan--\n        # gunakan hasil u\/ digabungkan dgn video output prespective-view \n        print(\"Jumlah R :\", len(RED_IMAGE))\n        bird_image, red_image = plot.bird_eye_view(frame, distances_mat, person_points, scale_w, scale_h, \n                                        risk_count, objects)\n        RED_IMAGE.append(red_image)\n        \n        for red_image in RED_IMAGE:\n            bird_image = cv2.addWeighted(red_image, 0.4, bird_image, 1 - 0.4, 0)\n        \n        img = plot.social_distancing_view(frame1, bxs_mat, boxes1, risk_count, bird_image)\n        \n        # resizing video output\n        img = imutils.resize(img, height=720)\n        \n        # write video\n        if count != 0:\n            output_movie.write(img)\n            #bird_movie.write(bird_image)\n            \n#             cv2.imshow('Bird Eye View', bird_image)\n            #cv2.imwrite(\"frame%d.jpg\" % count, img)\n            cv2.imwrite(\"Bird%d.jpg\" % count, bird_image)\n    \n        count = count + 1","b1626db3":"# initialize CentroidTracker Class\nct = CentroidTracker()\n\n# initialize confidence dan threshold\nconfid = 0.5\nthresh = 0.5\n\n# initialize 7 titik transformasi prespective-view ke bird-view\n# 4 titik pertama (bottom-left, bottom-right, top-right, top-left) digunakan u\/--\n# --melakukan transformasi prespective ke bird-view\n# 3 titik setelahnya digunakan untuk menghitung pixel-to-metric ratio\n# pts = [(27.29, 559.83), (1408.92, 815.57), (1957.83, 30.41), (1182.81, 30.41), \n#        (909.91, 642.47), (1057.27, 674.44), (1013.61, 559.83)]\n\n# pts u\/ video 480p\npts = [(0, 236.2), (615.86, 346.92), (853.5, 7), (500.25, 7), \n       (382.11, 271.98), (445.91, 285.82), (427, 236.2)]\n\n# Load Yolov3 weights\nweightsPath = \"..\/input\/yolo-coco-data\/yolov3.weights\"\nconfigPath = \"..\/input\/yolo-coco-data\/yolov3.cfg\"\n\nnet_yl = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\nln = net_yl.getLayerNames()\nln1 = [ln[i[0] - 1] for i in net_yl.getUnconnectedOutLayers()]\n\nnet_yl.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnet_yl.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n\n# initialize video path\nvideo_path = \"..\/input\/social-distancing\/Pedestrian 480p.mp4\"\n\nt1 = cv2.getTickCount()\n\n# processing social distancing detector\ncalculate_social_distancing(video_path, net_yl, ln1=ln1, points=pts)\n\nt2 = cv2.getTickCount()\n\nt3 = (t2 - t1)\/ cv2.getTickFrequency()\n\nprint(\"Waktu yang dibutuhkan :\", t3)","4fa83614":"R = []\nif len(R) == 0:\n    print(\"aaaa\")","1a9d28b6":"# Processing Social Distancing Detector","04516f4b":"# Plot Class\nPlot Class atau Kelas Ploting digunakan u\/ melakukan fungsi plotting atau drawing pada frame video","8985cc52":"# Utills Class\nUtills Class atau kelas utilitas berisi fungsi yang berguna dalam membantu\npenerapan sistem seperti :\n1. Transformasi dari prespective-view ke bird-view\n2. Menghitung pelanggaran social distancing ","9e58827f":"# Social Distancing Detektor Berbasis Computer Vision","f6f18e25":"# Centroid Tracker\nBerfungsi untuk mentracking centroid (titik tengah\/pusat) dari objek-\nyang dideteksi (orang).\nBerfungsi juga sebagai pendaftar ID (identitas) dari masing-\nmasing objek.\nBerbasis pada kode Adrian Rosebrock, https:\/\/www.pyimagesearch.com\/2018\/07\/23\/simple-object-tracking-with-opencv\/. Dengan beberapa penyesuaian"}}