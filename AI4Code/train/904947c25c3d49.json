{"cell_type":{"c0f939d0":"code","440f670d":"code","1dd32489":"code","4b673376":"code","ddf6e758":"code","dd1a45cc":"code","29efa05c":"code","172ef4a3":"code","a97f79ca":"code","4f1e2cda":"code","9d9d9e53":"code","28dcc4c0":"code","9d08f3b5":"code","ae980a36":"code","f8ff4397":"code","a7fbdd6e":"code","dca7a40b":"code","e83b2038":"code","75d43e3c":"code","b2436ccf":"code","9e8a7d22":"code","51c89451":"code","942539dd":"code","45419c16":"markdown","e95699f1":"markdown"},"source":{"c0f939d0":"# Importing required libraries.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns #visualisation\nimport matplotlib.pyplot as plt #visualisation\n%matplotlib inline \nsns.set(color_codes=True)\nimport os\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport statsmodels.api as sm\nfrom statsmodels.tools.eval_measures import mse, rmse\npd.options.display.float_format = '{:.5f}'.format\nimport warnings\nimport math\nimport scipy.stats as stats\nimport scipy\nfrom sklearn.preprocessing import scale\n# warnings.filterwarnings('ignore')","440f670d":"#Import train dataset\n# os.chdir('\/kaggle\/input')\ndf = pd.read_csv('..\/input\/train.csv')\ndf.head(5)","1dd32489":"# No duplicates found\nduplicate_rows_df = df[df.duplicated()]\nprint('Total duplicate rows:', duplicate_rows_df.shape)\nprint('\\nShape of Dataframe:',df.shape,'\\n\\nDataTypes:\\n',df.dtypes)","4b673376":"df.describe(include='all')","ddf6e758":"df.info()","dd1a45cc":"new_df = df.rename(columns={'loc.details':'details','location.Code':'branch_code','deposit_amount_2011':'2011','deposit_amount_2012':'2012','deposit_amount_2013':'2013','deposit_amount_2014':'2014','deposit_amount_2015':'2015','deposit_amount_2016':'2016','deposit_amount_2017':'Target'})\nnew_df.head()","29efa05c":"# Finding the relations between the variables.\nplt.figure(figsize=(20,10))\nmap= new_df.corr()\nsns.heatmap(map,cmap='BrBG',annot=True)","172ef4a3":"grid = sns.FacetGrid(new_df, size=2.2, aspect=5.6)\ngrid.map(sns.pointplot, 'state', 'Target', palette='deep')\ngrid.add_legend()","a97f79ca":"train = new_df.drop(columns ={'branch_code','id','headquarter','date_of_establishment','location','details'}, axis = 1)\ntrain.head()","4f1e2cda":"# Finding the null values\nprint(train.isnull().sum())\nprint(train.shape)\n# train = train.dropna() ","9d9d9e53":"# Remove Outliers\nQ1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3 - Q1\nIQR","28dcc4c0":"train = train[~((train < (Q1-1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]\ntrain.shape","9d08f3b5":"# Filling Missing Values with mean\ntrain['2011'].fillna(train['2011'].mean(), inplace=True)\ntrain['2012'].fillna(train['2012'].mean(), inplace=True)\ntrain['2013'].fillna(train['2013'].mean(), inplace=True)\ntrain['2014'].fillna(train['2014'].mean(), inplace=True)\ntrain['2015'].fillna(train['2015'].mean(), inplace=True)\ntrain = train.dropna() ","ae980a36":"# Adding Dummy Variables\n# generate binary values using get_dummies\ntrain = pd.get_dummies(train, columns=[\"state\"], prefix=[\"state\"] )\ntrain=train[['2011','2012','2013','2014','2015','2016','state_OH'\n,'state_NY'\n,'state_CT'\n,'state_NJ'\n,'state_TX'\n,'state_KY'\n# ,'state_WV'\n,'state_IL'\n,'state_LA'\n,'state_FL'\n,'state_AZ'\n,'state_UT'\n,'state_MI'\n,'state_WI','Target']]\ntrain.head()","f8ff4397":"# Finding the relations between the variables.\nplt.figure(figsize=(40,10))\nmap= train.corr()\nsns.heatmap(map,cmap='BrBG',annot=True)","a7fbdd6e":"Y=train['Target']\nX = train.drop(columns ={'Target'}, axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n\nprint('Training Data Count: {}'.format(X_train.shape[0]))\nprint('Testing Data Count: {}'.format(X_test.shape[0]))","dca7a40b":"X_train = sm.add_constant(X_train)\nresults = sm.OLS(y_train, X_train).fit()\nresults.summary()","e83b2038":"X_test = sm.add_constant(X_test)\n\ny_preds = results.predict(X_test)\n\nprint(\"Mean Absolute Error (MAE)         : {}\".format(mean_absolute_error(y_test, y_preds)))\nprint(\"Mean Squared Error (MSE) : {}\".format(mse(y_test, y_preds)))\nprint(\"Root Mean Squared Error (RMSE) : {}\".format(rmse(y_test, y_preds)))\nprint(\"Root Mean Squared Error (RMSE) : {}\".format(rmse(y_test, y_preds)))\nprint(\"Mean Absolute Perc. Error (MAPE) : {}\".format(np.mean(np.abs((y_test - y_preds) \/ y_test)) * 100))","75d43e3c":"X2 = train[['2016','2015','2014','2013','2011']]\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, Y, test_size = 0.3, random_state = 465)\n\nprint('Training Data Count:', X2_train.shape)\nprint('Testing Data Count::', X2_test.shape)\n\n\nX2_train = sm.add_constant(X2_train)\n\nresults2 = sm.OLS(y2_train, X2_train).fit()\n\nresults2.summary()","b2436ccf":"X2_test = sm.add_constant(X2_test)\n\ny2_preds = results2.predict(X2_test)\n\nprint(\"Mean Absolute Error (MAE)         : {}\".format(mean_absolute_error(y2_test, y2_preds)))\nprint(\"Mean Squared Error (MSE)          : {}\".format(mse(y2_test, y2_preds)))\nprint(\"Root Mean Squared Error (RMSE)    : {}\".format(rmse(y2_test, y2_preds)))\nprint(\"Root Mean Squared Error (RMSE)    : {}\".format(rmse(y2_test, y2_preds)))\nprint(\"Mean Absolute Perc. Error (MAPE)  : {}\".format(np.mean(np.abs((y2_test - y2_preds) \/ y2_test)) * 100))","9e8a7d22":"test = pd.read_csv('..\/input\/test.csv')\ntest.head(5)","51c89451":"# data = [[32079.00000\t,35971.50000\t,37237.50000\t,40362.00000\t,46021.50000]]\ntest1 = pd.get_dummies(test, columns=[\"state\"], prefix=[\"state\"] )\ntest1 = test1.drop(columns ={'location.Code','id','headquarter','date_of_establishment','location','loc.details','state_WV'}, axis = 1)\n\n# test1\ntest1 = sm.add_constant(test1)\ntest['deposit_amount_2017'] = results.predict(test1)\n# test = results2.predict(data)\n# test.to_csv('Output3.csv')","942539dd":"test","45419c16":"# Building Linear Regression Model","e95699f1":"## EDA"}}