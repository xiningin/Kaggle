{"cell_type":{"f2472838":"code","2823cc32":"code","8394bae8":"code","51f68437":"code","8deedeed":"code","e9408398":"code","d46c2a7f":"code","bf1441de":"code","5ae84a59":"code","a24bc05f":"code","841f7574":"code","e2e7ed53":"code","4c7f71a7":"code","fc95c706":"code","20033063":"code","f5f93f2c":"code","61411587":"code","c06b0723":"code","e6045fa9":"code","629a289e":"code","9335c7fb":"code","6cdbdaad":"code","e3f830a8":"code","a3400b5d":"code","2f191581":"code","03dc37ed":"code","26d7dddf":"code","ab74cb60":"code","deab94fa":"code","1696cbbf":"code","dbd3be81":"code","8a6f88ae":"code","163f4d27":"code","3d71e7e0":"code","afe5f8df":"code","06c4c4ca":"code","89370f5c":"markdown","3b4aa1ba":"markdown","6261e8c7":"markdown","e8814ae8":"markdown","4c04b71e":"markdown","857a1b83":"markdown","181a2e42":"markdown","145f90fe":"markdown","5f64afcf":"markdown","59329974":"markdown","e5d02b29":"markdown","ac48c13e":"markdown","56ae0913":"markdown","da63b46b":"markdown","e18494e2":"markdown","55b8d1f4":"markdown","c9ec8ed5":"markdown","37f94837":"markdown","c8a46fa9":"markdown"},"source":{"f2472838":"# Let`s import all packages that we may need:\n\nimport sys \nimport numpy as np # linear algebra\nfrom scipy.stats import randint\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. \nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.cross_validation import KFold # use for cross validation\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline # pipeline making\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nfrom sklearn.metrics import mean_squared_error,r2_score\n\n## for Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dropout","2823cc32":"## Data can be downloaded from: http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00235\/\n## Just open the zip file and grab the file 'household_power_consumption.txt' put it in the directory \n## that you would like to run the code. \n\n\ndf = pd.read_csv('..\/input\/household_power_consumption.txt', sep=';', \n                 parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, \n                 low_memory=False, na_values=['nan','?'], index_col='dt')","8394bae8":"df.head(15) ","51f68437":"df.info()","8deedeed":"df.dtypes","e9408398":"df.shape","d46c2a7f":"df.describe()","bf1441de":"df.columns","5ae84a59":"## finding all columns that have nan:\ndf_null=df.isnull().sum()\ndf_null[df_null!=0]","a24bc05f":"df=df.fillna(df.mean())# filling nan with mean in any columns","841f7574":"df_null=df.isnull().sum()\ndf_null[df_null!=0]","e2e7ed53":"df.Global_active_power.resample('D').sum().plot(title='Sum of Global Active Power for days ',color='green') \nplt.tight_layout()\nplt.show()  \n\ndf.Global_active_power.resample('D').mean().plot(title='Mean of Global Active Power for days', color='red') \nplt.tight_layout()\nplt.show()","4c7f71a7":"### mean and std of 'Global_intensity' resampled over day \nr = df.Global_intensity.resample('D').agg(['mean', 'std'])\nr.plot(subplots = True, title='Mean and Std of Global intensity resampled over day')\nplt.show()","fc95c706":"### mean and std of 'Global_reactive_power' resampled over day\nr2 = df.Global_reactive_power.resample('D').agg(['mean', 'std'])\nr2.plot(subplots = True, title='Mean and Std of Global reactive power resampled over day')\nplt.show()","20033063":"### Sum of 'Global_active_power' resampled over month\ndf['Global_active_power'].resample('M').mean().plot(kind='bar')\nplt.xticks(rotation=90)\nplt.ylabel('Global_active_power')\nplt.title('Average Global_active_power per month')\nplt.show()","f5f93f2c":"## Mean of 'Global_active_power' resampled over quarter\ndf['Global_active_power'].resample('Q').mean().plot(kind='bar')\nplt.xticks(rotation=60)\nplt.ylabel('Global_active_power')\nplt.title('Average Global_active_power per quarter')\nplt.show()","61411587":"## mean of 'Voltage' resampled over year\ndf['Voltage'].resample('AS').mean().plot(kind='bar', color='red')\nplt.xticks(rotation=60)\nplt.ylabel('Voltage')\nplt.title('Mean of Voltage per year')\nplt.show()","c06b0723":"## mean of 'Voltage' resampled over month\ndf['Voltage'].resample('Q').mean().plot(kind='bar', color='red')\nplt.xticks(rotation=60)\nplt.ylabel('Voltage')\nplt.title('Mean of Voltage per quarter')\nplt.show()","e6045fa9":"df['Sub_metering_1'].resample('Q').mean().plot(kind='bar', color='brown')\nplt.xticks(rotation=60)\nplt.ylabel('Sub_metering_1')\nplt.title('Mean of Sub_metering_1 per quarter')\nplt.show()","629a289e":"# Below I compare the mean of different features resampled over day. \n# specify columns to plot\ncols = [0, 1, 2, 3,4,5, 6]\ni = 1\ngroups=cols\nvalues = df.resample('D').mean().values\n# plot each column\nplt.figure(figsize=(15, 10))\nfor group in groups:\n\tplt.subplot(len(cols), 1, i)\n\tplt.plot(values[:, group])\n\tplt.title(df.columns[group], y=0.75, loc='right')\n\ti += 1\nplt.show()","9335c7fb":"## resampling over week and computing mean\ncols = [0, 1, 2, 3,4, 5, 6]\ni = 1\ngroups=cols\nvalues = df.resample('W').mean().values\n# plot each column\nplt.figure(figsize=(15, 10))\nfor group in groups:\n\tplt.subplot(len(cols), 1, i)\n\tplt.plot(values[:, group])\n\tplt.title(df.columns[group], y=0.75, loc='right')\n\ti += 1\n\nplt.show()","6cdbdaad":"## resampling over month and computing mean\ncols = [0, 1, 2, 3, 5, 6]\ni = 1\ngroups=cols\nvalues = df.resample('M').mean().values\n# plot each column\nplt.figure(figsize=(15, 10))\nfor group in groups:\n\tplt.subplot(len(cols), 1, i)\n\tplt.plot(values[:, group])\n\tplt.title(df.columns[group], y=0.75, loc='right')\n\ti += 1\n\nplt.show()","e3f830a8":"## resampling over week and computing mean\ndf.Global_reactive_power.resample('W').mean().plot(color='y', legend=True)\ndf.Global_active_power.resample('W').mean().plot(color='r', legend=True)\ndf.Sub_metering_1.resample('W').mean().plot(color='b', legend=True)\ndf.Global_intensity.resample('W').mean().plot(color='g', legend=True)\nplt.show()","a3400b5d":"## The correlations between 'Global_intensity', 'Global_active_power'\ndata_returns = df.pct_change()\nsns.jointplot(x='Global_intensity', y='Global_active_power', data=data_returns)  \n\nplt.show()","2f191581":"##\u00a0The correlations between 'Voltage' and  'Global_active_power'\nsns.jointplot(x='Voltage', y='Global_active_power', data=data_returns)  \nplt.show()","03dc37ed":"# Correlations among columns\nplt.figure(figsize=(8,8))\nplt.matshow(df.corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn',fignum=1)\nplt.title('Correlation Matrix (No Resampling)', size=15)\nplt.colorbar()\nplt.show()","26d7dddf":"# Correlations of mean of features resampled over months\nplt.figure(figsize=(8,8))\nplt.matshow(df.resample('M').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn',fignum=1)\nplt.title('Correlation Matrix (Month)', size=15)\nplt.colorbar()\nplt.margins(0.02)\n","ab74cb60":"plt.figure(figsize=(8,8))\nplt.matshow(df.resample('A').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn',fignum=1)\nplt.title('Correlation Matrix (Year)', size=15)\nplt.colorbar()\nplt.show()","deab94fa":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg\n ","1696cbbf":"## resampling of data over hour\ndf_resample = df.resample('h').mean() \ndf_resample.shape","dbd3be81":"values = df_resample.values \nvalues = values.astype('float32')\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\nprint(reframed.head())","8a6f88ae":"# drop columns we don't want to predict\nreframed.drop(reframed.columns[[8,9,10,11,12,13]], axis=1, inplace=True)\nprint(reframed.head())","163f4d27":"# split into train and test sets\nvalues = reframed.values\n\nn_train_time = 365*24\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\n##test = values[n_train_time:n_test_time, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape) \n# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features].","3d71e7e0":"\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n\n\n# fit network\nhistory = model.fit(train_X, train_y, epochs=20, batch_size=70, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\n","afe5f8df":"# make a prediction\nyhat = model.predict(test_X)\ntest_X = test_X.reshape((test_X.shape[0], 7))\n# invert scaling for forecast\ninv_yhat = np.concatenate((yhat, test_X[:, -6:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_X[:, -6:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","06c4c4ca":"## for a demonstration purpose, I only compare the predictions in 600 hours. \naa=[x for x in range(600)]\nplt.plot(aa, inv_y[:600], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:600], 'r', label=\"prediction\")\nplt.ylabel('Global_active_power', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show()","89370f5c":"##\u00a0* Below I resample over day, and show the sum and mean of Global_active_power. It is seen that mean and sum of resampled data set, have similar structure.","3b4aa1ba":"From above two plots it is seen that 'Global_intensity' and 'Global_active_power'  are highly correlated. But 'Voltage', 'Global_active_power' are less correlated. This is important observation for machine learning purpose.","6261e8c7":"Above I showed 7 input variables (input series) and the 1 output variable for 'Global_active_power' at the current  time in hour (depending on resampling). ","e8814ae8":"In order to reduce the computation time, and also get a quick result to test the model.  One can resmaple the data over** hour** (the original data are given in minutes). This will reduce the size of data from **2075259** to **34589 **but keep the overall strucure of data as shown in the above.   ","4c04b71e":"- The data includes 'nan' and '?' as a string. Both have been converter to numpy nan in importing stage (above) and treated the same. \n\n- Merged two columns 'Date' and 'Time' to 'dt'. \n\n- Converted the data to time-series type, by taking index to be the time. ","857a1b83":"#####  I will frame the supervised learning problem as predicting the **Global_active_power** at the current time (t) given the Global_active_power measurement and other features at the prior time step.","181a2e42":"# Model architecture\n\n### 1)  LSTM with 100 neurons in the first visible layer \n### 3) Dropout 20%\n### 4) 1 neuron in the output layer for predicting Global_active_power. \n### 5) The model will be fit for 20 training epochs with a batch size of 70.\n","145f90fe":"### * From above two plots it is seen that 'Global_intensity' and 'Global_active_power' correlated. But 'Voltage', 'Global_active_power' are less correlated. This is important observation for machine learning purpose. ","5f64afcf":"# Final remarks","59329974":"# Correlations among features","e5d02b29":"## Dealing with missing values  'nan' ","ac48c13e":"### * Here I have used the LSTM neural network which is now the state-of-the-art for sequencial problems. \n\n### * In order to reduce the computation time, and get some results quickly, I took the first year of data (resampled over hour) to train the model and the rest of data to test the model.  \n\n### * I put together a very simple LSTM neural-network to show that one can obtain reasonable predictions. However numbers of rows is too high and as a result the computation is very time-consuming (even for the simple model in the above it took few mins to be run on  2.8 GHz Intel Core i7).  \n  ","56ae0913":"The description of data can be found here:\nhttp:\/\/archive.ics.uci.edu\/ml\/datasets\/Individual+household+electric+power+consumption\n\nAttribute Information:\n1.date: Date in format dd\/mm\/yyyy\n\n2.time: time in format hh:mm:ss\n\n3.global_active_power: household global minute-averaged active power (in kilowatt)\n\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n\n5.voltage: minute-averaged voltage (in volt)\n\n6.global_intensity: household global minute-averaged current intensity (in ampere)\n\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.","da63b46b":"#\u00a0Data visualization","e18494e2":" It is seen from above that with resampling techniques one can change the correlations among features. This is important for feature  engineering.","55b8d1f4":"# Splitting the rest of data to train and validation sets","c9ec8ed5":"  It is seen from the above plots that the mean of 'Volage' over quarter is pretty much constant compared to other features. ","37f94837":"# Machine-Leaning: LSTM Data Preparation and feature engineering","c8a46fa9":"###\u00a0Note that in order to improve the model, one has to adjust epochs and batch_size."}}