{"cell_type":{"694d539a":"code","e6690493":"code","d13aba64":"code","3f3ffeae":"code","f4888726":"code","a61e4a50":"code","4b3cfabe":"code","bc50de6c":"code","627aec18":"code","9859ebaf":"markdown","176ee84e":"markdown"},"source":{"694d539a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e6690493":"import os # Handle working with files and directories\nimport tensorflow as tf # Loading images, storing them in tensors\nimport cv2\nimport PIL\nfrom PIL import Image\nPIL.Image.MAX_IMAGE_PIXELS = 933120000\nimport matplotlib.pyplot as plt","d13aba64":"pokemon_dir = '..\/input\/pokemon\/pokemon'","3f3ffeae":"# Convert an image to a jpeg\ndef convert_to_jpg(img_path):\n    # Convert png to jpeg\n    img = Image.open(img_path)\n    if img.mode == 'RGBA':\n        img.load()\n        background = Image.new(\"RGB\", img.size, (0,0,0))\n        background.paste(img, mask=img.split()[3])\n        img = np.array(background)\n    else:\n        img = img.convert('RGB')\n        img = np.array(img)\n    \n    return img\n        \n# Resize image to 128x128\ndef resize_img(img):\n    img = cv2.resize(img, (128,128))\n    return img\n\n# Normalize pixel values from -1 to 1\ndef normalize_img(img):\n    img = img \/ 127.5 - 1\n    return img\n\n# Open an image, convert to jpeg, resize if needed\ndef open_convert(img_path):\n    # png\n    if img_path[-4:] == '.png':\n        img = convert_to_jpg(img_path)\n    # jpeg\n    else:\n        img = Image.open(img_path)\n        img = img.convert('RGB')\n        img = np.array(img)\n\n        \n    # Convert to 128x128\n    img = resize_img(img)\n    \n    # Normalize img\n    img = normalize_img(img)\n    \n    # Return resized img\n    return img\n\n# Test\nimg = open_convert('..\/input\/pokemon\/pokemon\/Aerodactyl\/00000048.png~original')\n# img = Image.fromarray(img, 'RGB')\n# img.save('my.png')\nplt.imshow(img)\nplt.show()","f4888726":"# Contain images and labels\nimages = []\nlabels = []\n\n# How many images per pokemon to load\nimages_per_pokemon = 5\n\n# Keep track of current iteration\ncount = 0\n# Iterate through each pokemon folder\nfor pkmn in os.listdir(pokemon_dir):\n    pkmn_dir = os.path.join(pokemon_dir, pkmn)\n    \n    # Current number of images loaded for this pokemon\n    curr_imgs = 0\n    \n    # Add each image to the list\n    for img in sorted(os.listdir(pkmn_dir)):\n        # Attempt to add image and label to list\n        try:\n            images.append(open_convert(os.path.join(pkmn_dir, img)))\n            labels.append(pkmn)\n        except (ValueError, OSError):\n            continue\n        count += 1\n        if count % 1000 == 0:\n            print('Current iteration: ' + str(count))\n            \n        # Increment num images loaded\n        curr_imgs += 1\n        if curr_imgs >= images_per_pokemon:\n            break","a61e4a50":"plt.imshow(images[123])\nplt.show()","4b3cfabe":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, AveragePooling2D, Reshape, Input\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\n\ninput_shape = (images[0].shape)\n\nlatent_dim = 100\n\ndef make_gen():\n    Gen = Sequential()\n\n    Gen.add(Dense(256, input_dim=latent_dim))\n    Gen.add(LeakyReLU(alpha=0.2))\n    Gen.add(BatchNormalization(momentum=0.8))\n    Gen.add(Dense(512))\n    Gen.add(LeakyReLU(alpha=0.2))\n    Gen.add(BatchNormalization(momentum=0.8))\n    Gen.add(Dense(1024))\n    Gen.add(LeakyReLU(alpha=0.2))\n    Gen.add(BatchNormalization(momentum=0.8))\n    Gen.add(Dense(np.prod(input_shape), activation='tanh'))\n    Gen.add(Reshape(input_shape))\n\n    Gen.summary()\n    \n    noise = Input(shape=(latent_dim,))\n#     noise = Input(shape=input_shape)\n    img = Gen(noise)\n    \n    return Model(noise, img)\n    \ndef make_discr():\n    Discr = Sequential()\n\n    Discr.add(Conv2D(128, (3, 3), strides=(2, 2), input_shape=input_shape))\n    Discr.add(LeakyReLU(alpha=0.2))\n    Discr.add(AveragePooling2D(pool_size = (4, 4)))\n    Discr.add(Flatten())\n    Discr.add(Dense(units=512, activation='relu'))\n    Discr.add(Dense(units=1, activation='sigmoid'))\n    \n    Discr.summary()\n    \n    img = Input(shape=input_shape)\n    validity = Discr(img)\n    \n    return Model(img, validity)","bc50de6c":"optimizer = Adam(0.0002, 0.5)\n\nDiscr = make_discr()\nDiscr.compile(loss='binary_crossentropy',\n             optimizer=optimizer,\n             metrics=['accuracy'])\n\nGen = make_gen()\n\nz = Input(shape=(latent_dim,))\n# z = Input(shape=input_shape)\nimg = Gen(z)\n\nDiscr.trainable = False\n\nvalidity = Discr(img)\n\nCombined = Model(z, validity)\nCombined.compile(loss='binary_crossentropy', optimizer=optimizer)\n\nbatch_size = 128\n\nepochs = 45000\n\nsample_interval = 1000\n\nvalid = np.ones((batch_size, 1))\nfake = np.zeros((batch_size, 1))\n\n# images = np.array(images)\n# images = np.expand_dims(images, axis=3)\n\ndef sample_images(epoch):\n    r, c = 5, 5\n    noise = np.random.normal(0, 1, (r * c, latent_dim))\n#     noise = np.random.randn(r*c, 128, 128, 3)\n\n    gen_imgs = Gen.predict(noise)\n    \n    gen_imgs = 0.5 * gen_imgs + 0.5\n    \n#     for cnt in range(len(gen_imgs)):\n#         imgname = 'epoch%dimg%d' % (epoch, cnt)\n#         gen_imgs.to_csv(imgname, index=False)\n    \n    fig, axs = plt.subplots(r, c)\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n            axs[i,j].axis('off')\n            cnt += 1\n#     fig.savefig(\"\/%d.png\" % epoch)\n#     plt.close()\n    plt.show()\n    \ndef sample_big_images(epoch):\n    noise = np.random.normal(0, 1, (5, latent_dim))\n#     noise = np.random.randn(5, 128, 128, 3)\n    \n    gen_imgs = Gen.predict(noise)\n    \n    gen_imgs = 0.5 * gen_imgs + 0.5\n    \n    # Show 5 images\n    for i in range(5):\n        plt.imshow(gen_imgs[i, :, :, :])\n        plt.show()\n\n\nfor epoch in range(epochs):\n    idx = np.random.randint(0, len(images), batch_size)\n    imgs = np.array([images[j] for j in idx])\n    \n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n#     noise = np.random.randn(batch_size, 128, 128, 3)\n    \n    gen_imgs = Gen.predict(noise)\n    \n    d_loss_real = Discr.train_on_batch(imgs, valid)\n    d_loss_fake = Discr.train_on_batch(gen_imgs, fake)\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n    \n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n#     noise = np.random.randn(batch_size, 128, 128, 3)\n\n    g_loss = Combined.train_on_batch(noise, valid)\n    \n    if epoch % sample_interval == 0:\n        print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n        sample_images(epoch)\n        sample_big_images(epoch)\n    ","627aec18":"for i in range(5):\n    sample_images(epoch)\n    sample_big_images(epoch)","9859ebaf":"In order to facilitate more simple image loading and general uniformity, the following code block will be used to define functions that will convert images to the RGB color space and resize them to 128x128. Credit to https:\/\/github.com\/llSourcell\/Pokemon_GAN for writing similar functions for image processing.","176ee84e":"Relying heavily on the following two github projects for the structure of this GAN\n\nhttps:\/\/github.com\/eriklindernoren\/Keras-GAN\n\nhttps:\/\/github.com\/llSourcell\/Pokemon_GAN\n"}}