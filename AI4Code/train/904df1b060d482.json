{"cell_type":{"15778b62":"code","cc1029b6":"code","4b44916b":"code","4d286bff":"code","59ba0118":"code","7e88d96a":"code","4a769d23":"code","ec0382c3":"code","f31c6220":"code","775b87d9":"code","c92d50d0":"code","5a983862":"code","0917f5d3":"code","acf5fd5b":"code","f91c8570":"code","38f89098":"code","1a2d90ea":"code","bc77a24a":"code","49f3aaa9":"code","d5057d78":"code","66f007d5":"code","9402ac4d":"code","a48ddb3b":"code","52422530":"code","bd032b72":"code","20a443bf":"code","cc083ed2":"code","a3d2b13a":"code","5d813fee":"code","ea206623":"code","d9dabbd3":"code","371d7aad":"code","092e8a37":"code","953b5054":"code","e33f7564":"code","7dc97cc6":"code","908efd8d":"code","4b68d5d7":"code","bfc39cd8":"code","b9d1c013":"code","4c600ad5":"code","c35e471b":"code","3ed8bc33":"code","a4dac182":"code","36f800ab":"code","3dfd32b0":"markdown","b73d2513":"markdown","a61b80a1":"markdown","2f638271":"markdown","7023cc6e":"markdown","9e6d3aec":"markdown","d44bb265":"markdown","b54a9b90":"markdown","e87454c1":"markdown","051a1dea":"markdown","2920984a":"markdown","c320e422":"markdown","6b00f467":"markdown","c56e321f":"markdown","9b5e17c9":"markdown","07af4967":"markdown","b358e713":"markdown","7472ef23":"markdown","5fe5ffa2":"markdown","8ef370d6":"markdown","24ad3289":"markdown"},"source":{"15778b62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc1029b6":"data= pd.read_csv('..\/input\/building-energy-efficiency\/ENB2012_data.csv', delimiter=',',nrows = None)\ndata.dataframeName = 'ENB2012_data.csv'\nnRow, nCol = data.shape\nprint(f'There are {nRow} rows and {nCol} columns')","4b44916b":"data.head(2)","4d286bff":"data.shape     ## dimension of the data","59ba0118":"data.isnull().sum()      ##checking Null values","7e88d96a":"data.info()","4a769d23":"data.describe()     ## statistics of the data or data description","ec0382c3":"import matplotlib.pyplot as plt    ##Visualization library\ndata.hist(figsize=(20,10),)        ## Histogram plot for checking the distribution of each variables\nplt.show()","f31c6220":"#b) Density Plot\nimport seaborn as sns   ## visualization library      \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n##Independent variables\nfig, (ax1, ax2,ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 5))\nsns.distplot(data['X1'], ax = ax1)      ## Distribution\/density plot for X1(observation 1 of EEG Signals)\nsns.despine(ax=ax1)\nax1.set_xlabel('X1', fontsize=15)\nax1.set_ylabel('observation', fontsize=15)\nax1.set_title('X1_Relative_compactness', fontsize=15)\nax1.tick_params(labelsize=15)\n\nsns.distplot(data['X2'], ax = ax2)     ## Distribution\/density plot for X2(observation 2 of EEG Signals)\nsns.despine(ax = ax2)\nax2.set_xlabel('X2', fontsize=15)\nax2.set_ylabel('Observation', fontsize=15)\nax2.set_title('X2_Surface_Area', fontsize=15)\nax2.tick_params(labelsize=15)\n\nsns.distplot(data['X3'], ax = ax3)     ## Distribution\/density plot for X3(observation 3 of EEG Signals)\nsns.despine(ax=ax3)\nax3.set_xlabel('X3', fontsize=20)\nax3.set_ylabel('observation', fontsize=20)\nax3.set_title('x3_Wall_Area', fontsize=20)\nax3.tick_params(labelsize=20)\n\nfig, (ax4, ax5,ax6) = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 7))\n\nsns.distplot(data['X4'], ax = ax4)    ## Distribution\/density plot for X4(observation 4 of EEG Signals)\nsns.despine(ax = ax4)\nax4.set_xlabel('x4', fontsize=20)\nax4.set_ylabel('Observation', fontsize=20)\nax4.set_title('x4_Roof_Area', fontsize=20)\nax4.tick_params(labelsize=20)\n\nsns.distplot(data['X5'], ax = ax5)    ## Distribution\/density plot for X4(observation 4 of EEG Signals)\nsns.despine(ax = ax4)\nax5.set_xlabel('X5', fontsize=20)\nax5.set_ylabel('Observation', fontsize=20)\nax5.set_title('x5_overall_height', fontsize=20)\nax5.tick_params(labelsize=20)\n\nsns.distplot(data['X6'], ax = ax6)    ## Distribution plot for X4(observation 4 of EEG Signals)\nsns.despine(ax = ax4)\nax6.set_xlabel('X5', fontsize=20)\nax6.set_ylabel('Observation', fontsize=20)\nax6.set_title('x6_Orientation', fontsize=20)\nax6.tick_params(labelsize=20)\n\nplt.subplots_adjust(wspace=1)\nplt.tight_layout() ","775b87d9":"fig, (ax1, ax2,ax3,ax4) = plt.subplots(nrows = 1, ncols = 4, figsize = (20, 7))\n\nsns.distplot(data['X7'], ax = ax1)    ## Distribution\/density plot for X7\nsns.despine(ax = ax1)\nax1.set_xlabel('x7', fontsize=20)\nax1.set_ylabel('Observation', fontsize=20)\nax1.set_title('x7_Glazing_Area', fontsize=20)\nax1.tick_params(labelsize=20)\n\nsns.distplot(data['X8'], ax = ax2)    ## Distribution\/density plot for X8\nsns.despine(ax = ax2)\nax2.set_xlabel('X8', fontsize=20)\nax2.set_ylabel('Observation', fontsize=20)\nax2.set_title('x8_GlazingArea_distribution', fontsize=20)\nax2.tick_params(labelsize=20)\n\n## dependent variables\nsns.distplot(data['Y1'], ax = ax3)    ## Distribution\/density plot for Y1\nsns.despine(ax = ax3)\nax3.set_xlabel('Y1', fontsize=20)\nax3.set_ylabel('Observation', fontsize=20)\nax3.set_title('Y1_HeatingLoad', fontsize=20)\nax3.tick_params(labelsize=20)\n\nsns.distplot(data['Y2'], ax = ax4)    ## Distribution\/Density plot for Y2\nsns.despine(ax = ax4)\nax4.set_xlabel('Y2', fontsize=20)\nax4.set_ylabel('Observation', fontsize=20)\nax4.set_title('Y2_cooling_load', fontsize=20)\nax4.tick_params(labelsize=20)\n\nplt.subplots_adjust(wspace=1)\nplt.tight_layout() ","c92d50d0":"#C) Correlation\ndata.corr()","5a983862":"##Correlation plot\nplt.subplots(figsize=(20,10))\nsns.set(font_scale=1.2)\nsns.heatmap(data.corr(), annot=True)\nplt.show()","0917f5d3":"##Independent variables\nfig, (ax1, ax2, ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (30, 7))\nsns.boxplot(data['X1'], ax = ax1)      ## Boxplot for X1\nax1.set_xlabel('X1', fontsize=25)\nax1.set_ylabel('observation', fontsize=25)\nax1.set_title('X1_Relative_Compactness', fontsize=25)\nax1.tick_params(labelsize=25)\n\nsns.boxplot(data['X2'], ax = ax2)     ## Boxplot for X2\nax2.set_xlabel('x2', fontsize=25)\nax2.set_ylabel('Observation', fontsize=25)\nax2.set_title('X2_Surface_Area', fontsize=25)\nax2.tick_params(labelsize=25)\n\nsns.boxplot(data['X3'], ax = ax3)     ## Boxplot for X3\nax3.set_xlabel('X3', fontsize=25)\nax3.set_ylabel('observation', fontsize=25)\nax3.set_title('X3_Wall_Area', fontsize=25)\nax3.tick_params(labelsize=25)\n\n\n\nfig, ( ax4, ax5, ax6) = plt.subplots(nrows = 1, ncols = 3, figsize = (50, 10))\n\nsns.boxplot(data['X4'], ax = ax4)    ## Boxplot for X4\nax4.set_xlabel('X4', fontsize=35)\nax4.set_ylabel('Observation', fontsize=35)\nax4.set_title('X4_Roof_Area', fontsize=35)\nax4.tick_params(labelsize=35)\n\nsns.boxplot(data['X5'], ax = ax5)      ## Boxplot for X5\nax5.set_xlabel('X5', fontsize=35)\nax5.set_ylabel('observation', fontsize=35)\nax5.set_title('X5_overall_height', fontsize=35)\nax5.tick_params(labelsize=35)\n\nsns.boxplot(data['X6'], ax = ax6)     ## Boxplot for X5\nax6.set_xlabel('X6', fontsize=35)\nax6.set_ylabel('Observation', fontsize=35)\nax6.set_title('X6_Orientation', fontsize=35)\nax6.tick_params(labelsize=35)\n\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","acf5fd5b":"fig, (ax1, ax2, ax3,ax4) = plt.subplots(nrows = 1, ncols = 4, figsize = (30, 7))\nsns.boxplot(data['X7'], ax = ax1)     ## Boxplot for X7\nax1.set_xlabel('X7', fontsize=25)\nax1.set_ylabel('observation', fontsize=25)\nax1.set_title('X3_Glazing_Area', fontsize=25)\nax1.tick_params(labelsize=25)\n\nsns.boxplot(data['X8'], ax = ax2)    ## Boxplot for X8\nax2.set_xlabel('X8', fontsize=25)\nax2.set_ylabel('Observation', fontsize=25)\nax2.set_title('X8_GlazingArea_distribution', fontsize=25)\nax2.tick_params(labelsize=25)\n\n## dependent variable\nsns.boxplot(data['Y1'], ax = ax3)     ## Boxplot for Y1\nax3.set_xlabel('Y1', fontsize=25)\nax3.set_ylabel('observation', fontsize=25)\nax3.set_title('Y1_Heating_Load', fontsize=25)\nax3.tick_params(labelsize=25)\n\nsns.boxplot(data['Y2'], ax = ax4)    ## Boxplot for Y2\nax4.set_xlabel('Y2', fontsize=25)\nax4.set_ylabel('Observation', fontsize=25)\nax4.set_title('Y2_Cooling_Load', fontsize=25)\nax4.tick_params(labelsize=25)\n\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","f91c8570":"from scipy.stats import shapiro\n\nprint(\"Shapiro Normality Test\")\nprint(\"\\n\")\nprint(\"Null Hypothesis : Sample data of the variable is from from Normal distribution\")\nprint(\"Alternative Hypothesis : Sample data of the variable is not from from Normal distribution\")\nprint(\"\\n\")\n\n#perform Shapiro-Wilk test\nstatistic, p_value=shapiro(data)\n\nprint(data ,\" Shapiro statistic : %.6f, p value : %.6f\" % (statistic, p_value))\nalpha = 0.05\nif p_value > alpha:\n    print(data,\"variable data\" ,\"is from Normal Distribution (fail to reject H0)\")\nelse:\n    print(data,\"variable data\",\"is not from Normal Distribution (reject H0)\")   ","38f89098":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n# the independent variables set\nX = data.iloc[:, :-2]\n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\n  \nprint(vif_data)","1a2d90ea":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_rescaled = scaler.fit_transform(data)\nscaled_df = pd.DataFrame(data_rescaled)\nscaled_df.head()","bc77a24a":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(scaled_df.iloc[:, :-2], scaled_df.iloc[:, 8:10], \n                            test_size=0.2, random_state=42) ","49f3aaa9":"from sklearn.decomposition import PCA\n\npca = PCA(n_components =7,random_state=42)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","d5057d78":"explained_variance = pca.explained_variance_ratio_\nexplained_variance","66f007d5":"print(\"components:\", pca.components_)\nprint('\\n')\nprint(\"mean:      \", pca.mean_)\nprint('\\n')\nprint(\"covariance:\", pca.get_covariance())","9402ac4d":"#Multivariate Analysis\nfrom sklearn.linear_model import LinearRegression  \n  \nclassifier = LinearRegression()\nfit=classifier.fit(X_train, y_train)    ### model for Y1+Y2-- Multivariate linear regression","a48ddb3b":"y_pred = classifier.predict(X_test)","52422530":"print(\"Training set score:{:.3f}\".format(fit.score(X_train,y_train)))\nprint(\"Test set score:{:.3f}\".format(fit.score(X_test,y_test)))","bd032b72":"from sklearn import metrics  \nprint('Mean Absolute Error:', \n      metrics.mean_absolute_error(y_test, y_pred))","20a443bf":"print('Mean Squared Error:',metrics.mean_squared_error(y_test,y_pred))\nprint('Root Of Mean Squared Error:',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","cc083ed2":"fit1=classifier.fit(X_train, y_train.iloc[:,0])    ### model for Y1['Heating load']--multiple linear regression","a3d2b13a":"y_pred = classifier.predict(X_test)","5d813fee":"print(\"Training set score:{:.3f}\".format(fit1.score(X_train,y_train.iloc[:,0])))\nprint(\"Test set score:{:.3f}\".format(fit1.score(X_test,y_test.iloc[:,0])))","ea206623":"print('Mean Absolute Error for y1:', \n      metrics.mean_absolute_error(y_test.iloc[:,0], y_pred))\nprint('Mean Squared Error for y1:',metrics.mean_squared_error(y_test.iloc[:,0],y_pred))\nprint('Root Of Mean Squared Error for y1:',np.sqrt(metrics.mean_squared_error(y_test.iloc[:,0], y_pred)))","d9dabbd3":"fit2=classifier.fit(X_train, y_train.iloc[:,1])     ### model for Y2[cooling load]--Mutiple linear regression ","371d7aad":"print(\"Training set score:{:.3f}\".format(fit2.score(X_train,y_train.iloc[:,1])))\nprint(\"Test set score:{:.3f}\".format(fit2.score(X_test,y_test.iloc[:,1])))","092e8a37":"print('Mean Absolute Error for y1:', \n      metrics.mean_absolute_error(y_test.iloc[:,0], y_pred))\nprint('Mean Squared Error for y1:',metrics.mean_squared_error(y_test.iloc[:,0],y_pred))\nprint('Root Of Mean Squared Error for y1:',np.sqrt(metrics.mean_squared_error(y_test.iloc[:,0], y_pred)))","953b5054":"import statsmodels.api as sm\nX_trainO = sm.add_constant(X_train)","e33f7564":"model1 = sm.OLS(y_train.iloc[:,0], X_trainO)   ### model for Y1[Heating load]\nresults1 = model1.fit()\nprint(results1.summary())","7dc97cc6":"model2 = sm.OLS(y_train.iloc[:,1], X_trainO)     ### model for Y2[cooling load]\nresults2 = model2.fit()\nprint(results2.summary())","908efd8d":"### model for Y1+Y2-- Multivariate regression\nfrom sklearn import linear_model\nreg  = linear_model.RidgeCV(\n                    alphas = [10, 5, 2, 3, 1, 0.1, 0.3, 0.6, 0.9, 0.01, 0.05],\n                    cv = 5)\nreg.fit(X_train, y_train)\n\nprint(\"Best alpha: {}\\n\".format(reg.alpha_))\n    \nprint(\"R2: %.3f\" % reg.score(X_test, y_test))\n","4b68d5d7":"y_pred = reg.predict(X_test)","bfc39cd8":"from sklearn.metrics import mean_squared_error, r2_score\nprint(\"MSE: %.3f\\n\" % (mean_squared_error(y_test, y_pred))) \nprint(\"RMSE: %.3f\\n\" % np.sqrt(mean_squared_error(y_test, y_pred))) ","b9d1c013":"##model for Y1\nreg  = linear_model.RidgeCV(\n                    alphas = [10, 5, 2, 3, 1, 0.1, 0.3, 0.6, 0.9, 0.01, 0.05],\n                    cv = 5)\nreg.fit(X_train, y_train.iloc[:,0])\n\nprint(\"Best alpha: {}\\n\".format(reg.alpha_))\n    \nprint(\"R2: %.3f\" % reg.score(X_test, y_test.iloc[:,0]))","4c600ad5":"y_pred = reg.predict(X_test)\nprint(\"MSE: %.3f\\n\" % (mean_squared_error(y_test.iloc[:,0], y_pred))) \nprint(\"RMSE: %.3f\\n\" % np.sqrt(mean_squared_error(y_test.iloc[:,0], y_pred))) ","c35e471b":"##model for Y2\nreg  = linear_model.RidgeCV(\n                    alphas = [10, 5, 2, 3, 1, 0.1, 0.3, 0.6, 0.9, 0.01, 0.05],\n                    cv = 5)\nreg.fit(X_train, y_train.iloc[:,1])\n\nprint(\"Best alpha: {}\\n\".format(reg.alpha_))\n    \nprint(\"R2: %.3f\" % reg.score(X_test, y_test.iloc[:,1]))","3ed8bc33":"y_pred = reg.predict(X_test)\nprint(\"MSE: %.3f\\n\" % (mean_squared_error(y_test.iloc[:,1], y_pred))) \nprint(\"RMSE: %.3f\\n\" % np.sqrt(mean_squared_error(y_test.iloc[:,1], y_pred))) ","a4dac182":"from sklearn.svm import SVR\n# for cooling load or model Y2\nreg = SVR(C = 1000)\n    # Train the model using the training sets\nreg.fit(X_train, y_train.iloc[:,1])\n    \n    # Make predictions using the testing set\ny_pred = reg.predict(X_test)\n    \nprint(\"\\nMSE: %.3f\\n\" % mean_squared_error(y_test.iloc[:,1], y_pred))\nprint(\"RMSE: %.3f\\n\" % np.sqrt(mean_squared_error(y_test.iloc[:,1], y_pred)))  \nerror = (1\/y_test.shape[0])*np.sum(np.absolute(y_test.iloc[:,1] - y_pred), axis = 0)\nprint(\"MAE: %.3f\\n\" % error)  \nerror = (100\/y_test.shape[0])*np.sum(np.absolute(y_test.iloc[:,1] - y_pred)\/y_test.iloc[:,1], axis = 0)\nprint(\"MAPE: %.3f\\n\" % error)  \nerror = (100\/y_test.shape[0])*np.sum(np.absolute(y_test.iloc[:,1] - y_pred)\/((y_test.iloc[:,1]+y_pred)\/2), axis = 0)\nprint(\"SMAPE: %.3f\\n\" % error)  \nprint(\"R2: %.3f\\n\" % r2_score(y_test.iloc[:,1],y_pred))","36f800ab":"# For heating load or model Y1\nreg = SVR(C = 1000)\n    # Train the model using the training sets\nreg.fit(X_train, y_train.iloc[:,0])\n    \n    # Make predictions using the testing set\ny_pred = reg.predict(X_test)\n    \nprint(\"\\nMSE: %.3f\\n\" % mean_squared_error(y_test.iloc[:,0], y_pred))\nprint(\"RMSE: %.3f\\n\" % np.sqrt(mean_squared_error(y_test.iloc[:,0], y_pred)))  \nerror = (1\/y_test.shape[0])*np.sum(np.absolute(y_test.iloc[:,0] - y_pred), axis = 0)\nprint(\"MAE: %.3f\\n\" % error)  \nerror = (100\/y_test.shape[0])*np.sum(np.absolute(y_test.iloc[:,0] - y_pred)\/y_test.iloc[:,0], axis = 0)\nprint(\"MAPE: %.3f\\n\" % error)  \nerror = (100\/y_test.shape[0])*np.sum(np.absolute(y_test.iloc[:,0] - y_pred)\/((y_test.iloc[:,0]+y_pred)\/2), axis = 0)\nprint(\"SMAPE: %.3f\\n\" % error)  \nprint(\"R2: %.3f\\n\" % r2_score(y_test.iloc[:,0],y_pred))","3dfd32b0":"1. Explain probability in one sentence for a layman.\n- **Ans:** Probability is the statistical theory of chances of occurance of any event.\n\n2. Explain p-value in your own terms.\n- **Ans:** p-value is the probability value of test statistics[z-test,t-test,chi-square-test,F-test]. p-value helps to take decision about the null hypothesis. if p-value is less then significance level[alpha{0.05,0.01,0.1,etc.}] then we reject the null hypothesis, if it is greater than significance level then we failed to reject the null hypothesis.","b73d2513":"- Included 1st 6 principal components to improve accuracy and also to remove the multicollinearity issues.","a61b80a1":"- boxplot shows **no outliers** are present in any of variables","2f638271":"### OLS Regression","7023cc6e":"**Ridge Regression**","9e6d3aec":"Got the same accuracy for model[Y1+Y2] in Multivariate linear regression and multivariate ridge regression for training set.","d44bb265":"### Multivariate Statistical Analysis\nThe dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). The aim is to use the eight features to predict each of the two responses.\\n\\nSpecifically:\\\n- X1 Relative Compactness-Measure of compactnesss of the closure or building.  \n    - More compact the build less will be the empty area inside which needs to heated or cooled.\n      \n- X2 Surface Area- Surface area of the building.\n     \n      - \n- X3 Wall Area- area of building covered by width of the wall\n- X4 Roof Area - Area cover under roofs.\n\n    - Roof area is the actual area where cooling or heating would be required i.e inside the building.\n      \n- X5 Overall Height- overall height of the building.\n- X6 Orientation- orientation of building based on direction like(North facing,South facing and others)\n- X7 Glazing Area- means the total area of the wall which is glass.\n- X8 Glazing Area Distribution - How glazing area is distributed within the whole building.\n- y1 Heating Load- How much heating load is required to heat the building.\n- y2 Cooling Load- how much load is required to cold the building.\n\n**Relative compactness(X1),Surface Area(X2),Glazing Area(X7),Orientation(X6) these are the factors who can affect energy efficiency.**\n\n-------------------------------------------------------------------------------------------------------------------------------\n\n   - We perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. We simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. \n   \n   \n**Abstract:** This study looked into assessing the heating load and cooling load requirements of buildings (that is, energy efficiency) as a function of building parameters.","b54a9b90":"#### SVR-Support Vector Regression","e87454c1":"### Multivariate Linear Regression Analysis","051a1dea":"- if VIF=1 ; Not correlated   \n- If 1<VIF<5 ; Moderately correlated    \n- if VIF>=5,10; Highly correlated   \n\nso here our vif values are greater than 5, which shows high risk. Multicollinearity problem is there in bertween the independent variables.\n- to solve this multicollinearity problem we will dimension reduction techniques **PCA**.","2920984a":"- Correlation plot shows that there are strong relationship between independent variables and also in between dependent variable. \n- To remove this multicollinearity we will be using **dimension reduction techniques** to solve multicollinearity problem in the data.","c320e422":"### PCA","6b00f467":"- **Difference betwwen training score and testing score are very less or close which means that we avoided overfitting.This Means we have not over-fit the model.**\n- **Here the difference between MAE and RMSE are less ,means that error size are less.our data and model are more representative with respect to mean** ","c56e321f":"**If you learnt something new Upvote the notebook**\n\n# Thank You","9b5e17c9":"### Normality test\n- **Shapiro wilk's test**","07af4967":"SVR--Support Vector Regression model giving the good accuracy from ridge and multivariate linear regression.\n- Accuracy for SVR model-1 Y1 is 93.40% and for SVR model-2 Y2 is 92.5%.\n- Accuracy for Ridge model Y1+Y2 is 90.3%, for Ridge model Y1 is 91.2% and for Ridge model Y2 is 89.3%.\n- Accuracy for Multivariate linear regression model Y1+Y2 is 90.1%, for Multivariate linear regression model Y1 is 91.7%, for Multivariate linear regression model Y2 is 88.6%.\n        \n        \n- *As per the all models accuracy, models for Y1[Heating load] is the most efficient model and in that SVR[Support Vector Regression] model is best model among the all models.*","b358e713":"#### 1. overall model accuarcy is 90.1% for training and also for testing set 90.3% in multivariate linear regression model.\n#### 2. for Y1-overall model accuarcy is 91.70% for training and for testing set 91.20% in multiple linear regression model.\n#### 3. for Y2-overall model accuarcy is 88.60% for training and for testing set 89.30% in multiple linear regression model.\n\n  - As in multivariate model accuracy for training and testing is almost same, so we will consider the **Mulivariate regression model.**","7472ef23":"## EDA","5fe5ffa2":"data is not normal so we will normalize the data by using MinMaxscaler() function.","8ef370d6":"- Either variables are normally distributed or not.it is not clear. but somehow it seems it is normal for more clearity we will perform **Shapiro wilk's test**.","24ad3289":"#### VIF [Variance Inflation Factor]"}}