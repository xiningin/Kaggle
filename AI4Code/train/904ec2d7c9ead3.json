{"cell_type":{"05ddb2d1":"code","ac50d75e":"code","ac08f7f3":"code","33aa5ee9":"code","0565f2eb":"code","b829a0cd":"code","0b6b5303":"code","d903761e":"code","c0f7e9b9":"code","4b3fafe1":"code","e5254059":"code","3c1480ba":"code","ed31b525":"code","652c7ef3":"code","0429eaa3":"code","961b20ef":"code","4d08c2ee":"code","afc91ed3":"code","c9e155ae":"code","a75fa0f8":"code","a8227b8c":"code","f29d6082":"code","e9e520ed":"code","00083835":"code","298820ac":"code","f4156022":"code","f1f15993":"code","4a74eeb3":"code","213e1796":"code","65c41864":"code","fce16d23":"code","347ab69b":"code","932448e4":"code","6d158e8a":"code","8e96ffef":"code","fc9c2122":"code","700ee16f":"code","ee00a605":"code","b9eaeb13":"code","63d74b48":"code","a1d22914":"code","5034fc57":"code","b8cdf70e":"code","58d9994a":"code","b42200a4":"markdown","21205538":"markdown","8bd0a84a":"markdown","0d45ecfe":"markdown","17ee0037":"markdown","034e7f9a":"markdown"},"source":{"05ddb2d1":"import numpy as np \nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nimport scipy\n\nimport os","ac50d75e":"IMG_SIZE = 128","ac08f7f3":"!ls ..\/input\/chest_xray\/chest_xray\/test\/NORMAL | wc -l\n!ls ..\/input\/chest_xray\/chest_xray\/train\/NORMAL | wc -l\n!ls ..\/input\/chest_xray\/chest_xray\/val\/NORMAL | wc -l\n\n!ls ..\/input\/chest_xray\/chest_xray\/test\/PNEUMONIA | wc -l\n!ls ..\/input\/chest_xray\/chest_xray\/train\/PNEUMONIA | wc -l\n!ls ..\/input\/chest_xray\/chest_xray\/val\/PNEUMONIA | wc -l","33aa5ee9":"train_path = \"..\/input\/chest_xray\/chest_xray\/train\/\"\ntest_path  = \"..\/input\/chest_xray\/chest_xray\/test\/\"\nval_path   = \"..\/input\/chest_xray\/chest_xray\/val\/\"\n\ntrain_n = 1341 + 3875\ntest_n  = 234 + 390\nval_n   = 8 + 8","0565f2eb":"def display_img(path):\n    img = cv2.imread(path)\n    \n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n#     plt.axis(\"off\")","b829a0cd":"display_img(\"\/kaggle\/input\/chest_xray\/chest_xray\/train\/PNEUMONIA\/person480_virus_982.jpeg\")","0b6b5303":"%%time\ntrain_x = np.empty((train_n + val_n, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\ntrain_y = np.zeros((train_n + val_n), dtype=np.uint8)\n\ntrain_normal_n = 0\n\ni = 0\nj = train_n + val_n - 1\nfor dirname, _, filenames in os.walk(train_path):\n    for filename in tqdm(filenames):\n        if \"DS\" in filename:\n            continue\n        img = cv2.imread(os.path.join(dirname, filename))\n        if \"NORMAL\" in dirname:\n            train_x[i] = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n            train_y[i] = 0\n            i += 1\n        else:\n            train_x[j] = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n            train_y[j] = 1\n            j -= 1\n#         i += 1\n        \nfor dirname, _, filenames in os.walk(val_path):\n    for filename in tqdm(filenames):\n        if \"DS\" in filename:\n            continue\n        img = cv2.imread(os.path.join(dirname, filename))\n        if \"NORMAL\" in dirname:\n            train_x[i] = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n            train_y[i] = 0\n            i += 1\n        else:\n            train_x[j] = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n            train_y[j] = 1\n            j -= 1\n#         i += 1\n\ntrain_normal_n = i","d903761e":"%%time\ntest_x = np.empty((test_n, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\ntest_y = np.zeros((test_n), dtype=np.uint8)\n\ni = 0\nfor dirname, _, filenames in os.walk(test_path):\n    for filename in tqdm(filenames):\n        if \"DS\" in filename:\n            continue\n        img = cv2.imread(os.path.join(dirname, filename))\n        test_x[i] = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n        if \"NORMAL\" in dirname:\n            test_y[i] = 0\n        else:\n            test_y[i] = 1\n        i += 1","c0f7e9b9":"# train_pneumonia = np.sum(train_y)\n# train_normal    = len(train_y) - train_pneumonia\n\n# train_pneumonia = int(np.min([1.5 * np.min([train_normal, train_pneumonia]), train_pneumonia]))\n# train_normal    = int(np.min([1.5 * np.min([train_normal, train_pneumonia]), train_normal]))\n\n# print(train_normal, train_pneumonia)","4b3fafe1":"val_x = np.empty((600, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\nval_y = np.zeros((600), dtype=np.uint8)\nIMG_SIZE\nval_x[:300] = train_x[:300]\nval_y[:300] = train_y[:300]\n\nval_x[300:] = train_x[-300:]\nval_y[300:] = train_y[-300:]\n\ntrain_x = train_x[300:-300]\ntrain_y = train_y[300:-300]","e5254059":"train_x = train_x[:-1000]\ntrain_y = train_y[:-1000]","3c1480ba":"# pneumonia_begin = np.argmax(train_y)\n\n# indexes = np.random.permutation(pneumonia_begin)\n# np.random.shuffle(indexes)\n# indexes = indexes[:train_normal]\n\n# train_norm_x = train_x[indexes]\n# train_norm_y = train_y[indexes]","ed31b525":"# # train_x = train_x[pneumonia_begin:]\n# # train_y = train_y[pneumonia_begin:]\n\n# indexes = np.random.permutation(len(train_x) - len(train_norm_x))\n# np.random.shuffle(indexes)\n# indexes = indexes[:train_pneumonia]\n\n# train_pn_x = train_x[indexes + np.argmax(train_y)]\n# train_pn_y = train_y[indexes + np.argmax(train_y)]\n\n# print(np.sum(train_pn_y), len(train_pn_y))","652c7ef3":"# train_x = np.concatenate([train_norm_x, train_pn_x])\n# train_y = np.concatenate([train_norm_y, train_pn_y])","0429eaa3":"# indexes = np.random.permutation(len(train_x))\n# train_x = train_x[indexes]\n# train_y = train_y[indexes]\n\n# val_beg = int(0.7 * len(train_x))\n# val_x = train_x[val_beg:]\n# val_y = train_y[val_beg:]\n# train_x = train_x[:val_beg]\n# train_y = train_y[:val_beg]","961b20ef":"# train_x = train_x.reshape((-1, 224, 224, 1))\n# val_x = val_x.reshape((-1, 224, 224, 1))\n# test_x = test_x.reshape((-1, 224, 224, 1))\n\ntrain_y = train_y.reshape((-1, 1))\nval_y = val_y.reshape((-1, 1))\ntest_y = test_y.reshape((-1, 1))","4d08c2ee":"print(np.sum(train_y), \"of pneumonias and\", int(len(train_y) - np.sum(train_y)), \"of normals in train set\")\nprint(np.sum(val_y), \"of pneumonias and\", int(len(val_y) - np.sum(val_y)), \"of normals in validation set\")\nprint(np.sum(test_y), \"of pneumonias and\", int(len(test_y) - np.sum(test_y)), \"of normals in test set\")","afc91ed3":"train_x = train_x.astype(np.float32)\ntrain_x \/= np.max(train_x)\n\nval_x = val_x.astype(np.float32)\nval_x \/= np.max(val_x)\n\ntest_x = test_x.astype(np.float32)\ntest_x \/= np.max(test_x)","c9e155ae":"def to_multilabel(x):\n    res = np.empty((len(x), 2))\n    for i in range(len(x)):\n        if x[i][0] == 0:\n            res[i] = [1, 0]\n        else:\n            res[i] = [1, 1]\n    return res","a75fa0f8":"train_y = to_multilabel(train_y)\nval_y = to_multilabel(val_y)\ntest_y = to_multilabel(test_y)","a8227b8c":"# indexes_norm = np.arange(train_normal_n)\n# indexes_perm = np.random.permutation(train_normal_n)\n\n# dim = np.concatenate([[train_normal_n] ,train_x.shape[1:]]).ravel()\n\n# train_ext_x = 0.5 * train_x[indexes_norm] + 0.5 * train_x[indexes_perm] + (np.random.rand(dim[0], dim[1], dim[2], dim[3]) * 0.1 - 0.05)\n# train_ext_x = (train_ext_x - np.min(train_ext_x)) \/ (np.max(train_ext_x) - np.min(train_ext_x))\n\n# train_ext_y = 0.5 * train_y[indexes_norm] + 0.5 * train_y[indexes_perm]\n\n# del indexes_norm\n# del indexes_perm","f29d6082":"# train_x = np.concatenate([train_ext_x, train_x])\n# train_y = np.concatenate([train_ext_y, train_y])\n\n# del train_ext_x\n# del train_ext_y","e9e520ed":"# train_y = train_y.astype(np.uint8)\n# val_y   = val_y.astype(np.uint8)\n# test_y  = test_y.astype(np.uint8)","00083835":"# print(train_x.dtype, train_x.shape, np.min(train_x), np.max(train_x))\n# print(val_x.dtype, val_x.shape, np.min(val_x), np.max(val_x))\n# print(test_x.dtype, test_x.shape, np.min(test_x), np.max(test_x))\n\n# print(train_y.dtype, train_y.shape)\n# print(val_y.dtype, val_y.shape)\n# print(test_y.dtype, test_y.shape)","298820ac":"# print(np.sum(train_y, axis=0)[1], \"of pneumonias and\", np.sum(train_y, axis=0)[0] - np.sum(train_y, axis=0)[1], \"of normals in train set\")\n# print(np.sum(val_y, axis=0)[1], \"of pneumonias and\", np.sum(val_y, axis=0)[0] - np.sum(val_y, axis=0)[1], \"of normals in validation set\")\n# print(np.sum(test_y, axis=0)[1], \"of pneumonias and\", np.sum(test_y, axis=0)[0] - np.sum(test_y, axis=0)[1], \"of normals in test set\")","f4156022":"fig, ax = plt.subplots(5, 5, figsize=(25, 25))\n\nindexes = np.random.randint(0, len(train_x), 25)\nfor i in range(5):\n    for j in range(5):\n        ax[i][j].imshow(train_x[indexes[i * 5 + j]])\n        ax[i][j].set_title(f\"{indexes[i * 5 + j]} - {train_y[indexes[i * 5 + j]]}\")\n        ax[i][j].axis(\"off\")","f1f15993":"fig, ax = plt.subplots(5, 5, figsize=(25, 25))\n\nindexes = np.random.randint(0, len(val_x), 25)\nfor i in range(5):\n    for j in range(5):\n        ax[i][j].imshow(val_x[indexes[i * 5 + j]])\n        ax[i][j].set_title(f\"{indexes[i * 5 + j]} - {val_y[indexes[i * 5 + j]]}\")\n        ax[i][j].axis(\"off\")","4a74eeb3":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, SeparableConv2D, MaxPooling2D, Dropout, BatchNormalization, Input, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.densenet import DenseNet201\nfrom tensorflow.keras.utils import Sequence\n\nimport imgaug as ia\nimport imgaug.augmenters as iaa","213e1796":"class Generator(Sequence):\n    def __init__(self, train_x, train_y, batch_size=32, shuffle=True, mixup=True, augment=True):\n        self.train_x = train_x\n        self.train_y = train_y\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.mixup_ = mixup\n        self.augment = augment\n        \n        if augment:\n            sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n            self.seq = iaa.Sequential([\n                sometimes(iaa.CropAndPad(\n                    percent=(-0.05, 0.1),\n                    pad_mode=ia.ALL,\n                    pad_cval=(0, 255)\n                )),\n                sometimes(iaa.Affine(\n                    scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n    #                 translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n                    rotate=(-20, 20), # rotate by -20 to +20 degrees\n    #                 shear=(-10, 10), # shear by -16 to +16 degrees\n                    order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n                    cval=(0, 0), # if mode is constant, use a cval between 0 and 255\n                    mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n                )),\n                iaa.Fliplr(0.5),\n                iaa.GaussianBlur(sigma=(0, 3.0))\n            ])\n        \n        self.on_epoch_end()\n        \n    def __len__(self):\n        return int(np.floor(len(self.train_x) \/ self.batch_size))\n    \n    def __getitem__(self, index):\n        x = self.train_x[index * self.batch_size : (index + 1) * self.batch_size]\n        if self.augment:\n            x = (x * 255).astype(np.uint8)\n            x = self.seq.augment_images(x)\n        x = x.astype(np.float32) \/ np.max(x)\n        y = self.train_y[index * self.batch_size : (index + 1) * self.batch_size]\n        \n        if self.mixup_:\n            x, y = self.mixup(x, y)\n        return x, y\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            indexes = np.random.permutation(len(self.train_x)).astype(np.uint16)\n            self.train_x = self.train_x[indexes]\n            self.train_y = self.train_y[indexes]\n        \n    def mixup(self, x, y):\n        alphas = np.random.ranf(len(x))\n        x_alphas = alphas.reshape((-1, 1, 1, 1))\n        y_alphas = alphas.reshape((-1, 1))\n        indexes = np.random.permutation(len(x))\n        np.random.shuffle(indexes)\n        x = x_alphas * x[indexes] + (1.0 - x_alphas) * x\n        y = y_alphas * y[indexes] + (1.0 - y_alphas) * y\n        return x, y\n        \n    def get_batch(self, index):\n        return self.__getitem__(index)","65c41864":"BATCH_SIZE = 32\nEPOCHS = 50","fce16d23":"gen = Generator(train_x, train_y, batch_size=BATCH_SIZE, mixup=True, augment=True)","347ab69b":"imgs, labels = gen.get_batch(0)\nprint(imgs.dtype, imgs.shape, np.min(imgs), np.max(imgs))\n\nfig, ax = plt.subplots(5, 5, figsize=(25, 25))\nfig.suptitle(\"Augmentations\", fontsize=36)\nfor i in range(5):\n    for j in range(5):\n        ax[i][j].imshow(imgs[i * 5 + j])\n        ax[i][j].set_title(labels[i * 5 + j])\n        ax[i][j].axis(\"off\")","932448e4":"class FScore(tf.keras.callbacks.Callback):\n    def __init__(self, val_x, val_y):\n        self.val_x = val_x\n        self.val_y = val_y\n        \n        self.precisions = []\n        self.recalls = []\n        self.fscores = []\n    \n    def on_train_begin(self, logs={}):\n        pass\n\n    def on_epoch_end(self, epoch, logs={}):\n        precision, recall, fscore = self.score()\n        if np.isnan(precision):\n            precision = 0\n        if np.isnan(recall):\n            recall = 0\n        if np.isnan(fscore):\n            fscore = 0\n        \n        self.precisions.append(precision)\n        self.recalls.append(recall)\n        self.fscores.append(fscore)\n        \n        print(f\"precision: {precision}, recall: {recall}, fscore: {fscore}\")\n        \n        if np.argmax(np.array(self.fscores)) == len(self.fscores) - 1:\n            print(f\"Best fscore to date. Saving the model in '.\/weights_fscore.hdf5'\")\n            model.save(\"weights_fscore.hdf5\")\n            \n            \n    def score(self, x=[], y=[], threshold=0.5):\n        if len(x) == 0:\n            x = self.val_x\n        if len(y) == 0:\n            y = self.val_y\n            \n        y = (np.sum(y, axis=1) - 1).reshape((-1, 1))\n        y = y.clip(0, 1)\n        y = y.astype(np.uint8)\n        \n        y_pred = self.model.predict(x)\n        y_pred[y_pred < threshold] = 0\n        y_pred[y_pred != 0]        = 1\n        y_pred = (np.sum(y_pred, axis=1) - 1).reshape((-1, 1))\n        y_pred = y_pred.clip(0, 1)\n        y_pred = y_pred.astype(np.uint8)\n        \n        cm = confusion_matrix(y, y_pred)\n        tn, fp, fn, tp = cm.ravel()\n\n        precision = tp \/ (tp + fp)\n        recall    = tp \/ (tp + fn)\n        fscore    = 2 * (precision * recall) \/ (precision + recall)\n        return precision, recall, fscore\n            \n        \n    def get_scores(self):\n        return (self.precisions, self.recalls, self.fscores)","6d158e8a":"# model = Sequential()\n\n# model.add(Input(batch_shape=(None, 224, 224, 3)))\n# model.add(BatchNormalization())\n\n# model.add(Conv2D(64, activation='relu', kernel_size=(5, 5)))\n# model.add(Conv2D(64, activation='relu', kernel_size=(5, 5)))\n# model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n# model.add(Dropout(0.5))\n# model.add(BatchNormalization())\n\nbase_model = Xception(include_top=False, weights='imagenet',\n                      input_shape=(IMG_SIZE, IMG_SIZE, 3), \n                      pooling=None)\n\nmodel = Sequential()\nmodel.add(base_model)\n\nmodel.add(GlobalAveragePooling2D())\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(2, activation='sigmoid'))\n\nopt = Adam(learning_rate=0.00005)\nmodel.compile(optimizer=opt, loss='mean_squared_error')","8e96ffef":"model.summary()","fc9c2122":"es = EarlyStopping(monitor='val_loss', \n                   patience=10, \n                   verbose=1, \n                   mode='min')\nmc = ModelCheckpoint(\"weights_loss.hdf5\", \n                     monitor=\"val_loss\",\n                     save_weights_only=True,\n                     save_best_only=True, \n                     verbose=1)\nrl = ReduceLROnPlateau(monitor='val_loss', \n                       factor=0.5, \n                       patience=5, \n                       verbose=1, \n                       mode='min', \n                       cooldown=1)\ncl = CSVLogger(\"training.log\")\nfs = FScore(val_x, val_y)\n\nhistory = model.fit_generator(gen, validation_data=(val_x, val_y), use_multiprocessing=False,\n                              epochs=50, callbacks=[es, mc, rl, cl, fs])","700ee16f":"plt.plot(history.history['val_loss'])\nplt.plot(history.history['loss'])\nplt.legend([\"val_loss\", \"loss\"])","ee00a605":"precisions, recalls, fscores = fs.get_scores()","b9eaeb13":"plt.plot(precisions)\n# plt.plot(history.history['precision'])\n# plt.legend([\"val_precision\", \"precision\"])","63d74b48":"plt.plot(recalls)\n# plt.plot(history.history['recall'])\n# plt.legend([\"val_recall\", \"recall\"])","a1d22914":"plt.plot(fscores)\n# plt.plot(history.history['fscore'])\n# plt.legend([\"val_fscore\", \"fscore\"])","5034fc57":"def compute_score_inv(threshold):\n    _, _, score = fs.score(val_x, val_y, threshold)\n    return 1 - score\n\nsimplex = scipy.optimize.minimize(\n    compute_score_inv, 0.5#, method='nelder-mead'\n)\n\nbest_threshold = simplex['x'][0]\n\nprint(best_threshold)","b8cdf70e":"for dirname, _, filenames in os.walk('.'):\n    for filename in filenames:\n        if \".hdf5\" in filename:\n            try:\n                model.load_weights(filename)\n                pred = model.predict(val_x)\n                pred[pred > 0.5] = 1\n                pred[pred != 1] = 0\n                pred = (np.sum(pred, axis=1) - 1).reshape((-1, 1))\n                pred = pred.clip(0, 1)\n                cm = confusion_matrix((np.sum(val_y, axis=1)).reshape((-1, 1)) - 1, pred)\n                plt.figure()\n                plot_confusion_matrix(cm,figsize=(6,4), hide_ticks=True, cmap=plt.cm.Blues)\n                plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n                plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n                plt.show()\n\n\n                tn, fp, fn, tp = cm.ravel()\n\n                precision = tp \/ (tp + fp)\n                recall    = tp \/ (tp + fn)\n                fscore    = 2 * precision * recall \/ (precision + recall)\n\n                print(filename)\n                print(\"precision: \" + str(precision))\n                print(\"recall   : \" + str(recall))\n                print(\"fscore   : \" + str(fscore))\n            except:\n                pass","58d9994a":"for dirname, _, filenames in os.walk('.'):\n    for filename in filenames:\n        if \".hdf5\" in filename:\n            try:\n                model.load_weights(filename)\n                pred = model.predict(test_x)\n                pred[pred > 0.5] = 1\n                pred[pred != 1] = 0\n                pred = (np.sum(pred, axis=1) - 1).reshape((-1, 1))\n                pred = pred.clip(0, 1)\n                cm = confusion_matrix((np.sum(test_y, axis=1)).reshape((-1, 1)) - 1, pred)\n                plt.figure()\n                plot_confusion_matrix(cm,figsize=(6,4), hide_ticks=True, cmap=plt.cm.Blues)\n                plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n                plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n                plt.show()\n\n\n                tn, fp, fn, tp = cm.ravel()\n\n                precision = tp \/ (tp + fp)\n                recall    = tp \/ (tp + fn)\n                fscore    = 2 * precision * recall \/ (precision + recall)\n\n                print(filename)\n                print(\"precision: \" + str(precision))\n                print(\"recall   : \" + str(recall))\n                print(\"fscore   : \" + str(fscore))\n            except:\n                pass","b42200a4":"# Model","21205538":"## Shuffling the data and validation split","8bd0a84a":"# Data Generator ","0d45ecfe":"## Multilabeling","17ee0037":"# Data","034e7f9a":"## Oversampling"}}