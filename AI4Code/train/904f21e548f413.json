{"cell_type":{"9b758220":"code","5e091999":"code","0935aa62":"code","cdfabaa7":"code","191796ff":"code","e00b623c":"code","5e1bafb5":"code","95daa42e":"code","ccef9643":"code","9c277910":"code","6f34e8e9":"code","0bf2d96c":"code","1cdfdcd9":"code","c7c97619":"code","53f82f0f":"code","ae9f4e5a":"code","e4a6d35d":"code","9bfe83ed":"code","d571497b":"code","f35a075f":"code","7acc9704":"code","daca6537":"code","37217b3e":"code","75d37136":"code","d469ef65":"code","87e9ee7f":"code","08dfa62c":"code","88d6fc96":"code","b333c7de":"code","927c628b":"code","da9a8f5a":"code","0f394f4f":"code","54558ee5":"code","925f59c0":"code","1025bd12":"code","43791964":"code","f1d88fb0":"code","832abaa1":"code","b57f6804":"markdown","f5b457d0":"markdown","ea6587cf":"markdown","d7304300":"markdown","28ed0ce6":"markdown","b03fe2e2":"markdown","b283726a":"markdown","ddea9bf0":"markdown","ee04b3d0":"markdown","30af7e1a":"markdown","dc39edcb":"markdown","4f1ae36b":"markdown","fb0fe278":"markdown","8f019e21":"markdown","c4d3884d":"markdown","43222b93":"markdown","4c3ba5d3":"markdown","d3d26d1f":"markdown","26e35e89":"markdown","3ae604fa":"markdown","0cbd9bcd":"markdown","49d58a5b":"markdown","da78f3dc":"markdown","79200f52":"markdown","13fade7d":"markdown","6d4518eb":"markdown","c8dbfc15":"markdown","a24c650c":"markdown","79565d1f":"markdown"},"source":{"9b758220":"#Config\n%config IPCompleter.greedy=True #IPython autocomplete ON\n\n#Libraries: data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport graphviz\n\n#Libraries: visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n#Libraries: machine learning\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom subprocess import call\n\n#from sklearn.linear_model import LogisticRegression\n#from sklearn.svm import SVC, LinearSVC\n#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.neighbors import KNeighborsClassifier\n#from sklearn.naive_bayes import GaussianNB\n#from sklearn.linear_model import Perceptron\n#from sklearn.linear_model import SGDClassifier\n#from sklearn.model_selection import train_test_split\n","5e091999":"#Paths\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nmain_path = '\/kaggle\/input\/titanic\/'\ntrain_file = 'train.csv'\ntest_file = 'test.csv'\n\n#Dataframe creations\ndf_train = pd.read_csv(main_path + train_file)\ndf_test = pd.read_csv(main_path + test_file)","0935aa62":"#Explore the dimension\ndf_train.shape","cdfabaa7":"#Explore features and lines\ndf_train.head(10)\n#df_train.columns.values","191796ff":"#Explore features\ndf_train.info()\n#Select null values\n#df_train[df_train['Age'].isnull()]\n","e00b623c":"#Explore categorical features\ndf_train.describe(include = 'object')","5e1bafb5":"#Explore numerical features\ndf_train.describe()","95daa42e":"#Select some data to have a better knowledge of the dataset. Examples:\n#a)how is the data of the youngsters? \n#b)is the data from 'Cabin' useless?\n#df_train.loc[df_train['Age']<20]\n#df_train.loc[df_train['Cabin'].notnull()]","ccef9643":"df_train2 = df_train\ndf_test2 = df_test","9c277910":"df_train2['_FamilySize'] = df_train2['SibSp'] + df_train2['Parch'] + 1\ndf_test2['_FamilySize'] = df_test2['SibSp'] + df_test2['Parch'] + 1","6f34e8e9":"df_train2['_Title'] = df_train2['Name'].str.extract('([A-Za-z]+)\\.')\ndf_test2['_Title'] = df_test2['Name'].str.extract('([A-Za-z]+)\\.')","0bf2d96c":"df_train2['_Cabin'] = pd.isnull(df_train2['Cabin'])\ndf_test2['_Cabin'] = pd.isnull(df_test2['Cabin'])","1cdfdcd9":"df_train2['_Age'] = df_train2.groupby(['_Title'])['Age'].apply(lambda x: x.fillna(x.median()))\ndf_test2['_Age'] = df_train2.groupby(['_Title'])['Age'].apply(lambda x: x.fillna(x.median()))","c7c97619":"#Groups: 0,5,30,100\ndf_train2['_Age2'] = pd.cut(df_train2['_Age'], (0,5,30,100), labels=False )\ndf_test2['_Age2'] = pd.cut(df_test2['_Age'], (0,5,30,100), labels=False )\ndf_train2.head()","53f82f0f":"df_train2['_FamilySize'] = df_train2['_FamilySize'].replace([4, 5,6,7,8,9,10,11], 4 )\ndf_test2['_FamilySize'] = df_test2['_FamilySize'].replace([4, 5,6,7,8,9,10,11], 4 )\n","ae9f4e5a":"sns.countplot(x='Embarked',data=df_train2,palette='Set2')\nplt.show()","e4a6d35d":"freq_port = df_train2.Embarked.dropna().mode()[0]\ndf_train2['Embarked'] = df_train2['Embarked'].fillna(freq_port)\ndf_test2['Embarked'] = df_test2['Embarked'].fillna(freq_port)","9bfe83ed":"df_train2_sex = pd.get_dummies(df_train2['Sex'])\ndf_test2_sex = pd.get_dummies(df_test2['Sex'])\n\ndf_train2 = pd.concat([df_train2, df_train2_sex], axis=1)\ndf_test2 = pd.concat([df_test2, df_test2_sex], axis=1)\ndf_train2","d571497b":"#df_train.loc[(df_train['Sex']=='male') & (df_train['Age']<15)]","f35a075f":"#Create groups for titles\ndf_train2['_Title'] = df_train2['_Title'].replace(['Capt', 'Countess','Dr','Rev','Major','Col','Jonkheer','Mme'], 1)# Titles with distinction or rares\ndf_train2['_Title'] = df_train2['_Title'].replace(['Master','Mr','Sir','Don'], 2)# Males\ndf_train2['_Title'] = df_train2['_Title'].replace(['Mrs'], 3)# Married females\ndf_train2['_Title'] = df_train2['_Title'].replace(['Miss','Mlle','Ms','Dona','Lady'], 4)# Unmarried females\ndf_test2['_Title'] = df_test2['_Title'].replace(['Capt', 'Countess','Dr','Rev','Major','Col','Jonkheer','Mme'], 1)# Titles with distinction or rares\ndf_test2['_Title'] = df_test2['_Title'].replace(['Master','Mr','Sir','Don'], 2)# Males\ndf_test2['_Title'] = df_test2['_Title'].replace(['Mrs'], 3)# Married females\ndf_test2['_Title'] = df_test2['_Title'].replace(['Miss','Mlle','Ms','Dona','Lady'], 4)# Unmarried females\n\n","7acc9704":"df_train2['Embarked'] = df_train2['Embarked'].replace(['S'], 1)\ndf_train2['Embarked'] = df_train2['Embarked'].replace(['C'], 2)\ndf_train2['Embarked'] = df_train2['Embarked'].replace(['Q'], 3)\ndf_test2['Embarked'] = df_test2['Embarked'].replace(['S'], 1)\ndf_test2['Embarked'] = df_test2['Embarked'].replace(['C'], 2)\ndf_test2['Embarked'] = df_test2['Embarked'].replace(['Q'], 3)","daca6537":"df_train2 = df_train2.drop(['SibSp','Age','_Age','Parch','Ticket','Fare','Name','Cabin','Sex','female'],axis=1)\ndf_test2 = df_test2.drop(['SibSp','Age','_Age','Parch','Ticket','Fare','Name','Cabin','Sex','female'],axis=1)","37217b3e":"df_train2.info()\n#df_test2.info()","75d37136":"df_train2.head()\n#df_test2.head()","d469ef65":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation', y=1, size=15)\nsns.heatmap(df_train2.astype(float).corr(),linewidths=0.1,vmax=1.0, cmap=colormap, linecolor='white', annot=True)","87e9ee7f":"df_train2.corr()['Survived']","08dfa62c":"group_Sex = df_train2.groupby('male').agg({'PassengerId': 'count','Survived': 'mean'}) \ngroup_Sex \n#Result: high probability (74%) that woman survive","88d6fc96":"group_Pclass = df_train2.groupby('Pclass').agg({'PassengerId': 'count','Survived': 'mean'}) \ngroup_Pclass\n#Result: high probability (76%) that passengers in class 3 does not survive","b333c7de":"group_FamilySize = df_train2.groupby('_FamilySize').agg({'PassengerId': 'count', 'Survived': 'mean'})\ngroup_FamilySize\n#Result: very high probability (94%) that passengers with big family (+3) does not survive and high probability (70%) that passengers travelling alone does not survive","927c628b":"group_cabin = df_train2.groupby('_Cabin').agg({'PassengerId': 'count', 'Survived': 'mean'})\ngroup_cabin\n#Result: Cabin does not help to survive (only 29%)","da9a8f5a":"group_embarked = df_train2.groupby('Embarked').agg({'PassengerId': 'count', 'Survived': 'mean'})\ngroup_embarked\n#Result: Passengers embarked in C have more possibilities to survive","0f394f4f":"df_train2['_Age2'].hist(by=df_train['Survived'])\n#Results: for age, distributions are similar except for groups lower than 15 and greater than 30 where it seems that they have more posibilities to survive","54558ee5":"group_title = df_train2.groupby('_Title').agg({'PassengerId': 'count', 'Survived': 'mean'})\ngroup_title\n#Result: reverends do not survive! and there is no big difference between married\/unmarried women","925f59c0":"df_train2.head()","1025bd12":"X_train = df_train2.drop(['PassengerId','Survived','male','_Cabin'],axis=1)\nY_train = df_train2['Survived']\nX_test = df_test2.drop(['PassengerId','male','_Cabin'],axis=1)","43791964":"DecisionTree = DecisionTreeClassifier(criterion='entropy', splitter='best',max_depth=4, min_samples_leaf=10)\nDecisionTree.fit(X_train, Y_train)\nY_pred = DecisionTree.predict(X_test)\nResultDecisionTree = DecisionTree.score(X_train, Y_train)\nResultDecisionTree","f1d88fb0":"#Let's see our tree\ntree1_view = tree.export_graphviz(DecisionTree, out_file=None, feature_names = X_train.columns.values, rotate=True, class_names = ['Died', 'Survived']) \ntree1viz = graphviz.Source(tree1_view)\ntree1viz","832abaa1":"submission = pd.DataFrame({\n        \"PassengerId\": df_test2[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic2.csv', index=False)\n","b57f6804":"## Stage 3\/6 Explore and prepare the data","f5b457d0":"#### Age by blocks instead of numbers","ea6587cf":"### Family Size. Create a new block for +3","d7304300":"#### Create Cabin as boolean variable","28ed0ce6":"## Stage 1\/6 Question\nOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.\n\nOne of the reasons that the shipwreck led to such loss of life was:\n- there were not enough lifeboats for the passengers and crew\n- some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\nSpecifically,\n- to predict if a passenger survived the sinking of the Titanic or not. For each in the test set, you must predict a 0 or 1 value for the variable. Your score is the percentage of passengers you correctly predict. This is known simply as \"accuracy\u201d.","b03fe2e2":"#### Create a new feature FamilySize with the number of family members (SibSp + Parch)","b283726a":"#### Score with my Decision Tree: 0.75598\n#### Conclussion: It seems than just predicting that males die and females live scores 76.6%... so not sure about my model :(. Any suggestion anyone? ","ddea9bf0":"#### Complete embarked","ee04b3d0":"#### Coorrelation with 1 feature","30af7e1a":"#### Pearson Correlation Heatmap\nLet's see how features are related","dc39edcb":"#### Create new datasets for transformation","4f1ae36b":"## Stage 6\/6. Supply or submit the results.","fb0fe278":"# Titanic Competition in Kaggle\n\nThis is my first notebook to learn the basics for Machine Learning in Python.\n\nI will follow the following stages:\n1. Question.\n2. Acquiere data.\n3. Explore and prepare the data (iterative phase).\n - A: Meet the data \n - B: Transform the data (prepare, clean, wrangle and feature engineering)\n - C: Identify patterns\n4. Model, predict and solve the problem (iterative phase).\n5. Visualize, report, and present the problem solving steps and final solution.\n6. Supply or submit the results.\n\nAfter this exercise, I would like to go deeper in:\n - Python: lambas functions, Manage strings, other algorithms apart of Decision Tree","8f019e21":"#### Transform sex to dummies","c4d3884d":"#### Remove: Name, Age and Age_, SibSp, Parch, Ticket, Fare, Cabin, female, Sex","43222b93":"#### Correlation","4c3ba5d3":"#### Fill NaN Values in Age based on the median of the age of their title","d3d26d1f":"#### Decision Tree\n\nDecision Tree is a basic but common and useful classification algorithm. It is also the basis of slightly more advanced classification techniques such as Random Forest. The Titanic data set, comprised of both categorical and numerical features, is a great use-case for tree-based algorithms.","26e35e89":"## Stage 4\/6 Model, predict and solve the problem\n","3ae604fa":"#### Transform Ports(Embarked) in numbers (embarked order: 1. Southampton, 2. C, 3. Quens.)","0cbd9bcd":"#### Data set preparation: Training and test sets","49d58a5b":"#### Extract Title from Name","da78f3dc":"## Stage 2\/6 Acquiere data","79200f52":"#### Transform Titles to numbers","13fade7d":"Not many features are related:\n- Cabin is very related to the Pclass (>70). High class people are travelling in cabins. Let's keep both features by now.\n- Title is very related to Sex. I think Title is a mix between the following features: Sex, Pclass and Age. Pclass is adding vale to the model (reverends die) and Age too. Therefore, I will remove Sex to the model (how crazy I am)","6d4518eb":"### C. Identify patterns","c8dbfc15":"### A. Meet the data","a24c650c":"## Stage 5\/6. Visualize, report, and present the problem solving steps and final solution.","79565d1f":"### B. Transform the data (prepare, clean, wrangle and feature engineering)"}}