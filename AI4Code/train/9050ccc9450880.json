{"cell_type":{"a2d032f6":"code","55c3a7e8":"code","8bb1691a":"code","65059d6f":"code","57e30663":"code","c939f333":"code","629a7b41":"code","cc50831d":"code","c2db4334":"code","c71deba9":"code","f8d1580d":"code","9603691a":"code","3e5c3741":"code","f2da7653":"code","33ebfa83":"code","9ba118dc":"code","eba1abb6":"code","df83182a":"code","9ca0cc9d":"code","2997f47d":"code","65fc9ad0":"code","71b64e98":"code","9770b76a":"code","669cc573":"code","c491229f":"code","d74b8a47":"code","25c5e235":"code","594b7dc1":"code","f2f0072e":"code","05294d0d":"code","632ce191":"code","cee7a4be":"code","804056b9":"code","b6739282":"code","581420f9":"code","a7e4929d":"code","208df850":"code","f40584ba":"code","f484b748":"code","7dafa340":"code","0219e156":"code","102c4cf8":"code","61a4837b":"code","55d0cc83":"code","50f4676c":"code","2fff6a5f":"code","9b2c6fc6":"code","0504c103":"code","7a363d72":"code","af4ed494":"code","e912e521":"code","3cd180ae":"code","b6442bc7":"code","0c1906cb":"code","239f0513":"code","926520db":"code","5a2bf1a1":"code","10b2595a":"markdown","f4492260":"markdown","98728fd8":"markdown","61bd99a1":"markdown","87190e82":"markdown","8146da28":"markdown","76f0d957":"markdown","59a98fb8":"markdown","e8ac7d6d":"markdown","5088cf04":"markdown","81083786":"markdown","6e1ed005":"markdown","c968a961":"markdown","2085511d":"markdown","e75e4db2":"markdown","b81670d8":"markdown","037b3d3c":"markdown","2635d89e":"markdown","d7d09db5":"markdown","5d30e2c0":"markdown","064b18c9":"markdown","b2cb0393":"markdown","0c25d6fd":"markdown","9456f331":"markdown","98b5ec2a":"markdown"},"source":{"a2d032f6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","55c3a7e8":"from numpy import loadtxt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nimport pandas as pd \nfrom sklearn.metrics import f1_score","8bb1691a":"dataset = pd.read_csv('\/kaggle\/input\/creditscreening\/credit-screening.data')\n\ndataset.head()","65059d6f":"colNames = []\n\nfor i in range(15):\n    x = \"A\" + str(i+1)\n    colNames.append(x)\n\ncolNames.append('class')\ndataset.columns = colNames\ndataset.tail()","57e30663":"dataset.isna().sum()","c939f333":"dataset.dtypes","629a7b41":"dataset.replace('?', np.nan, inplace=True)\ndataset.isna().sum()","cc50831d":"dataset = dataset.fillna(method ='pad')\ndataset.isna().sum()","c2db4334":"dataset['A14'] = dataset['A14'].astype('int64')\ndataset['A2'] = dataset['A2'].astype('float64')\ndataset.dtypes","c71deba9":"dataset['A1'] = dataset['A1'].astype('category')\ndataset['A4'] = dataset['A4'].astype('category')\ndataset['A5'] = dataset['A5'].astype('category')\ndataset['A6'] = dataset['A6'].astype('category')\ndataset['A7'] = dataset['A7'].astype('category')\ndataset['A9'] = dataset['A9'].astype('category')\ndataset['A10'] = dataset['A10'].astype('category')\ndataset['A12'] = dataset['A12'].astype('category')\ndataset['A13'] = dataset['A13'].astype('category')\nprint(dataset.info())","f8d1580d":"dataset['A1'] = dataset['A1'].cat.codes\ndataset['A4'] = dataset['A4'].cat.codes\ndataset['A5'] = dataset['A5'].cat.codes\ndataset['A6'] = dataset['A6'].cat.codes\ndataset['A7'] = dataset['A7'].cat.codes\ndataset['A9'] = dataset['A9'].cat.codes\ndataset['A10'] = dataset['A10'].cat.codes\ndataset['A12'] = dataset['A12'].cat.codes\ndataset['A13'] = dataset['A13'].cat.codes","9603691a":"dataset.head()","3e5c3741":"np.random.seed(1337)","f2da7653":"dataset.to_csv('credit-screening-all-numerics.csv', index=None)","33ebfa83":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5,random_state=42, shuffle=False)\n\nX = dataset.iloc[:,:-1]\nY = dataset.iloc[:,-1]\n\nY.replace('+', 1, inplace=True)\nY.replace('-', 0, inplace=True)\n\nprint(len(X.columns))","9ba118dc":"X = (X-X.min())\/(X.max()-X.min())\nprint(X.head())","eba1abb6":"from keras import backend as K\nimport matplotlib.pyplot as plt","df83182a":"def doTrainAndEvaluation(hiddenlayerNodeCount, hiddenLayerActivation, outputLayerActivation, lossFunction, plot_history=False):\n    print(\"DoTrainAndEvaluation with hidden nodes=%s\" %hiddenlayerNodeCount)\n    \n    scores = []\n    fold = 0        \n    \n    for train_index, test_index in skf.split(X, Y):\n        fold = fold + 1\n        x_train, x_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n        \n        model = Sequential()\n        model.add(Dense(hiddenlayerNodeCount, input_dim=15, activation=hiddenLayerActivation, kernel_initializer='normal'))\n        model.add(Dense(1, activation=outputLayerActivation, kernel_initializer='normal'))\n        model.compile(loss=lossFunction, optimizer='adam', metrics=['acc'])\n        \n        history = model.fit(x_train, y_train, epochs=50, verbose=0, batch_size=100, validation_split=0.2)\n        \n        if plot_history == True:\n            print(\"###### Cross validation fold number = %s\" %fold)\n            plt.plot(history.history['acc'])\n            plt.plot(history.history['loss'])\n            plt.plot(history.history['val_loss'])\n            plt.title('Model loss')\n            plt.ylabel('Loss')\n            plt.xlabel('Epoch')\n            plt.legend(['accuracy','Train', 'Valildation'], loc='upper right')\n            plt.show()\n        \n        y_pred = model.predict(x_test, verbose=0)\n        y_pred = np.where(y_pred > 0.5, 1, 0)\n        f1 = f1_score(y_test, y_pred, average='macro')\n        scores.append(f1)\n\n    print(\"Hidden layer count={} Mean values for f1={}\".format(hiddenlayerNodeCount, np.mean(scores, axis=0)))\n    print(\"========================================Model complete========================================\")\n    \n    return np.mean(scores, axis=0)","9ca0cc9d":"import matplotlib.pylab as plt\n\ndef plot_summary(result_dict, s = \"hidden layer neuron count\"):      \n    items = result_dict.items()\n    x,y = zip(*items)\n    plt.plot(x, y)\n    plt.xlabel(s)\n    plt.ylabel('F1 value')\n    maximum_f1_value = max(y)\n    hidden_neuron_count = max(result_dict, key=lambda k: result_dict[k])\n\n    print(\"Maximum f1 val=\" + str(maximum_f1_value) + \", \" + s + \"=\" +  str(hidden_neuron_count))","2997f47d":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'sigmoid', 'sigmoid', 'mean_squared_error', plot_history=False)","65fc9ad0":"plot_summary(matrices_variation)","71b64e98":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'sigmoid', 'sigmoid', 'binary_crossentropy', plot_history=False)","9770b76a":"plot_summary(matrices_variation)","669cc573":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'relu', 'relu', 'binary_crossentropy', plot_history=False)","c491229f":"plot_summary(matrices_variation)","d74b8a47":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'relu', 'relu', 'mean_squared_error', plot_history=False)","25c5e235":"plot_summary(matrices_variation)","594b7dc1":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'tanh', 'tanh', 'mean_squared_error', plot_history=False)","f2f0072e":"plot_summary(matrices_variation)","05294d0d":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'tanh', 'tanh', 'binary_crossentropy', plot_history=False)","632ce191":"plot_summary(matrices_variation)","cee7a4be":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'linear', 'linear', 'mean_squared_error', plot_history=False)","804056b9":"plot_summary(matrices_variation)","b6739282":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'linear', 'linear', 'binary_crossentropy', plot_history=False)","581420f9":"plot_summary(matrices_variation)","a7e4929d":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'sigmoid', 'tanh', 'mean_squared_error', plot_history=False)","208df850":"plot_summary(matrices_variation)","f40584ba":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'sigmoid', 'tanh', 'binary_crossentropy', plot_history=False)","f484b748":"plot_summary(matrices_variation)","7dafa340":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'tanh', 'sigmoid', 'mean_squared_error', plot_history=False)","0219e156":"plot_summary(matrices_variation)","102c4cf8":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'tanh', 'sigmoid', 'binary_crossentropy', plot_history=False)","61a4837b":"plot_summary(matrices_variation)","55d0cc83":"def doTrainAndEvaluationTwoHiddenLayers(hidL1NodeCount, hidL1Activation,hidL2NodeCount, hidL2Activation, outputLActivation, lossFunction, plot_history=False):\n    print(\"DoTrainAndEvaluation with hidden Layer 1 nodes={} Hidden layer2 nodes={}\".format(hidL1NodeCount,hidL2NodeCount))\n    \n    scores = []\n    fold = 0        \n    \n    for train_index, test_index in skf.split(X, Y):\n        fold = fold+1\n        x_train, x_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n        \n        model = Sequential()\n        model.add(Dense(hidL1NodeCount, input_dim=15, activation=hidL1Activation, kernel_initializer='normal'))\n        model.add(Dense(hidL2NodeCount, activation=hidL2Activation, kernel_initializer='normal'))\n        model.add(Dense(1, activation=outputLActivation, kernel_initializer='normal'))\n        model.compile(loss=lossFunction, optimizer='adam', metrics=['acc'])\n        \n        history = model.fit(x_train, y_train, epochs=50, verbose=0, batch_size=100, validation_split=0.2)\n        \n        if plot_history == True:\n            print(\"###### Cross validation fold number = %s\" %fold)\n            plt.plot(history.history['acc'])\n            plt.plot(history.history['loss'])\n            plt.plot(history.history['val_loss'])\n            plt.title('History plot on Model loss')\n            plt.ylabel('loss')\n            plt.xlabel('Epoch')\n            plt.legend(['Acc','Train', 'Test'], loc='upper left')\n            plt.show()\n        \n        y_pred = model.predict(x_test, verbose=0)\n        y_pred = np.where(y_pred > 0.5, 1, 0)\n        f1 = f1_score(y_test, y_pred, average='macro')\n        scores.append(f1)\n\n    print(\"Mean values for f1={}\".format(np.mean(scores, axis=0)))\n    print(\"========================================Model complete========================================\")\n    \n    return np.mean(scores, axis=0)","50f4676c":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluationTwoHiddenLayers(i, 'sigmoid', i, 'sigmoid', 'sigmoid', 'mean_squared_error', plot_history=False)","2fff6a5f":"plot_summary(matrices_variation)","9b2c6fc6":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluationTwoHiddenLayers(i, 'tanh', i, 'tanh', 'tanh', 'mean_squared_error', plot_history=False)","0504c103":"plot_summary(matrices_variation)","7a363d72":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluationTwoHiddenLayers(i, 'tanh', i, 'tanh', 'tanh', 'binary_crossentropy', plot_history=False)","af4ed494":"plot_summary(matrices_variation)","e912e521":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluationTwoHiddenLayers(i, 'sigmoid', i, 'tanh', 'tanh', 'binary_crossentropy', plot_history=False)","3cd180ae":"plot_summary(matrices_variation)","b6442bc7":"from keras import regularizers\nimport numpy\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict \n\ndef addRegularization(regularization, showHistory=False):\n    variation = defaultdict(list)\n    scores = []\n    fold = 0        \n    for train_index, test_index in skf.split(X, Y):\n        fold = fold+1\n        x_train, x_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n\n        for i in numpy.arange(0, 0.01, 0.001): \n            model = Sequential()\n            if (regularization == \"l1\"):\n                model.add(Dense(14, input_dim=15, activation='linear', kernel_regularizer=regularizers.l1(i)))\n                model.add(Dense(1, activation='linear', kernel_regularizer=regularizers.l1(i)))\n            else:\n                model.add(Dense(14, input_dim=15, activation='linear', kernel_regularizer=regularizers.l2(i)))\n                model.add(Dense(1, activation='linear', kernel_regularizer=regularizers.l2(i)))\n                \n            model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n            history = model.fit(x_train, y_train, epochs=50, verbose=0, validation_split=0.2)\n\n            if showHistory == True:\n                print(\"###### Cross validation fold number = %s\" %fold)\n                plt.plot(history.history['acc'])\n                plt.plot(history.history['val_loss'])\n                plt.plot(history.history['loss'])\n                plt.title('Model loss and accuracy')\n                plt.ylabel('loss and accuracy')\n                plt.xlabel('Epoch')\n                plt.legend(['accuracy', 'validation loss', 'loss'], loc='upper right')\n                plt.show()\n\n            y_pred = model.predict(x_test, verbose=0)\n            y_pred = np.where(y_pred > 0.5, 1, 0)\n            f1 = f1_score(y_test, y_pred, average='macro')\n            scores.append(f1)\n           \n            variation[i].append(f1)\n\n    return variation","0c1906cb":"matrices_variation = addRegularization('l1', False)\nprint(matrices_variation)","239f0513":"avg_f1_perfl1 = dict()\n\nfor key, val in matrices_variation.items():\n    avg_f1_perfl1[key] = np.mean(val)\n    \nprint(avg_f1_perfl1)\nplot_summary(avg_f1_perfl1,\"F1 value variation with different lambda in L1\")","926520db":"matrices_variation = addRegularization('l2')\nprint(matrices_variation)","5a2bf1a1":"avg_f1_perfl2 = dict()\n\nfor key, val in matrices_variation.items():\n    avg_f1_perfl2[key] = np.mean(val)\n    \nprint(avg_f1_perfl2)\nplot_summary(avg_f1_perfl2, \"F1 value variation with different lambda in L2\")","10b2595a":"## Two hidden layer configurations","f4492260":"### Normalizing data","98728fd8":"### Model 4\n* loss=mean_squared_error\n* activaion=rectified liner unit (relu)\n* inputlayer=15\n* outputlayer=1","61bd99a1":"### Model 9.1\n* loss=binary_crossentropy\n* activaion= tanh and sigmoid\n* inputlayer=15\n* outputlayer=1","87190e82":"### Introduction of L1 regularization has improved the model by giving a f1 value of 0.8488710330237247 when lambda=0.009","8146da28":"### Adding L2 normalization","76f0d957":"### setting numpy seed for repeatability.","59a98fb8":"### Model 2\n\n* loss=binary_crossentropy\n* activaion=sigmoid\n* inputlayer=15\n* outputlayer=1","e8ac7d6d":"### Model 8\n* loss=mean_squared_error\n* activaion=sigmoid and tanh\n* inputlayer=15\n* outputlayer=1","5088cf04":"### Model 6\n* loss=binary_crossentropy\n* activaion=tanh\n* inputlayer=15\n* outputlayer=1","81083786":"### Model 7\n* loss=mean_squared_error\n* activaion=linear\n* inputlayer=15\n* outputlayer=1","6e1ed005":"### Model 9\n* loss=mean_squared_error\n* activaion= tanh and sigmoid\n* inputlayer=15\n* outputlayer=1","c968a961":"### Model 1","2085511d":"### Model 7 above has given out the best F1 value\n\n## Introduce Normalzation to above model 7","e75e4db2":"### Model 13\n* loss=binary_crossentropy\n* activaion= tanh and sigmoid\n* inputlayer=15\n* outputlayer=1","b81670d8":"### Adding L1 normalization","037b3d3c":"### Model 12\n* loss=binary_crossentropy\n* activaion= tanh\n* inputlayer=15\n* outputlayer=1","2635d89e":"### Model 10\n* loss=mean_squared_error\n* activaion= sigmoid\n* inputlayer=15\n* outputlayer=1\n","d7d09db5":"### Model 3\n* loss=binary_crossentropy\n* activaion=rectified liner unit (relu)\n* inputlayer=15\n* outputlayer=1","5d30e2c0":"### Model 5\n* loss=mean_squared_error\n* activaion=tanh\n* inputlayer=15\n* outputlayer=1","064b18c9":"### Model 11\n* loss=mean_squared_error\n* activaion= tanh\n* inputlayer=15\n* outputlayer=1","b2cb0393":"### As it can be seen above, introduction of L2 rgularization has Not improved the model","0c25d6fd":"### Model 8.1\n* loss=binary_crossentropy\n\n* activaion=sigmoid and tanh\n* inputlayer=15\n* outputlayer=1","9456f331":"* loss=mean_squared_error\n* activaion=sigmoid\n* inputlayer=15\n* outputlayer=1","98b5ec2a":"> ### Model 7.1\n* loss=binary_crossentropy\n* activaion=linear\n* inputlayer=15\n* outputlayer=1"}}