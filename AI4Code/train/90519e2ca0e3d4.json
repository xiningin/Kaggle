{"cell_type":{"68a6d9dd":"code","fab2bee5":"code","bf4550dd":"code","efc60554":"code","b87576d1":"code","07182a1d":"code","524dd43d":"code","854e40f9":"code","fe462eb8":"code","aec55688":"code","fe0596df":"code","e75a27db":"code","17b27137":"code","ec1fcb2a":"code","63ba7c70":"code","e04c475a":"code","acacbff5":"code","f6ec3ec7":"code","2e8f41db":"code","a98df5c9":"code","0f93ecf2":"code","3e52d641":"code","fcfa971c":"code","2318a92e":"code","d40269c9":"code","aeae47cb":"markdown","aa8f4979":"markdown","7986343a":"markdown","3639635c":"markdown","da559aa8":"markdown","b8d72798":"markdown","5690e527":"markdown","a1dc64dd":"markdown","b9d13a55":"markdown","319bc21a":"markdown","57a43350":"markdown","5b128910":"markdown","b0808eb7":"markdown","480b58e0":"markdown","ab80c9a0":"markdown","510d4d58":"markdown","5bf2a88b":"markdown","f5fef6fb":"markdown","e99576b8":"markdown","310d8f77":"markdown","0d3abbc9":"markdown","588199cf":"markdown"},"source":{"68a6d9dd":"import os\nimport glob\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\nimport random\nfrom tqdm.notebook import tqdm\nimport pydicom # Handle MRI images\n\nimport cv2  # OpenCV - https:\/\/docs.opencv.org\/master\/d6\/d00\/tutorial_py_root.html\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers\n","fab2bee5":"data_dir = Path('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/')\n\nmri_types = [\"FLAIR\", \"T1w\", \"T2w\", \"T1wCE\"]\nexcluded_images = [109, 123, 709] # Bad images","bf4550dd":"train_df = pd.read_csv(data_dir \/ \"train_labels.csv\",\n#                        index='id',\n#                       nrows=100000\n                      )\ntest_df = pd.read_csv(data_dir \/ \"sample_submission.csv\")\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\ntrain_df = train_df[~train_df.BraTS21ID.isin(excluded_images)]\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\n# print(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")","efc60554":"def load_dicom(path, size = 224):\n    ''' \n    Reads a DICOM image, standardizes so that the pixel values are between 0 and 1, then rescales to 0 and 255\n    \n    Not super sure if this kind of scaling is appropriate, but everyone seems to do it. \n    '''\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    # transform data into black and white scale \/ grayscale\n#     data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return cv2.resize(data, (size, size))","b87576d1":"def get_all_image_paths(brats21id, image_type, folder='train'): \n    '''\n    Returns an arry of all the images of a particular type for a particular patient ID\n    '''\n    assert(image_type in mri_types)\n    \n    patient_path = os.path.join(\n        \"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/%s\/\" % folder, \n        str(brats21id).zfill(5),\n    )\n\n    paths = sorted(\n        glob.glob(os.path.join(patient_path, image_type, \"*\")), \n        key=lambda x: int(x[:-4].split(\"-\")[-1]),\n    )\n    \n    num_images = len(paths)\n    \n    start = int(num_images * 0.25)\n    end = int(num_images * 0.75)\n\n    interval = 3\n    \n    if num_images < 10: \n        interval = 1\n    \n    return np.array(paths[start:end:interval])\n\ndef get_all_images(brats21id, image_type, folder='train', size=225):\n    return [load_dicom(path, size) for path in get_all_image_paths(brats21id, image_type, folder)]","07182a1d":"def get_all_data_for_train(image_type, image_size=32):\n    global train_df\n    \n    X = []\n    y = []\n    train_ids = []\n\n    for i in tqdm(train_df.index):\n        x = train_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'train', image_size)\n        label = x['MGMT_value']\n\n        X += images\n        y += [label] * len(images)\n        train_ids += [int(x['BraTS21ID'])] * len(images)\n        assert(len(X) == len(y))\n    return np.array(X), np.array(y), np.array(train_ids)","524dd43d":"def get_all_data_for_test(image_type, image_size=32):\n    global test_df\n    \n    X = []\n    test_ids = []\n\n    for i in tqdm(test_df.index):\n        x = test_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'test', image_size)\n        X += images\n        test_ids += [int(x['BraTS21ID'])] * len(images)\n\n    return np.array(X), np.array(test_ids)","854e40f9":"X, y, trainidt = get_all_data_for_train('T1wCE', image_size=32)\nX_test, testidt = get_all_data_for_test('T1wCE', image_size=32)","fe462eb8":"X.shape, y.shape, trainidt.shape","aec55688":"X_train, X_valid, y_train, y_valid, trainidt_train, trainidt_valid = train_test_split(X, y, trainidt, test_size=0.2, random_state=42)","fe0596df":"X_train.shape","e75a27db":"X_train = tf.expand_dims(X_train, axis=-1)\nX_valid = tf.expand_dims(X_valid, axis=-1)","17b27137":"X_train.shape","ec1fcb2a":"y_train = to_categorical(y_train)\ny_valid = to_categorical(y_valid)","63ba7c70":"# Define, train, and evaluate model\n# source: https:\/\/keras.io\/examples\/vision\/3D_image_classification\/\ndef get_model01(width=128, height=128, depth=64, name='3dcnn'):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n\n    inputs = tf.keras.Input((width, height, depth, 1))\n\n    x = tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.GlobalAveragePooling3D()(x)\n    x = tf.keras.layers.Dense(units=512, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n\n    outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    # Define the model.\n    model = tf.keras.Model(inputs, outputs, name=name)\n    \n    # Compile model.\n    initial_learning_rate = 0.0001\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n    )\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        metrics=[\"acc\"],\n    )\n    \n    return model\n\n","e04c475a":"def get_model02():\n    np.random.seed(0)\n    random.seed(12)\n    tf.random.set_seed(12)\n\n    inpt = keras.Input(shape=X_train.shape[1:])\n\n    h = keras.layers.experimental.preprocessing.Rescaling(1.0 \/ 255)(inpt)\n\n    h = keras.layers.Conv2D(64, kernel_size=(4, 4), activation=\"relu\", name=\"Conv_1\")(h)\n    h = keras.layers.MaxPool2D(pool_size=(2, 2))(h)\n\n    h = keras.layers.Conv2D(32, kernel_size=(2, 2), activation=\"relu\", name=\"Conv_2\")(h)\n    h = keras.layers.MaxPool2D(pool_size=(1, 1))(h)\n\n    h = keras.layers.Dropout(0.1)(h)\n\n    h = keras.layers.Flatten()(h)\n    h = keras.layers.Dense(32, activation=\"relu\")(h)\n\n    output = keras.layers.Dense(2, activation=\"softmax\")(h)\n\n    model = keras.Model(inpt, output)\n\n    roc_auc = tf.keras.metrics.AUC(name='roc_auc', curve='ROC')\n\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[roc_auc]\n    )\n    return model","acacbff5":"def get_model03():\n    np.random.seed(0)\n    random.seed(12)\n    tf.random.set_seed(12)\n\n    inpt = keras.Input(shape=X_train.shape[1:])\n\n    h = keras.layers.experimental.preprocessing.Rescaling(1.0 \/ 255)(inpt)\n\n    h = keras.layers.Conv2D(64, kernel_size=(4, 4), activation=\"relu\", name=\"Conv_1\")(h)\n    h = keras.layers.MaxPool2D(pool_size=(2, 2))(h)\n\n    h = keras.layers.Conv2D(32, kernel_size=(2, 2), activation=\"relu\", name=\"Conv_2\")(h)\n    h = keras.layers.MaxPool2D(pool_size=(1, 1))(h)\n\n    h = keras.layers.Dropout(0.1)(h)\n\n    h = keras.layers.Flatten()(h)\n    h = keras.layers.Dense(32, activation=\"relu\")(h)\n\n    output = keras.layers.Dense(2, activation=\"softmax\")(h)\n\n    model = keras.Model(inpt, output)\n\n    # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers\/schedules\/ExponentialDecay\n    \n    initial_learning_rate =  0.0001\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=100000,\n        decay_rate=0.96, \n        staircase=True\n    )\n  \n    roc_auc = tf.keras.metrics.AUC(name='roc_auc', curve='ROC')\n\n    model.compile(\n        loss=\"categorical_crossentropy\", \n#         loss=\"binary_crossentropy\", \n        \n#         optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n        optimizer=keras.optimizers.Adam(),\n\n        metrics=[roc_auc],\n    )\n    return model","f6ec3ec7":"checkpoint_filepath = \"best_model.h5\"\n\nmodel_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor=\"val_roc_auc\",\n    mode=\"max\",\n    save_best_only=True,\n    save_freq=\"epoch\",\n    verbose=1,\n)","2e8f41db":"# early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=tf.keras.metrics.AUC(), mode='auto', verbose=1, patience=5)\n# early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_roc_auc\", mode='max', patience=3)","a98df5c9":"# model = get_model02() # LB score 0.676\nmodel = get_model03() # LB score 0.5\nmodel.summary()\n","0f93ecf2":"# history = model.fit(x=X_train, y = y_train, epochs=20, \n#                     callbacks=[model_checkpoint_cb], \n#                     validation_data=(X_valid, y_valid))\n\nhistory = model.fit(x=X_train, y = y_train, epochs=40, \n                    callbacks=[model_checkpoint_cb, early_stopping_cb],\n                    validation_data=(X_valid, y_valid))","3e52d641":"model_best = tf.keras.models.load_model(filepath=checkpoint_filepath)","fcfa971c":"y_pred = model_best.predict(X_valid)\n\npred = np.argmax(y_pred, axis=1)\n\nresult = pd.DataFrame(trainidt_valid)\nresult[1] = pred\n\nresult.columns = [\"BraTS21ID\", \"MGMT_value\"]\nresult2 = result.groupby(\"BraTS21ID\", as_index=False).mean()\n\nresult2 = result2.merge(train_df, on=\"BraTS21ID\")\nauc = roc_auc_score(\n    result2.MGMT_value_y,\n    result2.MGMT_value_x,\n)\nprint(f\"Validation AUC={auc}\")","2318a92e":"y_pred = model_best.predict(X_test)\n\npred = np.argmax(y_pred, axis=1) #\n\nresult = pd.DataFrame(testidt)\nresult[1] = pred\npred","d40269c9":"result.columns=['BraTS21ID','MGMT_value']\n\nresult2 = result.groupby('BraTS21ID',as_index=False).mean()\nresult2['BraTS21ID'] = sample_submission['BraTS21ID']\n\n# Rounding... 0.907866 -> 0.9\nresult2['MGMT_value'] = result2['MGMT_value'].apply(lambda x:round(x*10)\/10)\n# result2['MGMT_value'] = result2['MGMT_value'] # No rounding\nresult2.to_csv('submission.csv',index=False)\nresult2","aeae47cb":"## Model 2\n\n- from: https:\/\/www.kaggle.com\/evanyao27\/team-9-second-week\/notebook\n- Validation AUC=0.9148664856146349","aa8f4979":"## Model 1\n- from:  https:\/\/www.kaggle.com\/ohbewise\/dataset-to-model-with-tensorflow\n- Keras code from here: https:\/\/keras.io\/examples\/vision\/3D_image_classification\/","7986343a":"# Train\/Validation Split","3639635c":"## Adding a Dimension","da559aa8":"## Early Stopping Callback\n\n- https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping","b8d72798":"# Predictions on the Test Set","5690e527":"# Submission File","a1dc64dd":"### Note that rerunning the cell below will change val_acc to val_acc_N and the model will not be saved.\n\nForce name:  https:\/\/www.kaggle.com\/c\/rsna-miccai-brain-tumor-radiogenomic-classification\/discussion\/276230\n\n    roc_auc = tf.keras.metrics.AUC(name='roc_auc', curve='ROC')\n    model.compile(optimizer=..., loss=..., metrics=[roc_auc, ...])\n","b9d13a55":"# Load Our Best Model","319bc21a":"# Load Images We Will Need","57a43350":"## One-hot encode labels","5b128910":"# Predictions on Validation Set","b0808eb7":"I'm reading through several existing notebooks and trying to distill down the information into a new notebook to help me understand the project.  All help appreciated!\n\n# References\n\n- [Advanced EDA - Brain Tumor Data](https:\/\/www.kaggle.com\/smoschou55\/advanced-eda-brain-tumor-data)\n- [Team 9 Second Week](https:\/\/www.kaggle.com\/evanyao27\/team-9-second-week)\n  - The only model that is working. get_model02()\n- [Dataset to Model with Tensorflow](https:\/\/www.kaggle.com\/ohbewise\/dataset-to-model-with-tensorflow)\n- [Brain Tumer Train Class Flair](https:\/\/www.kaggle.com\/lucamtb\/brain-tumer-train-class-flair)\n  - Uses TPU\n  - Generates a Tensorflow model: Brain_flair_model_effect_3e-05_0.0001.h5\n- [Brain Tumor very basic inference](https:\/\/www.kaggle.com\/lucamtb\/brain-tumor-very-basice-inference)\n  - Uses the above mentioned model: Brain_flair_model_effect_3e-05_0.0001.h5\n  - Add this Kaggle Dataset: https:\/\/www.kaggle.com\/lucamtb\/effect0-brain","480b58e0":"## Model 3\n\nAdding LR scheduler, early stopping, etc","ab80c9a0":"## Set up Model Checkpoint","510d4d58":"# Load Datasets","5bf2a88b":"# Load Libraries","f5fef6fb":"### There's a version that converts into grayscale: \n\n- https:\/\/www.kaggle.com\/smoschou55\/advanced-eda-brain-tumor-data\n","e99576b8":"# Configuration, Constants, Setup","310d8f77":"# Utility Functions","0d3abbc9":"## Define a 3D convolutional neural network","588199cf":"# Tensorflow Models"}}