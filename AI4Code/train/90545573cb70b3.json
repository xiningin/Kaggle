{"cell_type":{"b957d836":"code","934548cf":"code","5bce2d80":"code","5a607ad1":"code","73783679":"code","8c9f32c7":"code","0f486ce8":"code","ecc0a155":"code","3cc87e5f":"code","349f3c39":"code","0e710d61":"code","1511a04d":"code","a1e088bb":"code","b58698eb":"code","bb609e2b":"code","8c1d3a8b":"code","ba75e841":"code","208e1ffa":"code","49d4b630":"code","1ac61e88":"code","12e86c99":"code","0784ff31":"markdown","8e8001a5":"markdown","12c73c35":"markdown","e482c214":"markdown","7b6546bb":"markdown","0c552a3d":"markdown","803e4014":"markdown"},"source":{"b957d836":"import pandas as pd, numpy as np, os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns',None)\n\nimport random\nimport math\nfrom scipy import stats\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing, model_selection, metrics","934548cf":"%%time\n\ndir = '..\/input\/tabular-playground-series-nov-2021\/'\nz = '.csv'\n\ntrain = pd.read_csv('..\/input\/november21\/train.csv')\ntest = pd.read_csv(dir+'test'+z)\n\nsample_submission = pd.read_csv(dir+'sample_submission'+z)\n\ntrain_indx = train['id']\ntest_indx = test['id']\n\ny = train['target']\n\ntrain.drop(['id','target'],axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)\n","5bce2d80":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\n    \nTARGET = 'target'\nFOLD = 5\nSEED = 42\nN_ESTIMATORS=15000\nDEVICE = 'GPU'\n\nLOSS = 'CrossEntropy'\nEVAL_METRIC = \"AUC\"\n\nSTUDY_TIME = 60*60*8\nseed_everything(SEED)","5a607ad1":"#reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","73783679":"#reduce memory by changing its datatype datatype\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","8c9f32c7":"# helper functions\ndef get_auc(y_true, y_hat):\n    fpr, tpr, _ = roc_curve(y_true, y_hat)\n    score = auc(fpr, tpr)\n    return score","0f486ce8":"X = train\nX_test = test\n\ndel train, test\ngc.collect()","ecc0a155":"skew_cols = []\ncols = []\n\nfor col in X.columns:\n    if abs(X[col].skew(axis=0)) > 1:\n        skew_cols.append(col)\n    else:\n        cols.append(col)\n        \nmain_cols = skew_cols + cols","3cc87e5f":"#adding new columns and checking the score\n\n#mean median and std on skew_cols and cols columns (train set)\nX['skew_mean'] = X[skew_cols].mean(axis=1)\nX['skew_median'] = X[skew_cols].median(axis=1)\nX['skew_std'] = X[skew_cols].std(axis=1)\nX['skew_skew'] = X[skew_cols].skew(axis=1)\n\nX['col_mean'] = X[cols].mean(axis=1)\nX['col_median'] = X[cols].median(axis=1)\nX['col_std'] = X[cols].std(axis=1)\nX['col_skew'] = X[cols].skew(axis=1)\n\n#mean median and std on skew_cols and cols columns (test set)\nX_test['skew_mean'] = X_test[skew_cols].mean(axis=1)\nX_test['skew_median'] = X_test[skew_cols].median(axis=1)\nX_test['skew_std'] = X_test[skew_cols].std(axis=1)\nX_test['skew_skew'] = X_test[skew_cols].skew(axis=1)\n\nX_test['col_mean'] = X_test[cols].mean(axis=1)\nX_test['col_median'] = X_test[cols].median(axis=1)\nX_test['col_std'] = X_test[cols].std(axis=1)\nX_test['col_skew'] = X_test[cols].skew(axis=1)\n\n#mean median and std on all columns\nX['mean'] = X.mean(axis=1)\nX['median'] = X.median(axis=1)\nX['std'] = X.std(axis=1)\nX['skew'] = X.skew(axis=1)\n\nX_test['mean'] = X_test.mean(axis=1)\nX_test['median'] = X_test.median(axis=1)\nX_test['std'] = X_test.std(axis=1)\nX_test['skew'] = X_test.skew(axis=1)","349f3c39":"!pip install autoviz","0e710d61":"from autoviz.AutoViz_Class import AutoViz_Class\n\nAV = AutoViz_Class()\n\ntrain = X.join(y)\n\nfilename = \"\"\nsep = \",\"\ndft = AV.AutoViz(\n    filename,\n    sep=\",\",\n    depVar=\"target\",\n    dfte=train,\n    header=0,\n    verbose=0,\n    lowess=False,\n    chart_format=\"svg\",\n    max_rows_analyzed=150000,\n    max_cols_analyzed=30,\n)","1511a04d":"plt.rcParams['figure.dpi'] = 600\n\nfig = plt.figure(figsize=(4,2), facecolor='#f6f5f5')\ngs = fig.add_gridspec(2,2)\ngs.update(wspace=0.1, hspace=0.5)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514','#ff355d'])\n\nax0 = plt.subplot(gs[0,0])\nax1 = plt.subplot(gs[0,1])\nax2 = plt.subplot(gs[1,0])\nax3 = plt.subplot(gs[1,1])\n\nfor count in range(0,4):\n    for s in [\"right\", \"top\",\"left\"]:\n        locals()['ax' + str(count)].spines[s].set_visible(False)\n        locals()['ax' + str(count)].set_facecolor(background_color)\n\n    locals()['ax' + str(count)].set_facecolor(background_color)\n    locals()['ax' + str(count)].set_xlabel('',fontsize=4, weight='bold',)\n    locals()['ax' + str(count)].set_ylabel('',fontsize=4, weight='bold')\n\n    locals()['ax' + str(count)].tick_params(labelsize=3, width=0.5, length=1.5)\n    locals()['ax' + str(count)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()['ax' + str(count)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    \n    locals()['ax' + str(count)].axes.yaxis.set_visible(False)\n\nsns.kdeplot(X['f1'],ax=ax0,fill=True,ec='black',alpha=1,zorder=5,linewidth=0.4)\nsns.kdeplot(np.cbrt(X['f1']),ax=ax1,fill=True,ec='black',alpha=1,zorder=5,linewidth=0.4,color='#ff355d')\nsns.kdeplot(X['f2'],ax=ax2,fill=True,ec='black',alpha=1,zorder=5,linewidth=0.4)\nsns.kdeplot(np.cbrt(X['f2']),ax=ax3,fill=True,ec='black',alpha=1,zorder=5,linewidth=0.4,color='#ff355d')\n\n\nplt.show()\n","a1e088bb":"#thanks to DLastStark (copied from)\nfrom tqdm import tqdm\n\nfor col in tqdm(main_cols):\n    X[col+'_bin'] = X[col].apply(lambda x: 1 if np.cbrt(x)>0 else 0)\n    X_test[col+'_bin'] = X_test[col].apply(lambda x: 1 if np.cbrt(x)>0 else 0)","b58698eb":"std = preprocessing.MinMaxScaler(feature_range=(0,1))\n\nX[main_cols] = pd.DataFrame(std.fit_transform(X[main_cols]),columns = X[main_cols].columns)\nX_test[main_cols] = pd.DataFrame(std.transform(X_test[main_cols]),columns = X_test[main_cols].columns)","bb609e2b":"cols_bin = [col for col in X.columns.to_list() if col not in main_cols]\n\nX['bin_count'] = X[cols_bin].sum(axis=1)\nX_test['bin_count'] = X_test[cols_bin].sum(axis=1)","8c1d3a8b":"%%time\nfrom sklearn.feature_selection import mutual_info_regression\n_x = X.iloc[:5000,:].copy()\n_y = y.iloc[:5000].copy()\nmi_scores = mutual_info_regression(_x, _y)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=_x.columns)\nmi_scores = mi_scores.sort_values(ascending=False)","ba75e841":"import plotly.figure_factory as ff\nimport plotly.express as px\ntop = 100\nfig = px.bar(mi_scores, x=mi_scores.values[:top], y=mi_scores.index[:top])\nfig.update_layout(\n    title=f\"Top {top} Strong Relationships Between Feature Columns and Target Column\",\n    xaxis_title=\"Relationship with Target\",\n    yaxis_title=\"Feature Columns\",\n    yaxis={'categoryorder':'total ascending'},\n    colorway=[\"blue\"]\n)\nfig.show()\n\ndel _x, _y\ngc.collect()","208e1ffa":"assert X.columns.to_list() == X_test.columns.to_list() and X.shape[1] == X_test.shape[1]","49d4b630":"%%time\n\n#X, y, X_test\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\n\n# create list[tuples] of base_models\nmodels = [\n    ('lr',LogisticRegression(solver='liblinear')),\n]\n\n# create dictionaries to store predictions\noof_pred_tmp = dict()\ntest_pred_tmp = dict()\nscores_tmp = dict()\n\n# create cv\nkf = StratifiedKFold(n_splits=20, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    # create train, validation sets\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    # fit & predict all models on the same fold\n    for name, model in models:\n        if name not in scores_tmp:\n            oof_pred_tmp[name] = list()\n            oof_pred_tmp['y_valid'] = list()\n            test_pred_tmp[name] = list()\n            scores_tmp[name] = list()\n        if name != 'lr':\n            model.fit(\n                X_train, y_train,\n                eval_set=[(X_valid,y_valid)],\n                early_stopping_rounds=200,\n                verbose=0\n            )\n        else:\n            model.fit(\n                X_train, y_train,\n\n            )\n\n        \n        # validation prediction\n        pred_valid = model.predict_proba(X_valid)[:,1]\n        score = get_auc(y_valid, pred_valid)\n        \n        scores_tmp[name].append(score)\n        oof_pred_tmp[name].extend(pred_valid)\n        \n        print(f\"Fold: {fold + 1} Model: {name} Score: {score}\")\n        print('--'*20)\n        \n        # test prediction\n        y_hat = model.predict_proba(X_test)[:,1]\n        test_pred_tmp[name].append(y_hat)\n    \n    # store y_validation for later use\n    oof_pred_tmp['y_valid'].extend(y_valid)\n        \n# print overall validation scores\nfor name, model in models:\n    print(f\"Overall Validation Score | {name}: {np.mean(scores_tmp[name])}\")\n    print('::'*20)","1ac61e88":"# create df with base predictions on test_data\nbase_test_predictions = pd.DataFrame(\n    {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1) \n    for name in test_pred_tmp.keys()}\n)\n\n# save csv checkpoint\nbase_test_predictions.to_csv('.\/base_test_predictions.csv', index=False)\n\n# create simple average blend \nbase_test_predictions['simple_avg'] = base_test_predictions.mean(axis=1)\n\n# create submission file with simple blend average\nsimple_blend_submission = sample_submission.copy()\nsimple_blend_submission['target'] = base_test_predictions['simple_avg']\nsimple_blend_submission.to_csv('.\/simple_blend_submission.csv', index=False)","12e86c99":"# create training set for meta learner based on the oof_predictions of the base models\noof_predictions = pd.DataFrame(\n    {name:oof_pred_tmp[name] for name in oof_pred_tmp.keys()}\n)\n\n# save csv checkpoint\noof_predictions.to_csv('.\/oof_predictions.csv', index=False)\n\n# get simple blend validation score\ny_valid = oof_predictions['y_valid'].copy()\ny_hat_blend = oof_predictions.drop(columns=['y_valid']).mean(axis=1)\nscore = get_auc(y_valid, y_hat_blend)\n\nprint(f\"Overall Validation Score | Simple Blend: {score}\")\nprint('::'*20)","0784ff31":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nTabular Playground Series - Nov 2021\n<\/div>","8e8001a5":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nAutoviz\n<\/div>\n\n___","12c73c35":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nPreprocessing\n<\/div>\n\n___","e482c214":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nMutual info regression\n<\/div>\n\n___","7b6546bb":"### Idea From DLastStark\n\nBelow we can split the distribution into 2 bins where the 1st bin has all negative values and the other bin with all positive values after applying cube root. as we can see from the below distribution that they can be easily split by centre 0","0c552a3d":"<a><img src=\"https:\/\/i.ibb.co\/PWvpT9F\/header.png\" alt=\"header\" border=\"0\" width=800 height=400><\/a>","803e4014":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nLogistic Regression\n<\/div>\n\n___"}}