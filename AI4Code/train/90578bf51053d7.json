{"cell_type":{"17020c21":"code","27a4b7d7":"code","e266ee80":"code","4cca6dd0":"code","a4bf90b0":"code","4b7ce4c3":"code","0a1c74b1":"code","6c30f845":"code","a30e54c2":"code","172c1e2b":"code","6cdb81ca":"code","b92fb2d5":"code","41f5b328":"code","7596b0ee":"code","26a31a11":"code","f9693d2b":"code","a3428ced":"code","2d1ec2f9":"code","aefcd5a3":"code","e033bf2f":"code","947046ea":"code","0c3b1dc4":"code","963afec9":"code","0ca2dc30":"code","5f65a55c":"code","3faebf48":"code","ba729789":"code","968266f5":"code","0d4a5b2d":"code","c1db17fa":"code","e30eb8f5":"code","df70a5c5":"code","19bf685b":"code","96829180":"code","f26f1ec7":"code","e8f4db22":"code","4bd7f8f8":"code","26a254dc":"code","9698235c":"code","3ffcb3c4":"code","29acca95":"code","51b5e517":"code","2e4f8b93":"code","e58ddf99":"code","df3f85bf":"code","79e5d3ab":"code","1c473cfe":"code","4b33eac6":"code","35d60fff":"code","ebdb107e":"code","9a81e40c":"code","5a0b40a6":"code","9c86d30e":"code","74fa7416":"code","6b959f9e":"code","4932cba8":"code","23bcbf0c":"code","e9756f99":"code","0cdb7efd":"code","93c6dcc0":"code","675dfdec":"code","840dcbe2":"code","0cfa7c35":"code","2dea9816":"code","ae3f2a2a":"code","deaa8f9c":"code","aca4b53c":"code","25ebabb7":"code","4334e610":"code","334d17aa":"code","9d3bb201":"code","8a10cf9c":"code","6f9ffdc9":"code","a7f1fd0c":"code","a3919f18":"code","b43ff5cd":"code","cd344fae":"code","c95f89af":"code","9d18f54a":"code","c4ce75d9":"code","94b61c7b":"code","970f6a19":"code","2031fe87":"code","9114dadb":"code","6e942e51":"code","69bfa8d2":"code","46b7c395":"code","05075616":"code","b9245a39":"code","ac1f9d19":"code","68287c93":"code","cfcc0079":"code","60a40275":"code","7a69be94":"code","0c45870b":"code","4037561a":"code","70c0f955":"code","42c2e4b5":"code","9320b255":"code","d787602e":"code","9e928bb4":"code","d0c19bb4":"code","2c5e5511":"code","a4ca8b02":"code","9616aca8":"markdown","26d5e799":"markdown","8315de27":"markdown","be7fbd10":"markdown","9f9afd87":"markdown","41a460d4":"markdown","530c0907":"markdown","a97b85e1":"markdown","77b9c44e":"markdown","f632bc08":"markdown","9b2445b8":"markdown","c862a995":"markdown","c5ae81a6":"markdown","74f6d9f5":"markdown","d1ca3eb0":"markdown","eaee8346":"markdown","f854dfc7":"markdown","2c51e899":"markdown","da088eb6":"markdown","a81a8463":"markdown","187e6c5e":"markdown","0a39fe14":"markdown","ff5862de":"markdown","52929131":"markdown","f70f7b61":"markdown","596370d1":"markdown","b500e148":"markdown","e791fdd9":"markdown","92d7efe9":"markdown","d0e3dcde":"markdown","7c7f2315":"markdown","f653fc02":"markdown","d75def6b":"markdown","3f4bb18c":"markdown","ac4b504c":"markdown","6d50939f":"markdown","1cd056ed":"markdown","a2d63ed8":"markdown","d3dcd229":"markdown","7dd8b64a":"markdown","f612cb4a":"markdown","c3ef1ca1":"markdown","9b3f6908":"markdown","88d05f3f":"markdown","a3f0a098":"markdown","d92ab1a9":"markdown","43a69e2d":"markdown","3f2afb65":"markdown","8893b747":"markdown","6230fcc8":"markdown","d008796f":"markdown","83245ba1":"markdown","6de6947c":"markdown","00902fae":"markdown","7d5da6e8":"markdown","d04a4da3":"markdown","120ee081":"markdown","5d9595e3":"markdown","67a566e4":"markdown","7d7db1b2":"markdown","2f043dae":"markdown"},"source":{"17020c21":"# Standard data analysis and wrangling libraries\nimport numpy as np\nimport pandas as pd\n\n# Data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Various ML models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Splitting data into training\/testing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Cross Validation\nfrom sklearn.model_selection import cross_val_score\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import r2_score\n\n# Maps for Visualization\nimport folium\n\n# Formatting\npd.set_option('display.max_columns', None) # Show all column names. For example when running the df.head() method. \n\n# Warnings\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)","27a4b7d7":"# Import data\n\ndf = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')","e266ee80":"df.head()","4cca6dd0":"df.info()","a4bf90b0":"# Change date to datetime format\n\nclean_date = df.date.apply(lambda x: x.split('T')[0])\nclean_date","4b7ce4c3":"# Replace date column with the 'clean_date' variable\n\ndf['date'] = clean_date\ndf.head()","0a1c74b1":"# Convert date column to Pandas datetime format\n\ndf['date'] = pd.to_datetime(df['date'], format = '%Y\/%m\/%d')\n\ndf.head()","6c30f845":"df.price = df.price.astype(np.int64)","a30e54c2":"# Confirm price column was converted to type int\n\ndf.info()","172c1e2b":"df.head()","6cdb81ca":"df.columns","b92fb2d5":"# Select columns to plot\n\ncols_to_plot = df[['price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15']]","41f5b328":"# Function to plot scatterplots\n\ndef plot_scatterplots():\n    for i in cols_to_plot.columns:\n        cat_num = cols_to_plot[i].value_counts()\n        print('Graph for {}: Total = {}'.format(i.capitalize(), len(cat_num)))\n        sns.scatterplot(x=cat_num.index, y=cat_num)\n        plt.xticks(rotation=90)\n        plt.show()\n        \nplot_scatterplots()","7596b0ee":"# Over 6mil - Only 3 listings, let's see how many houses cost more than 4mil.\n\ndf.loc[df.price > 6000000]","26a31a11":"# Only 11 homes over $4mil.\n# I think we can safely drop these. \n\nprint(len(df.loc[df.price > 4000000]))\ndf.loc[df.price > 4000000]","f9693d2b":"# Drop homes over $4mil\n\ndf = df[df.price < 4000000]\ndf","a3428ced":"# 11 and 33 bedroom homes are clearly anomalies as they only have 1 listing each. 10 bedroom homes are also very rare with 3 listings. \n# I'll remove all homes with 10, 11, and 33 bedrooms. \n\ndf.bedrooms.value_counts()","2d1ec2f9":"# Drop 10, 11, and 33 bedroom homes\n\ndf = df[df.bedrooms < 10]","aefcd5a3":"# Let's examine homes with 0 bedrooms.\n\n# A house with 0 bedrooms doesn't make sense to me. There are houses with 0 bedrooms and 0 bathrooms but have huge lots? \n# Are these possibly offices? Was the data not entered correctly?\n# I'm going to drop all of these rows because I think we'll come up with more accurate models this way. \n\ndf[df.bedrooms == 0]","e033bf2f":"# Drop houses with 0 bedrooms\n\ndf = df[df.bedrooms > 0]","947046ea":"df.bathrooms.value_counts()","0c3b1dc4":"# A house with no bathrooms? Again, is this possibly some sort of store\/office? Still seems strange if that's the case. \n# I'm going to drop the 3 rows.\n\ndf[df.bathrooms == 0]","963afec9":"df = df[df.bathrooms > 0]","0ca2dc30":"df.bathrooms.value_counts()","5f65a55c":"# I'm going to drop houses that have between 1-3 value counts (number of bathrooms from above) as I think this will lead to more accurate results.\n\ndf = df[df.bathrooms != 6.75]\ndf = df[df.bathrooms != 7.50]\ndf = df[df.bathrooms != 8.00]\ndf = df[df.bathrooms != 6.50]\ndf = df[df.bathrooms != 6.25]","3faebf48":"# Looks better now.  \n\ndf.bathrooms.value_counts()","ba729789":"# These rows were dropped in one of the previous steps while removing outliers. \n\ndf[df.sqft_living > 12000]","968266f5":"df.sqft_lot.value_counts()","0d4a5b2d":"print(len(df[df.sqft_lot > 750000]))\ndf[df.sqft_lot > 750000]","c1db17fa":"# Let's drop the 11 rows of homes that have a lot of over 7500000 as they seem like outliers.\n\ndf = df[df.sqft_lot < 750000]","e30eb8f5":"# Seems like these were already dropped, let's check over 6500 sqft. \n\ndf[df['sqft_above'] > 8000]","df70a5c5":"# Only 6 entries. Let's drop these too. \n\ndf[df.sqft_above > 6500]","19bf685b":"df = df[df.sqft_above < 6500]","96829180":"df[df.sqft_basement > 3000]","f26f1ec7":"# Let's drop the 2 outliers\n\ndf = df[df.sqft_basement < 3000]","e8f4db22":"df[df.sqft_lot15 > 500000]","4bd7f8f8":"# Drop the 1 row\n\ndf = df[df.sqft_lot15 < 500000]","26a254dc":"cols_to_plot = df[['price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15']]\n\n\nplot_scatterplots()","9698235c":"df.head()","3ffcb3c4":"df.shape","29acca95":"df.describe()","51b5e517":"plt.figure(figsize=(10,4))\nplt.title(\"House Price Distribution\", size=18)\nsns.distplot(df[\"price\"])\n\nplt.xlabel('Average house price', size=15)\nplt.show()","2e4f8b93":"sns.boxplot(x = df['waterfront'], y=df['price'], palette=\"Accent\")\nplt.suptitle('House Prices and Waterfront Property', size=18)\nplt.xlabel('Waterfront (1 = Yes)', fontsize = 15)\nplt.ylabel('House Prices', fontsize = 15)\n\nplt.show()","e58ddf99":"plt.figure(figsize=(8,6))\nsns.boxplot(x = df['floors'], y=df['price'], palette=\"Set2\")\nplt.suptitle('House Prices and Number of Floors', size=18)\nplt.xlabel('Number of floors', fontsize = 15)\nplt.ylabel('House Prices', fontsize =15)\n\nplt.show()","df3f85bf":"df.floors.value_counts()","79e5d3ab":"# Create a pivot table with floors\/yr_built \n\npd.pivot_table(df, index='floors', values = 'yr_built').astype(int)","1c473cfe":"plt.figure(figsize=(10,6))\nsns.boxplot(x = df['bathrooms'], y=df['price'], palette=\"Set3_r\")\nplt.suptitle('House Prices and Number of Bathrooms', size=18)\nplt.xlabel('Number of Bathrooms', fontsize = 15)\nplt.ylabel('House Prices', fontsize = 15)\nplt.xticks(rotation=90)\n\nplt.show()","4b33eac6":"plt.figure(figsize=(8,6))\nsns.boxplot(x = df['grade'], y=df['price'], palette=\"PiYG\")\nplt.suptitle('House Prices and Grade Rating', size=18)\nplt.xlabel('Grade', fontsize = 15)\nplt.ylabel('House Prices', fontsize = 15)\n\nplt.show()","35d60fff":"df.grade.value_counts()","ebdb107e":"df.head()","9a81e40c":"# Create a dataframe for the top 5\/bottom 5 zipcodes in terms of average price. Then, combine the dataframes. \n\npd.options.display.float_format = \"{:.2f}\".format # Round to 2 decimal places\n\ntop = df.groupby('zipcode')['price'].mean().reset_index().sort_values('price', ascending=False)[:5]\nbot = df.groupby('zipcode')['price'].mean().reset_index().sort_values('price', ascending=False)[-5:]\ncombined = pd.concat([top, bot])\ncombined.sort_values('price', ascending=True, inplace=True)","5a0b40a6":"plt.figure(figsize=(6,8))\n\nsns.barplot(x='price', y='zipcode' , order = combined['zipcode'], orient='h', data=combined, palette='YlOrRd')\n\nplt.title('Neighborhood Price Comparison', size=18)\nplt.ylabel('Zipcode', size=15)\nplt.xlabel('Average Home Price', size=15)\nplt.xticks(rotation=45)\n\nplt.show()","9c86d30e":"# Convert zipcode to string type because the Folium Choropleth \"columns\" parameter only accepts strings\n\ncombined['zipcode'] = combined['zipcode'].astype(str)","74fa7416":"# Create a map with folium using the combined variable from above\n\nmy_map = folium.Map(width=500, height=400,location=[47.45,-122.22], zoom_start=9.3)\n\nfolium.Choropleth(\n    geo_data=\"\/kaggle\/input\/wa-json-file\/wa_washington_zip_codes_geo.min.json\",\n    data=combined,\n    columns=['zipcode', 'price'], \n    key_on='feature.properties.ZCTA5CE10', \n    fill_color='YlOrRd', \n    fill_opacity=0.7, \n    line_opacity=0.5,\n    nan_fill_color='clear', \n    nan_fill_opacity=0.1,\n    legend_name='HOUSE PRICE'\n).add_to(my_map)\n\nfolium.LayerControl().add_to(my_map)\n\nmy_map","6b959f9e":"# 157 waterfront homes in the dataset\n\ndf.waterfront.value_counts()","4932cba8":"# Only 1 waterfront property is located in the most expensive zipcode. I assumed that there would be a lot more. \n\ndf[(df['zipcode'] == 98039) & (df['waterfront'] == 1)]","23bcbf0c":"top_5_zipcodes = (98039, 98004, 98040, 98112, 98109)","e9756f99":"# How many waterfront homes are located in the top 5 zip codes? \n\nprint(top)\nprint('')\ndf[(df['zipcode'].isin(top_5_zipcodes)) & (df['waterfront'] == 1)]","0cdb7efd":"# Let's visualize the price distirubtion for all zipcodes on a map. \n\nzipcode_prices = df.groupby('zipcode')['price'].mean().reset_index().sort_values('price', ascending=False)\nzipcode_prices['zipcode'] = zipcode_prices['zipcode'].astype(str)\nzipcode_prices","93c6dcc0":"# Create a map with folium using data from the zipcode_prices variable created above.\n\nmy_map1 = folium.Map(width=500, height=400,location=[47.40,-122.02], zoom_start=8.4)\n\nfolium.Choropleth(\n    geo_data=\"\/kaggle\/input\/wa-json-file\/wa_washington_zip_codes_geo.min.json\",\n    data=zipcode_prices,\n    columns=['zipcode', 'price'], \n    key_on='feature.properties.ZCTA5CE10', \n    fill_color='YlOrRd', \n    fill_opacity=0.7, \n    line_opacity=0.4,\n    nan_fill_color='clear', \n    nan_fill_opacity=0.2,\n    legend_name='HOUSE PRICE'\n).add_to(my_map1)\n\nfolium.LayerControl().add_to(my_map1)\n\nmy_map1","675dfdec":"sns.lmplot(x='sqft_living', y='price', data=df, scatter_kws={'s':8}, hue='waterfront', height=6)\n\nplt.title('Size Relative to Price', size=18)\nplt.ylabel('House Price', size=15)\nplt.xlabel('Total Square Feet', size=15)\n\n\nplt.show()","840dcbe2":"# 116 different years within the dataset\n\ndf.yr_built.nunique()","0cfa7c35":"# 0 null values\n\ndf.yr_built.isnull().sum()","2dea9816":"# Check the oldest\/newest year built\n\nprint(df.yr_built.min())\nprint(df.yr_built.max())","ae3f2a2a":"plt.figure(figsize=(16,5))\n\nsns.lineplot(x='yr_built', y='price', data=df,linewidth=1.5, color='blueviolet', ci=None)\n\nplt.title('Average Home Price and Year Built', size=18)\nplt.ylabel('Average Price', size=15)\nplt.xlabel('Year', size=15)\n\nplt.show()","deaa8f9c":"# let's graph the below\n\ndf.corr()","aca4b53c":"df.columns.tolist()","25ebabb7":"# Let's put price as the last column for an easier interpretation of the correlation heatmap below.\n\ncols = ['id',\n 'date',\n 'bedrooms',\n 'bathrooms',\n 'sqft_living',\n 'sqft_lot',\n 'floors',\n 'waterfront',\n 'view',\n 'condition',\n 'grade',\n 'sqft_above',\n 'sqft_basement',\n 'yr_built',\n 'yr_renovated',\n 'zipcode',\n 'lat',\n 'long',\n 'sqft_living15',\n 'sqft_lot15',\n 'price',]","4334e610":"df = df[cols]","334d17aa":"corr = df.corr()\n\nplt.figure(figsize=(15,10))\nsns.heatmap(corr, cmap='YlOrBr', annot=True)\n\nplt.show()","9d3bb201":"# Sort the highest\/lowest correlated variables\n\nhigh_corr = df.corr()['price'].sort_values(ascending=False)\nhigh_corr","8a10cf9c":"# Drop the price column and variables with low correlations\n\npd.options.display.float_format = \"{:.4f}\".format # Lets show the results with 4 decimal places\n\ndropped_vars = ['price','id', 'date','long']\n\nX = df.drop(dropped_vars, axis='columns')\ny = df['price']","6f9ffdc9":"# Split up the training\/testing data\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","a7f1fd0c":"model_MLR = LinearRegression()\nmodel_MLR.fit(X_train, y_train)","a3919f18":"yhat_pred_MLR = model_MLR.predict(X_test)","b43ff5cd":"cross_val_MLR = round(np.mean(cross_val_score(model_MLR,X_train,y_train)),4)\nR2_MLR = round(r2_score(y_test, yhat_pred_MLR),4)\nMAE_MLR = int(mean_absolute_error(y_test,yhat_pred_MLR))","cd344fae":"model_LLR = Lasso()\nmodel_LLR.fit(X_train, y_train)","c95f89af":"yhat_pred_LLR = model_LLR.predict(X_test)","9d18f54a":"cross_val_LLR = round(np.mean(cross_val_score(model_LLR,X_train,y_train)),4)\nR2_LLR = round(r2_score(y_test, yhat_pred_LLR),4)\nMAE_LLR = int(mean_absolute_error(y_test,yhat_pred_LLR))","c4ce75d9":"model_RFR = RandomForestRegressor(n_estimators=100)\nmodel_RFR.fit(X_train, y_train)","94b61c7b":"yhat_pred_RFR = model_RFR.predict(X_test)","970f6a19":"cross_val_RFR = round(np.mean(cross_val_score(model_RFR,X_train,y_train)),4)\nR2_RFR = round(r2_score(y_test, yhat_pred_RFR),4)\nMAE_RFR = int(mean_absolute_error(y_test,yhat_pred_RFR))","2031fe87":"model_xgboost = XGBRegressor()\nmodel_xgboost.fit(X_train, y_train)","9114dadb":"yhat_pred_xgboost = model_xgboost.predict(X_test)","6e942e51":"cross_val_xgboost = round(np.mean(cross_val_score(model_xgboost,X_train,y_train)),4)\nR2_xgboost = round(r2_score(y_test, yhat_pred_xgboost),4)\nMAE_xgboost = int(mean_absolute_error(y_test,yhat_pred_xgboost))","69bfa8d2":"model_ETR = ExtraTreesRegressor()\nmodel_ETR.fit(X_train, y_train)","46b7c395":"yhat_pred_ETR = model_ETR.predict(X_test)","05075616":"cross_val_ETR = round(np.mean(cross_val_score(model_ETR,X_train,y_train)),4)\nR2_ETR = round(r2_score(y_test, yhat_pred_ETR),4)\nMAE_ETR = int(mean_absolute_error(y_test,yhat_pred_ETR))","b9245a39":"model_DTR = DecisionTreeRegressor()\nmodel_DTR.fit(X_train, y_train)","ac1f9d19":"yhat_pred_DTR = model_DTR.predict(X_test)","68287c93":"cross_val_DTR = round(np.mean(cross_val_score(model_DTR,X_train,y_train)),4)\nR2_DTR = round(r2_score(y_test, yhat_pred_DTR),4)\nMAE_DTR = int(mean_absolute_error(y_test,yhat_pred_DTR))","cfcc0079":"model_GBR = GradientBoostingRegressor()\nmodel_GBR.fit(X_train, y_train)","60a40275":"yhat_pred_GBR = model_GBR.predict(X_test)","7a69be94":"cross_val_GBR = round(np.mean(cross_val_score(model_GBR,X_train,y_train)),4)\nR2_GBR = round(r2_score(y_test, yhat_pred_GBR),4)\nMAE_GBR = int(mean_absolute_error(y_test,yhat_pred_GBR))","0c45870b":"model_ENR = ElasticNet()\nmodel_ENR.fit(X_train, y_train)","4037561a":"yhat_pred_ENR = model_ENR.predict(X_test)","70c0f955":"cross_val_ENR = round(np.mean(cross_val_score(model_ENR,X_train,y_train)),4)\nR2_ENR = round(r2_score(y_test, yhat_pred_ENR),4)\nMAE_ENR = int(mean_absolute_error(y_test,yhat_pred_ENR))","42c2e4b5":"model_MLP = MLPRegressor()\nmodel_MLP.fit(X_train, y_train)","9320b255":"yhat_pred_MLP = model_MLP.predict(X_test)","d787602e":"cross_val_MLP = round(np.mean(cross_val_score(model_MLP,X_train,y_train)),4)\nR2_MLP = round(r2_score(y_test, yhat_pred_MLP),4)\nMAE_MLP = int(mean_absolute_error(y_test,yhat_pred_MLP))","9e928bb4":"model_ABR = AdaBoostRegressor()\nmodel_ABR.fit(X_train, y_train)","d0c19bb4":"yhat_pred_ABR = model_ABR.predict(X_test)","2c5e5511":"cross_val_ABR = round(np.mean(cross_val_score(model_ABR,X_train,y_train)),4)\nR2_ABR = round(r2_score(y_test, yhat_pred_ABR),4)\nMAE_ABR = int(mean_absolute_error(y_test,yhat_pred_ABR))","a4ca8b02":"models = pd.DataFrame({\n    'Model': ['Multi Linear Regression', 'Lasso Linear Regression', 'Random Forest Regression', \n              'XGBOOST Regression', 'ExtraTreesRegressor', 'Decision Tree Regressor', \n              'Gradient Boosting Regressor', 'Elastic Net Regressor', \n              'Neural Network Regression','Ada Boost Regressor'],\n    'Mean Absolute Error': [MAE_MLR, MAE_LLR, MAE_RFR, \n              MAE_xgboost, MAE_ETR, MAE_DTR, \n              MAE_GBR, MAE_ENR, MAE_MLP, MAE_ABR],    \n    'R2 Score': [R2_MLR, R2_LLR, R2_RFR, \n              R2_xgboost, R2_ETR, R2_DTR, \n              R2_GBR, R2_ENR, R2_MLP, R2_ABR],    \n    'Cross Validation Score': [cross_val_MLR, cross_val_LLR, cross_val_RFR, \n              cross_val_xgboost, cross_val_ETR, cross_val_DTR, \n              cross_val_GBR, cross_val_ENR, cross_val_MLP, cross_val_ABR]})\n\nmodels.sort_values(by='Cross Validation Score', ascending=False)","9616aca8":"![image.png](attachment:image.png)","26d5e799":"- The baseline price for waterfront properties is higher on average than those without a waterfront.\n- As the house size increases, the price gap significantly widens between waterfront\/non waterfront houses.","8315de27":"### Bedrooms - outliers","be7fbd10":"### Date - Column\n\n- Remove the 'T000000' from the date column and convert to datetime format.","9f9afd87":"&nbsp;\n\nLooks better now with the clearest outliers removed. \n\nI believe that more outliers exist within the dataset, however removing too many rows could lead to having a model that overfits the data.\n\nMove on to Visualization and EDA.\n\n&nbsp;","41a460d4":"- There seems to be a general uptrend\/linear relationship in price with the increase in the number of bathrooms\n- Lots of outliers exist in the data\n- As the number approaches 5 bathrooms the price begins to fluctuate significantly. This is mainly due to having a limited number of houses that have that many bathrooms within the dataset. ","530c0907":"- Expensive neighborhoods are clustered together.\n- The most expensive neighborhood is located in close proximity to Lake Washington. How many waterfront homes from the dataset are located here?","a97b85e1":"Description of Grade Rating:\n\n**\"An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\"**\n\n- Many outliers in the data.\n- Appears to be a quadratic relationship between Grade and Price. \n- Significant increase in the price range as the grade increases.","77b9c44e":"### DECISION TREE REGRESSOR MODEL","f632bc08":"### House Price Distribution","9b2445b8":"## Ensure all outliers have been dropped","c862a995":"### MULTI - LINEAR REGRESSION MODEL","c5ae81a6":"&nbsp;\n\n# **Summary** <a id=\"5\"><\/a>\n\nThe XGBoost regression model had the strongest results based on the Cross Validation and R2 Score. Although, the Extra Trees Regressor, Random Forest Regression and Gradient Boosting Regressor are not very far behind. Another thing to note is that these models were not tuned. I'm positive that tuning the models would increase both the Cross Validation and R2 score and reduce the MAE for most, if not all models. I may revisit this notebook in the future and tune the models to examine the difference in results. ","74f6d9f5":"### Sqft above - outliers\n\n- Maybe remove listings that are over 8000sqft? Possibly even over 6500 - depending on the number of listings.","d1ca3eb0":"## Thank you for going through my notebook!\n\n#### Please leave any comments\/suggestions below. ","eaee8346":"### House Prices and Number of Floors","f854dfc7":"Brief inferences from the data:\n\n- The average house from the dataset has at least 3 bedrooms and 2 bathrooms\n- Average size is about 2100 sqft. \n- Very few waterfront properties\n- The average house was built in 1970","2c51e899":"### Sqft lot - outliers","da088eb6":"Initial thoughts on the data\n\n- Date should be converted to datetime format\n- Eliminate 'T000000' from every row in the date column\n- Convert price column to integers\n- What's the difference between sqft_living\/sqft_lot and sqft_living15\/sqft_lot15? (Screenshot below explains this)","a81a8463":"### RANDOM FOREST REGRESSOR MODEL\n","187e6c5e":"![image.png](attachment:image.png)","0a39fe14":"### House Prices and Waterfront Property","ff5862de":"### Price difference between the most\/least expensive homes\n\nHow does the zipcode impact the price of a home?","52929131":"- Waterfront properties are all  distributed within a specific range, without outliers.\n- Non-waterfront properties vary widely in price and thus have a large number of outliers.\n- As expected, waterfront properties are on average more expensive than regular homes - the mean price is $1,000,000 greater.","f70f7b61":"Initial thoughts on graphs above. \n\n1. Price - Remove houses over 6mil - possibly even houses over 4mil?\n2. Bedrooms - Remove over 30 bedrooms?\n3. Bathroom - 0? Look into this. How can a house not have any bathrooms? \n4. Sqft_living - Possily remove houses that are over 12,000sqft?\n5. sqft_lot - Remove anything over 750,000sqft?\n6. sqft_above - Possibly remove listings that are over 8000sqft? Maybe even over 6500sqft - depending on the number of listings.\n7. sqft_basement - Drop listings that are over 3000sqft?\n8. sqft_lot15 - Drop listings that are over 500,000sqft?","596370d1":"# **Importing** <a id=\"1\"><\/a>","b500e148":"### Sqft lot15 - outliers\n\nsqft_lot15 - Drop listings that are over 500,000 sqft?","e791fdd9":"### Price column - convert to integers\n\n- Prices should be integers, not floats. Pennies are irreleveant when dealing with high prices. ","92d7efe9":"### ELASTIC NET REGRESSOR MODEL","d0e3dcde":"### General Zipcode Price Comparison","7c7f2315":"### Year Built and Price\n\n- Does the year built notably impact the price? Are newer homes more expensive? ","f653fc02":"- The prices are  slightly skewed to the right.\n- May need to normalize the data.","d75def6b":"### House Prices and Grade Rating","3f4bb18c":"# **Data Cleaning** <a id=\"2\"><\/a>\n\nBased on the above cell, it appears that there is no missing data. There are 21613 rows in the dataset.\n\nPrepare data for EDA:\n\n- convert each column to the correct format\n- search for and remove outliers","ac4b504c":"### Sqft basement - outliers\n\nsqft_basement - Drop listings that are over 3000sqft?","6d50939f":"- Houses built before 1940 and after 2000 have a higher average price.\n- Houses approximately between 1945 - 1980 cost less on average.\n- Is this possibly due to most very old houses (before 1940) having been renovated?","1cd056ed":"### Top 5\/Bottom 5 Zipcode Price Comparison","a2d63ed8":"### ExtraTreesRegressor MODEL","d3dcd229":"&nbsp;\n\n# **Visualization and EDA** <a id=\"3\"><\/a>\n\n&nbsp;","7dd8b64a":"### LASSO - LINEAR REGRESSION MODEL","f612cb4a":"- Sqft_living (total square feet of the house, not including the lot) and Grade have robust correlations to price, similar to what we saw on the graphs earlier.\n- ID and zipcode have weak negative correalations.\n- I was expecting a strong positive correlation between sqft_lot and yr_built with price. Though it seems that other variables have a stronger relationship to price. ","c3ef1ca1":"# House Sales in King County, USA - Price Prediction\n&nbsp;\n### Welcome to my notebook!\n&nbsp;","9b3f6908":"### House Prices and Number of Bathrooms","88d05f3f":"I created this notebook to practice and better understand the typical workflow of a data science project. \n\nThe notebook contains the following components:\n1. Data Cleaning\n2. EDA & Data Visualization\n3. Model Building (Comparing 10 different regression models)\n\nBeing new to the DS field, I find that working on different datasets is the best way to improve my skills.\n\nThe notebook should be fairly easy to follow along as I tried documenting my logic every step of the way. \n\n**Any comments, suggestions, and\/or feedback is greatly appreciated!**","a3f0a098":"# **Model Building** <a id=\"4\"><\/a>\n\nThe following models will be compared:\n- Multi Linear Regression Model\n- Lasso Linear Regression Model\n- Random Forest Regression Model\n- XGBoost Regression Model\n- ExtraTreesRegressor Model\n- Decision Tree Regressor Model\n- Gradient Boosting Regressor Model\n- Elastic Net Regressor Model\n- Neural Network Regression Model\n- Ada Boost Regressor Model","d92ab1a9":"My hopothesis was incorrect. In a way, it's actually the opposite of what I predicted. Houses with 3 floors are on average newer compared to homes with any other number of floors. Let's explore other features. ","43a69e2d":"### ADA BOOST REGRESSOR - MODEL","3f2afb65":"- There is a large price gap between the bottom\/top 5 neighbourhoods. \n- Average price of a house in the bottom 5 is less than 250,000 whereas the top 5 seem to start at almost 1,000,000.\n- Let's map this to see the difference in location.","8893b747":"### Price - outliers","6230fcc8":"## Overview\n\n### 1. [Importing Libraries](#1)\n### 2. [Data Cleaning](#2) \n### 3. [Visualization and EDA](#3) \n### 4. [Model Building](#4)\n### 5. [Summary](#5)\n","d008796f":"### Sqft_living - outliers","83245ba1":"- Lots of outliers for houses that have between 1-3 floors.\n- Houses with 3.5 floors seem to have the smallest price fluctuations, although there are only 7 such properties in the dataset - insuficient for any accurate deduction. \n- There seems to be an uptrend in price as the number of floors increases, though decreases at 3 and 3.5 floors - I wonder why that is. Are houses that have 3 and 3.5 floors older? Let's run a comparison.","6de6947c":"### House Price and Size Comparison","00902fae":"## Check for outliers\n\n- Create simple scatterplots and remove outliers. ","7d5da6e8":"### MULTI-LAYER PERCEPTRON REGRESSOR MODEL","d04a4da3":"### XGBOOST REGRESSOR MODEL","120ee081":"- Only 11 out of the 157 waterfront homes in the dataset are located in the top 5 expensive zipcodes. \n- My assumption was that there would be a lot more waterfront homes in the 5 most expensive neighborhoods. ","5d9595e3":"### Bathrooms - outliers","67a566e4":"### Correlation Between Variables","7d7db1b2":"![image.png](attachment:image.png)","2f043dae":"### GRADIENT BOOSTING REGRESSOR MODEL"}}