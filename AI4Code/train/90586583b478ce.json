{"cell_type":{"1b125bb9":"code","809e3fa7":"code","d635e6b7":"code","a9181f82":"code","ccbee71a":"code","5afdbf31":"code","f4ac4314":"code","7489d7ba":"code","163c987b":"code","61f8ea42":"code","386b3e0c":"code","d5002409":"code","669b796d":"code","64408ed6":"code","73db35e8":"code","5eb8cc00":"code","fa9409c6":"code","0e97d5cc":"code","e9ee1cfe":"code","0add2098":"code","c311fe45":"code","a208e1e4":"code","7088b90e":"markdown","d7874305":"markdown","e923b5d2":"markdown","c99b11b2":"markdown","ed075594":"markdown","4d87f7eb":"markdown","ffdfa337":"markdown","f79455b7":"markdown"},"source":{"1b125bb9":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split","809e3fa7":"data = pd.read_csv(\"\/kaggle\/input\/lending-club-dataset\/lending_club_loan_two.csv\")\ndata.head()","d635e6b7":"print(f\"The Length of the data: {data.shape}\")\n\ndrop_columns = ['emp_title', 'emp_length', 'title', 'address', 'grade', \n                'issue_d', 'earliest_cr_line', 'term']\ndata.dropna(inplace=True)\ndata.drop(drop_columns, axis=1, inplace=True)\n\nprint(f\"The Length of the data: {data.shape}\")","a9181f82":"def pub_rec(number):\n    if number == 0.0:\n        return 0\n    else:\n        return 1\n    \ndef mort_acc(number):\n    if number == 0.0:\n        return 0\n    elif number >= 1.0:\n        return 1\n    else:\n        return number\n    \ndef pub_rec_bankruptcies(number):\n    if number == 0.0:\n        return 0\n    elif number >= 1.0:\n        return 1\n    else:\n        return number\n    \ndata['pub_rec'] = data.pub_rec.apply(pub_rec)\ndata['mort_acc'] = data.mort_acc.apply(mort_acc)\ndata['pub_rec_bankruptcies'] = data.pub_rec_bankruptcies.apply(pub_rec_bankruptcies)\ndata['loan_status'] = data.loan_status.map({'Fully Paid':0, 'Charged Off':1})\ndata.loc[(data.home_ownership == 'ANY') | (data.home_ownership == 'NONE'), 'home_ownership'] = 'OTHER'  ","ccbee71a":"dummies = ['sub_grade', 'verification_status', 'purpose', 'initial_list_status', \n           'application_type', 'home_ownership']\n\ndata = pd.get_dummies(data, columns=dummies, drop_first=True)","5afdbf31":"w_p = data.loan_status.value_counts()[0] \/ data.shape[0]\nw_n = data.loan_status.value_counts()[1] \/ data.shape[0]\n\nprint(f\"Weight of positive values {w_p}\")\nprint(f\"Weight of negative values {w_n}\")","f4ac4314":"X = data.drop('loan_status', axis=1)\ny = data.loan_status\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7489d7ba":"import xgboost as xgb\n\nfrom sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, cross_validate, GridSearchCV","163c987b":"xgb_clf = xgb.XGBClassifier()\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nclf = cross_validate(xgb_clf, X_train, y_train, scoring='accuracy', return_train_score=True, cv=kf)","61f8ea42":"clf['test_score']","386b3e0c":"clf['train_score']","d5002409":"print(f\"Mean train accuracy: {np.mean(clf['train_score']):.4f} +\/- {np.std(clf['train_score']):.4f}\")\nprint(f\"Mean test accuracy: {np.mean(clf['test_score']):.4f} +\/- {np.std(clf['test_score']):.4f}\")","669b796d":"xgb_clf = xgb.XGBClassifier()\n\nrkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=42)\n\nprint(f\"We expect K * n performance metrics: {5 * 10}\")\nclf = cross_validate(xgb_clf, X_train, y_train, scoring='accuracy', return_train_score=True, cv=rkf)\nprint(f\"Number of metrics obtained: {len(clf['test_score'])}\")","64408ed6":"clf['test_score']","73db35e8":"clf['train_score']","5eb8cc00":"print(f\"Mean train accuracy: {np.mean(clf['train_score']):.4f} +\/- {np.std(clf['train_score']):.4f}\")\nprint(f\"Mean test accuracy: {np.mean(clf['test_score']):.4f} +\/- {np.std(clf['test_score']):.4f}\")","fa9409c6":"xgb_clf = xgb.XGBClassifier()\n\nskf = StratifiedKFold(n_splits=5, random_state=42)\n\nclf = cross_validate(xgb_clf, X_train, y_train, scoring='accuracy', return_train_score=True, cv=skf)\nprint(f\"Number of metrics obtained: {len(clf['test_score'])}\")","0e97d5cc":"clf['test_score']","e9ee1cfe":"clf['train_score']","0add2098":"print(f\"Mean train accuracy: {np.mean(clf['train_score']):.4f} +\/- {np.std(clf['train_score']):.4f}\")\nprint(f\"Mean test accuracy: {np.mean(clf['test_score']):.4f} +\/- {np.std(clf['test_score']):.4f}\")","c311fe45":"xgb_clf = xgb.XGBClassifier()\n\nparam_grid = dict(\n    n_estimators= [100, 500, 1000], \n    max_depth= [2, 5, 10, 15],\n    learning_rate= [0.01, 0.1, 0.5, 0.9],\n    min_child_weight= [1, 2, 5], \n#     booster= ['gbtree', 'gblinear'], \n#     base_score= [0.25, 0.5, 0.75, 0.99]\n)\n\nhyperparameters_comb = 1\nfor keys, values in param_grid.items():\n    hyperparameters_comb *= len(values)\nprint(f\"Number of hyperparam combinations: {hyperparameters_comb}\")\n\nsearch = GridSearchCV(xgb_clf, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1, verbose=1)\nsearch.fit(X_train, y_train)","a208e1e4":"rf_clf = RandomForestClassifier()","7088b90e":"# \ud83d\udd04 Data Preprocessing","d7874305":"## 2. Repeated K-Fold","e923b5d2":"## 1. KFold","c99b11b2":"# Hyperparameter Optimization\n\n## Grid Search\n\n>- Exhaustive search through a specified subset of hyperparameters of a learning algorithm.\n>- Examines all possible combinations of the specified hyperparameters (Cartesian product of hyperparameters).\n\n### Limitations\n>- Curse of dimentionality: possible combinations grow exponentially with the number of hyperparameters.\n>- Computationally expensive.\n>- Hyperparameter values are determined manually.\n>- Not ideal for continuous hyperparameters.\n>- Does not explore the entire hyperparameter space (not feasible).\n>- It performs worse than other searches (for models with complex hyperparameter spaces).\n\n### Advantages\n>- For models with simpler hyperparameter spaces works well.\n>- It can be parallelized.\n\nGrid Search is the most expensive method in terms of total computation time. However, if run in parallel, it is fast in terms of wall clock time. Sometimes, we run a small grid, determine where the optimum lies, and then expand the grid in that direction.","ed075594":"# Loading The Data","4d87f7eb":"## Random Search\n\n>- Hyperparameter values are selected by independent (random) draws from uniform distribution of the hyperparameter space. Random Search selects the combinations of hyperparameter values at random from all the possible combinations given a hyperparameter space.\n\n---\n\n## Random Search vs Grid Search\n>- Some parameters affect performance a lot and some others don't (Low Effective Dimension). \n\n|      Random Search                                                |   Grid Search                            |\n|:------------------------------------------------------------------|-----------------------------------------:|\n| Allows the exploration of more dimensions of important parameters | Waste time exploring non-important dimensions |\n| Select values from a distribution of parameter values             | Parameters are defined manually |\n| Good for continuous parameters                                    | Good for discrete parameters |\n\n---\n\n## Considerations\n>- We choose a (computational) budget independently of the number of parameters and possible values.\n>- Adding parameters that do not influence the performance does not decrease efficiency of the search (if enough iterations are allowed).\n>- Important to specify a continuous distribution of the hyperparameter to take full advantage of the randomization.","ffdfa337":"## 3. Straitified K-Fold Cross-Validation","f79455b7":"# \ud83c\udfafHyperparameter Optimization for Machine Learning\n\nThe aim of this notebook:\n> - Discuss multiple ways to optimize hyperparameters.\n> - Understand the logic of each technique.\n> - Considerations when utilizing each technique.\n> - Master the use of Python open-source for hyperparameter tuning.\n\n---\n\n## Parameters in ML models\n> - The objective of a typical learning algorithm is to find a function `f` that minimizes a certain `loss` over a `dataset`.\n> - The learning algorithm produces `f` through the optimization of a training criteron with respect to a set of `parameters`.\n\n---\n\n## Hyperparameters in ML models\n> - Hyperparameters are parameters that are not directly learnt by the learning algorithm.\n> - Hyperparameters are specified outside of the training procedure.\n> - Hyperparameters control the capacity of the model, i.e., how flexible the model is to fit the data.\n> - Prevent over-fitting.\n> - Hyperparameters could have a big impact on the performance of the learning algorithm.\n> - Optimal hyperparameter settings often differ for different datasets. Therefore they should be optimized for each dataset.\n---\n\n## Hyperparameter Nature\n>- Some hyperparameters are discrete: Number of estimators in ensemble models.\n>- Some hyperparameters are continuous: Penalization coefficient, Number of samples per split.\n>- Some hyperparameters are categorical: Loss (deviance, exponential), Regularization (Lasso, Ridge)\n\n---\n\n## Parameters vs Hyperparameters\n\n|Parameters                  |   Hyperparameters |\n|:-------------------------|----------------------:|\n| - Intrinsic to model equation     | - Defined before training |\n| - Optimized during training | - Constrain the algorithm|\n\n> - The process of finding the best Hyperparameters for a given dataset is called `Hyperparameter Tuning` or `Hyperparameter Optimization`.\n\n---\n\n## Challenges\n>- We can't define a formula to find the hyperparameters.\n>- Try different combinations of hyperparameters and evaluate model performance. The critical step is to choose how many different combinations we are going to test.\n\nThe number of hyperparameter combination ---> the chance to get a better model ---> Computational cost\n\n>- How do we find the hyperparameter combinations to maximize performance while diminishing computational costs?\n\n---\n\n## Methods\nDifferent hyperparamete optimization strategies:\n>- Manual Search\n>- Grid Search\n>- Random Search\n>- Bayesian Optimization\n\n---\n\n## Generalization vs Over-fitting\n> Generalization is the ability of an algorithm to be effective across various inputs. The performance of the machine learning model is constant across different datasets (with the same distribution on the training data). When the model performs well on the train set, but not on new \/ naive data, the model over-fits to the training data.\n\n---\n\n## Training a Machine Learning Model\n> To prevent over-fitting, it is common practice to:\n> - Separate the data into a train and a test set.\n> - Train the model in the train set.\n> - Evaluate in the test set."}}