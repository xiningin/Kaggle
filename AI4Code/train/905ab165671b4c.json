{"cell_type":{"bd89ed99":"code","a0a141a3":"code","bf556ae9":"code","7ce8a9bb":"code","00f2bfd4":"code","d1e44812":"code","7e7cb4ce":"code","796896f9":"code","27e0f2b3":"code","3da52bd0":"code","ab1adffe":"code","b7059e10":"code","1a766976":"code","94308ae0":"code","32017585":"code","2d02545c":"code","4728d64e":"code","8a04bb77":"code","d30aa2d6":"code","6480196f":"code","a7758696":"code","11a71cdd":"code","1a11815c":"code","02e0512f":"code","f912c08b":"code","ed4583b9":"markdown","61f89f87":"markdown","dc6dd6c7":"markdown","7ff7f7e9":"markdown","38d0f6c4":"markdown","156b0c65":"markdown","b3ec5b24":"markdown","5aff8704":"markdown","8d84c111":"markdown","d8a7db69":"markdown","8764aef1":"markdown","28bd7244":"markdown","e554e128":"markdown","497c43f8":"markdown","596d3b7b":"markdown","363f1530":"markdown","4bd00961":"markdown","63fd55da":"markdown"},"source":{"bd89ed99":"import pandas as pd\nfrom tensorflow import keras","a0a141a3":"#PUT THE CSV DATA INTO A PANDAS DATAFRAME\ndigits_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndigits_train.tail()","bf556ae9":"import numpy as np\nimport matplotlib.pyplot as pt","7ce8a9bb":"digits_train.shape","00f2bfd4":"digits_train[\"label\"]","d1e44812":"import seaborn as sns\n\nY_train = digits_train[\"label\"]\nX_train = digits_train.drop(labels = \"label\", axis = 1)\n\ng = sns.countplot(Y_train)\n\nY_train.value_counts().sort_values(ascending=True)","7e7cb4ce":"X_train.isnull().any().describe()","796896f9":"test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\ntest.isnull().any().describe()","27e0f2b3":"#test = test.drop(labels = \"label\", axis = 1)\n\n#Normalize the data\nX_train = X_train \/ 255\ntest = test \/ 255","3da52bd0":"test.isnull().any().describe()","ab1adffe":"#TRAIN DATA\nX_train.head()\n","b7059e10":"#TEST DATA\ntest.head()","1a766976":"#Reshape images into 3D matrices\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","94308ae0":"import keras\nimport keras.utils\nfrom keras.utils import to_categorical\n#Encode labels to one hot vector. Ex -> 2 = (0,0,1,0,0,0,0,0,0,0)\nY_train = to_categorical(Y_train, num_classes = 10)","32017585":"import sklearn\nfrom sklearn.model_selection import train_test_split\n\n#Set the random seed\nrandom_seed = 2\n\n#Split the train and validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=random_seed)","2d02545c":"#some examples\ng = pt.imshow(X_train[0][:,:,0])","4728d64e":"#1. Neural Network with 2 hidden layers of 1024 units\n\n# - The input shape is a Flatten layer of 28x28 input shape (784)\n\n# - We introduce here the mean absolute error or MAE. For each prediction y_pred, MAE measures the disparity from \n#   the true target y_true by an absolute difference abs(y_true - y_pred)\n\n# - We should make a neural network with 10 binary outputs (True, False) depending on what number is it\n\n#INMPORTANT INFO: The loss function measures the disparity between the the target's true value and the value the model predicts\n\nfrom sklearn import preprocessing\nfrom tensorflow.keras import layers\n\nmodel1 = keras.Sequential([\n    layers.Flatten(input_shape=(28,28,1)),\n    layers.Dense(units=1024, activation='relu'),\n    layers.Dense(units=1024, activation='relu'),\n    layers.Dense(units=10),\n])\n\nmodel1.compile(\n    optimizer='adam',\n    loss='mae'\n)\n\nhistory1 = model1.fit(\n    X_train, Y_train,\n    validation_data=(X_val, Y_val),\n    batch_size=512,\n    epochs=10\n)\n\nhistory_df = pd.DataFrame(history1.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"MAE\")","8a04bb77":"#2. Neural Network with 2 hidden layers of 1024 units and Dropout\n\n\nmodel2 = keras.Sequential([\n    layers.Flatten(input_shape=(28,28,1)),\n    layers.Dropout(0.3),\n    layers.Dense(units=1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(units=1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(units=10),\n])\n\nmodel2.compile(\n    optimizer='adam',\n    loss='mae'\n)\n\nhistory2 = model2.fit(\n    X_train, Y_train,\n    validation_data=(X_val, Y_val),\n    batch_size=512,\n    epochs=10\n)\n\nhistory_df = pd.DataFrame(history2.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"MAE\")","d30aa2d6":"# 3. Introducing the BatchNormalization\n\nmodel3 = keras.Sequential([\n    layers.Flatten(input_shape=(28,28,1)),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(units=1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(units=1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(units=10),\n])\n\nmodel3.compile(\n    optimizer='adam',\n    loss='mae'\n)\n\nhistory3 = model3.fit(\n    X_train, Y_train,\n    validation_data=(X_val, Y_val),\n    batch_size=512,\n    epochs=10\n)\n\nhistory_df = pd.DataFrame(history3.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"MAE\")","6480196f":"# make a prediction\n#test = to_categorical(test, num_classes = 10)\ny = model3.predict(test)\ny.shape","a7758696":"print(y.max())","11a71cdd":"from sklearn import preprocessing\nimport pandas as pd\n\nscaler = preprocessing.MinMaxScaler()\nprint(scaler.fit(y))\n\nprint(scaler.data_max_)\n\nprint(scaler.transform(y))","1a11815c":"y = scaler.transform(y)\nprint(y)","02e0512f":"image_index = 2853\npt.imshow(test[image_index].reshape(28, 28),cmap='Greys')\npredict = test[image_index].reshape(28,28)\npred = model3.predict(test[image_index].reshape(1, 28, 28, 1))\nprint(pred.argmax())","f912c08b":"#Submit predictions\ny_pred = model3.predict(test)\ny_pred = np.argmax(y_pred,axis=1)\n\n#Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\nsub_file = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\npredictions = model3.predict(test)\nsubmission = pd.DataFrame({'ImageId': list(range(1, len(y_pred)+1)), 'Label':y_pred})\n\nsubmission.to_csv('.\/sample_submission.csv', index=False)","ed4583b9":"# DIGIT RECOGNIZER USING NEURAL NETWORKS","61f89f87":"10% for evaluation, 90% for training","dc6dd6c7":"We can see how is the proportion of the numbers \"label\" we have in our dataset. When we train the model it would be easier to recognize a 1 than a 5 due to a bigger training subset of data to learn.","7ff7f7e9":"# 2.4 Label Encoding","38d0f6c4":"# 2.1 Cleaning","156b0c65":"# 3. CNN","b3ec5b24":"# 2 Data Preparation","5aff8704":"# 2.2 Normalization","8d84c111":"**Pandas**\n\nFast, powerful, flexible and easy to use open source library, built on top of the Python programming language.\n\n**Pandas Dataframe**\n\nIt's a two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for \"array\" objects. Pandas dataframes are based on numpy ndarrays which we should know some about.\n","d8a7db69":"One-Hot Encoder\n\nThough label encoding is straight but it has the disadvantage that the numeric values can be misinterpreted by algorithms as having some sort of hierarchy\/order in them. This ordering issue is addressed in another common alternative approach called \u2018One-Hot Encoding\u2019. In this strategy, each category value is converted into a new column and assigned a 1 or 0 (notation for true\/false) value to the column.\n","8764aef1":"In my opininon the best model is the 3rd, so lets use it to do a prediction","28bd7244":"It's not too bad but we should prevent overfitting because the deviation between the loss function and the validation loss is growing while we are still training our net.\n\nWe will add Dropout to prevent this in our model.\n\n","e554e128":"# 2.3 Reshape Data","497c43f8":"Moreover de CNN converge faster on [0,1] data than on [0,255]","596d3b7b":"# 1 Understand the problem and import the more important libraries","363f1530":"# 3.1 Define the model","4bd00961":"# 2.5 Split training and validation sets","63fd55da":"Keras requieres an extra parameter at the end to define channels. We use only one due to gray scale image. If we were working with RGB we may use 3."}}