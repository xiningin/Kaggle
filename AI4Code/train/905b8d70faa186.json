{"cell_type":{"00cc0e32":"code","7e303827":"code","dcefc93c":"code","da3f52a0":"code","fe1918eb":"code","e2438754":"code","b1f7f356":"code","f3a7296f":"code","8fb58639":"code","eadcc01e":"code","fcd2a24a":"code","e5e9c8b7":"code","7b80f91a":"code","e66aec38":"code","0d8775d6":"code","5803be19":"code","18fb5026":"code","30856e48":"code","4c1c83a7":"code","f80d7300":"code","46dc4345":"code","e32fc732":"code","f60372f7":"code","0e451d2d":"code","1a53ed19":"code","900242d7":"code","4cf4f140":"code","dc53b267":"code","814746e7":"code","c18f86c7":"code","e8afb91e":"code","f8d14a7e":"code","4c19ae71":"code","1807c3c1":"code","439c4730":"code","30004fec":"markdown","39f0fcf5":"markdown","c13c509c":"markdown","857d3a0d":"markdown","1ae0f3c9":"markdown","61f887e4":"markdown","b7359587":"markdown","29f9b976":"markdown","b811bc2d":"markdown","1cab58f2":"markdown","bd980b20":"markdown","8bb6439b":"markdown","d997d5e8":"markdown","9afe8db6":"markdown","3c5f153b":"markdown","493cf815":"markdown","73009250":"markdown","47749b18":"markdown","5e59b877":"markdown","728be4b6":"markdown","d9079b74":"markdown","72fac71c":"markdown","10118f56":"markdown","e5670a79":"markdown","43911d11":"markdown","8c43a0a9":"markdown","9dd7504b":"markdown","1a738385":"markdown","13648bb0":"markdown"},"source":{"00cc0e32":"import sklearn\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt","7e303827":"data = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","dcefc93c":"# Checking if there are null values in the dataset\ndata.isnull()","da3f52a0":"# Deleting 'Unnamed: 32' column\ndata.drop(\"Unnamed: 32\",axis=1,inplace=True)","fe1918eb":"# Deleting 'id' column\ndata.drop(\"id\",axis=1,inplace=True)","e2438754":"# Take a look to the data columns:\nlist(data.columns)","b1f7f356":"data.head()","f3a7296f":"data.describe()","8fb58639":"# Mapping diagnosis to integer values\ndata['diagnosis']=data['diagnosis'].map({'M':1,'B':0})","eadcc01e":"features_mean= list(data.columns[1:11])\nfeatures_se= list(data.columns[11:20])\nfeatures_worst=list(data.columns[21:31])","fcd2a24a":"sns.set(style='darkgrid', font_scale=1.1)\nsns.countplot(data['diagnosis'],label=\"Count\")","e5e9c8b7":"corr = data[features_mean].corr()\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features_mean, yticklabels= features_mean)","7b80f91a":"# Based on correlation heatmap, we can select some of the variables to be used on prediction\npred_var = ['texture_mean','radius_mean','smoothness_mean','concavity_mean','symmetry_mean']","e66aec38":"g = sns.PairGrid(data, y_vars=pred_var, x_vars=['diagnosis'], aspect=0.8, height=3.0)\ng.map(sns.barplot, palette='muted')","0d8775d6":"data_target = data['diagnosis']\ndata_features = data.drop(['diagnosis'],axis=1)","5803be19":"# Splitting our dataset into training data and test data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_features, data_target, random_state=0)","18fb5026":"print(\"X_train shape: \", X_train.shape)\nprint(\"y_train shape: \", y_train.shape)","30856e48":"print(\"X_test shape: \", X_test.shape)\nprint(\"y_test shape: \", y_test.shape)","4c1c83a7":"# n_neighbors=1 is setting the number of nearest neighbours to 1.\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)","f80d7300":"# build the model on the training set, i.e. X_train and y_train.\nknn.fit(X_train, y_train)","46dc4345":"print(\"KNN-1 Accuracy on training set:  {:.3f}\".format(knn.score(X_train, y_train)))\nprint(\"KNN-1 Accuracy on test set: {:.3f}\".format(knn.score(X_test, y_test)))","e32fc732":"# specify one new instance to be predicted\nX_new = np.array([[18.99,\n10.30,\n123.8,\n1001,\n0.119,\n0.26,\n0.30,\n0.15,\n0.24,\n0.08,\n1.095,\n0.9053,\n8.65,\n157.4,\n0.0064,\n0.04904,\n0.05373,\n0.01587,\n0.03003,\n0.0053,\n25.38,\n17.33,\n186.5,\n2019,\n0.1642,\n0.6656,\n0.7119,\n0.2654,\n0.4601,\n0.1189]])","f60372f7":"prediction = knn.predict(X_new)\n\nprint(f\"Prediction: {'Malignant' if prediction == 1 else 'Benign'}\")","0e451d2d":"knn = KNeighborsClassifier(n_neighbors=4)","1a53ed19":"knn.fit(X_train, y_train)","900242d7":"print(\"KNN-4 - Accuracy on training set:  {:.3f}\".format(knn.score(X_train, y_train)))\nprint(\"KNN-4 - Accuracy on test set: {:.3f}\".format(knn.score(X_test, y_test)))","4cf4f140":"prediction = knn.predict(X_new)\n\nprint(f\"Prediction: {'Malignant' if prediction == 1 else 'Benign'}\")","dc53b267":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)","814746e7":"print(\"Decision Tree - Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Decision Tree - Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","c18f86c7":"tree = DecisionTreeClassifier(max_depth=3, random_state=12)\ntree.fit(X_train, y_train)\n\nprint(\"Decision Tree - Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Decision Tree - Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","e8afb91e":"prediction = tree.predict(X_new)\n\nprint(f\"Prediction: {'Malignant' if prediction == 1 else 'Benign'}\")","f8d14a7e":"tree = DecisionTreeClassifier(max_depth=2, random_state=12)\ntree.fit(X_train, y_train)\n\nprint(\"Decision Tree - Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Decision Tree - Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","4c19ae71":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators=1000, random_state=999, max_depth=3)\nforest.fit(X_train, y_train)","1807c3c1":"print(\"Random Forest - Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\nprint(\"Random Forest - Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))","439c4730":"prediction = forest.predict(X_new)\n\nprint(f\"Prediction: {'Malignant' if prediction == 1 else 'Benign'}\")","30004fec":"We can conclude that the Random Forest model proved to be the most accurate in the classification of breast cancer.\n\nThank you, this is a basic notebook from the stages of machine learning, if you liked it please vote.","39f0fcf5":"## Model: Decision Tree\n\nIt uses a decision tree to go from observations about an item to conclusions about the item's target value.","c13c509c":"### Improving the Decision Tree model\n\nTrying different max_depth in the decision tree model.","857d3a0d":"The new decision tree has lower accuracy on the training dataset, but higher accuracy on the test dataset.","1ae0f3c9":"## Analyzing data correlation\nA correlation matrix is a tabular data representing the \u2018correlations\u2019 between pairs of variables in a given data.\nEach row and column represents a variable, and each value in this matrix is the correlation coefficient between the variables represented by the corresponding row and column.\nThe Correlation matrix is an important data analysis metric that is computed to summarize data to understand the relationship between various variables and make decisions accordingly.","61f887e4":"There was an improvement in the accuracy of the model using 4 n_neighbors instead of 1.","b7359587":"# Introduction\n\nHello, in this notebook I'm going to show you how to use supervised learning in the task of identifying whether it is a benign or malignant breast cancer. Below, is shown the Data Science Process. It's based in it that i'm going to teach the step by step to reach the result. ","29f9b976":"The knn model with n_neighbors=1, has accuracy 100% on the training dataset. This means that it's over-fitting the training data.","b811bc2d":"![](https:\/\/miro.medium.com\/max\/3870\/1*eE8DP4biqtaIK3aIy1S2zA.png)","1cab58f2":"**Getting the frequency of the breast cancer diagnosis:**\n* 1 (Malignant)\n* 0 (Benign)","bd980b20":"# 3. Cleaning the data","8bb6439b":"![](https:\/\/lh4.googleusercontent.com\/v9UQUwaQTAXVH90b-Ugyw2_61_uErfYvTBtG-RNRNB_eHUFq9AmAN_2IOdfOETnbXImnQVN-wPC7_YzDgf7urCeyhyx5UZmuSwV8BVsV8VnHxl1KtgpuxDifJ4pLE23ooYXLlnc)\n\n*Basic Decision Tree example*","d997d5e8":"## Model: K-Nearest Neighbours\n\nKNN is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions).","9afe8db6":"# 1. Importing libraries\n\n**scikit-learn** is the most widely used Python library for machine learning. **pandas** is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\nbuilt on top of the Python programming language.","3c5f153b":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/76\/Random_forest_diagram_complete.png)\n","493cf815":"# 5. Creating a model\n\nWe need to know how well it performs. To do this, the data is splitted in two parts: 1) a training dataset that we use for building the model, and 2) a test dataset that we use for testing the accuracy of our model. We do this with the use of the train_test_split function, which shuffles the dataset randomly, and by default extracts 75% of the cases as training data and 25% of the cases as test data.","73009250":"The decision tree built has accuracy 100% on the training dataset. This means that our decision tree is over-fitting the training data.\nTo avoid overfitting (and hopefully improve the accuracy of the model on test data), we can stop before the entire tree is created. We can do this by setting the maximal depth of the tree.","47749b18":"### Improving the KNN model\n\nTrying different numbers of k nearest neighbours.","5e59b877":"### Building a model","728be4b6":"# Conclusion","d9079b74":"## Model: Random Forest\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees.","72fac71c":"The new decision tree has lower accuracy on the training dataset, but higher accuracy on the test dataset. max_depth higher than this, has lesses accuracies.","10118f56":"# 4. Exploring the data","e5670a79":"### Testing predictions using the model","43911d11":"**The data can be divided into \"mean\", \"se\" and \"worst\", so below it is done:**","8c43a0a9":"# 2. Loading the data\n","9dd7504b":"### Model evaluation\nTesting dataset to evaluate the accuracy of the model.","1a738385":"![](https:\/\/3.bp.blogspot.com\/-In1TiknFHSg\/XHaqqP8UzhI\/AAAAAAAAGSY\/0m6BSNsFKqIEDVJZyhSatsi7jL2Kb4pwwCLcBGAs\/s1600\/knn.jpg)","13648bb0":"# Code time:"}}