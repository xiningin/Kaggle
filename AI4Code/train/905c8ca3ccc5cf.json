{"cell_type":{"38d56cf2":"code","ae2a2f15":"code","6a516f9b":"code","713ee6a8":"code","1bc19745":"code","0171c881":"code","1b03c42e":"code","3797db52":"code","b7ee0ee7":"code","c0f1173e":"code","8c47e7de":"code","2e64ba2b":"code","77c9f51f":"code","4b74ca3a":"code","246ed1fd":"code","304e8f94":"code","4add9102":"code","cadf7130":"code","a160cf45":"code","a08b64b3":"code","291e79e1":"code","115d2b1b":"code","bf60b802":"code","515859d2":"code","ce8b90b6":"code","2ae19633":"code","df9f3f1a":"code","ec6660db":"code","9e906573":"code","8fe5ec6c":"code","79218448":"code","f3fc67f0":"code","2c32ba01":"code","bc8d2d9e":"code","3917d58c":"code","2a4faf06":"code","2894ac3c":"code","85756ad2":"code","0721cff9":"code","c06c56c4":"code","9364190f":"code","3e10482a":"code","e121269e":"code","213df135":"code","ca4722f2":"code","75b3c9e6":"code","09b11cd8":"code","df2a3efb":"code","51b4968b":"code","082d5949":"code","53160407":"code","52d516a9":"code","94331eac":"code","588d9d93":"code","883bcc9c":"code","2b104c35":"code","cc5db93c":"code","997faaf5":"markdown","b8455ea3":"markdown","076384d3":"markdown","d4b9a0d0":"markdown","701097a0":"markdown","3111bf8a":"markdown","64d46587":"markdown","97da9ddf":"markdown","543b1cb4":"markdown","c093c15b":"markdown","9304cfc7":"markdown","ef6ce9f3":"markdown","10ab7c51":"markdown","0262b1d1":"markdown","9dd050bd":"markdown","5583e511":"markdown","28fd18af":"markdown","5c54f889":"markdown","f4432360":"markdown","94bd4f78":"markdown","7b75e7ac":"markdown","5f883232":"markdown","678f8e9a":"markdown","ae6a46b5":"markdown","61297bc3":"markdown","0a605bb2":"markdown","6175b53e":"markdown","d488e179":"markdown","dec08fb1":"markdown","a7e37bbe":"markdown","31360359":"markdown","96630f22":"markdown","ff8d2ae7":"markdown","4cfd41e2":"markdown","c9101dbc":"markdown","c9302768":"markdown","5604f92d":"markdown","a2b6db9b":"markdown"},"source":{"38d56cf2":"# importar pacotes necess\u00e1rios\nimport numpy as np\nimport pandas as pd","ae2a2f15":"# definir par\u00e2metros extras\npd.set_option('precision', 4)\npd.set_option('display.max_columns', 100)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6a516f9b":"prefixo_arquivos = '\/kaggle\/input\/serpro-titanic\/'","713ee6a8":"# carregar arquivo de dados de treino\ntrain_data = pd.read_csv(prefixo_arquivos + 'titanic-train.csv', index_col='person')","1bc19745":"# carregar arquivo de dados de teste\ntest_data = pd.read_csv(prefixo_arquivos + 'titanic-test.csv', index_col='person')","0171c881":"# unir ambos os dados de treino e teste\ndata = pd.concat([train_data, test_data])\n\n# mostrar alguns exemplos de registros\ndata.head()","1b03c42e":"# transformar colunas textuais em categ\u00f3ricas\ndata['survived'] = data['survived'].map({'yes': 1, 'no': 0})","3797db52":"# extrair t\u00edtulos das pessoas a partir do nome\ndata['title'] = data['name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# exibir rela\u00e7\u00e3o entre t\u00edtulo e sexo\npd.crosstab(data['title'], data['sex']).T","b7ee0ee7":"# agregar t\u00edtulos incomuns\nreplacements = {\n    'Miss': ['Mlle', 'Ms'],\n    'Mrs': ['Mme'],\n    'Rare': ['Lady', 'Countess', 'Capt', 'Col', 'Don', \\\n             'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n}\nfor k, v in replacements.items():\n    data['title'] = data['title'].replace(v, k)\n    \n# exibir rela\u00e7\u00e3o entre t\u00edtulo e sexo\npd.crosstab(data['title'], data['sex']).T","c0f1173e":"# categorizar os valores dos t\u00edtulos\ntitle_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\ndata['title'] = data['title'].map(title_mapping)\ndata['title'] = data['title'].fillna(0)","8c47e7de":"# categorizar os valores dos sexos\ndata['sex'] = data['sex'].map({'female': 1, 'male': 0}).astype(int)","2e64ba2b":"# preencher e categorizar os valores dos portos de embarque\ndata['embarked'].fillna(data.embarked.mode()[0], inplace=True)\ndata['embarked'] = data['embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)","77c9f51f":"# preencher os valores da passagem\ndata['fare'].fillna(data.fare.mean(), inplace=True)","4b74ca3a":"# criar coluna com tamanho da fam\u00edlia\ndata['fsize'] = data['parch'] + data['sibsp'] + 1","246ed1fd":"# criar coluna indicando se estava sozinho\ndata['alone'] = 0\ndata.loc[data.fsize == 1, 'alone'] = 1","304e8f94":"# criar coluna contendo o deque\ndata['deck'] = data['cabin'].str[:1]\ndata['deck'] = data['deck'].fillna('N').astype('category')\ndata['deck'] = data['deck'].cat.codes","4add9102":"# criar coluna contendo o n\u00famero do quarto\ndata['room'] = data['cabin'].str.extract(\"([0-9]+)\", expand=False)\ndata['room'] = data['room'].fillna(0).astype(int)","cadf7130":"data.head()","a160cf45":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\ndef evaluate_regression_model(model, X, y):\n    kfold = KFold(n_splits=10, random_state=42)\n    results = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error', verbose=1)\n    score = (-1) * results.mean()\n    stddev = results.std()\n    print(model, '\\nScore: %.2f (+\/- %.2f)' % (score, stddev))\n    return score, stddev","a08b64b3":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\nage_models = [\n#    ('LR', LinearRegression(n_jobs=-1, fit_intercept=True, normalize=True)),\n    ('GBR', GradientBoostingRegressor(random_state=42)),\n    ('RFR', RandomForestRegressor(random_state=42)),\n    ('XGB', XGBRegressor(random_state=42, objective='reg:squarederror')),\n#    ('MLP', MLPRegressor(random_state=42, max_iter=500, activation='tanh',\n#                         hidden_layer_sizes=(10,5,5), solver='lbfgs')),\n#    ('GPR', GaussianProcessRegressor(random_state=42, alpha=0.01, normalize_y=True))\n]","291e79e1":"# selecionar dados para o treino\n\ncols = ['pclass', 'sibsp', 'parch', 'fare', 'title', 'age', 'alone']\n#cols = ['pclass', 'sibsp', 'parch', 'fare', 'sex', 'embarked', 'title', 'age', 'alone']\n\ndata_age = data[cols].dropna()\n\nX_age = data_age.drop(['age'], axis=1)\ny_age = data_age['age']\n\ndata_age.head()","115d2b1b":"data_age.corr()","bf60b802":"names = []\nscores = []\nlowest = 999\nbest_model = None\n\nfor name, model in age_models:\n    \n    score, stddev = evaluate_regression_model(model, X_age, y_age)\n    names.append(name)\n    scores.append(score)\n    \n    if score < lowest:\n        best_model = model\n        lowest = score","515859d2":"results = pd.DataFrame({'Age Model': names, 'Score': scores})\nresults.sort_values(by='Score', ascending=True)","ce8b90b6":"age_model = best_model\nage_model.fit(X_age, y_age)","2ae19633":"# preencher dados faltantes de idade a partir de uma regress\u00e3o\ndata['age_pred'] = age_model.predict(data[cols].drop('age', axis=1))\ndata.loc[data.age.isnull(), 'age'] = data['age_pred']\ndata.drop('age_pred', axis=1, inplace=True)\ndata.head()","df9f3f1a":"# existem colunas com dados nulos?\ndata[data.columns[data.isnull().any()]].isnull().sum()","ec6660db":"data.head()","9e906573":"# realizar normaliza\u00e7\u00e3o nos dados num\u00e9ricos cont\u00ednuos\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n#cols = ['age', 'fare']\ncols = ['age', 'fare', 'parch', 'pclass', 'sibsp', 'fsize']\n#cols = ['age', 'embarked', 'fare', 'parch', 'pclass', 'sex', 'sibsp', 'title', 'fsize', 'deck']\n#cols = ['age', 'embarked', 'fare', 'parch', 'pclass', 'sex', 'sibsp', 'title', 'fsize', 'deck', 'room']\n\n#for col in cols:\ndata.loc[:,cols] = scaler.fit_transform(data.loc[:,cols])","8fe5ec6c":"data.head()","79218448":"# importar os pacotes necess\u00e1rios para os algoritmos de classifica\u00e7\u00e3o\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier","f3fc67f0":"from datetime import datetime\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\n# avalia o desempenho do modelo, retornando o valor da precis\u00e3o\ndef evaluate_classification_model(model, X, y):\n    start = datetime.now()\n    kfold = KFold(n_splits=10, random_state=42)\n    results = cross_val_score(model, X, y, cv=kfold, scoring='accuracy', verbose=1)\n    end = datetime.now()\n    elapsed = int((end - start).total_seconds() * 1000)\n    score = 100.0 * results.mean()\n    stddev = 100.0 * results.std()\n    print(model, '\\nScore: %.2f (+\/- %.2f) [%5s ms]' % (score, stddev, elapsed))\n    return score, stddev, elapsed","2c32ba01":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\n# faz o ajuste fino do modelo, calculando os melhores hiperpar\u00e2metros\ndef fine_tune_model(model, params, X, y):\n    print('\\nFine Tuning Model:')\n    print(model, \"\\nparams:\", params)\n    kfold = KFold(n_splits=10, random_state=42)\n    grid = GridSearchCV(estimator=model, param_grid=params, scoring='accuracy', cv=kfold, verbose=1)\n    grid.fit(X, y)\n    print('\\nGrid Best Score: %.2f' % (grid.best_score_ * 100.0))\n    print('Best Params:', grid.best_params_)\n    return grid","bc8d2d9e":"# definir dados de treino\ntrain_data = data[data.survived.isnull() == False]\n\n# selecionar atributos para o modelo\ncols = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'title', 'fsize', 'alone', 'deck']\n#cols = ['pclass', 'sex', 'age', 'fare', 'title', 'fsize', 'deck']\n#cols = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'title', 'fsize', 'alone', 'deck', 'room']\n\nX_train = train_data[cols]\ny_train = train_data['survived']\n\nprint('Forma dos dados de treino:', X_train.shape, y_train.shape)","3917d58c":"train_data.corr()","2a4faf06":"# definir dados de teste\ntest_data = data[data.survived.isnull()]\n\nX_test = test_data[cols]\n\nprint('Forma dos dados de teste:', X_test.shape)","2894ac3c":"names = []\nmodels = []\nscores = []\nstddevs = []\ntimes = []\n\ndef add_model_info(name, model, score, stddev, elapsed):\n    names.append(name)\n    models.append((name, model))\n    scores.append(score)\n    stddevs.append(stddev)\n    times.append(elapsed)","85756ad2":"model = LogisticRegression(random_state=42, solver='newton-cg', C=0.1, multi_class='auto', max_iter=500)\n\nparams = dict(\n    solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    C=np.logspace(-3, 3, 7)\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('LR', model, score, stddev, elapsed)","0721cff9":"model = DecisionTreeClassifier(random_state=42, criterion='entropy', max_depth=6, min_samples_split=0.25)\n\n#criterion=\u2019mse\u2019, splitter=\u2019best\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n#min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, \n#min_impurity_decrease=0.0, min_impurity_split=None, presort=False\n\nparams = dict(\n    criterion=['gini','entropy'],\n    max_depth=[4, 6, 8, 10, 12, 14],\n    min_samples_split=[0.25, 0.5, 0.75, 1.0]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('DT', model, score, stddev, elapsed)","c06c56c4":"model = LinearDiscriminantAnalysis(solver='lsqr')\n\n#solver=\u2019svd\u2019, shrinkage=None, priors=None,\n#n_components=None, store_covariance=False, tol=0.0001\n\nparams = dict(\n    solver=['svd', 'lsqr'] #, 'eigen']\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('LDA', model, score, stddev, elapsed)","9364190f":"model = GaussianNB(priors=None, var_smoothing=1e-8)\n\n#priors=None, var_smoothing=1e-09\n\nparams = dict(\n    priors=[None],\n    var_smoothing=[1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('NB', model, score, stddev, elapsed)","3e10482a":"model = KNeighborsClassifier(n_neighbors=11, weights='uniform')\n\n#n_neighbors=5, weights=\u2019uniform\u2019, algorithm=\u2019auto\u2019, leaf_size=30, p=2, metric=\u2019minkowski\u2019,\n#metric_params=None, n_jobs=None\n\nparams = dict(\n    n_neighbors=[1, 3, 5, 7, 9, 11, 13],\n    weights=['uniform', 'distance']\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('KNN', model, score, stddev, elapsed)","e121269e":"model = SVC(random_state=42, C=10, gamma=0.1, kernel='rbf')\n\n#kernel=\u2019rbf\u2019, degree=3, gamma=\u2019auto_deprecated\u2019, coef0=0.0, tol=0.001, C=1.0, \n#epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1\n\nparams = dict(\n    C=[0.001, 0.01, 0.1, 1, 10, 100],\n    gamma=[0.001, 0.01, 0.1, 1, 10, 100],\n    kernel=['linear', 'rbf']\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('SVM', model, score, stddev, elapsed)","213df135":"model = MLPClassifier(random_state=42, solver='lbfgs', alpha=1, hidden_layer_sizes=(100,), activation='logistic')\n                \n#hidden_layer_sizes=(100, ), activation=\u2019relu\u2019, solver=\u2019adam\u2019, alpha=0.0001, batch_size=\u2019auto\u2019, \n#learning_rate=\u2019constant\u2019, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n#random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n#early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10\n\nparams = dict(\n    alpha=[1,0.1,0.01,0.001,0.0001,0],\n    hidden_layer_sizes=[(100,), (50,), (50,2), (5,5,2), (10,5,2)],\n    activation=['identity', 'logistic', 'tanh', 'relu'],\n    solver=['lbfgs', 'sgd', 'adam']\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('MLP', model, score, stddev, elapsed)","ca4722f2":"model = AdaBoostClassifier(DecisionTreeClassifier(random_state=42), random_state=42, n_estimators=50)\n\n#base_estimator=None, n_estimators=50, learning_rate=1.0,\n#algorithm=\u2019SAMME.R\u2019, random_state=None\n\nparams = dict(\n    n_estimators=[10, 25, 50, 100]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('ABDT', model, score, stddev, elapsed)","75b3c9e6":"from sklearn.ensemble import BaggingClassifier\n\nmodel = BaggingClassifier(random_state=42, n_estimators=100,\n                          max_samples=0.25, max_features=0.8)\n\n#base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0,\n#bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False,\n#n_jobs=None, random_state=None, verbose=0\n\nparams = dict(\n    n_estimators=[10, 50, 100, 500],\n    max_samples=[0.25, 0.5, 0.75, 1.0],\n    max_features=[0.7, 0.8, 0.9, 1.0]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('BC', model, score, stddev, elapsed)","09b11cd8":"model = ExtraTreesClassifier(random_state=42, n_estimators=100, max_depth=7, \n                             min_samples_split=0.25, max_features='auto')\n\n#n_estimators=\u2019warn\u2019, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2,\n#min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, \n#max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n#bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0,\n#warm_start=False, class_weight=None\n\nparams = dict(\n    n_estimators=[10, 50, 100, 500],\n    max_depth=[None, 3, 7, 11],\n    min_samples_split=[0.25, 0.5],\n    max_features=['auto', 0.7, 0.85, 1.0]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('ET', model, score, stddev, elapsed)","df2a3efb":"model = GradientBoostingClassifier(random_state=42, n_estimators=100, max_features=0.75,\n                                   max_depth=4, learning_rate=0.1, subsample=0.6)\n\n#loss=\u2019ls\u2019, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=\u2019friedman_mse\u2019, min_samples_split=2,\n#min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n#min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, \n#max_leaf_nodes=None, warm_start=False, presort=\u2019auto\u2019, validation_fraction=0.1, n_iter_no_change=None, \n#tol=0.0001\n\nparams = dict(\n    n_estimators=[100, 250, 500],\n    max_features=[0.75, 0.85, 1.0],\n    max_depth=[4, 6, 8, 10],\n    learning_rate=[0.05, 0.1, 0.15],\n    subsample=[0.4, 0.6, 0.8]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('GB', model, score, stddev, elapsed)","51b4968b":"model = RandomForestClassifier(random_state=42, n_estimators=100, max_features='auto', max_depth=5)\n\n#n_estimators=\u2019warn\u2019, criterion=\u2019mse\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n#min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, \n#min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, \n#verbose=0, warm_start=False\n\nparams = dict(\n    n_estimators=[10, 50, 100, 500],\n    max_features=['auto', 'sqrt', 'log2'],\n    max_depth=[None, 3, 5, 7, 9, 11, 13]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('RF', model, score, stddev, elapsed)","082d5949":"model = XGBClassifier(max_depth=3, n_estimators=100)\n\n#max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:squarederror',\n#booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, \n#colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, \n#base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain'\n\nparams = dict(\n    max_depth=[3, 5, 7, 9],\n    n_estimators=[50, 75, 100, 200]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('XGB', model, score, stddev, elapsed)","53160407":"estimators =  [\n    ('RF', RandomForestClassifier(random_state=42, n_estimators=100, max_features='auto', max_depth=5)),\n    ('BC', BaggingClassifier(random_state=42, n_estimators=100, max_samples=0.25, max_features=0.8)),\n    ('GB', GradientBoostingClassifier(random_state=42, max_depth=4, max_features=0.75,\n                                   n_estimators=100, learning_rate=0.1, subsample=0.6)),\n#    ('XGB', XGBClassifier(max_depth=3, n_estimators=100)),\n]\nmodel = VotingClassifier(estimators, n_jobs=-1, weights=(2,1,1))\n\n#estimators, weights=None, n_jobs=None\n\nparams = dict(\n    weights=[(1,1,1), (2,1,1), (3,1,1), (3,2,1), (2,2,1), (2,1,2), (5,4,3), (1,2,1), (1,1,2), ]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('VC', model, score, stddev, elapsed)","52d516a9":"model = RandomForestClassifier(random_state=42, max_features='auto', n_estimators=100)\nmodel.fit(X_train, y_train)\n\nimportances = pd.DataFrame({'feature': X_train.columns,\n                            'importance': np.round(model.feature_importances_, 3)})\nimportances = importances.sort_values('importance', ascending=False).set_index('feature')\nimportances.head(20)","94331eac":"importances.plot.bar()","588d9d93":"results = pd.DataFrame({'Algorithm': names, 'Score': scores, 'Std Dev': stddevs, 'Time (ms)': times})\nresults.sort_values(by='Score', ascending=False)","883bcc9c":"# criar diret\u00f3rio para os arquivos de envio\n#!test -d submissions || mkdir submissions","2b104c35":"prefixo_arquivo = 'titanic-submission'\nsufixo_arquivo = '06set'\n\nfor name, model in models:\n    print(model, '\\n')\n    \n    # treinar o modelo\n    model.fit(X_train, y_train)\n    \n    # executar previs\u00e3o usando o modelo\n    y_pred = model.predict(X_test)\n    vfunc = np.vectorize(lambda x: 'yes' if x > 0 else 'no')\n\n    # gerar dados de envio (submiss\u00e3o)\n    submission = pd.DataFrame({\n      'person': X_test.index,\n      'survived': vfunc(y_pred)\n    })\n    submission.set_index('person', inplace=True)\n\n    # gerar arquivo CSV para o envio\n    filename = '%s-p-%s-%s.csv' % (prefixo_arquivo, sufixo_arquivo, name.lower())\n    submission.to_csv(filename)","cc5db93c":"!head *.csv","997faaf5":"# gerar \"one hot encoding\" em atributos categ\u00f3ricos\n#cols = ['pclass', 'sex', 'embarked']\ncols = ['embarked', 'pclass', 'title', 'deck']\ndata = pd.get_dummies(data, columns=cols)","b8455ea3":"### Ensemble Methods","076384d3":"model = LinearSVC(random_state=42, max_iter=1000, C=10)\n\nparams = dict(\n    C=[0.001, 0.01, 0.1, 1, 10, 100]\n)\n","d4b9a0d0":"from lightgbm import LGBMClassifier\nmodel = LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss',\n                      boosting_type='goss', n_estimators=50, max_depth=11)\n\nparams = dict(\n    boosting_type=['gbdt', 'dart', 'goss'], #rf\n    objective=['binary'],\n    metric=['binary_logloss'],\n    #bagging_freq=[1, 5, 10],\n    #bagging_fraction=[0.25, 0.5],\n    n_estimators=[50, 75, 100, 200],\n    max_depth=[3, 5, 7, 9, 11]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('LGBM', model, score, stddev, elapsed)","701097a0":"## Transforma\u00e7\u00f5es nos dados","3111bf8a":"### Support Vector Machines","64d46587":"import matplotlib.pyplot as plt\n%matplotlib inline","97da9ddf":"### Outros algoritmos","543b1cb4":"prefixo_arquivo = 'submissions\/titanic-submission'\nsufixo_arquivo = '08jul'\nname = 'lgbm_' + params['boosting_type']\n\nprint(model, '\\n')\n\n# executar previs\u00e3o usando o modelo\ny_pred = clf.predict(X_test.values)\nvfunc = np.vectorize(lambda x: 'yes' if x > 0.5 else 'no')\n\n# gerar dados de envio (submiss\u00e3o)\nsubmission = pd.DataFrame({\n  'person': X_test.index,\n  'survived': vfunc(y_pred)\n})\nsubmission.set_index('person', inplace=True)\n\n# gerar arquivo CSV para o envio\nfilename = '%s-p-%s-%s.csv' % (prefixo_arquivo, sufixo_arquivo, name.lower())\nsubmission.to_csv(filename)","c093c15b":"## Gerar arquivos com resultados","9304cfc7":"prefixo_arquivo = 'submissions\/titanic-submission'\nsufixo_arquivo = '07jul'\nname = 'mlptf'\n\nprint(model, '\\n')\n\n# executar previs\u00e3o usando o modelo\ny_pred = model.predict(X_test).ravel()\nvfunc = np.vectorize(lambda x: 'yes' if x > 0.5 else 'no')\n\n# gerar dados de envio (submiss\u00e3o)\nsubmission = pd.DataFrame({\n  'person': X_test.index,\n  'survived': vfunc(y_pred)\n})\nsubmission.set_index('person', inplace=True)\n\n# gerar arquivo CSV para o envio\nfilename = '%s-p-%s-%s.csv' % (prefixo_arquivo, sufixo_arquivo, name.lower())\nsubmission.to_csv(filename)","ef6ce9f3":"### Neural network models","10ab7c51":"### Inferir idades faltantes dos passageiros","0262b1d1":"## Carga dos dados","9dd050bd":"## Compara\u00e7\u00e3o final entre os algoritmos","5583e511":"### Na\u00efve Bayes","28fd18af":"### Ensemble Learning Model\n\n- https:\/\/towardsdatascience.com\/automate-stacking-in-python-fc3e7834772e\n- https:\/\/github.com\/vecxoz\/vecstack","5c54f889":"# criar colunas adicionais\ndata['ageclass'] = data['age'] * data['pclass']\ndata['pfare'] = data['fare'] \/ data['fsize']","f4432360":"import lightgbm as lgb\n\nd_train = lgb.Dataset(X_train.values, label=y_train.values)\n\nparams = {}\nparams['random_state'] = 42\nparams['objective'] = 'binary'\nparams['boosting_type'] = 'goss' # gbdt, rf, dart, goss\n\n#params['learning_rate'] = 0.003\n#params['metric'] = 'binary_logloss'\n#params['sub_feature'] = 0.5\n#params['num_leaves'] = 10\n#params['min_data'] = 50\n#params['max_depth'] = 10\n\n#model = lgb.train(params, d_train, 100)\nmodel = lgb.cv(params, d_train, 100, nfold=10)","94bd4f78":"### Discriminant Analysis","7b75e7ac":"model = SGDClassifier(random_state=42, max_iter=100, tol=0.1)\n\nparams = {'max_iter':[100, 200, 350, 500, 1000], 'tol':[0.1, 0.01]}\n","5f883232":"def plot_line(loss, ylabel, xlabel='Epochs'):\n    fig = plt.figure()\n    plt.plot(loss)\n    plt.ylabel(ylabel)\n    plt.xlabel(xlabel)\n    plt.show()\n    plt.show()","678f8e9a":"plot_line(hist.history['loss'], 'loss')","ae6a46b5":"### Generalized Linear Models","61297bc3":"plot_line(hist.history['acc'], 'accuracy')","0a605bb2":"## Importa\u00e7\u00e3o dos pacotes","6175b53e":"#### LightGBM\n\n- https:\/\/github.com\/microsoft\/LightGBM\n- https:\/\/lightgbm.readthedocs.io\/en\/latest\/\n- https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc","d488e179":"## Avalia\u00e7\u00e3o e ajuste fino de cada modelo preditivo\n\n-  https:\/\/scikit-learn.org\/stable\/modules\/classes.html","dec08fb1":"### Decision Trees","a7e37bbe":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\n# Abaixo \u00e9 criado um classificador MLP com 20 neur\u00f4nios na camada intermedi\u00e1ria e dropout de 50%. Foi utilizada a fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigmoid, a fun\u00e7\u00e3o de loss mean squared error e o otimizador RMSProp. O treinamento \u00e9 realizado por 200 \u00e9pocas com batch size de 64.\n\nmodel = Sequential()\nmodel.add(Dense(20, input_dim=X_train.shape[1], activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])\n\nprint(model.summary())","31360359":"## Modelagem preditiva","96630f22":"#### XGBoost\n\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn","ff8d2ae7":"### Nearest Neighbors","4cfd41e2":"model = Perceptron(random_state=42, max_iter=100, tol=0.01)\n\nparams = dict(\n    max_iter=[100, 200, 350, 500, 1000],\n    tol=[0.1, 0.01, 0.001]\n)\n","c9101dbc":"loss, accuracy = model.evaluate(v_X, v_y)\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","c9302768":"## Avaliar import\u00e2ncia dos atributos no modelo","5604f92d":"hist = model.fit(X_train, y_train, batch_size=64, nb_epoch=200, verbose=False)","a2b6db9b":"!pip install lightgbm"}}