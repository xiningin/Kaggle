{"cell_type":{"ffa2c629":"code","98ef44cb":"code","b3419c36":"code","5aa0956b":"code","421d8998":"code","6938e4e5":"markdown","ce304ba5":"markdown","247fe4c9":"markdown","5e125fdb":"markdown"},"source":{"ffa2c629":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KernelDensity\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline","98ef44cb":"class RegressorConditional:\n    def get_o_cat(self, o):\n        return np.sum([o>pct for pct in self.percentiles], axis=0)\n    def __init__(self, model=ExtraTreesRegressor(\n        n_estimators=500, n_jobs=-1, bootstrap=True, oob_score=True)):\n        self.model = model\n    def fit(self, X, y):\n        targ = np.where(y>=0, np.log(1+np.abs(y)), -np.log(1+np.abs(y)))\n        self.model.fit(X, targ)\n        o = self.model.oob_prediction_\n        self.percentiles = np.percentile(o, list(range(10, 100, 10)))\n        o_cat = self.get_o_cat(o)\n        self.dist = {}\n        for oc in range(len(self.percentiles) + 1):\n            filt = [oi==oc for oi in o_cat]\n            kde = KernelDensity(kernel='exponential', metric='manhattan', bandwidth=0.3)\n            kde.fit(list(zip(y[filt])))\n            self.dist[oc] = np.exp(kde.score_samples(list(zip(range(-99, 100)))))\n            self.dist[oc] \/= sum(self.dist[oc])\n    def predict_proba(self, X):\n        o = self.model.predict(X)\n        o_cat = self.get_o_cat(o)\n        return np.array([self.dist[oc] for oc in o_cat])","b3419c36":"df = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory=False).select_dtypes(include=np.number)\ndf_play = df[df.NflId==df.NflIdRusher].copy()\n\nfeatures = df_play.drop('Yards', axis=1).select_dtypes(include=np.number).columns.tolist()","5aa0956b":"model = RegressorConditional()\nmodel.fit(df_play[features].fillna(-999), df_play.Yards)\n\nplt.figure(figsize=(12, 4))\nfor oc in model.dist:\n    plt.plot(model.dist[oc], label=oc)\nplt.xticks(list(range(-1, 200, 25)), list(range(-100, 101, 25)))\nplt.legend()\nplt.show()","421d8998":"from kaggle.competitions import nflrush\n\nnames = dict(zip(range(199), ['Yards%d' % i for i in range(-99, 100)]))\n\nenv = nflrush.make_env()\nfor df_test, _ in env.iter_test():\n    env.predict(pd.DataFrame([np.clip(np.cumsum(\n        model.predict_proba(df_test[df_test.NflId==df_test.NflIdRusher][features].fillna(-999))\n    ), 0, 1)]).rename(names, axis=1))\nenv.write_submission_file()","6938e4e5":"**With regression, we need to somehow translate the Yards predictions into a probability distribution. The class below implements another way to achieve this: it cuts the regressor predictions into (10) parts, and then directly estimates the conditional distribution of Yards given the predictions.**\n\nIt is important to use holdout predictions that do not overfit the train set. Note that the Yards are log-transformed to have a nicer distribution for the regression target, and that conditional distributions are slightly smoothed by a kernel density estimator.","ce304ba5":"**Train the model and have a look at the conditional distributions**","247fe4c9":"**Apply it to the test set**","5e125fdb":"> **Use all numeric features without any preprocessing**"}}