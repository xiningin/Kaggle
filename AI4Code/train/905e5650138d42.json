{"cell_type":{"73db345c":"code","81c9b9e1":"code","b5633338":"code","d19c5d99":"code","2338b5b6":"markdown","1ea7bb0b":"markdown","51b24275":"markdown","7317c18b":"markdown","80c38ce7":"markdown","46194b58":"markdown","d197851a":"markdown"},"source":{"73db345c":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\n# import some data to play with\niris = datasets.load_iris()\n\n# we only take the first two features. We could avoid this ugly slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']) #for background light color\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])  #for scatter points color\n\nfor k in [1, 5, 10]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(metric='euclidean', n_neighbors=k, weights='uniform')\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    # np.meshgrid - Make N-D coordinate arrays for vectorized evaluations of N-D scalar\/vector fields \n    # over N-D grids, given one-dimensional coordinate arrays x1, x2,\u2026, xn\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i, weights = 'uniform')\" % (k))\n\nplt.show()","81c9b9e1":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\n\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\ny = np.sin(X).ravel()\n\n# Add noise to targets\ny[::5] += 1 * (0.5 - np.random.rand(8))\n\n# Fit regression model\nn_neighbors = 5\nweights = 'uniform'\nknn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n\n# Now predict for values in T\nT = np.linspace(0, 5, 500)[:, np.newaxis]\ny_ = knn.fit(X, y).predict(T)\n\nplt.scatter(X, y, c='k', label='data')\nplt.plot(T, y_, c='g', label='prediction')\nplt.legend()\nplt.title(\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors, weights))\n\nplt.show()","b5633338":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\n\ndef make_meshgrid(x, y, h=.02):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\n# import some data to play with\niris = datasets.load_iris()\n# Take the first two features. We could avoid this by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nmodels = (svm.SVC(kernel='linear', C=C),\n          svm.LinearSVC(C=C, max_iter=10000),\n          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))\nmodels = (clf.fit(X, y) for clf in models)\n\n# title for the plots\ntitles = ('SVC with linear kernel',\n          'LinearSVC (linear kernel)',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel')\n\n# Set-up 2x2 grid for plotting.\nfig, sub = plt.subplots(2, 2)\nplt.subplots_adjust(wspace=0.4, hspace=0.4)\n\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nfor clf, title, ax in zip(models, titles, sub.flatten()):\n    plot_contours(ax, clf, xx, yy,\n                  cmap=plt.cm.coolwarm, alpha=0.8)\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xlabel('Sepal length')\n    ax.set_ylabel('Sepal width')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(title)\n\nplt.show()","d19c5d99":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 8, random_state=0)\niris = load_iris()\ncross_val_score(clf, iris.data, iris.target, cv=10)","2338b5b6":"**Linear Support Vector Machine**<br>\nClassifier margin is the maximum width the decision boundary area can be increased before hitting a data point. The linear classifier with maximum margin is a linear Support Vector Machine (LSVM). It is versatile as different Kernel functions can be specified for the decision function<br>\nParameters to be looked at:<br>\n1.C : Optional - default=1. The strength of regularization is determined by C. Larger values of C represent less regularization and will cause the model to fit the training set with these few errors as possible. Small values of C represents more regularization and more tolerant of errors on individual data points<br>\n2.tol : Tolerance for stopping criteria. Optional - default=1e-4. \n<br><br>\nRemarks:<br>\n1. Effective in high dimensional spaces. Still effective in cases where number of dimensions is greater than the number of samples.\n2. If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n3. Scales well to very large datasets.\n4. Works well with sparse data.","1ea7bb0b":"**K - Nearest Neighbors Classification**\n    \nSteps:\n1. Distance between the test data point with every training data point is computed. For computing the distance measures such as Euclidean distance, Hamming distance, etc. can be used.\n2. Model slects K nearest entries in the dataset which are closest to the test data point.\n3. Predict the label for test data point. eg. through majority vote \n\nParameters to be looked at:\n1. A distance metric: Typically Euclidean, Minkowski, etc.\n2. How many 'nearest' neighbors [n_neighbors] to look at? e.g. five (default)\n3. Optional weighting function on the neighbor points? The default value, weights = 'uniform', assigns uniform weights to each neighbor. weights = 'distance' assigns weights proportional to the inverse of the distance from the query point.\n4. How to aggregate the classes of neighbor points? Eg. Simple majority vote\n5. Algorithm is controlled through the keyword 'algorithm', which must be one of ['auto', 'ball_tree', 'kd_tree', 'brute']. When the default value 'auto' is passed, the algorithm attempts to determine the best approach from the training data.\n\nRemarks:\n1. A low value of \u201cK\u201d (close to 1) is more likely to overfit the training data and lead to worse accuracy on the test data, compared to higher values of \u201cK\u201d\n2. Not good for sparse data (with lot of features)\n3. Setting \u201ck\u201d to the number of points in the training set will result in a classifier that always predicts the majority class.\n4. K-nearest neighbors makes few assumptions about the structure of the data and gives potentially accurate but sometimes unstable predictions (sensitive to small changes in the training data)","51b24275":"This is part 2 of series ML Algos and how to use them. <br>\nIf you have not checked out part 1, kindly visit [ML Algos Part 1](https:\/\/www.kaggle.com\/nee2shaji\/for-novices-ml-algos-how-to-use-them-part-1) <br><br>\nThis Part 2 consists of following models\n* K - Nearest Neighbors Classification\n* Near Neighbours Regressor\n* Linear SVM\n* Kernalized SVM\n* Decision Tree\n","7317c18b":"**Kernalized SVM**<br>\nCan provide more complex models that can go beyond linear decision boundaries. They can be used for both classification and regression. In essence, one way to think about what kernelized SVMs do, is they take the original input data space and transform it to a new higher dimensional feature space, where it becomes much easier to classify the transform to data using a linear classifier<br>\nTypically work well for both low and high-dimensional data. On the negative side as the training set size increases, the run time, speed, and memory usage in the SVM training phase also increases. So for a large datasets with hundreds of thousands, or \nmillions of instances, an SVM may become less practical.<br><br>\n\nParameters to be looked at:<br>\n\n    3 parameters = kernel(polynomial, default: rbf), gamma(RBF kernel width), C(regularization parameter). \n    Typically C and gamma are tuned at the same time.<br>\n    \n1.RBF kernel has a parameter gamma. Gamma controls how far the influence of a single trending example reaches, which in turn affects how tightly the decision boundaries end up surrounding points in the input space. <br>\n2.Small gamma means a larger similarity radius. So that points farther apart are considered similar. Which results in more points being group together and smoother decision boundaries. <br>\n3.Small values of gamma give broader, smoother decision regions. While larger values of gamma give smaller, more complex decision regions.<br>\n4.If gamma is large, then C will have little to no effect. Well, if gamma is small, the model is much more constrained and \nthe effective C will be similar to how it would affect a linear classifier. <br>\n\n<br><br>\nRemarks:<br>\n1. The most important thing to remember when applying SVMs is that it's important to normalize the input data, so that all the features have comparable units that are on the same scale.\n2. For the radial basis function kernel, the similarity between two points and the transformed feature space is an exponentially decaying function of the distance between the vectors and the original input space. \n3. The kernelized support vector machine tries to find the decision boundary with maximum margin between classes using a linear classifier in the transformed feature space not the original input space.\n4. Difficult to interpret why a particular prediction was made\n\n","80c38ce7":"References:<br>\n* https:\/\/scikit-learn.org\/\n* Applied Machine Learning by Kevyn Collins-Thompson","46194b58":"**Near Neighbours Regressor**<br>\nKNeighborsRegressor implements learning based on the nearest neighbors of each query point, where is an integer value specified by the user. <br>\nRadiusNeighborsRegressor implements learning based on the neighbors within a fixed radius of the query point, where is a floating-point value specified by the user.<br>\nThe parameters to be tuned are almost same as K nearest neighbors classifier.","d197851a":"**Decision Tree**<br>\nCan be used for both regression and classification.<br><br>\nParameters to be looked at:<br>\n\n    3 key parameters:\n    max_depth - controls maximum depth (number of split points). Most common way to reduce tree complexity and overfitting.\n    min_samples_leaf - threshold for the minimum number of data instances a leaf can have to avoid further splitting.\n    max_leaf_nodes - limits the total number of nodes that are leaves in the tree\nIn practice, adjusting only one of these trees is typically enough to control most overfitting<br><br>\nRemarks:<br>\n1. The decision tree implementation and scikit-learn only implements pre-pruning. We can control tree complexity via pruning by limiting either the maximum depth of the tree using the max depth parameter or the maximum number of leafnodes using the max leafnodes parameter. We could also set a threshold on the minimum number of instances that must be in a node to consider splitting it. \n2. The decision algorithm can operate on the original training data pretty much as is. So decision trees tend to work well with data sets that have a mixture of feature types-- binary, categorical or continuous and with features that are on very different scales.\n3. No feature normalization or scaling typically needed.\n4. Works well with datasets using a mixture of feature types (continuous, categorical, binary)\n\n"}}