{"cell_type":{"3b882403":"code","122a8ec1":"code","0b9c9e06":"code","555d8ce9":"code","b2eab392":"code","69db68b3":"code","89207f33":"code","bd440e03":"code","39886c74":"code","a51dda14":"code","7ed6dd25":"code","19c6e0d6":"code","c047e0b2":"code","92e7108f":"code","7a87fb47":"code","e1e11976":"code","010c1c87":"code","2b4aa7a5":"code","4f035b6b":"code","97252be3":"code","d2e7b48f":"code","06d83ece":"code","a0cd20bd":"code","10b375d1":"code","d0bd5b3f":"code","b076f8ca":"code","c1f70cbb":"code","554e141d":"markdown","e9b0bf49":"markdown","4d478d4d":"markdown","c9ffcbc4":"markdown","4a28b959":"markdown","2d68418a":"markdown","cc33c955":"markdown","391f92b1":"markdown","f72af2b3":"markdown","c458e279":"markdown","8283d03e":"markdown","5f2073c6":"markdown","3df1f151":"markdown","9635667b":"markdown","4867c6ff":"markdown","05f05e1b":"markdown","a7624d5f":"markdown"},"source":{"3b882403":"import pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.pipeline import Pipeline\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, roc_auc_score\nfrom sklearn.tree import export_graphviz\nimport warnings \nwarnings.filterwarnings('ignore')","122a8ec1":"def plot_roc_curve(fprs, tprs):\n    \n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(8, 8))\n    \n    # Plotting ROC for each fold and computing AUC scores\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs), 1):\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC Fold {} (AUC = {:.3f})'.format(i, roc_auc))\n        \n    # Plotting ROC for random guessing\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8, label='Random Guessing')\n    \n    mean_tpr = np.mean(tprs_interp, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    \n    # Plotting the mean ROC\n    ax.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC (AUC = {:.3f} $\\pm$ {:.3f})'.format(mean_auc, std_auc), lw=2, alpha=0.8)\n    \n    # Plotting the standard deviation around the mean ROC Curve\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='$\\pm$ 1 std. dev.')\n    \n    ax.set_xlabel('False Positive Rate', size=15, labelpad=20)\n    ax.set_ylabel('True Positive Rate', size=15, labelpad=20)\n    ax.tick_params(axis='x', labelsize=15)\n    ax.tick_params(axis='y', labelsize=15)\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n\n    ax.set_title('ROC Curves of Folds', size=20, y=1.02)\n    ax.legend(loc='lower right', prop={'size': 13})\n    \n    plt.show()","0b9c9e06":"def plot_feature_importances(feature_importances, title, feature_names):\n    feature_importances = 100.0*(feature_importances\/max(feature_importances))\n    index_sorted = np.flipud(np.argsort(feature_importances))\n    pos = np.arange(index_sorted.shape[0])+0.5\n    \n    fig, ax = plt.subplots(figsize=(16,4))\n    plt.bar(pos,feature_importances[index_sorted])\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(90)\n    plt.xticks(pos,feature_names[index_sorted])\n    plt.ylabel('Relative Importance')\n    plt.title(title)\n    plt.show() ","555d8ce9":"df_train = pd.read_csv('\/kaggle\/input\/rs6-attrition-predict\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/rs6-attrition-predict\/test.csv')","b2eab392":"def extract_features(df, is_train=False):\n    # target\n    y = pd.DataFrame()\n    if is_train:\n        attrition_dict = {'No':0,'Yes':1}\n        df['Attrition'] = df['Attrition'].map(lambda x: attrition_dict[x])\n        y = df.Attrition\n        df.drop(['Attrition'], axis=1, inplace=True)\n    else:\n        y = df.user_id\n    df.drop(['user_id'], inplace=True, axis=1)\n    # BusinessTravel\n    businesstravel_dict = {'Non-Travel':0, 'Travel_Rarely':1, 'Travel_Frequently':2}\n    df['BusinessTravel'] = df['BusinessTravel'].map(lambda x: businesstravel_dict[x])\n    # Department\n    department_dict = {'Sales':0, 'Research & Development':1, 'Human Resources':2}\n    df['Department'] = df['Department'].map(lambda x: department_dict[x])\n    # EducationField\n    educationfield_dict = {'Life Sciences':0, 'Medical':1, 'Marketing':2, 'Technical Degree':3, 'Human Resources':4, 'Other':5}\n    df['EducationField'] = df['EducationField'].map(lambda x: educationfield_dict[x])\n    # Gender\n    gender_dict = {'Male':0, 'Female': 1}\n    df['Gender'] = df['Gender'].map(lambda x: gender_dict[x])\n    # JobRole\n    jobrole_dict = {'Sales Executive':0, \n                    'Research Scientist':1, \n                    'Laboratory Technician':2, \n                    'Manufacturing Director':3, \n                    'Healthcare Representative':4,\n                    'Manager':5, \n                    'Sales Representative':6,\n                    'Research Director':7,\n                    'Human Resources':8\n                   }\n    df['JobRole'] = df['JobRole'].map(lambda x: jobrole_dict[x])\n    # MaritalStatus\n    maritalstatus_dict = {'Single':0, 'Married':1, 'Divorced':2}\n    df['MaritalStatus'] = df['MaritalStatus'].map(lambda x: maritalstatus_dict[x])\n    # Over18\n    df.drop(['Over18'], inplace=True, axis=1)\n    # EmployeeNumber\n    df.drop(['EmployeeNumber'], inplace=True, axis=1)\n    # OverTime\n    overtime_dict = {'Yes':0, 'No':1}\n    df['OverTime'] = df['OverTime'].map(lambda x: overtime_dict[x])\n    return y, df","69db68b3":"target, train = extract_features(df_train, True)\nuser_id, test = extract_features(df_test, False)\ndel df_train\ndel df_test","89207f33":"def get_optimizer_params(model, model_params, train=train, test=test):\n    gridsearch = GridSearchCV(model, model_params, scoring='roc_auc', cv=5)\n    gridsearch.fit(train, target)\n    best_score = gridsearch.best_score_\n    print(\"Best score: %0.3f\" % best_score)\n    print(\"Best parameters set:\")\n    best_parameters = gridsearch.best_estimator_.get_params()\n    for param_name in sorted(gridsearch.param_grid.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n#     print(best_parameters)\n    return best_parameters\n\ndef get_model_result(model, model_params, model_name, scaler=None, test_size=0.5, train=train, test=test, feature_importance=False, gridsearch=True):\n    if scaler is not None:\n        train = scaler.fit_transform(train)\n        test = scaler.fit_transform(test)\n\n    if gridsearch is True:\n        best_params = get_optimizer_params(model, model_params)\n        model.set_params(**best_params)\n    X_train, X_val, y_train, y_val = train_test_split(train, target, test_size=test_size, random_state=2020, stratify=target)\n    model.fit(X_train, y_train)\n    prob_y_val = model.predict_proba(X_val)[:,1] if hasattr(model, 'predict_proba') else model.predict(X_val)\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_val, prob_y_val)\n    best_score = auc(trn_fpr, trn_tpr)\n    best_parameters = model.get_params()\n    plot_roc_curve([list(trn_fpr)], [list(trn_tpr)])\n        \n        \n    if feature_importance is True:\n        plot_feature_importances(model.feature_importances_, 'Importance of Features', train.columns)\n    result = pd.DataFrame()\n    result['user_id'] = user_id\n    result['Attrition'] = pd.DataFrame(model.predict_proba(test)[:,1] if hasattr(model, 'predict_proba') else model.predict(test))\n    result[['user_id', 'Attrition']].to_csv(f'result-{model_name}.csv', index=False, float_format='%.8f')\n    print('result of predict:\\n', result.head())\n    return best_score, best_parameters","bd440e03":"data = pd.concat([train, test]).corr() ** 2\ndata = np.tril(data, k=-1)\ndata[data==0] = np.nan","39886c74":"figure, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(np.sqrt(data), annot=False, cmap='viridis', ax=ax)","a51dda14":"data = train.corrwith(target).agg('square')\n\nfigure, ax = plt.subplots(figsize=(10, 10))\ndata.agg('sqrt').plot.bar(ax=ax)\ndel data","7ed6dd25":"model = Ridge()\nmodel_params = {'alpha': [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 7], 'tol': [0.0001, 0.001, 0.01, 0.1]}\nmodel_params = {'alpha': [2], 'tol': [0.0001]}\nstart = time.time()\nbest_score_ridge, best_params_ridge = get_model_result(model, model_params, 'ridge', StandardScaler())\nprint(time.time()-start)\ndel model","19c6e0d6":"model = Lasso()\nmodel_params = {'alpha': np.logspace(-10, -6, 50)}\nmodel_params = {'alpha': [1e-10]}\nstart = time.time()\nbest_score_lasso, best_params_lasso = get_model_result(model, model_params, 'lasso', StandardScaler())\nprint(time.time() - start)\ndel model","c047e0b2":"model = ElasticNet()\nmodel_params = {'alpha': np.logspace(-10, -4, 10), 'l1_ratio': np.logspace(-10, -4, 10)}\nmodel_params = {'alpha': [0.0001], 'l1_ratio': [1e-10]}\nstart = time.time()\nbest_score_elasticnet, best_params_elasticnet = get_model_result(model, model_params, 'elasticnet', StandardScaler())\nprint(time.time() - start)\ndel model ","92e7108f":"model = LogisticRegression(class_weight='balanced')\nmodel_params = {\n    'penalty': ['l1', 'l2'], \n    'C': [0.4, 0.5, 0.6],\n    'solver':['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']\n}\nmodel_params = {\n    'penalty': ['l1'], \n    'C': [0.5],\n    'solver':['liblinear']\n}\nstart = time.time()\nbest_score_lgr,best_params_lgr = get_model_result(model, model_params,'logisticregression', StandardScaler()) \nprint(time.time() - start)\ndel model ","7a87fb47":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))\nfig,ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,4))\nz = np.linspace(-3,3,num=100)\nax[0].plot(z,-np.log(sigmoid(z)),label='logistic',lw=2, color='b')\nax[1].plot(z,-np.log(1-sigmoid(z)),label='logistic',lw=2,color='b')\nax[0].plot(z,np.maximum(0,1-z),label='SVM',lw=2, color='r',linestyle='--')\nax[1].plot(z,np.maximum(0,1+z),label='SVM',lw=2,color='r',linestyle='--')\nax[0].set_title('y=1')\nax[1].set_title('y=0')\nax[0].set_xlabel('z')\nax[1].set_xlabel('z')\nax[0].set_ylabel('individual loss')\nax[1].set_ylabel('individual loss')\nax[0].legend()\nax[1].legend()\nplt.show()","e1e11976":"x = np.linspace(-4,4,num=100)\nl = 0\ngamma1=0.5\nf1 = np.exp(-gamma1*(x-l)*(x-l))\ngamma2=5\nf2 = np.exp(-gamma2*(x-l)*(x-l))\nplt.plot(x,f1,label=r'$\\gamma = 0.5$')\nplt.plot(x,f2,label=r'$\\gamma = 5$')\nplt.legend(fontsize = 14)\nplt.xlabel('x',fontsize = 14)\nplt.ylabel('similarity', fontsize = 14)\nplt.arrow(0,0.2,0,-0.18, head_width=0.2, head_length=0.05,lw=1,color='indianred')\nplt.text(-0.7,0.22,'landmark', color='indianred', fontsize=14)\nplt.show()","010c1c87":"# model = SVC()\n# model_params = {\n#     'C':[0.1,1,5,10,50,100, 200],\n#     'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n#     'gamma':[1,0.1,0.01,0.001]\n# }\n# best_score_svc, best_params_svc = get_model_result(model, model_params, 'SVC', StandardScaler())","2b4aa7a5":"tree = DecisionTreeClassifier(max_depth=2, random_state=2020)\ntree.fit(train, target)\ndot_data = export_graphviz(tree,\n                out_file=None,\n                feature_names=train.columns,\n                class_names=['Yes', 'No'],\n                rounded=True,\n                filled=True)\n\ngraph = graphviz.Source(dot_data)\ngraph.render() \ngraph","4f035b6b":"model = DecisionTreeClassifier(random_state=2020)\nmodel_params = {\n    'max_features': [0.8, 1.0], \n    'max_depth': [8, 9, 10],\n    'min_samples_leaf':[30, 40, 50]\n} \nmodel_params = {\n    'max_features': [1.0], \n    'max_depth': [9],\n    'min_samples_leaf':[30]\n}\nstart = time.time()\nbest_score_dtc, best_params_dtc = get_model_result(model, model_params, 'dtc', None, feature_importance=True)\nprint(time.time() - start)\ndel model  ","97252be3":"model = RandomForestClassifier(random_state=2020)\nmodel_params = {\n    'n_estimators':[500],\n    'n_jobs':[-1],\n    'max_features': [0.5, 0.6], \n    'max_depth': [8, 9, 10],\n    'min_samples_leaf':[8, 10],\n#     'random_state':[2020]\n}\nmodel_params = {\n    'n_estimators':[200],\n    'max_features': [0.6], \n    'max_depth': [10],\n    'min_samples_leaf':[10],\n}\nstart = time.time()\nbest_score_rfc, best_params_rfc = get_model_result(model, model_params, 'rfc', feature_importance=True)\nprint(time.time() - start)\ndel model ","d2e7b48f":"# rfc = RandomForestClassifier(best_params_rfc)\n# svm = SVC(best_params_svc)\n# lr = LogisticRegression(best_params_logisticregression)\n# best_score_voting, best_params_voting = get_model_result(VotingClassifier(estimators = [('rf',rfc), ('svm',svm), ('log', lr)], voting='soft'), 'voting', StandardScaler(), gridsearch=False)","06d83ece":"best_score_bagging, best_params_bagging = get_model_result(BaggingClassifier(LogisticRegression(**best_params_lgr),n_estimators=500, random_state=2020), 'bagging', StandardScaler(), gridsearch=False)\nbest_score_pasting, best_params_pasting = get_model_result(BaggingClassifier(LogisticRegression(**best_params_lgr),n_estimators=500, bootstrap_features=True, max_features=1.0, random_state=2020), 'pasting', StandardScaler(), gridsearch=False)","a0cd20bd":"model = ExtraTreesClassifier(random_state=2020)\nmodel_params = {\n    'n_estimators':[500],\n    'n_jobs':[-1], \n    'max_features': [0.5,0.6,0.7,0.8,0.9,1.0], \n    'max_depth': [10,11,12,13,14],\n    'min_samples_leaf':[1,10,100],\n#     'random_state':[0]\n} \nmodel_params = {\n    'n_estimators':[500],\n    'max_features': [0.5], \n    'max_depth': [12],\n    'min_samples_leaf':[10],  \n}\nstart = time.time()\nbest_score_etf, best_params_etf = get_model_result(model, model_params, 'etc',feature_importance=True)\nprint(time.time() - start)\ndel model","10b375d1":"model = xgb.XGBClassifier(random_state=2020,tree_method='gpu_hist', silent=1, booster='gbtree', objective='binary:logistic')\nmodel_params = {\n    'booster':['gbtree'],\n    'colsample_bytree': [0.5, 0.8],\n    'subsample': [0,3, 0.5],\n    'learning_rate': [0.075, 0.01],\n    'objective': ['binary:logistic'],\n    'max_depth': [ 7, 8, 9],\n    'num_parallel_tree': [0.1, 1, 10],\n    'min_child_weight': [0.2, 0.8],\n}\nmodel_params = {\n    'colsample_bytree': [0.5],\n    'subsample': [0.5],\n    'learning_rate': [0.075],\n    'max_depth': [9],\n    'num_parallel_tree': [1],\n    'min_child_weight': [0.2],\n}\n\nstart = time.time()\nbest_score_xgboost, best_params_xgboost = get_model_result(model, model_params, 'xgboost')\nprint(time.time()-start)\ndel model  ","d0bd5b3f":"model = lgb.LGBMClassifier(random_state=2020, device='gpu', gpu_platform_id=0, gpu_device_id=0, silent=1)\nmodel_params = {\n    'n_estimators': [200, 300, 400],\n    'learning_rate': [0.01, 0.1, 0.5],\n    'num_leaves':[10,100,400],\n    'colsample_bytree':[0.5,0.8, 1.0],\n    'subsample':[0.3,0.5,0.9],\n    'max_depth':[7, 10, 15],\n    'reg_alpha':[0.01, 0.2, 0.5],\n    'reg_lambda':[0.01, 0.3, 0.8],\n    'min_split_gain':[0.01, 0.1],\n    'min_child_weight': [1,2,4],\n}\nmodel_params = {\n    'n_estimators': [600],\n    'learning_rate': [0.1],\n    'num_leaves':[120],\n    'colsample_bytree':[0.5],\n    'subsample':[0.9],\n    'max_depth':[15],\n    'reg_alpha':[0.01, 0.2],\n    'reg_lambda':[0.4],\n    'min_split_gain':[0.01],\n}\nstart = time.time()\nbest_score_lightgbm, best_params_lightgbm = get_model_result(model, model_params, 'lightgbm')\nprint(time.time()-start)\ndel model ","b076f8ca":"\nmodel = cat.CatBoostClassifier(random_state=2020, task_type='GPU', allow_writing_files=False, silent=True, eval_metric='AUC', bootstrap_type='Bernoulli')\nmodel_params = {\n    'iterations': [200,600],\n    'learning_rate': [0.05, 0.3, 0.5],\n    'depth': [6, 7, 9, 10],\n    'l2_leaf_reg': [30, 40, 50],\n    'bootstrap_type': ['Bernoulli'],\n    'subsample': [0.5, 0.7, 1.0],\n    'scale_pos_weight': [4, 5, 10],\n}\nmodel_params = {\n    'iterations': [600],\n    'learning_rate': [ 0.5],\n    'depth': [10],\n    'l2_leaf_reg': [40],\n    'subsample': [0.5],\n    'scale_pos_weight': [8],\n}\nstart = time.time()\nbest_score_catboost, best_params_catboost = get_model_result(model, model_params, 'catboost')\nprint(time.time() - start)\ndel model ","c1f70cbb":"auc_value = [best_score_ridge, best_score_elasticnet, best_score_lgr, best_score_dtc, best_score_rfc, best_score_etf, best_score_xgboost, best_score_lightgbm, best_score_catboost, best_score_bagging, best_score_pasting]\nauc_label = ['ridge', 'elasticnet', 'lgr', 'dtc', 'rfc', 'etf', 'xgboost', 'lightgbm', 'catboost', 'bagging', 'pasting']\n# auc_time = [best_score_ridge, best_score_elasticnet, best_score_lgr, best_score_dtc, best_score_rfc, best_score_etf, best_score_xgboost, best_score_lightgbm, best_score_catboost]\nfigure, ax = plt.subplots(figsize=(16,4))\n\nplt.bar(range(len(auc_value)), auc_value, tick_label=auc_label)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)\nplt.title('Different AUC in Dataset by ML')\nplt.show()","554e141d":"## Linear models with Regularization\n**Normal equation**\nGiven an input sample $x=[x_1, x_2, \\cdots, x_n]$, linear regression predicts $y$ using the following equation:\n$$\\bar{y}=\\theta_0+\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_n x_n$$\nGiven $m$ training samples $x^{(1)}, x^{(2)}, \\cdots,x^{(m)}$, linear regression finds $\\theta$ that minimize the Mean Square Error(MSE) between $\\bar{y}$ and $y$:\n$$MSE(\\theta)=\\frac{1}{m}\\sum_{i=1}^m(\\bar{y}^{(i)}-y)^2=\\frac{1}{m}\\sum_{i=1}^m(\\theta \\cdot x^{(i)} - y)^2$$\nwhere $y$ is the true value and $\\bar{y}$ is the predicted value. The solution of this minimization problem is given by $\\frac{\\partial MSE}{\\partial \\theta}=0$, which can calcuated,\n$$\\theta=(X^T\\cdot X)^{-1}\\cdot X^T \\cdot y$$\nwhere $X$ is the input data matrix of size $m \\times (n + 1)$. Each row of $X$ corresponds to a sample, each columns corresponds to a feature. There are $(n+1)$ columns since there is a columns of $1$-th added to the $n$-features, corresponding to $\\theta_0$. And $\\bar{y}$ is a vector of true target values of size $m \\times 1$, and $\\theta$ is a vector of size $(n+1)\\times 1$. The $\\theta$ is the set of parameters that minimizes the cost function of linear regression $MSE(\\theta)$.\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial MSE}{\\partial \\theta} &= \\frac{\\partial \\frac{1}{m} \\sum_{i=1}^m ((\\theta \\cdot x^{(i)})^2 + y^2 - \\theta \\cdot x^{(i)}y)}{\\partial \\theta}\\\\\n&= \\frac{\\partial(\\frac{1}{m}\\theta^T \\theta \\sum_{i=1}^m \\sum_{j=1}^m x^{(i)}x^{(j)})}{\\partial \\theta} + \\frac{y^Ty}{\\partial \\theta} - \\frac{\\partial \\frac{2y}{m}\\sum_{i=1}^mx^{(i)}}{\\partial \\theta} \\\\\n&= \\frac{2}{m}(X^TX \\theta - X^Ty)\n\\end{aligned}\n\\end{equation}\n$$\nGD(Gradient Descent) is most useful when the cost function (here it is the MSE) does not have a clean and nice analytical solution. But it can also accelerate the training when you have a lot of input features or a large training set. So the gradient of $MSE(\\theta)$ over $\\theta$ is :\n$$\\nabla MSE(\\theta)= \\frac{2}{m}X^T(X \\cdot \\theta - \\bar{y})$$\nThe pseudo algorithm for gradient descent is:\n- Initialize parameter vector $\\theta$\n- Choose learning rate $\\mu$\n- For each step, update the parameters with a sample (means Stochastic Gradient Descent), a random selected sub-sample (means Mini-batch Gradient Descent), using all $m$ samples (means Batch gradient descent).\n- Iterate until certain number of steps has passed, or when the decrease of $MSE(\\theta)$ is smaller than a tolerance (means early stopping).\n","e9b0bf49":"**Lasso Regression**\n\nLASSO(Least Absolute Shrinkage and Selection Operator Regression)\ncost function:\n$$J(\\theta)=MSE(\\theta)+ \\alpha(|\\theta_1| + |\\theta_2| + \\cdots + |\\theta_n|)$$\nThe penalty is proportional to the *l1-norm* of theta.","4d478d4d":"## Support Vector Machine\n\nSVM(Support Vector Machine) is one of the most popular classification techniques in machine learning. Use the logistic regression cost with l$2$ regularization to define the loss function of svm.\n$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[y^{(i)}(-log(h^{(i)})) + (1-y^{(i)}) (-log(1-h^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$$\nwhere $h^{(i)}=\\sigma(\\theta^T \\cdot x^{(i)}) = \\frac{1}{1+e^{-\\theta^T \\cdot x^{(i)}}}$, \n\u5176\u4e2d\u4e00\u79cd\u89e3\u91ca\u662f\uff0c\u82e5$y^{(i)}=1$, \u5bf9\u4e8e\u635f\u5931\u51fd\u6570$J(\\theta)$\u7684\u8d21\u732e\u662f$-log(h^{(i)})$\uff0c\u5219\uff0c\u82e5$y^{(i)}=0$\uff0c\u5bf9\u4e8e\u635f\u5931\u51fd\u6570$J(\\theta)$\u7684\u8d21\u732e\u662f$-log(1-h^{(i)})$\uff0c\u5728SVM\u4e2d\uff0c\u4f7f\u7528$max(0, 1- \\theta^T \\cdot x^{(i)})$\u6765\u66ff\u4ee3$-log(h^{(i)})$\uff0c\u5e76\u4e14\u7528$(0, 1 + \\theta^T \\cdot x^{(i)})$\u6765\u66ff\u4ee3$-log(1-p^{(i)})$\u3002\u5b9a\u4e49$z=\\theta^T \\cdot x^{(i)}$\u5219","c9ffcbc4":"**CART**\n\u4e00\u68f5\u6811\u7684\u57fa\u672c\u5b9a\u4e49\u5982\u4e0b\uff1a\n$$f(x)=\\sum_{j=1}^T w_jI(x \\in R_j)$$\n\u5176\u4e2d$w_j$\u662f$j$-th\u53f6\u5b50\u8282\u70b9\u7684\u6743\u91cd\uff0c${R_j}(j=1,2,\\cdots, T)$\u79f0\u4e3a\u6570\u7684\u7ed3\u6784\uff0c$T$\u8868\u793a\u6811\u7ed3\u6784\u4e2d\u7684\u53f6\u5b50\u8282\u70b9\u6570\u3002$I(\\cdot)$\u662f\u4e00\u4e2a\u6307\u793a\u51fd\u6570\uff0c\u82e5$x$\u5c5e\u4e8e\u53f6\u5b50\u533a\u57df$R_j$\uff0c\u5219$I$\u4e3a1\uff0c\u53cd\u4e4b\u540c\u7406\u3002$f(x)$\u7684\u76ee\u7684\u5c31\u662f\u5728\u6837\u672c$x$\u843d\u5728\u533a\u57df$R_j$\u4e0a\u65f6\u9884\u6d4b$w_j$\u3002\n\n\u5728\u8bad\u7ec3\u6b64\u6811\u4e4b\u524d\uff0c\u9700\u8981\u5b9a\u4e49\u4ef7\u503c\u51fd\u6570\uff1a\n$$J(f)=\\sum_{i=1}^n L(y_i, f(x_i))=\\sum_{i=1}^nL(y_i, \\sum_{j=1}^T w_jI(x\\in R_j))$$\n- \u5728\u4e00\u4e2a\u56fa\u5b9a\u7ed3\u6784\u4e0b\u5b66\u4e60\u6743\u91cd\n\u5728\u7ed9\u5b9a\u533a\u57df$R_j$\u7684\u60c5\u51b5\u4e0b\uff0c\u5373$I(\\cdot)$\u662f\u786e\u5b9a\u7684\uff0c\u5219\u6b64\u65f6\u7684\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u5199\u4f5c\u4e3a\n$$J(f)=\\sum_{j=1}^T \\sum_{x_j\\in R_j}L(y_i, w_j)$$\n\u6700\u5c0f\u5316$J(f)$\uff0c\u5176\u5b9e\u5c31\u662f\u5bf9\u4e8e\u6bcf\u4e2a\u533a\u57df$j$\u6700\u5c0f\u5316$\\sum_{x_i \\in R_j} L(y_i, w_j)$\uff0c\u8fdb\u4e00\u6b65\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\n$$w_j^{*}=argmin_w\\sum_{x_i \\in R_j}L(y_i, w)$$\n\u5f53\u4f7f\u7528\u5e73\u65b9\u635f\u5931\u51fd\u6570\u65f6\uff0c\u5219$w_j^{*}=argmin\\sum_{x_i \\in R_j}(y_i -w)^2$\uff0c\u4e3a\u4e86\u6c42\u89e3\u6700\u4f18\u503c\uff0c\u4ee4\u4e00\u6b21\u5bfc\u6570\u4e3a0\uff0c\u5373$\\frac{\\partial \\sum_{x_i \\in R_j}(y_i - w)^2}{\\partial w}=0$\uff0c\u5219\n$$w_j^{*}=\\frac{\\sum_{x_i \\in R_j}y_i}{n_j}$$\n\u5176\u4e2d\uff0c$n_j$\u8868\u793a\u5728\u533a\u57df\uff08\u53f6\u5b50\uff09$R_j$\u7684\u6837\u672c\u6570\uff0c\u5f53\u4f7f\u7528\u5e73\u65b9\u635f\u5931\u65f6\uff0c\u5728\u533a\u57df$R_j$\u4e2d\uff0c\u8bc4\u4f30\u6743\u91cd$w_j$\u4e3a\u6b64\u533a\u57df\u7684\u5e73\u5747\u503c\uff0c\u5f53\u4f7f\u7528$L(y_i, w)=|y_i-w|$\u5373\u7edd\u5bf9\u503c\u635f\u5931\u65f6\uff0c\u6b64\u65f6\u7684$w_j^*$\u4e3a\u533a\u57df$R_j$\u7684\u4e2d\u4f4d\u6570\u3002\n- \u5728\u5df2\u77e5\u6743\u91cd\u4e0b\u5b66\u4e60\u7ed3\u6784\n\u82e5\u6743\u91cd$w_j^{*}$\u88ab\u7ed9\u51fa\uff0c\u5219\u4ef7\u503c\u51fd\u6570\u80fd\u591f\u88ab\u5199\u4e3a\n$$J(f)=\\sum_{j=1}^T \\sum_{x_j \\in R_j} \\sum_{x_i \\in R_j}L(y_i, w_j^{*})=\\sum_{j=1}^TL_j^{*}$$\n\u5176\u4e2d$L_j^*$\u4e3a\u53f6\u5b50\u8282\u70b9$j$\u7684\u7d2f\u52a0\u635f\u5931\u3002\u82e5\u8bad\u7ec3\u7684\u6811\u7ed3\u6784\u5728\u53f6\u5b50\u8282\u70b9$k$\u5904\uff0c\u8fdb\u884c\u5206\u5272\uff0c\u5219\u6b64\u65f6\u7684\u635f\u5931\u4e3a\n$$J_{before}=\\sum_{j\\neq k}L_j^{*} + L_k^{*}$$\n\u6309\u7167\u8282\u70b9$k$\u8fdb\u884c\u5206\u5272\uff0c\u53ef\u4ee5\u5f97\u5230\n$$J_{after}=\\sum_{j\\neq k} L_j^* + L_L^* + L_L^*$$\n\u5219\u5206\u5272\u8282\u70b9\u7684\u589e\u76ca\u4e3a\n$$Gain = J_{before}- J_{after}= L_k^{*} - (L_L^*+L_R^*)$$\n\u589e\u76ca\u8d8a\u5927\uff0c\u4ee3\u4ef7\u51fd\u6570$J$\u7684\u4e0b\u964d\u8d8a\u5927\u3002 \u5728\u6811\u8bad\u7ec3\u7684\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u53ef\u80fd\u7684\u5206\u5272\uff08\u8282\u70b9\u5206\u5272\uff0c\u7279\u5f81\u5206\u5272\uff0c\u7279\u5f81\u9608\u503c\uff09\u7684\u589e\u76ca\uff0c\u7136\u540e\u9009\u62e9\u4f7f\u589e\u76ca\u6700\u5927\u5316\u7684\u5206\u5272\u3002\n\u7531\u4e8e\u8282\u70b9$k$\u5df2\u7ecf\u786e\u5b9a\u5206\u5272\uff0c\u5219$L_k^{*}$\u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u6b64\u65f6\u4e3a\u4e86Gain\u6700\u5927\u5316\uff0c\u5373\u6700\u5c0f\u5316$(L_L^*+L_R^*)$\u3002\n\u5728\u5e73\u65b9\u635f\u5931\u51fd\u6570\u4e0b\uff0c\u5219\u53ef\u4ee5\u5f97\u5230\n$$L_L^*+L_R^*=\\sum_{x_i \\in left} (y_i - \\bar{y}_{left})^2 + \\sum_{x_i \\in right}(y_i -\\bar{y}_{right})^2=n_{left}MSE_{left}+n_{right}MSE_{right}$$\n\u5176\u4e2d$n_{left(right)}$\u662fleft(right)\u8282\u70b9\u4e2d\u8bad\u7ec3\u6837\u672c\u7684\u6570\u91cf\u3002 $\\bar{y}_{left(right)}$\u53ea\u662f\u5e73\u65b9\u635f\u5931\u4e0b\u5de6\uff08\u53f3\uff09\u8282\u70b9\u7684\u4f30\u8ba1\u6743\u91cd\uff0c\u5982\u4e0a\u4e00\u6b65\u6240\u793a\u3002 \u8981\u6700\u5c0f\u5316\u7684\u6570\u91cf\u4e0e\u6211\u4eec\u5728\u51b3\u7b56\u6811\u56de\u5f52\u90e8\u5206\u4e2d\u5b9a\u4e49\u7684\u6210\u672c\u51fd\u6570$J(k, t_K)$\u6210\u6bd4\u4f8b\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u5728\u8be5\u90e8\u5206\u4e2d\u5b9a\u4e49\u7684\u6240\u6709\u5185\u5bb9\u4ec5\u662f\u5c06\u666e\u901aCART\u7b97\u6cd5\u5e94\u7528\u4e8e\u5e73\u65b9\u635f\u5931\u7684\u7279\u5b9a\u60c5\u51b5\u3002\n- \u526a\u679d","4a28b959":"**Softmax Regrssion**\nSoftmax regression is used when there are more than two classes ($0$ and $1$) to classify. For each class $k$, there is a vector of parameters $\\theta_k$. The softmax score for a sample $x$ is $s_k(x) = \\theta_k^T \\cdot x$. The probability of sample $x$ being in class $k$ is:\n$$h_k = \\frac{e^{s_k(x)}}{\\sum_{j=1}^K e^{s_k(x)}}$$\nwhere $K$ is the total number of possible classes. Softmax Regression takes the class that has the highest probability $p$ as the predicted class. When training, the cost function is the **cross entropy**,\n$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K y_k^{(i)}log(h_k^{(i)})$$\nwhere $y_k^{(i)}=1$ if sample $i$ belongs to class $k$, and $y_k^{(i)}=0$ otherwise. \n\nThe gradient vector of cross entropy for class $k$ is:\n$$J(\\theta)^{\\prime}=\\frac{1}{m}\\sum_{i=1}^m(h_k^{(i)}-y_k^{(i)})x^{(i)}$$\nnote that $\\theta$ is a matrix, i.e., multi-vector.","2d68418a":"## Boosting","cc33c955":"**RandomForest**\n\u968f\u673a\u68ee\u6797\u662f\u51b3\u7b56\u6811\u7684\u96c6\u5408\uff0c\u5728\u968f\u673a\u68ee\u6797\u4e2d\uff0c\u6bcf\u4e2a\u51b3\u7b56\u6811\u90fd\u5728\u8bad\u7ec3\u96c6\u7684\u4e00\u4e2a\u968f\u673a\u5b50\u96c6\u4e0a\u8bad\u7ec3\uff0c\u8be5\u5b50\u96c6\u901a\u5e38\u662f\u66ff\u6362\u91c7\u6837\u7684\uff0c\u6837\u672c\u5927\u5c0f\u7b49\u4e8e\u8bad\u7ec3\u96c6\u5927\u5c0f\u3002\u5728\u5206\u7c7b\u7684\u60c5\u51b5\u4e0b\uff0c\u9009\u62e9\u6240\u6709\u6811\u4e2d\u5e73\u5747\u6982\u7387\u6700\u9ad8\u7684\u7c7b\u522b\u3002\u5728\u56de\u5f52\u7684\u60c5\u51b5\u4e0b\uff0c\u9884\u6d4b\u7684\u503c\u662f\u6240\u6709\u6811\u4e0a\u7684\u9884\u6d4b\u503c\u7684\u5e73\u5747\u503c\u3002\n- \u968f\u673a\u68ee\u6797\u76f8\u5bf9\u5355\u4e2a\u51b3\u7b56\u6811\u800c\u8a00\uff0c\u7531\u4e8e\u968f\u673a\u68ee\u6797\u672c\u8eab\u662f\u6c42\u7684\u5e73\u5747\u503c\uff0c\u5176\u65b9\u5dee\u4e5f\u4f1a\u51cf\u5c0f\uff0c\u901a\u5e38\u66f4\u591a\u60c5\u51b5\u4e0b\uff0c\u65b9\u5dee\u51cf\u5c11\u7684\u6548\u679c\u4f1a\u8865\u507f\u504f\u5dee\u7684\u589e\u5927\uff0c\u56e0\u6b64\u4f1a\u4ea7\u751f\u66f4\u597d\u7684\u6a21\u578b\u3002\n- sklearn\u4e2d\u6307\u51fa\uff0c\u7279\u5f81\u5206\u5272\u7684\u65f6\u5019\uff0crf\u53ea\u4f1a\u5728\u968f\u673a\u5b50\u96c6\u4e2d\u8fdb\u884c\u641c\u7d22\u3002","391f92b1":"**Linear SVM**\nSVM\u7684cost function\u4e3a\uff1a\n$$J(\\theta)=\\frac{1}{m} \\sum_{i=1}^m [y^{(i)}(max(0,1-\\theta^T \\cdot x^{(i)}))+ (1-y^{(i)})(max(0, \\theta^T \\cdot x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta^2_j$$\n\u7531\u4e8e\u5e38\u7cfb\u6570$\\frac{1}{m}$\u5e76\u4e0d\u4f1a\u5f71\u54cd\u6700\u7ec8\u7684\u7ed3\u679c\uff0c\u56e0\u6b64\u4e0a\u5f0f\u53ef\u4ee5\u5199\u6210\uff0c\n$$J(\\theta)=\\sum_{i=1}^m max(0, 1-t^{(i)}(\\theta^T \\cdot x^{(i)})) + \\frac{\\lambda}{2} \\sum_{j=1}^n \\theta^2_j$$\n\u5176\u4e2d\uff0c\u5f53$y^{(i)}=1$\u65f6$t^{(i)}=1$\uff0c\u5f53$y^{(i)}=0$\u65f6\uff0c$t^{(i)}=-1$\u3002\u4e3a\u4e86\u65b9\u4fbf\u8ba8\u8bba\uff0c\u5176\u4e2d\u57fa\u4e8e$l2$\u7684\u60e9\u7f5a\u9879\uff0c\u901a\u8fc7\u5229\u7528\u53c2\u6570$C$\u6765\u66ff\u4ee3\u53c2\u6570$\\lambda$\uff0c\u5e76\u4e14\u4f7f\u7528$w^T\\cdot x^{(i)}$\u6765\u66ff\u4ee3$\\theta^T \\cdot x^{(i)}$\uff0c\u5373\n$$J(w,b)=C\\sum_{i=1}^m max(0, 1-t^{(i)}(w^T \\cdot x^{(i)} + b)) + \\frac{1}{2} \\sum_{j=1}^m w_j^2$$\n\u5176\u4e2d$C$\u7b49\u4ef7\u4e8e$\\frac{1}{\\lambda}$\u3002\u8f83\u5927\u7684$C$\u4f1a\u5bfc\u81f4\u8f83\u4f4e\u7684\u504f\u5dee\u3001\u8f83\u9ad8\u7684\u65b9\u5dee\uff08\u53ef\u80fd\u5bfc\u81f4\u8fc7\u62df\u5408\uff09\uff0c\u8f83\u5c0f\u7684$C$\u5bfc\u81f4\u8f83\u9ad8\u7684\u504f\u5dee\u3001\u8f83\u5c0f\u7684\u65b9\u5dee\uff08\u53ef\u80fd\u5bfc\u81f4\u6b20\u62df\u5408\uff09\u3002\n\u51fd\u6570$$l(w^T \\cdot x^{(i)} + b) = max(0, 1-t^{(i)}(w^T \\cdot x^{(i)} + b))$$\u79f0\u4e4b\u4e3a*hinge loss*\u3002\u628a\u57fa\u4e8e$l_2$\u7684\u4ee3\u4ef7\u51fd\u6570$J$\u79f0\u4e4b\u4e3a*soft margin SVM*\u3002\n\u5728SVM\u4e2d\u7684\u4e24\u4e2amargins\u5b9a\u4e49\u5982\u4e0b\uff1a\n$$w^T \\cdot x^{(i)} + b = 1$$\n$$w^T \\cdot x^{(i)} + b = -1$$\nHinge loss \u60e9\u7f5a\u79f0\u4e4b\u4e3amargin violations\uff0c\u5f53$y=1$\u65f6\uff0c\u6837\u672c\u901a\u8fc7$w^T \\cdot x + b =1$margin\uff0chinge loss\u53d8\u6210\u975e0\u503c\u5e76\u4e14\u53ef\u4ee5\u5ea6\u91cf\u4ece$w^Tx+b$\u5230$1$\u4e4b\u95f4\u6709\u591a\u8fdc\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5047\u5982$y=0$\u6837\u672c\u901a\u8fc7$w^T \\cdot x+ b= -1$Margin\u65f6\uff0chinge loss\u4f9d\u7136\u4e3a\u975e0\u5143\u7d20\uff0c\u6b64\u65f6\u8981\u5ea6\u91cf\u7684\u662f$w^T \\cdot x + b$\u5230$-1$\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\n\n\u6b63\u5219\u9879$\\frac{1}{2} \\sum_{j=1}^n w_j^2$\u53ef\u4ee5\u8d77\u5230\u6269\u5927\u4e24\u4e2amargin\u7684\u4f5c\u7528\u3002\u5982\u5047\u8bbe$wx_0+b=0$,$wx_1+b=1$,\u548c$wx_{-1}+b=-1$\u8fd9\u4e09\u6761\u4e4b\u524d\uff0c\u5219\n$$x_1-x_0=\\frac{1}{w}$$\n$$x_0 -x_{-1} = \\frac{1}{w}$$\n\u7531\u4e0a\u53ef\u77e5\uff0c\u8f83\u5c0f\u7684$w$\uff0c\u610f\u5473\u8005\u4e24\u4e2amargin\u4e4b\u95f4\u8f83\u5927\u7684\u8ddd\u79bb\uff0c\u5373$x_1-x_{-1}=\\frac{2}{w}$\u3002\n\u5f53\u5728\u53c2\u6570\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u662f\u964d\u4f4e$J$\u7684\u503c\uff0c\u5373\u964d\u4f4e$||w||_2^2=\\sum_{j=1}^n w_j^2$\uff0c\u6b64\u65f6\u63d0\u5347\u4e24\u4e2amargin\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u56e0\u6b64\uff0cSVM\u53c8\u79f0\u4f5c\u201clarge margin classifier\u201d\u3002\n\u5728sklearn\u4e2d\u7684SVC\u5c31\u662f\u5229\u7528\u4e86soft margin\u3002\n\n\u5f53\u53c2\u6570$C$\u975e\u5e38\u5927(\u8d8b\u5411\u4e8e\u65e0\u7a77\u7684\u65f6\u5019)\u7684\u65f6\u5019\uff0cmargin\u5c06\u4f1a\u53d8\u7a84\uff0c margin violations\u5c06\u4e0d\u88ab\u5141\u8bb8\uff0c\u6b64\u65f6\u79f0\u4e4b\u4e3ahard margin SVM\uff0c\u5176\u76ee\u6807\u51fd\u6570\u4e3a\n$$minize \\ J(w,b)=\\frac{1}{2} \\sum_{j=1}^n w_j^2 $$\n$$subject\\ to t^{(i)}(w^T \\cdot x^{(i)} +b) \\geq 1$$\n\u5176\u4e2dSoft margin SVM\u5e73\u8861\u7684\u662fmargin widening\u548cmargin violations\uff08\u5373\u5728\u4e24\u4e2amargin\u4e4b\u95f4\u7684\u503c\uff09\u4e4b\u95f4\u7684\u5e73\u8861\u3002\n\n**Not-Linear SVM**\n- \u591a\u9879\u5f0f\u7279\u5f81\uff0c \u901a\u8fc7\u589e\u52a0C\u7684\u503c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\n- \u76f8\u4f3c\u7279\u5f81\n\u53e6\u4e00\u4e2a\u6784\u9020\u975e\u7ebf\u6027SVM\u7684\u65b9\u5f0f\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2a\u6570\u636e\u70b9$x$\u548clandmarks $l$\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u6765\u6dfb\u52a0\u65b0\u7684\u529f\u80fd\u3002\u76f8\u4f3c\u5ea6\u51fd\u6570\uff08\u6838\u51fd\u6570\uff09RBF(gaussian radial basis function)\u5b9a\u4e49\u5982\u4e0b\uff1a\n$$\\phi_r(x,l)=e^{-\\lambda ||x-l||^2}$$\n\u5176\u4e2d$||x-l||^2=(x_1-l_1)^2+(x_2-l_2)^2 + \\cdots + (x_n - l_n)^2$\uff0c\u4f8b\u5982","f72af2b3":"## Ensemble Learning\n\n**Ensemble with different classifiers**\n- voting","c458e279":"## Random Forests\n- Decision Tree\n- Random Forest\n- CART","8283d03e":"\u7531\u4e0a\u53ef\u77e5\uff1a\n- TotalWorkingYears\/(MaritalStatus\/OverTime)\u4e3a\u5206\u5272\u70b9\n- gini\u7cfb\u6570\n- samples\u8868\u793a\u6709\u591a\u5c11\u6837\u672c\u6ee1\u8db3\u5f53\u524d\u5206\u5272\u6761\u4ef6\n- values\u603b\u548c\u4e3asample\uff0c\u5176\u4e2dYes\u7684\u6837\u672c\u6570\u4e3a988,No\u7684\u6837\u672c\u6570\u4e3a188\n- class\u4e3a\u54ea\u4e2a\u7c7b\u5728\u6b64\u8282\u70b9\u4e2d\u5360\u5927\u591a\u6570\n\n\u4e3a\u4e86\u9884\u6d4b\uff0c\u4e00\u76f4\u6cbf\u7740\u6811\u7684\u8def\u5f84\uff0c\u76f4\u5230\u8fbe\u5230\u53f6\u8282\u70b9\u4e3a\u6b62\u3002\u6bcf\u4e2a\u7c7b\u522b\u7684\u6982\u7387\u662f\u53f6\u8282\u70b9\u4e2d\u6bcf\u4e2a\u7c7b\u522b\u7684\u8bad\u7ec3\u6837\u672c\u7684\u5206\u6570\uff0c\u5982\u5de6\u53f6\u8282\u70b9\u4ee5$\\frac{12}{12+27}$\uff0c\u6b64\u5c5e\u6027\u53ef\u4ee5\u901a\u8fc7DecisionTreeClassifier\u7684predict_proba()\u51fd\u6570\u83b7\u5f97\u3002\n\n**Gini**\n$$G=1-\\sum_{k=1}^np_k^2$$\n\u5176\u4e2d$p_k$\u662f\u7c7b\u522b$k$\u7684\u6bd4\u7387\uff0c\u5219\n$$G=1-(\\frac{12}{12+27})^2-(\\frac{27}{12+27})^2$$\n\n**Training**\n- Decision Tree Classifier\nCART\u7b97\u6cd5\u628a\u4e00\u4e2a\u8282\u70b9\u5206\u4e3a\u4e24\u4e2a\u5b69\u5b50\u8282\u70b9\uff0c\u5206\u5272\u7684\u6807\u51c6\u5305\u542b\u4e00\u4e2a\u7279\u5f81$k$\u548c\u4e00\u4e2a\u4e0e\u7279\u5f81\u76f8\u5173\u7684\u9608\u503c$t_k$\uff0c\u5176\u4e2d\u57fa\u4e8eGini\u7684\u635f\u5931\u51fd\u6570\u5b9a\u4e49\u5982\u4e0b\uff1a\n$$J(k,t_k)=G_{left} \\times \\frac{m_left}{m} + G_{right} \\times \\frac{m_{right}}{m}$$\n\u5176\u4e2d$G_{right}$\u548c$G_{left}$\u4e3a\u5de6\u53f3\u5b69\u5b50\u7684Gini\u71b5\uff0c$m_{left}$\u548c$m_{right}$\u4e3a\u5de6\u53f3\u5b69\u5b50\u7684\u6837\u672c\u6570\uff0c\u5219parent\u7684\u8282\u70b9\u6570$m=m_{left}+m_{right}$\u3002\u7b97\u6cd5\u505c\u6b62\u7684\u6761\u4ef6\u53ef\u4ee5\u4e3a\n$$G_{left} \\times \\frac{m_{left}}{m} + G_{right} \\times \\frac{m_{right}}{m} \\geq G_{parent}$$\n\u6216\u8005\u4f7f\u7528\u6811\u7684\u6700\u5927\u6df1\u5ea6\u4f5c\u4e3a\u505c\u6b62\u6761\u4ef6\u3002\n- CART\u7b97\u6cd5\u662f\u4e00\u79cd\u8d2a\u5a6a\u7b97\u6cd5\uff0c\u5b83\u641c\u7d22\u7684\u662f\u4e00\u4e2a\u5c40\u90e8\u6700\u4f18\u51b3\u7b56\u3002\u4f18\u5316\u5206\u88c2\u65f6\u65e0\u9700\u8981\u8003\u8651\u6b64\u65f6\u7684\u5206\u5272\u6811\u7ed3\u6784\u662f\u5426\u662f\u5168\u5c40\u6700\u4f18\u7684\u3002\n- \u5728\u4f7f\u7528\u7b97\u6cd5\u9ed8\u8ba4\u503c\u7684\u65f6\u5019\uff0c\u4e00\u5b9a\u7684\u8bad\u7ec3\u96c6\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u5f97\u5230\u786e\u5b9a\u7684\u7ed3\u6784\u3002\u4f46\u662f\u6709\u65f6\u5019\u4f1a\u663e\u793a\u4e00\u5b9a\u7684\u968f\u673a\u6027\uff0c\u8fd9\u662f\u7531\u4e8e\u7b97\u6cd5\u672c\u8eab\u5c06\u4f1a\u628a\u8f93\u5165\u7684\u7279\u5f81\u8fdb\u884c\u91cd\u6392\u5bfc\u81f4\u7684\u3002\n- \u5728sklearn\u4e2d\u53ef\u4ee5\u8bbe\u7f6emax_features\u8bbe\u7f6e\u4e3a\u5c0f\u4e8e\u603b\u7684features\u6570\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7b97\u6cd5\u5c06\u968f\u673a\u91c7\u6837\u7279\u5f81\u5b50\u96c6\uff0c\u7136\u540e\u4ece\u4e2d\u641c\u7d22\u6700\u4f73\u7684\u5206\u5272\u70b9\u3002\n- \u5904\u7406\u8fde\u7eed\u503c\u3002\u5728lightbgm\u4e2d\u91c7\u7528bins\u7684\u7b97\u6cd5\uff0c\u800c\u4e00\u822c\u7684\u51b3\u7b56\u6811\u5904\u7406\u8fde\u7eed\u53d8\u91cf\u5374\u662f\u53ea\u5c06\u8fde\u7eed\u53d8\u91cf\u505a\u79bb\u6563\u4e8c\u503c\u5316\u3002\u82e5\u7279\u5f81\u6709n\u4e2a\u503c${a_1, a_2, \\cdots, a_n}$\uff0c\u5219\u6709n-1\u4e2a\u5019\u9009\u5212\u5206\u8282\u70b9$T_a={\\frac{a_i+a_{i+1}}{2}|1\\leq i \\leq n-1}$\uff0c\u8fd9\u6837\uff0c\u53ef\u4ee5\u5728\u8fde\u7eed\u53d6\u503c\u4e2d\u9009\u62e9\u4e00\u4e2a\u6700\u4f73\u5206\u88c2\u8282\u70b9\u3002\u4e0e\u79bb\u6563\u5c5e\u6027\u4e0d\u540c\uff0c\u82e5\u5f53\u524d\u7ed3\u70b9\u5212\u5206\u5c5e\u6027\u4e3a\u8fde\u7eed\u503c\uff0c\u8be5\u5c5e\u6027\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u5176\u540e\u4ee3\u8282\u70b9\u7684\u5212\u5206\u5c5e\u6027\u3002\n- \u7f3a\u5931\u503c\u5904\u7406\u3002\u82e5\u6837\u672c$x$\u5728\u5212\u5206\u5c5e\u6027$a$\u4e0a\u7684\u53d6\u503c\u672a\u77e5\uff0c\u5219\u5c06$x$\u540c\u65f6\u5212\u5165\u6240\u6709\u5b50\u8282\u70b9\uff0c\u4e14\u6837\u672c\u6743\u503c\u5728\u4e0e\u5c5e\u6027\u503c$a_v$\u5bf9\u5e94\u7684\u5b50\u8282\u70b9\u4e2d\u8c03\u6574\u4e3a$r_v \\cdot Ent(\\cdot)$\uff0c\u76f4\u89c2\u7684\u770b\uff0c\u5c31\u662f\u8ba9\u540c\u4e00\u4e2a\u6837\u672c\u4ee5\u4e0d\u540c\u7684\u6982\u7387\u5212\u5206\u5230\u4e0d\u540c\u7684\u5b50\u8282\u70b9\u4e2d\u53bb\u3002\u5176\u4e2d\u5728\u5468\u5fd7\u534e\u4e66\u4e2d\u63d0\u5230\uff0c\u5bf9\u4e8e\u5c5e\u6027$a$,$l$\u8868\u793a\u7f3a\u5931\u503c\u6240\u5360\u7684\u6bd4\u4f8b\uff0c$p_k$\u8868\u793a\u65e0\u7f3a\u5931\u503c\u6837\u672c\u4e2d$k$\u7c7b\u6240\u5360\u7684\u6bd4\u4f8b\uff0c$r_v$\u5219\u8868\u793a\u65e0\u7f3a\u5931\u503c\u6837\u672c\u5728\u5c5e\u6027$a$\u4e0a\u53d6\u503c$a_v$\u7684\u6837\u672c\u6240\u5360\u7684\u6bd4\u4f8b\uff0c\u663e\u7136\uff0c$\\sum_{k=1}^{|y|}p_k=1, \\sum_{v=1}^V r_v=1$\u3002\u4f9d\u636e\u4e0a\u8ff0\u5b9a\u4e49\u5219\uff0c\u4fe1\u606f\u589e\u76ca\u7684\u8ba1\u7b97\u4e3a\n$$Gain(D,a) = l \\times Gain(D,a)=l \\times (Ent(D)-\\sum_{v=1}^V r_v Ent(D_v))$$\n\u5176\u4e2d\uff0c$Ent(D)=-\\sum_{k=1}^{|y|}p_klog_2p_k$\n- Decision Tree Regression\n\u4e0e\u5206\u7c7b\u4e0d\u540c\u7684\u662f\uff0c\u51b3\u7b56\u56de\u5f52\u635f\u5931\u51fd\u6570\u5728\u57fa\u4e8e\u7279\u5f81$k$\u7684\u60c5\u51b5\u4e0b\u6cbf\u7740\u9608\u503c$t_k$\u8fdb\u884c\u5206\u5272\uff0c\u5177\u4f53\u5b9a\u4e49\u5982\u4e0b:\n$$J(k,t_k)=MSE_{left} \\times \\frac{m_{left}}{m} + MSE_{right} \\times \\frac{m_{right}}{m}$$\n\u5176\u4e2d\uff0c$MSE_{left(right)}=\\frac{1}{m_{left(right)}} \\sum_{i \\in left(right)}(\\bar{y}_{left(right)}- y^{(i)})^2$\uff0c$\\bar{y}_{left(right)} = \\frac{1}{m_{left(right)}} \\sum_{i \\in left(right)}y^{(i)}$\u3002\n- \u9996\u5148\uff0c\u7b97\u6cd5\u6309\u7167\u7279\u5f81$k$\u548c\u7279\u5f81\u9608\u503c$t_k$\u5212\u5206\u5de6\u53f3\u5b69\u5b50\n- \u8ba1\u7b97\u5de6\u53f3\u5b69\u5b50\u7684\u5747\u503c$\\bar{y}_{left(right)}$\n- \u8ba1\u7b97\u5747\u65b9\u8bef\u5dee$MSE_{left}$\u548c$MSE_{right}$\n- \u904d\u5386\u6240\u6709\u7684$(k, t_k)$\uff0c\u4f7f\u5f97$J(k, t_k)$\u6700\u5c0f\u7684\u7ed3\u6784","5f2073c6":"## Different General ML","3df1f151":"## Logistic Regression\nThe function $g$ of **Logisitic Regression** named sigmoid function, i.e., $\\frac{1}{1+e^{-z}}$. \n\nHypothesis from linear regression: $z_{\\theta}(x)=\\theta_0x_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n = \\theta^T x$, so Logistic Hypothesis from composition of sigmoid\/logistic function and linear hypothesis:\n$$h_{\\theta}(x)=\\frac{1}{1+e^{-z_{\\theta}(x)}} = \\frac{1}{1+e^{-\\theta^Tx}}$$\n\nThe probability of $x$ being $1$ for the given $\\theta$, i.e.,\n$$h_{\\theta}(x)=\\frac{1}{1+e^{-z_{\\theta}(x)}}=\\frac{1}{1+e^{-\\theta^Tx}}$$\nAnd the probability of $x$ being $0$ for the given $\\theta$, i,e.,\n$$h_{\\theta}(x)=\\frac{e^{-z_{\\theta}(x)}}{1+e^{-z_{\\theta}(x)}}=\\frac{e^{-\\theta^Tx}}{1+e^{-\\theta^Tx}}$$\n\nUsually, logistic regression predicts $\\bar{y}=1$ if $\\bar{h} \\geq 0.5$, and $y = 0$ if $\\bar{h} < 0.5$. The **decision boundary** is given by $\\theta^T \\cdot x=0$. While training, the cost function is the **log loss**:\n$$J(\\theta)=-\\frac{1}{m} \\sum_{i=1}^m[y^{(i)}log(h_{\\theta}^{(i)})+(1-y^{(i)})log(1-h_{\\theta}^{(i)})]$$\nThe partial derivatives of log loss is:\n$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=\\frac{1}{m}\\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j^{(i)}$$\n\nThe following trick will come in handy:\n$$\n\\begin{equation}\n\\begin{aligned}\nh^{\\prime} &= (\\frac{1}{1+e^{-\\theta^Tx}})^{\\prime} \\\\\n& = - \\frac{1}{(1+e^{-\\theta^Tx})^2} \\cdot (1+e^{-\\theta^Tx})^{\\prime} \\\\\n& = - \\frac{1}{(1+e^{-\\theta^Tx})^2} \\cdot e^{-\\theta^Tx} \\cdot (-\\theta^Tx)^{\\prime} \\\\\n& = - \\frac{xe^{-\\theta^Tx}}{(1+e^{\\theta^Tx})^2} \\\\\n& = - \\frac{1}{1+e^{-\\theta^Tx}} \\cdot \\frac{e^{-\\theta^Tx}}{1+e^{-\\theta^Tx}} \\cdot x \\\\\n& = h(1-h)x\n\\end{aligned}\n\\end{equation}\n$$\n\n$$\n\\begin{equation}\n\\begin{aligned}\nJ(\\theta)^{\\prime} &= \\sum_{i=1}^m(y_i log^{\\prime}(h) + (1-y_i)log^{\\prime}(1-h))) \\\\\n& = \\sum ((y_i\\frac{1}{h}h^{\\prime})+(1-y_i)\\frac{1}{1-h}(1-h)^{\\prime}) \\\\\n& = \\sum (y_i (1-h)x_i - (1-y_i)hx_i )\\\\\n& = \\sum (y_i-h)x_i\n\\end{aligned}\n\\end{equation}\n$$\nwhere $i$ means $i$-th sample, and $j$ means $j$-th feature.","9635667b":"**Ensemble with same classifiers**\n- Randomly sample trainset samples for each classifier\n    - bagging, random sampling of trainset\n    - pasting, the method of randomly sampling training instances without replacement\n- Randomly sample features that are used to train each classifier\n- Specifically for decision trees, using random threshold for each feature","4867c6ff":"\u5176\u4e2d\u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff1a\n- similarity(\u6838)\u51fd\u6570\u7684\u8303\u56f4\u4e3a0\u52301\uff0c\u76f8\u4f3c\u5ea6\u6700\u9ad8\u4e3a1\uff0c\u6b64\u65f6landmark\u548csample\u503c\u76f8\u540c\uff0c\u4ee5\u6b64\u4e3a\u754c\u9650\uff0c\u8fdc\u79bb\u6b64\u754c\u9650\u7684similarity\u8d8a\u6765\u8d8a\u5c0f\n- $\\gamma$\u63a7\u5236\u7740$||x-l||^2$\u5bf9\u76f8\u4f3c\u5ea6\u7684\u5f71\u54cd\uff0c\u8f83\u5927\u7684$\\gamma$\u5bf9\u5e94\u7740\u8f83\u5c0f\u7684$||x-l||^2$\uff0c\u540c\u7406\u53cd\u4e4b\u3002\n\u521b\u5efalandmark\u7684\u65b9\u5f0f\u662f\uff0c\u5229\u7528$x$\u521b\u5efalandmark $l$\uff0c\u5373\n$$l^{(1)}=x^{(1)}, l^{(2)}=x^{(2)}, \\cdots, l^{(m)}=x^{(m)}$$\n\u5219\u6837\u672c$x^{(i)}$\u88ab\u8f6c\u4e3a\uff1a\n$$f_1^{(i)}=\\phi_\\lambda (x^{(i)}, l^{(1)})$$\n$$f_2^{(i)}=\\phi_\\lambda (x^{(i)}, l^{(2)})$$\n$$\\cdots$$\n$$f_3^{(i)}=\\phi_\\lambda (x^{(i)}, l^{(3)})$$\n\u5176\u4e2d$f_j^{(i)}$\u8868\u793a\u7684\u662f\u6837\u672c$x^{(i)}$\u548clandmark $l^{(j)}$\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff0c\u5b9a\u4e49\u5982\u4e0b\n$$f^{(i)}=[f_1^{(i)}, f_2^{(i)}, \\cdots, f_m^{(i)}]^T$$\n\u6b64\u65f6\u7528$w^T_f \\cdot f^{(i)} + b$\u6765\u53d6\u4ee3$w_x^T \\cdot x^{(i)} + b$ ","05f05e1b":"\n**Ridge Regression**\n\nRidge regression appy a regularization term proportional to the square of **l2-norm** of feature weights (not including the intercept). A common expression is:\n$$J(\\theta)=MSE(\\theta)+\\alpha(\\theta_1^2 + \\theta_2^2 + \\cdots + \\theta_n^2)$$","a7624d5f":"**Elastic Net**\n\nElastic net is somewhere between ridge regression and lasso regression. The cost function is:\n$$J(\\theta)=MSE(\\theta) + r lasso\\_penalty + (1-r) lasso\\_penalty$$\nin sklearn,\n$$J(\\theta)=MSE(\\theta) + \\alpha r lasso\\_penalty + \\alpha (1-r) lasso\\_penalty$$"}}