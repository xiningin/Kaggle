{"cell_type":{"3c0861f3":"code","c359695d":"code","9ca95035":"code","bcbdde6a":"code","95ae6203":"code","2b1f566e":"code","0499dfae":"code","b3e9d7d4":"code","595a1d08":"code","2e349f3b":"code","01843861":"code","914e6c3f":"code","f74f2c79":"code","abc6e53a":"code","41b6d269":"markdown","3af17432":"markdown","f4550661":"markdown","5bf17471":"markdown","008eabd4":"markdown","3187851b":"markdown","9f7b5f41":"markdown","d2bed303":"markdown","21b38a0a":"markdown","973e47fa":"markdown","86ffae00":"markdown","3865cdaa":"markdown"},"source":{"3c0861f3":"!pip install timm -q","c359695d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport timm","9ca95035":"# Load the data\nINPUT_PATH = '..\/input\/digit-recognizer\/'\nOUTPUT_PATH = '.\/'\n\ntest = pd.read_csv(INPUT_PATH + \"test.csv\")\ntest","bcbdde6a":"# detect and define device \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(device)","95ae6203":"class CFG:\n  N_CLASS = 10\n  BATCH_SIZE = 1024\n  model_name = 'resnet50'","2b1f566e":"class Digit_Inference_Dataset_Custom(Dataset):\n    def __init__(self, df, augmentations = None):\n        self.features = df.values\/255 # scale (greyscale) features\n        self.augmentations = augmentations \n\n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        image = self.features[idx].reshape((1, 28, 28))\n        return torch.FloatTensor(image)","0499dfae":"class Digit_Inference_Dataset_ResNet(Dataset):\n    def __init__(self, df, augmentations = None):\n        self.df = df\n        self.features = df[:].values\/255 # scale (greyscale) only features. do not scale target\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image = self.features[idx].reshape((28, 28))\n        image = torch.from_numpy(image).float()\n        image = torch.stack([image, image, image], dim = 0) # images must have 3 channels to enter timm models\n        return image","b3e9d7d4":"class Digit_Custom_Model(nn.Module):\n    def __init__(self):\n        super(Digit_Custom_Model, self).__init__()\n        \n        # Convolution to detect features and create feature maps: kernel = feature detector = filter\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5,5), padding=0)\n        \n        # activation\n        self.actv = nn.LeakyReLU()\n\n        # Batch normalization 1\n        self.batchnorm1 = nn.BatchNorm2d(32)\n        \n        # Max pool: down sample the detected features in feature maps\n        self.maxpool = nn.MaxPool2d(kernel_size=(2,2))\n\n        # Dropout\n        self.dropout = nn.Dropout(0.25) \n     \n        # Convolution\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5,5), padding=0)\n\n        # Batch normalization 2\n        self.batchnorm2 = nn.BatchNorm2d(64)        \n\n        # flatten the feature map: reduce dimensionality\n        self.flatten = nn.Flatten()\n\n        # Fully connected\n        self.fc1 = nn.Linear(64 * 4 * 4, 256)\n\n        # Batch normalization 3\n        self.batchnorm3 = nn.BatchNorm1d(256)  # 1 D because it is called after the flatten layer\n\n        # The last fully connected layer must output the number of classes\n        self.classifier = nn.Linear(256, CFG.N_CLASS)\n    \n    def forward(self, x):\n        # conv1 block\n        x = self.conv1(x)\n        x = self.actv(x)\n        x = self.batchnorm1(x)\n        x = self.maxpool(x)\n        # x = self.dropout(x)\n\n        # conv2 block\n        x = self.conv2(x)\n        x = self.actv(x)\n        x = self.batchnorm2(x)\n        x = self.maxpool(x)\n        # x = self.dropout(x)\n\n        # flatten\n        x = self.flatten(x)\n\n        # print(x.size())\n        \n        # Linear functions\n        x = self.fc1(x)\n        x = self.batchnorm3(x)\n        # x = self.dropout(x)\n        out = self.classifier(x)\n        \n        return out ","595a1d08":"class Digit_ResNet_Model(nn.Module):\n    def __init__(self, model_name = CFG.model_name, pretrained = True):\n        super().__init__()\n\n        self.model_name = model_name\n        self.cnn = timm.create_model(self.model_name, pretrained = pretrained, num_classes = CFG.N_CLASS)\n\n    def forward(self, x):\n        x = self.cnn(x)\n        return x","2e349f3b":"def softmax(x):\n    return np.exp(x)\/np.sum(np.exp(x), axis=1)[:, None]\n\ndef inference(test_loader_custom, test_loader_resnet):\n    start = time.time()\n\n    probs_custom = torch.Tensor()\n    probs_resnet = torch.Tensor()\n\n    ################################# model 1: custom model #################################\n    model_custom = Digit_Custom_Model() # instantiate custom model\n    model_custom.load_state_dict(torch.load(f\"..\/input\/pytorch-tutorial-cv-99-67-lb-99-26\/DigitModel_ep21.pth\")) # Load custom model\n    model_custom.eval() # eval mode\n    model_custom.to(device)\n\n    ################################# model 2: ResNet #################################\n    model_resnet = Digit_ResNet_Model() # instantiate ResNet model\n    model_resnet.load_state_dict(torch.load(f\"..\/input\/transfer-learning-with-timm-models-and-pytorch\/DigitModel_ep38.pth\")) # Load ResNet model\n    model_resnet.eval() # eval mode\n    model_resnet.to(device)\n\n    # disable gradients for inference\n    with torch.no_grad():\n      ################################# inference for custom model #################################      \n      for batch, X in enumerate(test_loader_custom):\n\n        X = X.to(device)\n\n        # compute predictions for custom model\n        pred_custom = model_custom(X)\n        y_pred_custom = softmax(pred_custom.detach().cpu().numpy()) # convert tensor to numpy to apply softmax\n\n        batch_probs_custom = torch.from_numpy(y_pred_custom) # convert np array back to torch tensor\n        probs_custom = torch.cat((probs_custom, batch_probs_custom), dim = 0) # concatenate softmax probabilities\n\n      ################################# inference for resnet #################################\n      for batch, X in enumerate(test_loader_resnet):\n\n        X = X.to(device)\n\n        # compute predictions for resnet model\n        pred_resnet = model_resnet(X)\n        y_pred_resnet = softmax(pred_resnet.detach().cpu().numpy()) # convert tensor to numpy to apply softmax\n\n        batch_probs_resnet = torch.from_numpy(y_pred_resnet) # convert np array back to torch tensor\n        probs_resnet = torch.cat((probs_resnet, batch_probs_resnet), dim = 0) # concatenate softmax probabilities\n\n    \n    # ensemble by probabilities: combination of the probabilities predicted by each model\n    ens = probs_custom * 0.6 + probs_resnet * 0.4\n    final_predictions = torch.argmax(ens, axis = 1) # indice of the highest probability in the ensemble (predicted digit\/class)\n    \n    # log\n    end = time.time()\n    time_delta = np.round(end - start, 5)     \n    print('Elapsed time: ', time_delta, \"s\")\n\n    # return probs_custom, probs_resnet\n    return final_predictions","01843861":"# instantiate Inference Dataset class (create inference Dataset)\ninference_dataset_custom = Digit_Inference_Dataset_Custom(test, augmentations=None)\ninference_dataset_resnet = Digit_Inference_Dataset_ResNet(test, augmentations=None)\n\n# create Inference DataLoader object from Dataset class object (for custom model)\ninference_dataloader_custom = DataLoader(inference_dataset_custom,\n                                         batch_size = CFG.BATCH_SIZE,\n                                         shuffle = False)\n\n# create Inference DataLoader object from Dataset class object (for ResNet50 model)\ninference_dataloader_resnet = DataLoader(inference_dataset_resnet,\n                                         batch_size = CFG.BATCH_SIZE,\n                                         shuffle = False)","914e6c3f":"# run inference\nfinal_predictions = inference(inference_dataloader_custom, inference_dataloader_resnet)\nfinal_predictions","f74f2c79":"submission = pd.read_csv(INPUT_PATH + \"sample_submission.csv\")\nsubmission[\"Label\"] = final_predictions\n\nsubmission.to_csv(OUTPUT_PATH + 'submission.csv', index = False)\nsubmission.head()","abc6e53a":"# check some predictions\n\nfig = plt.figure(figsize = (12, 12))\nfig.suptitle('Visualizing Predictions', fontsize = 24)\n\n# define a range of predictions to plot\nbegin = 0\nend = begin + 20\n\nfor i in range(begin, end):\n\n  img = np.array(test.iloc[i, :]).reshape(1, 1, 28, 28) # reshape to image dimensions\n  plt.subplot(4, 5, i + 1 - begin) # 4 rows and 5 columns plot \n  label = str(submission.loc[i, 'Label'])\n  plt.title(\"Predicted label: \" + label, color=\"red\") # write label in each image title\n  plt.imshow(np.squeeze(img), cmap='gray') # plot image\n  plt.axis('off')","41b6d269":"# Imports","3af17432":"# Dataset class (for custom model)","f4550661":"# Model class (ResNet50)","5bf17471":"Upvote if you found value in this notebook! \ud83d\ude00","008eabd4":"# Check predictions","3187851b":"# Inference and Ensemble","9f7b5f41":"# CFG","d2bed303":"# Load data","21b38a0a":"# Dataset class (for ResNet)","973e47fa":"# About this notebook\n\nIn this case of MNIST image classification, we cannot ensemble by taking the mean of predictions from different models, as we would do in a regression problem. This would make no sense, as this approach would, for example, take a predicted label (digit 6) from one model and a predicted label (digit 0) from other model and would output a 3, or even worse, output a float number (a class that does not exist).\n\nSo we have two options to ensemble models:\n- make predictions for several models and take the mode (most common predicted digit), just like a hard voting classifier with a majority rule; or\n- perform some calculation on the probabilities the models predicted for each class (like a soft voting classifier).\n\nIn this notebook I am ensembling only two models. So I will have to go with the second approach and I am taking a combination of the probabilities each model predicted for each sample to predict its class.\n\nFor that we will need:\n- Two Dataset classes (because the models used have different input shapes)\n- Two DataLoaders objects\n- Two Model classes\n- One inference function\n- Add data (two models): best custom model, best timm model\n- No training, only inference: just predict and ensemble.\n\nYou can find the models I am using on the links below, where they were trained:\n\n[Model 1: custom CNN model](https:\/\/www.kaggle.com\/hinepo\/pytorch-tutorial-cv-99-67-lb-99-26)\n\n[Model 2: ResNet50](https:\/\/www.kaggle.com\/hinepo\/transfer-learning-with-timm-models-and-pytorch)","86ffae00":"# Model class (custom model)","3865cdaa":"# Submission"}}