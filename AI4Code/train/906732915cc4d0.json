{"cell_type":{"481b89fd":"code","8020fb72":"code","ffc69d81":"code","5bd70fd9":"code","7c3166e4":"code","de43cf70":"code","7e0ea854":"code","723c4003":"code","fc025334":"code","a41a1063":"code","83730d1f":"code","d00d8b84":"code","45516676":"code","3635cd46":"code","9874fb7e":"code","6d04ad03":"code","e5fec007":"code","f6a6e1dd":"code","8341a313":"code","b86d4b5b":"code","d9beec2e":"code","684ba0df":"code","6428eef5":"code","cbb6db27":"code","5ff9bb9d":"code","e521c04e":"markdown","0fdf6f74":"markdown","a7af9084":"markdown","bb997432":"markdown","f934e760":"markdown","e4b01d2e":"markdown","fd2bdadf":"markdown","3d833673":"markdown","41de47d3":"markdown","01c15c4a":"markdown","8fef2e6c":"markdown","81c87c59":"markdown","863de331":"markdown","0a024b79":"markdown","bbd6b919":"markdown","89918368":"markdown","a2f9832f":"markdown","ba04626f":"markdown","2860d966":"markdown","2edf609d":"markdown","085c21aa":"markdown","a712beed":"markdown","645e86aa":"markdown","60aa2c70":"markdown","523c01b6":"markdown","364f365f":"markdown","126e7843":"markdown"},"source":{"481b89fd":"from PIL import Image\nfrom IPython.display import display\nimport numpy as np\n\n# you may want to keep logging enabled when doing your own work\nimport logging\nimport tensorflow as tf\ntf.get_logger().setLevel(logging.ERROR) # disable Tensorflow warnings for this tutorial\nimport warnings\nwarnings.simplefilter(\"ignore\") # disable Keras warnings for this tutorial\nimport keras\nfrom keras.applications import mobilenet_v2\n\nimport eli5","8020fb72":"model = mobilenet_v2.MobileNetV2(include_top=True, weights='imagenet', classes=1000)\n\n# check the input format\nprint(model.input_shape)\ndims = model.input_shape[1:3] # -> (height, width)\nprint(dims)","ffc69d81":"image_uri = '\/Users\/avnig\/XAI\/eli5\/notebooks\/imagenet-samples\/cat_dog.jpg'\n\n# check the image with Pillow\nim = Image.open(image_uri)\n\ndisplay(im)","5bd70fd9":"# we could resize the image manually\n# but instead let's use a utility function from `keras.preprocessing`\n# we pass the required dimensions as a (height, width) tuple\nim = keras.preprocessing.image.load_img(image_uri, target_size=dims) # -> PIL image\nprint(im)\ndisplay(im)","7c3166e4":"# we use a routine from `keras.preprocessing` for that as well\n# we get a 'doc', an object almost ready to be inputted into the model\n\ndoc = keras.preprocessing.image.img_to_array(im) # -> numpy array\nprint(type(doc), doc.shape)","de43cf70":"# dimensions are looking good\n# except that we are missing one thing - the batch size\n\n# we can use a numpy routine to create an axis in the first position\ndoc = np.expand_dims(doc, axis=0)\nprint(type(doc), doc.shape)","7e0ea854":"# `keras.applications` models come with their own input preprocessing function\n# for best results, apply that as well\n\n# mobilenetv2-specific preprocessing\n# (this operation is in-place)\nmobilenet_v2.preprocess_input(doc)\nprint(type(doc), doc.shape)","723c4003":"# take back the first image from our 'batch'\nimage = keras.preprocessing.image.array_to_img(doc[0])\nprint(image)\ndisplay(image)","fc025334":"# make a prediction about our sample image\npredictions = model.predict(doc)\nprint(type(predictions), predictions.shape)","a41a1063":"# check the top 5 indices\n# `keras.applications` contains a function for that\n\ntop = mobilenet_v2.decode_predictions(predictions)\ntop_indices = np.argsort(predictions)[0, ::-1][:5]\n\nprint(top)\nprint(top_indices)","83730d1f":"# we need to pass the network\n# the input as a numpy array\neli5.show_prediction(model, doc)","d00d8b84":"eli5.show_prediction(model, doc, image=image)","45516676":"cat_idx = 282 # ImageNet ID for \"tiger_cat\" class, because we have a cat in the picture\neli5.show_prediction(model, doc, targets=[cat_idx]) # pass the class id","3635cd46":"cat_idx = 285 # 'Egyptian cat'\ndisplay(eli5.show_prediction(model, doc, targets=[cat_idx]))","9874fb7e":"# we could use model.summary() here, but the model has over 100 layers. \n# we will only look at the first few and last few layers\n\nhead = model.layers[1:]\ntail = model.layers[:]\n\ndef pretty_print_layers(layers):\n    for l in layers:\n        info = [l.name, type(l).__name__, l.output_shape, l.count_params()]\n        pretty_print(info)\n\ndef pretty_print(lst):\n    s = ',\\t'.join(map(str, lst))\n    print(s)\n\npretty_print(['name', 'type', 'output shape', 'param. no'])\nprint('-'*100)\npretty_print([model.input.name, type(model.input), model.input_shape, 0])\npretty_print_layers(head)\nprint()\nprint('...')\nprint()\npretty_print_layers(tail)","6d04ad03":"for l in ['block_2_expand', 'block_9_expand', 'block_16_expand']:\n    print(l)\n    display(eli5.show_prediction(model, doc, layer=l)) # we pass the layer as an argument","e5fec007":"expl = eli5.explain_prediction(model, doc)","f6a6e1dd":"print(expl)","8341a313":"# we can access the various attributes of a target being explained\nprint((expl.targets[0].target, expl.targets[0].score, expl.targets[0].proba))","b86d4b5b":"image = expl.image\nheatmap = expl.targets[0].heatmap\n\ndisplay(image) # the .image attribute is a PIL image\nprint(heatmap) # the .heatmap attribute is a numpy array","d9beec2e":"heatmap_im = eli5.formatters.image.heatmap_to_image(heatmap)\nheatmap_im = eli5.formatters.image.expand_heatmap(heatmap, image, resampling_filter=Image.BOX)\ndisplay(heatmap_im)","684ba0df":"I = eli5.format_as_image(expl)\ndisplay(I)","6428eef5":"import matplotlib.cm\n\nI = eli5.format_as_image(expl, alpha_limit=1.0, colormap=matplotlib.cm.cividis)\ndisplay(I)","cbb6db27":"# first check the explanation *with* softmax\nprint('with softmax')\ndisplay(eli5.show_prediction(model, doc))\n\n\n# remove softmax\nl = model.get_layer(index=-1) # get the last (output) layer\nl.activation = keras.activations.linear # swap activation\n\n# save and load back the model as a trick to reload the graph\nmodel.save('tmp_model_save_rmsoftmax') # note that this creates a file of the model\nmodel = keras.models.load_model('tmp_model_save_rmsoftmax')\n\nprint('without softmax')\ndisplay(eli5.show_prediction(model, doc))","5ff9bb9d":"from keras.applications import nasnet\n\nmodel2 = nasnet.NASNetMobile(include_top=True, weights='imagenet', classes=1000)\n\n# we reload the image array to apply nasnet-specific preprocessing\ndoc2 = keras.preprocessing.image.img_to_array(im)\ndoc2 = np.expand_dims(doc2, axis=0)\nnasnet.preprocess_input(doc2)\n\nprint(model.name)\n# note that this model is without softmax\ndisplay(eli5.show_prediction(model, doc))\nprint(model2.name)\ndisplay(eli5.show_prediction(model2, doc2))","e521c04e":"Visualizing the heatmap:","0fdf6f74":"`format_as_image()` has a couple of parameters too:","a7af9084":"We see some slight differences. The activations are brighter. Do consider swapping out softmax if explanations for your model seem off.","bb997432":"The `alpha_limit` argument controls the maximum opacity that the heatmap pixels should have. It is between 0.0 and 1.0. Low values are useful for seeing the original image.\n\nThe `colormap` argument is a function (callable) that does the colorisation of the heatmap. See `matplotlib.cm` for some options. Pick your favourite color!\n\nAnother optional argument is `resampling_filter`. The default is `PIL.Image.LANCZOS` (shown here). You have already seen `PIL.Image.BOX`.","f934e760":"Now it's clear what is being highlighted. We just need to apply some colors and overlay the heatmap over the original image, exactly what `eli5.format_as_image()` does!","e4b01d2e":"These results should make intuitive sense for Convolutional Neural Networks. Initial layers detect 'low level' features, ending layers detect 'high level' features!\n\nThe `layer` parameter accepts a layer instance, index, name, or None (get layer automatically) as its arguments. This is where Grad-CAM builds its heatmap from.","fd2bdadf":"The model looks at the cat now!\n\nWe have to pass the class ID as a list to the `targets` parameter. Currently only one class can be explained at a time.","3d833673":"Let's convert back the array to an image just to check what we are inputting","41de47d3":"This time we will use the `eli5.explain_prediction()` and `eli5.format_as_image()` functions (that are called one after the other by the convenience function `eli5.show_prediction()`), so we can better understand what is going on.","01c15c4a":"## 5. Under the hood - `explain_prediction()` and `format_as_image()`","8fef2e6c":"## 8. Comparing explanations of different models\n\nAccording to the paper at https:\/\/arxiv.org\/abs\/1711.06104, if an explanation method such as Grad-CAM is any good, then explaining different models should yield different results. Let's verify that by loading another model and explaining a classification of the same image:","81c87c59":"## 7. Removing softmax\n\nThe original Grad-CAM paper (https:\/\/arxiv.org\/pdf\/1610.02391.pdf) suggests that we should use the output of the layer before softmax when doing Grad-CAM (use raw score values, not probabilities). Currently ELI5 simply takes the model as-is. Let's try and swap the softmax (logits) layer of our current model with a linear (no activation) layer, and check the explanation:","863de331":"## 6. Extra arguments to `format_as_image()`","0a024b79":"We can check the score (raw value) or probability (normalized score) of the neuron for the predicted class, and get the class ID itself","bbd6b919":"## 4. Choosing a hidden activation layer\n\nUnder the hood Grad-CAM takes a hidden layer inside the network and differentiates it with respect to the output scores. We have the ability to choose which hidden layer we do our computations on.\n\nLet's check what layers the network consists of:","89918368":"## 2. Explaining our model's prediction\n\nLet's classify our image and see where the network 'looks' when making that classification:","a2f9832f":"Looking good. Now we need to convert the image to a numpy array.","ba04626f":"## 3. Choosing the target class (target prediction)\n\nWe can make the model classify other objects and check where the classifier looks to find those objects.","2860d966":"Indeed there is a dog in that picture The class ID (index into the output layer) `243` stands for `bull mastiff` in ImageNet with 1000 classes (https:\/\/gist.github.com\/yrevar\/942d3a0ac09ec9e5eb3a ). \n\nBut how did the network know that? Let's check where the model \"looked\" for a dog with ELI5:","2edf609d":"# Explaining Keras image classifier predictions with Grad-CAM\n\nIf we have a model that takes in an image as its input, and outputs class scores, i.e. probabilities that a certain object is present in the image, then we can use ELI5 to check what is it in the image that made the model predict a certain class score. We do that using a method called 'Grad-CAM' (https:\/\/arxiv.org\/abs\/1610.02391).\n\nWe will be using images from ImageNet (http:\/\/image-net.org\/), and classifiers from `keras.applications`. \n\n\n## 1. Loading our model and data\n\nTo start out, let's get our modules in place","085c21aa":"We see that this image will need some preprocessing to have the correct dimensions! Let's resize it:","a712beed":"When explaining image based models, we can optionally pass the image associated with the input as a Pillow image object. If we don't, the image will be created from ``doc``. This may not work with custom models or inputs, in which case it's worth passing the image explicitly.","645e86aa":"Examining the structure of the `Explanation` object:","60aa2c70":"We can also access the original image and the Grad-CAM heatmap:","523c01b6":"Rough print but okay.\nLet's pick a few convolutional layers that are 'far apart' and do Grad-CAM on them:","364f365f":"And load our image classifier (a light-weight model from `keras.applications`).","126e7843":"We see that we need a numpy tensor of shape (batches, height, width, channels), with the specified height and width.\n\nLoading our sample image:"}}