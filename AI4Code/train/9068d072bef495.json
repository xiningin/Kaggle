{"cell_type":{"c1b71f10":"code","c3017760":"code","4770345f":"code","681a2e0d":"code","bf731d0f":"code","1dc888a3":"code","6cb88d91":"code","a853555d":"markdown","b5f78b87":"markdown"},"source":{"c1b71f10":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)","c3017760":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ndata = pd.concat([train_data,test_data])\n\n#data['excerpt']\u306b\u5bfe\u3057\u3066\u6539\u884c\u3092\u306a\u304f\u3059\u3002\ndata['excerpt'] = data['excerpt'].apply(lambda x: x.replace('\\n',''))\n\n#\u6539\u884c\u3092\u306a\u304f\u3057\u305fexcerpt\u3092\u6539\u884c\u3067\u5168\u7d50\u5408\u3001text\u306b\u4fdd\u5b58.\ntext  = '\\n'.join(data.excerpt.tolist())\n\nwith open('text.txt','w') as f:\n    f.write(text)","4770345f":"#pretrain\u30e2\u30c7\u30eb\u3067\u3042\u308broberta-base\u3092model\u3068\u3057\u3066\u6307\u5b9a.\nmodel_name = 'roberta-large'\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n\n#Tokenizer\n#Tokenizer\u306f\u4fdd\u5b58\u3057\u3066\u304a\u304f(\u306a\u3093\u306e\u305f\u3081\u306b\uff1f)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained('.\/clrp_roberta_large');","681a2e0d":"#\u3053\u306e\u6bb5\u968e\u3067\u306ftrain\/valid\u3068\u3082\u306b\u540c\u3058\u5168\u7d50\u5408\u3055\u308c\u305f\u30c7\u30fc\u30bf\u304c\u5143\u306b\u306a\u3063\u305fdataset\u72b6\u614b\u3002\n#tokenizer\u3092\u6307\u5b9a\u3057\u3066\u3001file_path\u306e\u6587\u5b57\u5217\u3092\u533a\u5207\u3063\u305f\u3082\u306e\u306b\u306a\u3063\u3066\u3044\u308b\uff1f\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", #mention train text file here\n    block_size=256)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", #mention valid text file here\n    block_size=256)\n\n#data_collator \n#Data collators are objects that will form a batch by using a list of \n#dataset elements as input. \n#These elements are of the same type as the elements of train_dataset or eval_dataset.\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\n#training_args\u306ftrainer\u3067\u6307\u5b9a\u3059\u308b\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u3053\u3068\u3067\u3001TrainingArguments\u95a2\u6570\u3067\u4e00\u5f0f\u3092\u5fc5\u8981\u306a\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u3066\u4e0e\u3048\u308b\ntraining_args = TrainingArguments(\n    output_dir=\".\/clrp_roberta_large_chk\", #select model path for checkpoint\n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy= 'steps',\n    save_total_limit=2,\n    eval_steps=200,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    report_to = \"none\")\n\n#\u30d7\u30ea\u30c8\u30ec\u30a4\u30cb\u30f3\u30b0\u306e\u30af\u30e9\u30b9\u3092\u4f5c\u6210\u3002\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","bf731d0f":"import torch\ntorch.cuda.empty_cache()","1dc888a3":"#\u4eca\u56de\u306e\u304a\u984c\u306b\u306a\u3063\u3066\u3044\u308bexcerpt\u3092roberta_base\u30e2\u30c7\u30eb\u306b\u30d7\u30ec\u5b66\u7fd2\u3055\u305b\u3066\u3001\n#clrp_roberta\u30e2\u30c7\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3057\u3066\u3044\u308b.\n#\u4ee5\u4e0b\u306e\uff11\u884c\u3092\u52d5\u4f5c\u3055\u305b\u3088\u3046\u3068\u3059\u308b\u3068\u30e1\u30e2\u30ea\u304c15GBit\u8d85\u3048\u3066STOP\u3059\u308b.\ntrainer.train()","6cb88d91":"\ntrainer.save_model(f'.\/clrp_roberta_large')","a853555d":"# This note book is Modified for Haruki's learning. \n# by Version2 Model is changed from roberta-base to distilroberta-base\n# By disabling CPU to finish distilroberta-base.\n\nThis notebooks shows how to pretrain any language model easily\n\n\n1. Pretrain Roberta Model: this notebook\n2. Finetune Roberta Model: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune<br\/>\n   Finetune Roberta Model [TPU]: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune-tpu\n3. Inference Notebook: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-inference\n4. Roberta + SVM: https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm\n","b5f78b87":"\u300cTransformer\u300d\u306f\u3001\u904e\u53bb\u306b\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u5206\u91ce\u3067\u591a\u304f\u4f7f\u308f\u308c\u3066\u3044\u305f\u300cRNN\u300d\uff08Recurrent Neural Network\uff09\u3084\u300cCNN\u300d\uff08Convolutional Neural Network\uff09\u3092\u300cSelf-Attention Layer\u300d\u306b\u5165\u308c\u66ff\u3048\u305f\u30e2\u30c7\u30eb\u3067\u3001\u767a\u8868\u4ee5\u6765\u3001\u591a\u304f\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30e2\u30c7\u30eb\u304c\u300cTransformer\u300d\u3067\u518d\u69cb\u7bc9\u3055\u308c\u3001\u904e\u53bb\u3092\u4e0a\u56de\u308b\u7d50\u679c\u3092\u4e0a\u3052\u307e\u3057\u305f\u3002"}}