{"cell_type":{"447757de":"code","040093dc":"code","fcbb5db6":"code","1bbebc92":"code","e291aad1":"code","48aa7026":"code","cb2ddb79":"code","9954a818":"code","f97e9026":"code","e17b84f9":"code","c2c15932":"code","c92af38d":"code","f8fb37e7":"code","02225969":"code","399dc1ff":"code","5472bbba":"code","cec667f4":"code","add1a855":"code","6e5e211c":"code","1568e59d":"code","07fa9ba0":"code","2484da09":"code","80758ce9":"code","c11760b1":"code","b2d8d4b3":"code","20b45ef5":"code","7710a3ca":"code","103e4985":"code","e466408d":"code","191f28de":"code","4fc3ddf1":"code","e2be9d19":"code","42bcd9cc":"code","3cb4fabd":"code","eddc499d":"code","99e15126":"code","e780ca1e":"code","6fd67137":"code","78b26fa9":"code","c285002c":"code","047f8ffd":"code","dc238f7c":"code","38e986bc":"code","4fcf37b8":"code","6a8e90ff":"code","84533a53":"code","14fa71df":"code","60ac25f0":"code","0103b3fe":"code","13ad1e6e":"code","bada19f9":"code","d47c9be1":"code","d86444e8":"code","2ebb90fc":"code","71ece114":"code","5084a19e":"code","fa772f51":"code","bdacb79b":"code","3a12d11f":"code","d7537d82":"code","5d35b1f4":"code","b17cef7b":"code","eb804f65":"code","9b2756d9":"code","1cb6592e":"code","ff2929ce":"code","9c154f03":"code","075d391a":"code","836761c8":"code","6b42ab55":"markdown","8eb48961":"markdown","2750c938":"markdown","c7c0c0a2":"markdown","81ffdabe":"markdown","863447ea":"markdown","06b46d71":"markdown"},"source":{"447757de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","040093dc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nsns.set(style='white', context='notebook', palette='deep')","fcbb5db6":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nIDtest = test[\"PassengerId\"]","1bbebc92":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","e291aad1":"train.loc[Outliers_to_drop] # Show the outliers rows","48aa7026":"# Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","cb2ddb79":"## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","9954a818":"# Fill empty and NaNs values with NaN\ndataset = dataset.fillna(np.nan)\n\n# Check for Null values\ndataset.isnull().sum()","f97e9026":"train.info()\ntrain.isnull().sum()","e17b84f9":"train.head()","c2c15932":"train.dtypes","c92af38d":"train.describe()","f8fb37e7":"# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \ng = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","02225969":"# Explore Age vs Sex, Parch , Pclass and SibSP\ng = sns.factorplot(y=\"Age\",x=\"Sex\",data=dataset,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=dataset,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Parch\", data=dataset,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"SibSp\", data=dataset,kind=\"box\")","399dc1ff":"# convert Sex into categorical value 0 for male and 1 for female\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})","5472bbba":"g = sns.heatmap(dataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)","cec667f4":"# Filling missing value of Age \n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"])\n                               & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","add1a855":"g = sns.factorplot(x=\"Survived\", y = \"Age\",data = train, kind=\"box\")\ng = sns.factorplot(x=\"Survived\", y = \"Age\",data = train, kind=\"violin\")","6e5e211c":"dataset[\"Name\"].head()","1568e59d":"# Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\ndataset[\"Title\"].head()","07fa9ba0":"g = sns.countplot(x=\"Title\",data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=45) ","2484da09":"# Convert to categorical values Title \ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)","80758ce9":"g = sns.countplot(dataset[\"Title\"])\ng = g.set_xticklabels([\"Master\",\"Miss\/Ms\/Mme\/Mlle\/Mrs\",\"Mr\",\"Rare\"])","c11760b1":"g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","b2d8d4b3":"# Drop Name variable\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","20b45ef5":"dataset.head()","7710a3ca":"# Create a family size descriptor from SibSp and Parch\ndataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1","103e4985":"g = sns.factorplot(x=\"Fsize\",y=\"Survived\",data = dataset)\ng = g.set_ylabels(\"Survival Probability\")","e466408d":"# Create new feature of family size\ndataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","191f28de":"g = sns.factorplot(x=\"Single\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"MedF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","4fc3ddf1":"# Explore SibSp feature vs Survived\ng = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","e2be9d19":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","42bcd9cc":"# Explore Age vs Survived\ng = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")","3cb4fabd":"# Explore Age distibution \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","eddc499d":"dataset[\"Fare\"].isnull().sum()\n#Fill Fare missing values with the median value\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())\n# Explore Fare distribution \ng = sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","99e15126":"# Apply log to Fare to reduce skewness distribution\ndataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","e780ca1e":"g = sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","6fd67137":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")","78b26fa9":"train[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","c285002c":"# convert to indicator values Title and Embarked \ndataset = pd.get_dummies(dataset, columns = [\"Title\"])\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")","047f8ffd":"dataset['Cabin'].head()","dc238f7c":"dataset['Cabin'].describe()","38e986bc":"dataset['Cabin'].isnull().sum()","4fcf37b8":"dataset[\"Cabin\"][dataset[\"Cabin\"].notnull()].head()","6a8e90ff":"# Replace the Cabin number by the type of cabin 'X' if not\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","84533a53":"g = sns.countplot(dataset[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])","14fa71df":"g = sns.factorplot(y=\"Survived\",x=\"Cabin\",data=dataset,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\ng = g.set_ylabels(\"Survival Probability\")","60ac25f0":"dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","0103b3fe":"dataset[\"Ticket\"].head()","13ad1e6e":"## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()","bada19f9":"dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")","d47c9be1":"# Create categorical values for Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")","d86444e8":"# Drop useless variables \ndataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","2ebb90fc":"dataset.head()","71ece114":"## Separate train dataset and test dataset\n\ntrain = dataset[:train_len]\ntest = dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","5084a19e":"## Separate train features and label \n\ntrain[\"Survived\"] = train[\"Survived\"].astype(int)\n\nY_train = train[\"Survived\"]\n\nX_train = train.drop(labels = [\"Survived\"],axis = 1)","fa772f51":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","bdacb79b":"# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","3a12d11f":"### META MODELING  WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,Y_train)\n\nada_best = gsadaDTC.best_estimator_","d7537d82":"gsadaDTC.best_score_","5d35b1f4":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,Y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","b17cef7b":"RFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","eb804f65":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","9b2756d9":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 20, 40, 80, 160, 320, 640]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","1cb6592e":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,Y_train,cv=kfold)","ff2929ce":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","9c154f03":"test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","075d391a":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, Y_train)","836761c8":"test_Survived = pd.Series(votingC.predict(test), name = \"Survived\")\nresults = pd.concat([IDtest, test_Survived], axis=1)\nresults.to_csv(\"ensemble_python_voting.csv\", index=False)","6b42ab55":"**5.2 Family size**\n\nWe can imagine that large families will have more difficulties to evacuate, looking for theirs sisters\/brothers\/parents during the evacuation. So, i choosed to create a \"Fize\" (family size) feature which is the sum of SibSp , Parch and 1 (including the passenger).","8eb48961":"**Categorical Values**","2750c938":"**Reference**","c7c0c0a2":"https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling","81ffdabe":"**Model Ensemble**","863447ea":"**Filling missing values**","06b46d71":"**Voting**"}}