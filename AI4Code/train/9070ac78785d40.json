{"cell_type":{"d45a11e9":"code","966089c3":"code","0a92f80a":"code","e8f20083":"code","7121586a":"code","c5bb53a6":"code","14f2eafc":"code","bb51d6a2":"code","cd131a42":"code","514fc6a5":"code","e4474c35":"code","f196b001":"code","66ec4a1d":"code","dcb36c7c":"code","447abd6c":"code","d3239064":"code","aec7bd14":"code","c096ca1b":"code","b2d35b7f":"code","d5bbc3f1":"code","6e28541b":"code","58c8f987":"code","f290723a":"code","2d0388fb":"code","1e39410a":"code","654d1a3b":"code","3d89e485":"code","d8832ce3":"code","516380ee":"markdown","a5336550":"markdown","391bbe16":"markdown","c81e67ca":"markdown","3b93a907":"markdown","6288be50":"markdown","3208b7b2":"markdown","3751c7e6":"markdown","e9fca9c8":"markdown","97d9fd43":"markdown","5681242e":"markdown","1dfc82da":"markdown","06b489e7":"markdown","58879f02":"markdown","8bc05164":"markdown","06f625be":"markdown","1746e12a":"markdown","98edaf35":"markdown","9772f631":"markdown","9e331709":"markdown","d4a12ee8":"markdown","0686a93b":"markdown","827dfe70":"markdown","a5fe6ad4":"markdown","e313cb34":"markdown","83e85c05":"markdown","aefad509":"markdown"},"source":{"d45a11e9":"import torch\nimport torchvision\nimport numpy as np\nimport pandas as pd\nfrom torch import nn, optim\n#from torchsummary import summary\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n#from torchvision.io import read_image\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.utils.data","966089c3":"import sys\nimport os\n\n# This cell imports helper functions from the linked dataset 'histopathsn'\n\nmodule_path = os.path.abspath(os.path.join('..\/input\/histopathsn'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\nmodule_path = os.path.abspath(os.path.join('..\/input\/d\/jaredfeng\/histopathsn'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom ImageData import *  # custom pytorch dataset class\nfrom EarlyStopper import * ","0a92f80a":"def reset_parameters(model):\n    '''\n    Reset all the parameters in the model to random weights, alleviates the\n    need to reinstantiate the model or restart the notebook\n    '''\n    for layer in model.children():\n        # reset the layers in the network\n        if hasattr(layer, 'reset_parameters'):\n            layer.reset_parameters()","e8f20083":"import shutil\n\nstain = False\n\nif not stain:\n  os.mkdir('patch_train')\n  os.mkdir('patch_dev')\n\n  modes = ['train', 'dev']\n  for mode in modes:\n    for cls in os.listdir(f'..\/input\/histopathology-dataset\/{mode}'):\n      for pat in os.listdir(f'..\/input\/histopathology-dataset\/{mode}\/{cls}'):\n        for img in os.listdir(f'..\/input\/histopathology-dataset\/{mode}\/{cls}\/{pat}'):\n          shutil.copyfile(f'..\/input\/histopathology-dataset\/{mode}\/{cls}\/{pat}\/{img}', f'.\/patch_{mode}\/{img}')","7121586a":"# add a patient column for the metadata\n\nlabels = pd.read_csv('..\/input\/d\/jaredfeng\/histopathsn\/train_labels.csv')\nfiles = list(labels['files'])\ntargets = list(labels.labels)\npatients = [ele.split('_')[0] for ele in files]\nnew_labels = pd.DataFrame({'files': files, 'labels': targets, 'patients': patients})\nnew_labels.to_csv('train_labels.csv', index=False)\n\nlabels = pd.read_csv('..\/input\/d\/jaredfeng\/histopathsn\/test_labels.csv')\nfiles = list(labels['files'])\ntargets = list(labels.labels)\npatients = [ele.split('_')[0] for ele in files]\nnew_labels = pd.DataFrame({'files': files, 'labels': targets, 'patients': patients})\nnew_labels.to_csv('test_labels.csv', index=False)","c5bb53a6":"# split train and validation, by creating two subset copies of the annotation csvs\nlabels = pd.read_csv('.\/train_labels.csv')\nval = labels.sample(round(len(labels)*0.1), random_state=841)\ntrain = labels.drop(val.index, axis=0)\nval.to_csv('val_labels.csv', index=False)\ntrain.to_csv('train_labels1.csv',index=False)","14f2eafc":"\nstain = False\n\nif not stain:\n  train_dir = '.\/patch_train'\n  test_dir = '.\/patch_dev'\n  norm = transforms.Normalize([152.83133,  95.23007, 139.88402], [48.893303, 50.264153, 43.022194])\nelse:\n  train_dir = '..\/input\/d\/jaredfeng\/histopathsn\/patch_he\/content\/patch_he'\n  test_dir = '..\/input\/d\/jaredfeng\/histopathsn\/test_patch_he\/content\/test_patch_he'\n  norm = transforms.Normalize([175.89737684, 137.27217724, 166.04510195],[40.82008022, 45.26915952, 35.66807751])\nIMG_SIZE = 200 # Max: 512\nBATCH_SIZE = 64\nNUM_WORKERS = 2\n\n# add simple augmentations to the training set\ntsfm = transforms.Compose([\n                           transforms.Resize(IMG_SIZE),\n                           norm,\n                           transforms.RandomApply(\n                            [transforms.RandomHorizontalFlip(),\n                            transforms.RandomVerticalFlip(),\n                             transforms.RandomRotation(90)],\n                            p=0.1)\n                           \n])\n\n# for all datasets, perform resize and normalization, with means & stds pre-computed\n# means and stds differ across stain-normalized patches and normal patches\ntest_transform = transforms.Compose([\n                     transforms.Resize(IMG_SIZE),\n                     norm\n])\n\n\n# get the data loaders \ntrain_data = ImageData(annotations_file='.\/train_labels1.csv', train_mode=True, img_dir=train_dir, transform=tsfm)\nval_data = ImageData(annotations_file='.\/val_labels.csv', train_mode=True, img_dir=train_dir, transform=test_transform)\ntest_data = ImageData(annotations_file='.\/test_labels.csv', train_mode=True, img_dir=test_dir, transform=test_transform)\n\nval_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)","bb51d6a2":"%%capture\n!pip install https:\/\/github.com\/ufoym\/imbalanced-dataset-sampler\/archive\/master.zip","cd131a42":"from torchsampler import ImbalancedDatasetSampler\nfrom CustomSampler import BalancingPatientPatchSampler\n\nbalancing_strat = 'patient'\n\n# if balancing strategy == class: Downsample majority class and oversample minority class\nif balancing_strat == 'class':\n  train_loader = torch.utils.data.DataLoader(\n      train_data,\n      sampler=ImbalancedDatasetSampler(train_data),\n      batch_size=BATCH_SIZE,\n      num_workers=NUM_WORKERS,\n      pin_memory=True\n  )\n# if balancing strategy == patient: Downsample majority patients and oversample minority patients\nelif balancing_strat == 'patient':\n    train_loader = torch.utils.data.DataLoader(\n      train_data,\n      sampler=BalancingPatientPatchSampler(train_data, num_samples=30000),\n      batch_size=BATCH_SIZE,\n      num_workers=NUM_WORKERS,\n      pin_memory=True\n  )\nelse:\n  train_loader = torch.utils.data.DataLoader(\n      train_data,\n      shuffle=True,\n      batch_size=BATCH_SIZE,\n      num_workers=NUM_WORKERS,\n      pin_memory=True)","514fc6a5":"\"\"\"\npatients = []\nfor batch in train_loader:\n  tmp = [batch['filename'][i].split('_')[0] for i in range(len(batch))]\n  patients.extend(tmp)\n\nfrom matplotlib import pyplot as plt\npatient_patch_count = np.unique(np.array(patients), return_counts=True)[1]\nplt.bar(np.arange(patient_patch_count.shape[0]), patient_patch_count)\nplt.xlabel('patients')\nplt.ylabel('# of patches')\n\"\"\"","e4474c35":"\"\"\"\n# calculate population mean and std of all 3 channels using training set\ntrain_data0 = ImageData(annotations_file='train_labels1.csv', train_mode=True, img_dir='\/content\/patch_he\/content\/patch_he', transform=test_transform)\ntrain_loader0 = DataLoader(train_data0, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\npop_mean, pop_std = [], []\nfor batch in train_loader0:   # loop to log all labels and compute pop mean and std.\n  y = batch['label']\n  y = list(y.cpu().numpy())\n  numpy_image = batch['image'].cpu().numpy()\n  # finding the mean value for each channel across all images in one batch\n  # calc mean n std for each channel in all images in a batch\n  batch_mean = np.mean(numpy_image, axis=(0,2,3)) # 0:batch, 1:channel, 2:height, 3: width\n  batch_std = np.std(numpy_image,axis=(0,2,3))\n  pop_mean.append(batch_mean)\n  pop_std.append(batch_std)\n\npop_mean = np.array(pop_mean).mean(axis=0)\npop_std = np.array(pop_std).mean(axis=0)\nprint(f'pop_mean: {pop_mean}, pop_std: {pop_std}')\n\nnorm = transforms.Normalize(pop_mean, pop_std)\n\"\"\"","f196b001":"# specify model_name, choose among ResNet, VGG, or DenseNet\nmodel_name = 'ResNet'\n\n# use gpu if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nif model_name == 'DenseNet':\n    model = torchvision.models.densenet121(pretrained=True).to(device)\n    model.classifier = nn.Sequential(nn.Linear(in_features=1024, out_features=3, bias=True), nn.Softmax()).to(device)\n\n    # freeze up lower level layers\n    for i, child in enumerate(model.children()):\n      if i == 0:\n        for j, grandchild in enumerate(child.children()):\n          if j <= 7:\n            for param in grandchild.parameters():\n              param.requires_grad=False\n            \nelif model_name == 'ResNet':\n    model = torchvision.models.resnet18(pretrained=True).to(device)\n    output_size = 3\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Sequential(nn.Linear(num_ftrs, output_size), nn.Softmax()).to(device)\n\n    # freeze up lower level layers\n    for i, child in enumerate(model.children()):\n      if i <= 6:\n        for grandchild in child.children():\n          for param in grandchild.parameters():\n            param.requires_grad = False\n            \nelse:\n    model = torchvision.models.vgg11(pretrained=True).to(device)\n    model.avgpool = nn.AdaptiveAvgPool2d(output_size=1).to(device)\n    model.classifier = nn.Sequential(\n        nn.Linear(512, 3),\n        nn.Softmax()\n    ).to(device)\n\n    # freeze up lower level layers\n    for i, child in enumerate(model.children()):\n      if i == 0:\n        for j, grandchild in enumerate(child.children()):\n          if j <= 15:\n            for param in grandchild.parameters():\n              param.requires_grad = False","66ec4a1d":"LR = 5*1e-4\nMOM = 0.9\nWD = 1e-4\n\n# create an optimizer object\noptimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOM, weight_decay=WD)\n\n# mean-squared error loss\ncriterion = nn.CrossEntropyLoss()\n\nif torch.cuda.is_available():\n  criterion = criterion.cuda()","dcb36c7c":"print(model.__class__.__name__)\ncudnn.benchmark = True\n\ntorch.manual_seed(42)\nepochs = 200\nval_accu_list = []  # get the val accu history for plotting\nmodel = model.float()\n\nimport math\nfrom time import time\n\nes = EarlyStopping(-math.inf, patience = 6, mode = 'max', path = 'checkpoint.pt')\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n\nfor epoch in range(epochs):\n    model.train()\n    loss = 0.0\n    st = time()\n    for i, batch in enumerate(train_loader):\n        X = batch['image'].to(device).float()\n        y = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(X)\n        train_loss = criterion(outputs, y)\n        train_loss.backward()\n        optimizer.step()\n        loss += train_loss.item()\n        if i % 25 == 24:  \n          print('[ep: %d, steps: %3d\/%d] train loss: %.6f, time\/step: %.4fs' %\n                (epoch + 1, i + 1, len(train_loader), loss \/ 25, (time()-st)\/25))\n          loss = 0.0\n          st = time()\n\n    correct = 0 \n    accuracy = 0 \n    loss = 0.0\n\n    # validate at the end of epoch\n    model.eval()\n    st = time()\n    for batch in val_loader:\n      X = batch['image'].to(device)\n      y = batch['label'].to(device)\n      \n      with torch.no_grad():\n        outputs = model(X.float())\n        val_loss = criterion(outputs, y)\n        loss += val_loss.item()\n        pred = np.argmax(outputs.cpu().numpy(), axis=1)\n        pred = torch.Tensor(pred).to(device)\n        correct += torch.sum(pred == y)\n\n    accuracy = correct\/(len(val_loader)*BATCH_SIZE)\n    loss \/= (len(val_loader))\n\n    # update the early stopper and record val accuracy\n    es.step(accuracy, model)\n    val_accu_list.append(accuracy)\n    # print end-of-epoch results\n    print('-'*60)\n    print(\"epoch : {}\/{}, valid accuracy = {:.6f}, valid loss = {:.6f}, time\/step: {:.4f}s\"\\\n          .format(epoch + 1, epochs, accuracy, loss, (time()-st)\/len(val_loader)))\n    print('-'*60)\n    # if early stopper says stop, then stop\n    if es.status():\n      print('early stopped!')\n      break\n    # update scheduler\n    scheduler.step(accuracy)","447abd6c":"%%capture\n! kaggle datasets download \"jaredfeng\/841projcheckpoints\"","d3239064":"if model.__class__.__name__ == 'DenseNet':\n# register the final layer before classifier so that the model outputs that layer as well during loader iterations\n# The outputs from this layer would serve as the feature vector for the non-neural model to learn & predict\n    activation = {}\n    def get_activation(name):\n        def hook(model, input, output):\n            activation[name] = torch.mean(output.detach(), dim=(2,3))\n        return hook\n    model.features.norm5.register_forward_hook(get_activation('out_features'))\nelse:\n    activation = {}\n    def get_activation(name):\n        def hook(model, input, output):\n            activation[name] = torch.mean(output.detach(), dim=(2,3))\n        return hook\n    model.avgpool.register_forward_hook(get_activation('out_features'))","aec7bd14":"# get the best model of all training epochs \nes = EarlyStopping(-math.inf, patience = 6, mode = 'max', path = 'checkpoint.pt')\nes.get_best_model(model)\n\ncorrect = 0 \naccuracy = 0 \nfilenames = []\npatients = []\ntrue_labels = []\npredicted_labels = []\nmodel.eval()\nfor i, batch in enumerate(test_loader):\n  X = batch['image'].to(device).float()\n  y = batch['label'].to(device)\n  batch_files = batch['filename']\n  true_labels.extend(list(y.cpu().numpy()))\n  batch_patients = [batch_files[j].split('_')[0] for j in range(len(batch_files))]\n  patients.extend(batch_patients)\n  filenames.extend(batch_files)\n  \n  with torch.no_grad():\n    outputs = model(X).cpu().numpy()\n    pred = np.argmax(outputs, axis=1)\n    predicted_labels.extend(pred)\n    pred = torch.Tensor(pred).to(device)\n    correct += torch.sum(pred == y)\n    if i == 0:\n      X_test_f = activation['out_features']\n    else:\n      X_test_f = torch.cat((X_test_f, activation['out_features']), dim=0)\n\naccuracy = correct\/(len(test_loader)*BATCH_SIZE)\nprint('test accuracy: ', accuracy.item())\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nsns.heatmap(confusion_matrix(true_labels, predicted_labels, normalize=None), cmap='Blues', annot=True)\n\nX_test_f = X_test_f.cpu().numpy()\ndf = pd.DataFrame(X_test_f)\ndf['true_label'] = true_labels\ndf['patient'] = patients\nX_test_f = pd.DataFrame(df.groupby(['patient']).mean()).drop(['true_label'], axis=1).to_numpy()\ny_test_f = pd.DataFrame(df.groupby(['patient']).mean())['true_label'].to_numpy()\n\n# prepare patch level statistics (for test) for the non-neural model\nfirst_lvl_res = pd.DataFrame({'file': filenames, 'patient': patients, 'true label': true_labels, 'pred': predicted_labels})\ninput_2nd = pd.DataFrame(first_lvl_res[['patient','true label','pred']].groupby(['patient', 'true label', 'pred']).size().unstack(fill_value=0).reset_index())\ninput_2nd = input_2nd.drop(['patient'], axis=1)\nX_test = input_2nd.drop(['true label'], axis=1)\nX_test = X_test.to_numpy()\nif X_test.shape[1] == 3:\n    X_test = X_test\/np.array([np.sum(X_test, axis=1), np.sum(X_test, axis=1), np.sum(X_test, axis=1)]).T\nelse: \n    X_test = X_test\/np.array([np.sum(X_test, axis=1), np.sum(X_test, axis=1)]).T\ny_test = input_2nd['true label'].to_numpy()","c096ca1b":"# prepare patch level statistics (for training) for the non-neural model\nmodel.eval()\n\ntrain_data = ImageData(annotations_file='train_labels.csv', train_mode=True, img_dir=train_dir, transform=test_transform)\ntrain_loader0 = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\nfilenames = []\npatients = []\ntrue_labels = []\npredicted_labels = []\n\nfor i, batch in enumerate(train_loader0):\n  X = batch['image'].to(device).float()\n  y = list(batch['label'].cpu().numpy())\n  batch_files = batch['filename']\n  true_labels.extend(y)\n  batch_patients = [batch_files[j].split('_')[0] for j in range(len(batch_files))]\n  patients.extend(batch_patients)\n  filenames.extend(batch_files)\n\n  with torch.no_grad():\n    outputs = model(X).cpu().numpy()\n    pred = list(np.argmax(outputs, axis=1))\n    predicted_labels.extend(pred)\n    if i == 0:\n      X_train_f = activation['out_features']\n    else:\n      X_train_f = torch.cat((X_train_f, activation['out_features']), dim=0)\n\n    \nfirst_lvl_res = pd.DataFrame({'file': filenames, 'patient': patients, 'true label': true_labels, 'pred': predicted_labels})\ninput_2nd = pd.DataFrame(first_lvl_res[['patient','true label','pred']].groupby(['patient', 'true label', 'pred']).size().unstack(fill_value=0).reset_index())\ninput_2nd = input_2nd.drop(['patient'], axis=1)\nX_train = input_2nd.drop(['true label'], axis=1).to_numpy()\nif X_train.shape[1] == 3:\n    X_train = X_train\/np.array([np.sum(X_train, axis=1), np.sum(X_train, axis=1), np.sum(X_train, axis=1)]).T\nelse: \n    X_train = X_train\/np.array([np.sum(X_train, axis=1), np.sum(X_train, axis=1)]).T\ny_train = input_2nd['true label'].to_numpy()\n\nX_train_f = X_train_f.cpu().numpy()\ndf = pd.DataFrame(X_train_f)\ndf['true_label'] = true_labels\ndf['patient'] = patients\nX_train_f = pd.DataFrame(df.groupby(['patient']).mean()).drop(['true_label'], axis=1).to_numpy()\ny_train_f = pd.DataFrame(df.groupby(['patient']).mean())['true_label'].to_numpy()\n","b2d35b7f":"from matplotlib import pyplot as plt\n\nplt.figure(0)\nyh_pred = np.argmax(X_train, axis=1)\nprint('TRAIN ACCURACY:', accuracy_score(y_train, yh_pred))\nsns.heatmap(confusion_matrix(y_train, yh_pred), cmap='Blues', annot=True)\n\nplt.figure(1)\nyh_pred = np.argmax(X_test, axis=1)\nprint('TEST ACCURACY:', accuracy_score(y_test, yh_pred))\nsns.heatmap(confusion_matrix(y_test, yh_pred), cmap='Blues', annot=True)","d5bbc3f1":"from sklearn.svm import SVC\n\nsvc = SVC(random_state=841, C=1, kernel='rbf', class_weight='balanced')\nsvc.fit(X_train, y_train)\n\nplt.figure(0)\nyh_pred = svc.predict(X_train)\nprint('TRAIN ACCURACY:', accuracy_score(y_train, yh_pred))\nsns.heatmap(confusion_matrix(y_train, yh_pred), cmap='Blues', annot=True)\n\nplt.figure(1)\nyh_pred = svc.predict(X_test)\nprint('TEST ACCURACY:', accuracy_score(y_test, yh_pred))\nsns.heatmap(confusion_matrix(y_test, yh_pred), cmap='Blues', annot=True)","6e28541b":"from xgboost import XGBClassifier\n\ncls = XGBClassifier(objective=\"multi:softmax\", random_state=2021, min_child_weight=30, n_estimators=1000, max_depth=3)\ncls.fit(X_train, y_train)\n\nplt.figure(0)\nyh_pred = cls.predict(X_train)\nprint('TRAIN ACCURACY:', accuracy_score(y_train, yh_pred))\nsns.heatmap(confusion_matrix(y_train, yh_pred), cmap='Blues', annot=True)\n\nplt.figure(1)\nyh_pred = cls.predict(X_test)\nprint('TEST ACCURACY:', accuracy_score(y_test, yh_pred))\nsns.heatmap(confusion_matrix(y_test, yh_pred), cmap='Blues', annot=True)\n","58c8f987":"from sklearn.svm import SVC\n\n#svc = SVC(random_state=841, C=0.1, kernel='rbf', class_weight='balanced')\nsvc = SVC(random_state=841, C=10, kernel='rbf', class_weight='balanced')\nsvc.fit(X_train_f, y_train_f)\n\nplt.figure(0)\nyh_pred = svc.predict(X_train_f)\nprint('TRAIN ACCURACY:', accuracy_score(y_train_f, yh_pred))\nsns.heatmap(confusion_matrix(y_train_f, yh_pred), cmap='Blues', annot=True)\n\nplt.figure(1)\nyh_pred = svc.predict(X_test_f)\nprint('TEST ACCURACY:', accuracy_score(y_test_f, yh_pred))\nsns.heatmap(confusion_matrix(y_test_f, yh_pred), cmap='Blues', annot=True)","f290723a":"%%capture\n!pip install optuna","2d0388fb":"from sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport optuna\n\ndef tune1(fix_params, num_trials):\n  def objective(trial):\n      accuracies = []\n      kf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 940)\n      tmp_params = fix_params\n      tune_params = {\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 6)\n        }\n      tmp_params.update(tune_params)\n      #print(tune_params)\n      cls = XGBClassifier(**tmp_params)\n      scores = []\n      for idx in kf.split(X=X_train_f,y=y_train_f):\n          train_idx, test_idx = idx[0], idx[1]\n          #print('train idx:', train_idx, 'test idx:', test_idx)\n          train_x = X_train_f[train_idx]\n          train_y = y_train_f[train_idx]\n          valid_x = X_train_f[test_idx]\n          valid_y = y_train_f[test_idx]\n          cls.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0)\n          #pred_probs = cls.predict_proba(valid_x)\n          pred = cls.predict(valid_x)\n          scores.append(accuracy_score(valid_y, pred))\n      return np.mean(scores)\n  study = optuna.create_study(direction=\"maximize\")\n  study.optimize(objective, n_trials=num_trials)\n  print(\"Best trial:\")\n  trial = study.best_trial\n  print(\"  Value: {}\".format(trial.value))\n  print(\"  Params: \")\n  for key, value in trial.params.items():\n      print(\"    {}: {}\".format(key, value))\n  fix_params.update(trial.params)\n  return fix_params\n\nparam1 = {\n    \"objective\": 'mutli:softmax',\n    \"tree_method\": \"gpu_hist\",\n    \"gamma\": 0,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"scale_pos_weight\": 1,\n    \"eta\": 0.1,\n    \"random_state\": 2021,\n    \"n_estimators\": 500,\n}\nTRIALS = 5\nparam2 = tune1(param1, num_trials=TRIALS)\nparam2.pop('gamma', None)","1e39410a":"def tune2(fix_params, num_trials):\n  def objective(trial):\n      accuracies = []\n      kf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 940)\n      tmp_params = fix_params\n      tune_params = {\n        \"gamma\": trial.suggest_float(\"gamma\", 0.01, 5.)\n        }\n      tmp_params.update(tune_params)\n      #print(tune_params)\n      cls = XGBClassifier(**tmp_params)\n      scores = []\n      for idx in kf.split(X=X_train_f,y=y_train_f):\n          train_idx, test_idx = idx[0], idx[1]\n          #print('train idx:', train_idx, 'test idx:', test_idx)\n          train_x = X_train_f[train_idx]\n          train_y = y_train_f[train_idx]\n          valid_x = X_train_f[test_idx]\n          valid_y = y_train_f[test_idx]\n          cls.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0)\n          #pred_probs = cls.predict_proba(valid_x)\n          pred = cls.predict(valid_x)\n          scores.append(accuracy_score(valid_y, pred))\n      return np.mean(scores)\n  study = optuna.create_study(direction=\"maximize\")\n  study.optimize(objective, n_trials=num_trials)\n  print(\"Best trial:\")\n  trial = study.best_trial\n  print(\"  Value: {}\".format(trial.value))\n  print(\"  Params: \")\n  for key, value in trial.params.items():\n      print(\"    {}: {}\".format(key, value))\n  fix_params.update(trial.params)\n  return fix_params\n\nTRIALS = 5\nparam3 = tune2(param2, num_trials = TRIALS)\nparam3.pop(\"subsample\", None)\nparam3.pop(\"colsample_bytree\", None)","654d1a3b":"def tune3(fix_params, num_trials):\n  def objective(trial):\n      accuracies = []\n      kf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 940)\n      tmp_params = fix_params\n      tune_params = {\n        \"subsample\":trial.suggest_float(\"subsample\", 0.6, 1.),\n        \"colsample_bytree\":trial.suggest_float(\"colsample_bytree\", 0.6, 1.)\n        }\n      tmp_params.update(tune_params)\n      #print(tune_params)\n      cls = XGBClassifier(**tmp_params)\n      scores = []\n      for idx in kf.split(X=X_train_f,y=y_train_f):\n          train_idx, test_idx = idx[0], idx[1]\n          #print('train idx:', train_idx, 'test idx:', test_idx)\n          train_x = X_train_f[train_idx]\n          train_y = y_train_f[train_idx]\n          valid_x = X_train_f[test_idx]\n          valid_y = y_train_f[test_idx]\n          cls.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0)\n          #pred_probs = cls.predict_proba(valid_x)\n          pred = cls.predict(valid_x)\n          scores.append(accuracy_score(valid_y, pred))\n      return np.mean(scores)\n  study = optuna.create_study(direction=\"maximize\")\n  study.optimize(objective, n_trials=num_trials)\n  print(\"Best trial:\")\n  trial = study.best_trial\n  print(\"  Value: {}\".format(trial.value))\n  print(\"  Params: \")\n  for key, value in trial.params.items():\n      print(\"    {}: {}\".format(key, value))\n  fix_params.update(trial.params)\n  return fix_params\n\nTRIALS = 5\nparam4 = tune3(param3, num_trials = TRIALS)","3d89e485":"def tune4(fix_params, num_trials):\n  def objective(trial):\n      accuracies = []\n      kf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 940)\n      tmp_params = fix_params\n      tune_params = {\n        \"alpha\":trial.suggest_float(\"alpha\", 1e-5, 1e+2)\n        }\n      tmp_params.update(tune_params)\n      #print(tune_params)\n      cls = XGBClassifier(**tmp_params)\n      scores = []\n      for idx in kf.split(X=X_train_f,y=y_train_f):\n          train_idx, test_idx = idx[0], idx[1]\n          #print('train idx:', train_idx, 'test idx:', test_idx)\n          train_x = X_train_f[train_idx]\n          train_y = y_train_f[train_idx]\n          valid_x = X_train_f[test_idx]\n          valid_y = y_train_f[test_idx]\n          cls.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0)\n          #pred_probs = cls.predict_proba(valid_x)\n          pred = cls.predict(valid_x)\n          scores.append(accuracy_score(valid_y, pred))\n      return np.mean(scores)\n  study = optuna.create_study(direction=\"maximize\")\n  study.optimize(objective, n_trials=num_trials)\n  print(\"Best trial:\")\n  trial = study.best_trial\n  print(\"  Value: {}\".format(trial.value))\n  print(\"  Params: \")\n  for key, value in trial.params.items():\n      print(\"    {}: {}\".format(key, value))\n  fix_params.update(trial.params)\n  return fix_params\n\nTRIALS = 5\nparam5 = tune4(param4, num_trials = TRIALS)\nparam5.pop(\"eta\", None)\nparam5.pop(\"n_estimators\", None)\nparam5.update({\"eta\":1e-2, \"n_estimators\":5000})\n","d8832ce3":"cls = XGBClassifier(**param5)\ncls.fit(X_train_f, y_train_f)\n\nplt.figure(0)\nyh_pred = cls.predict(X_train_f)\nprint('TRAIN ACCURACY:', accuracy_score(y_train_f, yh_pred))\nsns.heatmap(confusion_matrix(y_train_f, yh_pred), cmap='Blues', annot=True)\n\nplt.figure(1)\nyh_pred = cls.predict(X_test_f)\nprint('TEST ACCURACY:', accuracy_score(y_test_f, yh_pred))\nsns.heatmap(confusion_matrix(y_test_f, yh_pred), cmap='Blues', annot=True)","516380ee":"Then we tune the `gamma` (minimum loss reduction required to make a split)","a5336550":"### SVM (votes)\nRuns SVM on the votes.","391bbe16":"### XGB (features)\nRuns XGBoost on the feature vectors;\n\nWe use a random search package `optuna` [8] and stratified k-fold to tune hyperparameters for the XGBoost classifier. Search spaces are intuitively determined. However with experiments, we discover that running `optuna` on Colab and Kaggle kernels might generate different results.","c81e67ca":"## References:\n\n[1] M. Macenko, M. Niethammer, J. Marron, D. Borland, J. Woosley, X. Guan, C. Schmitt and N. Thomas, \"A Method for Normalizing Histology Slides for Quantitative Analysis\", University of North Carolina, Chapel Hill, 2009. [Online]. Available: http:\/\/wwwx.cs.unc.edu\/~mn\/sites\/default\/files\/macenko2009.pdf. [Accessed: 21- Dec- 2021]\n\n[2] M. Yang, \"Imbalanced Dataset Sampler\", GitHub, 2021. [Online]. Available: https:\/\/github.com\/ufoym\/imbalanced-dataset-sampler. [Accessed: 17- Dec- 2021]\n\n[3] G. Huang, Z. Liu, L. van der Maaten and K. Weinberger, \"Densely Connected Convolutional Networks\", 2016. [Online]. Available: https:\/\/openaccess.thecvf.com\/content_cvpr_2017\/papers\/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf. [Accessed: 21- Dec- 2021]\n\n[4] A. Riasatian, M. Babaie, D. Maleki, S. Kalra, M. Valipour, S. Hemati, M. Zaveri, A. Safarpoor, S. Shafiei, M. Afshari, M. Rasoolijaberi, M. Sikaroudi, M. Adnan, S. Shah, C. Choi, S. Damaskinos, C. Campbell, P. Diamandis, L. Pantanowitz, H. Kashani, A. Ghodsi and H. Tizhoosh, \"Fine-Tuning and Training of DenseNet for Histopathology Image Representation Using TCGA Diagnostic Slides\", 2021. [Online]. Available: https:\/\/arxiv.org\/abs\/2101.07903. [Accessed: 21- Dec- 2021]\n\n[5] K. He, X. Zhang, S. Ren and J. Sun, \"Deep Residual Learning for Image Recognition\", 2015. [Online]. Available: https:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2016\/papers\/He_Deep_Residual_Learning_CVPR_2016_paper.pdf. [Accessed: 21- Dec- 2021]\n\n[6] K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition\", 2015. [Online]. Available: https:\/\/arxiv.org\/abs\/1409.1556. [Accessed: 21- Dec- 2021]\n\n[7] T. Chen and C. Guestrin, \"XGBoost: A Scalable Tree Boosting System\", 2016. [Online]. Available: https:\/\/www.kdd.org\/kdd2016\/papers\/files\/rfp0697-chenAemb.pdf. [Accessed: 21- Dec- 2021]\n\n[8] T. Akiba, S. Sano, T. Yanase, T. Ohta and M. Koyama, \"Optuna: A Next-generation Hyperparameter Optimization Framework\", 2019. [Online]. Available: https:\/\/arxiv.org\/abs\/1907.10902. [Accessed: 21- Dec- 2021]","3b93a907":"#### Optimizer & Loss function","6288be50":"#### torch datasets and dataloaders","3208b7b2":"### (If only postprocessing) Download checkpoints of all runs","3751c7e6":"### XGB (votes)\nRuns XGBoost on the votes;\nOriginal paper of XGBoost: [7]","e9fca9c8":"Finally we tune `alpha`, which is the L1 regularization factor. After that, we reduce the learning rate and increase the number of trees to get the final XGBoost classifier on the feature vectors.","97d9fd43":"#### Training (with early stopping & learning rate scheduler)","5681242e":"Initially fix some main hyperparameters, tune `max_depth` and `min_child_weight` first","1dfc82da":"#### Visualization of patch distribution per patient (done pre-run)\n\nHere we plot bar graphs of the number of patches per patient. The x-axis represents different patients, and the y-axis is the number of patches.\n\nTo rerun the three cells below, simply choose one balancing strategy and create `train_loader` in the cell above.","06b489e7":"## Data Loaders, Sampling & Normalization Setup","58879f02":"## Stain Normalization (done pre-run)\n\nShould be conducted with both training & test sets.\n\nOriginal paper [1]: http:\/\/wwwx.cs.unc.edu\/~mn\/sites\/default\/files\/macenko2009.pdf","8bc05164":"## Packages & Helper Functions","06f625be":"### Get patch level prediction performance & feature vectors for image-level test  ","1746e12a":"## Patch Classifier Selection & Training\n\nChoose one of the following models as the patch-level classifier.","98edaf35":"#### Balancing strategies using samplers\n\nTo balance with respect to target labels, we adopt an external script for the sampler written by [2]. \n\nAnd to balance with respect to patients, we implement our own sampler by modifying the script by [2].","9772f631":"### Max Voting\nReturns the label that has the maximum votes in each patient","9e331709":"Then we tune `subsample` (proportion of dataset in the bootstrapped data) and `colsample_bytree` (proportion of features in the bootstrapped data). Both are tree-level bootstrapping hyperparameters.","d4a12ee8":"## File Path Operations (if using images without Stain norm)","0686a93b":"## Patch-level Inference & Postprocessing\n\nAfter training the patch classifier, we'd like to extract info from the well-trained (supposedly) patch classifier to train a patient-, or image-level classifier. There are two approaches to extract such info. \n\n(1) Say a patient has `m` patches, then the patch classifier would output `m` labels that we call '**votes**', each label could be `0`, `1`, or `2`. So for each patient, we count the three-way votes and divide them by the sum of votes. Now for a patient with `m` patches, they would have three numbers as the input into the patient classifier, `m_1\/m`, `m_2\/m`, `m_3\/m`, with `m_i` being the vote count for the `i`-th label. \n\n(2) Another method is to extract a '**feature vector**' for each patch, using the final layer of the convolutional network, right before the fully-connected layers. For a patient with `m` patches, they will have `m` feature vectors with length `L` (dependent on the CNN model, i.e. the patch classifier). So we then compute the element-wise average of the feature vectors, so that regardless of how many patches a patient has, their input for the patient classifier would always be of size `L`. For both the training and test sets, we perform the same postprocesses, and only pass the results from the training side to the patient classifier.","827dfe70":"### Get feature vectors & labels for image level training","a5fe6ad4":"### SVM (features)\nRuns SVM on the feature vectors","e313cb34":"#### DenseNet121, ResNet18, VGG11\n\nOriginal paper of DenseNets: [3];\n\nThe selection of layers to freeze is based on: [4];\n\nOriginal paper of ResNets: [5];\n\nOriginal paper of VGG networks: [6]","83e85c05":"## Image Classifier with Max Vote, SVM & XGB\n","aefad509":"#### Find means and sds for all channels (done pre-run)\n\nSee the `torchvision.transforms.Normalize()` functions in the data loader cell for the outputs"}}