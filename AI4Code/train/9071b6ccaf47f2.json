{"cell_type":{"6ca91f14":"code","13a56baa":"code","73c30ab9":"code","7a6599bb":"code","3048d4ad":"code","fb18dfc0":"code","6159f655":"code","728b77f4":"code","e25bfddd":"code","1f402b1b":"code","238e6e61":"code","1f0beb0d":"code","19e783a8":"code","6bf15684":"code","114e8429":"code","cf2746f5":"code","63f23fa2":"code","6a23bef1":"code","c30d5f40":"code","e1af724c":"code","ac381cc5":"code","a84a3e9b":"code","6ed9cf67":"code","ab2dd26e":"code","5f03b38e":"code","e023b6d8":"code","bab1e47a":"code","8307016e":"code","2db230a5":"code","0bcda378":"code","4a9003c0":"code","7aa26da7":"code","5ae3a89d":"code","c7b5722f":"code","9f4e9823":"code","89787d91":"code","39ee6459":"code","10da3d39":"code","87195a4d":"code","8a9cb3c5":"code","3ae90ae6":"code","93fe7842":"code","151275e9":"code","f0e7812a":"code","87816c4c":"code","151f5983":"code","5f6e887f":"code","be822448":"code","702ecc78":"code","3fbecaed":"code","40aca07d":"code","95278ba7":"code","c816d077":"code","7de9806e":"code","e4dd0956":"code","2c5f8850":"code","1265a7d1":"code","f5889aef":"code","ae53c5b9":"code","ba6e8a3a":"code","e1f8fac8":"code","3efbdf5b":"code","844371d3":"code","5decf37d":"code","4b14bcd1":"code","64dfa8d5":"code","442288e1":"code","b94541dd":"code","0e275d42":"code","db59e360":"code","7ebcc0d5":"code","319f1a58":"code","3a5ec82f":"code","25ad7bd2":"code","5c0ae26d":"code","3c85c0f3":"code","41f291e2":"code","98351ba8":"markdown","b857f964":"markdown","3be10aff":"markdown","56f1bd72":"markdown","2135c120":"markdown","7092286e":"markdown","5d591cca":"markdown","60170aca":"markdown","26acb599":"markdown","367986e5":"markdown","cf9435d0":"markdown","a779c1d4":"markdown","bcc7d8e6":"markdown","b62c28ca":"markdown","186ea1fa":"markdown","8a026ddd":"markdown","0148832c":"markdown","49382f45":"markdown","321aa7a3":"markdown","9228b551":"markdown","ef0259e2":"markdown","d1b3dc0c":"markdown","f0f76a6d":"markdown","7b34ca8c":"markdown","0c69afac":"markdown","ffacbcab":"markdown","a9ca8675":"markdown","df0a4a9e":"markdown","e8b1739b":"markdown","59b7f9b8":"markdown","e5554ea6":"markdown","e7ce334e":"markdown"},"source":{"6ca91f14":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","13a56baa":"df = pd.read_csv(r'..\/input\/telecom-churncsv\/telecom_churn.csv')","73c30ab9":"df.head(3)","7a6599bb":"df.isnull().sum()","3048d4ad":"print(f'We can directly use any method which gives us {df.isnull().sum().any()}, \\\nhence there are no null values. To check the duplicate we have {df.duplicated().any()}, hence no duplicates as well in the \\\ndataset.')","fb18dfc0":"df.info()","6159f655":"d1 = df.select_dtypes(exclude = 'object')\nd2 = df.select_dtypes(include = 'object')","728b77f4":"d1.describe()","e25bfddd":"print(f'The columns in the dataframe with numerical data types are :- {d1.columns.tolist()}. \\n\\nTotal number = {len(d1.columns.tolist())}')\nprint('\\n')\nprint(f'The columns which are the categorical variables are :- {d2.columns.tolist()}. \\n\\nTotal number = {len(d2.columns.tolist())}')","1f402b1b":"##We converted the columns for d1 and d2 from dataframes to lists. We remove phone numbers as it is of string type.\nnon_categorical=d1.columns.tolist()\ncategorical=d2.columns.tolist()","238e6e61":"categorical.remove('phone number')","1f0beb0d":"##We create a copy of the original dataframe for data wrangling so that we have something to bank upon if needed.\ndf1 = df.copy()","19e783a8":"df1['international plan'].replace(['no','yes'], [0,1],inplace=True)\ndf1['voice mail plan'].replace(['no','yes'], [0,1],inplace=True)","6bf15684":"##We could have done that using a loop only\nfor i in categorical[1:]:\n    print(df1[str(i)].replace([0,1],['no','yes']))\n    print('\\n\\n\\n\\n\\n\\n') #--for some spacing","114e8429":"df1[categorical].head(3)\n##Hence we see that that we have changed the variables","cf2746f5":"a = round((df1.churn.value_counts(normalize=True)[True])*100,2)\nb = round((df1.churn.value_counts(normalize=True)[False])*100,2)","63f23fa2":"df1.churn.value_counts(normalize=True).plot(kind='bar', figsize=(15,5), title = 'Porportion of churn vs Non-churn', color='r',xlabel='CATEGORIES',ylabel='PROPORTION')","6a23bef1":"print(f'Proportion of customers who churned: {round((df1.churn.value_counts(normalize=True)[True])*100,2)}%')\nprint(f'Proportion of customers who did not churn: {round((df1.churn.value_counts(normalize=True)[False])*100,2)}%')","c30d5f40":"d = dict()\nfor i in df1['state'].unique():\n    d[i]=round(df1[df1[\"state\"]==i][\"churn\"].mean()*100,0)","e1af724c":"[(k,v) for k,v in d.items() if v == max(list(d.values()))]","ac381cc5":"[(k,v) for k,v in d.items() if v == min(list(d.values()))]","a84a3e9b":"# bars = list(d.keys())\n# values = list(d.values())\n# plt.bar(bars,values)\nfig, ax=plt.subplots(figsize=(30,15))\nsns.countplot(data = df1, x='state', order=df1['state'].value_counts().index, palette='viridis', hue='churn')\nplt.xlabel('State', fontsize=30, fontweight='bold')\nplt.ylabel('Customers', fontsize=30, fontweight='bold')\nplt.title('State wise Customers', fontsize=30, fontweight='bold')\nplt.xticks(fontsize=25,rotation=90)\nplt.yticks(fontsize=30)\nplt.show()","6ed9cf67":"df1['servicesAvailed_years'] = round((df1['account length']\/12),0)","ab2dd26e":"yrs_churn = df1.loc[df1['churn'] == True]\nyrs_nonChurn = df1.loc[df1['churn'] == False]","5f03b38e":"fig, (ax_churned,ax_nonChruned) = plt.subplots(1,2,figsize=(17,8))\n\nyrs_churn['servicesAvailed_years'].hist(ax = ax_churned)\nyrs_nonChurn['servicesAvailed_years'].hist(ax = ax_nonChruned)\n\nax_churned.set(title = 'People who churned', ylabel='frequency', xlabel='value')\nax_nonChruned.set(title = 'People who did not churn', ylabel='frequency', xlabel='value')\nplt.show()","e023b6d8":"print(df1[df1['total intl calls']>0]['international plan'].unique())\nprint(df1[df1['number vmail messages']>0]['voice mail plan'].unique())\n##Here 0 denotes False while 1 denotes True as these features were encoded earlier.","bab1e47a":"print(str(round((len(df1.loc[(df1['total intl calls']>0) & (df1['international plan']==0)]['total intl calls']))\/\n                len(df1['total intl calls'])*100,2)) + '%')","8307016e":"df1['total intl calls'].plot(kind='hist', alpha=0.8, color = 'brown')","2db230a5":"z=df1.groupby(['churn']).agg({'customer service calls': 'count'}).reset_index()","0bcda378":"z.set_index([pd.Index(['People who did not churn','People who churned']),'churn'])","4a9003c0":"z['proportion'] = z['customer service calls']\/np.sum(z['customer service calls'])*100 ","7aa26da7":"z","5ae3a89d":"p = pd.DataFrame(np.random.randint(3,7,(60,2)), columns = list('ab'))\np.skew()","c7b5722f":"p.hist()","9f4e9823":"df2 = df1.drop({'churn'}, axis=1)","89787d91":"df2.columns","39ee6459":"from matplotlib.pyplot import figure\nfigure(figsize=(15,8))\nsns.heatmap(df2.corr(), xticklabels=df2.columns.values, yticklabels=df2.columns.values, \n            annot=True, cmap=\"nipy_spectral_r\",annot_kws={'size': 10},fmt=\".2f\")","10da3d39":"df1.drop({'phone number'},axis=1,inplace=True)","87195a4d":"d1.drop({'churn'}, axis=1, inplace=True)","8a9cb3c5":"df1['day_rate'] = df1['total day charge']\/df1['total day minutes']\ndf1['eve_rate'] = df1['total eve charge']\/df1['total eve minutes']\ndf1['night_rate'] = df1['total night charge']\/df1['total night minutes']","3ae90ae6":"df1.drop({'total day charge','total eve charge','total night charge'}, axis=1,inplace=True)","93fe7842":"df1.columns","151275e9":"d1_skew = d1.skew()","f0e7812a":"d1_skew.to_frame(name='Skew')","87816c4c":"d1_skew = d1_skew.to_frame(name = 'values')","151f5983":"skew_thresh = 0.75 ##We set the threshold here used for the log transform\nd1_skew.head(3)","5f6e887f":"d1_skew['values'] = np.round(d1_skew.values, decimals=3) #--we change the decimal places","be822448":"d1_skew.head(3)","702ecc78":"d1_skew_updated =d1_skew[d1_skew['values']>skew_thresh]","3fbecaed":"d1_skew_updated.drop({'area code'}, axis=0,inplace=True) #--We delete the area code column and change it to string type","40aca07d":"df1['area code'] = df1['area code'].astype('str')","95278ba7":"d1_skew_updated","c816d077":"fig, (ax_before,ax_after) = plt.subplots(1,2,figsize = (13,6))\n\ndf1['total intl calls'].hist(ax = ax_before, color='r') \n\ndf1['total intl calls'].apply(lambda x: np.log1p(x)).hist(ax = ax_after, color = 'r')","7de9806e":"##We apply the log transform on the columns.\nskew_final_list = list(d1_skew_updated.T.columns)\nskew_final_list","e4dd0956":"for c in skew_final_list:\n    df1[c] = df1[c].apply(lambda x: np.log1p(x))","2c5f8850":"df1[skew_final_list].head() ##--Hence the values have been changed tro their logarithmic values.","1265a7d1":"df[df['churn']==True]['customer service calls'].count()","f5889aef":"df[df['churn']==False]['customer service calls'].count()","ae53c5b9":"z = df.groupby('churn').agg({'customer service calls' : 'sum'})\nprint(f'The proportion of calls made by people of churned are : {(int(z.values[1])\/df[\"customer service calls\"].sum())*100} %')","ba6e8a3a":"a = df.groupby(['area code', 'churn']).agg({'customer service calls' : 'sum', 'state': 'count'}).reset_index()\na.rename(columns = {'state': 'Number of customers'})\n# a['Average calls'] = a['customer service calls']\/a['Number of customers']","e1f8fac8":"a['Average calls'] = a['customer service calls']\/a['state']","3efbdf5b":"a","844371d3":"test_df = df1[['area code','churn']]\ntest_df['churn'] = test_df['churn'].astype('object')","5decf37d":"test_df.head(10)","4b14bcd1":"contingency_table = pd.crosstab(test_df['area code'], test_df['churn'])\nprint('contingency_table :-\\n',contingency_table)","64dfa8d5":"#Observed values\nObserved_Values = contingency_table.values \nprint('Observed Values :- \\n', Observed_Values)","442288e1":"from scipy import stats","b94541dd":"b = stats.chi2_contingency(contingency_table)\nexpected_values = b[3]\nprint('Expected values:- \\n', expected_values)\nprint('\\n')\nprint('*'*80)\nprint('\\n')\nprint(b) #{This returns chi sq statistic, p-value,df and expected values in order. We have put b[3] above as we only \n#wanted expected values}","0e275d42":"n_rows = contingency_table.shape[0]\nn_cols = contingency_table.shape[1]\ndegrees_of_freedom = (n_rows-1)*(n_cols-1)\nprint('Degrees of freedom:- \\n', degrees_of_freedom)\nalpha=0.5","db59e360":"np.array([1,3])-np.array([2,3])","7ebcc0d5":"type(Observed_Values)","319f1a58":"chi_square = pd.DataFrame([(o-e)**2\/e for o,e in pd.Series(zip(Observed_Values,expected_values))])\nchi_square_statistic=np.sum(chi_square.values)\nchi_square_statistic","3a5ec82f":"from scipy.stats import chi2\ncritical_value = chi2.ppf(q=1-alpha,df=degrees_of_freedom)\nprint('Critical value is :', critical_value)","25ad7bd2":"#p-value\np_value = 1-chi2.cdf(x=chi_square_statistic, df=degrees_of_freedom)\nprint('P-value is:', p_value)","5c0ae26d":"comp = pd.Series({'Significance level':alpha,'Degree of Freedom':degrees_of_freedom, \n                     'chi-square statistic':chi_square_statistic, 'critical_value': critical_value, 'p-value':p_value })","3c85c0f3":"comp.to_frame(name='values')","41f291e2":"print(\"Comparing the test statistic with the critical value\")\nif chi_square_statistic>=critical_value:\n    print(\"Reject H0,There is a relationship between 2 variables\")\nelse:\n    print(\"Fail to reject H0,There is no relationship between the 2 variables\")\n\nprint(\"\\nComapring the p-value with the significance level\")\nif p_value<=alpha:\n    print(\"Reject H0,There is a relationship between 2 variables\")\nelse:\n    print(\"Fail to reject H0,There is no relationship between the 2 variables\")","98351ba8":"#### Checking the categorical variables","b857f964":"### Hence we conclude that customers have churned the most after completing 7-10 years(As per plot 1)","3be10aff":"### Now We look into the customer service calls made and the churning rate. We use the saved copy of the df1 for this purpose ie df","56f1bd72":"![image.png](attachment:image.png)\n\n### We compute the chi-square statistic with the help of the obeserved and the expected values","2135c120":"#### Note that the degrees of freedom given is same as what was calculated by the stats.chi2_contingency test from the library scipy.\n\n### Now below note that we can subtract two numpy arrays element wise as is shown below. We will use this further. This property of the numpy arrays is also called as the \"Broadcasting\"","7092286e":"# Hypothesis Testing","5d591cca":"### Hence we conclude that the following:\n\n#### 1. New Jersey and California topped the lists of states having the highest rate of churning equivalent to 26%.\n#### 2. Similarly -  Virginia, Arizona, Hawaii & Alaska were the states with the lowest chruning rate in the US, equivalent to 6% ","60170aca":"### Now we compute the critical value and p-value to compare our test statistic with ","26acb599":"## Creation of new variables","367986e5":"## We observe the State wise churn ","cf9435d0":"#### After skimming through the dataset of the telecom churn, we have a basic idea of the data which has around 3k samples with 21 attributes - 20 attributes as the features and 'churn' as the target variable. Customer churn refers to the situation of the customer leaving the current telecom provider\/customer switching to rival telecom provider for the telecom services.","a779c1d4":"### Applying the log transform\n\n#### Let us see on ''total intl calls''-the effect of converting the values to their log values in order to lower the skewness. After plotting the values we observe that the logarithm values are less skewed and nearly normal. This helps a great deal when using these features in any ML models.","bcc7d8e6":"### Now we are gonna remove the phone number column as it is comprises completely unique values(that is 3333 unique entries) which might not help us in the predictions.\n","b62c28ca":"#### Hence we see that there are no null values in the dataset. We can directly check it by the following ways:-","186ea1fa":"* The degrees of freedom is equal to \"(r-1)(c-1)\", where r is the number of rows and c is the number of columns in the contigency table. So here we have 3 rows and 2 columns.","8a026ddd":"### Now We look into the customer service calls made and the churning rate. We use the saved copy of the df1 for this purpose.","0148832c":"## EDA and Data cleaning","49382f45":"### Now we look into the proportion of churn against the proportion of non-churn","321aa7a3":"### We see that people have just made 2-6 calls at the most which explains the low number of customers opting for the international plans due to less usage of the same.","9228b551":"### Moreover, we observe that the call charges have correlation with some of the other features like the calls and minutes spend(day, evening, night) etc. So we calculate the rate calls and delete these charges' columns from the data set. This step is to tackle the multicollinearity that might prove to be helpful.","ef0259e2":"Now we will create the hypothesis testing. The hypothesis that we form is the following:-\n\n* H0(null hypothesis) - There is no relation between the area code and the churning rate.\n* H1(Alternate hypothesis) - There is a significant relation between the area code and the churning rate.\n\n##### We use the chi-square test to test this hypothesis. The test is applied when you have two categorical variables from a single population. It is used to determine whether there is a significant association between the two variables.\n\n##### Since the test has two object variables, hence this would be the most suited test for arriving at a conclusion.","d1b3dc0c":"### We have the skewness of all the numeric columns. We set a threshold and the columns violating the thresholds would be transformed into a log of its values to curb the skewness. This is known as the log transform of the data.","f0f76a6d":"### Encoding the categorical features\n##### Note that State column has 51 unique values so we are gonna treat them as string catefgoricals only. We are gonna vhange the other two.","7b34ca8c":"### We conclude that area 415 had the highest number of churns with the average calls being close to 2","0c69afac":"#### We see that Phone number is highly perfectly correlated with the state. Also, phone numbers are unique and would not provide much in the predictions if an ML model is deployed hence it is better if we delete the whole column(my understanding).\n\n#### Hence we delete the phone number column from the df1 that we have been working on.","ffacbcab":"#### Hence we see that there were no customers who made voice emails without the designated plan devised by the telecom provider, however, there were some customers who made International calls without the correspondong international plan.\n\n##### Let's see the proportion of those customers.","a9ca8675":"### Hence around 90% of the customers did not have the international plan but made international calls. Let us look at the number of international calls of the samples in the dataset.","df0a4a9e":"### Hence we observe that only 21% of the customer service calls were from those who ultimately churned which indicates that the calls might have been made for various other purposes apart from redressals.\n\n### We make a more detailed analysis below.","e8b1739b":"### Now we have the Account length which tells us the number of months the customer has been availing the services for. We create a new column with the services availed in years and then visualize the number of years churned customers availed the servcies for.","59b7f9b8":"### Vital Computations","e5554ea6":"### We first find the correlation between different features in order to check for the Multicollinearity. Multicollinearity refers to two or more features being highly correlated with one another in a model. Multicollinearity can be a problem in a regression model because we would not be able to distinguish between the individual effects of the independent variables on the dependent variable.\n\n#### We remove the column churn as that is our TARGET variable.","e7ce334e":"## Now we observe if there were any customers making international calls without any international plan or customers making voice emails without any corresponding plan"}}