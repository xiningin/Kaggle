{"cell_type":{"9eaf3285":"code","424b3f22":"code","289d7279":"code","2015ea45":"code","22d9943d":"code","8acd9ccf":"code","150ef6e5":"code","c1ee13a0":"code","8ccdad19":"code","1c2af125":"code","ed9b59dc":"code","370d6a76":"code","2f3c0c2b":"code","42704095":"code","8b52d7b3":"markdown","9339d4da":"markdown","a1972cfc":"markdown","9aafd05b":"markdown","a6e36d60":"markdown","6267ce98":"markdown","6d93dc13":"markdown","5e9ecc09":"markdown","f5520d19":"markdown","faf75d2e":"markdown"},"source":{"9eaf3285":"# General libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","424b3f22":"# File paths to get Kaggle data\ninput_path = '..\/input\/'\ntrain_path = input_path + 'train\/train\/'\ntest_path = input_path + 'test\/test\/'\n\n# Load data\ntrain_df = pd.read_csv(input_path + 'train.csv')\nsample = pd.read_csv(input_path + 'sample_submission.csv')\n\n# Get ids and labels\ntrain_id = train_df['id']\nlabels = train_df['has_cactus']\ntest_id = sample['id']","289d7279":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(train_id, labels, test_size=0.2)","2015ea45":"def fetch_images(ids, filepath):\n    # Array to load images into\n    arr = []\n    for img_id in ids:\n        img = plt.imread(filepath + img_id)\n        arr.append(img)\n        \n    # Turn into numpy array and normalize pixel values\n    arr = np.array(arr).astype('float32')\n    arr = arr \/ 255\n    return arr","22d9943d":"# Redefine sets to contain images and not ids\nx_train = fetch_images(ids=x_train, filepath=train_path)\nx_val = fetch_images(ids=x_val, filepath=train_path)\ntest = fetch_images(ids=test_id, filepath=test_path)\n\n# Get dimensions of each image\nimg_dim = x_train.shape[1:]","8acd9ccf":"fig, ax = plt.subplots(nrows=2, ncols=3)\nax = ax.ravel()\nplt.tight_layout(pad=0.2, h_pad=2)\n\nfor i in range(6):\n    ax[i].imshow(x_train[i])\n    ax[i].set_title('has_cactus = {}'.format(y_train.iloc[i]))","150ef6e5":"# Layers for the full model\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Flatten, Dropout, LeakyReLU, Activation\nfrom keras.layers.normalization import BatchNormalization\n\n# Pre-trained model\nfrom keras.applications.densenet import DenseNet201","c1ee13a0":"# Hyperparameters\nbatch_size = 64\nepochs = 30\nsteps = x_train.shape[0] \/\/ batch_size","8ccdad19":"# Inputs\ninputs = Input(shape=img_dim)\n\n# DenseNet\ndensenet201 = DenseNet201(weights='imagenet', include_top=False)(inputs)\n\n# Our FC layer\nflat1 = Flatten()(densenet201)\ndense1 = Dense(units=256, use_bias=True)(flat1)\nbatchnorm1 = BatchNormalization()(dense1)\nact1 = Activation(activation='relu')(batchnorm1)\ndrop1 = Dropout(rate=0.5)(act1)\n\n# Output\nout = Dense(units=1, activation='sigmoid')(drop1)\n\n# Create Model\nmodel = Model(inputs=inputs, outputs=out)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')","1c2af125":"from keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=2, mode='max')\n\nimg_aug = ImageDataGenerator(rotation_range=20, vertical_flip=True, horizontal_flip=True)\nimg_aug.fit(x_train)","ed9b59dc":"# Show architecture of model\nfrom keras.utils import plot_model\nprint(model.summary())\nplot_model(model, to_file='densenet201_model.png')","370d6a76":"model.fit_generator(img_aug.flow(x_train, y_train, batch_size=batch_size), \n                    steps_per_epoch=steps, epochs=epochs, \n                    validation_data=(x_val, y_val), callbacks=[reduce_lr], \n                    verbose=2)","2f3c0c2b":"test_pred = model.predict(test, verbose=2)","42704095":"sample['has_cactus'] = test_pred\nsample.to_csv('densenet_model.csv', index=False)","8b52d7b3":"## Fitting and getting results\n\nSince we're using `ImageDataGenerator`, we need to fit the model using Keras' `fit_generator()` instead of just `.fit()`:","9339d4da":"Finally, we need an array containing the actual images. The following function will fetch these images from the ids that we already have:","a1972cfc":"Here we create the architecture for the model. I like using Keras' Functional API to build the models but using the Sequential approach yields the same results:","9aafd05b":"We also want to get a validation set to get some metrics while training:","a6e36d60":"# Using Pre-trained DenseNet for Aerial Cactus Identification\nDataset from: https:\/\/www.kaggle.com\/c\/aerial-cactus-identification\/overview","6267ce98":"Finally, get predictions from the model and produce a submission file:","6d93dc13":"Finally, use `ReduceLROnPlateau` to deal with a plateauing learning rate and `ImageDataGenerator` to make sure our model is trained on different variations of the same picture:","5e9ecc09":"## Building the model\nThe model will consist of a pre-trained DenseNet, specifically DenseNet201 that consists of 201 layers and was trained using the imagenet dataset. Then it will be followed by a fully connected layer of our own:","f5520d19":"Lets see what some of these images look like. The resolution is so low (32x32 pixels) that it is hard to understand what the image shows. Intuitively, one can tell that the cacti in the images are shown as white lines:","faf75d2e":"## Loading and processing data  \nThe datasets loaded here contain only the ID number of the picture and the label (whether it contains a cactus or not). The actual image is contained in a sepparate folder and each file is named after its ID:"}}