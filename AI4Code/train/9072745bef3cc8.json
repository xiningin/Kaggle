{"cell_type":{"bfe0adf0":"code","fc39dbdb":"code","30612379":"code","3df77c81":"code","e027b693":"code","da50d4a7":"code","9249c5f9":"code","1d23c735":"code","137cde29":"code","c072e955":"code","180c7b14":"code","7ec87892":"code","338e29c7":"code","205b005d":"code","da9dc372":"code","d5de8d1a":"code","09e82e12":"code","25b3d1c1":"code","37ec8b90":"code","cd14b3b3":"code","5e519b37":"code","687e030d":"code","4276c57a":"code","6b446b88":"code","97facf2e":"code","748ee8ce":"code","ea26465b":"code","8b982672":"code","97c5f271":"code","0a87c75f":"code","bb1e8755":"code","ebc47c4a":"code","68faecd7":"code","bcf557c5":"code","f9f851e6":"code","cfe21c9f":"code","f12a5aa1":"code","230631a9":"code","c6c77add":"code","6386541f":"code","f6e96984":"code","0fc98826":"code","8fb13e16":"code","7d53ce9f":"code","16c81dba":"code","7e7fd6ba":"code","31a3bda1":"code","3a3e5ab9":"code","7c5a1cd8":"code","cba098f7":"code","6b270022":"code","f6c09c51":"code","52744f78":"code","3f631ee3":"code","560cb6b2":"code","ef2419cb":"code","07ff1db8":"code","220db45b":"code","9c81b78d":"code","294c7098":"code","0cf2433d":"code","bf0ce89d":"code","e8be43fd":"code","c30fff96":"code","c5a0f449":"code","1e471334":"code","ea71f2bd":"code","314b0e00":"code","3fe5ca8b":"code","af8b7d3a":"code","e3cca34d":"code","554359e9":"code","800d0efe":"markdown","47310fad":"markdown","cfd198cd":"markdown","a871e3b8":"markdown","dc177fb1":"markdown","99b25bcb":"markdown","cc2bf93a":"markdown","1b82239a":"markdown","ba5097c1":"markdown","ec65f3e1":"markdown","11cdc158":"markdown","dbcfa244":"markdown","4a9ca3a5":"markdown","445bf87e":"markdown","2a4328f0":"markdown","51afbb06":"markdown","83bf5df9":"markdown","0d528a55":"markdown","fa07d1a9":"markdown","8945c34b":"markdown","bd75a854":"markdown","0400b94f":"markdown","aafec72e":"markdown","e4af7a3a":"markdown","24ee3a78":"markdown","b0374263":"markdown","c0317e3c":"markdown","5b5c7062":"markdown","84815408":"markdown","c50d73b1":"markdown","0a6d7d7c":"markdown","14fd9aa1":"markdown","c2006871":"markdown","9cf95b06":"markdown","48f70eda":"markdown","a091643e":"markdown","2c67a00e":"markdown","df14ad09":"markdown","46c28126":"markdown"},"source":{"bfe0adf0":"import pandas as pd\n\ntrain = pd.read_csv('\/kaggle\/input\/tmdb-box-office-prediction\/train.csv')\n\nprint(train.shape)\ntrain.head()","fc39dbdb":"test = pd.read_csv('\/kaggle\/input\/tmdb-box-office-prediction\/test.csv')\nprint(test.shape)","30612379":"data = pd.concat([train, test], sort=False).reset_index()\ndata = data.drop('index', axis=1)\nprint(data.shape)","3df77c81":"import ast\n\ndict_columns = ['belongs_to_collection','genres','production_companies','production_countries','spoken_languages','Keywords','cast','crew']\n\ndef get_dict(item):\n    try:\n        new_item = ast.literal_eval(item)\n    except:\n        new_item = {}\n    return new_item\n\nfor col in dict_columns:\n#     train[col] = train[col].apply(lambda x: {} if pd.isnull(x) else ast.literal_eval(x))\n#     test[col] = test[col].apply(lambda x: {} if pd.isnull(x) else ast.literal_eval(x))\n#     data[col] = data[col].apply(lambda x: {} if pd.isnull(x) else ast.literal_eval(x))\n#     train[col] = train[col].apply(lambda x: get_dict(x))\n#     test[col] = test[col].apply(lambda x: get_dict(x))\n    data[col] = data[col].apply(lambda x: get_dict(x))","e027b693":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ntrain_null_pct = train.isnull().sum().sort_values() \/ len(train)\ntest_null_pct = test.isnull().sum().sort_values() \/ len(test)\n\nfig, ax = plt.subplots(1,2, figsize=(10,8), sharey=False)\nfig.subplots_adjust(wspace=0.8)\nax[0].barh(train_null_pct.index, train_null_pct)\nax[0].set_title('Train dataset Null')\nax[0].set_xlabel('Null proportion')\nax[1].barh(test_null_pct.index, test_null_pct)\nax[1].set_title('Test dataset Null')\nax[1].set_xlabel('Null proportion')\n\nprint(data.isnull().sum())","da50d4a7":"data['has_homepage'] = data['homepage'].apply(lambda x: 0 if pd.isnull(x) else 1)\ndata = data.drop('homepage', axis=1)\ndata.shape","9249c5f9":"data['tagline'] = data['tagline'].fillna('')","1d23c735":"data['overview'] = data['overview'].fillna('')","137cde29":"data = data.drop('poster_path', axis=1)\ndata.shape","c072e955":"data.loc[data['release_date'].isnull(), 'title']\n## Jails, Hospitals & Hip-Hop\n# It released on May 2000\ndata.loc[data['release_date'].isnull(), 'release_date'] = '05\/01\/2000'","180c7b14":"data.loc[data['title'].isnull(), ['id','original_title']]\ndata.loc[data['id']==5399, 'title'] = 'The Life of Guskou Budori'  #\u30b0\u30b9\u30b3\u30fc\u30d6\u30c9\u30ea\u306e\u4f1d\u8a18\ndata.loc[data['id']==5426, 'title'] = ''  #La V\u00e9rit\u00e9 si je Mens ! 3  # couldn't find english title\ndata.loc[data['id']==6629, 'title'] = 'Barefoot'  #Barefoot","7ec87892":"data.loc[data['runtime'].isnull(), 'runtime'] = data['runtime'].median()\ndata.loc[data['runtime']==0, 'runtime'] = data['runtime'].median()","338e29c7":"print(data['status'].isnull().sum())\nprint(train['status'].value_counts())\nprint(test['status'].value_counts())","205b005d":"data = data.drop('status', axis=1)\ndata.shape","da9dc372":"data['revenue'].min()","d5de8d1a":"import numpy as np\n\nfig, ax = plt.subplots(1,2, figsize=(10,5))\nax[0].hist(data['revenue'])\nax[0].set_title('revenue')\nax[1].hist(np.log(data['revenue']+1))\nax[1].set_title('log_revenue')\nplt.show()\n\ndata['log_revenue'] = np.log(data['revenue']+1)  # add 1 to avoid log(0)","09e82e12":"data['belongs_to_collection'] = data['belongs_to_collection'].apply(lambda x: len(x))","25b3d1c1":"tmp_train = data.iloc[:3000]\nisin_collection_rev = tmp_train.loc[tmp_train['belongs_to_collection']==1, 'revenue']\nnotin_collection_rev = tmp_train.loc[tmp_train['belongs_to_collection']==0, 'revenue']\n\nisin_collection_rev_log = tmp_train.loc[tmp_train['belongs_to_collection']==1, 'log_revenue']\nnotin_collection_rev_log = tmp_train.loc[tmp_train['belongs_to_collection']==0, 'log_revenue']\n\nfig, ax = plt.subplots(1,2, figsize=(12,5))\nax[0].boxplot([isin_collection_rev_log, notin_collection_rev_log])\nax[0].set_title('log_revenue')\nax[0].set_xticklabels(['in_collection', 'not_in_collection'])\nax[1].boxplot([isin_collection_rev, notin_collection_rev])\nax[1].set_title('revenue')\nax[1].set_xticklabels(['in_collection', 'not_in_collection'])\nplt.show()","37ec8b90":"plt.hist(data['budget'])\nplt.xlabel('Budget')\nplt.ylabel('Counts')","cd14b3b3":"data['log_budget'] = np.log(data['budget']+1)  # add 1 to avoid log(0)\nfig, ax = plt.subplots(1,2, figsize=(10,5))\nax[0].scatter(data['budget'], data['revenue'], alpha=0.1)\nax[0].set_title('budget - revenue')\nax[0].set_xlabel('budget')\nax[0].set_ylabel('revenue')\nax[1].scatter(data['log_budget'], data['log_revenue'], alpha=0.1)\nax[1].set_title('log_budget - log_revenue')\nax[1].set_xlabel('log_budget')\nax[1].set_ylabel('log_revenue')","5e519b37":"gen_cnt = data['genres'].apply(lambda x: len(x)).value_counts()\nplt.bar(gen_cnt.index, gen_cnt)\nplt.xticks(range(gen_cnt.index.max()+1))\nplt.title('Number of genres that movies in')\nplt.xlabel('Number of genres')\nplt.ylabel('Movie counts')\nplt.show()","687e030d":"import collections\nfrom wordcloud import WordCloud\n\ntotal_gen_list = []\n\ndef gen_list(x):\n    for i in x:\n        total_gen_list.append(i['name'])\n\n        \nfig, ax = plt.subplots(1,2, figsize=(20,7))\ndata['genres'].apply(lambda x: gen_list(x))\ngen_cnt = collections.Counter(total_gen_list).most_common()\nfor gen, cnt in gen_cnt[::-1]:\n    ax[0].barh(gen,cnt)\nax[0].set_title('Genre Frequencies')\n\nwordcloud = WordCloud(background_color='white', width=800, height=500).generate_from_frequencies(dict(gen_cnt))\nax[1].axis('off')\nax[1].imshow(wordcloud, interpolation = 'bilinear')\nax[1].set_title('Genre majorities')\nplt.show()","4276c57a":"data['genres_list'] = data['genres'].apply(lambda x: [i['name'] for i in x])\n\nfor gen in dict(gen_cnt).keys():\n    data['genre_'+gen] = data['genres_list'].apply(lambda x: 1 if gen in x else 0)","6b446b88":"tmp_train = data[:3000]\nplt.figure(figsize=(6,6))\nfor idx, gen in enumerate(dict(gen_cnt).keys()):\n    plt.boxplot(tmp_train.loc[tmp_train['genre_'+gen]==1,'log_revenue'], labels=[gen], positions=range(idx, idx+1), vert=False)\nplt.xlabel('log_revenue')","97facf2e":"data = data.drop(['genres', 'genres_list'], axis=1)","748ee8ce":"data = data.drop('imdb_id', axis=1)","ea26465b":"data['year'] = data['release_date'].str.split('\/').apply(lambda x: 2000+int(x[2]) if int(x[2]) < 19 else 1900+int(x[2]))\ndata.loc[data['year']==3900,'year'] = 2000   ## there is a typo in dataset\ndata['month'] = data['release_date'].str.split('\/').apply(lambda x: int(x[0]))\ndata = data.drop('release_date', axis=1)","8b982672":"fig, ax = plt.subplots(2,2, figsize=(13,10))\nax[0,0].scatter(data['year'], data['revenue'], alpha=0.3, s=20)\nax[0,0].set_title('year - revenue')\nax[0,0].set_xlabel('year')\nax[0,0].set_ylabel('revenue')\nax[0,1].scatter(data['year'], data['log_revenue'], alpha=0.3, s=20)\nax[0,1].set_xlabel('year')\nax[0,1].set_ylabel('log_revenue')\nax[0,1].set_title('year - log_revenue')\nax[1,0].scatter(data['month'], data['revenue'], alpha=0.3, s=20)\nax[1,0].set_xlabel('month')\nax[1,0].set_ylabel('revenue')\nax[1,0].set_title('month - revenue')\nax[1,1].scatter(data['month'], data['log_revenue'], alpha=0.3, s=20)\nax[1,1].set_title('month - log_revenue')\nax[1,1].set_xlabel('month')\nax[1,1].set_ylabel('log_revenue')\nplt.show()","97c5f271":"data = pd.get_dummies(data, columns=['month'], drop_first=True)","0a87c75f":"def find_director(x):\n    director=''\n    for i,v in enumerate(x):\n        if v['job']=='Director':\n            director = v['name']\n    return director\n\ndef find_writer(x):\n    writer=''\n    for i,v in enumerate(x):\n        if v['job'] == 'Writer':\n            writer = v['name']\n    return writer\n\ndata['director'] = data['crew'].apply(lambda x: find_director(x))\ndata['writer'] = data['crew'].apply(lambda x: find_writer(x))\n","bb1e8755":"collections.Counter(data['director']).most_common(20)","ebc47c4a":"collections.Counter(data['writer']).most_common(20)","68faecd7":"data = data.drop(['director', 'writer', 'crew'], axis = 1)","bcf557c5":"data['cast_list'] = data['cast'].apply(lambda x: [i['name'] for i in x])\ndata['n_cast'] = data['cast'].apply(lambda x: len(x))","f9f851e6":"total_cast_list = []\ndef get_cast_list(x):\n    total_cast_list.extend(x)\n\ndata['cast_list'].apply(lambda x: get_cast_list(x))\ntop_cast = list(dict(collections.Counter(total_cast_list).most_common(100)).keys())","cfe21c9f":"collections.Counter(total_cast_list).most_common(20)","f12a5aa1":"for cast in top_cast:\n    data['cast_'+cast] = data['cast_list'].apply(lambda x: 1 if cast in x else 0)","230631a9":"tmp_train = data[:3000]\nfor idx, cast in enumerate(top_cast[:23][::-1]):\n    cast_rev = tmp_train.loc[tmp_train['cast_'+cast]==1, 'log_revenue']\n    plt.boxplot(cast_rev, positions=range(idx, idx+1), labels=[cast], vert=False)\nplt.xlabel('log_revenue')","c6c77add":"data = data.drop(['cast', 'cast_list'], axis=1)","6386541f":"# choose language which \ntop_langs = dict(data['original_language'].value_counts()[:17]).keys()\n\nfor lang in top_langs:\n    data['lang_'+lang] = data['original_language'].apply(lambda x: 1 if lang == x else 0)","f6e96984":"data['n_spoken_languages'] = data['spoken_languages'].apply(lambda x: len(x))","0fc98826":"for lang in list(top_langs)[::-1]:\n    plt.barh(lang, data.loc[data['lang_'+lang]==1, 'log_revenue'])\nplt.xlabel('log_revenue')\nplt.title('original_language')","8fb13e16":"plt.scatter(data['n_spoken_languages'], data['log_revenue'], alpha=.3)\nplt.xlabel('n_spoken_languages')\nplt.ylabel('log_revenue')\nplt.title('spoken_languages')","7d53ce9f":"data = data.drop(['original_language','spoken_languages','n_spoken_languages'], axis=1)","16c81dba":"data['keyword_str'] = data['Keywords'].apply(lambda x: ', '.join([i['name'] for i in x]))","7e7fd6ba":"data['text'] = data['title'] + '. '+ data['tagline'] + '. ' + data['overview'] + '. ' + data['keyword_str']","31a3bda1":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nvect = CountVectorizer(ngram_range=(1,3), stop_words='english')\nX = vect.fit_transform(data['text'])\nlda = LatentDirichletAllocation(n_components=10, random_state = 0)\ndocument_topics = lda.fit_transform(X)","3a3e5ab9":"n = 8\n# Get features (tokens) from CountVectorizer\nfeature_names = np.array(vect.get_feature_names())\n# Find top n tokens\ntopics = dict()\nfor idx, component in enumerate(lda.components_): \n    top_n_indices = component.argsort()[:-(n + 1): -1] \n    topic_tokens = [feature_names[i] for i in top_n_indices] \n    topics[idx] = topic_tokens\n\ntopics","7c5a1cd8":"plt.figure(figsize=(5,5))\nfor k, v in collections.Counter(document_topics.argmax(axis=1)).items():\n    plt.bar(k,v)\nplt.xlabel('Topic clusters')\nplt.ylabel('Number of movies')\nplt.title('Topic frequencies')","cba098f7":"data['topics'] = document_topics.argmax(axis=1)\ntmp_train = data[:3000]\nplt.figure(figsize=(5,5))\nfor idx in range(10):\n    plt.boxplot(tmp_train.loc[tmp_train['topics']==idx, 'log_revenue'],positions=range(idx, idx+1), labels=[idx])\nplt.xlabel('topics')\nplt.ylabel('log_revenue')","6b270022":"data = pd.get_dummies(data, columns=['topics'], drop_first=True)","f6c09c51":"data = data.drop(['original_title','overview','tagline','title','Keywords', 'keyword_str','text'], axis=1)","52744f78":"data['n_production_countries'] = data['production_countries'].apply(lambda x: len(x))\ndata['n_production_companies'] = data['production_companies'].apply(lambda x: len(x))","3f631ee3":"company_list = []\ndef get_company_list(x):\n    for i in x:\n        company_list.append(i['name'])\ndata['production_companies'].apply(lambda x: get_company_list(x))\nfor company in dict(collections.Counter(company_list).most_common(30)).keys():\n    data['production_'+company] = data['production_companies'].apply(lambda x: 1 if company in [i['name'] for i in x] else 0)","560cb6b2":"data = data.drop(['production_companies', 'production_countries'], axis=1)","ef2419cb":"fig, ax = plt.subplots(1,2, figsize=(12,5))\nax[0].scatter(data['n_production_countries'], data['log_revenue'], alpha=0.1)\nax[0].set_xlabel('n_production_countries')\nax[0].set_ylabel('log_revenue')\nax[1].scatter(data['n_production_companies'], data['log_revenue'], alpha=0.1)\nax[1].set_xlabel('n_production_companies')\nax[1].set_ylabel('log_revenue')","07ff1db8":"tmp_train = data[:3000]\nplt.figure(figsize=(10,10))\nfor idx, company in enumerate(dict(collections.Counter(company_list).most_common(30)).keys()):\n    com_rev = tmp_train.loc[tmp_train['production_'+company]==1, 'log_revenue']\n    plt.boxplot(com_rev, positions=range(idx, idx+1), labels = [company], vert=False)\nplt.xlabel('log_revenue')","220db45b":"data['runtime_cat'] = pd.qcut(data['runtime'],10, labels=False)","9c81b78d":"plt.scatter(data['runtime_cat'], data['log_revenue'], alpha=0.1)","294c7098":"tmp_train = data[:3000]\nfor i in range(10):\n    plt.boxplot(tmp_train.loc[data['runtime_cat']==i,'log_revenue'], positions=range(i, i+1))\nplt.xlabel('runtime categories')\nplt.ylabel('log_revenue')","0cf2433d":"data = data = pd.get_dummies(data, columns=['runtime_cat'], drop_first=True)\ndata = data.drop(['runtime'],axis=1)","bf0ce89d":"def rmse_score(y1, y2):\n    return np.sqrt(np.power(y1-y2,2).mean())","e8be43fd":"X = data[:3000].drop(['id','revenue','log_revenue', 'budget'],axis=1)\ny = data[:3000]['log_revenue']\n\nsub_X = data[3000:].drop(['id','revenue','log_revenue','budget'], axis=1)","c30fff96":"from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)","c5a0f449":"from sklearn.linear_model import LinearRegression\n\nLR_model = LinearRegression()\nLR_model.fit(X_train,y_train)\ny_hat = LR_model.predict(X_test)\nprint(rmse_score(y_hat, y_test))\nsub_y = LR_model.predict(sub_X)","1e471334":"sub_csv = pd.DataFrame({'id':data[3000:]['id'], 'revenue': np.exp(sub_y)})\nsub_csv.to_csv('LR_predict.csv', index=False)\n\n## 2.40021","ea71f2bd":"from sklearn.svm import SVR\n\nSVR_model = SVR(C = 5)\nSVR_model.fit(X_train, y_train)\n\ny_hat = SVR_model.predict(X_test)\nprint(rmse_score(y_hat, y_test))\nsub_y = SVR_model.predict(sub_X)","314b0e00":"sub_csv = pd.DataFrame({'id':data[3000:]['id'], 'revenue': np.exp(sub_y)})\nsub_csv.to_csv('SVR_predict.csv', index=False)\n\n## 2.27148   # c=1.0\n## 2.21529   # c=5\n## 2.21807   # c=10","3fe5ca8b":"from sklearn.ensemble import RandomForestRegressor\n\nRF_model = RandomForestRegressor(random_state =0, n_estimators=500, max_depth=10)\nRF_model.fit(X_train, y_train)\n\ny_hat = RF_model.predict(X_test)\nprint(rmse_score(y_hat, y_test))\nsub_y = RF_model.predict(sub_X)","af8b7d3a":"sub_csv = pd.DataFrame({'id':data[3000:]['id'], 'revenue': np.exp(sub_y)})\nsub_csv.to_csv('RF_predict.csv', index=False)\n\n## 2.20717   #n_esimators=200, max_depth=8\n## 2.20054   #n_estimators=500, max_depth=10","e3cca34d":"from sklearn.neural_network import MLPRegressor\nMLP_model = MLPRegressor(random_state = 0, hidden_layer_sizes=(50,50))\nMLP_model.fit(X_train, y_train)\n\ny_hat = MLP_model.predict(X_test)\nprint(rmse_score(y_hat, y_test))\nsub_y = MLP_model.predict(sub_X)","554359e9":"sub_csv = pd.DataFrame({'id':data[3000:]['id'], 'revenue': np.exp(sub_y)})\nsub_csv.to_csv('MLP_predict.csv', index=False)\n\n## 2.29088   hidden=(30,500)\n## 2.40497   hidden=(100,)\n## 2.26413   hidden=(200,30)\n## 2.31559   hidden=(50,50)","800d0efe":"#### release_date\n'release_date' column has 'mm\/dd\/yy' data<br>","47310fad":"#### budget\n\nSome movies have '0' budget accidently.<br>\nHowever, this feature absolutely a great predictor for the revenue<br>","cfd198cd":"#### runtime\nThere would be the most favorable runtime.<br><br>\nCutting the runtime in percentile.<br>","a871e3b8":"Most of movies are in 1~4 genres","dc177fb1":"Some columns has string in dictionary format.<br>\nLet's translate string into dictionary","99b25bcb":"There are total 20 different genres for these movies.<br>\nMake dummy variables for each genres","cc2bf93a":"#### original_language & spoken_languages\nThe language is important for targeting the market.<br><br>\nWith original_language, create dummy variables,<br>\nwith spoken_languages, create the number of spoken languages which can be indicator of market expansion","1b82239a":"The movies which are in collection has relatively high revenue than those not in collection","ba5097c1":"'butget' has big scale, so it is also reasonable to take log on 'budget'","ec65f3e1":"#### poster_path\nDrop 'poster_path' column. We won't analize pictures","11cdc158":"Since Movie data has many unique values, <br>\nit is necessary to be transformed in representative values or categorical variables","dbcfa244":"#### status","4a9ca3a5":"## 1. Load Data","445bf87e":"#### Linear Regression","2a4328f0":"#### RandomForest Regressor","51afbb06":"#### imdb_id\nThis is unique id.<br>\nDrop the column","83bf5df9":"#### crew\nPeople tend to have high expectation if the director and writer of a movie are popular or well known.<br>\nHowever, the directors and writers are too sparse in our dataset. <br>\nIt is hard to use it as predictor. We dropped the column.","0d528a55":"## 3. Visualization","fa07d1a9":"## 2. Treat missing values","8945c34b":"#### cast\nTop actors can affact the revenue.<br>\nSelect Top 100 actors and create dummy variables.<br>\nThe number of cast may affect the revenue.","bd75a854":"#### genres","0400b94f":"It looks like there is some relationship between year & revenue<br><br>\nRevenue seems to have some pattern along to the month<br>\nMake dummy variables for month","aafec72e":"#### SVR","e4af7a3a":"It looks like there is some relationship between 'budget' and 'revenue'","24ee3a78":"#### revenue\nSince revenue has big range, it is reasonable to take log on 'revenue'","b0374263":"## 4. Model Building\nModels:\n<ul>\n    <li>Multiple Regression<\/li>\n    <li>SVM - SVR<\/li>\n    <li>Random Forest Regression<\/li>\n    <li>NN - MLPRegressor<\/li>\n    <li>SGD Regressor<\/li>\n<\/ul>","c0317e3c":"#### title\nThere are only 3 missing values on 'title' column.<br>\nHowever, the 'original_title' is available, so we can search the english title on the internet","5b5c7062":"#### release_date\nSince we have only 1 missing value in 'release_date' column, <br>\nWe can search on the internet","84815408":"#### belongs_to_collection\n\nIf it belongs to the collection, put 1 else 0","c50d73b1":"### Step:\n<ol>\n    <li>Load Data<\/li>\n    <li>Treat missing values<\/li>\n    <li>Visualization<\/li>\n    <li>Build Model<\/li>\n<\/ol>","0a6d7d7c":"#### title, tagline, Keywords, overview\nCombine all string information and create clusters for dividing topics.","14fd9aa1":"# Project 2\n\n<b>From Kaggle:<\/b>\n<p> https:\/\/www.kaggle.com\/c\/tmdb-box-office-prediction <\/p>\n\n<b>Goal:<\/b> Predicting Movie revenue from given dataset","c2006871":"'status' column has three different categorical value ('Released', 'Rumored', 'Post Production').<br>\nHowever, only test dataset has 'Post Production' and only few movies are in different status.<br>\nSo, it seems to be not useful. Drop the column","9cf95b06":"#### overview\nFill empty string('') in missing values","48f70eda":"#### runtime\nFill missing value in 'runtime' column with median.<br>\nReplace runtime 0 to median","a091643e":"#### tagline\n\nFill empty string('') in missing values","2c67a00e":"<p>There are missing values in 'homepage', 'tagline', 'overview', 'poster_path', 'release_date', 'runtime', 'status', 'title'<\/p>\n\n#### homepage\nCreate new column 'has_homepage', and put 1 if homepage is available, else 0<br>\nAnd drop 'homepage' column ","df14ad09":"#### production_companies, production_countries\nExpecting more countries and companies participated in filmmaking, more revenue will be collected.<br><br>\nSome companies are garanteed to make high quality movies.<br>\nCreate dummy variables for the top companies.","46c28126":"#### MLP Regressor"}}