{"cell_type":{"09cc940f":"code","0f3bada6":"code","85decf42":"code","44424991":"code","9930dfc2":"code","b3213e0b":"code","4f9e6ade":"code","2a9b47c1":"code","e326726d":"code","ddab4a57":"code","46b3c9a9":"code","c7412b73":"code","7f8d5fc2":"code","56b25ae4":"code","7b7e5952":"code","ba77ff6b":"code","da8f0faf":"code","f16d2ca4":"code","4d4d2e61":"code","5f35d396":"code","29abd61b":"code","e35de3a0":"code","e2f99715":"code","775fb602":"code","7b21087d":"code","62596524":"code","d8071890":"code","2214e2db":"code","f0bc883c":"code","9480bb1e":"code","e917620c":"code","a5dae2c3":"code","35c253d9":"code","8ac01d9d":"code","a606215e":"code","b9aea60f":"code","e478a035":"code","c9079c3e":"code","a4277648":"code","669f5488":"code","2c68d7f5":"code","e7f5d939":"code","6c0fe070":"code","251e6451":"code","36700362":"code","7269fe41":"code","c840ecb7":"code","f26c9990":"markdown","1c5d07aa":"markdown","5df54095":"markdown","93d01f2e":"markdown","635e9c17":"markdown","fc4061d7":"markdown","a430cb3b":"markdown","b4b94b4c":"markdown","5c8c818b":"markdown","c3fe4961":"markdown","33174f98":"markdown","0bdb26cf":"markdown","26bd49db":"markdown","5460819c":"markdown","34f685a6":"markdown","67ab08a6":"markdown","1e74ceab":"markdown"},"source":{"09cc940f":"keyword = \"demonetization\" \nnumber = 10000\nfilename = \"demonetization-tweets_Clusters.csv\"\nfile_count = \"demonetization-tweets\"","0f3bada6":"import pandas as pd\nimport numpy as np\nimport nltk\nimport os\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize,sent_tokenize,TweetTokenizer","85decf42":"# Identify the encoding of the data file\nimport chardet\nwith open('..\/input\/demonetizationtweetscsv\/demonetization-tweets.csv','rb') as f:\n    result = chardet.detect(f.read())  \nresult #Windows-1252","44424991":"# Import the data file\ndf=pd.read_csv('..\/input\/demonetizationtweetscsv\/demonetization-tweets.csv',encoding=result['encoding'])\ndf= df.drop(['Unnamed: 0'],axis=1)\ndf=df[0:number]\ndf=df['text']\ndf=pd.DataFrame({'tweet':df})\n\n#clean the tweets\ndf['cleaned_tweet']= df['tweet'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\ndf['cleaned_tweet']=df['cleaned_tweet'].replace(\"  \",\" \")\n\nwords_remove = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\", \"there\",\"all\",\"we\",\n                \"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\n                \"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\"has\",\"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\n                \"from\",\"com\",\"org\",\"like\",\"likes\",\"so\",\"said\",\"from\",\"what\",\"told\",\"over\",\"more\",\"other\",\n                \"have\",\"last\",\"with\",\"this\",\"that\",\"such\",\"when\",\"been\",\"says\",\"will\",\"also\",\"where\",\"why\",\n                \"would\",\"today\", \"in\", \"on\", \"you\", \"r\", \"d\", \"u\", \"hw\",\"wat\", \"oly\", \"s\", \"b\", \"ht\", \n                \"rt\", \"p\",\"the\",\"th\", \"n\", \"was\"]\n\ndef cleantext(df, words_to_remove=words_remove):\n    # removing emoticons from th tweets, wont help in topic modelling or semantic processing\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'<ed>','', regex = True)\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\B<U+.*>|<U+.*>\\B|<U+.*>','', regex = True)\n    \n    # convert tweets to lowercase\n    df['cleaned_tweet']=df['cleaned_tweet'].str.lower()\n    \n    # remove user mentions\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^(@\\w+)',\"\", regex=True)\n    \n    # remove 'rt' or retweet in the beginning\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^(rt @)',\"\",regex=True)\n    \n    #remove symbols\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[^a-zA-Z0-9]', \" \",regex=True)\n    \n    #remove punctuations\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[[]!\"#$%\\'()\\*+,-.\/:;<=>?^_`{|}]+',\"\", regex = True)\n    \n    #remove_URL(x)\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'https.*$',\"\",regex=True)\n    \n    # remove 'amp' in the text\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'amp',\"\",regex=True)\n    \n    #remove words of length 1 or 2\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\b[a-zA-Z]{1,2}\\b',\"\",regex=True)\n    \n    #remove extra spaces in the tweet\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^\\s+|\\s+$',\" \", regex=True)\n    \n    #remove stopwords and words_to_remove\n    stop_words=set(stopwords.words('english'))\n    mystopwords=[stop_words,'via',words_remove]\n    \n    # removing stopwords\n    df['fully_cleaned_tweet'] = df['cleaned_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in mystopwords]))\n    \n    return df\n\n#get the processed tweets\ndf=cleantext(df)","9930dfc2":"# Sentiment Analysis\nfrom textblob import TextBlob\ndf['sentiment']=df['fully_cleaned_tweet'].apply(lambda x:TextBlob(x).sentiment.polarity) #range -1 to 1\n","b3213e0b":"df.head(2)","4f9e6ade":"df['tokenized_tweet']=df['fully_cleaned_tweet'].apply(word_tokenize)\ndf.head(2)","2a9b47c1":"#if a word has a digit, remove that word\ndf['tokenized_tweet']=df['tokenized_tweet'].apply(lambda x: [y for y in x if not any(c.isdigit() for c in y)])","e326726d":"# Set values for various parameters\nnum_features=100 # Word vector dimensionality\nmin_word_count=1 # minimum word count\nnum_workers=4  # Number of threads to run in parallel\ncontext=10 # context window size","ddab4a57":"# Initilaize and train the model \nfrom gensim.models import word2vec\nprint('Training Model....')\nmodel= word2vec.Word2Vec(df['tokenized_tweet'],workers=num_workers,size=num_features,min_count=min_word_count,\n                        window=context)\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)","46b3c9a9":"vocab=list(model.wv.vocab)\ndef sentence_vector(sentence, model):\n    nwords=0\n    featureV=np.zeros(100, dtype='float32')\n    for word in sentence:\n        if word not in vocab:\n            continue\n        featureV=np.add(featureV, model[word])\n        nwords=nwords+1\n    if nwords>0:\n        featureV=np.divide(featureV,nwords)\n    return featureV\n\ntweet_vector= df['tokenized_tweet'].apply(lambda x: sentence_vector(x,model))\ntweet_vector= tweet_vector.apply(pd.Series)","c7412b73":"# Tweet vector should vary from 0 to 1 (normalise the vector)\n#Tweet vector should vary from 0 to 1 (normalize the vector)\nfor x in range(len(tweet_vector)):\n    x_min = tweet_vector.iloc[x].min()\n    x_max = tweet_vector.iloc[x].max()\n    X  = tweet_vector.iloc[x]\n    i = 0\n    if (x_max - x_min) == 0:\n        for y in X:\n            tweet_vector.iloc[x][i] = (1\/len(tweet_vector.iloc[x]))\n            i = i + 1\n    else:\n        for y in X:\n            tweet_vector.iloc[x][i] = ((y - x_min)\/(x_max - x_min))\n            i = i + 1\n\n","7f8d5fc2":"tweet_vector","56b25ae4":"# Scale the 'sentiment' vector\n# Sentiment varies from -1(Negative Sentiment) to +1(Positive Sentiment) polarity\ndef sentiment(x):\n    if x < 0.04:\n        return 0 #(Neutral sentiment)\n    elif x>0.04:\n        return 1 #(Positive Sentiment)\n    else:\n        return 0.5 #(Negative Sentiment)\n\ntweet_vector[100]=df['sentiment'].apply(lambda x: sentiment(x)) # Adding 100 coumn for sentiment","7b7e5952":"tweet_vector","ba77ff6b":"#Updating the 'sentiment' column in df also\ndf['sentiment'] = tweet_vector[100]\ndf.head(3)","da8f0faf":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score,silhouette_samples\n\nrange_n_clusters=[4,6,7,8,9,10,11,12,14]\nX= tweet_vector\nn_best_clusters=0\nsilhouette_best = 0\nfor n_clusters in range_n_clusters:\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.  \n    clusterer=KMeans(n_clusters=n_clusters,random_state=42)\n    cluster_labels=clusterer.fit_predict(X)\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg=silhouette_score(X,cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    \n    if silhouette_avg > silhouette_best:\n        silhouette_best = silhouette_avg\n        n_best_clusters = n_clusters","f16d2ca4":"print(n_best_clusters)\nprint(silhouette_best)","4d4d2e61":"clusterer=KMeans(n_clusters=n_best_clusters,random_state=42)\ncluster_labels=clusterer.fit_predict(X)","5f35d396":"np.unique(cluster_labels)","29abd61b":"#Array of tweets, the corresponding cluster number, sentiment\nfinaldf = pd.DataFrame({'cl_num': cluster_labels,'fully_cleaned_tweet': df['fully_cleaned_tweet'], 'cleaned_tweet': df['cleaned_tweet'], 'tweet': df['tweet'],'sentiment': df['sentiment']})\nfinaldf = finaldf.sort_values(by=['cl_num'])","e35de3a0":"df['cl_num']=cluster_labels\ndf.head(3)","e2f99715":"dfOrdered = pd.DataFrame(df)\n\n#Compute how many times a tweet has been 'retweeted' - that is, how many rows in dfOrdered are identical\ndfOrdered['tokenized_tweet'] = dfOrdered['tokenized_tweet'].apply(tuple)\ndfUnique = dfOrdered.groupby(['tweet', 'cleaned_tweet', 'fully_cleaned_tweet', 'sentiment','tokenized_tweet', 'cl_num']).size().reset_index(name=\"freq\")\ndfUnique = dfUnique.sort_values(by=['cl_num'])","775fb602":"dfUnique['tokenized_tweet'] = dfUnique['tokenized_tweet'].apply(list)\ndfOrdered['tokenized_tweet'] = dfOrdered['tokenized_tweet'].apply(list)","7b21087d":"dfUnique","62596524":"# Compute the silhouette scores for each sample\nsample_silhouette_values = silhouette_samples(X, cluster_labels)\n\npoor_cluster_indices = []\navg_cluster_sil_score = []\n\nfor i in range(n_best_clusters):\n# Aggregate the silhouette scores for samples belonging to\n# cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n        avgscore = (np.mean(ith_cluster_silhouette_values))   #average silhouette score for each cluster\n        avg_cluster_sil_score = np.append(avg_cluster_sil_score, avgscore)\n        print('Cluster',i, ':', avgscore)\n        if avgscore < 0.2:\n            poor_cluster_indices = np.append(poor_cluster_indices, i)\n            \n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]","d8071890":"poor_cluster_indices","2214e2db":"#remove those rows where cluster value match poor_cluster_indices \navg_cluster_sil_score_final = []\ncluster_name = np.unique(dfOrdered['cl_num'])\n\nif (len(poor_cluster_indices)!=0):\n    n_final_clusters = n_best_clusters - len(poor_cluster_indices)\n    for i in poor_cluster_indices:\n        dfUnique = dfUnique[dfUnique['cl_num'] != i]\n    for j in cluster_name:\n        if j not in poor_cluster_indices:    \n            avg_cluster_sil_score_final = np.append(avg_cluster_sil_score_final, avg_cluster_sil_score[j])\n            \n    cluster_name = np.unique(dfUnique['cl_num'])\n    ","f0bc883c":"dfUnique['cl_num']=abs(dfUnique['cl_num'])\ndfUnique=dfUnique.sort_values(by=['cl_num'])","9480bb1e":"tweets_to_consider = 'fully_cleaned_tweet'","e917620c":"final_clusters= np.unique(dfUnique['cl_num'])\nprint(final_clusters)","a5dae2c3":"# Store all tweets corrsponding to each cluster in a file\nfor i in final_clusters:\n    with open('.\/tweets_Cluster_'+str(i)+'.txt','w') as out:\n        y=''\n        for x in dfUnique[tweets_to_consider][dfUnique.cl_num==i]:\n            y=y+x+'. '\n        out.write(y)\n        out.close","35c253d9":"#A combination of (Noun, adjective, cardinal number, foreign word and Verb) are being extracted now\n#Extract chunks matching pattern. Patterns are:\n#1) Noun phrase (2 or more nouns occurring together. Ex United states of America, Abdul Kalam etc)\n#2) Number followed by Noun (Ex: 28 Terrorists, 45th President)\n#3) Adjective followed by Noun (Ex: Economic impact, beautiful inauguration)\n#4) Foreign word (Ex: Jallikattu, Narendra modi, Pappu)\n#5) Noun followed by Verb (Ex: Terrorists arrested)\n#And a combination of all 5\n        \nimport re\nimport nltk\n\nphrases = pd.DataFrame({'extracted_phrases': [], 'cluster_num': []})\n\n\nA = '(CD|JJ)\/\\w+\\s'  #cd or jj\nB = '(NN|NNS|NNP|NNPS)\/\\w+\\s'  #nouns\nC = '(VB|VBD|VBG|VBN|VBP|VBZ)\/\\w+\\s' #verbs\nD = 'FW\/\\w+\\s'  #foreign word\npatterns = ['('+A+B+')+', '('+D+B+')+','('+D+')+', '('+B+')+', '('+D+A+B+')+', \n           '('+B+C+')+', '('+D+B+C+')+', '('+B+A+B+')+', '('+B+B+C+')+'] \n\n\ndef extract_phrases(tag1, tag2, sentences):\n    extract_phrase = []\n    for sentence in sentences:\n        phrase = []\n        next_word = 0\n        for word, pos in nltk.pos_tag(nltk.word_tokenize(sentence)):\n            if next_word == 1:\n                next_word = 0\n                if pos == tag2:\n                    extract_phrase = np.append(extract_phrase,phrase + ' ' + word) \n            \n            if pos == tag1:\n                next_word = 1\n                phrase = word\n    return extract_phrase\n\nfor i in cluster_name:\n    File = open('.\/tweets_Cluster_'+str(i)+'.txt', 'r') #open file\n    lines = File.read() #read all lines\n    sentences = nltk.sent_tokenize(lines) #tokenize sentences\n\n    for sentence in sentences: \n        f = nltk.pos_tag(nltk.word_tokenize(sentence))\n        tag_seq = []\n        for word, pos in f:\n            tag_seq.append(pos+'\/'+ word)\n        X = \" \".join(tag_seq)\n\n        phrase = []\n        for j in range(len(patterns)):\n            if re.search(patterns[j], X):\n                phrase.append(' '.join([word.split('\/')[1] for word in re.search(patterns[j], X).group(0).split()]))\n    \n        k = pd.DataFrame({'extracted_phrases': np.unique(phrase), 'cluster_num': int(i)})\n    \n        phrases = pd.concat([phrases,k], ignore_index = True)\n\nprint(phrases)","8ac01d9d":"#For each phrase identified replace all the substrings by the largest phrase \n#Ex: lakh looted,40 lakh looted and Rs 40 lakh looted, replace all by single largest phrase - Rs 40 lakh looted \n#i.e. instead of 3 different phrases, there will be only one large phrase\n\nphrases_final = pd.DataFrame({'extracted_phrases': [], 'cluster_num': []})\nfor i in cluster_name:\n    phrases_for_each_cluster = []\n    cluster_phrases = phrases['extracted_phrases'][phrases.cluster_num == i]\n    cluster_phrases = np.unique(np.array(cluster_phrases))\n    for j in range(len(cluster_phrases)):\n        \n        phrase = cluster_phrases[j]\n        updated_cluster_phrases = np.delete((cluster_phrases), j)\n        if any(phrase in phr for phr in updated_cluster_phrases): \n            'y'\n        else: \n            #considering phrases of length greater than 1 only\n            if (len(phrase.split(' '))) > 1:\n                phrases_for_each_cluster.append(phrase)\n    k = pd.DataFrame({'extracted_phrases': phrases_for_each_cluster, 'cluster_num': int(i) })\n    \n    phrases_final = pd.concat([phrases_final,k], ignore_index = True)","a606215e":"phrases_final","b9aea60f":"#Term-frequency : For each cluster, calculate the number of times a given phrase occur in the tweets of that cluster\n\nphrases_final['term_freq'] = len(phrases_final)*[0]\n\nfor i in cluster_name:\n    for phrase in phrases_final['extracted_phrases'][phrases_final.cluster_num == i]:\n        tweets = dfUnique[tweets_to_consider][dfUnique.cl_num == i]\n        for tweet in tweets:\n            if phrase in tweet:\n                phrases_final['term_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] = phrases_final['term_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] + 1","e478a035":"phrases_final","c9079c3e":"#Document-frequency\nphrases_final['doc_freq'] = len(phrases_final)*[0]\n\n\n# for each phrase, compute the number of clusters that Sphrase occurs in\nfor phrase in phrases_final['extracted_phrases']:\n    for i in cluster_name:\n        all_tweets = ''\n        for tweet in dfUnique[tweets_to_consider][dfUnique.cl_num == i]:\n            all_tweets = all_tweets + tweet + '. ' \n        if phrase in all_tweets:\n            phrases_final['doc_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] = phrases_final['doc_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] + 1\n        ","a4277648":"import math\nphrases_final['doc_freq'] = phrases_final['doc_freq'].apply(lambda x: math.log10(n_best_clusters\/(x)) )","669f5488":"phrases_final['tf-idf'] = phrases_final['term_freq']*phrases_final['doc_freq']","2c68d7f5":"phrases_final","e7f5d939":"phrases_final['diff_tf-idf'] = len(phrases_final)*[0]\n\nnarrative = pd.DataFrame({'cl_num': [], 'abstraction': []})\nfor i in cluster_name: \n    # arrange in descending order of tf-idf score\n    phrases_final = phrases_final.sort_values(['cluster_num','tf-idf'], ascending=[1,0])\n    \n    #Break this distribution at a point where the difference between any consecutive phrases is maximum\n    #difference between consecutive values of tf-idf \n    phrases_final['diff_tf-idf'][phrases_final.cluster_num == i] = abs(phrases_final['tf-idf'][phrases_final.cluster_num == i] - phrases_final['tf-idf'][phrases_final.cluster_num == i].shift(1))\n\n    #The last value for each cluster will be 'NaN'. Replacing it with '0'. \n    phrases_final = phrases_final.fillna(0)\n    \n    phrases_final = phrases_final.reset_index(drop = True) #to avoid old index being added as a new column\n    if len(phrases_final[phrases_final.cluster_num == i]) != 0:\n        \n        #index corresponding to the highest difference\n \n        ind = (phrases_final['diff_tf-idf'][phrases_final.cluster_num == i]).idxmax()\n        \n        abstract = phrases_final['extracted_phrases'][:ind+1][phrases_final.cluster_num == i]\n    \n    \n        #store the abstraction corresponding to each cluster\n        k = pd.DataFrame({'cl_num': int(i), 'abstraction': abstract})\n        narrative = pd.concat([narrative,k], ignore_index = True)","6c0fe070":"dfUnique","251e6451":"#Assigning polarity based on the sentiment for each tweet 2=negative, 1=positive, 3=neutral\ndfUnique['polarity'] = np.NaN\ndfUnique['polarity'][dfUnique.sentiment == 0.5] = \"3\"\ndfUnique['polarity'][dfUnique.sentiment == 1] = \"1\"\ndfUnique['polarity'][dfUnique.sentiment == 0] = \"2\"","36700362":"from collections import Counter\n\n#find the highest occurring sentiment corresponding to each tweet\ndef find_mode(a):\n    b = Counter(a).most_common(3)\n    mode = []; c_max = 0\n    for a,c in b:\n        if c>c_max:\n            c_max = c\n        if c_max == c:\n            mode.append(a)  \n    print(mode)\n    mode.sort()\n    print(mode)\n    \n    ## if mode is 3&2 i.e. neutral and negative, assign the overall sentiment for that phrase as negative, \n    ## if mode is 3&1 i.e. neutral and positive, assign the overall sentiment for that phrase as positive,\n    ## if mode is 2&1 i.e. negative and positive, assign the overall sentiment for that phrase as neutal, \n    ## if mode is 3&2&1 i.e. negative, positive and neutral, assign the overall sentiment for that phrase as neutral\n    \n    if len(mode) == 1:\n        return mode[0]\n    \n    elif (len(mode) == 2) & (mode[1]=='3'):\n        return mode[0]\n    else:\n        return 3\n    \n#1=>+ve 2=>-ve 3=>Neutral\nnarrative['expression'] = -1\ndfUnique = dfUnique.reset_index(drop = True)\nfor i in cluster_name:\n    tweets = dfUnique[tweets_to_consider][dfUnique.cl_num == i]\n    abstracts = narrative['abstraction'][narrative.cl_num == i] \n    for abst in abstracts:\n        sent = []\n        for tweet, polarity in zip(dfUnique[tweets_to_consider][dfUnique.cl_num == i], dfUnique['polarity'][dfUnique.cl_num == i]):\n            if abst in tweet:\n                sent = np.append(sent, polarity)\n        \n        \n        if len(sent)!=0:\n            ## if mode is 3&2-2, 3&1-1, 2&1-3, 3&2&1 - 3\n            senti = find_mode(sent)\n            if senti == '2':\n                sent_value = \"Negative\"\n            elif senti == '1':\n                sent_value = \"Positive\"\n            else:\n                sent_value = \"Neutral\"\n            narrative['expression'][(narrative.abstraction == abst) & (narrative.cl_num == i)] = sent_value\n        ","7269fe41":"#sudo pip install xlwt\n#sudo pip3 install openpyxl\nfrom pandas import ExcelWriter\n\n#Save the narratives in an excel file \n\nwriter = pd.ExcelWriter('narrative.xlsx')\nfor i in cluster_name:\n    df1 = pd.DataFrame(dfUnique[['tweet','freq']][dfUnique.cl_num == i]).sort_values(['freq'], ascending = [0])\n    df1 = pd.DataFrame({'tweet': dfUnique['tweet'][dfUnique.cl_num == i], 'freq': dfUnique['freq'][dfUnique.cl_num == i]}) \n    df1 = df1.sort_values(['freq'], ascending = [0]) \n\n    df2 = pd.DataFrame({ 'abstraction': narrative['abstraction'][narrative.cl_num == i], 'expression': narrative['expression'][narrative.cl_num == i]})\n    df3 = pd.DataFrame({'abstraction': (len(df1)-len(df2))*['-'], 'expression': (len(df1)-len(df2))*['-']})\n    df2 = df2.append(df3)\n\n    df1 = df1.reset_index(drop=True)\n    df2 = df2.reset_index(drop=True)\n    df1['abstraction'] = df2['abstraction']\n    df1['expression'] = df2['expression']\n\n    df1.to_excel(writer,'narrative_cluster'+str(i))\n\nwriter.save()\n    ","c840ecb7":"narrative","f26c9990":"**Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1].**","1c5d07aa":"# Step 6: Calculate abstraction and expression for each narrative \nNote that each cluster represents a narrative","5df54095":"# Step 2: Clean the tweets\n","93d01f2e":"# Step 5: Cluster the narratives [= opinions + expressions]","635e9c17":"### For each phrase in each cluster, calculate tf-idf","fc4061d7":"# Step 1: Import data","a430cb3b":"### For each phrase in each cluster, calculate term frequency ","b4b94b4c":"# Step 4: Add sentiment to the tweet vector","5c8c818b":"### For each cluster find top few phrases and respective sentiment\n ","c3fe4961":"# Step 3: Vectorize the tweets","33174f98":"### Keeping the largest phrase","0bdb26cf":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#ff6666' role=\"tab\" aria-controls=\"home\"><center>Thank You \ud83d\ude4f <\/center><\/h1>\n","26bd49db":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#ff6666' role=\"tab\" aria-controls=\"home\"><center>Narrative Identification on Demonetization Tweets<\/center><\/h1>","5460819c":"### Find vector corresponding to each tweet\nTake the average of all word vectors in a tweet","34f685a6":"### Assign the sentiment to each extracted phrases\ncount the number of tweets, a phrase has occurred in positive, negative and neutral context. Assign the most occurred sentiment to the phrase","67ab08a6":"\n\n# Save the narratives in excel file\n With each sheet in the file representing 1 narrative ( == 1 cluster)","1e74ceab":"### Discard the clusters with poor Silhouette score"}}