{"cell_type":{"9a1497ce":"code","1a514ec7":"code","ec400147":"code","75adf3b8":"code","bd84b114":"code","676e5d47":"code","e44e4ff9":"code","c5f24d70":"code","5f253b7e":"code","b6d98ed7":"code","abce038d":"code","e4c89255":"code","d25fd341":"code","3e9d92f9":"code","d77c2892":"code","03e813c6":"code","2347a42c":"code","117182e8":"code","761d299f":"code","867b7ce0":"code","9f80556e":"code","fc24546e":"code","0fa6693a":"code","3096f8de":"code","6c06602a":"code","0f866185":"code","86d75d6d":"code","75cf3d09":"code","31c21cb1":"code","84bca181":"code","c7a831a0":"code","324b79ec":"code","7cfe24f3":"code","c9ed096a":"code","07809a22":"code","fba56aff":"code","3fcffcf2":"code","2218ddc8":"code","097b9fac":"code","c39a5b20":"code","41690e53":"code","590a6a45":"code","01d63e63":"code","5901ccb9":"code","0d0d0baf":"code","592fcd89":"code","76f6bc4f":"code","0c1879fd":"code","854f302e":"code","ff5edb0d":"code","9fbe27d0":"code","fa5ec774":"code","3a82b34f":"code","ed8a5c7a":"code","1b421ade":"code","13facaaa":"code","281d9056":"markdown","f47bc8d5":"markdown","051bcc39":"markdown","ba208b8e":"markdown","941a3346":"markdown","3548ae51":"markdown","70e252e7":"markdown","7189610f":"markdown","8c91ad5a":"markdown","b80e66cf":"markdown","4b358fb1":"markdown","41044f5e":"markdown","2badba95":"markdown","34a3a058":"markdown","2d61a6c6":"markdown","297708e1":"markdown","e28e5b06":"markdown","bc3fe603":"markdown","5a612424":"markdown","48486727":"markdown","34b07ab7":"markdown","2782de5d":"markdown","e097c462":"markdown","8db9add6":"markdown","984e62f8":"markdown","ea26b4bf":"markdown","6d8c9060":"markdown","5b11dc75":"markdown","a6222f6a":"markdown","09e402bf":"markdown"},"source":{"9a1497ce":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport math\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# preprocessing\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve, ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix, explained_variance_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2\n\n# models\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier, LassoCV\nfrom sklearn.svm import SVC, LinearSVC, SVR\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1a514ec7":"#!pip install autoviz\n#from autoviz.AutoViz_Class import AutoViz_Class","ec400147":"cv_n_split = 3\nrandom_state = 40\ntest_train_split_part = 0.2\nmetrics_all = {1 : 'r2_score', 2: 'acc', 3 : 'rmse', 4 : 're'}\nmetrics_now = [1, 2, 3, 4] # you can only select some numbers of metrics from metrics_all\ndata = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndata.head(5)","75adf3b8":"data.describe([.05, .95])","bd84b114":"data.describe()","676e5d47":"data.info()","e44e4ff9":"#pp.ProfileReport(data)","c5f24d70":"data = data.drop_duplicates()\ndata.shape","5f253b7e":"data.describe()","b6d98ed7":"data","abce038d":"data.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']","e4c89255":"pd.set_option('max_columns', len(data.columns)+1)\nlen(data.columns)","d25fd341":"data['sex'][data['sex'] == 0] = 'female'\ndata['sex'][data['sex'] == 1] = 'male'\n\ndata['chest_pain_type'][data['chest_pain_type'] == 1] = 'typical angina'\ndata['chest_pain_type'][data['chest_pain_type'] == 2] = 'atypical angina'\ndata['chest_pain_type'][data['chest_pain_type'] == 3] = 'non-anginal pain'\ndata['chest_pain_type'][data['chest_pain_type'] == 4] = 'asymptomatic'\n\ndata['fasting_blood_sugar'][data['fasting_blood_sugar'] == 0] = 'lower than 120mg\/ml'\ndata['fasting_blood_sugar'][data['fasting_blood_sugar'] == 1] = 'greater than 120mg\/ml'\n\ndata['rest_ecg'][data['rest_ecg'] == 0] = 'normal'\ndata['rest_ecg'][data['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndata['rest_ecg'][data['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\ndata['exercise_induced_angina'][data['exercise_induced_angina'] == 0] = 'no'\ndata['exercise_induced_angina'][data['exercise_induced_angina'] == 1] = 'yes'\n\ndata['st_slope'][data['st_slope'] == 1] = 'upsloping'\ndata['st_slope'][data['st_slope'] == 2] = 'flat'\ndata['st_slope'][data['st_slope'] == 3] = 'downsloping'\n\ndata['thalassemia'][data['thalassemia'] == 1] = 'normal'\ndata['thalassemia'][data['thalassemia'] == 2] = 'fixed defect'\ndata['thalassemia'][data['thalassemia'] == 3] = 'reversable defect'","3e9d92f9":"data.head(3)","d77c2892":"data['sex'] = data['sex'].astype('object')\ndata['chest_pain_type'] = data['chest_pain_type'].astype('object')\ndata['fasting_blood_sugar'] = data['fasting_blood_sugar'].astype('object')\ndata['rest_ecg'] = data['rest_ecg'].astype('object')\ndata['exercise_induced_angina'] = data['exercise_induced_angina'].astype('object')\ndata['st_slope'] = data['st_slope'].astype('object')\ndata['thalassemia'] = data['thalassemia'].astype('object')","03e813c6":"data.shape","2347a42c":"data = pd.get_dummies(data, drop_first=True)","117182e8":"data.head()","761d299f":"X_train, X_test, y_train, y_test = train_test_split(data.drop('target', 1), data['target'], test_size = .2, random_state=10) #split the data","867b7ce0":"model = RandomForestClassifier(max_depth=5)\nmodel.fit(X_train, y_train)","9f80556e":"estimator = model.estimators_[1]\nfeature_names = [i for i in X_train.columns]\n\ny_train_str = y_train.astype('str')\ny_train_str[y_train_str == '0'] = 'no disease'\ny_train_str[y_train_str == '1'] = 'disease'\ny_train_str = y_train_str.values","fc24546e":"#code from https:\/\/towardsdatascience.com\/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz #plot tree\n\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = feature_names,\n                class_names = y_train_str,\n                rounded = True, proportion = True, \n                label='root',\n                precision = 2, filled = True)\n\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\nfrom IPython.display import Image\nImage(filename = 'tree.png')","0fa6693a":"y_predict = model.predict(X_test)\ny_pred_quant = model.predict_proba(X_test)[:, 1]\ny_pred_bin = model.predict(X_test)\n# checking with the confusion matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred_bin)\nconfusion_matrix","3096f8de":"total=sum(sum(confusion_matrix))\n\nsensitivity = confusion_matrix[0,0]\/(confusion_matrix[0,0]+confusion_matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix[1,1]\/(confusion_matrix[1,1]+confusion_matrix[0,1])\nprint('Specificity : ', specificity)","6c06602a":"from sklearn.metrics import roc_curve, auc #for model evaluation\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for heart disease Random Forest Classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","0f866185":"auc(fpr, tpr)","86d75d6d":"import eli5 #for permutation importance\nfrom eli5.sklearn import PermutationImportance\nimport shap #for SHAP values\nperm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","75cf3d09":"from pdpbox import pdp, info_plots #for partial plots\nnp.random.seed(123) #ensure reproducibility\n\nbase_features = data.columns.values.tolist()\nbase_features.remove('target')\n\nfeat_name = 'num_major_vessels'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","31c21cb1":"feat_name = 'age'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","84bca181":"feat_name = 'st_depression'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","c7a831a0":"inter1  =  pdp.pdp_interact(model=model, dataset=X_test, model_features=base_features, features=['st_slope_upsloping', 'st_depression'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['st_slope_upsloping', 'st_depression'], plot_type='contour')\nplt.show()\n\ninter1  =  pdp.pdp_interact(model=model, dataset=X_test, model_features=base_features, features=['st_slope_flat', 'st_depression'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['st_slope_flat', 'st_depression'], plot_type='contour')\nplt.show()\n","324b79ec":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")","7cfe24f3":"shap.summary_plot(shap_values[1], X_test)","c9ed096a":"def heart_disease_risk_factors(model, patient):\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(patient)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], patient)","07809a22":"data_for_prediction = X_test.iloc[3,:].astype(float)\nheart_disease_risk_factors(model, data_for_prediction)","fba56aff":"print(X_train.iloc[:50].shape)","3fcffcf2":"shap_values = explainer.shap_values(X_train.iloc[:50])\nshap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[:50])","2218ddc8":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport warnings","097b9fac":"X = data.drop(['target'], axis = 1)\ny = data.target.values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","c39a5b20":"print(X_test.shape)","41690e53":"classifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(11, kernel_initializer = 'uniform', activation = 'relu', input_dim =19))\n# Adding the second hidden layer\nclassifier.add(Dense(11, kernel_initializer = 'uniform', activation = 'relu'))\n# Adding the output layer\nclassifier.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","590a6a45":"classifier.fit(X_train, y_train, batch_size = 10, epochs=100)","01d63e63":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)","5901ccb9":"ac1 = accuracy_score(y_test, y_pred.round())\nprint('accuracy of the model: ',ac1)","0d0d0baf":"#print(y_test)\n#print(y_pred.round())","592fcd89":"from sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(y_test, y_pred.round())\nsns.heatmap(cm1,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False)","76f6bc4f":"\nsensitivity = cm1[0,0]\/(cm1[0,0]+cm1[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = cm1[1,1]\/(cm1[1,1]+cm1[0,1])\nprint('Specificity : ', specificity)","0c1879fd":"#import scikitplot as skplt\n#import matplotlib.pyplot as plt\n\n\n#skplt.metrics.plot_roc_curve(y_test, y_pred.round())\n#plt.show()","854f302e":"auc(fpr, tpr)","ff5edb0d":"import shap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\npmodel = make_pipeline(\n    StandardScaler(),\n    MLPRegressor(hidden_layer_sizes=(5,),activation='logistic', max_iter=10000,learning_rate='invscaling',random_state=0)\n)\nmodel.fit(X_train,y_train)\n\nclassifier = shap.KernelExplainer(model.predict,X_train)\nshap_values = classifier.shap_values(X_test,nsamples=100)","9fbe27d0":"def heart_disease_risk_factors2(model, patient):\n\n    classifier = shap.KernelExplainer(model.predict,X_train)\n    shap_values = classifier.shap_values(patient)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], patient)\n\n","fa5ec774":"print(feature_names)","3a82b34f":"shap.summary_plot(shap_values,X_test, feature_names)","ed8a5c7a":"print(X_train.shape)","1b421ade":"shap_values = classifier.shap_values(X_train[3])\nshap.force_plot(classifier.expected_value, shap_values, X_test[3], feature_names)","13facaaa":"shap_values = classifier.shap_values(X_train[:50])\nshap.force_plot(classifier.expected_value, shap_values, X_test[:50], feature_names)","281d9056":"# First ML: Random Forest Classifier","f47bc8d5":"The area under the curve (auc) is nearly 90%. This shows the predictions will be highly accurate.","051bcc39":"I am interested in how to results found by the decision-tree fare.","ba208b8e":"# Explaining the Decision Tree\n\nThe first thing I noticed was the importance of the most prevailing factor. This led to the finding that Thalassaemia has a huge bearing. Using the 'permutation importance' model, let's explore further:\n","941a3346":"Their likelihood of getting heard disease is 77% whilst the base value is 56%. This main factor is because they have non-reversible thalassaemia.","3548ae51":"**Age** plays a factor for heart-disease, peaking at 57 years.\n\nMen are more than twice as likely to get heart disease than women.\n\n**Cp** - chest pain, 0 to 3\n0 - Typical Angina MOST\n1 - Atypical Angina\n2 - Non-agina pain 2ND MOST\n3 - Asymptomatic LEAST\n\n**Trstbps**, is the resting blood pressure upon admission. Normal values are 120\/80. In the dataset you can see that they are normal to high in general because this is a heart disease dataset.\n\nThe cholesterol is below 155mg\/dL. On this dataset, the cholesterol is on that level but there is an aggregation towards higher cholesterol going up to 400mg\/dL, even well above 500 mg\/dL.\n\n**fbs** - The fasting blood sugar is of two values, high and low. A high value is more than 120mg\/dL and anything below it is low. Most to the patients have low fasting blood sugar.\n\n**restecg** - the resting ECG results are measured as:\n0 - normal\n1 - ST-T wave abnormality\n2 - left ventricle hypertrophy (I.e. difficult for the left ventricle to push blood, thereby increasing the blood pressure)\nThere is just as much ST-T abnormality as there are normal ECG readings.\n\n**thalach** - a person\u2019s maximal heart-rate. This is your age take away 220. For example, a 50 year old\u2019s maximal heart rate is 170 bpm.\n\n**exang** - is the exercise induced angina. From this dataset we can see that this happens half as much as no exercise induced angina.\n\n**oldpeak** - low ST caused by exercise versus rest\n\n**slope** - slope of peak exercise\n1 - unsloping\n2 - flat\n3 - downsloping\n\n**ca** - the number of major vessels\n\n**thal** - thalassaemia blood disorder\n3 - normal\n6 - fixed defect\n7 - reversible defect\n\n**target** - heart disease\n0 - no\n1 - yes, heart disease of which there is a greater number than the normal.\n\nThere are a lot of positive correlations here. The ones that I am going to focus on for my data exploration are the ones that are deepest blue (positive correlation) with respect to heart disease, ie, target. These are:\n\nTarget and slope, ie, downsloping and heart disease are strongly correlated\nTarget and thalach, ie, the higher the maximal heart-rate, the higher likelihood of heart disease.\nThe higher the resting ECG, the higher likelihood how increased heart disease.\nThe higher the value of chest, the higher the likelihood of heart disases- this is interesting because, higher cp is asymptomatic, and non-angina pain is second highest.\n\nOther highly correlated areas on the heat map are:\nAge and the number of vessels\nSex and thalassaemia\nChest pain and thalac\nThe resting bp and age\nThe resting bp and slope\n\n\nThe resting heart rate tends to increase with age, as does cholesterol and low ST caused by exercise vs rest.\n\n","70e252e7":"Now combining all the patient information, and using this helpful tool to predict heart disease, I have the following interactive graph. As you hover above, you will get information on person individually. For ease, I have used patients up to 50.","7189610f":"[https:\/\/www.youtube.com\/watch?v=r8Kwmutod34](http:\/\/)\n0:13 - 1:13\n\nST depression is an indication of heart disease.","8c91ad5a":"The sensitivity and specificity using ANN is higher than Random Forest Classifier. This shows that ANN is providing greater predictive accuracy.","b80e66cf":"Onto my favourite approach: Pandas Profiling","4b358fb1":"Improving on the outputs to put results for 'sex' 'chest_pain_type' etc as categories","41044f5e":"Feature values, when they are 'red', there is more of them present. When they are 'blue', there is less of them present. The SHAP value indicates the impact the features have on disease. So here, a low number of blood vessels is indicative of heart disease. A high presence of thalassaemia defect is indicative of heart disease. A lot ST depression is indicative of heart disease and so forth.","2badba95":"Having a look at individual patients and their outcomes will be interesting. Let's look at the third patient on the list:","34a3a058":"This graph shows that as the number of blood vessels increases, heart disease decreases- which would make sense, considering blood can flow easily and we have nothing to worry about when blood is flowing.","2d61a6c6":"# Conclusion\n\nIn concluding, I have worked through the patient information, seeing what factors have caused heart disease. I did this using the Random Forest Model and ANN. This is helpful in therapeutics. For example, if a male patient came in complaining of chest-pain, then using the Random Forest Classifier Shap, on patient 37 (for example), their likelihood of getting heart disease is about 95%. The leading cause of this is his reversible thalassaemia defect. A physician can now explore the option of finding therapies to reverse the effects of thalassaemia defect. \n\nFrom a data analysis perspective, ANN produces results of over 90%. Going forward, for further analyses on the heart, this would be the predictive model of choice. It is interesting to note that ANN is an unsupervised machine learning model and it had such excellent predictive power.\n","297708e1":"This is a helpful way of exploring the gini, whereby, a high score indicates greater inequality. I like this method of looking at the data because you can visually and immediately see what factors can cause disease and by how much. The blue boxes are relating to disease and the red ones to no disease.","e28e5b06":"Using the Receiver Operator Curve:","bc3fe603":"Making the labels more descriptive:","5a612424":"As you can see, the dataset is 'clean'. There are no missing values, and the values are in numerical format. This means there is no data pre-processing.","48486727":"# Next, download the datasets","34b07ab7":"# The second ML: Artificial Neural Networks\n\n","2782de5d":"The sensitivity and specificity are 84% and 69% respectively. It will be interesting to compare these with those generated by the ANN.","e097c462":"Since there was one duplicate, we can remove this:","8db9add6":"# First task: Import Libraries","984e62f8":"# Using machine learning to predict heart disease and potentially inform therapies\n\n\nAim: Using the training data to create a model for predicting heart disease. \n\nHypothesis: Heart disease increases with factors such as cholesterol, genetic predisposition, gender and age. \n\nMethod: Firstly, preparing the data through exploring it using Pandas, cleaning it, and feature engineering to make the predictions visually comprehensive.\n\nSecondly, using two models of machine learning, to compare predictive outcomes. Using methods to ascertain predictive accuracies in each, using the confusion matrix, specificity & sensitivity, and area under the curve.\n\nThirdly analysing results gathered and forming conclusions. \n\nTechniques used: The first ML is, supervised learning through Random Forest Classifier. The benefits of using this algorithm:\n- We can avoid 'overfitting'. This can be problematic for unseen data.\n- It uses 'bagging' and 'boosting' to avoid overfitting hence making predictions better thereby improving accuracy\n\nThe second is unsupervised learning through Artificial Neural Networks (ANN). The benefit of using this algorithm is:\n- High predictive accuracy\n- Self-extracts features\n\n","ea26b4bf":"As we found out earlier in our permutation importance model, age, did not really have an input on heart disease, so this was expected. I must admit, I was expecting age to be a factor for heart disease but this is not the case.","6d8c9060":"# Exploratory Data Analysis and Feature Engineering","5b11dc75":"Importing the libraries and packages needed","a6222f6a":"It is fascinating that with our initial observations, we did notice that thalassaemia was a factor in heart disease, but so was number of vessels, gender and age.","09e402bf":"Sensitivity and specificity help in diagnostic testing. This is because sensitivity is the measure of a test's ability to accurately designate disease as positive. Specificity, is the opposite, to ability for the test to designate the non-disease as negative. The calculation for Sensitivity = True Positives \/ (True Positives + False Negatives) and for Specificity = True Negatives (True Negatives + False Positives)\n\nHere is what I got:"}}