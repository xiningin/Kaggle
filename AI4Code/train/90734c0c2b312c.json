{"cell_type":{"e582cb79":"code","9bdcc1e0":"code","d36cbff9":"code","d5b0e252":"code","2a02a349":"code","fa1df294":"code","a1b38a50":"code","912342ac":"code","df61b66e":"code","239fc702":"code","29e6440b":"code","d82b6f0d":"code","64b03012":"code","820d08de":"code","0d88af59":"code","2592243b":"code","a696f25a":"code","19285f40":"code","331fa2eb":"code","d2648a91":"code","c8458753":"code","c4380588":"code","acc91c72":"code","7ca1f81a":"code","5244c2e4":"code","8b7223bf":"code","a6db3808":"code","d05f0721":"code","6df52317":"code","9f47ff02":"code","01b08eba":"code","141cc214":"code","0242ad58":"code","6ade2dc1":"code","0a3f0a96":"code","de84f57c":"code","9e032883":"code","2bf2cf0a":"code","c4b0f8e4":"code","1cfcb18d":"code","2291a993":"code","a4c0c4e9":"code","7877710b":"code","fabd3041":"code","5039928e":"code","851832fa":"code","59bae7bf":"code","a411898b":"code","84537f6b":"markdown","02425c0a":"markdown","143e87b4":"markdown","ace75cc2":"markdown","8b2df372":"markdown","7a01407a":"markdown","f3d21b30":"markdown","11563a97":"markdown","66b2f9bb":"markdown","67603665":"markdown","9a39c053":"markdown","ffacea37":"markdown","50412bbd":"markdown","10a447eb":"markdown","e2e87398":"markdown","d992a6d5":"markdown"},"source":{"e582cb79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # To add Data Visulaizations \nimport seaborn as sns \nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\",None)\npd.set_option(\"display.max_rows\",None)\n\n#ML Libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier ,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve ,KFold\nfrom sklearn.metrics import roc_curve,accuracy_score,f1_score,auc,confusion_matrix,roc_auc_score,plot_confusion_matrix\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9bdcc1e0":"training = pd.read_csv ('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv ('\/kaggle\/input\/titanic\/train.csv')\n\ntraining['train_test'] = 1 #Model 1 for Yes\ntest['train_test'] = 0 # Model for non\ntest['Survived'] = np.NaN\nall_data = pd.concat([training,test])\n\n%matplotlib inline\nall_data.columns","d36cbff9":"# Understand the data\n# The Histograms and Boxplots in each CSV files. Understand trends and averages of the datas\n# Total Value Count (#bar charts would be created)\n# Missing data\n# Any correlation in metrics?\n# Areas to Explore?\n# Enginerring\n# PreProcessing Data \n# Scaling the Project\n","d5b0e252":"# Examine null values across different different data types in training data set and to understand the data set better \n    # As seen below Cabin and Age have the most null values. Will have to explore further. \ntraining.info()","2a02a349":"training.describe()\n#At First glance the survival rate is 38.38%","fa1df294":"#Seperating Numerical Columns\ntraining.describe().columns","a1b38a50":"#Breakout of Numerical and Categorical variables\ndf_numerical  = training[['Age','SibSp','Parch','Fare']] # Will use histogram to analyze\ndf_categorical = training[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']] # Value Counts to Analyze\n","912342ac":"#distributions for  numeric variables \nfor i in df_numerical.columns:\n    plt.hist(df_numerical[i])\n    plt.title(i)\n    plt.show()","df61b66e":"# Comparing survival rates across various factors such as age of survivors, fare they paid, family size or number of children.\n\npd.pivot_table (training, index = \"Survived\", values = ['Age','Parch','SibSp','Fare', ])","239fc702":"print(df_numerical.corr())\nsns.heatmap(df_numerical.corr()) # a heatmap to visually represent correlation between different numerical variables ","29e6440b":"for i in df_categorical.columns:\n    sns.barplot(df_categorical[i].value_counts().index,df_categorical[i].value_counts()).set_title(i)\n    plt.show()","d82b6f0d":"pd.pivot_table(training, index = 'Survived', columns = 'Pclass', values = 'Ticket', aggfunc = 'count')\n","64b03012":"pd.pivot_table(training, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count')","820d08de":"pd.pivot_table(training, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count')","0d88af59":"# The Cabin currently has too much info in the previous graph above.This is makes it difficult to digest and \n# break it down for further analysis.  The command below lets check if there's multiple cabins (lambda x:0 if  pd.isna(x)) us split by the spaces (else len x.split command)\ndf_categorical.Cabin\ntraining['multiple_cabins'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n\ntraining ['multiple_cabins'].value_counts()","2592243b":"#Pivot table based on to see if surivival rate had to do anything with having multiple cabins. \npd.pivot_table(training, index = 'Survived', columns = 'multiple_cabins', values = 'Ticket' ,aggfunc ='count')\n","a696f25a":"#creates categories based on the cabin letter (n stands for null)\n#will treat null values like it's own category\n\ntraining['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])","19285f40":"#Survival % based on cabin\nprint(training.cabin_adv.value_counts())\npd.pivot_table(training,index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')","331fa2eb":"#understand ticket values better \n#numeric vs non numeric \ntraining['num_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntraining['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)","d2648a91":"training['num_ticket'].value_counts()","c8458753":"#View all rows in dataframe. \npd.set_option(\"max_rows\", None)\ntraining['ticket_letters'].value_counts()","c4380588":"#difference in numeric vs non-numeric tickets in survival rate \npd.pivot_table(training,index='Survived',columns='num_ticket', values = 'Ticket', aggfunc='count')","acc91c72":"# Does a person's title indicate a better chance of survival? Maybe it could reflect if married or unmarried, dr or master,etc has a better chance of survival?\ntraining.Name.head(50)\ntraining['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#mr., ms., master. etc\n\ntraining['name_title'].value_counts()","7ca1f81a":"#create all categorical variables that we did above for both training and test sets \nall_data['multiple_cabins'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['num_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#impute nulls for continuous data \n\n#Median was used for age and fare as it was not normall distributed\nall_data.Age = all_data.Age.fillna(training.Age.median())\nall_data.Fare = all_data.Fare.fillna(training.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)\n\n#tried log norm of sibsp (not used)\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()\n\n# log norm of fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()\n\n# converted fare to category for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','multiple_cabins','num_ticket','name_title','train_test']])\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape","5244c2e4":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","8b7223bf":"from sklearn.model_selection import cross_val_score #Takes random samples of training data which models are run on and predicts based on this model. \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","a6db3808":"gnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d05f0721":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","6df52317":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","9f47ff02":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","01b08eba":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","141cc214":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","0242ad58":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","6ade2dc1":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","0a3f0a96":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","de84f57c":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","9e032883":"#Voting classifier takes all of the inputs and averages the results. For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote. For this, you generally want odd numbers\n#A \"soft\" classifier averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such\nfrom sklearn.ensemble import VotingC lassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb)], voting = 'soft') ","2bf2cf0a":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","c4b0f8e4":"voting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","1cfcb18d":"from sklearn.model_selection import GridSearchCV  #allows you to put in parameters and will provide the best results\nfrom sklearn.model_selection import RandomizedSearchCV ","2291a993":"#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","a4c0c4e9":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","7877710b":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","fabd3041":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","5039928e":"#Because the total feature space is so large, I used a randomized search to narrow down the paramters for the model. I took the best model from this and did a more granular search \n\"\"\"\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,500,1000], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,5,10,20,50,75,100,None],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [1,2,4,10],\n                                  'min_samples_split': [2,5,10]}\n                                  \nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf_rnd,'Random Forest')\"\"\"","851832fa":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')","59bae7bf":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')","a411898b":"best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","84537f6b":"Data is imported for analysis and will be working based of the Training set. Validations and predictions will also be based on the the training set. However, Final submissions will be based on a test set. ","02425c0a":"At an inital look, people on board who had a cabin had a higher rate of survival than those who didn't have a cabin (N) besides people in cabin A. ","143e87b4":"The data below shows that factors which may lead to a higher survival rate. The best factors of survival are:\n\n1. Fare paid\n2  Age of the peron\n3. If you classified as a man\n4. If you were a female. ","ace75cc2":"Titanic Project Machine Learning Data\n","8b2df372":"Data Cleaning\n\nSimplify Data by taking out uncessary fields\n\n2) Check if different tickets impact survival rates\n\n3)\n","7a01407a":"Some Data above does not follow normal distrubtion such as fair which seems to be really high at such a low level. ","f3d21b30":"The inspiration and credit of the modelling process comes from Ken Jee.","11563a97":"No Necessary correlation that multiple cabins might lead to a higher survival rate.","66b2f9bb":"The Data above suggests: 1)If you're young. Youve a higher chance of survival. the 28-30 age also probably has the highest survival rate because a majority of the people are in the 20-30 age bracket based on the data in the previous histrogram.\n\n2)People who paid more (fare) has a better chance of survival. Maybe easier access to get out?\n\n3) Your chance of survival as a parent is lower when you have children. Could be their priority was to get their kids to safety first. ","67603665":"Model Building (Baseline Validation Performance)\nTested Models with defaul parameteres . Following models used 5 fold cross validation to get a baseline. With a validation set basline, we can see how much tuning improves each of the models. Just because a model has a high basline on this validation set doesn't mean that it will actually do better on the eventual test set.\n\n* Naive Bayes \n* Logistic Regression \n* Decision Tree\n* K Nearest Neighbor \n* Random Forest \n* Support Vector Classifier\n* Xtreme Gradient Boosting \n* Soft Voting Classifier - All Models ","9a39c053":"1) Drop null values from Embarked (only 2)\n\n2) Include only relevant variables \nVariables: 'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'multiple_cabin', 'num_ticket', 'name_title'\n\n3) Do categorical transforms on all data. Usual ly we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder).\n\n4) Impute data with mean for fare and age (Should also experiment with median)\n\n5) Normalized fare using logarithm to give more semblance of a normal distribution\n\n6) Scaled data 0-1 with standard scaler","ffacea37":"Some takeways from the heat map.\n1. The number of families travelling together seems highly correlate (SibSp,Patch)\n2. Age and Children is negatively correlated\n","50412bbd":"**Model Imprpovements**\n\nAfter creating baselines, i wanted to see if the model results could be improved.  ","10a447eb":"Data Exploration \n\n1)Number Data \n* Histograms to understand distribution and Skewness\n* Correlation Plot\n* Pivot Table to compare survival rates across numeric \n\n2) Categorical data\n*  Bar chart to understand different classes aboard the titanic\n*  Pivot Tables to understand relationship of those who survived","e2e87398":"** Project Planninng**\n This is the most important step of working with any projects. An outline is created by me to show my thought process behind creating the project\n ","d992a6d5":"Overview\n1) Understand the Data\n2) Data Cleaning\n3) Data Exploration\n4) Feature Cleaning\n5) Data Processing for the Model\n6) Modelling\n"}}