{"cell_type":{"3ba03274":"code","08a35bb5":"code","707770f5":"code","69c8b11e":"code","f8ef783c":"code","26c4590f":"code","c2075d03":"code","1c2a28c4":"code","bcf7eda9":"code","6d5825b3":"code","8e29e200":"code","e5cb59a4":"code","27541ebf":"code","dbfaab70":"code","2064ffef":"code","f577030d":"code","f8af5680":"code","bc7dfe9a":"code","f57ae34b":"code","ce396380":"code","cf71d1d7":"code","5859b660":"code","d55ae924":"code","44b3d882":"markdown","8edf4636":"markdown","6bc32447":"markdown","ab496964":"markdown","8779da7e":"markdown","73c44720":"markdown","c296b79f":"markdown","335d0dda":"markdown","84bee097":"markdown","302516fa":"markdown","8a121e37":"markdown","0b001dc9":"markdown","e6a0ddf8":"markdown","57289daa":"markdown","e5b708bd":"markdown","e65fe40e":"markdown","8396db53":"markdown","1502f549":"markdown","42a5b1cb":"markdown","3f1e8662":"markdown","178b8224":"markdown","6456b15b":"markdown","ba05f86d":"markdown"},"source":{"3ba03274":"import numpy as np\nimport pandas as pd\npd.set_option(\"max_columns\", 200)        \nfrom umap import UMAP\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom wordcloud import WordCloud\nfrom sklearn.cluster import KMeans\nfrom matplotlib_venn import venn3","08a35bb5":"#\u00a0load the survey results\ndf = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/multiple_choice_responses.csv')\n\n#\u00a0drop the first row containing the questions\n\ndf.drop(0, inplace=True)","707770f5":"# create boolean masks of people who identify as Female and who are located in India\ngender_mask = df['Q2']=='Female'\ncountry_mask = df['Q3']=='India'\n\ndf_subset = df[gender_mask & country_mask]\n\n# you can then do your analysis, lets look at the mode of each column:\ndf_subset.mode().iloc[0:1,] # iloc is a hack to drop some NaN column","69c8b11e":"jobcloud = WordCloud(background_color='white').generate(\" \".join(df_subset['Q5'].dropna()))\n\nfig = plt.figure()\nplt.imshow(jobcloud)\nplt.axis('off')\nfig.set_size_inches(10,7)","f8ef783c":"# Make a list of all countries in SA\nsouth_american_countries = ['Brazil', 'Colombia', 'Argentina','Peru','Venezuela',\n                            'Chile', 'Ecuador', 'Bolivia', 'Paraguay', 'Uruguay',\n                            'Guyana', 'Suriname', 'French Guiana']\n\n# create boolean mask and use it\nSA_mask = df['Q3'].isin(south_american_countries)\ndf_subset = df[SA_mask]\n\n# again look at the mode of our subset:\ndf_subset.mode().iloc[0:1,] # iloc is a hack to drop some NaN columns","26c4590f":"# count number of respondents in each age group\nage_group_cnt = df_subset.groupby('Q1')['Q2'].count()\n\n#\u00a0plot the graph\nfig, ax = plt.subplots()\nax.bar(age_group_cnt.index, age_group_cnt.values, color='gray', alpha=0.80)\n\n# add axis labels etc.\nax.set_xlabel('Age Group')\nax.set_ylabel('Number of respondents')\nax.set_title('Age of of respondents in South America')\n_ = plt.setp(ax.get_xticklabels(), rotation=90)\nfig.set_size_inches(6,4)","c2075d03":"salary_mapping = {'$0-999':'low', '1,000-1,999':'low', \n                  '10,000-14,999':'low', '100,000-124,999':'high',\n                  '125,000-149,999':'high', '15,000-19,999':'low', \n                  '150,000-199,999':'high', '2,000-2,999':'low',\n                  '20,000-24,999':'low', '200,000-249,999':'high', \n                  '25,000-29,999':'medium', '250,000-299,999':'high',\n                  '3,000-3,999':'low','30,000-39,999':'medium',\n                  '300,000-500,000':'high', '4,000-4,999':'low',\n                  '40,000-49,999':'medium', '5,000-7,499':'low', \n                  '50,000-59,999':'medium', '60,000-69,999':'medium',\n                  '7,500-9,999':'low', '70,000-79,999':'medium', \n                  '80,000-89,999':'medium', '90,000-99,999':'medium',\n                  '> $500,000':'high'}\n\n# create new column for the income group and convert the old salary\ndf['income_group'] = df['Q10'].map(salary_mapping)\n\n# check the number of respondents in each income group\ndf.groupby('income_group')['Q1'].count()","1c2a28c4":"# Using our new group, we can then use boolean masking (example 1) to look at a particular earning bracket:\n\nincome_mask = df['income_group'] == 'high'\n\ndf_subset = df[income_mask]\ndf_subset.mode().iloc[0:1,]\n","bcf7eda9":"df.loc[~df['Q2'].isin(['Male', 'Female']), 'Q2'] = 'Other'\n\nfig, axes = plt.subplots(nrows=1, ncols=3)\n\nfor ax, income in zip(axes, ['low','medium','high']):\n    df_income = df[df['income_group']==income]\n    gender_count = df_income.groupby('Q2')['Q1'].count()\n    ax.pie(gender_count.values, labels=gender_count.index, autopct='%.1f',\n           colors=['#a6d99c','#b19cd9','#d9d09c'])\n    ax.set_title(income.capitalize() + ' income')\n\nfig.set_size_inches(12,5)","6d5825b3":"# get a df with just media questions in it\ncolumns_to_cluster = df.columns.str.contains('Q12')\ndf_media = df.loc[:, columns_to_cluster]\n\n# convert it to a binary df\ndf_media = pd.get_dummies(df_media).iloc[:,0:10]\n\n# clean up the column names\nnew_col_names = [col.split('_')[-1].split('(')[0].strip() for col in df_media.columns]\ndf_media.columns = new_col_names\n\n# optionally drop anyone who didn't select any media interaction\ndrop_mask = ~(df_media.sum(axis=1)==0)\ndf_media = df_media[drop_mask]\ndf_subset = df[drop_mask]","8e29e200":"# perform the clustering\ny_pred = KMeans(n_clusters=3, random_state=42, max_iter=10000).fit_predict(df_media.values)\n\n# add the cluster identification to the df\n\ndf_media['cluster_number'] = y_pred","e5cb59a4":"# inspect the different clustering\ncluster_sizes = df_media.groupby('cluster_number').sum()\ncluster_sizes","27541ebf":"fig, axes = plt.subplots(1, 3)\n\nfor i, (ax, cluster) in enumerate(zip(axes, cluster_sizes.index)):\n    # get the top three media types used in the cluster\n    values = cluster_sizes.loc[cluster,:]\n    top_three = values.sort_values()[-3:]\n    top_3_names = list(top_three.index)\n    # create the venn diagram, \n    masks = [(df_media[top_3_names[i]]==0, df_media[top_3_names[i]]==1) for i in [0,1,2]]\n    venn3(subsets=(len(df_media.loc[masks[0][1] & masks[1][0] & masks[2][0]]),\n                   len(df_media.loc[masks[0][0] & masks[1][1] & masks[2][0]]),\n                   len(df_media.loc[masks[0][1] & masks[1][1] & masks[2][0]]),\n                   len(df_media.loc[masks[0][0] & masks[1][0] & masks[2][1]]),\n                   len(df_media.loc[masks[0][1] & masks[1][0] & masks[2][1]]),\n                   len(df_media.loc[masks[0][0] & masks[1][1] & masks[2][1]]),\n                   len(df_media.loc[masks[0][1] & masks[1][1] & masks[2][1]])),\n          set_labels=(top_3_names[0], top_3_names[1], top_3_names[2]), \n          ax=ax)\n    # add titles to the plots\n    ax.set_title(f'Cluster {i+1}', fontsize=14)\n\nfig.set_size_inches(20,8)","dbfaab70":"# assign clusters to original df\ndf_subset['clusters'] = y_pred+1","2064ffef":"# look at mode of cluster 2\ndf_subset[df_subset['clusters']==2].mode()","f577030d":"# look at mode of cluster 3\ndf_subset[df_subset['clusters']==3].mode()","f8af5680":"# select just a subsection of the questions to cluster on\nquestions_to_use = ['Q4','Q5','Q6','Q14']\n# drop the first row\ntry:\n    df.drop(0, inplace=True, axis=0)\nexcept KeyError:\n    print('Row 0 does not exist')\ncolumn_mask =1","bc7dfe9a":"# one hot encode the questions\nencoded_df = pd.get_dummies(df[questions_to_use])\n\n# make the column names more readable\nstripped_columns = [col.split('_')[1] if not col.endswith('Other') else col for col in encoded_df.columns]\nencoded_df.columns = stripped_columns","f57ae34b":"umap_params = {'metric':'hamming', # hamming is a boolean distance metric\n               'n_neighbors':500, # focus more on global structure\n              'random_state':1, # use the random seeds to keep output reproducible\n               'transform_seed':1\n              }","ce396380":"embedder = UMAP(**umap_params)\n\nX_embedded = embedder.fit_transform(encoded_df)\n\n# add the coords in the 2D embedded space  for each instance\nencoded_df['x'] = X_embedded[:,0]\nencoded_df['y'] = X_embedded[:,1]\n\n# add income\nencoded_df['income_group'] = df['income_group']\n","cf71d1d7":"fig = px.scatter(encoded_df, x=\"x\", y=\"y\", hover_data=encoded_df.columns[:-2], color='Data Scientist')\nfig.show()","5859b660":"fig = px.scatter(encoded_df, x=\"x\", y=\"y\", hover_data=encoded_df.columns[:-2], color='Data Scientist')\nfig.layout.xaxis.range = (16.5,19)\nfig.layout.yaxis.range = (-50,-47)\nfig.show()","d55ae924":"encoded_df.loc[encoded_df['income_group'].isna(), 'income_group'] = 'No information'\nfig = px.scatter(encoded_df, x=\"x\", y=\"y\", hover_data=encoded_df.columns[:-2], color='income_group', opacity=0.5)\nfig.show()","44b3d882":"Again, we can extract some insights from this analysis: The highest number of South American respondents were from Brazil and the largest job type was Data Scientist.\n\nWith out subset, one can now move on and do more analysis. This time lets plot a bar graph of ages:","8edf4636":"### Example 1.3: Creating new high-level groups\n\nYou are an economically minded person and are interested in looking at two subsets of the data science communites: those on a low income and those on a high income. As a economic expert you believe you have a reliable mapping of reported income bracket to a 'low', 'medium' or 'high' income so you can use the DataFrame `.map` method to easily convert the reported income bracket to an income category:","6bc32447":"## Conclusion\n\nI have shown three different methods of identfying subsets of the Data Science community in the 2019 Kaggle DS\/ML survey. Domain knowledge can be used to identify subsets of potential interest and gennerally creates 'clear-cut' groups (Male vs Female, Country 1 vs Country 2). KMeans clustering can find subsets of the community not so obvious to the naked eye, but the groups our not necessarily that 'clean'. UMAP produces very clean groups which arise naturally in the data, but often with complex definitions (e.g., Masters students, who earn a lot of money who use Python).\n\nGoing further, I hope you can try different iterations of the methods shown in this kernel to identify subsets of the DS community represented in the 2019 Kaggle survey which will can be used to create your analytics story.","ab496964":"### 3.4 Using colour to look for additional patterns","8779da7e":"### 2.1 First prepare the data (using a one-hot encoded representation of the media question)","73c44720":"# Method 2: Using KMeans clustering\n\nLast years [2nd place entry](https:\/\/www.kaggle.com\/robikscube\/a-tale-of-4-kaggler-types-by-ide-use-2018-survey) by Rob Mulla used KMeans clustering to great effect to identify 4 types of Kagglers by their IDE use. In this section I follow this work to show you how you can identify subsets in this years Data Science survey challenge.\n\n\nIn this example I cluster on respondents *media use* (Q12 of the 2019 survey). But you can change this to use any question (or multiple questions) to identify subsets of the data science community of interest to you.","c296b79f":"# Identifying subsets of the data science community in the 2019 Kaggle survey\n\nThis years survey challenge relies on telling a story about a subset of the data science community represented in the survey:\n> The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration.\n\n\nIn the previous years [some winners](https:\/\/www.kaggle.com\/mhajabri\/africai) had a great narrative and some found [interesting subsets](https:\/\/www.kaggle.com\/robikscube\/a-tale-of-4-kaggler-types-by-ide-use-2018-survey#The-4-Types-of-Kagglers-(by-IDE-use) of the community to tell a story about. In this kernel I show some methods on how you can identify a subset of the DS community represented in 2019 Kaggle DS\/ML survey.","335d0dda":"# Method 3: Using UMAP\n\nWhile you can generate additional masks using the technique outlined in Method 1 to start investigating more and more niche subgroups of the data science community, another option is to cluster the data and see what subgroups exist 'naturally'. You can then select some (or one) of these sub-groups to focus your analysis around.\n\nFor this example, I will use [UMAP](https:\/\/umap-learn.readthedocs.io\/en\/latest\/) a manifold technique which can produce visualisations similar to the t-SNE algorithm. We will produce a 2 dimensional embedding of the dataset using UMAP and use this to identify some subgroups of the community.","84bee097":"### 2.2 Now perform the K-means clustering and inspect\n\nFor now we use 3 clusters, but this will need to be played with depending the depth of analysis you have in mind.\n\nYou can see that the clusters are not completely clean, but a subset of media usages dominate each cluster.","302516fa":"Just from looking at the mode of each question we  can already draw some insights from, for example the biggest fraction of female respondents from India are Students.\n\nYou can then progess and look at the subset in detail, for example using a wordcloud:","8a121e37":"### Lets look at the gender distribution across our new 'income_group' feature\n\nThis time lets create a pie chart of the gender distribution by the new feature 'income category':\n\nWhat you will see is there is a clear gender difference between different income categories: a smaller proportion of high earning respondents identify as Female.","0b001dc9":"### 3.1 Perform the UMAP embedding\n\nWe will use a high number of nearest neighbours parameters to maintain more of the global structure. This is with some time trade-off.","e6a0ddf8":"### 2.3 Plot venn diagrams of each cluster\n\nAs in the [original notebook](https:\/\/www.kaggle.com\/robikscube\/a-tale-of-4-kaggler-types-by-ide-use-2018-survey#The-4-Types-of-Kagglers-(by-IDE-use) we can use a Venn diagram to visualise the clusters. \n\n\nWe can see that Cluster 1 features people who primarily use Kaggle, Blogs or Youtube to engage with Data science media. Respondents in cluster 2 mainly use either Kaggle or Youtube but do sometimes engage with Journal publications. In cluster three people mainly use Kaggle and Blogs but also use Journal publications to engage with data science media. \n\nNot only does this clustering and graphic tell you about the state of data science media engagement (respondents mainly use Kaggle, Blogs and Youtube but also use Journal publications for their media access) but also allows you to identify different sub-groups of the community:\n>- Cluster 1: People who use Kaggle, Youtube and blogs in equal amounts.\n- Cluster 2: People who mainly use Kaggle and Youtube and some Journal publications.\n- Cluster 3: People who mainly use Kaggle and Blogs and some Journal publications.\n","57289daa":"### 3.2 Plot the overall embedding and investigate\n\nIn the following I plot a basic scatter graph of the 2D embeddings and colour it by whether the respondent is a Data Scientist or not. Hover over the individual instances to see what job, education level, company size and primary analysis software they used. How do the clusters form?","e5b708bd":"Looking at the mode we can already get some valuable insights; for example we can see that the highest number of 'high' earners are located in the United States.","e65fe40e":"### 3.3 Zoom in on  a cluster of interest centered at (17.6, -48) - what subgroup does this cluster correspond too?\n*(You can do this by interating with the above graph, but I will do it explicitly  in the next cell)*\n\n---\n\n\n\nYou can see that this cluster corresponds to Data scientists with a Master's degree who work for small companies (0-49 employees). This is not a 'sub-group' you would necessarily think of using a domain-knowledge approach only!\n\nHaving found this cluster, we can see it features a lot of respondents and would therefore be worth investigating further. We could look at Data scientists with Masters degree's who work for small companies and compare it with Data scientists with masters degrees who work at large companies. Asking questions such as:\n>- How do there earnings differ?\n- Do they use different software?\n- Do they have different age distributions?\n","8396db53":"### Perform some analysis on our identified subset community: Bar chart","1502f549":"Another option you can try is colouring the clusters by something they were *not* clustered using (e.g., the income group feature we made in Method 1) to see if any interesting patterns jump out (e.g., does income correlate with job type)? This can be used to help inform which subsets of the survey respondents could be interesting to investigate.\n\nFor example, in the following figure  I replot the 2 dimensional embedding of the data and look at income. \n\n\nSeveral things jump out:\n\n>1. In the top right of the figure primarily respondents with no income information has been clustered (without this information being provided). Looking at the instances in detail shows these are students and people who are not employed. The individual clusters within the group correspond to different highest education attainment levels (e.g., 'Bachelors' or 'Masters').\n2. Low income respondents make up the the biggest fraction of respondents.\n3. No other additional structure is obvious. This suggests that the data we had cluster on may not correlate strongly with income. ","42a5b1cb":"#### Perform some analysis on our identified community subset (Word cloud)","3f1e8662":"### Example 1.2: Advanced boolean masking\n\nYou are a european who spent several years of your life backpacking around South America and now, as a Data Scientist back living in Europe, you are considering migrating to South America to live and work. Motivated by this, you have decided to use the Kaggle DS\/ML survey to investigate what the Data science community looks like for South Americans and evaluate your potential prospects.\n\nWhile you could string together a large a number of conditionals (as in example 1) to extract this subset, you can take advantage of some other DataFrame methods to speed up your indexing:","178b8224":"# Method 1: Using 'domain' knowledge\n\nThe most direct way to extract subsets of the survey respondents is to use your domain knowledge to extract subsets you believe will create an interesting story. \n\nBelow I show several examples of using domain knowledge and how you can extract these using pandas.","6456b15b":"### 2.4 Use your clusters in your analysis\n\nGoing forward, you can use these cluster identifications (or subsets of the data science community) to compare these subsets of the data science community to create your survey story.\n\nFor example, below we look at the most frequently occuring answer to each question for clusters 2 and 3. From visual inspection we can see that for cluster 2 (using primarily Kaggle, Youtube and Journal publications) the most frequent respondents was a student and for cluster 3 (using Kaggle, Blogs and Journal publications) the most frequent respondents were Data Scientist.","ba05f86d":"### Example 1.1: Boolean masking\n\nYou are aware that there is a large gender imbalance in data science so wish to perform a study related to this. After reading some other [Kaggle kernels](https:\/\/www.kaggle.com\/parulpandey\/geek-girls-rising-myth-or-reality-wip) you find out that India has the smallest gender imbalance of any responding nation. You therefore decide to tell an analysis story about Female respondents in India to investigate the employment, income and education of Female repsondents in the nation with the smallest gender imbalance. \n\nThis can be performed with simple boolean masking as follows:"}}