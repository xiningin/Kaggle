{"cell_type":{"2671a72f":"code","18a63b4f":"code","f8b39759":"code","80520c0c":"code","987ec10c":"code","cb986148":"code","fd84caeb":"code","480b537d":"code","11d0f388":"code","de24ca44":"code","6fd218e7":"code","2168acf9":"code","00c8f012":"code","7c28c84f":"code","c5e788d4":"code","e523c288":"code","a0d9a9f7":"code","292b0aeb":"code","0da6d50f":"code","2cbe5ef9":"code","8a141883":"code","963fe5ec":"code","4bf00616":"code","94621d43":"code","2555ac3c":"code","769d7b18":"code","5e86b402":"code","9a58d2d5":"code","a0152b57":"code","a0332935":"code","ae0db265":"code","5bf33888":"code","f24e805e":"code","1cf3fdbb":"code","0a3fe372":"code","ac5a45a3":"code","212954a5":"code","0e38fd1d":"code","4d2c0bcd":"code","4031170a":"code","e234b37d":"code","289738df":"code","56963bfb":"code","d1222899":"code","5c28d524":"code","917a49f9":"code","f865da95":"code","fc27fa32":"code","c96f6c3d":"code","68ae419e":"code","d129f66b":"code","5cafa917":"code","dc9c3cd1":"code","02b0237a":"code","99553680":"code","390487c1":"code","fece697e":"code","4877e286":"code","41419cb5":"code","684d49f4":"code","d1afc4db":"code","170e34d6":"code","8214e4a3":"code","50149d6a":"code","abb8940b":"code","b859cde8":"code","519ff1b7":"code","b0b55c45":"code","ec92809c":"code","b0523916":"code","adf743f6":"code","249db9ec":"code","f89a9d66":"code","5c434730":"code","7834bd00":"markdown","8298c8a3":"markdown","5e3323da":"markdown","93a23784":"markdown","c48a62ed":"markdown","e41783eb":"markdown","8d0ace35":"markdown","8892f7b8":"markdown","a3f3ef78":"markdown","cd5de3ba":"markdown","00d66f50":"markdown","7e3e2425":"markdown","f98f98c2":"markdown","975d2bd1":"markdown","28029cf3":"markdown","cc361438":"markdown","b250be31":"markdown","01aaeb09":"markdown","948c12d6":"markdown","fa84cc1b":"markdown","943a0936":"markdown","2181b367":"markdown","ad663123":"markdown","10a71827":"markdown","2422e2e0":"markdown","a0b8bee5":"markdown","d10ea4b3":"markdown","686e4928":"markdown","98d3357d":"markdown","047391ec":"markdown","f5083566":"markdown","f6e9e9c3":"markdown","b026d73b":"markdown","726b49df":"markdown","05178028":"markdown","f5a71e4a":"markdown","cc4ee0bb":"markdown","1b39889d":"markdown","7e545509":"markdown","692b3e7d":"markdown","67d1724b":"markdown","957f3002":"markdown","ad1e8b51":"markdown","0dbde177":"markdown","31399cc0":"markdown","330f3590":"markdown","9325c5a3":"markdown","39a5275a":"markdown","7f886d78":"markdown","0b49362b":"markdown","3b4b354f":"markdown","3336d8fc":"markdown","99407b83":"markdown","29ccdaf8":"markdown","b2903edc":"markdown","b6cacb54":"markdown","1e949cb0":"markdown","435fa069":"markdown","c92cb1d2":"markdown","de05a29a":"markdown","1fef0f6c":"markdown","1554ca3a":"markdown","2addf57d":"markdown","b087f519":"markdown","0c040fa0":"markdown","c03f356c":"markdown","3718c876":"markdown","92e40173":"markdown","5b87e30f":"markdown","5e9f19b8":"markdown","38b99866":"markdown","7e5d4560":"markdown","8dcf7d3a":"markdown","32c916a6":"markdown","39c257cd":"markdown","e59008ec":"markdown","84380dd0":"markdown","c71dfa81":"markdown","3fbbec5c":"markdown","d915a856":"markdown","27719803":"markdown","e279fb2e":"markdown","303de63b":"markdown","45dc8416":"markdown","a7856b02":"markdown","9022ce2d":"markdown","e37f1735":"markdown","60a7bac4":"markdown"},"source":{"2671a72f":"# Libraries to help with reading and manipulating data\nimport numpy as np\nimport pandas as pd\n\n# Libraries to help with data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\n\nimport seaborn as sns\n\n# to scale the data using z-score\nfrom sklearn.preprocessing import StandardScaler\n\n# to compute distances\nfrom scipy.spatial.distance import cdist\n\n# to perform k-means clustering and compute silhouette scores\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.datasets import make_blobs\n\n\n# to visualize the elbow curve and silhouette scores\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n\n\n# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n\n# to compute distances\nfrom scipy.spatial.distance import pdist\n\n# to perform PCA\nfrom sklearn.decomposition import PCA\n\n# Pandas dataframe options\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_colwidth',400)\n\n\n# set the background for the graphs\nplt.style.use('ggplot')\n\n# For pandas profiling\nfrom pandas_profiling import ProfileReport\n\n# Printing style\nfrom tabulate import tabulate\n\n# Library to suppress warnings or deprecation notes \nimport warnings\nwarnings.filterwarnings('ignore')","18a63b4f":"!pip install openpyxl","f8b39759":"cc_data = pd.read_excel('..\/input\/credit-card-customers-segmentation\/Credit Card Customer Data.xlsx')","80520c0c":"data = cc_data.copy()","987ec10c":"print('There are {row} records, and {col} columns in the dataset'.format(row=data.shape[0], col=data.shape[1]))","cb986148":"data.isnull().sum()","fd84caeb":"data.duplicated().sum()","480b537d":"data.sample(10)","11d0f388":"data.dtypes","de24ca44":"data = data.set_index(['Sl_No'])","6fd218e7":"data.index.max()","2168acf9":"data['Customer Key'].nunique()","00c8f012":"data[data['Customer Key'].isin(data[data['Customer Key'].duplicated()]['Customer Key'].tolist())].sort_values('Customer Key')","7c28c84f":"def feature_name_standardize(df: pd.DataFrame):\n    df_ = df.copy()\n    df_.columns = [i.replace(\" \", \"_\").lower() for i in df_.columns]\n    return df_","c5e788d4":"data = feature_name_standardize(data)","e523c288":"data.describe().T","a0d9a9f7":"def summary(data: pd.DataFrame, x: str):\n    '''\n    The function prints the 5 point summary and histogram, box plot, \n    violin plot, and cumulative density distribution plots for each \n    feature name passed as the argument.\n    \n    Parameters:\n    ----------\n    \n    data: pd.Datafraame, the dataset\n    x: str, feature name\n    \n    Usage:\n    ------------\n    \n    summary(data, 'age')\n    '''\n    x_min = data[x].min()\n    x_max = data[x].max()\n    Q1 = data[x].quantile(0.25)\n    Q2 = data[x].quantile(0.50)\n    Q3 = data[x].quantile(0.75)\n    \n    dict={'Min': x_min, 'Q1': Q1, 'Q2': Q2, 'Q3': Q3, 'Max': x_max}\n    df = pd.DataFrame(data=dict, index=['Value'])\n    print(f'5 Point Summary of {x.capitalize()} Attribute:\\n')\n    print(tabulate(df, headers = 'keys', tablefmt = 'psql'))\n\n    fig = plt.figure(figsize=(16, 8))\n    plt.subplots_adjust(hspace = 0.6)\n    sns.set_palette('Pastel1')\n    \n    plt.subplot(221, frameon=True)\n    ax1 = sns.distplot(data[x], color = 'purple')\n    ax1.axvline(\n        np.mean(data[x]), color=\"purple\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax1.axvline(\n        np.median(data[x]), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram\n    plt.title(f'{x.capitalize()} Density Distribution')\n    \n    plt.subplot(222, frameon=True)\n    ax2 = sns.violinplot(x = data[x], palette = 'Accent', split = True)\n    plt.title(f'{x.capitalize()} Violinplot')\n    \n    plt.subplot(223, frameon=True, sharex=ax1)\n    ax3 = sns.boxplot(x=data[x], palette = 'cool', width=0.7, linewidth=0.6, showmeans=True)\n    plt.title(f'{x.capitalize()} Boxplot')\n    \n    plt.subplot(224, frameon=True, sharex=ax2)\n    ax4 = sns.kdeplot(data[x], cumulative=True)\n    plt.title(f'{x.capitalize()} Cumulative Density Distribution')\n    \n    plt.show()","292b0aeb":"print('We will check the summary of below columns: \\n', data.columns.tolist())\n","0da6d50f":"summary(data, 'avg_credit_limit')","2cbe5ef9":"summary(data, 'total_credit_cards')","8a141883":"summary(data, 'total_visits_bank')","963fe5ec":"summary(data, 'total_visits_online')","4bf00616":"summary(data, 'total_calls_made')","94621d43":"# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 1, 3))\n    else:\n        plt.figure(figsize=(n + 1, 3))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() \/ total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() \/ 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot","2555ac3c":"data['cc_spending_bin'] = pd.cut(data['avg_credit_limit'], bins=[3000,\t10000,\t18000, \t48000,\t200000]\n                                ,labels=['Very Low', 'Low', 'Mid', 'High'], include_lowest=True)\n","769d7b18":"labeled_barplot(data, 'cc_spending_bin', perc=True)","5e86b402":"labeled_barplot(data, 'total_credit_cards', perc=True)","9a58d2d5":"labeled_barplot(data, 'total_visits_bank', perc=True)","a0152b57":"labeled_barplot(data, 'total_visits_online', perc=True)","a0332935":"labeled_barplot(data, 'total_calls_made', perc=True)","ae0db265":"plt.figure(figsize=(20,20));\nsns.set(palette=\"Set2\");\nsns.pairplot(data.iloc[:, 1:], diag_kind='kde', corner=True);","5bf33888":"# Plotting correlation heatmap of the features\n\nmask = np.zeros_like(data.iloc[:, 1:].corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.set(rc={\"figure.figsize\": (8, 8)})\n\nsns.heatmap(\n    data.iloc[:, 1:].corr(),\n    cmap=sns.diverging_palette(20, 220, n=200),\n    annot=True,\n    mask=mask,\n    center=0,\n)\nplt.show()","f24e805e":"# Function to plot numerical feature by each category with target hue\n\ndef plot_numeric_by_cat(data: pd.DataFrame, category_columns: list, numeric_column: str, hue: str = None):\n    '''\n    The function plots a numerical feature in box plot by every category column specified in the list, \n    with hue of a target category\n    '''\n    num_cols = 2\n    num_rows = int(len(category_columns) \/2 + 1)\n\n    plt.figure(figsize=(20, 8*num_rows))\n    for i, col in enumerate(category_columns):\n        plt.subplot(num_rows, num_cols, i+1)\n        sns.set(palette=\"nipy_spectral\");\n        sns.boxplot(data=data, x=col, y=numeric_column, hue=hue, showfliers=True).set(title = numeric_column + ' vs. ' + col );","1cf3fdbb":"plot_numeric_by_cat(data\\\n                    ,category_columns=['total_credit_cards', 'total_visits_bank', 'total_visits_online', 'total_calls_made']\\\n                    ,numeric_column='avg_credit_limit')","0a3fe372":"# scaling the dataset before clustering\ndata_copy = data.copy()\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data.iloc[:,1:-1])","ac5a45a3":"clusters = range(1, 10)\nmeanDistortions = []\n\nfor k in clusters:\n    model = KMeans(n_clusters=k)\n    model.fit(data_scaled)\n    prediction = model.predict(data_scaled)\n    distortion = (\n        sum(\n            np.min(cdist(data_scaled, model.cluster_centers_, \"euclidean\"), axis=1)\n        )\n        \/ data_scaled.shape[0]\n    )\n\n    meanDistortions.append(distortion)\n\n\nplt.plot(clusters, meanDistortions, \"bo-\")\nplt.xlabel(\"k\")\nplt.ylabel(\"Average distortion\")\nplt.title(\"Selecting k with the Elbow Method\")\nfor x,y in zip(clusters, meanDistortions):\n\n    label = \"{:.2f}\".format(y)\n\n    plt.annotate(label, # this is the text\n                 (x + 0.2,y), # these are the coordinates to position the label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center') # horizontal alignment can be left, right or center\nplt.show()","212954a5":"sil_score = []\ncluster_list = list(range(2, 10))\nfor n_clusters in cluster_list:\n    clusterer = KMeans(n_clusters=n_clusters)\n    preds = clusterer.fit_predict((data_scaled))\n    # centers = clusterer.cluster_centers_\n    score = silhouette_score(data_scaled, preds)\n    sil_score.append(score)\n    \n\nplt.plot(cluster_list, sil_score, \"bo-\")\nplt.xlabel(\"k\")\nplt.ylabel(\"Silhouette Score \")\nplt.title(\"Selecting k using Silhouette Score\")\nfor x,y in zip(cluster_list, sil_score):\n\n    label = \"{:.2f}\".format(y)\n\n    plt.annotate(label, # this is the text\n                 (x + 0.2,y), # these are the coordinates to position the label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center') # horizontal alignment can be left, right or center\nplt.show()","0e38fd1d":"# finding optimal no. of clusters with silhouette coefficients\nvisualizer = SilhouetteVisualizer(KMeans(3, random_state=1))\nvisualizer.fit(data_scaled)\nvisualizer.show()","4d2c0bcd":"# finding optimal no. of clusters with silhouette coefficients\nvisualizer = SilhouetteVisualizer(KMeans(4, random_state=1))\nvisualizer.fit(data_scaled)\nvisualizer.show()","4031170a":"# let's take 3 as number of clusters\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(data_scaled)","e234b37d":"# adding kmeans cluster labels to the original and scaled dataframes\ndata_scaled_df = pd.DataFrame(data_scaled, columns=data.iloc[:,1:-1].columns.unique().tolist())\ndata['cluster'] = kmeans.labels_\ndata_scaled_df['cluster'] = kmeans.labels_","289738df":"plt.figure(figsize=(20, 26))\n\nsns.pairplot(data_scaled_df, diag_kind='kde', corner=True, hue='cluster', palette='Set2');","56963bfb":"import plotly as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter3d(\n    x= data['total_visits_bank'],\n    y= data['total_visits_online'],\n    z= data['total_calls_made'],\n    mode='markers',\n     marker=dict(\n        color = data['cluster'], \n        size= 20,\n        line=dict(\n            color= data['cluster'],\n            width= 12\n        ),\n        opacity=0.8\n     ),\n\n)\nd = [trace1]\nlayout = go.Layout(\n    title= 'Clusters',\n    scene = dict(\n            xaxis = dict(title  = 'Bank Visits'),\n            yaxis = dict(title  = 'Online Visits'),\n            zaxis = dict(title  = 'Call Customer Service')\n        )\n)\nfig = go.Figure(data=d, layout=layout)\npy.offline.iplot(fig)","d1222899":"sns.set(style='dark')\nsns.scatterplot(x=data['avg_credit_limit'], y=data['total_credit_cards'], hue=data['cluster'], palette=['blue', 'red', 'orange'])","5c28d524":"cluster_profile = data.iloc[:,1:].groupby('cluster').mean()\ncluster_profile['count_of_customers'] = data.groupby('cluster')['customer_key'].count()\n\ncluster_profile.style.highlight_max(color='lightgreen').highlight_min(color='pink')","917a49f9":"data_scaled_df.boxplot(by='cluster', layout=(3,2), figsize=(10,14));","f865da95":"k_means_clusters = data_scaled_df['cluster']\ndata_scaled_df.drop(columns=['cluster'], inplace=True)","fc27fa32":"# list of distance metrics\ndistance_metrics = ['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation',\n        'cosine', 'euclidean', 'hamming', 'jaccard', 'mahalanobis', 'matching', 'minkowski', 'seuclidean',  'sqeuclidean']\n\n# list of linkage methods\nlinkage_methods = ['complete', 'average', 'single', 'weighted']\n\nhigh_cophenet_corr = 0\nhigh_dm_lm = [0, 0]\n\nfor dm in distance_metrics:\n    for lm in linkage_methods:\n        Z = linkage(data_scaled_df, metric=dm, method=lm)\n        c, coph_dists = cophenet(Z, pdist(data_scaled_df))\n        print(\n            \"Cophenetic correlation for {} distance and {} linkage is {}.\".format(\n                dm.capitalize(), lm, c\n            )\n        )\n        if high_cophenet_corr < c:\n            high_cophenet_corr = c\n            high_dm_lm[0] = dm\n            high_dm_lm[1] = lm","c96f6c3d":"# printing the combination of distance metric and linkage method with the highest cophenetic correlation\nprint(\n    \"Highest cophenetic correlation is {}, which is obtained with {} linkage.\".format(\n        high_cophenet_corr, high_dm_lm[1]\n    )\n)","68ae419e":"# list of linkage methods\nlinkage_methods = [\"single\", \"complete\", \"average\", \"centroid\", \"ward\", \"weighted\"]\n\n# lists to save results of cophenetic correlation calculation\ncompare_cols = [\"Linkage\", \"Cophenetic Coefficient\"]\n\n# to create a subplot image\nfig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))\n\n# We will enumerate through the list of linkage methods above\n# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation\nfor i, method in enumerate(linkage_methods):\n    Z = linkage(data_scaled_df, metric=\"euclidean\", method=method)\n\n    dendrogram(Z, ax=axs[i])\n    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\")\n\n    coph_corr, coph_dist = cophenet(Z, pdist(data_scaled_df))\n    axs[i].annotate(\n        f\"Cophenetic\\nCorrelation\\n{coph_corr:0.2f}\",\n        (0.80, 0.80),\n        xycoords=\"axes fraction\",\n    )","d129f66b":"HCmodel = AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\", linkage=\"average\")\nHCmodel.fit(data_scaled_df)","5cafa917":"data_scaled_df['hc_cluster'] = HCmodel.labels_\ndata['hc_cluster'] = HCmodel.labels_","dc9c3cd1":"cluster_profile_hc = data.iloc[:,1:].groupby('hc_cluster').mean()\ncluster_profile_hc['count_of_customers'] = data.groupby('hc_cluster')['customer_key'].count()\n\ncluster_profile_hc.style.highlight_max(color='lightgreen').highlight_min(color='pink')","02b0237a":"data_scaled_df.boxplot(by='hc_cluster', layout=(3,2), figsize=(10,14));","99553680":"data[data['customer_key'].isin(data[data['customer_key'].duplicated()]['customer_key'].tolist())].sort_values('customer_key')","390487c1":"data_scaled_df_pca = data_scaled_df.drop('hc_cluster', axis=1).copy()","fece697e":"pca = PCA()\npca.fit(data_scaled_df_pca)","4877e286":"pca.explained_variance_ratio_","41419cb5":"# visualizing the variance explained by individual principal components\nsns.set(style='darkgrid')\nplt.figure(figsize=(10, 10))\nplt.plot(\n    range(1, 6), pca.explained_variance_ratio_.cumsum(), marker=\"o\", linestyle=\"--\"\n)\nplt.title(\"Explained Variances by Components\")\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")","684d49f4":"pca = PCA(\n    n_components=3, svd_solver=\"full\"\n)  # svd_solver=full helps in faster convergence in case of very large data set\npca.fit(data_scaled_df_pca)","d1afc4db":"# checking the variance explained by individual components.\nprint('Explained variance = {var} %'.format(var=round(pca.explained_variance_ratio_.sum(),2)*100))","170e34d6":"plt.figure(figsize=(10, 10))\nplt.plot(\n    range(1, 4), pca.explained_variance_ratio_.cumsum(), marker=\"o\", linestyle=\"--\"\n)\nplt.title(\"Explained Variances by Components\")\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")","8214e4a3":"subset_pca = pca.transform(data_scaled_df_pca)\nsubset_pca_df = pd.DataFrame(subset_pca)","50149d6a":"sns.pairplot(subset_pca_df, diag_kind='kde');","abb8940b":"# list of linkage methods\nlinkage_methods = [\"single\", \"complete\", \"average\", \"centroid\", \"ward\", \"weighted\"]\n\n# lists to save results of cophenetic correlation calculation\ncompare_cols = [\"Linkage\", \"Cophenetic Coefficient\"]\n\n# to create a subplot image\nfig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))\n\n# We will enumerate through the list of linkage methods above\n# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation\nfor i, method in enumerate(linkage_methods):\n    Z = linkage(subset_pca_df, metric=\"euclidean\", method=method)\n\n    dendrogram(Z, ax=axs[i])\n    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\")\n\n    coph_corr, coph_dist = cophenet(Z, pdist(subset_pca_df))\n    axs[i].annotate(\n        f\"Cophenetic\\nCorrelation\\n{coph_corr:0.2f}\",\n        (0.80, 0.80),\n        xycoords=\"axes fraction\",\n    )","b859cde8":"hc = AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\", linkage=\"complete\")\nhc_labels = hc.fit_predict(subset_pca)","519ff1b7":"# adding hierarchical cluster labels to the original dataframe\ndata_pca = data.drop(['cluster', 'hc_cluster'], axis=1)\ndata_pca['pca_hc_cluster'] = hc_labels","b0b55c45":"data_scaled_pca_df = data_scaled_df.drop('hc_cluster', axis=1)\ndata_scaled_pca_df['pca_hc_cluster'] = hc_labels","ec92809c":"cluster_profile2 = data_pca.groupby('pca_hc_cluster').mean()\ncluster_profile2['customer_count'] = (\n    data_pca.groupby('pca_hc_cluster')['customer_key'].count().values\n)","b0523916":"# let's display cluster profile\ncluster_profile2.style.highlight_max(color=\"lightgreen\", axis=0)","adf743f6":"pca_df = subset_pca_df.copy()\npca_df['pca_hc_cluster'] = hc_labels\n\npca_df = pca_df.rename(columns={0:'component_1', 1:'component_2', 2:'component_3'})\n","249db9ec":"sns.pairplot(pca_df, diag_kind='kde', hue='pca_hc_cluster');","f89a9d66":"import plotly as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter3d(\n    x= pca_df['component_1'],\n    y= pca_df['component_2'],\n    z= pca_df['component_3'],\n    mode='markers',\n     marker=dict(\n        color = pca_df['pca_hc_cluster'], \n        size= 20,\n        line=dict(\n            color= pca_df['pca_hc_cluster'],\n            width= 12\n        ),\n        opacity=0.8\n     ),\n\n)\nd = [trace1]\nlayout = go.Layout(\n    title= 'Clusters',\n    scene = dict(\n            xaxis = dict(title  = 'Component 1'),\n            yaxis = dict(title  = 'Component 2'),\n            zaxis = dict(title  = 'Component 3')\n        )\n)\nfig = go.Figure(data=d, layout=layout)\npy.offline.iplot(fig)","5c434730":"data_scaled_pca_df.boxplot(by='pca_hc_cluster', layout=(3,2), figsize=(10,14));","7834bd00":">- The `cophenetic correlation` is `highest for average linkage methods`.\n>- `3` appears to be the appropriate `number of clusters` from the dendrogram for average linkage.","8298c8a3":"Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample.\n\nThe AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:\n\n- `Ward` minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n\n- `Maximum or complete linkage` minimizes the maximum distance between observations of pairs of clusters.\n\n- `Average linkage` minimizes the average of the distances between all observations of pairs of clusters.\n\n- `Single linkage` minimizes the distance between the closest observations of pairs of clusters.\n\nSource: scikit-learn.org\n\nIn `single linkage` hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the length of the arrow between their two closest points.\n\n&nbsp;\n\n![](https:\/\/www.saedsayad.com\/images\/Clustering_single.png)\n\nIn `complete linkage` hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the length of the arrow between their two furthest points. \n\n&nbsp;\n\n![](https:\/\/www.saedsayad.com\/images\/Clustering_complete.png)\n\nIn `average linkage` hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the average length each arrow between connecting the points of one cluster to the other.  \n\n&nbsp;\n\n![](https:\/\/www.saedsayad.com\/images\/Clustering_average.png)","5e3323da":"## Sample records","93a23784":"# AllLife Bank Customer Segmentation\n\n## Context\n\nAllLife Bank wants to focus on its credit card customer base in the next financial year. They have been advised by their marketing research team, that the penetration in the market can be improved. Based on this input, the Marketing team proposes to run personalized campaigns to target new customers as well as upsell to existing customers. Another insight from the market research was that the customers perceive the support services of the back poorly. Based on this, the Operations team wants to upgrade the service delivery model, to ensure that customer queries are resolved faster. Head of Marketing and Head of Delivery both decide to reach out to the Data Science team for help.\n\n \n\n## Objective\n\nTo identify different segments in the existing customer, based on their spending patterns as well as past interaction with the bank, using clustering algorithms, and provide recommendations to the bank on how to better market to and service these customers.\n\n \n\n## Data Description\n\nThe data provided is of various customers of a bank and their financial attributes like credit limit, the total number of credit cards the customer has, and different channels through which customers have contacted the bank for any queries (including visiting the bank, online and through a call center).\n\n## Data Dictionary\n\n- `Sl_No`: Primary key of the records\n- `Customer Key`: Customer identification number\n- `Average Credit Limit`: Average credit limit of each customer for all credit cards\n- `Total credit cards`: Total number of credit cards possessed by the customer\n- `Total visits bank`: Total number of visits that customer made (yearly) personally to the bank\n- `Total visits online`: Total number of visits or online logins made by the customer (yearly)\n- `Total calls made`: Total number of calls made by the customer to the bank or its customer service department (yearly)","c48a62ed":"> The data is `right skewed`","e41783eb":"**For 90% variance explained, the number of components looks to be 3.**","8d0ace35":"## Customer Profiling - Visualize the Clusters with Features","8892f7b8":"### Heatmap","a3f3ef78":"> The data is slightly `right skewed`","cd5de3ba":"**Let's check the variance explained by individual components.**","00d66f50":"## Data Types","7e3e2425":"# Exploratory Data Analysis","f98f98c2":"## Libraries","975d2bd1":">- If we look at the data we see that there is a group which prefers online interactions with their bank, they have a much higher credit limit and also have more credit cards (cluster - 1). \n>- The customers who prefer in-person interactions tend to have the mid-range of credit cards and credit limit (cluster - 0). \n>- The customers who contact via phonecall are in another segment, who have lowest credit limit and number of cards (cluster - 2).","28029cf3":"### Average Credit Limit distribution by Each of the Other Attributes","cc361438":"### Total Credit Cards","b250be31":"> Creating a function to plot labeled bar plot of the features, with percentage label on data bars.","01aaeb09":">- It appears, the `method of contacting the bank (In Person\/ Online\/ Call)` drives the clustering mechanism predominantly, we'll explore this in below plot","948c12d6":"### Checking the clusters for the duplicated customer keys","fa84cc1b":"## Visualize the Silhouettes","943a0936":"**Let's also visualize the silhouettes created by each of the clusters for two values of K, 3 and 4**","2181b367":"# Actionable insights and Recommendations\n\nThere appears to be three distinct categories of customers:\n\n1. In-person users: prefer to handle bank transactions in person. They have the medium number of credit cards and the medium available credit. They are also the most active users.\n2. Phone users: prefer verbally handling transactions. They have fewest credit cards and lowest credit card limit.\n3. Online users: prefer digital transactions. They also have the most credit cards and the highest available credit. Least number of customers have this type of behaviour.\n\nThe customer preferences should be used to contact the customers. Online\/phone users will probably prefer email\/text notifications, while in-person users prefer mail notifications and upselling (when at the bank location). \n\nAlso, the phone and in-person customers should be reached out to promote online banking.","ad663123":"> The attribute is `right skewed` with a lot of `outliers`","10a71827":"### Pair Plot","2422e2e0":"> If we consider the duplicate records are actually updated records for the same customer, then it can be observed that 3 of the 5 customers have actually changed their clusters\/groups. It appears, providing credit limit increase, or turning the customers to digital banking customers, we can actually move the customers to a more desirable and profitable cluster.","a0b8bee5":"### Summary of Total Calls Made to the Bank","d10ea4b3":"> - `Average Credit Limit` has slightly positive correlation with `Total Credit Cards` (obviously) and `Total Number of Visits Online`, and slightly negative correlation with `Total Calls Made`\n> - `Total Credit Cards` and `Total Calls Made` are negatively correlated\n> - `Total Visits Online` is also negatively correlated with `Total Visits to Bank`","686e4928":"## Analyzing the records with duplicate customer keys","98d3357d":"## Build Agglomerative Clustering model\n\nAlso known as bottom-up approach or hierarchical agglomerative clustering (HAC). Bottom-up algorithms treat each data as a singleton cluster at the outset and then successively agglomerates pairs of clusters until all clusters have been merged into a single cluster that contains all data. ","047391ec":"**Let's check the silhouette scores.**","f5083566":"# K-means Clustering","f6e9e9c3":"## Labeled Bar-plots","b026d73b":"> The data is `right skewed` and has some `outliers` to the right","726b49df":"## Hierarchical Clustering on lower-dimensional data","05178028":"## Cophenetic Correlations\n\nThe cophenetic correlation for a cluster tree is defined as the linear correlation coefficient between the cophenetic distances obtained from the tree, and the original distances (or dissimilarities) used to construct the tree. Thus, it is a measure of how faithfully the tree represents the dissimilarities among observations.\n\nThe cophenetic distance between two observations is represented in a dendrogram by the height of the link at which those two observations are first joined. That height is the distance between the two subclusters that are merged by that link.\n\nThe magnitude of this value should be very close to 1 for a high-quality solution. This measure can be used to compare alternative cluster solutions obtained using different algorithms.","f5a71e4a":"## Standardizing Column Names","cc4ee0bb":"I am going to try many distance metrics and linkage methods to find the best combination.","1b39889d":"# Initial Setup","7e545509":"# Data Preprocessing","692b3e7d":"> The records for same Customer Key look really different from each other. I am assuming this is either mistake in the Customer Key assignment, or we are missing `current_version_indicator` in the dataset. As of now, I am going to consider these as two different customers. After the clustering, I will analyze the groups corresponding to these sets of records","67d1724b":"> Creating a credit card limit bin out of the available data in the avg credit limit feature","957f3002":"## Bi-variate Analysis","ad1e8b51":"## Checking keys\n\nThere are only one possible attribute for key, the Customer Key. We'll first convert the Sl_No to index, and then check the Customer Key. Since this data is to be used for customer segmentation, findig the customer key is essential.\n\nI have already checked this in excel, hence depicting the same in the notebook. Considering the small size of the dataset, is it really convenient to check a few initial things in the excel file itself first.","0dbde177":"## Silhouette Scores\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","31399cc0":"> There are no duplicate records","330f3590":"# Dataset Summary","9325c5a3":"## Loading the dataset","39a5275a":"### Total Calls Made","7f886d78":"Before clustering, we should always scale the data, because, different scales of features would result in unintentional importance to the feature of higher scale while calculating the distances.","0b49362b":"## Duplicate data","3b4b354f":"K-means is often referred to as Lloyd\u2019s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose samples from the dataset . After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.\n\nsource: www.scikit-learn.org","3336d8fc":"**We see that the cophenetic correlation is maximum with `Euclidean distance` and `Average Linkage`.**\n\n**Let's see the dendrograms for the different linkage methods.**","99407b83":"### Build Model","29ccdaf8":"The first step of univariate analysis is to check the distribution\/spread of the data. This is done using primarily `histograms` and `box plots`. Additionally we'll plot each numerical feature on `violin plot` and ` cumulative density distribution plot`. For these 4 kind of plots, we are building below `summary()` function to plot each of the numerical attributes. Also, we'll display feature-wise `5 point summary`.","b2903edc":"## Dendograms\n\nA dendrogram, in general, is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters. ","b6cacb54":"> We can see clear segmentations with respect to each pair of features","1e949cb0":"## Add the cluster numbers as a new attribute in the dataset","435fa069":"## Analyzing the segments using Box Plot","c92cb1d2":"**Silhouette score for 3 clusters is highest. So, we will choose 3 as value of k.**","de05a29a":"> The attribute is fairly normally distributed with a few spikes","1fef0f6c":"## Shape of the data","1554ca3a":"### Total Online Visits","2addf57d":">- If we look at the data we see that there is a group which prefers online interactions with their bank, they have a much higher credit limit and also have more credit cards (cluster - 2). \n>- The customers who prefer in-person interactions tend to have the mid-range of credit cards and credit limit (cluster - 0). \n>- The customers who contact via phonecall are in another segment, who have lowest credit limit and number of cards (cluster - 1).","b087f519":"Before starting clustering we'll remove the cluster column from the dataset.","0c040fa0":"## Missing Values","c03f356c":"### Assign cluster labels","3718c876":"### Cluster profile","92e40173":"### Credit Card Limit Bins","5b87e30f":"### Summary of Total Number of Credit Cards","5e9f19b8":"### Total Visits to the Bank","38b99866":"## PCA for Dimensionality Reduction","7e5d4560":"**Clearly, 3 clusters seem very reasonable for this case study**\n\n## Build the model with 3 centroids","8dcf7d3a":"**Appropriate value for k seems to be 3**","32c916a6":"> There appears to be 5 duplicates in the customer key","39c257cd":"### Analyzing the segments using Box Plot","e59008ec":"### Summary of Total Online Visits","84380dd0":"**Observations**\n\n- The cophenetic correlation is highest for average and centroid linkage method, but I will check with complete linkage as it has more distinct and separated clusters, and a cophenetic correlation of 0.89 (highest being 0.9).\n- 3 appears to be the appropriate number of clusters from the dendrogram for complete linkage as well.","c71dfa81":"# Actionable insights and Recommendations\n\nThere appears to be three distinct categories of customers:\n\n1. In-person users: prefer to handle bank transactions in person. They have the medium number of credit cards and the medium available credit. They are also the most active users.\n2. Phone users: prefer verbally handling transactions. They have fewest credit cards and lowest credit card limit.\n3. Online users: prefer digital transactions. They also have the most credit cards and the highest available credit. Least number of customers have this type of behaviour.\n\nThe customer preferences should be used to contact the customers. Online\/phone users will probably prefer email\/text notifications, while in-person users prefer mail notifications and upselling (when at the bank location). \n\nAlso, the phone and in-person customers should be reached out to promote online banking.","3fbbec5c":"> There are no missing values in the dataset","d915a856":"> All the attributes are integers","27719803":"## Finding the best numbr of centroids (K)\n\n#### Elbow Curve to get the right number of Clusters\n\nA fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The **Elbow Method** is one of the most popular methods to determine this optimal value of k.","e279fb2e":"## Column Statistinc","303de63b":"**Although there are only 5 dimensions, it'll be really cool to be able to visualize the clusters at 3 dimensional space without loosing much of the information. Let's use PCA to reduce the dimensions so that 90% of the variance in the data is explained.**","45dc8416":"### Summary of Total Number of Visits to the Bank","a7856b02":"# Hierarchical Clustering","9022ce2d":"## Univariate Analysis","e37f1735":"## Cluster Profiling","60a7bac4":"### Summary of Avergae Credit Limit"}}