{"cell_type":{"5e1198e5":"code","74c09ba0":"code","2e7894ba":"code","0e5bec9d":"code","1c739e28":"code","30794985":"code","14e4aa32":"code","de8d0932":"code","1804a69c":"code","e1c0cbbe":"code","a8cc0233":"code","5ae99a36":"code","b95d24bc":"code","779b7e38":"code","c16ddff1":"code","fe881f5d":"code","f30c4097":"code","3659bd1b":"code","c246211a":"code","5d149b8b":"code","11545e07":"code","0a2f29a7":"code","c85210e7":"code","99cb7c8d":"code","44486d67":"code","9a013002":"code","de9f018d":"code","ef829203":"code","4ea4cf90":"code","bba37666":"code","88438952":"code","0f350985":"code","fedd5340":"markdown","63fd1889":"markdown","ff10a4ed":"markdown","d1cb7536":"markdown","5d5fd90d":"markdown","5d32d277":"markdown","ca70bcb5":"markdown","13d6f93f":"markdown","f9b6b324":"markdown","ee47d4fc":"markdown","43dd322e":"markdown","ae9714b3":"markdown","075ace39":"markdown","7661bf7b":"markdown","1e4d580a":"markdown","f3592b57":"markdown"},"source":{"5e1198e5":"# IMPORT NECESSARY LIBRARIES\nimport warnings\nwarnings.filterwarnings('ignore')\nimport librosa\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport librosa.display\nfrom IPython.display import Audio\nimport numpy as np\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport IPython.display as ipd  # To play sound in the notebook\nimport os # interface with underlying OS that python is running on\nimport sys\nimport seaborn as sns","74c09ba0":"# path to the directory\nRAV = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/\"","2e7894ba":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '\/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","0e5bec9d":"plt.figure(figsize=(12, 5))\nplt.title('Count of Emotions', size=16)\nsns.countplot(RAV_df.labels, palette='cool')\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nplt.xticks(rotation=45)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","1c739e28":"# MALE NEUTRAL\nfname1=RAV+'Actor_01\/03-01-01-01-01-01-01.wav'\ndata, sr = librosa.load(fname1)\nipd.Audio(fname1) \n","30794985":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Male Neutral')\nplt.colorbar(format='%+2.0f dB');","14e4aa32":"# FEMALE NEUTRAL\nfname2=RAV+'Actor_14\/03-01-01-01-01-01-14.wav'\ndata, sr = librosa.load(fname2)\nipd.Audio(fname2) ","de8d0932":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Female Neutral')\nplt.colorbar(format='%+2.0f dB');","1804a69c":"# Pick a fearful track\nfname3 = RAV + 'Actor_14\/03-01-06-02-02-02-14.wav'  \ndata, sr = librosa.load(fname3)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(data, sr=sr)\nplt.title('Waveplot - Female Fearful')\n# Lets play the audio \nipd.Audio(fname3)","e1c0cbbe":"# Pick a happy track\nfname4 = RAV + 'Actor_14\/03-01-03-02-02-02-14.wav'  \ndata, sr = librosa.load(fname4)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(data, sr=sr)\nplt.title('Waveplot - Female Happy')\n\n# Lets play the audio \nipd.Audio(fname4)","a8cc0233":"# Gender - Female; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_08\/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()","5ae99a36":"# Gender - Male; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","b95d24bc":"# Gender - Female; Emotion - happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_12\/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Gender - Male; Emotion - happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_11\/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Plot the two audio waves together\nplt.figure(figsize=(16,10))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","779b7e38":"# NOISE\ndef noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n# STRETCH\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n# SHIFT\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n# PITCH\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","c16ddff1":"# Trying different functions above\npath = np.array(RAV_df['path'])[303]\ndata, sample_rate = librosa.load(path)","fe881f5d":"plt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(y=data, sr=sample_rate)\nAudio(path)","f30c4097":"x = noise(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","3659bd1b":"x = stretch(data)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","c246211a":"x = shift(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","5d149b8b":"x = pitch(data, sample_rate)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","11545e07":"def feat_ext(data):\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    return mfcc\n\ndef get_feat(path):\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    # normal data\n    res1 = feat_ext(data)\n    result = np.array(res1)\n    #data with noise\n    noise_data = noise(data)\n    res2 = feat_ext(noise_data)\n    result = np.vstack((result, res2))\n    #data with stretch and pitch\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sample_rate)\n    res3 = feat_ext(data_stretch_pitch)\n    result = np.vstack((result, res3))\n    return result","0a2f29a7":"RAV_df.head(2)","c85210e7":"X, Y = [], []\nfor path, emotion in zip(RAV_df['path'], RAV_df['labels']):\n    feature = get_feat(path)\n    for ele in feature:\n        X.append(ele)\n        Y.append(emotion)","99cb7c8d":"Features = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.to_csv('features.csv', index=False)\nFeatures.head()","44486d67":"csv = '..\/input\/speech-emotion-dataset\/features.csv'\nFeatures = pd.read_csv(csv)","9a013002":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=24, shuffle=True)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\nx_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","de9f018d":"model=Sequential()\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=16, activation='softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","ef829203":"rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\nhistory=model.fit(x_train, y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test), callbacks=[rlrp])","4ea4cf90":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","bba37666":"epochs = [i for i in range(100)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","88438952":"pred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)\n\ndf = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","0f350985":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","fedd5340":"**AUDIO WITH NOISE**","63fd1889":"**AUDIO WITH PITCH**","ff10a4ed":"**AUDIO WITH SHIFT**","d1cb7536":"**DATA SOURCE USED**\n\nWe have used the RAVDESS dataset in this project.It is one of the more common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders.\nHere's the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nHere's an example of an audio filename. 02-01-06-01-02-01-12.mp4","5d5fd90d":"**PROBLEM STATEMENT**\n\nThe purpose is to recognize the emotion and affective state of the speaker from his\/her speech signal. ","5d32d277":"For the same sentence being uttered, there is a clear distint difference between male and female in that females tends to have a higher pitch.","ca70bcb5":"For the happy track it actually felt like it was a fearful tone at the start, up until the end. We had to play it 3 or 4 times to finally be convienced that it is indeed a happy sound. Looking at the wave plot between the 2 files, we noticed the only difference is the amplitute wherein this happy track has a higher amplituted at various points.\n\n\nNext, we compare the MFCC feature for male and female angry audio clips","13d6f93f":"After listening to all augmented audio, it's decided to use noise, stretch and pitch for augmenting data.","f9b6b324":"**Feature Extraction**\n\nThe data provided in the form of audio cannot be understood by the models directly so we need to convert them into an understandable format for which feature extraction is to be done.","ee47d4fc":"# **SPEECH EMOTION RECOGNITION**\n\n\n**TABLE OF CONTENTS**\n* PROBLEM STATEMENT\n* DATA SOURCE USED\n* EXPLORATORY DATA ANALYSIS (EDA)","43dd322e":"**EDA**\n\nThe key features of the audio data are namely, MFCC (Mel Frequency Cepstral Coefficients), Mel Spectrogram and Chroma.\n\n* MFCC (Mel Frequency Cepstral Coefficients)- \nMFCC is taken on a Mel scale which is a scale that relates the perceived frequency of a tone to the actual measured frequency. It scales the frequency in order to match more closely what the human ear can hear. The envelope of the temporal power spectrum of the speech signal is representative of the vocal tract and MFCC accurately represents this envelope.\n\n\n* Mel Spectrogram- \nA Fast Fourier Transform is computed on overlapping windowed segments of the signal, and we get what is called the spectrogram. This is just a spectrogram that depicts amplitude which is mapped on a Mel scale.\n\n* Chroma- \nA Chroma vector is typically a 12-element feature vector indicating how much energy of each pitch class is present in the signal in a standard chromatic scale.\n\nFor the EDA we have used MFCC and Mel Spectogram","ae9714b3":"Next, we compare the waveplots of happy and fearful tracks","075ace39":"Now, we will compare mel spectograms of male and female neutral audio clips.","7661bf7b":"**NORMAL AUDIO**","1e4d580a":"**AUDIO WITH STRETCH**","f3592b57":"**Data Augmentation**\n* Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set.\n* To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed.\n* The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\n* In order to this to work adding the perturbations must conserve the same label as the original training sample."}}