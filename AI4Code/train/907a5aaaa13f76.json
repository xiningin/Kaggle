{"cell_type":{"7c855373":"code","3c62c700":"code","d4fd2f5b":"code","1d58aacb":"code","47504784":"code","c6b3180f":"code","a738e1ea":"code","0fd92767":"code","bf84f50e":"code","617f3b97":"markdown","76087d97":"markdown","5308178c":"markdown","2e0abd8f":"markdown","e8da6595":"markdown","af192231":"markdown","405bfaff":"markdown","06b1ab66":"markdown","49fbe1d1":"markdown","cab45dd8":"markdown"},"source":{"7c855373":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom __future__ import division, print_function\n# \u043e\u0442\u043a\u043b\u044e\u0447\u0438\u043c \u0432\u0441\u044f\u043a\u0438\u0435 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f Anaconda\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\nfrom sklearn.model_selection import validation_curve\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3c62c700":"data = pd.read_csv('..\/input\/mlcourse\/telecom_churn.csv').drop('State', axis=1)\ndata['International plan'] = data['International plan'].map({'Yes': 1, 'No': 0})\ndata['Voice mail plan'] = data['Voice mail plan'].map({'Yes': 1, 'No': 0})\n\ny = data['Churn'].astype('int').values\nX = data.drop('Churn', axis=1).values","d4fd2f5b":"data.head()","1d58aacb":"alphas = np.logspace(-2, 0, 20)\nsgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\nlogit_pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=2)), \n                       ('sgd_logit', sgd_logit)])\nval_train, val_test = validation_curve(logit_pipe, X, y,\n                                       'sgd_logit__alpha', alphas, cv=5,\n                                       scoring='roc_auc')","47504784":"def plot_with_err(x, data, **kwargs):\n    mu, std = data.mean(1), data.std(1)\n    lines = plt.plot(x, mu, '-', **kwargs)\n    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n                     facecolor=lines[0].get_color(), alpha=0.2)\n\nplot_with_err(alphas, val_train, label='training scores')\nplot_with_err(alphas, val_test, label='validation scores')\nplt.xlabel(r'$\\alpha$'); plt.ylabel('ROC AUC')\nplt.legend();","c6b3180f":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(degree=2, alpha=0.01):\n    train_sizes = np.linspace(0.05, 1, 20)\n    logit_pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=degree)), \n                           ('sgd_logit', SGDClassifier(n_jobs=-1, random_state=17, alpha=alpha))])\n    N_train, val_train, val_test = learning_curve(logit_pipe,\n                                                  X, y, train_sizes=train_sizes, cv=5,\n                                                  scoring='roc_auc')\n    plot_with_err(N_train, val_train, label='training scores')\n    plot_with_err(N_train, val_test, label='validation scores')\n    plt.xlabel('Training Set Size'); plt.ylabel('AUC')\n    plt.legend()","a738e1ea":"plot_learning_curve(degree=2, alpha=10)","0fd92767":"plot_learning_curve(degree=2, alpha=0.05)","bf84f50e":"plot_learning_curve(degree=2, alpha=1e-4)","617f3b97":"\n\nNow, what if we make the model even more complex by setting alpha = 10-4?\n\nOverfitting is seen - AUC decreases on both the training and the validation sets.\n","76087d97":"\n\nConstructing these curves can help understand which way to go and how to properly adjust the complexity of the model for new data.\n\nConclusions on the learning and validation curves:\n\n    Error on the training set says nothing about the quality of the model by itself\n    Cross-validation error shows how well the model fits the data (the existing trend in the data) while retaining the ability to generalize to new data\n    Validation curve is a graph showing the results on training and validation sets depending on the complexity of the model:\n        if the two curves are close to each other and both errors are large, it is a sign of underfitting\n        if the two curves are far from each other, it is a sign of overfitting\n    Learning Curve is a graph showing the results on training and validation sets depending on the number of observations:\n        if the curves converge, adding new data won't help, and it is necessary to change the complexity of the model\n        if the curves have not converged, adding new data can improve the result\n\n","5308178c":"\n\nNow that we have an idea of model validation, cross-validation, and regularization. Let's consider the bigger question:\n\nWhat to do if the quality of the model is dissatisfying?\n\n    Should we make the model more complicated or more simple?\n    Should we add more features?\n    Do we simply need more data for training?\n\nThe answers to these questions are not obvious. In particular, sometimes a more complex model can lead to a deterioration in performance. Other times, adding new observations will not bring noticeable changes. In fact, the ability to make the right decision and choose the right method to improve the model distinguishes a good professional from a bad one.\n\nWe will work our data on customer churn of telecom operator.\n","2e0abd8f":"# If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - That will keep me motivated to update it on a regular basis :-)","e8da6595":"\n\nThe trend is quite visible and is very common.\n\n    For simple models, training and validation errors are close and large. This suggests that the model underfitted, meaning it does not have a sufficient number of parameters.\n\n    For highly sophisticated models, training and validation errors differ significantly. This can be explained by overfitting. When there are too many parameters or regularization is not strict enough, the algorithm can be \"distracted\" by the noise in the data and lose track of the overall trend.\n\nHow much data is needed?\n\nThe more data the model uses, the better. But how do we understand whether new data will helpful in any given situation? For example, is it rational to spend $N$ for assessors to double the dataset?\n\nSince the new data can be unavailable, it is reasonable to vary the size of the training set and see how the quality of the solution depends on the amount of training data. This is how we get learning curves.\n\nThe idea is simple: we display the error as a function of the number of examples used in training. The parameters of the model are fixed in advance.\n","af192231":"Let's see what we get for the linear model. We will set the regularization coefficient to be quite large.","405bfaff":"\n\nA typical situation: for a small amounts of data, errors between training and cross-validation sets are quite different, indicating overfitting. For that same model but with a large amount of data, errors \"converge\", indicating underfitting.\n\nIf we add more data, error on the training set will not grow. On the other hand, the error on the test data will not be reduced.\n\nSo, we see that the errors \"converged\", and the addition of new data will not help. Actually this case is the most interesting for business. It is possible that we increase the size of the dataset by 10x, but, without changing the complexity of the model, this additional data may not help. Therefore the strategy of \"set once, then use 10 times\" might not work.\n\nWhat happens if we reduce the regularization coefficient to 0.05?\n\nWe see a good trend - the curves gradually converge, and if we move farther to the right i.e. add more data to the model, we can improve the quality on the validation set even more.\n","06b1ab66":"We will train logistic regression with stochastic gradient descent. Later in the course, we will have a separate article on this topic.","49fbe1d1":"# If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - That will keep me motivated to update it on a regular basis :-)","cab45dd8":"As a first step, we will construct validation curves showing how the quality (ROC-AUC) on training and test sets varies with the regularization parameter."}}