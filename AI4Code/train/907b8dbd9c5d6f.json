{"cell_type":{"7cc4e6ef":"code","7313687a":"code","62ab2f8a":"code","483079bb":"code","29a6d390":"code","10d72b90":"code","f477867e":"code","e6ededcd":"code","115b8b06":"code","02b792bf":"code","d80286c6":"code","d3ec8c27":"code","47c50c4b":"code","c602489e":"code","050b1fe8":"code","a5b813bd":"code","6fdc9218":"code","746b4ceb":"code","8b06b983":"code","5415b9cb":"code","c7b4c2b7":"code","c0e1ca09":"code","1eee1a36":"code","f78969f8":"code","a85d4f51":"code","f71549d3":"code","f359b925":"code","c4dcb4a5":"code","7e772c3e":"code","3a64d12f":"code","40f9a9c1":"code","0c4063f5":"code","0ba0c6e9":"markdown","6a103b1c":"markdown","bbe88d34":"markdown","8c13ee40":"markdown","3f01eb77":"markdown","af6b536e":"markdown","b1420351":"markdown","cca327bd":"markdown"},"source":{"7cc4e6ef":"import os\nimport time\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom collections import namedtuple\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\ntf.config.optimizer.set_jit(True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport warnings\nwarnings.filterwarnings (\"ignore\")\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","7313687a":"import torch\nprint(torch.cuda.is_available())","62ab2f8a":"BETA = 0.7\nALPHA = 0.6 # for simple mean subtracted features\n\nfeat_cols = [f'feature_{i}' for i in range(130)]\n\ntarget_cols = ['action_1', 'action_2', 'action_3','action', 'action_4']\n\ntarget_cols_volatile = ['action_3','action', 'action_4']\n\nf_mean = np.load('..\/input\/janest-mlp-models\/f_mean_after_85_include_zero_weight.npy').reshape(1,-1)\nspike_val = np.load('..\/input\/jane-street-train-data-final\/spike_common_vals_42.npy').reshape(1,-1)","483079bb":"# features for spike net\nfeat_spike_index = [1, 2, 3, 4, 5, 6, 10, 14, 16, 69, 70, 71, 73, 74, 75, 76, 79, 80, 81, 82, 85,\n                    86, 87, 88, 91, 92, 93, 94, 97, 98, 99, 100, 103, 104, 105, 106, 109, 111, 112, 115, 117, 118]\n\nfeatures_spike = [f'feature_{i}' for i in feat_spike_index]\nfeat_cols = [f'feature_{i}' for i in range(130)]\n\nresp_cols  = ['resp_1', 'resp_2', 'resp_3','resp', 'resp_4', ]  \nresp_cols_vol  = ['resp_3','resp', 'resp_4', ] \ncat_cols = [f+'_c' for f in features_spike]","29a6d390":"##### Making features for baseline torch models\nall_feat_cols = [col for col in feat_cols]\nall_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])","10d72b90":"# resnet for all five resp\nfeatures_2_index = [0, 1, 2, 3, 4, 5, 6, 15, 16, 25, 26, 35, \n             36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, \n             49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 63, 64, 65, \n             66, 67, 68, 69, 70, 71, 76, 77, 82, 83, 88, 89, 94, \n             95, 100, 101, 106, 107, 112, 113, 118, 119, 128, 129]\n\nfeatures_1_index = [0] + list(set(range(130)).difference(features_2_index))\n\nfeatures_1 = [f'feature_{i}' for i in features_1_index]\n\nfeatures_2 = [f'feature_{i}' for i in features_2_index]\n","f477867e":"# resnet for volatile days\nfeatures_1_index_v = [0,\n                   7, 8, 17, 18, 27, 28, 55, 72, 78, 84, 90, 96, 102, 108, 114, 120, 121,\n                   11, 12, 21, 22, 31, 32, 57, 74, 80, 86, 92, 98, 104, 110, 116, 124, 125] \n                # resp_1 resp_2 feat\n    \nfeatures_2_index_v = [0] + list(set(range(130)).difference(features_1_index_v))\n\nfeatures_1_v = [f'feature_{i}' for i in features_1_index_v]\n\nfeatures_2_v = [f'feature_{i}' for i in features_2_index_v]","e6ededcd":"##### Model&Data fnc\nclass ResMLP(nn.Module):\n    def __init__(self, hidden_size=256, \n                       output_size=len(target_cols), \n                       input_size=len(all_feat_cols),\n                       dropout_rate=0.2):\n        super(ResMLP, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(input_size)\n        self.dropout0 = nn.Dropout(0.2)\n\n        self.dense1 = nn.Linear(input_size, hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+input_size, hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, output_size)\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x\n    \n    \nclass SpikeNet(nn.Module):\n    def __init__(self, hidden_size=256,\n                 cat_dim=len(cat_cols),\n                 output_size=len(resp_cols),\n                 input_size=len(feat_cols),\n                 dropout_rate=0.2,\n                 alpha=ALPHA):\n        super(SpikeNet, self).__init__()\n        # self.embed = nn.Embedding(cat_dim, 2)\n        self.embed = nn.Linear(cat_dim, int(cat_dim*alpha))\n        self.emb_dropout = nn.Dropout(0.1)\n\n        self.batch_norm0 = nn.BatchNorm1d(input_size+int(cat_dim*alpha))\n        self.dropout0 = nn.Dropout(0.1)\n\n        self.dense1 = nn.Linear(input_size+int(cat_dim*alpha), hidden_size)\n        # nn.init.kaiming_normal_(self.dense1.weight.data)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(\n            hidden_size+input_size+int(cat_dim*alpha), hidden_size)\n        # nn.init.kaiming_normal_(self.dense2.weight.data)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        # nn.init.kaiming_normal_(self.dense3.weight.data)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, output_size)\n        # nn.init.kaiming_normal_(self.dense4.weight.data)\n\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n\n    def forward(self, x, x_cat):\n        #\n        x_cat = self.embed(x_cat)\n        # x_cat = self.emb_dropout(x_cat)\n        x = torch.cat([x, x_cat], dim=1)\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x = self.dense4(x)\n\n        return x","115b8b06":"torch_weights_1 = ['..\/input\/jane-street-train-data-final\/emb_fold_0_util_1351_auc_0.5536.pth',\n                '..\/input\/jane-street-train-data-final\/emb_fold_1_util_1232_auc_0.5539.pth',\n#                 '..\/input\/jane-street-train-data-final\/emb_fold_2_util_266_auc_0.5441.pth'\n                  ]\n\nN_TORCH = len(torch_weights_1)\n\ntorch_model_1 = []\nfor _fold in range(N_TORCH):\n    torch.cuda.empty_cache()\n    model = SpikeNet()\n    model.to(device)\n    model_weights = torch_weights_1[_fold]\n    try:\n        model.load_state_dict(torch.load(model_weights))\n    except:\n        model.load_state_dict(torch.load(model_weights, map_location=torch.device('cpu')))\n    model.eval()\n    torch_model_1.append(model)\n    print(f\"spike net {_fold} loaded.\")\n    \ntorch_model_1 = torch_model_1[:2]","02b792bf":"torch_weights_2 = ['..\/input\/jane-street-train-data-final\/emb_volatile_fold_0_util_1445_auc_0.5550.pth',\n                  '..\/input\/jane-street-train-data-final\/emb_volatile_fold_1_util_1225_auc_0.5557.pth',\n                  '..\/input\/jane-street-train-data-final\/emb_volatile_fold_2_util_240_auc_0.5455.pth']\n\nN_TORCH_2 = len(torch_weights_2)\n\ntorch_model_2 = []\nfor _fold in range(N_TORCH_2):\n    torch.cuda.empty_cache()\n    model = SpikeNet()\n    model.to(device)\n    model_weights = torch_weights_2[_fold]\n    try:\n        model.load_state_dict(torch.load(model_weights))\n    except:\n        model.load_state_dict(torch.load(model_weights, map_location=torch.device('cpu')))\n    model.eval()\n    torch_model_2.append(model)\n    print(f\"Volatile spike net {_fold} loaded.\")","d80286c6":"\ntorch_weights_3 = ['..\/input\/jane-street-train-data-final\/pt_volatile_0_util_1424_auc_0.5520.pth',\n                  '..\/input\/jane-street-train-data-final\/pt_volatile_1_util_1137_auc_0.5470.pth',\n#                   '..\/input\/jane-street-train-data-final\/pt_volatile_2_util_322_auc_0.5444.pth',\n                  ]\n\nN_TORCH_3 = len(torch_weights_3)\n\ntorch_model_3 = []\nfor _fold in range(N_TORCH_3):\n    torch.cuda.empty_cache()\n    model = ResMLP()\n    model.to(device)\n    model_weights = torch_weights_3[_fold]\n    try:\n        model.load_state_dict(torch.load(model_weights))\n    except:\n        model.load_state_dict(torch.load(model_weights, map_location=torch.device('cpu')))\n    model.eval()\n    torch_model_3.append(model)\n    print(f\"Volatile torch {_fold} loaded.\")\n    \ntorch_model_3 = torch_model_3[:2]","d3ec8c27":"# enable mish\nfrom tensorflow.keras import backend as K\n\nclass Mish(tf.keras.layers.Layer):\n\n    def __init__(self, **kwargs):\n        super(Mish, self).__init__(**kwargs)\n        self.supports_masking = True\n\n    def call(self, inputs):\n        return inputs * K.tanh(K.softplus(inputs))\n\n    def get_config(self):\n        base_config = super(Mish, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\ndef mish(x):\n\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n\ntf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n\ndef create_resnet_reg(n_features, n_features_2, n_labels, hidden_size, \n                  learning_rate=1e-3, label_smoothing = 0.005):    \n    input_1 = tf.keras.layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = tf.keras.layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(hidden_size, activation=\"mish\"), \n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(hidden_size\/\/2, activation = \"mish\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = tf.keras.layers.Concatenate()([input_2, input_3])\n\n    head_2 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(hidden_size, \"mish\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(hidden_size, \"mish\"),\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_concat = tf.keras.layers.Concatenate()([input_3_concat, input_4]) \n\n    head_3 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(hidden_size, kernel_initializer='lecun_normal', activation='mish'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(hidden_size\/\/2, kernel_initializer='lecun_normal', activation='mish'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_concat)\n\n\n    model = tf.keras.models.Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate), \n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n                  metrics=['AUC'])\n    \n    return model\n\n\ndef create_resnet(n_features, n_features_2, n_labels, hidden_size, learning_rate=1e-3, \n                  label_smoothing = 0.005):    \n    input_1 = tf.keras.layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = tf.keras.layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(hidden_size, activation=\"mish\"), \n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(hidden_size\/\/2, activation = \"mish\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = tf.keras.layers.Concatenate()([input_2, input_3])\n\n    head_2 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(hidden_size, \"mish\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(hidden_size\/\/2, \"mish\"),\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_concat = tf.keras.layers.Concatenate()([input_3_concat, input_4]) \n\n    head_3 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(hidden_size, kernel_initializer='lecun_normal', activation='mish'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_concat)\n\n\n    model = tf.keras.models.Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate), \n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n                  metrics=['AUC'])\n    \n    return model","47c50c4b":"tf_weights_0 = ['..\/input\/jane-street-train-data-final\/resnet_reg_fold_0_seed_1127802.h5',\n                  '..\/input\/jane-street-train-data-final\/resnet_reg_fold_1_seed_157157.h5',\n                  '..\/input\/jane-street-train-data-final\/resnet_reg_fold_2_res_seed_97275.h5']\n\nn_tf_0 = len(tf_weights_0)\n\ntf_model_0 = []\nfor _fold in range(n_tf_0):\n    tf.keras.backend.clear_session()\n    model = create_resnet_reg(len(features_1), len(features_2), len(resp_cols), hidden_size=256, label_smoothing=5e-03)\n    model_weights = tf_weights_0[_fold]\n    model.load_weights(model_weights)\n    model.call = tf.function(model.call, experimental_relax_shapes=True)\n    tf_model_0.append(model)\n    print(f\"regular tf resnet {_fold} loaded.\")\n\ntf_model_0 = tf_model_0[:2]","c602489e":"tf_weights_1 = ['..\/input\/jane-street-train-data-final\/resnet_volatile_fold_0_seed_1127802.h5',\n                '..\/input\/jane-street-train-data-final\/resnet_volatile_fold_1_seed_123835.h5',\n                '..\/input\/jane-street-train-data-final\/resnet_volatile_fold_2_seed_1127802.h5']\n\nn_tf_1 = len(tf_weights_1)\n\ntf_model_1 = []\nfor _fold in range(n_tf_1):\n    tf.keras.backend.clear_session()\n    model = create_resnet(len(features_1_v), len(features_2_v), len(resp_cols_vol), hidden_size=256, label_smoothing=5e-03)\n    model_weights = tf_weights_1[_fold]\n    model.load_weights(model_weights)\n    model.call = tf.function(model.call, experimental_relax_shapes=True)\n    tf_model_1.append(model)\n    print(f\"volatile tf resnet {_fold} loaded.\")","050b1fe8":"def create_autoencoder(input_dim,output_dim,noise=0.05):\n    i = Input(input_dim)\n    encoded = BatchNormalization()(i)\n    encoded = GaussianNoise(noise)(encoded)\n    encoded = Dense(64,activation='relu')(encoded)\n    decoded = Dropout(0.2)(encoded)\n    decoded = Dense(input_dim,name='decoded')(decoded)\n    x = Dense(32,activation='relu')(decoded)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n    \n    encoder = Model(inputs=i,outputs=encoded)\n    autoencoder = Model(inputs=i,outputs=[decoded,x])\n    \n    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse',\n                                                    'label_output':'binary_crossentropy'})\n    return autoencoder, encoder\n\ndef create_model(hp,input_dim,output_dim,encoder):\n    inputs = Input(input_dim)\n    \n    x = encoder(inputs)\n    x = Concatenate()([x,inputs]) #use both raw and encoded features\n    x = BatchNormalization()(x)\n    x = Dropout(hp.Float('init_dropout',0.0,0.4))(x)\n    \n    for i in range(hp.Int('num_layers',1,4)):\n        x = Dense(hp.Int(f'num_units_{i}',64,256))(x)\n        x = BatchNormalization()(x)\n        x = Lambda(tf.keras.activations.swish)(x)\n        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.4))(x)\n        \n    x = Dense(output_dim,activation='sigmoid')(x)\n    model = Model(inputs=inputs,outputs=x)\n    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),\n                  loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),\n                  metrics=[tf.keras.metrics.AUC(name = 'auc')])\n    return model","a5b813bd":"encoder_file = '..\/input\/jane-street-train-data-final\/encoder_reg.hdf5'\nhp_file = f'..\/input\/jane-street-train-data-final\/hp_ae_reg.pkl'\n\ntf.keras.backend.clear_session()\n_, encoder = create_autoencoder(len(feat_cols),len(target_cols),noise=0.1)\n\nencoder.load_weights(encoder_file)\nencoder.trainable = False\n\nmodel_fn = lambda hp: create_model(hp,len(feat_cols),len(target_cols), encoder)\ntf_models_2 = []\n\nhp = pd.read_pickle(hp_file)\nfor _fold in range(3):\n    tf.keras.backend.clear_session()\n    model = model_fn(hp)\n    model.load_weights(f'..\/input\/jane-street-train-data-final\/ae_reg_fold_{_fold}.hdf5')\n    model.call = tf.function(model.call, experimental_relax_shapes=True)\n    tf_models_2.append(model)\n    print(f\"Regular tf ae model {_fold} loaded\")\n    \ntf_models_2 = tf_models_2[:2]","6fdc9218":"tf.keras.backend.clear_session()\n_, encoder_vol = create_autoencoder(len(feat_cols),len(target_cols_volatile),noise=0.1)\n\nencoder_vol.load_weights('..\/input\/jane-street-train-data-final\/encoder_volatile.hdf5')\nencoder_vol.trainable = False\n\nmodel_fn_vol = lambda hp: create_model(hp,len(feat_cols),len(target_cols_volatile), encoder_vol)\ntf_models_3 = []\n\nhp = pd.read_pickle(f'..\/input\/jane-street-train-data-final\/hp_ae_volatile.pkl')\nfor _fold in range(3):\n    tf.keras.backend.clear_session()\n    model = model_fn_vol(hp)\n    model.load_weights(f'..\/input\/jane-street-train-data-final\/ae_volatile_fold_{_fold}.hdf5')\n    model.call = tf.function(model.call, experimental_relax_shapes=True)\n    tf_models_3.append(model)\n    print(f\"Volatile tf ae model {_fold} loaded\")\n    \ntf_models_3 = tf_models_3[:2]","746b4ceb":"## common for regular and volatile\nprint(\"Number of baseline models for all days\")\nprint(len(torch_model_1))\nprint(len(tf_model_0))\n\n## volatile\nprint(\"\\nNumber of models for volatile days\")\nprint(len(torch_model_2))\nprint(len(torch_model_3))\nprint(len(tf_model_1))\nprint(len(tf_models_3))\n\n## only regular day\nprint(\"\\nNumber of models for regular days\")\nprint(len(tf_models_2))","8b06b983":"def median_avg(predictions, beta=BETA):\n    '''\n    predictions should be of a vector shape (..., n_models)\n    beta: if beta is 0.5, then the middle 50% will be averaged\n    '''\n    sorted_predictions = np.sort(predictions)\n    n_model = len(sorted_predictions)\n    mid_point = n_model\/\/2+1\n    n_avg = int(n_model*beta)\n\n    to_avg = sorted_predictions[mid_point-n_avg\/\/2-1:mid_point+n_avg\/\/2]\n    \n    return to_avg.mean()","5415b9cb":"class RunningPDA:\n    '''\n    https:\/\/www.kaggle.com\/lucasmorin\/running-algos-fe-for-fast-inference?scriptVersionId=50754012\n    Modified by Ethan to add slope prediction using feature 64\n    inspired by Carl: https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance\n    '''\n    def __init__(self, past_mean=0, start=1000, end=2500, slope=0.00116):\n        self.day = -1\n        self.past_mean = past_mean # past day mean, initialized as the mean\n        self.cum_sum = 0\n        self.day_instances = 0 # current day instances\n        self.past_value = past_mean # the previous row's value, initialized as the mean\n        self.past_instances = 0 # instances in the past day\n        \n        self.start = start\n        self.end = end\n        self.slope = slope\n        self.start_value = None\n        self.end_value = None\n\n    def clear(self):\n        self.n = 0\n        self.windows.clear()\n\n    def push(self, x, date):\n        x = fast_fillna(x, self.past_value)\n        self.past_value = x\n        \n        # change of day\n        if date > self.day:\n            self.day = date\n            if self.day_instances > 0:\n                self.past_mean = self.cum_sum\/self.day_instances\n            self.past_instances = self.day_instances\n            self.day_instances = 1\n            self.cum_sum = x\n            \n            self.start_value, self.end_value = None, None\n            \n        else:\n            self.day_instances += 1\n            self.cum_sum += x\n        \n        if self.day_instances == self.start:\n            self.start_value = x[:, 64]\n        if self.day_instances == self.end:\n            self.end_value = x[:, 64]\n\n    def get_mean(self):\n        return self.cum_sum\/self.day_instances\n\n    def get_past_mean(self):\n        return self.past_mean\n\n    def get_past_trade(self):\n        return self.past_instances\n    \n    def predict_today_busy(self):\n        if self.start_value is None or self.end_value is None:\n            return False\n        return (self.end_value - self.start_value) \/ (self.end - self.start) < self.slope\n    \n    \ndef fast_fillna(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","c7b4c2b7":"th = 0.50\nf = median_avg\nHIGH_VOL = 5000\n\nGPU = torch.cuda.is_available()\n\npdm = RunningPDA(past_mean=f_mean, start=1000, end=2500, slope=0.00144)","c0e1ca09":"import janestreet\nenv = janestreet.make_env()\nenv_iter = env.iter_test()","1eee1a36":"########## GPU\npbar = tqdm(total=15219)\nfor (test_df, pred_df) in env_iter:\n\n    date = test_df['date'].values\n    x_tt = test_df[feat_cols].values\n\n    pdm.push(x_tt, date)\n    past_day_mean = pdm.get_past_mean()\n    past_day_vol = pdm.get_past_trade()\n\n    if test_df['weight'].values[0] > 0:\n\n        if np.isnan(x_tt.sum()):\n            x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * past_day_mean\n        x_cat = (x_tt[:,feat_spike_index] - spike_val).astype(np.int32)\n\n\n        ###### torch_pred_1: spikenet, 2 or 3 models\n        torch_preds_1 = [model(torch.tensor(x_tt,dtype=torch.float).to(device), \n                               torch.tensor(x_cat,dtype=torch.float).to(device))\\\n                           .sigmoid().detach().cpu().numpy()\\\n                           for model in torch_model_1]\n        torch_pred_1 = np.mean(torch_preds_1, axis=0)\n\n        ### tf resnet for regular days\n        x_tt_1 = x_tt[:,features_1_index]\n        x_tt_2 = x_tt[:,features_2_index]\n        tf_preds_0 = [model([x_tt_1, x_tt_2], training = False).numpy() for model in tf_model_0]\n        tf_pred_0 = np.mean(tf_preds_0, axis=0)\n\n        if past_day_vol > HIGH_VOL or pdm.predict_today_busy():\n            ####### spike net for volatile days\n            torch_preds_2 = [model(torch.tensor(x_tt,dtype=torch.float).to(device), \n                                torch.tensor(x_cat,dtype=torch.float).to(device))\\\n                             .sigmoid().detach().cpu().numpy()\\\n                           for model in torch_model_2]\n            torch_pred_2 = np.median(torch_preds_2, axis=0)\n\n            ####### vanilla torch for volatile days\n            cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n            cross_1_2 = x_tt[:, 1] \/ (x_tt[:, 2] + 1e-5)\n            feature_inp = np.c_[x_tt, cross_41_42_43.reshape(1, 1), cross_1_2.reshape(1, 1)]\n\n            torch_preds_3 = [model(torch.tensor(feature_inp,dtype=torch.float).to(device))\\\n                        .sigmoid().detach().cpu().numpy() for model in torch_model_3]\n            torch_pred_3 = np.mean(torch_preds_3, axis=0)\n\n            ### resnet for volatile days\n            x_tt_1_v = x_tt[:,features_1_index_v]\n            x_tt_2_v = x_tt[:,features_2_index_v]\n            tf_preds_1 = [model([x_tt_1_v, x_tt_2_v], training = False).numpy() for model in tf_model_1]\n            tf_pred_1 = np.median(tf_preds_1, axis=0)\n\n            ## ae+mlp model volatile days\n            tf_preds_3 = [model(x_tt, training = False).numpy() for model in tf_models_3]\n            tf_pred_3 = np.mean(tf_preds_3, axis=0)\n\n            #### concat blending\n            pred = np.c_[torch_pred_1, torch_pred_2, torch_pred_3, tf_pred_0, tf_pred_1, tf_pred_3].squeeze()\n            pred = f(pred, beta=0.6)\n        else:\n\n            # tf_preds for ae+mlp model\n            tf_preds_2 = [model(x_tt, training = False).numpy() for model in tf_models_2]\n            tf_pred_2 = np.mean(tf_preds_2, axis=0)\n\n            pred = np.c_[torch_pred_1, tf_pred_0, tf_pred_2].squeeze()\n            pred = f(pred)\n\n        pred_df.action.values[0] = int(pred >= th)\n    else:\n        pred_df.action.values[0] = 0\n\n    env.predict(pred_df)\n    pbar.update()\npbar.close()","f78969f8":"####### CPU\n\n# pbar = tqdm(total=15219)\n# for (test_df, pred_df) in env_iter:\n\n#     date = test_df['date'].values\n#     x_tt = test_df[feat_cols].values\n\n#     pdm.push(x_tt, date)\n#     past_day_mean = pdm.get_past_mean()\n#     past_day_vol = pdm.get_past_trade()\n\n#     if test_df['weight'].values[0] > 0:\n\n#         if np.isnan(x_tt.sum()):\n#             x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * past_day_mean\n#         x_cat = (x_tt[:,feat_spike_index] - spike_val).astype(np.int32)\n\n\n#         ###### torch_pred_1: spikenet, 2 or 3 models\n#         torch_preds_1 = [model(torch.tensor(x_tt,dtype=torch.float), \n#                                torch.tensor(x_cat,dtype=torch.float)).sigmoid().detach().numpy()\\\n#                            for model in torch_model_1]\n#         torch_pred_1 = np.mean(torch_preds_1, axis=0)\n\n#         ### tf resnet for regular days\n#         x_tt_1 = x_tt[:,features_1_index]\n#         x_tt_2 = x_tt[:,features_2_index]\n#         tf_preds_0 = [model([x_tt_1, x_tt_2], training = False).numpy() for model in tf_model_0]\n#         tf_pred_0 = np.mean(tf_preds_0, axis=0)\n\n#         if past_day_vol > HIGH_VOL or pdm.predict_today_busy():\n#             ####### spike net for volatile days\n#             torch_preds_2 = [model(torch.tensor(x_tt,dtype=torch.float), \n#                                 torch.tensor(x_cat,dtype=torch.float)).sigmoid().detach().numpy()\\\n#                            for model in torch_model_2]\n#             torch_pred_2 = np.median(torch_preds_2, axis=0)\n\n#             ####### vanilla torch for volatile days\n#             cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n#             cross_1_2 = x_tt[:, 1] \/ (x_tt[:, 2] + 1e-5)\n#             feature_inp = np.c_[x_tt, cross_41_42_43.reshape(1, 1), cross_1_2.reshape(1, 1)]\n\n#             torch_preds_3 = [model(torch.tensor(feature_inp,dtype=torch.float))\\\n#                         .sigmoid().detach().numpy() for model in torch_model_3]\n#             torch_pred_3 = np.mean(torch_preds_3, axis=0)\n\n#             ### resnet for volatile days\n#             x_tt_1_v = x_tt[:,features_1_index_v]\n#             x_tt_2_v = x_tt[:,features_2_index_v]\n#             tf_preds_1 = [model([x_tt_1_v, x_tt_2_v], training = False).numpy() for model in tf_model_1]\n#             tf_pred_1 = np.median(tf_preds_1, axis=0)\n\n#             ## ae+mlp model volatile days\n#             tf_preds_3 = [model(x_tt, training = False).numpy() for model in tf_models_3]\n#             tf_pred_3 = np.mean(tf_preds_3, axis=0)\n\n#             #### concat blending\n#             pred = np.c_[torch_pred_1, torch_pred_2, torch_pred_3, tf_pred_0, tf_pred_1, tf_pred_3].squeeze()\n#             pred = f(pred, beta=0.6)\n#         else:\n\n#             # tf_preds for ae+mlp model\n#             tf_preds_2 = [model(x_tt, training = False).numpy() for model in tf_models_2]\n#             tf_pred_2 = np.mean(tf_preds_2, axis=0)\n\n#             pred = np.c_[torch_pred_1, tf_pred_0, tf_pred_2].squeeze()\n#             pred = f(pred)\n\n#         pred_df.action.values[0] = int(pred >= th)\n#     else:\n#         pred_df.action.values[0] = 0\n\n#     env.predict(pred_df)\n#     pbar.update()\n# pbar.close()","a85d4f51":"torch_pred_1","f71549d3":"tf_pred_0","f359b925":"torch_pred_2 # should be in a busy day (day 2 in example test)","c4dcb4a5":"torch_pred_3 # should be in a busy day (day 2 in example test)","7e772c3e":"tf_pred_1 # should be in a busy day (day 2 in example test)","3a64d12f":"tf_pred_3 # should be in a busy day (day 2 in example test)","40f9a9c1":"np.c_[torch_pred_1, tf_pred_0, tf_pred_2].squeeze() # regular day ensemble","0c4063f5":"np.c_[torch_pred_1, torch_pred_2, torch_pred_3, tf_pred_0, tf_pred_1, tf_pred_3].squeeze() # busy day ensemble","0ba0c6e9":"## volatile day ","6a103b1c":"###### Version notes\n\nFinal submission: one overfit model (ver 2) and one serious model (ver 6)\n\n- Ver 1: test run with 3 embed+resnet models, 3 tf ae+mlp models, volatile and regular for each\n- Ver 2: including the overfit seed tf model (fixed a bug, forgot using past_day_mean in the submission pipeline)\n- Ver 3: final run ver 1: typo in the final sub, maybe too tired, a good test against the concat blending...(4 hours 50 min inference....time)\n- Ver 4: fixed the typo, decrease the number of models to be safe.\n- Ver 5: increased 1 more common model for both regular and volatile, only first two folds (more regular than fold 2) for regular days tf resnet model.\n- Ver 6: Rerun of ver 5 \n- Ver 7-8: Rerun of ver 4\n- Ver 9-11, 20-29: CPU rerun of ver 6\n- Ver 12-19, 30-: GPU rerun of ver 6","bbe88d34":"## tf model 2\n\n","8c13ee40":"### Settings and column names for different models","3f01eb77":"# Torch models","af6b536e":"# Final solution of Semper Augustus\n\n\n## Model performance on live stock market data update\n\n| Date of LB |  Ranking   | Overfit Ensemble (version 2) | delta | Local Best CV (version 6) |  delta |\n|:----------:|:----------:|:---------------------:|:--------:|---------------------|:---------:|\n|    Mar 5   | 99\/4245    |       4790.458        |          |       4541.474     |           |\n|   Mar 17   | 75\/4245    |       5153.324        |   +363   |       4952.939      | +411      |\n\n\n## Data preparation\nThe data contain 500 days of high frequency trading data from Jane Street, total 2.4 million rows.\n\n0. All data: only drop the two partial days and the two <2k `ts_id` days (done first).\n1. `fillna()` past day mean including all weight zero rows. \n2. ~~Most common values `fillna` for spike features rows.~~ (not any more after categorical embedding)\n4. Smoother data: aside from 1, query day > 85, ~~drop `ts_id` > 9000 days~~ (reduces CV).\n5. Final training uses only `weight > 0` rows, ~~with a randomly selected 40% of weight zero rows' weight being replaced by 1e-7 to reduce overfitting~~ (reduces CV so discarded).\n6. ~~A new de-noised target is generated with all five targets~~ (CV too good > 0.57 but leaderboard bad).\n\n## Models\n- (PT) PyTorch baseline with the skip connection mechanics, around 400k parameters, fast inference. Easy to get overfit.\n- (S) Carl found that some features have an extremely high number of common values. Based on close inspection. I have a conjecture that they are certain categorical features' embedding. So this model is designed to add an embedding block for these features. Also with the skip connection mechanics, around 300k parameters, best local CV (>0.555 on multiple folds) and best single model leaderboard score (7818).\n- (AE) Tensorflow implementation of an autoencoder + a small MLP net with skip connection in the first layer. Small net. Currently the best scored public ones with a serious CV using 3 folds ensemble.\n- (TF) Tensorflow Residual MLP using a filtering layer with high dropout rates to filter out hand-picked unimportant features suggested by Carl. The filter layer input is different for busy day model and regular day model.\n- (TF overfit) the infamous overfit model with a 1111 seed.\n\n## Train\n\n### Train-validation splits\nA grouped validation strategy based on a total of 100 days as validation, a 10-day gap between the last day of train and the first of valid, three folds.\n```python\nsplits = {\n          'train_days': (range(0,457), range(0,424), range(0,391)),\n          'valid_days': (range(467, 500), range(434, 466), range(401, 433)),\n          }\n```\n\n1. Volatile models: all data with only `resp`, `resp_3`, `resp_4` as targets.\n2. Smoother models: smoother data with all five `resp`s.\n~~3. De-noised models: smoother data with all five `resp`s + a de-noised target~~.\n4. Optimizer is simply Adam with a cosine annealing scheduler that allow warm restarts. Rectified Adam for tensorflow models.\n5. During training of torch models, a fine-tuning regularizer is applied each 10 epochs to maximize the utility function by choosing action being the sigmoid of the outputs (Only for torch models, I do not know how to incorporate this in `tensorflow` training, as tensorflow's custom loss function is not that straightforward to keep track of extra inputs between batches).\n\n### Fine-tuning using utility\nFor each date $i$, we define: for `r` representing the `resp` (response), `w` representing the `weight`, and `a` representing the `action` (1 for taking the trade, 0s for pass):\n<p align=\"center\">\n<img src=\"https:\/\/render.githubusercontent.com\/render\/math?math=%5Cdisplaystyle%20p_i%20%3D%20%5Csum_%7Bj%7D%20w_%7Bij%7D%20r_%7Bij%7D%20a_%7Bij%7D\">\n<\/p>\n\nThen it is summed up to \n<p align=\"center\">\n<img src=\"https:\/\/render.githubusercontent.com\/render\/math?math=%5Cdisplaystyle%20t%20%3D%20%5Cfrac%7B%5Csum%20p_i%20%7D%7B%5Csqrt%7B%5Csum%20p_i%5E2%7D%7D%20*%20%5Csqrt%7B%5Cfrac%7B250%7D%7B%7Ci%7C%7D%7D%2C\">\n<\/p>\n\nFinally the utility is computed by:\n<p align=\"center\">\n<img src=\"https:\/\/render.githubusercontent.com\/render\/math?math=%5Cdisplaystyle%20u%20%3D%20%5Cmin(%5Cmax(t%2C0)%2C%206)%20%20%5Csum_i%20p_i.\">\n<\/p>\n\nEssentially, without considering some real market constraint, when every `p_i` become positive, this is to maximize \n\n<p align= \"center\">\n<img src=\"https:\/\/render.githubusercontent.com\/render\/math?math=%5Cdisplaystyle%20%5Cleft(%5Csum_i%20p_i%5Cright)%5E2%20%5Ccdot%20%5Cleft(%20%0A%20%5Csum_i%20p_i%5E2%5Cright)%5E%7B-1%7D\">\n<\/p>\nWe have constructed a fine-tuner using this to train the SpikeNet, by replacing the discrete $p_i$ with a continuously changing one.\n\n\n## Submissions\n1. Local best CV ones within a three seeds bag. Final models: a set of `2(S) + 2(PT) + 2(AE) + 2(TF)` for smooth days, and `5(S) + 2(PT) + 2(AE) + 3(TF)` for volatile days.\n2. ~~Trained with all data using the \u201cpublic leaderboard as CV\u201d epochs determined earlier, plus the infamous tensorflow seed 1111 overfit model. The validation for this submission is based on the variation of the utility score in all train data among all 25-day non-overlapping spans~~.\n3. As our designated submission timed out (version 4)...we decided to choose an overfit model using this pipeline.\n\n### Inference pipeline\n1. CPU inference because the submission is CPU-bounded rather GPU. Torch models are usually faster than TF, TF models with `numba` backend enabled.\n2. Use `feature_64`'s [average gradient (a scaled version of $\\arcsin (t)$) suggest by Carl](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/208013#1135364), and the number of trades in the previous day as a criterion to determine the models to include. Reference: [slope test of the past day class by Ethan and iter_cv simulation written by Shuhao](https:\/\/www.kaggle.com\/ztyreg\/validate-busy-prediction), [slope validation](https:\/\/www.kaggle.com\/ztyreg\/validate-busy-prediction)\n3. Blending is always concatenating models in a bag then taking the middle 60%'s average (median if only 3 models), then concatenating again to take the middle 60% average (50% if a day is busy). For example, if we have `5 (PT) + 3 (AE) + 1 (TF)`, then `5 (PT)`'s predictions are concatenated and averaged along `axis 0` with the middle three, and `(AE)` submissions are taken the median. Lastly, the subs are concatenated again to take the middle 9 entries (15 total).","b1420351":"# Inference\n\nPrint the number of models in the ensemble below.","cca327bd":"# Tensorflow models"}}