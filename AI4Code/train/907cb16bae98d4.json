{"cell_type":{"574d758d":"code","a671eb21":"code","504b773f":"code","beff727f":"code","b9ad2997":"code","24131b9b":"code","a724e1fb":"code","0ce7c8c1":"code","cff4c9d8":"code","4d5d66b7":"code","a2220542":"code","81ebe64b":"markdown","0abf489d":"markdown","75a5d3e7":"markdown","086f91dc":"markdown","e9ee94bb":"markdown","48ef51cb":"markdown","68e17065":"markdown","b6624494":"markdown","055d389c":"markdown"},"source":{"574d758d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# specify GPU\ndevice = torch.device(\"cuda\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a671eb21":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample_submision = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","504b773f":"train.head()","beff727f":"test.head()","b9ad2997":"# split train dataset into train, validation sets\ntrain_excerpt, valid_excerpt, train_target, valid_target = train_test_split(train['excerpt'], train['target'], \n                                                                    random_state=2018, \n                                                                    test_size=0.3)","24131b9b":"# import BERT-base pretrained model\nbert = AutoModel.from_pretrained('bert-base-uncased')\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","a724e1fb":"#sample data\ntext_list = [\"When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.The floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.At each end of the room, on the wall, hung a beautiful bear-skin rug.These rugs were for prizes, one for the girls and one for the boys. And this was the game.The girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.This would have been an easy matter, but each traveller was obliged to wear snowshoes.\"]\n\n\nprint(text_list)\n# encode text\nsent_id = tokenizer.batch_encode_plus(text_list, padding=True)\n\n# output\nprint(sent_id)","0ce7c8c1":"# get length of all the messages in the train set\nseq_len = [len(i.split()) for i in train['excerpt']]\n\npd.Series(seq_len).hist(bins = 30)","cff4c9d8":"# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_excerpt.tolist(),\n    max_length = 200,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    valid_excerpt.tolist(),\n    max_length = 200,\n    pad_to_max_length=True,\n    truncation=True\n)","4d5d66b7":"## convert lists to tensors\n\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_target.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(valid_target.tolist())","a2220542":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","81ebe64b":"# Now we will create dataloaders for both train and validation set. These dataloaders will pass batches of train data and validation data as input to the model during the training phase.","0abf489d":"# Let's see how BERT Tokenizer works,We will try to encode a couple of sentences using the tokenizer.","75a5d3e7":"# EDA has been perforned in another notebook [commonlit-readability-prize-eda](http:\/\/www.kaggle.com\/deb009\/commonlit-readability-prize-eda\/)","086f91dc":"# Tokenize the Sentences","e9ee94bb":"# convert the integer sequences to tensors.","48ef51cb":"# Work in Progress.Thank you for your time.","68e17065":"# We will set the padding length as 200","b6624494":"# Importing dataset","055d389c":"# Import BERT Model and BERT Tokenizer"}}