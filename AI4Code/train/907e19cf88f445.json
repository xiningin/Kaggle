{"cell_type":{"6cd2ae1b":"code","0068d291":"code","b6aa824d":"code","31ca342d":"code","53433e1f":"code","15b8f9e8":"code","bd923775":"code","45015827":"code","2b2da068":"code","1aeb3135":"code","eb8bda2a":"code","515dcf9e":"code","6f4fe0b0":"code","70b1a72e":"code","ef9e432f":"code","cda436bd":"code","f3f51d39":"code","159b9186":"code","72d4f901":"code","bfdd180d":"code","7007a70b":"markdown","40fd998d":"markdown","ebebf508":"markdown","c35fcefa":"markdown","983db620":"markdown","aacb4ba4":"markdown","bef51500":"markdown"},"source":{"6cd2ae1b":"#reading the data\nimport pandas as pd\ndf=pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndf.head()","0068d291":"#shape of data frame\ndf.shape","b6aa824d":"print('Canacer detected', round(df['diagnosis'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Malignant ', round(df['diagnosis'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","31ca342d":"#drop Unnamed: 32\ndf1=df.drop(columns='Unnamed: 32')","53433e1f":"#there is no null values\nprint(df.info())\ndf1.diagnosis.value_counts()","15b8f9e8":"#Label Encoding to change the tha char value of diagnosis to 0s and 1\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndf1['diagnosis']= label_encoder.fit_transform(df1['diagnosis'])\ndf1.diagnosis.value_counts()","bd923775":"\n#checking duplicates\nprint(df1.duplicated().sum())","45015827":"#Defining x and y\nx=df1.iloc[:,2:]\ny=df1.diagnosis\n","2b2da068":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y)","1aeb3135":"#getting the n_component for the PCA\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n#PCA is largely affected by scales and different features might have different scales.\n#so we will standerise the data to get the components\nstdSC=StandardScaler()\nx_train_std=stdSC.fit_transform(x_train)\npcaModel=PCA()\npcaModel.fit(x_train_std)\npcaModel.explained_variance_\npcaModel.explained_variance_ratio_\n#pcaModel.explained_variance_ratio_*100\nnp.cumsum(pcaModel.explained_variance_ratio_*100)\n","eb8bda2a":"# create pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import  LogisticRegression\n#from sklearn.linear_model import Ridge\n\nlogistic = LogisticRegression(max_iter=10000, tol=0.1)\nsteps_4_model=[('std',StandardScaler()),('pca',PCA(n_components=24)),('log',LogisticRegression())]\n#parameters = [ {'model__alpha': np.arange(0, 0.2, 0.01) } ]\n\n\nfinalModel=Pipeline(steps=steps_4_model)\nfinalModel.fit(x_train,y_train)\nfinalModel.predict(x_test)\nprint(\"Train Score (Linear):\",finalModel.score(x_train,y_train))\nprint(\"Test Score (Linear):\",finalModel.score(x_test,y_test))\n","515dcf9e":"print(pd.crosstab(y_train,finalModel.predict(x_train)))\nprint(\"**********************************************\")\nprint(pd.crosstab(y_test,finalModel.predict(x_test)))","6f4fe0b0":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,finalModel.predict(x_test)))","70b1a72e":"#to visulaise the new dimension\n\nstdSc=StandardScaler()\nstdSc.fit(x)\nx_train_std=stdSc.transform(x_train)\n\npcaModel_24c=PCA(n_components=24)\npcaModel_24c.fit(x_train_std)\npcaModel_24c.explained_variance_\n(pcaModel_24c.explained_variance_ratio_*100)\nx_train_comp=pcaModel_24c.transform(x_train_std)\npd.DataFrame(x_train_comp).head()","ef9e432f":"#checking the visualisation with 2 components of the new dimension\nfrom matplotlib import pyplot as plt,style\nA=pd.DataFrame(x_train_comp[:,0:2],index=x_train.index)\n\n#pd.DataFrame[(A,columns=[\"c1\",\"c2\"])\nA.columns=[\"c1\",\"c2\"]\n\nimport seaborn as sns\n\n\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nsns.scatterplot(data=x_train,x=x_train.radius_mean,y=x_train.texture_mean,hue=y_train).set_title('orig_datset')\nplt.subplot(1,2,2)\nsns.scatterplot(data=A,x=A.c1,y=A.c2,hue=df1.diagnosis).set_title('with PCA')\n","cda436bd":"# create pipeline\nfrom sklearn.discriminant_analysis import  LinearDiscriminantAnalysis\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state= 0)\n\n\nlogistic = LogisticRegression(max_iter=10000, tol=0.1)\nsteps_4_model=[('std',StandardScaler()),('lda', LinearDiscriminantAnalysis(n_components=1)),('log',LogisticRegression())]\n#parameters = [ {'model__alpha': np.arange(0, 0.2, 0.01) } ]\n\n\nfinalModel=Pipeline(steps=steps_4_model)\nfinalModel.fit(x_train,y_train)\nfinalModel.predict(x_test)\nprint(\"Train Score (Linear):\",finalModel.score(x_train,y_train))\nprint(\"Test Score (Linear):\",finalModel.score(x_test,y_test))\n","f3f51d39":"print(pd.crosstab(y_train,finalModel.predict(x_train)))\nprint(pd.crosstab(y_test,finalModel.predict(x_test)))","159b9186":"#Visulaisation for LDA\n\n\n\nimport seaborn as sns\nstyle.use('ggplot')\n\nstdSc=StandardScaler()\nstdSc.fit(x)\nx_train_std=stdSc.transform(x_train)\n\nLDAModel_24c=LinearDiscriminantAnalysis(n_components=1)\nLDAModel_24c.fit(x_train_std,y_train)\n\nx_train_lda=LDAModel_24c.transform(x_train_std)\npd.DataFrame(x_train_lda).head()\n\n","72d4f901":"trainLDA=pd.DataFrame(x_train_lda,columns=[\"c1\"],index=x_train.index)\ntrainLDA['diagnosis']=y_train\ntrainLDA.head()","bfdd180d":"plt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nsns.scatterplot(data=x_train,x=x_train.radius_mean,y=x_train.texture_mean,hue=y_train).set_title('orig_datset')\nplt.subplot(1,2,2)\nsns.scatterplot(data=trainLDA,x=trainLDA.c1,y=0,hue='diagnosis').set_title('with LDA')","7007a70b":"# Dimensionality reduction with LDA ","40fd998d":"### The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the transform method.","ebebf508":"We can see that 24 componets define almost 99.9% of data .","c35fcefa":"0 or B : 212 \n\n1 or M : 357","983db620":"The data set is  a little bit unbalanced but We can consider this ratio for model creation","aacb4ba4":"# Dimensionality reduction with PCA","bef51500":"There are no null values"}}