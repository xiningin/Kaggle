{"cell_type":{"6d255f26":"code","5a0e840b":"code","cab35f70":"code","077253c5":"code","0af5c122":"code","ceb07e3b":"code","eebc3a22":"code","6830009f":"code","9c7f4bb6":"code","e3088994":"code","c98902d1":"code","23ec5f0b":"code","0768c8c1":"code","ee437eb1":"code","22d18936":"code","acd2aa73":"code","264dd05e":"code","7b3c4433":"markdown","aebe6e9d":"markdown","dd383261":"markdown","187fac40":"markdown","6fb7ff48":"markdown","cb5ba9cf":"markdown","cd43cf1c":"markdown","5b501683":"markdown","54d0f80c":"markdown","90aed134":"markdown","07175996":"markdown","d128c715":"markdown","1c6b8ce8":"markdown","cc75b9cc":"markdown","03a257a8":"markdown","19bb3646":"markdown","10c7448e":"markdown","d1699354":"markdown"},"source":{"6d255f26":"SAMPLE_SUB = \"..\/input\/deepfake-detection-challenge\/sample_submission.csv\"\nTRAIN_VIDEOS = \"..\/input\/deepfake-detection-challenge\/train_sample_videos\"\nTEST_VIDEOS = \"..\/input\/deepfake-detection-challenge\/test_videos\"\nTRAIN_JSON_PATH = \"..\/input\/deepfake-detection-challenge\/train_sample_videos\/metadata.json\"","5a0e840b":"#import packages\n\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\n%matplotlib inline","cab35f70":"# check the number of videos present in the train and test data.\n\nn_train_videos = len(os.listdir(TRAIN_VIDEOS))\nn_test_videos = len(os.listdir(TEST_VIDEOS))\n\ntrain_videos = os.listdir(TRAIN_VIDEOS)\ntest_videos = os.listdir(TEST_VIDEOS)\n\nprint(\"Number of training vidoes: \", n_train_videos - 1)\nprint(\"Number of testing videos: \", n_test_videos)","077253c5":"#read the json file\n\ndeepfake_labels = pd.read_json(TRAIN_JSON_PATH).T\n\ndeepfake_labels.head()","0af5c122":"#start with the basic analysis - Missing value analysis\n\nmissing_df = pd.DataFrame({\"Missing_Count\": deepfake_labels.isnull().sum(),\n                          \"Missing_Percent\": round(deepfake_labels.isnull().mean(),2)})\nmissing_df","ceb07e3b":"#check the distribution of labels\nplt.style.use(\"ggplot\")\nplt.rcParams['figure.figsize'] = 14,7\n\ndeepfake_labels.label.value_counts(normalize = True).plot(kind = \"barh\")\nplt.xlabel(\"Percentage of labels\")\nplt.title(\"Distribution of labels for videos\")\nplt.show()","eebc3a22":"# we have already created a list of training data and test data videos\n\nprint(\"Training videos: \" ,train_videos[:5])\nprint(\"Testing videos: \", test_videos[:5])","6830009f":"train_fake_lst = deepfake_labels[deepfake_labels[\"label\"] == \"FAKE\"].index.tolist()\ntrain_real_lst = deepfake_labels[deepfake_labels[\"label\"] == \"REAL\"].index.tolist()","9c7f4bb6":"#display frame\n\ndef display_frame(video, axis):\n    cap = cv2.VideoCapture(video)  \n    ret, frame = cap.read()\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    axis.imshow(frame)\n    axis.grid(False)\n    video_name = video.split(\"\/\")[-1]\n    axis.set_title(\"Video: \" + video_name, size = 30)","e3088994":"random_num = np.random.randint(0,len(train_fake_lst))\nfig,axs = plt.subplots(nrows = 1, ncols=3, figsize=(50,40))\nfor i in range(3):\n    display_frame(os.path.join(TRAIN_VIDEOS, train_fake_lst[random_num]), axs[i])","c98902d1":"random_num = np.random.randint(0,len(train_fake_lst))\nfig,axs = plt.subplots(nrows = 1, ncols=3, figsize=(50,40))\nfor i in range(3):\n    display_frame(os.path.join(TRAIN_VIDEOS, train_fake_lst[random_num]), axs[i])","23ec5f0b":"random_num = np.random.randint(0,len(train_real_lst))\nfig,axs = plt.subplots(nrows = 1, ncols=3, figsize=(50,40))\nfor i in range(3):\n    display_frame(os.path.join(TRAIN_VIDEOS, train_real_lst[random_num]), axs[i])","0768c8c1":"random_num = np.random.randint(0,len(train_real_lst))\nfig,axs = plt.subplots(nrows = 1, ncols=3, figsize=(50,40))\nfor i in range(3):\n    display_frame(os.path.join(TRAIN_VIDEOS, train_real_lst[random_num]), axs[i])","ee437eb1":"random_num = np.random.randint(0,len(train_real_lst))\nfig,axs = plt.subplots(nrows = 1, ncols=3, figsize=(50,40))\nfor i in range(3):\n    display_frame(os.path.join(TRAIN_VIDEOS, train_real_lst[random_num]), axs[i])","22d18936":"#checking the label distribution\n\ndeepfake_labels.label.value_counts(normalize = True)","acd2aa73":"sample_df = pd.read_csv(SAMPLE_SUB)\nsample_df.head()","264dd05e":"sample_df[\"label\"] = 0.65\nsample_df.to_csv(\"submission.csv\", index = False)","7b3c4433":"# Import Packages","aebe6e9d":"- To display a single frame from the video. We will use the function from the kernel **\"[Basic EDA Face Detection, split video and ROI](https:\/\/www.kaggle.com\/marcovasquez\/basic-eda-face-detection-split-video-and-roi)\"**","dd383261":"![](https:\/\/spectrum.ieee.org\/image\/MzQyNjU1OQ.jpeg)","187fac40":"# Data Analysis - MetaData File","6fb7ff48":"* Around 20% of the videos present doesn't have a original video associated with it. It could mean that these 77 videos might be REAL so that's why original video column is empty for these videos.","cb5ba9cf":"# Analysis of Training - FAKE Videos\n- Look at the FAKE videos present in training data","cd43cf1c":"> The data is comprised of .mp4 files, split into compressed sets of ~10GB apiece. A **metadata.json** accompanies each set of .mp4 files, and contains `filename`, `label` (REAL\/FAKE), `original` and `split` columns, listed below under Columns","5b501683":"# Submission","54d0f80c":"# Exploratory Data Analysis - Video Data","90aed134":"# Data Files\n\n* **train_sample_videos.zip** - a ZIP file containing a sample set of training videos and a metadata.json with labels. the full set of training videos is available through the links provided above.\n* **sample_submission.csv** - a sample submission file in the correct format.\n* **test_videos.zip** - a zip file containing a small set of videos to be used as a public validation set.","07175996":"In this [competition](https:\/\/deepfakedetectionchallenge.ai\/) produced in cooperation with Amazon, Microsoft, the nonprofit Partnership on AI, and academics from eight universities\u2014researchers around the world are vying to create automated tools that can spot fraudulent media. The competition was announced at the AI conference NeurIPS, and will accept entries through March 2020. Facebook has dedicated more than US $10 million for awards and grants.\n\n\nDeepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online. These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rights\u2014especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion. Identifying manipulated media is a technically demanding and rapidly evolving challenge that requires collaborations across the entire tech industry and beyond.","d128c715":"- We will look at the some of the FAKE and REAL Videos. First we will get the names of the FAKE and REAL videos into seperate lists for training and test data","1c6b8ce8":"- Training data is skewed. More than 80% of the data consisting of `FAKE` videos","cc75b9cc":"- Another REAL Video","03a257a8":"- Since our data is imbalanced (80% of data contains FAKE Video's). So we will assume that test data might also follows same distribution for sample submission.","19bb3646":"# Deepfake Detection Challenge\n- Identify videos with facial or voice manipulations","10c7448e":"# Analysis of Training - REAL Videos\n- Look at the REAL videos present in training data","d1699354":"DeepFake uses AI (artificial intelligence) and machine learning to manipulate videos or any other form of digital representations. They result in images, videos or just audios that appear to be real. Since DeepFake uses AI and machine learning, the tech analyzes the videos and images of the target person from all angles. Thereafter, the technology accurately mimics the behavior and speech of the target person."}}