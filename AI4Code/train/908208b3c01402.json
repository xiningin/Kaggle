{"cell_type":{"3c3e2cc8":"code","1b737727":"code","8c98aa7b":"code","24f7fad9":"code","8a187a4b":"code","5df678d7":"code","cf3fe436":"code","388eb09a":"code","b726904c":"code","d1a03c10":"code","d9557c5c":"code","1155dd88":"code","05ba635c":"code","65ad5a1f":"code","b759f272":"code","8eee82a2":"code","773138e3":"code","624c0ae5":"code","a9e44f71":"code","e0f183dc":"code","3a67dd1c":"code","b9231461":"code","f778e21c":"code","a3c39175":"code","9f0ccf17":"code","ff36eb5f":"code","d64096b6":"code","3fa27591":"code","ba47948f":"code","59824f43":"code","ff7b8394":"code","426445e7":"code","c195c1f7":"code","2cc5df18":"code","93ee148b":"code","ac7a60fa":"code","9356b00c":"code","76022e42":"code","383c75ef":"code","b838f909":"code","7717512b":"code","16f339b6":"code","d8119e32":"code","b85c37fe":"code","828f9142":"code","18bd56ff":"code","0b1a3529":"code","26eacf80":"code","b2cc6c57":"code","61be8c91":"code","51c84a5a":"code","4e15f80e":"code","482d9a57":"code","d220ddac":"code","5c1d5b92":"code","b44948a5":"code","f2fc615a":"code","90c4b152":"code","82f098bb":"code","54636740":"code","215a898f":"code","3ce07bbb":"code","6cf6bf99":"code","85825110":"code","fd34b3e2":"code","11893bd0":"code","3a1b9a33":"code","249af820":"markdown","d897f573":"markdown","3447593d":"markdown","d90d9244":"markdown","f1a2d667":"markdown","4ea1e2d5":"markdown","97380189":"markdown","80ec4039":"markdown","62235166":"markdown","e62b1532":"markdown","b61b384a":"markdown","d3d768a8":"markdown","06496a50":"markdown","ebbbdcae":"markdown","b9146a0c":"markdown","cca80609":"markdown","09f9813b":"markdown","e5cef143":"markdown","50e7195d":"markdown","2eed5202":"markdown","a8afce5a":"markdown","079a58e6":"markdown","26c0e0aa":"markdown","1174d975":"markdown","bb700c06":"markdown","be285f06":"markdown"},"source":{"3c3e2cc8":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.formula.api import ols\n\nhtl = pd.read_csv(\"\/kaggle\/input\/hotel-booking-demand\/hotel_bookings.csv\")\npd.set_option('display.max_columns', None)\nhtl.head()","1b737727":"# a general overview of data\n\nhtl.describe(include=\"all\").T","8c98aa7b":"# basic description about all attributes we have, including col_name,data type,NA count and unique value count.\ndescribe = []\nfor col in htl.columns:\n    describe.append(len(htl[col].value_counts()))\ndescribe = pd.DataFrame(list(zip(htl.columns,htl.dtypes,htl.isnull().sum(),htl.describe(include=\"all\").T['unique'],describe)),columns = ['col','type','NA','Ucount','count'])\ndescribe","24f7fad9":"# find numercial viariables which can be turned to categorical viariables. \n# This time we choose 200 as we discovered some variables like days_in_waiting_list are actually categorical variables too.\n\nto_cate = describe[describe['Ucount'].isna()][describe['count']<200]\nto_cate = to_cate[1:] # y=is_canceled stays as numeric.\n\nto_cate","8a187a4b":"# the agent \/ country\/ company is just too many categories and many of them just appeared once in data.\n# since we want to build a stable model, we'd better do something to group the small data together to avoid overfitting.\n# re-group the agent and contry and company group\n\nagent_cnt = htl['agent'].value_counts().reset_index()\nagent_cnt['sum']=agent_cnt['agent'].cumsum()\nagent_cnt['sum%']=agent_cnt['sum']\/agent_cnt.loc[332,'sum']\nagent_cnt['re_grp'] = agent_cnt[['index','sum%']].apply(lambda x : x['index'] if x['sum%'] <= 0.8 else 'OTH', axis =1)\nagent_cnt.columns = ['agent', 'count', 'acc_count', 'acc_count%','agent_grp'] \n\ncnty_cnt = htl['country'].value_counts().reset_index()\ncnty_cnt['sum']=cnty_cnt['country'].cumsum()\ncnty_cnt['sum%']=cnty_cnt['sum']\/cnty_cnt.loc[176,'sum']\ncnty_cnt['re_grp'] = cnty_cnt[['index','sum%']].apply(lambda x : x['index'] if x['sum%'] <= 0.8 else 'OTH', axis =1)\ncnty_cnt.columns = ['country', 'count', 'acc_count', 'acc_count%','cnty_grp'] \n\n\ncmp_cnt = htl['company'].value_counts().reset_index()\ncmp_cnt['sum']=cmp_cnt['company'].cumsum()\ncmp_cnt['sum%']=cmp_cnt['sum']\/cmp_cnt.loc[351,'sum']\ncmp_cnt['re_grp'] = cmp_cnt[['index','sum%']].apply(lambda x : x['index'] if x['sum%'] <= 0.8 else 'OTH', axis =1)\ncmp_cnt.columns = ['company', 'count', 'acc_count', 'acc_count%','cmp_grp'] \n\nprint(agent_cnt)\nprint(cnty_cnt)\nprint(cmp_cnt)","5df678d7":"# To better record our actions, create a record book to map the 'before' and 'after' value\n\ndata_clean = []\n\ndata_clean.append(agent_cnt[['agent_grp']].set_index(agent_cnt['agent']).to_dict())\ndata_clean.append(cnty_cnt[['cnty_grp']].set_index(cnty_cnt['country']).to_dict())\ndata_clean.append(cmp_cnt[['cmp_grp']].set_index(cmp_cnt['company']).to_dict())","cf3fe436":"# take a look\ndata_clean[0]['agent_grp']","388eb09a":"# take a look at days_in_waiting_list\n\nplt.hist(htl['days_in_waiting_list'], bins=50)","b726904c":"htl['days_in_waiting_list'].value_counts()","d1a03c10":"# change the variable type, create new variables.\n# Note we did create some new variables in Part One, but we can skip it for now because we will go back and look at the categorical variables again.\n\ndata = htl.copy()\n'''\ndata['is_children'] = ['Y' if x > 0 else 'N' for x in data['children']]\ndata['is_baby'] = ['Y' if x > 0 else 'N' for x in data['babies']]\ndata['is_agent'] = ['Y' if x > 0 else 'N' for x in data['agent']]\ndata['is_company'] = ['Y' if x > 0 else 'N' for x in data['company']]\ndata['is_parking'] = ['Y' if x > 0 else 'N' for x in data['required_car_parking_spaces']]\ndata['is_request'] = ['Y' if x > 0 else 'N' for x in data['total_of_special_requests']]\ndata['is_canceled_before'] = ['Y' if x > 0 else 'N' for x in data['previous_cancellations']]\ndata['is_changed'] = ['Y' if x > 0 else 'N' for x in data['booking_changes']]\n'''\ndata['is_waited'] = ['Y' if x > 0 else 'N' for x in data['days_in_waiting_list']]\n\ndata['is_room_changed'] = data[['reserved_room_type','assigned_room_type']].apply(lambda x:x['reserved_room_type'] != x['assigned_room_type'], axis=1)\n\nfor i in to_cate['col']:\n    data[i] = data[i].astype('str')\n    \ndata['is_room_changed'] = data['is_room_changed'].astype('str')\n","d9557c5c":"# mapping back company, country and agent values we created before\n\ndata['agent'] = data['agent'].map(data_clean[0]['agent_grp'])\ndata['country'] = data['country'].map(data_clean[1]['cnty_grp'])\ndata['company'] = data['company'].map(data_clean[2]['cmp_grp'])\n","1155dd88":"# Fill NA with 0: NA data for agent\/company just means the customer is not from an agent\/company. \n# The country value is simply missing and we can fill it this way and may intergrate it into other groups later.\n\ndata.agent.fillna(value='0', inplace=True)\ndata.company.fillna(value='0', inplace=True)\ndata.country.fillna(value= 'MISSING', inplace=True)","05ba635c":"# Write into record\n\ndata_clean.append({\"agentNaN\":'0'})\ndata_clean.append({\"countryNaN\":'MISSING'})\ndata_clean.append({\"companyNaN\":'0'})","65ad5a1f":"# make them categorical\n\ndata['agent'] = data['agent'].astype('str')\ndata['company'] = data['company'].astype('str')\n\n# and now check the new data\n\ncate = [var for var in data.columns if data[var].dtypes == 'object']\nnum = [var for var in data.columns if data[var].dtypes != 'object']\n\ndescribe_new = []\nfor col in data.columns:\n    describe_new.append(len(data[col].value_counts()))\ndescribe_new = pd.DataFrame(list(zip(data.columns,data.dtypes,data.isnull().sum(),data.describe(include=\"all\").T['unique'],describe_new)),columns = ['col','type','NA','Ucount','count'])\ndescribe_new","b759f272":"# check correlation for continuous variables\n\ncor = data.corr(method='pearson')\ncor","8eee82a2":"# Also there are reservation_status: 100% relate to 'is_canceled'.\n# And we need to remove 'reservation_status_date' and 'days_in_waiting_list' becuase we just built new categorical variable.\n\ncate = cate[0:23] + cate[24:27] + cate[29:] # remove  'reservation_status', 'reservation_status_date','days_in_waiting_list'\nnum = num[1:] #remove Y","773138e3":"# Create new dataset\n\ny = 'is_canceled'\nX = data[cate + num].copy()\n\nY = data[y].copy()","624c0ae5":"# Here we select variables using WOE and IV value. I used this function to fast calculate the value. Thanks to the author!\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\n\n__author__ = 'Denis Surzhko'\n\n\nclass WoE:\n    \"\"\"\n    Basic functionality for WoE bucketing of continuous and discrete variables\n    :param self.bins: DataFrame WoE transformed variable and all related statistics\n    :param self.iv: Information Value of the transformed variable\n    \"\"\"\n    def __init__(self, qnt_num=16, min_block_size=16, spec_values=None, v_type='c', bins=None, t_type='b'):\n        \"\"\"\n        :param qnt_num: Number of buckets (quartiles) for continuous variable split\n        :param min_block_size: minimum number of observation in each bucket (continuous variables)\n        :param spec_values: List or Dictionary {'label': value} of special values (frequent items etc.)\n        :param v_type: 'c' for continuous variable, 'd' - for discrete\n        :param bins: Predefined bucket borders for continuous variable split\n        :t_type : Binary 'b' or continous 'c' target variable\n        :return: initialized class\n        \"\"\"\n        self.__qnt_num = qnt_num  # Num of buckets\/quartiles\n        self._predefined_bins = None if bins is None else np.array(bins)  # user bins for continuous variables\n        self.type = v_type  # if 'c' variable should be continuous, if 'd' - discrete\n        self._min_block_size = min_block_size  # Min num of observation in bucket\n        self._gb_ratio = None  # Ratio of good and bad in the sample\n        self.bins = None  # WoE Buckets (bins) and related statistics\n        self.df = None  # Training sample DataFrame with initial data and assigned woe\n        self.qnt_num = None  # Number of quartiles used for continuous part of variable binning\n        self.t_type = t_type  # Type of target variable\n        if type(spec_values) == dict:  # Parsing special values to dict for cont variables\n            self.spec_values = {}\n            for k, v in spec_values.items():\n                if v.startswith('d_'):\n                    self.spec_values[k] = v\n                else:\n                    self.spec_values[k] = 'd_' + v\n        else:\n            if spec_values is None:\n                self.spec_values = {}\n            else:\n                self.spec_values = {i: 'd_' + str(i) for i in spec_values}\n\n    def fit(self, x, y):\n        \"\"\"\n        Fit WoE transformation\n        :param x: continuous or discrete predictor\n        :param y: binary target variable\n        :return: WoE class\n        \"\"\"\n        # Data quality checks\n        if not isinstance(x, pd.Series) or not isinstance(y, pd.Series):\n            raise TypeError(\"pandas.Series type expected\")\n        if not x.size == y.size:\n            raise Exception(\"Y size don't match Y size\")\n        # Calc total good bad ratio in the sample\n        t_bad = np.sum(y)\n        if t_bad == 0 or t_bad == y.size:\n            raise ValueError(\"There should be BAD and GOOD observations in the sample\")\n        if np.max(y) > 1 or np.min(y) < 0:\n            raise ValueError(\"Y range should be between 0 and 1\")\n        # setting discrete values as special values\n        if self.type == 'd':\n            sp_values = {i: 'd_' + str(i) for i in x.unique()}\n            if len(sp_values) > 100:\n                raise type(\"DiscreteVarOverFlowError\", (Exception,),\n                           {\"args\": ('Discrete variable with too many unique values (more than 100)',)})\n            else:\n                if self.spec_values:\n                    sp_values.update(self.spec_values)\n                self.spec_values = sp_values\n        # Make data frame for calculations\n        df = pd.DataFrame({\"X\": x, \"Y\": y, 'order': np.arange(x.size)})\n        # Separating NaN and Special values\n        df_sp_values, df_cont = self._split_sample(df)\n        # # labeling data\n        df_cont, c_bins = self._cont_labels(df_cont)\n        df_sp_values, d_bins = self._disc_labels(df_sp_values)\n        # getting continuous and discrete values together\n        self.df = df_sp_values.append(df_cont)\n        self.bins = d_bins.append(c_bins)\n        # calculating woe and other statistics\n        self._calc_stat()\n        # sorting appropriately for further cutting in transform method\n        self.bins.sort_values('bins', inplace=True)\n        # returning to original observation order\n        self.df.sort_values('order', inplace=True)\n        self.df.set_index(x.index, inplace=True)\n        return self\n\n    def fit_transform(self, x, y):\n        \"\"\"\n        Fit WoE transformation\n        :param x: continuous or discrete predictor\n        :param y: binary target variable\n        :return: WoE transformed variable\n        \"\"\"\n        self.fit(x, y)\n        return self.df['woe']\n\n    def _split_sample(self, df):\n        if self.type == 'd':\n            return df, None\n        sp_values_flag = df['X'].isin(self.spec_values.keys()).values | df['X'].isnull().values\n        df_sp_values = df[sp_values_flag].copy()\n        df_cont = df[np.logical_not(sp_values_flag)].copy()\n        return df_sp_values, df_cont\n\n    def _disc_labels(self, df):\n        df['labels'] = df['X'].apply(\n            lambda x: self.spec_values[x] if x in self.spec_values.keys() else 'd_' + str(x))\n        d_bins = pd.DataFrame({\"bins\": df['X'].unique()})\n        d_bins['labels'] = d_bins['bins'].apply(\n            lambda x: self.spec_values[x] if x in self.spec_values.keys() else 'd_' + str(x))\n        return df, d_bins\n\n    def _cont_labels(self, df):\n        # check whether there is a continuous part\n        if df is None:\n            return None, None\n        # Max buckets num calc\n        self.qnt_num = int(np.minimum(df['X'].unique().size \/ self._min_block_size, self.__qnt_num)) + 1\n        # cuts - label num for each observation, bins - quartile thresholds\n        bins = None\n        cuts = None\n        if self._predefined_bins is None:\n            try:\n                cuts, bins = pd.qcut(df[\"X\"], self.qnt_num, retbins=True, labels=False)\n            except ValueError as ex:\n                if ex.args[0].startswith('Bin edges must be unique'):\n                    ex.args = ('Please reduce number of bins or encode frequent items as special values',) + ex.args\n                    raise\n            bins = np.append((-float(\"inf\"), ), bins[1:-1])\n        else:\n            bins = self._predefined_bins\n            if bins[0] != float(\"-Inf\"):\n                bins = np.append((-float(\"inf\"), ), bins)\n            cuts = pd.cut(df['X'], bins=np.append(bins, (float(\"inf\"), )),\n                          labels=np.arange(len(bins)).astype(str))\n        df[\"labels\"] = cuts.astype(str)\n        c_bins = pd.DataFrame({\"bins\": bins, \"labels\": np.arange(len(bins)).astype(str)})\n        return df, c_bins\n\n    def _calc_stat(self):\n        # calculating WoE\n        # stat = self.df.groupby(\"labels\")['Y'].agg({'mean': np.mean, 'bad': np.count_nonzero, 'obs': np.size}).copy()\n        stat = self.df.groupby(\"labels\")[\"Y\"].agg([np.mean, np.count_nonzero, np.size])\n        stat = stat.rename(columns={'mean': 'mean', 'count_nonzero':'bad', 'size':'obs'})\n        if self.t_type != 'b':\n            stat['bad'] = stat['mean'] * stat['obs']\n        stat['good'] = stat['obs'] - stat['bad']\n        t_good = np.maximum(stat['good'].sum(), 0.5)\n        t_bad = np.maximum(stat['bad'].sum(), 0.5)\n        stat['woe'] = stat.apply(self._bucket_woe, axis=1) + np.log(t_good \/ t_bad)\n        iv_stat = (stat['bad'] \/ t_bad - stat['good'] \/ t_good) * stat['woe']\n        self.iv = iv_stat.sum()\n        # adding stat data to bins\n        self.bins = pd.merge(stat, self.bins, left_index=True, right_on=['labels'])\n        label_woe = self.bins[['woe', 'labels']].drop_duplicates()\n        self.df = pd.merge(self.df, label_woe, left_on=['labels'], right_on=['labels'])\n\n    def transform(self, x):\n        \"\"\"\n        Transforms input variable according to previously fitted rule\n        :param x: input variable\n        :return: DataFrame with transformed with original and transformed variables\n        \"\"\"\n        if not isinstance(x, pd.Series):\n            raise TypeError(\"pandas.Series type expected\")\n        if self.bins is None:\n            raise Exception('Fit the model first, please')\n        df = pd.DataFrame({\"X\": x, 'order': np.arange(x.size)})\n        # splitting to discrete and continous pars\n        df_sp_values, df_cont = self._split_sample(df)\n\n        # function checks existence of special values, raises error if sp do not exist in training set\n        def get_sp_label(x_):\n            if x_ in self.spec_values.keys():\n                return self.spec_values[x_]\n            else:\n                str_x = 'd_' + str(x_)\n                if str_x in list(self.bins['labels']):\n                    return str_x\n                else:\n                    raise ValueError('Value ' + str_x + ' does not exist in the training set')\n        # assigning labels to discrete part\n        df_sp_values['labels'] = df_sp_values['X'].apply(get_sp_label)\n        # assigning labels to continuous part\n        c_bins = self.bins[self.bins['labels'].apply(lambda z: not z.startswith('d_'))]\n        if not self.type == 'd':\n            cuts = pd.cut(df_cont['X'], bins=np.append(c_bins[\"bins\"], (float(\"inf\"), )), labels=c_bins[\"labels\"])\n            df_cont['labels'] = cuts.astype(str)\n        # Joining continuous and discrete parts\n        df = df_sp_values.append(df_cont)\n        # assigning woe\n        df = pd.merge(df, self.bins[['woe', 'labels']], left_on=['labels'], right_on=['labels'])\n        # returning to original observation order\n        df.sort_values('order', inplace=True)\n        return df.set_index(x.index)\n\n    def merge(self, label1, label2=None):\n        \"\"\"\n        Merge of buckets with given labels\n        In case of discrete variable, both labels should be provided. As the result labels will be marget to one bucket.\n        In case of continous variable, only label1 should be provided. It will be merged with the next label.\n        :param label1: first label to merge\n        :param label2: second label to merge\n        :return:\n        \"\"\"\n        spec_values = self.spec_values.copy()\n        c_bins = self.bins[self.bins['labels'].apply(lambda x: not x.startswith('d_'))].copy()\n        if label2 is None and not label1.startswith('d_'):  # removing bucket for continuous variable\n            c_bins = c_bins[c_bins['labels'] != label1]\n        else:\n            if not (label1.startswith('d_') and label2.startswith('d_')):\n                raise Exception('Labels should be discrete simultaneously')\n            bin1 = self.bins[self.bins['labels'] == label1]['bins'].iloc[0]\n            bin2 = self.bins[self.bins['labels'] == label2]['bins'].iloc[0]\n            spec_values[bin1] = label1 + '_' + label2\n            spec_values[bin2] = label1 + '_' + label2\n        new_woe = WoE(self.__qnt_num, self._min_block_size, spec_values, self.type, c_bins['bins'], self.t_type)\n        return new_woe.fit(self.df['X'], self.df['Y'])\n\n    def plot(self,figsize):\n        \"\"\"\n        Plot WoE transformation and default rates\n        :return: plotting object\n        \"\"\"\n        index = np.arange(self.bins.shape[0])\n        bar_width = 0.8\n        woe_fig = plt.figure(figsize = figsize)\n        plt.title('Number of Observations and WoE per bucket')\n        ax = woe_fig.add_subplot(111)\n        ax.set_ylabel('Observations')\n        plt.xticks(index + bar_width \/ 2, self.bins['labels'])\n        plt.bar(index, self.bins['obs'], bar_width, color='b', label='Observations')\n        ax2 = ax.twinx()\n        ax2.set_ylabel('Weight of Evidence')\n        ax2.plot(index + bar_width \/ 2, self.bins['woe'], 'bo-', linewidth=4.0, color='r', label='WoE')\n        handles1, labels1 = ax.get_legend_handles_labels()\n        handles2, labels2 = ax2.get_legend_handles_labels()\n        handles = handles1 + handles2\n        labels = labels1 + labels2\n        plt.legend(handles, labels)\n        woe_fig.autofmt_xdate()\n        return woe_fig\n\n    def optimize(self, criterion=None, fix_depth=None, max_depth=None, cv=3):\n        \"\"\"\n        WoE bucketing optimization (continuous variables only)\n        :param criterion: binary tree split criteria\n        :param fix_depth: use tree of a fixed depth (2^fix_depth buckets)\n        :param max_depth: maximum tree depth for a optimum cross-validation search\n        :param cv: number of cv buckets\n        :return: WoE class with optimized continuous variable split\n        \"\"\"\n        if self.t_type == 'b':\n            tree_type = tree.DecisionTreeClassifier\n        else:\n            tree_type = tree.DecisionTreeRegressor\n        m_depth = int(np.log2(self.__qnt_num))+1 if max_depth is None else max_depth\n        cont = self.df['labels'].apply(lambda z: not z.startswith('d_'))\n        x_train = np.array(self.df[cont]['X'])\n        y_train = np.array(self.df[cont]['Y'])\n        x_train = x_train.reshape(x_train.shape[0], 1)\n        start = 1\n        cv_scores = []\n        if fix_depth is None:\n            for i in range(start, m_depth):\n                if criterion is None:\n                    d_tree = tree_type(max_depth=i)\n                else:\n                    d_tree = tree_type(criterion=criterion, max_depth=i)\n                scores = cross_val_score(d_tree, x_train, y_train, cv=cv)\n                cv_scores.append(scores.mean())\n            best = np.argmax(cv_scores) + start\n        else:\n            best = fix_depth\n        final_tree = tree_type(max_depth=best)\n        final_tree.fit(x_train, y_train)\n        opt_bins = final_tree.tree_.threshold[final_tree.tree_.threshold > 0]\n        opt_bins = np.sort(opt_bins)\n        new_woe = WoE(self.__qnt_num, self._min_block_size, self.spec_values, self.type, opt_bins, self.t_type)\n        return new_woe.fit(self.df['X'], self.df['Y'])\n\n    @staticmethod\n    def _bucket_woe(x):\n        t_bad = x['bad']\n        t_good = x['good']\n        t_bad = 0.5 if t_bad == 0 else t_bad\n        t_good = 0.5 if t_good == 0 else t_good\n        return np.log(t_bad \/ t_good)\n\n","a9e44f71":"# Select the continuous variables by VI value \n\niv_n = {}\nfor col in num:\n    try:\n        iv_n[col] = WoE(v_type='c',t_type='b',qnt_num=5, spec_values={0: '0'} ).fit(data[col].copy(),data['is_canceled'].astype(int).copy()).iv \n    except ValueError:\n        try:\n            iv_n[col] = WoE(v_type='c',t_type='b',qnt_num=2, spec_values={0: '0'} ).fit(data[col].copy(),data['is_canceled'].astype(int).copy()).iv\n            print(col,'qnt_num=2')\n        except ValueError:\n            print(col)\n\nsort_iv_n = pd.Series(iv_n).sort_values(ascending=False)","e0f183dc":"# Sort the IV value and filter by 0.02\n\nvar_n_s =  list(sort_iv_n[sort_iv_n > 0.02].index)\nvar_n_s\n","3a67dd1c":"# But we also want to double check by statistics tests\n\nfrom scipy import stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.formula.api import ols\n\nsample_df = data.sample(3000) # We need to sample the data otherwise the P-value will be nonsense.\nsample_y = Y[sample_df.index]\n\nols_n = {}\nfor i in num:  #date not included\n    print(i,\":\")       \n    a = i\n    b = Y.name\n    formula = \"{} ~ C({})\".format(a,b)\n    fit = sm.stats.anova_lm(ols(formula=formula,data=sample_df).fit())\n    print(fit)\n    ols_n[i] = fit.iloc[0,4]\n    print('===============================================================')\nsort_ols_n = pd.Series(ols_n).sort_values(ascending=True)\nsort_ols_n","b9231461":"# filter by P<0.001\n\nvar_ols_n = list(sort_ols_n[sort_ols_n < 0.001].index)\nvar_ols_n","f778e21c":"# Select the categorical variables by VI value \n\niv_c = {}\nfor i in cate:\n    iv_c[i] = WoE(v_type='d').fit(X[i].copy(), Y.copy()).iv\n\n\nsort_iv_c = pd.Series(iv_c).sort_values(ascending = False)","a3c39175":"# Sort the IV value and filter by 0.02\n\nvar_c_s =  list(sort_iv_c[sort_iv_c > 0.02].index)\n\nvar_c_s","9f0ccf17":"# Tidy up and create new dataset to operate on\n\nX = data[var_c_s + var_n_s].copy()\nY = data[y].copy()\n\nX_rep = X.copy()","ff36eb5f":"# Continous Variable Exploration\n\nX_rep[var_n_s].describe().T","d64096b6":"# Check the outliers and data distribution\n\nplt.hist(X_rep['lead_time'], bins=20)","3fa27591":"plt.hist(X_rep['adr'], bins=20)","ba47948f":"X_rep['adr'].describe().T","59824f43":"# Take a look at the outliers\n\nprint(data[data['adr']>5000].T) \nprint('==========================================================')\nprint(data[data['adr']<0].T)","ff7b8394":"# Assign 0 to negative value and drop the 'over 5000' data. It's ok to not do anything here becuase we will do variable discretization later.\n\nX_rep[X_rep['adr']<0]\nX_rep.loc[14969,'adr'] = 0.0\n\nX_rep = X_rep.drop([48515])\nY = Y.drop([48515])\n\ndata_clean.append({'adr<0':'0'})\ndata_clean.append({'adr>5000':'DROP'})","426445e7":"# Take a look again\n\nfig = plt.figure()\nfor i in var_n_s:\n    print(i)\n    ax = fig.add_subplot(2,1,var_n_s.index(i)+1)\n#    plt.hist(X_rep[i], bins=20)\n    sns.distplot(X_rep[i], kde=True, fit=stats.norm, ax = ax)","c195c1f7":"# Continuous variable discretization\n\nfor i in var_n_s:\n    try:\n        X_rep[i+'_bins'] = pd.qcut(X_rep[i],5) \n        print(Y.astype('int64').groupby(X_rep[i+'_bins']).agg(['count', 'mean']))\n    except ValueError:\n        print('=======================================')\n        print(i,ValueError)\n        print('=======================================')\n\n","2cc5df18":"# append to data_clean. And map back to the dataset.\n\nfor i in var_n_s:\n    data_clean.append(X_rep[[i,i+\"_bins\"]].drop_duplicates().set_index(i).to_dict())\n    del X_rep[i]\n    X_rep.rename(columns={i+\"_bins\":i},inplace=True)","93ee148b":"# Start to explore categorical variables. \n# We try to figure out if the original groups are suitable for modelling or if we need to regoup them like what we did for agent\/country\/company.\n# So we calculate the mean(since the Y is 0 or 1, the mean is just the % of Y=1) and count(frequency) for each group\n\nvar_c_ex = {}\nfor i in var_c_s:\n    print(i,':',str(var_c_s.index(i)),':')\n    DemCluster_grp = data[[i,'is_canceled']].groupby(i,as_index = False)\n    DemC_C = DemCluster_grp['is_canceled'].agg({'mean' : 'mean',\n                                                   'count':'count'}).sort_values(\"mean\")\n    var_c_ex[var_c_s.index(i)]=DemC_C\n    print(DemC_C)\n    print('===================================')\n","ac7a60fa":"for i in [6,8,10,20]:\n    var_c_ex[i][\"count_cumsum\"]=var_c_ex[i][\"count\"].cumsum()\n    var_c_ex[i][\"new_\"+ var_c_ex[i].columns[0]] = var_c_ex[i][\"count_cumsum\"].apply(lambda x: x\/\/(len(data)\/4)).astype(int)\n    \nfor i in [1,3,9,11,12]:\n    var_c_ex[i][\"count_cumsum\"]=var_c_ex[i][\"count\"].cumsum()\n    var_c_ex[i][\"new_\"+ var_c_ex[i].columns[0]] = var_c_ex[i][\"count_cumsum\"].apply(lambda x: x\/\/(len(data)\/2)).astype(int)\n\nfor i in [7,19]:\n    var_c_ex[i][\"count_cumsum\"]=var_c_ex[i][\"count\"].cumsum()\n    var_c_ex[i][\"new_\"+ var_c_ex[i].columns[0]] = var_c_ex[i][\"count_cumsum\"].apply(lambda x: x\/\/(len(data)\/3)).astype(int)\n        \nvar_c_ex[4][\"count_cumsum\"]=var_c_ex[4][\"count\"].cumsum()\nvar_c_ex[4][\"new_\"+ var_c_ex[4].columns[0]] = var_c_ex[i][\"count_cumsum\"].apply(lambda x: x\/\/(len(data)\/7)).astype(int)\n\nvar_c_ex[15][\"count_cumsum\"]=var_c_ex[15][\"count\"].cumsum()\nvar_c_ex[15][\"new_\"+ var_c_ex[15].columns[0]] = var_c_ex[i][\"count_cumsum\"].apply(lambda x: x\/\/(len(data)\/5)).astype(int)\n\nvar_c_ex","9356b00c":"# Double check the regrouping. Unfortunatly we have to manually assign some group names.\n\nvar_c_ex[3]['new_previous_cancellations'] = [1,1,1,1,1,1,0,1,1,1,1,1,1,1,1]\nvar_c_ex[4]['new_agent'] = [1,1,1,1,1,1,1,0,2,2,2,2,2,3,4,5,6,6,6,6,6,6,6,6]\nvar_c_ex[6]['new_market_segment'][4] = 4\nvar_c_ex[7]['new_total_of_special_requests'][1] = 1\nvar_c_ex[8]['new_assigned_room_type'][11] = 3\nvar_c_ex[9]['new_booking_changes'] = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1]\nvar_c_ex[10]['new_distribution_channel'] = [0,0,1,2,2]\nvar_c_ex[11]['new_company'][[50,60,14,16,39,43]] = [0,0,0,0,0,0]\nvar_c_ex[12]['new_previous_bookings_not_canceled'][[18,39,54,43]] = [0,0,0,0]\nvar_c_ex[15]['new_stays_in_week_nights'][1] = 1\nvar_c_ex[15]['new_stays_in_week_nights'][12] = 2\nvar_c_ex[15]['new_stays_in_week_nights'][19] = 3\nvar_c_ex[15]['new_stays_in_week_nights'][0:10] = 5\nvar_c_ex[15]['new_stays_in_week_nights'][11:13] = 5\nvar_c_ex[15]['new_stays_in_week_nights'][14:17] = 5\nvar_c_ex[15]['new_stays_in_week_nights'][18:22] = 5\nvar_c_ex[15]['new_stays_in_week_nights'][23:] = 5\nvar_c_ex[19]['new_adults'] = [0,0,1,0,2,0,0,0,0,0,0,0,0,0]\nvar_c_ex[20]['new_reserved_room_type'][9] = 3","76022e42":"# add to clean data\n\nfor i in [1,3,4,6,7,8,9,10,11,12,15,19,20]:\n    data_clean.append(var_c_ex[i][[var_c_ex[i].columns[0],\"new_\"+ var_c_ex[i].columns[0]]].set_index(var_c_ex[i].columns[0]).to_dict())","383c75ef":"# map back to dataset\n\nj = 10\nfor i in [1,3,4,6,7,8,9,10,11,12,15,19,20]:\n    X_rep[var_c_ex[i].columns[0]] = X_rep[var_c_ex[i].columns[0]].map(data_clean[j][\"new_\"+ var_c_ex[i].columns[0]])\n    j = j+1","b838f909":"var_c_s = var_c_s[0:16] + var_c_s[17:]  # del arrival_date_week_number, we've already had the month attribute.","7717512b":"# Double check the data\n\nX_rep[var_c_s].describe().T","16f339b6":"# Since we transfer the continous variables to categorical ones, now we combine them together\n\nvar_c = var_c_s+var_n_s\nfor i in var_c:\n    print(i,\": \",len(X_rep[var_c][i].value_counts()))","d8119e32":"# put them in statistic test: chisq test\n\nsample_df = X_rep.sample(3000)\nsample_y = Y[sample_df.index]\n\nols_c = {}\nfor col in var_c:\n    print(col,\":\")\n    cross_table = pd.crosstab(sample_df[col],sample_y)\n\n    print(''' chisq = %6.4f \\n p-value = %6.4f \\n dof = %i \\n expected_freq = %s'''\n               %stats.chi2_contingency(cross_table))  \n    ols_c[col] = stats.chi2_contingency(cross_table)[1]\n    print('==========================================')\n        \n    \nsort_ols_c = pd.Series(ols_c).sort_values(ascending=True)\nsort_ols_c","b85c37fe":"# And filter the variables by P value\n\nvar_ols_c = list(sort_ols_c[sort_ols_c < 0.001].index)\nvar_ols_c","828f9142":"# Clear up the data\n\nX_rep = X_rep[var_ols_c]","18bd56ff":"# Transfer all the variables back to continous ones.\n\nfor i in var_ols_c:\n    X_rep[i+\"_woe\"] = WoE(v_type='d').fit_transform(X_rep[i],Y)","0b1a3529":"# append to clean data and delete the extra colomns\n\nfor i in var_ols_c:\n    data_clean.append(X_rep[[i,i+\"_woe\"]].drop_duplicates().set_index(i).to_dict())\n    del X_rep[i]\n    X_rep.rename(columns={i+\"_woe\":i},inplace=True)\n\nX_rep[var_ols_c].describe().T","26eacf80":"# Use Random Forest to get feature importance\n\nimport sklearn.ensemble as ensemble\n\nrfc = ensemble.RandomForestClassifier(criterion='entropy', n_estimators=100, max_features=0.5, min_samples_split=100)\nrfc_model = rfc.fit(X_rep, Y)\nrfc_model.feature_importances_\nrfc_fi = pd.DataFrame()\nrfc_fi[\"features\"] = list(X_rep.columns)\nrfc_fi[\"importance\"] = list(rfc_model.feature_importances_)\nrfc_fi=rfc_fi.set_index(\"features\",drop=True)\nvar_sort = rfc_fi.sort_values(by=\"importance\",ascending=False)\nvar_sort.plot(kind=\"bar\")","b2cc6c57":"# filter by 0.02\n\nvar_x = list(var_sort.importance[var_sort.importance > 0.02].index)\nvar_x","61be8c91":"# Now we get the final attributes.\n\nX_rep_reduc = X_rep[var_x].copy()\nX_rep_reduc.head()","51c84a5a":"# Split to test and train\n\nimport sklearn.model_selection as model_selection\n\nml_data = model_selection.train_test_split(X_rep_reduc, Y, test_size=0.3, random_state=2333)\ntrain_data, test_data, train_target, test_target = ml_data","4e15f80e":"from sklearn.model_selection import ParameterGrid, GridSearchCV\nimport sklearn.tree as tree\nimport sklearn.metrics as metrics\n\nparam_grid = {\n    'criterion':['entropy','gini'],\n    'max_depth':[7,8,10,12,15],\n    'min_samples_split':[10,20,50,100,200] \n}\nclf = tree.DecisionTreeClassifier(random_state = 233)\nclfcv = GridSearchCV(estimator=clf\n                     ,param_grid=param_grid\n                     ,scoring='roc_auc'\n                     ,cv=10)\n\nclfcv.fit(train_data, train_target)\n\nprint(clfcv.best_estimator_)\nprint(\"best accuracy:%f\" % clfcv.best_score_) #best accuracy:0.916438","482d9a57":"# Create confusion matrix. We focus on precision and recall value when Y = 1\n\ntrain_est  = clfcv.predict(train_data)  \ntrain_est_p= clfcv.predict_proba(train_data)[:,1]  \ntest_est   = clfcv.predict(test_data) \ntest_est_p = clfcv.predict_proba(test_data)[:,1] \n\nprint(metrics.confusion_matrix(test_target, test_est,labels=[0,1]))\nprint(metrics.classification_report(test_target, test_est))","d220ddac":"# Plot the ROC Curve\n\nfpr_test, tpr_test, th_test = metrics.roc_curve(test_target, test_est_p)\nfpr_train, tpr_train, th_train = metrics.roc_curve(train_target, train_est_p)\n\nplt.figure(figsize=[6,6])\nplt.plot(fpr_test, tpr_test, 'b-')\nplt.plot(fpr_train, tpr_train, 'r-')\nplt.title('ROC curve')\nplt.text(0.4, 0.8, 'AUC = %6.4f' %metrics.auc(fpr_test, tpr_test), ha='center')\nplt.show()\nprint('Train AUC = %6.4f' %metrics.auc(fpr_train, tpr_train)) \nprint('Test AUC = %6.4f' %metrics.auc(fpr_test, tpr_test))\n","5c1d5b92":"# How's the classification?\n\nred, blue = sns.color_palette(\"Set1\",2)\n\nsns.kdeplot(test_est_p[test_target==1], shade=True, color=red)\nsns.kdeplot(test_est_p[test_target==0], shade=True, color=blue)","b44948a5":"# Use default model\n\nrf1 = ensemble.RandomForestClassifier(oob_score=True,random_state = 233)\nrf1.fit(train_data, train_target)\n\nprint(\"accuracy:%f\"%rf1.oob_score_)","f2fc615a":"# Now we try to tune the model. And we do it step by step.\n\n# 1.test estimators\n\nparam_test1 = {\"n_estimators\":range(50,101,10)}\ngsearch1 = GridSearchCV(estimator=rf1\n                        ,param_grid=param_test1\n                        ,scoring='roc_auc'\n                        ,cv=10)\n\ngsearch1.fit(train_data, train_target)\n\nprint(gsearch1.best_params_) \nprint(\"best accuracy:%f\" % gsearch1.best_score_) ","90c4b152":"# 2.test criterion\n\nparam_test2 = {\"criterion\":['entropy','gini']}\nrf2 = ensemble.RandomForestClassifier(n_estimators = 100\n                                      ,random_state = 233)\ngsearch2 = GridSearchCV(estimator=rf2\n                        ,param_grid=param_test2\n                        ,scoring='roc_auc'\n                        ,cv=10)\n\ngsearch2.fit(train_data, train_target)\n\nprint(gsearch2.best_params_) \nprint(\"best accuracy:%f\" % gsearch2.best_score_) ","82f098bb":"# 3.test max_depth and min_sample_split\n\nparam_test3 = {'max_depth':range(11,16,2), 'min_samples_split':range(20,201,20)}\n\nrf3 = ensemble.RandomForestClassifier(n_estimators = 100\n                                      ,random_state = 233\n                                      ,criterion = 'entropy')\n\ngsearch3 = GridSearchCV(estimator = rf3\n                        ,param_grid = param_test3\n                        ,scoring='roc_auc'\n                        ,cv=10)\n\ngsearch3.fit(train_data, train_target)\ngsearch3.best_params_, gsearch3.best_score_","54636740":"# Fit test data and calculate the AUC\n\ntest_est   = gsearch3.predict(test_data)\ntest_est_p = gsearch3.predict_proba(test_data)[:,1] \ntrain_est  = gsearch3.predict(train_data)\ntrain_est_p= gsearch3.predict_proba(train_data)[:,1] \n\nfpr_test, tpr_test, th_test = metrics.roc_curve(test_target, test_est_p)\nfpr_train, tpr_train, th_train = metrics.roc_curve(train_target, train_est_p)\n\n\nprint('Train AUC = %.4f' %metrics.auc(fpr_train, tpr_train)) \nprint('Test AUC = %.4f' %metrics.auc(fpr_test, tpr_test)) \nprint(metrics.confusion_matrix(test_target, test_est, labels=[0, 1]))\n","215a898f":"# ROC Curve\n\nplt.figure(figsize=[4, 4])\nplt.plot(fpr_test, tpr_test, 'b-')\nplt.plot(fpr_train, tpr_train, 'r-')\nplt.title('ROC curve')\nplt.text(0.4, 0.8, 'AUC = %6.4f' %metrics.auc(fpr_test, tpr_test), ha='center')\nplt.show()","3ce07bbb":"# Need to scale the data first\n\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\n\ntrain_data_scale = min_max_scaler.fit_transform(train_data)\ntest_data_scale = min_max_scaler.fit_transform(test_data)","6cf6bf99":"# Fit the best model\n\nimport sklearn.linear_model as linear_model\n\nlogistic_model = linear_model.LogisticRegression(class_weight=None\n                                                 ,dual=False\n                                                 ,fit_intercept=True\n                                                 ,intercept_scaling=1\n                                                 ,penalty='l1'\n                                                 ,random_state=233\n                                                 ,solver='liblinear'\n                                                 ,tol=0.001)\nC = np.logspace(-3,1,base=10) \n\nparam_grid = {'C': C}\n\nclf_cv = GridSearchCV(estimator=logistic_model, \n                      param_grid=param_grid, \n                      cv=4, \n                      scoring='roc_auc')\n\nclf_cv.fit(train_data_scale, train_target)","85825110":"# Calculate AUC\n\nprint('C:',clf_cv.best_score_)\n\ntest_predict = clf_cv.predict(test_data_scale)\ntrain_predict = clf_cv.predict(train_data_scale)\ntest_proba = clf_cv.predict_proba(test_data_scale)[:,1]\ntrain_proba = clf_cv.predict_proba(train_data_scale)[:,1]\n\nfpr_test, tpr_test, th_test = metrics.roc_curve(test_target, test_proba)\nfpr_train, tpr_train, th_train = metrics.roc_curve(train_target, train_proba)\n\nprint('AUC = %6.4f' %metrics.auc(fpr_test, tpr_test)) ","fd34b3e2":"# ROC Curve\n\nplt.figure(figsize=[4, 4])\nplt.plot(fpr_test, tpr_test, 'b-')\nplt.plot(fpr_train, tpr_train, 'r-')\nplt.title('ROC curve')\nplt.text(0.4, 0.8, 'AUC = %6.4f' %metrics.auc(fpr_test, tpr_test), ha='center')\nplt.show()\n","11893bd0":"# Check if the logistic model is as good as trees. Can it distinguish the two class?\n\nred, blue = sns.color_palette(\"Set1\",2)\n\nsns.kdeplot(test_proba[test_target==1], shade=True, color=red)\nsns.kdeplot(test_proba[test_target==0], shade=True, color=blue)","3a1b9a33":"# Compare different threshold and different precision\/recall score.\n\nfor i in [0.25, 0.35, 0.5, 0.6, 0.75]:\n    prediction = (test_proba > i).astype('int')\n    confusion_matrix = pd.crosstab(test_target,prediction,\n                                   margins = True)\n    precision = confusion_matrix.iloc[1, 1] \/confusion_matrix.loc['All', 1]\n    recall = confusion_matrix.iloc[1, 1] \/ confusion_matrix.loc[1, 'All']\n    Specificity = confusion_matrix.iloc[0, 0] \/confusion_matrix.loc[0,'All']\n    f1_score = 2 * (precision * recall) \/ (precision + recall)\n    print('threshold: %s, precision: %.2f, recall:%.2f ,Specificity:%.2f , f1_score:%.2f'%(i, precision, recall, Specificity,f1_score))","249af820":"Lead_time seems clean.","d897f573":"### To recap, we utilised this dataset to predict future booking cancelations. \nDuring data manipulation, we discovered:\n* deposit_type\n* country\n* lead_time\n* market_segment\n* customer_type\n* is_room_changed\n* total_of_special_requests\n* required_car_parking_spaces\n* agent\n* booking_changes\nare the most relavent attributes to predict cancelation.\n\nWe visualised:\n* continuous data\n* categorical data\n* time seires\n* geo map\n* relation between attributes and Y\n\n\nWe dealed with:\n* missing data\n* outlier\n* create new features\n* regroup categorical variables\n* hyperparameter tuning\n\nAnd finnaly we use the model to make business decisions, by setting different threshold, we can have vaires of usage scenarios.\n","3447593d":"### Logistic Regression","d90d9244":"['lead_time', 'adr'] They are both important according to Information Value.","f1a2d667":"# Model Comparison","4ea1e2d5":"Only ['lead_time'] left. Okay, we will see later if 'adr' is important or not.\n\nNow let's look at the categorical variables.","97380189":"### Decision Tree","80ec4039":"# End\n\n### Feel free to contact me if you have any questions.\n### Please upvote and fork if you find this notebook useful! Many thanks","62235166":"#### We can see a positive correlation between lead_time and is_canceled, meaning the earlier people book their reservation, the more likely they will have second thoughts.","e62b1532":"### Random Forest","b61b384a":"#### Since it has too many 0, we create a new variable for it.","d3d768a8":"'adr' clearly has outliers","06496a50":"#### Now we have narrowed down both continous and categorical variables","ebbbdcae":"#### For each attribute we regroup the rest 20% of data as 'OTH', the top 80% data stays the same.","b9146a0c":"#### Both the Y = 1 and Y = 0 are classified pretty well when the probabilty is >0.8 or <0.2. But there are some blend-in in between.","cca80609":"# Load Data","09f9813b":"#### For each attribute, we group by its group, the % of Y=1 and its frequency. Sorted by cancel rate.\nAnd we get some ideas about the regrouping.\n\nThe followings are:\nvariable index, regroup num, vairable name\n* var_c_ex[1], 2, required_car_parking_spaces\n* var_c_ex[3], 2, previous_cancellations\n* var_c_ex[4], 7, agent\n* var_c_ex[6], 4, market_segment\n* var_c_ex[7], 3, total_of_special_requests\n* var_c_ex[8], 4, assigned_room_type\n* var_c_ex[9], 2, booking_changes\n* var_c_ex[10], 4, distribution_channel\n* var_c_ex[11], 2, company\n* var_c_ex[12], 2, previous_bookings_not_canceled\n* var_c_ex[15], 5, stays_in_week_nights 0 1 2 3 OTH\n* var_c_ex[16], DEL, arrival_date_week_number\n* var_c_ex[19], 3, adults\n* var_c_ex[20], 4, reserved_room_type","e5cef143":"# Feature Engineering - Create and select variables","50e7195d":"okay, these categorical variables can stay. For now.","2eed5202":"# Feature Engineering - Variable Exploration","a8afce5a":"### Only lead_time and adr are continuous variables.","079a58e6":"# Data Overview","26c0e0aa":"#### Looks like the logistic model doesn't perform as good as the trees.","1174d975":"#### Now we've transfered the continous variables to categorical variables.","bb700c06":"# Hotel Booking Data Analysis\n\n### - Data\n#### This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and\/or babies, and the number of available parking spaces, among other things.\n\n### - Question\n#### We would like to predict if a new reservation is likely to be canceled by customers and what's the probability of it? Also, how can we utilise this infomation and what can we do about it?\n\n### - Goal\n#### The aim is to do a complete data analysis including exploratory data analysis, feature engineering and finally choose the best model to solve our question by model comparison and parameter tuning.\n\n\n### - Content of this notebook\n#### This notebook is divided into 2 parts. \n#### The fist part includes data visulization, exploration and some feature engineering to help us better understand our data. Can be accessed here:https:\/\/www.kaggle.com\/feilinhu\/hotel-booking-data-visualisation\n#### This second part is the data modelling part and we will build and compare different models to satisfy different business needs.","be285f06":"### We can set our own threshold to meet business needs.\n* If the hotel want to provide better service for those who are not likely to cancel their reservation but has little resource. We can set the threshold low, so that we only focus on the 'most likely to come' customers.\n* If the hotel want to 'oversell' the rooms but doesn't want to lose credibility, we can set the threshold high:0.75, so that the precison is 99%. It's unlikely to hurt a loyal customer.\n* If the hotel want to have a double check with the customers, but doesn't know who to call, we can have the 'most likely no-show' customer list, so our personnel can call them first.\n\n#### We can utilise this model to different business scenarios instead of just output a Yes or No."}}