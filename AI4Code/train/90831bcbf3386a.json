{"cell_type":{"43cfdfc6":"code","69acea47":"code","63c63c1a":"code","2d08881f":"code","446616d5":"code","bc02b5e9":"code","42e81405":"code","1da0153b":"code","b20520cf":"code","163f01fb":"code","9318aabd":"code","689686d7":"code","d0755934":"code","7eb0fbd8":"code","aa546eb4":"code","0b56911a":"code","325beb21":"code","0af64f40":"code","9a4df429":"code","5b830dc6":"code","5a23a770":"code","ea64463b":"code","558bf142":"code","f8e129e5":"code","29993db2":"code","fa1f1225":"code","618594e1":"code","7b20e748":"code","43116909":"code","8bc26aec":"markdown","a7e5e884":"markdown","2d89adb0":"markdown","2670aa9c":"markdown","a55fc8b7":"markdown","91dff060":"markdown","94143beb":"markdown","f454f64f":"markdown","0b87ae54":"markdown"},"source":{"43cfdfc6":"kernel_mode = True\n\nimport sys\nif kernel_mode:\n    sys.path.insert(0, \"..\/input\/iterative-stratification\")\n    sys.path.insert(0, \"..\/input\/pytorch-lightning\")\n    sys.path.insert(0, \"..\/input\/gen-efficientnet-pytorch\")\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nimport math\nimport pickle\nfrom pickle import dump, load\nimport glob\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib.cm import get_cmap\nfrom matplotlib import rcParams\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder, MinMaxScaler, RobustScaler\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.manifold import TSNE\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nimport torch.optim as optim\n\nfrom torch.nn import Linear, BatchNorm1d, ReLU\nfrom torchvision import transforms\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.metrics.functional import classification\n\nimport geffnet\n\nimport cv2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.options.display.max_columns = None\nsns.set(style=\"darkgrid\")\n\nimport gc\ngc.enable()\n\nrand_seed = 1120\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"PyTorch Lightning Version: {pl.__version__}\")","69acea47":"!mkdir -p \/root\/.cache\/torch\/hub\/checkpoints\/\n!cp ..\/input\/gen-efficientnet-pretrained\/tf_efficientnet_*.pth \/root\/.cache\/torch\/hub\/checkpoints\/\n!ls -la \/root\/.cache\/torch\/hub\/checkpoints\/","63c63c1a":"model_type = \"b3\"\npretrained_model = f\"tf_efficientnet_{model_type}_ns\"\nexperiment_name = f\"deepinsight_efficientnet_v4_{model_type}\"\n\ndataset_folder = \"..\/input\/lish-moa\" if kernel_mode else \"\/workspace\/Kaggle\/MoA\/\"\n\nmodel_info = {\n    \"model_path\":\n    f\"..\/input\/deepinsight-efficientnet-v4-b3\/{experiment_name}\"\n    if kernel_mode else\n    f\"\/workspace\/Kaggle\/MoA\/completed\/deepinsight_efficientnet_v4_b3\/{experiment_name}\"\n}\n\nnum_workers = 2 if kernel_mode else 6\ngpus = [0]\n\nif model_type == \"b0\":\n    batch_size = 128\n    infer_batch_size = 256\n    image_size = 224  # B0\n    drop_rate = 0.2  # B0\n    resolution = 224\nelif model_type == \"b3\":\n    batch_size = 48\n    infer_batch_size = 512\n    image_size = 300  # B3\n    drop_rate = 0.3  # B3\n    resolution = 300\nelif model_type == \"b5\":\n    batch_size = 8\n    infer_batch_size = 16\n    image_size = 456  # B5\n    drop_rate = 0.4  # B5\n    resolution = 456\nelif model_type == \"b7\":\n    batch_size = 2\n    infer_batch_size = 4\n    # image_size = 800  # B7\n    image_size = 772  # B7\n    drop_rate = 0.5  # B7\n    resolution = 772\n\n# DeepInsight Transform\nperplexity = 5\n\ndrop_connect_rate = 0.2\nfc_size = 512\n\n# Swap Noise\nswap_prob = 0.15\nswap_portion = 0.1","2d08881f":"train_features = pd.read_csv(f\"{dataset_folder}\/train_features.csv\",\n                             engine='c')\ntrain_labels = pd.read_csv(f\"{dataset_folder}\/train_targets_scored.csv\",\n                           engine='c')\n\ntrain_extra_labels = pd.read_csv(\n    f\"{dataset_folder}\/train_targets_nonscored.csv\", engine='c')\n\ntest_features = pd.read_csv(f\"{dataset_folder}\/test_features.csv\", engine='c')\n\nsample_submission = pd.read_csv(f\"{dataset_folder}\/sample_submission.csv\",\n                                engine='c')","446616d5":"# Sort by sig_id to ensure that all row orders match\ntrain_features = train_features.sort_values(\n    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\ntrain_labels = train_labels.sort_values(by=[\"sig_id\"], axis=0,\n                                        inplace=False).reset_index(drop=True)\ntrain_extra_labels = train_extra_labels.sort_values(\n    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n\nsample_submission = sample_submission.sort_values(\n    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)","bc02b5e9":"train_features.shape, train_labels.shape, train_extra_labels.shape","42e81405":"test_features.shape","1da0153b":"category_features = [\"cp_type\", \"cp_dose\"]\nnumeric_features = [\n    c for c in train_features.columns\n    if c != \"sig_id\" and c not in category_features\n]\nall_features = category_features + numeric_features\ngene_experssion_features = [c for c in numeric_features if c.startswith(\"g-\")]\ncell_viability_features = [c for c in numeric_features if c.startswith(\"c-\")]\nlen(numeric_features), len(gene_experssion_features), len(\n    cell_viability_features)","b20520cf":"train_classes = [c for c in train_labels.columns if c != \"sig_id\"]\ntrain_extra_classes = [c for c in train_extra_labels.columns if c != \"sig_id\"]\nlen(train_classes), len(train_extra_classes)","163f01fb":"for df in [train_features, test_features]:\n    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})","9318aabd":"train_features[\"cp_type\"].value_counts()","689686d7":"train_features[\"cp_dose\"].value_counts()","d0755934":"train_features[\"cp_time\"].value_counts()","7eb0fbd8":"# Modified from DeepInsight Transform\n# https:\/\/github.com\/alok-ai-lab\/DeepInsight\/blob\/master\/pyDeepInsight\/image_transformer.py\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.manifold import TSNE\nfrom scipy.spatial import ConvexHull\nfrom matplotlib import pyplot as plt\nimport inspect\n\n\nclass DeepInsightTransformer:\n    \"\"\"Transform features to an image matrix using dimensionality reduction\n\n    This class takes in data normalized between 0 and 1 and converts it to a\n    CNN compatible 'image' matrix\n\n    \"\"\"\n    def __init__(self,\n                 feature_extractor='tsne',\n                 perplexity=30,\n                 pixels=100,\n                 random_state=None,\n                 n_jobs=None):\n        \"\"\"Generate an ImageTransformer instance\n\n        Args:\n            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n                class instance with method `fit_transform` that returns a\n                2-dimensional array of extracted features.\n            pixels: int (square matrix) or tuple of ints (height, width) that\n                defines the size of the image matrix.\n            random_state: int or RandomState. Determines the random number\n                generator, if present, of a string defined feature_extractor.\n            n_jobs: The number of parallel jobs to run for a string defined\n                feature_extractor.\n        \"\"\"\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n\n        if isinstance(feature_extractor, str):\n            fe = feature_extractor.casefold()\n            if fe == 'tsne_exact'.casefold():\n                fe = TSNE(n_components=2,\n                          metric='cosine',\n                          perplexity=perplexity,\n                          n_iter=1000,\n                          method='exact',\n                          random_state=self.random_state,\n                          n_jobs=self.n_jobs)\n            elif fe == 'tsne'.casefold():\n                fe = TSNE(n_components=2,\n                          metric='cosine',\n                          perplexity=perplexity,\n                          n_iter=1000,\n                          method='barnes_hut',\n                          random_state=self.random_state,\n                          n_jobs=self.n_jobs)\n            elif fe == 'pca'.casefold():\n                fe = PCA(n_components=2, random_state=self.random_state)\n            elif fe == 'kpca'.casefold():\n                fe = KernelPCA(n_components=2,\n                               kernel='rbf',\n                               random_state=self.random_state,\n                               n_jobs=self.n_jobs)\n            else:\n                raise ValueError((\"Feature extraction method '{}' not accepted\"\n                                  ).format(feature_extractor))\n            self._fe = fe\n        elif hasattr(feature_extractor, 'fit_transform') and \\\n                inspect.ismethod(feature_extractor.fit_transform):\n            self._fe = feature_extractor\n        else:\n            raise TypeError('Parameter feature_extractor is not a '\n                            'string nor has method \"fit_transform\"')\n\n        if isinstance(pixels, int):\n            pixels = (pixels, pixels)\n\n        # The resolution of transformed image\n        self._pixels = pixels\n        self._xrot = None\n\n    def fit(self, X, y=None, plot=False):\n        \"\"\"Train the image transformer from the training set (X)\n\n        Args:\n            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            y: Ignored. Present for continuity with scikit-learn\n            plot: boolean of whether to produce a scatter plot showing the\n                feature reduction, hull points, and minimum bounding rectangle\n\n        Returns:\n            self: object\n        \"\"\"\n        # Transpose to get (n_features, n_samples)\n        X = X.T\n\n        # Perform dimensionality reduction\n        x_new = self._fe.fit_transform(X)\n\n        # Get the convex hull for the points\n        chvertices = ConvexHull(x_new).vertices\n        hull_points = x_new[chvertices]\n\n        # Determine the minimum bounding rectangle\n        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n\n        # Rotate the matrix\n        # Save the rotated matrix in case user wants to change the pixel size\n        self._xrot = np.dot(mbr_rot, x_new.T).T\n\n        # Determine feature coordinates based on pixel dimension\n        self._calculate_coords()\n\n        # plot rotation diagram if requested\n        if plot is True:\n            # Create subplots\n            fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n            ax[0, 0].scatter(x_new[:, 0],\n                             x_new[:, 1],\n                             cmap=plt.cm.get_cmap(\"jet\", 10),\n                             marker=\"x\",\n                             alpha=1.0)\n            ax[0, 0].fill(x_new[chvertices, 0],\n                          x_new[chvertices, 1],\n                          edgecolor='r',\n                          fill=False)\n            ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n            plt.gca().set_aspect('equal', adjustable='box')\n            plt.show()\n        return self\n\n    @property\n    def pixels(self):\n        \"\"\"The image matrix dimensions\n\n        Returns:\n            tuple: the image matrix dimensions (height, width)\n\n        \"\"\"\n        return self._pixels\n\n    @pixels.setter\n    def pixels(self, pixels):\n        \"\"\"Set the image matrix dimension\n\n        Args:\n            pixels: int or tuple with the dimensions (height, width)\n            of the image matrix\n\n        \"\"\"\n        if isinstance(pixels, int):\n            pixels = (pixels, pixels)\n        self._pixels = pixels\n        # recalculate coordinates if already fit\n        if hasattr(self, '_coords'):\n            self._calculate_coords()\n\n    def _calculate_coords(self):\n        \"\"\"Calculate the matrix coordinates of each feature based on the\n        pixel dimensions.\n        \"\"\"\n        ax0_coord = np.digitize(self._xrot[:, 0],\n                                bins=np.linspace(min(self._xrot[:, 0]),\n                                                 max(self._xrot[:, 0]),\n                                                 self._pixels[0])) - 1\n        ax1_coord = np.digitize(self._xrot[:, 1],\n                                bins=np.linspace(min(self._xrot[:, 1]),\n                                                 max(self._xrot[:, 1]),\n                                                 self._pixels[1])) - 1\n        self._coords = np.stack((ax0_coord, ax1_coord))\n\n    def transform(self, X, empty_value=0):\n        \"\"\"Transform the input matrix into image matrices\n\n        Args:\n            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                where n_features matches the training set.\n            empty_value: numeric value to fill elements where no features are\n                mapped. Default = 0 (although it was 1 in the paper).\n\n        Returns:\n            A list of n_samples numpy matrices of dimensions set by\n            the pixel parameter\n        \"\"\"\n\n        # Group by location (x1, y1) of each feature\n        # Tranpose to get (n_features, n_samples)\n        img_coords = pd.DataFrame(np.vstack(\n            (self._coords, X.clip(0, 1))).T).groupby(\n                [0, 1],  # (x1, y1)\n                as_index=False).mean()\n\n        img_matrices = []\n        blank_mat = np.zeros(self._pixels)\n        if empty_value != 0:\n            blank_mat[:] = empty_value\n        for z in range(2, img_coords.shape[1]):\n            img_matrix = blank_mat.copy()\n            img_matrix[img_coords[0].astype(int),\n                       img_coords[1].astype(int)] = img_coords[z]\n            img_matrices.append(img_matrix)\n\n        return img_matrices\n\n    def fit_transform(self, X, empty_value=0):\n        \"\"\"Train the image transformer from the training set (X) and return\n        the transformed data.\n\n        Args:\n            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            empty_value: numeric value to fill elements where no features are\n                mapped. Default = 0 (although it was 1 in the paper).\n\n        Returns:\n            A list of n_samples numpy matrices of dimensions set by\n            the pixel parameter\n        \"\"\"\n        self.fit(X)\n        return self.transform(X, empty_value=empty_value)\n\n    def feature_density_matrix(self):\n        \"\"\"Generate image matrix with feature counts per pixel\n\n        Returns:\n            img_matrix (ndarray): matrix with feature counts per pixel\n        \"\"\"\n        fdmat = np.zeros(self._pixels)\n        # Group by location (x1, y1) of each feature\n        # Tranpose to get (n_features, n_samples)\n        coord_cnt = (\n            pd.DataFrame(self._coords.T).assign(count=1).groupby(\n                [0, 1],  # (x1, y1)\n                as_index=False).count())\n        fdmat[coord_cnt[0].astype(int),\n              coord_cnt[1].astype(int)] = coord_cnt['count']\n        return fdmat\n\n    @staticmethod\n    def _minimum_bounding_rectangle(hull_points):\n        \"\"\"Find the smallest bounding rectangle for a set of points.\n\n        Modified from JesseBuesking at https:\/\/stackoverflow.com\/a\/33619018\n        Returns a set of points representing the corners of the bounding box.\n\n        Args:\n            hull_points : an nx2 matrix of hull coordinates\n\n        Returns:\n            (tuple): tuple containing\n                coords (ndarray): coordinates of the corners of the rectangle\n                rotmat (ndarray): rotation matrix to align edges of rectangle\n                    to x and y\n        \"\"\"\n\n        pi2 = np.pi \/ 2.\n\n        # Calculate edge angles\n        edges = hull_points[1:] - hull_points[:-1]\n        angles = np.arctan2(edges[:, 1], edges[:, 0])\n        angles = np.abs(np.mod(angles, pi2))\n        angles = np.unique(angles)\n\n        # Find rotation matrices\n        rotations = np.vstack([\n            np.cos(angles),\n            np.cos(angles - pi2),\n            np.cos(angles + pi2),\n            np.cos(angles)\n        ]).T\n        rotations = rotations.reshape((-1, 2, 2))\n\n        # Apply rotations to the hull\n        rot_points = np.dot(rotations, hull_points.T)\n\n        # Find the bounding points\n        min_x = np.nanmin(rot_points[:, 0], axis=1)\n        max_x = np.nanmax(rot_points[:, 0], axis=1)\n        min_y = np.nanmin(rot_points[:, 1], axis=1)\n        max_y = np.nanmax(rot_points[:, 1], axis=1)\n\n        # Find the box with the best area\n        areas = (max_x - min_x) * (max_y - min_y)\n        best_idx = np.argmin(areas)\n\n        # Return the best box\n        x1 = max_x[best_idx]\n        x2 = min_x[best_idx]\n        y1 = max_y[best_idx]\n        y2 = min_y[best_idx]\n        rotmat = rotations[best_idx]\n\n        # Generate coordinates\n        coords = np.zeros((4, 2))\n        coords[0] = np.dot([x1, y2], rotmat)\n        coords[1] = np.dot([x2, y2], rotmat)\n        coords[2] = np.dot([x2, y1], rotmat)\n        coords[3] = np.dot([x1, y1], rotmat)\n\n        return coords, rotmat","aa546eb4":"class LogScaler:\n    \"\"\"Log normalize and scale data\n\n    Log normalization and scaling procedure as described as norm-2 in the\n    DeepInsight paper supplementary information.\n    \n    Note: The dimensions of input matrix is (N samples, d features)\n    \"\"\"\n    def __init__(self):\n        self._min0 = None\n        self._max = None\n\n    \"\"\"\n    Use this as a preprocessing step in inference mode.\n    \"\"\"\n    def fit(self, X, y=None):\n        # Min. of training set per feature\n        self._min0 = X.min(axis=0)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Global max. of training set from X_norm\n        self._max = X_norm.max()\n\n    \"\"\"\n    For training set only.\n    \"\"\"\n    def fit_transform(self, X, y=None):\n        # Min. of training set per feature\n        self._min0 = X.min(axis=0)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Global max. of training set from X_norm\n        self._max = X_norm.max()\n\n        # Normalized again by global max. of training set\n        return (X_norm \/ self._max).clip(0, 1)\n\n    \"\"\"\n    For validation and test set only.\n    \"\"\"\n    def transform(self, X, y=None):\n        # Adjust min. of each feature of X by _min0\n        for i in range(X.shape[1]):\n            X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Normalized again by global max. of training set\n        return (X_norm \/ self._max).clip(0, 1)","0b56911a":"class MoAImageSwapDataset(torch.utils.data.Dataset):\n    def __init__(self,\n                 features,\n                 labels,\n                 transformer,\n                 swap_prob=0.15,\n                 swap_portion=0.1):\n        self.features = features\n        self.labels = labels\n        self.transformer = transformer\n        self.swap_prob = swap_prob\n        self.swap_portion = swap_portion\n\n    def __getitem__(self, index):\n        normalized = self.features[index, :]\n\n        # Swap row features randomly\n        normalized = self.add_swap_noise(index, normalized)\n\n        normalized = np.expand_dims(normalized, axis=0)\n\n        # Note: we are setting empty_value=1 to follow the setup in the paper\n        image = self.transformer.transform(normalized, empty_value=1)[0]\n\n        # Resize to target size\n        gene_cht = cv2.resize(image, (image_size, image_size),\n                              interpolation=cv2.INTER_CUBIC)\n\n        # Convert to 3 channels\n        image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n\n        return {\"x\": image, \"y\": self.labels[index, :]}\n\n    def add_swap_noise(self, index, X):\n        if np.random.rand() < self.swap_prob:\n            swap_index = np.random.randint(self.features.shape[0], size=1)[0]\n            # Select only gene expression and cell viability features\n            swap_features = np.random.choice(\n                np.array(range(3, self.features.shape[1])),\n                size=int(self.features.shape[1] * self.swap_portion),\n                replace=False)\n            X[swap_features] = self.features[swap_index, swap_features]\n\n        return X\n\n    def __len__(self):\n        return self.features.shape[0]","325beb21":"class MoAImageDataset(torch.utils.data.Dataset):\n    def __init__(self, features, labels, transformer):\n        self.features = features\n        self.labels = labels\n        self.transformer = transformer\n\n    def __getitem__(self, index):\n        normalized = self.features[index, :]\n        normalized = np.expand_dims(normalized, axis=0)\n\n        # Note: we are setting empty_value=1 to follow the setup in the paper\n        image = self.transformer.transform(normalized, empty_value=1)[0]\n\n        # Resize to target size\n        gene_cht = cv2.resize(image, (image_size, image_size),\n                              interpolation=cv2.INTER_CUBIC)\n\n        # Convert to 3 channels\n        image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n\n        return {\"x\": image, \"y\": self.labels[index, :]}\n\n    def __len__(self):\n        return self.features.shape[0]\n\n\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, features, labels, transformer):\n        self.features = features\n        self.labels = labels\n        self.transformer = transformer\n\n    def __getitem__(self, index):\n        normalized = self.features[index, :]\n        normalized = np.expand_dims(normalized, axis=0)\n\n        # Note: we are setting empty_value=1 to follow the setup in the paper\n        image = self.transformer.transform(normalized, empty_value=1)[0]\n\n        # Resize to target size\n        gene_cht = cv2.resize(image, (image_size, image_size),\n                              interpolation=cv2.INTER_CUBIC)\n\n        # Convert to 3 channels\n        image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n\n        return {\"x\": image, \"y\": -1}\n\n    def __len__(self):\n        return self.features.shape[0]","0af64f40":"# Reference: https:\/\/github.com\/rwightman\/gen-efficientnet-pytorch\/blob\/master\/geffnet\/efficientnet_builder.py#L672\ndef initialize_weight_goog(m, n='', fix_group_fanout=True):\n    # weight init as per Tensorflow Official impl\n    # https:\/\/github.com\/tensorflow\/tpu\/blob\/master\/models\/official\/mnasnet\/mnasnet_model.py\n    if isinstance(m, nn.Conv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out \/\/= m.groups\n        m.weight.data.normal_(0, math.sqrt(2.0 \/ fan_out))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        fan_out = m.weight.size(0)  # fan-out\n        fan_in = 0\n        if 'routing_fn' in n:\n            fan_in = m.weight.size(1)\n        init_range = 1.0 \/ math.sqrt(fan_in + fan_out)\n        m.weight.data.uniform_(-init_range, init_range)\n        m.bias.data.zero_()\n\n\ndef initialize_weight_default(m, n=''):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        nn.init.kaiming_uniform_(m.weight,\n                                 mode='fan_in',\n                                 nonlinearity='linear')","9a4df429":"class MoAEfficientNet(pl.LightningModule):\n    def __init__(\n            self,\n            pretrained_model_name,\n            training_set=(None, None),  # tuple\n            valid_set=(None, None),  # tuple\n            test_set=None,\n            transformer=None,\n            num_classes=206,\n            in_chans=3,\n            drop_rate=0.,\n            drop_connect_rate=0.,\n            fc_size=512,\n            learning_rate=1e-3,\n            weight_init='goog'):\n        super(MoAEfficientNet, self).__init__()\n\n        self.train_data, self.train_labels = training_set\n        self.valid_data, self.valid_labels = valid_set\n        self.test_data = test_set\n        self.transformer = transformer\n\n        self.backbone = getattr(geffnet, pretrained_model)(\n            pretrained=True,\n            in_chans=in_chans,\n            drop_rate=drop_rate,\n            drop_connect_rate=drop_connect_rate,\n            weight_init=weight_init)\n\n        self.backbone.classifier = nn.Sequential(\n            nn.Linear(self.backbone.classifier.in_features, fc_size,\n                      bias=True), nn.ELU(),\n            nn.Linear(fc_size, num_classes, bias=True))\n\n        if self.training:\n            for m in self.backbone.classifier.modules():\n                initialize_weight_goog(m)\n\n        # Save passed hyperparameters\n        self.save_hyperparameters(\"pretrained_model_name\", \"num_classes\",\n                                  \"in_chans\", \"drop_rate\", \"drop_connect_rate\",\n                                  \"weight_init\", \"fc_size\", \"learning_rate\")\n\n    def forward(self, x):\n        return self.backbone(x)\n\n    def training_step(self, batch, batch_idx):\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n        x = x.float()\n        y = y.type_as(x)\n        logits = self(x)\n\n        loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"mean\")\n\n        self.log('train_loss',\n                 loss,\n                 on_step=True,\n                 on_epoch=True,\n                 prog_bar=True,\n                 logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n        x = x.float()\n        y = y.type_as(x)\n        logits = self(x)\n\n        val_loss = F.binary_cross_entropy_with_logits(logits,\n                                                      y,\n                                                      reduction=\"mean\")\n\n        self.log('val_loss',\n                 val_loss,\n                 on_step=True,\n                 on_epoch=True,\n                 prog_bar=True,\n                 logger=True)\n\n        return val_loss\n\n    def test_step(self, batch, batch_idx):\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n        x = x.float()\n        y = y.type_as(x)\n        logits = self(x)\n        return {\"pred_logits\": logits}\n\n    def test_epoch_end(self, output_results):\n        all_outputs = torch.cat([out[\"pred_logits\"] for out in output_results],\n                                dim=0)\n        print(\"Logits:\", all_outputs)\n        pred_probs = F.sigmoid(all_outputs).detach().cpu().numpy()\n        print(\"Predictions: \", pred_probs)\n        return {\"pred_probs\": pred_probs}\n\n    def setup(self, stage=None):\n        #         self.train_dataset = MoAImageDataset(self.train_data,\n        #                                              self.train_labels,\n        #                                              self.transformer)\n        self.train_dataset = MoAImageSwapDataset(self.train_data,\n                                                 self.train_labels,\n                                                 self.transformer,\n                                                 swap_prob=swap_prob,\n                                                 swap_portion=swap_portion)\n\n        self.val_dataset = MoAImageDataset(self.valid_data, self.valid_labels,\n                                           self.transformer)\n\n        self.test_dataset = TestDataset(self.test_data, None, self.transformer)\n\n    def train_dataloader(self):\n        train_dataloader = DataLoader(self.train_dataset,\n                                      batch_size=batch_size,\n                                      shuffle=True,\n                                      num_workers=num_workers,\n                                      pin_memory=True,\n                                      drop_last=False)\n        print(f\"Train iterations: {len(train_dataloader)}\")\n        return train_dataloader\n\n    def val_dataloader(self):\n        val_dataloader = DataLoader(self.val_dataset,\n                                    batch_size=infer_batch_size,\n                                    shuffle=False,\n                                    num_workers=num_workers,\n                                    pin_memory=True,\n                                    drop_last=False)\n        print(f\"Validate iterations: {len(val_dataloader)}\")\n        return val_dataloader\n\n    def test_dataloader(self):\n        test_dataloader = DataLoader(self.test_dataset,\n                                     batch_size=infer_batch_size,\n                                     shuffle=False,\n                                     num_workers=num_workers,\n                                     pin_memory=True,\n                                     drop_last=False)\n        print(f\"Test iterations: {len(test_dataloader)}\")\n        return test_dataloader\n\n    def configure_optimizers(self):\n        print(f\"Initial Learning Rate: {self.hparams.learning_rate:.6f}\")\n        optimizer = optim.Adam(self.parameters(),\n                               lr=self.hparams.learning_rate,\n                               weight_decay=weight_decay)\n\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                         T_max=T_max,\n                                                         eta_min=0,\n                                                         last_epoch=-1)\n\n        return [optimizer], [scheduler]","5b830dc6":"kfolds = 10\nskf = MultilabelStratifiedKFold(n_splits=kfolds,\n                                shuffle=True,\n                                random_state=rand_seed)\n\nlabel_counts = np.sum(train_labels.drop(\"sig_id\", axis=1), axis=0)\ny_labels = label_counts.index.tolist()","5a23a770":"def get_model(model_path, test_set, transformer):\n    model = MoAEfficientNet.load_from_checkpoint(\n        model_path,\n        pretrained_model_name=pretrained_model,\n        training_set=(None, None),  # tuple\n        valid_set=(None, None),  # tuple\n        test_set=test_set,\n        transformer=transformer,\n        drop_rate=drop_rate,\n        drop_connect_rate=drop_connect_rate,\n        fc_size=fc_size,\n        weight_init='goog')\n\n    model.freeze()\n    model.eval()\n    return model\n\n\ndef save_pickle(obj, model_output_folder, fold_i, name):\n    dump(obj, open(f\"{model_output_folder}\/fold{fold_i}_{name}.pkl\", 'wb'),\n         pickle.HIGHEST_PROTOCOL)\n\n\ndef load_pickle(model_output_folder, fold_i, name):\n    return load(open(f\"{model_output_folder}\/fold{fold_i}_{name}.pkl\", 'rb'))","ea64463b":"def mean_logloss(y_pred, y_true):\n    logloss = (1 - y_true) * np.log(1 - y_pred +\n                                    1e-15) + y_true * np.log(y_pred + 1e-15)\n    return np.mean(-logloss)","558bf142":"# Ensure Reproducibility\nseed_everything(rand_seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nkfold_submit_preds = np.zeros((test_features.shape[0], len(train_classes)))\nfor i, (train_index, val_index) in enumerate(\n        skf.split(train_features, train_labels[y_labels])):\n    print(f\"Inferencing on Fold {i} ......\")\n    print(train_index.shape, val_index.shape)\n\n    model_path = glob.glob(\n        f'{model_info[\"model_path\"]}\/fold{i}\/epoch*.ckpt')[0]\n\n    test = test_features[all_features].copy().values\n\n    # Load LogScaler (Norm-2 Normalization)\n    scaler = load_pickle(model_info[\"model_path\"], i, \"log-scaler\")\n    test = scaler.transform(test)\n\n    # Load DeepInsight Feature Map\n    transformer = load_pickle(model_info[\"model_path\"], i,\n                              \"deepinsight-transform\")\n\n    print(f\"Loading model from {model_path}\")\n    model = get_model(model_path, test_set=test, transformer=transformer)\n\n    trainer = Trainer(\n        logger=False,\n        gpus=gpus,\n        distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n        precision=16,\n        benchmark=False,\n        deterministic=True)\n    output = trainer.test(model, verbose=False)[0]\n    submit_preds = output[\"pred_probs\"]\n    kfold_submit_preds += submit_preds \/ kfolds\n\n    del model, trainer, scaler, transformer, test\n    torch.cuda.empty_cache()\n    gc.collect()","f8e129e5":"oof_predictions = np.load(\n    glob.glob(f'{model_info[\"model_path\"]}\/..\/oof*.npy')[0])\noof_loss = mean_logloss(oof_predictions, train_labels[train_classes].values)\nprint(f\"OOF Validation Loss: {oof_loss:.6f}\")","29993db2":"print(kfold_submit_preds.shape)\nkfold_submit_preds","fa1f1225":"np.mean(kfold_submit_preds)","618594e1":"submission = pd.DataFrame(data=test_features[\"sig_id\"].values,\n                          columns=[\"sig_id\"])\nsubmission = submission.reindex(columns=[\"sig_id\"] + train_classes)\nsubmission[train_classes] = kfold_submit_preds.clip(0, 1)\n# Set control type to 0 as control perturbations have no MoAs\nsubmission.loc[test_features['cp_type'] == 0, submission.columns[1:]] = 0\nsubmission.to_csv('submission.csv', index=False)","7b20e748":"submission","43116909":"torch.cuda.empty_cache()\ngc.collect()","8bc26aec":"## Implementation\n\nCheckout <a href=\"https:\/\/static-content.springer.com\/esm\/art%3A10.1038%2Fs41598-019-47765-6\/MediaObjects\/41598_2019_47765_MOESM1_ESM.pdf\" target=\"_blank\">DeepInsight paper supplementary information<\/a> for more details.","a7e5e884":"# Feature Encoding\nAs we only have three metadata features, a quick manual encoding process is done. All features are normalized into the value range of [0, 1].","2d89adb0":"<center>\n    <h1>DeepInsight EfficientNet-B3 NoisyStudent with PyTorch Lightning<\/h1>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-markpeng\/MoA\/pytorch_lightning.png\" alt=\"Pytorch Ligntning\" width=\"300\"\/>\n<\/center>\n<br\/>\n\nThis notebook demonstrates the inference of a fine-tuned EfficientNet-B3 NoisyStudent model using transformed 2D feature map images of MoA dataset. The model was trained under PyTorch Lightning architecture. The model files have been kept in private as a spirit of fair competition.\n\nFor more details about <em>DeepInsight Tranformation<\/em>, checkout the following notebook:\n\n<a href=\"https:\/\/www.kaggle.com\/markpeng\/deepinsight-transform\" target=\"_blank\">https:\/\/www.kaggle.com\/markpeng\/deepinsight-transform<\/a>\n\n10-folds Cross Validation Loss: **0.014932**\n\n***\n\nPlease upvote or cite this notebook if you like it, thanks!\n\n***\n\n**Reference:**\n\nSharma, Alok, Edwin Vans, D. Shigemizu, Keith A. Boroevich and T. Tsunoda (2019). \"_DeepInsight: A methodology to transform a non-image data to an image for convolution neural network architecture_,\" Scientific Reports, nature.com.","2670aa9c":"# Inference","a55fc8b7":"# Model Definition","91dff060":"# DeepInsight Transform - t-SNE 2D Embeddings\nBased on https:\/\/github.com\/alok-ai-lab\/DeepInsight, but with some corrections to the norm-2 normalization.\n\nMost of the credits should be given to the original authors!","94143beb":"# Dataset","f454f64f":"# EOF\nThanks for reading through this notebook.","0b87ae54":"# Load MoA Data"}}