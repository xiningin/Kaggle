{"cell_type":{"041ad586":"code","87deba3a":"code","c8fde6a9":"code","62078208":"code","347f76fe":"code","298e1592":"code","81d68194":"code","2c3e2aec":"code","61980e92":"code","9cdc6e10":"code","d59b3f0d":"code","3c663701":"code","93ec4aee":"code","36675c68":"code","a11b0705":"code","830f9a55":"code","7034445a":"code","88a64c9e":"code","791cbe1d":"code","cf5357c6":"code","efac6d4c":"code","16035f5b":"code","852823a2":"markdown","bb91d5c2":"markdown"},"source":{"041ad586":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport tensorflow as tf\nfrom keras.utils.data_utils import get_file\nimport tarfile\n\nseed = 42\nnp.random.seed(seed)","87deba3a":"plot_genre=pd.read_csv('..\/input\/plots.txt', sep='\\t', lineterminator='\\n')","c8fde6a9":"plot_genre.head()","62078208":"print(plot_genre.shape)\nplot_genre.head()","347f76fe":"plot_genre.dropna(inplace=True)","298e1592":"plot1=\"adventure|science fiction|detective\" # select 3 genres to generate \nplots=plot_genre[plot_genre['genres'].str.contains(plot1)]['plot']\nprint(plots.shape)","81d68194":"mask = (plots.str.len() <= 1000) # drop plots that have more than 1000 chars\nnot_huge_plots = plots.loc[mask]\n(not_huge_plots).shape","2c3e2aec":"strings=not_huge_plots.values.T.tolist()\ntext1 = ''.join(str(e) for e in strings)","61980e92":"print ('Length of text: {} characters'.format(len(text1)))","9cdc6e10":"#modified from https:\/\/github.com\/enriqueav\/lstm_lyrics\/blob\/master\/lstm_train_embedding.py\nfrom __future__ import print_function\nfrom keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\nimport numpy as np\nimport random\nimport sys\nimport io\nimport os","d59b3f0d":"# Parameters: change to experiment different configurations\nSEQUENCE_LEN = 30\nMIN_WORD_FREQUENCY = 10\nSTEP = 1\nBATCH_SIZE = 1024\n\n\ndef shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n    # shuffle at unison\n    print('Shuffling sentences')\n\n    tmp_sentences = []\n    tmp_next_word = []\n    \n    for i in np.random.RandomState(seed=42).permutation(len(sentences_original)):\n        tmp_sentences.append(sentences_original[i])\n        tmp_next_word.append(next_original[i])\n\n    cut_index = int(len(sentences_original) * (1.-(percentage_test\/100.)))\n    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n\n    print(\"Size of training set = %d\" % len(x_train))\n    print(\"Size of test set = %d\" % len(y_test))\n    return (x_train, y_train), (x_test, y_test)\n\n\n# Data generator for fit and evaluate\ndef generator(sentence_list, next_word_list, batch_size):\n    index = 0\n    while True:\n        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n        y = np.zeros((batch_size), dtype=np.int32)\n        for i in range(batch_size):\n            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n                x[i, t] = word_indices[w]\n            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n            index = index + 1\n        yield x, y\n\n\ndef get_model(dropout=0.2):\n    print('Build model...')\n    model = Sequential()\n    model.add(Embedding(input_dim=len(words), output_dim=1024))\n    model.add(Bidirectional(LSTM(256, return_sequences=False)))\n    if dropout > 0:\n        model.add(Dropout(dropout))\n    model.add(Dense(len(words)))\n    model.add(Activation('softmax'))\n    return model\n\n\n# Functions from keras-team\/keras\/blob\/master\/examples\/lstm_text_generation.py\ndef sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)","3c663701":"#os.listdir('..\/working\/checkpoints\/')","93ec4aee":"corpus = text1\n#examples = '..\/working\/examples.txt'\n\nif not os.path.isdir('.\/checkpoints\/'):\n    os.makedirs('.\/checkpoints\/')\nfile = open(\"..\/working\/checkpoints\/examples\", \"w\") \nfile.close() \n\ntext = text1.replace('\\n', ' \\n ')\nprint('Corpus length in characters:', len(text))","36675c68":"text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\nprint('Corpus length in words:', len(text_in_words))","a11b0705":"word_freq = {}\nfor word in text_in_words:\n    word_freq[word] = word_freq.get(word, 0) + 1\nwords = set(text_in_words)\nprint('Unique words:', len(words))","830f9a55":"word_indices = dict((c, i) for i, c in enumerate(words))\nindices_word = dict((i, c) for i, c in enumerate(words))","7034445a":"# cut the text in semi-redundant sequences of SEQUENCE_LEN words\nsentences = []\nnext_words = []\nignored = 0\nfor i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n        next_words.append(text_in_words[i + SEQUENCE_LEN])\nprint('All sequences:', len(sentences))","88a64c9e":"# x, y, x_test, y_test\n(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words)\n\nmodel = get_model()\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n\nfile_path = \"..\/working\/checkpoints\/examples\"\nmodel.summary()","791cbe1d":"checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n#print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n#early_stopping = EarlyStopping(monitor='val_acc', patience=50)\ncallbacks_list = [checkpoint]","cf5357c6":"h1=model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n                    steps_per_epoch=int(len(sentences)\/BATCH_SIZE) + 1,\n                    epochs=50,\n                    callbacks=callbacks_list,\n                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n                    validation_steps=int(len(sentences_test)\/BATCH_SIZE) + 1)","efac6d4c":"# generate text\nfor diversity in [0.2, 0.5, 1.0, 1.2]:\n    seed_index = np.random.randint(len(sentences+sentences_test))\n    seed = (sentences+sentences_test)[seed_index]\n    sentence=seed\n    print('----- diversity:', diversity)\n\n    print('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n    #print(' '.join(sentence))\n\n    for i in range(200):\n        x_pred = np.zeros((1, SEQUENCE_LEN))\n        for t, word in enumerate(sentence):\n            x_pred[0, t] = word_indices[word]\n\n        preds = model.predict(x_pred, verbose=0)[0]\n        next_index = sample(preds, diversity)\n        next_word = indices_word[next_index]\n\n        sentence = sentence[1:]\n        sentence.append(next_word)\n\n\n        sys.stdout.write(\" \"+next_word)\n        sys.stdout.flush()\n    print()\n    print()","16035f5b":"model.save_weights(file_path) # save weights for later use\n#os.listdir('..\/working\/checkpoints\/')\nstatinfo = os.stat('..\/working\/checkpoints\/examples')\nsize_w=statinfo.st_size\/1024\/1024\nprint(size_w)","852823a2":"# WORD level, no encoder decoder","bb91d5c2":"# to be continued"}}