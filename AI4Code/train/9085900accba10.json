{"cell_type":{"23fb5b38":"code","04b5e990":"code","eaaea1b9":"code","b19def12":"code","04971b19":"code","c4097fb7":"code","daad467f":"code","11c80dc6":"code","c47ce23f":"code","507f33af":"code","d43a3b7b":"code","f53702e9":"code","ffae4cb3":"code","e56ac312":"code","e84e2d6f":"code","da6b29ad":"code","0d14c3ff":"code","ae655a6b":"code","cc57049b":"code","e172754b":"code","b59d61bd":"code","0a4ea9b8":"code","b0804994":"code","5f8adc2b":"code","b9436692":"code","be56122f":"code","3f9c26e9":"code","d04fb71c":"code","ef2c5a0c":"code","507bdb51":"code","776c119a":"code","eb1b87cb":"code","91489678":"code","ebf4fa81":"code","261887b8":"code","c103adac":"code","0ae201e8":"code","8b86f82e":"code","8e4c2c16":"code","5950dbda":"code","be929033":"code","aa471a0e":"code","a83f9e91":"code","a4a62ea6":"code","48bd3cdb":"code","accc30d1":"code","e708dfd4":"code","4e1b8b25":"code","9b060819":"code","6ba1a0bc":"code","a468259b":"code","8fccabea":"markdown","bcd34826":"markdown","fded7582":"markdown","55835bcf":"markdown","ccb7bd15":"markdown","e606efd8":"markdown","d529019b":"markdown","bf202656":"markdown","fb9b0f79":"markdown","166ede0c":"markdown","901c9ff0":"markdown","49c7397a":"markdown","48a15962":"markdown","220bf892":"markdown","a4f3887d":"markdown","61ce647d":"markdown","8ba77549":"markdown","37cc8bbc":"markdown","4f1fc570":"markdown","ebacfe61":"markdown","0e0df1e8":"markdown","f89bbee5":"markdown","496f50c7":"markdown","b417ef20":"markdown","eb4c9083":"markdown","379f2f78":"markdown","db86b365":"markdown","900f5e5f":"markdown","55bd8391":"markdown","58b31401":"markdown","d2769f34":"markdown"},"source":{"23fb5b38":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","04b5e990":"df_samples = pd.read_csv(\"data\/EEG_data.csv\")\ndf_people = pd.read_csv(\"data\/datasets_106_24522_demographic_info.csv\")","eaaea1b9":"print(f\"EGG Brainwave samples shape: {df_samples.shape}\")\nprint(f\"People info: {df_people.shape}\")","b19def12":"df_samples.head(5)","04971b19":"df_people.head(5)","c4097fb7":"df_samples.info()","daad467f":"df_people.info()","11c80dc6":"#Dataframes Inner Join\nEGG_dataset = pd.merge(left=df_samples, right=df_people, left_on='SubjectID', right_on='subject ID')\n#duplicated column drop\nEGG_dataset.drop(columns='SubjectID', inplace=True)","c47ce23f":"print(f\"Complete dataset shape: {EGG_dataset.shape}\")","507f33af":"EGG_dataset.head(5)","d43a3b7b":"EGG_dataset.isnull().sum()","f53702e9":"from sklearn.impute import KNNImputer\n\ndef impute_dataset(df):\n    imputer = KNNImputer(missing_values=np.nan)\n    ds_idxs = df.index\n    ds_cols = df.columns \n    df = pd.DataFrame(imputer.fit_transform(df), index=ds_idxs, columns=ds_cols)\n    return df","ffae4cb3":"import numbers\n\ndef encode_categorical_features(df):\n    '''\n    This function encodes features with non numerical values.\n    Features with two values are incoded into 0 an 1 (binaries).\n    Features with more than two non numerical values are one-hot encoded with dummies\n    '''\n    to_binaries = []\n    to_encode = []\n    \n    for feature in df.columns:\n        values = df[feature].unique()\n        values = [x for x in values if not pd.isnull(x)]\n        if not all(isinstance(value, numbers.Number) for value in values):\n            if len(values) == 2:\n                to_binaries.append(feature)\n            else:\n                to_encode.append(feature)\n\n    for binary in to_binaries:\n        values = df[binary].unique()\n        values = [x for x in values if not pd.isnull(x)]\n        df[binary] = df[binary].map(lambda x: 0 if x == values[0] else 1 if x == values[1] else np.nan)\n\n    df = pd.get_dummies(df, columns=to_encode)\n    \n    return df","e56ac312":"encoded_df = encode_categorical_features(EGG_dataset)","e84e2d6f":"encoded_df.head(5)","da6b29ad":"selected_df = encoded_df.drop(columns=['VideoID', 'predefinedlabel'])","0d14c3ff":"selected_df.head(5)","ae655a6b":"from sklearn.preprocessing import StandardScaler\n\ndef scale_dataset(df, scaler=None):\n    ds_idxs = df.index\n    ds_cols = df.columns\n    \n    if scaler == None:\n        scaler = StandardScaler()\n        scaler = scaler.fit(df.values)\n        \n    df = pd.DataFrame(scaler.transform(df.values), index=ds_idxs, columns=ds_cols)\n    return df, scaler","cc57049b":"scaled_df, scaler = scale_dataset(selected_df)","e172754b":"scaled_df.head(5)","b59d61bd":"from sklearn.decomposition import PCA\n\ndef do_pca(df, n_components = None, pca=None):\n    if pca == None:\n        if n_components == None: \n            pca = PCA()\n        else:\n            pca = PCA(n_components=n_components)\n            \n    df_reduced = pca.fit_transform(df)\n    return pca, df_reduced","0a4ea9b8":"def pca_variance_plot(variance): #function inspired by the one used in an excercise of the lessons.\n    n_components = len(variance)\n    idxs = np.arange(n_components)\n \n    plt.figure(figsize=(20, 10))\n    ax = plt.subplot(111)\n    cumvals = np.cumsum(variance)\n    ax.bar(idxs, variance)\n    ax.plot(idxs, cumvals)\n \n    ax.xaxis.set_tick_params(width=2)\n    ax.yaxis.set_tick_params(width=5, length=20)\n \n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Variance Explained\")\n    plt.title('Explained Variance Per Principal Component')","b0804994":"pca, dataset_reduct = do_pca(scaled_df)","5f8adc2b":"pca_variance_plot(pca.explained_variance_ratio_)","b9436692":"def pca_results(full_dataset, pca): #This function has taken from an excercise of the PCA lessons (in helper_functions.py)\n    '''\n    Create a DataFrame of the PCA results\n    Includes dimension feature weights and explained variance\n    Visualizes the PCA results\n    '''\n    # Dimension indexing\n    dimensions = dimensions = ['{}'.format(i) for i in range(1,len(pca.components_)+1)]\n    # PCA components\n    components = pd.DataFrame(np.round(pca.components_, 4), columns = full_dataset.keys())\n    components.index = dimensions\n    # PCA explained variance\n    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n    variance_ratios.index = dimensions\n    # Create a bar plot visualization\n    fig, ax = plt.subplots(figsize = (14,8))\n    # Plot the feature weights as a function of the components\n    components.plot(ax = ax, kind = 'bar');\n    ax.set_ylabel(\"Feature Weights\")\n    ax.set_xticklabels(dimensions, rotation=0)\n    # Display the explained variance ratios\n    for i, ev in enumerate(pca.explained_variance_ratio_):\n        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Expl Var\\n          %.4f\"%(ev))\n    # Return a concatenated DataFrame\n    return pd.concat([variance_ratios, components], axis = 1)","be56122f":"pca_res = pca_results(scaled_df, pca)","3f9c26e9":"pca_res = pd.DataFrame(pca_res)\ndisplay(pca_res)","d04fb71c":"reduced_df = scaled_df.drop(columns=[' age', ' gender', ' ethnicity_Bengali',' ethnicity_English', ' ethnicity_Han Chinese'])","ef2c5a0c":"data_subset = {}\nfor v in reduced_df['user-definedlabeln'].unique():\n    data_subset[v] = reduced_df[reduced_df['user-definedlabeln'] == v]","507bdb51":"print(data_subset.keys())","776c119a":"data_subset[0.975097266665175].groupby(['subject ID']).agg('mean')","eb1b87cb":"data_subset[-1.0255387171989436].groupby(['subject ID']).agg('mean')","91489678":"reduced_df['subject ID'].unique()","ebf4fa81":"reduced_df = reduced_df[reduced_df['subject ID'] != -0.8681212082604366]","261887b8":"reduced_df = reduced_df[reduced_df['subject ID'] != 0.5279122818574891]","c103adac":"reduced_df['subject ID'].unique()","0ae201e8":"reduced_df.drop(columns=['subject ID'], inplace=True)","8b86f82e":"reduced_df.head(5)","8e4c2c16":"from sklearn.impute import KNNImputer\nimport numbers\n\ndef impute_dataset(df):\n    imputer = KNNImputer(missing_values=np.nan)\n    ds_idxs = df.index\n    ds_cols = df.columns \n    df = pd.DataFrame(imputer.fit_transform(df), index=ds_idxs, columns=ds_cols)\n    return df\n\ndef remove_outliers(EGG_data_df):\n    EGG_data_df = EGG_data_df[EGG_data_df['SubjectID'] != 2] #remove outlier student 3\n    EGG_data_df = EGG_data_df[EGG_data_df['SubjectID'] != 6] #remove outlier student 7\n    return EGG_data_df\n\ndef preprocess_data(EGG_data_df):\n    \n    EGG_data_df = impute_dataset(EGG_data_df)\n    EGG_data_df = remove_outliers(EGG_data_df)\n    EGG_data_df.drop(columns=['VideoID', 'predefinedlabel', 'SubjectID'], inplace=True)\n    return EGG_data_df\n    ","5950dbda":"dataset = pd.read_csv(\"data\/EEG_data.csv\")\ndataset = preprocess_data(dataset)","be929033":"dataset.head(5)","aa471a0e":"# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\ny = dataset['user-definedlabeln']\nX = dataset.drop(columns=['user-definedlabeln'])\n\n# Split the 'features' and 'income' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","a83f9e91":"from sklearn.metrics import fbeta_score, accuracy_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\n\n\ndef get_best_estimator(learner, hyperparameters_combinations, X_train, y_train):\n    '''\n    This function takes a classifier and a combination of parameters.\n    It returns a model tuned by the hyperparameters combination.\n    '''\n    #Get a scorer for Grid Search\n    scorer = make_scorer(fbeta_score, beta=0.5)\n    #Perform grid search on the classifier using 'scorer' as the scoring method \n    grid_obj = GridSearchCV(learner, hyperparameters_combinations, scoring=scorer)\n    #Fit the grid search object to the training data and find the optimal parameters\n    grid_fit = grid_obj.fit(X_train, y_train)\n    # Get the nest estimator\n    learner = grid_fit.best_estimator_\n    print(f\"Best params: {grid_fit.best_params_}\")\n    #return\n    return learner","a4a62ea6":"from time import time\n\ndef train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - hyperparameters_combinations: the dictionary containing hyperparameters possible values, for GridSearch Tuning\n       - sample_size: the size of samples (number) to be drawn from training set\n       - X_train: features training set\n       - y_train: income training set\n       - X_test: features testing set\n       - y_test: income testing set\n    '''\n    results = {}\n    \n    #Fit the learner to the training data again in order to record the training time\n    start = time() # Get start time\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time() # Get end time\n    \n    #Calculate the training time\n    results['train_time'] = end - start\n        \n    # Get the predictions on the test set(X_test),\n    #then get predictions on the first 300 training samples(X_train)\n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:300])\n    if learner.__class__.__name__ == 'XGBClassifier':\n        predictions_test = [round(value) for value in predictions_test]\n        predictions_train = [round(value) for value in predictions_train]\n    end = time() # Get end time\n    \n    #Calculate the total prediction time\n    results['pred_time'] = end - start\n   \n    # Compute accuracy on the first 300 training samples which is y_train[:300]\n    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)\n   \n    #Compute accuracy on test set using accuracy_score()\n    results['acc_test'] = accuracy_score(y_test, predictions_test)\n    \n    #Compute F-score on the the first 300 training samples using fbeta_score()\n    results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta=0.5)\n    \n    #Compute F-score on the test set which is y_test\n    results['f_test'] = fbeta_score(y_test, predictions_test, beta=0.5)\n        \n    # Success\n    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n    print('Classifier {} Accuracy {} f_score {}'.format(learner.__class__.__name__, results['acc_test'], results['f_test']))\n          \n    # Return the results\n    return results","48bd3cdb":"#Import the models from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\n\n#Initialize classifiers\nclf_A = DecisionTreeClassifier(random_state = 0)\nparameters_A = {'max_depth': list(range(4, 20, 2)),\n                'min_samples_split': list(range(2, 15, 2)),\n                'min_samples_leaf': list(range(1,  20, 2))\n               }\nclf_B = AdaBoostClassifier(random_state = 0)\nparameters_B = {'algorithm':['SAMME','SAMME.R'],\n                'n_estimators':[10, 40, 60, 100, 120, 130, 140]\n               }\nclf_C = SVC(random_state = 0)\nparameters_C = {'kernel': ['rbf'],\n                'C': list(np.arange(0.5, 1.5, 0.1)),\n                'gamma': ['scale', 'auto']\n               }\nclf_D = xgb.XGBClassifier(seed=0)\nparameters_D = {'base_score': list(np.arange(0.2, 0.5, 0.1)),\n                'n_estimators': [10, 40, 60, 100, 120, 130, 140],\n                'objective': ['binary:logistic']}\n\n\n#Collect results on the learners\nbest_estimators = {}\nfor clf, params in [(clf_A, parameters_A), (clf_B, parameters_B), (clf_C, parameters_C), (clf_D, parameters_D)]:\n    print(f'searching for best estimator: classifier {clf.__class__.__name__}')\n    best_estimators[clf.__class__.__name__] = get_best_estimator(clf, params, X_train, y_train)","accc30d1":"#Calculate the number of samples for 1%, 25%, 50%, and 100% of the training data\nsamples_100 = len(y_train)\nsamples_50 = int((len(y_train)\/100)*50)\nsamples_25 = int((len(y_train)\/100)*25)\nsamples_1 = int((len(y_train)\/100)*1)\n\n#Collect results on the learners\nresults = {}\nfor clf, params in [(best_estimators['DecisionTreeClassifier'], parameters_A),\n                    (best_estimators['AdaBoostClassifier'], parameters_B),\n                    (best_estimators['SVC'], parameters_C),\n                    (best_estimators['XGBClassifier'], parameters_D)\n                   ]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_25, samples_50, samples_100]):\n        results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_test, y_test)","e708dfd4":"import matplotlib.pyplot as pl\nimport matplotlib.patches as mpatches\nimport numpy as np\nimport pandas as pd\nfrom time import time\nfrom sklearn.metrics import f1_score, accuracy_score\n\n#taken from a notebook used in the lessons\ndef evaluate(results):\n    \"\"\"\n    Visualization code to display results of various learners.\n    \n    inputs:\n      - learners: a list of supervised learners\n      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n    \"\"\"\n  \n    # Create figure\n    fig, ax = pl.subplots(2, 3, figsize = (19,10))\n\n    # Constants\n    bar_width = 0.22\n    colors = ['#A00000','#00A0A0','#00A000', '#F5B041']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(['train_time', 'acc_train', 'f_train', 'pred_time', 'acc_test', 'f_test']):\n            for i in np.arange(4):\n                \n                # Creative plot code\n                ax[j\/\/3, j%3].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n                ax[j\/\/3, j%3].set_xticks([0.30, 1.30, 2.30, 3.30])\n                ax[j\/\/3, j%3].set_xticklabels([\"1%\", \"25%\", \"50%\", \"100%\"])\n                ax[j\/\/3, j%3].set_xlabel(\"Training Set Size\")\n                ax[j\/\/3, j%3].set_xlim((-0.1, 4))\n    \n    # Add unique y-labels\n    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n    ax[0, 1].set_ylabel(\"Accuracy Score\")\n    ax[0, 2].set_ylabel(\"F-score\")\n    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n    ax[1, 1].set_ylabel(\"Accuracy Score\")\n    ax[1, 2].set_ylabel(\"F-score\")\n    \n    # Add titles\n    ax[0, 0].set_title(\"Model Training\")\n    ax[0, 1].set_title(\"Accuracy Score on Training Subset\")\n    ax[0, 2].set_title(\"F-score on Training Subset\")\n    ax[1, 0].set_title(\"Model Predicting\")\n    ax[1, 1].set_title(\"Accuracy Score on Testing Set\")\n    ax[1, 2].set_title(\"F-score on Testing Set\")\n    \n    \n    # Set y-limits for score panels\n    ax[0, 1].set_ylim((0, 1))\n    ax[0, 2].set_ylim((0, 1))\n    ax[1, 1].set_ylim((0, 1))\n    ax[1, 2].set_ylim((0, 1))\n\n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    pl.legend(handles = patches, bbox_to_anchor = (-.80, 2.53), \\\n               loc = 'upper center', borderaxespad = 0., ncol = 4, fontsize = 'x-large')\n    \n    # Aesthetics\n    pl.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, x = 0.63, y = 1.05)\n    # Tune the subplot layout\n    # Refer - https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.pyplot.subplots_adjust.html for more details on the arguments\n    pl.subplots_adjust(left = 0.125, right = 1.2, bottom = 0.1, top = 0.9, wspace = 0.2, hspace = 0.3) \n    pl.tight_layout()\n    pl.show()","4e1b8b25":"evaluate(results)","9b060819":"selected_model = best_estimators['XGBClassifier']","6ba1a0bc":"def feature_plot(importances, X_train, y_train):\n    \n    # Display the five most important features\n    indices = np.argsort(importances)[::-1]\n    columns = X_train.columns.values[indices[:5]]\n    values = importances[indices][:5]\n\n    # Creat the plot\n    fig = pl.figure(figsize = (9,5))\n    pl.title(\"Normalized Weights for First Five Most Predictive Features\", fontsize = 16)\n    pl.bar(np.arange(5), values, width = 0.6, align=\"center\", color = '#00A000', \\\n          label = \"Feature Weight\")\n    pl.bar(np.arange(5) - 0.3, np.cumsum(values), width = 0.2, align = \"center\", color = '#00A0A0', \\\n          label = \"Cumulative Feature Weight\")\n    pl.xticks(np.arange(5), columns)\n    pl.xlim((-0.5, 4.5))\n    pl.ylabel(\"Weight\", fontsize = 12)\n    pl.xlabel(\"Feature\", fontsize = 12)\n    \n    pl.legend(loc = 'upper center')\n    pl.tight_layout()\n    pl.show()  ","a468259b":"feature_plot(best_estimators['AdaBoostClassifier'].feature_importances_, X_train, y_train)","8fccabea":"### 2.1 Data Structure\nWe need to understand how data is structured before doing our analysis.\n\nWhich are the features? How data is structured? How large is the dataset? What types of features are there? ","bcd34826":"#    Project:                            Confused Person EGG Brainwave","fded7582":"### 3.3 Feature scaling\nBefore we apply dimensionality reduction and unsupervised techniques to the data, we need to perform feature scaling. By this way, the principal component vectors are not influenced by the natural differences in scale for features.","55835bcf":"##  4) Searching for outliers\nThe dataset may have some outlier students. Every student has generated many samples. It is possible that some students labeled itself as confused or not confused, was wrong. Now, we look at the data in order to find students with feature showing very distant median values from the other ones.","ccb7bd15":"Moreover, what about SubjectID? This feature indicates the Identifier of the people who generated the detected brainwave. This is clearly unuseful for our purpose, but for now let's hold it. In order to find outliers, we want to look at the data and to see if some students samples are distant from the other ones. For this reason, we will drop the feature in a second moment.","e606efd8":"### 3.1 Encoding categorical features\nThe algorithms want the data to be numerical. Thus, we have to search for categorical features and to encode them.","d529019b":"## 7) Visualize Feature importance\nOne of the purposes of our project is understanding what features are most correlated to the brainwave confusion detection. When applying PCA to the data we could see what features had the higher variance. Now that we trained a supervised model, we cann see how much any feature is relevant for the final classification and as a consequence which features are more relevant in the confusion state brainwave. Let's build a function able to display this ranking.","bf202656":"### 3.3 Principal Component Analysis (PCA)\nData is ready for PCA. At first, we will apply PCA without indicating how many final features we want. By this way, we will be able to see the variance of each component. After taht, we will take a good decision on features.","fb9b0f79":"##  2) Data Exploration\nLet's explore our datasets","166ede0c":"## 1) Data Loading\n\nAt first, it is necessary to load the dataset: EGG brainwaves samples and people demographic info.  \nData is stored in the files \"data\/EEG_data.csv\", \"data\/datasets_106_24522_demographic_info.csv\".  The format of these two files is CSV. Every file has an header line with feature names. Every value is divided by the comma (\u2018,\u2019) separator. ","901c9ff0":"### 2.3 Searching for missing values\nThe algorithms does not allows the data to be missing, thus we have to be sure of it. If data has some NaNs values, we have to imupte them. If there are columns or rows with a high values of NaNs, they will be dropped.","49c7397a":"### 3.2 Feature selection\nThe dataset may have some features usless for our problem. If we apply our algorithms on the dataset, training our models with these features, we may obtain different results. Furthermore, the more features we have, the more computationally expensive our processes will be.","48a15962":"### 6.1 Algorithms selection","220bf892":"There aren't missing values: great!\nIn case of a new signal with missing values, we shoul be sure to impute it. For this reason, we build an imputation function able to fill NaNs with the mean value of the k-nearest neighbours.","a4f3887d":"What about Sex, Age and Ethnicity? Are these features useful for our purpose? A brain may work in similar but different ways on Male and Females. Age should impact the brain activity too.  Ethnicity? We don't know this without an important background. However, we can study feature variance by applying PCA, after that we can do a better evaluation.","61ce647d":"We can note that students 3 (-0.868121) and 7 (0.527912) has very distant mean values for a lot of features respect to the other students, both for confused and not confused. We have to reject these samples. After that, we can drop the Suvject ID column.","8ba77549":"The purpose of this project is the trial of developing some automation able to analyze a brainwave and to classify it, in order to understand if the person on which the signal has been extracted was in a mental state of confusion at the moment. \n\nWe want to understand if there are some patterns in the brainwaves signals able to correlate different frequencies to our specific mental state research. What are the frequencies that affect the confusion state the most?\n\nFurthermore, we want to build a classifier able to define if a brainwave signal sample owns to a confused person or not. Our question is \u201cIs this signal a confusion mental state?\u201d \n\nIn order to do this, we are going to use unsupervised and supervised Machine Learning techniques. ","37cc8bbc":"##  6) Model training, tuning and selection\nA supervised model has to be selected as the final model. In order to do this, we will train four different models and evaluate them. After that we will choose the best one. Every model will be tuned using different hyperparameters.","4f1fc570":"##  3) Feature transformation","ebacfe61":"#### 6.2.4 Classifiers evaluation\nWe want to evaluate the results obtained above. Thus, we want to buil a function able to display models metrics in order to have an easy way to compare them with a graph visualization","0e0df1e8":"#### 6.2.2 Classifiers creation and tuning\nNow the function in created. We have to run the pipeline on the three selected classifiers. At first, we obtain the best estimator for every algorithm. After that, we train and test every of them in order to evaluate their performance.","f89bbee5":"- **Decision Trees** \n\n**Decision Trees** model is used for classification problems. This algorithm is able to analyse data features and to understand how much information each of these fetaures can give to us, in order to make our prediction. The model can make really accurate predictions, because of each feature can be combined with every other feature according to its specific information gain. This type of model has also a bad behavior. It tends to overfit a lot, thus, we have to use it with high attention or use it with any ensemble method. \n\n- **Ensemble Methods** \n\nEnsemble methods are able to solve classification problems combining different classification models. It can be used for real world applications, especially when data is not linearly separable. We can make a prediction using different models combined with an ensemble method, such as Ada Boost or Bagging or Random Forest. Furthermore, they are able to create models fitting data well because of their ability of finding a good compromise between bias and variance. We will use **AdaBoost** and **XGBoost** (Extreme Gradient Boosting)\n\n- **Support Vector Machine**\n\n**Support Vector Machine** is able to make good classification predictions, defining models with a well-fitted boundary. This boundary is so good because of the use of the margin. This model has a high flexibility because of its parameters. This alorithm is able to work on different types of data, in fact, it has three kernels: linear, polynomial and Rbf. Moreover, an SVM model can give us the ability of tuning it in order to decide how much the model has to be precise: with the C parameter we can define how much weight we want to assign to the Classification error respect to the margin error. ","496f50c7":"#### 6.2.3 Classifiers evaluation metrics collection\nWe have the best estimator for every algorithm we decided to use. Now, It is time to evaluate them. Let's train the models on different percentage of data. We'll evaluate the accuracy score, fscore and execution-time for every model using training and testing datasets. Looking at these results, we will able to detect the best model according to its performance, accuracy and bias-variance balance.","b417ef20":"### 6.3 Model selection\nLooking at these graphs, we can clearly identify the models with the highest values in the scores: \n\n- The execution-time metric highlights a model that seems to be much more computationally expensive than the others: SVM. In the other hand, there is an algorithm resulting really fast: DecisionTrees. AdaBoost and XGBoost have anyway a good execution-time, thus, in case of their better performance in other metrics, we should not be much worry about the time comparison. \n\n- If we want to look for models with a bad bias-variance balance, we have to look at the other metrics trends. An underfitting model shows the training and testing scores converging to a low value. There aren\u2019t underfitting models here. An overfitting model, instead, shows training and testing error diverging at the data size increasing. There aren\u2019t overfitting models here. \n\n- The Accuracy and FScore metrics highlit that the XGBoost model is the one with the higher values. AdaBoost and DecisionTrees are really similar one to the other. SVM seems to be the worse model. \n\nAccording to our metrics, we select the XGBoost model as the one we want to use, with an accuracy of about 70%. XGBoostClassifier: {\u2018Accuracy\u2019: 0.674305216967333, \u2018f_score\u2019: 0.6878541076487251} ","eb4c9083":"There are two features that we don't need for. The first is the VideoID. This information is correlated to the context of the experiment and it is not a valuable data of the EGG brainwave. For this reason, we drop it. Another usless feature, is the 'predefinedlabel'. This feature indicates which confusion state was supposed to be detected by the experiment conductor before doing the test. We need for the 'user-definedlabel' because that's the label indicating if a signal is correlated to a confusion state.","379f2f78":"As we can see above, features correlated to the people are low impactive on the dataset. For this reason, we can drop them, without using PCA. Thus, we don't apply PCA to the dataset in order to get a feature extraction. We hust drop the usless columns.","db86b365":"##  5) Preprocessing function\nBuilding a function with pre-processing steps, accordig to the previous data exploration and analysis","900f5e5f":"#### Data structure\nEvery sample is associated to the person from who it has been detected. People data presents age, ethnicity and Sex. These two datasets are correlated by a feature 'SubjectID', a unique identifier for the 10 students. Its values are the range between 0 and 9 (Student 1 \u2013 10). As we can see, there are 12811 EGG samples and 10 people. Every feature is a numerical value in the samples dataset. In the demographic one, instead, there are two categorical features: Sex and Ethnicity. Sex is a binary classification feature: \u2018M\u2019 stands for \u201cMale\u201d and \u2018F\u2019 for \u201cFemale\u201d. Ethnicity, instead, has more values. These values are strings referring to the student origin.","55bd8391":"### 2.2 Joining datasets\nWe don't want to study the two datasets separately, thus, we have to join them. In order to do this, we have to consider their relational correlation: the samples dataset contain an \"External Key\" related to the people dataset \"Primary Key\".  The key feature (external\/internal) is \"SubjectID\" for both. ","58b31401":"##  5) Shuffle and Split Data\nData is ready for supervised classification. Let's shuffle samples and split them in training and testing data","d2769f34":"### 6.2 Creating a Training and Tuning Pipeline\n#### 6.2.1 Creating a tuning function and an evaluation function\nIn order to evaluate models, we have to create a training and tuning pipeline: every model is trained and tuned.\nThus, we have to define a function able to:\n - Fit the learner to the sampled training data and record the training time.\n - getting best estimator from grid search\n - Perform predictions on the test data X_test, and also on the first 300 training points.\n - Record the total prediction time.\n - Calculate the accuracy score for both the training subset and testing set.\n - Calculate the F-score for both the training subset and testing set.\n"}}