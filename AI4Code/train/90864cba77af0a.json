{"cell_type":{"4e14ddaa":"code","f2822d03":"code","bd4b5a96":"code","60057676":"code","3b1eb212":"code","7a628eb7":"code","a32d8919":"code","8a01f79c":"code","a8597f7d":"code","ab55e24d":"code","bab38d78":"code","b8a9547e":"code","22720170":"code","dd360117":"code","ac342545":"code","4bd8046c":"code","2eb708e7":"code","caa0087c":"code","03309942":"code","a5f624b6":"code","6e2e34d6":"code","3307a2e3":"code","c554a842":"code","c629c8a2":"code","43f4f8d7":"code","b674f113":"code","7f1e8956":"code","b4abe8ff":"markdown","6dca9207":"markdown","bf2ae604":"markdown","3be3376f":"markdown","a95fd2af":"markdown","e9dc4716":"markdown","47df18fe":"markdown","e9504a44":"markdown","a11f6593":"markdown"},"source":{"4e14ddaa":"# Core\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns; sns.set()\nimport glob\nimport random\nimport os\nimport cv2\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n#tesor fow & keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense,BatchNormalization,Dropout,Input\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D\nimport os\nfrom keras.applications.vgg16 import VGG16\nfrom sklearn.ensemble import RandomForestClassifier\n#cnn\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n","f2822d03":"seed = 42\nnp.random.seed =seed","bd4b5a96":"# start load each masked images & train images we found masked\n# image ended by _mask that is the separator to define images","60057676":"mask_files = glob.glob('..\/input\/lgg-mri-segmentation\/kaggle_3m\/*\/*_mask*')\n","3b1eb212":"#get trained files\ntrain_files = []\ndef load_train_files() :\n    for i in tqdm(mask_files):\n        image = i.replace('_mask','')\n        train_files.append(image)","7a628eb7":"load_train_files()","a32d8919":"plt.figure(figsize=(15,10))\nfor i in tqdm(range(10) ) :\n    images = random.choice(train_files)\n    plt.subplot(2,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid([])\n    img = mpimg.imread(images)\n    plt.imshow(img)","8a01f79c":"plt.figure(figsize=(15,10))\nfor i in tqdm(range(10)) :\n    images = random.choice(mask_files)\n    plt.subplot(2,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid([])\n    img = mpimg.imread(images)\n    plt.imshow(img)","a8597f7d":"#here load all traind image and recorde it inside array\ntarin_images=[]\ndef load_train_images():\n    for images in tqdm(train_files) :\n        image = cv2.imread(images,cv2.IMREAD_COLOR)\n        image = cv2.resize(image,(128,128))\n        image =cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n        tarin_images.append(image)","ab55e24d":"#here load all masked image and recorde it inside array\nmasked_=[]\ndef load_masked_images():\n    for masked in tqdm(mask_files) :\n        mask = cv2.imread(masked,0)\n        mask = cv2.resize(mask,(128,128))\n        masked_.append(mask)\n","bab38d78":"load_train_images()","b8a9547e":"load_masked_images()","22720170":"#here we havent much more memory to tarin 3929 so we need to reduce number of images to be able to train it\ntrain_images_reduced =tarin_images[:1000]\ntrain_masks_reduced =masked_[:1000]\nprint(len(train_images_reduced))\nprint(len(train_masks_reduced))","dd360117":"#start convert images to array\ntrain_images  = np.array(train_images_reduced)\nmasked_images = np.array(train_masks_reduced)","ac342545":"print('Train  Images  Shape  Is :{}'.format(train_images.shape))\nprint('Masked Images  Shape  Is :{}'.format(masked_images.shape))","4bd8046c":"x_train =  train_images\ny_train = masked_images\nprint(type(x_train))\nprint(type(y_train))","2eb708e7":"#here we will load imagenet weight & remove any dense layer from model\nvgg_model = VGG16(weights='imagenet',include_top=False,input_shape=(128,128,3))\nvgg_model.summary()","caa0087c":"from keras.engine.base_layer import Layer\n#remove dens layer\nfor layer in vgg_model.layers:\n\tlayer.trainable = False\nvgg_model.summary()    ","03309942":"# remember shape of x_train is (3929, 128, 128, 3) which is copatable with layer block1_conv2 \nnew_model = Model(inputs=vgg_model.input,outputs=vgg_model.get_layer('block1_conv2').output)\nnew_model.summary()","a5f624b6":"#get all features\nfeatures = new_model.predict(x_train,batch_size=16)","6e2e34d6":"plt.figure(figsize=(15,15))\nsquare = 8\nix=1\nfor _ in range(square):\n    for _ in range(square):\n        ax = plt.subplot(square, square, ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        plt.imshow(features[0,:,:,ix-1], cmap='gray')\n        ix +=1\nplt.show()","3307a2e3":"x = features\nprint('x  before reshapping  Is :{}'.format(x.shape))\n#shape of random forest is (x,y)only\nx = x.reshape(-1,x.shape[3])\ny = y_train.reshape(-1)\nprint('x  Shape  Is :{}'.format(x.shape))\nprint('y  Shape  Is :{}'.format(y.shape))","c554a842":"#remove zero pixels befor training\ndf = pd.DataFrame(x)\ndf['label'] = y\nprint(df['label'].unique())\nprint(df['label'].value_counts())","c629c8a2":"df = df[df['label']!=0]\nx_rf = df.drop(labels=['label'],axis=1)\ny_rf = df['label']","43f4f8d7":"#call rf classifier\nrf_model = RandomForestClassifier(n_estimators=50,random_state=42,verbose=True)\nrf_model.fit(x_rf,y_rf)","b674f113":"import pickle\nfilename = 'RF_model.sav'\npickle.dump(rf_model, open(filename, 'wb'))\n","7f1e8956":"saved_model = pickle.load(open('RF_model.sav', 'rb'))\ntest_img = cv2.imread('..\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_CS_4942_19970222\/TCGA_CS_4942_19970222_10_mask.tif', cv2.IMREAD_COLOR)       \n#test_img = cv2.imread('\/content\/drive\/MyDrive\/datasets\/brain_tumer.zip (Unzipped Files)\/lgg-mri-segmentation\/kaggle_3m\/TCGA_CS_4942_19970222\/TCGA_CS_4942_19970222_10_mask.tif', cv2.IMREAD_COLOR)       \ntest_img = cv2.resize(test_img, (128, 128))\ntest_img = cv2.cvtColor(test_img, cv2.COLOR_RGB2BGR)\ntest_img = np.expand_dims(test_img, axis=0)\n#call vgg model\ntest_image_feature = new_model.predict(test_img)\n#reshape image \ntest_image_feature = test_image_feature.reshape(-1,test_image_feature.shape[3])\n#start predect masked image\nnew_predection = saved_model.predict(test_image_feature)\nnew_predicted_image = new_predection.reshape(128,128)\nplt.imshow(new_predicted_image, cmap='gray')\nplt.show()","b4abe8ff":"# VGG16 Model","6dca9207":"# Handel VGG model \n\n1. chose VGG model Layer to be addaptive with  our data \n2. load VGG Weights  \n3. removing dense layer from model\n\n\n","bf2ae604":"\n# Import Data \n*   Train images\n*   Masked images\n\n","3be3376f":"# predected model & test result","a95fd2af":"# [many thanks to DigitalSreeni]('https:\/\/www.youtube.com\/c\/DigitalSreeni')","e9dc4716":"# handel data  with RandomForestClassifier\n\n1.   convert shape of features and label to same shape of RandomForestClassifier\n2.   convert x,y to random forest shape(x,y)\n1.   create data frame to reject zero pixel \n2.   List item\n","47df18fe":"\n\n# Import libraries.\n","e9504a44":"VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3\u00d73 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU\u2019s.","a11f6593":"![vgg16-1-e1542731207177.png](attachment:7526ad14-c065-4296-a596-8ce144b20671.png)"}}