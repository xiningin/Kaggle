{"cell_type":{"44f55931":"code","53dcdd47":"code","f3db12ad":"code","58674707":"code","aeddce36":"code","493dab79":"code","2aacefad":"code","da99d198":"code","eda1933b":"code","16cab465":"code","5750b80a":"code","6b4c5b36":"code","b2141efb":"code","d0d5d0d9":"code","bfb5931b":"code","71063a80":"code","59886840":"code","8e9f98ae":"code","4f6cf0c8":"code","4ac14aea":"code","fcd6a944":"code","a7c793e8":"code","52a9f1e2":"code","da2ff94f":"code","3c6f5b4c":"code","ee73e31e":"code","59807f92":"code","eec46583":"code","dab8a19d":"code","9650a9ac":"code","b463f88a":"code","09300f6f":"code","ca3b32e3":"code","6d381753":"code","bcbb7cc7":"code","e0ad7294":"code","6c81de2f":"code","6cade5ca":"code","59943fe3":"code","1bde97ce":"code","9ee19d53":"code","ff1784f9":"code","57fab4a5":"code","87ed36ae":"code","47476790":"code","99237113":"code","7cebbefa":"code","4317ffdf":"code","1e1ff72e":"code","ba946e1d":"code","c2b99d54":"code","da34f300":"code","f595ca33":"code","72ea5248":"code","11f6c19c":"code","3eb60aaa":"markdown","595d050c":"markdown","6a7afd3c":"markdown","51500c80":"markdown","16c7edfc":"markdown","3f8228c7":"markdown","5e29f3af":"markdown","4c9032c1":"markdown","6242f41b":"markdown","78d7a5e0":"markdown","8ca279c4":"markdown","fb6a83b0":"markdown","f7853313":"markdown"},"source":{"44f55931":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport ast\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.spatial.distance import cosine, correlation\nfrom surprise import Reader, Dataset, SVD, NormalPredictor, BaselineOnly, KNNBasic, NMF\nfrom surprise.model_selection import cross_validate, KFold ,GridSearchCV , RandomizedSearchCV\n\nfrom keras.models import Sequential\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import  Input, dot, concatenate\nfrom keras.models import Model\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\npd.set_option('display.max_rows',50)\npd.set_option('display.max_columns', 50)","53dcdd47":"def reduce_mem_usage(df):\n    # iterate through all the columns of a dataframe and modify the data type\n    #   to reduce memory usage.        \n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","f3db12ad":"credits = pd.read_csv(\"..\/input\/tmdb-movie-metadata\/tmdb_5000_credits.csv\")\nmovies = pd.read_csv(\"..\/input\/tmdb-movie-metadata\/tmdb_5000_movies.csv\")","58674707":"display(credits.head(5))\ndisplay(movies.head(5))","aeddce36":"credits.columns = ['id','tittle','cast','crew']\nmovies= movies.merge(credits,on='id')\nplots = movies['overview']\ntfidf = TfidfVectorizer(stop_words = 'english' , max_df = 4 , min_df= 1)\nplots = plots.fillna('')\ntfidf_matrix = tfidf.fit_transform(plots)\n","493dab79":"cos_similar = linear_kernel(tfidf_matrix , tfidf_matrix)\ncos_similar.shape","2aacefad":"indices = pd.Series(movies.index , index = movies['title']).drop_duplicates()","da99d198":"def get_movies(title):\n    idx = indices[title]\n    similar = list(enumerate(cos_similar[idx]))\n    similar = sorted(similar , key = lambda x: x[1] , reverse = True)\n    similar = similar[:11]\n    indic = []\n    for i in similar:\n        indic.append(i[0])\n    return movies['title'].iloc[indic]\n","eda1933b":"get_movies('Spider-Man 3')","16cab465":"get_movies('Toy Story')","5750b80a":"readme= open('..\/input\/movielens-100k-dataset\/ml-100k\/README','r') \nprint(os.listdir('..\/input\/movielens-100k-dataset\/ml-100k'))\nprint(readme.read()) ","6b4c5b36":"info = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.info' , sep=\" \", header = None)\ninfo.columns = ['Counts' , 'Type']\n\noccupation = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.occupation' , header = None)\noccupation.columns = ['Occupations']\n\nitems = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.item' , header = None , sep = \"|\" , encoding='latin-1')\nitems.columns = ['movie id' , 'movie title' , 'release date' , 'video release date' ,\n              'IMDb URL' , 'unknown' , 'Action' , 'Adventure' , 'Animation' ,\n              'Childrens' , 'Comedy' , 'Crime' , 'Documentary' , 'Drama' , 'Fantasy' ,\n              'Film_Noir' , 'Horror' , 'Musical' , 'Mystery' , 'Romance' , 'Sci_Fi' ,\n              'Thriller' , 'War' , 'Western']\n\ndata = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.data', header= None , sep = '\\t')\nuser = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.user', header= None , sep = '|')\ngenre = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.genre', header= None , sep = '|' )\n\ngenre.columns = ['Genre' , 'genre_id']\ndata.columns = ['user id' , 'movie id' , 'rating' , 'timestamp']\nuser.columns = ['user id' , 'age' , 'gender' , 'occupation' , 'zip code']\n","b2141efb":"display(info)\ndisplay(user.shape)\ndisplay(items.shape)\ndisplay(data.shape)","d0d5d0d9":"# Merging the columns with data table to better visualise\ndata = data.merge(user , on='user id')\ndata = data.merge(items , on='movie id')","bfb5931b":"# Data Cleaning for Model Based Recommandation System\ndef convert_time(x):\n    return datetime.utcfromtimestamp(x).strftime('%d-%m-%Y')\ndef date_diff(date):\n    d1 = date['release date'].split('-')[2]\n    d2 = date['rating time'].split('-')[2]\n    return abs(int(d2) - int(d1))\n\n# data.drop(columns = ['movie title' , 'video release date' , 'IMDb URL'] , inplace = True)\ndata.dropna(subset = ['release date'] , inplace = True)\n\nuser_details = data.groupby('user id').size().reset_index()\nuser_details.columns = ['user id' , 'number of user ratings']\ndata = data.merge(user_details , on='user id')\n\nmovie_details = data.groupby('movie id').size().reset_index()\nmovie_details.columns = ['movie id' , 'number of movie ratings']\ndata = data.merge(movie_details , on='movie id')\n\nuser_details = data.groupby('user id')['rating'].agg('mean').reset_index()\nuser_details.columns = ['user id' , 'average of user ratings']\ndata = data.merge(user_details , on='user id')\n\nmovie_details = data.groupby('movie id')['rating'].agg('mean').reset_index()\nmovie_details.columns = ['movie id' , 'average of movie ratings']\ndata = data.merge(movie_details , on='movie id')\n\n\nuser_details = data.groupby('user id')['rating'].agg('std').reset_index()\nuser_details.columns = ['user id' , 'std of user ratings']\ndata = data.merge(user_details , on='user id')\n\nmovie_details = data.groupby('movie id')['rating'].agg('std').reset_index()\nmovie_details.columns = ['movie id' , 'std of movie ratings']\ndata = data.merge(movie_details , on='movie id')\n\ndata['age_group'] = data['age']\/\/10\ndata['rating time'] = data.timestamp.apply(convert_time)\ndata['time difference'] = data[['release date' , 'rating time']].apply(date_diff, axis =1)\n\ndata['total rating'] = (data['number of user ratings']*data['average of user ratings'] + data['number of movie ratings']*data['average of movie ratings'])\/(data['number of movie ratings']+data['number of user ratings'])\ndata['rating_new'] = data['rating'] - data['total rating']\n\ndel movie_details\ndel user_details","71063a80":"pivot_table_user = pd.pivot_table(data=data,values='rating_new',index='user id',columns='movie id')\npivot_table_user = pivot_table_user.fillna(0)\npivot_table_movie = pd.pivot_table(data=data,values='rating',index='user id',columns='movie id')\npivot_table_movie = pivot_table_movie.fillna(0)","59886840":"user_based_similarity = 1 - pairwise_distances( pivot_table_user.values, metric=\"cosine\" )\nmovie_based_similarity = 1 - pairwise_distances( pivot_table_movie.T.values, metric=\"cosine\" )","8e9f98ae":"user_based_similarity = pd.DataFrame(user_based_similarity)\nuser_based_similarity.columns = user_based_similarity.columns+1\nuser_based_similarity.index = user_based_similarity.index+1\n\nmovie_based_similarity = pd.DataFrame(movie_based_similarity)\nmovie_based_similarity.columns = movie_based_similarity.columns+1\nmovie_based_similarity.index = movie_based_similarity.index+1","4f6cf0c8":"# Testing movie based Recommendation\n\ndef rec_movie(movie_id):\n    temp_table = pd.DataFrame(columns = items.columns)\n    movies = movie_based_similarity[movie_id].sort_values(ascending = False).index.tolist()[:11]\n    for mov in movies:\n#         display(items[items['movie id'] == mov])\n        temp_table = temp_table.append(items[items['movie id'] == mov], ignore_index=True)\n    return temp_table\ndef rec_user(user_id):\n    temp_table = pd.DataFrame(columns = user.columns)\n    us = user_based_similarity[user_id].sort_values(ascending = False).index.tolist()[:101]\n    for u in us:\n#         display(items[items['movie id'] == mov])\n        temp_table = temp_table.append(user[user['user id'] == u], ignore_index=True)\n    return temp_table","4ac14aea":"display(rec_movie(176))\ndisplay(rec_movie(11))","fcd6a944":"def user_rating(x):\n    similar_user = rec_user(x)\n    similar_user.drop(columns= ['age' , 'gender' , 'occupation' , 'zip code'] , inplace = True)\n    similar_user = similar_user.merge(pivot_table_movie , on= 'user id')\n    similar_user = similar_user.set_index('user id')\n    similar_user.replace(0, np.nan, inplace=True)\n    u_ratings = similar_user[similar_user.index==x]\n    similar_user.drop(similar_user.index[0] , inplace = True)\n    return u_ratings.append(similar_user.mean(axis = 0 , skipna = True), ignore_index = True)   ","a7c793e8":"display(user_rating(771))\ndisplay(user_rating(900))","52a9f1e2":"reader = Reader(rating_scale=(1, 5))\nsup_data = Dataset.load_from_df(data[['user id', 'movie title', 'rating']], reader)","da2ff94f":"algo = NormalPredictor()\ncross_validate(algo, sup_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","3c6f5b4c":"algo = SVD()\ncross_validate(algo, sup_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","ee73e31e":"algo = KNNBasic(k=20)\ncross_validate(algo, sup_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","59807f92":"algo = KNNBasic(sim_options={'user_based': False} , k=20) # https:\/\/surprise.readthedocs.io\/en\/stable\/prediction_algorithms.html#similarity-measure-configuration\ncross_validate(algo, sup_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","eec46583":"algo = NMF()\ncross_validate(algo, sup_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","dab8a19d":"sup_train = sup_data.build_full_trainset()\nalgo = SVD(n_factors = 200 , lr_all = 0.005 , reg_all = 0.02 , n_epochs = 40 , init_std_dev = 0.05)\nalgo.fit(sup_train)","9650a9ac":"def prediction_algo(uid = None , iid = None):\n    predictions = []\n    if uid is None:\n        for ui in sup_train.all_users():\n            predictions.append(algo.predict(ui, iid, verbose = False))\n        return predictions\n    \n    if iid is None:\n        for ii in sup_train.all_items():\n            ii = sup_train.to_raw_iid(ii)\n            predictions.append(algo.predict(uid, ii, verbose = False))\n        return predictions\n    return predictins.append(algo.predict(uid,iid,verbose = False))","b463f88a":"predictions = prediction_algo(uid = 112)\npredictions.sort(key=lambda x: x.est, reverse=True)\nprint('#### Best Recommanded Movies are ####')\nfor pred in predictions[:21]:\n#     print('Movie -> {} with Score-> {}'.format(sup_train.to_raw_iid(pred.iid) , pred.est))\n    print('Movie -> {} with Score-> {}'.format(pred.iid , pred.est))","09300f6f":"meta_data = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/movies_metadata.csv')\nkeywords = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/keywords.csv')\ncredits = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/credits.csv')\n\nmeta_data = meta_data[meta_data.id!='1997-08-20']\nmeta_data = meta_data[meta_data.id!='2012-09-29']\nmeta_data = meta_data[meta_data.id!='2014-01-01']\nmeta_data = meta_data.astype({'id':'int64'})\n\nmeta_data = meta_data.merge(keywords , on = 'id')\nmeta_data = meta_data.merge(credits , on = 'id')","ca3b32e3":"def null_values(df):\n    for col in df.columns:\n        if df[col].isnull().sum() != 0:\n            print('Total values missing in {} are {}'.format(col , df[col].isnull().sum()))\nnull_values(meta_data)","6d381753":"meta_data[meta_data['production_companies'].isnull()]\nmeta_data.dropna(subset=['production_companies'] , inplace = True)","bcbb7cc7":"def btc_function(data):\n    if type(data) == str:\n        return ast.literal_eval(data)['name'].replace(\" \",\"\")\n    return data\n# https:\/\/www.kaggle.com\/hadasik\/movies-analysis-visualization-newbie\ndef get_values(data_str):\n    if isinstance(data_str, float):\n        pass\n    else:\n        values = []\n        data_str = ast.literal_eval(data_str)\n        if isinstance(data_str, list):\n            for k_v in data_str:\n                values.append(k_v['name'].replace(\" \",\"\"))\n            return str(values)[1:-1]\n        else:\n            return None\n","e0ad7294":"meta_data['btc_name'] = meta_data.belongs_to_collection.apply(btc_function)\nmeta_data[['genres', 'production_companies', 'production_countries', 'spoken_languages' ,'keywords','cast', 'crew']] = meta_data[['genres', 'production_companies', 'production_countries', 'spoken_languages' ,'keywords' ,'cast' , 'crew']].applymap(get_values)\nmeta_data['is_homepage'] = meta_data['homepage'].isnull()","6c81de2f":"meta_data['status'] = meta_data['status'].fillna('')\nmeta_data['original_language'] = meta_data['original_language'].fillna('')\nmeta_data['btc_name'] = meta_data['btc_name'].fillna('')","6cade5ca":"meta_data.drop_duplicates(inplace = True)\nmeta_data.drop(index = [2584 , 201 , 963 , 5769 , 5931 , 5175, 5587 , 845, 9661 ,11448 , 4145 , 4394 , 11254 , 10511 , 13335 , 13334 , 13329 , 16345 , 16348 , 16349 , 9658 , 9662 , 4391 , 4395 , 846 , 849 , 850 , 5927 , 5932 , 24363 , 33395 , 14101] , inplace = True)","59943fe3":"def vector_values(df , columns , min_df_value):\n    c_vector = CountVectorizer(min_df = min_df_value)\n    df_1 = pd.DataFrame(index = df.index)\n    for col in columns:\n        print(col)\n        df_1 = df_1.join(pd.DataFrame(c_vector.fit_transform(df[col]).toarray(),columns =c_vector.get_feature_names(),index= df.index).add_prefix(col+'_'))\n    return df_1\nmeta_data_addon_1 = vector_values(meta_data , columns = ['status','original_language','genres', 'production_companies' ,'production_countries' , 'spoken_languages' , 'keywords' , 'cast' ,'crew'] ,min_df_value = 20)\nmeta_data_addon_2 = vector_values(meta_data , columns = ['btc_name'] , min_df_value = 2)","1bde97ce":"col = ['belongs_to_collection', 'genres' , 'homepage' , 'id' , 'imdb_id' , 'overview' ,'poster_path' , 'status' , 'original_language' , \n'production_companies', 'production_countries', 'spoken_languages', 'keywords',  'cast',  'crew', 'tagline','adult'  ]\nmeta_data.drop(columns = col , inplace = True)\ncol = [ 'video', 'is_homepage']\nfor c in col:\n    meta_data[c] = meta_data[c].astype(bool)\n    meta_data[c] = meta_data[c].astype(int)","9ee19d53":"def get_year(date):\n    return str(date).split('-')[0]\nmeta_data['popularity'] = meta_data['popularity'].astype(float)\nmeta_data['budget'] = meta_data['budget'].astype(float)\nmeta_data['vote_average_group'] = pd.qcut(meta_data['vote_average'], q=10, precision=2,duplicates = 'drop')\nmeta_data['popularity_group'] = pd.qcut(meta_data['popularity'], q=10, precision=2,duplicates = 'drop')\nmeta_data['vote_average_group'] =pd.qcut(meta_data['vote_average'], q=10, precision=2,duplicates = 'drop')\nmeta_data['runtime_group'] = pd.qcut(meta_data['runtime'], q=10, precision=2,duplicates = 'drop')\nmeta_data['budget_group'] = pd.qcut(meta_data['budget'], q=10, precision=2,duplicates = 'drop')\nmeta_data['revenue_group'] = pd.qcut(meta_data['revenue'], q=10, precision=2,duplicates = 'drop')\nmeta_data['vote_count_group'] = pd.qcut(meta_data['vote_count'], q=10, precision=2,duplicates = 'drop')\nmeta_data['release_year'] = meta_data['release_date'].apply(get_year)\nmeta_data['release_year'] = meta_data['release_year'].fillna('')\nmeta_data['release_year'] = meta_data['release_year'].astype(float)\nmeta_data['release_year_group'] = pd.qcut(meta_data['release_year'], q=10, precision=2,duplicates = 'drop')\nmeta_data['title_new'] = meta_data.apply(lambda x: str(x['title'])+' ('+str(x['release_date'])+')' , axis =1)","ff1784f9":"meta_data_addon_3 = pd.get_dummies(meta_data[['vote_average_group' , 'popularity_group' , 'runtime_group' , 'budget_group' , 'revenue_group' , 'vote_count_group' , 'release_year_group']])\nmeta_data_train = pd.concat([meta_data_addon_1,meta_data_addon_2,meta_data_addon_3 , meta_data[['video' , 'is_homepage']]] , axis = 1)","57fab4a5":"meta_data_train.index = meta_data['title_new']","87ed36ae":"del meta_data_addon_1,meta_data_addon_2,meta_data_addon_3\ngc.collect()","47476790":"def get_similar_movies(movie_title , num_rec = 10):\n    try:\n        sample_1 = 1 - pairwise_distances([meta_data_train.loc[movie_title].values] , meta_data_train.values , metric = 'cosine')\n        sample_1 = pd.DataFrame(sample_1.T , index = meta_data_train.index )\n        return sample_1.sort_values(by = 0 , ascending  = False).head(num_rec).index\n    except ValueError as e:\n        print(e)\n#         sample_1 = 1 - pairwise_distances(meta_data_train.loc[movie_title].values, meta_data_train.values , metric = 'cosine')\n#         sample_1 = pd.DataFrame(sample_1.T , index = meta_data_train.index )\n#         return sample_1.sort_values(by = 0 , ascending  = False).head(20).index.names","99237113":"print(get_similar_movies('Undisputed III : Redemption (2010-05-22)'))\nprint(get_similar_movies('Finding Nemo (2003-05-30)'))\nprint(get_similar_movies('Mindhunters (2004-05-07)'))\nprint(get_similar_movies('Thor (2011-04-21)'))\nprint(get_similar_movies('Kong: Skull Island (2017-03-08)'))","7cebbefa":"def multi_rec(seen_movies , num_rec = 10):\n    rec_movies = []\n    for mov in seen_movies:\n        rec_movies.append(get_similar_movies(mov , 5).values)\n    return rec_movies\nmulti_rec(['Star Wars: The Clone Wars (2008-08-05)' , 'Marvel One-Shot: Item 47 (2012-09-13)'])","4317ffdf":"data = data.sample(frac = 1)\ndata_train_x = np.array(data[['user id' , 'movie id']].values)\ndata_train_y = np.array(data['rating'].values)\nx_train, x_test, y_train, y_test = train_test_split(data_train_x, data_train_y, test_size = 0.2, random_state = 98)\nn_factors = 50\nn_users = data['user id'].max()\nn_movies = data['movie id'].max()","1e1ff72e":"user_input = Input(shape=(1,), name='User_Input')\nuser_embeddings = Embedding(input_dim = n_users+1, output_dim=n_factors, input_length=1,name='User_Embedding')(user_input)\nuser_vector = Flatten(name='User_Vector') (user_embeddings)\n\nmovie_input = Input(shape = (1,) , name = 'Movie_input')\nmovie_embeddings = Embedding(input_dim = n_movies+1 , output_dim = n_factors , input_length = 1 , name = 'Movie_Embedding')(movie_input)\nmovie_vector = Flatten(name = 'Movie_Vector')(movie_embeddings)\n\nmerged_vectors = concatenate([user_vector, movie_vector], name='Concatenation')\ndense_layer_1 = Dense(100 , activation = 'relu')(merged_vectors)\ndense_layer_3 = Dropout(.5)(dense_layer_1)\ndense_layer_2 = Dense(1)(dense_layer_3)\nmodel = Model([user_input, movie_input], dense_layer_2)","ba946e1d":"model.compile(loss='mean_squared_error', optimizer='adam' ,metrics = ['accuracy'] )\nmodel.summary()","c2b99d54":"SVG(model_to_dot( model,  show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))","da34f300":"history = model.fit(x = [x_train[:,0] , x_train[:,1]] , y =y_train , batch_size = 128 , epochs = 30 , validation_data = ([x_test[:,0] , x_test[:,1]] , y_test))","f595ca33":"loss , val_loss , accuracy , val_accuracy = history.history['loss'],history.history['val_loss'],history.history['accuracy'],history.history['val_accuracy']","72ea5248":"plt.figure(figsize = (12,10))\nplt.plot( loss, 'r--')\nplt.plot(val_loss, 'b-')\nplt.plot( accuracy, 'g--')\nplt.plot(val_accuracy,'-')\nplt.legend(['Training Loss', 'Validation Loss' , 'Training Accuracy' , 'Validation Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","11f6c19c":"score = model.evaluate([x_test[:,0], x_test[:,1]], y_test)\nprint(np.sqrt(score))","3eb60aaa":"# Collaborative Filtering  Based Recommendation System\nCollaborative filtering approaches build a model from user\u2019s past behavior (i.e. items purchased or searched by the user) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that user may have an interest in.","595d050c":"https:\/\/www.kaggle.com\/fuzzywizard\/rec-sys-collaborative-filtering-dl-techniques#4)-Matrix-Factorization-using-Deep-Learning-(Keras)","6a7afd3c":"# Matrix Factorization with Keras","51500c80":"Parameters:\t\n\n*     n_factors \u2013 The number of factors. Default is 100.\n*     n_epochs \u2013 The number of iteration of the SGD procedure. Default is 20.\n*     init_mean \u2013 The mean of the normal distribution for factor vectors initialization. Default is 0.\n*     init_std_dev \u2013 The standard deviation of the normal distribution for factor vectors initialization. Default is 0.1.\n*     lr_all \u2013 The learning rate for all parameters. Default is 0.005.\n*     reg_all \u2013 The regularization term for all parameters. Default is 0.02.\n","16c7edfc":"## Predictions","3f8228c7":"# Content Based Recommandation System \nContent-based filtering approaches uses a series of discrete characteristics of an item in order to recommend additional items with similar properties. Content-based filtering methods are totally based on a description of the item and a profile of the user\u2019s preferences. It recommends items based on user\u2019s past preferences.","5e29f3af":"# Movie Recommendations System\n\n\nRecommender System is a system that seeks to predict or filter preferences according to the user\u2019s choices. Recommender systems are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. \n\nThis Kernel make Recommendations on basis of \n* Content Based Recommendations \n* Similarity Based Recommendations \n* Collabrative Filtering Based Recommendations  \n* Recommendations based on Surpise Library\n* Factors Based Recommendations\n* Embedding Based Recommedations","4c9032c1":"As it is seen , it is working great and recommending correct movies. \n\n**Taking it to next Level**","6242f41b":"### Different Prediction Algorithms","78d7a5e0":"# Surprise Library Exploration","8ca279c4":"# Recommendation_systems_paperlist \n## Survey paper\n* Recommender systems survey [Knowledge-based systems 2013]\n* Deep Learning based Recommender System: A Survey and New Perspectives [2017]\n* A Survey on Session-based Recommender System [2019] [[__pdf__](https:\/\/arxiv.org\/pdf\/1902.04864.pdf)]\n\n## Recommendation Systems with Social Information \n* SoRec: Social Recommendation Using Probabilistic Matrix Factorization [CIKM 2008]\n* A Matrix Factorization Technique with Trust Propagation for Recommendation in Social Networks [RecSys 2010]\n* Recommender systems with social regularization [WSDM 2011]\n* On Deep Learning for Trust-Aware Recommendations in Social Networks [IEEE 2017]\n* Learning to Rank with Trust and Distrust in Recommender Systems [RecSys 2017]\n* Social Attentional Memory Network: Modeling Aspect- and Friend-level Differences in Recommendation [WSDM 2019]\n    - code : https:\/\/github.com\/chenchongthu\/SAMN\n* Session-based Social Recommendation via Dynamic Graph Attention Networks [WSDM 2019]\n  - code : https:\/\/github.com\/DeepGraphLearning\/RecommenderSystems\/tree\/master\/socialRec\n* Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems [WWW 2019]\n* Heterogeneous Graph Attention Network [WWW 2019]\n* Graph Neural Networks for Social Recommendation [WWW 2019]\n* GhostLink: Latent Network Inference for Influence-aware Recommendation [WWW 2019]\n* SamWalker: Social Recommendation with Informative Sampling Strategy [WWW 2019]\n* Social Recommendation with Optimal Limited Attention [KDD 2019]\n\n## Recommendation Systems with Text Information\n  ### Topic-based approach\n  * Collaborative topic modeling for recommending scientific articles [KDD 2011]\n    - code : https:\/\/github.com\/blei-lab\/ctr\n  * Hidden factors and hidden topics: understanding rating dimensions with review text [RecSys 2013]\n    - code : https:\/\/github.com\/lipiji\/HFT\n  * Jointly modeling aspects, ratings and sentiments for movie recommendation [KDD 2014]\n    - code : https:\/\/github.com\/nihalb\/JMARS\n  * Ratings meet reviews, a combined approach to recommend [RecSys 2014]\n  * Exploring User-Specific Information in Music Retrieval [SIGIR 2018]\n  * Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and Reviews [WWW 2018]\n    - code : https:\/\/github.com\/hustlingchen\/ALFM\n  * Exploiting Ratings, Reviews and Relationships for Item Recommendations in Topic Based Social Networks [WWW 2019]\n  \n  ### Deep learning-based approach\n  * Collaborative deep learning for recommender systems [KDD 2015]\n    - code : https:\/\/github.com\/js05212\/CDL\n  * Convolutional Matrix Factorization for Document Context-Aware Recommendation [RecSys 2016]\n    - code : https:\/\/github.com\/cartopy\/ConvMF\n  * Joint Deep Modeling of Users and Items Using Reviews for Recommendation [WSDM 2017]\n    - code : https:\/\/github.com\/chenchongthu\/DeepCoNN\n  * Transnets: Learning to transform for recommendation [RecSys 2017]\n    - code : https:\/\/github.com\/rosecatherinek\/TransNets\n  * Latent Cross: Making Use of Context in Recurrent Recommender Systems [WSDM 2018]\n  * Coevolutionary Recommendation Model: Mutual Learning between Ratings and Reviews [WWW 2018]\n  * Neural Attentional Rating Regression with Review-level Explanations [WWW 2018]\n    - code : https:\/\/github.com\/chenchongthu\/NARRE\n  * Learning Personalized Topical Compositions with Item Response Theory [WSDM 2019]\n  * Uncovering Hidden Structure in Sequence Data via Threading Recurrent Models [WSDM 2019]\n  * Gated Attentive-Autoencoder for Content-Aware Recommendation [WSDM 2019]\n    - code : https:\/\/github.com\/allenjack\/GATE\n  * DAML: Dual Attention Mutual Learning between Ratings and Reviews for Item Recommendation [KDD 2019]   \n    \n## Explainable Recommendation Systems\n* Social Collaborative Viewpoint Regression with Explainable Recommendations [WSDM 2017]\n* Explainable Recommendation via Multi-Task Learning in Opinionated Text Data [SIGIR 2018]\n* TEM: Tree-enhanced Embedding Model for Explainable Recommendation [WWW 2018]\n\n## Session-Based Recommendation Systems\n### Markov-chain based approach\n* Factorizing Personalized Markov Chains for Next-Basket Recommendation [WWW 2010]\n* Where You Like to Go Next: Successive Point-of-Interest Recommendation [IJCAI 2013]\n* Learning Hierarchical Representation Model for NextBasket Recommendation [SIGIR 2015]\n* Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation [ICDM 2016]\n* Translation-based Recommendation [RecSys 2017]\n    - code : https:\/\/drive.google.com\/file\/d\/0B9Ck8jw-TZUEVmdROWZKTy1fcEE\/view\n    \n### RNN based approach\n* Session-based Recommendations with Recurrent Neural Networks [ICLR 2016]\n  - code : https:\/\/github.com\/hidasib\/GRU4Rec\n* Neural Attentive Session-based Recommendation [CIKM 2017]\n  - code : https:\/\/github.com\/lijingsdu\/sessionRec_NARM\n* Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks [RecSys 2017]\n* When Recurrent Neural Networks meet the Neighborhood for Session-Based Recommendation [RecSys 2017]\n* Modeling User Session and Intent with an Attention-based Encoder-Decoder Architecture [RecSys 2017]\n* Learning from History and Present: Next-item Recommendation via Discriminatively Exploting Users Behaviors [KDD 2018]\n* Recurrent Neural Networks with Top-k Gains for Session-based Recommendations [CIKM 2018]\n* Hierarchical Context enabled Recurrent Neural Network for Recommendation. [AAAI 2019] \n* RepeatNet: A Repeat Aware Neural Recommendation Machine for Session-based Recommendation [AAAI 2019]\n  - code : https:\/\/github.com\/PengjieRen\/RepeatNet\n* Time is of the Essence: a Joint Hierarchical RNN and Point Process Model for Time and Item Predictions [WSDM 2019]\n  - code : https:\/\/github.com\/BjornarVass\/Recsys\n* Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks [KDD 2019]\n* AIR: Attentional Intention-Aware Recommender Systems [ICDE 2019]\n\n### CNN based approach \n* 3D Convolutional Networks for Session-based Recommendation with Content Features [RecSys 2017]\n* Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding [WSDM 2018]\n  - code : https:\/\/github.com\/graytowne\/caser_pytorch [Pytorch]\n  - code : https:\/\/github.com\/graytowne\/caser [Matlab]\n* Hierarchical Temporal Convolutional Networks for Dynamic Recommender Systems [WWW 2019]\n* A Simple Convolutional Generative Network for Next Item Recommendation [WSDM 2019]\n  - code : https:\/\/github.com\/graytowne\/caser_pytorch\n  \n### Graph based approach\n* Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba [KDD 2018]\n* Graph Convolutional Neural Networks for Web-Scale Recommender Systems [KDD 2018]\n* Session-based Recommendation with Graph Neural Networks [AAAI 2019]\n  - code : https:\/\/github.com\/CRIPAC-DIG\/SR-GNN\n* Session-based Social Recommendation via Dynamic Graph Attention Networks [WSDM 2019]\n  - code : https:\/\/github.com\/DeepGraphLearning\/RecommenderSystems\/tree\/master\/socialRec  \n* Graph Contextualized Self-Attention Network for Session-based Recommendation [IJCAI 2019]\n\n### Other approach\n* Diversifying Personalized Recommendation with User-session Context [IJCAI 2017]\n* Translation-based Factorization Machines for Sequential Recommendation [RecSys 2018]\n* Attention-Based Transactional Context Embedding for Next-Item Recommendation [AAAI 2018]\n* STAMP: Short-Term Attention\/Memory Priority Model for Session-based Recommendation [KDD 2018]\n  - code : https:\/\/github.com\/uestcnlp\/STAMP\n* Self-Attentive Sequential Recommendation [ICDM 2018]\n  - code : https:\/\/github.com\/kang205\/SASRec\n* Taxonomy-aware Multi-hop Reasoning Networks for Sequential Recommendation [WSDM 2019]\n  - code : https:\/\/github.com\/RUCDM\/TMRN\n* Hierarchical Neural Variational Model for Personalized Sequential Recommendation [WWW 2019]\n* BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer [CIKM 2019]\n* Hierarchical Gating Networks for Sequential Recommendation [KDD 2019]\n* Online Purchase Prediction via Multi-Scale Modeling of Behavior Dynamics [KDD 2019]\n* Streaming Session-based Recommendation [KDD 2019]\n* Hierarchical Gating Networks for Sequential Recommendation [KDD 2019]\n* Log2Intent: Towards Interpretable User Modeling via Recurrent Semantics Memory Unit [KDD 2019]\n\n### News Recommendation\n* Google news personalization: scalable online collaborative filtering [WWW 2007]\n* Personalized News Recommendation Based on Click Behavior [IUI 2009]\n* Personalized News Recommendation Using Twitter [IEEE 2013]\n* Recommending Personalized News in Short User Sessions [RecSys 2017]\n* Embedding-based News Recommendation for Millions of Users [KDD 2017]\n* DKN: Deep Knowledge-Aware Network for News Recommendation [WWW 2018] \n* NPA: Neural News Recommendation with Personalized Attention [KDD 2019]\n\n### Video Recommendation\n* Video suggestion and discovery for youtube: taking random walks through the view graph [WWW 2008]\n* The YouTube Video Recommendation System [RecSys 2010]\n* Deep Neural Networks for YouTube Recommendations [RecSys 2016]\n* Wide & Deep Learning for Recommender Systems [DLRS 2016]\n* Content-based Related Video Recommendations [NIPS 2016]\n\n### Music Recommendation\n* Playlist prediction via metric embedding [KDD 2012]\n* Deep content-based music recommendation [NIPS 2013]\n* Improving Content-based and Hybrid Music Recommendation using Deep Learning [MM 2014]\n* Content-aware collaborative music recommendation using pre-trained neural networks [ISMIR 2015] \n\n### Image Recommendation\n* Pagerank for product image search [WWW 2008]\n* Related Pins at Pinterest: The Evolution of a Real-World Recommender System [WWW 2017]\n* Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time [WWW 2018]\n\n## Time-aware Recommendation (Temporal Dynamics)\n* Time Weight Collaborative Filtering [CIKM 2005]\n* Collaborative Filtering with Temporal Dynamics [KDD 2009]\n* Opportunity Models for E-commerce Recommendation: Right Product, Right Time [SIGIR 2013] \n* Multi-rate deep learning for temporal recommendation [SIGIR 2016]\n* Recurrent Recommender Networks [WSDM 2017]\n* Recurrent Recommendation with Local Coherence [WSDM 2019]\n\n## Multi-Armed Bandit\n* A Contextual-Bandit Approach to Personalized News Article Recommendation [WWW 2010]\n* A survey of online experiment design with the stochastic multi-armed bandit [2015] [[__pdf__](https:\/\/arxiv.org\/pdf\/1510.00757.pdf)]\n* Collaborative filtering as a multi-armed bandit [NIPS 2015]\n* Online Context-Aware Recommendation with Time Varying Multi-Arm Bandit [KDD 2016]\n* Collaborative Filtering Bandits [SIGIR 2016]\n\n## Out of Category\n* Learning Multiple Similarities of Users and Items in Recommender Systems [ICDM 2017]\n* Neural Collaborative Filtering [WWW 2017]\n* MRNet-Product2Vec: A Multi-task Recurrent Neural Network for Product Embeddings [ECML-PKDD 2017]\n* A Gradient-based Adaptive Learning Framework for Efficient Personal Recommendation [RecSys 2017]\n* IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models [SIGIR 2017]\n  - code : https:\/\/github.com\/geek-ai\/irgan\n* Collaborative Memory Network for Recommendation Systems [SIGIR 2018]\n  - code : https:\/\/github.com\/tebesu\/CollaborativeMemoryNetwork\n* Variational Autoencoders for Collaborative Filtering [WWW 2018]\n* Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking [WWW 2018]\n* Causal Embeddings for Recommendation [RecSys 2018] \n  - https:\/\/github.com\/criteo-research\/CausE\n* Linked Variational AutoEncoders for Inferring Substitutable and Supplementary Items [WSDM 2019]\n  - https:\/\/github.com\/VRM1\/WSDM19\n* RecWalk: Nearly Uncoupled Random Walks for Top-N Recommendation [WSDM 2019]\n  - https:\/\/github.com\/nikolakopoulos\/RecWalk\n\n","fb6a83b0":"# Exploring MovieLens 100K Datasets","f7853313":"# Factors Based Recommendations\n#### This Methods work more by finding  similar movies rather than user ratings but work very fine "}}