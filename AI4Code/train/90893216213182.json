{"cell_type":{"563e44b6":"code","32158f14":"code","f87313b0":"code","3e9d8362":"code","eb21e19a":"code","be69996e":"code","710b6926":"code","69ff9e8e":"code","904618bc":"code","d9962fd0":"code","3ae94842":"code","2243515f":"code","fa2192ee":"markdown","d02dcd57":"markdown","7e2bf714":"markdown","6492b6a5":"markdown","fc131735":"markdown","e7162b10":"markdown","6106b086":"markdown"},"source":{"563e44b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32158f14":"!pip install pyspark","f87313b0":"import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nimport numpy as np\nimport scipy.sparse as sps\nfrom pyspark.ml.linalg import Vectors","3e9d8362":"from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator","eb21e19a":"path = '\/kaggle\/input\/car-acceptability-prediction\/train.csv'\n\ndf_train = spark.read.csv(path, header=True)\n\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df_train) \\\n            for column in list(set(df_train.columns)-set(['car_id', 'acceptability'])) ]\npipeline = Pipeline(stages=indexers)\ndf_train = pipeline.fit(df_train).transform(df_train)\n\nlabelIndexer = StringIndexer(inputCol=\"acceptability\", outputCol=\"indexedLabel\").fit(df_train)\n\ndf_train = VectorAssembler().setInputCols(['maintenance_price_index', \\\n                                            'carry_capacity_index', 'trunk_size_index', 'safety_index',\n                                            'buying_price_index']).setOutputCol('features').transform(df_train)\n\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df_train)","be69996e":"dt = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\nmodel = pipeline.fit(df_train)","710b6926":"(trainingData, testData) = df_train.randomSplit([0.7, 0.3])\n\ndt = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)","69ff9e8e":"path = '\/kaggle\/input\/car-acceptability-prediction\/test.csv'\n\ndf_test = spark.read.csv(path, header=True)\n\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df_test) \\\n            for column in list(set(df_test.columns)-set(['car_id'])) ]\npipeline = Pipeline(stages=indexers)\ndf_test = pipeline.fit(df_test).transform(df_test)\n\ndf_test = VectorAssembler().setInputCols(['maintenance_price_index', \\\n                                            'carry_capacity_index', 'trunk_size_index', 'safety_index',\n                                            'buying_price_index']).setOutputCol('features').transform(df_test)\n\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df_test)","904618bc":"predictions = model.transform(df_test)\npredictions.show()","d9962fd0":"from pyspark.sql.functions import *","3ae94842":"temp = predictions.select('car_id', 'prediction')\n\ntemp = temp.withColumn(\"prediction\", temp[\"prediction\"].cast('string'))\\\n.withColumnRenamed('prediction', 'acceptability')\n\ntemp = temp.replace('1.0', 'acc').replace('0.0', 'unacc').replace('2.0', 'good').replace('3.0', 'vgood')","2243515f":"temp.toPandas().to_csv('result.csv', index = False)","fa2192ee":"# **Test**","d02dcd57":"# **To CSV**","7e2bf714":"# **Train**","6492b6a5":"# **Decision Tree**","fc131735":"# **Evaluate**","e7162b10":"V\u00f5 Th\u00e0nh Trung D\u0169ng - 18520641\n\nNguy\u1ec5n Thanh T\u01b0\u1eddng Vi  - 18521636\n\nNguy\u1ec5n Tr\u1ecdng Thu\u1eadn - 18521471","6106b086":"# **Predict**"}}