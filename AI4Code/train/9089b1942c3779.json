{"cell_type":{"99fdd8b4":"code","a0744784":"code","dd33271a":"code","6552a0c1":"code","8280ceb1":"code","94af89f2":"code","7a92d584":"code","0f063246":"code","1e3102e7":"code","1f3c7880":"code","f07b6aaf":"code","df07554e":"code","f5b84513":"code","4f7e5145":"code","fda9f426":"code","1053e5c0":"code","cedbe263":"code","0a049e9c":"code","fc3edaa3":"code","364d7216":"code","a48b453f":"code","6376b4ad":"code","2a580084":"code","f04aea54":"code","fdbcac4b":"code","0a12b887":"code","dd44a0cf":"code","e4f04ba6":"code","a6a82d1e":"code","2d5dcf7a":"code","49e33e5f":"code","75e2c024":"code","5406b240":"code","b06e9ddc":"code","6fc6882f":"code","64970d3e":"code","b92dd125":"code","db842ee1":"code","ba99c810":"code","835e55ca":"code","c3bb3ef5":"code","53d5cc81":"code","35d4c4b7":"code","871143ac":"code","f9c03830":"code","c8c3a9e9":"code","a5415000":"code","9f8c4b27":"code","3293b05e":"code","425dc60d":"code","55fa475b":"code","d8811bd2":"code","88d326d7":"markdown","e1e4bcf0":"markdown","2b7b1eef":"markdown","530dc4b2":"markdown","f6024a3f":"markdown","1f5f1a3b":"markdown","00b286f1":"markdown","b60e304a":"markdown","14d3f377":"markdown","aade3b0b":"markdown","be26057f":"markdown","c6997251":"markdown","b0a350e7":"markdown","f2bc3a30":"markdown","238310aa":"markdown","9847b82a":"markdown","39e3a8b5":"markdown"},"source":{"99fdd8b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0744784":"#import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, KFold\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nimport random\nfrom sklearn.svm import SVC\nimport sklearn.metrics as sk\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import recall_score,auc, roc_auc_score, roc_curve,confusion_matrix,classification_report","dd33271a":"#change the dataset location\ndf = pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-additional-full.csv', sep = ';')\ndf.shape","6552a0c1":"#viewing data\ndf.head()","8280ceb1":"#checking descriptive stats\ndf.describe()","94af89f2":"#data info\ndf.info()\n#No null values in the data","7a92d584":"#Removing non-relevant variables\ndf1=df.drop(columns=['day_of_week','month','contact','poutcome'],axis=1)\ndf1","0f063246":"#Replacing all the binary variables to 0 and 1\ndf1.y.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.default.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.housing.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.loan.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1","1e3102e7":"#creating Dummies for categorical variables\ndf2 = pd.get_dummies(df1)\ndf2.head()","1f3c7880":"#Removing extra dummy variables & checking descriptive stats\ndf3=df2.drop(columns=['job_unknown','marital_divorced','education_unknown'],axis=1)\ndf3.describe().T","f07b6aaf":"#Correlation plot\nplt.figure(figsize=(14,8))\ndf3.corr()['y'].sort_values(ascending = False).plot(kind='bar')","df07554e":"#Creating binary classification target variable\ndf_target=df3[['y']].values\ndf_features=df3.drop(columns=['y'],axis=1).values\ndf_target1=df3[['y']]\ndf_features1=df3.drop(columns=['y'],axis=1)","f5b84513":"##Feature Selection\nfrom mlxtend.feature_selection import SequentialFeatureSelector\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier  \nfeature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1),  \n           k_features=12,\n           forward=True,\n           verbose=2,\n           scoring='roc_auc',\n           cv=2)\nfeatures = feature_selector.fit(df_features1,df_target1)\nfiltered_features= df_features1.columns[list(features.k_feature_idx_)] \nfiltered_features","4f7e5145":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\npca = PCA()\npca_X=pca.fit_transform(df_features)\npca.get_covariance()","fda9f426":"explained_variance=pca.explained_variance_ratio_\nexplained_variance.shape","1053e5c0":"plt.figure(figsize=(6, 4))\nplt.bar(range(40), explained_variance, alpha=0.5, align='center',label='individual explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.tight_layout()","cedbe263":"pca = PCA(n_components=2)\npca_X=pca.fit_transform(df_features)\npca.get_covariance()","0a049e9c":"explained_variance=pca.explained_variance_ratio_\nexplained_variance","fc3edaa3":"plt.figure(figsize=(8,4))\nplt.title('PCA Components')\nplt.scatter(pca_X[:,0], pca_X[:,1])","364d7216":"from sklearn.decomposition import FastICA \nica = FastICA(n_components=3, random_state=2) \nica_X=ica.fit_transform(df_features)\nica_X.shape","a48b453f":"plt.figure(figsize=(8,4))\nplt.title('ICA Components')\nplt.scatter(ica_X[:,0], ica_X[:,1])\nplt.scatter(ica_X[:,1], ica_X[:,2])\nplt.scatter(ica_X[:,2], ica_X[:,0])","6376b4ad":"from sklearn.random_projection import GaussianRandomProjection\nrca = GaussianRandomProjection(n_components=3, eps=0.1, random_state=2)\nrca_X=rca.fit_transform(df_features)\nrca_X.shape","2a580084":"plt.figure(figsize=(12,8))\nplt.title('RCA Components')\nplt.scatter(rca_X[:,0], rca_X[:,1])\nplt.scatter(rca_X[:,1], rca_X[:,2])\nplt.scatter(rca_X[:,2], rca_X[:,0])","f04aea54":"# plot data\nplt.scatter(\n   df_features[:, 0], df_features[:, 1],\n   c='white', marker='o',\n   edgecolor='black', s=50\n)\nplt.show()","fdbcac4b":"##Determining number of clusters\nfrom sklearn.cluster import KMeans \nSum_of_squared_distances = []\nK = range(1,16)\nfor k in K:\n    km = KMeans(n_clusters=k, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\n    km=km.fit(df_features)\n    Sum_of_squared_distances.append(km.inertia_)\n##Checking out which SSE is low for different types of k means value\nplt.plot(K,Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow method for optimal k')\nplt.show()","0a12b887":"# Fitting K-Means to the dataset\nfrom scipy import stats\n\nkmeans = KMeans(n_clusters = 2, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\nprediction = kmeans.fit_predict(df_features)\nprint(prediction)\n\nplt.scatter(df_features[:, 0], df_features[:, 1], c=prediction, s=50)\ncenters = kmeans.cluster_centers_","dd44a0cf":"centers = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)","e4f04ba6":"df_features1","a6a82d1e":"df_fs=df_features1[['age', 'housing_0','housing_1','housing_unknown', 'loan_0','loan_1','loan_unknown', 'duration', 'campaign',\n       'job_admin.', 'job_self-employed', 'job_technician', 'marital_single',\n       'education_university.degree']].values\ndf_fs","2d5dcf7a":"##Determining number of clusters\nfrom sklearn.cluster import KMeans \nSum_of_squared_distances = []\nK = range(1,16)\nfor k in K:\n    km = KMeans(n_clusters=k, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\n    km=km.fit(df_fs)\n    Sum_of_squared_distances.append(km.inertia_)\n##Checking out which SSE is low for different types of k means value\nplt.plot(K,Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow method for optimal k')\nplt.show()","49e33e5f":"# Fitting K-Means to the dataset\nfrom scipy import stats\n\nkmeans = KMeans(n_clusters = 2, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\nprediction = kmeans.fit_predict(df_fs)\nprint(prediction)\n\nplt.scatter(df_fs[:, 0], df_fs[:, 1], c=prediction, s=50, cmap='viridis_r')","75e2c024":"centers = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)","5406b240":"##Determining number of clusters\nfrom sklearn.cluster import KMeans \nSum_of_squared_distances = []\nK = range(1,16)\nfor k in K:\n    km = KMeans(n_clusters=k, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\n    km=km.fit(pca_X)\n    Sum_of_squared_distances.append(km.inertia_)\n##Checking out which SSE is low for different types of k means value\nplt.plot(K,Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow method for optimal k')\nplt.show()","b06e9ddc":"# Fitting K-Means to the dataset\nfrom scipy import stats\n\nkmeans = KMeans(n_clusters = 2, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\nprediction = kmeans.fit_predict(pca_X)\nprint(prediction)\n\nplt.scatter(pca_X[:, 0], pca_X[:, 1], c=prediction, s=50, cmap='viridis_r')\ncenters = kmeans.cluster_centers_\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)","6fc6882f":"##Determining number of clusters\nfrom sklearn.cluster import KMeans \nSum_of_squared_distances = []\nK = range(1,16)\nfor k in K:\n    km = KMeans(n_clusters=k, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\n    km=km.fit(ica_X)\n    Sum_of_squared_distances.append(km.inertia_)\n##Checking out which SSE is low for different types of k means value\nplt.plot(K,Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow method for optimal k')\nplt.show()","64970d3e":"# Fitting K-Means to the dataset\nfrom scipy import stats\n\nkmeans = KMeans(n_clusters = 2, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\nprediction = kmeans.fit_predict(ica_X)\nprint(prediction)\n\nplt.scatter(ica_X[:, 0], ica_X[:, 1], c=prediction, s=50, cmap='viridis_r')\ncenters = kmeans.cluster_centers_\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)","b92dd125":"##Determining number of clusters\nfrom sklearn.cluster import KMeans \nSum_of_squared_distances = []\nK = range(1,16)\nfor k in K:\n    km = KMeans(n_clusters=k, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\n    km=km.fit(rca_X)\n    Sum_of_squared_distances.append(km.inertia_)\n##Checking out which SSE is low for different types of k means value\nplt.plot(K,Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow method for optimal k')\nplt.show()","db842ee1":"# Fitting K-Means to the dataset\nfrom scipy import stats\n\nkmeans = KMeans(n_clusters = 2, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\nprediction = kmeans.fit_predict(rca_X)\nprint(prediction)\n\nplt.scatter(rca_X[:, 0], rca_X[:, 1], c=prediction, s=50, cmap='viridis_r')\ncenters = kmeans.cluster_centers_\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)","ba99c810":"## Expectation maximization analysis\nfrom sklearn.mixture import GaussianMixture\nem = GaussianMixture(n_components=2,random_state=2,covariance_type='tied')\nem_pred = em.fit_predict(df_features)\nem_probs = em.predict_proba(df_features)\n#em.means_\n#em.covariances_\nplt.scatter(df_features[:, 0], df_features[:, 1], c=em_pred, s=50, cmap='viridis_r')","835e55ca":"## Expectation maximization analysis\nfrom sklearn.mixture import GaussianMixture\nem = GaussianMixture(n_components=2,random_state=2,covariance_type='tied')\nem_pred = em.fit_predict(df_fs)\nem_probs = em.predict_proba(df_fs)\n#em.means_\n#em.covariances_\nplt.scatter(df_fs[:, 0], df_fs[:, 1], c=em_pred, s=50, cmap='viridis_r')","c3bb3ef5":"## Expectation maximization analysis\nfrom sklearn.mixture import GaussianMixture\nem = GaussianMixture(n_components=2,random_state=2,covariance_type='tied')\nem_pred = em.fit_predict(pca_X)\nem_probs = em.predict_proba(pca_X)\n#em.means_\n#em.covariances_\nplt.scatter(pca_X[:, 0], pca_X[:, 1], c=em_pred, s=50, cmap='viridis_r')","53d5cc81":"## Expectation maximization analysis\nfrom sklearn.mixture import GaussianMixture\nem = GaussianMixture(n_components=2,random_state=2,covariance_type='tied')\nem_pred = em.fit_predict(ica_X)\nem_probs = em.predict_proba(ica_X)\n#em.means_\n#em.covariances_\nplt.scatter(ica_X[:, 0], ica_X[:, 1], c=em_pred, s=50, cmap='viridis_r')","35d4c4b7":"## Expectation maximization analysis\nfrom sklearn.mixture import GaussianMixture\nem = GaussianMixture(n_components=2,random_state=2,covariance_type='tied')\nem_pred = em.fit_predict(rca_X)\nem_probs = em.predict_proba(rca_X)\n#em.means_\n#em.covariances_\nplt.scatter(rca_X[:, 0], rca_X[:, 1], c=em_pred, s=50, cmap='viridis_r')","871143ac":"x1_train, x1_test, y1_train, y1_test = train_test_split(pca_X, df_target, test_size = 0.3, random_state = 0)","f9c03830":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(32,activation=\"softmax\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(16,activation=\"softmax\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)","c8c3a9e9":"# Making the Confusion Matrix\ndef confusionmat(y,y_hat):\n  from sklearn.metrics import confusion_matrix,accuracy_score\n  cm = confusion_matrix(y, y_hat)\n  accu=accuracy_score(y,y_hat)\n  print(cm,\"\\n\")\n  print(\"The accuracy is\",accu)\n\n#Accuracy and Loss Curves\ndef learningcurve(history):\n  # list all data in history\n  print(history.history.keys())\n  # summarize history for accuracy\n  plt.plot(history.history['accuracy'])\n  plt.plot(history.history['val_accuracy'])\n  plt.title('model accuracy')\n  plt.ylabel('accuracy')\n  plt.xlabel('epoch')\n  plt.legend(['train', 'test'], loc='upper left')\n  plt.show()\n  # summarize history for loss\n  plt.plot(history.history['loss'])\n  plt.plot(history.history['val_loss'])\n  plt.title('model loss')\n  plt.ylabel('loss')\n  plt.xlabel('epoch')\n  plt.legend(['train', 'test'], loc='upper left')\n  plt.show()","a5415000":"# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)","9f8c4b27":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters = 2, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\nprediction = kmeans.fit_predict(x1_train)\n\nem = GaussianMixture(n_components=2,random_state=2,covariance_type='tied')\nem_pred = em.fit_predict(x1_train)\nem_probs = em.predict_proba(x1_train)\n\ntrain_df = pd.DataFrame()\ntrain_df['KM_Pred']=prediction\ntrain_df['EM_Prob']=em_probs[:,1]\ntrain_df['y']=y1_train\ntrain_df","3293b05e":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters = 2, n_init=10, max_iter=300, init = 'k-means++', random_state = 2)\nprediction = kmeans.fit_predict(x1_test)\n\nem = GaussianMixture(n_components=2,random_state=2,covariance_type='tied')\nem_pred = em.fit_predict(x1_test)\nem_probs = em.predict_proba(x1_test)\n\ntest_df = pd.DataFrame()\ntest_df['KM_Pred']=prediction\ntest_df['EM_Prob']=em_probs[:,1]\ntest_df['y']=y1_test\ntest_df","425dc60d":"#Creating binary classification target variable\ntrain_y=train_df[['y']].values\ntrain_x=train_df.drop(columns=['y'],axis=1).values\ntest_y=test_df[['y']]\ntest_x=test_df.drop(columns=['y'],axis=1)","55fa475b":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(32,activation=\"softmax\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(16,activation=\"softmax\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(train_x, train_y, batch_size = 10, epochs=100,validation_split=0.3)","d8811bd2":"# Predicting the Test set results\ny_pred = classifier.predict_classes(test_x)\npre_score = sk.average_precision_score(test_y, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(test_x, test_y)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(test_y,y_pred)\nlearningcurve(history)","88d326d7":"##With PCA","e1e4bcf0":"##With Feature Selection","2b7b1eef":"##With RCA","530dc4b2":"#**Data** **Preprocessing**","f6024a3f":"##RCA","1f5f1a3b":"#Expectation Maximization","00b286f1":"##ICA","b60e304a":"##With Feature Selection","14d3f377":"#ANN after PCA","aade3b0b":"##PCA","be26057f":"##Feature Selection","c6997251":"##With ICA","b0a350e7":"#K-Means","f2bc3a30":"##With PCA","238310aa":"##With ICA","9847b82a":"##With RCA","39e3a8b5":"#Task 5"}}