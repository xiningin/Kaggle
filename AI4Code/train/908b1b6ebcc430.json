{"cell_type":{"44a96d4c":"code","d9d2494c":"code","b085b203":"code","322d8bde":"code","7d6d4d8d":"code","b981a6ee":"code","9a440484":"code","6ca02d6b":"code","e6941051":"code","88d249d3":"code","a2b0c240":"code","fde6a8ec":"code","bfbde261":"code","b5bc75e4":"code","7aa9bdf6":"code","a7f70cbc":"code","1a4ced48":"code","980a4527":"code","dc30e8fe":"code","060460af":"code","9d9efe21":"code","b1e9cbf3":"code","e1ea1b54":"code","adef15e0":"code","e50e3149":"code","b9edf362":"code","7f01fd12":"code","6bf53122":"code","95e952c2":"code","99861f02":"code","b54ef5cf":"code","bb86be39":"code","217c7a4e":"code","73fb6011":"code","53fb98b9":"code","4b40c157":"code","e1dea35e":"code","c07b097c":"code","5fe8eaee":"code","d009fb5e":"code","54b8dc9a":"code","3f9e187e":"code","d35ab046":"code","19163a90":"code","f95a0865":"code","73f2e065":"code","c4237879":"code","4a4f89ca":"code","261f31cf":"code","acd57f8e":"code","36384395":"code","8df2320a":"code","64b77607":"code","4354938b":"code","a61f68f4":"code","8a6c432a":"code","ae272b8f":"code","756f1a95":"code","0017609a":"code","b0935762":"code","32d29d09":"code","6c521231":"code","736ee255":"markdown","1d2a6049":"markdown","69e54c84":"markdown","5e1b427d":"markdown","241e1155":"markdown","5b3e0e95":"markdown","338ebcf3":"markdown","5fe0e175":"markdown","a9c07a0a":"markdown","2474d8eb":"markdown","c68e4c43":"markdown","39130ff7":"markdown","6dea5d4e":"markdown"},"source":{"44a96d4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9d2494c":"#import warnings\n#warnings.filterwarnings(\"ignore\")","b085b203":"import warnings\nwarnings.filterwarnings(\"ignore\")","322d8bde":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","7d6d4d8d":"df_train.shape","b981a6ee":"df_train.head()","9a440484":"print(df_test.shape)\ndf_test.head()","6ca02d6b":"df_train.dtypes.value_counts()","e6941051":"df_test.dtypes.value_counts()","88d249d3":"plt.figure(figsize=(20,10))\nsns.heatmap(df_train.isna())","a2b0c240":"plt.figure(figsize=(20,10))\nsns.heatmap(df_test.isna())","fde6a8ec":"(df_train.isna().sum()\/df_train.shape[0]).sort_values(ascending=True)","bfbde261":"(df_test.isna().sum()\/df_test.shape[0]).sort_values(ascending=True)","b5bc75e4":"df_train = df_train[df_train.columns[df_train.isna().sum()\/df_train.shape[0] <0.8]]\ndf_train.head()","7aa9bdf6":"df_train.shape","a7f70cbc":"df_test = df_test[df_test.columns[df_test.isna().sum()\/df_test.shape[0] <0.8]]\ndf_test.head()","1a4ced48":"plt.figure(figsize=(20,10))\nsns.heatmap(df_train.isna())","980a4527":"plt.figure(figsize=(20,10))\nsns.heatmap(df_test.isna())","dc30e8fe":"df_train['SalePrice'].describe()","060460af":"df_train['SalePrice'].value_counts()","9d9efe21":"plt.figure()\nsns.displot(df_train['SalePrice'])","b1e9cbf3":"from scipy import stats\nfrom scipy.stats import norm","e1ea1b54":"sns.distplot(df_train['SalePrice'] , fit=norm)\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nplt.title('Normal distribution \\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","adef15e0":"plt.figure()\nstats.probplot(df_train['SalePrice'],plot=plt)\nplt.show()","e50e3149":"#df_train['SalePrice']=np.log(df_train['SalePrice'])","b9edf362":"sns.distplot(df_train['SalePrice'] , fit=norm)\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nplt.title('Normal distribution \\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","7f01fd12":"for col in df_train.select_dtypes('float'):\n    plt.figure()\n    sns.distplot(df_train[col])","6bf53122":"for col in df_train.select_dtypes('object'):\n    plt.figure()\n    sns.boxplot(y=df_train['SalePrice'],x=df_train[col],data=df_train)","95e952c2":"for col in df_train.select_dtypes('object'):\n    print(col,df_train[col].unique())","99861f02":"for col in df_train.select_dtypes('int'):\n    plt.figure()\n    sns.distplot(df_train[col])","b54ef5cf":"plt.figure(figsize=(20,10))\nsns.heatmap(df_train.corr(),annot=True)","bb86be39":"df_train.corr()['SalePrice'].sort_values(ascending=False)","217c7a4e":"sns.heatmap(df_train.corr()[['SalePrice','OverallQual']])","73fb6011":"highestcorr = df_train.corr().index[df_train.corr()[\"SalePrice\"] > 0.5]\n\nplt.figure(figsize=(20,8))\nsns.heatmap(df_train[highestcorr].corr(),annot=True)","53fb98b9":" df_train.groupby('MoSold')['SalePrice'].count().plot(kind='bar', alpha=0.3,title='Number')","4b40c157":"sns.scatterplot(x='SalePrice',y='GrLivArea',data=df_train)","e1dea35e":"df_train =df_train[(df_train['SalePrice']<700000)&(df_train['GrLivArea']<4000)]","c07b097c":"df_train.shape","5fe8eaee":"df_train.head()","d009fb5e":"df_train.dtypes.value_counts()","54b8dc9a":"y=df_train['SalePrice']\ndf_train=df_train.drop(['SalePrice','Id'],axis=1)\nID=df_test['Id']\ndf_test=df_test.drop('Id',axis=1)\n","3f9e187e":"df_train.select_dtypes(['int','float']).var().sort_values(ascending=False)","d35ab046":"from sklearn.preprocessing  import LabelEncoder\nfrom sklearn.model_selection import train_test_split","19163a90":"#df_train_quant = df_train.select_dtypes(include=['number'])\n#df_train_cat = df_train.select_dtypes(include=['object'])\ndef encoding(df):\n    le=LabelEncoder()\n    for  col in df.select_dtypes('object'):\n        df[col] = le.fit_transform(df[col])\n        \n    return df\n\ndef imputation(df):\n    #df['is na'] =(df['Parainfluenza 3'].isna()) | (df['Leukocytes'].isna())\n    #df=df.fillna(-999)\n    #df=df.dropna(axis=0)\n    df=df.fillna(0)\n    \n    return df\n\ndef preprocessing(df):\n  \n    df = encoding(df)\n    df=imputation(df)\n    #df= RobustScaler()\n    return df\n","f95a0865":"df_train=preprocessing(df_train)\nX_train, X_test, y_train, y_test = train_test_split(df_train, y, test_size=0.2, random_state=0)\nX_testfinal=preprocessing(df_test)","73f2e065":"print(X_train.shape,y_train.shape,X_testfinal.shape,y_test.shape)","c4237879":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\n","4a4f89ca":"def rmse(y_test,y_pred):\n    \"\"\"\n    Compute the RMSE between true labels and predictions.\n    \"\"\"\n    return np.sqrt(mean_squared_error(y_test, y_pred))\ndef evaluation(model):\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    \n    print('rmse :',rmse(y_test,y_pred))\n    \n    N, train_score, val_score=learning_curve(model,X_train,y_train,cv=4,train_sizes=np.linspace(0.1,1,10))\n    \n    plt.figure(figsize=(12,8))\n    plt.plot(N,train_score.mean(axis=1),label='train score')\n    plt.plot(N,val_score.mean(axis=1), label = 'validation score')\n    plt.legend()","261f31cf":"pd.DataFrame(model1.feature_importances_,index=X_train.columns).sort_values(by=0, ascending=False)","acd57f8e":"model2 = RandomForestRegressor(random_state=0) ","36384395":"evaluation(model2)","8df2320a":"pd.DataFrame(model2.feature_importances_,index=X_train.columns).plot.bar(figsize=(12,8))","64b77607":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","4354938b":"#tuned_parameters = {'n_estimators': [500, 700, 1000], 'max_depth': [None, 1, 2, 3], 'min_samples_split': [1, 2, 3]}\n#clf = GridSearchCV(RandomForestRegressor(), tuned_parameters, cv=5,  n_jobs=-1, verbose=1)\n#clf.fit(X_train, y_train)'''clf.best_params_","a61f68f4":"rf=RandomForestRegressor(min_samples_split=3,n_estimators=1000)","8a6c432a":"evaluation(rf)","ae272b8f":"rf.score(X_test,y_test)","756f1a95":"gb = XGBRegressor()\ngb.fit(X_train,y_train)\ny_pred = gb.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nrmse","0017609a":"#params_xgb = {'gamma': [0.25,0.03,0.04,0.05,], 'max_depth': [3,4,8], 'reg_alpha': [0,], 'reg_lambda': [1,], 'n_estimators': [500,1000,1500], 'learning_rate': [0.02,0.04,0.08],   'min_child_weight': [1,2,4,8],'n_estimators':[1000,1500]}\n\n\n\n#grid_xgb = GridSearchCV(XGBRegressor(objective='reg:squarederror'), XGBRegressor(objective='reg:squarederror', gamma=0.25,learning_rate=0.02,max_depth=3,min_child_weight=8,n_estimators=1000,reg_alpha=0,reg_lambda=1)\n                      \nxgb=XGBRegressor(objective='reg:squarederror',gamma=0.25,learning_rate=0.02,max_depth=3,min_child_weight=8,n_estimators=1000,reg_alpha=0,reg_lambda=1)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nrmse\n","b0935762":"model=xgb\nfinal_predictions= model.predict(X_testfinal)\ndf_submission = pd.DataFrame({'Id': ID, 'SalePrice': final_predictions})\nprint(df_submission.head())\ndf_submission.to_csv('submission.csv',index=False)","32d29d09":"df = pd.read_csv('submission.csv')","6c521231":"df.shape","736ee255":"# Submission","1d2a6049":"we need  to transform the saleprice variable into a more normal distribution ","69e54c84":"# Preprocessing","5e1b427d":"we see there are few outliers we need to remove","241e1155":"### Missing values","5b3e0e95":"### Drop the useless columns with 90% of NaN","338ebcf3":"# Models and predictions","5fe0e175":"### Target variable","a9c07a0a":"###  variables visualization","2474d8eb":"If you liked this notebook let me know with a +1, and if you have some ideas to improve my predictions  please give me your comments :)\n\nThanks\n","c68e4c43":"Hi everyone ! \n\nThis is my first competition project on kaggle. I applied basic feature engineering methods  and  differents models such as RandomForest and XGBoost. \nThis is a work in progress, I will work on the data cleaning and I will use Deep Learning and other models later.\n\nI am sure that there are a lot of things that I can improve. So please don't hesitate to give me your comments and ways to improve my predictions :)\n\nThanks","39130ff7":"# Exploratory Data Analysis","6dea5d4e":"we see that  SalePrice and OverallQual are highly correlated"}}