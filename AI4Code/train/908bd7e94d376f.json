{"cell_type":{"d8f57062":"code","1d093b1d":"code","5585b928":"code","946aed9c":"code","67543948":"code","ed5334c2":"code","96ce04b2":"code","4ab29762":"code","e66ccbe9":"code","379f2459":"code","cd1cba25":"code","42b442da":"code","0286504b":"code","d23e8bc5":"code","cb30ecba":"code","fa6e7808":"code","a7bc2fa6":"code","ae3a0ccf":"code","aaa98cd0":"code","b5236bb7":"code","3e493fb0":"code","d266d1a8":"markdown","4fbd0965":"markdown","86221a9f":"markdown","22a6db6e":"markdown","c02e4cf5":"markdown","3ff23253":"markdown","d14b821c":"markdown","2593236d":"markdown","d86c8c37":"markdown","e76474fd":"markdown","2e226c00":"markdown","5b88d978":"markdown","971e3374":"markdown","8a280dc4":"markdown","4d111853":"markdown","02dc7a5b":"markdown","dfc77b95":"markdown","79a93282":"markdown","870c197a":"markdown","a3349d63":"markdown"},"source":{"d8f57062":"!pip install neptune-client==0.9.1","1d093b1d":"!pip install neptune-contrib","5585b928":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import ensemble\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nimport optuna\nfrom optuna import visualization\nimport eli5\nfrom mlxtend import evaluate\nimport neptune\nneptune.init('shared\/sklearn-integration', api_token='ANONYMOUS')\nfrom neptunecontrib.monitoring.sklearn import log_classifier_summary\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","946aed9c":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf","67543948":"print(df.dtypes)\nprint(df.isnull().sum())","ed5334c2":"#df[\"TotalCharges\"] = df[\"TotalCharges\"].astype(float)\ndf.iloc[488]","96ce04b2":"df['TotalCharges'] = df['TotalCharges'].apply(lambda x: 0 if x == ' ' else x)\ndf[\"TotalCharges\"] = df[\"TotalCharges\"].astype(float)","4ab29762":"churn_map = {\"Yes\":1, \"No\":0}\ndf[\"Churn\"] = df[\"Churn\"].map(churn_map)\ndf[\"Churn\"].value_counts()","e66ccbe9":"y = df['Churn']\nX = df.drop(['Churn', 'customerID'], axis=1)","379f2459":"cat_list = list(X.select_dtypes(include=['object']).columns)\n\nfor i in range(len(cat_list)):\n    X[cat_list[i]] = preprocessing.LabelEncoder().fit_transform(X[cat_list[i]])","cd1cba25":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42)","42b442da":"def objective(trial, X, y):\n    params = {\n              'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n              'max_depth': trial.suggest_int('max_depth', 1, 30)\n    }\n    \n    rf = ensemble.RandomForestClassifier(class_weight='balanced', random_state=42, **params)\n    \n    score = model_selection.cross_val_score(rf, X, y, scoring='accuracy', n_jobs=-1, cv=5)\n    accuracy = score.mean()\n    return accuracy","0286504b":"sampler = optuna.samplers.TPESampler(seed=42)\nstudy = optuna.create_study(sampler = sampler, direction=\"maximize\")\nstudy.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=10)","d23e8bc5":"print(\"The best value is \",study.best_value)\nprint(\"\\nThe best parameters are \",study.best_params)","cb30ecba":"visualization.plot_parallel_coordinate(study)","fa6e7808":"clf = ensemble.RandomForestClassifier(**study.best_params, class_weight='balanced', random_state=42)\nclf.fit(X_train, y_train)\ny_proba = clf.predict_proba(X_test)","a7bc2fa6":"neptune.create_experiment(params=study.best_params, name='classification-example', tags=['RandomForest', 'classification'])","ae3a0ccf":"log_classifier_summary(clf, X_train, X_test, y_train, y_test)","aaa98cd0":"neptune.stop()","b5236bb7":"eli5.show_prediction(clf, X_test.iloc[0], feature_names=list(X_test.columns),show_feature_values=True, top=7)","3e493fb0":"eli5.show_prediction(clf, X_test.iloc[5], feature_names=list(X_test.columns),show_feature_values=True, top=7)","d266d1a8":"**I learned these techniques as I was writing this notebook. I intend to explore further and drill-down on each of tuning, tracking and explainablility to learn more**\n\n**I will make individual notebooks sharing my learnings on the depth of these topics, with more libraries and references if possible**\n\n\n**I hope you liked this notebook. If so, please consider to upvote it. Cheers !!**","4fbd0965":"As a bonus, optuna has some helpful plots to visualize the contribution of the paramters. The plot above shows how the accuracy increased from 74% to 79% and the combination of paramaters that has led to the final score across the 10 runs","86221a9f":"# ML Experiment tracking\n\nExperiment tracking is essential, as it allows us to focus on the development work and not maintaing the prototype versions and the metadata store.\n\n**For this, NEPTUNE can be very helpful**\n\nSome of the things that Neptune does: \n* Log and display metrics, parameters, images, and other ML metadata\n* Share results by sending a persistent link\n* Maintain data and associate model file","22a6db6e":"It also creates the confusion matrix, ROC-AUC curve, Precision-Recall curve and class prediction error grpahs automatically. This eliminated the need for me to plot and update these metrics and graphs for every iteration of the model training & testing phase.","c02e4cf5":"# EXPLAINABILITY\n\nDemistifying and interpreting ML models and their outputs is criticial, as it enables us to debug the mode behaviour, explain the output to a stake-holder and fine tune to the specific use-case or domain.\n\n**Here, I have focussed on Post-Hoc model explainability**\n\nPost hoc interpretability refers to the application of interpretation methods after model training","3ff23253":"Using ELI5 library, I was able to get the predictions for specific data points and the top factors that contribute to the prediction probabaility. \n\nAs shown, for both the class 0 and 1, the highest contributing factor is tenure, meaning that the feature is important to our classification\n\nThere are more interpretability techniques - which are not included here","d14b821c":"The parameters of the tuned model are also logged. \n\nI later noticed that the dataframe and the trained model file is also stored - have not included a sample image for this","2593236d":"There are no null\/NaNs in the dataset - one less thing to worry about. \n\nHowever, the Total Charges column is of object datatype, but it consists of float values - have to convert it to float type","d86c8c37":"Here, I chose 2 paramters to optimize - n_estimators and max_depth:\n* n_estimators - considers all the integers between 100 to 500\n* max_depth - considers all integers between 1 to 30\n\nIn the traditional GridSearchCV method, this would take hours to run as I have defined exhaustive number of data points to consider. However, by using Optuna I ran only 10 trials in a matter of few seconds and was able to optimize the model. Ta da!","e76474fd":"The Target feature is converted to numeric using map","2e226c00":"![Screenshot (93).png](attachment:58c32c78-094f-4362-9409-062ff78f571e.png)","5b88d978":"The CPU and RAM usage can also be monitored. This can be useful in the cases where a cloud platform charges us by the usgae amount, making resource tracking essential","971e3374":"As shown above, Experiment tracking with Neptune logs all the essential metrics whenever the log_summary function is called","8a280dc4":"# MODERN Machine Learning\n\nThe way in which we go about developing an ML model should be easy for the developer to keep track of the prototypes, enable colloborative development with the team and should utilize the computing resources efficiently.  There are 3 key things that can help us practice the modern ML development process:\n* Hyper-paramter tuning\n* Experiment tracking\n* Model output explainablility\n\nI found that some techniques and libraries can help achieve these 3 things - which after initial exploration I have compiled this notebook. \n\nThis is an intro to follow a ML development process which would make life a little bit easier.","4d111853":"![Screenshot (96).png](attachment:f3683314-5cad-4308-ae6e-9ba678a99d64.png)","02dc7a5b":"When I did usual typecasting using .astype (commented line) to convert the column to float, row 488 was sticking out as it was an empty string (as shown above). In such as case, setting coerce as True will make the it NaN during conversion, which is not ideal as I wanted to make it 0.0\n\nTo deal with this, any empty string has been converted to '0' string and then applied astype to get the intented outcome\n\n(P.S: There can be better ways to deal with this error, if so then please let me know in the comments)","dfc77b95":"**BASIC EDA & CLEANUP**","79a93282":"![Screenshot (94).png](attachment:b94a2baa-5a56-4217-88ee-eac564f7edbd.png)","870c197a":"# HYPER-PARAMETER TUNING\n\nI made all the data preparation required to run the RandomForestClassifier. Now, it is time to find the best paramters so that I can optimize the model. For this, my go to choice would be GridSerachCV\n\nIn GridSearchCV, the search space of each hyper-parameter is discrete. The optimizer fits for each of the hyper-parameter configurations and selects the best combination that gives the highest score. Many a times, I have faced with a long wait for this process to complete as it take upto 500 fits depending on the defined parameter space. Moreover, the defined parameter space is limited and because of this the search is not very exhaustive as well.\n\n**Now, Consider OPTUNA**\n\nOptuna is a framework for automatic optimization process of these hyperparameters. It finds optimal hyperparameter values by making use of different samplers. Specifically, it employs a Bayesian optimization algorithm called Tree-structured Parzen Estimator. \n\nWhat is Bayesian Optimization? It build a probability model of the object function and builds a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. The basic idea is spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. Simply, Bayesian optimization methods are efficient as they choose the next hyperparameters in an informed manner. \n","a3349d63":"![Screenshot (92).png](attachment:f0d6adf4-3738-4c7c-9513-6949a260f031.png)"}}