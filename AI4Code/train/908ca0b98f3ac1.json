{"cell_type":{"f0d9f29d":"code","d8d11c8b":"code","fbd1c3a1":"code","e3d8821e":"code","91c11add":"code","7a328f7e":"code","989e8c2c":"code","7fe58aa8":"code","0ecaebe9":"code","3c19c3c8":"code","fd52b035":"code","6a155741":"code","2e2060c6":"code","7d0d7d07":"code","c5262e7c":"code","18704240":"code","6e783bd5":"code","24c3b24b":"code","fb3769a8":"code","464a68c7":"code","deea7a68":"code","a710416e":"code","2e0bfc75":"code","a585390e":"code","a8025450":"code","9f0397d9":"code","77710391":"code","2da74a95":"code","5c4554ab":"code","6b31d0fb":"code","354e8d78":"code","96f6e3fd":"code","fc55d9c5":"code","15b4b643":"code","1a4cf3f1":"code","7caaef30":"code","b35889dd":"code","e6ed4744":"code","1c149e3a":"code","98e41295":"code","a3febaa9":"code","255b2cda":"code","76ca4736":"code","ad4bd771":"code","94f3c3cd":"code","7558fc14":"code","69bdb6a9":"code","2721318d":"code","810ab2c0":"markdown","8757285d":"markdown","f78722e0":"markdown","a7a43ea9":"markdown","ff9c392d":"markdown","d5854aa6":"markdown","16ad1b76":"markdown","9dcc5479":"markdown","78ed3d35":"markdown","dec83371":"markdown","959014d1":"markdown","facd7ef3":"markdown","b1f4916e":"markdown","90dd093f":"markdown","151cbe8f":"markdown","63c08f17":"markdown","f2fef00e":"markdown","f4a4fa9f":"markdown","46d8f9e8":"markdown","b0229cb5":"markdown","a5e12a42":"markdown","0107eda3":"markdown","09c133e0":"markdown","7e2cf2f1":"markdown","c2587166":"markdown","a589d00b":"markdown","60cf1775":"markdown","16058ff7":"markdown"},"source":{"f0d9f29d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.model_selection import GroupKFold\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# plt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","d8d11c8b":"# # Show how the files appear in the input folder\n# !ls -GFlash --color ..\/input","fbd1c3a1":"train_df = pd.read_csv('..\/input\/train.csv').sample(frac=0.04)\nprint(\"Train data shape\",train_df.shape)\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint(\"Test shape\",test_df.shape)","e3d8821e":"train_df.head()","91c11add":"test_df.head()","7a328f7e":"print('The training set has shape {}'.format(train_df.shape))\nprint('The test set has shape {}'.format(test_df.shape))","989e8c2c":"# Distribution of the target\ntrain_df['scalar_coupling_constant'].plot(kind='hist', figsize=(20, 5), bins=700, title='Distribution of the target scalar coupling constant')\nplt.show()","7fe58aa8":"# Number of of atoms in molecule\nfig, ax = plt.subplots(1, 2)\ntrain_df.groupby('molecule_name').count().sort_values('id')['id'].plot(kind='hist',\n                                                                       bins=25,\n                                                                       color=color_pal[6],\n                                                                      figsize=(20, 5),\n                                                                      title='# of Atoms in Molecule (Train Set)',\n                                                                      ax=ax[0])\ntest_df.groupby('molecule_name').count().sort_values('id')['id'].plot(kind='hist',\n                                                                       bins=25,\n                                                                       color=color_pal[2],\n                                                                      figsize=(20, 5),\n                                                                      title='# of Atoms in Molecule (Test Set)',\n                                                                     ax=ax[1])\nplt.show()","0ecaebe9":"typelist = train_df['type'].unique()\nprint(typelist)","3c19c3c8":"train_df.groupby(\"type\")['scalar_coupling_constant'].plot(kind='hist', figsize=(18, 5), bins=500, title='Distribution of the target coupling constant, given type');\n\n# plt.figure(figsize=(26, 24))\n# for i, col in enumerate(typelist):\n#     plt.subplot(4,2, i + 1)\n#     sns.distplot(train_df[train_df['type']==col]['scalar_coupling_constant'],color ='indigo')\n#     plt.title(col)","fd52b035":"train_df.head(1)","6a155741":"! cat ..\/input\/structures\/dsgdb9nsd_000001.xyz","2e2060c6":"structures = pd.read_csv('..\/input\/structures.csv')\nstructures.head()","7d0d7d07":"# 3D Plot!\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nexample = structures.loc[structures['molecule_name'] == 'dsgdb9nsd_000001']\nax.scatter(xs=example['x'], ys=example['y'], zs=example['z'], s=100)\nplt.suptitle('dsgdb9nsd_000001')\nplt.show()","c5262e7c":"dm = pd.read_csv('..\/input\/dipole_moments.csv')\ndm.head()","18704240":"mst = pd.read_csv('..\/input\/magnetic_shielding_tensors.csv')\nmst.head()","6e783bd5":"mul = pd.read_csv('..\/input\/mulliken_charges.csv')\nmul.head()","24c3b24b":"# Plot the distribution of mulliken_charges\nmul['mulliken_charge'].plot(kind='hist', figsize=(15, 5), bins=500, title='Distribution of Mulliken Charges')\nplt.show()","fb3769a8":"pote = pd.read_csv('..\/input\/potential_energy.csv')\npote.head()","464a68c7":"# Plot the distribution of potential_energy\npote['potential_energy'].plot(kind='hist',\n                              figsize=(15, 5),\n                              bins=500,\n                              title='Distribution of Potential Energy',\n                              color='b')\nplt.show()","deea7a68":"scc = pd.read_csv('..\/input\/scalar_coupling_contributions.csv')\nscc.head()","a710416e":"scc.groupby('type').count()['molecule_name'].sort_values().plot(kind='barh',\n                                                                color='grey',\n                                                               figsize=(15, 5),\n                                                               title='Count of Coupling Type in Train Set')\nplt.show()","2e0bfc75":"fig, ax = plt.subplots(2, 2, figsize=(20, 10))\nscc['fc'].plot(kind='hist', ax=ax.flat[0], bins=500, title='Fermi Contact contribution', color=color_pal[0])\nscc['sd'].plot(kind='hist', ax=ax.flat[1], bins=500, title='Spin-dipolar contribution', color=color_pal[1])\nscc['pso'].plot(kind='hist', ax=ax.flat[2], bins=500, title='Paramagnetic spin-orbit contribution', color=color_pal[2])\nscc['dso'].plot(kind='hist', ax=ax.flat[3], bins=500, title='Diamagnetic spin-orbit contribution', color=color_pal[3])\nplt.show()","a585390e":"scc = scc.merge(train_df)","a8025450":"# Downsample to speed up plot time.\nsns.pairplot(data=scc.sample(5000), hue='type', vars=['fc','sd','pso','dso','scalar_coupling_constant'])\nplt.show()","9f0397d9":"atom_count_dict = structures.groupby('molecule_name').count()['atom_index'].to_dict()","77710391":"train_df['atom_count'] = train_df['molecule_name'].map(atom_count_dict)\ntest_df['atom_count'] = test_df['molecule_name'].map(atom_count_dict)","2da74a95":"train_df.sample(600).plot(x='atom_count',\n                           y='scalar_coupling_constant',\n                           kind='scatter',\n                           color=color_pal[0],\n                           figsize=(20, 5),\n                           alpha=0.5)\nplt.show()","5c4554ab":"train_df.groupby('type')['scalar_coupling_constant'].mean().plot(kind='barh',\n                                                                 figsize=(15, 5),\n                                                                title='Average Scalar Coupling Constant by Type')\nplt.show()\ntype_mean_dict = train_df.groupby('type')['scalar_coupling_constant'].mean().to_dict()\ntest_df['scalar_coupling_constant'] = test_df['type'].map(type_mean_dict)\ntest_df[['id','scalar_coupling_constant']].to_csv('super_simple_submission.csv', index=False)","6b31d0fb":"def metric(df, preds):\n    df[\"prediction\"] = preds\n    maes = []\n    for t in df.type.unique():\n        y_true = df[df.type==t].scalar_coupling_constant.values\n        y_pred = df[df.type==t].prediction.values\n        mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n        maes.append(mae)\n    return np.mean(maes)","354e8d78":"def map_atom_info(df, atom_idx):\n    df = pd.merge(df, structures, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\ntrain_df = map_atom_info(train_df, 0)\ntrain_df = map_atom_info(train_df, 1)\n\ntest_df = map_atom_info(test_df, 0)\ntest_df = map_atom_info(test_df, 1)","96f6e3fd":"# https:\/\/www.kaggle.com\/seriousran\/just-speed-up-calculate-distance-from-benchmark\ntrain_p_0 = train_df[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = train_df[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = test_df[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = test_df[['x_1', 'y_1', 'z_1']].values\n\ntrain_df['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntest_df['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)","fc55d9c5":"# old, base from : @artgor's  kernel - https:\/\/www.kaggle.com\/artgor\/molecular-properties-eda-and-models\n# train_df['dist_to_type_mean'] = train_df['dist'] \/ train_df.groupby('type')['dist'].transform('mean')\n# test_df['dist_to_type_mean'] = test_df['dist'] \/ test_df.groupby('type')['dist'].transform('mean')\n\n### I made my own Versions: \n# train_df['dist_to_type_mean'] = train_df['dist'] \/ train_df.groupby('type')['dist'].transform('mean')\n# test_df['dist_to_type_mean'] = test_df['dist'] \/ test_df.groupby('type')['dist'].transform('mean')\n\n# train_df['dist_to_type_0_mean'] = train_df['dist'] \/ train_df.groupby('atom_0')['dist'].transform('mean')\n# test_df['dist_to_type_0_mean'] = test_df['dist'] \/ test_df.groupby('atom_0')['dist'].transform('mean')\n\n# train_df['dist_to_type_1_mean'] = train_df['dist'] \/ train_df.groupby('atom_1')['dist'].transform('mean')\n# test_df['dist_to_type_1_mean'] = test_df['dist'] \/ test_df.groupby('atom_1')['dist'].transform('mean')\n\n# train_df['molecule_type_dist_mean'] = train_df.groupby([ 'type'])['dist'].transform('mean')\n# test_df['molecule_type_dist_mean'] = test_df.groupby(['type'])['dist'].transform('mean')\n\n\ntrain_df['dist_to_type_0_mean'] = train_df['dist'] \/ train_df.groupby(['type','atom_0'])['dist'].transform('mean')\ntest_df['dist_to_type_0_mean'] = test_df['dist'] \/ test_df.groupby(['type','atom_0'])['dist'].transform('mean')\n\ntrain_df['dist_to_type_1_mean'] = train_df['dist'] \/ train_df.groupby(['type','atom_1'])['dist'].transform('mean')\ntest_df['dist_to_type_1_mean'] = test_df['dist'] \/ test_df.groupby(['type','atom_1'])['dist'].transform('mean')","15b4b643":"# make categorical variables\natom_map = {'H': 0,\n            'C': 1,\n            'N': 2}\ntrain_df['atom_0_cat'] = train_df['atom_0'].map(atom_map).astype('int')\ntrain_df['atom_1_cat'] = train_df['atom_1'].map(atom_map).astype('int')\ntest_df['atom_0_cat'] = test_df['atom_0'].map(atom_map).astype('int')\ntest_df['atom_1_cat'] = test_df['atom_1'].map(atom_map).astype('int')","1a4cf3f1":"# One Hot Encode the Type\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df['type'])], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df['type'])], axis=1)","7caaef30":"color_index = 0\naxes_index = 0\nfig, axes = plt.subplots(8, 1, figsize=(20, 20), sharex=True)\nfor mtype, d in train_df.groupby('type'):\n    d['dist'].plot(kind='hist',\n                  bins=1000,\n                  title='Distribution of Distance Feature for {}'.format(mtype),\n                  color=color_pal[color_index],\n                  ax=axes[axes_index])\n    if color_index == 6:\n        color_index = 0\n    else:\n        color_index += 1\n    axes_index += 1\nplt.show()","b35889dd":"train_df.shape","e6ed4744":"train_df.columns","1c149e3a":"train_df.head()","98e41295":"# Configurables\nFEATURES = ['atom_index_0', 'atom_index_1',\n            'atom_0_cat',\n            'x_0', 'y_0', 'z_0',\n            'atom_1_cat', \n            'x_1', 'y_1', 'z_1', 'dist', \n#             'dist_to_type_mean',\n            'atom_count',\n            '1JHC', '1JHN', '2JHC', '2JHH', '2JHN', '3JHC', '3JHH', '3JHN'\n            ,'dist_to_type_0_mean',\n       'dist_to_type_1_mean'\n           ]\n\n# # instead of whitelist, blacklist: # broken in feat importance part\n# DROP_FEATS = ['id', 'molecule_name', 'type','scalar_coupling_constant', 'atom_0','atom_1',]\n\nTARGET = 'scalar_coupling_constant'\nCAT_FEATS = ['atom_0','atom_1']\n## ORIG: \n# N_ESTIMATORS = 2000\n# VERBOSE = 500\n# EARLY_STOPPING_ROUNDS = 200\n# RANDOM_STATE = 529\n\n# faster:\nN_ESTIMATORS = 100\nVERBOSE = 50\nEARLY_STOPPING_ROUNDS = 5\nRANDOM_STATE = 529\n\n# ## whitelist feats: \nX = train_df[FEATURES]\nX_test = test_df[FEATURES]\n\n# ## exclude cols:\n# X = train_df.drop(DROP_FEATS,axis=1)\n# X_test = test_df.drop(DROP_FEATS,axis=1)\n\ny = train_df[TARGET]","a3febaa9":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\n\nlgb_params = {\n#     'num_leaves': 128, #  orig\n    'num_leaves': 64,\n#               'min_child_samples': 64, # orig\n              'min_child_samples': 32,\n              'objective': 'regression',\n#               'max_depth': 6, # ORIG\n                            'max_depth': 5,\n#               'learning_rate': 0.1, # orig\n              'learning_rate': 0.2,\n              \"boosting_type\": \"gbdt\",\n              \"subsample_freq\": 1,\n              \"subsample\": 0.9,\n              \"bagging_seed\": 11,\n              \"metric\": 'mae',\n              \"verbosity\": -1,\n              'reg_alpha': 0.1,\n              'reg_lambda': 0.4,\n              'colsample_bytree': 0.9\n         }\n\nRUN_LGB = True","255b2cda":"if RUN_LGB:\n    n_fold = 3\n    folds = KFold(n_splits=n_fold, shuffle=True, random_state=RANDOM_STATE)\n\n    # Setup arrays for storing results\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n\n    # Train the model\n    for fold_n, (train_idx, valid_idx) in enumerate(folds.split(X)):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n        model = lgb.LGBMRegressor(**lgb_params, n_estimators = N_ESTIMATORS, n_jobs = -1)\n        model.fit(X_train, y_train,\n                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                  eval_metric='mae',\n                  verbose=VERBOSE,\n                  early_stopping_rounds=EARLY_STOPPING_ROUNDS)\n\n        y_pred_valid = model.predict(X_valid)\n        y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n\n        # feature importance\n        fold_importance = pd.DataFrame()\n        fold_importance[\"feature\"] = FEATURES\n        fold_importance[\"importance\"] = model.feature_importances_\n        fold_importance[\"fold\"] = fold_n + 1\n        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n        prediction \/= folds.n_splits\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n        print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n        oof[valid_idx] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n        prediction += y_pred","76ca4736":"feature_importance.head()","ad4bd771":"if RUN_LGB:\n    # Save Prediction and name appropriately\n    submission_csv_name = 'submission_lgb_{}folds_{}CV.csv'.format(n_fold, np.mean(scores))\n    oof_csv_name = 'oof_lgb_{}folds_{}CV.csv'.format(n_fold, np.mean(scores))\n    fi_csv_name = 'fi_lgb_{}folds_{}CV.csv'.format(n_fold, np.mean(scores))\n\n    print('Saving LGB Submission as:')\n    print(submission_csv_name)\n    ss = pd.read_csv('..\/input\/sample_submission.csv')\n    ss['scalar_coupling_constant'] = prediction\n    ss.to_csv(submission_csv_name, index=False)\n    ss.head()\n    \n    # OOF\n    oof_df = train_df[['id','molecule_name','scalar_coupling_constant']].copy()\n    oof_df['oof_pred'] = oof\n    oof_df.to_csv(oof_csv_name, index=False)\n    \n    # Feature Importance\n    feature_importance.to_csv(fi_csv_name, index=False)","94f3c3cd":"feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False)[:40].plot(kind=\"bar\")","7558fc14":"if RUN_LGB:\n    # Plot feature importance as done in https:\/\/www.kaggle.com\/artgor\/artgor-utils\n    feature_importance[\"importance\"] \/= folds.n_splits\n    cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False)[:50].index\n\n    best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n    plt.figure(figsize=(15, 20));\n    ax = sns.barplot(x=\"importance\",\n                y=\"feature\",\n                hue='fold',\n                data=best_features.sort_values(by=\"importance\", ascending=False));\n    plt.title('LGB Features (avg over folds)');","69bdb6a9":"# from catboost import Pool, cv\n\n# RUN_CATBOOST_CV = False\n\n# if RUN_CATBOOST_CV:\n#     labels = train_df['scalar_coupling_constant'].values\n#     cat_features = ['type','atom_count','atom_0','atom_1']\n#     cv_data = train_df[['type','atom_count','atom_0','atom_1',\n#                         'x_0','y_0','z_0','x_1','y_1','z_1','dist']]\n#     cv_dataset = Pool(data=cv_data,\n#                       label=labels,\n#                       cat_features=cat_features)\n\n# ##     ITERATIONS = 100000 # ORIG\n#     ITERATIONS = 1234\n#     params = {\"iterations\": ITERATIONS,\n#               \"learning_rate\" : 0.02,\n#               \"depth\": 7,\n#               \"loss_function\": \"MAE\",\n#               \"verbose\": False,\n#               \"task_type\" : \"GPU\"}\n\n#     scores = cv(cv_dataset,\n#                 params,\n#                 fold_count=5, \n#                 plot=\"True\")\n    \n#     scores['iterations'] = scores['iterations'].astype('int')\n#     scores.set_index('iterations')[['test-MAE-mean','train-MAE-mean']].plot(figsize=(15, 5), title='CV (MAE) Score by iteration (5 Folds)')","2721318d":"# from catboost import CatBoostRegressor, Pool\n\n# # ITERATIONS = 200000 # Default\n# ITERATIONS = 1234 # faster\n\n# FEATURES = [#'atom_index_0',\n#             'atom_index_1',\n#             'atom_0',\n#             'x_0', 'y_0', 'z_0',\n#             'atom_1', \n#             'x_1', 'y_1', 'z_1',\n#             'dist', 'dist_to_type_mean',\n#             'atom_count',\n#             'type']\n# TARGET = 'scalar_coupling_constant'\n# CAT_FEATS = ['atom_0','atom_1','type']\n\n# train_dataset = Pool(data=train_df[FEATURES],\n#                   label=train_df['scalar_coupling_constant'].values,\n#                   cat_features=CAT_FEATS)\n\n# cb_model = CatBoostRegressor(iterations=ITERATIONS,\n#                              learning_rate=0.2,\n#                              depth=7,\n#                              eval_metric='MAE',\n#                              random_seed = 529,\n#                              task_type=\"GPU\")\n\n# # Fit the model\n# cb_model.fit(train_dataset, verbose=500)\n\n# # Predict\n# test_data = test_df[FEATURES]\n\n# test_dataset = Pool(data=test_data,\n#                     cat_features=CAT_FEATS)\n\n# ss = pd.read_csv('..\/input\/sample_submission.csv')\n# ss['scalar_coupling_constant'] = cb_model.predict(test_dataset)\n# ss.to_csv('basline_catboost_submission.csv', index=False)","810ab2c0":"## Target vs. Atom Count","8757285d":"# Additional Data\n*NOTE: additional data is provided for the molecules in Train only!*\n\n* If we want to use it for our model, we'll need to extract it externally and\/or predict it!\n\n* Good 101 on the pre-provided values: https:\/\/www.kaggle.com\/tarunpaparaju\/champs-competition-chemistry-background-and-eda\n\n","f78722e0":"When we look at the target `scalar_coupling_constant` in relation to the `atom_count` - there visually appears to be a relationship. We notice the gap in coupling constant values, between ~25 and ~75. It is rare to see a value within this range. Could this be a good case for a classification problem between the two clusters?","a7a43ea9":"## Save LGB Results, OOF, and Feature Importance\nIt's always a good idea to save your OOF, predictions and feature importances. You never know when they will come in handy in the future.\n\nWe'll save the Number of folds and CV score in the filename.","ff9c392d":"The target distribution is pretty interesting! Spikes near zero, and -20. There is also a good bit around 80. We will return to these files later.","d5854aa6":"# Catboost\n* Similar to LGBM and XGBoost. ","16ad1b76":"## Look at the xyz file for this example\n- The first number `5` is the number of atoms in this molecule.\n- A blank line....\n- Each following line is the element and their cartesian coordinates. So in this examples we have one Carbon atom and four Hydrogen atoms!","9dcc5479":"## train.csv and test.csv\nThe training set, where the first column `molecule_name` is the name of the molecule where the coupling constant originates (the corresponding XYZ file is located at .\/structures\/.xyz), the second `atom_index_0` and third column `atom_index_1` is the atom indices of the atom-pair creating the coupling and the fourth column `scalar_coupling_constant` is the scalar coupling constant that we want to be able to predict.\n\n#### Important: \n* Since this is a DEMO we will only read in a sample of the train data\n    * for realistic purposes, better would be to read in all the data, then","78ed3d35":"## dipole_moments.csv\n- contains the molecular electric dipole moments. These are three dimensional vectors that indicate the charge distribution in the molecule. The first column (molecule_name) are the names of the molecule, the second to fourth column are the X, Y and Z components respectively of the dipole moment.","dec83371":"# Evaluation Metric\n\nSubmissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type.\n\n![Eval Metric](https:\/\/i.imgur.com\/AK6z3Dn.png)\n\nWhere:\n\n- `T` is the number of scalar coupling types\n- `nt` is the number of observations of type t\n- `yi` is the actual scalar coupling constant for the observation\n- `yi^` is the predicted scalar coupling constant for the observation\n\nFor this metric, the MAE for any group has a floor of 1e-9, so that the minimum (best) possible score for perfect predictions is approximately -20.7232.","959014d1":"## potential_energy.csv\n- contains the potential energy of the molecules. The first column (molecule_name) contains the name of the molecule, the second column (potential_energy) contains the potential energy of the molecule.","facd7ef3":"# Baseline Models\n- using **atom_count** and **type** as categorical features\n- Run this in a notebook to see the interactive plot of training and test error metrics.","b1f4916e":"# LightGBM - CV","90dd093f":"# Predicting Molecular Properties\nCan you measure the magnetic interactions between a pair of atoms?\n\nIn this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the `scalar_coupling_constant`).\n https:\/\/www.kaggle.com\/c\/champs-scalar-coupling\/overview\/description. \n \n## General information\n* based on : https:\/\/www.kaggle.com\/robikscube\/exploring-molecular-properties-data\n* Forked\/based on: https:\/\/www.kaggle.com\/artgor\/molecular-properties-eda-and-models\n* https:\/\/www.kaggle.com\/robikscube\/exploring-molecular-properties-data\n* https:\/\/www.kaggle.com\/tarunpaparaju\/champs-competition-chemistry-background-and-eda\n\n* Feature engineering brute force: https:\/\/www.kaggle.com\/artgor\/brute-force-feature-engineering\n* https:\/\/www.kaggle.com\/adrianoavelar\/bond-calculation-lb-0-82  , Chemical Bond Calculation \n\n* https:\/\/www.kaggle.com\/buchan\/a-neural-network-approach  (Includes covalent bond calc - https:\/\/www.kaggle.com\/adrianoavelar\/bond-calculation-lb-0-82 - Important feature!)\n* https:\/\/www.kaggle.com\/borisdee\/predicting-mulliken-charges-with-acsf-descriptors  - Using external libraries + calc the features given for the train\n\n","151cbe8f":"## Structures.csv\nThis file contains the same information as the individual xyz structure files, but in a single file.\n\nThis csv is a lot more useable than the `xyz` files (for most usages, but not all!)","63c08f17":"Evaluation metric is important to understand as it determines how your model will be scored. Ideally we will set the loss function of our machine learning algorithm to use this metric so we can minimize the specific type of error.\n\nCheck out this kernel by `@abhishek` with code for the evaluation metric: https:\/\/www.kaggle.com\/abhishek\/competition-metric\n","f2fef00e":"## Relationship between Target and Features\n** Keep in mind these features are provided for the training data ONLY**","f4a4fa9f":"## structures.zip annd structures csv files.\nfolder containing molecular structure (xyz) files, where: \n- the first line is the number of atoms in the molecule,\n- followed by a blank line\n- and then a line for every atom, where the first column contains the atomic element (H for hydrogen, C for carbon etc.) and the remaining columns contain the X, Y and Z cartesian coordinates (a standard format for chemists and molecular visualization programs)\n\n\n...lets have a look at the first example from the training set!","46d8f9e8":"We can see each observation provides the:\n- molecule_name\n- atom_index_0 - index of the first atom pair\n- atom_index_1 - index of the second atom pair\n- scalar_coupling_constant (target)","b0229cb5":"## magnetic_shielding_tensors.csv\n- contains the magnetic shielding tensors for all atoms in the molecules. The first column (molecule_name) contains the molecule name, the second column (atom_index) contains the index of the atom in the molecule, the third to eleventh columns contain the XX, YX, ZX, XY, YY, ZY, XZ, YZ and ZZ elements of the tensor\/matrix respectively.","a5e12a42":"# Super Simple Baseline Model [1.239 Public LB]\nThe second simplest thing we can do as a model is predict that the target is the **average** value that we observe for that **type** in the training set!","0107eda3":"### Get list of molecule types\n* We'll see the target distribution is very different for the different types: We may want to make a model for each type seperately! \n* We also have relatively few types, so this is easily tractionable","09c133e0":"## scalar_coupling_contributions.csv\n- The scalar coupling constants in train.csv (or corresponding files) are a sum of four terms. scalar_coupling_contributions.csv contain all these terms.\n    - The first column (molecule_name) are the **name of the molecule**,\n    - the second **(atom_index_0)** and\n    - third column **(atom_index_1)** are the atom indices of the atom-pair,\n    - the fourth column indicates the **type of coupling**,\n    - the fifth column (fc) is the **Fermi Contact contribution**,\n    - the sixth column (sd) is the **Spin-dipolar contribution**,\n    - the seventh column (pso) is the **Paramagnetic spin-orbit contribution** and\n    - the eighth column (dso) is the **Diamagnetic spin-orbit contribution**.","7e2cf2f1":"## mulliken_charges.csv\n- contains the mulliken charges for all atoms in the molecules. The first column (molecule_name) contains the name of the molecule, the second column (atom_index) contains the index of the atom in the molecule, the third column (mulliken_charge) contains the mulliken charge of the atom.","c2587166":"# Distance Feature Creation\nThis feature was found from `@inversion` 's kernel here: https:\/\/www.kaggle.com\/inversion\/atomic-distance-benchmark\/output\nThe code was then made faster by `@seriousran` here: https:\/\/www.kaggle.com\/seriousran\/just-speed-up-calculate-distance-from-benchmark","a589d00b":"# The Data\n\nIn this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.\n\nFor this competition, you will not be predicting all the atom pairs in each molecule rather, you will only need to predict the pairs that are explicitly listed in the train and test files. For example, some molecules contain Fluorine (F), but you will not be predicting the scalar coupling constant for any pair that includes F.\n\nThe training and test splits are by molecule, so that no molecule in the training data is found in the test data.","60cf1775":"These plots are beautiful. It's a shame we don't have this data for the test set.","16058ff7":"The training set is larger than the test set.\n"}}