{"cell_type":{"b25e1efe":"code","6d6a85a1":"code","c59cf495":"code","0879aafd":"code","0a3a4bde":"code","d6950d28":"code","c722d8e5":"code","19f0f88d":"code","0769dd47":"code","98462c82":"code","b2dcaf4b":"code","3fcd8fdc":"code","4a99e750":"code","4ffd3036":"code","92fb7978":"code","64a03197":"code","f1bf2104":"code","13fbf464":"code","545242e2":"code","2bee788f":"code","8c2f83a2":"code","b9a6cb2a":"code","331fbc9f":"code","7b621f3f":"code","7736184d":"code","4fceb50d":"code","4e0077db":"code","41456c0d":"code","ee291dd7":"code","594c7492":"code","9bf66a57":"code","ed4d8a56":"code","c14cad9d":"code","060be128":"code","84e6497a":"code","8c3d8d3e":"code","b1aca4f1":"code","958726b3":"code","02d00c1c":"code","2e979a2f":"code","e334b972":"code","2ab50bf3":"code","559d7669":"code","76ff39b3":"code","0ce3a448":"code","8bc37281":"code","d62a3c83":"code","5de6a47b":"code","4e78f732":"code","b7b1c5ea":"code","948be0b3":"code","468e0965":"code","4d21cb9f":"code","2f8a1824":"code","f891188d":"code","03119922":"code","3743351b":"code","5ccc369b":"code","f7107969":"code","a72bab1b":"code","326d5fd8":"code","2399f027":"code","99148038":"code","351bf076":"code","a3b80fe6":"code","67a3d2ee":"code","cca91031":"code","b78f0cae":"code","7b70f7dd":"code","8f8e3036":"markdown","25541c06":"markdown","7afdaff0":"markdown","58c92cd0":"markdown","0c701399":"markdown","b3665a07":"markdown","47126e79":"markdown","fea9ee8e":"markdown","fdb6f1cf":"markdown","15f45f66":"markdown","d0ebb0cc":"markdown","bd5b76ba":"markdown","c1ca1bb1":"markdown","9680c47a":"markdown","dc2662b3":"markdown","d4ea9203":"markdown","025589e1":"markdown","9c5b8fc7":"markdown","234be712":"markdown","b6ac6f4a":"markdown","0fc80361":"markdown","c9ededd4":"markdown","835d5fbb":"markdown","bb779066":"markdown","70ff5b0a":"markdown","d2eddddd":"markdown","ccd55f6a":"markdown","b5e26ac3":"markdown","765de29c":"markdown","48142064":"markdown","295816e1":"markdown","d68ebc55":"markdown","6f59167d":"markdown","cfc13edf":"markdown","f312e11d":"markdown","57874d53":"markdown","81534888":"markdown","42af8486":"markdown","5cc28b81":"markdown","171abd85":"markdown","bcc51f25":"markdown","4eec0188":"markdown"},"source":{"b25e1efe":"# This Python 3 environment comes with many helpful analytics libraries installed on Kaggle\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\nimport os\nimport re\nimport math\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\nimport random       \nimport plotly.express as px\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers      # mitigate overfitting \nfrom kaggle_datasets  import KaggleDatasets    # import kaggle data files\n# Stop training when a monitored metric has stopped improving\nfrom tensorflow.keras.callbacks import EarlyStopping   \nprint(\"Tensorflow version \" + tf.__version__)  # verify tensorflow versionis 2.x","6d6a85a1":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. \n    # On Kaggle this is always the case.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","c59cf495":"# you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\nprint(GCS_DS_PATH)","0879aafd":"# Input data files are available in the read-only \"kaggle\/input\/\" directory\n#   image files are in TFRecords format, each of which contains a sequeence\n#   of records and can only be read sequentially.\n\nTFRec_selected = '512x512'\nfor dirpath, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if TFRec_selected in dirpath: # \n            print(os.path.join(dirpath, filename))","0a3a4bde":"IMAGE_SIZE = [512, 512] \n\nGCS_PATH = GCS_DS_PATH + '\/tfrecords-jpeg-512x512'\nAUTO = tf.data.experimental.AUTOTUNE \n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/val\/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test\/*.tfrec') \n\nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']  \n\n# 100 - 103\n\nprint (CLASSES)","d6950d28":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),    # shape [] means single element\n        # class is missing, to be predicted flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of (image, idnum) pairs\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    \n    # automatically interleaves reads from multiple file\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) \n    \n    # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.with_options(ignore_order) \n    \n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    dataset = dataset.map(read_labeled_tfrecord if labeled \n                          else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    return dataset\n","c722d8e5":"GCS_DS_PATH_EXT = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')\n\n# External data\nGCS_PATH_SELECT_EXT = {\n    192: '\/tfrecords-jpeg-192x192',\n    224: '\/tfrecords-jpeg-224x224',\n    331: '\/tfrecords-jpeg-331x331',\n    512: '\/tfrecords-jpeg-512x512'\n}\nGCS_PATH_EXT = GCS_PATH_SELECT_EXT[IMAGE_SIZE[0]]\n\nIMAGENET_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '\/imagenet' + GCS_PATH_EXT + '\/*.tfrec')\nINATURELIST_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '\/inaturalist' + GCS_PATH_EXT + '\/*.tfrec')\nOPENIMAGE_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '\/openimage' + GCS_PATH_EXT + '\/*.tfrec')\nOXFORD_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '\/oxford_102' + GCS_PATH_EXT + '\/*.tfrec')\nTENSORFLOW_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '\/tf_flowers' + GCS_PATH_EXT + '\/*.tfrec')\n\nADDITIONAL_TRAINING_FILENAMES = IMAGENET_FILES + INATURELIST_FILES + OPENIMAGE_FILES + OXFORD_FILES + TENSORFLOW_FILES  \n\nTRAINING_FILENAMES = TRAINING_FILENAMES + ADDITIONAL_TRAINING_FILENAMES","19f0f88d":"# parameters set for tfrecords-jpeg-512x512 TFRecord files\nIMAGE_SIZE        = [512, 512] \nHEIGHT            = IMAGE_SIZE[0]\nWIDTH             = IMAGE_SIZE[1]\nEPOCHS            = 20\nBATCH_SIZE        = 16 * strategy.num_replicas_in_sync\nNUM_TRAIN_IMAGES  = 12753\nNUM_VAL_IMAGES    = 3712\nNUM_TEST_IMAGES   = 7382\nSTEPS_PER_EPOCH   = NUM_TRAIN_IMAGES \/\/ BATCH_SIZE\nAUTO              = tf.data.experimental.AUTOTUNE\nTRAIN_FILENAMES   = tf.io.gfile.glob(GCS_DS_PATH + '\/tfrecords-jpeg-512x512\/train\/*.tfrec') \nVAL_FILENAMES     = tf.io.gfile.glob(GCS_DS_PATH + '\/tfrecords-jpeg-512x512\/val\/*.tfrec') \nTEST_FILENAMES    = tf.io.gfile.glob(GCS_DS_PATH + '\/tfrecords-jpeg-512x512\/test\/*.tfrec')","0769dd47":"SEED = 2020\n\ndef random_blockout(img, sl=0.1, sh=0.2, rl=0.4):\n    p=random.random()\n    if p>=0.25:\n        w, h, c = IMAGE_SIZE[0], IMAGE_SIZE[1], 3\n        origin_area = tf.cast(h*w, tf.float32)\n\n        e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n        e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh \/ rl)), tf.int32)\n\n        e_height_h = tf.minimum(e_size_h, h)\n        e_width_h = tf.minimum(e_size_h, w)\n\n        erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n        erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n\n        erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n        erase_area = tf.cast(erase_area, tf.uint8)\n\n        pad_h = h - erase_height\n        pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n        pad_bottom = pad_h - pad_top\n\n        pad_w = w - erase_width\n        pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n        pad_right = pad_w - pad_left\n\n        erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n        erase_mask = tf.squeeze(erase_mask, axis=0)\n        erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n\n        return tf.cast(erased_img, img.dtype)\n    else:\n        return tf.cast(img, img.dtype)\n\n    \ndef data_augment_v2(image, label):\n    # Thanks to the dataset.prefetch(AUTO) statement in the next function (below), this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n    \n    flag = random.randint(1,3)\n    coef_1 = random.randint(70, 90) * 0.01\n    coef_2 = random.randint(70, 90) * 0.01\n    \n    if flag == 1:\n        image = tf.image.random_flip_left_right(image, seed=SEED)\n    elif flag == 2:\n        image = tf.image.random_flip_up_down(image, seed=SEED)\n    else:\n        image = tf.image.random_crop(image, [int(IMAGE_SIZE[0]*coef_1), int(IMAGE_SIZE[0]*coef_2), 3],seed=SEED)\n        \n    image = random_blockout(image)\n    \n    return image, label ","98462c82":"import tensorflow_addons as tfa\n\n# Randomly make some changes to the images and return the new images and labels\ndef data_augment_v3(image, label):\n        \n    # Set seed for data augmentation\n    seed = 100\n    \n    # Randomly resize and then crop images\n    image = tf.image.resize(image, [720, 720])\n    image = tf.image.random_crop(image, [512, 512, 3], seed = seed)\n\n    # Randomly reset brightness of images\n    image = tf.image.random_brightness(image, 0.6, seed = seed)\n    \n    # Randomly reset saturation of images\n    image = tf.image.random_saturation(image, 3, 5, seed = seed)\n        \n    # Randomly reset contrast of images\n    image = tf.image.random_contrast(image, 0.3, 0.5, seed = seed)\n\n    # Randomly reset hue of images, but this will make the colors really weird, which we think will not happen\n    # in common photography\n    # image = tf.image.random_hue(image, 0.5, seed = seed)\n    \n    # Blur images\n    image = tfa.image.mean_filter2d(image, filter_shape = 10)\n    \n    # Randomly flip images\n    image = tf.image.random_flip_left_right(image, seed = seed)\n    image = tf.image.random_flip_up_down(image, seed = seed)\n    \n    # Fail to rotate and transform images due to some bug in TensorFlow\n    # angle = random.randint(0, 180)\n    # image = tfa.image.rotate(image, tf.constant(np.pi * angle \/ 180))\n    # image = tfa.image.transform(image, [1.0, 1.0, -250, 0.0, 1.0, 0.0, 0.0, 0.0])\n    \n    return image, label\n","b2dcaf4b":"# image augmentation                                  \ndef data_augment(image, label):\n    # Pad the image with a black, 3-pixel border\n    # image = tf.image.resize_with_crop_or_pad(image, HEIGHT + 6, WIDTH + 6)\n    # Randomly crop to original size from the padded image\n    # image = tf.image.random_crop(image, size=[*IMAGE_SIZE,3])\n    image = tf.image.random_flip_left_right(image)\n    #image = tf.image.random_contrast(image, 0.8, 1.2)\n    #image = tf.image.random_brightness(image, 0.1) \n    #image = tf.image.random_saturation(image, 0.7, 1.3)\n    return image, label \n\n# get training datatset with augmentation option\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","3fcd8fdc":"def get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec\n    # files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nprint('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","4a99e750":"strategy.num_replicas_in_sync","4ffd3036":"16 * strategy.num_replicas_in_sync","92fb7978":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\nds_train = get_training_dataset()\nds_valid = get_validation_dataset()\nds_test = get_validation_dataset()\n\n\nprint(\"Training:\" , ds_train)\nprint(\"Validation:\" , ds_valid)\nprint(\"Testing: \", ds_test)","64a03197":"np.set_printoptions(threshold=15, linewidth=80)\n\n\nprint(\"Training DataStream Shape: \")\nfor image, label in ds_train.take(5):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Training datasteam label examples: \", label.numpy())\n    ","f1bf2104":"print(\"Test DataStream Shape: \")\nfor image, idnum in ds_test.take(5):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test datasteam IDs examples: \", idnum.numpy().astype('U'))","13fbf464":"def batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n    # If no labels, only image IDs, return None for labels (this is the case for test data)\n    return numpy_images, numpy_labels\n    ","545242e2":"def title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], \n                                'OK' if correct else 'NO', \n                                u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct","2bee788f":"def display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize\/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize\/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)","8c2f83a2":"def display_batch_of_images(databatch, predictions=None, display_mismatches_only=False):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        if display_mismatches_only:\n            if predictions[i] != label:\n                subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n        else:        \n            subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()","b9a6cb2a":"def display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n","331fbc9f":"def display_training_curves_v2(training, validation, learning_rate_list, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title, color='b')\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.', 'learning rate'])        \n    \n    ax2 = ax.twinx() #The Axes. twinx() function in axes module of matplotlib library is used to create a twin Axes sharing the xaxis. \n    ax2.plot(learning_rate_list, 'g-')\n    ax2.set_ylabel('learning rate', color='g')","7b621f3f":"# get original training_dataset without augmentation\nori_train_set = get_training_dataset()\n\nori_image_batch = (next(iter(ori_train_set.unbatch().batch(16)))) # get a batch for \nimages, _ = batch_to_numpy_images_and_labels(ori_image_batch)\n\n# function to show image with random data augmentation\ndef show_aug(image):\n    plt.figure(figsize=(12,2))\n    plt.subplot(1,6,1)\n    plt.imshow(image)\n    plt.title('no augmentation')\n    plt.axis('off')\n    plt.subplot(1,6,3)\n    plt.imshow(tf.image.random_flip_left_right(image))       # augmented with random flip\n    plt.title('rdm flip L\/R')\n    plt.axis('off')    \n    plt.subplot(1,6,4)\n    plt.imshow(tf.image.random_contrast(image, 0.90, 0.99))  # augmented with contrast\n    plt.title('rdm contrast')\n    plt.axis('off')\n    plt.subplot(1,6,5)\n    plt.imshow(tf.image.random_brightness(image, 0.1))       # augmented with brightness\n    plt.title('rdm brightness')\n    plt.axis('off')\n    plt.subplot(1,6,6)\n    plt.imshow(tf.image.random_saturation(image, 0.8, 0.9))  # augmented with saturation\n    plt.title('rdm saturation')\n    plt.axis('off')\n    plt.subplot(1,6,2)\n    image = data_augment(image, None)\n    plt.imshow(image[0])  # any random combinations of the above augmenations, if any\n    plt.title('rdm aug combo')\n    plt.axis('off')    \n    plt.show()\n\n# show images\nprint('Training Dataset')\nprint('Sample Images: Original versus w\/ Random Augmentation')\nfor im in images:\n    show_aug(im)","7736184d":"ds_iter = iter(ds_train.unbatch().batch(50))\n\n#Use the Python next function to pop out the next batch in the stream and display it with the helper function.\none_batch = next(ds_iter)\ndisplay_batch_of_images(one_batch)\n","4fceb50d":"ds_iter = iter(ds_train.unbatch().batch(50))\n\n#Use the Python next function to pop out the next batch in the stream and display it with the helper function.\none_batch = next(ds_iter)\ndisplay_batch_of_images(one_batch)","4e0077db":"row = 4\ncol = 4\nBatch_elements = get_training_dataset().unbatch()\nsingle_element = tf.data.Dataset.from_tensors(next(iter(Batch_elements)))\n# Map the images to the data augmentation function for image processing\naugmented_element = single_element.repeat().map(data_augment).batch(row * col)\n\nfor (img, label) in augmented_element:\n    plt.figure(figsize = (15, int(15 * row \/ col)))\n    for j in range(row * col):\n        plt.subplot(row, col, j + 1)\n        plt.axis('off')\n        plt.imshow(img[j, ])\n    plt.show()\n    break","41456c0d":"# Map the images to the data augmentation function for image processing\naugmented_element = single_element.repeat().map(data_augment_v2).batch(row * col)\n\nfor (img, label) in augmented_element:\n    plt.figure(figsize = (15, int(15 * row \/ col)))\n    for j in range(row * col):\n        plt.subplot(row, col, j + 1)\n        #plt.axis('off')\n        plt.imshow(img[j, ])\n    plt.show()\n    break","ee291dd7":"augmented_element = single_element.repeat().map(data_augment_v3).batch(row * col)\n\nfor (img, label) in augmented_element:\n    plt.figure(figsize = (15, int(15 * row \/ col)))\n    for j in range(row * col):\n        plt.subplot(row, col, j + 1)\n        plt.axis('off')\n        plt.imshow(img[j, ])\n    plt.show()\n    break","594c7492":"# Check the image size(dimensions) before training the data\n[*IMAGE_SIZE, 3]","9bf66a57":"\",\".join(tf.keras.applications.__dir__())","ed4d8a56":"checkpoint_filepath = \"Petals_to_the_Metal-70K_images-trainable_True-MobileNetV2.h5\" #\"Petals_to_the_Metal-70K_images-trainable_True-DenseNet201.h5\"\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True\n)\n\n# This callback will stop the training when there is no improvement in the validation loss for three consecutive epochs. \nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)","c14cad9d":"NotFoundError = \"\"\"\nclass LRTensorBoard(TensorBoard):\n    def __init__(self, log_dir, **kwargs):  # add other arguments to __init__ if you need\n        super().__init__(log_dir=log_dir, **kwargs)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs.update({'lr': K.eval(self.model.optimizer.lr)})\n        super().on_epoch_end(epoch, logs)\n\nlr_tracking = LRTensorBoard(log_dir=\".\/lr_tracking\")\n\"\"\"","060be128":"class LearningRateTracking(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        keys = list(logs.keys())\n        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n        \n        #logs = logs or {}\n        #logs.update({'lr': K.eval(self.model.optimizer.lr)}) #optimizer._decayed_lr('float32').numpy()\n        #return \n\n#lr_tracking = LearningRateTracking()\n\n# For reading about EfficientNetB7 visit https:\/\/keras.io\/api\/applications\/efficientnet\/#efficientnetb7-function\nuse_efficientnet = False #tuning9\nif use_efficientnet:\n    !pip install -q efficientnet\n    from efficientnet.tfkeras import EfficientNetB7  ","84e6497a":"weight_per_class = True\n\nif weight_per_class:\n    from collections import Counter\n    import gc #Garbage Collector https:\/\/docs.python.org\/3\/library\/gc.html\n\n    gc.enable() #Enable automatic garbage collection.\n\n    def get_training_dataset_raw():\n        dataset = load_dataset(TRAINING_FILENAMES, labeled = True, ordered = False)\n        return dataset\n\n    raw_training_dataset = get_training_dataset_raw()\n\n    label_counter = Counter()\n    for images, labels in raw_training_dataset:\n        label_counter.update([labels.numpy()])\n\n    del raw_training_dataset    \n\n    TARGET_NUM_PER_CLASS = 122 #?\n\n    def get_weight_for_class(class_id):\n        counting = label_counter[class_id]\n        weight = TARGET_NUM_PER_CLASS \/ counting\n        return weight\n\n    weight_per_class = {class_id: get_weight_for_class(class_id) for class_id in range(104)}\n    \nif weight_per_class:\n    data = pd.DataFrame.from_dict(weight_per_class, orient='index', columns=['class_weight'])\n    plt.figure(figsize=(30, 9))\n\n    #barplot color based on value\n    bplot = sns.barplot(x=data.index, y='class_weight', data=data, palette= cm.Blues(data['class_weight']*0.15));\n    for p in bplot.patches:\n        bplot.annotate(format(p.get_height(), '.1f'), \n                       (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                       ha = 'center', va = 'center', \n                       xytext = (0, 9), \n                       textcoords = 'offset points')\n    plt.xlabel(\"Class\", size=14)\n    plt.ylabel(\"Class weight (inverse of %)\", size=14)","8c3d8d3e":"\"\"\"Model_type = \"Model1\"\nEPOCHS = 15\n\n#DenseNet201\ndef get_model1():\n    with strategy.scope():\n        dn201 = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        dn201.trainable = True # Full Training\n\n        model1 = tf.keras.Sequential([\n            dn201,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n        ])\n\n    model1.compile(\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n    return model1\n\n#Efficient Net B7\ndef get_model2():\n    with strategy.scope():\n        enb7 = efn.EfficientNetB7(weights='noisy-student', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        enb7.trainable = True # Full Training\n\n        model2 = tf.keras.Sequential([\n            enb7,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n        ])\n\n    model2.compile(\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n        )\n    return model2\n    \"\"\"","b1aca4f1":"ensemble_learning_models = False","958726b3":"'''\nAlternatively, data augmentation may be done by creating image preprocessing layers\n   and make them part of the model, as show below:  \n\ndata_augmentation = tf.keras.Sequential([\n   tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", seed = SEED),\n   tf.keras.layers.experimental.preprocessing.RandomRotation(0.2, seed = SEED)\n])\n'''\n\nif not ensemble_learning_models:\n    with strategy.scope():\n        \n        #pretrained_model = tf.keras.applications.VGG16\n        #pretrained_model = tf.keras.applications.DenseNet201\n        #pretrained_model = tf.keras.applications.InceptionResNetV2\n        #pretrained_model = tf.keras.applications.InceptionV3\n        #pretrained_model = tf.keras.applications.MobileNet\n        #pretrained_model = tf.keras.applications.MobileNetV2\n        #pretrained_model = tf.keras.applications.NASNetMobile\n        #pretrained_model = tf.keras.applications.ResNet50\n        #pretrained_model = tf.keras.applications.ResNet101V2\n        #pretrained_model = tf.keras.applications.VGG19\n        #pretrained_model = tf.keras.applications.Xception\n        #pretrained_model = tf.keras.applications.DenseNet201 \n        #pretrained_model = EfficientNetB7\n\n        pretrained_model = tf.keras.applications.MobileNetV2(\n            include_top=False ,\n            weights='imagenet', #tuning weights='noisy-student' instead of 'imagenet'\n                                #Self-training with Noisy Student improves ImageNet classification https:\/\/arxiv.org\/abs\/1911.04252) \n            #pooling='avg'\n            input_shape=[*IMAGE_SIZE, 3]\n        )\n\n        pretrained_model.trainable = True #tuning pretrained_model.trainable = True\n\n        model = tf.keras.Sequential([\n            pretrained_model, #Base pretrained on ImageNet to extract features from images\n\n            tf.keras.layers.GlobalAveragePooling2D(), ##Attach a new head to act as a classifier\n            #tf.keras.layers.Dropout(0.3), #tuning\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n        ])","02d00c1c":"  model.compile(\n        optimizer='nadam', #tuning2 optimizer='nadam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy'],\n    )\n    \nmodel.summary()","2e979a2f":"tf.keras.utils.plot_model(model, show_shapes=True)","e334b972":"if not ensemble_learning_models:\n    # Learning Rate Schedule for Fine Tuning #\n    def exponential_lr(epoch,\n                       start_lr = 0.00001, min_lr = 0.00001, max_lr = 0.00005 * strategy.num_replicas_in_sync, #tuning1\n                       rampup_epochs = 5, sustain_epochs = 0,\n                       exp_decay = 0.75): #tuning1\n\n        def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n            # linear increase from start to rampup_epochs\n            if epoch < rampup_epochs:\n                lr = ((max_lr - start_lr) \/\n                      rampup_epochs * epoch + start_lr)\n            # constant max_lr during sustain_epochs\n            elif epoch < rampup_epochs + sustain_epochs:\n                lr = max_lr\n            # exponential decay towards min_lr\n            else:\n                lr = ((max_lr - min_lr) *\n                      exp_decay**(epoch - rampup_epochs - sustain_epochs) +\n                      min_lr)\n            return lr\n        return lr(epoch,\n                  start_lr,\n                  min_lr,\n                  max_lr,\n                  rampup_epochs,\n                  sustain_epochs,\n                  exp_decay)\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(exponential_lr, verbose=True)\n\n    rng = [i for i in range(EPOCHS)]\n    y = [exponential_lr(x) for x in rng]\n    plt.plot(rng, y)\n    print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","2ab50bf3":"if not ensemble_learning_models:\n    history = model.fit(\n        ds_train,\n        validation_data=ds_valid,\n        epochs=EPOCHS,\n        steps_per_epoch=STEPS_PER_EPOCH,\n        callbacks=[lr_callback, checkpoint], # Model weights are saved at the end of every epoch, if it's the best seen so far\n        # https:\/\/www.tensorflow.org\/tutorials\/distribute\/multi_worker_with_keras\n        class_weight = weight_per_class #tuning11\n    )","559d7669":"if not ensemble_learning_models:\n    display_training_curves_v2( \n        history.history['loss'],\n        history.history['val_loss'],\n        history.history['lr'],\n        'loss',\n        211,\n    )\n\n    display_training_curves_v2(\n        history.history['sparse_categorical_accuracy'],\n        history.history['val_sparse_categorical_accuracy'],\n        history.history['lr'],\n        'accuracy',\n        212,\n    )","76ff39b3":"# Create plots of loss and accuracy on the training and validation sets.\n\nacc = history.history['sparse_categorical_accuracy']\nval_acc = history.history['val_sparse_categorical_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(1, len(history.history['loss'])+1)\n\nplt.figure(figsize=(14, 14))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.show()","0ce3a448":"zoom_after = 10\nif not ensemble_learning_models:\n    display_training_curves(\n        history.history['loss'][zoom_after:],\n        history.history['val_loss'][zoom_after:],\n        'loss',\n        211,\n    )\n\n    display_training_curves(\n        history.history['sparse_categorical_accuracy'][zoom_after:],\n        history.history['val_sparse_categorical_accuracy'][zoom_after:],\n        'accuracy',\n        212,\n    )","8bc37281":"checkpoint_filepath","d62a3c83":"if not ensemble_learning_models:\n    model.load_weights(checkpoint_filepath)","5de6a47b":"model.summary()","4e78f732":"#if ensemble_learning_models:\ntest_ds = get_test_dataset(ordered=True)\n        #best_alpha = 0.35\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobabilities = model.predict(test_images_ds)\npredictions = np.argmax(probabilities, axis=-1)\nprint(predictions)\n\n\nprint('Generating submission.csv file...')\n        # Get image ids from test set and convert to unicode\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n\n        # Write the submission file\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n        # Look at the first few predictions\n","b7b1c5ea":" !head submission.csv","948be0b3":"print(checkpoint_filepath)\ntflite_model_name = checkpoint_filepath.replace(\".h5\" , \".tflite\")\ntflite_model_name","468e0965":"def get_pretrained_model(model_name, image_dataset_weights, trainable=True):\n    pretrained_model= model_name(\n        include_top=False ,\n        weights=image_dataset_weights, #tuning10 weights='noisy-student' instead of 'imagenet'\n                                       #Self-training with Noisy Student improves ImageNet classification https:\/\/arxiv.org\/abs\/1911.04252) \n        input_shape=[*IMAGE_SIZE, 3]\n    )\n\n    pretrained_model.trainable = trainable #tuning8 pretrained_model.trainable = True\n    \n    model = tf.keras.Sequential([\n        pretrained_model, \n        tf.keras.layers.GlobalAveragePooling2D(), \n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])\n    \n    return model","4d21cb9f":"models = []\nhistories = []","2f8a1824":"!pip install -q efficientnet\n\nimport efficientnet.tfkeras as efn","f891188d":"# Need this line so Google will recite some incantations\n# for Turing to magically load the model onto the TPU\nwith strategy.scope():\n    enet = efn.EfficientNetB7(\n        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n        weights='imagenet',\n        include_top=False\n    )\n    \n    enet.trainable = True\n\n    model = tf.keras.Sequential([\n        enet,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])\n            \nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n    loss = 'sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy']\n)\n\nmodel.summary()\n\nmodels.append(model)","03119922":"history = model.fit(\n    get_training_dataset(), \n    steps_per_epoch=STEPS_PER_EPOCH,\n    epochs=EPOCHS,\n    callbacks=[lr_callback],\n    validation_data=None if ensemble_learning_models else get_validation_dataset()\n)\n\nhistories.append(history)","3743351b":"if not ensemble_learning_models:\n    display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n    display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)","5ccc369b":"from tensorflow.keras.applications import DenseNet201","f7107969":"with strategy.scope():\n    rnet = DenseNet201(\n        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n        weights='imagenet',\n        include_top=False\n    )\n    \n    rnet.trainable = True\n\n    model = tf.keras.Sequential([\n        rnet,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])\n        \nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n    loss = 'sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy']\n)\n\nmodel.summary()\n\nmodels.append(model)","a72bab1b":"history = model.fit(\n    get_training_dataset(), \n    steps_per_epoch=STEPS_PER_EPOCH,\n    epochs=EPOCHS, \n    callbacks=[lr_callback],\n    validation_data=None if ensemble_learning_models else get_validation_dataset()\n)\n\nhistories.append(history)","326d5fd8":"if not ensemble_learning_models:\n    display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n    display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)","2399f027":"if not ensemble_learning_models:\n    cmdataset = get_validation_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and labels, order matters.\n    images_ds = cmdataset.map(lambda image, label: image)\n    labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n    cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() # get everything as one batch\n    cm_probabilities = (models[0].predict(images_ds) + models[1].predict(images_ds)) \/ 2\n    cm_predictions = np.argmax(cm_probabilities, axis=-1)\n    print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n    print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)","99148038":"from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay","351bf076":"if not ensemble_learning_models:\n    cmdataset = get_validation_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and labels, order matters.\n    images_ds = cmdataset.map(lambda image, label: image)\n    labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n    cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() # get everything as one batch\n\n    m1 = models[0].predict(images_ds)\n    m2 = models[1].predict(images_ds)\n\n    scores = []\n    for alpha in np.linspace(0,1,100):\n        cm_probabilities = alpha*m1+(1-alpha)*m2\n        cm_predictions = np.argmax(cm_probabilities, axis=-1)\n        scores.append(f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro'))\n\n    print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n    print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)\n    plt.plot(scores)\n\n    best_alpha = np.argmax(scores)\/100\n    cm_probabilities = best_alpha*m1+(1-best_alpha)*m2\n    cm_predictions = np.argmax(cm_probabilities, axis=-1)\n","a3b80fe6":"if not ensemble_learning_models:\n    print(best_alpha, max(scores))","67a3d2ee":"if not ensemble_learning_models:\n    test_ds = get_test_dataset(ordered=True)\n    #best_alpha = 0.35\n\n    print('Computing predictions...')\n    test_images_ds = test_ds.map(lambda image, idnum: image)\n    probabilities1 = models[0].predict(test_images_ds)\n    probabilities2 = models[1].predict(test_images_ds)\n\n    probabilities = best_alpha * probabilities1 + (1 - best_alpha) * probabilities2\n\n    predictions = np.argmax(probabilities, axis=-1)\n    print(predictions)\n\n    print('Generating submission.csv file...')\n    # Get image ids from test set and convert to unicode\n    test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n\n    # Write the submission file\n    np.savetxt(\n        '..\/working\/sample_submission.csv',\n        np.rec.fromarrays([test_ids, predictions]),\n        fmt=['%s', '%d'],\n        delimiter=',',\n        header='id,label',\n        comments='',\n    )\n\n    # Look at the first few predictions\n   ","cca91031":"cmdataset = get_validation_dataset(ordered=True)\nimages_ds = cmdataset.map(lambda image, label: image)\nlabels_ds = cmdataset.map(lambda image, label: label).unbatch()\n\ncm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\n\nif not ensemble_learning_models:\n    print('using_ensemble_models')\n    probabilities1 = models[0].predict(images_ds)\n    probabilities2 = models[1].predict(images_ds)\n    cm_probabilities = best_alpha * probabilities1 + (1 - best_alpha) * probabilities2\nelse:\n    cm_probabilities = model.predict(images_ds)\n    \ncm_predictions = np.argmax(cm_probabilities, axis=-1)\n\nlabels = range(len(CLASSES))\ncmat = confusion_matrix(\n    cm_correct_labels,\n    cm_predictions,\n    labels=labels,\n)\ncmat = (cmat.T \/ cmat.sum(axis=1)).T # normalize","b78f0cae":"cmat","7b70f7dd":" !head sample_submission.csv","8f8e3036":"### Visualizing Model Performance\n\nFunctions used to known the training model performaces:\n* Loss\n* Metrics","25541c06":"# **Step6: Data Augmentation Sample**","7afdaff0":"[Track learning rate during Training](https:\/\/stackoverflow.com\/questions\/49127214\/keras-how-to-output-learning-rate-onto-tensorboard)\nNotFoundError: Container worker does not exist. (Could not find resource: worker\/_AnonymousVar8064) Encountered when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors.","58c92cd0":"Now we're ready to create a neural network for classifying images! We'll use what's known as transfer learning. With transfer learning, you reuse part of a pretrained model to get a head-start on a new dataset.\n\nFor this tutorial, we'll to use a model called VGG16 pretrained on [ImageNet](https:\/\/image-net.org\/)). Later, you might want to experiment with [other models ](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications)included with Keras. ([Xception](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/xception\/Xception) wouldn't be a bad choice.)\n\nThe distribution strategy we created earlier contains a [context manager](https:\/\/docs.python.org\/3\/reference\/compound_stmts.html#with), strategy.scope. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it's important to define your model in a strategy.scope() context.","0c701399":"You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 50 images.","b3665a07":"## To kepp track the model performance and findout the best suitable model through model-monitoring instance","47126e79":"## Data Augmentation Sample V2 (Implementing Image Processing)","fea9ee8e":"### Tuning the Additional [Flower Data](https:\/\/www.kaggle.com\/kirillblinov\/tf-flower-photo-tfrec)\n\nTo increase the proficiiency of data, I have to use the external flower dataset with the helping material from [Dmitry's](https:\/\/www.kaggle.com\/dmitrynokhrin\/densenet201-aug-additional-data) and [Araik's ](https:\/\/www.kaggle.com\/atamazian\/fc-ensemble-external-data-effnet-densenet)notebook. To visit the notebook to better understanding of the Ensamble learning and augmentation of the external dataste.","fdb6f1cf":"# Step 0 : Import Libraries\n\nwe begin this notebook by importing useful analytics libraries, in which we import statistical, data visualization and milidating overfitting libraries along with tensorflow and keras.","15f45f66":"### **Ensemble Learning Sample Code**","d0ebb0cc":"# **Step7: Defining The Model**","bd5b76ba":"### Data Augmentation of the External Data\n\nNote: In-depth understanding of the data augmentation, visit [Dmitry's Notebook](https:\/\/www.kaggle.com\/dmitrynokhrin\/densenet201-aug-additional-data)\n","c1ca1bb1":"if not ensemble_learning_models:\n    cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\n    score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n    precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n    recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n    plot_confusion_matrix(cmat, score, precision, recall)\n    print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))","9680c47a":"# **Step4: Data Pipelines**","dc2662b3":"# **Step8: Model Training**\n### Customize learning rate scheduler\n\nwe have to train this model using custom learning rate scheduling.\n","d4ea9203":"**To make TPU faster, increase the batch size**\n","025589e1":"# Step 3: Loading Data (Setting up the parameters)","9c5b8fc7":"# Data Directories","234be712":"TensorFlow Addons is a repository of contributions that conform to well-established API patterns, but implement new functionality not available in core TensorFlow. TensorFlow natively supports a large number of operators, layers, metrics, losses, and optimizers. [Read out more]([https:\/\/github.com\/tensorflow\/addons]) ","b6ac6f4a":"### Tuning Custom [Callbacks](https:\/\/www.tensorflow.org\/guide\/keras\/custom_callback)","0fc80361":"### Important : How to track learning rate during model training?\n\nNote: Stochastic gradient descent is an optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the back-propagation of errors algorithm, referred to as simply backpropagation.\nThe amount that the weights are updated during training is referred to as the step size or the \u201clearning rate.\u201d\nSpecifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.\nFor more information review the article of Jason Brownlee \"[How to Configure the Learning Rate When Training Deep Learning Neural Networks](https:\/\/machinelearningmastery.com\/learning-rate-for-deep-learning-neural-networks\/)\"","c9ededd4":"# Step 2: Loading The Competition Data\n\n### Get GCS Path\nWhen used with TPUs, datasets need to be stored in a [Google Cloud Storage](https:\/\/cloud.google.com\/storage\/) bucket. You can use data from any public GCS bucket by giving its path just like you would data from '\/kaggle\/input'. The following will retrieve the GCS path for this competition's dataset.","835d5fbb":"### Image Analysis with or without Augmentation ","bb779066":"### What TPUClusterResolver() does? <br>\nTPUs are network-connected accelerators and you must first locate them on the network. In TPUStrategy, the main object is to contain the necessary distributed training code that will work on TPUs with their 8 compute cores. Whenever, you use the TPUStrategy by instantiating your model in the scope of the strategy. This creates the model on the TPU. Model size is constrained by the TPU RAM only, not by the amount of memory available on the VM running your Python code. Model creation and model training use the usual Keras APIs. Further read about [TPUClusterResolver() ](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/ClusterResolver) and\n[Kaggle TPU Doc](https:\/\/www.kaggle.com\/docs\/tpu)","70ff5b0a":"# Introduction  \n\nWelcome to the Petals to the Metal competition! In this competition, you\u2019re challenged to build a machine learning model to classify 104 types of flowers based on their images.\nIn this tutorial notebook, you'll learn how to build an image classifier in Keras and train it on a Tensor Processing Unit (TPU). At the end, you'll have a complete project you can build off of with ideas of your own.\n\nTo improve classification accuracy of the model on the test dataset, the following are explored:\n\n* Input image size\n* Pretrained model and number of trainable parameters of final model\n* Data augmentation\n* Regularization techniques\n* Use of learning rate schedule\n\n","d2eddddd":"\n\n\n\n\n\n\nWhen used with TPUs, datasets are often serialized into [TFRecords](https:\/\/www.kaggle.com\/ryanholbrook\/tfrecords-basics). This is a format convenient for distributing data to each of the TPUs cores. We've hidden the cell that reads the TFRecords for our dataset since the process is a bit long. You could come back to it later for some guidance on using your own datasets with TPUs.","ccd55f6a":"## Data Augmentation Sample V3 (Implementing Image Processing)","b5e26ac3":"### Functions to Handle the Data","765de29c":"### Deploy the model on mobile and IOT\n\nTo deploy the models into iot and mobile devices, we need to convert the .h5 into [Tensorflow lite](https:\/\/www.tensorflow.org\/lite\/convert)\n\n![image.png](attachment:f253a559-b008-4a3b-93ed-f9d48f213fd9.png)\n","48142064":"### Ensemble Learning","295816e1":"### Data Augmentation\nInspired from Xuanzhi Huang and Rahul Paul's [notebook](https:\/\/www.kaggle.com\/xuanzhihuang\/flower-classification-densenet-201)\n\n","d68ebc55":"# Convert the model\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model\nwith open(tflite_model_name, 'wb') as f:\n    f.write(tflite_model)\n    \nprint('TFLiteConversion completed successfully \\U0001F680')  ","6f59167d":"### Defining of Model without Ensemble Methods","cfc13edf":"### Fit Model (Parameters)","f312e11d":"# **Step5: Data Exploration**","57874d53":"We'll use the distribution strategy when we create our neural network model. Then, TensorFlow will distribute the training among the eight TPU cores by creating eight different replicas of the model, one for each core.","81534888":"The test set is a stream of (image, idnum) pairs; idnum here is the unique identifier given to the image that we'll use later when we make our submission as a csv file","42af8486":"These datasets are tf.data.Dataset objects. You can think about a dataset in TensorFlow as a stream of data records. The training and validation sets are streams of (image, label) pairs.","5cc28b81":"You can use data from any public dataset here on Kaggle in just the same way. If you'd like to use data from one of your private datasets, see [here](https:\/\/www.kaggle.com\/docs\/tpu#tpu3pt5).\n","171abd85":"# Step 1: Distribution Strategy\nA TPU has eight different cores and each of these cores acts as its own accelerator. (A TPU is sort of like having eight GPUs in one machine.) We tell TensorFlow how to make use of all these cores at once through a distribution strategy. Run the following cell to create the distribution strategy that we'll later apply to our model.","bcc51f25":"TPU's is basically used to allocate the larger models having huge training inputs and batches, equipped with upto 128GB of high-speed memory allocation. In this notebook, we used images dataset having pixel size is 512 x 512px, and see how TPU v3-8 handle it.\n* num_parallel_reads=AUTO is used to automatically read multiple file.\n* experimental_deterministic = False, we used \"experimental_deterministic\" to maintain the order of the data. Here, we disable the enforcement order to shuffle the data anyway.\n\n","4eec0188":"### Calculate the Weight of each [Flower Class](https:\/\/www.kaggle.com\/xuanzhihuang\/flower-classification-densenet-201)"}}