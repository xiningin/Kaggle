{"cell_type":{"a7fd291b":"code","643cb3b1":"code","a3e989d6":"code","ae76b4af":"code","b2f0ebbe":"code","a901db2a":"code","ff8c67c7":"code","808c5099":"code","ae8cb4d9":"code","6d9e88f5":"code","3cf73f3c":"code","75fdac18":"code","62d00997":"code","91043ea7":"code","1b0f6f88":"code","7adff19b":"code","75be1f76":"code","31615542":"code","e985948d":"code","99908782":"code","f20004bf":"code","756484b4":"code","4f9566c3":"code","0b14f0e4":"code","f3a37d0d":"code","81c5e894":"code","9c044304":"code","f049bab1":"code","35604b91":"code","ac67712c":"code","0d31892c":"code","658b9fff":"code","dbc034cb":"code","a9ac1424":"code","0ecb43b1":"code","600a8ca8":"code","e1b8d40b":"code","5fb0b4cd":"code","bcec0b54":"code","784ed2f6":"code","e06bfa3d":"code","b474df66":"code","6f24f3f6":"code","7c362101":"code","cd7c5176":"code","850c8404":"code","b3cf8efb":"code","5051a68e":"code","4cf77bbd":"code","b691d350":"code","72852eae":"code","36601e78":"code","542e92c1":"code","ef91db38":"code","aebad1a4":"code","6cc42c59":"markdown","4f0a4551":"markdown","d84d3c19":"markdown","db1cc0b8":"markdown","8c5c0395":"markdown","7fb2e6b4":"markdown","fce12d35":"markdown","a9c033d8":"markdown","e6917823":"markdown","672d72ad":"markdown","e95e68c2":"markdown","3d3384b2":"markdown","95834cd1":"markdown","8b1feb31":"markdown","e08d73db":"markdown","391fd8f7":"markdown","6b8a0b35":"markdown","6da7f48b":"markdown","5f5e0c06":"markdown","b43eafc2":"markdown","8a1aa410":"markdown","aaeaaa4c":"markdown","bf71a42a":"markdown","f599d45d":"markdown","9199866b":"markdown","0e019c6e":"markdown","b7145401":"markdown","a0dda978":"markdown"},"source":{"a7fd291b":"from matplotlib import pyplot as plt # plotting image\nimport seaborn as sns","643cb3b1":"!pip install --upgrade mxnet-cu92mkl","a3e989d6":"import numpy as np # array processing\n\n# using mxnet\nimport mxnet as mx\nfrom mxnet import nd, gluon, autograd\nfrom mxnet.io import NDArrayIter\nfrom mxnet.gluon import nn\n\nimport os\nfrom glob import glob\n# from shutil import copyfile\n\nimport pandas as pd","ae76b4af":"def plot_mx_array(array):\n    assert array.shape[2] == 3, \"RGB Channel should be last\"\n    plt.imshow((array.clip(0, 255)\/255).asnumpy())","b2f0ebbe":"def augmentor(data, label):\n    # Normalizing pixel value : 0 ~ 1\n    data = data.astype('float32') \/ 255.\n    \n    # Augmentation list\n    aug_list = [\n        mx.image.ForceResizeAug(size=(128, 128)), # Resizing\n        mx.image.HorizontalFlipAug(p=0.5), # Horizontal Flip\n        mx.image.BrightnessJitterAug(brightness=0.2), # Jittering brightness\n        mx.image.HueJitterAug(hue=0.2), # Jittering Hue\n        mx.image.ContrastJitterAug(contrast=0.2) # Jittering Contrast \n    ]\n    \n    # Random Order Augmentation operation\n    augs = mx.image.RandomOrderAug(aug_list)\n    \n    # apply to data\n    data = augs(data)\n    \n    if np.random.rand() > 0.5:\n        data = data.swapaxes(0, 1)\n        \n    data = data.swapaxes(0, 2)\n    return data, label","a901db2a":"def testset_augmentor(data, label):\n    data = data.astype('float32') \/ 255.\n    aug = mx.image.ForceResizeAug(size=(128, 128)) # For predictions\n    data = aug(data)\n    data = data.swapaxes(0, 2)\n    return data, label","ff8c67c7":"training_dataset = mx.gluon.data.vision.ImageFolderDataset(\"..\/input\/split-data\/images\/train\", transform=augmentor)\ntest_dataset = mx.gluon.data.vision.ImageFolderDataset(\"..\/input\/split-data\/images\/test\", transform=testset_augmentor)","808c5099":"sample = training_dataset[0]\nsample_data = sample[0]","ae8cb4d9":"print(\"%d is Uninfected Cell\" % sample[1])\nplot_mx_array(sample_data.swapaxes(0, 2) * 255)","6d9e88f5":"sample = training_dataset[16900]\nsample_data = sample[0]","3cf73f3c":"print(\"%d is Infected Cell\" % sample[1])\nplot_mx_array(sample_data.swapaxes(0, 2) * 255)","75fdac18":"batch_size = 100\ntrain_loader = mx.gluon.data.DataLoader(training_dataset, shuffle=True, batch_size=batch_size)\ntest_loader = mx.gluon.data.DataLoader(test_dataset, shuffle=False, batch_size=10)","62d00997":"lenet = nn.HybridSequential(prefix='LeNet_')\nwith lenet.name_scope():\n    lenet.add(\n        nn.Conv2D(channels=8, kernel_size=(5, 5), activation='relu'),\n        nn.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n        nn.Conv2D(channels=8, kernel_size=(5, 5), activation='relu'),\n        nn.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n        nn.Conv2D(channels=16, kernel_size=(3, 3), activation='relu'),\n        nn.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n        nn.Conv2D(channels=16, kernel_size=(2, 2), activation='relu'),\n        nn.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n        nn.Conv2D(channels=32, kernel_size=(2, 2), activation='relu'),\n        nn.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n        nn.Flatten(),\n        nn.Dense(128, activation='relu'),\n        nn.Dense(2, activation=None),\n    )","91043ea7":"ctx = mx.gpu(0) if mx.context.num_gpus() > 0 else mx.cpu(0)\nlenet.initialize(mx.init.Xavier(), ctx=ctx)","1b0f6f88":"lenet.summary(nd.zeros((1, 3, 128, 128), ctx=ctx))","7adff19b":"trainer = gluon.Trainer(\n    params=lenet.collect_params(),\n    optimizer='adam',\n    optimizer_params={'learning_rate': 0.001},\n)","75be1f76":"metric = mx.metric.Accuracy()\nloss_function = gluon.loss.SoftmaxCrossEntropyLoss()","31615542":"num_epochs = 20\n\nfor epoch in range(num_epochs):\n    for inputs, labels in train_loader:\n        # Possibly copy inputs and labels to the GPU\n        inputs = inputs.as_in_context(ctx)\n        labels = labels.as_in_context(ctx)\n\n        # The forward pass and the loss computation need to be wrapped\n        # in a `record()` scope to make sure the computational graph is\n        # recorded in order to automatically compute the gradients\n        # during the backward pass.\n        with autograd.record():\n            outputs = lenet(inputs)\n            loss = loss_function(outputs, labels)\n\n        # Compute gradients by backpropagation and update the evaluation\n        # metric\n        loss.backward()\n        metric.update(labels, outputs)\n\n        # Update the parameters by stepping the trainer; the batch size\n        # is required to normalize the gradients by `1 \/ batch_size`.\n        trainer.step(batch_size=inputs.shape[0])\n\n    # Print the evaluation metric and reset it for the next epoch\n    name, acc = metric.get()\n    print('After epoch {}: {} = {}'.format(epoch + 1, name, acc))\n    metric.reset()\n#     if epoch % 20 == 0 or epoch == num_epochs - 1:\n    metric_test = mx.metric.Accuracy()\n    for inputs, labels in test_loader:\n        # Possibly copy inputs and labels to the GPU\n        inputs = inputs.as_in_context(ctx)\n        labels = labels.as_in_context(ctx)\n        metric_test.update(labels, lenet(inputs))\n    print('\\tTest: {} = {}'.format(*metric_test.get()))\n    ","e985948d":"from sklearn import metrics","99908782":"train_loader_pred = mx.gluon.data.DataLoader(training_dataset, shuffle=False, batch_size=20)\ntest_loader_pred = mx.gluon.data.DataLoader(test_dataset, shuffle=False, batch_size=20)","f20004bf":"scores_train = []\ntrain_label = []\ntrain_imgs = []\nfor inputs, labels in train_loader_pred:\n    # Possibly copy inputs and labels to the GPU\n    train_imgs.append(inputs)\n    train_label.append(labels)\n    inputs = inputs.as_in_context(ctx)\n    labels = labels.as_in_context(ctx)\n    outputs = lenet(inputs)\n    outputs = outputs.as_in_context(mx.cpu(0)).asnumpy()\n    scores_train.append(outputs)\n    \ntrain_imgs = nd.concat(*train_imgs, dim = 0)\ntrain_label = nd.concat(*train_label, dim = 0)","756484b4":"scores_test = []\ntest_label = []\ntest_imgs = []\nfor inputs, labels in test_loader_pred:\n    # Possibly copy inputs and labels to the GPU\n    test_imgs.append(inputs)\n    test_label.append(labels)\n    inputs = inputs.as_in_context(ctx)\n    labels = labels.as_in_context(ctx)\n    outputs = lenet(inputs)\n    outputs = outputs.as_in_context(mx.cpu(0)).asnumpy()\n    scores_test.append(outputs)\n\ntest_imgs = nd.concat(*test_imgs, dim = 0)\ntest_label = nd.concat(*test_label, dim = 0)","4f9566c3":"def compute_prob(x):\n    return np.exp(x) \/ np.exp(x).sum()","0b14f0e4":"train_scores = np.vstack([np.apply_along_axis(compute_prob, 1, arr) for arr in scores_train])\ntrain_scores_inf_prob = train_scores[:, 1]\n\ntest_scores = np.vstack([np.apply_along_axis(compute_prob, 1, arr) for arr in scores_test])\ntest_scores_inf_prob = test_scores[:, 1]","f3a37d0d":"fpr_train, tpr_train, _ = metrics.roc_curve(train_label.asnumpy(), train_scores_inf_prob)\nfpr_test, tpr_test, _ = metrics.roc_curve(test_label.asnumpy(), test_scores_inf_prob)","81c5e894":"# plotting\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_train, tpr_train, label='Train')\nplt.plot(fpr_test, tpr_test, label='Test')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='Set')\nplt.show()","9c044304":"train_auc = metrics.auc(fpr_train, tpr_train)\ntest_auc = metrics.auc(fpr_test, tpr_test)","f049bab1":"print(\"Train AUC : \", train_auc)\nprint(\"Test AUC : \", test_auc)","35604b91":"test_pred_dt = pd.DataFrame(data={\"idx\": [i for i in range(0, len(test_label))], \"label\": list(test_label.asnumpy()), \"prob\": list(test_scores_inf_prob)})","ac67712c":"test_pred_dt = test_pred_dt.sort_values(\"prob\")","0d31892c":"test_pred_dt.reset_index(inplace=True, drop=True)","658b9fff":"test_pred_dt.head()","dbc034cb":"test_pred_dt.tail()","a9ac1424":"plt.subplots(figsize=(10,10))\nlabel_dict = {\"0\": \"Uninfection\", \"1\": \"Infection\"}\nfor label in [0, 1]:\n    # Subset to the airline\n    subset = test_pred_dt[test_pred_dt['label'] == label]\n    \n    # Draw the density plot\n    sns.distplot(subset['prob'], hist = False, kde = True, \n                 kde_kws = {'shade': True, 'linewidth': 2, 'bw': 0.01},\n                 label = label_dict[str(label)])","0ecb43b1":"print(\"Number of Observation Per bin :\", len(test_pred_dt) \/\/ 10)\nprint(\"Number of Observation of Last bin :\", len(test_pred_dt) \/\/ 10 + len(test_pred_dt) % 10)","600a8ca8":"bins = []\nfor i in range(1, 11):\n    if i == 10:\n        bins.append(np.repeat(i, (len(test_pred_dt) \/\/ 10) + len(test_pred_dt) % 10))\n    else:\n        bins.append(np.repeat(i, len(test_pred_dt) \/\/ 10))        ","e1b8d40b":"bins_np = np.hstack(bins)\ntest_pred_dt[\"bins\"] = bins_np","5fb0b4cd":"pd.concat([test_pred_dt.head(3), test_pred_dt.tail(3)]) # For printing","bcec0b54":"test_pred_dt[\"bins\"] = test_pred_dt[\"bins\"].astype('category')","784ed2f6":"df1 = test_pred_dt[test_pred_dt[\"label\"] == 0].groupby(['bins']).size().reset_index(name='count')\ndf2 = test_pred_dt[test_pred_dt[\"label\"] == 1].groupby(['bins']).size().reset_index(name='count')","e06bfa3d":"by_bins = pd.merge(df1, df2, on=\"bins\")\nby_bins.rename(index=str, columns={\"count_x\": \"Uninf_count\", \"count_y\": \"Inf_count\"}, inplace=True)","b474df66":"by_bins[\"total_count\"] = by_bins[\"Uninf_count\"] + by_bins[\"Inf_count\"]","6f24f3f6":"by_bins[\"Uninf_cul_count\"] = by_bins[\"Uninf_count\"].cumsum()\nby_bins[\"Inf_cul_count\"] = by_bins[\"Inf_count\"].cumsum()","7c362101":"by_bins[\"Uninf_ratio\"] = by_bins[\"Uninf_count\"] \/ by_bins[\"total_count\"]\nby_bins[\"Inf_ratio\"] = by_bins[\"Inf_count\"] \/ by_bins[\"total_count\"]","cd7c5176":"by_bins[\"Uninf_cul_ratio\"] = by_bins[\"Uninf_cul_count\"]\/by_bins[\"Uninf_count\"].sum()\nby_bins[\"Inf_cul_ratio\"] = by_bins[\"Inf_cul_count\"]\/by_bins[\"Inf_count\"].sum()\nby_bins[\"Diff\"] = by_bins[\"Uninf_cul_ratio\"] - by_bins[\"Inf_cul_ratio\"]","850c8404":"result = by_bins.loc[:, [\"bins\", \"total_count\", \"Inf_count\",  \"Inf_cul_count\",  \"Inf_ratio\", \n                         \"Uninf_cul_ratio\", \"Inf_cul_ratio\", \"Diff\"]]","b3cf8efb":"result","5051a68e":"plt.subplots(figsize=(10,10))\nsns.lineplot(x=\"bins\", y=\"Uninf_cul_ratio\", data=result, color=\"darkblue\", label=\"Uninfection\", markers=True, dashes=True)\nsns.lineplot(x=\"bins\", y=\"Inf_cul_ratio\", data=result, color=\"orange\", label=\"Infection\", markers=True, dashes=True)\nplt.axvline(result.loc[result[\"Diff\"] == result[\"Diff\"].max(), \"bins\"][0], 0.05, 0.95, color=\"red\")","4cf77bbd":"Normal = int(len(test_pred_dt) * 0.30)\nAmbiguous = int(len(test_pred_dt) * 0.45)\nEndStage = len(test_pred_dt) - (Normal + Ambiguous)","b691d350":"print(\"Normal : %d\\tAmbiguous : %d\\tEndStage : %d\" % (Normal, Ambiguous, EndStage))","72852eae":"test_pred_dt[\"Sugg_Grade\"] = np.hstack([np.repeat(\"Normal\", Normal), \n                                        np.repeat(\"Ambiguous\", Ambiguous), \n                                        np.repeat(\"EndStage\", EndStage)])","36601e78":"Normal_InfRatio = test_pred_dt.loc[(test_pred_dt[\"Sugg_Grade\"] == \"Normal\"), \"label\"].mean()\nAmbiguous_InfRatio = test_pred_dt.loc[(test_pred_dt[\"Sugg_Grade\"] == \"Ambiguous\"), \"label\"].mean()\nEndStage_InfRatio = test_pred_dt.loc[(test_pred_dt[\"Sugg_Grade\"] == \"EndStage\"), \"label\"].mean()","542e92c1":"print(\"Normal : %.5f\\tAmbiguous : %.5f\\tEndStage : %.5f\" % (Normal_InfRatio, Ambiguous_InfRatio, EndStage_InfRatio))","ef91db38":"cutoff_df = test_pred_dt.loc[:, [\"prob\", \"Sugg_Grade\", \"label\"]].groupby(['Sugg_Grade']).agg({\"prob\": ['min', 'max'], \"Sugg_Grade\": ['count']})\ncutoff_df.columns = [\"_\".join(x) for x in cutoff_df.columns.ravel()]\ncutoff_df.sort_values(\"prob_min\", inplace=True)","aebad1a4":"cutoff_df","6cc42c59":"setting batch size and data loader","4f0a4551":" - Oh! that's really good performance.\n - model splits Infection and Uninfection very well.","d84d3c19":"## Let's compute other metric : AUROC","db1cc0b8":"create trainer for training model","8c5c0395":"define model","7fb2e6b4":"### Training & Testing","fce12d35":" - Red line is max(Diff) that is KS score.\n - KS score is 0.903241 that is very good performance.\n - The higher KS score, more spliting between Infection and Uninfection.\n - But it is not over yet.\n - How you do the grading can be decided by the people who want to use it.\n - In my example, I think the grade consists of 3 types.\n     + Normal, Ambiguous, Endstage\n     + From left to right, I will give 30%, 45%, and 25% composition of ratio.\n     + In credit scoring that using logistic regression, The composition of ratio is similar to Normal distribution.\n     + But If you use machine learning algorithm such as Deep learning, Random Forest, Xgboost etc, The composition of ratio have a few right skewed distribution.","a9c033d8":"### Infection Ratio each grade\n\n - It's really important.\n - We sort data by $Pr(Infection=1)$. (Increasingly)\n - So, From Normal to EndStage, Infection ratio is increasing.\n - If not increasing, I have to do it until the ratio increasingly.","e6917823":"For loading augmented image, I define function augmentor with mxnet.","672d72ad":"4) setting device and initializing model","e95e68c2":"### define loader for predictions","3d3384b2":" - Now, for computing Kolmogorov\u2013Smirnov statistics, I spilt to 10 bins equally using $Pr(Infection=1)$.","95834cd1":" - This is result of equally splitting 10 bins.\n - Uninf_cul_ratio is cumulative Uninfection ratio.\n - Inf_cul_ratio is also cumulative Infection ratio.\n - Each culmulative ratio is the estimated distribution score per label.\n - Max of Difference between each culmulative ratio per label is an estimation of Kolmogorov\u2013Smirnov statistics(below KS score)","8b1feb31":" - Good. The order of ratios must be maintained.","e08d73db":"## Preprocessing","391fd8f7":"## Modeling using mxnet","6b8a0b35":" - Before creating grade, We check density of probability per label.","6da7f48b":"## Create Infection Grade\n\n - Like credit scoring, I propose creating Infection Grade.\n - Because I think just detecting malaria is that we do not know how malaria progressed.\n - Even if the doctor looks at the progress stage and judges it, it will surely be a good reference if doctor confirm the grade with the model.\n - For creating grade, I use test set.","5f5e0c06":" - It is just simple grading in my thinking.\n - The reasons for grading are for patient management.\n     + So, my grade can be wrong.\n - Actually, It is not necessary to perform this sorting in situation that dectection is an important issue. \n - Doctors simply can get help if the model judges whether or not people are simply sick.","b43eafc2":"### compute scores","8a1aa410":" - Firt of grading, We need to sort observation by probability of infection.\n     + Grading is using concept of ordering $Pr(Infection=1)$.","aaeaaa4c":" - What is cut off score of each Grade?","bf71a42a":"setting metric and loss function","f599d45d":"### compute AUROC","9199866b":" - I want to directly load image data from path.\n - so, I add data that randomly split into train and test.\n - The ratio between train and test is 70:30.\n - And I add data augmentation code.","0e019c6e":"- I just create and train model that have simple architecture (Lenet)\n- It's very simple but not bad performance !\n- This is my baseline.","b7145401":"### My plan\n ~~1) Explicitly, I will split data into training set and test set. ~~ Assume that test set is in out of time.~~\n - 2) Hyper-parameter Optimizing or Using Pre-trained model\n - ~~3) I think that just using model predictions is not good in real world. So, like credit grade, I will create simple Infection grade.\n     ~~- For creating infection grade, we need to compute Kolmogorov\u2013Smirnov statistics.~~\n - 4) For interpretablility, I will use lime or grad-cam.","a0dda978":"## Load Image"}}