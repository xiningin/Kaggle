{"cell_type":{"cff79c8a":"code","3493c457":"code","23e146a6":"code","23c8938c":"code","c0b8eaef":"code","69d128ca":"code","a24c51cd":"code","e6b8cbac":"code","f56ff654":"code","e326facf":"code","9b1ae615":"code","7ea3113f":"code","368a603f":"markdown","347f9320":"markdown","23ef717d":"markdown","13801552":"markdown","e67a904e":"markdown","82d2643f":"markdown","492d29fe":"markdown","ac101310":"markdown","0e7d33df":"markdown","48f71092":"markdown","3759c4a6":"markdown","1fc11b1c":"markdown"},"source":{"cff79c8a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\nprint(\"TF version \", tf.__version__)\nfrom tensorflow import keras as kr\nfrom sklearn.model_selection import StratifiedKFold\n\nprint(os.listdir(\"..\/input\"))\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","3493c457":"Y_train = train_df['label'] # keep labels\nX_train = train_df.iloc[:,1:train_df.shape[1]].values # dataframe to numpy array\nX_test = test_df.values # dataframe to numpy array","23e146a6":"X_train = X_train.astype(float) \/ 255.\nX_test = X_test.astype(float) \/ 255.\n\nprint(\"Current Shape => X_train: %s | Y_train %s | X_test %s\" % (str(X_train.shape), str(Y_train.shape), str(X_test.shape) ))","23c8938c":"X_train = X_train.reshape(-1,28, 28, 1)\nX_test = X_test.reshape(-1,28, 28, 1)\nprint(\"After Reshape => X_train: %s | X_test %s\" % (str(X_train.shape), str(X_test.shape)))\n\nplt.imshow(X_train[25].reshape(28,28)) # let's see a sample of the Data","c0b8eaef":"\"\"\" Use Keras image data generator \"\"\"\n\nAUGMENTED_SAMPLES = 10000 # how many new samples\noriginal_size = X_train.shape[0] # training samples before augmentation\n\nimage_generator = kr.preprocessing.image.ImageDataGenerator(\n        rotation_range=22, zoom_range = 0.05, width_shift_range=0.05,\n        height_shift_range=0.05, horizontal_flip=False, vertical_flip=False, \n        data_format=\"channels_last\", zca_whitening=False, featurewise_center=True, featurewise_std_normalization=True)\n\nimage_generator.fit(X_train, augment=True) # fit generator on training data\n\naugm = np.random.randint(original_size, size=AUGMENTED_SAMPLES) # get random samples from the original dataset\nX_augmented = X_train[augm].copy()\nY_augmented = Y_train[augm].copy()\nX_augmented = image_generator.flow(X_augmented, np.zeros(AUGMENTED_SAMPLES), batch_size=AUGMENTED_SAMPLES, shuffle=False).next()[0]\n\n# append new data to our already existing train set\nX_train = np.concatenate((X_train, X_augmented))\nY_train = np.concatenate((Y_train, Y_augmented))\n\nprint('New Trainset Size: X %s - Y %s' % (str(X_train.shape), str(Y_train.shape)))\n\n\"\"\" Let's take a look at an augmented sample and the original \"\"\"\nim = 0 # select image\nprint('Original Image')\nplt.imshow(X_train[augm[im]].reshape(28,28))\nplt.show()\nprint('Augmented Image')\nplt.imshow(X_train[original_size+im].reshape(28,28))\nplt.show()","69d128ca":"\"\"\"\nLabels to hot encoded for cross-entropy\n\n[1 , 5 , ...]  -> [[0,1,0,0,0,0 .. ], [0,0,0,0,0,1,0,0 ...]]\n\"\"\"\nY_train_cat = kr.utils.to_categorical(Y_train, num_classes=10)\n\nprint(\"Categorical Y shape:  %s \" % str(Y_train_cat.shape) )","a24c51cd":"model = kr.models.Sequential()\n\nmodel.add(kr.layers.Conv2D(64, kernel_size= (3,3), activation='relu', input_shape=(28,28,1)))\nmodel.add(kr.layers.BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'))\nmodel.add(kr.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(kr.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))\nmodel.add(kr.layers.BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'))\nmodel.add(kr.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(kr.layers.Flatten())\nmodel.add(kr.layers.Dense(64, activation='relu'))\nmodel.add(kr.layers.Dense(10, activation='softmax'))\n\nopt = kr.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) # Adam Optimizer\n# early_stop = kr.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto', baseline=None)","e6b8cbac":"BATCH_SIZE = 128\nEPOCHS = 50\nLOGS = 2\nVALIDATION_SPLIT = 0.15\n\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(X_train,Y_train_cat, validation_split=VALIDATION_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=LOGS, shuffle=False, class_weight=None, sample_weight=None) # train","f56ff654":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","e326facf":"res = model.predict(X_test)\nres = res.argmax(axis=1)\n\nprint('Prediction Label Distribution')\ng = sns.countplot(Y_train)\nplt.show()","9b1ae615":"for i in range(0,10):\n    print('Predicted', res[i])\n    plt.imshow(X_test[i].reshape(28,28))\n    plt.show()","7ea3113f":"out = pd.Series(res,name=\"Label\")\nout = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),out],axis = 1)\nout.head()\nout.to_csv('keras_cnn_results.csv',index=False, sep=',')","368a603f":"**Append** Resutls to file","347f9320":"Predict the Test images\n> **argmax** takes the vector with the probabilities and returns the result label","23ef717d":"**Import Libraries** \n\nPandas | Numpy: Store and manipulate Data","13801552":"**Reshape Data** to 28x28x1 Arrays","e67a904e":"__Keras Classifier Model__\n\n> The Sequence of layers is:\n>*   Convolution Layer {64 units, 3x3 filter size, ReLU Activation Function}\n>*   Batch Normalization Layer -> Normalizes the activations of the previous layer at each batch\n>*   Max Pooling using 2x2 window size\n>*   Convolution Layer {32 units, 3x3 filter size, ReLU Activation Function}\n>*   Batch Normalization Layer\n>*   Max Pooling\n>*   Flatten Data (28x28) -> (784,) in order to use as input in the next Dense Layer\n>*   Normal Dense Layer {64 units, RELU Activation)\n>*   Ouput Layer with Softmax Activation Function -> one-hot vector with pobabilities for each class\n    ","82d2643f":"**Compile** and **Train** our Model","492d29fe":"Create a **Hot Vector** for the Labels\n> Note: This Representation of the Labels is imperative for the modeling of the NN's output","ac101310":"Create **Train** - **Test**[](http:\/\/) Instances\n\n> *Note*: we don't need to set a **validation** set by hand, during training we will set it as a parameter in Keras","0e7d33df":"**Data Augmentation**\n> Create new data, generated from the already know training data (e.g. rotated\/cropped\/zoomed original images)","48f71092":"__Normalize Data__","3759c4a6":"**Plot** the Training History\n> The plots show how the Accuracy and Loss of the training and the validation (unseen data) after each epoch.","1fc11b1c":"Let's see some results of our model"}}