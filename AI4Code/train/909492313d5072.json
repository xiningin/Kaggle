{"cell_type":{"d878baf0":"code","89c08117":"code","5c971659":"code","e8499c78":"code","c648026a":"code","c914a852":"code","27f01319":"code","06a1d499":"code","d00d62c0":"code","9659e7af":"code","4b8b5a4b":"code","1e110c68":"code","7a4f0f2a":"code","b2c5b80b":"code","a8b0c774":"code","466e672c":"code","589b5632":"code","58f8c0d5":"code","050588f6":"code","c736029a":"code","459f39b1":"code","c8a64120":"code","9971d474":"code","3e124468":"code","3c9ca6fd":"code","a1b5f9be":"code","10e4d4e5":"code","cf7d8f90":"code","acf82d81":"code","db6e0409":"code","2f22367f":"code","c344d383":"code","34eef76b":"code","7cdb35e7":"code","df029edc":"code","61830e5d":"code","f370bae6":"code","113f9d11":"code","a3719110":"code","ff361ef5":"code","db386b05":"code","877c614d":"code","dc4b9766":"code","b8eb7bc2":"code","7341ba84":"code","4b78e28f":"code","09b0a5ff":"code","17692a0f":"code","3a4c84e1":"code","bf87c17b":"code","7522d7f2":"markdown","037f6ece":"markdown","4354e6ed":"markdown","f4709d99":"markdown","98b63db7":"markdown","903ab063":"markdown","1468391e":"markdown","73905a6e":"markdown","8221e9d5":"markdown","ce714ad0":"markdown","dbf90d59":"markdown","189665af":"markdown","024f7b69":"markdown","3670619c":"markdown","51def67b":"markdown","7a84bdd9":"markdown","b8b56f63":"markdown","1b5fdebf":"markdown"},"source":{"d878baf0":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nfrom datetime import datetime\n\n%matplotlib inline","89c08117":"custom_date_parser = lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\ndataset = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/train.csv\",parse_dates=['date_time'],date_parser=custom_date_parser,)\ndataset_test = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/test.csv\",parse_dates=['date_time'],date_parser=custom_date_parser)\ndataset_test.head()","5c971659":"def extract_dt_feats(df):\n    # Extract month and hour\n    date_enc = pd.to_datetime(df.date_time)\n    month = date_enc.dt.month\n    hour = date_enc.dt.hour\n    year = date_enc.dt.year\n    date = date_enc.dt.date\n    date = pd.DataFrame(date)\n    # Add features, compute and add is_weekend\n    sin_cos_encoding(df, month, 'month', 12)\n    sin_cos_encoding(df, hour, 'hour', 23)\n    df['is_weekend'] = date_enc.dt.day_name().isin(['Saturday', 'Sunday'])*1\n    return df,date\ndef sin_cos_encoding(df, dt, feat_name, max_val):\n    # Encode variable using sin and cos\n    df['sin_' + feat_name] = np.sin(2 * np.pi * (dt\/max_val))\n    df['cos_' + feat_name] = np.cos(2 * np.pi * (dt\/max_val))\n    return None","e8499c78":"dataset,date = extract_dt_feats(dataset.copy())\ndate.rename(columns = {'date_time':'Only_Dates'}, inplace = True)\nresult = pd.concat([date, dataset], axis=1)\npd.to_datetime(result.Only_Dates)","c648026a":"import plotly.express as px\nfig = px.line(result, x='Only_Dates', y=\"target_carbon_monoxide\")\nfig.show()\n","c914a852":"import plotly.express as px\nfig = px.line(result, x='Only_Dates', y=\"target_benzene\")\nfig.show()","27f01319":"import plotly.express as px\nfig = px.line(result, x='Only_Dates', y=\"target_nitrogen_oxides\")\nfig.show()","06a1d499":"fig, axes = plt.subplots(2, 3, figsize=(18, 10))\ndataset.sin_month.plot(ax = axes[0,0])\ndataset.cos_month.plot(ax = axes[0,1])\naxes[0,2].scatter(x=dataset['sin_month'],y=dataset['cos_month'])\naxes[0,2].set_title(\"Two-feature transformation in 2D as a 24-hour clock\")\ndataset.sin_month.plot(ax = axes[1,0])\ndataset.cos_month.plot(ax = axes[1,1])\naxes[1,2].scatter(x=dataset['sin_hour'],y=dataset['cos_hour'])\naxes[1,2].set_title(\"Two-feature transformation in 2D as a 24-hour clock\")\naxes[0, 0].set_title(\"sin_month\")\naxes[0, 1].set_title(\"cos_month\")\naxes[1, 0].set_title(\"sin_hour\")\naxes[1, 1].set_title(\"cos_hour\")","d00d62c0":"TARGET_VARS = ['target_carbon_monoxide',\n               'target_benzene',\n               'target_nitrogen_oxides']\nsns.pairplot(dataset, hue='is_weekend', vars=TARGET_VARS, corner=True,\n            plot_kws={'alpha':.1})","9659e7af":"dataset_co = dataset[['date_time']+['deg_C']+['absolute_humidity']+['relative_humidity']+['sensor_1']+['sensor_2']+['sensor_3']+['sensor_4']+['sensor_5']\n                     +['target_carbon_monoxide']+['sin_hour']+['cos_hour']+['sin_month']+['cos_month']]\ndataset_ben = dataset[['date_time']+['deg_C']+['absolute_humidity']+['relative_humidity']+['sensor_1']+['sensor_2']+['sensor_3']+['sensor_4']+['sensor_5']\n                     +['target_benzene']+['sin_hour']+['cos_hour']+['sin_month']+['cos_month']]\ndataset_ni = dataset[['date_time']+['deg_C']+['absolute_humidity']+['relative_humidity']+['sensor_1']+['sensor_2']+['sensor_3']+['sensor_4']+['sensor_5']\n                     +['target_nitrogen_oxides']+['sin_hour']+['cos_hour']+['sin_month']+['cos_month']]","4b8b5a4b":"dataset.isnull().any()","1e110c68":"sns.heatmap(dataset_co.corr(), cmap='RdYlGn_r', vmax=1.0, vmin=-1 ,annot = True)\n\ndataset_co[dataset_co.columns[1:]].corr()['target_carbon_monoxide'][:]","7a4f0f2a":"sns.heatmap(dataset_ben.corr(), annot = True)\ndataset_ben[dataset_ben.columns[1:]].corr()['target_benzene'][:]","b2c5b80b":"sns.heatmap(dataset_ni.corr(), annot = True)\ndataset_ni[dataset_ni.columns[1:]].corr()['target_nitrogen_oxides'][:]","a8b0c774":"fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n\nfig.suptitle('Before Log-Transnformation')\n\nsns.histplot(dataset['deg_C'],ax=axes[0, 0],kde=True)\nsns.histplot(dataset['relative_humidity'],ax=axes[0, 1],kde=True)\nsns.histplot(dataset['absolute_humidity'],ax=axes[0, 2],kde=True)\nsns.histplot(dataset['sensor_1'],ax=axes[0, 3],kde=True)\nsns.histplot(dataset['sensor_2'],ax=axes[1, 0],kde=True)\nsns.histplot(dataset['sensor_3'],ax=axes[1, 1],kde=True)\nsns.histplot(dataset['sensor_4'],ax=axes[1, 2],kde=True)\nsns.histplot(dataset['sensor_5'],ax=axes[1, 3],kde=True)\n","466e672c":"# First we will try to do with most correlated independent features\ncorrelated_features = ['sensor_1','sensor_2','sensor_3','sensor_4','sensor_5']\nfor features in correlated_features:\n    dataset[features] =  np.log(dataset[features])\nfig, axes = plt.subplots(2, 4, figsize=(18, 10))\n\nfig.suptitle('After Log-Transnformation')\n\nsns.histplot(dataset['deg_C'],ax=axes[0, 0],kde=True)\nsns.histplot(dataset['relative_humidity'],ax=axes[0, 1],kde=True)\nsns.histplot(dataset['absolute_humidity'],ax=axes[0, 2],kde=True)\nsns.histplot(dataset['sensor_1'],ax=axes[0, 3],kde=True)\nsns.histplot(dataset['sensor_2'],ax=axes[1, 0],kde=True)\nsns.histplot(dataset['sensor_3'],ax=axes[1, 1],kde=True)\nsns.histplot(dataset['sensor_4'],ax=axes[1, 2],kde=True)\nsns.histplot(dataset['sensor_5'],ax=axes[1, 3],kde=True)","589b5632":"sns.displot(dataset_co['target_carbon_monoxide'],kde=True)\nx0 = pd.DataFrame(dataset['target_carbon_monoxide']).to_numpy()\nprint(skew(x0))","58f8c0d5":"sns.displot(dataset_ben['target_benzene'],kde=True)\nx1 = pd.DataFrame(dataset['target_benzene']).to_numpy()\nprint(skew(x1))","050588f6":"sns.displot(dataset_ni['target_nitrogen_oxides'],kde=True)\nx2 = pd.DataFrame(dataset['target_nitrogen_oxides']).to_numpy()\nprint(skew(x2))","c736029a":"y0 = np.log1p(x0)\nsns.displot(y0,kde = True)","459f39b1":"y1 = np.log1p(x1)\nsns.displot(y1,kde = True)","c8a64120":"y2 = np.log1p(x2)\nsns.displot(y2,kde = True)","9971d474":"dataset[TARGET_VARS] = np.log(dataset[TARGET_VARS] + 1)\nsns.pairplot(dataset, hue='is_weekend', vars=TARGET_VARS, corner=True,\n            plot_kws={'alpha':.1})","3e124468":"del dataset['date_time'] \ndel dataset['target_carbon_monoxide']\ndel dataset['target_benzene']\ndel dataset['target_nitrogen_oxides']\nX = dataset\ndataset.head()","3c9ca6fd":"X0_train, X0_test, y0_train, y0_test = train_test_split(X, y0, test_size=.2)","a1b5f9be":"from sklearn import linear_model\nmodel = linear_model.LinearRegression()\nmodel.fit(X0_train, y0_train)\nprint(\"Accuracy --> \", model.score(X0_test, y0_test)*100)","10e4d4e5":"#Train the model\nfrom sklearn.ensemble import RandomForestRegressor\nmodel0 = RandomForestRegressor(n_estimators= 800,min_samples_split= 2,min_samples_leaf= 1,max_features='sqrt',max_depth = 20,bootstrap =  False)\n#Fit\nmodel0.fit(X0_train, np.ravel(y0_train,order='C'))\n#Score\/Accuracy\nprint(\"Accuracy --> \", model0.score(X0_test, y0_test)*100)","cf7d8f90":"#GradientBoostingRegressor\n#Train the model\nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n#Fit\nGBR.fit(X0_train, np.ravel(y0_train))\nprint(\"Accuracy --> \", GBR.score(X0_test, y0_test)*100)","acf82d81":"#xgboost regressor\nimport xgboost\nclassifier = xgboost.XGBRegressor()\nclassifier.fit(X0_train, np.ravel(y0_train))\nprint(\"Accuracy --> \", classifier.score(X0_test, y0_test)*100)","db6e0409":"X1_train, X1_test, y1_train, y1_test = train_test_split(X, y1, test_size=.3)","2f22367f":"#Train the model\nfrom sklearn import linear_model\nmodel = linear_model.LinearRegression()\n#Fit the model\nmodel.fit(X1_train, y1_train)\n#Score\/Accuracy\nprint(\"Accuracy --> \", model.score(X1_test, y1_test)*100)","c344d383":"#Train the model\nfrom sklearn.ensemble import RandomForestRegressor\nmodel1 = RandomForestRegressor(n_estimators= 1200,min_samples_split = 2,min_samples_leaf =1,max_features= 'sqrt',max_depth=20,bootstrap= False)\n#Fit\nmodel1.fit(X1_train, np.ravel(y1_train,order='C'))\n#Score\/Accuracy\nprint(\"Accuracy --> \", model1.score(X1_test, y1_test)*100)","34eef76b":"#GradientBoostingRegressor\n#Train the model\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n#Fit\nmodel.fit(X1_train, np.ravel(y1_train))\nprint(\"Accuracy --> \", model.score(X1_test, y1_test)*100)","7cdb35e7":"#xgboost regressor\nimport xgboost\nclassifier = xgboost.XGBRegressor()\nclassifier.fit(X1_train, np.ravel(y1_train))\nprint(\"Accuracy --> \", classifier.score(X1_test, y1_test)*100)","df029edc":"X2_train, X2_test, y2_train, y2_test = train_test_split(X, y2, test_size=.3)","61830e5d":"#Train the model\nfrom sklearn import linear_model\nmodel = linear_model.LinearRegression()\n#Fit the model\nmodel.fit(X2_train, y2_train)\n#Score\/Accuracy\nprint(\"Accuracy --> \", model.score(X2_test, y2_test)*100)","f370bae6":"#Train the model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom pprint import pprint\nmodel2 = RandomForestRegressor(n_estimators= 1200,min_samples_split = 2,min_samples_leaf =1,max_features= 'sqrt',max_depth=20,bootstrap= False)\n#Fit\nmodel2.fit(X2_train, np.ravel(y2_train,order='C'))\n#Score\/Accuracy\nprint(\"Accuracy --> \", model2.score(X2_test, y2_test)*100)","113f9d11":"#GradientBoostingRegressor\n#Train the model\nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n#Fit\nGBR.fit(X2_train, np.ravel(y2_train))\nprint(\"Accuracy --> \", GBR.score(X2_test, y2_test)*100)","a3719110":"#xgboost regressor\nimport xgboost\nclassifier = xgboost.XGBRegressor()\nclassifier.fit(X2_train, np.ravel(y2_train))\nprint(\"Accuracy --> \", classifier.score(X2_test, y2_test)*100)","ff361ef5":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n    # Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n    # Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n    # Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n    # Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n                   'max_features': max_features,\n                   'max_depth': max_depth,\n                   'min_samples_split': min_samples_split,\n                   'min_samples_leaf': min_samples_leaf,\n                   'bootstrap': bootstrap}\npprint(random_grid)","db386b05":"from sklearn.model_selection import RandomizedSearchCV\n\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 40, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n","877c614d":"#For getting the parameters uncomment below lines and get the hyperparameters\n#Model0 = rf_random.fit(X0_train, np.ravel(y0_train,order='C'))\n#Model1 = rf_random.fit(X1_train, np.ravel(y1_train,order='C'))\n#Model2 = rf_random.fit(X2_train, np.ravel(y2_train,order='C'))\n#print(Model0.best_params_)\n#print(Model1.best_params_)\n#print(Model2.best_params_)","dc4b9766":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model0, random_state=1).fit(X0_train, y0_train)\neli5.show_weights(perm, feature_names = X0_train.columns.tolist())","b8eb7bc2":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model1, random_state=1).fit(X1_train, y1_train)\neli5.show_weights(perm, feature_names = X1_train.columns.tolist())","7341ba84":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model2, random_state=1).fit(X2_train, y2_train)\neli5.show_weights(perm, feature_names = X2_train.columns.tolist())","4b78e28f":"## Creating the function which shows the improvement in Hyperparameter tuning\ndef evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print(model.score(test_features, test_labels))\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","09b0a5ff":"#Bringing the test dataset into the frame\ndate_time = dataset_test['date_time']\ndataset_test,dates = extract_dt_feats(dataset_test.copy())","17692a0f":"correlated_features = ['sensor_1','sensor_2','sensor_3','sensor_4','sensor_5']\nfor features in correlated_features:\n    dataset_test[features] =  np.log(dataset_test[features])","3a4c84e1":"del dataset_test['date_time']\nXt = dataset_test\ndataframe_0 =pd.DataFrame(np.expm1(model0.predict(Xt)), columns=['target_carbon_monoxide']) \ndataframe_1=pd.DataFrame(np.expm1(model1.predict(Xt)), columns=['target_benzene'])\ndataframe_2=pd.DataFrame(np.expm1(model2.predict(Xt)), columns=['target_nitrogen_oxides']) ","bf87c17b":"result = pd.concat([date_time,dataframe_0, dataframe_1,dataframe_2], axis=1)\nresult.to_csv('submission1.csv',index = False)","7522d7f2":"Spliting the dataset to check the performance of the model","037f6ece":"We need to treat all the target features individually(All the datasets are extracted individually for convinence in visualization)\n\n1. Checking for missing values\n2. Correlation b\/w features and the target variables\n3. Feature selection\n4. Analysing the Target Variable \n5. Log - Transformations for skewed data\n6. Label encoding for categorical data (If needed)","4354e6ed":"Let us bring permutation importance \nHow does it works\nIt just finds the dependency of the a independent variable over the dependent variable \n\nThe top in the columns are more dependent \n\nYou can check my discussion regarding how permuatation importance works\n","f4709d99":"Random Forest Regressor for Model 2\n","98b63db7":"1. String to Datetime object\n2. Custom Parsing(Revolving) Function","903ab063":"So, I am Locking Up Random Forest Regressor for Model 1","1468391e":"Increasing the Performance of the Models\n\nWe will be performing HYPERPARAMETER TUNING\n(RandomSearchCV)","73905a6e":"## Tabular Playground Series July 21\n\nObjective : Predicting the Target Features\n\nDependent Variables : target_carbon_monoxide, target_benzene, target_nitrogen_oxides\n\nIndependent Variables : \ndate_time, deg_C, relative_humidity,     absolute_humidity,sensor_1, sensor_2, sensor_3, sensor_4, sensor_5\n\n#### Are these the only independent Variables ?? \n\nHow can we select 'data_time' being non-categorical, dtype string \n\nHere Comes Time Analysis\n\n#### What is the best suitable regressor to the data \n\nWe will be finding the regressor(model), suitable hyperparameters manually without using autoML","8221e9d5":"### Feature Engineering ++ ","ce714ad0":"So , I am locking Random Forest Regressor for Model 0 ","dbf90d59":"The sensors data is much more dependent than the deg_C ,absoule_humidity,relative_humidity\n\nThough the months and time are'nt much different they increase the accuracy of the data upto 8 percent when we tuned the hyperparameters perfectly","189665af":"1. Extracting the month and hour\n2. Encoding To sin,cos functions \n3. Adding New Independent Features to the train and test dataframe","024f7b69":"Skewness of the Target Variables","3670619c":"So ,the predicted values from the models are log-transformed values\n\nInversion of Log transformation has to be applied to make the predicted data normal","51def67b":"#### Importing Libraries","7a84bdd9":"#### Encoding cyclical continuous features - 24-hour time\n\nJust assume how to the time progresses\n\n00:00 to 24:00 constantly\n\nWell Here , There is linearity in the time \n\nBut the problem is the time distance b\/w 23:50 and 00:10 is probably 20 mins \n\nbut when the time is linear it must be equal to 23:40.\n\nSo we will relate or encode the feature data to sin,cos graph.\nJust imagine how cyclic they are in between -1 to 1\n\nHere is the representation of the plots of the graph where all the points of same hour,month are representing same point as they are collided\n","b8b56f63":"###  Time analysis \n","1b5fdebf":"Checking out different Regression models - Linear regression, Random forest Regressor, Gradient Boosting Regressor,XG Boost regressor"}}