{"cell_type":{"57c4f7a3":"code","16ff34aa":"code","6031d114":"code","c16326e5":"code","d92efe8c":"code","0bbc230f":"code","bfe0d53e":"code","d6d09156":"code","a942c5f0":"code","01c13551":"code","f183ba1c":"code","2ea2bd9c":"code","98a44843":"code","02abaf59":"code","d2a2426e":"code","3532f5fb":"code","3b7d43e8":"code","2e3d4596":"code","3ceca782":"code","dcbffad3":"code","151f5d37":"code","46eddf22":"code","d9eff17f":"code","255cd570":"code","05ad8851":"code","3e52f3bb":"code","b0721cc4":"code","716d4f7b":"code","652a7c05":"code","dc00479c":"code","8eae78e9":"markdown","66a3c020":"markdown"},"source":{"57c4f7a3":"f=open('..\/input\/simple-dialogs-for-chatbot\/dialogs.txt','r')\nfile = f.read()","16ff34aa":"qna_list = [f.split('\\t') for f in file.split('\\n')]\nprint(qna_list[:5])","6031d114":"que = [x[0] for x in qna_list]\nans = [x[1] for x in qna_list]\n\nprint(que[:5])\nprint(ans[:5])\nprint(len(que))\nprint(len(ans))","c16326e5":"from tensorflow.keras.preprocessing.text import Tokenizer","d92efe8c":"tk = Tokenizer(oov_token=\"<OOV>\")\ntk.fit_on_texts(que+ans)","0bbc230f":"tk.index_word","bfe0d53e":"tk.word_index","d6d09156":"tk.texts_to_sequences(['zaf'])","a942c5f0":"que_seq = tk.texts_to_sequences(que)\nans_seq = tk.texts_to_sequences(ans)","01c13551":"import matplotlib.pyplot as plt\nplt.hist([len(s) for s in que_seq])","f183ba1c":"plt.hist([len(s) for s in ans_seq])","2ea2bd9c":"MAX_SEQ_LEN = 12\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","98a44843":"pad_que_seq = pad_sequences(que_seq,maxlen=MAX_SEQ_LEN,padding='post')\npad_ans_seq = pad_sequences(ans_seq,maxlen=MAX_SEQ_LEN,padding='post')","02abaf59":"print(pad_que_seq[:5])\nprint(type(pad_que_seq))\nprint(pad_ans_seq[:5])\nprint(type(pad_ans_seq))","d2a2426e":"# Use tk.word_index (if tk.word_index changes texts_to_sequences uses changed dict)\ntk.texts_to_sequences(['i have'])","3532f5fb":"for key in tk.word_index:\n    tk.word_index[key] += 2","3b7d43e8":"tk.word_index['<PAD>']=0\ntk.word_index['<S>']=1\ntk.word_index['<E>']=2","2e3d4596":"a={1:2,3:4}\nb = {a[k]:k for k in a}","3ceca782":"tk.index_word = {tk.word_index[key]:key for key in tk.word_index}","dcbffad3":"que_seq = tk.texts_to_sequences(que)\nans_seq = tk.texts_to_sequences(ans)\npad_que_seq = pad_sequences(que_seq,maxlen=MAX_SEQ_LEN,padding='post')\npad_ans_seq = pad_sequences(ans_seq,maxlen=MAX_SEQ_LEN,padding='post')","151f5d37":"print(pad_que_seq.shape)\nprint(pad_ans_seq.shape)","46eddf22":"import numpy as np\ndec_input_seq = np.array([[1]+list(s[:-1]) for s in pad_ans_seq.copy()])\ndec_target_seq = [list(s) for s in pad_ans_seq.copy()]\n# print(dec_target_seq)\nfor i,l in enumerate(dec_target_seq):\n    try:\n        dec_target_seq[i][l.index(0)]=2\n    except:\n        dec_target_seq[i][-1]=2\n\ndec_target_seq = np.array(dec_target_seq)\nprint(dec_input_seq.shape)\nprint(dec_target_seq.shape)","d9eff17f":"enc_input_seq = pad_que_seq.copy()\nprint(enc_input_seq[:5])\nprint(dec_input_seq[:5])\nprint(dec_target_seq[:5])\nassert type(enc_input_seq) == type(dec_input_seq) and type(dec_input_seq) == type(dec_target_seq)\nassert enc_input_seq.shape == dec_input_seq.shape and dec_input_seq.shape == dec_target_seq.shape\nprint(enc_input_seq.shape)","255cd570":"import tensorflow as tf\nvocab_size = len(tk.word_index)\nprint(vocab_size)","05ad8851":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n        super(Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.enc_units = enc_units\n        self.concat = tf.keras.layers.Concatenate(axis=-1)\n        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n        self.pre_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.enc_units,return_sequences=True, return_state=True))\n        \n    def call(self, x):\n        # x shape == (batch_size, seq len)\n        # h,c = hidden\n        # h c shape == (batch_size, enc_units)\n        x = self.embedding(x)\n        # embedded x shape == (batch_size, seq_len, emb_dim)\n        x,fh,fs,bh,bs = self.pre_bi_lstm(x)\n        # x shape == (batch_size, seq_len, enc_units*2)\n        # fh,fs,bh,bs shape == (batch_size, enc_units)\n        return x\n\n\n\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self,seq_len):\n        super(Attention, self).__init__()\n#         self.units = units\n        self.seq_len = seq_len\n        self.repeat = tf.keras.layers.RepeatVector(self.seq_len)\n        self.concat = tf.keras.layers.Concatenate(axis=-1)\n        self.d1 = tf.keras.layers.Dense(10,activation='tanh')\n        self.d2 = tf.keras.layers.Dense(1,activation='relu')\n        self.softmax = tf.keras.layers.Softmax(axis=1)\n        \n    def call(self, a, s_prev):\n        # a.shape == (batch_size, seq_len, enc_units*2)\n        # s_prev.shape == (batch_size, dec_units)\n        s_prev = self.repeat(s_prev)\n        # s_prev.shape == (batch_size, seq_len, dec_unit)\n        a_s = self.concat([a,s_prev])\n        # a_s.shape == (batch_size, seq_len, dec_units*2)\n        out = self.d1(a_s)\n        # out.shape == (batch_size, seq_len, dense units)\n        out = self.d2(out)\n        # out.shape == (batch_size, seq_len, dense units==1)\n        attention_score = self.softmax(out)\n        # attention_score.shape == out.shape\n        context_vector = attention_score * a\n        # context_vector.shape == (batch_size, seq_len, dec_units)\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        # context_vector.shape == (batch_size, dec_units) == decoder input\n        context_vector=tf.expand_dims(context_vector,1)\n        return context_vector, attention_score\n\n\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self,vocab_size, dec_units, batch_size):\n        super(Decoder, self).__init__()\n#         self.batch_size = batch_size\n        self.vocab_size = vocab_size\n        self.dec_units = dec_units\n        self.lstm = tf.keras.layers.LSTM(self.dec_units,return_state=True)\n        self.d1 = tf.keras.layers.Dense(self.vocab_size,activation='softmax')\n\n    def call(self,x,hidden):\n        h,c = hidden\n        # h,c shape == (batch_size, dec_units)\n        # x.shape == (batch_size, 1, dec_units)\n        output, new_h, new_c = self.lstm(x,initial_state=[h,c])\n        # 3 output shape == (batch_size, dec_units)\n        output = self.d1(output)\n        # output shape == (batch_size, vocab_size)\n        return output, new_h, new_c\n    \nclass AttentionMachineTranslationModel(tf.keras.Model):\n    def __init__(self,vocab_size, embedding_dim, enc_units, dec_units, batch_size, seq_len,end_token=2):\n        super(AttentionMachineTranslationModel, self).__init__()\n        self.end_token=end_token\n        self.encoder = Encoder(vocab_size,embedding_dim,enc_units,batch_size)\n        self.attention = Attention(seq_len)\n        self.decoder = Decoder(vocab_size,dec_units,batch_size)\n        self.enc_units = enc_units\n        self.dec_units = dec_units\n        \n    def call(self, x):\n        a = self.encoder(x)\n        predict = []\n        h = tf.zeros((x.shape[0],self.dec_units))\n        c = tf.zeros((x.shape[0],self.dec_units))\n        for t in range(x.shape[1]):\n            context, attention = self.attention(a,h)\n            output, h, c = self.decoder(context, [h,c])\n            predict.append(output)\n        return tf.stack(predict,axis=1)\n    \n    def predict(self, x):\n        a = self.encoder(x)\n        predict = []\n        attentions = []\n        h = tf.zeros((x.shape[0],self.dec_units))\n        c = tf.zeros((x.shape[0],self.dec_units))\n        for t in range(x.shape[1]):\n            context, attention = self.attention(a,h)\n            output, h, c = self.decoder(context, [h,c])\n            predict.append(output)\n            attentions.append(attention)\n        return [tf.stack(predict,axis=1),attentions]\n        \n    ","3e52f3bb":"model = AttentionMachineTranslationModel(2523,512,128,256,128,12,2)\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='acc')])\nhistory=model.fit(enc_input_seq[:-13],dec_target_seq[:-13],batch_size=128,epochs=300)","b0721cc4":"import seaborn as sns\n\ndef pred():\n    query = input()\n    query_list = [query for _ in range(128)]\n    # print(query_list)\n    query_seq = tk.texts_to_sequences(query_list)\n#     print(query_seq)\n    query_seq = pad_sequences(query_seq,maxlen=MAX_SEQ_LEN,padding='post')\n    pad_index = query_seq[0].tolist().index(0)\n    pred,attentions = model.predict(np.array(query_seq))\n    one_pred = pred.numpy()[0]\n    pred_index = one_pred.argmax(axis=-1)\n    for end_in,i in enumerate(pred_index):\n        if tk.index_word[i]=='<E>':\n            end_index = end_in\n            break\n        print(tk.index_word[i],end=' ')\n    att = []\n    for at in attentions:\n        att.append(at[0].numpy())\n    heatmap = att[0].copy()\n    for a in att[1:]:\n        heatmap=np.concatenate([heatmap,a],axis=1)\n    plt.figure(figsize=(16,8))\n    sns.heatmap(heatmap[:pad_index,:end_index],cmap='gray',yticklabels=tk.sequences_to_texts([query_seq[0]])[0].split()[:pad_index],\n                xticklabels=tk.sequences_to_texts([pred_index[:end_index]])[0].split())\n    \n    plt.show()\npred()","716d4f7b":"# Too small dataset -> Overfitting \n# 'i can think of' don't matter what question is, they just come after specific words\n# But \"there's\", \"nothing\" pay attention to 'there','something'\n# It can be inferred that 'there' and 'something' do not appear often in other sentences.\n# They don't pay lots of attention to \"i'm, not ..\" <- words that appear very often","652a7c05":"def print_random_qna():\n    import random\n    for _ in range(10):\n        r = random.randint(0,len(que)-1)\n        print([que[r],ans[r]])","dc00479c":"print_random_qna()","8eae78e9":"# EDA","66a3c020":"# MODELING"}}