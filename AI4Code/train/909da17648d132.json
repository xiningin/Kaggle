{"cell_type":{"1b588ce5":"code","6e89a0fb":"code","1701c2d1":"code","5198e8b7":"code","b91829b9":"code","31341626":"code","9c6df5eb":"code","152a5bed":"code","bd5cda1e":"code","44149d2e":"code","0d5734d8":"code","73d7f460":"code","50ba67ec":"code","b5587d10":"code","d5936dcb":"code","19079a1f":"code","82e8a17a":"markdown","2f03b404":"markdown","62bd1fdf":"markdown","6ba9166f":"markdown","fc63080d":"markdown","cbfdbb3c":"markdown","f894dbff":"markdown","c55dbd3a":"markdown","3c66c2f5":"markdown","46bd109e":"markdown","1ac037a7":"markdown","cc5d701d":"markdown"},"source":{"1b588ce5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.losses import categorical_crossentropy\nfrom sklearn.metrics import accuracy_score\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))","6e89a0fb":"data = pd.read_csv('..\/input\/fer2013.csv')\n#check data shape\ndata.shape","1701c2d1":"#preview first 5 row of data\ndata.head(5)","5198e8b7":"#check usage values\n#80% training, 10% validation and 10% test\ndata.Usage.value_counts()","b91829b9":"#check target labels\nemotion_map = {0: 'Angry', 1: 'Digust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\nemotion_counts = data['emotion'].value_counts(sort=False).reset_index()\nemotion_counts.columns = ['emotion', 'number']\nemotion_counts['emotion'] = emotion_counts['emotion'].map(emotion_map)\nemotion_counts","31341626":"# Plotting a bar graph of the class distributions\nplt.figure(figsize=(6,4))\nsns.barplot(emotion_counts.emotion, emotion_counts.number)\nplt.title('Class distribution')\nplt.ylabel('Number', fontsize=12)\nplt.xlabel('Emotions', fontsize=12)\nplt.show()","9c6df5eb":"def row2image(row):\n    pixels, emotion = row['pixels'], emotion_map[row['emotion']]\n    img = np.array(pixels.split())\n    img = img.reshape(48,48)\n    image = np.zeros((48,48,3))\n    image[:,:,0] = img\n    image[:,:,1] = img\n    image[:,:,2] = img\n    return np.array([image.astype(np.uint8), emotion])\n\nplt.figure(0, figsize=(16,10))\nfor i in range(1,8):\n    face = data[data['emotion'] == i-1].iloc[0]\n    img = row2image(face)\n    plt.subplot(2,4,i)\n    plt.imshow(img[0])\n    plt.title(img[1])\n\nplt.show()  ","152a5bed":"#split data into training, validation and test set\ndata_train = data[data['Usage']=='Training'].copy()\ndata_val   = data[data['Usage']=='PublicTest'].copy()\ndata_test  = data[data['Usage']=='PrivateTest'].copy()\nprint(\"train shape: {}, \\nvalidation shape: {}, \\ntest shape: {}\".format(data_train.shape, data_val.shape, data_test.shape))","bd5cda1e":"#initilize parameters\nnum_classes = 7 \nwidth, height = 48, 48\nnum_epochs = 50\nbatch_size = 64\nnum_features = 64","44149d2e":"# CRNO stands for Convert, Reshape, Normalize, One-hot encoding\n# (i) convert strings to lists of integers\n# (ii) reshape and normalise grayscale image with 255.0\n# (iii) one-hot encoding label, e.g. class 3 to [0,0,0,1,0,0,0]\n\ndef CRNO(df, dataName):\n    df['pixels'] = df['pixels'].apply(lambda pixel_sequence: [int(pixel) for pixel in pixel_sequence.split()])\n    data_X = np.array(df['pixels'].tolist(), dtype='float32').reshape(-1,width, height,1)\/255.0   \n    data_Y = to_categorical(df['emotion'], num_classes)  \n    print(dataName, \"_X shape: {}, \", dataName, \"_Y shape: {}\".format(data_X.shape, data_Y.shape))\n    return data_X, data_Y\n\n    \ntrain_X, train_Y = CRNO(data_train, \"train\") #training data\nval_X, val_Y     = CRNO(data_val, \"val\") #validation data\ntest_X, test_Y   = CRNO(data_test, \"test\") #test data","0d5734d8":"model = Sequential()\n\n#module 1\nmodel.add(Conv2D(2*2*num_features, kernel_size=(3, 3), input_shape=(width, height, 1), data_format='channels_last'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(2*2*num_features, kernel_size=(3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n#module 2\nmodel.add(Conv2D(2*num_features, kernel_size=(3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(2*num_features, kernel_size=(3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n#module 3\nmodel.add(Conv2D(num_features, kernel_size=(3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(num_features, kernel_size=(3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n#flatten\nmodel.add(Flatten())\n\n#dense 1\nmodel.add(Dense(2*2*2*num_features))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\n#dense 2\nmodel.add(Dense(2*2*num_features))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\n#dense 3\nmodel.add(Dense(2*num_features))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\n#output layer\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7), \n              metrics=['accuracy'])\n\nmodel.summary()","73d7f460":"# data generator\ndata_generator = ImageDataGenerator(\n                        featurewise_center=False,\n                        featurewise_std_normalization=False,\n                        rotation_range=10,\n                        width_shift_range=0.1,\n                        height_shift_range=0.1,\n                        zoom_range=.1,\n                        horizontal_flip=True)\n\n\nes = EarlyStopping(monitor='val_loss', patience = 10, mode = 'min', restore_best_weights=True)\n\nhistory = model.fit_generator(data_generator.flow(train_X, train_Y, batch_size),\n                                steps_per_epoch=len(train_X) \/ batch_size,\n                                epochs=num_epochs,\n                                verbose=2, \n                                callbacks = [es],\n                                validation_data=(val_X, val_Y))","50ba67ec":"fig, axes = plt.subplots(1,2, figsize=(18, 6))\n# Plot training & validation accuracy values\naxes[0].plot(history.history['acc'])\naxes[0].plot(history.history['val_acc'])\naxes[0].set_title('Model accuracy')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_xlabel('Epoch')\naxes[0].legend(['Train', 'Validation'], loc='upper left')\n\n# Plot training & validation loss values\naxes[1].plot(history.history['loss'])\naxes[1].plot(history.history['val_loss'])\naxes[1].set_title('Model loss')\naxes[1].set_ylabel('Loss')\naxes[1].set_xlabel('Epoch')\naxes[1].legend(['Train', 'Validation'], loc='upper left')\nplt.show()","b5587d10":"test_true = np.argmax(test_Y, axis=1)\ntest_pred = np.argmax(model.predict(test_X), axis=1)\nprint(\"CNN Model Accuracy on test set: {:.4f}\".format(accuracy_score(test_true, test_pred)))","d5936dcb":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    #else:\n        #print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    fig, ax = plt.subplots(figsize=(12,6))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","19079a1f":"# Plot normalized confusion matrix\nplot_confusion_matrix(test_true, test_pred, classes=emotion_labels, normalize=True, title='Normalized confusion matrix')\nplt.show()","82e8a17a":"## Visualize Training Performance","2f03b404":"----------------------------------------------------------------------------------\n## Pre-processing data\n#### Summary:\n1. Splitting dataset into 3 parts: train, validation, test\n1. Convert strings to lists of integers\n1. Reshape to 48x48 and normalise grayscale image with 255.0\n1. Perform one-hot encoding label, e.g. class 3 to [0,0,0,1,0,0,0]","62bd1fdf":"## Dataset Overview","6ba9166f":"## Evaluate Test Performance","fc63080d":"----------------------------------------------------------------------------------\n## Building CNN Model\n\n### CNN Architecture:  \n* Conv -> BN -> Activation -> Conv -> BN -> Activation -> MaxPooling \n* Conv -> BN -> Activation -> Conv -> BN -> Activation -> MaxPooling \n* Conv -> BN -> Activation -> Conv -> BN -> Activation -> MaxPooling \n* Flatten\n* Dense -> BN -> Activation\n* Dense -> BN -> Activation\n* Dense -> BN -> Activation\n* Output layer","cbfdbb3c":"---------------------------------------------------------------------------------------\n## Import libraries","f894dbff":"## Introduction\nFrom Kaggle open resource, we had **training** dataset, **public test** dataset (which is then used as validation dataset for our project), and further a **private test** dataset (same size with public test dataset and will be used as data for evaluating the prediction performance).\n\nImage set of 35,887 examples, with training-set : **80%** validation-set : **10%** test-set : **10%**.\n\n## Objectives\n(i) To apply Convolutional neural networks (CNN) for facial expression recognition.\n \n(ii) To correctly classify each facial image into one of the seven facial emotion categories: **anger**, **disgust**, **fear**, **happiness**, **sadness**, **surprise**, and **neutral**.\n","c55dbd3a":"## More Analysis using Confusion Matrix\n\nConfusion Matrix is applied and plotted to find out which emotion usually get confused with each other.","3c66c2f5":"---------------------------------------------------------------------------------------------------\n### Extra:\n\n### Batch Normalization\n\n#### (i) Apply Batch-Normalization before or after activation function?\n*Regarding this issue, it is still occasionally a topic of debate. However, in this project, we applied BN before ReLu as we are trying to follow the original BatchNorm paper. The following is the exact text from the paper...*\n    \n> We add the BN transform immediately before the nonlinearity, by normalizing x = Wu+ b. \nWe could have also normalized the layer inputs u, but since u is likely the output of another nonlinearity, \nthe shape of its distribution is likely to change during training, and constraining its first and second \nmoments would not eliminate the covariate shift. In contrast, Wu + b is more likely to have a symmetric, \nnon-sparse distribution, that is \u201cmore Gaussian\u201d (Hyv\u00a8arinen & Oja, 2000); normalizing it is likely to \nproduce activations with a stable distribution.\n\n\n#### (ii) Is it better to use DropOut together with Batch-Normalization?\n*Again, it is another common debate topic in DL community. In this project, we totally eliminate the use of dropout and focusing batch-normalization technique only. This is because, Batch normalization already offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.* \n\n> Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout\n\nMore details -> [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https:\/\/arxiv.org\/abs\/1502.03167).\n\n\n\n\n---------------------------------------------------------------------------------------------------\n","46bd109e":"Notice that the later two subplots share the same y-axis with the first subplot. \n\nThe size of **train**, **validation**, **test** are **80%**, **10%** and **10%**, respectively. \n\nThe exact number of each class of these datasets are written on top of their x-axis bar. ","1ac037a7":"-------------------------------------------------------------------\n\n**Future Work:**\n\n    (i) To further fine tuning model using grid_search, specifically:\n        a. Different optimizer such as Adam, RMSprop, Adagrad.\n        b. experimenting dropout with batch-normalization.\n        c. experimenting different dropout rates. \n\n    (ii) To collect more data and train the model with balance dataset.\n\n","cc5d701d":"#### Let's look at some images..."}}