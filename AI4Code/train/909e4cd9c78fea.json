{"cell_type":{"79e60540":"code","fbdb5732":"code","a0ab2442":"code","ab1aa610":"code","b34e61e6":"code","d4f3a39f":"code","79913cce":"code","0cbcbc86":"code","c55999a1":"code","8c39ac88":"code","46c3c619":"code","c1ba4dee":"code","59d3628b":"code","190ac6f1":"code","ecac1610":"code","e83500fb":"code","3c11cb5e":"code","537c77bb":"code","99d7ec8c":"code","9cecf607":"code","d8d65ae6":"code","954df942":"code","0db539c8":"code","cece704b":"code","d1ba88be":"code","9cabd759":"code","47f31149":"code","a7451bbb":"markdown","8bc621e7":"markdown"},"source":{"79e60540":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fbdb5732":"data = pd.read_csv('\/kaggle\/input\/all-census-data\/elementary_2015_16.csv')\ndata","a0ab2442":"!pip install dabl\nimport dabl\n!pip install missingno\nimport missingno as msno","ab1aa610":"india_raw = data[[\"TOTAL POULATION\",\"FEMALE LITERACY\", 'MALE LITERACY', 'STATE NAME', 'DISTRICT NAME', 'PERCENTAGE URBAN POPULATION', 'GROWTH RATE', 'SEX RATIO', 'OVERALL LITERACY', 'AREA (SQ. KM) (AREA SQKM)']]\nmsno.matrix(india_raw)","b34e61e6":"india_raw","d4f3a39f":"india = india_raw.dropna(axis = 0)\nmsno.matrix(india)","79913cce":"dabl.plot(X = india, target_col = 'FEMALE LITERACY')","0cbcbc86":"import sklearn as skl","c55999a1":"#Drop missing target and look at column names\nindia_sk = india_raw.dropna(axis = 0, subset = ['FEMALE LITERACY'])\nindia_sk.columns","8c39ac88":"msno.matrix(india_sk)","46c3c619":"#Assign an X and a y\nX_raw = india_sk.drop(columns = ['MALE LITERACY', 'FEMALE LITERACY', 'OVERALL LITERACY'])\ny = india_sk.set_index('DISTRICT NAME')['FEMALE LITERACY']","c1ba4dee":"X_raw","59d3628b":"print(len(np.unique(india_sk.get('DISTRICT NAME'))))\nprint(np.unique(india_sk.get('DISTRICT NAME')))","190ac6f1":"#There is a district name for every row, so we can drop that column (or set it to the index)\nX = X_raw.copy().set_index('DISTRICT NAME')","ecac1610":"print(len(np.unique(india_sk.get('STATE NAME'))))","e83500fb":"#Too many unique state names, so will use label encoding (as opposed to one hot encoding)\nlb = skl.preprocessing.LabelEncoder()\nX_numsonly = X.copy()\nX_numsonly['STATE NAME'] = lb.fit_transform(X_numsonly.get('STATE NAME'))\nX_numsonly","3c11cb5e":"def ProcessAndRegress (n_estimators, X, y):\n    \n    pipe = skl.pipeline.Pipeline(steps = [\n        ('Impute', skl.impute.SimpleImputer(strategy = 'median')),\n        ('Regress', skl.ensemble.RandomForestRegressor(n_estimators = n_estimators))\n    ])\n    \n    scores = -1 * skl.model_selection.cross_val_score(pipe, X, y, cv = 5, scoring='neg_mean_absolute_error')\n    \n    return scores.mean()","537c77bb":"maes_estimators = {\n    50:ProcessAndRegress(50,X_numsonly,y),\n    100:ProcessAndRegress(100,X_numsonly,y),\n    150:ProcessAndRegress(150,X_numsonly,y),\n    200:ProcessAndRegress(200,X_numsonly,y),\n    250:ProcessAndRegress(250,X_numsonly,y),\n    300:ProcessAndRegress(300,X_numsonly,y),\n    350:ProcessAndRegress(350,X_numsonly,y),\n    400:ProcessAndRegress(400,X_numsonly,y),\n    450:ProcessAndRegress(450,X_numsonly,y),\n    500:ProcessAndRegress(500,X_numsonly,y),\n    550:ProcessAndRegress(550,X_numsonly,y),\n    600:ProcessAndRegress(600,X_numsonly,y),\n    650:ProcessAndRegress(650,X_numsonly,y),\n    700:ProcessAndRegress(700,X_numsonly,y),\n    800:ProcessAndRegress(800,X_numsonly,y)\n}","99d7ec8c":"plt.plot(maes_estimators.keys(), maes_estimators.values())","9cecf607":"def leafnodes(max_leaf_nodes, X, y):\n    model = skl.pipeline.Pipeline(steps = [\n        ('Impute', skl.impute.SimpleImputer(strategy = 'median')),\n        ('Regress', skl.ensemble.RandomForestRegressor(n_estimators = 300, max_leaf_nodes = max_leaf_nodes))\n    ])\n    scores = -1 * skl.model_selection.cross_val_score(model, X, y, cv = 5, scoring='neg_mean_absolute_error')\n    return scores.mean()","d8d65ae6":"maes_nodes = {\n    2:leafnodes(2,X_numsonly,y),\n    3:leafnodes(3,X_numsonly,y),\n    4:leafnodes(4,X_numsonly,y),\n    5:leafnodes(5,X_numsonly,y),\n    6:leafnodes(6,X_numsonly,y),\n    7:leafnodes(7,X_numsonly,y),\n    8:leafnodes(8,X_numsonly,y),\n    9:leafnodes(9,X_numsonly,y),\n    10:leafnodes(10,X_numsonly,y),\n    11:leafnodes(11,X_numsonly,y),\n    12:leafnodes(12,X_numsonly,y),\n    13:leafnodes(13,X_numsonly,y),\n    14:leafnodes(14,X_numsonly,y),\n    15:leafnodes(15,X_numsonly,y),\n    20:leafnodes(20,X_numsonly,y),\n}","954df942":"plt.plot(maes_nodes.keys(), maes_nodes.values())","0db539c8":"#We can take 300 as the best number of estimators and 7 as the best max leaf nodes for our model. Then:\nfinal_model = skl.ensemble.RandomForestRegressor(n_estimators = 300, max_leaf_nodes = 7)\n\n#Last preprocessing:\nimputer = skl.impute.SimpleImputer(strategy='median')\nimputed_X = pd.DataFrame(imputer.fit_transform(X_numsonly))\n\nX_train, X_valid, y_train, y_valid = skl.model_selection.train_test_split(imputed_X, y, train_size = 0.8, test_size = 0.2)","cece704b":"#Fitting model, predicting values of y_valid and scoring model\nfinal_model.fit(X_train, y_train)\n\nskl.metrics.mean_absolute_error(y_valid, final_model.predict(X_valid))","d1ba88be":"#Appending predicted values to table with real values per each district\nresults_table = pd.DataFrame().assign(True_Female_Literacy = y_valid).assign(Predicted = final_model.predict(X_valid))\nresults_table","9cabd759":"#Check feature importances of model:\nfeature_importances = {}\ni = 0\nfor col in X.columns:\n    feature_importances[col] = (final_model.feature_importances_[i]*100).round(2)\n    i = i+1\n\nfeature_importances","47f31149":"#Score model using r^2\nr2_score = skl.metrics.r2_score(y_valid, final_model.predict(X_valid))\nr2_score","a7451bbb":"**Attempting to predict Female Literacy rates in different districts in India using some socio-economic data**\n> Most of the techniques used are based off of my understanding of the Kaggle Intermediate Machine Learning course.","8bc621e7":"**Now using SciKit Learn:**"}}