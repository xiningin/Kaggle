{"cell_type":{"3ea09907":"code","2d0f9bad":"code","c552ff89":"code","ed5b2aa9":"code","4413ab03":"code","7309d26e":"code","cb2260aa":"code","43b6e4cf":"code","a1f0045e":"code","ff3f06c7":"code","cc64649e":"code","d0373b78":"code","a7195a48":"code","c36995e8":"code","f8d3de39":"code","33ff25b8":"code","12657366":"code","ad755b4e":"markdown","006e39c2":"markdown","4506cd18":"markdown","62b736c4":"markdown","cb51e726":"markdown","569fe795":"markdown","437f635b":"markdown","cbce0a6f":"markdown","a74790aa":"markdown","df35aab3":"markdown","8141041e":"markdown"},"source":{"3ea09907":"# Setup notebook\nfrom pathlib import Path\nfrom learntools.time_series.style import *  # plot style settings\nfrom learntools.time_series.utils import plot_periodogram, seasonal_plot\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error\nfrom wordcloud import WordCloud\n\n\ncomp_dir = Path('..\/input\/store-sales-time-series-forecasting')\n\nholidays_events = pd.read_csv(\n    comp_dir \/ \"holidays_events.csv\",\n    dtype={\n        'type': 'category',\n        'locale': 'category',\n        'locale_name': 'category',\n        'description': 'category',\n        'transferred': 'bool',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nholidays_events = holidays_events.set_index('date').to_period('D')\n\nstore_sales = pd.read_csv(\n    comp_dir \/ 'train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\naverage_sales = (\n    store_sales\n    .groupby('date').mean()\n    .squeeze()\n    .loc['2017']\n)\n\noil = pd.read_csv(\n    comp_dir \/ 'oil.csv',\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\noil = oil.set_index('date').to_period('D')\n\n# National and regional holidays in the training set\nholidays = (\n    holidays_events\n    .query(\"locale in ['National', 'Regional']\")\n    .loc['2017':'2017-08-15', ['description']]\n    .assign(description=lambda x: x.description.cat.remove_unused_categories())\n)\n\nX_holidays = pd.get_dummies(holidays)","2d0f9bad":"y = average_sales.copy()\n\n# YOUR CODE HERE\nfourier = CalendarFourier(freq='M', order=4)\ndp = DeterministicProcess(\n    index=y.index,\n    constant=True,\n    order=1,\n    # YOUR CODE HERE\n    additional_terms=[fourier],\n    seasonal=True,\n    drop=True,\n)\nX = dp.in_sample()","c552ff89":"def get_LR_pred(X, y, model=None):\n    model = model if model else LinearRegression().fit(X, y)\n    \n    if len(y.shape) == 1:\n        return pd.Series(model.predict(X),index=X.index)\n    else:\n        return pd.DataFrame(model.predict(X), index=X.index, columns=y.columns)\n\n#y_pred = get_LR_pred(X, y)\n\ndef plot_pred(y, y_pred=None, index=None, targ_ax=None, \n              plot_title=None, ylabel=\"items sold\", \n              pred_label=\"Seasonal\", no_pp=False):\n    y = y[index] if index is not None else y\n        \n    pp = {} if no_pp else plot_params\n    targ_ax = y.plot(**pp, alpha=0.5, title=plot_title, ylabel=ylabel, ax=targ_ax)\n\n    if y_pred is not None:\n        targ_ax = plot_pred(y_pred, index=index, targ_ax=targ_ax, ylabel=None, no_pp=True)\n    \n    targ_ax.legend()\n    \n    return  targ_ax\n    \n#plot_pred(y, y_pred);\n    ","ed5b2aa9":"# Join to training data\nX2 = X.join(holidays.groupby('date').min(), on='date').fillna(0.0)\n\ny_pred = get_LR_pred(X2, y)  \nax = plot_pred(y, y_pred, plot_title=\"Average Sales\")","4413ab03":"wdw=14\noil_moving_sum = oil.loc['2017'].diff().rolling(\n    window=wdw,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=wdw\/\/2,  # choose about half the window size\n).sum()              # compute the mean (could also do median, std, min, max, ...)\n\nplot_pred(oil.loc['2017'], plot_title='Oil Price', ylabel='');\nplot_pred(oil_moving_sum, plot_title=f'Oil Trend', ylabel='trend');","7309d26e":"oiltrend = X.join(oil_moving_sum.groupby('date').min(), on='date').fillna(0.0)\noilraw = X.join(oil.loc['2017'], on='date').fillna(0.0)\n\nfig, axes =  plt.subplots(3,1, figsize=(11, 14))\n\ny_pred = get_LR_pred(X2, y)  \nmsle = mean_absolute_error(y, y_pred)\nplot_pred(y, y_pred, plot_title=f'Without Oil Trend - MAE: {msle:.2f}', targ_ax=axes[0]);\n\ny_pred = get_LR_pred(oiltrend, y)\nmsle = mean_absolute_error(y, y_pred)\nplot_pred(y, y_pred, plot_title=f'With Oil Trend - MAE: {msle:.2f}', targ_ax=axes[1]);\n\ny_pred = get_LR_pred(oilraw, y)\nmsle = mean_absolute_error(y, y_pred)\nplot_pred(y, y_pred, plot_title=f'With Oil Raw - MAE: {msle:.2f}', targ_ax=axes[2]);","cb2260aa":"promo = pd.read_csv(\n    comp_dir \/ 'train.csv',\n    usecols=['store_nbr', 'family', 'date', 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'onpromotion': 'float32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\n\npromo['date'] = promo.date.dt.to_period('D')\npromo = promo.set_index(['store_nbr', 'family', 'date']).sort_index()\naverage_promo = (\n    promo\n    .groupby('date').sum()\n    .squeeze()\n    .loc['2017']\n)\n\nwdw=7\npromo_ma = average_promo.loc['2017'].rolling(\n    window=wdw,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=wdw\/\/2,  # choose about half the window size\n).mean()              # compute the mean (could also do median, std, min, max, ...)\n\nplot_pred(average_promo, promo_ma, \n          plot_title=f\"{wdw}-Day Moving Average\", ylabel=\"items on promotion\");","43b6e4cf":"promotrend = X.join(promo_ma.groupby('date').min(), on='date').fillna(0.0)\npromoraw = X.join(average_promo, on='date').fillna(0.0)\n\nfig, axes =  plt.subplots(3,1, figsize=(11, 14))\n\ny_pred = get_LR_pred(X2, y)\nmsle = mean_absolute_error(y, y_pred)\nplot_pred(y, y_pred, plot_title=f'Without Promo Trend - MAE: {msle:.2f}', targ_ax=axes[0]);\n\ny_pred = get_LR_pred(promotrend, y) \nmsle = mean_absolute_error(y, y_pred)\nplot_pred(y, y_pred, plot_title=f'With Promotion Trend - MAE: {msle:.2f}', targ_ax=axes[1]);\n\ny_pred = get_LR_pred(promoraw, y)\nmsle = mean_absolute_error(y, y_pred)\nplot_pred(y, y_pred, plot_title=f'With Promotion Raw - MAE: {msle:.2f}', targ_ax=axes[2]);","a1f0045e":"y = store_sales.unstack(['store_nbr', 'family']).loc[\"2017\"]\ny_pred = get_LR_pred(promoraw, y) \nabse = (y - y_pred).abs() \/ y.mean().replace(0,1)\n\nfig, axes =  plt.subplots(2,1, figsize=(11, 8))\n\nplot_pred(y, y_pred, index=('sales', '1', 'PRODUCE'), targ_ax=axes[0],\n          plot_title=\"Sales for Store 1 and PRODUCE\");\n\nax = plot_pred(abse, index=('sales', '1', 'PRODUCE'), targ_ax=axes[1],\n               plot_title=\"Error Metric for Store 1 and PRODUCE\")\nax.set_ylim((-0.1,1));","ff3f06c7":"abse = (y - y_pred).abs() \/ y.mean().replace(0,1)\nabse_srt = abse.describe().T.sort_values(by='mean', ascending=False)\nabse_srt","cc64649e":"fig, axes = plt.subplots(5,1, figsize=(11,18))\ni = 0\nfor idx in abse_srt.nlargest(5, 'mean').index.values:\n    ax = plot_pred(y, y_pred, index=idx, targ_ax=axes[i], \n              plot_title=f'Sales for Store {idx[1]} and {idx[2]}')\n    #axes[i].set_ylim([-0.1,0.2])\n    i +=1","d0373b78":"eps = abse_srt.reset_index().groupby('store_nbr')['mean'].sum()\nax = eps.sort_values().plot(kind='bar',alpha=0.5, ylabel=\"\",\n                            title=\"Error per Store\")\nplt.xticks(rotation=0);","a7195a48":"top5 = eps.nlargest(5).index.values\nmin5 = eps.nsmallest(5).index.values\n\ntmp = abse_srt.reset_index()\nwc = []\nfor store_nbr in top5:\n    q = (tmp.store_nbr==store_nbr)\n    wc = wc+ [f.replace(' ', '_') for f in tmp[q].nlargest(5, 'mean').family.values]\n\nwordcloud = WordCloud(width=1400, height=400, #background_color='red',\n                      random_state=1, colormap='Reds', \n                      collocations=False).generate(' '.join(wc))# Plot\nplt.imshow(wordcloud);\nplt.axis(\"off\");\n","c36995e8":"wc = []\nfor store_nbr in min5:\n    q = (tmp.store_nbr==store_nbr)\n    wc = wc+ [f.replace(' ', '_') for f in tmp[q].nlargest(5, 'mean').family.values]\n\nwordcloud = WordCloud(width=1400, height=400, #background_color='red',\n                      random_state=1, colormap='Greens', \n                      collocations=False).generate(' '.join(wc))# Plot\nplt.imshow(wordcloud);\nplt.axis(\"off\");","f8d3de39":"top5_error_families = ['BABY CARE', 'BOOKS', 'HARDWARE', 'HOME APPLIANCES',  'SCHOOL AND OFFICE SUPPLIES']\nother = [f for f in tmp.family.unique() if f not in top5_error_families]\nstore_sales.loc[:,top5_error_families,:]\n\ntef = (\n    store_sales.loc[:,top5_error_families,:]\n    .groupby('date').mean()\n    .squeeze()\n    .loc['2017']\n)\n\noth = (\n    store_sales.loc[:,other,:]\n    .groupby('date').mean()\n    .squeeze()\n    .loc['2017']\n)\n\n#plot_periodogram(tef);","33ff25b8":"fourier = CalendarFourier(freq='Y', order=4)\ndp = DeterministicProcess(\n    index=tef.index,\n    constant=True,\n    order=1,\n    # YOUR CODE HERE\n    additional_terms=[fourier],\n    seasonal=True,\n    drop=True,\n)\nXtef = dp.in_sample()\ny_pred = get_LR_pred(Xtef, tef)  \nplot_pred(tef, y_pred, \n          plot_title=f\"Average Sales Top5 Error Product Families\");","12657366":"fourier = CalendarFourier(freq='M', order=4)\ndp = DeterministicProcess(\n    index=oth.index,\n    constant=True,\n    order=1,\n    # YOUR CODE HERE\n    additional_terms=[fourier],\n    seasonal=True,\n    drop=True,\n)\nXoth = dp.in_sample()\npromoraw = Xoth.join(average_promo, on='date').fillna(0.0)\ny_pred = get_LR_pred(promoraw, oth)  \nplot_pred(oth, y_pred, \n          plot_title=f\"Average Sales other\");","ad755b4e":"## Error Analysis\nThe previous models were trained on the average sales over all customers and all product families, in order to identify an all-over seasonal pattern for the store sales. In this section the models will predict the sales for each store and product family combination. Then the error is used to search for hints on how to improve allover performance.\n\nTo compare the error of product families the MAE is not sufficient. Families with high sales will tend to have higher MAE than families that only sell a few items. Therefore the MAE is divided by the average sales per store and product family combination.\n\nHere is the plot of a sample store product family combinations actual sales and predcites sales, followed by the error metric for this store.","006e39c2":" Both wordclouds have the following product families in commomn (in alphabetical order)\n  - BABY CARE\n  - BOOKS\n  - HARDWARE\n  - HOME APPLIANCES\n  - SCHOOL AND OFFICE SUPPLIES\n  \nThese seem to have a different seasonality patterns. Therefore the datastet was split into \"Top5 Error Product Families\" and \"Other\". And each got their own model.\n\n * Top 5 Error Product Families: [Fourier transforms](https:\/\/www.kaggle.com\/ryanholbrook\/seasonality#Fourier-Features-and-the-Periodogram) for a yearly pattern\n * Other: [Fourier transforms](https:\/\/www.kaggle.com\/ryanholbrook\/seasonality#Fourier-Features-and-the-Periodogram) for a monthly pattern","4506cd18":"### Oil Price and Oil Price Trend\nThe blue seasonal pattern in the chart above, follows the average stores sales very well. However, there are many occassions where the peaks and dips are over- or undershot. The amount of items on promotion, and the oil price could be a good predictor to stretch or squeeze the seasonal pattern.\n\n>Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.\n\nThe following charts allow to compare the daily oil price with the moving sum of the changes in the oil price. The oil price itself may not have a good predictive power. The perception of the current oil price depends on the previous prices. For instance in March an oil price of 50 was bad, since it dropped from 54. However in July it was great, since it rose from 44.\n\nIn order to visualize the trend, the oil data has been transformed. First the difference to the price of the previous day is calculated. Then the differences were summed up in a 14-day window. The following charts allow to compare the \"raw\" oil price and the resulting oil trend. In March when the price dropped from 54, the oil trend is below -4. In the beginning of August the same oil price has a trend value of around +2.","62b736c4":"In order to find out if the error is related to product families, the five product families with the largest error are extracted for every store. Both wordclouds show the product families that have been the hardest to predict. The red one shows the words of the stores with the largest error and the green one for the stores with the smallest error.","cb51e726":"The following charts compare the previous model with one model trained with the trend and another one with the raw oil price. The differences are very little, hence the mean absolute error (MAE) of each model is also given.","569fe795":"\n\nThe following table shows statistics for the calculated error, sorted by the mean. First we see the top five with the highest error. And at the bottom the five with the lowest error. The latter are probably product families that are not sold in the given store.","437f635b":"# Corporaci\u00f3n Favorita Store Sales Prediction\nThis notebook shows time-series forecasting for store sales on data from Corporaci\u00f3n Favorita, a large Ecuadorian-based grocery retailer.\n\nIt is divided into two sections\n- Feature Engineering and Comparison\n- Error Analysis\n\n\n## Feature Engineering and Comparison\nThis section is built on top of this [notebook](https:\/\/www.kaggle.com\/ryanholbrook\/exercise-seasonality), which provided a set of well performing seasonality features, as shown in the following chart.","cbce0a6f":"The following charts show the top five error combinations along with their actual sales and their predictions. We can see that we have some extrem cases here. These stores sell only 1 or 2 items within several months.","a74790aa":"The following charts compare the initial model with one model trained with the trend and another one with the unprocessed items on promotion. The vanilla model outperforms the other models on the MAE metric. However, it seems to be more accurate in August.","df35aab3":"### Items on Promotion\nThe following chart shows the average amount of promo items along with the trend. In oppose to the oil data, the trend is calculated by taking the moving average in a 7-day window.","8141041e":"\nIn the follwoing approach the error is summed up for every store, as shown in the following bar chart."}}