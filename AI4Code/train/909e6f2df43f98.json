{"cell_type":{"7386c327":"code","4d60724a":"code","4a626c19":"code","d227a61d":"code","5280cf67":"code","907992be":"code","d8a8b2c3":"code","b13ae69b":"code","613b5f3d":"code","3b88dcf4":"code","dfb0a6ab":"code","5d5a807a":"code","1d8ad2ac":"code","1b83673f":"code","6db4e5fd":"code","a8740c98":"code","e077a9f0":"code","7016a68a":"code","4af2f7f4":"code","9a7b393e":"code","a25f42b9":"code","2170fb7e":"code","9fe4b09e":"code","52fc9213":"code","6668a3e9":"code","d01fd2bb":"code","4f0278f0":"code","e0902997":"code","d476b9c2":"code","cd063047":"code","ef5be2cb":"code","504f8c59":"code","62e14311":"markdown","938bad12":"markdown","57a03ef6":"markdown","6f697a82":"markdown"},"source":{"7386c327":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d60724a":"import torch","4a626c19":"#printing a value in torch\nb = torch.tensor(1)\nprint(b)\nprint(type(b))","d227a61d":"#matrix initializtion \n\na = torch.zeros((5,5))\na","5280cf67":"#generating a random matrix\n\na = torch.randn((3,3))\nb = torch.randn((3,3))","907992be":"print(a,'\\n',b)","d8a8b2c3":"#basic opeations\n\n# matrix addition\nprint(torch.add(a,b), '\\n')\n\n# matrix subtraction\nprint(torch.sub(a,b), '\\n')\n\n# matrix multiplication\nprint(torch.mm(a,b), '\\n')\n\n# matrix division\nprint(torch.div(a,b))","b13ae69b":"#transpose of a matrix\n\ntp = torch.t(a)\nprint(a)\nprint('Transpose')\nprint(tp)","613b5f3d":"#concat\n\na = [1,3,5]\nb = [2,8,6]\n\ntens_1 = torch.tensor([a,b])\nprint(tens_1)\n\ntens_2 = torch.tensor([b,a])\nprint(tens_2)","3b88dcf4":"torch.cat((tens_1,tens_2))","dfb0a6ab":"#concat hoeizontally\n\ntorch.cat((tens_1,tens_2),dim=1)","5d5a807a":"#reshaping tensor\nreshape_tens1 = tens_1.reshape(3,2)\nreshape_tens1","1d8ad2ac":"#converting a numpy array to tensor\n\nnumpy_arr = np.array([[1,23,25],[23,52,10]])\nprint('numpy array \\n\\n',numpy_arr)\ntensor_arr = torch.tensor(numpy_arr)\nprint('\\n')\nprint('torch array \\n\\n', tensor_arr)","1b83673f":"#grad func\n\n\na = torch.ones((4,5),requires_grad=True)\na","6db4e5fd":"b = a +12\nc = b.mean()\nprint(b,c)","a8740c98":"#computing gradients\n\nc.backward()\nprint(a.grad)","e077a9f0":"# importing the optim module\nfrom torch import optim\n\n# adam\n## adam = optim.Adam(model.parameters(), lr=learning_rate)\n\n# sgd\n## SGD = optim.SGD(model.parameters(), lr=learning_rate)","7016a68a":"a = [2,3,5,6]\nb = [4,5,7,8]\nc = [1,2,3,4]\n\nx = torch.Tensor([a,b,c])\ny = torch.Tensor([[5],[10],[15]])","4af2f7f4":"x","9a7b393e":"y","a25f42b9":"#Sigmoid Function\ndef sigmoid (x):\n    val = 1\/(1 + torch.exp(-x))\n    return val\n\n#Derivative of Sigmoid Function\/\ndef derivatives_sigmoid(x):\n    derv_val = sigmoid(x) * (1 - sigmoid(x))\n    return derv_val","2170fb7e":"epoch=7000 #Setting training iterations\nlr=0.1 #Setting learning rate\ninputlayer_neurons = x.shape[1] #number of features in data set\nhiddenlayer_neurons = 3 #number of hidden layer neurons\noutput_neurons = 1 #number of neurons in output layer\n\n#weight and bias initialization\nwh=torch.randn(inputlayer_neurons, hiddenlayer_neurons).type(torch.FloatTensor)\nbh=torch.randn(1, hiddenlayer_neurons).type(torch.FloatTensor)\nwout=torch.randn(hiddenlayer_neurons, output_neurons)\nbout=torch.randn(1, output_neurons)","9fe4b09e":"for i in range(epoch):\n    #Forward Propogation\n    hidden_layer_input1 = torch.mm(x, wh)\n    hidden_layer_input = hidden_layer_input1 + bh\n    hidden_layer_activations = sigmoid(hidden_layer_input)\n\n    output_layer_input1 = torch.mm(hidden_layer_activations, wout)\n    output_layer_input = output_layer_input1 + bout\n    output = sigmoid(output_layer_input)\n\n    #Backpropagation\n    E = y-output\n    slope_output_layer = derivatives_sigmoid(output)\n    slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)\n    d_output = E * slope_output_layer\n    Error_at_hidden_layer = torch.mm(d_output, wout.t())\n    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n    wout += torch.mm(hidden_layer_activations.t(), d_output) *lr\n    bout += d_output.sum() *lr\n    wh += torch.mm(x.t(), d_hiddenlayer) *lr\n    bh += d_output.sum() *lr","52fc9213":"print('actual :\\n', y, '\\n')\nprint('predicted :\\n', output)","6668a3e9":"x_values = [i for i in range(11)]\nx_train = np.array(x_values, dtype=np.float32)\nx_train = x_train.reshape(-1, 1)\n\ny_values = [1,6,11,16,21,25,32,36,41,46,50]\ny_train = np.array(y_values, dtype=np.float32)\ny_train = y_train.reshape(-1, 1)","d01fd2bb":"x_train","4f0278f0":"y_train","e0902997":"import torch\nfrom torch.autograd import Variable\nclass linearRegression(torch.nn.Module):\n    def __init__(self, inputSize, outputSize):\n        super(linearRegression, self).__init__()\n        self.linear = torch.nn.Linear(inputSize, outputSize)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out","d476b9c2":"inputDim = 1        # takes variable 'x' \noutputDim = 1       # takes variable 'y'\nlearningRate = 0.01 \nepochs = 400\n\nmodel = linearRegression(inputDim, outputDim)\n##### For GPU #######\nif torch.cuda.is_available():\n    model.cuda()","cd063047":"criterion = torch.nn.MSELoss() \noptimizer = torch.optim.SGD(model.parameters(), lr=learningRate)","ef5be2cb":"for epoch in range(epochs):\n    # Converting inputs and labels to Variable\n    if torch.cuda.is_available():\n        inputs = Variable(torch.from_numpy(x_train).cuda())\n        labels = Variable(torch.from_numpy(y_train).cuda())\n    else:\n        inputs = Variable(torch.from_numpy(x_train))\n        labels = Variable(torch.from_numpy(y_train))\n\n    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n    optimizer.zero_grad()\n\n    # get output from the model, given the inputs\n    outputs = model(inputs)\n\n    # get loss for the predicted output\n    loss = criterion(outputs, labels)\n    print(loss)\n    # get gradients w.r.t to parameters\n    loss.backward()\n\n    # update parameters\n    optimizer.step()\n\n    print('epoch {}, loss {}'.format(epoch, loss.item()))","504f8c59":"import matplotlib.pyplot as plt\n\nwith torch.no_grad(): # we don't need gradients in the testing phase\n    if torch.cuda.is_available():\n        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n    else:\n        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n    print(predicted)\n\nplt.clf()\nplt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\nplt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\nplt.legend(loc='best')\nplt.show()","62e14311":"Practice neural network","938bad12":"![image.png](attachment:5464a5fb-e87d-4794-9f80-5843578137e6.png)","57a03ef6":"Simple linear regression in python","6f697a82":"Popular optimizers are given below"}}