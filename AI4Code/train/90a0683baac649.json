{"cell_type":{"1a14cda9":"code","aa252404":"code","0283b3da":"code","3c7be7eb":"code","520c06de":"code","40adb64b":"code","cf9b1f1e":"code","da37abe0":"code","cb93ba96":"code","7ba260a7":"code","a5fc2a03":"code","f7d18f67":"code","3112fe46":"code","890c46ee":"code","1053e015":"code","5d5de939":"code","83315535":"code","e1866410":"code","7b58443f":"code","96b16f77":"code","b6c6cb2a":"code","2313f8aa":"code","0b41210d":"code","2dbc1288":"code","3e14d2ca":"code","60d8871d":"code","463d013d":"code","e6de335d":"code","559efdef":"markdown","146c09df":"markdown","b3f03244":"markdown","327740df":"markdown","1f1f8b41":"markdown","9b7b1d3d":"markdown","eec91daf":"markdown","24350f2e":"markdown","c0144b49":"markdown","67607a57":"markdown","e1ef4f71":"markdown","9ec66660":"markdown","7c19ec49":"markdown","d22f5210":"markdown","005cb501":"markdown","26e2c6b1":"markdown","ac3cec05":"markdown"},"source":{"1a14cda9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport seaborn as sns\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# We dont Probably need the Gridlines. Do we? If yes comment this line\nsns.set(style=\"ticks\")\n\nflatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nflatui = sns.color_palette(flatui)\n","aa252404":"# https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\nimport scipy.stats as ss\nfrom collections import Counter\nimport math \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\nimport numpy as np","0283b3da":"player_df = pd.read_csv(\"..\/input\/data.csv\")","3c7be7eb":"numcols = ['Overall', 'Crossing','Finishing',  'ShortPassing',  'Dribbling','LongPassing', 'BallControl', 'Acceleration','SprintSpeed', 'Agility',  'Stamina','Volleys','FKAccuracy','Reactions','Balance','ShotPower','Strength','LongShots','Aggression','Interceptions']\ncatcols = ['Preferred Foot','Position','Body Type','Nationality','Weak Foot']","520c06de":"player_df = player_df[numcols+catcols]","40adb64b":"traindf = pd.concat([player_df[numcols], pd.get_dummies(player_df[catcols])],axis=1)\nfeatures = traindf.columns\n\ntraindf = traindf.dropna()","cf9b1f1e":"traindf = pd.DataFrame(traindf,columns=features)","da37abe0":"y = traindf['Overall']>=87\nX = traindf.copy()\ndel X['Overall']","cb93ba96":"X.head()","7ba260a7":"len(X.columns)","a5fc2a03":"feature_name = list(X.columns)\n# no of maximum features we need to select\nnum_feats=30","f7d18f67":"def cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\ncor_support, cor_feature = cor_selector(X, y,num_feats)\nprint(str(len(cor_feature)), 'selected features')","3112fe46":"cor_feature","890c46ee":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nchi_selector = SelectKBest(chi2, k=num_feats)\nchi_selector.fit(X_norm, y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","1053e015":"chi_feature","5d5de939":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(X_norm, y)","83315535":"rfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","e1866410":"rfe_feature","7b58443f":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\"), max_features=num_feats)\nembeded_lr_selector.fit(X_norm, y)","96b16f77":"embeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","b6c6cb2a":"embeded_lr_feature","2313f8aa":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\nembeded_rf_selector.fit(X, y)","0b41210d":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","2dbc1288":"embeded_rf_feature","3e14d2ca":"from sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\n\nlgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\nembeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\nembeded_lgb_selector.fit(X, y)","60d8871d":"embeded_lgb_support = embeded_lgb_selector.get_support()\nembeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)), 'selected features')","463d013d":"embeded_lgb_feature","e6de335d":"\npd.set_option('display.max_rows', None)\n# put all selection together\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df.head(num_feats)","559efdef":"# Set some Parameters","146c09df":"This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.\u00a0\nFor example, Lasso, and RF have their own feature selection methods. Lasso Regularizer forces a lot of feature weights to be zero.\u00a0\nHere we use Lasso to select variables.","b3f03244":"![](https:\/\/cdn-images-1.medium.com\/max\/2400\/1*Feid5O1I9KethU8WX45CTg.png)","327740df":"# 2. Chi-Square Features","1f1f8b41":"## Reading the Data and Preprocessing","9b7b1d3d":"We could also have used a LightGBM. Or an XGBoost object as long it has a feature_importances_ attribute.","eec91daf":"# 3. Recursive Feature Elimination","24350f2e":"# BONUS","c0144b49":"This is another filter-based method.\u00a0\nIn this method, we calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values.","67607a57":"This Kernel is an extension for one of my blogs on Feature selection. \n\nDo check it out: ","e1ef4f71":"\n# Conclusion\n\nFeature engineering and feature selection are critical parts of any machine learning project.\u00a0\nWe strive for accuracy in our models and one cannot get accuracy without revisiting these pieces again and again.\nIn this article, I tried to explain some of the most used feature selection techniques as well as my workflow when it comes to feature selection.\u00a0\nI also tried to provide some intuition into these methods but you should probably try to see more into it and try to incorporate these methods into your work.\nDo read my [post on feature engineering](https:\/\/medium.com\/r\/?url=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hitchhikers-guide-to-feature-extraction-b4c157e96631) too if you are interested.\n\n---\n\nIf you want to learn more about Data Science, I would like to call out this excellent [course by Andrew Ng](https:\/\/medium.com\/r\/?url=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fmachine-learning%3FranMID%3D40328%26ranEAID%3DlVarvwc5BD0%26ranSiteID%3DlVarvwc5BD0-btd7XBdF681VKxRe2H_Oyg%26siteID%3DlVarvwc5BD0-btd7XBdF681VKxRe2H_Oyg%26utm_content%3D2%26utm_medium%3Dpartners%26utm_source%3Dlinkshare%26utm_campaign%3DlVarvwc5BD0%26source%3Dpost_page---------------------------). This was the one that got me started. Do check it out.\n\n\n---\n\nI am going to be writing more of such posts in the future too. Follow me up at [**Medium**](https:\/\/medium.com\/@rahul_agarwal?source=post_page---------------------------) or Subscribe to my [**blog**](http:\/\/eepurl.com\/dbQnuX?source=post_page---------------------------) to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter [@mlwhiz](https:\/\/twitter.com\/MLWhiz?source=post_page---------------------------).","9ec66660":"# 4. Lasso: SelectFromModel","7c19ec49":"This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.\nWe can also use RandomForest to select features based on feature importance.\nWe calculate feature importance using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance.","d22f5210":"# 5. Tree-based: SelectFromModel","005cb501":"# 1. Pearson correlation","26e2c6b1":"This is a filter-based method.\u00a0\nWe check the absolute value of the Pearson's correlation between the target and numerical features in our dataset. We keep the top n features based on this criterion.","ac3cec05":"This is a wrapper based method. As I said before, wrapper methods consider the selection of a set of features as a search problem.\u00a0\nFrom sklearn Documentation:\n\n>>The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nAs you would have guessed we could use any estimator with the method. In this case, we use LogisticRegression and the RFE observes the coef_ attribute of the LogisticRegression object"}}