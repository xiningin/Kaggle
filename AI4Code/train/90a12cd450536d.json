{"cell_type":{"eaa03bf5":"code","2abcc660":"code","c7d9e209":"code","b75f2f02":"code","04e740bc":"code","24825076":"code","ed6942f2":"code","e2f5a4d1":"code","f1833db7":"code","cccb340c":"code","50a8f749":"code","4f556fe6":"code","497f217f":"code","38d08328":"code","df56208e":"code","60c2dbcb":"code","a8e6b9ba":"code","2765e001":"code","163580d7":"code","9c21c65e":"code","d3060c64":"code","84b9962d":"code","834ef377":"code","93e82b99":"code","497de096":"code","5bcfc398":"code","f4b92e62":"code","9162da62":"code","bbabea50":"code","033603be":"code","6c7bcb26":"code","c76b4a1b":"code","e043fa3d":"code","31a0bbd1":"code","0a650597":"code","e792d1b2":"markdown","c2ed142b":"markdown","d43a7cfb":"markdown","80f48ccf":"markdown","40428914":"markdown","f0618d42":"markdown","4bfaf781":"markdown","4d41a1cc":"markdown","ed7ed5a3":"markdown"},"source":{"eaa03bf5":"import numpy as np \nimport pandas as pd \nimport glob\n\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","2abcc660":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    \n    book_train_subset = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_{dataType}.parquet\/stock_id={stock_id}\/')\n    book_train_subset.sort_values(by=['time_id', 'seconds_in_bucket'])\n\n    book_train_subset['bas'] = (book_train_subset[['ask_price1', 'ask_price2']].min(axis = 1)\n                                \/ book_train_subset[['bid_price1', 'bid_price2']].max(axis = 1)\n                                - 1)                               \n\n    \n    book_train_subset['wap'] = (book_train_subset['bid_price1'] * book_train_subset['ask_size1'] +\n                            book_train_subset['ask_price1'] * book_train_subset['bid_size1']) \/ (\n                            book_train_subset['bid_size1']+ book_train_subset['ask_size1'])\n\n    book_train_subset['log_return'] = (book_train_subset.groupby(by = ['time_id'])['wap'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    \n    stock_stat = pd.merge(\n        book_train_subset.groupby(by = ['time_id'])['log_return'].agg(realized_volatility).reset_index(),\n        book_train_subset.groupby(by = ['time_id'], as_index = False)['bas'].mean(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat['stock_id'] = stock_id\n    \n    return stock_stat\n\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df\n\nfrom sklearn.metrics import r2_score\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\nrs = 69420","c7d9e209":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')","b75f2f02":"%%time\ntrain_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\ntrain_dataSet = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')","04e740bc":"x = gc.collect()","24825076":"y = train_dataSet['target'].values\nX = train_dataSet.drop(['stock_id', 'time_id', 'target'], axis = 1).values\n\nX.shape, y.shape","ed6942f2":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=rs, shuffle=False)","e2f5a4d1":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","f1833db7":"xgb = XGBRegressor(tree_method='gpu_hist', random_state = rs, n_jobs= - 1)\n\nlgbm = LGBMRegressor(device='gpu', random_state=rs)","cccb340c":"%%time\nxgb.fit(X_train, y_train)","50a8f749":"preds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds), 6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the naive XGBOOST prediction: R2 score: {R2}, RMSPE: {RMSPE}')","4f556fe6":"import optuna\nfrom optuna.samplers import TPESampler\n\ndef objective(trial, data=X, target=y):\n    \n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=rs, shuffle=False)\n    \n    param = {\n        'tree_method':'gpu_hist', \n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)}\n    \n    model = XGBRegressor(**param)\n    \n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n    model.fit(X_train ,y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n    \n    preds = model.predict(X_test)\n    \n    rmspe = rmspe(y_test, preds)\n    \n    return rmspe","497f217f":"study = optuna.create_study(sampler=TPESampler(), direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective, n_trials=1000, gc_after_trial=True)","38d08328":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","df56208e":"optuna.visualization.plot_optimization_history(study)","60c2dbcb":"optuna.visualization.plot_param_importances(study)","a8e6b9ba":"best_xgbparams = study.best_params\nbest_xgbparams","2765e001":"xgb = XGBRegressor(**best_xgbparams, tree_method='gpu_hist')","163580d7":"%%time\nxgb.fit(X_train ,y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n\npreds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds), 6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the naive Tuned XGB prediction: R2 score: {R2}, RMSPE: {RMSPE}')","9c21c65e":"%%time\nlgbm.fit(X_train, y_train)","d3060c64":"preds = lgbm.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds),6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds),6)\nprint(f'Performance of the naive LIGHTGBM prediction: R2 score: {R2}, RMSPE: {RMSPE}')","84b9962d":"def objective(trial):\n    \n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=rs, shuffle=False)\n    valid = [(X_test, y_test)]\n    \n    param = {\n        \"device\": \"gpu\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 500),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 4000),\n#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100000, 700000),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)}\n\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n    model = LGBMRegressor(**param)\n    \n    model.fit(X_train, y_train, eval_set=valid, verbose=False, callbacks=[pruning_callback], early_stopping_rounds=100)\n\n    preds = model.predict(X_test)\n    \n    rmspe = rmspe(y_test, preds)\n    return rmspe","834ef377":"study = optuna.create_study(sampler=TPESampler(), direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective, n_trials=1000, gc_after_trial=True)","93e82b99":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","497de096":"optuna.visualization.plot_optimization_history(study)","5bcfc398":"optuna.visualization.plot_param_importances(study)","f4b92e62":"best_lgbmparams = study.best_params\nbest_lgbmparams","9162da62":"lgbm = LGBMRegressor(**best_lgbmparams, device='gpu')","bbabea50":"%%time\nlgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, early_stopping_rounds=100)\n\npreds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds), 6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the Naive Tuned LIGHTGBM prediction: R2 score: {R2}, RMSPE: {RMSPE}')","033603be":"def_xgb = XGBRegressor(tree_method='gpu_hist', random_state = rs, n_jobs= - 1)\n\ndef_lgbm = LGBMRegressor(device='gpu', random_state=rs)","6c7bcb26":"from sklearn.ensemble import StackingRegressor\n\n\nestimators = [('def_xgb', def_xgb),\n              ('def_lgbm', def_lgbm),\n              ('tuned_xgb', xgb)]\n\nclf = StackingRegressor(estimators=estimators, final_estimator=lgbm, verbose=1)","c76b4a1b":"%%time\nclf.fit(X_train, y_train)","e043fa3d":"preds = clf.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds),6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the Naive STACK prediction: R2 score: {R2}, RMSPE: {RMSPE}')","31a0bbd1":"test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest_dataSet = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntest_dataSet = test_dataSet.drop(['stock_id', 'time_id'], axis = 1)\n\ny_pred = test_dataSet[['row_id']]\nX_test = test_dataSet.drop(['row_id'], axis = 1).fillna(0)","0a650597":"y_pred = y_pred.assign(target = clf.predict(X_test))\ny_pred.to_csv('submission.csv',index = False)","e792d1b2":"# Models","c2ed142b":"# XGB Optuna Tuning","d43a7cfb":"# XGBoost","80f48ccf":"# Import Libraries","40428914":"# Submission","f0618d42":"# LGBM Optuna","4bfaf781":"# Stacking Regressor","4d41a1cc":"# Helper Functions","ed7ed5a3":"# LightGBM"}}