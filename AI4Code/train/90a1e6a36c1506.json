{"cell_type":{"bb635640":"code","2c69461f":"code","7e2a9263":"code","a67b6cfa":"code","2f1c4209":"code","2047ef1c":"code","0513beb5":"code","ef57733d":"code","caf37056":"code","b3a4d58c":"code","c4257248":"markdown","1f6f84fd":"markdown","714cc775":"markdown","bfbab8cf":"markdown"},"source":{"bb635640":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport PIL\nimport os\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom keras.applications.vgg16 import VGG16","2c69461f":"def prep_img(image_path):\n    img = cv2.resize(cv2.imread(image_path), (224, 224))\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.expand_dims(tf.squeeze(tf.Variable([img])), axis=0)\n    return img","7e2a9263":"vgg = VGG16(include_top=True, weights='..\/input\/vgg16-weights\/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n\n# Finding out the name of all the layers in VGG16\nfor layers in vgg.layers:\n  print(f\"{layers.name} ---> {layers.output_shape}\")","a67b6cfa":"def gram_mat(tensor):\n    x,y,z,k = tensor.shape\n    tensor = tf.reshape(tensor, (y,z,k))\n    tensor = tf.reshape(tensor, [-1])\n    tensor = tf.reshape(tensor, (y*z,k))\n    gram_matrix = tf.linalg.matmul(tensor, tensor, transpose_a = True)\n    # gram_matrix = tf.expand_dims(tf.expand_dims(gram_matrix, axis=0), axis=0)\n    return gram_matrix\/(y*z)    ","2f1c4209":"def load_model():\n    vgg = VGG16(include_top=True, weights='..\/input\/vgg16-weights\/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n    vgg.trainable = False\n    content_layers = ['block4_conv2']\n    style_layers = [ 'block2_conv1' ,'block3_conv1', 'block4_conv1', 'block5_conv1']\n    content_output = vgg.get_layer(content_layers[0]).output \n    style_output = [vgg.get_layer(style_layer).output for style_layer in style_layers]\n    gram_style_output = [gram_mat(layer_output) for layer_output in style_output]\n    \n    model = keras.Model([vgg.input], [content_output, gram_style_output])\n    return model","2047ef1c":"def calc_loss(style_outputs, content_outputs, style_target, content_target):\n    #Initialize parememters\n    style_weight = 0.02\n    content_weight = 0.1\n    style_loss = 0\n    #Calc content loss\n    c_loss = (content_outputs - content_target)**2\n    content_loss = tf.reduce_mean(c_loss)\n    #Calc Style loss\n    for i in range(len(style_outputs)):\n        error = style_outputs[i] - style_target[i]\n        style_loss += tf.reduce_mean(error**2)\n        \n    #Calc Total loss\n    total_loss = content_weight*content_loss + style_weight*style_loss\n    return total_loss","0513beb5":"def train_model(image, itter, opt):\n    with tf.GradientTape() as tape:\n        #tape.watch(image)\n        output = vgg_model(image)\n        loss = calc_loss(output[1], output[0], style_target, content_target)\n    gradient = tape.gradient(loss, image)\n    new_img = opt.apply_gradients([(gradient, image)])\n    image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))\n   \n    \n    if itter % 100 == 0:\n        print (tf.reshape(image, (1,-1)))\n        tf.print(\"Loss = {}\".format(loss))","ef57733d":"  c_img_path = ('..\/input\/images\/Content_i.jpeg')\n  s_img_path = ('..\/input\/images\/Style_i.jpg')\n    \n  content_image = prep_img(c_img_path)\n  style_image = prep_img(s_img_path)\n\n  vgg_model = load_model()\n  opt = tf.optimizers.Adam(learning_rate=0.5, beta_1=0.99, epsilon=0.1)\n  content_target = vgg_model(np.array(content_image))[0]\n  style_target = vgg_model(np.array(style_image))[1]","caf37056":"  ## Run Model\n  itterations = 301\n  image = tf.Variable((content_image), dtype = tf.float32)\n  for i in range(itterations):\n    train_model((image), i, opt)","b3a4d58c":"  ## Show Image\n  tensor = image*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)>3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  #tensor = cv2.resize(tensor, (1016,925), interpolation = cv2.INTER_AREA)\n  tensor =  PIL.Image.fromarray(tensor)\n  tensor = cv2.cvtColor(np.array(tensor), cv2.COLOR_BGR2RGB)\n  plt.imshow(tensor)\n  plt.show()","c4257248":"## Gram Matrix","1f6f84fd":"# This Style Transfer algorithm is based on \"A Neural Algorithm of Artistic Style\" paper.\n\nPlease see paper link: https:\/\/arxiv.org\/pdf\/1508.06576.pdf\n","714cc775":"# Content Layers are defined as \"**block4_conv2**\"\n# Style Layers are defined as  \"**block2_conv1**\", \"**block3_conv1**\", \"**block4_conv1**\", \"**block5_conv1**\"\n\nIn the Original paper the team also included layer \"block1_conv1\" in the style-layers group but I have found \n\nthat using only those 4 layers (and excluding \"block1_conv1\") is better for photos in general since it reduces the distortion on a higher \n\npixel resolution - it has a more significant effect on detail complex images like human faces.\n","bfbab8cf":"Gram Matrix represents a correlation between different features in the style photo (pixels, colors, shapes).\n\nIn this spesific case the matrix is used to calculate the style layers loss function.\n\nThe Gram matrix is a product of a dot function of a matrix by it's transposed shape: \n\nGram = M * M.T\n\n"}}