{"cell_type":{"2189aa2c":"code","421c77e4":"code","bfe9caf4":"code","0fdf3f2d":"code","c6291e25":"code","9f7c7089":"code","3926e4cc":"code","9c371fc8":"code","6fb855dd":"code","301b295f":"code","c95e5fca":"code","d9c974f3":"code","3922334d":"code","25fb33ee":"code","488e5c98":"code","2a1b6a31":"code","e64d8460":"code","6e1d46af":"code","407e6369":"code","a4da797f":"code","5608dd63":"code","611c140b":"code","7ec03afb":"code","25b6054a":"code","3ea9e759":"code","cb90eb15":"code","e2ad6950":"code","cde2d553":"code","62107812":"code","520fa8b0":"code","440247b6":"code","2a9586b8":"code","697eb41e":"code","d212a0d1":"code","2f026133":"code","dbd4c2cb":"code","b96c1a8a":"code","076c41a8":"code","34c830ed":"code","340dbfd8":"code","ef5b8599":"code","7e613d2a":"code","d098b8d0":"code","f55e9744":"code","738e6178":"code","8a050843":"code","7cf2ef52":"code","3e950436":"code","753df6b8":"code","09c58b6c":"code","1f9f1316":"code","2f246729":"code","ed0acb38":"code","b9920087":"code","d68ccb72":"code","59644ef3":"code","60e77102":"code","624db43f":"code","cfe405bf":"code","e7a61f72":"code","8d9380a1":"code","214b412f":"code","34de9049":"code","09a3ff5e":"code","40d23925":"code","7601c9db":"code","8c0b04ca":"code","efdc6cea":"code","36148977":"markdown","f6069997":"markdown","b3d78794":"markdown","023c9698":"markdown","3069c7b4":"markdown","c61493ec":"markdown","5b6d1dfe":"markdown","cc4c384f":"markdown","32f4fcd1":"markdown","872f7c6c":"markdown","2f7d58c0":"markdown","07ae8578":"markdown","352b6dd0":"markdown","7682fd95":"markdown","c8c34e9f":"markdown","b2f7f94b":"markdown","564e2979":"markdown","ebfa5fe9":"markdown","36d30a3a":"markdown"},"source":{"2189aa2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","421c77e4":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nimport tensorflow as tf","bfe9caf4":"Train = pd.read_csv(\"..\/input\/dont-overfit-ii\/train.csv\")\nTest = pd.read_csv(\"..\/input\/dont-overfit-ii\/test.csv\")","0fdf3f2d":"Train.head()","c6291e25":"Test.head()","9f7c7089":"# Column names of the training data\nTrain.columns","3926e4cc":"# Column names of the test data\nTest.columns","9c371fc8":"# Print the information of the train data \nprint(\"Train data information:\")\nprint(Train.info(), end = '\\n\\n')\nprint(\"Shape:\", Train.shape, end = '\\n\\n\\n', sep = '\\t')\n\n\n# Print the information of the test data \nprint(\"Test data information:\")\nprint(Test.info(), end = '\\n\\n')\nprint(\"Shape:\", Test.shape, end = '\\n', sep = '\\t')\n","6fb855dd":"Train.describe()","301b295f":"Test.describe()","c95e5fca":"Train_Nulls = []\nfor col in Train.columns:\n    Train_Nulls.append(Train[col].isnull().sum())\n\nprint(Train_Nulls)","d9c974f3":"Test_Nulls = []\nfor col in Test.columns:\n    Test_Nulls.append(Test[col].isnull().sum())\n\nprint(Test_Nulls)","3922334d":"Train.duplicated().sum()","25fb33ee":"Test.duplicated().sum()","488e5c98":"# Specify the predictors\npredictors = list(Train.columns) \npredictors.remove('target')\nsns.pairplot(data = Train,x_vars=predictors[1:10], y_vars=predictors[1:10], hue = 'target', diag_kind = 'hist')","2a1b6a31":"# Regression plot\nsns.regplot(x='0',y='target',data = Train)","e64d8460":"def getCorr(x,y):\n    corr, _ = scipy.stats.pearsonr(x, y) \n    return corr","6e1d46af":"detailed_features=[]\nfor feature in range(300):\n    sns.regplot(x=str(feature),y='target',data = Train)\n    detailed_features.append({'feature':feature,'slope':getCorr(Train['target'],Train[str(feature)])})","407e6369":"def getSlope(df):\n    return abs(df['slope'])","a4da797f":"detailed_features.sort(key=getSlope,reverse=True)","5608dd63":"features_to_save=[]\nfor iteration in range(20):\n    features_to_save.append(detailed_features[iteration]['feature'])","611c140b":"def getDfProcessed(df_toProcess,features_to_save):\n    df_processed = pd.DataFrame() \n    for iteration in features_to_save:\n        feature = df_toProcess[str(iteration)].values\n        df_processed[str(iteration)] = feature \n    return df_processed","7ec03afb":"dftrain_processed = getDfProcessed(Train,features_to_save)","25b6054a":"dftest_processed = getDfProcessed(Test,features_to_save)","3ea9e759":"# Isolate the taeget column\nY = Train['target']\n\n# Extract the train features\nX = dftrain_processed.copy()\n# X.drop(['id', 'target'], axis = 1, inplace = True)","cb90eb15":"# Test.drop(['id'], axis = 1, inplace = True)","e2ad6950":"# Features scaling\nScaler = MinMaxScaler()\nX = pd.DataFrame(Scaler.fit_transform(X))\nTest_1 = dftest_processed.copy()\nTest_1 = pd.DataFrame(Scaler.fit_transform(Test_1))","cde2d553":"# [Train validation] splitting\nX_Train, X_Valid, Y_Train, Y_Valid = train_test_split(X,Y, train_size=0.8, test_size=0.2,random_state=0)","62107812":"# from sklearn.metrics import roc_auc_score\n# from sklearn.linear_model import SGDClassifier\n# from sklearn import tree\n# from sklearn.svm import SVC\n# from sklearn.neighbors import NearestCentroid\n# from sklearn.naive_bayes import GaussianNB\n# # # Intiate the model\n# # naivebayes = GaussianNB()\n# # # Fit the model\n# # naivebayes.fit(X_train, y_train)","520fa8b0":"# Models = [GaussianNB(), RandomForestClassifier(),LogisticRegression(),SGDClassifier(),tree.DecisionTreeClassifier(),NearestCentroid(),SVC()]\n\n# ModelNames = ['GaussianNB', 'RandomForestclassifier','LogisticRegression','SGDClassifier','DecisionTreeClassifier','NearestCentroid','SVC']\n# AUC = []\n# Model_AUC = {}\n\n# for model in Models:\n#     # Intialize the model\n#     Classifier = model\n#     # Fitting\n#     Classifier.fit(X_Train,Y_Train)\n# #     print(\"model_name : \",model, end = '\\n')\n# #     print(Classifier.get_params(), end = '\\n\\n\\n')\n    \n#     # Prediction\n#     Valid_Prediction = Classifier.predict(X_Valid)\n    \n#     # ROC\n#     auc = roc_auc_score(Y_Valid, Valid_Prediction)\n#     AUC.append(auc)\n\n\n# # Result    \n# Model_AUC = {'Classifier' : ModelNames, 'AUC': AUC}\n# Model_AUC_df = pd.DataFrame(Model_AUC)  \n# Model_AUC_df","440247b6":"# import the class\nfrom sklearn.linear_model import LogisticRegression\n\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression(solver='lbfgs', max_iter=1000)\n\n# fit the model with data\nlogreg.fit(X_Train, Y_Train)\n\n# Predict the test values\ny_pred = logreg.predict(X_Valid)","2a9586b8":"logreg.classes_","697eb41e":"# logreg.coef_","d212a0d1":"#### Model Evaluation\n# import the metrics class\nfrom sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(Y_Valid, y_pred))\nprint(\"Precision:\",metrics.precision_score(Y_Valid, y_pred))\nprint(\"Recall:\",metrics.recall_score(Y_Valid, y_pred))","2f026133":"# Let's call the confusion matrix method\ncnf_matrix = metrics.confusion_matrix(Y_Valid, y_pred)\ncnf_matrix","dbd4c2cb":"# Now, we will try to display it in a nicer way.\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","b96c1a8a":"y_pred_proba = logreg.predict_proba(X_Valid)[::,1]\nfpr, tpr, _ = metrics.roc_curve(Y_Valid,  y_pred_proba)\nauc = metrics.roc_auc_score(Y_Valid,  y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","076c41a8":"from sklearn.model_selection import cross_validate\ncv_results = cross_validate(logreg, X, Y, cv=4, return_train_score = True, scoring=('accuracy', 'precision', 'recall', 'f1'))\n# cv_results","34c830ed":"cv_df = pd.DataFrame(cv_results)\ncv_df","340dbfd8":"# Let's create our function that can change the threshold\ndef predict_by_threshold(model, test_set , threshold):\n    import numpy as np\n    preds = np.where(model.predict_proba(test_set)[:,1] > threshold, 1, 0)\n    return preds ","ef5b8599":"# Let's see the output if the threshold was 0.7\ny_pred_th = predict_by_threshold(logreg, X_Valid, 0.8) ","7e613d2a":"print(\"Accuracy:\",metrics.accuracy_score(Y_Valid, y_pred_th))\nprint(\"Precision:\",metrics.precision_score(Y_Valid, y_pred_th))\nprint(\"Recall:\",metrics.recall_score(Y_Valid, y_pred_th))","d098b8d0":"logreg_predictions = logreg.predict(Test_1)","f55e9744":"dfsubmission = pd.DataFrame() \ndfsubmission['id'] = Test['id'] \ndfsubmission['target'] = logreg_predictions","738e6178":"dfsubmission","8a050843":"dfsubmission.to_csv('submission.csv', index=False) ","7cf2ef52":"model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n                                    tf.keras.layers.Dense(7, activation=tf.nn.relu),  \n                                    tf.keras.layers.Dense(2, activation=tf.nn.softmax)])\nmodel.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\nmodel.fit(X, Y, epochs=10)","3e950436":"predictions_list = list(model.predict(Test_1))","753df6b8":"def getPredictions(prediction_list):\n    prediction_list_processced = []\n    for iteration in prediction_list :\n        prediction_list_processced.append(int(round(iteration[1])))\n    return prediction_list_processced","09c58b6c":"dfsubmission_2 = pd.DataFrame() \ndfsubmission_2['id'] = Test['id'] \ndfsubmission_2['target'] = getPredictions(predictions_list)","1f9f1316":"dfsubmission_2","2f246729":"dfsubmission_2.to_csv('submission_2.csv', index=False) ","ed0acb38":"# Import Naive Bayes classifier\nfrom sklearn.naive_bayes import GaussianNB\n# Intiate the model\nnaivebayes = GaussianNB()\n# Fit the model\nnaivebayes.fit(X_Train, Y_Train)\n","b9920087":"cv_results = pd.DataFrame(cross_validate(naivebayes, X, Y, cv=4, return_train_score = True, scoring=('accuracy', 'precision', 'recall', 'f1')))\ncv_results","d68ccb72":"# Adjust display presion\npd.set_option(\"display.precision\", 8)\n\n# Using of predict \nnaivebayes_predict = naivebayes.predict(X_Valid)\n# Using of predict_proba\nnaivebayes_predict_proba = naivebayes.predict_proba(X_Valid)\n# Generate a Dataframe\nresults = pd.DataFrame({'Actual': Y_Valid.tolist(), \n                      'Predicted Class': naivebayes_predict.tolist(), \n                      'Predicted Class Probapitiy': naivebayes_predict_proba[:, 1].tolist()})","59644ef3":"# Print the head of the Actual vs. Predicted DataFrame\nresults.head(10)","60e77102":"naivebayes_predictions = naivebayes.predict(Test_1)","624db43f":"dfsubmission_3 = pd.DataFrame() \ndfsubmission_3['id'] = Test['id'] \ndfsubmission_3['target'] = naivebayes_predictions","cfe405bf":"dfsubmission_3","e7a61f72":"dfsubmission_3.to_csv('submission_3_.csv', index=False) ","8d9380a1":"# Import the Model \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n\n# Intiate the model\ndtree = DecisionTreeClassifier()\n\n# Fitting the Model \ndtree.fit(X_Train, Y_Train)\n\n# Predict for testing\ndtree_predict = dtree.predict(X_Valid)\n\n# Print the accuracy\nprint('Decision Tree accuracy: ', metrics.accuracy_score(Y_Valid, dtree_predict))\n","214b412f":"# Let's call the confusion matrix method\ncnf_matrix = metrics.confusion_matrix(Y_Valid, dtree_predict)\n\n\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","34de9049":"cv_results = pd.DataFrame(cross_validate(dtree, X, Y, cv=4, return_train_score = True, scoring=('accuracy', 'precision', 'recall', 'f1')))\ncv_results","09a3ff5e":"dtree_predictions = dtree.predict(Test_1)","40d23925":"dfsubmission_4 = pd.DataFrame() \ndfsubmission_4['id'] = Test['id'] \ndfsubmission_4['target'] = dtree_predictions","7601c9db":"dfsubmission_4.to_csv('submission_4.csv', index=False) ","8c0b04ca":"# Import Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n# Intiate the model\nrandom_forset = RandomForestClassifier(n_estimators=100)\n# Fit the Model\nrandom_forset.fit(X_Train, Y_Train) \n# Predict for testing\nrandom_forset_predict = random_forset.predict(X_Valid)\n# Let's see the perforamce of our Model using the Cross Validation Method\ncv_results_random_forest = pd.DataFrame(cross_validate(random_forset, X, Y, cv=4, return_train_score = True, scoring=('accuracy', 'precision', 'recall', 'f1')))\n# View the results\ncv_results_random_forest.head()","efdc6cea":"# Import ExtraTrees Classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n# Intiate the model\nextra_tree = ExtraTreesClassifier(n_estimators=100)\n# Fit the Model\nextra_tree.fit(X_Train, Y_Train) \n# Predict for testing\nextra_tree_predict = extra_tree.predict(X_Valid)\n# Let's see the perforamce of our Model using the Cross Validation Method\ncv_results_extra_tree = pd.DataFrame(cross_validate(extra_tree, X, Y, cv=4, return_train_score = True, scoring=('accuracy', 'precision', 'recall', 'f1')))\n# View the results\ncv_results_extra_tree.head()","36148977":"## **EDA**","f6069997":"#### 4. DecisionTreeClassifier","b3d78794":"## **Reading the data**","023c9698":"#### **No duplicates** in both the train and the test data","3069c7b4":"#### Discovering the shape and the data types of the data","c61493ec":"## Grid search","5b6d1dfe":"#### Check the duplicates","cc4c384f":"## Feature selection","32f4fcd1":"#### Check the missing values","872f7c6c":"## **Models**","2f7d58c0":"#### **1.** LogisticRegression","07ae8578":"#### Summary statistics","352b6dd0":"#### 5. RandomForestClassifier","7682fd95":"#### **No null values** in both the train and the test data","c8c34e9f":"#### 6. ExtraTreesClassifier","b2f7f94b":"### Data preparation and splitting","564e2979":"#### 3. GaussianNB","ebfa5fe9":"## **Exploring the data**","36d30a3a":"#### 2. Neural Network"}}