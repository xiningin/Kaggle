{"cell_type":{"552b5761":"code","08792a88":"code","bbb127fe":"code","544b76e0":"code","2c7c88de":"code","a39ad8fe":"code","5d7924b9":"code","effcd6f3":"code","4b54dbb0":"code","fc7af46b":"code","7267108c":"code","7ffd8758":"code","22fd8772":"markdown"},"source":{"552b5761":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, SpatialDropout2D, Conv2D, MaxPooling2D, AveragePooling1D, Reshape\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n\nimport matplotlib.pyplot as plt\nprint(os.listdir('..\/input'))","08792a88":"x_train = pd.read_csv('..\/input\/train.csv')\nx_train.head()","bbb127fe":"# set up training data and labels\ndim_x = 28\ndim_y = 28\nbatch_size=32\n\n# read in data\/labels\nx_train.shape\ny_train = np.array(x_train['label'])\nx_train.drop('label', axis = 1, inplace = True)\nx_train = np.array(x_train.values)\n\nprint(\"data shapes\", x_train.shape, y_train.shape, \"classes: \",len(np.unique(y_train)))\n\nclasses = len(np.unique(y_train))\nx_train = x_train.reshape((-1, dim_x,dim_y,1))\n# convert labels to one-hot\nprint(np.unique(y_train))\ny = np.zeros((np.shape(y_train)[0],len(np.unique(y_train))))\n\n# convert index labels to one-hot\nfor ii in range(len(y_train)):\n    #print(y_train[ii])\n    y[ii,y_train[ii]] = 1\ny_train = y","544b76e0":"# split into training\/validation\nno_validation = int(0.1 * (x_train.shape[0]))\n\nx_val = x_train[0:no_validation,...]\ny_val = y_train[0:no_validation,...]\n\nx_train = x_train[no_validation:,...]\ny_train = y_train[no_validation:,...]\n\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n\n# define image generators with mild augmentation\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\\\n                                   rotation_range=30,\\\n                                   width_shift_range=0.025,\\\n                                   height_shift_range=0.025,\\\n                                   shear_range=0.35,\\\n                                   zoom_range=0.075)\n\ntrain_generator = train_datagen.flow(x=x_train,\\\n                                     y=y_train,\\\n                                     batch_size=batch_size,\\\n                                     shuffle=True)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\nval_generator = test_datagen.flow(x=x_val,\\\n                                    y=y_val,\\\n                                    batch_size=batch_size,\\\n                                    shuffle=True)","2c7c88de":"# define model AlexNet (but topless)\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=96, kernel_size=(5,5), strides=1,input_shape=(dim_x,dim_y,1), activation=tf.nn.relu))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=256, kernel_size=(5,5), strides=1, activation=tf.nn.relu))\n#model.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=1, activation=tf.nn.relu))\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=1, activation=tf.nn.relu))\nmodel.add(SpatialDropout2D(rate=0.67))\nmodel.add(Conv2D(filters=250, kernel_size=(3,3), strides=1, activation=tf.nn.relu))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Flatten())\nmodel.add(Reshape((250,1)))\nmodel.add(AveragePooling1D(pool_size=25,strides=25))\nmodel.add(Reshape(([10])))\nmodel.add(Activation(tf.nn.softmax))\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])","a39ad8fe":"def learning_schedule(epoch):\n    if epoch <= 1:\n        lr = 3e-4\n    elif epoch <= 10:\n        lr = 1e-5\n    elif epoch <= 50:\n        lr = 3e-6\n    elif epoch <= 150:\n        lr = 1e-6\n    else:\n        lr = 1e-8\n    return lr\n\n# callbacks\nlrate = LearningRateScheduler(learning_schedule)\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=600, verbose=1, mode='auto')","5d7924b9":"steps_per_epoch = int(len(y_train)\/batch_size)\n\nmax_epochs = 4096\n\nhistory = model.fit_generator(generator=train_generator,\\\n                                steps_per_epoch=steps_per_epoch,\\\n                                validation_data=val_generator,\\\n                                validation_steps=50,\\\n                                epochs=max_epochs,\\\n                                callbacks=[early, lrate],\\\n                                verbose=2)","effcd6f3":"plt.figure(figsize=(15,12))\nplt.subplot(211)\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title(\"Accuracy and Loss\",fontsize=28)\nplt.ylabel('accuracy',fontsize=24)\nplt.legend(['Train','Val'],fontsize=18)\n\nplt.subplot(212)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epoch',fontsize=24)\nplt.ylabel('loss',fontsize=24)\nplt.legend(['Train','Val'],fontsize=18)\nplt.show()","4b54dbb0":"x_test = pd.read_csv('..\/input\/test.csv')\nx_test.head()\n\nx_test = np.array(x_test.values)\nx_test = x_test \/ 255.\n\nprint(\"data shape\", x_test.shape)\n\nx_test = x_test.reshape((-1, dim_x,dim_y,1))\n","fc7af46b":"# predict!\ny_pred = model.predict(x_test)","7267108c":"# visualize success (?) :\/\n\ndef imshow_w_labels(img,  pred,count):\n    plt.subplot(1,4,count+1)\n    plt.imshow(img, cmap=\"gray\")\n    plt.title(\"Prediction: %i, \"%(pred), fontsize=24)\n    \n    \ncount = 0\nmask = [1,3,3,7]\nplt.figure(figsize=(24,6))\nfor kk in range(50,600):\n    \n    if y_pred[kk,:].argmax() == mask[count]:\n        imshow_w_labels(x_test[kk,:,:,0],y_pred[kk,...].argmax(), count)\n        count += 1\n    if count >= 4: break\nplt.show()","7ffd8758":"\n# convert one-hot predictions to indices\nresults = np.argmax(y_pred,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","22fd8772":"# Topless AlexNet \nThe classic AlexNet (famous for kicking off the deep AI summer by winning the 2012 ImageNet competition) canonically has 5 convolutional followed by 3 fully connected layers. Those 3 dense layer on top may actually be more of a fashionable legacy than a necessary architectural feature, and we can often get away with average pooling instead of using any dense layers.  \n\nhttps:\/\/papers.nips.cc\/paper\/4824-imagenet-classification-with-deep-convolutional-neural-networks\nhttps:\/\/karpathy.github.io\/2019\/04\/25\/recipe\/ - Karpathy mentions the topless trend in conv-nets under the regularization tips section"}}