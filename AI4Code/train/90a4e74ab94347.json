{"cell_type":{"ab834d3f":"code","c47cf2b4":"code","17b8a678":"code","fb756f93":"code","03eae6bc":"code","2f8b6d2d":"code","7f320422":"code","b6c4777d":"code","05997d15":"code","cd39bdfd":"code","61a47797":"code","61c651b4":"code","b4680a9c":"code","f9ab44a5":"code","afc87541":"code","53050e35":"markdown","244cd847":"markdown","c6ec266d":"markdown","02cc578a":"markdown"},"source":{"ab834d3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# Split test data\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential  # Model type to be used\nfrom keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model\nfrom keras.utils import np_utils                         # NumPy related tools\nfrom keras.callbacks import LearningRateScheduler\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\nfrom tensorflow_addons.optimizers import AdamW\nfrom matplotlib import pyplot as plt # Visual\n\nseed=64209","c47cf2b4":"tf.test.gpu_device_name()# Is gpu working","17b8a678":"inputDims = (28, 28, 10)# 28x28 images with 10 classes\n\n# Input: uri -- csv location\n#        dims -- tuple of format (width, height, numClasses) \n#        test -- boolean: if test, uri was the test url, otherwise false\n# Output: X - Numpy tensors\n#         Y - Labels\ndef processCSV(uri, dims, test):\n    df = pd.read_csv(uri)\n    if test:\n        return(df, df.values.reshape(-1, dims[0], dims[1], 1) \/ 255.0)\n    else: \n        return(df, df.drop(['label'],axis=1).values.reshape(-1, dims[0], dims[1], 1) \/ 255.0, tf.keras.utils.to_categorical(df['label'].values, num_classes=dims[2]))\n\n# submission_df = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","fb756f93":"# Import data\ntrain_df, x_train, y_train = processCSV('..\/input\/digit-recognizer\/train.csv', inputDims, False)\ntest_df, x_test = processCSV('..\/input\/digit-recognizer\/test.csv', inputDims, True)\n\n\n# Show that my resizing works :D\nplt.imshow(x_train[3], cmap='gray')\nplt.show()","03eae6bc":"# Create CSVs for Case 2\n\n# Case 2: split half the data out\nx_train_flat = x_train.reshape(-1,inputDims[0]*inputDims[1])\nx2_train, x2_test, y2_train, y2_test = train_test_split(x_train_flat, y_train, test_size=0.5, random_state=seed)\n\ny2_train = np.argmax(y2_train, axis=1)\ny2_test = np.argmax(y2_test, axis=1)","2f8b6d2d":"#x2_train\nview = x2_train.reshape(-1, inputDims[0], inputDims[1], 1)\n# Show that my resizing works :D\nplt.imshow(view[3], cmap='gray')\nplt.show()\n\n#train_df.columns.values\ndf_train2 = pd.DataFrame(np.hstack((y2_train.reshape(-1, 1),x2_train)), index=None, columns=train_df.columns.values)\ndf_test2 = pd.DataFrame(np.hstack((y2_test.reshape(-1, 1),x2_test)), index=None, columns=train_df.columns.values)\n\n#train_df\ndf_train2.to_csv('train_halfImages.csv',  index = None)\ndf_test2.to_csv('test_halfImages.csv',  index = None)\n\n# Back to tf\/keras format\nx2_train = x2_train.reshape(-1, inputDims[0], inputDims[1], 1)\nx2_test = x2_test.reshape(-1, inputDims[0], inputDims[1], 1)\ny2_train = tf.keras.utils.to_categorical(y2_train, num_classes=inputDims[2])\ny2_test = tf.keras.utils.to_categorical(y2_test, num_classes=inputDims[2])","7f320422":"# Part 3\n# Mask out bad values\ny3 =  np.argmax(y_train, axis=1)\nmask = y3 < 5\nx3_train = x_train_flat[mask]\ny3_train = y3[mask]\n\nx3_test = x_train_flat[~mask]\ny3_test = y3[~mask]\n\ndf_train3 = pd.DataFrame(np.hstack((y3_train.reshape(-1, 1),x3_train)), index=None, columns=train_df.columns.values)\ndf_test3 = pd.DataFrame(np.hstack((y3_test.reshape(-1, 1),x3_test)), index=None, columns=train_df.columns.values)\n\ndf_train3.to_csv('train_halfDigits.csv',  index = None)\ndf_test3.to_csv('test_halfDigits.csv',  index = None)\n\n# Reparse back into tf\/keras compatible format\n\nx3_train = x3_train.reshape(-1, inputDims[0], inputDims[1], 1)\nx3_test = x3_test.reshape(-1, inputDims[0], inputDims[1], 1)\ny3_train = tf.keras.utils.to_categorical(y3_train, num_classes=inputDims[2])\ny3_test = tf.keras.utils.to_categorical(y3_test, num_classes=inputDims[2])","b6c4777d":"inputSize = inputDims[0]*inputDims[1]\nlayerSizes = [784, 627, 484, 256, 64]\noutputSize = inputDims[2]\ndropout = .3\n\ndef makeModel():\n    # NN model\n    model = Sequential()\n    rng = initializer=tf.keras.initializers.GlorotNormal(seed=seed)\n    # CNN Layers \n    model.add(Conv2D(16, kernel_size=(3,3), activation='relu', input_shape=(inputDims[0], inputDims[1], 1), kernel_initializer=rng))\n    #model.add(MaxPooling2D(pool_size=(3, 3)))\n    #model.add(Dropout(dropout))\n    model.add(Conv2D(32, kernel_size=(3,3), activation='relu', kernel_initializer=rng))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n    model.add(Conv2D(64, kernel_size=(5,5), activation='relu', kernel_initializer=rng))\n    model.add(Conv2D(64, kernel_size=(5,5), activation='relu', kernel_initializer=rng))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n    model.add(Dropout(dropout))\n    model.add(Flatten())\n\n\n    '''layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%\n    layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n    layer_dropout(rate = 0.25) %>%\n    '''\n    model.add(Dense(layerSizes[0], kernel_initializer=rng), ) #(518,) is not a typo -- that represents a 784 length vector\n    model.add(Activation('relu'))\n    # Middle layers\n    for layerSz in layerSizes[1:]:\n        model.add(Dense(layerSz, kernel_initializer=rng))\n        model.add(Activation('relu'))\n        model.add(Dropout(dropout))\n\n    # Output Layer \n    model.add(Dense(units=outputSize, activation='softmax', kernel_initializer=rng))\n\n    # Summary\n    model.summary()\n    return model\nmodel = makeModel()","05997d15":"# HyperParams for training\ninitial_learning_rate = 1e-3\nmaxEpochs = 20\n\nschedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n        [500], [1e-0, 1e-1])\n\nstep = tf.Variable(0, trainable=False)\n#optimizer = AdamW(learning_rate=initial_learning_rate*schedule(step), weight_decay=1e-1*schedule(step), beta_1=0.9, beta_2=0.98)\noptimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate*schedule(step))\n","cd39bdfd":"'''\nimport math \n\n# Dynamic step size\ndef lr_step_decay(epoch, lr):\n    drop_rate = 0.5\n    epochs_drop = 10.0\n    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch\/epochs_drop))\n    '''\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n\nhistory = model.fit(x_train, y_train, batch_size=128, epochs=maxEpochs, validation_split=0.1, verbose=1)\n\n","61a47797":"plt.plot(history.history['accuracy'])\nplt.title('Model 1 accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","61c651b4":"results = model.predict(x_test).argmax(axis=1)\n#results\n#plt.imshow(x_test[1], cmap='gray')\n#plt.show()\n","b4680a9c":"ImageID = np.arange(len(results))+1\nOut = pd.DataFrame([ImageID,results]).T\nOut.rename(columns = {0:'ImageId', 1:'Label'})\nOut.to_csv('submission.csv', header =  ['ImageId', 'Label' ], index = None)","f9ab44a5":"model = makeModel()\n\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\nhistory = model.fit(x2_train, y2_train,\n          batch_size=128, epochs=maxEpochs,\n          verbose=1,\n          validation_data=(x2_test, y2_test)\n          )#callbacks=[LearningRateScheduler(lr_step_decay, verbose=1)])\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model 2 accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n","afc87541":"model = makeModel()\n\n\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\nhistory = model.fit(x3_train, y3_train,\n          batch_size=128, epochs=maxEpochs,\n          verbose=1,\n          validation_data=(x3_test, y3_test)\n          )#callbacks=[LearningRateScheduler(lr_step_decay, verbose=1)])\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model 3 accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n","53050e35":"# Importing of data and preprocessing\n\n","244cd847":"# Output data for kaggle","c6ec266d":"# Part 2","02cc578a":"# Part 3"}}