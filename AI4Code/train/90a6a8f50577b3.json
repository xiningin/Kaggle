{"cell_type":{"1d143428":"code","9f506f71":"code","a1e26d1f":"code","d1a286af":"markdown","91926a66":"markdown","8d257046":"markdown","6bb1ecf9":"markdown"},"source":{"1d143428":"\"\"\"\nProject:    Classification of Sloan Digital Sky Survey (SDSS) Objects\nPurpose:    Data Exploration\n\n@author:    Kevin Trinh\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef pieChart(sdss_df):\n    '''Plot a pie chart for label count.'''\n    label_counts = sdss_df['class'].value_counts()\n    colors = ['skyblue', 'red', 'gold']\n    fig1, ax1 = plt.subplots()\n    ax1.pie(label_counts, labels=['Galaxy', 'Stars', 'Quasars'],\n            autopct='%1.2f%%', startangle=45, colors=colors)\n    ax1.axis('equal')\n    plt.title('SDSS Object Classes')\n    plt.show()\n\ndef distribution(sdss_df, axes, feature, row):\n    '''Plot the distribution of a space object w.r.t. a given feature.'''\n    labels = np.unique(sdss_df['class'])\n    colors = ['skyblue', 'gold', 'red']\n    for i in range(len(labels)):\n        label = labels[i]\n        ax = sns.distplot(sdss_df.loc[sdss_df['class']==label, feature], \n                          kde=False, bins=30, ax=axes[row, i], color=colors[i])\n        ax.set_title(label)\n        if (i == 0):\n            ax.set(ylabel='Count')\n            \ndef equitorial(sdss_df, row):\n    '''Plot equitorial coordinates of observations.'''\n    labels = np.unique(sdss_df['class'])\n    colors = ['skyblue', 'gold', 'red']\n    label = labels[row]\n    sns.lmplot(x='ra', y='dec', data=sdss_df.loc[sdss_df['class']==label],\n               hue='class', palette=[colors[row]], scatter_kws={'s': 2}, \n               fit_reg=False, height=4, aspect=2)\n    plt.ylabel('dec')\n    plt.title('Equitorial coordinates')\n    \n\ndef main():\n\n    # read in SDSS data\n    filepath = '..\/input\/sloan-digital-sky-survey-dr16\/Skyserver_12_30_2019 4_49_58 PM.csv'\n    sdss_df = pd.read_csv(filepath, encoding='utf-8')\n\n    # define lists of relevant features\n    geo = ['ra', 'dec']\n    nonugriv = ['redshift', 'plate', 'mjd', 'fiberid']\n    ugriv = ['u', 'g', 'r', 'i', 'z']\n\n    # plot pie chart of label count\n    pieChart(sdss_df)\n\n    # plot equitorial coordinates of observations\n    for row in range(3):\n        equitorial(sdss_df, row)\n        plt.show()\n    \n    # plot the distribution of non-geo and non-ugriv features\n    fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(12, 14))\n    plt.subplots_adjust(wspace=.4, hspace=.4)\n    for row in range(len(nonugriv)):\n        feat = nonugriv[row]\n        distribution(sdss_df, axes, feat, row)\n    plt.show()\n        \n    # plot the distribution of ugriv features\n    fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(12, 15))\n    plt.subplots_adjust(wspace=.4, hspace=.4)\n    for row in range(len(ugriv)):\n        feat = ugriv[row]\n        distribution(sdss_df, axes, feat, row)\n    plt.show()\n\nmain()","9f506f71":"\"\"\"\nProject:    Classification of Sloan Digital Sky Survey (SDSS) Objects\nPhase:      Feature engineering and ML classification\n\nAlgorithm:  Random Forest\n\nSteps:      1) Import libraries\n            2) Read, shuffle, and partition data\n            3) Restructure data as inputs for DNN\n            4) Feature Engineering\n            5) Create and train DNN\n            6) Make predictions on validation sets\n            7) Fine-tune models for highest performance on validation set\n            8) Make predictions on test set\n            9) Evaluate model with confusion matrix\n\n@author:    Kevin Trinh\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n\n# read in and shuffle SDSS data\nfilename = '..\/input\/sloan-digital-sky-survey-dr16\/Skyserver_12_30_2019 4_49_58 PM.csv'\nsdss_df = pd.read_csv(filename, encoding='utf-8')\nsdss_df = sdss_df.sample(frac=1)\n\n# drop physically insignificant columns\nsdss_df = sdss_df.drop(['objid', 'specobjid', 'run', 'rerun', 'camcol',\n                        'field'], axis=1)\n\n\n# partition SDSS data (60% train, 20% validation, 20% test)\ntrain_count = 60000\nval_count = 20000\ntest_count = 20000\n\ntrain_df = sdss_df.iloc[:train_count]\nvalidation_df = sdss_df.iloc[train_count:train_count+val_count]\ntest_df = sdss_df.iloc[-test_count:]\n\n\n# obtain feature dataframes\nX_train = train_df.drop(['class'], axis=1)\nX_validation = validation_df.drop(['class'], axis=1)\nX_test = test_df.drop(['class'], axis=1)\n\n\n# encode labels as arbitrary integer classes\nle = LabelEncoder()\nlabels = le.fit_transform(sdss_df['class'])\n\ny_train = labels[:train_count]\ny_validation = labels[train_count:train_count+val_count]\ny_test = labels[-test_count:]\n\n# scale features\nscaler = StandardScaler()\nscaler.fit(X_train) # fit scaler to training data only\nX_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\nX_validation = pd.DataFrame(scaler.transform(X_validation), columns=X_validation.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_validation.columns)\n\n# apply principal component analysis to wavelength intensities\npca = PCA(n_components=3)\ndfs = [X_train, X_validation, X_test]\nfor i in range(len(dfs)):\n    df = dfs[i]\n    ugriz = pca.fit_transform(df[['u', 'g', 'r', 'i', 'z']])\n    df = pd.concat((df, pd.DataFrame(ugriz)), axis=1)\n    df.rename({0: 'PCA1', 1: 'PCA2', 2: 'PCA3'}, axis=1, inplace=True)\n    df.drop(['u', 'g', 'r', 'i', 'z'], axis=1, inplace=True)\n    dfs[i] = df\nX_train, X_validation, X_test = dfs\n\n# create a random forest model\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\npreds = rfc.predict(X_validation)\nmodel_acc = (preds == y_validation).sum().astype(float) \/ len(preds) * 100\n\nprint('Validation Accuracy: %3.5f' % (model_acc))","a1e26d1f":"# evaluate the random forest\npreds = rfc.predict(X_test)\nmodel_acc = (preds == y_test).sum().astype(float) \/ len(preds) * 100\n\nprint('Test Accuracy: %3.5f\\n' % (model_acc))\n\n# analyze feature importance in random forest model\nimportances = pd.DataFrame({\n    'Feature': X_validation.columns,\n    'Importance': rfc.feature_importances_\n})\nimportances = importances.sort_values(by='Importance', ascending=False)\nimportances = importances.set_index('Feature')\nprint(importances)\n\n# plot confusion matrices for both models\nlabels = np.unique(sdss_df['class'])\n\nax = plt.subplot(1, 1, 1)\nax.set_aspect(1)\ncm = confusion_matrix(y_test, preds)\nsns.heatmap(cm, annot=True, fmt='d', xticklabels = labels,\n            yticklabels = labels, cbar_kws={'orientation': 'horizontal'})\nplt.xlabel('Actual values')\nplt.title('Random Forest Classifier')\n\nplt.show()\n\n# compute precision and recall\nprecision = np.diag(cm) \/ np.sum(cm, axis = 0)\nrecall = np.diag(cm) \/ np.sum(cm, axis = 1)\n\nprint('Recall: ', precision)\nprint('Precision: ', recall)","d1a286af":"# Conclusion\n\nMy RFC obtained a 99.2% classification accuracy with fairly low run-time. The precision and recall is also generally high, thus demonstrating the strength of my model.\n\nI've also tackled this classification problem using [deep neural network](https:\/\/www.kaggle.com\/ktrinh\/sdss-classification-with-deep-neural-networks), though I got a lower accuracy with a significantly longer run-time.\n\nIf you'd like to check out my other Kaggle kernels or learn my about my background, feel free to check out my [website](http:\/\/www.kevinttrinh.com\/data-science\/)!","91926a66":"# Introduction\n\nFounded by the University of Chicago, the [Sloan Digital Sky Survey (SDSS)](http:\/\/https:\/\/www.sdss.org\/) uses a wide-angle optical telescope at the Apache Point Observatory in New Mexico, United States to create multi-colored 3D maps of a third of our sky. The survey includes spectral information on all detected astronomical objects which include stars, galaxies, and quasars. Data is periodically and publically released. This notebook will cover Data Release 16 (DR16).\n\nThis data set contains 100,000 labelled samples and 17 features. However, clever feature engineering allows me to reduce our number of features and yield classification accuracies above 99%. For this exercise, I'll compare two ML models -- deep neural networks and random forests -- on their ability to classify stars, galaxies, and quasars in the SDSS DR16 data set.\n\n\n**Approach**\n1. Data exploration\n2. Feature Engineering\n3. Construct ML Models (DNN and Random Forest)\n4. Test and evaluate models\n\nFor organization, I like to separate my data exploration and ML Python scripts.\n\n# Data exploration\n\nOff the bat, I recognized that there are a few features that bear no physical significance whatsoever: objid, specobjid, run, rerun, camcol, and field. I'll omit these features from further discussion.\n\nLet's generate the following figures:\n* Pie chart of labels\n* Equitorial coordinates of observations\n* Histogram distributions of redshift, plate, modified julien day, and fiber id\n* Histogram distributions of Thuan-Gunn Astronomic Magnitude system","8d257046":"# Machine Learning (Testing)\n\nI then evaluated my model against the test set, measured the importance of each feature (which is a nice property of decision trees), and plotted a confusion matrix. Unlike my training and validation set, I ran the following code ONCE for good practice so that my test data can truly be considered 'unseen' data.\n\nMy model yielded a test accuracy of 99.2% which is better than my validation performance. As previously predicted, redshift is highly correlated with the astronomical object label (58.3% importance). The next three most important features are the PCA features, though these features are much less influential than redshift.\n\nAccording to my confusion matrix:\n\n* Galaxies have a precision of 99.5% and recall of 99.0%.\n* Quasars have a precision of 95.4% and recall of 98.2%.\n* Stars have a precision of 99.8% and 99.8%.\n\nIn other words, my model performs much better on galaxies and stars than quasars. This makes sense because quasars are the minority in my 100,000 samples.","6bb1ecf9":"I noticed a few interesting things about my figures.\n\nFirst, my dataset consists of mostly galaxies and stars, though quasars are a sizeable minority (10.58% of samples). A skilled data scientist (which I am not) may factor this in when creating curated batches for ML models to improve model performance.\n\nSecond, the equitorial coordinates of stars are distributed differently than that of galaxies and quasars. This could be due to the Milky Way's shape as a barred spiral galaxy. We're more likely to see stars from our own solar system if we make observations along the plane of our galaxy. These coordinates could be useful in separating stars from other classes.\n\nThird, redshift appears to be very telling of the observed object's class. The axes of each class are drastically different. This makes sense because redshift results from observing objects travelling away from the observer at fast speeds. Stars travel away from us fast; galaxies travel away from us at faster rates; quasars travel away from us at even faster rates.\n\nFourth, the bulk of galaxies observed have low plate numbers and fiber IDs relative to quasars and stars. \n\nFifth, the distributions among classes for the ugriv variables look the same, and differences between classes are present but subtle. It is worth noting that each ugriv feature (i.e. band) is ordinally related to each other, so some additional processing of these features should be done prior to any ML.\n\n# Machine Learning (Training)\n\nNote: The below script is broken into two parts: 1) feature engineering, training, and validation, and 2) plots and predictions.\n\nI shuffled, partitioned, and reformated my data to feed into my RFC. While I have three different labels, I can convert them to arbitrary integers given that I'm working with decision trees (i.e. no dummy variables or one-hot-encoding needed). I scaled all features and used three principal component analysis (PCA) features to replace the u, g, r, i, and z features.\n\nNext, I trained my RFCs using 200 trees and obtained a validation accuracy of 98.4%."}}