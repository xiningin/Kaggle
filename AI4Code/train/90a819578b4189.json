{"cell_type":{"783c94dd":"code","1c6141a4":"code","1c4d49fe":"code","8d7746a6":"code","49817471":"code","3fdf1407":"code","337c144d":"code","d23698a2":"code","5219f15b":"code","16c2f52d":"code","9a07f432":"code","759513e2":"code","93746a67":"code","0ede42bc":"code","4b70a89b":"code","e8216d74":"code","8ffe14da":"code","3540dc26":"code","9ca043ec":"code","228989d7":"code","8efb7948":"code","e7021b03":"code","89fc2fe1":"code","62b78593":"code","4e0a79b1":"code","67d5c75f":"code","e87ee1ad":"code","850c0590":"code","15bf97f9":"code","10c87c12":"code","712edd32":"code","d766263b":"code","7b702149":"code","0c846a2b":"code","1cab74bc":"markdown","83b62137":"markdown","a0c2051c":"markdown","bd80e55a":"markdown","de057cfb":"markdown","74843fd6":"markdown","94744f9c":"markdown","b0b1dccc":"markdown","727408e4":"markdown","385467cf":"markdown","ebcac8c7":"markdown","770d885d":"markdown","d77ee920":"markdown","6d6fb15d":"markdown","3ca863c1":"markdown","51c1394c":"markdown","f46e9d85":"markdown","3d771e34":"markdown","6b847f7e":"markdown","9dae5dbd":"markdown","3c403cd1":"markdown","b044e805":"markdown","e96c059f":"markdown","085059f3":"markdown","3cae5828":"markdown","dc2f1a8b":"markdown","fcb7e0b3":"markdown","aaa6f03e":"markdown","c2e42e16":"markdown","0e243997":"markdown","d3605f15":"markdown"},"source":{"783c94dd":"!pip install pyspark;","1c6141a4":"# Import other modules not related to PySpark\nimport os\nimport sys\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport matplotlib\nfrom mpl_toolkits.mplot3d import Axes3D\nimport math\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom datetime import *\nimport statistics as stats\n# This helps auto print out the items without explixitly using 'print'\nInteractiveShell.ast_node_interactivity = \"all\" \n%matplotlib inline","1c4d49fe":"# Import PySpark related modules\nimport pyspark\nfrom pyspark.rdd import RDD\nfrom pyspark.sql import Row\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import functions\nfrom pyspark.sql.functions import lit, desc, col, size, array_contains\\\n, isnan, udf, hour, array_min, array_max, countDistinct\nfrom pyspark.sql.types import *\n\nMAX_MEMORY = '15G'\n# Initialize a spark session.\nconf = pyspark.SparkConf().setMaster(\"local[*]\") \\\n        .set('spark.executor.heartbeatInterval', 10000) \\\n        .set('spark.network.timeout', 10000) \\\n        .set(\"spark.core.connection.ack.wait.timeout\", \"3600\") \\\n        .set(\"spark.executor.memory\", MAX_MEMORY) \\\n        .set(\"spark.driver.memory\", MAX_MEMORY)\ndef init_spark():\n    spark = SparkSession \\\n        .builder \\\n        .appName(\"Pyspark guide\") \\\n        .config(conf=conf) \\\n        .getOrCreate()\n    return spark\n\nspark = init_spark()\nfilename_data = '..\/input\/fitrec-dataset\/endomondoHR.json'\n# Load the main data set into pyspark data frame \ndf = spark.read.json(filename_data, mode=\"DROPMALFORMED\")\nprint('Data frame type: ' + str(type(df)))","8d7746a6":"print('Data overview')\ndf.printSchema()\nprint('Columns overview')\npd.DataFrame(df.dtypes, columns = ['Column Name','Data type'])","49817471":"print('Data frame describe (string and numeric columns only):')\ndf.describe().toPandas()\n\nprint(f'There are total {df.count()} row, Let print first 2 data rows:')\ndf.limit(2).toPandas()","3fdf1407":"print('Columns overview')\npd.DataFrame(df.dtypes, columns = ['Column Name','Data type'])","337c144d":"string_columns = ['gender', 'sport', 'url']\nnumeric_columns = ['id','userId']\narray_columns = ['altitude', 'heart_rate', 'latitude', 'longitude', 'speed', 'timestamp']\nmissing_values = {} \nfor index, column in enumerate(df.columns):\n    if column in string_columns:    # check string columns with None and Null values\n#         missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull()).count()\n#         missing_values.update({column: missing_count})\n        missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull()).count()\n        missing_values.update({column:missing_count})\n    if column in numeric_columns:  # check zeroes, None, NaN\n        missing_count = df.where(col(column).isin([0,None,np.nan])).count()\n        missing_values.update({column:missing_count})\n    if column in array_columns:  # check zeros and NaN\n        missing_count = df.filter(array_contains(df[column], 0) | array_contains(df[column], np.nan)).count()\n        missing_values.update({column:missing_count})\nmissing_df = pd.DataFrame.from_dict([missing_values])\nmissing_df","d23698a2":"# We create new column to count the number of timestamps recorded per row\/workout, named as 'PerWorkoutRecordCount' column\ndf = df.withColumn('PerWorkoutRecordCount', size(col('timestamp')))\n\n\n# This part is writen as a function to be used again later\ndef user_activity_workout_summarize(df):\n    user_count = format(df.select('userId').distinct().count(), ',d')\n    workout_count = format(df.select('id').distinct().count(), ',d')\n    activity_count = str(df.select('sport').distinct().count())\n    sum_temp = df.agg(functions.sum('PerWorkoutRecordCount')).toPandas()\n    total_records_count = format(sum_temp['sum(PerWorkoutRecordCount)'][0],',d')\n    columns=['Users count', 'Activity types count','Workouts count', 'Total records count']\n    data = [[user_count], [activity_count], [workout_count], [total_records_count]]\n    sum_dict = {column: data[i] for i, column in enumerate(columns)}\n    sum_df = pd.DataFrame.from_dict(sum_dict)[columns]\n    gender_user_count = df.select('gender','userId').distinct().groupBy('gender').count().toPandas()\n    gender_activities_count = df.groupBy('gender').count().toPandas()\n    gender_user_activity_count = gender_user_count.join(\n        gender_activities_count.set_index('gender'), on='gender'\n        , how='inner', lsuffix='_gu'\n    )\n    gender_user_activity_count.columns = ['Gender', '# of users', 'Activities (workouts) count']\n    \n    return sum_df, gender_user_activity_count\n\nsum_dfs= user_activity_workout_summarize(df)\nprint('\\nOverall data set summary on users, workouts and number of records (pre-filtering):')\nsum_dfs[0]","5219f15b":"print('Number of workouts that have less than 50 records and statistic summary:')\nremoved_df = df.select('PerWorkoutRecordCount').where(df.PerWorkoutRecordCount < 50) \\\n               .toPandas().describe().astype(int)\nremoved_df.rename(columns = {'PerWorkoutRecordCount': 'PerWorkoutRecordCount <50'}, inplace=True)\nremoved_df.T","16c2f52d":"ranked_sport_users_df = df.select(df.sport, df.userId) \\\n    .distinct() \\\n    .groupBy(df.sport) \\\n    .count() \\\n    .orderBy(\"count\", ascending=False)\n\n# Top 5 workout types\nhighest_sport_users_df = ranked_sport_users_df.limit(5).toPandas()\n# Rename column name : 'count' --> Users count\nhighest_sport_users_df.rename(columns = {'count':'Users count'}, inplace = True)\n# Caculate the total users, we will this result to compute percentage later\ntotal_sports_users = ranked_sport_users_df.groupBy().sum().collect()[0][0]","9a07f432":"ranked_sport_users_df.collect()[:5]","759513e2":"highest_sport_users_df_renamed = highest_sport_users_df\n# Compute the percentage of top 5 workout type \/ total users\nhighest_sport_users_df_renamed['percentage'] = highest_sport_users_df['Users count'] \\\n    \/ total_sports_users * 100\n\n# We assign the rest of users belong to another specific group that we call 'others'\nothers = {\n      'sport': 'others'\n    , 'Users count': total_sports_users - sum(highest_sport_users_df_renamed['Users count'])\n    , 'percentage': 100 - sum(highest_sport_users_df_renamed['percentage'])\n}\n\nhighest_sport_users_df_renamed = highest_sport_users_df_renamed.append(\n    others, ignore_index=True\n)\nprint('Top 5 sports that have the most users participated:')\nhighest_sport_users_df_renamed\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(0.35))\n\nplot0 =   axs[0].bar(x=highest_sport_users_df_renamed['sport']\n                     , height=highest_sport_users_df_renamed['Users count'])\ntitle0 =  axs[0].set_title('Users count', fontsize = 'small')\nxlabel0 = axs[0].set_xlabel('Sport', fontsize = 'small')\nylabel0 = axs[0].set_ylabel('Users count', fontsize = 'small')\nxsticks_label = axs[0].set_xticklabels(highest_sport_users_df_renamed['sport'] \n                                       ,rotation = 'vertical', fontsize='small')\nexplode = (0.1, 0.1, 0.3, 0.3, 0.3, 0.1)\ntitle1 = axs[1].set_title('User ratio', fontsize = 'small')\nplot1 = axs[1].pie(\n      x=highest_sport_users_df_renamed['percentage']\n    , labels=highest_sport_users_df_renamed['sport']\n    , autopct='%1.1f%%', shadow=True, explode=explode, startangle=90\n    , radius=1\n)\n\ntext = fig.text(0.5, 1.02, 'Top 5 sports having the most users', ha='center', va='top', transform=fig.transFigure)","93746a67":"# Let quick overview activities by gender\n# we have something like this\nactivities_by_gender = df.groupBy('sport', 'gender').count().toPandas() \nactivities_by_gender[:5]","0ede42bc":"total_activities = ranked_sport_users_df.count()\nprint(f'There are total: {total_activities} activities and here is the chart for activities based on gender:')\n# Add the infor of activities based on gender\nactivities_by_gender = df.groupBy('sport', 'gender').count().toPandas()\n# Visualize\nfig = plt.figure(figsize=(12, 25))\ngrid_size = (1,1);\nax = plt.subplot2grid(grid_size, (0,0), colspan=1, rowspan=1)\nplot = activities_by_gender.groupby(['sport', 'gender']).agg(np.mean).groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()).unstack().plot(kind='barh', stacked=True, width=1  ## APPLY UNSTACK TO RESHAPE DATA\n                , edgecolor='black', ax=ax, title='List of all activities by gender')\nylabel = plt.ylabel('Sport (Activity)');\nxlabel = plt.xlabel('Participation percentage by gender');\nlegend = plt.legend(\n    sorted(activities_by_gender['gender'].unique()), loc='center left', bbox_to_anchor=(1.0, 0.5)\n)\nparam_update = plt.rcParams.update({'font.size': 16});\nax = plt.gca()\nformatter = ax.xaxis.set_major_formatter(mtick.PercentFormatter());\na = fig.tight_layout()\nplt.show()","4b70a89b":"activities_by_gender_df = activities_by_gender.pivot_table(\n    index=\"sport\", columns=\"gender\", values='count', fill_value=0) \\\n    .reset_index().rename_axis(None, axis=1)\n\nactivities_by_gender_df['total'] = activities_by_gender_df['male'] \\\n        + activities_by_gender_df['female'] \\\n        + activities_by_gender_df['unknown']\nactivities_by_gender_df['percentage'] = activities_by_gender_df['total'] \\\n    \/ sum(activities_by_gender_df['total']) * 100\ntop_activities_by_gender_df = activities_by_gender_df.sort_values(\n    by='percentage', ascending=False\n).head(5)\n\nothers = {'sport' : 'others'}\nfor column in ['female', 'male', 'unknown', 'total', 'percentage']:\n    value = sum(activities_by_gender_df[column]) - sum(top_activities_by_gender_df[column])\n    others.update({column: value})\ntop_activities_by_gender_df = top_activities_by_gender_df.append(others, ignore_index=True)\ntop_activities_by_gender_df = top_activities_by_gender_df.sort_values(\n    by='percentage', ascending=False\n)\ntop_activities_by_gender_df\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(0.35))\n\nplot0 = axs[0].bar(x=top_activities_by_gender_df['sport']\n                   , height=top_activities_by_gender_df['total'])\ntitle0 = axs[0].set_title('Workout count', fontsize='small')\nxlabel0 = axs[0].set_xlabel('Sport', fontsize='small')\nylabel0 = axs[0].set_ylabel('Workout count (times)', fontsize='small')\nxsticks_label = axs[0].set_xticklabels(top_activities_by_gender_df['sport']\n                                       , rotation='vertical', fontsize='small')\nexplode = (0.1, 0.1, 0.3, 0.3, 0.3, 0.3)\ntitle1 = axs[1].set_title('Workout ratio', fontsize = 'small')\nplot1 = axs[1].pie(\n    x=top_activities_by_gender_df['percentage']\n    , labels=top_activities_by_gender_df['sport']\n    , autopct='%1.1f%%', shadow=True, explode=explode, radius=1\n)\n\ntext = fig.text(0.5, 1.02, 'Top 5 sports that were most participated'\n                , ha='center', va='top', transform=fig.transFigure)","e8216d74":"min_number_of_sports = 1\n\nsport_df = df \\\n    .select(df.userId, df.gender, df.sport) \\\n    .distinct() \\\n    .groupBy(df.userId, df.gender) \\\n    .count()    \n\nuser_more_sports_df = sport_df \\\n                    .filter(sport_df[\"count\"] > min_number_of_sports) \\\n                    .orderBy(\"count\", ascending = False) \\\n                    .toPandas()\nuser_more_sports_df.rename(columns = {'count':'Sports count'}, inplace = True)\nuser_more_sports_df.describe().astype(int).T","8ffe14da":"plot = user_more_sports_df.boxplot(column='Sports count', by='gender', fontsize='small', figsize=(6,7))","3540dc26":"print('\\nPlot of workouts distribution by activity type:')\nplot_size_x, plot_size_y = 5, 5\nfigsize_x, figsize_y = plot_size_x * 4 + 3, plot_size_y * 13 + 1\nfigsize=(figsize_x, figsize_y)\nfig = plt.figure(figsize=figsize) #\ngrid_size = (13,4)\nax = plt.subplot2grid(grid_size, (0,0), colspan=1, rowspan=1)\n#fig, ax = plt.subplots()\nPerWorkoutRecordCount_dist = df.select('PerWorkoutRecordCount', 'sport').toPandas().hist(\n    column='PerWorkoutRecordCount', bins=10, sharex = False, grid=True\n    , xlabelsize='small', ylabelsize='small', by='sport', ax = ax\n    , layout = grid_size, figsize=figsize\n    )\na = fig.tight_layout()\ntitle = fig.text(0.5, 1, 'Distribution of records count per workout by sport', ha='center' \n         , fontsize='small', transform=fig.transFigure);\nxlabel = fig.text(\n    0.5, 0.01, '# of records\/workout', va='bottom', ha='center', transform=fig.transFigure\n)\nylabel = fig.text(0.01, 0.5, 'Frequency (count)', va='center', rotation='vertical');","9ca043ec":"# Filter df with at least 10 records (as we are assumming if any user_id with less then 10 record would not be meaningful)\nqualified_df = df \\\n    .select(df.sport, df.userId, df.gender) \\\n    .groupBy(df.sport, df.userId, df.gender) \\\n    .count()\nqualified_df = qualified_df.filter(qualified_df[\"count\"] >= 10) \\\n    .orderBy(\"count\", ascending = False)","228989d7":"print('Number of users having more than 10 workouts:')\nqualified_pd_df = qualified_df.select(\"userId\", \"gender\").distinct() \\\n    .groupBy(qualified_df.gender).count().toPandas()\nqualified_pd_df.rename(columns={'count': 'Users count'}, inplace=True)\nqualified_pd_df\nqualified_users_count = sum(qualified_pd_df['Users count'])\ntotal_users_count = df.select('userId').distinct().count()\nqualified_percentage = round((qualified_users_count \/ total_users_count),2) * 100\nprint('\\nSo there is {} \/ {} of users qualifying the 10 historical records criteria, which is {:.2f}%' \\\n      .format(qualified_users_count, total_users_count, qualified_percentage)\n     )","8efb7948":"# LOOK AGAIN THE TIMESTAMP COLUMN\ndf.limit(3).toPandas()","e7021b03":"# Convert a timestamp column into Datetime.Datetime, to be used for .withColumn function later\ndef to_time(timestamp_list):\n    # convert to datetime and minus 7 hours due to the difference in Endomondo time window with utc time as the dataset description\n    return [datetime.fromtimestamp(t) - timedelta(hours=7) for t in timestamp_list]\n\n# Register 'to_time' function into UDF pyspark framework\nudf_to_time = udf(to_time, ArrayType(elementType=TimestampType()))\n\n# Support function to get the duration (in minutes) of a list of datetime values, to be used for withColumn function later\ndef get_duration(datetime_list):\n    time_dif = max(datetime_list) - min(datetime_list)\n    return time_dif.seconds\/60\n\n# Register the support function 'get_duration' as a user defined function into pyspark framework\nudf_get_duration = udf(get_duration, FloatType())\n\n# Support function to get the workout start time of the datetime list, to be used for withColumn function later\ndef get_start_time(datetime_list):\n    return min(datetime_list)\n\n# Register the support function 'get_start_time' as a user defined function into pyspark framework\nudf_get_start_time = udf(get_start_time, TimestampType())\n\n# Support function to get list of intervals within a workout\ndef get_interval(datetime_list):\n    if len(datetime_list) == 1:\n        return [0]\n    else:\n        interval_list = []\n        for i in range(0, len(datetime_list)-1):\n            interval = (datetime_list[i+1] - datetime_list[i]).seconds\n            interval_list.append(interval)\n        return interval_list\n\n# Register the support function 'get_interval' as a user defined function into pyspark framework    \nudf_get_interval = udf(get_interval, ArrayType(elementType=IntegerType()))\n\n# Create new 'date_time' column to convert from timestamp into python's datetime format for later usage\ndf = df.withColumn('date_time', \n    udf_to_time('timestamp'))\n\n# Create 'workout_start_time' column to get the start time of each workout\/row:\ndf = df.withColumn('workout_start_time', hour(udf_get_start_time('date_time')))\n\n# Create duration column from the date_time column just created, using the udf function udf_get_duration defined above\ndf = df.withColumn('duration', udf_get_duration('date_time'))\n\n# Create interval column from the date_time column, using the udf function udf_get_interval defined above\ndf = df.withColumn('interval', udf_get_interval('date_time'))\n\nprint('New columns (''date_time'', ''workout_start_time'' in hour\\\n, ''duration'' in minutes & ''interval'' in seconds)\\n, first 5 rows:')\ndf.select('timestamp','date_time', 'workout_start_time', 'duration', 'interval').limit(5).toPandas()\n\n","89fc2fe1":"print('\\nLet''s look at the statistics of the new duration column (in minutes):')\ndf.select('duration').toPandas().describe().T","62b78593":"\nprint('\\nPlotting distribution of duration per sport type:')\nplot_size_x, plot_size_y = 5, 5\nfigsize_x, figsize_y = plot_size_x * 4 +3, plot_size_y * 13 + 1\nfigsize = (figsize_x, figsize_y)\nfig = plt.figure(figsize=figsize) #\ngrid_size = (13,4)\nax = plt.subplot2grid(grid_size, (0,0), colspan=1, rowspan=1)\n\nduration_dist = df.select('duration', 'sport').toPandas().hist(\n    column='duration', by='sport', bins=15, sharex = False, grid=True\n    , xlabelsize='small', ylabelsize='small' , ax = ax\n    , layout = grid_size, figsize=figsize\n    )\na = fig.tight_layout()\ntitle = fig.text(0.5, 1, 'Distribution of workout duration by sport'\n             , ha='center', va='center', transform=fig.transFigure\n            )\nxlabel = fig.text(0.5, 0.01, 'Workout duration (minutes)'\n             , ha='center', va='center', transform=fig.transFigure)\nylabel = fig.text(0.01, 0.5, 'Frequency (count)', va='center', rotation='vertical');","4e0a79b1":"#  Helper function to calculate statistic(s) of the column name from a tuple x of (sport, records list of the column)\n#, the stats to calculate is also given as an input\ndef calculate_stats(x,column_name, stat_list):\n    sport, records_list = x\n    stat_dict = {'sport': sport}\n    if 'min' in stat_list:\n        min_stat = min(records_list)\n        stat_dict.update({'min ' + column_name : min_stat})\n    if 'max' in stat_list:\n        max_stat = max(records_list)\n        stat_dict.update({'max ' + column_name: max_stat})\n    if 'mean' in stat_list:\n        average_stat = stats.mean(records_list)\n        stat_dict.update({'mean ' + column_name: average_stat})\n    if 'stdev' in stat_list:\n        std_stat = stats.stdev(records_list)\n        stat_dict.update({'stdev ' + column_name: std_stat})\n    if '50th percentile' in stat_list:\n        median_stat = stats.median(records_list)\n        stat_dict.update({'50th percentile ' + column_name: median_stat})\n    if '25th percentile' in stat_list:\n        percentile_25th_stat = np.percentile(records_list, 25)\n        stat_dict.update({'25th percentile ' + column_name: percentile_25th_stat})\n    if '75th percentile' in stat_list:\n        percentile_75th_stat = np.percentile(records_list, 75)\n        stat_dict.update({'75th percentile ' + column_name: percentile_75th_stat})\n    if '95th percentile' in stat_list:\n        percentile_95th_stat = np.percentile(records_list, 95)\n        stat_dict.update({'95th percentile ' + column_name: percentile_95th_stat})\n    return stat_dict\n\ndef to_list(a):\n    return a\n\ndef extend(a, b):\n    a.extend(b)\n    return a\n\ndef retrieve_array_column_stat_df(df, column_name, stat_list):\n    # Convert sport & \"column_name\" to RDD to easily calculate the statistics of intervals by sports\n    sport_record_rdd = df.select('sport', column_name).rdd \\\n    .map(tuple).combineByKey(to_list, extend, extend).persist()\n\n    # Calculate statistics of the input column by calling calculate_stats function defined above\n    record_statistic_df = pd.DataFrame(sport_record_rdd.map(\n        lambda x: calculate_stats(x, column_name,stat_list)).collect()\n                                      )\n    # Set proper dataframe column orders\n    columns_order = ['sport'] + [stat + ' ' + column_name for stat in stat_list]\n    # Re order columns\n    return record_statistic_df[columns_order]\n\nstat_list = ['min', '25th percentile', 'mean', '50th percentile',\n                     '75th percentile', '95th percentile', 'max', 'stdev']\ninterval_statistic_df = retrieve_array_column_stat_df(df, column_name='interval', stat_list=stat_list)\nprint('\\nLet\\'s look at statistic for interval, in seconds (by sport):' )\ninterval_statistic_df","67d5c75f":"print('\\nSummarize statistics of interval sport:')\nbar_columns = ['25th percentile interval', '50th percentile interval'\n               , '75th percentile interval', '95th percentile interval']\nline_columns1 = ['min interval', 'mean interval'] \nline_columns2 = ['max interval', 'stdev interval'] \ninterval_statistic_df = interval_statistic_df.sort_values(\n    by='95th percentile interval', ascending=False\n)\nfigsize=(13, 59)\nfig, axs = plt.subplots(nrows=7, figsize=figsize)\n\nd = axs[0].set_title('Interval statistics by sport', fontsize=18)\nfor i in range (7):\n    interval_statistic_sub_df = interval_statistic_df.iloc[i*7:i*7+7,]\n    #interval_statistic_sub_df\n    plot1 = interval_statistic_sub_df[['sport'] + bar_columns] \\\n        .groupby(['sport']).agg(np.mean).plot(\n        kind='bar', stacked=True, grid=False, alpha=0.5, edgecolor='black', ax=axs[i], \n    )\n    plot2 = interval_statistic_sub_df[['sport'] + line_columns1].plot(x='sport', ax=axs[i], marker='o')\n    ax2 = axs[i].twinx()\n    plot3 = interval_statistic_sub_df[['sport'] + line_columns2].plot( x='sport', ax=ax2, marker='o', color=['m', 'g'])\n    a = axs[i].legend(loc='center left', fontsize=16, bbox_to_anchor=(1.2, 0.5), frameon=False)\n    a = ax2.legend(  labels=['max interval (right)', 'stdev interval (right)']\n                   , loc=\"center left\", fontsize=16, bbox_to_anchor=(1.2, 0.11), frameon=False)\n    b = axs[i].set_xticklabels(interval_statistic_sub_df['sport'],rotation = 'horizontal', fontsize='small')\n    c = axs[i].set_xlabel('Sport (Activity)', fontsize='small');\n    d = axs[i].set_ylabel('Quantiles Statistics + min\/mean\\n(second)', fontsize=16);\n    e = ax2.set_ylabel('Max\/stdev Statistics\\n(second)', fontsize=16)\n    for tick in axs[i].yaxis.get_major_ticks():\n        a = tick.label.set_fontsize(16) \n    ax2.tick_params(axis='y', labelsize=16)\n    b = plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=True)\n\nplt.subplots_adjust(hspace=0.2)\nplt.show();\n","e87ee1ad":"# Retrive the table of gender, sport and workout_start_time for plotting\nstart_time_df = df.select('gender', 'sport','workout_start_time').toPandas()","850c0590":"activities = start_time_df['sport'].unique()\nplot_size_x, plot_size_y = 5, 5\nfigsize_x, figsize_y = (plot_size_x + 0.5) * 4 +3, (plot_size_y + 1) * 13 + 1\n\n\nnrows, ncols = 13, 4\na = fig.subplots_adjust(hspace = 1, wspace = 1)\nfig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figsize_x, figsize_y))\nprint('\\nPlotting distribution of workout start time per sport type, break down by gender:')\na = plt.setp(axs, xticks=[0, 4, 8, 12, 16, 20])\nfor index, sport in enumerate(activities):\n    row_index, col_index = divmod(index, ncols)\n    male_start_time_list = start_time_df[(start_time_df.sport == sport) & \n                                            (start_time_df.gender == 'male')]['workout_start_time']\n    female_start_time_list = start_time_df[(start_time_df.sport == sport) & \n                                            (start_time_df.gender == 'female')]['workout_start_time']\n    unknown_start_time_list = start_time_df[(start_time_df.sport == sport) & \n                                            (start_time_df.gender == 'unknown')]['workout_start_time']\n    if len(male_start_time_list) > 0:\n        male_dist = axs[row_index, col_index].hist(male_start_time_list,\n                                      bins = 12, alpha=0.5, label='male', range=(0, 23))\n    if len(female_start_time_list) > 0:    \n        female_dist = axs[row_index, col_index].hist(female_start_time_list,\n                                      bins = 12, alpha=0.5, label='female', range=(0, 23))\n    if len(unknown_start_time_list) > 0:\n        unknown_dist = axs[row_index, col_index].hist(unknown_start_time_list,\n                                      bins = 12, alpha=0.5, label = 'unknown', range=(0, 23))\n    b= axs[row_index, col_index].set_title('Activitiy: ' + sport, fontsize='small')\n    a = axs[row_index, col_index].legend(loc=\"upper left\", fontsize='small')\n    a = plt.setp(axs[row_index, col_index].get_xticklabels(), fontsize='small')\n\nfor i in range(1,4):\n    x = axs[12, i].set_visible(False)\na = fig.tight_layout()\nz = fig.text(0.5, 1, 'Distribution of workout started time (hour) by sport'\n             , ha='center', va='top', transform=fig.transFigure)\ny = fig.text(0.5, 0.01, 'Workout started hour in a day (hour)'\n             , ha='center', va='bottom', transform=fig.transFigure)\nz = fig.text(0.02, 0.5, 'Frequency (count)', va='center', rotation='vertical');","15bf97f9":"stat_list = ['min', '25th percentile', 'mean', '95th percentile', 'max', 'stdev']\nheart_rate_statistic_df = retrieve_array_column_stat_df(df, column_name='heart_rate', stat_list=stat_list)","10c87c12":"# Support function helping to sample data\ndef sampling_data(max_users_per_gender, max_workouts_per_sport):\n    '''\n        max_users_per_gender: maximum number of user to be selected randomly per gender\n        max_workouts_per_sport: maximum number of activities to be selected per sport \n        (the sports existing in selected users)\n    '''\n    # Get unique list of userId and gender, for sampling purpose\n    users_genders = df.select('userId', 'gender').distinct().toPandas()\n    # Use 'sample' function to pick up to 3 userId per gender from the unique userId list\n    random_x_users_per_gender = users_genders.groupby('gender')['userId'].apply(\n                lambda s: s.sample(min(len(s), max_users_per_gender))\n    )\n\n    # Apply filter on the main pyspark dataframe for sampling\n    samples_by_gender = df.where(df.userId.isin(list(random_x_users_per_gender)))\n\n    # Next, generate the unique activity ids and sport types list from the sampled data set \n    workout_sports = samples_by_gender.select('id', 'sport').distinct().toPandas()\n    # Use 'sample' function to pick up to 10 activity ids for each kind of sport \n    random_y_workouts_per_sport = workout_sports.groupby('sport')['id'].apply(\n        lambda s: s.sample(min(len(s), max_workouts_per_sport))\n    )\n\n    # Apply filter to the sampled dataset to continue reduce the number of workouts per activity type\n    samples_by_gender_and_sport = samples_by_gender.where(df.id.isin(list(random_y_workouts_per_sport)))\n    return samples_by_gender_and_sport","712edd32":"# Use 2 variable to determine the sampling criteria: \n# maximum users per gender and maximum workouts per sport\nmax_users_per_gender, max_workouts_per_sport = 20, 15\n\n# Collect the sampled data set to Pandas to be used with plot features\npd_df = sampling_data(max_users_per_gender, max_workouts_per_sport).toPandas()\nprint('\\nSampled data overview (only string and numeric columns):')\npd_df.describe()","d766263b":"# Lambda function to flatten a list of lists into a big single list\nflattern = lambda l: set([item for sublist in l for item in sublist])\n\nnormalized_datetime_list = []\nfor index,data_row in pd_df.iterrows():\n    min_date_time = min(data_row['date_time'])\n    normalized_datetime_list.append(\n        [(date_time - min_date_time).seconds for date_time in data_row['date_time']]\n    )\n\npd_df['normalized_date_time'] = normalized_datetime_list\n\nprint('New normalized datetime (first 7 rows):')\npd_df.head(7)[['userId', 'sport', 'date_time','normalized_date_time']]\n\nprint('\\nPlot raw heart rate (sampled) by normalized time:')\n\nsport_list = pd_df['sport'].unique()\n# Define the length of the figure dynamically depends on the length of the sport list\nfig, axs = plt.subplots(len(sport_list), figsize=(15, 6*len(sport_list)))\nsubplot_adj = fig.subplots_adjust(hspace = 0.6)\nplot_setp = plt.setp(axs, yticks=range(0,250,20))\n\nfor sport_index, sport in enumerate(sport_list):\n    workout = pd_df[pd_df.sport == sport]\n    max_time = max(flattern(workout.normalized_date_time))\n    for workout_index, data_row in workout.iterrows():\n        label = 'user: ' + str(data_row['userId']) + ' - gender: ' + data_row['gender']\n        plot_i = axs[sport_index].plot(\n            data_row['normalized_date_time'], data_row['heart_rate'], label=label\n        )\n    title_i = axs[sport_index].set_title('Activitiy: ' + sport, fontsize='small')\n    xlabel_i = axs[sport_index].set_xlabel('Time (sec)', fontsize='small')\n    xsticklabels_i = axs[sport_index].set_xticklabels(\n        range(0, max_time, 500),rotation = 'vertical', fontsize=9\n    )\n    ysticklabels_i = axs[sport_index].set_yticklabels(range(0,250,20),fontsize='small')\n    legend_i = axs[sport_index].legend(\n        loc='center left', bbox_to_anchor=(1.0, 0.5), prop={'size': 9}\n    )\n\nx_label = fig.text(0.04, 0.5, 'Heart rate (bpm)', va='center', rotation='vertical')\nchart_title = fig.text(0.5, 1.3, 'Raw heart rate (sample) by normalized time', \n            ha='center', va='center', fontsize='small', transform=axs[0].transAxes)\n","7b702149":"pd_df_small = sampling_data(max_users_per_gender=2, max_workouts_per_sport=2).toPandas()\nprint('Sampled data (2 user, 2 workouts per sport):')\npd_df_small[['userId', 'gender','sport','id', 'workout_start_time'\n             ,'PerWorkoutRecordCount', 'duration', 'longitude', 'latitude', 'altitude']].describe()","0c846a2b":"def get_fixed_mins_maxs(mins, maxs):\n    deltas = (maxs - mins) \/ 12.\n    mins = mins + deltas \/ 4.\n    maxs = maxs - deltas \/ 4.\n\n    return [mins, maxs]\n\nworkout_count = pd_df_small.shape[0]\nncols = 3\nnrows = math.ceil(workout_count\/ncols)\n#workout_count\nfig = plt.figure(figsize=(8 * (ncols + 0.5), 8*nrows))\n\na = fig.subplots_adjust(hspace = 0.2, wspace=0.5)\n#c = plt.setp(axs, yticks=range(0,250,20))\n\nprint('Plot workout path in 3D graphs per each workout:')\nfor row_index, row in pd_df_small.iterrows():\n    if row_index==2:\n        text = ax.text2D(\n            0.01, 1, \"Workout path (longitude\/latitude\/altitude)\"\n            , fontsize=18, transform=ax.transAxes\n        )\n    min_long = min(row['longitude']) - stats.stdev(row['longitude'])\n    max_long = max(row['longitude']) + stats.stdev(row['longitude'])\n    minmax_long = get_fixed_mins_maxs(min_long, max_long)\n    #minmax_long\n    min_lat = min(row['latitude']) - stats.stdev(row['latitude'])\n    max_lat = max(row['latitude']) + stats.stdev(row['latitude'])\n    minmax_lat = get_fixed_mins_maxs(min_lat, max_lat)\n    #minmax_lat\n    min_alt = min(row['altitude']) - stats.stdev(row['altitude'])\n    max_alt = max(row['altitude']) + stats.stdev(row['altitude'])\n    minmax_alt = get_fixed_mins_maxs(min_alt, max_alt)\n    #minmax_alt\n    ax = fig.add_subplot(nrows, ncols, row_index + 1, projection='3d')\n    title = 'Activitiy: ' + row['sport'] + ' - Gender: ' + row['gender'] \\\n        + '\\nRecords: ' + str(int(row['PerWorkoutRecordCount'])) \\\n        + ' - Duration: ' + str(int(row['duration'])) + ' minutes'\n    title = ax.set_title(title, fontsize=16)\n    scatter = ax.scatter(row['longitude'], row['latitude'], row['altitude'], c='r', marker='o')\n    plot = ax.plot3D(\n        row['longitude'], row['latitude'], row['altitude'], c='gray', label='Workout path'\n    )\n    \n    x_label = ax.set_xlabel('Longitude (Degree)', fontsize=16)\n    y_label = ax.set_ylabel('Latitude (Degree)', fontsize=16)\n    z_label = ax.set_zlabel('Altitude (m)', fontsize=16, rotation = 0)\n    for t in ax.xaxis.get_major_ticks():\n        font_size = t.label.set_fontsize(16)\n    for t in ax.yaxis.get_major_ticks():\n        font_size = t.label.set_fontsize(16)\n    for t in ax.zaxis.get_major_ticks():\n        font_size = t.label.set_fontsize(16)\n    legend = ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n    ax.zaxis.set_rotate_label(False)\n    #b = plt.setp(ax.get_xticklabels(), rotation=41)\n    #b = plt.setp(ax.get_yticklabels(), rotation=-30)\n    plt.gca().xaxis.set_major_formatter(mtick.FormatStrFormatter('%.3f'))\n    plt.gca().yaxis.set_major_formatter(mtick.FormatStrFormatter('%.3f'))\n    ax.pbaspect = [4, 2, 0.5]\n    xlims = ax.set_xlim(minmax_long)\n    ylims = ax.set_ylim(minmax_lat)\n    # Some sports will not have altitude change so check it before set z limit\n    if minmax_alt[0] != minmax_alt[1]: zlims = ax.set_zlim(minmax_alt)\n    # Do this trick to enable tight_layout for 3D plot:\n    for spine in ax.spines.values():\n        b = spine.set_visible(False)\nplt.rcParams['legend.fontsize'] = 16\na = plt.tight_layout()\nplt.show()","1cab74bc":" As we can see, The total records for this dataset is over 111M records. It's really large size","83b62137":"<a class=\"anchor\" id=\"0.1\"><\/a>\n[](http:\/\/)\n\n# **Table of Contents**\n\n\n1.\t[Initialize pyspark framework and load data into pyspark's dataframe](#1)\n2.\t[Overview of Dataset](#2)\n3.\t[Detect missing values and abnormal zeroes](#3)\n4.\t[Pyspark lazy evaluation](#4)\n5.\t[Explolatory Data analysis in Pyspark](#5)    \n6.\t[Unstack pyspark dataframe](#6)\n7.\t[Pyspark UDF Registering](#7)\n8.\t[Convert row objects to Spark Resilient Distributed Dataset (RDD)](#8)\n","a0c2051c":"### Workout start time\nOnce again, we use histogram chart to look at the distribution of workouts' started hours, grouped by sport and broken down by gender. We divide a day into intervals of 2 hours, so there are totally 12 buckets.","bd80e55a":"# **8. Convert row objects to Spark Resilient Distributed Dataset (RDD)** <a class=\"anchor\" id=\"7\"><\/a>\n[Go back to table of contents](#0.1)\n\nIn this plot, we will practice how to convert the row object to RDD format in Pyspark through:\n\n```python\nrdd = df.rdd.map(tuple)\nor\nrdd = df.rdd.map(list)\n```\n\nThe advanced of RDD format is: Each data set is divided into logical parts and these can be easily computed on different nodes of the cluster. They can be operated in parallel and are fault-tolerant, so that the process is stable and very fast\n\nIf we run this code such as on Zeppelin which intergrated with Pyspark clusters,we can see how speed RDD spark is.\n\nRDD is very important concept in Spark and you can deep into more in here :\nhttps:\/\/www.educba.com\/what-is-rdd\/\n\nTo practice this concept, we look at the `interval` column and have some statistics for it. we will calculate some major statistics (min\/max\/mean\/average\/standar deviation and 4 quantiles 25th\/50th\/75th\/95th) info in pySpark, convert to Rdd and plot them.\n\n","de057cfb":"Thank you for reading my work, wish you strong and stay safe","74843fd6":"Once again, similar to the user participation, running, walking and biking are also the dominant contribution interm of number of workout counts. However, the only different is that pure running and biking activities count is much bigger than those of the remaining sports, and the total count of those 2 already take up to more than 85% of total activities. \n\nLet play with some question such as how many people participated in more than 1 sport.","94744f9c":"Now we plot those numbers in bar (for quantiles statistics) and line charts (for min\/max\/mean\/stdev) for a more visualized feel.  \n*Note: Due to the fact that the maximum interval and stdev have a much higher order of magnitude compared to the remaining columns, we need to put those 2 columns in a separate y axis on the right.*","b0b1dccc":"# **3. Detect missing values and abnormal zeroes** <a class=\"anchor\" id=\"3\"><\/a>\n\n[Go back to table of contents](#0.1)\n\nAfter having a first sight of the columns, the first thing we should check is if the data set having any missing value.\n- For string columns, we check for `None` and `null`\n- For numeric columns, we check for zeroes and `NaN`\n- For array type columns, we check if the array contain zeroes or `NaN`","727408e4":"From the distribution charts above, it can be seen that most of the sports have activities started either in the morning or eveneing (bimodal distribution), which does make sense. There are a few activities happening during the timeframe of 0-4 o'clock, which is quite odd.  \n<br \/>\n\n### Look deeper into row level information","385467cf":"*From the duration summary & distribution plot, majority of the activities happens in 1-2 hours, only a few sports with a few cases each type happened in longer durations, such as moutain bike, hiking, sailing, etc..*","ebcac8c7":"### Distribution of records count per workout\n\n*For a more detailed observation, we break down the record count per activity into each individual sport. <br \/>Based on the distribution, the maximum records per workout is 500, but not all workouts and sport types reach that number.*","770d885d":"*Based on the summary, there are 822 persons participated more than 1 sport. Among them in average a person take part in about 3 sports and there is some person playing up to 16 sports!* <br \/>\nNow we look at the statistic by gender in box plot:","d77ee920":"# **2. Overview of Dataset** <a class=\"anchor\" id=\"2\"><\/a>\n\n[Go back to table of contents](#0.1)\n\n### Schema, columns & datatypes of the data set:\n   *The data set has both single value columns (int, string) and columns made of arrays\/list.*","6d6fb15d":"Looking at the quantiles statistic, up to 95% of the interval data set does not have the interval larger than 400 seconds, while there are just a few outliers that made the maximum intervals reach up to 86400 seconds (a full days).","3ca863c1":"### We create 4 helper function for 'timestamp' column as described above then convert them to UDF","51c1394c":"# **7. Pyspark UDF Registering** <a class=\"anchor\" id=\"6\"><\/a>\n[Go back to table of contents](#0.1)\n\n\nTo be in short, Registering UDF to Pyspark is the process of **turning Python Functions into PySpark Functions (UDF)**\n\n![Registering%20UDF%20to%20Pypark.JPG](attachment:Registering%20UDF%20to%20Pypark.JPG)\n\nWhen we run the code on Spark clusters, this technique will speed up the process and save valuable executed time.\n\nTo learn more about Pyspark UDF, you can visit https:\/\/changhsinlee.com\/pyspark-udf\/\n\nOkie now we will start to apply registering python function to UDF in the 'timestamp' column\n\n \nThis column is very important if we use this dataset to predict something, like predict heart rate to soonly detect some bad signal on user's heart. Because this is a type of time series analysis, so we will look at the `timestamp` column carefully.\n### Creating some new features from `timestamp`\nAs seen before `timestamp` column contains records of timestamp series of a single workout (a data row) and is stored in UNIX timestamp format. To have more insights on this column, we will create 4 more new columns from it:  \n- `date_time`: Convert UNIX timestamp into python's datetime format   \n- `duration`: Total time of a single workout, in minute    \n  *In order to get the workout `duration`, we get the difference between max and min of the datetime list of each workout.*\n- `workout_start_time`: Determine when, which hour of the day a workout start  \n  *For `workout_start_time`, it's the hour part of the first datetime record of a workout.*\n- `interval`: List of time lapses between each single timestamp record in a single workout, in second   \n  *And for `interval`, we will calculate it by taking the difference between 2 consecutive timestamp records within a workout.*\n\nIn order to attach these 4 features to SparkdDataFrame, we register them with PYSPARK UDF \n\n```python\nfunction_to_udf = udf(function, Datatype()) \n\n# Datatype() can be floattype(), TimestampType(), etc\n\n```\n","f46e9d85":"# **4. Pyspark lazy evaluation** <a class=\"anchor\" id=\"4\"><\/a>\n\n[Go back to table of contents](#0.1)\n\nHere we will begin to be familiar with some of Advanced Spark feature: **Lazy evaluation**.\n\n<font color=\"red\"><b>Lazy evaluation<\/b><\/font> enhances the power of Apache Spark by reducing the execution time of the RDD operations. It maintains the lineage graph to remember the operations on RDD. we can simply remember that all processing in Pyspark is abstraction, When we want to return the results, actually we tell Spark what is the eventual answer you're interested and it figures out best way to get there. As a result, it optimizes the performance and achieves fault tolerance. \n\nIn order to see the result, we have to call Spark.collect(). \n\nNormolly, we can show the results with the syntax: df.take(k) or df.limit(k) to get the results with k row. \n\nWhen K become large number, These 2 way above takes a long time to complete the process. Because this syntax above did not utilize the power of Pyspark processing (Lazy evaluation).\nIn order to quickly processing , We should use df.collect()[:k] to return the k row as we want.\n\nYou can read more about Pyspark lazy evaluation in : https:\/\/data-flair.training\/blogs\/apache-spark-lazy-evaluation\/","3d771e34":"*The boxplot showed that except the outliers, both males and females have nearly the same distribution of sport participation.*","6b847f7e":"<h2 style=\"text-align:center;font-size:200%;;\">Advanced Pyspark for Exploratory Data Analysis <\/h2>\n<h3  style=\"text-align:center;\"><span class=\"label label-success\">Lazy evaluation<\/span> <span class=\"label label-success\">UDF registering<\/span> <span class=\"label label-success\">Spark RDD<\/span> <span class=\"label label-success\">Data Processing<\/span> <span class=\"label label-success\">Explored Analysis<\/span> <span class=\"label label-success\">Visualization<\/span><\/h3>","9dae5dbd":"From the statistic of `duration` column, it can be observed that workout duration can last from 0 minute and up to 1 full day (1440 minutes = 24 hours). The duration of 0 might be for workouts that only have 1 single record only, so the min and the max timestamp would be the same.<br \/>\nNow it's plot time for duration:\n","3c403cd1":"### Workout displacements\nWe will have some visualization on 3 displacement\/geometry info columns (`longitude`,`latitude` & `altitude`).    \nSince the geometry location of each user and workout is different from each others, we only plot a few single workouts in 3D plots to have a look on the workout route.\n","b044e805":"We want reshape the table above to flatten the gender column so that we can visualize on it. I draw a simple draft as follow \n![Unstack%20dataframe.JPG](attachment:Unstack%20dataframe.JPG)\n\nTo reshape the table like this in Pyspark, we use \n```python\ndf.unstack()\n```","e96c059f":"we will normalize the time for all workouts by calulating the duration (in seconds) of each timestamp record from the first record of a workout (the first datetime element of the list in that workout). <br \/>\nThen we plot the heart rate on this normalized time, grouping by sport.","085059f3":"# **5. Explolatory Data analysis** <a class=\"anchor\" id=\"5\"><\/a>\n\n[Go back to table of contents](#0.1)\n\nLet first do some chart indicate the top 5 workout types as we evaluated above","3cae5828":"The data shows that running, walking and biking-related activities are the most spent by users, which is quite reasonable due to those exercises' convenience without much investment :3","dc2f1a8b":"### Now, we look at the duration of each workout (in minutes). First is some typical statistics.","fcb7e0b3":"## The benefit of learning Pyspark\n\n* If your team are planning to do project with really large dataset, Spark is really great choice because of its power for <font color=\"red\"><b>handling bigdata<\/b><\/font> .\nHere is the comparision of Pyspark vs Pandas from Databrick:  \n![Spark_vs_Pandas.JPG](attachment:Spark_vs_Pandas.JPG)\nSource: https:\/\/databricks.com\/blog\/2018\/05\/03\/benchmarking-apache-spark-on-a-single-node-machine.html\n\n\n* If you are planning to <font color=\"red\"><b>land a job<\/b><\/font> in a company with really big data ecosystem,  be familiar with Spark will be a <font color=\"red\"><b>good plus<\/b><\/font>  and makes you different than other applicants. (that's from my experience)\n\n\nIn case you are new to pyspark, you can check it first,\nan introduction guide to Pyspark :https:\/\/www.kaggle.com\/tientd95\/pyspark-for-data-science","aaa6f03e":"Let's look at the top pareto of 5 sports that have the most participation.","c2e42e16":"# **1. Initialize pyspark framework and load data into pyspark's dataframe** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Go back to table of contents](#0.1)\n\n\nThe dataset we use is from : Datasource: https:\/\/sites.google.com\/eng.ucsd.edu\/fitrec-project\/home \n\nThis dataset is about calculating the heart rate of people, along with other relating features: gender, weather condition, sport type, GPS, etc","0e243997":"*Due to the huge amount of users and workout numbers, we just picked randomly up to a x number of users per gender (ex, 5), and up to y workouts per activity type (ex, 10).<br \/>*","d3605f15":"# **6. UNSTACK PYSPARK DATAFRAME** <a class=\"anchor\" id=\"5\"><\/a>\n[Go back to table of contents](#0.1)"}}