{"cell_type":{"1d49510e":"code","f7740999":"code","23a4f0d4":"code","04b55e8c":"code","ce25867d":"code","986ed4b3":"code","277ac930":"code","2c325d7d":"code","6400e882":"code","2ba23c38":"code","d38c436d":"code","034e1471":"code","ce6f03e6":"code","9219e03e":"code","88369f9a":"code","7878895f":"code","f95639cb":"code","865533d2":"code","ca6f1de7":"code","9584dcca":"code","faf493a6":"code","4d0cf6cc":"code","8a67a566":"code","4b12a784":"code","1b1c9433":"code","a37c5d39":"code","3efb9b17":"code","22f78872":"code","71308f7f":"code","0827e2b3":"code","7dc774fc":"code","4a916d42":"code","a11d37d3":"code","bedcf4d1":"code","eaa7fc89":"code","38c0ac4a":"code","139bdee3":"code","f5348e06":"code","d6859477":"code","6c9e0d01":"code","fb7a6008":"code","5b5b2a0e":"code","7f77d479":"code","2ba87673":"code","18ccbe44":"code","2051956a":"code","e7f1f0e7":"code","25e6f597":"code","d554a182":"code","7d2459a8":"code","2e07f447":"code","5c938b95":"code","22f71a09":"code","50232b86":"code","85bb4f81":"code","a7df4af7":"markdown","e1ac2d07":"markdown","47fccb86":"markdown","8a9ac622":"markdown","d23e2c9c":"markdown","13dcf7b8":"markdown","bc0b6b0e":"markdown","02eebe42":"markdown","46c86486":"markdown","22ba2f6b":"markdown","1128b367":"markdown","e0f5b2ae":"markdown","5508e481":"markdown","0cf46ecb":"markdown","12304c5d":"markdown","848810de":"markdown","81ee9b29":"markdown","7902fc94":"markdown","ddd910dd":"markdown","4cabfb0e":"markdown","5af09be9":"markdown","8d5fb5e2":"markdown"},"source":{"1d49510e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","f7740999":"data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndata.shape","23a4f0d4":"data.head()","04b55e8c":"label = data['label']\nlabel.shape","ce25867d":"dependent = data.drop('label',axis=1)\ndependent.shape","986ed4b3":"plt.figure(figsize=(7,7))\nrow_index = 100\n\ngrid_data = np.array(dependent.iloc[row_index]).reshape(28,28)\nplt.imshow(grid_data, interpolation = 'none', cmap= \"gray\")\nplt.show()\nprint(label.iloc[row_index])","277ac930":"from sklearn.preprocessing import StandardScaler","2c325d7d":"dependent_scaled = StandardScaler().fit_transform(dependent)\ndependent_scaled.shape","6400e882":"from sklearn.decomposition import PCA","2ba23c38":"pca = PCA(n_components = 2)","d38c436d":"pca_data = pca.fit_transform(dependent_scaled)","034e1471":"new_pca_data = np.vstack((pca_data.T,label)).T","ce6f03e6":"new_pca_data.shape","9219e03e":"new_pca_df = pd.DataFrame(data = new_pca_data, columns = ('1st','2nd','label'))\nnew_pca_df.head()","88369f9a":"import seaborn as sns","7878895f":"sns.set_style(style='dark')","f95639cb":"\nsns.FacetGrid(new_pca_df, hue='label', height=8 ) \\\n    .map(plt.scatter, '1st', '2nd') \\\n    .add_legend()\nplt.show()","865533d2":"pca_3d = PCA(n_components=3)","ca6f1de7":"pca_3d_data = pca_3d.fit_transform(dependent_scaled)","9584dcca":"new_pca_3d_data = np.vstack((pca_3d_data.T,label)).T","faf493a6":"new_pca_3d_data.shape","4d0cf6cc":"new_pca_3d_df = pd.DataFrame(data = new_pca_3d_data, columns = ('1st','2nd','3rd','label'))\nnew_pca_3d_df.head()","8a67a566":"import plotly.express as px","4b12a784":"fig = px.scatter_3d(new_pca_3d_df,\n    x='1st',\n    y='2nd',\n    z='3rd',\n    color='label', title='PCA')\n\nfig.show()","1b1c9433":"from sklearn.decomposition import KernelPCA","a37c5d39":"kernel_pca_3d_linear = KernelPCA(n_components=3, kernel='linear', n_jobs=-1)\nkernel_pca_3d_rbf = KernelPCA(n_components=3, kernel='rbf', n_jobs=-1)\nkernel_pca_3d_poly = KernelPCA(n_components=3, kernel='poly', n_jobs=-1)\nkernel_pca_3d_cosine = KernelPCA(n_components=3, kernel='cosine', n_jobs=-1)","3efb9b17":"kernel_pca_3d_linear_df = kernel_pca_3d_linear.fit_transform(dependent_scaled[:10000])\nkernel_pca_3d_rbf_df = kernel_pca_3d_rbf.fit_transform(dependent_scaled[:10000])\nkernel_pca_3d_poly_df = kernel_pca_3d_poly.fit_transform(dependent_scaled[:10000])\nkernel_pca_3d_cosine_df = kernel_pca_3d_cosine.fit_transform(dependent_scaled[:10000])","22f78872":"kernel_pca_3d_linear_df = pd.DataFrame(data = kernel_pca_3d_linear_df, columns = ('1st','2nd','3rd'))\nkernel_pca_3d_rbf_df = pd.DataFrame(data = kernel_pca_3d_rbf_df, columns = ('1st','2nd','3rd'))\nkernel_pca_3d_poly_df = pd.DataFrame(data = kernel_pca_3d_poly_df, columns = ('1st','2nd','3rd'))\nkernel_pca_3d_cosine_df = pd.DataFrame(data = kernel_pca_3d_cosine_df, columns = ('1st','2nd','3rd'))","71308f7f":"kernel_pca_3d_linear_df['label'] = label[:10000]\nkernel_pca_3d_rbf_df['label'] = label[:10000]\nkernel_pca_3d_poly_df['label'] = label[:10000]\nkernel_pca_3d_cosine_df['label'] = label[:10000]","0827e2b3":"plot1 = px.scatter_3d(kernel_pca_3d_linear_df,\n    x='1st',\n    y='2nd',\n    z='3rd',\n    color='label', title='linear')\n\nplot2 = px.scatter_3d(kernel_pca_3d_rbf_df,\n    x='1st',\n    y='2nd',\n    z='3rd',\n    color='label', title='rbf')\n\nplot3 = px.scatter_3d(kernel_pca_3d_poly_df,\n    x='1st',\n    y='2nd',\n    z='3rd',\n    color='label', title='poly')\n\nplot4 = px.scatter_3d(kernel_pca_3d_cosine_df,\n    x='1st',\n    y='2nd',\n    z='3rd',\n    color='label',title='cosine')","7dc774fc":"plot1.show()","4a916d42":"plot2.show()","a11d37d3":"plot3.show()","bedcf4d1":"plot4.show()","eaa7fc89":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis ","38c0ac4a":"lda = LinearDiscriminantAnalysis(n_components=2)","139bdee3":"lda_data = lda.fit_transform(dependent_scaled, label)","f5348e06":"lda_df = pd.DataFrame(lda_data)","d6859477":"lda_df['label'] = label","6c9e0d01":"lda_df.columns = ['1st', '2nd', 'label']","fb7a6008":"lda_df.head()","5b5b2a0e":"sns.FacetGrid(lda_df, hue='label', height=8) \\\n    .map(plt.scatter, '1st', '2nd') \\\n    .add_legend()\nplt.show()","7f77d479":"lda_3d = LinearDiscriminantAnalysis(n_components=3)","2ba87673":"lda_3d_df = lda_3d.fit_transform(dependent_scaled[:10000],label[:10000])\nlda_3d_df = pd.DataFrame(lda_3d_df, columns=['1st','2nd','3rd'])\nlda_3d_df['label'] = label[:10000]","18ccbe44":"plot = px.scatter_3d(lda_3d_df, x='1st', y='2nd', z='3rd', color='label', title='LDA-3D')\nplot.show()","2051956a":"from sklearn.manifold import TSNE","e7f1f0e7":"tsne= TSNE(n_components = 2, n_jobs=-1)","25e6f597":"dependent_scaled.head()","d554a182":"tsne_data = tsne.fit_transform(dependent_scaled[:10000])","7d2459a8":"new_tsne_df = pd.DataFrame(data = tsne_data, columns = ('1st','2nd'))","2e07f447":"new_tsne_df['label'] = label[:10000]","5c938b95":"sns.FacetGrid(new_tsne_df, hue='label', height=8) \\\n    .map(plt.scatter, '1st', '2nd') \\\n    .add_legend()\nplt.show()\n# and BINGO!!","22f71a09":"tsne3d = TSNE(n_components = 3, n_jobs = -1).fit_transform(dependent_scaled[:10000])","50232b86":"tsne_df = pd.DataFrame(tsne3d, columns=['1st', '2nd', '3rd'])\ntsne_df['label'] = label[:10000]","85bb4f81":"fig = px.scatter_3d(tsne_df, x='1st', y='2nd', z='3rd', color='label', title='TSNE 3-D')\nfig.show()","a7df4af7":"TSNE has 2 very important hyperparameters.\n\n1) Perplexity => this refers to how many points does u want to consider for finding the probabilty of them being close by.\n\n2) n_iter => number of iterations u want to perform before aborting.\n\nYou should generally take high value for n_iter (say 2000 to 5000), and  observe the t-SNE plot wheather it is reaching to some stable form after n_iter number of iterations.\nIn short try different perplexity with constant high iterations.\n\nAfter trying different values I am going for the default values of \"perplexity\" and \"n_iter\" in this case.","e1ac2d07":"PCA class has a parameter \"n_components\" which decides the number of dimensions we want after reduction.","47fccb86":"n_components means to what dimensions you want to reduce the dataset to.\nTSNE support only 2 and 3 dimensions","8a9ac622":"## Stage-6 Plotting the t-SNE \n\nfinally we will see how good does t-SNE work.","d23e2c9c":"## Stage-2 Standardization of Data\n\nWell PCA tries to preserve the maximum variance along a dimension by projecting the features on that dimension so it becomes necessary to scale the features to not give a particular feature advantage.\n","13dcf7b8":"## Stage-5 Understanding and Applying t-SNE\n\nt-SNE = Student's t distributed Stoschastic Neighbourhood Embedding\n\nIn brief t-SNE assigns high probabilty over a pair to the points which are closer to each other in high dimension and low probabilty to those which are far apart. Now it tries to replicate that probabilty distribution in the low dimension and that helps it in preserving the structure.\n\nIn t-SNE centering shouldn't matter since the algorithm only operates on distances between points, however rescaling is necessary if you want the different dimensions to be treated with equal importance, since the 2-norm will be more heavily influenced by dimensions with large variance so we will still use standardised data.\n\nWell TSNE is and iterative algorithm.","bc0b6b0e":"Hope you liked it.\nIf found useful please upvote \ud83d\udc4d\ud83d\udc4d\n\nHere is a link for deeper visualisation for t-SNE, I recommend you visit : https:\/\/distill.pub\/2016\/misread-tsne\/ with this you will be able to tune TSNE for better results.","02eebe42":"# t-SNE vs PCA vs LDA\nI recently got to know about t-SNE and was very suprised to see the difference in the results among t-SNE and PCA, So I wanted to share what I found in the implementation\nof this amazing algorithm.","46c86486":"well as you can see that the results are not that great. Majority of the points are colliding in a cluster or so and that happens because in the higher dimension they have a proper structure which is completey lost when trying to apply PCA. Well don't get disappointed the next thing up (t-SNE) does a amazing job.","22ba2f6b":"### Visualising a row of the data and cross-checking the loaded data","1128b367":"## Stage-4 Plotting PCA results ","e0f5b2ae":"Now we have to add our \"pca_data\" and \"label\" to plot them over labels.\n\nTo do so we will use numpy function of vertical stack and then convert them into a dataframe.","5508e481":"Again to plot the results we will have to make a new dataset with labels and tsne_data and the make a dataframe.","0cf46ecb":"## Stage-3 Understanding and Applying PCA\n\nKeeping it brief PCA tries to maximize the varience (range of feature) so as to keep the most important features of datasets preserved, but it has some flaws in it and that is it can't preserve the structure of the dataset which is solved by t-SNE.\n\nexample :- For simplisity let us take a 2-dim dataset and reduce it to a 1-dim dataset.\n\n\nIn the left image we could see some 2-dim. dataset without standardisation.\n\nIn the right side image after standardisation the PCA looks for such a dimension\/vector that the variance after the projection on the new_plane is maximum. In this case that new_plane is f' and this new_plane will become our new 1-Dim\/featured dataset\n\n![Untitled.png](attachment:Untitled.png)","12304c5d":"### Let's See how does  kernel PCA fare (to reduce complexity we will use only 10k rows)","848810de":"#### None of the kernel PCA give satisfactory seperation, now let's try LDA ","81ee9b29":"## Stage-1 Loading the dataset\nwe will use the \"digit recognizer\" dataset to see the visual differences of the outcomes after dimansanality reduction from PCA and t-SNE.\n\n1) Import the normal Libraries\n\n2) Load the dataset\n\n3) Split the dataset in dependent and independent variables","7902fc94":"## PCA in 3-D ","ddd910dd":"We could see that even the 3-D don't provide much of a seperation. It is really messy.","4cabfb0e":"### LDA gave much better results than PCA and kernel_pca, but can we get something even better, let's try TSNE.","5af09be9":"As you can see in compare to PCA t-SNE has done tarrific work in dimensanality reduncation.","8d5fb5e2":"Amazing results. We could easily see the different layers of clysters."}}