{"cell_type":{"baa7b61e":"code","22cfbac8":"code","4e8bc357":"code","a0fb195f":"code","73684e91":"code","f25132ce":"code","c1fcb01c":"code","9ec97771":"code","195ac948":"code","c3d4c4cf":"code","c6f1269d":"code","c0cf58c8":"code","9d5f849c":"code","d47c09ec":"code","1c9c05d9":"code","f9b5bb84":"code","de4077a4":"code","4cad2b2c":"code","c8bedb69":"code","df9cf6b1":"code","e45e8f2b":"code","b08d1efe":"code","8082c1b7":"code","37cd6fe8":"code","55ed0f92":"code","b5364a58":"code","795941a1":"code","c592ebb5":"code","cc12b89b":"code","eb3990ad":"code","2f07aaf0":"code","a7b187c8":"markdown","d3809ac0":"markdown","991604a9":"markdown","90fc8194":"markdown","1e2a8f1d":"markdown","b5415c39":"markdown","d17feade":"markdown","65c44531":"markdown","c4e418ac":"markdown","091a79fe":"markdown","b5a81a82":"markdown","901b8bc2":"markdown","a730575c":"markdown","753444e3":"markdown","573d245f":"markdown","fc864fe1":"markdown"},"source":{"baa7b61e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Text Preprocessing libraries\nimport nltk\nnltk.download('stopwords')\nimport re \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","22cfbac8":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","4e8bc357":"#Training data set\ntrain.head(10)","a0fb195f":"test.head(10)","73684e91":"train['text'][152]","f25132ce":"print('Shape of Training data:-',train.shape)\nprint('Shape of Test data:-',test.shape) ","c1fcb01c":"#Null values\ntrain.isnull().sum()","9ec97771":"test.isnull().sum()","195ac948":"#train.drop(columns=['keyword','location'],axis='columns',inplace=True)\n#test.drop(columns=['keyword','location'],axis='columns',inplace=True)","c3d4c4cf":"#My target values\ntrain['target'].value_counts()","c6f1269d":"train.describe()","c0cf58c8":"# prettier graphs!\nplt.style.use('ggplot')","9d5f849c":"target_counts=train.target.value_counts()\nsns.barplot(y=target_counts,x=target_counts.index)\nplt.title(\"Counting the values in target column\")\nplt.ylabel('Sample')\nplt.xlabel('Target')\n","d47c09ec":"my_labels=['Non-Disaster','Disaster']\nmy_color=['Blue','Green']\nplt.figure(figsize=(15,7))\nplt.pie(train['target'].value_counts(),labels=my_labels,colors=my_color,autopct='%1.1f%%')\nplt.legend()\nplt.show()","1c9c05d9":"my_disaster_tweets=train[train['target']==1]['text']\nmy_disaster_tweets[:10]","f9b5bb84":"non_disaster_tweets=train[train['target']==0]['text']\nnon_disaster_tweets[:10]","de4077a4":"plt.figure(figsize=(15,10))\nwc=WordCloud(max_words=500,background_color='White',width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train[train.target==1].text))\nplt.imshow(wc,interpolation='bilinear')","4cad2b2c":"plt.figure(figsize=(15,10))\nwc=WordCloud(max_words=500,background_color='White',width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train[train.target==0].text))\nplt.imshow(wc,interpolation='bilinear')","c8bedb69":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(18,5))\nchar_len=train[train['target']==1]['text'].str.len()\nax1.hist(char_len,color='#db680f',edgecolor='black')\nax1.set_title('Disaster Tweets')\nchar_len2=train[train['target']==0]['text'].str.len()\nax2.hist(char_len2,color='#03639e',edgecolor='black')\nax2.set_title('Non-Disater Tweets')\nplt.suptitle(\"Length of Characters in text\",fontsize=20)\nplt.show()","df9cf6b1":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(18,5))\nchar_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(char_len,color='#c40a0d',edgecolor='black')\nax1.set_title('Disaster Tweets')\nchar_len2=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(char_len2,color='#0893a6',edgecolor='black')\nax2.set_title('Non-Disater Tweets')\nplt.suptitle(\"Length of words in text\",fontsize=20)\nplt.show()","e45e8f2b":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(18,5))\nchar_len_dis = train[train['target']==1]['text'].str.split().apply(lambda x:  [len(i) for i in x])\nsns.distplot(char_len_dis.map(lambda x: np.mean(x)),ax=ax1,color='green')\nax1.set_title(\"Disaster Tweets\")\nchar_len_ndis= train[train['target']==0]['text'].str.split().apply(lambda x:  [len(i) for i in x])\nsns.distplot(char_len_ndis.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title(\"Non-Disaster Tweets\")\nplt.suptitle(\"Average word counts\",fontsize=20)\nplt.show()","b08d1efe":"def sample_corpus(target):\n  corpus=[]\n  for x in train[train['target']==target]['text'].str.split():\n    for i in x:\n      corpus.append(i)\n  return corpus","8082c1b7":"from collections import defaultdict\n\ndef stopwords_analysis(data,func,target):\n  value_list=[]\n  for labels in range(0,len(target)):\n    dic=defaultdict(int)\n    corpus = func(target[labels])\n\n    for words in corpus:\n      dic[words]+=1\n    top=sorted(dic.items(),key=lambda x: x[1],reverse=True)[:20]\n    x_items,y_values=zip(*top)\n    value_list.append(x_items)\n    value_list.append(y_values)\n\n  #ploting the the figure\n  fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,8))\n  ax1.barh(value_list[0],value_list[1],color='b')\n  ax1.set_title(\"Non-Disaster Tweets\")\n\n  ax2.barh(value_list[2],value_list[3],color='red')\n  ax2.set_title(\"Disaster Tweets\")\n\n  plt.suptitle(\"Top Stop words in text\")\n  plt.show()\n\nstopwords_analysis(train,sample_corpus,[0,1])","37cd6fe8":"import string \n\ndef punctuation_analysis(data,func,target):\n    values_list = []\n    special = string.punctuation\n    for labels in range(0,len(target)):\n        dic = defaultdict(int)\n        corpus = func(target[labels])\n        for i in corpus:\n            if i in special:\n                dic[i]+=1\n        x_items,y_values = zip(*dic.items())\n        values_list.append(x_items)\n        values_list.append(y_values)\n\n    #ploting the the figure\n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n    ax1.bar(values_list[0],values_list[1],color=\"b\", linewidth=1.2)\n    ax1.set_title(\"Non-Disaster Tweets\")\n    \n    ax2.bar(values_list[2],values_list[3],color=\"red\",edgecolor='black', linewidth=1.2)\n    ax2.set_title(\"Disaster Tweets\")\n            \n    plt.suptitle(\"Punctuations in text\")\n    plt.show()\n\n\n\npunctuation_analysis(train,sample_corpus,[0,1])","55ed0f92":"# Checking Null values\nmissing_train = train.isnull().sum()  \nmissing_test = test.isnull().sum()  \nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\nmissing_train = missing_train[missing_train>0].sort_values()\nax1.pie(missing_train,autopct='%1.1f%%',startangle=30,explode=[0.9,0],labels=[\"keyword\",\"location\"],colors=['red','#afe84d'])\nax1.set_title(\"Null values present in Train Dataset\")\n\nmissing_test = missing_test[missing_test>0].sort_values()\nax2.pie(missing_test,autopct='%1.1f%%',startangle=30,explode=[0.9,0],labels=[\"keyword\",\"location\"],colors=['Red','#6c1985'])\nax2.set_title(\"Null values present in Test Dataset\")\nplt.suptitle(\"Distribution of Null Values in Dataset\")\nplt.tight_layout()\nplt.show()","b5364a58":"!pip install contractions","795941a1":"stop_words=nltk.corpus.stopwords.words('english')\ni=0\n#sc=SpellChecker()\n#data=pd.concat([train,test])\nimport contractions\nfrom nltk.stem import SnowballStemmer\nnltk.download('wordnet')\nnltk.download('punkt')\nwnl=WordNetLemmatizer()\nstemmer=SnowballStemmer('english')\nfor doc in train.text:\n  doc=re.sub(r'https?:\/\/\\S+|www\\.\\S+','',doc)\n  doc=re.sub(r'<.*?>','',doc)\n  doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\n  #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\n  doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\n  #doc=' '.join([sc.correction(i) for i in doc.split()])\n  doc=contractions.fix(doc)\n  tokens=nltk.word_tokenize(doc)\n  filtered=[token for token in tokens if token not in stop_words]\n  doc=' '.join(filtered)\n  train.text[i]=doc\n  i+=1\ni=0\nfor doc in test.text:\n  doc=re.sub(r'https?:\/\/\\S+|www\\.\\S+','',doc)\n  doc=re.sub(r'<.*?>','',doc)\n  doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\n  #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\n  doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\n  #doc=' '.join([sc.correction(i) for i in doc.split()])\n  doc=contractions.fix(doc)\n  tokens=nltk.word_tokenize(doc)\n  filtered=[token for token in tokens if token not in stop_words]\n  doc=' '.join(filtered)\n  test.text[i]=doc\n  i+=1","c592ebb5":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(ngram_range=(1,1)) \n\n#    ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, \n#    and (2, 2) means only bigrams.\n\ncv_matrix=cv.fit_transform(train.text).toarray()\ntrain_df=pd.DataFrame(cv_matrix,columns=cv.get_feature_names())\ntest_df=pd.DataFrame(cv.transform(test.text).toarray(),columns=cv.get_feature_names())\ntrain_df.head()","cc12b89b":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(ngram_range=(1,1),use_idf=True)\nmat=tfidf.fit_transform(train.text).toarray()\ntrain_df=pd.DataFrame(mat,columns=tfidf.get_feature_names())\ntest_df=pd.DataFrame(tfidf.transform(test.text).toarray(),columns=tfidf.get_feature_names())\ntrain_df.head()","eb3990ad":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nmodel=LogisticRegression()\nmodel.fit(train_df,train.target)\nprint(f1_score(model.predict(train_df),train.target))\npred=model.predict(test_df)","2f07aaf0":"pd.DataFrame({\n    'id':test.id,\n    'target':pred\n}).to_csv('submission.csv',index=False)","a7b187c8":"Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.","d3809ac0":"# Logistic Regression","991604a9":"From the above distributions, it can be observed that the average word count for disaster tweets are found to be in the range(7-7.5) while for non-disaster tweets are in the range of (4.5-5).","90fc8194":"**Defaultdict** is a container like dictionaries present in the module collections. Defaultdict is a sub-class of the dict class that returns a dictionary-like object. ","1e2a8f1d":"The above Bar Charts displays the top 10 punctuations in tweets. From the bar chart, it is observed that the most occuring punctuation in both disaster\/non-disaster tweets is \"-\"(350+) while the least occuring for non-disaster are \"%\",\"\/:\",\"$\",\"_\" and for disaster tweets is \"=>\", \")\".","b5415c39":"# Importing Libraries ","d17feade":"#Character Length","65c44531":"Let's start by analysing total number of characters in text.","c4e418ac":"Checking shape of train and test datasets. Note that the test dataset does not have 'target' column.","091a79fe":"# Exploratory Data Analysis (EDA)","b5a81a82":"#WordCloud","901b8bc2":"Checking which all columns contain NaN values(is missing). 'location' is missing a lot in both the train and test data sets\n\n","a730575c":"From the above histograms, it can be observed that the words count for disaster and non-disaster tweets are in the range of (15-20).","753444e3":"#Average word length","573d245f":"\"**Corpus** is a large collection of texts. It is a body of written or spoken material upon which a linguistic analysis is based. \"","fc864fe1":"# Analysing number of words in text."}}