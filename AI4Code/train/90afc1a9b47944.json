{"cell_type":{"0a6f84a2":"code","58b22d9c":"code","ab4f34f0":"code","17a5aac0":"code","b911f5c0":"code","e210a001":"code","f30188d4":"code","5ac44fba":"code","958d7548":"code","652d0006":"code","05f1d537":"code","f5c00e34":"code","1a938a40":"code","84c119ab":"code","18aa038e":"code","d1615825":"code","1d01c61a":"code","39a9d059":"code","e8635b14":"code","390c4bee":"code","b6868a3a":"code","e315f617":"code","7abe119f":"code","6af62acc":"code","b7620f56":"code","9bf4f5b7":"code","f00bd1e0":"code","af9dfba2":"code","f66f6eb4":"code","d8a5a8f5":"code","97fbb2a9":"code","fd3ad719":"code","581d9ddb":"code","885015f4":"code","28be073f":"code","ef0526e3":"code","ce90ab67":"code","dc4089ab":"code","a45cff67":"code","48064459":"code","90d9d6ae":"code","e1b5a9a2":"markdown","efc89b19":"markdown","79ee5982":"markdown","2d2a9e53":"markdown","2e42aef2":"markdown","67d0a78c":"markdown","632da281":"markdown"},"source":{"0a6f84a2":"# Importing necessary libraries\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nboston = load_boston()","58b22d9c":"print(boston.data.shape)","ab4f34f0":"print(boston.feature_names)","17a5aac0":"print(boston.target.shape)","b911f5c0":"print(boston.DESCR)","e210a001":"# Loading data into pandas dataframe\nbos = pd.DataFrame(boston.data)\nprint(bos.head())","f30188d4":"#noramlization for fast convergence to minima\n#bos = (bos - bos.mean())\/bos.std()\n#bos.head()","5ac44fba":"bos['PRICE'] = boston.target\n\nX = bos.drop('PRICE', axis = 1)\nY = bos['PRICE']","958d7548":"# Split data into train and test\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","652d0006":"X_train.mean()","05f1d537":"# Standardization\n\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nX_train = std.fit_transform(X_train)\nX_test = std.fit_transform(X_test)","f5c00e34":"X_train","1a938a40":"'''\n# code source:https:\/\/medium.com\/@haydar_ai\/learning-data-science-day-9-linear-regression-on-boston-housing-dataset-cd62a80775ef\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\nlm = LinearRegression()\nlm.fit(X_train, Y_train)\n\nY_pred = lm.predict(X_test)\n\nprint('Coefficients: \\n', lm.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(Y_test, Y_pred))\n# Explained variance score: 1 is perfect prediction\n# print(\"R^2 score: %.2f\"  % lm.score(X_test, Y_test))\nprint('Variance score: %.2f' % r2_score(Y_test, Y_pred))\n\nplt.scatter(Y_test, Y_pred)\nplt.xlabel(\"Prices: $Y_i$\")\nplt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\nplt.title(\"Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$\")\nplt.show()\n'''","84c119ab":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nclf = SGDRegressor()\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\n\nprint(\"Coefficients: \\n\", clf.coef_)\nprint(\"Y_intercept\", clf.intercept_)","18aa038e":"# Imported necessary libraries\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np","d1615825":"# Data loaded \nbostan = load_boston()","1d01c61a":"# Data shape\nbostan.data.shape","39a9d059":"# Feature name\nbostan.feature_names","e8635b14":"# This is y value i.e. target\nbostan.target.shape","390c4bee":"# Convert it into pandas dataframe\ndata = pd.DataFrame(bostan.data, columns = bostan.feature_names)\ndata.head()","b6868a3a":"# Statistical summary\ndata.describe()","e315f617":"#noramlization for fast convergence to minima\ndata = (data - data.mean())\/data.std()\ndata.head()","7abe119f":"data.mean()","6af62acc":"\n#from sklearn.preprocessing import StandardScaler\n#std = StandardScaler()\n#data = std.fit_transform(data)\n#data","b7620f56":"# MEDV(median value is usually target), change it to price\ndata[\"PRICE\"] = bostan.target\ndata.head()","9bf4f5b7":"# Target and features\nY = data[\"PRICE\"]\nX = data.drop(\"PRICE\", axis = 1)","f00bd1e0":"\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n","af9dfba2":"#x_train = (x_train - x_train.mean())\/ x_train.std()\n#x_test = (x_test - x_train.mean())\/ x_test.std()","f66f6eb4":"#std = StandardScaler()\n#x_train = std.fit_transform(x_train)\n#x_test = std.fit_transform(x_test)","d8a5a8f5":"#x_train[0:,0:5]","97fbb2a9":"x_train[\"PRICE\"] = y_train\n#x_test[\"PRICE\"] = y_test","fd3ad719":"#x_train[\"PRICE\"] = y_train\n#x_test[\"PRICE\"] = y_test","581d9ddb":"def cost_function(b, m, features, target):\n    totalError = 0\n    for i in range(0, len(features)):\n        x = features\n        y = target\n        totalError += (y[:,i] - (np.dot(x[i] , m) + b)) ** 2\n    return totalError \/ len(x)","885015f4":"# The total sum of squares (proportional to the variance of the data)i.e. ss_tot \n# The sum of squares of residuals, also called the residual sum of squares i.e. ss_res \n# the coefficient of determination i.e. r^2(r squared)\ndef r_sq_score(b, m, features, target):\n    for i in range(0, len(features)):\n        x = features\n        y = target\n        mean_y = np.mean(y)\n        ss_tot = sum((y[:,i] - mean_y) ** 2)\n        ss_res = sum(((y[:,i]) - (np.dot(x[i], m) + b)) ** 2)\n        r2 = 1 - (ss_res \/ ss_tot)\n    return r2","28be073f":"def gradient_decent(w0, b0, train_data, x_test, y_test, learning_rate):\n    n_iter = 500\n    partial_deriv_m = 0\n    partial_deriv_b = 0\n    cost_train = []\n    cost_test = []\n    for j in range(1, n_iter):\n        \n        # Train sample\n        train_sample = train_data.sample(160)\n        y = np.asmatrix(train_sample[\"PRICE\"])\n        x = np.asmatrix(train_sample.drop(\"PRICE\", axis = 1))\n        # Test sample\n        #x_test[\"PRICE\"] = [y_test]\n        #test_data = x_test\n        #test_sample = test_data.sample()\n        #y_test = np.asmatrix(test_sample[\"PRICE\"])\n        #x_test = np.asmatrix(test_sample.drop(\"PRICE\", axis = 1))\n        \n        for i in range(len(x)):\n            partial_deriv_m += np.dot(-2*x[i].T , (y[:,i] - np.dot(x[i] , w0) + b0))\n            partial_deriv_b += -2*(y[:,i] - (np.dot(x[i] , w0) + b0))\n        \n        w1 = w0 - learning_rate * partial_deriv_m \n        b1 = b0 - learning_rate * partial_deriv_b\n        \n        if (w0==w1).all():\n            #print(\"W0 are\\n\", w0)\n            #print(\"\\nW1 are\\n\", w1)\n            #print(\"\\n X are\\n\", x)\n            #print(\"\\n y are\\n\", y)\n            break\n        else:\n            w0 = w1\n            b0 = b1\n            learning_rate = learning_rate\/2\n       \n            \n        error_train = cost_function(b0, w0, x, y)\n        cost_train.append(error_train)\n        error_test = cost_function(b0, w0, np.asmatrix(x_test), np.asmatrix(y_test))\n        cost_test.append(error_test)\n        \n        #print(\"After {0} iteration error = {1}\".format(j, error_train))\n        #print(\"After {0} iteration error = {1}\".format(j, error_test))\n        \n    return w0, b0, cost_train, cost_test","ef0526e3":"# Run our model\nlearning_rate = 0.001\nw0_random = np.random.rand(13)\nw0 = np.asmatrix(w0_random).T\nb0 = np.random.rand()\n\noptimal_w, optimal_b, cost_train, cost_test = gradient_decent(w0, b0, x_train, x_test, y_test, learning_rate)\nprint(\"Coefficient: {} \\n y_intercept: {}\".format(optimal_w, optimal_b))\n\n'''\nerror = cost_function(optimal_b, optimal_w, np.asmatrix(x_test), np.asmatrix(y_test))\nprint(\"Mean squared error:\",error)\n'''\n\nplt.figure()\nplt.plot(range(len(cost_train)), np.reshape(cost_train,[len(cost_train), 1]), label = \"Train Cost\")\nplt.plot(range(len(cost_test)), np.reshape(cost_test, [len(cost_test), 1]), label = \"Test Cost\")\nplt.title(\"Cost\/loss per iteration\")\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Cost\/Loss\")\nplt.legend()\nplt.show()\n\n#error = cost_function(optimal_b, optimal_w, np.asmatrix(x_test), np.asmatrix(y_test))\n#print(\"Mean squared error: %.2f\" % error)","ce90ab67":"# Sklearn SGD\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(Y_test, Y_pred))\n# Explained variance score: 1 is perfect prediction\nprint(\"Variance score: %.2f\" % r2_score(Y_test, Y_pred))","dc4089ab":"# Implemented SGD\n# The mean squared error\nerror = cost_function(optimal_b, optimal_w, np.asmatrix(x_test), np.asmatrix(y_test))\nprint(\"Mean squared error: %.2f\" % (error))\n# Explained variance score : 1 is perfect prediction\nr_squared = r_sq_score(optimal_b, optimal_w, np.asmatrix(x_test), np.asmatrix(y_test))\nprint(\"Variance score: %.2f\" % r_squared)","a45cff67":"# Scatter plot of test vs predicted\n# sklearn SGD\nplt.figure(1)\nplt.subplot(211)\nplt.scatter(Y_test, Y_pred)\nplt.xlabel(\"Prices: $Y_i$\")\nplt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\nplt.title(\"Prices vs Predicted prices: Sklearn SGD\")\nplt.show()\n\n# Implemented SGD\nplt.subplot(212)\nplt.scatter([y_test], [(np.dot(np.asmatrix(x_test), optimal_w) + optimal_b)])\nplt.xlabel(\"Prices: $Y_i$\")\nplt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\nplt.title(\"Prices vs Predicted prices: Implemented SGD\")\nplt.show()","48064459":"# Distribution of error\ndelta_y_im = np.asmatrix(y_test) - (np.dot(np.asmatrix(x_test), optimal_w) + optimal_b)\ndelta_y_sk = Y_test - Y_pred\nimport seaborn as sns;\nimport numpy as np;\nsns.set_style('whitegrid')\nsns.kdeplot(np.asarray(delta_y_im)[0], label = \"Implemented SGD\", bw = 0.5)\nsns.kdeplot(np.array(delta_y_sk), label = \"Sklearn SGD\", bw = 0.5)\nplt.title(\"Distribution of error: $y_i$ - $\\hat{y}_i$\")\nplt.xlabel(\"Error\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()","90d9d6ae":"# Distribution of predicted value\nsns.set_style('whitegrid')\nsns.kdeplot(np.array(np.dot(np.asmatrix(x_test), optimal_w) + optimal_b).T[0], label = \"Implemented SGD\")\nsns.kdeplot(Y_pred, label = \"Sklearn SGD\")\nplt.title(\"Distribution of prediction $\\hat{y}_i$\")\nplt.xlabel(\"predicted values\")\nplt.ylabel(\"Density\")\nplt.show()","e1b5a9a2":"# Stochastic Gradient Decent(SGD) for Linear Regression","efc89b19":"In this kernel we will be implementing SGD on LinearRegression from scarch using python and we will be also comparing sklearn implementation SGD and our implemented SGD.","79ee5982":"**observations**\n* The mean squared error(mse) is quite high means the regression line does not fit the data properly. i.e. average squared difference between the actual target value and predicted target value is high. lower value is better.\n* r-squared score is 0.88, means the fit explain 88% of the total variation in the data about the average.\n* After looking at the error graph we can say +ve side of the graph, error is more.\n* By looking at the distribution of predicted value graph, It is clear that prediction of implemented SGD and sklearn SGD both are ovelapping(not perfectly) but the density of sklearn SGD is ~58% whereas implemented SGD is ~62% means the implemented SGD is predicting high but in actual it is not.","2d2a9e53":"# Comparison between sklearn SGD and implemented SGD in python ","2e42aef2":"**Observations**\n\n* Overall we can say the regression line not fits data perfectly but it is okay. But our goal is to find the line\/plane that best fits our data means minimize the error i.e. mse should be close to 0.\n* MSE is 28.54 means the total loss(squared difference of true\/actual target value and predicted target value). 0.0 is perfect i.e. no loss.\n* coefficient of determination tells about the goodness of fit of a model and here, r^2 is 0.70 which means regression prediction does not perfectly fit the data. An r^2 of 1 indicates that regression prediction perfect fit the data.","67d0a78c":"# Boston House price prediction using SGD","632da281":"**Conclusions**\n* While comparing scikit-learn implemented linear regression and explicitly implemented linear regression using optimization algorithm(sgd) in python we see there are not much differences between both of them.\n* Both of the model are not perfect but okay."}}