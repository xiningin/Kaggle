{"cell_type":{"f73304c0":"code","9051ed1d":"code","b653e373":"code","2734b04d":"code","ed50f106":"code","503f2750":"code","9fbb6847":"code","9dd4c560":"code","60746837":"code","7f89ed48":"code","154f0bdb":"code","ea7a4b3e":"code","870dfeb4":"code","06f40b13":"code","d08c4fae":"code","37b634fb":"code","99d1e046":"code","b3584277":"code","a531e2e7":"code","1125b48f":"code","74e16320":"markdown","ca2c6e5c":"markdown","13e87224":"markdown","eba68d8b":"markdown","59fc257e":"markdown","83a19c58":"markdown","4c669c62":"markdown","24c75f77":"markdown","88638bf4":"markdown","12872586":"markdown","23f73637":"markdown","f36d720a":"markdown","fd4a1f63":"markdown","f7ac695b":"markdown","4ff78802":"markdown"},"source":{"f73304c0":"%%bash\n\necho \"TPU_DEPS_INSTALLED=${TPU_DEPS_INSTALLED:-}\"\nif [[ -z \"${TPU_DEPS_INSTALLED:-}\" ]]; then\n    echo \"Installing TPU dependencies.\"\n    pip install --upgrade pip\n# Nightly builds can be unstable, I have been discussing this with others, and now I take \n# this advise from @Kirderf and apply it myself \n# python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n    curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py \\\n         -o pytorch-xla-env-setup.py\n    python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev # --version nightly\n    export TPU_DEPS_INSTALLED=true\n    echo \"TPU dependencies installed.\"\nelse\n   echo \"TPU dependencies already exist. Skipping step.\"\nfi","9051ed1d":"%%bash\nexport XLA_USE_BF16=1\nexport XRT_TPU_CONFIG=\"tpu_worker;0;10.240.1.2:8470\"\nls -lash *.whl \nrm -fr *.whl || true","b653e373":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nimport transformers\nfrom transformers import RobertaModel, RobertaConfig\n\nwarnings.filterwarnings('ignore')","2734b04d":"from transformers import get_linear_schedule_with_warmup, AdamW\n\nimport torch_xla.core.xla_model as xm # using TPUs\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\n\nfrom tqdm.autonotebook import tqdm\n\nfrom joblib import Parallel, delayed\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ed50f106":"def print_to_console(string_to_print, end='\\n', flush=False):\n    if accelerator_device == \"tpu\":\n        xm.master_print(string_to_print) \n    else:\n        print(string_to_print, end=end, flush=flush)","503f2750":"accelerator_device = \"tpu\"\n# import tensorflow as tf\n# print(f'Tensorflow version {tf.__version__}')\n\nimport os\n\nTPU_WORKER = os.environ[\"TPU_NAME\"]\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# print(f\"Running on TPU: {tpu.cluster_spec().as_dict()['worker']}\")\nprint_to_console(f\"TPU_WORKER: {TPU_WORKER}\")\n    \n# tf.config.experimental_connect_to_cluster(tpu)\n# strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# REPLICAS_OR_WORKERS = strategy.num_replicas_in_sync\nREPLICAS_OR_WORKERS = xm.xrt_world_size()\nprint_to_console(f'REPLICAS: {REPLICAS_OR_WORKERS}')\nTPU_CORES=8\nprint_to_console(f'TPU_CORES: {TPU_CORES}')","9fbb6847":"cpu_count = os.cpu_count()\nMAX_LEN = 128\nmultiple_workers = REPLICAS_OR_WORKERS > 1\nTRAIN_BATCH_SIZE = 32 # originall 16 * REPLICAS_OR_WORKERS\nVALID_BATCH_SIZE = 16\nROBERTA_PATH = \"..\/input\/roberta-base\"","9dd4c560":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n\nseed = 42\nseed_everything(seed)","60746837":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len=MAX_LEN): # original max_len=96\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file='..\/input\/roberta-base\/vocab.json', \n            merges_file='..\/input\/roberta-base\/merges.txt', \n            lowercase=True,\n            add_prefix_space=True)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        \n        ids, masks, tweet, offsets = self.get_input_data(row)\n                \n        data = {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'masks': torch.tensor(masks, dtype=torch.long),\n            'tweet': tweet,\n            'offsets': torch.tensor(offsets, dtype=torch.long),\n        }        \n    \n        if self.labeled:\n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = torch.tensor(start_idx, dtype=torch.long) \n            data['end_idx'] = torch.tensor(end_idx, dtype=torch.long)\n\n        return data\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        tweet = \" \" + \" \".join(row.text.lower().split())\n        encoding = self.tokenizer.encode(tweet)\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n                \n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        ids = torch.tensor(ids)\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        offsets = torch.tensor(offsets)\n        \n        return ids, masks, tweet, offsets\n        \n    def get_target_idx(self, row, tweet, offsets):\n        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n\n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx\n        \ndef get_train_val_loaders(df, train_idx, val_idx, batch_size=TRAIN_BATCH_SIZE):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        TweetDataset(train_df),\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True\n    )\n    train_loader = torch.utils.data.DataLoader(\n        TweetDataset(train_df), \n        batch_size=TRAIN_BATCH_SIZE, \n        num_workers=cpu_count, # num_workers=2\n        sampler=train_sampler,\n        drop_last=True)\n\n    val_sampler = torch.utils.data.distributed.DistributedSampler(\n        TweetDataset(val_df),\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True\n    )\n    val_loader = torch.utils.data.DataLoader(\n        TweetDataset(val_df), \n        batch_size=VALID_BATCH_SIZE,\n        sampler=val_sampler,\n        num_workers=2) # num_workers=2\n\n    dataloaders_dict = {\"Training\": train_loader, \"Validation\": val_loader}\n\n    return dataloaders_dict\n\ndef get_test_loader(df, batch_size=VALID_BATCH_SIZE):\n    test_sampler = torch.utils.data.distributed.DistributedSampler(\n        TweetDataset(df),\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True\n    )\n    loader = torch.utils.data.DataLoader(\n        TweetDataset(df), \n        batch_size=VALID_BATCH_SIZE,\n        sampler=test_sampler,\n        num_workers=2) # num_workers=2\n    return loader","7f89ed48":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        \n        config = RobertaConfig.from_pretrained(\n            f'{ROBERTA_PATH}\/config.json', output_hidden_states=True)    \n        config.output_hidden_states = True\n        self.roberta = RobertaModel.from_pretrained(\n            f'{ROBERTA_PATH}\/pytorch_model.bin', config=config)\n\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(config.hidden_size, 2)\n        nn.init.normal_(self.fc.weight, std=0.02)\n        nn.init.normal_(self.fc.bias, 0)\n\n    def forward(self, input_ids, attention_mask):\n        _, _, hs = self.roberta(input_ids, attention_mask)\n         \n        x = torch.stack([hs[-1], hs[-2], hs[-3]])\n        x = torch.mean(x, 0)\n        x = self.dropout(x)\n        x = self.fc(x)\n        start_logits, end_logits = x.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n                \n        return start_logits, end_logits","154f0bdb":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss","ea7a4b3e":"def get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    return jaccard(true, pred)","870dfeb4":"def set_to_device(data_, field_name, device, data_type=torch.long):\n    field = data_[field_name]\n    return field.to(device, dtype=data_type)","06f40b13":"def train_model(device, model, dataloaders_dict, criterion, optimizer, num_epochs, filename, scheduler):\n    for epoch in range(num_epochs):\n        for phase in ['Training', 'Validation']:\n            if phase == 'Training':\n                print_to_console(f'Started training Epoch: {epoch + 1}\/{num_epochs}')\n                model.train()\n            else:\n                print_to_console(f'Started validation Epoch: {epoch + 1}\/{num_epochs}')       \n                model.eval()\n\n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            \n            data_para_loader = pl.ParallelLoader(dataloaders_dict[phase], [device])\n            data_para_loader = data_para_loader.per_device_loader(device)\n            tk0 = tqdm(data_para_loader, total=len(dataloaders_dict[phase]), desc=f'{phase}: {epoch + 1}\/{num_epochs}, {filename}')\n            \n            for index, data in enumerate(tk0):\n                if index % 500 == 0:\n                    print_to_console(f'Started Training: index={index}\/{len(tk0)}')\n                ids = set_to_device(data, 'ids', device)\n                masks = set_to_device(data, 'masks', device)\n                tweet = data['tweet']\n                offsets = data['offsets'].cpu().detach().numpy()\n                start_idx = set_to_device(data, 'start_idx', device)\n                end_idx = set_to_device(data, 'end_idx', device)\n\n                model.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'Training'):\n                    \n                    optimizer.zero_grad()\n                    start_logits, end_logits = model(ids, masks)\n                    \n                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n                    \n                    if phase == 'Training':\n                        loss.backward()\n                        xm.optimizer_step(optimizer)\n                        scheduler.step()\n\n                    epoch_loss += loss.item() * len(ids)\n                    \n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n\n                    for i in range(len(ids)):                        \n                        jaccard_score = compute_jaccard_score(\n                            tweet[i],\n                            start_idx[i],\n                            end_idx[i],\n                            start_logits[i], \n                            end_logits[i], \n                            offsets[i])\n                        epoch_jaccard += jaccard_score\n                        if index % 500 == 0:\n                            print(f'{i}\/{len(ids)-1}: epoch_loss: {epoch_loss}, jaccard_score: {jaccard_score}', end=\"\\r\", flush=True)\n\n            epoch_loss = epoch_loss \/ len(dataloaders_dict[phase].dataset)\n            epoch_jaccard = epoch_jaccard \/ len(dataloaders_dict[phase].dataset)\n            \n            print_to_console('')\n            print_to_console('Epoch {}\/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n\n    print_to_console(f'Saving model to {filename}')\n    xm.save(model.state_dict(), filename)","d08c4fae":"num_epochs = 3\nfolds=10\nLEARNING_RATE = 4e-5\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)","37b634fb":"def get_optimizer_params(model):    \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\n        \"bias\",\n        \"LayerNorm.bias\",\n        \"LayerNorm.weight\"\n    ]\n\n    optimizer_parameters = [\n        {\n            'params': [\n                p for n, p in param_optimizer if not any(\n                    nd in n for nd in no_decay\n                )\n            ], \n         'weight_decay': 0.001\n        },\n        {\n            'params': [\n                p for n, p in param_optimizer if any(\n                    nd in n for nd in no_decay\n                )\n            ], \n            'weight_decay': 0.0\n        },\n    ]\n    \n    return optimizer_parameters","99d1e046":"def run(fold, train_idx, val_idx):\n    saved_model_filename = f'roberta_fold{fold}.pth'\n    print_to_console(f'Looking for saved model file {saved_model_filename}...')\n    if os.path.exists(saved_model_filename):\n        print_to_console(f'Model file {saved_model_filename} found, skipping process (delete model file in order to re-run the process...')\n        return\n    else:\n        print_to_console(f'Model file {saved_model_filename}, not found proceeding with model building process...')\n\n    print_to_console(\"\")\n    print_to_console(f'Fold: {fold}')\n    MX = TweetModel()\n    \n    if fold >= TPU_CORES:\n        fold = fold % TPU_CORES\n\n    device = xm.xla_device(fold + 1)\n    model = MX.to(device)\n    \n    num_train_steps = int(len(train_df) \/ TRAIN_BATCH_SIZE \/ REPLICAS_OR_WORKERS * num_epochs)\n    \n    num_train_steps = int(\n        len(train_df) \/ TRAIN_BATCH_SIZE * num_epochs\n    )\n    optimizer = AdamW(\n        get_optimizer_params(model), \n        lr=LEARNING_RATE * REPLICAS_OR_WORKERS, # xm.xrt_world_size()\n        betas=(0.9, 0.999)\n    )\n\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n        \n    num_batches = int(len(train_df) \/ TRAIN_BATCH_SIZE)\n\n    criterion = loss_fn    \n    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, TRAIN_BATCH_SIZE)\n\n    train_model(\n        device,\n        model, \n        dataloaders_dict,\n        criterion, \n        optimizer, \n        num_epochs,\n        saved_model_filename,\n        scheduler\n    )","b3584277":"%env JOBLIB_TEMP_FOLDER=\/tmp\n%env JOBLIB_START_METHOD=\"forkserver\"  ### commented out helped, usually its set to stop Parallel from hanging or going idle\n%env TMPDIR=\/tmp","a531e2e7":"%%time\ntrain_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntrain_df['text'] = train_df['text'].astype(str)\ntrain_df['selected_text'] = train_df['selected_text'].astype(str)\n\nis_incorrect_fn = lambda row: (\" \" + row.text + \" \").find(\" \" + row.selected_text + \" \") < 0\nfiltered_bad_data = train_df.apply(is_incorrect_fn, axis=1)\nprint(f\"train before correcting {train_df.shape}\")\ntrain_df = train_df[~filtered_bad_data].copy().reset_index(drop=True)\nprint(f\"train after correcting {train_df.shape}\")\n\nUSABLE_TPU_CORES=int(TPU_CORES*(3\/4))\nprint_to_console(f'USABLE_TPU_CORES: {USABLE_TPU_CORES}')\n\n### Note there are minor issues using joblib's Parallel\/delay with multiple TPUs\n### The process can hang on a training or validation session of a fold.\n### Workaround: wait till all other folds finish, click on Cancel Run and then re-run just this cell\n### As we have saved the other models, it will only start fromt the fold that has never been created (sort of pipeline architecture)\nParallel(n_jobs=USABLE_TPU_CORES, backend=\"threading\")(\n    delayed(run)(fold, train_idx, val_idx) for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment))\n)\n### Alternatively you can use xmp.spawn() see solution in the next cell, but it's still being worked on for the moment.\n","1125b48f":"!ls -lash *.pth","74e16320":"%%time\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=TPU_CORES, start_method='fork')","ca2c6e5c":"def _mp_fn(rank, flags):\n    xm.master_print(f'TPU worker {rank}: triggering task for this TPU worker')\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = run(rank, train_idx_list[rank], val_idx_list[rank])","13e87224":"### Credits and citations","eba68d8b":"### Alternative way to spawn \/ run tasks in parallel using Pytorch's MultiProc for XLA, instead of Joblib's parallels\n(please convert the following cells into code cells to execute them, may need some work still to get it to work instead of Parallel from joblib)","59fc257e":"# Seed","83a19c58":"### TPU installation","4c669c62":"# Loss Function","24c75f77":"# Data Loader","88638bf4":"# Model","12872586":"train_idx_list = {}\nval_idx_list = {}\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment))\n    train_idx_list[fold] = train_idx\n    val_idx_list[fold] = val_idx","23f73637":"Thanks to the tips on https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/159221, I upated my TPU\/Pytorch installation process.","f36d720a":"## Full credits to the origin author [@shoheiazuma](https:\/\/kaggle.com\/shoheiazuma) of the original notebook. But also big thanks to [@abhishek](https:\/\/www.kaggle.com\/abhishek) for his example notebooks and videos on how to build and run models on TPUs and multiple TPUs.\n\nI did some tidying and reorganisation of the code to learn more about how to switch code between CPU\/GPU\/TPU. There can be more improvements as we go along please join me in simplifying the process of writing and running code on CPUs, GPUs and TPUs. \n\nPlease feel free to answer there as well as comment below.\n\n#### Forked from https:\/\/www.kaggle.com\/shoheiazuma\/tweet-sentiment-roberta-pytorch\n\n### This is a training version of the notebook, the [inference version can be found here](https:\/\/www.kaggle.com\/neomatrix369\/tse2020-roberta-pytorch-multi-tpu-10-skfd-2-2).","fd4a1f63":"# Training Function","f7ac695b":"# Evaluation Function","4ff78802":"# Training"}}