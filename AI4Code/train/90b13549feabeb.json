{"cell_type":{"4ef61cb3":"code","b58fbcc8":"code","41d9d802":"code","afc7b7fd":"code","15e08e4c":"code","f38c3b84":"code","5d444b19":"code","c6d143e3":"code","94cc4e2c":"code","37aa805f":"code","858e1b2e":"code","8677dde7":"code","235a59b0":"code","3c1cb12a":"code","cb5935c5":"code","6252c0ee":"code","f4680d32":"code","dc657a30":"code","595aafb6":"code","85e64c37":"code","f6c19a90":"code","2684ad03":"code","0963a413":"code","e03682b9":"markdown","0f64df44":"markdown","f5e2fbe7":"markdown","883da5c9":"markdown","86f533ae":"markdown","4a442030":"markdown","66238299":"markdown","ec3d0ca5":"markdown","978fe2c8":"markdown","8274eb33":"markdown","36fc0a4c":"markdown","b63adbdc":"markdown","242c65cc":"markdown","f1684e0e":"markdown","95d5b4d1":"markdown","55d4c48a":"markdown","57db3ef2":"markdown"},"source":{"4ef61cb3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score, train_test_split, learning_curve, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, ElasticNet\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import make_pipeline\n","b58fbcc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","41d9d802":"df = pd.read_csv('\/kaggle\/input\/VLabs-DC\/sales_20_21_train.csv', header=0, parse_dates=['DT_VENDA'])\n# df = pd.read_csv('https:\/\/raw.githubusercontent.com\/marcos-mansur\/vlabs-challenge\/main\/Data\/sales_20_21_train.csv', header=0, parse_dates=['DT_VENDA'])\ndf_sub = pd.read_csv('\/kaggle\/input\/VLabs-DC\/sample_submission.csv',header=0)","afc7b7fd":"df.head()","15e08e4c":"def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate plot: learning curve\n\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, figsize=(20, 10))\n\n    axes.set_title(title)\n    if ylim is not None:\n        axes.set_ylim(*ylim)\n    axes.set_xlabel(\"Training examples\")\n    axes.set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes, scoring='neg_root_mean_squared_error',\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes.grid()\n    # plot intervalos de spread \n    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    # curvas\n    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes.legend(loc=\"best\")\n    return plt","f38c3b84":"def split_per_dates(df,train_start,target_start, sub_start):\n    \" divide os dados de treino, valida\u00e7\u00e3o e submiss\u00e3o por data\"\n    # target do treinamento\n    df_target = df[df.DT_VENDA > target_start]\n    y_train = df_target.groupby('ID_CLIENTE')['VALOR'].sum()\n    # dados de treinamento\n    df_train = df[(df['DT_VENDA']>train_start) & (df['DT_VENDA']<target_start)].copy()\n    # dados para previs\u00e3o\n    df_test = df[df['DT_VENDA']>sub_start]\n    return df_train, y_train, df_test\n\ndef treated_data(df_pp):\n    \" transforma o dataset aberto por compras ordenado no tempo \"\n    \" em um dataset por cliente e cria features \u00fateis\"\n    \n    # intervalos de tempo\n    delta90 = pd.to_timedelta(90,unit='d')\n    delta120 = pd.to_timedelta(120,unit='d')\n    delta150 = pd.to_timedelta(150,unit='d')\n    delta180 = pd.to_timedelta(180,unit='d')\n    delta360 = pd.to_timedelta(360,unit='d')\n    delta270 = pd.to_timedelta(270,unit='d')\n\n    DICT_AUX_DATAS = {'90':delta90,\n                      '120':delta120,\n                      '150':delta150,\n                      '180':delta180}\n\n    max_date = df_pp['DT_VENDA'].max()\n    \n    #agrupar por cliente somando o valor\n    cust_revenue = df_pp.groupby(['ID_CLIENTE'])['VALOR'].sum().copy()\n    # transformar em df\n    cust_rev = pd.DataFrame(cust_revenue)\n    # feature com n\u00famero de compras feitas no per\u00edodo de teste\n    cust_rev['Frequency'] = df_pp.groupby(['ID_CLIENTE'])['ID_VENDA'].count()\n\n\n    # media entre os valores das compras de cada cliente\n    cust_rev['valor medio'] = df_pp.groupby('ID_CLIENTE')['VALOR'].mean()\n    # mediana\n    cust_rev['valor mediana'] = df_pp.groupby('ID_CLIENTE')['VALOR'].median()\n    # desvio padrao\n    cust_rev['valor desvio'] = df_pp.groupby('ID_CLIENTE')['VALOR'].std()\n    \n    \n    # data da ultima compra\n    cust_rev['Recency'] = max_date - df_pp.groupby(['ID_CLIENTE'])['DT_VENDA'].max()\n    cust_rev['Recency'] = cust_rev['Recency'].apply(lambda x: x.days)\n    \n    # soma e m\u00e9dia do VALOR de diferentes per\u00edodos de cada cliente    \n    for item, valor in DICT_AUX_DATAS.items():\n        cust_rev[f'valor_{item}m'] = df_pp[\n              df_pp['DT_VENDA']>(df_pp['DT_VENDA'].max() - valor)].groupby(['ID_CLIENTE'])['VALOR'].sum()\n        cust_rev[f'valormedio_{item}m'] = df_pp[\n              df_pp['DT_VENDA']>(df_pp['DT_VENDA'].max() - valor)].groupby(['ID_CLIENTE'])['VALOR'].mean()\n\n    # soma do valor de 3 meses h\u00e1 1 ano atr\u00e1s \n    ref = df_pp['DT_VENDA'].max()\n    cust_rev[f'valorsum_1ano_atras_m'] = df_pp[\n            (df_pp['DT_VENDA'] < (ref - delta270) ) &\n           (df_pp['DT_VENDA'] > (ref - delta360) )\n            ].groupby(['ID_CLIENTE'])['VALOR'].sum()\n            \n    return cust_rev.fillna(0)\n\ndef transform_df(df):\n    df = df.copy()\n    \n    # divide os dados de treino e previs\u00e3o pela data\n    df_train, target_train, df_test = split_per_dates(df,train_start,target_start,sub_start)\n    # modifica o df e cria features nos dados de treino\n    df_train1 = treated_data(df_train)\n    # para alinhar o index do target do treino com os dados de treino\n    df_train2 = df_train1.join(other=target_train, on='ID_CLIENTE', lsuffix='_sum', rsuffix='_TARGET')\n    x_train = df_train2.drop('VALOR_TARGET',axis=1)\n    y_train = df_train2.VALOR_TARGET.fillna(0)\n    \n    # modifica o df e cria features nos dados de test\n    x_test = treated_data(df_test)\n    return x_train, y_train, x_test","5d444b19":"# intervalo de 90 dias\ndelta90 = pd.to_timedelta(90,unit='d')\n# data mais recente do dataset\nmax_date = df['DT_VENDA'].max()\n# data de in\u00edcio do target\ntarget_start = max_date - delta90\n# data de in\u00edcio dos dados de treino\ntrain_start = target_start - 5*delta90\n# data de in\u00edcio dos dados de previs\u00e3o (submiss\u00e3o)\nsub_start = max_date - 5*delta90","c6d143e3":"x_train_total, y_train_total, x_test = transform_df(df)\nx_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train_total, \n                                                                            y_train_total, \n                                                                            random_state=0,\n                                                                            test_size=0.3)","94cc4e2c":"x_train_total.head()","37aa805f":"y_train_total.head()","858e1b2e":"x_test.head()","8677dde7":"en = ElasticNet(random_state=0)\nen.fit(x_train_split, y_train_split)\nen_pred = en.predict(x_test_split)\n# valida\u00e7\u00e3o direta\nscore_dir = mean_squared_error(y_test_split,en_pred, squared=False)\nprint('Valida\u00e7\u00e3o direta RMSE: ',score_dir)","235a59b0":"# valida\u00e7\u00e3o cruzada\nscore = cross_val_score(en, X=x_train_total, y=y_train_total, cv=5, scoring='neg_root_mean_squared_error')\nprint('RMSE: ',-score.mean(), '\\nstd = :', score.std())","3c1cb12a":"# plot curva de aprendizado com 70% dos dados\ntitle = \"Learning Curves em um split dos dados - ElasticNet\"\nplot_learning_curve(ElasticNet(random_state=0), title, x_train_split, y_train_split, cv=5, n_jobs=-1)","cb5935c5":"# curva de aprendizado com todos os dados\ntitle = \"Learning Curves nos dados totais - ElasticNet\"\nplot_learning_curve(ElasticNet(random_state=0), title, x_train_total, y_train_total, cv=5, n_jobs=-1)","6252c0ee":"canais = df['CANAL'].unique()\n# define listas para receber os dados separados por canal\nx_train_aux = [[] for x in range(len(canais))]\ny_train_aux = [[] for x in range(len(canais))]\nx_test_aux = [[] for x in range(len(canais))]\n\n# divide o dataset por canal\nfor i, canal in enumerate(canais):\n    df_aux = df[df['CANAL']==canal]\n    x_train_aux[i], y_train_aux[i], x_test_aux[i] = transform_df(df_aux)","f4680d32":"for i,canal in zip(range(5),canais): \n    print(canal, x_train_aux[i].shape)","dc657a30":"# cria lista de modelos\nmodelos = [ElasticNet(random_state=0) for x in range(len(canais))]\n# lista para receber as predi\u00e7\u00f5es\npreds = [[] for x in range(len(canais))]","595aafb6":"# treina e faz a previs\u00e3o para cada modelo\nfor i, modelo in enumerate(modelos):\n    preds[i] = modelo.fit(x_train_aux[i], y_train_aux[i]).predict(x_test_aux[i])","85e64c37":"# cria lista para receber as previs\u00f5es transformadas para dataframe\ndf_preds=[[] for x in range(5)]\nfor i, canal in enumerate(canais):\n    df_preds[i] = pd.DataFrame(preds[i], index=x_test_aux[i].index)\n\ndf_sub = pd.read_csv('\/kaggle\/input\/VLabs-DC\/sample_submission.csv',header=0)\n# Join nas previs\u00f5es dos diferentes canais\nfor prediction in df_preds:\n     df_sub = df_sub.join(prediction,on='ID_CLIENTE', lsuffix='_sum', rsuffix='_TARGET')\n\n# soma as previs\u00f5es dos canais em uma coluna \"FINAL\"\ndf_sub['FINAL'] = df_sub.iloc[:,1:].sum(axis=1)\n# drop nas previs\u00f5es para ficar s\u00f3 a soma\ndf_sub.drop(list(df_sub.columns[1:-1]),axis=1,inplace=True)\n# corrige o nome das colunas para submiss\u00e3o\ndf_sub.columns = ['ID_CLIENTE','VALOR']","f6c19a90":"df_sub","2684ad03":"# melhor score na public leaderboard\n#df_sub5 = pd.read_csv(r'C:\\Users\\mmansur\\VLabs\\Submissions\\best_score.csv')\n\n# valida\u00e7\u00e3o direta\n#score_dir = mean_squared_error(df_sub5,df_sub, squared=False)\n#print('Valida\u00e7\u00e3o direta RMSE: ',score_dir)","0963a413":"df_sub.to_csv('submission_VLabs.csv', index=False)","e03682b9":"# Carregar dados","0f64df44":"## Pr\u00e9-processamento","f5e2fbe7":"# Fun\u00e7\u00f5es","883da5c9":"# Estrat\u00e9gia\n\nPrimeiro, vamos dividir os dados por data em x de treino (per\u00edodo dos dados para treinar o modelo), y de treino (o per\u00edodo do target do treino) e x de teste (o per\u00edodo para gerar a previs\u00e3o dos pr\u00f3ximos 90 dias).\n\nPara treinar o modelo, usaremos todo os dados menos as compras dos ultimos 90 dias, que ser\u00e3o o target do trieno. Para explicitar a tend\u00eancia mais recente dos dados para o modelo, agrupamos os valores dos \u00faltimos 3, 4, 5 e 6 meses criado features (vari\u00e1veis) com a soma, m\u00e9dia e desvio padr\u00e3o do valor das compras nesses per\u00edodos. Afim de transmitir a sazonalidade anual para o modelo, criamos uma feature com a soma das compras dos 3 meses do ano anterior. Essa \u00faltima melhorou a performance significativamente. ","86f533ae":"N\u00e3o sei o que ta acontecendo aqui kkkkkkkk","4a442030":"# Carregar Libs","66238299":"## Plot curvas de aprendizado","ec3d0ca5":"# Pr\u00e9-processamento\n\nTransformar o dataset original no formato pr\u00f3prio para previs\u00e3o.","978fe2c8":"# Modelo - ElasticNet\n\nUsaremos um modelo linear com regulariza\u00e7\u00f5es l1 e l2 vizando minimizar o impacto dos ru\u00eddos dos dados, do ru\u00eddo inerente a prever valores futuros a partir de uma quantidade e variedade limitada de dados que muito provavelmente n\u00e3o carrega toda a vari\u00e2ncia dos dados e o impacto da correla\u00e7\u00e3o das vari\u00e1veis que criamos nos pesos da regress\u00e3o.","8274eb33":"Outro meio usado para validar vagamente a predi\u00e7\u00e3o antes de gastar uma submiss\u00e3o no kaggle foi comparar com a melhor submiss\u00e3o que t\u00ednhamos at\u00e9 o momento.","36fc0a4c":"# Pr\u00f3ximos passos\n\nPontos a melhorar:\n- investigar cada feature individualmente, se est\u00e1 ajudando ou atrapalhando. \n- suavizar outliers vizando melhorar a performance de modelo linear\n- testamos uma normaliza\u00e7\u00e3o que n\u00e3o funcionou bem, h\u00e1 mais para explorar em rela\u00e7\u00e3o a normaliza\u00e7\u00f5es\n- investigar distribui\u00e7\u00e3o das vari\u00e1veis. Aplicar transforma\u00e7\u00f5es vizando aproximar de uma distriu\u00e7\u00e3o normal pode melhorar modelos lineares\n- hyperparameter tuning, principalmente alpha e l1_rate\n- produzir diferentes modelos e stackar utilizando as solu\u00e7\u00f5es como input","b63adbdc":"O desvio padr\u00e3o alto indica que o modelo est\u00e1 muito sens\u00edvel ao conjunto de dados usado. Variar a random_seed do train_test_split tamb\u00e9m altera significativamente o score, o que refor\u00e7a essa ideia.","242c65cc":"# Submiss\u00e3o","f1684e0e":"# Dividindo por canal\n\nOs padr\u00f5es de compras em lojas f\u00edsicas, por televendas, whatsapp, ecm e ifood s\u00e3o diferentes, por isso, treinamos um modelo com as compras de cada canal e somamos as previs\u00f5es finais. Essa solu\u00e7\u00e3o deu uma pontua\u00e7\u00e3o muito semelhante (diferen\u00e7a de d\u00e9cimos no leaderboard) a treinar apenas um modelo com os dados de todos os canais, mas como \u00e9 um pouco mais elaborada, resolvi compartilhar essa.","95d5b4d1":"Esse c\u00f3digo n\u00e3o vai rodar aqui, foi feito localmente. Retornou RMSE = 20.26 em rela\u00e7\u00e3o \u00e0 melhor previs\u00e3o que t\u00ednhamos (um modelo mais simples, com menos features), o que quer dizer que est\u00e1 no caminho certo, vai melhorar ou piorar um poquinho a melhor solu\u00e7\u00e3o at\u00e9 o momento. Nesse caso, melhorou o RMSE em ~7 pontos.","55d4c48a":"Curva de aprendizado com 70% dos dados n\u00e3o indica overfitting, coerente com aprendizado real.","57db3ef2":"# Ol\u00e1!\n\nEste \u00e9 o meu notebook e \u00e9 fruto do trabalho conjunto da equipe \"Resili\u00eancia em ML\" para a competi\u00e7\u00e3o VLabs Data Challenge, onde deseja-se prever o LTV dos pr\u00f3ximos 90 dias aberto por cliente a partir de 14 meses de dados de compras em diferentes canais. \n\nA equipe \u00e9 composta por:\n- Marcos Mansur\n- Thiago Ouverney\n- Aron Alvernaz\n- Lucas Valim\n\nEsse notebook retrata o modelo com melhor score que fizemos."}}