{"cell_type":{"141273f4":"code","96a52735":"code","eb8b2ad0":"code","c4e30278":"code","88d3f49b":"code","2012229d":"code","7a8b7486":"code","2ff64c68":"code","8d7aebbf":"code","18e02135":"code","f0417d94":"code","f5209663":"code","47b8b373":"code","5971d1b9":"code","667eb742":"code","ef55bfc4":"code","c456e888":"code","0c171235":"code","d70e48ab":"code","8d6e2210":"code","fc707850":"code","03eeb564":"code","8060eda3":"code","15b17c64":"code","375ade4e":"code","2875377f":"code","72bcb405":"code","8b051826":"code","c1b3bcb4":"code","5dff7aa9":"code","a2b10442":"code","e4c566c9":"code","c731a72f":"code","5c7889c0":"code","db1cc33f":"code","3d436859":"code","61db6704":"code","799870fc":"code","9f2d43d7":"markdown","aa0b75fc":"markdown","ec472ce4":"markdown","74d122bb":"markdown","00e51ae0":"markdown","6d291291":"markdown","9aec95a6":"markdown","e6454c4c":"markdown","34a85184":"markdown","86f037e7":"markdown","f3409620":"markdown","2b0f2995":"markdown","7d01bb41":"markdown","8fa9a481":"markdown","1385bf92":"markdown","1a31b981":"markdown","690266d6":"markdown","7976f162":"markdown","78078293":"markdown","a2651dd7":"markdown","bedcb13f":"markdown","fe044c4d":"markdown","8458115b":"markdown","39ee73c6":"markdown","c1f02107":"markdown","0cd040b2":"markdown","db503616":"markdown","f4cbfa36":"markdown","d042ecef":"markdown","9b9f485b":"markdown"},"source":{"141273f4":"#Assumption\u2013 The features are normally or approximately normally distributed.\n\n#Step-1: Importing Necessary Dependencies\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport seaborn as sns","96a52735":"#Step-2: Read and Load the Dataset\ndf = pd.read_csv('..\/input\/placement-data-full-class\/Placement_data_full_class.csv')\ndf.sample(5)\ndf.info()","eb8b2ad0":"df.describe()","c4e30278":"plt.figure(figsize=(16,5))\nplt.subplot(2,3,1)\nsns.distplot(df['degree_p'])\nplt.subplot(2,3,2)\nsns.distplot(df['salary'])\nplt.subplot(2,3,3)\nsns.distplot(df['hsc_p'])\nplt.subplot(2,3,4)\nsns.distplot(df['ssc_p'])\nplt.subplot(2,3,5)\nsns.distplot(df['mba_p'])\nplt.subplot(2,3,6)\nsns.distplot(df['etest_p'])\nplt.show()","88d3f49b":"import matplotlib.pyplot as plt\nplt.scatter(df['degree_p'],df['salary'])\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()","2012229d":"#Step-4: Finding the Boundary Values\nHighest_allowed_degree=df['degree_p'].mean() + 3*df['degree_p'].std()\nLowest_allowed_degree=df['degree_p'].mean() - 3*df['degree_p'].std()\nHighest_allowed_Salary=df['salary'].mean() + 3*df['salary'].std()\nLowest_allowed_Salary=df['salary'].mean() - 3*df['salary'].std()\n\nprint(\"Highest allowed_degree\",Highest_allowed_degree)\nprint(\"Lowest allowed_degree : \",Lowest_allowed_degree)\nprint(\"Highest allowed Salary\",Highest_allowed_Salary)\nprint(\"Lowest allowed Salary\",Lowest_allowed_Salary)","7a8b7486":"#Step-5: Finding the Outliers\nOutlier_data=df[(df['salary'] > Highest_allowed_Salary) | (df['salary'] < Lowest_allowed_Salary)].append(df[(df['degree_p'] > Highest_allowed_degree) | (df['degree_p'] < Lowest_allowed_degree)])\nOutlier_data","2ff64c68":"#plot outliers\nimport matplotlib.pyplot as plt\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(Outlier_data['degree_p'],Outlier_data['salary'])\n#changel lim according to your data\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()","8d7aebbf":"#Step-5: Finding the Outliers\ndf_c=df.copy()\ndf_c['degree_p'][df_c['degree_p']>Highest_allowed_degree]=Highest_allowed_degree\ndf_c['degree_p'][df_c['degree_p']<Lowest_allowed_degree]=Lowest_allowed_degree\n\ndf_c['salary'][df_c['salary']>Highest_allowed_Salary]=Highest_allowed_Salary\ndf_c['salary'][df_c['salary']<Lowest_allowed_Salary]=Lowest_allowed_Salary","18e02135":"#plot outliers\nplt.scatter(df_c['degree_p'],df_c['salary'])\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(Outlier_data['degree_p'],Outlier_data['salary'])\n#changel lim according to your data\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()","f0417d94":"#just apply pands dataframe filter\ndf_t=df.copy()\ndf_t=df_t[(df_t['salary'] < Highest_allowed_Salary) & (df_t['salary'] > Lowest_allowed_Salary)]\ndf_t=df_t[(df_t['degree_p'] < Highest_allowed_degree) & (df_t['degree_p'] > Lowest_allowed_degree)]\nplt.scatter(df_t['degree_p'],df_t['salary'])\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(Outlier_data['degree_p'],Outlier_data['salary'])\n#changel lim according to your data\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\n","f5209663":"df_n=df.copy()\ndf_n['salary'][(df_n['salary'] >= Highest_allowed_Salary) | (df_n['salary'] <= Lowest_allowed_Salary)]=np.nan\ndf_n['degree_p'][(df_n['degree_p'] >= Highest_allowed_degree) | (df_n['degree_p'] <= Lowest_allowed_degree)]=np.nan\ndf_n.describe()","47b8b373":"from numpy.random import randn\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom matplotlib import pyplot\ndata=df['salary'].dropna().copy()\n\n# histogram of the raw data\npyplot.hist(data, bins=100)\npyplot.show()\n# reshape data to have rows and columns\n\n# discretization transform the raw data\nkbins = KBinsDiscretizer(n_bins=25, encode='ordinal', strategy='kmeans')\ndata=np.asarray(data)\ndata = data.reshape((len(data),1))\n\ndata_trans = kbins.fit_transform(data)\n# summarize first few rows\n\n# histogram of the transformed data\npyplot.hist(data_trans, bins=25)\npyplot.show()","5971d1b9":"df.shape","667eb742":"from scipy import stats\nimport pandas as pd\ndf = pd.read_csv('..\/input\/placement-data-full-class\/Placement_data_full_class.csv')\ndf=df.dropna()\ndf.index=[i for i in range(0,148)]#reindexing | change accordingle to reset index of df\nd=pd.DataFrame(stats.zscore(df['salary']),columns=['z_score'])\nd=d[(d['z_score']>3) | (d['z_score']<-3)]\nd.head()","ef55bfc4":"df.shape[0]","c456e888":"degree=[]\nsalary=[]\nfor i in df.index:\n    if( i in d.index): \n        salary.append(df.loc[i]['salary'])\n        degree.append(df.loc[i]['degree_p'])","0c171235":"print(salary,degree)","d70e48ab":"import matplotlib.pyplot as plt\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(degree,salary)\nplt.show()","8d6e2210":"from sklearn.ensemble import IsolationForest\ndf = pd.read_csv('..\/input\/placement-data-full-class\/Placement_data_full_class.csv')\ndf=df.dropna()\ndf.index=[i for i in range(0,148)]#reindexing\nmodel=IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.05),max_features=1.0)\nmodel.fit(df[['ssc_p']],df[['hsc_p']])\n\ndg=pd.DataFrame({'ssc_p':df['ssc_p'],\n                 'score':model.decision_function(df[['ssc_p']]),\n                 'anomaly':model.predict(df[['ssc_p']]),\n                 'hsc_p':df['hsc_p']})\nimport matplotlib.pyplot as plt\ndg2=dg[dg['anomaly']==-1]\ndg2","fc707850":"\nplt.scatter(dg['ssc_p'],dg['hsc_p'])\nplt.scatter(dg2['ssc_p'],dg2['hsc_p'])\nplt.show()","03eeb564":"anomaly=dg.loc[dg['anomaly']==-1]\nanomaly_index=list(anomaly.index)\nprint(anomaly)","8060eda3":"\ndf = pd.read_csv('..\/input\/placement-data-full-class\/Placement_data_full_class.csv')\ndf=df.dropna()[['degree_p','salary']]\ndf.index=[i for i in range(0,148)]#reindexing | change accordingle to reset index of df\n\n\n#before DBSCAN you must scale your dataset\nstscaler = StandardScaler().fit(df)\ndf = pd.DataFrame(stscaler.transform(df))\nprint(df)\ndf.describe()","15b17c64":"\ndbsc = DBSCAN(eps = 1.3, min_samples = 25).fit(df)\nlabels = dbsc.labels_\nprint(Counter(labels))\n","375ade4e":"outliers=df[dbsc.labels_==-1]\noutliers","2875377f":"df.head()","72bcb405":"plt.scatter(df[0],df[1])\nplt.scatter(outliers[0],outliers[1])\nplt.xlabel(\"Degree_p\")\nplt.ylabel(\"salary\")\nplt.show()","8b051826":"import pandas as pd\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n%matplotlib inline","c1b3bcb4":"from sklearn.covariance import EllipticEnvelope\nimport pandas as pd\ndf = pd.read_csv('..\/input\/placement-data-full-class\/Placement_data_full_class.csv')\ndf=df.dropna()[['degree_p','salary']]\ndf.index=[i for i in range(0,148)]#reindexing | change accordingle to reset index of df\n\n#before E.E. you must scale your dataset\nstscaler = StandardScaler().fit(df)\ndf = pd.DataFrame(stscaler.transform(df))\nclf = EllipticEnvelope(contamination=0.02,random_state=100)\nclf.fit(df)\nee_scores = pd.Series(clf.decision_function(df)) \nee_predict = clf.predict(df)\nprint(Counter(ee_predict))\noutliers=df[ee_predict==-1]\noutliers","5dff7aa9":"plt.scatter(df[0],df[1])\nplt.scatter(outliers[0],outliers[1])\nplt.xlabel(\"Degree_p\")\nplt.ylabel(\"Salary\")\nplt.show()","a2b10442":"from sklearn import svm\nimport pandas as pd\ndf = pd.read_csv('..\/input\/placement-data-full-class\/Placement_data_full_class.csv')\ndf=df.dropna()[['degree_p','salary']]\ndf.index=[i for i in range(0,148)]#reindexing | change accordingle to reset index of df\n#before this you must scale your dataset\nstscaler = StandardScaler().fit(df)\ndf = pd.DataFrame(stscaler.transform(df))\nclf=svm.OneClassSVM(nu=0.05,kernel='rbf',gamma=.01)\nclf.fit(df)\npredict=clf.predict(df)\noutliers=df[predict==-1]\nprint(Counter(predict))\noutliers\n","e4c566c9":"plt.scatter(df[0],df[1])\nplt.scatter(outliers[0],outliers[1])\nplt.xlabel(\"Degree_p\")\nplt.ylabel(\"Salary\")\nplt.show()","c731a72f":"from sklearn.neighbors import LocalOutlierFactor\ndf = pd.read_csv('..\/input\/placement-data-full-class\/Placement_data_full_class.csv')\ndf=df.dropna()[['degree_p','salary']]\ndf.index=[i for i in range(0,148)]#reindexing | change accordingle to reset index of df\n\nstscaler = StandardScaler().fit(df)\ndf = pd.DataFrame(stscaler.transform(df))\nclf = LocalOutlierFactor(n_neighbors=25, contamination=.09)\ny_pred = clf.fit_predict(df)\nLOF_Scores = clf.negative_outlier_factor_\nLOF_pred=pd.Series(y_pred)\noutliers=df[LOF_pred==-1]\nprint(Counter(LOF_pred))\noutliers","5c7889c0":"plt.scatter(df[0],df[1])\nplt.scatter(outliers[0],outliers[1])\nplt.xlabel(\"Degree_p\")\nplt.ylabel(\"Salary\")\nplt.show()","db1cc33f":"import numpy as np\nimport pandas as pd\ndf = pd.read_csv('..\/input\/placement-data-full-class\/Placement_data_full_class.csv')\n\ndf=df.dropna()[['degree_p','salary']]\n\ndf.index=[i for i in range(0,148)]#reindexing | change accordingle to reset index of df\n\nstscaler = StandardScaler().fit(df)\ndf = pd.DataFrame(stscaler.transform(df))\ndf","3d436859":"\nq1, q3= np.percentile(df[1],[25,75])\niqr=q3-q1\n#Lower and upper bound for the outliers\nlower_bound = q1 -(2.5 * iqr) \nupper_bound = q3 +(2.5 * iqr)\n","61db6704":"outliers=df[(df[1]>upper_bound) | (df[1]<lower_bound)]\noutliers","799870fc":"plt.scatter(df[1],df[1])\nplt.scatter(outliers[1],outliers[1])\nplt.xlabel(\"Salary\")\nplt.ylabel(\"Salary\")\nplt.show()","9f2d43d7":"# 1. Outlier Detection Stratgies for diffrent-diffrent destribution","aa0b75fc":"# 4. Descretization","ec472ce4":"# **1. Capping Code**","74d122bb":"Lets do some experiment","00e51ae0":"# 3. Treat outliers as a missing value Code:","6d291291":"**The method, step-by-step:**\n\n1. For each point P, do the following:\n1. Calculate distances between P and every other point (manhattan = |x1-x2| + |y1-y2|) = dist(p1,p2)\n1. Find the Kth closest point (Kth nearest neighbor\u2019s distance=K-Dist(P))\n1. Find the K closest points (those whose distances are smaller than the Kth point), the K-distance neighborhood of P, Nk(P).\n1. Find its density (Local Reachability Density= LRDk(p) \u2014 a measure of how close its neighbors are to it), basically the inverse of the avg distance between point p and its neighbors. The lower the density, the farther p is from its neighbors.\n1. Find its local outlier factor, LOFk(p)   =   sum(reachability distances of neighbors to P) x sum(densities of neighbors). LOFk(P) is basically the sum of the distances between P and its neighboring points, weighted by the sum those points\u2019 densities (how far they are from their k neighboring points).\n\n\n**Additional Details**\n\n1.  For step 2, If 2 points have the same distance to P, then just select one as the next closest, and the other as the next next closest.\n1. For step 4, LRD = Local Reachability Density = inverse(avg reachability distance between P and its neighbors) <= 1. The word reachability is used because if a neighbor is closer to P than it\u2019s Kth neighbor, then the distance of the Kth neighbor is used instead as a means of smoothing\n1. For step 4, each reachability distance of a point P\u2019s k neighbors is reachdistk(n1<-p) = max(distk(n1), dist(n1,p))\n\n1. For step 4, total distances of neighboring points is divided by the number of neighboring points (or ||Nk(P)||), computed using the results of step 3\n\n\n**Scenarios affecting LOF values:**\nHigher LOF values indicate a greater anomaly level and that\n\nLOFk(p) =sum(reachability distances of its neighbors to P) x sum(neighbor densities)\n\nThe LOF for a point P will have a:\n\n1. High value if \u2192 P is far from its neighbors and its neighbors have high densities (are close to their neighbors) (LOF = (high distance sum) x (high density sum) = High value)\n1. Less high value if -> P is far from its neighbors, but its neighbors have low densities (LOF = (high sum) x (low sum) = middle value)\n1. Less high value if -> P is close to its neighbors and its neighbors have low densities (LOF = (low sum) x (low sum) = low value )\n1. Adjusting K:\n1. Increase K too much and you\u2019re just looking for outliers with respect to the entire dataset, so points far away from the highest density regions could be misclassified as outliers, even though they themselves reside in a cluster of points.\n1. Reduce K too much and you\u2019re looking for outliers with respect to very small local regions of points. This could also lead to the misclassification as outliers.","9aec95a6":"# 3. Technique to handle Outliers\n","e6454c4c":"# 3.4 Elliptic Envelope\nis intuitively built on the premise that data comes from a known distribution. If we draw an ellipse around the gaussian distribution of data, anything that lies outside the ellipse will be considered an outlier.\nThe assumption in the Elliptic Envelope is that normal data points are Gaussian distributed.","34a85184":"After Capping let see our data","86f037e7":"# Type of outlier:\n\n**1. Univerient**\nOutliers can be of two kinds: univariate and multivariate. Univariate outliers can be found when looking at a distribution of values in a single feature space. \n\n**2. Multivarient**\nMultivariate outliers can be found in a n-dimensional space (of n-features). Looking at distributions in n-dimensional spaces can be very difficult for the human brain, that is why we need to train a model to do it for us.\n\nOutliers can also come in different flavours, depending on the environment: point outliers, contextual outliers, or collective outliers. \n\n\n**3. Point Outlier**\nPoint outliers are single data points that lay far from the rest of the distribution.\n\n**4. Contextual outliers**\nContextual outliers can be noise in data, such as punctuation symbols when realizing text analysis or background noise signal when doing speech recognition.\n\n\n ","f3409620":"# Most common causes of outliers on a data set:\n* Data entry errors (human errors)\n* Measurement errors (instrument errors)\n* Experimental errors (data extraction or experiment planning\/executing errors)\n* Intentional (dummy outliers made to test detection methods)\n* Data processing errors (data manipulation or data set unintended mutations)\n* Sampling errors (extracting or mixing data from wrong or various sources)\n* Natural (not an error, novelties in data)","2b0f2995":"By \u2018tagging\u2019 or removing the data points that lay beyond a given threshold we are classifying data into outliers and not outliers\n\nZ-score is a simple, yet powerful method to get rid of outliers in data if you are dealing with parametric distributions in a low dimensional feature space. For nonparametric problems Dbscan and Isolation Forests can be good solutions.","7d01bb41":"# Outliers\n![](https:\/\/miro.medium.com\/max\/2400\/1*F_yiILIE954AZPgPADx76A.png)\n\nOutliers are extreme values that deviate from other observations on data , they may indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample.","8fa9a481":"**Ploting of outlier over data**","1385bf92":"The method, step-by-step:\n1. Randomly select a point not already assigned to a cluster or designated as an outlier. Determine if it\u2019s a core point by seeing if there are at least min_samples points around it within epsilon distance.\n1. Create a cluster of this core point and all points within epsilon distance of it (all directly reachable points).\n1. Find all points that are within epsilon distance of each point in the cluster and add them to the cluster. Find all points that are within epsilon distance of all newly added points and add these to the cluster. Rinse and repeat. (i.e. perform \u201cneighborhood jumps\u201d to find all density-reachable points and add them to the cluster).\n\nLingo underlying the above:\n1. Any point that has at least min_samples points within epsilon distance of it will form a cluster. This point is called a core point. The core point will itself count towards the min_samples requirement.\n2. Any point within epsilon distance of a core point, but does not have min_samples points that are within epsilon distance of itself is called a borderline point and does not form its own cluster.\n3. A border point that is within epsilon distance of multiple core points (multiple epsilon balls) will arbitrarily end up in just one of these resultant clusters.\n4. Any point that is randomly selected that is not found to be a core point or a borderline point is called a noise point or outlier and is not assigned to any cluster. Thus, it does not contain at least min_samples points that are within epsilon distance from it or is not within epsilon distance of a core point.\n5. The epsilon-neighborhood of point p is all points within epsilon distance of p, which are said to be directly reachable from p.\n6. A point contained in the neighborhood of a point directly reachable from p is not necessarily directly reachable from p, but is density-reachable.\n7. Any point that can be reached by jumping from neighborhood to neighborhood from the original core point is density-reachable.","1a31b981":"# 3.3 DBScane Anomaly Detection\nThis is a clustering algorithm (an alternative to K-Means) that clusters points together and identifies any points not belonging to a cluster as outliers. It\u2019s like K-means, except the number of clusters does not need to be specified in advance.","690266d6":"# 1.1 For Normal or Gaussian Destribution\nIdentifying an observation as an outlier depends on the underlying distribution of the data. In this section, we limit the discussion to univariate data sets that are assumed to follow an approximately normal distribution. If the normality assumption for the data being tested is not valid, then a determination that there is an outlier may in fact be due to the non-normality of the data rather than the prescence of an outlier.\nFor this reason, it is recommended that you generate a normal probability plot of the data before applying an outlier test. Although you can also perform formal tests for normality, the prescence of one or more outliers may cause the tests to reject normality when it is in fact a reasonable assumption for applying the outlier test.\n\nIn addition to checking the normality assumption, the lower and upper tails of the normal probability plot can be a useful graphical technique for identifying potential outliers. In particular, the plot can help determine whether we need to check for a single outlier or whether we need to check for multiple outliers.\n\nThe box plot and the histogram can also be useful graphical tools in checking the normality assumption and in identifying potential outliers.\n\nUse empirical relations of Normal distribution.\n\n\u2013 The data points which fall below mean-3*(sigma) or above mean+3*(sigma) are outliers.\n\nwhere mean and sigma are the average value and standard deviation of a particular column.\n\n![](https:\/\/miro.medium.com\/max\/679\/0*y7kVHEQPQKBg3Cga.)\n\ngood \u2018thumb-rule\u2019 z score (how to calculate z score value desecribed in below section) thresholds can be: 2.5, 3, 3.5 or more standard deviations.","7976f162":"# 2 How to Remove or Replace Outlier \n# **2.1 Caping :** \nIn this technique, we cap our outliers data and make the limit i.e, above a particular value or less than that value, all the values will be considered as outliers, and the number of outliers in the dataset gives that capping number.\n\n![](https:\/\/miro.medium.com\/max\/1400\/0*M4ZSmi8idYdsyzEp.png)\n\n# **2.2 Trimming :** \nIt excludes the outlier values from our analysis. By applying this technique our data becomes thin when there are more outliers present in         the dataset. Its main advantage is its fastest nature.\n\n![](https:\/\/i.pinimg.com\/564x\/20\/0a\/1a\/200a1ab75f158986c52d3b59e3b1a501.jpg)\n\n\n# **2.3 Treat outliers as a missing value :** \nBy assuming outliers as the missing observations, treat them accordingly i.e, same as those of missing values.\n\n![](https:\/\/i.pinimg.com\/564x\/68\/d0\/cf\/68d0cf2571b5a39eb3fca23d5bba8c70.jpg)\n\n# **2.4 Discretization or Binning :** \nin this technique, by making the groups we include the outliers in a particular group and force them to behave in the same manner as those of other points in that group. \n![](https:\/\/cxl.com\/wp-content\/uploads\/2017\/01\/010211_dp_table_big.png)","78078293":"**print the anomalies**","a2651dd7":"![](https:\/\/miro.medium.com\/max\/670\/1*OIXCo35Vvzr9qoUbV2fHMA.png)","bedcb13f":"# 1.2 For Non Gaussian Destribution and For Skewed distributions\n\nIn case of Non Gaussian Destribution one way is to transform the data into normal destribution,or you can Use Inter-Quartile Range (IQR) proximity rule.\n\n\u2013 The skewed data points which fall below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are outliers.\n\nwhere Q1 and Q3 are the 25th and 75th percentile of the dataset respectively, and IQR represents the inter-quartile range and given by Q3 \u2013 Q1.\nand also for other destribution we can go with the percentile value:\n\n**For Skewed distributions:**\n\nUse Inter-Quartile Range (IQR) proximity rule.\n\n![](https:\/\/naysan.ca\/wp-content\/uploads\/2020\/06\/box_plot_ref_needed.png)\n\n**For other destribution :**\n\nUse percentile-based approach.\n\n![](https:\/\/acutecaretesting.org\/-\/media\/acutecaretesting\/articles\/fig-6-example.jpg?h=402&w=750)\n","fe044c4d":"Now you can impute the Nan value with some imputers see the below link how to impute the Nan value\nhttps:\/\/www.kaggle.com\/mukulkirti\/handle-missing-value","8458115b":"# In this Notbook we will see:\n\n**1. Outlier Detection Stratgies for diffrent-diffrent destribution**\n\n    1.1 For Normal or Gaussian Destribution\n\n    1.2 For Non Gaussian Destribution or For Skewed distributions\n   \n**2. How to Remove or Replace Outlier**\n\n    2.1 Caping\n\n    2.2 Trimming\n\n    2.3 Treat outliers as a missing value\n\n    2.4 Discretization or Binning\n       \n**3. Technique to handle Outliers**\n\nOutiler model are based on:\n\nProbabilistic and Statistical Modeling (parametric)\n\nLinear Regression Models (PCA, LMS)\n\nProximity Based Models (non-parametric)\n\nInformation Theory Models\n\nHigh Dimensional Outlier Detection Methods (high dimensional sparse data)\n\nHere are Few Algo we are going to discous:\n\n    3.1 Z-Score or Extreme Value Analysis (parametric)\n\n    3.2 Isolation Forest\n\n    3.3 DBSCAN\n    \n    3.4 Elliptical Envelop\n\n    3.5 One class SVM\n    \n    3.6 Local Outlier Factor\n\n    3.7 Boxplot (IQR based)","39ee73c6":"# 3.6 Local Outlier Factor\nLOF uses density-based outlier detection to identify local outliers, points that are outliers with respect to their local neighborhood, rather than with respect to the global data distribution. The higher the LOF value for an observation, the more anomalous the observation.\nThis is useful because not all methods will not identify a point that\u2019s an outlier relative to a nearby cluster of points (a local outlier) if that whole region is not an outlying region in the global space of data points.\nA point is labeled as an outlier if the density around that point is significantly different from the density around its neighbors.\nIn the below feature space, LOF is able to identify P1 and P2 as outliers, which are local outliers to Cluster 2 (in addition to P3).\n","c1f02107":"# 3.5. One-Class Support Vector Machines\nThe OneClassSVM is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.","0cd040b2":"# 3.7 Interquartile Range(IQR)\nIQR finds the data point that lies outside the overall distribution of the dataset.\nAny data point that falls outside of 1.5 times of an interquartile range below the 1st quartile and above the 3rd quartile is considered an Outlier.\n\n![](https:\/\/miro.medium.com\/max\/875\/1*FUNw6hD0X-L-WkilD6u3Qg.png)","db503616":"Implementation Considerations:\n1. You may need to standardize \/ scale \/ normalize your data first.\n2. Be mindful of data type and the distance measure. I\u2019ve read that the gower distance metric can be used for mixed data types. I\u2019ve implemented Euclidean, here, which needs continuous variables, so I removed gender.\n3. You will want to optimize epsilon and min_samples.","f4cbfa36":"# 3.1 Z-Score or Extreme Value Analysis (parametric)\n\nThe z-score or standard score of an observation is a metric that indicates how many standard deviations a data point is from the sample\u2019s mean, assuming a gaussian distribution. This makes z-score a parametric method. Very frequently data points are not to described by a gaussian distribution, this problem can be solved by applying transformations to data ie: scaling it.\nSome Python libraries like Scipy and Sci-kit Learn have easy to use functions and classes for a easy implementation along with Pandas and Numpy.\nAfter making the appropriate transformations to the selected feature space of the dataset, the z-score of any data point can be calculated with the following expression:\n\n![](https:\/\/miro.medium.com\/max\/170\/0*TwXvmgI5j7ArPPq4.)\n\nWhen computing the z-score for each sample on the data set a threshold must be specified. Some good \u2018thumb-rule\u2019 thresholds can be: 2.5, 3, 3.5 or more standard deviations.\n\n![](https:\/\/miro.medium.com\/max\/679\/0*y7kVHEQPQKBg3Cga.)","d042ecef":"# **2. Triming Code**","9b9f485b":"# 3.2 Isolation Forest\n\nThis is a non-parametric method for large datasets in a one or multi dimensional feature space.\n\nAn important concept in this method is the isolation number.\nisolation forests are an effective method for detecting outliers or novelties in data. It is a relatively novel method based on binary decision trees. Sci-Kit Learn\u2019s implementation is relatively simple and easy to understand.\nIsolation forest\u2019s basic principle is that outliers are few and far from the rest of the observations. To build a tree (training), the algorithm randomly picks a feature from the feature space and a random split value ranging between the maximums and minimums. This is made for all the observations in the training set. To build the forest a tree ensemble is made averaging all the trees in the forest.\nThen for prediction, it compares an observation against that splitting value in a \u201cnode\u201d, that node will have two node children on which another random comparisons will be made. The number of \u201csplittings\u201d made by the algorithm for an instance is named: \u201cpath length\u201d. As expected, outliers will have shorter path lengths than the rest of the observations.\nAn outlier score can computed for each observation:\n\nThe isolation number is the number of splits needed to isolate a data point. This number of splits is ascertained by following these steps:\n\n1. A point \u201ca\u201d to isolate is selected randomly.\n\n2. A random data point \u201cb\u201d is selected that is between the minimum and maximum value and different from \u201ca\u201d.\n\n3. If the value of \u201cb\u201d is lower than the value of \u201ca\u201d, the value of \u201cb\u201d becomes the new lower limit.\n\n4. If the value of \u201cb\u201d is greater than the value of \u201ca\u201d, the value of \u201cb\u201d becomes the new upper limit.\n\nThis procedure is repeated as long as there are data points other than \u201ca\u201d between the upper and the lower limit.\n\nIt requires fewer splits to isolate an outlier than it does to isolate a non-outlier, i.e. an outlier has a lower isolation number in comparison to a non-outlier point. A data point is therefore defined as an outlier if its isolation number is lower than the threshold.\n\nThe threshold is defined based on the estimated percentage of outliers in the data, which is the starting point of this outlier detection algorithm.\n\nAn explanation with images of the isolation forest technique is available at https:\/\/quantdare.com\/isolation-forest-algorithm\/.\n\n"}}