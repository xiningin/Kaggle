{"cell_type":{"e46e9f84":"code","33cf190c":"code","7c17d460":"code","4298e9d6":"code","71b1884e":"code","cb8d9658":"code","392d9661":"code","0713ff47":"code","10cd1bb6":"code","aed6e6f1":"code","934fc2db":"code","8d74c75c":"code","61a89438":"code","02987155":"code","fffc2a87":"code","e4727940":"code","f3eecfc2":"code","d499d1bd":"code","73e1c92b":"code","037d592a":"code","a1ee8664":"code","d17f57fe":"code","dc38652c":"code","acab952e":"code","507d4a6c":"code","e7e85d1b":"code","321be879":"code","ff7bf5c1":"code","a2f46fdf":"code","7a3f2b07":"code","50bffd8c":"code","a6ae6155":"code","5ee1b03e":"code","102e780b":"code","b184f4ce":"code","4c09a8ff":"code","f70899b9":"markdown","e2d6d59e":"markdown","76008638":"markdown","25f064b2":"markdown","dd1d576a":"markdown","23ed6eab":"markdown","727d0b70":"markdown","47f9b855":"markdown","1c4f759a":"markdown","21ee522f":"markdown","a4865de3":"markdown","955d723d":"markdown","9f95b2ea":"markdown","d21ce129":"markdown","c0b5d2d4":"markdown"},"source":{"e46e9f84":"#import library\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport nltk\nnltk.download('stopwords')\nimport re\nimport string\nstring.punctuation\nimport matplotlib.pyplot as plt\nfrom nltk.stem.porter import PorterStemmer\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport math\nfrom sklearn import metrics","33cf190c":"txt = pd.read_csv('..\/input\/physics-vs-chemistry-vs-biology\/dataset\/train.csv')","7c17d460":"txt.shape","4298e9d6":"txt.head()","71b1884e":"#selection data\ndf1 = txt[['Comment']]\ndf1.head()","cb8d9658":"#defining the function to remove emoji\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  \n                           u\"\\U0001F300-\\U0001F5FF\"  \n                           u\"\\U0001F680-\\U0001F6FF\" \n                           u\"\\U0001F1E0-\\U0001F1FF\"  \n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags = re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\n#applying the function\ndf1['No_Emoji'] = df1['Comment'].apply(lambda x: remove_emoji(x))\ndf1.head()","392d9661":"#defining to remove punctuation\ndef remove_punctuation(text):\n    punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n    return punctuationfree\n\ndf1[\"Comment_Punctuation\"] = df1[\"No_Emoji\"].apply(lambda text: remove_punctuation(text))\ndf1.head()","0713ff47":"def remove_html(text):\n    html_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return html_pattern.sub(r'', text)\n\n#applying function to the column\ndf1['Comment_Clean'] = df1['Comment_Punctuation'].apply(lambda x: remove_html(x))\ndf1.head()","10cd1bb6":"#setting lower case\ndf1[\"Comment_Lower\"] = df1[\"Comment_Clean\"].str.lower()\ndf1.head()","aed6e6f1":"#defining function for tokenization\ndef tokenization(text):\n    tokens = re.split('W+', text)\n    return tokens\n\n#applying function to the column\ndf1['Comment_Tokenied'] = df1['Comment_Lower'].apply(lambda x: tokenization(x))\ndf1.head()","934fc2db":"#stop words present in the library\nstopwords = nltk.corpus.stopwords.words('english')\n\n#defining the function to remove stopwords from tokenized text\ndef remove_stopwords(text):\n    output= [i for i in text if i not in stopwords]\n    return output\n\n#applying the function\ndf1['No_Stopwords'] = df1['Comment_Tokenied'].apply(lambda x:remove_stopwords(x))\ndf1.head()","8d74c75c":"#defining the object for stemming\nporter_stemmer = PorterStemmer()\n\n#defining a function for stemming\ndef stemming(text):\n    stem_text = [porter_stemmer.stem(word) for word in text]\n    return stem_text\n\n#applying the function\ndf1['Comment_Stemmed'] = df1['No_Stopwords'].apply(lambda x: stemming(x))\ndf1.head()","61a89438":"#defining the object for lemmatizing\nlemmatizer = WordNetLemmatizer()\n\n#defining a function for lemmatizing\ndef lemmatize_words(text):\n    lemma_text = [lemmatizer.lemmatize(word) for word in text]\n    return lemma_text\n                \n#applying the function\ndf1[\"Comment_lemmatized\"] = df1[\"Comment_Stemmed\"].apply(lambda text: lemmatize_words(text))\ndf1.head()","02987155":"#selection data\ndf2 = df1[['Comment_Clean']]\ndf2.head()","fffc2a87":"#create function to get subjectivity\ndef getSubjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\n#create function to get polarity\ndef getPolarity(text):\n    return TextBlob(text).sentiment.polarity\n\n#apply function to data \ndf2['Subjectivity'] = df2['Comment_Clean'].apply(getSubjectivity)\ndf2['Polarity'] = df2['Comment_Clean'].apply(getPolarity)\ndf2.head()","e4727940":"#create function to get sentiment data\ndef getSentiment(score):\n    if score < 0:\n        return 'Negative'\n    elif score == 0:\n        return 'Neutral'\n    else:\n        return 'Positive'\n\n#apply function to data\ndf2['Sentiment'] = df2['Polarity'].apply(getSentiment)\ndf2.head()","f3eecfc2":"positive = \" \".join(df2[df2.Sentiment == 'Positive']['Comment_Clean'].values)\nw = WordCloud(width = 700, height = 400, random_state = 10, max_font_size = 100).generate(positive)\n\nplt.figure(figsize = (10,6))\nplt.imshow(w, interpolation = \"bilinear\")\nplt.title(\"Wordcloud of Positive Comment\")\nplt.axis('off')\nplt.show()","d499d1bd":"neutral = \" \".join(df2[df2.Sentiment == 'Neutral']['Comment_Clean'].values)\nw = WordCloud(width = 700, height = 400, random_state = 10, max_font_size = 100, colormap = 'Set1').generate(neutral)\n\nplt.figure(figsize = (10,6))\nplt.imshow(w, interpolation = \"bilinear\")\nplt.title(\"Wordcloud of Neutral Comment\")\nplt.axis('off')\nplt.show()","73e1c92b":"negative = \" \".join(df2[df2.Sentiment == 'Negative']['Comment_Clean'].values)\nw = WordCloud(width = 700, height = 400, random_state = 10, max_font_size = 100, colormap = 'Set2').generate(negative)\n\nplt.figure(figsize = (10,6))\nplt.imshow(w, interpolation = \"bilinear\")\nplt.title(\"Wordcloud of Negative Comment\")\nplt.axis('off')\nplt.show()","037d592a":"#visualize sentiment\nplt.figure(figsize = (8,6))\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\ndf2['Sentiment'].value_counts().plot(kind = 'bar', color = 'deepskyblue')\nplt.title(\"Sentiment Analysis of Comment\")\nplt.show()","a1ee8664":"#selection data\njoin = (df2['Comment_Clean'], df2['Sentiment'])\ndf3 = pd.concat(join, axis = True)\ndf3.head()","d17f57fe":"#transform data to categorical variable\ndf3['Sentiment'] = df3['Sentiment'].astype('category')\nprint(df3.dtypes)","dc38652c":"#handling categorical data\ndf3['Sentiment'] = df3['Sentiment'].cat.codes\ndf3.head()","acab952e":"#split data\nX = df3.drop('Sentiment', axis = 1)\ny = df3['Sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","507d4a6c":"#defining function for feature scaling\ndef vectorize(data, tfidf_vect_fit):\n    X_tfidf = tfidf_vect_fit.transform(data)\n    words = tfidf_vect_fit.get_feature_names()\n    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n    X_tfidf_df.columns = words\n    return(X_tfidf_df)\n\n#applying function feature scaling\ntfidf_vect = TfidfVectorizer()\ntfidf_vect_fit = tfidf_vect.fit(X_train['Comment_Clean'])\nX_train = vectorize(X_train['Comment_Clean'], tfidf_vect_fit)","e7e85d1b":"#build model\nrf = RandomForestClassifier()\n%time rf.fit(X_train, y_train)\nscore = cross_val_score(rf, X_train, y_train.values.ravel(), cv = 5) \nprint(score)\nscore.mean()","321be879":"#prediction\nX_test = vectorize(X_test['Comment_Clean'], tfidf_vect_fit)\ny_pred = rf.predict(X_test)\nprint(y_pred)","ff7bf5c1":"#report of MSE, MAE, RMSE\nmse = metrics.mean_squared_error(y_test, y_pred)\nprint('Mean Squared Error : '+ str(mse))\nmae = metrics.mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error : '+ str(mae))\nrmse = math.sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error : '+ str(rmse))","a2f46fdf":"#confusion matrix\nmatrix = metrics.confusion_matrix(y_test, y_pred)\nprint(matrix)\n\n#heatmap matrix\nplt.figure(figsize = (8,6))\nsns.heatmap(matrix, annot = True, fmt = '.0f', square = True)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Actual\")\nplt.show()","7a3f2b07":"#classification report\nreport = metrics.classification_report(y_test, y_pred)\nprint(report)","50bffd8c":"#defining feature\nfeature = pd.Series(rf.feature_importances_, index = X_train.columns).sort_values(ascending = False)\nprint(feature)","a6ae6155":"#visualize feature\nfeature.nlargest(20).plot(kind = 'bar',figsize = (10,6), color = 'orangered')\nplt.title(\"Top 20 Important Features\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Score\")\nplt.show()","5ee1b03e":"#visualize topic\nplt.figure(figsize = (8,6))\nsns.countplot(txt['Topic'], palette = 'Set1')\nplt.title(\"Topic\")\nplt.show()","102e780b":"#selection data\njoin = (txt['Topic'], df2['Sentiment'])\ndf4 = pd.concat(join, axis = True)\ndf4.head()","b184f4ce":"#group topic & sentiment\ntopic_sentiment = df4.groupby(['Topic', 'Sentiment']).size().reset_index(name = 'Count')\nprint(topic_sentiment)","4c09a8ff":"#visualize topic ~ sentiment\nplt.figure(figsize = (10,6))\nsns.barplot(x = 'Topic', y = 'Count', hue = 'Sentiment', data = topic_sentiment)\nplt.title(\"Topic ~ Sentiment\")\nplt.show()","f70899b9":"# NLP Text Classification","e2d6d59e":"## Random Forest Model ","76008638":"## Visualization","25f064b2":"### Stemming","dd1d576a":"## Sentiment Analysis ","23ed6eab":"### Removal HTML Attributes","727d0b70":"### Lower Casing","47f9b855":"## Data Extraction","1c4f759a":"### Lemmatization","21ee522f":"## Text Processing","a4865de3":"### Tokenization","955d723d":"## Check Feature Importance","9f95b2ea":"### Removal Stopwords","d21ce129":"### Removal Emoji ","c0b5d2d4":"### Removal Punctuation"}}