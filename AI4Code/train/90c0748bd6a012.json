{"cell_type":{"679b5436":"code","0e336f23":"code","a84f7234":"code","6adab18e":"code","90adabc8":"code","fbb13b5c":"code","02c07a63":"code","2d87877b":"code","1a52879e":"code","c3bb3e6a":"code","1e665ef2":"code","d8dc9900":"code","5b412f0c":"code","eaa08ed5":"code","77646b84":"code","6ffe830e":"code","90b6769e":"code","69532b6a":"code","edece2c3":"code","f5847c33":"code","8b02f148":"code","8749271a":"code","1344c231":"code","ec266ffb":"code","f57e6b4a":"code","191f5efb":"code","b7677b06":"code","7c023a70":"code","59892ce7":"code","8c32a140":"code","e3fad4fb":"code","0309e322":"code","78fbeac5":"code","76103f97":"code","2922a938":"code","07c89f7a":"code","a0db368d":"code","8a306ffa":"code","d2755f08":"code","a065f29b":"code","7cd6e26a":"code","ec15e0a2":"code","8cf4c22b":"code","554a33f8":"code","b11b3b0f":"code","faf35b5a":"code","453f3ae4":"code","221e1e79":"code","2f224033":"code","877761ee":"code","62833715":"code","c73c60a9":"code","eb1d712c":"code","667f7887":"code","c6150033":"code","727286c2":"code","30ed370f":"code","b7066833":"code","a6d3b46e":"code","3285c3b0":"code","e5c46248":"code","caf8540a":"code","f368fffe":"code","ef5db183":"code","eff5987e":"code","e84a74b9":"code","7991b93f":"code","49347664":"code","c33f3f80":"code","999bf0b2":"code","422cc279":"code","2a1dd9c2":"code","506252f3":"code","ea3277c8":"markdown","5538dfad":"markdown","b0bc9221":"markdown","c2b846c1":"markdown","06393f7d":"markdown","e93d6978":"markdown","ec357527":"markdown","99e1885b":"markdown","5275ef1f":"markdown","c53f30a4":"markdown","5af04596":"markdown","56ff6c56":"markdown","ed56e674":"markdown","dccf00bd":"markdown","ea19b322":"markdown","4b5aa837":"markdown","f103dd81":"markdown","afb732e0":"markdown","439d1094":"markdown","26edfa88":"markdown","f75bb0a8":"markdown","460f917e":"markdown","293e256a":"markdown","7a4f66fb":"markdown","efcc8399":"markdown","ab4ec5d4":"markdown","482ffcbe":"markdown","25ffc874":"markdown","50e73e73":"markdown","24059d70":"markdown","da9979eb":"markdown","50736f38":"markdown","f31c0534":"markdown","8df240b9":"markdown","875b9fb6":"markdown","02540a12":"markdown","fe109f25":"markdown","84435296":"markdown","b40d371f":"markdown","cf125e7e":"markdown","fb819228":"markdown","fdf11f9f":"markdown","1fdfd6c0":"markdown","03e9df41":"markdown","3d0b80e7":"markdown","3e31bd24":"markdown","f5c6f85c":"markdown","a39474fe":"markdown","b395a14a":"markdown","01a2a5b4":"markdown"},"source":{"679b5436":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport random\nimport os\nimport glob\nfrom tqdm.notebook import tqdm\nimport plotly\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Preprocessing the data\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nimport keras\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom keras.models import Model,load_model\nfrom tensorflow.keras.layers import Dense, Dropout,Conv2D,Flatten,MaxPooling2D\nfrom keras import backend as K\nfrom tensorflow.keras.layers.experimental import preprocessing","0e336f23":"# Look at the training, testing and validation data numbers\ntrain_data = glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/**\/*.jpeg')\ntest_data = glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/test\/**\/*.jpeg')\nval_data = glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/**\/*.jpeg')\n\nprint(f\"Training Set has: {len(train_data)} images\")\nprint(f\"Testing Set has: {len(test_data)} images\")\nprint(f\"Validation Set has: {len(val_data)} images\")","a84f7234":"DIR = \"..\/input\/chest-xray-pneumonia\/chest_xray\/\"\ndata = [\"train\", \"test\", \"val\"]\npneumonia_train = []\npneumonia_test = []\npneumonia_val = []\nnormal_train = []\nnormal_test = []\nnormal_val = []\n\nfor s in data:\n    if s==\"train\":\n        path = os.path.join(DIR, s)\n        norm = glob.glob(os.path.join(path, \"NORMAL\/*.jpeg\"))\n        pneu = glob.glob(os.path.join(path, \"PNEUMONIA\/*.jpeg\"))\n        normal_train.extend(norm)\n        pneumonia_train.extend(pneu)\n    elif s==\"test\":\n        path = os.path.join(DIR, s)\n        norm = glob.glob(os.path.join(path, \"NORMAL\/*.jpeg\"))\n        pneu = glob.glob(os.path.join(path, \"PNEUMONIA\/*.jpeg\"))\n        normal_test.extend(norm)\n        pneumonia_test.extend(pneu)\n    else:\n        path = os.path.join(DIR, s)\n        norm = glob.glob(os.path.join(path, \"NORMAL\/*.jpeg\"))\n        pneu = glob.glob(os.path.join(path, \"PNEUMONIA\/*.jpeg\"))\n        normal_val.extend(norm)\n        pneumonia_val.extend(pneu)\n\nprint(\"Train Set -->\")\nprint(f\"Total Pneumonia Images: {len(pneumonia_train)}\")\nprint(f\"Total Normal Images: {len(normal_train)}\")\n\nprint(\"Test Set -->\")\nprint(f\"Total Pneumonia Images: {len(pneumonia_test)}\")\nprint(f\"Total Normal Images: {len(normal_test)}\")\n\nprint(\"Val Set -->\")\nprint(f\"Total Pneumonia Images: {len(pneumonia_val)}\")\nprint(f\"Total Normal Images: {len(normal_val)}\")","6adab18e":"plot={\"train\":[pneumonia_train,normal_train],\"test\":[pneumonia_test,normal_test],\"val\":[pneumonia_val,normal_val]}","90adabc8":"from plotly.subplots import make_subplots\n#Create a 1x2 subplot\nfig = make_subplots(rows=1, cols=3,specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n# create empty figure trace\nfigure_traces = []\n# Get the Express fig broken down as traces and add the traces to the proper plot within in the subplot\ni=1\nfor p in plot:\n    fig.add_trace(go.Pie(labels=[\"Normal\",\"Pneumonia\"],\n            values=[len(plot[p][0]),len(plot[p][1])],\n            marker={'colors':['orange','darkred']},\n            hole=0.2),\n            row=1,col=i)\n    i+=1\n    fig.update_layout(template=\"plotly_dark\",title=\"Normal vs Pneumonia\")\nfig.show()","fbb13b5c":"### Train folder\nfig=plt.figure(figsize=(15, 15))\ncolumns = 3; rows = 4\nfor i in range(1, columns*rows +1):\n    if i <=6:\n        img = cv2.imread(glob.glob(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/*.jpeg\")[i])\n    else:\n        img = cv2.imread(glob.glob(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/*.jpeg\")[i])\n    img = cv2.resize(img, (128, 128))\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\n    plt.title(label=\"NORMAL\" if i<=6 else \"PNEUNOMIA\")\n    plt.axis(False)","02c07a63":"def subtract_gaussian_bg_image(im):\n    k = np.max(im.shape)\/10\n    bg = cv2.GaussianBlur(im ,(0,0) ,k)\n    return cv2.addWeighted (im, 4, bg, -4, 128)","2d87877b":"def subtract_median_bg_image(im):\n    k = np.max(im.shape)\/\/20*2+1\n    bg = cv2.medianBlur(im, k)\n    return cv2.addWeighted (im, 4, bg, -4, 128)","1a52879e":"def subtract_average_bg_image(im):\n    bg = cv2.blur(im, (10,10))\n    return bg","c3bb3e6a":"img_arr=[]\nfor i in range(0, 4):\n    img = cv2.imread(glob.glob(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/*.jpeg\")[i])\n    img = cv2.resize(img, (512, 512))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img1 = subtract_gaussian_bg_image(img)\n    img_arr.append(img1)\n    img2 = subtract_median_bg_image(img)\n    img_arr.append(img2)\n    img3 = subtract_average_bg_image(img)\n    img_arr.append(img3)","1e665ef2":"## Loop over 3 image to produce guassian bluring and median\nfig=plt.figure(figsize=(15, 15))\ncolumns = 3; rows = 4\nfor i in range(0, len(img_arr)):\n    fig.add_subplot(rows, columns, i+1)\n    plt.imshow(img_arr[i])\n    plt.axis(False)\n    if i+1 in [1,4,7,10]:\n        plt.title(\"Gaussian Blur\")\n    elif i+1 in [2,5,8,11]:\n        plt.title(\"Median Blur\")\n    elif i+1 in [3,6,9,12]:\n        plt.title(\"Average Blur\")","d8dc9900":"### Geometric transformations\n\ndef affine_tf(img):\n    rows, cols, ch = img.shape\n    pts1 = np.float32([[50, 50],\n                       [200, 50], \n                       [50, 200]])\n    pts2 = np.float32([[10, 100],\n                       [200, 50], \n                       [100, 250]]) \n    M = cv2.getAffineTransform(pts1, pts2)\n    dst = cv2.warpAffine(img, M, (cols, rows))\n    return dst\n\ndef rotation(img): \n    dst = cv2.rotate(img, cv2.cv2.ROTATE_90_CLOCKWISE)\n    return dst\n\ndef resize(img):\n    dst = cv2.resize(img, (780, 540),interpolation = cv2.INTER_NEAREST)\n    return dst\n\ndef colormaps(img):\n    dst = cv2.applyColorMap(img, cv2.COLORMAP_JET)\n    return dst","5b412f0c":"img_arr=[]\nfor i in range(0, 4):\n    img = cv2.imread(glob.glob(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/*.jpeg\")[i])\n    img1 = affine_tf(img)\n    img_arr.append(img1)\n    img2 = rotation(img)\n    img_arr.append(img2)\n    img3 = resize(img)\n    img_arr.append(img3)\n    img4 = colormaps(img)\n    img_arr.append(img4)","eaa08ed5":"## Loop over 4 image to produce guassian bluring and median\nfig=plt.figure(figsize=(15, 15))\ncolumns = 4; rows = 4\nfor i in range(0, len(img_arr)):\n    fig.add_subplot(rows, columns, i+1)\n    plt.imshow(img_arr[i])\n    plt.axis(False)\n    if i+1 in [1,5,9,13]:\n        plt.title(\"Affine Transform\")\n    elif i+1 in [2,6,10,14]:\n        plt.title(\"Rotation\")\n    elif i+1 in [3,7,11,15]:\n        plt.title(\"Resize\")\n    elif i+1 in [4,8,12,16]:\n        plt.title(\"colormaps\")","77646b84":"## Images are of different sizes fixing the size to 180\nimageSize = 180\nbatchSize = 32\nNUM_CLASSES = 2 ## since there are 2 distinct classes","6ffe830e":"# Define Training and Validation Data Generator with Augmentations\ngen = ImageDataGenerator(\n    rescale=1\/255.,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rotation_range=0.4,\n    zoom_range=0.4\n)","90b6769e":"# Flow the data into the Data Generator\nTrain_gen = gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\",\n    target_size=(180, 180),\n    batch_size=16\n)\nTest_gen = gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/test\",\n    target_size=(180, 180),\n    batch_size=8\n)","69532b6a":"Train_gen.class_indices","edece2c3":"model=tf.keras.Sequential([\n                 Conv2D(filters=128, kernel_size=(3,3), activation='relu', input_shape=(180, 180, 3)),\n                 MaxPooling2D(pool_size=(2,2)),\n                 Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n                 MaxPooling2D(pool_size=(2,2)),\n                 Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n                 MaxPooling2D(pool_size=(2,2)),\n                 Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n                 MaxPooling2D(pool_size=(2,2)),\n                 Flatten()\n])\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))","f5847c33":"model.summary()","8b02f148":"# Compile the model and see it's structure and parameters\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","8749271a":"!pip install visualkeras","1344c231":"import visualkeras\nvisualkeras.layered_view(model).show() # display using your system viewer\n##visualkeras.layered_view(model, to_file='output.png') # write to disk\nvisualkeras.layered_view(model, to_file='output.png').show() # write and show\n\nvisualkeras.layered_view(model)","ec266ffb":"from keras.utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)","f57e6b4a":"## we will run for 5 epoch only\nhist = model.fit(\n    Train_gen,\n    epochs=5,\n    validation_data=Test_gen\n)","191f5efb":"# Save the model\nmodel.save(\"best_model.hdf5\")","b7677b06":"# plt.figure(figsize=(16, 9))\n# plt.plot(hist.history['loss'], label=\"Train Loss\")\n# plt.plot(hist.history['val_loss'], label=\"Valid Loss\")\n# plt.legend()\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Loss\")\n# plt.title(\"Loss over the Epochs\")\n# plt.show()\n\n# summarize history for metric\nfig=go.Figure()\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['loss'],\n                         name=\"Training MAE\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='green')))\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['val_loss'],\n                         name=\"Validation MAE\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='red')))\nfig.update_layout(title=\"CNN Model Loss: Model Metric - Training and Validation\",\n                  xaxis_title=\"Epochs\",\n                  yaxis_title=\"Loss\",\n                  template=\"plotly_dark\",\n#                   annotations=[dict(showarrow=True,\n#                                     x=10,\n#                                     y=1.16,\n#                                     ax=0,\n#                                     ay=-40,\n#                                     text=\"MobileNet : Model stopped : <br>Overfitting Zone\",\n#                                     xanchor=\"center\",\n#                                     xshift=10,\n#                                     opacity=0.7,\n#                                     font=dict(\n#                                     color=\"yellow\",\n#                                     size=12),\n#                                     arrowcolor=\"yellow\",\n#                                     arrowsize=5,\n#                                     arrowwidth=0.5,\n#                                     arrowhead=2)\n#                               ]\n                 )\n# fig.add_shape(type=\"rect\",\n#     x0=9.5, y0=1.103, x1=13.5, y1=1.15,\n#     line=dict(\n#         color=\"yellow\",\n#         width=1,\n#         dash='dash'\n#     ),\n# )\nfig.show()","7c023a70":"# Evaluate the model\nscores = model.evaluate_generator(Test_gen)\nprint(\"Loss of the model: %.2f\"%(scores[0]))\nprint(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))","59892ce7":"Val_gen = gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/val\",\n    target_size=(180, 180),\n    batch_size=1\n)","8c32a140":"Val_gen.class_indices","e3fad4fb":"Val_gen.n### Batch size","0309e322":"STEP_SIZE_TEST=Val_gen.n\nVal_gen.reset()\nVal_pred=model.predict_generator(Val_gen,steps=STEP_SIZE_TEST,verbose=1)","78fbeac5":"## get prediction for Val data set\nprediction=[]\nfor i in range(Val_pred.shape[0]):\n    prediction.append(\"Normal\" if Val_pred[i][0]>0.5 else \"Pneumonia\")","76103f97":"val_data={}\nfor i in glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL\/*.jpeg'):\n    val_data[i[-25:]]=\"NORMAL\"\nfor i in glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA\/*.jpeg'):\n    val_data[i[-29:]]=\"PNEUMONIA\"","2922a938":"val_data","07c89f7a":"val_df=pd.DataFrame(columns=['image','label','prediction'])\nimage=[]\nlabel=[]\nfor k,v in val_data.items():\n    image.append(k)\n    label.append(v)\n    \nval_df['image']=image\nval_df['label']=label\nval_df['prediction']=prediction","a0db368d":"val_df","8a306ffa":"## Images are of different sizes fixing the size to 180\nimageSize = 180\nbatchSize = 32\nNUM_CLASSES = 2 ## since there are 2 distinct classes","d2755f08":"# Define Training and Validation Data Generator with Augmentations\ngen = ImageDataGenerator(\n    rescale=1\/255.,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rotation_range=0.4,\n    zoom_range=0.4\n)","a065f29b":"# Flow the data into the Data Generator\nTrain_gen = gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\",\n    target_size=(180, 180),\n    batch_size=16\n)\nTest_gen = gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/test\",\n    target_size=(180, 180),\n    batch_size=8\n)","7cd6e26a":"Train_gen.class_indices","ec15e0a2":"model_densetnet = tf.keras.applications.DenseNet201(input_shape=(180,180,3),include_top=False,weights='imagenet',pooling='avg')\nmodel_densetnet.trainable = False","8cf4c22b":"input = model_densetnet.input\n# Rebuild top\nx = tf.keras.layers.Dense(128,activation=\"relu\")(model_densetnet.output)\nx = tf.keras.layers.BatchNormalization()(x)\ntop_dropout_rate = 0.2\nx = tf.keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\noutput = tf.keras.layers.Dense(2, activation='softmax')(x)\nmodel = tf.keras.Model(inputs=input, outputs=output)","554a33f8":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","b11b3b0f":"model.summary()","faf35b5a":"### we will train for 5 epochs only\nhist = model.fit(\n    Train_gen,\n    epochs=5,\n    validation_data=Test_gen\n)","453f3ae4":"model.save(\"DenseNet201.h5\")","221e1e79":"# plt.figure(figsize=(16, 9))\n# plt.plot(hist.history['loss'], label=\"Train Loss\")\n# plt.plot(hist.history['val_loss'], label=\"Valid Loss\")\n# plt.legend()\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Loss\")\n# plt.title(\"Loss over the Epochs\")\n# plt.show()\n\n# summarize history for metric\nfig=go.Figure()\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['loss'],\n                         name=\"Training Loss\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='green')))\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['val_loss'],\n                         name=\"Validation Loss\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='red')))\nfig.update_layout(title=\"DenseNet : Model Metric - Training and Validation\",\n                  xaxis_title=\"Epochs\",\n                  yaxis_title=\"Loss\",\n                  template=\"plotly_dark\",\n#                   annotations=[dict(showarrow=True,\n#                                     x=10,\n#                                     y=1.16,\n#                                     ax=0,\n#                                     ay=-40,\n#                                     text=\"MobileNet : Model stopped : <br>Overfitting Zone\",\n#                                     xanchor=\"center\",\n#                                     xshift=10,\n#                                     opacity=0.7,\n#                                     font=dict(\n#                                     color=\"yellow\",\n#                                     size=12),\n#                                     arrowcolor=\"yellow\",\n#                                     arrowsize=5,\n#                                     arrowwidth=0.5,\n#                                     arrowhead=2)\n#                               ]\n                 )\n# fig.add_shape(type=\"rect\",\n#     x0=9.5, y0=1.103, x1=13.5, y1=1.15,\n#     line=dict(\n#         color=\"yellow\",\n#         width=1,\n#         dash='dash'\n#     ),\n# )\nfig.show()","2f224033":"# plt.figure(figsize=(16, 9))\n# plt.plot(hist.history['accuracy'], label=\"Train Accuracy\")\n# plt.plot(hist.history['val_accuracy'], label=\"Valid Accuracy\")\n# plt.legend()\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Accuracy\")\n# plt.title(\"Accuracy over the Epochs\")\n# plt.show()\n\n\n# summarize history for metric\nfig=go.Figure()\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['accuracy'],\n                         name=\"Training Acc\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='green')))\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['val_accuracy'],\n                         name=\"Validation Acc\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='red')))\nfig.update_layout(title=\"DenseNet : Model Metric - Training and Validation\",\n                  xaxis_title=\"Epochs\",\n                  yaxis_title=\"Accuracy\",\n                  template=\"plotly_dark\",\n#                   annotations=[dict(showarrow=True,\n#                                     x=10,\n#                                     y=1.16,\n#                                     ax=0,\n#                                     ay=-40,\n#                                     text=\"MobileNet : Model stopped : <br>Overfitting Zone\",\n#                                     xanchor=\"center\",\n#                                     xshift=10,\n#                                     opacity=0.7,\n#                                     font=dict(\n#                                     color=\"yellow\",\n#                                     size=12),\n#                                     arrowcolor=\"yellow\",\n#                                     arrowsize=5,\n#                                     arrowwidth=0.5,\n#                                     arrowhead=2)\n#                               ]\n                 )\n# fig.add_shape(type=\"rect\",\n#     x0=9.5, y0=1.103, x1=13.5, y1=1.15,\n#     line=dict(\n#         color=\"yellow\",\n#         width=1,\n#         dash='dash'\n#     ),\n# )\nfig.show()","877761ee":"# Evaluate the model\nscores = model.evaluate_generator(Test_gen)\nprint(\"Loss of the model: %.2f\"%(scores[0]))\nprint(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))","62833715":"Val_gen = gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/val\",\n    target_size=(180, 180),\n    batch_size=1\n)","c73c60a9":"# Evaluate the model\nscores = model.evaluate_generator(Val_gen)\nprint(\"Loss of the model: %.2f\"%(scores[0]))\nprint(\"Val Accuracy: %.2f%%\"%(scores[1] * 100))","eb1d712c":"STEP_SIZE_TEST=Val_gen.n\nVal_gen.reset()\nVal_pred=model.predict_generator(Val_gen,steps=STEP_SIZE_TEST,verbose=1)","667f7887":"## get prediction for Val data set\nprediction=[]\nfor i in range(Val_pred.shape[0]):\n    prediction.append(\"Normal\" if Val_pred[i][0]>0.5 else \"Pneumonia\")","c6150033":"val_data={}\nfor i in glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL\/*.jpeg'):\n    val_data[i[-25:]]=\"NORMAL\"\nfor i in glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA\/*.jpeg'):\n    val_data[i[-29:]]=\"PNEUMONIA\"","727286c2":"val_df=pd.DataFrame(columns=['image','label','prediction'])\nimage=[]\nlabel=[]\nfor k,v in val_data.items():\n    image.append(k)\n    label.append(v)\n    \nval_df['image']=image\nval_df['label']=label\nval_df['prediction']=prediction","30ed370f":"val_df","b7066833":"im_size = 299\nBATCH_SIZE = 64\nn_labels = 2","a6d3b46e":"# Define Training and Validation Data Generator with Augmentations\ngen = ImageDataGenerator(\n    rescale=1\/255.,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rotation_range=0.4,\n    zoom_range=0.4\n)","3285c3b0":"# Flow the data into the Data Generator\nTrain_gen = gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\",\n    target_size=(im_size, im_size),\n    batch_size=BATCH_SIZE\n)\nTest_gen = gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/test\",\n    target_size=(im_size, im_size),\n    batch_size=BATCH_SIZE\n)","e5c46248":"base_model = tf.keras.applications.Xception(\nweights=\"imagenet\",  # Load weights pre-trained on ImageNet.\ninput_shape=(299, 299, 3),\ninclude_top=False)  # Do not include the ImageNet classifier at the top.\n\n# Freeze the base_model\nbase_model.trainable = False\n\n# The base model contains batchnorm layers. We want to keep them in inference mode\n# when we unfreeze the base model for fine-tuning, so we make sure that the\n# base_model is running in inference mode here.\ninputs = base_model.input\n#x = tf.keras.layers.Dense(128,activation=\"relu\")(base_model.output)\nx = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\nx = tf.keras.layers.Dropout(0.2)(x)  # Regularize with dropout\noutputs = tf.keras.layers.Dense(2)(x)\nmodel = tf.keras.Model(inputs, outputs)\n\nmodel.summary()","caf8540a":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nepochs = 10\nhistory=model.fit(Train_gen, epochs=epochs, validation_data=Test_gen)","f368fffe":"model.save(\"xception.h5\")","ef5db183":"# plt.figure(figsize=(16, 9))\n# plt.plot(hist.history['loss'], label=\"Train Loss\")\n# plt.plot(hist.history['val_loss'], label=\"Valid Loss\")\n# plt.legend()\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Loss\")\n# plt.title(\"Loss over the Epochs\")\n# plt.show()\n\n# summarize history for metric\nfig=go.Figure()\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['loss'],\n                         name=\"Training loss\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='green')))\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['val_loss'],\n                         name=\"Validation loss\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='red')))\nfig.update_layout(title=\"Xception : Model Metric - Training and Validation\",\n                  xaxis_title=\"Epochs\",\n                  yaxis_title=\"Loss\",\n                  template=\"plotly_dark\",\n#                   annotations=[dict(showarrow=True,\n#                                     x=10,\n#                                     y=1.16,\n#                                     ax=0,\n#                                     ay=-40,\n#                                     text=\"MobileNet : Model stopped : <br>Overfitting Zone\",\n#                                     xanchor=\"center\",\n#                                     xshift=10,\n#                                     opacity=0.7,\n#                                     font=dict(\n#                                     color=\"yellow\",\n#                                     size=12),\n#                                     arrowcolor=\"yellow\",\n#                                     arrowsize=5,\n#                                     arrowwidth=0.5,\n#                                     arrowhead=2)\n#                               ]\n                 )\n# fig.add_shape(type=\"rect\",\n#     x0=9.5, y0=1.103, x1=13.5, y1=1.15,\n#     line=dict(\n#         color=\"yellow\",\n#         width=1,\n#         dash='dash'\n#     ),\n# )\nfig.show()","eff5987e":"# plt.figure(figsize=(16, 9))\n# plt.plot(hist.history['accuracy'], label=\"Train Accuracy\")\n# plt.plot(hist.history['val_accuracy'], label=\"Valid Accuracy\")\n# plt.legend()\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Accuracy\")\n# plt.title(\"Accuracy over the Epochs\")\n# plt.show()\n\n# summarize history for metric\nfig=go.Figure()\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['accuracy'],\n                         name=\"Training accuracy\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='green')))\nfig.add_trace(go.Scatter(x=[n for n in range(1,6)],\n                         y=hist.history['val_accuracy'],\n                         name=\"Validation accuracy\",\n                         mode=\"markers+lines\",\n                         marker=dict(color='red')))\nfig.update_layout(title=\"Xception : Model Metric - Training and Validation\",\n                  xaxis_title=\"Epochs\",\n                  yaxis_title=\"Loss\",\n                  template=\"plotly_dark\",\n#                   annotations=[dict(showarrow=True,\n#                                     x=10,\n#                                     y=1.16,\n#                                     ax=0,\n#                                     ay=-40,\n#                                     text=\"MobileNet : Model stopped : <br>Overfitting Zone\",\n#                                     xanchor=\"center\",\n#                                     xshift=10,\n#                                     opacity=0.7,\n#                                     font=dict(\n#                                     color=\"yellow\",\n#                                     size=12),\n#                                     arrowcolor=\"yellow\",\n#                                     arrowsize=5,\n#                                     arrowwidth=0.5,\n#                                     arrowhead=2)\n#                               ]\n                 )\n# fig.add_shape(type=\"rect\",\n#     x0=9.5, y0=1.103, x1=13.5, y1=1.15,\n#     line=dict(\n#         color=\"yellow\",\n#         width=1,\n#         dash='dash'\n#     ),\n# )\nfig.show()","e84a74b9":"# Evaluate the model\nscores = model.evaluate_generator(Test_gen)\nprint(\"Loss of the model: %.2f\"%(scores[0]))\nprint(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))","7991b93f":"Val_gen = gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/val\",\n    target_size=(299, 299),\n    batch_size=1\n)","49347664":"# Evaluate the model\nscores = model.evaluate_generator(Val_gen)\nprint(\"Loss of the model: %.2f\"%(scores[0]))\nprint(\"Val Accuracy: %.2f%%\"%(scores[1] * 100))","c33f3f80":"STEP_SIZE_TEST=Val_gen.n\nVal_gen.reset()\nVal_pred=model.predict_generator(Val_gen,steps=STEP_SIZE_TEST,verbose=1)","999bf0b2":"## get prediction for Val data set\nprediction=[]\nfor i in range(Val_pred.shape[0]):\n    prediction.append(\"Normal\" if Val_pred[i][0]>0.5 else \"Pneumonia\")","422cc279":"val_data={}\nfor i in glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL\/*.jpeg'):\n    val_data[i[-25:]]=\"NORMAL\"\nfor i in glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA\/*.jpeg'):\n    val_data[i[-29:]]=\"PNEUMONIA\"","2a1dd9c2":"val_df=pd.DataFrame(columns=['image','label','prediction'])\nimage=[]\nlabel=[]\nfor k,v in val_data.items():\n    image.append(k)\n    label.append(v)\n    \nval_df['image']=image\nval_df['label']=label\nval_df['prediction']=prediction","506252f3":"val_df","ea3277c8":"### Predict on Val set","5538dfad":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Simple CNN model&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","b0bc9221":"### Visualize the model","c2b846c1":"### X rays with Gaussian Bluring + Median Bluring + Average Bluring","06393f7d":"### Add DenseNet201 layer","e93d6978":"<a id=\"PS\"><\/a>\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 20px;\">\n        <img align='left' src=\"https:\/\/emojis.slackmojis.com\/emojis\/images\/1621897920\/41239\/question_box.gif?1621897920\">\n    Problem Statement and Introduction to Dataset\n<\/div><\/center>\n\n    \nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia\/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia\/Normal).\n\nChest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients\u2019 routine clinical care.\n\nFor the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n","ec357527":"### Plot the loss ","99e1885b":"<a id=\"eda\"><\/a>\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 20px;\">\n    <img src=\"https:\/\/cdn4.iconfinder.com\/data\/icons\/science-and-technology-5-7\/65\/229-512.png\" align='left' width=50>\n    EDA and Image Augmentation\n    <\/div><\/center>","5275ef1f":"### Get Actual Classes","c53f30a4":"### Predict on Val set","5af04596":"### Greyscale","56ff6c56":"### Averaging\n\n<p style=\"color:orange;font-size:15px\">During this operation, the image is convolved with a box filter (normalized). In this process, the central element of the image is replaced by the average of all the pixels in the kernel area.<\/p>","ed56e674":"<a id=\"model\"><\/a>\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 20px;\">\n    <img src=\"https:\/\/cdn4.iconfinder.com\/data\/icons\/data-science-blue-red\/60\/040_-_Algorithm-512.png\" align=\"left\" width=60>\n    Model Building<\/div><\/center>","dccf00bd":"### Fit the model","ea19b322":"### Plot the loss curve","4b5aa837":"<h3 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Model Building&nbsp;&nbsp;&nbsp;&nbsp;<\/h3>\n<center><img class=\"emojidex-emoji\" src=\"https:\/\/cdn.emojidex.com\/emoji\/seal\/processing.png\" emoji-code=\"processing\" alt=\"processing\" ><\/center>","f103dd81":"### Prepare Image Data Generator","afb732e0":"### Get actual classes","439d1094":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Transformations&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>\n\n<b>1. Affine Transformation<\/b>\n\nIn Affine transformation, all parallel lines in the original image will still be parallel in the output image. To find the transformation matrix, we need three points from input image and their corresponding locations in the output image. Then cv2.getAffineTransform will create a 2\u00d73 matrix which is to be passed to cv2.warpAffine.\n\n<b>2. Rotation<\/b>\n\ncv2.rotate() method is used to rotate a 2D array in multiples of 90 degrees.\n\n<b>3. Scaling\/Resizing<\/b>\n\nImage resizing refers to the scaling of images. Scaling comes in handy in many image processing as well as machine learning applications. It helps in reducing the number of pixels from an image and that has several advantages e.g. It can reduce the time of training of a neural network as more is the number of pixels in an image more is the number of input nodes that in turn increases the complexity of the model.\nIt also helps in zooming in images. Many times we need to resize the image i.e. either shrink it or scale up to meet the size requirements. OpenCV provides us several interpolation methods for resizing an image.\n\n<b>4. Color Maps<\/b>\n\nApplying different types of color maps.","26edfa88":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">DenseNet201&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","f75bb0a8":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Get the class labels in Train,Test and Val&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","460f917e":"### Predict on Val set","293e256a":"### Median\n\n<p style=\"color:orange;font-size:15px\">The Median blur operation is similar to the other averaging methods. Here, the central element of the image is replaced by the median of all the pixels in the kernel area. This operation processes the edges while removing the noise.\n    <\/p>","7a4f66fb":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Read the Dataset&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","efcc8399":"<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 20px;\">\n    <img src=\"https:\/\/cdn1.iconfinder.com\/data\/icons\/artificial-intelligence-processes\/3600\/10-512.png\" align='left' width=50>\n    Xception Architechture\n<\/div><\/center>\n    \n<h4>1. XceptionNet<\/h4>\n    \nConvolutional Neural Networks (CNN) have come a long way, from the LeNet-style, AlexNet, VGG models, which used simple stacks of convolutional layers for feature extraction and max-pooling layers for spatial sub-sampling, stacked one after the other, to Inception and ResNet networks which use skip connections and multiple convolutional and max-pooling blocks in each layer. Since its introduction, one of the best networks in computer vision has been the Inception network. The Inception model uses a stack of modules, each module containing a bunch of feature extractors, which allow them to learn richer representations with fewer parameters.\n    \n<center><img src=\"https:\/\/miro.medium.com\/max\/1666\/1*t6qfo9ucYza_lbLfg5-p_w.png\"><\/center>\n    \nAs we see in figure above, the Xception module has 3 main parts. The Entry flow, the Middle flow (which is repeated 8 times), and the Exit flow.\n    \nThe entry flow has two blocks of convolutional layer followed by a ReLU activation. The diagram also mentions in detail the number of filters, the filter size (kernel size), and the strides.\n    \nThere are also various Separable convolutional layers. There are also Max Pooling layers. When the strides are different than one, the strides are also mentioned. There are also Skip connections, where we use \u2018ADD\u2019 to merge the two tensors. It also shows the shape of the input tensor in each flow. For example, we begin with an image size of 299x299x3, and after the entry flow, we get an image size of 19x19x728.\n    \nSimilarly, for the Middle flow and the Exit flow, this diagram clearly explains the image size, the various layers, the number of filters, the shape of filters, the type of pooling, the number of repetitions, and the option of adding a fully connected layer in the end.\n\nAlso, all Convolutional and Separable Convolutional layers are followed by batch normalization.\n    \n<h4>2. Separable Layers<\/h4>\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*DfE-E_4TqPKbn-5J9EDHww.png\">\n    \n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">   \nSeparable convolutions consist of first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes the resulting output channels.- From Keras Documentation\n    <\/div><\/center>\n    \n<h4>3. Why is separable convolution better than normal convolution?<\/h4>\n    \nIf we were to use a normal convolution on the input tensor, and we use a filter\/kernel size of 3x3x3 (kernel size \u2014 (3,3) and 3 feature maps). And the total number of filters we want is 64. So a total of 3x3x3x64.\nInstead, in separable convolution, we first use 3x3x1x3 in depthwise convolution and 1x1x3x64 in pointwise convolution.\n    \nThe difference lies in the dimensionality of the filters.<br>\n<b>Traditional Convolutional layer = 3x3x3x64 = 1,728<\/b><br>\n<b>Separable Convolutional layer = (3x3x1x3)+(1x1x3x64) = 27+192 = 219<\/b>\n\nAs we see, separable convolution layers are way more advantageous than traditional convolutional layers, both in terms of computation cost as well as memory. The main difference is that in the normal convolution, we are transforming the image multiple times. And every transformation uses up 3x3x3x64 = 1,728 multiplications. In the separable convolution, we only transform the image once \u2014 in the depthwise convolution. Then, we take the transformed image and simply elongate it to 64 channels. Without having to transform the image over and over again, we can save up on computational power.","ab4ec5d4":"### Plot the accuracy","482ffcbe":"<h1 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: orange; background-color: #ffffff;\">Pneumonia Detection Using Chest Xrays<\/h1>\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><font color=\"red\">CNN+DenseNet201+Xception<\/font><\/h2>","25ffc874":"<a id=\"results\"><\/a>\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 20px;\">\n    <img src=\"https:\/\/image.flaticon.com\/icons\/png\/512\/992\/992848.png\" align=\"left\" width=40>\n    Conclusion and Results\n<\/div><\/center>\n    \n* Using the simple version of CNN with multiple layers of Con2d with Maxpooling2d reached a accuracy of 82% on the test set.\n* Using the DenseNet201 reached a accuracy of 85.4% on the test set.\n* Using the Xception reached a accuracy of 74.4% on the test set.\n* The next version of notebook will deal with the pretrained models like Resnet50.","50e73e73":"<h3 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Model Building&nbsp;&nbsp;&nbsp;&nbsp;<\/h3>\n<center><img class=\"emojidex-emoji\" src=\"https:\/\/cdn.emojidex.com\/emoji\/seal\/processing.png\" emoji-code=\"processing\" alt=\"processing\" ><\/center>","24059d70":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Bluring&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","da9979eb":"### Evaluate the model on Test set","50736f38":"### Get prediction for all xrays in Val","f31c0534":"### Quite a few fluctuation in the loss of the Test set are observed.","8df240b9":"### Accuracy plot","875b9fb6":"### Cut off at epoch =5; post that the val accuracy decreases.","02540a12":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Xception&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","fe109f25":"### Get actual classes","84435296":"### Gaussian Blur\n\n<p style=\"color:orange;font-size:15px\">In Gaussian Blur operation, the image is convolved with a Gaussian filter instead of the box filter. The Gaussian filter is a low-pass filter that removes the high-frequency components are reduced.\n    <\/p>","b40d371f":"### Add Model Layers","cf125e7e":"### Trying more epcohs may give better results on validation set","fb819228":"### Plot the model","fdf11f9f":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Import Libraries&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","1fdfd6c0":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Distribution plots&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","03e9df41":"<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 20px;\">\n    <img src=\"https:\/\/cdn1.iconfinder.com\/data\/icons\/artificial-intelligence-processes\/3600\/10-512.png\" align='left' width=50>\n    DenseNet Architechture\n<\/div><\/center>\n    \n<center><img src=\"https:\/\/miro.medium.com\/max\/948\/1*GeK21UAbk4lEnNHhW_dgQA.png\"><\/center>\n    \n* Background\n\nDensely Connected Convolutional Networks, DenseNets, are the next step on the way to keep increasing the depth of deep convolutional networks.\n\n* Motivation\n    \nThe problems arise with CNNs when they go deeper. This is because the path for information from the input layer until the output layer (and for the gradient in the opposite direction) becomes so big, that they can get vanished before reaching the other side.\nDenseNets simplify the connectivity pattern between layers introduced in other architectures:\n1. Highway Networks\n2. Residual Networks\n3. Fractal Networks\n    \nThe authors solve the problem ensuring maximum information (and gradient) flow. To do it, they simply connect every layer directly with each other.\nInstead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse.\n \n* What problem DenseNets solve?\n    \nCounter-intuitively, by connecting this way DenseNets require fewer parameters than an equivalent traditional CNN, as there is no need to learn redundant feature maps.\nFurthermore, some variations of ResNets have proven that many layers are barely contributing and can be dropped. In fact, the number of parameters of ResNets are big because every layer has its weights to learn. Instead, DenseNets layers are very narrow (e.g. 12 filters), and they just add a small set of new feature-maps.\nAnother problem with very deep networks was the problems to train, because of the mentioned flow of information and gradients. DenseNets solve this issue since each layer has direct access to the gradients from the loss function and the original input image.\n    \n* Structure\n    \nTraditional feed-forward neural networks connect the output of the layer to the next layer after applying a composite of operations.\nWe have already seen that normally this composite includes a convolution operation or pooling layers, a batch normalization and an activation function.\n\nThe equation for this would be:\n\n <b>x<sub>l<\/sub>=H<sub>l<\/sub>(x<sub>l-1<\/sub>)<\/b>\n\nResNets extended this behavior including the skip connection, reformulating this equation into:\n\n<b>x<sub>l<\/sub>=H<sub>l<\/sub>(x<sub>l-1<\/sub>)<\/b> + <b>x<sub>l-1<\/sub><\/b>\n    \nDenseNets make the first difference with ResNets right here. DenseNets do not sum the output feature maps of the layer with the incoming feature maps but concatenate them.\nConsequently, the equation reshapes again into:\n\n<b>x<sub>l<\/sub>=H<sub>l<\/sub>([x<sub>0<\/sub>,x<sub>1<\/sub>,x<sub>2<\/sub>,....,x<sub>l-1<\/sub>])\n\nResNets, DenseNets are divided into DenseBlocks, where the dimensions of the feature maps remains constant within a block, but the number of filters changes between them. These layers between them are called Transition Layers and take care of the downsampling applying a batch normalization, a 1x1 convolution and a 2x2 pooling layers.\n    \nNow we are ready to talk about the growth rate. Since we are concatenating feature maps, this channel dimension is increasing at every layer. If we make H_l to produce k feature maps every time, then we can generalize for the l-th layer:\n    \nk<sub>l<\/sub>=k<sub>0<\/sub> +k * (l-1)\n\nThis hyperparameter k is the growth rate. The growth rate regulates how much information is added to the network each layer. How so?\nWe could see the feature maps as the information of the network. Every layer has access to its preceding feature maps, and therefore, to the collective knowledge. Each layer is then adding a new information to this collective knowledge, in concrete k feature maps of information.\n\nReference: https:\/\/towardsdatascience.com\/understanding-and-visualizing-densenets-7f688092391a\n","3d0b80e7":"<html>\n    <p style='color:Black; font-size:30px; font-family:Papyrus;padding:3px;text-align:center'><b>What Does a Chest X-Ray Show?<\/b><\/p>\n\n<center><img src=\"https:\/\/images.medicinenet.com\/images\/article\/main_image\/chest-x-ray.jpg\"><\/center>\n    \n* A chest X-ray is most commonly used to detect abnormalities in the lungs, but can also detect abnormalities in the heart, aorta, and the bones of the thoracic area.\n    \n* Extra metallic objects, such as jewelry are removed from the chest and neck areas for a chest X-ray to avoid interference with X-ray penetration and improve the accuracy of the interpretation.\n    \n<b>How Pneumonia is detected:<\/b>\n\nChest x-ray: An x-ray exam will allow your doctor to see your lungs, heart and blood vessels to help determine if you have pneumonia. When interpreting the x-ray, the radiologist will look for white spots in the lungs (called infiltrates) that identify an infection. This exam will also help determine if you have any complications related to pneumonia such as abscesses or pleural effusions (fluid surrounding the lungs).","3e31bd24":"### Evaluate the model on test set","f5c6f85c":"### Plot the loss","a39474fe":"### Evaluate the model on Val set","b395a14a":"### Evaluate on Val set","01a2a5b4":"<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <img src=\"https:\/\/cdn2.iconfinder.com\/data\/icons\/digital-and-internet-marketing-3-1\/50\/144-512.png\" width=50 align='left'>\n    Contents\n<\/div><\/center>\n    \n<p id=\"toc\"><\/p>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#PS\">&nbsp;&nbsp;&nbsp;&nbsp;1.Problem Statement<\/a><\/h3>\n\n---\n    \n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#eda\">&nbsp;&nbsp;&nbsp;&nbsp;2.EDA and Image Augmentation<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#model\">&nbsp;&nbsp;&nbsp;&nbsp;3.Model Building<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#results\">&nbsp;&nbsp;&nbsp;&nbsp;4.Conclusion and Results<\/a><\/h3>\n\n---\n"}}