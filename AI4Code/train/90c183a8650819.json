{"cell_type":{"bdef761e":"code","a66c988f":"markdown","52f614f5":"markdown","84ff5089":"markdown","9c9b5cc0":"markdown","aad0476a":"markdown","a36c6c95":"markdown"},"source":{"bdef761e":"\"\"\"\nfrom google.cloud import storage\nfrom zipfile import ZipFile\nfrom tqdm import tqdm\n\n# Get the bucket\ndef getstoragebucket():\n    # Create a client instance\n    storage_client = storage.Client()\n    # Get bucket name\n    bucket = storage_client.get_bucket('<bucket name>')\n    return bucket\n\n# read zip files and copy on storage bucket\ndef copy_images_on_data_storage(filename):\n    bucket = getstoragebucket()\n    with ZipFile(filename, 'r') as myzip:\n        for contentfilename in tqdm(myzip.namelist()):\n            contentfile = myzip.read(contentfilename)\n            # create a blob with same hierarchy\n            blob = bucket.blob(contentfilename)\n            blob.upload_from_string(contentfile)\n            \nif __name__ == '__main__':\n    filename = f'<absolute path of data .zip file'\n    # Example \/home\/<username>\/...\/<.zip file name>\n    copy_images_on_data_storage(filename)\n    print('Successfully copied all file in storage bucket')\n\"\"\"","a66c988f":"## Create VM Instance\nLogin with your account id\n\nCreate a new VM instance \n\nGo to Console -> Compute Engine -> VM Instances\n\nClick CREATE INSTANCE\n\nfor details click [https:\/\/cloud.google.com\/compute\/docs\/instances\/create-start-instance](http:\/\/)\n\nKeep boot disk size atleast 200GB as we have to download data zip file. (Check data size before changing boot disk). By default it is 100GB.\n\nFollow this link for details https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/compute\/disks\/resize \n\nYou can do using GUI also. Go to Compute Engine -> Disks. It will list down all persistence disks attached to VM. Click on the disk and edit.","52f614f5":"## Unzip and copy data on newly created storage\n\nI have written a script for reading zipped files and storing on storage account\n\nThis script can run from two places:\n\nBoth places i prefer to run the script using tmux. In this case it does not depends on local machine connection or connection with Jupyter Lab server if using Jupyter Lab for running the script as described below.\n\n1. Console -> Compute Engine -> VM instance -> SSH -> Open in Browser Window\n\n2. Console -> AI Platform\n\n     Create Notebook ( This i created to run my experiments)\n     \n     Click Open JupyterLab - It will start jupyter session.\n     \n     From JupyterLab -> File -> New -> Terminal and follow same commands as mentioned below.\n     \nTo start with i prefer first option as it is simple.\n     \n\nRun below commands\n\ntmux\n\nsudo nano copy_images.py\n\nCopy below script, save and exit\n\npython copy_images.py\n\nChange values in between angle brackets (<>)\n\nTwo things to change : bucket name and absolute path of zip file\n","84ff5089":"## Install kaggle\n\nGo to Console -> Compute Engine -> VM Instances -> Select VM instance and start it.\n\nOn right most corner click SSH and select \"Open in Browser Window\"\n\nRun either of the commands\n\npip install kaggle\n\nor\n\nconda install -c conda-forge kaggle\n\nGo to kaggle.com -> My Account\n\nScroll down and click \"Create API Token\". It will start downloading kaggle.json \n\nGo to \/home\/<username>\/\n    \nCheck for .kaggle folder if not there, create new folder named as \u201c.kaggle\u201d\n\nsudo mkdir .kaggle\n\ncd .kaggle\n\n### Copy content of kaggle.json by creating a new file with same name on console\n\nsudo nano kaggle.json\n\ncopy the contents of locally downloaded kaggle.json, save and exit\n\n### Command to give permission to kaggle.json\n\nchmod 600 kaggle.json \n\nYou can try with 755 if does not work.\n\n\n","9c9b5cc0":"## Create storage \n\nGo to Console -> Storage -> CREATE BUCKET\n\nGive unique name for your bucket\n\nGive size, like i created storage for 500GB. It can be incresed later so can start with less also.\n","aad0476a":"## Download data zip file on VM instance\n\nGo to Console -> Compute Engine -> VM Instance \n\nStart VM instance on which you want to download data\n\nClick SSH -> Open in Browser Window\n\nMake a folder in which data should be downloaded.\n\nChange directory to newly created directory and run below command\n\nkaggle competitions download -c  <competition name> \n\nCan copy this command from data tab of competition.\n\nIt will start downloading file and it will not take much time\n","a36c6c95":"Hope I have covered almost everything required to unzip and copy files on google storage account. But it took around 50 hours for 180GB .zip file, which is quite long. Please let me know if someone knows a quicker solution. \n\nHope it helps."}}