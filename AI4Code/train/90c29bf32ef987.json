{"cell_type":{"5f67579d":"code","601a1fed":"code","167dbc0c":"code","c28ff759":"code","42520387":"code","86b0785d":"code","2afde6aa":"code","9078ba7e":"code","a4042789":"code","fdd7c9b5":"code","9da6923f":"code","e603e456":"code","be05744f":"code","ae865160":"code","e31ca1eb":"code","ccb619e5":"markdown","66d5fd73":"markdown","9726fb2f":"markdown","60626f69":"markdown","3e9f0ce9":"markdown","69294185":"markdown","b2e268da":"markdown","62b85c93":"markdown","e2e16aef":"markdown","ed232223":"markdown"},"source":{"5f67579d":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport random\nfrom numpy import linalg\nfrom sklearn.linear_model import LinearRegression","601a1fed":"# Generate random numbers\ndata = pd.DataFrame(np.random.randint(low=-3, high=3, size=(3, 3)))\n\ndata.columns = ['num 1', 'num 2', 'num 3']\ndata.head()","167dbc0c":"# Get data-frame values into numpy array\nnum_array = data.values\nnum_array","c28ff759":"# We can call linalg.norm function to compute norm\n# ord=1 represents L(p=1) norm\nlinalg.norm(num_array, ord=1, axis=1)","42520387":"# Import few more necessary libraries\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\nimport seaborn as sns\n%matplotlib inline","86b0785d":"# Generate some random data-points\ndata = pd.DataFrame(np.random.randint(low=1, high=15, size=(5,2)))\ndata.columns = ['x', 'y']\ndata.head()","2afde6aa":"# Plot (x,y) value\n# fit_reg=False doesn't fix a regression line\nsns.lmplot('x',\n           'y',\n           data=data,\n           fit_reg=False,\n           scatter_kws={'s': 100})\n\nplt.title('Plotted data-points')\n\nplt.xlabel('x')\nplt.ylabel('y')","9078ba7e":"# Get data-frame values into numpy array\nxy_array = data.values\nxy_array","a4042789":"# Compute L(p=2) norm\nlinalg.norm(xy_array, ord=2, axis=1)","fdd7c9b5":"from sklearn.linear_model import Ridge\n# Define matplotlib figure size to draw\nrcParams['figure.figsize'] = 10, 7\n\n# Generate an array that contains necessary data-points to draw a cosinr curve\nx = np.array([r*np.pi\/180 for r in range(70,300,7)])\n\n#Define cosine curve range for plotting\nm_plt = {1:231,3:232,6:233,9:234,12:235}\nalpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\nmodels_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n\n# Reproducability seed\nnp.random.seed(100)\n\n# Adding some random noise\nnz = np.random.normal(0,0.15,len(x))\ny = np.cos(x) + nz\n\n# Set x,y value as coordinate\ncs = np.column_stack([x,y])\n\n# Convert column_stack plotted value into data frame\ndata = pd.DataFrame(cs, columns=['x','y'])\n\n# Plot generated data-points\nplt.plot(data['x'],data['y'],'.')","9da6923f":"# Linear regression generic function\n# This function returns Residual Sum of Squares and the number of estimated coefficients\n#Import Linear Regression model from scikit-learn.\nfrom sklearn.linear_model import LinearRegression\ndef lr(d, p, mp):\n    \n    # Initialize prediction variable\n    prd=['x']\n    if p>=2:\n        prd.extend(['x_%d'%i for i in range(2,p+1)])\n    linreg = LinearRegression(normalize=True)\n    linreg.fit(d[prd],d['y'])\n    y_pred = linreg.predict(d[prd])\n    \n    if p in mp:\n        plt.subplot(mp[p])\n        plt.tight_layout()\n        plt.plot(d['x'],y_pred)\n        plt.plot(d['x'],d['y'],'.')\n        plt.title('%d'%p)\n    \n    # construct predefined format\n    res = sum((y_pred-data['y'])**2)\n    \n    et = [res]\n    et.extend([linreg.intercept_])\n    et.extend(linreg.coef_)\n    \n    return et","e603e456":"# Generate data\nfor i in range(2,12):\n    data['x_%d'%i] = data['x']**i\n    \nprint(data.head())","be05744f":"# Save generated result and plot\ndata_column = ['rss','intercept'] + ['x%d'%i for i in range(1,12)]\ndata_index = ['power of %d'%i for i in range(1,12)]\nc_mat = pd.DataFrame(index=data_index, columns=data_column)\n\n# Visualize results\nfor i in range(1,12):\n    c_mat.iloc[i-1,0:i+2] = lr(data, p=i, mp=m_plt)\n    \n# Display coefficient data table\npd.options.display.float_format = '{:,.3g}'.format\n\nc_mat","ae865160":"# Generic l2 regularization method\ndef l2_norm(d, pred, a, mp={}):\n    # fitting data\n    l2n = Ridge(alpha=a,normalize=True)\n    l2n.fit(d[pred],d['y'])\n    y_pred = l2n.predict(d[pred])\n    \n    #Check alpha value and plot \n    if a in mp:\n        plt.subplot(mp[a])\n        \n        plt.tight_layout()\n        plt.plot(d['x'],y_pred)\n        plt.plot(d['x'],d['y'],'.')\n        plt.title('Lambda: %.3g'%a)\n    \n    #Return the result in pre-defined format\n    res = sum((y_pred-data['y'])**2)\n    \n    x = [res]\n    x.extend([l2n.intercept_])\n    x.extend(l2n.coef_)\n    \n    return x","e31ca1eb":"# Prediction variable initialization\np=['x']\np.extend(['x_%d'%i for i in range(2,12)])\n\n# Store coefficients\ndata_column = ['rss','intercept'] + ['x%d'%i for i in range(1,12)]\ndata_index = ['lambda=%.2g'%alpha_ridge[i] for i in range(0,10)]\nmat_l2 = pd.DataFrame(index=data_index, columns=data_column)\n\nfor i in range(10):\n    mat_l2.iloc[i,] = l2_norm(data, p, alpha_ridge[i], models_to_plot)\n    \n# Display coefficient table\npd.options.display.float_format = '{:,.2g}'.format\n\nmat_l2","ccb619e5":"In the above table, x1, x2,..., x11 represent the magnitude of coefficient. In top row, while the L2 regularization factor lambda is small, the magnitude of coefficients reduce very significantly. Small coefficients always lead to a simple and less complicated model.\n<br>\n<br>\nRef:\n<br>\nDeep Learning- Ian Goodfellow and Yoshua Bengio and Aaron Courville\n<br>\nhttp:\/\/neuralnetworksanddeeplearning.com\/","66d5fd73":"**L2 REGULARIZATION IMPLEMENTATION:**","9726fb2f":"Now using the data, we can find out that which person scored the most using *L(p=1) norm*.","60626f69":"The above table illustrates that the polynomial regression model gets more complicated with the increase of the power of x. And with the increase of the polynomial regression model complexity, the model coefficient size increases exponentially. From the above table, we see that for power of 11, the coefficient size is maximum. Here the large coefficients explain that features are getting a lot of emphasis, which means that the polynomial regression model is trying to fit very well with the features. This situation looks good with our current feature but can provide poor output for future data. At this point, we are going to apply L2 regularization to see the effects.","3e9f0ce9":"**L(p=2) NORM:**\n<br>\nThe *L(p=2) norm* of a vector is the square root of the sum of the absolute values squared.\n<img src=\"https:\/\/i.imgur.com\/axN5NeF.gif\" width=\"130px\"\/>\n<br>\n*L(p=2) norm* is basically the distance among features or data-points in euclidean space. Suppose that we have a vector *V = [v1, v2]*. We can compute the *L(p=2) norm* of vector *V* simply computing the cartesian distance between *v1* and *v2*.\n<img src=\"https:\/\/i.imgur.com\/atCEQrL.gif\" width=\"140px\"\/>","69294185":"This kernel explains regularization technique with example using *L2* parameter norm. First of all, the kernel provides an intuition about *L(p=1)* and *L(p=2)* vector norm and then shows how can we use *L(p=2)* norm for regularization.\n<br>\n<br>\n**VECTOR NORM:**\n<br>\n<br>\nThe definition of norm depends on the context of usage. In linear algebra, a norm can be defined as a function that is used to measure the length or size of a given vector in a vector space. In general, the equation for *L-p* norm is:\n<img src=\"https:\/\/i.imgur.com\/ZjO2EHF.gif\" width=\"160px\"\/>\n<br>\nIn this equation, when *p = 1*, we call it *L1 norm* or *L(p=1) norm*, and when *p = 2*, we call it *L2 norm* or *L(p=2) norm*.\n<br>\n<br>\n**L(p=1) NORM:**\n<br>\nThe equation for *L(p=1) norm* is:\n<img src=\"https:\/\/i.imgur.com\/WiiFhig.gif\" width=\"120px\"\/>\n<br>\nSo, *L(p=1)* norm of a given vector is the sum of absolute value of all vector elements.\n<br><br>\nFor example, among three person each of them is given a task to roll a dice and the number range is in between -3 to 3. Positive number indicates right movement and negative number indicates left movements.\nThe following program first generates three random numbers for each person and then compute the *L(p=1) norm* to find the total number of movements.","b2e268da":"The plotted data points represent a cosine curve. In order to add some random noise, we can not visualize it exactly. We will estimate the cosine curve through polynomial regression using the following generated data. The polynomial regression is applied here with the power of x.","62b85c93":"At this point, we have an intuition about vector norm in the context of linear algebra. Now lets explore the roles of *L2 norm* in regularization technique in machine learning.\n<br>\n<br>\n**REGULARIZATION:**\n<br><br>\nIn machine learning, one of the major task is to find a prediction function that minimizes the average loss on a training dataset. Regularization has various definition depending on the context. Generally, regularization technique is used to prevent overfitting in machine learning. We can define regularization as a set of actions that makes our prediction function fit with training data in a less well manner so that the prediction function can fit with the new data better. In other words, we can define it as a process of model simplification. It does this by penalizing the loss function.\n<br>\n<br>\n**HOW PENALIZING LOSS FUNCTION HELPS TO SIMPLIFY THE MODEL?**\n<br><br>\nSuppose that we have the following plotted data-points where red data-points represent training set and green data-points represent testing set.\n<img src=\"https:\/\/i.imgur.com\/0Oh93oq.png\" width=\"300px\"\/>\n<br>\nSince this data looks relatively linear, we have used linear regression to model the relationship between feature and label. So, the model will be a line that best fit with the training data-points. When we have a lot of measurements, we can say confidently that linear regression shows the accurate relationship between feature and label. Now let's consider the fact that we only have two training measurements. Following the linear regression approach, we draw a line (red) that best with our two training measuremnts with mean squared error. From the diagram, we see that the new line overlaps all two measurements exactly. So, the sum of the mean squared error is 0 or very close to 0 in such case.\n<br><br>\nBut if we consider this line (red) for the testing measurements, then the sum of mean squared error for testing data is large. This scenario indicates that the new line that best fit with our two training measurements, has high varience for these testing measurements. At this point we can say that the new line is overfit with our two training measurements.\n<br><br>\nThis situation can be overcome using L2 regularization technique, also known as Ridge regression. L2 regularization technique computes a new line that does not fit well with the training data. L2 regularization is applied on the loss function and basically introduces a small amount of bias that helps the new line to fit with our training measurements.\n<br><br>\nLet's consider the following loss function that is the sum of the squared difference between the actual value and the predicted value.\n\n<img src=\"https:\/\/i.imgur.com\/A8rveAf.gif\" width=\"200px\"\/>\n<br>\nThe regularization technique works on the assumption that smaller weights generate simpler model and thus avoid overfitting. In order to achieve a regularized loss function, we add a regularization term along with the loss function.\n<img src=\"https:\/\/i.imgur.com\/ckI9vMg.gif\" width=\"220px\"\/>\n<br>\nHere \u03b2 is called the regularization term. It adds up penalty that is equal to the square of the magnitude of coefficients. In the following equation, \u03bb is called regularization factor and the range of the value of \u03bb is 0 to \u221e.\n<img src=\"https:\/\/i.imgur.com\/oNGIWSf.gif\" width=\"100px\"\/>\n<br>\nThe following example shows the impact of model complexity and how L2 regularization penalizes the magnitude of coefficients.\n<br><br>\nFirst of all, let's simulate a cosine curve and add some random noise.","e2e16aef":"In the above graph, we can compute the nearest data-point from the origin (0,0) position using *L(p=2) norm*.","ed232223":"The following code block computes the distance to the data-points position in 2 dimensional space."}}