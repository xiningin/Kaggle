{"cell_type":{"ba64afea":"code","2fdd8139":"code","b7b4b591":"code","2f3e6872":"code","9b71e07d":"code","3d53c557":"code","faa59c2a":"code","357e8960":"code","6bb564a0":"code","2229c35c":"code","b7925fa6":"code","4acd1ff6":"code","58e04c15":"code","e9a7d330":"code","a186cb10":"code","9b738025":"code","82b15466":"code","d27ba5b8":"code","aa9c4434":"markdown","7e3f950f":"markdown","ebdee5bd":"markdown","0cafa942":"markdown","6a2d69f9":"markdown","406f3f76":"markdown"},"source":{"ba64afea":"#Importing Stuff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict\nimport string\nimport tensorflow as tf\nimport re\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","2fdd8139":"#Reading the data\ntrain=pd.read_csv('..\/input\/nlp-getting-started\/train.csv',index_col='id')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv',index_col='id')\ny=train['target']","b7b4b591":"#Dropping these columns to focus only on the actual tweets.\ntrain.drop(['location','keyword'],inplace=True,axis=1)\ntest.drop(['location','keyword'],inplace=True,axis=1)","2f3e6872":"#Extracting all the words from the tweets in the Training Data\ndef create_corpus(train):\n    corpus=[]\n    num_words = 0\n\n    for i in range(len(train)):\n        for j in range(len(train.iloc[i,0].split())):\n            corpus.append(train.iloc[i,0].split()[j])\n    return corpus\n\n#Creating a Dict for counting the number of times each word occurs\n\ndef word_count(corpus):\n    wcount=defaultdict(int)\n    for i in corpus:\n        wcount[i]+=1\n    return wcount\n        \n\n\n#Creating function to return 10 most used words\ndef most_used(x):\n    return sorted(words.items(),key=lambda kv: kv[1],reverse=True)[:10]","9b71e07d":"#You can also use these to check for the test data\nwords = word_count(create_corpus(train))\ntop=most_used(words)\n\n#Visualising the most used words in the training dataset\na,b=zip(*top)\nplt.bar(a,b)","3d53c557":"print(train['text'].nunique(),train['text'].count())  #The difference in their value shows that there are identical tweets","faa59c2a":"duplicate = train[train.duplicated('text',keep = False)] #Shows all repeated tweets with same text\nduplicate2 = train[train.duplicated(keep=False)]   #Shows all tweets with same text and target\nduplicate[~duplicate.isin(duplicate2)].dropna() #Shows all tweets with same text but different targets","357e8960":"#We need to manually rectify these mistakes as we cannot do anything else\n\ntrain.loc[train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target'] = 0\ntrain.loc[train['text'] == 'Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife', 'target'] = 0\ntrain.loc[train['text'] == 'To fight bioterrorism sir.', 'target'] = 0\ntrain.loc[train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4', 'target'] = 1\ntrain.loc[train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring', 'target'] = 1\ntrain.loc[train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target'] = 0\ntrain.loc[train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target'] = 0\ntrain.loc[train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target'] = 1\ntrain.loc[train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG', 'target'] = 1\ntrain.loc[train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target'] = 0\ntrain.loc[train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target'] = 0\ntrain.loc[train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target'] = 0\ntrain.loc[train['text'] == \"Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!\", 'target'] = 0\ntrain.loc[train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target'] = 0\ntrain.loc[train['text'] == \"Caution: breathing may be hazardous to your health.\", 'target'] = 1\ntrain.loc[train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target'] = 0\ntrain.loc[train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target'] = 0\ntrain.loc[train['text'] == \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\", 'target'] = 0","6bb564a0":"duplicate = train[train.duplicated('text',keep = False)]\nduplicate2 = train[train.duplicated(keep=False)]\nduplicate[~duplicate.isin(duplicate2)].dropna() #this will return empty dataframe now as we corrected all the labels","2229c35c":"#Use regex to clean the data\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    return text \n\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] # take max of 2 consecutive letters\n\ndef unique_char(rep, text):\n    substitute = re.sub(r'(\\w)\\1+', rep, text)\n    return substitute\n\ntrain['text']=train['text'].apply(lambda x : remove_url(x))\ntrain['text']=train['text'].apply(lambda x : remove_punct(x))\ntrain['text']=train['text'].apply(lambda x : remove_emoji(x))\ntrain['text']=train['text'].apply(lambda x : decontraction(x))\ntrain['text']=train['text'].apply(lambda x : seperate_alphanumeric(x))\ntrain['text']=train['text'].apply(lambda x : unique_char(cont_rep_char,x))\n\ntest['text']=test['text'].apply(lambda x : remove_url(x))\ntest['text']=test['text'].apply(lambda x : remove_punct(x))\ntest['text']=test['text'].apply(lambda x : remove_emoji(x))\ntest['text']=test['text'].apply(lambda x : decontraction(x))\ntest['text']=test['text'].apply(lambda x : seperate_alphanumeric(x))\ntest['text']=test['text'].apply(lambda x : unique_char(cont_rep_char,x))","b7925fa6":"#Create corpus and sentences\nX=train.append(test)\ncorpus=create_corpus(X)\n\ntrain_sentences=[]\nfor i in range(len(train)):\n    train_sentences.append(train.iloc[i,0])\ntest_sentences=[]\nfor i in range(len(test)):\n    test_sentences.append(test.iloc[i,0])\n    \nembedding_dim = 100\noov_tok = \"<OOV>\"\ntrunc_type='post'\npadding_type='post'\nmax_length=120","4acd1ff6":"#Tokenise out data and create sequences\ntokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(corpus)\n\nword_index = tokenizer.word_index\nvocab_size=len(word_index)\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","58e04c15":"#Use predefined embeddings to give us an advantage\nembeddings_index = {};\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt',encoding = \"utf-8\") as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","e9a7d330":"train_padded = np.array(train_padded)\ny=np.array(y)\ntest_padded = np.array(test_padded)","a186cb10":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length,weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv1D(32, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=3),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv1D(32, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=3),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","9b738025":"model.fit(train_padded, y, epochs=30)","82b15466":"result=model.predict(test_padded)\nresult=np.rint(result)\nresult=result.astype('int16')\nresult","d27ba5b8":"res=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv',index_col=None)  #Creating a submission file\nres['target']=result\nres.to_csv('submission.csv',index=False)","aa9c4434":"# Data Cleaning","7e3f950f":"# Model","ebdee5bd":"# References\n[www.kaggle.com\/ezeanyi\/nlp-cleaning-glove-and-lstm](http:\/\/)\n[www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert](http:\/\/)\n[www.kaggle.com\/janvichokshi\/disaster-tweets-cleaning-lstm-and-embedding](http:\/\/)\n\nAs Someone who is completely new to this these notebooks really helped me understand a lot things. Please check them out.","0cafa942":"# Predictions","6a2d69f9":"# Helper Functions","406f3f76":"# Tokens,Sequences,Padding"}}