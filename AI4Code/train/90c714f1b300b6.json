{"cell_type":{"816dade6":"code","d4ae797f":"code","e828bee2":"code","44c964f6":"code","534977c7":"code","cc4ac4cb":"code","37834884":"code","da477814":"code","c1a4ad42":"code","12c037e3":"code","ebed5964":"code","6c3ae206":"code","4491ca48":"code","ef1d1e06":"code","6c29478e":"code","ba2bbd96":"code","aec60a94":"code","b748f587":"code","18117cef":"code","c3fe6210":"code","b1a4a414":"code","56fb06ed":"markdown","5d5481ab":"markdown","f2e676da":"markdown","598a0ad8":"markdown"},"source":{"816dade6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d4ae797f":"import pandas as pd\ndf=pd.read_csv('\/kaggle\/input\/apartment-benchmark-prices\/Dataset.csv',index_col=0,header=0,squeeze=True)\nvalidation=pd.read_csv('\/kaggle\/input\/apartment-benchmark-prices\/Validation.csv',index_col=0,header=0,squeeze=True)\nprint('Dataset %d, Validation %d' %(len(df),len(validation)))","e828bee2":"import matplotlib.pyplot as plt\ndf.plot()","44c964f6":"plt.figure()\nplt.subplot(211)\ndf.hist()\nplt.subplot(212)\ndf.plot(kind='kde')\nplt.show()","534977c7":"#Check stationarity\nfrom statsmodels.tsa.stattools import adfuller\n\nresult=adfuller(df.diff().dropna())\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.4f' % (key,value)) ","cc4ac4cb":"#Plot ACF and PACF for first order differencing\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt \nimport numpy as np\n\nplt.figure()\nplt.subplot(211)\nplot_acf(df.diff().dropna(),ax=plt.gca())\nplt.subplot(212)\nplot_pacf(df.diff().dropna(),ax=plt.gca())\nplt.show()","37834884":"# Check second order differencing\n\nresult=adfuller(df.diff().diff().dropna())\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.4f' % (key,value)) ","da477814":"#Plot ACF and PACF for second order differencing\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt \nimport numpy as np\n\nplt.figure()\nplt.subplot(211)\nplot_acf(df.diff().diff().dropna(),ax=plt.gca())\nplt.subplot(212)\nplot_pacf(df.diff().diff().dropna(),ax=plt.gca())\nplt.show()","c1a4ad42":"# Manually configure ARIMA\n\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom math import sqrt\nimport numpy as np\n\n# Calculate mean absolute percentage error\ndef mean_abs_percent_error(obs,yhat):\n    obs, yhat=np.array(obs),np.array(yhat)\n    return np.mean(np.abs((obs-yhat)\/obs))*100\n\n# Prepare data\nX=df.values\nX=X.astype('float32')\ntrain_size=int(len(X)*0.7)\ntrain, test=X[0:train_size], X[train_size:]\n\n# Walk-forward validation\nhistory=[x for x in train]\npredictions=list()\nfor i in range(len(test)):\n    #predict\n    model=ARIMA(history,order=(1,2,1))\n    model_fit=model.fit(disp=0)\n    yhat=model_fit.forecast()[0]\n    predictions.append(yhat)\n    #observation\n    obs=test[i]\n    history.append(obs)\n    print('Predicted= %.4f, Expected= %.4f' % (yhat,obs))\n\n# Report performance\nrmse=sqrt(mean_squared_error(test,predictions))\nprint('RMSE: %.4f' % (rmse))","12c037e3":"#plot residual errors for ARIMA model\nfrom pandas import DataFrame\n#errors\nresiduals=[test[i]-predictions[i] for i in range(len(test))]\nresiduals=DataFrame(residuals)\nplt.figure()\nplt.subplot(211)\nresiduals.hist(ax=plt.gca())\nplt.subplot(212)\nresiduals.plot(kind='kde',ax=plt.gca())\nplt.show()","ebed5964":"residuals.describe()","6c3ae206":"print(model_fit.summary())","4491ca48":"#summarize residual errors from bias corrected forecasts\n#walk-forward validation\nhistory=[x for x in train]\npredictions=list()\nbias=216\nfor i in range(len(test)):\n    #predict\n    model=ARIMA(history,order=(1,2,1))\n    model_fit=model.fit(disp=0)\n    yhat=bias+float(model_fit.forecast()[0])\n    predictions.append(yhat)\n    #observation\n    obs=test[i]\n    history.append(obs)\n# Report performance\nrmse=sqrt(mean_squared_error(test,predictions))\nmape=mean_abs_percent_error(test,predictions)\nprint('RMSE: %.4f, Mean Absolute Percentage Error: %.4f' %(rmse,mape))\n\n#Summarize residual errors\nresiduals=[test[i]-predictions[i] for i in range(len(test))]\nresiduals=DataFrame(residuals)\nprint('')\nprint('Residuals:')\nprint(residuals.describe())\n\n#Plot residual errors\nplt.figure()\nplt.subplot(211)\nresiduals.hist(ax=plt.gca())\nplt.subplot(212)\nresiduals.plot(kind='kde',ax=plt.gca())\nplt.show()","ef1d1e06":"#Evaluate finalized model on validation dataset\nfrom pandas import read_csv\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport numpy as np\n\n#Load and prepare datasets\ndf=read_csv('\/kaggle\/input\/apartment-benchmark-prices\/Dataset.csv',index_col=0,header=0,squeeze=True)\nX=df.values.astype('float32')\nhistory=[x for x in X]\nValidation=read_csv('\/kaggle\/input\/apartment-benchmark-prices\/Validation.csv',index_col=0,header=0,squeeze=True)\ny=Validation.values.astype('float32')\nmodel=ARIMA(X,order=(1,2,1))\nmodel_fit=model.fit(disp=0)\nbias=216\n#Make first prediction\npredictions=list()\nyhat=bias+float(model_fit.forecast()[0])\npredictions.append(y[0])\nprint('Predicted= %.4f, Expected= %.4f' % (yhat,y[0]))\n\n#Rolling forecasts\nfor i in range(1,len(y)):\n    #predict\n    model=ARIMA(history,order=(1,2,0))\n    model_fit=model.fit(disp=0)\n    yhat=bias+float(model_fit.forecast()[0])\n    predictions.append(yhat)\n    #Observation\n    obs=y[i]\n    history.append(obs)\n    print('Predicted= %.4f, Expected= %.4f' % (yhat,obs))\n    \n#Report performance\nrmse=sqrt(mean_squared_error(y,predictions))\nmape=mean_abs_percent_error(y,predictions)\nprint('')\nprint('RMSE: %.4f, Mean Absolute Percentage Error: %.4f' % (rmse,mape))\nplt.plot(y)\nplt.plot(predictions,color='red')\nplt.show()\n","6c29478e":"#Let's try the model's prediction power for 12-steps\n#Load and prep data\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndf=pd.read_csv('\/kaggle\/input\/apartment-benchmark-prices\/Dataset.csv',header=0,squeeze=True)\ndf.drop('Date',axis=1,inplace=True)\nindex=pd.date_range(start='2005',periods=153,freq='M')\ndf.index=index\nX=df['Apartment']\n\nValidation=pd.read_csv('\/kaggle\/input\/apartment-benchmark-prices\/Validation.csv',header=0,squeeze=True)\nValidation.drop('Date',axis=1,inplace=True)\nindex2=pd.date_range(start='2017-10',periods=12,freq='M')\nValidation.index=index2\ny=Validation['Apartment']\n\n#Direct forecast\nmod1=ARIMA(X,order=(1,2,1),freq='M')\nmodel_fit=mod1.fit()\nfc,se,conf=model_fit.forecast(12,alpha=0.05)\n\n#Turn into pandas Series\nfc_series=pd.Series(fc,index=Validation.index)\nlower_series=pd.Series(conf[:,0],index=Validation.index)\nupper_series=pd.Series(conf[:,1],index=Validation.index)\n\n#Get residuals\nresid1=[y[i]-fc_series[i] for i in range(len(y))]\n\n\n#Report performance\nrmse=sqrt(mean_squared_error(y,fc_series))\nmape=mean_abs_percent_error(y,fc_series)\nprint('RMSE: %.4f, Mean Absolute Percentage Error: %.4f' % (rmse,mape))\n\n#Plot graph\nplt.plot(y,label='Expected')\nplt.plot(fc_series,label='Predicted',color='red')\nplt.fill_between(lower_series.index,lower_series,upper_series,\n                color='k',alpha=.15)\nplt.show()\n\n","ba2bbd96":"#How would an exponential smoothing model perform\n#in comparison?\n\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\ndf=pd.read_csv('\/kaggle\/input\/apartment-benchmark-prices\/Dataset.csv',header=0,squeeze=True)\ndf.drop('Date',axis=1,inplace=True)\nindex=pd.date_range(start='2005',periods=153,freq='M')\ndf.index=index\nX=df['Apartment']\n\n#Split into train & test sets\nX=X.astype('float32')\ntrain_size=int(len(X)*0.7)\ntrain, test=X[0:train_size], X[train_size:]\n\n# Walk-forward validation\nhistory=[x for x in train]\npredictions=list()\nfor i in range(len(test)):\n    #predict\n    model=ExponentialSmoothing(history,trend='add',damped=True)\n    model_fit=model.fit(remove_bias=True)\n    yhat=model_fit.forecast()[0]\n    predictions.append(yhat)\n    #observation\n    obs=test[i]\n    history.append(obs)\n    print('Predicted= %.4f, Expected= %.4f' % (yhat,obs))\n\n# Report performance\nrmse=sqrt(mean_squared_error(test,predictions))\nprint('RMSE: %.4f' % (rmse))\n","aec60a94":"#Let's try the exponential smoothing \n#model's prediction power for 12-steps\n#Load and prep data\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndf=pd.read_csv('\/kaggle\/input\/apartment-benchmark-prices\/Dataset.csv',header=0,squeeze=True)\ndf.drop('Date',axis=1,inplace=True)\nindex=pd.date_range(start='2005',periods=153,freq='M')\ndf.index=index\nX=df['Apartment']\n\nValidation=pd.read_csv('\/kaggle\/input\/apartment-benchmark-prices\/Validation.csv',header=0,squeeze=True)\nValidation.drop('Date',axis=1,inplace=True)\nindex2=pd.date_range(start='2017-10',periods=12,freq='M')\nValidation.index=index2\ny=Validation['Apartment']\n\n#Direct forecast\nmod2=ExponentialSmoothing(X,trend='add',damped=True,freq='M')\nmodel_fit=mod2.fit(remove_bias=True)\nfc=model_fit.forecast(12)\n\n#Get residuals\nresid2=[y[i]-fc[i] for i in range(len(y))]\n\n\n#Report performance\nrmse=sqrt(mean_squared_error(y,fc))\nmape=mean_abs_percent_error(y,fc)\nprint('RMSE: %.4f, Mean Absolute Percentage Error: %.4f' % (rmse,mape))\n\n#Plot graph\nplt.plot(y,label='Expected')\nplt.plot(fc,label='Predicted',color='red')\nplt.show()\n","b748f587":"#Blending the forecasts from \n#ARIMA and ETS models.\n#Weights using RMSE\n\nBlend_fc=(0.47*fc_series+0.53*fc)\n\n#Report performance\nrmse=sqrt(mean_squared_error(y,Blend_fc))\nmape=mean_abs_percent_error(y,Blend_fc)\nprint('RMSE: %.4f, Mean Absolute Percentage Error: %.4f' % (rmse,mape))\n\n#Plot graph\nplt.plot(y,label='Expected')\nplt.plot(Blend_fc,label='Predicted')\nplt.show()","18117cef":"#OK! Let's try and forecast for the 12 months\n#from Sep 2018 and see what we get.\n\nforecast_index=pd.date_range(start='2018-10',periods=12,freq='M')\n\n#stick validation data to the dataset to train.\ndf1=df.append(Validation)\nX=df1['Apartment']\n\n#retrain mod1 and mod2 on df1\n#Direct forecast\nmod1=ARIMA(X,order=(1,2,1),freq='M')\nmodel_fit=mod1.fit()\nfc,se,conf=model_fit.forecast(12,alpha=0.05)\n\n#Turn into pandas Series\nfc_series=pd.Series(fc,index=forecast_index)\nlower_series=pd.Series(conf[:,0],index=forecast_index)\nupper_series=pd.Series(conf[:,1],index=forecast_index)\n\nmod2=ExponentialSmoothing(X,trend='add',damped=True,freq='M')\nmodel_fit=mod2.fit(remove_bias=True)\nfc=model_fit.forecast(12)\n\n#Blending the forecasts from \n#ARIMA and ETS models.\n#Weights using RMSE\n\nforecast=0.47*fc_series+0.53*fc","c3fe6210":"#Import actuals for Oct 2018 ~ Sep 2019\n#https:\/\/www.crea.ca\/housing-market-stats\/mls-home-price-index\/hpi-tool\/\nactuals=[693200,677500,675200,670200,671500,667500,667500,664200,654700,653200,654000,651500]\ny=pd.Series(actuals,index=forecast_index)","b1a4a414":"#Report performance\nrmse=sqrt(mean_squared_error(y,forecast))\nmape=mean_abs_percent_error(y,forecast)\nprint('RMSE: %.4f Mean Absolute Percent Error: %.4f' % (rmse,mape))\n\n#Plot graph\nplt.plot(y,label='Expected')\nplt.plot(forecast,label='Predicted',color='red')\nplt.fill_between(lower_series.index,lower_series,upper_series,\n                color='k',alpha=.15)\nplt.show()\n","56fb06ed":"Both the p-value and ADF statistic suggests that the first order differentiation isn't sufficient to attain stationarity. ","5d5481ab":"The second order of differencing attained stationarity.","f2e676da":"The data shows a trend but no seasonality. Distribution is near Gaussian.  ","598a0ad8":"This is a simple exercise in time series forecasting.  The ARIMA and Exponential Smoothing models were validated on 12 unseen data points (Oct 2017 ~ Sep 2018).  Forecasts from the models were blended using RMSE as weights. This is then used to generate projections for the next 12 months (Oct 2018 ~ Sep 2019).  Mean absolute percentage error: 2.87%.\n\n<p>Benchmark Prices for Apartment in Greater Vancouver<\/p>\n<p>Source: CREA<\/p>\n<p>Dataset: Jan 2005 ~ Sep 2017<\/p>\n<p>Validation: Oct 2017 ~ Sep 2018<\/p>\n<p>Frequency: Monthly<\/p>\n"}}