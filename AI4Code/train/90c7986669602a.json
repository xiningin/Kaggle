{"cell_type":{"69e1e6dd":"code","1c84d60e":"code","e4bdea36":"code","113da57d":"code","84857158":"code","94871588":"code","57cd2efc":"code","9f4bb3f5":"code","9562120f":"code","3331e44d":"code","25af0e7f":"code","ae7a6cb0":"code","a1375606":"code","7bdc93b2":"code","a2bc6e04":"code","0891d2dd":"code","a191b637":"code","00b4b5a0":"code","1df9d31a":"code","f20706fb":"code","728a2947":"code","489eb6b8":"code","24b5afb8":"code","47ab3d69":"code","32b9e4f6":"code","36a113ec":"code","02309f32":"code","b7539bcc":"code","76874138":"code","3145b467":"code","8dc2d76a":"code","2323df27":"code","2a39d144":"code","b7749490":"code","d078f489":"code","7fb7c1f4":"code","e50de875":"code","92b8aad2":"code","824a32f4":"code","02f79339":"code","380c28f3":"code","f2226714":"code","2e9ea635":"markdown","ce7a24e1":"markdown","34bb269a":"markdown","7dad1a7c":"markdown","ce028a74":"markdown","5429903e":"markdown","e6bc4e1d":"markdown","9d3e7294":"markdown","b9565d44":"markdown","42154d6f":"markdown","d2b4ba76":"markdown","09b630a5":"markdown","686f8ade":"markdown","24d61d88":"markdown","49015bbc":"markdown","a037a98c":"markdown"},"source":{"69e1e6dd":"# Import Libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly.graph_objs as px\nimport plotly.express as ex\nfrom plotly.subplots import make_subplots\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Importing the Keras libraries and packages\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU,PReLU,ELU\nfrom keras.layers import Dropout\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import RandomSearch\n\n# Feature Scaling and metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score","1c84d60e":"# Read the csv file\ndata = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')\ndata.head()","e4bdea36":"# Shape of the raw data\nprint(\"Shape of Data raw data: {}\".format(data.shape))","113da57d":"data.info()","84857158":"# Checking the duplicate value\ndata.duplicated().sum()","94871588":"# Checking the null values\ndata.isnull().sum()","57cd2efc":"# Checking the null values with heatmap\nsns.heatmap(data.isnull(),yticklabels = False,cbar = False,cmap = 'viridis')","9f4bb3f5":"# Dropping the column which will not make big impact on dependent variables\ndata.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1, inplace=True)","9562120f":"# Stastical analysis of the data\ndata.describe()","3331e44d":"# Detecting the ouliers\ndef detect_outlier(data):\n    outlier = []\n    threshold = 3\n    mean = np.mean(data)\n    std = np.std(data)\n    for i in data:\n        z_score = (i - mean)\/std\n        if np.abs(z_score)>threshold:\n            outlier.append(i)\n    return outlier","25af0e7f":"CreditScore_list = data['CreditScore'].tolist()\nBalance_list = data['Balance'].tolist()\nEstimatedSalary_list = data['EstimatedSalary'].tolist()","ae7a6cb0":"CreditScore_outlier = detect_outlier(CreditScore_list)\nCreditScore_outlier","a1375606":"Balance_outlier = detect_outlier(Balance_list)\nBalance_outlier","7bdc93b2":"EstimatedSalary_outlier = detect_outlier(EstimatedSalary_list)\nEstimatedSalary_outlier","a2bc6e04":"# Shape of Data before removing the outliers\nprint(\"Shape of Data before removing outliers: {}\".format(data.shape))","0891d2dd":"# Removing the outliers\ndata.drop(data[data['CreditScore'] <= 359].index, inplace = True)","a191b637":"#Shape of Data after removing the outliers\nprint(\"Shape of Data after removing outliers: {}\".format(data.shape))","00b4b5a0":"plt.figure(figsize =(7,5))\nsns.set_style('darkgrid')\nsns.countplot(x = 'Exited', data = data)\nplt.title('Exited(No\/Yes)')\ndata['Exited'].value_counts()","1df9d31a":"plt.pie(data['Exited'].value_counts(), labels = ['No', 'Yes'], shadow = True, autopct = '%1.2f%%');","f20706fb":"plt.figure(figsize =(7,5))\nsns.set_style('darkgrid')\nsns.countplot(x = 'Exited', hue = 'Gender', data = data)\nplt.title('Exited against Gender');\npd.DataFrame(data.groupby(['Gender', 'Exited'])['Exited'].count())","728a2947":"plt.figure(figsize =(7,5))\nsns.set_style('darkgrid')\nsns.countplot(x = 'Exited', hue = 'Geography', data = data)\nplt.title('Exited against Geography');\npd.DataFrame(data.groupby(['Geography', 'Exited'])['Exited'].count())","489eb6b8":"plt.figure(figsize =(7,5))\nsns.set_style('darkgrid')\nsns.countplot(x = 'HasCrCard', hue = 'Geography', data = data)\nplt.title('Credit card used against Geography');\npd.DataFrame(data.groupby(['Geography', 'HasCrCard'])['HasCrCard'].count())","24b5afb8":"plt.figure(figsize =(7,5))\nsns.set_style('darkgrid')\nsns.countplot(x = 'HasCrCard', hue = 'Gender', data = data)\nplt.title('Credit card used against Gender');\npd.DataFrame(data.groupby(['Gender', 'HasCrCard'])['HasCrCard'].count())","47ab3d69":"fig = ex.box(data, x=\"Exited\", y=\"CreditScore\", color = 'Geography')\nfig.update_layout(title_text=\"Different Country with mean Credit scores(Exited(No\/Yes))\")\nfig.show();","32b9e4f6":"fig = ex.box(data, x=\"Exited\", y=\"EstimatedSalary\", color = 'Geography')\nfig.update_layout(title_text=\"Different Country with mean salary(Exited(No\/Yes))\")\nfig.show();","36a113ec":"fig = make_subplots(rows=1, cols=1)\n\nhist=px.Histogram(x=data['CreditScore'],name='Credit Score Histogram')\n\nfig.add_trace(hist,row=1,col=1)\n\nfig.update_layout(height=500, width=700, title_text=\"Distribution of the Credit score\")\nfig.show()","02309f32":"fig = make_subplots(rows=1, cols=1)\n\nhist=px.Histogram(x=data['Age'],name='Age Histogram')\n\nfig.add_trace(hist,row=1,col=1)\n\nfig.update_layout(height=500, width=700, title_text=\"Distribution of the Customer Ages\")\nfig.show()","b7539bcc":"# Correlation between the features using heatmap\ncorrmat = data.corr()\nplt.figure(figsize=(10,8))\n#plot heat map\nsns.heatmap(corrmat, annot=True, cmap=\"RdYlGn\")","76874138":"# Split the Dataset\nX= data.drop(['Exited'], axis = 1)\ny = data['Exited']","3145b467":"# Creating dummy variables\nDummies = pd.get_dummies(X[['Geography', 'Gender']],drop_first=True)\nX = X.drop(['Geography', 'Gender'], axis = 1)\nX = pd.concat([X, Dummies], axis = 1)","8dc2d76a":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Feature Scaling using standard scaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","2323df27":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 16, kernel_initializer = 'he_uniform',activation='relu',input_dim = 11))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 8, kernel_initializer = 'he_uniform',activation='relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nmodel_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 128, epochs = 100, verbose=0)","2a39d144":"# summarize history for accuracy\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b7749490":"# Part 3 - Making the predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\n# Making the classification report\nprint(classification_report(y_test, y_pred))\n\n# Making the Confusion Matrix\nprint(confusion_matrix(y_test, y_pred))\n\n# Calculate the Accuracy\nprint(accuracy_score(y_test, y_pred))","d078f489":"def build_model(hp):\n    \n    model = keras.Sequential()\n    counter = 0\n    \n    for i in range(hp.Int('num_layers',min_value=1,max_value=10)):\n        if counter == 0:\n            model.add(layers.Dense(hp.Int('units_' + str(i),\n                                min_value=8,\n                                max_value=128,\n                                step=8), kernel_initializer = 'he_uniform', activation='relu',input_dim = 11))\n            model.add(Dropout(hp.Choice('dropout' + str(i), values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])))\n        else:\n            model.add(layers.Dense(units=hp.Int('units_' + str(i),\n                                            min_value=8,\n                                            max_value=128,\n                                            step=8),\n                               activation='relu', kernel_initializer = 'he_uniform'))\n            model.add(Dropout(hp.Choice('dropout' + str(i), values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])))\n        counter+=1\n    \n    # Adding the output layer\n    model.add(layers.Dense(1, activation='sigmoid', kernel_initializer = 'glorot_uniform'))\n    # Compiling the ANN\n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n        loss='binary_crossentropy',\n        metrics=['accuracy'])\n    return model        ","7fb7c1f4":"tuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    seed=42,\n    max_trials=5,\n    executions_per_trial=3,\n    directory='project4',\n    project_name='Churn modelling4')","e50de875":"tuner.search(X_train, y_train, epochs=5, batch_size=128, \n             validation_data=(X_test, y_test))","92b8aad2":"tuner.get_best_hyperparameters()[0].values","824a32f4":"model = tuner.get_best_models(num_models=1)[0]","02f79339":"model.fit(X_train, y_train, epochs=200, initial_epoch=6, validation_data=(X_test,y_test), verbose = 0)","380c28f3":"model.summary()","f2226714":"# Evaluate the best model.\nloss, accuracy = model.evaluate(X_test, y_test)","2e9ea635":"## Hyperparameter Optimization using keras tuner\n<li>How many number of hidden layers we should have?\n<li>How many number of neurons we should have in each hidden layers?\n<li>How can be use different dropout in case of overfitting?\n<li>Learning Rate","ce7a24e1":"# Churn Modelling\n\nCustomer churn is an imperative issue that is frequently connected with the existing cycle of the business.In recent year, churn prediction is becoming a very important issue in Banking and telecom industry. In order to deal with this problem, the industry must recognize these customers before they churn. Therefore, developing a unique classifier that will predict future churns is vital.\n<br>\n![Customer Churn](https:\/\/miro.medium.com\/max\/3384\/1*WqId29D5dN_8DhiYQcHa2w.png)\n<br>\nThis data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.\n\n### Summary about the features and their types in the dataset:\n<li>1) <b>RowNumber<\/b> : Serial number\n<li>2) <b>CustomerId<\/b> :Unique Ids for bank customer identification\n<li>3) <b>Surname<\/b> : Customer's last name\n<li>4) <b>CreditScore<\/b> : Credit score of the customer\n<li>5) <b>Geography<\/b> : The country from which the customer belongs\n<li>6) <b>Gender<\/b> : Male or Female\n<li>7) <b>Age<\/b> : The age of the customer\n<li>8) <b>Tenure<\/b> : Number of years for which the customer has been with the bank\n<li>9) <b>Balance<\/b> : Bank balance of the customer\n<li>10) <b>NumOfProducts<\/b> : Number of bank products the customer is utilising\n<li>11) <b>HasCrCard<\/b> : Binary Flag for whether the customer holds a credit card with the bank or not\n<li>12) <b>IsActiveMember<\/b> : Binary Flag for whether the customer is an active member with the bank or not\n<li>13) <b>EstimatedSalary<\/b> : Estimated salary of the customer in Dollars\n<li>14) <b>Exited<\/b> : Binary flag 1 if the customer closed account with bank and 0 if the customer is retained","34bb269a":"> After performing hyperparameter tuning using keras tuner accuracy, somewhat increased to **0.8449** from **0.8669**. Overall, the Keras Tuner library is a nice and easy to learn option to perform hyperparameter tuning for your Keras and Tensorflow 2.0 models.","7dad1a7c":"## Data Preprocessing\n<li>Splitting the dataset into dependent and independent variables.\n<li>Creating dummy variables of categorical column using OneHotEncoding.\n<li>Determining train and test set.\n<li>Feature scaling using standard scaler","ce028a74":"## Introduction\nThe main goal of this notebook will be exploring the data and predicting the fact whether the customer left the bank using Artifcial neural network and also how to perform hyperparameter optimization using Keras Tuner. I have also performed Exploratory Data Analysis and feature engineering.","5429903e":"> There are 10000 rows and 14 columns in the raw dataset.","e6bc4e1d":"> <li>Proportionally more female customers are exited from the bank as comapred to male customers.\n> <li>Proportionally more German customers are exited from the bank as comapred to other two countries.\n> <li>More number of customers from France have credit card as comapred to other two countries.\n> <li>Proportionality of customer having credit card against gender is almost equal.","9d3e7294":"> Credit score and Estimated salary does not effect much on exit rates. German People exited more and having high credit score as compared to other two countries.","b9565d44":"> Dropping RowNumber, CustomerId and Surname column from the dataset since it will not make big impact on dependent variable.","42154d6f":"## Exploring the Dataset\n<br>\nIn this step we are going to get the statistics and summary of the data which includes following steps.\n<br>\n<br>\n<li>Shape of the raw dataset.\n<li>Duplicate and null values if any.\n<li>Dropping the column which will not make big impact on dependent variables.\n<li>Detecting the outliers and removing the outliers data.","d2b4ba76":"> As we can see from the above graphs, 20.31% of data samples represent the churn customers and 79.69% customer represent the not churn customers.","09b630a5":"## Importing Necesaary Libraries","686f8ade":"## Exploratory Data Analysis\n<br>\nIn this step we are going to plot graphs to get a deeper insights of the data. Also we are going to illustrate the relationship between different variables using visualization tools.\n<li>Proportion of churn vs not churns customers\n<li>Customer churn vs not churns against Gender\n<li>Customer churn vs not churns against country\n<li>Credit card usage according to country\n<li>Credit card usage according to gender\n<li>Country with highest credit score\n<li>Country with highest Estimated salary\n<li>Distribution of Credit score of customer\n<li>Distribution of Customer Age","24d61d88":"> There is no duplicate and missing values in the dataset.","49015bbc":"> We can see that distribution of customer ages and credit score in our dataset follows a fairly normal distribution; thus we can use these features with the normality assumption.","a037a98c":"## Data Modelling\n### Artificial Neural Network (ANN)"}}