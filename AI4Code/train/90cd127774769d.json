{"cell_type":{"93bf2b45":"code","72bf6af0":"code","00187c8e":"code","b4b48c51":"code","263dd39a":"code","d1bebd33":"code","3395b21c":"code","aa38c613":"code","0a589187":"code","0c45ce53":"code","26aede2a":"code","79460c85":"code","61b7d3de":"code","2b781d31":"code","e1275562":"code","c5c2d900":"code","bce68f1b":"code","1cfe5e34":"code","18ef7512":"code","ab35e130":"code","fce4343e":"code","78dff5d2":"markdown","897cf131":"markdown","3a3ce184":"markdown","10c0be51":"markdown","7390ba16":"markdown","41c2cb4b":"markdown","aabe19c7":"markdown","ceb88087":"markdown","b5578177":"markdown","02573a14":"markdown","02b727c6":"markdown","f759c8c0":"markdown","6f0b7f06":"markdown","0e6d6ce0":"markdown","509dcd5e":"markdown","445e06bd":"markdown","68afb007":"markdown","ada32026":"markdown"},"source":{"93bf2b45":"import numpy as np\nimport copy\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split","72bf6af0":"#Data loading\ncsvFile = '..\/input\/StudentsPerformance.csv'\nst = pd.read_csv(csvFile)\nprint(st.info())","00187c8e":"st = st.rename(columns = {'parental level of education' : \"parentLE\", 'race\/ethnicity' : 'race', 'test preparation course': 'prepCourse', 'math score' : 'maths', 'reading score': 'reading', 'writing score' : 'writing'})\nst['total'] = st.maths + st.reading + st.writing\nst['average'] = round((st.maths + st.reading + st.writing) \/ 3, 1)","b4b48c51":"#st['race'].replace(regex = True, inplace = True, to_replace = r'\\group ', value = r'')\nst.parentLE = st.parentLE.replace(\"bachelor's degree\", \"BSc\") \nst.parentLE = st.parentLE.replace(\"master's degree\", 'MSc')\nst.parentLE = st.parentLE.replace(\"associate's degree\", 'Associate')\nst.prepCourse = st.prepCourse.replace('none', 'False')\nst.prepCourse = st.prepCourse.replace('completed', 'True')\n\n#st['race'] = st['race'].astype('category')\nst['prepCourse'] = st['prepCourse'].map({'False':False, 'True':True})\n#st['lunch'] = st['lunch'].astype('category')\n#st['gender'] = st['gender'].astype('category')\n#st['parentLE'] = st['parentLE'].astype('category')\n\nprint(st.info())","263dd39a":"fig, axs = plt.subplots(3, 1, sharex=True, figsize = (8,7))\nsns.swarmplot(x=\"gender\", y=\"maths\",hue=\"prepCourse\", data=st, size = 3, ax = axs[0])\nsns.swarmplot(x=\"gender\", y=\"reading\",hue=\"prepCourse\", data=st, size = 3, ax = axs[1])\nsns.swarmplot(x=\"gender\", y=\"writing\",hue=\"prepCourse\", data=st, size = 3, ax = axs[2])\nplt.show()","d1bebd33":"#gender performance across subjects\nscores = st.drop(['total', 'average', 'parentLE', 'race', 'lunch'], 1)\nscores = pd.melt(scores, ('gender','prepCourse'), ('maths', 'reading', 'writing'))\nsns.boxplot(x = \"variable\", y = 'value', hue = 'gender', data = scores)\nplt.xlabel('Subject')\nplt.ylabel('Score')\nplt.show()","3395b21c":"#reading vs writing scores\ndef r2(x, y):\n    return stats.pearsonr(x, y)[0] ** 2\nsns.jointplot(st.reading, st.writing, kind=\"reg\", stat_func=r2)\nplt.xlabel('Reading scores')\nplt.ylabel('Writing scores')\nplt.show()","aa38c613":"#Math scores higher than average\nplt.clf()\nmaths_higher_than_average = st.gender[st.maths > st.average]\nsns.countplot(maths_higher_than_average)\nplt.ylabel('Number of students with maths scores higher than their average')\nplt.show()","0a589187":"#scores related to race\/ethnicity\nsns.boxplot(x='gender', y='total', hue = 'race', data = st)\nplt.xlabel('Total score')\nplt.show()","0c45ce53":"#scores related to parents' education\nsns.boxplot(x='gender', y='total', hue = 'parentLE', data = st)\nplt.xlabel('Total score')\nplt.show()","26aede2a":"#This last one represents the number of students with parents divided into categories of race\/etnicity and level of education\nraceEdu = st[['race', 'parentLE']].groupby(['race', 'parentLE']).size().reset_index()\nraceEdu.rename(columns = {0 : 'count'}, inplace = True)\nraceEdu = raceEdu.pivot_table(values='count', index='race', columns='parentLE')\nraceEdu.plot(kind='bar', stacked=True, figsize=(18.5, 10.5))","79460c85":"print(st.gender.unique())\nprint(st.race.unique())\nprint(st.parentLE.unique())\nprint(st.lunch.unique())\nprint(st.prepCourse.unique())","61b7d3de":"students = st\nstudents['gender'] = students['gender'].apply(lambda g: 1 if g == \"male\" else 0)\nstudents['lunch'] = students['lunch'].apply(lambda l: 1 if l == \"standard\" else 0)\nstudents['race'] = students['race'].apply(lambda r: 0 if r == \"group A\" else \n                                                     1 if r == \"group B\" else\n                                                     2 if r == \"group C\" else\n                                                     3 if r == \"group D\" else 4)\nstudents['parentLE'] = students['parentLE'].apply(lambda r: 0 if r == \"some high school\" else \n                                                     1 if r == \"high school\" else\n                                                     2 if r == \"some college\" else\n                                                     3 if r == \"Associate\" else\n                                                     4 if r == \"BSc\" else 5)\n\nstudents['prepCourse'] = students['prepCourse'].astype('int64')\nstudents['average'] = students['average'].astype('int64')\nprint(students.info())","2b781d31":"plt.matshow(students.corr())\nplt.gca().xaxis.tick_bottom()\nplt.xticks(range(len(students.columns)), students.columns)\nplt.xticks(rotation = 90)\nplt.yticks(range(len(students.columns)), students.columns)\nplt.colorbar()\nplt.title(\"Correlation of all variables\")\nplt.show()","e1275562":"#source - https:\/\/www.analyticsvidhya.com\/blog\/2018\/08\/k-nearest-neighbor-introduction-regression-python\/\ndef optimalK(maxK, X_train, X_test, y_train, y_test):\n    from math import sqrt\n    from sklearn.metrics import mean_squared_error\n\n    rmse_val = [] #to store rmse values for different k\n    for K in range(maxK):\n        K = K+1\n        model = KNeighborsRegressor(n_neighbors = K)\n\n        model.fit(X_train, y_train)  #fit the model\n        pred=model.predict(X_test) #make prediction on test set\n        error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n        rmse_val.append(error) #store rmse values\n\n    #plotting the rmse values against k values\n    curve = pd.DataFrame(rmse_val) #elbow curve \n    curve.plot()\n    \n    print(\"Optimal k value is: %s\" % (rmse_val.index(min(rmse_val)) + 1))\n    print('RMSE is %s' % (min(rmse_val)))\n    \n    return(rmse_val.index(min(rmse_val)) + 1)","c5c2d900":"X_train, X_test, y_train, y_test = train_test_split(students[['race','parentLE', 'gender', 'prepCourse', 'lunch']],\n                                                    students.average, test_size=0.30)\n\noptimalK(100, X_train, X_test, y_train, y_test)","bce68f1b":"X_train, X_test, y_train, y_test = train_test_split(students[['writing']],\n                                                    students.average, test_size=0.30)\n\noptK_maths = optimalK(100, X_train, X_test, y_train, y_test)","1cfe5e34":"X_train, X_test, y_train, y_test = train_test_split(students[['race','parentLE', 'gender', 'prepCourse', 'lunch', 'writing']],\n                                                    students.average, test_size=0.30)\n\noptK_maths = optimalK(100, X_train, X_test, y_train, y_test)","18ef7512":"#so RFE DOES NOT WORK ON KNN regression\n#knn = KNeighborsRegressor(n_neighbors=optK_maths)\n#knn.fit(X_train,y_train)\n\n#from sklearn.feature_selection import RFE\n#\n#rfe = RFE(knn, 3)\n#rfe = rfe.fit(X_train, y_train)\n# summarize the selection of the attributes\n#print(rfe.support_)\n#print(rfe.ranking_)\n\n#pred_knn = knn.predict(X_test)\n#print('Actual:    %s' % y_test[1:7].values)\n#print('Predicted: %s' % pred_knn[1:7])","ab35e130":"X_train, X_test, y_train, y_test = train_test_split(students[['gender', 'prepCourse', 'writing']],\n                                                    students.reading, test_size=0.33)\n\noptK_reading = optimalK(100, X_train, X_test, y_train, y_test)","fce4343e":"knn = KNeighborsRegressor(n_neighbors=optK_reading)\nknn.fit(X_train,y_train)\n\npred_knn = knn.predict(X_test)\nprint('Actual:    %s' % y_test[1:7].values)\nprint('Predicted: %s' % pred_knn[1:7])\nprint(\"Score is: %s\" % (knn.score(X_test, y_test)))\nprint(\"Correlation bn predicted and actual values: %s\" % (round(np.corrcoef(pred_knn, y_test)[0, 1], 3)))","78dff5d2":"### Predicting scores in reading\n\nIn addition to the 'preparation course' and 'gender' variables, for the prediction of reading I will also use the writing scores.","897cf131":"It's no surprise that a single grade has much more predictive power that all the other social variables which are only categorical.\n\nOn the last figure about the average score of students we can see that when combined all social information and the writing grade compliment each other and improve the model.","3a3ce184":"## Additional cleanup in preparation for the KNN (K nearest neighbour regression classifier)","10c0be51":"On average boys perform better in maths, whereas writing and reading is reserved for girls domination.\n\n### Reading vs writing scores","7390ba16":"Glad you joined for the exploration of the students perfomrnace dataset. This one is my first python kernel so I will be glad if you like it.\nLets dive straight in.\n\n## Data loading and cleaning","41c2cb4b":"## Preparation for regression based on KNN (K means neighbor)","aabe19c7":"For guys having a MSc parent seems to give an advantage on average, but it is not as pronounced with the girls.\n\n### Parents divided into categories of race\/etnicity and level of education","ceb88087":"### Predicting average scores\n\nNext on we will use the social variables in an attempt to build a model that predicts the average grade of the students.","b5578177":"Interestingly enough, we see that the correlation between the reading and writng scores seems to be fairly high.\n\n### Math scores higher than average","02573a14":"It seems like taking a preparation course paid off well in respect to the writing and reading scores, but the results are\nnot so firm regarding the maths scores.","02b727c6":"\n\nThat was the end of my first python kernel. If you liked it as much as I did developing it, please upvote.","f759c8c0":"Even though, C seems to be the one with highest absolute number of students, the comparison to the others reveals no clear\npattern as to certain enthnic group having more people (parents) with certain education level.","6f0b7f06":"## Data visualization\nAs the data looks better now, we cruise into the visualization part.\n\n### Does preparation matter\nNext on we explore how well did students perfomem based on whether they to a prepCourse or not.","0e6d6ce0":"The data is still not in the most clean of conditions so a bit of renaming and changes of column types are applied.","509dcd5e":"Here we see that there is a certain pattern of how well students did in total, depending on their race\/etnicity.\n\n### Scores related to parents' education","445e06bd":"The root square standard error is relatively high pointing out that the precision of the prediction will be in the range of 13 points. The model is not precise primarily due to the fact that in this first case we only used categorical variables.\n\nLet's try to predict the average score based only on a single grade rather than all other social factors.","68afb007":"The barplot shows the number of students whose math score is higher than their average:(reading + writing + maths)\/3. This result is not to say that the score of boys is that much higher, rather that if they are better in any of the three subjects most likely it is gonna be maths.\n\n### Scores related to race\/ethnicity","ada32026":"We could be faily precise with predicting the reading score based on writing marks having a RMSE of approx. 4.3. "}}