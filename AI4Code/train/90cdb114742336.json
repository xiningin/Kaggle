{"cell_type":{"bf2384f6":"code","55875802":"code","15400c72":"code","e430f830":"code","2c07fd22":"code","c7f60ccf":"code","4f0ea545":"code","1a4333ee":"code","3647a770":"code","95e475b0":"code","2b90e9da":"code","af8a8aa4":"code","44a22049":"code","cf629189":"code","ab18bb58":"code","619ae206":"code","c61841cb":"code","31b04de3":"code","8aab199a":"code","eda2ac38":"code","723d06f5":"code","5e4c6002":"code","7afbfc7c":"code","239b964b":"code","b1684ace":"code","c25476b7":"code","c3bedbeb":"code","3c52c19b":"code","0e625983":"code","0f58815d":"code","337c57eb":"code","7f5f91de":"code","8f1a7cfe":"code","fcd9a45d":"code","471b93ef":"code","7149fd97":"code","315e7451":"code","4f215007":"code","cc4a7ca1":"code","5a2e53bb":"code","20aa2bd7":"code","667f7ad6":"code","a835d51b":"code","2bb2c4fb":"code","bce451e6":"code","968fa7a4":"code","52fb1aae":"code","276a36ab":"code","ac58cedb":"code","c1aeb648":"code","f2183952":"code","5693f9ed":"code","f730e09c":"code","23684c34":"code","a9a265c1":"code","cd1c10bf":"code","854011e7":"code","8d4b85a8":"code","63527bd5":"code","5cc6ca4a":"code","ee00cb5e":"code","431bb824":"code","8728d9fc":"code","5f3fa430":"code","8c695722":"code","8effc0ad":"code","90fd3e52":"code","96ece502":"code","cdd9ed83":"code","8892287b":"code","1e24e605":"code","685f933a":"code","c2217a9f":"code","4e9c0295":"code","6bac0b50":"code","fd8ec8f3":"code","810c6a57":"code","fd44cd04":"code","6f5a4dcf":"code","30119686":"code","b11bb4e0":"code","53d28f78":"code","d51fde5f":"code","3e7d1fd6":"code","b1c4b8bb":"code","e9bf071d":"code","eaf08df1":"code","65611eda":"code","09197d0a":"code","9c23a711":"code","0ae086d8":"code","bca8666d":"code","bc3d9a3c":"code","340a07f6":"code","044c7a33":"code","530c78dc":"markdown","217daed2":"markdown","acc3396d":"markdown","87d570b9":"markdown","3ffcc427":"markdown","d86adc03":"markdown","7bd9fa30":"markdown","c26ba6da":"markdown","53a5cca7":"markdown","3774e309":"markdown","5797644d":"markdown","de1307a2":"markdown","19c49cca":"markdown","72e117d8":"markdown","68ce6196":"markdown","61a57d1f":"markdown","d5b12564":"markdown","1b5c0b82":"markdown","f72ac31b":"markdown","b7a2bfed":"markdown","69caf7d5":"markdown","4152a8e5":"markdown","3d29f0b7":"markdown","9b3a463e":"markdown","d2c35ed4":"markdown","2a75026e":"markdown","31d84520":"markdown","d6187f9f":"markdown"},"source":{"bf2384f6":"import pandas as pd\nimport numpy as np\n\nimport json\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.metrics import confusion_matrix\n# from sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\n# from sklearn.svm import SVC\n# from catboost import CatBoostClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import AdaBoostClassifier\n\nimport keras\nimport tensorflow as tf\n\n# import pprint\nimport gc\nimport os\nimport tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pandas display option\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_row', 1500)\npd.set_option('max_colwidth', 150)\npd.set_option('display.float_format', '{:.2f}'.format)\n\n# data load option\ndtypes = {\"event_id\":\"object\", \"game_session\":\"object\", \"timestamp\":\"object\", \n          \"event_data\":\"object\", \"installation_id\":\"object\", \"event_count\":\"int16\", \n          \"event_code\":\"int16\", \"game_time\":\"int32\", \"title\":\"category\", \n          \"type\":\"category\", \"world\":\"category\"}\nlabel = {\"game_session\":\"object\", \"installation_id\":\"object\", \"title\":\"category\", \n         \"num_correct\":\"int8\", \"num_incorrect\":\"int8\", \n         \"accuracy\":\"float16\", \"accuracy_group\":\"int8\"}\n\n# hyper parameter\nloss_type = \"category\" # mse\/category\ndp_log = True\n# window = 70\nbatch_sizes = 1\nvalidation = True\nscale_type = \"minmax\" # minmax\/robust\/standard","55875802":"train = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/train.csv\", dtype=dtypes)\ntest = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/test.csv\", dtype=dtypes)\nlabel_ = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv\", dtype=label)\n# sample = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv\")\n# specs = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/specs.csv\")","15400c72":"# calculating accuracy\nclass accuracy:\n    def __init__(self, df):\n        self.df = df\n\n        \n    # Assessment evaluation-Cart Balancer (Assessment)\n    def cart_assessment(self):\n        _ = self.df.query(\"title=='Cart Balancer (Assessment)' and event_id=='d122731b'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n\n    def cart_assessment_2(self):\n        _ = self.df.query(\"title=='Cart Balancer (Assessment)' and event_id=='b74258a0'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"]=1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Chest Sorter (Assessment)\n    def chest_assessment(self):\n        _ = self.df.query(\"title=='Chest Sorter (Assessment)' and event_id=='93b353f2'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n    \n    def chest_assessment_2(self):\n        _ = self.df.query(\"title=='Chest Sorter (Assessment)' and event_id=='38074c54'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"]=1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Cauldron Filler (Assessment)\n    def cauldron_assessment(self):\n        _ = self.df.query(\"title=='Cauldron Filler (Assessment)' and event_id=='392e14df'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n\n    def cauldron_assessment_2(self):\n        _ = self.df.query(\"title=='Cauldron Filler (Assessment)' and event_id=='28520915'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Mushroom Sorter (Assessment)\n    def mushroom_assessment(self):\n        _ = self.df.query(\"title=='Mushroom Sorter (Assessment)' and event_id=='25fa8af4'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n    \n    def mushroom_assessment_2(self):\n        _ = self.df.query(\"title=='Mushroom Sorter (Assessment)' and event_id=='6c930e6e'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Bird Measurer (Assessment)\n    def bird_assessment(self):\n        _ = self.df.query(\"title=='Bird Measurer (Assessment)' and event_id=='17113b36'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n    \n    def bird_assessment_2(self):\n        _ = self.df.query(\"title=='Bird Measurer (Assessment)' and event_id=='f6947f54'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]\/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n\n# quadratic kappa\ndef quadratic_kappa(actuals, preds, N=4):\n    w = np.zeros((N,N))\n    O = confusion_matrix(actuals, preds)\n    for i in range(len(w)): \n        for j in range(len(w)):\n            w[i][j] = float(((i-j)**2)\/(N-1)**2)\n    \n    act_hist=np.zeros([N])\n    for item in actuals: \n        act_hist[item]+=1\n    \n    pred_hist=np.zeros([N])\n    for item in preds: \n        pred_hist[item]+=1\n                         \n    E = np.outer(act_hist, pred_hist);\n    E = E\/E.sum();\n    O = O\/O.sum();\n    \n    num=0\n    den=0\n    for i in range(len(w)):\n        for j in range(len(w)):\n            num+=w[i][j]*O[i][j]\n            den+=w[i][j]*E[i][j]\n    return (1 - (num\/den))","e430f830":"test[\"timestamp\"] = pd.to_datetime(test.timestamp)\ntest.sort_values([\"timestamp\", \"event_count\"], ascending=True, inplace=True)\n\n_ = accuracy(test).cart_assessment()\n_ = _.append(accuracy(test).chest_assessment(), ignore_index=True)\n_ = _.append(accuracy(test).cauldron_assessment(), ignore_index=True)\n_ = _.append(accuracy(test).mushroom_assessment(), ignore_index=True)\n_ = _.append(accuracy(test).bird_assessment(), ignore_index=True)\n\ntest = test[test.installation_id.isin(pd.unique(_.installation_id))]\ntest = test.merge(_, how=\"left\", on=[\"installation_id\", \"game_session\"])","2c07fd22":"df_test = []\nidx = 0\nfor _, val in tqdm.tqdm_notebook(test.groupby(\"installation_id\", sort=False)):\n# for _, val in tqdm.notebook.tqdm(test.groupby(\"installation_id\", sort=False)):\n    val.reset_index(drop=True, inplace=True)\n    _ = val.query(\"type=='Assessment'\")\n    _ = _[~_.accuracy_group.isnull()]\n    session = _.reset_index().groupby(\"game_session\", sort=False).index.first().values\n    for j in session:\n        sample = val[:j+1]\n        sample[\"ID\"] = idx\n        idx += 1\n        df_test.append(sample)","c7f60ccf":"label = pd.DataFrame(columns=[\"ID\", \"accuracy_group\"])\nfor i in tqdm.tqdm_notebook(df_test):\n# for i in tqdm.notebook.tqdm(df_test):\n    label = pd.concat([label, i.iloc[-1:, -2:]], sort=False)\n\nlabel.reset_index(drop=True, inplace=True)\nlabel.accuracy_group = label.accuracy_group.astype(\"int8\")","4f0ea545":"df = train[train.installation_id.isin(pd.unique(label_.installation_id))]\ndel train\ndf = df.merge(label_.loc[:, [\"installation_id\", \"game_session\", \"title\", \"accuracy_group\"]], \n              on=[\"installation_id\", \"game_session\", \"title\"], how=\"left\")\ndf[\"timestamp\"] = pd.to_datetime(df.timestamp)\ndf.sort_values([\"timestamp\", \"event_count\"], ascending=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)","1a4333ee":"df_train = []\nidx = max(label.ID)+1\nfor _, val in tqdm.tqdm_notebook(df.groupby(\"installation_id\", sort=False)):\n# for _, val in tqdm.notebook.tqdm(df.groupby(\"installation_id\", sort=False)):\n    val.reset_index(drop=True, inplace=True)\n    session = val.query(\"type=='Assessment'\").reset_index().groupby(\"game_session\", sort=False).index.first().values\n    for j in session:\n        if ~np.isnan(val.iat[j, -1]):\n            sample = val[:j+1]\n            sample[\"ID\"] = idx\n            idx += 1\n            df_train.append(sample)","3647a770":"for i in tqdm.tqdm_notebook(df_train):\n# for i in tqdm.notebook.tqdm(df_train):\n    label = pd.concat([label, i.iloc[-1:, -2:]], sort=False)\n\nlabel.reset_index(drop=True, inplace=True)\nlabel.accuracy_group = label.accuracy_group.astype(\"int8\")\nlabel = label.merge(pd.get_dummies(label.accuracy_group, prefix=\"y\"), left_on=[\"ID\"], right_index=True)\n\ndf_test.extend(df_train)\ndf_train = df_test\ndel df_test","95e475b0":"display(df_train[0].head()), display(label.head())","2b90e9da":"col = {}","af8a8aa4":"ID = []\nworld = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # world_log\n    _ = i.groupby([\"ID\", \"world\"]).size().reset_index()\n    ID.extend(_.ID)\n    world.extend(_.world)\n    size.extend(_[0])\n\nworld_log = pd.DataFrame(data={\"ID\":ID, \"world\":world, \"size\":size})\nworld_log = world_log.pivot_table(index=\"ID\", columns=\"world\", values=\"size\")\nworld_log = world_log.fillna(0)\nworld_log.columns.name = None\nworld_log.reset_index(inplace=True)\nworld_log = world_log.loc[:, [\"ID\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]","44a22049":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(world_log.merge(label).query(\"accuracy_group==@i\")[val], label=i)\n    plt.legend()\n    \n    plt.subplot(2, 4, idx+5)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(np.log(world_log.merge(label).query(\"accuracy_group==@i\")[val]+1), label=i)\n    plt.legend()\nplt.show()","cf629189":"world_log = world_log.add_suffix(\"_l\")\nworld_log.rename(columns={\"ID_l\":\"ID\"}, inplace=True)\nif dp_log==True:\n    world_log.iloc[:, 1:] = np.log(world_log.iloc[:, 1:]+1)\ngc.collect()","ab18bb58":"world_log.head()","619ae206":"ID = []\nworld = []\ngame_time = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # world_time\n    _ = i.groupby([\"ID\", \"world\", \"game_session\"]).game_time.max().reset_index()\n    ID.extend(_.ID)\n    world.extend(_.world)\n    game_time.extend(_.game_time)\n\nworld_time = pd.DataFrame(data={\"ID\":ID, \"world\":world, \"game_time\":game_time})\nworld_time = world_time.groupby([\"ID\", \"world\"]).sum().reset_index()\nworld_time = world_time.pivot_table(index=\"ID\", columns=\"world\", values=\"game_time\")\nworld_time = world_time.fillna(-1)\nworld_time.columns.name = None\nworld_time[\"ID\"] = world_time.index\nworld_time.reset_index(drop=True, inplace=True)\nworld_time = world_time.loc[:, [\"ID\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]","c61841cb":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(world_time.merge(label).query(\"accuracy_group==@i\")[val], label=i)\n    plt.legend()\n    \n    plt.subplot(2, 4, idx+5)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(np.log(world_time.merge(label).query(\"accuracy_group==@i\")[val]+2), label=i)\n    plt.legend()\nplt.show()","31b04de3":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    sns.distplot(world_time[val])\n#     plt.title(val)\n    plt.subplot(2, 4, idx+5)\n    sns.distplot(np.log(world_time[val]+2))\n#     plt.title(val)\nplt.show()","8aab199a":"world_time.drop(columns=[\"NONE\"], inplace=True)\nworld_time = world_time.add_suffix(\"_t\")\nworld_time.rename(columns={\"ID_t\":\"ID\"}, inplace=True)\nif dp_log==True:\n    world_time.iloc[:, 1:] = np.log(world_time.iloc[:, 1:]+2)\ngc.collect()","eda2ac38":"world_time.head()","723d06f5":"ID = []\nworld = []\ngame_session = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # world_session\n    _ = i.groupby([\"ID\", \"world\"]).game_session.nunique().reset_index()\n    ID.extend(_.ID)\n    world.extend(_.world)\n    game_session.extend(_.game_session)\n\nworld_session = pd.DataFrame(data={\"ID\":ID, \"world\":world, \"game_session\":game_session})\nworld_session = world_session.pivot_table(index=\"ID\", columns=\"world\", values=\"game_session\")\nworld_session = world_session.fillna(0)\nworld_session.columns.name = None\nworld_session[\"ID\"] = world_session.index\nworld_session.reset_index(drop=True, inplace=True)\nworld_session = world_session.loc[:, [\"ID\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]","5e4c6002":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(world_session.merge(label).query(\"accuracy_group==@i\")[val], label=i)\n    plt.legend()\n    \n    plt.subplot(2, 4, idx+5)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(np.log(world_session.merge(label).query(\"accuracy_group==@i\")[val]+1), label=i)\n    plt.legend()\nplt.show()","7afbfc7c":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    sns.distplot(world_session[val])\n#     plt.title(val)\n    plt.subplot(2, 4, idx+5)\n    sns.distplot(np.log(world_session[val]+1))\n#     plt.title(val)\nplt.show()","239b964b":"world_session = world_session.add_suffix(\"_s\")\nworld_session.rename(columns={\"ID_s\":\"ID\"}, inplace=True)\nif dp_log==True:\n    world_session.iloc[:, 1:] = np.log(world_session.iloc[:, 1:]+1)\ngc.collect()","b1684ace":"world_session.head()","c25476b7":"ID = []\nevent_id = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # event_id\n    _ = i.groupby([\"ID\", \"event_id\"]).size().reset_index()\n    ID.extend(_.ID)\n    event_id.extend(_.event_id)\n    size.extend(_[0])\n\nevent_id = pd.DataFrame(data={\"ID\":ID, \"event_id\":event_id, \"size\":size})\nevent_id = event_id.pivot_table(index=\"ID\", columns=\"event_id\", values=\"size\")\nevent_id = event_id.fillna(0)\nevent_id.columns.name = None\nevent_id.index.name = None\nevent_id[\"ID\"] = event_id.index\nevent_id.reset_index(drop=True, inplace=True)","c3bedbeb":"if dp_log==True:\n    event_id.iloc[:, :-1] = np.log(event_id.iloc[:, :-1]+1)\n#     event_id.iloc[:, 1:] = np.log(event_id.iloc[:, 1:]+1)\ngc.collect()","3c52c19b":"event_id.head()","0e625983":"None","0f58815d":"ID = []\ngame_time = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # play_time\n    _ = i.groupby([\"ID\", \"game_session\"]).game_time.max().reset_index()\n    ID.extend(_.ID)\n    game_time.extend(_.game_time)\n\nplay_time = pd.DataFrame(data={\"ID\":ID, \"game_time\":game_time})\nplay_time = play_time.groupby([\"ID\"]).sum().reset_index()\nplay_time.reset_index(drop=True, inplace=True)\nplay_time = play_time.fillna(0)","337c57eb":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(play_time.merge(label).query(\"accuracy_group==@i\")[\"game_time\"], label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(play_time.merge(label).query(\"accuracy_group==@i\")[\"game_time\"]+1), label=i)\nplt.legend()\nplt.show()","7f5f91de":"if dp_log==True:\n    play_time.iloc[:, 1:] = np.log(play_time.iloc[:, 1:]+1)\ngc.collect()","8f1a7cfe":"play_time.head()","fcd9a45d":"gap_time = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_train):\n    # gap_time\n    _ = i.groupby([\"ID\"]).timestamp.agg([\"min\", \"max\"])\n    _.columns.name = None\n    gap_time = pd.concat([gap_time, _], sort=True)\n\ngap_time.reset_index(inplace=True)\ngap_time[\"gap\"] = gap_time[\"max\"]-gap_time[\"min\"]\ngap_time[\"gap\"] = gap_time[\"gap\"].astype(\"int\")","471b93ef":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(gap_time.merge(label).query(\"accuracy_group==@i\")[\"gap\"], label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(gap_time.merge(label).query(\"accuracy_group==@i\")[\"gap\"]+1), label=i)\nplt.legend()\nplt.show()","7149fd97":"gap_time.drop(columns=[\"max\", \"min\"], inplace=True)\nif dp_log==True:\n    gap_time.iloc[:, 1:] = np.log(gap_time.iloc[:, 1:]+1)\ngc.collect()","315e7451":"gap_time.head()","4f215007":"session_count = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_train):\n    # session_count\n    _ = i.groupby([\"ID\"]).game_session.nunique().reset_index()\n    _.columns.name = None\n    session_count = pd.concat([session_count, _], sort=True)\n\nsession_count.reset_index(drop=True, inplace=True)","cc4a7ca1":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(session_count.merge(label).query(\"accuracy_group==@i\")[\"game_session\"], bins=50, label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(session_count.merge(label).query(\"accuracy_group==@i\")[\"game_session\"]), bins=50, label=i)\nplt.legend()\nplt.show()","5a2e53bb":"if dp_log==True:\n    session_count.iloc[:, 1:] = np.log(session_count.iloc[:, 1:])\ngc.collect()","20aa2bd7":"session_count.head()","667f7ad6":"session_length = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_train):\n    # session_length\n#     _ = i.query(\"type!='Clip'\").groupby([\"ID\", \"game_session\"]).size().groupby([\"ID\"]).mean().reset_index().rename(columns={0:\"session_length\"})\n    _ = i.groupby([\"ID\", \"game_session\"]).size().groupby([\"ID\"]).mean().reset_index().rename(columns={0:\"session_length\"})\n    _.columns.name = None\n    session_length = pd.concat([session_length, _], sort=True)\n\nsession_length.reset_index(drop=True, inplace=True)","a835d51b":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(session_length.merge(label).query(\"accuracy_group==@i\")[\"session_length\"], bins=50, label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(session_length.merge(label).query(\"accuracy_group==@i\")[\"session_length\"]), bins=50, label=i)\nplt.legend()\nplt.show()","2bb2c4fb":"if dp_log==True:\n    session_length.iloc[:, 1:] = np.log(session_length.iloc[:, 1:])\ngc.collect()","bce451e6":"session_length.head()","968fa7a4":"session_time = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_train):\n    # session_time\n    _ = i.groupby([\"ID\", \"game_session\"]).game_time.max().groupby([\"ID\"]).mean().reset_index()\n    _.columns.name = None\n    session_time = pd.concat([session_time, _], sort=True)\n\nsession_time.reset_index(drop=True, inplace=True)","52fb1aae":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(session_time.merge(label).query(\"accuracy_group==@i\")[\"game_time\"], bins=50, label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(session_time.merge(label).query(\"accuracy_group==@i\")[\"game_time\"]+1), bins=50, label=i)\nplt.legend()\nplt.show()","276a36ab":"if dp_log==True:\n    session_time.iloc[:, 1:] = np.log(session_time.iloc[:, 1:]+1)\ngc.collect()","ac58cedb":"session_time.head()","c1aeb648":"ID = []\ntypes = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # types\n    _ = i.groupby([\"ID\", \"type\"]).size().reset_index()\n    ID.extend(_.ID)\n    types.extend(_.type)\n    size.extend(_[0])\n\ntypes = pd.DataFrame(data={\"ID\":ID, \"type\":types, \"size\":size})\ntypes = types.pivot_table(index=\"ID\", columns=\"type\", values=\"size\")\ntypes.columns.name = None\ntypes.index.name = None\ntypes = types.fillna(0)\ntypes[\"ID\"] = types.index\ntypes = types.loc[:, [\"ID\", \"Activity\", \"Assessment\", \"Clip\", \"Game\"]]","f2183952":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"Activity\", \"Assessment\", \"Clip\", \"Game\"]):\n    plt.subplot(2, 4, idx+1)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(types.merge(label).query(\"accuracy_group==@i\")[val], label=i)\n    plt.legend()\n    \n    plt.subplot(2, 4, idx+5)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(np.log(types.merge(label).query(\"accuracy_group==@i\")[val]+1), label=i)\n    plt.legend()\nplt.show()","5693f9ed":"if dp_log==True:\n    types.iloc[:, 1:] = np.log(types.iloc[:, 1:]+1)\ngc.collect()","f730e09c":"types.head()","23684c34":"ID = []\ntitle = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # title\n    _ = i.groupby([\"ID\", \"title\"]).size().reset_index()\n    ID.extend(_.ID)\n    title.extend(_.title)\n    size.extend(_[0])\n\ntitle = pd.DataFrame(data={\"ID\":ID, \"title\":title, \"size\":size})\ntitle = title.pivot_table(index=\"ID\", columns=\"title\", values=\"size\")\ntitle.columns.name = None\ntitle.index.name = None\ntitle = title.fillna(0)\ntitle[\"ID\"] = title.index","a9a265c1":"if dp_log==True:\n    title.iloc[:, :-1] = np.log(title.iloc[:, :-1]+1)\ngc.collect()","cd1c10bf":"title.head()","854011e7":"assessment = pd.DataFrame(columns=[\"ID\", \"title\"])\nfor i in tqdm.tqdm_notebook(df_train):\n    # assessment\n    _ = i.tail(1).loc[:, [\"ID\", \"title\"]].reset_index(drop=True)\n    assessment = pd.concat([assessment, _], sort=False)\n\nassessment['Assessment_1'] = 0\nassessment['Assessment_2'] = 0\nassessment['Assessment_3'] = 0\nassessment['Assessment_4'] = 0\nassessment['Assessment_5'] = 0\n\nassessment.loc[assessment.title=='Mushroom Sorter (Assessment)', 'Assessment_1'] = 1\nassessment.loc[assessment.title=='Cauldron Filler (Assessment)', 'Assessment_2'] = 1\nassessment.loc[assessment.title=='Chest Sorter (Assessment)', 'Assessment_3'] = 1\nassessment.loc[assessment.title=='Cart Balancer (Assessment)', 'Assessment_4'] = 1\nassessment.loc[assessment.title=='Bird Measurer (Assessment)', 'Assessment_5'] = 1","8d4b85a8":"_ = assessment.merge(label).groupby([\"title\", \"accuracy_group\"]).size().reset_index()\n_.accuracy_group = _.accuracy_group.astype(\"object\")\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"title\", y=0, hue=\"accuracy_group\", data=_, dodge=True, alpha=.7)\nplt.show()","63527bd5":"plt.figure(figsize=(20, 10))\nplt.bar(\"title\", height=\"count\", \n        data=assessment.groupby(\"title\").size().reset_index().rename(columns={0:\"count\"}))\nplt.show()","5cc6ca4a":"del assessment[\"title\"]\n# assessment = assessment.loc[:, [\"ID\", \"Assessment_1\", \"Assessment_2\", \"Assessment_3\", \"Assessment_4\", \"Assessment_5\"]]","ee00cb5e":"assessment.head()","431bb824":"time = pd.DataFrame(columns=[\"ID\", \"timestamp\"])\nfor i in tqdm.tqdm_notebook(df_train):\n    # time\n    _ = i.tail(1).loc[:, [\"ID\", \"timestamp\"]]\n    time = pd.concat([time, _], sort=False)\n\ntime.reset_index(drop=True, inplace=True)\ntime[\"hour\"] = time.timestamp.dt.hour\ntime[\"hour\"] = time.hour.astype(\"object\")\ntime = time.merge(pd.get_dummies(time.hour, prefix=\"hour\"), how=\"left\", \n                  left_index=True, right_index=True)\ntime.drop(columns=[\"timestamp\", \"hour\"], inplace=True)","8728d9fc":"time.head()","5f3fa430":"ID = []\ngame_title = []\ngame_round = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    if \"Game\" in i.type.unique():\n        _ = i.query(\"type=='Game'\").loc[:, [\"ID\", \"title\", \"event_data\"]].set_index([\"ID\", \"title\"]).event_data.apply(lambda x: json.loads(x)[\"round\"]).groupby([\"ID\", \"title\"]).max().reset_index()\n        ID.extend(list(_.ID))\n        game_title.extend(_.title)\n        game_round.extend(_.event_data)\n        \ngame = pd.DataFrame(data={\"ID\":ID, \"game_title\":game_title, \"round\":game_round})\ngame = game.pivot_table(index=\"ID\", columns=\"game_title\", values=\"round\")\ngame.reset_index(inplace=True)\ngame.columns.name = None\ngame = game.fillna(-1)\n\nID = pd.DataFrame(data={\"ID\":range(0, len(df_train))})\ngame = ID.merge(game, how=\"left\")\ngame = game.fillna(-1)\n\ngame = game.add_suffix(\"_r\")\ngame.rename(columns={\"ID_r\":\"ID\"}, inplace=True)","8c695722":"game.head()","8effc0ad":"# data_set = [world_log, world_time, world_session, event_id, play_time, gap_time, session_count, session_length, session_time, types, title, assessment, time, game]\n# _ = pd.concat(data_set, axis=1, keys=[\"ID\"])","90fd3e52":"_ = world_log.merge(world_time, how=\"left\", on=[\"ID\"])\n_ = _.merge(world_session, how=\"left\", on=[\"ID\"])\n_ = _.merge(event_id, how=\"left\", on=[\"ID\"])\n_ = _.merge(play_time, how=\"left\", on=[\"ID\"])\n_ = _.merge(gap_time, how=\"left\", on=[\"ID\"])\n_ = _.merge(session_count, how=\"left\", on=[\"ID\"])\n_ = _.merge(session_length, how=\"left\", on=[\"ID\"])\n_ = _.merge(session_time, how=\"left\", on=[\"ID\"])\n_ = _.merge(types, how=\"left\", on=[\"ID\"])\n_ = _.merge(title, how=\"left\", on=[\"ID\"])\n_ = _.merge(assessment, how=\"left\", on=[\"ID\"])\n_ = _.merge(time, how=\"left\", on=[\"ID\"])\n_ = _.merge(game, how=\"left\", on=[\"ID\"])","96ece502":"train_x_col = list(_.columns)\ntrain_y_col = [\"accuracy_group\", \"y_0\", \"y_1\", \"y_2\", \"y_3\"]","cdd9ed83":"_.to_csv(\"train.csv\", index=False)\nlabel.to_csv(\"label.csv\", index=False)","8892287b":"if loss_type==\"mse\":\n    if scale_type==\"minmax\":\n        scaler = MinMaxScaler()\n    elif scale_type==\"robust\":\n        scaler = RobustScaler()\n    elif scale_type==\"standard\":\n        scaler = StandardScaler()\n    scaler_y = MinMaxScaler()\n    train_x = scaler.fit_transform(_.loc[:, train_x_col[1:]])\n#     train_y = scaler_y.fit_transform([_.loc[:, \"accuracy_group\"]])\n    train_y = label.loc[:, train_y_col]\n    print(train_x[0])\n    print(train_y.iloc[0, :])\nelif loss_type==\"category\":\n    if scale_type==\"minmax\":\n        scaler = MinMaxScaler()\n    elif scale_type==\"robust\":\n        scaler = RobustScaler()\n    elif scale_type==\"standard\":\n        scaler = StandardScaler()\n    train_x = scaler.fit_transform(_.loc[:, train_x_col[1:]])\n    train_y = label.loc[:, train_y_col]\n    print(train_x[0])\n    print(train_y.iloc[0, :])","1e24e605":"class_weights = class_weight.compute_class_weight('balanced', np.unique(label.accuracy_group),\n                                                  label.accuracy_group)\nnp.unique(label.accuracy_group), class_weights","685f933a":"train_y[\"class_weight\"] = 0\ntrain_y.loc[train_y.accuracy_group==0, \"class_weight\"] = class_weights[0]\ntrain_y.loc[train_y.accuracy_group==1, \"class_weight\"] = class_weights[1]\ntrain_y.loc[train_y.accuracy_group==2, \"class_weight\"] = class_weights[2]\ntrain_y.loc[train_y.accuracy_group==3, \"class_weight\"] = class_weights[3]","c2217a9f":"if validation:\n    train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, random_state=1228)\n    display(train_x.shape, train_y.shape)    ","4e9c0295":"for i in range(len(train_x[0])):\n    print(i, min(train_x[:, i]), max(train_x[:, i]))","6bac0b50":"# leakyrelu = keras.layers.LeakyReLU(alpha=0.3)\nleakyrelu = tf.nn.leaky_relu","fd8ec8f3":"model = keras.models.Sequential()\n\nmodel.add(keras.layers.Dense(128, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(256, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(256, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(128, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(64, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(32, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(16, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nif loss_type==\"mse\":\n    model.add(keras.layers.Dense(1, activation=\"linear\"))\n    model.compile(loss=\"mse\", optimizer=\"Adam\")\nelif loss_type==\"category\":\n    model.add(keras.layers.Dense(4, activation=\"softmax\"))\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=['categorical_accuracy'])","810c6a57":"# keras.backend.reset_uids()","fd44cd04":"if not os.path.exists(\"model\"):\n    os.mkdir(\"model\")","6f5a4dcf":"if validation:\n    if loss_type==\"mse\":\n        model.fit(x=train_x, y=train_y.loc[:, [\"accuracy_group\"]], \n                  validation_data=[val_x, val_y.loc[:, [\"accuracy_group\"]]], \n                  epochs=50, batch_size=batch_sizes, shuffle=True, class_weight=class_weight)\n    elif loss_type==\"category\":\n        model.fit(x=train_x, y=train_y.loc[:, [\"y_0\", \"y_1\", \"y_2\", \"y_3\"]].values, \n                  validation_data=[val_x, val_y.loc[:, [\"y_0\", \"y_1\", \"y_2\", \"y_3\"]].values], \n                  epochs=1000, batch_size=batch_sizes, shuffle=True, \n                  sample_weight=train_y.loc[:, [\"class_weight\"]].values.flatten(), \n                  callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_categorical_accuracy\", \n                                                           patience=100, mode=\"auto\"), \n                             keras.callbacks.ModelCheckpoint(\"model\/weights.{epoch:02d}-{val_categorical_accuracy:.3f}.hdf5\", \n                                                             monitor='val_categorical_accuracy', \n                                                             verbose=0, save_best_only=True, save_weights_only=False, \n                                                             mode=\"auto\", period=1)])","30119686":"if validation==False:\n    if loss_type==\"mse\":\n        model.fit(train_x, train_y.values, epochs=150, batch_size=batch_sizes, verbose=1, validation_split=.1, shuffle=True)\n    elif loss_type==\"category\":\n        model.fit(train_x, train_y.values, epochs=100, batch_size=batch_sizes, verbose=1, validation_split=.1, shuffle=True)","b11bb4e0":"# model.fit(train_x, _.accuracy_group.values, epochs=20, batch_size=10, verbose=1, validation_split=.1, shuffle=True)","53d28f78":"if loss_type==\"mse\":\n    plt.figure(figsize=(40, 20))\n    plt.subplot(2, 1, 1)\n    plt.plot(model.history.history[\"loss\"], \"o-\", alpha=.4, label=\"loss\")\n    plt.plot(model.history.history[\"val_loss\"], \"o-\", alpha=.4, label=\"val_loss\")\n    plt.axhline(1.2, linestyle=\"--\", c=\"C2\")\n    plt.legend()\n    plt.subplot(2, 1, 2)\n    plt.plot(model.history.history[\"loss\"][3:], \"o-\", alpha=.4, label=\"loss\")\n    plt.plot(model.history.history[\"val_loss\"][3:], \"o-\", alpha=.4, label=\"val_loss\")\n    plt.axhline(1.1, linestyle=\"--\", c=\"C2\")\n    plt.legend()\n    plt.show()\n\nelif loss_type==\"category\":\n    plt.figure(figsize=(40, 20))\n    plt.subplot(2, 1, 1)\n    plt.plot(model.history.history[\"loss\"], \"o-\", alpha=.4, label=\"loss\")\n    plt.plot(model.history.history[\"val_loss\"], \"o-\", alpha=.4, label=\"val_loss\")\n    plt.axhline(1.05, linestyle=\"--\", c=\"C2\")\n    plt.legend()\n    plt.subplot(2, 1, 2)\n    plt.plot(model.history.history[\"categorical_accuracy\"], \"o-\", alpha=.4, label=\"categorical_accuracy\")\n    plt.plot(model.history.history[\"val_categorical_accuracy\"], \"o-\", alpha=.4, label=\"val_categorical_accuracy\")\n    plt.axhline(.65, linestyle=\"--\", c=\"C2\")\n    plt.legend()\n    plt.show()","d51fde5f":"np.sort(os.listdir(\"model\"))","3e7d1fd6":"model = keras.models.load_model(\"model\/\"+np.sort(os.listdir(\"model\"))[-1], custom_objects={'leaky_relu': tf.nn.leaky_relu})\nnp.sort(os.listdir(\"model\"))[-1]","b1c4b8bb":"if validation:\n    if loss_type==\"mse\":\n        result = model.predict(val_x)\n        result[result <= 1.12232214] = 0\n        result[np.where(np.logical_and(result > 1.12232214, result <= 1.73925866))] = 1\n        result[np.where(np.logical_and(result > 1.73925866, result <= 2.22506454))] = 2\n        result[result > 2.22506454] = 3\n        result = result.astype(\"int\")\n        print(quadratic_kappa(val_y.accuracy_group, result))\n    elif loss_type==\"category\":\n        result = model.predict(val_x)\n        print(quadratic_kappa(val_y.accuracy_group, result.argmax(axis=1)))","e9bf071d":"if validation==False:\n    if loss_type==\"mse\":\n        result = model.predict(train_x)\n        result[result <= 1.12232214] = 0\n        result[np.where(np.logical_and(result > 1.12232214, result <= 1.73925866))] = 1\n        result[np.where(np.logical_and(result > 1.73925866, result <= 2.22506454))] = 2\n        result[result > 2.22506454] = 3\n        result = result.astype(\"int\")\n        print(quadratic_kappa(train_y.accuracy_group, result))\n    elif loss_type==\"category\":\n        result = model.predict(train_x)\n        print(quadratic_kappa(train_y.accuracy_group, result.argmax(axis=1)))","eaf08df1":"test = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/test.csv\", dtype=dtypes)\ntest[\"timestamp\"] = pd.to_datetime(test.timestamp)\n\nlabel = []\ndf_test = []\nfor idx, val in tqdm.tqdm_notebook(test.groupby([\"installation_id\"])):\n    label.append(idx)\n    df_test.append(val)","65611eda":"col = {}\nfor i in [\"world_log\", \"world_time\", \"world_session\", \"event_id\", \"play_time\", \"gap_time\", \"session_count\", \"session_length\", \"session_time\", \"types\", \"title\", \"assessment\", \"time\", \"game\"]:\n    vars()[i].rename(columns={\"ID\":\"installation_id\"}, inplace=True)\n    col[i] = list(vars()[i].columns)\n\n# world_log\ninstallation_id = []\nworld = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # world_log\n    _ = i.groupby([\"installation_id\", \"world\"]).size().reset_index()\n    installation_id.extend(_.installation_id)\n    world.extend(_.world)\n    size.extend(_[0])\n\nworld_log = pd.DataFrame(data={\"installation_id\":installation_id, \"world\":world, \"size\":size})\nworld_log = world_log.pivot_table(index=\"installation_id\", columns=\"world\", values=\"size\")\nworld_log = world_log.fillna(0)\nworld_log.columns.name = None\nworld_log.reset_index(inplace=True)\nworld_log = world_log.loc[:, [\"installation_id\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]\nworld_log = world_log.add_suffix(\"_l\")\nworld_log.rename(columns={\"installation_id_l\":\"installation_id\"}, inplace=True)\nworld_log = world_log.loc[:, col[\"world_log\"]]\nworld_log = world_log.fillna(0)\n\n# world_time\ninstallation_id = []\nworld = []\ngame_time = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # world_time\n    _ = i.groupby([\"installation_id\", \"world\", \"game_session\"]).game_time.max().reset_index()\n    installation_id.extend(_.installation_id)\n    world.extend(_.world)\n    game_time.extend(_.game_time)\n\nworld_time = pd.DataFrame(data={\"installation_id\":installation_id, \"world\":world, \"game_time\":game_time})\nworld_time = world_time.groupby([\"installation_id\", \"world\"]).sum().reset_index()\nworld_time = world_time.pivot_table(index=\"installation_id\", columns=\"world\", values=\"game_time\")\nworld_time = world_time.fillna(-1)\nworld_time.columns.name = None\nworld_time[\"installation_id\"] = world_time.index\nworld_time.reset_index(drop=True, inplace=True)\nworld_time = world_time.loc[:, [\"installation_id\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]\nworld_time = world_time.add_suffix(\"_t\")\nworld_time.rename(columns={\"installation_id_t\":\"installation_id\"}, inplace=True)\nworld_time = world_time.loc[:, col[\"world_time\"]]\nworld_time = world_time.fillna(-1)\n\n# world_session\ninstallation_id = []\nworld = []\ngame_session = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # world_session\n    _ = i.groupby([\"installation_id\", \"world\"]).game_session.nunique().reset_index()\n    installation_id.extend(_.installation_id)\n    world.extend(_.world)\n    game_session.extend(_.game_session)\n\nworld_session = pd.DataFrame(data={\"installation_id\":installation_id, \"world\":world, \"game_session\":game_session})\nworld_session = world_session.pivot_table(index=\"installation_id\", columns=\"world\", values=\"game_session\")\nworld_session = world_session.fillna(0)\nworld_session.columns.name = None\nworld_session[\"installation_id\"] = world_session.index\nworld_session.reset_index(drop=True, inplace=True)\nworld_session = world_session.loc[:, [\"installation_id\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]\nworld_session = world_session.add_suffix(\"_s\")\nworld_session.rename(columns={\"installation_id_s\":\"installation_id\"}, inplace=True)\nworld_session = world_session.loc[:, col[\"world_session\"]]\nworld_session = world_session.fillna(0)\n\n# event_id\ninstallation_id = []\nevent_id = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # event_id\n    _ = i.groupby([\"installation_id\", \"event_id\"]).size().reset_index()\n    installation_id.extend(_.installation_id)\n    event_id.extend(_.event_id)\n    size.extend(_[0])\n\nevent_id = pd.DataFrame(data={\"installation_id\":installation_id, \"event_id\":event_id, \"size\":size})\nevent_id = event_id.pivot_table(index=\"installation_id\", columns=\"event_id\", values=\"size\")\nevent_id = event_id.fillna(0)\nevent_id.columns.name = None\nevent_id.index.name = None\nevent_id[\"installation_id\"] = event_id.index\nevent_id.reset_index(drop=True, inplace=True)\nevent_id = event_id.loc[:, col[\"event_id\"]]\nevent_id = event_id.fillna(0)\n\n# play_time\ninstallation_id = []\ngame_time = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # play_time\n    _ = i.groupby([\"installation_id\", \"game_session\"]).game_time.max().reset_index()\n    installation_id.extend(_.installation_id)\n    game_time.extend(_.game_time)\n\nplay_time = pd.DataFrame(data={\"installation_id\":installation_id, \"game_time\":game_time})\nplay_time = play_time.groupby([\"installation_id\"]).sum().reset_index()\nplay_time.reset_index(drop=True, inplace=True)\nplay_time = play_time.fillna(0)\nplay_time = play_time.loc[:, col[\"play_time\"]]\nplay_time = play_time.fillna(0)\n\n# gap_time\ngap_time = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_test):\n    # gap_time\n    _ = i.groupby([\"installation_id\"]).timestamp.agg([\"min\", \"max\"])\n    _.columns.name = None\n    gap_time = pd.concat([gap_time, _], sort=True)\n\ngap_time.reset_index(inplace=True)\ngap_time[\"gap\"] = gap_time[\"max\"]-gap_time[\"min\"]\ngap_time[\"gap\"] = gap_time[\"gap\"].astype(\"int\")\ngap_time = gap_time.loc[:, col[\"gap_time\"]]\ngap_time = gap_time.fillna(0)\n\n# session_count\nsession_count = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_test):\n    # session_count\n    _ = i.groupby([\"installation_id\"]).game_session.nunique().reset_index()\n    _.columns.name = None\n    session_count = pd.concat([session_count, _], sort=False)\n\nsession_count.reset_index(drop=True, inplace=True)\nsession_count = session_count.loc[:, col[\"session_count\"]]\nsession_count = session_count.fillna(0)\n\n# session_length\nsession_length = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_test):\n    # session_length\n#     _ = i.query(\"type!='Clip'\").groupby([\"installation_id\", \"game_session\"]).size().groupby([\"installation_id\"]).mean().reset_index().rename(columns={0:\"session_length\"})\n    _ = i.groupby([\"installation_id\", \"game_session\"]).size().groupby([\"installation_id\"]).mean().reset_index().rename(columns={0:\"session_length\"})\n    _.columns.name = None\n    session_length = pd.concat([session_length, _], sort=False)\n\nsession_length.reset_index(drop=True, inplace=True)\nsession_length = session_length.loc[:, col[\"session_length\"]]\nsession_length = session_length.fillna(0)\n\n# session_time\nsession_time = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_test):\n    # session_time\n    _ = i.groupby([\"installation_id\", \"game_session\"]).game_time.max().groupby([\"installation_id\"]).mean().reset_index()\n    _.columns.name = None\n    session_time = pd.concat([session_time, _], sort=False)\n\nsession_time.reset_index(drop=True, inplace=True)\nsession_time = session_time.loc[:, col[\"session_time\"]]\nsession_time = session_time.fillna(0)\n\n# types\ninstallation_id = []\ntypes = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # types\n    _ = i.groupby([\"installation_id\", \"type\"]).size().reset_index()\n    installation_id.extend(_.installation_id)\n    types.extend(_.type)\n    size.extend(_[0])\n\ntypes = pd.DataFrame(data={\"installation_id\":installation_id, \"type\":types, \"size\":size})\ntypes = types.pivot_table(index=\"installation_id\", columns=\"type\", values=\"size\")\ntypes.columns.name = None\ntypes.index.name = None\ntypes = types.fillna(0)\ntypes[\"installation_id\"] = types.index\ntypes = types.loc[:, [\"installation_id\", \"Activity\", \"Assessment\", \"Clip\", \"Game\"]]\ntypes = types.loc[:, col[\"types\"]]\ntypes = types.fillna(0)\n\n# title\ninstallation_id = []\ntitle = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # title\n    _ = i.groupby([\"installation_id\", \"title\"]).size().reset_index()\n    installation_id.extend(_.installation_id)\n    title.extend(_.title)\n    size.extend(_[0])\n\ntitle = pd.DataFrame(data={\"installation_id\":installation_id, \"title\":title, \"size\":size})\ntitle = title.pivot_table(index=\"installation_id\", columns=\"title\", values=\"size\")\ntitle.columns.name = None\ntitle.index.name = None\ntitle = title.fillna(0)\ntitle[\"installation_id\"] = title.index\ntitle = title.loc[:, col[\"title\"]]\ntitle = title.fillna(0)\n\n# assessment\nassessment = pd.DataFrame(columns=[\"installation_id\", \"title\"])\nfor i in tqdm.tqdm_notebook(df_test):\n    # assessment\n    _ = i.tail(1).loc[:, [\"installation_id\", \"title\"]].reset_index(drop=True)\n    assessment = pd.concat([assessment, _], sort=False)\n\nassessment['Assessment_1'] = 0\nassessment['Assessment_2'] = 0\nassessment['Assessment_3'] = 0\nassessment['Assessment_4'] = 0\nassessment['Assessment_5'] = 0\n\nassessment.loc[assessment.title=='Mushroom Sorter (Assessment)', 'Assessment_1'] = 1\nassessment.loc[assessment.title=='Cauldron Filler (Assessment)', 'Assessment_2'] = 1\nassessment.loc[assessment.title=='Chest Sorter (Assessment)', 'Assessment_3'] = 1\nassessment.loc[assessment.title=='Cart Balancer (Assessment)', 'Assessment_4'] = 1\nassessment.loc[assessment.title=='Bird Measurer (Assessment)', 'Assessment_5'] = 1\ndel assessment[\"title\"]\nassessment = assessment.loc[:, col[\"assessment\"]]\nassessment = assessment.fillna(0)\n\n# time\ntime = pd.DataFrame(columns=[\"installation_id\", \"timestamp\"])\nfor i in tqdm.tqdm_notebook(df_test):\n    # time\n    _ = i.tail(1).loc[:, [\"installation_id\", \"timestamp\"]]\n    time = pd.concat([time, _], sort=False)\n\ntime.reset_index(drop=True, inplace=True)\ntime[\"hour\"] = time.timestamp.dt.hour\ntime[\"hour\"] = time.hour.astype(\"object\")\ntime = time.merge(pd.get_dummies(time.hour, prefix=\"hour\"), how=\"left\", \n                  left_index=True, right_index=True)\ntime.drop(columns=[\"timestamp\", \"hour\"], inplace=True)\ntime = time.loc[:, col[\"time\"]]\ntime = time.fillna(0)\n\n# game\ninstallation_id = []\ngame_title = []\ngame_round = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    if \"Game\" in i.type.unique():\n        _ = i.query(\"type=='Game'\").loc[:, [\"installation_id\", \"title\", \"event_data\"]].set_index([\"installation_id\", \"title\"]).event_data.apply(lambda x: json.loads(x)[\"round\"]).groupby([\"installation_id\", \"title\"]).max().reset_index()\n        installation_id.extend(list(_.installation_id))\n        game_title.extend(_.title)\n        game_round.extend(_.event_data)\n        \ngame = pd.DataFrame(data={\"installation_id\":installation_id, \"game_title\":game_title, \"round\":game_round})\ngame = game.pivot_table(index=\"installation_id\", columns=\"game_title\", values=\"round\")\ngame.reset_index(inplace=True)\ngame.columns.name = None\ngame = game.fillna(-1)\n\ninstallation_id = pd.DataFrame(data={\"installation_id\":label})\ngame = installation_id.merge(game, how=\"left\")\ngame = game.fillna(-1)\ngame = game.add_suffix(\"_r\")\ngame.rename(columns={\"installation_id_r\":\"installation_id\"}, inplace=True)\ngame = game.loc[:, col[\"game\"]]\ngame = game.fillna(-1)","09197d0a":"if dp_log==True:\n    world_log.iloc[:, 1:] = np.log(world_log.iloc[:, 1:]+1)\n\n# world_time.drop(columns=[\"NONE\"], inplace=True)\nif dp_log==True:\n    world_time.iloc[:, 1:] = np.log(world_time.iloc[:, 1:]+2)\n\nif dp_log==True:\n    world_session.iloc[:, 1:] = np.log(world_session.iloc[:, 1:]+1)\n\nif dp_log==True:\n    event_id.iloc[:, :-1] = np.log(event_id.iloc[:, :-1]+1)\n#     event_id.iloc[:, 1:] = np.log(event_id.iloc[:, 1:]+1)\n\nif dp_log==True:\n    play_time.iloc[:, 1:] = np.log(play_time.iloc[:, 1:]+1)\n\n# gap_time.drop(columns=[\"max\", \"min\"], inplace=True)\nif dp_log==True:\n    gap_time.iloc[:, 1:] = np.log(gap_time.iloc[:, 1:]+1)\n\nif dp_log==True:\n    session_count.iloc[:, 1:] = np.log(session_count.iloc[:, 1:])\n\nif dp_log==True:\n    session_length.iloc[:, 1:] = np.log(session_length.iloc[:, 1:])\n\nif dp_log==True:\n    session_time.iloc[:, 1:] = np.log(session_time.iloc[:, 1:]+1)\n\nif dp_log==True:\n    types.iloc[:, 1:] = np.log(types.iloc[:, 1:]+1)\n\nif dp_log==True:\n    title.iloc[:, :-1] = np.log(title.iloc[:, :-1]+1)","9c23a711":"_ = world_log.merge(world_time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(world_session, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(event_id, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(play_time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(gap_time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(session_count, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(session_length, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(session_time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(types, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(title, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(assessment, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(game, how=\"left\", on=[\"installation_id\"])\ntrain_x_col[0] = \"installation_id\"\n_ = _.loc[:, train_x_col]\n_ = _.fillna(-1)\n_.to_csv(\"test.csv\", index=False)\n\ntest_x = scaler.transform(_.loc[:, train_x_col[1:]])","0ae086d8":"result = model.predict(test_x)","bca8666d":"# result[result <= 1.12232214] = 0\n# result[np.where(np.logical_and(result > 1.12232214, result <= 1.73925866))] = 1\n# result[np.where(np.logical_and(result > 1.73925866, result <= 2.22506454))] = 2\n# result[result > 2.22506454] = 3\n# result = result.astype(\"int\")","bc3d9a3c":"if loss_type==\"mse\":\n    submission = pd.DataFrame({\"installation_id\":_.installation_id, \"accuracy_group\":result.flatten()})\n    submission.to_csv(\"submission.csv\", index=False)\nelif loss_type==\"category\":\n    submission = pd.DataFrame({\"installation_id\":_.installation_id, \"accuracy_group\":result.argmax(axis=1)})\n    submission.to_csv(\"submission.csv\", index=False)","340a07f6":"plt.figure(figsize=(20, 10))\nplt.hist(submission.accuracy_group)\nplt.show()","044c7a33":"np.unique(submission.accuracy_group, return_counts=True)","530c78dc":"## Event_id\nHow many times call event_id","217daed2":"## Title\nWhat title is played?","acc3396d":"### Session time\nHow long did you play in each session on average? (mean, time)","87d570b9":"## Last Assessment type\ntarget Assessment type","3ffcc427":"### Session_count\nHow many session is opend?","d86adc03":"## Assessment time\nWhen did player submit assessment?","7bd9fa30":"### world_time\nHow long did play in each world","c26ba6da":"## Game time","53a5cca7":"2019 Data Science Bowl\n===\nDamien Park  \n2019.11.14  \n\nversion\n---\n* ver 35. event_code aggregates by type  \n* ver 36. fix null event_code in test data set\n* ver 37. take log and standardscaling  \n* ver 38. counting event_id-0.488  \n* ver 39. improving code efficiency(rolling, memory management)\n* ver 40. modeling\n* ver 47. fix minor error(columns)\n* ver 54. category, true, 20, true, minmax\n* ver 55. category, true, 20, true, standard\n* ver 55. category, true, 1, true, minmax","3774e309":"## Data Prepareing","5797644d":"## GAME\nIn Type Game, we can find round feature.","de1307a2":"## Duration","19c49cca":"---\n# Feature Engineering","72e117d8":"## Session","68ce6196":"## World","61a57d1f":"### Session length\nHow long did you play in each session on average? (mean, log)","d5b12564":"### world_session\nHow many session is opend by world","1b5c0b82":"## Type","f72ac31b":"---\nThe end of notebook","b7a2bfed":"---\n# Modeling","69caf7d5":"---\n# Scaling \/ Data Split","4152a8e5":"### world_log\nHow many log in each world","3d29f0b7":"### play_time\nHow long play game","9b3a463e":"### gap_time\nThe gap between start and end","d2c35ed4":"---\n# Predict","2a75026e":"---\n# Merge all data set\nworld_log, world_time, world_session  \nevent_id  \nplay_time, gap_time  \nsession_count, session_length, session_time  \ntypes  \ntitle  \nassessment  \ntime  \ngame  ","31d84520":"### Split data by ID","d6187f9f":"---"}}