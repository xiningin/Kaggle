{"cell_type":{"20f444f5":"code","898f72a6":"code","355c5515":"code","3ee7ecdc":"code","9c2de638":"code","8027f697":"code","5ddd8e8b":"code","0def60f8":"code","50ff83e4":"code","969f2428":"code","cd479cd2":"code","7a3fd273":"code","99a3368c":"code","07053b88":"code","bbe322bb":"code","4dafc71c":"code","7fce5a2b":"code","e7c071c0":"code","1b9393b9":"code","1c3e8719":"markdown","e74f9c39":"markdown","74a78a81":"markdown","7b4ec61a":"markdown","63759a74":"markdown","5e6d97ba":"markdown","74f4acc8":"markdown","48b9b3c3":"markdown"},"source":{"20f444f5":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline \n#plotting directly without requering the plot()\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\") #ignoring most of warnings, cleaning up the notebook for better visualization\n\npd.set_option('display.max_columns', 500) #fixing the number of rows and columns to be displayed\npd.set_option('display.max_rows', 500)\n\nprint(os.listdir(\"..\/input\")) #showing all the files in the ..\/input directory\n\n# Any results you write to the current directory are saved as output. Kaggle message :D","898f72a6":"train = pd.read_csv('..\/input\/train.csv', parse_dates=True, infer_datetime_format=True, dayfirst=True)\ntest = pd.read_csv('..\/input\/test.csv', parse_dates=True, infer_datetime_format=True, dayfirst=True)","355c5515":"print('----- DETAILS of our DATASET -----')\nprint('Training shape: %s' %str(train.shape))\nprint('Testing shape: %s' %str(test.shape))\nprint('First date entry for the training dataset: %s' %train.index.min())\nprint('Last date entry for the training dataset: %s' %train.index.max())\nprint('First date entry for the testing dataset: %s' %test['date'].min())\nprint('Last date entry for the testing dataset: %s' %test['date'].max())\nprint('Number of unique items, training set: %s' %train['item'].nunique())\nprint('Number of unique items, testing set: %s' %test['item'].nunique())\ntrain.head()","3ee7ecdc":"# Concatenating train & test\ntrain['split'] = 'train'\ntest['split'] = 'test'\ndf_total = pd.concat([train,test], sort=False)\nprint('Total shape: {}'.format(df_total.shape))\ndel train, test\ngc.collect()\ndf_total.head()","9c2de638":"df_total['date'] = pd.to_datetime(df_total['date'], dayfirst=True,infer_datetime_format=True)\ndf_total['month'] = df_total['date'].dt.month\ndf_total['year'] = df_total['date'].dt.year\ndf_total['weekday'] = df_total['date'].dt.weekday\ndf_total['day_of_month'] = df_total['date'].dt.day\ndf_total['day_of_year'] = df_total['date'].dt.dayofyear\ndf_total.tail()","8027f697":"#sns.set_style('ticks') #setting the style for our plots\nplt.style.use('dark_background') #another style,this one is from the matplotlib\n\nfig = plt.figure(figsize=(16,14))\n\nplt.subplot(5,1,1)\nsns.lineplot(y='sales', x='year', data=df_total, label='Sales');plt.title('Sales per year')\nplt.subplot(5,1,2)\nsns.lineplot(y='sales', x='month', data=df_total, label='Sales');plt.title('Sales per month')\nplt.subplot(5,1,3)\nsns.lineplot(y='sales', x='weekday', data=df_total, label='Sales');plt.title('Sales per weekday')\nplt.subplot(5,1,4)\nsns.barplot(x=df_total['store'], y=df_total['sales'], errwidth=0,palette=\"PuBuGn_d\");plt.title('Sales distribution across stores')\nplt.subplot(5,1,5)\nsns.barplot(x=df_total['item'], y=df_total['sales'], errwidth=0,palette=\"PuBuGn_d\");plt.title('Sales distribution across items')\nplt.tight_layout(h_pad=2.5)","5ddd8e8b":"# taking the log1+ of our target(sales)\ndf_total['sales'] = np.log1p(df_total.sales.values)","0def60f8":"fig = plt.figure(figsize=(16,5))\ndf_total.set_index('date')['sales'].plot(); plt.title('Distribution of sales log1p')","50ff83e4":"val_months = (df_total.year==2017) & (df_total.month.isin([1,2,3]))\nskip = (df_total.year==2017) & (~val_months)\ndf_total.loc[(val_months), 'split'] = 'val'\ndf_total.loc[(skip), 'split'] = 'skip'\ntrain = df_total.loc[df_total.split.isin(['train','val','test']), :]\ntrain_labels = train.loc[train['split'] == 'train', 'sales'].values.reshape((-1))\ntrain_labels_val = train.loc[train['split'] == 'val', 'sales'].values.reshape((-1))\nprint('Shape Training set: %s' %str(train[train['split']== 'train'].shape))\nprint('Shape Validation set: %s' %str(train[train['split']== 'val'].shape))\nprint('Shape Testing set: %s' %str(train[train['split']== 'test'].shape))\nprint('train labels(Y): %s' %str(train_labels.shape))\nprint('train labels for the validation set(Y_val): %s' %str(train_labels_val.shape))","969f2428":"fig = plt.figure(figsize=(16,5))\ndf_total.set_index('date', inplace=True)\ndf_total.loc[df_total['split'] == 'train', 'sales'].plot(c='r', label='train')\ndf_total.loc[df_total['split'] == 'val', 'sales'].plot(c='b', label='validation')\ndf_total.loc[df_total['split'] == 'skip', 'sales'].plot(c='g', label='skip')\nplt.title('Log 1+ Sales, training, skipped part and validation set.')\nplt.legend()","cd479cd2":"lags=[91,98,105,112,119,126,182,364,546,728] #lag for previous months, 91 days until 728 days\nalpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5] #It's gonna be used for the exponential weighted moving average\n\ntrain.loc[train['split'] == 'val', 'sales'] = np.nan \n#setting all the sales validation which is gonna be used for our transformation below, \n##this is gonna help our model generalize better\n####we have saved our validation labels before, so it's ok\n\ntemp_groupby = train.groupby(['store','item'])\n\n### creating lag features for the number of days in lag ###\nfor i in lags:\n    train['_'.join(['sales', 'lag', str(i)])] = \\\n                temp_groupby['sales'].shift(i).values + np.random.normal(scale=1.6, size=(len(train),)) #normalizing term\n\n### creating the exponential weighted average, using the same days as in lag ###\nfor a in alpha:\n    for i in lags:\n        train['_'.join(['sales', 'lag', str(i), 'ewm', str(a)])] = \\\n            temp_groupby['sales'].shift(i).ewm(alpha=a).mean().values\n\n### creating the rolling mean for 1 year and 1 year and half ###\nfor w in [364,546]:\n    train['_'.join(['sales', 'rmean', str(w)])] = \\\n            temp_groupby['sales'].shift(1).rolling(window=w, \n                                                  min_periods=10,\n                                                  win_type='triang').mean().values +\\\n            np.random.normal(scale=1.6, size=(len(train),)) #normalizing term\n\ndel temp_groupby #--cleaning the memory up--\ngc.collect()\n","7a3fd273":"train.head()","99a3368c":"fig = plt.figure(figsize=(16,5))\nplt.subplot(2,1,1)\nsns.lineplot(y='sales_lag_364_ewm_0.95', x='month',  data=train, label=\"12M EWM\");\nsns.lineplot(y='sales', x='month',  data=train, label=\"Normal Sales\");\nsns.lineplot(y='sales_lag_364', x='month',  data=train, label=\"sales 12M lag\");\nplt.legend()\nplt.subplot(2,1,2)\nsns.lineplot(y='sales_rmean_364', x='month',  data=train, label=\"12M mean\");\nsns.lineplot(y='sales', x='month',  data=train, label=\"Normal Sales\");\nsns.lineplot(y='sales_rmean_546', x='month',  data=train, label=\"sales 20M mean\");\nplt.tight_layout(h_pad=1.5)","07053b88":"train = pd.get_dummies(train, columns=['store','item','day_of_month','weekday','month'])\nprint('Shape after creating the dummies: {}'.format(train.shape))","bbe322bb":"# Final train, validation and testing datasets\ntrain_val = train.loc[train.split=='val', :]\ntrain_X = train.loc[train.split=='train', :]\ntest_sub = train.loc[train.split == 'test',:]\nprint('Training shape:{}, Validation shape:{}, Labels X: {}, Labels Validation: {}, Testing shape: {}'\n      .format(train_X.shape, train_val.shape,train_labels.shape,train_labels_val.shape, test_sub.shape))","4dafc71c":"import time #implementing in this function the time spent on training the model\nfrom catboost import CatBoostRegressor, Pool\nimport lightgbm as lgb\nimport xgboost as xgb\nimport gc\n\n####### FUNCTIONS FOR CALCULATING THE SMAPE LOSS ##########\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False\n\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds==0)&(target==0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds-target)\n    denom = np.abs(preds)+np.abs(target)\n    smape_val = (200*np.sum(num\/denom))\/n\n    return smape_val\n\ndef train_model(X, X_val, y, y_val, params=None, model_type='lgb', plot_feature_importance=False, model=None):\n\n    mask = ['date', 'sales', 'split', 'id', 'year']\n    cols = [col for col in train.columns if col not in mask]\n    evals_result={}\n    if model_type == 'lgb':\n        start = time.time()\n        X_train = lgb.Dataset(data=X.loc[:,cols].values, label=y, \n                       feature_name=cols)\n        \n        X_valid = lgb.Dataset(data=X_val.loc[:,cols].values, label=y_val, \n                     reference=X_train, feature_name=cols)\n        \n        model = lgb.train(params, X_train, num_boost_round=params['num_boost_round'], \n                      valid_sets=[X_train, X_valid], feval=lgbm_smape, \n                      early_stopping_rounds=params['early_stopping_rounds'], \n                      evals_result=evals_result, verbose_eval=500)\n            \n        y_pred_valid = model.predict(X_val.loc[:, cols].values, num_iteration=model.best_iteration)\n        y_pred_valid = np.expm1(y_pred_valid)\n        y_val = np.expm1(y_val)\n        \n        end = time.time()\n        \n        #y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n        \n        print('SMAPE validation data: {}'.format(smape(y_pred_valid, y_val)))\n        \n        if plot_feature_importance:\n            # feature importance\n            fig, ax = plt.subplots(figsize=(12,10))\n            lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n            ax.grid(False)\n            plt.title(\"LightGBM - Feature Importance\", fontsize=15)\n            \n        print('Total time spent: {}'.format(end-start))\n        return model\n            \n    if model_type == 'xgb':\n        start = time.time()\n        train_data = xgb.DMatrix(data=X.loc[:,cols].values, label=y)\n        valid_data = xgb.DMatrix(data=X_val.loc[:,cols].values, label=y_val)\n\n        watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n        model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, \n                          early_stopping_rounds=200, verbose_eval=500, params=params)\n        \n        y_pred_valid = model.predict(xgb.DMatrix(X_val.loc[:,cols].values), ntree_limit=model.best_ntree_limit)\n        \n        end = time.time()\n\n        print('SMAPE validation data: {}'.format(smape(y_pred_valid, y_val)))\n        \n        print('Total time spent: {}'.format(end-start))\n        return model\n            \n    if model_type == 'cat':\n        start = time.time()\n        model = CatBoostRegressor(eval_metric='MAE', **params)\n        model.fit(X.loc[:,cols].values, y, eval_set=(X_val.loc[:,cols].values, y_val), \n                  cat_features=[], use_best_model=True)\n\n        y_pred_valid = model.predict(X_val.loc[:,cols].values)\n        \n        print('SMAPE validation data: {}'.format(smape(y_pred_valid, y_val)))\n        \n        if plot_feature_importance:\n            feature_score = pd.DataFrame(list(zip(X.loc[:,cols].dtypes.index, \n                                                  model.get_feature_importance(Pool(X.loc[:,cols], label=y, cat_features=[])))), columns=['Feature','Score'])\n            feature_score = feature_score.sort_values(by='Score', kind='quicksort', na_position='last')\n            feature_score[:50].plot('Feature', 'Score', kind='barh', color='c', figsize=(12,10))\n            plt.title(\"Catboost Feature Importance plot\", fontsize = 14)\n            plt.xlabel('')\n        \n        end = time.time()\n\n        print('Total time spent: {}'.format(end-start))\n        return model\n        \n    # Clean up memory\n    gc.enable()\n    del model, y_pred_valid, X_test,X_train,X_valid, y_pred, y_train, start, end,evals_result\n    gc.collect()\n\n","7fce5a2b":"params = {\n          'num_leaves': 10,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 6,\n         'learning_rate': 0.02,\n        'num_boost_round': 25000, \n        'early_stopping_rounds':200,\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         'metric': 'mae',\n         \"lambda_l1\": 0.2,\n}\nlgb_model = train_model(train_X,train_val,train_labels,train_labels_val,params, plot_feature_importance=True)","e7c071c0":"params_cat = {\n    'iterations': 2000,\n    'max_ctr_complexity': 6,\n    'random_seed': 42,\n    'od_type': 'Iter',\n    'od_wait': 50,\n    'verbose': 50,\n    'depth': 4\n}\n\ncat_model = train_model(train_X,train_val,train_labels,train_labels_val,params_cat,model_type='cat', plot_feature_importance=True)","1b9393b9":"mask = ['date', 'sales', 'split', 'id', 'year']\ncols = [col for col in train.columns if col not in mask]\npredict_lgb = lgb_model.predict(test_sub.loc[:,cols].values, num_iteration=lgb_model.best_iteration)\npredict_cat = cat_model.predict(test_sub.loc[:,cols].values)\npredict_avg = (predict_lgb+predict_cat)\/2\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['sales'] = np.expm1(predict_avg)\nsub.to_csv('lgb_model.csv', index=False)\nsub.head()","1c3e8719":"** Some information that can be derived from the plots above: **\n\n\n***\n","e74f9c39":"** loading the training and testing dataset **","74a78a81":"## Feature Engineering\n### Grouping the items by month, creating a new feature for it.\n### As the testing set has 3 months ahead, I am gonna be using 3 months before as the lag value, accordingly, I am gonna create features for the 3 months before in the training set, calculating the mean, std and trend.","7b4ec61a":"* Finally, separating the training set and validation set.","63759a74":"Extracting all the information from the data given:","5e6d97ba":"** Using pandas to create dummies in order to enhance the performance of the LGB model**","74f4acc8":"**In order to make features engineering easier, let's create a dataframe with both testing and training set together**","48b9b3c3":"#### Labeling the validation data that is gonna be used, every 3 months is gonna be used for validation starting in january\n* Gonna separate the labels, our target for validation and training set"}}