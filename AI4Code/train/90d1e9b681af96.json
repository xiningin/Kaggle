{"cell_type":{"86c950ed":"code","5431aca2":"code","9083ada0":"code","1d742ddb":"code","d93b5cc7":"code","ce1faa23":"code","92b55416":"code","0379d108":"code","d8bdab5b":"code","496331f1":"code","1800cea6":"code","ca524910":"code","8b75792b":"code","dea20fcf":"code","fd039f89":"code","db7bbfaa":"code","09e8617d":"code","652dcd6b":"code","e15ce621":"code","07b45c44":"code","18b3d5b5":"code","42fc3bc7":"code","8bd4ec41":"code","e79c8feb":"code","b12b9bd6":"code","6c198807":"code","5fe88649":"code","4fcf64b1":"code","9eb2b603":"code","e4742292":"code","74c7a57f":"code","3a901269":"code","cd1dd4a0":"code","76c6756d":"code","da16f16d":"code","f87a4365":"code","d2ccb45b":"code","10c1ef3c":"code","64eb6a70":"code","9e2170d1":"code","187308cc":"code","39dae2b3":"code","f23fd05e":"code","eaf29117":"code","e7714cdd":"code","71d7de8d":"code","616706ee":"code","a3801b82":"code","06b602bd":"code","9c93ae1c":"code","adabf827":"code","e93092b3":"code","4fedab32":"code","d9be1581":"code","d697f562":"code","017629cc":"code","36a0d01a":"code","fcbdc422":"code","c301b70d":"code","0bf1b550":"code","38f9ad47":"code","ea9dadcd":"code","b62256b3":"code","2fd86e2f":"code","c9118a8e":"code","1070636d":"code","859eda72":"code","669cd62f":"code","b9ce844c":"code","ffbfe59b":"markdown","e4f8d555":"markdown","3f68d792":"markdown","88d016cf":"markdown","92a38c9e":"markdown","8bd05efb":"markdown","b90341e6":"markdown","1e473f38":"markdown","2c4bfc97":"markdown","f80f29e4":"markdown","a1bca4fd":"markdown","77923ddf":"markdown","b637acc5":"markdown","c0d2e84a":"markdown","ef8f070b":"markdown","ddfa6af0":"markdown","52ea9c70":"markdown","838a8e33":"markdown","7e42663b":"markdown","3695596b":"markdown","be95fe1d":"markdown","1a847a67":"markdown","be8f2250":"markdown","c8bbda08":"markdown","8d2de469":"markdown","3691db55":"markdown","f4a4c025":"markdown","a21437ec":"markdown","f299714b":"markdown","b3f128c8":"markdown","5eca6f93":"markdown","43aefe1b":"markdown","df30ead6":"markdown","58d65665":"markdown","730ecbfe":"markdown","b6a0285c":"markdown","88810fc4":"markdown","eb19b57c":"markdown","492390ba":"markdown","5f9f68c9":"markdown","24b964eb":"markdown","7eb1c198":"markdown","65a1bd63":"markdown","91ed2848":"markdown","a39fa03b":"markdown","e5ed599e":"markdown","cc87446b":"markdown","031e8dbe":"markdown","ab0827d5":"markdown","2a6beb02":"markdown","54739ea5":"markdown","91e82c2c":"markdown","a0f458a2":"markdown","269a31da":"markdown","bb9b9799":"markdown","710bdfe3":"markdown","b7296146":"markdown","375d3f4f":"markdown","571340b5":"markdown","fac81492":"markdown","2977db81":"markdown","86d2e276":"markdown","cbe41dcb":"markdown","ceea5ace":"markdown","c2d0bdc6":"markdown","d50cefc3":"markdown","e22145f7":"markdown","7581e0bc":"markdown","8e270905":"markdown","10f1d6af":"markdown","c7b4f8bc":"markdown","71ddeee8":"markdown","5c9202a3":"markdown","6eb6a3fa":"markdown","f5853f8f":"markdown","312c908d":"markdown","8e3f80b4":"markdown","c0f95560":"markdown","ab69afc0":"markdown","4641bfa9":"markdown","a45ab006":"markdown","1f3ba27d":"markdown","60591499":"markdown","f89dabbb":"markdown","c2ab0a92":"markdown","21a5b2b7":"markdown","1067fb46":"markdown","5169ae55":"markdown","5344b54a":"markdown","08db87c4":"markdown","5152e9b6":"markdown","22b28785":"markdown"},"source":{"86c950ed":"#################################################################\n# Libraries\n#################################################################\n\nimport pandas as pd \npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport numpy as np\nimport seaborn as sns\n\nprint(\"Library Import.... Complete\")","5431aca2":"#################################################################\n# Symlinks\n#################################################################\n\nevent_type = pd.read_csv(\"..\/input\/telstra-competition-dataset\/event_type.csv\")\nlog_feature = pd.read_csv(\"..\/input\/telstra-competition-dataset\/log_feature.csv\")\nresource_type = pd.read_csv(\"..\/input\/telstra-competition-dataset\/resource_type.csv\")\nsample_submission = pd.read_csv(\"..\/input\/telstra-competition-dataset\/sample_submission.csv\")\nseverity_type = pd.read_csv(\"..\/input\/telstra-competition-dataset\/severity_type.csv\")\ntest = pd.read_csv(\"..\/input\/telstra-competition-dataset\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/telstra-competition-dataset\/train.csv\")\n\nprint(\"Data Loading.... Complete\")","9083ada0":"#################################################################\n# Data Frame Options\n#################################################################\n\n#To print dataframe in full without truncation\npd.set_option('expand_frame_repr', False) \t#Print DF in single row\npd.set_option('display.max_columns', None) \t#Print all columns\npd.set_option('display.max_rows', None) \t#Print all rows\n\nprint(\"DF Options Loading.... Complete\")","1d742ddb":"#################################################################\n# Helper Function\n#################################################################\n\ndef mergefiles(dfs):\n    countfiles = len(dfs)\n\n    for i in range(countfiles):\n        if i == 0:\n            dfm = dfs[i]\n        else:\n            dfm = pd.merge(dfm,dfs[i],on=\"id\")\n    \n    return dfm\n\nprint(\"Helper function Loading.... Complete\")","d93b5cc7":"# Getting dataframe details e.g columns, data types, total entries etc\ntrain.info()","ce1faa23":"# Viewing top few lines of the dataframe\ntrain.head()","92b55416":"# Reviewing rows\nprint('Total row entries               : ', len(train.index))\nprint('Number of unique ids            : ', len(train.id.unique()))\nprint('Number of unique location       : ', len(train.location.unique()))\nprint('Number of unique fault_severity : ', len(train.fault_severity.unique()))","0379d108":"# Checking if there are missing values in the data frame\ntrain.isnull().sum()","d8bdab5b":"# Getting dataframe details e.g columns, data types, total entries etc\ntest.info()","496331f1":"# Viewing top few lines of the dataframe\ntest.head()","1800cea6":"# Reviewing rows\nprint('Total row entries               : ', len(test.index))\nprint('Number of unique ids            : ', len(test.id.unique()))\nprint('Number of unique location       : ', len(test.location.unique()))","ca524910":"# Checking if there are missing values in the data frame\ntest.isnull().sum()","8b75792b":"#Create new column with origin information\ntrain['istrain'] = 1 #Train set\ntest['istrain'] = 0 #Test set\n\n#Merge dataframes\ndata = train.append(test, sort=False)\nprint(\"Train and Test data merger.... Complete\")","dea20fcf":"# Getting dataframe details e.g columns, data types, total entries etc\ndata.info()","fd039f89":"# Viewing top few lines of the dataframe\ndata.head()","db7bbfaa":"# Viewing bottom few lines of the dataframe\ndata.tail()","09e8617d":"# Reviewing rows\nprint('Total row entries               : ', len(data.index))\nprint('Number of unique ids            : ', len(data.id.unique()))\nprint('Number of unique location       : ', len(data.location.unique()))\nprint('Number of unique fault_severity : ', len(data.fault_severity.unique()))","652dcd6b":"# Checking if there are missing values in the data frame\ndata.isnull().sum()","e15ce621":"#Stripping the string 'location '\ndata.location = data.location.str.lstrip('location ').astype('int')\n\ndata.head()","07b45c44":"#Frequency of location\nplt.figure(figsize=(22,6))\n\ncol='location'\nax=sns.countplot(x = col,\n                 data = data,\n                 order = data[col].value_counts().index)\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\", fontsize=6)\n\nplt.show()","18b3d5b5":"# Frequence of location\n# - Zoom to view top 50\nplt.figure(figsize=(22,6))\n\ncol='location'\nax=sns.countplot(x = col,\n                 data = data,\n                 order = data[col].value_counts()[:50,].index)\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\", fontsize=10)\n\nplt.show()","42fc3bc7":"#Frequency of fault_severity\nplt.figure(figsize=(12,6))\n\ncol='fault_severity'\npd.value_counts(data[col]).plot.bar()\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\n\nplt.show()","8bd4ec41":"#Frequency of istrain\nplt.figure(figsize=(12,6))\n\ncol='istrain'\npd.value_counts(data[col]).plot.bar()\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\n\nplt.show()","e79c8feb":"#location vs fault_severity\ndata.plot.scatter(x='id',\n                  y='location',\n                  c='fault_severity',\n                  colormap='viridis',\n                  figsize=(18,12))","b12b9bd6":"# Getting dataframe details e.g columns, data types, total entries etc\nevent_type.info()","6c198807":"# Viewing top few lines of the dataframe\nevent_type.head()","5fe88649":"# Reviewing rows\nprint('Total row entries               : ', len(event_type.index))\nprint('Number of unique ids            : ', len(event_type.id.unique()))\nprint('Number of unique event_type     : ', len(event_type.event_type.unique()))","4fcf64b1":"# Checking if there are missing values in the data frame\nevent_type.isnull().sum()","9eb2b603":"#Removing event_type from column\nevent_type.event_type= event_type.event_type.str.lstrip('\"event_type ')\n\nevent_type.head()","e4742292":"#Frequency of event_type\nplt.figure(figsize=(18,6))\n\ncol='event_type'\nax=sns.countplot(x = col,\n                 data = event_type,\n                 order = event_type[col].value_counts().index)\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\", fontsize=10)\n\nplt.show()","74c7a57f":"#One Hot Encoding using Panda\nevent_type = pd.get_dummies(event_type, columns=['event_type'])\n\n# Viewing top few lines of the dataframe\nevent_type.head(5)","3a901269":"# Getting dataframe details e.g columns, data types, total entries etc\nlog_feature.info()","cd1dd4a0":"# Viewing top few lines of the dataframe\nlog_feature.head()","76c6756d":"# Reviewing rows\nprint('Total row entries            : ', len(log_feature.index))\nprint('Number of unique ids         : ', len(log_feature.id.unique()))\nprint('Number of unique log_feature : ', len(log_feature.log_feature.unique()))\nprint('Number of unique volume      : ', len(log_feature.volume.unique()))","da16f16d":"# Checking if there are missing values in the data frame\nlog_feature.isnull().sum()","f87a4365":"#Removing log_feature string\nlog_feature.log_feature = log_feature.log_feature.map(lambda x: x.lstrip('feature '))\n\nlog_feature.head()","d2ccb45b":"#Frequency of log_feature\nplt.figure(figsize=(25,6))\n\ncol='log_feature'\npd.value_counts(log_feature[col]).plot.bar()\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\n\nplt.show()","10c1ef3c":"#Frequency of log_feature\n# - Zoom to view the top 50\nplt.figure(figsize=(25,6))\n\ncol='log_feature'\npd.value_counts(log_feature[col])[:50,].plot.bar()\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\n\nplt.show()","64eb6a70":"#Frequency of volume\nplt.figure(figsize=(25,6))\n\ncol='volume'\npd.value_counts(log_feature[col]).plot.bar()\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\n\nplt.show()","9e2170d1":"#Frequency of volume\n# - Zoom to view top 50\nplt.figure(figsize=(25,6))\n\ncol='volume'\npd.value_counts(log_feature[col])[:50,].plot.bar()\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\n\nplt.show()","187308cc":"# Getting dataframe details e.g columns, data types, total entries etc\nresource_type.info()","39dae2b3":"# Viewing top few lines of the dataframe\nresource_type.head()","f23fd05e":"# Reviewing rows\nprint('Total row entries               : ', len(resource_type.index))\nprint('Number of unique ids            : ', len(resource_type.id.unique()))\nprint('Number of unique resource_type  : ', len(resource_type.resource_type.unique()))","eaf29117":"# Checking if there are missing values in the data frame\nresource_type.isnull().sum()","e7714cdd":"#Removing resource_type from column before applying one hot encoding\nresource_type.resource_type = resource_type.resource_type.str.lstrip('resource_type ')\n\nresource_type.head()","71d7de8d":"# Frequency of resource_type\nplt.figure(figsize=(12,6))\n\ncol='resource_type'\npd.value_counts(resource_type[col]).plot.bar()\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\n\nplt.show()","616706ee":"#One Hot Encoding using Panda\nresource_type = pd.get_dummies(resource_type, columns=['resource_type'])\n\nresource_type.head()","a3801b82":"# Getting dataframe details e.g columns, data types, total entries etc\nseverity_type.info()","06b602bd":"# Viewing top few lines of the dataframe\nseverity_type.head()","9c93ae1c":"# Reviewing rows\nprint('Total row entries               : ', len(severity_type.index))\nprint('Number of unique ids            : ', len(severity_type.id.unique()))\nprint('Number of unique severity_type  : ', len(severity_type.severity_type.unique()))","adabf827":"# Checking if there are missing values in the data frame\nseverity_type.isnull().sum()","e93092b3":"#Removing severity_type from column before applying one hot encoding\nseverity_type.severity_type = severity_type.severity_type.str.lstrip('severity_type ')\n\nseverity_type.head()","4fedab32":"#Frequency of Severity Type\nplt.figure(figsize=(12,6))\n\ncol='severity_type'\npd.value_counts(severity_type[col]).plot.bar()\n\nplt.xlabel(col)\nplt.ylabel('Frequency')\n\nplt.show()","d9be1581":"#One Hot Encoding using Panda\nseverity_type = pd.get_dummies(severity_type, columns=['severity_type'])\n\nseverity_type.head()","d697f562":"# Getting dataframe details e.g columns, data types, total entries etc\nsample_submission.info()","017629cc":"# Viewing top few lines of the dataframe\nsample_submission.head()","36a0d01a":"#event_type after one hot encoding is 'sum'med, this is same as 'count' too\nevent_type = event_type.groupby('id', sort=False).agg(sum).add_prefix('sum_').reset_index()\n\nevent_type.head()","fcbdc422":"len(event_type.index)","c301b70d":"#log_feature's 'log_feature' is counted, 'volume' is 'sum'med and 'average'd\nlog_feature = log_feature.groupby('id', sort=False).agg(count_log_feature=('log_feature','count'), \n                                                        sum_volume=('volume', 'sum'),\n                                                        mean_volume=('volume', 'mean')\n                                                      ).reset_index()\nlog_feature.head()","0bf1b550":"len(log_feature.index)","38f9ad47":"#resource_type after one hot encoding is 'sum'med, this is same as 'count' too\nresource_type = resource_type.groupby('id', sort=False).agg(sum).add_prefix('sum_').reset_index()\n\nresource_type.head()","ea9dadcd":"# Merging dataframes\ndfs = [data, log_feature, severity_type, resource_type, event_type] # list of dataframes\nresult = mergefiles(dfs)\n\nresult.head()\n","b62256b3":"len(result.index)","2fd86e2f":"# Checking if there are missing values in the data frame\nresult.isnull().sum()","c9118a8e":"#                        (Dataset)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \n#  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n#  \u2502          Training      \u2502 Validation \u2502  \u2502  Test  \u2502\n#  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \n# Splitting train data from result datafram by istrain column\ntrain = result[result['istrain'] == 1]\ntrain=train.reset_index(drop=True)\ntrain.head()","1070636d":"# Splitting test data from result datafram by istrain column\ntest = result[result['istrain'] == 0]\ntest=test.reset_index(drop=True)\ntest.head()","859eda72":"#################################################################\n# Dataset (Train Test Split)\n# - Train (80% of train_new)\n# - Validation (20% of train_new)\n#################################################################\nfrom sklearn.model_selection import train_test_split\n\n# Selected features - Training data\nX = train.drop(columns='fault_severity')\n\n# Prediction target - Training data\ny = train['fault_severity']\n\n# Selected features - Test data\nx = test.drop(columns='fault_severity')\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\nprint(\"Train Validation Split Complete\")","669cd62f":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB\n#################################################################\n# Pipeline\n#################################################################\npipe_gnb = Pipeline([\n    ('scl', None),\n    ('pca', None),\n    ('clf', GaussianNB())\n    ])\n\n#################################################################\n# Parameter\n#################################################################\nparameters_gnb = {\n        'clf__priors':[None]\n    }\n\n#################################################################\n# Grid Search\n#################################################################\ngrid_gnb = GridSearchCV(pipe_gnb,\n    param_grid=parameters_gnb,\n    scoring='neg_mean_absolute_error',\n    cv=5,\n    refit=True) \n\nprint(\"Pipeline Complete\")","b9ce844c":"import time\nstart_time = time.time()\n\ngrid = grid_gnb\n\nprint('Performing model optimization...')\n\nprint('\\nEstimator: GaussianNB')   \n    \n# Fit grid search   \ngrid.fit(X_train, y_train)\n    \n#Calculate the Mean Absolute Error score.\nmae = grid.score(X_valid,y_valid)\n\n#################################################################\n# Prediction\n#################################################################\n#Predict using the test data with selected features\ny_pred = grid.predict_proba(x)\n\n# Transform numpy array to dataframe\ny_pred = pd.DataFrame(y_pred)\n\n# Rearrange dataframe\ny_pred.columns = ['predict_0', 'predict_1', 'predict_2']\n\ny_pred.insert(0, 'id', x['id'])\n\nfname = \"telstra_baseline_GaussinNB_predict.csv\"\n\n# Save to CSV\ny_pred.to_csv(fname, index = False, header=True)\n\nprint(\"Best params                       : %s\" % grid.best_params_)\nprint(\"Best training data MAE score      : %s\" % grid.best_score_)    \nprint(\"Best validation data MAE score (*): %s\" % mae)\nprint(\"Modeling time                     : %s\" % time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)))\nprint('y_pred                            :\\n %s' % y_pred.head())\n\n#Performing model optimizations...\n#Estimator: GaussianNB\n#Best params                       : {'clf__priors': None}\n#Best training data MAE score      : -0.7249354898893497\n#Best validation data MAE score (*): -0.7020988490182803\n#Modeling time                     : 00:00:00\n#y_pred                            :\n#       id  predict_0  predict_1     predict_2\n#0  11066   0.999972   0.000028  6.850049e-20\n#1  18000   0.001623   0.007001  9.913755e-01\n#2  16964   0.998155   0.001845  5.382375e-20\n#3   4795   0.000167   0.999833  2.295166e-09\n#4   3392   0.109785   0.073884  8.163309e-01","ffbfe59b":"<font size=\"5\"><b>3.1 Split Train Validation Dataset<\/b><\/font><br>","e4f8d555":"*location* is a categorical or object variable","3f68d792":"<font size=\"5\"><b>2.2 Merging Dataframes<\/b><\/font><br>\nAll data, log_feature, severity_type, resource_type, and event_type dataframes are merged","88d016cf":"Observation: volume 1,2,3, and 4 are above 2500","92a38c9e":"*location* is a categorical or object variable.","8bd05efb":"severity_type has no null values","b90341e6":"Observation: Most location are below 75. Not clear those are above 75. Unable to determine outlier as have no further information on data and how it is collected","1e473f38":"*event_type* has no null values","2c4bfc97":"Observation: No fault(0) has the higest frequency, followed by minor fault(1) and major fault(2). An expected pattern, nothing unusual.","f80f29e4":"<font size=\"5\"><b>Cleaning: Event Type<\/b><\/font><br>","a1bca4fd":"Observation: severity_type 1 and 2 occurs more than others","77923ddf":"Observation: resource_type 2 and 8 occurs much more frequently than others","b637acc5":"No null values except for fault severity which is fine as its for test data.","c0d2e84a":"<font size=\"6\"><b>**Telstra Competition Description**<\/b><\/font>\n\nRefer: https:\/\/www.kaggle.com\/c\/telstra-recruiting-network\/data\n\n<font size=\"4\"><b>Introduction:<\/b><br><\/font>\nThe aim on this Notebook is two fold:<br> \na) to understand this competition which includes its goal, the supplied data files, the given features and the submission format;<br> \nb) to come up with a baseline model which is built using simple Machine Learning principles. <br>\nIt considers minor feature engineering and hyperparameter tuning. Concepts like Pipeline, GridSearchCV (hyperparameter turning), and Panda (dataframe manipulation) will be explored. As this is our baseline model, we are not expecting it to produce the best result compared with those produced by other participants.\n\n<font size=\"4\"><b>Competition Goal:<\/b><br><\/font>\nTo predict Telstra network's fault severity at a time at a particular location based on the log data available.<br>\nThe time at a particular location information is captured by the *id* feature.<br>\n\n&emsp; Y = *fault_severity*, X = <features\\> <br>\n    \n<font size=\"4\"><b>Submission format:<\/b><br><\/font>\n&emsp; *id,predict_0,predict_1,predict_2* \n\n<font size=\"4\"><b>Given log files:<\/b><br><\/font>\n&emsp; *train.csv* \u2013 the training data set for fault severity<br>\n&emsp; *test.csv* \u2013 the test set for fault severity<br>\n&emsp; *sample_submission.csv* \u2013 a sample submission file in the correct format<br>\n&emsp; *event_type.csv* \u2013 event type related to the main dataset<br>\n&emsp; *log_feature.csv* \u2013 features extracted from log files<br>\n&emsp; *resource_type.csv* \u2013 type of resource related to the main dataset<br>\n&emsp; *severity_type.csv* \u2013  severity type of a warning message coming from the log<br>\n\n<font size=\"4\"><b>Note:<\/b><br><\/font>\na) *severity_type* is a feature extracted from the log files (in *severity_type.csv*). Often this is a severity type of a warning message coming from the log. *severity_type* is categorical. It does not have an ordering.<br>\nb) *fault_severity* is a measurement of actual reported faults from users of the network and is the target variable (in train.csv). *fault_severity* has 3 categories: *0,1,2* (*0* meaning no fault, *1* meaning only a few, and *2* meaning many).<br>  \n\n<font size=\"4\"><b>Discussion:<\/b><br><\/font>\na) Telstra requires us to predict the fault_severity of their network given the *id*. The failure could be caused by many parameters such as network outtage, congestion, equiptment failures, vandalism, maintanence and etc. Usually alarms would be triggered which usually been grouped by its *severity_type* e.g warning alarms (yellow), error alarms (red) etc. <br>\nb) Aiming to use *event_type.csv*, *log_feature.csv*, *resource_type.csv*  and *severity_type.csv*  logs to get as much information possible to predict the *fault_severity*<br>\nc) Many parameters in the log files does not have explaination, we try not to interpret any parameters beyond the given information as it may lead to wrong conclusion.\n","ef8f070b":"<font size=\"5\"><b>Cleaning: Severity Type<\/b><\/font><br>","ddfa6af0":"log_feature as it is a categorical variable, need to convert it to integer to take advantage of ML principles. One approach is to apply label encoding to the column as hot encoding is not suitable as too large unique values i.e 386. The 2nd approach is to drop the string 'location '. Will follow the 2nd approach as it preserve the current numerical values while label encoder assigns random numbers. We are well aware this approach or label encoding will at times gives a sense of ordinality for a nominal integer to a ML. We are going to live with it, as hot encoding is not suitable for location as it has large unique values i.e 1126.","52ea9c70":"<font size=\"6\"><b>Preparation<\/b><br><\/font>\nData loading, importing relevant libraries, setting dataframe global options and placeholder for helper functions if needed.","838a8e33":"<font size=\"5\"><b>Visualizing: Severity Type<\/b><\/font><br>","7e42663b":"<font size=\"5\" ><b>1.5 Analysis: Event type <\/b><\/font><br>","3695596b":"Observation: log_feature 312, 232, 82, 203, 313 are above 2000","be95fe1d":"Observation: An expected as train data and test data are merged earlier, origin marked at istrain column, nothing unusual.","1a847a67":"*location* is categorical or object variables","be8f2250":"<font size=\"3\" ><b>Columns:<\/b><\/font> *id*, *location*, *fault_severity*, *istrain*<br>","c8bbda08":"*log_feature* is a categorical or object variable","8d2de469":"<font size=\"5\"><b>Hot Encoding: Event Type<\/b><\/font><br>","3691db55":"There are null values for *fault_severity* but this is expected as test set does not have fault_severity column","f4a4c025":"<font size=\"3\" ><b>Columns:<\/b><\/font> *id*, *event_type*<br>","a21437ec":"<font size=\"5\" ><b>1.1 Analysis: Train set<\/b><\/font><br>","f299714b":"<font size=\"5\"><b>Visualizing: Data<\/b><\/font><br>","b3f128c8":"The sample submission file need to be studied too to know what is the expectation and how it need to be submitted.","5eca6f93":"<font size=\"3\" ><b>Columns:<\/b><\/font> *id*, *location* *fault_severity* <br>","43aefe1b":"<font size=\"3\" ><b>Columns:<\/b><\/font> *id*, *severity_type*<br>","df30ead6":"*train* has no null values","58d65665":"Observation: event_type 11,35,34,15 and 20 appears more. Without knowing what the event_type means could not count it as outliers.","730ecbfe":"severity_type is a categorical variable with unique values of just 10. One hot encoding is suitable for it.\nRemoving \"severity_type \" so its easier to do one hot encoding","b6a0285c":"<font size=\"3\" ><b>Columns:<\/b><\/font> *id*, *log_feature*, *volume*<br>","88810fc4":"The row entries are larger than the unique *id*s and *event_type*. Means the unique *id*s and *event_type* are repeated in the rows.<br> ","eb19b57c":"<font size=\"5\" ><b>1.8 Analysis: Severity Type <\/b><\/font><br>","492390ba":"The row entries are larger than the unique *location* and *fault_severity*. Means the unique *location*s and *fault_severity* are repeated in the rows","5f9f68c9":"*location* as it is a categorical variable, need to convert it to integer to take advantage of ML principles. <br>\nOne approach is to apply label encoding to the column as hot encoding is not suitable as too large unique values i.e 1126. <br>\nThe 2nd approach is to drop the string 'location '. <br>\nWill follow the 2nd approach as it preserve the current numerical values while label encoder assigns random numbers.<br>\nWe are well aware this approach or label encoding will at times gives a sense of ordinality for a nominal integer to a ML. <br>\nWe are going to live with it, as hot encoding is not suitable for location as it has large unique values i.e 1126.","24b964eb":"<font size=\"6\"><b>1. Exploratory Data Analysis (EDA):<\/b><br><\/font>\nAnalyzing the file, manipulating columns if needed, drawing plots and deriving insights","7eb1c198":"<font size=\"3\" ><b>Columns:<\/b><\/font> *id*, *resource_type*<br>","65a1bd63":"The row entries are larger than the unique *id*s and *resource_type*. Means the unique *id*s and *resource_type* are repeated in the rows","91ed2848":"<font size=\"4\"><b>2.1.4 Resource Type<\/b><\/font><br>\nDataframe consists of 21076 row entries with the below columns, <br>\n&emsp; *id* <br>\n&emsp; *resource_type_1-10* <br>\nNeed to consolidate the rows with new feature to make it 18552 as the entries are 21076.","a39fa03b":"<font size=\"5\"><b>3.3 Modeling and Prediction<\/b><\/font><br>\nMean Absolute Error (MAE) is a performance metric for Regression and not for Clustering. Will continue use it as it was the metric required by the competition.","e5ed599e":"Observation: fault_severity 2 (many faults) seems to occur more at location > 600 .","cc87446b":"<font size=\"5\"><b>Cleaning: Resource Type<\/b><\/font><br>","031e8dbe":"log_feature has no null values","ab0827d5":"<font size=\"5\"><b>Discussion: Severity Type<\/b><\/font><br>\n\n<font size=\"4\"><b>Frame Dimension:<\/b><\/font><br>\n&emsp;  18552 entries x 2 columns\n\n<font size=\"4\"><b>Columns:<\/b><\/font><br>\n&emsp; *id*                - unique identifier with time and location information<br>\n&emsp; *severity_type*          - severity type identifier. (1-5), no ordinality. Was explicitly mentioned in the description of the data that there is no ordering to these five categories.<br>\n\n<font size=\"4\"><b>Observation:<\/b><br><\/font>\n&emsp; a) 18552 unique *id*. Same as test and train combined *id*.  <br>\n&emsp; b) *severity_type* is a categorical or object variable which later converted to int.<br>\n&emsp; c) 5 unique *severity_type*. As unique values much lesser than total 18552 entries, the unique *severity_type* has been repeated in multiple rows <br>\n&emsp; d) severity_type 1 and 2 seems to occur very frequently. Without knowing the meaning of this type, can't denote it as outlier.<br>","2a6beb02":"*test* has no null values","54739ea5":" *resource_type* is a categorical or object variable","91e82c2c":"<font size=\"4\"><b>2.1.1 Data<\/b><\/font><br>\nDataframe consists of 18552 row entries with the below columns, <br>\n&emsp; *id* <br> \n&emsp; *location* <br> \n&emsp; *fault_severity*, and <br>\n&emsp; *istrain*.<br> \nThese features are fine for now. <br>\nNeed not consolidate the rows as the entries are 18552.","a0f458a2":"<font size=\"5\"><b>3.2 Workflow<\/b><\/font><br>\nPipeline, hyperparameter tuning","269a31da":"*event_type* ,though only left with integers now, is a categorical variable with unique values of just 53. One hot encoding is suitable for it.<br> ","bb9b9799":"<font size=\"3\" ><b>Columns:<\/b><\/font> *id*, *location*<br>","710bdfe3":"The row entries are larger than the unique *id*s, *volume* and *log_feature*. Means the unique *id*s, *volume* and *log_features *are repeated in the rows","b7296146":"<font size=\"6\"><b>3. Data Modeling<\/b><br><\/font>\nBaseline modeling using GaussianNB","375d3f4f":"The row entries are larger than the unique *location*. Means the unique *location*s are repeated in the rows","571340b5":"<font size=\"5\"><b>Cleaning: Data<\/b><\/font><br>","fac81492":"<font size=\"5\" ><b>1.2 Analysis: Test set<\/b><\/font><br>","2977db81":"The row entries are larger than the unique *location* and *fault_severity*. Means the unique *location*s are repeated in the rows","86d2e276":"<font size=\"5\"><b>Discussion: Sample Submission<\/b><\/font><br>\n\n<font size=\"4\"><b>Frame Dimension:<\/b><\/font><br>\n&emsp;  11171 entries x 4 columns<br>\n\n<font size=\"4\"><b>Columns:<\/b><\/font><br>\n&emsp; *id*                - unique identifier with time and location information<br>\n&emsp; *predict_0*          - the probability the prediction is 0 (no fault)<br>\n&emsp; *predict_1*          - the probability the prediction is 1 (minor fault)<br>\n&emsp; *predict_2*          - the probability the prediction is 2 (major fault)<br>\n\n<font size=\"4\"><b>Observation:<\/b><br><\/font>\n&emsp; a) The submission file expect the probability of each prediction by each *id* in the test file.  <br>\n","cbe41dcb":"<font size=\"5\"><b>2.3 Split Train Test Dataset<\/b><\/font><br>\nSplitting using istrain column","ceea5ace":"Observation: Most log_feature are below 2000","c2d0bdc6":"<font size=\"5\"><b>Discussion: Data<\/b><\/font><br>\n\n<font size=\"4\"><b>Frame Dimension:<\/b><\/font><br>\n&emsp;  18552 entries x 4 columns\n\n<font size=\"4\"><b>Columns:<\/b><\/font><br>\n&emsp; a) *id*                - unique identifier with time and location information<br>\n&emsp; b) *location*          - location identifier<br>\n&emsp; c) *fault_severity*    - label or target variable<br>\n&emsp; d) *istrain*            - origin of data, train (1) or test (0)<br>\n\n<font size=\"4\"><b>Observation:<\/b><br><\/font>\n&emsp; a) 18552 unique *id* (Train:7381, Test: 11171)<br>\n&emsp; b) 1126 unique *location*. Values (1-1126). No ordinality<br>\n&emsp; c) *location* is a categorical or object variable which later converted to int<br>\n&emsp; d) *fault_severity* suffers from imbalanced class problem as only have 7381 entries (evident with the null values) compared with rest that have 18552 entries. This not abnormal as *test* data do not have *fault_severity* information and that we are expected to predict it.<br>\n&emsp; e) Most *location* are below 25. Not clear those are above 25. Unable to determine outlier as have no further information on data and how it is collected. No clear pattern for the top 50 frequent *location*.<br>\n&emsp; f) *fault_severity* 2 (many faults) seems to occur more at location > 600.<br>\n&emsp; g) Nothing unusual at *istrain* column.<br>\n\n","d50cefc3":"resource_type has no null values","e22145f7":"resource_type is a categorical variable though removed resource_type string with unique values of just 10. One hot encoding is suitable for it.\n","7581e0bc":"<font size=\"5\" ><b>1.4 Analysis: Data set<\/b><\/font><br>","8e270905":"<font size=\"5\"><b>2.1 Features Engineering<\/b><\/font><br>","10f1d6af":"<font size=\"5\"><b>Hot Encoding: Resource Type<\/b><\/font><br>","c7b4f8bc":"<font size=\"5\"><b>Visualizing: Resource Type<\/b><\/font><br>","71ddeee8":"<font size=\"5\" ><b>1.9 Analysis: Submission <\/b><\/font><br>","5c9202a3":"*severity_type* is a categorical or object variable","6eb6a3fa":" <font size=\"6\"><b>2. Merging Data and Log Dataframes<\/b><br><\/font>\nFeatures creation and merging into single dataframe. \nLog Dataframes: event_type, log_features, resource_type, severity_type<br>\n\nThe general idea is as below,<br>\n&emsp; a) Have only 1 dataframe which consist all features<br>\n&emsp; b) The dataframe consist unique ids, hence any more than 1 types in any feature must me collated by an operation e.g sum, count etc.<br>\n&emsp; c) Create several basic features.<br>","f5853f8f":"<font size=\"5\" ><b>1.6 Analysis: Log Feature <\/b><\/font><br>","312c908d":"<font size=\"5\" ><b>1.3 Combining Train and Test sets into Data set.<\/b><\/font><br>\n\nTrain and Test files has common *id* and *locatio*n columns. Can combine both files. fault_severity for test data would be filled as **null** and additional field *istrain* would be created to denote where the data came from.","8e3f80b4":"<font size=\"4\"><b>2.1.5 Severity type<\/b><\/font><br>\nDataframe consists of 18552 row entries with the below columns, <br>\n&emsp; *id* <br> \n&emsp; *severity_type_1-5* <br>\nThese features are fine for now. <br>\nNeed not consolidate the rows as the entries are 18552.\n","c0f95560":"Columns: id, predict_0, predict_1, predict_2","ab69afc0":"<font size=\"5\"><b>Discussion: Event Type<\/b><\/font><br>\n\n<font size=\"4\"><b>Frame Dimension:<\/b><\/font><br>\n&emsp;  31170 entries x 2 columns\n\n<font size=\"4\"><b>Columns:<\/b><\/font><br>\n&emsp; *id*                - unique identifier with time and location information<br>\n&emsp; *event_type*          - event type identifier. (1-15,17-54), no ordinality<br>\n\n<font size=\"4\"><b>Observation:<\/b><br><\/font>\n&emsp; a) 18552 unique *id*. Same as test and train combined *id*. As unique values lesser than total 31170 entries, unique *id* has been repeated in multiple rows. <br>\n&emsp; b) 53 unique *event_type*. As unique values much lesser than total 31170 entries, the unique *event_type* has been repeated in multiple row for many *id* <br>\n&emsp; c) *event_type* is a categorical or object variable which later converted to int.<br>\n&emsp; d) there is no null value<br>\n&emsp; e) event_type 11,35,34,15 and 20 appears more. <br>","4641bfa9":"Observation: Most volume are below 2500","a45ab006":"<font size=\"4\"><b>2.1.3 Log Feature<\/b><\/font><br>\nDataframe consists of 58671 row entries with the below columns, <br>\n&emsp; *id* <br>\n&emsp; *log_feature* <br>\n&emsp; *volume* <br>\nNeed to consolidate the rows with new feature to make it 18552 as the entries are 58671.","1f3ba27d":"<font size=\"5\"><b>Hot Encoding: Severity Type<\/b><\/font><br>","60591499":"<font size=\"6\"><b>4. Discussion<\/b><br><br><\/font>\n\nPerforming model optimizations...<br>\nEstimator: GaussianNB<br>\nBest params                   : {'clf__priors': None}<br>\nBest training data MAE score  : -0.7249354898893497<br>\nBest validation data MAE score: -0.7020988490182803<br>\nModeling time                 : 00:00:00<br>\n\nfname : telstra_baseline_GaussinNB_predict.csv <br>\nModeling            : 0.7020988490182803 <br>\nPublic Score (MAE)  : 2.14256<br>\nPrivate Score (MAE) : 2.19197<br>\n\nNote: <br>\n&emsp; a) The training and validation data's MAE are in same scale.<br>\n&emsp; b) The public and private scores are much higher than the validation data's MAE suggesting overfitting.<br>\n\nGaussianNB though known as an easily scalable to larger datasets model and extremely fast, it suffer from zero probability problem and known to be a bad estimator. \n\nFuture Work:<br>\n&emsp; a) Consider more features<br>\n&emsp; b) Consider more clustering models.<br>","f89dabbb":"<font size=\"5\"><b>Visualizing: Log Feature<\/b><\/font><br>","c2ab0a92":"Observation: No clear pattern for the top 50 frequent location.","21a5b2b7":"<font size=\"5\"><b>Cleaning:Log Feature<\/b><\/font><br>","1067fb46":"*event_type* is a categorical or object variable","5169ae55":"<font size=\"5\"><b>Discussion: Resource Type<\/b><\/font><br>\n\n<font size=\"4\"><b>Frame Dimension:<\/b><\/font><br>\n&emsp;  21076 entries x 2 columns\n\n<font size=\"4\"><b>Columns:<\/b><\/font><br>\n&emsp; *id*                - unique identifier with time and location information<br>\n&emsp; *resource_type*          - resource type identifier. (0-10), no ordinality<br>\n\n<font size=\"4\"><b>Observation:<\/b><br><\/font>\n&emsp; a) 18552 unique *id*. Same as test and train combined *id*. As unique values lesser than total 21076 entries, unique *id* has been repeated in multiple rows. <br>\n&emsp; b) *event_type* is a categorical or object variable which converted to int<br>\n&emsp; c) 386 unique *resource_type*. As unique values much lesser than total 21076 entries, the unique *resource_type* has been repeated in multiple row for many *id* <br>\n&emsp; d) there is no null values<br>\n&emsp; e) resource_type 8 and 2 occurs very frequently. Without knowing what is represents, can't categorize it as outlier.<br>\n","5344b54a":"<font size=\"5\"><b>Visualizing: Event Type<\/b><\/font><br>","08db87c4":"<font size=\"5\" ><b>1.7 Analysis: Resource Type<\/b><\/font><br>","5152e9b6":"<font size=\"4\"><b>2.1.2 Event Type<\/b><\/font><br>\nDataframe consists of 31170 row entries with the below columns, <br>\n&emsp; *id* <br>\n&emsp; *event_type_1-15* <br>\n&emsp; *event_type_17-54* <br>\nNeed to consolidate the rows with new feature to make it 18552 as the entries are 31170.\n","22b28785":"<font size=\"5\"><b>Discussion: Log Feature<\/b><\/font><br>\n\n<font size=\"4\"><b>Frame Dimension:<\/b><\/font><br>\n&emsp;  58671 entries x 3 columns\n\n<font size=\"4\"><b>Columns:<\/b><\/font><br>\n&emsp; *id*                - unique identifier with time and location information<br>\n&emsp; *log_feature*          - log feature identifier. (1-386), no ordinality<br>\n&emsp; *volume*          - log feature identifier<br>\n\n<font size=\"4\"><b>Observation:<\/b><br><\/font>\n&emsp; a) 18552 unique *id*. Same as test and train combined *id*. As unique values lesser than total 58671 entries, unique *id* has been repeated in multiple rows. <br>\n&emsp; b) *log_feature* is a categorical or object variable which later converted to int<br>\n&emsp; c) 386 unique *log_feature*. As unique values much lesser than total 58671 entries, the unique *log_feature* has been repeated in multiple row for many *id* <br>\n&emsp; d) No point to analyze the uniqueness of *volume* as it is depended on *log_feature*. Most *volume* are below 2500.<br>\n&emsp; e) there is no null values<br>\n&emsp; f) log_feature : Most *log_feature* are below 2000. *log_feature* 312, 232, 82, 203, 313 are above 2000.<br>"}}