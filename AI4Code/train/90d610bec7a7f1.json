{"cell_type":{"beb8abf6":"code","bac34b02":"code","7eb68634":"code","c59c7b4a":"markdown"},"source":{"beb8abf6":"import spacy\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnlp = spacy.load('en_core_web_lg')\nimport numpy as np\nimport pandas as pd\n!pip install bert-extractive-summarizer\nfrom summarizer import Summarizer\nmodel = Summarizer()\n\nimport torch\nfrom transformers import *\nqa_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nqa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","bac34b02":"import re\nimport os\nimport json\n# keep only documents with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file using 3 columns (abstract, title, authors),\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['title','journal','abstract','authors','doi','publish_time','sha','full_text_file'])\nprint ('All CORD19 documents ',df.shape)\n#fill na fields\ndf=df.fillna('no data provided')\n#drop duplicate titles\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n#keep only 2020 dated papers\ndf=df[df['publish_time'].str.contains('2020')]\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\n#show 5 lines of the new dataframe\ndf=search_focus(df)\nprint (\"COVID-19 focused docuemnts \",df.shape)\n#df.head()\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\n\nfor index, row in df.iterrows():\n    if ';' not in row['sha'] and os.path.exists('\/kaggle\/input\/CORD-19-research-challenge\/'+row['full_text_file']+'\/'+row['full_text_file']+'\/pdf_json\/'+row['sha']+'.json')==True:\n        with open('\/kaggle\/input\/CORD-19-research-challenge\/'+row['full_text_file']+'\/'+row['full_text_file']+'\/pdf_json\/'+row['sha']+'.json') as json_file:\n            data = json.load(json_file)\n            body=format_body(data['body_text'])\n            keyword_list=['TB','incidence','age']\n            #print (body)\n            body=body.replace(\"\\n\", \" \")\n\n            df.loc[index, 'abstract'] = body.lower()\n\ndf=df.drop(['full_text_file'], axis=1)\ndf=df.drop(['sha'], axis=1)\ndf.head()","7eb68634":"from IPython.display import display, Markdown, Latex, HTML\n\ndef remove_stopwords(text,stopwords):\n    text = \"\".join(c for c in text if c not in ('!','.',',','?','(',')','-'))\n    text_tokens = word_tokenize(text)\n    #remove stopwords\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n    str1=''\n    str1=' '.join(word for word in tokens_without_sw)\n    return str1\n\ndef score_sentence(search,sentence):\n        main_doc=nlp(sentence)\n        search_doc=nlp(search)\n        sent_score=main_doc.similarity(search_doc)\n        return sent_score\n\ndef process_question(df,search,focus):\n    df_table = pd.DataFrame(columns = [\"pub_date\",\"title\",\"excerpt\",\"rel_score\"])\n    df1 = df[df['abstract'].str.contains(focus)]\n    search=remove_stopwords(search,stopwords)\n    for index, row in df1.iterrows():\n        sentences = row['abstract'].split('. ')\n        pub_sentence=''\n        hi_score=0\n        for sentence in sentences:\n            if len(sentence)>75 and focus in sentence:\n                rel_score=score_sentence(search,sentence)\n                if rel_score>.85:\n                    sentence=sentence.capitalize()\n                    if sentence[len(sentence)-1]!='.':\n                        sentence=sentence+'.'\n                    pub_sentence=pub_sentence+' '+sentence\n                    if rel_score>hi_score:\n                        hi_score=rel_score\n        if pub_sentence!='':\n            authors=row[\"authors\"].split(\" \")\n            link=row['doi']\n            title=row[\"title\"]\n            score=hi_score\n            linka='https:\/\/doi.org\/'+link\n            linkb=title\n            final_link='<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka,linkb)\n            #author_link='<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka,authors[0]+' et al.')\n            #sentence=pub_sentence+' '+author_link\n            sentence=pub_sentence\n            #sentence='<p fontsize=tiny\" align=\"left\">'+sentence+'<\/p>'\n            to_append = [row['publish_time'],final_link,sentence,score]\n            df_length = len(df_table)\n            df_table.loc[df_length] = to_append\n    df_table=df_table.sort_values(by=['rel_score'], ascending=False)\n    return df_table\n\ndef prepare_summary_answer(text,model):\n    #model = pipeline(task=\"summarization\")\n    return model(text)\n\ndef answer_question(question,text,tokenizer,model):\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n    input_ids = tokenizer.encode(input_text)\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    # show qeustion and text\n    #tokenizer.decode(input_ids)\n    return answer\n\n###### MAIN PROGRAM ######\n\n# questions\n#'''\nsearch=[\n'Do cancer patients have more risk of being infected with COVID-19?',\n'Do cancer patients have a higher risk of a severe case?',\n'Do cancer patients at a higher risk mortality or death?',\n'What type of cancer or malignancy can put patients at higher risk for severity or death?'\n]\n#'''\n\nfocus='cancer'\n\nz=0\n\nfor question in search:\n    # process with spacy model and return df\n    df_table=process_question(df,question,focus)\n    \n    #use only the top 5 papers excerpts for summarizaiton\n    df_answer=df_table.head(10)\n    \n    qa_text=''\n    \n    #loop through df to assemble sentences\n    for index, row in df_answer.iterrows():\n        qa_text=qa_text+' '+row['excerpt']\n    \n    qa_text=qa_text[:511]\n    \n    #use only the top 20 papers excerpts for summarizaiton\n    df_summary=df_table.head(10)\n    \n    summary_text=''\n    \n    #loop through df to assemble sentences\n    for index, row in df_summary.iterrows():\n        summary_text=summary_text+' '+row['excerpt']\n\n    # show markdown so link can be crated\n    display(Markdown('# '+question))\n    \n    qa_answer=answer_question(question,qa_text,qa_tokenizer,qa_model)\n    qa_answer=qa_answer.replace(' ##','')\n    display(HTML('<h4> Answer: <\/h4><i>'+qa_answer+'<\/i>'))\n    \n    #summarize questions\n    summary_answer=prepare_summary_answer(summary_text,model)\n    \n    #summary_answer=summary_answer[0]['summary_text']\n    display(HTML('<h4> Summarized Answer: <\/h4><i>'+summary_answer+'<\/i>'))\n    display(HTML('<h5>results limited to 5 for ease of scanning<\/h5>'))\n    \n    #limit the size of the df for the html table\n    df_table=df_table.head(5)\n    \n    #convert df_table to html and display\n    df_table=HTML(df_table.to_html(escape=False,index=False))\n    display(df_table)\n    \n    z=z+1\n\nprint ('done') ","c59c7b4a":"# ** Infection Among Cancer Patients 2.0**\n\n![](https:\/\/sportslogohistory.com\/wp-content\/uploads\/2018\/09\/georgia_tech_yellow_jackets_1991-pres-1.png)\n\n**Executive Summary:** Unsupervised scientific literature understanding system that accepts natural language questions and returns specific answers from the CORD19 scientific paper corpus. The answers are wholly generated by the system from the publicatons cited below the answer. There is also a summary answer of the relevant text and a table with the top 5 articles for ease of scanning.\n\n**APPROACH:**\n- meta.csv is loaded - system uses full text articles where available\n- the natural language questions for the task are contained in a list\n- there is a list of focusing keywords to focus on documents that specifically relate to the topic\n- the natural language questions have the stop words removed for passing to spacy for sentence comparison\n- the full text documents are parsed at sentence level and compared to the question\n- https:\/\/spacy.io\/usage\/vectors-similarity spacy comparison\n- the most relevant sentences are returned in a dataframe and sent to BERT QA and BERT summarizer\n- https:\/\/web.stanford.edu\/class\/archive\/cs\/cs224n\/cs224n.1194\/reports\/default\/15848021.pdf\n- https:\/\/pypi.org\/project\/bert-extractive-summarizer\/\n- The question,summary answers and table of relevant scientific papers are returned in HTML format for user review.\n\n**Scientific rigor:** Is the solution evidence based? i.e. does it leverage robust data?  Yes, the solution accesses the Open Research Dataset (CORD-19). CORD-19 is a resource of over 59,000 scholarly articles, including over 47,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.\n\n**Scientific model\/strategy:** Did the solution employ a robust scientific method? The solution relies on the underlying scientific documents to provide the scientific model or strategy.  The system learns concepts and topics from a MASS of documents and boils them down for the user to understand answers that are contained in the literature. Obviously the quality of the literature fed to the system will an impact on the relaiability of the asnwers to the questions presented.\n\n**Unique and novel insight** a. Does the solution identify information (new data, features, insights etc) that is yet to be \u201cuncovered?\u201d  Yes, this system provides novel insight to the topics posed realting to cancer.  The biggest advantage is that each week when the CORD19 documents are supplemented with thousdands of new documents, the system can quickly analyze all the documents and revise answers based on new information that has been discovered and reported in the updated documents. \n\n**Market Translation and Applicability** a. Does the solution resolve an existing market need for either an individual, health institution or policy maker? Yes, when any researcher or decision maker wants to understand the broader concepts of any topic and know what is know about specific or even general qustions, the system can quickly and easily review a huge corpus and provide answers to questions that can help humnas focus there analysis or questions.\n\n**Speed to market** a. Does it apply to an existing product vision such as a self assessment tool or policy decision-making tool? Yes the system could easily be scaled to a wed-based system that could be easily impelemented with \n\n**Longevity of solution in market** a. Is the solution one that could be used in various markets through time? Yes, the system can be provided different dcouments and questions and can be repurposed to any research goal such as understanding drug or vaccine targets etc.\n\n**Ability of user to collaborate and contribute to other solutions within the Kaggle community** a. Did the user provide expertise and or resources in the form of datasets or models to their fellow Kaggle members? Yes, I have created and shared about 50 COVID-19 notebooks with Kaggle users. Here is a link to the notebooks I have shared https:\/\/www.kaggle.com\/mlconsult\/notebooks  In addition, my work has been cited on the Kaggle contirbutions page numerous times. Follow this link https:\/\/www.kaggle.com\/covid-19-contributions and see the data scientist attributions at the bottom of the tables.\n"}}