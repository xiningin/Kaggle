{"cell_type":{"99c9c56a":"code","0a0a8c37":"code","bcf7cce9":"code","999d006a":"code","37e8ad9f":"code","96814ced":"code","51f02448":"code","31f82df2":"code","a8821cdc":"code","2d75c3f7":"code","72ebd2ad":"code","f6d1739c":"code","8971f9d4":"code","4c680fda":"code","32ca832d":"code","6c8f4ea9":"code","48f477d0":"code","77c835e6":"code","b1e83a7a":"code","73372608":"code","e55204ab":"markdown","8c3060ca":"markdown","39252b2b":"markdown","2eb8bcb9":"markdown","30a8d8ac":"markdown","0ec9edb3":"markdown","3c057a20":"markdown","ecaea7eb":"markdown","bf944d1a":"markdown"},"source":{"99c9c56a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a0a8c37":"# Data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n##### Scikit Learn modules needed for Logistic Regression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler , StandardScaler\nfrom sklearn.metrics import classification_report\n\n# Plotting libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)\n%matplotlib inline","bcf7cce9":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","999d006a":"print(\"Frequency of labels:\\n{}\".format(df[\"Class\"].value_counts()))","37e8ad9f":"# Train & Test split\nx_train, x_test, y_train, y_test = train_test_split(df.iloc[:,:-1],df[\"Class\"],test_size=0.20,\n                                                    random_state=21)\n\nprint('Shape of Training Xs:{}'.format(x_train.shape))\nprint('Shape of Test Xs:{}'.format(x_test.shape))\nprint('Shape of Training y:{}'.format(y_train.shape))\nprint('Shape of Test y:{}'.format(y_test.shape))","96814ced":"# Build Model\nclf = RandomForestClassifier(criterion= \"entropy\",random_state= 42)\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_test)\n# score=clf.score(x_test,y_test)","51f02448":"print(\"Accuracy % of Random forest on test data: {}\".format(score))","31f82df2":"print(\"Values of label in test data: \\n{}\".format(y_test.value_counts()))","a8821cdc":"print(\"Baseline Model Performance\\n\")\nprint(\"----------------------------------------\")\nprint(classification_report(y_test, y_predicted))","2d75c3f7":"## Below packages are needed for Hyper Parameter Tuning of an Algorithm in Scikit Learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","72ebd2ad":"# We create the preprocessing pipelines for both numeric and categorical data.\nnumeric_features = df.columns[:-1]\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', MinMaxScaler())])\n\n# categorical_features = \n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        #('cat', categorical_transformer, categorical_features)\n    ])\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', RandomForestClassifier(random_state= 42))])","f6d1739c":"param_grid = {\n    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n    'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n    'classifier__criterion': [\"gini\",\"entropy\"],\n#     'classifier__max_features': [\"auto\",\"sqrt\",\"log2\"],\n#     'classifier__max_depth':[2,4,6],\n#     'classifier__n_estimators':[10,50,150,200]\n}\n\ngrid_search = GridSearchCV(clf, param_grid, cv=10, iid=False,verbose = 1,n_jobs= -1)\ngrid_search.fit(x_train, y_train)\ny_predicted_grid_rf = grid_search.predict(x_test)\n\nprint((\"best Model from grid search: %.3f\"\n       % grid_search.score(x_test, y_test)))\n# Print your best combination of hyper parameters\nprint(\"Optimum setting of hyperparameters:................\")\ngrid_search.best_params_","8971f9d4":"print(\"Grid Searched Model Performance with Hyper parameter tuning \\n\")\nprint(\"----------------------------------------\")\nprint(classification_report(y_test, y_predicted_grid_rf))","4c680fda":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import BaggingClassifier","32ca832d":"# We create the preprocessing pipelines for both numeric and categorical data.\nnumeric_features = df.columns[:-1]\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', MinMaxScaler())])\n\n# categorical_features = \n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        #('cat', categorical_transformer, categorical_features)\n    ])\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', BaggingClassifier(base_estimator = LogisticRegression(),random_state= 42))])","6c8f4ea9":"param_grid = {\n    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n    'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n    'classifier__max_samples': [0.8,1.0],\n    'classifier__max_features': [0.6,0.8,1.0],\n    'classifier__bootstrap': [True,False],\n    'classifier__bootstrap_features': [True,False],\n#     'classifier__max_features': [\"auto\",\"sqrt\",\"log2\"],\n#     'classifier__max_depth':[2,4,6],\n#     'classifier__n_estimators':[10,50,150,200]\n}\n\ngrid_search = GridSearchCV(clf, param_grid, cv=10, iid=False,verbose = 1,n_jobs= -1)\ngrid_search.fit(x_train, y_train)\ny_predicted_bagging = grid_search.predict(x_test)\n\nprint((\"best Model from grid search: %.3f\"\n       % grid_search.score(x_test, y_test)))\n# Print your best combination of hyper parameters\nprint(\"Optimum setting of hyperparameters:................\")\ngrid_search.best_params_","48f477d0":"print(\"Grid Searched Model Performance with Hyper parameter tuning \\n\")\nprint(\"----------------------------------------\")\nprint(classification_report(y_test, y_predicted_bagging))","77c835e6":"# Confusion Matrix\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted_bagging)\nnp.set_printoptions(precision=2)\ncnf_matrix","b1e83a7a":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","73372608":"#With Normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes= [0,1],\n                      title='Confusion matrix, without normalization')\n# With normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes= [0,1], normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","e55204ab":"### Build Baseline Random Forest Classifier\n- the F1 score of the model and particularly that of Fradulent transactions will be used for selection of optium algoritm for prediction","8c3060ca":"### Build Processing Pipeline with Hyper Parmenter Grid tuning[](http:\/\/)","39252b2b":"### Build an ensemble model using Bagging technique\n- logistic regression as the base estimator will be used.\n- Both sub sampling by features and subset of data will be part of hyper parameter grid tuning","2eb8bcb9":"### Loading Required Libraries","30a8d8ac":"### Use Case Statement\n- Predict Fradulent credit card transaction from the given data.\n- Label of 1 - fraudulent transaction , 0 - genuine transaction\n\n### Key Steps take to build model\n1. Build Data Pipeline\n    * Data preprocessing\n    * Transformation of data\n    * Build baseline model with default hyper parameters\n    * Define hyper parameter grid\n    * Choose optimum prediction model whose F1 score is highest for fradulent transactons","0ec9edb3":"#### Define the grid (dictionary format) for hyer parameters of Random Forest classifiers","3c057a20":"### Load Data","ecaea7eb":"### Split Overall data into Training & Test Set","bf944d1a":"#### Define the grid (dictionary format) for hyer parameters of Random Forest classifiers"}}