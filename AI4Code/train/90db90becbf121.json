{"cell_type":{"993ac6bc":"code","9918bf52":"code","4135dd6e":"code","e01e6da4":"code","1541b653":"code","f9079a7c":"code","001a6bd2":"code","65e3595a":"code","80b91318":"code","d46d6ea7":"code","6f279fbc":"code","0fd84ab2":"code","4aca4061":"code","e02b840d":"code","3b96e680":"code","3ce5219d":"code","6f417fc9":"code","1cdcdbff":"code","219c7acb":"code","9e6aba80":"code","e0d03757":"code","510fa410":"code","9097aa52":"code","4ab12688":"code","6c188877":"code","cede36bb":"code","45bc18be":"code","3692d961":"code","5649acc5":"code","1f2373cd":"code","27569be1":"code","cb65cfde":"code","a89a730f":"code","d5607902":"code","50674617":"code","e271b0c9":"markdown","900dd07f":"markdown"},"source":{"993ac6bc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n#import os\n#print(os.listdir(\"..\/input\"))\n#print(os.getcwd())\n\ndataset=pd.read_csv('..\/input\/diabetes.csv')\ndataset.columns","9918bf52":"dataset.shape","4135dd6e":"dataset.head(5)","e01e6da4":"dataset.describe()","1541b653":"# taking the transpose of the dataset\ndataset.describe().T","f9079a7c":"# Minimum value is observed to be 0 for Glucose, BloodPressure,SkinThickness,Insulin, BMI \n# The same can be observed in the histograms that follows\n# This cannot be true that means ..they are incorrect values and we deal with the incorrect values by first replacing \n# the 0's with nan's\n\n\ndataset_copy =dataset.copy(deep=True)\ndataset_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = dataset_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(to_replace=0,value=np.NaN)     \n \nprint(dataset_copy.isnull().sum())\n","001a6bd2":"# visualise the missing data\nsns.heatmap(dataset_copy.isnull(),cbar=False)\nfig =plt.gcf()\nfig.set_size_inches(10,8)\n","65e3595a":"len(dataset_copy.index)","80b91318":"# With so much of Missing data in the columns : Insulin and then SkinThickness, it is better to drop these two columns\n# Let us see what's the percentage of missing data\ndef missing(data):\n    print(\"Missing values in %\")\n    print(round(((data.isnull().sum() * 100)\/ len(data)),2).sort_values(ascending=False))\n    \nmissing(dataset_copy)    \n","d46d6ea7":"# clearly Insulin has Most of the missing data. We drop this column for the analysis purpose and see how \n# it impacts out prediction model","6f279fbc":"dataset_copy2 = dataset_copy.drop(columns=['Insulin'], axis=1)\ndataset_copy2.head(5)","0fd84ab2":"# To fill these NaNs, the data distribution needs to be understood","4aca4061":"# Dataset Correlation Matrix\ncorr=dataset_copy2.corr()\ncorr\n","e02b840d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.heatmap(corr,annot=True)\nfig =plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","3b96e680":"# Now we shall visualise the data to have a better understanding of the various features values distribution\n\ndataset_copy2.hist(bins=50,figsize=(20,15))\nplt.show()","3ce5219d":"# check for missing data\n# dataset.isnull().sum() # actual data set \ndataset_copy2.isnull().sum()","6f417fc9":"# The data visualisation has helped us in better way otherwise we would not have obtained the incorrect data present \n# in the dataset. We cannot just eliminate the patients wit null\/zero values. This would remove a lot of important \n# data. Another way if to calculate the mean value for a column and substitute the value","1cdcdbff":"# Imputing the nan values for the columns in accordance with their distribution, we get\n","219c7acb":"\ndataset_copy2['Glucose'].fillna(dataset_copy2['Glucose'].mean(), inplace = True)\ndataset_copy2['BloodPressure'].fillna(dataset_copy2['BloodPressure'].mean(), inplace = True)\ndataset_copy2['SkinThickness'].fillna(dataset_copy2['SkinThickness'].mean(), inplace = True)\n#dataset_copy2['Insulin'].fillna(dataset_copy2['Insulin'].mean(), inplace = True)\ndataset_copy2['BMI'].fillna(dataset_copy2['BMI'].mean(), inplace = True)\n\n# Number of Pregnancy can be 0 - so we do not do transformation on that feature\n","9e6aba80":"dataset_copy2.isnull().sum()","e0d03757":"#Plotting after NaN removal\n\ndataset_copy2.hist(bins=50,figsize=(20,15))\nplt.show()","510fa410":"# Skewness:\n# A left-skewed distribution has a long left tail. Left-skewed distributions are also called negatively-skewed \n# distributions. That\u2019s because there is a long tail in the negative direction on the number line. \n# The mean is also to the left of the peak.\n\n# A right-skewed distribution has a long right tail. Right-skewed distributions are also called positive-skewed\n# distributions. That\u2019s because there is a long tail in the positive direction on the number line. \n# The mean is also to the right of the peak.\n\n","9097aa52":"# Splitting the data into training set and Test Set\n\ndataset_copy2.head(5)","4ab12688":"\nX=dataset_copy2.iloc[:,0:7].values\ny=dataset_copy2.iloc[:,-1].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)","6c188877":"from sklearn.preprocessing import StandardScaler,MinMaxScaler\nsc_X = StandardScaler()\n#sc_X =MinMaxScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.fit_transform(X_test)","cede36bb":"#X_traindf = pd.DataFrame(data=X_train_scaled)\n#X_traindf.head()\n","45bc18be":"# Select and train a model\n# Comparing Multiple Algorithms","3692d961":"## import all the algorithms we want to test\n#from sklearn.linear_model import LogisticRegression\n#classifier=LogisticRegression()\n#classifier.fit(X_train,y_train)\n\n## Fitting Naive Bayes to the Training set\n#from sklearn.naive_bayes import GaussianNB\n#classifier = GaussianNB()\n#classifier.fit(X_train, y_train)\n\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel='linear')\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nfrom sklearn.metrics import accuracy_score\nprint('The accuracy of this model is: ', accuracy_score(y_pred, y_test))","5649acc5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor","1f2373cd":"# import the sklearn utility to compare algorithms\nfrom sklearn import model_selection","27569be1":"models =[]\n\nmodels.append(('LR', LogisticRegression(solver='lbfgs')))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('NB', GaussianNB()))\n#models.append(('SVC', SVC(gamma='auto')))\n#models.append(('LSVC', LinearSVC()))\nmodels.append(('RFC', RandomForestClassifier(n_estimators=100)))\nmodels.append(('DTR', DecisionTreeRegressor()))","cb65cfde":"seed=7\nresults=[]\nnames =[]\nX=X_train\nY=y_train\n","a89a730f":"# Every algorithm is tested and results are\n# collected and printed\n\n\n\n\nfor name, model in models:\n    kfold = model_selection.KFold(\n        n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(\n        model, X, Y, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (\n        name, cv_results.mean(), cv_results.std())\n    print(msg)","d5607902":"# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","50674617":"#Logistic Regression seems to be the preferred algorithm for these datasets","e271b0c9":"Data Cleaning: Here we deal with missing data or some incorrect data because Machine learning Algorithms cannot \nproperly with such type of data","900dd07f":"Observations:\n more number of subjects are within the age range of 20 to 30\n Patients having diabetes is less than patients with no diabtes\n \n Some parameters have 0 (null values) for some features which cannot be factually correct like for example Blood Pressure of some people is shown to be 0. or 0 BMI etc. We need to deal with such data during the data transformation phase"}}