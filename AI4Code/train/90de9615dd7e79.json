{"cell_type":{"6a4a94bc":"code","fba20f08":"code","ad911ce8":"code","0cd71035":"code","51adb418":"code","cf2a7b6a":"code","61c7cddc":"code","1c09cfcd":"code","5d8a9f73":"code","3a6c0583":"code","b762d9d8":"code","08e09cc9":"code","689a6018":"code","8ac0eb81":"code","3548d104":"code","2bf85017":"code","eecf5feb":"code","13c1affe":"code","1a69aab2":"code","b01f5b0a":"code","66b00d30":"code","e1456cd5":"code","8338f298":"code","74d23ac7":"code","759a70c6":"code","50007251":"code","6d008c23":"code","1ab0147b":"markdown","64af04f9":"markdown","e8f9b47a":"markdown"},"source":{"6a4a94bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fba20f08":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","ad911ce8":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))","0cd71035":"#copy trainand test ids\ntrain_id = train[\"PassengerId\"]\ntest_id =test[\"PassengerId\"]","51adb418":"# drop PassengerId columns\ntrain.drop(columns=[\"PassengerId\"], inplace = True)\ntest.drop(columns=[\"PassengerId\"], inplace = True)","cf2a7b6a":"print(\"The train data size after dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","61c7cddc":"msno.matrix(train)","1c09cfcd":"plt.figure()\nsns.boxplot(\"Fare\",data=train)\nplt.figure()\nsns.boxplot(\"Age\",data=train)","5d8a9f73":"train = train[train[\"Fare\"]< 500]\ntrain = train[train[\"Age\"] < 80]","3a6c0583":"n_train = train.shape[0]\nn_test = test.shape[0]\ny_train = train.Survived.values\nall_data = pd.concat([train,test]).reset_index(drop = True)\nall_data.drop(columns = [\"Survived\"],inplace = True)\nprint(\"all data shape is\", all_data.shape)","b762d9d8":"all_data_na = pd.DataFrame(all_data.isna().sum()\/len(all_data)*100, columns =[\"Missing Ratio\"])\nall_data_na.sort_values(by=[\"Missing Ratio\"], ascending = False, inplace = True)\nall_data_na","08e09cc9":"plt.figure(figsize = (10,8))\nsns.barplot(x=all_data_na.index,y=all_data_na[\"Missing Ratio\"])","689a6018":"# Filling NaN values\nall_data[\"Cabin\"] = all_data[\"Cabin\"].fillna(\"None\")\nall_data[\"Age\"] = all_data[\"Age\"].fillna(all_data[\"Age\"].median())\nall_data[\"Fare\"] = all_data[\"Fare\"].fillna(all_data[\"Fare\"].median())","8ac0eb81":"col = all_data.columns.to_list()\n\nfor i in col:\n    print(all_data[i].value_counts())","3548d104":"def format_name(df):\n    df['Lname'] = df.Name.apply(lambda x: x.split(' ')[0])\n    df['NamePrefix'] = df.Name.apply(lambda x: x.split(' ')[1])\n    return df   \ndef drop_features(df):\n    df.drop(columns =[\"Ticket\",\"Embarked\",\"Name\"],inplace = True)\n    return df\nformat_name(all_data)\ndrop_features(all_data)","2bf85017":"# Splitting numerical and categorical features\nobject_features = all_data.select_dtypes(include = \"object\")\nnumerical_features = all_data.select_dtypes(exclude = \"object\")","eecf5feb":"le = LabelEncoder()\nob_col=object_features.columns.to_list()\nfor i in ob_col:\n    le.fit(object_features[i])\n    object_features[i] = le.transform(object_features[i])","13c1affe":"from sklearn.preprocessing import MinMaxScaler\nnum_cols = [\"Age\",\"Fare\"]\nNm = MinMaxScaler()\nfor i in num_cols:\n    Nm.fit(numerical_features[[i]].values)\n    numerical_features[i] = Nm.transform(numerical_features[[i]])","1a69aab2":"# Combine numerical and categorical features\nall_data = pd.concat([object_features,numerical_features],axis =1)\n\n# split all_data into train and test sets\ntrain = all_data[:n_train]\ntest = all_data[n_train:]\nprint(\"Train set:\", train.shape)\nprint(\"Test set:\", test.shape)","b01f5b0a":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","66b00d30":"n_folds = 5 \nkf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n\ndef model_eva(model):\n    score = cross_val_score(model, train.values, y_train, cv = kf)\n    return score","e1456cd5":"DTC = DecisionTreeClassifier()\nKNC = KNeighborsClassifier()\nLSVC = LinearSVC()\nRC = RidgeClassifier()\nSGDC = SGDClassifier()\nxg = xgb.XGBRFClassifier()\nlg = lgb.LGBMClassifier()\nGBC = GradientBoostingClassifier()","8338f298":"score = model_eva(DTC)\nprint(\"Decision Tree Classifier score: {:.4f} and Std.: Dev:{:.4f}\".format(score.mean(), score.std()))\nscore = model_eva(KNC)\nprint(\"KNeighborsClassifier score: {:.4f} and Sdt Dev.: {:.4f}\".format(score.mean(), score.std()))\nscore = model_eva(LSVC)\nprint(\"Linear Support Vector Classifier score: {:.4f} and Sdt Dev.: {:.4f}\".format(score.mean(),score.std()))\nscore = model_eva(RC)\nprint(\"RidgeClassifier score: {:.4f} and Sdt Dev.: {:.4f}\".format(score.mean(),score.std()))\nscore = model_eva(SGDC)\nprint(\"SGDC score: {:.4f} and Sdt Dev.: {:.4f}\".format(score.mean(),score.std()))\nscore = model_eva(xg)\nprint(\"XGB score: {:.4f} and Sdt Dev.: {:.4f}\".format(score.mean(),score.std()))\nscore = model_eva(lg)\nprint(\"LGB score: {:.4f} and Sdt Dev.: {:.4f}\".format(score.mean(),score.std()))\nscore = model_eva(GBC)\nprint(\"GBC score: {:.4f} and Sdt Dev.: {:.4f}\".format(score.mean(),score.std()))","74d23ac7":"xg.fit(train, y_train)\nxgb_train_pred = xg.predict(train)\nxgb_pred = xg.predict(test)\n\nlg.fit(train, y_train)\nlgb_train_pred = lg.predict(train)\nlgb_pred = lg.predict(test)\n\nGBC.fit(train, y_train)\nGBC_train_pred = GBC.predict(train)\nGBC_pred = GBC.predict(test)\n","759a70c6":"ensemble = xgb_train_pred*0.3+GBC_train_pred*0.3+lgb_train_pred*0.4\nensemble = ensemble.round()\naccuracy_score(y_train,ensemble)","50007251":"ensemble_pred = xgb_pred*0.3+GBC_pred*0.3+lgb_pred*0.4\nensemble_pred = np.rint(ensemble_pred)","6d008c23":"sub = pd.DataFrame()\nsub[\"PassengerId\"] = test_id\nsub[\"Survived\"] = ensemble_pred.astype(\"int64\")\nsub.to_csv(\"Submission.csv\",index = False)","1ab0147b":"# Modeling","64af04f9":"# Data Exploration","e8f9b47a":"# Feature Engineering"}}