{"cell_type":{"b48d77e4":"code","f4d7f6c8":"code","d9fb59e7":"code","95c78fd1":"code","0c59b1d6":"code","dae72939":"code","0ad08c97":"code","53e91076":"code","366d48d6":"code","ec3f12f2":"code","89694604":"code","b5eaad95":"code","e33627cc":"code","c4111289":"code","688f976a":"code","86b23a0e":"code","ddff0f50":"markdown","9b4ea90c":"markdown","a7b6d540":"markdown","658602ef":"markdown","bda0df78":"markdown","7c9e04ec":"markdown","2fe6cbff":"markdown","983aa139":"markdown","5bc23fd3":"markdown","c6efe367":"markdown","dd6d70df":"markdown","1589a808":"markdown"},"source":{"b48d77e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f4d7f6c8":"import pandas as pd\n\n\n\ndf = pd.read_csv('\/kaggle\/input\/marvel-vs-dc-imdb-dataset\/Marvel_DC_imdb.csv')\n\ndel df['Unnamed: 0']\nprint(df.head())\ndf.describe()","d9fb59e7":"import seaborn as sns\n\nsns.heatmap(df.isnull())","95c78fd1":"df['MvsDC'] = df['Category'] == 'Marvel'\nprint(df.head())","0c59b1d6":"df_marvel = df[df['MvsDC']]\ndf_DC = df[df['MvsDC'] == False]\ndf_marvel['IMDB_Score'].dropna()\ndf_DC['IMDB_Score'].dropna()\nprint(df_DC.head())","dae72939":"df_marvel.info()","0ad08c97":"df_DC.info()","53e91076":"print(df_marvel.describe())\nprint(df_DC.describe())","366d48d6":"import spacy \nfrom math import floor\n\nnlp = spacy.load(\"en_core_web_sm\")","ec3f12f2":"df.dropna(inplace=True,subset=['IMDB_Score'])\n\ndescr = [nlp(doc) for doc in  df['Description'] ]\nvotes =[]\nscore_keys=['0','1','2','3','5','6','7','8','9']\n#for imdb in df['IMDB_Score']:\n#    votes.append({ sc: floor(imdb) == int(sc) }for sc in score_keys  )\nvotes = [{'0': floor(imdb) == 0,'1': floor(imdb) == 1,'2': floor(imdb) == 2,'3': floor(imdb) == 3,\n         '4': floor(imdb) == 4,'5': floor(imdb) == 5,'6': floor(imdb) == 6,'7': floor(imdb) == 7,\n         '8': floor(imdb) == 8,'9': floor(imdb) == 9} for imdb in df['IMDB_Score']]\n    \nvotes  = [{\"cats\": labels} for labels in votes]\n    \n\nfor ent in descr[30].ents:\n    print(ent.text)\n    \nprint(votes[30])","89694604":"from sklearn.model_selection import train_test_split\n\n\nX = descr\nylabels = votes\n\nX_train, X_test,y_train,y_test = train_test_split(X,ylabels,test_size=0.001)\n\n#y_train = np.array(y_train)\n#y_oh = OneHotEncoder(sparse=False)\n#y_oh = y_oh.fit_transform(y_train.reshape(len(y_train),1))\n\nX_sp = [(X_train[i],y_train[i]) for i in range(len(y_train))]\nprint(X_sp[:1])","b5eaad95":"from spacy.util import minibatch\n\n\n#textcat = nlp.create_pipe(\n#              \"textcat_multilabel\")\nn_iter = 10\ntextcat = nlp.create_pipe('textcat',\n              config={\n                \"exclusive_classes\": True,\n                \"architecture\": \"ensemble\"})\n\n\n\n\ntextcat.add_label(\"0\")\ntextcat.add_label(\"1\")\ntextcat.add_label(\"2\")\ntextcat.add_label(\"3\")\ntextcat.add_label(\"4\")\ntextcat.add_label(\"5\")\ntextcat.add_label(\"6\")\ntextcat.add_label(\"7\")\ntextcat.add_label(\"8\")\ntextcat.add_label(\"9\")\n\n\nnlp.add_pipe(textcat)\n\ntextcat.labels\n","e33627cc":"import random\n\ndef train(model, train_data, optimizer):\n    losses = {}\n    random.seed(1)\n    random.shuffle(train_data)\n    \n    batches = minibatch(train_data, size=8)\n    for batch in batches:\n        # train_data is a list of tuples [(text0, label0), (text1, label1), ...]\n        # Split batch into texts and labels\n        texts, labels = zip(*batch)\n        \n        # Update model with texts and labels\n        model.update(texts,labels,sgd=optimizer,losses=losses)\n        \n    return losses","c4111289":"optimizer = nlp.begin_training()\ntrain_data = X_sp\nn_iter =5\nfor i in range(n_iter):\n    losses = train(nlp, train_data, optimizer)\n    print(losses['textcat'])","688f976a":"def predict(nlp, docs): \n    # Use the model's tokenizer to tokenize each input text\n    #docs = [nlp.tokenizer(text) for text in texts]\n    \n    # Use textcat to get the scores for each doc\n    textcat= nlp.get_pipe('textcat')\n    scores,_ = textcat.predict(docs)\n    print(scores)\n    \n    # From the scores, find the class with the highest score\/probability\n    predicted_class = scores.argmax(axis=1)\n    \n    return predicted_class\n\ndef find_key(input_dict, value):\n    return next((k for k, v in input_dict.items() if v == value), None)\n\ntexts =X_test\ntrue_sc =[]\nfor y in y_test:\n    # list out keys and values separately\n    true_sc.append(int(find_key(y['cats'],True)))\n\npredictions = predict(nlp, texts)\ntrue_pred = np.array(predictions == true_sc)\n#print(predictions)\n#print(true_sc)\n#print(true_pred)\nacc = sum(true_pred)\/len(true_sc)\n\n \nfor p, t,sc in zip(predictions, texts,true_sc):\n    print(f\"{textcat.labels[p]}: {t} , true scores: {sc}  \\n\")\n    \nprint('Accuracy = ', acc)    ","86b23a0e":"desc_inv = 'Green lantern spends the whole day doing taxes calculations.'\n\ndocs_inv = [nlp.tokenizer(desc_inv)]\n\nprint(docs_inv)\n\npred_inv= predict(nlp,docs_inv)\n\nprint(pred_inv)","ddff0f50":"There are a lot of nans. We can visualize them easily.","9b4ea90c":"We then execute the training.","a7b6d540":"The only numeric features are IMBD scores and metascores. I will create another one to distinguish Marvel and DC in a boolean way.","658602ef":"We prepare the model for training.","bda0df78":"The model seems to work very well!\n\nAn alternative would be to use vector embeddings, which we have already thanks to spacy processing, and use sklearn with a support vector machine.\n\nNow we are ready to give votes to invented superhero movies description.","7c9e04ec":"We have a trained model capable of guessing the votes of a superhero movie given its description. lol","2fe6cbff":"nennenWe need to provide the data in the correct fashion. For a multiclassification problem i will have ten classes, from 0 to 9, which are the floor values of the imdb scores. They will be encoded as one-hot dictionaries in votes.","983aa139":"Here we create the data in a way that spacy likes. A tuple with strings and labels.","5bc23fd3":"# Data import and first analysis\n \nLet's start by import the data and give it a look.","c6efe367":"We can see the average for the two numerical features of the movies. In particular we see that the IMDB score for DC is higher than marvel, while it is the opposite for the metascore.","dd6d70df":"# Score classification by description\n\nWe go for a kind of crazy idea. We want to understand if a superhero movie is good only through its description. We will use the spacy library for this, as feature the description column, and as target the IMDB score. ","1589a808":"We define the training function. "}}