{"cell_type":{"d4fe8a1e":"code","5410962d":"code","0fc8ba71":"code","63c4cb07":"code","a92a1935":"code","69932416":"code","ffbef487":"code","0232f689":"code","5cda7c08":"code","aac79b42":"code","26dab837":"code","224dba1a":"markdown","7c492c15":"markdown","ac181ced":"markdown","21ebeac8":"markdown","2f721363":"markdown","57099320":"markdown","f1755c88":"markdown","b1489b07":"markdown","c37b63a3":"markdown"},"source":{"d4fe8a1e":"import gc\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm  \nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\nimport tensorflow.keras\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.models import Sequential, Model, load_model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.losses import mean_squared_error as mse_loss\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.model_selection import StratifiedKFold\n","5410962d":"def fill_weather_dataset(weather_df, mode_type):\n    \n    weather_df.loc[:,'timestamp'] = weather_df['timestamp'].astype(str)\n    # Find Missing Dates\n    time_format = \"%Y-%m-%d %H:%M:%S\"\n    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n    total_hours = int(((end_date - start_date).total_seconds() + 3600) \/ 3600)\n    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n\n    for site_id in range(16):\n        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n        new_rows['site_id'] = site_id\n        weather_df = pd.concat([weather_df,new_rows])\n\n        weather_df = weather_df.reset_index(drop=True) \n\n         \n\n    # Add new Features\n    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n    weather_df[\"hour\"] = weather_df[\"datetime\"].dt.hour\n    weather_df[\"weekday\"] = weather_df[\"datetime\"].dt.weekday\n#    \n    #Use IterativeImputer to fill missing value \n#    df_weather_timestamp = weather_df.timestamp\n#    weather_df = weather_df.drop(['timestamp','datetime'],axis=1)\n#    imp = IterativeImputer(max_iter=20, random_state=0)\n#    df_weather_train_np = imp.fit_transform(weather_df)\n#    weather_df = pd.DataFrame(df_weather_train_np, columns=weather_df.columns)\n#    weather_df.loc[:,'timestamp'] = df_weather_timestamp\n    \n    # Reset Index for Fast Update\n    weather_df = weather_df.set_index(['site_id','day','month'])\n    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n    air_temperature_filler = air_temperature_filler.fillna(method='ffill')\n    weather_df.update(air_temperature_filler,overwrite=False)\n\n    # Step 1\n    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n    # Step 2\n    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n\n    weather_df.update(cloud_coverage_filler,overwrite=False)\n\n    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n    due_temperature_filler = pd.DataFrame(due_temperature_filler.fillna(method='ffill'),columns=[\"dew_temperature\"])\n    weather_df.update(due_temperature_filler,overwrite=False)\n\n    # Step 1\n    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n    # Step 2\n    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n\n    weather_df.update(sea_level_filler,overwrite=False)\n\n    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n    wind_direction_filler =  pd.DataFrame(wind_direction_filler.fillna(method='ffill'),columns=[\"wind_direction\"])\n    weather_df.update(wind_direction_filler,overwrite=False)\n\n    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n    wind_speed_filler =  pd.DataFrame(wind_speed_filler.fillna(method='ffill'),columns=[\"wind_speed\"])\n    weather_df.update(wind_speed_filler,overwrite=False)\n     \n\n    # Step 1\n    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n    # Step 2\n    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n    weather_df.update(precip_depth_filler,overwrite=False)\n    if mode_type == 'Dnn':\n        holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n                    \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n                    \"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n                    \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n                    \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n                    \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n                    \"2019-01-01\"] \n        weather_df[\"is_holiday\"] = (weather_df.datetime.dt.date.astype(\"str\").isin(holidays)).astype(int)\n        \n        beaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n        for item in beaufort:\n            weather_df.loc[(weather_df['wind_speed']>=item[1]) & (weather_df['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n\n\n \n    weather_df = weather_df.drop(['offset','datetime'],axis=1) \n    weather_df = weather_df.reset_index()     \n\n\n    return weather_df\n","0fc8ba71":"# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","63c4cb07":"def data_building_processing(df_data):\n    '''===========Building data processing======================'''\n    print('Processing building data...')\n\n    lbl = LabelEncoder() \n    lbl.fit(list(df_data['primary_use'].values)) \n    df_data['primary_use'] = lbl.transform(list(df_data['primary_use'].values))\n    imp = IterativeImputer(max_iter=30, random_state=0)\n    df_build = imp.fit_transform(df_data)\n    df_data = pd.DataFrame(df_build, columns=df_data.columns)\n    df_data.loc[:,'floor_count'] = df_data['floor_count'].apply(int)\n    df_data.loc[:,'year_built'] = df_data['year_built'].apply(int)  \n\n#    df_data['year_built_1920'] = df_data['year_built'].apply(lambda x: 1 if x<1920 else 0 )\n#    df_data['year_built_1920_1950'] = df_data['year_built'].apply(lambda x: 1 if 1920<=x & x<1950 else 0 )\n#    df_data['year_built_1950_1970'] = df_data['year_built'].apply(lambda x: 1 if 1950<=x & x<1970 else 0 )\n#    df_data['year_built_1970_2000'] = df_data['year_built'].apply(lambda x: 1 if 1970<=x & x<2000 else 0 )\n#    df_data['year_built_2000'] = df_data['year_built'].apply(lambda x: 1 if x>=2000 else 0 )\n    return df_data","a92a1935":"def features_engineering(df, mode_type):\n    \n\n    classify_columns = ['building_id','meter','site_id','primary_use',\n                        'hour','weekday'] \n    # Sort by timestamp\n    df.sort_values(\"timestamp\")\n    df.reset_index(drop=True)\n    \n    # Add more features\n    df['square_feet'] =  np.log1p(df['square_feet'])\n    \n    drop = [\"timestamp\",\"sea_level_pressure\", \"wind_direction\", \"wind_speed\",]\n    df = df.drop(drop, axis=1)\n    gc.collect()\n    \n    # Encode Categorical Data\n    for i in tqdm(classify_columns):\n        le = LabelEncoder()\n        df[i] = le.fit_transform(df[i])\n        \n    if mode_type == 'Dnn':\n        numericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n              \"dew_temperature\", \"precip_depth_1_hr\", \"floor_count\", 'beaufort_scale']\n        print('Start working with numerical characteristics...')\n        for i in tqdm(numericals):\n            ss_X=StandardScaler() \n            df.loc[:,i] = ss_X.fit_transform(df[i].values.reshape(-1, 1))    \n    \n    return df","69932416":"def data_pro(df_data, df_weather_train, df_weather_test, df_building, data_type='train',mode_type='lgb'):\n    ## REducing memory\n    df_data = reduce_mem_usage(df_data,use_float16=True)\n    df_building = reduce_mem_usage(df_building,use_float16=True)\n    '''===Align local timestamps===='''\n    weather = pd.concat([df_weather_train,df_weather_test],ignore_index=True)\n    weather_key = ['site_id', 'timestamp']\n    temp_skeleton = weather[weather_key + ['air_temperature']].drop_duplicates(subset=weather_key).sort_values(by=weather_key).copy()\n    # calculate ranks of hourly temperatures within date\/site_id chunks\n    temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n    # create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)\n    df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n    # Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n    site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\n    site_ids_offsets.index.name = 'site_id'\n    \n    def timestamp_align(df):\n        df['offset'] = df.site_id.map(site_ids_offsets)\n        df['timestamp_aligned'] = (df.timestamp - pd.to_timedelta(df.offset, unit='H'))\n        df['timestamp'] = df['timestamp_aligned']\n        del df['timestamp_aligned']\n        return df\n \n    if data_type == 'test':\n        print(\"Test data detected...\")\n        df_weather_test = timestamp_align(df_weather_test)\n        df_weather_test = fill_weather_dataset(df_weather_test, mode_type)\n        df_weather_test = reduce_mem_usage(df_weather_test,use_float16=True)\n        #merge\n        df_building = data_building_processing(df_building)\n        df_data = pd.merge(df_data, df_building, on='building_id', how='left')\n        df_data = df_data.merge(df_weather_test,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])\n    \n        print(\"Start feature processing...\")    \n        df_data = features_engineering(df_data, mode_type)\n        gc.collect()\n        \n        return df_data\n    \n    elif data_type == 'train':\n        print(\"Train data detected...\")\n        df_weather_train = timestamp_align(df_weather_train)\n        df_weather_train = fill_weather_dataset(df_weather_train, mode_type)\n        df_weather_train = reduce_mem_usage(df_weather_train,use_float16=True)\n        #merge\n        \n        df_building = data_building_processing(df_building)\n        df_data = df_data.merge(df_building, left_on='building_id',right_on='building_id',how='left')\n        df_data = df_data.merge(df_weather_train,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])\n        target_data = df_data['meter_reading']\n        df_data = df_data.drop('meter_reading',axis=1)\n        print(\"Start feature processing...\")\n        \n        df_data = features_engineering(df_data, mode_type)\n        gc.collect()\n\n        return df_data, target_data","ffbef487":"def cal_rmsle(Ytrue, Yfit):\n    rmsle = K.sqrt(K.mean(K.square(Ytrue - Yfit), axis=0))\n    return (rmsle) \n\ndef plot_history(network_history):\n    plt.figure()\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.plot(network_history.history['loss'])\n    plt.plot(network_history.history['val_loss'])\n    plt.legend(['Training', 'Validation']) \n    plt.figure()\n    plt.xlabel('Epochs')\n    plt.ylabel('rmsle')\n    plt.plot(network_history.history['cal_rmsle'])\n    plt.plot(network_history.history['val_cal_rmsle'])\n    plt.legend(['Training', 'Validation'], loc='lower right')\n    plt.show()\n    \ndef get_keras_data(df, num_cols, cat_cols):\n    cols = num_cols + cat_cols\n    X = {col: np.array(df[col]) for col in cols}\n    return X\n\ndef train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold, model_save_path, patience=3):\n    early_stopping = EarlyStopping(patience=patience, verbose=1)\n    model_checkpoint = ModelCheckpoint(model_save_path+\"model_\" + str(fold) + \".hdf5\",\n                                       save_best_only=True, verbose=1, monitor='val_loss', mode='min')\n\n    history = keras_model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs,\n                            validation_data=(X_v, y_valid), verbose=1,\n                            callbacks=[early_stopping, model_checkpoint])\n    #draw \n    plot_history(history)\n    keras_model = load_model(model_save_path+\"model_\" + str(fold) + \".hdf5\", \n                             custom_objects={'cal_rmsle': cal_rmsle,})\n    \n    return keras_model","0232f689":"train_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/train.csv')\ntrain_df = train_df[train_df['building_id'] != 1099 ]\ntrain_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\nbuilding_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/building_metadata.csv')\nweather_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_train.csv\",parse_dates=[\"timestamp\"],)\ndf_weather_test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_test.csv\",parse_dates=[\"timestamp\"],)\ntrain_data,target_data = data_pro(train_df, weather_df, df_weather_test, building_df, data_type='train', mode_type='Dnn')\ntarget_data = np.log1p(target_data)","5cda7c08":"def model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \ndropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001):\n\n    #Inputs\n    site_id = Input(shape=[1], name=\"site_id\")\n    building_id = Input(shape=[1], name=\"building_id\")\n    meter = Input(shape=[1], name=\"meter\")\n    primary_use = Input(shape=[1], name=\"primary_use\")\n    square_feet = Input(shape=[1], name=\"square_feet\")\n    year_built = Input(shape=[1], name=\"year_built\")\n    air_temperature = Input(shape=[1], name=\"air_temperature\")\n    cloud_coverage = Input(shape=[1], name=\"cloud_coverage\")\n    dew_temperature = Input(shape=[1], name=\"dew_temperature\")\n    hour = Input(shape=[1], name=\"hour\")\n    precip = Input(shape=[1], name=\"precip_depth_1_hr\")\n    weekday = Input(shape=[1], name=\"weekday\")\n    beaufort_scale = Input(shape=[1], name=\"beaufort_scale\")\n    day = Input(shape=[1], name=\"day\")\n    month = Input(shape=[1], name=\"month\")\n    week = Input(shape=[1], name=\"week\")\n    is_holiday = Input(shape=[1], name=\"is_holiday\")\n    floor_count = Input(shape=[1], name=\"floor_count\")\n   \n    #Embeddings layers\n    emb_site_id = Embedding(16, 2,name=\"emb_site_id\")(site_id)\n    emb_building_id = Embedding(1449, 6, name=\"emb_building_id\")(building_id)\n    emb_meter = Embedding(4, 2, name=\"emb_meter\")(meter)\n    emb_primary_use = Embedding(16, 2, name=\"emb_primary_use\")(primary_use)\n    emb_hour = Embedding(24, 3, name=\"emb_hour\")(hour)\n    emb_weekday = Embedding(7, 2, name=\"emb_weekday\")(weekday)\n    emb_month = Embedding(13, 2, name=\"emb_month\")(month)\n\n    concat_emb = concatenate([\n           Flatten() (emb_site_id)\n         , Flatten() (emb_building_id)\n         , Flatten() (emb_meter)\n         , Flatten() (emb_primary_use)\n         , Flatten() (emb_hour)\n         , Flatten() (emb_weekday)\n         , Flatten() (emb_month)\n    ])\n    \n    categ = Dropout(dropout1)(Dense(dense_dim_1,activation='relu') (concat_emb))\n    categ = BatchNormalization()(categ)\n    categ = Dropout(dropout2)(Dense(dense_dim_2,activation='relu') (categ))\n    \n    #main layer\n    main_l = concatenate([\n          categ\n        , square_feet\n        , year_built\n        , air_temperature\n        , cloud_coverage\n        , dew_temperature\n        , precip\n        , beaufort_scale\n        ,day\n        ,week\n        ,is_holiday\n        ,floor_count\n    ])\n    \n    main_l = Dropout(dropout3)(Dense(dense_dim_3,activation='relu') (main_l))\n    main_l = BatchNormalization()(main_l)\n    main_l = Dropout(dropout4)(Dense(dense_dim_4,activation='relu') (main_l))\n    \n    #output\n    output = Dense(1) (main_l)\n\n    model = Model([ site_id,\n                    building_id, \n                    meter, \n                    primary_use, \n                    square_feet, \n                    year_built, \n                    air_temperature,\n                    cloud_coverage,\n                    dew_temperature, \n                    hour,\n                    weekday, \n                    precip,\n                    beaufort_scale,\n                    day,\n                    month,\n                    week,\n                    is_holiday,\n                    floor_count], output)\n\n    model.compile(optimizer = optimizers.Adam(lr=lr),\n                  loss= mse_loss,\n                  metrics=[cal_rmsle])\n    return model","aac79b42":"\nmodels = []\nlr=0.001\nbatch_size = 1024\nepochs = 4\nfolds = 2\nseed = 2019\nmodel_save_path = \"..\/\"\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\nnumericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n              \"dew_temperature\", \"precip_depth_1_hr\", \"floor_count\", 'beaufort_scale']\ncategoricals = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"weekday\",  \"meter\",'day','month','week','is_holiday',]\nall_unique = dict(zip([i for i in categoricals],[train_data[i].unique() for i in categoricals]))\n\nfor fold_n, (train_index, valid_index) in enumerate(kf.split(train_data, train_data['building_id'])):\n    print('Fold:', fold_n) \n    X_train, X_valid = train_data.iloc[train_index], train_data.iloc[valid_index]\n    y_train, y_valid = target_data.iloc[train_index], target_data.iloc[valid_index]\n    X_t = get_keras_data(X_train , numericals, categoricals)\n    X_v = get_keras_data(X_valid, numericals, categoricals)\n    keras_model = model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \n                        dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=lr)\n    mod = train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n, model_save_path, patience=3)\n    models.append(mod)\n    print('*'* 50)\n    del(X_train, X_valid, y_train, y_valid, X_t, X_v)\n    gc.collect()\n    \ndel(train_df, weather_df, df_weather_test, building_df)","26dab837":"#Load test data\n#df_test = pd.read_csv('..\/input\/ashrae-energy-prediction\/test.csv')\n#row_ids = df_test[\"row_id\"]\n#df_test.drop(\"row_id\", axis=1, inplace=True)\n#building_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/building_metadata.csv')\n#weather_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_train.csv\",parse_dates=[\"timestamp\"],)\n#df_weather_test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_test.csv\",parse_dates=[\"timestamp\"],)\n#df_test = data_pro(df_test, weather_df, df_weather_test, building_df, data_type='test', mode_type='Dnn')\n#df_test.loc[df_test['beaufort_scale'] == 9,'beaufort_scale'] = 8\n\n\n#i=0\n#target_tests = np.zeros((df_test.shape[0]),dtype=np.float32)\n#step_size = 5000\n#for j in tqdm(range(int(np.ceil(df_test.shape[0]\/step_size)))):\n#    for_prediction = get_keras_data(df_test.iloc[i:i+step_size],numericals, categoricals)\n#    target_tests[i:min(i+step_size,df_test.shape[0])] = \\\n#        np.expm1(sum([model.predict(for_prediction, batch_size=1024)[:,0] for model in models])\/len(models))\n#    i+=step_size\n\n#results_df = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": np.clip(target_tests, 0, a_max=None)})\n#results_df['meter_reading'] = results_df['meter_reading'].round(4)\n#del row_ids,target_tests\n#gc.collect()\n#results_df.to_csv(\"submission.csv\", index=False)","224dba1a":"Training model","7c492c15":"Building Model:the network structure refers to this https:\/\/www.kaggle.com\/isaienkov\/keras-nn-with-embeddings-for-cat-features-1-15","ac181ced":"DNN submission","21ebeac8":"**Data generator**\n\nWeather data processing function","2f721363":"Collect data for feature engineering","57099320":"Reduce memory function","f1755c88":"Building data processing function","b1489b07":"Main Processingfunction","c37b63a3":"Define some functions"}}