{"cell_type":{"9e38f94e":"code","0b1ffae8":"code","f2239aa1":"code","f6820d85":"code","fd96a974":"markdown","faed0b25":"markdown","e33370e1":"markdown","3ad61117":"markdown"},"source":{"9e38f94e":"import pandas as pd\nimport numpy as np\n\n\ndef load_data_v2(para_train_path, para_test_path):\n\n    temp_train_data = pd.read_csv(para_train_path)\n    temp_test_data = pd.read_csv(para_test_path)\n    # \u8fde\u63a5\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u7684\u6240\u6709\u6837\u672c\n    # \u7b2c\u4e00\u5217\u662f\u5e8f\u53f7\uff0c\u8bad\u7ec3\u96c6\u7684\u6700\u540e\u4e00\u5217\u662f\u6807\u7b7e\n    test_id = np.array(temp_test_data['Id'])\n    temp_all_features = pd.concat((temp_train_data.iloc[:, 1:-1], temp_test_data.iloc[:, 1:]))\n#     print(temp_all_features.shape)\n    \n    # \u83b7\u53d6\u6570\u503c\u578b\u6570\u636e\u7684\u7d22\u5f15\n    temp_numeric_features_idx = temp_all_features.dtypes[temp_all_features.dtypes != 'object'].index\n    # \u83b7\u53d6\u5b57\u7b26\u578b\u6570\u636e\u7684\u7d22\u5f15\n    temp_str_features_idx = temp_all_features.dtypes[temp_all_features.dtypes == 'object'].index\n    \n    # \u6570\u503c\u578b\u7279\u5f81\u5904\u7406\uff1a\n    # \u6807\u51c6\u5316\u6570\u503c\u578b\u6570\u636e\n    temp_all_features[temp_numeric_features_idx] = temp_all_features[temp_numeric_features_idx].apply(lambda x: (x - x.mean()) \/ x.std())\n    num_features = temp_all_features[temp_numeric_features_idx]\n#     print(num_features.shape)\n    # \u6807\u51c6\u5316\u540e\uff0c\u53ef\u4ee5\u4f7f\u75280\u6765\u4ee3\u66ff\u7f3a\u5931\u503c\n    num_features = num_features.fillna(0)\n#     print('\u6570\u503c\u578b\u7279\u5f81\u4e2d\u6709' + repr(sum(num_features.isnull().any())) + '\u4e2a\u7279\u5f81\u5b58\u5728\u7a7a\u503c')\n    \n    # \u5b57\u7b26\u578b\u7279\u5f81\u5904\u7406\uff1a\n    # \u8fdb\u884c\u7f16\u7801\n    str_features = temp_all_features[temp_str_features_idx]\n#     print(str_features.shape)\n#     print('\u5b57\u7b26\u578b\u578b\u7279\u5f81\u4e2d\u6709' + repr(sum(str_features.isnull().any())) + '\u4e2a\u7279\u5f81\u5b58\u5728\u7a7a\u503c')\n    str_fe_isnull = str_features.isnull().any()\n    str_fe_index = str_features.isnull().any().index\n#     print(str_fe_isnull)\n#     print(str_fe_index)\n    str_fe_isnull_index = []\n    for i in range(len(str_fe_isnull)):\n        if str_fe_isnull[i] == True:\n            str_fe_isnull_index.append(str_fe_index[i])\n#     print(str_fe_isnull_index)\n    # \u5904\u7406\u6240\u6709\u5b57\u7b26\u578b\u542b\u7a7a\u503c\u7279\u5f81\n    for fe_name in str_fe_isnull_index:\n        str_features[fe_name] = str_features[fe_name].fillna('none')\n#     print('\u5b57\u7b26\u578b\u578b\u7279\u5f81\u4e2d\u6709' + repr(sum(str_features.isnull().any())) + '\u4e2a\u7279\u5f81\u5b58\u5728\u7a7a\u503c')\n    # \u5bf9\u6240\u6709\u5b57\u7b26\u578b\u7279\u5f81\u8fdb\u884c\u7f16\u7801\u64cd\u4f5c\n    str_features = pd.get_dummies(str_features)\n    # \u62fc\u63a5\u6570\u503c\u578b\u7279\u5f81\u4e0e\u5b57\u7b26\u578b\u7279\u5f81\n    new_features = pd.concat((num_features, str_features), axis=1)\n#     print(new_features.shape)\n#     new_features.to_csv('new_features_v2.csv', index=False)\n\n    # \u4e3b\u52a8\u65b0\u589e\u7279\u5f81\n    new_features['TotalSF'] = new_features['TotalBsmtSF'] + new_features['1stFlrSF'] + new_features['2ndFlrSF']\n    new_features['Total_sqr_footage'] = (new_features['BsmtFinSF1'] + new_features['BsmtFinSF2'] + new_features['1stFlrSF'] + new_features['2ndFlrSF'])\n    new_features['Total_Bathrooms'] = (new_features['FullBath'] + (0.5 * new_features['HalfBath']) + new_features['BsmtFullBath'] + (0.5 * new_features['BsmtHalfBath']))\n    \n    # \u6570\u636e\u5212\u5206\n    temp_num_train = len(temp_train_data)\n    ret_train_data = np.array(new_features[:temp_num_train].values).astype('float32')\n    ret_test_data = np.array(new_features[temp_num_train:].values).astype('float32')\n    # \u4fee\u6539\u56de\u5f52\u76ee\u6807\u7684\u6574\u4f53\u504f\u5ea6\n    ret_train_label = np.log((temp_train_data.values[:, -1]).astype('float32').reshape(-1, 1))\n\n    return ret_train_data, ret_train_label, ret_test_data, test_id\n\n# \u539f\u59cb\u7248\u672c\u65b9\u6cd5\uff0c\u4f1a\u65b0\u589e\u591a\u4f59\u7279\u5f81\ndef load_data_v1(para_train_path, para_test_path):\n\n    temp_train_data = pd.read_csv(para_train_path)\n    temp_test_data = pd.read_csv(para_test_path)\n    # \u8fde\u63a5\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u7684\u6240\u6709\u6837\u672c\n    # \u7b2c\u4e00\u5217\u662f\u5e8f\u53f7\uff0c\u8bad\u7ec3\u96c6\u7684\u6700\u540e\u4e00\u5217\u662f\u6807\u7b7e\n    test_id = np.array(temp_test_data['Id'])\n    temp_all_features = pd.concat((temp_train_data.iloc[:, 1:-1], temp_test_data.iloc[:, 1:]))\n\n    # \u83b7\u53d6\u6570\u503c\u578b\u6570\u636e\u7684\u7d22\u5f15\n    temp_numeric_features_idx = temp_all_features.dtypes[temp_all_features.dtypes != 'object'].index\n    # \u6807\u51c6\u5316\u6570\u636e\n    temp_all_features[temp_numeric_features_idx] = temp_all_features[temp_numeric_features_idx].apply(\n        lambda x: (x - x.mean()) \/ x.std())\n    # \u6807\u51c6\u5316\u540e\uff0c\u53ef\u4ee5\u4f7f\u75280\u6765\u4ee3\u66ff\u7f3a\u5931\u503c\n    temp_all_features = temp_all_features.fillna(0)\n    # \u79bb\u6563\u503c\u5904\u7406\uff1a\n    # \u4f8b\u5982\u67d0\u7279\u5f81\u6709\u4e24\u4e2a\u4e0d\u540c\u7684\u79bb\u6563\u503c\uff0c\u5219\u8be5\u5c5e\u6027\u5c06\u88ab\u5904\u7406\u4e3a\u4e8c\u7ef4\uff1a0 1 \u6216\u8005 1 0\n    # \u4e09\u4e2d\u4e0d\u540c\u7684\u79bb\u6563\u503c\u65f6\uff0c\u5219\u5bf9\u4e8e 0 0 1\u3001 0 1 0 \u4ee5\u53ca 1 0 0\uff0c\u4ee5\u6b64\u7c7b\u63a8\n    # \u6b64\u5904\u9009\u62e9\u4e86dummy_na=True\uff0c\u4f1a\u5bfc\u81f4\u65b0\u589e\u591a\u5217\u5168\u96f6\u7279\u5f81\n    temp_all_features = pd.get_dummies(temp_all_features, dummy_na=True)\n    temp_all_features.to_csv('new_features_v1.csv', index=False)\n\n    # \u6570\u636e\u5212\u5206\n    temp_num_train = len(temp_train_data)\n    ret_train_data = np.array(temp_all_features[:temp_num_train].values).astype('float32')\n    ret_test_data = np.array(temp_all_features[temp_num_train:].values).astype('float32')\n    ret_train_label = np.log((temp_train_data.values[:, -1]).astype('float32').reshape(-1, 1))\n\n    return ret_train_data, ret_train_label, ret_test_data, test_id","0b1ffae8":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.liner_in = nn.Linear(314, 32)\n        self.liner_mid = nn.Linear(32, 32)\n        self.liner_out = nn.Linear(32, 1)\n\n    def forward(self, input):\n        x = F.relu(self.liner_in(input))\n        x = F.relu(self.liner_mid(x))\n        output = F.relu(self.liner_out(x))\n        return output\n        \n\n# \u5b9a\u4e49\u6b63\u786e\u7387\u8ba1\u7b97\u65b9\u6cd5,R^2\u6cd5\ndef accuracy(y_pred, y):\n    return 1 - ((((y_pred - y)**2).mean())\/y.var())\n\n\ndef torch_trainer(dataset, epoches, batch_size, learning_rate):\n    # \u8d85\u53c2\u6570\u5b9a\u4e49\n    model = MLP()\n    loss_fn = nn.MSELoss()\n    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    # \u5c01\u88c5dataloader\n    train_X = torch.from_numpy(dataset[0])\n    train_Y = torch.from_numpy(dataset[1])\n    test_X = torch.from_numpy(dataset[2])\n    test_Y = torch.from_numpy(dataset[3])\n    train_ds = TensorDataset(train_X, train_Y)\n    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    test_ds = TensorDataset(test_X, test_Y)\n    test_dl = DataLoader(test_ds, batch_size=batch_size)\n\n    acc_train_list = []\n    loss_train_list = []\n    acc_test_list = []\n    loss_test_list = []\n    best_acc = 0\n    \n    # \u8bad\u7ec3\u5faa\u73af\n    with tqdm(total=epoches, desc='\u6b63\u5728\u8bad\u7ec3', postfix=dict) as pbar:\n        for epoch in range(epoches):\n            for x, y in train_dl:\n                y_pred = model(x)\n                loss = loss_fn(y_pred, y)\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n            with torch.no_grad():\n                acc_train = round(accuracy(model(train_X), train_Y).item(), 3)\n                loss_train = round(loss_fn(model(train_X), train_Y).data.item(), 3)\n                acc_test = round(accuracy(model(test_X), test_Y).item(), 3)\n                loss_test = round(loss_fn(model(test_X), test_Y).data.item(), 3)\n                acc_train_list.append(acc_train)\n                loss_train_list.append(loss_train)\n                acc_test_list.append(acc_test)\n                loss_test_list.append(loss_test)\n                if acc_test > best_acc:\n                    best_acc = acc_test\n                pbar.set_postfix(**{'\u8bad\u7ec3\u96c6\u6b63\u786e\u7387': acc_train,\n                                    '\u8bad\u7ec3\u96c6\u635f\u5931': loss_train,\n                                    '\u9a8c\u8bc1\u96c6\u6b63\u786e\u7387': acc_test,\n                                    '\u9a8c\u8bc1\u96c6\u635f\u5931': loss_test,\n                                    '\u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u6700\u9ad8\u503c':best_acc\n                                    })\n                pbar.update()\n\n    # \u7ed8\u5236\u8bad\u7ec3\u8fc7\u7a0b\n    x_arr = range(1, epoches + 1)\n    plt.plot(x_arr, acc_train_list, label='acc_train', linewidth=1, color='r')\n    plt.plot(x_arr, loss_train_list, label='loss_train', linewidth=1, color='b')\n    plt.plot(x_arr, acc_test_list, label='acc_test', linewidth=1, color='y')\n    plt.plot(x_arr, loss_test_list, label='loss_test', linewidth=1, color='g')\n    plt.ylim(0,1)\n    plt.xlabel('epoch')\n    plt.ylabel('values')\n    plt.legend()\n    plt.show()\n\n    return model","f2239aa1":"train_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntrain_data, train_label, test_data, test_id = load_data_v2(train_path, test_path)\n# print(train_data.shape)\n# print(train_label.shape)\n# print(test_data.shape)\n# \u4ece\u8bad\u7ec3\u96c6\u5206\u5272\u5b50\u8bad\u7ec3\u96c6\u4e0e\u5b50\u9a8c\u8bc1\u96c6\nnum_subset = int(len(train_data)*0.7)\nsubset_train_data = train_data[:num_subset]\nsubset_train_label = train_label[:num_subset]\nsubset_test_data = train_data[num_subset:]\nsubset_test_label = train_label[num_subset:]\nprint(subset_train_data.shape)\nprint(subset_train_label.shape)\nprint(subset_test_data.shape)\nprint(subset_test_label.shape)\ndataset = [train_data, train_label, subset_test_data, subset_test_label]\nmodel = torch_trainer(dataset, epoches=5000, batch_size=200, learning_rate=0.00001)","f6820d85":"model_output = model(torch.from_numpy(test_data))\nwith torch.no_grad():\n    output = pd.DataFrame({'Id': test_id, 'SalePrice': np.exp(np.array(model_output).reshape(-1))})\n    output.to_csv('submission.csv', index=False)\n    print(output)","fd96a974":"# \u591a\u5c42\u611f\u77e5\u673a\u5b9a\u4e49","faed0b25":"# \u8bfb\u53d6\u6570\u636e\u5e76\u9884\u5904\u7406","e33370e1":"# \u8bad\u7ec3","3ad61117":"# \u4fdd\u5b58\u7ed3\u679c"}}