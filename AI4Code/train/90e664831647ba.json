{"cell_type":{"8580cfe9":"code","ad27ba96":"code","be400372":"code","c4ab04e5":"code","b364d84d":"code","6bcaa450":"code","aa67e5b2":"code","7a8e5b86":"code","331f522f":"code","124898ef":"code","398687cb":"code","ac802b1f":"code","aa6890eb":"code","f13f7f3c":"code","2dc5e473":"code","bc7cf93c":"code","9d51ef21":"code","5a1f0eb1":"code","47db7712":"code","e15a452a":"code","115f67e8":"code","37bc360b":"code","c1cb4ae3":"code","4d31ddb8":"code","34fc09d4":"markdown","1dd57696":"markdown","ec0a08e2":"markdown"},"source":{"8580cfe9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad27ba96":"pip install bs4","be400372":"import bz2        #for data extraction\nimport random\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nnltk.download(\"wordnet\")\nlemmatizer = WordNetLemmatizer()\nnltk.download(\"stopwords\") \nfrom nltk.corpus import stopwords\nnltk.download(\"words\")\n\nimport re\nfrom bs4 import BeautifulSoup          #To remove html tags while preprocessing reviews.         #To remove html tags while preprocessing reviews.","c4ab04e5":"train_file = bz2.BZ2File('..\/input\/amazonreviews\/train.ft.txt.bz2')\ntrain_file_lines = train_file.readlines()\ntrain_file_lines=random.sample(train_file_lines, 100000)\ndel train_file","b364d84d":"print(\"number of reviews in train data:\",len(train_file_lines),\"\\n\\n\")\ntrain_file_lines[:3]","6bcaa450":"# PRE-PROCESSING:REMOVING STOPWORDS,NON-ENGLISH WORDS AND LEMMATIZING AND FILTERING WORDS WITH LENGTH LESS THAN 3.\n\ndef remove_underscores(sentence):\n    sentence= sentence.replace(\"_\",\" \")\n    return sentence\n\nstop_words = set(stopwords.words('english'))\nenglish_words = set(nltk.corpus.words.words())\n\ndef remove_extra_words(sentence):     #remove stop words and meaningless words\n    new_sentence=\"\"\n    for w in sentence.split():\n        w=w.lower()\n        if w in english_words and w not in stop_words and w.isalpha():\n            new_sentence=new_sentence+\" \"+w\n    return new_sentence\n\ndef lemmatize_and_filter(sentence, min_word_length):        #to lemmatize words and lose the ones with length less than equal to 3.\n    sent = \"\"\n    for word in sentence.split():\n        word=word.lower()\n        if len(lemmatizer.lemmatize(word))>3:\n            sent= sent+\" \"+lemmatizer.lemmatize(word)\n    return(sent)","aa67e5b2":"#re.sub(r'[^\\w\\s],\"\",string) to remove punctuations.\n#re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', string) to remove urls.\n#re.split(\"__label__[1|2]\",string)[1] to get the 2nd element of the split, i.e, the review\n#strip() to remove whitespaces\n#re.findall(\"1|2\", str(lines))[0] to store the first number encountered in the line of the review as the label of the review.\n#BeautifulSoup(string, \"lxml\").text) to remove html tags\n\n\ndef split_label_review(data):\n    #takes in list of raw data and returns list of string sentences and the list of their corresponding labels.\n    labels = [int(re.findall(\"1|2\", str(lines))[0]) for lines in data]\n    reviews = [re.split(\"__label__[1|2]\",str(lines))[1].strip().lower() for lines in data]\n    return(reviews,labels)\n\n\ndef preprocessed_data(data):        #takes in list of raw data and returns list of processed string sentences and the list of their corresponding labels.\n    reviews = [lemmatize_and_filter(remove_extra_words(BeautifulSoup(re.sub(r'[^\\w\\s]|^https?:\\\/\\\/.*[\\r\\n]*|\\d+', '', remove_underscores(str(lines)).strip().lower()), \"lxml\").text),3) for lines in data]\n    return(reviews)","7a8e5b86":"#  **TRAIN-VALIDATION SPLIT**\n\ndef split_train_into_tain_validate(data_,validation_ratio):\n    \n    data=data_\n    \n    train_size = int(len(data)*(1-validation_ratio))\n    \n    random.shuffle(data)\n\n    validate_samples= data[train_size:]\n\n    data = data[:train_size]\n    \n    return(data,validate_samples)","331f522f":"# **NEGATIVE POSITIVE SPLIT**\n\n#SPLITING THE DATA INTO POSITIVE AND NEGATIVE REVIEWS SEPARATELY.\nimport re\ndef split_into_negative_positive(data):\n    negative_reviews=[]      # list of all negative reviews\n    positive_reviews=[]      # list of all positive reviews\n\n    for lines in train_file_lines:\n        lines= str(lines).lower()\n        x=re.findall(\"1|2\", lines)[0]\n        if x==\"1\":\n            negative_reviews.append(lines)\n        elif x==\"2\":\n            positive_reviews.append(lines)\n    return(negative_reviews, positive_reviews)","124898ef":"negative_reviews,positive_reviews= split_into_negative_positive(train_file_lines)\n\nnegative_reviews,labels = split_label_review(negative_reviews)\nprint(\"First few negative reviews:\",negative_reviews[:5])\nnegative_reviews=preprocessed_data(negative_reviews)\n\npositive_reviews,labels = split_label_review(positive_reviews)\nprint(\"First few positive reviews:\",positive_reviews[:5])\npositive_reviews=preprocessed_data(positive_reviews)\n\ndel positive_reviews, negative_reviews,labels","398687cb":"# **CLASSIFICATION [POSITIVE OR NEGATIVE REVIEW-> SENTIMENT ANALYSIS]**\n\n## TENSORFLOW (PREPROCESSING) : CREATING THE DATA TO TRAIN-TEST THE CLASSIFIER.\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","ac802b1f":"train_file_lines,validation_file_lines = split_train_into_tain_validate(train_file_lines,0.25)\nprint(\"Reduced number of training samples is {} and number of samples for validation is {}.\".format(len(train_file_lines), len(validation_file_lines)))\n\nprint(\"SAMPLES OF UNPROCESSED DATA:\\n FOR TRAINING\")\nprint(train_file_lines[:10])\nprint(\"\\n\\n\\n\\n FOR VALIDATION\")\nprint(validation_file_lines[:10])","aa6890eb":"training_sentences, training_labels =  split_label_review(train_file_lines)\ntraining_sentences=preprocessed_data(training_sentences)\n\ndel train_file_lines","f13f7f3c":"tokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(training_sentences)\n\n\"\"\"\nword_index = tokenizer.word_index\nword_index #returns a dictionary with key= words, values= tokens(numbers)\"\"\"","2dc5e473":"training_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences)\n\ndel training_sentences,training_sequences\n\n# Need this block to get it to work with TensorFlow \nimport numpy as np\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)","bc7cf93c":"vocab_size = max(list(tokenizer.word_index.values()))+1\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 10),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='relu')])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel.summary()","9d51ef21":"validation_sentences, validation_labels = split_label_review(validation_file_lines)\nvalidation_sentences=preprocessed_data(validation_sentences)\n\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences)\n\nvalidation_padded = np.array(validation_padded)\nvalidation_labels = np.array(validation_labels)\n\ndel validation_sentences,validation_sequences","5a1f0eb1":"\nhistory = model.fit(training_padded, training_labels, epochs=100, validation_data=(validation_padded, validation_labels),verbose=1,callbacks = [ tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=.25, patience=2, cooldown=0)]\n)","47db7712":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()","e15a452a":"plot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","115f67e8":"test_file = bz2.BZ2File('..\/input\/amazonreviews\/test.ft.txt.bz2')\ntest_file_lines = test_file.readlines()\ntest_file_lines=random.sample(test_file_lines, 50000)\ndel test_file\n\nprint(\"number of reviews in train data:\",len(test_file_lines),\"\\n\\n\")\ntest_file_lines[:3]\n\ntesting_sentences, testing_labels =  split_label_review(test_file_lines)\ntesting_sentences=preprocessed_data(testing_sentences)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences)\n\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)\n\ndel testing_sentences,testing_sequences","37bc360b":"def continuous_to_categorical(labellist):\n    threshold=float(input(\"Enter threshold:\"))\n    new_label=np.zeros(len(labellist),int)\n    for i in range(len(labellist)):\n        if labellist[i]<threshold:\n            new_label[i]=1\n        else:\n            new_label[i]=2\n    del labellist\n    return new_label\n    \n    ","c1cb4ae3":"print(np.unique(testing_labels))","4d31ddb8":"from sklearn.metrics import accuracy_score\naccuracy_score(testing_labels, (continuous_to_categorical(model.predict(testing_padded))))","34fc09d4":"# EXTRACTING DATA ","1dd57696":"i.EXTRACTING THE REVIEWS IN STRING TYPE.\n\nii.REMOVING LABELS, NUMBERS, URLs,HTML TAGS AND PUNCTUATIONS.\n\niii.TRANSFORMING ALL REVIEWS TO LOWER CASE.\n\niv. REMOVING STOPWORDS.\n\nv. LEMMATIZING WORDS.","ec0a08e2":"# IMPORTING LIBRARIES"}}