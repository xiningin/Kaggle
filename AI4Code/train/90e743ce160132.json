{"cell_type":{"75632a71":"code","1c841275":"code","d4de4d78":"code","5462e8f8":"code","910db050":"code","617ac316":"code","aa89a9ab":"code","cbaa4913":"markdown","97f3617b":"markdown","892d51b2":"markdown","69ff3697":"markdown","042ce2dd":"markdown","b076a55b":"markdown","6c131321":"markdown"},"source":{"75632a71":"import numpy as np\nfrom sklearn import datasets\n\n# Load the diabetes dataset\ndiabetes = datasets.load_diabetes()\n\n\n# Use only one feature\ndiabetes_X = diabetes.data[:, np.newaxis, 2]\n\n# Split the data into training\/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n\n# Split the targets into training\/testing sets\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test = diabetes.target[-20:]","1c841275":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.show()","d4de4d78":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create linear regression object\nregr = linear_model.Ridge(alpha=0.3)\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train) \n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.show()\n\n","5462e8f8":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create linear regression object\nregr = linear_model.Lasso(alpha=0.1)\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train) \n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.show()","910db050":"from sklearn.preprocessing import PolynomialFeatures\n\nX = np.arange(6).reshape(3, 2)\npoly = PolynomialFeatures(2)\nprint(poly.fit_transform(X))\npoly = PolynomialFeatures(interaction_only=True)\n# If true, only interaction features are produced: features that are products of at most degree distinct input features\nprint(poly.fit_transform(X))","617ac316":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\npolynomial_features = PolynomialFeatures(degree=3)\n\ndiabetes_X_poly_train = polynomial_features.fit_transform(diabetes_X_train)\nregr = linear_model.LinearRegression()\nregr.fit(diabetes_X_poly_train, diabetes_y_train)\ndiabetes_y_pred = regr.predict(polynomial_features.fit_transform(diabetes_X_test))\n\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.show()","aa89a9ab":"from sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nX, y = load_iris(return_X_y=True)\nclf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X, y)\nclf.predict(X[:2, :])\nprint(clf.predict_proba(X[:2, :]))\nprint(clf.score(X, y))","cbaa4913":"**Logistic Regression**<br>\nThe logistic function transforms real-valued input to an output number y between 0 and 1, interpreted as the probability the input object belongs to the positive class, given its input features.<br><br>\nParameters to be looked at:\n    1. Parameter C controls amount of regularization (default 1.0). Higher C -> less regularization, Lower C -> more regularization.\n    2. solver : Algorithm to use in the optimization problem. Values: {\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019}, optional (default=\u2019liblinear\u2019).\n        - For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones.\n        - For multiclass problems, only \u2018newton-cg\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018lbfgs\u2019 handle multinomial loss; \u2018liblinear\u2019 is limited to one-versus-rest schemes.\n    3. multi_class : {\u2018ovr\u2019, \u2018multinomial\u2019, \u2018auto\u2019}, optional (default=\u2019ovr\u2019)\n    If the option chosen is \u2018ovr\u2019, then a binary problem is fit for each label. For \u2018multinomial\u2019 the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. \nRemarks:\n    - L2 regularization is 'on' by default (like ridge regression)\n    - As with regularized linear regression, it can be important to normalize all features so that they are on the same scale.","97f3617b":"Lets start out with few supervised learning models.<br><br>\nThis Part 1 consists of following models<br>\n* Linear Regression - OLS (Ordinary Least Squares)\n* Ridge Regression\n* Lasso Regression\n* Polynomial Features with Linear Regression\n* Logistic Regression\n\n<br>\nThis is the dataset that we would be using for our regression models.\n","892d51b2":"**Lasso Regression**<br>\nLasso regression is another form of regularized linear regression that uses an L1 regularization penalty for training (instead of ridge's L2 penalty)<br>\nL1 penalty: Minimize the sum of the absolute values of the coefficients<br>\nThis has the effect of setting parameter weights in w to zero for the least influential variables. This is called a sparse solution: a kind of feature selection<br><br>\nParameters to be looked at: <br>The alpha parameter controls the degree of sparsity of the estimated coefficients.$$\\alpha$$\n<br>\n        When to use ridge vs lasso regression:\n        1. Many small\/medium sized effects: use ridge.\n        2. Only a few variables with medium\/large effect: use lasso.","69ff3697":"**Polynomial Features with Linear Regression**<br>\nGenerate new features consisting of all polynomial combinations of the original features. The degree of the polynomial specifies how many variables participate at a time in each new feature. This is still a weighted linear combination of features, so it's still a linear model, and can use same least-squares estimation method.<br><br>\nRemarks:<br>\nBeware of polynomial feature expansion with high as this can lead to complex models that overfit. Thus, polynomial feature expansion is often combined with a regularized learning method like ridge regression.","042ce2dd":"References:<br>\n* https:\/\/scikit-learn.org\/\n* Applied Machine Learning by Kevyn Collins-Thompson","b076a55b":"**Ridge Regression**\n\nSteps:<br>\nRidge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares<br>\nParameters to be looked at:$$\\alpha$$<br>\nRemarks:<br>\n* Regularisation less effective when relatively smaller amount of training data compared to number of features in training data\n* Helpful when you think there are many variables that have small or medium effect on the output variable\n* The influence of the regularization term is controlled by the alpha parameter. Higher alpha means more regularization and simpler models","6c131321":"**Linear Regression - OLS (ordinary Least Squares)**\n\nSteps:\n\nInput instance - feature vector: x = (x<sub>0<\/sub>,x<sub>1<\/sub>,...x<sub>n<\/sub>)<br>\nPredicted output: y = w<sub>0<\/sub>x<sub>0<\/sub> + w<sub>1<\/sub>x<sub>1<\/sub> + ... w<sub>n<\/sub>x<sub>n<\/sub> + b<br>\nFinds w (feature weights\/model coefficients) and b(constant bias term\/intercept) that minimizes the mean squared error of the linear model: the sum of squared differences between predicted target and actual target values.\n\n\nParameters to be looked at:<br>\nNo parameters\n\nRemarks:\n1. No parameters to control model complexity\n2. Linear models make strong assumptions about the structure of the data and give stable but potentially inaccurate predictions.\n"}}