{"cell_type":{"574e6f7b":"code","e1af4aab":"code","03610a6c":"code","912a3dad":"code","d259b7a8":"code","082c42fe":"code","eb5e271e":"code","b954f951":"code","d777c2c0":"code","f8afea77":"code","cc06e488":"code","ad2fe0be":"code","2c6cfc4a":"code","f0f6cd0b":"code","0a69f06c":"code","3d11331b":"code","5951dc80":"code","cdf68dea":"code","5a0f9c6e":"code","1e4356fd":"code","fc22eef8":"code","db279971":"code","94111350":"code","2743c6c2":"code","a5df0627":"code","7c2672ea":"code","7ce49136":"code","90f9ed38":"code","aecf9fe5":"code","f989d4d5":"code","9e941c9b":"code","efbd8993":"code","2f0e4c03":"code","e2cc7434":"code","e0c3b19a":"code","efa24926":"code","bdbbb2dd":"code","94e2197c":"code","7a950e22":"code","e2fbe2b8":"code","90b35b77":"code","9a1eb4fb":"code","31f6a31b":"code","fdcc9508":"code","669a061b":"code","d4842539":"code","be192368":"code","a7f549b7":"code","27a204c1":"code","46c16394":"code","bfbec883":"code","314e5fbd":"code","dd1815c5":"markdown","d646f5c3":"markdown","63accf2f":"markdown","b54bb5c3":"markdown","bdc42628":"markdown","e65348e2":"markdown","d3938a1e":"markdown","9e89e04f":"markdown","7b536b6d":"markdown","706ece20":"markdown","828249cf":"markdown","dae0a1f1":"markdown","e591fedf":"markdown","c1494826":"markdown","b64623ed":"markdown","f463d0d1":"markdown","26ead2df":"markdown","56c92520":"markdown","1476b48b":"markdown","e357cefa":"markdown","511bbed5":"markdown","9c64de88":"markdown","b03fd3f6":"markdown","64692be9":"markdown","0fd87fe6":"markdown","9543fba5":"markdown","33dd0288":"markdown","8c783a8e":"markdown","04189d94":"markdown","e0d616e1":"markdown","24e9fb96":"markdown","e4635af9":"markdown","bc7dd6ed":"markdown","d21e8ad2":"markdown","682c4b68":"markdown","9d3eaa3e":"markdown","a44c4d5a":"markdown","e662e601":"markdown","a68dd6a4":"markdown","9db93766":"markdown","eda08dd1":"markdown","4a4f7c6d":"markdown","29b83298":"markdown","caf9fc50":"markdown","4f363804":"markdown","b8fc2a6d":"markdown","4181b83a":"markdown","18468bb4":"markdown","6a5c0c0d":"markdown","c32e61ee":"markdown","03076479":"markdown","e282f0ee":"markdown","f598580d":"markdown","685bc1f5":"markdown"},"source":{"574e6f7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1af4aab":"# Import the necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score,recall_score\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nsns.set(color_codes=True)\n%matplotlib inline\n%config InlineBackend.figure_formats = {'png', 'retina'}\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","03610a6c":"# Load Dataset\ndf_train = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")\n\n# Display 5 rows of the training data\ndf_train.head()","912a3dad":"# Display dataset shape\nprint(\"train shape: \",df_train.shape)\nprint(\"test shape : \", df_test.shape)","d259b7a8":"# Display descriptive statistics of training data\ndf_train.describe()","082c42fe":"# Display training data information\ndf_train.info()","eb5e271e":"# Create a list of columns which will be used in modelling\ncolumns = ['city',\n           'city_development_index',\n           'gender',\n           'relevent_experience',\n           'enrolled_university',\n           'education_level',\n           'major_discipline',\n           'experience',\n           'company_size',\n           'company_type',\n           'last_new_job',\n           'training_hours']","b954f951":"# Encode categorical variables\n\n# Import package\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Instantiate encoder\nencoder = OrdinalEncoder()\n\n# Define a function for encoding\ndef encode(train_data, test_data):\n    '''function to encode non-null data and replace it in the original data'''\n    #retains only non-null values\n    train_data_nonulls = np.array(train_data.dropna())\n    test_data_nonulls = np.array(test_data.dropna())\n    #reshapes the data for encoding\n    train_impute_reshape = train_data_nonulls.reshape(-1,1)\n    test_impute_reshape = test_data_nonulls.reshape(-1,1)\n    #encode date\n    train_impute_ordinal = encoder.fit_transform(train_impute_reshape)\n    test_impute_ordinal = encoder.transform(test_impute_reshape)\n    #Assign back encoded values to non-null values\n    train_data.loc[train_data.notnull()] = np.squeeze(train_impute_ordinal)\n    test_data.loc[test_data.notnull()] = np.squeeze(test_impute_ordinal)\n    return train_data, test_data\n\n# Apply encoding funtion using a for loop to apply it to each column\nfor column in columns:\n    df_train[column], df_test[column] = encode(df_train[column], df_test[column])","d777c2c0":"# Check if the columns were encoded properly\ndf_train.head()","f8afea77":"# Check the balance of target value\ndf_train.target.value_counts()","cc06e488":"# Check each variable's data type\ndf_train.info()","ad2fe0be":"# Convert object data into float\ndf_train['gender'] = df_train['gender'].astype(float)\ndf_train['enrolled_university'] = df_train['enrolled_university'].astype(float)\ndf_train['education_level'] = df_train['education_level'].astype(float)\ndf_train['major_discipline'] = df_train['major_discipline'].astype(float)\ndf_train['experience'] = df_train['experience'].astype(float)\ndf_train['company_size'] = df_train['company_size'].astype(float)\ndf_train['company_type'] = df_train['company_type'].astype(float)\ndf_train['last_new_job'] = df_train['last_new_job'].astype(float)","2c6cfc4a":"# Check the data type\ndf_train.info()","f0f6cd0b":"# Drop rows with one or more missing values\n# Keep only rows that have no missing values\ndf_train_nonmis = pd.DataFrame(df_train).dropna(how='any', axis=1)\nprint(df_train_nonmis.info())","0a69f06c":"# Separate the dataset into features and target\nXnonmis = df_train_nonmis.drop(['enrollee_id','city','target'],axis=1)\nynonmis = df_train_nonmis['target']\n\n# Split the data into train and test\n# Since test data does not contain the target, split the training data into train and test to evaluate the model performance \nXnonmis_train, Xnonmis_test, ynonmis_train, ynonmis_test = train_test_split(Xnonmis, ynonmis, test_size=.30, \n                                                                            stratify=ynonmis, random_state=101)","3d11331b":"# Scale the data\n\n# Standardize the columns the values of which are out of 0-1 range\nscaler = StandardScaler().fit(Xnonmis_train)\n\nXnonmis_train = scaler.transform(Xnonmis_train)\nXnonmis_test = scaler.transform(Xnonmis_test)","5951dc80":"# Initiate the model\nnonmis_lm = LogisticRegression()\n\n# Fit the model\nnonmis_lm.fit(Xnonmis_train, ynonmis_train.ravel())\n\n# Make Predictions\nnonmis_lm_pred = nonmis_lm.predict(Xnonmis_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nnonmis_lm_accuracy = accuracy_score(ynonmis_test, nonmis_lm_pred)\nnonmis_lm_precision = precision_score(ynonmis_test, nonmis_lm_pred)\nnonmis_lm_recall = recall_score(ynonmis_test, nonmis_lm_pred)\nnonmis_lm_f1 = 2 * (nonmis_lm_precision * nonmis_lm_recall) \/ (nonmis_lm_precision + nonmis_lm_recall)\n\n# Calculate AUC score\nnonmis_lm_probs = nonmis_lm.predict_proba(Xnonmis_test)\nnonmis_lm_probs = nonmis_lm_probs[:,1]\nnonmis_lm_auc = roc_auc_score(ynonmis_test, nonmis_lm_probs)\n\n# Display the metrics\nprint(\"Logistic Regression: Rows without missing values\")\nprint(\" - Accuracy : \",'{:.3f}'.format(nonmis_lm_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(nonmis_lm_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(nonmis_lm_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(nonmis_lm_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(ynonmis_test,nonmis_lm_pred))","cdf68dea":"# Create countplot for \"gender\"\nfig, ax =plt.subplots(4,2,figsize=(20,16))\nsns.countplot(df_train['gender'], ax=ax[0,0])\nsns.countplot(df_train['enrolled_university'], ax=ax[0,1])\nsns.countplot(df_train['major_discipline'], ax=ax[1,0])\nsns.countplot(df_train['experience'], ax=ax[1,1])\nsns.countplot(df_train['company_size'], ax=ax[2,0])\nsns.countplot(df_train['company_type'], ax=ax[2,1])\nsns.countplot(df_train['last_new_job'], ax=ax[3,0])\n\nfig.show()","5a0f9c6e":"# Impute missing values with mode\ndf_imp = df_train\ndf_imp.fillna(df_imp.mode().iloc[0],inplace=True)\ndf_imp.info()","1e4356fd":"# Create dummies for nominal variables\ndf_imp = pd.concat([df_imp,pd.get_dummies(df_imp[\"gender\"], prefix=\"gender\")], \n                   axis=1).drop(columns=[\"gender\"])\ndf_imp = pd.concat([df_imp,pd.get_dummies(df_imp[\"enrolled_university\"], prefix=\"enrolled_university\")], \n                   axis=1).drop(columns=[\"enrolled_university\"])\ndf_imp = pd.concat([df_imp,pd.get_dummies(df_imp[\"major_discipline\"], prefix=\"major_discipline\")], \n                   axis=1).drop(columns=[\"major_discipline\"])\ndf_imp = pd.concat([df_imp,pd.get_dummies(df_imp[\"company_type\"], prefix=\"company_type\")], \n                   axis=1).drop(columns=[\"company_type\"])","fc22eef8":"# Separate the dataset into features and target\nX_imp = df_imp.drop(['enrollee_id','target'],axis=1)\ny_imp = df_imp['target']\n\n# Split the data into train and test\n# Since test data does not contain the target, split the training data into train and test to evaluate the model performance \nX_imp_train, X_imp_test, y_imp_train, y_imp_test = train_test_split(X_imp, y_imp, test_size=.30,stratify=y_imp, random_state=101)","db279971":"# Scale the data\n\n# Standardize the columns the values of which are out of 0-1 range\nscaler = StandardScaler().fit(X_imp_train)\n\nX_imp_train = scaler.transform(X_imp_train)\nX_imp_test = scaler.transform(X_imp_test)","94111350":"# Initiate the model\nimp_lm = LogisticRegression()\n\n# Fit the model\nimp_lm.fit(X_imp_train, y_imp_train.ravel())\n\n# Make Predictions\nimp_lm_pred = imp_lm.predict(X_imp_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nimp_lm_accuracy = accuracy_score(y_imp_test, imp_lm_pred)\nimp_lm_precision = precision_score(y_imp_test, imp_lm_pred)\nimp_lm_recall = recall_score(y_imp_test, imp_lm_pred)\nimp_lm_f1 = 2 * (imp_lm_precision * imp_lm_recall) \/ (imp_lm_precision + imp_lm_recall)\n\n# Calculate AUC score\nimp_lm_probs = imp_lm.predict_proba(X_imp_test)\nimp_lm_probs = imp_lm_probs[:,1]\nimp_lm_auc = roc_auc_score(y_imp_test, imp_lm_probs)\n\n# Display the metrics\nprint(\"Logistic Regression: Imputation with Mode\")\nprint(\" - Accuracy : \",'{:.3f}'.format(imp_lm_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(imp_lm_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(imp_lm_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(imp_lm_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_imp_test,imp_lm_pred))","2743c6c2":"# Initiate the model\nimp_rf = RandomForestClassifier()\n\n# Fit the model\nimp_rf.fit(X_imp_train, y_imp_train.ravel())\n\n# Make Predictions\nimp_rf_pred = imp_rf.predict(X_imp_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nimp_rf_accuracy = accuracy_score(y_imp_test, imp_rf_pred)\nimp_rf_precision = precision_score(y_imp_test, imp_rf_pred)\nimp_rf_recall = recall_score(y_imp_test, imp_rf_pred)\nimp_rf_f1 = 2 * (imp_rf_precision * imp_rf_recall) \/ (imp_rf_precision + imp_rf_recall)\n\n# Calculate AUC score\nimp_rf_probs = imp_rf.predict_proba(X_imp_test)\nimp_rf_probs = imp_rf_probs[:,1]\nimp_rf_auc = roc_auc_score(y_imp_test, imp_rf_probs)\n\n# Display the metrics\nprint(\"Random Forest: Imputation with Mode\")\nprint(\" - Accuracy : \",'{:.3f}'.format(imp_rf_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(imp_rf_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(imp_rf_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(imp_rf_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_imp_test,imp_rf_pred))","a5df0627":"# Initiate the model\nimp_xgb = XGBClassifier()\n\n# Fit the model\nimp_xgb.fit(X_imp_train, y_imp_train.ravel())\n\n# Make Predictions\nimp_xgb_pred = imp_xgb.predict(X_imp_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nimp_xgb_accuracy = accuracy_score(y_imp_test, imp_xgb_pred)\nimp_xgb_precision = precision_score(y_imp_test, imp_xgb_pred)\nimp_xgb_recall = recall_score(y_imp_test, imp_xgb_pred)\nimp_xgb_f1 = 2 * (imp_xgb_precision * imp_xgb_recall) \/ (imp_xgb_precision + imp_xgb_recall)\n\n# Calculate AUC score\nimp_xgb_probs = imp_xgb.predict_proba(X_imp_test)\nimp_xgb_probs = imp_xgb_probs[:,1]\nimp_xgb_auc = roc_auc_score(y_imp_test, imp_xgb_probs)\n\n# Display the metrics\nprint(\"XGBoost: Imputation with Mode\")\nprint(\" - Accuracy : \",'{:.3f}'.format(imp_xgb_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(imp_xgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(imp_xgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(imp_xgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_imp_test,imp_xgb_pred))","7c2672ea":"# Since this is an imbalanced data, apply SMOTE to the training set\nfrom imblearn.over_sampling import SMOTE\nsmote=SMOTE()\nX_smote_imp_train, y_smote_imp_train = smote.fit_sample(X_imp_train,y_imp_train)\n\n# Check if SMOTE were properly applied\ny_smote_imp_train.value_counts()","7ce49136":"# Initiate the model\nsmote_imp_lm = LogisticRegression()\n\n# Fit the model\nsmote_imp_lm.fit(X_smote_imp_train, y_smote_imp_train.ravel())\n\n# Make Predictions\nsmote_imp_lm_pred = smote_imp_lm.predict(X_imp_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_imp_lm_accuracy = accuracy_score(y_imp_test, smote_imp_lm_pred)\nsmote_imp_lm_precision = precision_score(y_imp_test, smote_imp_lm_pred)\nsmote_imp_lm_recall = recall_score(y_imp_test, smote_imp_lm_pred)\nsmote_imp_lm_f1 = 2 * (smote_imp_lm_precision * smote_imp_lm_recall) \/ (smote_imp_lm_precision + smote_imp_lm_recall)\n\n# Calculate AUC score\nsmote_imp_lm_probs = smote_imp_lm.predict_proba(X_imp_test)\nsmote_imp_lm_probs = smote_imp_lm_probs[:,1]\nsmote_imp_lm_auc = roc_auc_score(y_imp_test, smote_imp_lm_probs)\n\n# Display the metrics\nprint(\"Logistic Regression: Imputation with Mode: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_imp_lm_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_imp_lm_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_imp_lm_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_imp_lm_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_imp_test,smote_imp_lm_pred))","90f9ed38":"# Initiate the model\nsmote_imp_rf = RandomForestClassifier()\n\n# Fit the model\nsmote_imp_rf.fit(X_smote_imp_train, y_smote_imp_train.ravel())\n\n# Make Predictions\nsmote_imp_rf_pred = smote_imp_rf.predict(X_imp_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_imp_rf_accuracy = accuracy_score(y_imp_test, smote_imp_rf_pred)\nsmote_imp_rf_precision = precision_score(y_imp_test, smote_imp_rf_pred)\nsmote_imp_rf_recall = recall_score(y_imp_test, smote_imp_rf_pred)\nsmote_imp_rf_f1 = 2 * (smote_imp_rf_precision * smote_imp_rf_recall) \/ (smote_imp_rf_precision + smote_imp_rf_recall)\n\n# Calculate AUC score\nsmote_imp_rf_probs = smote_imp_rf.predict_proba(X_imp_test)\nsmote_imp_rf_probs = smote_imp_rf_probs[:,1]\nsmote_imp_rf_auc = roc_auc_score(y_imp_test, smote_imp_rf_probs)\n\n# Display the metrics\nprint(\"Random Forest: Imputation with Mode: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_imp_rf_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_imp_rf_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_imp_rf_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_imp_rf_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_imp_test,smote_imp_rf_pred))","aecf9fe5":"# Initiate the model\nsmote_imp_xgb = XGBClassifier()\n\n# Fit the model\nsmote_imp_xgb.fit(X_smote_imp_train, y_smote_imp_train.ravel())\n\n# Make Predictions\nsmote_imp_xgb_pred = smote_imp_xgb.predict(X_imp_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_imp_xgb_accuracy = accuracy_score(y_imp_test, smote_imp_xgb_pred)\nsmote_imp_xgb_precision = precision_score(y_imp_test, smote_imp_xgb_pred)\nsmote_imp_xgb_recall = recall_score(y_imp_test, smote_imp_xgb_pred)\nsmote_imp_xgb_f1 = 2 * (smote_imp_xgb_precision * smote_imp_xgb_recall) \/ (smote_imp_xgb_precision + smote_imp_xgb_recall)\n\n# Calculate AUC score\nsmote_imp_xgb_probs = smote_imp_xgb.predict_proba(X_imp_test)\nsmote_imp_xgb_probs = smote_imp_xgb_probs[:,1]\nsmote_imp_xgb_auc = roc_auc_score(y_imp_test, smote_imp_xgb_probs)\n\n# Display the metrics\nprint(\"XGBoost: Imputation with Mode: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_imp_xgb_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_imp_xgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_imp_xgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_imp_xgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_imp_test,smote_imp_xgb_pred))","f989d4d5":"# Impute missing values with KNN\nfrom sklearn.impute import KNNImputer\nfrom numpy import isnan\n\n# Copy the dataset for KNN imputation\ndf_knn = df_train\n\n# Separate the dataset into features and target\nX_knn = df_knn.drop(['enrollee_id','city','target'],axis=1)\ny_knn = df_knn['target']\n\n# Define imputer\nimputer = KNNImputer()\n\n# fit on the dataset\nimputer.fit(X_knn)\n\n# Transform the dataset\nX_knn = imputer.transform(X_knn)\n\n# Convert to dataframe\nX_knn = pd.DataFrame(X_knn)\nX_knn.columns = [\"city_development_index\",\"gender\",\"relevent_experience\",\"enrolled_university\",\"education_level\",\"major_discipline\",\n                 \"experience\",\"company_size\",\"company_type\",\"last_new_job\",\"training_hours\"]\nX_knn.info()","9e941c9b":"# Create dummies for nominal variables\nX_knn = pd.concat([X_knn,pd.get_dummies(X_knn[\"gender\"], prefix=\"gender\")], \n                   axis=1).drop(columns=[\"gender\"])\nX_knn = pd.concat([X_knn,pd.get_dummies(X_knn[\"enrolled_university\"], prefix=\"enrolled_university\")], \n                   axis=1).drop(columns=[\"enrolled_university\"])\nX_knn = pd.concat([X_knn,pd.get_dummies(X_knn[\"major_discipline\"], prefix=\"major_discipline\")], \n                   axis=1).drop(columns=[\"major_discipline\"])\nX_knn = pd.concat([X_knn,pd.get_dummies(X_knn[\"company_type\"], prefix=\"company_type\")], \n                   axis=1).drop(columns=[\"company_type\"])","efbd8993":"# Split the data into train and test\nX_knn_train, X_knn_test, y_knn_train, y_knn_test = train_test_split(X_knn, y_knn, test_size=.30,stratify=y_knn, random_state=101)","2f0e4c03":"# Initiate the model\nknn_lm = LogisticRegression()\n\n# Fit the model\nknn_lm.fit(X_knn_train, y_knn_train.ravel())\n\n# Make Predictions\nknn_lm_pred = knn_lm.predict(X_knn_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nknn_lm_accuracy = accuracy_score(y_knn_test, knn_lm_pred)\nknn_lm_precision = precision_score(y_knn_test, knn_lm_pred)\nknn_lm_recall = recall_score(y_knn_test, knn_lm_pred)\nknn_lm_f1 = 2 * (knn_lm_precision * knn_lm_recall) \/ (knn_lm_precision + knn_lm_recall)\n\n# Calculate AUC score\nknn_lm_probs = knn_lm.predict_proba(X_knn_test)\nknn_lm_probs = knn_lm_probs[:,1]\nknn_lm_auc = roc_auc_score(y_knn_test, knn_lm_probs)\n\n# Display the metrics\nprint(\"Logistic Regression: KNN Imputation\")\nprint(\" - Accuracy : \",'{:.3f}'.format(knn_lm_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(knn_lm_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(knn_lm_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(knn_lm_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_knn_test,knn_lm_pred))","e2cc7434":"# Initiate the model\nknn_rf = RandomForestClassifier()\n\n# Fit the model\nknn_rf.fit(X_knn_train, y_knn_train.ravel())\n\n# Make Predictions\nknn_rf_pred = knn_rf.predict(X_knn_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nknn_rf_accuracy = accuracy_score(y_knn_test, knn_rf_pred)\nknn_rf_precision = precision_score(y_knn_test, knn_rf_pred)\nknn_rf_recall = recall_score(y_knn_test, knn_rf_pred)\nknn_rf_f1 = 2 * (knn_rf_precision * knn_rf_recall) \/ (knn_rf_precision + knn_rf_recall)\n\n# Calculate AUC score\nknn_rf_probs = knn_rf.predict_proba(X_knn_test)\nknn_rf_probs = knn_rf_probs[:,1]\nknn_rf_auc = roc_auc_score(y_knn_test, knn_rf_probs)\n\n# Display the metrics\nprint(\"Random Forest: KNN Imputation\")\nprint(\" - Accuracy : \",'{:.3f}'.format(knn_rf_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(knn_rf_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(knn_rf_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(knn_rf_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_knn_test,knn_rf_pred))","e0c3b19a":"# Initiate the model\nknn_xgb = XGBClassifier()\n\n# Fit the model\nknn_xgb.fit(X_knn_train, y_knn_train.ravel())\n\n# Make Predictions\nknn_xgb_pred = knn_xgb.predict(X_knn_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nknn_xgb_accuracy = accuracy_score(y_knn_test, knn_xgb_pred)\nknn_xgb_precision = precision_score(y_knn_test, knn_xgb_pred)\nknn_xgb_recall = recall_score(y_knn_test, knn_xgb_pred)\nknn_xgb_f1 = 2 * (knn_xgb_precision * knn_xgb_recall) \/ (knn_xgb_precision + knn_xgb_recall)\n\n# Calculate AUC score\nknn_xgb_probs = knn_xgb.predict_proba(X_knn_test)\nknn_xgb_probs = knn_xgb_probs[:,1]\nknn_xgb_auc = roc_auc_score(y_knn_test, knn_xgb_probs)\n\n# Display the metrics\nprint(\"XGBoost: Imputation with Mode\")\nprint(\" - Accuracy : \",'{:.3f}'.format(knn_xgb_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(knn_xgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(knn_xgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(knn_xgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_knn_test,knn_xgb_pred))","efa24926":"# Since this is an imbalanced data, apply SMOTE to the training set\nsmote=SMOTE()\nX_smote_knn_train, y_smote_knn_train = smote.fit_sample(X_knn_train,y_knn_train)\n\n# Check if SMOTE were properly applied\ny_smote_knn_train.value_counts()","bdbbb2dd":"# Initiate the model\nsmote_knn_lm = LogisticRegression()\n\n# Fit the model\nsmote_knn_lm.fit(X_smote_knn_train, y_smote_knn_train.ravel())\n\n# Make Predictions\nsmote_knn_lm_pred = smote_knn_lm.predict(X_knn_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_knn_lm_accuracy = accuracy_score(y_knn_test, smote_knn_lm_pred)\nsmote_knn_lm_precision = precision_score(y_knn_test, smote_knn_lm_pred)\nsmote_knn_lm_recall = recall_score(y_knn_test, smote_knn_lm_pred)\nsmote_knn_lm_f1 = 2 * (smote_knn_lm_precision * smote_knn_lm_recall) \/ (smote_knn_lm_precision + smote_knn_lm_recall)\n\n# Calculate AUC score\nsmote_knn_lm_probs = smote_knn_lm.predict_proba(X_knn_test)\nsmote_knn_lm_probs = smote_knn_lm_probs[:,1]\nsmote_knn_lm_auc = roc_auc_score(y_knn_test, smote_knn_lm_probs)\n\n# Display the metrics\nprint(\"Logistic Regression: Imputation with KNN: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_knn_lm_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_knn_lm_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_knn_lm_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_knn_lm_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_knn_test,smote_knn_lm_pred))","94e2197c":"# Initiate the model\nsmote_knn_rf = RandomForestClassifier()\n\n# Fit the model\nsmote_knn_rf.fit(X_smote_knn_train, y_smote_knn_train.ravel())\n\n# Make Predictions\nsmote_knn_rf_pred = smote_knn_rf.predict(X_knn_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_knn_rf_accuracy = accuracy_score(y_knn_test, smote_knn_rf_pred)\nsmote_knn_rf_precision = precision_score(y_knn_test, smote_knn_rf_pred)\nsmote_knn_rf_recall = recall_score(y_knn_test, smote_knn_rf_pred)\nsmote_knn_rf_f1 = 2 * (smote_knn_rf_precision * smote_knn_rf_recall) \/ (smote_knn_rf_precision + smote_knn_rf_recall)\n\n# Calculate AUC score\nsmote_knn_rf_probs = smote_knn_rf.predict_proba(X_knn_test)\nsmote_knn_rf_probs = smote_knn_rf_probs[:,1]\nsmote_knn_rf_auc = roc_auc_score(y_knn_test, smote_knn_rf_probs)\n\n# Display the metrics\nprint(\"Random Forest: Imputation with Mode: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_knn_rf_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_knn_rf_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_knn_rf_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_knn_rf_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_knn_test,smote_knn_rf_pred))","7a950e22":"# Initiate the model\nsmote_knn_xgb = XGBClassifier()\n\n# Fit the model\nsmote_knn_xgb.fit(X_smote_knn_train, y_smote_knn_train.ravel())\n\n# Make Predictions\nsmote_knn_xgb_pred = smote_knn_xgb.predict(X_knn_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_knn_xgb_accuracy = accuracy_score(y_knn_test, smote_knn_xgb_pred)\nsmote_knn_xgb_precision = precision_score(y_knn_test, smote_knn_xgb_pred)\nsmote_knn_xgb_recall = recall_score(y_knn_test, smote_knn_xgb_pred)\nsmote_knn_xgb_f1 = 2 * (smote_knn_xgb_precision * smote_knn_xgb_recall) \/ (smote_knn_xgb_precision + smote_knn_xgb_recall)\n\n# Calculate AUC score\nsmote_knn_xgb_probs = smote_knn_xgb.predict_proba(X_knn_test)\nsmote_knn_xgb_probs = smote_knn_xgb_probs[:,1]\nsmote_knn_xgb_auc = roc_auc_score(y_knn_test, smote_knn_xgb_probs)\n\n# Display the metrics\nprint(\"XGBoost: Imputation with Mode: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_knn_xgb_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_knn_xgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_knn_xgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_knn_xgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_knn_test,smote_knn_xgb_pred))","e2fbe2b8":"# Separate the dataset into features and target\nX = df_train.drop(['enrollee_id','city','target'],axis=1)\ny = df_train['target']","90b35b77":"# Create dummies for nominal variables\n# Set dummy_na=True to include NaN as a dummy variable \nX = pd.concat([X,pd.get_dummies(X[\"gender\"], prefix=\"gender\", dummy_na=True)], \n                   axis=1).drop(columns=[\"gender\"])\nX = pd.concat([X,pd.get_dummies(X[\"enrolled_university\"], prefix=\"enrolled_university\",  dummy_na=True)],\n                   axis=1).drop(columns=[\"enrolled_university\"])\nX = pd.concat([X,pd.get_dummies(X[\"major_discipline\"], prefix=\"major_discipline\",  dummy_na=True)], \n                   axis=1).drop(columns=[\"major_discipline\"])\nX = pd.concat([X,pd.get_dummies(X[\"company_type\"], prefix=\"company_type\",  dummy_na=True)], \n                   axis=1).drop(columns=[\"company_type\"])","9a1eb4fb":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30,stratify=y, random_state=101)","31f6a31b":"# Initiate the model\nbase_xgb = XGBClassifier()\n# Fit the model\nbase_xgb_model = base_xgb.fit(X_train, y_train.ravel())\n# Make Predictions\nbase_xgb_pred=base_xgb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbase_xgb_accuracy = accuracy_score(y_test, base_xgb_pred)\nbase_xgb_precision = precision_score(y_test, base_xgb_pred)\nbase_xgb_recall = recall_score(y_test, base_xgb_pred)\nbase_xgb_f1 = 2 * (base_xgb_precision * base_xgb_recall) \/ (base_xgb_precision + base_xgb_recall)\n\n# Calculate AUC score\nbase_xgb_probs = base_xgb.predict_proba(X_test)\nbase_xgb_probs = base_xgb_probs[:,1]\nbase_xgb_auc = roc_auc_score(y_test, base_xgb_probs)\n\n# Display the metrics\nprint(\"XGBClassifier: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(base_xgb_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(base_xgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(base_xgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(base_xgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,base_xgb_pred))","fdcc9508":"# import LightGBM\nimport lightgbm as lgb","669a061b":"# Initiate the model\nbase_lgb = lgb.LGBMClassifier()\n# Fit the model\nbase_lgb_model = base_lgb.fit(X_train, y_train.ravel())\n# Make Predictions\nbase_lgb_pred=base_lgb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbase_lgb_accuracy = accuracy_score(y_test, base_lgb_pred)\nbase_lgb_precision = precision_score(y_test, base_lgb_pred)\nbase_lgb_recall = recall_score(y_test, base_lgb_pred)\nbase_lgb_f1 = 2 * (base_lgb_precision * base_lgb_recall) \/ (base_lgb_precision + base_lgb_recall)\n\n# Calculate AUC score\nbase_lgb_probs = base_lgb.predict_proba(X_test)\nbase_lgb_probs = base_lgb_probs[:,1]\nbase_lgb_auc = roc_auc_score(y_test, base_lgb_probs)\n\n# Display the metrics\nprint(\"LightGBM Classifier: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(base_lgb_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(base_lgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(base_lgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(base_lgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,base_lgb_pred))","d4842539":"# Count the target\ny.value_counts()","be192368":"# T - no. of total samples\n# P - no. of positive samples\n# scale_pos_weight = percent of negative \/ percent of positive\n# which translates to:\n# scale_pos_weight = (100*(T-P)\/T) \/ (100*P\/T)\n# which further simplifies to beautiful:\n# scale_pos_weight = T\/P - 1\nT = 14381 + 4777\nP = 4777\nscale_pos_weight = T\/P - 1\nscale_pos_weight","a7f549b7":"# Initiate the model\nweighted_xgb = XGBClassifier(scale_pos_weight=scale_pos_weight)\n# Fit the model\nweighted_xgb_model = weighted_xgb.fit(X_train, y_train.ravel())\n# Make Predictions\nweighted_xgb_pred=weighted_xgb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nweighted_xgb_accuracy = accuracy_score(y_test, weighted_xgb_pred)\nweighted_xgb_precision = precision_score(y_test, weighted_xgb_pred)\nweighted_xgb_recall = recall_score(y_test, weighted_xgb_pred)\nweighted_xgb_f1 = 2 * (weighted_xgb_precision * weighted_xgb_recall) \/ (weighted_xgb_precision + weighted_xgb_recall)\n\n# Calculate AUC score\nweighted_xgb_probs = weighted_xgb.predict_proba(X_test)\nweighted_xgb_probs = weighted_xgb_probs[:,1]\nweighted_xgb_auc = roc_auc_score(y_test, weighted_xgb_probs)\n\n# Display the metrics\nprint(\"XGBClassifier: Weighted\")\nprint(\" - Accuracy : \",'{:.3f}'.format(weighted_xgb_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(weighted_xgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(weighted_xgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(weighted_xgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,weighted_xgb_pred))","27a204c1":"# Initiate the model\nweighted_lgb = lgb.LGBMClassifier(scale_pos_weight=scale_pos_weight)\n# Fit the model\nweighted_lgb_model = weighted_lgb.fit(X_train, y_train.ravel())\n# Make Predictions\nweighted_lgb_pred=weighted_lgb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nweighted_lgb_accuracy = accuracy_score(y_test, weighted_lgb_pred)\nweighted_lgb_precision = precision_score(y_test, weighted_lgb_pred)\nweighted_lgb_recall = recall_score(y_test, weighted_lgb_pred)\nweighted_lgb_f1 = 2 * (weighted_lgb_precision * weighted_lgb_recall) \/ (weighted_lgb_precision + weighted_lgb_recall)\n\n# Calculate AUC score\nweighted_lgb_probs = weighted_lgb.predict_proba(X_test)\nweighted_lgb_probs = weighted_lgb_probs[:,1]\nweighted_lgb_auc = roc_auc_score(y_test, weighted_lgb_probs)\n\n# Display the metrics\nprint(\"LightGBM Classifier: Weighted\")\nprint(\" - Accuracy : \",'{:.3f}'.format(weighted_lgb_accuracy))\nprint(\" - Recall   : \",'{:.3f}'.format(weighted_lgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(weighted_lgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(weighted_lgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,weighted_lgb_pred))","46c16394":"# Summarize results with Tables \npd.options.display.float_format = '{:.3f}'.format\n\npf_list = [\n    [smote_imp_lm_recall, smote_imp_lm_f1, smote_imp_lm_auc],\n    [smote_imp_rf_recall, smote_imp_rf_f1, smote_imp_rf_auc],\n    [smote_imp_xgb_recall, smote_imp_xgb_f1, smote_imp_xgb_auc],\n    [smote_knn_lm_recall, smote_knn_lm_f1, smote_knn_lm_auc],\n    [smote_knn_rf_recall, smote_knn_rf_f1, smote_knn_rf_auc],\n    [smote_knn_xgb_recall, smote_knn_xgb_f1, smote_knn_xgb_auc],\n    [weighted_xgb_recall, weighted_xgb_f1, weighted_xgb_auc],\n    [weighted_lgb_recall, weighted_lgb_f1, weighted_lgb_auc]]\n\npf_df = pd.DataFrame(pf_list)\npf_df.index = ['Logistic: Mode Imputation','Random Forest: Mode Imputation','XGBoost: Mode Imputation',\n               'Logistic: KNN Imputation','Random Forest: KNN Imputation','XGBoost: KNN Imputation',\n              'XGBoost: No Imputation', 'LightGBM: No Imputation']\npf_df.columns = ['Recall', 'F1','AUC']\n\npf_df","bfbec883":"# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Set parameters\nparameters = [{\n    'learning_rate':[0.01, 0.1,0.2],\n    'n_estimators':[10, 20,30,40],\n    'max_depth':[3,5,7,9,10],\n    'verbose':[-1],\n    'min_data_in_leaf':[30,40,50,60,70],\n    'num_leaves':[10, 20, 30]\n}]\n\n# Grid Search: Maximize AUC score\nclassifier = GridSearchCV(lgb.LGBMClassifier(scale_pos_weight=scale_pos_weight), parameters, scoring='roc_auc', cv=3, n_jobs=-1)\nclassifier.fit(X_train, y_train)\nprint(\"Accuracy score (train): \", classifier.score(X_train, y_train))\nprint(\"Accuracy score (test): \", classifier.score(X_test, y_test))\nprint(classifier.best_estimator_) # Best parameter","314e5fbd":"# Initiate the model\nbest_lgb = lgb.LGBMClassifier(learning_rate=0.2, max_depth=7, min_data_in_leaf=30, n_estimators=40, num_leaves=10,verbose=-1,\n                              scale_pos_weight=scale_pos_weight)\n# Fit the model\nbest_lgb_model = best_lgb.fit(X_train, y_train.ravel())\n# Make Predictions\nbest_lgb_pred=best_lgb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbest_lgb_accuracy = accuracy_score(y_test, best_lgb_pred)\nbest_lgb_precision = precision_score(y_test, best_lgb_pred)\nbest_lgb_recall = recall_score(y_test, best_lgb_pred)\nbest_lgb_f1 = 2 * (best_lgb_precision * best_lgb_recall) \/ (best_lgb_precision + best_lgb_recall)\n\n# Calculate AUC score\nbest_lgb_probs = best_lgb.predict_proba(X_test)\nbest_lgb_probs = best_lgb_probs[:,1]\nbest_lgb_auc = roc_auc_score(y_test, best_lgb_probs)\n\n# Display the metrics\nprint(\"LightGBM Classifier: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(best_lgb_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(best_lgb_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(best_lgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(best_lgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(best_lgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,best_lgb_pred))","dd1815c5":"### 2.2.3. XGBoost: Mode Imputation - SMOTE","d646f5c3":"### 2.1.1. Logistic Regression: Mode Imputation - Imbalanced Data","63accf2f":"- Weighted LightGBM without imputation achieve the best performance.\n- So let's tune its hyperparameter to maximize AUC.","b54bb5c3":"### 4.1.2. LightGBM Classifier with Data without Imputation: Imbalanced Data","bdc42628":"# 4. Data without Dropping and Imputation","e65348e2":"#### The AUC score slightly improved.","d3938a1e":"### Apply SMOTE","9e89e04f":"- First, let's make prediction using logistic regression.\n- Here, let's drop the observations with one or more missing values.","7b536b6d":"## 5. Summary of Results","706ece20":"### Rebalance Data by Weighting Classes","828249cf":"## 3.2. Prediction with KNN Imputation - SMOTE","dae0a1f1":"Only 6 variables remain.","e591fedf":"### 3.2.1. Logistic Regression: KNN Imputation - SMOTE","c1494826":"## Mode Imputation","b64623ed":"## 2.2. Prediction with Mode Imputation & SMOTE","f463d0d1":"### 3.1.3. XGBoost: KNN Imputation - Imbalanced Data","26ead2df":"The categorical variables were encoded to numeric.","56c92520":"### 4.2.2. LightGBM Classifier with Data without Imputation: Weighted","1476b48b":"Object variables were converted into float.","e357cefa":"## Data Preprocessing","511bbed5":"# Prediction of Data Scientists' Job Change","9c64de88":"The dataset is imbalanced. So rebalance the data using scale_pos_weight.","b03fd3f6":"- \"gender\",\"enrolled_university\",\"major_discipline\",\"experience\",\"company_size\",\"company_type\",\"last_new_job\" contain missing values.\n- Majory of the variables are categorical variables. They need to be encoded.","64692be9":"## Load and Explore Dataset","0fd87fe6":"- The model contains only some of the whole variables. So it lost much information.\n- So, next, let's impute missing variables.","9543fba5":"### 2.1.2. Random Forest: Mode Imputation - Imbalanced Data","33dd0288":"## 6. Gridsearch for LightGBM","8c783a8e":"THe data still contains object data. So let's convert them into float manually.","04189d94":"## 3.1. Prediction with KNN Imputation - Imbalanced Data","e0d616e1":"### 3.1.2. Logistic Regression: KNN Imputation - Imbalanced Data","24e9fb96":"- Let's imput missing values by a traditional way: since the focal variables are categorical, let's use the mode for imputation.\n- As we saw above, \"gender\",\"enrolled_university\",\"major_discipline\",\"experience\",\"company_size\",\"company_type\",\"last_new_job\" contain missing values. So let's check the distribution of these variables.","e4635af9":"## 2.1. Prediction with Imbalanced Data: Mode Imputation","bc7dd6ed":"### 3.1.2. Random Forest: KNN Imputation - Imbalanced Data","d21e8ad2":"### Apply SMOTE","682c4b68":"### 3.2.3. XGBoost: KNN Imputation - SMOTE","9d3eaa3e":"Missing values were imputed successfully.","a44c4d5a":"## 4.1. Prediction with Data without Imputation  - Imbalanced Data","e662e601":"### 4.2.1. XGB Classifier with Data without Imputation: Weighted","a68dd6a4":"- LightGBClassifier with scale pos weight achieved the highest performance.\n- So let's tune parameters of this model.","9db93766":"# 3. Prediction with KNN Imputation","eda08dd1":"1. Binary Classification Problem: Predict \"Change\" or \"Not Change\"**\n2. The data contains a lot of missing values, and the target variable is imbalanced.\n3. Apply various algorithm using various methods of handling missing values and data rebalancing.\n  - Methods for Handling missing data: 1) Dropping, 2) Mode imputation, 3) KNN imputation, 4) No imputation\n  - Data Rebalancing a) Imbalanced data, b) SMOTE, c) Scale Pos Weight  \n4. Algorithm:\n - Logistic Regression (1, 2, 3, a, b)\n - Random Forest (2, 3, a, b)\n - XGBoostn (2, 3, 4, a, b, c)\n - LightGBM (4, a, c)","4a4f7c6d":"### 2.2.2. Random Forest: Mode Imputation - SMOTE","29b83298":"# 1. Logistic Regression with Data: Observations with missing values were dropped","caf9fc50":"### 2.1.3. XGBoost: Mode Imputation - Imbalanced Data","4f363804":"## 4.2. Prediction with Data without Imputation - Class Weighted","b8fc2a6d":"### 2.2.1. Logistic Regression: Mode Imputation - SMOTE","4181b83a":"Let's summarize the results:\n  - For data: 1) Mode imputation, 2) KNN imputation, 3) Without imputation\n  - For P\/N balance: SMOTE rebalanced or Positive weighted\n  - For algorithm: Logistic regression, Random forest, XGBoost, LightGBM","18468bb4":"- The data is imbalanced.","6a5c0c0d":"# 2. Prediction with Mode Imputation","c32e61ee":"### 4.1.1. XGBoost Classifier with Data without Imputation: Imbalanced Data","03076479":"### Keep only rows without missing values","e282f0ee":"### Make Prediction","f598580d":"## KNN Imputation","685bc1f5":"### 3.2.2. Random Forest: KNN Imputation - SMOTE"}}