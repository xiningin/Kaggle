{"cell_type":{"1dc5fd93":"code","6844ab21":"code","c061bac9":"code","c98ecef2":"code","b25ff353":"code","0bbcbe54":"code","662a9e29":"code","4d3f866e":"code","95ed5346":"code","cc278201":"code","f46f3725":"code","0c1e14bb":"code","78b21e39":"code","1788fc9d":"code","cd235e82":"code","7fdd16b0":"code","480de0d3":"code","8dfb411e":"code","b6aeaa86":"code","44cb448a":"code","3588ed7c":"code","d696a1bf":"code","fc350f31":"code","98a64fbe":"code","f9182c0b":"code","e981b800":"code","fa195d62":"code","e305750e":"code","97ccbf9d":"markdown","b5218d3a":"markdown","8a903e08":"markdown","268f9a3e":"markdown","712d3e15":"markdown","fbd5a07c":"markdown","ff0d8bac":"markdown","3e63f107":"markdown","ac6d6216":"markdown","f2708c3c":"markdown"},"source":{"1dc5fd93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6844ab21":"# Load the Datasets\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","c061bac9":"# First 5 row for train dataset\ntrain.head()","c98ecef2":"# First 5 row for test dataset\ntest.head()","b25ff353":"print(f\"Train dataset shape {train.shape}\")\nprint(f\"Test dataset shape {test.shape}\")","0bbcbe54":"print(f\"Null value for train dataset: {train.isna().sum()}\")\nprint(\"--------------------\")\nprint(f\"Null value for train dataset: {test.isna().sum()}\")","662a9e29":"# Function for drop columns\n\ndef drop_col(trainORtest, col_name):\n    trainORtest.drop(col_name, axis=1, inplace=True)","4d3f866e":"# drop unnecessary column\n\ndrop_col(train, \"keyword\")\ndrop_col(train, \"location\")\n\ndrop_col(test, \"keyword\")\ndrop_col(test, \"location\")","95ed5346":"text_message = train[\"text\"]\nprint(text_message)","cc278201":"def clean_data(name):\n    # Replace email addresses with 'email'\n    processed = name.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n                                     'emailaddress')\n\n    # Replace URLs with 'webaddress'\n    processed = processed.str.replace(r'^http\\:\/\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\/\\S*)?$',\n                                      'webaddress')\n\n    # Replace money symbols with 'moneysymb' (\u00a3 can by typed with ALT key + 156)\n    processed = processed.str.replace(r'\u00a3|\\$', 'moneysymb')\n\n    # Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n    processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n                                      'phonenumbr')\n\n    # Replace numbers with 'numbr'\n    processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n\n    # Remove punctuation\n    processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n\n    # Replace whitespace between terms with a single space\n    processed = processed.str.replace(r'\\s+', ' ')\n\n    # Remove leading and trailing whitespace\n    processed = processed.str.replace(r'^\\s+|\\s+?$', '')\n\n    # change words to lower case - Hello, HELLO, hello are all the same word\n    processed = processed.str.lower()\n    \n    return processed","f46f3725":"clean_train = clean_data(train[\"text\"])\nclean_test = clean_data(test[\"text\"])","0c1e14bb":"from nltk.corpus import stopwords\n\nstop_words = set(stopwords.words(\"english\"))\n\nclean_train = clean_train.apply(lambda x:\" \".join(term for term in x.split() if term not in stop_words))\n\nclean_test = clean_test.apply(lambda x:\" \".join(term for term in x.split() if term not in stop_words))","78b21e39":"clean_train","1788fc9d":"from nltk.stem import PorterStemmer\n\nps = PorterStemmer()\n\nclean_train = clean_train.apply(lambda x:\" \".join([ps.stem(word) for word in x.split()]))\n\nclean_test = clean_test.apply(lambda x:\" \".join([ps.stem(word) for word in x.split()]))","cd235e82":"clean_train","7fdd16b0":"from nltk.stem import WordNetLemmatizer\n\nwl = WordNetLemmatizer()\n\nclean_train = clean_train.apply(lambda x:\" \".join([wl.lemmatize(word) for word in x.split()]))\n\nclean_test = clean_test.apply(lambda x:\" \".join([wl.lemmatize(word) for word in x.split()]))","480de0d3":"clean_test","8dfb411e":"train[\"text\"] = clean_train\ntest[\"text\"] = clean_test","b6aeaa86":"# Spliting train and test set\n\nfrom sklearn.model_selection import train_test_split\n\nseed = 42\n\nX = train.text\ny = train.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)","44cb448a":"# some important libraries\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","3588ed7c":"# accuracy score function\n\ndef acc_summary(pipeline, X_train, y_train, X_test, y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    y_pred = sentiment_fit.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n   \n    print(\"-\"*30)\n    \n    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n    \n    print(\"-\"*30)\n    \n    return accuracy","d696a1bf":"# some model and their performance\n\nnames = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"Bernouli\", \"PassiveAggressiveClassifier\",\n     \"Naive Bayes\", \"SVC\"]\n\nclassifiers = [\n    KNeighborsClassifier(n_neighbors=3),\n    DecisionTreeClassifier(random_state=0),\n    RandomForestClassifier(n_estimators=100),\n    LogisticRegression(),\n    MultinomialNB(),\n    BernoulliNB(),\n    PassiveAggressiveClassifier(max_iter=50),\n    SVC(kernel=\"linear\")\n]\n    \nzipped_clf = zip(names, classifiers)\ntvec = TfidfVectorizer()\n    \ndef compare_clf(classifier=zipped_clf, vectorizer=tvec, n_features=10000, ngram_range=(1, 1)):\n    result = []\n    vectorizer.set_params(stop_words=stop_words, max_features=n_features, ngram_range=ngram_range)\n    for n, c in classifier:\n        checker_pipeline = Pipeline([\n            (\"vectorizer\", vectorizer),\n            (\"classifier\", c)\n        ])\n        clf_acc = acc_summary(checker_pipeline, X_train, y_train, X_test, y_test)\n        print(\"Model result for {}\".format(n))\n        print(c)\n        result.append((n, clf_acc))\n    return result","fc350f31":"trigram_result = compare_clf()","98a64fbe":"trigram_result","f9182c0b":"# prediction\n\ndef prediction(pipeline, testtext):\n    sentiment_fit = pipeline.fit(X_train,y_train)\n    y_pred = sentiment_fit.predict(testtext)\n    \n    return y_pred","e981b800":"# Use TfidfVectorizer\n# use of pipeline\nvectorizer=TfidfVectorizer()\nchecker_pipeline = Pipeline([\n            ('vectorizer', vectorizer),\n            ('classifier', LogisticRegression())\n        ])\nvectorizer.set_params(stop_words=None, max_features=100000, ngram_range=(1,4))\nprediction=prediction(checker_pipeline,test['text'])","fa195d62":"prediction","e305750e":"'''\nindex = test.id\nnewFrame = pd.DataFrame({\"id\":index, \"target\":prediction})\nnewFrame.to_csv(\"realnot.csv\", index=False)\n'''","97ccbf9d":"# Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word. Text preprocessing includes both Stemming as well as Lemmatization.","b5218d3a":"![Sentiment.png](attachment:Sentiment.png)","8a903e08":"# Stemming is the process of producing morphological variants of a root\/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words \u201cchocolates\u201d, \u201cchocolatey\u201d, \u201cchoco\u201d to the root word, \u201cchocolate\u201d and \u201cretrieval\u201d, \u201cretrieved\u201d, \u201cretrieves\u201d reduce to the stem \u201cretrieve\u201d.","268f9a3e":"# Applications of lemmatization are:\n\n*      Used in comprehensive retrieval systems like search engines.\n*      Used in compact indexing\n","712d3e15":"# Natural Language Processing or NLP is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages.","fbd5a07c":"# If you Like, please upvote","ff0d8bac":"# Text processing","3e63f107":"# If you like it, please upvote","ac6d6216":"# The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words.","f2708c3c":"# Implement the best model"}}