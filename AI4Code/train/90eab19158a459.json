{"cell_type":{"624d8423":"code","5f288970":"code","c888c6c6":"code","b3927eeb":"code","ae66ec34":"code","1d61f6f3":"code","84eb7f5f":"markdown","b138fb9d":"markdown","87d53dca":"markdown","53e1d74a":"markdown","d3c7f507":"markdown","a4cba059":"markdown"},"source":{"624d8423":"!pip install bert-for-tf2","5f288970":"import numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nfrom tensorflow_core.python.keras.layers import Dense, Input\n#from tensorflow_core.python.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow_core.python.keras.models import Model\nfrom tensorflow_core.python.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nfrom bert.tokenization import bert_tokenization\nimport math\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\n","c888c6c6":"def clean_text(text):\n    new_text = []\n    for each in text.split():\n        if each.isalpha():\n            new_text.append(each)\n         \n    cleaned_text = ' '.join(new_text)\n    cleaned_text = re.sub(r'https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+','',cleaned_text)\n    \n    return cleaned_text","b3927eeb":"def bert_encode(texts, tokenizer, max_len =512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n\n    for text in texts:\n        text = tokenizer.tokenize(text)\n        text = text[:max_len-2]\n        input_sequence = ['[CLS]'] + text +['[SEP]']\n        pad_len = max_len - len(input_sequence)\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0]*pad_len\n        pad_masks = [1]*len(input_sequence) + [0]*pad_len\n        segment_ids = [0]*max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n\ndef build_bert(bert_layer, max_len =512):\n    adam = Adam(lr=6e-6)\n    #main_input = Input(shape =(max_len,), dtype ='int32')\n    input_word_ids = Input(shape = (max_len,),dtype ='int32')\n    input_mask = Input(shape = (max_len,),dtype ='int32')\n    segment_ids = Input(shape = (max_len,),dtype ='int32')\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:,0,:]\n    out = Dense(1, activation ='sigmoid')(clf_output)\n    \n    model = Model(inputs =[input_word_ids, input_mask, segment_ids], outputs =out)\n    model.compile(optimizer=adam ,loss = 'binary_crossentropy', metrics =['accuracy'])\n\n    return model\n\n\nbert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2', trainable=True)\ntrain_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\ntrain_data['text'] = train_data['text'].apply(lambda x: clean_text(x))\ntest_data['text'] = test_data['text'].apply(lambda x: clean_text(x))\n#train_text = list(train_data['text'])\n#test_text = list(test_data['text'])\n#train_labels = np.array(train_data['target'])\n\nall_models = []\nall_loss = []\nskf = StratifiedKFold(n_splits = 5, random_state =42, shuffle=True)\nfor fold,(train_idx,val_idx) in enumerate(skf.split(train_data['text'],train_data['target'])):\n    print('Fold:'+str(fold))\n    train_input = bert_encode(train_data.loc[train_idx,'text'], tokenizer, max_len=100)\n    train_labels = train_data.loc[train_idx,'target']\n    valid_input = bert_encode(train_data.loc[val_idx,'text'], tokenizer, max_len=100)\n    valid_labels = train_data.loc[val_idx,'target']\n    \n    model = build_bert(bert_layer, max_len=100)\n    model.fit(train_input, train_labels,epochs =3, batch_size = 16)\n    valid_loss, valid_acc = model.evaluate(valid_input,valid_labels, batch_size =16)\n    all_models.append(model)\n    all_loss.append(valid_loss)\n    \n#train_input = bert_encode(train_text, tokenizer, max_len=100)\n#test_input = bert_encode(test_text, tokenizer, max_len=100)\n#train_labels = np.array(train_data['target'])\n#model = build_bert(bert_layer, max_len=100)\n#model.summary()\n#model.fit(train_input, train_labels, epochs =3, batch_size = 16, validation_split=0.2)\n#results = model.predict(test_input).round().astype('int')\n#train_pred = model.predict(train_input).round().astype('int')\n#submission_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n#submission_data['target'] = results\n#submission_data.to_csv('\/kaggle\/working\/submission.csv',index=False, header=True)\n","ae66ec34":"test_text = list(test_data['text'])\ntest_input = bert_encode(test_text, tokenizer, max_len=100)\n#results = np.zeros((test_input[0].shape[0],1))\nmin_loss_index = all_loss.index(min(all_loss))\nresults = all_models[min_loss_index].predict(test_input)\nsubmission_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission_data['target'] = results.round().astype('int')\n#for index,model in enumerate(all_models):\n    \n    #results += model.predict(test_input)\/len(all_models)\n    \n#submission_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n#submission_data['target'] = np.round(results).astype('int')\nsubmission_data.to_csv('\/kaggle\/working\/submission.csv',index=False, header=True)\n","1d61f6f3":"def plot_confusion_matrix(y_true,y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels = np.unique(y_true))\n    cm_sum = np.sum(cm, axis = 1, keepdims=True)\n    cm_percent = cm\/cm_sum.astype('float')*100\n    annot = np.empty_like(cm).astype(str)\n    rows, columns = np.shape(cm)\n    \n    for i in range(rows):\n        for j in range(columns):\n            c = cm[i,j]\n            p = cm_percent[i,j]      \n            if c == 0:\n                annot[i, j] = ''\n            else:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n    \n    #annot = pd.DataFrame(annot)\n    cm = pd.DataFrame(cm, index = np.unique(y_true),columns = np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig,ax = plt.subplots(figsize=(8,8))\n    plt.title('Confusion Matrix for Bert model')\n    sns.heatmap(cm, annot=annot, ax=ax,fmt='' )\n\n\ntrain_text = list(train_data['text'])\ntrain_labels = list(train_data['target'])\nall_train_input = bert_encode(train_text, tokenizer, max_len =100)\ntrain_pred = np.zeros((all_train_input[0].shape[0],1))\nfor model in all_models:\n    train_pred += model.predict(all_train_input)\/len(all_models)\n    \ntrain_pred = np.round(train_pred).astype('int')\n\nplot_confusion_matrix(train_labels,train_pred)\n","84eb7f5f":"Invoke all the needed libraries","b138fb9d":"Generate the output based on models after K-fold validation","87d53dca":"Install bert for tensorflow version > 2.0. It is important because common bert does not support tensorflow 2.0+","53e1d74a":"Clean Text","d3c7f507":"draw confusion matrix for training data","a4cba059":"Tokenize the texts and use K-fold validation to build models"}}