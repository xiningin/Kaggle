{"cell_type":{"0634b11b":"code","7f9c1caa":"code","e7c84d81":"code","c899714b":"code","46a90fcb":"code","0bf58ca4":"code","2ae5ef98":"code","b5546e6b":"code","e2168111":"code","f9cf4be6":"code","ed669ccc":"code","2b1a2854":"code","b5fa9c0b":"code","787fb0fa":"code","1c2ff2de":"code","c74daee0":"code","14037dfe":"code","ce2a6bba":"code","87bc6cd2":"code","36b9d491":"code","28ea8b8a":"code","8210e6c3":"markdown","14641f78":"markdown","305c672a":"markdown","ed7def27":"markdown","715014cd":"markdown","512b93a7":"markdown","dc3ab6ff":"markdown","d722ffd4":"markdown","52881bf0":"markdown","b703eb9c":"markdown"},"source":{"0634b11b":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom matplotlib import pyplot\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7f9c1caa":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv', index_col = 'id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv', index_col = 'id')\n\nTARGET_NAME = 'target'\n\ntrain[TARGET_NAME] = train[TARGET_NAME].str.slice(start=6).astype(int) - 1\n\nall_df = pd.concat([train, test]).drop('target', axis = 1).reset_index(drop=True)\n\nall_features = all_df.columns","e7c84d81":"!pip install sweetviz -q\nimport sweetviz as sv","c899714b":"feature_config = sv.FeatureConfig(force_num=[\"target\"])\n\ntps_comparison_report = sv.compare([train,'Train'], [test,'Test'], target_feat='target', feat_cfg = feature_config)\ntps_comparison_report.show_notebook(w=840, h=8000, scale=0.8)","46a90fcb":"!pip install -U dataprep -q","0bf58ca4":"from dataprep.eda import create_report\nfrom dataprep.eda import plot_diff","2ae5ef98":"create_report(all_df).show()","b5546e6b":"plot_diff([train.drop('target', axis = 1),test])","e2168111":"# This part of code is from MLJAR AutoML - https:\/\/mljar.com\/automated-machine-learning\/k-means-features\/\n\n#df_all_scaled = pd.DataFrame()\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import StandardScaler\n\nn_clusters = min(max(8, int(np.log10(all_df.shape[0]) * 8)), all_df.shape[1])\n\nscale = StandardScaler(copy=True, with_mean=True, with_std=True)\ndf_all_scaled = scale.fit_transform(all_df)\n\nkmeans = MiniBatchKMeans(n_clusters=n_clusters, init=\"k-means++\")\nkmeans.fit(df_all_scaled)\n\nn_clusters = kmeans.cluster_centers_.shape[0]\nnew_features = [f\"Dist_Cluster_{i}\" for i in range(n_clusters)]\nnew_features += [\"Cluster\"]\n\ndistances = kmeans.transform(df_all_scaled)\nclusters = kmeans.predict(df_all_scaled)","f9cf4be6":"all_df = pd.concat([pd.DataFrame(all_df, columns = all_features), pd.DataFrame(distances, columns = new_features[:-1])], axis = 1)\nall_df[new_features[-1]] = pd.Series(clusters) \n\ntrain_df = all_df[:len(train)]\ntrain_df['target'] = train.target\ntest_df = all_df[len(train):]","ed669ccc":"train_df.head(5)","2b1a2854":"test_df.head(5)","b5fa9c0b":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 8 * 3600 # Time in seconds for automl run","787fb0fa":"pip install -U lightautoml -q","1c2ff2de":"from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\n\nimport pandas as pd","c74daee0":"task = Task('multiclass',)\n\nroles = {\n    'target': TARGET_NAME,\n    'drop': ['id'],\n}","14037dfe":"automl = TabularUtilizedAutoML(task = task, \n                               timeout = TIMEOUT,\n                               cpu_limit = N_THREADS,\n                               general_params = {'use_algos': [['lgb_tuned', 'cb_tuned'], ['lgb_tuned', 'cb_tuned']]},\n                               tuning_params = {'max_tuning_time': 1200},\n                               reader_params = {'n_jobs': N_THREADS},\n                               max_runs_per_config=10\n                               )\noof_pred = automl.fit_predict(train, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","ce2a6bba":"test_pred = automl.predict(test)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred[:10], test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(log_loss(train_df[TARGET_NAME].values, oof_pred.data)))","87bc6cd2":"submission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\n\nsubmission.iloc[:, 1:] = test_pred.data\nsubmission.to_csv(\"lightautoml_submission.csv\", index = False)","36b9d491":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\n\npalette = itertools.cycle(sns.color_palette())\n\nplt.figure(figsize=(16, 8))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    c = next(palette)\n    sns.histplot(submission, x = f'Class_{i+1}', color=c)\nplt.suptitle(\"Class prediction distribution\")","28ea8b8a":"submission.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","8210e6c3":"## 2.EXPERIMENTAL STEP - K-Means Features (inspired by MLJAR)\n","14641f78":"We make comparision analysis. Sweetviz provides more methods:\n\nUse this version when you have 2 data sets to compare together (e.g. Train versus Test). This is a very useful report!\n> **compare(source: Union[pd.DataFrame, Tuple[pd.DataFrame, str]],\n>             compare: Union[pd.DataFrame, Tuple[pd.DataFrame, str]],\n>             target_feat: str = None,\n>             feat_cfg: FeatureConfig = None,\n>             pairwise_analysis: str = 'auto')**\n\n\nUse this when you want to compare 2 some populations within the same dataset. This is also a very useful report, especially when coupled with target feature analysis!\n>**compare_intra(source_df: pd.DataFrame,\n                  condition_series: pd.Series,\n                  names: Tuple[str, str],\n                  target_feat: str = None,\n                  feat_cfg: FeatureConfig = None,\n                  pairwise_analysis: str = 'auto')**\n\nUse this version when there is only a single dataset to analyze, and you do not wish to compare subpopulations together (e.g. male vs female)\n>**analyze(source: Union[pd.DataFrame, Tuple[pd.DataFrame, str]],\n        target_feat: str = None,\n        feat_cfg: FeatureConfig = None,\n        pairwise_analysis: str = 'auto')**\n\nThe best example I found: https:\/\/colab.research.google.com\/drive\/1-md6YEwcVGWVnQWTBirQSYQYgdNoeSWg?usp=sharing#scrollTo=oMV8HHX4t1aA","305c672a":"## 3. AUTOML - LightAutoML","ed7def27":"## 3. SUMBISSION","715014cd":"Let's take LightAutoML make the magic :)","512b93a7":"#### I appreciate and feedback and support. Thank you Kaggles! ","dc3ab6ff":"### 1A. SWEETVIZ","d722ffd4":"## This is my MEGA Auto notebook - AutoEDA + new features (read from MLJAR logs) + AutoML (LightAutoML)\n\nI am really interested in magic of Auto solutions. They are better and better everyday. Recently (TPS05) I spent a lot of time playing with AutoML and trying to find best solution. They are ideal for rapid prototyping and learning really great models. In this notebook I am going to show you full Auto Pipeline:\n\n<div class=\"alert alert-success\">\n  <strong>Notebook scope:<\/strong>\n    <ul>\n        <li>AutoEDA using:<\/li>\n        <ul>\n            <li>sweetviz<\/li>\n            <li>dataprep<\/li>\n        <\/ul>\n        <li>New features inspired by MLJAR - K-Means Features<\/li>\n        <li>AutoML using LightAutoML<\/li>\n        <li>Data experiments - PowerTransformer (Yeo-Johnson Transform)<\/li>\n    <\/ul>\n<\/div>","52881bf0":"## 1. AutoEDA\n\nLet's make very fast EDA analysis. I think that in this competition it really powerful. In 30 second (just 4 lines of code) you achieve full overview of data we have in dataset.  No more code needed at this time.\n\n**Click on report - you can find more details about feature**","b703eb9c":"### 1B. DATAPREP"}}