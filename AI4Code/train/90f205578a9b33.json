{"cell_type":{"288f34de":"code","76556a77":"code","f780c25d":"code","1dbe7b2d":"code","9a589f59":"code","05e9f798":"code","9ea9d55f":"code","d33e3890":"code","41558d60":"code","6e455d19":"code","6655efb7":"code","413c4df6":"code","1f148fcc":"code","cbfd6cce":"code","34ef0df5":"code","24458bd3":"code","5fd8c354":"code","4a75b1b1":"code","f5490149":"code","9dcf08bf":"code","8498348a":"code","9ced2c0e":"code","9c13b135":"code","023841c3":"markdown","c7a84da9":"markdown","d29146d1":"markdown","db3e2209":"markdown","6b41e73c":"markdown"},"source":{"288f34de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\n#pad_sequence is used to make sure that input length of every sentence is same\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM,Dropout\nfrom tensorflow.keras.layers import Dense\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","76556a77":"df=pd.read_csv(\"\/kaggle\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv\")\ndf.head()","f780c25d":"#Combining lower values and assigning it as 0\nfor j in [1,2,3]:\n    df.loc[df[\"Rating\"]==j,\"Rating\"]=0","1dbe7b2d":"#Combining lower values and assigning it as 1\nfor i in [5,4]:\n    df.loc[df[\"Rating\"]==i,\"Rating\"]=1","9a589f59":"df.head()","05e9f798":"sns.countplot(x=\"Rating\",data=df)","9ea9d55f":"df.isnull().sum()","d33e3890":"#Splitting into independent and dependent variable\nX=df[\"Review\"]\ny=df[\"Rating\"]","41558d60":"X.shape,y.shape","6e455d19":"#Setting the vocabulary size\nvoc_size=10000","6655efb7":"#Downloading all the stopwords\nnltk.download(\"stopwords\")","413c4df6":"ps=PorterStemmer()\ncorpus=[]\nfor i in range(len(X)):\n    #Removing everything other than alphabets from the text\n    review=re.sub(\"[^a-zA-Z]\",\" \",X[i])\n    #Converting the reviews into lowercase texts\n    review=review.lower()\n    review=review.split()\n    #removing all the stopwords from the reviews\n    review=[ps.stem(word) for word in review if word not in stopwords.words(\"english\")]\n    review=\" \".join(review)\n    corpus.append(review)","1f148fcc":"#we will do one hot encoding for the corpus. It is alloting every word an index according to the vocabulary size\nonehot=[one_hot(words,voc_size) for words in corpus]","cbfd6cce":"#We are converting every sentence into same length by adding 0 in front of the text if it is not of the desired length.\n#Desired length = maximum length of the sentence that is present in the review\nembedded_docs=pad_sequences(onehot,padding=\"pre\")\nembedded_docs","34ef0df5":"len(embedded_docs)","24458bd3":"len(embedded_docs[0])","5fd8c354":"embedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=len(X[0])))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])","4a75b1b1":"model.summary()","f5490149":"len(embedded_docs),y.shape","9dcf08bf":"#Creating new independent and dependent variables\nimport numpy as np\nX_final=np.array(embedded_docs)\ny_final=np.array(y)","8498348a":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_final,y_final,test_size=0.3,random_state=0)","9ced2c0e":"history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5,batch_size=64)","9c13b135":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","023841c3":"# Data Preprocessing","c7a84da9":"# Visualising the result","d29146d1":"# Creating the Model","db3e2209":"# Import Dataset","6b41e73c":"# Training the model"}}