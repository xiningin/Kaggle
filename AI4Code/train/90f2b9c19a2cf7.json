{"cell_type":{"03c32a0d":"code","a645adbf":"code","ef953b37":"code","fc61b596":"code","18e506ef":"code","9038f63e":"code","148adaef":"code","4f33d399":"code","434d2cbd":"code","e7e8c4a3":"code","6c4ddfac":"code","901c0371":"code","d1789da6":"code","6313b39e":"code","352baeb6":"code","4f62a0d6":"code","462cd1ac":"code","8e7a6144":"code","98366696":"code","04fba560":"code","ffcfb52d":"code","0b11eb5f":"code","4e87cce7":"markdown","4d0ab748":"markdown","3ffed97b":"markdown","9a7d63d0":"markdown","280df600":"markdown","167384cb":"markdown","eb469b9b":"markdown","0728d45a":"markdown","9703158c":"markdown","5729f098":"markdown","a6ee9533":"markdown","8e3a41d3":"markdown","6a9c96a0":"markdown","b0da4546":"markdown","18b72910":"markdown","db088a28":"markdown","3ce72b1a":"markdown","6b9fc6e3":"markdown","51983258":"markdown","1c82e767":"markdown"},"source":{"03c32a0d":"import warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nimport os\nimport re\nimport json\nimport glob\nfrom collections import defaultdict\nfrom textblob import TextBlob\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\n\nimport nltk\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 4000000\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom tqdm.autonotebook import tqdm\nimport string\n\n%matplotlib inline\n\nos.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/')","a645adbf":"# reading csv files and train & test file paths\ntrain_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","ef953b37":"train_df.head(6)","fc61b596":"train_df.info()","18e506ef":"[print(f\"{col}:{len(train_df[col].unique())}\") for col in train_df.columns]   #finding unique values in each column","9038f63e":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","148adaef":"%%time\ntqdm.pandas()   #tqdm is used to show any code running with a progress bar. \ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","4f33d399":"train_df.head()","434d2cbd":"%%time\ntqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))","e7e8c4a3":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n#     text = re.sub(\"\/'+\/g\", ' ', text)\n    \n    return text","6c4ddfac":"%%time\ntqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","901c0371":"# %%time \n# tqdm.pandas()\n# sample_sub['text'] = sample_sub['text'].progress_apply(text_cleaning)","d1789da6":"words =list( train_df['cleaned_label'].values)\nstopwords=['ourselves', 'hers','the','of','and','in', 'between', 'yourself', 'but', 'again','of', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\nsplit_words=[]\nfor word in words:\n    lo_w=[]\n    list_of_words=str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\nfor wordlist in split_words:\n    allwords += wordlist","6313b39e":"mostcommon = FreqDist(allwords).most_common(100)\nwordcloud = WordCloud(width=1600, height=800, background_color='white', stopwords=STOPWORDS).generate(str(mostcommon))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Top 100 Most Common Words in cleaned_label', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()\n\nmostcommon_small = FreqDist(allwords).most_common(25)\nx, y = zip(*mostcommon_small)\nplt.figure(figsize=(50,30))\nplt.margins(0.02)\nplt.bar(x, y)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.tight_layout(pad=0)\nplt.title('Freq of 25 Most Common Words in cleaned_label', fontsize=60)\nplt.show()","352baeb6":"def prepare_text(text, nlp=nlp):\n    '''\n    Returns the text after stop-word removal and lemmatization.\n    text - Sentence to be processed\n    nlp - Spacy NLP model\n    '''\n    doc = nlp(text)\n    lemma_list = [token.lemma_ for token in doc if not token.is_stop]\n    lemmatized_sentence = ' '.join(lemma_list)\n    \n    return lemmatized_sentence","4f62a0d6":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","462cd1ac":"# %%time\n# tqdm.pandas()\n# train_df['text'] = train_df['text'].progress_apply(prepare_text)","8e7a6144":"temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)\nid_list = []\nlables_list = []\nfor index, row in tqdm(sample_sub.iterrows()):\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","98366696":"submission = pd.DataFrame()\nsubmission['Id'] = id_list\nsubmission['PredictionString'] = lables_list","04fba560":"# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\nsubmission.head()","ffcfb52d":"submission.to_csv('submission.csv', index=False)","0b11eb5f":"submission","4e87cce7":"<a id='1'><\/a>\n# <p style=\"background-color:red; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Table of Content<\/p>\n* [1. Importing necessary modules and libraries\ud83d\udcda](#1)\n* [2. Data Exploration\ud83d\udd0d](#2)\n* [3. Data Cleaning\ud83d\udd27](#3)\n* [4. Data Vizualization\ud83c\udfa8](#4)\n* [5. Baseline model and Submission\ud83d\udcdd](#5)","4d0ab748":"We have our text appended in our train dataframe.","3ffed97b":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">1. Importing necessary modules and libraries\ud83d\udcda<\/p>","9a7d63d0":"# Upvote ........","280df600":"### 100 Most common words (cleaned_label) - WordCloud","167384cb":"Great! we don't have any null values.","eb469b9b":"<a id='0'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Coleridge Initiative\ud83d\udd8b\ud83d\udcdd - EDA\ud83d\udcda & Baseline Model\ud83c\udfaf <\/p>","0728d45a":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Data Exploration\ud83d\udd0d<\/p>","9703158c":"### Special thanks to helper notebooks \ud83d\ude4f\ud83c\udffb:- \n1. [Tabular Data Preparation, Basic EDA and Baseline](https:\/\/www.kaggle.com\/manabendrarout\/tabular-data-preparation-basic-eda-and-baseline)\n2. [Coleridge - Data Loading, EDA & Simple Submission](https:\/\/www.kaggle.com\/poornap\/coleridge-data-loading-eda-simple-submission)\n3. [Coleridge Initiative - EDA + Na\u00efve Submission \ud83d\udcda](https:\/\/www.kaggle.com\/josephassaker\/coleridge-initiative-eda-na-ve-submission\/data)","5729f098":"<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. Baseline model and Submission\ud83d\udcdd<\/p>","a6ee9533":"We have our data cleaned!","8e3a41d3":"We are provided with 4 main pieces of data:\n\n* `train.csv:` The CSV file containing all the metadata of the publications, such as their title and the dataset they utilize.\n* `train:` The directory containing the actual publications that are referenced in train.csvin JSON format.\n* `test:` The directory containing the actual publications that will be used for testing purposes (thus, with no ground truth CSV file available).\n* `sample_submission.csv:` The CSV file containing all the publications IDs in the test set, for which we'll have to populate the prediction column.","6a9c96a0":"Also, we have the text of for the sample_submission file","b0da4546":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Cleaning\ud83d\udd27<\/p>","18b72910":"<a id='0'><\/a>\n# <p style=\"background-color:blue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px; color:white;\">Please upvote the notebook as well if you find it useful and forking the notebook\ud83c\udfaf <\/p>","db088a28":"It takes time!\ud83d\ude44","3ce72b1a":"# <p style=\"font-family:newtimeroman; text-align:center; fontsize:150%\">Coleridge Initiative - Show US the Data<br>Discover how data is used for the public good<\/p>\n![CI_logo.jpg](attachment:CI_logo.jpg)","6b9fc6e3":"<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. Data Vizualization\ud83c\udfa8<\/p>","51983258":"<a id='1'><\/a>\n## <p style=\"text-align:center;\">Data Description<\/p>\ntrain.csv - labels and metadata for the training set train\/test directory - the full text of the training\/test set's publications in JSON format, broken into sections with section titles\n\n* `id` - publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets.\n* `pub_title` - title of the publication (a small number of publications have the same title).\n* `dataset_title` - the title of the dataset that is mentioned within the publication.\n* `dataset_label` - a portion of the text that indicates the dataset.\n* `cleaned_label` - the dataset_label, as passed through the clean_text function from the Evaluation page.\n\nsample_submission.csv - a sample submission file in the correct format.\n* `Id` - publication id.\n* `PredictionString` - To be filled with equivalent of cleaned_label of train data.","1c82e767":"## Hey There! \ud83d\ude4c\ud83c\udffb\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f\nIn this notebook basically we have to predict text for some strings by using nlp techniques.\n*** \n>The objective of the competition is to identify the mention of datasets within scientific publications.\n\nThis competition challenges data scientists to show how publicly funded data are used to serve science and society. Evidence through data is critical if government is to address the many threats facing society, including; pandemics, climate change, Alzheimer\u2019s disease, child hunger, increasing food production, maintaining biodiversity, and addressing many other challenges. Yet much of the information about data necessary to inform evidence and science is locked inside publications.\n\nThe Coleridge Initiative is a not-for-profit organization originally established at New York University. It was set up in order to inform the decision-making of the Commission on Evidence-based Policymaking and has since worked with dozens of government agencies at the federal, state, and local levels to ensure that data are more effectively used for public decision-making.\n\nIt achieves this goal by working with the agencies to create value for the taxpayer from the careful use of data by building new technologies to enable secure access to and sharing of confidential microdata and by training agency staff to acquire modern data skills."}}