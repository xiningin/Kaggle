{"cell_type":{"a9ed3641":"code","cffbed1a":"code","a1338869":"code","46efc113":"code","5eb4364f":"code","8061c7c2":"code","6532c0ac":"code","b1fa7972":"code","f282a793":"code","36cce303":"code","e81448a5":"code","37b78197":"code","7495edfb":"code","1726feae":"code","35280ad6":"code","2984f0c7":"markdown","3efdb633":"markdown","c9692f07":"markdown","46e8242d":"markdown","e0408eb9":"markdown","b0f398c1":"markdown","32cc6a7b":"markdown","8b9376c2":"markdown","db49726c":"markdown","89c45b21":"markdown","d496ac6e":"markdown","506586bf":"markdown","b7976fa5":"markdown","3e5b789f":"markdown","034884c7":"markdown"},"source":{"a9ed3641":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport nltk\nfrom sklearn import feature_extraction, linear_model, model_selection\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cffbed1a":"train_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntrain_data.info()","a1338869":"test_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest_data.info()","46efc113":"train_data.head()","5eb4364f":"train_data = train_data.fillna(value = 'No data')\ntrain_data.info()","8061c7c2":"train_data.target.value_counts().plot(kind='bar')","6532c0ac":"most_freq_location = train_data.location.value_counts()[:10].sort_values(ascending = False)\nmost_freq_location = most_freq_location.drop('No data')\nmost_freq_location","b1fa7972":"#from nltk.corpus import stopwords\n#stop_words = stopwords.words('english')\n\n#It seems that removing stopwords makes the model less efficient. I'm going to comment this part out for now.","f282a793":"#train_data['text'] = train_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))#\ntrain_data['text'] = train_data['text'].str.replace('[^\\w\\s]','')\ntrain_data['text'] = train_data['text'].str.replace(\"https?:\/\/[A-Za-z0-9.\/]*\", \"\")\nprint(train_data.text)","36cce303":"count_vectorizer = feature_extraction.text.CountVectorizer()","e81448a5":"train_vectors = count_vectorizer.fit_transform(train_data[\"text\"])\n\ntest_vectors = count_vectorizer.transform(test_data[\"text\"])","37b78197":"train_y = train_data.target\ntrain_x = train_vectors","7495edfb":"x_train, x_val, y_train, y_val = model_selection.train_test_split(train_x, train_y)\nclf = linear_model.LogisticRegression(random_state=0, max_iter = 150).fit(x_train, y_train)\nclf.score(x_val, y_val)","1726feae":"predictions = clf.predict(test_vectors)","35280ad6":"output = pd.DataFrame({'id': test_data.id,\n                       'target': predictions})\noutput.to_csv('submission.csv', index = False)\nprint('submission saved!')","2984f0c7":"# 2. Training dataset overview","3efdb633":"1. Loading the data\n2. Data overview\n3. Data cleanup\n4. Running the model\n","c9692f07":"# 3. Processing tweet text for prediction\n","46e8242d":"Now, that we've established our training subsets, we can set up our model. Because our dependent variable is binary, I am going to try and evaluate logistic regression model. ","e0408eb9":"Ok, the validation score (mean accuracy on our test subset 'x_val' and labels 'y_val') is pretty high. Now, to the prediction:","b0f398c1":"Now the dataset is looking well, we can take a closer look on it.","32cc6a7b":"In order to use a Logistic Regression model we have to vectorize our tweets first. We're going to do that using scikitlearn's CountVectorizer.","8b9376c2":"# 1. Loading the data","db49726c":"# NLP with disaster tweets\n\n\nThis notebook is an attempt to create a submision to *Real or not? NLP with disaster tweets* competition.\n\nAs I have little experience with natural language processing, this will be more less trial and error thing. \n\n","89c45b21":"... to be expanded","d496ac6e":"Before working on tweet text, we're going to clean up the text: remove stop words, punctuation and urls.","506586bf":"So, as we now have both datasets loaded, we can take a closer look at the training data.","b7976fa5":"Looks like we have countries and cities mixed. I don't want to clean this up yet as we might need the most exact location there is available, but it looks like the vast majority of tweets is from the US.","3e5b789f":"We have to handle the missing values in 'keyword' and 'location' columns. For both of them a good way to do this is to just put \"no data\" in the empty cells.","034884c7":"# 4. Model fitting and prediction "}}