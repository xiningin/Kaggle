{"cell_type":{"d612e149":"code","f7a2caa6":"code","71984c5e":"code","f8b8df80":"code","6f9085b2":"code","7e848d38":"code","659a3645":"code","ef2f89a0":"code","2d5b40a7":"code","ef28b529":"code","1c270083":"code","4e66b37f":"code","0e50f4f1":"markdown","9057cd1e":"markdown","d63c4674":"markdown","370b11d2":"markdown","9d8eae2b":"markdown","b2ae81da":"markdown","49e82486":"markdown","fa74bacf":"markdown","28b09620":"markdown","595204a9":"markdown","3f23c575":"markdown","d001f7fd":"markdown"},"source":{"d612e149":"from gensim.utils import tokenize\nimport re\nimport tensorflow as tf\nimport numpy as np\nfrom spacy.lang.es import Spanish\nimport matplotlib.pyplot as plt\n%matplotlib inline","f7a2caa6":"document = input()","71984c5e":"#Without spaces and all letters lower\nc = document.strip().lower()\ndoc = c.split()\n\nnlp = Spanish()\ntext = nlp(c)\n\n#Tokenize by words\ntoken_list = [w.text for w in text]\n\nwords_set = set(token_list)\nvocab_size = len(words_set)\n\n#Tokenize by senceteces\nsenteces = re.compile('[.,]').split(c)","f8b8df80":"word2idx = {}\nidx2word = {}\n\nfor index, word in enumerate(words_set):\n  word2idx[word]  = index\n  idx2word[index] = word","6f9085b2":"def to_categorical(word_idx, vocab_size):\n  one_hoted = np.zeros(vocab_size)\n  one_hoted[word_idx] = 1\n  return np.array(one_hoted)","7e848d38":"def skip_gram(senteces, wz):\n\n  data =[]\n\n  for sentece in senteces:\n    sentece = sentece.split()\n    for i, word in enumerate(sentece):\n      for nb_word in sentece[max(i-wz, 0): min(i+wz, len(sentece))+1]:\n        if word != nb_word:\n           data.append([word, nb_word])\n  return data","659a3645":"class word2vec:\n  def __init__(self, vocab_size=0, embedding_size=16, optimizer='sgd', epochs=1000):\n    self.vocab_size = vocab_size\n    self.embedding_size = embedding_size\n    self.epochs= epochs\n    if optimizer == 'adam':\n      self.optimizer = tf.optimizers.Adam()\n    else:\n      self.optimizer = tf.optimizers.SGD()\n  \n  def vectorized(self, word_idx):\n    return (self.w1+self.b1)[word_idx]\n  \n  def train(self, x_train=None, y_train=None):\n    if x_train is not None and y_train is not None:\n    \n      #first hidden layer\n      self.w1 = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_size], dtype=tf.float32)) #weights matrix\n      print(self.w1.shape)\n      self.b1 = tf.Variable(tf.random.normal([self.embedding_size])) #Bias vector\n\n      #output layers\n      self.w2 = tf.Variable(tf.random.normal([self.embedding_size, self.vocab_size], dtype=tf.float32))\n      self.b2 = tf.Variable(tf.random.normal([self.vocab_size], dtype=tf.float32))\n      \n      \n      for epoch in range(self.epochs):\n        with tf.GradientTape() as t:\n          loss = []\n          #Start train\n          f_out = tf.add(tf.matmul(x_train, self.w1), self.b1)\n          output = tf.nn.softmax(tf.add(tf.matmul(f_out, self.w2), self.b2))\n\n          #Compute loss\n          cross_entropy_loss = tf.reduce_mean(-tf.math.reduce_sum(y_train * tf.math.log(output), axis=[1]))\n            \n\n        gradients = t.gradient(cross_entropy_loss, [self.w1, self.b1, self.w2, self.b2])\n        self.optimizer.apply_gradients(zip(gradients,[self.w1, self.b1, self.w2, self.b2]))\n        loss.append(cross_entropy_loss)\n        if epoch % 50 == 0:\n          print('Loss {}'.format(cross_entropy_loss))\n\n    return loss","ef2f89a0":"skip = skip_gram(senteces, wz = 2)\n\nx_train = []\ny_train = []\n\nfor data_word in skip:\n   x_train.append(to_categorical(word2idx[ data_word[0] ], vocab_size))\n   y_train.append(to_categorical(word2idx[ data_word[1] ], vocab_size))\n\nx_train = np.asarray(x_train, dtype='float32')\ny_train = np.asarray(y_train, dtype='float32')\nprint('x_train dimentions: {}, y_train dimentions {}'.format(x_train.shape, y_train.shape))","2d5b40a7":"w2v = word2vec(vocab_size=vocab_size, embedding_size=50,  optimizer='adam', epochs=10000)\nloss = w2v.train(x_train, y_train)","ef28b529":"word_vecs = []\n\nfor word in words_set:\n vector = w2v.vectorized(word2idx[word])\n word_vecs.append(vector)\n\nvectors = np.array(word_vecs)","1c270083":"from sklearn.manifold import TSNE\nfrom sklearn import preprocessing\n\nmodel = TSNE(n_components=2)\nvectors = model.fit_transform(vectors) \n\n","4e66b37f":"x = []\ny = []\nfor coordinate in vectors:\n  x.append(coordinate[0])\n  y.append(coordinate[1])\n\n\nplt.figure(figsize=(15,15))\nfor i, word in enumerate(words_set):\n  plt.scatter(x[i], y[i])\n  plt.annotate(word,\n               xy=(x[i], y[i]),\n               xytext=(5, 2),\n               textcoords='offset points',\n               ha='right',\n               va='bottom')\nplt.show()","0e50f4f1":"\n> # <strong>Read data<\/strong>\n\n\nThe first step we must do to work with text and models and machine learning is to pre-process our data as these models do not work with text, they love numbers, the first step is to take that dataset of our choice and read them, usually we find in these files in .json or .txt format, however this time we will use the python input function and read our data.\n\n<h2>Lowercase and white spaces<\/h2>\n\nCapital letters and indentations have a lot of grammatical meaning and for us humans it is very important, however, for a Machine Learning model in this case, this is just noise and would not contribute anything on the contrary be cause us problems for that reason we put everything in small letters and suppress the indentations.\n\n<h2>Tokenization<\/h2>\n\nLet's think a little if our models are fanatics of numbers. How can we transform those character chains into numbers? *IDEA*: What if we try to code the words as numbers? If to begin with we must tokenize and it is simply to take all our text and make a list with each and every word, that is to say, each element of the list will be a component of our list, playing with the ways of tokenizing and the methods of python we can tokenize by words or by sentences, we will see that this of is particularly useful soon.\n\n\n\n","9057cd1e":"\n\n> # <strong>Diccionario<\/strong>\n\n\u00a1Necesitaremos crear nuestro propio diccionario! Ya que nuestro dataset contiene n palabras distintas nuestra lista ser\u00e1 de longitud n. Para navegar en ella crearemos un diccionario que dado una pabalabra nos regrese en indice y dado un indice nos regrese una palabra.","d63c4674":"\n\n> # <Strong>Modelo<\/strong>\n\nEl modelo es tan sencillo como un una simple red neuronal multicapa con una capa oculta la cual crearemos sin los API, por el contrario crearemos las matrices desde cero, para que luego sea mas sencillo tomarlas.\n\n\n","370b11d2":"\n\n> # <strong> Skip-Gram <\/strong>\n\nWhen we train a Supervise Deep Learning we need Data and Label, but in this case we only have corpus data. Then we build and algorithm and generate label.. if our sentence is a list of word so, we take a simple word and take 2 neighbor from the rigth side and 2 neighbor from the left side and these are the label for this simple word.\n\nNow let's think about, why does it make sense.\n\nOur problem is that with one-hot representation all words has the same distance on a plane, I mean with one-hot representation we say that, each word has the same relationship, for example, Queen, Princess, and Car have the same relationship and this is not correct because, Queen and princess have more relationship than Queen and Car.. \n\nWith skip-gram we take a word and the neighbors are the label, neighbor word could have a relationship, So a neural netword learn to predict a neighbor word given a word to solve this task the neural network learn to encode this one-hot vector to classify it. Then if we take the hidden layer we have a representation where word that have a strong relationship are closer than another word that not.\n","9d8eae2b":"\n\n> # <strong>PREPROCESSING<\/strong>\n\n<h2>Leer datos<\/h2>\n<body>El primer paso que debemos realizar para trabajar con texto y modelos e machine learning es preprocesar nuestros datos ya que dichos modelos no trabajan con texto, a ellos le encantan los n\u00fameros, el primer paso es tomar ese dataset de nuestro gusto y leerlos, usualmente encontramos entos archivos en formato .json o .txt, sin embargo esta vez usaremos la funci\u00f3n input de python y as\u00ed leeremos nuestros datos.\n<\/body>\n\n<h2>Min\u00fasculas y espacios en blanco<\/h2>\n<body>\nLas may\u00fasculas y las sangr\u00edas tienen mucho significado gramatical y para nosotros los humanos es muy importante, sin embargo, para un modelo de Machine Learning en este caso, \u00e9sto es s\u00f3lo ruido y no aportarian nada por lo el contrario ser causarnos problemas por esa raz\u00f3n ponemos todo en minusculas y suprimimos las sangr\u00ecas.\n<\/body>\n\n<h2>Tokenizar<\/h2>\n<body>\nPensemos un poco si nuestros modelos son fan\u00e1ticos de los n\u00fameros \u00bfComo podemos transformar esas cadenas de caracteres a numeros? \n*IDEA*: \u00bfY si tratamos codificar las palabras como numeros? SI entonces para empezar a con ello debemos tokenizar y es simplmente tomar todo nuestro texto y hacer una lista con todas y cada una de las palabras, es decir, cada elemento de la lista ser\u00e1 una compomente de nuestra lista, jugando con las maneras de tokenizar y los metodos de python podemos tokenizar por palabras o por oraciones, veremos que esto de es particularmente \u00fatil en breve.\n<\/body>","b2ae81da":"\n\n> # <strong>One-Hot Encoding<\/strong>\n\nLa primera idea para trabajar con texto fue decir transformemos cada palabra del diccionario en un numero, y hacemos one-hot encoding, es decir, si tenemos n palabras en el diccionario creamos un vector de tama\u00f1o n y si tomamos la n-\u00e9sima palabra entonces la n-\u00e9sima compomentes del vector ser\u00e1 1 y el resto cero. *Anlog\u00eda* Eso es como tomar un libro completo para cada palabra. *risas*\n\n\u00c9ste razonamiento es muy \u00f9til cuando en muchos casos, el problema de trabajar con texto y one hot-encoding es que estamos diciendo que todas las palabras del diccionario son equi-distantes, es decir, estamos afirmando qu\u00e9 la palabras como Manzana, Pera y Herramienta tienen la misma relaci\u00f3n y eso no es cierto. \n\nPara corregir \u00e9ste problema naci\u00f3 [word2vec](https:\/\/arxiv.org\/abs\/1301.3781) \u00e9ste modelo de deep learning trata de predecir dada una cuales son los vecinos (Skip-Gram) ya veremos como funciona esto.","49e82486":"![](https:\/\/chrisalbon.com\/images\/machine_learning_flashcards\/One-Hot_Encoding_print.png)","fa74bacf":"\n\n>  # <Strong>Model<\/strong>\n\n\nThe model is as simple as a simple multi-layer neural network with a hidden layer which we will create without the APIs, on the contrary we will create the arrays from scratch, so that later it will be easier to take them.","28b09620":"\n\n> # <strong>Dictionary<\/strong>\n\nWe will need a own dictionary, and thus to get a word given a number and get a number given a word. This will has more sense later.","595204a9":"![](https:\/\/cdn-images-1.medium.com\/max\/1600\/0*Q7o7qvuJf7W6JNeY.png)","3f23c575":"![](https:\/\/www.researchgate.net\/publication\/334209824\/figure\/fig3\/AS:776808106577938@1562216893819\/Word2vec-skip-gram-and-analogies-a-Target-words-LiCoO2-and-LiMn2O4-are-represented.png)","d001f7fd":"\n\n> # <strong>Skip-gram<\/strong>\n\nSi queremos entrenar un modelo de machine learning de manera supervisada necesitamos etiquetas para ense\u00f1arle al modelo a que categoria pertenece cada entrada y s\u00f2lo tenemos un corpus de texto? Entonces contruimos un algoritmo para generar esas etiquetas. Dado el problema queremos predecir las palabras vecinaas dada alguna palabra. Con skip-gram nos paramos en cada una de las palabras de la oraci\u00f3n y tomamos las dos de la derecha y las dos de la izquierda, esas cuatro ser\u00e1n las etiquetas para la palabra.\n\nAhora pensemos porque esto tiene sentido.\n\nPrimero, nuestro problema es que con one-hot decimos que todas las palabras tienen la misma relaci\u00f3n por ejemplo que perro y carro tiene la misma relaci\u00f3n que perro y lobo, la verdad es que no es cierto por tal motivo queremos una representaci\u00f3n mas real.\n\nSi ponemos a una red neuronal que dada una palabra pueda predecir las palabras vecinas entonces la red se ver\u00e1 en la neceidad de aprender a codificar en la capa oculta esas relaciones que estamos buscando, despu\u00e9s solo la tomamos y problema resulto."}}