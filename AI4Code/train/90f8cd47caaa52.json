{"cell_type":{"6ee564a7":"code","4218da1b":"code","ddc33e58":"code","c15762d4":"code","f37804f4":"code","a16cf1a6":"code","c4b90341":"code","99ee55b6":"code","0e485378":"code","1f426b95":"code","189cc3bb":"code","84f53c18":"code","9b5cd880":"code","e352caf5":"code","89ee6621":"code","b44991b0":"code","302b8366":"code","5635a9c7":"code","3ba0c9c7":"code","bf7fd61d":"code","48337bec":"code","cf4349fe":"code","66101168":"code","b2e1106a":"code","09eb445c":"code","5966c27a":"code","4d3423d8":"code","5d3d1afe":"code","f256cb7d":"code","56780633":"code","3e53da30":"code","b0217238":"code","20d80b63":"code","f3b6fc05":"markdown","4de43de5":"markdown","f7293e55":"markdown","30e934e1":"markdown","4c8bd35d":"markdown","f25e5b8f":"markdown","a0ee9f63":"markdown","bfcd0c6d":"markdown","0e105161":"markdown","87c3d979":"markdown","6922dc88":"markdown","ac23d3ed":"markdown","ab144d05":"markdown","987bb0f5":"markdown","0681a02c":"markdown","b386ef57":"markdown","75f864ed":"markdown"},"source":{"6ee564a7":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import f1_score, precision_score, recall_score, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n\n# Models\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\n\nfrom joblib import dump","4218da1b":"df = pd.read_csv('\/kaggle\/input\/drug-classification\/drug200.csv')\ndf.sample(5)","ddc33e58":"print(df.Drug.value_counts())\nsns.countplot(x='Drug', data=df)","c15762d4":"df.info()","f37804f4":"df.head()","a16cf1a6":"def data_encoding(df):\n    df.Sex = LabelEncoder().fit_transform(df.Sex)\n    df.BP = LabelEncoder().fit_transform(df.BP)\n    df.Cholesterol = LabelEncoder().fit_transform(df.Cholesterol)\n\n\n# data_encoding(df)\n# df.head()","c4b90341":"''' Helper functions for plotting '''\n\n\ndef plot_histplot(column, ax=None):\n    sns.histplot(x=column, color='#65b87b', alpha=.7, ax=ax)\n    \n    \ndef plot_countplot(column, ax=None):\n    with sns.axes_style('ticks'):\n        sns.countplot(x=column, palette=sns.color_palette('rocket'), ax=ax)\n        sns.despine(offset=6)\n        \n        \ndef plot_barplot(x, y, ax=None):\n    sns.barplot(x=x, y=y, palette=sns.color_palette('rocket'))\n    \n    \ndef plot_boxplot(x, y, ax=None):\n    sns.boxplot(x=x, y=y)","99ee55b6":"plot_histplot(df.Age)","0e485378":"plot_histplot(df.Na_to_K)","1f426b95":"f, ax = plt.subplots(1, 2, figsize=(16, 4))\n\nplot_countplot(df[df.Sex == 'M'].BP, ax=ax[0])\nplot_countplot(df[df.Sex == 'F'].BP, ax=ax[1])\n\nax[0].set_title('Male - BP')\nax[1].set_title('Female - BP')","189cc3bb":"f, ax = plt.subplots(1, 2, figsize=(16, 4))\n\nplot_countplot(df[df.Sex == 'M'].Cholesterol, ax=ax[0])\nplot_countplot(df[df.Sex == 'F'].Cholesterol, ax=ax[1])\n\nax[0].set_title('Male - Cholesterol')\nax[1].set_title('Female - Cholesterol')","84f53c18":"sns.regplot(x=df.Age, y=df.Na_to_K)","9b5cd880":"plot_boxplot(df.Cholesterol, df.Na_to_K)","e352caf5":"plot_boxplot(df.BP, y=df.Na_to_K)","89ee6621":"plot_countplot(df.Drug)","b44991b0":"plot_boxplot(df.Drug, df.Age)","302b8366":"data_encoding(df)\ndf.head()","5635a9c7":"x = df[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values\ny = df.Drug.values\n\n# Scaling x\nx = StandardScaler().fit_transform(x)\n\nprint(f'Dataset size: {len(x)}')","3ba0c9c7":"x_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.3, random_state=3\n)\n\nprint(f'Training set size: {len(x_train)}')\nprint(f'Test set size: {len(x_test)}')","bf7fd61d":"# For cross validation\nskf = StratifiedKFold(n_splits=10)","48337bec":"models = [\n    LogisticRegression(), \n    SGDClassifier(), \n    KNeighborsClassifier(), \n    GaussianNB(), \n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    SVC(),\n]","cf4349fe":"for model in models:\n    scores = cross_val_score(model, x_train, y_train, cv=skf)\n    print(f'== {model} ==')\n    print(f'Cross-Validation mean-score: {scores.mean()}')\n    \n    print()","66101168":"# Parameter tuning\n\ndef dt_param_selection(x, y, nfolds):\n    criterion = ['gini', 'entropy']\n    splitter = ['best', 'random']\n    max_depth = [1, 2, 3, 4, 5]\n\n    param_grid = {\n        'criterion': criterion, \n        'splitter': splitter, \n        'max_depth': max_depth\n    }\n\n    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=nfolds)\n    grid_search.fit(x, y)\n    return grid_search.best_params_\n\n\nbest_params_ = dt_param_selection(x_train, y_train, skf)\nbest_params_","b2e1106a":"# Cross Validation\n\nmodel = DecisionTreeClassifier(criterion='gini', max_depth=4, splitter='best')\nscores = cross_val_score(model, x_train, y_train, cv=skf)\nprint(scores.mean())","09eb445c":"model = DecisionTreeClassifier(criterion='gini', max_depth=4, splitter='best')\nmodel.fit(x_train, y_train)","5966c27a":"y_test_pred = model.predict(x_test)\n\nprint(f\"Prediction: \\n{pd.DataFrame(y_test_pred)[0].value_counts()}\")","4d3423d8":"print(f\"Actual: \\n{pd.DataFrame(y_test).value_counts()}\")","5d3d1afe":"print(f'Model Score: {model.score(x_test, y_test)}')\nprint(f'f1-score: {f1_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'precision score: {precision_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'recall score: {recall_score(y_test, y_test_pred, average=\"weighted\")}')","f256cb7d":"print(classification_report(y_test, y_test_pred))","56780633":"# Saving the model\ndump(model, 'model.joblib')","3e53da30":"!pip install pydotplus","b0217238":"from io import StringIO\nimport pydotplus\nimport matplotlib.image as mpimg\nfrom sklearn import tree\n%matplotlib inline","20d80b63":"dot_data = StringIO()\n\nfilename = \"drugtree.png\"\nfeatureNames = df.columns[0:5]\ntargetNames = df[\"Drug\"].unique().tolist()\n\nout=tree.export_graphviz(model,feature_names=featureNames, out_file=dot_data, class_names= np.unique(y_train), filled=True,  special_characters=True,rotate=False)  \n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(filename)\nimg = mpimg.imread(filename)\nplt.figure(figsize=(100, 200))\nplt.imshow(img,interpolation='nearest')","f3b6fc05":"`Age` is not correlated to `Na_to_K`","4de43de5":"## \ud83c\udf40 Modelling\n\nLet's create our `AI`.\n\n![](https:\/\/media.giphy.com\/media\/xT0xepagSrUXfM1eNi\/giphy.gif)","f7293e55":"# Drugs Classifier using Decision Tree\n\nHere [Drug Classification](https:\/\/www.kaggle.com\/prathamtripathi\/drug-classification) dataset by [Pratham Tripathi](https:\/\/www.kaggle.com\/prathamtripathi) is used to create a classifier that classifies `drugs` on the basis of it `properites` using `Decision Tree`.\n\n![](https:\/\/media.giphy.com\/media\/xT8qB2zDVGj7ly4moU\/giphy.gif)","30e934e1":"Both `male` & `female` have `high cholesterol`","4c8bd35d":"- Sex\n    - Female - 0\n    - Male - 1\n- BP\n    - HIGH - 0\n    - LOW - 1\n    - Normal - 2\n- Cholesterol\n    - HIGH - 0\n    - LOW - 1\n    \nSince we are using `DecisionTreeClassifier` algorithm for classification, `LabelEncoding` is ok, otherwise if we are using something else where numbers matter, there we should use `OneHotEncoding`.","f25e5b8f":"## \ud83c\udf69 Exploratory Data Analysis","a0ee9f63":"### Data preparation: Encoding","bfcd0c6d":"`Label encoding vs OneHot encoding` \ud83d\udc49 [Source_1](https:\/\/towardsdatascience.com\/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b) and [Source_2](https:\/\/datascience.stackexchange.com\/questions\/9443\/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor)","0e105161":"> `drugB` is majorly consumed by people whose age is greater than 60 while other durgs are majorly consumed by people whose age is lesser than 60.\n>\n> `DrugY` is consumed more than other drugs while `drugB` and `drugA` are consumed by less number of people","87c3d979":"This will be done after `EDA` so that we can get insight into data & don't need to worry about mapping fig plot's x & y labels to original values","6922dc88":"## \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f Data preparation","ac23d3ed":"### \ud83d\udc1a Visualization","ab144d05":"Less number of `males` have `normal BP` compared to `females`. Large proportion of both the genders have a `high BP`","987bb0f5":"Looing if the dataset is `balanced` or not.","0681a02c":"## \ud83e\udd8b Evaluation","b386ef57":"---\n\nI'll wrap things up there. If you want to find some other answers then go ahead `edit` this kernel. If you have any `questions` then do let me know.\n\nIf this kernel helped you then don't forget to \ud83d\udd3c `upvote` and share your \ud83c\udf99 `feedback` on improvements of the kernel.\n\n![](https:\/\/media.giphy.com\/media\/Md9UQRsv94yCAjeA1w\/giphy.gif)\n\n---","75f864ed":"- Sex\n    - Female - 0\n    - Male - 1\n- BP\n    - HIGH - 0\n    - LOW - 1\n    - NORMAL - 2\n- Cholesterol\n    - HIGH - 0\n    - NORMAL - 1\n    \nSince we are using `DecisionTreeClassifier` algorithm for classification, `LabelEncoding` is ok, otherwise if we are using something else where numbers matter, there we should use `OneHotEncoding`."}}