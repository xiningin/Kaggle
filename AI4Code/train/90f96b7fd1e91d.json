{"cell_type":{"98956725":"code","f83750fb":"code","3ba96bbb":"code","c0c5d026":"code","5294278d":"code","94aa37e7":"code","d88732e2":"code","fef7af30":"code","b79a981f":"code","f0dd0865":"code","eb73bb29":"code","4c3641b2":"code","4b7d5cb9":"code","f9b9d69a":"code","ff66be29":"code","d5e10e60":"code","321ccbfd":"code","1675c82d":"code","0284fd99":"code","e17910ec":"code","d3521268":"code","c26630b5":"code","12a0e667":"code","ffe82cbd":"code","013684db":"code","bbeba32e":"code","3e88d6c9":"code","62f0c9e4":"code","6c74fcf9":"code","1f57e497":"markdown"},"source":{"98956725":"import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport os\nimport matplotlib.pyplot as plt","f83750fb":"def extract_features(df):\n    # container\n    data = []\n    \n    # features from the acoustic data set only, since the segments contain nothing else\n    max_acoustic = df['acoustic_data'].max()\n    mean_acoustic = df['acoustic_data'].mean()\n    \n    data.append([mean_acoustic])\n    data.append([df['acoustic_data'].std()])\n    data.append([max_acoustic])\n    data.append([df['acoustic_data'].min()])\n    \n    #number of `peaks` -> above mean + (max-mean)\/2 -> any value above (max + mean)\/2\n    signal_values = df['acoustic_data'].loc[df['acoustic_data'] > (max_acoustic + mean_acoustic) \/ 2.]\n    \n    signal_values = np.array(signal_values)\n    \n    data.append([signal_values.shape[0]]) # number of peaks\n    \n    data.append(np.correlate(df['acoustic_data'].values[::1000],\n                       df['acoustic_data'].values[::1000], mode='same')) # auto-correlate 0.01 % of the data\n                                                                         # to see how self-similair it is\n                                             \n    acoustic_histo = np.histogram(df['acoustic_data'], bins=75)\n    data.append(acoustic_histo[0]) # bins\n    data.append(acoustic_histo[0]) # values\n    \n    data.append(np.abs(np.fft.fft(df['acoustic_data'].values[::1000], n=100)))\n    \n    # we must flatten out the features\n    return [item for sublist in data for item in sublist]","3ba96bbb":"TextFileReader = pd.read_csv('..\/input\/train.csv', chunksize=150000) # the segment files contain 150000 lines each!\n\nreduced_data = dict()\ncounter = 0\n\nfor df in TextFileReader:\n    reduced_data[counter] = dict()\n    last_time_to_failure = df['time_to_failure'].values[::-1][0]\n    reduced_data[counter][last_time_to_failure] = extract_features(df)\n    counter += 1\n    if counter % 250 == 0: print('%d segments - done.' % counter)","c0c5d026":"TextFileReader = pd.read_csv('..\/input\/train.csv', chunksize=150000, skiprows=25000)\n\nfor df in TextFileReader:\n    df.columns = ['acoustic_data', 'time_to_failure']\n    reduced_data[counter] = dict()\n    last_time_to_failure = df['time_to_failure'].values[::-1][0]\n    reduced_data[counter][last_time_to_failure] = extract_features(df)\n    counter += 1\n    if counter % 250 == 0: print('%d segments - done.' % counter)","5294278d":"TextFileReader = pd.read_csv('..\/input\/train.csv', chunksize=150000, skiprows=75000)\n\nfor df in TextFileReader:\n    df.columns = ['acoustic_data', 'time_to_failure']\n    reduced_data[counter] = dict()\n    last_time_to_failure = df['time_to_failure'].values[::-1][0]\n    reduced_data[counter][last_time_to_failure] = extract_features(df)\n    counter += 1\n    if counter % 250 == 0: print('%d segments - done.' % counter)","94aa37e7":"len(reduced_data) # number of segments achieved that we could predict on!","d88732e2":"dataframes = []\n\nfor index in range(len(reduced_data)):\n    df = pd.DataFrame.from_dict(reduced_data[index], orient='index')\n    df['_id'] = index\n    df['ttf'] = df.index\n    df.set_index('_id', inplace=True)\n    dataframes.append(df)\n    \ndel reduced_data","fef7af30":"for df in dataframes:\n    df.to_csv('df_all.csv', mode='a', header=False, index=False)\n\ndel dataframes","b79a981f":"train = pd.read_csv('df_all.csv', header=None)\nos.remove('df_all.csv')\n\ntrain.head()","f0dd0865":"from sklearn.preprocessing import normalize","eb73bb29":"train = train.dropna()\ntrain.shape","4c3641b2":"X = normalize(train.values[:, :405])\ny = train.values[:, 405]","4b7d5cb9":"del train","f9b9d69a":"import tensorflow as tf\ntf.enable_eager_execution()","ff66be29":"X_ = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n\ndataset = tf.data.Dataset.from_tensor_slices((X_, y))\nsequences = dataset.batch(1, drop_remainder=True)","d5e10e60":"for seq, target in sequences.take(1):\n    print(seq.shape, target)","321ccbfd":"BATCH_SIZE = 6\n\nBUFFER_SIZE = 20000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","1675c82d":"if tf.test.is_gpu_available():\n    rnn = tf.keras.layers.CuDNNGRU\nelse:\n    import functools\n    rnn = functools.partial(\n        tf.keras.layers.GRU, recurrent_activation='sigmoid')","0284fd99":"def build_model(rnn_units, batch_size):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.InputLayer(input_shape=(1, 405), batch_size=batch_size),\n        tf.keras.layers.Dense(1024, activation='relu'),\n        rnn(rnn_units),\n        tf.keras.layers.Dense(256, activation='relu'),\n        tf.keras.layers.Dense(1024, activation='relu'),\n        tf.keras.layers.Dense(1, activation='relu')\n    ])\n    return model","e17910ec":"model = build_model(1024, BATCH_SIZE)","d3521268":"model.summary()","c26630b5":"def loss(labels, logits):\n    return tf.keras.losses.MSE(labels, logits)","12a0e667":"model.compile(\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\n    loss = loss)","ffe82cbd":"EPOCHS = 35\n\nsamples_per_epoch = X.shape[0]\nsteps_per_epoch = samples_per_epoch \/\/ BATCH_SIZE\n\nhistory = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch)","013684db":"model_ = build_model(rnn_units=1024, batch_size=1)\n\nweights = model.get_weights()\n\nmodel_.set_weights(weights)","bbeba32e":"test_files = os.listdir('..\/input\/test\/')","3e88d6c9":"result = dict()\nfor file in test_files:\n    \n    df = pd.read_csv('..\/input\/test\/' + file)\n    \n    data = np.array(extract_features(df))\n\n    X_test = normalize(data.reshape(1, -1))\n    X_test = X_test.reshape(1, 1, 405)\n    prediction = model_.predict(X_test)[0]\n    result[file[::-1][4:][::-1]] = prediction","62f0c9e4":"result_df = pd.DataFrame.from_dict(result, orient='index', columns=['time_to_failure'])\nresult_df.head(n=2)","6c74fcf9":"result_df.to_csv('.\/submission.csv', columns=['time_to_failure'], index_label='seg_id')","1f57e497":"Los Alamos National Laboratory - Earthquake analysis\n------------------------------------------------------------------------------"}}