{"cell_type":{"21401deb":"code","b46ecc0a":"code","d563bb11":"code","56728a28":"code","aa6e3cc3":"code","19c5d94c":"code","20cdb474":"code","434d6484":"code","234997f7":"code","af21130f":"code","56a11712":"code","5c40f482":"code","fbd1d663":"code","efd5024d":"code","7e429b4c":"code","164ab135":"code","e623d1f4":"code","b63fea82":"markdown","932a8039":"markdown","92a6d2cf":"markdown","9a3b0f89":"markdown"},"source":{"21401deb":"#vous trouverez ci dessous le lien de dossier :\nhttps:\/\/github.com\/prajnasb\/observations\/tree\/master\/experiements\/data","b46ecc0a":"#vous trouverez ci dessous le lien de dataset :\nhttps:\/\/github.com\/prajnasb\/observations\/tree\/master\/experiements\/data","d563bb11":"import cv2,os\n\ndata_path='\/content\/gdrive\/MyDrive\/dataset'\ncategories=os.listdir(data_path) \nlabels=[i for i in range(len(categories))]\n\nlabel_dict=dict(zip(categories,labels)) #creer dictionnaire  nom categorie et label\n\nprint(label_dict)\nprint(categories)\nprint(labels)","56728a28":"img_size=100 #toutes les images ont le meme size \ndata=[] #liste vide pour enregistrer les images\ntarget=[] #liste vide pour enregistrer les labels\n\nfor category in categories:\n    folder_path=os.path.join(data_path,category) \n    img_names=os.listdir(folder_path) \n        \n    for img_name in img_names:\n        img_path=os.path.join(folder_path,img_name)\n        img=cv2.imread(img_path) #importer tt les images de chaque categorie dans img\n\n        try:\n            gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)           \n            #Changer le couleur de tous les images en gris\n            resized=cv2.resize(gray,(img_size,img_size))\n            # on met un size commun pour tous les images \n            data.append(resized)\n            target.append(label_dict[category])\n            \n        except Exception as e:\n            print('Exception:',e)\n            #si l'image n est pas disponible ou il y a un erreur on passe a l'image suivante ","aa6e3cc3":"import numpy as np\n\ndata=np.array(data)\/255.0 #rendre les pixels en 0 et 1 (0 (black) to 255 (white))\ndata=np.reshape(data,(data.shape[0],img_size,img_size,1)) #Give a new shape to an array without changing its data\ntarget=np.array(target) #convertir target en array\n\nfrom keras.utils import np_utils\n\nnew_target=np_utils.to_categorical(target)","19c5d94c":"target","20cdb474":"np.save('data',data) #data contient les images \nnp.save('target',new_target) #target contient categorie","434d6484":"import numpy as np\n\ndata=np.load('\/content\/data.npy')\ntarget=np.load('\/content\/target.npy')","234997f7":"from keras.models import Sequential\nfrom keras.layers import Dense,Activation,Flatten,Dropout\nfrom keras.layers import Conv2D,MaxPooling2D\nfrom keras.callbacks import ModelCheckpoint\n\nmodel=Sequential()\n\n#first convolution layer\nmodel.add(Conv2D(200,(3,3),input_shape=data.shape[1:]))  #the convolutional layer will learn 200 filters avec a kernel size 3*3\n# Max pooling  used to reduce the spatial dimensions of the output volume\nmodel.add(Activation('relu')) \nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n#The second convolution layer\nmodel.add(Conv2D(100,(3,3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n#Flatten layer to stack the output convolutions from second convolution layer\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))#minimiser le surapprentissage (overffiting)\n\nmodel.add(Dense(50,activation='relu'))\n\nmodel.add(Dense(2,activation='softmax'))\n#The Final layer with two outputs for two categories\n\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","af21130f":"from sklearn.model_selection import train_test_split\n\ntrain_data,test_data,train_target,test_target=train_test_split(data,target,test_size=0.1)","56a11712":"checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\nhistory=model.fit(train_data,train_target,epochs=20,callbacks=[checkpoint],validation_split=0.2)","5c40f482":"from matplotlib import pyplot as plt\n\nplt.plot(history.history['loss'],'r',label='training loss')\nplt.plot(history.history['val_loss'],label='validation loss')\nplt.xlabel('# epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","fbd1d663":"plt.plot(history.history['accuracy'],'r',label='training accuracy')\nplt.plot(history.history['val_accuracy'],label='validation accuracy')\nplt.xlabel('# epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","efd5024d":"print(model.evaluate(test_data,test_target)) #on trouve accracy 95,6% et un loss de 15%","7e429b4c":"from keras.models import load_model\nimport cv2\nimport numpy as np","164ab135":"model = load_model('\/content\/drive\/MyDrive\/model-017.model')\n\nface_clsfr=cv2.CascadeClassifier('\/content\/drive\/MyDrive\/haarcascade_frontalface_default.xml')\nsource=cv2.VideoCapture(0)\n\nlabels_dict={0:'MASK',1:'NO MASK'}\ncolor_dict={0:(0,255,0),1:(0,0,255)}","e623d1f4":"while(True):\n\n    ret,img=source.read()\n    source=cv2.VideoCapture(0)\n    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    faces=face_clsfr.detectMultiScale(gray,1.3,5)  \n\n    for x,y,w,h in faces:\n    \n        face_img=gray[y:y+w,x:x+w]\n        resized=cv2.resize(face_img,(100,100))\n        normalized=resized\/255.0\n        reshaped=np.reshape(normalized,(1,100,100,1))\n        result=model.predict(reshaped)\n\n        label=np.argmax(result,axis=1)[0]\n      \n        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n        \n        \n    cv2.imshow('LIVE',img)\n    key=cv2.waitKey(1)\n    \n    if(key==27):\n        break\n        \ncv2.destroyAllWindows()\nsource.release()","b63fea82":"#Detecter les images ","932a8039":"#Traitement des donn\u00e9es:","92a6d2cf":"la base de donn\u00e9es contient en total 1376 images : 690 images de personnes portant le mask et 686 images de personne sans mask\n","9a3b0f89":"# Convolutional Neural Network: CNN"}}