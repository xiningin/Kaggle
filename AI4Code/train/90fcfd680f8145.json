{"cell_type":{"a71e1c00":"code","5abef1c7":"code","0cbed51a":"code","a0201393":"code","1207aa22":"code","db93a824":"code","454f93c7":"code","9741227c":"code","c0532f59":"code","471182a0":"code","4591ca34":"code","3f745225":"code","1fd04c50":"code","baa180e4":"code","49385b73":"code","4bc58dcc":"code","9cf3ba23":"code","6930b17e":"code","5e3775a7":"code","7befd048":"code","0bdafc5d":"code","7269c611":"code","a215b4db":"code","e001b2d6":"code","c2d76b89":"code","04bc11c7":"code","97674fac":"code","849817cc":"code","778015d7":"code","d2cd9ed5":"code","0db94eb3":"code","babd1b43":"code","fabd4518":"code","0b16ec30":"code","cb0b4bad":"code","1c06a342":"code","d9283c50":"code","90bcf85d":"code","5bca25a5":"code","698943e1":"code","16d2ed36":"code","dfff29f9":"code","cb824403":"code","7010e4ad":"code","4cc62fe2":"code","913b19ff":"code","bd6edf3e":"code","5f59593f":"code","ee9f1d06":"code","896d3e04":"code","8082ba1d":"code","2b5c0b2b":"code","54bea9e2":"code","f080c716":"code","77e5425d":"code","1a27e89c":"code","18d1faa5":"code","d9e14538":"code","22556617":"code","22baa8a4":"code","d19c7bf5":"code","612eccc9":"markdown","52d5b2c1":"markdown","07334b8e":"markdown","24a41798":"markdown","87562aab":"markdown","2f1ffc71":"markdown","22cc1814":"markdown","5f5fe8c3":"markdown","591f5a49":"markdown","8d66d739":"markdown","1a64378f":"markdown","43df2239":"markdown","753b7147":"markdown","d0fd2410":"markdown","8fa21b9a":"markdown","9dd98a45":"markdown","235c0f29":"markdown","68f13864":"markdown","9e3ec8a6":"markdown","02107f81":"markdown","090a7582":"markdown","f3faeb8f":"markdown","7a7363e4":"markdown","f0b68b3a":"markdown","a0c9556a":"markdown","aebd64e6":"markdown","75f9532e":"markdown","5b838c64":"markdown","d9f0306f":"markdown","a568a093":"markdown","67a294d5":"markdown","b8e3386e":"markdown","5082960e":"markdown","a4af4aaa":"markdown","e3e8ae5d":"markdown","cd1585a4":"markdown","ecff6f92":"markdown","60158cd1":"markdown","f2be6e47":"markdown","3b3d3eb5":"markdown","05651c47":"markdown","6db301ca":"markdown","b9b4a6e6":"markdown","b9f68f89":"markdown","cc7f17fe":"markdown","eb19a8f8":"markdown","a6bdef1d":"markdown","e535c5f8":"markdown","36b5a135":"markdown","c13b8ca5":"markdown"},"source":{"a71e1c00":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# data viualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# machine learning \nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Let's ignore warnings for now\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5abef1c7":"# Import the train & test data\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest = df_test.copy() # making a copy of test data to make predictions\ngender_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\") # example of submission file","0cbed51a":"# Let's check shape (number of rows, number of columns) of the train & test data\nprint(\"Train data - rows:\",train.shape[0],\", columns:\", train.shape[1])\nprint()\nprint(\"Test data - rows:\",test.shape[0],\", columns:\", test.shape[1])","a0201393":"# training data\ntrain.head(10) # first 10 rows","1207aa22":"# test data \ndf_test.head() # first 5 rows","db93a824":"# Let's have a look at the submisison dataframe\ngender_submission.head()","454f93c7":"gender_submission.shape","9741227c":"train.describe()","c0532f59":"## Let's visualize the missing values using plot\nsns.heatmap(train.isnull(), yticklabels = False, cbar = False)\nplt.show()","471182a0":"# Missing values in the train data\ntrain.isnull().sum().sort_values(ascending = False)  ## using sort_values we can sort values in descending order","4591ca34":"# Missing values in test data\ntest.isnull().sum().sort_values(ascending = False) ","3f745225":"# Let's check for different data types in the train dataset\ntrain.dtypes","1fd04c50":"# Let's check for different data types in the test dataset\ntest.dtypes","baa180e4":"train.head()","49385b73":"# How many people survived\nprint(train.Survived.value_counts())\nprint()\nplt.figure(figsize=(20,1))\nsns.countplot(y= \"Survived\", data = train)\nplt.show()","4bc58dcc":"sns.countplot(train['Pclass'])\nplt.show()","9cf3ba23":"# Let's check missing values\ntrain.Pclass.isnull().sum()","6930b17e":"train.Name.value_counts()","5e3775a7":"# Let's drop this Name & PassengerId from data\ntrain.drop(columns = [\"Name\",\"PassengerId\"], axis = 1, inplace = True)\ntest.drop(columns = [\"Name\",\"PassengerId\"], axis = 1, inplace = True)","7befd048":"# Let's view the distribution of Sex\nplt.figure(figsize=(15, 2))\nsns.countplot(y=\"Sex\", data=train);","0bdafc5d":"sns.countplot(x = 'Survived', hue = 'Sex', data = train)\nplt.show()","7269c611":"# Let's check for missing values in train data\ntrain.Sex.isnull().sum()","a215b4db":"train['Age'].hist(bins = 50, color = 'blue')\nplt.show()","e001b2d6":"# Let's check for missing values in Age feature\ntrain.Age.isnull().sum()","c2d76b89":"sns.countplot(train['SibSp'])\nplt.show()","04bc11c7":"sns.countplot(train['Parch'])\nplt.show()","97674fac":"# Let's see how many kind's of ticket's are there using plot\nsns.countplot(y=\"Ticket\", data=train);","849817cc":"# Let's see how many kind's of ticket's are there\ntrain.Ticket.value_counts()","778015d7":"# How many kinds of Ticket are there?\nprint(\"There are {} unique Ticket values.\".format(len(train.Ticket.unique())))","d2cd9ed5":"# Let's drop this feature from our dataset\ntrain.drop(\"Ticket\", axis = 1, inplace = True)\ntest.drop(\"Ticket\", axis = 1, inplace = True)","0db94eb3":"train['Fare'].hist(bins = 50, color = 'red')\nplt.show()","babd1b43":"# Let's drop this feature because we already have class\ntrain.drop(\"Fare\", axis = 1, inplace = True)\ntest.drop(\"Fare\", axis = 1, inplace = True)","fabd4518":"train.Cabin.value_counts()","0b16ec30":"# Let's drop Cabin feature\ntrain.drop(\"Cabin\", axis = 1, inplace = True)\ntest.drop(\"Cabin\", axis =1, inplace = True)","cb0b4bad":"# Let's check what kind of values are in Embarked\ntrain.Embarked.value_counts()","1c06a342":"sns.countplot(train['Embarked'])\nplt.show()","d9283c50":"# Let's check for missing values in Embarked\ntrain[\"Embarked\"].isnull().sum()","90bcf85d":"# Let's remove Embarked rows which are missing values\nprint(len(train))\ntrain = train.dropna(subset=['Embarked'])\nprint(len(train))","5bca25a5":"# Let's see features correlation matrix using heatmap\nplt.figure(figsize=(8,7))\nsns.heatmap(train.corr(), annot = True, cmap = \"coolwarm\")\nplt.show()","698943e1":"## Let's impute missing values of Age feature using Pclass since they have the highest correlation in absolute numbers\ntrain['Age'] = train.groupby(\"Pclass\")['Age'].transform(lambda x: x.fillna(x.median()))\ntest['Age'] = test.groupby(\"Pclass\")['Age'].transform(lambda x: x.fillna(x.median()))","16d2ed36":"print(train[\"Age\"].isnull().sum())\nprint(test[\"Age\"].isnull().sum())","dfff29f9":"train.head()","cb824403":"test.head()","7010e4ad":"# Let's convert the categorical variables into dummy\/indicator variables using get_dummies() \ntrain = pd.get_dummies(data = train, columns = [\"Sex\", \"Embarked\", \"Pclass\"])\ntest = pd.get_dummies(data = test, columns = [\"Sex\", \"Embarked\", \"Pclass\"])","4cc62fe2":"# Let's split the dataset into data and labels\nX_train = train.drop(\"Survived\", axis = 1)  # data\ny_train = train[\"Survived\"] # labels","913b19ff":"# Let's check the shape of the data without labels\nX_train.shape","bd6edf3e":"# Let's check the shape of the labels\ny_train.shape","5f59593f":"# Let's write a function that runs the requested algorithm and returns the accuracy metrics\ndef fit_model(algo, X_train, y_train, cv):\n    \n    model = algo.fit(X_train, y_train)    \n    y_pred = algo.predict(X_train)    \n    accuracy = round(accuracy_score(y_train, y_pred) * 100 , 2)\n    \n    #cross validation\n    y_pred_cv = cross_val_predict(algo, X_train, y_train, cv = cv)\n    # cross validation accuracy\n    accuracy_cv = round(accuracy_score(y_train, y_pred_cv) * 100 , 2)\n    \n    return y_pred_cv, accuracy, accuracy_cv","ee9f1d06":"# Logistic Regression\n\ny_pred_cv_lr, accuracy_lr, accuracy_cv_lr = fit_model(LogisticRegression(random_state = 3),\n                                                     X_train, y_train, 10)\n\nprint(\"Accuracy : \",accuracy_lr)\nprint(\"Accuracy CV :\",accuracy_cv_lr)","896d3e04":"# Random Forest\nrf = RandomForestClassifier(n_estimators = 100, random_state = 3)\nrf.fit(X_train, y_train)\n\ny_train_pred = rf.predict(X_train)\n\nprint('Confusion Matrix : ','\\n', confusion_matrix(y_train, y_train_pred))\nprint()\nprint(\"Accuracy : \", round(accuracy_score(y_train, y_train_pred) * 100, 2))","8082ba1d":"# let's optimize hyperparameters in random forest classifier\nfrom scipy.stats import randint as sp_randint\nrfc = RandomForestClassifier(random_state=3)\n\nparams = {'n_estimators' : sp_randint(50,200),\n         'max_depth' : sp_randint(2,100),\n          'max_depth' : sp_randint(2,100),\n         'min_samples_split' : sp_randint(2,100),\n         'min_samples_leaf' : sp_randint(1,200),\n         'criterion' : ['gini', 'entropy']}\n\n# RandomizedSearchCV\nrsearch_rfc = RandomizedSearchCV(rfc, param_distributions = params, n_iter = 100, cv = 3, scoring = 'roc_auc', n_jobs = -1,\\\n                             return_train_score = True, random_state = 3)\n\nrsearch_rfc.fit(X_train,y_train)","2b5c0b2b":"# Print best hyperparameters\nrsearch_rfc.best_params_","54bea9e2":"pd.DataFrame(rsearch_rfc.cv_results_).head(2)","f080c716":"# let's fit our model to the training set with best hyperparameters\nrfc = RandomForestClassifier(**rsearch_rfc.best_params_, random_state = 3)\n\nrfc.fit(X_train, y_train)\n\ny_train_pred = rfc.predict(X_train)\n\nprint('Confusion Matrix : ','\\n', confusion_matrix(y_train, y_train_pred))\nprint()\nprint(\"Accuracy : \", round(accuracy_score(y_train, y_train_pred) * 100, 2))","77e5425d":"# Feature Importance\nimp = pd.DataFrame(rfc.feature_importances_, index = X_train.columns, columns = ['imp'])\nimp = imp.sort_values(by ='imp', ascending = False)\nimp","1a27e89c":"# Let's make a prediction on the test dataset using our random forest\n\ny_test_pred = rfc.predict(test)","18d1faa5":"y_test_pred[:20]","d9e14538":"# Let's create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = df_test['PassengerId']\nsubmission['Survived'] = y_test_pred # our model predictions on the test dataset\nsubmission.head()","22556617":"# Let's check shape of test and submission DataFrame\nprint(submission.shape)\nprint(df_test.shape)","22baa8a4":"# Let's convert submisison dataframe to csv\n\nsubmission.to_csv('rf_submission.csv', index=False)\nprint('Submission CSV is ready!')","d19c7bf5":"# let's check the submission csv\nsubmissions_check = pd.read_csv(\"rf_submission.csv\")\nsubmissions_check.head()","612eccc9":"## Target Feature: Survived\n\nDescription: Whether the passenger survived or not.\n\nKey: 0 = did not survive, 1 = survived\n\nThis is the variable we want our machine learning model to predict based off all the others.","52d5b2c1":"#### Now let's define a function to fit machine learning algorithms","07334b8e":"Another categorical variable, it looks a bit complicated.\n\nSince there are too many missing values, we won't use Cabin for our model.","24a41798":"After downloading the data, we need to get it into the notebook.\n\nTo read the dataset, we will use the read_csv() Pandas method:","87562aab":"## Feature: Pclass \n\nDescription: The ticket class of the passenger.\n\nKey: 1 = 1st, 2 = 2nd, 3 = 3rd","2f1ffc71":"## Import the libraries","22cc1814":"Hyperparameter Tuning is the technique of choosing an optimal set of hyperparameters (i.e. parameters that control the learning process) for our learning algorithm. We can select these hyperparameters by using hyperparameter optimization or by manual tuning.","5f5fe8c3":"There are 7 categories in Parch feature. ","591f5a49":"Embarked is a categorical variable because there are 3 categories which a passenger could have boarded on.","8d66d739":"Above we can see that **38% out of the training-set survived the Titanic**. We can also see that the passenger ages range from 0.4 to 80. On top of that we can already detect some features, that contain missing values, like the 'Age' feature.","1a64378f":"This is my first notebook at **Kaggle**.\n\nIn this notebook I will try to predict survival on the Titanic based on the given data(features) in the datset. And also I will do feature engineering and make different models.\n\nSo, Let's look into data","43df2239":"## Random Forest","753b7147":"Let's print our optimal hyperparameters set.","d0fd2410":"## Feature: SibSp\n\nDescription: The number of siblings\/spouses the passenger has aboard the Titanic.","8fa21b9a":"## Let's start Building Machine Learning Models\n\nNow our data has been converted into numbers, we can run different machine learning algorithms over it to find which model can give the best result.","9dd98a45":"it's not looking informative at all.","235c0f29":"## Feature: Embarked\n\nDescription: The port where the passenger boarded the Titanic.\n\nKey: C = Cherbourg, Q = Queenstown, S = Southampton","68f13864":"## Feature: Fare\nDescription: How much the ticket cost.","9e3ec8a6":"No missing values for 'Age' now.","02107f81":"## Datatypes in the dataframe","090a7582":"\n#### If you like it, please upvote! :)","f3faeb8f":"Here, we can see there are no missing values in Pclass.\n","7a7363e4":"## Feature: Age\n\nDescription: The age of the passenger.","f0b68b3a":"So, here we can see submission dataframe and test dataframe are of equal length(418 rows).","a0c9556a":"## Feature Encoding\n\nNow we can encode the features so they're ready to be used with our machine learning models.","aebd64e6":"## Feature: Sex\n\nDescription: The sex of the passenger (male or female).","75f9532e":"The distribution for \"Age\" looks good, but there are 177 missing values.","5b838c64":"From the above heatmap, we can see the correlations are pretty low, so no multicollinearity present here.\n\nFrom the above heatmap we can see highest correlated variable to \"Age\" in absolute values is \"Pclass\", with |corr(Age, Pclass)| = 0.37.\n\nSo let's impute missing values of Age feature using Pclass","d9f0306f":"The rule of thumb is that it's better to impute the missing values rather than dropping the features altogether. After all, there is still some info hidden in these features.\n\nAfter the following analysis, we will conclude that we can easily impute 'Age' missing values.","a568a093":"Now, let's have a first look at our training data!","67a294d5":"## Let's explore each of these features individually","b8e3386e":"## Feature: Cabin\nDescription: Cabin number","5082960e":"## Import the dataset","a4af4aaa":"## Feature: Ticket\n\nDescription: The ticket number of the boarding passenger.","e3e8ae5d":"**How can we deal with the 2 missing values of Embarked?**\n\nOne option is to drop the missing rows.\n\nAnother option would be to randomly assign a value of C, Q or S to each row.\n\nFor now, we will remove those rows.","cd1585a4":"Featrue importance shows how much each feature contributed to the model.\n\nWe can take this information and remove features which don't contribute much to reduce dimenstionality (and save compute), also we can improve features which don't offer much to the overall model.","ecff6f92":"## Data Descriptions\n\n* **Survival:** 0 = No, 1 = Yes\n\n\n* **pclass (Ticket class):** 1 = 1st, 2 = 2nd, 3 = 3rd\n\n\n* **sex:** Sex\n\n\n* **Age:** Age in years\n\n\n* **sibsp:** number of siblings\/spouses aboard the Titanic\n\n\n* **parch:** number of parents\/children aboard the Titanic\n\n\n* **ticket:** Ticket number\n\n\n* **fare:** Passenger fare\n\n\n* **cabin:** Cabin number\n\n\n* **embarked:** Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton","60158cd1":"Another nice categorical feature with 7 categories.","f2be6e47":"Another \"nice\" categorical feature here with 2 categories.","3b3d3eb5":"## Feature: Parch\n\nDescription: The number of parents\/children the passenger has aboard the Titanic.","05651c47":"### Let's seperate the data first","6db301ca":"So, it's a continuous feature and from the above plot we can see that it's right skewed.\n\nAs we already have Class feature which is more relevant so, we will not use this feature to build our model.","b9b4a6e6":"## Feature: Name\n\nDescription: The name of the passenger.","b9f68f89":"A \"clean\" categorical feature here, with 3 categories.","cc7f17fe":"Every row has a unique name. This is equivalent to the passenger ID. But name could be used differently.\n\nWe can use the Name feature also to build our model by creating new features out of the names or by reducing the number of different names.","eb19a8f8":"## Logistic Regression","a6bdef1d":"### Hyperparameter Tuning","e535c5f8":"We can clearly see some missing values here. Especially in the cabin column.\n\nIt's important to visualise missing values early to know where the major holes are in dataset.\n\nKnowing this informaiton will help in EDA and figuring out what kind of data cleaning and preprocessing is needed.","36b5a135":"It is obvious from this plot that females have a much better chance of surviving.","c13b8ca5":"That doesn't look too good, Let's look at it using another way."}}