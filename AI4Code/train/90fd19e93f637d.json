{"cell_type":{"a8db6bbf":"code","91b5e6a8":"code","117055e6":"code","25857013":"code","95ab62e4":"code","3b2077d8":"code","7515c39c":"code","ff7df040":"code","052a2100":"code","d591fd3e":"code","8316074e":"code","c039be94":"code","3412c198":"code","de2529ca":"code","ed7f5490":"code","22e1da88":"code","e7641686":"code","1fa6a201":"code","8fa0025d":"code","7e572cc8":"code","3a212dd5":"code","50900f60":"code","b02edce2":"code","28f2e1e8":"code","56b833ad":"code","e63db263":"code","9f449adf":"code","77c5113c":"code","9d7b79bf":"code","79335d7e":"code","01903c69":"code","90081fa5":"code","faedf2c7":"code","33bbb305":"code","0b20c9d6":"code","a071086c":"code","6f0d30b4":"code","15ff4ecd":"code","359f6b60":"code","e56ab25c":"code","4ed7e5d0":"code","39663a7b":"code","eac4f5b9":"code","53a4a725":"code","40421447":"code","c9199133":"code","656d9138":"code","09858196":"code","effb35f8":"code","0f0eabaa":"code","5c71b8ef":"code","6c575e6b":"code","3491a406":"code","dd4f05d5":"markdown","a972c79d":"markdown","29141628":"markdown","130aa22e":"markdown","b7f3f2c5":"markdown","1006b69c":"markdown","7eff5ee4":"markdown","42fd0030":"markdown","77a73ac2":"markdown","06ea1c20":"markdown","eaf68f23":"markdown","b18edbd9":"markdown","ee0a5bda":"markdown","4ed24966":"markdown","a178a7d9":"markdown","21d70c00":"markdown","9f8a9579":"markdown","e63d2a64":"markdown","853770e1":"markdown","8ef444b4":"markdown","d0c190c1":"markdown","20b422bf":"markdown","4af9f179":"markdown","d1ee84ed":"markdown","163a51da":"markdown","ab9def2b":"markdown","01171f18":"markdown"},"source":{"a8db6bbf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","91b5e6a8":"import pandas as pd \nimport numpy as np","117055e6":"# Import datasets\ntrain = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","25857013":"# Calculating number of na values in dataframe\ntrain.isnull().sum()","95ab62e4":"# Droping na values\ntrain.dropna(inplace=True)","3b2077d8":"# Now no na values\ntrain.isnull().sum()","7515c39c":"train.head()","ff7df040":"import matplotlib.pyplot as plt","052a2100":"def plotWordClouds(df_text,sentiment):\n    text = \" \".join(str(tmptext) for tmptext in df_text)\n    text = text.lower()\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=300,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(text)\n  \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title('WordCloud - ' + sentiment)\n    plt.show()         \n","d591fd3e":"subtext = train[train['sentiment']=='positive']['selected_text']\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS) \nplotWordClouds(subtext,'positive')\n","8316074e":"subtext = train[train['sentiment']=='neutral']['selected_text']\nplotWordClouds(subtext,'neutral')\n","c039be94":"subtext = train[train['sentiment']=='negative']['selected_text']\nplotWordClouds(subtext,'negative')","3412c198":"# Make all the text lowercase - casing doesn't matter when \n# we choose our selected text.\ntrain['text'] = train['text'].apply(lambda x: x.lower())\ntest['text'] = test['text'].apply(lambda x: x.lower())","de2529ca":"import re","ed7f5490":"def text_data(str): # taking only the text data\n  str_new = \" \".join(re.findall(\"[a-zA-Z]+\", str))\n  return (str_new)","22e1da88":"train['text'] = train['text'].apply(lambda x: text_data(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x: text_data(x))","e7641686":"train.head()","1fa6a201":"# Make training\/test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val = train_test_split(train, train_size = 0.9, random_state = 0)","8fa0025d":"X_train.head()","7e572cc8":"import nltk\nnltk.download(\"all\")","3a212dd5":"def tokenize(str_new): # tokenization\n  tokens = []\n  tokens = nltk.word_tokenize(str_new)\n  return (tokens)","50900f60":"X_train['text'] = X_train['text'].apply(lambda x: tokenize(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: tokenize(x))","b02edce2":"from nltk.corpus import stopwords\ndef remove(tokens): # removing stopwords & punctuations\n  removed = []\n  stopword = stopwords.words('english') \n  for w in tokens:\n    if (w not in stopword):\n      removed.append(w)\n  return (removed)","28f2e1e8":"X_train['text'] = X_train['text'].apply(lambda x: remove(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: remove(x))","56b833ad":"def tagging(removed):\n  pos = []\n  pos.append(nltk.pos_tag(removed))\n  return (pos)","e63db263":"X_train['text'] = X_train['text'].apply(lambda x: tagging(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: tagging(x))","9f449adf":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet","77c5113c":"def lemmatization(pos):\n  lem = WordNetLemmatizer()\n  lemma = []\n  for word in pos:\n    for w in word:\n      pos_value = \"\"\n      if (w[1].startswith('J')):\n        pos_value = wordnet.ADJ\n      elif (w[1].startswith('V')):\n        pos_value = wordnet.VERB\n      elif (w[1].startswith('N')):\n        pos_value = wordnet.NOUN\n      elif (w[1].startswith('R')):\n        pos_value = wordnet.ADV \n      else:\n        continue\n      lemma.append(lem.lemmatize(w[0],pos_value))\n  return (lemma)","9d7b79bf":"X_train['text'] = X_train['text'].apply(lambda x: lemmatization(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: lemmatization(x))","79335d7e":"def to_string(lemma): # converting the data from list to string format for tfidf\n  str1 = ' '.join([elem for elem in lemma]) \n  return (str1)","01903c69":"X_train['text'] = X_train['text'].apply(lambda x: to_string(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: to_string(x))","90081fa5":"X_train.head()","faedf2c7":"pos_train = X_train[X_train['sentiment'] == 'positive'] # contains all positive\nneutral_train = X_train[X_train['sentiment'] == 'neutral']\nneg_train = X_train[X_train['sentiment'] == 'negative']","33bbb305":"# CountVectorizer will help calculate word counts\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(analyzer='word',max_df=0.95,stop_words='english')","0b20c9d6":"X_train_cv = cv.fit_transform(X_train['text']) # has only text data","a071086c":"X_pos = cv.transform(pos_train['text'])\nX_neutral = cv.transform(neutral_train['text'])\nX_neg = cv.transform(neg_train['text'])","6f0d30b4":"pos_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\nneutral_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\nneg_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())","15ff4ecd":"# Create dictionaries of the words within each sentiment group, where the values are the proportions of tweets that contain those words\n\npos_words = {}\nneutral_words = {}\nneg_words = {}","359f6b60":"for k in cv.get_feature_names(): # for every word \n    pos = pos_count_df[k].sum() # number of times the positive word occurs\n    neutral = neutral_count_df[k].sum()\n    neg = neg_count_df[k].sum()\n    pos_words[k] = pos\/pos_train.shape[0] # pos_train.shape[0] - total positive documents, therefore positive word\/ positive documents, term frequency\n    neutral_words[k] = neutral\/neutral_train.shape[0]\n    neg_words[k] = neg\/neg_train.shape[0]","e56ab25c":"# We need to account for the fact that there will be a lot of words used in tweets of every sentiment.  \n# Therefore, we reassign the values in the dictionary by subtracting the proportion of tweets in the other \n# sentiments that use that word.\npos_words_adj = {}\nneutral_words_adj = {}\nneg_words_adj = {}","4ed7e5d0":"for key, value in pos_words.items():\n    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key]) # this is basically idf, which means how positive is the word, does it occur in negative & neutral\n    \nfor key, value in neutral_words.items():\n    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])\n\nfor key, value in neg_words.items():\n    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])","39663a7b":"def calculate_selected_text(df_row, tol = 0):\n    \n    tweet = df_row['text']\n    sentiment = df_row['sentiment']\n    \n    if(sentiment == 'neutral'):\n        return tweet \n    \n    elif(sentiment == 'positive'):\n        dict_to_use = pos_words_adj # Calculate word weights using the pos_words dictionary\n    elif(sentiment == 'negative'):\n        dict_to_use = neg_words_adj # Calculate word weights using the neg_words dictionary\n        \n    words = tweet.split()\n    words_len = len(words)\n    subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)] # taking combination of words 1 gram, 2 gram, ..., with 1st word, 2nd word\n        \n    score = 0\n    selection_str = '' # This will be our choice\n    lst = sorted(subsets, key = len) # Sort candidates by length for each subset\n    \n    \n    for i in range(len(subsets)): # for all possiblities\n        \n        new_sum = 0 # Sum for the current substring\n        \n        # Calculate the sum of weights for each word in the substring\n        for p in range(len(lst[i])): # go length wise\n            if(lst[i][p] in dict_to_use.keys()):\n                new_sum += dict_to_use[lst[i][p]]\n                \n            \n        # If the sum is greater than the score, update our current selection\n        if(new_sum > score + tol): \n            score = new_sum\n            selection_str = lst[i]\n            #tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n\n    # If we didn't find good substrings, return the whole text\n    if(len(selection_str) == 0):\n        selection_str = words                   \n        \n    return ' '.join(selection_str)","eac4f5b9":"tol = 0.0015\n\nX_val['predicted_selection'] = ''\n\nfor index, row in X_val.iterrows(): # rows in X validation set\n    \n    selected_text = calculate_selected_text(row, tol)\n    \n    X_val.loc[X_val['textID'] == row['textID'], ['predicted_selection']] = selected_text","53a4a725":"def jaccard(str1, str2): # to see similarity\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c)) ","40421447":"X_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n\nprint('The jaccard score for the validation set is:', np.mean(X_val['jaccard']))","c9199133":"X_val","656d9138":"pos_tr = train[train['sentiment'] == 'positive']\nneutral_tr = train[train['sentiment'] == 'neutral']\nneg_tr = train[train['sentiment'] == 'negative']","09858196":"cv = CountVectorizer(analyzer='word',max_df=0.95, min_df=2,stop_words='english')\n\nfinal_cv = cv.fit_transform(train['text'])\n\nX_pos = cv.transform(pos_tr['text'])\nX_neutral = cv.transform(neutral_tr['text'])\nX_neg = cv.transform(neg_tr['text'])\n\npos_final_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\nneutral_final_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\nneg_final_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())","effb35f8":"pos_words = {}\nneutral_words = {}\nneg_words = {}\n\nfor k in cv.get_feature_names():\n    pos = pos_final_count_df[k].sum()\n    neutral = neutral_final_count_df[k].sum()\n    neg = neg_final_count_df[k].sum()\n    \n    pos_words[k] = pos\/(pos_tr.shape[0])\n    neutral_words[k] = neutral\/(neutral_tr.shape[0])\n    neg_words[k] = neg\/(neg_tr.shape[0])","0f0eabaa":"neg_words_adj = {}\npos_words_adj = {}\nneutral_words_adj = {}\n\nfor key, value in neg_words.items():\n    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n    \nfor key, value in pos_words.items():\n    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n    \nfor key, value in neutral_words.items():\n    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])","5c71b8ef":"tol = 0.001\n\nfor index, row in test.iterrows():\n    \n    selected_text = calculate_selected_text(row, tol)\n    \n    sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text","6c575e6b":"sample.head()","3491a406":"sample.to_csv('submission.csv', index = False)","dd4f05d5":"Importing dataset","a972c79d":"POS tagging","29141628":"Converting the train & test data to lowercase","130aa22e":"Pre-processing the data","b7f3f2c5":"We calculate the jaccard score to see how accurate is our prediction, we basically see the similarity","1006b69c":"Calculating & removing na values","7eff5ee4":"Here we determine the answer for the X_val, we make subset of words such that 1 gram, 2 gram, ..., then we calculate the score for the validation input using the scores for each word calculated in the previous cell, the subset with the highest score is the answer","42fd0030":"Term frequency value is calculated, here for positive, pos contains the number of times a word occurs in the positive sentiment (datframe), pos_train.shape[0] will denote the number of documents ","77a73ac2":"Using count vectorizer to convert words to numbers","06ea1c20":"Writing the prediction in the submission.csv file","eaf68f23":"Removing stopwords","b18edbd9":"Score for each word (TF-IDF value)","ee0a5bda":"Prediction","4ed24966":"Count Vectorizer","a178a7d9":"Making a wordcloud representation","21d70c00":"Splitting the train data into train & validation set as we want to see how the mdethod we created would would on the final test data","9f8a9579":"Creating different dataset for different sentiments","e63d2a64":"Now we fit the model on our entire train data & calculate the prediction for test data ","853770e1":"Lemmatization","8ef444b4":"#Nrutya Doshi J013 \n#Rishabh Jain J021\n#Reuben Rapose J040 ","d0c190c1":"Inverse document frequency is calculated here, for positive, the tf of negative & neutral is subtracted from positive, to determine how positive a word is","20b422bf":"Cleaning the X_train data","4af9f179":"Importing important libraries","d1ee84ed":"Tokenization","163a51da":"Taking only words that contain alphabets in train data i.e. removing punctuations & numbers","ab9def2b":"Converting list to string","01171f18":"We caculate a score for each word, which is like a tf-idf score"}}