{"cell_type":{"03fca81d":"code","c0c6b689":"code","25e56e5f":"code","ea117b9b":"code","332013e7":"code","f5f74281":"code","80dc9010":"code","d7a85c4c":"code","6250af9e":"code","375ec741":"code","6adb1eaf":"code","14870257":"code","f6d7f89f":"code","553d46e5":"code","dcf1a883":"code","5bf5af0c":"code","d0218738":"code","68ea1a71":"code","708137c0":"code","90ba1441":"code","664ec9d2":"code","8a4dabb2":"code","5e0bba42":"code","523c3a11":"code","80d20c6e":"code","283673a1":"code","fdcaf8cf":"code","603b9ddd":"code","ef3abc5b":"code","057b8947":"code","d2a5624a":"code","bdab4a91":"code","1f0c6c47":"code","d0e9fa49":"code","217f4881":"code","68769da7":"code","641036c4":"code","e63dcdbf":"code","4bee12b4":"code","2a146070":"code","75829ac4":"code","5a3473be":"code","deb527e3":"code","c73cb38f":"code","e3d3ac34":"code","f2d6a8bc":"code","686bcf6f":"code","db400a29":"code","9091c39e":"code","d78c6e43":"code","d8a5de5b":"code","691709d8":"code","1e84e438":"code","57ffe921":"code","51d77320":"code","5f7d4fb5":"code","47efaad1":"code","b22e60a8":"code","5c596e87":"code","2bca7b3e":"code","1cb94871":"code","34409dd3":"code","ca03f422":"code","7c49bbe7":"code","8842e658":"code","95e3df30":"code","29152219":"code","e1a6caca":"code","193707cf":"code","0df51e45":"code","93767782":"code","a8c98da6":"markdown","d06c591c":"markdown","b7f625ae":"markdown","4fbc09e2":"markdown","183825d7":"markdown","2a4c8773":"markdown","45b2d89b":"markdown","b2047569":"markdown","674be043":"markdown","c4726db6":"markdown","bb0a6351":"markdown","83b7b0b3":"markdown","078b245a":"markdown","ce05e6d1":"markdown","d54a98a3":"markdown","247b2b95":"markdown","082c20b2":"markdown","218a9fd6":"markdown","13fe5b7b":"markdown","16c3da11":"markdown","c222ab24":"markdown","f6051911":"markdown"},"source":{"03fca81d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport numpy as np\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","c0c6b689":"df = pd.read_csv(\"..\/input\/craigslist-carstrucks-data\/vehicles.csv\")","25e56e5f":"df.columns","ea117b9b":"drop_columns = ['id', 'url', 'region', 'region_url', 'VIN', 'image_url', 'description', 'lat', 'long', 'posting_date']\ndf.drop(drop_columns, axis=1, inplace=True)","332013e7":"from pandas_profiling import ProfileReport\nprof = ProfileReport(df)\nprof.to_notebook_iframe()","f5f74281":"# set the threshold for price\n# transform year to age\n# drop model column\n# evaluate other categorical in fuel column\n# set the threshold for odometer and seek the connection with age column\n# pick just clean title status\n# drop county column\n# drop state\n# drop duplicates\n# missing value analysis","80dc9010":"# set the threshold for price","d7a85c4c":"df[df['price'] == df['price'].min()]","6250af9e":"df.sort_values(by='price', ascending=False).head()","375ec741":"df = df[(df['price'] <= 50000) & (df['price'] >= 1000) ]","6adb1eaf":"# transform year to age","14870257":"df['age'] = 2021 - df['year']","f6d7f89f":"df.drop(['year'], axis=1, inplace=True)","553d46e5":"# drop model, county and state column\ndf.drop(columns=['model', 'county', 'state'], inplace=True)","dcf1a883":"# pick just clean title status\ndf.drop(['title_status'], axis=1, inplace=True)","5bf5af0c":"df['fuel'].value_counts()","d0218738":"# set the threshold for odometer and seek the connection with age column\ndf.sort_values(by='odometer').head()","68ea1a71":"# give the minimum threshold for odometer and age, which make sense for used car\n# I give an age threshold of 1 years old at minimum age\ndf = df[df['age'] >= 1]","708137c0":"# and for the odometer, \n# Americans drive an average of 14,300 miles per year, according to the Federal Highway Administration. (https:\/\/www.thezebra.com\/resources\/driving\/average-miles-driven-per-year\/)\ndf = df[df['odometer'] >= 14000]","90ba1441":"df.drop(['paint_color'], axis=1, inplace=True)","664ec9d2":"df = df.drop_duplicates()","8a4dabb2":"df.head()","5e0bba42":"sns.boxplot(data=df, y='odometer')","523c3a11":"df[df['odometer'] == df['odometer'].max()]","80d20c6e":"# based on 14300 miles per year, if we multiple 100 times for 100 years it will be 1430000, so i set to the maximum in the digit which is 9999999\ndf = df[df['odometer'] <= 9999999 ]","283673a1":"sns.boxplot(data=df, y='odometer')","fdcaf8cf":"df['manufacturer'].fillna('other', inplace=True)","603b9ddd":"df = df.dropna()","ef3abc5b":"from pandas_profiling import ProfileReport\nprof = ProfileReport(df)\nprof.to_notebook_iframe()","057b8947":"df.head()","d2a5624a":"from sklearn.preprocessing import LabelEncoder\n\ncat_columns = ['manufacturer', 'condition', 'cylinders', 'fuel', 'transmission', 'drive', 'size', 'type']\n\nle = {}\n\nfor col in cat_columns:\n    if col in df.columns:\n        le[col] = LabelEncoder()\n        le[col].fit(list(df[col].astype(str).values))\n        df[col] = le[col].transform(list(df[col].astype(str).values))","bdab4a91":"df.head()","1f0c6c47":"y = df['price']\nX = df.drop(columns='price')","d0e9fa49":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","217f4881":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","68769da7":"def model_selection(X_train, y_train, X_test, y_test, models):\n    \n    from sklearn.metrics import max_error, mean_absolute_error, mean_squared_error, r2_score\n    \n    R2_result = []\n    MSE_result = []\n    str_models = []\n    \n    for model in models:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        MSE = mean_squared_error(y_test, y_pred)\n        R2 = r2_score(y_test, y_pred)       \n        R2_result.append(R2)\n        MSE_result.append(MSE)  \n        str_models.append(str(model))\n    \n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,10))\n\n    ax1.plot(R2_result)\n    ax1.set_ylabel('R2_score')\n\n\n    ax2.plot(str_models,np.sqrt(MSE_result))\n    ax2.set_ylabel('RMSE_result')\n    ax2.set_xticklabels(str_models, rotation=90)\n    plt.tight_layout()\n    \n    return pd.DataFrame({'models':models, 'R2':R2_result, 'RMSE':np.sqrt(MSE_result)}) ","641036c4":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor","e63dcdbf":"models = [LinearRegression(), Ridge(), Lasso(), BayesianRidge(), SVR(), KNeighborsRegressor(),\n          DecisionTreeRegressor(), BaggingRegressor(), RandomForestRegressor(), AdaBoostRegressor(), GradientBoostingRegressor(),\n          HistGradientBoostingRegressor(), MLPRegressor(), XGBRegressor()]\n\nmodel_selection(X_train, y_train, X_test, y_test, models)   ","4bee12b4":"def acc_CV(model, X, y):\n    from sklearn.model_selection import cross_val_score\n\n    accuracies = cross_val_score(estimator = model, X= X, y=y,  cv=10)\n    accuracies.mean()\n    accuracies.std()\n    print('akurasi  {:.2f}% +\/- {:.2f}%' .format(accuracies.mean()*100, accuracies.std()*100))","2a146070":"rf = RandomForestRegressor()\nHGB = HistGradientBoostingRegressor()\nXGB = XGBRegressor()","75829ac4":"acc_CV(rf, X_train, y_train)","5a3473be":"acc_CV(HGB, X_train, y_train)","deb527e3":"acc_CV(XGB, X_train, y_train)","c73cb38f":"def tuning_param(X, y, model, parameters):\n    \n    from sklearn.model_selection import GridSearchCV   \n        \n    scores = []\n   \n    fig, axs = plt.subplots(len(parameters))\n    k = 0   \n    for parameter in parameters:\n        \n        clf = GridSearchCV(estimator = model, param_grid = parameter, cv=3, scoring='r2', n_jobs=-1)\n        clf.fit(X, y)\n        \n\n        for name_param, val_param in parameter.items():\n            \n            grid_mean_scores = clf.cv_results_['mean_test_score']\n\n            if len(parameters) == 1:\n               axs.plot(val_param, grid_mean_scores)\n               axs.set_xlabel(name_param)\n               axs.set_ylabel('R2') \n            \n            else:\n                axs[k].plot(val_param, grid_mean_scores)\n                axs[k].set_xlabel(name_param)\n                axs[k].set_ylabel('R2')\n                \n                k+=1\n            \n            \n        scores.append({'parameter':name_param,\n                       'best_R2':clf.best_score_,\n                       'best_value':clf.best_params_})\n            \n    plt.tight_layout()       \n    return pd.DataFrame(scores, columns=['parameter', 'best_R2', 'best_value'])","e3d3ac34":"tuning_param(X_train, y_train, rf, [{'n_estimators': np.arange(100, 1000, 50)}])","f2d6a8bc":"tuning_param(X_train, y_train, rf, [{'min_samples_split':np.arange(1, 25, 1)}])","686bcf6f":"tuning_param(X_train, y_train, rf, [{'min_samples_leaf':np.arange(1, 10, 1)}])","db400a29":"tuning_param(X_train, y_train, rf, [{'max_depth':np.arange(10,250, 10)}])","9091c39e":"tuning_param(X_train, y_train, rf, [{'max_features':['auto', 'sqrt']}])","d78c6e43":"def model_randomCV(X, y, model, parameters):\n    \n    from sklearn.model_selection import RandomizedSearchCV\n    \n    randCV = RandomizedSearchCV(estimator=model, param_distributions=parameters, n_jobs=-1, cv=5)\n    \n    randCV.fit(X, y)\n   \n    print('best_parameters: ' + str(randCV.best_params_))\n    print('best_score: ' + str(randCV.best_score_))\n    print('best_estimator: ' + str(randCV.best_estimator_))    \n    \n    return pd.DataFrame(randCV.cv_results_).sort_values(by='rank_test_score')","d8a5de5b":"parameters = {'n_estimators': np.arange(150, 260, 10), 'max_features':['auto', 'sqrt'], 'max_depth':np.arange(100,210, 10), \n             'min_samples_split':np.arange(8, 15, 1), 'min_samples_leaf':np.arange(1,6,1)}\n\nmodel_randomCV(X_train, y_train, rf, parameters)","691709d8":"def fit_check(model, kfolds):\n    \n    from sklearn.model_selection import KFold\n    from sklearn.metrics import mean_squared_error\n    \n    kf = KFold(n_splits=kfolds)\n    list_training_error = []\n    list_testing_error = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        model.fit(X_train, y_train)\n        y_train_data_pred = model.predict(X_train)\n        y_test_data_pred = model.predict(X_test)\n        fold_training_error = np.sqrt(mean_squared_error(y_train, y_train_data_pred)) \n        fold_testing_error = np.sqrt(mean_squared_error(y_test, y_test_data_pred))\n        list_training_error.append(fold_training_error)\n        list_testing_error.append(fold_testing_error)\n    \n    figsize=(5,5)\n    plt.plot(range(1, kf.get_n_splits() + 1), np.array(list_training_error).ravel(), 'o-', label = 'training')\n    plt.plot(range(1, kf.get_n_splits() + 1), np.array(list_testing_error).ravel(), 'o-', label = 'testing')\n    plt.xlabel('number of fold')\n    plt.ylabel('RMSE')\n    plt.title('RMSE across folds')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","1e84e438":"tuning_param(X_train, y_train, HGB, [{'learning_rate':np.arange(0.1, 1, 0.1)}])","57ffe921":"tuning_param(X_train, y_train, HGB, [{'max_leaf_nodes':np.arange(50, 150, 10)}])","51d77320":"tuning_param(X_train, y_train, HGB, [{'max_iter': np.arange(100, 600, 10)}])","5f7d4fb5":"tuning_param(X_train, y_train, HGB, [{'max_depth': np.arange(0, 500, 10)}])","47efaad1":"tuning_param(X_train, y_train, HGB, [{'min_samples_leaf': np.arange(0, 100, 10)}])","b22e60a8":"tuning_param(X_train, y_train, HGB, [{'l2_regularization': np.arange(0, 100, 10)}])","5c596e87":"parameters = {'learning_rate':np.arange(0.1, 0.4, 0.1), 'max_leaf_nodes':np.arange(100, 150, 10), 'max_iter': np.arange(200, 400, 10), \n             'max_depth': np.arange(50, 150, 10), 'min_samples_leaf':np.arange(0,50,10)}\n\nmodel_randomCV(X_train, y_train, HGB, parameters)","2bca7b3e":"tuning_param(X_train, y_train, XGB, [{'n_estimators': np.arange(50, 250, 10)}])","1cb94871":"tuning_param(X_train, y_train, XGB, [{'max_depth': np.arange(1, 11, 1)}])","34409dd3":"tuning_param(X_train, y_train, XGB, [{'eta': np.arange(0.1, 1, 0.1)}])","ca03f422":"tuning_param(X_train, y_train, XGB, [{'eta': np.arange(0.1, 0.3, 0.01)}])","7c49bbe7":"tuning_param(X_train, y_train, XGB, [{'subsample': np.arange(0.1, 1, 0.1)}])","8842e658":"tuning_param(X_train, y_train, XGB, [{'colsample_bytree': np.arange(0.1, 1, 0.1)}])","95e3df30":"parameters = {'n_estimators': np.arange(125, 175, 5), 'max_depth': np.arange(4, 9, 1), 'eta': np.arange(0.1, 0.4, 0.1), \n             'subsample': [1], 'colsample_bytree': [1]}\n\nmodel_randomCV(X_train, y_train, XGB, parameters)","29152219":"HGB.fit(X_train, y_train)","e1a6caca":"def predicted_price(manufacturer, condition, cylinders, fuel, odometer, transmission, drive, size, type, age):\n    \n    x = np.zeros(10)\n    x[0] = le['manufacturer'].transform([manufacturer])\n    x[1] = le['condition'].transform([condition])\n    x[2] = le['cylinders'].transform([cylinders])\n    x[3] = le['fuel'].transform([fuel])\n    x[4] = odometer\n    x[5] = le['transmission'].transform([transmission])\n    x[6] = le['drive'].transform([drive])\n    x[7] = le['size'].transform([size])\n    x[8] = le['type'].transform([type])\n    x[9] = age\n    \n    x = scaler.transform([x])\n    \n    \n    return HGB.predict(x)","193707cf":"predicted_price('toyota', 'excellent', '4 cylinders', 'gas', 1000000.0 , 'automatic', 'rwd', 'mid-size', 'sedan', 20)","0df51e45":"predicted_price('toyota', 'excellent', '4 cylinders', 'gas', 1000000.0 , 'automatic', 'rwd', 'mid-size', 'sedan', 1)","93767782":"predicted_price('ford', 'excellent', '4 cylinders', 'gas', 1000000.0 , 'automatic', 'rwd', 'mid-size', 'sedan', 1)","a8c98da6":"it can calculate in one time using those code above, but because computational reason here i am using one parameter for one code.","d06c591c":"## XGB Model","b7f625ae":"## HGB model","4fbc09e2":"## Random Forest model","183825d7":"I compare some regression algorithm to get the basic idea of the idea of how is the model behave on the default parameters on the algorithms. here I choose the top 3 algorithms","2a4c8773":"### Compare the regression models","45b2d89b":"I used the label encoder rather than dummy to get simpler model with the least features in the model, cause in the data the categorical column ratio is 8 out of 10. I can also use dummy with the same result, but its increase the computational cost.","b2047569":"### Tuning parameters","674be043":"https:\/\/github.com\/codebasics\/py\/blob\/master\/ML\/15_gridsearch\/15_grid_search.ipynb\nhttps:\/\/github.com\/justmarkham\/scikit-learn-videos\/blob\/master\/08_grid_search.ipynb","c4726db6":"### Tuning using GridSearchCV per parameters\n","bb0a6351":"### Prepare the dataset","83b7b0b3":"### Tuning parameters","078b245a":"the performance using hyperparameter tuning increase from 0.775 to 0.781, even its not big change but still its the performance progress. but before using it in the final model, we should check the underfitting and overfitting from model to get the confidence that the model is works for the unknown data \/ unseen samples ","ce05e6d1":"here in the kaggle notebook, my fit_check got an error. I don't know why, cause in Jupyter Notebook works very well. and I think Iam too lazy to fix it here, so here in my kaggle I skip this process. if you want to check my complete works, you can visit my github at https:\/\/github.com\/RodzanIskandar\/used_car_price_prediction","d54a98a3":"### Hyperparameter using RandomizedsearchCV ","247b2b95":"The result from the HGB model is kind of trade off, the HGB with hyperparameter tuning got the better R2 but the worse overfitting case, and vice versa for the HGB default","082c20b2":"1. Overfitting is when the model\u2019s error on the training set (i.e. during training) is very low but then, the model\u2019s error on the test set (i.e. unseen samples) is large!\n\n2. Underfitting is when the model\u2019s error on both the training and test sets (i.e. during training and testing) is very high.\n\nhttps:\/\/towardsdatascience.com\/is-your-model-overfitting-or-maybe-underfitting-an-example-using-a-neural-network-in-python-4faf155398d2","218a9fd6":"### underfit and overfit check","13fe5b7b":"From the tuning_param function we can see the effect of the parameters change to R2 performance and the parameters result can be the value range for the hyperparameter tuning using RandomizedsearchCV. ","16c3da11":"### Hyperparameter using RandomizedsearchCV","c222ab24":"### Cross Validation for the best model in model comparasion","f6051911":"### Final predicted model"}}