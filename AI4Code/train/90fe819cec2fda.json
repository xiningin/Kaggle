{"cell_type":{"f814f6b9":"code","c7d233d1":"code","0e28b18b":"code","547e42d2":"code","e592b3f8":"code","779fb843":"code","813002d2":"code","29a55260":"code","9c9ebf69":"code","ff58d77d":"code","83f021f7":"code","2146839d":"code","67a0c7b5":"code","dcc6811e":"code","61d4125f":"code","5d019600":"code","a9dbe2e0":"code","05d0c139":"code","4822f0a8":"code","9c63b240":"code","8e4cba36":"code","ba5bc8de":"code","a290ea88":"code","81c23f88":"code","97f39862":"code","3adc8786":"code","ae9d6290":"code","e178bb66":"code","f61684ef":"code","61d88561":"code","692e3af1":"code","d60821b2":"code","1db14f7e":"code","9db40f79":"code","a8fcba3e":"code","7acad010":"code","3d52ee25":"code","7cb537ea":"code","b9f8d81c":"code","a5d15c48":"code","7e3a3172":"code","49761f7d":"code","9e285c99":"code","a29603d3":"markdown","3f023c11":"markdown","f73510d1":"markdown","e5aea755":"markdown","c43c0419":"markdown","3836d938":"markdown","15f31696":"markdown","482531c5":"markdown","65eae8b6":"markdown","9a6607fe":"markdown","752bc10c":"markdown","025114f0":"markdown","c153d389":"markdown","62870417":"markdown","97306ab3":"markdown","9b813661":"markdown","492e96f0":"markdown","61f6d83d":"markdown","9b562d58":"markdown","669e81b2":"markdown","f368e389":"markdown","a4bfa5bb":"markdown","22259295":"markdown","2df83b90":"markdown","342d4d80":"markdown","c6a3c7b2":"markdown"},"source":{"f814f6b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7d233d1":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","0e28b18b":"#importing dataviz packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#plotting scatterplot to show a variable's homoscedasticity\nsns.regplot(x = train['YearBuilt'], y = train['SalePrice'], scatter_kws={\"color\": \"red\"}, line_kws={\"color\": \"black\"})\nplt.title(\"YearBuilt vs SalePrice\")","547e42d2":"#plotting QQ plot to further check for normality\nimport pylab \nimport scipy.stats as stats\n\nstats.probplot(train['SalePrice'], dist=\"norm\", plot=pylab)\npylab.show()","e592b3f8":"#plotting distribution of SalePrice\n\nsns.distplot(train['SalePrice'])\nplt.title(\"Sale Price Distribution\")","779fb843":"import seaborn as sns\n\nplt.subplots(figsize = (40,30))\n\ncorr = train.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask=mask, cmap = 'Greys', annot = True)\nplt.title(\"Heatmap for all variables\")\nplt.show()","813002d2":"#to filter out variables, let's drop all variables between - 0.3 to 0.3 in correlation coefficient with the SalePrice\ncorr[(corr['SalePrice'] < 0.3) & (corr['SalePrice'] > -0.3)].index.to_list()","29a55260":"sns.lineplot(x = 'MoSold', y = 'SalePrice', data = train)\nplt.title(\"Monthly Sale Price\")","9c9ebf69":"sns.catplot(x = 'Neighborhood', y = 'SalePrice', data = train, height=8.27, aspect=11.7\/8.27)\nplt.xticks(rotation = 45)","ff58d77d":"sns.catplot(x = 'Neighborhood', y = 'YearBuilt', data = train, height=8.27, aspect=11.7\/8.27)\nplt.xticks(rotation = 45)","83f021f7":"#plotting GrLivArea vs SalePrice to check outliers\nsns.regplot(x = train['GrLivArea'], y = train['SalePrice'], scatter_kws={\"color\": \"red\"}, line_kws={\"color\": \"black\"})\nplt.title(\"GrLivArea vs SalePrice\")","2146839d":"train.drop(train[train['GrLivArea'] >= 4000].index, inplace = True)","67a0c7b5":"#combining both train and test set for imputing missing values\ndf = pd.concat([train, test], axis = 0)\n\nmissing = df.isnull().sum().sort_values(ascending = False).reset_index().rename(columns = {\"index\":\"var_name\", 0:\"missing_count\"})\nmissing['missing_percent'] = (missing['missing_count']\/len(df)) * 100\nmissing[missing['missing_percent'] != 0]","dcc6811e":"train.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], \n        axis = 1, inplace = True)\n\ntest.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], \n        axis = 1, inplace = True)","61d4125f":"missing_df = missing[(missing['missing_percent'] < 40) & (missing['missing_percent'] != 0)]\nmissing_df","5d019600":"#creating mode imputation function\ndef mode_imputation(col, data):\n    data[col] = data[col].fillna(data[col].mode()[0])\n\nfillna_mode = ['Utilities', 'BsmtFullBath', 'BsmtHalfBath', 'GarageArea', 'BsmtFinSF2', 'Exterior1st', 'TotalBsmtSF', \n               'BsmtUnfSF', 'Electrical', 'BsmtFinSF1', 'KitchenQual', 'SaleType', 'Exterior2nd', 'MSZoning', 'Functional']\n\nfor col in fillna_mode:\n    mode_imputation(col, train)\n    \nfor col in fillna_mode:\n    mode_imputation(col, test)","a9dbe2e0":"#creating none imputation function for object types of data\ndef na_impute(col, data):\n    if data[col].dtype==np.object:\n        data[col] = data[col].fillna('None')\n    else:\n        data[col] = data[col].fillna(0)\n\nfillna_none = ['GarageQual', 'GarageFinish', 'GarageCond', 'GarageType', 'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1',\n             'MasVnrType', 'MasVnrArea']\n\nfor col in fillna_none:\n    na_impute(col, train)\n    \nfor col in fillna_none:\n    na_impute(col, test)","05d0c139":"#Add BsmtFullBath, BsmtHalfBath, FullBath, HalfBath together to form TotalBath\ntrain['TotalBath'] = train['BsmtFullBath'] + (0.5 * train['BsmtHalfBath']) + train['FullBath'] + 0.5 * train['HalfBath']\ntest['TotalBath'] = test['BsmtFullBath'] + (0.5 * test['BsmtHalfBath']) + test['FullBath'] + 0.5 * test['HalfBath']\n\n#Add 1stFlrSF, 2ndFlrSF to form TotalSF\ntrain['TotalSF'] = train['1stFlrSF'] + train['2ndFlrSF']\ntest['TotalSF'] = test['1stFlrSF'] + test['2ndFlrSF']\n\n\n#Add OpenPorchSF, EnclosedPorch,3SsnPorch, ScreenPorch as TotalPorchArea\ntrain['TotalPorchArea'] = train['OpenPorchSF'] + train['EnclosedPorch'] + train['3SsnPorch'] + train['ScreenPorch']\ntest['TotalPorchArea'] = test['OpenPorchSF'] + test['EnclosedPorch'] + test['3SsnPorch'] + test['ScreenPorch']\n\n#drop columns\nto_drop = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', '1stFlrSF', '2ndFlrSF', \n           'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'GarageYrBlt', 'GarageCars'\n           ,'TotRmsAbvGrd', 'LandSlope', 'LowQualFinSF','TotRmsAbvGrd', 'LotFrontage'\n           , 'MSSubClass', 'OverallCond', 'PoolArea', 'MiscVal']\n\n#drop the original columns\ntrain.drop(to_drop,axis = 1, inplace = True)\ntest.drop(to_drop,axis = 1, inplace = True)\n","4822f0a8":"#mapping all ordinal variables first\n\n#mapping variable 'Functional'\nfunctional_di = {'Typ':7, 'Min1':6,'Min2':5,'Mod':4,'Maj1':3,'Maj2':2,'Sev':1, 'Sal':0}\ntrain['Functional'] = train['Functional'].replace(functional_di)\ntest['Functional'] = test['Functional'].replace(functional_di)\n\n\n#mapping variables with similar categories\nqual_cond_di =  {'Ex': 5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'None':0}\nqual_to_map = ['ExterQual', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond', 'ExterCond']\n\nfor col in qual_to_map:\n    train[col] = train[col].replace(qual_cond_di)\n    test[col] = test[col].replace(qual_cond_di)\n\n#mapping variable 'Electrical'\nelectrical_di = {'SBrkr':5 ,'FuseA':4 ,'FuseF':3,'FuseP':2,'Mix':1}\ntrain['Electrical'] = train['Electrical'].replace(electrical_di)\ntest['Electrical'] = test['Electrical'].replace(electrical_di)\n\n#mapping variable Utilities\nutilities_di = {'AllPub':4,'NoSewr':3,'NoSeWa':2,'ELO':1}\ntrain['Utilities'] = train['Utilities'].replace(utilities_di)\ntest['Utilities'] = test['Utilities'].replace(utilities_di)\n\n#mapping variables BsmtFinType1 and BsmtFinType2\nbsmt_di = {'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'NA':0}\nbsmt_map = ['BsmtFinType1','BsmtFinType2']\n\nfor col in bsmt_map:\n    train[col] = train[col].replace(bsmt_di)\n    test[col] = test[col].replace(bsmt_di)","9c63b240":"for col in bsmt_map:\n    train[col] = train[col].replace('None', 0)\n    test[col] = test[col].replace('None', 0)","8e4cba36":"train_examples_length = len(train)\n\ndataset = pd.concat([train, test], axis = 0)\n\n#getting all nominal categorical variables\nnominal_cat = train.select_dtypes(include = 'object').columns.to_list()\n\n#getting dummies\ndataset = pd.get_dummies(dataset[nominal_cat], drop_first = True)\n\n#slicing train and test\ndummies_train = dataset[:train_examples_length]\ndummies_test = dataset[train_examples_length:]\n\n\n#dropping nominal_cat variables\ntrain.drop(nominal_cat, axis = 1 , inplace = True)\ntest.drop(nominal_cat, axis = 1, inplace = True)","ba5bc8de":"train['SalePrice'] = train[\"SalePrice\"].apply(np.log)","a290ea88":"#show that we now have a gaussian-like distribution for SalePrice now\nsns.distplot(train['SalePrice'])","81c23f88":"#scaling all our numerical variables to make gradient descent converge faster\n\nfrom sklearn.preprocessing import StandardScaler\n\nto_normalize = ['LotArea','YearBuilt','YearRemodAdd','MasVnrArea','BsmtFinSF1',\n                'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GrLivArea','BedroomAbvGr',\n                'KitchenAbvGr','GarageArea','WoodDeckSF','TotalSF','TotalPorchArea']\n\nscaler = StandardScaler()\n\nscaler.fit(train[to_normalize])\nnumerical_cat_train = scaler.transform(train[to_normalize])\n\nscaler.fit(test[to_normalize])\nnumerical_cat_test = scaler.transform(test[to_normalize])","97f39862":"ordinal_variables = ['Functional', 'ExterQual', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'GarageQual', \n                    'GarageCond', 'ExterCond', 'Electrical', 'Utilities', 'BsmtFinType1','BsmtFinType2']\n\n\nordinal_array_train = np.array(train[ordinal_variables])\nordinal_array_test = np.array(test[ordinal_variables])","3adc8786":"#creating our y variable (SalePrice)\ny = train['SalePrice'].to_numpy().reshape(-1, 1)","ae9d6290":"train_np = np.concatenate([numerical_cat_train, ordinal_array_train, dummies_train], axis = 1)\ntest_np = np.concatenate([numerical_cat_test, ordinal_array_test, dummies_test], axis = 1)","e178bb66":"#splitting our data into training and validation set\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_np, y, test_size = 0.2, random_state = 40)\n","f61684ef":"from sklearn.linear_model import LinearRegression\n\n#creating a linear regression object\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_pred = linreg.predict(X_val)","61d88561":"#scoring our prediction\nfrom sklearn.metrics import mean_squared_error\n\nmean_squared_error(y_pred, y_val)","692e3af1":"regularization_parameters = [0.001, 0.001, 0.01, 0.05, 0.1]\n\nfrom sklearn.linear_model import Lasso \nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\n\nlasso_mse = {}\nfor i in regularization_parameters:\n    lasso = Lasso(alpha = i)\n    lasso.fit(X_train, y_train)\n    y_pred = lasso.predict(X_val)\n    mse = mean_squared_error(y_pred, y_val)\n    lasso_mse[i] = mse\n    \nridge_mse = {}\nfor i in regularization_parameters:\n    ridge = Ridge(alpha = i)\n    ridge.fit(X_train, y_train)\n    y_pred = lasso.predict(X_val)\n    mse = mean_squared_error(y_pred, y_val)\n    ridge_mse[i] = mse\n\nelasticnet_mse = {}\nfor i in regularization_parameters:\n    elasticnet = ElasticNet(alpha = i)\n    elasticnet.fit(X_train, y_train)\n    y_pred = elasticnet.predict(X_val)\n    mse = mean_squared_error(y_pred, y_val)\n    elasticnet_mse[i] = mse","d60821b2":"lasso_mse","1db14f7e":"ridge_mse","9db40f79":"elasticnet_mse","a8fcba3e":"#adding xgboost and SVR\nimport xgboost as xgb\n\nxgboost = xgb.XGBRegressor(verbosity = 0)\nxgboost.fit(X_train, y_train)","7acad010":"y_val_pred = xgboost.predict(X_val)\nmean_squared_error(y_val_pred, y_val)","3d52ee25":"#adding decision tree regressor \n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n#instantiating hyperparameters to choose from\nmax_depth = [2,4,6,8,10,12]\n\n\ndtregressor = DecisionTreeRegressor(criterion = 'mse')\nparam_grid = [{'max_depth':max_depth,\n               'splitter':['best','random']}]\n\ngs = GridSearchCV(estimator = dtregressor, param_grid = param_grid)\ngs = gs.fit(X_train, y_train)\n","7cb537ea":"y_pred_val = gs.predict(X_val)\nmean_squared_error(y_pred_val, y_val)","b9f8d81c":"lasso = Lasso(alpha = 0.001)\nlasso = lasso.fit(X_train, y_train)\n\nelasticnet = ElasticNet(alpha = 0.001)\nelasticnet = elasticnet.fit(X_train, y_train)","a5d15c48":"def blendModels(X):\n    return (0.25 * lasso.predict(X) +\n            0.25 * gs.predict(X) +\n            0.25 * xgboost.predict(X) +\n            0.25 * elasticnet.predict(X))","7e3a3172":"y_pred = blendModels(test_np)","49761f7d":"#inverse log SalePrice with np.exp\ny_pred = np.exp(y_pred)","9e285c99":"y_pred = pd.DataFrame(y_pred, columns = ['SalePrice'])\nresults = pd.concat([test['Id'], y_pred], axis = 1)\nresults.to_csv('predictions.csv',index=False)\n","a29603d3":"As you can see, SalePrice is skewed to the right. We will normalize SalePrice later on using log transformation in our Data PreProcessing Stage.","3f023c11":"# Feature Scaling","f73510d1":"This is an example of hetereoscedasticity. SalePrice in the 1920s-1980s have a visually uniform variance. But the older YearBuilt houses (1880) has greater variance. As we go on to the aughts, there are also greater variance in SalePrice. We will fix this by applying log transformation to our dependent variable later on in preprocessing.","e5aea755":"# Assumptions of Regression\nIn order to create a linear regression model, the data we use to model must satisfy these 5 assumptions:\n1. Linearity\n2. Homoscedasticity\n3. Normality\n4. Multicollinearity\n\nIf any of these assumptions are not met, then our model will produce a biased\/misleading result. \n\n","c43c0419":"# Prediction","3836d938":"# 2. Homoscedasticity\nHomoscedasticity means \"same variance\" in the relationship between the dependent and independent variables. When homoscedasticity is violated, we get what's called heteroscedasticity, in which some of the residuals in the data will have greater variance compared to some other data points. \n\n","15f31696":"# Feature Engineering","482531c5":"Next steps:\n- Use K-Fold for training","65eae8b6":"# Data Preprocessing","9a6607fe":"# Exploring Our Variables\n \n**From the heatmap, we can see several multicollinearities:**\n- GarageYrBlt and YearBuilt: this makes sense. It's less likely that the garage is built separately from the house. So YearBuilt has a positive correlation with GarageYrBlt. \n- GarageArea and GarageCars\n- GrLivArea and TotalRmsAbvGrd\n- TotalBsmtSF and 1stFlrSF\n- GarageCond and GarageQual\n- LandSlope and LandContour\n- LotFrontage -- this variable won't be necessary if we have variables like area\n\nWe will drop 'GarageYrBlt', 'GarageCars', 'TotalRmsAbvGrd', 'GarageCond', 'LandSlope' variables during data preprocessing. \n\n**Some other features we will create:**\n- Add BsmtFullBath, BsmtHalfBath, FullBath, HalfBath together to form TotalBath\n- Add 1stFlrSF, 2ndFlrSF to form TotalSF\n- Add OpenPorchSF, EnclosedPorch,3SsnPorch, ScreenPorch as TotalPorchArea\n- LowQualFinSF (note to self: Question about LowQualFinSF what is the proportion of LowQualFinSF compared to GrLivArea? We need to be careful before adding them both together as TotalArea.)\n- TotalRmsAbvGrd (GrLivArea is enough. This is the same case as GarageArea and GarageCars.)\n\n\n**Categorical variables we will need to encode:**\n\n- Functional\n- MSSubClass\n- MSZoning\n- Street\n- LotShape\n- LandContour\n- Neighborhood (this is interesting to check out in EDA as well: it should be that the location of the house, i.e. the neighborhood, will greatly affect SalePrice)\n- Condition1\n- Condition2\n- BldgType\n- HouseStyle\n- OverallQual\n- OverallCond\n- RoofStyle\n- RoofMatl\n- Exterior1st\n- Exterior2nd\n- MasVnrType\n- MasVnrArea\n- ExterQual\n- ExterCond (potential multicollinearity with ExterQual)\n- Foundation\n- BsmtQual\n- BsmtCond (potential multicollinearity with BsmtQual)\n- BsmtExposure\n- BsmtFinType1\n- BsmtFinType2\n- Heating\n- HeatingQC\n- CentralAir\n- Electrical\n- KitchenQual\n- Utilities\n- LotConfig\n- Fireplace Quality\n- GarageType\n- GarageQual\n- GarageCond (Potential multicollinearity with GarageQual)\n- MoSold (if we have YrSold, would it be necessary to have MoSold? Does the time of the year affect SalePrice?)\n- SaleType\n- SaleCondition","752bc10c":"We will drop:\n* MSSubClass\n* OverallCond\n* PoolArea\n* MiscVal\n\nAll others will be used to create another feature (see below).\n\nI'm reluctant to drop MoSold and YrSold, because as you can see in the Exploratory Data Analysis above that there is some sort of trend in MoSold and SalePrice. ","025114f0":"# Submission","c153d389":"- Add BsmtFullBath, BsmtHalfBath, FullBath, HalfBath together to form TotalBath\n- Add 1stFlrSF, 2ndFlrSF to form TotalSF\n- Add OpenPorchSF, EnclosedPorch,3SsnPorch, ScreenPorch as TotalPorchArea\n- LowQualFinSF (note to self: Question about LowQualFinSF what is the proportion of LowQualFinSF compared to GrLivArea? We need to be careful before adding them both together as TotalArea.)\n- TotalRmsAbvGrd (GrLivArea is enough. This is the same case as GarageArea and GarageCars.)","62870417":"# Neighborhood vs YearBuilt\n\nInterestingly, we see that neighborhood \"Old Town\" has the oldest built houses. NoRidge and CollegeCr seem to have the newer houses.","97306ab3":"# Modelling","9b813661":"# Normalize SalePrice Using Log Transformation","492e96f0":"# 1. Linearity\n\nIn linear regression models, we assume that the value of dependent variable y has a linear relationship with the independent variables. Recall the formula y = mx + c from your high school math class. y in this case is the dependent variable we are trying to predict, the m is the coefficient \/ slope of the line, x is the independent variable, and c is the intercept (when x = 0, what is the value of y?). ","61f6d83d":"# Encoding Nominal Categorical Variables\n\nThere are two types of categorical variables:\n1. Ordinal - there is a clear ordering to the categories - such as \"Bad, Good, Excellent\"\n2. Nominal - no intrinsic ordering to the variable, such as \"Male or Female\"\n\nWe need a different approach for each types of categorical variables. \n* \n\nCategorical variables we will need to encode:\n\n* Functional - Ordinal\n* MSSubClass - Nominal\n* MSZoning - Nominal\n* Street - Nominal\n* LotShape - Nominal\n* LandContour - Nominal\n* Neighborhood - Nominal \n* Condition1 - Nominal\n* Condition2 - Nominal\n* BldgType - Nominal\n* HouseStyle - Nominal\n* OverallQual - Ordinal\n* OverallCond - Ordinal\n* RoofStyle - Nominal\n* RoofMatl - Nominal\n* Exterior1st - Nominal - note : housing materials of course affect housing prices. Could this category be ordinal? If so, how would we assign value to each of the materials? I think we should just proceed to declare this category as nominal. \n* Exterior2nd - Nominal\n* MasVnrType - Nominal\n* ExterQual - Ordinal\n* ExterCond - Ordinal\n* Foundation - Nominal\n* BsmtQual - Ordinal\n* BsmtCond - Ordinal\n* BsmtExposure - Nominal\n* BsmtFinType1 - Ordinal\n* BsmtFinType2 - Ordinal\n* Heating - Nominal\n* HeatingQC - Ordinal\n* CentralAir - Nominal\n* Electrical - Ordinal\n* KitchenQual - Ordinal\n* Utilities - Ordinal\n* LotConfig - Nominal\n* Fireplace Quality - Ordinal\n* GarageType - Nominal\n* GarageQual - Ordinal\n* GarageCond (Potential multicollinearity with GarageQual) - Ordinal\n* MoSold - Nominal\n* SaleType - Nominal\n* SaleCondition - Nominal","9b562d58":"* GarageQual\t- 159 missing values, similar to GarageFinish and GarageCond. Maybe it has no garage?\t\n* GarageFinish\t159\t\n* GarageCond\t159\n* GarageType\t157\n* BsmtExposure\t82 - Similar missing value count to BsmtCond and BsmtQual, maybe there is no basement for said rows. Impute \n* BsmtCond\t82\t\n* BsmtQual\t81\t\n* BsmtFinType2\t80\t\n* BsmtFinType1\t79\t\n* MasVnrType\t24\t- Also similar missing value count to MasVnrArea. I assume they're the same ones. \n* MasVnrArea\t23\t\n* MSZoning\t4\t- We can impute with mode, according to the neighborhood.\n* Utilities\t2\t- We can impute using mode. Ditto for the rest of the data. \n* Functional\t2\t\n* BsmtFullBath\t2\t\n* BsmtHalfBath\t2\t\n* GarageArea\t1\t\n* BsmtFinSF2\t1\t\n* Exterior1st\t1\t\n* TotalBsmtSF\t1\t\n* GarageCars\t1\t\n* BsmtUnfSF\t1\t\n* Electrical\t1\t\n* BsmtFinSF1\t1\t\n* KitchenQual\t1\t\n* SaleType\t1\t\n* Exterior2nd\t1\t","669e81b2":"# 4. No Multicollinearity\nMulticollinearity is when our independent variables are correlated with each other. This poses a problem for our linear model. Recall the formula y = mx + c. The m, or the coefficient, represents the change you would expect to see in y when there is a 1 unit change in x. When dependent variables are correlated, that means a change in one variable will also change another variable, making our model unstable and prone to fluctuations with small changes in variable values. \n\nA quick skim through the variables in our data shows a lot of multicollinearity that we will need to fix. For example, GarageCars and GarageArea have a linear relationship. This makes sense, the more area the garage has, the more cars it has. Another example would be 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', and 'TotalBsmtSF'. Those 4 would definitely have a linear relationship with TotalBsmtSF. ","f368e389":"# Neighborhood vs SalePrice","a4bfa5bb":"# Month Sold and Sale Price\n\n- Month 4 (April) has the lowest SalePrice\n- Month 9 (September) has the highest SalePrice","22259295":"# 3. Normality: Checking for normality on our target (dependent) variable: SalePrice\n\nBefore creating our model, we must first make sure our target (dependent) variable, in this case, SalePrice, must be normally distributed. \n\nSince we optimize (find parameters\/coefficients for the model) using Mean Squared Error, a linear regression model is sensitive to the dependent target's outliers. Those outliers in the dependent variable could disproportionately affect parameter optimization. ","2df83b90":"# Imputing Null Value Plan\n\n","342d4d80":"# Remove Outliers\n\n> Potential Pitfalls (Outliers): Although all known errors were corrected in the data, no\nobservations have been removed due to unusual values and all final residential sales\nfrom the initial data set are included in the data presented with this article. There are\nfive observations that an instructor may wish to remove from the data set before giving\nit to students (a plot of SALE PRICE versus GR LIV AREA will quickly indicate these\npoints). Three of them are true outliers (Partial Sales that likely don\u2019t represent actual\nmarket values) and two of them are simply unusual sales (very large houses priced\nrelatively appropriately). I would recommend removing any houses with more than\n4000 square feet from the data set (which eliminates these five unusual observations)\nbefore assigning it to students.\n\nhttp:\/\/jse.amstat.org\/v19n3\/decock.pdf","c6a3c7b2":"# Handling Missing Data\nWe will first check for missing data. If missing data is more than 40%, we will drop that variable altogether since it would be difficult to impute meaningful values when more than 40% of the data is missing."}}