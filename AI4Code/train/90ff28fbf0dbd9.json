{"cell_type":{"96ca9364":"code","7387e9cd":"code","1a45d9fa":"code","616e4718":"code","b113e5f1":"markdown","fc05fc72":"markdown","09704c94":"markdown","2a2951b8":"markdown","46d94310":"markdown","3083c9c0":"markdown"},"source":{"96ca9364":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\n\ntrain=pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\n\n\nX=train.drop(['loss','id'],axis=1)\ny=train['loss']\ntest=test.drop('id',axis=1)","7387e9cd":"params = {'n_estimators':5000,\n          'learning_rate': 0.02,\n          'subsample': 0.5,\n          'colsample_bytree': 0.7,\n          'max_depth': 6,\n          'booster': 'gbtree',\n          'tree_method': 'gpu_hist',\n          'reg_lambda': 60,\n          'reg_alpha': 60,\n           'n_jobs': 4}","1a45d9fa":"\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n\nsplits = 12\nstf = StratifiedKFold(n_splits=splits, shuffle=True)\noof= np.zeros((X.shape[0],))\nprediction = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(stf.split(X, y)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    model = XGBRegressor(**params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",verbose=0)\n    \n    prediction += model.predict(test) \/ splits\n    oof[valid_id] = model.predict(X_valid)\n    oof[oof < 0] = 0\n\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof[valid_id]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n\n      ","616e4718":"sub=pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\nsub[\"loss\"] = prediction\n\nsub.to_csv('submission.csv', index=False)\n","b113e5f1":"# Introduction\n\n* XGBoost stands for Extreme Gradient Boosting.\n\n* XGBoost is a library designed and optimized for boosting trees algorithms. Gradient boosting trees model is originally proposed by Friedman et al. The underlying algorithm of XGBoost is similar, specifically it is an extension of the classic gbm algorithm. By employing multi-threads and imposing regularization, XGBoost is able to utilize more computational power and get more accurate prediction.\n\n* XGB is builted around gradient boosting (GBM) as core\n \n**An Efficient Algorithm the reasons are:**\n1. The computational part is implemented in C++.\n2. It can be multi-threaded on a single machine.\n3. It preprocesses the data before the training algorithm.\n![](https:\/\/raw.githubusercontent.com\/szilard\/benchm-ml\/145e029e1d092539566ece52d528196b4117b411\/2-rf\/x-plot-time.png)","fc05fc72":"# features of XGBoost\n\n**Model Features**\n* Gradient Boosting algorithm also called gradient boosting machine including the learning rate.\n* Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels.\n* Regularized Gradient Boosting with both L1 and L2 regularization.\n\n**System Features**\n* Parallelization of tree construction using all of your CPU cores during training.\n* Distributed Computing for training very large models using a cluster of machines.\n* Out-of-Core Computing for very large datasets that don\u2019t fit into memory.\n* Cache Optimization of data structures and algorithm to make best use of hardware.\n\n**Algorithm Features**\n* Sparse Aware implementation with automatic handling of missing data values.\n* Block Structure to support the parallelization of tree construction.\n* Continued Training so that you can further boost an already fitted model on new data.\n","09704c94":"# 1. Bagging\nThe idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result.\n\n# 2. Boosting\n* Boosting is an ensemble technique that learns from previous predictor mistakes to make better predictions in the future. The technique combines several weak base learners to form one strong learner, thus significantly improving the predictability of models. \n* Boosting works by arranging weak learners in a sequence, such that weak learners learn from the next learner in the sequence to create better predictive models.","2a2951b8":"# ensemble methods\nEnsemble methods are techniques that aim at improving the accuracy of results in models by combining multiple models instead of using a single model. The combined models increase the accuracy of the results significantly. This has boosted the popularity of ensemble methods in machine learning.\n![](https:\/\/cdn.corporatefinanceinstitute.com\/assets\/ensemble-methods.png)","46d94310":"# IMPLEMENTATION OF XGB-","3083c9c0":"# Gradient descent vs Gradient boosting vs Gradient boosted trees\n* **Gradient descent** is an algorithm for finding a set of parameters that optimizes a loss function. \n\n* **Gradient boosting** is a technique for building an ensemble of weak models such that the predictions of the ensemble minimize a loss function. \n\n* **Gradient boosted trees**  application of gradient boosting on Decision trees, here individual weak models are trees.\n\n* **XGBoost** is one of the implementations of gradient boosted trees and it comes with a lot of hyperparameters which makes learning more easier. \n\n"}}