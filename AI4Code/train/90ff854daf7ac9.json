{"cell_type":{"515e4e68":"code","8152dfbc":"code","677f89e1":"code","a32ad93e":"code","88980f80":"code","e2832ff1":"code","9bb3f4c1":"code","81056943":"code","81d11cc2":"code","dd13ebe9":"code","8c57ad64":"code","7d52c1a1":"code","aab99001":"code","6d638124":"code","07550355":"code","017d1db1":"code","2a7787b6":"code","81da4e1b":"code","3eed9ecd":"code","3eb1003d":"code","d0605575":"markdown","685199c0":"markdown","9d938bef":"markdown","7f60f148":"markdown","b8df4a7c":"markdown","2fc6dcce":"markdown","a627aa1c":"markdown","e648c3c6":"markdown","4ebc5354":"markdown","5ff3e051":"markdown","610e2161":"markdown","09876227":"markdown"},"source":{"515e4e68":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8152dfbc":"# Set the current working directory for easier path calls\nos.chdir('\/kaggle\/input\/house-prices-advanced-regression-techniques')\n\n# Load the training data\nraw_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('test.csv')\nraw_data.head()","677f89e1":"print(f\"Amount of columns = {len(list(raw_data))}\\nAmount of rows = {len(raw_data)}\\nSo our dataset has dimensions 1460x81\")","a32ad93e":"# But first, let's stick the training and test data together so we can investigate and manipulate the data more easily. Also, let's set the target and IDs aside for now.\ntarget = raw_data['SalePrice']\ntest_Id = test_data['Id']\n\n# Let's remember where to split\nraw_len, test_len = len(raw_data), len(test_data)\n\nraw_data = raw_data.drop(['SalePrice'], axis=1)\nraw_data = pd.concat([raw_data, test_data], axis=0)\nraw_data = raw_data.drop(['Id'], axis=1)","88980f80":"# Counts the NaN types in a feature column and prints the number if it's not 0. (Also collects the really messy features in a list, and the less messy in another.)\nmessy_features = []\nless_messy_features = []\nfor feature in list(raw_data):\n    nan_count = raw_data[feature].isna().sum()\n    if nan_count:\n        print(f\"{feature} contains, {nan_count} NaN values\")\n        if nan_count > 0.40 * len(raw_data):\n            messy_features.append(feature)\n        else:\n            less_messy_features.append(feature)","e2832ff1":"print(f\"Here are the extremely messy feature we collected:\\n{messy_features}\\nLet's remove them for now.\")","9bb3f4c1":"less_messy_data = raw_data.drop(messy_features, axis=1)\nn_features = len(list(less_messy_data))\nprint(f\"We removed 5 features so our training data now has dimension:\\n1460x{n_features}\")","81056943":"less_messy_data[less_messy_features].head(20)","81d11cc2":"# There are some features that appear numerical, but are actually categorical, let's change them before going further.\n# less_messy_data['MSSubClass'] = less_messy_data['MSSubClass'].apply(str)\n# less_messy_data['OverallCond'] = less_messy_data['OverallCond'].astype(str)\n# less_messy_data['YrSold'] = less_messy_data['YrSold'].astype(str)\n# less_messy_data['MoSold'] = less_messy_data['MoSold'].astype(str)","dd13ebe9":"# Two collections of categorical and numerical features, respectively.\nfirst_elements = [less_messy_data[i].iloc[0] for i in list(less_messy_data)]\ncategorical_features = []\nnumerical_features = []\nfor i in range(len(first_elements)):\n    if type(first_elements[i]) == str:\n        categorical_features.append(list(less_messy_data)[i])\n    else:\n        numerical_features.append(list(less_messy_data)[i])","8c57ad64":"for feature in categorical_features:\n    most_occurring_category = less_messy_data[feature].value_counts().index[0]\n    less_messy_data[feature] = less_messy_data[feature].fillna(most_occurring_category)\n    \nless_messy_data.head(20)","7d52c1a1":"for feature in numerical_features:\n    feature_mean = less_messy_data[feature].mean()\n    less_messy_data[feature] = less_messy_data[feature].fillna(np.floor(feature_mean))\n    \nless_messy_data.head(20)","aab99001":"dummies = []\nfor cat in categorical_features:\n    dummies.append(pd.get_dummies(less_messy_data[cat]))\n    less_messy_data = less_messy_data.drop([cat], axis=1)\n    \nn_dummies = len(dummies)\ndummies = [less_messy_data] + dummies\nless_messy_data = pd.concat(dummies, axis=1)\n    \nless_messy_data.head(5)","6d638124":"# Let's just make sure that the one-hot column names are unique\ncolumns = list(less_messy_data)\nadd_num = 1\nfor _ in range(10):\n    for i in range(1, len(columns)):\n        if columns[i] in columns[:i]:\n            columns[i] = columns[i] + str(add_num)\n    add_num += 1\n\nless_messy_data.columns = columns","07550355":"print(f\"We now have {len(list(less_messy_data))} because of the one-hot encoding\")","017d1db1":"from sklearn.preprocessing import MinMaxScaler\nremaining_numerical_features = list(less_messy_data)[:(n_features - n_dummies)]\nscaler = MinMaxScaler()\nscaled = scaler.fit_transform(less_messy_data[remaining_numerical_features])\nless_messy_data[remaining_numerical_features] = scaled\nless_messy_data.head(5)","2a7787b6":"test_data = less_messy_data.iloc[raw_len:, :]\nless_messy_data = less_messy_data.iloc[:raw_len, :]","81da4e1b":"import xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy import stats\nfrom sklearn.model_selection import RandomizedSearchCV\n\ntest_parameters = {'n_estimators': [150, 300, 450, 600, 750, 1000],\n                  'learning_rate': [0.01, 0.07, 0.1, 0.25, 0.4, 0.6, 1],\n                  'subsample': [0.3, 0.5, 0.7, 0.9],\n                  'max_depth': [3, 4, 5, 6, 7, 8, 9],\n                  'colsample_bytree': [0.5, 0.7, 0.9],\n                  'min_child_weight': [1, 2, 3, 4]\n                  }\n\nxgb_randsearch = RandomizedSearchCV(XGBRegressor(), \n                                    param_distributions = test_parameters, \n                                    n_iter = 100, \n                                    scoring='neg_mean_squared_error',\n                                    verbose = 3,\n                                    n_jobs = -1)\n\nxgb_randsearch.fit(less_messy_data, target)","3eed9ecd":"# XGB\nprediction = xgb_randsearch.predict(test_data)\nprediction = pd.DataFrame(prediction, columns=['SalePrice'])\nsubmission = np.zeros((len(prediction), 2))\nsubmission[:, 0] = test_Id.values.flatten()\nsubmission[:, 1] = prediction.values.flatten()\nsubmission = pd.DataFrame(submission, columns=['Id', 'SalePrice'])\nsubmission.Id = submission.Id.astype(int)\nsubmission.to_csv('..\/..\/..\/kaggle\/working\/base_line_submission.csv', index=False)","3eb1003d":"submission","d0605575":"#### Let's do a quick normalization of our data, just for good measure","685199c0":"#### Now it's time for cleaning up the remaining features. Let's first investigate what type data these features contain.","9d938bef":"#### Let's now make a predictions and put together our submission for kaggle.","7f60f148":"# The goal of this notebook is to introduce a base-line model for the House Prices data set.","b8df4a7c":"## Let's load the data and do some simple data exploration\/analysis","2fc6dcce":"## Okay, enough with the data manipulation, let's finally do some training, base-line here we come!","a627aa1c":"#### Let's split the training and test sets into two again.","e648c3c6":"### Looks like there is a lot of missing data. Let's investigate that!","4ebc5354":"#### Okay, so we can see that most of these messy categorical features have a category that shines through the most. It's not pretty, but we will replace the missing values with these most frequently occurring categories. Which will preserve more information for our base-line model than if we were simply to remove them.","5ff3e051":"#### Okay, the data looks good. We have it all clean, there are no NaN values and if we want, we're ready for feature engineering. Let's not dabble too much on that front, but let's at least do _something_. Let's at least one-hot encode all our categorical features, that should do the trick.","610e2161":"#### Great, so now we've taken care of that problem. We can then take care of the messy features that contain continous values. Let's just fill those with their mean. Again, this is a simple solution, but it will definitely do for our base-line.","09876227":"#### That's quite a lot of missing data for many of the features of our dataset. We will have to do some feature engineering\/cleaning to make up for that. Another option is also to remove the features where the majority of the data are missing values and subsequently do some simpler clean-up on the features that contain few missing values. We will lose _some_ information for whatever algorithm we decide to employ by doing it this way, however it makes a good start for at least a base-line model."}}