{"cell_type":{"5ccc11fd":"code","d026c9ef":"code","f32d15d4":"code","893906c8":"code","b53e88fa":"code","87fe787e":"code","3806a545":"code","da63a64b":"code","ea6e1ab3":"code","907b0177":"code","6ac8ef2c":"code","f6d572a1":"code","54ec8cd4":"code","94de3d5e":"code","75df91fd":"code","cb8eb574":"code","26da7ce5":"code","64b381c1":"code","f20ed97b":"code","ab64fe76":"code","c23769a5":"code","908a51cf":"code","40887954":"code","21c2d2d6":"code","c3f5ad8d":"markdown","63328a89":"markdown","c7c5fbf3":"markdown","aed547c9":"markdown","176bf3fc":"markdown","67254bd7":"markdown","44a9c1fe":"markdown","d0e7fd93":"markdown","468c7c54":"markdown","9a531135":"markdown","a781ea81":"markdown","93684995":"markdown","fbcfdf20":"markdown","fb1e4653":"markdown"},"source":{"5ccc11fd":"%matplotlib inline","d026c9ef":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns","f32d15d4":"np.random.seed(10)\n\nn_samples = 30\n\ndef true_fun(X):\n    return np.cos(1.5 * np.pi * X)\n\nX = np.sort(np.random.rand(n_samples))\nnoise_size = 0.1\ny = true_fun(X) + np.random.randn(n_samples) * noise_size","893906c8":"np.random.rand(n_samples)","b53e88fa":"X.shape","87fe787e":"plt.scatter(X, y)","3806a545":"linear_regression = LinearRegression()\nlinear_regression.fit(X.reshape((30, 1)), y)","da63a64b":"print(linear_regression.intercept_)\nprint(linear_regression.coef_)","ea6e1ab3":"# equally spaced array of 100 values between 0 and 1, like the seq function in R\nX_to_pred = np.linspace(0, 1, 100).reshape(100, 1)\n\npreds = linear_regression.predict(X_to_pred)\n\nplt.scatter(X, y)\nplt.plot(X_to_pred, preds)\nplt.show()","907b0177":"X**2","6ac8ef2c":"X2 = np.column_stack((X, X**2))\nX2","f6d572a1":"linear_regression.fit(X2, y)","54ec8cd4":"print(linear_regression.intercept_)\nprint(linear_regression.coef_)","94de3d5e":"# equally spaced array of 100 values between 0 and 1, like the seq function in R\nX_p = np.linspace(0, 1, 100).reshape(100, 1)\nX_to_pred = np.column_stack((X_p, X_p**2))\n\npreds = linear_regression.predict(X_to_pred)\n\nplt.scatter(X, y)\nplt.plot(X_p, preds)\nplt.show()","75df91fd":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nnp.random.seed(9876789)","cb8eb574":"# We load a datase compiled by A.M. Guerry in the 1830's looking at social factors like crime and literacy\n# http:\/\/vincentarelbundock.github.io\/Rdatasets\/doc\/HistData\/Guerry.html\n# In general, statsmodels can download any of the toy datasets provided in R, and provides\n# the same documentation from within Python\ndta = sm.datasets.get_rdataset(\"Guerry\", \"HistData\", cache=True)\nprint(dta.__doc__)","26da7ce5":"original_df = dta.data\noriginal_df.head()","64b381c1":"# Now, let's select a subset of columns\nsubsetted_df = original_df[['Lottery', 'Literacy', 'Wealth', 'Region']]\nsubsetted_df.head(100)","f20ed97b":"df = dta.data[['Lottery', 'Literacy', 'Wealth', 'Region']].dropna()\ndf.head(100)","ab64fe76":"# Next, let's fit the model by using a formula, just as we can in R, then running .fit()\n# We regress the amount of money bet on the lottery on literacy, wealth, region, and\n# and interaction between literacy and wealth.\nmod = smf.ols(formula='Lottery ~ Literacy + Wealth + Region + Literacy:Wealth', data=df)\nres = mod.fit()\nprint(res.summary())","c23769a5":"# Next, we add polynomial terms for wealth, i.e., wealth^2 and wealth^3\nmod = smf.ols(formula='Lottery ~ Literacy + Wealth + I(Wealth ** 2.0) + I(Wealth ** 3.0) + Region + Literacy:Wealth', data=df)\nres = mod.fit()\nprint(res.summary())","908a51cf":"res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region)', data=df).fit()\nprint(res.params) # Print estimated parameter values","40887954":"print(res.bse) # Print standard errors for the estimated parameters","21c2d2d6":"print(res.predict()) # Print fitted values","c3f5ad8d":"The scikit-learn linear regression class has the same programming interface we saw with k-NN:","63328a89":"Let's fit a model of the form $y \\sim x + x^2$ to try and capture some of the non-linearity in the underlying cosine function.","c7c5fbf3":"Let's make a random dataset where X is uniformly distributed between 0 and 1, and y is a cosine function plus noise:","aed547c9":"We can get the parameters of the fit:","176bf3fc":"[http:\/\/statsmodels.sourceforge.net\/devel\/examples\/notebooks\/generated\/ols.html](http:\/\/statsmodels.sourceforge.net\/devel\/examples\/notebooks\/generated\/ols.html)","67254bd7":"## Statsmodels","44a9c1fe":"And we can print the predictions as a line:","d0e7fd93":"[http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression_)","468c7c54":"If it were an integer code instead of a string, we could explicitly make `Region` categorical like this:","9a531135":"The `statsmodels` package provides statistical functionality a lot like R's for doing OLS.  It should already be available on your machine if you've setup Anaconda.  Otherwise, you can run ```conda install statsmodels``` at a terminal\/prompt.","a781ea81":"### Using A Formula to Fit to a Pandas Dataframe","93684995":"[http:\/\/statsmodels.sourceforge.net\/0.6.0\/examples\/notebooks\/generated\/formulas.html](http:\/\/statsmodels.sourceforge.net\/0.6.0\/examples\/notebooks\/generated\/formulas.html)","fbcfdf20":"Here, we'll see examples of how to use the scikit-learn linear regression class, as well as the statsmodels OLS function, which is much more similar to R's lm function.","fb1e4653":"# Lecture 2: Linear Regressions"}}