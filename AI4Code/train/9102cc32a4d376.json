{"cell_type":{"03b357d8":"code","6edf59f8":"code","b8c6c6e5":"code","7a685547":"code","d6d86825":"code","f393ca3a":"code","217c8b36":"code","35a1a385":"code","202b9737":"code","cf8b816a":"code","f5204b63":"code","948246f9":"code","cab81a87":"code","decd0ef5":"code","e277e4c1":"code","025c8182":"code","944a77ec":"code","bb0c5bbe":"code","a498e3a2":"code","9b450ab4":"code","b8b35b47":"code","ec580b33":"code","ebe8cc2f":"code","ffd950db":"code","e2882c60":"code","51c8c79c":"code","141d480c":"code","e24b8ec9":"code","f3887e9b":"code","b2a657f0":"code","c1b6637e":"code","0a6c7f8c":"code","2d67d84e":"code","83f6a7eb":"code","a88b5575":"markdown","8b89154e":"markdown","2837a0a5":"markdown","d3b3721c":"markdown","1e78f555":"markdown","d13fe096":"markdown"},"source":{"03b357d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6edf59f8":"#IMPORTING THE DATA \npd.set_option('display.max_columns',None)\ntrain_full=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\",index_col=\"Id\")\ntest_full=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\",index_col=\"Id\")\ntrain_full.head()","b8c6c6e5":"test_full.head(2)","7a685547":"train_full.columns","d6d86825":"Categorical_cols=[cname for cname in train_full.columns if train_full[cname].dtypes=='object']\nNumerical_cols=set(train_full.columns)-set(Categorical_cols)","f393ca3a":"'''factor=4\nfor col in Numerical_cols:\n    upper_limit=train_full[col].mean()+(factor*train_full[col].std())\n    lower_limit=train_full[col].mean()-(factor*train_full[col].std())\n    train_full = train_full[(train_full[col] < upper_limit) & (train_full[col] > lower_limit)]'''","217c8b36":"train_full.describe()","35a1a385":"train_full.info()","202b9737":"train_full.Alley.value_counts()","cf8b816a":"#columns to get null values replced by 0\nOK_columns=['Alley','FireplaceQu','PoolQC','Fence','MiscFeature']\nX_full=train_full.copy()\nX_full=X_full.drop('SalePrice',axis=1)\nY=train_full['SalePrice']\nX_full[OK_columns]=X_full[OK_columns].fillna(0)\nX_full.info()","f5204b63":"#Getting Numerical columns and Categorical columns\nCategorical_cols=[cname for cname in X_full.columns if X_full[cname].dtypes=='object']\nNumerical_cols=[cname for cname in X_full.columns if X_full[cname].dtypes in ['float64','int64']]\n\nprint('Printing Categorical Columns\\n\\n',Categorical_cols)\nprint('\\n\\n\\n')\nprint('Printing Numerical Columns\\n\\n',Numerical_cols)\n","948246f9":"#Lets impute it\nfrom sklearn.impute import SimpleImputer\ncatimp=SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nnumimp=SimpleImputer(missing_values=np.nan, strategy='mean')\n\n#train\nimputed_categorical_train=pd.DataFrame(catimp.fit_transform(X_full[Categorical_cols].astype(str)))\nimputed_numerical_train=pd.DataFrame(numimp.fit_transform(X_full[Numerical_cols]))\nimputed_categorical_train.columns=X_full[Categorical_cols].columns\nimputed_numerical_train.columns=X_full[Numerical_cols].columns\nimputed_X_full=imputed_categorical_train.join(imputed_numerical_train)\n\n#test\nimputed_categorical_test=pd.DataFrame(catimp.transform(test_full[Categorical_cols]))\nimputed_numerical_test=pd.DataFrame(numimp.transform(test_full[Numerical_cols]))\nimputed_categorical_test.columns=test_full[Categorical_cols].columns\nimputed_numerical_test.columns=test_full[Numerical_cols].columns\nimputed_test_full=imputed_categorical_test.join(imputed_numerical_test)\n\nimputed_X_full.info()\n","cab81a87":"\"\"\"\nnew_vis = train_full.drop(Categorical_cols,axis=1)\nplt.figure(figsize=(30,30))\n\ncorrelation = new_vis.corr()\nprint(correlation)\n\nsns.heatmap(correlation,annot=True)\n\"\"\"","decd0ef5":"for col in Numerical_cols:\n    imputed_X_full[col]=(imputed_X_full[col]-imputed_X_full[col].mean())\/imputed_X_full[col].std()","e277e4c1":"#finding bad columns\ngood_columns=[cname for cname in Categorical_cols if set(imputed_X_full[cname]) == set(imputed_test_full[cname])]\nprint(good_columns)\nbad_columns=list(set(Categorical_cols)-set(good_columns))\nprint('\\n\\n')\nprint(bad_columns)","025c8182":"#OnehotEncoding and label encodings\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n\"\"\"#Finding low cardinality columns \nlow_cardinality_col=[cname for cname in good_columns if imputed_X_full[cname].nunique()<10]\nprint(low_cardinality_col)\n\n#Encoding low cardinality columns using one hot encoding\nOH=OneHotEncoder(handle_unknown='ignore', sparse=False)\nOne_Hot_Encoded_columns_train=pd.DataFrame(OH.fit_transform(imputed_X_full[low_cardinality_col]))\nOne_Hot_Encoded_columns_test=pd.DataFrame(OH.transform(imputed_test_full[low_cardinality_col]))\n\nOne_Hot_Encoded_columns_train.index=imputed_X_full.index\nOne_Hot_Encoded_columns_test.index=imputed_test_full.index\n\n#finding high cardinality columns\nhigh_cardinality_col=list(set(good_columns)-set(low_cardinality_col))\nprint(high_cardinality_col)\n\n#Encoding high cardinality columns using label encoding\nlabel_encoder=LabelEncoder()\nLabel_Encoded_columns_train=pd.DataFrame(label_encoder.fit_transform(imputed_X_full[high_cardinality_col]))\nLabel_Encoded_columns_test=pd.DataFrame(label_encoder.transform(imputed_test_full[high_cardinality_col]))\nLabel_Encoded_columns_test.columns=imputed_test_full[high_cardinality_col].columns\nLabel_Encoded_columns_train.columns=imputed_X_full[high_cardinality_col].columns\nX_train=Label_Encoded_columns_train.join(One_Hot_Encoded_columns_train)\nX_train=X_train.join(imputed_X_full[Numerical_cols])\nX_test=Label_Encoded_columns_test.join(One_Hot_Encoded_columns_test)\nX_test=X_test.join(imputed_test_full[Numerical_cols])\nX_train.describe()\n#One_Hot_Encoded_columns_train\n#Label_Encoded_columns_train.head()\"\"\"\n\n#Encoding every categorical using label encoding\nlabel_encoder=LabelEncoder()\nLabel_Encoded_columns_train=imputed_X_full[good_columns].apply(label_encoder.fit_transform)\nLabel_Encoded_columns_test=imputed_test_full[good_columns].apply(label_encoder.fit_transform)\nLabel_Encoded_columns_test.columns=imputed_test_full[good_columns].columns\nLabel_Encoded_columns_train.columns=imputed_X_full[good_columns].columns\nX_train=Label_Encoded_columns_train.join(imputed_X_full[Numerical_cols])\n\nX_test=Label_Encoded_columns_test.join(imputed_test_full[Numerical_cols])\n\nX_train.describe()\n#One_Hot_Encoded_columns_train\n#Label_Encoded_columns_train.head()\n\n\n\"\"\"X_train=imputed_X_full.drop(Categorical_cols,axis=1)\nX_test=imputed_test_full.drop(Categorical_cols,axis=1)\ncorrelation_correction=['YrSold','MiscVal','PoolArea','BsmtHalfBath','LowQualFinSF','BsmtFinSF2','OverallCond','MSSubClass']\nX_train=X_train.drop(correlation_correction,axis=1)\nX_test=X_test.drop(correlation_correction,axis=1)\"\"\"\n","944a77ec":"'''sns.barplot(x=train_full['CentralAir'],y=train_full[\"SalePrice\"])'''","bb0c5bbe":"'''sns.barplot(x=train_full['MSSubClass'],y=train_full[\"SalePrice\"])'''","a498e3a2":"'''sns.barplot(x=train_full['Street'],y=train_full[\"SalePrice\"])'''","9b450ab4":"'''sns.barplot(x=train_full['BsmtExposure'],y=train_full[\"SalePrice\"])'''","b8b35b47":"'''sns.barplot(x=train_full['BsmtFinType1'],y=train_full[\"SalePrice\"])'''","ec580b33":"print(train_full.SalePrice.describe())\nplt.figure(figsize=(9,8));\nsns.distplot(train_full.SalePrice, color='r', bins=100,hist_kws={'alpha':0.2})","ebe8cc2f":"set(train_full.dtypes.tolist())\nnum_data=train_full.select_dtypes(include=['float64','int64'])\n#X_train.hist(figsize=(16,20), bins=50, xlabelsize=8, ylabelsize=8);","ffd950db":"#corelation of numerical data\nnum_corr=num_data.corr()['SalePrice'][:-1]#because sale price is the latest column and we dont want self correlation\nprint(num_corr)\ngolden_num_features=num_corr[num_corr>0.5].index\nprint(golden_num_features)\nrest_features=golden_num_features=num_corr[num_corr<0.3].index","e2882c60":"'''X_train=X_train.drop(rest_features,axis=1)\nX_test=X_test.drop(rest_features,axis=1)'''","51c8c79c":"columnlist=[col for col in X_train.columns if col in Numerical_cols]\nX_train.describe()\nprint(columnlist)","141d480c":"Y.head()","e24b8ec9":"\"\"\"#THE LAST MERGE\nX_Full=X_train.copy()\nX_Full.append(Y)\nX_Full.SalePrice\"\"\"\n","f3887e9b":"\"\"\"#REMOVING OUTLIERS\nfactor=3\nfor col in columnlist:\n    upper_limit=X_train[col].mean()+(factor*X_train[col].std())\n    lower_limit=X_train[col].mean()-(factor*X_train[col].std())\n    X_Full = X_train[(X_train[col] < upper_limit) & (X_train[col] > lower_limit)]\n    \n    \n#X_FU.describe()\"\"\"","b2a657f0":"'''\nnew_vis = X_train.drop(low_cardinality_col,axis=1)\nplt.figure(figsize=(14,7))\n\ncorrelation = new_vis.corr()\nprint(correlation)\n\nsns.heatmap(correlation)\n'''","c1b6637e":"\n\n\nfrom sklearn.model_selection import train_test_split\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_train, Y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n","0a6c7f8c":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n# Define the model\nmy_model_3 = XGBRegressor(n_estimators=2000,learning_rate=0.01)\n\n# Fit the model\nmy_model_3.fit(X_train_full,y_train) # Your code here\n\n# Get predictions on test\npredictions_3 = my_model_3.predict(X_valid_full)\n\n#Get predictions on train\npredictions_train=my_model_3.predict(X_train_full)\n\n# Calculate MAE on test\nmae_3 = mean_absolute_error(y_valid,predictions_3)\n\n# Calculate MAE on train\nmae = mean_absolute_error(y_train,predictions_train)\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error: on test\" , mae_3)\n# Uncomment to print MAE\nprint(\"Mean Absolute Error: on train\" , mae)","2d67d84e":"preds_test=preds_test =my_model_3.predict(X_test)","83f6a7eb":"output = pd.DataFrame({'Id': test_full.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","a88b5575":"# **HANDLING MISSING VALUES**","8b89154e":"# **FUN WITH DATA**","2837a0a5":"**REMARKS**\n\noutliers are present in the data","d3b3721c":"# ENCODINGS","1e78f555":"**REMARKS**\n\n* Alley with 93.77% null data\n    * *but NA in data only means no alley access-probable replace-0*\n* FireplaceQu with about 50% null data\n    * *but NA in data only means no fire place-probable replace-0*\n* PoolQC with 99.5% null data\n    * *but NA in data only means no Pool-probable replace-0*\n* Fence with 81% null data\n    * *but NA in data only means no fencing-probable replace-0*\n* MiscFeature with 96.3% null data\n    * *but NA in data only means no MiscFeature-probable replace-0*\n","d13fe096":"# DATA ANALYSIS"}}