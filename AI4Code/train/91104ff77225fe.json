{"cell_type":{"f83001f1":"code","93d33861":"code","1efb4d40":"code","33febddd":"code","6d621681":"code","a1ae11ed":"code","7228211a":"code","2d4df47f":"code","bfe2574f":"code","9f812447":"code","0748ba6c":"code","812f6eb3":"code","0a85edb5":"code","c71dd88d":"code","353f4a01":"code","a32e87a2":"code","134b0539":"code","d123d8b4":"code","f0417122":"code","93209975":"code","f138d4c2":"code","481f8281":"code","1f0188e5":"code","9ad5f46b":"code","42395197":"code","f421ef80":"code","b09b2eea":"code","2e909c56":"code","de088e72":"code","c5c46585":"code","312ece79":"code","73a0a393":"code","6cf02aca":"code","c132834f":"code","cf755667":"code","af615e14":"code","0a89f051":"code","0c6e88a2":"code","f391c0a4":"code","f41232d6":"code","314dd951":"code","0c3bea44":"code","c8ddcad2":"code","8490120f":"code","f07f7d19":"code","955a6c15":"code","c6b8381c":"code","9c52138e":"code","3c5a64c2":"code","6f2a28af":"code","ab7e5587":"code","64523f2a":"code","c3fca0df":"code","a50d1151":"code","9c099b10":"code","23185c97":"code","c1cac2c1":"code","7e935847":"code","734cb438":"code","8e703641":"code","20b8d7e6":"code","2a227446":"code","87b1c8dc":"code","31ec26d9":"code","48d8c0a4":"code","989d329d":"code","986fad40":"code","f247ddc6":"code","e0050e86":"code","48175407":"code","135660bc":"code","187e6002":"code","28caaff7":"code","3a650898":"code","6828bfea":"code","f28d9a9c":"code","a171b6bc":"code","de546929":"code","a2929fa3":"code","13ee4cba":"code","6b56739d":"code","a4e14656":"code","1300804c":"code","b43bfd83":"code","73242a58":"code","1fef6d90":"code","1d0701c5":"code","c739c262":"code","864af2b5":"code","3228af5d":"code","6b3254b5":"code","d09b46df":"code","0cead5df":"code","86f086c7":"code","0116af0d":"code","bfb50d85":"code","db4e9115":"code","d407272f":"code","04ac2eba":"code","51f22bf5":"code","1359e47c":"code","305ba6f9":"code","9d017800":"code","cde48551":"code","e6d66e84":"code","f5dfefbd":"code","23edf679":"code","53d0d396":"code","d0938bc0":"code","af5e03b3":"code","9c19aebc":"code","8da9618d":"code","f0e60e4b":"code","80225bff":"code","41cad753":"code","086e0138":"code","ec6fac3c":"code","ef8081e2":"code","6df02cd5":"code","6f152447":"code","53624069":"code","5dc502f6":"code","9ba6f074":"code","5e28da92":"code","d41f2209":"code","fe973e6f":"code","e0e1211c":"code","8912f167":"code","512f22c6":"code","a1b08052":"code","b30ff656":"code","81388e75":"code","1980afcd":"code","092f1b6d":"code","cb79d561":"code","de392842":"code","113541aa":"code","58c9fea0":"code","3e47af7b":"code","ec3cd2f0":"code","9d711204":"code","24897034":"code","c70a6c22":"code","3b806dee":"code","f5823170":"code","99722c65":"code","aa8a3a19":"code","6d990529":"code","4e7d5c32":"code","851244a8":"code","0bee1fa6":"code","01fcea6c":"code","87b95b59":"code","d050e9a2":"code","d569a28d":"code","f210f380":"markdown","0513e608":"markdown","ac9589a6":"markdown","6b72e0da":"markdown","c59a847b":"markdown","71029225":"markdown","496e7eb0":"markdown","6e81677a":"markdown","ca278637":"markdown","97e39073":"markdown","442797ab":"markdown","067fcc18":"markdown","5b82066d":"markdown","dade2825":"markdown","515b4dd0":"markdown","396a3f5e":"markdown","b8210ad2":"markdown","165325f9":"markdown","1be8e082":"markdown","844696b5":"markdown","7f855924":"markdown","4b69e7bd":"markdown","b7447e38":"markdown","34191d51":"markdown","ad8d5973":"markdown","1da3186b":"markdown","29aaa4f0":"markdown","a8a5e1b2":"markdown","e213b8e2":"markdown","1bae40dd":"markdown","8bbdead6":"markdown","8fd489e6":"markdown","2f3be75d":"markdown","e930bbda":"markdown","4f7f07c6":"markdown","f2adc530":"markdown","ec33819c":"markdown","a7c86c4a":"markdown","e5eb5165":"markdown","f916180c":"markdown","9429f99e":"markdown","762c2d80":"markdown","b764c77a":"markdown","b42045bf":"markdown","d378d0f0":"markdown","059f65c5":"markdown","65bfcca4":"markdown","08bfd18a":"markdown","1e9dfbff":"markdown","e21561d5":"markdown","d9680e04":"markdown","3e7f91e7":"markdown","bec762b0":"markdown","54e06e9c":"markdown","e3678a0f":"markdown","b3cae4d3":"markdown","5b0935ea":"markdown","37059755":"markdown","e16d8d25":"markdown","bcb2b1b7":"markdown","e815e60c":"markdown","26239e26":"markdown","1576f834":"markdown","a85141d2":"markdown","df5bb6c9":"markdown","4008a48b":"markdown","9923b42f":"markdown","ea6b4518":"markdown","1268bf6d":"markdown","183afdff":"markdown","f1572e0c":"markdown","232591c7":"markdown","81fe63c5":"markdown","e979e587":"markdown","ab62cd35":"markdown","9acd51cc":"markdown","2739a3a7":"markdown","38f151f0":"markdown","daeb81d1":"markdown","7a7f89f6":"markdown","ad5f1eff":"markdown","75987c8d":"markdown","01f9f04f":"markdown","0619bfc1":"markdown","3fa11c40":"markdown","40442285":"markdown","ba87fcac":"markdown","a9eb1ccb":"markdown","cf43e51f":"markdown","3e02255b":"markdown","bf8b21ee":"markdown","af997c72":"markdown","4b08fb8d":"markdown","eb904015":"markdown","c5b955ab":"markdown","b2954595":"markdown","8f1c4b54":"markdown","0036586b":"markdown","fb009737":"markdown","5310c376":"markdown","fc883f9a":"markdown","5e07dd63":"markdown","2396d916":"markdown","ab773bdb":"markdown","5b4d606f":"markdown","d79d73a7":"markdown","f4f14ca5":"markdown","9ce918a2":"markdown","0c2dccea":"markdown","1af0067d":"markdown","c1a5b284":"markdown","96e6729d":"markdown","ddd909c7":"markdown","ee13de73":"markdown","2b94bca1":"markdown","e1fb2eef":"markdown","5e0aed76":"markdown","b1db14e2":"markdown","463676cf":"markdown","4b79f4cf":"markdown","d1cda757":"markdown","4b0b672a":"markdown","0b16a064":"markdown","a0faa5ac":"markdown","d4145eb8":"markdown","33575785":"markdown","91dc6c49":"markdown","ba185afe":"markdown","4ddc17e9":"markdown","813277a8":"markdown","d5834e3b":"markdown","93ca7a40":"markdown","049e2e30":"markdown","019e0178":"markdown","f4c00c25":"markdown","151479f6":"markdown","41797c00":"markdown","49460447":"markdown","33d6e86c":"markdown","ebd5ad6c":"markdown","f431c7f6":"markdown","b87b747f":"markdown","0e3f5586":"markdown","2015628b":"markdown","67867303":"markdown","890f7a9d":"markdown","91930a07":"markdown","c60f1d1c":"markdown","7d228572":"markdown","8966f351":"markdown","3326264e":"markdown","5695337f":"markdown","9215eecd":"markdown","29703aab":"markdown","765cef0f":"markdown","8948ca9e":"markdown","9af9fbd7":"markdown","fd7a6fd0":"markdown","30778cec":"markdown","8ff2c27e":"markdown","8abe8bac":"markdown","228efbe4":"markdown","a0ebfe65":"markdown","73cdad10":"markdown","ef6467a5":"markdown","b227e6fa":"markdown","0ee83a1d":"markdown","8a2522a5":"markdown","bad8f52d":"markdown","9e7abe5b":"markdown","c4527c82":"markdown","39eaad2e":"markdown","0b44c08a":"markdown","9177a556":"markdown","53188700":"markdown","bffcb569":"markdown","a5721ea8":"markdown","ae151fa8":"markdown","7c9707de":"markdown","652f0307":"markdown","6c916cc3":"markdown","8ffeec3a":"markdown","c53ff055":"markdown","9d72ac49":"markdown","92ea2154":"markdown","a912dc29":"markdown"},"source":{"f83001f1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\n# used to supress display of warnings\nimport warnings\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB # using Gaussian algorithm from Naive Bayes\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n","93d33861":"# suppress display of warnings\nwarnings.filterwarnings('ignore')\n\n# display all dataframe columns\npd.options.display.max_columns = None\n\n# to set the limit to 3 decimals\npd.options.display.float_format = '{:.7f}'.format\n\n# display all dataframe rows\npd.options.display.max_rows = None","1efb4d40":"# Reading Bank_Personal_Loan_Modelling data\ntbankdf = pd.read_csv(\"..\/input\/personal-loan\/Bank_Personal_Loan_Modelling-1.xlsx\")","33febddd":"# Get the top 5 rows\ntbankdf.head()","6d621681":"# Get the shape of Bank_Personal_Loan_Modelling data\ntbankdf.shape","a1ae11ed":"print(\"Number of rows = {0} and Number of Columns = {1} in Data frame\".format(tbankdf.shape[0],tbankdf.shape[1]))","7228211a":"# Check datatypes\ntbankdf.dtypes","2d4df47f":"# Check Data frame info\ntbankdf.info()","bfe2574f":"# Column names of Data frame\ntbankdf.columns","9f812447":"# Check duplicates in a data frame\ntbankdf.duplicated().sum()","0748ba6c":"# Create a boxplot for all the continuous features\ntbankdf.boxplot(column = ['Age', 'Experience', 'Income', 'CCAvg', 'Mortgage'], figsize = (20,10))","812f6eb3":"# Check Outliers in CCAvg column - Making sure\nsns.boxplot(x = tbankdf['CCAvg'])","0a85edb5":"# Check Outliers in ZIP Code column\nsns.boxplot(x = tbankdf['ZIP Code'])","c71dd88d":"tbankdf_outliers = pd.DataFrame(tbankdf[['Income','CCAvg','Mortgage','ZIP Code']])\n\n# Calculate IQR\nQ1 = tbankdf_outliers.quantile(0.25)\nQ3 = tbankdf_outliers.quantile(0.75)\n\nIQR = Q3 - Q1\nprint(IQR)","353f4a01":"# We can use IQR score to filter out the outliers by keeping only valid values\n\n# Replace every outlier on the upper side by the upper whisker - for Income, CCAvg and Mortgage columns\nfor i, j in zip(np.where(tbankdf_outliers > Q3 + 1.5 * IQR)[0], np.where(tbankdf_outliers > Q3 + 1.5 * IQR)[1]):\n    \n    whisker  = Q3 + 1.5 * IQR\n    tbankdf_outliers.iloc[i,j] = whisker[j]\n    \n# Replace every outlier on the lower side by the lower whisker - for ZIP Code column\nfor i, j in zip(np.where(tbankdf_outliers < Q1 - 1.5 * IQR)[0], np.where(tbankdf_outliers < Q1 - 1.5 * IQR)[1]): \n    \n    whisker  = Q1 - 1.5 * IQR\n    tbankdf_outliers.iloc[i,j] = whisker[j]","a32e87a2":"# Remove 'Income', 'CCAvg' and 'Mortgage' from tbankdf\ntbankdf.drop(columns = tbankdf[['Income','CCAvg','Mortgage','ZIP Code']], inplace=True)","134b0539":"# Add 'Income', 'CCAvg' and 'Mortgage' from tbankdf_outliers to tbankdf\ntbankdf = pd.concat([tbankdf, tbankdf_outliers], axis = 1)","d123d8b4":"# Create a boxplot for all the continuous features after removing Outliers\ntbankdf.boxplot(column = ['Age', 'Experience', 'Income', 'CCAvg', 'Mortgage'], figsize = (20,10))","f0417122":"# Check Outliers in ZIP Code column\nsns.boxplot(x = tbankdf['ZIP Code'])","93209975":"# Summary Statistics\ntbankdf.describe().T","f138d4c2":"# Check customers who are Advanced\/Professional - Education level = 3 and their experience is less than 1 year\ntbankdf[(tbankdf['Experience'] < 1) & (tbankdf['Education'] == 3)].shape","481f8281":"# Check the customers whose experience is less than 1 year\ntbankdf[(tbankdf['Experience'] < 0)].shape","1f0188e5":"# Check the presence of missing values\ntbankdf.isnull().sum()","9ad5f46b":"# Remove ID column from Data frame which does not influence Personal Loan\ntbankdf.drop(['ID'], axis = 1, inplace = True)","42395197":"# Get top 5 rows after removing ID\ntbankdf.head()","f421ef80":"sns.distplot(tbankdf['Age']);\nplt.title(\"Age distribution\")\nplt.show()","b09b2eea":"sns.distplot(tbankdf['Experience']);\nplt.title(\"Experience distribution\")\nplt.show()","2e909c56":"sns.distplot(tbankdf['ZIP Code']);\nplt.title(\"ZIP Code distribution\")\nplt.show()","de088e72":"sns.distplot(tbankdf['Income']);\nplt.title(\"Income distribution\")\nplt.show()","c5c46585":"sns.distplot(tbankdf['CCAvg']);\nplt.title(\"CCAvg distribution\")\nplt.show()","312ece79":"sns.distplot(tbankdf['Mortgage']);\nplt.title(\"Mortgage distribution\")\nplt.show()","73a0a393":"sns.countplot(tbankdf['Family']);\nplt.title(\"Family distribution\")\nplt.show()","6cf02aca":"sns.countplot(tbankdf['Education'])\nplt.title(\"Education distribution\")\nplt.show()","c132834f":"sns.countplot(tbankdf['Securities Account']);\nplt.title(\"Securities Account\")\nplt.show()","cf755667":"sns.countplot(tbankdf['CD Account']);\nplt.title(\"CD Account distribution\")\nplt.show()","af615e14":"sns.countplot(tbankdf['Online']);\nplt.title(\"Online distribution\")\nplt.show()","0a89f051":"sns.countplot(tbankdf['CreditCard']);\nplt.title(\"CreditCard distribution\")\nplt.show()","0c6e88a2":"# Check the Personal Loan of Customers across different education level: 1 - Undergrad, 2 - Graduate, 3 - Advanced\/Professional\nedu_pl_table = pd.crosstab(index = tbankdf['Education'], columns = tbankdf['Personal Loan'])\n\nedu_pl_table.plot(kind = 'bar', figsize=(8,8), stacked = True)\nplt.title(\"Proportion of Education level in Personal Loan\")\nplt.show()","f391c0a4":"undergrad_pl = tbankdf[tbankdf['Education'] == 1]['Personal Loan'].value_counts()[1]\ngrad_pl = tbankdf[tbankdf['Education'] == 2]['Personal Loan'].value_counts()[1]\nprof_pl = tbankdf[tbankdf['Education'] == 3]['Personal Loan'].value_counts()[1]\n\nyes_pl = tbankdf['Personal Loan'].value_counts()[1]\nno_pl = tbankdf['Personal Loan'].value_counts()[0]\n\nprint([undergrad_pl, grad_pl, prof_pl], [yes_pl])\nprint(f'Proportions of undergrad, graduate, advanced customers for Personal loan = ,{round(93\/480,2)}%, {round(182\/480,2)}%, {round(205\/480,2)}% respectively')","f41232d6":"from statsmodels.stats.proportion import proportions_ztest\n\n# Z-test proportions: More than 2 samples not implemented yet, hence I am passing two elements\nt_statistic, p_value = proportions_ztest([undergrad_pl, prof_pl], [yes_pl])\n\nprint(\"undergrad and prof t_statistic\", t_statistic)\nprint(\"undergrad and prof p_value\", p_value)\n\nt_statistic, p_value = proportions_ztest([grad_pl, prof_pl], [yes_pl])\n\nprint(\"grad and prof t_statistic\", t_statistic)\nprint(\"grad and prof p_value\", p_value)","314dd951":"reject_null = False\nif p_value < 0.05:\n    reject_null = True \nelse: \n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","0c3bea44":"# Check the distribution of Personal loan is same accross Professional customers with family members 1,2,3 and 4\nprof_family_pl = pd.crosstab(index = [tbankdf['Personal Loan'],tbankdf['Family']], columns = [tbankdf['Education'] == 3])\n\nprof_family_pl","c8ddcad2":"import scipy.stats as stats\n\nchi_sq_Stat, p_value, deg_freedom, exp_freq = stats.chi2_contingency(prof_family_pl)\n\nprint('Chi-square statistic %3.5f P value %1.6f Degrees of freedom %d' %(chi_sq_Stat, p_value,deg_freedom))","8490120f":"reject_null = False\nif p_value < 0.05:\n    reject_null = True\nelse:\n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","f07f7d19":"# Check the distribution of Personal loan across different ages of customers\nsns.boxplot(x = tbankdf['Personal Loan'], y = tbankdf['Age']);\nplt.show()","955a6c15":"from scipy.stats import ttest_ind\n\nage_pl = tbankdf[tbankdf['Personal Loan'] == 1].Age\nage_nopl = tbankdf[tbankdf['Personal Loan'] == 0].Age\n\nt_statistic, p_value = ttest_ind(age_pl, age_nopl)\n\nprint(\"t_statistic=\",t_statistic)\nprint(\"p_value=\",p_value)","c6b8381c":"reject_null = False\nif p_value < 0.05:\n    reject_null = True\nelse:\n    reject_null = False\n    \nprint(\"reject null? :\", str(reject_null))","9c52138e":"# Check the distribution of CCAvg across family members 1,2,3 and 4\nsns.boxplot(x = tbankdf['Family'], y = tbankdf['CCAvg']);","3c5a64c2":"ccavg_family_df = tbankdf[['Family','CCAvg']]","6f2a28af":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nlm = ols('CCAvg ~ Family',data=ccavg_family_df).fit()\naov_table = sm.stats.anova_lm(lm, typ=2)\n\np_value = aov_table['PR(>F)'][0]\naov_table","ab7e5587":"reject_null = False\nif p_value < 0.05:\n    reject_null = True\nelse:\n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","64523f2a":"# Check the distribution of CCAvg across Education level\nsns.boxplot(x = tbankdf['Education'], y = tbankdf['CCAvg']);","c3fca0df":"ccavg_education_df = tbankdf[['Education','CCAvg']]","a50d1151":"lm = ols('CCAvg ~ Education',data=ccavg_education_df).fit()\naov_table = sm.stats.anova_lm(lm, typ=2)\n\np_value = aov_table['PR(>F)'][0]\naov_table","9c099b10":"reject_null = False\nif p_value < 0.05:\n    reject_null = True\nelse:\n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","23185c97":"# Check the distribution of Income across Education level\nsns.boxplot(x = tbankdf['Education'], y = tbankdf['Income']);","c1cac2c1":"income_education_df = tbankdf[['Income','Education']]","7e935847":"lm = ols('Income ~ Education',data=income_education_df).fit()\naov_table = sm.stats.anova_lm(lm, typ=2)\n\np_value = aov_table['PR(>F)'][0]\naov_table","734cb438":"reject_null = False\nif p_value < 0.05:\n    reject_null = True\nelse:\n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","8e703641":"# Check the distribution of Mortgage across Education level\nsns.boxplot(x = tbankdf['Education'], y = tbankdf['Mortgage']);","20b8d7e6":"mortgage_education_df = tbankdf[['Mortgage','Education']]","2a227446":"lm = ols('Income ~ Education',data=income_education_df).fit()\naov_table = sm.stats.anova_lm(lm, typ=2)\n\np_value = aov_table['PR(>F)'][0]\naov_table","87b1c8dc":"reject_null = False\nif p_value < 0.05:\n    reject_null = True\nelse:\n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","31ec26d9":"# Check the distribution of Mortgage across Family Members\nsns.boxplot(x = tbankdf['Family'], y = tbankdf['Mortgage']);","48d8c0a4":"mortgage_family_df = tbankdf[['Mortgage','Family']]","989d329d":"lm = ols('Mortgage ~ Family',data=mortgage_family_df).fit()\naov_table = sm.stats.anova_lm(lm, typ=2)\n\np_value = aov_table['PR(>F)'][0]\naov_table","986fad40":"reject_null = False\nif p_value < 0.05:\n    reject_null = True\nelse:\n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","f247ddc6":"# Check the distribution of Credit Card across different ages of customers\nsns.boxplot(x = tbankdf['CreditCard'], y = tbankdf['Age']);","e0050e86":"# Check the distribution of Online across different ages of customers\nsns.boxplot(x = tbankdf['Online'], y = tbankdf['Age']);","48175407":"# Check the distribution of CD Account across different ages of customers\nsns.boxplot(x = tbankdf['CD Account'], y = tbankdf['Age']);","135660bc":"# Check the distribution of Securities Account across different ages of customers\nsns.boxplot(x = tbankdf['Securities Account'], y = tbankdf['Age']);","187e6002":"# Check the distribution of Personal Loan customers with Avg.spending on credit cards per month\nsns.scatterplot(x = tbankdf['CCAvg'], y = tbankdf['Income'], hue = tbankdf['Personal Loan']);","28caaff7":"pl_ccavg = tbankdf[tbankdf['Personal Loan'] == 1].CCAvg\nnopl_ccavg = tbankdf[tbankdf['Personal Loan'] == 0].CCAvg\n\nt_statistic, p_value = ttest_ind(pl_ccavg, nopl_ccavg)\n\nprint(\"t_statistic =\",t_statistic)\nprint(\"p_value =\",p_value)","3a650898":"reject_null = False\nif p_value < 0.05:\n    reject_null = True\nelse:\n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","6828bfea":"# Summary Statistics\ntbankdf.describe().T","f28d9a9c":"sns.pairplot(tbankdf, diag_kind = 'kde', hue = 'Personal Loan', corner = True);","a171b6bc":"# Check the Correlation\ntbankdf.corr()","de546929":"sns.pairplot(tbankdf[['Age', 'Experience', 'Income', 'CCAvg', 'Mortgage','ZIP Code']], kind = 'reg', corner = True);","a2929fa3":"plt.figure(figsize = (15,8))\nsns.heatmap(tbankdf.corr(), annot = True, linewidths=.2, center = 1);\nplt.show()","13ee4cba":"sns.countplot(tbankdf['Personal Loan']);\nplt.title(\"Personal Loan Distribution\")\nplt.show()","6b56739d":"n_true = len(tbankdf.loc[tbankdf['Personal Loan'] == 1])\nn_false = len(tbankdf.loc[tbankdf['Personal Loan'] == 0])\nprint(\"Number of true cases: {0} ({1:2.2f}%)\".format(n_true, (n_true \/ (n_true + n_false)) * 100 ))\nprint(\"Number of false cases: {0} ({1:2.2f}%)\".format(n_false, (n_false \/ (n_true + n_false)) * 100))","a4e14656":"from sklearn.utils import resample","1300804c":"# Get the majority and minority classes\nds_majority = tbankdf[tbankdf['Personal Loan']==0]\nds_minority = tbankdf[tbankdf['Personal Loan']==1]","b43bfd83":"tbankdf['Personal Loan'].value_counts()","73242a58":"# Upsample minority class\nds_minority_upsampled = resample(ds_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=tbankdf['Personal Loan'].value_counts()[0],    # to match majority class\n                                 random_state=1) # reproducible results","1fef6d90":"# Combine majority class with upsampled minority class\nds_upsampled = pd.concat([ds_majority, ds_minority_upsampled])","1d0701c5":"# Display new class counts\nds_upsampled['Personal Loan'].value_counts()","c739c262":"# Check the dimensions\nds_upsampled.shape","864af2b5":"# Copy up sampled data to Original data frame\ntbankdf = ds_upsampled","3228af5d":"# Check the dimensions\ntbankdf.shape","6b3254b5":"# import zscore for scaling the data\nfrom scipy.stats import zscore","d09b46df":"# Apply zscore on independent features - Income, CCAvg, Mortgage\ntbankdf[['Income','CCAvg','Mortgage','ZIP Code']] = tbankdf[['Income','CCAvg','Mortgage','ZIP Code']].apply(zscore)","0cead5df":"# Check the Nomalized values\ntbankdf.head()","86f086c7":"# Considering all Predictors except Experience feature\nx = tbankdf[['Age', 'Family', 'Education',\n       'Securities Account', 'CD Account', 'Online', 'CreditCard', 'Income',\n       'CCAvg', 'Mortgage','ZIP Code']]\n\ny = tbankdf['Personal Loan']","0116af0d":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 1)","bfb50d85":"print('x_train shape : ({0},{1})'.format(x_train.shape[0], x_train.shape[1]))\nprint('y_train shape : ({0},)'.format(y_train.shape[0]))\nprint('x_test shape : ({0},{1})'.format(x_test.shape[0], x_test.shape[1]))\nprint('y_test shape : ({0},)'.format(y_test.shape[0]))","db4e9115":"# Fit the model on training set\nlr_model = LogisticRegression(random_state=0, solver = 'liblinear')\nlr_model.fit(x_train, y_train)","d407272f":"coef_df = pd.DataFrame(lr_model.coef_)\ncoef_df['intercept'] = lr_model.intercept_\nprint(coef_df)","04ac2eba":"# Predict on Test set\ny_predict = lr_model.predict(x_test)","51f22bf5":"# Accuracy of Training data set\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(lr_model.score(x_train, y_train) * 100))","1359e47c":"# Accuracy of Test data set\nprint(\"Accuracy of Test data set: {0:.4f} %\".format(lr_model.score(x_test, y_test) * 100))","305ba6f9":"# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors = 3)\n\n# fitting the model\nknn.fit(x_train, y_train)\n\n# predict the response\ny_pred = knn.predict(x_test)\n\n# evaluate accuracy\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(knn.score(x_train, y_train) * 100))\nprint(\"Accuracy of Test data set: {0:.4f} %\".format(knn.score(x_test, y_test) * 100))","9d017800":"# instantiate learning model (k = 5)\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# fitting the model\nknn.fit(x_train, y_train)\n\n# predict the response\ny_pred = knn.predict(x_test)\n\n# evaluate accuracy\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(knn.score(x_train, y_train) * 100))\nprint(\"Accuracy of Test data set: {0:.4f} %\".format(knn.score(x_test, y_test) * 100))","cde48551":"# instantiate learning model (k = 9)\nknn = KNeighborsClassifier(n_neighbors=9)\n\n# fitting the model\nknn.fit(x_train, y_train)\n\n# predict the response\ny_pred = knn.predict(x_test)\n\n# evaluate accuracy\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(knn.score(x_train, y_train) * 100))\nprint(\"Accuracy of Test data set: {0:.4f} %\".format(knn.score(x_test, y_test) * 100))","e6d66e84":"# creating odd list of K for KNN\nmyList = list(range(1,26))\n\n# subsetting just the odd ones\nneighbors = list(filter(lambda x: x % 2 != 0, myList))","f5dfefbd":"# empty list that will hold accuracy scores\nac_scores = []\n\n# perform accuracy metrics for values from 1,3,5....25\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train, y_train)\n    # predict the response\n    y_pred = knn.predict(x_test)\n    # evaluate accuracy\n    scores = accuracy_score(y_test, y_pred)\n    ac_scores.append(scores)\n\n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors = %d\" % optimal_k)","23edf679":"# plot misclassification error vs k\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","53d0d396":"# Use optimum value of K = 1 and check the accuracy\nknn = KNeighborsClassifier(n_neighbors = 1)\n\n# fitting the model\nknn.fit(x_train, y_train)\n\n# predict the response\ny_pred = knn.predict(x_test)\n\n# evaluate accuracy\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(knn.score(x_train, y_train) * 100))\nprint(\"Accuracy of Test data set: {0:.4f} %\".format(knn.score(x_test, y_test) * 100))\n\nprint(\"Recall score: \",recall_score(y_test, y_pred))","d0938bc0":"# Use K = 3 as the final model for prediction\nknn = KNeighborsClassifier(n_neighbors = 3)\n\n# fitting the model\nknn.fit(x_train, y_train)\n\n# predict the response\ny_pred = knn.predict(x_test)\n\n# evaluate accuracy\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(knn.score(x_train, y_train) * 100))\nprint(\"Accuracy of Test data set: {0:.4f} %\".format(knn.score(x_test, y_test) * 100))","af5e03b3":"# create the model\npl_model = GaussianNB()\n\npl_model.fit(x_train, y_train.ravel())","9c19aebc":"pl_predict = pl_model.predict(x_test)","8da9618d":"# Accuracy of Training data set\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(pl_model.score(x_train, y_train) * 100))\n\n# Accuracy of Test data set\nprint(\"Accuracy of Test data set: {0:.4f} %\".format(pl_model.score(x_test, y_test) * 100))","f0e60e4b":"lr_cm = metrics.confusion_matrix(y_test, y_predict, labels=[1, 0])\nprint(lr_cm)\n\nsns.heatmap(lr_cm, annot=True,  fmt='.2f', xticklabels = [\"PL\", \"No PL\"] , yticklabels = [\"PL\", \"No PL\"] )\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","80225bff":"# Logistics Regression - Classification report\ncr = classification_report(y_test, y_predict)\nprint(cr)","41cad753":"knn_cm = metrics.confusion_matrix(y_test, y_pred, labels=[1, 0])\nprint(knn_cm)\n\nsns.heatmap(knn_cm, annot=True,  fmt='.2f', xticklabels = [\"PL\", \"No PL\"] , yticklabels = [\"PL\", \"No PL\"] )\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","086e0138":"# KNN - Classification report\ncr = classification_report(y_test, y_pred)\nprint(cr)","ec6fac3c":"nb_cm = metrics.confusion_matrix(y_test, pl_predict, labels=[1, 0])\nprint(nb_cm)\n\nsns.heatmap(nb_cm, annot=True,  fmt='.2f', xticklabels = [\"PL\", \"No PL\"] , yticklabels = [\"PL\", \"No PL\"] )\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","ef8081e2":"# Naive Bayes - Classification report\ncr = classification_report(y_test, pl_predict)\nprint(cr)","6df02cd5":"logreg = LogisticRegression(solver = \"liblinear\")\n\nskf = StratifiedKFold(n_splits=10, random_state=None)\nskf.get_n_splits(x,y) # x - Predictors, y - Target variable\n\naccuracy = []\n\nfor train_index, test_index in skf.split(x,y):\n    #print(\"Train: \",train_index, \"Validation: \", test_index)\n    \n    x1_train, x1_test = x.iloc[train_index], x.iloc[test_index]\n    y1_train, y1_test = y.iloc[train_index], y.iloc[test_index]\n    \n    logreg.fit(x1_train, y1_train)\n    prediction = logreg.predict(x1_test)\n    score = accuracy_score(prediction, y1_test)\n    accuracy.append(score)\n    \nprint(\"10 different experiment scores: \",accuracy)","6f152447":"print(\"Mean score of 10 different experiments: {0:.4f} %\".format(np.array(accuracy).mean() * 100))","53624069":"# As mentioned in abvoe steps, using K = 3 as the final model for prediction\nknnclassifier = KNeighborsClassifier(n_neighbors = 3)\n\nskf = StratifiedKFold(n_splits=10, random_state=None)\nskf.get_n_splits(x,y) # x - Predictors, y - Target variable\n\naccuracy = []\n\nfor train_index, test_index in skf.split(x,y):\n    #print(\"Train: \",train_index, \"Validation: \", test_index)\n    \n    x1_train, x1_test = x.iloc[train_index], x.iloc[test_index]\n    y1_train, y1_test = y.iloc[train_index], y.iloc[test_index]\n    \n    knnclassifier.fit(x1_train, y1_train)\n    prediction = knnclassifier.predict(x1_test)\n    score = accuracy_score(prediction, y1_test)\n    accuracy.append(score)\n    \nprint(\"10 different experiment scores: \",accuracy)","5dc502f6":"print(\"Mean score of 10 different experiments: {0:.4f} %\".format(np.array(accuracy).mean() * 100))","9ba6f074":"gaussian_nb = GaussianNB()\n\nskf = StratifiedKFold(n_splits=10, random_state=None)\nskf.get_n_splits(x,y) # x - Predictors, y - Target variable\n\naccuracy = []\n\nfor train_index, test_index in skf.split(x,y):\n    #print(\"Train: \",train_index, \"Validation: \", test_index)\n    \n    x1_train, x1_test = x.iloc[train_index], x.iloc[test_index]\n    y1_train, y1_test = y.iloc[train_index], y.iloc[test_index]\n    \n    gaussian_nb.fit(x1_train, y1_train)\n    prediction = gaussian_nb.predict(x1_test)\n    score = accuracy_score(prediction, y1_test)\n    accuracy.append(score)\n    \nprint(\"10 different experiment scores: \",accuracy)","5e28da92":"print(\"Mean score of 10 different experiments: {0:.4f} %\".format(np.array(accuracy).mean() * 100))","d41f2209":"y_pred_logistic = lr_model.decision_function(x_test)\nlogistic_fpr, logistic_tpr, threshold = roc_curve(y_test, y_pred_logistic)\nauc_logistic = auc(logistic_fpr, logistic_tpr)\n\ny_knn_score = knn.predict_proba(x_test)\nknn_fpr, knn_tpr, threshold = roc_curve(y_test, y_knn_score[:, 1])\nauc_knn = auc(knn_fpr, knn_tpr)\n\ny_gnb_score = pl_model.predict_proba(x_test)\ngnb_fpr, gnb_tpr, threshold = roc_curve(y_test, y_gnb_score[:, 1])\nauc_gnb = auc(gnb_fpr, gnb_tpr)\n\nplt.figure(figsize=(5, 5), dpi=100)\nplt.plot([0, 1], [0, 1],'r--')\nplt.plot(knn_fpr, knn_tpr, linestyle='-', marker='.', label='KNN (auc = %0.3f)' % auc_knn)\nplt.plot(logistic_fpr, logistic_tpr, marker='.', label='Logistic (auc = %0.3f)' % auc_logistic)\nplt.plot(gnb_fpr, gnb_tpr, linestyle='-', marker='.', label='Naive Bayes (auc = %0.3f)' % auc_gnb)\n\nplt.title('Receiver Operating Characteristic')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.legend()\n\nplt.show()","fe973e6f":"# ANOVA feature selection for numeric input and categorical output - For ANOVA we use \"f_classif\" statistical measure\nfrom sklearn.datasets import make_classification\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Considering only numerical features\nx = tbankdf[['Age', 'Income', 'CCAvg', 'Mortgage', 'Experience','ZIP Code']]\n\ny = tbankdf['Personal Loan']\n\n# define feature selection\nfs = SelectKBest(score_func=f_classif, k=4)\n\n# apply feature selection\nx_selected = fs.fit_transform(x, y)\nprint(x_selected.shape)\n\nprint(fs.get_support())","e0e1211c":"# Chi-Squared test for  feature selection for categorical input and categorical output\nfrom sklearn.feature_selection import chi2\n\nx = tbankdf[['Family', 'Education', 'Securities Account', 'CD Account', 'Online', 'CreditCard']]\n\nbestfeatures = SelectKBest(score_func=chi2, k=4)\nfit = bestfeatures.fit(x,y)\n\nprint(bestfeatures.get_support())","8912f167":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)","512f22c6":"#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns","a1b08052":"featureScores","b30ff656":"x = tbankdf[['Age', 'Family', 'Education',\n       'Securities Account', 'CD Account', 'Online', 'CreditCard', 'Income',\n       'CCAvg', 'Mortgage', 'Experience','ZIP Code']]\n\ny = tbankdf['Personal Loan']","81388e75":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)","1980afcd":"print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers","092f1b6d":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","cb79d561":"from sklearn.feature_selection import RFECV\n\ncv_estimator = LogisticRegression(solver = 'liblinear')\n\ncv_estimator.fit(x_train, y_train)\ncv_selector = RFECV(cv_estimator,cv= 10, step=1,scoring='accuracy')\ncv_selector = cv_selector.fit(x_train, y_train)\nrfecv_mask = cv_selector.get_support() #list of booleans\nrfecv_features = [] \nfor bool, feature in zip(rfecv_mask, x_train.columns):\n    if bool:\n        rfecv_features.append(feature)\n        \nprint('Optimal number of features :', cv_selector.n_features_)\nprint('Best features :', rfecv_features)","de392842":"# Considering features from Filter methods because it gives better  accuracy than Wrapper methods\nx = tbankdf[['Family', 'Education', 'Securities Account', 'CD Account', 'Online', 'CreditCard', 'Income', 'CCAvg','ZIP Code']]\n\ny = tbankdf['Personal Loan']","113541aa":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 1)","58c9fea0":"print('x_train shape : ({0},{1})'.format(x_train.shape[0], x_train.shape[1]))\nprint('y_train shape : ({0},)'.format(y_train.shape[0]))\nprint('x_test shape : ({0},{1})'.format(x_test.shape[0], x_test.shape[1]))\nprint('y_test shape : ({0},)'.format(y_test.shape[0]))","3e47af7b":"# Fit the model on training set\nlr_model = LogisticRegression(random_state=0)\nlr_model.fit(x_train, y_train)","ec3cd2f0":"coef_df = pd.DataFrame(lr_model.coef_)\ncoef_df['intercept'] = lr_model.intercept_\nprint(coef_df)","9d711204":"# Predict on Test set\ny_predict = lr_model.predict(x_test)","24897034":"# Accuracy of Training data set\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(lr_model.score(x_train, y_train) * 100))","c70a6c22":"# Accuracy of Test data set\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(lr_model.score(x_test, y_test) * 100))","3b806dee":"skf = StratifiedKFold(n_splits=10, random_state=None)\nskf.get_n_splits(x,y) # x - Predictors, y - Target variable\n\naccuracy = []\n\nfor train_index, test_index in skf.split(x,y):\n    #print(\"Train: \",train_index, \"Validation: \", test_index)\n    \n    x1_train, x1_test = x.iloc[train_index], x.iloc[test_index]\n    y1_train, y1_test = y.iloc[train_index], y.iloc[test_index]\n    \n    lr_model.fit(x1_train, y1_train)\n    prediction = lr_model.predict(x1_test)\n    score = accuracy_score(prediction, y1_test)\n    accuracy.append(score)\n    \nprint(\"10 different experiment scores: \",accuracy)","f5823170":"np.array(accuracy).mean()\n\nprint(\"Mean score of 10 different experiments: {0:.4f} %\".format(np.array(accuracy).mean() * 100))","99722c65":"# Use K = 3 as the final model for prediction\nknn = KNeighborsClassifier(n_neighbors = 3)\n\n# fitting the model\nknn.fit(x_train, y_train)\n\n# predict the response\ny_pred = knn.predict(x_test)\n\n# evaluate accuracy\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(knn.score(x_train, y_train) * 100))\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(knn.score(x_test, y_test) * 100))","aa8a3a19":"skf = StratifiedKFold(n_splits=10, random_state=None)\nskf.get_n_splits(x,y) # x - Predictors, y - Target variable\n\naccuracy = []\n\nfor train_index, test_index in skf.split(x,y):\n    #print(\"Train: \",train_index, \"Validation: \", test_index)\n    \n    x1_train, x1_test = x.iloc[train_index], x.iloc[test_index]\n    y1_train, y1_test = y.iloc[train_index], y.iloc[test_index]\n    \n    knn.fit(x1_train, y1_train)\n    prediction = knn.predict(x1_test)\n    score = accuracy_score(prediction, y1_test)\n    accuracy.append(score)\n    \nprint(\"10 different experiment scores: \",accuracy)","6d990529":"np.array(accuracy).mean()\n\nprint(\"Mean score of 10 different experiments: {0:.4f} %\".format(np.array(accuracy).mean() * 100))","4e7d5c32":"# create the model\nnb_model = GaussianNB()\n\nnb_model.fit(x_train, y_train.ravel())\n\nnb_predict = nb_model.predict(x_test)\n\n# Accuracy of Training data set\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(nb_model.score(x_train, y_train) * 100))\n\n# Accuracy of Test data set\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(nb_model.score(x_test, y_test) * 100))","851244a8":"skf = StratifiedKFold(n_splits=10, random_state=None)\nskf.get_n_splits(x,y) # x - Predictors, y - Target variable\n\naccuracy = []\n\nfor train_index, test_index in skf.split(x,y):\n    #print(\"Train: \",train_index, \"Validation: \", test_index)\n    \n    x1_train, x1_test = x.iloc[train_index], x.iloc[test_index]\n    y1_train, y1_test = y.iloc[train_index], y.iloc[test_index]\n    \n    nb_model.fit(x1_train, y1_train)\n    prediction = nb_model.predict(x1_test)\n    score = accuracy_score(prediction, y1_test)\n    accuracy.append(score)\n    \nprint(\"10 different experiment scores: \",accuracy)","0bee1fa6":"np.array(accuracy).mean()\n\nprint(\"Mean score of 10 different experiments: {0:.4f} %\".format(np.array(accuracy).mean() * 100))","01fcea6c":"y_pred_logistic = lr_model.decision_function(x_test)\nlogistic_fpr, logistic_tpr, threshold = roc_curve(y_test, y_pred_logistic)\nauc_logistic = auc(logistic_fpr, logistic_tpr)\n\ny_knn_score = knn.predict_proba(x_test)\nknn_fpr, knn_tpr, threshold = roc_curve(y_test, y_knn_score[:, 1])\nauc_knn = auc(knn_fpr, knn_tpr)\n\ny_gnb_score = nb_model.predict_proba(x_test)\ngnb_fpr, gnb_tpr, threshold = roc_curve(y_test, y_gnb_score[:, 1])\nauc_gnb = auc(gnb_fpr, gnb_tpr)\n\nplt.figure(figsize=(5, 5), dpi=100)\nplt.plot([0, 1], [0, 1],'r--')\nplt.plot(knn_fpr, knn_tpr, linestyle='-', marker='.', label='KNN (auc = %0.3f)' % auc_knn)\nplt.plot(logistic_fpr, logistic_tpr, marker='.', label='Logistic (auc = %0.3f)' % auc_logistic)\nplt.plot(gnb_fpr, gnb_tpr, linestyle='-', marker='.', label='Naive Bayes (auc = %0.3f)' % auc_gnb)\n\nplt.title('Receiver Operating Characteristic')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.legend()\n\nplt.show()","87b95b59":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# define models and parameters\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\npenalty = ['none', 'l1', 'l2', 'elasticnet']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0,verbose=1)\ngrid_result = grid_search.fit(x_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","d050e9a2":"# define models and parameters\nmodel = KNeighborsClassifier()\nn_neighbors = range(1, 21, 2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n\n# define grid search\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0,verbose=1)\ngrid_result = grid_search.fit(x_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","d569a28d":"# define models and parameters\nnb_classifier = GaussianNB()\nparams_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# define grid search\ngs_NB = GridSearchCV(estimator=nb_classifier, param_grid=params_NB, cv=cv, verbose=1, scoring='accuracy')\ngrid_result = gs_NB.fit(x_train, y_train);\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","f210f380":"<p style = \"font-size:20px; color: #007580 \"><strong> Modelling - KNN<\/strong><\/p> ","0513e608":"* **Age column seems to be uniformly distributed**","ac9589a6":"1. No duplicates in dataset\n2. We had outliers in these 'Income','CCAvg' and'Mortgage columns, handled these outliers by replacing every outlier with upper side of the whisker\n3. We had outliers in 'ZIP Code' column also, handled these outliers by replacing every outlier with lower side of the whisker\n4. We had outliers in 'Experience' column, I mean negative values in experience and cleaned that data.\n5. No missing values in dataset\n6. Removed irrelevant columns 'ID' which does not influence target variable","6b72e0da":"#### Cross Validation - Stratified K Fold CV","c59a847b":"#### Numerical columns - Age, Experience, ZIP  Code, Income, CCAvg and Mortgage","71029225":"#### From the above output, we see that except for the column CCAvg all our columns datatype is int64.\n\n#### Ordinal Categorical columns - Family, Education\n\n#### Binay Categorical columns - Personal Loan, Securities Account, CD Account, Online, CreditCard\n\n#### Numerical columns - Income, CCAvg, Mortgage, Age, Experience, ID, ZIP Code","496e7eb0":"### j. Is the distribution of Online is same across different ages of customers?","6e81677a":"* **ZIP Code column - Moderately left skewed distribution**","ca278637":"<a id = '7.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 7. Model Building and Validation <\/h2> ","97e39073":"<p style = \"font-size:20px; color: #007580 \"><strong> Shape of the data <\/strong><\/p> ","442797ab":"<a id = '7.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.1 Sampling Techniques - Create Training and Test Set <\/strong><\/p> ","067fcc18":"<a id = '4.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.4 Check Missing Values <\/strong><\/p> ","5b82066d":"#### Hence we fail to reject Null Hypothesis (we have enough (95% and 99%) evidence to prove that \"Distribution of Personal loan is same across different ages of customers\")","dade2825":"* **Target variable:** Personal Loan\n* **Predictors (Input varibles):** Age, Experience, Income, Family, CCAvg, Education, Mortgage, Securities Account, CD Account, Online, CreditCard, ZIP Code","515b4dd0":"<p style = \"font-size:20px; color: #007580 \"><strong> Get the target column distribution <\/strong><\/p> ","396a3f5e":"#### Plot misclassification error vs k (with k value on X-axis) using matplotlib.","b8210ad2":"#### Categorical columns - Family, Education, Personal Loan, Securities Account, CD Account, Online, CreditCard","165325f9":"<a id = '9.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 9.2 Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans<\/strong><\/p> ","1be8e082":"#### 5. Decide to Reject or Accept  Null Hypothesis","844696b5":"#### Hence we reject Null Hypothesis (we have enough (95% and 99%) evidence to prove that \"Distribution of Mortgage is same across Family Membersl\")","7f855924":"<p style = \"font-size:20px; color: #007580 \"><strong> Using Logistic Regression model<\/strong><\/p> ","4b69e7bd":"###############################################################################################################################","b7447e38":"<a id = '4.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 4. Data Cleaning <\/h2> ","34191d51":"### a. Fit the model on Training Set","ad8d5973":"### a. Fit the model on Training Set","1da3186b":"<a id = '5.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.3 Study Summary Statistics <\/strong><\/p>","29aaa4f0":"#### Hence we fail to reject Null Hypothesis (we have enough (95% and 99%) evidence to prove that \"Distribution of Personal Loan is not same across customers Avg.spending on credit cards per month\")","a8a5e1b2":"#### Hence we reject Null Hypothesis (we have enough (95% and 99%) evidence to prove that \"Distribution of Mortgage is not same across Education level\")","e213b8e2":"#### Run the KNN with no of neighbours to be 1,3,5..25 and Find the optimal number of neighbours from the above list using the Mis classification error","1bae40dd":"* **By looking into above plot, we can say that \"Distribution of Online is same across different ages of customers\". So we don't need Hypothesis testing**","8bbdead6":"* **So we have 90.40% customers in current data set who doesn't have Personal Loan with Thera Bank and rest of 9.60% have Personal Loan.**\n\n* **It is not a good distribution True\/False cases of Personal Loan in data.**","8fd489e6":"### Attribute Information\n\n* **ID :**           Customer ID\n* **Age :**          Customer's age in completed years\n* **Experience :**   #years of professional experience\n* **Income :**       Annual income of the customer (**Dollor 000**)\n* **ZIP Code :**     Home Address ZIP code.\n* **Family :**       Family size of the customer\n* **CCAvg :**        Avg. spending on credit cards per month (**Dollor 000**)\n* **Education :**    Education Level.\n        1. Undergrad\n        2. Graduate\n        3. Advanced\/Professional\n* **Mortgage :**     Value of house mortgage if any. (**Dollor 000**)\n\n* **Personal Loan :** Did this customer accept the personal loan offered in the last campaign?\n* **Securities Account :** Does the customer have a securities account with the bank?\n* **CD Account :**    Does the customer have a certificate of deposit (CD) account with the bank?\n* **Online :**        Does the customer use internet banking facilities?\n* **Credit card :**   Does the customer use a credit card issued by UniversalBank?","2f3be75d":"<p style = \"font-size:20px; color: #007580 \"><strong> Print the confusion matrix for all the above models<\/strong><\/p> ","e930bbda":"* **Above plot shows, CreditCard distribution is not equal**","4f7f07c6":"<a id = '10.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 10. Optimization <\/h2> ","f2adc530":"<p style = \"font-size:20px; color: #007580 \"><strong> Modelling - Naive Bayes<\/strong><\/p> ","ec33819c":"<a id = '7.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.4 Performance Metrics<\/strong><\/p> ","a7c86c4a":"**We can infer the below points by at the above graph**\n\n1. KNN - ROC Curve is too good, definitely it is a overfit on training data and performs badly on Production data\n\n2. Logistic Regression model ROC curve is better than Naive Bayes model\n\n3. Logistic Regression model has AUC = 96.4% which is more than Naive Bayes AUC = 93.1%\nAnd we can choose good threshold = 0.9 which can convert probability output to classifications\n\n**Note: We can suggest Thera Bank that, perhaps they can consider threshold at point 0.9 probability that every customers can repay the loan so Thera Bank does not loose the Money.**\n\n**Threshold >= 0.9, customers can repay the loan.**\n\n**Threshold < 0.9, customers can not repay the loan.**","e5eb5165":"* **By looking into above plot, we can say that \"Distribution of Mortgage is not same across Education level\". But is the difference statistically significant?**","f916180c":"1. I am able to predict the likelihood of a liability customer buying personal loans with below details.\n\n    **Cross-Validation Accuracy before feature selection:**\n    \n    Logistic Regression model accuracy = 89.4358 %,\n    KNN model accuracy = 98.3296 %,\n    Naive Bayes model accuracy = 84.5133 %\n    \n    **Cross-Validation Accuracy after feature selection:**\n    \n    Logistic Regression model accuracy = 90.1327 %,\n    KNN model accuracy = 98.7168 %,\n    Naive Bayes model accuracy = 84.6128 %,\n    \n2. By looking at KNN model cross-validation accuracy and ROC curve, we can say that \"KNN model is extremely good fit for Training data and terrible fit for testing data and Production probability of such models failing would be usually higher\". So we do not consider this model to predict the likelihood of a liability customer buying personal loans.\n\n\n3. By comparing the Logistic Regression model and Naive Bayes cross-validation accuracies, we can say that \"Logistic Regression model performs better than Naive Bayes model in Production\".\n\n    Even by looking at ROC Curve, Logistic Regression model AUC (Area Under Curve) = 96.1% more than Naive Bayes model AUC = 93.1%. Hence we consider Logistic Regression as best model.\n\n4. By considering the Logistic Regression as best model, we can choose good threshold = 0.9 which can convert probability output to classifications.\n\n    Note: We can suggest Thera Bank that, perhaps they can consider threshold at point 0.9 probability that every customers can repay the loan so Thera Bank does not loose the Money.\n\n    Threshold >= 0.9, customers can repay the loan.\n\n    Threshold < 0.9, customers can not repay the loan.","9429f99e":"<a id = '8.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 8.3 Feature Selection Summary<\/strong><\/p> ","762c2d80":"### k. Is the distribution of CD Account is same across different ages of customers?","b764c77a":"1. I am able to predict the likelihood of a liability customer buying personal loans with an accuracy 90.41 %\n\n2. By considering the Logistic Regression as best model, we can choose good threshold = 0.9 which can convert probability output to classifications.\n\n    Note: We can suggest Thera Bank that, perhaps they can consider threshold at point 0.9 probability that every customers can repay the loan so Thera Bank does not loose the Money.\n\n    Threshold >= 0.9, customers can repay the loan.\n\n    Threshold < 0.9, customers can not repay the loan.\n    \n3. 'Income' seems to be highly correlated with the Personal Loan\n4. 'CCAvg', 'Education', 'Family' and 'CD Account' are also an deciding factor of the Personal Loan\n5. Few features like 'Online', 'Credit Card' are not having much relationship with Personal Loan\n6. Standardization of data improves accuracy drastically\n7. Final set of features used to achieve an accuracy 90.41 % after performing feature selection, cross validation and hyperparameter tuning are:\n    'Family', 'Education', 'Securities Account', 'CD Account', 'Age', 'Mortgage', 'Income', 'CCAvg',  'ZIP Code'\n\n8. Finally Logistic Regression is our best model.\n\n<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Thanks for reading \ud83d\ude42<\/strong><\/p>","b42045bf":"* **Above plot shows, CD Account distribution is not equal**","d378d0f0":"* **By looking into above graph, we can say that \"Professional customers Personal loan is differ significantly from the Undergrad and Graduate customers\". But is the difference statistically significant?**","059f65c5":"#### 1. State the H0 and Ha\n\n* H<sub>0<\/sub>: x&#772;<sub>1<\/sub> = x&#772;<sub>2<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> = 0, **Distribution of Personal loan is same across different ages of customers**\n\n* H<sub>A<\/sub>: x&#772;<sub>2<\/sub> &ne; x&#772;<sub>1<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> &ne; 0, **Distribution of Personal loan is not same across different ages of customers**\n\n#### 2. Decide the significance level: alpha = 0.05\n\n#### 3. Identify the test-statistic: 2 sample t-test\n\n#### 4. Calculate the p_value using test-statistic\/t-score","65bfcca4":"<a id = '7.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.5 Overall Summary - Before feature selection<\/strong><\/p> ","08bfd18a":"#### Hence we fail to reject Null Hypothesis, we have enough (95% and 99%) evidence to prove that ,the Professional customers Personal loan is differ from Undergrad customers)\n\n#### Hence we reject Null Hypothesis, we have enough (95% and 99%) evidence to prove that ,the Professional customers Personal loan is not differ from Graduate customers)","1e9dfbff":"<p style = \"font-size:20px; color: #007580 \"><strong> Modelling - Logistic Regression <\/strong><\/p> ","e21561d5":"#### 1. State the H0 and Ha\n\n* H<sub>0<\/sub>: x&#772;<sub>1<\/sub> = x&#772;<sub>2<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> = 0, **Distribution of CCAvg is same across Education level**\n\n* H<sub>A<\/sub>: x&#772;<sub>2<\/sub> &ne; x&#772;<sub>1<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> &ne; 0, **Distribution of CCAvg is not same across Education level**\n\n#### 2. Decide the significance level: alpha = 0.05\n\n#### 3. Identify the test-statistic:\n\nHere we have multiple ram sizes\/groups. Analysis of variance can determine whether the means of three or more groups are different. ANOVA uses F-tests to statistically test the equality of means.\n\n#### 4. Calculate the p_value using ANOVA table","d9680e04":"<a id = '7.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.3 Give your reasoning on which is the best model in this case and why it performs better?<\/strong><\/p> ","3e7f91e7":"<a id = '4.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.5 Data Cleaning Summary <\/strong><\/p> ","bec762b0":"* **As we know, when K = 1, then we can say that \"KNN model is extremely good fit for Training data and terrible fit for testing data and Production probability of such models failing would be usually higher.\"**\n* **So we consider K = 3 as the final model for prediction**","54e06e9c":"#### Hence we reject Null Hypothesis (we have enough (95% and 99%) evidence to prove that \"Distribution of CCAvg is not same across Education level\")","e3678a0f":"<p style = \"font-size:20px; color: #007580 \"><strong> Modelling and Validation - Naive Bayes<\/strong><\/p> ","b3cae4d3":"<a id = '11.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 11. Conclusion <\/h2> ","5b0935ea":"<p style = \"font-size:20px; color: #007580 \"><strong> Naive Bayes Model - Confusion matrix<\/strong><\/p> ","37059755":"### Domain:\n\nBanking","e16d8d25":"<a id = '5.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.1 Univariate Analysis <\/strong><\/p> ","bcb2b1b7":"<p style = \"font-size:20px; color: #007580 \"><strong> Managing class imbalance <\/strong><\/p> ","e815e60c":"### Data Description:\n\nThe file Bank.xls contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.","26239e26":"### m. Is the distribution of Personal Loan is same across customers Avg.spending on credit cards per month?","1576f834":"#### 1. State the H0 and Ha\n\n* H<sub>0<\/sub>: x&#772;<sub>1<\/sub> = x&#772;<sub>2<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> = 0, **Distribution of Mortgage is same across Education level**\n\n* H<sub>A<\/sub>: x&#772;<sub>2<\/sub> &ne; x&#772;<sub>1<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> &ne; 0, **Distribution of Mortgage is not same across Education level**\n\n#### 2. Decide the significance level: alpha = 0.05\n\n#### 3. Identify the test-statistic:\n\nHere we have multiple ram sizes\/groups. Analysis of variance can determine whether the means of three or more groups are different. ANOVA uses F-tests to statistically test the equality of means.\n\n#### 4. Calculate the p_value using ANOVA table","a85141d2":"<p style = \"font-size:20px; color: #007580 \"><strong> KNN Model - Confusion matrix<\/strong><\/p> ","df5bb6c9":"1. I am able to predict the likelihood of a liability customer buying personal loans with below details.\n\n    **Cross-Validation Accuracy:**\n    \n    Logistic Regression model accuracy = 89.4358 %,\n    KNN model accuracy = 98.3296 %,\n    Naive Bayes model accuracy = 84.5133 %\n    \n    \n2. By looking at KNN model cross-validation accuracy and ROC curve, we can say that \"KNN model is extremely good fit for Training data and terrible fit for testing data and Production probability of such models failing would be usually higher\". So we do not consider this model to predict the likelihood of a liability customer buying personal loans.\n\n\n3. By comparing the Logistic Regression model and Naive Bayes cross-validation accuracies, we can say that \"Logistic Regression model performs better than Naive Bayes model in Production\".\n\n    Even by looking at ROC Curve, Logistic Regression model AUC (Area Under Curve) = 96.4% more than Naive Bayes model AUC = 93.1%. Hence we consider Logistic Regression as best model.\n\n4. By considering the Logistic Regression as best model, we can choose good threshold = 0.9 which can convert probability output to classifications.\n\n    Note: We can suggest Thera Bank that, perhaps they can consider threshold at point 0.9 probability that every customers can repay the loan so Thera Bank does not loose the Money.\n\n    Threshold >= 0.9, customers can repay the loan.\n\n    Threshold < 0.9, customers can not repay the loan.","4008a48b":"#### 5. Decide to Reject or Accept  Null Hypothesis","9923b42f":"#### 1. State the H0 and Ha\n\n#### Ho = The proportions of Professional customers Personal loan is not differ from others\n#### Ha = The proportions of Professional customers Personal loan is differ from others\n\n#### 2. Decide the significance level: alpha = 0.05\n\n#### 3. Identify the test-statistic: Z-test of proportions\n\n#### 4. Calculate the p_value using test-statistic","ea6b4518":"#### Remove ID column","1268bf6d":"### a. Do Professional customers Personal loan is differ significantly from the Undergrad and Graduate customers?","183afdff":"<p style = \"font-size:20px; color: #007580 \"><strong> KNN with Hyperparameter Tuning<\/strong><\/p> ","f1572e0c":"### i. Is the distribution of Credit Card is same across different ages of customers?","232591c7":"#### 5. Decide to Reject or Accept  Null Hypothesis","81fe63c5":"* **Above plot shows, Online distribution is not equal**","e979e587":"* **Above plot shows, Family distribution is not equal**","ab62cd35":"<br>\n<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Supervised Learning Project<\/h2> \n<br>","9acd51cc":"<p style = \"font-size:20px; color: #007580 \"><strong> Logistic Regression Model - Confusion matrix<\/strong><\/p> ","2739a3a7":"<a id = '4.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.2 Check Outliers <\/strong><\/p> ","38f151f0":"### b. Evaluation of the model \/ Validate Training model on Test set","daeb81d1":"1. **Final set of best features from Filter methods:**\n\n  **'Income', 'CCAvg', 'Education', 'Family', 'CD Account', 'Age', 'Mortgage', 'Securities Account', 'ZIP Code'**\n  \n2. **Final set of best features from Wrapper methods:**\n\n  **'Family', 'Education', 'Securities Account', 'CD Account', 'Online', 'CreditCard', 'Income', 'CCAvg', 'ZIP Code'**\n  \n  **Note:** I have tried both the methods and I have got slightly better accuracy in Filter methods than Wrapper methods","7a7f89f6":"**We can infer the below points by at the above graph**\n\n1. KNN - ROC Curve is too good, definitely it is a overfit on training data and performs badly on Production data\n\n2. Logistic Regression model ROC curve is better than Naive Bayes model\n\n3. Logistic Regression model has AUC = 96.1% which is more than Naive Bayes AUC = 93.1%\n   And we can choose good threshold = 0.9 which can convert probability output to classifications\n\n**Note: We can suggest Thera Bank that, perhaps they can consider threshold at point 0.9 probability that every customers can repay the loan so Thera Bank does not loose the Money.**\n\n**Threshold >= 0.9, customers can repay the loan.**\n\n**Threshold < 0.9, customers can not repay the loan.**","ad5f1eff":"* **Income column - Right skewed distribution -- Income is skewed to higher values**","75987c8d":"<a id = '5.6'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.6 EDA (Exploratory Data Analysis) Summary <\/strong><\/p> ","01f9f04f":"<p style = \"font-size:20px; color: #007580 \"><strong> Variable Identification <\/strong><\/p> ","0619bfc1":"#### 5. Decide to Reject or Accept  Null Hypothesis","3fa11c40":"### e. Is the distribution of CCAvg is same across Education level?","40442285":"**Note:**\n\n1. Considering features from Wrapper methods - we get Mean score of 10 different experiments: 89.4912 % is after stratified cross validation.\n    \n2. Considering features from Filter methods - we get Mean score of 10 different experiments: 90.0111 % is after stratified cross validation.\n    ","ba87fcac":"#### 5. Decide to Reject or Accept Null Hypothesis","a9eb1ccb":"<p style = \"font-size:20px; color: #007580 \"><strong> Cross Validation - Stratified K Fold CV<\/strong><\/p> ","cf43e51f":"<a id = '7.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.2 Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans <\/strong><\/p> ","3e02255b":"#### 1. State the H0 and Ha\n\n* H<sub>0<\/sub>: x&#772;<sub>1<\/sub> = x&#772;<sub>2<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> = 0, **Distribution of CCAvg is same across family members 1,2,3 and 4**\n\n* H<sub>A<\/sub>: x&#772;<sub>2<\/sub> &ne; x&#772;<sub>1<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> &ne; 0, **Distribution of CCAvg is not same across family members 1,2,3 and 4**\n\n#### 2. Decide the significance level: alpha = 0.05\n\n#### 3. Identify the test-statistic:\n\nHere we have multiple ram sizes\/groups. Analysis of variance can determine whether the means of three or more groups are different. ANOVA uses F-tests to statistically test the equality of means.\n\n#### 4. Calculate the p_value using ANOVA table","bf8b21ee":"<a id = '4.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.1 Check Duplicates <\/strong><\/p> ","af997c72":"* **From Chi-Squared test, we can see top 4 categorical features are --> 'CD Account', 'Education', 'Family' and 'Securities Account'**","4b08fb8d":"<a id = '9.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 9. Model Building and Validation <\/h2> ","eb904015":"####  Recursive Feature Elimination with Cross Validation","c5b955ab":"### b. Is the distribution of Personal loan is same accross Professional customers with family members 1,2,3 and 4?","b2954595":"### c. Is the distribution of Personal loan is same across different ages of customers?","8f1c4b54":"#### Hence we reject Null Hypothesis (we have enough (95% and 99%) evidence to prove that \"Distribution of CCAvg is not same across family members 1,2,3 and 4\")","0036586b":"* **The above output prints the important summary statistics of all the numeric variables like the mean, median (50%), minimum, and maximum values, along with the standard deviation.**\n\n* **Age and Experience columns are uniformly distributed**\n* **Income and CCAvg columns -- Right Skewed Distribution --- both are skewed to higer values**","fb009737":"<a id = '0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #007580; color : #fed049; border-radius: 5px 5px; text-align:center; font-weight: bold\" >Table of Contents<\/h2> \n\n1. [Overview](#1.0)\n2. [Import the necessary libraries](#2.0)\n3. [Data Collection](#3.0)\n4. [Data Cleaning](#4.0)\n\t- [4.1 Check Duplicates](#4.1)\n\t- [4.2 Check Outliers](#4.2)\n\t- [4.3 Working with Outliers: Correcting, Removing](#4.3)\n\t- [4.4 Check Missing Values](#4.4)\n\t- [4.5 Data Cleaning Summary](#4.5)\n5. [EDA (Data Analysis and Preparation)](#5.0)\n\t- [5.1 Univariate Analysis](#5.1)\n\t- [5.2 Bivariate Analysis and Hypothesis Testing](#5.2)\n\t- [5.3 Study Summary Statistics](#5.3)\n\t- [5.4 Multivariate Analysis](#5.4)\n\t- [5.5 Study Correlation](#5.5)\n\t- [5.6 EDA (Exploratory Data Analysis) Summary](#5.6)\n6. [Feature Engineering](#6.0)\n\t- [6.1 Variable Transformation (Normalization and Scaling)](#6.1)\n7. [Model Building and Validation](#7.0)\n\t- [7.1 Sampling Techniques - Create Training and Test Set](#7.1)\n\t- [7.2 Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans](#7.2)\n\t- [7.3 Give your reasoning on which is the best model in this case and why it performs better?](#7.3)\n\t- [7.4 Performance Metrics](#7.4)\n\t- [7.5 Overall Summary - Before feature selection](#7.5)\n8. [Feature Selection Methods](#8.0)\n\t- [8.1 Filter methods](#8.1)\n\t- [8.2 Wrapper methods](#8.2)\n\t- [8.3 Feature Selection Summary](#8.3)\n9. [Model Building and Validation](#9.0)\n\t- [9.1 Sampling Techniques - Create Training and Test Set](#9.1)\n\t- [9.2 Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans](#9.2)\n\t- [9.3 Overall Summary - After Feature Selection](#9.3)\n10. [Optimization](#10.0)\n11. [Conclusion](#11.0)","5310c376":"### Context\n\nThis case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.","fc883f9a":"<a id = '9.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 9.1 Sampling Techniques - Create Training and Test Set<\/strong><\/p> ","5e07dd63":"* **By looking into above plot, we can say that \"Distribution of Income is not same across Education level\". But is the difference statistically significant?**","2396d916":"**The confusion matrix**\n\n**True Positives (TP):** We correctly predicted that **1194** customers bought the Loan\n\n**True Negatives (TN):** We correctly predicted that **1219** customers did not buy the Loan\n\n**False Positives (FP):** We incorrectly predicted that **143** customers will buy the loan (a \"Type I error\")\n\n**False Negatives (FN):** We incorrectly predicted that **156** customers will not buy the loan (a \"Type II error\")","ab773bdb":"#### 5. Decide to Reject or Accept Null Hypothesis","5b4d606f":"#### Observations:","d79d73a7":"#### 1. State the H0 and Ha\n\n* H<sub>0<\/sub>: x&#772;<sub>1<\/sub> = x&#772;<sub>2<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> = 0, **Distribution of Personal Loan is same across customers Avg.spending on credit cards per month**\n\n* H<sub>A<\/sub>: x&#772;<sub>2<\/sub> &ne; x&#772;<sub>1<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> &ne; 0, **Distribution of Personal Loan is not same across customers Avg.spending on credit cards per month**\n\n#### 2. Decide the signifiance level: alpha = 0.05\n\n#### 3. Identify the test-statistic: 2 sample t-test\n\n#### 4. Calculate the p_value using test-statistic\/t-score","f4f14ca5":"### f. Is the distribution of Income is same across Education level?","9ce918a2":"* **By looking into above plot, we can say that \"Distribution of CCAvg is not same across family members 1,2,3 and 4\". But is the difference statistically significant?**","0c2dccea":"* **By looking into above graph, we can say that \"Distribution of Personal Loan is not same across customers Avg.spending on credit cards per month\". But is the difference statistically significant?**","1af0067d":"#### 1. State the H0 and Ha\n\n#### Ho = Distribution of Personal loan is same accross Professional customers with family members 1,2,3 and 4\n#### Ha = Distribution of Personal loan is not same accross Professional customers with family members 1,2,3 and 4\n\n#### 2. Decide the significance level: alpha = 0.05\n\n#### 3. Identify the test-statistic: We use the chi-square test of independence to find out the difference of categorical variables \n\n#### 4. Calculate the p_value using test-statistic","c1a5b284":"* By looking into above plot, we can say that 'Income', 'CCAvg', 'Education', 'Family', 'CD Account', 'Experience', 'ZIP Code' 'Age' and 'Mortgage' are having high scores and more relevant to target variable - 'Personal Loan'\n\n* If we observe, 'Age' and 'Experience' have same score so we can consider any one.\n\n* By combining Univariate results with Feature importance, both looks similar except 'Securities Account' which is having least score in both Feature importance and Chi-squared test\n\n* **Final set of best features from Filter methods:**\n\n    **'Income', 'CCAvg', 'Education', 'Family', 'CD Account', 'Age', 'Mortgage' and 'ZIP Code'**","96e6729d":"### Objective\n\n**The classification goal is to predict the likelihood of a liability customer buying personal loans.**","ddd909c7":"* **By looking into above plot, we can say that \"Distribution of Mortgage is not same across Family Members\". But is the difference statistically significant?**","ee13de73":"<p style = \"font-size:20px; color: #007580 \"><strong> Naive Bayes with Hyperparameter Tuning<\/strong><\/p> ","2b94bca1":"<a id = '5.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.4 Multivariate Analysis <\/strong><\/p> ","e1fb2eef":"* **Mortgage column - Right skewed distribution -- Mortgage is skewed to higher values**","5e0aed76":"* **By looking into above plot, we can say that \"Distribution of CCAvg is not same across Education level\". But is the difference statistically significant?**","b1db14e2":"* **Above plot shows, Securities Account distribution is not equal**","463676cf":"<a id = '5.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 5. EDA (Data Analysis and Preparation) <\/h2> ","4b79f4cf":"**The confusion matrix**\n\n**True Positives (TP):** We correctly predicted that **1096** customers bought the Loan\n\n**True Negatives (TN):** We correctly predicted that **1178** customers did not buy the Loan\n\n**False Positives (FP):** We incorrectly predicted that **184** customers will buy the loan (a \"Type I error\")\n\n**False Negatives (FN):** We incorrectly predicted that **254** customers will not buy the loan (a \"Type II error\")","d1cda757":"<a id = '8.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 8.1 Filter methods<\/strong><\/p> ","4b0b672a":"### d. Is the distribution of CCAvg is same across family members 1,2,3 and 4?","0b16a064":"<p style = \"font-size:20px; color: #007580 \"><strong> Logistic Regression with Hyperparameter Tuning<\/strong><\/p> ","a0faa5ac":"#### Hence we reject Null Hypothesis (we have enough (95% and 99%) evidence to prove that \"Distribution of Income is not same across Education level\")","d4145eb8":"#### Setting Options","33575785":"<a id = '1.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 1. Overview <\/h2> ","91dc6c49":"<a id = '5.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.5 Study Correlation <\/strong><\/p> ","ba185afe":"* **RFECV method is giving optimal number of features:**\n    **'Family', 'Education', 'Securities Account', 'CD Account', 'Online', 'CreditCard', 'Income', 'CCAvg', 'ZIP Code'**","4ddc17e9":"<p style = \"font-size:20px; color: #007580 \"><strong>  Cross Validation - Stratified K Fold CV<\/strong><\/p> ","813277a8":"* **CCAvg column - Right skewed distribution -- CCAvg is skewed to higher values**","d5834e3b":"#### 1. State the H0 and Ha\n\n* H<sub>0<\/sub>: x&#772;<sub>1<\/sub> = x&#772;<sub>2<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> = 0, **Distribution of Mortgage is same across Family Members**\n\n* H<sub>A<\/sub>: x&#772;<sub>2<\/sub> &ne; x&#772;<sub>1<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> &ne; 0, **Distribution of Mortgage is not same across Family Members**\n\n#### 2. Decide the significance level: alpha = 0.05\n\n#### 3. Identify the test-statistic:\n\nHere we have multiple ram sizes\/groups. Analysis of variance can determine whether the means of three or more groups are different. ANOVA uses F-tests to statistically test the equality of means.\n\n#### 4. Calculate the p_value using ANOVA table","93ca7a40":"<p style = \"font-size:20px; color: #007580 \"><strong>  Modelling and Validation - KNN<\/strong><\/p> ","049e2e30":"#### 5. Decide to Reject or Accept Null Hypothesis","019e0178":"<p style = \"font-size:20px; color: #007580 \"><strong> Using KNN model<\/strong><\/p> ","f4c00c25":"* **By looking into above frequency table, we can say that Professional customers whose family with 1 or 2 members applied more for Personal loan than with more than 2 family members**","151479f6":"### l. Is the distribution of Securities Account is same across different ages of customers?","41797c00":"<p style = \"font-size:20px; color: #007580 \"><strong> Pairplot for checking the Correlation <\/strong><\/p> ","49460447":"* **Above plot shows, Education distribution is not equal**","33d6e86c":"#### 1. State the H0 and Ha\n\n* H<sub>0<\/sub>: x&#772;<sub>1<\/sub> = x&#772;<sub>2<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> = 0, **Distribution of Income is same across Education level**\n\n* H<sub>A<\/sub>: x&#772;<sub>2<\/sub> &ne; x&#772;<sub>1<\/sub>, or x&#772;<sub>2<\/sub> - x&#772;<sub>1<\/sub> &ne; 0, **Distribution of Income is not same across Education level**\n\n#### 2. Decide the significance level: alpha = 0.05\n\n#### 3. Identify the test-statistic:\n\nHere we have multiple ram sizes\/groups. Analysis of variance can determine whether the means of three or more groups are different. ANOVA uses F-tests to statistically test the equality of means.\n\n#### 4. Calculate the p_value using ANOVA table","ebd5ad6c":"* **After performing the Accuracy metrics for different values of K from 1,3,5....25 then found that K = 1 is optimum value.**","f431c7f6":"#### 5. Decide to Reject or Accept  Null Hypothesis","b87b747f":"<p style = \"font-size:20px; color: #007580 \"><strong>  Cross Validation - Stratified K Fold CV<\/strong><\/p> ","0e3f5586":"**Note:**\n\n1. Considering features from Wrapper methods - we get Mean score of 10 different experiments: 84.4690 % is after stratified cross validation.\n    \n2. Considering features from Filter methods - we get Mean score of 10 different experiments:  84.5575 % is after stratified cross validation.\n    ","2015628b":"1. Logistic Regression with Hyperparameter Tuning ---\n    Best: 0.904080 using {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n    \n2. KNN with Hyperparameter Tuning ---\n    Best: 0.989781 using {'metric': 'manhattan', 'n_neighbors': 1, 'weights': 'uniform'}\n    \n3. Naive Bayes with Hyperparameter Tuning ---\n    Best: 0.874685 using {'var_smoothing': 0.0015199110829529332}\n    \n4. By looking into above results, we can say that Logistic Regression is best model compared to Naive Bayes model and KNN model is definately overfit on training data and perform badly on test data.","67867303":"#### a. Univariate Selection","890f7a9d":"* **By looking into above plot, we can say that \"Distribution of Securities Account is same across different ages of customers\". So we don't need Hypothesis testing**","91930a07":"**The confusion matrix**\n\n**True Positives (TP):** We correctly predicted that **1350** customers bought the Loan\n\n**True Negatives (TN):** We correctly predicted that **1303** customers did not buy the Loan\n\n**False Positives (FP):** We incorrectly predicted that **59** customers will buy the loan (a \"Type I error\")\n\n**False Negatives (FN):** We incorrectly predicted that **0** customers will not buy the loan (a \"Type II error\")","c60f1d1c":"<p style = \"font-size:20px; color: #007580 \"><strong> Data type of each attribute <\/strong><\/p> ","7d228572":"* **By looking into above plot, we can say that \"Distribution of Personal loan is same across different ages of customers\". But is it statistically significant?**","8966f351":"* **Looking at above result, ID and ZIP Code columns don't not makes sense whether customer will purchase Personal Loan or not. So we will remove them Data frame.**\n\n* **Also Experience column is having negative values - Deal this issue in Feature Selection step**","3326264e":"<a id = '8.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 8. Feature Selection Methods <\/h2> ","5695337f":"<p style = \"font-size:20px; color: #007580 \"><strong> Performance Metrics - ROC Curve<\/strong><\/p> ","9215eecd":"* **By looking into above plot, we can say that \"Distribution of CD Account is same across different ages of customers\". So we don't need Hypothesis testing**","29703aab":"### g. Is the distribution of Mortgage is same across Education level?","765cef0f":"<a id = '2.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 2. Import the necessary libraries <\/h2> ","8948ca9e":"<a id = '5.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.2 Bivariate Analysis and Hypothesis Testing <\/strong><\/p>","9af9fbd7":"<p style = \"font-size:20px; color: #007580 \"><strong> Heatmap for checking the Correlation <\/strong><\/p> ","fd7a6fd0":"**Note:** The **first array contains the list of row numbers** and **second array respective column numbers** in tbankdf_outliers data frame","30778cec":"* **From ANOVA test, we can see top 4 numerical features are --> 'Age', 'Income', 'CCAvg', 'Mortgage'**","8ff2c27e":"* **Above plot shows, Personal Loan distribution is not equal**","8abe8bac":"* **Experience column seems to be uniformly distributed**","228efbe4":"### c. Evaluation of the model \/ Validate Training model on Test set","a0ebfe65":"<p style = \"font-size:20px; color: #007580 \"><strong>  Modelling and Validation - Logistic Regression<\/strong><\/p> ","73cdad10":"* **Looking at the plot above; there are no outliers left in the data frame.**","ef6467a5":"* **Looking at the Correlation table; 'Income', 'CCAvg', 'CD Account' features are influencing the Personal Loan.**\n\n* **'Personal Loan' feature is having Moderate Positive Correlation with 'Income' feature**\n* **'Personal Loan' feature is having Low Positive Correlation with 'CD Account' and 'CCAvg' features**\n* **'Personal Loan' feature is having negligible correlation with 'Education', 'Mortgage' and 'Family' features**\n* **Except 'Income', 'CCAvg', 'CD Account', 'Education', 'Family' and 'Mortgage' features, all other features are having very weak relationship with 'Personal Loan' feature and does not account for making statistical decision (of correlation)**\n\n\n* **'Age' and 'Experience' features are having equivalent to Perfect Positive Correlation = 0.99 which is greater than standard 0.70 (70%) correlation**\n* **So we can't use 'Age' and 'Experience' at a same time to predict our dependant variable**\n* **Among these two we can use any one by comparing machine learning model with 'Age' and other with 'Experience' results**\n\n\n* **'Income' and 'CCAvg' features are having Moderate Positive Correlation**\n* **'Securities Account' and 'CD Account' features are having Low Positive Correlation**\n* **'CD Account' and 'CreditCard' features are having close to Low Positive Correlation**","b227e6fa":"* **Looking at the plot above; Income, CCAvg and Mortgage columns have outliers and we need to treat those outliers.**","0ee83a1d":"### b. Find the intercept and coefficient","8a2522a5":"1. Except 'Income', 'CCAvg', 'CD Account', 'Education', 'Family' and 'Mortgage' features, all other features are having very weak relationship with 'Personal Loan' feature and does not account for making statistical decision (of correlation)\n\n2. 'Age' and 'Experience' features are having equivalent to Perfect Positive Correlation = 0.99 which is greater than standard 0.70 (70%) correlation. So we can't use 'Age' and 'Experience' at a same time to predict our dependant variable\n\nAmong these two we can use any one by comparing machine learning model with 'Age' and other with 'Experience' results\n\n3. Target variable - 'Personal Loan' distribution is not equal (true cases: 480 (9.60%), false cases: 4520 (90.40%))\n\n4. Target variable - 'Personal Loan' class imbalance prbblem is handled using up sampling technique.","bad8f52d":"<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Here are my other notebooks, please have a look and definitely you will find it useful. Happy reading \ud83d\ude42<\/strong><\/p>\n<ol>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/industrial-safety-complete-solution\">Industrial Safety - Complete Solution<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/eda-statistical-analysis-hypothesis-testing\">EDA - Statistical Analysis - Hypothesis Testing<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/random-forest-with-bootstrap-sampling-for-beginner\">Random Forest with Bootstrap Sampling for beginner<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/amazon-electronics-eda-recommender-system\">Amazon Electronics - EDA - Recommender System<\/a><\/li>\n<\/ol>","9e7abe5b":"<a id = '6.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 6. Feature Engineering <\/h2> ","c4527c82":"### h. Is the distribution of Mortgage is same across Family Members?","39eaad2e":"<p style = \"font-size:20px; color: #007580 \"><strong> Calculate Personal Loan ratio of True\/False from outcome variable <\/strong><\/p> ","0b44c08a":"**Note:**\n\n1. Considering features from Wrapper methods - we get Mean score of 10 different experiments: 98.9823 % is after stratified cross validation.\n    \n2. Considering features from Filter methods - we get Mean score of 10 different experiments: 98.3960 % is after stratified cross validation.\n    ","9177a556":"<a id = '9.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 9.3 Overall Summary - After Feature Selection<\/strong><\/p> ","53188700":"#### Hence we reject Null Hypothesis (we have enough (95% and 99%) evidence to prove that \"distribution of Personal loan is not same accross Professional customers with family members 1,2,3 and 4\")","bffcb569":"#### 5. Decide to Reject or Accept Null Hypothesis","a5721ea8":"#### b. Feature Importance\n\nFeature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.","ae151fa8":"<a id = '4.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.3 Working with Outliers: Correcting, Removing <\/strong><\/p> ","7c9707de":"<p style = \"font-size:20px; color: #007580 \"><strong> Using Naive Bayes model<\/strong><\/p> ","652f0307":"* **By looking into above plot, we can say that \"Distribution of Credit Card is same across different ages of customers\". So we don't need Hypothesis testing**","6c916cc3":"<a id = '3.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 3. Data Collection <\/h2> ","8ffeec3a":"<a id = '8.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 8.2 Wrapper methods<\/strong><\/p> ","c53ff055":"<p style = \"font-size:20px; color: #007580 \"><strong> Model Optimization Summary<\/strong><\/p> ","9d72ac49":"* **Looking at above graph, we can say that \"as K value increases then Misclassification Error also increases\".**","92ea2154":"<a id = '6.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 6.1 Variable Transformation (Normalization and Scaling) <\/strong><\/p> ","a912dc29":"<p style = \"font-size:20px; color: #007580 \"><strong> ROC Curve<\/strong><\/p> "}}