{"cell_type":{"4ce216d4":"code","f3e00a26":"code","7acc9352":"code","e87b7dce":"code","1a3f4720":"code","3bcb8a48":"code","73192534":"code","b5a43b71":"code","935f66e9":"code","04f5e361":"code","8917ec1e":"code","695a780a":"code","0234ff89":"code","d26075bd":"code","2060a437":"code","76288a4b":"code","1048bcbb":"code","67a8dfe2":"code","3a1b15d2":"code","1d14aee6":"code","28218119":"code","54d17d52":"code","b31ebf2f":"code","9e7937b8":"code","a23469ba":"code","731d2405":"code","b503613c":"code","40be07f7":"code","e6d8bc28":"code","a81fb177":"markdown","c0997295":"markdown","55f002a8":"markdown","28f8fde7":"markdown","be254fbb":"markdown","61c4132e":"markdown","47cdba08":"markdown","3557fa38":"markdown","53fb1fb1":"markdown","03ddfa5a":"markdown","21c55847":"markdown","d91f488a":"markdown","c955f075":"markdown"},"source":{"4ce216d4":"import logging, os\n\nlogging.disable(logging.WARNING)\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"","f3e00a26":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.utils import shuffle\nfrom transformers import AutoTokenizer, TFAutoModel\n\n%matplotlib inline","7acc9352":"df = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv')","e87b7dce":"df.head()","1a3f4720":"print (\"total rows: \", len(df))","3bcb8a48":"df.isnull().sum()","73192534":"subject_df = df.drop(['ID', 'TITLE', 'ABSTRACT'], axis=1)\n\nsubject_per_entry = subject_df.sum(axis=1)\nsubject_per_entry.hist(grid=False)\nplt.show()","b5a43b71":"column_names = subject_df.columns\nX_axis = range(len(column_names))\n\nplt.figure(figsize=(12, 4))\nplt.bar(X_axis, df.drop(['ID', 'TITLE', 'ABSTRACT'], axis=1).sum())\nplt.xticks(X_axis, column_names)\n\nplt.show()","935f66e9":"tokenizer = AutoTokenizer.from_pretrained(\"allenai\/scibert_scivocab_uncased\")","04f5e361":"title_lens = [ len(tokenizer.tokenize(x)) for x in df['TITLE'] ]\nabstract_lens = [ len(tokenizer.tokenize(x)) for x in df['ABSTRACT'] ]","8917ec1e":"plt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.hist(title_lens, bins=25)\nplt.title('Len of title')\n\nplt.subplot(1, 2, 2)\nplt.hist(abstract_lens, bins=25)\nplt.title('Len of abstract')\n\nplt.show()","695a780a":"def tokenize(x: tf.Tensor, max_length=20):\n    x = x.numpy().decode('utf-8')\n    tokens = tokenizer.encode_plus(x, \n                                   padding='max_length', \n                                   return_tensors=\"tf\", \n                                   max_length=max_length, \n                                   truncation=True,  \n                                   pad_to_max_length=True, \n                                   return_token_type_ids=False)\n    return tokens['input_ids'], tokens['attention_mask']","0234ff89":"def convert_to_tf_tensor(title, abstract, label):\n    title = tf.squeeze(tf.py_function(tokenize, [title], Tout=[tf.int32, tf.int32]))\n    abstract = tf.squeeze(tf.py_function(tokenize, [abstract, 300], Tout=[tf.int32, tf.int32]))\n    return title[0], title[1], abstract[0], abstract[1], tf.expand_dims(label, axis=-1)","d26075bd":"def map_to_dict(title_inputs_ids, title_attention_mask, abstract_input_ids, abstract_attention_mask, labels):\n    inputs = {\n        'title_input_ids': title_inputs_ids,\n        'title_attention_mask': title_attention_mask,\n        'abstract_input_ids': abstract_input_ids, \n        'abstract_attention_mask': abstract_attention_mask\n    }\n    return inputs, labels","2060a437":"total_rows = len(df)\n\ndf = shuffle(df)\ntrain_size = int(total_rows * 0.8)\nvalid_size = int(total_rows * 0.1)\n\ndf_train = df[:train_size]\ndf_valid = df[train_size : train_size+valid_size]\ndf_test = df[train_size+valid_size:]","76288a4b":"train_label = df_train.drop(['ID', 'TITLE', 'ABSTRACT'], axis=1)\ntrain_data = tf.data.Dataset.from_tensor_slices((df_train['TITLE'], df_train['ABSTRACT'], train_label))\n\nvalid_label = df_valid.drop(['ID', 'TITLE', 'ABSTRACT'], axis=1)\nvalid_data = tf.data.Dataset.from_tensor_slices((df_valid['TITLE'], df_valid['ABSTRACT'], valid_label))\n\ntest_label = df_test.drop(['ID', 'TITLE', 'ABSTRACT'], axis=1)\ntest_data = tf.data.Dataset.from_tensor_slices((df_test['TITLE'], df_test['ABSTRACT'], test_label))","1048bcbb":"train_ds = (train_data\n            .shuffle(1000)\n            .map(convert_to_tf_tensor)\n            .map(map_to_dict)\n            .batch(16)\n            .prefetch(tf.data.AUTOTUNE))\n\nvalid_ds = valid_data.map(convert_to_tf_tensor).map(map_to_dict).batch(16)\ntest_ds = test_data.map(convert_to_tf_tensor).map(map_to_dict).batch(16)","67a8dfe2":"# get scibert ready\nbert = TFAutoModel.from_pretrained(\"giacomomiolo\/scibert_reupload\")\nbert.trainable = False","3a1b15d2":"# Inputs\ntitle_input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"title_input_ids\")\ntitle_attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"title_attention_mask\")\nabstract_input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"abstract_input_ids\")\nabstract_attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"abstract_attention_mask\")\n\n# Title Embedding\ntitle_output = bert(input_ids=title_input_ids, attention_mask=title_attention_mask)\ntitle_emb = tf.reduce_mean(title_output['last_hidden_state'], axis=1)\ntitle_emb = tf.keras.layers.Dense(512, activation='relu')(title_emb)\n\n# Abstract Embedding\nabstract_output = bert(input_ids=abstract_input_ids, attention_mask=abstract_attention_mask)\nabstract_emb = tf.reduce_mean(abstract_output['last_hidden_state'], axis=1)\nabstract_emb = tf.keras.layers.Dense(512, activation='relu')(abstract_emb)\n\nx = tf.concat((title_emb, abstract_emb), axis=-1)\n\n# MLP\nx = tf.keras.layers.Dense(256, activation='relu')(x)\nx = tf.keras.layers.Dense(256, activation='relu')(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.Dense(6)(x)\n\n# for multi-binary prediction. Each label (dimension) is independent from other label.\noutput = tf.expand_dims(x, axis=-1)  \n\n# Create Model\ninputs = {\n    \"title_input_ids\": title_input_ids, \n    \"title_attention_mask\": title_attention_mask,\n    \"abstract_input_ids\": abstract_input_ids, \n    \"abstract_attention_mask\": abstract_attention_mask\n}\nmodel = tf.keras.Model(inputs=inputs, outputs=output)","1d14aee6":"model.summary()","28218119":"model.compile(\n    optimizer='adam', \n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics='accuracy')","54d17d52":"model_history = model.fit(train_ds, \n                          epochs=3, \n                          callbacks=tf.keras.callbacks.EarlyStopping(patience=1), \n                          validation_data=valid_ds)","b31ebf2f":"result = model.evaluate(test_ds)\nprint (f'test loss: {result[0]}')\nprint (f'test acc:  {result[1]}')","9e7937b8":"preds = model.predict(test_ds)\npreds = tf.squeeze(preds)\npreds = tf.where(tf.math.sigmoid(preds) >= 0.5, 1.0, 0.0)","a23469ba":"test_label.head(5)","731d2405":"print (preds[:5])","b503613c":"column_names = list(test_label.columns)\nprint (column_names)","40be07f7":"acc_scores = []\nf1_scores = []\nfor i, col_name in enumerate(column_names):\n    acc = accuracy_score(test_label[col_name], preds[:,i]).item()\n    f1 = f1_score(test_label[col_name], preds[:,i], average='macro')\n    \n    acc_scores.append(acc)\n    f1_scores.append(f1)\n    \n    print (col_name)\n    print ('acc: {:.3f}'.format(acc))\n    print ('f1 : {:.3f}'.format(f1))\n    print ()","e6d8bc28":"X_axis = np.arange(len(column_names))\n\nplt.figure(figsize=(12, 6))\nplt.bar(X_axis-0.2, acc_scores, width=0.4, label='acc')\nplt.bar(X_axis+0.2, f1_scores, width=0.4, label='f1')\n\nplt.xticks(X_axis, column_names)\nplt.ylabel('prediction score')\nplt.legend()\nplt.show()","a81fb177":"# Modeling","c0997295":"Most articles are only in one category. However, there are also around 6,000 articles that consist of multiple categories.","55f002a8":"# Load Dataset","28f8fde7":"**Token Lengths**:\n- In average, the title has length around 12 tokens.\n- The abstract, on the other hand, has around 200 tokens. It can goes up to 700 tokens, but there aren't much.","be254fbb":"# Introduction","61c4132e":"# Preprocess Dataset","47cdba08":"From the predictions and the labels above, we can see that the model are doing pretty good. <br\/> \nLet's observe the result more quantitatively below.","3557fa38":"The subjects are imbalance. The Biology and Fiance subjects are very few compared to CS or Physics.","53fb1fb1":"Let's make the result more appealing","03ddfa5a":"Apparently, all data is clean. No record 404. :D","21c55847":"The model gets most of the prediction right. If we base only on accuracy, the model can easily identify \"Biology\" and \"Finance\" articles. However, this is not the since the data for these subjects are imbalance. We see in the figure on the very top of this notebook illustrates the two subjects are very few. Meaning the model can achieve good score if its prediction biases toward 0. Hence, we use **F1 Score** to verify that. <br\/>\n\nWith **F1 score**, we can see its value drop for those two subjects. Then, we can infer that the model is definitely bias toward 0 label. Yet, the model can still maintain good predictions for them.","d91f488a":"- **Dataset**: The data consists of some scientific articles that includes **title** and its **abstract**. The objective is to predict which category the article is in based on these two unstructured texts. There are six categories, and each article can belong to more than one category.\n- **Modeling**: We use SciBERT, a variant of BERT model, to obtain embedding for the title and abstract. We use **huggingface** library for the BERT model. We concateate the embedding and feed to MLP layer to predict 1\/0 for each subject **independently**.\n- **Result**: Obtain substantial results on accuracy and F1 score metrics.","c955f075":"# Inference"}}