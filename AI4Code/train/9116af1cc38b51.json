{"cell_type":{"2dcab6d1":"code","774aabcc":"code","5a134707":"code","820a54dd":"code","bbca8e7a":"code","f3d861e9":"code","196f8f73":"code","9bab5f3e":"code","e4be12d3":"code","57e7bb03":"code","fd8e0b5c":"code","dc51a9ce":"code","e41586cd":"code","109cbbec":"code","75143bac":"code","0a63a963":"code","122ff210":"code","e76047bc":"code","bec07912":"code","18270c59":"code","c45a93bb":"code","b455ee41":"code","fd97e2da":"code","8530d87c":"code","f7cbd554":"code","d0e5b2f2":"code","090aa3be":"code","55843f95":"code","6439f7a2":"code","b6244d97":"code","6b1a37bf":"code","3298ca99":"code","1e8c8362":"code","b752edbd":"code","e69ef1ee":"code","c23e8c0b":"code","b3c53f9e":"code","b7bc40b8":"code","3a535b6b":"code","e0541c26":"code","1ca31a48":"code","1dc1c0c1":"code","e89f2dbc":"markdown","45159780":"markdown","f02ffb66":"markdown","0f12be45":"markdown","dc985229":"markdown","db18bd42":"markdown","33b06c04":"markdown","bd146e7c":"markdown","051bf780":"markdown","b8047f61":"markdown","9869820d":"markdown","bee2ccee":"markdown","fd23ac47":"markdown","fb7a431c":"markdown","4bbe44d2":"markdown","6fb05e29":"markdown","b00a1419":"markdown","112d663f":"markdown","41ddc37e":"markdown","c8ef1d3a":"markdown","d52d0dea":"markdown","2a90a588":"markdown","18ebebc3":"markdown","de77027b":"markdown","05012b84":"markdown","5837a28b":"markdown","3b108806":"markdown","6964b66a":"markdown","0d38e511":"markdown","27dddd54":"markdown","23967163":"markdown","a0369589":"markdown","9e4d21d3":"markdown","4f484945":"markdown"},"source":{"2dcab6d1":"import pandas as pd \nimport timeit\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport random\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport timeit\n%matplotlib inline\nMain_Start_time = timeit.default_timer()","774aabcc":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5a134707":"pip install ppscore","820a54dd":"mds=pd.read_csv('\/kaggle\/input\/train.csv')\nmds","bbca8e7a":"print(mds.y.describe())\nsns.kdeplot(mds.y)","f3d861e9":"mds.describe(include=[np.object])","196f8f73":"import ppscore as pps\n#pps.matrix(mds_temp)\ntemp=mds.iloc[::,2:10]\ntest1=list(temp.columns)\nfor i in test1:\n    temp[i].value_counts().plot()\n    print(i)","9bab5f3e":"mds.isnull().sum()\/len(mds)*100","e4be12d3":"temp=mds.iloc[::,11:]\ntemp1=temp.var()\ntemp1","57e7bb03":"temp=mds.iloc[::,11:]\ntemp1=temp.var()\ntemp2=pd.DataFrame(temp1,columns=['col_value'])\ntest=temp2[temp2['col_value']<0.10]\nmds.drop(test.index,inplace=True,axis=1)\nmds","fd8e0b5c":"mds.drop('ID',axis=1,inplace=True)\nmds","dc51a9ce":"import category_encoders as ce \nstart_time = timeit.default_timer()\ntemp_encd=mds.copy()\ntest1=temp_encd.drop('y',axis=1)\ncolums1=[{'X0','X1','X2','X3','X4','X5','X6','X8'}]\nX0_enc= ce.BinaryEncoder(cols=['X0'])\nX1_enc= ce.BinaryEncoder(cols=['X1'])\nX2_enc= ce.BinaryEncoder(cols=['X2'])\nX3_enc= ce.BinaryEncoder(cols=['X3'])\nX4_enc= ce.BinaryEncoder(cols=['X4'])\nX5_enc= ce.BinaryEncoder(cols=['X5'])\nX6_enc= ce.BinaryEncoder(cols=['X6'])\nX8_enc= ce.BinaryEncoder(cols=['X8'])\n\nX0_bin=X0_enc.fit_transform(X=mds['X0'])\nX1_bin=X1_enc.fit_transform(X=mds['X1'])\nX2_bin=X2_enc.fit_transform(X=mds['X2'])\nX3_bin=X3_enc.fit_transform(X=mds['X3'])\nX4_bin=X4_enc.fit_transform(X=mds['X4'])\nX5_bin=X5_enc.fit_transform(X=mds['X5'])\nX6_bin=X6_enc.fit_transform(X=mds['X6'])\nX8_bin=X8_enc.fit_transform(X=mds['X8'])\n\nmds_temp=pd.concat([mds,X0_bin,X1_bin,X2_bin,X3_bin,X4_bin,X5_bin,X6_bin,X8_bin],axis=1)\nmds_temp.drop({'X0','X1','X2','X3','X4','X5','X6','X8'},inplace=True,axis=1)\nelapsed = round(timeit.default_timer() - start_time,2)\nprint(mds_temp.shape,mds.shape)\nprint(\"Time taken for Binary Encoding is :\",elapsed)\nmds_temp","e41586cd":"mds_temp.info()","109cbbec":"# Remove Outliers, As observed based on target variable. As per standards, In case, if I consider Q1-25% and Q3-75% \n# unfortunately I loose nearly 30% of datapoints. However I have only small volume of data. If you observe below distplot\n# & boxplot plotted on target variable it seems only little amount of outliers we have.So, In my opinion 10% of \n# outliers is enough here.\n#print(mds_temp.shape)\n#Q1 = mds_temp.quantile(0.005)\n#print(Q1)\n#Q3 = mds_temp.quantile(0.995)\n#print(Q3)\n#IQR = Q3 - Q1\n#print(IQR)\n#mds_out = mds_temp[~((mds_temp < (Q1 - 1.5 * IQR)) |(mds_temp > (Q3 + 1.5 * IQR))).any(axis=1)]\n#print(mds_out.shape)","75143bac":"mds.y.groupby(pd.cut(mds.y, np.arange(1,265,10))).count().plot(kind='bar')","0a63a963":"sns.boxplot(mds_temp.y)","122ff210":"i=mds_temp.shape[0]\nmds_temp=mds_temp[mds_temp['y']<130]\nmds_temp=mds_temp[mds_temp['y']>75]\nprint(\"No of Records Removed \",i -mds_temp.shape[0])","e76047bc":"sns.distplot(mds_temp.y,bins=10)","bec07912":"mds_temp.y.groupby(pd.cut(mds_temp.y, np.arange(75,130,10))).count().plot(kind='bar')\nplt.xlabel('Duration')\nplt.ylabel('Duration (seconds)')\nplt.show()","18270c59":"mds_temp","c45a93bb":"###############Running Other Models ########################\n## Creating Train_test Function\ndef Train_Test(sam_size=1,scal=False):\n    mds_temp_Train=mds_temp.sample(frac=sam_size,random_state=123)\n    y=mds_temp_Train['y']\n    X= mds_temp_Train.drop('y',axis=1)\n\n    if scal==True:\n        X=preprocessing.scale(X) \n        X=pd.DataFrame(X)\n        return X,y\n    else: return X,y","b455ee41":"summ = None\nif summ is not None:\n    del summ\ncol_list=['Model_Name','MenAbErr','MenSqErr','RMSE','Min_Err','Max_Err','Comments','Sample','time_sec']\nsumm=pd.DataFrame(columns=col_list)\nsumm['MenAbErr'] = summ['MenAbErr'].astype(float)\nsumm['MenSqErr'] = summ['MenSqErr'].astype(float)\nsumm['RMSE']     = summ['RMSE'].astype(float)\nsumm['Min_Err']  = summ['Min_Err'].astype(float)\nsumm['Max_Err']  = summ['Max_Err'].astype(float)\nsumm['Sample']   = summ['Sample'].astype(int)\nsumm['time_sec'] = summ['time_sec'].astype(float)\nsumm.info()","fd97e2da":"from sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing \nX,y=Train_Test(1,True)\nscaler=StandardScaler()\nX_norm = scaler.fit_transform(X)\npca = PCA()\npca.fit_transform(X_norm)\nvariance_ratio_cum_sum=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\nprint(variance_ratio_cum_sum)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.annotate('34',xy=(34, .8148))","8530d87c":"comp=34\nx_pca = PCA(n_components=comp)\nX_norm_final = x_pca.fit_transform(X_norm)\n# correlation between the variables after transforming the data with PCA is 0\ncorrelation = pd.DataFrame(X_norm_final).corr()\nsns.heatmap(correlation, vmax=1, square=True,cmap='viridis')\nplt.title('Correlation between different features')","f7cbd554":"start_time = timeit.default_timer()\nX=pd.DataFrame(X_norm_final)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\nreg =linear_model.LinearRegression()\nreg.fit(X_train,y_train)\ny_pred=reg.predict(X_test)\n#print(\"reg.intercept_=> %10.10f\" %(reg.intercept_))\n#print(list(zip(feature_columns, reg.coef_)))\n\n#################Calculate the Error Percentages###########\nModel_Name=\"PCA with Scaler\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='PCA used with '+str(comp)\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nelapsed = round(timeit.default_timer() - start_time,2)\n\nprint('Mean Absolute Error   :',mabs)  \nprint('Mean Squared Error    :',mse )  \nprint('Root Mean Squared     :',rmse )\nprint(\"Maximum Error is      :\",Min_Error)\nprint(\"Minimum Error is      :\",Max_Error)\nprint(\"PCA run with :        :\",comp)\nprint(\"Time Take to Run PCS  :\",elapsed)\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n\n#summ.drop({'Min_Err','Max_Err'},inplace=True,axis=1)\nsumm","d0e5b2f2":"#summ.drop(11,axis=0)","090aa3be":"start_time = timeit.default_timer()\nX,y=Train_Test(1,False)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\nreg =linear_model.LinearRegression()\nreg.fit(X_train,y_train)\ny_pred=reg.predict(X_test)\n\n#################Calculate the Error Percentages###########\nModel_Name=\"Linear Regeression\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse_val=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='Stand Scaler'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\nprint('Mean Absolute Error   :',mabs)  \nprint('Mean Squared Error    :',mse )  \nprint('Root Mean Squared     :',rmse_val )\nprint(\"Maximum Error is      :\",Min_Error)\nprint(\"Minimum Error is      :\",Max_Error)\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse_val,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n\n#summ.drop({'Min_Err','Max_Err'},inplace=True,axis=1)\nsumm","55843f95":"start_time = timeit.default_timer()\nX,y=Train_Test()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111,test_size=.2)\ny_null = np.zeros_like(y_test, dtype=int)\ny_null.fill(y_test.mean())\ny_pred=reg.predict(X_test)\n\nModel_Name=\"NULL Val Linear Regeression\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse_Null=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='NULL Values'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\nprint('Mean Absolute Error             :',mabs)  \nprint('Mean Squared Error              :',mse )  \nprint('Root Mean Squared Error For NuLL:',rmse_Null )\nprint(\"Maximum Error is                :\",Min_Error)\nprint(\"Minimum Error is                :\",Max_Error)\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse_Null,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n\nsumm\n","6439f7a2":"from sklearn import preprocessing\nstart_time = timeit.default_timer()\nX,y=Train_Test(1,True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nreg =linear_model.LinearRegression()\n\nreg.fit(X_train,y_train)\n#print(\"reg.intercept_=> %10.10f\" %(reg.intercept_))\n#print(list(zip(feature_columns, reg.coef_)))\ny_pred=reg.predict(X_test)\n\n#################Calculate the Error Percentages###########\nModel_Name=\"Linear Regerr with Scaler\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='Stand Scaler'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse_val )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse_val,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n\n#summ.drop({'Min_Err','Max_Err'},inplace=True,axis=1)\nsumm\n\n\n","b6244d97":"start_time = timeit.default_timer()\nimport xgboost as xgb\n#from xgboost import plot_importance, plot_tree\n#from sklearn.model_selection import RandomizedSearchCV ,cross_val_score, KFold\n\nX,y=Train_Test(1,True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=111)\n\nmodel = xgb.XGBRegressor()\nmodel.fit(X_train,y_train)\nprint(model)\ny_pred = model.predict(data=X_test)\n\nModel_Name=\"XGBoost\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='Standardised  Data'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\nprint('\\nMean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n#summ.drop({'Min_Err','Max_Err'},inplace=True,axis=1)\nsumm","6b1a37bf":"start_time = timeit.default_timer()\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\n## training the model\n\nX,y=Train_Test()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111)\n\n#ridgeReg = Ridge(alpha=0.05, normalize=True)\nridgeReg = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\nridgeReg.fit(X_train,y_train)\n\n#clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n\ny_pred = ridgeReg.predict(X_test)\n\nModel_Name=\"Linear Ridge Regeression\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='Scale is Preprocessing'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm\n","3298ca99":"start_time = timeit.default_timer()\nfrom sklearn.tree import DecisionTreeRegressor\nX,y=Train_Test()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\ndt=DecisionTreeRegressor()\ndt.fit(X_train,y_train)\ny_pred=dt.predict(X_test)\n\n\nModel_Name=\"DecisionTreeRegressor\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm","1e8c8362":"start_time = timeit.default_timer()\nfrom sklearn.ensemble import RandomForestRegressor\nX,y=Train_Test()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nrf=RandomForestRegressor()\nrf.fit(X_train,y_train)\ny_pred=rf.predict(X_test)\n\nModel_Name=\"RandomForestRegressor\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm","b752edbd":"start_time = timeit.default_timer()\nfrom sklearn.ensemble import AdaBoostRegressor\nab=AdaBoostRegressor()\nX,y=Train_Test()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nab.fit(X_train,y_train)\ny_pred=ab.predict(X_test)\n\nModel_Name=\"AdaBoostRegressor\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm\n","e69ef1ee":"start_time = timeit.default_timer()\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor()\nX,y=Train_Test()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\ngb.fit(X_train,y_train)\ny_pred = gb.predict(X_test)\n\nModel_Name=\"GradientBoostingRegressor\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm","c23e8c0b":"start_time = timeit.default_timer()\nimport lightgbm as lgb\nlgbm = lgb.LGBMRegressor()\nX,y=Train_Test()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nlgbm.fit(X_train,y_train)\ny_pred = lgbm.predict(X_test)\n\nModel_Name=\"Light GBM\"\nmabs=round(metrics.mean_absolute_error(y_test, y_pred),3)\nmse= round(metrics.mean_squared_error(y_test, y_pred),3)\nrmse=round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3)\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=round(df.Error.min(),3)\nMax_Error=round(df.Error.max(),3)\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm\n","b3c53f9e":"elapsed = round(timeit.default_timer() - Main_Start_time,2)\nprint(\" Time Taken to Run All the Model is elapsed:\",elapsed)\n#sns.barplot(x='Model_Name',y='RMSE',data=summ,hue='Comments')","b7bc40b8":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n#X,y=Train_Test(.01,True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(96, activation='relu', input_shape=[len(X.keys())]),\n    tf.keras.layers.Dense(96, activation='relu'),\n    tf.keras.layers.Dense(1)\n  ])\n\n#Choose optimizer\noptimizer = tf.keras.optimizers.Adam()\n\n#Compile model with mean squared error as loss function\nmodel.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])\nmodel.summary()","3a535b6b":"EPOCHS = 400\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\nhistory=model.fit(X, y,epochs=EPOCHS, validation_split = 0.1,callbacks=[early_stop], verbose=1)","e0541c26":"# Dataframe created for \nhist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","1ca31a48":"loss, mae, mse = model.evaluate(X, y, verbose=2)\n\nprint(\"Testing set Mean Absolute Error: {:5.2f} RH\".format(mae))","1dc1c0c1":"#model predictions on test data\ntest_predictions = model.predict(X).flatten()\n\nplt.scatter(y, test_predictions)\nplt.xlabel('True Values [RH]')\nplt.ylabel('Predictions [RH]')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\n_ = plt.plot([-100, 100], [-100, 100])","e89f2dbc":"# Summary Report .\n1. We can Observe PCA with Standard Scaller are More less Performing the Same Level \n1. Total Time Taken for Running the model is Less than 2 Minutes\n1. None of the Model are taking any time which is sign of proper scaling and low volume of data.","45159780":"## Checking Distribution After removal of Outliers","f02ffb66":"# Mercedes-Benz Greener Manufacturing\n\nSince the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler\u2019s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams.\nTo ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler\u2019s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations are complex and time-consuming without a powerful algorithmic approach. As one of the world\u2019s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler\u2019s production lines.\n\n1. Attribute information\nThis dataset contains an anonymized set of variables (X0 to X385), each representing a custom feature in a Mercedes car. \n** Note: This is confidential data and we will not be able to provide description for each of the anonymized set of variables.\nFor example, a variable could be 4WD, added air suspension, or a head-up display.\nThe ground truth is labeled \u2018y\u2019 and represents the time (in seconds) that the car took to pass testing for each variable.\n\n","0f12be45":"## Dropping ID column as its not Required","dc985229":"## Linear Regeression with Standard Scaller ","db18bd42":"## Ada Boost Regressor Model","33b06c04":"## Creating Function to sample the data","bd146e7c":"## Linear Regeression with Normal Values ","051bf780":"## Describe Non Numeric Columns\n\n1. We can see X0-X8 contains some characters .\n","b8047f61":"## Importing Data Set","9869820d":"## PCA Method","bee2ccee":"## Importing Data Frame.","fd23ac47":"## Removing All the Variable were variance is less than 10\n1. First for iloc[::,11:] is for Non Numerica Data which we will handle seperatly","fb7a431c":"## Counter Plot to Show Distribution of Y","4bbe44d2":"# Using Variance Method to Remove Data.","6fb05e29":"## Light GBM Regerssor Model","b00a1419":"## Decision Tree Regressor Model","112d663f":"## Box PLot to Understand Outliers in the Dataset","41ddc37e":"## Gradient Boosting Regressor Model ","c8ef1d3a":"### Plotting Histogram to See the Correlation Matrix","d52d0dea":"## Checking the Describe Method on Output Variable ","2a90a588":"## Binary Encoding Method for Catagorical Values . \n1. Binary Encoding Method is used which will create 47 Columns for 8 Columns \n1. One Hot Encoding was used earlier and it was giving 187 Columns for same data set so discarded.\n1. The Code for Binary Coding is Boiler Plate Code and Can be Used Any where. ","18ebebc3":"### Running PCA Model with Output from Previous Model .","de77027b":"## Removing Outliers based upon above two Plots.\n1. From the Above Graph its Clear entire data is between 75 and 130 .\n1. Removing all records Higher than 130 and Less than 75.\n1. There are 128 Records which Gets Removed . ","05012b84":"## Checking Distribution After removal of Outliers. ","5837a28b":"## Random Forest Regressor Model","3b108806":"## Checking the Values in No Numeric Objects","6964b66a":"## Defining Variance and Storing in Temp Variable","0d38e511":"## Creating Function to Store the results . ","27dddd54":"## Linder Ridge Regerrision","23967163":"# Running Maching Learning Algorithums ","a0369589":"## XG Boost with Standard Scaler","9e4d21d3":"## Null Value to Check Efficiency of Model ","4f484945":"## Checking Null Values\n1. We can see the no of Rows and Columns returned is 4209 * 378 which shape of our dataframe\n1. We can conclude there is no missing values in Dataframe."}}