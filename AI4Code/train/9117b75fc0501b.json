{"cell_type":{"788c885b":"code","588a106b":"code","db54c74c":"code","a5467e75":"code","c10136ff":"code","f1e641f7":"code","06ac6af4":"code","f73d4fb4":"code","2371cf99":"code","8a8d428d":"code","b75b7e8e":"code","a801711c":"code","455b33f5":"code","8bfd3314":"code","1dab0743":"code","341cdffb":"code","26e4f377":"code","08d74cad":"code","b7b53e31":"code","691a40ea":"code","502edc93":"code","cf480112":"code","730c0a5a":"code","1e20c2a7":"code","5b8b1168":"code","4e3a686c":"markdown","373c35ac":"markdown","fa691cad":"markdown","b13b157d":"markdown","a520edc9":"markdown","e985c4a6":"markdown","fd41dabb":"markdown"},"source":{"788c885b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","588a106b":"x_1 = np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/X.npy\")\ny_1 = np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/Y.npy\")\nimg_size = 64 #given in the contents\nplt.subplot(1,2,1)\nplt.imshow(x_1[260].reshape(img_size,img_size)) #To create a 64 to 64 pic we used .shape(img_size,img_size)\nplt.axis(\"off\") #To visualize prettier\nplt.subplot(1,2,2)\nplt.imshow(x_1[900].reshape(img_size,img_size))\nplt.axis(\"off\")","db54c74c":"x = np.concatenate((x_1[204:409],x_1[822:1027] ),axis = 0) # Zeros located in first range and ones is second\nz = np.zeros(205)\no = np.ones(205)\ny = np.concatenate((z,o),axis = 0).reshape(x.shape[0],1)\nprint(f\"X shape:  {x.shape} \")\nprint(r\"Y shape: \",y.shape)","a5467e75":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.15,random_state = 42)\nnumber_of_train = x_train.shape[0]\nnumber_of_test = x_test.shape[0]\n","c10136ff":"# We want to make sure these features are 2D\nx_train_flatten = x_train.reshape(number_of_train,x_train.shape[1]*x_train.shape[2])\nx_test_flatten = x_test.reshape(number_of_test,x_test.shape[1]*x_test.shape[2])\nprint(f\"X train flatten: {x_train_flatten.shape}\")\nprint(f\"X test flatten: {x_test_flatten.shape}\")\n","f1e641f7":"x_train = x_train_flatten.T\nx_test = x_test_flatten.T\ny_train = y_train.T\ny_test = y_test.T\nprint(f\"x train: {x_train.shape}\")\n\nprint(f\"x test: {x_test.shape}\")\n\nprint(f\"y train: {y_train.shape}\")\n\nprint(f\"x test: {y_test.shape}\")","06ac6af4":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter = 150)\nprint((f\"test accuracy: {logreg.fit(x_train.T,y_train.T).score(x_test.T,y_test.T)}\"))\nprint((f\"train accuracy: {logreg.fit(x_train.T,y_train.T).score(x_train.T,y_train.T)}\"))","f73d4fb4":"def sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","2371cf99":"def initialize_paramaters_and_layer_sizes_MN(x_train,y_train):\n    parameters = {\"weight1\":np.random.randn(3,x_train.shape[0]) * 0.1,\n                 \"bias1\":np.zeros((3,1)),\n                 \"weight2\":np.random.randn(y_train.shape[0],3) * 0.1,\n                 \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters\n","8a8d428d":"def forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","b75b7e8e":"def compute_cost_NN(A2,y,parameters):\n    logprobs = np.multiply(np.log(A2),y)\n    cost = -np.sum(logprobs)\/y.shape[1]\n    return cost","a801711c":"def backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","455b33f5":"def update_parameters_NN(parameters,grads,learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"] - learning_rate*grads[\"weight1\"],\n                 \"bias1\": parameters[\"bias1\"] - learning_rate*grads[\"bias1\"],\n                 \"weight2\": parameters[\"weight2\"] - learning_rate*grads[\"weight2\"],\n                 \"bias2\": parameters[\"bias2\"] - learning_rate*grads[\"bias2\"]}\n    return parameters\n","8bfd3314":"def predict_NN(parameters,x_test):\n    A2,cache = forward_propagation_NN(x_test,parameters)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    for i in range(A2.shape[1]):\n        if A2[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    \n    return y_prediction","1dab0743":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_paramaters_and_layer_sizes_MN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)","341cdffb":"x_train,x_test,y_train,y_test = x_train.T,x_test.T,y_train.T,y_test.T","26e4f377":"from keras.wrappers.scikit_learn import KerasClassifier \nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # Build layers library\n\ndef build_classifier():\n    classifier = Sequential() \n    classifier.add(Dense(units = 8,kernel_initializer = \"uniform\",activation = \"relu\",input_dim = x_train.shape[1])) #we need to give pixels by using input_dim\n    classifier.add(Dense(units = 4,kernel_initializer = \"uniform\",activation = \"relu\"))\n    classifier.add(Dense(units = 1,kernel_initializer = \"uniform\",activation = \"sigmoid\"))\n    classifier.compile(optimizer = \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier,epochs= 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmeans = accuracies.mean()\nvariance = accuracies.std()\nprint(f\"Accuracy Mean: {str(mean)}\")\nprint(f\"Accuracy Variance: {str(variance)}\")","08d74cad":"x_train,x_test,y_train,y_test = x_train.T,x_test.T,y_train.T,y_test.T","b7b53e31":"from sklearn.metrics import confusion_matrix\nimport itertools\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPool2D,BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n#\n\nmodel.add(Conv2D(filters = 8,kernel_size = (5,5),padding = \"Same\",\n                activation = \"relu\",input_shape = (64,64,1)))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n#\n\nmodel.add(Conv2D(filters = 16,kernel_size = (5,5),padding = \"Same\",\n                activation = \"relu\",))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n#\n\nmodel.add(Conv2D(filters = 32,kernel_size = (5,5),padding = \"Same\",\n                activation = \"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n#\n\nmodel.add(Conv2D(filters = 64,kernel_size = (3,3),padding = \"Same\", #No need to have input_shape again since we can use the same one here without writing again.\n                activation = \"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2), strides = (2,2))) #pool'u gezdirirken ne kadar atlayacagimizi stride ile belirliyoruz. 2 basamak burda.\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n\n# Fully Connected\nmodel.add(Flatten())\n\n\nmodel.add(Dense(600,activation = \"relu\")) #hidden layer\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(320,activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(320,activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(320,activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(300,activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(10,activation = \"softmax\")) #output layer\nmodel.summary()","691a40ea":"optimizer = Adam(lr = 0.001,beta_1=0.9, beta_2=0.999) # the optimizer tries to find the best learning rate for our model.","502edc93":"model.compile(optimizer=optimizer,loss = \"binary_crossentropy\",metrics = [\"accuracy\"])","cf480112":"epochs = 200\nbatch_size = 10","730c0a5a":"datagen = ImageDataGenerator(rotation_range=0.9,\n                            zoom_range= 0.5,\n                            width_shift_range= 0.5,\n                            height_shift_range=0.5)\ndatagen.fit(x_train)","1e20c2a7":"history = model.fit_generator(datagen.flow(x_train,y_train,batch_size = batch_size),\n                             epochs=epochs,validation_data= (x_test,y_test),steps_per_epoch=x_train.shape[0] \/\/ batch_size)","5b8b1168":"plt.figure(figsize=(24,8))\n\nplt.subplot(1,2,1)\nplt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\", c=\"red\", linewidth=4,alpha = 0.65)\nplt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\", c=\"blue\", linewidth=4,alpha = 0.65)\nplt.legend()\n\n\nplt.subplot(1,2,2)\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\", c=\"red\", linewidth=4,alpha = 0.65)\nplt.plot(history.history[\"loss\"], label=\"Training Loss\", c=\"blue\", linewidth=4,alpha = 0.65)\nplt.legend()\n\nplt.suptitle(\"ACCURACY \/ LOSS\",fontsize=18)\n\nplt.show()","4e3a686c":"y_train = y_train.T\ny_train.shape","373c35ac":"# USING KERAS -- L-Layer Neural Network","fa691cad":"x_train = x_train.reshape(-1,64,64,1)\nx_test = x_test.reshape(-1,64,64,1)\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)","b13b157d":"https:\/\/playground.tensorflow.org\/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.58111&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false","a520edc9":"\nArtificial Neural Network with Pytorch library.\nPytorch is one of the frame works like keras.\nIt eases implementing and constructing deep learning blocks.\nArtificial Neural Network: https:\/\/www.kaggle.com\/kanncaa1\/pytorch-tutorial-for-deep-learning-lovers","e985c4a6":"y_test = y_test.T\ny_test.shape","fd41dabb":"# IMPLEMENTING CNN"}}