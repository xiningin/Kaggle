{"cell_type":{"2bd6c31c":"code","c4c42ef0":"code","cefd8e88":"code","ecc0ce33":"code","a6148b6f":"code","0bcf262c":"code","90a25d89":"code","d9807751":"code","3424ac77":"code","b634a519":"code","3ed64830":"code","3deae5ea":"code","80235e00":"code","bb0ddf4b":"code","50cc65db":"code","4c838f28":"code","04ce9ea0":"code","7dd004b8":"code","55aa358f":"code","c04bdc89":"code","d6d2512b":"code","ae2849d6":"code","547631ac":"code","5b418b91":"code","3a43a498":"code","8a504c20":"code","a58f3e35":"code","1c80573f":"code","dd734e27":"code","39e2794c":"code","060096b8":"code","8ca2a0ec":"code","ba27f66b":"code","24176af9":"code","8b7c6ee2":"code","2d116635":"code","5e0ce998":"code","760286d0":"code","9f7de905":"code","9c3104b8":"code","84a07166":"code","0161993c":"code","726854c1":"code","d74e9e16":"code","ec8b2fed":"code","c7c4b751":"code","80051f51":"code","79f4801e":"code","8db906d2":"code","ac639675":"code","a9b9ee59":"code","b4018e86":"markdown"},"source":{"2bd6c31c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4c42ef0":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","cefd8e88":"df = pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/car data.csv')\ndf.head()","ecc0ce33":"df.shape","a6148b6f":"df.isnull().sum()","0bcf262c":"df.nunique()","90a25d89":"print(df['Seller_Type'].unique())\nprint(df['Transmission'].unique())\nprint(df['Owner'].unique())\nprint(df['Fuel_Type'].unique())","d9807751":"df.columns","3424ac77":"final_dataset = df[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',\n       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']]","b634a519":"final_dataset.head()","3ed64830":"final_dataset['Current_Year'] = 2020","3deae5ea":"final_dataset['No_of_Years'] = final_dataset['Current_Year'] - final_dataset['Year']","80235e00":"final_dataset.head()","bb0ddf4b":"final_dataset.drop(['Year'], axis = 1, inplace = True)","50cc65db":"final_dataset.head(2)","4c838f28":"final_dataset.drop(['Current_Year'], axis = 1, inplace = True)","04ce9ea0":"final_dataset.head(2)","7dd004b8":"final_dataset = pd.get_dummies(final_dataset, drop_first=True)","55aa358f":"final_dataset.head(2)","c04bdc89":"final_dataset.corr()","d6d2512b":"sns.heatmap(final_dataset.corr())","ae2849d6":"sns.pairplot(final_dataset)\n","547631ac":"import matplotlib.pyplot as plt\n%matplotlib inline","5b418b91":"corrmat=final_dataset.corr() \ntop_corr_features=corrmat.index \nplt.figure(figsize=(13,8)) \n#plot heat map \nsns.heatmap(final_dataset[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","3a43a498":"final_dataset.head(2)","8a504c20":"x = final_dataset.iloc[:,1:]\ny = final_dataset.iloc[:,0]","a58f3e35":"x.head(2)","1c80573f":"y.head(2)","dd734e27":"# Feature Importance\nfrom sklearn.ensemble import ExtraTreesRegressor","39e2794c":"model = ExtraTreesRegressor()\nmodel.fit(x,y)","060096b8":"print(model.feature_importances_)","8ca2a0ec":"feat_importance = pd.Series(model.feature_importances_, index = x.columns)\nfeat_importance.nlargest(5).plot(kind='barh')\nplt.show()","ba27f66b":"x_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)","24176af9":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","8b7c6ee2":"from sklearn.ensemble import RandomForestRegressor","2d116635":"rf = RandomForestRegressor()","5e0ce998":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\nprint(n_estimators)","760286d0":"from sklearn.model_selection import RandomizedSearchCV","9f7de905":"\n#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","9c3104b8":"\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","84a07166":"\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()","0161993c":"\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","726854c1":"\nrf_random.fit(x_train,y_train)","d74e9e16":"\nrf_random.best_params_","ec8b2fed":"rf_random.best_score_","c7c4b751":"predictions=rf_random.predict(x_test)","80051f51":"\nsns.distplot(y_test-predictions)","79f4801e":"plt.scatter(y_test,predictions)","8db906d2":"\nfrom sklearn import metrics","ac639675":"print('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","a9b9ee59":"import pickle\n# open a file, where you ant to store the data\nfile = open('random_forest_regression_model.pkl', 'wb')\n\n# dump information to that file\npickle.dump(rf_random, file)","b4018e86":"from sklearn.model_selection import train_test_split"}}