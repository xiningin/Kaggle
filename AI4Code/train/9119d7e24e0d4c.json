{"cell_type":{"884d6e10":"code","7562f59c":"code","7abcad7e":"code","e9094214":"code","69d1efd6":"code","49055fca":"code","bf680a99":"code","cdd30ade":"code","0bd936d9":"code","ca1bf8a3":"code","aab45f9f":"code","c57b3fff":"code","9772b20d":"code","6573622c":"code","0be214de":"code","21589e52":"code","2e303ea8":"code","1a74c04e":"code","3e02ca32":"code","330b7b06":"code","4a56c868":"code","561699b6":"markdown","a5119c71":"markdown","4599056c":"markdown","04f06094":"markdown"},"source":{"884d6e10":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nimport random\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import callbacks\nfrom keras import backend as K","7562f59c":"#Reproducible results\nfrom numpy.random import seed\nseed(42)\nfrom tensorflow.random import set_seed\nset_seed(42)\n\nrandom.seed(42)\nos.environ['PYTHONHASHSEED'] = str(42)","7abcad7e":"#from tensorflow.python.client import device_lib\n#print(device_lib.list_local_devices())","e9094214":"df_train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv', index_col = 'id')\nY_train = df_train['target'].copy()\nX_train = df_train.copy().drop('target', axis = 1)\n\nX_test = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv', index_col = 'id')","69d1efd6":"class_map = {'Class_1': 0,\n            'Class_2': 1,\n            'Class_3': 2,\n            'Class_4': 3,\n            'Class_5': 4,\n            'Class_6': 5,\n            'Class_7': 6,\n            'Class_8': 7,\n            'Class_9': 8}\nY_train = Y_train.map(class_map).astype('int')\nY_train","49055fca":"#Converting target series to matrix for multiclass classification on Keras\n\nY_train = to_categorical(Y_train)\nY_train","bf680a99":"cce = keras.losses.CategoricalCrossentropy()\n#Custom metric from Pourchot added 24\/06\ndef custom_metric(y_true, y_pred):\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss\n\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_custom_metric',\n    patience=12,\n    min_delta=0.0000001,\n    restore_best_weights=True,\n)\n\nplateau = callbacks.ReduceLROnPlateau(\n    monitor='val_custom_metric',\n    factor = 0.5,                                     \n    patience = 2,                                   \n    min_delt = 0.0000001,                                \n    cooldown = 0,                               \n    verbose = 1\n) ","cdd30ade":"def get_model():\n    model = keras.Sequential([\n        layers.Input(shape = (75,)),\n        layers.Embedding(360, 8, ),\n        layers.Conv1D(16, kernel_size=1, activation='relu'), #added 22\/06\n        layers.Flatten(),\n        layers.Dropout(0.3), #added 20\/06\n        #layers.BatchNormalization(input_shape = [75]),\n        layers.Dense(units = 128, activation = 'relu', kernel_initializer='he_normal'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(units = 64, activation = 'relu', kernel_initializer='he_normal'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(units = 32, activation = 'relu', kernel_initializer='he_normal'),        \n        layers.BatchNormalization(),\n        layers.Dropout(0.2),    \n        layers.Dense(9, activation = 'softmax', kernel_initializer='lecun_normal'), #init add 27\/06 based on BIZEN\n    ])\n    \n    return model","0bd936d9":"keras.backend.clear_session()\n\nmodel = get_model()\nmodel.compile(loss='categorical_crossentropy'\n              , optimizer = keras.optimizers.Adam(learning_rate=0.0002), metrics=custom_metric)\n\nmodel.summary()","ca1bf8a3":"X_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 42\n                                                    , stratify = Y_train)","aab45f9f":"history = model.fit(X_train_split, Y_train_split,\n          batch_size = 128, epochs = 100,\n          validation_data=(X_val_split, Y_val_split),\n          callbacks=[early_stopping, plateau]);","c57b3fff":"score = model.evaluate(X_val_split, Y_val_split, verbose = 0)\nprint('Test loss: {}'.format(score[0]))\nprint('Test accuracy: {}%'.format(score[1] * 100))","9772b20d":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss)')\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.legend(['train', 'test'], loc='best')\nplt.show()","6573622c":"Y_train = df_train['target'].copy()\nY_train = Y_train.map(class_map).astype('int')\nY_train","0be214de":"def prediction (X_train, Y_train, X_test):\n    \n    keras.backend.clear_session()\n\n    kfold = StratifiedKFold(n_splits = 25)\n\n    y_pred = np.zeros((100000,9))\n    train_oof = np.zeros((200000,9))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xval = X_train.iloc[val_idx]\n        yval = Y_train.iloc[val_idx]\n        \n        ytrain = to_categorical(ytrain)\n        yval = to_categorical(yval)\n        \n        # fit model for current fold\n        model = get_model()\n        model.compile(loss='categorical_crossentropy'\n                      , optimizer = keras.optimizers.Adam(learning_rate=0.0002), metrics=custom_metric)\n        \n        model.fit(xtrain, ytrain,\n        batch_size = 128, epochs = 100,\n        validation_data=(xval, yval),\n        callbacks=[early_stopping, plateau]);\n\n        #create predictions\n        y_pred += model.predict(X_test)\/kfold.n_splits\n        print(y_pred)\n               \n        val_pred = model.predict(xval)\n        # getting out-of-fold predictions on training set\n        train_oof[val_idx] = val_pred\n        \n        # calculate and append logloss\n        fold_logloss = metrics.log_loss(yval,val_pred)\n        print(\"Logloss: {0:0.5f}\". format(fold_logloss))\n  \n    return y_pred, train_oof","21589e52":"nn_pred, train_oof = prediction (X_train, Y_train, X_test)","2e303ea8":"print(\"Logloss: {0:0.6f}\".format(metrics.log_loss(Y_train,train_oof)))","1a74c04e":"train_oof = pd.DataFrame(train_oof, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\ntrain_oof","3e02ca32":"pred_test = pd.DataFrame(nn_pred, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\npred_test","330b7b06":"train_oof.to_csv('nn_train_oof.csv', index=False)\ntrain_oof","4a56c868":"output = pred_test\noutput['id'] = X_test.index\noutput.to_csv('submission.csv', index=False)\n\noutput","561699b6":"## Making Predictions","a5119c71":"## Importing Libraries and Datasets","4599056c":"## Creating and Evaluating the NN","04f06094":"# <center>Tabular Playground Series - June\/2021<center>\n## <center>Keras Neural Network with Embedding Layer<center>\n---\n\nNeural Network inspired by [@pourchot's](https:\/\/www.kaggle.com\/pourchot) notebook [Simple Keras embedding in 10 folds](https:\/\/www.kaggle.com\/pourchot\/simple-keras-embedding-in-10-folds). If you find this notebook useful, please consider upvoting both this and [@pourchot's](https:\/\/www.kaggle.com\/pourchot)\n\nMy other notebooks in this competition:\n- [Tabular Playground Series - June\/2021: Starter - EDA + Base LightGBM](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-starter-eda-base-lgbm)\n- [Tabular Playground Series - June\/2021: Simple Neural Network with Keras](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-simple-nn-with-keras)\n- [Tabular Playground Series - June\/2021: Wide and Deep Neural Network with Keras](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-wide-and-deep-nn-w-keras)\n- [Tabular Playground Series - June\/2021: LightAutoML with KNN Features](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-lightautoml-w-knn-feats)\n- [Tabular Playground Series - June\/2021: Keras Neural Network with Skip Connections](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-keras-nn-with-skip-connections)\n"}}