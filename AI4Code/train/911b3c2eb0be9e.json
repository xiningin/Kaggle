{"cell_type":{"c9534457":"code","5e36cfb5":"code","ecab6daa":"code","7f189132":"code","88f9d401":"code","f93d7f50":"code","e35d244b":"code","9946ab60":"code","808d6fec":"code","0159a6ed":"code","a6ab700c":"code","83191caa":"code","edbffc32":"code","cb40f171":"code","8b354328":"code","acc888f1":"code","e057b266":"code","65bac951":"code","130417af":"code","27ab2382":"code","fb02d0c4":"code","abed7c83":"code","17ebe40a":"code","eb91cbe7":"code","7e9c529f":"code","02d6ad34":"code","32a65716":"code","ab23f3be":"code","679f723e":"code","b600b274":"code","42559fa7":"code","0cab0216":"code","ff462611":"code","22a3a446":"code","3e61a95f":"code","e2ea9c35":"code","ffe91042":"code","ce30e45a":"code","1a714e08":"code","3b563d84":"code","7e66deaf":"code","d4368c91":"code","d63081f0":"code","64f5f246":"code","6858fe2f":"code","923da028":"code","e58de857":"code","069607ad":"code","471d8584":"code","d29e4a17":"code","a9082ab9":"code","b74f2ead":"code","0003d674":"code","4beeade4":"code","f133fe7b":"code","f4d2a344":"code","488f0b12":"code","053e1e54":"code","13937dbc":"code","c3da41fe":"code","0626e8cc":"code","a2cf37c5":"code","ac312b90":"code","7d4afaf3":"code","d5d322bb":"markdown","125c7fe7":"markdown","0ccd0537":"markdown","4f315f39":"markdown","f1f00813":"markdown","69b6bd8a":"markdown","857c1d6c":"markdown","f69a6018":"markdown","c4dc6e83":"markdown","84bcf050":"markdown","ed169186":"markdown","b8c071d1":"markdown","c3f6e595":"markdown","9783615c":"markdown","88fbb04a":"markdown","fd4e5297":"markdown","bbf0d01d":"markdown","1fb107c0":"markdown","725f34e2":"markdown","a8ac4cf5":"markdown","7a64b3ac":"markdown","4881ad55":"markdown","4bc23f87":"markdown"},"source":{"c9534457":"# Basic data science pakages\nimport numpy as np ### For numerical computation\nimport pandas as pd ### For working with data\n\n# For Creating visualizations \n%matplotlib inline\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\n\n# To handle imbalanced data\nfrom imblearn.over_sampling import SMOTE\n\n# For creating training and test set\nfrom sklearn.model_selection import train_test_split\n\n# For column transformation\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, LabelEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\n\n# To make pipeline (or automate all the model creation works)\nfrom sklearn.pipeline import make_pipeline\n\n# Machine learning models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# For Hyperparameter Optimization\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# For evaluating model\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n\n# Remove all kinds of warning\nfrom warnings import filterwarnings \nfilterwarnings('ignore')","5e36cfb5":"# Type of graph that we want\nplt.style.use('seaborn-whitegrid')","ecab6daa":"# Maximum number of columns and rows that it will show\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 10)","7f189132":"# Loading the dataset\ndf = pd.read_csv('..\/input\/income-adult\/adult_data.csv')","88f9d401":"# Showing the data\ndf.head()","f93d7f50":"# Shape of the data\ndf.shape","e35d244b":"# Stripping the unnecessary spaces in the names of columns\ndf.columns = df.columns.str.strip()","9946ab60":"# Total Description of the data\ndf.describe()","808d6fec":"# Total information of data\ndf.info()","0159a6ed":"# Stripping out unnecessary spaces in the values of categorical columns\nfor i in df.columns:\n    if df[i].dtype == object:\n        df[i] = df[i].str.strip()","a6ab700c":"# Looking into each columns individually\nfor i in df.columns:\n    print('*' * 100)\n    print('{}:- {}\\n{}\\n'.format(i, df[i].nunique(), df[i].unique()))\n    print(pd.DataFrame({'count': df[i].value_counts(), '%': df[i].value_counts(normalize = True)}))\n    print('\/' * 100, '\\n\\n')","83191caa":"# Depict same information for education (so, removing it)\ndf.drop('education-num', axis = 1, inplace = True)","edbffc32":"# Distribution of values in columns\nfor i in df.columns:\n    print(i)\n    try: \n        df[i].plot.hist(bins = 30)\n        plt.show()\n    except:\n        plt.barh(df[i].value_counts().index, df[i].value_counts().values)\n        plt.show()","cb40f171":"# Finding which columns have the missing values\nmissing_val = []\nfor i in df.columns:\n    if ('?' in df[i].unique()):\n        print(i)\n        missing_val.append(i)","8b354328":"# Looking into each columns which have missing values\nfor i in missing_val:\n    print(i, ':-')\n    print('_'*20)\n    print(df[i].value_counts())\n    print('*'*20, '\\n\\n')","acc888f1":"# Filling missing values of native-country\ndf['native-country'].replace('?', df['native-country'].value_counts().index[0], inplace = True)","e057b266":"# Column (native-country) after filling the missing values\ndf['native-country'].value_counts()","65bac951":"# Filling misssing values of occupation\nreplace_occ = df.loc[df['occupation'] != '?', 'occupation'].sample(len(df.loc[df['occupation'] == '?', 'occupation']))\n\nreplace_occ.index = df.loc[df['occupation'] == '?'].index\n\ndf.loc[df['occupation'] == '?', 'occupation'] = replace_occ","130417af":"# Column (occupation) after filling the missing values\ndf['occupation'].value_counts()","27ab2382":"# Filling missing values of workclass\nreplace_wkc = df.loc[df['workclass'] != '?', 'workclass'].sample(len(df.loc[df['workclass'] == '?', 'workclass']))\n\nreplace_wkc.index = df.loc[df['workclass'] == '?'].index\n\ndf.loc[df['workclass'] == '?', 'workclass'] = replace_wkc","fb02d0c4":"# Column (workclass) after filling the missing values\ndf['workclass'].value_counts()","abed7c83":"# It is a class which will help in checking outliers in different columns of the data\n# Here, It uses three techniques to find the outliers (we can use anyone of them)\n# The techniques are:-\n# * IQR\n# * Z score\n# * Standard Deviation\n\nclass Outliers(object):\n    def __init__(self, df, col):\n        self.df = df\n        self.col = col\n        self.min = df[col].min()\n        self.max = df[col].max()\n        self.mean = df[col].mean()\n        self.std = df[col].std()\n        self.median = df[col].median()\n        self.quantile_25 = df[col].quantile(0.25)\n        self.quantile_75 = df[col].quantile(0.75)\n    \n    @property\n    def info(self):\n        \n        print('{}:- '.format(self.col))\n        print('Minimum:- {}'.format(self.min))\n        print('Maximum:- {}'.format(self.max))\n        print('Mean:- {}'.format(self.mean))\n        print('Median:- {}'.format(self.median))\n        print('Standard Deviation:- {}'.format(self.std))\n        print('First Quantile:- {}'.format(self.quantile_25))\n        print('Third Quantile:- {}'.format(self.quantile_75))\n        \n        \nclass IQR(Outliers):\n    def __init__(self, df, col):\n        super().__init__(df, col) \n        \n        self.IQR = self.quantile_75 - self.quantile_25\n        self.lower_bound = self.quantile_25 - (1.5 * self.IQR)\n        self.upper_bound = self.quantile_75 + (1.5 * self.IQR)\n        \n    def iqr_outliers(self):\n        \n        return self.df.loc[(self.df[self.col] < self.lower_bound) | (self.df[self.col] > self.upper_bound), self.col].values\n    \n    def removed_outliers(self):\n        return self.df.loc[(self.df[self.col] > self.lower_bound) & (self.df[self.col] < self.upper_bound)]\n    \n\nclass Z_score(Outliers):\n    def __init__(self, df, col):\n        super().__init__(df, col)\n        \n        pass\n    \n    def z_score_outliers(self):\n        outlier = []\n        for i in self.df[self.col]:\n            z = (i - self.mean) \/ self.std\n            if abs(z) > 3:\n                outlier.append(i)\n                \n        return outlier\n    \n    def removed_outliers(self):\n        \n        df_copy = self.df\n        for i in self.z_score_outliers():\n            df_copy = df_copy.loc[df_copy[self.col] != i]\n            \n        return df_copy\n    \nclass StandardDeviation(Outliers):\n    def __init__(self, df, col):\n        super().__init__(df, col)\n        pass\n    \n    @property\n    def std_calc(self):\n        lower_std = self.mean - (3 * self.std)\n        upper_std = self.mean + (3 * self.std)\n        \n        return lower_std, upper_std\n    \n    def std_outliers(self):\n        lower_std, upper_std = self.std_calc\n        return self.df.loc[(self.df[self.col] < lower_std) | (self.df[self.col] > upper_std), self.col].values\n    \n    def removed_outliers(self):\n        lower_std, upper_std = self.std_calc\n        return self.df.loc[(self.df[self.col] > lower_std) & (self.df[self.col] < upper_std)]","17ebe40a":"# It will show the outliers of the columns\n\nfor i in df.columns:\n    if df[i].dtype != object:\n        \n        out = Outliers(df, i)\n        iqr = IQR(df, i)\n        z_score = Z_score(df, i)\n        std = StandardDeviation(df, i)\n        \n        print('Column:- {}\\n'.format(i))\n        print('INFO:- \\n')\n        out.info\n        \n        print('\\nOutlier with IQR:- {}\\n'.format(i))\n        print(iqr.iqr_outliers())\n        print('----------> dataset shape after removing outliers with iqr:- {}\\n'.format(iqr.removed_outliers().shape))\n        \n        print('\\nOutlier with Z_score:- {}\\n'.format(i))\n        print(z_score.z_score_outliers())\n        print('----------> dataset shape after removing outliers with z_score:- {}\\n'.format(z_score.removed_outliers().shape))\n        \n        print('\\nOutlier with Standard deviation:- {}\\n'.format(i))\n        print(std.std_outliers())\n        print('----------> dataset shape after removing outliers with Standard Deviation:{}\\n'.format(std.removed_outliers().shape))\n        print('*'*100)","eb91cbe7":"# Here, after looking at the outliers we came to the point where we will remove outliers of\n# ---> age (z_score or Standard deviation)\n# ---> fnlwgt (z_score or standard deviation)\n# ---> hours per week (z_score or standard deviation)\n# ---> capital gain (average of different groups)\n# ---> capital loss (average of different groups)","7e9c529f":"# Removing outliers of age, fnlwgt, hours-per-week\ndf = Z_score(df, 'age').removed_outliers()\ndf = StandardDeviation(df, 'fnlwgt').removed_outliers()\ndf = Z_score(df, 'hours-per-week').removed_outliers()","02d6ad34":"df = df.reset_index().rename({'index': 'new_index'}, axis = 1).drop('new_index', axis = 1)","32a65716":"# Working on outliers of capital-gain 1\n# making the group in which we will substitute the mean values of that group\ncap_gn = {}\nfor i in np.arange(1, 110000, 10000):\n    \n        cap_gn[str(i) + ' - ' + str(i + 10000)] = df.loc[(df['capital-gain'] >= i) & (df['capital-gain'] < i + 10000), 'capital-gain'].mean()\n        \nfor i,j in cap_gn.items():\n    if j is np.nan:\n        cap_gn[i] = 0\n        \ncap_gn","ab23f3be":"# Working on outliers of capital-gain 2\n# Substituing mean values in capital-gain\nfor i in range(len(df['capital-gain'])):\n    for j,k in cap_gn.items(): \n        t = int(j.split(' ')[0])\n        r = int(j.split(' ')[-1])\n        if (df.loc[i, 'capital-gain'] >= t) & (df.loc[i, 'capital-gain'] < r):\n            df.loc[i, 'capital-gain'] = k\n    ","679f723e":"# capital-gain after handling outliers\n# Here, it still don't look good, but i'm gonna keep it (because we can't always remove outliers)\n# You can change\/remove if you want\n\n\ndf['capital-gain'].value_counts()","b600b274":"# Doing same as like the capital-loss\ncap_ls = {}\nfor i in np.arange(1, 6000, 1000):\n    \n        cap_ls[str(i) + ' - ' + str(i + 1000)] = df.loc[(df['capital-loss'] >= i) & (df['capital-loss'] < i + 1000), 'capital-loss'].mean()\n        \nfor i,j in cap_ls.items():\n    if j is np.nan:\n        cap_ls[i] = 0\n        \ncap_ls","42559fa7":"# same as capital-gain\nfor i in range(len(df['capital-loss'])):\n    for j,k in cap_ls.items(): \n        t = int(j.split(' ')[0])\n        r = int(j.split(' ')[-1])\n        if (df.loc[i, 'capital-loss'] >= t) & (df.loc[i, 'capital-loss'] < r):\n            df.loc[i, 'capital-loss'] = k\n    ","0cab0216":"# # Here also, it don't look good, but i'm gonna keep it (because we can't always remove outliers)\n# You can change\/remove outliers if you want\n\ndf['capital-loss'].value_counts()","ff462611":"# Here, we can see that we have imbalanced data\npd.DataFrame({'count': df.salary.value_counts(), '%': df.salary.value_counts(normalize = True)})","22a3a446":"# It will help in balancing imbalanced data\nsmote = SMOTE()","3e61a95f":"# The imbalanced data\ndf.head()","e2ea9c35":"# Transforming columns into numerical from categorical values\n# as it supports numerical and not categorical values\n# And also scaling the existing numerical values \n\nOrdenc = OrdinalEncoder()\nlabenc = LabelEncoder()\nscale = StandardScaler()\n\nnum_enc = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\ncat_enc = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\ncat_enc_tar = ['salary']\n\ndf[cat_enc] = Ordenc.fit_transform(df[cat_enc])\ndf[cat_enc_tar] = labenc.fit_transform(df[cat_enc_tar])\ndf[num_enc] = scale.fit_transform(df[num_enc])","ffe91042":"# Balancing the data\nX_train, y_train = smote.fit_resample(df.loc[:, :'salary'], df['salary'])","ce30e45a":"# Data before balanced\ndf.head()","1a714e08":"# Converting the balanced data into dataframe\nX_train = pd.DataFrame(X_train, columns = df.columns[:-1])\ny_train = pd.DataFrame(y_train, columns = ['salary'])","3b563d84":"# Building the balanced dataframe \n\na = pd.DataFrame(Ordenc.inverse_transform(X_train[cat_enc]), columns = cat_enc)\nb = pd.DataFrame(labenc.inverse_transform(y_train), columns = ['salary'])\nc = pd.DataFrame(scale.inverse_transform(X_train[num_enc]), columns = num_enc)\n\ndf = pd.merge(a, c, left_index = True, right_index = True)\ndf = pd.merge(df, b, left_index = True, right_index = True)","7e66deaf":"# Balanced data frame\ndf.head()","d4368c91":"# Shape of the balanced dataframe\ndf.shape","d63081f0":"# Balanced data for salary column\ndf.salary.value_counts()","64f5f246":"# age cannot be of float type\ndf['age'] = df['age'].astype('int')","6858fe2f":"col = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'salary']\ntotal_col = ['workclass', 'education', 'marital-status', 'occupation',\n             'relationship', 'race', 'sex', 'native-country', 'salary', 'age', 'fnlwgt', 'capital-gain', 'capital-loss', \n             'hours-per-week']\n\ntrans_col = make_column_transformer((OrdinalEncoder(), col),\n                                    remainder = 'passthrough')\n\ncorr_df = trans_col.fit_transform(df)\ncorr_df = pd.DataFrame(corr_df, columns = total_col)\ncorr_df","923da028":"plt.figure(figsize = (12, 10))\n\nsns.heatmap(corr_df.corr(), annot = True)\nplt.show()","e58de857":"age_dis = pd.cut(df['age'], [10, 20, 30,40,50,60,70,80,90, 100])\ntemp_df = df.copy()\ntemp_df['age_dis'] = age_dis\n\nviz_age_dis_sal = temp_df.groupby(['age_dis', 'salary'])[['salary']].count().unstack()['salary']\n\nage_d = np.arange(len(viz_age_dis_sal.index))\nwidth = 0.3\n\nplt.figure(figsize = (10, 6))\n\nplt.barh(age_d - width\/2, viz_age_dis_sal.loc[:, '<=50K'], height = width, label = '<=50K', alpha = 0.75, edgecolor = 'black')\nplt.barh(age_d + width\/2, viz_age_dis_sal.loc[:, '>50K'], height = width, label = '>50K', alpha = 0.75, edgecolor = 'black')\n\nplt.title('Salary distribution in each age group\\n', fontsize = 25)\nplt.xlabel('\\nFrequency', fontsize = 20)\nplt.ylabel('Age group\\n', fontsize = 20)\n\nplt.legend(frameon = True, fontsize = 15, shadow = True)\nplt.yticks(np.arange(0,7), age_dis.unique().sort_values())\nplt.show()","069607ad":"occ_gr = df['occupation'].value_counts()\n\nplt.figure(figsize = (10, 10))\n\nplt.pie(occ_gr.values, labels = occ_gr.index, \n        wedgeprops = {'edgecolor': 'black'}, \n        textprops = {'fontsize': 15},\n        autopct = '%1.2f%%', \n        shadow = True, \n        explode = np.full(len(occ_gr), 0.05))\n\nplt.title('Popular Occupations\\n', fontsize = 35)\nplt.show()","471d8584":"wk_hr_dis = pd.cut(df['hours-per-week'], 10)\n\ntemp_wk = df.copy()\ntemp_wk['work_hr_dis'] = wk_hr_dis\n\nviz_wk_hr_sal = temp_wk.groupby(['work_hr_dis', 'salary'])[['salary']].count().unstack()['salary']\n\nplt.figure(figsize = (15, 7))\n\nhr_d = np.arange(len(wk_hr_dis.unique()))\nwidth = 0.4\n\nplt.bar(hr_d - width \/ 2, viz_wk_hr_sal['<=50K'], width = width, alpha = 0.75, edgecolor = 'black', label = '<=50k')\nplt.bar(hr_d + width \/ 2, viz_wk_hr_sal['>50K'], width = width, alpha = 0.75, edgecolor = 'black', label = '>50k')\n\nplt.title('Salary as compared to the working hours\\n', fontsize = 25)\nplt.xlabel('\\nWorking hour range', fontsize = 20)\nplt.ylabel('Frequency\\n', fontsize = 20)\n\nplt.xticks(hr_d, wk_hr_dis.unique().sort_values())\nplt.legend(frameon = True, shadow = True, fontsize = 15, loc = 'best')\nplt.show()","d29e4a17":"age_wk_hr = df.groupby('age')[['hours-per-week']].aggregate(np.mean)\nwk_hr_median = df['hours-per-week'].median()\n\nplt.figure(figsize = (8, 5))\n\nplt.plot(age_wk_hr.index, age_wk_hr.iloc[:, 0], linewidth = 5, color = 'steelblue', alpha = 0.5)\n\nplt.fill_between(age_wk_hr.index, age_wk_hr.iloc[:, 0], wk_hr_median, \n                 where = (age_wk_hr.iloc[:, 0] > wk_hr_median),\n                 interpolate= True, alpha = 0.25, color = 'green')\n\nplt.fill_between(age_wk_hr.index, age_wk_hr.iloc[:, 0], wk_hr_median, \n                 where = (age_wk_hr.iloc[:, 0] < wk_hr_median),\n                 interpolate= True, alpha = 0.25, color = 'red')\n\n\nplt.axhline(wk_hr_median, color = 'red', linewidth = 2, \n            label = 'Working hour Median')\n\nplt.title('Work Time according to Age\\n', fontsize = 25)\nplt.xlabel('\\nAge', fontsize = 20)\nplt.ylabel('Hours per Week\\n', fontsize = 20)\n\nplt.legend(loc = 'best', frameon = True, shadow = True, fontsize = 15)\nplt.show()","a9082ab9":"df_temp = df.copy()\n\ndf_temp['salary_>50K'] = df_temp['salary'].map({'<=50K': 0, '>50K': 1})\ndf_temp['salary_<=50K'] = df_temp['salary'].map({'<=50K': 1, '>50K': 0})\n\nage_sal = df_temp.groupby('age')[['salary_<=50K', 'salary_>50K']].mean()\n\nplt.figure(figsize = (10, 5))\n\nplt.plot(age_sal.index, age_sal['salary_<=50K'], label = 'Chance of earning less than 50K')\nplt.plot(age_sal.index, age_sal['salary_>50K'], label = 'Chance of earning more than 50K')\n\nplt.title('Chance of earning salary more than less than 50K\\n', fontsize = 25)\nplt.xlabel('\\nAge', fontsize = 20)\nplt.ylabel('Chance of earning salary\\n', fontsize = 20)\n\nplt.legend(loc = 'best', frameon = True, shadow = True, fontsize = 15)\nplt.show()","b74f2ead":"df_temp = df.copy()\n\ndf_temp['salary'] = df_temp['salary'].map({'<=50K': 0, '>50K': 1})\n\nage_wkhr_sal = df_temp.groupby(['age', 'hours-per-week'])[['salary']].aggregate(np.mean).unstack().fillna(0)['salary']\n\nplt.figure(figsize = (10, 7))\n\nplt.scatter(df_temp['age'], df_temp['hours-per-week'], c = df_temp['sex'].map({'Female': 1, 'Male': 2}),\n            s = df_temp['salary'].map({0: 1, 1: 2}) * 10, cmap = 'summer')\n\nplt.title('Age|Work Hour|Sex|Salary\\n', fontsize = 25)\nplt.xlabel('\\nAge', fontsize = 20)\nplt.ylabel('Work Hour\\n', fontsize = 20)\n    \nplt.colorbar()\nplt.show()","0003d674":"df_temp = df.copy()\n\ndf_temp['salary'] = df_temp['salary'].map({'<=50K': 0, '>50K': 1})\n\nms_sal = df_temp.groupby('marital-status')[['salary']].aggregate(np.mean).sort_values(by = 'salary', ascending = True)\n\nplt.figure(figsize = (10, 5))\n\nplt.barh(ms_sal.index, ms_sal.iloc[:, 0], color = '#FFBAA0', edgecolor = 'black', alpha = 0.75)\n\nplt.title('Income according to marital-status\\n', fontsize = 25)\nplt.xlabel('\\nPercent of people earning income more than 50K', fontsize = 20)\nplt.ylabel('Marital-Status\\n', fontsize = 20)\n\nplt.show()","4beeade4":"df_temp = df.copy()\n\ndf_temp['salary'] = df_temp['salary'].map({'<=50K': 0, '>50K': 1})\n\ngen_sal = df_temp.groupby('sex')[['salary']].mean()\n\nplt.figure(figsize = (8, 10))\n\nplt.bar(gen_sal.index, gen_sal['salary'], color = '#CCFFA0', alpha = 0.75)\n\nplt.title('Earning\\'s more than 50K\\n', fontsize = 25)\nplt.xlabel('\\nGender', fontsize = 20)\nplt.ylabel('Percentange\\n', fontsize = 20)\n\nplt.show()","f133fe7b":"df_temp = df.copy()\n\ndf_temp['salary'] = df['salary'].map({'<=50K': 0, '>50K': 1})\n\nocc_sal = df_temp.groupby('occupation')[['salary']].mean().sort_values(by = 'salary', ascending = True)\n\nplt.barh(occ_sal.index, occ_sal['salary'], alpha = 0.5, color = 'steelblue', edgecolor = 'black')\n\nplt.title('Salary according to Occupation\\n', fontsize = 25)\nplt.xlabel('\\nPerecent of people earning more than 50K', fontsize = 20)\nplt.ylabel('Occupation\\n', fontsize = 20)\n\nplt.show()","f4d2a344":"class Tuning_types():\n    grid = GridSearchCV\n    random = RandomizedSearchCV\n    \nclass Classifiers():\n\n    # I've manually set some parameters of log because of some errors in my system\n    # (you can try doing it without setting it manually)\n    log = LogisticRegression(solver = 'liblinear', max_iter = 1000) \n    dt = DecisionTreeClassifier()\n    rf = RandomForestClassifier()\n    svc = SVC()\n    gnb = GaussianNB()\n    knn = KNeighborsClassifier()\n    \n    \n\n\n\nclass Model(object):\n    \n    target_col = df.columns[-1]\n    test_size = 0.25\n    \n    def __init__(self, df):\n        \n        self.df = df\n        self.X = df.drop(Model.target_col, axis = 1).copy()\n        self.y = df[Model.target_col].copy()\n        \n        \n    def desc_cols(self):    \n        oe = []\n        ohe = []\n        \n        for i in self.df.columns[:-1]:\n            if self.df[i].dtype == object:\n                if self.df[i].nunique() <= 7:\n                    oe.append(i)\n\n                else:\n                    ohe.append(i)\n                    \n        return oe, ohe\n        \n        \n    @property\n    def train_test_set(self):\n  \n        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, stratify = self.y, \n                                                            test_size = Model.test_size, random_state = 0)\n        \n        return X_train, X_test, y_train, y_test\n    \n    def transform_columns(self, ss = None, oe = [], ohe = []):\n        \n        def ss_choice():\n            if ss == None:\n                return make_column_selector(dtype_include = ['int', 'float'])\n            else:\n                return ss\n            \n            \n        temp = ''\n        \n        if ((oe == []) & (ohe == [])):\n            oe, ohe = self.desc_cols()\n            \n        elif (ohe == []) & (oe != []):\n            temp, ohe = self.desc_cols()\n            \n        elif (oe == []) & (ohe != []):\n            oe, temp = self.desc_cols()\n        \n        trans_col = make_column_transformer((StandardScaler(), ss_choice()), \n                                           (OneHotEncoder(handle_unknown = 'ignore', sparse = False), ohe), \n                                           (OrdinalEncoder(), oe), \n                                           remainder  = 'passthrough')\n        \n        return trans_col\n    \n    @property\n    def target_col_trans_col(self):\n        \n        return make_column_transformer((LabelEncoder(), Model.target_col), \n                                       remainder = 'passthrough')\n\n\n\n\n\n\nclass Logistic_Regression(Model):\n    def __init__(self, df = df):\n        super().__init__(df)\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_log = Classifiers().log\n        self.tune = Tuning_types()\n        \n    def pipe_log(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_log)\n        \n        return pipe\n    \n    def tune_params_log(self,\n                    tuning = 'grid',\n                    penalty = ['l1', 'l2'],\n                    dual = [False],\n                    tol = [0.0001, 0.001, 0.01, 0.1], \n                    C = [1, 2, 4, 5],\n                    fit_intercept = [True],\n                    intercept_scaling = [1],\n                    class_weight = [None],\n                    random_state = [None],\n                    multi_class = ['auto'],\n                    verbose = [0],\n                    warm_start = [False],\n                    n_jobs = [None],\n                    l1_ratio = [None],\n                                   ):\n        \n        params = {\n            'logisticregression__penalty': penalty,\n            'logisticregression__dual': dual,\n            'logisticregression__tol': tol,\n            'logisticregression__C': C,\n            'logisticregression__fit_intercept': fit_intercept,\n            'logisticregression__intercept_scaling': intercept_scaling,\n            'logisticregression__class_weight': class_weight,\n            'logisticregression__random_state': random_state,\n            'logisticregression__multi_class': multi_class,\n            'logisticregression__verbose': verbose,\n            'logisticregression__warm_start': warm_start,\n            'logisticregression__n_jobs': n_jobs,\n            'logisticregression__l1_ratio': l1_ratio,\n        }\n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_log(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_log(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n\n            \n            \nclass Decision_Tree(Model):\n    def __init__(self, df = df):\n        super().__init__(df)\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_dt = Classifiers().dt\n        self.tune = Tuning_types()\n    \n    \n    def pipe_dt(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_dt)\n        \n        return pipe\n    \n    def tune_params_dt(self, \n                    tuning = 'grid', \n                    criterion = ['entropy', 'gini'],\n                    splitter = ['best'],\n                    max_depth = [None],\n                    min_samples_split = np.arange(2, 11, 2),\n                    min_samples_leaf = np.arange(1,6),\n                    min_weight_fraction_leaf = [0.0],\n                    max_features = [None],\n                    random_state = [None],\n                    max_leaf_nodes = [None],\n                    min_impurity_decrease = [0.0],\n                    min_impurity_split = [None],\n                    class_weight = [None],\n                    ccp_alpha = [0.0]):\n    \n        params = {\n            'decisiontreeclassifier__criterion': criterion,\n            'decisiontreeclassifier__splitter': splitter,\n            'decisiontreeclassifier__max_depth': max_depth,\n            'decisiontreeclassifier__min_samples_split': min_samples_split,\n            'decisiontreeclassifier__min_samples_leaf': min_samples_leaf,\n            'decisiontreeclassifier__min_weight_fraction_leaf': min_weight_fraction_leaf,\n            'decisiontreeclassifier__max_features': max_features,\n            'decisiontreeclassifier__random_state': random_state,\n            'decisiontreeclassifier__max_leaf_nodes': max_leaf_nodes,\n            'decisiontreeclassifier__min_impurity_decrease': min_impurity_decrease,\n            'decisiontreeclassifier__min_impurity_split': min_impurity_split,\n            'decisiontreeclassifier__class_weight': class_weight,\n            'decisiontreeclassifier__ccp_alpha': ccp_alpha,\n        }\n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_dt(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_dt(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n\n\n\n\n        \n        \nclass Random_Forest(Model):\n            \n\n    def __init__(self, df = df):\n        super().__init__(df)\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_rf = Classifiers().rf\n        self.tune = Tuning_types()\n    \n    \n    def pipe_rf(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_rf)\n        \n        return pipe\n    \n    def tune_params_rf(self, \n                    tuning = 'grid', \n                    n_estimators = [50, 100, 150],\n                    criterion = ['entropy', 'gini'],\n                    max_depth = [None],\n                    min_samples_split = np.arange(2, 11, 2),\n                    min_samples_leaf = [1],\n                    min_weight_fraction_leaf = [0.0],\n                    max_features = ['auto'],\n                    max_leaf_nodes = [None],\n                    min_impurity_decrease = [0.0],\n                    min_impurity_split = [None],\n                    bootstrap = [True],\n                    oob_score = [False],\n                    n_jobs = [None],\n                    random_state = [None],\n                    verbose = [0],\n                    warm_start = [False],\n                    class_weight = [None],\n                    ccp_alpha = [0.0],\n                    max_samples = [None]\n                   ):\n        \n        RandomForestClassifier()\n        params = {\n            'randomforestclassifier__n_estimators': n_estimators,\n            'randomforestclassifier__criterion': criterion,\n            'randomforestclassifier__max_depth': max_depth,\n            'randomforestclassifier__min_samples_split': min_samples_split,\n            'randomforestclassifier__min_samples_leaf': min_samples_leaf,\n            'randomforestclassifier__min_weight_fraction_leaf': min_weight_fraction_leaf,\n            'randomforestclassifier__max_features': max_features,\n            'randomforestclassifier__max_leaf_nodes': max_leaf_nodes,\n            'randomforestclassifier__min_impurity_decrease': min_impurity_decrease,\n            'randomforestclassifier__min_impurity_split': min_impurity_split,\n            'randomforestclassifier__bootstrap': bootstrap,\n            'randomforestclassifier__oob_score': oob_score,\n            'randomforestclassifier__n_jobs': n_jobs,\n            'randomforestclassifier__random_state': random_state,\n            'randomforestclassifier__verbose': verbose,\n            'randomforestclassifier__warm_start': warm_start,\n            'randomforestclassifier__class_weight': class_weight,\n            'randomforestclassifier__ccp_alpha': ccp_alpha,\n            'randomforestclassifier__max_samples': max_samples\n        }\n        \n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_rf(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_rf(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n        \n        \n        \n        \nclass Support_Vector_Machine(Model):\n    \n        \n    def __init__(self, df = df):\n        super().__init__(df)\n\n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_svc = Classifiers().svc\n        self.tune = Tuning_types()\n    \n    \n    def pipe_svc(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_svc)\n        \n        return pipe\n    \n    def tune_params_svc(self, \n                    tuning = 'grid', \n                    C = [1.0],\n                    kernel = ['linear', 'rbf', 'poly'],\n                    degree = [3],\n                    gamma = ['scale'],\n                    coef0 = [0.0],\n                    shrinking = [True],\n                    probability = [False],\n                    tol = [1e-3],\n                    cache_size = [200],\n                    class_weight = [None],\n                    verbose = [False],\n                    max_iter = [-1],\n                    decision_function_shape = ['ovr'],\n                    break_ties = [False],\n                    random_state = [None]\n                   ):\n        \n        params = {\n            'svc__C': C,\n            'svc__kernel': kernel,\n            'svc__degree': degree,\n            'svc__gamma': gamma,\n            'svc__coef0': coef0,\n            'svc__shrinking': shrinking,\n            'svc__probability': probability,\n            'svc__tol': tol,\n            'svc__cache_size': cache_size,\n            'svc__class_weight': class_weight,\n            'svc__verbose': verbose,\n            'svc__max_iter': max_iter,\n            'svc__decision_function_shape': decision_function_shape,\n            'svc__break_ties': break_ties,\n            'svc__random_state': random_state\n        }\n\n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_svc(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_svc(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n        \n        \n    \nclass Gaussian_Naive_Bayes(Model):\n    \n    def __init__(self, df = df):\n        super().__init__(df)\n\n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_gnb = Classifiers().gnb\n        self.tune = Tuning_types()\n    \n    \n    def pipe_gnb(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_gnb)\n        \n        return pipe\n    \n    def tune_params_gnb(self, \n                    tuning = 'grid', \n                    var_smoothing = [1e-10, 1e-09, 1e-8],\n                    priors = [None],\n                    ):\n        \n        params = {\n            'gaussiannb__var_smoothing': var_smoothing,\n            'gaussiannb__priors': priors\n        }\n    \n    \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_gnb(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_gnb(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n        \n        \n        \n        \nclass KNearest_Neighbor(Model):\n    \n    def __init__(self, df = df):\n        super().__init__(df)\n\n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_knn = Classifiers().knn\n        self.tune = Tuning_types()\n        \n    def pipe_knn(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_knn)\n        \n        return pipe\n    \n    \n    def tune_params_knn(self, \n                    tuning = 'grid', \n                    n_neighbors = np.arange(5, 31, 5),\n                    weights = ['uniform'],\n                    algorithm = ['auto'],\n                    leaf_size = [30],\n                    p = [1, 2],\n                    metric = ['minkowski'],\n                    metric_params = [None],\n                    n_jobs = [None]\n                   ):\n        \n        params = {\n            'kneighborsclassifier__n_neighbors': n_neighbors,\n            'kneighborsclassifier__weights': weights,\n            'kneighborsclassifier__algorithm': algorithm,\n            'kneighborsclassifier__leaf_size': leaf_size,\n            'kneighborsclassifier__p': p,\n            'kneighborsclassifier__metric': metric,\n            'kneighborsclassifier__metric_params': metric_params,\n            'kneighborsclassifier__n_jobs': n_jobs\n        }\n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_knn(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_knn(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n        \n        \n\nclass Evaluate(Logistic_Regression, \n               Decision_Tree, \n               Random_Forest, \n               Support_Vector_Machine, \n               Gaussian_Naive_Bayes, \n               KNearest_Neighbor):\n\n    def __init__(self, df):\n        super().__init__(df)\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n\n\n    def show(self, y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test):\n\n        accuracy_tr_nm = accuracy_score(self.y_train, y_pred_nm_train)\n        accuracy_ts_nm = accuracy_score(self.y_test, y_pred_nm_test)\n        \n        accuracy_tr_tm = accuracy_score(self.y_train, y_pred_tm_train)\n        accuracy_ts_tm = accuracy_score(self.y_test, y_pred_tm_test)\n        \n        acc = pd.DataFrame({'Training Set': [accuracy_tr_nm, accuracy_tr_tm], \n                      'Testing Set': [accuracy_ts_nm, accuracy_ts_tm]},\n                     index = ['Before parameter optimization', \n                              'After parameter optimization'])\n        \n        print('\/' * 100, '\\n')\n        print('Acccuracy Score:- ')\n        print('*' * 50)\n        print(acc)\n        print('*' * 50, '\\n')\n        \n        \n        # Confusion Matrix\n        confusion_matrix_tr_nm = confusion_matrix(self.y_train, y_pred_nm_train)\n        confusion_matrix_ts_nm = confusion_matrix(self.y_test, y_pred_nm_test)\n        \n        confusion_matrix_tr_tm = confusion_matrix(self.y_train, y_pred_tm_train)\n        confusion_matrix_ts_tm = confusion_matrix(self.y_test, y_pred_tm_test)\n        print('Confusion Matrix:- ')\n        print('*' * 50)\n        print('Confusion Matrix (Training Set):- ')\n        print('Before parameter tuning:- \\n', confusion_matrix_tr_nm)\n        print('After parameter tuning:- \\n', confusion_matrix_tr_tm)\n        print('\\n')\n        print('Confusion Matrix (Testing Set):- ')\n        print('Before parameter tuning:- \\n', confusion_matrix_ts_nm)\n        print('After parameter tuning:- \\n', confusion_matrix_ts_tm)\n        print('*' * 50, '\\n')\n        \n        # Classification Report\n        classification_report_tr_nm = classification_report(self.y_train, y_pred_nm_train)\n        classification_report_ts_nm = classification_report(self.y_test, y_pred_nm_test)\n        \n        classification_report_tr_tm = classification_report(self.y_train, y_pred_tm_train)\n        classification_report_ts_tm = classification_report(self.y_test, y_pred_tm_test)\n        \n        print('Classification Report:- ')\n        print('*' * 50)\n        print('Classification Report (Training Set):- ')\n        print('Before parameter tuning:- \\n', classification_report_tr_nm)\n        print('After parameter tuning:- \\n', classification_report_tr_tm)\n        \n        print('\\n')\n        print('Classification Report (Testing Set):- ')\n        print('Before parameter tuning:- \\n', classification_report_ts_nm)\n        print('After parameter tuning:- \\n', classification_report_ts_tm)\n        print('*' * 50, '\\n')\n        \n        print('\/' * 100)\n        \n\n\n    \n    @property\n    def evaluate_log(self):\n        \n        normal_model = self.pipe_log().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_log().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n\n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n        \n        \n    @property\n    def evaluate_dt(self):\n        \n        normal_model = self.pipe_dt().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_dt().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n\n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n    @property\n    def evaluate_rf(self):\n        \n        normal_model = self.pipe_rf().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_rf().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n\n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n        \n    @property\n    def evaluate_svc(self):\n        \n        normal_model = self.pipe_svc().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_svc().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n        \n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n        \n    @property\n    def evaluate_gnb(self):\n        \n        normal_model = self.pipe_gnb().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_gnb().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n        \n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n    \n    \n    @property\n    def evaluate_knn(self):\n        \n        normal_model = self.pipe_knn().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_knn().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n        \n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n    \n    # This method will evaluate all the model\n    def evaluate_all(self, *models):\n        \n        for i in models:\n            if i == 'logisticregression':\n                self.evaluate_log\n                \n            elif i == 'decisiontreeclassifier':\n                self.evaluate_dt\n                \n            elif i == 'randomforestclassifier':\n                self.evaluate_rf\n                \n            elif i == 'supportvectorclassifier':\n                self.evaluate_svc\n                \n            elif i == 'gaussiannaivebayes':\n                self.evaluate_gnb\n                \n            elif i == 'knearestneighbor':\n                self.evaluate_knn\n            \n","488f0b12":"ev = Evaluate(df)","053e1e54":"ev.evaluate_log","13937dbc":"ev.evaluate_dt","c3da41fe":"ev.evaluate_rf","0626e8cc":"ev.evaluate_svc","a2cf37c5":"ev.evaluate_gnb","ac312b90":"ev.evaluate_knn","7d4afaf3":"# You can change or set different hyperparameters (Its not necessary that mine is perfect.)\n# Changing hyperparameters may give better results\n\n# But I have kept it like this because of time and system barriers.","d5d322bb":"### EXPLORATORY DATA ANALYSIS","125c7fe7":"##### Popular Occupation","0ccd0537":"##### 1) Logistic Regression","4f315f39":"### IMPORTING MODULES","f1f00813":"##### Salary distribution in each age group","69b6bd8a":"##### 3) Random Forest ","857c1d6c":"### HANDLING MISSING VALUES (?)\nIn this dataset, we don't have missing values in the form of NaN values but in the form of a string values, i.e; **'?'**.","f69a6018":"##### 6) KNearest Neighbors","c4dc6e83":"##### 4) Support Vector Classifier","84bcf050":"### HANDLING IMBALANCED DATA","ed169186":"### LOADING THE DATA","b8c071d1":"##### 2) Decision Tree","c3f6e595":"##### 5) Gaussian Naive Bayes","9783615c":"##### Chance of earning salary more than less than 50K","88fbb04a":"###### Correlation between columns","fd4e5297":"##### Earning's more than 50K according to gender","bbf0d01d":"##### Salary as compared to the working hours","1fb107c0":"##### Work Time according to Age","725f34e2":"##### Age|Work Hour|Sex|Salary","a8ac4cf5":"### HANDLING OUTLIERS","7a64b3ac":"##### Salary according to Occupation","4881ad55":"##### Percent of people earning income more than 50K","4bc23f87":"### MODEL CREATION"}}