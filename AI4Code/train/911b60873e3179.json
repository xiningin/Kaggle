{"cell_type":{"9b864f35":"code","0687b1ce":"code","09d3ee5c":"code","e7caa5f8":"code","aed9c718":"code","37df3f03":"code","b3e33214":"code","eda42665":"code","c064559b":"code","03286100":"code","e831a1b0":"code","4dddc1bc":"code","9361988a":"code","c8a62199":"code","78b6ff06":"code","9a0652a9":"code","0990707b":"code","ded9a1ae":"code","3f9b8878":"code","3d7fe36a":"code","12137d76":"code","1a754efb":"code","3b814cbe":"code","f8b645c8":"code","c85c3b6d":"code","561a504c":"code","68c188f2":"code","e24a526e":"code","b2a713d0":"code","e8112338":"code","5af14ae1":"code","6af035e1":"code","00870c2f":"markdown"},"source":{"9b864f35":"# Load libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)\nsns.set_context(\"talk\")\n\nimport graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.model_selection import train_test_split # split  data into training and testing sets\nfrom sklearn.model_selection import cross_val_score # cross validation\nfrom sklearn.metrics import confusion_matrix, classification_report # creates a confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix # draws a confusion matrix\n","0687b1ce":"# Load data\n\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_train.columns= df_train.columns.str.lower()\n\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_test.columns= df_test.columns.str.lower()\n\ndf_submission_example = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","09d3ee5c":"df_train.head()","e7caa5f8":"df_test.head()","aed9c718":"# Handle missing values\n\nprint(f\"{df_train.shape}\\n{df_train.isnull().sum()\/len(df_train)*100}\\n\")\nprint(f\"{df_test.shape}\\n{df_test.isnull().sum()\/len(df_test)*100}\\n\")\n\ndf_train = df_train.drop(['passengerid','cabin','name','ticket','cabin'],axis=1)\ndf_test = df_test.drop(['cabin','name','ticket','cabin'],axis=1)\n\ndf_train.fillna(df_train.median(), inplace=True)\ndf_train['embarked'] = df_train['embarked'].fillna(df_train['embarked'].mode()[0])\ndf_train['sex'] = df_train['sex'].map({'male':0, 'female':1})\ndf_test.fillna(df_test.median(), inplace=True)\ndf_test['embarked'] = df_test['embarked'].fillna(df_test['embarked'].mode()[0])\ndf_test['sex'] = df_test['sex'].map({'male':0, 'female':1})\n\nprint(f\"{df_train.shape}\\n{df_train.isnull().sum()\/len(df_train)*100}\\n\")\nprint(f\"{df_test.shape}\\n{df_test.isnull().sum()\/len(df_test)*100}\\n\")","37df3f03":"# Quick and dirty (baseline)\n\nX = df_train.drop('survived',axis=1)\ny = df_train.survived\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nX_train_encoded = pd.get_dummies(X_train, columns=['pclass','embarked'])\nX_test_encoded = pd.get_dummies(X_test, columns=['pclass','embarked'])\n\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(X_train_encoded, y_train);\n\ndata = export_graphviz(dt,out_file=None,feature_names=X_train_encoded.columns,class_names=['Deceased', 'Survived'],\n                         filled=True, rounded=True,  \n                         special_characters=True)\ngraph = graphviz.Source(data)\ngraph","b3e33214":"fig, ax = plt.subplots(figsize=(7, 7))\nplt.grid(False)\nplot_confusion_matrix(dt, X_test_encoded, y_test, display_labels=['Deceased', 'Survived'], ax=ax);","eda42665":"y_pred = dt.predict(X_test_encoded)\nprint(classification_report(y_test, y_pred))","c064559b":"# Cost Complexity Pruning\n\npath = dt.cost_complexity_pruning_path(X_train_encoded, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.set_xlabel(\"ccp_alpha\")\nax.set_ylabel(\"impurities\")\nax.set_title(\"ccp_alpha vs impurities for training set\")\nax.plot(ccp_alphas, impurities, marker='o', label=\"train\", drawstyle=\"steps-post\")\n\nccp_alphas = ccp_alphas[:-1]\n\ndts = []\nfor ccp_alpha in ccp_alphas:\n    dt = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    dt.fit(X_train_encoded, y_train)\n    dts.append(dt)","03286100":"train_scores = [dt.score(X_train_encoded, y_train) for dt in dts]\ntest_scores = [dt.score(X_test_encoded, y_test) for dt in dts]\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()","e831a1b0":"dt = DecisionTreeClassifier(random_state=42, ccp_alpha=0.010)\nscores = cross_val_score(dt, X_train_encoded, y_train, cv=5)\ndf = pd.DataFrame(data={'tree': range(5), 'accuracy': scores})\n\ndf.plot(x='tree', y='accuracy', marker='o', linestyle='--');","4dddc1bc":"alpha_loop_values = []\n\nfor ccp_alpha in ccp_alphas:\n    dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    scores = cross_val_score(dt, X_train_encoded, y_train, cv=5)\n    alpha_loop_values.append([ccp_alpha, np.mean(scores), np.std(scores)])\n    \nalpha_results = pd.DataFrame(alpha_loop_values, \n                             columns=['alpha', 'mean_accuracy', 'std'])\n\nalpha_results.plot(x='alpha', \n                   y='mean_accuracy', \n                   yerr='std', \n                   marker='o', \n                   linestyle='--');","9361988a":"alpha_results[np.logical_and(alpha_results['alpha'] > 0.01,\n                             alpha_results['alpha'] < 0.012)]","c8a62199":"ideal_ccp_alpha = alpha_results[np.logical_and(alpha_results['alpha'] > 0.01,\n                                               alpha_results['alpha'] < 0.012)]['alpha']\nideal_ccp_alpha = float(ideal_ccp_alpha)\nideal_ccp_alpha","78b6ff06":"dt_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ideal_ccp_alpha)\ndt_pruned = dt_pruned.fit(X_train_encoded, y_train)","9a0652a9":"fig, ax = plt.subplots(figsize=(7, 7))\nplt.grid(False)\nplot_confusion_matrix(dt_pruned, \n                      X_test_encoded, \n                      y_test, \n                      display_labels=['Deceased', 'Survived'],\n                      ax=ax);","0990707b":"y_pred = dt_pruned.predict(X_test_encoded)\nprint(classification_report(y_test, y_pred))","ded9a1ae":"data = export_graphviz(dt_pruned,out_file=None,feature_names=X_train_encoded.columns,class_names=['Deceased', 'Survived'],\n                         filled=True, rounded=True,  \n                         special_characters=True)\ngraph = graphviz.Source(data)\ngraph","3f9b8878":"# Submission\n\nX_test_encoded_submission = pd.get_dummies(df_test, columns=['pclass','embarked'])\nX_test_encoded_submission = X_test_encoded_submission.drop('passengerid',axis=1)\n\ny_pred_submission = dt_pruned.predict(X_test_encoded_submission)\n\nmy_submission = pd.DataFrame({'PassengerId': df_test.passengerid, 'Survived': y_pred_submission})\nmy_submission.to_csv('submission_dt.csv', index=False)\nmy_submission.head()","3d7fe36a":"X_train_encoded.head()\n#X_test_encoded","12137d76":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train_encoded, y_train)","1a754efb":"fig, ax = plt.subplots(figsize=(7, 7))\nplt.grid(False)\nplot_confusion_matrix(clf, \n                      X_test_encoded, \n                      y_test, \n                      display_labels=['Deceased', 'Survived'],\n                      ax=ax);","3b814cbe":"y_pred = clf.predict(X_test_encoded)\nprint(classification_report(y_test, y_pred))","f8b645c8":"# Cost Complexity Pruning\n\npath = dt.cost_complexity_pruning_path(X_train_encoded, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.set_xlabel(\"ccp_alpha\")\nax.set_ylabel(\"impurities\")\nax.set_title(\"ccp_alpha vs impurities for training set\")\nax.plot(ccp_alphas, impurities, marker='o', label=\"train\", drawstyle=\"steps-post\")\n\nccp_alphas = ccp_alphas[:-1]\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = RandomForestClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    clf.fit(X_train_encoded, y_train)\n    clfs.append(clf)","c85c3b6d":"train_scores = [clf.score(X_train_encoded, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test_encoded, y_test) for clf in clfs]\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()","561a504c":"clf = RandomForestClassifier(random_state=42, ccp_alpha=0.010)\nscores = cross_val_score(clf, X_train_encoded, y_train, cv=5)\ndf = pd.DataFrame(data={'tree': range(5), 'accuracy': scores})\n\ndf.plot(x='tree', y='accuracy', marker='o', linestyle='--');","68c188f2":"alpha_loop_values = []\n\nfor ccp_alpha in ccp_alphas:\n    clf = RandomForestClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    scores = cross_val_score(clf, X_train_encoded, y_train, cv=5)\n    alpha_loop_values.append([ccp_alpha, np.mean(scores), np.std(scores)])\n    \nalpha_results = pd.DataFrame(alpha_loop_values, \n                             columns=['alpha', 'mean_accuracy', 'std'])\n\nalpha_results.plot(x='alpha', \n                   y='mean_accuracy', \n                   yerr='std', \n                   marker='o', \n                   linestyle='--',\n                  figsize=(25,5));","e24a526e":"alpha_results[np.logical_and(alpha_results['alpha'] > 0.002,\n                             alpha_results['alpha'] < 0.003)].sort_values(by='mean_accuracy')","b2a713d0":"clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=0.002598)\nclf_pruned = clf_pruned.fit(X_train_encoded, y_train)","e8112338":"fig, ax = plt.subplots(figsize=(7, 7))\nplt.grid(False)\nplot_confusion_matrix(clf_pruned, \n                      X_test_encoded, \n                      y_test, \n                      display_labels=['Deceased', 'Survived'],\n                      ax=ax);","5af14ae1":"y_pred = clf_pruned.predict(X_test_encoded)\nprint(classification_report(y_test, y_pred))","6af035e1":"# Submission\n\ny_pred_submission_clf = clf_pruned.predict(X_test_encoded_submission)\n\nmy_submission = pd.DataFrame({'PassengerId': df_test.passengerid, 'Survived': y_pred_submission_clf})\nmy_submission.to_csv('submission_clf.csv', index=False)\nmy_submission.head(20)","00870c2f":"Create a model that predicts which passengers survived the \"unsinkable\" Titanic shipwreck. \nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.  \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc). "}}