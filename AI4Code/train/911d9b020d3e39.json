{"cell_type":{"b86539dd":"code","003a8aae":"code","31582d2d":"code","282e2b85":"code","0b1aac1e":"code","6870701f":"code","7f0c5731":"code","d94621de":"code","05ccfdb4":"code","0fac8ec9":"code","c865ed49":"code","d76147df":"code","10a30bf2":"code","d1bda94c":"code","de9b0c21":"code","871ea4c5":"code","cba285f0":"code","50d155e8":"code","b6e2085b":"code","a231ea8e":"code","9b4603d2":"code","a598a7b9":"code","afed72d2":"code","356460ee":"code","5f2a4cbe":"code","861c7234":"code","22a8c86e":"code","a2e29e10":"code","b70be17c":"code","669bccb3":"code","de299845":"code","c185074a":"markdown","3ce23ef5":"markdown","16472fd4":"markdown","4cad8afa":"markdown","48ed649f":"markdown","e7d91b19":"markdown","5b06cf79":"markdown","aa599c00":"markdown","831f78ce":"markdown","2b595f50":"markdown","9dcacb10":"markdown","879ddac6":"markdown","87ba76e7":"markdown","f6fc0ce9":"markdown","537be701":"markdown","3304c9f6":"markdown","10c1b99e":"markdown","defbc95f":"markdown","f84263b2":"markdown","9fb80ea9":"markdown","7676e477":"markdown","52759d27":"markdown","5f655046":"markdown","6c7aab25":"markdown","89a527e4":"markdown","98eb505e":"markdown","76c5e73a":"markdown"},"source":{"b86539dd":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","003a8aae":"pip install skompiler","31582d2d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LogisticRegression,LogisticRegressionCV\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV\nfrom sklearn.preprocessing import scale\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nimport time\nfrom skompiler import skompile","282e2b85":"pd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)","0b1aac1e":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","6870701f":"df.head()","7f0c5731":"df.shape","d94621de":"df.describe()","05ccfdb4":"X = df.drop(\"Outcome\",axis=1)\ny= df[\"Outcome\"] #We will predict Outcome(diabetes) ","0fac8ec9":"X.shape","c865ed49":"y.shape","d76147df":"X_train = X.iloc[:600]\nX_test = X.iloc[600:]\ny_train = y[:600]\ny_test = y[600:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","10a30bf2":"logistic_regression = LogisticRegression(random_state=0,solver=\"liblinear\").fit(X_train,y_train)","d1bda94c":"logistic_regression","de9b0c21":"logistic_regression.intercept_","871ea4c5":"logistic_regression.coef_","cba285f0":"lr_statsmodel = sm.Logit(y_train,X_train).fit()","50d155e8":"lr_statsmodel.summary()","b6e2085b":"logistic_regression","a231ea8e":"y_pred = logistic_regression.predict(X_test)","9b4603d2":"cm = confusion_matrix(y_test,y_pred)","a598a7b9":"cm","afed72d2":"print(\"Our Accuracy is: \", (96+34)\/(96+12+26+34))","356460ee":"accuracy_score(y_test,y_pred)","5f2a4cbe":"print(classification_report(y_test,y_pred))","861c7234":"accuracies= cross_val_score(estimator=logistic_regression,\n                            X=X_train,y=y_train,\n                            cv=10)\nprint(\"Average Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standart Deviation of Accuracies: {:.2f} %\".format(accuracies.std()*100))","22a8c86e":"logistic_regression.predict(X_test)[:10]","a2e29e10":"results =pd.DataFrame(logistic_regression.predict_proba(X_test)[:10],\n             columns=[\"Possibility of 0\",\"Possibility of 1\"])\n\nresults[\"Class\"]=[1 if i>0.5 else 0 for i in results[\"Possibility of 1\"]]","b70be17c":"results","669bccb3":"logistic_regression = LogisticRegression(random_state=0,solver=\"liblinear\").fit(X,y)","de299845":"logistic_regression_roc = roc_auc_score(y,logistic_regression.predict(X))\n\nfp,tp,trshld = roc_curve(y,logistic_regression.predict_proba(X)[:,1])\nplt.figure()\nplt.plot(fp,tp,label=\"Area Under Curve(AUC)\" %logistic_regression_roc)\nplt.plot([0,1],[0,1],\"r--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic(ROC)\")\nplt.show()","c185074a":"### Model Tuning & Validation","3ce23ef5":"### Model","16472fd4":"## Content\n","4cad8afa":"## Logistic Regression","48ed649f":"**Most Important Parameters of Logistic Regression**","e7d91b19":"As you can see *predict()* function gives us directly classes. If we want to get probabilites of each classes, we can use **predict_proba()** function. We can also give manual treshold to classify.","5b06cf79":"  Logistic Regression is so similar to linear Regression because both the models use linear equations for predictions. But the main difference is that Logistic regression is used to handle the classification problems wheras Linear Regression is used to handle regression problems. For example, we can try to predict the price of a house by using linear regression because we are predicting a price, a continuous number. But if we want to understand if an email is spam or not, we need to use Logistic Regression because we try to solve a binary classification problem (problem with two class values, *Spam* or *Not Spam* in this example).  \n\n**Logistic(Sigmoid - \u03c3) Function: The core of the method**\n\n  The logistic function, also called the sigmoid function was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It\u2019s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n  \n  The formula of sigmoid function as below:\n  \n  ![image.png](attachment:image.png)\n  \n  Photo is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fwww.oreilly.com%2Flibrary%2Fview%2Fhands-on-automated-machine%2F9781788629898%2Faa703b68-f6df-40fd-b196-cdbef2b0c7db.xhtml&psig=AOvVaw3I4aV7wKnwIREXWNOOK5Ze&ust=1627894816803000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCIjR1v66j_ICFQAAAAAdAAAAABAD)","aa599c00":"Here e is base of the natural logarithms (Euler\u2019s number) and value is the actual numerical value that we want to transform. \n\n  Input values (x) are combined linearly using weights or coefficient values to predict an output value y that equals to \u03c3(x).","831f78ce":"**Logistic Regression Predicts Probabilities**\n\nLogistic regression models the probability of the default class (e.g. the first class).\n\nFor example, if we are modeling people\u2019s gender as male or female from their weight, then the first class could be female and the logistic regression model could be written as the probability of female given a person\u2019s weight, or more formally:\n\n*P(gender=female|weight)*\n\nWe need to be aware of that the probability prediction must be transformed into a binary values (0 or 1) in order to actually make a probability prediction. \n\nLogistic regression is a linear method, but the predictions are transformed using the *logistic(sigmoid) function*. \n\n","2b595f50":"Because we are doing a classification case, we will create a **confusion matrix** in order to evaluate out model.","9dcacb10":"### Theory","879ddac6":"**Classification with Logistic Regression**\n\n- Logistic Regression (Theory - Model- Tuning and Validation)","87ba76e7":"**Maximum-likelihood Estimation**\n\nThe coefficients (Beta values b) of the logistic regression algorithm must be estimated from our training data. This can be done using maximum-likelihood estimation.\n\nMaximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms, although it does make assumptions about the distribution of our data. \n\nThe best coefficients would result in a model that would predict a value very close to 1 (e.g. female) for the default class and a value very close to 0 (e.g. male) for the other class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that minimize the error in the probabilities predicted by the model to those in the data.","f6fc0ce9":"For a real world example, we will work with **Pima Indians Diabetes Database** data set by UCI Machine Learning.\n\nIt can be downloaded [here](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database).\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nWe will try to predict whether the patient has diabetes or not.","537be701":"- Penalty: Used to specify the norm used in the penalization. The \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers support only l2 penalties. \u2018elasticnet\u2019 is only supported by the \u2018saga\u2019 solver. If \u2018none\u2019 (not supported by the liblinear solver), no regularization is applied. default=\u2019l2\u2019\n\n- C: Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n\n- Class_weight: Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n\n- random_state: Used when solver == \u2018sag\u2019, \u2018saga\u2019 or \u2018liblinear\u2019 to shuffle the data.","3304c9f6":"Now we will try to tune our model by using **K-Fold Cross Validation**.","10c1b99e":"We will also see roc auc score.","defbc95f":"- **true positive**: for correctly predicted event values. -96\n- **false positive**: for incorrectly predicted event values. - 34\n- **true negative**: for correctly predicted no-event values. -12\n- **false negative**: for incorrectly predicted no-event values.-26\n","f84263b2":"## Resources","9fb80ea9":"We will also create a model with **statsmodel** to get more information.","7676e477":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n- [**Logistic Regression by Statquest**](https:\/\/www.youtube.com\/watch?v=yIYKR4sgzI8&ab_channel=StatQuestwithJoshStarmer)\n\n- [**The Ultimate Guide to Regression & Classification**](https:\/\/www.superdatascience.com\/blogs\/the-ultimate-guide-to-regression-classification)\n\n- [**Logistic Regression for Machine Learning**](https:\/\/machinelearningmastery.com\/logistic-regression-for-machine-learning\/)\n\n- [**Logistic Regression by Stanford University**](https:\/\/web.stanford.edu\/class\/stats202\/notes\/Classification\/Logistic-regression.html)\n\n- [**What is a Confusion Matrix in Machine Learning?**](https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/)","52759d27":"Now we're going to split our dataset to train and test set. We will choose almost 20% of dataset as test size.","5f655046":"## Importing Libraries","6c7aab25":"In order to see all rows and columns, we will increase max display numbers of dataframe.","89a527e4":"### Prediction","98eb505e":"The function seems as below:\n\n![image.png](attachment:image.png)\n\nPhoto is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fdataaspirant.com%2F5-sigmoid-function%2F&psig=AOvVaw3I4aV7wKnwIREXWNOOK5Ze&ust=1627894816803000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCIjR1v66j_ICFQAAAAAdAAAAABAV)","76c5e73a":"**Created by Berkay Alan**\n\n**Classification with Logistic Regression**\n\n**5 August 2021**\n\n**For more Tutorial:** https:\/\/github.com\/berkayalan"}}