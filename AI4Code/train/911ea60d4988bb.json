{"cell_type":{"df2a75ba":"code","fa0fadba":"code","3972302c":"code","377d113d":"code","19353d33":"code","d8d3815c":"code","3a9beef5":"code","8481e23c":"code","7f73bd93":"code","08d99864":"code","e78bedc8":"code","4d4dea2d":"code","1b5df0d8":"code","6c4dce55":"code","20b368b9":"code","1afc6bd7":"code","8ddfe50a":"code","5d3abae6":"code","5c30a6e8":"code","aa59081e":"code","e0e38161":"code","31a71ad2":"code","490b46f9":"code","b58f192b":"code","01a4a7b0":"code","a8bab8da":"code","db903a0b":"code","7f2db8d2":"code","913e735a":"code","de11f960":"code","75ffc5dd":"code","0777fd12":"code","cd401038":"code","d571aafa":"code","2deea6a3":"code","89bd7d57":"code","5a84a61d":"code","ea31181a":"code","9f9583ef":"code","f7c20272":"code","08321d30":"code","4917aba4":"code","8e276802":"code","dc3a5538":"code","ec91959d":"code","95ab757e":"code","05b590db":"code","8e041bf3":"code","b4b0ffb5":"code","0007c76d":"code","e04a09da":"code","05529d76":"code","80ef63f7":"code","8d3a7ebc":"markdown","0eca0709":"markdown","ab52f365":"markdown","b2e00788":"markdown","2cf8fa0d":"markdown","53c275e2":"markdown","9efd09e6":"markdown","d39dcecd":"markdown","43d88fd4":"markdown","16fb7164":"markdown","7f7eda39":"markdown","ed8c24ae":"markdown","d56acfb6":"markdown","8580dc8f":"markdown","56034f5a":"markdown","4bd178aa":"markdown","99478b85":"markdown","d21eda04":"markdown","4e0f2458":"markdown","c13a5c85":"markdown","1e5d66c9":"markdown","27b3bd5d":"markdown","f2436cb3":"markdown","bdc5c6eb":"markdown","820307b3":"markdown","6c7acad1":"markdown","dc0d94d0":"markdown","9d973465":"markdown","40256af8":"markdown","e068dddd":"markdown","8ffe94eb":"markdown","96b84430":"markdown","77c7c9e6":"markdown","fed8a508":"markdown","68eaf748":"markdown","0b0ec8cb":"markdown","ad7e7035":"markdown","612f5891":"markdown","c1a5d10f":"markdown","b2275680":"markdown","838873ae":"markdown","3c2ed3f7":"markdown","e14b9f30":"markdown"},"source":{"df2a75ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa0fadba":"import pandas as pd\nimport nltk, re, multiprocessing\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nimport spacy\nfrom zipfile import ZipFile\nimport os\nfrom wordcloud import WordCloud\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models.fasttext import FastText\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE","3972302c":"data = pd.read_csv(\"\/kaggle\/input\/medium-articles\/articles.csv\")\ndata.head()","377d113d":"data.shape","19353d33":"data[\"author\"].nunique()","d8d3815c":"cloud=WordCloud(colormap=\"Reds\",width=600,height=400).generate(str(data[\"text\"]))\nfig=plt.figure(figsize=(12,12))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')\nplt.title(\"Word Cloud on uncleaned text\", size = 20)","3a9beef5":"pd.set_option('display.max_colwidth', None)\ndata[\"text\"].head()","8481e23c":"def cleaned_data_3(text):\n    cleaned_txt = re.sub(\"AI\",\"Artificial Intelligence\", text)\n    return cleaned_txt\ndata[\"text\"] = data[\"text\"].apply(cleaned_data_3)","7f73bd93":"pd.set_option('display.max_colwidth', None)\ndata[\"title\"].head()","08d99864":"def cleaned_text(text_data):\n    clean = text_data.lower()\n    clean = re.sub(\"\\n\",\" \",clean)\n    clean = re.sub(\"http\\S+\",\" \",clean)\n    clean = re.sub(\"www\\S+\",\" \",clean)\n    #clean = re.sub(r\"[,-.:;]\",\" \", clean)\n    clean=re.sub(\"[^a-z]\",\" \",clean)\n    clean=clean.lstrip()\n    clean=re.sub(\"\\s{2,}\",\" \",clean)\n    return clean\ndata[\"cleaned_data\"] = data[\"text\"].apply(cleaned_text)","e78bedc8":"data[\"cleaned_data\"].head()","4d4dea2d":"nlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(str(data[\"cleaned_data\"]))\nfor token in doc: \n  print(token, token.pos_)\n  #print(token, token.lemma_)","1b5df0d8":"list = [token.pos_ for token in doc]\nlist = pd.Series(list)\nplt.style.use(\"dark_background\")\nlist.value_counts().plot(figsize = (12,6), kind = \"bar\", color = \"r\")\nplt.title(\"Frequency of POS tagger\", size = 22)\nplt.xlabel(\"POS tagger\", size = 18)","6c4dce55":"data['Number_of_words'] = data['cleaned_data'].apply(lambda x:len(str(x).split()))","20b368b9":"plt.style.use('dark_background')\nplt.figure(figsize=(12,6))\nsns.distplot(data['Number_of_words'],kde = False,color=\"springgreen\", bins = 100)\nplt.title(\"Frequency distribution of number of words from each text\", size = 20)\nplt.xlabel(\"Number of words\", size = 18)","1afc6bd7":"len(data[data[\"Number_of_words\"]>5000])","8ddfe50a":"cloud=WordCloud(colormap=\"spring\",width=600,height=400).generate(str(data[\"cleaned_data\"]))\nfig=plt.figure(figsize=(12,12))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')\nplt.title(\"Word Cloud on cleaned text\", size = 22)","5d3abae6":"stop=stopwords.words('english')\nstop.extend([\"make\",\"get\",\"also\",\"use\",\"using\",\"used\",\"even\",\"though\",\"could\",\"would\",\"us\",\"much\",\"uses\",\"makes\",\"part\"])\ndata[\"stopwords_rem\"]=data[\"cleaned_data\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","5c30a6e8":"data[\"stopwords_rem\"].head()","aa59081e":"def cleaned_text_2(text):\n    #cleaned = text_data.lower()\n    cleaned = re.sub(\"networks\",\"network\",text)\n    cleaned = re.sub(\"weeks\",\"week\",cleaned)\n    cleaned = re.sub(\"boxes\",\"box\",cleaned)\n    cleaned = re.sub(\"algorithms\",\"algorithm\",cleaned)\n    cleaned = re.sub(\"functions\",\"function\",cleaned)\n    cleaned = re.sub(\"nets\",\"network\",cleaned)\n    cleaned = re.sub(\"proposals\",\"proposal\",cleaned)\n    cleaned = re.sub(\"imitative ai\",\"imitative artificial intelligence\",cleaned)\n    cleaned = re.sub(\"deep rl\",\"deep reinforcement learning\",cleaned)\n    cleaned = re.sub(\"word vec\",\"word2vec\",cleaned)#crartificial\n    cleaned = re.sub(\"crartificial intelligenceg\",\"artificial intelligence\",cleaned)\n    cleaned = re.sub(\"nlp\",\"natural language processing\",cleaned)\n    cleaned = re.sub(\"openartificial\",\"open artificial\",cleaned)\n    cleaned = re.sub(\"intelligences\",\"intelligence\",cleaned)\n    return cleaned\ndata[\"stopwords_rem\"] = data[\"stopwords_rem\"].apply(cleaned_text_2)","e0e38161":"data[\"stopwords_rem\"] = data[\"stopwords_rem\"].apply(lambda x: ' '.join([word for word in x.split() if len(word)>1]))","31a71ad2":"plt.style.use('ggplot')\nplt.figure(figsize=(14,6))\nfreq=pd.Series(\" \".join(data[\"stopwords_rem\"]).split()).value_counts()[:30]\nfreq.plot(kind=\"bar\", color = \"orangered\")\nplt.title(\"30 most frequent words\",size=20)","490b46f9":"data['Number_of_words_after_stpwrd'] = data['stopwords_rem'].apply(lambda x:len(str(x).split()))","b58f192b":"plt.style.use('dark_background')\nplt.figure(figsize=(12,6))\nsns.distplot(data['Number_of_words_after_stpwrd'],kde = False,color=\"yellow\", bins = 100)\nplt.title(\"Histogram showing number of words from each doc after stop removal\", size = 18)\nplt.xlabel(\"Number of words\", size = 18)","01a4a7b0":"tokens = data[\"stopwords_rem\"].apply(lambda x: nltk.word_tokenize(x))","a8bab8da":"#phrases = Phrases(tokens, min_count = 20, threshold = 50, delimiter=b'_')\n#phrases = Phrases(tokens, min_count = 25, threshold = 40, delimiter=b'_')\nphrases = Phrases(tokens, min_count = 27, threshold = 41.5, delimiter=b'_')\nbigram = Phraser(phrases)","db903a0b":"bigram.phrasegrams","7f2db8d2":"#trigram = Phrases(bigram[tokens], min_count=22, delimiter=b' ', threshold = 50)\n#trigram = Phrases(bigram[tokens], min_count = 15, threshold = 25, delimiter=b'_')\ntrigram = Phrases(bigram[tokens], min_count = 18, threshold = 26, delimiter=b'_')","913e735a":"trigram_final = Phraser(trigram)\ntrigram_final.phrasegrams","de11f960":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","75ffc5dd":"with ZipFile('glove.6B.zip', 'r') as zip: \n    # printing all the contents of the zip file \n    zip.printdir()\n    zip.extractall()","0777fd12":"t = Tokenizer()\nt.fit_on_texts(data[\"stopwords_rem\"])\nencoded_docs = t.texts_to_sequences(data[\"stopwords_rem\"])","cd401038":"print(encoded_docs)","d571aafa":"embeddings_dict = {}\nwith open(\"glove.6B.100d.txt\", 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], \"float32\")\n        embeddings_dict[word] = vector","2deea6a3":"embeddings_dict[\"deep\"]","89bd7d57":"embeddings_dict[\"random\"]","5a84a61d":"embeddings_dict[\"basic\"]","ea31181a":"vocab_size = len(t.word_index) + 1\nprint(vocab_size)","9f9583ef":"t.word_index","f7c20272":"embedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","08321d30":"embedding_matrix[2]   # Here 2 is the word index, and from above we can check 2 is for the word \"data\"","4917aba4":"embeddings_dict[\"data\"]","8e276802":"def cosine_similarity(A, B):\n \n    dot = np.dot(A,B)\n    norma = np.sqrt(np.dot(A,A))\n    normb = np.sqrt(np.dot(B,B))\n    cos = dot \/ (norma*normb)\n    return cos","dc3a5538":"cosine_similarity(embedding_matrix[1],embedding_matrix[4])","ec91959d":"cosine_similarity(embeddings_dict[\"loss\"],embeddings_dict[\"function\"])","95ab757e":"multiprocessing.cpu_count()","05b590db":"tokens_3 = [word for word in trigram_final[tokens]] ","8e041bf3":"fastext_mdl = FastText(tokens_3,\n                      window = 5,\n                      size = 100,\n                      alpha = 0.01,\n                      min_alpha = 0.0005,\n                      workers = multiprocessing.cpu_count(),\n                      seed = 42)","b4b0ffb5":"len(fastext_mdl.wv.vocab)","0007c76d":"print(fastext_mdl.wv['artificial_intelligence'])","e04a09da":"print(fastext_mdl.wv.similarity(w1='artificial_intelligence', w2='machine_learning'))","05529d76":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity = 30, n_components=2, init='pca', n_iter=2000, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(15, 13)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        #plt.annotate(labels[i],\n                     #xy=(x[i], y[i]),\n                     #xytext=(5, 2),\n                     #textcoords='offset points',\n                     #ha='right',\n                     #va='bottom')\n    plt.show()","80ef63f7":"sns.set_style('whitegrid')\n#plt.style.use(\"ggplot\")\ntsne_plot(fastext_mdl)","8d3a7ebc":"So we can see from the above output that most of the bi-grams and tri-grams are genuine, only few in the mids are some of the non-essentials, we need to do a little more cleaning to remove them. Also one thing, we are creating bi-grams and tri-grams for the \"fasttext\" trainable embedding model, we will feed these to the fasttext model from gensim, if we would have been using the pre-trained fasttext embedding, then we couldn't feed the bigrams and trigrams, because pre-trained embeddings are trained on unigrams\/single word, so embeddings for bigrams or trigrams is not available in the pre-trained models.","0eca0709":"# Tri-gram","ab52f365":"# Medium Article Text Analysis","b2e00788":"Now let's define a function that will give us the cosine similarity. Also it's fromula is pretty simple \n![image.png](attachment:image.png)","2cf8fa0d":"# Bi-Gram ","53c275e2":"Now let's a create wordcloud on the cleaned text data.","9efd09e6":"Now let's create a bigram model using Gensim library. We need to convert the input in token form, as Gensim's Phrases takes it in that form only. Then we will try different threshold value before finally settling to one value, so that the important we don't remove important bigrams from our document, and also we need to take care that insignificant bigrams are not created in our document, so we need to take care and maintain a balance. ","d39dcecd":"We will first download the model. We have to extract the text file from it.","43d88fd4":"Now let's see the vector representation of the word \"artificial\". It's dimension will be 200. ","16fb7164":"Now using the below piece of code, we will convert our data into sequence of integers.","7f7eda39":"Now let's remove single characters from our text data.","ed8c24ae":"# TSNE plot for the word-embeddings","d56acfb6":"Let's see the similarity between the words \"**learning**\" and \"**intelligence**\", and their indexes are 1 and 4 respectively. And the theory is if the similarity score is closer to 1, then the words are similar, cause as we know cos 0 degree is 1, so it works on that principle.","8580dc8f":"So now the format of the text file is \"a word is given, then it's corresponding vectors are given, then again, next word and it's corresponding vectors.\" So using below piece of code, we will try to get the vectors of all the words that are there in the vocabulary of the file that we have downloaded. Also we have used here the file with vectors of dimension 100, as can be seen above, we had 4 options, words with vectors 50, 100, 200 and 300, I have used vectors of size 100. ","56034f5a":"Now let's check for some random words, as Standford here in this file used all the wikipedia data, so they must have a rich vocabulary, so let's try to get vectors for some random words.","4bd178aa":"Now we will check the vocabulary size of our text data.","99478b85":"So from the above plot, we can see that most of the POS taggers are nouns. Now let's see word distribution ","d21eda04":"So in our dataset, there are 182 total authors, who have written articles. Now before jumping straight to text processing, we will form a word cloud to see what are there in our text.","4e0f2458":"# Basic Text Pre-processing","c13a5c85":"Bigrams are created based on scores, so only those bigrams will be created which will have score more than the threshold that we set for the model.","1e5d66c9":"# GloVe Pre-trained word-embedding","27b3bd5d":"So we have 4 multi-core CPUs available, which we will use as workers.","f2436cb3":"# POS Tagging\n\n\nNow let's do POS tagging using Spacy.","bdc5c6eb":"Here our aim will be mainly on text analysis.","820307b3":"So here the similarity score between **learning** and **intelligence** is 0.40877, so the angle is somewhere in between 60 - 75 degree, so they are not quite similar. Now let's try to check the similarity of some other words.","6c7acad1":"So what are the parameters that I used in the FastText word-embedding model, they are:-\n1. min_count -> It filters out all the word, which has frequency below the limit that we have chosen.\n2. window -> It is, while the model trains for creating embedding for a particular word, it uses 5 words from the left, to the word it is creating embedding for, and 5 words to the right, as context.\n3. size -> It is the dimension of the embedding.\n4. alpha -> It is initial learning rate.\n5. min_alpha -> The learning rate in these pre-trained algorithms are kept flexible, so later in the stage, when the model needs to converge and it is very close to the convergence point, then the model lowers the learning rate to minimum value, and converges.\n6. workers -> It is used basically so as to speed up the process, as you know this kinda pre-trained models use neural network architecture, so it is a bit time-taking, so using multiple CPUs may reduce the training time.\n7. seed -> Seed is used so that the next time when you run the model, the output doesn't change, try it yorself, don't use seed in your model, the next time when you run the model, the output will be different.","dc0d94d0":"Using Keras's tokenizer, we can index the words in our vocabulary, which will be used to extract embeddings for the words in our vocabulary, from the pre-trained embeddings.","9d973465":"Here, I am not using pre-trained FastText embeddings, here I am using gensim library which provides us the support to train the embeddings from our own data, but very soon I will try the pre-trained version of FastText. ","40256af8":"Now let's try to see the similarity score of \"machine learning\" and \"artificial intelligence\". It will be in the range of 0 - 1. The more closer it will be towards 1, the more similar they are.","e068dddd":"Then we will create a dictionary, which contains the words as the key from the pre-trained Glove word-embeddings, and the embeddings as values.","8ffe94eb":"Now we will initialize or create arrays for all the members present in our vocabulary with zero, and then we will assign the embeddings from the Glove dictionary to the words present in our vocabulary. You can understand it from the below code.","96b84430":"![MEDIUM%20ARTICLE%20LOGO.png](attachment:MEDIUM%20ARTICLE%20LOGO.png)","77c7c9e6":"# Word-Embedding using Fasttext","fed8a508":"Also let's see for title column. But here we will work mainly with text column.","68eaf748":"**Now the thing that is remaining is I need to create a documentary similarity using both Fasttext and Glove, and see how are they performing differently. Also further, I need to clean the data a little further.**","0b0ec8cb":"Now let's see the vocabulary size of our data.","ad7e7035":"I am feeling like pre-trained Glove embeddings from all the wikipedia data is not so good for my data, we need to look at some other alternative.","612f5891":"A little more text cleaning, and I am not directly using lemmatization here because of a reason, as here in our documents, words like \"Machine Learning\", \"Deep Learning\", etc exists, so I can't use lemmatization here.","c1a5d10f":"Also let's see the top 30 most frequent words in our text.","b2275680":"Now let's confirm whether word_index \"2\" is for the word \"data\" or not by directly matching this with the word-embedding from the Glove pre-trained embedding.","838873ae":"Before creating bigram, we should remove stop words, so that bigram formation happens only on real ones.","3c2ed3f7":"Now let's see how many documents belong to word size of more than 5000.","e14b9f30":"Now let's try to see how word counts have changed for documents after removing stopwords"}}