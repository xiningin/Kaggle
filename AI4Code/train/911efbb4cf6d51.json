{"cell_type":{"63e6a52e":"code","4366b129":"code","52f739f8":"code","59d47ee4":"code","66cd7739":"code","3bb2bc7a":"code","c75d266a":"code","c25adffe":"code","e8672a1b":"code","1ec205ee":"code","c8d6af3b":"code","a57750cf":"code","2ab700e5":"code","6b5e9108":"code","d5522a6a":"code","6e48fb03":"code","ecc074c7":"code","1b294d57":"code","ddfecbbd":"code","1957dcc2":"code","c0f85c2e":"code","0acfe90d":"code","3e70ef1c":"code","c6353721":"code","7ac0c50e":"code","002e8348":"code","47662bdc":"code","29f38a6b":"code","ba61683f":"code","a59ecb22":"code","3f5fe0a5":"code","120c34db":"code","072a975a":"markdown","703c74bc":"markdown","cee3ff12":"markdown","f010e664":"markdown","5220f095":"markdown","e025faf4":"markdown","bebf5a7f":"markdown"},"source":{"63e6a52e":"import pandas as pd\nimport numpy as np","4366b129":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score","52f739f8":"data=pd.read_csv('..\/input\/dataset\/Financial Distress.csv')","59d47ee4":"data.head()","66cd7739":"data.columns","3bb2bc7a":"data['Financial Distress'].value_counts()","c75d266a":"X, y = data.loc[:,data.columns!='Financial Distress'], data.loc[:,'Financial Distress'].values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=123,stratify=data['Financial Distress'])","c25adffe":"# We have used stratified above to split the data distribution in equal manner\nprint(pd.value_counts(y_train)\/y_train.size * 100)\nprint(pd.value_counts(y_test)\/y_test.size * 100)","e8672a1b":"from sklearn.ensemble import RandomForestClassifier\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\n\naccuracy_score(y_test, rfc_pred)","1ec205ee":"# f1 score\nf1_score(y_test, rfc_pred)\n","c8d6af3b":"# confusion matrix\npd.DataFrame(confusion_matrix(y_test, rfc_pred))","a57750cf":"# recall score\nrecall_score(y_test, rfc_pred)","2ab700e5":"from sklearn.utils import resample\n","6b5e9108":"X['class']=y","d5522a6a":"X.columns","6e48fb03":"# separate minority and majority classes\nnot_distress = X[X['class']==0]\ndistress = X[X['class']==1]\n\n# upsample minority\nfraud_upsampled = resample(distress,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_distress), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_distress, fraud_upsampled])\n\n# check new class counts\nupsampled['class'].value_counts()","ecc074c7":"# trying logistic regression again with the balanced dataset\ny_train = upsampled['class']\nX_train = upsampled.drop('class', axis=1)\n\nupsampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nupsampled_pred = upsampled.predict(X_test)\n","1b294d57":"# Checking accuracy\naccuracy_score(y_test, upsampled_pred)","ddfecbbd":"# f1 score\nf1_score(y_test, upsampled_pred)","1957dcc2":"# confusion matrix\npd.DataFrame(confusion_matrix(y_test, upsampled_pred))","c0f85c2e":"recall_score(y_test, upsampled_pred)","0acfe90d":"# downsample majority\nnot_fraud_downsampled = resample(not_distress,\n                                replace = False, # sample without replacement\n                                n_samples = len(distress), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, distress])\n\n# checking counts\ndownsampled['class'].value_counts()","3e70ef1c":"y_train = downsampled['class']\nX_train = downsampled.drop('class', axis=1)\n\nundersampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nundersampled_pred = undersampled.predict(X_test)","c6353721":"# Checking accuracy\naccuracy_score(y_test, undersampled_pred)","7ac0c50e":"# f1 score\nf1_score(y_test, undersampled_pred)","002e8348":"# confusion matrix\npd.DataFrame(confusion_matrix(y_test, undersampled_pred))","47662bdc":"recall_score(y_test, undersampled_pred)","29f38a6b":"from imblearn.over_sampling import SMOTE\n\n# Separate input features and target\ny = data['Financial Distress']\nX = data.drop('Financial Distress', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n\nsm = SMOTE(random_state=27, ratio=1.0)\nX_train, y_train = sm.fit_sample(X_train, y_train)","ba61683f":"smote = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nsmote_pred = smote.predict(X_test)\n\n# Checking accuracy\naccuracy_score(y_test, smote_pred)\n","a59ecb22":"# f1 score\nf1_score(y_test, smote_pred, average='weighted')\n","3f5fe0a5":"# confustion matrix\npd.DataFrame(confusion_matrix(y_test, smote_pred))\n","120c34db":"recall_score(y_test, smote_pred)\n","072a975a":"Over here undersampling worked the best but many other algorithms like LightGBM, XGBoost will work better which I will try to cover later in this notebook.","703c74bc":"Accuracy is not the best metric to use when evaluating imbalanced datasets as it can be misleading. Metrics that can provide better insight include:\n\n    Confusion Matrix: a talbe showing correct predictions and types of incorrect predictions.\n    Precision: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier's exactness. Low precision indicates a high number of false positives.\n    Recall: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier's completeness. Low recall indicates a high number of false negatives.\n    F1: Score: the weighted average of precision and recall.\n\nSince our main objective with the dataset is to prioritize accuraltely classifying financial instability cases the recall score can be considered our main metric to use for evaluating outcomes.","cee3ff12":"Oversampling Minority Class\n\nOversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don't have a ton of data to work with. A con to consider when undersampling is that it can cause overfitting and poor generalization to your test set.\n\nWe will use the resampling module from Scikit-Learn to randomly replicate samples from the minority class.\nImportant Note\n\nAlways split into test and train sets BEFORE trying any resampling techniques! Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets! This can allow our model to simply memorize specific data points and cause overfitting.","f010e664":"Undersampling Majority Class\n\nUndersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable.\n\nWe will again use the resampling module from Scikit-Learn to randomly remove samples from the majority class.","5220f095":"I have modeified the target values if \"Financial Distress\" if it is greater than -0.50 the company should be considered as healthy (0). Otherwise, it would be regarded as financially distressed (1). ","e025faf4":"Generate Synthetic Samples\n\nSMOTE or Synthetic Minority Oversampling Technique is a popular algorithm to creates sythetic observations of the minority class.","bebf5a7f":"## Trying different algorithm"}}