{"cell_type":{"9e5d28ac":"code","e01a8389":"code","bceda13b":"code","62ba8bcc":"code","22aae814":"code","3c617f79":"code","4a6d98a9":"code","a36e1c24":"code","cea7fb3b":"code","5bae61a6":"code","f81b30b3":"code","00b42eb8":"code","52451bb7":"code","b80e577c":"code","fd60ee9e":"code","5d4f7d4f":"code","58e4f6e9":"code","c61ad219":"code","8c2a4063":"code","416dd77f":"code","c71e74cc":"code","073d1f68":"code","23604dde":"code","d891af22":"code","06febf84":"code","ef8a6ca0":"code","30bbd755":"code","22c7ff91":"code","b91a64af":"code","44c660c9":"code","6aa09380":"code","6bfcbf15":"code","958be6d7":"code","d0ba4d8d":"code","fda84be4":"code","9a2f2136":"code","8c2aae2f":"code","487788e3":"code","a222d718":"code","b6edbbc8":"code","9e07e475":"code","083b2f00":"code","3faf1d57":"code","e7c71973":"code","d86168f1":"code","2aac009c":"code","244133ff":"code","d0af68e6":"code","e6a812e9":"code","cb22cea2":"code","60160f4b":"code","f9028b57":"code","9de7bab2":"code","18f3dc7a":"code","5879893c":"code","5d4a86ed":"code","56c06e13":"code","b8684003":"code","7922433e":"markdown","4cd613ac":"markdown","01a5fd4e":"markdown"},"source":{"9e5d28ac":"#importing libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#using seaborn for styling\nsns.set()","e01a8389":"#Reading csv values and converting it into DataFrames\nTrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nTest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","bceda13b":"#finding the number of row and column in Test dataset\nTest.shape","62ba8bcc":"#finding the number of row and column in train dataset\nTrain.shape","22aae814":"#Understanding the structure of test dataset\nTrain.head(5)","3c617f79":"#Exploring the name of categorical variables in the train dataset\nTrain.select_dtypes( include = \"object\").columns","4a6d98a9":"#Finding out the number of categorical Variables in train dataset\nTrain.select_dtypes(include = \"object\").columns.shape","a36e1c24":"#Exploring the name of Numerical variables in the dataset\nTrain.select_dtypes( exclude =\"object\").columns","cea7fb3b":"#Finding out the number of categorical Variables in train dataset\nTrain.select_dtypes( exclude =\"object\").columns.shape","5bae61a6":"#Here is a shortcut just use info method to find out non -null count,Dtype\nTest.info()","f81b30b3":"#using describe() to know descriptive stats\n#round() function is used to round upto 2 decimals\n#transpose() is used for interchanging rows into columns or vice versa\nTrain.describe().transpose().round(2)","00b42eb8":"#By default describe() only show numerical attributes ,show we need to use include \nTrain.describe(include = \"object\").transpose()","52451bb7":"#Exploring the SalePrice which is our dependent variable which needs to be predicted\nTrain[\"SalePrice\"].describe()","b80e577c":"#Plotting density plot with distplot\n#KDE is kernel density estimation,a method to estimate the density plot\nsns.distplot(Train[\"SalePrice\"],hist = True ,rug = True,\n            kde = True )\nplt.ylabel(\"Density\")","fd60ee9e":"\nprint('Skewness: %f' % Train[\"SalePrice\"].skew())\nprint(\"Kurtosis : %f\" %Train[\"SalePrice\"].kurtosis())","5d4f7d4f":"#scatter plot grlivarea\/saleprice\nplt.scatter( y = Train[\"SalePrice\"],x = Train[\"GrLivArea\"]\n          ,alpha = 0.5, color = \"Red\")\nplt.ylabel(\"SalePrice\")\nplt.xlabel(\"GrlivArea\")\nplt.title(\"Scatter Plot\")","58e4f6e9":"#scatter plot totalbsmtsf\/saleprice\nplt.scatter( y= Train[\"SalePrice\"] , \n            x = Train[\"TotalBsmtSF\"], alpha = 0.5)\nplt.ylabel(\"SalePrice\")\nplt.xlabel(\"TotalBsmtSF\")\n","c61ad219":"#box plot overallqual\/saleprice\nfig = sns.boxplot(y = Train[\"SalePrice\"] , x =\n           Train[\"OverallQual\"]\n        ,width=0.5, fliersize=5)","8c2a4063":"#SalePrice\/YearBuilt boxplot\nf,ax = plt.subplots(figsize = (15,8))\nfig = sns.boxplot(y =  Train[\"SalePrice\"], x=Train[\"YearBuilt\"]\n           ,width = 0.5)\nx = plt.xticks(rotation = 90)\ny = plt.yticks(rotation = 90)\nplt.savefig(\"sample.pdf\")\n","416dd77f":"#Drawing the correlation matrix using heatmap\n#Correlation measures how much a variable is correlated to another variable\ncorrmat = Train.corr()\nplt.subplots(figsize = (12,9))\nsns.heatmap(corrmat,cmap=\"YlGnBu\",vmax = 0.8, square= 1)","c71e74cc":"#saleprice correlation matrix\ncols = corrmat.nlargest(10,\"SalePrice\")[\"SalePrice\"].index\ncm = np.corrcoef(Train[cols].values.T)\nax = sns.heatmap(cm ,square = True,cbar = 1, xticklabels= cols.values,\n           yticklabels = cols.values , annot = True\n           , fmt = \".2f\" , annot_kws = { \"size\": 9})\nplt.show()\n","073d1f68":"#Pair plot for top 10 correlated variables\ncols = [\"SalePrice\" , \"OverallQual\",\"GrLivArea\",\n        \"GarageCars\" , \"TotalBsmtSF\" , \"FullBath\" ,\n       \"YearBuilt\"]\nsns.pairplot(Train[cols])\nplt.savefig(\"pairplot.pdf\")\nplt.show()","23604dde":"#missing data of Train\ntotal = Train.isnull().sum().sort_values( ascending = False)\npercent = (Train.isnull().sum()\/Train.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total , percent],axis = 1 , keys = [\"Total\",\"Percent\"])\nmissing_data.head(20)","d891af22":"#missing data of Test\ntotal1 = Test.isnull().sum().sort_values( ascending = False)\npercent1 = (Test.isnull().sum()\/Test.isnull().count()).sort_values(ascending = False)\nmissing_data1 = pd.concat([total1, percent1],axis = 1 , keys = [\"Total1\",\"Percent1\"])\nmissing_data1[missing_data1[\"Total1\"]>0]\n","06febf84":"#Deleting all the columns in train dataset which have more than one  missing value\nTrain =  Train.drop(missing_data[missing_data[\"Total\"]>1].index, axis = 1)","ef8a6ca0":"#Deleting all the columns in test dataset which we have deleted in the train dataset\nmissing_data[missing_data[\"Total\"]>1].index\nTest = Test.drop(labels = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage',\n       'GarageCond', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual',\n       'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual',\n       'MasVnrArea', 'MasVnrType'] , axis = 1)","30bbd755":"#Dealing with the missing numerical and categorical data of test dataset separately.\n#Filling the missing value of categorical data with the mode of that columns.\n\nTest['MSZoning']=Test['MSZoning'].fillna(Test['MSZoning'].mode()[0])\nTest['Functional']=Test['Functional'].fillna(Test['Functional'].mode()[0])\nTest['Utilities']=Test['Utilities'].fillna(Test['Utilities'].mode()[0])\nTest['Exterior2nd']=Test['Exterior2nd'].fillna(Test['Exterior2nd'].mode()[0])\nTest['KitchenQual']=Test['KitchenQual'].fillna(Test['KitchenQual'].mode()[0])\nTest['SaleType']=Test['SaleType'].fillna(Test['SaleType'].mode()[0])\nTest['Exterior1st']=Test['Exterior1st'].fillna(Test['Exterior1st'].mode()[0])\n\n#Filling the missing numerical value with its mean\nfor num_col in ['BsmtHalfBath', 'BsmtFullBath', 'GarageCars', 'GarageArea','BsmtFinSF1', 'TotalBsmtSF', 'BsmtUnfSF', 'BsmtFinSF2']:\n    Test[num_col]= Test[num_col].fillna(Test[num_col].mean())\n","22c7ff91":"#Standardization of SalePrice \nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nsaleprice_scaled = std.fit_transform(Train[\"SalePrice\"].values.reshape(-1,1))\n#To know the variation in the SalePrice\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10] # Ten Lowest saleprice of house\nhigh_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]# Ten Largest saleprice of house\n","b91a64af":"#Finding the outliers in the GrLivArea\nfig = plt.scatter(x = Train[\"GrLivArea\"] , y = Train[\"SalePrice\"])\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")","44c660c9":"#Finding out the index of the outliers\nTrain[\"GrLivArea\"].sort_values(ascending = False)[:2]","6aa09380":"#Deleting the outliers\nTrain = Train.drop( index = 1298 , axis = 0)\nTrain = Train.drop(index = 523,axis = 0)\n\n#Now that's good ,not outliers\nplt.scatter(x = Train[\"GrLivArea\"] , y = Train[\"SalePrice\"])\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")","6bfcbf15":"#Finding out the outliers in TotalBsmtSf\nplt.scatter(x = Train[\"TotalBsmtSF\"] ,\n            y = Train[\"SalePrice\"])\nplt.xlabel(\"TotalBsmtSf\")\nplt.ylabel(\"SalePrice\")","958be6d7":"#Creating a new column \"HasBsmt\" in the train dataset which hold binary variable (1 0r 0) on the basis that the house have basement or not.\n#Creating a series having the same index as train dataset\nTrain[\"HasBsmt\"] = pd.Series(len(Train[\"TotalBsmtSF\"]),index = Train.index)\n#Filling all the rows of \"HasBsmt\" column with zero\nTrain[\"HasBsmt\"] = 0\n#filling  the rows of \"HasBsmt\" with 1 where \"TotalBsmt\" is greater than zero.\nTrain.loc[Train[\"TotalBsmtSF\"]>0,\"HasBsmt\"] = 1\n\n#Doing all the same thing with Test Dataset\nTest[\"HasBsmt\"] = pd.Series(len(Test[\"TotalBsmtSF\"]),index = Test.index)\nTest[\"HasBsmt\"] = 0\nTest.loc[Test[\"TotalBsmtSF\"]>0,\"HasBsmt\"] = 1\n","d0ba4d8d":"#Plotting the density plot of SalePrice \nfrom scipy import stats\nfrom scipy.stats import norm\nfig = sns.distplot(Train[\"SalePrice\"],fit = norm)\nplt.ylabel(\"Density\")\nplt.show()\n#Plotting the probability plot\nres = stats.probplot(Train[\"SalePrice\"], plot = plt)\n","fda84be4":"#Logrithmic Transformation of SalePrice\nTrain[\"SalePrice\"] = np.log(Train[\"SalePrice\"])\nf = stats.probplot(Train[\"SalePrice\"] , plot = plt)","9a2f2136":"#After neccesary transformation , we can say that SalePrice is normally distributed.\ny = sns.distplot(Train[\"SalePrice\"] , fit = norm)","8c2aae2f":"#Prob plot and density plot of GrLivArea\nf= sns.distplot(Train[\"GrLivArea\"], fit = norm)\nfig = plt.figure()\nx= stats.probplot(Train[\"GrLivArea\"],plot = plt)","487788e3":"#Logirthmic Transformation on Both Train and Test Dataset\nTrain[\"GrLivArea\"] = np.log(Train[\"GrLivArea\"])\nTest[\"GrLivArea\"] = np.log(Test[\"GrLivArea\"])","a222d718":"#After Transformation\nfig = sns.distplot(Train[\"GrLivArea\"], fit = norm)\nfig = plt.figure()\nf = stats.probplot(Train[\"GrLivArea\"], plot = plt)","b6edbbc8":"#Doing the same with TotalBsmtSF\nsns.distplot(Train[\"TotalBsmtSF\"],fit = norm)\nfig = plt.figure()\nr = stats.probplot(Train[\"TotalBsmtSF\"],plot = plt)","9e07e475":"#Doing the logarithmic Transformation for all the non zeros valuse in the TotalBsmtSF column for both the \nTrain.loc[Train[\"HasBsmt\"]==1,'TotalBsmtSF'] = np.log(Train[\"TotalBsmtSF\"])\nTest.loc[Test[\"HasBsmt\"]==1,'TotalBsmtSF'] = np.log(Test[\"TotalBsmtSF\"])\n","083b2f00":"sns.distplot(Train[Train[\"HasBsmt\"]>0][\"TotalBsmtSF\"], fit = norm)\nfig = plt.figure()\nf = stats.probplot(Train[Train[\"TotalBsmtSF\"]>0][\"TotalBsmtSF\"],plot = plt)","3faf1d57":"#Concating both the dataset into single DataSet\nfinal_df=pd.concat([Train,Test],axis=0)","e7c71973":"final_df","d86168f1":"final_df.shape","2aac009c":"#Finding all the name of categorical varibles\"\nTrain.select_dtypes(include = object).columns","244133ff":"#Storing all the categorical variables in the \"columns\" list\ncolumns  = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'ExterQual', 'ExterCond', 'Foundation', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional',\n       'PavedDrive', 'SaleType', 'SaleCondition']","d0af68e6":"#category_onehot_multcols is function that takes list of categorical columns and covert it into dummy variables\ndef category_onehot_multcols(multcolumns):\n    df_final=final_df\n    i=0\n    for fields in multcolumns:\n        \n        print(fields)\n        df1=pd.get_dummies(final_df[fields],drop_first=True)\n        \n        final_df.drop([fields],axis=1,inplace=True)\n        if i==0:\n            df_final=df1.copy()\n        else:\n            \n            df_final=pd.concat([df_final,df1],axis=1)\n        i=i+1\n       \n        \n    df_final=pd.concat([final_df,df_final],axis=1)\n        \n    return df_final","e6a812e9":"#Coverting all the categorical columns into dummy variables \nfinal_df=category_onehot_multcols(columns)","cb22cea2":"final_df.shape","60160f4b":"#Removing the duplicates\nfinal_df = final_df.loc[:,~final_df.columns.duplicated()]\n","f9028b57":"final_df.shape","9de7bab2":"#Splitting the final_df into df_Train and df_Test.\n#As we know that in our \"submission.csv\" ,house id starts from 1458.\ndf_Train=final_df.iloc[:1458,:]\ndf_Test=final_df.iloc[1458:,:]","18f3dc7a":"#Deleting the SalePrice From df_test\ndf_Test.drop([\"SalePrice\"],axis=1,inplace = True)","5879893c":"#Fom training of our Linear Regression model\nX_train = df_Train.drop([\"SalePrice\"],axis = 1)\ny_train = df_Train[\"SalePrice\"]\ny_train\ndf_Train[\"SalePrice\"]","5d4a86ed":"from sklearn.linear_model import LinearRegression\n#Creating instance of LinearRegression()\nlr = LinearRegression()\n#Using fit method to obtain linear Regression\nlr.fit(X_train,y_train)\n#Using linear regression to predict the the HousePrices of test dataset\ny_pred = lr.predict(df_Test)\n#Taking exponential of y_pred\npredictions =np.exp(y_pred)","56c06e13":"#Making dataframe \"submit\" according to the given submission file.\ntid = df_Test[\"Id\"]\nsubmit = pd.DataFrame({\"Id\" : tid, \"SalePrice\" : predictions})\nsubmit.head()","b8684003":"#Coverting the DataFrame submit into csv file.\nsubmit.to_csv(\"house_price_prediction.csv\",index = False)","7922433e":"# Relationship of SalePrice with categorical variables","4cd613ac":"# Relationship of SalePrice with other numerical Variables","01a5fd4e":"from these values of Skewness and Kurtosis we can infer that it is \ndeviated from the normal distribution  and have positive skewness\nlater we will do the necessary transformation to make it a normal\nditribution"}}