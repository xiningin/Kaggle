{"cell_type":{"168bbdb2":"code","f2d39838":"code","0673be76":"code","2152d97c":"code","8df68be1":"code","e72116cb":"code","339bd96a":"code","cd60cfe2":"code","3d603067":"code","fdb5f9b0":"code","d4e67165":"code","99366b57":"code","bff9b3a6":"code","7cd23c85":"code","a29820b4":"code","896e21a8":"code","8786d5e8":"code","963070ee":"code","c7a25079":"code","4f4fa4d6":"code","5ee2e649":"code","fcf25e84":"code","04f4845b":"code","3255bd41":"code","15bdafed":"code","4987ecc6":"code","175fa91b":"code","006244c8":"code","977f795f":"code","31a151e1":"code","52a3c35c":"code","5ab49848":"code","2c08bbe0":"code","9efe2417":"code","34e00bec":"code","26bd29c4":"code","d1f17f3d":"code","781ca5aa":"code","fd9535e5":"code","c889c1c0":"code","12fc1290":"code","1be597be":"code","fba893b6":"code","5366e856":"code","53fafcb1":"code","4717200f":"code","1e945d42":"code","dbf62b0b":"code","56aa9fd8":"markdown","09cf19e2":"markdown","72f1b2e1":"markdown","a079b249":"markdown","76dc0698":"markdown","db7cb939":"markdown","36de9961":"markdown","3bcccc7b":"markdown","13519d5b":"markdown","ff88c7e9":"markdown","f7d3aa32":"markdown"},"source":{"168bbdb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n%time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2d39838":"%%time\n#Plotting Tools\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport random\nimport datetime\nimport warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")\n\n# Tools\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score ,accuracy_score ,mean_absolute_error,mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold , KFold ,train_test_split\nfrom sklearn.ensemble import VotingClassifier\n\n#Models\nimport optuna\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n","0673be76":"%%time\nfilename = '\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv'\nn = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)\ns = 500000 #desired sample size\nskip = sorted(random.sample(range(1,n+1),n-s))","2152d97c":"%%time\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv' ,skiprows=skip)\ntrain_df.head()","8df68be1":"%%time\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv')\ntest_df.head()","e72116cb":"%%time\ndef memory_reduce(dataframe):\n    \"\"\"\n    Use this function to reduce size of dataframe in memory\n    before use = make sure you handeled all null vaues  in dataframe\n    input = [dataframe]\n    output= [dataframe]\n    \"\"\"\n    start_time = datetime.datetime.now()\n    \n    if sum(dataframe.isnull().sum()) > 0 : \n        print(\"DataFrame could not have null Values ,Please Fill Null values before use this function\")\n    else:  \n        init_momry_size = dataframe.memory_usage(deep =True).sum() \/1024 ** 2\n#         print(f'Memory Size of data  is equal to  {init_momry_size} MB')  # Memory Size Before apply Function \n\n        for column in dataframe.columns :\n            NA_coulmns = []\n            if dataframe[column].dtype != \"object\":\n#                 print(f'******************** Column: {column} *************************')\n#                 print(f'Column  data type : {dataframe[column].dtype} \\n')\n                is_int = False\n                column_max_value = dataframe[column].max()\n                column_min_value = dataframe[column].min()\n                \n                as_int = dataframe[column].fillna(0).astype(np.int64)\n                result = (dataframe[column] - as_int)\n                result = result.sum()\n                if  result > -0.01 and result < 0.01:\n                    is_int =True\n                # if column value is int   \n                if is_int:\n                    if column_min_value >=0:\n                        if column_max_value < 255:\n                            dataframe[column] = dataframe[column].astype(np.uint8)\n                        elif column_max_value < 65536 : \n                            dataframe[column]= dataframe[column].astype(np.uint16)\n                        elif column_max_value < 4294967295:\n                            dataframe[column] = dataframe[column].astype(np.uint32)\n                        else:\n                            dataframe[column] = dataframe[column].astype(np.uint64)\n                    else:\n                        if column_min_value > np.iinfo(np.int8).min and column_max_value < np.iinfo(np.int8).max:\n                            dataframe[column] = dataframe[column].astype(np.int8)\n                        elif column_min_value > np.iinfo(np.int16).min and column_max_value < np.iinfo(np.int16).max:\n                            dataframe[column] = dataframe[column].astype(np.int16)\n                        elif column_min_value > np.iinfo(np.int32).min and column_max_value < np.iinfo(np.int32).max:\n                            dataframe[column] = dataframe[column].astype(np.int32)\n                        elif column_min_value > np.iinfo(np.int64).min and column_max_value < np.iinfo(np.int64).max:\n                            dataframe[column] = dataframe[column].astype(np.int64)\n                # If coulmn Value is float\n                else:\n                    dataframe[column] = dataframe[column].astype(np.float32)\n#                 print(f'column data type changed to {dataframe[column].dtype}')\n#                 print('************************************************************ \\n')\n    final_memory_size = dataframe.memory_usage(deep =True).sum() \/1024 ** 2\n\n    end_time = datetime.datetime.now()\n    \n    print(f'Memory Size of data before applying function to DateFrame: {init_momry_size} MB')\n    print(f'memory size changed after applying function to DataFrame : {final_memory_size} MB')\n    print(f'memory size has been optimized {round((final_memory_size \/ init_momry_size) * 100 ,2) } %')\n    print(\"\\n\")\n    \n    print('**************************************************')\n    print(f'Task started at: {start_time}')\n    print(f'Task ended at:   {end_time}')\n    print(f'Task total time: {end_time - start_time}')\n    print('\\n')\n    \n    return dataframe","339bd96a":"%%time\ntrain_df  = memory_reduce(train_df)","cd60cfe2":"%%time\ntest_df =memory_reduce(test_df)","3d603067":"%%time\ntrain_df.shape , test_df.shape","fdb5f9b0":"%%time\ntrain_df.isnull().sum()","d4e67165":"%%time\ntrain_df.info()","99366b57":"%%time\ntrain_df.describe().T","bff9b3a6":"%%time\nY = train_df.target\nX = train_df.drop(['id','target'],axis=1)\nX.head()","7cd23c85":"%%time\nY.head()","a29820b4":"id_test = test_df.id\nid_test.head()","896e21a8":"%%time\nX_test = test_df.drop(\"id\",axis=1)\nX_test.head()","8786d5e8":"%%time\ndel train_df,test_df","963070ee":"gc.collect()","c7a25079":"%%time\nfeatures = X.columns.to_list()\nprint(features,end='')\nprint(\"\\n\")","4f4fa4d6":"%%time\ndef get_categorical_columns(dataframe,verbose=True):\n    \"\"\"\n    Use this function to retun categorical columns of a dataframe\n    input : dataframe\n    output: list \n    \n    \"\"\"\n    categorical_columns = []\n    for column in dataframe.columns :\n        if dataframe.dtypes[column] in ['int8' ,'int16',\"int32\" ,\"int64\" ,'uint8']:\n            categorical_columns.append(column)\n    print(f' There are {len(categorical_columns)} categorical columns in Dataframe')\n    print(categorical_columns,end='')\n    print(\"\\n\")\n    return categorical_columns","5ee2e649":"%%time\ncategorical_columns = get_categorical_columns(X)","fcf25e84":"%%time\n\ndef get_continuous_columns(dataframe,verbose=True):\n    \"\"\"\n    Use this function to retun countinous  columns of a dataframe\n    input : dataframe\n    output: list \n    \n    \"\"\"\n    \n    continuos_column = []\n    for column in dataframe.columns:\n        if dataframe.dtypes[column] in ['float16' ,'float32' ,'float64']:\n            continuos_column.append(column)\n    print(f'There are {len(continuos_column)} continous  columns  in Datframe')\n    print(continuos_column,end=\"\")\n    print(\"\\n\")\n    return continuos_column","04f4845b":"%%time \ncontinous_columns = get_continuous_columns(X)","3255bd41":"%%time\nX = memory_reduce(X)","15bdafed":"%%time\nX_test= memory_reduce(X_test)","4987ecc6":"%%time\nsns.histplot(data=Y,palette=\"viridis\",bins=10 , color =\"green\")","175fa91b":"%%time\ntarget_distrbution  = pd.DataFrame(Y.value_counts() \/ len(Y))\ntarget_distrbution.T","006244c8":"%%time\nsns.countplot(Y ,palette=\"Set3\")","977f795f":"%%time\nplt.bar([1,2], [len(categorical_columns) ,len(continous_columns)])\nplt.xticks([1,2] , ['categorical' , 'continous'])\nplt.show()","31a151e1":"%time\nstart_time  =datetime.datetime.now()\nsns.set_style(\"whitegrid\")\nnrows = (len(categorical_columns) \/\/ 6) + 1\nsize_y = nrows * 4\nfig ,axes =  plt.subplots(nrows=nrows,ncols=6,figsize=( 30, size_y ))\n\ncounter = 0\nfor i in range (1, nrows+1):\n    for j in range (1,7):\n        if counter >= len(categorical_columns):\n            break\n        else:\n            subchart = sns.histplot(data =X[categorical_columns] , x = str(X[categorical_columns].columns[counter]),\n                                    ax= axes[(i-1),(j-1)] ,color = \"green\",label = \"Train\" ,bins=2)\n            subchart = sns.histplot(data =X_test[categorical_columns]  , x = str(X_test[categorical_columns].columns[counter]),\n                                    ax = axes[(i-1),(j-1)] ,color = \"red\",label = \"test\" ,bins=2)\n            counter += 1\n    print(f'row {i} out of {nrows} has been plotted') #print this line to monitor progress of plottin you can comment this line\nend_time = datetime.datetime.now()\n\nprint('\\n')\nprint('**************************************************')\nprint(f'Plotting Started at : {start_time}')\nprint(f'Plotting ended at : {end_time}')\nprint(f'total plotting time is : {end_time - start_time}')","52a3c35c":"%time\nstart_time  =datetime.datetime.now()\nsns.set_style(\"whitegrid\")\nnrows = (len(continous_columns) \/\/ 6) + 1\nsize_y = nrows * 4\nfig ,axes =  plt.subplots(nrows=nrows,ncols=6,figsize=( 30, size_y ))\n\ncounter = 0\nfor i in range (1, nrows+1):\n    for j in range (1,7):\n        if counter >= len(continous_columns):\n            break\n        else:\n            subchart = sns.histplot(data =X[continous_columns] , x = str(X[continous_columns].columns[counter]),ax= axes[(i-1),(j-1)] ,color = \"blue\",label = \"Train\",bins=2)\n            subchart = sns.histplot(data =X_test[continous_columns]  , x = str(X_test[continous_columns].columns[counter]),ax = axes[(i-1),(j-1)] ,color = \"red\",label = \"test\",bins=2)\n            counter += 1\n    print(f'row {i} out of {nrows} has been plotted')\nend_time = datetime.datetime.now()\n\nprint('\\n')\nprint('**************************************************')\nprint(f'Plotting Started at : {start_time}')\nprint(f'Plotting  ended at :  {end_time}')\nprint(f'total plotting time is : {end_time - start_time}')","5ab49848":"%time\nmatrix = np.triu(X[categorical_columns].corr())\nplt.figure(figsize=(20 ,10))\nsns.heatmap(X[categorical_columns].corr() , annot= False , cmap=\"coolwarm\" , mask=matrix , linecolor=\"white\" ,cbar=True ,linewidths=0.1)\nplt.title('Categorical Corrlation Matrix')\nplt.show()","2c08bbe0":"%time\nstart_time = datetime.datetime.now()\n\ncorr_df = X[continous_columns].corr().abs()\nhigh_corr = np.where(corr_df >0.02)\nhigh_corr = [(corr_df.columns[x] , corr_df.columns[y]) for x,y  in zip (*high_corr) if x !=y and x >y]\n\nend_time = datetime.datetime.now()\n\nprint('\\n')\nprint('**************************************************')\nprint(f'Task started at: {start_time}')\nprint(f'Task ended at:   {end_time}')\nprint(f'Task total time: {end_time - start_time}')","9efe2417":"%time\nstart_time = datetime.datetime.now()\n\nhigh_corr_features=[]\nfor x in high_corr:\n    for item in x:\n        if item not in high_corr_features:\n            high_corr_features.append(item)\nmatrix = np.triu(X[high_corr_features].corr())\nplt.figure(figsize=(20 ,10))\nsns.heatmap(X[high_corr_features].corr() , annot= False , cmap=\"coolwarm\" , mask=matrix , linecolor=\"white\" ,cbar=True ,linewidths=0.1)\nplt.title('High corraltion  for continous columns matrix')\nplt.show()\n\nend_time = datetime.datetime.now()\n\nprint('\\n')\nprint('**************************************************')\nprint(f'Task started at: {start_time}')\nprint(f'Task ended at:   {end_time}')\nprint(f'Task total time: {end_time - start_time}')","34e00bec":"%time\nstart_time = datetime.datetime.now()\n\ncorr_df = X.corr().abs()\nhigh_corr = np.where(corr_df >0.02)\nhigh_corr = [(corr_df.columns[x] , corr_df.columns[y]) for x,y  in zip (*high_corr) if x !=y and x >y]\n\nend_time = datetime.datetime.now()\n\nprint('\\n')\nprint('**************************************************')\nprint(f'Task started at: {start_time}')\nprint(f'Task ended at:   {end_time}')\nprint(f'Task total time: {end_time - start_time}')","26bd29c4":"%time\nstart_time = datetime.datetime.now()\n\nhigh_corr_features=[]\nfor x in high_corr:\n    for item in x:\n        if item not in high_corr_features:\n            high_corr_features.append(item)\nmatrix = np.triu(X[high_corr_features].corr())\nplt.figure(figsize=(20 ,10))\nsns.heatmap(X[high_corr_features].corr() , annot= False , cmap=\"coolwarm\" , mask=matrix , linecolor=\"white\" ,cbar=True ,linewidths=0.1)\nplt.title('High corraltion  for  all features matrix')\nplt.show()\n\nend_time = datetime.datetime.now()\n\nprint('\\n')\nprint('**************************************************')\nprint(f'Task started at: {start_time}')\nprint(f'Task ended at:   {end_time}')\nprint(f'Task total time: {end_time - start_time}')","d1f17f3d":"%%time\ngc.collect()","781ca5aa":"%%time\nstart_time = datetime.datetime.now()\n\nX[\"n_missing\"] = X[features].isna().sum(axis=1)\nX_test[\"n_missing\"] = X_test[features].isna().sum(axis=1)\n\nX[\"std\"] = X[features].std(axis=1)\nX_test[\"std\"] = X_test[features].std(axis=1)\n\nfeatures += ['n_missing' , 'std']\n\nend_time = datetime.datetime.now()\n\nprint('**************************************************')\nprint(f'Task started at: {start_time}')\nprint(f'Task ended at:   {end_time}')\nprint(f'Task total time: {end_time - start_time}')\nprint('\\n')","fd9535e5":"%%time\n\nstart_time = datetime.datetime.now()\n\nsimple_imputer = SimpleImputer(strategy=\"mean\")\n\nX[features] = simple_imputer.fit_transform(X[features])\nX_test[features] = simple_imputer.transform(X_test[features])\n\n\nend_time = datetime.datetime.now()\n\nprint('**************************************************')\nprint(f'Task started at: {start_time}')\nprint(f'Task ended at:   {end_time}')\nprint(f'Task total time: {end_time - start_time}')\nprint('\\n')","c889c1c0":"gc.collect()","12fc1290":"%%time\nstart_time = datetime.datetime.now()\n\nstandard_scaler = StandardScaler()\nX[features] = standard_scaler.fit_transform(X[features])\nX_test[features] = standard_scaler.transform(X_test[features])\n\nend_time = datetime.datetime.now()\nprint('**************************************************')\nprint(f'Task started at: {start_time}')\nprint(f'Task ended at:   {end_time}')\nprint(f'Task total time: {end_time - start_time}')\nprint('\\n')","1be597be":"%%time\ndef objective(trial , data=X, target= Y):    \n    params ={\"max_depth\" :trial.suggest_int(\"max_depth\" ,2,8) , \n          \"learning_rate\" : trial.suggest_float(\"learning_rate\" , 0.005 , 0.2),\n          \"n_estimators\" : trial.suggest_int(\"n_estimators\" , 1000 ,5000),\n          \"min_child_weight\" : trial.suggest_int(\"min_child_weight\" , 1,500),\n          \"gamma\" : trial.suggest_float(\"gamma\" ,0.0001 , 1.0 , log = True),\n          \"alpha\": trial.suggest_float(\"alpha\" , 0.0001 , 10 ,log = True),\n          \"lambda\": trial.suggest_float(\"lambda\" ,0.0001, 10.0 , log = True),\n          \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\" , 0.1 , 0.8), \n          \"subsample\": trial.suggest_float(\"subsample\" , 0.1,0.9),\n          \"tree_method\" : \"gpu_hist\",\n          \"booster\" : \"gbtree\",\n           \"random_state\": 228 ,\n           \"use_label_encoder\" : False,\n           \"eval_metric\" : \"auc\"\n          }\n    model = XGBClassifier(**params)\n    scores = []\n    K = StratifiedKFold(n_splits=4,random_state=228 , shuffle=True)\n    for i ,(train_idx , val_idx) in enumerate(K.split(X,Y)):\n        X_train ,X_val = X.iloc[train_idx],X.iloc[val_idx]\n        Y_train ,Y_val = Y.iloc[train_idx],Y.iloc[val_idx]\n        model.fit(X_train ,Y_train ,eval_set = [(X_val,Y_val)] ,early_stopping_rounds =300 ,verbose = False)\n        \n        train_prediction = model.predict_proba(X_train)[:,1]\n        train_score = roc_auc_score(Y_train ,train_prediction)\n        \n        validate_prediction = model.predict_proba(X_val)[:,1]\n        validate_score = roc_auc_score(Y_val , validate_prediction)\n        scores.append((train_score , validate_score))\n        \n        print(f\"Fold {i+1} | AUC : {validate_score} \")\n        \n    scores = pd.DataFrame(scores ,columns=[\"train Score\" , \"Validation Score\"])\n    return scores[\"Validation Score\"].mean()","fba893b6":"%%time\nstart_time = datetime.datetime.now()\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective ,n_trials= 50)\n\nprint('\\n')\nprint('*************************************************')\nprint(\"Numbers of finished trials : \" , len(study.trials))\nprint(\"Best Trials : \", study.best_trial.params)\nprint(\"Best Values : \" , study.best_value)\n\nend_time = datetime.datetime.now()\nprint('\\n')\nprint('**************************************************')\nprint(f'Task started at: {start_time}')\nprint(f'Task ended at:   {end_time}')\nprint(f'Task total time: {end_time - start_time}')\nprint('\\n')","5366e856":"%%time\nxgb_params = study.best_trial.params\nxgb_params","53fafcb1":"gc.collect()","4717200f":"%%time \nstart_time = datetime.datetime.now()\n\nfolds = StratifiedKFold(n_splits=5,random_state=228,shuffle=True)\npredictions = np.zeros(len(X_test))\nfor fold,(train_idx,validate_idx) in enumerate(folds.split(X,Y)):\n    X_train,X_validate = X.iloc[train_idx] ,X.iloc[validate_idx]\n    Y_train,Y_validate = Y.iloc[train_idx] ,Y.iloc[validate_idx]\n    xgb_model = XGBClassifier(**xgb_params ,tree_method= \"gpu_hist\",booster = \"gbtree\" ,random_state = 228,use_label_encoder = False ,eval_metric = \"auc\")\n    xgb_model.fit(X_train,Y_train,eval_set = [(X_validate,Y_validate)],verbose =False,early_stopping_rounds =300)\n    predictions += xgb_model.predict_proba(X_test)[:,1] \/folds.n_splits\n    print(f'fold {fold} completed')\n    \nend_time = datetime.datetime.now()\n\nprint('\\n')\nprint('**************************************************')\nprint(f'Task started at: {start_time}')\nprint(f'Task ended at:   {end_time}')\nprint(f'Task total time: {end_time - start_time}')\nprint('\\n')","1e945d42":"gc.collect()","dbf62b0b":"%%time\n\nsubmit = pd.DataFrame({\"id\":id_test , \"target\": predictions})\nsubmit.to_csv(\"\/kaggle\/working\/xgb_submit.csv\",index=False)","56aa9fd8":"# Prediction Prepration","09cf19e2":"# Import Data ","72f1b2e1":"The is comes from this notebook : \nhttps:\/\/www.kaggle.com\/davidcoxon\/first-look-at-october-data","a079b249":"Distruburion of catecorical columns","76dc0698":"Distrubution of Continous columns ","db7cb939":"# Optuna ","36de9961":"# XG_boost","3bcccc7b":"# Exploratory Data Analysis","13519d5b":"High Correlation Features","ff88c7e9":"# Data Preprocessing","f7d3aa32":"# Analayze Data"}}