{"cell_type":{"baf7ce39":"code","7e092439":"code","73814fad":"code","de0eb01f":"code","788a9e5d":"code","e6d1575f":"code","50aa522a":"code","5c498166":"code","3f59b764":"code","6c2937ef":"code","b96adb6d":"code","08355ed3":"code","7445c4ef":"code","afe18b7e":"code","b287add0":"code","f70fabc2":"code","2b5c1a32":"code","6a6c159f":"code","bb687ae1":"code","134b9857":"code","4ef8d313":"markdown","f27ba17d":"markdown","ce127b02":"markdown","76bb8fae":"markdown","e7500157":"markdown","ceb3cf82":"markdown","e153d5b9":"markdown","474ec204":"markdown","b8712103":"markdown","e23800a0":"markdown"},"source":{"baf7ce39":"import numpy as np # linearalgebra and data handling\nimport pandas as pd # data handling and manipulation \nimport spacy # lemmatizing of the text data\nimport re, unicodedata # cleaning of the text data","7e092439":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv') # loading the train data\ntrain_df","73814fad":"docs = train_df.text\ntrain_target = train_df.target","de0eb01f":"nlp = spacy.load('en_core_web_lg', parse=True, tag=True, entity=True)","788a9e5d":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","e6d1575f":"def contractions(text):\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"i'll\", \"i will\", text)\n    text = re.sub(r\"she'll\", \"she will\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"here's\", \"here is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    return text","50aa522a":"normalized_docs = []\nfor doc in docs:\n    doc = doc.lower() # characters to lowercase\n    doc = remove_URL(doc)\n    doc = remove_html(doc)\n    doc = remove_emoji(doc)\n    doc = contractions(doc)\n    doc = re.sub(r'[0-9]', '', doc) # removing numbers\n    doc = re.sub(r'[&(),.#:\/\/?!]', '', doc) # verwijderen speciale tekens\n    doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc) # Verwijderen extra newlines\n    doc = re.sub(' +', ' ', doc) # Verwijderen overbodige whitespace\n    doc = unicodedata.normalize('NFKD', doc).encode('ascii', 'ignore').decode('utf-8', 'ignore') # verwijderen accented tekens\n    doc = nlp(doc)\n    doc = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in doc]) # Lemmatizeren van woorden\n    normalized_docs.append(doc)\nprint('Docs normalized')","5c498166":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_df","3f59b764":"new_docs = test_df.text","6c2937ef":"normalized_new_docs = []\nfor doc in new_docs:\n    doc = doc.lower() #verkleinen\n    doc = contractions(doc)\n    doc = remove_URL(doc)\n    doc = remove_html(doc)\n    doc = remove_emoji(doc)\n    doc = re.sub(r'[0-9]', '', doc) # verwijderen cijfers\n    doc = re.sub(r'[&(),.#]', '', doc) # verwijderen speciale tekens\n    doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc) # Verwijderen extra newlines\n    doc = re.sub(' +', ' ', doc) # Verwijderen overbodige whitespace\n    doc = unicodedata.normalize('NFKD', doc).encode('ascii', 'ignore').decode('utf-8', 'ignore') # verwijderen accented tekens\n    doc = nlp(doc)\n    doc = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in doc]) # Lemmatizeren van woorden\n    normalized_new_docs.append(doc)\nprint('New docs normalized')","b96adb6d":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntv = TfidfVectorizer(use_idf=True, min_df=10, max_df=0.5, smooth_idf = True)\ntv_train_features = tv.fit_transform(normalized_docs)\ntv_test_features = tv.transform(normalized_new_docs)\n\nprint('TFIDF model: Train features shape: {}'.format(tv_train_features.shape))\nprint('TFIDF model: Test features shape: {}'.format(tv_test_features.shape))","08355ed3":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nknn = KNeighborsClassifier(n_neighbors=99)\nknn.fit(tv_train_features, train_target)\ncv_scores = cross_val_score(knn, tv_train_features, train_target, cv=5)\nprint('CV Accuracy (5-fold) of K nearest neighbours:', cv_scores)\nprint('Mean CV Accuracy of K nearest neighbours:', np.mean(cv_scores))","7445c4ef":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\n\nmnb = MultinomialNB(alpha = 0.01)\nmnb.fit(tv_train_features, train_target)\ncv_scores = cross_val_score(mnb, tv_train_features, train_target, cv=5, scoring='f1')\nprint('CV Accuracy (5-fold) of Multinomial Naive Bayes:', cv_scores)\nprint('Mean CV Accuracy of Multinomial Naive Bayes:', np.mean(cv_scores))","afe18b7e":"from sklearn.naive_bayes import ComplementNB\n\ncnb = ComplementNB(alpha = 1)\ncnb.fit(tv_train_features, train_target)\ncv_scores = cross_val_score(cnb, tv_train_features, train_target, cv=5, scoring='f1')\nprint('CV Accuracy (5-fold) of Complement Naive Bayes:', cv_scores)\nprint('Mean CV Accuracy of Complement Naive Bayes:',np.mean(cv_scores))","b287add0":"from sklearn.svm import LinearSVC\n\nsvm = LinearSVC(penalty='l2', C=.1, random_state=42)\nsvm.fit(tv_train_features, train_target)\ncv_scores = cross_val_score(svm, tv_train_features, train_target, cv=5, scoring='f1')\nprint('CV Accuracy (5-fold) of Support Vector Machine:', cv_scores)\nprint('Mean CV Accuracy of Support Vector Machine:', np.mean(cv_scores))","f70fabc2":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier(max_iter=1000, tol=0.001)\nsgd.fit(tv_train_features, train_target)\ncv_scores = cross_val_score(sgd, tv_train_features, train_target, cv=5)\nprint('CV Accuracy (5-fold) of Support Vector Machine:', cv_scores)\nprint('Mean CV Accuracy of Support Vector Machine:', np.mean(cv_scores))","2b5c1a32":"conf = []\nlabels = []\nTHRESHOLD = 2\nfor doc in tv_test_features:\n    votes = 0\n    for clf in [svm, mnb, knn, cnb, sgd]:\n        vote = clf.predict(doc)\n        votes += vote\n    if votes > 2:\n        labels.append(1)\n        conf.append(votes\/5)\n    else:\n        labels.append(0)\n        conf.append(1 - votes\/5)","6a6c159f":"predictions = labels","bb687ae1":"output_df = pd.DataFrame({'id':test_df.id, 'target':predictions, 'confidence':conf})\noutput_df","134b9857":"output_df[['id','target']].to_csv('submission.csv', index=False)","4ef8d313":"## Vectorizing of the text data\nThe vectorization of both the train and test corpus takes place below. A sub-module of the Scikit-learn module [<sup>1<sup>](https:\/\/scikit-learn.org\/stable\/) is used for this. Scikit-learn is free *machine learning* module for the Python programming language. It features various classification, regression and clustering algorithms including support for vector machines, random forests, Naive Bayes and K-means, and is designed to work with the NumPy and SciPy numeric modules from Python.\n\nThe TfidfVectorizer that is present in one of the Scikit-learn sub-modules is used for the vectorisation of the tweets. This function transforms the raw texts into numerical vectors using the TF-IDF method. The vectors of the train documents are stored in the variable `tv_train_features` and the vectors of the test documents in the variable` tv_test_features`. You can find more information about how TfidfVectorizer works [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidf#sklearn.feature_extraction.text. Find TfidfVectorizer). More information about text feature extraction and transformation can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction).\n\nBelow you see that the train and test corpus consist of 7613 and 3263 tweets respectively and that 1362 unique terms were used for the vectorization.","f27ba17d":"## Linear models\nNext up, two models that could fall into the linear model category We start with a Support Vector Machine and then further develop the Stochastic Gradient Descent model. For more information about Support Vector Machines, see the Scikit-learn documentation, which can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/svm.html). And for more information about the Stochastic Gradient Descent you can find [here](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#stochastic-gradient-descent-sgd).\n### Support Vector Machine\nAs with the previous models, a model is loaded, trained and cross-validated below. For more information about the `LinearSVC` function click [here] (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html).","ce127b02":"## Combining the 5 classifiers in one vote classifier\nThe Vote Classifier is further coded below. Each document is classified by the 5 models trained above, after which the target with more than 2 votes is actually assigned to the document. The reliability scores are also calculated, ie how many votes a target has received divided by the total number of possible votes. Under the Vote Classifier you can see an example of the final output. This is then written to an pandas DataFrame for sumbission to the competition.","76bb8fae":"### Complement Naive Bayes\nBelow, a model is loaded, trained and cross-validated in the same way as with the previous models. For more information about the `ComplementNB` function click [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB).","e7500157":"### Stochastic Gradient Descent\nAs with the previous models, a model is loaded, trained and cross-validated below. For more information about the `SGDClassifier` function click [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html).","ceb3cf82":"# Creating and fitting the different single classifiers\nSubsequently, 5 models are loaded and trained using the variable tv_train_features and train_label_names using the Scikit-learn module. To check the accuracy of the models, a cross-validation is applied to each model so that the models can be set up properly. When the models are trained, they will be used later in the Vote Classifier.\n### K nearest neighbours\nThe first model is loaded below, namely a K-Nearest neighbor model. We also load a function which will help us with the cross-validation of the models.\nFor more information about the working methods of K-Nearest neighbors models see the Scikit-learn documentation, which can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/neighbors.html). For more information about the `KNeighborClassifier` function click [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).","e153d5b9":"## Traincorpus\nBelow the train corpus is loaded using the pandas module. The texts from the train corpus are stored in the variable `docs` and the labels of the tweets are stored in the variable `train_target`. An NLP model is then loaded from the spaCy module in order to be able to apply lematisation to the words in the normalization step. We also create 4 new functions which help us normalize the tweets, like expanding contractions and removing emojis. The objective of the normalization step is to clean the tweets. This is done with the help of the re and spaCy module. The normalized documents are stored in the variable `normalized_docs`.","474ec204":"# Real or Not? NLP with Disaster Tweets\n## Introduction\nTwitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\nI spent the last couple of months developing new text analysis skills. \nIn this notebook I try to use these skills to create a model that can predict as accurately as possible which tweets are about real disasters and which are not.\n\n## Libraries\nThe required libraries for classification are loaded below. Below you will find a list of the libraries used and a link to the documentation of these libraries with further explanation.\n\n* Numpy: An open source extension to the Python programming language with the goal of adding support for large, multi-dimensional arrays and matrices, along with a large library of math functions to work with these arrays.[<sup>1<sup>](https:\/\/numpy.org\/)\n* Pandas: An open source module with powerful, user-friendly data structures and data analysis tools for the Python programming language.[<sup>2<\/sup>](https:\/\/pandas.pydata.org\/)\n* spaCy: spaCy is an open-source software library for advanced natural language processing, written in the Python and Cython programming languages.[<sup>3<sup>](https:\/\/spacy.io\/)\n* re: This standard Python module offers regular expression matching operations[<sup>4<sup>](https:\/\/docs.python.org\/3\/library\/re.html)","b8712103":"## Naive Bayes\nNext up we load in two probabilistic models. We start with the Multinomial Naive Bayes model and then apply the Complement Naive Bayes model. For more global information about the working method of probabilistic models see the Scikit-learn documentation, which can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB).\n### Multinomial Naive Bayes\nBelow, a model is loaded, trained and cross-validated in the same way as with the previous model. For more information about the `MultinomialNB` function click [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB).","e23800a0":"## Testcorpus\nBelow the test corpus is loaded using the pandas module.The tweets are stored in the variable `new_docs`. The documents are then normalized in the exact same way as the text documents, after which they are stored in the variable new_docs_normalized."}}