{"cell_type":{"ca02ba14":"code","a83d5c3b":"code","033003ff":"code","ea023f97":"code","89968f9c":"code","627d08ce":"code","87554704":"code","3981807c":"code","0bc4f550":"code","faad6bf4":"code","483779f6":"code","0670218a":"code","83fd0d6c":"code","fc47fd1a":"code","31c2e3da":"code","52bfe3ef":"code","cae96833":"code","d2590e6b":"code","baad3f8a":"code","a127814b":"code","ed51a9fb":"code","84c55a47":"code","b1bcdc5c":"code","64943bc1":"markdown","2034752a":"markdown","dbf77d47":"markdown","bcfd7b38":"markdown","400c51a0":"markdown","7cb19a8e":"markdown","fdf8973c":"markdown","5ff8289a":"markdown","9dbcd18b":"markdown","01541f92":"markdown","6a25658f":"markdown"},"source":{"ca02ba14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a83d5c3b":"# \u6578\u64da\u5206\u6790\u7684 Class\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# \u5716\u50cf\u5316\u7684 Class\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# \u6a5f\u5668\u5b78\u7fd2\u7684 Class\nimport xgboost as xgb\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\n\n# \u7528\u4ee5\u986f\u793a\u544a\u8a34\u7684 Class\nfrom IPython.display import display\nfrom IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n# \u7528\u4ee5\u7121\u8996\u7a0b\u5f0f\u8b66\u544a\nimport warnings\nwarnings.filterwarnings(\"ignore\")","033003ff":"# \u8f09\u5165\u6578\u64da\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_data = df_train.append(df_test)","ea023f97":"df_data.head()","89968f9c":"# Convert Sex\ndf_data['Sex_Code'] = df_data['Sex'].map({'female' : 1, 'male' : 0}).astype('int')","627d08ce":"# Filling missing values\ndf_data['Fare'] = df_data['Fare'].fillna(df_data['Fare'].median())\n\n# Making Bins\ndf_data['FareBin_4'] = pd.qcut(df_data['Fare'], 4)\ndf_data['FareBin_5'] = pd.qcut(df_data['Fare'], 5)\ndf_data['FareBin_6'] = pd.qcut(df_data['Fare'], 6)\n\nlabel = LabelEncoder()\ndf_data['FareBin_Code_4'] = label.fit_transform(df_data['FareBin_4'])\ndf_data['FareBin_Code_5'] = label.fit_transform(df_data['FareBin_5'])\ndf_data['FareBin_Code_6'] = label.fit_transform(df_data['FareBin_6'])\n\n# cross tab\ndf_4 = pd.crosstab(df_data['FareBin_Code_4'],df_data['Pclass'])\ndf_5 = pd.crosstab(df_data['FareBin_Code_5'],df_data['Pclass'])\ndf_6 = pd.crosstab(df_data['FareBin_Code_6'],df_data['Pclass'])\n\ndisplay_side_by_side(df_4,df_5,df_6)\n\n# plots\nfig, [ax1, ax2, ax3] = plt.subplots(1, 3,sharey=True)\nfig.set_figwidth(18)\nfor axi in [ax1, ax2, ax3]:\n    axi.axhline(0.5,linestyle='dashed', c='black',alpha = .3)\ng1 = sns.factorplot(x='FareBin_Code_4', y=\"Survived\", data=df_data,kind='bar',ax=ax1)\ng2 = sns.factorplot(x='FareBin_Code_5', y=\"Survived\", data=df_data,kind='bar',ax=ax2)\ng3 = sns.factorplot(x='FareBin_Code_6', y=\"Survived\", data=df_data,kind='bar',ax=ax3)\n# close FacetGrid object\nplt.close(g1.fig)\nplt.close(g2.fig)\nplt.close(g3.fig)","87554704":"# Filling missing values\ndf_data['Age'] = df_data['Age'].fillna(df_data['Age'].median())\n\n# Making Bins\ndf_data['AgeBin_4'] = pd.qcut(df_data['Age'], 4,duplicates ='drop')\ndf_data['AgeBin_5'] = pd.qcut(df_data['Age'], 5,duplicates ='drop')\ndf_data['AgeBin_20'] = pd.qcut(df_data['Age'], 20,duplicates ='drop')\n\nlabel = LabelEncoder()\ndf_data['AgeBin_Code_4'] = label.fit_transform(df_data['AgeBin_4'])\ndf_data['AgeBin_Code_5'] = label.fit_transform(df_data['AgeBin_5'])\ndf_data['AgeBin_Code_20'] = label.fit_transform(df_data['AgeBin_20'])\n\n# cross tab\ndf_4 = pd.crosstab(df_data['AgeBin_Code_4'],df_data['Pclass'])\ndf_5 = pd.crosstab(df_data['AgeBin_Code_5'],df_data['Pclass'])\ndf_6 = pd.crosstab(df_data['AgeBin_Code_20'],df_data['Pclass'])\n\ndisplay_side_by_side(df_4,df_5,df_6)\n\n# plots\nfig, [ax1, ax2, ax3] = plt.subplots(1, 3,sharey=True)\nfig.set_figwidth(18)\nfor axi in [ax1, ax2, ax3]:\n    axi.axhline(0.5,linestyle='dashed', c='black',alpha = .3)\ng1 = sns.factorplot(x='AgeBin_Code_4', y=\"Survived\", data=df_data,kind='bar',ax=ax1)\ng2 = sns.factorplot(x='AgeBin_Code_5', y=\"Survived\", data=df_data,kind='bar',ax=ax2)\ng3 = sns.factorplot(x='AgeBin_Code_20', y=\"Survived\", data=df_data,kind='bar',ax=ax3)\n# close FacetGrid object\nplt.close(g1.fig)\nplt.close(g2.fig)\nplt.close(g3.fig)","3981807c":"# Ticket\ndf_data['Family_size'] = df_data['SibSp'] + df_data['Parch'] + 1","0bc4f550":"sns.countplot(df_data['Parch'], hue=df_data['Survived'])\ndisplay(df_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().round(3))","faad6bf4":"sns.countplot(df_data['SibSp'], hue=df_data['Survived'])\ndisplay(df_data[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().round(3))","483779f6":"deplicate_ticket = []\nfor tk in df_data.Ticket.unique():\n    tem = df_data.loc[df_data.Ticket == tk, 'Fare']\n    #print(tem.count())\n    if tem.count() > 1:\n        #print(df_data.loc[df_data.Ticket == tk,['Name','Ticket','Fare']])\n        deplicate_ticket.append(df_data.loc[df_data.Ticket == tk,['Name','Ticket','Fare','Cabin','Family_size','Survived']])\ndeplicate_ticket = pd.concat(deplicate_ticket)\ndeplicate_ticket.head(14)","0670218a":"# the same ticket family or friends\ndf_data['Connected_Survival'] = 0.5 # default \nfor _, df_grp in df_data.groupby('Ticket'):\n    if (len(df_grp) > 1):\n        for ind, row in df_grp.iterrows():\n            smax = df_grp.drop(ind)['Survived'].max()\n            smin = df_grp.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                df_data.loc[df_data['PassengerId'] == passID, 'Connected_Survival'] = 1\n            elif (smin==0.0):\n                df_data.loc[df_data['PassengerId'] == passID, 'Connected_Survival'] = 0\n#print\nprint('people keep the same ticket: %.0f '%len(deplicate_ticket))\nprint(\"people have connected information : %.0f\" \n      %(df_data[df_data['Connected_Survival']!=0.5].shape[0]))\ndf_data.groupby('Connected_Survival')[['Survived']].mean().round(3)","83fd0d6c":"df_data['Has_Age'] = df_data['Age'].isnull().map(lambda x : 0 if x == True else 1)\nfig, [ax1, ax2] = plt.subplots(1, 2)\nfig.set_figwidth(18)\nax1 = sns.countplot(df_data['Pclass'],hue=df_data['Has_Age'],ax=ax1)\nax2 = sns.countplot(df_data['Sex'],hue=df_data['Has_Age'],ax=ax2)\npd.crosstab(df_data['Has_Age'],df_data['Sex'],margins=True).round(3)","fc47fd1a":"# Masks\nMask_Has_Age_P12_Survived = ( (df_data.Has_Age == 1) & (df_data.Pclass != 3 ) & (df_data.Survived == 1) )\nMask_Has_Age_P12_Dead = ( (df_data.Has_Age == 1) & (df_data.Pclass != 3 ) & (df_data.Survived == 0) )\n# Plot\nfig, ax = plt.subplots( figsize = (15,9) )\nax = sns.distplot(df_data.loc[Mask_Has_Age_P12_Survived, 'Age'],kde=False,bins=10,norm_hist=True,label='Survived') \nax = sns.distplot(df_data.loc[Mask_Has_Age_P12_Dead, 'Age'],kde=False,bins=10,norm_hist=True,label='Dead')\nax.legend()\nax.set_title('Age vs Survived in Pclass = 1 and  2',fontsize = 20)","31c2e3da":"# extracted title using name\ndf_data['Title'] = df_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf_data['Title'] = df_data['Title'].replace(['Capt', 'Col', 'Countess', 'Don',\n                                               'Dr', 'Dona', 'Jonkheer', \n                                                'Major','Rev','Sir'],'Rare') \ndf_data['Title'] = df_data['Title'].replace(['Mlle', 'Ms','Mme'],'Miss')\ndf_data['Title'] = df_data['Title'].replace(['Lady'],'Mrs')\ndf_data['Title'] = df_data['Title'].map({\"Mr\":0, \"Rare\" : 1, \"Master\" : 2,\"Miss\" : 3, \"Mrs\" : 4 })\nTi = df_data.groupby('Title')['Age'].median()\nTi","52bfe3ef":"Ti_pred = df_data.groupby('Title')['Age'].median().values\ndf_data['Ti_Age'] = df_data['Age']\n# Filling the missing age\nfor i in range(0,5):\n # 0 1 2 3 4 5\n    df_data.loc[(df_data.Age.isnull()) & (df_data.Title == i),'Ti_Age'] = Ti_pred[i]\ndf_data['Ti_Age'] = df_data['Ti_Age'].astype('int')\ndf_data['Ti_Minor'] = ((df_data['Ti_Age']) < 16.0) * 1","cae96833":"minor = ['Sex_Code','Pclass','FareBin_Code_5','Connected_Survival','Ti_Minor']","d2590e6b":"# split training set the testing set\n# \u5206\u958b\u8fd4 training set \u540c test set\ndf_train = df_data[:len(df_train)]\ndf_test = df_data[len(df_train):]","baad3f8a":"# Training set and labels\nX = df_train.drop(labels=['Survived','PassengerId'],axis=1)\nY = df_train['Survived']","a127814b":"df_testX = df_test[minor]\ndf_trainX = X[minor]","ed51a9fb":"# Using default parameter\nmodel1 = xgb.XGBClassifier()\nmodel1.fit(df_trainX, Y)\nans = model1.predict(df_testX)\nsubmit = pd.DataFrame({\"PassengerId\": df_test['PassengerId'],\n                      \"Survived\":ans.astype(int)})\nsubmit.to_csv(\"my_submission_default.csv\",index=False)\n\nprint(\"Your submission was successfully saved!\")","84c55a47":"# Submission with hyperparameter tunning\nimport time\n\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\n\nx_train,x_valid ,y_train, y_valid=  train_test_split(df_trainX, Y, test_size=0.33, random_state=42)\n\nclf = xgb.XGBClassifier()\n\nparam_grid = {\n        'silent': [False],\n        'max_depth': [6, 10, 15, 20],\n        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n        'gamma': [0, 0.25, 0.5, 1.0],\n        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n        'n_estimators': [100]}\n\nfit_params = {'eval_metric': 'mlogloss',\n              'early_stopping_rounds': 10,\n              'eval_set': [(x_valid, y_valid)]}\n\nrs_clf = RandomizedSearchCV(clf, param_grid, n_iter=20,\n                            n_jobs=1, verbose=2, cv=2,\n                            scoring='neg_log_loss', refit=False, random_state=42)\nprint(\"Randomized search..\")\nsearch_time_start = time.time()\nrs_clf.fit(x_train, y_train)\nprint(\"Randomized search time:\", time.time() - search_time_start)\n\nbest_score = rs_clf.best_score_\nbest_params = rs_clf.best_params_\nprint(\"Best score: {}\".format(best_score))\nprint(\"Best params: \")\nfor param_name in sorted(best_params.keys()):\n    print('%s: %r' % (param_name, best_params[param_name]))","b1bcdc5c":"# Using default parameter\nmodel2 = xgb.XGBClassifier(max_depth=20,\n                           min_child_weight=1,\n                           learning_rate=0.1,\n                           n_estimators=100,\n                           silent=True,\n                           objective='binary:logistic',\n                           gamma=0.25,\n                           max_delta_step=0,\n                           subsample=1,\n                           colsample_bytree=0.8,\n                           colsample_bylevel=0.8,\n                           reg_alpha=0,\n                           reg_lambda=0,\n                           scale_pos_weight=1,\n                           seed=1,\n                           missing=None)\nmodel2.fit(df_trainX, Y)\nans = model2.predict(df_testX)\nsubmit = pd.DataFrame({\"PassengerId\": df_test['PassengerId'],\n                      \"Survived\":ans.astype(int)})\nsubmit.to_csv(\"my_submission_tunning.csv\",index=False)\n\nprint(\"Your submission was successfully saved!\")","64943bc1":"# This Notebook is mainly focus on XGboost hyperparameter optimization. For EDA, please click on the link above to find more detail. ","2034752a":"# The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","dbf77d47":"## The Public Score without tunning is : 0.81339\n## The Public Score with  tunning is   : 0.81818 (+0.00479)\n","bcfd7b38":"## \u9023\u7d50(Connected_Survival)\n\n\u9019\u500b\u7279\u5fb5\u76f8\u7576\u6709\u610f\u601d\uff0c\u4e3b\u8981\u662f\u767c\u73fe\u4e86\u4e58\u5ba2\u6301\u6709\u76f8\u540c\u7684\u8239\u7968\u610f\u5473\u8457\u4ed6\u5011\u53ef\u80fd\u662f\u5bb6\u4eba\u6216\u662f\u670b\u53cb\uff0c\u800c\u5728\u8a13\u7df4\u96c6\u4e0a\u9019\u4e9b\u4e92\u76f8\u6709\u9023\u7d50\u7684\u4eba\u5e38\u5e38\u662f\u4e00\u8d77\u6d3b\u4e0b\u4f86\u6216\u662f\u4e00\u8d77\u55aa\u547d\uff0c\u6211\u5011\u5f9e\u7968\u6839\u7684\u7279\u5fb5Ticket\u958b\u59cb\u770b\u8d77\u5728891\u500b\u7968\u6839\u8cc7\u8a0a\u4e2d\uff0c\u7368\u7acb\u7684\u6709681\u9805\uff0c\u9019\u8868\u793a\u4e00\u5b9a\u6709\u4e58\u5ba2\u662f\u6301\u6709\u76f8\u540c\u7684\u7968\u6839\uff0c\u9019\u610f\u5473\u8457\u4ed6\u5011\u53ef\u80fd\u4e00\u8d77\u5206\u4eab\u67d0\u4e00\u5340\u7684\u5ea7\u4f4d......\n\nFrom https:\/\/medium.com\/@yulongtsai\/https-medium-com-yulongtsai-titanic-top3-8e64741cc11f","400c51a0":"# Goal\nIt is your job to predict if a passenger survived the sinking of the Titanic or not.\nFor each in the test set, you must predict a 0 or 1 value for the variable.\n\n# Metric\nYour score is the percentage of passengers you correctly predict. This is known as accuracy.","7cb19a8e":"#### \u95dc\u65bc\u7968\u50f9\uff0c\u7968\u50f9\u5177\u6709\u9023\u7e8c\u6027\uff0c\u6240\u4ee5\u6211\u5730\u5c07\u7968\u50f9\u5206\u5272\u6703\u5e7e\u4efd\u3002\n\u9996\u5148\uff0c\u6211\u5730\u7528\u6578\u64da\u65e2Median \u88dc\u56de\u6578\u64da\u5305\u5165\u9762\u7f3a\u5c11\u65e2\u90e8\u4efd\u3002\n\u4e4b\u5f8c\u6211\u5730\u5206\u5272\u7968\u50f9\uff0c\u6211\u5730\u5206\u5272\u5e74\u9f61\u6703 4\u4efd, 5\u4efd, 6\u4efd\u3002\n\u518d\u4ee5\u5716\u8868\u8868\u793a\uff0c\u767c\u73fe\u5206\u5c31 6\u4efd\u65e2\u6642\u5019\uff0c\u5176\u4e2d\u4e00\u7d44\u65e2\u751f\u5b58\u7387\u8d85\u904e50%","fdf8973c":"#### \u95dc\u65bc\u5e74\u9f61\uff0c\u5e74\u9f61\u5177\u6709\u9023\u7e8c\u6027\uff0c\u6240\u4ee5\u6211\u5730\u5c07\u5e74\u9f61\u5206\u5272\u6703\u5e7e\u4efd\u3002\n\u9996\u5148\uff0c\u6211\u5730\u7528\u6578\u64da\u65e2Median \u88dc\u56de\u6578\u64da\u5305\u5165\u9762\u7f3a\u5c11\u65e2\u90e8\u4efd\u3002\n\u4e4b\u5f8c\u6211\u5730\u5206\u5272\u5e74\u9f61\uff0c\u6211\u5730\u5206\u5272\u5e74\u9f61\u6703 4\u4efd, 5\u4efd, 20\u4efd\u3002\n\u518d\u4ee5\u5716\u8868\u8868\u793a\uff0c\u767c\u73fe\u5206\u5c31 20\u4efd\u65e2\u6642\u5019\uff0c\u5176\u4e2d\u4e00\u7d44\u65e2\u751f\u5b58\u7387\u8d85\u904e50%","5ff8289a":"[Reference material_\u6a5f\u5668\u5b78\u7fd2\u5c08\u6848 Kaggle\u7af6\u8cfd-\u9435\u9054\u5c3c\u865f\u751f\u5b58\u9810\u6e2c(Top 3%)](https:\/\/medium.com\/@yulongtsai\/https-medium-com-yulongtsai-titanic-top3-8e64741cc11f)","9dbcd18b":"#### \u6e2f\u53e3\u6709\u7f3a\u5c11\u597d\u5c11\u91cf\u6578\u64da\uff0c\u6211\u76f4\u63a5\u586b\u88dc\u70ba\u6700\u591a\u4eba\u4e0a\u8239\u65e2\u6e2f\u53e3 'S'\u3002\u4e4b\u5f8c\u6211\u5730\u518d\u4ee5\u5716\u8868\u7747\u4e0b\u6e2f\u53e3\u540c\u751f\u5b58\u7387\u7684\u95dc\u4fc2\uff0c\u767c\u89ba\u6e2f\u53e3'C'\u65e2\u4eba\u670955%\u751f\u5b58\u7387\u3002\n\u56e0\u70ba\u751f\u5b58\/\u6b7b\u4ea1\u4fc2\u4e8c\u5143\u5206\u4f48\uff0c\u6240\u4ee5\u7279\u5fb5\u8207\u751f\u5b58\u7387\u7684\u95dc\u4fc2\u8981\u8d85\u904e50%\u4ee5\u4e0a\u624d\u53ef\u4ee5\u8aaa\u9019\u500b\u7279\u5fb5\u5c0d\u751f\u5b58\u7387\u78ba\u5be6\u6709\u91cd\u5927\u65e2\u5f71\u97ff\u3002","01541f92":"#### \u8239\u8259\uff0c\u8239\u8259\u65e2\u540d\u7a31\u6709\u591a\u53c8\u6709\u5514\u4e00\u6a23\uff0c\u53c8\u6709\u7f3a\u5c11\u6578\u64da\uff0c\u6211\u5730\u5148\u5c07\u6578\u64da\u88dc\u56de\u4f86\uff0c\u6211\u5730\u4ee5'X'(\u7121\u6578\u64da)\uff0c\u4e4b\u5f8c\u6211\u5730\u5c07\u8239\u8259\u540d\u7a31\u5b57\u982d\u8868\u793a\u8239\u8259\u3002\u518d\u4ee5\u5716\u8868\u986f\u793a\uff0c\u767c\u73fe\u67d0\u5e7e\u500b\u8239\u8259\u7684\u751f\u5b58\u7387\u6bd4\u8f03\u9ad8\u3002","6a25658f":"# EDA\n\u4eca\u6b21\u4f7f\u7528 Xgboost with bayesian-optimization \uff0c\u9078\u53d6\u7684 Features \u6709\u6027\u5225\uff0cPclass, \u7968\u50f9\uff0cConnectection\uff0cTi_Minor\u3002"}}