{"cell_type":{"1451dbca":"code","8a1fa642":"code","b8f6b605":"code","6159504b":"code","43039648":"code","b28e3b5b":"code","e9bee9d7":"code","8b9f8685":"code","39b37d2a":"code","da0e5ce2":"code","b5dde785":"code","02a38c70":"code","eb25131c":"code","e5ce546a":"code","79a74917":"code","d1ed7b13":"code","1ffeaab0":"code","624e3f6d":"code","941aab6e":"code","218403a8":"code","15d37e40":"code","1fc73202":"code","f8da5a86":"code","de8024a8":"code","26c8f9e9":"code","50a69fbf":"code","1f55dfee":"code","54bb19ba":"code","6d341d3d":"code","055a003e":"code","0dc3d3aa":"code","f804e16f":"code","bd766aab":"code","67b76f3f":"code","74e253e6":"code","8a1a1d81":"code","87703031":"code","cd8395b7":"code","80315d9f":"code","eaac35d7":"code","8aaaa43b":"code","5dbd10b5":"code","bf1e819c":"markdown","22415beb":"markdown","fa6e1cef":"markdown","d6883440":"markdown","3f399e7f":"markdown","f1311e22":"markdown","89970375":"markdown","2965b86e":"markdown","dcfb0d58":"markdown","443d9840":"markdown","20c0390c":"markdown","1ec59ccc":"markdown","d294ac88":"markdown"},"source":{"1451dbca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a1fa642":"# importing essential libraries\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\nfrom sklearn.metrics import precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b8f6b605":"# reading data\ndat = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","6159504b":"# getting columns\ndat.columns","43039648":"dat","b28e3b5b":"# describing data\ndat.describe()","e9bee9d7":"# identifying correlation\ndat.corr()","8b9f8685":"# identifying skewness in data\ndat.skew()","39b37d2a":"# checking for null values\ndat.isnull().sum()","da0e5ce2":"# checking for duplicate values\ndat.duplicated().sum()","b5dde785":"g = sns.countplot(dat['DEATH_EVENT'])#checking for class imbalance\ng.set_xticklabels(['0','1'])\nplt.show()","02a38c70":"def plot_numeric_features(feature):#code to visualize distribution, scatterplot and boxplot\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5), dpi=110)\n    \n    sns.distplot(dat[feature], ax=ax1)\n    sns.scatterplot(dat[feature], dat[\"DEATH_EVENT\"], ax=ax2)\n    sns.boxplot(dat[feature],orient='h', ax=ax3, width=0.2)\n\n    print(f\"Skewness Coefficient of {feature} is {dat[feature].skew():.2f}\")\n    ax1.set_yticks([])\n    \n    return plt","eb25131c":"import warnings\nwarnings.filterwarnings('ignore')","e5ce546a":"plot_numeric_features(\"age\").show()\n","79a74917":"plot_numeric_features(\"anaemia\").show()","d1ed7b13":"plot_numeric_features(\"creatinine_phosphokinase\").show()","1ffeaab0":"plot_numeric_features(\"diabetes\").show()","624e3f6d":"plot_numeric_features(\"ejection_fraction\").show()","941aab6e":"plot_numeric_features(\"high_blood_pressure\").show()","218403a8":"plot_numeric_features(\"platelets\").show()","15d37e40":"plot_numeric_features(\"serum_creatinine\").show()","1fc73202":"plot_numeric_features(\"serum_sodium\").show()","f8da5a86":"plot_numeric_features(\"sex\").show()","de8024a8":"plot_numeric_features(\"smoking\").show()","26c8f9e9":"plot_numeric_features(\"time\").show()","50a69fbf":"dat=dat[dat['creatinine_phosphokinase']<2000] #removing outliers in creatinine_phosphokinase","1f55dfee":"dat=dat[dat['ejection_fraction']<65] #removing outliers in ejection_fraction","54bb19ba":"dat=dat[(dat['platelets']>100000) & (dat['platelets']<450000)] #removing outliers in platelets","6d341d3d":"dat=dat[dat['serum_creatinine']<2] #removing outliers in serum_creatinine","055a003e":"dat=dat[dat['serum_sodium']>126] #removing outliers in serum_sodium","0dc3d3aa":"dat","f804e16f":"dat.skew()# skew can be seen reduced after removing outliers","bd766aab":"g = sns.countplot(dat['DEATH_EVENT']) #checking imbalance after removing outliers\ng.set_xticklabels(['0','1'])\nplt.show()","67b76f3f":"x = dat[[c for c in dat.columns if c != 'DEATH_EVENT']] #separating features\ny = dat['DEATH_EVENT']#separating target","74e253e6":"from imblearn.over_sampling import SMOTE #importing important libraries for oversampling data\nfrom collections import Counter\n\nsmote = SMOTE()\n\n# fit predictor and target variable\nx_smote, y_smote = smote.fit_resample(x, y)\n\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_smote))","8a1a1d81":"g = sns.countplot(y_smote) #again plotting for imbalance\ng.set_xticklabels(['0','1'])\nplt.show()","87703031":"from sklearn.model_selection import train_test_split #splitting data\n\nX_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.4, random_state=0)\nX_train.shape","cd8395b7":"from sklearn.preprocessing import MinMaxScaler #scaling all the features\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n#print(X_train_scaled)\n","80315d9f":"# importing various libraries\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n","eaac35d7":"# creating instances of classifiers\ncla = {\n\"LogisticRegression\":LogisticRegression(),\n'Random_Forest': RandomForestClassifier(n_estimators=100),\n'Gradient_Boosting': GradientBoostingClassifier(max_depth=5),\n'K_NN': KNeighborsClassifier(n_neighbors=5, weights='distance'),\n\"ADABOOST\":AdaBoostClassifier(random_state=1),\n\"xgboost\":xgb.XGBClassifier(random_state=1,learning_rate=0.01),\n\"CatBoost\":CatBoostClassifier()\n\n}","8aaaa43b":"#y_test","5dbd10b5":"#calculating accuracies and recall\nfrom sklearn.metrics import recall_score\nfor name, model in cla.items():\n    model.fit(X_train_scaled, y_train)\n    y_test_pre = model.predict(X_test_scaled)\n    print(model.score(X_test_scaled,y_test))\n    print(recall_score(y_test, y_test_pre))\n    print('-----------')\n","bf1e819c":"as data is still imbalanced we will need to do either oversampling or undersampling in oversampling we increase the datapoints in the class where data present is less and in undersampling we decrease the amount of datapoints present in the class where data present is more.\n\nas of now we will do oversampling by making synthetic data which is obtained through k nearest neighbour trick","22415beb":"clearly in above plot their are outliers(outliers are the random noise in the data which disturbs our prediction. anything outside the rightmost whisker of box-plot is outlier as you can see points lying outside rightmost whiskers these points are outliers)\n\nhere any point after 2000 is outlier","fa6e1cef":"above it can be seen that class \"zero\" is dominant and thus this class imbalance can harm our prediction so we have to counter it","d6883440":"### Plotting\nhere we will try to plot distribution and boxplot of all the features and scatterplot of DEATH_EVENT and features. this plotting will help us to remove outliers","3f399e7f":"clearly in above plot their are outliers(outliers are the random noise in the data which disturbs our prediction. anything outside the rightmost whisker of box-plot is outlier as you can see points lying outside rightmost whiskers these points are outliers)\n\nhere any point after 65 is outlier","f1311e22":"### Here prime focus will be on recall as we cannot tolerate false negative(i.e we dont want a person with heart failure risk to be predicted as healthy)","89970375":"here we will remove outliers","2965b86e":"clearly in above plot their are outliers(outliers are the random noise in the data which disturbs our prediction. anything outside the rightmost whisker of box-plot is outlier as you can see points lying outside rightmost whiskers these points are outliers)\n\nhere any point after 2 is outlier","dcfb0d58":"# Reading and Visualization the Data\u00b6\n","443d9840":"let's check for skew agian","20c0390c":"clearly in above plot their are outliers(outliers are the random noise in the data which disturbs our prediction. anything outside the rightmost whisker of box-plot is outlier as you can see points lying outside rightmost whiskers these points are outliers)\n\nhere any point before 126 is outlier","1ec59ccc":"# Working on various models","d294ac88":"clearly in above plot their are outliers(outliers are the random noise in the data which disturbs our prediction. anything outside the rightmost whisker of box-plot is outlier as you can see points lying outside rightmost whiskers these points are outliers)\n\nhere any point after 100000 and below 450000 is outlier"}}