{"cell_type":{"9b6e04d8":"code","ede8dffa":"code","fc4c34b0":"code","cc496338":"code","b31eda74":"code","efefbc72":"code","0a684c8f":"code","47b18d1f":"code","b1311a88":"code","fde90010":"code","8e90c466":"markdown","64668c7e":"markdown","3391b836":"markdown"},"source":{"9b6e04d8":"%%capture\nimport time\nstart = time.time()\n!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq\n!pip install -U --no-build-isolation --no-deps ..\/input\/transformers-master\/ -qq","ede8dffa":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\ntorch.set_grad_enabled(False)\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering,AutoConfig, AutoModel, XLMRobertaTokenizerFast\nfrom datasets import Dataset\nfrom chaii_utils import prepare_validation_features, postprocess_qa_predictions\nfrom tqdm.notebook import tqdm\nfrom sklearn import preprocessing\nimport torch.nn.functional as F\n# from torch.optim.swa_utils import AveragedModel\nimport collections\nimport re\nimport os\nfrom glob import glob\n# pretrained_paths = ['..\/input\/rembert-pt']\n# pretrained_paths = ['..\/input\/infoxlm-large-squad2']\npretrained_paths = ['..\/input\/rembert-pt', '..\/input\/muril-large-pt\/muril-large-cased', '..\/input\/infoxlm-large-squad2', '..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2']\n\n# model_paths = ['..\/input\/0829-rembert']\n# model_paths = ['..\/input\/xlm-roberta-4']\nmodel_paths = ['..\/input\/0829-rembert', '..\/input\/muril-ep1-full', '..\/input\/xlm-roberta-4', '..\/input\/rob-lr1e-5-wd0-do01-ds05']\nmodel_exts = [[], [2], [], [3,4]]\nmymodels = [False,False,False,False]\nmax_lengths = [512,512,512,512]\ndoc_strides = [128,128,128,128]\nweights = [1,1,1,1]\n\n# pretrained_paths = ['..\/input\/muril-large-pt\/muril-large-cased']\n# model_paths = ['..\/input\/muril-ep1-full']\n# model_exts = [[0,1,2,3]]\n# mymodels = [False]\n# max_lengths = [512]\n# doc_strides = [128]\n# weights = [1]\n\ndevice = 'cuda'\ndosoftmax = False","fc4c34b0":"def my_mean(list_of_lists):\n    maxlen = max([len(l) for l in list_of_lists])\n    for i in range(len(list_of_lists)):\n        while len(list_of_lists[i]) < maxlen:\n            list_of_lists[i].append(np.nan)\n    return np.nanmax(list_of_lists, axis=1)\n\ndef get_char_logits(examples, features, raw_predictions):\n    all_start_logits, all_end_logits = raw_predictions\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    all_char_start_logits = {}\n    all_char_end_logits = {}\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n        context = example[\"context\"]\n        char_start_logits = [[] for _ in range(len(context))]\n        char_end_logits = [[] for _ in range(len(context))]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            for token_index, om in enumerate(offset_mapping):\n                if om is None:\n                    continue\n                start_char_idx, end_char_idx = om[0], om[1]\n                for char_index in range(start_char_idx, end_char_idx):\n                    char_start_logits[char_index].append(start_logits[token_index])\n                    char_end_logits[char_index].append(end_logits[token_index])\n        char_start_logits = my_mean(char_start_logits)\n        char_start_logits[np.isnan(char_start_logits)] = 0\n        char_end_logits = my_mean(char_end_logits)\n        char_end_logits[np.isnan(char_end_logits)] = 0\n        all_char_start_logits[example['id']] = char_start_logits\n        all_char_end_logits[example['id']] = char_end_logits\n    \n    return all_char_start_logits, all_char_end_logits\n\ndef beam_search(char_start_logits, char_end_logits, beam_size=20, max_len=100):\n    beam_start_indices = np.argsort(char_start_logits)[-beam_size:]\n    beam_end_indices = np.argsort(char_end_logits)[-beam_size:]\n    idx_pairs = []\n    scores = []\n    for sidx in beam_start_indices:\n        for eidx in beam_end_indices:\n            leng = eidx - sidx\n            if leng > 0 and leng < max_len:\n                idx_pairs.append([sidx, eidx])\n                scores.append(char_start_logits[sidx]+char_end_logits[eidx])\n    best_score = -1e5\n    for i, score in enumerate(scores):\n        if score > best_score:\n            best_score = score\n            besti = i\n    if best_score == -1e5:\n        return [0, 0]\n    return idx_pairs[besti]\n\ndef remove_consdup(a, reserve_head=True):\n    array = a if reserve_head else np.flip(a)\n    last_ele = 1e5\n    for i in range(len(array)):\n        if array[i] != last_ele:\n            last_ele = array[i]\n        else:\n            array[i] = -1e5\n    return array if reserve_head else np.flip(array)\n\ndef get_predictions(examples, all_char_start_logits, all_char_end_logits):\n    predictions = {}\n    for example_index, example in enumerate(tqdm(examples)):\n        id_ = example['id']\n        context = example[\"context\"]\n        char_start_logits = remove_consdup(all_char_start_logits[id_], reserve_head=True)\n        char_end_logits = remove_consdup(all_char_end_logits[id_], reserve_head=False)\n        best_start_idx, best_end_idx = beam_search(char_start_logits, char_end_logits, max_len=100)\n        predictions[id_] = context[best_start_idx:best_end_idx+1]\n    return predictions\n\ndef left_strip(s):\n    reg = [' ', '\\n', '(', ')', '.', ',', \"'\", '\\t', '''\"''', '[', ']', '-', '_', '!', '?', '#', '*']\n    while True:\n        if (s[0] not in reg) or (len(s) < 2):\n            break\n        s = s[1:]\n    return s\n\ndef right_strip(s):\n    reg = [' ', '\\n', '(', ')', '.', ',', \"'\", '\\t', '''\"''', '[', ']', '-', '_', '!', '?', '#', '*']\n    while True:\n        if (s[-1] not in reg) or (len(s) < 2):\n            break\n        s = s[:-1]\n    return s\n\ndef find_year(s):\n    match = re.match(r'.*([1-3][0-9]{3})', s)\n    if match is None:\n        return s\n    y = match.group(1)\n    if y + ' \u0908.' in s:\n        return y + ' \u0908.'\n    if y + ' \u0908.\u092a\u0942.' in s:\n        return y + ' \u0908.\u092a\u0942.'\n    return y\n\ndef find_year_v2(s):\n    words = s.split()\n    year_words = []\n    for w in words:\n        if len(w) == 4 and w.isdigit():\n            year_words.append(w)\n    if year_words:\n        s1 = ' '.join(year_words)\n        if s1 + ' \u0908.' in s:\n            return s1 + ' \u0908.'\n        if s1 + ' \u0908.\u092a\u0942.' in s:\n            return s1 + ' \u0908.\u092a\u0942.'\n        return s1\n    return s\n\ndef get_number(s):\n    s1 = [c for c in s if c.isdigit() or c in [' ', ',']]\n    s2 = ''\n    for c in s1:\n        if s2 + c in s:\n            s2 += c\n        else:\n            break\n    if not s2:\n        return s\n    return s2\n\ndef get_country(s):\n    tamil_countries = ['\u0b86\u0baa\u0bcd\u0b95\u0bbe\u0ba9\u0bbf\u0bb8\u0bcd\u0ba4\u0bbe\u0ba9\u0bcd', '\u0b85\u0bb2\u0bcd\u0baa\u0bc7\u0ba9\u0bbf\u0baf\u0bbe', '\u0b85\u0bb2\u0bcd\u0b9c\u0bc0\u0bb0\u0bbf\u0baf\u0bbe', '\u0b85\u0bae\u0bc6\u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0ba9\u0bcd \u0b9a\u0bae\u0bcb\u0bb5\u0bbe',\n       '\u0b85\u0ba9\u0bcd\u0b9f\u0bcb\u0bb0\u0bbe', '\u0b85\u0b99\u0bcd\u0b95\u0bcb\u0bb2\u0bbe', '\u0b85\u0b99\u0bcd\u0b95\u0bc1\u0baf\u0bbf\u0bb2\u0bcd\u0bb2\u0bbe', '\u0b86\u0ba3\u0bcd\u0b9f\u0bbf\u0b95\u0bc1\u0bb5\u0bbe & \u0baa\u0bbe\u0bb0\u0bcd\u0baa\u0bc1\u0b9f\u0bbe',\n       '\u0b85\u0bb0\u0bcd\u0b9c\u0bc6\u0ba9\u0bcd\u0b9f\u0bbf\u0ba9\u0bbe', '\u0b86\u0bb0\u0bcd\u0bae\u0bc7\u0ba9\u0bbf\u0baf\u0bbe', '\u0b85\u0bb0\u0bc1\u0baa\u0bbe', '\u0b86\u0bb8\u0bcd\u0ba4\u0bbf\u0bb0\u0bc7\u0bb2\u0bbf\u0baf\u0bbe', '\u0b86\u0bb8\u0bcd\u0ba4\u0bbf\u0bb0\u0bbf\u0baf\u0bbe',\n       '\u0b85\u0b9c\u0bb0\u0bcd\u0baa\u0bc8\u0b9c\u0bbe\u0ba9\u0bcd', '\u0baa\u0bb9\u0bbe\u0bae\u0bbe\u0bb8\u0bcd, \u0ba4\u0bbf', '\u0baa\u0bb9\u0bcd\u0bb0\u0bc8\u0ba9\u0bcd', '\u0baa\u0b99\u0bcd\u0b95\u0bb3\u0bbe\u0ba4\u0bc7\u0bb7\u0bcd',\n       '\u0baa\u0bbe\u0bb0\u0bcd\u0baa\u0b9f\u0bbe\u0bb8\u0bcd', '\u0baa\u0bc6\u0bb2\u0bbe\u0bb0\u0bb8\u0bcd', '\u0baa\u0bc6\u0bb2\u0bcd\u0b9c\u0bbf\u0baf\u0bae\u0bcd', '\u0baa\u0bc6\u0bb2\u0bbf\u0bb8\u0bcd', '\u0baa\u0bc6\u0ba9\u0bbf\u0ba9\u0bcd',\n       '\u0baa\u0bc6\u0bb0\u0bcd\u0bae\u0bc1\u0b9f\u0bbe', '\u0baa\u0bc2\u0b9f\u0bbe\u0ba9\u0bcd', '\u0baa\u0bca\u0bb2\u0bbf\u0bb5\u0bbf\u0baf\u0bbe', '\u0baa\u0bcb\u0bb8\u0bcd\u0ba9\u0bbf\u0baf\u0bbe & \u0bb9\u0bc6\u0bb0\u0bcd\u0b9a\u0b95\u0bcb\u0bb5\u0bbf\u0ba9\u0bbe',\n       '\u0baa\u0bcb\u0b9f\u0bcd\u0bb8\u0bcd\u0bb5\u0bbe\u0ba9\u0bbe', '\u0baa\u0bbf\u0bb0\u0bc7\u0b9a\u0bbf\u0bb2\u0bcd', '\u0baa\u0bbf\u0bb0\u0bbf\u0b9f\u0bcd\u0b9f\u0bbf\u0bb7\u0bcd \u0bb5\u0bbf\u0bb0\u0bcd\u0b9c\u0bbf\u0ba9\u0bcd \u0b87\u0bb8\u0bcd. ', '\u0baa\u0bc1\u0bb0\u0bc1\u0ba9\u0bc7',\n       '\u0baa\u0bb2\u0bcd\u0b95\u0bc7\u0bb0\u0bbf\u0baf\u0bbe', '\u0baa\u0bc1\u0bb0\u0bcd\u0b95\u0bbf\u0ba9\u0bbe \u0baa\u0bbe\u0b9a\u0bcb', '\u0baa\u0bb0\u0bcd\u0bae\u0bbe', '\u0baa\u0bc1\u0bb0\u0bc1\u0ba3\u0bcd\u0b9f\u0bbf', '\u0b95\u0bae\u0bcd\u0baa\u0bcb\u0b9f\u0bbf\u0baf\u0bbe',\n       '\u0b95\u0bc7\u0bae\u0bb0\u0bc2\u0ba9\u0bcd', '\u0b95\u0ba9\u0b9f\u0bbe', '\u0b95\u0bc7\u0baa\u0bcd \u0bb5\u0bc6\u0bb0\u0bcd\u0b9f\u0bc7', '\u0b95\u0bc7\u0bae\u0ba9\u0bcd \u0ba4\u0bc0\u0bb5\u0bc1\u0b95\u0bb3\u0bcd',\n       '\u0bae\u0ba4\u0bcd\u0ba4\u0bbf\u0baf \u0b86\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95 \u0baa\u0bbf\u0bb0\u0ba4\u0bbf\u0ba8\u0bbf\u0ba4\u0bbf', '\u0b9a\u0bbe\u0b9f\u0bcd', '\u0b9a\u0bbf\u0bb2\u0bbf', '\u0b9a\u0bc0\u0ba9\u0bbe', '\u0b95\u0bca\u0bb2\u0bae\u0bcd\u0baa\u0bbf\u0baf\u0bbe',\n       '\u0b95\u0bca\u0bae\u0bb0\u0bcb\u0bb8\u0bcd', '\u0b95\u0bbe\u0b99\u0bcd\u0b95\u0bcb, \u0b9f\u0bc6\u0bae\u0bcd. \u0baa\u0bbf\u0bb0\u0ba4\u0bbf\u0ba8\u0bbf\u0ba4\u0bbf. ', '\u0b95\u0bbe\u0b99\u0bcd\u0b95\u0bcb, \u0b95\u0bc1\u0b9f\u0bbf\u0baf\u0bb0\u0b9a\u0bc1. \u0b87\u0ba9\u0bcd ',\n       '\u0b95\u0bc1\u0b95\u0bcd \u0ba4\u0bc0\u0bb5\u0bc1\u0b95\u0bb3\u0bcd', '\u0b95\u0bcb\u0bb8\u0bcd\u0b9f\u0bbe \u0bb0\u0bbf\u0b95\u0bbe', \"\u0b95\u0bcb\u0b9f\u0bcd \u0b9f\u0bbf \u0b90\u0bb5\u0bb0\u0bbf\", '\u0b95\u0bc1\u0bb0\u0bcb\u0bb7\u0bbf\u0baf\u0bbe',\n       '\u0b95\u0bbf\u0baf\u0bc2\u0baa\u0bbe', '\u0b9a\u0bc8\u0baa\u0bcd\u0bb0\u0bb8\u0bcd', '\u0b9a\u0bc6\u0b95\u0bcd \u0b95\u0bc1\u0b9f\u0bbf\u0baf\u0bb0\u0b9a\u0bc1', '\u0b9f\u0bc6\u0ba9\u0bcd\u0bae\u0bbe\u0bb0\u0bcd\u0b95\u0bcd', '\u0b9c\u0bbf\u0baa\u0bc2\u0b9f\u0bcd\u0b9f\u0bbf',\n       '\u0b9f\u0bca\u0bae\u0bbf\u0ba9\u0bbf\u0b95\u0bbe', '\u0b9f\u0bca\u0bae\u0bbf\u0ba9\u0bbf\u0b95\u0ba9\u0bcd \u0b95\u0bc1\u0b9f\u0bbf\u0baf\u0bb0\u0b9a\u0bc1', '\u0b95\u0bbf\u0bb4\u0b95\u0bcd\u0b95\u0bc1 \u0ba4\u0bbf\u0bae\u0bcb\u0bb0\u0bcd', '\u0b88\u0b95\u0bcd\u0bb5\u0b9f\u0bbe\u0bb0\u0bcd',\n       '\u0b8e\u0b95\u0bbf\u0baa\u0bcd\u0ba4\u0bc1', '\u0b8e\u0bb2\u0bcd \u0b9a\u0bbe\u0bb2\u0bcd\u0bb5\u0b9f\u0bbe\u0bb0\u0bcd', '\u0b8e\u0b95\u0bcd\u0b95\u0bc1\u0bb5\u0b9f\u0bcb\u0bb0\u0bbf\u0baf\u0bb2\u0bcd \u0b95\u0bbf\u0ba9\u0bbf\u0baf\u0bbe', '\u0b8e\u0bb0\u0bbf\u0ba4\u0bcd\u0bb0\u0bbf\u0baf\u0bbe',\n       '\u0b8e\u0bb8\u0bcd\u0b9f\u0bcb\u0ba9\u0bbf\u0baf\u0bbe', '\u0b8e\u0ba4\u0bcd\u0ba4\u0bbf\u0baf\u0bcb\u0baa\u0bcd\u0baa\u0bbf\u0baf\u0bbe', '\u0b83\u0baa\u0bbe\u0bb0\u0bcb \u0ba4\u0bc0\u0bb5\u0bc1\u0b95\u0bb3\u0bcd', '\u0baa\u0bbf\u0b9c\u0bbf', '\u0baa\u0bbf\u0ba9\u0bcd\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1',\n       '\u0baa\u0bbf\u0bb0\u0bbe\u0ba9\u0bcd\u0bb8\u0bcd', '\u0baa\u0bbf\u0bb0\u0bc6\u0b9e\u0bcd\u0b9a\u0bc1 \u0b95\u0baf\u0bbe\u0ba9\u0bbe', '\u0baa\u0bbf\u0bb0\u0bc6\u0b9e\u0bcd\u0b9a\u0bc1 \u0baa\u0bbe\u0bb2\u0bbf\u0ba9\u0bc7\u0b9a\u0bbf\u0baf\u0bbe', '\u0b95\u0baa\u0bcb\u0ba9\u0bcd',\n       '\u0b95\u0bbe\u0bae\u0bcd\u0baa\u0bbf\u0baf\u0bbe, \u0ba4\u0bbf', '\u0b95\u0bbe\u0b9a\u0bbe \u0baa\u0b95\u0bc1\u0ba4\u0bbf', '\u0b9c\u0bbe\u0bb0\u0bcd\u0b9c\u0bbf\u0baf\u0bbe', '\u0b9c\u0bc6\u0bb0\u0bcd\u0bae\u0ba9\u0bbf', '\u0b95\u0bbe\u0ba9\u0bbe',\n       '\u0b9c\u0bbf\u0baa\u0bcd\u0bb0\u0bbe\u0bb2\u0bcd\u0b9f\u0bb0\u0bcd', '\u0b95\u0bbf\u0bb0\u0bc0\u0bb8\u0bcd', '\u0b95\u0bbf\u0bb0\u0bc0\u0ba9\u0bcd\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1', '\u0b95\u0bbf\u0bb0\u0bc6\u0ba9\u0b9f\u0bbe', '\u0b95\u0bc1\u0bb5\u0bbe\u0b9f\u0bb2\u0bc2\u0baa\u0bcd',\n       '\u0b95\u0bc1\u0bb5\u0bbe\u0bae\u0bcd', '\u0b95\u0bc1\u0bb5\u0bbe\u0ba4\u0bcd\u0ba4\u0bae\u0bbe\u0bb2\u0bbe', '\u0b95\u0bc1\u0bb0\u0bcd\u0ba9\u0bcd\u0b9a\u0bbf', '\u0b95\u0bbf\u0ba9\u0bbf\u0baf\u0bbe', '\u0b95\u0bbf\u0ba9\u0bbf\u0baf\u0bbe-\u0baa\u0bbf\u0b9a\u0bbe\u0bb5\u0bcd',\n       '\u0b95\u0baf\u0bbe\u0ba9\u0bbe', '\u0bb9\u0bc8\u0b9f\u0bcd\u0b9f\u0bbf', '\u0bb9\u0bcb\u0ba3\u0bcd\u0b9f\u0bc1\u0bb0\u0bbe\u0bb8\u0bcd', '\u0bb9\u0bbe\u0b99\u0bcd\u0b95\u0bbe\u0b99\u0bcd', '\u0bb9\u0b99\u0bcd\u0b95\u0bc7\u0bb0\u0bbf',\n       '\u0b90\u0bb8\u0bcd\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1', '\u0b87\u0ba8\u0bcd\u0ba4\u0bbf\u0baf\u0bbe', '\u0b87\u0ba8\u0bcd\u0ba4\u0bcb\u0ba9\u0bc7\u0b9a\u0bbf\u0baf\u0bbe', '\u0b88\u0bb0\u0bbe\u0ba9\u0bcd', '\u0b88\u0bb0\u0bbe\u0b95\u0bcd', '\u0b85\u0baf\u0bb0\u0bcd\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1',\n       '\u0b90\u0bb2\u0bcd \u0b86\u0b83\u0baa\u0bcd \u0bae\u0bc7\u0ba9\u0bcd', '\u0b87\u0bb8\u0bcd\u0bb0\u0bc7\u0bb2\u0bcd', '\u0b87\u0ba4\u0bcd\u0ba4\u0bbe\u0bb2\u0bbf', '\u0b9c\u0bae\u0bc8\u0b95\u0bcd\u0b95\u0bbe', '\u0b9c\u0baa\u0bcd\u0baa\u0bbe\u0ba9\u0bcd',\n       '\u0b9c\u0bc6\u0bb0\u0bcd\u0b9a\u0bbf', '\u0b9c\u0bcb\u0bb0\u0bcd\u0b9f\u0bbe\u0ba9\u0bcd', '\u0b95\u0b9c\u0b95\u0bb8\u0bcd\u0ba4\u0bbe\u0ba9\u0bcd', '\u0b95\u0bc6\u0ba9\u0bcd\u0baf\u0bbe', '\u0b95\u0bbf\u0bb0\u0bbf\u0baa\u0ba4\u0bbf',\n       '\u0b95\u0bca\u0bb0\u0bbf\u0baf\u0bbe, \u0bb5\u0b9f\u0b95\u0bcd\u0b95\u0bc1', '\u0b95\u0bca\u0bb0\u0bbf\u0baf\u0bbe, \u0ba4\u0bc6\u0bb1\u0bcd\u0b95\u0bc1', '\u0b95\u0bc1\u0bb5\u0bc8\u0ba4\u0bcd', '\u0b95\u0bbf\u0bb0\u0bcd\u0b95\u0bbf\u0bb8\u0bcd\u0ba4\u0bbe\u0ba9\u0bcd',\n       '\u0bb2\u0bbe\u0bb5\u0bcb\u0bb8\u0bcd', '\u0bb2\u0bbe\u0b9f\u0bcd\u0bb5\u0bbf\u0baf\u0bbe', '\u0bb2\u0bc6\u0baa\u0ba9\u0bbe\u0ba9\u0bcd', '\u0bb2\u0bc6\u0b9a\u0bcb\u0ba4\u0bcb', '\u0bb2\u0bc8\u0baa\u0bc0\u0bb0\u0bbf\u0baf\u0bbe', '\u0bb2\u0bbf\u0baa\u0bbf\u0baf\u0bbe',\n       '\u0bb2\u0bbf\u0b9a\u0bcd\u0b9f\u0bc6\u0ba9\u0bcd\u0bb8\u0bcd\u0b9f\u0bc0\u0ba9\u0bcd', '\u0bb2\u0bbf\u0ba4\u0bc1\u0bb5\u0bc7\u0ba9\u0bbf\u0baf\u0bbe', '\u0bb2\u0b95\u0bcd\u0b9a\u0bae\u0bcd\u0baa\u0bb0\u0bcd\u0b95\u0bcd', '\u0bae\u0b95\u0bcd\u0b95\u0bbe\u0bb5\u0bcd',\n       '\u0bae\u0b9a\u0bbf\u0b9f\u0bcb\u0ba9\u0bbf\u0baf\u0bbe', '\u0bae\u0b9f\u0b95\u0bbe\u0bb8\u0bcd\u0b95\u0bb0\u0bcd', '\u0bae\u0bb2\u0bbe\u0bb5\u0bbf', '\u0bae\u0bb2\u0bc7\u0b9a\u0bbf\u0baf\u0bbe', '\u0bae\u0bbe\u0bb2\u0ba4\u0bcd\u0ba4\u0bc0\u0bb5\u0bc1\u0b95\u0bb3\u0bcd',\n       '\u0bae\u0bbe\u0bb2\u0bbf', '\u0bae\u0bbe\u0bb2\u0bcd\u0b9f\u0bbe', '\u0bae\u0bbe\u0bb0\u0bcd\u0bb7\u0bb2\u0bcd \u0ba4\u0bc0\u0bb5\u0bc1\u0b95\u0bb3\u0bcd', '\u0bae\u0bbe\u0bb0\u0bcd\u0b9f\u0bbf\u0ba9\u0bbf\u0b95\u0bcd',\n       '\u0bae\u0bcc\u0bb0\u0bbf\u0b9f\u0bbe\u0ba9\u0bbf\u0baf\u0bbe', '\u0bae\u0bca\u0bb0\u0bbf\u0bb7\u0bbf\u0baf\u0bb8\u0bcd', '\u0bae\u0baf\u0bcb\u0b9f\u0bcd', '\u0bae\u0bc6\u0b95\u0bcd\u0b9a\u0bbf\u0b95\u0bcb',\n       '\u0bae\u0bc8\u0b95\u0bcd\u0bb0\u0bcb\u0ba9\u0bc7\u0b9a\u0bbf\u0baf\u0bbe, \u0b83\u0baa\u0bc6\u0b9f\u0bcd. \u0b9a\u0bc6\u0baf\u0bbf\u0ba9\u0bcd\u0b9f\u0bcd', '\u0bae\u0bbe\u0bb2\u0bcd\u0b9f\u0bcb\u0bb5\u0bbe', '\u0bae\u0bca\u0ba9\u0bbe\u0b95\u0bcd\u0b95\u0bcb', '\u0bae\u0b99\u0bcd\u0b95\u0bcb\u0bb2\u0bbf\u0baf\u0bbe',\n       '\u0bae\u0bbe\u0ba9\u0bcd\u0b9a\u0bc6\u0bb0\u0bbe\u0b9f\u0bcd', '\u0bae\u0bca\u0bb0\u0bbe\u0b95\u0bcd\u0b95\u0bcb', '\u0bae\u0bca\u0b9a\u0bbe\u0bae\u0bcd\u0baa\u0bbf\u0b95\u0bcd', '\u0ba8\u0bae\u0bc0\u0baa\u0bbf\u0baf\u0bbe', '\u0ba8\u0bb5\u0bcd\u0bb0\u0bc1',\n       '\u0ba8\u0bc7\u0baa\u0bbe\u0bb3\u0bae\u0bcd', '\u0ba8\u0bc6\u0ba4\u0bb0\u0bcd\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1', '\u0ba8\u0bc6\u0ba4\u0bb0\u0bcd\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1 \u0b85\u0ba3\u0bcd\u0b9f\u0bbf\u0bb2\u0bbf\u0bb8\u0bcd',\n       '\u0ba8\u0bbf\u0baf\u0bc2 \u0b95\u0bb2\u0bbf\u0b9f\u0bcb\u0ba9\u0bbf\u0baf\u0bbe', '\u0ba8\u0bbf\u0baf\u0bc2\u0b9a\u0bbf\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1', '\u0ba8\u0bbf\u0b95\u0bb0\u0b95\u0bc1\u0bb5\u0bbe', '\u0ba8\u0bc8\u0b9c\u0bb0\u0bcd',\n       '\u0ba8\u0bc8\u0b9c\u0bc0\u0bb0\u0bbf\u0baf\u0bbe', '\u0b8e\u0ba9\u0bcd. \u0bae\u0bb0\u0bbf\u0baf\u0bbe\u0ba9\u0bbe \u0ba4\u0bc0\u0bb5\u0bc1\u0b95\u0bb3\u0bcd', '\u0ba8\u0bcb\u0bb0\u0bcd\u0bb5\u0bc7', '\u0b93\u0bae\u0ba9\u0bcd', '\u0baa\u0bbe\u0b95\u0bbf\u0bb8\u0bcd\u0ba4\u0bbe\u0ba9\u0bcd',\n       '\u0baa\u0bb2\u0bbe\u0bb5\u0bcd', '\u0baa\u0ba9\u0bbe\u0bae\u0bbe', '\u0baa\u0baa\u0bcd\u0baa\u0bc1\u0bb5\u0bbe \u0ba8\u0bbf\u0baf\u0bc2 \u0b95\u0bbf\u0ba9\u0bbf\u0baf\u0bbe', '\u0baa\u0bb0\u0bbe\u0b95\u0bc1\u0bb5\u0bc7', '\u0baa\u0bc6\u0bb0\u0bc1',\n       '\u0baa\u0bbf\u0bb2\u0bbf\u0baa\u0bcd\u0baa\u0bc8\u0ba9\u0bcd\u0bb8\u0bcd', '\u0baa\u0bcb\u0bb2\u0ba8\u0bcd\u0ba4\u0bc1', '\u0baa\u0bcb\u0bb0\u0bcd\u0b9a\u0bcd\u0b9a\u0bc1\u0b95\u0bb2\u0bcd', '\u0baa\u0bc1\u0bb5\u0bc7\u0bb0\u0bcd\u0b9f\u0bcd\u0b9f\u0bcb \u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0bcb', '\u0b95\u0ba4\u0bcd\u0ba4\u0bbe\u0bb0\u0bcd',\n       '\u0bb0\u0bc0\u0baf\u0bc2\u0ba9\u0bbf\u0baf\u0ba9\u0bcd', '\u0bb0\u0bc1\u0bae\u0bc7\u0ba9\u0bbf\u0baf\u0bbe', '\u0bb0\u0bb7\u0bcd\u0baf\u0bbe', '\u0bb0\u0bc1\u0bb5\u0bbe\u0ba3\u0bcd\u0b9f\u0bbe', '\u0b9a\u0bc6\u0baf\u0bbf\u0ba9\u0bcd\u0b9f\u0bcd \u0bb9\u0bc6\u0bb2\u0bbf\u0ba9\u0bbe',\n       '\u0b9a\u0bc6\u0baf\u0bbf\u0ba9\u0bcd\u0b9f\u0bcd \u0b95\u0bbf\u0b9f\u0bcd\u0bb8\u0bcd & \u0ba8\u0bc6\u0bb5\u0bbf\u0bb8\u0bcd', '\u0b9a\u0bc6\u0baf\u0bbf\u0ba9\u0bcd\u0b9f\u0bcd \u0bb2\u0bc2\u0b9a\u0bbf\u0baf\u0bbe', '\u0b9a\u0bc6\u0baf\u0bbf\u0ba9\u0bcd\u0b9f\u0bcd \u0baa\u0bbf\u0baf\u0bb0\u0bcd & \u0bae\u0bbf\u0b95\u0bcd\u0b95\u0bc1\u0bb2\u0bcb\u0ba9\u0bcd',\n       '\u0b9a\u0bc6\u0baf\u0bbf\u0ba9\u0bcd\u0b9f\u0bcd \u0bb5\u0bbf\u0ba9\u0bcd\u0b9a\u0bc6\u0ba9\u0bcd\u0b9f\u0bcd \u0bae\u0bb1\u0bcd\u0bb1\u0bc1\u0bae\u0bcd \u0b95\u0bbf\u0bb0\u0bc6\u0ba9\u0b9f\u0bc8\u0ba9\u0bcd\u0bb8\u0bcd', '\u0b9a\u0bae\u0bcb\u0bb5\u0bbe', '\u0b9a\u0bbe\u0ba9\u0bcd \u0bae\u0bb0\u0bbf\u0ba9\u0bcb',\n       'Sao Tome & Principe', '\u0b9a\u0bb5\u0bc2\u0ba4\u0bbf \u0b85\u0bb0\u0bc7\u0baa\u0bbf\u0baf\u0bbe', '\u0b9a\u0bc6\u0ba9\u0b95\u0bb2\u0bcd', '\u0b9a\u0bc6\u0bb0\u0bcd\u0baa\u0bbf\u0baf\u0bbe',\n       '\u0b9a\u0bc0\u0bb7\u0bc6\u0bb2\u0bcd\u0bb8\u0bcd', '\u0b9a\u0bbf\u0baf\u0bb0\u0bbe \u0bb2\u0bbf\u0baf\u0bcb\u0ba9\u0bcd', '\u0b9a\u0bbf\u0b99\u0bcd\u0b95\u0baa\u0bcd\u0baa\u0bc2\u0bb0\u0bcd', '\u0bb8\u0bcd\u0bb2\u0bcb\u0bb5\u0bbe\u0b95\u0bcd\u0b95\u0bbf\u0baf\u0bbe',\n       '\u0bb8\u0bcd\u0bb2\u0bcb\u0bb5\u0bc7\u0ba9\u0bbf\u0baf\u0bbe', '\u0b9a\u0bbe\u0bb2\u0bae\u0ba9\u0bcd \u0ba4\u0bc0\u0bb5\u0bc1\u0b95\u0bb3\u0bcd', '\u0b9a\u0bcb\u0bae\u0bbe\u0bb2\u0bbf\u0baf\u0bbe', '\u0ba4\u0bc6\u0ba9\u0bcd \u0b86\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0bbe',\n       '\u0bb8\u0bcd\u0baa\u0bc6\u0baf\u0bbf\u0ba9\u0bcd', '\u0b87\u0bb2\u0b99\u0bcd\u0b95\u0bc8', '\u0b9a\u0bc2\u0b9f\u0bbe\u0ba9\u0bcd', '\u0b9a\u0bc1\u0bb0\u0bbf\u0ba9\u0bbe\u0bae\u0bcd', '\u0bb8\u0bcd\u0bb5\u0bbe\u0b9a\u0bbf\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1',\n       '\u0bb8\u0bcd\u0bb5\u0bc0\u0b9f\u0ba9\u0bcd', '\u0b9a\u0bc1\u0bb5\u0bbf\u0b9f\u0bcd\u0b9a\u0bb0\u0bcd\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1', '\u0b9a\u0bbf\u0bb0\u0bbf\u0baf\u0bbe', '\u0ba4\u0bc8\u0bb5\u0bbe\u0ba9\u0bcd', '\u0ba4\u0b9c\u0bbf\u0b95\u0bbf\u0bb8\u0bcd\u0ba4\u0bbe\u0ba9\u0bcd',\n       '\u0ba4\u0bbe\u0ba9\u0bcd\u0b9a\u0bbe\u0ba9\u0bbf\u0baf\u0bbe', '\u0ba4\u0bbe\u0baf\u0bcd\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1', '\u0b9f\u0bcb\u0b95\u0bcb', '\u0b9f\u0bcb\u0b99\u0bcd\u0b95\u0bbe', '\u0b9f\u0bbf\u0bb0\u0bbf\u0ba9\u0bbf\u0b9f\u0bbe\u0b9f\u0bcd & \u0b9f\u0bca\u0baa\u0bbe\u0b95\u0bcb',\n       '\u0ba4\u0bc1\u0ba9\u0bbf\u0b9a\u0bbf\u0baf\u0bbe', '\u0ba4\u0bc1\u0bb0\u0bc1\u0b95\u0bcd\u0b95\u0bbf', '\u0ba4\u0bc1\u0bb0\u0bcd\u0b95\u0bcd\u0bae\u0bc6\u0ba9\u0bbf\u0bb8\u0bcd\u0ba4\u0bbe\u0ba9\u0bcd', '\u0b9f\u0bb0\u0bcd\u0b95\u0bcd\u0bb8\u0bcd & \u0b95\u0bc6\u0baf\u0bcd\u0b95\u0bcb\u0bb8\u0bcd',\n       '\u0ba4\u0bc1\u0bb5\u0bbe\u0bb2\u0bc1', '\u0b89\u0b95\u0bbe\u0ba3\u0bcd\u0b9f\u0bbe', '\u0b89\u0b95\u0bcd\u0bb0\u0bc8\u0ba9\u0bcd', '\u0b90\u0b95\u0bcd\u0b95\u0bbf\u0baf \u0b85\u0bb0\u0baa\u0bc1 \u0b8e\u0bae\u0bbf\u0bb0\u0bc7\u0b9f\u0bcd\u0bb8\u0bcd',\n       '\u0baf\u0bc1\u0ba9\u0bc8\u0b9f\u0bc6\u0b9f\u0bcd \u0b95\u0bbf\u0b99\u0bcd\u0b9f\u0bae\u0bcd', '\u0baf\u0bc1\u0ba9\u0bc8\u0b9f\u0bc6\u0b9f\u0bcd \u0bb8\u0bcd\u0b9f\u0bc7\u0b9f\u0bcd\u0bb8\u0bcd', '\u0b89\u0bb0\u0bc1\u0b95\u0bc1\u0bb5\u0bc7', '\u0b89\u0bb8\u0bcd\u0baa\u0bc6\u0b95\u0bbf\u0bb8\u0bcd\u0ba4\u0bbe\u0ba9\u0bcd',\n       '\u0bb5\u0ba9\u0bc1\u0bb5\u0bbe\u0b9f\u0bc1', '\u0bb5\u0bc6\u0ba9\u0bbf\u0b9a\u0bc1\u0bb2\u0bbe', '\u0bb5\u0bbf\u0baf\u0b9f\u0bcd\u0ba8\u0bbe\u0bae\u0bcd', '\u0bb5\u0bbf\u0bb0\u0bcd\u0b9c\u0bbf\u0ba9\u0bcd \u0ba4\u0bc0\u0bb5\u0bc1\u0b95\u0bb3\u0bcd',\n       '\u0bb5\u0bbe\u0bb2\u0bbf\u0bb8\u0bcd \u0b85\u0ba3\u0bcd\u0b9f\u0bcd \u0b83\u0baa\u0bc1\u0b9f\u0bc1\u0ba9\u0bbe', '\u0bb5\u0bc6\u0bb8\u0bcd\u0b9f\u0bcd \u0baa\u0bc7\u0b99\u0bcd\u0b95\u0bcd', '\u0bb5\u0bc6\u0bb8\u0bcd\u0b9f\u0bb0\u0bcd\u0ba9\u0bcd \u0b9a\u0bb9\u0bbe\u0bb0\u0bbe', '\u0baf\u0bc7\u0bae\u0ba9\u0bcd',\n       '\u0b9a\u0bbe\u0bae\u0bcd\u0baa\u0bbf\u0baf\u0bbe', '\u0b9c\u0bbf\u0bae\u0bcd\u0baa\u0bbe\u0baa\u0bcd\u0bb5\u0bc7']\n    for c in tamil_countries:\n        if c in s:\n            return c\n    return s\n\ndef my_pp(s, question, context):\n    s = left_strip(s)\n    \n    # A.G. \n    sp = s.split()\n    if len(sp) >= 2 and sp[-1][-1] == '.' and sp[-2][-1] == '.':\n        return s\n    \n#     # xxx (xxx\n#     if '(' in s:\n#         s = s[:s.find('(')]\n    \n    s = right_strip(s)\n    \n    \n    # nico pp\n    tamil_ad = \"\u0b95\u0bbf.\u0baa\u0bbf\"\n    tamil_bc = \"\u0b95\u0bbf.\u0bae\u0bc1\"\n    tamil_km = \"\u0b95\u0bbf.\u0bae\u0bc0\"\n    hindi_ad = \"\u0908\"\n    hindi_bc = \"\u0908.\u092a\u0942\"\n    if any([s.endswith(tamil_ad), s.endswith(tamil_bc), s.endswith(tamil_km), s.endswith(hindi_ad), s.endswith(hindi_bc)]) and s+\".\" in context:\n        s = s+\".\"\n    \n#     # hindi which year + tamil which year \u0b8e\u0ba8\u0bcd\u0ba4 \u0b86\u0ba3\u0bcd\u0b9f\u0bc1\n    if '\u0915\u093f\u0938 \u0935\u0930\u094d\u0937' in question or '\u0b8e\u0ba8\u0bcd\u0ba4 \u0b86\u0ba3\u0bcd\u0b9f\u0bc1' in question:\n        s = find_year(s)\n    \n    # hindi which year v2\n#     if '\u0915\u093f\u0938 \u0935\u0930\u094d\u0937' in question:\n#         s = find_year_v2(s)\n    \n    # tamil area\n    if question.endswith('\u0baa\u0bb0\u0baa\u0bcd\u0baa\u0bb3\u0bb5\u0bc1 \u0b8e\u0ba9\u0bcd\u0ba9?'):\n        if s[-1].isnumeric():\n            if s + ' \u0b9a\u0ba4\u0bc1\u0bb0 \u0b95\u0bbf\u0bb2\u0bcb\u0bae\u0bc0\u0b9f\u0bcd\u0b9f\u0bb0\u0bcd\u0b95\u0bb3\u0bcd' in context:\n                s = s + ' \u0b9a\u0ba4\u0bc1\u0bb0 \u0b95\u0bbf\u0bb2\u0bcb\u0bae\u0bc0\u0b9f\u0bcd\u0b9f\u0bb0\u0bcd\u0b95\u0bb3\u0bcd'\n            elif s + ' \u0b9a\u0ba4\u0bc1\u0bb0 \u0b95\u0bbf\u0bb2\u0bcb \u0bae\u0bc0\u0b9f\u0bcd\u0b9f\u0bb0\u0bcd' in context:\n                s = s + ' \u0b9a\u0ba4\u0bc1\u0bb0 \u0b95\u0bbf\u0bb2\u0bcb \u0bae\u0bc0\u0b9f\u0bcd\u0b9f\u0bb0\u0bcd'\n            elif s + ' \u0b9a\u0ba4\u0bc1\u0bb0 \u0b95\u0bbf\u0bb2\u0bcb \u0bae\u0bc0\u0b9f\u0bcd\u0b9f\u0bb0\u0bcd\u0b95\u0bb3\u0bcd' in context:\n                s = s + ' \u0b9a\u0ba4\u0bc1\u0bb0 \u0b95\u0bbf\u0bb2\u0bcb \u0bae\u0bc0\u0b9f\u0bcd\u0b9f\u0bb0\u0bcd\u0b95\u0bb3\u0bcd'\n            elif s + ' \u0b9a\u0ba4\u0bc1\u0bb0 \u0bae\u0bc8\u0bb2\u0bcd\u0b95\u0bb3\u0bcd' in context:\n                s = s + ' \u0b9a\u0ba4\u0bc1\u0bb0 \u0bae\u0bc8\u0bb2\u0bcd\u0b95\u0bb3\u0bcd'\n            elif s + ' \u0b9a\u0ba4\u0bc1\u0bb0' in context:\n                s = s + ' \u0b9a\u0ba4\u0bc1\u0bb0'\n            elif s + ' \u0b95\u0bbf.\u0bae\u0bc0\u00b2' in context:\n                s = s + ' \u0b95\u0bbf.\u0bae\u0bc0\u00b2'\n        else:\n            if s + '\u00b2' in context:\n                s = s + '\u00b2'\n            elif s + '2' in context:\n                s = s + '2'\n    \n    # hindi number\n    if '\u0938\u0902\u0916\u094d\u092f\u093e' in question:\n        s = get_number(s)\n        \n    #a(b) -> a\n    if '(' in s:\n        s = s[:s.find('(')]\n        \n    #tamil country extract\n    if question.endswith('\u0ba8\u0bbe\u0b9f\u0bc1 \u0b8e\u0ba4\u0bc1?') or question.startswith('\u0b8e\u0ba8\u0bcd\u0ba4 \u0ba8\u0bbe\u0b9f\u0bc1'):\n        s = get_country(s)\n    \n    # tamil years old\n    if '\u0bb5\u0baf\u0ba4\u0bbf\u0bb2\u0bcd' in question:\n        s = get_number(s)\n        \n    # negative number recover\n#     if s[0].isdigit():\n#         if context.count(s) == context.count(' -' + s):\n#             s = '-' + s        \n        \n    # fix typos\n    # japan\n    if s == '\u0b9a\u0baa\u0bcd\u0baa\u0bbe\u0ba9\u0bcd' and '\u0b9c\u0baa\u0bcd\u0baa\u0bbe\u0ba9\u0bcd' in context:\n        return '\u0b9c\u0baa\u0bcd\u0baa\u0bbe\u0ba9\u0bcd'\n    # mubai\n    if s == '\u092e\u0941\u0902\u092c\u0908' and '\u092e\u0941\u092e\u094d\u092c\u0908' in context:\n        return '\u092e\u0941\u092e\u094d\u092c\u0908'\n        \n    return s\n    \n        \ndef softmax(array):\n    ctx = np.exp(array)\n    return ctx \/ np.sum(ctx)\n\ndef tez_pp(s):\n    from string import punctuation\n    s = \" \".join(s.split())\n    s = s.strip(punctuation)\n    return s\n\ndef get_len(x):\n    x['seq_len'] = len(x['attention_mask'])\n    return x\n\ndef get_batches(features, batch_size=16, pad_id=0):\n    attention_mask = features['attention_mask']\n    input_ids = features['input_ids']\n    ret = []\n    for i in tqdm(range(0, len(attention_mask), batch_size)):\n        batch_attention_mask = attention_mask[i:i+batch_size]\n        batch_input_ids = input_ids[i:i+batch_size]\n        maxlen = max([len(x) for x in batch_attention_mask])\n        padded_batch_attention_mask = [x+[pad_id]*(maxlen-len(x)) for x in batch_attention_mask]\n        padded_batch_input_ids = [x+[pad_id]*(maxlen-len(x)) for x in batch_input_ids]\n        batch = {\n            'attention_mask': torch.tensor(padded_batch_attention_mask, device=device),\n            'input_ids': torch.tensor(padded_batch_input_ids, device=device),\n        }\n        ret.append(batch)\n    return ret\n\ndef pad_back(logits):\n    return F.pad(logits, pad=(0, 512-logits.shape[1]), mode='constant', value=-1e5)","cc496338":"test = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\ntest_dataset = Dataset.from_pandas(test)\nexample_id_to_index = {k: i for i, k in enumerate(test_dataset[\"id\"])}\nblend_start_logits, blend_end_logits = collections.defaultdict(list), collections.defaultdict(list)\nfor i in range(len(pretrained_paths)):\n    print(f'#model: {i}')\n    start1 = time.time()\n    pretrained_path = pretrained_paths[i]\n    model_path = model_paths[i]\n    mymodel = mymodels[i]\n    max_length = max_lengths[i]\n    doc_stride = doc_strides[i]\n    \n    if i != 3:\n        tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n        pad_on_right = tokenizer.padding_side == \"right\"\n        test_features = test_dataset.map(\n            lambda x: prepare_validation_features(x, tokenizer, max_length, doc_stride, pad_on_right, padding=False),\n            batched=True,\n            remove_columns=test_dataset.column_names,\n            num_proc=2\n        )\n        test_features = test_features.map(get_len, num_proc=2)\n        test_features = test_features.sort('seq_len')\n        test_dataloader = get_batches(test_features, batch_size=16, pad_id=tokenizer.pad_token_id)\n    model = ChaiiModelLoadHead(pretrained_path).to(device) if mymodel else AutoModelForQuestionAnswering.from_pretrained(pretrained_path).to(device)\n    model.eval()\n    all_start_logits = []\n    all_end_logits = []\n    print(f'init time {(time.time()-start1)\/\/60} minutes')\n    \n    start1 = time.time()\n    exts = model_exts[i]\n    weight_paths = glob(os.path.join(model_path, '*.pt'))\n    for ii, ext in enumerate(exts):\n        weight_paths = [x for x in weight_paths if f'{ext}.pt' not in x]\n    for weight_path in weight_paths:\n        print('weight:', weight_path)\n        model.load_state_dict(torch.load(weight_path))\n        start_logits = []\n        end_logits = []\n        for batch in tqdm(test_dataloader, leave=False):\n            pred = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n            start_logits.append(pad_back(pred.start_logits))\n            end_logits.append(pad_back(pred.end_logits))\n        start_logits = torch.cat(start_logits, dim=0)\n        end_logits = torch.cat(end_logits, dim=0)\n        start_logits = start_logits.cpu().numpy()\n        end_logits = end_logits.cpu().numpy()\n        all_start_logits.append(start_logits)\n        all_end_logits.append(end_logits)\n    all_start_logits = np.array(all_start_logits)\n    all_end_logits = np.array(all_end_logits)\n    all_start_logits = np.mean(all_start_logits, axis=0)\n    all_end_logits = np.mean(all_end_logits, axis=0)\n    raw_predictions = [all_start_logits, all_end_logits]\n    print(f'inference time {(time.time()-start1)\/\/60} minutes')\n    \n    start1 = time.time()\n    all_char_start_logits, all_char_end_logits = get_char_logits(test_dataset, test_features, raw_predictions)\n    for k in all_char_start_logits:\n        if dosoftmax:\n            blend_start_logits[k].append(softmax(all_char_start_logits[k]))\n            blend_end_logits[k].append(softmax(all_char_end_logits[k]))\n        else:\n            blend_start_logits[k].append(all_char_start_logits[k])\n            blend_end_logits[k].append(all_char_end_logits[k])\n    print(f'char mapping time {(time.time()-start1)\/\/60} minutes')","b31eda74":"\nblended_start_logits, blended_end_logits = {}, {}\nfor k in blend_start_logits:\n    blended_start_logits[k] = np.average(np.array(blend_start_logits[k]), axis=0, weights=weights)\n    blended_end_logits[k] = np.average(np.array(blend_end_logits[k]), axis=0, weights=weights)\npredictions = get_predictions(test_dataset, blended_start_logits, blended_end_logits)\npredictions","efefbc72":"sub = test.copy().reset_index(drop=True)\nsub['PredictionString'] = sub['id'].apply(lambda r: predictions[r])\n# sub['PredictionString'] = sub['PredictionString'].apply(my_pp)\n# sub['PredictionString'] = sub['PredictionString'].apply(tez_pp)\nfor i in range(len(sub)):\n    pred = sub.loc[i, 'PredictionString']\n    ctx = sub.loc[i, 'context']\n    qst = sub.loc[i, 'question']\n    sub.loc[i, 'PredictionString'] = my_pp(pred, qst, ctx)\n","0a684c8f":"train = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')","47b18d1f":"# test for leak\nfor i in range(len(sub)):\n    question = sub.loc[i, 'question']\n    context = sub.loc[i, 'context']\n    matches = train[train['question'] == question].reset_index(drop=True)\n    for j in range(len(matches)):\n        new_ans = matches.loc[j, 'answer_text']\n        if new_ans in context:\n            sub.loc[i, 'PredictionString'] = new_ans\n            break\n    ","b1311a88":"sub = sub[['id', 'PredictionString']]\nsub.to_csv('submission.csv', index=False)\nsub","fde90010":"duration = time.time() - start\nprint(f'duration: {duration\/\/60} minutes')","8e90c466":"# Blend","64668c7e":"# Utils","3391b836":"# Infer"}}