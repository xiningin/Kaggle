{"cell_type":{"a5e7ac3e":"code","60c65ead":"code","71d1210c":"code","52e2b668":"code","99be99d1":"code","32e33037":"code","733954a0":"code","f3dfa470":"code","ec57f635":"code","f39f8f46":"code","52e94cfd":"code","0d135a7a":"code","b6093517":"code","42d1019d":"code","168e310d":"code","be77c7e3":"code","3226f0fb":"code","e72e15f1":"code","3b53e877":"code","5dd3946f":"code","6987dc2d":"code","52a07028":"code","f3a79e25":"markdown","79279942":"markdown","c41a702e":"markdown","69f12f4a":"markdown","5036b5c3":"markdown","83a825e2":"markdown"},"source":{"a5e7ac3e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Linear, LayerNorm, ReLU, Dropout\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm\nimport os\nimport copy\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom torch.utils.data import Dataset,TensorDataset, DataLoader,RandomSampler\nimport time,datetime\nimport tensorflow as tf\nimport keras.backend as K\nimport tensorflow.keras.layers as L\n#gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n#sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\ndef allocate_gpu_memory(gpu_number=0):\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n\n    if physical_devices:\n        try:\n            print(\"Found {} GPU(s)\".format(len(physical_devices)))\n            tf.config.set_visible_devices(physical_devices[gpu_number], 'GPU')\n            tf.config.experimental.set_memory_growth(physical_devices[gpu_number], True)\n            print(\"#{} GPU memory is allocated\".format(gpu_number))\n        except RuntimeError as e:\n            print(e)\n    else:\n        print(\"Not enough GPU hardware devices available\")\nallocate_gpu_memory()","60c65ead":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\ndef rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse(y_actual, y_pred, num_scored=len(pred_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) \/ num_scored\n    return score\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    base_fea = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n    bpps_nb_fea = np.array(df['bpps_nb'].to_list())[:,:,np.newaxis]\n    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea,bpps_nb_fea], 2)\n\n\ntrain = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('..\/input\/stanford-covid-vaccine\/test.json', lines=True)\n\ndef read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    # normalized non-zero number\n    # from https:\/\/www.kaggle.com\/symyksr\/openvaccine-deepergcn\n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) \/ bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr\n\ntrain['bpps_sum'] = read_bpps_sum(train)\ntest['bpps_sum'] = read_bpps_sum(test)\ntrain['bpps_max'] = read_bpps_max(train)\ntest['bpps_max'] = read_bpps_max(test)\ntrain['bpps_nb'] = read_bpps_nb(train)\ntest['bpps_nb'] = read_bpps_nb(test)\n\nfrom sklearn.cluster import KMeans\n\nkmeans_model = KMeans(n_clusters=200, random_state=110).fit(preprocess_inputs(train)[:,:,0])\ntrain['cluster_id'] = kmeans_model.labels_","71d1210c":"def gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=256, type=0):\n    inputs = L.Input(shape=(seq_len, 6))\n    \n    # split categorical and numerical features and concatenate them later.\n    categorical_feat_dim = 3\n    categorical_fea = inputs[:, :, :categorical_feat_dim]\n    numerical_fea = inputs[:, :, 3:]\n\n    embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(categorical_fea)\n    reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    reshaped = L.concatenate([reshaped, numerical_fea], axis=2)\n    \n    if type == 0:\n        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    elif type == 1:\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    elif type == 2:\n        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n    elif type == 3:\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n    \n    truncated = hidden[:, :pred_len]\n    out = L.Dense(5, activation='linear')(truncated)\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n    model.compile(tf.keras.optimizers.Adam(), loss=mcrmse)\n    return model\n\nkeras_model = build_model()\nkeras_model.load_weights('..\/input\/gru-lstm-with-feature-engineering-and-augmentation\/modelGRU_LSTM1_cv0.h5')\nkeras_model.layers","52e2b668":"device = torch.device('cuda:%s'%0 if torch.cuda.is_available() else 'cpu')\ndef Init_params(shape,w=None,b=None):\n    if w is None:\n        w = torch.nn.Parameter(torch.empty(*shape))\n        nn.init.xavier_uniform_(w)\n    else:\n        w = torch.nn.Parameter(w)\n    if b is None:\n        b = torch.nn.Parameter(torch.zeros(shape[1]))\n    else:\n        b = torch.nn.Parameter(b)\n    return w,b\n\nclass GRU(nn.Module):\n    def __init__(self,input_dim,hidden_dim,w_i=None,b_i=None,w_h=None,b_h=None):\n        super(GRU, self).__init__()\n        self.w_i,self.b_i = Init_params([input_dim,3*hidden_dim],w_i,b_i)\n        self.w_h,self.b_h = Init_params([hidden_dim,3*hidden_dim],w_h,b_h)\n        self.hd = hidden_dim\n    def forward(self,x):\n        hidden = torch.zeros((x.shape[0], self.hd)).to(device)\n        output = []\n        for i in range(x.shape[1]):\n            x_z = torch.matmul(x[:,i,:],self.w_i[:,:self.hd]) + self.b_i[:self.hd]\n            x_r = torch.matmul(x[:,i,:],self.w_i[:,self.hd:2*self.hd]) + self.b_i[self.hd:2*self.hd]\n            x_n = torch.matmul(x[:,i,:],self.w_i[:,2*self.hd:]) + self.b_i[2*self.hd:]\n\n            h_z = torch.matmul(hidden,self.w_h[:,:self.hd]) + self.b_h[:self.hd]\n            h_r = torch.matmul(hidden,self.w_h[:,self.hd:2*self.hd]) + self.b_h[self.hd:2*self.hd]\n            h_n = torch.matmul(hidden,self.w_h[:,2*self.hd:]) + self.b_h[2*self.hd:]\n\n            z = torch.sigmoid(x_z+h_z)\n            r = torch.sigmoid(x_r+h_r)\n            n = torch.tanh(x_n+r*h_n)\n            h = (1-z)*n + z*hidden\n            hidden = h\n            output.append(h.unsqueeze(1))\n        return torch.cat(output,1)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        num_target=5\n        w = torch.Tensor(keras_model.layers[2].get_weights()[0])\n        self.cate_emb = nn.Embedding.from_pretrained(w,freeze=False)\n        self.gru = GRU(100*3+3, 256, torch.Tensor(keras_model.layers[-4].get_weights()[0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[2][0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[1]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[2][1]).contiguous())\n        self.reverse_gru = GRU(100*3+3, 256, torch.Tensor(keras_model.layers[-4].get_weights()[3]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[5][0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[4]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[5][1]).contiguous())\n        self.gru1 = GRU(512, 256, torch.Tensor(keras_model.layers[-3].get_weights()[0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[2][0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[1]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[2][1]).contiguous())\n        self.reverse_gru1 = GRU(512, 256, torch.Tensor(keras_model.layers[-3].get_weights()[3]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[5][0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[4]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[5][1]).contiguous())\n        self.predict = nn.Linear(512,num_target)\n        for i,(n,p) in enumerate(self.predict.named_parameters()):\n            if i == 0:\n                p.data = torch.nn.Parameter(torch.Tensor(keras_model.layers[-1].get_weights()[0].T).contiguous())\n            if i == 1:\n                p.data = torch.nn.Parameter(torch.Tensor(keras_model.layers[-1].get_weights()[1]).contiguous())\n\n    def forward(self, cateX,contX):\n        cate_x = self.cate_emb(cateX).view(cateX.shape[0],cateX.shape[1],-1)\n        sequence = torch.cat([cate_x,contX],-1)\n        x = self.gru(sequence)\n        reverse_x = torch.flip(self.reverse_gru(torch.flip(sequence,[1])),[1])\n        sequence = torch.cat([x,reverse_x],-1)\n        x = self.gru1(sequence)\n        reverse_x = torch.flip(self.reverse_gru1(torch.flip(sequence,[1])),[1])\n        x = torch.cat([x,reverse_x],-1)\n        x = F.dropout(x,0.5,training=self.training)\n        predict = self.predict(x)\n        return predict\npytorch_model = Net()\npytorch_model.to(device)","99be99d1":"# check 1 samples\nx = preprocess_inputs(train[:1])\ncate_x = torch.LongTensor(x[:,:,:3]).to(device)\ncont_x = torch.Tensor(x[:,:,3:]).to(device)\ny = np.array(train[:1][pred_cols].values.tolist()).transpose((0, 2, 1))","32e33037":"keras_y = keras_model.predict(x)\nkeras_y","733954a0":"pytorch_model.eval()\npytorch_y = pytorch_model(cate_x,cont_x).detach().cpu().numpy()\npytorch_y","f3dfa470":"# output difference between Keras and Pytorch\nnp.mean(np.abs(keras_y-pytorch_y[:,:68,:]))","ec57f635":"# check all samples\n\ngkf = GroupKFold(n_splits=5)\nkeras_predict = []\npytorch_predict = []\ntargets = []\n\nfor fold, (train_index, valid_index) in enumerate(gkf.split(train,  train['reactivity'], train['cluster_id'])):\n    keras_model.load_weights('..\/input\/gru-lstm-with-feature-engineering-and-augmentation\/modelGRU_LSTM1_cv%s.h5'%fold)\n    t_valid = train.iloc[valid_index]\n    t_valid = t_valid[t_valid['SN_filter'] == 1]\n    valid_x = preprocess_inputs(t_valid)\n    valid_count = valid_x.shape[0]\n    valid_cate_x = torch.LongTensor(valid_x[:,:,:3])\n    valid_cont_x = torch.Tensor(valid_x[:,:,3:])\n    valid_y = torch.Tensor(np.array(t_valid[pred_cols].values.tolist()).transpose((0, 2, 1)))\n\n    valid_data = TensorDataset(valid_cate_x,valid_cont_x,valid_y)\n    valid_data_loader = DataLoader(dataset=valid_data,shuffle=False,batch_size=32,num_workers=1)\n    valid_y = valid_y.numpy()\n    targets.append(valid_y)\n    \n    # Keras predict\n    keras_predict.append(keras_model.predict(valid_x))\n    \n    # Pytorch predict and save oof\n    pytorch_model = Net()\n    pytorch_model.to(device)\n    \n    pytorch_model.eval()\n \n    all_pred = []\n\n    for data in valid_data_loader:\n        cate_x,cont_x,y = [x.to(device) for x in data]\n        outputs = pytorch_model(cate_x,cont_x)\n        all_pred.append(outputs.detach().cpu().numpy())\n    all_pred = np.concatenate(all_pred,0)[:,:68,:]\n    pytorch_predict.append(all_pred)","f39f8f46":"for i in range(5):\n    print('fold %s output difference between Keras and Pytorch:'%i,np.mean(np.abs(keras_predict[i]-pytorch_predict[i])))","52e94cfd":"def Metric(target,pred):\n    metric = 0\n    for i in range(target.shape[-1]):\n        metric += (np.sqrt(np.mean((target[:,:,i]-pred[:,:,i])**2))\/target.shape[-1])\n    return metric","0d135a7a":"for i in range(5):\n    print('fold %s'%i,'|','metric of keras outputs:%.6f'%Metric(targets[i],keras_predict[i]),'|','metric of pytorch outputs:%.6f'%Metric(targets[i],pytorch_predict[i]))","b6093517":"# let's check keras loss fuction\n\ndef rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse(y_actual, y_pred, num_scored=5):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) \/ num_scored\n    return score\n\nfor i in range(5):\n    print('fold %s'%i,mcrmse(targets[i],keras_predict[i]))","42d1019d":"# let's check the average of keras loss fuction outputs\nfor i in range(5):\n    print('fold %s'%i,K.mean(mcrmse(targets[i],keras_predict[i])))","168e310d":"# Finaly, let's check the submission of pytorch is same as keras.\n\n# predict test\ndef Pred(df):\n    test_x = preprocess_inputs(df)\n    test_cate_x = torch.LongTensor(test_x[:,:,:3])\n    test_cont_x = torch.Tensor(test_x[:,:,3:])\n    test_data = TensorDataset(test_cate_x,test_cont_x)\n    test_data_loader = DataLoader(dataset=test_data,shuffle=False,batch_size=64,num_workers=1)\n    all_id = []\n    for i,row in df.iterrows():\n        for j in range(row['seq_length']):\n            all_id.append(row['id']+'_%s'%j)\n\n    all_id = np.array(all_id).reshape(-1,1)\n    all_pred = np.zeros(len(all_id)*5).reshape(len(all_id),5)\n    for fold in range(5):\n        keras_model.load_weights('..\/input\/gru-lstm-with-feature-engineering-and-augmentation\/modelGRU_LSTM1_cv%s.h5'%fold)\n        model = Net()\n        model.to(device)\n        model.eval()\n        t_all_pred = []\n        for data in test_data_loader:\n            cate_x,cont_x = [x.to(device) for x in data]\n            outputs = model(cate_x,cont_x)\n            t_all_pred.append(outputs.detach().cpu().numpy())\n        t_all_pred = np.concatenate(t_all_pred,0)\n        all_pred += t_all_pred.reshape(-1,5)\n    all_pred \/= 5\n    sub = pd.DataFrame(all_pred,columns=['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C'])\n    sub['id_seqpos'] = all_id\n    return sub\npublic_sub = Pred(test.loc[test['seq_length']==107])\nprivate_sub = Pred(test.loc[test['seq_length']==130])\npytorch_sub = pd.concat([public_sub,private_sub]).reset_index(drop=True)\npytorch_sub = pytorch_sub[['id_seqpos']+pred_cols]","be77c7e3":"pytorch_sub = pytorch_sub.sort_values(by=['id_seqpos']).reset_index(drop=True)","3226f0fb":"keras_sub = pd.read_csv('..\/input\/gru-lstm-with-feature-engineering-and-augmentation\/submission.csv')","e72e15f1":"keras_sub = keras_sub.sort_values(by=['id_seqpos']).reset_index(drop=True)","3b53e877":"keras_sub.head()","5dd3946f":"pytorch_sub.head()","6987dc2d":"# submission difference between Keras and Pytorch\nnp.mean(np.abs(keras_sub[pred_cols].values-pytorch_sub[pred_cols].values))","52a07028":"pytorch_sub.to_csv('.\/submission.csv',index=False)","f3a79e25":"### Keras loss fuction returns a list instead of a number. So how is val_loss calculated in keras training? I guess it's the average of the loss list.","79279942":"### Evaluations of  keras outputs and pytorch outputs are 0.24+. This is significantly worse than the val_loss  in keras training. Why?","c41a702e":"I joined this competition due to [impressive kernel](https:\/\/www.kaggle.com\/its7171\/gru-lstm-with-feature-engineering-and-augmentation) last week. But the pain began since I transferred [Keras into Pytorch](https:\/\/www.kaggle.com\/daishu\/why-keras-is-better-than-pytorch).\n\nIn my experiments, CV is always worse(about 0.02) in Pytorch than Keras. Because the difference couldn't be found, I cannot sleep well, but tonight, it is a sweet dream.","69f12f4a":"### The truth has become known to all. The keras loss fuction is uncorrect. It calculats every sample's mcrmse, then returns the average. ","5036b5c3":"Evaluations of keras outputs and pytorch outputs","83a825e2":"Firstly, let's reproduce Keras output by Pytorch. Because the implement of GRU in Pytorch is different to Keras, so I writed a GRU by Pytorch."}}