{"cell_type":{"d1911c25":"code","1596bbf9":"code","9936fda7":"code","190eccff":"code","e980467d":"code","55800392":"code","752c7490":"code","dc1b853c":"code","228568d9":"code","adbe38ec":"code","a83257cb":"code","4b9e97b2":"code","8a445285":"code","9ffd719b":"code","8da5861e":"code","0e8861ac":"code","a010c036":"code","4fa25360":"code","363550c9":"code","87be9c65":"code","88bdb211":"code","e08b8040":"code","fed85e7c":"code","b603c544":"code","f368c14e":"code","5e2fc690":"markdown"},"source":{"d1911c25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt","1596bbf9":"# \u8b80\u53d6\u6a94\u6848 (Read Data)\ndata = pd.read_csv('..\/input\/train.csv')\n\n# \u96a8\u6a5f\u62bd\u6a23 \u6bd4\u7387\u70ba100% (Randomly Smaple data, ratio is 100%)\ndata = data.sample(frac = 1)\n\n# \u986f\u793a\u8cc7\u6599\u7684\u524d\u4e94\u7b46 (Show Data)\ndata.head()","9936fda7":"# \u65b0\u589eLength\u6b04\u4f4d\uff0c\u7d00\u9304\u6bcf\u500b\u6a19\u984c\u6587\u5b57\u7e3d\u9577\u5ea6\n# (Create a new column called Length that record every Headline length)\ndata['Length'] = [len(headline) for headline in data['title'].fillna('')]\ndata.head()","190eccff":"detail = data['Length'].describe()\nprint(detail)","e980467d":"# Get all fake news\nfliter = data['label'] == 1\npos = data[fliter]\nprint('\u662f\u5047\u65b0\u805e\u7684\u6578\u91cf(\u539f\u59cb)\uff1a', len(pos))\n\n# Get all true news\nfliter = data['label'] == 0\nneg = data[fliter]\nprint('\u771f\u65b0\u805e\u7684\u6578\u91cf(\u539f\u59cb)\uff1a', len(neg))\n\nthe_mean = min(len(pos), len(neg))\nprint(\"==============================================\")\n\n# \u6293\u53d6\u6240\u6709 Label\u70ba1\u7684 \u8cc7\u6599\uff0c\u4e26\u91cd\u65b0\u6d17\u724c (Random Shuffle)\np_data = pos.sample(n = the_mean)\nprint('\u53d6\u6a23\u662f\u5047\u65b0\u805e\u8cc7\u6599\u7e3d\u6578\uff1a', len(p_data))\n\n# \u6293\u53d6\u6240\u6709 Label\u70ba0\u7684 \u8cc7\u6599\uff0c\u4e26\u91cd\u65b0\u6d17\u724c (Random Shuffle)\nn_data = neg.sample(n = the_mean)\nprint('\u53d6\u6a23\u4e0d\u662f\u5047\u65b0\u805e\u8cc7\u6599\u7e3d\u6578\uff1a', len(n_data))","55800392":"test_split = 0.2\ntrain_split = 1 - test_split\n\n# \u96a8\u6a5f\u62bd\u6a2380%\u7684\u8cc7\u6599\u7576\u8a13\u7df4\u8cc7\u6599 \u800c\u5269\u4e0b\u768420%\u5247\u7576\u70ba\u6e2c\u8a66\u8cc7\u6599\n# (20% for Testing Data, others 80% for Training Data)\np_train_data = p_data.sample(frac = train_split)\np_test_data = p_data.drop(p_train_data.index)\n\nn_train_data = n_data.sample(frac = train_split)\nn_test_data = n_data.drop(n_train_data.index)\n\n# \u5408\u4f75\u5169\u500b\u985e\u5225\u7684\u8a13\u7df4\u8cc7\u6599\u8207\u6e2c\u8a66\u8cc7\u6599\n# (Combined fake news and true news)\ntrain_data = pd.concat([p_train_data, n_train_data])\ntest_data = pd.concat([p_test_data, n_test_data])\n\n# \u5168\u90e8\u96a8\u6a5f\u6d17\u724c (Random Shuffle)\ntrain_data = train_data.sample(frac = 1)\ntest_data = test_data.sample(frac = 1)\n\nx_train_data = train_data['title'].fillna('')\ny_train_data = train_data['label']\nx_test_data = test_data['title'].fillna('')\ny_test_data = test_data['label']\n\nprint('Train Data\u7684Feature\u6578\u91cf(\u5df2\u6df7\u5408\u975e\u5047\u8207\u5047\u65b0\u805e)\uff1a', len(x_train_data))\nprint('Train Data\u7684Label\u6578\u91cf(\u5df2\u6df7\u5408\u975e\u5047\u8207\u5047\u65b0\u805e)\uff1a', len(y_train_data))\nprint('Test Data\u7684Feature\u6578\u91cf(\u5df2\u6df7\u5408\u975e\u5047\u8207\u5047\u65b0\u805e)\uff1a', len(x_test_data))\nprint('Test Data\u7684Label\u6578\u91cf(\u5df2\u6df7\u5408\u975e\u5047\u8207\u5047\u65b0\u805e)\uff1a', len(y_test_data))","752c7490":"from keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer","dc1b853c":"# token\u5b57\u5178\u6578\u91cf (\u6700\u5e38\u51fa\u73fe\u76844000\u5b57) (Create a token dictionary)\ntoken_num = 4000 \n\n# \u64f7\u53d6\u591a\u5c11\u56fa\u5b9a\u9577\u5ea6\u5b57\u6578 (\u6293\u6a19\u984c\u5b57\u6578\u7684\u5e73\u5747\u503c 72\u500b\u9577\u5ea6)\n# (Get a fix length, we chose the mean of the Headline length)\ndata_length = int(detail['mean'])\n\n# \u8f38\u5165\u5411\u91cf\u7dad\u5ea6 (Word Embeding output vector dimension)\noutput_length = 32 \n\ndropout = 0.5\nlstm_dim = 256","228568d9":"token = Tokenizer(num_words = token_num, filters = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n')\ntoken.fit_on_texts(x_train_data)","adbe38ec":"x_train_seq = token.texts_to_sequences(x_train_data)\nx_test_seq = token.texts_to_sequences(x_test_data)","a83257cb":"x_train = sequence.pad_sequences(x_train_seq, maxlen = data_length)\nx_test = sequence.pad_sequences(x_test_seq, maxlen = data_length)","4b9e97b2":"from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Bidirectional, TimeDistributed\nfrom keras.callbacks import EarlyStopping","8a445285":"model = Sequential()\nmodel.add(Embedding(output_dim = output_length, input_dim = token_num, input_length = data_length))\nmodel.add(Dropout(dropout))\n\n# using BLSTM (this will be better than LSTM, Avg acc is around 0.94\nmodel.add(Bidirectional(LSTM(lstm_dim), merge_mode = 'sum'))\nmodel.add(Dropout(dropout))\n\n# using LSTM, Avg acc is around 0.93\n# model.add(LSTM(lstm_dim))\n# model.add(Dropout(dropout))\n\nmodel.add(Dense(units = 256, activation = 'relu'))\nmodel.add(Dropout(dropout))\n\nmodel.add(Dense(units = 1, activation = 'sigmoid'))\nprint(model.summary())","9ffd719b":"model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","8da5861e":"# \u63d0\u65e9\u7d50\u675f\nes = EarlyStopping(monitor = 'val_loss', patience = 2, verbose = 2)","0e8861ac":"train_history = model.fit(x = x_train,\n                         y = y_train_data,\n                         validation_split = 0.2,\n                         epochs = 20,\n                         batch_size = 512,\n                         verbose = 1,\n                         callbacks = [es])","a010c036":"%matplotlib inline\ndef show_train_history(train_history, train, validation):\n    plt.plot(train_history.history[train])\n    plt.plot(train_history.history[validation])\n    plt.title('Train History')\n    plt.ylabel(train)\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'validation'], loc = 'upper left')\n    plt.show()","4fa25360":"show_train_history(train_history, 'acc', 'val_acc')\nshow_train_history(train_history, 'loss', 'val_loss')","363550c9":"scores = model.evaluate(x = x_test, y = y_test_data)\nscores[1]","87be9c65":"# \u8b80\u53d6\u6a94\u6848\ntest_data = pd.read_csv('..\/input\/test.csv')\n\n# \u986f\u793a\u8cc7\u6599\u7684\u524d\u4e94\u7b46\ntest_data.head()","88bdb211":"test_id = test_data['id']\nx_test_data = test_data['title'].fillna(\"\")","e08b8040":"x_test_seq = token.texts_to_sequences(x_test_data)\nx_test = sequence.pad_sequences(x_test_seq, maxlen = data_length)\n\npredict = model.predict_classes(x_test)\npredict_classes = predict.reshape(-1)","fed85e7c":"test_data['label'] = [predict for predict in predict_classes]\ntest_data.head()","b603c544":"result = test_data[['id', 'label']]\nresult.head()","f368c14e":"# Any results you write to the current directory are saved as output.\nresult.to_csv('submission.csv', index = False)","5e2fc690":"# \u8b80\u53d6\u6e2c\u8a66\u8cc7\u6599 (Loading Test CSV)\n**\u63a5\u4e0b\u4f86\u532f\u5165\u6e2c\u8a66\u8cc7\u6599\uff0c\u4e26\u4f7f\u7528\u525b\u525b\u8a13\u7df4\u597d\u7684\u6a21\u578b\u4f86\u9032\u884c\u9810\u6e2c\uff0c\u4e26\u8f38\u51fa\u6a94\u6848\u3002**"}}