{"cell_type":{"92b9c5b9":"code","0b92b5b7":"code","a2020995":"code","b26baecd":"code","66a6983b":"code","7249b756":"code","9d6d37e0":"code","07248861":"code","0e92e912":"code","692773a0":"code","2539229a":"code","95f6c034":"code","093e245a":"code","fae69c23":"code","c95abf8b":"code","650a9c1d":"code","c64e4366":"code","db1221b3":"code","307c2478":"code","79c6a03b":"code","e71d7556":"code","a3e9784d":"code","a0ae347d":"code","91705c39":"code","b744c171":"code","76e34352":"code","f51225a1":"code","3125da95":"code","b40a7a96":"code","222ae666":"code","75c24c5a":"code","b1a4ce5b":"code","6b207e89":"code","cec6bac1":"code","98371180":"code","38e6f807":"code","2624ba86":"code","f94ffced":"code","cac6ad5d":"code","008c23a6":"code","78f9f299":"code","c26c7238":"code","19aabdaf":"code","659c21e2":"code","9e3f861e":"code","1e498b45":"code","17a320c3":"code","b5ce7de6":"code","b811936f":"code","57090a34":"code","f671cab8":"markdown","217ad014":"markdown","e08a5793":"markdown","c3c2f270":"markdown","73f50039":"markdown","095198d5":"markdown","98651165":"markdown","fa648a7c":"markdown","faf03608":"markdown","3130b8a6":"markdown","df54fd7d":"markdown","04b75b8d":"markdown","5d7c7a83":"markdown","4ab7293b":"markdown","5fd63850":"markdown","a192e0c0":"markdown","50f24379":"markdown","ed45956e":"markdown","0817a81e":"markdown","00a93d4a":"markdown","21e1ea85":"markdown","b001fa50":"markdown","770c476e":"markdown","0d7654d6":"markdown","c3b82e76":"markdown","81273883":"markdown","4b8cd99f":"markdown","8cd2cae3":"markdown","5496745c":"markdown","ed8557f5":"markdown","256b907d":"markdown","14b19397":"markdown","58e82189":"markdown","83282824":"markdown","ad5832f1":"markdown","a4301a8e":"markdown","14d33776":"markdown","0f91eec9":"markdown","1c1a8db5":"markdown","aefde6fd":"markdown","be3bec21":"markdown","8c33fcc6":"markdown","741cc1c6":"markdown","bae0c7ba":"markdown","eb54dfda":"markdown","6d6195e5":"markdown","87f54815":"markdown","7d2b455b":"markdown","cb786cbd":"markdown","2f0d5eeb":"markdown","3811604a":"markdown"},"source":{"92b9c5b9":"# data processing packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# visualization packages\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\n# categorical feature encoding packages\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# for reproducibility\nrandom_state = 47","0b92b5b7":"#load the data\ndata_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# pull off the survived feature from the training set\n# this will get dropped at certain points in the data engineering, and we will need to add it back on\ndata_train_survived = data_train.Survived\n\n#collect everything together for convenient feature encoding\ndata = pd.concat([data_train.drop(columns=['Survived']),data_test]).reset_index().drop(columns=['index'])\n\n#the features\nprint('The features: ')\nprint(data.columns.to_list())\n\ndata.head()","a2020995":"print('Percentage in each class:')\nprint(data_train.Pclass.value_counts().sort_index() \/ len(data_train))\nprint()\n\nprint('Survival rate:')\nprint(data_train.groupby('Pclass')['Survived'].mean())\nprint()\n\nprint('Correlation with survival: ')\nprint(data_train[['Pclass','Survived']].corr().iloc[0,1])\n\n\n# same information in graphical format\npalette = ['indianred','mediumseagreen']\n\nplt.figure(figsize=(16,8))\nsns.countplot(x='Pclass', hue='Survived', data=data_train, palette = palette)\n    \nplt.xlabel('Class')\nplt.ylabel('Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()","b26baecd":"# we start off by encoding it numerically\nfrom sklearn.preprocessing import LabelEncoder\n\nsex_encoder = LabelEncoder()\ndata.Sex = sex_encoder.fit_transform(data.Sex)\ndata_train.Sex = sex_encoder.transform(data_train.Sex)\ndata_test.Sex = sex_encoder.transform(data_test.Sex)\n\n# now let's examine the data\nprint('Percentage:')\nprint(data_train.Sex.value_counts().sort_index() \/ len(data_train))\nprint()\n\nprint('Survival rate:')\nprint(data_train.groupby('Sex')['Survived'].mean())\nprint()\n\nprint('Correlation with survival: ')\nprint(data_train[['Sex','Survived']].corr().iloc[0,1])\n\n\n# same information in graphical format\nplt.figure(figsize=(16,8))\nsns.countplot(x='Sex', hue='Survived', data=data_train, palette = palette)\n    \nplt.xlabel('Sex')\nplt.ylabel('Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()","66a6983b":"print('Number of null values: ')\nprint(data_train[['Age']].isnull().sum()[0])\nprint(round(100*data_train[['Age']].isnull().sum()[0]\/len(data_train),1),'% of training set')\nprint()\n\nprint('Correlation with survival: ')\nprint(data_train[['Age','Survived']].corr().iloc[0,1])\n\n\nsurvived = (data_train['Survived'] == 1)\n\nplt.figure(figsize=(16,8))\nbins=np.arange(0,85,3)\n\nax = sns.distplot(data_train[~survived].Age,bins=bins,color=palette[0],kde=False)\nax = sns.distplot(data_train[survived].Age,bins=bins,color=palette[1],kde=False)\n\nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()","7249b756":"print('Average age: ')\nprint(data_train.groupby('Sex')['Age'].mean())\nprint()\n\nprint(data_train.groupby('Pclass')['Age'].mean())\nprint()","9d6d37e0":"# simple correlation\nprint('Correlation with survival: ')\nprint(data_train[['SibSp','Survived']].corr().iloc[0,1])\n\n#figures looking at counts vs age for survivors and not\nplt.figure(figsize=(16, 8))\nsns.countplot(x='SibSp', hue='Survived', data=data_train, palette = palette)\n    \nplt.xlabel('SibSp')\nplt.ylabel('Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()\n\n# plotting survival rate as a function of SibSp\nbar_count = [0,1,2,3,4,5,8]\nbar_width = 0.85 \n\nsurvival_rate = data_train.groupby('SibSp')['Survived'].mean().tolist()\nperished_rate = [1-x for x in survival_rate]\n\nplt.figure(figsize=(16, 8))\n\nplt.bar(bar_count, survival_rate, color='#4BA473', edgecolor='white', width=bar_width, label='Survived')\nplt.bar(bar_count, perished_rate, bottom=survival_rate, color='#BF6A6A', edgecolor='white', width=bar_width, label='Perished')\n\nplt.legend(loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\nplt.xlabel('SibSp')\n\nplt.show()","07248861":"# simple correlation\nprint('Correlation with survival: ')\nprint(data_train[['Parch','Survived']].corr().iloc[0,1])\n\n#figures looking at counts vs age for survivors and not\nplt.figure(figsize=(16, 8))\nsns.countplot(x='Parch', hue='Survived', data=data_train, palette = palette)\n    \nplt.xlabel('Parch')\nplt.ylabel('Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()\n\n# plotting survival rate as a function of SibSp\nbar_count = [0,1,2,3,4,5,6]\nbar_width = 0.85 \n\nsurvival_rate = data_train.groupby('Parch')['Survived'].mean().tolist()\nperished_rate = [1-x for x in survival_rate]\n\nplt.figure(figsize=(16, 8))\n\nplt.bar(bar_count, survival_rate, color='#4BA473', edgecolor='white', width=bar_width, label='Survived')\nplt.bar(bar_count, perished_rate, bottom=survival_rate, color='#BF6A6A', edgecolor='white', width=bar_width, label='Perished')\n\nplt.legend(loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\nplt.xlabel('Parch')\n\nplt.show()","0e92e912":"# the size of a family is the number of siblings\/spouses, plus the number of parents\/children, plus 1 (the individual themselves)\ndata['FamilySize'] = data['SibSp'] + data['Parch'] + 1\ndata_train['FamilySize'] = data['FamilySize'].iloc[:891]\ndata_test['FamilySize'] = data['FamilySize'].iloc[891:]\n\n# simple correlation\nprint('Correlation with survival: ')\nprint(data_train[['FamilySize','Survived']].corr().iloc[0,1])\n\n# figures looking at counts vs age for survivors and not\nplt.figure(figsize=(16, 8))\nsns.countplot(x='FamilySize', hue='Survived', data=data_train, palette = palette)\n    \nplt.xlabel('FamilySize')\nplt.ylabel('Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()\n\n# plotting survival rate as a function of SibSp\nbar_count = [1,2,3,4,5,6,7,8,11]\nbar_width = 0.85 \n\nsurvival_rate = data_train.groupby('FamilySize')['Survived'].mean().tolist()\nperished_rate = [1-x for x in survival_rate]\n\nplt.figure(figsize=(16, 8))\n\nplt.bar(bar_count, survival_rate, color='#4BA473', edgecolor='white', width=bar_width, label='Survived')\nplt.bar(bar_count, perished_rate, bottom=survival_rate, color='#BF6A6A', edgecolor='white', width=bar_width, label='Perished')\n\nplt.legend(loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\nplt.xlabel('FamilySize')\n\nplt.show()","692773a0":"# define a function that calculates party size row by row\ndef calc_party_size(row):\n    return (data['Ticket']==row['Ticket']).sum()\n\n# create the PartySize feature\ndata['PartySize'] = data.apply(calc_party_size,axis=1)\ndata_train['PartySize'] = data['PartySize'].iloc[:891].tolist()\ndata_test['PartySize'] = data['PartySize'].iloc[891:].tolist()\n\n\n# simple correlation\nprint('Correlation with survival: ')\nprint(data_train[['PartySize','Survived']].corr().iloc[0,1])\n\n# figures looking at counts vs age for survivors and not\nplt.figure(figsize=(16, 8))\nsns.countplot(x='PartySize', hue='Survived', data=data_train, palette = palette)\n    \nplt.xlabel('PartySize')\nplt.ylabel('Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()\n\n# plotting survival rate as a function of SibSp\nbar_count = [1,2,3,4,5,6,7,8,11]\nbar_width = 0.85 \n\nsurvival_rate = data_train.groupby('PartySize')['Survived'].mean().tolist()\nperished_rate = [1-x for x in survival_rate]\n\nplt.figure(figsize=(16, 8))\n\nplt.bar(bar_count, survival_rate, color='#4BA473', edgecolor='white', width=bar_width, label='Survived')\nplt.bar(bar_count, perished_rate, bottom=survival_rate, color='#BF6A6A', edgecolor='white', width=bar_width, label='Perished')\n\nplt.legend(loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\nplt.xlabel('PartySize')\n\nplt.show()","2539229a":"# filling the missing values\ndata['Embarked'] = data['Embarked'].fillna('S')\ndata_train['Embarked'] = data_train['Embarked'].fillna('S')\ndata_test['Embarked'] = data_test['Embarked'].fillna('S')\n\n# displaying survival rate\nprint('Survival rates by emarked: ')\nprint(data_train.groupby('Embarked')['Survived'].mean())\n\n# survival count plots grouped by embarked\nplt.figure(figsize=(16, 8))\nsns.countplot(x='Embarked', hue='Survived', data=data_train, palette = palette)\n    \nplt.xlabel('Embarked')\nplt.ylabel('Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()\n\n# getting groupby's that list for a given embark point, percentage in each class\nclass_counts = data_train.groupby('Embarked')['Pclass'].value_counts() \nembarked_counts = data_train.Embarked.value_counts()\n\ns_percentages = class_counts['S']\/embarked_counts['S']\nq_percentages = class_counts['Q']\/embarked_counts['Q']\nc_percentages = class_counts['C']\/embarked_counts['C']\n\n# at this point, listed by embark point, now list by class\nthird_class_percentages = [s_percentages[3],c_percentages[3],q_percentages[3]]\nsecond_class_percentages = [s_percentages[2],c_percentages[2],q_percentages[2]]\nfirst_class_percentages = [s_percentages[1],c_percentages[1],q_percentages[1]]\n\n# create plot showing percentage in each class by embarked point\nbar_count = ['S','C','Q']\nbar_width = 0.85 \n\nplt.figure(figsize=(16, 8))\n\nplt.bar(bar_count, third_class_percentages, color='#BF6A6A', edgecolor='white', width=bar_width, label='Third Class')\nplt.bar(bar_count, second_class_percentages,bottom=third_class_percentages, color='#ffe680', edgecolor='white', width=bar_width, label='Second Class')\nplt.bar(bar_count, first_class_percentages,bottom=list(map(np.add,second_class_percentages,third_class_percentages)), color='#4BA473', edgecolor='white', width=bar_width, label='First Class')\n\nplt.legend(loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\nplt.xlabel('Embarked')\n\nplt.show()","95f6c034":"print('Percent male by embarked')\nprint(data_train.groupby(['Embarked'])['Sex'].mean())","093e245a":"def calc_fare_per_person(row):\n    return row['Fare']\/row['PartySize']\n\ndata['FareByHead'] = data.apply(calc_fare_per_person,axis=1)\ndata_train['FareByHead'] = data['FareByHead'].iloc[:891].tolist()\ndata_test['FareByHead'] = data['FareByHead'].iloc[891:].tolist()","fae69c23":"print('Average fare by head as function of class:')\nprint(data_train.groupby('Pclass')['FareByHead'].mean())\nprint()\n\n# simple correlation\nprint('Correlation fare with survival: ')\nprint(data_train[['Fare','Survived']].corr().iloc[0,1])\nprint()\n\nprint('Correlation fare by head with survival: ')\nprint(data_train[['FareByHead','Survived']].corr().iloc[0,1])\n\n# filling in nulls\ndata['Fare'] = data['Fare'].fillna(7.335304)\ndata['FareByHead'] = data['FareByHead'].fillna(7.335304)\ndata_test['Fare'] = data_test['Fare'].fillna(7.335304)\ndata_test['FareByHead'] = data_test['FareByHead'].fillna(7.335304)","c95abf8b":"print('Class-wise correlation of Fare with Survived: ')\nprint('First:',data_train.groupby(['Pclass'])[['Fare','Survived']].corr().loc[1].iloc[0,1])\nprint('Second:',data_train.groupby(['Pclass'])[['Fare','Survived']].corr().loc[2].iloc[0,1])\nprint('Third:',data_train.groupby(['Pclass'])[['Fare','Survived']].corr().loc[3].iloc[0,1])\nprint()\n\nprint('Class-wise correlation of FareByHead with Survived: ')\nprint('First:',data_train.groupby(['Pclass'])[['FareByHead','Survived']].corr().loc[1].iloc[0,1])\nprint('Second:',data_train.groupby(['Pclass'])[['FareByHead','Survived']].corr().loc[2].iloc[0,1])\nprint('Third:',data_train.groupby(['Pclass'])[['FareByHead','Survived']].corr().loc[3].iloc[0,1])","650a9c1d":"# FareByHead distribution for each class\nplt.figure(figsize=(16,8))\nbins = np.arange(0,135,3)\n\nsns.distplot(data_train[data_train.Pclass == 1].FareByHead,bins=bins,kde=False,label='First Class')\nsns.distplot(data_train[data_train.Pclass == 2].FareByHead,bins=bins,kde=False,label='Second Class')\nsns.distplot(data_train[data_train.Pclass == 3].FareByHead,bins=bins,kde=False,label='Third Class')\n\nplt.legend(loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()\n\n# survived and not survived vs. farebyhead\nplt.figure(figsize=(16,8))\n\nsns.distplot(data_train[data_train.Survived == 0].FareByHead,bins=bins,kde=False,label='Perished',color=palette[0])\nsns.distplot(data_train[data_train.Survived == 1].FareByHead,bins=bins,kde=False,label='Survived',color=palette[1])\n\nplt.legend(loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()","c64e4366":"data['FamilyName'] = data['Name'].apply(lambda x : x.split(', ')[0])\ndata['Title'] = data['Name'].apply(lambda x : x.split(', ')[1].split('. ')[0])\ndata = data.drop(columns=['Name'])\n\ndata_train['FamilyName'] = data['FamilyName'].iloc[:891]\ndata_train['Title'] = data['Title'].iloc[:891]\ndata_train = data_train.drop(columns=['Name'])\n\ndata_test['FamilyName'] = data['FamilyName'].iloc[891:]\ndata_test['Title'] = data['Title'].iloc[891:]\ndata_test = data_test.drop(columns=['Name'])","db1221b3":"data_train.Title.value_counts()","307c2478":"# survival count plots grouped by embarked\nplt.figure(figsize=(16, 8))\nsns.countplot(x='Title', hue='Survived', data=data_train, palette = palette)\n    \nplt.xlabel('Embarked')\nplt.ylabel('Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\n\nplt.show()","79c6a03b":"# create new deck feature from cabin, \ndata['Deck'] = data['Cabin'].apply(lambda x : x[0] if type(x)==str else 'M')\ndata = data.drop(columns=['Cabin'])\n\ndata_train['Deck'] = data_train['Cabin'].apply(lambda x : x[0] if type(x)==str else 'M')\ndata_train = data_train.drop(columns=['Cabin'])\n\ndata_test['Deck'] = data_test['Cabin'].apply(lambda x : x[0] if type(x)==str else 'M')\ndata_test = data_test.drop(columns=['Cabin'])","e71d7556":"print('Percent occupancy:')\nprint(data_train.Deck.value_counts().sort_index() \/ len (data_train))\nprint()\n\nprint('Survival Rate: ')\nprint(data_train.groupby('Deck')['Survived'].mean())\nprint()","a3e9784d":"plt.figure(figsize=(16,8))\nsns.set(style=\"darkgrid\")\n\nsns.countplot(x='Deck',order=['A','B','C','D','E','F','G','T'],hue='Survived',data=data_train,palette=palette)\n    \nplt.xlabel('Deck')\nplt.ylabel('Passenger Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\nplt.title('Title')\n\nplt.show()\n\nplt.figure(figsize=(16,8))\nsns.countplot(x='Deck',order=['M'],hue='Survived',data=data_train,palette=palette)\n    \nplt.xlabel('Deck')\nplt.ylabel('Passenger Count')    \nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\n    \nplt.legend(['Perished','Survived'],loc='upper right', bbox_to_anchor=(1, 1), prop={'size': 15})\nplt.title('Title')\n\nplt.show()","a0ae347d":"pd.options.mode.chained_assignment = None  # default='warn'\n\ndef impute_age(data_all,groupby_features=[],impute_strat='mean'):\n    # strategies: 'mean', 'median'\n    df, df_train, df_test = data_all[0].copy(), data_all[1].copy(), data_all[2].copy()\n    \n    if groupby_features == []:\n        if impute_strat == 'mean':\n            df['Age'] = df['Age'].fillna(df['Age'].mean())\n        elif impute_strat == 'median':\n            df['Age'] = df['Age'].fillna(df['Age'].median())\n        \n    else:\n        if impute_strat == 'mean':\n            df['Age'] = df.groupby(groupby_features)['Age'].apply(lambda x : x.fillna(x.mean()))\n        elif impute_strat == 'median':\n            df['Age'] = df.groupby(groupby_features)['Age'].apply(lambda x : x.fillna(x.median()))\n    \n    # apply the same changes to the training and test sets\n    df_train = df.iloc[:891]\n    df_train['Survived'] = data_train_survived\n    df_test = df.iloc[891:]\n        \n    return df, df_train, df_test","91705c39":"def encode_title(data_all,binning_strat='none',encoding_strat='drop'):\n    # binning strategies: 'none', 'coarse', 'fine'\n    # encoding strategies: 'drop', 'label', 'freq', 'one_hot'\n    df, df_train, df_test = data_all[0].copy(), data_all[1].copy(), data_all[2].copy()\n    \n    # gather together all titles that simply mean 'Miss'\n    df['Title'] = df['Title'].replace(['Miss','Ms', 'Mlle','Mme'], 'Miss')\n    \n    # bin the titles according to different stategies\n    if binning_strat == 'none':\n        pass\n    \n    elif binning_strat == 'coarse':\n        df['Title'] = df['Title'].replace(['Dr','Col','Major','Jonkheer','Capt','Sir','Don','Dona','Rev','Lady','the Countess'], 'Dr\/Military\/Noble\/Clergy')\n    \n    elif binning_strat == 'fine':\n        df['Title'] = df['Title'].replace(['Col','Major','Capt'], 'Military')\n        df['Title'] = df['Title'].replace(['Jonkheer','Sir','Don','Dona','Lady','the Countess'], 'Noble')\n        df['Title'] = df['Title'].replace(['Rev'], 'Clergy')\n    \n    # encode the titles numerically according to different strategies\n    if encoding_strat == 'drop':\n        df = df.drop(columns=['Title'])\n        \n    elif encoding_strat == 'label':\n        encoder = LabelEncoder()\n        df['Title'] = encoder.fit_transform(df['Title'])\n    \n    elif encoding_strat == 'freq':\n        # size of each category\n        encoding = df.groupby('Title').size()\n        # get frequency of each category\n        encoding = encoding\/len(df)\n        df['Title'] = df['Title'].map(encoding)\n    \n    elif encoding_strat == 'one_hot':\n        encoder = OneHotEncoder(sparse=False)\n        df = df.join(\n                        pd.DataFrame(encoder.fit_transform(df['Title'].to_numpy().reshape(-1,1)),\n                            columns=encoder.get_feature_names(['Title']),\n                            index=df.index)\n                    ).drop(columns=['Title'])\n    \n    # apply the same changes to the training and test sets\n    df_train = df.iloc[:891]\n    df_train['Survived'] = data_train_survived\n    df_test = df.iloc[891:]\n        \n    return df, df_train, df_test","b744c171":"def encode_deck(data_all,binning_strat='none',encoding_strat='one_hot'):\n    #binning strategies: 'none', 'fine', 'coarse'\n    #encoding strategies: 'drop', 'label', 'freq', 'one_hot'\n    df, df_train, df_test = data_all[0].copy(), data_all[1].copy(), data_all[2].copy()\n    \n    # bin the titles according to different stategies\n    # at the very least, we will always group T with A\n    df['Deck'] = df['Deck'].replace(['A','T'],'A')\n        \n    if binning_strat == 'none':\n        pass\n    \n    elif binning_strat == 'fine':\n        df['Deck'] = df['Deck'].replace(['B', 'C'], 'BC')\n        df['Deck'] = df['Deck'].replace(['D', 'E'], 'DE')\n        \n    elif binning_strat == 'coarse':\n        df['Deck'] = df['Deck'].replace(['B', 'C'], 'BC')\n        df['Deck'] = df['Deck'].replace(['D', 'E'], 'DE')\n        df['Deck'] = df['Deck'].replace(['F', 'G'], 'FG')\n    \n    # encode the deck numerically according to different strategies\n    if encoding_strat == 'drop':\n        df = df.drop(columns=['Deck'])\n        \n    elif encoding_strat == 'label':\n        df['DeckMissing'] = 1*(df['Deck'] == 'M')\n        \n        encoder = LabelEncoder()\n        df['Deck'] = encoder.fit_transform(df['Deck'])\n    \n    elif encoding_strat == 'freq':\n        df['DeckMissing'] = 1*(df['Deck'] == 'M')\n        \n        # size of each category\n        encoding = df.groupby('Deck').size()\n        # get frequency of each category\n        encoding = encoding\/len(df)\n        df['Deck'] = df['Deck'].map(encoding)\n    \n    elif encoding_strat == 'one_hot':\n        encoder = OneHotEncoder(sparse=False)\n        df = df.join(\n                        pd.DataFrame(encoder.fit_transform(df['Deck'].to_numpy().reshape(-1,1)),\n                            columns=encoder.get_feature_names(['Deck']),\n                            index=df.index)\n                    ).drop(columns=['Deck'])\n    \n    # apply the same changes to the training and test sets\n    df_train = df.iloc[:891]\n    df_train['Survived'] = data_train_survived\n    df_test = df.iloc[891:]\n        \n    return df, df_train, df_test","76e34352":"# for reproducibility\nnp.random.seed(seed=random_state)\n\n# function that calculates the survival rate for a passenger's family in the training data set\ndef calc_family_survival_rate(row):\n    # first isolate which passengers in the training set are family members\n    family_in_train = (data_train['FamilyName'] == row['FamilyName'])\n    \n    # if the passenger is also in the training set, we manually set him to false\n    # including him counts a data leakage, since information on whether or not he survived is then included in the feature\n    # without this, we overfit: although our 5-fold cv score rises from 0.84 to 0.90 in model training, our competition score lowers from 0.79425 to 0.75837\n    if row.name <= 890:\n        family_in_train.iloc[row.name] = False\n    \n    # numerator and denominator for the survival rate\n    family_survived_in_train = data_train[family_in_train].Survived.sum()\n    family_size_in_train = family_in_train.sum()\n    \n    # if there's no data to generalize from, generate a random number, otherwise give survival rate\n    if family_size_in_train == 0:\n        return 1, np.random.rand()\n    else:\n        return 0, family_survived_in_train \/ family_size_in_train\n\n# function that calculates the survival rate for a passenger's party in the training data set\ndef calc_party_survival_rate(row):\n    # first isolate which passengers in the training set are in the same party\n    party_in_train = (data_train['Ticket'] == row['Ticket'])\n    \n    # if the passenger is also in the training set, we manually set him to false\n    # including him counts a data leakage, since information on whether or not he survived is then included in the feature\n    # without this, we overfit: although our 5-fold cv score rises from 0.84 to 0.90 in model training, our competition score lowers from 0.79425 to 0.75837\n    if row.name <= 890:\n        party_in_train.iloc[row.name] = False\n    \n    # numerator and denominator for the survival rate\n    party_survived_in_train = data_train[party_in_train].Survived.sum()\n    party_size_in_train = party_in_train.sum()\n    \n    # if there's no data to generalize from, generate a random number, otherwise give survival rate\n    if party_size_in_train == 0:\n        return 1, np.random.rand()\n    else:\n        return 0, party_survived_in_train \/ party_size_in_train\n\n# generate the family_survival_rate_NA and family_survival_rate features and separate them into two columns\ndata['family_survival_rate'] = data.apply(calc_party_survival_rate,axis=1)\ndata[['family_survival_rate_NA','family_survival_rate']] = pd.DataFrame(data['family_survival_rate'].tolist(), index=data.index) \n\n# copy the data to the training and test sets\ndata_train[['family_survival_rate_NA','family_survival_rate']] = data[['family_survival_rate_NA','family_survival_rate']].iloc[:891]\ndata_test[['family_survival_rate_NA','family_survival_rate']] = data[['family_survival_rate_NA','family_survival_rate']].iloc[891:]\n\n# generate the party_survival_rate_NA and party_survival_rate features and separate them into two columns\ndata['party_survival_rate'] = data.apply(calc_party_survival_rate,axis=1)\ndata[['party_survival_rate_NA','party_survival_rate']] = pd.DataFrame(data['party_survival_rate'].tolist(), index=data.index)\n\n# copy the data to the training and test sets\ndata_train[['party_survival_rate_NA','party_survival_rate']] = data[['party_survival_rate_NA','party_survival_rate']].iloc[:891]\ndata_test[['party_survival_rate_NA','party_survival_rate']] = data[['party_survival_rate_NA','party_survival_rate']].iloc[891:]\n\n# we no longer need the information on family name\ndata = data.drop(columns=['FamilyName'])\ndata_train = data_train.drop(columns=['FamilyName'])\ndata_test = data_test.drop(columns=['FamilyName'])\n\n# we no longer need the information on ticket number\ndata = data.drop(columns=['Ticket'])\ndata_train = data_train.drop(columns=['Ticket'])\ndata_test = data_test.drop(columns=['Ticket'])","f51225a1":"def num_process(column,data_all,n_bins=10,strategy='none'):\n    # strategires: 'none', 'remove', 'log', 'equal_width', 'log_equal_width', 'equal_freq'\n    df, df_train, df_test = data_all[0].copy(), data_all[1].copy(), data_all[2].copy()\n    \n    if strategy == 'none':\n        pass\n    \n    elif strategy == 'drop':\n        df = df.drop(columns=[column])\n    \n    elif strategy == 'log':\n        df[column] = np.log(df[column]+1)\n    \n    elif strategy == 'equal_width':\n        df[column] = pd.cut(df[column],n_bins,labels=False)\n    \n    elif strategy == 'log_equal_width':\n        df[column] = np.log(df[column]+1)\n        df[column] = data.cut(df[column],n_bins,labels=False)\n    \n    elif strategy == 'equal_freq':\n        df[column] = data.qcut(df[column],n_bins,labels=False)\n    \n    # apply the same changes to the training and test sets\n    df_train = df.iloc[:891]\n    df_train['Survived'] = data_train_survived\n    df_test = df.iloc[891:]\n        \n    return df, df_train, df_test\n    \ndef cat_process(column,data_all,strategy='none'):\n    # strategies: 'none', 'drop', label', 'freq', 'one_hot'\n    df, df_train, df_test = data_all[0].copy(), data_all[1].copy(), data_all[2].copy()\n    \n    #do nothing\n    if strategy == 'none':\n        pass\n    \n    #drop column\n    elif strategy == 'drop':\n        df = df.drop(columns=[column])\n        \n        df_train = df.iloc[:891]\n        df_train['Survived'] = data_train_survived\n        df_test = df.iloc[891:]\n        \n        return df, df_train, df_test\n    \n    \n    #if neither of these, then we want to do some encoding\n    #first fill na's so can send through the encoder\n    na_str = 'NA'\n    df[column] = df[column].apply(lambda x : x if (type(x)==str or type(x)==int) else na_str)\n    \n    #applies simple ordinal label encoder\n    if strategy == 'label':\n        encoder = LabelEncoder()\n        df[column] = encoder.fit_transform(df[column])\n    \n    #frequency encoding\n    elif strategy == 'freq':\n        # size of each category\n        encoding = df.groupby(column).size()\n        # get frequency of each category\n        encoding = encoding\/len(df)\n        df[column] = df[column].map(encoding)\n    \n    #one-hot encodes\n    elif strategy == 'one_hot':\n        encoder = OneHotEncoder(sparse=False)\n        df = df.join(\n                        pd.DataFrame(encoder.fit_transform(df[column].to_numpy().reshape(-1,1)),\n                            columns=encoder.get_feature_names([column]),\n                            index=df.index)\n                    ).drop(columns=[column])\n\n    \n    # apply the same changes to the training and test sets\n    df_train = df.iloc[:891]\n    df_train['Survived'] = data_train_survived\n    df_test = df.iloc[891:]\n        \n    return df, df_train, df_test","3125da95":"data.head()","b40a7a96":"# define default strategies for the preprocessing function\ndef_Age_strat = {'groupby_features':['Pclass','Sex'],\n                         'impute_strat':'mean'}\ndef_Title_strat = {'binning_strat':'coarse',\n                          'encoding_strat':'one_hot'}\ndef_Deck_strat = {'binning_strat':'none',\n                         'encoding_strat':'one_hot'}\n\ndef_family_survival_rate_strat = {'n_bins':10,\n                                      'strategy':'none'}\ndef_party_survival_rate_strat = {'n_bins':10,\n                                      'strategy':'none'}\n\ndef_Pclass_strat = 'none'\ndef_Sex_strat = 'none'\ndef_Embarked_strat = 'one_hot'\n\ndef_SibSp_strat = {'n_bins':10,\n                          'strategy':'none'}\ndef_Parch_strat = {'n_bins':10,\n                          'strategy':'none'}\ndef_FamilySize_strat = {'n_bins':10,\n                          'strategy':'none'}\ndef_PartySize_strat = {'n_bins':10,\n                          'strategy':'none'}\ndef_Fare_strat = {'n_bins':10,\n                          'strategy':'none'}\ndef_FareByHead_strat = {'n_bins':10,\n                          'strategy':'none'}\n\ndef_strat = {'Age':def_Age_strat,\n                'Title':def_Title_strat,\n                'Deck':def_Deck_strat,\n                'family_survival_rate':def_family_survival_rate_strat,\n                'party_survival_rate':def_party_survival_rate_strat,\n                'Pclass':def_Pclass_strat,\n                'Sex':def_Sex_strat,\n                'Embarked':def_Embarked_strat,\n                'SibSp':def_SibSp_strat,\n                'Parch':def_Parch_strat,\n                'FamilySize':def_FamilySize_strat,\n                'PartySize':def_PartySize_strat,\n                'Fare':def_Fare_strat,\n                'FareByHead':def_FareByHead_strat}\n\n# function for automated data preprocessing\ndef preprocessing(data_all,strat=def_strat):\n    df, df_train, df_test = data_all[0].copy(), data_all[1].copy(), data_all[2].copy()\n    \n    # always drop the passenger id\n    df, df_train, df_test = num_process(data_all=[df,df_train,df_test],column='PassengerId',strategy='drop')\n\n    # strategy for dealing with the five special features we focussed on\n    df, df_train, df_test = impute_age([df,df_train,df_test],groupby_features=strat['Age']['groupby_features'],impute_strat=strat['Age']['impute_strat'])\n    df, df_train, df_test = encode_title([df,df_train,df_test],binning_strat=strat['Title']['binning_strat'],encoding_strat=strat['Title']['encoding_strat'])\n    df, df_train, df_test = encode_deck([df,df_train,df_test],binning_strat=strat['Deck']['binning_strat'],encoding_strat=strat['Deck']['encoding_strat'])\n    \n    df, df_train, df_test = num_process(data_all=[df,df_train,df_test],column='family_survival_rate',n_bins=strat['family_survival_rate']['n_bins'],strategy=strat['family_survival_rate']['strategy'])\n    df, df_train, df_test = cat_process(data_all=[df,df_train,df_test],column='family_survival_rate_NA',strategy=strat['family_survival_rate']['strategy'])\n    \n    df, df_train, df_test = num_process(data_all=[df,df_train,df_test],column='party_survival_rate',n_bins=strat['party_survival_rate']['n_bins'],strategy=strat['party_survival_rate']['strategy'])\n    df, df_train, df_test = cat_process(data_all=[df,df_train,df_test],column='party_survival_rate_NA',strategy=strat['party_survival_rate']['strategy'])\n\n    # loop through remaining categorical features\n    for feature in ['Pclass','Sex','Embarked']:\n        df, df_train, df_test = cat_process(data_all=[df,df_train,df_test],column=feature,strategy=strat[feature])\n    \n    # loop through remaining numerical features\n    for feature in ['SibSp','Parch','FamilySize','PartySize','Fare','FareByHead']:\n        df, df_train, df_test = num_process(data_all=[df,df_train,df_test],column=feature,n_bins=strat[feature]['n_bins'],strategy=strat[feature]['strategy'])\n        \n    return df, df_train, df_test\n\nfrom sklearn.base import clone\n\n# function that will loop over data preprocessing strategies, train and evaluate a model, and give the results\ndef feature_engineering_loop(data_all,clf,strats=[def_strat],cv_folds=5,verbose=True):\n    df, df_train, df_test = data_all[0].copy(), data_all[1].copy(), data_all[2].copy()\n    \n    cv_scores = []\n    cv_scores_mean = []\n    \n    for x in range(len(strats)):\n        df_processed_train = preprocessing(data_all=[df,df_train,df_test],strat=strats[x])[1]\n        \n        X = df_processed_train.drop(columns=['Survived']).to_numpy()\n        y = df_processed_train.Survived.to_numpy()\n        \n        model = clone(clf)\n        cv_score = cross_val_score(model,X,y,cv=cv_folds)\n        cv_score_mean = cv_score.mean()\n        \n        cv_scores.append(cv_score)\n        cv_scores_mean.append(cv_score.mean())\n        \n        if verbose:\n            print('Completed model',x+1,'of',len(strats),'\\n')\n            print('\\tcv-score:',cv_score)\n            print('\\tmean:',cv_score_mean)\n            print()\n    \n    best_index = np.argmax(cv_scores_mean)\n    best_strat = strats[best_index]\n    best_cv_score = cv_scores[best_index]\n    best_cv_score_mean = cv_scores_mean[best_index]\n    \n    return {'best_index':best_index,\n               'best_strat':best_strat,\n               'best_cv_score':best_cv_score,\n               'best_cv_score_mean':best_cv_score_mean,\n               'cv_scores':cv_scores,\n               'cv_scores_mean':cv_scores_mean,\n               'strats':strats}","222ae666":"data_processed, data_processed_train, data_processed_test = preprocessing(data_all=[data,data_train,data_test])\n\ndata_processed.head()","75c24c5a":"#separating into feature and target variables\nX = data_processed_train.drop(columns=['Survived']).to_numpy()\ny = data_processed_train.Survived.to_numpy()\n\n# train a random forest classifier with the above feature engineering strategy\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf_rf = RandomForestClassifier(n_estimators=40,\n              criterion='entropy',\n              max_features='auto',\n              min_samples_leaf=4,\n              min_samples_split=5,\n              max_depth=5,\n              random_state=random_state,\n              bootstrap=True,\n              verbose = False,\n              n_jobs=-1\n            )\n\ncv = cross_val_score(clf_rf,X,y,cv=5)\n\nprint('Performance: ')\nprint(cv)\nprint(cv.mean())","b1a4ce5b":"from copy import deepcopy\nfrom itertools import product\n\n# the strategies we will iterate over\ntitle_strats = ['none','fine','coarse']\ndeck_strats = ['none','fine','coarse']\n\n# create the list of strategies that goes over all options\nstrats = []\n\nfor pair in product(title_strats,deck_strats):\n    strat = deepcopy(def_strat)\n    \n    strat['Title']['binning_strat'] = pair[0]\n    strat['Deck']['binning_strat'] = pair[1]\n    \n    strats.append(strat)\n\n# try out the strategies\nloop_result = feature_engineering_loop([data,data_train,data_test],clf_rf,strats=strats,cv_folds=5,verbose=True)\n\n# report on the final result\nprint('\\nBest strategy: (title, deck) =',(loop_result['strats'][loop_result['best_index']]['Title']['binning_strat'],loop_result['strats'][loop_result['best_index']]['Deck']['binning_strat']))\nprint('\\twith mean score:',loop_result['best_cv_score_mean'])","6b207e89":"# create dictionary containing all results\n# key = (title_strat,deck_strat), value = mean_cv_score\nscores = loop_result['cv_scores_mean']\nresults_dict = {(loop_result['strats'][x]['Title']['binning_strat'],loop_result['strats'][x]['Deck']['binning_strat']):scores[x] for x in range(len(scores))}\n\n# process into dataframe that seaborn can read\nser = pd.Series(list(results_dict.values()),\n                  index=pd.MultiIndex.from_tuples(results_dict.keys()))\ndf = ser.unstack().fillna(0)\n\n# plot it\nplt.figure(figsize=(10,8))\n\nax = sns.heatmap(df,annot=True,fmt=\".2%\",cmap=\"Blues\")\nax.invert_xaxis()\n\nplt.xlabel('Deck')\nplt.ylabel('Title')\nplt.show()\n\n# create a strategy that keeps track of our best choices as we go\nbest_strat = deepcopy(def_strat)\nbest_strat['Title']['binning_strat'] = 'coarse'\nbest_strat['Deck']['binning_strat'] = 'fine'","cec6bac1":"# the strategies we will iterate over\ngroupby_strats = [[],['Sex'],['Pclass'],['SibSp'],['Parch'],['Pclass','Sex'],['Pclass','SibSp'],['Pclass','Parch']]\nimpute_strats = ['mean','median']\n\n# create the list of strategies that goes over all options\nstrats = []\n\nfor pair in product(groupby_strats,impute_strats):\n    strat = deepcopy(best_strat)\n    \n    strat['Age']['groupby_features'] = pair[0]\n    strat['Age']['impute_strat'] = pair[1]\n    \n    strats.append(strat)\n\n# try out the strategies\nloop_result = feature_engineering_loop([data,data_train,data_test],clf_rf,strats=strats,cv_folds=5,verbose=True)\n\n# report on the final result\nprint('\\nBest strategy:',loop_result['strats'][loop_result['best_index']]['Age'])\nprint('\\twith mean score:',loop_result['best_cv_score_mean'])","98371180":"print(loop_result['strats'][4]['Age'])\nprint(loop_result['strats'][10]['Age'])\nprint(loop_result['strats'][13]['Age'])","38e6f807":"best_strat['Age']['groupby_features'] = ['Pclass', 'Sex']\nbest_strat['Age']['impute_strat'] = 'mean'","2624ba86":"# the features we will experiment with\nfeatures_list = ['SibSp','Parch','FamilySize','PartySize','Fare','FareByHead']\n\n# the strategies we will iterate over\nstrategies = ['none','drop']\n\n# create the list of strategies that goes over all options\nstrats = []\n\nfor tup in product(strategies,repeat=len(features_list)):\n    strat = deepcopy(best_strat)\n    \n    for x in range(len(tup)):\n        strat[features_list[x]]['strategy'] = tup[x]\n    \n    strats.append(strat)\n    \n# try out the strategies\nloop_result = feature_engineering_loop([data,data_train,data_test],clf_rf,strats=strats,cv_folds=5,verbose=True)\n\n# report on the final result\nprint('\\nBest strategy:')\n\nfor feature in features_list:\n    print('\\t'+feature+':',loop_result['strats'][loop_result['best_index']][feature]['strategy'])\n    \nprint('\\n\\twith mean score:',loop_result['best_cv_score_mean'])","f94ffced":"best_strat['SibSp']['strategy'] = 'drop'\nbest_strat['Parch']['strategy'] = 'drop'\nbest_strat['FamilySize']['strategy'] = 'none'\nbest_strat['PartySize']['strategy'] = 'none'\nbest_strat['Fare']['strategy'] = 'drop'\nbest_strat['FareByHead']['strategy'] = 'none'","cac6ad5d":"# the features we will experiment with\nfeatures_list = ['family_survival_rate','party_survival_rate']\n\n# the strategies we will iterate over\nstrategies = ['none','drop']\n\n# create the list of strategies that goes over all options\nstrats = []\n\nfor tup in product(strategies,repeat=len(features_list)):\n    strat = deepcopy(best_strat)\n    \n    for x in range(len(tup)):\n        strat[features_list[x]]['strategy'] = tup[x]\n    \n    strats.append(strat)\n    \n# try out the strategies\nloop_result = feature_engineering_loop([data,data_train,data_test],clf_rf,strats=strats,cv_folds=5,verbose=True)\n\n# report on the final result\nprint('\\nBest strategy:')\n\nfor feature in features_list:\n    print('\\t'+feature+':',loop_result['strats'][loop_result['best_index']][feature]['strategy'])\n    \nprint('\\n\\twith mean score:',loop_result['best_cv_score_mean'])","008c23a6":"# strategies by feature\nembarked_strats = ['label','freq','one_hot'] # none doesn't work here since not in numerical format yet\npclass_strats = ['none','freq', 'one_hot'] # label encoding is the same as none here since already in numerical format\n\n# create the list of strategies that goes over all options\nstrats = []\n\nfor (embarked_strat,pclass_strat) in product(embarked_strats,pclass_strats):\n    strat = deepcopy(best_strat)\n    \n    strat['Embarked'] = embarked_strat\n    strat['Pclass'] = pclass_strat\n    \n    strats.append(strat)\n    \n# try out the strategies\nloop_result = feature_engineering_loop([data,data_train,data_test],clf_rf,strats=strats,cv_folds=5,verbose=True)\n\n# report on the final result\nprint('\\nBest strategy:')\nprint('\\tEmbarked strat:',loop_result['strats'][loop_result['best_index']]['Embarked'])\nprint('\\tPclass strat:',loop_result['strats'][loop_result['best_index']]['Pclass'])\n    \nprint('\\n\\twith mean score:',loop_result['best_cv_score_mean'])","78f9f299":"# create dictionary containing all results\n# key = (embarked_strat,pclass_strat), value = mean_cv_score\nscores = loop_result['cv_scores_mean']\nresults_dict = {(loop_result['strats'][x]['Embarked'],loop_result['strats'][x]['Pclass']) : scores[x] for x in range(len(scores))}\n\n# process into dataframe that seaborn can read\nser = pd.Series(list(results_dict.values()),\n                  index=pd.MultiIndex.from_tuples(results_dict.keys()))\ndf = ser.unstack()\n\n# plot it\nplt.figure(figsize=(10,8))\n\nax = sns.heatmap(df,annot=True,fmt=\".2%\",cmap=\"Blues\")\nax.invert_xaxis()\n\nplt.xlabel('Pclass')\nplt.ylabel('Embarked')\nplt.show()","c26c7238":"# strategies by feature\nencoding_strats = ['label','freq','one_hot']\n\n# create the list of strategies that goes over all options\nstrats = []\n\nfor (title_strat,deck_strat) in product(encoding_strats,repeat=2):\n    strat = deepcopy(best_strat)\n    \n    strat['Title']['encoding_strat'] = title_strat\n    strat['Deck']['encoding_strat'] = deck_strat\n    \n    strats.append(strat)\n    \n# try out the strategies\nloop_result = feature_engineering_loop([data,data_train,data_test],clf_rf,strats=strats,cv_folds=5,verbose=True)\n\n# report on the final result\nprint('\\nBest strategy:')\nprint('\\tTitle strat:',loop_result['strats'][loop_result['best_index']]['Title']['encoding_strat'])\nprint('\\tDeck strat:',loop_result['strats'][loop_result['best_index']]['Deck']['encoding_strat'])\n    \nprint('\\n\\twith mean score:',loop_result['best_cv_score_mean'])","19aabdaf":"# create dictionary containing all results\n# key = (embarked_strat,pclass_strat), value = mean_cv_score\nscores = loop_result['cv_scores_mean']\nresults_dict = {(loop_result['strats'][x]['Title']['encoding_strat'],loop_result['strats'][x]['Deck']['encoding_strat']) : scores[x] for x in range(len(scores))}\n\n# process into dataframe that seaborn can read\nser = pd.Series(list(results_dict.values()),\n                  index=pd.MultiIndex.from_tuples(results_dict.keys()))\ndf = ser.unstack()\n\n# plot it\nplt.figure(figsize=(10,8))\n\nax = sns.heatmap(df,annot=True,fmt=\".2%\",cmap=\"Blues\")\nax.invert_xaxis()\n\nplt.xlabel('Deck')\nplt.ylabel('Title')\nplt.show()","659c21e2":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# feature engineering according to our best strategy\ndata_processed, data_processed_train, data_processed_test = preprocessing(data_all=[data,data_train,data_test],strat=best_strat)\n\n#separating into feature and target variables\nX = data_processed_train.drop(columns=['Survived']).to_numpy()\ny = data_processed_train.Survived.to_numpy()\n\n# Random Forest Classifier\nrf = RandomForestClassifier(random_state=random_state)\nparam_grid = {'n_estimators': np.arange(25,50,5),\n                'criterion':['gini','entropy'],\n                'bootstrap': [True],\n                'max_depth': np.arange(10,15,1),\n                'max_features': ['auto','sqrt'],\n                'min_samples_leaf': [2],\n                'min_samples_split': [2]\n             }\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X,y)\n\n# SVC classifier\nsvc = SVC(verbose=False,random_state=random_state)\nparam_grid = {'C': np.arange(20,35,2),\n                'kernel': ['rbf'],\n                'gamma': np.arange(0.00000000001,0.02,0.002),\n                'coef0': np.arange(-119.99,-40,20)\n             }\n\nclf_svc = GridSearchCV(svc,param_grid=param_grid,cv=5,verbose=True,n_jobs=-1)\nbest_clf_svc = clf_svc.fit(X,y)\n\n# XGBClassifier\nparam_grid = {'n_estimators': np.arange(10,40,5),\n              'learning_rate': np.arange(0.15,0.31,0.05),\n              'reg_lambda': np.arange(0.45,0.7,0.05),\n              'min_split_loss': np.arange(0,0.05,0.01) \n}\nxgb = XGBClassifier(seed=random_state)\n\nclf_xgb = GridSearchCV(xgb,param_grid=param_grid,cv=5,verbose=True,n_jobs=-1)\nbest_clf_xgb = clf_xgb.fit(X,y)\n\n# AdaBoostClassifier\nparam_grid = {'n_estimators': np.arange(10,30,5),\n              'learning_rate': np.arange(0.5,0.7,0.002),\n              'algorithm': ['SAMME.R']   \n}\nada = AdaBoostClassifier(random_state=random_state)\n\nclf_ada = GridSearchCV(ada,param_grid=param_grid,cv=5,verbose=True,n_jobs=-1)\nbest_clf_ada = clf_ada.fit(X,y)","9e3f861e":"# best results for Random Forest\nbest_params = best_clf_rf.best_params_\nprint('Best Parameters for Random Forest: ')\nprint(best_params)\nprint()\n\nclf_rf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n              criterion=best_params['criterion'],\n              bootstrap=True,\n              max_depth=best_params['max_depth'],\n              max_features=best_params['max_features'],\n              min_samples_leaf=best_params['min_samples_leaf'],\n              min_samples_split=best_params['min_samples_split'],\n              random_state=random_state)\n\ncv = cross_val_score(clf_rf,X,y,cv=5)\n\nprint('Performance: ')\nprint(cv)\nprint(cv.mean())\nprint()\nprint()","1e498b45":"# best results for SVC\nbest_params = best_clf_svc.best_params_\nprint('Best Parameters for SVC: ')\nprint(best_params)\nprint()\n\nclf_svc = SVC(C=best_params['C'],\n              gamma=best_params['gamma'],\n              coef0=best_params['coef0'],\n              kernel=best_params['kernel'],\n              probability=True,\n              verbose=False,\n              random_state=random_state)\n\ncv = cross_val_score(clf_svc,X,y,cv=5)\n\nprint('Performance: ')\nprint(cv)\nprint(cv.mean())\nprint()\nprint()","17a320c3":"# best results for XGBoost\nbest_params = best_clf_xgb.best_params_\nprint('Best Parameters for XGBoost: ')\nprint(best_params)\nprint()\n\nclf_xgb = XGBClassifier(n_estimators=best_params['n_estimators'],\n                        learning_rate=best_params['learning_rate'],\n                        reg_lambda=best_params['reg_lambda'],\n                        min_split_loss=best_params['min_split_loss'],\n                        seed=random_state)\n\ncv = cross_val_score(clf_xgb,X,y,cv=5)\n\nprint('Performance: ')\nprint(cv)\nprint(cv.mean())\nprint()\nprint()","b5ce7de6":"# best results for AdaBoost\nbest_params = best_clf_ada.best_params_\nprint('Best Parameters for AdaBoost: ')\nprint(best_params)\nprint()\n\nclf_ada = AdaBoostClassifier(n_estimators=best_params['n_estimators'],\n                        learning_rate=best_params['learning_rate'],\n                        algorithm=best_params['algorithm'],\n                        random_state=random_state)\n\ncv = cross_val_score(clf_ada,X,y,cv=5)\n\nprint('Performance: ')\nprint(cv)\nprint(cv.mean())\nprint()\nprint()","b811936f":"from sklearn.ensemble import VotingClassifier\nclf_hard = VotingClassifier(estimators = [('rf',clf_rf),('svc',clf_svc),('xgb',clf_xgb),('ada',clf_ada)], voting = 'hard') \nclf_soft = VotingClassifier(estimators = [('rf',clf_rf),('svc',clf_svc),('xgb',clf_xgb),('ada',clf_ada)], voting = 'soft') \n\nprint('Hard voting cross-validation:',cross_val_score(clf_hard,X,y,cv=5))\nprint('Hard voting mean mean :',cross_val_score(clf_hard,X,y,cv=5).mean())\nprint()\n\nprint('Soft voting cross-validation:',cross_val_score(clf_soft,X,y,cv=5))\nprint('Soft voting mean mean :',cross_val_score(clf_soft,X,y,cv=5).mean())\n","57090a34":"clf_rf.fit(X,y)\ny_pred = clf_rf.predict(data_processed_test.to_numpy())\n\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = data_test['PassengerId']\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions_rf.csv', header=True, index=False)\n\nclf_svc.fit(X,y)\ny_pred = clf_svc.predict(data_processed_test.to_numpy())\n\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions_svc.csv', header=True, index=False)\n\nclf_xgb.fit(X,y)\ny_pred = clf_xgb.predict(data_processed_test.to_numpy())\n\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions_xgb.csv', header=True, index=False)\n\nclf_ada.fit(X,y)\ny_pred = clf_ada.predict(data_processed_test.to_numpy())\n\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions_ada.csv', header=True, index=False)\n\nclf_hard.fit(X,y)\ny_pred = clf_hard.predict(data_processed_test.to_numpy())\n\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions_hard.csv', header=True, index=False)\n\nclf_soft.fit(X,y)\ny_pred = clf_soft.predict(data_processed_test.to_numpy())\n\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions_soft.csv', header=True, index=False)","f671cab8":"The difference doesn't seem to be much, but dropping `'SibSp'` and `'Parch'` in favor of `'FamilySize'` and `'PartySize'`, as well as dropping `'Fare'` in favor of `'FareByHead'` raises our cross-validated training score from 0.84622 to 0.84958 and our submission score from 0.79425 to 0.79904. Thus the benefit appears to be generalizeable. Let's save this to our best strategy so far","217ad014":"# Experimentation\n\nFinally, let's see how the model behaves depending on how we perform preprocessing. We will perform a few experiments focusing on a few specific features. We do not do a grid search over all options since the amount of computational time needed is prohibitive.\n\n## Binning Title and Deck\n\nFirst let's experiment with binning the `'Title'` and `'Deck'` features. As discused in our exploratory data analysis, we are left with a large number of classes for these two features, some with very small sample size. Hence there is a tradeoff when binning is performed: it makes the results more generalizeable and less prone to over-fitting, but at the same time we are losing what may be important information. We will try all possible options","e08a5793":"## Fare\n`'Fare'` is the amount that the ticket that person is on, cost. It is basically equivalent to `'Pclass'` since different classes were put in different price brackets, however, not all tickets of the same class cost the same amount. Hence, `'Fare'` goes a bit further than `'Pclass'` in signifying status and may be useful. This is particularly true of 1st class tickets, which as we shall see, have a far greater range in price than 2nd or 3rd, helping us to distinguish the true elites amongst the elite.\n\nTo analyze the `'Fare'`, we also use the `'PartySize'` feature, which is interesting in it's own right, that keeps track of the number of passengers sharing the same ticket number. This allows us to determine the cost per individual, which we call `'FareByHead'`, which should be more indicitive of status than the overall cost. (Any information lost is still contained in the `'PartySize'` feature.)\n\nUsing the `'FareByHead'` feature, we then impute the value for the single missing data point in the `'Fare'` column. This individual, passenger 1044, is traveling alone and so we impute it with the average fare by head for a third class passenger.","c3c2f270":"# Feature Engineering\n\nWe have given a broad overview of the data and taken care of most of the null values. In this section we will provide strategies for\n* imputing the null values in `'Age'`\n* dealing with the proliferation of titles and decks\n* processing the information on what family or party and individual has travelled with\n* standardizing, normalizing, or binning numerical data\nWe will encode each of these strategies in a callable function, which we'll run on the relevant feature before we train the model. This will allow us to test a large number of strategies efficiently and evaluate their performance.\n","73f50039":"## Embarked\n\n`'Embarked'` signifies a passenger's point of departure, with C = Cherbourg, Q = Queenstown, S = Southampton. There are two null values, but a simple Google search yields that both passengers departed at Southampton, and so we impute the missing values with `'S'`. We see that Cherbourg has a significantly higher survival rate than the other two. Since a passenger's embarking location clearly has no direct effect on survival, this is likely due to correlation with other features that are more directly tied to it, such as class or deck. The below graph shows indeed shows this relationship with class","095198d5":"## Cabin","98651165":"## Encoding Title\n\nAs we saw before, the title feature is quite predictive of survival, but there is a proliferation of titles, many of which are similar, and have small sample size. It may be helpful to bin these into categories that are more generalizable. We define a function that will do this, and which allows us to select a few different binning strategies, including doing nothing. In all cases however, we will at least group `'Miss'`, `'Ms'`, and `'Mlle'` together, since thse mean the same thing. We will also group `'Mme'` in with this, since madam is a catch all for Ms. or Mrs., and the single passenger with title listed as `'Mme'` appears to be young and unmarried.\n\nThe function `encode_title` requires an argument `binning_strat` that tells us how we want to bin these titles otherwise. The options are `'none'`, `'coarse'`, which puts Dr\/Military\/Noble\/Clergy together, and `'fine'`, which keeps these categories separate. There is also an `encoding_strat` argument which can take the values `'drop'`, `'label'`, `'freq'`, and `'one_hot'`. Here `'drop'` signifies the feature should be dropped entirely while as the remaining options signify the obvious type of numerical encoding.","fa648a7c":"Now we build a voting classifier","faf03608":"Let's take a look at how survival rate varies by title.","3130b8a6":"As such, it is not suprising that survival rate is itself very highly correlated with `'Embarked'`. It is is by far the highest for departure from Cherbourg where about half the passengers were first class. However, this is not the entire story, as survival rate is slightly higher for Queenstown (94% third class) than for Southampton (55% third class). This may in part be due to the fact that, for whatever reason, a significantly higher percentage of those that embarked at Southampton were male","df54fd7d":"## Imputing Age\n\nBesides deck, the age feature has the largest number of null values, standing at 21% of the total dataset. We will have to impute these values according to some strategy. Below we define a function that will impute the age of a passenger with either the mean or median age of passengers of a certain 'type'. The `'impute_strat'` argument tells the function whether to use the mean or median, while the `'groupby_features'` argument tells it how it should organize passengers in to 'types'. This is important since, as we have seen, the age distribution depends very sensitively on class, sex, family size, and more. Moreover, due to the 'women and children first' mentality, having a more sophisticated means of correctly identifying the age can go a long way toward improving the acuracy of the model.","04b75b8d":"The available decks with percent occupancy (in the training set) are","5d7c7a83":"Indeed, from the below graph, it looks like `'one_hot'` is best for the `'Embarked'` feature. `'Pclass'` is relatively insensitive to the strategy but one_hot encoding is consistently the worst, likely because the ordinal information is actually meaningful for `'Pclass'`.","4ab7293b":"## Age\n","5fd63850":"# Training the Model\n\nFinally, let's take the feature engineering strategy we have decided upon after experimentation, and try to optimize the model parameters to squeze one last bit of performance out of it. To this point, a random forest with hyperparameters\n\n`RandomForestClassifier(n_estimators=40,\n              criterion='entropy',\n              max_features='auto',\n              min_samples_leaf=4,\n              min_samples_split=5,\n              max_depth=5,\n              random_state=random_state,\n              bootstrap=True,\n              verbose = False,\n              n_jobs=-1\n            )`\n            \nhas achieved a 5-fold cross-validated training score of 0.84958 and a competition score of 0.79904. Can we do any better? To date, we have achieved maximum training score of 0.85858 with a tuned random forest with hyper-parameters\n\n`{'bootstrap': True, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 25}`\n\nhowever, this did not improve the competition score, which was also 0.79904 for this model. No other model had a higher competition score.","a192e0c0":"# Make a submission","50f24379":"This may not be the full story though: if we go class by class, than the correlation of `'Survived'` with total fare is actually better than with fare by head. This may simply be because larger groups tend to have higher survival rates, in which case this is already accounted for in the `'PartySize'` feature. In either case, we'll have to see when it comes time to training the model which is most useful.","ed45956e":"Now lets deal with the `'Cabin'` feature. This is 77% null values, however for what data there is, there seems to be significant predictive value. First off, the specific cabin number is of course far too much information and mostly meaningless. We'll collect only the information on the deck and encode any missing values as `'M'`.","0817a81e":"The fare by head is, as expected, strongly dependent on class, and has better correlation with survival than the total fare, though not by much.","00a93d4a":"We will use `'FamilyName'` later on to create some more intersting features. For now, let's focus on `'Title'`. We see there is a proliferation of titles, but most passengers are a `'Mr'`, `'Miss'`, `'Mrs'`, or `'Master'`. The remaining titles have very small sample sizes and fitting to these may not be statistically meaningful. Moreover, there are some titles that clearly mean the same thing, such a `'Miss'`, `'Ms'`, and `'Mlle'` (which is French for `'Miss'`). When it comes to feature engineering, we will try to group some of these together and see if it improves model performance.","21e1ea85":"## Experimenting with how we encode categorical variables\n\nWe can also vary the encoding method for categorical variables: `'none'`, `'label'`, `'freq'`, `'one_hot'`. I expect `'one_hot'` to generically work the best, but let's investigate to see. The categorical features we consider at `'Title'`, `'Deck'`, `'Embarked'`, and `'Pclass'`. Let's first take a look at `'Embarked'`, and `'Pclass'`. Since embarked is not an ordinal category, I expect this to best be encoded with `'one_hot'` and since `'Pclass'` is already numerically and ordinarily encoded, I expect strategy shouldn't matter for this feature.","b001fa50":"There's actually three strategies that give the same best score:","770c476e":"As discussed in the feature engineering section, there are various strategies we could use to encode the categorical data and process the numerical data. Some features may be useful to include, and other not. The focus of this notebook is to experiment with various methods of feature engineering and to this end we have created some predefined functions for carrying out this pre-processing according to whatever strategy we choose. Let's collect this all into a 'pipeline' function which we may easily call. Below we define this function with a reasonable default strategy.","0d7654d6":"Doing the same with `'family_survival_rate'` and `'party_survival_rate'`, it seems we want to keep them both for the best model, which is unsuprising.","c3b82e76":"# Data Exploration\n\nLet's bein by taking a look at our dataset and planning what we would like to do with it. Let's load the data and take a look at the features we have available to us.","81273883":"We display the model's performance as it depends on binning strategy in a heatmap below. It seems that binning generally has a small positive effect, however our `'coarse'` strategy for binning decks goes somewhat too far (my opinion too when I wrote it). In the future we will work with binning strategy:\n\n(title_strat, deck_strat) = ('coarse', 'fine')","4b8cd99f":"Finally, let's look at `'Title'` and `'Deck'`. The encoding strategies we may use for either are `'label'`, `'freq'`, `'one_hot'`.","8cd2cae3":"## Pclass\n\nPclass is the class of ticket that the passenger bought. The majority of passengers are third class and survival rate is very strongly dependent on which class you are in. Correlation with survival is amongst the highest among all features. This one is a must keep. There are no null values in the training or test sets.","5496745c":"## Parch\n\n`'Parch'` is the number of parents and children a passenger has on board. Broadly it looks similar to the `'SibSp'` data. 76% of passengers in the training set have `'Parch == 0'`, and among these, the survival rate is 35%. There is a significantly higher survival rate for `'Parch' == 1,2,3'` , which then falls off a cliff for 4 or greater.","ed8557f5":"Unfortunately for such an important variable, about 20% of the values in the training set are missing. We will have to impute these, and there a number of strategies we could choose. Simple imputing with the mean or meadian of the entire data set could lead to some large error given the sensitive dependence of the survival rate on age, so if we could do better we would like to use some of the other information in the data to make a more informed guess for the age. When we come to feature engineering, we will investigate the effect this has in more detail. For now we simply note that the average age depends greatly on the demographics of the passenger we are looking at","256b907d":"Age is probably the most relevant feature after class and sex. Although the overall correlation with survival is small, we can see in the plot below that survival rates vary quite drastically depending on age, with the young having the best chance of survival.","14b19397":"Let's run the preprocessing function with the default strategy just to see how it performs. After preprocessing, the data looks like this.","58e82189":"When the deck is available, it the survival rate has significant dependence on it, as seen in the graph below. However, the most notable feature of the data is the difference in survival rates between those passengers for which deck is available (67%), and those for which it is not (30%). It's not clear why this is the case, it may be simply (as suggested by G\u00fcne\u015f Evitan), that it was more difficult to retrieve ticket information for those who perished. In either case, it is worth keeping the `'Deck'` feature, and perhaps singling out whether or not it's value is `'M'`.","83282824":"## Encoding Deck\n\nThe same can be said of the deck feature: there are a large number of possible values, some with small cardinality, but moreover we also have the great majority of these data points taking on the value `'M'`, which signifies that the data was in fact missing from the original data. Below we define a function that will bin, encode, and decide how to handle the missing values. There are only two options for `binning_strat`, which includes `none` and course which groups the decks as {AT},{BC},{DE},{FG}. This splitting was chosen from inspecting the graphs for surival rate and picking where it appeared there was the biggest changes in behavior. The `encoding_strat` argument may take the values `'drop'`, `'label'`, `'freq'`, and `'one_hot'`. For label and frequency encoding we also create a feature `'DeckMissing'` to keep track of whether the data was in fact available, as this seems to have the largest correlation with survival rate.","ad5832f1":"We see the correlation with survival (about 2%) is less than for `'SibSp'` (4%) and `'Parch'` (8%), however a more detailed look at the survival rate vs. family size reveals it's rather featureful.","a4301a8e":"## Sex\nThe only feature more correlated with survival is sex. Below, `'male'` is encoded as 1 and `'female'` as 0. It turns out, only about a third of the passengers on the Titanic were female, but if your were female, you had about a 3\/4 chance of survival, far higher than that of men. A model could go a very long way including only these two features.","14d33776":"## Party Size\nWe can do the same with the party size, which we define to be the number of people on the same ticket. This is different from the family size since families may not all travel in the same group, or individuals may travel with non-family members. We see below that this feature has similar behavior with regards to survival rate as `'SibSp'`, `'Parch'`, and `'FamilySize'`.","0f91eec9":"## Looking at family and party survival rates\n\nOne of the most interesting bits of feature engineering done in the referenced notebook is target encoding the survival rate of families. That is, since we expect that families face the crisis together, information about which family an individual belongs to and the survival of other members of that family can go a long way toward modeling the result. The same can be said of members traveling in the same cabin, which we refer to as a party.\n\nNow we create a feature `'family_survival_rate'` that gives the average survival rate of that family. We have to be somewhat careful here how we do this since there are certain cases in which we don't want to use this information as it is entirely meaningless when an individual is travelling alone or when an individual is a member of a family appearing only in the test set and not the train set. Hence, we create a special feature `'family_survival_rate_NA'` that keeps track of this. When `'family_survival_rate_NA == 1'`, we simply fill in `'family_survival_rate'` with junk in the hopes that the model will learn to ignore the survival rate in this case. There are several strategies to do this, we have chosen to fill them with a random number between 0 and 1, trying to achieve as little correlation as possible with the target variable. If we do not teach the model to ignore `'family_survival_rate'` when `'FamilySize'` is 1 the model will drastically overfit since this is the majority of data points, and is then a feature that simply indicates survival or not.\n\nWe will do the same for parties that are traveling together, creating feature `'party_survival_rate'` and `'party_survival_rate_NA'`.\n\nNow, we have grouped together families by family names. One obvious difficulty here is that having the same family name doesn't necessarily imply they are in the same family. It's possible that with extra work we could account for this, however, we will leave this for future work.\n\nSince the survival rate is less meaningful for small families (small sample size), it may be worth experimenting with blending the raw survival rate with the average survival rate for small families.\n\n","1c1a8db5":"The following functions are general purpose functions for processing the remaining numerical and categorical features. For instance, `num_process` allows us to do nothing, drop the feature, take its log, or do binning. Similarly `cat_process` allows us to do nothing, drop the feature, label encode, or one-hot encode.","aefde6fd":"We then create and train a random forest classifier. 0.8417299604544599","be3bec21":"## Family Size\n\nA simple feature we can deduce from `'SibSp'` and `'Parch'` is the size of a family that's traveling together. Note that this is somewhat different from the `'PartySize'` feature about to be defined size a party may include servants, aquaintances, etc. Let's create this feature and take a look at the same information we did for `'SibSp'` and `'Parch'`.","8c33fcc6":"# Experimenting with feature engineering in the Titanic dataset\n\nFor my first competition on Kaggle I decided I'd focus on the role of feature engineering in developing a robust model. One of my main inspirations in this is the wonderful notebook on [advanced feature engineering](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial) by Gunes Evitan. The main goal here will be creating a notebook where various feature engineering choices may be easily altered so that their effect on the final performance of the model can be evaluated. Hopefully, this will be useful to myself and others at the beginning of their data science journeys to learn some of the art behind data science and get a feel for what works and what doesn't.\n\nAfter creating this notebook, the main lessons I've taken are\n* feature engineering has a larger effect on results than model selection\n* engineering new features that look at the data in a different way is the biggest way to improve performance (in the noebook below these would be the family and party survival rate features)\n* ordinal categorical features tend to be better encoded with label or frequency encoding while non-ordinal ones work better with one-hot encoding\n* binning a categorical variable can have a mild positive effect, especially when cardinality is high and some categories have small value counts, but this needs to be done intelligently with awareness of trends in the data, to make sure one is not washing them out\n\nTo date we have mainly focussed on engineering the new features `'family_survival_rate'` and `'party_survival rate'`, as done in Evitan's notebook, as well as processing categorical features. In the future it would be interesting to investigate processing the numerical features further. To date we have been able to achieve a top 5% score at 0.799.","741cc1c6":"Below we can see how `'FareByHead'` is distributed for each class.","bae0c7ba":"## Name\n\nThe full name of a passenger is obviously too much information, so let's just pull the surname and title and drop the rest","eb54dfda":"We'll go with `{'groupby_features': ['Pclass', 'Sex'], 'impute_strat': 'mean'}`.","6d6195e5":"## SibSp\n\n`'SibSp'` is the number of siblings and spouses that the passenger is travelling with. About two thirds of the passengers have `'SibSp == 0'` with a survival rate among these of about 0.35. Beyond this, by itself, it does not have a particularly high linear correlation with survival. However, upon closer investigation, we see there is a stark difference in survival rates at different values of `'SibSp'`. Above 2, the survival rate falls precipitously, and above 4, there are no survivors in the training data. The number of passengers with `'SibSp'` above 4 is a small percentage of the total number of passengers, but still sufficiently large in absolute number, 12, for this to be statistically meaningful.","87f54815":"As expected, `'one_hot'` is best for both. I doubt there will be much improvement from further tweaking the feature engineering along these lines. By far the greatest improvement seems to come from including the novel features `'family_survival_rate'` and `'party_survival_rate'`, which increased performance on the training data from 0.8283 to 0.8496. In the future, it seems like I should follow my gut on encoding, using one-hot for non-ordinal categories, label encoding for ordinal ones, genreally keeping all mildly relevant features, and putting the bulk of my time into creating novel features that bring something new to the data.","7d2b455b":"## Experimenting with dropping features\n\nI don't expect dropping features will help much, the model should be able to pick out which features are important and which aren't, but let's at least confirm this expectation bears out. Below we train a bunch of models where we have made different choices as to which numerical features well will keep and which we will drop.","cb786cbd":"There are a fair number of missing values for Age and Cabin, which we'll have to decide how to deal with. For now, let's take care of Embarked and Fare since theres only a few. These data points are","2f0d5eeb":"## Imputing Age\n\nThere are a few things we can experiment with for imputing age: we could use either the mean or the median, and we can control which features we group by to calculate this. We will start with `'best_strat'` and then try the following options","3811604a":"# Training our model\n\nLet's take a final look at our data before we process it and train the model."}}