{"cell_type":{"186fbb54":"code","61df82da":"code","77484ed0":"code","e61defe6":"code","d090fd91":"code","eebdc50b":"code","c4e90026":"code","0900d2fe":"code","66fed42c":"code","50dbd4af":"code","78ecf79a":"code","8b8f6c74":"code","7626a1cf":"code","27a26592":"code","8c8ce869":"code","790cb3f1":"markdown","4add490c":"markdown","58d9a650":"markdown","d833babd":"markdown","9c1c73cc":"markdown","58a0f2d6":"markdown","49b7a583":"markdown","99a21910":"markdown","55d912f8":"markdown","029dd704":"markdown","ac298800":"markdown","1be46039":"markdown","3d3d9db6":"markdown"},"source":{"186fbb54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv),\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","61df82da":"data=pd.read_csv(\"\/kaggle\/input\/lower-back-pain-symptoms-datasetlabelled\/Dataset_spine.csv\")","77484ed0":"data.head()","e61defe6":"data.info()","d090fd91":"data.Class_att=[1 if each ==\"Abnormal\" else 0 for each in data.Class_att]","eebdc50b":"y=data.Class_att.values\nx_data=data.drop([\"Class_att\"],axis=1)\n\nx=(x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))","c4e90026":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)","0900d2fe":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\n\nprint(\"Logistic Regression score:\",lr.score(x_test,y_test))","66fed42c":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)\n\nprint(\"{} nn score: {}\".format(3,knn.score(x_test,y_test)))\n\nscore_list=[]\n\nfor each in range(1,15):\n    knn=KNeighborsClassifier(n_neighbors=each)\n    knn.fit(x_train,y_train)\n    score_list.append(knn.score(x_test,y_test))\n\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k\")\nplt.ylabel(\"Scores\")\nplt.show()\n\nknn=KNeighborsClassifier(n_neighbors=7)\nknn.fit(x_train,y_train)\n\nprint(\"according to the graph the max score value is at k = 7 and score is :\",knn.score(x_test,y_test))","50dbd4af":"from sklearn.svm import SVC\n\nsvm=SVC(random_state=42)\nsvm.fit(x_train,y_train)\n\nprint(\"svm score:\",svm.score(x_test,y_test))","78ecf79a":"from sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\nnb.fit(x_train,y_train)\n\nprint(\"nb score:\",nb.score(x_test,y_test))","8b8f6c74":"from sklearn.tree import DecisionTreeClassifier\n\ndf=DecisionTreeClassifier(random_state=42)\ndf.fit(x_train,y_train)\n\nprint(\"Desicion Tree Classification score:\",df.score(x_test,y_test))","7626a1cf":"from sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(x_train,y_train)\n\nprint(\"Random Forest Classification score:\",rf.score(x_test,y_test))","27a26592":"y_pred=rf.predict(x_test)\ny_true=y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","8c8ce869":"class_method=[lr,knn,svm,nb,df,rf]\nmethod_acc=[]\nmethod_name=[\"lr\",\"knn\",\"svm\",\"nb\",\"df\",\"rf\"]\n\nfor each in class_method:\n    method_acc.append(each.score(x_test,y_test)*100)\n    \nplt.plot(method_name,method_acc)\nplt.xlabel(\"Methods of Classifiaction\")\nplt.ylabel(\"Accuracies of Methods (%)\")\nplt.show()","790cb3f1":"<a id=\"9\"><\/a><br>\n## Random Forest Classification","4add490c":"<a id=\"8\"><\/a><br>\n## Decision Tree Classification","58d9a650":"<a id=\"10\"><\/a><br>\n## Confusion Matrix","d833babd":"<a id=\"3\"><\/a><br>\n# Train Test Split","9c1c73cc":"<a id=\"7\"><\/a><br>\n## Naive Bayes","58a0f2d6":"<a id=\"11\"><\/a><br>\n# Conclusion","49b7a583":"<a id=\"5\"><\/a><br>\n## KNN","99a21910":"<a id=\"6\"><\/a><br>\n## SVM","55d912f8":"# Introduction\nIn this code I tried to show you classification types with using sklearn.\n\n1. [Entreing and Normalizing Data](#1)\n1. [Train Test Split](#3)\n1. [Classification Types](#2)\n    * [Logistic Regression Classification](#4)\n    * [KNN](#5)\n    * [SVM](#6)\n    * [Naive Bayes](#7)\n    * [Decision Tree Classification](#8)\n    * [Random Forest Classification](#9)\n    * [Confusion Matrix](#10)\n1. [Conclusion](#11)","029dd704":"In conclusion, we can say that random forest classification is has the greatest accuracy ans svm has the least accuracy. \nIf you want to get more information about classification you can visit: https:\/\/www.kaggle.com\/kanncaa1\/machine-learning-tutorial-for-beginners","ac298800":"<a id=\"1\"><\/a><br>\n# Entering and Normalizing Data","1be46039":"<a id=\"4\"><\/a><br>\n## Logistic Regression Classification","3d3d9db6":"<a id=\"2\"><\/a><br>\n# Classification Types"}}