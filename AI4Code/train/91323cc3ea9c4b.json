{"cell_type":{"10f8851a":"code","b0936640":"code","03ce8cc0":"code","b483963d":"code","b49d3350":"code","a110df1b":"code","28aff3db":"code","919e342b":"code","e354d044":"code","dc31d613":"code","0066ebce":"code","a91b649f":"code","f53db512":"code","cca3ed02":"code","33936516":"code","54f71acc":"code","ade59478":"code","42c2f5d1":"code","3194389f":"code","ddb5459b":"code","8b01667f":"code","6d70e7d0":"code","70e210e5":"code","90594ab6":"code","c97069a9":"code","573e1912":"code","cfead196":"code","159f3f62":"code","47b137b4":"code","cc1338bd":"code","cc723698":"code","597612f7":"code","d3d6893a":"code","bf612ee9":"code","101fa13e":"code","c4313beb":"code","5d9f4501":"code","b07540d2":"code","05b8e261":"code","fe2edcdd":"code","1870f41c":"code","ca83cd66":"code","5768b16b":"code","37ff572d":"code","1a69b674":"code","6c39b76f":"code","f6b872e4":"code","266bad5c":"code","e03df4fe":"code","d820eefd":"code","cdf035be":"code","9da18435":"code","8368f734":"code","93ebebad":"code","93b5cce6":"code","7e976ae6":"code","b80b7945":"code","94360d04":"code","4348038e":"code","286d655b":"code","e59225d3":"markdown","36b21c59":"markdown","1f2d66b5":"markdown","c2898cb1":"markdown","24854932":"markdown","96f949b8":"markdown","2dea8ccc":"markdown","add93682":"markdown","2fa0fe0d":"markdown","baead2e3":"markdown","d31aef9f":"markdown","4984fb7a":"markdown","a5f1bbf8":"markdown","0629d5e1":"markdown","dd593ff5":"markdown","183a2ef5":"markdown","b83edc1d":"markdown","4fd81fa6":"markdown","23cb963e":"markdown","9850ae38":"markdown"},"source":{"10f8851a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nimport time\nfrom sklearn import preprocessing\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\nimport gc\nfrom tqdm import tqdm\n\nfrom scipy.stats import describe\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n# Any results you write to the current directory are saved as output.","b0936640":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","03ce8cc0":"import os\nprint(os.listdir(\"..\/input\"))\n","b483963d":"#Loading Train and Test Data\ntrain = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ntest = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"{} observations and {} features in train set.\".format(train.shape[0],train.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(test.shape[0],test.shape[1]))","b49d3350":"train.head()","a110df1b":"test.head()","28aff3db":"train.target.describe()","919e342b":"plt.figure(figsize=(12, 5))\nplt.hist(train.target.values, bins=200)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()\n","e354d044":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=train.target.values)\nplt.show()","dc31d613":"plt.figure(figsize=(12, 5))\nplt.hist(train.feature_1.values, bins=200)\nplt.title('Histogram feature_1 counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","0066ebce":"plt.figure(figsize=(12, 5))\nplt.hist(train.feature_2.values, bins=200)\nplt.title('Histogram feature_2 counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","a91b649f":"plt.figure(figsize=(12, 5))\nplt.hist(train.feature_3.values, bins=200)\nplt.title('Histogram feature_3 counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","f53db512":"from IPython.display import YouTubeVideo\nYouTubeVideo('UJkxFhFRFDA')","cca3ed02":"train['first_active_month'] = pd.to_datetime(train['first_active_month'])\ntest['first_active_month'] = pd.to_datetime(test['first_active_month'])\ntrain['elapsed_time'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\ntest['elapsed_time'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days\n\ntrain['outliers'] = 0\ntrain.loc[train['target'] < -30, 'outliers'] = 1\n\n\n\ntarget = train['target']\ndel train['target']\n\ntrain.head()","33936516":"new_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv', parse_dates=['purchase_date'])\nhistorical_transactions = pd.read_csv('..\/input\/historical_transactions.csv', parse_dates=['purchase_date'])\n\ndef binarize(df):\n    for col in ['authorized_flag', 'category_1']:\n        df[col] = df[col].map({'Y':1, 'N':0})\n    return df\n\nhistorical_transactions = binarize(historical_transactions)\nnew_transactions = binarize(new_transactions)","54f71acc":"historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])\nnew_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])\n\nhistorical_transactions = reduce_mem_usage(historical_transactions)\nnew_transactions = reduce_mem_usage(new_transactions)\n\nagg_fun = {'authorized_flag': ['sum', 'mean']}\nauth_mean = historical_transactions.groupby(['card_id']).agg(agg_fun)\nauth_mean.columns = ['_'.join(col).strip() for col in auth_mean.columns.values]\nauth_mean.reset_index(inplace=True)\n\nauthorized_transactions = historical_transactions[historical_transactions['authorized_flag'] == 1]\nhistorical_transactions = historical_transactions[historical_transactions['authorized_flag'] == 0]","ade59478":"historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\nauthorized_transactions['purchase_month'] = authorized_transactions['purchase_date'].dt.month\nnew_transactions['purchase_month'] = new_transactions['purchase_date'].dt.month","42c2f5d1":"def aggregate_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'category_1': ['sum', 'mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'category_3_A': ['mean'],\n        'category_3_B': ['mean'],\n        'category_3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'city_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_month': ['mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'min', 'max'],\n        'month_lag': ['min', 'max']\n        }\n    \n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n","3194389f":"history = aggregate_transactions(historical_transactions)\nhistory.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\nhistory[:5]","ddb5459b":"authorized = aggregate_transactions(authorized_transactions)\nauthorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]\nauthorized[:5]","8b01667f":"new = aggregate_transactions(new_transactions)\nnew.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\nnew[:5]","6d70e7d0":"def aggregate_per_month(history):\n    grouped = history.groupby(['card_id', 'month_lag'])\n\n    agg_func = {\n            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n\n    intermediate_group = grouped.agg(agg_func)\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    intermediate_group.reset_index(inplace=True)\n\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True)\n    \n    return final_group\n#___________________________________________________________\nfinal_group =  aggregate_per_month(historical_transactions) \nfinal_group[:10]","70e210e5":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\n\ntrain = pd.merge(train, authorized, on='card_id', how='left')\ntest = pd.merge(test, authorized, on='card_id', how='left')\n\ntrain = pd.merge(train, new, on='card_id', how='left')\ntest = pd.merge(test, new, on='card_id', how='left')\n\ntrain = pd.merge(train, final_group, on='card_id', how='left')\ntest = pd.merge(test, final_group, on='card_id', how='left')\n\ntrain = pd.merge(train, auth_mean, on='card_id', how='left')\ntest = pd.merge(test, auth_mean, on='card_id', how='left')","90594ab6":"del history, new, final_group, auth_mean, authorized\ngc.collect()\ngc.collect()","c97069a9":"features = [c for c in train.columns if c not in ['card_id', 'first_active_month', 'outliers']]\ncategorical_feats = [c for c in features if 'feature_' in c]","573e1912":"print(train.shape)\nprint(test.shape)","cfead196":"train.to_csv('train_1.csv', index=False)\ntest.to_csv('test_1.csv', index=False)\ntarget.to_csv('target.csv', index=False)","159f3f62":"oof_ridge = np.zeros(train.shape[0])\npredictions_ridge = np.zeros(test.shape[0])\n\ntst_data = test.copy()\ntst_data.fillna((tst_data.mean()), inplace=True)\n\ntst_data = tst_data[features].values\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n    print(\"fold n\u00b0{}\".format(fold_+1))\n    trn_data, trn_y = train.iloc[trn_idx][features], target.iloc[trn_idx].values\n    val_data, val_y = train.iloc[val_idx][features], target.iloc[val_idx].values\n    \n    trn_data.fillna((trn_data.mean()), inplace=True)\n    val_data.fillna((val_data.mean()), inplace=True)\n    \n    trn_data = trn_data.values\n    val_data = val_data.values\n\n    clf = Ridge(alpha=100)\n    clf.fit(trn_data, trn_y)\n    \n    oof_ridge[val_idx] = clf.predict(val_data)\n    predictions_ridge += clf.predict(tst_data) \/ folds.n_splits\n\nnp.save('oof_ridge', oof_ridge)\nnp.save('predictions_ridge', predictions_ridge)\nnp.sqrt(mean_squared_error(target.values, oof_ridge))","47b137b4":"del tst_data\ngc.collect()","cc1338bd":"param = {'num_leaves': 120,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"min_child_samples\": 30,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):    \n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=200)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions_lgb += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n    \nnp.save('oof_lgb', oof_lgb)\nnp.save('predictions_lgb', predictions_lgb)\nnp.sqrt(mean_squared_error(target.values, oof_lgb))","cc723698":"'''xgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx][features], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx][features]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test[features]), ntree_limit=xgb_model.best_ntree_limit+50) \/ folds.n_splits\n    \nnp.save('oof_xgb', oof_xgb)\nnp.save('predictions_xgb', predictions_xgb)\nnp.sqrt(mean_squared_error(target.values, oof_xgb))'''","597612f7":"del train, test\ngc.collect()","d3d6893a":"'''new_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv', parse_dates=['purchase_date'])\nhistorical_transactions = pd.read_csv('..\/input\/historical_transactions.csv', parse_dates=['purchase_date'])\n\ndef binarize(df):\n    for col in ['authorized_flag', 'category_1']:\n        df[col] = df[col].map({'Y':1, 'N':0})\n    return df\n\nhistorical_transactions = binarize(historical_transactions)\nnew_transactions = binarize(new_transactions)'''","bf612ee9":"'''def read_data(input_file):\n    df = pd.read_csv(input_file)\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n    return df\n#_________________________________________\ntrain = read_data('..\/input\/train.csv')\ntest = read_data('..\/input\/test.csv')\n\ntrain['outliers'] = 0\ntrain.loc[train['target'] < -30, 'outliers'] = 1\n\ntarget = train['target']\ndel train['target']'''","101fa13e":"'''historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])\nnew_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])\n\nhistorical_transactions = reduce_mem_usage(historical_transactions)\nnew_transactions = reduce_mem_usage(new_transactions)'''","c4313beb":"'''historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\nnew_transactions['purchase_month'] = new_transactions['purchase_date'].dt.month\n\ndef aggregate_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['sum', 'mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'category_3_A': ['mean'],\n        'category_3_B': ['mean'],\n        'category_3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'city_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_month': ['mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    \n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n'''","5d9f4501":"'''history = aggregate_transactions(historical_transactions)\nhistory.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\nhistory[:5]'''","b07540d2":"'''new = aggregate_transactions(new_transactions)\nnew.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\nnew[:5]'''","05b8e261":"'''def aggregate_per_month(history):\n    grouped = history.groupby(['card_id', 'month_lag'])\n\n    agg_func = {\n            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n\n    intermediate_group = grouped.agg(agg_func)\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    intermediate_group.reset_index(inplace=True)\n\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True)\n    \n    return final_group\n#___________________________________________________________\nfinal_group =  aggregate_per_month(historical_transactions) \nfinal_group[:10]'''","fe2edcdd":"'''train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\n\ntrain = pd.merge(train, new, on='card_id', how='left')\ntest = pd.merge(test, new, on='card_id', how='left')\n\ntrain = pd.merge(train, final_group, on='card_id')\ntest = pd.merge(test, final_group, on='card_id')\n\nfeatures = [c for c in train.columns if c not in ['card_id', 'first_active_month', 'outliers']]\ncategorical_feats = [c for c in features if 'feature_' in c]'''","1870f41c":"'''del history, new, final_group\ngc.collect()\ngc.collect()'''","ca83cd66":"'''train[features].to_csv('train_2.csv', index=False)\ntest[features].to_csv('test_2.csv', index=False)'''","5768b16b":"'''param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\noof_lgb_2 = np.zeros(len(train))\npredictions_lgb_2 = np.zeros(len(test))\nstart = time.time()\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):    \n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n    oof_lgb_2[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    predictions_lgb_2 += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nnp.save('oof_lgb_2', oof_lgb_2)\nnp.save('predictions_lgb_2', predictions_lgb_2)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb_2, target)**0.5))'''","37ff572d":"'''xgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\noof_xgb_2 = np.zeros(len(train))\npredictions_xgb_2 = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):    \n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx][features], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb_2[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx][features]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb_2 += xgb_model.predict(xgb.DMatrix(test[features]), ntree_limit=xgb_model.best_ntree_limit+50) \/ folds.n_splits\n    \nnp.save('oof_xgb_2', oof_xgb_2)\nnp.save('predictions_xgb_2', predictions_xgb_2)\nnp.sqrt(mean_squared_error(target.values, oof_xgb_2))'''","1a69b674":"'''del train, test\ngc.collect()\ngc.collect()'''","6c39b76f":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nhist_trans = pd.read_csv('..\/input\/historical_transactions.csv')\nnew_merchant_trans = pd.read_csv('..\/input\/new_merchant_transactions.csv')","f6b872e4":"for df in [hist_trans,new_merchant_trans]:\n    df['category_2'].fillna(1.0,inplace=True)\n    df['category_3'].fillna('A',inplace=True)\n    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)","266bad5c":"def get_new_columns(name,aggs):\n    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]","e03df4fe":"for df in [hist_trans,new_merchant_trans]:\n    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n    df['year'] = df['purchase_date'].dt.year\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n    df['month'] = df['purchase_date'].dt.month\n    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n    df['hour'] = df['purchase_date'].dt.hour\n    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n    #https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation\/discussion\/73244\n    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)\/\/30\n    df['month_diff'] += df['month_lag']","d820eefd":"aggs = {}\nfor col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n    aggs[col] = ['nunique']\n\naggs['purchase_amount'] = ['sum','max','min','mean','var']\naggs['installments'] = ['sum','max','min','mean','var']\naggs['purchase_date'] = ['max','min']\naggs['month_lag'] = ['max','min','mean','var']\naggs['month_diff'] = ['mean']\naggs['authorized_flag'] = ['sum', 'mean']\naggs['weekend'] = ['sum', 'mean']\naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\nfor col in ['category_2','category_3']:\n    hist_trans[col+'_mean'] = hist_trans.groupby([col])['purchase_amount'].transform('mean')\n    aggs[col+'_mean'] = ['mean']    \n\nnew_columns = get_new_columns('hist',aggs)\nhist_trans_group = hist_trans.groupby('card_id').agg(aggs)\nhist_trans_group.columns = new_columns\nhist_trans_group.reset_index(drop=False,inplace=True)\nhist_trans_group['hist_purchase_date_diff'] = (hist_trans_group['hist_purchase_date_max'] - hist_trans_group['hist_purchase_date_min']).dt.days\nhist_trans_group['hist_purchase_date_average'] = hist_trans_group['hist_purchase_date_diff']\/hist_trans_group['hist_card_id_size']\nhist_trans_group['hist_purchase_date_uptonow'] = (datetime.datetime.today() - hist_trans_group['hist_purchase_date_max']).dt.days\ntrain = train.merge(hist_trans_group,on='card_id',how='left')\ntest = test.merge(hist_trans_group,on='card_id',how='left')\ndel hist_trans_group;gc.collect();gc.collect()","cdf035be":"aggs = {}\nfor col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n    aggs[col] = ['nunique']\naggs['purchase_amount'] = ['sum','max','min','mean','var']\naggs['installments'] = ['sum','max','min','mean','var']\naggs['purchase_date'] = ['max','min']\naggs['month_lag'] = ['max','min','mean','var']\naggs['month_diff'] = ['mean']\naggs['weekend'] = ['sum', 'mean']\naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\nfor col in ['category_2','category_3']:\n    new_merchant_trans[col+'_mean'] = new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n    aggs[col+'_mean'] = ['mean']\n    \nnew_columns = get_new_columns('new_hist',aggs)\nhist_trans_group = new_merchant_trans.groupby('card_id').agg(aggs)\nhist_trans_group.columns = new_columns\nhist_trans_group.reset_index(drop=False,inplace=True)\nhist_trans_group['new_hist_purchase_date_diff'] = (hist_trans_group['new_hist_purchase_date_max'] - hist_trans_group['new_hist_purchase_date_min']).dt.days\nhist_trans_group['new_hist_purchase_date_average'] = hist_trans_group['new_hist_purchase_date_diff']\/hist_trans_group['new_hist_card_id_size']\nhist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - hist_trans_group['new_hist_purchase_date_max']).dt.days\ntrain = train.merge(hist_trans_group,on='card_id',how='left')\ntest = test.merge(hist_trans_group,on='card_id',how='left')\ndel hist_trans_group;gc.collect();gc.collect()","9da18435":"del hist_trans;gc.collect()\ndel new_merchant_trans;gc.collect()\ntrain.head(5)","8368f734":"train['outliers'] = 0\ntrain.loc[train['target'] < -30, 'outliers'] = 1\ntrain['outliers'].value_counts()","93ebebad":"for df in [train,test]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n                     'new_hist_purchase_date_min']:\n        df[f] = df[f].astype(np.int64) * 1e-9\n    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n\nfor f in ['feature_1','feature_2','feature_3']:\n    order_label = train.groupby([f])['outliers'].mean()\n    train[f] = train[f].map(order_label)\n    test[f] = test[f].map(order_label)","93b5cce6":"train_columns = [c for c in train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\ntarget = train['target']\ndel train['target']","7e976ae6":"param = {'num_leaves': 31,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 4590}\n\noof_lgb_3 = np.zeros(len(train))\npredictions_lgb_3 = np.zeros(len(test))\nstart = time.time()\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):    \n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][train_columns], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n    oof_lgb_3[val_idx] = clf.predict(train.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)\n    \n    predictions_lgb_3 += clf.predict(test[train_columns], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nnp.save('oof_lgb_3', oof_lgb_3)\nnp.save('predictions_lgb_3', predictions_lgb_3)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb_3, target)**0.5))","b80b7945":"xgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\noof_xgb_3 = np.zeros(len(train))\npredictions_xgb_3 = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):    \n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx][train_columns], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb_3[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx][train_columns]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb_3 += xgb_model.predict(xgb.DMatrix(test[train_columns]), ntree_limit=xgb_model.best_ntree_limit+50) \/ folds.n_splits\n    \nnp.save('oof_xgb_3', oof_xgb_3)\nnp.save('predictions_xgb_3', predictions_xgb_3)\nnp.sqrt(mean_squared_error(target.values, oof_xgb_3))","94360d04":"train_stack = np.vstack([oof_lgb, oof_lgb_3, oof_xgb_3]).transpose()\ntest_stack = np.vstack([predictions_lgb, predictions_lgb_3, predictions_xgb_3]).transpose()\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(train_stack.shape[0])\npredictions = np.zeros(test_stack.shape[0])\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, train['outliers'].values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n\n    clf = Ridge(alpha=1)\n    clf.fit(trn_data, trn_y)\n    \n    oof[val_idx] = clf.predict(val_data)\n    predictions += clf.predict(test_stack) \/ folds.n_splits\n\n\nnp.sqrt(mean_squared_error(target.values, oof))","4348038e":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission['target'] = predictions\nsample_submission.to_csv('stacker_4.csv', index=False)","286d655b":"target['target'] = target\n\ntrain[train_columns+['target']].to_csv('train_3.csv', index=False)\ntest[train_columns].to_csv('test_3.csv', index=False)","e59225d3":"Seems like a pretty nice normal-looking distribution, except for the few anomalous elements at teh far left. They will have to be dealt with separately.\n\nLet's look at the \"violin\" version of the same plot. ","36b21c59":"Let's now look at the distributions of various \"features\"","1f2d66b5":"Seems like a very wide range of values, relatively spaking. Let's take a look at the graph of the distribution:","c2898cb1":"Seems farily straightforward - just ID, first active months, three anonimous features, and target firld for train set.\n\nLet's take a look at the target variable:\n","24854932":"Here is a gratuitous embedding of YouTube video of 'The Girl From Ipanema'. For no good reason.","96f949b8":"A great thing about stackign is that you can not only use different set of models, but also create the same models with a different set of features. Here we'll use features from the wonderful [Elo world kernel](https:\/\/www.kaggle.com\/fabiendaniel\/elo-world) by Fabien Daniel:","2dea8ccc":"Now some XGBoost:","add93682":"Let's see what files we have in the input directory:","2fa0fe0d":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still **very** raw. I will work on it as my very limited time permits, and hope to expend it in the upcoming days and weeks.\n\nNB: Most of the feature engineering and some of the modeling is based on [Peter Hurford's excellent kernel](https:\/\/www.kaggle.com\/peterhurford\/you-re-going-to-want-more-categories-lb-3-737\/notebook) , and [another one by Konrad Banachewicz.](https:\/\/www.kaggle.com\/konradb\/lgb-fe-lb-3-707)\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA.","baead2e3":"To be continued ...","d31aef9f":"# Feature Engineering","4984fb7a":"Now we add another set of features from [this kernel](https:\/\/www.kaggle.com\/chauhuynh\/my-first-kernel-3-699).","a5f1bbf8":"# Second Set of Features and Models","0629d5e1":"This version uses outlier labeling and stratification from [this kernel](https:\/\/www.kaggle.com\/chauhuynh\/my-first-kernel-3-699).","dd593ff5":"Finally, we'll stack them all together:","183a2ef5":"We see that in addition to the usual,`train`, `test` and `sample_submission` files, we also have `merchants`, `historical_transactions`, `new_merchant_transactions`, and even one (**HORROR!!!**) excel file - `Data_Dictionary`. The names of the files are pretty self-explanatory, but we'll take a look at them and explore them. First, let's look at the `train` and `test` files.","b83edc1d":"For now I am not including plots for the test set, as they at first approsimation look very similar.\n\nA couple of things that stand out are:\n\n1. There are only a handful of values for each of the three features.\n2. They are discrete\n3. They are relatively eavenly distributed.\n\nAll of this suggests that these features are categorical and have been label-encoded. ","4fd81fa6":"# Modeling\n\nNow let's do some of what everyone is here for - modeling. We'll start with a simple Ridge regression model. ","23cb963e":"Yup, there is that little bump on the far left again.","9850ae38":"3.78 CV is not bad, but it's far from what the best models can do in this competition. Let's take a look at a few non-linear models. We'll start with LightGBM."}}