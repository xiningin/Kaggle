{"cell_type":{"f9604b05":"code","3665ceed":"code","78700ce0":"code","756d9ca1":"code","da4c8d01":"code","e4efc697":"code","af5b7110":"code","c0f7f812":"code","3fda1825":"code","c4beb55f":"code","6a10adc1":"code","481113f2":"code","2ab2d3a9":"code","ff6a3a9b":"code","d9758b76":"code","3c668339":"code","67319893":"code","955aa128":"code","af6f04b0":"code","93222bd6":"code","200614df":"code","2a77c1b2":"code","0048edb3":"code","8b59fc81":"markdown","8eee9e21":"markdown","70d8fdd8":"markdown","738a3efa":"markdown","9a163c52":"markdown","e73d0733":"markdown","93d05417":"markdown","cd5c10a6":"markdown","364a8c6b":"markdown","4c0e38ba":"markdown","692e2777":"markdown","a70faf52":"markdown","a292bd2a":"markdown","2b5cff22":"markdown","090ea806":"markdown","20160cc0":"markdown","372c2f66":"markdown","caa06395":"markdown","ad0b0059":"markdown","544a9673":"markdown"},"source":{"f9604b05":"import os\nimport gc\nimport ast\nimport time\nimport wandb\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","3665ceed":"# This is used to download the model from the huggingface hub\nMODEL_NAME = 'roberta-large'\n\nMODEL_CHECKPOINT = \"..\/input\/feedback-prize-pytorch-robertalarge-itpt\/roberta_large-itpt-e0\"\n\n# Path where to download the model\nMODEL_PATH = 'model'\n\nRUN_NAME = f\"{MODEL_NAME}-itpt-e1\"\n\n# Max length for the tokenization and the model\n# For BERT-like models it's 512 in general\nMAX_LENGTH = 512\n\n# The overlapping tokens when chunking the texts\n# Possibly a power of 2 would have been better\n# Tried with 386 and didn't improve\nDOC_STRIDE = 128\n\n# Training configuration\n# 5 epochs with different learning rates (inherited from Chris')\n# Haven't tried variations yet\nconfig = {'train_batch_size': 4,\n          'valid_batch_size': 2,\n          'epochs': 5,\n          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n          'max_grad_norm': 10,\n          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n          'model_name': MODEL_NAME,\n          'max_length': MAX_LENGTH,\n          'doc_stride': DOC_STRIDE,\n          }","78700ce0":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    wandb.init(project=\"feedback-prize\", entity=\"dataista\", name=RUN_NAME, config=config)\nexcept:\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","756d9ca1":"df_all = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\nprint(df_all.shape)\ndisplay(df_all.head())","da4c8d01":"if os.path.isfile(\"..\/input\/feedback-prize-train-ner-csv\/train_NER.csv\"):\n    df_texts = pd.read_csv(\"..\/input\/feedback-prize-train-ner-csv\/train_NER.csv\",\n                           converters={'entities':ast.literal_eval, 'text_split': ast.literal_eval})\nelse:\n    # https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533\n    train_names, train_texts = [], []\n    for f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))):\n        train_names.append(f.replace('.txt', ''))\n        train_texts.append(open('..\/input\/feedback-prize-2021\/train\/' + f, 'r').read())\n\n        df_texts = pd.DataFrame({'id': train_names, 'text': train_texts})\n\n    df_texts['text_split'] = df_texts.text.str.split()\n    \n    \n    # https:\/\/www.kaggle.com\/cdeotte\/pytorch-bigbird-ner-cv-0-615\n    all_entities = []\n    for _, row in tqdm(df_texts.iterrows(), total=len(df_texts)):\n\n        total = len(row['text_split'])\n        entities = [\"O\"] * total\n\n        for _, row2 in df_all[df_all['id'] == row['id']].iterrows():\n            discourse = row2['discourse_type']\n            list_ix = [int(x) for x in row2['predictionstring'].split(' ')]\n            entities[list_ix[0]] = f\"B-{discourse}\"\n            for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n        all_entities.append(entities)\n\n    df_texts['entities'] = all_entities\n    df_texts.to_csv('train_NER.csv',index=False)\n\n    \nprint(df_texts.shape)\ndf_texts.head()","e4efc697":"# Check that we have created one entity\/label for each word correctly\n(df_texts['text_split'].str.len() == df_texts['entities'].str.len()).all()","af5b7110":"# Create global dictionaries to use during training and inference\n\n# https:\/\/www.kaggle.com\/cdeotte\/pytorch-bigbird-ner-cv-0-615\noutput_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nLABELS_TO_IDS = {v:k for k,v in enumerate(output_labels)}\nIDS_TO_LABELS = {k:v for k,v in enumerate(output_labels)}\n\nLABELS_TO_IDS","c0f7f812":"# CHOOSE VALIDATION INDEXES\nIDS = df_all.id.unique()\nprint(f'There are {len(IDS)} train texts. We will split 90% 10% for validation.')\n\n# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)\n\n# CREATE TRAIN SUBSET AND VALID SUBSET\ndf_train = df_texts.loc[df_texts['id'].isin(IDS[train_idx])].reset_index(drop=True)\ndf_val = df_texts.loc[df_texts['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n\nprint(f\"FULL Dataset : {df_texts.shape}\")\nprint(f\"TRAIN Dataset: {df_train.shape}\")\nprint(f\"TEST Dataset : {df_val.shape}\")","3fda1825":"def download_model():\n    # https:\/\/www.kaggle.com\/cdeotte\/pytorch-bigbird-ner-cv-0-615\n    os.mkdir(MODEL_PATH)\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n    tokenizer.save_pretrained(MODEL_PATH)\n\n    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n    config_model.num_labels = 15\n    config_model.save_pretrained(MODEL_PATH)\n\n    \n    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_CHECKPOINT, \n                                                               config=config_model)\n    backbone.save_pretrained(MODEL_PATH)\n    print(f\"Model downloaded to {MODEL_PATH}\/\")\n    \ndownload_model()","c4beb55f":"# Instantiate the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","6a10adc1":"# This function is a simple map between text_split and entities\n# We have verified that we have a 1:1 mapping above\n# See above: (df_texts['text_split'].str.len() == df_texts['entities'].str.len()).all() == True\ndef get_labels(word_ids, word_labels):\n    label_ids = []\n    for word_idx in word_ids:                            \n        if word_idx is None:\n            label_ids.append(-100)\n        else:\n            label_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n    return label_ids\n\n# Tokenize texts, possibly generating more than one tokenized sample for each text\ndef tokenize(df, to_tensor=True, with_labels=True):\n    \n    # This is what's different from a longformer\n    # Read the parameters with attention\n    encoded = tokenizer(df['text_split'].tolist(),\n                        is_split_into_words=True,\n                        return_overflowing_tokens=True,\n                        stride=DOC_STRIDE,\n                        max_length=MAX_LENGTH,\n                        padding=\"max_length\",\n                        truncation=True)\n\n    if with_labels:\n        encoded['labels'] = []\n\n    encoded['wids'] = []\n    n = len(encoded['overflow_to_sample_mapping'])\n    for i in range(n):\n\n        # Map back to original row\n        text_idx = encoded['overflow_to_sample_mapping'][i]\n        \n        # Get word indexes (this is a global index that takes into consideration the chunking :D )\n        word_ids = encoded.word_ids(i)\n        \n        if with_labels:\n            # Get word labels of the full un-chunked text\n            word_labels = df['entities'].iloc[text_idx]\n        \n            # Get the labels associated with the word indexes\n            label_ids = get_labels(word_ids, word_labels)\n            encoded['labels'].append(label_ids)\n        encoded['wids'].append([w if w is not None else -1 for w in word_ids])\n    \n    if to_tensor:\n        encoded = {key: torch.as_tensor(val) for key, val in encoded.items()}\n    return encoded","481113f2":"%%time\n# Tokenize both training and validation dataframes\ntokenized_train = tokenize(df_train)\ntokenized_val = tokenize(df_val)","2ab2d3a9":"# Original number of rows\nlen(df_train)","ff6a3a9b":"# Number of samples generated when chunking\nlen(tokenized_train['input_ids'])","d9758b76":"# Back-reference. \n# The first 2 ones mean that the second row was split into 2 samples\n# And the 3 twos mean that the third row was split into 3 samples\ntokenized_train['overflow_to_sample_mapping'][:10]","3c668339":"# Further exploration of the cases for those who are interested:\n\n## Original text:\n#print(df_train.iloc[1]['text'])\n## The four 512-token chunks generated by the tokenization procedure:\n#print(tokenizer.decode(tokenized_train['input_ids'][1]))\n#print(\"========\")\n#print(tokenizer.decode(tokenized_train['input_ids'][2]))","67319893":"class FeedbackPrizeDataset(Dataset):\n    def __init__(self, tokenized_ds):\n        self.data = tokenized_ds\n\n    def __getitem__(self, index):\n        item = {k: self.data[k][index] for k in self.data.keys()}\n        return item\n\n    def __len__(self):\n        return len(self.data['input_ids'])\n    \n\n# Create Datasets and DataLoaders for training and validation dat\n\nds_train = FeedbackPrizeDataset(tokenized_train)\ndl_train = DataLoader(ds_train, batch_size=config['train_batch_size'], \n                      shuffle=True, num_workers=2, pin_memory=True)\n\nds_val = FeedbackPrizeDataset(tokenized_val)\ndl_val = DataLoader(ds_val, batch_size=config['valid_batch_size'], \n                    shuffle=False, num_workers=2, pin_memory=True)","955aa128":"# https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533\ndef train(model, optimizer, dl_train, epoch):\n    \n    time_start = time.time()\n    \n    # Set learning rate to the one in config for this epoch\n    for g in optimizer.param_groups: \n        g['lr'] = config['learning_rates'][epoch]\n    lr = optimizer.param_groups[0]['lr']\n    \n    \n    epoch_prefix = f\"[Epoch {epoch+1:2d} \/ {config['epochs']:2d}]\"\n    print(f\"{epoch_prefix} Starting epoch {epoch+1:2d} with LR = {lr}\")\n    \n    # Put model in training mode\n    model.train()\n    \n    # Accumulator variables\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for idx, batch in enumerate(dl_train):\n        \n        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n        labels = batch['labels'].to(config['device'], dtype = torch.long)\n\n        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n                               return_dict=False)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        loss_step = tr_loss\/nb_tr_steps\n        \n        if idx % 200 == 0:\n            \n            print(f\"{epoch_prefix}     Steps: {idx:4d} --> Loss: {loss_step:.4f}\")\n        \n   \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        \n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n        \n        wandb.log({'Train Loss (Step)': loss_step, 'Train Accuracy (Step)' : tr_accuracy \/ nb_tr_steps})\n        \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=config['max_grad_norm']\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n\n    epoch_loss = tr_loss \/ nb_tr_steps\n    tr_accuracy = tr_accuracy \/ nb_tr_steps\n    \n    torch.save(model.state_dict(), f'pytorch_model_e{epoch}.bin')\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    elapsed = time.time() - time_start\n    \n    print(epoch_prefix)\n    print(f\"{epoch_prefix} Training loss    : {epoch_loss:.4f}\")\n    print(f\"{epoch_prefix} Training accuracy: {tr_accuracy:.4f}\")\n    print(f\"{epoch_prefix} Model saved to pytorch_model_e{epoch}.bin  [{elapsed\/60:.2f} mins]\")\n    wandb.log({'Train Loss (Epoch)': epoch_loss, 'Train Accuracy (Epoch)' : tr_accuracy})\n    print(epoch_prefix)","af6f04b0":"# from Rob Mulla @robikscube\n# https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter \/ len_gt\n    overlap_2 = inter\/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP \/ (TP + 0.5*(FP+FN))\n    return my_f1_score","93222bd6":"def inference(dl):\n    \n    # These 2 dictionaries will hold text-level data\n    # Helping in the merging process by accumulating data\n    # Through all the chunks\n    predictions = defaultdict(list)\n    seen_words_idx = defaultdict(list)\n    \n    for batch in dl:\n        ids = batch[\"input_ids\"].to(config['device'])\n        mask = batch[\"attention_mask\"].to(config['device'])\n        outputs = model(ids, attention_mask=mask, return_dict=False)\n        \n        del ids, mask\n        \n        batch_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy() \n    \n        # Go over each prediction, getting the text_id reference\n        for k, (chunk_preds, text_id) in enumerate(zip(batch_preds, batch['overflow_to_sample_mapping'].tolist())):\n            \n            # The word_ids are absolute references in the original text\n            word_ids = batch['wids'][k].numpy()\n            \n            # Map from ids to labels\n            chunk_preds = [IDS_TO_LABELS[i] for i in chunk_preds]        \n            \n            for idx, word_idx in enumerate(word_ids):                            \n                if word_idx == -1:\n                    pass\n                elif word_idx not in seen_words_idx[text_id]:\n                    # Add predictions if the word doesn't have a prediction from a previous chunk\n                    predictions[text_id].append(chunk_preds[idx])\n                    seen_words_idx[text_id].append(word_idx)\n    \n    final_predictions = [predictions[k] for k in sorted(predictions.keys())]\n    return final_predictions\n\n\n# https:\/\/www.kaggle.com\/zzy990106\/pytorch-ner-infer\n# code has been modified from original\n# I moved the iteration over the batches to inference because  \n# samples from the same text might have be split into different batches\ndef get_predictions(df, dl):\n    \n    all_labels = inference(dl)\n    final_preds = []\n    \n    for i in range(len(df)):\n        idx = df.id.values[i]\n        pred = all_labels[i]\n        preds = []\n        j = 0\n        \n        while j < len(pred):\n            cls = pred[j]\n            if cls == 'O': pass\n            else: cls = cls.replace('B','I')\n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n            if cls != 'O' and cls != '' and end - j > 7:\n                final_preds.append((idx, cls.replace('I-',''), \n                                    ' '.join(map(str, list(range(j, end))))))\n            j = end\n        \n    df_pred = pd.DataFrame(final_preds)\n    df_pred.columns = ['id','class','predictionstring']\n    return df_pred","200614df":"def validate(model, df_all, df_val, dl_val, epoch):\n    \n    time_start = time.time()\n    \n    # Put model in eval model\n    model.eval()\n    \n    # Valid targets: needed because df_val has a subset of the columns\n    df_valid = df_all.loc[df_all['id'].isin(IDS[valid_idx])]\n\n    # OOF predictions\n    oof = get_predictions(df_val, dl_val)\n\n    # Compute F1-score\n    f1s = []\n    classes = oof['class'].unique()\n    \n    epoch_prefix = f\"[Epoch {epoch+1:2d} \/ {config['epochs']:2d}]\"\n    print(f\"{epoch_prefix} Validation F1 scores\")\n    \n    f1s_log = {}\n    for c in classes:\n        pred_df = oof.loc[oof['class']==c].copy()\n        gt_df = df_valid.loc[df_valid['discourse_type']==c].copy()\n        f1 = score_feedback_comp(pred_df, gt_df)\n        print(f\"{epoch_prefix}   * {c:<10}: {f1:4f}\")\n        f1s.append(f1)\n        f1s_log[f'F1 {c}'] = f1\n    \n    elapsed = time.time() - time_start\n    print(epoch_prefix)\n    print(f'{epoch_prefix} Overall Validation F1: {np.mean(f1s):.4f} [{elapsed:.2f} secs]')\n    print(epoch_prefix)\n    f1s_log['Overall F1'] = np.mean(f1s)\n    wandb.log(f1s_log)","2a77c1b2":"config_model = AutoConfig.from_pretrained(MODEL_PATH+'\/config.json') \nmodel = AutoModelForTokenClassification.from_pretrained(\n                   MODEL_PATH+'\/pytorch_model.bin',config=config_model)\nmodel.to(config['device']);","0048edb3":"# Instantiate optimizer\noptimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])\n\n# Loop\nfor epoch in range(config['epochs']):\n    print()\n    train(model, optimizer, dl_train, epoch)\n    validate(model, df_all, df_val, dl_val, epoch)\n    \nprint(\"Final model saved as 'pytorch_model.bin'\")\ntorch.save(model.state_dict(), 'pytorch_model.bin')","8b59fc81":"# Model instantiation\n\nWe have just downloaded the base model from the Internet to `MODEL_PATH` at the beginning of the notebook. Now we will instantiate it.","8eee9e21":"# Load data and files\n\n\nCode from: https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533\n\nIn addition to loading the train dataframe, we will load all the train and text files and save them in a dataframe.","70d8fdd8":"The functions below were modified largely to handle the fact that one text might have more than one prediction.\n\nThe second `for` in `inference` (`for k, (chunk_preds, text_id) in enumerate...`) does the merge, using `predictions` and `seen_word_idx` to accumulate results.\n\n","738a3efa":"# A short exploration of the tokenization procedure","9a163c52":"# Create Train and Validation splits\n\n(from https:\/\/www.kaggle.com\/cdeotte\/pytorch-bigbird-ner-cv-0-615)","e73d0733":"# Validation functions\n\nWe will infer in batches using our data loader which is faster than inferring one text at a time with a for-loop.  The metric code is taken from Rob Mulla's great notebook [here][3]. Our model achieves validation F1 score 0.617! \n\nCode taken and adapted from Chris Deotte. In turn his work is based on [this][1] and [this][2].\n\nThe adaptions are the minimal required to handle the fact that one text might have generated more than one model sample.\n\nThe key `overflow_to_sample_mapping` is a mapping from the sample back to the original text.\n\nDuring inference our model will make predictions for each subword token. Some single words consist of multiple subword tokens. In the code below, we use a word's first subword token prediction as the label for the entire word. We can try other approaches, like averaging all subword predictions or taking `B` labels before `I` labels etc.\n\nMoreover, since there are a large overlaps, for long texts there will be more than one prediction for various token. In this version, we are using the first prediction found and dropping all the rest. A voting mechanism could be implemented.\n\n\n[1]: https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533\n[2]: https:\/\/www.kaggle.com\/zzy990106\/pytorch-ner-infer\n[3]: https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch","93d05417":"# Training function\nThe PyTorch training function is taken from Raghavendrakotala's great notebook [here][1]. It uses a masked loss which avoids computing loss when target is `-100` (that's the reason of those `-100` around).\n\nI did some simple modifications.\n\n\n[1]: https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533","cd5c10a6":"The submission is done in the following notebook: [\ud83d\udcd6 PyTorch- \"ShortFormer\" w\/Chunks - Infer [0.604]](https:\/\/www.kaggle.com\/julian3833\/pytorch-shortformer-w-chunks-infer-0-604)","364a8c6b":"# Training \/ validation loop\n\nWe loop for `config['epoch']` with a train\/validate schema.","4c0e38ba":"# WANDB","692e2777":"# Imports","a70faf52":"Moved the validation code into a function in order to validate during each epoch:","a292bd2a":"# Convert Train Text to NER Labels\nWe will now convert all text words into NER labels and save in a dataframe.","2b5cff22":"# Configuration","090ea806":"The evaluation code is long and is inherited from this notebook https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch.\nIt consists of 2 functions that calculate the F1 score:\n\n```python\ndef calc_overlap(row):\n    ...\n\ndef score_feedback_comp(pred_df, gt_df):\n    ...\n    \n```\n\nI have hidden the code in the cell below:","20160cc0":"# Download model from huggingface  hub\n\nHere we download the model using the Internet and we store it in the path `MODEL_PATH`.\n\nWe will create a dataset from this notebook's output so that local path will be available for the submission notebook.\n\nThe `Internet Disabled` restriction only applies to submission notebooks, so this is a valid workflow.\n\nThe dataset is public and available here: https:\/\/www.kaggle.com\/julian3833\/feedback-prize-roberta-weights. It is used in the [inference notebook](https:\/\/www.kaggle.com\/julian3833\/pytorch-shortformer-w-chunks-infer-0-604).","372c2f66":"## Dataset class\n\nWith the functional tokenization we performed above, the dataset class is trivial.","caa06395":"# Tokenization and chunking\n\nThis is **the main added value** of these notebooks.\n\nIn particular, the call to the `tokenizer` with the following parameters:\n\n* The text split already into words, in combination with `is_split_into_words=TRue`, as used by Chris Deotte and explained [here](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/tokenizer#transformers.PreTrainedTokenizer.prepare_for_tokenization.is_split_into_words).\n* `return_overflowing_tokens=True`, which activates the \"chunking\" mechanism (aka: will generate more than one tokenized sample for texts with more than 512 tokens).\n* `stride`: the size of the overlap between chunked parts of a text\n\n`return_overflowing_tokens=True`, besides creating the extra samples for long texts, sets the key `overflow_to_sample_mapping` in the resulting dictionary, which has the index of the original text that generated each of the samples.\n\nMoreover, the `word_ids(idx)` method returns a back-reference to the word index in the original text, indexed correctly no matter the chunk, doing a lot of the heavy-lifting. This is, for each token in the tokenized output, it says which word of the original text generated that token. ","ad0b0059":"# \ud83d\udcd6 PyTorch \"ShortFormer\" - RoBERTa w\/Chunks - Train [0.604]\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/31779\/logos\/header.png)\n\n### A NER \"ShortFormer\" with chunks, strides, and all the clumsy stuff\n\n**This notebook is a baseline model for the competition [Feedback Prize - Evaluating Student Writing](https:\/\/www.kaggle.com\/c\/feedback-prize-2021). It approaches the problems as a token classification problem (\"NER\"-like) and builds a RoBERTa base model with `max_length=512`. In order to do so, it manages the chunking with stride of the texts with length greater than 512 (and the posterior merge).**\n\nIt is a kind of follow-up of the public work, and relies heavily on the awesome public BigBird baseline by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte): [PyTorch - BigBird - NER - [CV 0.615]](https:\/\/www.kaggle.com\/cdeotte\/pytorch-bigbird-ner-cv-0-615). That notebook, in turn, uses code from the following ones:\n* [Fine-Tunned on Roberta-base as NER problem [0.533]](https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533) by [RAGHAVENDRAKUTTALA](https:\/\/www.kaggle.com\/raghavendrakotala)\n* [\ud83c\udf93 Student Writing Competition [Twitch Stream]](https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch) by [Rob Mulla](https:\/\/www.kaggle.com\/robikscube\/)\n* [Pytorch NER infer](https:\/\/www.kaggle.com\/zzy990106\/pytorch-ner-infer) by [zzy](https:\/\/www.kaggle.com\/zzy990106)\n\nDon't forget to upvote all these excellent kernels.\n\n\n\n### This is the training notebook.\n### The inference notebook is here: [\ud83d\udcd6 PyTorch- \"ShortFormer\" w\/Chunks - Infer [0.604]](https:\/\/www.kaggle.com\/julian3833\/pytorch-roberta-w-chunks-infer-0-604)\n\n&nbsp;\nI loved the dual training\/inference nature of Chris' notebook, but it was too much for me right now -I'm learning pytorch- so I unrolled it into the old Training\/Inference way that we are used to. \n\n\nBoth mostly follow Chris'. The main differences are:\n1. At the tokenizing step, where I used the hugging face tokenizer functionality to leverage the chunking. See that step for details about the implementation\n2. Validation is now performed on a per-epoch fashion\n3. The `inference` and `get_predictions` functions had to be adapted to the chunking as well.\n\n\n\n# Please _DO_ upvote if you found this kernel useful or interesting! \ud83e\udd17\n\n&nbsp;\n&nbsp;\n\n&nbsp;\n&nbsp;\n\n---\n\n# Oh, ($n^2$)oo!: Some context\n\n\nTransformer models are great. We all love them. _But_ the self-attention mechanism - the core of the Transformer architecture - has a matrix multiplication that scales quadratically with the input sequence length (at least) in terms of memory. The $QK^T$ costs a lot. And it makes the vanilla Transformer prohibitive for long sequences. This lead to the `512` tokens max length in the BERT-like models we see and use constantly.\n\nThere is research in the direction of reducing the cost of the attention operation so it scales in a slower fashion with the input length. Two recent models from this research are [LongFormer](https:\/\/arxiv.org\/abs\/2004.05150) and [BigBird](https:\/\/arxiv.org\/abs\/2007.14062), both put on the table by Chris Deotte in this competition (at least for me). \nThose models propose both slight variations of the self-attention mechanism that reduce the memory dependency to $O(n)$, this is, to scale linearly with the length of the input sequences. Both methods are \"sparse attention\" methods, meaning that, instead of each token attending to (and receiving attention from) all of the others, this cross-attention is pruned to a small number of tokens. In Longformer, there are 2 flavors of local windows (normal and dilated) and a global per-task attention, while in BigBird there is a window, a random and a global attention.\n\n\n   <center><img src=\"https:\/\/i.imgur.com\/t4MYmbj.png\" width=\"50%\"><\/center>\n      <center><i>From the LongFormer <a href=\"https:\/\/arxiv.org\/abs\/2004.05150\">paper<\/a><\/i><\/center>\n\n\n   <center><img src=\"https:\/\/i.imgur.com\/4bkL2JA.png\" width=\"50%\"><\/center>\n      <center><i>From the BigBird <a href=\"https:\/\/arxiv.org\/abs\/2007.14062\">paper<\/a><\/i><\/center>\n\n&nbsp;\n&nbsp;\n\nThe lower cost of these sparse self-attention mechanisms allows these models to handle up to `4096` in a normal GPU, this is, `8x` what a normal Transformer can.\n\nGiven the lengths of the texts in this competition, it is no surprise that the current public work is focused on those so called \"Longformer\" models and it is probable that they will be an important part of the final ensemble solutions.\n\nBut... but..., on the other hand, the old-fashioned 512-token models _do have_ a mechanism to cope with their sequence length limitations. For a given sequence of length greater than `512`, before longformer-like models, the NLP community would:\n\n1. Split it into chunks of `512` tokens (possibly with some overlap)\n2. Use the model to process those chunks\n3. Merge back the predictions over the chunks to obtain predictions over the full text\n\nThis mechanism was used, for example, during the recently finished [ChaII competition](https:\/\/www.kaggle.com\/c\/chaii-hindi-and-tamil-question-answering), starting from [Darek K\u0142eczek](https:\/\/www.kaggle.com\/thedrcat)'s [baseline](https:\/\/www.kaggle.com\/thedrcat\/chaii-eda-baseline).\n\n\nIt is possible that this mechanism is still relevant although the sparse-attention models.\n\nIt is possible that [\"ShortFormers\"](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/discussion\/297461) have something to say in this competition? Maybe add some variance to an ensemble? ... Or even more?\n\n\n---\n\n\n&nbsp;\n&nbsp;\n\n&nbsp;\n&nbsp;\n\n\nOk, let's go!","544a9673":"# Please _DO_ upvote if you found this kernel useful or interesting! \ud83e\udd17"}}