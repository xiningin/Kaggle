{"cell_type":{"07f9aea6":"code","0ee89fa0":"code","dc4ced1c":"code","b880a27c":"code","a19833f6":"code","832858da":"code","963551bc":"code","19e1b7b4":"code","7c6e5ade":"code","afcddb02":"code","fed2deee":"code","590044d4":"code","ca244cb4":"code","067d8564":"code","4adb7682":"code","71488400":"code","72cadde6":"code","261574b6":"code","4db5a024":"code","25c5e149":"code","4e8e879b":"code","465b0b69":"code","8ead9740":"code","4986cc88":"code","520c9e50":"code","0c27bf4b":"code","ad6d077f":"code","1697f067":"code","7a47b525":"markdown","918667e8":"markdown","c4506bdf":"markdown","048d4e03":"markdown","6b562649":"markdown","2f03956e":"markdown","7390bcc3":"markdown","2b04f811":"markdown","0b44d918":"markdown","919535f8":"markdown","e0a37bb6":"markdown","713e3863":"markdown","a9014a2e":"markdown"},"source":{"07f9aea6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0ee89fa0":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dc4ced1c":"#reading dataset\ndf = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","b880a27c":"#take quick look at our dataset\ndf.head()","a19833f6":"df.info()","832858da":"dropout = ['Unnamed: 32', 'id'] #selecting columns to drop\ndf.drop(dropout, axis=1, inplace=True)","963551bc":"mappings = {'M': 0, \"B\": 1} #mapping for diagnosis column\ndf.replace({\"diagnosis\": mappings}, inplace=True)","19e1b7b4":"#Let's look at the class distribution of diagnosis column\nsns.countplot(df['diagnosis'])","7c6e5ade":"corr_matrix = df.corr() #correlation between variables\ncorr_matrix","afcddb02":"#Looking at correlation matrix is overwhelming so let's plot the correlation matrix.\nplt.figure(figsize=(18,18))\nsns.heatmap(corr_matrix, annot=True)\nplt.show()","fed2deee":"corr_matrix['diagnosis'].sort_values(ascending=False)","590044d4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","ca244cb4":"def train_models(X_train, y_train, X_test, y_test, X, y):\n    '''training models and gathering accuracy score and cross validation score for each model'''\n    #Algorithms we are going to use\n    models = [LogisticRegression(random_state=42, max_iter=150), SVC(kernel='linear', C=1), \n            RandomForestClassifier(n_estimators=20)]\n    model_names = ['logisticRegression', 'SupportVectorClassifier', 'RandomForest']\n    accuracy = []\n    cross_score = [] \n    for model in models:\n        model.fit(X_train, y_train)\n        accuracy.append(accuracy_score(y_test, model.predict(X_test)))\n        cross_score.append(cross_val_score(model, X, y, cv=5).mean())\n        \n    result_df = pd.DataFrame(list(zip(accuracy, cross_score)),columns=['accuracy','cross_score'], index=model_names)\n        \n    return result_df","067d8564":"#selecting columns which has correlation less than 0.9\ncolumns = np.full((corr_matrix.shape[0],), True, dtype=bool)\nfor i in range(corr_matrix.shape[0]):\n    for j in range(i+1, corr_matrix.shape[0]):\n        if corr_matrix.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False","4adb7682":"selected_columns = df.columns[columns] #these are the columns which has correlation less that 0.9\nselected_columns","71488400":"copy_df = df[selected_columns].copy()","72cadde6":"X = np.array(copy_df.drop('diagnosis', axis=1))\ny = np.array(copy_df['diagnosis'])\n\n#train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","261574b6":"corr_result = train_models(X_train, y_train, X_test, y_test, X, y)\ncorr_result","4db5a024":"new_df = df.copy() #copy df\n\nX = new_df.drop('diagnosis', axis=1)\ny = new_df['diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #train test split","25c5e149":"def evaluate_metric(X_test, y_test, model):\n    \"\"\"Evalutaion metric for our classifier\"\"\"\n    accuracy = accuracy_score(y_test, model.predict(X_test))\n    return accuracy","4e8e879b":"def forward_feature_selection(X_train, X_test, y_train, y_test, n):\n    \"\"\"Forward feature selection return set of features\"\"\"\n    feature_set = []\n    for num_features in range(n):\n        metric_list = []\n        model = LogisticRegression(random_state=42)\n        for feature in X_train.columns:\n            if feature not in feature_set:\n                f_set = feature_set.copy()\n                f_set.append(feature)\n                model.fit(X_train[f_set], y_train)\n                metric_list.append((evaluate_metric(X_test[f_set], y_test, model), feature))\n        metric_list.sort(key=lambda x : x[0], reverse = True)\n        feature_set.append(metric_list[0][1])\n    return feature_set","465b0b69":"forward_selection_selected_features = forward_feature_selection(X_train, X_test, y_train, y_test, 10) #selecting top 10 features \nforward_selection_selected_features ","8ead9740":"new_df = new_df[forward_selection_selected_features]","4986cc88":"forward_result = train_models(X_train, y_train, X_test, y_test, new_df, y)\nforward_result","520c9e50":"back_df = df.copy()\n\nX = back_df.drop('diagnosis', axis=1)\ny = back_df['diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","0c27bf4b":"cols = list(X.columns)\npmax = 1\nwhile(len(cols) > 0):\n    p = []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y, X_1)\n    res = model.fit()\n    p = pd.Series(res.pvalues.values[1:], index=cols)\n    pmax = max(p)\n    feature_with_max_p = p.idxmax()\n    if (pmax>0.05):\n        cols.remove(feature_with_max_p)\n    else:\n        break\n        \nbackward_elimination_selected_feature = cols\nbackward_elimination_selected_feature","ad6d077f":"back_df = back_df[backward_elimination_selected_feature]","1697f067":"backward_selection_result = train_models(X_train, y_train, X_test, y_test, back_df, y)\nbackward_selection_result","7a47b525":"**Forward Feature Selection**","918667e8":"* Meaning of correlation 1 means, they are positively correlated.\n* Meaning of correaltion 0 means, they are not having any correlion at all.\n* Meaning of correlation -1 means, they are negatively correlated.","c4506bdf":"* Dataset has 33 columns out of which 32 are type float64 and one column is object type.\n* Unnamed: 32 column has all NaN values.\n* We will remove the id and Unnamed : 32 columns.\n","048d4e03":"**Backward Elimination Method**","6b562649":"**Conclusion**\n* Main motive to share this notebook is selecting features by different methods and using those feature apply different algorithms and see performance\n* I'm newbie if I did some mistakes feel free to comment down.","2f03956e":"* First we have created function which takes train set, test set , X (all features) and y (label) and this function returns dataframe containing accuracy score and cross validation score of each algorithm.\n* We are going to use LogisticRegression, SupportVectorClassifier and RandomForest algorithms.","7390bcc3":"* Accuracy score of LogisticRegression is 0.92 and Cross Validation Score is 0.94.\n* Accuracy score of SupportVectorClassifier is 0.92 and Cross Validation Score is 0.95.\n* Accuracy score of RandomForest is 0.95 and Cross Validation Score is 0.95.","2b04f811":"**This notebook is for basic feature selection methods and appling different algorithms**\n* If you find this helpful upvote it and for any queries feel free to comment down.","0b44d918":"Class label diagnosis is object type so we have to convert it into int. We will do this by mapping values 0 for Malignant and 1 for Benign.","919535f8":"Looking at correlation between class label(diagnosis) and every other variables, we can see that only four variables are positively correlated and every other variables are negatively correlated with the class label(diagnosis).","e0a37bb6":"**Selecting Features depends on Correlation**","713e3863":"* Accuracy score of LogisticRegression is 0.94 and Cross Validation Score is 0.94.\n* Accuracy score of SupportVectorClassifier is 0.95 and Cross Validation Score is 0.95.\n* Accuracy score of RandomForest is 0.99 and Cross Validation Score is 0.95.","a9014a2e":"* Accuracy score of LogisticRegression is 0.92 and Cross Validation Score is 0.92.\n* Accuracy score of SupportVectorClassifier is 0.90 and Cross Validation Score is 0.94.\n* Accuracy score of RandomForest is 0.93 and Cross Validation Score is 0.95."}}