{"cell_type":{"f2d96eec":"code","e6feb7d2":"code","96a714b3":"code","4788637f":"code","9d71b1e1":"code","d45dd20f":"code","978b313b":"code","155b6a0c":"code","e4022a63":"code","2717e76b":"code","07f3578a":"markdown","db48ecdc":"markdown","4ea2de1b":"markdown","fae25e50":"markdown","4638c326":"markdown","52c85448":"markdown","cdfdb503":"markdown","bf3121cb":"markdown","91a9b56d":"markdown","8d2ad31d":"markdown","285a736c":"markdown"},"source":{"f2d96eec":"import numpy as np # linear algebra\nfrom sklearn.datasets import load_wine\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfigsize=(10,5)","e6feb7d2":"X, y = load_wine(return_X_y=True)\nprint(f\"X.shape={X.shape}, y.shape={y.shape}\")","96a714b3":"X_std = (X-X.mean(axis=0))\/X.std(axis=0)","4788637f":"cov_mat = np.cov(X_std.T)\nprint(f\"Covariance matrix shape:{cov_mat.shape}\")","9d71b1e1":"# Eigen decompose the covariance matrix\neigen_values, eigen_vectors = np.linalg.eig(cov_mat)\neigen_pairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]","d45dd20f":"# Sort the eigen pairs by eigen values in descending order\neigen_pairs.sort(key=lambda k: k[0], reverse=True)","978b313b":"total_var = np.sum([i[0] for i in eigen_pairs])\nexpl_var = [i[0]\/total_var for i in eigen_pairs]\ncumulative_var = np.cumsum(expl_var)","155b6a0c":"plt.figure(figsize=figsize)\nplt.bar(range(1,X.shape[1]+1), expl_var, alpha=0.5, align='center',label='Individual explained variance')\nplt.step(range(1,X.shape[1]+1), cumulative_var, where='mid',label='Cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component')\nplt.legend(loc='best')\nplt.grid();\nplt.tight_layout();","e4022a63":"w = np.hstack((eigen_pairs[0][1][:, np.newaxis],eigen_pairs[1][1][:, np.newaxis]))\nprint(f\"Transformation matrix shape:{w.shape}\")","2717e76b":"X_pca = X_std.dot(w)\nplt.figure(figsize=figsize)\ncolors =['tab:blue','tab:orange','tab:green']\nmarkers=list('svo')\nfor l,c,m in zip(np.unique(y), colors, markers):\n    plt.scatter(X_pca[y==l, 0],\n                X_pca[y==l, 1],\n                color='white',\n                edgecolor=c,\n                label=l,\n                marker=m\n               )\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.grid();\nplt.legend()\nplt.tight_layout();","07f3578a":"### 7. Transform the input dataset using $W$\n\n\\begin{align*}\nX^\\prime = XW\n\\end{align*}\n\n> $X \\in \\mathbb{R}^{n\\mathbb{x}m}$ $n$ samples and $m$ features \n\n> $W \\in \\mathbb{R}^{m\\mathbb{x}k}$ $m$ features and $k$ components \n\n> $X^\\prime \\in \\mathbb{R}^{n\\mathbb{x}k}$ $n$ samples and $k$ principal components ","db48ecdc":"The open source UCI wine dataset built into sklearn has 13 features for each sample. The target is one of 3 cultivars from Italy. The targets are not used by PCA and are used here only for illustrating PCA results in the end.","4ea2de1b":"### 2. Create covariance matrix\n\nCovariance between features $j$ and $k$ is given by &ndash;\n\n\\begin{align*}\n\\sigma_{jk}= \\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n}(x_j^{(i)}-\\mu_j)(x_k^{(i)}-\\mu_k)\n\\end{align*}\n\nA covariance matrix with $m$ features will look like this&ndash;\n\n\\begin{align*}\n\\Sigma = \\begin{bmatrix}\\sigma_{1}^2&\\sigma_{12}&\\ldots&\\sigma_{1m}\\\\\\sigma_{21}&\\sigma_{2}^2&\\ldots&\\sigma_{2m}\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\\\sigma_{m1}&\\sigma_{m2}&\\ldots&\\sigma_{m}^2\\end{bmatrix}\n\\end{align*}\n\n> $\\Sigma \\in \\mathbb{R}^{m\\mathbb{x}m}$\n","fae25e50":"We can see that first two principal components account for roughly 60% of the explained variance","4638c326":"### 1. Standardize the  $m$-dimensional dataset\n\nPCA is sensitive to the scaling of input variables. Input $X$ must be standardized before implementing PCA.\n\n\\begin{align*}\nX_{std} = \\frac{(X - \\mu)}{\\sigma}\n\\end{align*}","52c85448":"### 5. Select $k$ eigenvectors\n\nSince our goal is to reduce dimensionality, we want to only select a few eigenvectors. However, it is important to select $k$ such that most of the variance is explained by the components we select\n\nExplained variance ratio = $\\frac{\\lambda_j}{\\sum_{j=1}^{d}\\lambda_j}$","cdfdb503":"## Principal Component Analysis (PCA)\n\nPCA is an unsupervised dimensionality reduction technique that can be used for feature extraction, wherein we can construct a lower dimensional subspace from a dataset with large number of features.\n\nThe goal of PCA is to find directions of maximum variance in high-dimensional data and project the data into a new subspace with equal or fewer dimensions.\n\nHere we explore PCA step by step using ````numpy```` alone. ````scikit```` is referenced only for the sample UCI wine dataset.","bf3121cb":"PCA can be summarized in following steps-\n\n1. Standardize the $m$-dimensional dataset\n2. Create covariance matrix\n3. Perform eigen decomposition of the covariance matrix\n4. Sort eigenvectors in descending order of their corresponding eigenvalues\n5. Select $k$ eigenvectors with the largest eigenvalues, where $k$ is the number of principal components we are interested in $(k \\leq m)$\n6. Create projection matrix $W$ from top $k$ eigenvectors\n7. Transform the input dataset using $W$ to get new $k$-dimensional features \n\n","91a9b56d":"### 3. Perform eigen decomposition of the covariance matrix\n\nWe want to find eigenpairs of the covariance matrix such that&ndash;\n\n\\begin{align*}\n\\Sigma v = \\lambda v\n\\end{align*}\n\n>$\\lambda$ is a scalar, the eigenvalue\n\nEigenvalues are simply the coefficients attached to eigenvectors, which give the axes magnitude. In this case, they are the measure of the data\u2019s covariance.","8d2ad31d":"### 4. Sort eigenvectors \n\nSort eigenvectors in descending order of their corresponding eigenvalues","285a736c":"### 6. Create projection matrix $W$ from top $k$ eigenvectors\n\nCreate the projection matrix $W$ using top $k$ eigenvectors. \n\n> $W \\in \\mathbb{R}^{m \\mathbb{x} k}$"}}