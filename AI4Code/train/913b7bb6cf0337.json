{"cell_type":{"f932e712":"code","8b9f0102":"code","2f0339d4":"code","b8a8f6d4":"code","40d0156d":"code","8c972156":"code","ef0053c8":"code","f7327623":"code","55f12777":"code","794a0c41":"code","ea8719cd":"code","cc3f0f3d":"code","06b41469":"code","87fd990a":"code","cf202523":"code","ab4b83b1":"code","2857bbc3":"code","a4dd6a16":"code","252b02f5":"code","23b82acd":"code","7e9ffc37":"code","cf6e0cfb":"markdown","f8529fcb":"markdown","35082041":"markdown"},"source":{"f932e712":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow.keras.utils import normalize\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8b9f0102":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","2f0339d4":"X = train[[col for col in train.columns if \"stop\" in col]]","b8a8f6d4":"y = train[[col for col in train.columns if \"start\" in col]]","40d0156d":"X= np.array(X)\ny = np.array(y)","8c972156":"#X = normalize(X)\n#y = normalize(y)","ef0053c8":"X= X.reshape(-1, 20, 20, 1)","f7327623":"model = Sequential()\n# Adds a densely-connected layer with 64 units to the model:\nmodel.add(Conv2D(64,(3,3), activation = 'relu', input_shape = X.shape[1:]))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n# Add another:\nmodel.add(Conv2D(64,(3,3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\n# Add a softmax layer with 10 output units:\nmodel.add(Dense(400, activation='softmax'))\n\nmodel.compile(optimizer=\"adam\",\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","55f12777":"model.fit(X,y, epochs=10, batch_size=32, validation_split=0.1)","794a0c41":"test.head()","ea8719cd":"x_test = test.drop(['id', 'delta'], axis=1)","cc3f0f3d":"x_test = np.array(x_test).reshape(-1,20,20,1)","06b41469":"# = normalize(x_test)","87fd990a":"predictions = model.predict(x_test)","cf202523":"#np.argmax(predictions[0][340])\n#int(round(predictions[1][0]))\npredictions = predictions.round()\npredictions = predictions.astype(int)\n#predicted_val = [int(round(p)) for p in predictions]","ab4b83b1":"column_names  = ['start.'+str(i) for i in range(1,401)]","2857bbc3":"submission = pd.DataFrame(data=predictions,    # values\n              columns=column_names)","a4dd6a16":"id_col = test[['id']]","252b02f5":"submission['id'] = id_col","23b82acd":"submission.head()","7e9ffc37":"submission.to_csv(\"submission.csv\", index = False)","cf6e0cfb":"That's all . However will try this example with some other way. In the meantime please provide your suggestions","f8529fcb":"I have tried with skipping normalization as well adding it didn't make much difference. However normalization is an important step but in case of binary I was'nt sure","35082041":"Hello All !\n\nAgain back with another kaggle attempt. \n\nTaken reference from [Cats and Dogs Kernel](https:\/\/www.kaggle.com\/ruchibahl18\/cats-vs-dogs-basic-cnn-tutorial)\n\nUsing CNN to resolve this problem. Got a score of 0.14294. However still think rounding result might cause problem. Any suggestions welcome :)\n"}}