{"cell_type":{"73ea7ca6":"code","f6a7ea1b":"code","2b0732f9":"code","ee34523c":"code","38459e12":"code","a5c4ee1d":"code","0c3cd84e":"code","5fad0c26":"code","c598479c":"code","4e062793":"code","c56781ee":"code","447202b3":"code","95b48ea9":"code","f8aa2677":"code","fa76a5ae":"code","f4b91ba2":"code","f59bad0d":"code","905d9fdf":"code","6405d6c0":"code","bfee7761":"code","1bf5c3b8":"code","71c09201":"code","73422598":"code","6fd0e960":"code","db7c5b75":"code","73ec2bab":"code","6a3bef15":"code","e1afb6a4":"code","a2864194":"markdown","50af65f4":"markdown","27179352":"markdown"},"source":{"73ea7ca6":"#Importing Libraries\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport time\nfrom BorutaShap import BorutaShap\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score,plot_confusion_matrix,accuracy_score\nfrom sklearn.feature_selection import SelectKBest,chi2,RFECV\nfrom sklearn.decomposition import PCA","f6a7ea1b":"#Loading Data\ndata = pd.read_csv(\"..\/input\/breastcancer-dataset\/data.csv\")\ndata.info()\ndata.head()","2b0732f9":"#Separating target from features\ncol = data.columns\ny = data['diagnosis']\ncol_drop = ['id','diagnosis','Unnamed: 32']\nx = data.drop(col_drop,axis=1)","ee34523c":"#Plot Diagnosis Distribution\nax = sns.countplot(y,label=\"Count\",palette=\"RdBu_r\")\nB,M = y.value_counts()\nprint(\"Number of Benign Tumors: \",B)\nprint(\"Number of Malign Tumors: \",M)","38459e12":"#Features Statistics\nx.describe()","a5c4ee1d":"#Normalizing Dataset\ndata = x\ndata_std = (data - data.mean())\/data.std()","0c3cd84e":"#Violin Plot\nfig,ax = plt.subplots(figsize = (15,35))\n\nplt.subplot(3,1,1)\ndata = pd.concat([y,data_std.iloc[:,:10]],axis=1)\ndata = pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nsns.violinplot(data=data,hue='diagnosis',x='features',y='value',split=True,inner='quartile')\nplt.xticks(rotation=45)\n\nplt.subplot(3,1,2)\ndata = pd.concat([y,data_std.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nsns.violinplot(data=data,hue='diagnosis',x='features',y='value',split=True,inner='quartile')\nplt.xticks(rotation=45)\n\nplt.subplot(3,1,3)\ndata = pd.concat([y,data_std.iloc[:,20:30]],axis=1)\ndata = pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nsns.violinplot(data=data,hue='diagnosis',x='features',y='value',split=True,inner='quartile')\nplt.xticks(rotation=45)\n","5fad0c26":"#Jointplots for correlation\nsns.jointplot(x['concavity_mean'],x['concave points_mean'],kind=\"regg\")\nsns.jointplot(x['concavity_worst'],x['concave points_worst'],kind=\"regg\")","c598479c":"#SwarmPlots - To determine class separability\nfig,ax = plt.subplots(figsize = (15,40))\n\nplt.subplot(3,1,1)\ndata = pd.concat([y,data_std.iloc[:,:10]],axis=1)\ndata = pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nsns.swarmplot(data=data,hue='diagnosis',x='features',y='value',palette=\"cubehelix\")\nplt.xticks(rotation=45)\n\nplt.subplot(3,1,2)\ndata = pd.concat([y,data_std.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nsns.swarmplot(data=data,hue='diagnosis',x='features',y='value',palette=\"cubehelix\")\nplt.xticks(rotation=45)\n\nplt.subplot(3,1,3)\ndata = pd.concat([y,data_std.iloc[:,20:30]],axis=1)\ndata = pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nsns.swarmplot(data=data,hue='diagnosis',x='features',y='value',palette=\"cubehelix\")\nplt.xticks(rotation=45)","4e062793":"#Heatmap\nfig,ax = plt.subplots(figsize=(20,20))\nsns.heatmap(x.corr(),annot=True,linewidth=0.5,fmt=\".1f\",ax=ax)","c56781ee":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)","447202b3":"# Removing cols by looking at heatmap (Minimal Feature selection)\ndrop_col = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean',\n            'radius_se','perimeter_se','compactness_se','concave points_se',\n            'radius_worst','perimeter_worst','area_worst','texture_worst','compactness_worst','concave points_worst']\ndf = x.drop(drop_col,axis=1)\nx_train1,x_test1,y_train1,y_test1 = train_test_split(df,y,test_size=0.3,random_state=42)\nfig,ax = plt.subplots(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True,linewidth=0.5,fmt=\".1f\",ax=ax)","95b48ea9":"#Univariate feature selection\nselect_feature = SelectKBest(chi2,k=10).fit(x_train1,y_train1)\nx_train2 = select_feature.transform(x_train1)\nx_test2 = select_feature.transform(x_test1)","f8aa2677":"#Principal Component Analysis\nx_train_norm = (x_train - x_train.mean())\/(x_train.max()-x_train.min())\nx_test_norm = (x_test - x_test.mean())\/(x_test.max()-x_test.min())","fa76a5ae":"pca = PCA()\npca.fit(x_train_norm)\nplt.figure(figsize=(10,8))\nsns.lineplot(data=np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"No of components\")\nplt.ylabel(\"Cumulative explained variance\")","f4b91ba2":"#XGBoost using Minimal feature selection\nmodel1 = xgb.XGBClassifier(random_state=42)\nmodel1.fit(x_train1,y_train1)\ny_pred1 = model1.predict(x_test1)\nprint(\"Accuracy: \",accuracy_score(y_test1,y_pred1))\nplot_confusion_matrix(model1,x_test1,y_test1,cmap=plt.cm.Blues)\nplt.grid(False)","f59bad0d":"#XGBost using Univariate feature selection\nmodel2 = xgb.XGBClassifier(random_state=42)\nmodel2.fit(x_train2,y_train1)\ny_pred2 = model2.predict(x_test2)\nprint(\"Accuracy: \",accuracy_score(y_test1,y_pred2))\nplot_confusion_matrix(model2,x_test2,y_test1,cmap=plt.cm.Blues)\nplt.grid(False)","905d9fdf":"#XGBost using Recursive feature selection with cross validation\nmodel3 = xgb.XGBClassifier()\nrfecv = RFECV(estimator=model3,step=1,cv=5,n_jobs=-1,min_features_to_select=10,scoring='accuracy').fit(x_train,y_train)\ny_pred3 = rfecv.predict(x_test)\nprint(\"Accuracy: \",accuracy_score(y_test,y_pred3))\nplot_confusion_matrix(rfecv,x_test,y_test,cmap=plt.cm.Blues)\nplt.grid(False)","6405d6c0":"num_features = [i for i in range(1,len(rfecv.grid_scores_)+1)]\ncv_scores = rfecv.grid_scores_\nsns.lineplot(num_features,cv_scores)\nprint(cv_scores)","bfee7761":"#Random Forest using Boruta Shap\nfeature_selector = BorutaShap(importance_measure='shap',classification=True)\nfeature_selector.fit(x_train,y_train,random_state=0,n_trials=100)","1bf5c3b8":"feature_selector.plot(which_features='all')","71c09201":"x_train_boruta = feature_selector.Subset()\nx_test_boruta = x_test[['concave points_mean', 'texture_mean', 'radius_se', 'texture_worst', 'symmetry_worst', 'perimeter_worst', 'concavity_worst', 'radius_mean', 'concavity_mean', 'area_worst', 'smoothness_worst', 'area_se', 'area_mean', 'compactness_worst', 'radius_worst', 'perimeter_se', 'concave points_worst', 'fractal_dimension_worst', 'compactness_mean', 'perimeter_mean']]\n","73422598":"model7 = xgb.XGBClassifier(random_state=42,learning_rate=0.01,n_estimators=1000)\nmodel7.fit(x_train_boruta,y_train)\ny_pred7 = model7.predict(x_test_boruta)\nprint(\"Accuracy: \",accuracy_score(y_test,y_pred7))\nplot_confusion_matrix(model7,x_test_boruta,y_test,cmap=plt.cm.Blues)\nplt.grid(False)","6fd0e960":"#Logistic Regression using Univariate feature selection\nmodel4 = LogisticRegression(C=500,penalty='l2',max_iter=500)\nmodel4.fit(x_train2,y_train1)\ny_pred4 = model4.predict(x_test2)\nprint(\"Accuracy: \",accuracy_score(y_test1,y_pred4))\nplot_confusion_matrix(model4,x_test2,y_test1,cmap=plt.cm.Blues)\nplt.grid(False)\n","db7c5b75":"#SVM using Univariate feature selection\nmodel5 = SVC(C=500,gamma='scale',kernel='poly',degree=1)\nmodel5.fit(x_train2,y_train1)\ny_pred5 = model5.predict(x_test2)\nprint(\"Accuracy: \",accuracy_score(y_test1,y_pred5))\nplot_confusion_matrix(model5,x_test2,y_test1,cmap=plt.cm.Blues)\nplt.grid(False)","73ec2bab":"#Random Forest Classifier using Univariate Linear Regression\nmodel6 = RandomForestClassifier(n_estimators=1000,max_depth=20,criterion='entropy',bootstrap=True)\nmodel6.fit(x_train2,y_train1)\ny_pred6 = model6.predict(x_test2)\nprint(\"Accuracy: \",accuracy_score(y_test1,y_pred6))\nplot_confusion_matrix(model6,x_test2,y_test1,cmap=plt.cm.Blues)\nplt.grid(False)","6a3bef15":"#Naive Bayes Classifier using Univariate feature selection\nmodel8 = GaussianNB()\nmodel8.fit(x_train2,y_train1)\ny_pred8 = model8.predict(x_test2)\nprint(\"Accuracy: \",accuracy_score(y_test1,y_pred8))\nplot_confusion_matrix(model8,x_test2,y_test1,cmap=plt.cm.Blues)\nplt.grid(False)","e1afb6a4":"#K-Nearest Neighbors using Univariate feature selection\nmodel9 = KNeighborsClassifier(n_neighbors=8)\nmodel9.fit(x_train2,y_train1)\ny_pred9 = model9.predict(x_test2)\nprint(\"Accuracy: \",accuracy_score(y_test1,y_pred9))\nplot_confusion_matrix(model9,x_test2,y_test1,cmap=plt.cm.Blues)\nplt.grid(False)","a2864194":"# Exploratory Data Analysis (EDA)","50af65f4":"# Classification Model","27179352":"# Feature Engineering"}}