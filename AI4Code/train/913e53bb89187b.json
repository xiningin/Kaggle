{"cell_type":{"c71bf38a":"code","ab214cce":"code","ab99ace6":"code","c7e4e7f0":"code","51dd4b76":"code","1b2096cb":"code","498994fa":"code","9bd7f379":"code","07c6eb86":"code","780e9291":"code","0e2d0fa5":"code","aaf90085":"code","3f657a72":"code","07d3e630":"code","ccdbc412":"code","d4932ea5":"code","362df641":"code","6283be73":"code","93014673":"code","b03af096":"code","bc2ca904":"code","a36badf2":"code","edbf24a1":"code","6c1cf236":"code","3a3bda4e":"code","786224ce":"code","68300438":"code","81417367":"code","177efb9a":"code","abd19818":"code","052f8776":"code","9e8a076f":"code","9f4a44df":"code","1e70e3e7":"code","7900c582":"code","5ccc3e61":"code","f0b7e96a":"markdown","e7f1cdb6":"markdown","cf48c3b0":"markdown","08e25b89":"markdown","754521ca":"markdown","186c6217":"markdown","e65fbc46":"markdown","42f44be3":"markdown","7be76575":"markdown","e5e304cf":"markdown","1216cb30":"markdown","2d03ba05":"markdown","c4836f7c":"markdown","3715f24f":"markdown","a891271a":"markdown","c98c84f9":"markdown","280b0e9e":"markdown","17cf17e2":"markdown","bc43cc22":"markdown","3780abf7":"markdown","c05c0092":"markdown","377b3ad4":"markdown","d00b4b6e":"markdown","70c48cfc":"markdown","3b41dd36":"markdown","e1342837":"markdown","2b64e1a0":"markdown","9823abd4":"markdown","23e3edd7":"markdown","e26cc0d0":"markdown","efaee4c9":"markdown","d7833d84":"markdown","38b0786e":"markdown","b2ecf164":"markdown","507cf180":"markdown","d06b5864":"markdown","961935ea":"markdown","cb25feaa":"markdown","9e34f918":"markdown","6413344c":"markdown","43279127":"markdown","fac0a712":"markdown","719190f1":"markdown","374139b8":"markdown","82ac11a3":"markdown","a00748d0":"markdown","c3601b11":"markdown"},"source":{"c71bf38a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ab214cce":"#hide some warnings :p\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","ab99ace6":"kaggle_path = '..\/input\/titanic\/'\ntrain = pd.read_csv(kaggle_path + 'train.csv')\ntest = pd.read_csv(kaggle_path + 'test.csv')\nsubmission = pd.read_csv(kaggle_path + 'gender_submission.csv')","c7e4e7f0":"print(train.shape)\ntrain.head()","51dd4b76":"print(test.shape)\ntest.head()","1b2096cb":"train.isna().sum()","498994fa":"test.isna().sum()","9bd7f379":"train['Name'][0:10]","07c6eb86":"titles = ['Mr.', 'Mrs.', 'Miss.', 'Ms.', 'Master.', 'Dr.']   #creating list of relevant titles","780e9291":"print('Names with missing ages insight:\\n')\ntotal = 0\nfor t in titles:\n    count1 = train['Name'][train['Age'].isna()][train['Name'].str.find(t)!=-1].count()\n    count2 = test['Name'][test['Age'].isna()][test['Name'].str.find(t)!=-1].count()\n    print('Names having', t, 'in them are:', count1+count2)\n    total += count1","0e2d0fa5":"total == train['Age'].isna().sum()","aaf90085":"age_dict = {}\nfor t in titles:\n    # Mean of a title in train set\n    trm = train['Age'][train['Name'].str.find(t)!=-1][train['Age'].notna()].mean()\n    #mean of title in test set\n    tsm = test['Age'][test['Name'].str.find(t)!=-1][test['Age'].notna()].mean()\n    if np.isnan(trm):\n        trm = 0\n    if np.isnan(tsm):\n        tsm = 0\n    avg = round( (trm + tsm) \/ 2)\n    print('average age of Names having',t ,'in them is: ', avg)\n    age_dict[t] = avg","3f657a72":"age_dict","07d3e630":"def fill_na_names(df):\n    missing_ages = df['Name'][df['Age'].isna()]\n    index = df['Name'][df['Age'].isna()].index\n    for name,i in zip(missing_ages, index):\n        for ttl in age_dict:\n            if name.find(ttl) != -1:\n                df.loc[i, 'Age'] = age_dict[ttl]\n                \n            \nfill_na_names(train)\nfill_na_names(test)","ccdbc412":"train['Age'].isna().sum(), test['Age'].isna().sum()","d4932ea5":"m = test[test['Fare'].isna()]\ni = test[test['Fare'].isna()].index\nm","362df641":"# let's see if he shares a ticket with someone else\ntest[test.Ticket=='3701']","6283be73":"m1 = test['Fare'][test['Pclass']==3].mean()\nm2 = test['Fare'][test['Age']> 50].mean()\nm3 = test['Fare'][test['Embarked']=='S'].mean()\nm4 = test['Fare'][test['Sex']=='male'].mean()\nm5 = test['Fare'][test['SibSp']==0].mean()\nm6 = test['Fare'][test['Parch']==0].mean()\n\navg = round( (m1+ m2+ m3 + m4 + m5 + m6) \/ 6 )\n\ntest.loc[i,'Fare'] = avg\nprint('Filled ', round(avg))","93014673":"train.head(2)","b03af096":"def add_fam_col(df):\n    for n,ind in zip(df['Name'].values, df['Name'].index):\n        df.loc[ind, 'Family'] = n.split(',')[0]\n        \nadd_fam_col(train)\nadd_fam_col(test)","bc2ca904":"train.head(2)","a36badf2":"df = pd.concat([train, test], join='outer')\ndf.shape","edbf24a1":"df.columns","6c1cf236":"\nuseless_feats = ['PassengerId', 'Survived', 'Name', 'SibSp', \n                 'Parch', 'Ticket', 'Cabin', 'Embarked']\ndf.drop(useless_feats, axis=1,inplace=True)\ndf.head(2)","3a3bda4e":"df = pd.get_dummies(df, drop_first=True)\ndf.head(1)","786224ce":"X = df[:train.shape[0]]\ntst = df[train.shape[0]:]\ny = train['Survived']\nprint(X.shape, tst.shape, y.shape)\nX.head(2)","68300438":"from sklearn.preprocessing import scale\nX = scale(X)\ntst = scale(tst)","81417367":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nparameter_grid = {\n                 'max_depth' : [None, 10, 12, 20],\n                 'n_estimators': [None, 50, 10],\n                 'max_features': [None, 'sqrt'],\n                 'min_samples_split': [None, 2, 3, 10],\n                 'min_samples_leaf': [None, 1, 3, 10],\n                 'bootstrap': [True, False],\n                 }\n\nforest = RandomForestClassifier(n_jobs=2)\n\nM1 = GridSearchCV(forest,\n                           scoring='accuracy',\n                           param_grid=parameter_grid,\n                           cv=3,\n                           n_jobs=-1,\n                           verbose=1)\n\nM1.fit(X, y)\n\nparameters = M1.best_params_\nprint('Best score: ' , M1.best_score_ * 100)\nprint('Best estimator: ' , M1.best_estimator_)\n","177efb9a":"from sklearn.svm import SVC\n\n\nparameter_grid = {\n                 'kernel': [None, 'rbf', 'sigmoid', 'linear'],\n                    'C'  : [0,0.25,0.5,1,2,3,4],\n                 'gamma' : [None, 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 5, 20, 50],\n                  'class_weight' : [None, 'balanced']\n    \n                 }\n\nM2 = GridSearchCV(SVC(),\n                      scoring = 'accuracy',\n                      cv=3,\n                      param_grid= parameter_grid,\n                      n_jobs=-1,\n                      verbose=1\n                     ).fit(X,y)\n\nM2.fit(X,y)\n\nprint('Best score: ', M2.best_score_ * 100)\nprint('Best estimator: ', M2.best_estimator_)\n","abd19818":"from sklearn.neighbors import KNeighborsClassifier\n\ngrid = { 'n_neighbors': [1, 5, 15, 25, 30, 40],\n        'weights': ['distance'],\n        'leaf_size': [None, 1, 3, 10, 25, 40],\n        'p':[1,2]\n       }\n\n\nM3 = GridSearchCV(KNeighborsClassifier(),\n                      scoring='accuracy',\n                      cv=3,\n                      param_grid=grid,\n                      n_jobs=-1,\n                      verbose=1\n)\n\nM3.fit(X, y)\n\nprint('Best score: ', M3.best_score_ * 100)\nprint('Best estimator: ', M3.best_estimator_)\n","052f8776":"from xgboost import XGBClassifier\n\nparameter_grid = {\n                 'max_depth' : [None, 5, 7, 10 ],\n                 'max_delta_step': [None, 1, 2],\n                 'n_estimators': [None,10, 20, 30, 40],\n                 'colsample_bylevel': [None,0.2, 0.5, 0.8],\n                 'colsample_bytree': [None,0.2, 0.6],\n                 'subsample': [None,0.01,0.1, 0.3, 0.4,1],\n                 }\n\nM4 = GridSearchCV(XGBClassifier(),\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=3,\n                               n_jobs=-1,\n                               verbose=1)\n\nM4.fit(X, y)\n\nprint('Best score: ', M4.best_score_ * 100)\nprint('Best estimator: ', M4.best_estimator_)\n","9e8a076f":"from sklearn.linear_model import LogisticRegression\n\nparameter_grid = {\n    'C' : [0.01, 0.1, 0.5, 1, 2],\n    'penalty' : ['l1', 'l2', 'elasticnet'],\n    'class_weight' : [None, 'balanced'],\n    'solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n    'max_iter' : [100, 500, 1000]\n\n}\n\nM5 = GridSearchCV(LogisticRegression(),\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=3,\n                               n_jobs=-1,\n                               verbose=1)\n\nM5.fit(X, y)\n\nprint('Best score: ', M5.best_score_ * 100)\nprint('Best estimator: ', M5.best_estimator_)","9f4a44df":"from sklearn.ensemble import BaggingClassifier\n\nparameter_grid = {\n    'base_estimator': [LogisticRegression(), SVC()],\n    'n_estimators' : [10, 20, 30, 40],\n}\n\nM6 = GridSearchCV(BaggingClassifier(),\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=3,\n                               n_jobs=-1,\n                               verbose=1)\n\nM6.fit(X, y)\n\nprint('Best score: ', M6.best_score_ * 100)\nprint('Best estimator: ', M6.best_estimator_)\n","1e70e3e7":"temp = pd.read_csv('..\/input\/titanic-leaked\/titanic.csv')\noriginal = temp['Survived']","7900c582":"from sklearn.metrics import accuracy_score\n\npreds_M1 = M1.predict(tst)\npreds_M2 = M2.predict(tst)\npreds_M3 = M3.predict(tst)\npreds_M4 = M4.predict(tst)\npreds_M5 = M5.predict(tst)\npreds_M6 = M6.predict(tst)\n\n\nscore_M1 = accuracy_score(original, preds_M1)\nscore_M2 = accuracy_score(original, preds_M2)\nscore_M3 = accuracy_score(original, preds_M3)\nscore_M4 = accuracy_score(original, preds_M4)\nscore_M5 = accuracy_score(original, preds_M5)\nscore_M6 = accuracy_score(original, preds_M6)\n\nprint('score with model 1: ', score_M1*100)\nprint('score with model 2: ', score_M2*100)\nprint('score with model 3: ', score_M3*100)\nprint('score with model 4: ', score_M4*100)\nprint('score with model 5: ', score_M5*100)\nprint('score with model 6: ', score_M6*100)","5ccc3e61":"subm_dir = kaggle_path + 'gender_submission.csv'\nsubmit_file = pd.read_csv(subm_dir)\nsubmit_file['Survived'] = preds_M6\nsubmit_file.to_csv('gender_submission.csv', index=False)","f0b7e96a":"If you take a good look at names, every name tells us 3 things:\n1. The family name\n2. The title of name such as Mr. Mrs. Miss. etc.\n3. The actual name of person\n","e7f1cdb6":"# *Creating An Additional Family Feature*","cf48c3b0":"To score higher in kaggle competitions it usually is the case that final model is derived out of multiple previous models.\nI will try to make a boosting model by using Logistic Regression and SVM","08e25b89":"Note that we should always check train and test data sets simultaneously to gain more insights and avoid inconsistency issues later on.","754521ca":"Thing is that I kinda discovered the original outcome of test dataset :p <br>\nSo I will just compare my output with that instead of submitting the file everytime. <br> <br>\n*I hope that is legal because the results have been out there for a long time and they would have been removed, I meand Kaggle Competition holders must have saw that. Besides it is not a serious competition so I guess it is fine at the end. :)*\n\n(If you choose to upload that document straight up, woe to you! I mean what's the purpose of this competition then.)","186c6217":"# 5. Logistic Regression","e65fbc46":"There was an empty 'Fare' value in test. I could have put average of fare or mode of fare but I decided to be smart about it! (I guess)","42f44be3":"# Loading Data And Exploring Data","7be76575":"Now we should be having a cool nice dictionary of titles and their average ages.","e5e304cf":"Let's drop useless features","1216cb30":"We have two files basically.\n1. Data of people who boarded titanic and outcome of wether they survived or not. (train data)\n2. Data of people who boarded titanic and task of this competition is to predict their outcome i.e wether they survived or not.","2d03ba05":"Now let's try to compute the mean age of Names with titles who do not have missing ages.\n\nThis fancy code computes average age of various titles from both train and test datasets.","c4836f7c":"*(Observe that there is no empty 'Fare' value in train dataset. If we had not explored test set and had made model without checking test set, it would have thrown error when evaluating the test set.)*","3715f24f":"<big> The Missing Data: <\/big>","a891271a":"# Merging train and test datasets","c98c84f9":"The name of N\/A(empty) values in our data has these titles:\n1. Mr.  \n2. Mrs.\n3. Miss.\n4. Master. \n5. Dr.\n\nMaster is fancy way of calling little boys who are not worthy of being called a Mister yet :) ","280b0e9e":"# Predicting Test Data","17cf17e2":"Let's fill the 'Age' of missing people with missing 'Age'. This fancy function will do that job.","bc43cc22":"And time to scale features to get their values as less as possible. This will speed up and improve the modeling process!","3780abf7":"A cool family feature has been added!","c05c0092":"It is integral to merge both train and test datasets before feeding input to the model","377b3ad4":"Hopefully rest of the score would be attained by using Cabin and Embarked features accurately.\nBut I got lower score with them included so I dropped them and got better results.","d00b4b6e":"# 1. Random Forest","70c48cfc":"(first of all it is not that great feature analysis but still it will help create a good consisten model!!)","3b41dd36":"Time to get prepare data for our model.","e1342837":"# **Creating Classification Models**","2b64e1a0":"# Feature Engineering","9823abd4":"Cheers! Have a nice day!","23e3edd7":"Observe that every Name tells us their family names.\n\nA thought:\n- Rich families would have had more influence and thus they had higher chances of survival\n- Similarly poor families would have had less chances of survival comparatively\n\nSo it can be helpful to add a feature of Family that tell which Family does one belong to.","e26cc0d0":"Sweet!","efaee4c9":"Also please do give advices to improve this model especially about the Cabin feature <br>\nIf you think I did any screw up, let me know about that too hehe.","d7833d84":"No one else with that ticket.\n\nLet's just fill the Fare with mean based on his Pclass, Age, Embarked port, SibSp and Parch","38b0786e":"1. It seems like the 'Cabin' column has immense number of empty values (but dont worry we will just drop it at the end :D )\n2. The 'Age' column is another issue to deal with as it has huge amount of N\/As as well\n3. There's is one row empty 'Fare' value in test data. \n\nLet's see what we can do about this.","b2ecf164":"# 3. K Nearest Neighbors","507cf180":"# *Exploring names:*","d06b5864":"Wasn't so much of a special kernel but considering i did most of the stuff in it on my own and managed to end up in top 17% is a very good thing for me and I am very happy with it. Been only a month since I got into Machine Learing.","961935ea":"Bruh this is seriously unexpected. <br>\nThe boosted model(BaggingClassifier) got 82% acuracy on test dataset!<br>\nBeginner's luck I must say!","cb25feaa":"*This fancy function pulls Family name from names of people and makes a new Family column*","9e34f918":"# 4. XGB Classifier","6413344c":"I will use GridSearchCV which is a sweet feature you get from Sklearn. It basically allows you to run a model with different hyperparameters in a fast efficient way.","43279127":"(This can be verified by this code)","fac0a712":"Let's convert non-numeric features to categorical values.","719190f1":"# *Filling Fare*","374139b8":"# 6. Boosting Classifier","82ac11a3":"**Finally!!**\n\n*We will try different models and see which one works best.*","a00748d0":"Let's see if the 'Age' was filled or not.","c3601b11":"# 2. Support Vector Classifier (SVM)"}}