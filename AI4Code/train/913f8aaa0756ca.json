{"cell_type":{"277536fc":"code","f61db4f5":"code","0f514354":"code","168049a5":"code","5068eb04":"code","96c5d826":"code","b634e0f1":"code","9a6d5e9b":"code","924b713e":"code","037cd255":"code","92cf3456":"code","c19ab81c":"code","4040bb09":"code","e336d7b8":"code","4e7c6606":"code","3efaf529":"code","6f3503da":"code","e8c227a1":"code","6c6d77ba":"code","35473d20":"code","4142d136":"markdown","6ac3d70f":"markdown","0fe19878":"markdown","915f807e":"markdown","ecf23b7a":"markdown","00257856":"markdown","db919e0a":"markdown","a9c7cb13":"markdown","8d2c7324":"markdown","25f991a6":"markdown","06268235":"markdown","96effbc3":"markdown","705b61c8":"markdown"},"source":{"277536fc":"import numpy as np\nimport regex as re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nos.environ['WANDB_API_KEY'] = '0' # to silence warning\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\nfrom transformers import *\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f61db4f5":"DATA_PATH = '\/kaggle\/input\/contradictory-my-dear-watson-eng\/'\nMODEL_PATH = '\/kaggle\/input\/tf-roberta\/'","0f514354":"for dirname, _, filenames in os.walk(DATA_PATH):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","168049a5":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)","5068eb04":"train = pd.read_csv(DATA_PATH + '\/trans_train.csv', index_col=False)\ntest = pd.read_csv(DATA_PATH + '\/trans_test.csv', index_col=False)","96c5d826":"def basic_clean(row):\n    row['hypothesis'] = re.sub(' +', ' ', row['hypothesis']).strip().lower()\n    row['premise'] = re.sub(' +', ' ', row['premise']).strip().lower()\n    return row\n\n# Remove NaN rows\ntrain = train[train['hypothesis'].notna()]\ntrain = train[train['premise'].notna()]\ntest = test[test['hypothesis'].notna()]\ntest = test[test['premise'].notna()]\n\n# Remove double spaces and starting\/ending as well\ntrain = train.apply(basic_clean, axis=1).reset_index()\ntest = test.apply(basic_clean, axis=1).reset_index()","b634e0f1":"MAX_LEN = 256\n\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=MODEL_PATH + 'vocab-roberta-base.json', \n    merges_file=MODEL_PATH + 'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)","9a6d5e9b":"def encode_sentence(s):\n    return tokenizer.encode(s)","924b713e":"encode_sentence('I love machine learning')","037cd255":"def roberta_encode(df, tokenizer):\n    ct = df.shape[0]\n    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32')\n\n    for k, row in df.iterrows():\n        enc_hypothesis = tokenizer.encode(row['hypothesis'])\n        enc_premise = tokenizer.encode(row['premise'])\n        \n        input_length = len(enc_hypothesis.ids) + len(enc_premise.ids) + 4\n        if input_length > MAX_LEN:\n            continue\n        \n        input_ids[k,:input_length] = [0] + enc_hypothesis.ids + [2,2] + enc_premise.ids + [2]\n        \n        attention_mask[k,:input_length] = 1\n        \n        type_sep = np.zeros_like([0])\n        type_s1 = np.zeros_like(enc_hypothesis.ids)\n        type_s2 = np.ones_like(enc_premise.ids)\n        \n        z = [type_sep, type_s1, type_sep, type_sep, type_s2, type_sep]\n        token_type_ids[k,:input_length] = np.concatenate(z)\n\n    return {\n        'input_word_ids': input_ids,\n        'input_mask': attention_mask,\n        'input_type_ids': token_type_ids\n    }","92cf3456":"train_input = roberta_encode(train, tokenizer)\ntest_input = roberta_encode(test, tokenizer)","c19ab81c":"def build_model():\n    input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n    input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n    input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n\n    config = RobertaConfig.from_pretrained(MODEL_PATH + 'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(MODEL_PATH + 'pretrained-roberta-base.h5', config=config)\n    x = bert_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n    \n    # Huggingface transformers have multiple outputs, embeddings are the first one\n    # let's slice out the first position\n    x = x[0]\n    \n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.Conv1D(1, 1)(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(3, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy'])\n    \n    return model","4040bb09":"with strategy.scope():\n    model = build_model()\n    model.summary()","e336d7b8":"accuracy = []\nhistory = []\n\nVER = 'v0'\nEPOCHS = 4\nKFOLDS = 5\n\n# Our batch size will depend on number of replicas\nBATCH_SIZE= 16 * strategy.num_replicas_in_sync\n\npred_test = np.zeros((test.shape[0], 3))\n\nskf = StratifiedKFold(n_splits=KFOLDS, shuffle=True, random_state=42)\nfor fold, (idxT, idxV) in enumerate(skf.split(train_input['input_word_ids'], train.label.values)):\n    print('#' * 25)\n    print('# FOLD %i' % (fold + 1))\n    print('#' * 25)\n    \n    K.clear_session()\n    with strategy.scope():\n        print('Building model...')\n        model = build_model()\n        \n        sv = tf.keras.callbacks.ModelCheckpoint(\n            '%s-roberta-%i.h5' % (VER, fold),\n            monitor='val_loss',\n            verbose=1,\n            save_best_only=True,\n            save_weights_only=True,\n            mode='auto',\n            save_freq='epoch')\n\n        kfold_train_input = {\n            'input_word_ids': train_input['input_word_ids'][idxT,],\n            'input_mask': train_input['input_mask'][idxT,],\n            'input_type_ids': train_input['input_type_ids'][idxT,]}\n        kfold_train_output = train.label.values[idxT,]\n        \n        kfold_val_input = {\n            'input_word_ids': train_input['input_word_ids'][idxV,],\n            'input_mask': train_input['input_mask'][idxV,],\n            'input_type_ids': train_input['input_type_ids'][idxV,]}\n        kfold_val_output = train.label.values[idxV,]\n\n        print('Training...')\n        kfold_history = model.fit(kfold_train_input,\n                                  kfold_train_output,\n                                  epochs=EPOCHS,\n                                  batch_size=BATCH_SIZE,\n                                  verbose=1,\n                                  callbacks=[sv],\n                                  validation_data=(kfold_val_input, kfold_val_output))\n        history.append(kfold_history)\n\n        print('Loading model...')\n        model.load_weights('%s-roberta-%i.h5' % (VER, fold))\n\n        # Compute prediction for this fold\n        print('Predicting Test...')\n        pred_test += model.predict(test_input) \/ skf.n_splits\n\n        # Display fold accuracy\n        print('Predicting OOF...')\n        oof = [np.argmax(i) for i in model.predict(kfold_val_input)]\n        kfold_accuracy = accuracy_score(oof, kfold_val_output)\n        accuracy.append(kfold_accuracy)\n        print('> FOLD %i - Accuracy: %.4f' % (fold + 1, kfold_accuracy))\n        print()","4e7c6606":"print('> OVERALL KFold CV Accuracy: %.4f' % np.mean(accuracy))","3efaf529":"plt.figure(figsize=(10, 10))\nplt.title('Accuracy')\n\nfor i, hist in enumerate(history):\n    xaxis = np.arange(len(hist.history['accuracy']))\n    plt.subplot(3, 2, i + 1)\n    plt.plot(xaxis, hist.history['accuracy'], label='Train set')\n    plt.plot(xaxis, hist.history['val_accuracy'], label='Validation set')\n    plt.gca().title.set_text('Fold %d accuracy curve' % (i + 1))\n    plt.legend()","6f3503da":"plt.figure(figsize=(10, 10))\nplt.title('Loss')\n\nfor i, hist in enumerate(history):\n    xaxis = np.arange(len(hist.history['accuracy']))\n    plt.subplot(3, 2, i + 1)\n    plt.plot(xaxis, hist.history['loss'], label='Train set')\n    plt.plot(xaxis, hist.history['val_loss'], label='Validation set')\n    plt.gca().title.set_text('Fold %d loss curve' % (i + 1))\n    plt.legend()","e8c227a1":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = [np.argmax(i) for i in pred_test]","6c6d77ba":"submission.head()","35473d20":"submission.to_csv('submission.csv', index=False)","4142d136":"# Evaluation","6ac3d70f":"# Prepare data","0fe19878":"We train with 5 Stratified KFolds. Each fold, the best model weights are saved and then reloaded before oof prediction and test prediction. Therefore you can run this code offline and upload your K-Fold models to a private Kaggle dataset. Then run this notebook and comment out the line `model.fit()`. Instead your notebook will load your model weights from offline training in the line `model.load_weights()`. Update this to have the correct path. Also make sure you change the KFold seed below to match your offline training. Then this notebook will proceed to use your offline models to predict oof and predict test.","915f807e":"# Generating & Submitting Predictions","ecf23b7a":"# Create and train model","00257856":"Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the premise and the hypothesis ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\nIn this notebook, we'll look at the \"*Contradictory, My Dear Watson*\" competition dataset, clean\/translate the data and build a preliminary model using RoBERTa.","db919e0a":"We use a pretrained roBERTa base model and add a custom question answer head. First tokens are input into `bert_model` and we use BERT's first output, i.e. `x[0]` below. These are embeddings of all input tokens and have shape `(batch_size, MAX_LEN, 768)`. Next we apply `tf.keras.layers.Conv1D(filters=1, kernel_size=1)` and transform the embeddings into shape `(batch_size, MAX_LEN, 1)`. We then flatten this and apply softmax, so our final output from `x` has shape `(batch_size, 3)`. These are one hot encodings of the target class.","a9c7cb13":"Let's set up our TPU:","8d2c7324":"RoBERTa uses three kind of input data: input word IDs, input masks, and input type IDs. These allow the model to know that the premise and hypothesis are distinct sentences, and also to ignore any padding from the tokenizer.\n\nNow, we're going to encode all of our premise\/hypothesis pairs for input into RoBERTa.","25f991a6":"The submission file will consist of the ID column and a prediction column. We can just copy the ID column from the test file, make it a dataframe, and then add our prediction column.","06268235":"On this notebook, I have used code from other notebooks to clean the data (spellchecking), translate it into English and build a RoBERTa model. The notebooks are:\n- https:\/\/www.kaggle.com\/anasofiauzsoy\/tutorial-notebook\n- https:\/\/www.kaggle.com\/rli596\/contradictory-my-dear-watson-data-aug\n- https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705\/comments\n\nI did the spellchecking+translation process on my own computer, so here I am going to use the *processed dataset*.","96effbc3":"Tokenizers turn sequences of words into arrays of numbers. Let's look at an example:","705b61c8":"# Introduction"}}