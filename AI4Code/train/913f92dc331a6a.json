{"cell_type":{"e4c0caba":"code","eb694ccd":"code","f000bfe9":"code","425bc41b":"code","bc76775a":"code","e76cda7a":"code","cb318889":"code","a3200b04":"code","f5b503de":"code","87c1cf3a":"code","3189799a":"code","4d500ac9":"code","edf11177":"code","eaa8f8ae":"code","9d51ea66":"code","339c845e":"code","57cfd241":"code","a950bf6f":"code","0867dea3":"code","bd675040":"code","c5fac7e8":"code","c4f5aa63":"code","414fa0cb":"code","c72173a5":"code","290291e4":"code","88e4c503":"code","201b8c6f":"code","51ffeb72":"code","ce7cd7a6":"code","dbd1decd":"code","14b000d1":"code","1303a68f":"code","a8db9f34":"code","8c97a71f":"code","db884b80":"markdown","77263be0":"markdown","c1b7e3c6":"markdown"},"source":{"e4c0caba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb694ccd":"df=pd.read_csv(\"..\/input\/capstone-project-rg\/job posting dataset.csv\")","f000bfe9":"df.sample(10)","425bc41b":"df.shape","bc76775a":"df.columns","e76cda7a":"df.info()","cb318889":"rws = df.job_description ","a3200b04":"df.info(memory_usage='deep')","f5b503de":"df.job_description[0:10]","87c1cf3a":"from sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split \nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk import sent_tokenize, word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nimport re ","3189799a":"count_vec=CountVectorizer()\nX=df.job_title  \nY=df.job_description\n","4d500ac9":"print(X.head())\nprint(Y.head())","edf11177":"X_train, X_test, Y_train, Y_test= train_test_split(X,Y,test_size=0.5)\ncount_vec=CountVectorizer(stop_words='english').fit(X_train)\ndf_train=pd.DataFrame(count_vec.transform(X_train).todense(),\n                     columns=count_vec.get_feature_names())\ndf_test=pd.DataFrame(count_vec.transform(X_test).todense(), columns=count_vec.get_feature_names())\nprint (df_train.shape)\nprint (df_test.shape)\nprint (Y_train.shape)\nprint (Y_test.shape)","eaa8f8ae":"import gc\ngc.collect()","9d51ea66":"df[\"job_description\"] = df[\"job_description\"].str.lower()\ndf.head()","339c845e":"import string\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndf[\"text_wo_punct\"] = df[\"job_description\"].apply(lambda text: remove_punctuation(text))\ndf.head()","57cfd241":"df[\"text_wo_punct\"]","a950bf6f":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","0867dea3":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndf[\"text_wo_stop\"] = df[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\ndf.head()","bd675040":"from collections import Counter\ncnt = Counter()\nfor text in df[\"text_wo_stop\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)","c5fac7e8":"FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndf[\"text_wo_stopfreq\"] = df[\"text_wo_stop\"].apply(lambda text: remove_freqwords(text))\ndf.head()","c4f5aa63":"# Drop the two columns which are no more needed \ndf.drop([\"text_wo_punct\", \"text_wo_stop\"], axis=1, inplace=True)\n\nn_rare_words = 10\nRAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\ndef remove_rarewords(text):\n    \"\"\"custom function to remove the rare words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n\ndf[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\ndf.head()","414fa0cb":"from nltk.stem.porter import PorterStemmer\n\n# Drop the two columns \ndf.drop([\"text_wo_stopfreq\", \"text_wo_stopfreqrare\"], axis=1, inplace=True) \n\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ndf[\"text_stemmed\"] = df[\"job_description\"].apply(lambda text: stem_words(text))\ndf.head()","c72173a5":"df.text_stemmed","290291e4":"from nltk.stem.snowball import SnowballStemmer\nSnowballStemmer.languages","88e4c503":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ndf[\"text_lemmatized\"] = df[\"job_description\"].apply(lambda text: lemmatize_words(text))\ndf.head()","201b8c6f":"df.text_lemmatized","51ffeb72":"import re as re\ndef remove_tags(string):\n    result = re.sub('<.*?>','',string)\n    return result\ndf['with_out_tags']=df['text_lemmatized'].apply(lambda cw : remove_tags(cw))\nprint(\"\\nSentences without tags':\")\ndf.head()","ce7cd7a6":"df.with_out_tags.sample(10)","dbd1decd":"from tensorflow.keras.preprocessing.text import Tokenizer \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords   \nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\npd.set_option(\"display.max_colwidth\", 200)\nwarnings.filterwarnings(\"ignore\")","14b000d1":"import matplotlib.pyplot as plt\ntext_word_count = []\nsummary_word_count = []\n\n# populate the lists with sentence lengths\nfor i in df['with_out_tags']:\n      text_word_count.append(len(i.split()))\n\nfor i in df['job_title']:\n     summary_word_count.append(len(i.split()))\n\nlength_df = pd.DataFrame({'text':text_word_count,'summary': summary_word_count })\n#length_df2 = pd.DataFrame({'summary': summary_word_count})\nlength_df.hist(bins = 30)\n#length_df2.hist(bins = 30)\nplt.show()","1303a68f":"max_len_text=80 \nmax_len_summary=10","a8db9f34":"from sklearn.model_selection import train_test_split\nx_tr,x_val,y_tr,y_val=train_test_split(df['job_title'],df['with_out_tags'],test_size=0.1,random_state=0,shuffle=True)","8c97a71f":"#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer()\nx_tokenizer.fit_on_texts(list(x_tr))\n\n#convert text sequences into integer sequences\nx_tr    =   x_tokenizer.texts_to_sequences(x_tr) \nx_val   =   x_tokenizer.texts_to_sequences(x_val)\n\n#padding zero upto maximum length\nx_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \nx_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n\nx_voc_size   =  len(x_tokenizer.word_index) +1","db884b80":"## Frequent Words ","77263be0":"## Preprocessing ","c1b7e3c6":"## Stop Words"}}