{"cell_type":{"c7dec2d4":"code","5a5f1480":"code","3c50afdf":"code","0bd18320":"code","b59addbe":"code","fe8dd27c":"code","84e4bb53":"code","1c01b163":"code","5b33bc56":"code","b7c5f57c":"code","778eb350":"code","1610db16":"code","e7eecca7":"code","5b3a9105":"code","7ecfeb42":"code","7e69be48":"code","3748cb3f":"code","ffe9c868":"code","c3948a0c":"code","f7bfa329":"code","8e551845":"code","d417f595":"code","e208cc17":"code","6d904249":"code","57cb6cc0":"code","e3f7c843":"markdown","41f72d0c":"markdown","5c344b66":"markdown","4820dc0a":"markdown","04805bc2":"markdown","615fe8d3":"markdown","e507f4b6":"markdown"},"source":{"c7dec2d4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5a5f1480":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom keras.callbacks import ReduceLROnPlateau","3c50afdf":"train_df = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\")\ntest_df = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")","0bd18320":"test = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")\ny = test['label']","b59addbe":"train_df.head()","fe8dd27c":"plt.figure(figsize = (10,10)) # Label Count\nsns.set_style(\"darkgrid\")\nsns.countplot(train_df['label'])","84e4bb53":"y_train = train_df['label']\ny_test = test_df['label']\ndel train_df['label']\ndel test_df['label']","1c01b163":"from sklearn.preprocessing import LabelBinarizer\nlabel_binarizer = LabelBinarizer()\ny_train = label_binarizer.fit_transform(y_train)\ny_test = label_binarizer.fit_transform(y_test)","5b33bc56":"x_train = train_df.values\nx_test = test_df.values","b7c5f57c":"# Normalize the data\nx_train = x_train \/ 255\nx_test = x_test \/ 255","778eb350":"# Reshaping the data from 1-D to 3-D as required through input by CNN's\nx_train = x_train.reshape(-1,28,28,1)\nx_test = x_test.reshape(-1,28,28,1)","1610db16":"f, ax = plt.subplots(2,5) \nf.set_size_inches(10, 10)\nk = 0\nfor i in range(2):\n    for j in range(5):\n        ax[i,j].imshow(x_train[k].reshape(28, 28) , cmap = \"gray\")\n        k += 1\n    plt.tight_layout()    ","e7eecca7":"# With data augmentation to prevent overfitting\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","5b3a9105":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","7ecfeb42":"model = Sequential()\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'softplus' , input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'softplus'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Flatten())\nmodel.add(Dense(units = 24 , activation = 'softmax'))\n\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","7e69be48":"history = model.fit(datagen.flow(x_train,y_train, batch_size = 128) ,epochs = 10 , validation_data = (x_test, y_test) , callbacks = [learning_rate_reduction])","3748cb3f":"print(\"Accuracy of the model is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","ffe9c868":"epochs = [i for i in range(20)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(16,9)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Validation Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'r-o' , label = 'Testing Loss')\nax[1].set_title('Testing Accuracy & Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","c3948a0c":"predictions = model.predict_classes(x_test)\nfor i in range(len(predictions)):\n    if(predictions[i] >= 9):\n        predictions[i] += 1\npredictions[:5]        ","f7bfa329":"classes = [\"Class \" + str(i) for i in range(25) if i != 9]\nprint(classification_report(y, predictions, target_names = classes))","8e551845":"cm = confusion_matrix(y,predictions)","d417f595":"cm = pd.DataFrame(cm , index = [i for i in range(25) if i != 9] , columns = [i for i in range(25) if i != 9])","e208cc17":"plt.figure(figsize = (15,15))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","6d904249":"correct = np.nonzero(predictions == y)[0]","57cb6cc0":"i = 0\nfor c in correct[:6]:\n    plt.subplot(3,2,i+1)\n    plt.imshow(x_test[c].reshape(28,28), cmap=\"gray\", interpolation='none')\n    plt.title(\"Predicted Class {},Actual Class {}\".format(predictions[c], y[c]))\n    plt.tight_layout()\n    i += 1","e3f7c843":"**The dataset seems balanced as for each training label , enough training examples exist**","41f72d0c":"**We perform a grayscale normalization to reduce the effect of illumination's differences.Moreover the CNN converges faster on [0..1] data than on [0..255].**","5c344b66":"**Preview of first 10 images**","4820dc0a":"# Loading the ASL dataset","04805bc2":"## CNN Activation\nIn this book, I design and test all the experimental CNN models. To design my models, I use the python programming language. To perform the experimental review I use three different datasets, those are- Sign Language MNIST, Digit-Recognizer MNIST, and Skin Cancer MNIST. I use 10 epochs for the experimental fold. For the experimental purpose, I use one fold for each model. In this experiment, I used a total of twenty-seven similar types of CNN models. In this book, I analyze the performances of the CNN models for the different activation functions. After analyzing the results, I recommend the activation functions based on the performances for the convolutional layer in neural networks. The goal of this experiment is to help the developer to get a clear concept of the activation functions for the convolutional layers of the networks.","615fe8d3":"**Some of the Correctly Predicted Classes**","e507f4b6":"# Data Visualization and Preprocessing"}}