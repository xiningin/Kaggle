{"cell_type":{"c3e72c94":"code","1de8d074":"code","08ce63f0":"code","c086a4eb":"code","a702a367":"code","1c4ef3fa":"code","4293458f":"code","fc9e5f5a":"code","d6dc98e1":"code","b90f1fae":"code","b2d39297":"code","4afbc160":"code","7da3a97f":"code","9bad3b65":"code","e235949e":"code","ef95f799":"code","71edc75f":"code","70914ffc":"code","a72d7dce":"code","a560568a":"code","ea70a857":"code","3691c6ae":"code","39e19567":"code","a69c0ed4":"code","f1409c10":"code","4dfad4d1":"code","9918333c":"code","c69a97fe":"code","950e5f1f":"code","64f9e28a":"code","bf554895":"code","87a62897":"code","29d4f679":"code","1dd11680":"code","c704e32c":"code","612be9fa":"code","833626ac":"markdown","d29fd3fe":"markdown","3c8a193a":"markdown","9df248ac":"markdown","f5e38675":"markdown","c7f3bc24":"markdown","67e0a427":"markdown","fd279177":"markdown","49313de1":"markdown","aacb1d24":"markdown","9c83e42f":"markdown","55296465":"markdown","665b1633":"markdown","e87bea8e":"markdown","5f9f0ca6":"markdown","08513a7b":"markdown","a41cf4e8":"markdown","b821aba6":"markdown","272018d2":"markdown","d981f7a3":"markdown","26b4fd0c":"markdown","a07a9e5e":"markdown","2c38cf57":"markdown","67c38984":"markdown","40e3825c":"markdown","9f4f3df0":"markdown","ee22da27":"markdown","d02292e1":"markdown","1d0bbf4c":"markdown","fae0c337":"markdown","a04bc1d9":"markdown"},"source":{"c3e72c94":"#Import Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import  RobustScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","1de8d074":"# Setting Options\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)\n\nimport warnings \nfrom warnings import filterwarnings \nfrom sklearn.exceptions import DataConversionWarning \nwarnings.filterwarnings(\"ignore\") \nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)","08ce63f0":"# Importing Dataset\n\ndf=pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf.head()","c086a4eb":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n\n    print(\"##################### Describe #####################\")\n    print(dataframe.describe().T)\n\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)","a702a367":"def grab_col_names(dataframe, cat_th=10, car_th=20): #cat_th ve car_th de\u011ferleri projeye g\u00f6re de\u011fi\u015febilir.\n\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","1c4ef3fa":"for col in cat_cols:\n    print(df.groupby(col).agg({\"Salary\":\"mean\"}))","4293458f":"def cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n\nfor col in cat_cols:\n    cat_summary(df, col)\n","fc9e5f5a":"def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n\nfor col in num_cols:\n    print(col,\"-\", check_outlier(df, col))\nfor col in num_cols:\n    sns.boxplot(x=df[col])\n    plt.show()","d6dc98e1":"clf = LocalOutlierFactor(n_neighbors=5)\nnum_cols2 = [col for col in num_cols if \"Salary\" not in col]\nclf.fit_predict(df[num_cols2])\ndf_scores = clf.negative_outlier_factor_\nnp.sort(df_scores)[0:5]\nscores = pd.DataFrame(np.sort(df_scores))\nscores.plot(stacked=True, xlim=[0, 50], style='.-')\nplt.show()","b90f1fae":"th = np.sort(df_scores)[1]\ndf[df_scores < th]\ndf.drop(df[df_scores < th].index, inplace=True)","b2d39297":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)\n","4afbc160":"df = pd.get_dummies(df, columns=cat_cols, drop_first=True)","7da3a97f":"scaler = RobustScaler()\ndff= pd.DataFrame(scaler.fit_transform(df), columns=df.columns)","9bad3b65":"imputer = KNNImputer(n_neighbors=5)\ndff=pd.DataFrame(imputer.fit_transform(dff), columns=dff.columns)\ndff.head()","e235949e":"dff = pd.DataFrame(scaler.inverse_transform(dff), columns=dff.columns)\n\ndf =dff\ndf.head()\nmissing_values_table(df)","ef95f799":"df.head()","71edc75f":"corr_matrix = df.corr()\nkot = corr_matrix[(corr_matrix>=.75) & (corr_matrix !=1)]\nplt.figure(figsize=(12,8))\nsns.heatmap(kot, cmap=\"Reds\")\nplt.show()\n\ndf.corr().unstack().sort_values().drop_duplicates().sort_values(ascending=False)","70914ffc":"df[\"NEW_CAT_CATBAT\"] = pd.qcut(df[\"CAtBat\"], 3, labels=[\"lowset\",\"midst\",\"atop\"])\ndf[\"NEW_CAT_CWALKS\"] = pd.qcut(df[\"CWalks\"], 3, labels=[\"lowset\",\"midst\",\"atop\"])\ndf[\"NEW_CAT_CRUNS\"] = pd.qcut(df[\"CRuns\"], 3, labels=[\"lowset\",\"midst\",\"atop\"])\ndf[\"NEW_CAT_ASSISTS\"] = pd.qcut(df[\"Assists\"], 3, labels=[\"lowset\",\"midst\",\"atop\"])\ndf[\"NEW_CAT_ERRORS\"] = pd.qcut(df[\"Errors\"], 3, labels=[\"ace\",\"passable\",\"nonstarter\"])\n\ndf.groupby(\"League_N\").agg({\"Salary\":\"mean\"})\ndf.groupby(\"NewLeague_N\").agg({\"Salary\":\"mean\"})","a72d7dce":"df.loc[(df[\"League_N\"]==0) & (df[\"NewLeague_N\"] == 0), \"NEW_SITUATION_BY_TRANSFER\"] = \"ace\"\ndf.loc[(df[\"League_N\"]==1) & (df[\"NewLeague_N\"] == 0), \"NEW_SITUATION_BY_TRANSFER\"] = \"comer\"\ndf.loc[(df[\"League_N\"]==1) & (df[\"NewLeague_N\"] == 1), \"NEW_SITUATION_BY_TRANSFER\"] = \"partial_comer\"\ndf.loc[(df[\"League_N\"]==0) & (df[\"NewLeague_N\"] == 1), \"NEW_SITUATION_BY_TRANSFER\"] = \"nonstarter\"","a560568a":"# The players can make a lot of mistakes, but they can also make a lot of mistakes, let's take the ratio of these two as a variable.\ndf[\"NEW_WALKS_ERRORS\"] = np.where(df[\"Errors\"] == 0, 0, df[\"Walks\"] \/ df[\"Errors\"])","ea70a857":"performans_list_86_87 = df.columns[0:6]\nperformans_list_all = df.columns[7:13]\nfor i in range(0,6):\n    df[f'NEW_{performans_list_86_87[i]}_{performans_list_all[i]}'] = np.where(df[performans_list_all[i]] == 0, 0, df[performans_list_86_87[i]] \/ df[performans_list_all[i]])\n","3691c6ae":"df[\"New_CWalks_mean\"] = df[\"CWalks\"]\/df[\"Years\"]\ndf[\"New_CAtBat_mean\"] = df[\"CAtBat\"]\/df[\"Years\"]\ndf[\"New_CWalks_mean\"] = df[\"CWalks\"]\/df[\"Years\"]\ndf[\"New_CHmRun_mean\"] = df[\"CHmRun\"]\/df[\"Years\"]\ndf[\"New_CRBI_mean\"] = df[\"CRBI\"]\/df[\"Years\"]\ndf[\"New_CWalks_mean\"] = df[\"CWalks\"]\/df[\"Years\"]","39e19567":"# Some Variables Taken From Kaggle:\n\n# New Features https:\/\/www.kaggle.com\/mehmettuzcu\/salary-prediction-models\ndf[\"Hits_Success\"] = (df[\"AtBat\"] - df[\"Hits\"])\ndf[\"NEW_ATBAT_CATBAT_RATE\"] = df[\"AtBat\"] \/ df[\"CAtBat\"]\ndf[\"NEW_HMRUN_RATE\"] = np.where(df[\"CHmRun\"] !=0,df[\"HmRun\"] \/ df[\"CHmRun\"],0 )\ndf[\"NEW_RUN_RATE\"] = np.where(df[\"CRuns\"] !=0,df[\"Runs\"] \/ df[\"CRuns\"],0 )\ndf[\"NEW_RBI_RATE\"] = np.where(df[\"CRBI\"] !=0,df[\"RBI\"] \/ df[\"CRBI\"],0 )\ndf[\"NEW_HITS_RATE\"] = np.where(df[\"CHits\"] !=0,df[\"Hits\"] \/ df[\"CHits\"],0 )\ndf[\"NEW_WALKS_RATE\"] =  np.where(df[\"CWalks\"] !=0,df[\"Walks\"] \/ df[\"CWalks\"],0 )\ndf[\"NEW_WALKS_RATE\"] = np.where(df[\"CAtBat\"] !=0,df[\"CHits\"] \/ df[\"CAtBat\"],0 )\n\ndf[\"NEW_TOTAL_BASES\"] = ((df[\"CHits\"] * 2) + (4 * df[\"CHmRun\"]))\ndf[\"NEW_SLUGGIN_PERCENTAGE\"] = df[\"NEW_TOTAL_BASES\"] \/ df[\"CAtBat\"]\ndf[\"NEW_ISOLETED_POWER\"] = df[\"NEW_SLUGGIN_PERCENTAGE\"] - df[\"NEW_WALKS_RATE\"]\ndf[\"NEW_TRIPLW_CROWN\"] = (df[\"CHmRun\"] * 0.4) + (df[\"CRBI\"] * 0.25) + (df[\"NEW_WALKS_RATE\"] * 0.35)","a69c0ed4":"# https:\/\/www.kaggle.com\/kemalgunay\/salary-prediction-ml-pipeline-main-function\n\ndf[\"NEW_OrtCHits\"] = df[\"CHits\"] \/ df[\"Years\"]\ndf[\"NEW_OrtCruns\"] = df[\"CRuns\"] \/ df[\"Years\"]\ndf[\"NEW_Average\"] = df[\"Hits\"] \/ df[\"AtBat\"]\ndf['NEW_PutOutsYears'] = df['PutOuts'] * df['Years']\ndf[\"NEW_RBIWalksRatio\"] = np.where(df[\"Walks\"] !=0,df[\"RBI\"] \/ df[\"Walks\"],0 )\ndf[\"NEW_CHmRunCAtBatRatio\"] = df[\"CHmRun\"] \/ df[\"CAtBat\"]","f1409c10":"# https:\/\/www.kaggle.com\/ozlemilgun\/salary-prediction-with-hitters-data-set\n\ndf['NEW_HITS_PER_GAME'] = df[\"Hits\"] \/ 9\ndf['NEW_CHITS_PER_GAME'] = df[\"CHits\"] \/ 9\ndf[\"NEW_CWALKS_PER_GAME\"] = df[\"CWalks\"] \/ 9\ndf[\"NEW_WALKS_PER_GAME\"] = df[\"Walks\"] \/ 9\ndf[\"NEW_CHMRUN_PER_GAME\"] = df[\"CHmRun\"] \/ 9\ndf[\"NEW_HMRUN_PER_GAME\"] = df[\"HmRun\"] \/ 9","4dfad4d1":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","9918333c":"df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n\nscaler = RobustScaler()\nnum_cols.remove(\"Salary\")\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\ndf.shape","c69a97fe":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\n\ndef find_optimum_random_state(regressor):\n    dict = {\"random_state\": [], \"rmse\": []}\n    for i in range(1, 200):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=i)\n        model = regressor(random_state=20).fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        dict[\"random_state\"].append(i)\n        dict[\"rmse\"].append(str(np.sqrt(mean_squared_error(y_test, y_pred))))\n        split_df = pd.DataFrame(dict)\n        a = split_df.rmse.min()\n        random_state_1 = split_df.loc[split_df[\"rmse\"] == a, \"random_state\"].to_string(index=False)\n\n\n    dict = {\"random_state\": [], \"rmse\": []}\n    for i in range(1, 200):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=int(random_state_1))\n        model = regressor(random_state=i).fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        dict[\"random_state\"].append(i)\n        dict[\"rmse\"].append(str(np.sqrt(mean_squared_error(y_test, y_pred))))\n        split_df = pd.DataFrame(dict)\n        a = split_df.rmse.min()\n        random_state_2 = split_df.loc[split_df[\"rmse\"] == a, \"random_state\"].to_string(index=False)\n    print(f\"For train_test_split: {(random_state_1)}, for model: {(random_state_2)}\")\n\n\nfind_optimum_random_state(GradientBoostingRegressor) \nfind_optimum_random_state(RandomForestRegressor)","950e5f1f":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\n\nmodels = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))\n          ]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","64f9e28a":"\ngbm_params = {\"max_depth\": [3,4,5],\n             \"subsample\": [1,0.5,0.7],\n             \"learning_rate\": [0.4,0.06,0.08],\n             \"n_estimators\": [200,250,300]}\n\nrf_params = {\"max_depth\": [10, 12,14,  None],\n             \"max_features\": [9,8, \"auto\"],\n             \"min_samples_split\": [4,5],\n             \"n_estimators\": [145,140,150]}\n\n\nlightgbm_params = {\"learning_rate\": [0.1, 0.12,0.14],\n                   \"n_estimators\": [100,150,200],\n                   \"colsample_bytree\": [0.95, 0.9, 1]}\n\n\n\n\nregressors = [(\"GBM\", GradientBoostingRegressor(random_state=79), gbm_params),\n              (\"RF\", RandomForestRegressor(random_state=186), rf_params),\n              ('LightGBM', LGBMRegressor(random_state=43), lightgbm_params)]","bf554895":"best_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=5, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model","87a62897":"voting_reg = VotingRegressor(estimators=[('RF', best_models[\"RF\"]),\n                                         ('GBM', best_models[\"GBM\"])])\n\nvoting_reg.fit(X, y)\n\n\nnp.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=5, scoring=\"neg_mean_squared_error\")))","29d4f679":"lightgbm_final = best_models[\"LightGBM\"].fit(X,y)\ny_pred = pd.DataFrame(lightgbm_final.predict(X), columns=[\"Salary_Pred\"])\ny_real = df.filter([\"Salary\"])\nrmse = np.mean(np.sqrt(-cross_val_score(lightgbm_final, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\ngraph = sns.regplot(x=y_real, y=y_pred, scatter_kws={'color': 'b', 's': 5}, ci=False, color=\"r\")\ngraph.set_title(f\"LightGBM RMSE: {round(rmse, 2)}\")\ngraph.set_xlabel(\"Salary\")\ngraph.set_ylabel(\"Predicted Salary\")\nplt.xlim(0, 1700)\nplt.ylim(bottom=0)\nplt.show()","1dd11680":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\nplot_importance(lightgbm_final, X)","c704e32c":"\nLasso_model = Lasso().fit(X,y)\n\n# Lasso Regression - Hyperparameter Optimization:\n\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n\ncoefs = []\nalphas= 10**np.linspace(10,-2,200)*0.5\nlasso_cv = LassoCV(alphas = alphas,cv = 10,max_iter=100000)\nlasso_cv_model = lasso_cv.fit(X,y)\n\n#Optimum alpha:\n\nlasso_cv_model.alpha_\n\n# Lasso Regression - Tuned Model:\n\nls = Lasso(lasso_cv_model.alpha_)\nlasso_tuned_model = ls.fit(X, y)\n\ny_pred = lasso_tuned_model.predict(X)\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y, y_pred)))\nprint(\"r2 Score:\", r2_score(y, y_pred)) \n\n# 10-Fold Cross-Validation:\n\nprint(\"10 - Fold CV Score:\",\n      np.mean(np.sqrt(-cross_val_score(lasso_tuned_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))) \n","612be9fa":"ridge_model = Ridge().fit(X,y)\n\n# Ridge Regression - Hyperparameter Optimization:\n\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n\ncoefs = []\nalphas= np.random.randint(0,1000,100)\nridge_cv = RidgeCV(alphas = alphas,scoring=\"neg_mean_squared_error\",cv = 10,normalize=True)\nridge_cv_model = ridge_cv.fit(X,y)\n\n#Optimum alpha:\n\nridge_cv_model.alpha_\n\n# Lasso Regression - Tuned Model:\n\nridge_tuned_model = Ridge(alpha=ridge_cv_model.alpha_).fit(X,y)\n\n\ny_pred = ridge_tuned_model.predict(X)\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y, y_pred)))\nprint(\"r2 Score:\", r2_score(y, y_pred)) \n\n# 10-Fold Cross-Validation:\n\nprint(\"10 - Fold CV Score:\",\n      np.mean(np.sqrt(-cross_val_score(lasso_tuned_model, X, y, cv=10, scoring=\"neg_mean_squared_error\"))))\n\n","833626ac":"Let's look at the correlation and examine correlations above 0.5 as there are too many numerical variables.","d29fd3fe":"Set the number of n nearest neighbors to be 5 and run KNN and apply it to dff.","3c8a193a":"**Let's separate the categorical and numeric variables according to our new df.**","9df248ac":"**Others:**\nLasso & Ridge Regression","f5e38675":"There are 59 missing values in our Salary variable.\nLet's fill these missing values with KNN.\nFirst, let's encode our categorical variables.","c7f3bc24":"Let's do an outlier analysis.","67e0a427":"**About Dataset**\n\nThe dataset is part of the data used in the 1988 ASA Graphics Section Poster Session. Salary data originally from Sports Illustrated, April 20, 1987. 1986 and career statistics are from the 1987 Baseball Encyclopedia Update, published by Collier Books, Macmillan Publishing Company, New York.","fd279177":"**Stacking & Ensemble Learning**","49313de1":"**Let's find the optimum random state**\n\nWe will use the random_state argument to always achieve the same result. In the holdout method, I am trying to reach the optimum by trying the random states between 1 and 200 quickly.","aacb1d24":"**Feature Importance**","9c83e42f":"**Let's start.**\nFirst of all, let's start by importing the libraries we will use and make our settings in order to use the kernel\/console comfortably.","55296465":"let's scale our data and save it as dff.","665b1633":"**What did I do and why?**\n\nIt is possible to predict many things with machine learning. House prices, churn rates, disease forecasts are prime examples of this.\nThe variable that is tried to be estimated is dependent, and the variables used in estimation are independent variables. Models with numerical dependent variables are regression models, and models with categorical variables are classification models.\nHitters is an ideal dataset for people who want to practice machine learning because it requires analysis from many points.\nAs a result of the models we will establish, we will try to reach the root mean square error (rmse) value by which we will evaluate our Regression model success by using the relevant dataset.","e87bea8e":"Let's do multivariate outlier analysis with LOF","5f9f0ca6":"**Base Models**","08513a7b":"Let's set some variables to evaluate the performance of the players based on their years in their careers.","a41cf4e8":"**Final Model - LightGBM**","b821aba6":"**Let's examine our categorical variables according to our target variable.**","272018d2":"**Exploratory Data Analysis**","d981f7a3":"In order to observe the values in dff, let's undo the scale operation and assign it to df again.","26b4fd0c":"Let's examine the missing values.","a07a9e5e":"**First Observation:**\n- Our number of observations is very low with 322, it may not be suitable for this data set to apply deletion in outliers and missing values.\n- 3 categorical variables appear, the rest are numerical variables.\n- The missing observation is not visible for now, except for the Salary variable.","2c38cf57":"Let's encode our categorical variables and standardize our numeric variables.","67c38984":"**Let's separate the categorical and numeric variables with grab_col_names.**","40e3825c":"![](https:\/\/www.incimages.com\/uploaded_files\/image\/1920x1080\/getty_482805043_185511.jpg)","9f4f3df0":"The main elbow appears after the 1st index, we can delete this value.","ee22da27":"**Let's derive new variables (Feature Eng)**","d02292e1":"The average salary seems higher in league A than in league N.\nE position salary average is higher than W position salary average.","1d0bbf4c":"The number of players in league A is always higher in late 1986 and early 1987.\nThe number of players in the W position is more than E.","fae0c337":"**Automated Hyperparameter Optimization w\/GridSearchCV**","a04bc1d9":"Variables starting with C in the data set give the total performance of the players in their career.\nThe data for the years 86-87, where the relevant data are available, have a high correlation with the variables expressing career starting with C.\nBy dividing these variables, let's express how the actor's performance that year was compared to his entire career."}}