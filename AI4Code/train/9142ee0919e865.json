{"cell_type":{"8aaaf035":"code","8176c92a":"code","5d15e0b0":"code","ab6ed32f":"code","ffe9f1df":"code","fdaceda4":"code","577d9c7c":"code","36160d73":"code","59683c6f":"code","75f0dbd6":"code","8950ca04":"code","c5a2f863":"code","e9c94200":"code","de1874df":"code","aba95e60":"markdown","8fa33abc":"markdown","81077cd3":"markdown","888a9222":"markdown","07ef7f4b":"markdown","96e73cc0":"markdown","b2406e4b":"markdown","0fc1e7ca":"markdown"},"source":{"8aaaf035":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8176c92a":"import pandas as pd\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nimport torch.utils.data\nimport matplotlib.ticker as ticker\nfrom random import randint\nimport tensorflow as tf ","5d15e0b0":"def load_data(path):\n    df = pd.read_csv(path, header=None)\n    X = df[0].values\n    y = df[1].values\n    x_tok = Tokenizer(char_level=True, filters='')\n    x_tok.fit_on_texts(X)\n    y_tok = Tokenizer(char_level=True, filters='')\n    y_tok.fit_on_texts(y)\n    \n    X = x_tok.texts_to_sequences(X)\n    y = y_tok.texts_to_sequences(y)\n    \n    X = pad_sequences(X)\n    y = np.asarray(y)\n    \n    return X, y, x_tok.word_index, y_tok.word_index\n\nX, y, x_wid, y_wid= load_data('..\/input\/data.csv')\nx_id2w = dict(zip(x_wid.values(), x_wid.keys()))\ny_id2w = dict(zip(y_wid.values(), y_wid.keys()))\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nprint('train size: {} - test size: {}'.format(len(X_train), len(X_test)))","ab6ed32f":"# hidden size for one LSTM model\nhidden_size = 128\nlearning_rate = 0.001\ndecoder_learning_ratio = 0.1\n# Vocabulary Set of input sentences ( + 1 because you need padding characters)\ninput_size = len(x_wid) + 1\n\ninput_size = len(x_wid) + 1\n\n# +2  because you need start character and end character\noutput_size = len(y_wid) + 2\n# 2 characters located at the end\nsos_idx = len(y_wid) \neos_idx = len(y_wid) + 1\n\nmax_length = y.shape[1]\nprint(\"input vocab: {} - output vocab: {} - length of target: {}\".format(input_size, output_size, max_length))","ffe9f1df":"def decoder_sentence(idxs, vocab):\n    text = ''.join([vocab[w] for w in idxs if (w > 0) and (w in vocab)])\n    return text","fdaceda4":"class Encoder(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        # embedding vector of word\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        # GRU Model variant of RNN learns vector representation of sentence\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        \n    def forward(self, input):\n        # Input: S x B\n        embedded = self.embedding(input)\n        output, hidden = self.gru(embedded)\n        return output, hidden # S x B x H, 1 x B x H","577d9c7c":"class Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n    \n    def forward(self, hidden, encoder_outputs):\n        \"\"\" The model receives the current hidden state of the decoder model and hidden states of Encoder Model\n        Encoder_Output: T x B x H\n        Hidden: S x B x H \"\"\"\n        # Tranpose about the correct shape to receive the matrix\n        encoder_outputs = torch.transpose(encoder_outputs, 0, 1) # B x T x H\n        hidden = torch.transpose(torch.transpose(hidden, 0, 1), 1, 2) # B x H x S\n        # Calculating e, is the hidden interaction and the hidden state of the encoder model\n        energies = torch.bmm(encoder_outputs, hidden) # B x Tx S\n        energies = torch.transpose(energies, 1, 2) # B x S x T\n        # Calculating alpha, is the weight of the weighted average should be calculated by the softmax function\n        attn_weights = F.softmax(energies, dim = -1) # B x S x T\n        \n        # Calculating context vector by the weighted average\n        output = torch.bmm(attn_weights, encoder_outputs) # B x S x H\n        # Returns the necessary dimension\n        output = torch.transpose(output, 0, 1) # S x B x H\n        attn_weights = torch.transpose(attn_weights, 0, 1) # S x B x T\n        # Return context vector and alpha weights for demonstration purposes Attention\n        return output, attn_weights","36160d73":"class Decoder(nn.Module):\n    def __init__(self, output_size, hidden_size, dropout):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # vector representation for words of output\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        # define attention model\n        self.attn = Attention(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        # Decoder: GRU Model\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        # Predicting the words at each moment, we join the two hidden and context together\n        self.concat = nn.Linear(self.hidden_size * 2, hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n    \n    def forward(self, input, hidden, encoder_outputs):\n        # input: S x B\n        # encoder_outputs: B x S x H\n        # hidden: 1 x B x H\n        embedded = self.embedding(input) # 1 x B x H\n        embedded = self.dropout(embedded)\n        # Represention of Sentence\n        rnn_output, hidden = self.gru(embedded, hidden) # S x B x H, 1 x B x H\n        # Calculating context vector base on hidden states\n        context, attn_weights = self.attn(rnn_output, encoder_outputs) # S x B x H\n        # Concat hidden state of decoder model current and context vector for predict\n        concat_input = torch.cat((rnn_output, context), -1)\n        concat_output = torch.tanh(self.concat(concat_input))  #SxBxH\n        \n        output = self.out(concat_output) # S x B x output_size\n        return output, hidden, attn_weights","59683c6f":"encoder = Encoder(input_size, hidden_size)\ndecoder = Decoder(output_size, hidden_size, 0.1)\n\n# Initialize optimizers and criterion\nencoder_optimizer = optim.Adam(encoder.parameters(), lr = learning_rate)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr = learning_rate * decoder_learning_ratio)\ncriterion = nn.CrossEntropyLoss()\n\n\ninput_encoder = torch.randint(1, input_size, (34, 6), dtype = torch.long)\nencoder_outputs, hidden = encoder(input_encoder)\ninput_decoder = torch.randint(1, output_size, (10, 6), dtype = torch.long)\noutput, hidden, attn_weights = decoder(input_decoder, hidden, encoder_outputs)","75f0dbd6":"def forward_and_compute_loss(inputs, targets, encoder, decoder, criterion):\n    batch_size = inputs.size()[1]\n    \n    # define two start and end character\n    sos = Variable(torch.ones((1, batch_size), dtype=torch.long)*sos_idx)\n    eos = Variable(torch.ones((1, batch_size), dtype=torch.long)*eos_idx)\n    \n    # input of decoder model need add start character\n    decoder_inputs = torch.cat((sos, targets), dim=0)\n    # output predict of decoder model need add end character\n    decoder_targets = torch.cat((targets, eos), dim=0)\n    \n    # forward calculating hidden states of sentence\n    encoder_outputs, encoder_hidden = encoder(inputs)\n    # Calculating output of decoder model\n    output, hidden, attn_weights = decoder(decoder_inputs, encoder_hidden, encoder_outputs)\n    \n    output = torch.transpose(torch.transpose(output, 0, 1), 1, 2) # BxCxS\n    decoder_targets = torch.transpose(decoder_targets, 0, 1)\n    # Calculating loss \n    loss = criterion(output, decoder_targets)\n    \n    return loss, output\n\ndef train(inputs, targets,  encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n    \n    encoder.train()\n    decoder.train()\n    \n    # zero gradient, must do every time update gradient\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    \n    # Predict of each time the position has the largest prob\n    train_loss, output = forward_and_compute_loss(inputs, targets,encoder, decoder,criterion)    \n    \n    train_loss.backward()\n    # Updating one step\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    \n    # Return Loss for Print\n    return train_loss.item()\n\ndef evaluate(inputs, targets, encoder, decoder, criterion):\n    \n    encoder.eval()\n    decoder.eval()\n    # Calculating loss\n    eval_loss, output = forward_and_compute_loss(inputs, targets, encoder, decoder,criterion)\n    output = torch.transpose(output, 1, 2)\n    \n    # Predict of each time the position has the largest prob\n    pred_idx = torch.argmax(output, dim=-1).squeeze(-1)\n    pred_idx = pred_idx.data.cpu().numpy()\n    \n    # Return Loss and prediction result\n    return eval_loss.item(), pred_idx\n\ndef predict(inputs, encoder, decoder, target_length=max_length):\n    # When predicting we need to calculate the results immediately at each time\n    # then stop the word predicted to be calculating the next word     \n    batch_size = inputs.size()[1]\n    \n    # The first input of the decoder model is the starting character, we predict the next character, \n    # then use this character to predict the next word.\n    decoder_inputs = Variable(torch.ones((1, batch_size), dtype=torch.long)*sos_idx)\n    \n    # Calculating hidden state of encoder model, \n    # As the vector of words, we need to calculate the vector context based on these hidden states\n    encoder_outputs, encoder_hidden = encoder(inputs)\n    hidden = encoder_hidden\n    \n    preds = []\n    attn_weights = []\n    \n    # Calculating Every word at every moment\n    for i in range(target_length):\n        # Predict the first word \n        output, hidden, attn_weight = decoder(decoder_inputs, hidden, encoder_outputs)\n        output = output.squeeze(dim=0)\n        pred_idx = torch.argmax(output, dim=-1)\n        \n        # Change the next input with the word that has been predicted\n        decoder_inputs = Variable(torch.ones((1, batch_size), dtype=torch.long)*pred_idx)\n        preds.append(decoder_inputs)\n        attn_weights.append(attn_weight.detach())\n    \n    preds = torch.cat(preds, dim=0)\n    preds = torch.transpose(preds, 0, 1)\n    attn_weights = torch.cat(attn_weights, dim=0)\n    attn_weights = torch.transpose(attn_weights, 0, 1)\n    return preds, attn_weights","8950ca04":"epochs = 20\nbatch_size = 64\n\nencoder = Encoder(input_size, hidden_size)\ndecoder = Decoder(output_size, hidden_size, 0.1)\n\n# Initialize optimizers and criterion\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\ncriterion = nn.CrossEntropyLoss()\n\nX_val = torch.tensor(X_test, dtype=torch.long)\ny_val = torch.tensor(y_test, dtype=torch.long)\nX_val = torch.transpose(X_val, 0, 1)\ny_val = torch.transpose(y_val, 0, 1)\n\nfor epoch in range(epochs):\n    for idx in range(len(X_train)\/\/batch_size):\n        # input \u0111\u1ea7u v\u00e0o c\u1ee7a ch\u00fang ta l\u00e0 timestep first nh\u00e9. \n        X_train_batch = torch.tensor(X_train[batch_size*idx:batch_size*(idx+1)], dtype=torch.long)\n        y_train_batch = torch.tensor(y_train[batch_size*idx:batch_size*(idx+1)], dtype=torch.long)\n        \n        X_train_batch = torch.transpose(X_train_batch, 0, 1)\n        y_train_batch = torch.transpose(y_train_batch, 0, 1)\n        train_loss= train(X_train_batch, y_train_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n    eval_loss, preds = evaluate(X_val, y_val, encoder, decoder, criterion)\n    \n    print('Epoch {} - train loss: {:.3f} - eval loss: {:.3f}'.format(epoch, train_loss, eval_loss))\n    print_idx = np.random.randint(0, len(preds), 3)\n    for i in print_idx:\n        x_val = decoder_sentence(X_val[:,i].numpy(), x_id2w)\n        y_pred = decoder_sentence(preds[i], y_id2w)\n        print(\" {:<35s}\\t{:>10}\".format(x_val, y_pred))","c5a2f863":"preds, attn_weights = predict(X_val ,encoder, decoder, target_length = 10)","e9c94200":"def show_attention(input_sentence, output_words, attentions):\n    # Set up figure with colorbar\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attentions.numpy(), cmap='bone')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticks(np.arange(len(input_sentence)))\n    ax.set_xticklabels(list(input_sentence), rotation=90)\n    ax.set_yticks(np.arange(len(output_words)))\n    ax.set_yticklabels(list(output_words))\n    ax.grid()\n    ax.set_xlabel('Input Sequence')\n    ax.set_ylabel('Output Sequence')\n    plt.show()","de1874df":"show_idx = randint(0, len(preds))\ntext_x = decoder_sentence(X_val[:,show_idx].numpy(), x_id2w)\ntext_y = decoder_sentence(preds[show_idx].numpy(), y_id2w)\nattn_weight = attn_weights[show_idx, :, -len(text_x):]\nshow_attention(text_x, text_y, attn_weight)","aba95e60":"**Data Preprocessing**","8fa33abc":"**Attention is ALL you NEED**\n\n![image.png](attachment:image.png)","81077cd3":"**Reference:**\n\nhttps:\/\/github.com\/pbcquoc\/attention_tutorial","888a9222":"**Encoder**\n\nThis model receives input as sentences, you can see the hidden state h1, h2, h3, h4 as representations of each word, and want to synthesize the context vector on this information.\n\n**Attention**\n\nThis model learns the alpha weights on h1, h2, h3, h4 and then synthesizes this context vector on this alpha weighted average\n\n**Decoder**\n\nAt the time of prediction, you use additional context vector to add information to the model.","07ef7f4b":"**Define Model**\n\n1. Encoder: LSTM Model used to learn the representation of sentences\n2. Attention used to learn how to combine to create context vector\n3. Decoder: LSTM Model, we will combine context vector into this model to predict the words at each time \n\n![image.png](attachment:image.png)","96e73cc0":"**Training and Evaluating**","b2406e4b":"**Prediction**","0fc1e7ca":"**Define Parameter**"}}