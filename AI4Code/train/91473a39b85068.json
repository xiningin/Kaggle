{"cell_type":{"e5973cd6":"code","47ad7340":"code","4a28eaec":"code","22aa745c":"code","5580e967":"code","b31d1aa9":"code","bc30a04e":"code","2f3ccb02":"code","b441fc69":"code","fb56847c":"code","b84151f3":"code","9a96e210":"code","d457cb34":"code","ca570702":"code","2f4702d7":"code","11414acb":"code","ddd52476":"code","223c06d4":"code","677f4add":"code","5dba1f79":"code","e4faee5f":"code","851c7880":"code","9a32d06b":"code","15bd9286":"code","9877751a":"code","be903a47":"code","9ad62eae":"code","c7f1e9d2":"code","444567dc":"code","bbd10cb5":"code","3a9fe091":"code","a229ae84":"code","e4ec90ca":"code","ebf3b840":"code","d9205d09":"code","b522be33":"code","925f3a3e":"code","cfe079eb":"code","3f70de6a":"code","6814abf8":"code","bcf21df1":"code","b49b477b":"code","cb89a466":"markdown","ea48915f":"markdown","90f88b8d":"markdown","3106b30d":"markdown","b662b9e2":"markdown","c82d0074":"markdown","4e796e12":"markdown","2652500b":"markdown","cc5be0af":"markdown","dae05f7f":"markdown","38f33cd5":"markdown","b1282051":"markdown","0d12452a":"markdown","1678c7f7":"markdown","6b777785":"markdown","c64ea9a8":"markdown","75a5b8ff":"markdown","bb82a2e4":"markdown","7645b6a3":"markdown","e7588d6e":"markdown","648fff43":"markdown","4547f542":"markdown","365d6de3":"markdown","41099de5":"markdown","f164ccb6":"markdown","fb6e42bb":"markdown","ef482da4":"markdown","684b6e7e":"markdown","259242a0":"markdown","5dec5bd3":"markdown","a5aaf86e":"markdown"},"source":{"e5973cd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47ad7340":"import pandas as pd\nimport numpy as np\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#from wordcloud import WordCloud\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,precision_score,recall_score","4a28eaec":"df = pd.read_csv(\"\/kaggle\/input\/facebook-recruiting-iii-keyword-extraction\/Train.zip\")\ndf.head()","22aa745c":"print(\"Dataframe shape : \", df.shape)","5580e967":"df = df.iloc[:300000, :]\nprint(\"Shape of Dataframe after subsetting : \", df.shape)","b31d1aa9":"duplicate_pairs = df.sort_values('Title', ascending=False).duplicated('Title')\nprint(\"Total number of duplicate questions : \", duplicate_pairs.sum())\ndf = df[~duplicate_pairs]\nprint(\"Dataframe shape after duplicate removal : \", df.shape)","bc30a04e":"df[\"tag_count\"] = df[\"Tags\"].apply(lambda x : len(x.split()))","2f3ccb02":"df[\"tag_count\"].value_counts()","b441fc69":"print( \"Maximum number of tags in a question: \", df[\"tag_count\"].max())\nprint( \"Minimum number of tags in a question: \", df[\"tag_count\"].min())\nprint( \"Average number of tags in a question: \", df[\"tag_count\"].mean())","fb56847c":"sns.countplot(df[\"tag_count\"])\nplt.title(\"Number of tags in questions \")\nplt.xlabel(\"Number of Tags\")\nplt.ylabel(\"Frequency\")","b84151f3":"vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\ntag_bow = vectorizer.fit_transform(df['Tags'])","9a96e210":"print(\"Number of questions :\", tag_bow.shape[0])\nprint(\"Number of unique tags :\", tag_bow.shape[1])","d457cb34":"tags = vectorizer.get_feature_names()\nprint(\"Few tags :\", tags[:10])","ca570702":"freq = tag_bow.sum(axis=0).A1\ntag_to_count_map = dict(zip(tags, freq))","2f4702d7":"list = []\nfor key, value in tag_to_count_map.items():\n  list.append([key, value]) ","11414acb":"tag_df = pd.DataFrame(list, columns=['Tags', 'Counts'])\ntag_df.head()","ddd52476":"tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\nplt.plot(tag_df_sorted['Counts'].values)\nplt.grid()\nplt.title(\"Distribution of frequency of tags based on appeareance\")\nplt.xlabel(\"Tag numbers for most frequent tags\")\nplt.ylabel(\"Frequency\")","223c06d4":"plt.plot(tag_df_sorted['Counts'][0:100].values)\nplt.grid()\nplt.title(\"Top 100 tags : Distribution of frequency of tags based on appeareance\")\nplt.xlabel(\"Tag numbers for most frequent tags\")\nplt.ylabel(\"Frequency\")","677f4add":"plt.plot(tag_df_sorted['Counts'][0:100].values)\nplt.scatter(x=np.arange(0,100,5), y=tag_df_sorted['Counts'][0:100:5], c='g', label=\"quantiles with 0.05 intervals\")\nplt.scatter(x=np.arange(0,100,25), y=tag_df_sorted['Counts'][0:100:25], c='r', label = \"quantiles with 0.25 intervals\")\nfor x,y in zip(np.arange(0,100,25), tag_df_sorted['Counts'][0:100:25]):\n    plt.annotate(s=\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.01, y+30))\n\nplt.title('first 100 tags: Distribution of frequency of tags based on appeareance')\nplt.grid()\nplt.xlabel(\"Tag numbers for most frequent tags\")\nplt.ylabel(\"Frequency\")\nplt.legend()","5dba1f79":"print(\"{} tags are used more than 25 times\".format(tag_df_sorted[tag_df_sorted[\"Counts\"]>25].shape[0]))\nprint(\"{} tags are used more than 50 times\".format(tag_df_sorted[tag_df_sorted[\"Counts\"]>50].shape[0]))","e4faee5f":"tag_to_count_map\ntupl = dict(tag_to_count_map.items())\nword_cloud = WordCloud(width=1600,height=800,).generate_from_frequencies(tupl)\nplt.figure(figsize = (12,8))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.tight_layout(pad=0)","851c7880":"i=np.arange(20)\ntag_df_sorted.head(20).plot(kind='bar')\nplt.title('Frequency of top 20 tags')\nplt.xticks(i, tag_df_sorted['Tags'])\nplt.xlabel('Tags')\nplt.ylabel('Counts')\nplt.show()","9a32d06b":"stop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")","15bd9286":"qus_list=[]\nqus_with_code = 0\nlen_before_preprocessing = 0 \nlen_after_preprocessing = 0 \nfor index,row in df.iterrows():\n    title, body, tags = row[\"Title\"], row[\"Body\"], row[\"Tags\"]\n    if '<code>' in body:\n        qus_with_code+=1\n    len_before_preprocessing+=len(title) + len(body)\n    body=re.sub('<code>(.*?)<\/code>', '', body, flags=re.MULTILINE|re.DOTALL)\n    body = re.sub('<.*?>', ' ', str(body.encode('utf-8')))\n    title=title.encode('utf-8')\n    question=str(title)+\" \"+str(body)\n    question=re.sub(r'[^A-Za-z]+',' ',question)\n    words=word_tokenize(str(question.lower()))\n    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n    qus_list.append(question)\n    len_after_preprocessing += len(question)\ndf[\"question\"] = qus_list\navg_len_before_preprocessing=(len_before_preprocessing*1.0)\/df.shape[0]\navg_len_after_preprocessing=(len_after_preprocessing*1.0)\/df.shape[0]\nprint( \"Avg. length of questions(Title+Body) before preprocessing: \", avg_len_before_preprocessing)\nprint( \"Avg. length of questions(Title+Body) after preprocessing: \", avg_len_after_preprocessing)\nprint (\"% of questions containing code: \", (qus_with_code*100.0)\/df.shape[0])","9877751a":"preprocessed_df = df[[\"question\",\"Tags\"]]\nprint(\"Shape of preprocessed data :\", preprocessed_df.shape)","be903a47":"vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\ny_multilabel = vectorizer.fit_transform(preprocessed_df['Tags'])","9ad62eae":"def tags_to_consider(n):\n    tag_i_sum = y_multilabel.sum(axis=0).tolist()[0]\n    sorted_tags_i = sorted(range(len(tag_i_sum)), key=lambda i: tag_i_sum[i], reverse=True)\n    yn_multilabel=y_multilabel[:,sorted_tags_i[:n]]\n    return yn_multilabel\n\ndef questions_covered_fn(numb):\n    yn_multilabel = tags_to_consider(numb)\n    x= yn_multilabel.sum(axis=1)\n    return (np.count_nonzero(x==0))","c7f1e9d2":"questions_covered = []\ntotal_tags=y_multilabel.shape[1]\ntotal_qus=preprocessed_df.shape[0]\nfor i in range(100, total_tags, 100):\n    questions_covered.append(np.round(((total_qus-questions_covered_fn(i))\/total_qus)*100,3))","444567dc":"plt.plot(np.arange(100,total_tags, 100),questions_covered)\nplt.xlabel(\"Number of tags\")\nplt.ylabel(\"Number of questions covered partially\")\nplt.grid()\nplt.show()\nprint(questions_covered[9],\"% of questions covered by 1000 tags\")\nprint(\"Number of questions that are not covered by 100 tags : \", questions_covered_fn(1000),\"out of \", total_qus)","bbd10cb5":"yx_multilabel = tags_to_consider(1000)\nprint(\"Number of tags in the subset :\", y_multilabel.shape[1])\nprint(\"Number of tags considered :\", yx_multilabel.shape[1],\"(\",(yx_multilabel.shape[1]\/y_multilabel.shape[1])*100,\"%)\")","3a9fe091":"X_train, X_test, y_train, y_test = train_test_split(preprocessed_df, yx_multilabel, test_size = 0.2,random_state = 42)\nprint(\"Number of data points in training data :\", X_train.shape[0])\nprint(\"Number of data points in test data :\", X_test.shape[0])","a229ae84":"vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, tokenizer = lambda x: x.split(), ngram_range=(1,3))\nX_train_multilabel = vectorizer.fit_transform(X_train['question'])\nX_test_multilabel = vectorizer.transform(X_test['question'])","e4ec90ca":"print(\"Training data shape X : \",X_train_multilabel.shape, \"Y :\",y_train.shape)\nprint(\"Test data shape X : \",X_test_multilabel.shape,\"Y:\",y_test.shape)","ebf3b840":"clf = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l2'))\nclf.fit(X_train_multilabel, y_train)\ny_pred = clf.predict(X_test_multilabel)","d9205d09":"print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\nprint(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\nprint(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\nprint(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))","b522be33":"qus_list=[]\nqus_with_code = 0\nlen_before_preprocessing = 0 \nlen_after_preprocessing = 0 \nfor index,row in df.iterrows():\n    title, body, tags = row[\"Title\"], row[\"Body\"], row[\"Tags\"]\n    if '<code>' in body:\n        qus_with_code+=1\n    len_before_preprocessing+=len(title) + len(body)\n    body=re.sub('<code>(.*?)<\/code>', '', body, flags=re.MULTILINE|re.DOTALL)\n    body = re.sub('<.*?>', ' ', str(body.encode('utf-8')))\n    title=title.encode('utf-8')\n    question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+ body\n    question=re.sub(r'[^A-Za-z]+',' ',question)\n    words=word_tokenize(str(question.lower()))\n    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n    qus_list.append(question)\n    len_after_preprocessing += len(question)\n    \ndf[\"question_with_more_wt_title\"] = qus_list\navg_len_before_preprocessing=(len_before_preprocessing*1.0)\/df.shape[0]\navg_len_after_preprocessing=(len_after_preprocessing*1.0)\/df.shape[0]\nprint( \"Avg. length of questions(Title+Body) before preprocessing: \", avg_len_before_preprocessing)\nprint( \"Avg. length of questions(Title+Body) after preprocessing: \", avg_len_after_preprocessing)\nprint (\"% of questions containing code: \", (qus_with_code*100.0)\/df.shape[0])","925f3a3e":"preprocessed_df = df[[\"question_with_more_wt_title\",\"Tags\"]]\nprint(\"Shape of preprocessed data :\", preprocessed_df.shape)","cfe079eb":"X_train, X_test, y_train, y_test = train_test_split(preprocessed_df, yx_multilabel, test_size = 0.2,random_state = 42)\nprint(\"Number of data points in training data :\", X_train.shape[0])\nprint(\"Number of data points in test data :\", X_test.shape[0])","3f70de6a":"vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, tokenizer = lambda x: x.split(), ngram_range=(1,3))\nX_train_multilabel = vectorizer.fit_transform(X_train['question_with_more_wt_title'])\nX_test_multilabel = vectorizer.transform(X_test['question_with_more_wt_title'])","6814abf8":"print(\"Training data shape X : \",X_train_multilabel.shape, \"Y :\",y_train.shape)\nprint(\"Test data shape X : \",X_test_multilabel.shape,\"Y:\",y_test.shape)","bcf21df1":"clf = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l2'))\nclf.fit(X_train_multilabel, y_train)\ny_pred = clf.predict(X_test_multilabel)","b49b477b":"print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\nprint(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\nprint(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\nprint(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))","cb89a466":"### Bar plot of top 20 tags","ea48915f":"### Mapping to an Machine Learning Problem\nSince we understand the Business problem well, let\u2019s try to pose it as a proper Machine Learning problem. The first thing to do is to aquire the data and understand it.","90f88b8d":"### Observations:\n\n\"c#\", \"java\", \"php\", \"android\", \"javascript\", \"jquery\", \"C++\" are some of the most frequent tags.","3106b30d":"### Data Field Explaination\n\nDataset contains 6,034,195 rows. The columns in the table are:\n\nId - Unique identifier for each question\n\nTitle - The question's title\n\nBody - The body of the question\n\nTags - The tags associated with the question in a space-seperated format (all lowercase, should not contain tabs '\\t' or ampersands '&')\n","b662b9e2":"### Word map for most frequent Tags","c82d0074":"### ACKNOWLEDGMENT\nSpecial Thanks **Joseph Stalin Peter** for continuous support and guidence.","4e796e12":"We have a very high dimensional data and we need to built many models in a binary representation. To tackle this, I have taken the help of Logistic Regression with One vs Rest classifier. The classifier takes each of the labels and train 1000 logistic regression models. Training a Logistic Regression model is very cheap and easy when compare to other models like Support Vector Machines (SVM), Random Forest etc..and it performs really well on high dimensional data.","2652500b":"#### Basic Data Analysis on Tags\nFrequency of tag_count","cc5be0af":"### Featurization of Training Data","dae05f7f":"### Business Objectives and Constraints\nPredict as many tags as possible with high precision and recall.\nSo for this problem we should get high precision and high recall rates. \n\nFor example, let\u2019s assume that we have a title, description with 4 tags. If we want to predict any of the tags we should have a high precision value i.e, we have to be really sure that the predicted tag belongs to the given question. Also, we want to have a high Recal rate, which means If the tag actually supposed to be present, we want it to be present most number of times.","38f33cd5":"### Metric values of macro and micro f1 score are significantly increased when compare to previous model.","b1282051":"Since the number of records in the data is very large(6034195) so let's consider a small subset of data for faster computing.","0d12452a":"### Loading Data ","1678c7f7":"### Frequency of each tag","6b777785":"### Multi-label Classification: \nMulti-label classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A question on Stack Overflow might be about any of C, Pointers, FileIO and\/or memory-management at the same time or none of these. Go through the link and know more details about Multi Label Classification ( http:\/\/scikit-learn.org\/stable\/modules\/multiclass.html)","c64ea9a8":"Key Points:\n\ni. Majority of the most frequent tags are programming language.\n\nii. C# is the top most frequent programming language.\n\niii. Android, IOS, Linux and windows are among the top most frequent operating systems.","75a5b8ff":"## Data Overview\n\nRefer: https:\/\/www.kaggle.com\/c\/facebook-recruiting-iii-keyword-extraction\/data\n\nAll of the data is in 2 files: Train and Test.\n\nTrain.csv contains 4 columns: Id,Title,Body,Tags.\n\nTest.csv contains the same columns but without the Tags, which you are to predict.\n\nSize of Train.csv - 6.75GB\n\nSize of Test.csv - 2GB\n\nNumber of rows in Train.csv = 6034195\n\nThe questions are randomized and contains a mix of verbose text sites as well as sites related to math and programming. The number of questions from each site may vary, and no filtering has been performed on the questions (such as closed questions).","bb82a2e4":"### Splitting into train and test set with 80:20 ratio","7645b6a3":"### Fitting Logistic Regression with OneVsRest Classifier","e7588d6e":"### Why Logistic Regression\nOur One vs Rest classifier can take any model and give results. Here I have limited myself only to Logistic Regression and not used other models like Support Vector Machines, Random Forest, Gradient Boosting Decision Trees etc is because of Multi-Fold. i.e, we have a very high dimensional data with 500 models to build and less time complexity. In this case the mentioned other models like SVM, Random Forest and GBDT cannot work well when compare to Logistic Regression. I have still used Linear SVM with SGD Classifier considering loss value as hinge to get good results but failed! Logistic Regression is much more faster than any other model.","648fff43":"### Machine learning models\n#### Multilabel problem - Handling tags","4547f542":"### Example Data point\nTitle:  Implementing Boundary Value Analysis of Software Testing in a C++ program?\nBody : \n\n        #include<\n        iostream>\\n\n        #include<\n        stdlib.h>\\n\\n\n        using namespace std;\\n\\n\n        int main()\\n\n        {\\n\n                 int n,a[n],x,c,u[n],m[n],e[n][4];\\n         \n                 cout<<\"Enter the number of variables\";\\n         cin>>n;\\n\\n         \n                 cout<<\"Enter the Lower, and Upper Limits of the variables\";\\n         \n                 for(int y=1; y<n+1; y++)\\n         \n                 {\\n                 \n                    cin>>m[y];\\n                 \n                    cin>>u[y];\\n         \n                 }\\n         \n                 for(x=1; x<n+1; x++)\\n         \n                 {\\n                 \n                    a[x] = (m[x] + u[x])\/2;\\n         \n                 }\\n         \n                 c=(n*4)-4;\\n         \n                 for(int a1=1; a1<n+1; a1++)\\n         \n                 {\\n\\n             \n                    e[a1][0] = m[a1];\\n             \n                    e[a1][1] = m[a1]+1;\\n             \n                    e[a1][2] = u[a1]-1;\\n             \n                    e[a1][3] = u[a1];\\n         \n                 }\\n         \n                 for(int i=1; i<n+1; i++)\\n         \n                 {\\n            \n                    for(int l=1; l<=i; l++)\\n            \n                    {\\n                 \n                        if(l!=1)\\n                 \n                        {\\n                    \n                            cout<<a[l]<<\"\\\\t\";\\n                 \n                        }\\n            \n                    }\\n            \n                    for(int j=0; j<4; j++)\\n            \n                    {\\n                \n                        cout<<e[i][j];\\n                \n                        for(int k=0; k<n-(i+1); k++)\\n                \n                        {\\n                    \n                            cout<<a[k]<<\"\\\\t\";\\n               \n                        }\\n                \n                        cout<<\"\\\\n\";\\n            \n                    }\\n        \n                 }    \\n\\n        \n                 system(\"PAUSE\");\\n        \n                 return 0;    \\n\n        }\\n\n        \n\\n\\n <p>The answer should come in the form of a table like<\/p>\\n\\n\n    <pre><code>       \n    1            50              50\\n       \n    2            50              50\\n       \n    99           50              50\\n       \n    100          50              50\\n       \n    50           1               50\\n       \n    50           2               50\\n       \n    50           99              50\\n       \n    50           100             50\\n       \n    50           50              1\\n       \n    50           50              2\\n       \n    50           50              99\\n       \n    50           50              100\\n\n    <\/code><\/pre>\\n\\n\n    <p>if the no of inputs is 3 and their ranges are\\n\n    1,100\\n\n    1,100\\n\n    1,100\\n\n    (could be varied too)<\/p>\\n\\n\n    <p>The output is not coming,can anyone correct the code or tell me what\\'s wrong?<\/p>\\n'\nTags : 'c++ c'","365d6de3":"### Performance Metrics\nFor a standard Binary of Multi-class classification problems, we can use performance metrics like Precision,Recall,F1-Score,Log-loss,AUC Curve etc..But for the present Multi-Label problem the mentioned metrics may not work well. As part of the business requirement we want high precision and recall rates for each and every predicted tag. We can use F1 Score here as it only gives good value if both the Precision and Recall are high. The F1 Score performs really well for Binary classifications. So for Multi Label Setting we can modify it into two types Micro Averaged F1 Score and Macro Averaged F1 Score\n\nPlease go through the below links to understand more about Precision, Recall, Micro and Macro averaged f1 scores.\nhttps:\/\/medium.com\/@klintcho\/explaining-precision-and-recall-c770eb9c69e9","41099de5":"### Text preprocessing","f164ccb6":"### One Vs Rest Classifier\n\nAlso known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier.\n\nThis strategy can also be used for multilabel learning, where a classifier is used to predict multiple labels for instance, by fitting on a 2-d matrix in which cell [i, j] is 1 if sample i has label j and 0 otherwise.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multiclass.OneVsRestClassifier.html","fb6e42bb":"### Observations:\n\n* Maximum number of tags in a question: 5\n* Minimum number of tags in a question: 1\n* Average number of tags per question: 2.92\n* Most of the questions have either 2 or 3 tags","ef482da4":"# What is StackOverflow\n### Description\n##### Stack Overflow is the largest, most trusted online community for developers to learn, share their programming knowledge, and build their careers. It is something which every programmer use one way or another. Each month, over 50 million developers come to Stack Overflow to learn, share their knowledge, and build their careers. It features questions and answers on a wide range of topics in computer programming. The website serves as a platform for users to ask and answer questions, and, through membership and active participation, to vote questions and answers up or down and edit questions and answers in a fashion similar to a wiki or Digg. As of April 2014 Stack Overflow has over 4,000,000 registered users, and it exceeded 10,000,000 questions in late August 2015. Based on the type of tags assigned to questions, the top eight most discussed topics on the site are: Java, JavaScript, C#, PHP, Android, jQuery, Python and HTML. https:\/\/stackoverflow.com\/.\n\n### Business Problem\nThe problem says that we will be provided a bunch of questions. A question in Stack Overflow contains three segments Title, Description and Tags. By using the text in the title and description we should suggest the tags related to the subject of the question automatically. These tags are extremely important for the proper working of Stack Overflow.","684b6e7e":"#### Total number of unique tags","259242a0":"### Data preprocessing\nChecking for duplicates","5dec5bd3":"### Observations:\n\n* 144 tags are used more than 25 times.\n* 59 tags are used more than 50 times.\n* C# is most frequently used tag 778 times.\n* Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this probelm.","a5aaf86e":"Micro and Macro f1 values are 9 and 32 then we improved to 13 and 36 using one vs rest it's not giving impressive results but time and data permit i consider out 6 million data points I took only 300k points if we took more points we can improve performance metrics values."}}