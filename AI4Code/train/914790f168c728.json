{"cell_type":{"da20b6e3":"code","7fa6dc35":"code","2f158b8c":"code","64ab0c5d":"code","db91639a":"code","a9ea679c":"code","f779eb19":"code","2c252ef9":"code","c2c7b013":"code","feb8b154":"code","e4727b3e":"code","d48bd339":"code","8d130407":"code","fbee8526":"code","ffbed52e":"code","606ac8f4":"code","18ff980d":"code","a3dcfad4":"code","60e044b3":"code","d7a84b9a":"code","53962f00":"code","4b8e0fdc":"code","16c4f38f":"code","481cc24e":"code","e131fa25":"code","242a25ef":"code","e3588ab5":"code","422942ee":"code","bcc0fd54":"code","07698269":"code","cb42f95e":"code","7ce276eb":"code","2d69408f":"markdown","9224ee36":"markdown","f6f1b288":"markdown","8bf68b81":"markdown","b24cf5ae":"markdown","032b64dd":"markdown","25a37f46":"markdown","07a525cf":"markdown","f9c7f696":"markdown","7db517ad":"markdown","bd141b5b":"markdown"},"source":{"da20b6e3":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))","7fa6dc35":"#Verify which embeddings are provided\n!ls ..\/input\/embeddings\n#there are 4 embeddings provided with the dataset","2f158b8c":"#import packages\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport spacy\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\nfrom bs4 import BeautifulSoup\nimport unicodedata\nfrom collections import defaultdict\nimport string\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import log_loss\nfrom tqdm import tqdm\nstopwords = stopwords.words('english')\nsns.set_context('notebook')","64ab0c5d":"#Code adapted from: https:\/\/www.kaggle.com\/arunsankar\/key-insights-from-quora-insincere-questions\n#import the different datasets and print the characteristics of each\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')\n\n#Print the different statistics of the different files\nprint('Train data: \\nRows: {}\\nCols: {}'.format(train.shape[0],train.shape[1]))\nprint(train.columns)\n\nprint('\\nTest data: \\nRows: {}\\nCols: {}'.format(test.shape[0],test.shape[1]))\nprint(test.columns)\n\nprint('\\nSubmission data: \\nRows: {}\\nCols: {}'.format(sub.shape[0],sub.shape[1]))\nprint(sub.columns)","db91639a":"#View the first 5 entries of the training data\ntrain.head()","a9ea679c":"#View information about the train dataset\ntrain.info()\n##1306122 observations and 3 columns","f779eb19":"#check for the number of positive and negative classes\npd.crosstab(index = train.target, columns = \"count\" )\n#There seems to be unbalanced classes in the dataset [first issue]","2c252ef9":"#Code sourced from : https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc\n\n#import the wordcloud package\nfrom wordcloud import WordCloud, STOPWORDS\n\n#Define the word cloud function with a max of 200 words\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    #define additional stop words that are not contained in the dictionary\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n    #Generate the word cloud\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    #set the plot parameters\n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  ","c2c7b013":"#Select insincere questions from training dataset\ninsincere = train.loc[train['target'] == 1]\n#run the function on the insincere questions\nplot_wordcloud(insincere[\"question_text\"], title=\"Word Cloud of Insincere Questions\")","feb8b154":"#Select sincere questions from training dataset\nsincere = train.loc[train['target'] == 0]\n#run the function on the insincere questions\nplot_wordcloud(sincere[\"question_text\"], title=\"Word Cloud of Sincere Questions\")","e4727b3e":"def ngram_extractor(text, n_gram):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n# Function to generate a dataframe with n_gram and top max_row frequencies\ndef generate_ngrams(df, col, n_gram, max_row):\n    temp_dict = defaultdict(int)\n    for question in df[col]:\n        for word in ngram_extractor(question, n_gram):\n            temp_dict[word] += 1\n    temp_df = pd.DataFrame(sorted(temp_dict.items(), key=lambda x: x[1])[::-1]).head(max_row)\n    temp_df.columns = [\"word\", \"wordcount\"]\n    return temp_df\n\n#Function to construct side by side comparison plots\ndef comparison_plot(df_1,df_2,col_1,col_2, space):\n    fig, ax = plt.subplots(1, 2, figsize=(20,10))\n    \n    sns.barplot(x=col_2, y=col_1, data=df_1, ax=ax[0], color=\"royalblue\")\n    sns.barplot(x=col_2, y=col_1, data=df_2, ax=ax[1], color=\"royalblue\")\n\n    ax[0].set_xlabel('Word count', size=14)\n    ax[0].set_ylabel('Words', size=14)\n    ax[0].set_title('Top words in sincere questions', size=18)\n\n    ax[1].set_xlabel('Word count', size=14)\n    ax[1].set_ylabel('Words', size=14)\n    ax[1].set_title('Top words in insincere questions', size=18)\n\n    fig.subplots_adjust(wspace=space)\n    \n    plt.show()","d48bd339":"#Obtain sincere and insincere ngram based on 1 gram (top 20)\nsincere_1gram = generate_ngrams(train[train[\"target\"]==0], 'question_text', 1, 20)\ninsincere_1gram = generate_ngrams(train[train[\"target\"]==1], 'question_text', 1, 20)\n#compare the bar plots\ncomparison_plot(sincere_1gram,insincere_1gram,'word','wordcount', 0.25)","8d130407":"#Obtain sincere and insincere ngram based on 2 gram (top 20)\nsincere_2gram = generate_ngrams(train[train[\"target\"]==0], 'question_text', 2, 20)\ninsincere_2gram = generate_ngrams(train[train[\"target\"]==1], 'question_text', 2, 20)\n#compare the bar plots\ncomparison_plot(sincere_2gram,insincere_2gram,'word','wordcount', 0.25)","fbee8526":"#Obtain sincere and insincere ngram based on 3 gram (top 20)\nsincere_3gram = generate_ngrams(train[train[\"target\"]==0], 'question_text', 3, 20)\ninsincere_3gram = generate_ngrams(train[train[\"target\"]==1], 'question_text', 3, 20)\n#compare the bar plots\ncomparison_plot(sincere_3gram,insincere_3gram,'word','wordcount', 0.25)","ffbed52e":"# Number of words in the questions\ntrain[\"word_count\"] = train[\"question_text\"].apply(lambda x: len(str(x).split()))\ntest[\"word_count\"] = test[\"question_text\"].apply(lambda x: len(str(x).split()))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"word_count\", y=\"target\", data=train, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Word Count distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","606ac8f4":"# Number of characters in the questions\ntrain[\"char_length\"] = train[\"question_text\"].apply(lambda x: len(str(x)))\ntest[\"char_length\"] = test[\"question_text\"].apply(lambda x: len(str(x)))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"char_length\", y=\"target\", data=train, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Character Length', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Character Length distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","18ff980d":"# Number of stop words in the questions\ntrain[\"stop_words_count\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest[\"stop_words_count\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"stop_words_count\", y=\"target\", data=train, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Number of stop words', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Number of Stop Words distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","a3dcfad4":"# Number of punctuations in the questions\ntrain[\"punc_count\"] = train[\"question_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest[\"punc_count\"] = test[\"question_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"punc_count\", y=\"target\", data=train[train['punc_count']<train['punc_count'].quantile(.99)], ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Number of punctuations', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Punctuation distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","60e044b3":"# Number of upper case words in the questions\ntrain[\"upper_words\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"upper_words\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"upper_words\", y=\"target\", data=train[train['upper_words']<train['upper_words'].quantile(.99)], ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Number of Upper case words', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Upper case words distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","d7a84b9a":"# Number of title words in the questions\ntrain[\"title_words\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"title_words\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"title_words\", y=\"target\", data=train[train['title_words']<train['title_words'].quantile(.99)], ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Number of Title words', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Title words distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","53962f00":"# Mean word length in the questions\ntrain[\"word_length\"] = train[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"word_length\"] = test[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"word_length\", y=\"target\", data=train[train['word_length']<train['word_length'].quantile(.99)], ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Mean word length', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Distribution of mean word length', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","4b8e0fdc":"# nlp = spacy.load('en_core_web_sm')\n# # Clean text before feeding it to model\n# punctuations = string.punctuation\n\n# # Define function to cleanup text by removing personal pronouns, stopwords, puncuation and reducing all characters to lowercase \n# def cleanup_text(docs, logging=False):\n#     texts = []\n#     for doc in tqdm(docs):\n#         doc = nlp(doc, disable=['parser', 'ner'])\n#         tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n#         #remove stopwords and punctuations\n#         tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n#         tokens = ' '.join(tokens)\n#         texts.append(tokens)\n#     return pd.Series(texts)","16c4f38f":"# # Cleanup text and make sure it retains original shape\n# print('Original training data shape: ', train['question_text'].shape)\n# train_cleaned = cleanup_text(train['question_text'], logging=True)\n# print('Cleaned up training data shape: ', train_cleaned.shape)","481cc24e":"#use 90-10 split for validation dataset\ntrain, val_df = train_test_split(train, test_size=0.1)","e131fa25":"# embdedding setup\n# Source https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n#Based on https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing\n#GloVe is the most comprehensive word embedding\n\nembeddings_index = {}\nf = open('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","242a25ef":"# Convert values to embeddings\ndef text_to_array(text):\n    empyt_emb = np.zeros(300)\n    text = text[:-1].split()[:30]\n    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n    embeds+= [empyt_emb] * (30 - len(embeds))\n    return np.array(embeds)\n\n# train_vects = [text_to_array(X_text) for X_text in tqdm(train[\"question_text\"])]\nval_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"question_text\"][:3000])])\nval_y = np.array(val_df[\"target\"][:3000])","e3588ab5":"# Data providers\nbatch_size = 128\n\ndef batch_gen(train):\n    n_batches = math.ceil(len(train) \/ batch_size)\n    while True: \n        train = train.sample(frac=1.)  # Shuffle the data.\n        for i in range(n_batches):\n            texts = train.iloc[i*batch_size:(i+1)*batch_size, 1]\n            text_arr = np.array([text_to_array(text) for text in texts])\n            yield text_arr, np.array(train[\"target\"][i*batch_size:(i+1)*batch_size])","422942ee":"#import Bi-Directional LSTM \nfrom keras.models import Sequential\nfrom keras.layers import CuDNNLSTM, Dense, Bidirectional\nimport math","bcc0fd54":"#Define the model architecture\nmodel = Sequential()\nmodel.add(Bidirectional(CuDNNLSTM(64, return_sequences=True),\n                        input_shape=(30, 300)))\nmodel.add(Bidirectional(CuDNNLSTM(64)))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","07698269":"mg = batch_gen(train)\n#remember to change the number of epochs\nmodel.fit_generator(mg, epochs=10,\n                    steps_per_epoch=1000,\n                    validation_data=(val_vects, val_y),\n                    verbose= True)","cb42f95e":"# prediction part\nbatch_size = 256\ndef batch_gen(test):\n    n_batches = math.ceil(len(test) \/ batch_size)\n    for i in range(n_batches):\n        texts = test.iloc[i*batch_size:(i+1)*batch_size, 1]\n        text_arr = np.array([text_to_array(text) for text in texts])\n        yield text_arr\n\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nall_preds = []\nfor x in tqdm(batch_gen(test)):\n    all_preds.extend(model.predict(x).flatten())","7ce276eb":"#Submit predictions\ny_te = (np.array(all_preds) > 0.5).astype(np.int)\n\nsubmit_df = pd.DataFrame({\"qid\": test[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)","2d69408f":"**EDA plot 2 - side by side plot comparison using N-gram**\n\nAn n-gram is a contiguous sequence of n items from a given sample of text or speech. Different definitions of n-grams will allow for the identification of the most prevalent words\/sentences in the training data and thus help distinguish what comprises insincere and sincere questions.\n\nIt should be noted that prior to displaying individual words or sentences, the text will first be tokenized (based on a desired integer) and then put into a dataframe which will be used to construct side by side plots. \n\nTokenization is, generally, an early step in the NLP process, a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. ","9224ee36":"# Modelling","f6f1b288":"# Data Preparation \n\n","8bf68b81":"# Evaluation\n\nThe current model configuration leads to a score of around 0.547 \n\nThe next steps for this kernel will be:\n\n* Investigate how dimensionality reduction techniques affect the model\n* Implement oversampling to cater for unbalanced positive classes\n* Implement the callback history in the model defintion to allow for the mapping of training and testing error (overfitting detection)\n* Apply rules of thumb for LSTM architecture defintion (sourced from journals)\n* Implement text treatment as per: https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-text-preprocessing-v2\n\n**Thanks for reading so far!**\n\n**Any comments, advice, recommendations and upvotes would be much appreciated!**","b24cf5ae":"**Introduction** \n\nThe aim of this notebook is to leverage insights from several public kernels to eventually formalize a workflow for NLP beginners on kaggle, like me.  Any comments, recommendations and insights would thus be very appreciated!\n\n**Credits**\n\nEDA code was heavily sourced from: \n \nhttps:\/\/www.kaggle.com\/arunsankar\/key-insights-from-quora-insincere-questions\n\nData Pre-processing code adapted from:\n\nhttps:\/\/www.kaggle.com\/enerrio\/scary-nlp-with-spacy-and-keras\n\nWord Embedding model was heavily adapted from:\n\nhttps:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing\n\nLSTM architecture was adapted from:\n\nhttps:\/\/www.kaggle.com\/mihaskalic\/lstm-is-all-you-need-well-maybe-embeddings-also\n\n\n**Analysis Sections**\n* Data Understanding\n    * EDA plot 1 - Word Cloud\n    * EDA plot 2 - side by side plot comparison using N-gram\n    * EDA Plot 3 - Word count distribution, Character Length Distribution, Stop words, Punctuation, Upper case\n    * Overview of EDA results\n* Data Preparation \n* Modelling\n* Evaluation\n\n**Other helpful kernels and links**\n\n*Kernels* :\n\n* https:\/\/www.kaggle.com\/mjbahmani\/a-data-science-framework-for-quora\n* https:\/\/www.kaggle.com\/shujian\/test-the-difficulty-of-this-classification-tasks\n* https:\/\/www.kaggle.com\/enerrio\/scary-nlp-with-spacy-and-keras\n* https:\/\/www.kaggle.com\/wakamezake\/visualizing-word-vectors\n\n*Links*\n* https:\/\/www.kdnuggets.com\/2017\/02\/natural-language-processing-key-terms-explained.html\n\n**Next Steps**\n\n* Investigate how dimensionality reduction techniques affect the model\n* Implement oversampling to cater for unbalanced positive classes\n\n**Thanks for reading!**\n","032b64dd":"There are two ways of fitting an NLP model:\n\n* Without the use of Word Embeddings (this will include steps such as lemmentization, removal of stopwords, punctuation and standardizing the characters)\n* With Word Embeddings (this should involve limited pre-processing steps as compared to the above)\n\nThis version of the kernel will focus on the use of Word Embeddings for sentiment analysis, with some sample code for text pre-processing should word embeddings not be present  (as shown below).\n","25a37f46":"Word Embeddings allow words that are used in similar ways to result in having similar vector representations, naturally capturing their meaning.\n\nFurther details at: https:\/\/machinelearningmastery.com\/what-are-word-embeddings\/","07a525cf":"# **Data Understanding**\n \nIn order to start the anlaysis, the nature of the datasets and the files provided will first need to be examined. This normally involves examining the shape of the datasets,  followed by an EDA, with the objective to understand what defines a question as insincere or sincere.\n","f9c7f696":"**EDA plot 1 - Word Cloud**\n\nWord clouds can identify trends and patterns that would otherwise be unclear or difficult to see in a tabular format. Frequently used keywords stand out better in a word cloud. Common words that might be overlooked in tabular form are highlighted in larger text making them pop out when displayed in a word cloud.","7db517ad":"**EDA Plot 3 - Word count distribution, Character Length Distribution, Stop words, Punctuation, Upper case **","bd141b5b":"*Overview of EDA results*\n\nThe EDA has helped outline a few characteristics which define insincere questions as such:\n\n* Insincere questions are mostly focused at politics, religion and can contain profanity.\n* Insincere questions (Class 1) are generally more lengthy, with the exceptions of certain outliers for sincere questions. They thus have more stop words, punctions, characters, a higher average word count and title words.\n* Insincere questions are mostly lower case.\n* There are no missing cases.\n\nThe EDA has also revealed a few issues about the dataset which can be regrouped as:\n\n* Unbalanced classes : a model trained on the current split of classes for the target variable will create a model more apt at predicting the '0' class rather than the '1' class, leading to false negatives in the predictions.\n* Uneven length of questions: the questions asked do not have a standard length and can thus lead to some questions being longer or shorter than others. \n* Unstandardized letter cases\n* Punctuations\n* Stop Words\n* Outliers"}}