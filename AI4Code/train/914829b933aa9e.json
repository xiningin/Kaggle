{"cell_type":{"d4e13b24":"code","809b55e9":"markdown"},"source":{"d4e13b24":"#!\/usr\/bin\/python3\n# coding=utf-8\n#===========================================================================\n# This is a minimal script to perform a classification on the kaggle \n# 'Titanic' data set using XGBoost Python API \n# Carl McBride Ellis (12.IV.2020)\n#===========================================================================\n#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas  as pd\nimport xgboost as xgb\nfrom   xgboost import XGBClassifier\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data  = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#===========================================================================\n# select some features of interest (\"ay, there's the rub\", Shakespeare)\n#===========================================================================\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n\n#===========================================================================\n# for the features that are categorical we use pd.get_dummies:\n# \"Convert categorical variable into dummy\/indicator variables.\"\n#===========================================================================\nX_train       = pd.get_dummies(train_data[features])\ny_train       = train_data[\"Survived\"]\nfinal_X_test  = pd.get_dummies(test_data[features])\n\n#===========================================================================\n# XGBoost classification: \n# Parameters: \n# n_estimators  \"Number of gradient boosted trees. Equivalent to number of \n#                boosting rounds.\"\n# learning_rate \"Boosting learning rate (xgb\u2019s \u201ceta\u201d)\"\n# max_depth     \"Maximum depth of a tree. Increasing this value will make \n#                the model more complex and more likely to overfit.\" \n#===========================================================================\nclassifier = XGBClassifier(n_estimators=750,learning_rate=0.02,max_depth=3)\nclassifier.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict 'Survived' for the test data\n#===========================================================================\npredictions = classifier.predict(final_X_test)\n\n#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","809b55e9":"This is a minimalist script which applies XGBoost classification to the 'Titanic' data set. It produces a score of around 0.77990, but this is not good, nor is it the point: the purpose of this script is to serve as a basic starting framework from which you can launch your own feature engineering."}}