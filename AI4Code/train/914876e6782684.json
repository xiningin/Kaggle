{"cell_type":{"bd004640":"code","d4a707f9":"code","946245af":"code","cdd1ec15":"code","3085e9d8":"code","1af7fefa":"code","85df6250":"code","d352144c":"code","78343e53":"code","61fe891e":"code","4bdc6714":"code","8df0a6d7":"code","bcc7e173":"code","b331bc63":"code","6156036b":"code","65c7edfa":"code","d1389e43":"code","88af6492":"code","079dcd5a":"code","eb212e8c":"code","e6c7a388":"code","f7f75d24":"code","5e32b853":"code","1a7583f6":"code","c21f4978":"code","da3d4755":"code","7e48708e":"code","855e5e1b":"code","825f512f":"code","075b8bd8":"code","b64c1e20":"code","df4190f6":"code","09690377":"code","436c30fb":"code","3ac8b732":"code","86263a33":"code","67d0c3fb":"code","46874195":"code","9be3f087":"code","5d1a330a":"code","1c6c55b3":"code","ed0f010f":"code","930099cf":"markdown","0c515046":"markdown","5f6bd85d":"markdown","e2c506f7":"markdown","b6667f22":"markdown","4ce27340":"markdown","fa58652b":"markdown","c38dc87e":"markdown","55d6acb7":"markdown","6eb24b5e":"markdown","ea40a597":"markdown","2696bef8":"markdown","b396f6a9":"markdown","0182e63e":"markdown","2bc77831":"markdown","5f50d8d8":"markdown","ea478e2e":"markdown","53085147":"markdown","fd63a5e7":"markdown","2498f6a7":"markdown","7bcc01c2":"markdown"},"source":{"bd004640":"!pip uninstall tensorflow -y \n!pip install tensorflow==1.15","d4a707f9":"# !wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n# !wget https:\/\/raw.githubusercontent.com\/merishnaSuwal\/bert\/master\/optimization.py\n# !wget https:\/\/raw.githubusercontent.com\/merishnaSuwal\/bert\/master\/run_classifier.py\n# !wget https:\/\/raw.githubusercontent.com\/merishnaSuwal\/bert\/master\/tokenization.py","946245af":"!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/optimization.py\n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/run_classifier.py\n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py","cdd1ec15":"import numpy as np # linear algebra\nimport re, os\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nimport datetime\nfrom datetime import datetime\n\n# BERT\nimport optimization\nimport run_classifier\nimport tokenization\nimport tensorflow_hub as hub","3085e9d8":"# Checking tensorflow version (should be 1.15)\ntf.__version__","1af7fefa":"# Get the file details\ndirectory = []\nfile = []\ntitle = []\ntext = []\nlabel = []\ndatapath = '..\/input\/bbc-full-text-document-classification\/bbc-fulltext (document classification)\/bbc\/' \nfor dirname, _ , filenames in os.walk(datapath):\n    #print('Directory: ', dirname)\n    #print('Subdir: ', dirname.split('\/')[-1])\n    # remove the Readme.txt file\n    # will not find file in the second iteration so we skip the error\n    try:\n        filenames.remove('README.TXT')\n    except:\n        pass\n    for filename in filenames:\n        directory.append(dirname)\n        file.append(filename)\n        label.append(dirname.split('\/')[-1])\n        fullpathfile = os.path.join(dirname,filename)\n        with open(fullpathfile, 'r', encoding=\"utf8\", errors='ignore') as infile:\n            intext = ''\n            firstline = True\n            for line in infile:\n                if firstline:\n                    title.append(line.replace('\\n',''))\n                    firstline = False\n                else:\n                    intext = intext + ' ' + line.replace('\\n','')\n            text.append(intext)\n","85df6250":"DATA_COLUMN = 'text'\nLABEL_COLUMN = 'label'\n\nfulldf = pd.DataFrame(list(zip(directory, file, title, text, label)), \n               columns =['directory', 'file', 'title', 'text', 'label'])\n\ndf = fulldf.filter(['text','label'], axis=1)\ndf.head()","d352144c":"df.shape","78343e53":"df.isnull().sum()","61fe891e":"for label in np.unique(df['label']):\n    print(label)","4bdc6714":"# Checking number of records of each label\ndf['label'].value_counts().sort_values(ascending=False).plot(kind='bar')","8df0a6d7":"le = LabelEncoder()\ndf['label'] = le.fit_transform(df['label'])\ndf.head()","bcc7e173":"REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n#     text = re.sub(r'\\W+', '', text)\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text","b331bc63":"df['text'] = df['text'].apply(clean_text)\ndf['text'] = df['text'].str.replace('\\d+', '')\ndf.head()","6156036b":"def get_split(text1):\n    l_total = []\n    l_parcial = []\n    if len(text1.split())\/\/150 >0:\n        n = len(text1.split())\/\/150\n    else: \n        n = 1\n    for w in range(n):\n        if w == 0:\n            l_parcial = text1.split()[:200]\n            l_total.append(\" \".join(l_parcial))\n        else:\n            l_parcial = text1.split()[w*150:w*150 + 200]\n            l_total.append(\" \".join(l_parcial))\n    return l_total","65c7edfa":"df['text_split'] = df['text'].apply(get_split)\ndf.head()","d1389e43":"# Set the output directory for saving model file\nOUTPUT_DIR = '\/bert_news_category'\n\n#@markdown Whether or not to clear\/delete the directory and create a new one\nDO_DELETE = True #@param {type:\"boolean\"}\n\nif DO_DELETE:\n    try:\n        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n#         tf.compat.v1.gfile.DeleteRecursively(OUTPUT_DIR)\n    except:\n        pass\n\ntf.gfile.MakeDirs(OUTPUT_DIR)\nprint('***** Model output directory: {} *****'.format(OUTPUT_DIR))","88af6492":"train, val = train_test_split(df, test_size=0.2, random_state=35)\ntrain.reset_index(drop=True, inplace=True)\ntrain.head(2)","079dcd5a":"# Get labels\nlabel_list = [x for x in np.unique(train.label)]\nlabel_list","eb212e8c":"val.reset_index(drop=True, inplace=True)\nval.head(2)","e6c7a388":"val.shape, train.shape","f7f75d24":"train_l = []\nlabel_l = []\nindex_l =[]\nfor idx,row in train.iterrows():\n    for l in row['text_split']:\n        train_l.append(l)\n        label_l.append(row['label'])\n        index_l.append(idx)\nlen(train_l), len(label_l), len(index_l)","5e32b853":"val_l = []\nval_label_l = []\nval_index_l = []\nfor idx,row in val.iterrows():\n    for l in row['text_split']:\n        val_l.append(l)\n        val_label_l.append(row['label'])\n        val_index_l.append(idx)\nlen(val_l), len(val_label_l), len(val_index_l)","1a7583f6":"train_df = pd.DataFrame({DATA_COLUMN:train_l, LABEL_COLUMN:label_l})\ntrain_df.head()","c21f4978":"val_df = pd.DataFrame({DATA_COLUMN:val_l, LABEL_COLUMN:val_label_l})\nval_df.head()","da3d4755":"# X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.20, random_state=42)","7e48708e":"# Use the InputExample class from BERT's run_classifier code to create examples from the data\ntrain_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)\n\nval_InputExamples = val.apply(lambda x: run_classifier.InputExample(guid=None, \n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)","855e5e1b":"train_InputExamples\n","825f512f":"print(\"Row 0 - guid of training set : \", train_InputExamples.iloc[0].guid)\nprint(\"\\n__________\\nRow 0 - text_a of training set : \", train_InputExamples.iloc[0].text_a)\nprint(\"\\n__________\\nRow 0 - text_b of training set : \", train_InputExamples.iloc[0].text_b)\nprint(\"\\n__________\\nRow 0 - label of training set : \", train_InputExamples.iloc[0].label)","075b8bd8":"BERT_MODEL_HUB = \"https:\/\/tfhub.dev\/google\/bert_uncased_L-12_H-768_A-12\/1\"\n\ndef create_tokenizer_from_hub_module():\n  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n  with tf.Graph().as_default():\n    bert_module = hub.Module(BERT_MODEL_HUB)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n    with tf.Session() as sess:\n#     with tf.compat.v1.Session() as sess:\n        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n                                            tokenization_info[\"do_lower_case\"]])\n      \n    return tokenization.FullTokenizer(\n      vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ntokenizer = create_tokenizer_from_hub_module()","b64c1e20":"len(tokenizer.vocab.keys())","df4190f6":"#Here is what the tokenised sample of the first training set observation looks like\nprint(tokenizer.tokenize(train_InputExamples.iloc[0].text_a))","09690377":"MAX_SEQ_LENGTH = 200\n\ntrain_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\nval_features = run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)","436c30fb":"#Example on first observation in the training set\nprint(\"Sentence : \", train_InputExamples.iloc[0].text_a)\nprint(\"-\"*30)\nprint(\"Tokens : \", tokenizer.tokenize(train_InputExamples.iloc[0].text_a))\nprint(\"-\"*30)\nprint(\"Input IDs : \", train_features[0].input_ids)\nprint(\"-\"*30)\nprint(\"Input Masks : \", train_features[0].input_mask)\nprint(\"-\"*30)\nprint(\"Segment IDs : \", train_features[0].segment_ids)","3ac8b732":"def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n                 num_labels):\n  \n    bert_module = hub.Module(\n        BERT_MODEL_HUB,\n        trainable=True)\n    bert_inputs = dict(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids)\n    bert_outputs = bert_module(\n      inputs=bert_inputs,\n      signature=\"tokens\",\n      as_dict=True)\n\n    # Use \"pooled_output\" for classification tasks on an entire sentence.\n    # Use \"sequence_outputs\" for token-level output.\n    output_layer = bert_outputs[\"pooled_output\"]\n    # with tf.Session() as sess:\n    output_layer1 = bert_outputs[\"pooled_output\"]\n    # output_layer1 = 999\n    hidden_size = output_layer.shape[-1].value\n\n    # Create our own layer to tune for politeness data.\n    output_weights = tf.get_variable(\n      \"output_weights\", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    output_bias = tf.get_variable(\n      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n\n    with tf.variable_scope(\"loss\"):\n\n        # Dropout helps prevent overfitting\n        output_layer = tf.nn.dropout(output_layer, keep_prob=0.8)\n\n        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        # Convert labels into one-hot encoding\n        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n        # If we're predicting, we want predicted labels and the probabiltiies.\n        if is_predicting:\n            return (predicted_labels, log_probs, output_layer1)\n\n        # If we're train\/eval, compute loss between predicted and actual label\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        \n        return (loss, predicted_labels, log_probs)","86263a33":"def model_fn_builder(num_labels, learning_rate, num_train_steps,\n                     num_warmup_steps):\n    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n    \n    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n        input_ids = features[\"input_ids\"]\n        input_mask = features[\"input_mask\"]\n        segment_ids = features[\"segment_ids\"]\n        label_ids = features[\"label_ids\"]\n\n        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n\n        # TRAIN and EVAL\n        if not is_predicting:\n\n            (loss, predicted_labels, log_probs) = create_model(\n            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n            train_op = optimization.create_optimizer(\n              loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n\n            # Calculate evaluation metrics. \n            def metric_fn(label_ids, predicted_labels):\n                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n                true_pos = tf.metrics.true_positives(\n                    label_ids,\n                    predicted_labels)\n                true_neg = tf.metrics.true_negatives(\n                    label_ids,\n                    predicted_labels)   \n                false_pos = tf.metrics.false_positives(\n                    label_ids,\n                    predicted_labels)  \n                false_neg = tf.metrics.false_negatives(\n                    label_ids,\n                    predicted_labels)\n\n                return {\n                    \"eval_accuracy\": accuracy,\n                    \"true_positives\": true_pos,\n                    \"true_negatives\": true_neg,\n                    \"false_positives\": false_pos,\n                    \"false_negatives\": false_neg,\n                    }\n\n            eval_metrics = metric_fn(label_ids, predicted_labels)\n\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                return tf.estimator.EstimatorSpec(mode=mode,\n                  loss=loss,\n                  train_op=train_op)\n            else:\n                return tf.estimator.EstimatorSpec(mode=mode,\n                    loss=loss,\n                    eval_metric_ops=eval_metrics)\n        else:\n            (predicted_labels, log_probs, output_layer) = create_model(\n            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n            predictions = {\n              'probabilities': log_probs,\n              'labels': predicted_labels,\n              'pooled_output': output_layer\n            }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n    # Return the actual model function in the closure\n    return model_fn","67d0c3fb":"BATCH_SIZE = 16\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 1.0\n# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\nWARMUP_PROPORTION = 0.1\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 300\nSAVE_SUMMARY_STEPS = 100\n\n# Compute train and warmup steps from batch size\nnum_train_steps = int(len(train_features) \/ BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\n# Specify output directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir=OUTPUT_DIR,\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n\n# Specify output directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir=OUTPUT_DIR,\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)","46874195":"num_train_steps, len(label_list)\n","9be3f087":"model_fn = model_fn_builder(\n  num_labels=len(label_list),\n  learning_rate=LEARNING_RATE,\n  num_train_steps=num_train_steps,\n  num_warmup_steps=num_warmup_steps)\n\nestimator = tf.estimator.Estimator(\n  model_fn=model_fn,\n  config=run_config,\n  params={\"batch_size\": BATCH_SIZE})","5d1a330a":"train_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=False)\n\n# Create an input function for validating. drop_remainder = True for using TPUs.\nval_input_fn = run_classifier.input_fn_builder(\n    features=val_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)","1c6c55b3":"# Training\nprint(f'Beginning Training!')\ncurrent_time = datetime.now()\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint(\"Training took time \", datetime.now() - current_time)","ed0f010f":"#Evaluating the model with Validation set\nestimator.evaluate(input_fn=val_input_fn, steps=None)","930099cf":"Get unique labels","0c515046":"### Getting train and validation set as dataframe","5f6bd85d":"#### 2. Converting the train and validation features to InputFeatures that BERT understands.","e2c506f7":"### Encode the labels into numeric","b6667f22":"### Split into 80% training and 20% validation","4ce27340":"Checking the shape of the dataframe","fa58652b":"### Get the data as dataframe","c38dc87e":"### Check shape of train and validation data","55d6acb7":"### Initializing the model and the estimator\n","6eb24b5e":"Getting bert functions","ea40a597":"### Validation","2696bef8":"### Fine tuning the BERT model\n\nThe BERT model can be applied for any kind of classification task by fine-tuning it.","b396f6a9":"### Processing text for BERT model\n\n","0182e63e":"Checking if null values exist","2bc77831":"#### 1. Preparing the input data, i.e create **InputExample** using the BERT\u2019s constructor.","5f50d8d8":"### Creating prediction model","ea478e2e":"### Read the training dataset","53085147":"### Training the model","fd63a5e7":"Setting output directory for BERT","2498f6a7":"### Import necessary libraries","7bcc01c2":"### Perform preprocessing to text"}}