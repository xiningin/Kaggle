{"cell_type":{"34e7fe6b":"code","b8b12c36":"code","829f6c68":"code","48114aa1":"code","7f087067":"code","781cd2e7":"markdown","fab261ee":"markdown","989e4275":"markdown","0fd7180f":"markdown","70ff2bdf":"markdown","3bbfba0b":"markdown","1100af66":"markdown"},"source":{"34e7fe6b":"#as an exmaple let's form a distribution by randomly collecting the samples.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as ss \nimport matplotlib.pyplot as plt\nfrom statistics import mean\nlst=[]\nimport random\nfor i in range(100):\n  lst.append(random.uniform(0,100)) #generates random numbers between 0-100\nsns.histplot(data=lst, kde=True)","b8b12c36":"def Average(lst):\n    return mean(lst)\nlst1=[]\nfor i in range(10000):\n  k=random.sample(lst,10)\n  lst1.append(Average(k))\nsns.histplot(data=lst1, kde=True)","829f6c68":"lst_bin=[1 for i in range(60)] + [0 for i in range(140)]\nprint(lst_bin)\nlst1=[]\nfor i in range(500):\n    k=random.sample(lst_bin,100)\n    lst1.append(Average(k))\nsns.histplot(data=lst1, kde=True)","48114aa1":"r=ss.skewnorm.rvs(a=27,size=1500)# a parameter decides the skewness with 0 being normal distribution\nsns.histplot(data=r)","7f087067":"lst_skew=[]\nfor i in range(1000):\n  k=random.sample(list(r),10)\n  lst_skew.append(Average(k))\nsns.histplot(data=lst_skew, kde=True)","781cd2e7":"**<font size =\"5\">Now, as per the definition.If I randomly collect samples and take their averages and form a data then the histplot of that data will be normally distributed irrespective of how the initially distribution is.<\/font>**","fab261ee":"# CLT for skewed data\n<font size=\"5\">Now let's see how does this theorem work on skewed data<\/font>","989e4275":"# CLT for binary distribution\n> <font size=\"5\">Suprisingly the Central Limit Theroum also works for Binary level data with a condition attached to it.<\/font>\n\n<font size=\"4\">**CONDITION:**The Central Limit Theorem applies even to binomial populations like this provided that the minimum of np and n(1-p) is at least 5, where \"n\" refers to the sample size, and \"p\" is the probability of \"success\" on any given trial.<\/font> ","0fd7180f":"<font size=\"5\">Let's take an example and verify this:<\/font>\n>  **sample size =200**\n    \n> **success probability =0.3(30 percent population classified as yes or success)**\n    \n>  **np=200*0.3=60**\n    \n>  **n(1-p)=200*0.7=140**\n    \n> **Now min(50,140) >=5 so this should satisfy the central theorem.**\n\n![image.png](attachment:159460f5-4a9c-4412-9d4a-35b3d39d25b1.png)","70ff2bdf":"<font size=\"4\">**It still gives Normal Distribution**<\/font>","3bbfba0b":"\n# <font size =\"4\"> Central Limit theorem looks so simple yet so fascinating and powerful.<\/font>\n\n\n<font size ='3'><div class=\"alert alert-block alert-info\">Central Limit theorem states that if you have a distribution with mean(\u00b5) and standard deviation (\u03c3) (assuming that you have sufficiently large samples in the distribution) then the distribution of the average of sample means will be approximately normally distributed.<\/div><\/font>","1100af66":"**This one almost looks normally distributed. This is what we are talking about**"}}