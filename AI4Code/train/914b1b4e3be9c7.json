{"cell_type":{"02ff4b68":"code","42427fb2":"code","6e37c2c2":"code","5747416f":"code","2ba6b2e8":"code","a55a3f25":"code","fa589273":"code","7486334e":"code","b9df31e0":"code","b2ef06af":"code","776cfe45":"code","7be012d2":"code","5c0eb7d0":"code","40765566":"code","daf0e953":"code","3da08824":"code","e4e614ff":"code","dc419ebd":"code","6f0033c9":"code","13b810c5":"code","ace8a73b":"code","cc13f90d":"code","5587d65d":"code","18571d36":"markdown","b9d153c7":"markdown","22d8c3ab":"markdown","47b51a68":"markdown","10f24008":"markdown","9caefc70":"markdown","8bddf78f":"markdown","39bb2a02":"markdown","3191a8f6":"markdown","6319e82c":"markdown"},"source":{"02ff4b68":"import pandas as pd\nimport os\nimport numpy as np\nnp.set_printoptions(threshold=np.inf)\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import normalize","42427fb2":"def load_dataset():\n    csv_path = os.path.join(\"..\/input\/credit-card-customers\/BankChurners.csv\")\n    return pd.read_csv(csv_path)\n\nbank_data = load_dataset()","6e37c2c2":"bank_data = bank_data.drop(columns=[\n    \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\",\n    \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\"\n])","5747416f":"X = bank_data.drop(\"Attrition_Flag\", axis=1)\nY = bank_data[\"Attrition_Flag\"]","2ba6b2e8":"folds = StratifiedKFold(n_splits=3)\n\nfor train_index, test_index in folds.split(X,Y):\n    X_train, X_test = X.loc[train_index], X.loc[test_index]\n    Y_train, Y_test = Y[train_index], Y[test_index]","a55a3f25":"cat_att = [\n    \"Gender\",\n    \"Education_Level\",\n    \"Marital_Status\",\n    \"Income_Category\",\n    \"Card_Category\"\n]\n\ncolumn_trans = make_column_transformer(\n    (\n        OneHotEncoder(), cat_att\n    ),\n    remainder=\"passthrough\"\n)","fa589273":"final_X_train = column_trans.fit_transform(X_train)\nfinal_X_test = column_trans.fit_transform(X_test)","7486334e":"cat_to_num = {\n    \"Attrition_Flag\": {\"Existing Customer\": 0, \"Attrited Customer\": 1}\n}\n\nY_train, Y_test = Y_train.to_frame(), Y_test.to_frame()\n\nfinal_Y_train = Y_train.replace(cat_to_num)\nfinal_Y_test = Y_test.replace(cat_to_num)\n\nfinal_Y_train = final_Y_train.values\nfinal_Y_test = final_Y_test.values","b9df31e0":"n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# rfc_max_features = [\n#     \"auto\", \n#     \"sqrt\", \n#     \"log2\"\n# ]\n\n# rfc_class_weight = [\n#     \"balanced\",\n#     \"balanced_subsample\"\n# ]\n\nrfc_max_depth = [10, 20]\n\nrfc_min_sample_split = [2, 5]\n\n\nrfc_params = {\n    'n_estimators': n_estimators,\n    # \"class_weight\": rfc_class_weight,\n    # \"max_features\": rfc_max_features,\n    \"max_depth\": rfc_max_depth,\n    \"min_samples_split\": rfc_min_sample_split\n    }","b2ef06af":"rfc_gs = GridSearchCV(\n    estimator= RandomForestClassifier(),\n    param_grid= rfc_params,\n    cv= 5   \n)","776cfe45":"# svm_C = [1, 10, 20]\n# kernel = [\"rbf\", \"linear\"]\n\n# svm_param = {\n#     \"C\": svm_C,\n#     \"kernel\": kernel\n# }","7be012d2":"# svm_gs = GridSearchCV(\n#     estimator= svm.SVC(),\n#     param_grid= svm_param,\n#     cv= 5\n# )","5c0eb7d0":"# lr_C = [1, 10, 20]\n# solver = [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n\n# lr_param = {\n#     \"C\": lr_C,\n#     \"solver\": solver\n# }","40765566":"# lr_gs = GridSearchCV(\n#     estimator= LogisticRegression(),\n#     param_grid= lr_param,\n#     cv= 5\n# )","daf0e953":"rfc_gs.fit(final_X_train, final_Y_train)\n# svm_gs.fit(final_X_train, final_Y_train)\n# lr_gs.fit(final_X_train, final_Y_train)","3da08824":"rfc_gs_results = pd.DataFrame(rfc_gs.cv_results_)\n# svm_gs_results = pd.DataFrame(svm_rs.cv_results_)\n# lr_gs_results = pd.DataFrame(lr_rs.cv_results_)","e4e614ff":"rfc_gs_results = rfc_gs_results[[\n    \"param_n_estimators\",\n    # \"param_class_weight\",\n    # \"param_max_features\",\n    \"param_max_depth\",\n    \"param_min_samples_split\",\n    \"mean_test_score\"\n]]","dc419ebd":"# svm_gs_results = svm_gs_results[[\n#     \"param_kernel\",\n#     \"param_C\",\n#     \"mean_test_score\"\n# ]]","6f0033c9":"# lr_gs_results = lr_gs_results[[\n#     \"param_solver\",\n#     \"param_C\",\n#     \"mean_test_score\"\n# ]]","13b810c5":"rfc_Y_pred = rfc_gs.predict(final_X_test)\n# svm_Y_pred = svm_gs.predict(final_X_test)\n# lr_Y_pred = lr_gs.predict(final_X_test)","ace8a73b":"rfc_score = accuracy_score(final_Y_test, rfc_Y_pred)\n# svm_score = accuracy_score(final_Y_test, svm_Y_pred)\n# lr_score = accuracy_score(final_Y_test, lr_Y_pred)","cc13f90d":"# (tn, fp, fn, tp)\nrfc_conf_matrix = confusion_matrix(final_Y_test, rfc_Y_pred)\n# svm_conf_matrix = confusion_matrix(final_Y_test, svm_Y_pred).ravel()\n# lr_conf_matrix = confusion_matrix(final_Y_test, lr_Y_pred).ravel()","5587d65d":"models = [\n    \"Random Forest Classifier\",\n    # \"Support Vector Machines\",\n    # \"Logestic Regression\"\n]\n\nbest_params = [\n    rfc_gs.best_params_,\n    # svm_gs.best_params_,\n    # lr_gs.best_params_\n]\n\nacc_scores = [\n    rfc_score,\n    # svm_score,\n    # lr_score\n]\n\nconf_matrices = [\n    rfc_conf_matrix,\n    # svm_conf_matrix,\n    # lr_conf_matrix\n]\n\n# for i in range(0, 3):\nprint(\"Model: {}\".format(models[0]))\nprint(\"Best Params: {}\".format(best_params[0]))\nprint(\"Accuracy Score: {}\".format(acc_scores[0]))\ntn, fp, fn, tp = conf_matrices[0].ravel()\nprint(\"Confusion Matrix: {}\".format((tn, fp, fn, tp)))\n    # print(\"\\n\\n\")","18571d36":"We will be tuning the n_estomators, class_weight, max_features, max_depth, and min_sample_split parameters of Random Forest Classifier. \n\nThen use GridSearchCV to, put simply, find the best combination of the those parameters that will give us the best MEAN score using the train set. ","b9d153c7":"The Attrition_Flag attribute will be the labels for the data. \n\nWe will use OneHotEncoder() to change the categorical attributes to numarical types. ","22d8c3ab":"After fitting the train data, we can see the results of using the different combinations of parameters. ","47b51a68":"# Conlusion\n\n\nAt the beginning these were my results: \n\nModel: Random Forest Classifier\nBest Params: {'min_samples_leaf': 1, 'n_estimators': 1200}\nAccuracy Score: 0.9013333333333333\nConfusion Matrix: (2823   10  323  219)\n\n\nI think we have slightly improved. We now have:\n\nModel: Random Forest Classifier\nBest Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 1200}\nAccuracy Score: 0.9016296296296297\nConfusion Matrix: (2825, 8, 324, 218)\n\n\n\nThats all, hopefully someone found this useful lol\n \n\n\nThank you :) ","10f24008":"## predict the values \n\nHere we are going to predict the labels using the test set.\n\nThen using accuracy_score and the confusion matrix to evaluate the models performance. ","9caefc70":"Here are the results of our model. \n\nWe are printing the model name. The models best parameters, since GridSearchCV has an attribute to display the best params. The models accuracy score. The models confusion matrix. \n\nThe confusion matrix is in the form of ==> (true negative, false positive, false negative, true positive) ","8bddf78f":"The next four (4) cells is similar code to prepare the hyper parameters for tuning. The first two (2) cells are for support vector machine model (SVM) and the next two (2) after that is for a logestic regression model. I commented them out becasue, again, computation time reasons. But, just simply uncomment them and use it just how i used the Random Forest Classifier. :)","39bb2a02":"# Introduction \n\nThis notebook will not have too many markdowns explaining whats going on. I will get straight to the point and if certain things you need more information\/explanation about, then Google or YouTube it :) \n\nHere we are working with a dataset of around 10,000 credit card customers. Our goal is to predict which customer is going to get churned. \n\nThis is a classification problem. We are also working with labeled data so it will be a supervised learning. Thus, we are going to work with Random Forest Classifier. \n\nNOTE: The code for using logistic regression and support vector machine is included. it is just commented out because my computer cant fit\/train the models in a reasonable amount of time. \n\nWe are going to pre-process the data and split it into a train set, and a test set using make_column_transformation and StratifiedKFold. Then we will use GridSearchCV to tune the hyper parameters of a Random Forest Classifier. We will fit the model using our training sets. Finally, use our test set to put our model to worl. We will be using accuracy_score and a confusion matrix to evaluate our model at the end. \n\nEnjoy :) ","3191a8f6":"## fitting the data ","6319e82c":"## Random Forest Classifier \n\nWe will first define our hyper parameters that we want to tune. Some of the parameters are commented out. That is just because it takes FOREVER to train the model because the way GridSearchCV works. Basically GridSearchCV is an exhaustive search technique where it tries every combination of the different parameters. So as you can tell, the number of combinations can be super high and i am not planning on putting my laptop through that lol. "}}