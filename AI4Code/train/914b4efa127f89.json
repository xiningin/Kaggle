{"cell_type":{"a6521a5b":"code","9ca08b15":"code","637328bc":"code","ce5a5158":"code","5b185107":"code","217e0268":"code","f8495b57":"code","87e09452":"code","40da9d28":"code","76952f69":"code","a53a8f2e":"code","4bfe115c":"code","aec98154":"code","3f12f04e":"code","19db8c9a":"code","c2573c94":"code","1a2a624a":"code","ff36e494":"code","004c438b":"code","67665341":"code","861cc3af":"code","83f9f480":"code","956dac8b":"code","f9a628e7":"code","771395c3":"code","8a9612ec":"code","7d7ab389":"code","cf3511d7":"code","d86fa4c6":"code","b359c43e":"code","0afac91f":"code","e5e7fc29":"code","cd46c00f":"code","b9b925ea":"code","7a8a03a1":"markdown","9762c914":"markdown","780000a2":"markdown"},"source":{"a6521a5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ca08b15":"# XGBoost on Otto dataset, Tune n_estimators\nfrom pandas import read_csv\nimport os\nimport pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib\n#matplotlib.use('Agg')\nfrom matplotlib import pyplot\nimport time\nimport numpy as np\n# load data\n\n#df_basedata_train_0 = pd.read_csv(r'C:\\Vibhaas\\Artist of Analytics\\Algorithms\\XGBoost\\Otto_train.csv')\ndf_basedata_train_0 = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf_basedata_train_0.head()","637328bc":"df_basedata_train_0.shape","ce5a5158":"df_basedata_train_0.info()","5b185107":"# Converting Total Charges to a numerical data type.\ndf_basedata_train_0.TotalCharges = pd.to_numeric(df_basedata_train_0.TotalCharges, errors='coerce')\ndf_basedata_train_0.info()\n","217e0268":"# Find Categorical Variables\ns = (df_basedata_train_0.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(\"Categorical variables:\")\nprint(object_cols)","f8495b57":"df_1 = pd.get_dummies(df_basedata_train_0, columns=[\"gender\",\"Partner\",\"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\",\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\", \"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"Contract\",\"PaperlessBilling\",\"PaymentMethod\", \"Churn\"   ],drop_first=True)\ndf_1.head()","87e09452":"df_1.isnull().sum()","40da9d28":"#Removing missing values \ndf_1.dropna(inplace = True)","76952f69":"df_1.isnull().sum()","a53a8f2e":"df_1 = df_1.rename(columns={'Churn_Yes': 'target'})\n\n## Drop ID Features\ndf_2=df_1.drop(['customerID'],axis=1)\ndf_2.head()","4bfe115c":"\ncorr = df_2.corr().tail(1)\ncorr.sort_values(by='target',axis=1)\n\n#import for visualization\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig, ax = plt.subplots(figsize=(20,5))\n\nplt.title(\"Correlation Plot\")\nplt.xlabel(\"Independent Vairables\")\nplt.ylabel(\"Correlation with Dependent Variable\")\nplt.xticks(rotation=90)\n\nax = sns.barplot(data=corr.sort_values(by='target',axis=1))\n\n\n#import for visualization\nimport seaborn as sns\nf,ax = plt.subplots(figsize=(35, 1))\nsns.heatmap(df_2.corr().tail(1).sort_values(by='target',axis=1), annot=True, linewidths=0.5, fmt='.2f',ax=ax)","aec98154":"# Machine Learning \n\nX = df_2.drop(\"target\", axis=1)\ny = df_2[\"target\"]","3f12f04e":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=50, max_features='sqrt')\nclf = clf.fit(X, y)\n\n#have a look at the importance of each feature.\nfeatures = pd.DataFrame()\nfeatures['feature'] = X.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(10, 10))","19db8c9a":"# Standardize\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nscale.fit(X)\npredictor_standard = scale.transform(X)\n","c2573c94":"from sklearn.model_selection import train_test_split\n# Split data to 80% training data and 20% of test to check the accuracy of our model\nX_train, X_test, y_train, y_test = train_test_split(predictor_standard, y, test_size=0.20, random_state=0)\n\n","1a2a624a":"#Model\n\nfrom sklearn.svm import SVC\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport time\n","ff36e494":"#create an array of models\nmodels = []\nmodels.append((\"LR\",LogisticRegression()))\nmodels.append((\"NB\",GaussianNB()))\nmodels.append((\"RF\",RandomForestClassifier()))\nmodels.append((\"SVC\",SVC()))\nmodels.append((\"Dtree\",DecisionTreeClassifier()))\nmodels.append((\"XGB\",xgb.XGBClassifier()))\nmodels.append((\"KNN\",KNeighborsClassifier()))\n\nresult = []\n#measure the accuracy \nfor name,model in models:\n    start_time = time.time()\n    kfold = KFold(n_splits=5, random_state=22)\n    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n    print(name, cv_result)\n    print(\"-\"*5,name, \" Mean accuracy of cross-validation: \", format(round(cv_result.mean(),4)))\n    execution_time = (time.time() - start_time)\n    result.append((name,cv_result.mean(), execution_time ))\n    print(name,\"--- %s seconds ---\" % (time.time() - start_time))\n\npd.DataFrame(result)\n    \ndf = pd.DataFrame(result, columns =['Algo', 'Accuracy', 'execution_time']) \ndf \n\n\n","004c438b":"# sort df by Count column\npd_df = df.sort_values(['Accuracy']).reset_index(drop=True)\nprint (pd_df)","67665341":"\nplt.figure(figsize=(8,8))\n# plot bar chart with index as x values\nax = sns.barplot(pd_df.index, pd_df.Accuracy)\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nax.set(xlabel=\"Algo\", ylabel=\"Acurracy\")\n# add proper Dim values as x labels\n\nax.set_xticklabels(pd_df.Algo)\nfor item in ax.get_xticklabels(): item.set_rotation(90)\nfor i, v in enumerate(pd_df[\"Accuracy\"].iteritems()):        \n    ax.text(i ,v[1], \"{:,}\".format(v[1]), color='m', va ='bottom', rotation=45)\nplt.tight_layout()\nplt.show()\n","861cc3af":"from sklearn.model_selection import train_test_split\n# Split data to 80% training data and 20% of test to check the accuracy of our model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n\n\n#Model\n\nfrom sklearn.svm import SVC\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport time\n\n\n#create an array of models\nmodels = []\nmodels.append((\"LR\",LogisticRegression()))\nmodels.append((\"NB\",GaussianNB()))\nmodels.append((\"RF\",RandomForestClassifier()))\nmodels.append((\"SVC\",SVC()))\nmodels.append((\"Dtree\",DecisionTreeClassifier()))\nmodels.append((\"XGB\",xgb.XGBClassifier()))\nmodels.append((\"KNN\",KNeighborsClassifier()))\n\nresult = []\n#measure the accuracy \nfor name,model in models:\n    start_time = time.time()\n    kfold = KFold(n_splits=5, random_state=22)\n    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n    print(name, cv_result)\n    print(\"-\"*5,name, \" Mean accuracy of cross-validation: \", format(round(cv_result.mean(),4)))\n    execution_time = (time.time() - start_time)\n    result.append((name,cv_result.mean(), execution_time ))\n    print(name,\"--- %s seconds ---\" % (time.time() - start_time))\n\npd.DataFrame(result)\n    \ndf = pd.DataFrame(result, columns =['Algo', 'Accuracy', 'execution_time']) \ndf \n\n\n# sort df by Count column\npd_df = df.sort_values(['Accuracy']).reset_index(drop=True)\npd_df","83f9f480":"plt.figure(figsize=(8,8))\n# plot bar chart with index as x values\nax = sns.barplot(pd_df.index, pd_df.Accuracy)\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nax.set(xlabel=\"Algo\", ylabel=\"Acurracy\")\n# add proper Dim values as x labels\n\nax.set_xticklabels(pd_df.Algo)\nfor item in ax.get_xticklabels(): item.set_rotation(90)\nfor i, v in enumerate(pd_df[\"Accuracy\"].iteritems()):        \n    ax.text(i ,v[1], \"{:,}\".format(v[1]), color='m', va ='bottom', rotation=45)\nplt.tight_layout()\nplt.show()\n","956dac8b":"from sklearn.model_selection import train_test_split\n# Split data to 80% training data and 20% of test to check the accuracy of our model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n","f9a628e7":"data_dmatrix = xgb.DMatrix(data=X,label=y)","771395c3":"import xgboost as xgb\nxg_clas = xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1, max_depth = 5, alpha = 10, n_estimators = 10)","8a9612ec":"xg_clas.fit(X_train,y_train)\n\ny_pred = xg_clas.predict(X_test)","7d7ab389":"from sklearn.metrics import accuracy_score\nscore = accuracy_score(y_test, y_pred)\nprint(\"Accuracy is \" + str(round((score*100), 2))+\"%\")","cf3511d7":"import matplotlib.pyplot as plt\n\nxgb.plot_tree(xg_clas,num_trees=0)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()","d86fa4c6":"params = {\"objective\":\"reg:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,  'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3, num_boost_round=50,early_stopping_rounds=10, as_pandas=True, seed=123)","b359c43e":"cv_results.head()\n","0afac91f":"import matplotlib.pyplot as plt\n\nxgb.plot_tree(xg_reg,num_trees=0)\nplt.rcParams['figure.figsize'] = [50, 50]\nplt.show()","e5e7fc29":"from xgboost import plot_tree\nimport matplotlib.pyplot as plt\n\n# plot single tree\nplot_tree(xg_reg)\nplt.show()","cd46c00f":"xgb.plot_importance(xg_clas)\nplt.rcParams['figure.figsize'] = [10, 10]\n\n    \nplt.show()","b9b925ea":"for x in range(9):\n    xgb.plot_tree(xg_clas,num_trees=x)\n    #plt.rcParams['figure.figsize'] = [10, 10]\n    plt.show()","7a8a03a1":"**EDA**","9762c914":"Considerable size to tryout Classication problem.","780000a2":"**Attempt 3 XGBoost**"}}