{"cell_type":{"fa7be8ec":"code","425e834e":"code","76ab74c8":"code","c4bcde19":"code","f00d873a":"code","4c062edc":"code","28dfb0cc":"code","71f2d10e":"code","585cfe14":"code","97ce575c":"code","a6af9829":"code","80155621":"code","28943ec2":"code","4406a3c5":"code","aca958f0":"code","f861a8a7":"code","73851d5f":"code","93cbadb6":"code","da4cf74d":"code","cf4d4f87":"code","b10b9d09":"code","fd70c132":"code","4f943b8f":"code","31e47b7b":"code","2b0bb919":"code","57bcc0bc":"code","6c490dfa":"code","72b27df3":"markdown","865b893e":"markdown","b8a6b598":"markdown","7748a57e":"markdown","d04d26f0":"markdown","567a86eb":"markdown","d9b190cc":"markdown","992d439e":"markdown","2798c645":"markdown","64536958":"markdown"},"source":{"fa7be8ec":"import pandas as pd\n\npmc = pd.read_csv('..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_pmc.csv', delimiter=',', encoding='utf-8')\nnoncommon = pd.read_csv('..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_noncomm_use.csv', delimiter=',', encoding='utf-8')\ncommon_use = pd.read_csv('..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_comm_use.csv', delimiter=',', encoding='utf-8')","425e834e":"frames  = [pmc[['paper_id','title','abstract','text']],noncommon[['paper_id','title','abstract','text']],common_use[['paper_id','title','abstract','text']]]\narticles = pd.concat(frames)\narticles.head(5)\n","76ab74c8":"# install pke for keyphrase extraction\n!pip install git+https:\/\/github.com\/boudinfl\/pke.git","c4bcde19":"from pke.base import LoadFile\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nimport string\nfrom collections import defaultdict\nimport os\nimport sys\nimport gzip\n\nstoplist = list(stopwords.words('english')) + list(string.punctuation)\n\n\nfrequencies = defaultdict(int)\ndelimiter='\\t'\n\n# initialize number of documents\nnb_documents = 0\n\n\n#Note: im limit this item becuase is very time consumed task for my notebook\nlimite_dataset_article = 50\nif limite_dataset_article is not None:\n    my_df_articles = articles.head(limite_dataset_article)\nelse :\n    my_df_articles = articles\n\noutput_file = 'df.tsv.gz'\nfor index,article in my_df_articles.iterrows():\n    content = str(article.title) + ' ' + str(article.abstract) + ' '+str(article.text)\n    doc = LoadFile()\n    doc.load_document(input=str(content))\n    # n is number of n-gram\n    doc.ngram_selection(n=3)\n    doc.candidate_filtering(stoplist=stoplist)\n     # loop through candidates\n    for lexical_form in doc.candidates:\n        frequencies[lexical_form] += 1\n    nb_documents += 1\n\n    if nb_documents % 10 == 0:\n        print(\"{} docs, memory used: {} mb\".format(nb_documents,\n                                                          sys.getsizeof(\n                                                              frequencies)\n                                                          \/ 1024 \/ 1024))\n# create directories from path if not exists\nif os.path.dirname(output_file):\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# dump the df container\nwith gzip.open(output_file, 'wb') as f:\n    # add the number of documents as special token\n    first_line = '--NB_DOC--' + delimiter + str(nb_documents)\n    f.write(first_line.encode('utf-8') + b'\\n')\n    for ngram in frequencies:\n        line = ngram + delimiter + str(frequencies[ngram])\n        f.write(line.encode('utf-8') + b'\\n')\n\n","f00d873a":"from pke.unsupervised import KPMiner\nfrom pke import load_document_frequency_file\n\ndef phrase_extraction(input_text):\n    extractor = KPMiner()\n    extractor.load_document(input=input_text, language='en')\n    lasf = 1\n    cutoff = 400\n    extractor.candidate_selection(lasf=lasf, cutoff=cutoff, stoplist=stoplist)\n\n    # load document frequency file\n    df = load_document_frequency_file(input_file='df.tsv.gz')\n    alpha = 2.3\n    sigma = 3.0\n    try:\n        extractor.candidate_weighting( alpha=alpha, sigma=sigma,df=df)\n    except Exception as es :\n        print(es)\n    keyphrases = extractor.get_n_best(n=20)\n    return keyphrases\n\n\n#Note: im limit this item becuase is very time consumed task for my notebook\nlimite_dataset_article = 5\nif limite_dataset_article is not None:\n    my_df_articles = articles.head(limite_dataset_article)\nelse :\n    my_df_articles = articles\n\nfor index,article in my_df_articles.iterrows():\n    content = str(article.title) + ' ' + str(article.abstract) + ' '+str(article.text)\n    print('ArticleId: ' , article.paper_id)\n    key_phrases = phrase_extraction(content)\n    print(key_phrases)\n    print('*'*100)\n\n","4c062edc":"#read Article KeyPhrase and Article Title\nimport pandas as pd\n\ndf = pd.read_csv('..\/input\/covid19keyphraseextraction\/Covid-19Keywords.csv', delimiter=',', encoding='utf-8')\ntitle_abs_df = pd.read_csv('..\/input\/covid19titlesabstract\/titles_abs.csv', delimiter=',', encoding='utf-8')","28dfb0cc":"df.head(5)","71f2d10e":"title_abs_df.head(5)","585cfe14":"#prepare Graph Nodes \nnodes = pd.DataFrame()\ndf1 = df.rename(columns={\"Keywords\": \"name\"})\nnodes['name'] = df1.name.unique()\nnodes['nodeid'] = nodes.index.values\n# if want filter document that contains specific keyphrase \nfilter_doc_by_keyphrase=[]\n\nif len(filter_doc_by_keyphrase) != 0 :\n    query =' or '.join(['name.str.contains(\"{}\")'.format(word) for word in filter_doc_by_keyphrase])\n    filter = nodes.query(query)\n    df2 = pd.merge(df1, filter, on='name', how='inner')\n    filtered_df = pd.merge(df1, df2, on='ArticleId', how='inner')\n    nodes =  pd.DataFrame()\n    nodes['name'] = filtered_df.name_x.unique()\n    nodes['nodeid'] = nodes.index.values\n\ndf1","97ce575c":"\nfrom wordcloud import WordCloud\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n\n#mpl.rcParams['figure.figsize']=(8.0,6.0)    #(6.0,4.0)\nmpl.rcParams['font.size']=12               #10 \nmpl.rcParams['savefig.dpi']=1000             #72 \nmpl.rcParams['figure.subplot.bottom']=.1 \ndef plot_wordcloud(words_df,count_words = 200,keyrank=5,title = 'Most common top  keyphrases in the articles '):\n    words_df = words_df.query('KeyRank < {}'.format(keyrank))\n    words_df = words_df[['ArticleId','name']]\n    frequency = words_df.groupby('name')['ArticleId'].count().reset_index()\n    frequency = frequency.sort_values(by='ArticleId', ascending=False).head(count_words)\n    frequencies = {}\n    for index,word in frequency.iterrows():\n        keyword = word[0].replace(' ','_')\n        count = word[1]\n        frequencies[keyword] = count\n    wordcloud = WordCloud(min_font_size=12,max_font_size=80 ,width=1200, height=800).generate_from_frequencies(frequencies)\n    plt.figure(figsize=(15,10))\n    \n    plt.title(title, fontdict={'size': 22, 'color': 'green', \n                                      'verticalalignment': 'bottom'})\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n","a6af9829":"plot_wordcloud(df1,title = 'Most common top 5 keyphrases in all articles text')","80155621":"def filter_df(filter_doc_by_keyphrase):\n    query =' or '.join(['name ==\"{}\" '.format(word) for word in filter_doc_by_keyphrase])\n    dff = df1[['name']]\n    filter = dff.query(query)\n    df2 = pd.merge(df1, filter, on='name', how='inner')\n    filtered_df = pd.merge(df1, df2, on='ArticleId', how='inner')\n    result = filtered_df.rename(columns={\"KeywordsId_x\": \"KeywordsId\",\"name_x\":\"name\",\"KeyRank_x\":\"KeyRank\",\"StemmedKeywords_x\":\"StemmedKeywords\",\"KpminerKeywordsWeight_x\":\"KpminerKeywordsWeight\"})\n    result = result[[\"KeywordsId\",\"ArticleId\",\"name\",\"StemmedKeywords\",\"KpminerKeywordsWeight\",\"KeyRank\"]]\n    return result","28943ec2":"filter_doc_by_keyphrase = ['covid-19','sars-cov-2','coronavirus disease 2019','covid-19 outbreak','covid-19 patients','2019-ncov infection','covid-19 epidemic','sars-cov-2 infection','covid-19 cases','2019-ncov outbreak']\n\ncovid_19df = filter_df(filter_doc_by_keyphrase)\nplot_wordcloud(covid_19df,keyrank=10,title = 'Most common top 5 keyphrases in article that covid-19 in main topic')","4406a3c5":"filter_doc_by_keyphrase = ['public health','global health','population health']\n\ncovid_19df = filter_df(filter_doc_by_keyphrase)\nplot_wordcloud(covid_19df,keyrank=5,title = 'Most common top 10 keyphrases in articles with public health topic ')","aca958f0":"\n#The minimum number of times that two words have appeared in different articles as important key phrases\ncoo_occurence_count = 4\n\n# max keyphrase rank in each article\nkeyranks = 15\n\n#prepare Graph Edges\ndf1 = df[['ArticleId','Keywords','KeyRank']]\ndf1 = df1.rename(columns={\"Keywords\": \"name\"})\ndf1 = pd.merge(df1,nodes,on='name',how='inner')\ndf1 = df1.query('KeyRank < {}'.format(keyranks))\ndf1 = df1[['ArticleId','nodeid']]\ndf2 = df1\ndf3 = pd.merge(df1, df2, on='ArticleId')\nco_matrix = df3.query('nodeid_x != nodeid_y ')\nco_matrix = co_matrix.groupby(['nodeid_x' , 'nodeid_y']).count().reset_index()\nimportant_relation = co_matrix.query('ArticleId > {}'.format(coo_occurence_count))\nedges = important_relation.rename(columns={\"nodeid_x\": \"source\",\"nodeid_y\":\"target\",\"ArticleId\":\"weight\"})\n","f861a8a7":"#Set Node and Edges in networkx \nimport networkx as nx\nG = nx.from_pandas_edgelist(edges, 'source', 'target', ['weight'])\nnx.set_node_attributes(G,  pd.Series(nodes.name, index=nodes.nodeid).to_dict(),'name')\n#label_dict = pd.Series(nodes.name, index=nodes.nodeid).to_dict()\n","73851d5f":"#Generates community sets determined by label propagation\nfrom networkx.algorithms.community import label_propagation_communities\nc = list(label_propagation_communities(G))\nnode_class = {}\nj = 0\nfor part in c :\n    if len(part) \/ len(G.nodes) > 0.01 :\n        j += 1\n        for node in part:\n            node_class[node] = j\n    else:\n        for node in part:\n             node_class[node] = 0","93cbadb6":"#Use Eigenvector Centrality to measure of the influence of a node in a network\ncentrality = nx.eigenvector_centrality(G)\nnodes_centerality = {}\nfor v, c in centrality.items() :\n    nodes_centerality[v] = c","da4cf74d":"# use pyvis to build a python based approach to constructing and visualizing network\n!pip install pyvis","cf4d4f87":"!pip install fa2","b10b9d09":"#ForceAtlas2 is a force-directed layout close to other algorithms used for network spatialization\nfrom fa2 import ForceAtlas2\nforceatlas2 = ForceAtlas2(\n                        # Behavior alternatives\n                        outboundAttractionDistribution=True,  # Dissuade hubs\n                        linLogMode=False,  # NOT IMPLEMENTED\n                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n                        edgeWeightInfluence=0.5,\n\n                        # Performance\n                        jitterTolerance=1.0,  # Tolerance\n                        barnesHutOptimize=True,\n                        barnesHutTheta=1.3,\n                        multiThreaded=False,  # NOT IMPLEMENTED\n\n                        # Tuning\n                        scalingRatio=5.0,\n                        strongGravityMode=False,\n                        gravity=1.0,\n\n                        # Log\n                        verbose=True)\n#get Graph  nodes position \npositions = forceatlas2.forceatlas2_networkx_layout(G, pos=None, iterations=2000)\n\n","fd70c132":"#load graph data to pyvis network \nfrom pyvis.network import Network\n\n#create pyvis netwoek graph\nGraph = Network(height=\"550px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\",notebook=True)\n\n#node class color \ncolors = [ '#6a81f9', '#00cccc',  '#f9ac26',  '#ac26f9',  '#26f9ad',  '#f92672',  '#bb9990',  '#dda0dd',  '#ffc0cb',  '#db7093',  '#da70d6',  '#ff4500',  '#ba55d3',\n          '#9370db',  '#ff34b3',  '#ffa07a',  '#ffec8b',  '#ff6a6a',  '#c1ffc1',  '#bf3eff',  '#9932cc',  '#00eeee',  '#8ee5ee',  '#98f5ff',  '#5f9ea0',  '#f7c297',\n          '#f6be90',  '#8a2be2',  '#f8c79e','#faebd7',  '#6495ed',  '#00ced1',  '#1e90ff',  '#ffb6c1',  '#f08080',  '#20b2aa',  '#2e8b57',  '#98fb98',  '#87ceeb',\n          '#9acd32',  '#ffd700',  '#00ff7f']\n\n# min max number normalizer for normalize graph algorithm output value\ndef normalize_number(value,min_actual,max_actual,min_range,max_range):\n    return (max_range - min_range) \/ (max_actual - min_actual) * (value - min_actual) + min_range\n\n\n# prepare node to pyvis network \nlabel_dict = pd.Series(nodes.name, index=nodes.nodeid).to_dict()\nsizes = []\nlabels = []\nnodes_colors = []\nx =[]\ny = []\ntitles = []\n\n# pyvis node graph scale\npyvis_min_node_size = 2\npyvis_max_node_size = 100\n\n# Eigenvector Centrality output min and max value\nmin_actual = min(nodes_centerality.values())\nmax_actual = max(nodes_centerality.values())\n\nfor node in G.nodes:\n    sizes.append(normalize_number(nodes_centerality[node],min_actual,max_actual,pyvis_min_node_size,pyvis_max_node_size))\n    labels.append(label_dict[node])\n    nodes_colors.append(colors[node_class[node]])\n    x.append(positions[node][0])\n    y.append(positions[node][1])\n    titles.append('')\nGraph.add_nodes(list(G.nodes), label=labels, size=sizes,x=x,y=y,color=nodes_colors,title = titles)\n","4f943b8f":"#pyvis edge prepare \npyvis_edges = []\nmin_weight = min(edges.weight.values).item(0)\nmax_weight = max(edges.weight.values).item(0)\nfor edge in G.edges :\n    weight = normalize_number(G.edges[edge[0],edge[1]]['weight'],min_weight,max_weight,1,30)\n    pyvis_edges.append([edge[0], edge[1],weight])\nGraph.add_edges(pyvis_edges)\nneighbor_map = Graph.get_adj_list()\n\n# get node adjacency neighbores\nfor node in Graph.nodes:\n    node[\"title\"] += \" Neighbors:<br>\" + \"<br>\".join([label_dict[nn] for nn in neighbor_map[node[\"id\"]]])\n    node[\"value\"] = len(neighbor_map[node[\"id\"]])","31e47b7b":"#graph  setting show \nGraph.force_atlas_2based(gravity=-550)\nGraph.options.interaction.hideEdgesOnDrag = True\nGraph.show_buttons(filter_=['','nodes','interaction'])\n\n#show graph \nGraph.show(\"Covid-19-graph.html\")\n#important setting in graph setting \n#change physics solver to forceAtlas2Based and  low gravitationalConstant \n#for fix node positin check True fix x,y","2b0bb919":"!pip install flashtext","57bcc0bc":"#example of graph nodes for found most realted artilce that entered node has co-occurrence \nwords = ['transmission','covid-19']","6c490dfa":"#print article that cotain specific keywords in graph \nfrom flashtext import KeywordProcessor\nfrom nltk.tokenize import sent_tokenize\n\nkeyword_processor = KeywordProcessor()\nkeyword_processor.case_sensitive=False\n\nif len(words) > 0:\n    for word in words:\n        keyword_processor.add_keyword(word, '\\033[0m\\033[43m{}\\033[0m'.format(word))\n    words_df = pd.DataFrame(words,columns=['Keywords'])\n    df1 = df[['ArticleId','Keywords','KeyRank']]\n    #keyranks is Keyphrase rank thershold when graph edge prepare\n    df1 = df1[df1['KeyRank']< keyranks]\n    df2 = pd.merge(df1, words_df, on='Keywords', how='inner')\n    Articlesdf = df2.groupby(['ArticleId'])['Keywords'].count().reset_index()\n    Articlesdf = Articlesdf.query('Keywords=={}'.format(len(words)))\n    result = pd.merge(df, Articlesdf, on='ArticleId', how='inner')\n    SortedArticlesDf =  pd.merge(df2, Articlesdf, on='ArticleId', how='inner')\n    SortedArticlesDf = SortedArticlesDf.groupby(['ArticleId'])['KeyRank'].sum().reset_index()\n    print('Found {} Articles from covid-19 Full text article dataset that this  words \"{}\" has in main keyphrase and words has co-occurence \\n\\n '.format(len(SortedArticlesDf),','.join(words)))\n    for article_id in SortedArticlesDf.sort_values(by='KeyRank', ascending=True)['ArticleId']:\n        print('ArticleId: ', article_id+'\\n')\n        article = result[result['ArticleId'] == article_id]\n        try:\n            title = title_abs_df[title_abs_df['ArticleId'] == article_id]['Title'].values[0]\n            title = keyword_processor.replace_keywords(title)\n            print('Article Title:',title+'\\n' )\n        except:\n            print('Article Title: \\n')\n\n        try:\n            abstract  = title_abs_df[title_abs_df['ArticleId'] == article_id]['Abstract'].values[0]\n            print('Article Abstract:')\n            for sentence in sent_tokenize(abstract):\n                print(keyword_processor.replace_keywords(sentence))\n        except:\n            print('Article Abstract: \\n')\n        print('\\n'+'Article Full Text Keywords : ')\n        keywords = ', '.join(article['Keywords_x'])\n        keywords = keyword_processor.replace_keywords(keywords)\n        print(keywords)\n        print('\\n'+'*' * 100)","72b27df3":"also we can filtered words co-occurence\n","865b893e":"Note : I have chosen a more optimistic way of selecting candidate words, because of some limitations, I put only final result of articles extracted keyphrases in the \"covid19keyphraseextraction\/Covid-19Keywords.csv\" path ","b8a6b598":"# Kpminer Compute Covid-19 Articles words DF \nCompute Document Frequency (DF) counts from a collection of documents. \nN-grams up to 3-grams are extracted and converted to their n-stems forms.\nThose containing a token that occurs in a stoplist are filtered out.\nOutput file is in compressed (gzip) tab-separated-values format (tsv.gz)","7748a57e":"# important setting when network graph html showing \n\n please schroll down graph ifram and set following settings:\n\n   **1.set fix node positin checkbox True in node setting  **\n   ![image.jpg](https:\/\/i.imgur.com\/yHIr6Ef.jpg)\n\n   **2.in interaction section set navigationButtons chekbox True   **\n\n   ![image.png](https:\/\/i.imgur.com\/iu6Dr8e.jpg)\n","d04d26f0":"# Prepare Data\n**Note : I use output of cord-19 eda parse json and generate csv file notbook in my work**\n\nThen i concatenate all 3 FullText Articles csv Files ","567a86eb":"After computing document frequency file use df file in kpminer algorithm","d9b190cc":"#COVID-19 Full Text Article KeyPhrase Extraction\n\nKeyphrase extraction is a textual information processing task concerned with the automatic extraction of representative and characteristic phrases from a document that express all the key aspects of its content. Keyphrases constitute a succinct conceptual summary of a document, which is very useful in digital information management systems for semantic indexing, faceted search, document clustering and classification.\n\nThere exist both supervised and unsupervised keyphrase extraction methods. Unsupervised methods are popular because they are domain independent and do not need labeled training data  so I use  unsupervised methode.\nThe most important merit of unsupervised models is that they can work in a new area or language with little or no adjustments. We can divide the unsupervised techniques into two subsets statistical models  and graphbased models \n\n**Statistical models:** are models that select candidates based on their statistical features Such as TF and IDF,Sentence Position, Word Position and etc.  The most popular statistical  algorithm is TFIDF, Kpminer, Yake and etc.\n\n**Graph-based Models:** These approaches typically consist of two steps: (1) building a graph representation of the document with words as nodes and semantic relation between them as edges; (2) ranking nodes using a graph-theoretic measure and using the top-ranked ones as keyphrases .The most popular Graph-based   algorithm is Text Rank, Single Rank, Topic Rank, Multipartiti Rank, Scake and etc.\nMost of unsupervised  models are implemented using PKE (an open source python-based keyphrase extraction toolkit) and I use PKE toolkit\n\n**Which unsupervised  algorithm  is best for Covid-19 Full Text Article ?**\n\nBased my experience and test in  multipe keyphrase extraction dataset that contain long  scientific text for example Krapavin, semeval 2010, Nguyen, statistical Models result is better than Graph based models also when we use each dataset DF  count in kpminer algorithm. the results make a significant improvement from other algorithms. So we use Kpminer algorithm for this task purpose. Also we use genism phrase for better bigram and trigram candid phrase detection. first we train genism phrase  based on all covid-19 article for detect commen phrase in this domain  article then we use trained model for detect commen phrase in each article and normal ngram for specific phrase in article.\n\nIn the fellow we shown my tests in Keyphrase Extraction Dataset.\nfor kpminer algorithm use DF of each article and use my prposed way for candid phrase detection.\n![image.png](attachment:image.png)\n","992d439e":"if you better graph visullization you can also use Gehpi Software networks for articles keyphrase co-occurrence graph.\n ![image.png](https:\/\/i.imgur.com\/blw8k25.png)\n![image.jpg](https:\/\/i.imgur.com\/Fa3C9Br.jpg)","2798c645":"#  Found Related Article From Graph Node\n**In the following step we want to found most related artilce based on graph nodes and print Article metadat**\n \n i use flashtext for found entered node between artcile metadata  for coloring entered word","64536958":"part of graph about covid-19\n ![image.png](https:\/\/i.imgur.com\/9S4es6h.jpg)"}}