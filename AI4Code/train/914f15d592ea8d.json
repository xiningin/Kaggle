{"cell_type":{"6fa18375":"code","1455c3d6":"code","781cbdca":"code","d082b084":"code","38e14cba":"code","6d0c362a":"code","4855805a":"code","6dc9c9c8":"code","045c8654":"code","33d1cf9b":"code","975d96bd":"code","0bddf816":"code","629a32ab":"code","1d8cf2ec":"code","38878368":"code","533b0e67":"code","5dfaa989":"code","7e3472ed":"code","49e6757b":"code","0e70f712":"code","5333f60f":"code","2f4362d0":"markdown","56b29eac":"markdown","311c121d":"markdown","7f3fbf9b":"markdown","01578302":"markdown","a657cdbe":"markdown","d70112ed":"markdown","38f4c3a3":"markdown","6d8ff2fb":"markdown","e970da3e":"markdown","5187ea90":"markdown","a59097da":"markdown","b20f7fd1":"markdown","56b4b6ab":"markdown","b91b419a":"markdown"},"source":{"6fa18375":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier","1455c3d6":"train = pd.read_csv(\"..\/input\/quora-question-pairs-feature-extraction-2\/train.csv\")\ntest = pd.read_csv(\"..\/input\/quora-question-pairs-feature-extraction-2\/test.csv\")\ntrainlabel = pd.read_csv(\"..\/input\/quora-question-pairs-feature-extraction-2\/trainlabel.csv\")","781cbdca":"dtrain = xgb.DMatrix(train, label = trainlabel)","d082b084":"p = 0.369197853026293\npos_public = (0.55410 + np.log(1 - p)) \/ np.log((1 - p) \/ p)\npos_private = (0.55525 + np.log(1 - p)) \/ np.log((1 - p) \/ p)\naverage = (pos_public + pos_private) \/ 2\nprint (pos_public, pos_private, average)","38e14cba":"w0 = average * (1 - p) \/ ((1 - average) * p)\nprint(w0)","6d0c362a":"w1 = average \/ p\nw2 = (1 - average) \/ (1 - p)\nprint(w1, w2)","4855805a":"def weighted_log_loss(preds, dtrain):\n    label = dtrain.get_label()\n    return \"weighted_logloss\", -np.mean(w1 * label * np.log(preds) + w2 * (1 - label) * np.log(1 - preds))","6dc9c9c8":"params = {}\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"logloss\"\nparams[\"eta\"] = 0.1\nparams[\"max_depth\"] = 6\nparams[\"min_child_weight\"] = 1\nparams[\"gamma\"] = 0\nparams[\"subsample\"] = 0.8\nparams[\"colsample_bytree\"] = 0.9\nparams[\"scale_pos_weight\"] = 0.3632\nparams[\"tree_method\"] = \"gpu_hist\"  # \u4f7f\u7528GPU\u52a0\u901f\u7684\u76f4\u65b9\u56fe\u7b97\u6cd5\nparams['max_bin'] = 256\n\nmodel1 = xgb.cv(params, dtrain, num_boost_round = 2000, nfold = 10, \n                feval = weighted_log_loss, early_stopping_rounds = 200, \n                verbose_eval = 50)","045c8654":"fix_params = {}\nfix_params[\"objective\"] = \"binary:logistic\"\nfix_params[\"eval_metric\"] = \"logloss\"\nfix_params[\"eta\"] = 0.1\nfix_params[\"gamma\"] = 0\nfix_params[\"subsample\"] = 0.8\nfix_params[\"colsample_bytree\"] = 0.9\nfix_params[\"scale_pos_weight\"] = 0.3632\nfix_params[\"tree_method\"] = \"gpu_hist\"\nfix_params[\"max_bin\"] = 256\n\nevaluation_list = []\nfor depth in [5, 6]:\n    for child_weight in [1, 2.5, 4]:\n        params = {**fix_params, **{\"max_depth\": depth, \"min_child_weight\": child_weight}}\n        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n                            feval = weighted_log_loss, early_stopping_rounds = 100)\n        # evaluation\u8bb0\u5f55\u4e86\u6bcf\u4e00\u8f6e\u8fed\u4ee3\u7684\u4ea4\u53c9\u9a8c\u8bc1\u7ed3\u679c\n        evaluation_list.append(evaluation)\n        \nfor depth in [7, 8]:\n    for child_weight in [4, 5, 6]:\n        params = {**fix_params, **{\"max_depth\": depth, \"min_child_weight\": child_weight}}\n        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n                            feval = weighted_log_loss, early_stopping_rounds = 100)\n        # evaluation\u8bb0\u5f55\u4e86\u6bcf\u4e00\u8f6e\u8fed\u4ee3\u7684\u4ea4\u53c9\u9a8c\u8bc1\u7ed3\u679c\n        evaluation_list.append(evaluation)\n\nevaluation_panel = pd.DataFrame()\nfor evaluation in evaluation_list:\n    # evaluation\u7684\u6700\u540e\u4e00\u884c\u5373\u76f8\u5e94\u53c2\u6570\u7ec4\u5408\u7684\u7ed3\u679c\n    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\nevaluation_panel","33d1cf9b":"fix_params = {}\nfix_params[\"objective\"] = \"binary:logistic\"\nfix_params[\"eval_metric\"] = \"logloss\"\nfix_params[\"eta\"] = 0.1\nfix_params[\"gamma\"] = 0\nfix_params[\"subsample\"] = 0.8\nfix_params[\"colsample_bytree\"] = 0.9\nfix_params[\"scale_pos_weight\"] = 0.3632\nfix_params[\"tree_method\"] = \"gpu_hist\"\nfix_params[\"max_bin\"] = 256\n\nevaluation_list = []\nfor depth in [5, 6, 7]:\n    for child_weight in [3, 3.5, 4, 4.5]:\n        params = {**fix_params, **{\"max_depth\": depth, \"min_child_weight\": child_weight}}\n        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n                            feval = weighted_log_loss, early_stopping_rounds = 100)\n        evaluation_list.append(evaluation)\n\nevaluation_panel = pd.DataFrame()\nfor evaluation in evaluation_list:\n    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\nevaluation_panel","975d96bd":"fix_params = {}\nfix_params[\"objective\"] = \"binary:logistic\"\nfix_params[\"eval_metric\"] = \"logloss\"\nfix_params[\"eta\"] = 0.1\nfix_params[\"gamma\"] = 0\nfix_params[\"subsample\"] = 0.8\nfix_params[\"colsample_bytree\"] = 0.9\nfix_params[\"scale_pos_weight\"] = 0.3632\nfix_params[\"tree_method\"] = \"gpu_hist\"\nfix_params[\"max_depth\"] = 6\nfix_params[\"min_child_weight\"] = 4\n\nevaluation_list = []\nfor bin in [200, 230, 256, 280]:\n    params = {**fix_params, **{\"max_bin\": bin}}\n    evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n                        feval = weighted_log_loss, early_stopping_rounds = 100)\n    evaluation_list.append(evaluation)\n\nevaluation_panel = pd.DataFrame()\nfor evaluation in evaluation_list:\n    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\nevaluation_panel","0bddf816":"fix_params = {}\nfix_params[\"objective\"] = \"binary:logistic\"\nfix_params[\"eval_metric\"] = \"logloss\"\nfix_params[\"eta\"] = 0.08\nfix_params[\"gamma\"] = 0\nfix_params[\"subsample\"] = 0.8\nfix_params[\"colsample_bytree\"] = 0.9\nfix_params[\"scale_pos_weight\"] = 0.3632\nfix_params[\"tree_method\"] = \"gpu_hist\"\nfix_params[\"max_depth\"] = 6\nfix_params[\"min_child_weight\"] = 3.5\n\nevaluation_list = []\nfor bin in [220, 240, 270]:\n    params = {**fix_params, **{\"max_bin\": bin}}\n    evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n                        feval = weighted_log_loss, early_stopping_rounds = 100)\n    evaluation_list.append(evaluation)\n\nevaluation_panel = pd.DataFrame()\nfor evaluation in evaluation_list:\n    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\nevaluation_panel","629a32ab":"fix_params = {}\nfix_params[\"objective\"] = \"binary:logistic\"\nfix_params[\"eval_metric\"] = \"logloss\"\nfix_params[\"eta\"] = 0.1\nfix_params[\"gamma\"] = 0\nfix_params[\"scale_pos_weight\"] = 0.3632\nfix_params[\"tree_method\"] = \"gpu_hist\"\nfix_params[\"max_depth\"] = 6\nfix_params[\"min_child_weight\"] = 4\nfix_params[\"max_bin\"] = 256\n\nevaluation_list = []\nfor row in [0.7, 0.8, 0.9]:\n    for col in [0.7, 0.8, 0.9]:\n        params = {**fix_params, **{\"subsample\": row, \"colsample_bytree\": col}}\n        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n                            feval = weighted_log_loss, early_stopping_rounds = 100)\n        evaluation_list.append(evaluation)\n\nevaluation_panel = pd.DataFrame()\nfor evaluation in evaluation_list:\n    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\nevaluation_panel","1d8cf2ec":"fix_params = {}\nfix_params[\"objective\"] = \"binary:logistic\"\nfix_params[\"eval_metric\"] = \"logloss\"\nfix_params[\"eta\"] = 0.1\nfix_params[\"gamma\"] = 0\nfix_params[\"scale_pos_weight\"] = 0.3632\nfix_params[\"tree_method\"] = \"gpu_hist\"\nfix_params[\"max_depth\"] = 6\nfix_params[\"min_child_weight\"] = 4\nfix_params[\"max_bin\"] = 256\n\nevaluation_list = []\nfor row in [0.75, 0.8, 0.85]:\n    for col in [0.85, 0.9]:\n        params = {**fix_params, **{\"subsample\": row, \"colsample_bytree\": col}}\n        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n                            feval = weighted_log_loss, early_stopping_rounds = 100)\n        evaluation_list.append(evaluation)\n\nevaluation_panel = pd.DataFrame()\nfor evaluation in evaluation_list:\n    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\nevaluation_panel","38878368":"params = {}\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"logloss\"\nparams[\"eta\"] = 0.06\nparams[\"gamma\"] = 0\nparams[\"scale_pos_weight\"] = 0.3632\nparams[\"tree_method\"] = \"gpu_hist\"\nparams[\"max_depth\"] = 6\nparams[\"min_child_weight\"] = 4\nparams[\"max_bin\"] = 256\nparams[\"subsample\"] = 0.8\nparams[\"colsample_bytree\"] = 0.9\n\nmodel6 = xgb.cv(params, dtrain, num_boost_round = 6000, nfold = 10, \n                feval = weighted_log_loss, early_stopping_rounds = 150, \n                verbose_eval = 50)","533b0e67":"params = {}\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"logloss\"\nparams[\"eta\"] = 0.04\nparams[\"gamma\"] = 0\nparams[\"scale_pos_weight\"] = 0.3632\nparams[\"tree_method\"] = \"gpu_hist\"\nparams[\"max_depth\"] = 6\nparams[\"min_child_weight\"] = 4\nparams[\"max_bin\"] = 256\nparams[\"subsample\"] = 0.8\nparams[\"colsample_bytree\"] = 0.9\n\nmodel4 = xgb.cv(params, dtrain, num_boost_round = 6000, nfold = 10, \n                feval = weighted_log_loss, early_stopping_rounds = 150, \n                verbose_eval = 50)","5dfaa989":"params = {}\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"logloss\"\nparams[\"eta\"] = 0.02\nparams[\"gamma\"] = 0\nparams[\"scale_pos_weight\"] = 0.3632\nparams[\"tree_method\"] = \"gpu_hist\"\nparams[\"max_depth\"] = 6\nparams[\"min_child_weight\"] = 4\nparams[\"max_bin\"] = 256\nparams[\"subsample\"] = 0.8\nparams[\"colsample_bytree\"] = 0.9\n\nmodel2 = xgb.cv(params, dtrain, num_boost_round = 6000, nfold = 10, \n                feval = weighted_log_loss, early_stopping_rounds = 150, \n                verbose_eval = 50)","7e3472ed":"params = {}\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"logloss\"\nparams[\"eta\"] = 0.02\nparams[\"gamma\"] = 0\nparams[\"scale_pos_weight\"] = 0.3632\nparams[\"tree_method\"] = \"gpu_hist\"\nparams[\"max_depth\"] = 6\nparams[\"min_child_weight\"] = 4\nparams[\"max_bin\"] = 256\nparams[\"subsample\"] = 0.8\nparams[\"colsample_bytree\"] = 0.9\n\ndtest = xgb.DMatrix(test)\n\nt = pd.read_csv(\"..\/input\/quora-question-pairs\/test.csv\")","49e6757b":"model = xgb.train(params, dtrain, num_boost_round = 3600)\nprediction = model.predict(dtest)\n\nsub = pd.DataFrame()\nsub['test_id'] = t[\"test_id\"]\nsub['is_duplicate'] = prediction\nsub.to_csv('submission3600.csv', index=False)","0e70f712":"model = xgb.train(params, dtrain, num_boost_round = 3800)\nprediction = model.predict(dtest)\n\nsub = pd.DataFrame()\nsub['test_id'] = t[\"test_id\"]\nsub['is_duplicate'] = prediction\nsub.to_csv('submission3800.csv', index=False)","5333f60f":"model = xgb.train(params, dtrain, num_boost_round = 4100)\nprediction = model.predict(dtest)\n\nsub = pd.DataFrame()\nsub['test_id'] = t[\"test_id\"]\nsub['is_duplicate'] = prediction\nsub.to_csv('submission4100.csv', index=False)","2f4362d0":"## 3.3 \u8bad\u7ec3\u6700\u7ec8\u6a21\u578b\u5e76\u63d0\u4ea4\n\n\u4e0a\u4e00\u8282\u4e2d\u7684\u6a21\u578b\u5747\u5728\u5230\u8fbe\u5141\u8bb8\u7684\u6700\u5927\u57fa\u5b66\u4e60\u5668\u6570\u91cf\u524d\u65e9\u505c\uff0c\u89c2\u5bdf\u4ed6\u4eec\u5728\u9a8c\u8bc1\u96c6\u4e0a\u7684\u52a0\u6743\u5bf9\u6570\u635f\u5931\uff0c\u660e\u663e\u53ef\u89c1\u5f53\u5b66\u4e60\u7387\u4e3a0.02\u65f6\u6700\u4f18\u3002\u9a8c\u8bc1\u96c6\u7684\u52a0\u6743\u5bf9\u6570\u635f\u5931\u57284450\u68f5\u6811\u4ee5\u540e\u5f00\u59cb\u4e0a\u5347\uff0c\u4e3a\u4e86\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u672c\u6587\u9002\u5f53\u51cf\u5c11\u4e86\u57fa\u5b66\u4e60\u5668\u6570\u91cf\uff0c\u6700\u7ec8\u540c\u65f6\u63d0\u4ea4\u4e86\u57fa\u5b66\u4e60\u5668\u6570\u91cf\u4e3a3600\u30013800\u548c4100\u7684\u6a21\u578b\u3002\n\n**\u5b9e\u6d4b\u65e0\u8bba\u662f\u5728Pubic\u699c\u5355\u8fd8\u662fPrivate\u699c\u5355\u4e2d\uff0c3600\u7684\u8868\u73b0\u90fd\u662f\u6700\u597d\u7684\uff0c**\u5176Public Scores\u4e3a0.16898\uff0c\u6392\u540d626\uff0cPrivate Scores\u4e3a0.17358\uff0c\u6392\u540d626\u3002","56b29eac":"\u7559\u610f\u5230\u8bad\u7ec3\u96c6\u7684\u6837\u672c\u5206\u5e03\u4e8e\u6d4b\u8bd5\u96c6\u7684\u6709\u6240\u4e0d\u540c\u3002\n\n\u8bad\u7ec3\u96c6\u6b63\u4f8b\u7684\u6bd4\u4f8b\u662f0.3691\uff0c\u5c06\u8fd9\u4e2a\u8bad\u7ec3\u96c6\u5747\u503c\u4f5c\u4e3a\u5bf9\u6d4b\u8bd5\u96c6\u7684\u9884\u6d4b\u4e0a\u4f20\uff0cPublic Leaderboard\u4e0a\u7684\u5f97\u5206\u4e3a0.55410\uff0cPrivate Leaderboard\u4e0a\u7684\u5f97\u5206\u4e3a0.55525\u3002\u6839\u636e\u5bf9\u6570\u635f\u5931\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u53ef\u4ee5\u9006\u63a8\u51faPublic\u7684\u6b63\u4f8b\u6bd4\u4f8b\u4e3a0.174247\uff0cprivate\u7684\u6b63\u4f8b\u6bd4\u4f8b\u4e3a0.176394\u3002\u4e3a\u4e86\u4f7f\u5f97Public\u548cPrivate\u699c\u4e0a\u7684\u5f97\u5206\u5c3d\u53ef\u80fd\u63a5\u8fd1\uff0c\u76f4\u63a5\u53d6\u4e24\u8005\u5e73\u5747\uff0c\u5f970.175320\uff0c\u4f5c\u4e3a\u5bf9\u6d4b\u8bd5\u96c6\u6b63\u4f8b\u7684\u4f30\u8ba1\uff0c\u57fa\u4e8e\u6b64\u5bf9\u8bad\u7ec3\u96c6\u8fdb\u884c\u9002\u5f53\u8c03\u6574\u3002","311c121d":"\u6700\u4f18\u7684\u884c\u91c7\u6837\u7387\u4e3a0.8\uff0c\u5217\u91c7\u6837\u7387\u4e3a0.9\u3002","7f3fbf9b":"\u7531\u4e0a\u8868\u53ef\u89c1\uff0c\u9ed8\u8ba4\u53c2\u6570max_bin=256\u4e3a\u6700\u4f18\u503c\uff0c\u4f46\u7bb1\u5b50\u6570\u4ece200\u53d8\u5316\u5230230\u518d\u53d8\u5316\u5230256\u65f6\uff0c\u5bf9\u6570\u635f\u5931\u5148\u5347\u540e\u964d\uff0c\u5e76\u4e0d\u7a33\u5b9a\uff0c\u4e8e\u662f\u672c\u6587\u5728\u8fd9\u4e2a\u8303\u56f4\u5185\u53c8\u9009\u53d6\u4e86\u591a\u4e2a\u503c\u8fdb\u884c\u5c1d\u8bd5\u3002","01578302":"\u91cd\u70b9\u89c2\u5bdf\u4e0a\u8868\u7b2c3\u884ctest-weighted_logloss-mean\uff0c\u4ee3\u8868\u4e86\u5728\u9a8c\u8bc1\u96c6\u4e0a\u7684\u52a0\u6743\u5bf9\u6570\u635f\u5931\u3002\u7b2c7\u5217\u81f3\u7b2c12\u5217\u7684\u6a21\u578b\u56e0\u4e3a\u65e9\u505c\u673a\u5236\u5728650\u68f5\u6811\u524d\u63d0\u524d\u505c\u6b62\u8bad\u7ec3\uff0c\u5bf9\u6bd4\u524d6\u5217\u7ed3\u679c\u53ef\u77e5\uff0c\u7b2c6\u5217\u6700\u4f18\uff0c\u5bf9\u6570\u635f\u5931\u4e3a0.190239\uff0c\u4ee3\u8868\u4e86max_depth=6, min_child_weight=4\u7684\u7ec4\u5408\u3002\u56f4\u7ed5\u8fd9\u4e2a\u7ec4\u5408\u7ee7\u7eed\u6784\u5efa\u641c\u7d22\u8303\u56f4\u3002","a657cdbe":"## 3.1 \u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u5177\u6709\u4e0d\u540c\u7684\u5206\u5e03","d70112ed":"## 3.2.5 \u8c03\u6574\u5b66\u4e60\u7387eta\u548c\u57fa\u5b66\u4e60\u5668\u6570\u91cfnum_boost_round","38f4c3a3":"\u53e6\u4e00\u65b9\u9762\uff0c\u867d\u7136\u8bad\u7ec3\u65f6logloss\u7684\u8ba1\u7b97\u7684\u5374\u4f1a\u6839\u636escale_pos_weight\u6709\u6240\u8c03\u6574\uff0c\u4e5f\u5c31\u662f\u8bf4scale_pos_weight\u53c2\u6570\u7684\u786e\u53ef\u4ee5\u5f71\u54cd\u5230XGBoost\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4f46\u5bf9\u4e8e\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u7528\u4e8e\u4ea4\u53c9\u9a8c\u8bc1\u7684xgb.cv\u51fd\u6570\u5374\u6ca1\u6709\u8fd9\u6837\u7684\u6548\u679c\uff0c\u8be5\u51fd\u6570\u7684\u4ea4\u53c9\u9a8c\u8bc1logloss\u4ecd\u7136\u662f\u6839\u636e\u6b63\u8d1f\u4f8b\u7b49\u6743\u91cd\u7684\u65b9\u6cd5\u8ba1\u7b97\u5f97\u5230\u3002\u56e0\u6b64\uff0c\u672c\u6587\u91cd\u65b0\u5b9a\u4e49\u4e86\u4e00\u4e2a\u52a0\u6743\u5bf9\u6570\u635f\u5931\u51fd\u6570\uff0c\u8f93\u5165\u7ed9xgb.cv\u51fd\u6570\u7684feval\u53c2\u6570\u7528\u4e8e\u4ea4\u53c9\u9a8c\u8bc1\u7684\u635f\u5931\u8ba1\u7b97\uff0c\u800c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u51fd\u6570\u5219\u65e0\u9700\u81ea\u5b9a\u4e49\u8ba1\u7b97\u3002\n\n\u8981\u7b49\u4ef7\u4e8e\u8ba9\u6b63\u4f8b\u7684\u6570\u91cf\u4ece0.3691\u4e0b\u964d\u52300.1753\uff0c\u800c\u6b63\u8d1f\u4f8b\u7684\u603b\u91cf\u7ef4\u6301\u4e0d\u53d8\uff0c\u5e94\u5bf9\u6b63\u4f8b\u548c\u8d1f\u4f8b\u5206\u522b\u65bd\u52a00.4749\u76841.3074\u7684\u6743\u91cd\u3002","6d8ff2fb":"\u7531\u4e0a\u8868\u53ef\u89c1\uff0c\u7b2c6\u5217\u7684\u7ec4\u5408\u6700\u4f18\uff0c\u884c\u91c7\u6837\u7387\u53d60.8\uff0c\u5217\u91c7\u6837\u7387\u53d60.9\uff0c\u8fdb\u4e00\u6b65\u6784\u9020\u641c\u7d22\u8303\u56f4\u3002","e970da3e":"\u4ece\u4e0a\u8868\u53ef\u89c1\uff0c\u7b2c7\u5217\u6700\u4f18\uff0cmax_depth=6, min_child_weight=4\u7684\u7ec4\u5408\u7684\u786e\u8868\u73b0\u6700\u597d\u3002\n\n## 3.2.3 \u8c03\u6574\u76f4\u65b9\u56fe\u6700\u5927\u7bb1\u5b50\u6570max_bin","5187ea90":"# 3 XGBoost\u6a21\u578b\u8bad\u7ec3","a59097da":"\u57fa\u5b66\u4e60\u5668\u6570\u91cf\u5927\u7ea6\u4e3a650\u68f5\uff0c\u56e0\u4e3a\u9a8c\u8bc1\u96c6\u7684\u52a0\u6743\u5bf9\u6570\u635f\u5931test-weighted_logloss\u5728650\u68f5\u6811\u4ee5\u540e\u51fa\u73b0\u4e86\u4e0a\u5347\u3002\n\n## 3.2.2 \u8c03\u6574\u6700\u5927\u6df1\u5ea6max_depth\u548cmin_weight\n\n\u4ea4\u53c9\u9a8c\u8bc1\u901a\u5e38\u53ef\u4f7f\u7528Scikit-Learn\u7684GridSearchCV\u51fd\u6570\uff0c\u4f46\u6211\u5728XGBoost\u7684Scikit-Learn API\u53c2\u6570\u9875\u91cc\u6ca1\u627e\u5230GPU\u76f8\u5173\u9009\u9879\uff0c\u5f3a\u884c\u8ba9tree_method=\u201cgpu_hist\u201d\u53c8\u62a5\u9519\uff0c\u6240\u4ee5\u6211\u6000\u7591GridSearchCV\u4e0d\u652f\u6301GPU\u4f7f\u7528\uff0c\u88ab\u8feb\u4f7f\u7528for\u5faa\u73af\uff0c\u5e76\u7528evaluation_depth_childweight\u8bb0\u5f55\u4e0b\u6240\u6709\u7684\u4ea4\u53c9\u9a8c\u8bc1\u5386\u53f2\u3002","b20f7fd1":"\u6bd4\u8f83\u4e24\u8f6e\u8c03\u53c2\u7684\u7ed3\u679c\uff0c\u9009\u53d6max_bin=256\u7684\u786e\u6700\u4f18\u3002\n\n## 3.2.4 \u8c03\u6574\u884c\u91c7\u6837\u7387subsample\u548c\u5217\u91c7\u6837\u7387colsample_bytree","56b4b6ab":"XGBoost\u6709scale_pos_weight\u53c2\u6570\u53ef\u7528\u4e8e\u8c03\u6574\u6b63\u8d1f\u6837\u672c\u7684\u6743\u91cd\uff0c\u5f88\u5bb9\u6613\u8ba1\u7b97\uff0c\u8981\u7b49\u4ef7\u4e8e\u8ba9\u8bad\u7ec3\u96c6\u6b63\u4f8b\u7684\u6570\u91cf\u4ece0.3691\u4e0b\u964d\u81f30.1753\uff0c\u5e94\u8ba9\u6b63\u4f8b\u7684\u6743\u91cd\u53d8\u4e3a\u539f\u6765\u76840.3632\u3002","b91b419a":"## 3.2 XGBoost\u53c2\u6570\u8c03\u6574\n\n\u53c2\u6570\u8c03\u6574\u7684\u6d41\u7a0b\u5982\u4e0b\uff1a\n\nnum_boost_round \u2192 max_depth & min_child_weight \u2192 max_bin(\u56e0\u4e3a\u6709GPU\u52a0\u901f) \u2192 subsample & colsample_bytree \u2192 eta & num_boost_round\n\n## 3.2.1 \u8c03\u6574\u57fa\u5b66\u4e60\u5668\u6570\u91cfnum_boost_round"}}