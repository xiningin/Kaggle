{"cell_type":{"4baf35c4":"code","48d5bd14":"code","f568d78d":"code","137092bd":"code","e787bd90":"code","ea7d8de4":"code","0cdce901":"code","bf863d38":"code","a1217042":"code","7b461e61":"code","35b2ec3c":"code","d2819f97":"code","90ac8910":"code","f9945ea1":"code","0629e0e8":"code","949fbf70":"code","81f4877e":"code","96b656fb":"code","56c12ffb":"code","27679b2b":"code","50f2a316":"code","0e2306ec":"code","63027f7a":"code","223bb72f":"code","d58e36c7":"code","36be2676":"markdown","cf59a0c3":"markdown","0aa537d3":"markdown","641d4fb2":"markdown","35bfabdc":"markdown","4e8eadce":"markdown","2252d68c":"markdown","fe0cc52a":"markdown","0130fdcc":"markdown","aeaf813e":"markdown","cb93201f":"markdown","92de7e1e":"markdown","4fc8ee36":"markdown"},"source":{"4baf35c4":"import os\nfor dirname,_,filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname,filename))","48d5bd14":"import time\nstart = time.time()","f568d78d":"## Importing standard libraries\n%pylab inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","137092bd":"## Importing sklearn libraries\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn import preprocessing","e787bd90":"## Keras Libraries for Neural Networks\n\nfrom keras.models import Sequential\nfrom keras.layers import merge\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers import Convolution2D, Convolution1D, MaxPooling2D\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping","ea7d8de4":"## Read data from the CSV file\nparent_data = pd.read_csv('\/kaggle\/input\/train.csv.zip')\ndata = parent_data.copy()   \ndata.pop('id')","0cdce901":"## read test file\ntest = pd.read_csv('\/kaggle\/input\/test.csv.zip')\ntestId = test.pop('id')","bf863d38":"data.head()","a1217042":"data.shape","7b461e61":"data.describe()","35b2ec3c":"## Since the labels are textual, so we encode them categorically\nspecies_label = data.pop('species')\nspecies_label = LabelEncoder().fit(species_label).transform(species_label)\nprint(species_label.shape)","d2819f97":"## We will be working with categorical crossentropy function\n## It is required to further convert the labels into \"one-hot\" representation\n# from keras import utils as np_utils\none_hot = to_categorical(species_label)\nprint(one_hot.shape)","90ac8910":"preprocessed_train_data = preprocessing.MinMaxScaler().fit(data).transform(data)\npreprocessed_train_data = StandardScaler().fit(data).transform(data)\n\nprint(preprocessed_train_data.shape)","f9945ea1":"## we need to perform the same transformations from the training set to the test set\ntest = preprocessing.MinMaxScaler().fit(test).transform(test)\ntest = StandardScaler().fit(test).transform(test)","0629e0e8":"sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345)\ntrain_index, val_index = next(iter(sss.split(preprocessed_train_data, one_hot)))\n\nx_train, x_val = preprocessed_train_data[train_index], preprocessed_train_data[val_index]\ny_train, y_val = one_hot[train_index], one_hot[val_index]\n\nprint(\"x_train dim: \",x_train.shape)\nprint(\"x_val dim:   \",x_val.shape)","949fbf70":"model = Sequential()\n\nmodel.add(Dense(768,input_dim=192,  kernel_initializer='glorot_normal', activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(768, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(99, activation='softmax'))\n\nmodel.summary()","81f4877e":"## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\n\nmodel.compile(loss='categorical_crossentropy',optimizer='adam', metrics = [\"accuracy\"])","96b656fb":"%%time\nearly_stopping = EarlyStopping(monitor='val_loss', patience=300)\n\nhistory = model.fit(x_train, y_train,batch_size=192,epochs=2500 ,verbose=1,\n                    validation_data=(x_val, y_val),callbacks=[early_stopping])","56c12ffb":"## we need to consider the loss for final submission to leaderboard\n## print(history.history.keys())\nprint('val_acc: ',max(history.history['val_acc']))\nprint('val_loss: ',min(history.history['val_loss']))\nprint('train_acc: ',max(history.history['acc']))\nprint('train_loss: ',min(history.history['loss']))\n\nprint()\nprint(\"train\/val loss ratio: \", min(history.history['loss'])\/min(history.history['val_loss']))","27679b2b":"## summarize history for loss\n## Plotting the loss with the number of iterations\nplt.semilogy(history.history['loss'])\nplt.semilogy(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_loss.png')","50f2a316":"## Plotting the error with the number of iterations\n## With each iteration the error reduces smoothly\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_accuracy.png')","0e2306ec":"yPred = model.predict_proba(test)","63027f7a":"## Converting the test predictions in a dataframe as depicted by sample submission\nsubmission = pd.DataFrame(yPred,index=testId,columns=sort(parent_data.species.unique()))","223bb72f":"submission.to_csv('leafClassificationSubmission.csv')\n\n## print run time\nend = time.time()\nprint(round((end-start),2), \"seconds\")","d58e36c7":"submission.head()","36be2676":"The species are in string format, as such to convert it to one hot encoding format, first I have to labelling it in assistance with labelencoder.","cf59a0c3":"[<center><h1>Kaggle: Leaf Classification<\/h1><\/center>](https:\/\/www.kaggle.com\/c\/leaf-classification)","0aa537d3":"Get the path of training and testing set","641d4fb2":"\nPublic Score :  0.15218<br>\nPrivate Score : 0.15218<br>\nAs it's a regression problem I better be solving this without deep learning model. Whatever, I'm implementing neural network here.","35bfabdc":"After one hot encoding, we have got 99 different species.","4e8eadce":"192 is the column size , glorot normal is used for text based cnn. 768 is chosen arbitrarily.","2252d68c":"---------\n\nEarlier` we used a 4 layer network but the result came out to be overfitting the test set. We dropped the count of neurones in the network and also restricted the number of layers to 3 so as to keep it simple.\nInstead of submitting each test sample as a one hot vector we submitted each samples as a probabilistic distribution over all the possible outcomes. This \"may\" help reduce the penalty being exercised by the multiclass logloss thus producing low error on the leaderboard! ;)\nAny suggestions are welcome!","fe0cc52a":"Most of the learning algorithms are prone to feature scaling <br>\nStandardising the data to give zero mean =)","0130fdcc":"Measure the elapsed time of the whole procedure, which starts with \"start\" and ends with \"end\" function.","aeaf813e":"remove index = False from to_csv","cb93201f":"column is the input dimension","92de7e1e":"Separate \"species\" column to set label, which is why we can't see any column numbers left after calling shape method","4fc8ee36":"Use stratifiedshufflesplit, as there exists subspecies in the image set"}}