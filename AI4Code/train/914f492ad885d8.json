{"cell_type":{"2e90f47c":"code","2c1563f9":"code","9a6266f7":"code","aa13b1e4":"code","de22e774":"code","d648b96c":"code","1b9dc039":"code","f40c0e7b":"code","fabd1c50":"code","e2609c44":"code","864d709e":"code","9bb0ae56":"code","c6019bd6":"code","66637094":"code","8307a647":"code","ae1d04bc":"code","22ac7cef":"code","e6babbdd":"code","235c2c88":"code","0186977d":"code","2f14088a":"code","5021bd67":"code","5def8617":"code","948a8702":"code","9734aa83":"code","65cd1e8e":"code","44d6b8b9":"code","af1852dd":"code","5466ce5f":"code","adfae5bc":"code","0f2264a5":"code","331ee8b6":"code","fb61f37d":"code","f38b288a":"code","97776102":"code","37914b7f":"code","97c61c99":"code","bfdc5b24":"code","6ba3c915":"code","0aba71a4":"code","d89309c7":"code","2de91086":"code","02697166":"code","8b1bce65":"code","e0a00431":"code","d1b28ee4":"code","a5879b60":"code","40871563":"code","d61f4a77":"code","2f3feecb":"code","398da1aa":"code","f49e0f4c":"code","65d0ae67":"code","3c1ec233":"code","862c6f98":"code","956bf96b":"code","9fbd03d9":"code","9f99f59a":"code","d104482a":"markdown","e02a7bf3":"markdown","1a4b1594":"markdown","e3215498":"markdown","9a71c0a7":"markdown","427ad3ce":"markdown","0f9c106d":"markdown","5d9727a9":"markdown","f8509541":"markdown","22604dd0":"markdown","597b10b1":"markdown","7f24efa3":"markdown","f9a452c5":"markdown","02012f93":"markdown","c44de0b1":"markdown","00f3c507":"markdown","5488c87c":"markdown","0cf2d180":"markdown","d11ff69b":"markdown","0461283e":"markdown","82bbfcf5":"markdown","35d44ace":"markdown","f7b8ff4b":"markdown","619cc218":"markdown","2d984e2b":"markdown","7a20511e":"markdown","e69a42d8":"markdown","f192af15":"markdown","525e0a61":"markdown","a396599c":"markdown","96f2cb72":"markdown","a015f29d":"markdown","1f0308b6":"markdown","5ddd22f2":"markdown","9d6cebe9":"markdown","b579010b":"markdown","c61d7de8":"markdown","683f5ee4":"markdown","38dc1932":"markdown","65b9db49":"markdown","dad9e564":"markdown","2469175d":"markdown","91e7c95f":"markdown","bd0eb8f1":"markdown","789a66fb":"markdown"},"source":{"2e90f47c":"#import libraries\n\n#data analysis & preprocessing\nimport pandas as pd\nimport numpy as np \n\n#visualization tools \nimport matplotlib.pyplot as plt \nimport seaborn as sns","2c1563f9":"#import the training and test dataset to panda dataframe\ntitanic_train = pd.read_csv(\"..\/input\/titanic\/train.csv\") \ntitanic_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n#concatenate training and test dataset  into one dataframe\ndataset = pd.concat([titanic_train, titanic_test])\n\n#display first 5 rows of training and test dataset \ndisplay(titanic_train.head())\ndisplay(titanic_test.head())","9a6266f7":"#overall description of dataframes, scanning for missing value \n\ndisplay(titanic_train.info())\ndisplay(titanic_test.info())","aa13b1e4":"#overall view of the statistic of the variable\n\ndisplay(titanic_train.describe(include='all'))\ndisplay(titanic_test.describe(include='all'))","de22e774":"#subset new dataframe to observe the relation between Passenger Class, Sex and Age.\n\ndata_age_pclass_sex = dataset[['Pclass','Sex','Age']]\n\n#visualizing the distribution of age across Passenger Class and Sex\nsns.set_style('dark')\n\nplot = sns.displot(data=data_age_pclass_sex, x='Age', col='Sex',row='Pclass', kde=True, bins=8)\nplot.fig.suptitle(\"Distribution of Age across different Passenger Class and Sex\", fontsize=20)\nplot.fig.subplots_adjust(top=0.93)","d648b96c":"#Calculate the median age for each Passenger class and sex group\n\nfill_age_median = pd.DataFrame(data_age_pclass_sex.groupby(['Pclass','Sex'], as_index=False)['Age'].median())\nfill_age_median","1b9dc039":"#fill missing value according to Passenger class and Sex \n\nprint('For Training Dataset')\nprint('Before, there are ' + str(titanic_train['Age'].isnull().sum()) + ' missing values in \"Age\" column')\n\n#fill missing value \nfor pclass in fill_age_median['Pclass'].unique():\n    for sex in fill_age_median['Sex'].unique():\n        titanic_train.loc[(titanic_train['Pclass']==pclass)&(titanic_train['Sex']==str(sex))&(titanic_train['Age'].isnull()==True), 'Age'] \\\n        = float(fill_age_median.loc[(fill_age_median['Pclass']==pclass)&(fill_age_median['Sex']==str(sex))]['Age'])\n  \nprint('After we fill it, now there are ' + str(titanic_train['Age'].isnull().sum()) + ' missing values in \"Age\" column')\n\nprint('-'*75)\n\nprint('For Test Dataset')\nprint('Before, there are ' + str(titanic_test['Age'].isnull().sum()) + ' missing values in \"Age\" column')\n\n#fill missing value \nfor pclass in fill_age_median['Pclass'].unique():\n    for sex in fill_age_median['Sex'].unique():\n        titanic_test.loc[(titanic_test['Pclass']==pclass)&(titanic_test['Sex']==str(sex))&(titanic_test['Age'].isnull()==True), 'Age'] \\\n        = float(fill_age_median.loc[(fill_age_median['Pclass']==pclass)&(fill_age_median['Sex']==str(sex))]['Age'])\n  \nprint('After we fill it, now there are ' + str(titanic_test['Age'].isnull().sum()) + ' missing values in \"Age\" column')\n\n","f40c0e7b":"#Check the missing value in the 'Embarked' column\n\ntitanic_train[titanic_train['Embarked'].isnull()]","fabd1c50":"#fill the missing value with the most frequent data point aka mode\n\ntitanic_train['Embarked'].fillna(titanic_train['Embarked'].mode()[0],inplace=True)\n\nassert titanic_train['Embarked'].isnull().sum() == 0 #truth assertion to check if the missing value had been filled","e2609c44":"dataset[dataset['Fare'].isnull()]","864d709e":"#subset new dataframe to observe the relation between Fare vs Passenger Class and Embarkation Port \n\ndata_fare_pclass_embarked = dataset[['Fare','Pclass','Embarked']]\nfare_data = data_fare_pclass_embarked[(dataset['Pclass']==3)&(dataset['Embarked']=='S')]['Fare']\nfare_data.describe()","9bb0ae56":"#Fill Fare missing value with median \n\ntitanic_test['Fare'].fillna(fare_data.median(), inplace = True)\n\nassert titanic_train['Fare'].isnull().sum() == 0 #truth assertion to check if the missing value had been filled","c6019bd6":"#drop the \"Cabin\" column\n\nprint('For Training Dataset')\nprint('The shape of dataset before we drop the \"Cabin\" column is ' + str(titanic_train.shape))\ntitanic_train.dropna(axis='columns',inplace=True)\nprint('The shape of dataset after we drop the \"Cabin\" column is ' + str(titanic_train.shape))\n\nprint('-'*75)\n\nprint('For Test Dataset')\nprint('The shape of dataset before we drop the \"Cabin\" column is ' + str(titanic_test.shape))\ntitanic_test.dropna(axis='columns',inplace=True)\nprint('The shape of dataset after we drop the \"Cabin\" column is ' + str(titanic_test.shape))\n","66637094":"display(titanic_train.info())\ndisplay(titanic_test.info())","8307a647":"#investigate the survival rate of different Sex \n\nsex_surv = titanic_train[['Sex','Survived']].groupby('Sex', as_index=False).mean()\ndisplay(sex_surv)\n\nsns.barplot(data=sex_surv, x='Sex', y='Survived')\nplt.title('Survival Rate on Different Sex', fontsize=15)\nplt.show()","ae1d04bc":"#using pie chart to visualized the percentage of survival on different sex\n\npie_0 = titanic_train[['Sex','Survived']][titanic_train['Survived']==0].value_counts(subset='Sex').reset_index()\npie_0 = pd.DataFrame(pie_0).rename(columns={0:'Counts'})\npie_0.sort_values('Sex', ascending=True, inplace=True)\n\npie_1 = titanic_train[['Sex','Survived']][titanic_train['Survived']==1].value_counts(subset='Sex').reset_index()\npie_1 = pd.DataFrame(pie_1).rename(columns={0:'Counts'})\npie_1.sort_values('Sex', ascending=True, inplace=True)\n\n\n#create subplots for pie chart\n\nfig, ax = plt.subplots(1,2)\n\n# plot pie chart for survived = 0\nax[0].pie(pie_0['Counts'], labels=pie_0['Sex'],autopct='%0.2f%%', shadow=False)\nax[0].title.set_text('Sex Proportion of Deceased Passenger')\n\n# plot pie chart for survived = 1\nax[1].pie(pie_1['Counts'], labels=pie_1['Sex'],autopct='%0.2f%%', shadow=False)\nax[1].title.set_text('Sex Proportion of Survived Passenger')\n\nfig.tight_layout(pad=0.0)\nplt.show()","22ac7cef":"#Observe distribution of age and survavibility\n\nage_surv = titanic_train[['Age','Survived']]\n\nplt.title(\"Distribution of Age on Survived and Deceased Passenger\", fontsize=18)\nplot = sns.histplot(data=age_surv, x='Age', hue='Survived', bins=16, alpha=0.5)","e6babbdd":"#calculating the survival rate of passenger below 15 years \n\nprint('Survival Rate of Passenger below 15 years is ' + str(age_surv[age_surv['Age']<=15]['Survived'].mean()) )\nprint('Survival Rate of Passenger above 15 years is ' + str(age_surv[age_surv['Age']>15]['Survived'].mean()) )","235c2c88":"#Observe distribution of age & pclass vs survavibility\n\npclass_surv = titanic_train[['Pclass','Survived']].groupby('Pclass', as_index=False).mean()\ndisplay(pclass_surv)\n\n#Point plot to visualize survival rate among different passenger classes \nplt.title(\"Survival Rate of Different Passenger Classes\", fontsize=18)\nplot = sns.pointplot(data=pclass_surv, x='Pclass', y='Survived', ci='sd', linestyles='--')","0186977d":"#Observe distribution of fare and survavibility\n\nfare_pclass_embarked_surv = titanic_train[['Fare','Survived','Pclass','Embarked']]\n\ngrid = sns.FacetGrid(fare_pclass_embarked_surv, row='Embarked', col='Pclass', hue='Survived', sharex=False, sharey=False)\ngrid.map(plt.hist, 'Fare', alpha=0.5, bins=10)\ngrid.fig.suptitle(\"Distribution of Fare on Survived and Deceased Passenger in Various Passenger Class and Embarkation Port\", fontsize=18)\ngrid.fig.subplots_adjust(top=0.9)\ngrid.add_legend()","2f14088a":"df = fare_pclass_embarked_surv\n\nfor embarked in set(df['Embarked']):\n    for pclass in set(df['Pclass']):\n    \n        print('For Pclass = ' + str(pclass) + ' & Embarked = ' + str(embarked))\n        value = df[(df['Embarked']==embarked)&(df['Pclass']==pclass)]['Fare'].quantile(0.10)\n        \n        print('10% quantile Fare is ' + str(value))\n        \n        print('Survival Rate for Bottom 10% Fare Price is ' + \\\n        str( df[(df['Embarked']==embarked)&(df['Pclass']==pclass)&(df['Fare']<value)]['Survived'].mean() ) )\n        \n        print('Survival Rate for Top 90% Fare Price is ' + \\\n        str( df[(df['Embarked']==embarked)&(df['Pclass']==pclass)&(df['Fare']>=value)]['Survived'].mean() ) )\n        \n        print(' ')","5021bd67":"#investigate the effect of embarkation port to the survival chance of passenger \n\nembarked_surv = titanic_train[['Embarked','Survived','Pclass','Sex']]\nembarked_pie = pd.DataFrame(embarked_surv['Embarked'].value_counts())\nembarked_pie.reset_index(inplace=True)\n\ndisplay(embarked_pie)\ndisplay(embarked_surv.groupby('Embarked', as_index=False)['Survived'].mean())\n\nfig, ax = plt.subplots()\n# plot pie chart \nax.pie(embarked_pie['Embarked'], labels=embarked_pie['index'],autopct='%0.2f%%', shadow=False)\nplt.title('Embarkation Port of Titanic Passenger in Training Dataset')\nplt.show()","5def8617":"#Count plot for Embarkated & Pclass vs Survival Rate\nsns.countplot(x='Embarked', hue='Pclass', data=embarked_surv)\nplt.title('Embarkation Port & Pclass Effect on Survival Rate', fontsize=15)","948a8702":"#Barplot for Embarkated & Sex vs Survival Rate\nsns.countplot(x='Embarked', hue='Sex',  data=embarked_surv)\nplt.title('Embarkation Port & Sex Effect on Survival Rate', fontsize=15)","9734aa83":"#Survival Rate different Passenger Class & Embarkation Port\nemb_pclass_surv = pd.DataFrame(embarked_surv.groupby(['Embarked','Pclass'],as_index=True)['Survived'].mean())\ndisplay(emb_pclass_surv)\nemb_pclass_surv.reset_index(inplace=True)\nsns.catplot(x='Embarked', y='Survived', hue='Pclass', data=emb_pclass_surv, kind='bar')\nplt.title('Survival Rate across different Passenger Class & Embarkation Port',fontsize=15)","65cd1e8e":"#Survival Rate across Sex & Embarkation Port\nemb_sex_surv = pd.DataFrame(embarked_surv.groupby(['Embarked','Sex'],as_index=True)['Survived'].mean())\ndisplay(emb_sex_surv)\nemb_sex_surv.reset_index(inplace=True)\nsns.catplot(x='Embarked', y='Survived', hue='Sex', data=emb_sex_surv, kind='bar')\nplt.title('Survival Rate across different Sex & Embarkation Port',fontsize=15)","44d6b8b9":"#subset the dataframe to examine the relation between number of Siblings\/Spouses vs Survived\n\nsibsp_surv = titanic_train[['SibSp','Survived']].groupby('SibSp', as_index=False)['Survived'].mean()\n\nsibsp_count = pd.DataFrame(titanic_train['SibSp'].value_counts()).reset_index()\nsibsp_count.drop(columns='index',inplace=True)\nsibsp_count.rename(columns={'SibSp':'Counts'}, inplace=True)\n\nsibsp_surv_concat = pd.concat([sibsp_surv,sibsp_count], axis='columns', ignore_index=False)\ndisplay(sibsp_surv_concat)\n\nplt.figure()\n\nax = sns.pointplot(x='SibSp', y='Survived', data=sibsp_surv_concat, linestyles='--')\nax2 = ax.twinx()\nsibsp_surv_concat.plot(x='SibSp', y='Counts', ax=ax2, color=\"g\", kind='bar', alpha=0.3)\nax.set_ylabel('Mean Survival Rate')\nax2.set_ylabel('Counts')\nplt.title('Number of Siblings\/Spouses vs Survived', fontsize=15)\nplt.show()","af1852dd":"#subset the dataframe to examine the relation between number of Parents\/Children vs Survived\n\nparch_surv = titanic_train[['Parch','Survived']].groupby('Parch', as_index=False)['Survived'].mean()\n\nparch_count = pd.DataFrame(titanic_train['Parch'].value_counts()).reset_index()\nparch_count.drop(columns='index',inplace=True)\nparch_count.rename(columns={'Parch':'Counts'}, inplace=True)\n\nparch_surv_concat = pd.concat([parch_surv,parch_count], axis='columns', ignore_index=False)\ndisplay(parch_surv_concat)\n\nplt.figure()\nax = sns.pointplot(x='Parch', y='Survived', data=parch_surv_concat, linestyles='--')\nax2 = ax.twinx()\nparch_surv_concat.plot(x='Parch', y='Counts', ax=ax2, color=\"g\", kind='bar', alpha=0.3)\nax.set_ylabel('Mean Survival Rate')\nax2.set_ylabel('Counts')\nplt.title('Number of Parents\/Children vs Survived', fontsize=15)\nplt.show()","5466ce5f":"#Inspect the 'companion' aspect on the survivability of passenger\n\ncompanion = titanic_train[['SibSp','Parch','Pclass','Fare','Survived']].copy()\ncompanion['comp'] = (companion['SibSp'] + companion['Parch'])\n\ndisplay(companion.head())\ndisplay(companion.info())","adfae5bc":"#subset the dataframe to examine the relation between number of Companions vs Survived\n\ncomp_surv = companion[['comp','Survived']].groupby('comp', as_index=False)['Survived'].mean()\n\ncomp_count = pd.DataFrame(companion['comp'].value_counts()).reset_index()\ncomp_count.drop(columns='index',inplace=True)\ncomp_count.rename(columns={'comp':'Counts'}, inplace=True)\n\ncomp_surv_concat = pd.concat([comp_surv,comp_count], axis='columns', ignore_index=False)\ndisplay(comp_surv_concat)\n\nplt.figure()\nax = sns.pointplot(x='comp', y='Survived', data=comp_surv_concat, linestyles='--')\nax2 = ax.twinx()\ncomp_surv_concat.plot(x='comp', y='Counts', ax=ax2, color=\"g\", kind='bar', alpha=0.3)\nax.set_ylabel('Mean Survival Rate')\nax2.set_ylabel('Counts')\nplt.title('Number of Companions vs Survived', fontsize=15)\nplt.show()","0f2264a5":"#calculate the mean survival rate of passenger with 4 or more companions\n\nprint('Mean urvival rate of passenger with 4 or more companions is ' \\\n      + str(comp_surv_concat[comp_surv_concat['comp']>=4]['Survived'].mean()))","331ee8b6":"#Investigate the relation between Ticket Number and Survival\n\nunique_ticket = set(titanic_train['Ticket'])\ndisplay(len(unique_ticket))\n\nticket_surv = titanic_train[['Ticket','Survived']].groupby('Ticket', as_index=False)['Survived'].mean()\n\nticket_count = pd.DataFrame(titanic_train['Ticket'].value_counts()).reset_index()\nticket_count.drop(columns='index',inplace=True)\nticket_count.rename(columns={'Ticket':'Counts'}, inplace=True)\n\nticket_surv_concat = pd.concat([ticket_surv,ticket_count], axis='columns', ignore_index=False)\ndisplay(ticket_surv_concat)","fb61f37d":"#Investigate 'Name' column\n\nunique_name = set(titanic_train['Name'])\ndisplay(unique_name)","f38b288a":"#Extract 'Title' from 'Name'\n\ntitanic_train['Title'] = titanic_train['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntitanic_train['Title'].value_counts()","97776102":"#Create The Title Group and define 'Others' group\n\ntitle_group = titanic_train['Title'].unique()\nothers = np.delete(arr=title_group,obj=range(0,4))\n\ntitanic_train['Title'] = titanic_train['Title'].replace(others, 'Others')\ntitanic_train['Title'].value_counts()","37914b7f":"#Survival Trend among Title Group\n\ntitle_surv = titanic_train[['Title','Survived']].groupby('Title', as_index=False)['Survived'].mean()\ntitle_surv","97c61c99":"#drop 'ticket' column in training dataset\n\nprint('Columns in training dataset before dropping')\ndisplay(titanic_train.columns)\n\ntitanic_train.drop(columns='Ticket', inplace = True)\n\nprint('Columns in training dataset after dropping')\ndisplay(titanic_train.columns)","bfdc5b24":"#drop 'ticket' column in test dataset\n\nprint('Columns in test dataset before dropping')\ndisplay(titanic_test.columns)\n\ntitanic_test.drop(columns='Ticket', inplace = True)\n\nprint('Columns in test dataset after dropping')\ndisplay(titanic_test.columns)","6ba3c915":"#Add feature 'Child' for 'Age' <= 15\n\n#for training dataset\ntitanic_train.loc[titanic_train['Age']<=15, 'Child'] = 1\ntitanic_train.loc[titanic_train['Age']>15, 'Child'] = 0\n\n#for test dataset\ntitanic_test.loc[titanic_test['Age']<=15, 'Child'] = 1\ntitanic_test.loc[titanic_test['Age']>15, 'Child'] = 0","0aba71a4":"#Add feature 'Bot10' for bottom 10% quantile 'Fare' for each 'Pclass' & 'Embarked'\n\nfor embarked in set(titanic_train['Embarked']):\n    for pclass in set(titanic_train['Pclass']):\n        \n        bot10 = titanic_train[(titanic_train['Embarked']==embarked)&(titanic_train['Pclass']==pclass)]['Fare'].quantile(0.10)\n        \n        #for training dataset\n        titanic_train.loc[(titanic_train['Embarked']==embarked)&(titanic_train['Pclass']==pclass)\\\n        &(titanic_train['Fare']<bot10), 'Bot10'] = 1\n        \n        titanic_train.loc[(titanic_train['Embarked']==embarked)&(titanic_train['Pclass']==pclass)\\\n        &(titanic_train['Fare']>=bot10), 'Bot10'] = 0\n        \n        #for test dataset\n        titanic_test.loc[(titanic_test['Embarked']==embarked)&(titanic_test['Pclass']==pclass)\\\n        &(titanic_test['Fare']<bot10), 'Bot10'] = 1\n        \n        titanic_test.loc[(titanic_test['Embarked']==embarked)&(titanic_test['Pclass']==pclass)\\\n        &(titanic_test['Fare']>=bot10), 'Bot10'] = 0","d89309c7":"#Add feature 'Comp' \n\ntitanic_train.loc[:,'Comp'] = titanic_train['SibSp'] + titanic_train['Parch']\ntitanic_test.loc[:,'Comp'] = titanic_test['SibSp'] + titanic_test['Parch']\n\n#Recategorize 'Comp' into ordinal feature\n#for training dataset\ntitanic_train.loc[titanic_train['Comp']>=4, 'Comp'] = 0\ntitanic_train['Comp'] = titanic_train['Comp'].map({0:1, 1:2, 2:3, 3:4})\n\n#for test dataset\ntitanic_test.loc[titanic_test['Comp']>=4, 'Comp'] = 0\ntitanic_test['Comp'] = titanic_test['Comp'].map({0:1, 1:2, 2:3, 3:4})\n\n#Check if the result is as intended\ndisplay(titanic_train.head())\ndisplay(titanic_test.head())","2de91086":"#Drop 'SibSp','Parch'\ntitanic_train.drop(columns=['SibSp','Parch'], inplace=True)\ntitanic_test.drop(columns=['SibSp','Parch'], inplace=True)","02697166":"#Add feature 'Title'\n\ntitanic_test['Title'] = titanic_test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ntitle_group = titanic_test['Title'].unique()\nothers = np.delete(arr=title_group,obj=range(0,4))\n\ntitanic_test['Title'] = titanic_test['Title'].replace(others, 'Others')\ntitanic_test['Title'].value_counts()","8b1bce65":"#Recategorize 'Title' into ordinal feature\n#for training dataset\ntitanic_train['Title'] = titanic_train['Title'].map({'Mr':0, 'Others':1, 'Master':2, 'Miss':3, 'Mrs':4})\n\n#for test dataset\ntitanic_test['Title'] = titanic_test['Title'].map({'Mr':0, 'Others':1, 'Master':2, 'Miss':3, 'Mrs':4})","e0a00431":"#Drop 'Name'\ntitanic_train.drop(columns=['Name'], inplace=True)\ntitanic_test.drop(columns=['Name'], inplace=True)","d1b28ee4":"#Map 'Sex' column into numerical data\n\ntitanic_train['Sex'] = titanic_train['Sex'].map({'male':0, 'female':1})\ntitanic_test['Sex'] = titanic_test['Sex'].map({'male':0, 'female':1})","a5879b60":"#Create Dummy Feature for 'Embarked' column\n\ndummy_train = pd.get_dummies(titanic_train['Embarked'], prefix='Embarked', drop_first=True)\ndummy_test = pd.get_dummies(titanic_test['Embarked'], prefix='Embarked', drop_first=True)\n\n#Concatenate original dataset with the dummy dataset\n\ntitanic_train = pd.concat([titanic_train,dummy_train], axis='columns', ignore_index=False)\ntitanic_test = pd.concat([titanic_test,dummy_test], axis='columns', ignore_index=False)\n\n#Drop 'Embarked' column as it has been \ntitanic_train.drop(columns='Embarked', inplace=True)\ntitanic_test.drop(columns='Embarked', inplace=True)","40871563":"#Check if the result is as intended\ndisplay(titanic_train.head())\ndisplay(titanic_test.head())","d61f4a77":"#Import MinMaxScaler from sklearn library \nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\ntitanic_train[['Age','Fare']] = scaler.fit_transform(titanic_train[['Age','Fare']])\ntitanic_test[['Age','Fare']] = scaler.fit_transform(titanic_test[['Age','Fare']])\n\ndisplay(titanic_train.head())\ndisplay(titanic_test.head())","2f3feecb":"#import machine learning algorithm\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#import train test split\nfrom sklearn.model_selection import train_test_split\n\n#import evaluation metrics\nfrom sklearn.metrics import accuracy_score\n\n#import gridsearchCV for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV","398da1aa":"#define features that will be used in ML models\n\n#basic feature without additional feature\nbase_feature = list(titanic_train.columns.drop(['PassengerId', 'Survived','Title','Child','Bot10','Comp']))\n\n#basic feature + 'Title' Feature \nfeature_1 = list(titanic_train.columns.drop(['PassengerId', 'Survived','Bot10','Comp','Child']))\n\n#basic feature + 'Child' Feature \nfeature_2 = list(titanic_train.columns.drop(['PassengerId', 'Survived','Title','Bot10','Comp']))\n\n#basic feature + 'Bot10' Feature\nfeature_3 = list(titanic_train.columns.drop(['PassengerId', 'Survived','Title','Child','Comp']))\n\n#basic feature + 'Comp' Feature\nfeature_4 = list(titanic_train.columns.drop(['PassengerId', 'Survived','Title','Child','Bot10']))\n\n#basic feature + all additional feature\nfull_feature = list(titanic_train.columns.drop(['PassengerId', 'Survived']))\n\nfeatures = {'base feature':base_feature, 'feature 1':feature_1, 'feature 2':feature_2, \\\n            'feature 3':feature_3, 'feature 4':feature_4, 'full feature':full_feature}","f49e0f4c":"#dictionary to store accuracy score for each models and feature\nacc_score = {'feature':[],'logreg_score':[],'DT_score':[],'RF_score':[]}\n\nfor key, value in features.items():\n\n    #Split The Training Model \n    X_train, X_test, y_train, y_test = train_test_split(titanic_train[value], titanic_train['Survived'], \\\n    test_size = 0.2, random_state = 42)\n         \n    acc_score['feature'].append(key)\n\n    #Logistic Regression Model \n    logreg = LogisticRegression()\n    logreg.fit(X_train, y_train)\n    y_pred = logreg.predict(X_test)\n    \n    acc_score['logreg_score'].append(accuracy_score(y_test,y_pred)*100)\n    \n    #Decision Tree Model\n    decision_tree = DecisionTreeClassifier()\n    decision_tree.fit(X_train, y_train)\n    y_pred = decision_tree.predict(X_test)\n    \n    #acc_score['feature'].append(key)\n    acc_score['DT_score'].append(accuracy_score(y_test,y_pred)*100)\n    \n    #Random Forest Model\n    random_forest = RandomForestClassifier()\n    random_forest.fit(X_train, y_train)\n    y_pred = random_forest.predict(X_test)\n    \n    #acc_score['feature'].append(key)\n    acc_score['RF_score'].append(accuracy_score(y_test,y_pred)*100)\n    \nacc_score = pd.DataFrame.from_dict(acc_score)\nacc_score","65d0ae67":"#Check the importance of each feature \n\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances","3c1ec233":"#Define the whole training set \n\nfeature = list(titanic_train.columns.drop(['PassengerId', 'Survived','Child','Bot10']))\nX_train = titanic_train[feature]\ny_train = titanic_train['Survived']","862c6f98":"#hyperparameter tuning for logistic regression\n\n#set the tuning parameter\ntuning_param1 = {'C':[0.001,0.009,0.01,0.09,0.1,0.9,1,9,10,50],'penalty':['l2']}\ntuning1 = GridSearchCV(estimator=logreg,param_grid=tuning_param1, scoring='accuracy'\\\n, cv=3, verbose=2, n_jobs=-1, refit=True)\n\n#fit the data \ntuning1.fit(X_train,y_train)\n\ndisplay(tuning1.best_score_*100)\ndisplay(tuning1.best_params_)","956bf96b":"#hyperparameter tuning for decision tree\n\n#set the tuning parameter\ntuning_param2 = {\n    'criterion': ['gini','entropy'],\n    'max_depth': range(2,10),\n    'min_samples_split': range(2,10),\n    'min_samples_leaf': range(1,5)\n}\ntuning2 = GridSearchCV(estimator = decision_tree, param_grid = tuning_param2, scoring='accuracy'\\\n, cv = 3, verbose=2, n_jobs=-1, refit=True)\n\n#fit the data \ntuning2.fit(X_train, y_train)\n\ndisplay(tuning2.best_params_)\ndisplay(tuning2.best_score_*100)","9fbd03d9":"#hyperparameter tuning for random forest\n\n#set the tuning parameter\n#tuning_param3 = {\n    #'criterion': ['gini', 'entropy'],\n    #'max_depth': [5, 7, 9],\n    #'max_features': [3, 4, 5],\n    #'min_samples_leaf': [10, 15, 20], \n    #'min_samples_split': [20, 25, 30], \n    #'n_estimators': [100, 500, 1000] \n#}\n#tuning3 = GridSearchCV(estimator = random_forest, param_grid = tuning_param3,scoring='accuracy'\\\n#, cv = 3, verbose=2, n_jobs=-1, refit=True)\n\n#tuning3.fit(X_train,y_train)\n\n#display(tuning3.best_params_)\n#display(tuning3.best_score_*100)","9f99f59a":"#Define Training Dataset\nX_train = titanic_train[feature]\ny_train = titanic_train['Survived']\n\n#Define Test Dataset\nX_test = titanic_test[feature]\n\n#Apply the best algorithm & tuning parameter \nrandom_forest = RandomForestClassifier(criterion='gini', max_depth=7, max_features=3, min_samples_leaf=10\\\n,min_samples_split=20, n_estimators=100)\n\n#Fit training dataset to the algorithm\nrandom_forest.fit(X_train,y_train)\n\n#Make prediction \ny_pred = random_forest.predict(X_test)\n\n#Submission file \nsubmission = pd.DataFrame({'PassengerId':titanic_test['PassengerId'],'Survived':y_pred})\nsubmission.to_csv('submission_RF.csv', index=False)","d104482a":"# References \n\nThis notebook is created based on several references below :\n- <a href=\"https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\">Titanic Data Science Solution<\/a>\n- <a href=\"https:\/\/www.kaggle.com\/c\/titanic\">Titanic - Machine Learning from Disaster<\/a>\n- <a href=\"https:\/\/towardsdatascience.com\/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a\">One-Hot-Encoding, Multicollinearity and the Dummy Variable Trap<\/a>","e02a7bf3":"Now the \"Cabin\" column should be the last column that contains missing value\n\nSo we drop the \"Cabin\" column as it contains incomplete data","1a4b1594":"Next is to add new feature 'Child', 'Bot10' and 'Comp'","e3215498":"Conclusion : \n- Drop 'Ticket'\n- Add Feature 'Child' for 'Age' under 15 years\n- Add Feature 'Bot10' for bottom 10% quantile 'Fare' from each 'Pclass' & 'Embarked'\n- Add Feature 'Comp' from 'SibSp' + 'Parch', then drop the original feature \n- Add Feature 'Title' from 'Name', then drop 'Name'\n- Keep 'PassengerId', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked'","9a71c0a7":"There are several observation from the figure above : \n- In passenger class 1, generally we can say that higher fare price result on better survival chance except for embarkation port Queenstown where too little sample to generate meaningful conclusion.\n- For passenger class 2 and 3, the pattern are random as we observe some of the highest paying passenger did not survived.\n- Most of the deceased passenger is coming from bottom 10% fare price of each passenger class and embarkation port. We should investigate this by calculating the survival rate of bottom 10% and top 90%.","427ad3ce":"# TITANIC SURVIVAL PREDICTION PROBLEM","0f9c106d":"# Result Submission","5d9727a9":"The effect of embarkation port to the survival chance will be investigated in the following section. We also know that Female Passenger and Higher Passenger Class had better survival chance, so we're going to analyze the composition of passenger based on sex and passenger class to check if embarkation port did have a correlation to survival.","f8509541":"# Model Evaluation and Selection \n\nThe 'full feature' did not always score the highest in the Machine Learning models I use. I will check the importance of each feature as well.\n\nFurthermore, the result above is based on one particular randomized state, therefore I should use hyperparameter tuning to search for the best hyperparameter and use cross validation to further validate the accuracy of our model and avoid overfit. After that I'll be able to choose the best performing model to make the final prediction.","22604dd0":"After I define the features that will be run to determine whether the feature engineering increases the accuracy and what best machine learning model for this classification problem. ","597b10b1":"For passenger with 3 or less companion, we can see that there are a general trend that mean survival rate increases as the number of companions increases and the sum already accounts for more than 90% of the sample which I can say it good enough to represent the general trend. \n\nI also group the passenger who travel with 4 or more companion into one category and they have lower survival rate compare to passenger who travels alone. We can use this number of companions as one of the ordinal feature as follow :\n- passenger with 4 or more companions (group '0')\n- passenger with 0 companion (group '1')\n- passenger with 1 companion (group '2')\n- passenger with 2 companions (group '3') \n- passenger with 3 companions (group '4')\n\nNext, investigate the relation between ticket number and name vs survival","7f24efa3":"Apparently 'Title' is quite important, so I will try to add 'Title' into the final feature that will be used for final prediction","f9a452c5":"In this notebook, I will use machine learning tools to solve Titanic survival prediction problem. The dataset can be accessed <a href=\"https:\/\/www.kaggle.com\/c\/titanic\">here<\/a>\n\nThe goal is to build a predictive model based on information in the given training dataset to guess the survival of the passenger on the test dataset. This is a classficication problem, thus I will use machine learning algorithm best suited to solve classfication problem.\n\nSince the question is already been defined, the workflow I will use are stated as following : \n1. Data Understanding\n2. Data Preprocessing \n3. Exploratory Data Analysis (EDA)\n4. Feature Selection & Engineering\n5. Fit & Train Machine Learning Models \n6. Model Evaluation & Selection \n7. Result Submission\n\n*But keep in mind that this workflow is only to give a general sense of direction, I might jump between steps if it's necessary since the process is iterative and closed-cycle.\n","02012f93":"Since the standard deviation in that particular category of Passenger Class and Embarkation Port is almost 100% of mean value, it is more suitable to use median value instead of mean value.","c44de0b1":"One down ! We move on to the next variable, \"Embarked\". Let's take a look at the data with missing value ","00f3c507":"# Feature Selection & Engineering \n\nIn this section, I will use the information we got from EDA to further prepare the dataset before fit it to the machine learning algorithm.\n\nI will have to drop the 'Name' and 'Ticket' column first as we don't see any usable feature from those columns.","5488c87c":"We have created dummy features for 'Embarked'. Notice that the drop_first customisation is set as True to avoid multicolinearity and reduce the dimensionality of the features. You can read more about dummy variable, multicolinearity and dimensionality \n<a href=\"https:\/\/towardsdatascience.com\/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a\">here<\/a>\n\nYou may notice that the 'Age' and 'Fare' is relatively bigger than the other feature which may cause problem in weight and sensitivity problems in the mathematical operation. It needs to be scaled before we fit the data into the machine learning models","0cf2d180":"# Data Preprocessing \n\nThe missing data need to be addressed before we fit and train the data to the machine learning algorithm since it may impair the quality of the prediction.\n\nThis is an exciting steps since we can build some hypothesis on how to best fill the missing data since there are no the 'right' way to do it. For example, I'm going to take a look at the relation between Pclass, Sex and Age to fill the missing value for age.","d11ff69b":"I need to transform some of the remaining feature into numerical form as the machine learning models I am going to use will not process the non-numeric data. That includes 'Sex' and 'Embarked' column. \n\nThere are many techniques can be used to encode non-numerical data into numerical data, such as simply changing the label into numerical data such as '0','1', etc. The method works well in binary data, but may cause confusion on machine learning models as it may intepret them as ordinal data rather than nominal when there is more than 2 category. Another way to encode is to create dummy features which return '1' to its dummy and '0' to other dummies.","0461283e":"# Fit and Train Machine Learning Models","82bbfcf5":"There is no discoverable pattern in ticket number as it has 681 unique ticket number, it will not be relevant for predictive model as it contains some missing data. Therefore, 'Ticket' column will be dropped from training dataset.","35d44ace":"The training dataset comprises 891 rows and 12 columns, but there are missing data in age, cabin and embarked column. Meanwhile the test dataset also contain missing value in age, fare and cabin column.\n\nBefore we jump into handling the missing data, let's take a look on the overall statistics of each variable.","f7b8ff4b":"We can observe from the visualisation above that passenger below 15 years have better survival chance than those above. This information might be useful for feature engineering. \n\nThe two visualization above also confirms the hypothesis that Woman and Children have high survival rate.\n\nThere are no other general trend for age apart from the explanation above, so we may use it to generate feature and the drop the age column afterward.\n\nNext, we will see the survival chance of different passenger classes","619cc218":"As we've observed before, the survival rate drop as passenger class number increases except anomaly in Queenstown probably because of insufficient amount of data. \n\nThere is no anomaly observed in combination of Embarkation Port and Sex of Passenger, as expected female passenger generally have higher survival rate.\n\n\nAnother interesting fact is that passenger embarked from Cherbourg have higher survival rate than average survival rate  across all passenger class. While we can't concluded whether embarkation port did have correlation to survival, this fact can be considered as a feature in the predictive model.\n\nNext, we should take a look at the relation between number of Siblings\/Spouses and Parents\/Children versus the survival of passenger.","2d984e2b":"We check the dataset one more time to make sure that the dataset is ready to be used","7a20511e":"# Data Understanding \n\nUnderstanding the dataset is crucial first step in data science process because we can take a look at the dataset, determine its quality before we move into preprocessing.","e69a42d8":"For 'Name', I can extract title as a feature that can be useful in feature engineering.","f192af15":"The datasets are ready to be used in machine learning models.","525e0a61":"From the calculation above, we saw that generally the top 90% has better survival probability than the bottom 10% except the anomalies in passenger class 2 from embarkation port Cherbourg and Queenstown which may be caused by lack of sufficient amount of data.\n\nThe separation by 10% quantile fare made huge different on the survival rate, with some of the bottom 10% even have 0% survival rate. This information is essential and definitely should be considered as one of the feature in the predictive model.\n\nSo I will rectify my initial hypothesis which is \"Higher ticket fare could lead to higher chance of survival\" to \"Very low ticket fare correlate to very low chance of survival\" because based on the visualisation, higher ticket fare does not necessarily mean higher chance of survival.","a396599c":"I will split the training dataset in order the check the accuracy of each machine learning models before using the model to predict the outcome of test dataset.\n\nFirst I will train the algorithm with the basic feature and then check the accuracy when the additional feature is added and then add them all together.","96f2cb72":"We can see that as Passenger Class increases, the survivability rate decreases, confirming our hypothesis that Passenger Class can be a useful feature in predicting the survivability of passenger.\n\nNext on, let's check on the factor of Fare on survivability of the passenger. As I have assumed before that fare price is affected by Passenger Class and Embarkation Port, I will examine the Fare distribution of survived and deceased passenger based on their passenger class and embarkation port.","a015f29d":"There are some frequently occuring 'Title' such as 'Mr', 'Miss', 'Mrs' and 'Master' while we can group the rest of the title as 'Others'","1f0308b6":"Random Forest Classifier had the best accuracy therefore I will use it to make final prediction.","5ddd22f2":"We still have 1 missing value from Fare column in test dataset. Common sense tells us that fare are decided by Passenger class and embarkation port. ","9d6cebe9":"We can observe some interesting insights such as : \n- The survival rate of passenger from training dataset is 38.38% \n- Majority of the passenger is male (843 out of 1309)\n- Age contains several missing values in both training and test dataset, with a relative big standard variation.\n- Cabin have a lot of missing values with only 295 out of 1309 data points, therefore the data is not complete and we might drop the whole column since it can't be considered as a feature in our predictive model.\n- 1 missing value on fare column in test dataset.\n- Embarked has just 2 missing values, we can fill the missing value with the most frequent data 'S'","b579010b":"We saw different type of distribution and median across age and passenger class, therefore we need to fill the missing age value accordingly.\n\n\nNote that we use the age median value from the combination of both training and test since it contains missing value in both dataframes, so it makes more sense to consider the whole dataset","c61d7de8":"# Exploratory Data Analysis (EDA)\n\nIn Exploratory Data Analysis, I'm going to explore deeper into the dataset and find the relation between feature and desired outcome. We also can formulate some preliminary hipothesis, investigate and proof\/modify it through our exploration. Preliminary hypothesis is not mandatory but could be useful for Data Scientist in the beginning of EDA.\n\nFor example, my preliminary hypotheses are : \n- Women and Children could have higher possibility to survive since they are usually prioritized in the rescue process\n- Higher passenger class could have higher possibility to survive  \n- Higher ticket fare could lead to higher chance of survival\n- Number of sibling\/spouse and parent\/child on board could have some relation with the survival of passenger\n- Embarkation port does not have relation toward survival \n\n*Note that the hypothesis could alter as we make discovery through EDA","683f5ee4":"Since there is only one missing value, we will observe the fare value that match the passenger class and embarkation port profile of the missing data point (Pclass = 3 and Embarked = 'S')","38dc1932":"There are no general trend that can be observed from the number of Siblings\/Spouses or Parents\/Children to the number of survival. However, notice that majority of the passenger have at most 2 Siblings\/Spouses and 2 Parents\/Children, which can be a cut-off point and treating other data as outliers. Also there are a lot of the passenger travels alone.\n\nWe may further inspect the relation by using \"companion\" which is the sum of number of Siblings\/Spouses and Parents\/Children. ","65b9db49":"We can see that there's no more missing data in the training dataset and all the datatype is as expected, it's time to proceed to the next stage.","dad9e564":"From the visualization above, we can observe that 'Sex' can be one of the important features in our predictive model as it has quite distinct result over its variants.\n\nNext we observe the distribution of age and survivability","2469175d":"'Title' can be grouped as an ordinal group such as what I did with 'Comp'","91e7c95f":"We can see from the pie chart that the majority of the passenger embarked from Southampton. Interestingly, passenger who embarked from Cherbourg had higher survival rate (55.35%) compared to others which is closer to the the average survival rate (38.38%). We should take a look at the composition of the passenger from each embarkation port.","bd0eb8f1":"Notice that the amount of passenger increases from higher to lower passenger class generally except in Cherbourg where the major part of passenger is Passenger Class 1. This may be the factor that skewed its higher survivability rate compared to other embarkation port.\n\nOverall, there are more male passenger except in Queenstown where the proportion of female and male are almost the same. This may be the cause of Queenstown high survival rate despite majority of the passenger are in Passenger Class 3.\n\nMajority passenger embarked from Southampton consist of male and passenger class 3. This fact may correlate to lower than average survival rate for Passenger from Southampton.\n\nNevertheless, we should further investigate into the data before making any meaningful conclusion.","789a66fb":"\n\nHere we take a look at first 5 rows of the training dataset. It contains information about each passenger for each rows and different variable for each columns. We can group the variables :\n\nNumerical\n\n- Continuous : Age, Fare\n- Discrete : SibSp, Parch\n\nCategorical\n\n- Ordinal : Pclass\n- Nominal : PassengerId, Survived, Name, Sex, Ticket, Cabin, Embarked\n\n\n\nFrom those columns, notice that some value are missing in the cabin columns. So we investigate further for the missing value in the dataset.\n\n"}}