{"cell_type":{"3a15eeb2":"code","813a5c5b":"code","97ff7078":"code","4d39c9f3":"code","077d67d1":"code","ab68dfcf":"code","3b437e2c":"code","e83d171d":"code","910cb1a4":"code","2695120a":"code","bdcfddbc":"code","6adcfd24":"code","f627b01d":"code","2df04f2c":"code","b9136d3f":"code","8fafd090":"code","72eb024f":"code","0a181af1":"code","b43f08bc":"code","633e7548":"code","16331239":"code","52d5282e":"code","619293a1":"code","2b87d7cb":"code","be6d7cf0":"code","323559fb":"code","4e96c71d":"code","fe02f25b":"code","802f85d0":"code","77cf7bae":"code","b4053da6":"code","8365c010":"code","b58482c3":"code","a92508ca":"code","8d9320da":"code","f9d5c728":"code","efa37635":"code","fe0ac0ea":"code","aa6844ab":"code","6ea2795e":"code","ab40a1ca":"code","b7698b74":"code","dbf17c3c":"code","9cf08e54":"code","4fa18983":"code","8a1ea4a3":"code","b36b5382":"code","2f722437":"code","adb163e9":"code","6d3a56b0":"code","2bda0cb2":"code","406f7f2a":"code","fdc387bf":"markdown","f6a73f49":"markdown"},"source":{"3a15eeb2":"import pandas as pd\nimport numpy as np","813a5c5b":"# Importing Housing.csv\nhousing = pd.read_csv('\/kaggle\/input\/housing\/newhousing.csv')","97ff7078":"# Looking at the first five rows\nhousing.head()","4d39c9f3":"housing.shape","077d67d1":"# What type of values are stored in the columns?\nhousing.info()","ab68dfcf":"housing.columns","3b437e2c":"# Putting feature variable to X\nX = housing[['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad',\n       'guestroom', 'basement', 'hotwaterheating', 'airconditioning',\n       'parking', 'prefarea', 'semi-furnished', 'unfurnished',\n       'areaperbedroom', 'bbratio']]\n\n# Putting response variable to y\ny = housing['price']","e83d171d":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.distplot(y)\nplt.show()","910cb1a4":"y.skew()","2695120a":"from scipy import stats\ny=stats.boxcox(y)[0]","bdcfddbc":"sns.distplot(y)\nplt.show()","6adcfd24":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Let's see the correlation matrix \nplt.figure(figsize = (16,10))     # Size of the figure\nsns.heatmap(X.corr(),annot = True)\nplt.show()","f627b01d":"#creating correlation matrix for the given data\ncorrmat = np.corrcoef(X.transpose())","2df04f2c":"#Make a diagonal matrix with diagonal entry of Matrix corrmat\np=np.diagflat(corrmat.diagonal())","b9136d3f":"# subtract diagonal entries making all diagonals 0\ncorrmat_diag_zero = corrmat - p\nprint(\"max corr:\",corrmat_diag_zero.max(), \", min corr: \", corrmat_diag_zero.min(),)\n","8fafd090":"# Retrieve the (i,j) index for which matrix has maximum value\nij_max = np.unravel_index(corrmat_diag_zero.argmax(), corrmat_diag_zero.shape)\nprint(\"ij_max\",ij_max)\nprint(\"Maximum correlation :\",corrmat_diag_zero[ij_max])","72eb024f":"# Retrieve the (i,j) index for which matrix has absolute minimum value\nij_min = np.unravel_index(np.absolute(corrmat).argmin(), corrmat.shape)\nprint(\"ij_min\",ij_min)\nprint(\"Minimum correlation :\",corrmat_diag_zero[ij_min])","0a181af1":"#random_state is the seed used by the random number generator, it can be any integer.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n\nfrom sklearn.preprocessing import PowerTransformer\n","b43f08bc":"X_train.shape\n# We have 15 variables after splitting the data","633e7548":"scaler = PowerTransformer()\nXtrain=scaler.fit_transform(X_train) \nXtest=scaler.transform(X_test) ","16331239":"Xtrain.shape","52d5282e":"xtrain_df = pd.DataFrame(Xtrain,columns=X_train.columns)\nXtrain[:,0].max()\nxtrain_df['area'].max()","619293a1":"y_train.shape","2b87d7cb":"xtest_df = pd.DataFrame(Xtest,columns=X_train.columns)\nxtest_df['guestroom'].min()","be6d7cf0":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import  linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Create linear regression object\nregr = linear_model.LinearRegression()\n# Train the model using the training sets\nregr.fit(Xtrain, y_train)\n# Make predictions using the testing set\ny_pred = regr.predict(Xtest)","323559fb":"print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n","4e96c71d":"# Explained variance score: 1 is perfect prediction\nprint('R2 score: %.2f' % r2_score(y_test, y_pred))","fe02f25b":"from sklearn.metrics import explained_variance_score\nexplained_variance_score(y_test, y_pred) ","802f85d0":"#Importing the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA( random_state=42)","77cf7bae":"#Doing the PCA on the train data\npca.fit(Xtrain)","b4053da6":"components = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1],'Feature':X.columns })\ncomponents","8365c010":"colnames = list(X_train.columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1],'PC3':pca.components_[2],\n                       'PC4':pca.components_[3],'PC5':pca.components_[4],\n                       'PC6':pca.components_[5],'PC7':pca.components_[6],'PC8':pca.components_[7],\n                       'PC9':pca.components_[8],'PC10':pca.components_[9],'PC11':pca.components_[10],\n                       'PC12':pca.components_[11],'PC13':pca.components_[12],\n                       'PC14':pca.components_[13],'PC15':pca.components_[14],'Feature':colnames})","b58482c3":"pcs_df","a92508ca":"pca.explained_variance_","8d9320da":"print(\"pca.explained_variance_ratio_: \",pca.explained_variance_ratio_.round(3)*100)","f9d5c728":"print (pca.explained_variance_ratio_.cumsum())","efa37635":"np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)","fe0ac0ea":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.vlines(x=10, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=.946, xmax=15, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","aa6844ab":"product = np.dot(pca.components_[0],pca.components_[1])\nproduct.round(5)","6ea2795e":"%matplotlib inline\nfig = plt.figure(figsize = (20,12))\nplt.scatter(components.PC1, components.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(components.Feature):\n    plt.annotate(txt, (components.PC1[i],components.PC2[i]))\nplt.tight_layout()\nplt.show()","ab40a1ca":"pca_train = pca.transform(Xtrain)\npca_train.shape","b7698b74":"pca_train","dbf17c3c":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(pca_train.transpose())\ncorrmat","9cf08e54":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (20,10))\nsns.heatmap(corrmat,annot = True)\nplt.show()","4fa18983":"# 1s -> 0s in diagonals\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n# we see that correlations are indeed very close to 0","8a1ea4a3":"#Applying selected components to the test data - 13 components\npca_test = pca.transform(Xtest)\npca_test.shape","b36b5382":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import  linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Create linear regression object\nregrpca = linear_model.LinearRegression()\n# Train the model using the principal components of the transformed training sets\nregrpca.fit(pca_train, y_train)\n# Make predictions using the principal components of the transformed testing set\ny_pca_pred = regrpca.predict(pca_test)\n","2f722437":"print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pca_pred))\n","adb163e9":"# Explained variance score: 1 is perfect prediction\nprint('R2 score: %.2f' % r2_score(y_test, y_pca_pred))","6d3a56b0":"pca = PCA(n_components=10,random_state=42)\n#Scale and transform data to get Principal Components\n\nXtrain_reduced = pca.fit_transform(Xtrain)\nXtest_reduced = pca.transform(Xtest)\nregrpca6 = linear_model.LinearRegression()\n# Train the model using the principal components of the transformed training sets\nregrpca6.fit(Xtrain_reduced, y_train)\n# Make predictions using the principal components of the transformed testing set\ny_pred = regrpca6.predict(Xtest_reduced)\n\n","2bda0cb2":"print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n","406f7f2a":"# Explained variance score: 1 is perfect prediction\nprint('R2 score: %.2f' % r2_score(y_test, y_pred))","fdc387bf":"### Importing and Understanding Data","f6a73f49":"## Splitting Data into Training and Testing Sets"}}