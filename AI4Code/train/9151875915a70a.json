{"cell_type":{"fadc8764":"code","f04329bd":"code","b01c95a5":"code","b527961d":"code","861d0ffe":"code","a7aee15b":"code","c8a104ad":"code","c6b4ee1b":"code","c663eb60":"code","67e80e39":"code","e851ae7c":"code","8705676c":"code","03bc630b":"code","34a9fcbf":"code","2e0899ab":"code","fa557824":"code","eb8f6909":"code","b1d90e61":"code","8dd06432":"code","79d44380":"code","fa9ac7cb":"code","88930c38":"code","3e347c42":"code","f6bea99c":"code","8d258983":"code","16cf1539":"code","4528a2d7":"code","7194f13d":"code","96a07e83":"code","f3aeca06":"code","4b750594":"code","cb863742":"code","3fe834ac":"code","207d5eac":"code","bfbcce75":"code","e6483089":"code","cf654f78":"markdown","65584148":"markdown","6dfdb5ff":"markdown","12b1298c":"markdown","e6f48ffa":"markdown","ed543201":"markdown","4cf4d4a0":"markdown","dec7af1f":"markdown","9c467043":"markdown","8325de7f":"markdown","6f909d9b":"markdown","4fc1ba08":"markdown","2bcca85a":"markdown","53ef2859":"markdown","433c824a":"markdown","86d19de8":"markdown","b9d5d923":"markdown","ff7aef32":"markdown","96dcec5b":"markdown","9ef978ea":"markdown","fc1b2f73":"markdown","49e780b7":"markdown","a8e28461":"markdown","b35b6703":"markdown","fb7c8b7f":"markdown","96ff146e":"markdown","bc7002ee":"markdown"},"source":{"fadc8764":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV\n\n# Modelling - sklearn\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Modeling - NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import ReduceLROnPlateau\n#import tensorflow as tf\n\n# Metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error as mse\n\nimport warnings\nwarnings.simplefilter('ignore')","f04329bd":"# Download training data\ntrain = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/train.csv')","b01c95a5":"# Display the first 5 rows of the training dataframe.\ntrain.head()","b527961d":"# Information for training data\ntrain.info()","861d0ffe":"# Download test data\ntest = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/test.csv')","a7aee15b":"# Display the 7 last rows of the training dataframe\ntest.tail()","c8a104ad":"test.info()","c6b4ee1b":"# Select the stations with the most data in training dataset\ntrain = train.drop(['Id','3','4','5','6','7'], axis = 1)\ntrain = train.dropna().reset_index(drop=True)\ntrain.info()","c663eb60":"# Display the statistics for training data\ntrain.describe()","67e80e39":"# Selecting a target featute and removing it from training dataset\ntarget = train.pop('target')","e851ae7c":"# Select the stations with the most data in test dataset\ntest = test.drop(['Id','3','4','5','6','7'], axis = 1)\ntest = test.dropna().reset_index(drop=True)","8705676c":"# Display basic information about the test data\ntest.info()","03bc630b":"# Training data splitting to new training (part of the all training) and validation data\ntrain_all = train.copy()\ntarget_all = target.copy()\ntrain, valid, target_train, target_valid = train_test_split(train_all, target_all, test_size=0.3, random_state=0)","34a9fcbf":"train","2e0899ab":"# Display information about new training data\ntrain.info()","fa557824":"# Display information about validation data\nvalid.info()","eb8f6909":"def acc(y_true, y_pred):\n    # Calculation accuracy of prediction\n    return r2_score(y_true, y_pred)","b1d90e61":"# Creation the dataframe with the resulting score of all models\nresult = pd.DataFrame({'model' : ['NN Regressor', 'NN Regressor with Dropout', 'MLP Regressor'], \n                       'train_score': 0, 'train_mse': 0, 'valid_score': 0, 'valid_mse': 0})\nresult","8dd06432":"batch_size_num = 48\n#batch_size_num = int(len(train)\/5)\n#batch_size_num","79d44380":"%%time\ndef build_nn():\n\n    # Initializing the NN with 3 layers including 2 hidden layers\n    model = Sequential()\n\n    # The first hidden layer of the NN with input data\n    model.add(Dense(units=4, activation='relu', input_shape=(len(train.columns),)))\n    \n    # The second hidden layer of the NN\n    model.add(Dense(units=3, activation='relu'))\n    \n    # The output layer\n    model.add(Dense(units=1, activation='sigmoid'))\n\n    # Compiling the NN\n    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_mse', \n                                    patience=2, \n                                    verbose=1, \n                                    factor=0.5, \n                                    min_lr=0.001)\n    return model\n\nnn_model = build_nn()\nnn_model.fit(train, target_train, batch_size=batch_size_num, epochs=500, validation_data=(valid, target_valid), verbose=0)\n\n# Drawing metrics plot\nplt.plot(nn_model.history.history['mse'])\nplt.title('Metrics of NN model')\nplt.xlabel('Epochs')\nplt.ylabel('Metrics \"Mean Square Error\"') \nplt.show()\n\n# Prediction for training data\ny_train_nn = nn_model.predict(train)\n\n# Accuracy of model\nacc_pred = round(acc(target_train, y_train_nn), 1)\nprint(f'Accuracy of NN model model training is {acc_pred}')\n\n# Save to result DataFrame\nresult.loc[result['model'] == 'NN Regressor', 'train_score'] = acc_pred\nresult.loc[result['model'] == 'NN Regressor', 'train_mse'] = mse(target_train, y_train_nn)","fa9ac7cb":"# NN structure and parameters\nnn_model.summary()","88930c38":"# Print rounded acc_pred to 2 decimal values after the text\ny_val_nn = nn_model.predict(valid)\nacc_pred_valid = round(acc(target_valid, y_val_nn),2)\nresult.loc[result['model'] == 'NN Regressor', 'valid_score'] = acc_pred_valid\nresult.loc[result['model'] == 'NN Regressor', 'valid_mse'] = round(mse(target_valid, y_val_nn),2)\nprint(f'Accuracy of NN Regressor model prediction for valid dataset is {acc_pred_valid}')","3e347c42":"%%time\ndef build_nn2():\n\n    # Initializing the NN with 3 layers including 2 hidden layers and Dropout\n    model = Sequential()\n\n    # The first hidden layer of the NN with input data\n    model.add(Dense(units=4, activation='relu', input_shape=(len(train.columns),)))\n    \n    # Dropout\n    model.add(Dropout(0.2))\n    \n    # The second hidden layer of the NN\n    model.add(Dense(units=3, activation='relu'))\n    \n    # The output layer\n    model.add(Dense(units=1, activation='sigmoid'))\n\n    # Compiling the NN\n    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n    \n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_mse', \n                                    patience=3, \n                                    verbose=1, \n                                    factor=0.05, \n                                    min_lr=0.001)\n    \n    return model\n\nnn_model2 = build_nn2()\nnn_model2.fit(train, target_train, batch_size=batch_size_num, epochs=500, validation_data=(valid, target_valid), verbose=2)\n\n# Drawing metrics plot\nplt.plot(nn_model2.history.history['mse'])\nplt.title('Metrics of NN Regressor with Dropout')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Square Error') \nplt.show()\n\n# Prediction for training data\ny_train_nn2 = nn_model2.predict(train)\n\n# Accuracy of model\nacc_pred = round(acc(target_train, y_train_nn2), 2)\nprint(f'Accuracy of NN Regressor with Dropout training is {acc_pred}')\n\n# Save to result DataFrame\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'train_score'] = acc_pred\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'train_mse'] = round(mse(target_train, y_train_nn),2)","f6bea99c":"# Print rounded acc_pred to 2 decimal values after the text\ny_val_nn2 = nn_model2.predict(valid)\nacc_pred_valid = round(acc(target_valid, y_val_nn2),2)\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'valid_score'] = acc_pred_valid\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'valid_mse'] = mse(target_valid, y_val_nn2)\nprint(f'Accuracy of NN Regressor with Dropout prediction for valid dataset is {acc_pred_valid}')","8d258983":"%%time\n# MLPRegressor\nmlp = MLPRegressor()\nparam_grid = {'hidden_layer_sizes': [i for i in range(2,10)],\n              'solver': ['sgd'],\n              'learning_rate': ['adaptive'],\n              'max_iter': [100]\n              }\n\n# Training model\nmlp_CV = GridSearchCV(mlp, param_grid=param_grid, cv=5, verbose=False)\nmlp_CV.fit(train, target_train)\nprint(mlp_CV.best_params_)\n\n# Prediction for training data\ny_train_mlp = mlp_CV.predict(train)\n\n# Accuracy of model\nacc_pred = round(acc(target_train, y_train_mlp), 2)\nprint(f'Accuracy of MLP Regressor model training is {acc_pred}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'MLP Regressor', 'train_score'] = acc_pred\nresult.loc[result['model'] == 'MLP Regressor', 'train_mse'] = mse(target_train, y_train_nn)","16cf1539":"# Print rounded acc_pred to 2 decimal values after the text\ny_val_mlp = mlp_CV.predict(valid)\nacc_pred_valid = round(acc(target_valid, y_val_mlp),2)\nresult.loc[result['model'] == 'MLP Regressor', 'valid_score'] = acc_pred_valid\nresult.loc[result['model'] == 'MLP Regressor', 'valid_mse'] = round(mse(target_valid, y_val_mlp),2)\nprint(f'Accuracy of MLP Regressor model prediction for valid dataset is {acc_pred_valid}')","4528a2d7":"# Prediction of target for test data for all models\ny_test_nn = nn_model.predict(test)\ny_test_nn2 = nn_model2.predict(test)\ny_test_mlp = mlp_CV.predict(test)","7194f13d":"def plot_prediction(target, y_nn, y_nn2, y_mlp, data_name, MAV=0.5):\n    # Building plot with target, Maximum allowable value (MAV) and \n    # prediction for the data_name (training, validation or test) data by 3 models\n    \n    x = np.arange(len(y_nn))\n    plt.figure(figsize=(16,10))\n    if target is not None:\n        plt.scatter(x, target, label = \"Target data\", color = 'g')\n    plt.scatter(x, y_nn, label = \"NN prediction\", color = 'b')\n    plt.scatter(x, y_nn2, label = \"NN with Dropout\", color = 'm')\n    plt.scatter(x, y_mlp, label = \"MLP prediction\", color = 'y')\n    plt.plot(x, np.full(len(y_nn), MAV), label = \"Maximum allowable value\", color = 'r')\n    plt.title(f'Prediction for the {data_name} data')\n    plt.legend(loc='best')\n    plt.grid(True)","96a07e83":"plot_prediction(target_train, y_train_nn, y_train_nn2, y_train_mlp, 'training')","f3aeca06":"plot_prediction(target_valid, y_val_nn, y_val_nn2, y_val_mlp, 'validation')","4b750594":"plot_prediction(None, y_test_nn, y_test_nn2, y_test_mlp, 'test')","cb863742":"# Display results of modeling\nresult.sort_values(by=['valid_score', 'train_score'], ascending=False)","3fe834ac":"# Select models with minimal overfitting\nresult_best = result[(result['train_score'] - result['valid_score']).abs() < 5]\nresult_best.sort_values(by=['valid_score', 'train_score'], ascending=False)","207d5eac":"# Select the best model\nresult_best.nlargest(1, 'valid_score')","bfbcce75":"# Find a name of the best model (with maximal valid score)\nbest_model_name = result_best.loc[result_best['valid_score'].idxmax(result_best['valid_score'].max()), 'model']","e6483089":"print(f'The best model is \"{best_model_name}\"')","cf654f78":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","65584148":"### TASK: Experiment with:\n* batch size\n* units in layers \n* activation functions\n* epochs number\n* patience and factor in ReduceLROnPlateau\n* Dropout parameter","6dfdb5ff":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE & Preprocessing data](#3)\n    - [Statistics & FE](#3.1)\n1. [Modeling](#4)\n    - [NN Regressor](#4.1)\n    - [NN Regressor with Dropout](#4.2)\n    - [MLP Regressor](#4.3)    \n1. [Test prediction](#5)\n1. [Results visualization](#6)\n1. [Select the best model](#7)","12b1298c":"## 5. Test prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","e6f48ffa":"![image.png](attachment:image.png)\n* 1 - the source of the river (see at the station first on the left), \n* ....\n* 8 (target) - the place of water intake in Vinnytsia (see at the station in the lower right corner)","ed543201":"### 4.3. MLP Regressor<a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","4cf4d4a0":"Dataset has data of the Ammonium ions concentration in river water (the maximum permissible value in Ukraine is 0.5 mg\/cub. dm).\n\nAmmonium ions (NH4) concentration is measured in mg\/cub. dm (ie milligrams in the cubic decimeter).\n\nDatasets has data of river water quality from 8 consecutive stations of the state water monitoring system for Pivdennyi Bug river (from the source of the river to the water intake of the city of Vinnytsia).\n\nTarget is a NH4 concentration in the river crossection with the water intake of the Vinnytsia city.\n\nData for the 1997-2019.","dec7af1f":"**ADDITIONAL TASK:** Experiment with number of hidden layers and metrics.","9c467043":"## Dataset [Ammonium prediction in river water](https:\/\/www.kaggle.com\/vbmokin\/ammonium-prediction-in-river-water)\n### Thanks to @vbmokin https:\/\/www.kaggle.com\/vbmokin\/covid-19-ua-one-region-forecasting","8325de7f":"The analysis showed that many values are only available in stations 1 and 2, while others have much less data. I propose select only these two stations.","6f909d9b":"## 7. Select the best model <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","4fc1ba08":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","2bcca85a":"**ADDITIONAL TASK:** Experiment with number of hidden layers and metrics.","53ef2859":"### 4.2. NN Regressor with Dropout<a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","433c824a":"## 6. Visualization<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","86d19de8":"### 4.1. NN Regressor<a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","b9d5d923":"## 3. EDA & FE & Preprocessing data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","ff7aef32":"### Possible Tasks:\n\n* Analysis of data dependences, including EDA.\n\n* Prediction the target data (water quaity in the target station) with the highest accuracy.\n\n* Analysis of impact on the prediction accuracy in target station from the different number of stations (1, 2, ... 7).","96dcec5b":"### Map of the stations:\nhttp:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index\n\n![image.png](attachment:image.png)\n\nThe upper reaches of the Pivdennyi Bug river","9ef978ea":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","fc1b2f73":"### 3.3. Training data splitting<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","49e780b7":"## Acknowledgements\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)\n* [Datasets for river water quality prediction](https:\/\/www.kaggle.com\/vbmokin\/datasets-for-river-water-quality-prediction)\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models)\n* [The system \"MONITORING AND ENVIRONMENTAL ASSESSMENT OF WATER RESOURCES OF UKRAINE\", State Agency of Water Resources of Ukraine](http:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index)","a8e28461":"### TASK: Experiment with:\n* batch size\n* units in layers \n* activation functions\n* epochs number\n* patience and factor in ReduceLROnPlateau","b35b6703":"### TASK: Experiment with:\n* hidden_layer_sizes (maximum value)\n* max_iter\n* cv (cross-validation)","fb7c8b7f":"### 3.1. Statistics & FE<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","96ff146e":"### Example of the NN structure:\n\n![image.png](attachment:image.png)","bc7002ee":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)"}}