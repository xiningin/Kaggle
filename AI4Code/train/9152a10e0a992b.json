{"cell_type":{"8d525faa":"code","1c7bc31a":"code","a782c57c":"code","4df04b37":"code","2f845ab7":"code","d13487c7":"code","04546449":"code","af3b5a39":"code","3f20bc47":"code","aacc5d6c":"code","8d4cc6c8":"code","8c396a43":"code","935a8ced":"code","0c3ea0bd":"code","26ebf278":"code","bedb0e29":"code","08d48e7a":"code","95264787":"markdown","2aed08ca":"markdown","756498e2":"markdown"},"source":{"8d525faa":"import tensorflow as tf\nimport numpy as np\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM","1c7bc31a":"#explore the dataset\ndataset=pd.read_csv('..\/input\/ece657aw20asg4coronavirus\/time_series_covid19_confirmed_global.csv')\ndataset.columns.values","a782c57c":"#extract the data \nontario_data=dataset[dataset['Province\/State'].isin(['Ontario'])]\nontario_data=ontario_data.drop(['Province\/State','Country\/Region','Lat','Long'],axis=1)\ndate=ontario_data.columns.values.tolist()\nfeature=ontario_data.values\ndata = { 'number':feature[0]\n      }\ndf = pd.DataFrame(data)\ndf.index=date\ndf","4df04b37":"#drop the zero number\ndf=df.drop(index=['1\/22\/20', '1\/23\/20', '1\/24\/20', '1\/25\/20'])\ndf","2f845ab7":"#preprocess the data(we have to normalization of data to the range 0 to 1 to fit the LSTM architecture)\ndata_df=df['number'].values\ndata_df=data_df.reshape(-1,1)\nscaler = MinMaxScaler(feature_range=(0, 1))\ndata = scaler.fit_transform(data_df)","d13487c7":"#create dataset function(step refers to how much former information has relationship with the label)\ndef create_dataset(dataset, step):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-step-1):\n        a = dataset[i:(i+step), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + step, 0])\n    return np.array(dataX), np.array(dataY)","04546449":"#create the dataset and split into train and test\n#For the number of train set is very small,I just pick 10% of the data to be test set.\nX,Y=create_dataset(data,4)\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.1,random_state=42)\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\nX_train.shape","af3b5a39":"#define the architecture of the model\ndef create_model():\n  model = Sequential()\n  model.add(LSTM(32,input_shape=(4,1),return_sequences=True,activation='relu'))\n  model.add(LSTM(64,return_sequences=True))\n  model.add(LSTM(128))\n  model.add(Dense(1))\n  return model","3f20bc47":"#train the model\nmodel=create_model()\nlr_reduce =tf.keras.callbacks.ReduceLROnPlateau('val_loss',patience=3,factor=0.3,min_lr=0.00001)\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['mae'])\nmodel.fit(X_train,Y_train, epochs=200, batch_size=1, validation_data=(X_test, Y_test),validation_freq=10,callbacks=[lr_reduce])","aacc5d6c":"#prediction\npredictions=[data_df[-1]]\nnewdata=data.copy()\n\nfor i in range(10):\n  prediction=model.predict(newdata[-4:,:].reshape(1,4,1))\n  predictions.append(float(prediction*predictions[-1]))\n  newdata=np.append(newdata,prediction, axis=0)","8d4cc6c8":"#plot\ndate_predict=list(range(84,84+8))\nplt.plot(data_df)\nplt.plot(date_predict,predictions[3:])\nplt.legend(['Origin', 'Prediction'], loc='upper left')","8c396a43":"#using the dataset which I created\ndataset_waterloo=pd.read_csv('..\/input\/time-series-covid19-confirmed-waterloo\/time_series_covid19_confirmed_waterloo.csv')\ndata_df_w=dataset_waterloo['Cumulative cases'].values\ndata_df_w=data_df_w.reshape(-1,1)\nscaler = MinMaxScaler(feature_range=(0, 1))\ndata_w = scaler.fit_transform(data_df_w)","935a8ced":"#create and split\nX_W,Y_W=create_dataset(data_w,4)\nX_train_W,X_test_W,Y_train_W,Y_test_W=train_test_split(X_W,Y_W,test_size=1,random_state=42)\nX_train_W = np.reshape(X_train_W, (X_train_W.shape[0],X_train_W.shape[1],1))\nX_test_W = np.reshape(X_test_W, (X_test_W.shape[0],X_test_W.shape[1],1))\nX_train_W.shape","0c3ea0bd":"#using the same weight to predict the data\npredictions=[data_df_w[-1]]\nnewdata=data.copy()\n\nfor i in range(10):\n  prediction=model.predict(newdata[-4:,:].reshape(1,4,1))\n  predictions.append(float(prediction*predictions[-1]))\n  newdata=np.append(newdata,prediction, axis=0)","26ebf278":"#plot\ndate_predict=list(range(45,45+8))\nplt.plot(data_df_w)\nplt.plot(date_predict,predictions[3:])\nplt.legend(['Origin', 'Prediction'], loc='upper left')","bedb0e29":"#retrain the model by the dataset of the region of Waterloo\nmodel=create_model()\nlr_reduce =tf.keras.callbacks.ReduceLROnPlateau('val_loss',patience=3,factor=0.3,min_lr=0.00001)\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['mae'])\nmodel.fit(X_train_W,Y_train_W, epochs=200, batch_size=1, validation_data=(X_test_W, Y_test_W),validation_freq=10,callbacks=[lr_reduce])","08d48e7a":"#predict it\npredictions=[data_df_w[-1]]\nnewdata=data.copy()\n\nfor i in range(10):\n  prediction=model.predict(newdata[-4:,:].reshape(1,4,1))\n  predictions.append(float(prediction*predictions[-1]))\n  newdata=np.append(newdata,prediction, axis=0)\ndate_predict=list(range(45,45+8))\nplt.plot(data_df_w)\nplt.plot(date_predict,predictions[3:])\nplt.legend(['Origin', 'Prediction'], loc='upper left')","95264787":"### I have shared the data file on gooledrive and feel free to use it.The file link:https:\/\/drive.google.com\/file\/d\/1N1BKwjxzI-ZGPV0dOuKGV8z1bWbNFAnr\/view?usp=sharing\n### And The data was gather from https:\/\/www.regionofwaterloo.ca\/en\/health-and-wellness\/positive-cases-in-waterloo-region.aspx and you could upgrade the data refers to that website.","2aed08ca":"# Conlusion\n## 1)We can find that using different weights to predict the same data will result different,which means if we want to predict a new data set,even though we use the same model architecture,we have to retrain the model to get the new weight which may fit our new data set.\n## 2)From the above plot, we can find predicted by model,the confirmed cases will surpass 1400 in 10 days if we don't take any action. Maybe the number is overestimated,but we still have to stay at home and do not go out except essiential reason. ","756498e2":"### For this Notebook, firstly I use the origin dataset to predict the confirmed cases in Ontario with LSTM which is an improved RNN architecture. I implement it by using tensorflow and Keras which we used in assignment 3.After that, I created a dataset which includes the number of cumulative cases by symptom onset date in the region of Waterloo.Then I retrian my model and predict the number of confirmed cases in a few days. "}}