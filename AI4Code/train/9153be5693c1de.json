{"cell_type":{"7c6a7f2b":"code","363632e9":"code","8a7212d5":"code","d5755fac":"code","10826766":"code","85263559":"code","043296bf":"code","50091e5a":"code","1570fbc3":"code","888024b6":"code","da518133":"markdown","6aee880c":"markdown","1461086b":"markdown","1c7e624e":"markdown","5c441a6b":"markdown","068b1d00":"markdown","a78dcddf":"markdown","8e290fb0":"markdown","e028129b":"markdown","36ff439b":"markdown"},"source":{"7c6a7f2b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import StandardScaler","363632e9":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain.head()","8a7212d5":"# concate dataset(train, test)\ndf = pd.concat(objs = [train, test], axis = 0).reset_index(drop = True)\n\n# fill null-values in Age\nage_by_pclass_sex = df.groupby(['Sex', 'Pclass'])['Age'].median()\ndf['Age'] = df.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n\n# create Cabin_Initial feature\ndf['Cabin_Initial'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['A', 'B', 'C', 'T'], 'ABCT')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['D', 'E'], 'DE')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['F', 'G'], 'FG')\ndf['Cabin_Initial'].value_counts()\n\n# fill null-values in Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')\n\n# fill null-values in Fare\ndf['Fare'] = df['Fare'].fillna(df.groupby(['Pclass', 'Embarked'])['Fare'].median()[3]['S'])\n\n\n# binding function for Age and Fare\ndef binding_band(column, binnum):\n    df[column + '_band'] = pd.qcut(df[column].map(int), binnum)\n\n    for i in range(len(df[column + '_band'].value_counts().index)):\n        print('{}_band {} :'.format(column, i), df[column + '_band'].value_counts().index.sort_values(ascending = True)[i])\n        df[column + '_band'] = df[column + '_band'].replace(df[column + '_band'].value_counts().index.sort_values(ascending = True)[i], int(i))\n        \n    df[column + '_band'] = df[column + '_band'].astype(int)    \n    \n    return df.head()\n\nbinding_band('Age',8)\nbinding_band('Fare', 6)\n\n\n# create Initial feature\ndf['Initial'] = 0\nfor i in range(len(df['Name'])):\n    df['Initial'].iloc[i] = df['Name'][i].split(',')[1].split('.')[0].strip()\n\nMrs_Miss_Master = []\nOthers = []\n\nfor i in range(len(df.groupby('Initial')['Survived'].mean().index)):\n    if df.groupby('Initial')['Survived'].mean()[i] > 0.5:\n        Mrs_Miss_Master.append(df.groupby('Initial')['Survived'].mean().index[i])\n    elif df.groupby('Initial')['Survived'].mean().index[i] != 'Mr':\n        Others.append(df.groupby('Initial')['Survived'].mean().index[i])\n    \ndf['Initial'] = df['Initial'].replace(Mrs_Miss_Master, 'Mrs\/Miss\/Master')\ndf['Initial'] = df['Initial'].replace(Others, 'Others')    \n    \n# create Alone feature\ndf['Alone'] = 0\ndf['Alone'].loc[(df['SibSp'] + df['Parch']) == 0] = 1\n\n# create Companinon's survival rate feature\ndf['Ticket_Number'] = df['Ticket'].replace(df['Ticket'].value_counts().index, df['Ticket'].value_counts())\ndf['Family_Size'] = df['Parch'] + df['SibSp'] + 1\ndf['Companion_Survival_Rate'] = 0\nfor i, j in df.groupby(['Family_Size', 'Ticket_Number'])['Survived'].mean().index:\n    df['Companion_Survival_Rate'].loc[(df['Family_Size'] == i) & (df['Ticket_Number'] == j)] = df.groupby(['Family_Size', 'Ticket_Number'])[\"Survived\"].mean()[i, j]\n    \ncomb_sum = df.loc[df['Family_Size'] == 5]['Survived'].sum() + df.loc[df['Ticket_Number'] == 3]['Survived'].sum()\ncomb_counts = df.loc[df['Family_Size'] == 5]['Survived'].count() + df.loc[df['Ticket_Number'] == 3]['Survived'].count()\nmean = comb_sum \/ comb_counts\n\ndf['Companion_Survival_Rate'] = df['Companion_Survival_Rate'].fillna(mean)    \n\n# select categorical features\ncate_col = []\nfor i in [4, 11, 12, 15]:\n    cate_col.append(df.columns[i])\n\ncate_df = pd.get_dummies(df.loc[:,(cate_col)], drop_first = True)\ndf = pd.concat(objs = [df, cate_df], axis = 1).reset_index(drop = True)\n\ndf = df.drop(['Name', 'Sex', 'Age', 'Ticket', 'Fare', 'Embarked', 'Cabin_Initial', 'SibSp', 'Parch',\n              'Cabin', 'Initial', 'Ticket_Number', 'Family_Size'], axis = 1)\n\n# split data\ndf = df.astype(float)\ntrain = df[:891]\ntest = df[891:]\n\ntrain_X = StandardScaler().fit_transform(train.drop(columns = ['PassengerId', 'Survived']))\ntrain_y = train.iloc[:, 1]\ntest_X = StandardScaler().fit_transform(test.drop(columns = ['PassengerId', 'Survived']))","d5755fac":"class AdalineBGD(object):\n    def __init__(self, eta = 0.01, n_iter = 50, random_state = 1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n    \n    def fit(self, X, y):\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc = 0, scale = 0.01,\n                             size = 1 + X.shape[1])\n        self.cost_ = []\n        \n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            output = self.activation(net_input)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            cost = (errors**2).sum() \/ 2\n            self.cost_.append(cost)\n        return self\n    \n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n    \n    def activation(self, X):\n        return X\n    \n    def predict(self, X):\n        return np.where(self.activation(self.net_input(X)) >= 0, 1, 0)","10826766":"fig, ax = plt.subplots(1, 2, figsize = (10, 4))\nPCbgd1 = AdalineBGD(eta = 0.01, n_iter = 10)\nPCbgd1.fit(train_X, train_y)\nax[0].plot(range(1, len(PCbgd1.cost_) + 1),\n          np.log10(PCbgd1.cost_), marker = 'o')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('log(SSE)')\nax[0].set_title('Adaline - learning rate 0.01')\n\nPCbgd2 = AdalineBGD(eta = 0.0001, n_iter = 10)\nPCbgd2.fit(train_X, train_y)\nax[1].plot(range(1, len(PCbgd2.cost_) + 1),\n          np.log10(PCbgd2.cost_), marker = 'o')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('log(SSE)')\nax[1].set_title('Adaline - Learning rate 0.0001')\nplt.show()","85263559":"fig, ax = plt.subplots(figsize = (15, 6))\nPCbgd3 = AdalineBGD(eta = 0.0005, n_iter = 10)\nPCbgd3.fit(train_X, train_y)\nax.plot(range(1, len(PCbgd3.cost_) + 1),\n          np.log10(PCbgd3.cost_), marker = 'o')\nax.set_xlabel('Epochs')\nax.set_ylabel('log(SSE)')\nax.set_title('Adaline - Learning rate 0.0001')\nplt.show()","043296bf":"class AdalineSGD(object):\n    def __init__(self, eta = 0.1, n_iter = 10,\n                shuffle = True, random_state = None):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.w_initialized = False\n        self.shuffle = shuffle\n        self.random_state = random_state\n        \n    def fit(self, X, y):\n        self._initialize_weights(X.shape[1])\n        self.cost_ = []\n        \n        for i in range(self.n_iter):\n            if self.shuffle:\n                X, y = self._shuffle(X, y)\n            cost = []\n            for xi, target in zip(X, y):\n                cost.append(self._update_weight(xi, target))\n            avg_cost = sum(cost) \/ len(y)\n            self.cost_.append(avg_cost)\n        return self\n    \n    def partial_fit(self, X, y):\n        # train data not reset weights\n        if not self.w_initialized:\n            self._initialize_weight(X.shape[1])\n        if y.ravel().shape[0] > 1:\n            for xi, target in zip(X, y):\n                self._update_weight(xi, target)\n        else:\n            self._update_weight(X, y)\n        return self\n    \n    def _shuffle(self, X, y):\n        r = self.rgen.permutation(len(y))\n        return X[r], y[r]\n    \n    def _initialize_weights(self, m):\n        self.rgen = np.random.RandomState(self.random_state)\n        self.w_ = self.rgen.normal(loc = 0, scale = 0.01,\n                                  size = 1 + m)\n        self.w_initialized = True\n        \n    def _update_weight(self, xi, target):\n        output = self.activation(self.net_input(xi))\n        error = (target - output)\n        self.w_[1:] += self.eta * xi.dot(error)\n        self.w_[0] += self.eta * error\n        cost = (error**2) \/ 2\n        return cost\n    \n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n    \n    def activation(self, X):\n        return X\n    \n    def predict(self, X):\n        return np.where(self.activation(self.net_input(X)) >= 0, 1, 0)","50091e5a":"fig, ax = plt.subplots(1, 2, figsize = (10, 4))\n\nfor i, eta in enumerate([0.05, 0.0001]):\n    AD = AdalineSGD(eta = eta, n_iter = 10)\n    AD.fit(train_X, train_y)\n    ax[i].plot(range(1, len(AD.cost_) + 1),\n              np.log10(AD.cost_), marker = 'o')\n    ax[i].set_title('Adaline - Learing rate {}'.format(eta))\n    ax[i].set_xlabel('Epochs')\n    ax[i].set_ylabel('Log(SSE)')\n        \n","1570fbc3":"fig, ax = plt.subplots(figsize = (15, 6))\nADsgd = AdalineSGD(eta = 0.005, n_iter = 10)\nADsgd.fit(train_X, train_y)\nax.plot(range(1, len(PCbgd3.cost_) + 1),\n          np.log10(PCbgd3.cost_), marker = 'o')\nax.set_xlabel('Epochs')\nax.set_ylabel('log(SSE)')\nax.set_title('Adaline - Learning rate 0.005')\nplt.show()","888024b6":"PCbgd = AdalineBGD(eta = 0.0005, n_iter = 1)\nPCbgd.fit(train_X, train_y)\npredict = [PCbgd.predict(test_X)]\nADsgd = AdalineSGD(eta = 0.0001, n_iter = 1)\nADsgd.fit(train_X, train_y)\npredict.append(ADsgd.predict(test_X))\n\nfor i in range(len(predict)):\n    submission = pd.DataFrame(columns = ['PassengerId', 'Survived'])\n    submission['PassengerId'] = df['PassengerId'][891:].map(int)\n    submission['Survived'] = predict[i]\n    submission.to_csv('Perceptron_submission_{}.csv'.format(i + 1), header = True, index = False)","da518133":"<b>\n\nAdalineBGD : 0.65550\n  \nAdalineSGD : 0.69617\n    \nPerceptron : 0.75358\n\nAdjusted Perceptron : 0.75837\n\nEnsemble : 0.76794 \n    \n<\/b>\n\n\nWe can see that Adaline algorithms is more overfitting than other algorithms. So I have to adjust hyperparameter very lower. I think because of sensitiveness about overfitting, Adalines's accuracy is low.","6aee880c":"### Cost Gragh compared by Epochs","1461086b":"### Cost Gragh compared by Epochs","1c7e624e":"We can see that 0.01 learing rate pass global minimum. However 0.0001 learing rate can't reach global minimum point. So it needs to be adjusted.","5c441a6b":"Feature engineering\nWhen I joined titanic survived prediction competition, I wrote the notebook, [Titanic Survival Prediction(EDA, Ensemble)](https:\/\/www.kaggle.com\/choihanbin\/titanic-survival-prediction-eda-ensemble). So I will pick the features to use based on it.","068b1d00":"### load the dataset\n","a78dcddf":"Now we will make model to use stochastic gradient descent. The defference with batch gradient descent is to update weigths by each training samples, not All training set.\n\nSo it is faster in convergence than batch gradient descent. To take satisfied result, it is important to input shuffled training set.","8e290fb0":"Adaline algorithm is very similar with [perceptron algorithm](https:\/\/www.kaggle.com\/choihanbin\/titanic-survival-prediction-with-perceptron) before I created. To put it simply, It caculate the output value y-hat, and it is used to update weights.\n\nThe biggest difference between Perceptron and Adaline algorithm is Adaline algorithm use linear activation function instead of unit stair function. It takes advantage that it can be defferentiable. So we can find weight that to minimize cost function. \n\nI will compare Adaline algorithm with batch gradient descent and stochastic gradient descent.\n\nbatch gradient descent will be used to calculate weights to update based on <b>all train set's samples.<\/b>","e028129b":"# Titanic survival rate prediction by Adaptive linear neuron\n\n\nThis notebook goal:\n\n- Understanding Adaline(Adaptive linear neuron) algorithm and stochasitc gradient descent principle\n- Trying to predict titanic survived.\n- Checking accuracy of models created without being imported through the library.\n\n### You can learn about Adaptive linear neuron principle in [my blog](https:\/\/konghana01.tistory.com\/14). ","36ff439b":"## Best Adaline Model"}}