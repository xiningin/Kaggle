{"cell_type":{"ab88b968":"code","2fa945e0":"code","c290c02f":"code","51a53bc9":"code","4eb3e052":"code","e538be21":"code","04a4e9d5":"code","4f899b9f":"code","7cac7a75":"code","a5f7e146":"code","033a379a":"code","a2fa5b1f":"markdown","4ada1a26":"markdown","d8ef5f58":"markdown","4a1ff05f":"markdown","aa3ea589":"markdown","fa5e0d31":"markdown","8f38af02":"markdown"},"source":{"ab88b968":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2fa945e0":"df = pd.read_csv(\"..\/input\/email-spam-classification-dataset-csv\/emails.csv\")\ndf.head(20)","c290c02f":"df.isnull().sum()","51a53bc9":"df.describe()","4eb3e052":"df.corr()","e538be21":"X = df.iloc[:,1:3001]\nX","04a4e9d5":"Y = df.iloc[:,-1].values\nY","4f899b9f":"train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size = 0.25)","7cac7a75":"mnb = MultinomialNB(alpha=1.9)         # alpha by default is 1. alpha must always be > 0. \n# alpha is the '1' in the formula for Laplace Smoothing (P(words))\nmnb.fit(train_x,train_y)\ny_pred1 = mnb.predict(test_x)\nprint(\"Accuracy Score for Naive Bayes : \", accuracy_score(y_pred1,test_y))","a5f7e146":"svc = SVC(C=1.0,kernel='rbf',gamma='auto')         \n# C here is the regularization parameter. Here, L2 penalty is used(default). It is the inverse of the strength of regularization.\n# As C increases, model overfits.\n# Kernel here is the radial basis function kernel.\n# gamma (only used for rbf kernel) : As gamma increases, model overfits.\nsvc.fit(train_x,train_y)\ny_pred2 = svc.predict(test_x)\nprint(\"Accuracy Score for SVC : \", accuracy_score(y_pred2,test_y))","033a379a":"rfc = RandomForestClassifier(n_estimators=100,criterion='gini')\n# n_estimators = No. of trees in the forest\n# criterion = basis of making the decision tree split, either on gini impurity('gini'), or on infromation gain('entropy')\nrfc.fit(train_x,train_y)\ny_pred3 = rfc.predict(test_x)\nprint(\"Accuracy Score of Random Forest Classifier : \", accuracy_score(y_pred3,test_y))","a2fa5b1f":"# Creating the NB Model\n\nIn this project we are clasifying mails typed in by the user as either 'Spam' or 'Not Spam'. Our original dataset was a folder of 5172 text files containing the emails.\n\nNow let us understand why we have separated the words from the mails. This is because, this is a text-classification problem. When a spam classifier looks at a mail, it searches for potential words that it has seen in the previous spam emails. If it finds a majority of those words, then it labels it as 'Spam'. **Why did I say majority ? -->**\n\n*CASE 1* : suppose let's take a word 'Greetings'. Say, it is present in both 'Spam' and 'Not Spam' mails.\n\n*CASE 2* : Let's consider a word 'lottery'.Say, it is present in only 'Spam' mails.\n\n*CASE 3* : Let's consider a word 'cheap'. Say, it is present only in spam.\n\nIf now we get a test email, and it contains all the three words metioned above, there's high probability that it is a 'Spam' mail.\n\nThe most effective algorithm for text-classification problems is the Naive Bayes algorithm, that works on the classic Bayes' theorem. This theorem works on every individual word in the test data to make predictions(the conditional probability with higher probability is the predicted result). \n\n________________________________________________________________________________________________________________________\n\nSay, our test email(S)is,*\"You have won a lottery\"*\n\n**HOW NAIVE BAYES WORKS ON THIS DATA -->**\n\nP(S) = P('You') * P('have') * P('won') * P('a') * P('lottery') ____ 1\n\nTherefore, P(S|Spam) = P('You'|Spam) * P('have'|Spam) * P('won'|Spam) * P('a'|Spam) * P('lottery'|Spam) ____ 2 \n\nSame calculation for P(S|Not_Spam) ____ 3\n\nIf 2 > 3, then 'Spam' Else, 'Not_Spam'.\n\n**WHAT IF THE PROBABILITY IS ZERO ?** Here comes the concept of Laplace Smoothing, where P(words) = (word_count + 1)\/(total_no_of_words + no_of_unique_words)\n\n________________________________________________________________________________________________________________________\n\nHere, we'll work on the existing Multinomial Naive Bayes classifier (under scikit-learn). To further understand how well Naive Bayes works for text-classification, we'll use another standard classifier, SVC, to see how the two models perform.\n\n**WILL ENSEMBLE MODELS WORKS BETTER ?** Let us see. We will use Random Forests to compare.","4ada1a26":"# Support Vector Machines\n\nSupport Vector Machine is the most sought after algorithm for classic classification problems. SVMs work on the algorithm of Maximal Margin, i.e, to find the maximum margin or threshold between the support vectors of the two classes (in binary classification). The most effective Support vector machines are the soft maximal margin classifier, that allows one misclassification, i.e, the model starts with low bias(slightly poor performance) to ensure low variance later.\n\n________________________________________________________________________________________________________________________\n\nLet us see the model performance.","d8ef5f58":"# Naive Bayes","4a1ff05f":"## Ending notes:\n\nThis was a purely comparative study to check the workability of the dataset that I created, and to check how conventional models perform on my dataset. In my next kernel, I will show the code behind the extraction of this dataset from the raw text files.","aa3ea589":"As expected, Random Forest Classifier performs the best among the three. Decision tree classifiers are excellent classifiers. Random forest is a popular ensemble model that uses a forest of decision trees. So, obviously, combibining the accuracy of 100 trees (as n_estimators=100 here), will create a powerful model.","fa5e0d31":"# Random Forests (Bagging)\n\nEnsemble methods turn any feeble model into a highly powerful one. Let us see if ensemble model can perform better than Naive Bayes","8f38af02":"As expected, SVM's performance is slightly poorer than Multinomia Naive Bayes"}}