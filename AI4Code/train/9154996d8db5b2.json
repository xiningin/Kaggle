{"cell_type":{"90b4b8d3":"code","c21266e5":"code","91b0d9f4":"code","d3e133ce":"code","976eeca4":"code","429a09cf":"code","dc90c172":"code","91802573":"code","4dfc823b":"code","3c390984":"code","30ed107c":"code","700b473e":"code","724569e6":"markdown","4d257ef6":"markdown","a420171d":"markdown","3e181035":"markdown","9d960001":"markdown","9b7ac4bf":"markdown","f0efb06c":"markdown","cb2be09b":"markdown"},"source":{"90b4b8d3":"import os\nimport pandas as pd\nfrom tqdm import tqdm, trange\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertPreTrainedModel, BertModel\n\nfrom transformers import AutoConfig, AutoTokenizer","c21266e5":"MODEL_OUT_DIR = '\/kaggle\/working\/models\/my_model'\nTRAIN_FILE_PATH = '..\/input\/data\/train.tsv'\nVALID_FILE_PATH = '..\/input\/data\/dev.tsv'\n## Model Configurations\nMAX_LEN_TRAIN = 30\nMAX_LEN_VALID = 30\nBATCH_SIZE = 32\nLR = 2e-5\nNUM_EPOCHS = 2\nNUM_THREADS = 1  ## Number of threads for collecting dataset\nMODEL_NAME = 'bert-base-uncased'\n\n\nif not os.path.isdir(MODEL_OUT_DIR):\n    os.makedirs(MODEL_OUT_DIR)\n","91b0d9f4":"class SSTDataset(Dataset):\n\n    def __init__(self, filename, maxlen, tokenizer): \n        #Store the contents of the file in a pandas dataframe\n        self.df = pd.read_csv(filename, delimiter = '\\t')\n        #Initialize the tokenizer for the desired transformer model\n        self.tokenizer = tokenizer\n        #Maximum length of the tokens list to keep all the sequences of fixed size\n        self.maxlen = maxlen\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):    \n        #Select the sentence and label at the specified index in the data frame\n        sentence = self.df.loc[index, 'sentence']\n        label = self.df.loc[index, 'label']\n        #Preprocess the text to be suitable for the transformer\n        tokens = self.tokenizer.tokenize(sentence) \n        tokens = ['[CLS]'] + tokens + ['[SEP]'] \n        if len(tokens) < self.maxlen:\n            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] \n        else:\n            tokens = tokens[:self.maxlen-1] + ['[SEP]'] \n        #Obtain the indices of the tokens in the BERT Vocabulary\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens) \n        input_ids = torch.tensor(input_ids) \n        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n        attention_mask = (input_ids != 0).long()\n        return input_ids, attention_mask, label","d3e133ce":"class BertForSentimentClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        #The classification layer that takes the [CLS] representation and outputs the logit\n        self.cls_layer = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        #Feed the input to Bert model to obtain contextualized representations\n        reps, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        #Obtain the representations of [CLS] heads\n        cls_reps = reps[:, 0]\n        # cls_reps = self.dropout(cls_reps)\n        logits = self.cls_layer(cls_reps)\n        return logits\n","976eeca4":"def train(model, criterion, optimizer, train_loader, val_loader, epochs):\n    best_acc = 0\n    for epoch in trange(epochs, desc=\"Epoch\"):\n        model.train()\n        for i, (input_ids, attention_mask, labels) in enumerate(tqdm(iterable=train_loader, desc=\"Training\")):\n            optimizer.zero_grad()  \n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n            loss = criterion(input=logits.squeeze(-1), target=labels.float())\n            loss.backward()\n            optimizer.step()\n        val_acc, val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(epoch, val_acc, val_loss))\n        if val_acc > best_acc:\n            print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_acc, val_acc))\n            best_acc = val_acc\n            model.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n            config.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n            tokenizer.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n","429a09cf":"def get_accuracy_from_logits(logits, labels):\n    #Get a tensor of shape [B, 1, 1] with probabilities that the sentiment is positive\n    probs = torch.sigmoid(logits.unsqueeze(-1))\n    #Convert probabilities to predictions, 1 being positive and 0 being negative\n    soft_probs = (probs > 0.5).long()\n    #Check which predictions are the same as the ground truth and calculate the accuracy\n    acc = (soft_probs.squeeze() == labels).float().mean()\n    return acc\n\ndef evaluate(model, criterion, dataloader, device):\n    model.eval()\n    mean_acc, mean_loss, count = 0, 0, 0\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            logits = model(input_ids, attention_mask)\n            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n            mean_acc += get_accuracy_from_logits(logits, labels)\n            count += 1\n    return mean_acc \/ count, mean_loss \/ count","dc90c172":"## Configuration loaded from AutoConfig \nconfig = AutoConfig.from_pretrained(MODEL_NAME)\n## Tokenizer loaded from AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n## Creating the model from the desired transformer model\nmodel = BertForSentimentClassification.from_pretrained(MODEL_NAME, config=config)\n## GPU or CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n## Putting model to device\nmodel = model.to(device)\n## Takes as the input the logits of the positive class and computes the binary cross-entropy \ncriterion = nn.BCEWithLogitsLoss()\n## Optimizer\noptimizer = optim.Adam(params=model.parameters(), lr=LR)\n","91802573":"## Training Dataset\ntrain_set = SSTDataset(filename=TRAIN_FILE_PATH, maxlen=MAX_LEN_TRAIN, tokenizer=tokenizer)\nvalid_set = SSTDataset(filename=VALID_FILE_PATH, maxlen=MAX_LEN_VALID, tokenizer=tokenizer)","4dfc823b":"## Data Loaders\ntrain_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\nvalid_loader = DataLoader(dataset=valid_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)","3c390984":"train(model=model, \n      criterion=criterion,\n      optimizer=optimizer, \n      train_loader=train_loader,\n      val_loader=valid_loader,\n      epochs = NUM_EPOCHS\n      )","30ed107c":"def classify_sentiment(sentence):\n    with torch.no_grad():\n        tokens = tokenizer.tokenize(sentence)\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        input_ids = torch.tensor(input_ids).to(device)\n        input_ids = input_ids.unsqueeze(0)\n        attention_mask = (input_ids != 0).long()\n        attention_mask = attention_mask.to(device)\n        logit = model(input_ids=input_ids, attention_mask=attention_mask)\n        prob = torch.sigmoid(logit.unsqueeze(-1))\n        prob = prob.item()\n        soft_prob = prob > 0.5\n        if soft_prob == 1:\n            print('Positive with probability {}%.'.format(int(prob*100)))\n        else:\n            print('Negative with probability {}%.'.format(int(100-prob*100)))","700b473e":"sentence = 'it is organised '\nclassify_sentiment(sentence)","724569e6":"\n## Evaluate Function","4d257ef6":"## Building Model","a420171d":"## Creating Dataset","3e181035":"Here we are using AutoConfig and AutoTokenizer <> Give the name of the model we want to use and the config and tokenizer for the model will be loaded accordingly. [Check this out](https:\/\/huggingface.co\/transformers\/model_doc\/auto.html)","9d960001":"## Train Function","9b7ac4bf":"## Create Analysis","f0efb06c":"## Imports and Libraries","cb2be09b":"## Configurations"}}