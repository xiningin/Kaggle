{"cell_type":{"16b3cbbe":"code","9cbe8afe":"code","bbd5bd80":"code","081391bf":"code","b7c9793c":"code","7f45633a":"code","8d2690f9":"code","b0e8ee9a":"code","8e11844c":"code","810e55e8":"code","94a6cd63":"code","43583494":"code","ac92aa38":"code","0a64f0b0":"code","1e38e0fc":"code","f09558b4":"code","b9205e1b":"code","8c37a88b":"code","7adf89e7":"code","de05e30b":"code","ef4e2e89":"markdown","98dfc0a8":"markdown","8f7ac39a":"markdown"},"source":{"16b3cbbe":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.applications import VGG16,ResNet50\nimport tensorflow as tf","9cbe8afe":"train_dir = '..\/input\/facemask-split\/dataset_rgb_split\/dataset_rgb_split\/train'\ntest_dir = '..\/input\/facemask-split\/dataset_rgb_split\/dataset_rgb_split\/test'","bbd5bd80":"def load_dataset(path):\n    X = []\n    y = []\n    labels = os.listdir(path)\n    for label in labels:\n        files = os.listdir(os.path.join(path, label))\n        for file in files:\n            img_path = os.path.join(path, label, file)\n            img = cv2.imread(img_path,1)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img_resize = cv2.resize(img, (50,50))\n            X.append(img_resize)\n            y.append(label)\n    return X,y","081391bf":"X_test = []\ny_test = []\n\nX_test,y_test_before_transformed = load_dataset(test_dir)","b7c9793c":"X_test = np.array(X_test).reshape(-1,50,50,3)\ny_test_before_transformed = np.array(y_test_before_transformed).reshape(-1,1)","7f45633a":"X_test = X_test.astype('float32') \/ 255.0","8d2690f9":"lb = LabelEncoder()\ny_test_transformed = lb.fit_transform(y_test_before_transformed)\ny_test = tf.keras.utils.to_categorical(y_test_transformed)","b0e8ee9a":"batch_training = 2048\nbatch_val = 4096","8e11844c":"train_datagen = ImageDataGenerator(horizontal_flip=True,\n                                   rescale=1.\/255,\n                                   rotation_range=20,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=2,\n                                   zoom_range=0.2,\n                                   validation_split=0.2,\n                                   dtype=np.float32)","810e55e8":"train_generator = train_datagen.flow_from_directory(\n    train_dir,  # This is the source directory for training images\n    target_size=(50, 50),  # All images will be resized to this size\n    batch_size=batch_training,\n    color_mode='rgb',\n    subset='training',\n    class_mode='categorical')\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,  # This is the source directory for training images\n    target_size=(50, 50),  # All images will be resized to this size\n    batch_size=batch_val,\n    color_mode='rgb',\n    subset='validation',\n    class_mode='categorical')","94a6cd63":"callbacks = [\n    EarlyStopping(patience=10, verbose=1,restore_best_weights=True),\n    ReduceLROnPlateau(factor=0.9, patience=5, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('faceMaskModel.h5', verbose=1, save_best_only=True)#, save_weights_only=True\n]\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3,3), activation='relu', input_shape=X_test.shape[1:],padding='same'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Conv2D(64, (3,3), activation='relu',padding='same'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Conv2D(64, (5,5), activation='relu',padding='same'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Conv2D(128, (5,5), activation='relu',padding='same'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Conv2D(128, (7,7), activation='relu',padding='same'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.2))# prevent overfitting\nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.2))# prevent overfitting\nmodel.add(Dense(3, activation='softmax'))\n\n# Trainable params should always more than training dataset\nprint(model.summary()) \n\n# Compile the model\nmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])","43583494":"history = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples\/\/batch_training,  # total images = batch * steps\n    epochs=100,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples\/\/batch_val,\n    verbose=1,\n    callbacks=callbacks)","ac92aa38":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\n\nplt.plot(epochs, accuracy, \"b\", label=\"trainning accuracy\")\nplt.plot(epochs, val_accuracy, \"r\", label=\"validation accuracy\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, \"b\", label=\"trainning loss\")\nplt.plot(epochs, val_loss, \"r\", label=\"validation loss\")\nplt.legend()\nplt.show()\n\n(loss,acc) = model.evaluate(X_test, y_test, verbose=0)\nprint('Model Evaluation')\nprint('Loss: %.3f' % loss)\nprint('Accuracy: %.3f' % acc)\nprint()","0a64f0b0":"X_test.shape[1:]","1e38e0fc":"callbacks_vgg = [\n    EarlyStopping(patience=10, verbose=1,restore_best_weights=True),\n    ReduceLROnPlateau(factor=0.9, patience=10, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('faceMaskModel-vgg.h5', verbose=1, save_best_only=True)#, save_weights_only=True\n]\n\nmodel_vgg = Sequential()\nmodel_vgg.add(VGG16(include_top=False,input_shape=X_test.shape[1:],pooling='max',weights='imagenet'))#\nmodel_vgg.add(Dense(1000, activation='relu'))\nmodel_vgg.add(Dropout(0.2))# prevent overfitting\nmodel_vgg.add(Dense(1000, activation='relu'))\nmodel_vgg.add(Dropout(0.2))# prevent overfitting\nmodel_vgg.add(Dense(3, activation='softmax'))\n# model_vgg.layers[0].trainable=True\n\n# Trainable params should always more than training dataset\nprint(model_vgg.summary()) \n\n# Compile the model\nmodel_vgg.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n\n#if color channel is 3, then it will take very long to train\n#so we only used grayscale, thus need self build ","f09558b4":"history_vgg = model_vgg.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples\/\/batch_training,  # total images = batch * steps\n    epochs=100,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples\/\/batch_val,\n    verbose=1,\n    callbacks=callbacks_vgg)","b9205e1b":"accuracy = history_vgg.history['accuracy']\nval_accuracy = history_vgg.history['val_accuracy']\nloss = history_vgg.history['loss']\nval_loss = history_vgg.history['val_loss']\nepochs = range(len(accuracy))\n\nplt.plot(epochs, accuracy, \"b\", label=\"training accuracy\")\nplt.plot(epochs, val_accuracy, \"r\", label=\"validation accuracy\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, \"b\", label=\"training loss\")\nplt.plot(epochs, val_loss, \"r\", label=\"validation loss\")\nplt.legend()\nplt.show()\n\n(loss,acc) = model_vgg.evaluate(X_test, y_test, verbose=0)\nprint('Model Evaluation')\nprint('Loss: %.3f' % loss)\nprint('Accuracy: %.3f' % acc)\nprint()","8c37a88b":"callbacks_resnet = [\n    EarlyStopping(patience=10, verbose=1,restore_best_weights=True),\n    ReduceLROnPlateau(factor=0.9, patience=10, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('faceModel-ResNet.h5', verbose=1, save_best_only=True)#, save_weights_only=True\n]\n\nmodel_resnet = Sequential()\nmodel_resnet.add(ResNet50(include_top=False,pooling='max',input_shape=X_test.shape[1:],weights='imagenet'))#weights=None, \nmodel_resnet.add(Dense(1000, activation='relu'))\nmodel_resnet.add(Dropout(0.2))# prevent overfitting\nmodel_resnet.add(Dense(1000, activation='relu'))\nmodel_resnet.add(Dropout(0.2))# prevent overfitting\nmodel_resnet.add(Dense(3, activation='softmax'))\n# model_resnet.layers[0].trainable=True\n\n# Trainable params should always more than training dataset\nprint(model_resnet.summary()) \n\n# Compile the model\nmodel_resnet.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])","7adf89e7":"history_resnet = model_resnet.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples\/\/batch_training,  # total images = batch * steps\n    epochs=100,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples\/\/batch_val,\n    verbose=1,\n    callbacks=callbacks_resnet)","de05e30b":"accuracy = history_resnet.history['accuracy']\nval_accuracy = history_resnet.history['val_accuracy']\nloss = history_resnet.history['loss']\nval_loss = history_resnet.history['val_loss']\nepochs = range(len(accuracy))\n\nplt.plot(epochs, accuracy, \"b\", label=\"training accuracy\")\nplt.plot(epochs, val_accuracy, \"r\", label=\"validation accuracy\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, \"b\", label=\"training loss\")\nplt.plot(epochs, val_loss, \"r\", label=\"validation loss\")\nplt.legend()\nplt.show()\n\n(loss,acc) = model_resnet.evaluate(X_test, y_test, verbose=0)\nprint('Model Evaluation')\nprint('Loss: %.3f' % loss)\nprint('Accuracy: %.3f' % acc)\nprint()","ef4e2e89":"<a href=\".\/faceMaskModel-vgg.h5\"> Download File <\/a>","98dfc0a8":"# VGG (batch=4096)","8f7ac39a":"# ResNet50 (batch_training=2048, batch_val=4096)"}}