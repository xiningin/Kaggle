{"cell_type":{"aac10019":"code","9302714a":"code","efdc2841":"code","553027fe":"code","f00fce11":"code","2f1ea22f":"code","f6752083":"code","79324266":"code","8fb14e8a":"code","6c922654":"code","ba3498c9":"code","671e80bf":"code","8230ac0e":"code","dc1194f7":"code","c620e136":"code","35efba50":"code","6b7c3646":"code","8d97343f":"code","50d72e32":"code","95c96edc":"code","a0c83ccd":"code","a15f3973":"code","6214de80":"code","a9dc4588":"code","e8b15a58":"code","50df0c28":"code","640eed0e":"code","e591a3a3":"code","7aa8de8d":"code","7e9e54f6":"code","f2d45d82":"code","ad674692":"code","6557b0d3":"code","29009035":"code","32ba2e01":"code","4104159d":"code","2c9dc75c":"code","8bc27345":"code","1aa0c948":"code","06b9ceab":"code","efece2ec":"code","9715cf48":"code","54d013a1":"code","31339f83":"code","74625e85":"code","658ff52d":"code","0509d2ea":"code","ba72731e":"code","8c257b11":"code","727d3b5b":"markdown","0b3a5e0e":"markdown","909161e6":"markdown","642d39b1":"markdown"},"source":{"aac10019":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9302714a":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom time import time \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom math import sqrt\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nimport torch.optim as optim\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import explained_variance_score\nwarnings.filterwarnings('ignore')","efdc2841":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')","553027fe":"train.head(3)","f00fce11":"test.head(3)","2f1ea22f":"print('training set details:')\nprint('-' * 21)\nprint('shape:')\nprint()\nprint(train.shape)\nprint('*' * 50)\nprint('null columns:')\nprint()\nprint(train.isnull().sum())\nprint('*' * 50)\nprint('total nulls:')\nprint()\nprint(sum(train.isnull().sum()))\nprint('*' * 50)\nprint('info:')\nprint()\nprint(train.info())","f6752083":"print('testing set details:')\nprint('-' * 20)\nprint('shape:')\nprint()\nprint(test.shape)\nprint('*' * 50)\nprint('null columns:')\nprint()\nprint(test.isnull().sum())\nprint('*' * 50)\nprint('total nulls:')\nprint()\nprint(sum(test.isnull().sum()))\nprint('*' * 50)\nprint('info:')\nprint()\nprint(test.info())","79324266":"train.describe()","8fb14e8a":"train['loss'].describe().to_frame()","6c922654":"test.describe()","ba3498c9":"plt.subplots(figsize = (12,8))\nsns.countplot(train['loss'])","671e80bf":"from sklearn.feature_selection import VarianceThreshold as vt\nfeatures = train.iloc[:,1:101]\ntarget = train['loss']\nv = vt(threshold = 0.8)\nv = v.fit(features, target)\n\ncols = v.get_support(indices = True)\ncols.shape","8230ac0e":"x_train = train.iloc[:, 1:101]\nx_test = test.drop('id', axis = 1)","dc1194f7":"mm = MinMaxScaler().fit(x_train)\nx_train_mm = mm.transform(x_train)\nx_test_mm = mm.transform(x_test)","c620e136":"xtrain_data = pd.DataFrame(x_train_mm)\nxtest_data = pd.DataFrame(x_test_mm)\nxtrain_data['loss'] = train['loss']","35efba50":"print('xtrain_data shape:')\nprint()\nprint(xtrain_data.shape)\nprint('*' * 50)\nprint('xtest_data shape:')\nprint()\nprint(xtest_data.shape)","6b7c3646":"X_train1 = x_train.copy()\nX_test1 = x_test.copy()\nnum_cols = x_train.columns\nfor i in num_cols:\n    scale = StandardScaler().fit(X_train1[[i]])\n    X_train1[i] = scale.transform(X_train1[[i]])\n    X_test1[i] = scale.transform(X_test1[[i]])\n\nX_train1[\"loss\"] = train[\"loss\"]","8d97343f":"\nif \"Set\" not in X_train1.columns:\n    X_train1[\"Set\"] = np.random.choice([\"train\", \"valid\"], p =[.8, .2], size=(X_train1.shape[0],))\n\ntrain_indices = X_train1[X_train1.Set==\"train\"].index\nvalid_indices = X_train1[X_train1.Set==\"valid\"].index","50d72e32":"unused_feature = ['Set']\ntarget = 'loss'\nfeature = [ col for col in X_train1.columns if col not in unused_feature+[target]] ","95c96edc":"#train_data\nX_train = X_train1[feature].values[train_indices]\ny_train = X_train1[target].values[train_indices].reshape(-1, 1)\n#valid data \nX_valid = X_train1[feature].values[valid_indices]\ny_valid = X_train1[target].values[valid_indices].reshape(-1, 1)","a0c83ccd":"!pip install torchvision\nimport torch","a15f3973":"!pip install pytorch-tabnet","6214de80":"seed = 35","a9dc4588":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.metrics import Metric\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport seaborn as sns\nsns.set()","e8b15a58":"tabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 5,\n    gamma = 1.3,\n    lambda_sparse = 1e-3,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(\n        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = seed,\n    verbose = 10\n)","50df0c28":"# gpu ON.  10 minuts\n\nunsupervised_model = TabNetPretrainer(**tabnet_params)","640eed0e":"unsupervised_model.fit(\n    X_train=X_train,\n    eval_set=[X_valid],\n    pretraining_ratio=0.8,\n)","e591a3a3":"from pytorch_tabnet.tab_model import TabNetRegressor\nmax_epochs = 200\nBs = 8192\nclf = TabNetRegressor(  verbose = 10 ,\n                       optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":10, \n                                         \"gamma\":0.9},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='sparsemax'\n                     )","7aa8de8d":"clf.fit(\n    X_train=X_train, y_train=y_train,\n    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n    eval_name=['train', 'valid'],\n    eval_metric=['rmsle','rmse'],\n    max_epochs=max_epochs,\n    patience=15,\n    batch_size=Bs, virtual_batch_size=128,\n    num_workers=0,\n    drop_last=False,\n    from_unsupervised=unsupervised_model\n)","7e9e54f6":"print(f\"BEST VALID SCORE  : {clf.best_cost}\")","f2d45d82":"test_pred= X_test1.to_numpy()\npredictions =  clf.predict(test_pred)","ad674692":"feat_importances = clf.feature_importances_\nindices = np.argsort(feat_importances)","6557b0d3":"sample = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\npredictions = pd.DataFrame(predictions) # from array to dataframe\nsample['loss'] = predictions\nsample.to_csv('p_tabnet_with_pretrain_submission.csv',index=False)\nsample","29009035":"!pip install pycaret==2.3.1\n\n# pycaret\u306e\u30a4\u30f3\u30dd\u30fc\u30c8 \u3002gpu off. kiiroi warning derukedo kinisinai.\n#from pycaret.classification import *\nfrom pycaret.regression import *","32ba2e01":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')","4104159d":"# \u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3092\u5b9f\u65bd henna data derukedo ok.\n\nsetup(train, target='loss', session_id=1) ","2c9dc75c":"# \u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3092\u5b9f\u65bd\n\nsetup(train, target='loss', session_id=1) \n\n# \u30e2\u30c7\u30eb\u9593\u306e\u6bd4\u8f03\u3092\u5b9f\u65bd\n\nbest_model = compare_models()  # \u5168\u3066\u306e\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u3057\u3001\u8a55\u4fa1\u3059\u308b  120 minutes over.\u3081\u3061\u3083\u304f\u3061\u3083\u6642\u9593\u304b\u304b\u308b\u3002","8bc27345":"lgbm = create_model('lightgbm')\ntuned_lgbm = tune_model(lgbm)","1aa0c948":"predictions = predict_model(tuned_lgbm, data=test)\n\npredictions.head(5)","06b9ceab":"sample = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n#predictions = pd.DataFrame(predictions) # from array to dataframe\nsample['loss'] = predictions['Label']\nsample.to_csv('pycaret_tuned_lgbm_submission.csv',index=False)\nsample","efece2ec":"catbst = create_model('catboost')\ntuned_catbst = tune_model(catbst)","9715cf48":"predictions = predict_model(tuned_catbst, data=test)\n\npredictions.head(5)","54d013a1":"sample = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n#predictions = pd.DataFrame(predictions) # from array to dataframe\nsample['loss'] = predictions['Label']\nsample.to_csv('pycaret_tuned_catboost_submission.csv',index=False)\nsample","31339f83":"mlp = create_model('mlp')\ntuned_mlp = tune_model(mlp)","74625e85":"predictions = predict_model(tuned_mlp, data=test)\n\npredictions.head(5)","658ff52d":"sample = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n#predictions = pd.DataFrame(predictions) # from array to dataframe\nsample['loss'] = predictions['Label']\nsample.to_csv('pycaret_tuned_mlp_submission.csv',index=False)\nsample","0509d2ea":"# train a voting regressor on all models in library\n#blender = blend_models()\n# train a voting regressoron specific models\n#lr = create_model('lr')  #Linear Regression\n#ridge = create_model('ridge')\n#en = create_model('en')   #Elastic Net\n#br = create_model('br')   #Bayesian Ridge\n#dt = create_model('dt')\n#gbr = create_model('gbr')    #Gradient Boosting Regressor\n#mlp = create_model('mlp')\n#xgboost = create_model('xgboost')\n\n#rf = create_model('rf')\n#adaboost = create_model('ada')\nblender_specific = blend_models(estimator_list = [tuned_lgbm,tuned_catbst])","ba72731e":"predictions = predict_model(blender_specific, data=test)\n\npredictions.head(5)","8c257b11":"sample = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n#predictions = pd.DataFrame(predictions) # from array to dataframe\nsample['loss'] = predictions['Label']\nsample.to_csv('pycaret_blend_model_submission.csv',index=False)\nsample","727d3b5b":"tabnet --- train","0b3a5e0e":"TabNet params   ---   pretrain.","909161e6":"pycaret\u3000\u3067\u89e3\u6790\u3002  lgbm, catboost, mlp,  blend_model.","642d39b1":"mattaku fit sinai."}}