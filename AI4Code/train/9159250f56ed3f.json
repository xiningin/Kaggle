{"cell_type":{"0921f1a0":"code","15e33ffa":"code","9733c425":"code","c00ffb7a":"code","1e79dd15":"code","a97c21eb":"code","037aaa88":"code","3b0b74d0":"code","39b5c55b":"code","46a1da20":"code","af68b1b3":"code","f9da7579":"code","2042feff":"code","840bb59f":"code","87490c96":"code","1fe5e17b":"code","737a8631":"code","125eb7c7":"code","03c34412":"code","e911b359":"code","fc8e7e9c":"code","71ec13ed":"code","6a2eed15":"code","c378db67":"code","fa932eab":"code","cef3929a":"code","e7c1d73e":"markdown","e1e12e80":"markdown","073a1948":"markdown","ecdf4707":"markdown","199d21a4":"markdown","8be54075":"markdown","7b377039":"markdown","663a052d":"markdown","ac336395":"markdown","520b5207":"markdown","a86bf886":"markdown","d92a038e":"markdown","3358d89a":"markdown","7d7774ed":"markdown","27076b66":"markdown","ae427bc4":"markdown","95aef134":"markdown","650e5a4f":"markdown","55360471":"markdown","a1df5b4e":"markdown"},"source":{"0921f1a0":"import pandas as pd\npd.set_option('display.max_columns', 500)\nfrom hyperopt import fmin, tpe, hp, STATUS_OK\nfrom functools import partial\nimport numpy as np\nnp.set_printoptions(suppress=True)\nimport cudf\nfrom datetime import datetime, date\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ncalendar = pd.read_csv(\"..\/input\/predict-future-sales-supplementary\/calendar.csv\")\nitem_cat = pd.read_csv(\"..\/input\/predict-future-sales-supplementary\/item_category.csv\")\ncurrency = pd.read_csv(\"..\/input\/predict-future-sales-supplementary\/usd-rub.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nshops = pd.read_csv(\"..\/input\/predict-future-sales-supplementary\/shops-translated.csv\")\nsub = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\ntrain","15e33ffa":"train = train.drop_duplicates()\ntrain = train[(train.item_cnt_day > 0) & (train.item_price > 0)]","9733c425":"print(shops[shops.shop_id.isin([0, 57])]['Name'])\nprint(shops[shops.shop_id.isin([1, 58])]['Name'])\nprint(shops[shops.shop_id.isin([40, 39])]['Name'])","c00ffb7a":"train.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\ntrain.loc[train.shop_id == 40, 'shop_id'] = 39\ntest.loc[test.shop_id == 40, 'shop_id'] = 39","1e79dd15":"feature_generation = dict(\n    use_2014 = False,\n    ignore_price_outlier = True,\n    ignore_item_cnt_outlier = True,\n    last_n_month_sales = False,\n    clip_price = False,\n    clip_item_cnt_day = False,\n    clip_item_cnt_month = True,\n    month_quarter_season_sin_cos = True,\n    add_mul_decompose = True,\n    item_cat = True,\n    shop_cat = True,\n    freq_encoding = False,\n    is_month_holiday = True,\n    most_popular_item = True,\n    last_price_number = True,\n    item_price_stats = True,\n)\n\ncat_features = [\"shop_id\", \"item_id\"]\nencode_cols = []","a97c21eb":"if feature_generation.get(\"use_2014\"):\n    train = train[train.date_block_num >=12]","037aaa88":"if feature_generation[\"ignore_price_outlier\"]:\n    min_price, max_price = train[\"item_price\"].quantile([0.1, 0.9])\n    train = train[train.item_price <= max_price]\n    print(\"Ignore item price outlier\")\n    print(f\"Min price: {min_price}, Max price: {max_price}\")\n    \nelif feature_generation[\"clip_price\"]:\n    min_price, max_price = train[\"item_price\"].quantile([0.1, 0.9])\n    train[\"item_price\"] = train[\"item_price\"].clip(upper=max_price)\n    print(\"Clip price\")\n    print(f\"Min price: {min_price}, Max price: {max_price}\")\n\nif feature_generation[\"clip_item_cnt_day\"]:\n    max_cnt = train[\"item_cnt_day\"].quantile(0.9)\n    train[\"item_cnt_day\"] = train[\"item_cnt_day\"].clip(lower = 0, upper=max_cnt)\n    print(\"Clip item_cnt_day\")\n    print(f\"Max item cnt day: {max_cnt}\")\n    \nelif feature_generation[\"ignore_item_cnt_outlier\"]:\n    max_cnt = train[\"item_cnt_day\"].quantile(0.9)\n    train = train[train.item_cnt_day <= max_cnt]\n    print(\"Ignore item_cnt outlier\")\n    print(f\"Max item cnt day: {max_cnt}\")","3b0b74d0":"train = pd.merge(train, test, on=[\"shop_id\", \"item_id\"], how=\"inner\")\ntrain = train.drop(\"ID\", axis=1)\ntrain = cudf.from_pandas(train)\ntrain = train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg({\"item_price\": \"mean\", \"item_cnt_day\": \"sum\", \"date\": \"min\"}).reset_index()\ntrain = train.to_pandas()\ntrain.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)\ntrain","39b5c55b":"if feature_generation.get(\"clip_item_cnt_month\"):\n    train[\"item_cnt_month\"] = train[\"item_cnt_month\"].clip(0, 20)\ntrain","46a1da20":"if feature_generation.get(\"is_month_holiday\"):\n    calendar = pd.read_csv(\"..\/input\/predict-future-sales-supplementary\/calendar.csv\")\n    calendar[\"date\"] = pd.to_datetime(calendar.date, format = \"%Y-%m-%d\")\n    calendar['month'] = pd.DatetimeIndex(calendar['date']).month\n    calendar['date_block_num'] = calendar['date'].dt.to_period('M').sort_values().factorize()[0]\n    train = train.merge(calendar.groupby(\"date_block_num\")[\"holiday\"].sum().reset_index(), on=\"date_block_num\")\n    test[\"date_block_num\"] = 34\n    test = test.merge(calendar.groupby(\"date_block_num\")[\"holiday\"].sum().reset_index(), on=\"date_block_num\")\n    test = test.drop(\"date_block_num\", axis=1)\ntrain","af68b1b3":"shop_item_price = cudf.from_pandas(train).groupby([\"shop_id\", \"item_id\"]).agg({\"item_price\": \"mean\"}).reset_index()\nshop_item_price = shop_item_price.to_pandas()\nitem_price = cudf.from_pandas(train).groupby([\"item_id\"]).agg({\"item_price\": \"mean\"}).reset_index()\nitem_price = item_price.to_pandas()\ntr = pd.merge(test, shop_item_price, how=\"left\", on=[\"shop_id\", \"item_id\"])\ntr = pd.merge(tr, item_price, how=\"left\", on=\"item_id\")\ntr.fillna(0, inplace=True)\ntr[\"item_price_mean\"] = tr.item_price_x + tr.item_price_y\ntr.loc[tr.item_price_mean == 0, \"item_price_mean\"] = item_price.item_price.mean()\ntr.drop([\"item_price_x\", \"item_price_y\"], axis=1, inplace=True)\ntest[\"item_price\"] = tr.item_price_mean\ntest = test.drop(\"ID\", axis=1)\ntest[\"date_block_num\"] = 34\ntest[\"date\"] = \"01.11.2015\"\ntrain_new = pd.concat([train.sort_values(\"date\"), test], axis=0, ignore_index=True)\ndel train\ndel tr\ndel item_price\ndel shop_item_price\ntrain_new","f9da7579":"if feature_generation.get(\"last_n_month_sales\"):\n    train_new = cudf.from_pandas(train_new)\n    groupby_obj = train_new.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]) # Took a long time\n    periods = [1, 2]\n    for p in periods:\n        train_new[f\"last_{p}_month_sales\"] = groupby_obj[\"item_cnt_month\"].shift(periods = p)\n    del groupby_obj\n    train_new = train_new.to_pandas()\ntrain_new","2042feff":"if feature_generation.get(\"month_quarter_season_sin_cos\"):\n    train_new[\"date\"] = pd.to_datetime(train_new.date, format = \"%d.%m.%Y\")\n    train_new[\"year\"] = pd.DatetimeIndex(train_new[\"date\"]).year\n    train_new['month'] = pd.DatetimeIndex(train_new['date']).month\n    train_new['quarter'] = pd.DatetimeIndex(train_new['date']).quarter\n    train_new['season'] = train_new.month % 12 \/\/ 3 + 1\n    month_in_year = 12\n    train_new['month_sin'] = np.sin(2*np.pi*train_new.month\/month_in_year)\n    train_new['month_cos'] = np.cos(2*np.pi*train_new.month\/month_in_year)\n    quarters_in_year = 4\n    train_new['quarter_sin'] = np.sin(2*np.pi*train_new.quarter\/quarters_in_year)\n    train_new['quarter_cos'] = np.cos(2*np.pi*train_new.quarter\/quarters_in_year)\n    seasons_in_year = 4\n    train_new['season_sin'] = np.sin(2*np.pi*train_new.season\/seasons_in_year)\n    train_new['season_cos'] = np.cos(2*np.pi*train_new.season\/seasons_in_year)\n    \n    for col in train_new.columns:\n        if \"_sin\" in col or \"_cos\" in col:\n            train_new[col] = np.clip(train_new[col].values, 0.0, 1.0).astype(\"float16\")\n        \ntrain_new","840bb59f":"if feature_generation.get(\"add_mul_decompose\"):\n    from statsmodels.tsa.seasonal import seasonal_decompose\n\n    decompose_cols =  [\"item_price\"]\n\n    for col in decompose_cols:\n        decomp = seasonal_decompose(train_new[col], freq=52, model='additive', extrapolate_trend='freq')\n        train_new[f\"{col}_trend_add\"] = decomp.trend\n        train_new[f\"{col}_seasonal_add\"] = decomp.seasonal\n        decomp = seasonal_decompose(train_new[col], freq=52, model='multiplicative', extrapolate_trend='freq')\n        train_new[f\"{col}_trend_mul\"] = decomp.trend\n        train_new[f\"{col}_seasonal_mul\"] = decomp.seasonal\n\n    train_new = train_new.sort_values(\"date\").reset_index(drop=True)\ntrain_new","87490c96":"if feature_generation.get(\"item_cat\"):\n    train_new = train_new.merge(item_cat.drop(\"item_name_translated\", axis=1), how=\"inner\", on=\"item_id\").reset_index(drop=True)\n    cat_features.append(\"item_cat1\")\n    cat_features.append(\"item_cat2\")\n    encode_cols.append(\"item_cat1\")\n    encode_cols.append(\"item_cat2\")\n    train_new[\"item_cat2\"].fillna(train_new[\"item_cat2\"].value_counts().idxmax(), inplace=True)\nif feature_generation[\"shop_cat\"]:\n    train_new = train_new.merge(shops.drop(\"Name\", axis=1), how=\"inner\", on=\"shop_id\").reset_index(drop=True)\n    cat_features.append(\"City\")\n    cat_features.append(\"Type\")\n    encode_cols.append(\"City\")\n    encode_cols.append(\"Type\")\ntrain_new","1fe5e17b":"if feature_generation.get(\"is_month_holiday\"):\n    train_new[\"is_month_holiday\"] = 0\n    train_new.loc[train_new.month == 1, \"is_month_holiday\"] = 1\n    cat_features.append(\"is_month_holiday\")\ntrain_new","737a8631":"if feature_generation.get(\"most_popular_item\"):\n    most_popular_itemCat1 = train_new.loc[train_new.groupby([\"item_cat1\"])[\"item_cnt_month\"].idxmax().dropna()][[\"item_cat1\", \"item_id\"]]\n    most_popular_itemCat1.rename(columns={\"item_id\": \"most_popular_itemCat1_itemid\"}, inplace=True)\n    train_new = train_new.merge(most_popular_itemCat1, how=\"left\", on=\"item_cat1\")\n    train_new[\"most_popular_itemCat1_itemid\"].fillna(train_new.groupby([\"item_cat1\"])[\"item_cnt_month\"].count().idxmax(), inplace=True)\n    cat_features.append(\"most_popular_itemCat1_itemid\")\n    \n    most_popular_itemCat2 = train_new.loc[train_new.groupby([\"item_cat2\"])[\"item_cnt_month\"].idxmax().dropna()][[\"item_cat2\", \"item_id\"]]\n    most_popular_itemCat2.rename(columns={\"item_id\": \"most_popular_itemCat2_itemid\"}, inplace=True)\n    train_new = train_new.merge(most_popular_itemCat2, how=\"left\", on = \"item_cat2\")\n    train_new[\"most_popular_itemCat2_itemid\"].fillna(train_new.groupby([\"item_cat2\"])[\"item_cnt_month\"].count().idxmax(), inplace=True)\n    cat_features.append(\"most_popular_itemCat2_itemid\")\n    \n    most_popular_shop_item = train_new.loc[train_new.groupby([\"shop_id\"])[\"item_cnt_month\"].idxmax().dropna()][[\"item_id\", \"shop_id\"]]\n    most_popular_shop_item.rename(columns={\"item_id\": \"most_popular_shop_item\"}, inplace=True)\n    train_new = train_new.merge(most_popular_shop_item, how=\"left\", on= \"shop_id\")\n    train_new[\"most_popular_shop_item\"].fillna(train_new.groupby([\"shop_id\"])[\"item_cnt_month\"].count().idxmax(), inplace=True)\n    cat_features.append(\"most_popular_shop_item\")\n    \ntrain_new","125eb7c7":"if feature_generation.get(\"item_price_stats\"):\n    stats = [\"min\", \"max\", \"mean\", \"median\", \"std\"]\n    groupby_methods = [\n        [\"shop_id\", \"item_id\"], [\"item_cat1\", \"item_id\"], [\"item_cat2\", \"item_id\"], [\"item_cat1\", \"item_cat2\", \"item_id\"],\n        [\"shop_id\", \"item_cat1\", \"item_id\"], [\"shop_id\", \"item_cat2\", \"item_id\"], [\"shop_id\", \"item_cat1\", \"item_cat2\", \"item_id\"]]\n    for method in groupby_methods:\n        for stat in stats:\n            method_name = \"_\".join(method)\n            train_new[f\"{method_name}_\" + stat] = train_new.groupby(method)[\"item_price\"].transform(stat)\n            train_new[f\"{method_name}_\" + stat].fillna(train_new[f\"{method_name}_\" + stat].mean(), inplace=True)\ntrain_new\n        ","03c34412":"if feature_generation.get(\"last_price_number\"):\n    res = []\n    for idx, val in train_new[\"item_price\"].iteritems():\n        if val < 10:\n            res.append(val)\n        elif 10 <= val < 100:\n            res.append(val % 10)\n        elif 100 <= val < 1000:\n            res.append(val % 100)\n        elif 1000 <= val < 10000:\n            res.append(val % 1000)\n        elif 10000 <= val:\n            res.append(val % 10000)\n    train_new[\"last_price_number\"] = [round(x) for x in res]\ntrain_new","e911b359":"if feature_generation.get(\"freq_encoding\"):\n    encoded_cols = [\"shop_id\", \"item_id\",\n                    \"item_cat1\", \"item_cat2\", \"City\", \"Type\", \n                    \"most_popular_itemCat1_itemid\",\"most_popular_itemCat2_itemid\", \n                    \"most_popular_shop_item\"]\n    for col in encoded_cols:\n        encoding = train_new.groupby(col).size()\n        encoding = encoding \/ len(train_new)\n        train_new[col + \"_FreqEnc\"] = train_new[col].map(encoding)\n        train_new = train_new.drop(col, axis=1)\n        cat_features.remove(col)\ntrain_new","fc8e7e9c":"cols_to_drop = [\"date\", \"month\", \"quarter\", \"season\"]\n\ntr = train_new[train_new.date_block_num <= 33].drop(cols_to_drop, axis=1)\ntest = train_new[train_new.date_block_num == 34].drop(cols_to_drop, axis=1)\n\ndel train_new","71ec13ed":"tr.info()","6a2eed15":"cat_features","c378db67":"from catboost import Pool, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nMONTHS = 33\n\nITERATIONS = 20000\nMAX_EVALS = 75\nEARLY_STOP = ITERATIONS \/\/ 10\nrstate = np.random.RandomState(42) \n\ndef run_train(df, df_test, cat_features):\n    \n    for cat in cat_features:\n        df[cat] = df[cat].astype(str)\n        df_test[cat] = df_test[cat].astype(str)\n    df = df.reset_index(drop=True)\n    \n    df_test = df_test.reset_index(drop=True)\n    \n    \n    X_tr = df[df.date_block_num < MONTHS].drop([\"date_block_num\", \"item_cnt_month\"], axis=1)\n    y_tr = df[df.date_block_num < MONTHS][\"item_cnt_month\"].to_frame()\n    X_val = df[df.date_block_num >= MONTHS].drop([\"date_block_num\", \"item_cnt_month\"], axis=1)\n    y_val = df[df.date_block_num >= MONTHS][\"item_cnt_month\"].to_frame()\n    \n    df_test = df_test.drop([\"item_cnt_month\"], axis=1)\n    \n    train_pool = Pool(X_tr, y_tr[\"item_cnt_month\"], cat_features = cat_features)\n    val_pool = Pool(X_val, y_val[\"item_cnt_month\"], cat_features = cat_features)\n    test_pool = Pool(df_test, cat_features=cat_features)\n    integer_params = ['depth','min_data_in_leaf','max_bin']\n    \n    def objective_func(params, train_pool, val_pool):\n        for param in integer_params:\n            params[param] = int(params[param])\n        # if params['bootstrap_type']['bootstrap_type'] == 'Bayesian':\n        #    bagging_temp = params['bootstrap_type'].get('bagging_temperature')\n        #    params['bagging_temperature'] = bagging_temp\n        if params['grow_policy']['grow_policy'] == 'LossGuide':\n            max_leaves = params['grow_policy'].get('max_leaves')\n            params['max_leaves'] = int(max_leaves)\n        # params['bootstrap_type'] = params['bootstrap_type']['bootstrap_type']\n        params['grow_policy'] = params['grow_policy']['grow_policy']\n\n        params['fold_len_multiplier'] = max(params['fold_len_multiplier'], 1)\n\n        print(params)\n        model = CatBoostRegressor(iterations = ITERATIONS, loss_function = \"RMSE\", task_type=\"GPU\", devices='0:1',\n                                  eval_metric = \"RMSE\", verbose=False, **params)\n        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds = EARLY_STOP)\n        loss = model.get_best_score()[\"validation\"][\"RMSE\"]\n        del model\n        print(loss)\n        return {\"loss\": loss, \"status\": STATUS_OK}\n\n\n    CB_MAX_DEPTH = 11\n    bootstrap_type = [{'bootstrap_type':'Poisson'}, \n                      {'bootstrap_type':'MVS'},\n                       {'bootstrap_type':'Bayesian',\n                        'bagging_temperature' : hp.uniform('bagging_temperature', 0, 1)},\n                      {'bootstrap_type':'Bernoulli'}] \n    LEB = ['No', 'AnyImprovement', \n           'Armijo'\n          ]\n    grow_policy = [{'grow_policy':'SymmetricTree'},\n                   {'grow_policy':'Depthwise'},\n                   {'grow_policy':'Lossguide',\n                    'max_leaves': hp.quniform('max_leaves', 2, 32, 1)}]\n    space ={\n            'depth': hp.quniform('depth', 7, CB_MAX_DEPTH, 1),\n            'max_bin' : hp.quniform('max_bin', 1, 35, 1), \n            'l2_leaf_reg' : hp.loguniform('l2_leaf_reg', 1, 10),\n            'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 1, 25, 1),\n            # 'bootstrap_type' : hp.choice('bootstrap_type', bootstrap_type),\n            'learning_rate' : hp.uniform('learning_rate', 0.05, 1),\n            'leaf_estimation_backtracking' : hp.choice('leaf_estimation_backtracking', LEB),\n            'grow_policy': hp.choice('grow_policy', grow_policy),\n            'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n           }\n    fn = partial(objective_func, train_pool=train_pool, val_pool=val_pool)\n    print(\"Tuning hyperparams...\")\n    best_params = fmin(fn = fn, space=space, algo=tpe.suggest, max_evals = MAX_EVALS, rstate=rstate)\n\n#     best_params['bootstrap_type'] = bootstrap_type[best_params['bootstrap_type']]['bootstrap_type']\n    best_params['grow_policy'] = grow_policy[best_params['grow_policy']]['grow_policy']\n    best_params['leaf_estimation_backtracking'] = LEB[best_params['leaf_estimation_backtracking']]   \n\n    for param in integer_params:\n        best_params[param] = int(best_params[param])\n    if 'max_leaves' in best_params:\n        best_params['max_leaves'] = int(best_params['max_leaves'])\n    \n\n    model = CatBoostRegressor(iterations = ITERATIONS, loss_function = \"RMSE\", custom_metric = \"RMSE\", task_type=\"GPU\", devices='0:1',\n                              verbose=1000, **best_params)\n    model.fit(train_pool, eval_set=val_pool, early_stopping_rounds = EARLY_STOP, plot=True)\n    \n    # Finetuning\n    model2 = CatBoostRegressor(iterations = 2000, loss_function = \"RMSE\", custom_metric = \"RMSE\", task_type=\"GPU\", devices='0:1',\n                              verbose=1000, **best_params)\n    model2.fit(train_pool, eval_set=val_pool, early_stopping_rounds = EARLY_STOP, plot=True, init_model=model2)\n    model2.fit(val_pool, plot=True)\n        \n    df_test[\"predictions\"] = model2.predict(test_pool)\n    \n    return df_test, model","fa932eab":"df_test, model = run_train(tr, test, cat_features)\ndf_test","cef3929a":"sub[\"item_cnt_month\"] = df_test[\"predictions\"].clip(0, 20)\nsub.to_csv(\"submission.csv\", index=False)\nsub.head(30)","e7c1d73e":"# Training Part","e1e12e80":"## Clip item_cnt_month in range [0, 20]","073a1948":"## Outlier Removal\/Winsorization\nIntuition: well it's outliers","ecdf4707":"## Jan seems to have peak sales among all years\nIntuition: Jan among all years witnessed peak sales","199d21a4":"## Monthly Groupby","8be54075":"## Use from 2014 only\nIntuition: ignoring old data may help","7b377039":"## Holiday features\nIntuition: people tend to buy more during holidays (and actually January sees peak sales)","663a052d":"# Sanity check","ac336395":"## Most popular ItemID of itemCat1, itemCat2, ShopID","520b5207":"## Seasonal decompose features\n","a86bf886":"Detect duplicate shops","d92a038e":"# Load data","3358d89a":"## More info about Shop and Item features\nIntuition: shop type, item category might help","7d7774ed":"## Cyclic features: Month, Week, Seasonality\nIntuition: the sudden change in month from Dec to Jan (from 12 to 1) may confuse the model","27076b66":"# Feature Engineering","ae427bc4":"## Frequency Encoding","95aef134":"## item_price statistics features using groupby of various column combinations","650e5a4f":"## item_price 99, 49 features\nIntuition: maybe it will help the model to see something if the price is 1099, 109, 499?","55360471":"## Fill item_price in test set with the corresponding item_id\/shop_id pair in train set. Fill NaNs with mean price of all items.","a1df5b4e":"## Lagged features\nIntuition: previous sales from last months may help"}}