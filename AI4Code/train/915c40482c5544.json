{"cell_type":{"b219ed6d":"code","9549e454":"code","624e910f":"code","136b6772":"code","dd6c4217":"code","8e6f6eb5":"code","93e078a7":"code","3dc1fb1e":"code","2f4c5c04":"code","fbe95eaf":"code","401d745c":"code","ee1d6c04":"code","e7f63399":"code","b8a14698":"code","0a9453fb":"code","1b795867":"code","78795d32":"code","0b959be0":"code","82d9411f":"code","325e9e35":"code","d1378666":"code","7d8e6b5b":"code","32bf64b2":"code","40fe61d2":"code","8b349d82":"code","8c5c1ac7":"code","b1833e46":"code","c1e9b0f4":"code","7f05bd7f":"code","22a13302":"code","f0a07227":"code","9f3c6cb9":"code","6a66983d":"code","7e7731c6":"code","af71877a":"code","f84fc669":"code","f664eadf":"code","afbfa340":"code","c0bbb502":"code","b69da8c8":"code","bce33170":"code","3e88f059":"code","fca672ed":"code","dbbc2074":"code","c53e1e72":"code","0a2c7a83":"code","976ac24e":"markdown","0a9d1f7d":"markdown","89875964":"markdown","74b11cf2":"markdown","34d8d2b4":"markdown","1a0c9e33":"markdown","d2fe0d80":"markdown","4f6a29c8":"markdown","9334660c":"markdown","1d72a7d4":"markdown","f5c89970":"markdown","67dd72b6":"markdown","72961046":"markdown","14246371":"markdown","0d34a5c2":"markdown","f994764e":"markdown","0de8b463":"markdown","823dd5b8":"markdown","ff7fa64b":"markdown","7c76dbea":"markdown","187948e2":"markdown","376fc80d":"markdown"},"source":{"b219ed6d":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport shutil\nimport glob\nimport random\nimport tqdm.notebook as tqdm\nfrom PIL import Image\n\nimport torch\nfrom torch import nn, optim\nfrom torchvision import transforms, datasets, models\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg","9549e454":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ['CUDA_LAUNCH_BLOCKING']='1'\nnp.random.seed(0)\ntorch.manual_seed(0)","624e910f":"model = torch.load('\/kaggle\/input\/modelproj2\/model.pth')","136b6772":"NORFOLK_PATH = '\/kaggle\/input\/stanford-dogs-dataset\/images\/Images\/n02094114-Norfolk_terrier\/'\nNORWICH_PATH = '\/kaggle\/input\/stanford-dogs-dataset\/images\/Images\/n02094258-Norwich_terrier\/'\nDANE_PATH = '\/kaggle\/input\/stanford-dogs-dataset\/images\/Images\/n02109047-Great_Dane\/'","dd6c4217":"random_norfolk = NORFOLK_PATH + random.choice(os.listdir(NORFOLK_PATH))\nrandom_norwich = NORWICH_PATH + random.choice(os.listdir(NORWICH_PATH))\nrandom_dane = DANE_PATH + random.choice(os.listdir(DANE_PATH))","8e6f6eb5":"transform = {\n    \"train\": transforms.Compose([\n         transforms.Resize((224, 224)),\n         transforms.RandomHorizontalFlip(p=0.5),\n         transforms.RandomRotation(degrees=(-90, 90)),\n         transforms.RandomVerticalFlip(p=0.5),\n         transforms.ToTensor(),\n         transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n         ]),\n    \"validation\": transforms.Compose([\n         transforms.Resize((224, 224)),\n         transforms.ToTensor(),\n         transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n         ]),\n    \"holdout\": transforms.Compose([\n         transforms.Resize((224, 224)),\n         transforms.ToTensor(),\n         transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n         ])\n}","93e078a7":"dest = '\/kaggle\/working\/small_ds'\nos.makedirs(dest + '\/n02094114-Norfolk_terrier\/')\nos.makedirs(dest + '\/n02094258-Norwich_terrier\/')\nos.makedirs(dest + '\/n02109047-Great_Dane\/')\n\nshutil.copy(random_norfolk, dest + '\/n02094114-Norfolk_terrier\/')\nshutil.copy(random_norwich, dest + '\/n02094258-Norwich_terrier\/')\nshutil.copy(random_dane, dest + '\/n02109047-Great_Dane\/')","3dc1fb1e":"small_ds = datasets.ImageFolder(root='\/kaggle\/working\/small_ds', transform=transform[\"holdout\"])\nsmall_dl = DataLoader(small_ds, batch_size=1, shuffle=True, drop_last=False)","2f4c5c04":"def get_actual_img(class_name):\n    if 'Norfolk' in class_name:\n        return dest + '\/n02094114-Norfolk_terrier\/' + os.listdir(dest + '\/n02094114-Norfolk_terrier\/')[0]\n    elif 'Norwich' in class_name:\n        return dest + '\/n02094258-Norwich_terrier\/' + os.listdir(dest + '\/n02094258-Norwich_terrier\/')[0]\n    return dest + '\/n02109047-Great_Dane\/' + os.listdir(dest + '\/n02109047-Great_Dane\/')[0]","fbe95eaf":"model.eval()\nfor image, label in small_dl: \n    image, label = image.to(device), label.to(device)\n    with torch.no_grad():\n        predictions = model(image).to(device)\n    ind = np.argmax(predictions.cpu().numpy())\n    pred_lab = [class_name for class_name, lab in small_ds.class_to_idx.items() if lab == ind]\n    act_lab = [class_name for class_name, lab in small_ds.class_to_idx.items() if lab == label]\n    img_path = get_actual_img(act_lab[0])\n    img = mpimg.imread(img_path)\n    plt.figure()\n    plt.title('Actual label: ' + act_lab[0] + '\\n' + 'Predicted label: ' + pred_lab[0])\n    plt.imshow(img)","401d745c":"norf_norw = '\/kaggle\/working\/Norfolk-Norwich\/'\nnorf_dane = '\/kaggle\/working\/Norfolk-Dane\/'\nnorw_dane = '\/kaggle\/working\/Norwich-Dane\/'\nshutil.copytree('\/kaggle\/input\/dogs-small' , norf_norw ,\n                ignore=shutil.ignore_patterns(\"n02109047-Great_Dane\"))\nshutil.copytree('\/kaggle\/input\/dogs-small', norf_dane ,\n                ignore=shutil.ignore_patterns('n02094258-Norwich_terrier'))\nshutil.copytree('\/kaggle\/input\/dogs-small', norw_dane ,\n                ignore=shutil.ignore_patterns('n02094114-Norfolk_terrier'))","ee1d6c04":"datasetss = {\n    'norfolk-norwich-train': datasets.ImageFolder(root='\/kaggle\/working\/Norfolk-Norwich\/train', \n                                                  transform=transform[\"train\"]),\n    'norfolk-norwich-validation': datasets.ImageFolder(root='\/kaggle\/working\/Norfolk-Norwich\/validation', \n                                                  transform=transform[\"validation\"]),\n    'norfolk-norwich-holdout': datasets.ImageFolder(root='\/kaggle\/working\/Norfolk-Norwich\/holdout', \n                                                  transform=transform[\"holdout\"]),\n    \n    'norfolk-dane-train': datasets.ImageFolder(root='\/kaggle\/working\/Norfolk-Dane\/train', \n                                                  transform=transform[\"train\"]),\n    'norfolk-dane-validation': datasets.ImageFolder(root='\/kaggle\/working\/Norfolk-Dane\/validation', \n                                                  transform=transform[\"validation\"]),\n    'norfolk-dane-holdout': datasets.ImageFolder(root='\/kaggle\/working\/Norfolk-Dane\/validation', \n                                                  transform=transform[\"holdout\"]),\n    \n    'norwich-dane-train': datasets.ImageFolder(root='\/kaggle\/working\/Norwich-Dane\/train', \n                                                  transform=transform[\"train\"]),\n    'norwich-dane-validation': datasets.ImageFolder(root='\/kaggle\/working\/Norwich-Dane\/validation', \n                                                  transform=transform[\"validation\"]),\n    'norwich-dane-holdout': datasets.ImageFolder(root='\/kaggle\/working\/Norwich-Dane\/validation', \n                                                  transform=transform[\"holdout\"])\n}","e7f63399":"dataloaders = {\n    'norfolk-norwich-train': DataLoader(datasetss['norfolk-norwich-train'] ,\n                                        batch_size=32, shuffle=True, drop_last=False),\n    'norfolk-norwich-validation': DataLoader(datasetss['norfolk-norwich-validation'], \n                                        batch_size=32, shuffle=True, drop_last=False),\n    'norfolk-norwich-holdout': DataLoader(datasetss['norfolk-norwich-holdout'], \n                                        batch_size=32, shuffle=True, drop_last=False),\n    \n    'norfolk-dane-train': DataLoader(datasetss['norfolk-dane-train'] ,\n                                        batch_size=32, shuffle=True, drop_last=False),\n    'norfolk-dane-validation': DataLoader(datasetss['norfolk-dane-validation'] ,\n                                        batch_size=32, shuffle=True, drop_last=False),\n    'norfolk-dane-holdout': DataLoader(datasetss['norfolk-dane-holdout'] ,\n                                        batch_size=32, shuffle=True, drop_last=False),\n    \n    'norwich-dane-train': DataLoader(datasetss['norwich-dane-train'] ,\n                                        batch_size=32, shuffle=True, drop_last=False),\n    'norwich-dane-validation': DataLoader(datasetss['norwich-dane-validation'] ,\n                                        batch_size=32, shuffle=True, drop_last=False),\n    'norwich-dane-holdout': DataLoader(datasetss['norwich-dane-holdout'] ,\n                                        batch_size=32, shuffle=True, drop_last=False),\n}","b8a14698":"model_nfnw = models.densenet121(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model_nfnw.parameters():\n        param.requires_grad = False\n        \nnum_ftrs = model_nfnw.classifier.in_features\nmodel_nfnw.classifier = nn.Linear(num_ftrs, 2)\nmodel_nfnw.to(device)\nloss_criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_nfnw.parameters(), lr=0.01)","0a9453fb":"epochs = 20\ntraining_losses = []\nvalidation_losses = []\n\ntraining_acc = []\nvalidation_acc = []\n\nfor e in range(epochs):\n    print(\"In epoch\", e)\n    running_loss = 0\n    running_acc = 0\n    model_nfnw.train()\n    for images, labels in dataloaders['norfolk-norwich-train']: \n        images, labels = images.to(device), labels.to(device)\n        predictions = model_nfnw(images).to(device) \n        _, preds = torch.max(predictions, 1)\n        loss = loss_criterion(predictions, labels) \n        optimizer.zero_grad() \n        loss.backward() \n        optimizer.step()\n        \n        running_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n        running_loss += loss.item()\n    valid_loss = 0\n    valid_acc = 0\n    model_nfnw.eval()\n    for images, labels in dataloaders['norfolk-norwich-validation']: \n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            predictions = model_nfnw(images).to(device)\n            _, preds = torch.max(predictions, 1)\n            loss = loss_criterion(predictions, labels)\n        valid_loss += loss.item()\n        valid_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n    print(f\"\\tTraining loss: {running_loss\/len(dataloaders['norfolk-norwich-train'])}\")\n    print(f\"\\tValidation loss: {valid_loss\/len(dataloaders['norfolk-norwich-validation'])}\")   \n    print(f\"\\n\\tTraining accuracy: {running_acc\/len(dataloaders['norfolk-norwich-train'])}\")\n    print(f\"\\tValidation accuracy: {valid_acc\/len(dataloaders['norfolk-norwich-validation'])}\")  \n    training_losses.append(running_loss\/len(dataloaders['norfolk-norwich-train']))\n    validation_losses.append(valid_loss\/len(dataloaders['norfolk-norwich-validation']))\n    training_acc.append(running_acc\/len(dataloaders['norfolk-norwich-train']))\n    validation_acc.append(valid_acc\/len(dataloaders['norfolk-norwich-validation']))","1b795867":"plt.plot(range(1, 21), training_losses, \"-b\", label=\"training\")\nplt.plot(range(1, 21), validation_losses, \"-r\", label=\"validation\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","78795d32":"plt.plot(range(1, 21), training_acc, \"-b\", label=\"training\")\nplt.plot(range(1, 21), validation_acc, \"-r\", label=\"validation\")\nplt.title(\"Training vs Validation Accuracies\")\nplt.legend(loc=\"upper right\")\nplt.show()","0b959be0":"holdout_loss = 0\nholdout_accuracy = 0\nmodel_nfnw.eval()\nfor images, labels in dataloaders['norfolk-norwich-holdout']: \n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model_nfnw(images).to(device)\n        loss = loss_criterion(predictions, labels)\n    holdout_loss += loss.item()\n    holdout_accuracy += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\nprint(f\"Holdout loss: {holdout_loss\/len(dataloaders['norfolk-norwich-holdout'])}\")\nprint(f\"Holdout accuracy: {holdout_accuracy\/len(dataloaders['norfolk-norwich-holdout'])}\") ","82d9411f":"model_nfd = models.densenet121(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model_nfd.parameters():\n        param.requires_grad = False\n        \nnum_ftrs = model_nfd.classifier.in_features\nmodel_nfd.classifier = nn.Linear(num_ftrs, 2)\nmodel_nfd.to(device)\nloss_criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_nfd.parameters(), lr=0.01)","325e9e35":"datasetss['norfolk-dane-train'].class_to_idx","d1378666":"epochs = 20\ntraining_losses = []\nvalidation_losses = []\n\ntraining_acc = []\nvalidation_acc = []\n\nfor e in range(epochs):\n    print(\"In epoch\", e)\n    running_loss = 0\n    running_acc = 0\n    model_nfd.train()\n    for images, labels in dataloaders['norfolk-dane-train']: \n        images, labels = images.to(device), labels.to(device)\n        predictions = model_nfd(images).to(device) \n        _, preds = torch.max(predictions, 1)\n        loss = loss_criterion(predictions, labels) \n        optimizer.zero_grad() \n        loss.backward() \n        optimizer.step()\n        \n        running_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n        running_loss += loss.item()\n    valid_loss = 0\n    valid_acc = 0\n    model_nfd.eval()\n    for images, labels in dataloaders['norfolk-dane-validation']: \n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            predictions = model_nfd(images).to(device)\n            _, preds = torch.max(predictions, 1)\n            loss = loss_criterion(predictions, labels)\n        valid_loss += loss.item()\n        valid_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n    print(f\"\\tTraining loss: {running_loss\/len(dataloaders['norfolk-dane-train'])}\")\n    print(f\"\\tValidation loss: {valid_loss\/len(dataloaders['norfolk-dane-validation'])}\")   \n    print(f\"\\n\\tTraining accuracy: {running_acc\/len(dataloaders['norfolk-dane-train'])}\")\n    print(f\"\\tValidation accuracy: {valid_acc\/len(dataloaders['norfolk-dane-validation'])}\")  \n    training_losses.append(running_loss\/len(dataloaders['norfolk-dane-train']))\n    validation_losses.append(valid_loss\/len(dataloaders['norfolk-dane-validation']))\n    training_acc.append(running_acc\/len(dataloaders['norfolk-dane-train']))\n    validation_acc.append(valid_acc\/len(dataloaders['norfolk-dane-validation']))","7d8e6b5b":"plt.plot(range(1, 21), training_losses, \"-b\", label=\"training\")\nplt.plot(range(1, 21), validation_losses, \"-r\", label=\"validation\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","32bf64b2":"plt.plot(range(1, 21), training_acc, \"-b\", label=\"training\")\nplt.plot(range(1, 21), validation_acc, \"-r\", label=\"validation\")\nplt.title(\"Training vs Validation Accuracies\")\nplt.legend(loc=\"upper right\")\nplt.show()","40fe61d2":"holdout_loss = 0\nholdout_accuracy = 0\nmodel_nfd.eval()\nfor images, labels in dataloaders['norfolk-dane-holdout']: \n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model_nfd(images).to(device)\n        loss = loss_criterion(predictions, labels)\n    holdout_loss += loss.item()\n    holdout_accuracy += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\nprint(f\"Holdout loss: {holdout_loss\/len(dataloaders['norfolk-dane-holdout'])}\")\nprint(f\"Holdout accuracy: {holdout_accuracy\/len(dataloaders['norfolk-dane-holdout'])}\") ","8b349d82":"model_nwd = models.densenet121(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model_nwd.parameters():\n        param.requires_grad = False\n        \nnum_ftrs = model_nwd.classifier.in_features\nmodel_nwd.classifier = nn.Linear(num_ftrs, 2)\nmodel_nwd.to(device)\nloss_criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_nwd.parameters(), lr=0.01)","8c5c1ac7":"epochs = 20\ntraining_losses = []\nvalidation_losses = []\n\ntraining_acc = []\nvalidation_acc = []\n\nfor e in range(epochs):\n    print(\"In epoch\", e)\n    running_loss = 0\n    running_acc = 0\n    model_nwd.train()\n    for images, labels in dataloaders['norwich-dane-train']: \n        images, labels = images.to(device), labels.to(device)\n        predictions = model_nwd(images).to(device) \n        _, preds = torch.max(predictions, 1)\n        loss = loss_criterion(predictions, labels) \n        optimizer.zero_grad() \n        loss.backward() \n        optimizer.step()\n        \n        running_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n        running_loss += loss.item()\n    valid_loss = 0\n    valid_acc = 0\n    model_nwd.eval()\n    for images, labels in dataloaders['norwich-dane-validation']: \n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            predictions = model_nwd(images).to(device)\n            _, preds = torch.max(predictions, 1)\n            loss = loss_criterion(predictions, labels)\n        valid_loss += loss.item()\n        valid_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n    print(f\"\\tTraining loss: {running_loss\/len(dataloaders['norwich-dane-train'])}\")\n    print(f\"\\tValidation loss: {valid_loss\/len(dataloaders['norwich-dane-validation'])}\")   \n    print(f\"\\n\\tTraining accuracy: {running_acc\/len(dataloaders['norwich-dane-train'])}\")\n    print(f\"\\tValidation accuracy: {valid_acc\/len(dataloaders['norwich-dane-validation'])}\")  \n    training_losses.append(running_loss\/len(dataloaders['norwich-dane-train']))\n    validation_losses.append(valid_loss\/len(dataloaders['norwich-dane-validation']))\n    training_acc.append(running_acc\/len(dataloaders['norwich-dane-train']))\n    validation_acc.append(valid_acc\/len(dataloaders['norwich-dane-validation']))","b1833e46":"plt.plot(range(1, 21), training_losses, \"-b\", label=\"training\")\nplt.plot(range(1, 21), validation_losses, \"-r\", label=\"validation\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","c1e9b0f4":"plt.plot(range(1, 21), training_acc, \"-b\", label=\"training\")\nplt.plot(range(1, 21), validation_acc, \"-r\", label=\"validation\")\nplt.title(\"Training vs Validation Accuracies\")\nplt.legend(loc=\"upper right\")\nplt.show()","7f05bd7f":"holdout_loss = 0\nholdout_accuracy = 0\nmodel_nfd.eval()\nfor images, labels in dataloaders['norwich-dane-holdout']: \n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model_nwd(images).to(device)\n        loss = loss_criterion(predictions, labels)\n    holdout_loss += loss.item()\n    holdout_accuracy += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\nprint(f\"Holdout loss: {holdout_loss\/len(dataloaders['norwich-dane-holdout'])}\")\nprint(f\"Holdout accuracy: {holdout_accuracy\/len(dataloaders['norwich-dane-holdout'])}\") ","22a13302":"shutil.copytree('\/kaggle\/input\/dogs-small' , '\/kaggle\/working\/mislabelled')","f0a07227":"to_move = random.sample(os.listdir('\/kaggle\/working\/mislabelled\/train\/n02109047-Great_Dane\/'), \n              round(len(os.listdir('\/kaggle\/working\/mislabelled\/train\/n02109047-Great_Dane\/'))*0.2))\nfor file in to_move:\n    shutil.move('\/kaggle\/working\/mislabelled\/train\/n02109047-Great_Dane\/'+file, \n                '\/kaggle\/working\/mislabelled\/train\/' + random.choice(['n02094258-Norwich_terrier\/', \n                                                                      'n02094114-Norfolk_terrier\/']))","9f3c6cb9":"dataset = {\n    \"train\": datasets.ImageFolder(root='\/kaggle\/working\/mislabelled\/train', transform=transform[\"train\"]),\n    \"validation\": datasets.ImageFolder(root='\/kaggle\/working\/mislabelled\/validation', transform=transform[\"validation\"]),\n    \"holdout\": datasets.ImageFolder(root='\/kaggle\/working\/mislabelled\/holdout', transform=transform[\"holdout\"])\n}","6a66983d":"dataloaders = {\n    \"train\": DataLoader(dataset['train'], batch_size=32, shuffle=True, drop_last=False),\n    \"validation\": DataLoader(dataset['validation'], batch_size=32, shuffle=True, drop_last=False),\n    \"holdout\": DataLoader(dataset['holdout'], batch_size=32, shuffle=True, drop_last=False),\n}","7e7731c6":"model_mis = models.densenet121(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model_mis.parameters():\n        param.requires_grad = False\n        \nnum_ftrs = model_mis.classifier.in_features\nmodel_mis.classifier = nn.Linear(num_ftrs, 3)\nmodel_mis.to(device)\nloss_criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_mis.parameters(), lr=0.01)","af71877a":"epochs = 20\ntraining_losses = []\nvalidation_losses = []\n\ntraining_acc = []\nvalidation_acc = []\n\nfor e in range(epochs):\n    print(\"In epoch\", e)\n    running_loss = 0\n    running_acc = 0\n    model_mis.train()\n    for images, labels in dataloaders['train']: \n        images, labels = images.to(device), labels.to(device)\n        predictions = model_mis(images).to(device) \n        _, preds = torch.max(predictions, 1)\n        loss = loss_criterion(predictions, labels) \n        optimizer.zero_grad() \n        loss.backward() \n        optimizer.step()\n        \n        running_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n        running_loss += loss.item()\n    valid_loss = 0\n    valid_acc = 0\n    model_mis.eval()\n    for images, labels in dataloaders['validation']: \n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            predictions = model_mis(images).to(device)\n            _, preds = torch.max(predictions, 1)\n            loss = loss_criterion(predictions, labels)\n        valid_loss += loss.item()\n        valid_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n    print(f\"\\tTraining loss: {running_loss\/len(dataloaders['train'])}\")\n    print(f\"\\tValidation loss: {valid_loss\/len(dataloaders['validation'])}\")   \n    print(f\"\\n\\tTraining accuracy: {running_acc\/len(dataloaders['train'])}\")\n    print(f\"\\tValidation accuracy: {valid_acc\/len(dataloaders['validation'])}\")  \n    training_losses.append(running_loss\/len(dataloaders['train']))\n    validation_losses.append(valid_loss\/len(dataloaders['validation']))\n    training_acc.append(running_acc\/len(dataloaders['train']))\n    validation_acc.append(valid_acc\/len(dataloaders['validation']))","f84fc669":"plt.plot(range(1, 21), training_losses, \"-b\", label=\"training\")\nplt.plot(range(1, 21), validation_losses, \"-r\", label=\"validation\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","f664eadf":"plt.plot(range(1, 21), training_acc, \"-b\", label=\"training\")\nplt.plot(range(1, 21), validation_acc, \"-r\", label=\"validation\")\nplt.title(\"Training vs Validation Accuracies\")\nplt.legend(loc=\"upper right\")\nplt.show()","afbfa340":"holdout_loss = 0\nholdout_accuracy = 0\nmodel_mis.eval()\nfor images, labels in dataloaders['holdout']: \n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model_mis(images).to(device)\n        loss = loss_criterion(predictions, labels)\n    holdout_loss += loss.item()\n    holdout_accuracy += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\nprint(f\"Holdout loss: {holdout_loss\/len(dataloaders['holdout'])}\")\nprint(f\"Holdout accuracy: {holdout_accuracy\/len(dataloaders['holdout'])}\") ","c0bbb502":"def get_avg_image(path, name):\n    # Access all PNG files in directory\n    allfiles=os.listdir(path)\n    imlist=[filename for filename in allfiles if  filename[-4:] in [\".jpg\",\".JPG\"]]\n\n    # Assuming all images are the same size, get dimensions of first image\n    w,h=Image.open(path + imlist[0]).size\n    N=len(imlist)\n\n    # Create a numpy array of floats to store the average (assume RGB images)\n    arr=np.zeros((224,224,3),np.float)\n\n    # Build up average pixel intensities, casting each image as an array of floats\n    for im in imlist:\n        imarr=np.array(Image.open(path + im).resize((224, 224)),dtype=np.float)\n        arr=arr+imarr\/N\n\n    # Round values in array and cast as 8-bit integer\n    arr=np.array(np.round(arr),dtype=np.uint8)\n\n    # Generate, save and preview final image\n    out=Image.fromarray(arr,mode=\"RGB\")\n    # out.save(\"Average.png\")\n    # out.show()\n    plt.figure()\n    plt.title(name)\n    plt.imshow(out)\n    plt.show()","b69da8c8":"plt.figure()\nget_avg_image('\/kaggle\/input\/dogs-small\/train\/n02094114-Norfolk_terrier\/', 'Norfolk Terrier')\nplt.figure()\nget_avg_image('\/kaggle\/input\/dogs-small\/train\/n02094258-Norwich_terrier\/', 'Norwich Terrier')\nplt.figure()\nget_avg_image('\/kaggle\/input\/dogs-small\/train\/n02109047-Great_Dane\/', 'Great Dane')","bce33170":"dataset = {\n    \"train\": datasets.ImageFolder(root='\/kaggle\/input\/dogs-small\/train', \n                                  transform=transform[\"train\"]),\n    \"validation\": datasets.ImageFolder(root='\/kaggle\/input\/dogs-small\/validation', \n                                       transform=transform[\"validation\"]),\n    \"holdout\": datasets.ImageFolder(root='\/kaggle\/input\/dogs-small\/holdout', \n                                    transform=transform[\"holdout\"])\n}\n\ndataloaders = {\n    \"train\": DataLoader(dataset['train'], batch_size=32, shuffle=True, drop_last=False),\n    \"validation\": DataLoader(dataset['validation'], batch_size=32, shuffle=True, drop_last=False),\n    \"holdout\": DataLoader(dataset['holdout'], batch_size=32, shuffle=True, drop_last=False),\n}","3e88f059":"model_new = models.densenet121(pretrained=False)\nnum_ftrs = model_new.classifier.in_features\nmodel_new.classifier = nn.Linear(num_ftrs, 3)\nmodel_new.to(device)\nloss_criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_new.parameters(), lr=0.01)","fca672ed":"epochs = 20\ntraining_losses = []\nvalidation_losses = []\n\ntraining_acc = []\nvalidation_acc = []\n\nfor e in range(epochs):\n    print(\"In epoch\", e)\n    running_loss = 0\n    running_acc = 0\n    model_new.train()\n    for images, labels in dataloaders['train']: \n        images, labels = images.to(device), labels.to(device)\n        predictions = model_new(images).to(device) \n        _, preds = torch.max(predictions, 1)\n        loss = loss_criterion(predictions, labels) \n        optimizer.zero_grad() \n        loss.backward() \n        optimizer.step()\n        \n        running_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n        running_loss += loss.item()\n    valid_loss = 0\n    valid_acc = 0\n    model_new.eval()\n    for images, labels in dataloaders['validation']: \n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            predictions = model_new(images).to(device)\n            _, preds = torch.max(predictions, 1)\n            loss = loss_criterion(predictions, labels)\n        valid_loss += loss.item()\n        valid_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n    print(f\"\\tTraining loss: {running_loss\/len(dataloaders['train'])}\")\n    print(f\"\\tValidation loss: {valid_loss\/len(dataloaders['validation'])}\")   \n    print(f\"\\n\\tTraining accuracy: {running_acc\/len(dataloaders['train'])}\")\n    print(f\"\\tValidation accuracy: {valid_acc\/len(dataloaders['validation'])}\")  \n    training_losses.append(running_loss\/len(dataloaders['train']))\n    validation_losses.append(valid_loss\/len(dataloaders['validation']))\n    training_acc.append(running_acc\/len(dataloaders['train']))\n    validation_acc.append(valid_acc\/len(dataloaders['validation']))","dbbc2074":"holdout_loss = 0\nholdout_accuracy = 0\nmodel_new.eval()\nfor images, labels in dataloaders['holdout']: \n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model_new(images).to(device)\n        loss = loss_criterion(predictions, labels)\n    holdout_loss += loss.item()\n    holdout_accuracy += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\nprint(f\"Holdout loss: {holdout_loss\/len(dataloaders['holdout'])}\")\nprint(f\"Holdout accuracy: {holdout_accuracy\/len(dataloaders['holdout'])}\") ","c53e1e72":"def saliency_map(img_path, model, transform):\n    img = Image.open(img_path)\n    img = transform(img)\n    img = img.reshape(1, 3, 224, 224)\n    img = img.to(device)\n    img.requires_grad_()\n    output = model(img)\n    output_idx = output.argmax()\n    output_max = output[0, output_idx]\n    output_max.backward()\n    saliency, _ = torch.max(img.grad.data.abs(), dim=1) \n    saliency = saliency.reshape(224, 224)\n    image = img.reshape(-1, 224, 224)\n    \n    \n    fig, ax = plt.subplots(1, 2)\n    fig.set_figheight(5)\n    fig.set_figwidth(5)\n    ax[0].imshow(image.cpu().detach().numpy().transpose(1, 2, 0))\n    ax[0].axis('off')\n    ax[1].imshow(saliency.cpu(), cmap='hot')\n    ax[1].axis('off')\n    plt.tight_layout()\n#     fig.suptitle('The Image and Its Saliency Map')\n    plt.show()","0a2c7a83":"print('Norfolk Terriers - Train')\nfor image in os.listdir('\/kaggle\/input\/dogs-small\/train\/n02094114-Norfolk_terrier\/'):\n    saliency_map('\/kaggle\/input\/dogs-small\/train\/n02094114-Norfolk_terrier\/' + image, model, \n                 transform['train'])\nprint('Norwich Terriers - Train')\nfor image in os.listdir('\/kaggle\/input\/dogs-small\/train\/n02094258-Norwich_terrier\/'):\n    saliency_map('\/kaggle\/input\/dogs-small\/train\/n02094258-Norwich_terrier\/' + image, model, \n                 transform['train'])\nprint('Great Dane - Train')\nfor image in os.listdir('\/kaggle\/input\/dogs-small\/train\/n02109047-Great_Dane\/'):\n    saliency_map('\/kaggle\/input\/dogs-small\/train\/n02109047-Great_Dane\/' + image, model, \n                 transform['train'])\n\nprint('Norfolk Terriers - validation')\nfor image in os.listdir('\/kaggle\/input\/dogs-small\/validation\/n02094114-Norfolk_terrier\/'):\n    saliency_map('\/kaggle\/input\/dogs-small\/validation\/n02094114-Norfolk_terrier\/' + image, model, \n                 transform['validation'])\nprint('Norwich Terriers - validation')\nfor image in os.listdir('\/kaggle\/input\/dogs-small\/validation\/n02094258-Norwich_terrier\/'):\n    saliency_map('\/kaggle\/input\/dogs-small\/validation\/n02094258-Norwich_terrier\/' + image, model, \n                 transform['validation'])\nprint('Great Dane - validation')\nfor image in os.listdir('\/kaggle\/input\/dogs-small\/validation\/n02109047-Great_Dane\/'):\n    saliency_map('\/kaggle\/input\/dogs-small\/validation\/n02109047-Great_Dane\/' + image, model, \n                 transform['validation'])\n\nprint('Norfolk Terriers - holdout')\nfor image in os.listdir('\/kaggle\/input\/dogs-small\/holdout\/n02094114-Norfolk_terrier\/'):\n    saliency_map('\/kaggle\/input\/dogs-small\/holdout\/n02094114-Norfolk_terrier\/' + image, model, \n                 transform['holdout'])\nprint('Norwich Terriers - holdout')\nfor image in os.listdir('\/kaggle\/input\/dogs-small\/holdout\/n02094258-Norwich_terrier\/'):\n    saliency_map('\/kaggle\/input\/dogs-small\/holdout\/n02094258-Norwich_terrier\/' + image, model, \n                 transform['holdout'])\nprint('Great Dane - holdout')\nfor image in os.listdir('\/kaggle\/input\/dogs-small\/holdout\/n02109047-Great_Dane\/'):\n    saliency_map('\/kaggle\/input\/dogs-small\/holdout\/n02109047-Great_Dane\/' + image, model, \n                 transform['holdout'])","976ac24e":"### PART 1\n### Task 1","0a9d1f7d":"### Norwich - Dane","89875964":"#### Norfolk-Norwich","74b11cf2":"The results show that distinguishing Norfolk and Norwich breeds is harder than Dane and the remaining two. We can see that the performance on Dane-Norfolk and Dane-Norwich is too high. However, Norfolk-Norwich is not performing well.","34d8d2b4":"#### Norfolk-Dane","1a0c9e33":"### Paper Discussion\n#### Paper 1. Evaluating Saliency Map Explanations for Convolutional Neural Networks: A User Study\nThe authors examine saliency maps in the context of CNNs and see how they can explain those \"black-box\" models. They use layerwise relevance propogation to evaluate the performance of saliency maps. What they did was to show people images and ask them to predict whether a model could correctly classify it. Half of the users were given saliency while the other half was not. Apart from that, classification scores were also one of the variables they controlled. So, the users were presented with these combinations:\n* \u2002 Saliency maps not shown and scores not shown (Baseline)\n* \u2002 Saliency maps not shown and scores shown\n* \u2002 Saliency maps shown and scores not shown\n* \u2002 Saliency maps shown and scores shown.\n\nEven though there are numerous limitation discussed in the paper, the results show that those with saliency maps answered correctly more than those without:\n\n![results](https:\/\/www.linkpicture.com\/q\/Screen-Shot-2021-12-06-at-13.26.51.png)","d2fe0d80":"The maps above help us see which parts of images caused classification to be happen the way it happened. Happily, in most images it makes sense and we see that the parts with a dog are highlighted more. Even though some images look very ambiguous, and do not show a clear part that is highlighted, most of the images and their maps actually make sense.","4f6a29c8":"### Task 3","9334660c":"#### Saliency Maps","1d72a7d4":"### Part Three","f5c89970":"I think one possible bias of this dataset is caused by some dogs having more photos available, while others have less. Our 3 dogs have different number of images even though these number do not differ that much of each other. Additionally, in our case 2 of 3 classes are very similar to each other, while the third is very different from those two. These aspects cause some of the dogs be recognized better, while others not.\n\nThis could be fixed by gathering more data for those dogs that are underrepresented. This could fix the bias. Plus, we could add some classes of dogs that are not that similar between each other to improve the diversity.","67dd72b6":"As you can see from below, the performance of the model is very much relying on the pretrained features. The results we get with model being not pretrained are much worse than what we had before.","72961046":"### Imports","14246371":"### PART 2","0d34a5c2":"As expected and as you can see from the graphs below, this modification distorts the model and makes the performance more unstable and unreliable. I have kept original labels in validation dataset and as you can see sometimes performance is normal and sometimes it is very bad, which makes the model very unreliable. The training loss is increasing with epochs, which is also weird.","f994764e":"- Some of the images have other objects in background, which can distort the results. There were dogs playing with a toy in several cases.\n- Also, there are images when a human is present + they are in different age categories (infant, adult, etc.). \n- Some of the photos are taken from close distance and some from long.\n- The lighting may also be one of the issues,since some photos are very dark, while others are very bright.\n- There are cases when several dogs are present in the same photo, which can also hurt the performance of the model. + the dogs can be of the same breed and also of different ones.\n- In some of the photos the background is very similar to the color of the dog.\n- Some of the photos include only faces of dogs, while others the whole silhouette. \n- Some categories include dogs of different ages, which look very differently.","0de8b463":"### Task 2","823dd5b8":"- I believe when we have just faces of dogs, our model should perform better because it will not have confusing backgrounds, other people, objects, dogs, etc. distorting the model. This could potentially result in better results. However, if say there are two dogs of different breeds that have very similar head but different body, this could affect the model negatively. However, since the probability of this happening is very low, in general we can say that having only heads will have a positive effect.\n- Having only background without dogs should influence the model in a very negative way. This should make the classification accuracy go down and the model perform awful.\n- As discussed above, the performance of the model is very much relying on the pretrained features. The results we get with model being not pretrained are much worse than what we had before. So, this is effecting the performance of the model in a very negative way.","ff7fa64b":"I think this edit will distort the results and have a negative effect on the performance of the model.","7c76dbea":"The average image we get for a single class shows us how even the colors of dogs are different for different classes. We can see that mostly dogs are in the center (or at least this is what we can infer from the average image). It is hard to make conclusion about all of the points above, since the average image shows a lot of noise.\n\nSource of code below: [Stackoverflow](https:\/\/stackoverflow.com\/questions\/17291455\/how-to-get-an-average-picture-from-100-pictures-using-pil)","187948e2":"#### Paper 2. Testing and validating machine learning classifiers by metamorphic testing\nThe authors use the concept of \"metamorphic testing\", which uses properties of functions such that it is possible to predict expected changes to the output for particular changes to the input. The papers starts with discussing the basic concepts of supervised learning and starts describing the approach they use. They utilize the concept of looking for metamorphic relations for supervised learning. The ones I liked most were \"permutation of class labels\" and \"addition of uninformative attributes\". They tested them on kNNs and Naive Bayes Classifier. In conclusion, they mentioned that this is only a testing method, and if there are no violations, one cannot conclude correctness, but if there are violations, then we can say the model is not valid.","376fc80d":"I think that these models should do a better job in distinguishing the dogs, since it can focus on differences between just 2 breeds instead of 3. Let's see what happens indeed:"}}