{"cell_type":{"3447f91d":"code","44387b35":"code","683090a1":"code","a9732ae6":"code","32c8f84b":"code","fe6ccb80":"code","861ea8a8":"code","c708c499":"code","16f5e040":"code","e3b4e29a":"code","e6aa9805":"code","5819f193":"code","de991afa":"code","9bd03b32":"markdown","8c7c3ac1":"markdown"},"source":{"3447f91d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))","44387b35":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')\n\n# set seed\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(0)","683090a1":"import pickle\nimport os\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomImageDataset(Dataset):\n   \n    def __init__(self, csv_file, root_dir, transform=None):\n        self.df = pd.read_csv(root_dir + csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \n        img_name = os.path.join(\n            self.root_dir,\n            str(self.df.iloc[idx]['molecule_name']) + '_' + str(self.df.iloc[idx]['id']) + '.pkl'\n        )\n        \n        with open (img_name, 'rb') as fp:\n            image = pickle.load(fp)\n        \n        for c in range(5):\n            image[c] = np.clip(image[c], 0, 255) \/ 255\n        \n        img = torch.from_numpy(np.array(image))\n        img = img.type(torch.FloatTensor)\n        \n        sample = {'image': img,\n                  'target': self.df.iloc[idx]['scalar_coupling_constant']}\n\n        if self.transform:\n            sample['image'] = self.transform(sample['image'])\n\n        return sample['image'], sample['target']","a9732ae6":"images_path =\"..\/input\/full-images-1jhn\/image_1jhn\/Image_1JHN\/\"\n\nimage_dataset = CustomImageDataset(\n    csv_file='description.csv',\n    root_dir=images_path)","32c8f84b":"from sklearn.model_selection import GroupKFold\ngroup_kfold = GroupKFold(n_splits=5)\n\ndf = pd.read_csv(images_path + 'description.csv', index_col = 0)\ndf.reset_index(drop=True, inplace=True)\n\nX = df[['id', 'molecule_name']].copy()\ny = df['scalar_coupling_constant']\ngroups = df['molecule_name'].unique()\n\nfolds = []\nfor train_idx, valid_idx in group_kfold.split(X, y, X['molecule_name']):\n    folds.append([train_idx, valid_idx])","fe6ccb80":"index_fold = 1","861ea8a8":"# from torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 300\n\n# convert data to a normalized torch.FloatTensor\ntransform = transforms.Compose([\n    transforms.Normalize((0.5, 0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5, 0.5))\n    ])\n\ntrain_data = CustomImageDataset(\n    csv_file='description.csv',\n    root_dir = images_path,\n#     transform=transform\n)\n\ntrain_idx, valid_idx = folds[index_fold][0], folds[index_fold][1]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data,\n                                           batch_size=batch_size,\n                                           sampler=train_sampler,\n                                           num_workers=num_workers\n                                          )\nvalid_loader = torch.utils.data.DataLoader(train_data,\n                                           batch_size=batch_size, \n                                           sampler=valid_sampler,\n                                           num_workers=num_workers\n                                          )","c708c499":"class CNN(nn.Module):\n    \"\"\"CNN.\"\"\"\n\n    def __init__(self):\n        \"\"\"CNN Builder.\"\"\"\n        super(CNN, self).__init__()\n\n        self.conv_layer = nn.Sequential(\n\n            # Conv Layer block 1\n            nn.Conv2d(in_channels=5, out_channels=32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Conv Layer block 2\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout2d(p=0.05),\n\n            # Conv Layer block 3\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n\n        self.fc_layer = nn.Sequential(\n            nn.Dropout(p=0.3),\n            nn.Linear(4096, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.3),\n            nn.Linear(512, 1)\n        )\n\n    def forward(self, x):\n        x = self.conv_layer(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_layer(x)\n        return x","16f5e040":"# create a complete CNN\nmodel = CNN()\nprint(model)\n\n# move tensors to GPU if CUDA is available\nif train_on_gpu:\n    model.cuda()","e3b4e29a":"import torch.optim as optim\n\n# specify loss function (categorical cross-entropy)\ncriterion = nn.SmoothL1Loss()\n\n# specify optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.0003)","e6aa9805":"# number of epochs to train the model\nimport time\nn_epochs = 500\nloss_per_iter = []\nstart_time = time.time()\n\nvalid_loss_min = np.Inf # track change in validation loss\n\noutput_file_nb = 0\nmax_output_file_nb = 50\n\nfor epoch in range(1, n_epochs+1):\n\n    if (time.time() - start_time) \/ 3600 > 1:\n        output_file_nb += 1\n        print('Last iteration ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min, valid_loss))\n        torch.save(model.state_dict(), 'model_last.pt')\n        break\n    \n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n    for ind, (data, target) in enumerate(train_loader):\n        print(ind, end='\\r')\n        \n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        \n        optimizer.zero_grad()\n        output = model(data)\n        \n        loss = criterion(output.view(data.shape[0]), target.float())\n        loss.backward()\n        optimizer.step()\n    \n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n    # validate the model #\n    ######################\n    model.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)    \n        loss = criterion(output.view(data.shape[0]), target.float())    \n        valid_loss += loss.item() * data.size(0)\n \n    # calculate average losses\n    train_loss = train_loss \/ len(train_loader.sampler)\n    valid_loss = valid_loss \/ len(valid_loader.sampler)\n        \n    # print training\/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    loss_per_iter.append([train_loss, valid_loss])\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        output_file_nb += 1\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), f'model_best.pt')\n        valid_loss_min = valid_loss\n        \n        if epoch > 20:\n            output_file_nb += 1\n            torch.save(model.state_dict(), f'model_t{round(train_loss, 3)}_v{round(valid_loss, 3)}_ep{epoch}.pt')\n        \n    elif valid_loss <= 1.1 * valid_loss_min and epoch > 20:\n        output_file_nb += 1\n        print('Validation loss saved at ({:.6f}).  Saving model ...'.format(valid_loss))\n        torch.save(model.state_dict(), f'model_t{round(train_loss, 3)}_v{round(valid_loss, 3)}_ep{epoch}.pt')\n    \n    if output_file_nb > max_output_file_nb: break","5819f193":"import matplotlib.pyplot as plt\nplt.figure(figsize=(14,8))\nplt.plot(np.array(loss_per_iter)[:, 0], 'o-', label = 'train')\nplt.plot(np.array(loss_per_iter)[:, 1], 'o-', label = 'valid')\nplt.legend()\nplt.show()","de991afa":"print(np.array(loss_per_iter)[:, 0].min())\nprint(np.array(loss_per_iter)[:, 1].min())","9bd03b32":"## 2. Cross validation","8c7c3ac1":"### 1. Dataloader\n source: https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html"}}