{"cell_type":{"9afb0b95":"code","5e963213":"code","ec985fdd":"code","50dd6aa9":"code","d2f83754":"code","ee3f4936":"code","13cfe177":"code","31761480":"code","057aedd4":"code","40e4befe":"code","c44cbf8c":"code","816f3008":"code","da846719":"code","6e1080e1":"code","1e12a2b7":"code","fad26a62":"code","a567bd75":"markdown"},"source":{"9afb0b95":"!pip install kaggle_environments","5e963213":"from kaggle_environments import evaluate, make\nimport numpy as np\nimport gym\nimport random\nfrom random import choice\nfrom tqdm import tqdm","ec985fdd":"env = make(\"connectx\")\nenv.render()","50dd6aa9":"class QTable:\n    def __init__(self, action_space):\n        self.table = dict()\n        self.action_space = action_space\n        \n    def add_item(self, state_key):\n        self.table[state_key] = list(np.zeros(self.action_space.n))\n    \n    def __call__(self, state):\n        board = state.board[:] # Get a copy\n        board.append(state.mark)\n        state_key = np.array(board).astype(str)\n        state_key = hex(int(''.join(state_key), 3))[2:]\n        if state_key not in self.table.keys():\n            self.add_item(state_key)\n            \n        return self.table[state_key]\n    ","d2f83754":"# Environment parameters\ncols = 7\nrows = 6\n\naction_space = gym.spaces.Discrete(cols)\nobservation_space = gym.spaces.Discrete(cols * rows)","ee3f4936":"# configure hyper-parameters\nalpha =  0.1\ngamma = 0.6\nepsilon = 0.99\nmin_epsilon = 0.1\n\nepisodes = 15000\nalpha_decay_step = 1000\nalpha_decay_rate = 0.9\nepsilon_decay_rate = 0.9999","13cfe177":"q_table = QTable(action_space)\ntrainer = env.train([None, \"negamax\"])\n\nall_epochs = []\nall_total_rewards = []\nall_avg_rewards = []\nall_q_table_rows = []\nall_epsilons = []\n\nfor i in tqdm(range(episodes)):\n    state = trainer.reset()\n    epsilon = max(min_epsilon, epsilon * epsilon_decay_rate)\n    epochs, total_rewards = 0, 0\n    done = False\n    \n    while not done:\n        if random.uniform(0, 1) < epsilon:\n            action = choice([c for c in range(action_space.n) if state.board[c] == 0])\n        \n        else:\n            row = q_table(state)[:]\n            selected_items = []\n            for j in range(action_space.n):\n                if state.board[j] == 0:\n                    selected_items.append(row[j])\n                else:\n                    selected_items.append(-1e7)\n            action = int(np.argmax(selected_items))\n                \n        next_state, reward, done, info = trainer.step(action)\n        \n        # apply new rules\n        if done:\n            if reward == 1:\n                reward = 20\n            elif reward == 0:\n                reward = -20\n            else:\n                reward = 10\n                \n        else:\n            reward = -0.05\n        \n        old_value = q_table(state)[action]\n        next_max = np.argmax(q_table(next_state))\n        \n        # update q value\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        q_table(state)[action] = new_value\n        \n        state = next_state\n        epochs += 1\n        total_rewards += reward\n    \n    all_epochs.append(epochs)\n    all_total_rewards.append(total_rewards)\n    avg_rewards = np.mean(all_total_rewards[max(0, i - 100) : (i + 1)])\n    all_avg_rewards.append(avg_rewards)\n    all_q_table_rows.append(len(q_table.table))\n    all_epsilons.append(epsilon)\n    \n    if (i +1) % alpha_decay_step == 0:\n        alpha += alpha_decay_rate\n    ","31761480":"tmp_dict_q_table = q_table.table.copy()\ndict_q_table = dict()\n\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))","057aedd4":"def my_agent(observation, configuration):\n    from random import choice\n    \n    q_table = dict_q_table\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n    \n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    action = q_table[state_key]\n    \n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    return action","40e4befe":"# Run against negamax\nenv.reset()\nenv.run([my_agent, \"negamax\"])\nenv.render(mode=\"ipython\")","c44cbf8c":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    # env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","816f3008":"agent = \"\"\"def my_agent(observation, configuration):\n    from random import choice\n    \n    q_table = \"\"\"+ str(dict_q_table).replace(\" \", \"\") +\"\"\"\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n    \n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    action = q_table[state_key]\n    \n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    return action \"\"\"","da846719":"with open(\"submission.py\", 'w') as f:\n    f.write(agent)","6e1080e1":"# import saved agent\nfrom submission import my_agent","1e12a2b7":"# Evaluate saved agent\ndef mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","fad26a62":"# Play the agent with human\nenv.play([my_agent, None], width=500, height=450)","a567bd75":"This is a copy of [Hieu Phung](https:\/\/www.kaggle.com\/phunghieu\/connectx-with-q-learning)\n.He used gym to train the agent but am using kaggle environment"}}