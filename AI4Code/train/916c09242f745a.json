{"cell_type":{"b8d80cd8":"code","e4bd0fac":"code","6f76a108":"code","50b30592":"code","16414285":"code","1c52ec15":"code","b54ddd35":"code","01c91dc8":"code","f2ef7d54":"code","0356da70":"code","b9dd966b":"markdown","c8c9c2dd":"markdown","483b70d5":"markdown"},"source":{"b8d80cd8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4bd0fac":"data = pd.read_csv('\/kaggle\/input\/ipo-history-data\/ipo_data.csv')","6f76a108":"data.head()","50b30592":"data['target'] = data['Listing  Gains(%)'].apply(lambda x: 1 if x>0 else 0)\ndata.head()","16414285":"data.columns","1c52ec15":"df = data[['Issue Size (in crores)', 'QIB', 'HNI','RII', 'Total', 'Issue','target']]\ndf.head()","b54ddd35":"from sklearn.preprocessing import StandardScaler\nss= StandardScaler()\ntrain_X = ss.fit_transform(df.drop('target',axis=1))\ntrain_Y = df['target']","01c91dc8":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","f2ef7d54":"models = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n    cv_results = cross_val_score(model, train_X, train_Y, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","0356da70":"from matplotlib import pyplot as plt\nplt.boxplot(results, labels=names)\nplt.title('Algorithm Comparison')\nplt.show()","b9dd966b":"It would be advantageous to apply for an IPO with the motive of earning some listing gain, only if the Listing Gain is +ve. So, we construct a target variable which is 1 when the listing gain is positive and 0 otherwise","c8c9c2dd":"* Dropping 'Date' and 'Profile' for now to keep the example simple.\n* Dropping these columns as they are not relevant to this task -'Listing Open', 'Listing Close','Listing  Gains(%)', 'CMP', 'Current Gains (%)', ","483b70d5":"As we can see, we are able to predict with an accuracy of ~67%. Which is not bad compared to the random prediction of 50%. \n\nThis can be improved even more with:\n1. feature engineering - using the links provided in the \"Profile\" column to accumulate even more data about the IPO\n2. time series analysis - using the \"Date\" column "}}