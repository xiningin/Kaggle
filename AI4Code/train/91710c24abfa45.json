{"cell_type":{"33ff113d":"code","7b959fa8":"code","04245674":"code","40702d42":"code","35d49e70":"code","cbb52e77":"code","a3a15da2":"code","0d2b090d":"code","380010ec":"code","e69deb0c":"code","28cd9be3":"code","0b0da7b4":"code","2bb2545f":"code","abb3626e":"code","20c7a9ab":"code","e212ec61":"code","4a23c61a":"code","33ffdba7":"code","09e9218e":"code","1f40b9f7":"code","2689e76b":"code","73102fad":"code","73bcc6c2":"code","746c1bc0":"code","fa6330d3":"code","b05fd69c":"code","79c18505":"code","554c2f54":"code","aa7b4283":"code","54ee88c9":"code","8a4f7807":"code","a3e1965c":"code","77e4fe22":"code","43621d35":"code","72714b06":"code","6f2af857":"code","d61e44a5":"code","aaa8d6e9":"code","7aa0de0c":"code","24aec48a":"code","6624188a":"code","9c42f865":"code","43723602":"code","af584fc0":"code","309a831d":"code","bc0d7d15":"code","f9c1c371":"code","2b4c522a":"code","0f48ca83":"code","0815952d":"code","89c78704":"code","c7f434bc":"code","cb17c0bc":"code","1dfdf959":"code","3f38240b":"code","f88e2a3a":"code","b62a48eb":"code","6996d196":"markdown","89b3de4b":"markdown","68d2f0ba":"markdown","7d1022fb":"markdown","00e098b1":"markdown","d937ebb2":"markdown","7a03fcfa":"markdown","5527515e":"markdown","8c1fdc21":"markdown","821bc921":"markdown","b6352b55":"markdown","74f59e18":"markdown","320935b4":"markdown","0edfb6bf":"markdown","87aa6d21":"markdown","a577aeaf":"markdown","42f4b4ff":"markdown","73dcd722":"markdown","18fe7e68":"markdown","ace2a147":"markdown","549b9398":"markdown","312fe334":"markdown","e4a0d702":"markdown","0851ef7c":"markdown","40c48bbb":"markdown","997f5bf3":"markdown"},"source":{"33ff113d":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n%matplotlib inline\nsns.set_style(\"darkgrid\")","7b959fa8":"#reading the csv file\ndata = pd.read_csv(\"..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")","04245674":"#data description\ndata.describe()","40702d42":"#checking the first 5 rows\ndata.head()","35d49e70":"#making the serial number the index\ndata = data.set_index(\"Serial No.\")","cbb52e77":"#checking the last few lines\ndata.tail()","a3a15da2":"#checking the shape of the dataframe and the columns\nprint(data.shape)\nprint()\nprint(data.columns)","0d2b090d":"#renaming the columns\ndata = data.rename(columns = {'Serial No.' : 'Sno', 'GRE Score':'GRE', 'TOEFL Score':'TOEFL', 'University Rating':'UniRating', 'Chance of Admit':'Chance'})","380010ec":"#checking the first 5 rows again\ndata.head()","e69deb0c":"#checking the null values\ndata.isnull().sum()","28cd9be3":"#checking the information of the dataframe once again\ndata.info()","0b0da7b4":"#we need to change the dtype of research as category\ndata['Research'] = data['Research'].astype('category')\ndata = data.rename(columns={'Chance of Admit ' : 'Chance'})","2bb2545f":"#checking the dtypes now\ndata.info()","abb3626e":"sns.distplot(data['GRE'])\nplt.title(\"Distribution of the GRE scores.\")","20c7a9ab":"sns.distplot(data['TOEFL'])\nplt.title(\"Distribution of the TOEFL scores.\")","e212ec61":"sns.distplot(data['CGPA'])\nplt.title(\"Distribution of the CGPAs.\")","4a23c61a":"sns.catplot(x='Research', data=data, kind='count')\nplt.title(\"Number of students who have done some research.\")","33ffdba7":"#Now we are interested in relationships between different variables","09e9218e":"f, ax = plt.subplots(figsize=(5,5), dpi=200)\nax = sns.kdeplot(data.CGPA, data.GRE, cmap=\"Reds\", shade=True)\nplt.title(\"CGPA vs GRE Scores\")","1f40b9f7":"f, ax = plt.subplots(figsize=(5,5), dpi=200)\nax = sns.kdeplot(data.CGPA, data.TOEFL, cmap=\"Blues\", shade=True)\nplt.title(\"CGPA vs TOEFL Scores\")","2689e76b":"data = data.rename(columns={'LOR ':'LOR'})\nax = sns.jointplot(data.SOP, data.LOR,  kind='reg')","73102fad":"data.columns","73bcc6c2":"data.head()\nx = data.loc[:,:]","746c1bc0":"f, ax = plt.subplots(figsize=(10,10), dpi = 300)\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(x.corr(), annot=True, fmt='.3g', center=0, linewidths=0, cbar=True, square=True)\nplt.title(\"Correlation of the variables with themselves\")","fa6330d3":"#importing necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX = data.drop(['Chance'], axis=1)\ny = data.Chance\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n","b05fd69c":"#lets do linear regression\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprediction = model.predict(X_test)\nprint(\"Linear Regression mean squared error : \", np.sqrt(mean_squared_error(y_test, prediction)))\nprint(\"Linear Regression R^2 score : \", model.score(X_test, y_test))\n\nx = np.arange(1,101)\nf, ax = plt.subplots(figsize=(5,5), dpi=200)\nsns.scatterplot(x=x, y=y_test, color='r', alpha=0.3)\nsns.regplot(x=x, y=prediction, color='b')\nplt.title(\"Linear Regression\")","79c18505":"#now we take a look at Ridge Regression\nfrom sklearn.linear_model import Ridge\n\nmaxa = 0\nmaxs = 0\nmina = 0\nmins = 1\n\n\nl = np.arange(0,10,0.01)\nscorel = []\nfor i in l:\n    model = Ridge(alpha=i)\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    scorel.append(score)\n    if score > maxs:\n        maxs = score\n        maxa = i\n    if score < mins:\n        mins = score\n        mina = i\n\nmodel = Ridge(alpha=1)\nmodel.fit(X_train, y_train)\nprediction = model.predict(X_test)\nprint(\"Alpha of the Ridge : \", model.alpha)\nprint(\"Ridge Regression mean squared error : \", np.sqrt(mean_squared_error(y_test, prediction)))\nprint(\"Ridge Regression R^2 score : \", model.score(X_test, y_test))\npredAlpha1 = prediction\nprint(\"-\"*70)\n\nmodel = Ridge(alpha=mina)\nmodel.fit(X_train, y_train)\nprediction = model.predict(X_test)\nprint(\"Alpha of the Ridge (worst alpha) : \", model.alpha)\nprint(\"Ridge Regression mean squared error : \", np.sqrt(mean_squared_error(y_test, prediction)))\nprint(\"Ridge Regression R^2 score : \", model.score(X_test, y_test))\npredAlphaWorst = prediction\nprint(\"-\"*70)\n\nmodel = Ridge(alpha=maxa)\nmodel.fit(X_train, y_train)\nprediction = model.predict(X_test)\nprint(\"Alpha of the Ridge (best alpha) : \", model.alpha)\nprint(\"Ridge Regression mean squared error : \", np.sqrt(mean_squared_error(y_test, prediction)))\nprint(\"Ridge Regression R^2 score : \", model.score(X_test, y_test))\npredAlphaBest = prediction","554c2f54":"f, ax = plt.subplots(figsize=(20,5), dpi = 300)\nplt.plot(predAlpha1, label='Prediction for Alpha = Unity', marker='o')\nplt.plot(predAlphaBest, label='Prediction for Alpha = Best', marker='_' )\nplt.plot(predAlphaWorst, label='Prediction for Alpha = Worst', marker='.')\nplt.legend()\nplt.title(\"Ridge Regreesion\")\nplt.show()","aa7b4283":"f, ax = plt.subplots(figsize=(15,10), dpi=300)\nsns.scatterplot(x=l, y=scorel, marker='+')\nplt.title(\"Alpha vs R^2 score for Ridge Regression\")","54ee88c9":"f, ax = plt.subplots(figsize=(15,10), dpi=300)\nsns.scatterplot(x=x, y=y_test, color='r', alpha=0.3)\nsns.regplot(x=x, y=prediction, color='b')\nplt.title(\"Ridge Regression\")","8a4f7807":"#now we do lasso regression\nfrom sklearn.linear_model import Lasso\nx = data\n\nmaxa = 0\nmaxs = 0\nmina = 0\nmins = 1\n\n\nl = np.arange(0.1,10,0.01)\nscorel = []\nfor i in l:\n    model = Lasso(alpha=i)\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    scorel.append(score)\n    if score > maxs:\n        maxs = score\n        maxa = i\n    if score < mins:\n        mins = score\n        mina = i\n\nmodel = Lasso(alpha=1)\nmodel.fit(X_train, y_train)\nprediction = model.predict(X_test)\nprint(\"Coefficients of the Lasso model : \", model.coef_)\nprint(\"Lasso Regression mean squared error : \", np.sqrt(mean_squared_error(y_test, prediction)))\nprint(\"Lasso Regression R^2 score : \", model.score(X_test, y_test))\npredAlpha1 = prediction\nprint(\"-\"*70)\n\nmodel = Lasso(alpha=mina)\nmodel.fit(X_train, y_train)\nprediction = model.predict(X_test)\nprint(\"Coefficients of the Lasso model (worst score) : \", model.coef_)\nprint(\"Lasso Regression mean squared error : \", np.sqrt(mean_squared_error(y_test, prediction)))\nprint(\"Lasso Regression R^2 score : \", model.score(X_test, y_test))\npredAlphaWorst = prediction\nprint(\"-\"*70)\n\nmodel = Lasso(alpha=maxa)\nmodel.fit(X_train, y_train)\nprediction = model.predict(X_test)\nprint(\"Coefficients of the Lasso model (best score) : \", model.coef_)\nprint(\"Lasso Regression mean squared error : \", np.sqrt(mean_squared_error(y_test, prediction)))\nprint(\"Lasso Regression R^2 score : \", model.score(X_test, y_test))\npredAlphaBest = prediction\n","a3e1965c":"f, ax = plt.subplots(figsize=(20,5), dpi = 300)\nplt.plot(predAlpha1, label='Prediction for Default Coefficients', marker='o')\nplt.plot(predAlphaBest, label='Prediction for Best Coefficients', marker='_' )\nplt.plot(predAlphaWorst, label='Prediction for Worst Coefficients', marker='.')\nplt.legend()\nplt.title(\"Lasso Regression\")\nplt.show()","77e4fe22":"f, ax = plt.subplots(figsize=(15,10), dpi=300)\nsns.scatterplot(x=l, y=scorel, marker='+')\nplt.title(\"Alpha vs R^2 score for Lasso Regression\")","43621d35":"data.describe()","72714b06":"#Now we do cross validation for linear regression \nfrom sklearn.model_selection import cross_val_score\n\nmodel = LinearRegression()\nX = data.loc[:,:'CGPA']\ny = data['Chance']\nk = 5\n\ncv_res = cross_val_score(model, X, y, cv=k)\nprint(\"Cross-validation scores : \", cv_res)\nprint(\"Cross-validation score average : \", np.sum(cv_res)\/k)","6f2af857":"#now we make a column in the dataframe that has categorical data\n#the categorical data will be chance of getting an admit","d61e44a5":"data.describe()","aaa8d6e9":"#list comprehension for the new column\ndata['ChanceCat'] = ['Very likely' if i > 0.82 else 'Likely' if i > 0.72 else 'Somewhat likely' if i > 0.63 else 'Not likely' for i in data['Chance']]","7aa0de0c":"#checking if the changes took place\ndata[['Chance','ChanceCat']].head()","24aec48a":"#changing the data type of the newly created column as categorical data\ndata['ChanceCat'] = data['ChanceCat'].astype('category')","6624188a":"#plotting the ChanceCat\nf, ax = plt.subplots(figsize=(8,6), dpi=200)\nsns.boxenplot(x='ChanceCat', y='Chance', data=data, order=['Not likely', 'Somewhat likely', 'Likely', 'Very likely'])\nplt.ylabel(\"Probability\")\nplt.title(\"Chance Distribution of getting admitted\")","9c42f865":"#Let's do logistic regression on the newly created categorical data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_curve, confusion_matrix, classification_report","43723602":"data['ChanceInt'] = [1 if i=='Likely' else 0 if i=='Not likely' else 0 if i=='Somewhat likely' else 1 for i in data['ChanceCat']]\ndata['ChanceInt'] = data['ChanceInt'].astype('category')\nX = data.loc[:,'GRE':'Research']\nX = np.array(X)\nX = preprocessing.scale(X)\ny = data['ChanceInt']","af584fc0":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\nf, ax = plt.subplots(figsize=(7,7), dpi=250)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()\nprint(\"R^2 score : \", model.score(X_test, y_test))","309a831d":"X = data.loc[:,:'Chance']\ny = data['ChanceCat']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ny_prediction = model.predict_proba(X_test)\ny_test_dummies = pd.get_dummies(y_test,drop_first=False).values\n\nfpr = dict()\ntpr = dict()\nthresholds = dict()\n\nfor i in range(4):\n    fpr[i],tpr[i],thresholds[i] = roc_curve(y_test_dummies[:,i],y_prediction[:,i])\n\nf, ax = plt.subplots(figsize=(10,7), dpi=200)    \nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr[0],tpr[0])\nplt.plot(fpr[1],tpr[1])\nplt.plot(fpr[2],tpr[2])\nplt.plot(fpr[3],tpr[3])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()\nprint(\"R^2 score : \", model.score(X_test, y_test))","bc0d7d15":"#Now let's take a look at KNN","f9c1c371":"data.columns\n#the columns ChanceCat and ChanceInt are redundant values\n#we should drop the column ChanceInt to save memory\ndata.drop('ChanceInt', axis=1)","2b4c522a":"#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nX = data.loc[:,'GRE':'Research']\ny = data['ChanceCat']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state=1)\n\ntrain_score = []\ntest_score = []\nk = np.arange(1,50)\nfor i in range(1,50):\n    knn =KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    train_score.append(knn.score(X_train, y_train))\n    test_score.append(knn.score(X_test, y_test))\n    \nf, ax = plt.subplots(figsize=(10,10), dpi=250)\nsns.lineplot(x=k, y=train_score, label='Train Scores')\nsns.lineplot(x=k, y=test_score, label ='Test Scores')\nplt.legend()\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Score\")\nplt.title(\"Number of neighbours vs Train Test scores\")\n\nprint(\"Best accuracy is {} with k = {}\".format(np.max(test_score), 1+test_score.index(np.max(test_score))))","0f48ca83":"#appling cross-validation in KNN\n\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(X_train,y_train)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","0815952d":"f,ax = plt.subplots(figsize=(5,5), dpi=200)\nsns.stripplot(x='ChanceCat', y='Chance', data=data, dodge=True, alpha=0.25, zorder=1, order=['Not likely', 'Somewhat likely', 'Likely', 'Very likely'])\nsns.pointplot(x='ChanceCat', y='Chance', data=data, dodge=.523, join=False, palette='dark', markers='d', scale=.75, ci=None, order=['Not likely', 'Somewhat likely', 'Likely', 'Very likely'])\nplt.title(\"Chances of getting admitted\")\n","89c78704":"data.head()","c7f434bc":"l = np.arange(1,501)\nf, ax = plt.subplots(figsize=(10,10), dpi=300)\nsns.scatterplot(l,'Chance', data=data, hue='ChanceCat', palette=sns.diverging_palette(145, 280, s=85, l=25, n=4))\nplt.title(\"Distribution of the Chances\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Probability\")\n#color palette = sns.diverging_palette(145, 280, s=85, l=25, n=7)\n#sns.cubehelix_palette(4, start=2, rot=0, dark=0, light=.85, reverse=True)","cb17c0bc":"#Generating feature importance\nfrom sklearn.ensemble import RandomForestRegressor\n\nclassifier = RandomForestRegressor()\nX = data.loc[:,'GRE':'Research']\ny = data.loc[:,'Chance']\n\nclassifier.fit(X,y)\nfeature_names = X.columns\nimpFrame = pd.DataFrame()\nimpFrame['Features'] = X.columns\nimpFrame['Importance'] = classifier.feature_importances_\nimpFrame = impFrame.sort_values(by=['Importance'], ascending=True)","1dfdf959":"f, ax = plt.subplots(figsize=(10,10), dpi=200)\nplt.barh([1,2,3,4,5,6,7], impFrame['Importance'], align='center', alpha=0.5)\nplt.yticks([1,2,3,4,5,6,7], impFrame['Features'])\nplt.xlabel('Importance')\nplt.title('Feature Importances')\nplt.show()","3f38240b":"X = data.loc[:,'GRE':'Chance']\nsns.pairplot(X, hue='Chance', palette=sns.dark_palette(\"purple\"))","f88e2a3a":"X = data.loc[:,:'ChanceCat']\nsns.pairplot(X, hue='ChanceCat', hue_order=['Very likely','Likely','Somewhat likely','Not likely'], palette=sns.diverging_palette(145, 280, s=85, l=25, n=8))","b62a48eb":"print(impFrame)","6996d196":"Performing some exploratory data analysis","89b3de4b":"Cross-validating in KNN method","68d2f0ba":"We saw that students with variety of marks have applied for admissions.","7d1022fb":"# Now we implement ML algorithms","00e098b1":"Topics covered are : Linear Regression, Lasso Regression, Ridge Regression, Logistic Regression with ROC Curves, KNeighborsClassifier and RandomForestRegressor","d937ebb2":"Linear Regression","7a03fcfa":"### :) ","5527515e":"In the above graph, the blue line is regression line plotted by regplot() of seaborn. The red dots are true values while blue ones are predicted!","8c1fdc21":"Visualizing some distributions to better understand the data","821bc921":"Significance of the features","b6352b55":"Let's continue with the visualizations!","74f59e18":"Relationship between various features","320935b4":"<table>\n    <tr>\n        <th>Model Name<\/th>\n        <th>R^2 Score<\/th>\n    <\/tr>\n    <tr>\n        <td>Linear Regression<\/td><td>0.898286909853386 <\/td>\n    <\/tr>\n    <tr>\n        <td>Linear Regression with 5-fold Cross Validation<\/td><td>0.8090109217125606<\/td>\n    <\/tr>\n    <tr>\n        <td>Ridge Regression wih alpha 1.45<\/td><td>0.8984857987897976<\/td>\n    <\/tr>\n    <tr>\n        <td>Lasso Regression with coefficients [0.00728218 0.00436988 0 0 0 0 0]<\/td><td>0.7344731754961297<\/td>\n    <\/tr>\n    <tr>\n        <td>Logistic Regression for 2 output variables<\/td><td>0.91<\/td>\n    <\/tr>\n    <tr>\n        <td>Logistic Regression for 4 output variables(1 vs All)<\/td><td>0.55<\/td>\n    <\/tr>\n    <tr>\n        <td>K Nearest Neighbours for k=24<\/td><td>0.67<\/td>\n    <\/tr>\n    <tr>\n        <td>K Nearest Neighbours using tuned hyperparameter crossvalidation for k=43<\/td><td>0.6174017132383197<\/td>\n    <\/tr>\n<\/table>  ","0edfb6bf":"In the above graph, the blue line is regression line plotted by regplot() of seaborn. The red dots are true values while blue ones are predicted!","87aa6d21":"## Thank you! ","a577aeaf":"### Suggestions are welcomed!","42f4b4ff":"Lasso Regression","73dcd722":"# Supervised Learning","18fe7e68":"Importin dependencies","ace2a147":"##### Results:","549b9398":"K Nearest Neighbors using KNighborsClassifier","312fe334":"Finding out the significance of features","e4a0d702":"Dataset available in the repo. \nAlso, here is a link to that dataset : https:\/\/www.kaggle.com\/mohansacharya\/graduate-admissions","0851ef7c":"Logistic Regression","40c48bbb":"Let's perform cross validation on splitting for Linear Regression method. ","997f5bf3":"Ridge Regression"}}