{"cell_type":{"a0549044":"code","6928ba09":"code","bf66603c":"code","9820c7b5":"code","b170c3fb":"code","24648c40":"code","55b79406":"code","199d0018":"code","c37a529e":"code","ac94b423":"code","3e0f498a":"code","2f98d796":"code","edb424f2":"code","7a8fd343":"code","0d901afc":"code","38211fa3":"code","a67c4717":"code","3b5e9568":"code","4961321a":"code","ddcb2a9d":"code","9b2dea48":"markdown","8a4dcb8e":"markdown","93058fd9":"markdown","69c6d0db":"markdown","bd681433":"markdown"},"source":{"a0549044":"import tensorflow as tf","6928ba09":"# import os\n# import pprint\n# import tensorflow as tf\n\n# if 'COLAB_TPU_ADDR' not in os.environ:\n#   print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n# else:\n#   tpu_address = 'grpc:\/\/' + os.environ['COLAB_TPU_ADDR']\n#   print ('TPU address is', tpu_address)\n\n#   with tf.Session(tpu_address) as session:\n#     devices = session.list_devices()\n    \n#   print('TPU devices:')\n#   pprint.pprint(devices)","bf66603c":"###=============== Import necessary Libraries ==================\nimport os\nimport csv\nimport json\nimport string\nimport keras\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nfrom math import floor\nimport spacy\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nimport time\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Concatenate, Add, Flatten, CuDNNLSTM\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.engine.topology import Layer\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import defaultdict\n\n\n# keras libraries\nfrom keras.models import Model, load_model,Sequential\nfrom keras.layers import Dense, Input, Dropout,Bidirectional, GRU, Activation, concatenate, Embedding, SpatialDropout1D\nfrom keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D ,GlobalMaxPool1D, GlobalAvgPool1D, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence, text\nfrom keras import layers\n\n","9820c7b5":"train = pd.read_csv('..\/input\/gapdevelopment\/repository\/google-research-datasets-gap-coreference-83135f2\/gap-development.tsv',delimiter='\\t',encoding='utf-8')\ntest = pd.read_csv('..\/input\/gapdevelopment\/repository\/google-research-datasets-gap-coreference-83135f2\/gap-test.tsv',delimiter='\\t',encoding='utf-8');\nvalidation = pd.read_csv('..\/input\/gapdevelopment\/repository\/google-research-datasets-gap-coreference-83135f2\/gap-validation.tsv',delimiter='\\t',encoding='utf-8');\n\ntrain.shape\n","b170c3fb":"test.head()","24648c40":"validation.head()","55b79406":"true_B = train.loc[train['B-coref']== True ]\ntrue_B.drop('A',axis=1,inplace=True)\ntrue_B.drop('A-offset',axis=1,inplace=True)\ntrue_B.drop('A-coref',axis=1,inplace=True)\ntrue_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\ntrue_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\ntrue_A = train.loc[train['A-coref']== True ]\ntrue_A.drop('B',axis=1,inplace=True)\ntrue_A.drop('B-offset',axis=1,inplace=True)\ntrue_A.drop('B-coref',axis=1,inplace=True)\ntrue_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_A = train.loc[train['A-coref']== False ]\nfalse_A.drop('B',axis=1,inplace=True)\nfalse_A.drop('B-offset',axis=1,inplace=True)\nfalse_A.drop('B-coref',axis=1,inplace=True)\nfalse_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_B = train.loc[train['B-coref']== False ]\nfalse_B.drop('A',axis=1,inplace=True)\nfalse_B.drop('A-offset',axis=1,inplace=True)\nfalse_B.drop('A-coref',axis=1,inplace=True)\nfalse_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\nfalse_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\n","199d0018":"# GERANDO O DATAFRAME DE TREINAMENTO\nframes = [true_A,false_A,true_B,false_B]\nnew_train=pd.concat(frames)\nnew_train.loc[new_train['A-offset']== float('nan')]\nnew_train.dropna(how='all')","c37a529e":"# gerar dataframe de teste\n\ntrue_test_B = test.loc[test['B-coref']== True ]\ntrue_test_B.drop('A',axis=1,inplace=True)\ntrue_test_B.drop('A-offset',axis=1,inplace=True)\ntrue_test_B.drop('A-coref',axis=1,inplace=True)\ntrue_test_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\ntrue_test_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\ntrue_test_A = test.loc[test['A-coref']== True ]\ntrue_test_A.drop('B',axis=1,inplace=True)\ntrue_test_A.drop('B-offset',axis=1,inplace=True)\ntrue_test_A.drop('B-coref',axis=1,inplace=True)\ntrue_test_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_test_A = test.loc[test['A-coref']== False ]\nfalse_test_A.drop('B',axis=1,inplace=True)\nfalse_test_A.drop('B-offset',axis=1,inplace=True)\nfalse_test_A.drop('B-coref',axis=1,inplace=True)\nfalse_test_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_test_B = test.loc[test['B-coref']== False ]\nfalse_test_B.drop('A',axis=1,inplace=True)\nfalse_test_B.drop('A-offset',axis=1,inplace=True)\nfalse_test_B.drop('A-coref',axis=1,inplace=True)\nfalse_test_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\nfalse_test_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n","ac94b423":"frames = [true_test_A,false_test_A,true_test_B,false_test_B]\nnew_test=pd.concat(frames)\nnew_test.loc[new_test['A-offset']== float('nan')]\nnew_test.dropna(how='all')","3e0f498a":"# gerar dataframe de valida\u00e7\u00e3o\n\ntrue_validation_B = validation.loc[validation['B-coref']== True ]\ntrue_validation_B.drop('A',axis=1,inplace=True)\ntrue_validation_B.drop('A-offset',axis=1,inplace=True)\ntrue_validation_B.drop('A-coref',axis=1,inplace=True)\ntrue_validation_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\ntrue_validation_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\ntrue_validation_A = validation.loc[validation['A-coref']== True ]\ntrue_validation_A.drop('B',axis=1,inplace=True)\ntrue_validation_A.drop('B-offset',axis=1,inplace=True)\ntrue_validation_A.drop('B-coref',axis=1,inplace=True)\ntrue_validation_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_validation_A = validation.loc[validation['A-coref']== False ]\nfalse_validation_A.drop('B',axis=1,inplace=True)\nfalse_validation_A.drop('B-offset',axis=1,inplace=True)\nfalse_validation_A.drop('B-coref',axis=1,inplace=True)\nfalse_validation_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_validation_B = validation.loc[validation['B-coref']== False ]\nfalse_validation_B.drop('A',axis=1,inplace=True)\nfalse_validation_B.drop('A-offset',axis=1,inplace=True)\nfalse_validation_B.drop('A-coref',axis=1,inplace=True)\nfalse_validation_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\nfalse_validation_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)","2f98d796":"frames = [true_validation_A,false_validation_A,true_validation_B,false_validation_B]\nnew_validation=pd.concat(frames)\nnew_validation.loc[new_validation['A-offset']== float('nan')]\nnew_validation.dropna(how='all')","edb424f2":"def dot_product(x, kernel):\n\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n\n      \nclass AttentionWithContext(Layer):\n\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number \u03b5 to the sum.\n        # a \/= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n","7a8fd343":"# NOVA TOKENIZA\u00c7\u00c3O\n\ndef prepare_data(data, label = None, test=False):    \n\n    \n    text = []\n    A = []\n    Pronoun = []\n    distances = []\n    for row in data[['Text','A','Pronoun','A-offset','Pronoun-offset']].values:\n        text.append(row[0])\n        A.append(row[1])\n        Pronoun.append(row[2])\n        distances.append(row[4]-row[3])\n        \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    tokenizer.fit_on_texts(text)\n    texts = tokenizer.texts_to_sequences(text)  \n    texts = pad_sequences(texts, maxlen=maxlen)\n    word_index = tokenizer.word_index\n    prev_text = []\n    \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    A_terms = tokenizer.texts_to_sequences(A)  \n    A_terms = pad_sequences(A_terms, maxlen=100)\n     \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    \n    pronouns = tokenizer.texts_to_sequences(Pronoun)  \n    pronouns = pad_sequences(A_terms, maxlen=100)\n    \n    X = []\n    \n    i = 0 \n    data_examples = len(data)\n    for i in range(0,data_examples):\n        aux = np.append(texts[i],A_terms[i])\n        aux = np.append(aux,pronouns[i])\n        X.append(np.append(aux,distances[i]))\n\n    X = np.asarray(X)\n    print(X.shape)\n    #Y = pd.get_dummies(data[label]).values\n    \n    Y = pd.get_dummies(data[\"A-coref\"].values)\n    if test == True:\n        return X, word_index, tokenizer\n    else:\n        return X, Y, word_index, tokenizer\n    ","0d901afc":"def get_previous_text(data,test=False):\n    \n    text_before_A = []\n    text_before_Pronoun = []\n    for row in data[['Text','A-offset','Pronoun-offset']].values:\n        text_before_A.append(row[0][:row[1]-1])\n        text_before_Pronoun.append(row[0][:row[2]-1])\n        \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    tokenizer.fit_on_texts(text_before_A)\n    text_A = tokenizer.texts_to_sequences(text_before_A)  \n    texts_A = pad_sequences(text_A, maxlen=maxlen)\n    word_index = tokenizer.word_index\n    \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    tokenizer.fit_on_texts(text_before_Pronoun)\n    text_pronoun = tokenizer.texts_to_sequences(text_before_Pronoun)  \n    text_pronoun = pad_sequences(text_pronoun, maxlen=maxlen)\n    word_index = tokenizer.word_index\n    \n    X = []\n    \n    i = 0 \n    data_examples = len(data)\n    for i in range(0,data_examples):\n        aux = np.append(text_pronoun[i],texts_A[i])\n        X.append(aux)\n\n    X = np.asarray(X)\n    #Y = pd.get_dummies(data[label]).values\n    \n    Y = pd.get_dummies(data[\"A-coref\"].values)\n    if test == True:\n        return X, word_index, tokenizer\n    else:\n        return X, Y, word_index, tokenizer\n    \n","38211fa3":"train_X, y, word_index, tokenizer = prepare_data(train)\ntrain_auX = train_X\nX_test, Y_test, word_index, tokenizer = prepare_data(test)\ntest_auX =  X_test\nvalidation_X, y_validation, word_index, tokenizer = prepare_data(validation)\nvalidation_auX = validation_X","a67c4717":"# Tokeniza\u00e7\u00e3o \nmaxlen = 220\nembed_size = 500\nmax_features = 7000\n\n# treinamento\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer_list = list(train.Text.values)\ntokenizer.fit_on_texts(tokenizer_list)\n\ntrain_X = tokenizer.texts_to_sequences(train.Text.values)\ntrain_auX = tokenizer.texts_to_sequences(train.Text.values)\n\ntrain_X = pad_sequences(train_X, maxlen=maxlen)\ntrain_auX = pad_sequences(train_auX, maxlen=maxlen)\n\ny = pd.get_dummies(train[\"A-coref\"].values)\nword_index = tokenizer.word_index\nmax_features = len(word_index)\n\n# Valida\u00e7\u00e3o\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer_list = list(validation.Text.values)\ntokenizer.fit_on_texts(tokenizer_list)\n\nvalidation_X = tokenizer.texts_to_sequences(validation.Text.values)\nvalidation_auX = tokenizer.texts_to_sequences(validation.Text.values)\n\nvalidation_X = pad_sequences(validation_X, maxlen=maxlen)\nvalidation_auX = pad_sequences(validation_auX, maxlen=maxlen)\n\ny_validation = pd.get_dummies(validation[\"A-coref\"].values)\nword_index = tokenizer.word_index\n\n\n# Teste\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer_list = list(test.Text.values)\ntokenizer.fit_on_texts(tokenizer_list)\n\ntest_X = tokenizer.texts_to_sequences(test.Text.values)\ntest_auX = tokenizer.texts_to_sequences(test.Text.values)\n\ntest_X = pad_sequences(test_X, maxlen=maxlen)\ntest_auX = pad_sequences(test_auX, maxlen=maxlen)\n\ny_test = pd.get_dummies(test[\"A-coref\"].values)\nword_index = tokenizer.word_index\n\n","3b5e9568":"def lstm_attention():\n  inp1 = Input(shape=(maxlen,))\n  inp2 = Input(shape=(maxlen,))\n\n  model1_out = Embedding(max_features, embed_size)(inp1)\n  model1_out = Bidirectional(LSTM(256, return_sequences=True))(model1_out)\n  model1_out = AttentionWithContext()(model1_out)\n  model1_out = Dropout(0.1)(model1_out)\n\n  model2_out = Embedding(max_features, embed_size)(inp2)\n  model2_out = Bidirectional(LSTM(256, return_sequences=True))(model2_out)\n  model2_out = AttentionWithContext()(model2_out)\n  model2_out = Dropout(0.1)(model2_out)\n\n  merged_out = keras.layers.Concatenate(axis=1)([model1_out, model2_out])\n\n  merged_out = Dense(32, activation=\"relu\")(merged_out)\n  merged_out = Dropout(0.1)(merged_out)\n  \n  merged_out = Dense(2, activation=\"sigmoid\")(merged_out)\n  model = Model(inputs=[inp1,inp2], outputs=merged_out)\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n  \n  return model\n","4961321a":"model = lstm_attention()\nprint(model.summary())\nmodel.fit([train_X, train_auX], y, batch_size=512, epochs=30, validation_data=([validation_X,validation_auX],y_validation))\n\n","ddcb2a9d":"model.evaluate([test_X, test_auX],y_test,verbose=1,batch_size=100)","9b2dea48":"Treinamento e valida\u00e7\u00e3o do modelo","8a4dcb8e":"<a href=\"https:\/\/colab.research.google.com\/github\/rezwanh001\/Kaggle-Competition-Gendered-Pronoun-Resolution\/blob\/master\/Gendered_Pronoun_Resolution_Version_10.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","93058fd9":"Ap\u00f3s resultados n\u00e3o satisfat\u00f3rios, procuramos metodologias para melhorar o processo encontramos implementa\u00e7\u00f5es de LSTM utilizando o mecanismo Attention para melhorar a compreens\u00e3o para sequencias mais longas. \n\nUma descri\u00e7\u00e3o simplificada do mecanismo pode ser encontrada no post: https:\/\/towardsdatascience.com\/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f\nArtigo original: https:\/\/www.cs.cmu.edu\/~diyiy\/docs\/naacl16.pdf\nExemplo de uso de attention com outra RNN: https:\/\/www.kaggle.com\/keyit92\/end2end-coref-resolution-by-attention-rnn","69c6d0db":"Foram implementadas t\u00ears m\u00e9todos para criar as features:\n1 - Utilizando o texto, pronome e a palavra A\n2 - Utilizando o texto antes da palavra A e o texto antes do pronome\n3 - Apenas o texto\n\nTodas apresentaram resultados semelhantes.\n","bd681433":"Teste do modelo"}}