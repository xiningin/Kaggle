{"cell_type":{"c043cfec":"code","6c5f62a7":"code","435a87b6":"code","0ee48e23":"code","5ee95c74":"code","004bf4fa":"code","f5603840":"code","14c918b6":"code","d00574ed":"code","b89e46ea":"code","0f3c1841":"code","c742e813":"code","3f53c67d":"code","78d0cc7d":"code","55bcd2bb":"code","98e35c33":"code","0cf10d41":"code","bb544d96":"code","c6971639":"code","1c4ae63e":"code","71d45cac":"code","ec5fdd3a":"code","baf01eb2":"code","16413df1":"code","4642fd5e":"code","73e43f67":"code","1444efd7":"code","5624dc30":"code","4052337a":"code","013df880":"code","41ffcdf8":"code","cb6a1a09":"code","c960737d":"code","ae1a5514":"code","d9233d7f":"code","8abf21d6":"code","97884d9b":"code","69213078":"code","624f5aa5":"code","48256902":"code","a888fc05":"code","a4e7a126":"code","6b4eebe1":"code","3111e9ca":"code","5c8805b2":"code","6164bcc6":"code","e5381d29":"code","bf8e40db":"markdown","929fd23a":"markdown","3d10d4f6":"markdown","3a43024c":"markdown","1c9a1fb6":"markdown","0ca692c5":"markdown","a52f006d":"markdown","7220d2a7":"markdown","04e8ce83":"markdown","cce450d5":"markdown","1b7a3395":"markdown","2087d627":"markdown","6157fd21":"markdown","46424d54":"markdown","08b579f4":"markdown","98630688":"markdown","bc007c19":"markdown","1ebcba6c":"markdown","346c405a":"markdown","1fce135d":"markdown","b1a62ee5":"markdown","0396c2a0":"markdown","d50332b9":"markdown","a6e53fa4":"markdown","650bb2a3":"markdown","33e04413":"markdown","4582d443":"markdown","1896ec41":"markdown","fa890e85":"markdown","cb12a670":"markdown","eacaf95e":"markdown","85933de3":"markdown","b7a40536":"markdown","aee3f05d":"markdown","57dd6eaf":"markdown","f0e4daf5":"markdown","2a84ae2f":"markdown","49092562":"markdown","604fad84":"markdown","517323c0":"markdown","3020c30f":"markdown","b87c32f2":"markdown","08115856":"markdown","51314c09":"markdown","08ba4d7b":"markdown","30579b32":"markdown","f5549301":"markdown","ecd8185e":"markdown","97eba804":"markdown","1ca55d4f":"markdown","fba57e9e":"markdown","acd3be8f":"markdown","2718e3ea":"markdown","d4bd57cf":"markdown"},"source":{"c043cfec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6c5f62a7":"training_data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntraining_data.head()","435a87b6":"import gensim","0ee48e23":"(\n    training_data\n    .text\n    .apply(lambda x: x.split(\" \"))\n    .head()\n    .values\n)","5ee95c74":"training_corpus = training_data.text.apply(lambda x: x.split(\" \"))\nmodel = gensim.models.Word2Vec(sentences=training_corpus)","004bf4fa":"model.wv.vocab","f5603840":"model.wv['Forest']","14c918b6":"model.wv.most_similar(\"Forest\")","d00574ed":"model.wv.vectors","b89e46ea":"class MyBackpack():\n    pass","0f3c1841":"class MyBackPack():\n    def __init__(self):\n        self.container = []\n        \n    def hold(self, obj):\n        self.container.append(obj)","c742e813":"backpack = MyBackPack()\nbackpack.hold(\"candy\")","3f53c67d":"print(backpack.container)","78d0cc7d":"class Container():\n    def __init__(self):\n        self.message = \"I am a container whoo.\"","55bcd2bb":"class MyBackPack(Container):\n    def __init__(self):\n        super().__init__()\n        self.container = []\n        \n    def hold(self, obj):\n        self.container.append(obj)","98e35c33":"backpack = MyBackPack()\nbackpack.hold(\"candy\")\nprint(backpack.message)","0cf10d41":"import torch \nimport torch.nn as nn","bb544d96":"class FirstNeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        pass","c6971639":"class FirstNeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, data_input):\n        output_layer1 = self.layer1(data_input)\n        output_layer1 = self.relu(output_layer1)\n        output_layer2 = self.layer2(output_layer1)\n        return output_layer2","1c4ae63e":"first_neural_network = FirstNeuralNetwork(300, 100, 2)\nprint(first_neural_network)","71d45cac":"# Gather the sentences from the training data\nsentence_length = training_data.text.apply(lambda x: x.split(\" \")).apply(len).max()\nsentence_length","ec5fdd3a":"# Get a dictionary mapping of the vocab\nword_map = {\n    word:idx\n    for idx,word in enumerate(model.wv.vocab, start=2)\n}\nword_map","baf01eb2":" (\n    training_data\n    .text\n    .apply(lambda x: x.split(\" \"))\n)[0]","16413df1":"training_sentence_data = (\n    training_data\n    .text\n    .apply(lambda x: list(map(lambda word: word_map[word] if word in word_map else 1, x.split(\" \"))))\n)\nprint(training_sentence_data[0])","4642fd5e":"training_sentence_data = (\n    list(\n        map(\n            lambda x: pd.np.pad(x, (0, sentence_length-len(x))), \n            training_sentence_data\n        )\n    )\n)\ntraining_sentence_data[0]","73e43f67":"torch.LongTensor(training_sentence_data[0])","1444efd7":"training_sentence_data = (\n    list(\n        map(\n            lambda x: torch.LongTensor(x), \n            training_sentence_data\n        )\n    )\n)\ntraining_sentence_data[0]","5624dc30":"pd.np.random.seed(100)","4052337a":"model.wv.vectors.shape","013df880":"word_vectors_for_training = pd.np.insert(\n    model.wv.vectors,   \n    0, \n    pd.np.random.uniform(model.wv.vectors.min(),model.wv.vectors.max(),100),\n    axis=0\n)\n\nword_vectors_for_training = pd.np.insert(\n    word_vectors_for_training,   \n    0, \n    pd.np.zeros(100),\n    axis=0\n)\nword_vectors_for_training = torch.FloatTensor(word_vectors_for_training)\nword_vectors_for_training.shape","41ffcdf8":"class FirstNeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.embedding_layer = nn.EmbeddingBag.from_pretrained(word_vectors_for_training, mode=\"mean\")\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, data_input):\n        embedded_data_input = self.embedding_layer(data_input)\n        output_layer1 = self.layer1(embedded_data_input)\n        output_layer1 = self.relu(output_layer1)\n        output_layer2 = self.layer2(output_layer1)\n        # return the predictions but drop the axis\n        return output_layer2.squeeze()","cb6a1a09":"# finalize the training data\ntraining_sentence_data = torch.stack(training_sentence_data)\n\n# Define the network\nfirst_neural_network = FirstNeuralNetwork(100, 50, 1)","c960737d":"import torch.utils.data as data","ae1a5514":"dataset = data.TensorDataset(training_sentence_data, torch.FloatTensor(training_data.target.values))\ndataloader = data.DataLoader(dataset, batch_size=256)","d9233d7f":"import torch.optim as optim","8abf21d6":"optimizer = optim.Adam(first_neural_network.parameters(), lr=0.001)\nloss_fn = nn.BCEWithLogitsLoss()","97884d9b":"loss = nn.BCEWithLogitsLoss()\nin_mat = torch.randn(3, requires_grad=True)\ntar_mat = torch.empty(3).random_(2)\noutput = loss(in_mat, tar_mat)\nprint(in_mat)\nprint(tar_mat)\nprint(output)","69213078":"#progress bar\nimport tqdm\n\n# number of epochs\nfor n in tqdm.tqdm(range(100)):\n    avg_loss = []\n\n    for batch in dataloader:\n        # batch [0] - the sentence data or X\n        # batch [1] - the label for each sentence or Y\n        \n        # for every back pass you need to zero out the optimizer\n        # less you get residual gradients\n        optimizer.zero_grad()\n        \n        # pass the model into the batch\n        # this line is the same as calling first_neural_network.foward(batch)\n        # yay shortcuts\n        output = first_neural_network(batch[0])\n        \n        # Calculate the loss function\n        loss = loss_fn(output, batch[1])\n        \n        # Save the loss for each epoch\n        avg_loss.append(loss.item())\n        \n        # Tell pytorch to calculate the gradient\n        loss.backward()\n        \n        # tell pytorch to pass the gradients back into the model\n        optimizer.step()\n    print(pd.np.mean(avg_loss))","624f5aa5":"# load testing data\ntesting_data = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntesting_data.head()","48256902":"testing_sentence_data = (\n    testing_data\n    .text\n    .apply(lambda x: list(map(lambda word: word_map[word] if word in word_map else 1, x.split(\" \"))))\n)\nprint(testing_sentence_data[0])","a888fc05":"testing_sentence_data = (\n    list(\n        map(\n            lambda x: pd.np.pad(x, (0, sentence_length-len(x))), \n            testing_sentence_data\n        )\n    )\n)\ntesting_sentence_data[0]","a4e7a126":"testing_sentence_data = (\n    list(\n        map(\n            lambda x: torch.LongTensor(x), \n            testing_sentence_data\n        )\n    )\n)\n\n# finalize the testing data\ntesting_sentence_data = torch.stack(testing_sentence_data)","6b4eebe1":"test_dataset = data.TensorDataset(testing_sentence_data)\ntest_dataloader = data.DataLoader(test_dataset, batch_size=256)","3111e9ca":"prediction_data = testing_data.loc[:,[\"id\",\"text\"]].copy()\nlowest_idx=0\nfor t_batch in test_dataloader:\n        # batch [0] - the sentence data or X\n        \n        \n        # pass the model into the batch\n        # this line is the same as calling first_neural_network.foward(batch)\n        # yay shortcuts\n        output = nn.functional.softmax(first_neural_network(t_batch[0]))\n        bsize = len(output)\n        prediction_data.loc[range(lowest_idx,lowest_idx+bsize),\"prob_target\"] = (\n            output.detach().numpy()\n        )\n        lowest_idx += bsize\nprediction_data.head()","5c8805b2":"prediction_data.loc[prediction_data[\"log_target\"]<0.5,\"target\"]=0\nprediction_data.loc[prediction_data[\"log_target\"]>0.5,\"target\"]=1","6164bcc6":"prediction_data.to_csv(\n    \"submission.csv\",\n    sep=\",\",\n    columns=[\"id\",\"target\"],\n    header=True,\n    index=False\n)","e5381d29":"pd.read_csv(\"submission.csv\").head()","bf8e40db":"Gensim's word2vec model takes sentences as a parameter. This variable assumes the data being passed in are list of list of strings (example of input provided in the code). This means that each \"document\" is contained within one entry of the list. Luckily, using this dataset each tweet is consider their own document which makes this process very simple.","929fd23a":"Note that running the line below will begin the training process and model is the word embeddings network that we just trained. For this tutorial I'll be using default parameters, but the [documentation](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html) explains each parameter (scroll down to the end to get the idea). ","3d10d4f6":"Now the library has been loaded. So the first step is to [read the documentation](https:\/\/radimrehurek.com\/gensim\/auto_examples\/index.html). Just kidding. The first step to training ones own word embeddings is to pick the model they want to use. Gensim has a few models implemented in their library such as vanilla [Word2vec](https:\/\/papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), [Doc2vec](https:\/\/cs.stanford.edu\/~quocle\/paragraph_vector.pdf) and Facebook's [fastText](https:\/\/arxiv.org\/abs\/1607.04606). For the sake of this introudction I'll be using Word2Vec, but try out others yourself.","3a43024c":"The second beauty of pytorch is that you can take a look at your network by printing it out. This network says that we have two layers and an activation function. The first takes 300 features and maps them onto a 100 feature space. The second takes the 100 features passes it into an activation function and then maps them onto a 2 feature space.","1c9a1fb6":"Now the last step is to 0 pad each sentence so every sentence is the same length for the network to test.","0ca692c5":"Now the one important concept to learn is object inheritance. This works almost like genetic inheritance, where offspring \"inherits\" attributes from their parents. In this case objects \"inherits\" methods and attributes from its *parent* class.","a52f006d":"Awesome now every word in each sentence is associated with number that our network can use. One problem is that every sentence is just a numpy array but we need tensors. Question is how can we convert these numbers to tensors? Answer is that pytorch has forseen this issue and makes it super easy to convert from numpy arrays to tensors. Just call the tensor class.","7220d2a7":"convert our numpy matrix to a series of tensors","04e8ce83":"Now the backpack holds \"candy\". Confirmed below:","cce450d5":"Now we have our data loader and we are almost ready to train the model. Next step is to figure out the loss function and optmizer. In our case we are going to use the BinaryCrossEntropy loss function and the Adam optmimizer. To get the optimizer we will need to import the torch.optim library.","1b7a3395":"# Pytorch","2087d627":"So now we have words associated with numbers. This is important the nerual network model can know which embedding index to use. The next step is that we need to map words in our corpus onto the individual numbers themselves. Code below:","6157fd21":"## Object-Oriented Programming","46424d54":"Now is the time to explain pytorch (Yay!!). Main reason I like pytorch is that it uses object oriented programming to build deep learning networks and makes the code SOOO MUCH easier to read and understand.","08b579f4":"Ok that was simple. Now we need to get our data together to train a network.","98630688":"Before we can train the model we now have to set up the data loading section. This involves constructing a dataloader object and a dataset object that pytorch conveniently provides for you. To get these objects we will have to import torch.utils.data.","bc007c19":"You may be wondering why the double import and the reason for this is that pytorch wraps all their deep learning layers under the nn package. The first import contains functions for objects called Tensors, which is a fancy way of describing a free form matrix ([more info here](https:\/\/en.wikipedia.org\/wiki\/Tensor)). This tutorial describes the syntax and creates a simple network, but there are a TON of resources out there that implements complex networks etc. [Pytorch's documentation here](https:\/\/pytorch.org\/docs\/stable\/nn.html). [Introduction to Pytorch Here](https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/pytorch_tutorial.html)","1ebcba6c":"For neural networks in NLP we often find words that are out of our vocabulary (words that may not have appeared in our training set). When this happens usually an unknown token is used to represent this problem. Conveniently, for this tutorial the number 1 represent unknown token for the nerual network to process.","346c405a":"Note that the only difference is that the embedding layer was added in both the init and the forward method.","1fce135d":"So lets create our neural network. This network is a simple two layer neural network that takes features in the first layer and outputs predictions within the second layer.","b1a62ee5":"A useful library to train word embeddings is the gensim library. This library was constructed to process and create word vectors with ease. So first step is to load the data (everybody loves PANDAS!!) and import the library. ","0396c2a0":"Pytorch is a deep learning library (like Tensorflow) that allows you to create and run deep learning models. This section is designed to teach you how pytorch works.","d50332b9":"Now that you have ran through this quick tutorial you should now be able to train your own models and get your own word vectors.\nNote that I glossed over a lot more this library can do, so it would do you wonders to take a look at the documentation of the library and familiarize yourself with everything.","a6e53fa4":"Now lets instantiate the model and take a look at what pytorch does:","650bb2a3":"The init function is called the constructor that instantiates the object, which is a fancy term that means constructs the object in memory. Note that all functions within the class has to have the keyword self which refers to itself. ","33e04413":"To perform inheritance in python you have to do the following:","4582d443":"As you may note the mybackpack object had a message variable that was not declared in its class. This is important to know as pytorch uses this concept to allow you to construct your own models.\n\nAwesome hopefully you get the concept of classes, how to creating methods within classes and how to allow classes \"inherit\" other classes","1896ec41":"This section is designed to teach you how to train your own word embedding vectors and assumes you know the concept of [word embedding vectors](https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa). (if not click the link)  \n\nPremise: Pretrained vectors are awesome since majority of the work has been done for you; however, not all pretrained vectors are appropiate for all tasks.\ne.g. using twitter embeddings to predict newspaper articles. ","fa890e85":"One thing to note is that pytorch handles backpropagation automatically which makes updating the weights of the network as simple as function calling. (loss.backwards()) With everything in place we can now set up the training loop to train the network:","cb12a670":"load the testing data","eacaf95e":"First thing to do is import the torch package.","85933de3":"Within this class description you will start to see some basic object oriented concepts. The nerual network is the sibiling of the nn.Module class. Every model you create within pytorch will inherit some form of the nn.Module class, which is one reason why pytorch is quite intuitive.","b7a40536":"convert each text to a testing matrix. Label each word in the word embedding matrix by its index in the word embedding matrix otherwise label it 1.","aee3f05d":"To get the word vectors themselves you have to call: \"model.wv.vectors\"","57dd6eaf":"# Word Embeddings","f0e4daf5":"Before we dive into pytorch one needs to understand the concept of object oriented programming. This central idea is based on the idea of an object. Think of an object as a simple entity like a Cat, Car or a backpack. These objects are able to perform tasks such as meow, move foward or carry something. (Obviously a car can't meow, but you get the idea) So why does any of this matter? Well in programming languages like Java and Python you can construct your own objects also referred to as classes. For example:","2a84ae2f":"Machines cannot process words only numbers. Given that problem we need to define a way to map words to numbers. An easy way to do this is to use word embeddings, which is what was described above. Pytorch lets you incorporate your own word embeddings through a layer called the embedding layer. This layer is designed to map words directly with the emedding vectors as input. The only catch is that we have to individually map the words to indicies first. I'll visually show you in a second.**","49092562":"There are a vast majority of options for neural networks and this is a VERY simple example. The world is your oyster, so feel free to being shipping out your own forms of network architecture.","604fad84":"Within this forward function the data is passed into the first layer and then the data is passed into an activation function which is then passed into the second layer which then gets outputed for downstream operations (discussed below).","517323c0":"Now the last step is to 0 pad each sentence so every sentence is the same length for the network to learn.","3020c30f":"Now we have everything we need to set up the network. So now it is time to update the nerual network to contain the embedding layer and process the data","b87c32f2":"Now lets construct the dataloader and a dataset so our model can learn.","08115856":"This line of code constructs the mybackpack class (object) and we have yet to add methods for it. So let's add a method to the class.","51314c09":"Wow that was fast. Well now we have our model and now we can do interesting things such as observe words in our vocabulary, look at the vectors themselves and observe word similarity. Note that anytime you want to use wordvectors from the model object you have to call: \"model.wv\". Older packages of this library allowed you to just do \"model['word']\", but has since be depreciated","08ba4d7b":"run the testing tweets through the model","30579b32":"Note that twitter data is messy and it would be wise to do some preprocessing of the data to make sure you dont associate the word \"Forest\" with unhelpful words like 4 and 2.","f5549301":"### Train a model","ecd8185e":"## Pytorch","97eba804":"Voila we have successfully trained a neural network!! Now let's run it on our testing data.\n","1ca55d4f":"Every model created using pytorch must define a function called forward, which takes in the data you want to pass into your network and spits out the output of the network. This function is important to implement, because it is how your network learns to function. So lets fill in the foward function now:","fba57e9e":"For this tutorial I'll be using the getting started twitter data challenge.","acd3be8f":"In the class declaration the contain class is in parenthesis from the mybackpack class. This means the container class is the parent of mybackpack class. The super keyword says access the parent class and instantiate it before creating the child class. So going back to holding the candy string example lets see inheritance at work:","2718e3ea":"# Tutorial for training Word Embeddings and Pytorch","d4bd57cf":"So now we created our class that represents a backpack. Lets put something in the backpack."}}