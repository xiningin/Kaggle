{"cell_type":{"e006f6d6":"code","0640f831":"code","392ed90b":"code","6332c122":"code","c33f82b3":"code","7b86daa6":"code","41fa9335":"code","1eeb3852":"code","e62ed2d3":"code","56bf2a36":"code","2fa2cd77":"code","e94ea72e":"code","a964e428":"code","f6bbaa62":"code","aa432a68":"code","e897d501":"code","4c1ee9ee":"code","6e669427":"code","c32e8273":"code","8a2aed48":"code","7efd0deb":"code","8d828843":"code","c2a60ed6":"code","ed159572":"code","273e74dc":"code","706d1a51":"code","0dc238b9":"code","c902ee87":"code","8bb92c9f":"code","c7519ec3":"code","ae08a9d4":"code","40a5e21c":"code","809d2a3b":"code","b9749411":"code","9e769715":"code","77401895":"code","7025e65d":"code","4a6faa6c":"code","7728e04f":"code","de5d9172":"code","7548f1d7":"code","32e8eaae":"code","818de500":"code","87f4df69":"code","b3565f0c":"code","35103be3":"markdown","9d4dfab2":"markdown","3fddf5b9":"markdown","29545815":"markdown","5df301c6":"markdown","b6061220":"markdown","2fab231e":"markdown","49cfba06":"markdown","8a8de1ca":"markdown","fd602ab0":"markdown"},"source":{"e006f6d6":"# Importing Libraries\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer, OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport eli5","0640f831":"# Read characters dataset\ncharacters = pd.read_csv('..\/input\/movie-dialog-corpus\/movie_characters_metadata.tsv', sep='\\t', \n                            warn_bad_lines=False, \n                            error_bad_lines=False, \n                            header=None)\n\ncharacters.head()","392ed90b":"# Naming the column \ncharacters.columns=['charID','charName','movieID','movieName','gender','posCredits']\ncharacters.head()","6332c122":"characters.describe()","c33f82b3":"characters.shape","7b86daa6":"# To find all the Null values in the data and sum them up with columns\ncharacters.isnull().sum()","41fa9335":"characters = characters.dropna(axis=0)","1eeb3852":"characters.gender.value_counts()","e62ed2d3":"characters = characters.replace({\n    \"m\":\"M\",\n    \"f\":\"F\"\n})\ncharacters.gender.value_counts()","56bf2a36":"# Remove '?' value from the gender column\ncharacters = characters[characters.gender != '?']\ncharacters.gender.unique()","2fa2cd77":"characters.gender = characters.gender.apply(lambda x: 0 if x in 'M' else 1)\ncharacters.gender.value_counts()","e94ea72e":"characters.posCredits.value_counts()","a964e428":"characters.posCredits = characters.posCredits.apply(lambda x: '10+' if not x in ['1','2','3','4','5','6','7','8','9'] else x)\ncharacters.posCredits.value_counts()","f6bbaa62":"lines = pd.read_csv('..\/input\/movie-dialog-corpus\/movie_lines.tsv', sep='\\t', \n                       error_bad_lines=False,\n                       warn_bad_lines=False, \n                       header=None)\nlines.head()","aa432a68":"# Naming the column \nlines.columns=['lineID','charID','movieID','charName','dialogue']\nlines.head()","e897d501":"lines.isnull().sum()","4c1ee9ee":"movies = pd.read_csv(\"..\/input\/movie-dialog-corpus\/movie_titles_metadata.tsv\", sep='\\t', \n                     error_bad_lines=False, warn_bad_lines=False, header=None)\nmovies.columns = ['movieID','movieName','releaseYear','rating','votes','genres']\n\nmovies.head()","6e669427":"titles = movies[['movieID', 'releaseYear']]\ntitles.releaseYear = pd.to_numeric(movies.releaseYear.apply(lambda x: str(x)[:4]), errors='coerce')\ntitles = movies.dropna()\ntitles.info()","c32e8273":"df = pd.merge(lines, characters, how='inner', \n              on=['charID','movieID', 'charName'],\n              left_index=False, right_index=False, \n              sort=True, copy=False, indicator=False)\ndf.head()","8a2aed48":"df.info()","7efd0deb":"df.shape","8d828843":"df.isnull().sum()","c2a60ed6":"df = df.dropna(axis=0)\ndf.isnull().sum()","ed159572":"df = pd.merge(df, titles, how='inner', on=['movieID'],\n         left_index=False, right_index=False, sort=True,\n         copy=False, indicator=False)\ndf.head()","273e74dc":"df['charCount'] = df.dialogue.str.len()\ndf['wordCount'] = df.dialogue.str.count(' ') + 1\ndf.head()","706d1a51":"lemmatizer = WordNetLemmatizer()\ndef clean_dialogue( dialogue ):\n    # Remove all punctuation marks       \n    letters = re.sub(\"[^a-zA-Z]\", \" \", dialogue) \n    \n    # Convert to lower case, split into individual words\n    words = letters.lower().split()                             \n    \n    # Stopwords\n    stopwordsEN = set(stopwords.words(\"english\"))\n    meaningful_words = [lemmatizer.lemmatize(word) for word in words if not word in stopwordsEN]   \n    \n    # Join the words back into one string separated by space, \n    # and return the result.\n    return(\" \".join(meaningful_words))\n\ndf['cleaned_dialogue'] = df['dialogue'].apply(clean_dialogue)\ndf.head()","0dc238b9":"train = df.groupby(['charID', 'movieID', 'charName', 'gender', 'posCredits']). \\\n            agg({'charCount' : ['median'], \n                 'wordCount' : ['median'],\n                 'charID' : ['count'],\n                 'cleaned_dialogue' : [lambda x : ' '.join(x)]\n                })\n\n## Renaming columns by aggregate functions\ntrain.columns = ['charCountMedian', 'wordCountMedian', 'charID_Count', 'cleaned_dialogue']\n\ntrain.reset_index(inplace=True)\ntrain.head()","c902ee87":"train.shape","8bb92c9f":"fig = plt.figure(figsize=(8,5))\nax = sns.countplot(data = train, x = 'gender', hue='gender')\nfor container in ax.containers:\n    ax.bar_label(container)","c7519ec3":"sns.boxplot(data = train, x = 'gender', y = 'charID_Count', hue = 'gender')\nplt.show()","ae08a9d4":"sns.boxplot(data = train, x = 'gender', y = 'wordCountMedian', hue = 'gender')\nplt.show()","40a5e21c":"sns.boxplot(data = train, x = 'gender', y = 'charCountMedian', hue = 'gender')","809d2a3b":"sns.scatterplot(data = train, x = 'wordCountMedian', y = 'charID_Count', \n                hue = 'gender', alpha = 0.5) \nplt.show()","b9749411":"X = train.copy()\nX.drop(['gender', 'charID', 'movieID', 'charName'], axis=1, inplace=True)\ny = train.gender","9e769715":"X.shape","77401895":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                  test_size=0.2, random_state = 10)","7025e65d":"X.info()","4a6faa6c":"class Converter(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_frame):\n        return data_frame.values.ravel()\n\nnumeric_features = ['charCountMedian', 'wordCountMedian', 'charID_Count']\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', MinMaxScaler()),\n    ('normalize', Normalizer())\n])\n\ncategorical_features = ['posCredits']\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\nvectorizer_features = ['cleaned_dialogue']\nvectorizer_transformer = Pipeline(steps=[\n    ('con', Converter()),\n    ('tf', TfidfVectorizer())])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features),\n        ('vec', vectorizer_transformer, vectorizer_features)\n])\n\nrandom_forest = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier',  RandomForestClassifier(n_estimators=120, min_samples_leaf=10, \n                                                            max_features=0.7, n_jobs=-1, oob_score=True))])\n\nsvm = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', SVC(kernel=\"rbf\", C=0.025, probability=True, gamma='auto'))])\n\nsgd = Pipeline(steps=[('preprocessor', preprocessor),\n                     ('classifier', SGDClassifier(max_iter=1000, tol=1e-3, n_jobs=-1))])\n\nknn = Pipeline(steps=[('preprocessor', preprocessor),\n                     ('classifier', KNeighborsClassifier(n_neighbors=3, leaf_size=15, n_jobs=-1))])\n\nxgb = Pipeline(steps=[('preprocessor', preprocessor),\n                     ('classifier', XGBClassifier(objective=\"binary:logistic\"))])","7728e04f":"random_forest.fit(X_train, y_train)\nsvm.fit(X_train, y_train)\nsgd.fit(X_train, y_train)\nknn.fit(X_train, y_train)\nxgb.fit(X_train, y_train)","de5d9172":"def results(name, model):\n    preds = model.predict(X_test)\n\n    print(name + \" score: %.2f\" % model.score(X_test, y_test))\n    print(classification_report(y_test, preds))\n    labels = ['Male', 'Female']\n\n    conf_matrix = confusion_matrix(y_test, preds)\n    plt.figure(figsize= (10,6))\n    sns.heatmap(conf_matrix, \n                annot=True, fmt=\"d\", cmap='Blues')\n    plt.title(\"Confusion Matrix for \" + name)\n    plt.ylabel('True Class')\n    plt.xlabel('Predicted Class')","7548f1d7":"results(\"Random Forest\", random_forest)\nresults(\"SVC\", svm)\nresults(\"SGD\", sgd)\nresults(\"K-Nearest Neighbors\", knn)\nresults(\"XGBoost\", xgb)","32e8eaae":"vect_columns = list(xgb.named_steps['preprocessor'].named_transformers_['vec'].named_steps['tf'].get_feature_names())\nonehot_columns = list(xgb.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names(input_features=categorical_features))\nnumeric_features_list = list(numeric_features)\nnumeric_features_list.extend(onehot_columns)\nnumeric_features_list.extend(vect_columns)","818de500":"weights = eli5.explain_weights_df(sgd.named_steps['classifier'], top=30, feature_names=numeric_features_list)\nweights.head(15)","87f4df69":"weights.tail(14)","b3565f0c":"eli5.explain_weights_df(random_forest.named_steps['classifier'], top=30, feature_names=numeric_features_list)","35103be3":"**Business Problem** - classification model to classify the gender of a Hollywood movie character based on their dialogues in the movie. \n\nAfter looking into the data files, we observe that we need only movie_lines and movie_characters_metadata files to solve our business problem.","9d4dfab2":"# Merging all cleaned Data","3fddf5b9":"1. Removing stopwords\n2. Converting to lowercase and splitting into words\n3. Lemmatize every word","29545815":"There are some missing values which we need to work.","5df301c6":"About 50% of gender is not known. So, we have to remove it as we dont have enough information about the gender.","b6061220":"# Character Dataset","2fab231e":"# Feature Extraction","49cfba06":"# Movie Lines Data","8a8de1ca":"As most of the values are in the range 1-10, we can encode them as 1-9 and 10+","fd602ab0":"# Feature Engineering\nExtracting Features to train the model"}}