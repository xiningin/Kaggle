{"cell_type":{"ac494fc8":"code","12eca02e":"code","acc77003":"code","e53186b8":"code","6d1d306f":"code","8313ada2":"code","84d4a3a4":"code","3f4e1794":"code","f8d12baf":"code","ad11fe51":"code","5e16e30c":"code","4abeaf20":"code","da0b7518":"code","0d3bbd2a":"code","cb19c956":"code","5d833b8c":"code","53d3e016":"code","c2148c52":"code","d4ab3137":"code","945b2d0f":"code","467c0a23":"code","11429fce":"code","9bd391a1":"code","c488bee9":"code","3b7e2640":"code","41f75cb2":"code","b3980ba5":"code","1cf8d864":"code","b0d0a1e0":"code","6316ce60":"code","4e4096fe":"code","e42a0b51":"markdown","38d550a4":"markdown","1771d695":"markdown","4c57255a":"markdown","f12e14b2":"markdown","871ac44d":"markdown","abcd37ce":"markdown","64836011":"markdown","744015b5":"markdown","9f892d49":"markdown","e72f0266":"markdown","b9256102":"markdown","e360a750":"markdown","50e47323":"markdown","1f726456":"markdown","86872199":"markdown","871269f5":"markdown","95acf07d":"markdown","0ef72d14":"markdown","372b5b48":"markdown","9312760f":"markdown","6c8c64f5":"markdown","a0ec653f":"markdown","c70839d3":"markdown","e0e01ae5":"markdown","c4bce495":"markdown","f530b1ab":"markdown","7c1eebff":"markdown","13f2fe46":"markdown","eb39f6eb":"markdown"},"source":{"ac494fc8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt","12eca02e":"colors = ['r', 'g', 'b'] # Searborn colors (red, green and blue)\n# Note that there is no relation of value with the colors, these are just used to beautify the results.","acc77003":"X_train = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\")","e53186b8":"# Get few records on training set\nX_train.head()","6d1d306f":"# Get the shape\nX_train.shape","8313ada2":"# Drop the id column as it is not required\nX_train = X_train.drop(\"Id\", axis=1)","84d4a3a4":"X_train[\"PoolQC\"].isna().sum() # Just checking for correctness","3f4e1794":"# NaN Values in features\nfeatures_with_null_values = []\nn_rows = len(X_train.index)\nfor feature in X_train.columns:\n    sum_nan = X_train[feature].isna().sum()\n    if sum_nan > 0:\n        percentage_nan = sum_nan*100\/n_rows\n        print(feature, \"{0:.3f}%\".format(percentage_nan))\n        features_with_null_values.append(feature)\n    else:\n        pass","f8d12baf":"# List of features with NaN Values\nfeatures_with_null_values","ad11fe51":"# Visualization for features with null values\nplt.figure(figsize=(10,5))\nsns.heatmap(X_train[features_with_null_values].isna(), yticklabels=False)","5e16e30c":"# Relationship between Null values and response variable\nfor feature in features_with_null_values:\n    # Copy the dataset\n    X = X_train.copy()\n    \n    # Place 1 where missing values are found and 0 where not\n    X[feature] = np.where(X[feature].isnull(), 1, 0)\n    \n    # Median is used for plotting because there might be outliers which may change mean.\n    data = X.groupby(feature)['SalePrice'].median()\n    data = pd.DataFrame(data) # Convert Series to DataFrame\n    sns.barplot(x=data.index, y=data.SalePrice) # Plot bar chart\n    plt.title(feature)\n    plt.show()","4abeaf20":"# Analysing Years\n# First get the columns that contains either \"Year\" or \"Yr\"\nfeature_with_year = []\n\nfor feature in X_train.columns:\n    if \"Yr\" in feature or \"Year\" in feature:\n        feature_with_year.append(feature)\n\nfeature_with_year","da0b7518":"# Analysing the years\ni=0\nfor feature in feature_with_year:\n    sns.lineplot(x = X_train[feature], y = X_train['SalePrice'], color=colors[i%3])\n    plt.show()\n    i += 1","0d3bbd2a":"# Confirm whether sale price is actually decreasing or not.\nX_train.groupby(\"YrSold\")['SalePrice'].median().plot()\nplt.xlabel(\"Year Sold\")\nplt.ylabel(\"Sale Price\")\nplt.show()","cb19c956":"for feature in feature_with_year:\n    if feature == \"YrSold\":\n        continue\n    # This line of code plots the line graph between NumberOfYears and SalePrice for each \"Year\" feature\n    sns.lineplot(X_train['YrSold'] - X_train[feature], X_train['SalePrice'], color=colors[i%3])\n    plt.xlabel(feature)\n    plt.ylabel(\"SalePrice\")\n    plt.show()\n    i+=1","5d833b8c":"# Separate Numerical and Categorical Features\nnumerical_features = []\ncategorical_features = []\nfor feature in X_train.columns:\n    if X_train[feature].dtypes == 'O': # Categorical feature if datatype is object.\n        categorical_features.append(feature)\n    else: # Numerical if otherwise\n        numerical_features.append(feature)\n\nprint(\"Numerical Features are: \", numerical_features)\nprint(\"\\n\\nCategorical Features are: \", categorical_features)","53d3e016":"# Code to just check one or two\nX_train.info()","c2148c52":"# Seperate Discrete Features with continuous one in numerical features\ndiscrete_features = []\ncontinuous_features = []\nfor feature in numerical_features:\n    if len(X_train[feature].unique()) <= 10 and feature not in feature_with_year:\n        discrete_features.append(feature)\n    elif feature not in feature_with_year:\n        continuous_features.append(feature)\n\nprint(\"Discrete Features are: \", discrete_features)","d4ab3137":"for feature in discrete_features:\n    data = X_train.groupby(feature)['SalePrice'].median()\n    data = pd.DataFrame(data) # Convert Series to DataFrame\n    sns.barplot(x = data.index, y = data.SalePrice)\n    plt.xlabel(feature)\n    plt.ylabel(\"Sale Price\")\n    plt.show()","945b2d0f":"# Analysis for continuous features\nfor feature in continuous_features:\n    sns.distplot(X_train[feature], kde=True, color=colors[i%3])\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.show()\n    i+=1","467c0a23":"for feature in continuous_features:\n    sns.scatterplot(X_train[feature], X_train[\"SalePrice\"], color=colors[i%3])\n    plt.xlabel(feature)\n    plt.ylabel(\"Sale Price\")\n    plt.show()\n    i+=1","11429fce":"# Again Calculating Discrete and Continuous Features but with updated threshold.\ndiscrete_features = []\ncontinuous_features = []\nfor feature in numerical_features:\n    if len(X_train[feature].unique()) <= 15 and feature not in feature_with_year:\n        discrete_features.append(feature)\n    elif feature not in feature_with_year:\n        continuous_features.append(feature)","9bd391a1":"# Correlation between continuous features\nplt.figure(figsize = (20,7))\nsns.heatmap(X_train[continuous_features].corr(), cmap=\"YlGnBu\", annot = True)\nplt.title(\"Correlation For Continuous Features\")\nplt.show()","c488bee9":"# Check correlation for discrete with SalePrice\nX_discrete = pd.concat([X_train[discrete_features], X_train['SalePrice']], axis=1)\nplt.figure(figsize = (15, 7))\nsns.heatmap(X_discrete.corr(), cmap=\"YlGnBu\", annot=True)\nplt.title(\"Correlation for Discrete Features\")\nplt.show()","3b7e2640":"# Relationship between categorical features and sale price.\nfor feature in categorical_features:\n    data = X_train.groupby(feature)['SalePrice'].median()\n    data = pd.DataFrame(data) # Convert Series to DataFrame\n    sns.barplot(x=data.index, y=data.SalePrice)\n    plt.xlabel(feature)\n    plt.ylabel(\"Sale Price\")\n    plt.show()","41f75cb2":"# Continuous Feature - Outliers\nfor feature in continuous_features:\n    sns.boxplot(y=X_train[feature], color=colors[i%3])\n    plt.ylabel(feature)\n    plt.show()\n    i+=1","b3980ba5":"# Log transform the data\nX_log_transform = X_train.copy() # Will not affect the real dataset\nfor feature in continuous_features:\n    if feature != \"SalePrice\":\n        X_log_transform[feature] = np.log1p(X_log_transform[feature])","1cf8d864":"# pd.pandas.set_option('display.max_columns', None)\nX_log_transform.head()","b0d0a1e0":"# Scatter plot between continuous feature and SalePrice\nfor feature in continuous_features:\n    sns.scatterplot(x = X_log_transform[feature], y = X_log_transform[\"SalePrice\"], color=colors[i%3])\n    plt.xlabel(feature)\n    plt.ylabel(\"SalePrice\")\n    plt.show()\n    i+=1","6316ce60":"# Plotting distribution plot after log transform.\nfor feature in continuous_features:\n    sns.distplot(X_log_transform[feature], kde=True, color=colors[i%3])\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()\n    i+=1","4e4096fe":"# Checking for outliers.\nfor feature in continuous_features:\n    sns.boxplot(y=X_log_transform[feature], color=colors[i%3])\n    plt.ylabel(feature)\n    plt.show()\n    i+=1","e42a0b51":"# Continuous features","38d550a4":"### Interpretation\n* From the above plots, we can see that as the recent years approach, the SalePrice increase.\n* However this is not the case for the last one.\n* Ee can clearly see that there is a decrease in sale price as the year is increased.\n* Lets confirm this.","1771d695":"* As evident from the above graph, the sale price does decrease as number of years go by.\n* This is quite counterintuitive as the price should increase.\n* Thus, we would see the relation between each \"Year\" feature with \"YrSold\"","4c57255a":"### Interpretation\n* From the above figures we can see that in some feature cases like GrLivArea, Garage Area etc, there is a positive relation.\n* Hence, we can say that log transform did help us in visualizing and comparing the data set as it scales the continuous features.\n* Another purpose of log transform is that it helps in making the skewed data, less skewed.\n* Lets check that out.","f12e14b2":"### Interpretation\n* We can see that Alley, FlreplaceQu, PoolQC, Fence and MiscFeature has a lot of missing values.\n* This is also evident from the percentage values.","871ac44d":"* As we can see there are multiple columns that have some NaN values.\n* Further there are not just numerical features, there are some categorical features also.","abcd37ce":"* The below code plots the relationship between null values and median SalePrice.\n* We choose median because there may be outliers we don't have information about.\n* Because of them, the mean can be drastically different\/skewed whereas median is not susceptible to outliers.","64836011":"### Interpretation\n* Here we can see that clearly many features have been normalised.\n* Note that this might not be true for some feature that may follow bimodal or multimodal distribution.","744015b5":"### Interpretation\n* There is a strong correlation between SalePrice and OverallQual.\n* This was also evident from the discrete bar charts because of the exponential rise.\n* Further, we will have to look at it again after handling the missing values.","9f892d49":"### Interpretation\n* Boxplot can detect outliers.\n* This means that if any observations above or below 3*IQR (Inter Quartile Range) can be considered as an outlier.\n* As seen in the above graphs, there are some features for which this is true and hence outliers are present.\n* This can be more clearly explored once log transform is performed.\n* Thus, now lets perform it finally.","e72f0266":"# Temporal Variables","b9256102":"## Log-Transformation\n* This is a part of feature engineering but still we are performing it here to get bettern insights about outliers.\n* Log gives a measurement of how \"big\" a number is in comparision to another number (base of log).\n* Suppose we are comparing weights\/mass of the heavenly bodies in solar system.\n* ![Screenshot from 2022-01-24 08-45-35.png](attachment:c5a2e2ba-6bee-4be0-9101-84cb462d8d43.png)\n* Compared to Sun, all others seem to have similar mass.\n* Now take log and then compare.\n* ![Screenshot from 2022-01-24 08-47-18.png](attachment:a8ddd57a-f0b5-45e9-95bb-58ddda5e3725.png)\n* Thus, log scales down the value that can be compared.\n\n**Click [here](https:\/\/towardsdatascience.com\/logarithms-what-why-and-how-ff9d050d3fd7) for the source.**\n\n* Further, the larger values tend to bias the model and influence them to a great extent.\n* **Logarithm scales the data, reduces the range of data, brings linearity to the data and removes skewness to a certain extent while preserving the distribution.**\n* **Note that log-transformation should be used only for continuous features due to obviuous reasons.**","e360a750":"# Conclusion\n* Data Analysis is one of the most crucial step in Machine Learning Project.\n* As evident from the above notebook, we get very great insights regarding the data.\n* This will be helpful in other processes of the lifecycle including Feature Engineering, Feature Selection and Scaling.\n* Stay tuned for these notebooks. Will be publishing them soon.","50e47323":"# Outliers","1f726456":"* As we can see that there are a lot of features that have more than 70% missing values.\n* Along with this there are features that have less than 10% as missing values.\n* All these would be taken care in the feature engineering session.","86872199":"### Interpretation\n* Firstly note that there are still some other discrete features like TotRmsAbvGrd, MoSold, MSSubClass etc.\n    * This shows that we must increase threshold for discrete features to 15.\n    * This also shows that this step should be done immediately after separating discrete and continuous features.\n* This also proves the fact the most of the features are **skewed** and **needs log transform**.","871269f5":"# Discrete Features","95acf07d":"# Exploratory Data Analysis (EDA)\n* Import the libraries\n* Set Theme for seaborn\n* Fetched training and test dataset\n* Get intuition about the features containing NaN values.\n* Relationship of NaN values of different feature with SalePrice.\n* Relationship of **temporal** features with SalePrice.\n* Separate Numerical and Categorical Features\n* For numerical, get insights about the discrete features.\n* Get the distribution of continuous features.\n* Get the relationship between continuous feature and SalePrice.\n* Check for correlation between (continuous and discrete features) and SalePrice.\n* For categorical, get bar charts for relationship between different categories of different features and SalePrice.\n* Check for outliers using boxplots.\n* Apply Log transform on continuous features and repeat the steps.","0ef72d14":"### Interpretations\n* The first graph proves that there is some relationship between discrete features and SalePrice.\n* It shows that the overall SalePrice increases exponentially as the overall quality increases.\n* This provides an idea about the distribution of the overall quality.\n* Other graphs show unpredictable patterns but at least we know that there is some realtionship between them and SalePrice (though they can't be identified right now).","372b5b48":"# Categorical Features","9312760f":"### Interpretation\n* Here we can see that many of the features have skewed normal distribution like LotArea.\n* Can use logarithmic transform to make it less skewed or even normal distribution.\n* Others have either bimodal or multimodal distribution.\n* This helps in checking out assumtions regarding some models and we can use other testing methods for other than guassian distribution.\n* Further, features like TotRmsAbvGrd seems to have discrete values. This might mean that we need to increase our threshold to 15 instead of 10.","6c8c64f5":"# Lifecycle\n* This notebook implements first part of lifecycle process i.e Data Analysis or Exploratory Data Analysis (EDA).\n* Following is the list of lifecycle processes.\n    * **Data Analysis or EDA** (Content of this notebook)\n    * Feature Engineering\n    * Feature Selection\n    * Pipelining\n    * Model Selection and Building\n    * Model Deployment\n* All these are performed on a very famous kaggle competition known as House Price Prediction.","a0ec653f":"# Numerical and Categorical Features","c70839d3":"* From the above graphs it is clear that a lot of features that have missing values are strongly related to SalePrice.\n* This can be concluded as in some of the features, the median of SalePrice is higher for null values than for not-null values.\n* This indicates that not all missing values can be ignored and we can create a new category (if the feature it corresponds to is a categorical feature) called \"Others\".","e0e01ae5":"### Explanations\n* In above code there are two conditions.\n* One is obvious that column should not be an year as there is a possibility that year might have less than 10 unique values.\n* Other one is that number of unique classes\/values must be less than 10 i.e 10 is the threshold for discrete features.\n* Based on that we obtained a list of features which can be considered for discrete features.\n* Now, lets see the relationship between them and SalePrice.","c4bce495":"### Interpretation\n* Sale Price decreases if a house built is older compared to newer ones.\n* Same for Garage Built and RemodAdd.\n* Now, this is quite intuitive. As the house becomes older, the \n* Further, it provides some idea\/insights about what is actually happening in the data.","f530b1ab":"# Null Values","7c1eebff":"### Interpretations\n* We can see that there is some correlation but not much between each pair of predictors.\n* Further there are some features like GrLivArea which have good correlation with SalePrice.\n* However, there might be some correlation error because of null values if any. Hence it should be done again after missing values are handled.","13f2fe46":"## Correlation\n* This is one of the most important step that also helps in feature selection.\n* Calculated as covariance of two variables divided by the product of std dev of each feature.\n* Correlation tells us whether one of multiple features depend on another feature.\n* It is useful because it can also help in imputing missing values.\n    * Suppose you have two features that have high positive correlation.\n    * And further suppose that one of the feature has some missing values.\n    * We can easily drop that feature without much loss of information.\n    * This happens because of correlation.\n* Further, we can say that two variables are highly correlated if they have correlations above 0.5 or 0.7 depeneds on the threshold that we set.\n* However, features that have correlation below 0.5 must not be considered as correlated features.\n* Multicollinearity - Happens when one feature can be predicted by other or combination of other features with high accuracy.\n* This leads to skewed results.\n\n> Which models are\/are-not affected by correlation?\n* Decision Trees and Boosted Trees algorithm are not affected by correlation as their algorithm doesn't make any assumptions about it.\n* Further, for decision trees when it splits, the model takes into account, the tree that will choose only one of the perfectly correlated features.\n* Logistic Regression and Linear Regression are affected by this problem.\n\n* There are two methods to detect correlation\n    * Pearson Correlation Coefficient - Used when there is roughly linear relationship between features.\n    * Spearman Correlation Coefficient - Used when there is non-linear relationship between features.","eb39f6eb":"### Interpretation\n* The above plot is better than default dataset.\n* Even though there are still some features for which it is not clear whether outliers are there of not, there are many features that contain outliers like LotFrontage, LotArea etc."}}