{"cell_type":{"253e4c5e":"code","e3fc2925":"code","08d442a6":"code","538d5115":"code","4af54bd9":"code","cbf1abcd":"code","d1be54b3":"code","10c9e535":"code","d6a008d9":"code","bf105d95":"code","e3861b83":"code","2793acd7":"code","89e72067":"code","826adeea":"code","395a7d40":"code","947b3076":"code","8cd02c71":"code","a847a03a":"markdown","71e31874":"markdown","f8ff0105":"markdown","a665ca62":"markdown","073f3dfc":"markdown","a205d27a":"markdown","042dce65":"markdown","dd9d3a5d":"markdown","c860b6c1":"markdown","141ce835":"markdown","3290e404":"markdown","9385787e":"markdown","10dbd7b7":"markdown","a429c5ec":"markdown","0273ed55":"markdown"},"source":{"253e4c5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e3fc2925":"import matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm, tree\n#%matplotlib inline","08d442a6":"# load the data\nlabeled_images = pd.read_csv('..\/input\/train.csv')\nimages = labeled_images.iloc[0:5000,1:]\nlabels = labeled_images.iloc[0:5000,:1]\ntrain_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)","538d5115":"# now we gonna load the second image, reshape it as matrix than display it\ni=1\nimg=train_images.iloc[i].values\nimg=img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train_labels.iloc[i,0])\n","4af54bd9":"# Todo: Put your code here","cbf1abcd":"#train_images.iloc[i].describe()\n#print(type(train_images.iloc[i]))\nplt.hist(train_images.iloc[i])","d1be54b3":"# create histogram for each class (data merged per class)\n# Todo\n#print(train_labels.iloc[:5])\ndata1 = train_images.iloc[1]\ndata2 = train_images.iloc[3]\ndata1 = np.array(data1)\ndata2 = np.array(data2)\ndata3 = np.append(data1,data2)\nprint(len(data3))\nplt.hist(data3)","10c9e535":"clf = svm.SVC()\nclf.fit(train_images, train_labels.values.ravel())\nclf.score(test_images,test_labels)\n","d6a008d9":"# Put your verification code here\n# Todo\nprint(train_labels.values.ravel())\nprint(np.unique(test_labels)) # to see class number","bf105d95":"test_images[test_images>0]=1\ntrain_images[train_images>0]=1\n\nimg=train_images.iloc[i].values.reshape((28,28))\nplt.imshow(img,cmap='binary')\nplt.title(train_labels.iloc[i])","e3861b83":"# now plot again the histogram\nplt.hist(train_images.iloc[i])","2793acd7":"clf = svm.SVC()\nclf.fit(train_images, train_labels.values.ravel())\nclf.score(test_images,test_labels)","89e72067":"# Test again to data test\ntest_data=pd.read_csv('..\/input\/test.csv')\ntest_data[test_data>0]=1\nresults=clf.predict(test_data[0:5000])","826adeea":"# separate code section to view the results\nprint(results)\nprint(len(results))","395a7d40":"# dump the results to 'results.csv'\ndf = pd.DataFrame(results)\ndf.index.name='ImageId'\ndf.index+=1\ndf.columns=['Label']\ndf.to_csv('results.csv', header=True)","947b3076":"#check if the file created successfully\nprint(os.listdir(\".\"))","8cd02c71":"# from https:\/\/www.kaggle.com\/rtatman\/download-a-csv-file-from-a-kernel\n\n# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(df)","a847a03a":"Now plot the histogram within img","71e31874":"# Retrain the model\nUsing the now adjusted data, let's retrain our model to see the improvement","f8ff0105":"### Q1\nNotice in the above we used _images.iloc?, can you confirm on the documentation? what is the role?","a665ca62":"# Library Import\nFor starter import any machine libary we wanted to use. SKLearn is good choice for beginner, the question is what the algorithm we interested to test. \nHere's what we are going to need:\n1. At least a classification algorithm (SVM or Decision Tree is a Good Choice)\n2. Matplotlib\n3. Preprocessing tools\n4. Train test split\nAnd since we have been import numpy and panda no need to import them. ","073f3dfc":"### Q4\nIn above, did you see score() function?, open SVM.score() dokumentation at SKLearn, what does it's role?. Does it the same as MAE discussed in class previously?.\nAscertain it through running the MAE. Now does score() and mae() prooduce the same results?. ","a205d27a":"### Q2\nNow plot an image for each image class","042dce65":"### Improving Performance\nDid you noticed, that the performance is so miniscule in range of ~0.1. Before doing any improvement, we need to analyze what are causes of the problem?. \nBut allow me to reveal one such factor. It was due to pixel length in [0, 255]. Let's see if we capped it into [0,1] how the performance are going to improved.","dd9d3a5d":"# Q6\nAlhamdulillah, we have completed our experiment. Here's things to do for your next task:\n* What is the overfitting factor of SVM algorithm?. Previously on decision tree regression, the factor was max_leaf nodes. Do similar expriment using SVM by seeking SVM documentation!\n* Apply Decision Tree Classifier on this dataset, seek the best overfitting factor, then compare it with results of SVM. \n* Apply Decision Tree Regressor on this dataset, seek the best overfitting factor, then compare it with results of SVM & Decision Tree Classifier. Provides the results in table\/chart. I suspect they are basically the same thing. \n* Apply Decision Tree Classifier on the same dataset, use the best overfitting factor & value.  But use the unnormalized dataset, before the value normalized to [0,1]\n\n","c860b6c1":"### Q5\nBased on this finding, Can you explain why if the value is capped into [0,1] it improved the performance significantly?. \nPerharps you need to do several self designed test to see why. ","141ce835":"### Q3\nCan you check in what class does this histogram represent?. How many class are there in total for this digit data?. How about the histogram for other classes","3290e404":"### Prediction labelling\nIn Kaggle competition, we don't usually submit the end test data performance on Kaggle. But what to be submitted is CSV of the prediction label stored in a file. ","9385787e":"# Disclaimer\nThe data in this notebook is mostly copied from [https:\/\/www.kaggle.com\/archaeocharlie\/a-beginner-s-approach-to-classification ](http:\/\/https:\/\/www.kaggle.com\/archaeocharlie\/a-beginner-s-approach-to-classification). I intended to do modification later to the tutorial, so please permit me for using it. ","10dbd7b7":"# Load Data\nIn case you haven't imported Digit Recognizer dataset from the competition, please do so. Then load the data with pandas. \nFor simplicity we'll only load first 5000 train images then split them again into our personal train & test set for testing.","a429c5ec":"### Train the model\nNow we are ready to train the model, for starter let's use SVM. For the learning most model in SKLearn adopt the usual fit() and predict(). ","0273ed55":"# Data Download\nWe have the file, can listed it but how we are take it from sever. Thus we also need to code the download link. "}}