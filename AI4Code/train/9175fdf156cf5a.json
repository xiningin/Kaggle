{"cell_type":{"16a2bad9":"code","8ef06599":"code","ed28904f":"code","607aa792":"code","abc921bb":"code","f6f8f6a9":"code","29649074":"code","50ad8010":"code","97579001":"code","de52a739":"code","0f1fa268":"code","cbb78507":"code","b44fcd41":"code","a9cd3781":"code","2de2f74b":"code","88a0eaaf":"code","40af00f3":"code","9b412c32":"code","25d66255":"code","b7e0dddb":"code","39088517":"code","83dd571b":"code","f923b229":"code","f73e5192":"code","6fc3d5be":"code","96040c24":"code","45ead578":"code","84034173":"code","a649bd2e":"code","ffdb3d38":"code","e9e2c458":"code","7530e528":"code","abd3b55b":"code","5aaef74a":"code","0f6fd4a8":"code","7d5d05c4":"code","afa424fe":"code","212a185b":"code","59ddd6ba":"code","13cbd4d5":"markdown","ffe7471f":"markdown","60e27248":"markdown","c55a80a2":"markdown","1c33c07d":"markdown","fcea150e":"markdown","c5c12215":"markdown","91ed061b":"markdown","e06e847e":"markdown","c8138a71":"markdown","8ce9c6c2":"markdown","512c36cb":"markdown","1d15cece":"markdown","829a1f6c":"markdown","c9e1c966":"markdown","23f24509":"markdown","234a2f94":"markdown","8e32019e":"markdown","b3e600f2":"markdown","5ff4970a":"markdown","5b914707":"markdown"},"source":{"16a2bad9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn.decomposition as decomposition\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport regex as re\nimport nltk\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.svm import LinearSVC\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.svm import SVC","8ef06599":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","ed28904f":"train_df.shape","607aa792":"test_df.shape","abc921bb":"train_df.head()","f6f8f6a9":"train_df[train_df[\"target\"] == 0][\"text\"].values[1]","29649074":"train_df[train_df[\"target\"] == 1][\"text\"].values[1]","50ad8010":"#visualization : Disaster and non disaster tweets\n%matplotlib inline\nsns.countplot (x='target', data = train_df, palette = 'Blues_d')","97579001":"#Convert to lower case\ntrain_df[\"text\"]=train_df[\"text\"].str.lower()\ntest_df[\"text\"]=test_df[\"text\"].str.lower()\n#Punctuations\nsymbols = \"!\\\"#$%&()*+-.\/:;<=>?@[\\]^_`{|}~\\n\"\nfor i in symbols:\n    train_df[\"text\"] = train_df[\"text\"].str.replace(i,' ')\n    test_df[\"text\"] = test_df[\"text\"].str.replace(i,' ')\n\ndef remove_https(text):    \n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    return text\ntrain_df['text'] = train_df['text'].apply(lambda x: remove_https(x))\ntest_df['text'] = test_df['text'].apply(lambda x: remove_https(x))\n\nimport re\n\nemoji = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n\ndef strip_emoji(text):\n    return emoji.sub(r'', text)\n\ntrain_df['text'] = train_df['text'].apply(lambda x: strip_emoji(x))\ntest_df['text'] = test_df['text'].apply(lambda x: strip_emoji(x))\n\n","de52a739":"#Remove stop words\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ntrain_df['text'] = train_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntest_df['text'] = test_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","0f1fa268":"from nltk.stem import WordNetLemmatizer\n\nlm = WordNetLemmatizer()\n\ndef lemmatization(text):\n    text = [lm.lemmatize(word,pos=\"v\") for word in text.split()]\n    return ' '.join(text)\ntrain_df['text'] = train_df['text'].apply(lemmatization)\ntest_df['text'] = test_df['text'].apply(lemmatization)","cbb78507":"#Disaster tweets dataframe\ndisaster_tweets =train_df[train_df.target==1]","b44fcd41":"tweets=[]\nfor t in disaster_tweets.text:\n    tweets.append(t)\ntweets[:5]","a9cd3781":"#Convert to pandas series \ndisaster_tweet_text=pd.Series(tweets).str.cat(sep='')","2de2f74b":"#Remove stop words\nfrom wordcloud import STOPWORDS\nstop_words=[\"https\",\"news\",\"via\",\"will\",\"amp\",\"now\"] + list(STOPWORDS)","88a0eaaf":"wordcloud=WordCloud(width = 1600, height = 900,max_font_size=200,stopwords=stop_words).generate(disaster_tweet_text)","40af00f3":"plt.figure(figsize=(12,11))\nplt.imshow(wordcloud)","9b412c32":"count_vectorizer = feature_extraction.text.CountVectorizer(input = 'train_df[\"text\"]', ngram_range=(1,2))\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])","25d66255":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","b7e0dddb":"train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])","39088517":"print(train_vectors.shape)\nprint(test_vectors.shape)","83dd571b":"from sklearn.feature_extraction.text import TfidfVectorizer","f923b229":"tfidfconvertor = TfidfVectorizer(max_features=5000, min_df = 5, max_df = .7,ngram_range=(1,2), stop_words= stopwords.words('english'))","f73e5192":"train_matrix_tfidf = tfidfconvertor.fit_transform(train_df[\"text\"])\n#test_matrx_tfidf = tfidfconvertor.transform(test_df[\"text\"])\npd.DataFrame(train_matrix_tfidf.toarray(),columns=tfidfconvertor.get_feature_names())","6fc3d5be":"test_matrix_tfidf = tfidfconvertor.transform(test_df[\"text\"])\ntest_matrix_tfidf.shape","96040c24":"## Our vectors are really big, so we want to push our model's weights\n## toward 0 without completely discounting different words - ridge regression \n## is a good way to do this.\nclf = linear_model.RidgeClassifier()","45ead578":"scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","84034173":"scores = model_selection.cross_val_score(clf, train_matrix_tfidf, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","a649bd2e":"svc_model = LinearSVC()\nsvc_model.fit(train_matrix_tfidf, train_df[\"target\"])\n","ffdb3d38":"scores = model_selection.cross_val_score(svc_model, train_matrix_tfidf, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","e9e2c458":"params_grid=[{'kernel':['rbf'],'gamma':[1e-3,1e-4],'C':[1,10,100,1000]},\n            {'kernel':['linear'],'C':[1,10,100]}]\n","7530e528":"svm_model=GridSearchCV(SVC(),params_grid,cv=5)\nsvm_model1=svm_model.fit(train_matrix_tfidf,train_df[\"target\"])","abd3b55b":"print(svm_model1.best_score_)\nprint(svm_model1.best_params_)","5aaef74a":"svm=SVC(kernel='rbf',gamma=0.001,C=1000)\nsvm.fit(train_matrix_tfidf, train_df[\"target\"])","0f6fd4a8":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","7d5d05c4":"sample_submission[\"target\"] = svm.predict(test_matrix_tfidf)","afa424fe":"sample_submission.head()","212a185b":"sample_submission.to_csv(\"SVM_rbf_tfidf_ngram3.csv\", index=False)","59ddd6ba":"sample_submission.shape","13cbd4d5":"## Lemmatization\nLemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization converts the word to its meaningful base form (a dictionary word)","ffe7471f":"#### Ridge classification using CountVectorizer","60e27248":"Let's do predictions on our training set and build a submission for the competition.","c55a80a2":"First, an example of what is NOT a disaster tweet.","1c33c07d":"## A quick look at our data\n\nLet's look at our data...","fcea150e":"# Pre-processing the data#","c5c12215":"# Drawbacks of Count Vectorizer\nThis is a Pug.\\\nThis is a Pomeranian.\\\nThis is a Labrador.\n\nLet's consider these to be 3 documents in a corpus. The count vectorizer does not capture the importance of a word.\n\n| This | is | a | Pug| Pomeranian | Labrador |\n| --- | --- | --- | --- | --- | --- |\n| 1 | 1 | 1 | 1 | 0 | 0 |\n| 1 | 1 | 1 | 0 | 1 | 0 |\n| 1 | 1 | 1 | 0 | 0 | 1 |\n\nIn the fist document 'This', 'is' , 'a' and 'Pug', all have the same weight. Whereas 'Pug' is the most important data\n","91ed061b":"And one that is a disaster tweet:","e06e847e":"### Tuning the params","c8138a71":"# Building vectors\n\nThe theory behind the model we'll build in this notebook is pretty simple: the words contained in each tweet are a good indicator of whether they're about a real disaster or not (this is not entirely correct, but it's a great place to start).\n\nWe'll use scikit-learn's `CountVectorizer` to count the words in each tweet and turn them into data our machine learning model can process.\n\nNote: a `vector` is, in this context, a set of numbers that a machine learning model can work with. We'll look at one in just a second.","8ce9c6c2":"### SVC","512c36cb":"Let's test our model and see how well it does on the training data. For this we'll use `cross-validation` - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n\nThe metric for this competition is F1, so let's use that here.","1d15cece":"The above tells us that:\n1. There are 73 unique words (or \"tokens\") in the first five tweets.\n2. The first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\n\nNow let's create vectors for all of our tweets.","829a1f6c":"#### Ridge classification using TFIDF","c9e1c966":"# TF-IDF (Term frequesncy and inverse document frequency)\n\nTF-IDF resolves the above problem by identifying the importance of a word.\n\nTerm frequesncy measures the frequency of a word in the document.\nInverse document measures how rare the word is in a corpus.\n\nThe product of these two values result in 'Importance of a word'\n","23f24509":"# NLP : Disaster Tweets\n\nNLP - or *Natural Language Processing* - is shorthand for a wide array of techniques designed to help machines learn from text. Natural Language Processing powers everything from chatbots to search engines, and is used in diverse tasks like sentiment analysis and machine translation.\n\nIn this notebook we'll look at this disaster tweets dataset, use a simple technique to process it,  the classification problem, build a machine learning model, and submit predictions for a score.","234a2f94":"# Word Cloud for disaster tweets","8e32019e":"Read the train and test datasets.","b3e600f2":"# Our model\n\nAs we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n\nWhat we're assuming here is a _linear_ connection. So let's build a linear model and see!","5ff4970a":"### Word Cloud\n\nWord Clouds are effective ways to highlight popular and trending words in text data based on frequency\nHere we can see 'flood','fire','suicide bomber','MH370 Malaysia','wild fire' are mostly used in the tweets.","5b914707":"### Ridge Regression"}}