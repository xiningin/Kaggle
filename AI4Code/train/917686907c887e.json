{"cell_type":{"11e278bf":"code","ce6d0c59":"code","3e6fe115":"code","2cf5d6c9":"code","d4f03342":"code","3c0fd0db":"code","369c1b80":"code","6acbaf27":"code","2c0737cc":"code","7bb0ced1":"code","9da4d549":"code","2cfa281e":"code","69c4bc58":"code","f65ab776":"code","2ccc127f":"code","c89de74e":"code","8ce714a5":"code","eb59d1d8":"code","5da14937":"code","06b19322":"code","14436c87":"markdown","cd610b0d":"markdown","14f60ecc":"markdown","f3db9ed3":"markdown","34e22e9e":"markdown","2fd1bb0c":"markdown","04b48105":"markdown","08b9b370":"markdown","f913ec21":"markdown","f3bf80fa":"markdown","ff8645d0":"markdown","e02f0345":"markdown","c48bf619":"markdown","99704f27":"markdown","ca69e724":"markdown","8ee16fbd":"markdown","aaae24eb":"markdown","5b1227fd":"markdown"},"source":{"11e278bf":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport seaborn as sns\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\n# Import the 3 dimensionality reduction methods\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA","ce6d0c59":"df = pd.read_csv('..\/input\/iris-dataset\/iris.data.csv', header=None, sep=',')","3e6fe115":"df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\ndf.dropna(how=\"all\", inplace=True) # drops the empty line at file-end\n\ndf.tail()","2cf5d6c9":"# split data table into data X and class labels y\n\nX = df.iloc[:,0:4].values\ny = df.iloc[:,4].values","d4f03342":"\n\n# plotting histograms\ndata = []\n\nlegend = {0:False, 1:False, 2:False, 3:True}\n\ncolors = {'Iris-setosa': '#0D76BF', \n          'Iris-versicolor': '#00cc96', \n          'Iris-virginica': '#EF553B'}\n\nfor col in range(4):\n    for key in colors:\n        trace = dict(\n            type='histogram',\n            x=list(X[y==key, col]),\n            opacity=0.75,\n            xaxis='x%s' %(col+1),\n            marker=dict(color=colors[key]),\n            name=key,\n            showlegend=legend[col]\n        )\n        data.append(trace)\n\nlayout = dict(\n    barmode='overlay',\n    xaxis=dict(domain=[0, 0.25], title='sepal length (cm)'),\n    xaxis2=dict(domain=[0.3, 0.5], title='sepal width (cm)'),\n    xaxis3=dict(domain=[0.55, 0.75], title='petal length (cm)'),\n    xaxis4=dict(domain=[0.8, 1], title='petal width (cm)'),\n    yaxis=dict(title='count'),\n    title='Distribution of the different Iris flower features'\n)\n\nfig = dict(data=data, layout=layout)\n#py.iplot(fig, filename='exploratory-vis-histogram')","3c0fd0db":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)","369c1b80":"\nimport numpy as np\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) \/ (X_std.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","6acbaf27":"\nprint('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))\n","2c0737cc":"cov_mat = np.cov(X_std.T)\n\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","7bb0ced1":"cor_mat1 = np.corrcoef(X_std.T)\n\neig_vals, eig_vecs = np.linalg.eig(cor_mat1)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","9da4d549":"cor_mat2 = np.corrcoef(X.T)\n\neig_vals, eig_vecs = np.linalg.eig(cor_mat2)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","2cfa281e":"u,s,v = np.linalg.svd(X_std.T)\nu","69c4bc58":"\nfor ev in eig_vecs:\n    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\nprint('Everything ok!')","f65ab776":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort()\neig_pairs.reverse()\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","2ccc127f":"tot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\ntrace1 = dict(\n    type='bar',\n    x=['PC %s' %i for i in range(1,5)],\n    y=var_exp,\n    name='Individual'\n)\n\ntrace2 = dict(\n    type='scatter',\n    x=['PC %s' %i for i in range(1,5)], \n    y=cum_var_exp,\n    name='Cumulative'\n)\n\ndata = [trace1, trace2]\n\nlayout=dict(\n    title='Explained variance by different principal components',\n    yaxis=dict(\n        title='Explained variance in percent'\n    ),\n    annotations=list([\n        dict(\n            x=1.16,\n            y=1.05,\n            xref='paper',\n            yref='paper',\n            text='Explained Variance',\n            showarrow=False,\n        )\n    ])\n)\nfig = dict(data=data, layout=layout)\n#py.iplot(fig, filename='')","c89de74e":"matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), \n                      eig_pairs[1][1].reshape(4,1)))\n\nprint('Matrix W:\\n', matrix_w)","8ce714a5":"\nY = X_std.dot(matrix_w)","eb59d1d8":"data = []\n\nfor name, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), colors.values()):\n    trace = dict(\n        type='scatter',\n        x=Y[y==name,0],\n        y=Y[y==name,1],\n        mode='markers',\n        name=name,\n        marker=dict(\n            color=col,\n            size=12,\n            line=dict(\n                color='rgba(217, 217, 217, 0.14)',\n                width=0.5),\n            opacity=0.8)\n    )\n    data.append(trace)\n\nlayout = dict(\n    showlegend=True,\n    scene=dict(\n        xaxis=dict(title='PC1'),\n        yaxis=dict(title='PC2')\n    )\n)\n\nfig = dict(data=data, layout=layout)\n#py.iplot(fig, filename='projection-matrix')","5da14937":"from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std)","06b19322":"data = []\n\nfor name, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), colors.values()):\n\n    trace = dict(\n        type='scatter',\n        x=Y_sklearn[y==name,0],\n        y=Y_sklearn[y==name,1],\n        mode='markers',\n        name=name,\n        marker=dict(\n            color=col,\n            size=12,\n            line=dict(\n                color='rgba(217, 217, 217, 0.14)',\n                width=0.5),\n            opacity=0.8)\n    )\n    data.append(trace)\n\nlayout = dict(\n        xaxis=dict(title='PC1', showline=False),\n        yaxis=dict(title='PC2', showline=False)\n)\nfig = dict(data=data, layout=layout)\n#py.iplot(fig, filename='pca-scikitlearn')","14436c87":"After sorting the eigenpairs, the next question is \"how many principal components are we going to choose for our new feature subspace?\" A useful measure is the so-called \"explained variance,\" which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components.","cd610b0d":"**Covariance Matrix**\n\nThe classic approach to PCA is to perform the eigendecomposition on the covariance matrix \u03a3, which is a d\u00d7d matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:\n\n\u03c3jk=1n\u22121\u2211Ni=1(xij\u2212x\u00afj)(xik\u2212x\u00afk).\nWe can summarize the calculation of the covariance matrix via the following matrix equation:\n\u03a3=1n\u22121((X\u2212x\u00af)T(X\u2212x\u00af))\nwhere x\u00af is the mean vector x\u00af=\u2211k=1nxi.\nThe mean vector is a d-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset.","14f60ecc":"**Preparing the Iris Dataset**\n\nFor the following tutorial, we will be working with the famous \"Iris\" dataset that has been deposited on the UCI machine learning repository\n(https:\/\/archive.ics.uci.edu\/ml\/datasets\/Iris).\n\nThe iris dataset contains measurements for 150 iris flowers from three different species.\n\nThe three classes in the Iris dataset are:\n\nIris-setosa (n=50)\nIris-versicolor (n=50)\nIris-virginica (n=50)\nAnd the four features of in Iris dataset are:\n\nsepal length in cm\nsepal width in cm\npetal length in cm\npetal width in cm\nLoading the Dataset\nIn order to load the Iris data directly from the UCI repository, we are going to use the superb pandas library. If you haven't used pandas yet, I want encourage you to check out the pandas tutorials. If I had to name one Python library that makes working with data a wonderfully simple task, this would definitely be pandas!","f3db9ed3":"**Shortcut - PCA in scikit-learn**\n\nFor educational purposes, we went a long way to apply the PCA to the Iris dataset. But luckily, there is already implementation in scikit-learn.","34e22e9e":"In order to decide which eigenvector(s) can be dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\nIn order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top k eigenvectors.","2fd1bb0c":"The plot above clearly shows that most of the variance (72.77% of the variance to be precise) can be explained by the first principal component alone. The second principal component still bears some information (23.03%) while the third and fourth principal components can safely be dropped without losing to much information. Together, the first two principal components contain 95.8% of the information.\n\nIt's about time to get to the really interesting part: The construction of the projection matrix that will be used to transform the Iris data onto the new feature subspace. Although, the name \"projection matrix\" has a nice ring to it, it is basically just a matrix of our concatenated top k eigenvectors.\n\nHere, we are reducing the 4-dimensional feature space to a 2-dimensional feature subspace, by choosing the \"top 2\" eigenvectors with the highest eigenvalues to construct our d\u00d7k-dimensional eigenvector matrix W.","04b48105":"**A Summary of the PCA Approach**\n\nStandardize the data.\nObtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\nSort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues where k is the number of dimensions of the new feature subspace (k\u2264d)\/.\nConstruct the projection matrix W from the selected k eigenvectors.\nTransform the original dataset X via W to obtain a k-dimensional feature subspace Y.","08b9b370":"**Correlation Matrix**\n\nEspecially, in the field of \"Finance,\" the correlation matrix typically used instead of the covariance matrix. However, the eigendecomposition of the covariance matrix (if the input data was standardized) yields the same results as a eigendecomposition on the correlation matrix, since the correlation matrix can be understood as the normalized covariance matrix. Eigendecomposition of the standardized data based on the correlation matrix:","f913ec21":"Standardizing\nWhether to standardize the data prior to a PCA on the covariance matrix depends on the measurement scales of the original features. Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales. Although, all features in the Iris dataset were measured in centimeters, let us continue with the transformation of the data onto unit scale (mean=0 and variance=1), which is a requirement for the optimal performance of many machine learning algorithms.","f3bf80fa":"**Exploratory Visualization**\n\nTo get a feeling for how the 3 different flower classes are distributes along the 4 different features, let us visualize them via histograms.","ff8645d0":"**2 - Selecting Principal Components**\n\nThe typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1, which can confirmed by the following two lines of code:","e02f0345":"\nEigendecomposition of the raw data based on the correlation matrix:","c48bf619":"We can clearly see that all three approaches yield the same eigenvectors and eigenvalue pairs:\n\nEigendecomposition of the covariance matrix after standardizing the data.\nEigendecomposition of the correlation matrix.\nEigendecomposition of the correlation matrix after standardizing the data.","99704f27":"**1 - Eigendecomposition - Computing Eigenvectors and Eigenvalues**\n\nThe eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.","ca69e724":"**3 - Projection Onto the New Feature Space**\n\nIn this last step we will use the 4\u00d72-dimensional projection matrix W to transform our samples onto the new subspace via the equation\nY=X\u00d7W, where Y is a 150\u00d72 matrix of our transformed samples.","8ee16fbd":"Our iris dataset is now stored in form of a 150\u00d74 matrix where the columns are the different features, and every row represents a separate flower sample. Each sample row x can be pictured as a 4-dimensional vector\n\nxT=\u239b\u239d\u239c\u239c\u239cx1x2x3x4\u239e\u23a0\u239f\u239f\u239f=\u239b\u239d\u239c\u239c\u239csepal lengthsepal widthpetal lengthpetal width\u239e\u23a0\u239f\u239f\u239f","aaae24eb":"**Singular Vector Decomposition******\n\nWhile the eigendecomposition of the covariance or correlation matrix may be more intuitiuve, most PCA implementations perform a Singular Vector Decomposition (SVD) to improve the computational efficiency. So, let us perform an SVD to confirm that the result are indeed the same:","5b1227fd":"**Loading the Dataset**\n\nIn order to load the Iris data directly from the UCI repository, we are going to use the superb pandas library. If you haven't used pandas yet, I want encourage you to check out the pandas tutorials. If I had to name one Python library that makes working with data a wonderfully simple task, this would definitely be pandas!\n"}}