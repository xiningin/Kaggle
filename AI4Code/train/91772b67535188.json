{"cell_type":{"46067999":"code","86942833":"code","11ac0f74":"code","4e239249":"code","085a6bf9":"code","7d46cae3":"code","a65811c9":"code","da45e5a1":"code","3774105d":"code","64f04e3a":"code","e2e6bb7d":"code","c634e83f":"code","5f576e12":"code","0c565fec":"code","5e2ac1f3":"markdown","2119ea12":"markdown","e330ab55":"markdown","cdf01e8c":"markdown","c3519023":"markdown","f94f3496":"markdown","b85ee01c":"markdown","fde263c5":"markdown","5ec9e030":"markdown","21bb32d8":"markdown","f0c46d8d":"markdown","6630547d":"markdown","562282af":"markdown","738bdc64":"markdown","ceedc75d":"markdown","7833452b":"markdown","b6ad8251":"markdown","ecc1aac1":"markdown"},"source":{"46067999":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nfrom sklearn.metrics import r2_score, accuracy_score\n%matplotlib inline\n\nfrom sklearn import metrics, linear_model, svm, neighbors, gaussian_process, naive_bayes, tree, ensemble, neural_network\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86942833":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.head()","11ac0f74":"pp.ProfileReport(df)","4e239249":"#Drop the duplicate values (one row)\ndf = df.drop_duplicates()","085a6bf9":"cat_val = []\ncont_val = []\nfor column in df.columns:\n    if len(df[column].unique()) <= 10:\n        cat_val.append(column)\n    else:\n        cont_val.append(column)\ncat_val","7d46cae3":"cat_val.remove('target')\ndf = pd.get_dummies(df, columns = cat_val)","a65811c9":"y = df['target']\nx = df.drop(['target'], axis = 1)\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=40)","da45e5a1":"scaler = MinMaxScaler()\nscaler2 = RobustScaler()\n\nxtrain[cont_val] = scaler.fit_transform(xtrain[cont_val])\nxtrain[cont_val] = scaler2.fit_transform(xtrain[cont_val])\n\nxtest[cont_val] = scaler.transform(xtest[cont_val]) \nxtest[cont_val] = scaler2.transform(xtest[cont_val])","3774105d":"xtrain.head()","64f04e3a":"xtest.head()","e2e6bb7d":"model_Results_Supervised = {\n    'LogRegres': {\n        'model': linear_model.LogisticRegression(),\n        'params': {\n            'C': range(1, 3),\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n        }\n    },\n    'Ridge': {\n        'model': linear_model.RidgeClassifier(),\n        'params': {\n            'alpha': range(1, 5)\n        }\n    },\n    'SGD': {\n        'model': linear_model.SGDClassifier(),\n        'params': {\n            'alpha': [0.35, 0.40, 0.45, 0.5, 0.55, 0.6, 0.65]\n        }\n    },\n    'Perceptron': {\n        'model': linear_model.Perceptron(),\n        'params': {\n        }\n    },\n    'PAC': {\n        'model': linear_model.PassiveAggressiveClassifier(),\n        'params': {\n            'C': range(1, 10)\n        }\n    },\n    'SVC': {\n        'model': svm.SVC(),\n        'params': {\n            'C': range(1, 5),\n            'kernel': ['rbf', 'linear']\n        }\n    },\n    'nuSVC': {\n        'model': svm.NuSVC(),\n        'params': {\n            'nu': [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1],\n            'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n        }\n    },\n    'LinearSVC': {\n        'model': svm.LinearSVC(),\n        'params': {\n            'loss': ['hinge', 'squared_hinge'],\n            'C': range(1, 10)\n        }\n    },\n    'KNN': {\n        'model': neighbors.KNeighborsClassifier(),\n        'params': {\n            'n_neighbors': range(1, 30)\n        }\n    },\n    'NearestCentroid': {\n        'model': neighbors.NearestCentroid(),\n        'params': {\n        }\n    },\n    'GPC': {\n        'model': gaussian_process.GaussianProcessClassifier(),\n        'params': {\n        }\n    },\n    'DecisionTree': {\n        'model': tree.DecisionTreeClassifier(),\n        'params': {\n            'min_samples_leaf': range(1, 20)\n    },\n    'Bagging': {\n        'model': ensemble.BaggingClassifier(),\n        'params': {\n            'max_features': [0.8, 0.85, 0.9, 0.95, 1.00],\n            'n_estimators': range(1, 10)\n        }\n    },\n    'ExtraTrees': {\n        'model': ensemble.ExtraTreesClassifier(),\n        'params': {\n            'n_estimators': [115],\n            'max_features': range(5, 15)\n        }\n    },\n    'AdaBoost': {\n        'model': ensemble.AdaBoostClassifier(),\n        'params': {\n            'learning_rate': [0.09, 0.095, 0.1, 0.105, 0.105, 0.11, 0.115, 0.12]\n        }\n    },\n    'GradientBoost': {\n        'model': ensemble.GradientBoostingClassifier(),\n        'params': {\n            'learning_rate': [0.01, 0.02, 0.03, 0.04,0.05, 0.06]\n        }\n    },\n    'XGBoost': {\n        'model': xgb.XGBClassifier(objective='reg:squarederror'),\n        'params': {\n            'learning_rate': [0.08, 0.09, 0.1],\n            'max_depth': range(1, 3)\n        }\n    },\n    'MLP': {\n        'model': neural_network.MLPClassifier(),\n        'params': {\n            'hidden_layer_sizes': range(2, 5),\n            'solver': ['lbfgs', 'sgd', 'adam']\n        }\n    }\n}\n}","c634e83f":"#Fitting the data and putting model metrics in a readable table to validate the models\nscores = []\n\nfor model_Name, mp, in model_Results_Supervised.items():\n\n    clf = GridSearchCV(mp['model'], mp['params'], cv = 10)\n    clf.fit(xtrain, ytrain)\n    \n    pred_train = clf.predict(xtrain)\n    pred_test = clf.predict(xtest)\n\n    r2_acc_train = r2_score(ytrain, pred_train) * 100\n    r2_acc_test = r2_score(ytest, pred_test) * 100\n            \n    acc_train = metrics.accuracy_score(ytrain, pred_train) * 100\n    acc_test = metrics.accuracy_score(ytest, pred_test) * 100\n                \n    scores.append({\n    'model': mp['model'],\n    'r2_train_score': r2_acc_train,\n    'r2_test_score': r2_acc_test,\n    'acc_train': acc_train,\n    'acc_test': acc_test,\n    'best_params': clf.best_params_\n})\n    model_table = pd.DataFrame(scores, columns = ['model', 'r2_train_score', 'r2_test_score', 'acc_train', 'acc_test', 'best_params'])\n    model_table = model_table.sort_values(by=['acc_test'], ascending=False)","5f576e12":"model_table","0c565fec":"model_table.head()","5e2ac1f3":"# Explaining Dataset","2119ea12":"It is important to explain what the data we are looking at is. To the vast majority of people, these variables below (cp, trestbps, restecg, etc) are not well understood. ","e330ab55":"Using many popular modeling techniques, we will finetune our models to give us the method which gives us the best accuracy on the data.","cdf01e8c":"We also know that there are categorical features and we need to make dummy features for them. \n\nRather than check individually for them, let's make an algorithm to test for them below:","c3519023":"# 2. Downloading Dataset","f94f3496":"As we can see, the training and testing data is now scaled to much lower values after scaling.","b85ee01c":"# 4. Prepare for modeling","fde263c5":"Let's display the top scoring models.","5ec9e030":"# 6. Model evalutions","21bb32d8":"# 7. Conclusion","f0c46d8d":"We can tell a few things from this dataset:\n\n* There are 165 people with heart disease and 138 people without, meaning the data is well balanced\n* There are no null values\n* There is one duplicate row","6630547d":"# 1. Importing Libraries","562282af":"Great! Now lets create these new dummies using Pandas get_dummies function","738bdc64":"**age** - age in years\n\n**sex** - (1 = male; 0 = female)\n\n**cp** - chest pain type\n0: Typical angina: chest pain related decrease blood supply to the heart\n1: Atypical angina: chest pain not related to heart\n2: Non-anginal pain: typically esophageal spasms (non heart related)\n3: Asymptomatic: chest pain not showing signs of disease\n\n**trestbps** - resting blood pressure (in mm Hg) anything above 130-140 is typically cause for concern\n\n**chol** - serum cholestoral in mg\/dl; above 200 is cause for concern\n\n**fbs** - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false); '>126' mg\/dL signals diabetes\n\n**restecg** - resting electrocardiographic results\n0: Nothing to note\n1: ST-T Wave abnormality; signals non-normal heart beat\n2: Possible or definite left ventricular hypertrophy: muscle wall of the heart\u2019s left pumping chamber, ventricle, becomes thickened\n\n**thalach** - maximum heart rate achieved\n\n**exang** - exercise induced angina (1 = yes; 0 = no)\n\n**oldpeak** - ST depression induced by exercise relative to rest; more stress is a cause for concern\n\n**slope** - the slope of the peak exercise ST segment\n0: Upsloping: better heart rate with excercise (uncommon)\n1: Flatsloping: minimal change (typical healthy heart)\n2: Downslopins: signs of unhealthy heart\n\n**ca** - number of major vessels (0-3) colored by flourosopy\ncolored means good blood flow \n\n**thal** - thalium stress result\n1,3: normal\n6: fixed defect: prior defect\n7: reversable defect: no proper blood movement when excercising\n\n**target** - have disease or not (1=yes, 0=no)","ceedc75d":"# 5. Tune models","7833452b":"It is always good practice to split the dataset into training and testing before you scale the data to prevent [data leakage.](https:\/\/towardsdatascience.com\/data-leakage-in-machine-learning-10bdd3eec742#:~:text=Data%20Leakage%20in%20Machine%20Learning%201%20Introduction.%20When,training%20data-sets.%203%20Causes%20of%20Data%20Leakage.%20)\nIn short, scaling the data is necessary because it reduces variability of magnitude in our dataset. This is a problem because many algorithms use [Euclidean Distance](https:\/\/en.wikipedia.org\/wiki\/Euclidean_distance) calculations, essentially meaning finding the distance between two points. Varied magnitudes in our data mess up this distance. \n\nFor example, someone who is age 37 can have a chol of 300, but because the cholestrol levels are almost 10x higher than the age, the cholesteral has a much bigger weight than the age does (much farther away). MinMax scaling reduces the values between (0, 1) but it takes into account the entire column. We want to split the data first because we don't want any test data to *leak* into our training data to give us an unfair advantage.","b6ad8251":"# 3. EDA","ecc1aac1":"To go further in depth on our dataset, an Exploratory Data Analysis, or EDA, is necessary to give a general idea of our dataset. Giving appropriate statistical trends and analysis can show if how consistent and well-conducted the dataset is."}}