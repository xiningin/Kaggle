{"cell_type":{"18cbdc32":"code","80ec7429":"code","1e418ceb":"code","755137d3":"code","1a7a9a0f":"code","bc6d1e20":"code","26aa5b23":"code","bed98eee":"code","593a1bcf":"code","5ab13544":"code","20e60c2f":"code","b71b26b2":"code","2e217ada":"code","8b61f59e":"code","4d9e24a5":"code","e4ba1e47":"code","022ac8de":"code","af2ff403":"code","74d20dca":"code","94036b9b":"code","5de07fc6":"code","88602466":"code","bd4d7e9b":"code","dc4f2359":"code","63a99aa4":"code","4d1c6aa5":"code","38a0d824":"code","453b842d":"code","c5e76f4e":"code","0f26490a":"code","d94744ab":"code","3d3920bb":"code","1f1796c4":"code","af7309ce":"code","17f8a480":"code","036b41ea":"code","52d836d5":"code","8b95fb88":"code","3f40c58d":"code","79f57de7":"code","69d88cd9":"code","cfd1042a":"code","e1bbb65a":"code","c4e87188":"code","e135c45a":"code","05079a3a":"code","f9142164":"code","96e5aa8c":"code","7de74f41":"code","9038cede":"code","76123868":"code","96fc476a":"code","c073ab5b":"code","39364119":"code","78b5b521":"code","ec079183":"code","db4337e8":"code","a91cdf31":"code","2954e2d0":"code","b370d415":"code","740590a7":"code","fec0787b":"code","d505222f":"code","e8c9595e":"code","786042e0":"code","1f7a0977":"markdown","f18beb18":"markdown","67bb1200":"markdown","4e6978a8":"markdown"},"source":{"18cbdc32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80ec7429":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\npaths = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname, filename))","1e418ceb":"paths","755137d3":"train0 = '\/kaggle\/input\/homoglyph-domain-attacks\/domains_train.txt'\ntrain1 = '\/kaggle\/input\/homoglyph-domain-attacks\/fake_train.txt'","1a7a9a0f":"\n\nwith open(train0, 'rb') as f:\n  text = f.read()","bc6d1e20":"mylines = []                              # Declare an empty list\nwith open (train0, 'rb') as myfile:  # Open file lorem.txt\n    for line in myfile:                   # For each line of text,\n        mylines.append(line)              # add that line to the list.\n    for element in mylines:               # For each element in the list,\n        print(element, end='')","26aa5b23":"line=[]\nfor l in mylines:\n    line.append(l)\n","bed98eee":"mylines[2]","593a1bcf":"import re\ntarget_string = str(line)\n# all word starts with substring 'ma' and ends with substring 'go'\ntext = re.findall(r'\\b[a-zA-Z]\\w+.comq\\b', target_string, re.I)\n# output 'mango'\ntext","5ab13544":"target_string = str(line)\n# all word starts with substring 'ma' and ends with substring 'go'\ntext1 = re.findall(r'\\b[a-zA-Z]\\w+.comr\\b', target_string, re.I)\n# output 'mango'","20e60c2f":"len(text1)","b71b26b2":"t1 = []\nfor t in text1:\n    temp = str(t)\n    temp = t.replace(\"x00\",\"\")\n    temp = temp.replace(\"comr\",\"com\")\n    t1.append(temp)\n    ","2e217ada":"#final Process data for Domain_link Train.\nt1","8b61f59e":"t2 = []\nfor t in text1:\n    temp = str(t)\n    temp = t.replace(\"x00\",\"\")\n    temp = temp.replace(\"comr\",\"com\")\n    t2.append(temp)\n    ","4d9e24a5":"t2","e4ba1e47":"train = t1+t2","022ac8de":"train[0]","af2ff403":"!pip install xlsxwriter","74d20dca":"# Writing to an excel \n# sheet using Python\nimport xlsxwriter\n \nworkbook = xlsxwriter.Workbook('Domains_train.xlsx')\nworksheet = workbook.add_worksheet()\n \n# Start from the first cell.\n# Rows and columns are zero indexed.\nrow = 0\ncolumn = 0\nworksheet.write(row, column, \"links\")\nrow = 1\nfor item in train :\n \n    # write operation perform\n    worksheet.write(row, column, item)\n \n    # incrementing the value of row by one\n    # with each iterations.\n    row += 1\n     \nworkbook.close()","94036b9b":"train1","5de07fc6":"mylines = []                              # Declare an empty list\nwith open (train1, 'rb') as myfile:  # Open file lorem.txt\n    for line in myfile:                   # For each line of text,\n        mylines.append(line)              # add that line to the list.\n    for element in mylines:               # For each element in the list,\n        print(element, end='')","88602466":"line=[]\nfor l in mylines:\n    line.append(l)\n","bd4d7e9b":"target_string = str(line)\n\ntext = re.findall(r'\\b[a-zA-Z]\\w+.comq\\b', target_string, re.I)\n","dc4f2359":"text","63a99aa4":"len(text)","4d1c6aa5":"t1 = []\nfor t in text:\n    temp = str(t)\n    temp = t.replace(\"x00\",\"\")\n    temp = temp.replace(\"comq\",\"com\")\n    temp = temp.replace(\"x\",\"\")\n    temp = temp.replace(\"b0\",\"\")\n    \n    t1.append(temp)\n    ","38a0d824":"t1","453b842d":"target_string = str(line)\n\ntext1 = re.findall(r'\\b[a-zA-Z]\\w+.comr\\b', target_string, re.I)\n","c5e76f4e":"text1","0f26490a":"len(text1)","d94744ab":"t2 = []\nfor t in text1:\n    temp = str(t)\n    temp = t.replace(\"x00\",\"\")\n    temp = temp.replace(\"comr\",\"com\")\n    temp = temp.replace(\"x\",\"\")\n    temp = temp.replace(\"b0\",\"\")\n\n    t2.append(temp)\n    ","3d3920bb":"train = t1+t2","1f1796c4":"len(train)","af7309ce":"train","17f8a480":"train[0]","036b41ea":"# Writing to an excel \n# sheet using Python\nimport xlsxwriter\n \nworkbook = xlsxwriter.Workbook('fake_train.xlsx')\nworksheet = workbook.add_worksheet()\n \n# Start from the first cell.\n# Rows and columns are zero indexed.\nrow = 0\ncolumn = 0\nworksheet.write(row, column, \"links\")\nrow = 1\nfor item in train :\n \n    # write operation perform\n    worksheet.write(row, column, item)\n \n    # incrementing the value of row by one\n    # with each iterations.\n    row += 1\n     \nworkbook.close()","52d836d5":"!pip install xlsxwriter","8b95fb88":"!pip install openpyxl","3f40c58d":"X1 = pd.read_excel('.\/Domains_train.xlsx')\nX1[\"label\"] = 1\nX1","79f57de7":"X2 = pd.read_excel('.\/fake_train.xlsx')\nX2[\"label\"] = 0\nX2","69d88cd9":"result = X1.append([X2])","cfd1042a":"result","e1bbb65a":"#Shuffle train dataset\nresult = result.sample(frac = 1)\nX,y = result[\"links\"],result[\"label\"]","c4e87188":"import xlsxwriter\n \nworkbook = xlsxwriter.Workbook('full_dataset.xlsx')\nworksheet = workbook.add_worksheet()\n \n# Start from the first cell.\n# Rows and columns are zero indexed.\nrow = 0\ncolumn = 0\n \nlinks =  result[\"links\"]\nlabel = result[\"label\"]\nworksheet.write(row, column, \"links\")\nrow=1\n# iterating through content list\nfor item in links :\n \n    # write operation perform\n    worksheet.write(row, column, item)\n \n    # incrementing the value of row by one\n    # with each iterations.\n    row += 1\n    \ncolumn=1\nrow=0\nworksheet.write(row, column, \"label\")\nrow=1\nfor l in label :\n \n    # write operation perform\n    worksheet.write(row, column, l)\n \n    # incrementing the value of row by one\n    # with each iterations.\n    row += 1\n     \nworkbook.close()","e135c45a":"import numpy as np\nimport regex as re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statistics\nimport math\nimport os\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\nfrom transformers import RobertaTokenizer, TFRobertaModel\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","05079a3a":"# Detect hardware, return appropriate distribution strategy (you can see that it is pretty easy to set up).\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set (always set in Kaggle)\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)","f9142164":"import numpy as np\nimport regex as re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statistics\nimport math\nimport os\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\nfrom transformers import RobertaTokenizer, TFRobertaModel\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","96e5aa8c":"import pandas as pd\ndf = pd.read_excel(\".\/full_dataset.xlsx\")","7de74f41":"df","9038cede":"X_data = df[['links']].to_numpy().reshape(-1)\ny_data = df[['label']].to_numpy().reshape(-1)","76123868":"import numpy as np\ncategories = df[['label']].values.reshape(-1)\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ncounter_categories = Counter(categories)\ncategory_names = counter_categories.keys()\ncategory_values = counter_categories.values()\n\ny_pos = np.arange(len(category_names))\n\nplt.figure(1, figsize=(10, 5))\nplt.bar(y_pos, category_values, align='center', alpha=0.5)\nplt.xticks(y_pos, category_names)\nplt.ylabel('Number of texts')\nplt.title('Distribution of texts per category')\nplt.gca().yaxis.grid(True)\nplt.show()\n\nprint(counter_categories)","96fc476a":"MAX_LEN = 256\ndef roberta_encode(texts, tokenizer):\n    ct = len(texts)\n    \n    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n\n    for k, text in enumerate(texts):\n        # Tokenize\n        tok_text = tokenizer.tokenize(text)\n        \n        # Truncate and convert tokens to numerical IDs\n        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n        \n        input_length = len(enc_text) + 2\n        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n        \n        # Add tokens [CLS] and [SEP] at the beginning and the end\n        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n        \n        # Set to 1s in the attention input\n        attention_mask[k,:input_length] = 1\n\n    return {\n        'input_word_ids': input_ids,\n        'input_mask': attention_mask,\n        'input_type_ids': token_type_ids\n    }","c073ab5b":"category_to_id = {}\ncategory_to_name = {}\n\nfor index, c in enumerate(y_data):\n    if c in category_to_id:\n        category_id = category_to_id[c]\n    else:\n        category_id = len(category_to_id)\n        category_to_id[c] = category_id\n        category_to_name[category_id] = c\n    \n    y_data[index] = category_id\n\n# Display dictionary\ncategory_to_name","39364119":"n_texts = len(X_data)\nprint('Texts in dataset: %d' % n_texts)\n\ncategories = df['label'].unique()\nn_categories = len(categories)\nprint('Number of categories: %d' % n_categories)\n\nprint('Done!')","78b5b521":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=777)","ec079183":"MODEL_NAME = \"roberta-base\"\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")","db4337e8":"\nX_train = roberta_encode(X_train, tokenizer)\nX_test = roberta_encode(X_test, tokenizer)\n\ny_train = np.asarray(y_train, dtype='int32')\ny_test = np.asarray(y_test, dtype='int32')","a91cdf31":"!pip install keras","2954e2d0":"def build_model(n_categories):\n    MAX_LEN=256\n    with strategy.scope():\n        input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n        input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n        input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n\n        # Import RoBERTa model from HuggingFace\n        roberta_model = TFRobertaModel.from_pretrained(MODEL_NAME)\n        x = roberta_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n\n        # Huggingface transformers have multiple outputs, embeddings are the first one,\n        # so let's slice out the first position\n        x = x[0]\n\n        x = tf.keras.layers.Dropout(0.1)(x)\n        x = tf.keras.layers.Flatten()(x)\n        x = tf.keras.layers.Dense(256, activation='relu')(x)\n        x = tf.keras.layers.Dense(n_categories, activation='softmax')(x)\n\n        model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\n\n        return model","b370d415":"\nwith  strategy.scope():\n    model = build_model(n_categories)\n    model.summary()","740590a7":"with strategy.scope():\n    print('Training...')\n    history = model.fit(X_train,\n                        y_train,\n                        epochs=3,\n                        batch_size=8 * strategy.num_replicas_in_sync,\n                        verbose=1,\n                        validation_data=(X_test, y_test))","fec0787b":"# This plot will look much better if we train models with more epochs, but anyway here is\nplt.figure(figsize=(10, 10))\nplt.title('Accuracy')\n\nxaxis = np.arange(len(history.history['accuracy']))\nplt.plot(xaxis, history.history['accuracy'], label='Train set')\nplt.plot(xaxis, history.history['val_accuracy'], label='Validation set')\nplt.legend()","d505222f":"def plot_confusion_matrix(X_test, y_test, model):\n    y_pred = model.predict(X_test)\n    y_pred = [np.argmax(i) for i in model.predict(X_test)]\n\n    con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()\n\n    con_mat_norm = np.around(con_mat.astype('float') \/ con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n    label_names = list(range(len(con_mat_norm)))\n\n    con_mat_df = pd.DataFrame(con_mat_norm,\n                              index=label_names, \n                              columns=label_names)\n\n    figure = plt.figure(figsize=(10, 10))\n    sns.heatmap(con_mat_df, cmap=plt.cm.Blues, annot=True)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","e8c9595e":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1] * 100))","786042e0":"plot_confusion_matrix(X_test, y_test, model)","1f7a0977":"# Data Preprocessing ","f18beb18":"# Covert fake train to csv And also preprocess the data","67bb1200":"# Model Traning TPU Must On","4e6978a8":"target_string = str(line)\n# all word starts with substring 'ma' and ends with substring 'go'\ntext = re.findall(r'\\b[a-zA-Z]\\w+.comq\\b', target_string, re.I)\n# output 'mango'"}}