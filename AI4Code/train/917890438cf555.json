{"cell_type":{"efadc537":"code","0afacd97":"code","0b3b7cc1":"code","48829de3":"code","c2c17d2f":"code","c0675d70":"code","138bf3da":"code","70a1ace4":"code","6caa10cb":"code","c3ef2191":"code","a0d8ddcd":"code","5e6d2f27":"code","ab356836":"code","62f6d11a":"code","3b461d60":"code","3955df0e":"code","9e87ea5f":"code","0d20ff07":"code","7de36930":"code","5c9f8b76":"code","8ec9c146":"code","55a16f37":"code","a0cdee70":"code","d35c9940":"code","3ec99d75":"code","cbe97d3b":"code","cfec9bee":"code","1625dd39":"code","eaae8402":"code","5cbda83b":"code","14cc5edc":"code","54d7c655":"code","9d23d195":"code","100ad6fd":"code","4dbb37e4":"code","544cbe4d":"code","b54e4f80":"code","b76d0667":"code","322bf969":"code","6d266311":"code","4eae9414":"code","c9202dc0":"code","e2257565":"code","1ab7c78e":"code","c4c5f2d0":"code","6910b034":"code","8208f626":"code","75470535":"code","72ca1227":"code","3d117487":"code","036f9847":"code","32a20708":"code","698baff7":"code","d121b5e1":"code","0fec697f":"code","ae2010c5":"code","5263d1f4":"code","410932ed":"code","1dfadfd6":"code","6254b3d0":"code","50d71b93":"code","c5359504":"code","7565d343":"code","e8624378":"code","eea4d070":"code","c29f1ced":"code","ef2614d5":"code","ac3978c1":"code","07afb18b":"code","7ac666f3":"code","4080ce6f":"code","195a0f0b":"code","2cec77ba":"code","661b8f42":"code","20f13c2b":"code","280b53e7":"code","071ca933":"code","09eeb2bb":"code","d2641d0c":"code","2b763a79":"code","6ccfa373":"code","b47e12fa":"code","b44e7186":"code","bf8680b1":"code","c3af8063":"code","9da9a832":"code","9ae1fbdc":"code","fc2d760f":"code","fbbdd966":"code","3c950640":"code","a402ef75":"code","9045b795":"code","888bc5e4":"code","1a802995":"code","fd88ec35":"code","b20d4b76":"code","f0d1b430":"code","1c2bf3e0":"code","999bbc1c":"code","935c3ad7":"code","31d5aa0a":"code","f41da283":"code","1d4fdce1":"code","b228fefa":"code","53de56cd":"code","6a5fd52d":"code","6a5675ab":"code","d76229c3":"code","0ece514f":"code","17450757":"code","aa725345":"code","de30262d":"code","2b2defa6":"code","0e432f8d":"code","b6e218c1":"code","029449a3":"code","6b2412e9":"code","cbb8e451":"code","e54c5838":"code","fc775b71":"code","65715558":"code","0f7822fa":"code","b3ceefc2":"code","e483679a":"code","6182438d":"code","140aaa7f":"code","62901093":"code","3562cad1":"code","ed797bd6":"code","6bbc5f3d":"code","1b68c0be":"code","3abb80fd":"code","7ad893fb":"code","dc23969d":"code","20068612":"code","8872a108":"code","033a6a44":"code","f218bd80":"code","b630fe44":"code","77a08580":"code","3819b426":"code","291e49d6":"code","4486fa74":"code","402aa864":"code","13cd9b02":"code","403b5ecc":"code","285672c6":"code","c381704e":"code","1e8f32d0":"code","20d2523f":"code","877c71b1":"code","239d0718":"code","84817520":"code","7b2c4c4b":"code","f47de2c4":"code","69bc0465":"code","8ec5a554":"code","f24994c3":"code","35b1802a":"code","6d5b4ac9":"code","f9e220a9":"code","3b82a0ed":"code","90f69242":"markdown","9cea6aa0":"markdown","faa2e4cb":"markdown","2ea0fdb7":"markdown","c1027de2":"markdown","4bb3d96f":"markdown","0ee79458":"markdown","c5de9835":"markdown","804fa902":"markdown","4cbbd474":"markdown","a5dd0988":"markdown","c44f43d9":"markdown","292bb54c":"markdown","31d56528":"markdown","71504616":"markdown","9dd1149a":"markdown","593c3009":"markdown","7d590386":"markdown","8378121c":"markdown","87577262":"markdown","d9b9ce2e":"markdown","7847bf8f":"markdown","fa3d9f60":"markdown","dd9c6b82":"markdown","2fc2fb9a":"markdown","1e96baa7":"markdown","687f3ac7":"markdown","6f5cafc8":"markdown","4e0b0cc1":"markdown","e4c9ca75":"markdown","a50db7f0":"markdown","bd7b7299":"markdown","2a4ff2d8":"markdown","d42e9ff3":"markdown","b5f16678":"markdown","450c141f":"markdown","6fe6a626":"markdown","6637a636":"markdown","b9ded502":"markdown","2f2a4d1d":"markdown","a0e55120":"markdown","6b2b99ae":"markdown","60edccb9":"markdown","a12d9e3a":"markdown","c15f00b5":"markdown","2813e96d":"markdown","f6fc35a8":"markdown","08e01d82":"markdown","1cef5222":"markdown","0d970e03":"markdown","c92bafbd":"markdown","0c83a6f9":"markdown","eef49f92":"markdown","19adb2c3":"markdown","07806ed4":"markdown","25496c09":"markdown","41e1b123":"markdown","d7722826":"markdown","9d117efd":"markdown","e867175e":"markdown","41befca4":"markdown","67edb274":"markdown","cfee2194":"markdown","8f6206f6":"markdown","cd83b92d":"markdown","50e7c16c":"markdown","9569b8fd":"markdown","cd418735":"markdown","f62f1a17":"markdown","50e75dd5":"markdown","fd3b36eb":"markdown","1948b1c0":"markdown","be0e0aa7":"markdown","3abb5d60":"markdown","26ae88da":"markdown","eb92472a":"markdown","a0608280":"markdown","d091465f":"markdown"},"source":{"efadc537":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0afacd97":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import xticks\nimport seaborn as sns\n\n\n# Data display coustomization\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', 100)","0b3b7cc1":"leads_data_dict = pd.read_excel('\/kaggle\/input\/leads-dataset\/Leads Data Dictionary.xlsx', skiprows=2)\nleads_data_dict.drop(leads_data_dict.columns[0], axis=1,inplace=True)\nleads_data_dict","48829de3":"leadscore = pd.read_csv('\/kaggle\/input\/leads-dataset\/Leads.csv')","c2c17d2f":"leadscore.head()","c0675d70":"leadscore = leadscore.replace('Select', np.nan)","138bf3da":"### This function will generate a table of features, total NULL values, and %age of NULL values in it.\ndef findNullValuesPercentage(dataframe):\n    totalNullValues = dataframe.isnull().sum().sort_values(ascending=False)\n    percentageOfNullValues = round((dataframe.isnull().mean()).sort_values(ascending=False),2)\n    featuresWithPrcntgOfNullValues = pd.concat([totalNullValues, percentageOfNullValues], axis=1, keys=['Total Null Values', 'Percentage of Null Values'])\n    return featuresWithPrcntgOfNullValues","70a1ace4":"### this function will create BarPlot for our visualization.\n\ndef createCountPlot(keyVariable, plotSize):\n    fig, axs = plt.subplots(figsize = plotSize)\n    plt.xticks(rotation = 90)\n    dataframe = leadscore.copy()\n    dataframe[keyVariable] = dataframe[keyVariable].fillna('Missing Values')\n    ax = sns.countplot(x=keyVariable, data=dataframe)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}'.format(height\/len(dataframe) * 100),\n                ha=\"center\") ","6caa10cb":"\n#This function will just drop the list of features(inplace) provided to it and print them.\ndef dropTheseFeatures(features):\n    print('Dataset shape before dropping the features {}'.format(leadscore.shape))\n    print('*****------------------------------------------*****')\n    for col in features:\n        print('Removing the column {}'.format(col))\n        leadscore.drop(col, axis=1, inplace=True)\n    print('*****------------------------------------------*****')\n    print('Dataset shape after dropping the features {}'.format(leadscore.shape))","c3ef2191":"#This function will genrate a table which is populated with feature name and the %age of count of unique values in it.\ndef genarateUniqueValuePercentagePlot(features):\n    cols=4\n    rows = len(features)\/\/cols +1\n    fig = plt.figure(figsize=(16, rows*5))\n    for plot, feature in enumerate(features):\n        fig.add_subplot(rows,cols,plot+1)\n        ax = sns.countplot(x=leadscore[feature], data=leadscore) \n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/len(leadscore) * 100),\n                ha=\"center\") ","a0d8ddcd":"### Function to generate heatmaps\ndef generateHeatmaps(df, figsize):\n    plt.figure(figsize = figsize)        # Size of the figure\n    sns.heatmap(df.corr(),annot = True, annot_kws={\"fontsize\":7})\n","5e6d2f27":"#As the name suggests this function will plot AUC-ROC curve.\ndef plot_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs)\n                                            #, drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","ab356836":"def getRegressionMetrics(actual,predicted):\n    from sklearn.metrics import precision_score, recall_score\n    m={}\n    confusion = metrics.confusion_matrix(actual, predicted )\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    m['sensitivity']=TP \/ float(TP+FN)\n    m['specificity']=TN \/ float(TN+FP)\n    m['recall']=recall_score(actual, predicted)\n    m['precision']=precision_score(actual, predicted)\n    m['accuracy']=metrics.accuracy_score(actual, predicted)\n    m['F1-score']=metrics.f1_score(actual, predicted, average='weighted')\n    \n    print(confusion)\n    for metric in m:\n        print(metric + ': ' + str(round(m[metric],2)))","62f6d11a":"leadscore.shape","3b461d60":"leadscore.info()","3955df0e":"#Check if any duplicated value is present in the ID and Lead Number columns\nprint(sum(leadscore.duplicated('Prospect ID')) == 0)\nprint(sum(leadscore.duplicated('Lead Number')) == 0)","9e87ea5f":"dropTheseFeatures(['Prospect ID'])","0d20ff07":"numUniquesInFeatures = leadscore.nunique().sort_values()\nnumUniquesInFeatures","7de36930":"dropFeaturesWithSingleVal=[]\nfor feature in numUniquesInFeatures.index:\n#     print(feature, numUniquesInFeatures[feature])\n    if numUniquesInFeatures[feature] == 1:\n        dropFeaturesWithSingleVal.append(feature)\ndropFeaturesWithSingleVal","5c9f8b76":"dropTheseFeatures(dropFeaturesWithSingleVal)","8ec9c146":"genarateUniqueValuePercentagePlot(['A free copy of Mastering The Interview', 'Newspaper Article', 'Search','Through Recommendations',\n             'X Education Forums', 'Converted', 'Do Not Call', 'Do Not Email', 'Newspaper', 'Digital Advertisement'])","55a16f37":"dropHighySkewedFeatures = ['Newspaper Article', 'Search','Through Recommendations',\n             'X Education Forums', 'Do Not Call', 'Do Not Email', 'Newspaper', 'Digital Advertisement']\ndropTheseFeatures(dropHighySkewedFeatures)","a0cdee70":"findNullValuesPercentage(leadscore)","d35c9940":"dropHighMissingValuesFeatues = ['How did you hear about X Education', 'Lead Profile', 'Lead Quality', \n                                'Asymmetrique Profile Score','Asymmetrique Activity Score','Asymmetrique Profile Index',\n                                'Asymmetrique Activity Index']\ndropTheseFeatures(dropHighMissingValuesFeatues)","3ec99d75":"findNullValuesPercentage(leadscore)","cbe97d3b":"dropTheseFeatures(['Tags'])","cfec9bee":"findNullValuesPercentage(leadscore)","1625dd39":"createCountPlot('City', (10,5))","eaae8402":"leadscore['City'].fillna('Mumbai', inplace=True)\n\ncreateCountPlot('City', (10,5))","5cbda83b":"createCountPlot('Specialization', (15,5))","14cc5edc":"leadscore['Specialization'].value_counts(normalize=True, dropna=False).head()","54d7c655":"\n# impute the Missing Values with Finance, HR and  Marketing Management, each of them equally.\nleadscore['Specialization'].iloc[:1000].fillna('Human Resource Management', inplace=True)\nleadscore['Specialization'].iloc[1001:2000].fillna('Marketing Management', inplace=True)\nleadscore['Specialization'].iloc[2000:].fillna('Finance Management', inplace=True)","9d23d195":"leadscore['Specialization'].unique()","100ad6fd":"leadscore.Specialization.replace(to_replace=['Supply Chain Management',\n       'IT Projects Management', \n       'Marketing Management',\n       'Retail Management',\n       'Hospitality Management',\n       'Healthcare Management'], value='Other Management', inplace=True)","4dbb37e4":"leadscore.Specialization.replace(to_replace=[\n       'Media and Advertising',\n       'Travel and Tourism', \n       'Banking, Investment And Insurance', 'International Business',\n       'E-COMMERCE',\n       'Services Excellence',\n       'Rural and Agribusiness',\n       'E-Business'], value='Others', inplace=True)","544cbe4d":"leadscore['Specialization'].value_counts(normalize=True, dropna=False)","b54e4f80":"createCountPlot('Specialization', (10,5))","b76d0667":"createCountPlot('What matters most to you in choosing a course', (15,4))","322bf969":"createCountPlot('What is your current occupation', (10,7.5))","6d266311":"leadscore['What is your current occupation'].fillna('Not Specified', inplace=True)\n\nleadscore['What is your current occupation'] = leadscore['What is your current occupation'].replace(['Student', 'Housewife','Businessman'], 'Other')\n\nleadscore['What is your current occupation'].value_counts()","4eae9414":"createCountPlot('What is your current occupation', (10,7.5))","c9202dc0":"fig, axs = plt.subplots(figsize = (20,4))\nplt.xticks(rotation = 90)\nsns.countplot('Country', data=leadscore)","e2257565":"dropTheseFeatures(['What matters most to you in choosing a course','Country'])","1ab7c78e":"findNullValuesPercentage(leadscore)","c4c5f2d0":"leadscore.dropna(inplace=True)","6910b034":"findNullValuesPercentage(leadscore)","8208f626":"leadscore.shape","75470535":"leads_data_dict[(leads_data_dict['Variables']=='Last Activity') | (leads_data_dict['Variables']=='Last Notable Activity')]","72ca1227":"fig, axs = plt.subplots(figsize = (12,4))\nplt.xticks(rotation = 90)\nsns.countplot('Last Notable Activity', data=leadscore)","3d117487":"dropTheseFeatures(['Last Notable Activity'])","036f9847":"round(leadscore['Last Activity'].value_counts(normalize=True, ascending=False), 2)","32a20708":"leadscore['Last Activity'] = leadscore['Last Activity'].replace([           \n                                                                'Form Submitted on Website',       \n                                                                'Unreachable',                     \n                                                                'Unsubscribed',                    \n                                                                'Had a Phone Conversation',        \n                                                                'View in browser link Clicked',    \n                                                                'Approached upfront',              \n                                                                'Email Received',                  \n                                                                'Email Marked Spam',               \n                                                                'Resubscribed to emails',          \n                                                                'Visited Booth in Tradeshow'], 'Miscellaneous')","698baff7":"\ncreateCountPlot('Last Activity', (7, 5))\n","d121b5e1":"leadscore['A free copy of Mastering The Interview'].value_counts()","0fec697f":"leadscore['A free copy of Mastering The Interview'].replace({'Yes':1, 'No':0}, inplace=True)\nleadscore['A free copy of Mastering The Interview'].value_counts()","ae2010c5":"createCountPlot('City', (7, 5))","5263d1f4":"leadscore['City'] = leadscore['City'].replace(['Thane & Outskirts', 'Other Metro Cities', 'Other Cities',\n       'Other Cities of Maharashtra', 'Tier II Cities'], 'Not Mumbai Cities')","410932ed":"leadscore['City'].value_counts()","1dfadfd6":"createCountPlot('City', (7, 5))","6254b3d0":"leadscore['What is your current occupation'].value_counts()","50d71b93":"createCountPlot('What is your current occupation', (10, 5))","c5359504":"sns.countplot(leadscore['What is your current occupation'], hue=leadscore.Converted)\n\nplt.show()","7565d343":"fig, axs = plt.subplots(figsize = (15, 7.5))\nsns.countplot(leadscore['Specialization'], hue=leadscore.Converted)\naxs.set_xticklabels(axs.get_xticklabels(),rotation=90)\nplt.show()","e8624378":"findNullValuesPercentage(leadscore)","eea4d070":"sns.boxplot('Page Views Per Visit', data=leadscore)","c29f1ced":"q1 = leadscore['Page Views Per Visit'].quantile(0.05) #---- lower range taken\nq4 = leadscore['Page Views Per Visit'].quantile(0.95) #----- higher range taken\n\nleadscore['Page Views Per Visit'][leadscore['Page Views Per Visit']<=q1] = q1 #----- capping of lower range \nleadscore['Page Views Per Visit'][leadscore['Page Views Per Visit']>=q4] = q4 #----- capping of higher range","ef2614d5":"sns.boxplot('Page Views Per Visit', data=leadscore)","ac3978c1":"sns.boxplot(x=leadscore.Converted,y=leadscore['Page Views Per Visit'])\nplt.show()","07afb18b":"leadscore['Total Time Spent on Website'].describe()","7ac666f3":"sns.distplot(leadscore['Total Time Spent on Website'])","4080ce6f":"leadscore['Total Time Spent on Website'] = leadscore['Total Time Spent on Website'].apply(lambda x: round((x\/60), 2))\nsns.distplot(leadscore['Total Time Spent on Website'], )","195a0f0b":"sns.boxplot(x=leadscore.Converted, y=leadscore['Total Time Spent on Website'])\nplt.show()","2cec77ba":"sns.boxplot(y = 'TotalVisits', x = 'Converted', data = leadscore)\nplt.show()","661b8f42":"Q3 = leadscore.TotalVisits.quantile(0.95)\nleadscore = leadscore[(leadscore.TotalVisits <= Q3)]\nQ1 = leadscore.TotalVisits.quantile(0.05)\nleadscore = leadscore[(leadscore.TotalVisits >= Q1)]\nsns.boxplot(y=leadscore['TotalVisits'])\nplt.show()","20f13c2b":"sns.boxplot(y='TotalVisits', x='Converted', data=leadscore)","280b53e7":"createCountPlot('Lead Source', (10,5))","071ca933":"leadscore['Lead Source'].unique()","09eeb2bb":"leadscore['Lead Source'] = leadscore['Lead Source'].replace(['blog', 'Pay per Click Ads', \n                                                'bing', 'Social Media','WeLearn', 'Click2call', 'Live Chat', \n                                                'welearnblog_Home', 'youtubechannel', 'testone', 'Press_Release', 'NC_EDM'], 'Other')\n\n","d2641d0c":"leadscore['Lead Source'] = leadscore['Lead Source'].replace('google', 'Google')","2b763a79":"fig, axs = plt.subplots(figsize = (10, 5))\nsns.countplot('Lead Source', hue='Converted', data=leadscore)","6ccfa373":"leadscore['Lead Origin'].describe()","b47e12fa":"fig, axs = plt.subplots(figsize = (10, 5))\nsns.countplot('Lead Origin', hue='Converted', data=leadscore)","b44e7186":"#Drop all the rows with `Lead Import` as Lead Origin\nleadscore.drop(leadscore[leadscore['Lead Origin'] == 'Lead Import'].index, inplace=True)","bf8680b1":"fig, axs = plt.subplots(figsize = (10, 5))\nsns.countplot('Lead Origin', hue='Converted', data=leadscore)","c3af8063":"fig, axs = plt.subplots(figsize = (15, 5))\nsns.countplot('Last Activity', hue='Converted', data=leadscore)","9da9a832":"\n\nleadscore.columns","9ae1fbdc":"leadscore_corr = leadscore[['Lead Origin', 'Lead Source', 'Converted', 'TotalVisits',\n       'Total Time Spent on Website', 'Page Views Per Visit', 'Last Activity',\n       'Specialization', 'What is your current occupation', 'City',\n       'A free copy of Mastering The Interview']]","fc2d760f":"generateHeatmaps(leadscore_corr, (12,8))","fbbdd966":"leadscore.shape","3c950640":"leadscore.head()","a402ef75":"leadscore.info()","9045b795":"categorical_feature =  leadscore.select_dtypes(include=['object']).columns\ncategorical_feature","888bc5e4":"dummy = pd.get_dummies(leadscore['Specialization'], prefix  = 'Specialization')\ndummy = dummy.drop(['Specialization_Business Administration'], 1)\nleads_dummified = pd.concat([leadscore, dummy], axis = 1)","1a802995":"dummy = pd.get_dummies(leadscore['Lead Source'], prefix  = 'Lead_Source')\ndummy = dummy.drop(['Lead_Source_Facebook'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","fd88ec35":"dummy = pd.get_dummies(leadscore['Last Activity'], prefix  = 'Last_Activity')\ndummy = dummy.drop(['Last_Activity_Email Link Clicked'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","b20d4b76":"leadscore['Lead Origin'].value_counts()","f0d1b430":"dummy = pd.get_dummies(leadscore['Lead Origin'], prefix = 'Lead_Origin', drop_first=True)\n# dummy = dummy.drop(['Lead_Origin_Lead Add Form'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","1c2bf3e0":"dummy = pd.get_dummies(leadscore['What is your current occupation'], prefix = 'Occupation')\ndummy = dummy.drop(['Occupation_Other'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","999bbc1c":"dummy = pd.get_dummies(leadscore['City'], prefix = 'City')\ndummy = dummy.drop(['City_Not Mumbai Cities'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","935c3ad7":"leads_dummified.drop(['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization',\n       'What is your current occupation', 'City'], axis=1, inplace=True)","31d5aa0a":"leads_dummified.shape","f41da283":"leads_dummified.head(5)","1d4fdce1":"from sklearn.model_selection import train_test_split\nnp.random.seed(0)\nlead_df_train,lead_df_test=train_test_split(leads_dummified,train_size=0.7,random_state=100)","b228fefa":"X_train = lead_df_train.drop(['Converted','Lead Number'], axis=1)\ny_train = lead_df_train['Converted']","53de56cd":"X_train.info()","6a5fd52d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnum_cols=X_train.select_dtypes(include=['float64', 'int64']).columns\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_train.head(5)","6a5675ab":"import statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\nleads_reg = LogisticRegression()\n\nrfe = RFE(leads_reg, 15)\nrfe = rfe.fit(X_train, y_train)","d76229c3":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","0ece514f":"rfe_selected_features = X_train.columns[rfe.support_]\nrfe_selected_features","17450757":"X_train_sm = sm.add_constant(X_train[rfe_selected_features])\nmodel1 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nresult = model1.fit()\nresult.summary()","aa725345":"rfe_selected_features = rfe_selected_features.drop('Lead_Source_Organic Search', 1)\nrfe_selected_features","de30262d":"X_train_sm = sm.add_constant(X_train[rfe_selected_features])\nmodel2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nresult = model2.fit()\nresult.summary()","2b2defa6":"rfe_selected_features = rfe_selected_features.drop('Lead_Source_Reference', 1)\nrfe_selected_features","0e432f8d":"X_train_sm = sm.add_constant(X_train[rfe_selected_features])\nmodel3 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nresult = model3.fit()\nresult.summary()","b6e218c1":"\nrfe_selected_features = rfe_selected_features.drop('Last_Activity_Miscellaneous', 1)\nrfe_selected_features","029449a3":"X_train_sm = sm.add_constant(X_train[rfe_selected_features])\nmodel4 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nresult = model4.fit()\nresult.summary()","6b2412e9":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train[rfe_selected_features].columns\nvif['VIF'] = [variance_inflation_factor(X_train[rfe_selected_features].values, i) for i in range(X_train[rfe_selected_features].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","cbb8e451":"final_selected_features = rfe_selected_features","e54c5838":"y_train_pred = result.predict(X_train_sm)\ny_train_pred[:10]","fc775b71":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","65715558":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Lead_Score_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = leadscore['Lead Number']\ny_train_pred_final.head()","0f7822fa":"y_train_pred_final['Lead_Score'] = round((y_train_pred_final['Lead_Score_Prob'] * 100),0)\n\ny_train_pred_final.head()","b3ceefc2":"y_train_pred_final['Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","e483679a":"y_train_pred_final['Lead_Score'] = round((y_train_pred_final['Lead_Score_Prob'] * 100),0)\ny_train_pred_final['Lead_Score'] = y_train_pred_final['Lead_Score'].astype(int)\ny_train_pred_final.head()","6182438d":"from sklearn import metrics\nconfusion_matrix = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead )\nprint(confusion_matrix)","140aaa7f":"print('Accuracy for the Model 4 is {}%'.format(round(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead),2)*100 ))","62901093":"TP = confusion_matrix[1,1] # true positive \nTN = confusion_matrix[0,0] # true negatives\nFP = confusion_matrix[0,1] # false positives\nFN = confusion_matrix[1,0] # false negatives","3562cad1":"sensitivity = round((TP \/ float(TP+FN)),2)\nspecificity = round((TN \/ float(TN+FP)),2)\n\nprint('Sensitivity is {}% and Specificity is {}%'.format(sensitivity*100, specificity*100))","ed797bd6":"plot_roc(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead)","6bbc5f3d":"y_train_pred_final","1b68c0be":"numbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","3abb80fd":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificty', 'Precision'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    preci = cm1[1,1]\/(cm1[1,1]+cm1[0,1])   #TP\/TP+FP\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci, preci]   \nprint(cutoff_df)","7ad893fb":"sns.set_style('whitegrid')\nsns.set_context('paper')\n\ncutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificty'], figsize=(10,6))\nplt.xticks(np.arange(0,1,step=.05), size=8)\nplt.yticks(size=12)\nplt.show()","dc23969d":"cut_off = 0.36","20068612":"y_train_pred_final['Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map( lambda x: 1 if x > cut_off else 0)\ny_train_pred_final.head()","8872a108":"getRegressionMetrics(y_train_pred_final.Converted,y_train_pred_final.Predicted_Hot_Lead)","033a6a44":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob)","f218bd80":"\nplt.figure(figsize=(8, 4), dpi=100, facecolor='w', edgecolor='k', frameon='True')\nplt.title('Precision vs Recall')\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.xticks(np.arange(0, 1, step=0.05))\nplt.show()","b630fe44":"lead_df_test.head()","77a08580":"lead_df_test[num_cols] = scaler.transform(lead_df_test[num_cols])","3819b426":"rfe_selected_features","291e49d6":"X_test = lead_df_test[rfe_selected_features]\ny_test = lead_df_test[['Lead Number', 'Converted']]","4486fa74":"print(X_test.shape)\nprint(y_test.shape)","402aa864":"X_test_sm = sm.add_constant(X_test)\ny_test_pred = result.predict(X_test_sm)\ny_test_pred[:10]","13cd9b02":"# Coverting it to df\ny_pred_df = pd.DataFrame(y_test_pred)\n# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\n# Remove index for both dataframes to append them side by side \ny_pred_df.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n# Append y_test_df and y_pred_df\ny_pred_final = pd.concat([y_test_df, y_pred_df],axis=1)\n# Renaming column \ny_pred_final= y_pred_final.rename(columns = {0 : 'Lead_Score_Prob'})\ny_pred_final.head()","403b5ecc":"y_pred_final['Predicted_Hot_Lead'] = y_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > cut_off else 0)\ny_pred_final.head()","285672c6":"getRegressionMetrics(y_pred_final.Converted,y_pred_final.Predicted_Hot_Lead)","c381704e":"y_pred_final['Lead_Score'] = round((y_pred_final['Lead_Score_Prob'] * 100),0)\ny_pred_final['Lead_Score'] = y_pred_final['Lead_Score'].astype(int)","1e8f32d0":"y_pred_final.head()","20d2523f":"leads_dummified.shape","877c71b1":"leads_dummified.head()","239d0718":"leads_dummified[num_cols] = scaler.transform(leads_dummified[num_cols])\nleads_dummified.head()","84817520":"cleaned_lead_sm = sm.add_constant(leads_dummified[rfe_selected_features])\ncleaned_predicted = result.predict(cleaned_lead_sm)\ncleaned_predicted","7b2c4c4b":"final_lead_score_df = leadscore.copy()\nfinal_lead_score_df.head()","f47de2c4":"final_lead_score_df['Lead Score']=round(cleaned_predicted*100,2)\nfinal_lead_score_df.head()","69bc0465":"hot_leads = final_lead_score_df.sort_values(by='Lead Score',ascending=False)[['Lead Number','Lead Score']]\nhot_leads[hot_leads['Lead Score']>36] ","8ec5a554":"final_lead_score_df['Is_Hot_Lead'] = final_lead_score_df['Lead Score'].map(lambda x: 1 if x > 36 else 0)\nfinal_lead_score_df.sort_values(by='Lead Score',ascending=False).head(10)","f24994c3":"coeff = result.params[1:]\ncoeff","35b1802a":"feature_relevance = 100.0 * (coeff \/ coeff.max())\nfeature_relevance","6d5b4ac9":"sorted_idx = np.argsort(feature_relevance,kind='quicksort',order='list of str')\nsorted_idx","f9e220a9":"\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(10,6))\nax =  fig.add_subplot(1, 1, 1)\nax.barh(pos, feature_relevance[sorted_idx], align='center', color = 'tab:blue',alpha=0.8)\nax.set_yticks(pos)\nax.set_yticklabels(np.array(rfe_selected_features)[sorted_idx], fontsize=12)\nax.set_xlabel('Relative Feature Importance', fontsize=14)\n\nplt.tight_layout()   \nplt.show()","3b82a0ed":"\npd.DataFrame(feature_relevance).reset_index().sort_values(by=0,ascending=False).head(3)","90f69242":"###### Treating Specialization ","9cea6aa0":"###### So we getting almost similar results for test data as well, with 1% of deviation\n###### Now we will add the Lead Score to each leads","faa2e4cb":"###### Now, we will plot the above data in a bar plot, to visulaize it better","2ea0fdb7":"   ###### The above summary shows that the total time is given in minutes. We will convert these to Hours first","c1027de2":"###### For the above feature also the data seems to be skewed, so we assume that almost all the leads wan to join the academy for Better career prospects. So we wil remove this feature as well, later.","4bb3d96f":"###### From above plot we see that most of the leads are coming from sources like, `Olark Chat`, `Organic Search` , `Direct Traffic` and `Google` However their conversion rate is low. But the conversion rate for `Reference` and `Welingak WebSite` is quite high. So we should definitely consider these Lead Source in our model.","0ee79458":"###### The ROC AUC value, we got, is 0.80. This  indicates the we have  a good  model and is capable of distinguising the classes.\n\n###### Now we will find the prediction on the train data using this model and also calculate the cut-off threshold values ","c5de9835":"##### The above list is showing the list of customers having score more than 36 .","804fa902":"###### Based on the above heatmap we can say that `Page Views Per Visit` and `Total Visit` are correlated, Othere than these, there aren't any highly correlated features. \n###### Finally we have completed our analysis using visualization . Now we start the Data preparation with the cleansed dataset.","4cbbd474":"###### So we can see from the above plot that most of the leads are coming from Mumbai city, how ever the conversion rate is considerably low.","a5dd0988":"###### The country plot also seems to be skewed. After imputing the Missing Values with mode, `India`, it will become 97%. So we will drop this as well. \n\n##### Dropping `What matters most to you in choosing a course` and `Country` in this step.","c44f43d9":"###### Finally we get the all cleaned dataset with 9074 rows and 12 features.\n\n### We will perform some univariate and bivariate analysis ","292bb54c":"###### We will add a new column `Predicted Hot Lead` assuming  threshold value as 0.5","31d56528":"###### We can see from the above table that `Lead Profile` and `How did you hear about X Education` columns have 74% and above missing values. Also assuming a cut-off of 45% the `Asymmetrique` score and Index features are also on higher side. Imputing these features will not be a good idea, because it can make the model biased.   So we decide to drop these columns.","71504616":"###### 1. Accuracy","9dd1149a":"###### From the `Lead Origin` plot, we can see that most of the leads were identified from Landing Page submission and then by APIs. \n###### However the conversion rate of `Lead Add Form` is comparitably good. \n##### So we will drop only the `Lead Import` which is insignificant in this case, and it will unnecessarily create extra dummy variable.","593c3009":"###### For the current occupation column we will impute the missing values with mode, ie, `Unemployed`.","7d590386":"#### The above are top 10 leads which have high score and hence they have high chance of convertion\n#### Now we will determine the importance or the selected features\n###### Getting the coefficients from the final model summary","8378121c":"#####  The `Last Notable Activity` is  last  activity performed by the student. It is not relevant for our modelling purpose. So we will drop it here.","87577262":"###### With the help of the confusion matrix, we will calculate the below metrics, which will be used late for model evaluation later.\n","d9b9ce2e":"###### FInally we get the final result table of the test data, which contains the Leads Score for each leads, based on which we can determine whether a lead is HotLead or Cold Lead.\n###### Generate leadscore for the whole dataset, which was created after the EDA, before train_test_split. ","7847bf8f":"###### A feature with only one unique value is not useful for model building because this feature has zero variance. So we will drop all such clumns.","fa3d9f60":"###### VIF for all the variables are also less than 3. SO we can go ahead with this model with these features. \n\n###### Now we can proceed to derive the predictions, probabilities and  LeadScore on the trainin data.","dd9c6b82":"###### Analyzing City columns","2fc2fb9a":"###### Next we will deal with the reamianing columns with high NULL values. These are:\n\n###### `City`\n###### `Specialization`\n###### `What matters most to you in choosing a course` \n###### `What is your current occupation`\n###### `Country`","1e96baa7":"###### The Prospect ID and Lead Number are all unique values and not required for the model building.\n> We will drop one of them, since Lead Number seems be simply an index number so we will keep it and remove\nthe prospectID","687f3ac7":"###### Starting with `Last Notable Activity` ","6f5cafc8":"###### X Education has a period of 2 months every year during which they hire some interns. The sales team has around 10 interns allotted to them. So, during this phase, they wish to make the lead conversion more aggressive. So, they want almost all of the potential leads (i.e. the customers who have been predicted as 1 by the model) to be converted and hence, want to make phone calls to as much of such people as possible. Suggest a good strategy they should employ at this stage.\n\n> Since the resources in the sales team has increased, they can target for as much leads as possible. The cutoff criteria of Hot Leads can be lowered. We can target for 90% sensitivity which is at Lead score 20. At this point the accuracy is also 75% which is considerably good. At this cut off the salesperson can first target the top Hot Leads, which have more chances to get converted.","4e0b0cc1":"##### The precision vs recall tradeoff value from the above graph is at 0.41\n#### From the precision-recall graph above, we get the optimum threshold value as close to .41. However our business requirement here is to have Lead Conversion Rate around 80%.  \n\n#### This is already achieved with our earlier threshold  value of 0.36. So we will stick to this value.\n###### Now making predcitions on test data","e4c9ca75":"###### Calaculate the Lead score based on the `Lead_Score_Prob` and generate the confusion matrix.","a50db7f0":"###### Lead Add form have higest Conversion rate","bd7b7299":"###### analyzing the trend of `Page Views Per Visit' using boxplot ","2a4ff2d8":"\n###### `Working Professional` are can be a important feature to since their conversion rate is very high.\n###### However the datasets contains mostly Unemployed leads","d42e9ff3":"###### Similarly, at times, the company reaches its target for a quarter before the deadline. During this time, the company wants the sales team to focus on some new work as well. So, during this time, the company\u2019s aim is to not make phone calls unless it\u2019s extremely necessary, i.e. they want to minimize the rate of useless phone calls.\n\n> In this case, as there is a restriction on calling the leads, the sales team should not call, those leads which have very less chance of conversion. They should try to reach those customers which have more chance.\nHere, we should avoid false positive count as much as possible. That means we should target to achieve high precision. Precision is the positive predictive value or the fraction of the positive predictions that are actually positive. So, the sales team should focus on the leads having score more than 65 (as at lead score 60 the precision is 82 and at 70 the precision is ~82%, at this cutoff the accuracy is ~80%) to achieve more than 85% precision.\n\n>Also, for the remaining leads with low Lead Score, since the company doesn\u2019t aim to make phone calls, they can use other methods of reaching the customers like, automated emails and SMSes. By this strategy, we can reach to most of the customers.","b5f16678":"###### SInce all the p-values are less as ecpexted, so we can check the VIF for multicolinearity among the variables. ","450c141f":"\n###### From above plots we see that Median for converted and not converted leads are almost same. So We cannot say anything about the lead conversion based on Page Views ","6fe6a626":"###### SMS sent has highest conversion rate.","6637a636":"###### Handling the `Score Variables`. These variables are the ones which were not present with the sales team before making the call. After making the calls the sales team assigned their values to each lead after their discussion. So, in the real scenario these will not be availabe at the time of model building. So we will drop these columns. The score variables in this datasets are:\n- Tags\n- Lead Quality\n- Lead Profile\n- Asymmetrique Activity Index\n- Asymmetrique Profile Index\n- Asymmetrique Activity Score\n- Asymmetrique Profile Score ","b9ded502":"###### There are lot of rows in various categorical columns which have value 'Select' values present in the dataset. `Select` corresponds to the user having not made any selection. So we can replace it with NaN.","2f2a4d1d":"###### Check the columns reatained by rfe using rfe support and ranking","a0e55120":"\n###### Now we have only few NULL values left. So we will drop those rows with NULLs","6b2b99ae":"###### Now we check the features with 2 unique values\n- A free copy of Mastering The Interview              \n- Newspaper Article                                   \n- Search                                              \n- Through Recommendations                             \n- X Education Forums                                  \n- Converted (Target Variable)                                     \n- Do Not Call                                         \n- Do Not Email                                        \n- Newspaper                                           \n- Digital Advertisement                               ","60edccb9":"###### From the data dictionary  we see that `Last Notable Activity` is defined very similar to `Last Activity`","a12d9e3a":"\n###### We see that  leads who have done specialization in  `Management` specially in `FInance Management` have higher number of leads as well as leads converted. This variable is highly significant and will help in model building.","c15f00b5":"###### We will perform the some analysis and cleaning operation for `Last Activity`","2813e96d":"###### From the abve plots, we can see that  `A free copy of Mastering The Interview` and `Converted (Target variable)`, all other features are higly skewed. These variables will not help in the model buidling so we will drop these.","f6fc35a8":"###### After trying several models, we finally chose this Model, becuase it fullfilled the below criteria\n\n-  <font color = blue> All variables have p-value < 0.05.\n-  All the features have very low VIF values, meaning, there is hardly any muliticollinearity among the features. This is also evident from the heat map as well.\n-  The overall accuracy of 79% at a probability threshold of <bold> 0.36 <\/bold> on the test dataset is also acceptable.<\/font>\n\n###### <font color = blue> We can also tweak the  probability threshold value with in turn will decrease or increase the Sensitivity and increase or decrease the Specificity of the model, based on the business requirements. <\/font>\n    \n###### <font color = blue> High Sensitivity ensures that the leads who are likely to be convertted are correctly predicted where as high Specificity will ensure that leads that are on the cut-off of the probability of getting converted or not are not selected. <\/font>\n\n","08e01d82":"###### Before moving ahead  with EDA, we will define some helper functions which will be used frequently in the rest of the analysis.","1cef5222":"###### To understand this better, we will find relative coefficients of the features. This will help us in camparing the features better.\n","0d970e03":"###### Test and Train Split","c92bafbd":"######  This value of Sensitivity does not fullfill our objective ,as the CEO of the company has given a ballpark of the target lead conversion rate to be around 80%. This implies that the threshold value we chose ealier`[50%]` is not correct. Thus we need a better threshold.","0c83a6f9":"###### We see that there are many outliers in the higher side of the data. \n###### We will remove theese the outliers by capping using soft range capping.","eef49f92":"### 1.  Performing EDA on the Lead Score Dataset","19adb2c3":"###### We see that the lead conversion rate is high in case of Working Profesionals. Also most of the leads are `Unemployed`. Other categories in occupation are negligible.\n\n###### Now we take a look a Country column","07806ed4":"###### The below features were selected for the model in the train step","25496c09":"###### Here we will impute the Missing Values with mode , ie, `Mumbai` in this case.","41e1b123":"###### For the above 7 categorical columns, dummy encoding is required.\n###### We will create the dummies and drop the catergory which has the least frequency in each column\n","d7722826":"###### As you can see, at about a threshold of 0.36, the curves of accuracy, sensitivity and specificity intersect, and they all take a value of around 80%. With this threshold we will predict the Hot Lead again.","9d117efd":"###### From above plots we see that Median for converted and not converted leads are almost same. So We cannot say anything about the lead conversion based on TotalVisits\n###### Analyzing Lead Source and Lead Origin","e867175e":"###### Find the lead score and add a `Lead Score` column to the above table","41befca4":"###### We will check the 'Prospect ID' & for 'Lead Number' columns for each datapoint to check for any duplicates.","67edb274":"######  Now check the NULL values in remaining features.","cfee2194":"### The top three variables which contributed the most in lead conversion are: \n1. Lead Origin\n1. What is the Current Occupation? \n1. LeadSource.\n\n\n> The categories in each variable which are important are:\n1. Working Professionals (in Current Occupation)\n1. Lead Add Form (Lead Origin) and \n1. Welingak Website (in Lead Source).","8f6206f6":"\n###### Here we observe that, the specializations realted to `Finance, HR and  Marketing Management` have most  of the leads and reamaining  catergories share a comparably less percentage. \n###### So we will take the following steps for the imputations.\n\n- impute the `Missing Values` with `Finance, HR and  Marketing Management`, each of them equally.\n- `Business Administration (4.36%)` and `Operations Management (5.44%)` share a considerable share. So we will keep them as it is.\n- Combine remaining Management specialization in to a sigle category,`Other Managements`\n- The specializations with less than 4% share, into a single category as `Other Specializations`\n\n###### This will  help us in reducing the complexity of the model later, by keeping the count of dummy variables low.","cd83b92d":"###### Replace the ctaegories with lower count with `Others` category to reduce the dummy variable. This is avoid the model complexity","50e7c16c":"###### Analyzing Total Time Spent on Website","9569b8fd":"###### Fromt the above plot and data, we can find out the TOP  3 features, using the feature_relevance dataframe","cd418735":"###### Creating dummy variables for all categorical columns","f62f1a17":"###### For this column we will just replace Yes:1, No:0","50e75dd5":"###### Scaling the numerical columns","fd3b36eb":"###### Now we will start with model building using the stats model and select top 15 significant features using the RFE","1948b1c0":"###### Scaling the cleaned dataset again ","be0e0aa7":"###### Find metrics of the test data resutls","3abb5d60":"###### The above confusion matrix shows better results than the previously calculated one, using 0.5 as cutoff.\n###### Our  business requirement of getting the sensitvity value above 80% is also achieved in this model. Also the accuracy and F1 score is pretty good.","26ae88da":"###### A free copy of Mastering The Interview","eb92472a":"###### We will perform some exploratory data analysis and understand the data better by following the below steps:\n\n- Checking the shape, columns, datatypes etc. of the dataset\n- Assessing out of place values\n- Checking for duplicate values\n- Checking for null values\n- Dropping unnecessary columns","a0608280":"###### From the above list, we have already dropped all the features in ealier steps except `Tag` variable.","d091465f":"###### The above plot tells that  Leads spending more time on the website are more likely to be converted.\n###### Therefore, we can suggest to the company to make the website more reliable and attractable for the leads, so that they spend more time on the website."}}