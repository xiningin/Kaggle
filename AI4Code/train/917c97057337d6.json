{"cell_type":{"5c461127":"code","d6bd7970":"code","0545eecf":"code","6c6243f5":"code","7549a696":"code","868acce8":"code","b7918505":"code","64532dfd":"code","7f1ce38f":"code","953172f0":"code","8fd502fc":"code","a4a5f0b6":"code","34feb0d1":"code","1ed810cb":"code","3c93555c":"code","be3d59b5":"code","17ab0a05":"code","5d187435":"code","ff969fcc":"code","5acb07f9":"code","846ffc52":"code","56499f00":"code","e5a51e3b":"markdown","7cb5ccce":"markdown","6e370263":"markdown","46021975":"markdown","374a65a7":"markdown","ec888a59":"markdown"},"source":{"5c461127":"# Functions to read and show images.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n   \nd0 = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\n\nprint(d0.head(5)) # print first five rows of d0.\n\n# save the labels into a variable l.\nl = d0['label']\n\n# Drop the label feature and store the pixel data in d.\nd = d0.drop(\"label\",axis=1)","d6bd7970":"print(d.shape)\nprint(l.shape)","0545eecf":"# display or plot a number.\nplt.figure(figsize=(7,7))\nidx = 1\n\ngrid_data = d.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\n\nprint(l[idx])","6c6243f5":"labels = l\ndata = d\n\nprint(\"the shape of sample data = \", data.shape)\n","7549a696":"# Data-preprocessing: Standardizing the data\n\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nprint(standardized_data.shape)\n","868acce8":"#find the co-variance matrix which is : A^T * A\nsample_data = standardized_data\n\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T , sample_data)\n\nprint ( \"The shape of variance matrix = \", covar_matrix.shape)\n","b7918505":"# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space.\n\nfrom scipy.linalg import eigh \n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues.\nvalues, vectors = eigh(covar_matrix, eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\n\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","64532dfd":"# projecting the original data sample on the plane \n#formed by two principal eigen vectors by vector-vector multiplication.\n\nimport matplotlib.pyplot as plt\nnew_coordinates = np.matmul(vectors, sample_data.T)\n\nprint (\" resultanat new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)","7f1ce38f":"import pandas as pd\n\n# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","953172f0":"import pandas as pd\ndf=pd.DataFrame()\ndf['1st']=[-5.558661,-5.043558,6.193635 ,19.305278]\ndf['2nd']=[-1.558661,-2.043558,2.193635 ,9.305278]\ndf['label']=[1,2,3,4]","8fd502fc":"import seaborn as sn\nimport matplotlib.pyplot as plt\nsn.FacetGrid(df, hue=\"label\", height=6).map(plt.scatter, '1st', '2nd').add_legend()\nplt.show()","a4a5f0b6":"sn.scatterplot(x=\"1st\",y=\"2nd\",hue=\"label\",data=df)","34feb0d1":"# ploting the 2d data points with seaborn\nimport seaborn as sn\nsn.FacetGrid(dataframe, hue=\"label\", height=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","1ed810cb":"sn.scatterplot(x=\"1st_principal\",y=\"2nd_principal\",legend=\"full\",hue=\"label\",data=dataframe)","3c93555c":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()\n","be3d59b5":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)\n\n","17ab0a05":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, labels)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsn.FacetGrid(pca_df, hue=\"label\", height=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","5d187435":"# PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n\n# If we take 200-dimensions, approx. 90% of variance is expalined.","ff969fcc":"# TSNE\n\nfrom sklearn.manifold import TSNE\n\ndata = standardized_data\n\nmodel = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data = model.fit_transform(data)\n\n\n# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()","5acb07f9":"from sklearn.manifold import TSNE\nmodel = TSNE(n_components=2, random_state=0, perplexity=50)\ntsne_data = model.fit_transform(data) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50')\nplt.show()","846ffc52":"model = TSNE(n_components=2, random_state=0, perplexity=50,  n_iter=5000)\ntsne_data = model.fit_transform(data) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50, n_iter=5000')\nplt.show()","56499f00":"model = TSNE(n_components=2, random_state=0, perplexity=2)#with less perplexity we have less interpretable model.\ntsne_data = model.fit_transform(data) \n\n# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 2')\nplt.show()","e5a51e3b":"# t-SNE using Scikit-Learn\n\n**Bonus Tip :** Always run t-SNE with multiple perplexity values and multiple value of iterations to understand the actual shape as well as to see if the shape has reaches a stable configuration.","7cb5ccce":"# Principal Component Analysis (PCA) :\n\nPrincipal Component Analysis or PCA is a linear feature extraction technique. It performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. It does so by calculating the eigenvectors from the covariance matrix. The eigenvectors that correspond to the largest eigenvalues (the principal components) are used to reconstruct a significant fraction of the variance of the original data.\n\nIn simpler terms, PCA combines your input features in a specific way that you can drop the least important feature while still retaining the most valuable parts of all of the features. As an added benefit, each of the new features or components created after PCA are all independent of one another.\n\n# t-Distributed Stochastic Neighbor Embedding (t-SNE):\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It is extensively applied in image processing, NLP, genomic data and speech processing. To keep things simple, here\u2019s a brief overview of working of t-SNE:\n\n* The algorithms starts by calculating the probability of similarity of points in high-dimensional space and calculating the probability of similarity of points in the corresponding low-dimensional space. The similarity of points is calculated as the conditional probability that a point A would choose point B as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian (normal distribution) centered at A.\n\n* It then tries to minimize the difference between these conditional probabilities (or similarities) in higher-dimensional and lower-dimensional space for a perfect representation of data points in lower-dimensional space.To measure the minimization of the sum of difference of conditional probability t-SNE minimizes the sum of Kullback-Leibler divergence of overall data points using a gradient descent method.\n\n* Note Kullback-Leibler divergence or KL divergence is is a measure of how one probability distribution diverges from a second, expected probability distribution.\n\n* Those who are interested in knowing the detailed working of an algorithm can refer to this notebook.\n\n* In simpler terms, t-Distributed stochastic neighbor embedding (t-SNE) minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding.\n\n* In this way, t-SNE maps the multi-dimensional data to a lower dimensional space and attempts to find patterns in the data by identifying observed clusters based on similarity of data points with multiple features. However, after this process, the input features are no longer identifiable, and you cannot make any inference based only on the output of t-SNE. Hence it is mainly a data exploration and visualization technique.\n\n# PCA vs. t-SNE :\n\nAlthough both PCA and t-SNE have their own advantages and disadvantages, some key differences between PCA and t-SNE can be noted as follows:\n\n* t-SNE is computationally expensive and can take several hours on million-sample datasets where PCA will finish in seconds or minutes.\n\n* PCA it is a mathematical technique, but t-SNE is a probabilistic one.\n\n* Linear dimensionality reduction algorithms, like PCA, concentrate on placing dissimilar data points far apart in a lower dimension representation. But in order to represent high dimension data on low dimension, non-linear manifold, it is essential that similar data points must be represented close together, which is something t-SNE does not PCA.\n\n* Sometimes in t-SNE different runs with the same hyperparameters may produce different results hence multiple plots must be observed before making any assessment with t-SNE, while this is not the case with PCA.\n\n* Since PCA is a linear algorithm, it will not be able to interpret the complex polynomial relationship between features while t-SNE is made to capture exactly that.\n\n**In this notebook, we see PCA and t-SNE Visualization on mnist dataset.**\n**If this helped you, some upvotes would be very much appreciated - that's where I get my motivation!**","6e370263":"# Load MNIST Data ","46021975":"# PCA for dimensionality redcution (not for visualization)","374a65a7":"# PCA using Scikit-Learn","ec888a59":"#  2D Visualization using PCA "}}