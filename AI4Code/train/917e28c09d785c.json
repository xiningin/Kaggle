{"cell_type":{"9dfd02ca":"code","eef89dd2":"code","5afd6025":"code","a1c9f995":"code","60e1517b":"code","7a9f8a93":"markdown","3f94499f":"markdown","ac982a14":"markdown","f00c13de":"markdown","a782314c":"markdown","66225ded":"markdown","2e3eee80":"markdown"},"source":{"9dfd02ca":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef MMD(x, y, kernel):\n    \"\"\"Emprical maximum mean discrepancy. The lower the result\n       the more evidence that distributions are the same.\n\n    Args:\n        x: first sample, distribution P\n        y: second sample, distribution Q\n        kernel: kernel type such as \"multiscale\" or \"rbf\"\n    \"\"\"\n    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n    \n    dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n    dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n    dxy = rx.t() + ry - 2. * zz # Used for C in (1)\n    \n    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n                  torch.zeros(xx.shape).to(device),\n                  torch.zeros(xx.shape).to(device))\n    \n    if kernel == \"multiscale\":\n        \n        bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n        for a in bandwidth_range:\n            XX += a**2 * (a**2 + dxx)**-1\n            YY += a**2 * (a**2 + dyy)**-1\n            XY += a**2 * (a**2 + dxy)**-1\n            \n    if kernel == \"rbf\":\n      \n        bandwidth_range = [10, 15, 20, 50]\n        for a in bandwidth_range:\n            XX += torch.exp(-0.5*dxx\/a)\n            YY += torch.exp(-0.5*dyy\/a)\n            XY += torch.exp(-0.5*dxy\/a)\n      \n      \n\n    return torch.mean(XX + YY - 2. * XY)","eef89dd2":"%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom scipy.stats import dirichlet \nfrom torch.distributions.multivariate_normal import MultivariateNormal \n\n\nm = 20 # sample size\nx_mean = torch.zeros(2)+1\ny_mean = torch.zeros(2)\nx_cov = 2*torch.eye(2) # IMPORTANT: Covariance matrices must be positive definite\ny_cov = 3*torch.eye(2) - 1\n\npx = MultivariateNormal(x_mean, x_cov)\nqy = MultivariateNormal(y_mean, y_cov)\nx = px.sample([m]).to(device)\ny = qy.sample([m]).to(device)\n\nresult = MMD(x, y, kernel=\"multiscale\")\n\nprint(f\"MMD result of X and Y is {result.item()}\")\n\n# ---- Plotting setup ----\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(8,4), dpi=100)\n#plt.tight_layout() \ndelta = 0.025\n\nx1_val = np.linspace(-5, 5, num=m)\nx2_val = np.linspace(-5, 5, num=m)\n\nx1, x2 = np.meshgrid(x1_val, x2_val)\n\npx_grid = torch.zeros(m,m)\nqy_grid = torch.zeros(m,m)\n\n\nfor i in range(m):\n    for j in range(m):\n        px_grid[i,j] = multivariate_normal.pdf([x1_val[i],x2_val[j]], x_mean, x_cov)\n        qy_grid[i,j] = multivariate_normal.pdf([x1_val[i],x2_val[j]], y_mean, y_cov)\n\n\nCS1 = ax1.contourf(x1, x2, px_grid,100, cmap=plt.cm.YlGnBu)\nax1.set_title(\"Distribution of $X \\sim P(X)$\")\nax1.set_ylabel('$x_2$')\nax1.set_xlabel('$x_1$')\nax1.set_aspect('equal')\nax1.scatter(x[:10,0].cpu(), x[:10,1].cpu(), label=\"$X$ Samples\", marker=\"o\", facecolor=\"r\", edgecolor=\"k\")\nax1.legend()\n\nCS2 = ax2.contourf(x1, x2, qy_grid,100, cmap=plt.cm.YlGnBu)\nax2.set_title(\"Distribution of $Y \\sim Q(Y)$\")\nax2.set_xlabel('$y_1$')\nax2.set_ylabel('$y_2$')\nax2.set_aspect('equal')\nax2.scatter(y[:10,0].cpu(), y[:10,1].cpu(), label=\"$Y$ Samples\", marker=\"o\", facecolor=\"r\", edgecolor=\"k\")\nax2.legend()\n#ax1.axis([-2.5, 2.5, -2.5, 2.5])\n\n# Add colorbar and title\nfig.subplots_adjust(right=0.8)\ncbar_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])\ncbar = fig.colorbar(CS2, cax=cbar_ax)\ncbar.ax.set_ylabel('Density results')\nplt.suptitle(f\"MMD result: {round(result.item(),3)}\",y=0.95, fontweight=\"bold\")\nplt.show()\n","5afd6025":"import matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\nroot = \"\/\"\nBATCH_SIZE = 500\nN_INP = 32\nN_OUT = 784\nN_GEN_EPOCHS = 20\nKERNEL_TYPE = \"multiscale\"\n\n# Class definition Generative with Maximum Mean Discrepancy (GMMD)\nclass GMMD(nn.Module):\n    def __init__(self, n_start, n_out):\n        super(GMMD, self).__init__()\n        self.fc1 = nn.Linear(n_start, 1000)\n        self.fc2 = nn.Linear(1000, 600)\n        self.fc3 = nn.Linear(600, 1000)\n        self.fc4 = nn.Linear(1000, n_out)\n\n    def forward(self, samples):\n        x = torch.sigmoid(self.fc1(samples))\n        x = torch.sigmoid(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        x = torch.sigmoid(self.fc4(x))\n        return x\n\n# use gpu if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# dataloader\ntrans = transforms.Compose([transforms.ToTensor()])\ntrain_set = datasets.EMNIST(root=root, split=\"digits\",train=True, transform=trans, download=True)\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_set,\n        batch_size=BATCH_SIZE,\n        shuffle=True)\n\n# define the GMMD\n\ngmmd_net = GMMD(N_INP, N_OUT).to(device)\n\ntry:\n  gmmd_net.load_state_dict(torch.load(\"gmmd.pth\"))\n  print(\"Model parameters are loaded\")\n  \nexcept:\n  pass\n\ngmmd_optimizer = optim.RMSprop(gmmd_net.parameters(), lr=0.004)\n\ndef train_one_step(x, samples):\n    samples = Variable(samples).to(device)\n    gen_samples = gmmd_net(samples)\n\n    loss = MMD(x, gen_samples, KERNEL_TYPE)\n    gmmd_optimizer.zero_grad()\n    loss.backward()\n    gmmd_optimizer.step()\n\n    return loss\n  \ndef MMD(x, y, kernel):\n    \"\"\"Emprical maximum mean discrepancy. The lower the result, the more evidence that distributions are the same.\n\n    Args:\n        x: first sample, distribution P\n        y: second sample, distribution Q\n        kernel: kernel type such as \"multiscale\" or \"rbf\"\n    \"\"\"\n    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n    \n    dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n    dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n    dxy = rx.t() + ry - 2. * zz # Used for C in (1)\n    \n    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n                  torch.zeros(xx.shape).to(device),\n                  torch.zeros(xx.shape).to(device))\n    \n    if kernel == \"multiscale\":\n        \n        bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n        for a in bandwidth_range:\n            XX += a**2 * (a**2 + dxx)**-1\n            YY += a**2 * (a**2 + dyy)**-1\n            XY += a**2 * (a**2 + dxy)**-1\n            \n    if kernel == \"rbf\":\n      \n        bandwidth_range = [10, 15, 20, 50]\n        for a in bandwidth_range:\n            XX += torch.exp(-0.5*dxx\/a)\n            YY += torch.exp(-0.5*dyy\/a)\n            XY += torch.exp(-0.5*dxy\/a)\n      \n      \n\n    return torch.mean(XX + YY - 2. * XY)\n","a1c9f995":"# training loop\niterations = 0\nZ = torch.randn((5800,BATCH_SIZE, N_INP))\nfor ep in range(N_GEN_EPOCHS):\n    avg_loss = 0\n    resampling_limit = 300 # From paper\n    \n    for idx, (x, _) in enumerate(train_loader):\n        iterations += 1\n        x = x.view(x.size()[0], -1)\n        x = Variable(x).to(device)\n        \n        # normal random noise between [0, 1]\n        random_noise = Z[idx,:,:]\n\n        loss = train_one_step(x, random_noise)\n        avg_loss += loss.item()\n    \n        if iterations % 300 == 0:\n           Z = random_noise = torch.randn((5800,BATCH_SIZE, N_INP))     \n       \n           \n    avg_loss \/= (idx + 1)\n    print(f\"GMMD Training: {ep}. epoch completed, average loss: {avg_loss}\")\n\ntorch.save(gmmd_net.state_dict(), \"gmmd.pth\")","60e1517b":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\n\nZ = random_noise = torch.randn((5800,BATCH_SIZE, N_INP))\ntrans = transforms.Compose([transforms.ToTensor()])\ntest_set = datasets.EMNIST(root=root, split=\"digits\",train=False, transform=trans, download=True)\nview_data = [test_set[i][0] for i in range(4 * 8)]\nplt.gray()\n\nprint(\"Images generated by GMMD\")\n\nfor r in range(4):\n  for c in range(8):\n    ax = plt.subplot(4, 8, r * 8 + c + 1)\n\n    noise = torch.randn((1, N_INP)).to(device)\n    y = gmmd_net(noise)\n    plt.imshow(y.detach().squeeze().cpu().reshape(28, 28))\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n","7a9f8a93":"After training, we can sample from the model and visualize them so that it is possible to inspect the quality of generated instances. Following code draws 32 samples from our trained neural network.","3f94499f":"# References\n\n[[1]]() Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. *Journal of Machine Learning Research*, 13(Mar):723\u2013773, 2012.\n\n[[2]]() Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural\nnetworks via maximum mean discrepancy optimization. *arXiv preprint arXiv:1505.03906*, 2015.\n\n[[3]]() Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. *arXiv\npreprint arXiv:1502.02761*, 2015\n\n[[4]]() Lynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric W Pellegrini, Ralf S\nKlessen, Lena Maier-Hein, Carsten Rother, and Ullrich Kothe. Analyzing inverse problems with invertible neural networks. *arXiv preprint arXiv:1808.04730*, 2018.","ac982a14":"# Implementation of MMD\n\nIn this section, we code the equation (5) in pytorch. This implementation benefits greatly from the pytorch forum discussion in [here](https:\/\/discuss.pytorch.org\/t\/maximum-mean-discrepancy-mmd-and-radial-basis-function-rbf\/1875). In addition to MMD, curious reader can find custom loss function implementations using pytoch in [here](https:\/\/discuss.pytorch.org\/t\/build-your-own-loss-function-in-pytorch\/235). Main idea behind the code is first obtaining the similarity matrices between  $X$ and $X$, $X$ and $Y$, finally $Y$ and $Y$ with given distance metric, then plugging the results to kernel specific function such as exponential. \n\nFor example, let's say kernel in question is gaussian meaning \n\n$$ k(\\mathbf{x_{i}}, \\mathbf{x_{j}}) = \\exp \\left(\\frac{- \\Vert \\mathbf{x_{i}} - \\mathbf{x_{j}} \\Vert^{2}}{2\\sigma^{2}}\\right) = \\exp \\left(\\frac{-1}{\\sigma^{2}} [\\mathbf{x_{i}}^\\intercal \\mathbf{x_{i}} - 2 \\mathbf{x_{i}}^\\intercal \\mathbf{x_{j}} + \\mathbf{x_{j}}^\\intercal \\mathbf{x_{j}}]\\right) $$\n\nIf we can construct a matrix with elements such that for every i and j corresponding element is $[\\mathbf{x_{i}}^\\intercal \\mathbf{x_{i}} - 2 \\mathbf{x_{i}}^\\intercal \\mathbf{x_{j}} + \\mathbf{x_{j}}^\\intercal \\mathbf{x_{j}}]$, then it is possible to just plug that matrix into `pytorch.exp()` for the result.","f00c13de":"# Implementation of \"Training Generative Neural Networks via Maximum Mean Discrepancy [[2]](#references)\"\n\nPaper specifications of setting and neural network used in the experimental results:\n\n- Dataset is **EMNIST** (For some reason MNIST downloader of torchvision throws and error).\n- Input size is not specified in the paper, so chosen size in this example is 32. Output size is the same as the dimension of **EMNIST** dateset instances.\n- 3 fully connected hidden layers with the size of 1000, 600 and 1000. Preferred activation function is RELU.\n- Batch size is 500.\n- Optimizer is RMSprop.\n- Kernel function is radial basis function.\n\nObservations:\n\n- In the paper, an extensive hyperparameter search is conducted so kernel choice, learning rate etc. in this example is arbitrary and decided purely based on exemplary reasons.\n- Since no hyperparameter tunning is performed, `multiscale` kernel is found to be more successful than gaussian kernel contrary to the paper. In fact, when kernel type is chosen as `rbf`, a clear mode collapse occurs. ","a782314c":"# Formal Definition\n\nMaximum mean discrepancy (MMD) can be defined in two different ways which are equivalent to each other:\n\n1. *MMD* is a distance (difference) between *feature means*.\n2. *MMD* is an integral probability metric (which will not be covered in this post. Please refer to [[1]](#references)).\n\nLet's start with the concepts used in definition of feature means. Firstly given an $X$, a feature map $\\phi$ maps $X$ to an another space $\\mathcal{F}$ such that $\\phi(X) \\in \\mathcal{F}$. Assuming $\\mathcal{F}$ satisfies the necessary conditions, we can benefit from the kernel trick to compute the inner product in $\\mathcal{F}$:\n\n$$ X, Y \\text{ such that} ~~ k(X, Y) = \\langle \\phi(X), \\phi(Y) \\rangle_{\\mathcal{F}} $$\n\n**1. Feature means:** Given a probability measure $P$ on $\\mathcal{X}$, feature means (or mean embedding as sometimes called in the literature) is an another feature map that takes $\\phi(X)$ and maps it to the means of every coordinate of $\\phi(X)$:\n\n$$ \\mu_{p} ~ (\\phi(X)) = \\left[ E[\\phi(X_{1}], \\cdots , E[\\phi(X_{m}] \\right]^{T} \\tag{1} $$ \n\nInner product of feature means of $X \\sim P$ and $Y \\sim Q$ can be written in terms of kernel function such that:\n\n$$ \\langle \\mu_{P}~(\\phi(X), \\mu_{Q}~(\\phi(Y) \\rangle_{\\mathcal{F}} = E_{P,Q}~ [\\langle \\phi(X), \\phi(Y) \\rangle_{\\mathcal{F}}] = E_{P,Q}~ [k(X,Y)] \\tag{2}$$\n\n**2. Maximum mean discrepancy:** Given $X,Y$ maximum mean discrepancy is the distance between feature means of $X,Y$:\n\n$$ MMD^{2}(P,Q) = \\Vert \\mu_{P} - \\mu_{Q} \\Vert^{2} _{\\mathcal{F}} \\tag{3} $$ \n\nFor convenience we have left out the $\\phi(\\cdot)$ parts. If we use the norm induced by the inner product such that $\\Vert x \\Vert = \\sqrt{\\langle x, x \\rangle}$, the equation (3) becomes\n\n$$ MMD^{2}(P,Q) = \\langle \\mu_{P} - \\mu_{Q}, ~ \\mu_{P} - \\mu_{Q} \\rangle = \\langle \\mu_{P}, \\mu_{P} \\rangle -2 \\langle \\mu_{P}, \\mu_{Q} \\rangle + \\langle \\mu_{Q}, \\mu_{Q} \\rangle $$\n\nUsing the equation (2), finally above expression becomes\n\n$$ MMD^{2}(P,Q) = E_{P}~ [k(X,X)] - 2 E_{P,Q}~ [k(X,Y)] + E_{Q}~ [k(Y,Y)] \\tag{4} $$\n\nWe will use the equation (4) in the derivation of empirical estimate of the MMD.\n\n- **Remark:** The term maximum in maximum mean discrepancy comes from the second definition of MMD which is an integral probability metric including a supremum. This definition utilize a supremum and a function belonging to a unit ball $F$ in Reproducing Kernel Hilbert Space. We are only giving the formal definition and remind the reader that both definition are equivalent. For further information please refer to [[1]](#references)\n\n$$ MMD(P, Q; \\mathcal{F}) = \\underset{\\Vert f \\Vert \\leq 1}{sup~} E_{X} [f(X)] - E_{Y} [f(Y)] $$\n\n**3. Empirical estimation of MMD:** Even though we are working with distributions so far, in real life settings we don't have access to the underlying distribution of our data. For this reason, it is possible to use an estimate for the equation (4) with following formula:\n\n\n$$ MMD^{2}(X,Y) = \\underbrace{\\frac{1}{m (m-1)} \\sum_{i} \\sum_{j \\neq i} k(\\mathbf{x_{i}}, \\mathbf{x_{j}})}_\\text{A} - \\underbrace{2 \\frac{1}{m.m} \\sum_{i} \\sum_{j} k(\\mathbf{x_{i}}, \\mathbf{y_{j}})}_\\text{B} + \\underbrace{\\frac{1}{m (m-1)} \\sum_{i} \\sum_{j \\neq i} k(\\mathbf{y_{i}}, \\mathbf{y_{j}})}_\\text{C} \\tag{5} $$\n\nOne interpretation of the equation (5) is that $\\mathbf{x_{i}}$'s are the data points we already have and $\\mathbf{y_{i}}$'s are the generated examples so that MMD score guides us towards the evaluation of underlying distributions. In the next section, we will demonstrate how to implement the equation (5) in pytorch and use the MMD as a training criteria in generative models such that our distribution model doesn't diverge from the given data.","66225ded":"# MAXIMUM MEAN DISCREPANCY (MMD) IN MACHINE LEARNING\n\n\nMaximum mean discrepancy (MMD) is a kernel based statistical test used to determine whether given two distribution are the same which is proposed in [[1]](#references). MMD can be used as a loss\/cost function in various machine learning algorithms such as density estimation, generative models as shown in [[2]](#references), [[3]](#references) and also in invertible neural networks utilized in inverse problems as in [[4]](#references). As opposed to generative adversarial networks (GANs) which require a solution to a complex min-max optimization problem, MMD criteria can be used as simpler discriminator. \n\nMain advantages are easy implementation and rich kernel based theory behind the idea that lends itself to a formal analysis. On the other hand, disadvantages are subjectively \"mediocre\" sample results compared to GANs as reported in [[2]](#references), [[3]](#references) and the same computational cost overheads regarding kernel based methods when the feature size of the data is reasonable large.  \n\n**Notation:**\n- Random variables are denotes with capital $X$.\n- Vectors are column matrices.\n- Vectors are denoted with roman bold type $\\mathbf{x}$.\n- Probability distribution of expected values are not shown explicitly unless unclear from context.\n\n**Note:** Reader who is not interested in formal mathematical derivations can skip to [Implementation of MMD](#implementation_of_mmd) section.\n","2e3eee80":"## Code practice\n\nIn this section, we compare two multivariate gaussian distribution with different mean and covariance"}}