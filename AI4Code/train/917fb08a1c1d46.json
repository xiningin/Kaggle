{"cell_type":{"c3a0c55f":"code","e7e40aa8":"code","cc91f7a4":"code","8bfa4a26":"code","828d6794":"code","f224b874":"code","0bbe3783":"code","dfa27e4a":"code","b41ff802":"code","75ab2851":"code","75651938":"code","32606123":"code","f0663d55":"code","d5f7f78f":"code","687a492c":"code","4858f67a":"code","c7903424":"code","e417bd2c":"code","9ba01b42":"code","268603aa":"code","b5fe678a":"code","d4c3a317":"code","dc966f46":"code","fa845638":"code","84f68615":"code","c146f071":"code","87299086":"code","39eeed2a":"code","238a69e4":"code","5b015aad":"code","054d7aa7":"code","8c75cf5c":"code","89d7196e":"code","e54e369a":"code","e9851bf3":"code","036df792":"code","b8d06656":"code","3d2b3279":"code","31ffbdc0":"code","e3f6faef":"code","c1d03429":"code","becb54e9":"code","67662308":"code","9de8dec9":"code","8163fb9c":"code","c7e991ff":"code","02bb816a":"code","d6e1462f":"markdown","8031ac09":"markdown","55903b0e":"markdown"},"source":{"c3a0c55f":"import pandas as pd\nimport numpy as np","e7e40aa8":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df  = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ndf = pd.concat([train_df, test_df])\ndf.shape","cc91f7a4":"df.head()","8bfa4a26":"df.isna().sum() \/ df.shape[0] * 100","828d6794":"train_df.describe()","f224b874":"test_df.describe()","0bbe3783":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# colore settings:\nsns.set(rc={'axes.facecolor':\"#EBE0BA\",\n            \"figure.facecolor\":\"#E0D3AF\",\n            \"grid.color\":\"#E0D3AF\",\n            \"axes.edgecolor\":\"#424949\",\n            \"axes.labelcolor\":\"#424949\",\n            \"text.color\":\"#424949\" # color for headlines and sub headlines\n           }) \n\n# font size settings\nsns.set_context(rc={\"axes.labelsize\" : 15})\n\n# change font tipe to Times New Roman: (newspaper look)\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\nsurvival_pal = {0: \"#943126\", 1:\"#1D8348\"}\nscreening_df = train_df.copy()\ng = sns.pairplot(screening_df, hue=\"Survived\", palette = survival_pal ,height=2, aspect=1.63)\ng.map_lower(sns.kdeplot, levels=4, color=\"#424949\")\ng.fig.subplots_adjust(top=0.95)\ng.fig.suptitle(\"Pairplot for numeric features\", fontsize=\"28\");","dfa27e4a":"# build figure\nfig = plt.figure(figsize=(25,5))\n\n# add grid to figure\ngs = fig.add_gridspec(1,3)\n\n# fill grid with subplots\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\n# adjust subheadline fontsize\nax0.set_title('Pclass = 1', fontsize=20)\nax1.set_title('Pclass = 2', fontsize=20)\nax2.set_title('Pclass = 3', fontsize=20)\n\n# adjust lable fontsize\nax0.tick_params(labelsize=15)\nax1.tick_params(labelsize=15)\nax2.tick_params(labelsize=15)\n\n# plot data into subplots \nsns.kdeplot(data=screening_df[screening_df['Pclass']==1], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax0, linewidth = 3, ec=\"#424949\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==2], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax1, linewidth = 3, ec=\"#424949\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==3], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax2, linewidth = 3, ec=\"#424949\").set(xlabel=\"Fare\", ylabel=\"\")\n\n# add headline\nfig.subplots_adjust(top=0.8)\nfig.suptitle('Fare-Distribution for each class', fontsize=\"28\");","b41ff802":"fig = plt.figure(figsize=(25, 4))\nax = sns.boxplot(x=\"Age\", y=\"Pclass\",hue=\"Survived\",data=screening_df, orient=\"h\", palette={0: \"#E6B0AA\", 1:\"#A9DFBF\"}, linewidth = 3)\nax.tick_params(labelsize=15)\n# add headline\nfig.subplots_adjust(top=0.8)\nfig.suptitle(\"Age\/Pclass and Survived\", fontsize=\"28\");","75ab2851":"fg = sns.displot(\n    screening_df, x=\"Age\", col=\"Pclass\", row=\"Sex\", kde=True, palette=survival_pal, hue = 'Survived',\n    binwidth=3, height=4, facet_kws=dict(margin_titles=True), aspect=1.63, linewidth = 1)\nfg.set_xticklabels(fontsize=15)\n# change range for x axis\nplt.xlim(0, 85)\n# add headline\nfg.fig.subplots_adjust(top=0.85)\nfg.fig.suptitle('Age distribution per Pclass and Sex', fontsize=\"28\");","75651938":"try:\n    sex_dict = {'male': 0, 'female': 1}\n    embarked_dict = {'S': 1, 'C': 2, 'Q': 3}\n    screening_df = screening_df[screening_df['Embarked'].notnull()].copy()\n    screening_df.loc[:, 'Sex'] = screening_df.loc[:,'Sex'].map(lambda x: sex_dict[x])\n    screening_df.loc[:, 'Embarked'] = screening_df.loc[:, 'Embarked'].map(lambda x: embarked_dict[x])\nexcept:\n    print('already converted')\n# We will use one hot encoder later on. Otherwise the natural order of those numbers could irritate the ML algorithm.\n# Correlations:\nimport numpy as np\n\n# claculate correlations:\ncorr = screening_df.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# build figure\nf, ax = plt.subplots(figsize=(25, 15))\n# change x- and y-label size\nax.tick_params(axis='both', which='major', labelsize=15)\n# plot mast\nsns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, square=True, linewidths=1, linecolor=\"#424949\", annot=True, \n                cbar_kws={\"shrink\": 0.6}).set_title('Pairwise correlation', fontsize=\"28\");","32606123":"col_drops = ['PassengerId', 'Cabin', 'Ticket', 'Name']","f0663d55":"train_df.dropna(subset=['Embarked'], inplace=True)\ntrain_df.loc[:, \"Age\"] = train_df.groupby(['Pclass', 'Sex']).transform(lambda x: x.fillna(x.mean())) ","d5f7f78f":"train_df = train_df.loc[:, ~train_df.columns.isin(col_drops)].copy()","687a492c":"target = 'Survived'","4858f67a":"x_train = train_df.drop(columns=target)\ny_train = train_df[target]","c7903424":"x_train.head()","e417bd2c":"x_train['Pclass'].value_counts()","9ba01b42":"x_train['Sex'].value_counts()","268603aa":"x_train['SibSp'].value_counts()","b5fe678a":"x_train['Parch'].value_counts()","d4c3a317":"x_train['Fare'].describe()","dc966f46":"x_train['Embarked'].value_counts()","fa845638":"x_train.isna().sum()","84f68615":"def one_hot_encoding(df, column):\n    # Get one hot encoding of columns B\n    one_hot_df = pd.get_dummies(df[column])\n    # Drop input column as it is now encoded\n    df = df.drop(column, axis=1)\n    # Join the encoded df\n    \n    df = df.join(one_hot_df)\n    return df","c146f071":"x_train = one_hot_encoding(x_train, 'Embarked')","87299086":"x_train = one_hot_encoding(x_train, 'Sex')","39eeed2a":"x_train.head()","238a69e4":"from sklearn.preprocessing import StandardScaler","5b015aad":"scaler = StandardScaler()","054d7aa7":"x_train[['Age', 'Fare']] = scaler.fit_transform(x_train[['Age', 'Fare']])","8c75cf5c":"x_train['Age'].describe()","89d7196e":"x_train['Fare'].describe()","e54e369a":"from sklearn.model_selection import train_test_split, GridSearchCV\nimport lightgbm as lgb\nfrom tqdm import tqdm","e9851bf3":"params = {\n    'application': 'binary', # for binary classification\n    'boosting': 'gbdt', # traditional gradient boosting decision tree\n    'num_iterations': 100, \n    'learning_rate': 0.05,\n    'num_leaves': 62,\n    'device': 'cpu', # you can use GPU to achieve faster learning\n    'max_depth': -1, # <0 means no limit\n    'max_bin': 510, # Small number of bins may reduce training accuracy but can deal with over-fitting\n    'lambda_l1': 5, # L1 regularization\n    'lambda_l2': 10, # L2 regularization\n    'metric' : 'binary_error',\n    'subsample_for_bin': 200, # number of samples for constructing bins\n    'subsample': 1, # subsample ratio of the training instance\n    'colsample_bytree': 0.8, # subsample ratio of columns when constructing the tree\n    'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n    'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n    'min_child_samples': 5# minimum number of data needed in a leaf\n}\n\n# Initiate classifier to use\nmdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n          objective = 'binary', \n          n_jobs = 5, \n          silent = True,\n          max_depth = params['max_depth'],\n          max_bin = params['max_bin'], \n          subsample_for_bin = params['subsample_for_bin'],\n          subsample = params['subsample'], \n          min_split_gain = params['min_split_gain'], \n          min_child_weight = params['min_child_weight'], \n          min_child_samples = params['min_child_samples'])\n\n# To view the default model parameters:\nmdl.get_params()","036df792":"gridParams = {\n    'learning_rate': [0.005, 0.01],\n    'n_estimators': [8,16,24],\n    'num_leaves': [8,12,16], # large num_leaves helps improve accuracy but might lead to over-fitting\n    'boosting_type' : ['gbdt', 'dart'], # for better accuracy -> try dart\n    'objective' : ['binary'],\n    'max_bin':[255, 510], # large max_bin helps improve accuracy but might slow down training progress\n    'random_state' : [42],\n    'colsample_bytree' : [0.65],\n    'subsample' : [0.7],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4],\n    }\n\ngrid = GridSearchCV(mdl, gridParams, verbose=1, cv=3, n_jobs=-1)\n# Run the grid\ngrid.fit(x_train, y_train)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)","b8d06656":"pd.DataFrame(grid.cv_results_)","3d2b3279":"params['colsample_bytree'] = grid.best_params_['colsample_bytree']\nparams['learning_rate'] = grid.best_params_['learning_rate'] \nparams['max_bin'] = grid.best_params_['max_bin']\nparams['num_leaves'] = grid.best_params_['num_leaves']\nparams['reg_alpha'] = grid.best_params_['reg_alpha']\nparams['reg_lambda'] = grid.best_params_['reg_lambda']\nparams['subsample'] = grid.best_params_['subsample']","31ffbdc0":"X_train, X_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state = 42)","e3f6faef":"mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n          objective = 'binary', \n          n_jobs = 5, \n          silent = True,\n          max_depth = params['max_depth'],\n          max_bin = params['max_bin'], \n          subsample_for_bin = params['subsample_for_bin'],\n          subsample = params['subsample'], \n          min_split_gain = params['min_split_gain'], \n          min_child_weight = params['min_child_weight'], \n          min_child_samples = params['min_child_samples'],\n          col_sample_by_tree=params['colsample_bytree'],\n          learning_rate=params['learning_rate'],\n          num_leaves=params['num_leaves'],\n          reg_alpha=params['reg_alpha'],\n          reg_lambda=params['reg_lambda'],)","c1d03429":"mdl.fit(X_train, y_train)","becb54e9":"mdl.get_params()","67662308":"p_test = mdl.predict_proba(X_valid)","9de8dec9":"from sklearn import metrics","8163fb9c":"metrics.roc_auc_score(y_valid, p_test[:,1])","c7e991ff":"import matplotlib.pyplot as plt","02bb816a":"metrics.plot_roc_curve(mdl, X_valid, y_valid)  \nplt.show()   ","d6e1462f":"# Load Best Model","8031ac09":"# GridSearchCV","55903b0e":"# Feat. Engineering"}}