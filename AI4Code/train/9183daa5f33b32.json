{"cell_type":{"b4d7c75a":"code","5e0a7cb1":"code","ff931e24":"code","914ea4ff":"code","29ab2988":"code","caa6dfa6":"code","9617948d":"code","b4ea3b85":"code","c46f8162":"code","ba765a59":"code","b613f10d":"code","82b010cd":"code","78b7291b":"code","98174d50":"code","3650cdb1":"code","6ddf10f7":"code","f949b189":"code","887b765d":"code","b6c27be4":"code","92928af6":"code","d2655f9e":"code","e7c77c40":"code","89d3a703":"code","8d7b58fb":"code","beb4450d":"code","2b96afdf":"code","b945abc6":"code","9e670fde":"code","87e6045b":"code","f4324f58":"code","d52f2b1b":"code","60e1d91a":"code","05f5eb74":"code","183868ed":"code","463548ed":"code","7276d645":"code","cddc5cf1":"code","9dd8e0d7":"code","f8a1c26e":"code","b1020141":"code","639b11ab":"code","e757d99b":"code","99a5ff9f":"code","3c92123a":"code","18e266f7":"code","f4e3cff4":"code","a512ffdc":"code","823f628a":"code","1bc446ac":"code","a952ccea":"code","c14f5ef0":"code","95dd2038":"code","4d5e0b34":"code","2e5d62e4":"code","2d805d08":"code","99243146":"code","6add610a":"code","25b2c16b":"code","db4d312d":"code","912f889b":"code","3cd2c7cd":"code","aac56d99":"code","23ee1f31":"code","89a12747":"code","53f2d6c4":"markdown","22515109":"markdown","02905da0":"markdown","d323aeb7":"markdown","d755ec87":"markdown","1fec4592":"markdown","c0fb8efb":"markdown","20cfb35d":"markdown","a048212c":"markdown","b05fbd55":"markdown"},"source":{"b4d7c75a":"import pandas as pd\nimport re\nimport os\nimport time\nimport random\nimport numpy as np\n\ntry:\n  %tensorflow_version 2.x # enable TF 2.x in Colab\nexcept Exception:\n  pass\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\n# from google.colab import drive\nimport pickle\n\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\nfrom nltk.translate.bleu_score import corpus_bleu","5e0a7cb1":"!git clone https:\/\/github.com\/aashnakanuga\/dl-math-word-problem-solving.git","ff931e24":"tf.__version__","914ea4ff":"# Mount drive\n# drive.mount('\/gdrive')\n# drive_root = '\/gdrive\/My Drive\/'","29ab2988":"# os.listdir('.\/dl-math-word-problem-solving\/data_final.pkl')","caa6dfa6":"with open('.\/dl-math-word-problem-solving\/data_final.pkl', 'rb') as f:\n  df = pickle.load(f)","9617948d":"df.shape","b4ea3b85":"df.head()","c46f8162":"def convert_eqn(eqn):\n  '''\n  Add a space between every character in the equation string.\n  Eg: 'x = 23 + 88' becomes 'x =  2 3 + 8 8'\n  '''\n  elements = list(eqn)\n  return ' '.join(elements)","ba765a59":"input_exps = list(df['Question'].values)","b613f10d":"target_exps = list(df['Equation'].apply(lambda x: convert_eqn(x)).values)","82b010cd":"# Input: Word Problem\ninput_exps[:5]","78b7291b":"# Target: Equation\ntarget_exps[:5]","98174d50":"len(pd.Series(input_exps)), len(pd.Series(input_exps).unique())","3650cdb1":"len(pd.Series(target_exps)), len(pd.Series(target_exps).unique())","6ddf10f7":"def preprocess_input(sentence):\n  '''\n  For the word problem, convert everything to lowercase, add spaces around all\n  punctuations and digits, and remove any extra spaces. \n  '''\n  sentence = sentence.lower().strip()\n  sentence = re.sub(r\"([?.!,\u2019])\", r\" \\1 \", sentence)\n  sentence = re.sub(r\"([0-9])\", r\" \\1 \", sentence)\n  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n  sentence = sentence.rstrip().strip()\n  return '<start> ' + sentence + ' <end>'","f949b189":"def preprocess_target(sentence):\n  '''\n  For the equation, convert it to lowercase and remove extra spaces\n  '''\n  sentence = sentence.lower().strip()\n  return '<start> ' + sentence + ' <end>'","887b765d":"preprocessed_input_exps = list(map(preprocess_input, input_exps))\npreprocessed_target_exps = list(map(preprocess_target, target_exps))","b6c27be4":"preprocessed_input_exps[:5]","92928af6":"preprocessed_target_exps[:5]","d2655f9e":"def tokenize(lang):\n  '''\n  Tokenize the given list of strings and return the tokenized output\n  along with the fitted tokenizer.\n  '''\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n  lang_tokenizer.fit_on_texts(lang)\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n\n  return tensor, lang_tokenizer","e7c77c40":"input_tensor, inp_lang_tokenizer = tokenize(preprocessed_input_exps)","89d3a703":"len(inp_lang_tokenizer.word_index)","8d7b58fb":"input_tensor","beb4450d":"target_tensor, targ_lang_tokenizer = tokenize(preprocessed_target_exps)","2b96afdf":"len(targ_lang_tokenizer.word_index)","b945abc6":"target_tensor","9e670fde":"# Creating training and validation sets\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n                                                                                                target_tensor,\n                                                                                                test_size=0.05,\n                                                                                                random_state=42)","87e6045b":"len(input_tensor_train)","f4324f58":"len(input_tensor_val)","d52f2b1b":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train)\/\/BATCH_SIZE\nembedding_dim = 32\nunits = 256\nvocab_inp_size = len(inp_lang_tokenizer.word_index)+1\nvocab_tar_size = len(targ_lang_tokenizer.word_index)+1\ndropout = 0.5\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","60e1d91a":"vocab_inp_size, vocab_tar_size","05f5eb74":"example_input_batch, example_target_batch = next(iter(dataset))\nexample_input_batch.shape, example_target_batch.shape","183868ed":"class Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    \n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    \n    # Bidirectional GRU Unit\n    self.gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform',\n                                   dropout = dropout))\n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n    output, state1, state2 = self.gru(x, initial_state = hidden)\n    state = [state1,state2]\n    return output, state\n\n  def initialize_hidden_state(self):\n    return [tf.zeros((self.batch_sz, self.enc_units)),tf.zeros((self.batch_sz, self.enc_units))]","463548ed":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)","7276d645":"class BahdanauAttention(tf.keras.layers.Layer):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, query, values):\n    \n    # we are doing this to perform addition to calculate the score\n    hidden_with_time_axis = tf.expand_dims(query, 1)\n\n    score = self.V(tf.nn.tanh(\n        self.W1(values) + self.W2(hidden_with_time_axis)))\n\n    # Get attention_weights\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights","cddc5cf1":"attention_layer = BahdanauAttention(100)","9dd8e0d7":"class Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n\n    #LSTM Unit\n    self.lstm = tf.keras.layers.LSTM(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n\n    # used for attention\n    self.attention = BahdanauAttention(self.dec_units)\n\n  def call(self, x, hidden, enc_output):\n    context_vector, attention_weights = self.attention(hidden, enc_output)\n\n    x = self.embedding(x)\n    \n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the LSTM\n    output, state, cell_state = self.lstm(x)\n\n    output = tf.reshape(output, (-1, output.shape[2]))\n\n    x = self.fc(output)\n\n    return x, state, attention_weights","f8a1c26e":"decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)","b1020141":"optimizer = tf.keras.optimizers.Adam(lr=0.001)\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  # Apply a mask to paddings (0)\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","639b11ab":"# !rm -r \/gdrive\/My\\ Drive\/ADL\\ Project\/checkpoints\/training_checkpoints\/aashna_bidir_new","e757d99b":"!mkdir drive","99a5ff9f":"checkpoint_dir = os.path.join(\".\/drive\", \"ADL Project\/checkpoints\")\ncheckpoint_dir = os.path.join(checkpoint_dir, \"training_checkpoints\/aashna_bidir_new\")\n\nprint(\"Checkpoints directory is\", checkpoint_dir)\nif os.path.exists(checkpoint_dir):\n  print(\"Checkpoints folder already exists\")\nelse:\n  print(\"Creating a checkpoints directory\")\n  os.makedirs(checkpoint_dir)\n\n\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","3c92123a":"latest = tf.train.latest_checkpoint(checkpoint_dir)\nlatest","18e266f7":"if latest:\n  epoch_num = int(latest.split('\/')[-1].split('-')[-1])\n  checkpoint.restore(latest)\nelse:\n  epoch_num = 0","f4e3cff4":"epoch_num","a512ffdc":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n\n  with tf.GradientTape() as tape:\n    enc_output, enc_hidden = encoder(inp, enc_hidden)\n    # print(enc_output.shape)\n    # print(len(enc_hidden))\n    # print(enc_hidden[0].shape)\n    dec_hidden = enc_hidden[0]\n\n    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n\n    # Teacher forcing - feeding the target as the next input, to ensure proper training\n    for t in range(1, targ.shape[1]):\n      # passing enc_output to the decoder\n      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n\n      loss += loss_function(targ[:, t], predictions)\n\n      # using teacher forcing\n      dec_input = tf.expand_dims(targ[:, t], 1)\n\n  batch_loss = (loss \/ int(targ.shape[1]))\n\n  variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, variables)\n\n  optimizer.apply_gradients(zip(gradients, variables))\n\n  return batch_loss","823f628a":"EPOCHS = 5\n\nfor epoch in range(epoch_num, EPOCHS):\n  start = time.time()\n\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n\n    if batch % 100 == 0:\n        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                     batch,\n                                                     batch_loss.numpy()))\n  checkpoint.save(file_prefix = checkpoint_prefix)\n  print('Saved epoch: {} at {}'.format(epoch+1, checkpoint_dir))\n\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss \/ steps_per_epoch))\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","1bc446ac":"def max_length(tensor):\n    return max(len(t) for t in tensor)","a952ccea":"max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)","c14f5ef0":"def evaluate(sentence):\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n\n    # Preprocess the input\n    sentence = preprocess_input(sentence)\n\n    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                           maxlen=max_length_inp,\n                                                           padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    \n    result = ''\n\n    hidden = [tf.zeros((1, units)),tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n\n    dec_hidden = enc_hidden[0]\n    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                             dec_hidden,\n                                                             enc_out)\n\n        # storing the attention weights to plot later on\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n\n        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n\n        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n            return result, sentence, attention_plot\n\n        # the predicted ID is fed back into the model\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result, sentence, attention_plot","95dd2038":"# function for plotting the attention weights\ndef plot_attention(attention, sentence, predicted_sentence):\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attention, cmap='viridis')\n\n    fontdict = {'fontsize': 14}\n\n    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()","4d5e0b34":"def translate(sentence):\n    result, sentence, attention_plot = evaluate(sentence)\n\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))\n\n    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n    plot_attention(attention_plot, sentence.split(' '), result.split(' '))","2e5d62e4":"# restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","2d805d08":"def evaluate_accuracy(inputs):\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n    sentence = ''\n    for i in range(len(inputs.numpy()[0])):\n      if inputs.numpy()[0][i] != 0: #padding token\n        sentence += inp_lang_tokenizer.index_word[inputs.numpy()[0][i]] + ' '\n\n    inputs = tf.convert_to_tensor(inputs)\n    \n    result = ''\n    result_seq = ''\n    \n    hidden = [tf.zeros((1, units)),tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    \n    dec_hidden = enc_hidden[0]\n    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n    \n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                             dec_hidden,\n                                                             enc_out)\n\n        # storing the attention weights to plot later on\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        \n        result_seq += str(predicted_id) +' '\n        \n        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n\n        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n            return result_seq, result, sentence, attention_plot\n\n        # the predicted ID is fed back into the model\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result_seq, result, sentence, attention_plot","99243146":"dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\ndataset_val = dataset_val.batch(1, drop_remainder=True)","6add610a":"y_true = []\ny_pred = []\nacc_cnt = 0\n\na = 0\nfor (inp_val_batch, target_val_batch) in iter(dataset_val):\n  a += 1\n  if a % 50 == 0:\n    print(a)\n    print(\"Accuracy count: \",acc_cnt)\n    print('------------------')\n\n  target_sentence = ''\n  for i in target_val_batch.numpy()[0]:\n    if i!= 0:\n      target_sentence += (targ_lang_tokenizer.index_word[i] + ' ')\n  target_sentence = target_sentence.split('<start> ')[1]\n  # print('True:{}'.format(target_sentence))\n  y_true.append([target_sentence.split(' ')])\n\n  res_seq, res, sent, att = evaluate_accuracy(inp_val_batch)\n  y_pred.append(res.split(' '))\n  \n  if target_sentence == res:\n    acc_cnt += 1","25b2c16b":"print('Corpus BLEU score of the model: ', corpus_bleu(y_true, y_pred))","db4d312d":"print('Accuracy of the model: ', acc_cnt\/len(input_tensor_val))","912f889b":"check_str = ' '.join([inp_lang_tokenizer.index_word[i] for i in input_tensor_val[0] if i != 0][1:-1])","3cd2c7cd":"check_str","aac56d99":"translate(check_str)","23ee1f31":"translate(\"Jerry had 135 pens. John took 19 from him. How many does Jerry have left?\")","89a12747":"translate(\"Jerry had 100 pens. John took 30 pens from Jerry. How many pens are left with Jerry?\")","53f2d6c4":"### Evaluation","22515109":"### Training","02905da0":"This model does not overfit as the baseline did with the old dataset. It gives reasonable scores for the dataset. However, we can see from the attention plots that even though a lot of translations are correct or very close, the word it is supposed to give to attention to while translating is not correct.\n\nThis is why we also decided to experiment with Transformer models as well to see if it will give better results.\n\nSources:\n1. https:\/\/web.stanford.edu\/class\/cs224n\/reports\/custom\/15843468.pdf\n2. https:\/\/www.tensorflow.org\/tutorials\/text\/nmt_with_attention#top_of_page","d323aeb7":"### Create a tf.data dataset","d755ec87":"#### Translation and Attention Plots","1fec4592":"### Conclusions","c0fb8efb":"### Encoder Decoder Model","20cfb35d":"### Preprocessing and Tokenizing the Input and Target exps","a048212c":"### Creating the dataset of word problems\n\n*Please add the correct path to load the data file*","b05fbd55":"### Checkpoints"}}