{"cell_type":{"8ba19654":"code","e02628f0":"code","c9cae252":"code","562e68e2":"code","ad0d7e5e":"code","b30ae061":"code","01e8166f":"code","d879439f":"code","d34eb6c5":"code","25ee5ee8":"code","7bfa5a26":"code","f2b5bc3b":"code","22a7b397":"code","aa23dd1d":"code","1d3eaca6":"code","6f0f9beb":"code","a4d64e1f":"code","62a53b9a":"code","1e659769":"code","bceee681":"code","82851292":"code","f82b5fca":"code","ae41fe7f":"code","1423997e":"code","e05d3fe2":"code","3a1e6794":"code","5e7ae99c":"code","a3d61422":"code","5cf4a25a":"code","fad6cd42":"code","80db858b":"code","5ff86573":"code","673c0deb":"code","161bc533":"code","7666d457":"code","24e0e04a":"markdown","0da344aa":"markdown","4db9f66d":"markdown","25d713c6":"markdown","49c0831f":"markdown","91841426":"markdown","3eb62e93":"markdown","20114c68":"markdown","acf1468c":"markdown","a4a70744":"markdown","43b6e498":"markdown","4d6d2a8e":"markdown","4fa8bd4b":"markdown","60ab98b0":"markdown","cc62337c":"markdown","8501a19e":"markdown","fe2ec390":"markdown","c94540d7":"markdown","27169641":"markdown","fe2756e0":"markdown","7af2d0ab":"markdown","2ddbfff7":"markdown","e573e3a4":"markdown","f8a60a21":"markdown","4315fd24":"markdown","4bb35f03":"markdown"},"source":{"8ba19654":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e02628f0":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c9cae252":"print(tf.__version__)","562e68e2":"data = pd.read_csv(\"\/kaggle\/input\/mushroom-classification\/mushrooms.csv\")\ndata.head()","ad0d7e5e":"data.shape","b30ae061":"data.dtypes","01e8166f":"data.isnull().sum()","d879439f":"data[\"veil-type\"].value_counts()","d34eb6c5":"data.drop(columns=[\"veil-type\"],inplace=True)","25ee5ee8":"plt.style.use(\"ggplot\")\ndata[\"class\"].value_counts().plot(kind=\"bar\", \n                                  figsize = (8,5), color = \"darkviolet\")\nplt.title(\"Frequency of the classes of our Target variable\", size=20)\nplt.xlabel(\"Target Variable\", size = 16)\nplt.ylabel(\"Frequency\", size = 16)","7bfa5a26":"data.shape","f2b5bc3b":"X = data.drop(columns=[\"class\"], axis = 1)\ny = data[\"class\"]","22a7b397":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)","aa23dd1d":"def prepare_inputs(X_train, X_test):\n    oe = OrdinalEncoder()\n    oe.fit(X_train)\n    X_train_enc = oe.transform(X_train)\n    X_test_enc = oe.transform(X_test)\n    return X_train_enc, X_test_enc","1d3eaca6":"def prepare_targets(y_train, y_test):\n    le = LabelEncoder()\n    le.fit(y_train)\n    y_train_enc = le.transform(y_train)\n    y_test_enc = le.transform(y_test)\n    return y_train_enc, y_test_enc","6f0f9beb":"def select_features(X_train, y_train, X_test):\n    fs = SelectKBest(score_func=chi2, k='all')\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs","a4d64e1f":"X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\ny_train_enc, y_test_enc = prepare_targets(y_train, y_test)\nX_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)","62a53b9a":"for i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.figure(figsize = (12,6))\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.title(\"Feature Importance Score\", size = 20)\nplt.xlabel(\"Features\/ Variables\", size = 16, color = \"black\")\nplt.ylabel(\"Importance Score\", size = 16, color = \"black\")\nplt.show()","1e659769":"X_train.columns","bceee681":"X_new = X.drop(columns = [\"cap-surface\", \"cap-color\", \"odor\", \"gill-attachment\", \"stalk-shape\",\n                         \"stalk-color-above-ring\", \"stalk-color-below-ring\", \"veil-color\", \"ring-number\"])","82851292":"X_dummies = pd.get_dummies(X_new, drop_first=True, columns = [\"cap-shape\", \"bruises\",\n                                                          \"gill-spacing\", \"gill-size\", \"gill-color\",\n                                                          \"stalk-root\", \"stalk-surface-above-ring\",\n                                                          \"stalk-surface-below-ring\", \"ring-type\",\n                                                          \"spore-print-color\", \"population\", \"habitat\"])","f82b5fca":"X_dummies.shape","ae41fe7f":"X_dummies.columns","1423997e":"label = LabelEncoder()\ndata[\"class_2\"]=label.fit_transform(data[\"class\"])\ny_encoded = data[\"class_2\"]","e05d3fe2":"X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_dummies, y_encoded, test_size = 0.2, random_state = 0, stratify = y_encoded)","3a1e6794":"X_train_2.shape","5e7ae99c":"model = Sequential()\nmodel.add(Dense(X_dummies.shape[1], activation = 'relu', input_dim = X_dummies.shape[1]))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))","a3d61422":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","5cf4a25a":"history = model.fit(X_train_2, y_train_2, batch_size = 1024, epochs = 15, validation_data=(X_test_2, y_test_2), verbose = 1)","fad6cd42":"def plotLearningCurve(history,epochs):\n  epochRange = range(1,epochs+1)\n  plt.figure(figsize = (12,6))\n  plt.plot(epochRange,history.history['accuracy'])\n  plt.plot(epochRange,history.history['val_accuracy'])\n  plt.title('Model Accuracy')\n  plt.xlabel('Epoch')\n  plt.ylabel('Accuracy')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()\n\n  plt.figure(figsize = (12,6))\n  plt.plot(epochRange,history.history['loss'])\n  plt.plot(epochRange,history.history['val_loss'])\n  plt.title('Model Loss')\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()","80db858b":"plotLearningCurve(history,15)","5ff86573":"y_pred = model.predict_classes(X_test_2)","673c0deb":"cm=confusion_matrix(y_test_2, y_pred)\ncm","161bc533":"plt.figure(figsize=(8,6))\nsns.set(font_scale=1.2)\nsns.heatmap(cm, annot=True, fmt = 'g', cmap=\"Reds\", cbar = False)\nplt.xlabel(\"Predicted Label\", size = 18)\nplt.ylabel(\"True Label\", size = 18)\nplt.title(\"Confusion Matrix Plotting\", size = 20)","7666d457":"print(classification_report(y_test_2, y_pred))","24e0e04a":"So as can be seen from the visualization, our dataset seems pretty balanced.","0da344aa":"Here we will remove the veil-type feature, as it has only one level, so it is not useful for the model, so we will remove this.\n\n**Now let's check whether our dataset is balanced or not.**","4db9f66d":"# Data Preparation for NN Model","25d713c6":"First we will import all the required libraries.","49c0831f":"**Note: Also very soon I am planning to work on this same dataset using \"PyTorch\"**","91841426":"# Model Compilation","3eb62e93":"# Mushroom Classification","20114c68":"# Model Fitting","acf1468c":"Now we will check the version of the tensorflow that is being imported.","a4a70744":"Now we will predict the result.","43b6e498":"![boletus-mushrooms.jpg](attachment:boletus-mushrooms.jpg)","4d6d2a8e":"Finally we will train the model, with batch size 1024, and number of epochs as 15. Always remember batch size and epochs are the two important hyperparameters, if you don't chose these values correctly, then either you may overfit the model, or you may underfit the model. So you have to be pretty careful with that.","4fa8bd4b":"**Here we will select features with higher importance score.** So by looking at them we may select feature (1, 3, 6, 7, 8, 10, 11, 12, 17, 18, 19 and 20). Basically I have selected feature with importance score of more than 100, there is no rule that you have to do selection in this way only, it is completely on you what importance score you chose as a deciding factor, but one has to take the features with higher score.\n\nNow we'll actually see what are those features, I mean let's see the name of the features, and to see the names I used the above piece of code. They are:- \n\n1 - \"cap-shape\"\n\n3 - \"bruises\"\n\n6 - \"gill-spacing\"\n\n7 - \"gill-size\"\n\n8 - \"gill-color\"\n\n10 - \"stalk-root\"\n\n11 - \"stalk-surface-above-ring\n\n12 - \"stalk-surface-below-ring\"\n\n17 - \"ring-type\"\n\n18 - \"spore-print-color\"\n\n19 - \"population\"\n\n20 - \"habitat\"\n\nSo now the number of variables\/features has reduced from 21 to 12, which is good, so now we will build our model with these 12 features.","60ab98b0":"Now before feeding these variables to the neural network, we will create dummies for all the variables which we just selected from the above procedure. So in the below piece of code we will drop all the other variables, and take only those which we selected from the feature selection mechanism.","cc62337c":"Also just after fitting the model, we will try to visualize how the accuracy has gone up after 15 epochs, and we will also check for the loss, i.e., how it has gone down.","8501a19e":"So after dummy creation, total number of our features is now 52, earlier before feature selection, it was 95. We can see the names if we want by the below piece of code.","fe2ec390":"Also we will be using \"adam\" optimizer algorithm, and \"binary_crossentropy\" as loss function.","c94540d7":"# Model Performance","27169641":"Now we will do actual feature selection.","fe2756e0":"# Feature Selection\n\n\nFeature selection is important here as still after the removal of \"veil-type\", there are 22 variables, and after creating creating dummies for these categorical variables, the number of variables is bound to increase, so we need to find a way to remove some of the insignificant variables, and we will do it using **Chi-Squared Feature Selection**, but before that we need to convert the all the input variables into ordinal format. But before doing anything else, we will first split the dataset into train and test.","7af2d0ab":"# Model Architecture","2ddbfff7":"Now we will have a look at the dataset.","e573e3a4":"And as can be seen all the variables are categorical in nature.\n\nNow we will check whether our dataset has any missing values or not.","f8a60a21":"Now we will split the data into train and test, and our splitting size will be 80-20. But before that let's encode our target feature, we are not using the above encoded one, because we have use it in the feature selection, so for precaution purpose we will create another variable as y_encoded, and put the encoded target variable there.","4315fd24":"Now let's see other evaluating paramters.","4bb35f03":"Now we will create our NN model, which contains 2 hidden layers with 16 units each, as can be seen in the below code, and the activation function we will be using for the hidden layers are \"relu\", and \"sigmoid\" for the output layer."}}