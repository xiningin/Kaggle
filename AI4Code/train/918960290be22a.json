{"cell_type":{"ae1df7d1":"code","167fb262":"code","d9b64d2e":"code","61e9371e":"code","69ff4f4c":"code","04252e7a":"code","cd767323":"code","6d984a86":"code","b5b9b658":"code","6499f52f":"code","6900e22a":"code","e5b9a41f":"code","9379689f":"code","d39f1e6b":"code","4e03d2d3":"code","9c433e88":"code","c568433d":"code","5ac8a07e":"code","8267bd9f":"code","bbc4e736":"code","ac1ce0bd":"code","f3723c31":"code","4dab05e3":"code","199b6a4f":"code","a01e6d37":"markdown","254d50fc":"markdown","92198a19":"markdown","297f3ce5":"markdown","b060ec08":"markdown","9e26de4a":"markdown","93360794":"markdown","609a2d85":"markdown","deaa08e1":"markdown","caa57203":"markdown","47e5210f":"markdown","46796e41":"markdown"},"source":{"ae1df7d1":"# All libraries we will need\nimport pandas as pd # To store data as a dataframe\nimport requests # to get the data of an url\nfrom bs4 import BeautifulSoup # to parse the html data and find what we want\nimport re # Regular expressions library, it may be useful \nprint('Setup complete!')","167fb262":"# We need the url of the page we are gonna scrape, we will start with the first page\nurl = 'https:\/\/www.kaggle.com\/rankings.json?group=datasets&page=1&pageSize=20'\nresponse = requests.get(url) # Get content of page","d9b64d2e":"import json\n# Get json object, take a look at how data is structured\njson_resp = json.loads(response.text)\n#json_resp # Uncomment to see structure ","61e9371e":"#Counts for tier\njson_resp['counts']","69ff4f4c":"# We need to sum the first 3 tiers, the ones that count for the datasets ranking (grandmaster, master, expert)\nrankingTiers = json_resp['counts'][0:3]\nrankingTiers","04252e7a":"# We need the number of rankers in dataset to know how many pages of 20 (every page has a fixed size of 20 users) we need to scrappe.\nnumberOfRankers = 0\nfor tier in rankingTiers:\n    numberOfRankers += tier['count']\nprint(numberOfRankers)","cd767323":"# Round up using ceil as we need an extra page for the last ones\nimport math\nnumPagsToScrappe = math.ceil(numberOfRankers \/ 20)\nnumPagsToScrappe","6d984a86":"# Get the kagglers users data \nusersList = json_resp['list']\nusersList[0] # Show first user of the list","b5b9b658":"# Data to extract\n\ncurrentRanking = []\ndisplayName = []\nthumbnailUrl = []\nuserId = []\nuserUrl = []\ntier = []\npoints = []\njoined = []\ntotalGoldMedals = []\ntotalSilverMedals = []\ntotalBronzeMedals = []\n\n","6499f52f":"baseURL = 'https:\/\/www.kaggle.com\/rankings.json?group=datasets&pageSize=20&page='\n\nfor page in range(1, numPagsToScrappe + 1): # Page query starts at 1, and its a range()  function so we need to add 1 \n    # We need the url of the page we are gonna scrape\n    pageToScrape = baseURL + str(page) # To acces the multiple pages\n    resp = requests.get(pageToScrape) # Get content of page\n    json_response = json.loads(resp.text) # Get JSON object \n    jsonRespListUsers = json_response['list'] # Get list of users of the JSON object\n    \n    for user in range(0, len(jsonRespListUsers)):\n        currentRanking.append(jsonRespListUsers[user]['currentRanking'])\n        displayName.append(jsonRespListUsers[user]['displayName'])\n        #thumbnailUrl.append(jsonRespListUsers[user]['thumbnailUrl'])\n        userId.append(jsonRespListUsers[user]['userId'])\n        userUrl.append(jsonRespListUsers[user]['userUrl'])\n        tier.append(jsonRespListUsers[user]['tier'])\n        points.append(jsonRespListUsers[user]['points'])\n        joined.append(jsonRespListUsers[user]['joined'])\n        totalGoldMedals.append(jsonRespListUsers[user]['totalGoldMedals'])\n        totalSilverMedals.append(jsonRespListUsers[user]['totalSilverMedals'])\n        totalBronzeMedals.append(jsonRespListUsers[user]['totalBronzeMedals'])\n\n        ","6900e22a":"# Create dataFrame with the information we have\ntopKagglersDatasets = pd.DataFrame({\n    'displayName':displayName,\n    'currentRanking':currentRanking,\n    #'thumbnailUrl':thumbnailUrl,\n    'userId':userId,\n    'userUrl':userUrl,\n    'tier':tier,\n    'points':points,\n    'userJoinDate':joined,\n    'totalGoldMedals':totalGoldMedals,\n    'totalSilverMedals':totalSilverMedals,\n    'totalBronzeMedals':totalBronzeMedals\n\n})","e5b9a41f":"topKagglersDatasets.head(7)","9379689f":"topKagglersDatasets.shape","d39f1e6b":"topKagglersDatasets.to_csv('topKagglersDatasets.csv', index=False)","4e03d2d3":"userPage = 'https:\/\/www.kaggle.com\/cdeotte'\n\nuserResp = requests.get(userPage)\n\njsonStr = userResp.text # Get all text from url\n","9c433e88":"jsonStr = jsonStr.split('{\"userId\"')","c568433d":"jsonStr = '{\"userId\"' + jsonStr[1]","5ac8a07e":"jsonStr = jsonStr.split(');')[0] # Get everything before ); ","8267bd9f":"bioObj = json.loads(jsonStr) # Create a JSON object containing all info","bbc4e736":"#bioObj # Uncomment to check all info about about user","ac1ce0bd":"bioObj['datasetsSummary'] # We could use all this information to complement the dataset\n# This data contains a summary of datasets category and highlights of the top 3 most popular datasets (by number of votes)","f3723c31":"bioObj['datasetsSummary']['highlights'][0]","4dab05e3":"# Data to extract in profile\n\ncountry = []\nregion = []\ncity = []\ngitHubUserName = []\ntwitterUserName = []\nlinkedInUrl = []\nwebsiteUrl = []\noccupation = []\norganization = []\n\n# Dataset summary info\n\ntotalResults = []\nrankPercentage = []\nrankCurrent = []\nrankHighest = []\n\n# Dataset summary -> highlights \n\ntop1title = []\ntop1date = []\ntop1medal = []\ntop1score = []\ntop1url = []\n\ntop2title = []\ntop2date = []\ntop2medal = []\ntop2score = []\ntop2url = []\n\ntop3title = []\ntop3date = []\ntop3medal = []\ntop3score = []\ntop3url = []\n","199b6a4f":"\"\"\"\nimport time\n\nbaseUrl2 = 'https:\/\/www.kaggle.com'\nfor row in topKagglersDatasets['userUrl']:\n    time.sleep(0.5) # You have to wait between request, try not to saturate server sending to much requests. You have to be polite in data mining.\n    # Get profile URL\n    profileUrl = baseUrl2 + row\n\n    # Get content of profile URL\n    userResp = requests.get(profileUrl)\n    jsonStr2 = userResp.text # Get all text from url\n    \n    # Split text content to extract JSON content\n    jsonStr2 = jsonStr2.split('{\"userId\"')\n    jsonStr2 = '{\"userId\"' + jsonStr2[1]\n    jsonStr2 = jsonStr2.split(');')[0] # Get everything before ); \n    \n    # Create JSON object for easier manipulation\n    bioObj = json.loads(jsonStr2) # Create a JSON object containing all data\n    \n    \n    country.append(bioObj['country'])\n    region.append(bioObj['region'])\n    city.append(bioObj['city'])\n    gitHubUserName.append(bioObj['gitHubUserName'])\n    twitterUserName.append(bioObj['twitterUserName'])\n    linkedInUrl.append(bioObj['linkedInUrl'])\n    websiteUrl.append(bioObj['websiteUrl'])\n    occupation.append(bioObj['occupation'])\n    organization.append(bioObj['organization'])\n\n    \n    totalResults.append(bioObj['datasetsSummary']['totalResults'])\n    rankPercentage.append(bioObj['datasetsSummary']['rankPercentage'])\n    rankCurrent.append(bioObj['datasetsSummary']['rankCurrent'])\n    rankHighest.append(bioObj['datasetsSummary']['rankHighest'])\n    \n    top1title.append(bioObj['datasetsSummary']['highlights'][0]['title'])\n    top1date.append(bioObj['datasetsSummary']['highlights'][0]['date'])\n    top1medal.append(bioObj['datasetsSummary']['highlights'][0]['medal'])\n    top1score.append(bioObj['datasetsSummary']['highlights'][0]['score'])\n    top1url.append(bioObj['datasetsSummary']['highlights'][0]['url'])\n\n    top2title.append(bioObj['datasetsSummary']['highlights'][1]['title'])\n    top2date.append(bioObj['datasetsSummary']['highlights'][1]['date'])\n    top2medal.append(bioObj['datasetsSummary']['highlights'][1]['medal'])\n    top2score.append(bioObj['datasetsSummary']['highlights'][1]['score'])\n    top2url.append(bioObj['datasetsSummary']['highlights'][1]['url'])\n\n    top3title.append(bioObj['datasetsSummary']['highlights'][2]['title'])\n    top3date.append(bioObj['datasetsSummary']['highlights'][2]['date'])\n    top3medal.append(bioObj['datasetsSummary']['highlights'][2]['medal'])\n    top3score.append(bioObj['datasetsSummary']['highlights'][2]['score'])\n    top3url.append(bioObj['datasetsSummary']['highlights'][2]['url'])\n\"\"\"","a01e6d37":"### The resulting dataset can be found: https:\/\/www.kaggle.com\/ajpass\/top-datasets-kagglers-ranking?select=topKagglersDatasets.csv","254d50fc":"There are too many requests, server doesnt like that, but here can you find how all data is collected:","92198a19":"We can see that the request URL is : https:\/\/www.kaggle.com\/rankings?group=datasets&page=1&pageSize=20\nAnd if you scroll down you can see the query string parameters, which are the ones after '?', in this case:\n\n1. group=datasets\n2. page=1\n3. pageSize=20\n\nYou can alter the query order freely, it doesn't change the output, the next querys means exactly the same:\n\n* https:\/\/www.kaggle.com\/rankings?group=datasets&page=1&pageSize=20\n* https:\/\/www.kaggle.com\/rankings?group=datasets&pageSize=20&page=1\n* https:\/\/www.kaggle.com\/rankings?page=1&pageSize=20&group=datasets\n\n","297f3ce5":"Data Mining - Web Scrapping: The saga:\n1. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapper-vol-1-pokedex # Scrapping a pokedex, all pokemon with all stats\n2. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-2-pokedex-pandas # Scrapping a pokedex, using a diferent method than number 1, easier but in case a column has multiple values, in some cases you may need to do some cleaning. Result similar as 1, only changes: Type and ID. \n3. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapper-vol-3-sudoku-to-string # Extract sudokus data and transform it to string\n4. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-66-kaggle-datasets # It's about scrapping the top 20 kagglers in datasets ranking\n5. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-4-kaggle-datasets2 # Second take at kaggle datasets scrapping ","b060ec08":"We will click at the first one and go to preview. We will see the JSON data nice formated, we can inspect it. \n![networkTab4.png](attachment:networkTab4.png)","9e26de4a":"In the response and preview we can see the output we will receive with this querys, but they're not what we are after, we need the json data. \nTo extract the data that we need we will click at XHR.\n\nAs you scroll more you will see that new querys containing json will appear. (Instead of rankings? we will use rankings.json?)\n![networkTab3.png](attachment:networkTab3.png)\n\nThe query only changes the page number, we will keep this in mind for scrapping this website.","93360794":"As we scroll in the dataset ranking we can see that every time we scroll 20 kagglers, it updates the data, this means we are facing an infinite scrolling system. \nTo approach an infinite scrolling system we need to know how the data is loaded. In the last tutorial we found that the data is json format. ","609a2d85":"First we will find the way to extract all information. We need to split the text to create a JSON object with the data.","deaa08e1":"Take number 2 at Kaggle Datasets web scrapping. \n\nThe last scrapping was sloppy (somewhat dirty) and I went for the easiest thing to scrappe, the 20 first top kagglers. The problems that I faced in the last scrapping were:\n* How to know the type of data I needed to extract as it was json data and I expected a simple HTML structure.\n* How to extract and handle that data. \n\nWhat I couldn't resolve was how to get more than a list of the top 20 kagglers.\nWe will take a look at the process to approach this scrapping. \n","caa57203":"Right click, inspect element, then we go to the network tab, and click F5 to refresh page, you will see that now we get all the network data.\n\n\n![networkTab.png](attachment:networkTab.png)","47e5210f":"### We can go an extra step to take all biography data.\nI've been looking for a better way to access biography data, but haven't found it, so we will do it the same way as the last tutorial, but with a good JSON format.\n","46796e41":"After refreshing, we will scroll up in the network tab and click in the first one (the most important), a new sub-tab will appear (Headers by default selected).\n\n![networkTab2.png](attachment:networkTab2.png)"}}