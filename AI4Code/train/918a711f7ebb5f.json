{"cell_type":{"f384e636":"code","7ee7c953":"code","1ec83554":"code","15172e18":"code","8c00c570":"code","489314f4":"code","a292c9e3":"code","49df4448":"code","53c48286":"code","be25c3b4":"code","70d98b17":"code","1657c601":"code","1568ef72":"code","3761be2a":"code","96baecf5":"code","c283b27a":"code","c1bf6d52":"code","188f411a":"code","259bfc0f":"code","35851ce8":"code","c5b19842":"code","306e4307":"code","b25eadc1":"code","b25d5059":"code","6abf86cf":"code","9dff4002":"code","338937b0":"code","6a43121e":"code","c108fbe9":"code","54020722":"code","ba7ef729":"code","212f45fc":"code","a3e03f16":"code","d8dc89b9":"code","c3bba159":"code","20f2e5af":"code","8e4513d8":"code","0d306ceb":"code","776bc230":"code","96285ab8":"code","c2b8e4a2":"code","9fcace3a":"code","bcd77421":"code","4200550d":"code","716ed75a":"code","65df7c53":"markdown","cb5643fb":"markdown","6d486a05":"markdown","76a8ca3e":"markdown","1e4fe9ad":"markdown","a2d2ffd7":"markdown","5a8fd068":"markdown","8a95383a":"markdown","274ea8f7":"markdown","77305f29":"markdown","ad2010e3":"markdown","f196acdf":"markdown","e71c947b":"markdown","aabc6df0":"markdown","b590a2b5":"markdown","eebd6a42":"markdown","107b75d1":"markdown","1c72941d":"markdown","12edc45b":"markdown","ab87edd2":"markdown","5487c19d":"markdown","af77bced":"markdown"},"source":{"f384e636":"#Importing Libraries\nimport pandas as pd\n\n# For Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To Scale our data\nfrom sklearn.preprocessing import scale\n\n# To perform KMeans clustering \nfrom sklearn.cluster import KMeans\n\n# To perform Hierarchical clustering\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","7ee7c953":"help(KMeans)","1ec83554":"#reading Dataset\nretail = pd.read_csv(\"..\/input\/Online Retail.csv\",  sep = ',',encoding = \"ISO-8859-1\", header= 0)\n\n# parse date\nretail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'], format = \"%d-%m-%Y %H:%M\")","15172e18":"# Let's look top 5 rows\nretail.head()","8c00c570":"#Sanity Check\nretail.shape\nretail.describe()\nretail.info()","489314f4":"#Na Handling\nretail.isnull().values.any()\nretail.isnull().values.sum()\nretail.isnull().sum()*100\/retail.shape[0]","a292c9e3":"#dropping the na cells\norder_wise = retail.dropna()","49df4448":"#Sanity check\norder_wise.shape\norder_wise.isnull().sum()","53c48286":"#RFM implementation\n\n# Extracting amount by multiplying quantity and unit price and saving the data into amount variable.\namount  = pd.DataFrame(order_wise.Quantity * order_wise.UnitPrice, columns = [\"Amount\"])\namount.head()","be25c3b4":"#merging amount in order_wise\norder_wise = pd.concat(objs = [order_wise, amount], axis = 1, ignore_index = False)\n\n#Monetary Function\n# Finding total amount spent per customer\nmonetary = order_wise.groupby(\"CustomerID\").Amount.sum()\nmonetary = monetary.reset_index()\nmonetary.head()","70d98b17":"#monetary.drop(['level_1'], axis = 1, inplace = True)\n#monetary.head()","1657c601":"#Frequency function\nfrequency = order_wise[['CustomerID', 'InvoiceNo']]","1568ef72":"# Getting the count of orders made by each customer based on customer ID.\nk = frequency.groupby(\"CustomerID\").InvoiceNo.count()\nk = pd.DataFrame(k)\nk = k.reset_index()\nk.columns = [\"CustomerID\", \"Frequency\"]\nk.head()","3761be2a":"#creating master dataset\nmaster = monetary.merge(k, on = \"CustomerID\", how = \"inner\")\nmaster.head()","96baecf5":"recency  = order_wise[['CustomerID','InvoiceDate']]\nmaximum = max(recency.InvoiceDate)","c283b27a":"#Generating recency function\n\n# Filtering data for customerid and invoice_date\nrecency  = order_wise[['CustomerID','InvoiceDate']]\n\n# Finding max data\nmaximum = max(recency.InvoiceDate)\n\n# Adding one more day to the max data, so that the max date will have 1 as the difference and not zero.\nmaximum = maximum + pd.DateOffset(days=1)\nrecency['diff'] = maximum - recency.InvoiceDate\nrecency.head()","c1bf6d52":"# recency by customerid\na = recency.groupby('CustomerID')","188f411a":"a.diff.min()","259bfc0f":"#Dataframe merging by recency\ndf = pd.DataFrame(recency.groupby('CustomerID').diff.min())\ndf = df.reset_index()\ndf.columns = [\"CustomerID\", \"Recency\"]\ndf.head()","35851ce8":"#Combining all recency, frequency and monetary parameters\nRFM = k.merge(monetary, on = \"CustomerID\")\nRFM = RFM.merge(df, on = \"CustomerID\")\nRFM.head()","c5b19842":"# outlier treatment for Amount\nplt.boxplot(RFM.Amount)\nQ1 = RFM.Amount.quantile(0.25)\nQ3 = RFM.Amount.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Amount >= Q1 - 1.5*IQR) & (RFM.Amount <= Q3 + 1.5*IQR)]","306e4307":"# outlier treatment for Frequency\nplt.boxplot(RFM.Frequency)\nQ1 = RFM.Frequency.quantile(0.25)\nQ3 = RFM.Frequency.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Frequency >= Q1 - 1.5*IQR) & (RFM.Frequency <= Q3 + 1.5*IQR)]","b25eadc1":"# outlier treatment for Recency\nplt.boxplot(RFM.Recency)\nQ1 = RFM.Recency.quantile(0.25)\nQ3 = RFM.Recency.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Recency >= Q1 - 1.5*IQR) & (RFM.Recency <= Q3 + 1.5*IQR)]","b25d5059":"RFM.head(20)","6abf86cf":"# standardise all parameters\nRFM_norm1 = RFM.drop(\"CustomerID\", axis=1)\nRFM_norm1.Recency = RFM_norm1.Recency.dt.days\n\nfrom sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\nRFM_norm1 = standard_scaler.fit_transform(RFM_norm1)","9dff4002":"RFM_norm1 = pd.DataFrame(RFM_norm1)\nRFM_norm1.columns = ['Frequency','Amount','Recency']\nRFM_norm1.head()","338937b0":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","6a43121e":"hopkins(RFM_norm1)","c108fbe9":"# Kmeans with K=5\nmodel_clus5 = KMeans(n_clusters = 5, max_iter=50)\nmodel_clus5.fit(RFM_norm1)","54020722":"from sklearn.metrics import silhouette_score\nsse_ = []\nfor k in range(2, 15):\n    kmeans = KMeans(n_clusters=k).fit(RFM_norm1)\n    sse_.append([k, silhouette_score(RFM_norm1, kmeans.labels_)])","ba7ef729":"plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1]);","212f45fc":"# sum of squared distances\nssd = []\nfor num_clusters in list(range(1,21)):\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=50)\n    model_clus.fit(RFM_norm1)\n    ssd.append(model_clus.inertia_)\n\nplt.plot(ssd)","a3e03f16":"# analysis of clusters formed\nRFM.index = pd.RangeIndex(len(RFM.index))\nRFM_km = pd.concat([RFM, pd.Series(model_clus5.labels_)], axis=1)\nRFM_km.columns = ['CustomerID', 'Frequency', 'Amount', 'Recency', 'ClusterID']\n\nRFM_km.Recency = RFM_km.Recency.dt.days\nkm_clusters_amount = \tpd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Amount.mean())\nkm_clusters_frequency = \tpd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Frequency.mean())\nkm_clusters_recency = \tpd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Recency.mean())","d8dc89b9":"df = pd.concat([pd.Series([0,1,2,3,4]), km_clusters_amount, km_clusters_frequency, km_clusters_recency], axis=1)\ndf.columns = [\"ClusterID\", \"Amount_mean\", \"Frequency_mean\", \"Recency_mean\"]\ndf.head()","c3bba159":"sns.barplot(x=df.ClusterID, y=df.Amount_mean)\n","20f2e5af":"sns.barplot(x=df.ClusterID, y=df.Frequency_mean)","8e4513d8":"sns.barplot(x=df.ClusterID, y=df.Recency_mean)","0d306ceb":"# heirarchical clustering\nmergings = linkage(RFM_norm1, method = \"single\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","776bc230":"mergings = linkage(RFM_norm1, method = \"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","96285ab8":"clusterCut = pd.Series(cut_tree(mergings, n_clusters = 5).reshape(-1,))\nRFM_hc = pd.concat([RFM, clusterCut], axis=1)\nRFM_hc.columns = ['CustomerID', 'Frequency', 'Amount', 'Recency', 'ClusterID']","c2b8e4a2":"#summarise\nRFM_hc.Recency = RFM_hc.Recency.dt.days\nkm_clusters_amount = \tpd.DataFrame(RFM_hc.groupby([\"ClusterID\"]).Amount.mean())\nkm_clusters_frequency = \tpd.DataFrame(RFM_hc.groupby([\"ClusterID\"]).Frequency.mean())\nkm_clusters_recency = \tpd.DataFrame(RFM_hc.groupby([\"ClusterID\"]).Recency.mean())","9fcace3a":"df = pd.concat([pd.Series([0,1,2,3,4]), km_clusters_amount, km_clusters_frequency, km_clusters_recency], axis=1)\ndf.columns = [\"ClusterID\", \"Amount_mean\", \"Frequency_mean\", \"Recency_mean\"]\ndf.head()","bcd77421":"#plotting barplot\nsns.barplot(x=df.ClusterID, y=df.Amount_mean)","4200550d":"sns.barplot(x=df.ClusterID, y=df.Frequency_mean)","716ed75a":"sns.barplot(x=df.ClusterID, y=df.Recency_mean)","65df7c53":"## Silhouette Analysis\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","cb5643fb":"### Outlier Treatment","6d486a05":"#### Frequency Value","76a8ca3e":"<hr>","1e4fe9ad":"### Data quality check and cleaning","a2d2ffd7":"# K-Mean Clustering","5a8fd068":"### RFM combined DataFrame","8a95383a":"## Heirarchical Clustering","274ea8f7":"We will be using the online reatil trasnational dataset to build a RFM clustering and choose the best set of customers.","77305f29":"### Let's look at KMeans package help to better understand the KMeans implementation in Python using SKLearn","ad2010e3":"##### Merging Amount and Frequency columns","f196acdf":"### Reading the Data Set","e71c947b":"#### If in the above result you get a column with name level_1, uncomment the below code and run it, else ignore it and keeping moving.","aabc6df0":"#### Monetary Value","b590a2b5":"### Extracting R(Recency), F(Frequency), M(Monetary) columns form the data that we imported in.","eebd6a42":"## Sum of Squared Distances","107b75d1":"### Recency Value","1c72941d":"## K-Means with some K","12edc45b":"**Overview**<br>\nOnline retail is a transnational data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.","ab87edd2":"## Hopkins Statistics:\nThe Hopkins statistic, is a statistic which gives a value which indicates the cluster tendency, in other words: how well the data can be clustered.\n\n- If the value is between {0.01, ...,0.3}, the data is regularly spaced.\n\n- If the value is around 0.5, it is random.\n\n- If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster.","5487c19d":"Some usefull links to understand Hopkins Statistics:\n- [WikiPedia](https:\/\/en.wikipedia.org\/wiki\/Hopkins_statistic)\n- [Article](http:\/\/www.sthda.com\/english\/articles\/29-cluster-validation-essentials\/95-assessing-clustering-tendency-essentials\/)","af77bced":"### Scaling the RFM data"}}