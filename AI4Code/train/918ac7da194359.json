{"cell_type":{"24a6fdc6":"code","dfc3526c":"code","0b03708c":"code","5fb602aa":"code","1ea454df":"code","0e914199":"code","e750f8d8":"code","6461ebd3":"markdown","3aaa00ca":"markdown","773deffe":"markdown","b1c2a267":"markdown","b4429fac":"markdown","5d19202c":"markdown","749b8ce6":"markdown"},"source":{"24a6fdc6":"import os\nimport matplotlib.pyplot as plt\nimport glob\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\nfrom keras.models import Model\nfrom keras.objectives import mean_squared_error\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\nfrom keras.initializers import random_normal\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.callbacks import Callback\n\nfrom sklearn.metrics import cohen_kappa_score, f1_score\nfrom sklearn.model_selection import KFold, train_test_split","dfc3526c":"df_train = pd.read_csv(\"..\/input\/liverpool-ion-switching\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/liverpool-ion-switching\/test.csv\")\n\n# I don't use \"time\" feature\ntrain_input = df_train[\"signal\"].values.reshape(-1,4000,1)#number_of_data:1250 x time_step:4000\ntrain_input_mean = train_input.mean()\ntrain_input_sigma = train_input.std()\ntrain_input = (train_input-train_input_mean)\/train_input_sigma\ntest_input = df_test[\"signal\"].values.reshape(-1,10000,1)#\ntest_input = (test_input-train_input_mean)\/train_input_sigma\n\n#train_target = df_train[\"open_channels\"].values.reshape(-1,4000,1)#regression\ntrain_target = pd.get_dummies(df_train[\"open_channels\"]).values.reshape(-1,4000,11)#classification\n\nidx = np.arange(train_input.shape[0])\ntrain_idx, val_idx = train_test_split(idx, random_state = 111,test_size = 0.2)\n\nval_input = train_input[val_idx]\ntrain_input = train_input[train_idx] \nval_target = train_target[val_idx]\ntrain_target = train_target[train_idx] \n\nprint(\"train_input:{}, val_input:{}, train_target:{}, val_target:{}\".format(train_input.shape, val_input.shape, train_target.shape, val_target.shape))","0b03708c":"def cbr(x, out_layer, kernel, stride, dilation):\n    x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef se_block(x_in, layer_n):\n    x = GlobalAveragePooling1D()(x_in)\n    x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n    x = Dense(layer_n, activation=\"sigmoid\")(x)\n    x_out=Multiply()([x_in, x])\n    return x_out\n\ndef resblock(x_in, layer_n, kernel, dilation, use_se=True):\n    x = cbr(x_in, layer_n, kernel, 1, dilation)\n    x = cbr(x, layer_n, kernel, 1, dilation)\n    if use_se:\n        x = se_block(x, layer_n)\n    x = Add()([x_in, x])\n    return x  \n\ndef UUnet(input_shape=(None,1)):\n    layer_n = 64\n    kernel_size = 7\n    depth = 2\n\n    input_layer = Input(input_shape)    \n    input_layer_1 = AveragePooling1D(5)(input_layer)\n    input_layer_2 = AveragePooling1D(25)(input_layer)\n    \n    ##########################\n    ### First U-Net: Big-U ###\n    ##########################\n    \n    ########## Encoder 1\n    x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n    for i in range(depth):\n        x = resblock(x, layer_n, kernel_size, 1)\n    out_0 = x\n\n    x = cbr(x, layer_n*2, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*2, kernel_size, 1)\n    out_1 = x\n\n    x = Concatenate()([x, input_layer_1])    \n    x = cbr(x, layer_n*3, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*3, kernel_size, 1)\n    out_2 = x\n\n    x = Concatenate()([x, input_layer_2])    \n    x = cbr(x, layer_n*4, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*4, kernel_size, 1)\n    \n    ########### Decoder 1\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_2])\n    x = cbr(x, layer_n*3, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_1])\n    x = cbr(x, layer_n*2, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_0])\n    x = cbr(x, layer_n, kernel_size, 1, 1)   \n    \n    #############################\n    ### Second U-Net: Small-U ###\n    #############################\n    \n    ########## Encoder 2\n    #x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n    for i in range(depth):\n        x = resblock(x, layer_n, kernel_size, 1)\n    out_0 = x\n\n    x = cbr(x, layer_n*2, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*2, kernel_size, 1)\n    out_1 = x\n\n    x = Concatenate()([x, input_layer_1])    \n    x = cbr(x, layer_n*3, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*3, kernel_size, 1)\n    \n    ########### Decoder 2\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_1])\n    x = cbr(x, layer_n*2, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_0])\n    x = cbr(x, layer_n, kernel_size, 1, 1) \n    \n    ###############################\n    ### Third U-Net: Smallest-U ###\n    ###############################\n    \n    ########## Encoder 2\n    #x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n    for i in range(depth):\n        x = resblock(x, layer_n, kernel_size, 1)\n    out_0 = x\n    \n    x = cbr(x, layer_n*2, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*2, kernel_size, 1)\n    \n    ########### Decoder 2\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_0])\n    x = cbr(x, layer_n, kernel_size, 1, 1)\n    \n    #classifier\n    x = Conv1D(11, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n    out = Activation(\"softmax\")(x)\n    \n    model = Model(input_layer, out)\n    \n    return model\n\n\ndef augmentations(input_data, target_data):\n    #flip\n    if np.random.rand()<0.5:    \n        input_data = input_data[::-1]\n        target_data = target_data[::-1]\n\n    return input_data, target_data\n\n\ndef Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n    x=[]\n    y=[]\n  \n    count=0\n    idx_1 = np.arange(len(input_dataset))\n    #idx_2 = np.arange(len(input_dataset))\n    np.random.shuffle(idx_1)\n    #np.random.shuffle(idx_2)\n\n    while True:\n        for i in range(len(input_dataset)):\n            input_data = input_dataset[idx_1[i]]\n            target_data = target_dataset[idx_1[i]]\n            #input_data_mix = input_dataset[idx_2[i]]\n            #target_data_mix = target_dataset[idx_2[i]]\n\n            if is_train:\n                input_data, target_data = augmentations(input_data, target_data)\n                #input_data_mix, target_data_mix = augmentations(input_data_mix, target_data_mix)\n                \n            x.append(input_data)\n            y.append(target_data)\n            count+=1\n            if count==batch_size:\n                x=np.array(x, dtype=np.float32)\n                y=np.array(y, dtype=np.float32)\n                inputs = x\n                targets = y       \n                x = []\n                y = []\n                count=0\n                yield inputs, targets\n\nclass macroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis=2).reshape(-1)\n\n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n        f1_val = f1_score(self.targets, pred, average=\"macro\")\n        print(\"val_f1_macro_score: \", f1_val)\n                \ndef model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n    hist = model.fit_generator(\n        Datagen(train_inputs, train_targets, batch_size, is_train=True),\n        steps_per_epoch = len(train_inputs) \/\/ batch_size,\n        epochs = n_epoch,\n        validation_data=Datagen(val_inputs, val_targets, batch_size),\n        validation_steps = len(val_inputs) \/\/ batch_size,\n        callbacks = [lr_schedule, macroF1(model, val_inputs, val_targets)],\n        shuffle = False,\n        verbose = 1\n        )\n    return hist\n\n\ndef lrs(epoch):\n    if epoch < 50:\n        lr = learning_rate\n    elif epoch < 100:\n        lr = learning_rate\/10\n    elif epoch < 200:\n        lr = learning_rate\/100\n    elif epoch < 300:\n        lr = learning_rate\/1000\n    else:\n        lr = learning_rate\/10000\n    return lr\n","5fb602aa":"K.clear_session()\nmodel = UUnet()\nprint(model.summary())","1ea454df":"learning_rate=0.0008 # originally 0.0005\nn_epoch=350\nbatch_size=32\n\nlr_schedule = LearningRateScheduler(lrs)\n\n#classifier\nmodel.compile(loss=categorical_crossentropy, \n              optimizer=Adam(lr=learning_rate), \n              metrics=[\"accuracy\"])\n\nhist = model_fit(model, train_input, train_target, val_input, val_target, n_epoch, batch_size)","0e914199":"pred = np.argmax((model.predict(val_input)+model.predict(val_input[:,::-1,:])[:,::-1,:])\/2, axis=2).reshape(-1)\ngt = np.argmax(val_target, axis=2).reshape(-1)\nprint(\"SCORE_oldmetric: \", cohen_kappa_score(gt, pred, weights=\"quadratic\"))\nprint(\"SCORE_newmetric: \", f1_score(gt, pred, average=\"macro\"))","e750f8d8":"pred = np.argmax((model.predict(test_input)+model.predict(test_input[:,::-1,:])[:,::-1,:])\/2, axis=2).reshape(-1)\n\ndf_sub = pd.read_csv(\"..\/input\/liverpool-ion-switching\/sample_submission.csv\", dtype={'time':str})\ndf_sub.open_channels = np.array(np.round(pred,0), np.int)\ndf_sub.to_csv(\"submission.csv\",index=False)","6461ebd3":"## Define Model\nThis section defines U-Net(se-resnet base).\nInput and output of the U-Net are follows:\n* Input: 4000 time steps of \"signal\"\n* Output: 4000 time steps of 11 class \"open_channels\"\n\nI've changed the model from regressor to classifier at version 3. So the output has the multi class.","3aaa00ca":"## Import Library","773deffe":"## Training","b1c2a267":"## Predict and Submit\n","b4429fac":"## Introduction\nIn this kernel, I'd like to share the approach using 1D Convolutional Neural Networks(1D CNN). 1D CNN is sometimes effective to analyze time series data. This kernel introduces U-Net architecture with 1D CNN.","5d19202c":"## Load and Split Dataset\nSimply split the input data into certain length.","749b8ce6":"# UUU-Net: An Ensemble of U-Nets with Diminishing Depths\n\n- Built-upon: https:\/\/www.kaggle.com\/kmat2019\/u-net-1d-cnn-with-keras\n- Actually like Ladder-net (https:\/\/arxiv.org\/abs\/1810.07810) and S-unet (https:\/\/ieeexplore.ieee.org\/abstract\/document\/8842560)\n- A similar implementation with bi-directional LSTMs in BUSU-Net, and my paper can be found at: https:\/\/arxiv.org\/abs\/2003.01581\n- Modified to a chain of U-Nets with diminishing depths\n- UU-Net results:\n    - With 60 epochs, public score is 0.922 \n    - With 350 & 500 epochs, public score is 0.932\n- Latest version: \n    - changed lr scheduler\n    - added small (third) U-Net block"}}