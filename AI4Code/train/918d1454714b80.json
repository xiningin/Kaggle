{"cell_type":{"56510f23":"code","d01388c7":"code","f597db91":"code","fa505254":"code","6a87c8a1":"code","ffa65124":"code","ae2c3d14":"code","1439b5c3":"code","21893577":"markdown","7a0323dd":"markdown","3bc466f8":"markdown","72c21d81":"markdown","552f03e3":"markdown","0b2b5c4e":"markdown","af40fcd7":"markdown"},"source":{"56510f23":"import os\nimport random\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tqdm.notebook import trange\n\nrandom.seed(42)\nnp.random.seed(42)\ncv = StratifiedKFold(5, shuffle=True)\n\ndroot = os.environ.get('KAGGLE_DIR', '..\/input')\nold_y = pd.read_csv(\n    f'{droot}\/november21\/train.csv', usecols=['target']\n).target.astype(np.float32).to_numpy()\ntrain = pd.read_csv(\n    f'{droot}\/tabular-playground-series-nov-2021\/train.csv', dtype=np.float32\n).drop(columns=['id'])\nX, new_y = StandardScaler().fit_transform(train.drop(columns=['target'])), train.target","d01388c7":"clf = LogisticRegression()\n\ndef clf_pred(train_idx, val_idx):\n    clf.fit(X[train_idx], new_y[train_idx])\n    return clf.decision_function(X[val_idx])\n\ndef old_pred(train_idx, val_idx):\n    return old_y[val_idx]\n\ndef auc_cvs(stratify=new_y, predict=old_pred):\n    return np.array([\n        roc_auc_score(new_y[val_idx], predict(train_idx, val_idx)) \n        for train_idx, val_idx in cv.split(X, stratify)\n    ])\n\nscores = auc_cvs()\nscores","f597db91":"diff = scores.max() - scores.min()\nreal_score = roc_auc_score(new_y, old_y)\nprint(f'max() - min() = {diff:.4f}, std() = {scores.std():.4f}, real auc = {real_score:.4f}')","fa505254":"scores = auc_cvs(new_y != old_y)\nscores","6a87c8a1":"diff = scores.max() - scores.min()\nreal_score = roc_auc_score(new_y, old_y)\nprint(f'max() - min() = {diff:.4f}, std() = {scores.std():.4f}, real auc = {real_score:.4f}')","ffa65124":"spread_stratify_label = np.concatenate([auc_cvs(predict=clf_pred) for _ in trange(10)])\nspread_stratify_flipped = np.concatenate([auc_cvs(new_y != old_y, predict=clf_pred) for _ in trange(10)])\n\nff.create_distplot(\n    [spread_stratify_label, spread_stratify_flipped],\n    ['stratify=target', 'stratify=target != old_target'],\n    bin_size=.00025, show_curve=False\n)","ae2c3d14":"spread_stratify_flipped.std(), spread_stratify_label.std()","1439b5c3":"ff.create_distplot(\n    [spread_stratify_label.reshape(-1, 5).mean(axis=1), spread_stratify_flipped.reshape(-1, 5).mean(axis=1)],\n    ['stratify=target', 'stratify=target != old_target'],\n    bin_size=.00025, show_curve=False\n)","21893577":"Achieve stable CV scores\n==\n\nIn which we [use the old labels](https:\/\/www.kaggle.com\/criskiev\/november21) to get very stable CV scores. What I mean by stable CV score, is that all the splits achieve roughly the same score -- as opposed to having 1 out of 5 splits score, say, `.753`, but the rest score, say `.748`. \n\nWe'll do this experiment in a very simple way -- we'll just use the pre-flip labels as our predictions. We already know that this should score around `.7486`, so let's check what our CVs are for that:","7a0323dd":"The example here is typical for what we want to avoid, there's a huge difference between the best and worst split, and none of them are even very close to the real score of this prediction:","3bc466f8":"That's a fairly big change. But does this also affect real models? We've not been doing any training. Well, I think this should also affect real models, so let's try running a few tests:","72c21d81":"It might look like stratifying by `target` creates a better-performing classifier. Is that the case, though? Let's plot the spread of the mean CV scores:","552f03e3":"One of these cases is clearly less spread out, as we could also see by comparing std:","0b2b5c4e":"Let's repeat this, but stratify by whether the label was flipped or not (`old != new`):","af40fcd7":"Well, no. It looks like we receive the same OOF predictions from these, the differences are so small that it could easily just be some kind of accumulated rounding error."}}