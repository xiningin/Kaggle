{"cell_type":{"0dc351f5":"code","b5ec2996":"code","83d85ea0":"code","adec5d62":"code","bf9774b4":"code","e1f81d9c":"code","902c0329":"code","d7a045e7":"code","0bd59d5f":"code","ac48d986":"code","21015bb5":"code","1051f227":"code","6c42ca53":"code","7bf4406e":"code","85106e6a":"code","944c8a36":"code","bc2b8cd4":"code","5b54ae1f":"code","588bdfe4":"code","a877467f":"code","a225f15b":"code","89317b02":"code","f10d09f9":"code","22fa6889":"code","4979db49":"code","eccf1032":"code","880f0d72":"code","1f0cc8b5":"code","9f83dc75":"code","4dc1e34f":"code","364a3e0d":"code","c0b56f4d":"code","d03438f4":"code","e5a21622":"code","d0c15eee":"code","760fa831":"code","2c04f10b":"code","be906dbe":"code","d00826e2":"code","fd2559d1":"code","417e3797":"code","d1e34bde":"code","332cad3e":"code","4bc5e5c0":"code","0a3194f6":"code","6cde63a5":"code","9998ebcb":"code","e538a833":"code","09d46baf":"code","d3f3b091":"code","696dda2d":"code","224760ee":"code","5b9c7dcf":"code","770a5ad9":"code","8368fc4c":"code","96844481":"code","43149465":"code","768c655c":"code","512106e8":"code","e793e1c0":"code","54d626a3":"code","fbba2540":"code","0635964f":"code","5b6ea077":"code","ccf06928":"code","8fecbb62":"code","63c4e300":"markdown","5db3562a":"markdown","fe825280":"markdown","fe37f247":"markdown","8eb24d95":"markdown","11a3f4ae":"markdown","51210fb6":"markdown","63c0a0c5":"markdown"},"source":{"0dc351f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5ec2996":"cd \/kaggle\/input\/adult-income-dataset\/","83d85ea0":"import itertools\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.preprocessing import OneHotEncoder\nfrom pandas import get_dummies\nimport matplotlib as mpl\nfrom sklearn import neighbors\nfrom sklearn import mixture\nfrom matplotlib import pyplot as plt\nfrom sklearn.mixture import GaussianMixture\nfrom scipy.stats import multivariate_normal\nfrom sklearn.decomposition import LatentDirichletAllocation as lda\n#Almost none of these are useful","adec5d62":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pymc3 as pm\n#This is the most important module\nimport arviz as az\nimport matplotlib.lines as mlines\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import OrderedDict\nimport theano\nimport theano.tensor as tt\nimport itertools\nfrom IPython.core.pylabtools import figsize\npd.set_option('display.max_columns', 30)\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix","bf9774b4":"from pandas import get_dummies","e1f81d9c":"\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pymc3 as pm\nimport random\nimport matplotlib\nimport plotnine\nfrom plotnine import ggplot, aes, geom_point, geom_jitter, geom_smooth, geom_histogram, geom_line, geom_errorbar, stat_smooth, geom_ribbon\nfrom plotnine.facets import facet_wrap\nimport seaborn as sns\nimport theano.tensor as tt\nfrom scipy.special import expit\nfrom scipy.special import logit\nfrom scipy.stats import cauchy\nfrom sklearn.metrics import roc_auc_score\n##from skmisc.loess import loess\nimport warnings\nwarnings.filterwarnings(\"ignore\") # plotnine is causing all kinds of matplotlib warnings","902c0329":"from collections import OrderedDict\nfrom time import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport seaborn\nimport theano as thno\nimport theano.tensor as T\n\nfrom scipy import integrate\nfrom scipy.optimize import fmin_powell\n\nprint(f\"Running on PyMC3 v{pm.__version__}\")","d7a045e7":"#The only relevant biases are heuristics I can think of are - the availability effect - that people only see very \n#successful people from certain professions of TV; and so, assume that those professions are very high paying,when they\n#may, in fact, be very (very) low paying except for the tail of the distribution\n\n\n%matplotlib inline","0bd59d5f":"%config InlineBackend.figure_format = 'retina'\nRANDOM_SEED = 8927\nnp.random.seed(RANDOM_SEED)","ac48d986":"def run_models(df, upper_order=5):\n    \"\"\"\n    Convenience function:\n    Fit a range of pymc3 models of increasing polynomial complexity.\n    Suggest limit to max order 5 since calculation time is exponential.\n    \"\"\"\n\n    models, traces = OrderedDict(), OrderedDict()\n\n    for k in range(1, upper_order + 1):\n\n        nm = f\"k{k}\"\n        fml = create_poly_modelspec(k)\n\n        with pm.Model() as models[nm]:\n\n            print(f\"\\nRunning: {nm}\")\n            pm.glm.GLM.from_formula(fml, df, family=pm.glm.families.Binomial())\n\n            traces[nm] = pm.sample(1000, tune=1000, init=\"adapt_diag\", return_inferencedata=True)\n\n    return models, traces\n\n\ndef plot_traces(traces, model, retain=0):\n    \"\"\"\n    Convenience function:\n    Plot traces with overlaid means and values\n    \"\"\"\n    with model:\n        ax = pm.traceplot(\n            traces[-retain:],\n            lines=tuple([(k, {}, v[\"mean\"]) for k, v in pm.summary(traces[-retain:]).iterrows()]),\n        )\n\n        for i, mn in enumerate(pm.summary(traces[-retain:])[\"mean\"]):\n            ax[i, 0].annotate(\n                f\"{mn:.2f}\",\n                xy=(mn, 0),\n                xycoords=\"data\",\n                xytext=(5, 10),\n                textcoords=\"offset points\",\n                rotation=90,\n                va=\"bottom\",\n                fontsize=\"large\",\n                color=\"#AA0022\",\n            )\n\n\ndef create_poly_modelspec(k=1):\n    \"\"\"\n    Convenience function:\n    Create a polynomial modelspec string for patsy\n    \"\"\"\n    return (\n        \"income ~ educ + hours + age \" + \" \".join([f\"+ np.power(age,{j})\" for j in range(2, k + 1)])\n    ).strip()","21015bb5":"###When in doubt, default to Gelman.  Gelman recommends Cauchy priors as default in a Bayesian Logistic Regression","1051f227":"#!pip install scikit-misc\n\n\n\n## Define custom functions\n\ninvlogit = lambda x: 1\/(1 + tt.exp(-x))\n\ndef trace_predict(trace, X):\n    y_hat = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X, np.transpose(trace['beta']) )) )\n    return(y_hat)\n\n\n# Define prediction helper function\n# for more help see: https:\/\/discourse.pymc.io\/t\/how-to-predict-new-values-on-hold-out-data\/2568\ndef posterior_predict(trace, model, n=1000, progressbar=True):\n    with model:\n        ppc = pm.sample_posterior_predictive(trace,n, progressbar=progressbar)\n    \n    return(np.mean(np.array(ppc['y_obs']), axis=0))\n\n\n## I much prefer the syntax of tidyr gather() and spread() to pandas' pivot() and melt()\ndef gather( df, key, value, cols ):\n    id_vars = [ col for col in df.columns if col not in cols ]\n    id_values = cols\n    var_name = key\n    value_name = value\n    return pd.melt( df, id_vars, id_values, var_name, value_name )\n\n\ndef spread( df, index, columns, values ):\n    return df.pivot(index, columns, values).reset_index(level=index).rename_axis(None,axis=1)\n\n\n## define custom plotting functions\n##Define a Loess Plot - I think you can now just download data for it\ndef fit_loess(df, transform_logit=False):\n    l = loess(df[\"value\"],df[\"target\"])\n    l.fit()\n    pred_obj = l.predict(df[\"value\"],stderror=True)\n    conf = pred_obj.confidence()\n    \n    yhat = pred_obj.values\n    ll = conf.lower\n    ul = conf.upper\n    \n    df[\"loess\"] = np.clip(yhat,.001,.999)\n    df[\"ll\"] = np.clip(ll,.001,.999)\n    df[\"ul\"] = np.clip(ul,.001,.999)\n    \n    if transform_logit:\n        df[\"logit_loess\"] = logit(df[\"loess\"])\n        df[\"logit_ll\"] = logit(df[\"ll\"])\n        df[\"logit_ul\"] = logit(df[\"ul\"])\n    \n    return(df)\n\n#Not entirely sure what Loess plots do...\n\ndef plot_loess(df, features):\n    \n    z = gather(df[[\"id\",\"target\"]+features], \"feature\", \"value\", features)\n    z = z.groupby(\"feature\").apply(fit_loess, transform_logit=True)\n    z[\"feature\"] = pd.to_numeric(z[\"feature\"])\n\n    plot = (\n        ggplot(z, aes(\"value\",\"logit_loess\",ymin=\"logit_ll\",ymax=\"logit_ul\")) + \n        geom_point(alpha=.5) + \n        geom_line(alpha=.5) + \n        geom_ribbon(alpha=.33) + \n        facet_wrap(\"~feature\")\n    )\n    return(plot)\n\n#I've seen these helper functions defined in several ways","6c42ca53":"data = pd.read_csv('adult.csv')","7bf4406e":"data = data[~pd.isnull(data[\"income\"])]\n#data[data[\"native-country\"] == \" United-States\"].sample(5)\n#ValueError: a must be greater than 0 unless no samples are taken","85106e6a":"income = 1 * (data[\"income\"] == \" >50K\")","944c8a36":"data.columns","bc2b8cd4":"data = data[[\"educational-num\", \"occupation\", \"hours-per-week\", \"income\"]]\n\n# Scale age by 10, it helps with model convergence.\n#data[\"age\"] = data[\"age\"] \/ 10.0\n#data[\"age2\"] = np.square(data[\"age\"])\n#data[\"income\"] = income\n#I definitely did not know this","5b54ae1f":"data['occupation'].unique()","588bdfe4":"def replace_occupation(job):\n    \"\"\"\n    This function codes the highest job earned.\n    \"\"\"\n    if job == 'Machine-op-inspct':\n        return 14\n    elif job == 'Farming-fishing':\n        return 13\n    elif job == 'Protective-serv':\n        return 12\n    elif job == '?':\n        return 11\n    elif job == 'Other-service':\n        return 10\n    elif job == 'Prof-specialty':\n        return 9\n    elif job == 'Craft-repair':\n        return 8\n    elif job == 'Adm-clerical':\n        return 7\n    elif job == 'Exec-managerial':\n        return 6\n    elif job == 'Tech-support':\n        return 6\n    elif job == 'Sales':\n        return 4\n    elif job == 'Transport-moving':\n        return 3\n    elif job == 'Handlers-cleaners':\n        return 2\n    elif job == 'Armed-Forces':\n        return 1","a877467f":"data['occupation'] = data['occupation'].apply(lambda x: replace_occupation(x))","a225f15b":"data = data.dropna()","89317b02":"data['occupation'] = data['occupation'].astype(int)","f10d09f9":"#can't use a kernel denisity estimation plot so I'll use a pair plot instead\ng = seaborn.pairplot(data)\n","22fa6889":"# Compute the correlation matrix\ncorr = data.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = seaborn.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nseaborn.heatmap(\n    corr,\n    mask=mask,\n    cmap=cmap,\n    vmax=0.3,\n    linewidths=0.5,\n    cbar_kws={\"shrink\": 0.5},\n    ax=ax,\n);","4979db49":"#data['native-country'].unique()","eccf1032":"#It should be useful to map the income distribution by country.","880f0d72":"#It seems like it would be very useful to separate this dataframe into several dataframes by country.\n#so far, I've simply limited it to the United States\n#However, a Kernel Density Estimation plot would be inappropriate here because is is mostly categorical data\n\n","1f0cc8b5":"data['occupation']","9f83dc75":"#I do not know how to assign the location and scale parameters for the Cauchy \n#I would do well to learn it because Gelman recommends it, and the Half_Cauchy, all the time\n\nlower=-10**6\nhigher=10**6\nwith pm.Model() as first_model:\n    #priors on parameters\n    beta_0=pm.Cauchy('beta_0', alpha=100, beta= 10)\n    beta_ed_num=pm.Cauchy('beta_ed_num', alpha=100, beta=10)\n    beta_occ=pm.Cauchy('beta_occ', alpha=100, beta=10)\n    beta_hpw=pm.Cauchy('hours-per-week', alpha=100, beta=10)\n  \n    #the probability of belonging to class 1\n    p = pm.Deterministic('p', pm.math.sigmoid(beta_0+beta_ed_num*data['educational-num']+\n                                              beta_occ*data['occupation']+\n                                              beta_hpw*data['hours-per-week']))\n","4dc1e34f":"#%pip install pymc3\n#All requirements already satisified","364a3e0d":"with first_model:\n    #fit the data \n    observed=pm.Bernoulli(\"income\", p, observed=data[\"income\"])\n    start=pm.find_MAP()\n    step=pm.Metropolis()\n    \n    #samples from posterior distribution \n    trace=pm.sample(25000, step=step, start=start)\n    burned_trace=trace[15000:]","c0b56f4d":"beta_0 = pm.Normal(\"b0\", mu=0, sigma=100)\n#TypeError: No model on context stack, which is needed to instantiate distributions. \n#Add variable inside a 'with model:' block, or use the '.dist' syntax for a standalone distribution.","d03438f4":"with pm.Model() as logistic_model:\n   pm.glm.GLM.from_formula(\n       \"income ~ educational-number + occupation + hours_per_week\", data, family=pm.glm.families.Binomial()\n   )\ntrace = pm.sample(1000, tune=1000, init=\"adapt_diag\")\n\n#p = pm.Deterministic('p', pm.math.sigmoid(beta_0+beta_ed_num*data['educational_num']+\n #                              beta_occ*data['occupation']+\n#                             beta_hpwt*data['hours-per-week']))\n\n#p = pm.Deterministic('p', pm.math.sigmoid(beta_0+beta_ed_num*data['educational_num']+\n#                              beta_hww*data['hours_per_weej']+\n#                              beta_light*df['Light']+\n#                              beta_co2*df['CO2']+\n#                              beta_humid_ratio*df['occupation']))\n\n#Not sure what the Deterministic (as opposed Stochastic) operator does here\n\n#PatsyError: Error evaluating factor: NameError: name 'hours' is not defined\n#   income ~ educational-number + occupation + hours-per-week\n#                                              ^^^^^","e5a21622":"y_simple = data['outcome']\nx_n = 'duration' \nx_0 = data[x_n].values\nx_c = x_0 - x_0.mean()\n\nwith pm.Model() as model_simple:\n    \u03b1 = pm.Normal('\u03b1', mu=0, sd=10)\n    \u03b2 = pm.Normal('\u03b2', mu=0, sd=10)\n    #B for beta\n    \n    \u03bc = \u03b1 + pm.math.dot(x_c, \u03b2)    \n    \u03b8 = pm.Deterministic('\u03b8', pm.math.sigmoid(\u03bc))\n    bd = pm.Deterministic('bd', -\u03b1\/\u03b2)\n    \n    y_1 = pm.Bernoulli('y_1', p=\u03b8, observed=y_simple)\n\n    trace_simple = pm.sample(1000, tune=1000)","d0c15eee":"random.seed(432532) # comment out for new random samples\n\nrand_feats = [str(x) for x in random.sample(range(0,300), 12)]\ndfp = gather(data[[\"id\",\"target\"]+rand_feats], \"feature\", \"value\", rand_feats)\nsns.set(style=\"ticks\")\n\nsns.pairplot(spread(dfp, \"id\", \"feature\", \"value\").drop(\"id\",1))\n\nplotnine.options.figure_size = (12,9)\n\nrandom.seed(432532) # comment out for new random samples\n\nrand_feats = [str(x) for x in random.sample(range(0,300), 12)]\nplot_loess(data,rand_feats)\n\ndef make_model(X, y, cauchy_scale):\n    model = pm.Model()\n\n    with model:\n\n        # Priors for unknown model parameters\n        alpha = pm.Normal('alpha', mu=0, sd=3)\n        beta = pm.Cauchy('beta', alpha=0, beta=cauchy_scale, shape=X.shape[1])\n        mu = pm.math.dot(X, beta)\n\n        # Likelihood (sampling distribution) of observations\n        y_obs = pm.Bernoulli('y_obs', p=invlogit(alpha + mu),  observed=y)\n    \n    model.name = \"linear_c_\"+str(cauchy_scale)\n    return(model)\n\n\n\nshape_par = .0175\n\nprior_df = pd.DataFrame({\"value\": np.arange(-2,2,.01)})\nprior_df = prior_df.assign(dens = cauchy.pdf(prior_df[\"value\"],0,shape_par))\ncauchy_samples = cauchy.rvs(0, shape_par, 10000)\n\nprint(\"percent non-zero coefs :\", 1-np.mean((cauchy_samples < .1) & (cauchy_samples > -.1)))\n\nplotnine.options.figure_size = (8,6)\nggplot(prior_df, aes(x=\"value\", y=\"dens\")) + geom_line()\n\n\n\n\n","760fa831":"coeffs=['beta_0', 'beta_temp', 'beta_humid', 'beta_light', 'beta_co2', 'beta_humid_ration']\nd=dict()\nfor item in coeffs:\n    d[item]=[burned_trace[item].mean()]\n    \nresult_coeffs=pd.DataFrame.from_dict(d)    \nresult_coeffs\n#coeff_result=pd.DataFrame(d)    \n#coeff_result","2c04f10b":"corr=df.iloc[:, 1:-1].corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Draw the heatmap with the mask and correct aspect ratio\nax=sns.heatmap(corr, mask=mask,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n","be906dbe":"# Histogram of Incomes\n#probably will not work\nplt.hist(data['income'], bins = 14)\nplt.xlabel('income')\nplt.ylabel('Variables')\nplt.title('Distribution of Incomes')","d00826e2":"#data.corr()['income'].sort_values()","fd2559d1":"#the get dummies variable separates it into a hundred columns\n","417e3797":"#dummy_df = pd.get_dummies(X)","d1e34bde":"#dummy_df.head()\ndummy_df.columns","332cad3e":"\"\"\"from sklearn.model_selection import train_test_split\n# df is features and labels are the targets \n# Split by putting 25% in the testing set\nX_train, X_test, y_train, y_test = train_test_split(df, labels, \n                                                   test_size = 0.25,\n                                                    random_state=42)\"\"\"","4bc5e5c0":"\"\"\"# X_train is our training data, we will make a copy for plotting\nX_plot = X_train.copy()\n# Compare grades to the median\nX_plot['relation_median'] = (X_plot['Grade'] >= 12)\nX_plot['Grade'] = X_plot['Grade'].replace({True: 'above', \n                                          False: 'below'})\n# Plot all variables in a loop\nplt.figure(figsize=(12, 12))\nfor i, col in enumerate(X_plot.columns[:-1]):\n    plt.subplot(3, 2, i + 1)\n    subset_above = X_plot[X_plot['relation_median'] == 'above']\n    subset_below = X_plot[X_plot['relation_median'] == 'below']\n    sns.kdeplot(subset_above[col], label = 'Above Median')\n    sns.kdeplot(subset_below[col], label = 'Below Median')\n    plt.legend()\n    plt.title('Distribution of %s' % col)\n    \nplt.tight_layout()\"\"\"","0a3194f6":"comp = pm.stats.compare(model_dict, ic=\"LOO\", method='BB-pseudo-BMA')\n\n# generate posterior predictions for original data\nfor i in range(0,len(traces)):\n    y_hat = trace_predict(traces[i], X)\n    print(\"scale = \",cauchy_scale_pars[i],\", training AUCROC:\",roc_auc_score(y,y_hat))\n    \n# print comparisons\ncomp","6cde63a5":"with pm.Model() as logistic_model:\n    pm.glm.GLM.from_formula(\n        \"income ~ age + age2 + educ + hours\", data, family=pm.glm.families.Binomial()\n    )\n    trace = pm.sample(1000, tune=1000, init=\"adapt_diag\")","9998ebcb":"plot_traces(trace, logistic_model);\n","e538a833":"plt.figure(figsize=(9, 7))\nseaborn.jointplot(trace[\"age\"], trace[\"educ\"], kind=\"hex\", color=\"#4CB391\")\nplt.xlabel(\"beta_age\")\nplt.ylabel(\"beta_educ\");","09d46baf":"def lm_full(trace, age, educ, hours):\n    shape = np.broadcast(age, educ, hours).shape\n    x_norm = np.asarray([np.broadcast_to(x, shape) for x in [age \/ 10.0, educ, hours]])\n    return 1 \/ (\n        1\n        + np.exp(\n            -(\n                trace[\"Intercept\"]\n                + trace[\"age\"] * x_norm[0]\n                + trace[\"age2\"] * (x_norm[0] ** 2)\n                + trace[\"educ\"] * x_norm[1]\n                + trace[\"hours\"] * x_norm[2]\n            )\n        )\n    )\n\n\n# Linear model with hours == 50 and educ == 12\nlm = lambda x, samples: lm_full(samples, x, 12.0, 50.0)\n\n# Linear model with hours == 50 and educ == 16\nlm2 = lambda x, samples: lm_full(samples, x, 16.0, 50.0)\n\n# Linear model with hours == 50 and educ == 19\nlm3 = lambda x, samples: lm_full(samples, x, 19.0, 50.0)","d3f3b091":"Plot the posterior predictive distributions of P(income > $50K) vs. age\npm.plot_posterior_predictive_glm(\n    trace, eval=np.linspace(25, 75, 1000), lm=lm, samples=100, color=\"blue\", alpha=0.15\n)\npm.plot_posterior_predictive_glm(\n    trace,\n    eval=np.linspace(25, 75, 1000),\n    lm=lm2,\n    samples=100,\n    color=\"green\",\n    alpha=0.15,\n)\npm.plot_posterior_predictive_glm(\n    trace, eval=np.linspace(25, 75, 1000), lm=lm3, samples=100, color=\"red\", alpha=0.15\n)\n\nimport matplotlib.lines as mlines\n\nblue_line = mlines.Line2D([\"lm\"], [], color=\"b\", label=\"High School Education\")\ngreen_line = mlines.Line2D([\"lm2\"], [], color=\"g\", label=\"Bachelors\")\nred_line = mlines.Line2D([\"lm3\"], [], color=\"r\", label=\"Grad School\")\nplt.legend(handles=[blue_line, green_line, red_line], loc=\"lower right\")\nplt.ylabel(\"P(Income > $50K)\")\nplt.xlabel(\"Age\")\nplt.show()","696dda2d":"b = trace[\"educ\"]\nplt.hist(np.exp(b), bins=20, density=True)\nplt.xlabel(\"Odds Ratio\")\nplt.show()","224760ee":"pm.plot_posterior_predictive_glm(\n    trace, eval=np.linspace(25, 75, 1000), lm=lm, samples=100, color=\"blue\", alpha=0.15\n)\npm.plot_posterior_predictive_glm(\n    trace,\n    eval=np.linspace(25, 75, 1000),\n    lm=lm2,\n    samples=100,\n    color=\"green\",\n    alpha=0.15,\n)\npm.plot_posterior_predictive_glm(\n    trace, eval=np.linspace(25, 75, 1000), lm=lm3, samples=100, color=\"red\", alpha=0.15\n)\n\nimport matplotlib.lines as mlines\n\nblue_line = mlines.Line2D([\"lm\"], [], color=\"b\", label=\"High School Education\")\ngreen_line = mlines.Line2D([\"lm2\"], [], color=\"g\", label=\"Bachelors\")\nred_line = mlines.Line2D([\"lm3\"], [], color=\"r\", label=\"Grad School\")\nplt.legend(handles=[blue_line, green_line, red_line], loc=\"lower right\")\nplt.ylabel(\"P(Income > $50K)\")\nplt.xlabel(\"Age\")\nplt.show()","5b9c7dcf":"models_lin, traces_lin = run_models(data, 3)\n","770a5ad9":"model_trace_dict = dict()\nfor nm in [\"k1\", \"k2\", \"k3\"]:\n    model_trace_dict.update({nm: traces_lin[nm]})\n\ndfwaic = pm.compare(model_trace_dict, ic=\"WAIC\", scale=\"deviance\")\npm.compareplot(dfwaic);\n#There \n#Hierarchical linear models use all Information Criterion.\n#There's a Bayesian Logistic Regression online that uses DIC and one that uses WAIC","8368fc4c":"comp_abridged = pm.stats.compare(dict(zip(models[0:-2], traces[0:-2])), ic=\"LOO\", method='BB-pseudo-BMA')\ncomp_abridged\n","96844481":"model1 = models[5]\ntrace1 = traces[5]\nmodel1.name\n'","43149465":"coefs = pm.summary(trace1, varnames=[\"beta\"], alpha=.10)\n\ntop_coefs = (coefs\n             .assign(abs_est = abs(coefs[\"mean\"]), non_zero = np.sign(coefs[\"hpd_5\"]) == np.sign( coefs[\"hpd_95\"]))\n             .sort_values(\"abs_est\", ascending=False)\n            ).head(20)\n\ntop_coefs","768c655c":"plotnine.options.figure_size = (12,9)\n\nregex = re.compile(\"__(.*)\")\ntop_feats = [regex.search(x)[1] for x in list(top_coefs.index)]\n\nplot_loess(df, top_feats)","512106e8":"def generate_submission(trace, file_suffix=\"\"):\n\n    test_predictions = trace_predict(trace, X2)\n\n    submission  = pd.DataFrame({'id':df2.id, 'target':test_predictions})\n    submission.to_csv(\"submission_\"+file_suffix+\".csv\", index = False)\n    return(None)\n\nfor model in model_dict.keys():\n    generate_submission(model_dict[model], model.name)","e793e1c0":"comp_MA = pm.stats.compare(dict(zip(models[0:-3], traces[0:-3])), ic=\"LOO\", method='BB-pseudo-BMA')\n\n# do prediction from averaged model\nppc_w = pm.sample_posterior_predictive_w(traces[0:-3], 4000, [make_model(X2,np.zeros(19750),c) for c in cauchy_scale_pars[0:-3]],\n                        weights=comp_MA.weight.sort_index(ascending=True),\n                        progressbar=True)\n                        \ny_hatMA = np.mean(np.array(ppc_w['y_obs']), axis=0)\nsubmission  = pd.DataFrame({'id':df2.id, 'target':y_hatMA})\nsubmission.to_csv('submission_MA.csv', index = False)\n","54d626a3":"non_lin_feats = [\"276\",\"91\",\"240\",\"246\",\"253\",\"255\",\"268\",\"118\",\"240\",\"7\",\"167\",\"65\",\"33\"]\n\nplot_loess(df, non_lin_feats)\n","fbba2540":"def make_polymodel(X, y):\n    \n    with pm.Model() as model:\n        \n        # Priors for unknown model parameters\n        alpha = pm.Normal('alpha', mu=0, sd=3)\n        beta1 = pm.Cauchy('beta', alpha=0, beta=.07, shape=X.shape[1])\n        beta2 = pm.Cauchy('beta^2', alpha=0, beta=.07, shape=X.shape[1])\n        beta3 = pm.Cauchy('beta^3', alpha=0, beta=.07, shape=X.shape[1])\n        \n        mu1 = pm.math.dot(X, beta1)\n        mu2 = pm.math.dot(np.power(X,2), beta2)\n        mu3 = pm.math.dot(np.power(X,3), beta3)\n        \n        p = invlogit(alpha + mu1 + mu2 + mu3)\n        \n        # Likelihood (sampling distribution) of observations\n        y_obs = pm.Bernoulli('y_obs', p=p,  observed=y)\n        \n    return(model)","0635964f":"make_polymodel(X, y)","5b6ea077":"!pip install scikit-misc\n\n\n\n## Define custom functions\n\ninvlogit = lambda x: 1\/(1 + tt.exp(-x))\n\ndef trace_predict(trace, X):\n    y_hat = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X, np.transpose(trace['beta']) )) )\n    return(y_hat)\n\n\n# Define prediction helper function\n# for more help see: https:\/\/discourse.pymc.io\/t\/how-to-predict-new-values-on-hold-out-data\/2568\ndef posterior_predict(trace, model, n=1000, progressbar=True):\n    with model:\n        ppc = pm.sample_posterior_predictive(trace,n, progressbar=progressbar)\n    \n    return(np.mean(np.array(ppc['y_obs']), axis=0))\n\n\n## I much prefer the syntax of tidyr gather() and spread() to pandas' pivot() and melt()\ndef gather( df, key, value, cols ):\n    id_vars = [ col for col in df.columns if col not in cols ]\n    id_values = cols\n    var_name = key\n    value_name = value\n    return pd.melt( df, id_vars, id_values, var_name, value_name )\n\n\ndef spread( df, index, columns, values ):\n    return df.pivot(index, columns, values).reset_index(level=index).rename_axis(None,axis=1)\n\n\n## define custom plotting functions\n\ndef fit_loess(df, transform_logit=False):\n    l = loess(df[\"value\"],df[\"target\"])\n    l.fit()\n    pred_obj = l.predict(df[\"value\"],stderror=True)\n    conf = pred_obj.confidence()\n    \n    yhat = pred_obj.values\n    ll = conf.lower\n    ul = conf.upper\n    \n    df[\"loess\"] = np.clip(yhat,.001,.999)\n    df[\"ll\"] = np.clip(ll,.001,.999)\n    df[\"ul\"] = np.clip(ul,.001,.999)\n    \n    if transform_logit:\n        df[\"logit_loess\"] = logit(df[\"loess\"])\n        df[\"logit_ll\"] = logit(df[\"ll\"])\n        df[\"logit_ul\"] = logit(df[\"ul\"])\n    \n    return(df)\n\n\ndef plot_loess(df, features):\n    \n    z = gather(df[[\"id\",\"target\"]+features], \"feature\", \"value\", features)\n    z = z.groupby(\"feature\").apply(fit_loess, transform_logit=True)\n    z[\"feature\"] = pd.to_numerc(z[\"feature\"])\n\n    plot = (\n        ggplot(z, aes(\"value\",\"logit_loess\",ymin=\"logit_ll\",ymax=\"logit_ul\")) + \n        geom_point(alpha=.5) + \n        geom_line(alpha=.5) + \n        geom_ribbon(alpha=.33) + \n        facet_wrap(\"~feature\")\n    )\n    return(plot)","ccf06928":"lb, ub = np.percentile(b, 2.5), np.percentile(b, 97.5)\n\nprint(\"P({:.3f} < O.R. < {:.3f}) = 0.95\".format(np.exp(lb), np.exp(ub)))\nP(1.378 < O.R. < 1.413) = 0.95","8fecbb62":"models_lin, traces_lin = run_models(data, 3)\n","63c4e300":"**Exploratory Data Analysis**","5db3562a":"**Plot Posterior Predictive Distribution**","fe825280":"**This Notebook is Still Loaded with the Code that I am Reverse Engineering.  Ith is very much still underconstruction**","fe37f247":"**Apply Model**","8eb24d95":"**Data Cleaning**","11a3f4ae":"**Check Goodness of Fit**","51210fb6":"Income correlates strongly with intellegnce, age and nationality.  It make correlate less strongly with other factors.","63c0a0c5":"**Clearly State Priors**"}}