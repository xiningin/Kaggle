{"cell_type":{"f5e1b553":"code","fbd845d3":"code","9fec33f1":"code","d9a5403c":"code","7b110a64":"code","1bb61851":"code","442c9ecc":"code","f0a73fb6":"code","c717e57c":"code","ef6a4b5c":"code","c9a501ed":"code","7ba2dff1":"code","ab27137d":"code","0890086b":"code","a4e86837":"code","26bf9a0c":"code","9b91e9ec":"code","13e9cc39":"code","da4056c1":"code","88d11b2b":"code","3a64330f":"code","a418203a":"code","d0aff5e8":"code","ab3ae261":"code","f83b8881":"code","f3303672":"code","e5a0b10f":"markdown","6bc979da":"markdown"},"source":{"f5e1b553":"# Importing the required libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n","fbd845d3":"# Reading the csv file and putting it into 'df' object.\ndf = pd.read_csv(\"..\/input\/Barley.data.csv\")\ndf.head()","9fec33f1":"#collecting indepedent variables in X\nX = df.iloc[:,1:332]\nX_col = X.columns\nX.head()","d9a5403c":"#collecting depedent variable in Y\nY=df['Predictor']\nY=pd.DataFrame(Y)","7b110a64":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nY_encoded = Y.apply(le.fit_transform)\nY_encoded.head()","1bb61851":"#Savitzky-Golay filter with second degree derivative.\nfrom scipy.signal import savgol_filter \n\nsg=savgol_filter(X,window_length=11, polyorder=3, deriv=2, delta=1.0)","442c9ecc":"sg_x=pd.DataFrame(sg, columns=X_col)\n\nsg_x.head()","f0a73fb6":"# Splitting the data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =train_test_split(sg_x, Y_encoded ,\n                                                    test_size=0.2,\n                                                    random_state=123,stratify=Y)","c717e57c":"from sklearn.ensemble import RandomForestClassifier\n\nRf = RandomForestClassifier(random_state=52)\nRf_fit=Rf.fit(X_train, y_train)","ef6a4b5c":"\ny_pred = Rf_fit.predict(X_test)","c9a501ed":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(\"Test Result:\\n\")        \nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test,y_pred))) ","7ba2dff1":"#Reduction of variables using Recursive Feature Elimination(RFE) techineque\n\nfrom sklearn.feature_selection import RFE\n\n# RFE with 10 features\n\nrfe_10 = RFE(Rf,10)\n\nrfe_10.fit(X_train, y_train)\n\n# selected features\nfeatures_bool = np.array(rfe_10.support_)\nfeatures = np.array(X_col)\nresult = features[features_bool]\nprint('10 selected Features:')\nprint(result)            ","ab27137d":"y_pred = rfe_10.predict(X_test)\n\nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred))) ","0890086b":"# RFE with 15 features\nrfe_15 = RFE(Rf,15)\n\n# fit with 15 features\nrfe_15.fit(X_train, y_train)\n\n# selected features\nfeatures_bool = np.array(rfe_15.support_)\nfeatures = np.array(X_col)\nresult = features[features_bool]\nprint('15 selected Features:')\nprint(result)        \n","a4e86837":"y_pred = rfe_15.predict(X_test)\n\nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred))) ","26bf9a0c":"# RFE with 17 features\n\nrfe_17 = RFE(Rf,17)\n\n# fit with 17 features\nrfe_17.fit(X_train, y_train)\n\n# selected features\nfeatures_bool = np.array(rfe_17.support_)\nfeatures = np.array(X_col)\nresult = features[features_bool]\nprint('17 selected Features:')\nprint(result)     ","9b91e9ec":"y_pred = rfe_17.predict(X_test)\n\nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred))) ","13e9cc39":"# RFE with 20 features\n\nrfe_20 = RFE(Rf,20)\n\n# fit with 20 features\nrfe_20.fit(X_train, y_train)\n\n# selected features\nfeatures_bool = np.array(rfe_20.support_)\nfeatures = np.array(X_col)\nresult = features[features_bool]\nprint('20 selected Features:')\nprint(result)     ","da4056c1":"y_pred = rfe_20.predict(X_test)\n\nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred))) ","88d11b2b":"#collecting variables selected by Rfe 17 in X_imp\nX_imp=sg_x[['756','759','783','789','790','842','843','854','858','954','982','983',\n             '1001','1002','1038','1059','1064']]\nX_col_imp=X_imp.columns\nX_imp.head()","3a64330f":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 =train_test_split(X_imp, Y_encoded,\n                                                    test_size=0.2,\n                                                    random_state=123,stratify=Y)\nX_train1.head()","a418203a":"from sklearn.ensemble import RandomForestClassifier\n\nRf = RandomForestClassifier(random_state=52)\nRf_fit=Rf.fit(X_train1, y_train1)","d0aff5e8":"y_pred1 = Rf.predict(X_test1)","ab3ae261":"print(\"Test Result:\\n\")        \nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test1, y_pred1)))\nprint(\"Classification Report: \\n {}\\n\".format(classification_report(y_test1, y_pred1)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test1,y_pred1))) ","f83b8881":"#sorting features with their importance\nfeature_importances = pd.DataFrame(Rf.feature_importances_,\n                                   index = X_col_imp,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n\nprint(feature_importances)","f3303672":"features =X_col_imp\nimportances = Rf.feature_importances_\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='gray', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","e5a0b10f":"conclusion:- Random Forest is giving accuracy of 80% using all 331 variables. If we reduce Variables using techineque Recursive Feature Elimination(RFE),using 17 variables (756, 759, 783, 789, 790, 842, 843, 854, 858, 954, 982, 983, 1001, 1002, 1038, 1059, & 1064) model is giving acurracy of 77%. Further increase in Variables will not affecting model accuracy as much.","6bc979da":"Pre-processing methods are helpful in eliminating noise generated by spectral data. Raw spectral data were thus processed using a combination of scatter corrections that include Standard Normal Variate (SNV) as well as first and second degree derivatives. The Savitzky-Golay and Gap-segment derivative smoothing filtering algorithms also usefull in eliminating noise.\n\nHere I am using Savitzky-Golay filter with second degree derivative."}}