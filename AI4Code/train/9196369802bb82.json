{"cell_type":{"1aaa5eda":"code","543684d0":"code","d7a9d9ec":"code","78d8b049":"code","91198363":"code","f7d4058f":"code","13503f6d":"code","4efcbe2e":"code","0fb09ad4":"code","e63272ec":"code","fe972ca7":"code","425c5102":"code","2de32540":"code","52f65ed9":"code","090f4f80":"code","7e84fbd3":"code","f7df83da":"code","49d89fb8":"code","b4551d53":"code","ceed8a8d":"code","883a4443":"code","b079005a":"code","2431b4b2":"code","33654c54":"code","74da476d":"code","24c9be7a":"code","9642c581":"code","a8940011":"code","b20e7212":"code","3f1fe368":"code","247bf0a8":"code","f911702e":"code","658954a2":"code","497d03df":"code","6ab6c7da":"code","d935b057":"code","2bdb2a45":"code","79c8cf76":"code","727fede4":"code","2b9d7a52":"code","8d62d3c7":"code","311ecf05":"code","56d794f6":"code","97c0af05":"code","479d173f":"code","caad0f5d":"code","79d778be":"code","ff51a172":"code","1271be7d":"code","96505c72":"code","01cc49c4":"code","2b43bda7":"code","98145ad0":"code","a7cdec0d":"code","8ab6c593":"code","e1a7ce31":"code","14e8d043":"code","1b5608d1":"code","9b9e056d":"code","b11311ac":"code","27723327":"code","002a735d":"code","37f8f592":"code","c60e6403":"code","8691a3b0":"code","f7be0f5a":"code","10fa3ca0":"code","e1ebb339":"code","63aaebde":"code","ae14dd89":"code","5b0e2af3":"code","0aa7b8e5":"code","10dd7761":"code","2ad295d9":"code","f28c084d":"code","dbf6ea6b":"code","e4566cb9":"code","0746e2b0":"code","62a53de3":"code","2afad24e":"code","38b56d60":"code","4c3025a9":"code","1943a33e":"code","95c93cb7":"code","c51804a7":"code","4f2c05ac":"markdown","fef646ba":"markdown","8fd91c0b":"markdown","2e5a9368":"markdown","7790c8e2":"markdown","0e383ab9":"markdown","da396439":"markdown","b4e640e8":"markdown","36bbba71":"markdown","229ef906":"markdown","ac3d649e":"markdown","07088662":"markdown","c565fa03":"markdown","2ffadcc8":"markdown","bb547bb1":"markdown","103ff9b3":"markdown","35ff38f2":"markdown","b4994c28":"markdown","a08f0dd3":"markdown","994b8eff":"markdown"},"source":{"1aaa5eda":"from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport sys\nimport matplotlib.pyplot as plt\nimport datetime\nimport os\nfrom pandas_profiling import ProfileReport\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import RandomizedSearchCV\nimport collections","543684d0":"# os.chdir(\"E:\/pythonProject\") he seleccionado este directorio de trabajo. Comentar celda si no es necesario usarla","d7a9d9ec":"raw = pd.read_csv(\"..\/input\/london-bike-sharing-dataset\/london_merged.csv\")\nraw = raw.rename(columns = {'cnt': 'Count', 't1': 'Temperature', 't2': 'Feels Like', 'hum': 'Humidity', 'wind_speed': 'Wind Speed',\n                           'weather_code':'Weather Code', 'is_holiday':'Holiday', 'is_weekend':'Weekend', 'season':'Season'})\nraw = raw.reset_index()","78d8b049":"raw","91198363":"# raw['Holiday'] = raw['Holiday'].replace(0, False)\n# raw['Holiday'] = raw['Holiday'].replace(1, True)\n# raw['Holiday'] = raw['Holiday'].astype('bool')\n\n# raw['Weekend'] = raw['Weekend'].replace(0, False)\n# raw['Weekend'] = raw['Weekend'].replace(1, True)\n# raw['Weekend'] = raw['Weekend'].astype('bool')\n\nraw['Season'] = raw['Season'].replace({0: 'Spring', 1: 'Summer', 2: 'Autumn', 3: 'Winter'})\nraw['Weather Code'] = raw['Weather Code'].replace({1: 'Clear', 2: 'Few Clouds', 3: 'Broken Clouds', 4: 'Cloudy',\n                                                  7: 'Light Rain', 10: 'Thunderstorm', 26: 'Snowfall', 94: 'Freezing Fog'})\n\nraw['timestamp'] = pd.to_datetime(raw['timestamp'])\nraw.dtypes","f7d4058f":"raw['hour'] = raw['timestamp'].apply(lambda time: time.hour) \nraw['month'] = raw['timestamp'].apply(lambda time: time.month)\nraw['day_of_week'] = raw['timestamp'].apply(lambda time: time.dayofweek)\n\n# Renombramos los d\u00edas de la semana\ndate_names = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'} \nraw['day_of_week'] = raw['day_of_week'].map(date_names)\n\nraw.drop('timestamp', axis = 1, inplace = True)","13503f6d":"raw","4efcbe2e":"profile = ProfileReport(raw)","0fb09ad4":"profile","e63272ec":"# Creaci\u00f3n del dataset bueno\n\nlondon = raw.join(pd.get_dummies(raw['Weather Code']), on = raw['df_index']).drop(columns = ['Weather Code'])\\\n.join(pd.get_dummies(raw['Season']), on = raw['df_index']).drop(columns = ['Season'])\\\n.join(pd.get_dummies(raw['day_of_week']), on = raw['df_index']).drop(columns = ['day_of_week'])\\\n.drop(columns = ['df_index'])","fe972ca7":"results = london['Count'] #variable objetivo\nfeatures = london.drop(columns = ['Count']) #variables independientes","425c5102":"x_train, x_test, y_train, y_test = train_test_split(features, results, test_size = 0.20, shuffle = False)","2de32540":"lr = LinearRegression()\nlr.fit(x_train, y_train)","52f65ed9":"lr.score(x_test, y_test)","090f4f80":"print(\"regresi\u00f3n lineal: \", mean_absolute_error(lr.predict(x_test), y_test),\n      \"\\nPrediciendo la media: \", mean_absolute_error(np.ones([y_test.shape[0]]) * np.mean(y_test), y_test))","7e84fbd3":"error_lr = y_test - lr.predict(x_test)","f7df83da":"plt.hist(error_lr, bins = np.arange(-2000, 2000, 50)) #error sin normalizar\n\nplt.title('Error regresi\u00f3n lineal (en n\u00fam. bicis)')\nplt.xlabel('Error')\nplt.ylabel('Frecuencia')","49d89fb8":"stdscl = StandardScaler()\nstd_x_train = stdscl.fit_transform(x_train.values)\nstd_x_test = stdscl.fit_transform(x_test.values)","b4551d53":"svr = SVR()\nsvr.fit(std_x_train, y_train)","ceed8a8d":"print(\"SVR: \", mean_absolute_error(svr.predict(std_x_test), y_test),\n      \"\\nPrediciendo la media: \", mean_absolute_error(np.ones([y_test.shape[0]]) * np.mean(y_test), y_test))","883a4443":"error_svr = y_test - svr.predict(std_x_test)","b079005a":"plt.hist(error_svr, bins = np.arange(-1000, 4600, 50)) #error sin normalizar\n\nplt.title('Error SVR (en n\u00fam. bicis)')\nplt.xlabel('Error')\nplt.ylabel('Frecuencia')","2431b4b2":"brr = BayesianRidge()\nbrr.fit(x_train, y_train)","33654c54":"print(\"Naive Bayes: \", mean_absolute_error(brr.predict(x_test), y_test),\n      \"\\nPrediciendo la media: \", mean_absolute_error(np.ones([y_test.shape[0]]) * np.mean(y_test), y_test))","74da476d":"error_brr = y_test - brr.predict(x_test)","24c9be7a":"plt.hist(error_brr, bins = np.arange(-2000, 4600, 50)) #error sin normalizar\n\nplt.title('Error NB (en n\u00fam. bicis)')\nplt.xlabel('Error')\nplt.ylabel('Frecuencia')","9642c581":"rf = RandomForestRegressor()\nrf.fit(x_train, y_train)","a8940011":"rf.score(x_test, y_test) #En realidad, esta m\u00e9trica no es muy importante, aunque una puntuaci\u00f3n alta es alentadora.","b20e7212":"print(\"Random Forest: \", mean_absolute_error(rf.predict(x_test), y_test),\n      \"\\nPrediciendo la media: \", mean_absolute_error(np.ones([y_test.shape[0]]) * np.mean(y_test), y_test))","3f1fe368":"london['Count'].max()","247bf0a8":"plt.hist(london['Count'], bins = np.arange(0, 6500, 100))\n\nplt.title('Distribuci\u00f3n de la variable objetivo')\nplt.xlabel('Valor')\nplt.ylabel('Frecuencia')","f911702e":"rf.predict(x_test)","658954a2":"error = y_test - rf.predict(x_test)","497d03df":"plt.hist(error, bins = np.arange(-1500, 1500, 50)) #error sin normalizar\n\nplt.title('Distribuci\u00f3n del error de la predicci\u00f3n (RF)')\nplt.xlabel('Error')\nplt.ylabel('Frecuencia')","6ab6c7da":"error.mean(), error.std() #no est\u00e1 mal pero tenemos mucha desviaci\u00f3n est\u00e1ndar","d935b057":"error_norm = error\/y_test\nerror_norm","2bdb2a45":"plt.hist(error_norm, bins = np.arange(-3, error_norm.max(), 0.1))\n\nplt.title('Distribuci\u00f3n del error normalizado de la predicci\u00f3n (RF)')\nplt.xlabel('Error')\nplt.ylabel('Frecuencia')","79c8cf76":"error_norm.mean(), error_norm.std()","727fede4":"plt.scatter(y_test, error_norm) #el modelo se equivoca algo m\u00e1s cuanto m\u00e1s peque\u00f1o es el n\u00famero de bicis que se cogen\n\nplt.title('Error normalizado vs N\u00famero de bicis')\nplt.xlabel('N\u00famero de bicis')\nplt.ylabel('Error')","2b9d7a52":"plt.scatter(rf.predict(x_test), y_test, alpha = 0.5)\nplt.plot(y_test, y_test, color = 'magenta')","8d62d3c7":"y_test[y_test < 1000]","311ecf05":"rf.predict(x_test)[y_test < 1000]","56d794f6":"error_peque\u00f1o = y_test[y_test < 1000] - rf.predict(x_test)[y_test < 1000]","97c0af05":"error_peque\u00f1o.mean()","479d173f":"(y_test[y_test < 1000] - y_test[y_test < 1000].mean()).mean() #en recuentos peque\u00f1os parece \"mucho\" mejor predecir la media","caad0f5d":"plt.scatter(y_test[y_test < 1000], rf.predict(x_test)[y_test < 1000])\n\nplt.xlabel('N\u00famero de bicis real')\nplt.ylabel('Predicci\u00f3n')\nplt.title('Predicci\u00f3n Random Forest vs Valores reales')","79d778be":"plt.scatter(y_test[y_test < 1000], error_peque\u00f1o)\n\nplt.xlabel('N\u00famero de bicis')\nplt.ylabel('Error')\nplt.title('Error Random Forest a lo largo de las observaciones')","ff51a172":"rf.get_params() #base para ver qu\u00e9 puedo tunear","1271be7d":"rf.feature_importances_","96505c72":"importances = rf.feature_importances_\nindices = np.argsort(importances)\nfeature_names = features.keys()\n\nplt.title('Importancia de variables')\nplt.barh(range(len(indices)), importances[indices], color = 'magenta', align = 'center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Importancia relativa')\nplt.show()","01cc49c4":"# a\u00f1ado una columna random\n\nlondon['random'] = np.random.random(london['Clear'].size) #que tenga la misma longitud que las otras columnas!","2b43bda7":"results_2 = london['Count'] #variable objetivo\nfeatures_2 = london.drop(columns = ['Count']) #variables independientes","98145ad0":"x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(features_2, results_2, test_size = 0.20, shuffle = False)","a7cdec0d":"rf_2 = RandomForestRegressor()\nrf_2.fit(x_train_2, y_train_2)","8ab6c593":"rf_2.score(x_test_2, y_test_2)","e1a7ce31":"print(\"Random Forest: \", mean_absolute_error(rf_2.predict(x_test_2), y_test_2),\n      \"\\nPrediciendo la media: \", mean_absolute_error(np.ones([y_test.shape[0]]) * np.mean(y_test_2), y_test_2))","14e8d043":"importances_2 = rf_2.feature_importances_\nindices_2 = np.argsort(importances_2)\nfeature_names_2 = features_2.keys()\n\nplt.title('Importancia de variables')\nplt.barh(range(len(indices_2)), importances_2[indices_2], color = 'magenta', align = 'center')\nplt.yticks(range(len(indices_2)), [feature_names_2[i] for i in indices_2])\nplt.xlabel('Importancia relativa')\nplt.show()","1b5608d1":"dict_no_ordenado = dict(zip(rf.feature_importances_, x_train.columns))","9b9e056d":"ordered = collections.OrderedDict(sorted(dict_no_ordenado.items()))","b11311ac":"ordered","27723327":"selected_features = np.array(list(ordered.values()))[-1:-15:-1]","002a735d":"rf.fit(x_train.loc[:,selected_features], y_train)","37f8f592":"rf.score(x_test.loc[:, selected_features], y_test)","c60e6403":"print(\"Random Forest: \", mean_absolute_error(rf.predict(x_test.loc[:,selected_features]), y_test),\n      \"\\nPrediciendo la media: \", mean_absolute_error(np.ones([y_test.shape[0]]) * np.mean(y_test), y_test))","8691a3b0":"plt.scatter(london['hour'], london['Count'])\n\nplt.xlabel('Hora')\nplt.ylabel('N\u00famero de bicis')\nplt.title('N\u00famero de bicis vs Hora del d\u00eda')","f7be0f5a":"# Random Forest con tuneo de par\u00e1metros\n# N\u00famero de \u00e1rboles\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 1000, num = 10)]\n# Tuneamos el n\u00famero de variables a usar en cada caso\nmax_features = ['auto', 'sqrt']\n# M\u00e1ximo n\u00famero de niveles en cada \u00e1rbol\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Sampsize m\u00ednimo para cada nodo\nmin_samples_split = [2, 5, 10]\n# Sampsize m\u00ednimo para cada hoja del \u00e1rbol\nmin_samples_leaf = [1, 2, 4]\n# M\u00e9todo de selecci\u00f3n\nbootstrap = [True, False]\n# Creamos la rejilla con los valores de arriba\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","10fa3ca0":"rf_3 = RandomForestRegressor()\n# B\u00fasqueda aleatoria de par\u00e1metros con validaci\u00f3n cruzada, \n# buscamos entre muchas combinaciones distintas.\nrf_random = RandomizedSearchCV(estimator = rf_3, param_distributions = random_grid,\n                               n_iter = 100, cv = 3, verbose = 3,\n                               n_jobs = 5) #n_jobs = n\u00famero de n\u00facleos del procesador (cambiar en funci\u00f3n del ordenador)\nrf_random.fit(x_train, y_train)","e1ebb339":"print(\"Random Forest tuneado: \", mean_absolute_error(rf_random.predict(x_test), y_test),\n      \"\\nPrediciendo la media: \", mean_absolute_error(np.ones([y_test.shape[0]]) * np.mean(y_test), y_test))","63aaebde":"rf_random.best_params_","ae14dd89":"rf_random.predict(x_test)","5b0e2af3":"error_random = y_test - rf_random.predict(x_test)","0aa7b8e5":"plt.hist(error_random, bins = np.arange(-1500, 1500, 50)) #error sin normalizar\n\nplt.title('Distribuci\u00f3n del error de la predicci\u00f3n (RF)')\nplt.xlabel('Error')\nplt.ylabel('Frecuencia')","10dd7761":"error_random.mean(), error_random.std() #seguimos con mucha desviaci\u00f3n est\u00e1ndar","2ad295d9":"error_random_norm = error_random\/y_test\nerror_random_norm","f28c084d":"plt.hist(error_random_norm, bins = np.arange(-3, error_random_norm.max(), 0.1))\n\nplt.title('Distribuci\u00f3n del error normalizado de la predicci\u00f3n (RF)')\nplt.xlabel('Error')\nplt.ylabel('Frecuencia')","dbf6ea6b":"error_random_norm.mean(), error_random_norm.std()","e4566cb9":"plt.scatter(y_test, error_random_norm, alpha = 0.3)\n\nplt.title('Error normalizado vs N\u00famero de bicis')\nplt.xlabel('N\u00famero de bicis')\nplt.ylabel('Error')","0746e2b0":"plt.scatter(rf_random.predict(x_test), y_test, alpha = 0.5)\nplt.plot(y_test, y_test, color = 'magenta') #de momento parece que el tuneo no est\u00e1 siendo exitoso","62a53de3":"y_test[y_test < 1000]","2afad24e":"rf_random.predict(x_test)[y_test < 1000]","38b56d60":"error_peque\u00f1o_random = y_test[y_test < 1000] - rf_random.predict(x_test)[y_test < 1000]","4c3025a9":"error_peque\u00f1o_random.mean()","1943a33e":"(y_test[y_test < 1000] - y_test[y_test < 1000].mean()).mean() #Sigue siendo mejor predecir la media en recuentos peque\u00f1os","95c93cb7":"plt.scatter(y_test[y_test < 1000], rf_random.predict(x_test)[y_test < 1000])\nplt.plot(y_test[y_test < 1000], y_test[y_test < 1000], color = \"magenta\")\n\nplt.xlabel('N\u00famero de bicis real')\nplt.ylabel('Predicci\u00f3n')\nplt.title('Predicci\u00f3n Random Forest vs Valores reales')","c51804a7":"plt.scatter(y_test[y_test < 1000], error_peque\u00f1o_random)\nplt.axhline(y=0, color='m', linestyle='-')\n\nplt.xlabel('N\u00famero de bicis')\nplt.ylabel('Error')\nplt.title('Error Random Forest a lo largo de las observaciones')","4f2c05ac":"Aqu\u00ed extraigo nuevas variables a partir de \"timestamp\": a\u00f1o, mes, d\u00eda, tramo horario (ma\u00f1ana, tarde, noche)","fef646ba":"Parece un modelo bastante bueno cuando hay muchas bicis pero \"no tan bueno\" cuando hay pocas. Centr\u00e9monos ahora en las observaciones con pocas bicis:","8fd91c0b":"## Trabajo fin de m\u00e1ster: Santander Bikes (an\u00e1lisis y predicci\u00f3n)\n\nEste notebook contiene todo el c\u00f3digo realizado, del cual se obtienen los gr\u00e1ficos y las m\u00e9tricas incluidas en el informe en PDF. Dado que no se ha seleccionado una semilla, los resultados pueden variar sensiblemente, pero nunca de manera que alteren las conclusiones, pues el trabajo se ha ejecutado varias veces y las salidas apenas han diferido por unas pocas cifras decimales. El c\u00f3digo est\u00e1 comentado y se explica sucintamente la utilidad de la mayor\u00eda de las celdas. Sin embargo, la explicaci\u00f3n detallada de las salidas y la interpretaci\u00f3n se encuentran \u00fanicamente en el informe. Sirva esto, pues, como anexo.","2e5a9368":"Seguimos sin hacer progresos.","7790c8e2":"#### SVR","0e383ab9":"#### NB","da396439":"Cargamos el archivo y renombramos las columnas para mayor claridad en la interpretaci\u00f3n:","b4e640e8":"Hacemos un an\u00e1lisis inicial de los datos usando ProfileReport:","36bbba71":"Hemos visto que la hora parece muy importante, y que muchas variables no aportan ninguna informaci\u00f3n. \u00bfPodr\u00edamos construir un modelo m\u00e1s simple sin perder informaci\u00f3n? Veamos si haciendo una selecci\u00f3n de variables podemos conseguirlo.","229ef906":"Parece que eliminar todas esas variables no afecta de manera significativa al modelo. Veamos a continuaci\u00f3n una distribuci\u00f3n de nuestra variable objetivo en funci\u00f3n de la hora:","ac3d649e":"Nuestra regresi\u00f3n lineal parece ligeramente mejor que predecir \u00fanicamente la media para todas las observaciones test, pero no es un gran avance. Probemos con otros modelos.","07088662":"### Ideas de mejora para el futuro:\n\n#### - mejorar los datos (hacer m\u00e1s features que est\u00e9n mejor) O mejorar los modelos (tunear par\u00e1metros del RF)\n\n#### - probar a entrenar el modelo con pocos datos e ir subiendo: \u00bfa partir de qu\u00e9 punto el aprendizaje del modelo \"se estanca\"?\n\n#### - buscar m\u00e1s\"m\u00e9tricas de calidad\" para ver si mi modelo es bueno en las cosas que importan\n\n#### - probar una red neuronal y entrenarla con datos en vivo","c565fa03":"Cambiamos los valores de las variables que son categ\u00f3ricas:","2ffadcc8":"Estas predicciones han sido hechas a partir de un Random Forest sin ning\u00fan tuneo. Descubramos sus par\u00e1metros para ver si es posible tunear alguno de ellos y mejorar la precisi\u00f3n del modelo.","bb547bb1":"Pr\u00e1cticamente el mismo resultado.","103ff9b3":"Parece un modelo bastante bueno cuando hay muchas bicis pero \"no tan bueno\" cuando hay pocas. Centr\u00e9monos ahora en las observaciones con pocas bicis:","35ff38f2":"#### RF","b4994c28":"Parece que la hora es la variable m\u00e1s importante con mucha diferencia. \u00bfEs eso verdad? Puede que nuestro modelo no est\u00e9 ponderando correctamente. Para ver si el modelo funciona bien, vamos a a\u00f1adir una variable aleatoria que llamaremos \"random\" y que, como cabe esperar, no aportar\u00e1 absolutamente nada al modelo. Si el modelo est\u00e1 bien construido, esta variable tendr\u00e1 una importancia nula o casi nula.","a08f0dd3":"No parece haber una relaci\u00f3n muy lineal entre la hora y el n\u00famero de bicis (casi todo al ir a trabajar o al salir del trabajo), pero se observa claramente que existe una raz\u00f3n para que esta variable sea tan importante.","994b8eff":"#### REGRESI\u00d3N LINEAL"}}