{"cell_type":{"4cafef47":"code","ff9d6bf8":"code","46feaf36":"code","6ba2cf75":"code","a7b03467":"code","4bdf9062":"code","f2d16b29":"code","bd546422":"code","a858e8b5":"code","dd181969":"code","f78b9553":"code","f5e8218a":"code","8f67b96f":"code","8dbfa9f1":"code","f72e19fb":"code","8f4c2ac6":"code","758c742e":"code","90925039":"code","3096c8cb":"code","af77a9d4":"code","bdd750a9":"code","3ea7ae8b":"code","08f2d292":"code","efb2b3d2":"code","a9fd1063":"code","53edf112":"code","300a2e49":"code","4a2335bb":"code","b09996dd":"code","94a92a2e":"code","1ef090f5":"code","d172af97":"code","5970001f":"code","b544b80b":"code","fd36cda0":"code","88c0b0dd":"code","294bd72a":"code","c6f807c8":"code","69d357ea":"code","c2abe21a":"code","94e705ef":"code","179fc52c":"code","ef6b9d6a":"code","9c3140ca":"code","76cf9cf2":"code","5e7b6ba6":"markdown","d2ee4324":"markdown","86c5ab7c":"markdown","65096bf5":"markdown","799e15e0":"markdown","6ad9c7e6":"markdown","ebd29e44":"markdown","9b2f2e54":"markdown","7cb88080":"markdown","1eef48b7":"markdown","4d36f932":"markdown","22cf6aec":"markdown","0eef64ae":"markdown","be72f6f4":"markdown","14bf16d6":"markdown","fe7d5b40":"markdown","148fc538":"markdown","17a729db":"markdown","caef3991":"markdown","31c59440":"markdown","0be3bf5c":"markdown","2fd43284":"markdown","d9257fb2":"markdown"},"source":{"4cafef47":"#Setting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport matplotlib.lines as mlines\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","ff9d6bf8":"#Read file\ndata = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")","46feaf36":"#Preliminary inspection of data\ndata.head()","6ba2cf75":"#Checking missing data - everything is ready\ndata.isna().sum()","a7b03467":"#Looking for and visualizing correlations with correlation matrix on raw data\nfig, ax = plt.subplots(figsize = (10,10))\nax = sn.heatmap(data.corr(), annot=True, fmt='.2f')\nplt.show()","4bdf9062":"#Plotting the highest correlations\nfig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(10,20))\nred_dot = mlines.Line2D([], [], color='darkred', marker='o', linestyle='None',\n                          markersize=5, label='Failure')\nblue_dot = mlines.Line2D([], [], color='darkblue', marker='o', linestyle='None',\n                          markersize=5, label='No Failure')\nscatter1 = ax0.scatter(x=data.age, y=data.ejection_fraction, c=data.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax0.set(title = \"Heart Failure and Ejection Fraction\", ylabel=\"Ejection Fraction, %\")\nax0.grid(color=\"lightgrey\", zorder=0)\nax0.legend(handles=[blue_dot, red_dot])\nscatter2 = ax1.scatter(x=data.age, y=data.serum_creatinine, c=data.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax1.set(title = \"Heart Failure and Creatinine Level\", ylabel=\"Creatinine Level, mg\/dL\")\nax1.grid(color=\"lightgrey\", zorder=0)\nax1.legend(handles=[blue_dot, red_dot])\nscatter3 = ax2.scatter(x=data.age, y=data.serum_sodium, c=data.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax2.set(title = \"Heart Failure and Sodium Level\", xlabel=\"Age\", ylabel=\"Sodium Level, mEq\/L\")\nax2.grid(color=\"lightgrey\", zorder=0)\nax2.legend(handles=[blue_dot, red_dot])\nscatter4 = ax3.scatter(x=data.age, y=data.time, c=data.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax3.set(title = \"Heart Failure and Follow-up period\", xlabel=\"Age\", ylabel=\"Follow-up period (days)\")\nax3.grid(color=\"lightgrey\", zorder=0)\nax3.legend(handles=[blue_dot, red_dot]);","f2d16b29":"subset_men = data[data.sex==1]\nsubset_women = data[data.sex==0]\nsubset_smokers = data[data.smoking==1]","bd546422":"#Correlation matrices on sex subsets\nfig, (ax1, ax2) = plt.subplots(figsize = (15,7.5), nrows=1, ncols=2, sharey=True)\nsn.heatmap(subset_men.corr(), ax=ax1, annot=True, fmt='.2f', cbar=False)\nsn.heatmap(subset_women.corr(), ax=ax2, annot=True, fmt='.2f');\n","a858e8b5":"#Plotting things for men only\nred_dot = mlines.Line2D([], [], color='darkred', marker='o', linestyle='None',\n                          markersize=5, label='Failure')\nblue_dot = mlines.Line2D([], [], color='darkblue', marker='o', linestyle='None',\n                          markersize=5, label='No Failure')\nfig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(10,15))\nscatter = ax0.scatter(x=subset_men.age, y=subset_men.ejection_fraction, c=subset_men.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax0.set(title = \"Heart Failure and Ejection Fraction\", ylabel=\"Ejection Fraction, %\")\nax0.grid(color=\"lightgrey\", zorder=0)\nax0.legend(handles=[blue_dot, red_dot])\nscatter = ax1.scatter(x=subset_men.age, y=subset_men.serum_creatinine, c=subset_men.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax1.set(title = \"Heart Failure and Creatinine Level\", ylabel=\"Creatinine Level, mg\/dL\")\nax1.grid(color=\"lightgrey\", zorder=0)\nax1.legend(handles=[blue_dot, red_dot])\nscatter = ax2.scatter(x=subset_men.age, y=subset_men.serum_sodium, c=subset_men.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax2.set(title = \"Heart Failure and Sodium Level\", ylabel=\"Sodium Level, mEq\/L\")\nax2.grid(color=\"lightgrey\", zorder=0)\nax2.legend(handles=[blue_dot, red_dot])\nscatter = ax3.scatter(x=subset_men.age, y=subset_men.time, c=subset_men.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax3.set(title = \"Heart Failure and Follow-up period\", xlabel=\"Age\", ylabel=\"Follow-up period (days)\")\nax3.grid(color=\"lightgrey\", zorder=0)\nax3.legend(handles=[blue_dot, red_dot]);","dd181969":"#Plotting things for women only\nfig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(10,20))\nscatter = ax0.scatter(x=subset_women.age, y=subset_women.ejection_fraction, c=subset_women.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax0.set(title = \"Heart Failure and Ejection Fraction\", ylabel=\"Ejection Fraction, %\")\nax0.grid(color=\"lightgrey\", zorder=0)\nax0.legend(handles=[blue_dot, red_dot])\nscatter = ax1.scatter(x=subset_women.age, y=subset_women.serum_creatinine, c=subset_women.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax1.set(title = \"Heart Failure and Creatinine Level\", ylabel=\"Creatinine Level, mg\/dL\")\nax1.grid(color=\"lightgrey\", zorder=0)\nax1.legend(handles=[blue_dot, red_dot])\nscatter = ax2.scatter(x=subset_women.age, y=subset_women.serum_sodium, c=subset_women.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax2.set(title = \"Heart Failure and Sodium Level\", ylabel=\"Sodium Level, mEq\/L\")\nax2.grid(color=\"lightgrey\", zorder=0)\nax2.legend(handles=[blue_dot, red_dot])\nscatter = ax3.scatter(x=subset_women.age, y=subset_women.time, c=subset_women.DEATH_EVENT, zorder=3, cmap=\"coolwarm\")\nax3.set(title = \"Heart Failure and Follow-up period\", xlabel=\"Age\", ylabel=\"Follow-up period (days)\")\nax3.grid(color=\"lightgrey\", zorder=0)\nax3.legend(handles=[blue_dot, red_dot]);","f78b9553":"# We are going to check three models and choose the best of them\n#Import models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","f5e8218a":"# Preparing a function for evaluating the models\n\ndef evaluate_preds(y_true, y_preds):\n    \"\"\"\n    Performs evaluation comparison on y_true labels vs. y_pred labels.\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_preds)\n    precision = precision_score(y_true, y_preds)\n    recall = recall_score(y_true, y_preds)\n    f1 = f1_score(y_true, y_preds)\n    metric_dict = {\"accuracy\": round(accuracy, 2),\n                   \"precision\": round(precision, 2), \n                   \"recall\": round(recall, 2),\n                   \"f1\": round(f1, 2)}\n    print(f\"Acc: {accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1 score: {f1:.2f}\")\n\n    return metric_dict","8f67b96f":"# The data seems to be sorted on time column. Let's correct it\ndata_schuffled = data.sample(frac = 1)\nX = data_schuffled.drop(\"DEATH_EVENT\", axis=1)\ny = data_schuffled.DEATH_EVENT","8dbfa9f1":"# Splitting in test and train sets\nnp.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)","f72e19fb":"# Scaling (normalizing) X\nscaler=StandardScaler()\nscaler.fit(X)\nX_train_n = scaler.transform(X_train)\nX_test_n = scaler.transform(X_test)","8f4c2ac6":"# Declaring models\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"LogisticReg\": LogisticRegression(),\n          \"RandomForest\": RandomForestClassifier()}\n\n# A function to select the best model\ndef find_best_model(models, X_train, X_test, y_train, y_test):\n    scores = {}\n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        scores[name] = model.score(X_test, y_test)\n    return scores","758c742e":"# Let's see which algorithm performs best\nscores = find_best_model(models=models, X_train=X_train_n, X_test=X_test_n, y_train=y_train, y_test=y_test)\nscores","90925039":"# Let's tune the LogReg\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 40),\n                \"solver\": [\"liblinear\"]}\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=40,\n                                verbose=True)\n\n# Fit random hyperparameter search model and show the score\nrs_log_reg.fit(X_train_n, y_train)\nrs_log_reg.score(X_test_n, y_test)","3096c8cb":"rs_log_reg.best_params_","af77a9d4":"# Let's cross-validate this model\nrs_best_n = LogisticRegression(C = 0.011253355826007646, solver=\"liblinear\")\nrs_best_n.fit(X_train_n, y_train)\ncross_val_score(rs_best_n, scaler.transform(X), y, cv=5)","bdd750a9":"# Making predictions\ny_preds_n = rs_best_n.predict(X_test_n)","3ea7ae8b":"# Evaluating the model and plotting a confusion matrix\nevaluate_preds(y_test, y_preds_n);\nconf_mat = confusion_matrix(y_test, y_preds_n)\nsn.heatmap(conf_mat\/np.sum(conf_mat), fmt='.2%', annot=True, cbar=False,yticklabels=[\"No Failure\", \"Failure\"],\n    xticklabels=[\"Predicted No Failure\", \"Predicted Failure\"]);","08f2d292":"# Setting, fitting the model, getting R2\nRFCn = RandomForestClassifier()\nRFCn.fit(X_train_n, y_train)\nRFCn.score(X_test_n, y_test)","efb2b3d2":"# Cross-Validating the model\ncross_val_score(RFCn, scaler.transform(X), y, cv=5)","a9fd1063":"# Let's tune the RandomForest\ngrid = {\"n_estimators\": [10, 50, 100, 200, 500],\n        \"max_depth\": [None, 5, 10, 20, 30],\n        \"max_features\": [\"auto\", \"sqrt\"],\n        \"min_samples_split\": [2, 4, 6],\n        \"min_samples_leaf\": [1, 2, 4]}\n\nclf = RandomForestClassifier(n_jobs=1)\nrs_clf = RandomizedSearchCV(estimator=clf,\n                            param_distributions=grid,\n                            n_iter=40, \n                            cv=5)\n\n# Fit random hyperparameter search model\nrs_clf.fit(X_train_n, y_train)\nrs_clf.score(X_test_n, y_test)","53edf112":"rs_clf.best_params_","300a2e49":"clf_best = RandomForestClassifier(n_estimators=100, min_samples_leaf=4, min_samples_split=6, max_features=\"auto\", max_depth=10)\nclf_best.fit(X_train_n, y_train)\nclf_best.score(X_test_n, y_test)","4a2335bb":"# And let's cross-validate it\ncross_val_score(clf_best, scaler.transform(X), y, cv=5)","b09996dd":"# This is the final cross-validated R2\ncross_val_score(clf_best, scaler.transform(X), y, cv=5).mean()","94a92a2e":"# Let's make predictions\ny_preds_n = clf_best.predict(X_test_n)","1ef090f5":"# Evaluating and confisuon matrix\nevaluate_preds(y_test, y_preds_n);\nconf_mat = confusion_matrix(y_test, y_preds_n)\nsn.heatmap(conf_mat\/np.sum(conf_mat), fmt='.2%', annot=True, cbar=False,yticklabels=[\"No Failure\", \"Failure\"],\n    xticklabels=[\"Predicted No Failure\", \"Predicted Failure\"]);","d172af97":"X = data_schuffled.drop(\"DEATH_EVENT\", axis=1)\ny = data_schuffled.DEATH_EVENT\nnp.random.seed(42)\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X,y, test_size=0.3)\nclf_best.fit(scaler.transform(X_train1), y_train1)\nprint(f\"Rsquared: {clf_best.score(scaler.transform(X_test1), y_test1)}, Cross-validated Rsquared: {cross_val_score(clf_best, scaler.transform(X), y, cv=5).mean()}\")","5970001f":"y_preds1 = clf_best.predict(scaler.transform(X_test1))\nevaluate_preds(y_test1, y_preds1);\nconf_mat = confusion_matrix(y_test1, y_preds1)\nsn.heatmap(conf_mat\/np.sum(conf_mat), fmt='.2%', annot=True, cbar=False,yticklabels=[\"No Failure\", \"Failure\"],\n    xticklabels=[\"Predicted No Failure\", \"Predicted Failure\"]);","b544b80b":"modelled_data = pd.DataFrame(X_test1.reset_index(drop=True), columns=data.drop(\"DEATH_EVENT\", axis=1).columns)\nmodelled_data[\"real_death\"]=y_test1.reset_index(drop=True)\nmodelled_data[\"predicted_death\"]=y_preds1\n\n# Make a column with badly predicted outcomes\nmodelled_data[\"false\"] = abs(modelled_data.real_death - modelled_data.predicted_death)\nmodelled_data.head()","fd36cda0":"# Let's compare the mean values of all parameters for good and bad predictions\nmodelled_data[modelled_data.false==1].mean()","88c0b0dd":"modelled_data[modelled_data.false==0].mean()","294bd72a":"red_dot = mlines.Line2D([], [], color='darkred', marker='o', linestyle='None',\n                          markersize=5, label='Bad Predict')\nblue_dot = mlines.Line2D([], [], color='darkblue', marker='o', linestyle='None',\n                          markersize=5, label='Good Predict')\n\nfig, [ax1, ax2, ax3, ax4, ax5, ax6] = plt.subplots(nrows=6,figsize=(5,30), sharex=True)\nax1.scatter(modelled_data.age, modelled_data.platelets, c=modelled_data.false, cmap=\"coolwarm\")\nax1.legend(handles=[blue_dot, red_dot])\nax1.set(ylabel=\"Platelets\")\nax2.scatter(modelled_data.age, modelled_data.creatinine_phosphokinase, c=modelled_data.false, cmap=\"coolwarm\")\nax2.set(ylabel=\"Creatinine Phosphokinase\")\nax3.scatter(modelled_data.age, modelled_data.ejection_fraction, c=modelled_data.false, cmap=\"coolwarm\")\nax3.set(ylabel=\"Ejection Fraction\")\nax4.scatter(modelled_data.age, modelled_data.serum_creatinine, c=modelled_data.false, cmap=\"coolwarm\")\nax4.set(ylabel=\"Serum Creatinine\")\nax5.scatter(modelled_data.age, modelled_data.serum_sodium, c=modelled_data.false, cmap=\"coolwarm\")\nax5.set(ylabel=\"Serum Sodium\")\nax6.scatter(modelled_data.age, modelled_data.time, c=modelled_data.false, cmap=\"coolwarm\")\nax6.set(ylabel=\"Time\");","c6f807c8":"y_proba = clf_best.predict_proba(scaler.transform(X_test1))\nmodelled_data[\"predicted0\"] = y_proba[:,0]\nmodelled_data[\"predicted1\"] = y_proba[:,1]\n# Assessing the quality of prediction: if any of predictions have a probability > 80%, we will treat is as a good one\nmodelled_data[\"pred_qual\"] = np.where(np.logical_or(modelled_data[\"predicted0\"] >0.8, modelled_data[\"predicted1\"] >0.8), 1, 0)","69d357ea":"modelled_data[\"pred_qual\"].sum()","c2abe21a":"# And plot it again\nfig, [ax1, ax2, ax3, ax4, ax5, ax6] = plt.subplots(nrows=6,figsize=(5,30), sharex=True)\nax1.scatter(modelled_data.age, modelled_data.platelets, c=modelled_data.pred_qual, cmap=\"coolwarm\")\nax1.legend(handles=[blue_dot, red_dot])\nax1.set(ylabel=\"Platelets\")\nax2.scatter(modelled_data.age, modelled_data.creatinine_phosphokinase, c=modelled_data.pred_qual, cmap=\"coolwarm\")\nax2.set(ylabel=\"Creatinine Phosphokinase\")\nax3.scatter(modelled_data.age, modelled_data.ejection_fraction, c=modelled_data.pred_qual, cmap=\"coolwarm\")\nax3.set(ylabel=\"Ejection Fraction\")\nax4.scatter(modelled_data.age, modelled_data.serum_creatinine, c=modelled_data.pred_qual, cmap=\"coolwarm\")\nax4.set(ylabel=\"Serum Creatinine\")\nax5.scatter(modelled_data.age, modelled_data.serum_sodium, c=modelled_data.pred_qual, cmap=\"coolwarm\")\nax5.set(ylabel=\"Serum Sodium\")\nax6.scatter(modelled_data.age, modelled_data.time, c=modelled_data.pred_qual, cmap=\"coolwarm\")\nax6.set(ylabel=\"Time\");","94e705ef":"data_death = data[data.DEATH_EVENT==1]\ndata_nodeath = data[data.DEATH_EVENT==0].sample(n=100)\ndata_balanced = pd.concat([data_death, data_nodeath], axis=0, ignore_index=True)","179fc52c":"# Let's shuffle the balanced dataset\ndata_balanced_s = data_balanced.sample(frac=1)\ndata_balanced_s.reset_index(inplace=True, drop=True)\ndata_balanced_s.head()","ef6b9d6a":"X = data_balanced_s.drop(\"DEATH_EVENT\", axis=1)\ny = data_balanced_s.DEATH_EVENT\nnp.random.seed(42)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X,y, test_size=0.2)\nclf_best.fit(scaler.transform(X_train2), y_train2)\nclf_best.score(scaler.transform(X_test2), y_test2)","9c3140ca":"cross_val_score(clf_best, scaler.transform(X), y, cv=5).mean()","76cf9cf2":"y_preds1 = clf_best.predict(scaler.transform(X_test1))\nevaluate_preds(y_test1, y_preds1);\nconf_mat = confusion_matrix(y_test1, y_preds1)\nsn.heatmap(conf_mat\/np.sum(conf_mat), fmt='.2%', annot=True, cbar=False,yticklabels=[\"No Failure\", \"Failure\"],\n    xticklabels=[\"Predicted No Failure\", \"Predicted Failure\"]);","5e7b6ba6":"### As a conclusion, the best predictions were made on a balanced set. Although it has less than 200 samples. I would recommend to collect more data for patients with bad outcome and try again","d2ee4324":"#### A bit better.","86c5ab7c":"#### Well, there is no obvious pattern","65096bf5":"### But still no visual correlations","799e15e0":"#### Here all the splits are at least above 80%, but still the difference is large","6ad9c7e6":"### The last thing to try - the dataset is unbalanced. We have fewer deaths as good outcomes. Let's try to balance the set - but we will lose a lot of data.","ebd29e44":"#### Let's try to increase the test subset to 30%","9b2f2e54":"## Getting things ready","7cb88080":"### Well, the correlations seem to be a bit different. It might make sence to analyze the different sexes separately later","1eef48b7":"#### We have too many false negatives, which is not great for a heart disease. Let's try the Random Forest","4d36f932":"#### Didn't get any better","22cf6aec":"#### Well, this one is much better...","0eef64ae":"#### The model performs a bit better than the logistic regression, but yields still too many false negatives","be72f6f4":"#### There might be 2 tendencies: we managed to predict the outcome for old (>70 years) patients better. And also there seems to be a bad predictions for those who stayed for longer time","14bf16d6":"#### Already good. Let's try to tune the models","fe7d5b40":"### There are no obvious correlations - probably age plays a major role...","148fc538":"#### There is a remarkable difference in splits. So not that perfect","17a729db":"#### No spectacular differences. Let's plot the good and bad outcomes","caef3991":"### Let's see which rows were badly predicted","31c59440":"# Analysis of heart failure set - some data visualization and classification model\n\n#### Hi there,\n#### here I plotted some scatters to look \"visually\" for potential patterns in the dataset and then tried to find the best classification model","0be3bf5c":"#### Let's assess the probabilities. We are going to consider every probability more than 80% as a good prediction; less than 80% - as a bad one","2fd43284":"## Let's set a classifier then","d9257fb2":"### The correlation matrix shows that there is absolute no correlation between death event and sex and smoking. I decided to look at the subsets separately to check the correlations inside these groups"}}