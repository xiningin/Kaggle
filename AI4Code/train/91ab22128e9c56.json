{"cell_type":{"95af6e16":"code","d15c69f1":"code","45a32e6c":"code","dda94076":"code","e5286b48":"code","ec362740":"code","bd461dc7":"code","180ce4f8":"code","8835deb5":"code","2865d21a":"code","282e7b64":"code","b7eed45c":"code","6ebcb4ad":"code","ee8bd5c3":"code","66c82eab":"code","6d475936":"code","2b8543fa":"code","c36379f7":"code","d87c8ab8":"code","c8340368":"code","2f65d37b":"code","c5f68e47":"code","2d8ec190":"code","fbe62b3d":"code","693b7843":"code","ddf75b15":"code","929fce63":"code","5b1043f7":"code","60fffbc8":"code","297ee9bf":"code","6805c11d":"code","9876abcd":"code","ee037f1b":"code","f4762d30":"code","1ad7fe96":"code","8420b8a6":"code","cd1a0e04":"code","5143c199":"code","96cd6654":"code","cd051517":"code","5c5e28d0":"code","29145104":"markdown","6d3662c8":"markdown","06f77054":"markdown","1658cb5f":"markdown","d86244a9":"markdown","aa0a844d":"markdown","19c37c1c":"markdown","6bad0140":"markdown","2292adcb":"markdown","8afadb40":"markdown","28acb018":"markdown","89d36446":"markdown","e7359028":"markdown","68f527fa":"markdown","7da55fec":"markdown","5ca8a8e5":"markdown","9102a422":"markdown"},"source":{"95af6e16":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport json\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\nimport re\nfrom re import finditer\nimport spacy\nimport nltk, string\nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\n#python -m pip install covid19_tools\nimport covid19_tools as cvt\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom collections import defaultdict\nimport os\n\nnlp = spacy.load(\"en_core_web_sm\")  # load model package \"en_core_web_sm\"\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n   # for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Any results you write to the current directory are saved as output.","d15c69f1":"## Loading the Data\ncorona_df = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\ncorona_df.rename(columns={'sha':'paper_id'}, inplace=True) ## Renaming sha to paper_id\ncorona_df.head(3)","45a32e6c":"corona_df.shape #(51078, 18)","dda94076":"## Dropping Dulpicates and NA values from abstract column\ncorona_df.drop_duplicates(['abstract'], inplace=True)\ncorona_df.dropna(subset=['abstract'], inplace=True)\n\ncorona_df.shape","e5286b48":"df_corona = corona_df.drop(columns = ['paper_id', 'source_x', 'pmcid', 'license', 'Microsoft Academic Paper ID', \\\n                               'WHO #Covidence', 'has_pdf_parse', 'has_pmc_xml_parse', 'full_text_file'])\ndf_corona.shape","ec362740":"## Sub Tasks within the Tasks\nmedtasks_df = pd.DataFrame({'sub_tasks': ['Resources to support skilled nursing facilities and long term care facilities',\n    'Mobilization of surge medical staff to address shortages in overwhelmed communities',\n    'Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with\/without other organ failure \u2013 particularly \\\n      for viral etiologies',\n    'Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients',\n    'Outcomes data for COVID-19 after mechanical ventilation adjusted for age',\n    'Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not \\\n      limited to, possible cardiomyopathy and cardiac arrest',\n    'Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level',\n    'Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks',\n    'Best telemedicine practices, barriers and faciitators, and specific actions to remove\/expand them within and across state \\\n      boundaries',\n    'Guidance on the simple things people can do at home to take care of sick people and manage disease',\n    'Oral medications that might potentially work',\n    'Use of Artificial Intelligence AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in \\\n      a way that could not be done manually',\n    'Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce \\\n    protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity,\\\n      efficiency, and outcomes',\n    'Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention \\\n      control, transmission, and clinical trials',\n    'Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials',\n    'Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients \\\n     (e.g. steroids, high flow oxygen)',\n]})\nmedtasks_df","bd461dc7":"## Data Cleansing for Questions Asked\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\nstop_words = set(stopwords.words('english'))","180ce4f8":"## Converting to Lowercase\nmedtasks_df['sub_tasks_lower'] = medtasks_df['sub_tasks'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\nmedtasks_df","8835deb5":"## Remove Punctuations\nmedtasks_df['sub_tasks_lower'] = medtasks_df['sub_tasks_lower'].str.replace('[^\\w\\s]','')\nmedtasks_df","2865d21a":"## Stop Words in Questions and their Removal\nmedtasks_df['stopwords'] = medtasks_df['sub_tasks'].apply(lambda x: len([x for x in x.split() if x in stop_words]))\n\nmedtasks_df['sub_tasks_lower'] = medtasks_df['sub_tasks_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n\nmedtasks_df['stopwords_left'] = medtasks_df['sub_tasks_lower'].apply(lambda x: len([x for x in x.split() if x in stop_words]))\nmedtasks_df","282e7b64":"## Tokenization (dividing the text into a sequence of words or sentences) and \n## Lemmatization (converts the word into its root word, rather than just stripping the suffices (Stemming))\n\ndef tokenize_tasks(text):\n    return ' '.join(lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text))\n\nmedtasks_df['sub_tasks_lower'] = medtasks_df.sub_tasks_lower.apply(tokenize_tasks)\nmedtasks_df","b7eed45c":"## Dropping unnecessary Columns and Renaming Columns\ndf_medtasks = medtasks_df.drop(columns=['stopwords', 'stopwords_left'])\ndf_medtasks = df_medtasks.rename(columns={\"sub_tasks_lower\": \"sub_tasks_cleansed\"})\ndf_medtasks","6ebcb4ad":"## Reduce literature set to include those mentioning Covid 19 or its synonyms. \n## This is acheived using the method supplied in covid19-tools provided by Andy White. \n\n## Does paper discuss Covid-19, SARS, MERS, etc.? Looking for papers that specifically refer to the recent outbreak, \n## known variously as Covid-19, SARS-CoV-2, 2019-nCoV, Wuhan Pneumonia etc.\ncovid19_synonyms = ['covid','coronavirus disease 19','sars cov 2','2019 ncov','2019ncov',r'2019 n cov\\b', r'2019n cov\\b',\n                    'ncov 2019',r'\\bn cov 2019','coronavirus 2019','wuhan pneumonia','wuhan virus','wuhan coronavirus',\n                    r'coronavirus 2\\b']\n\n## Counts Synonyms and adds disease_covid19 column to DF\ndf_corona, covid_vals = cvt.count_and_tag(df_corona, covid19_synonyms, 'disease_covid19')\n\n## Check the Papers which discuss Covid\ndf_corona.tag_disease_covid19.value_counts()","ee8bd5c3":"## Filtering data where the disease_covid19 is True\ncovid_df = df_corona[df_corona['tag_disease_covid19'] == True ]\ncovid_df = covid_df.reset_index()\ncovid_df = covid_df.drop(['index'], axis=1)\ncovid_df","66c82eab":"## Displaying Counts of Covid and it's Synonyms\ncovid_vals.sort_values(ascending=False)","6d475936":"## Plotting Covid and it's Synonym Counts\nfig = go.Figure(data=go.Scatter(x=covid_vals.sort_values().index.values,\n                                y=covid_vals.sort_values(),\n                                mode='lines+markers',\n                                marker=dict(color=\"red\", size=12)\n))\n\nfig.update_layout(title='Count of Covid Synonyms', xaxis_title='Covid and Synonyms', yaxis_title='Count')\nfig.show()","2b8543fa":"## Breaking abstracts into sentences\ncovid_df['org abstract'] = covid_df['abstract']\ncovid_df_by_sentence = covid_df.set_index(covid_df.columns.drop('abstract',1).tolist())\\\n.abstract.str.split('\\. ', expand=True).stack().reset_index()\\\n.rename(columns={0:'abstract'})\ncovid_df_by_sentence","c36379f7":"## Converting to Lowercase\ncovid_df_by_sentence['abstract_lower'] = covid_df_by_sentence['abstract'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ncovid_df_by_sentence\n\n## Remove Punctuations\ncovid_df_by_sentence['abstract_lower'] = covid_df_by_sentence['abstract_lower'].str.replace('[^\\w\\s]','')\ncovid_df_by_sentence\n\n## Stop Words in Questions and their Removal\ncovid_df_by_sentence['stopwords'] = covid_df_by_sentence['abstract_lower'].apply(lambda x: len([x for x in x.split() if x in stop_words]))\n\ncovid_df_by_sentence['abstract_lower'] = covid_df_by_sentence['abstract_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n\ncovid_df_by_sentence['stopwords_left'] = covid_df_by_sentence['abstract_lower'].apply(lambda x: len([x for x in x.split() if x in stop_words]))\ncovid_df_by_sentence\n\n## Tokenization and Lemmatization \ncovid_df_by_sentence['abstract_lower'] = covid_df_by_sentence.abstract_lower.apply(tokenize_tasks)\ncovid_df_by_sentence","d87c8ab8":"## Dropping unnecessary Columns and Renaming Columns\nsentence_df = covid_df_by_sentence.drop(columns=['stopwords', 'stopwords_left'])\nsentence_df = sentence_df.rename(columns={\"abstract_lower\": \"abstract_cleansed\"})\nsentence_df","c8340368":"## Creating a complete combined list of questions and abstracts\nlst_complete = [\"\".join(x) for x in (df_medtasks['sub_tasks_cleansed'])]\nlst_complete\nlst_complete = lst_complete + [\"\".join(x) for x in (sentence_df['abstract_cleansed'])]\nlst_complete","2f65d37b":"## Embedding through USE(Universal Sentence Encoder) - a tensor flow utility\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")\nembeddings = embed(lst_complete)\n#print(embeddings)","c5f68e47":"## Semantic similarity of two sentences can be trivially computed as the inner product of the encodings\nimport seaborn as sns\ndef plot_similarity(labels, features, rotation):\n    corr = np.inner(features, features)\n    sns.set(font_scale=1.2)\n    g = sns.heatmap(\n      corr,\n      xticklabels=labels,\n      yticklabels=labels,\n      vmin=0,\n      vmax=1,\n      cmap=\"YlOrRd\")\n    g.set_xticklabels(labels, rotation=rotation)\n    g.set_title(\"Semantic Textual Similarity\")\n\nplot_similarity(lst_complete[0:5], embeddings[0:5], 90)","2d8ec190":"corr = np.inner(embeddings, embeddings)\ncorr","fbe62b3d":"leng = len(medtasks_df)\n#sentence_df\nsim_score_df = pd.DataFrame({'Similarity':corr[leng:,0], 'Abstract_per_Sent':sentence_df['abstract'],\\\n                             'Date':sentence_df['publish_time'],'Title':sentence_df['title'],\\\n                             'Authors':sentence_df['authors'],\\\n                             'Abstract':sentence_df['org abstract'],\\\n                             'URL':sentence_df['url']})\n#sim_score_df.head()","693b7843":"## Finding Articles for every question\ndef article_search (index,corr,sentence_df,leng):\n\n    sim_score_df = pd.DataFrame({'Similarity':corr[leng:,index], 'Abstract_per_Sent':sentence_df['abstract'],\\\n                             'Date':sentence_df['publish_time'],'Title':sentence_df['title'],'Authors':sentence_df['authors'],\\\n                             'Abstract':sentence_df['org abstract'],'URL':sentence_df['url']})\n    \n    df_sim_score = sim_score_df.sort_values('Similarity',ascending = False )\n\n    ## Top 50 articles\n    df_sim50 = df_sim_score[:10]\n\n    df_sim50 = df_sim50.reset_index()\n    \n    ## Dropping unnecessary Columns\n    df_sim50 = df_sim50.drop(['index'], axis=1)\n    df_sim50 = df_sim50.drop(['Similarity'], axis=1)\n\n    df_sim50 = df_sim50.apply(lambda x: x.str.slice(0, 1000))\n    df_sim50[\"Authors\"] = df_sim50[\"Authors\"].str[:100]\n    \n    return df_sim50","ddf75b15":"ques1_Ans = article_search (0,corr,sentence_df,len(medtasks_df))\n#ques1_Ans\nques1_Ans = ques1_Ans.style.set_properties(**{'text-align': 'left'})\nques1_Ans","929fce63":"ques2_Ans = article_search (1,corr,sentence_df,len(medtasks_df))\n#ques2_Ans\nques2_Ans = ques2_Ans.style.set_properties(**{'text-align': 'left'})\nques2_Ans","5b1043f7":"ques3_Ans = article_search (2,corr,sentence_df,len(medtasks_df))\n#ques3_Ans\nques3_Ans = ques3_Ans.style.set_properties(**{'text-align': 'left'})\nques3_Ans","60fffbc8":"ques4_Ans = article_search (3,corr,sentence_df,len(medtasks_df))\n#ques4_Ans\nques4_Ans = ques4_Ans.style.set_properties(**{'text-align': 'left'})\nques4_Ans","297ee9bf":"ques5_Ans = article_search (4,corr,sentence_df,len(medtasks_df))\n#ques5_Ans\nques5_Ans = ques5_Ans.style.set_properties(**{'text-align': 'left'})\nques5_Ans","6805c11d":"ques6_Ans = article_search (5,corr,sentence_df,len(medtasks_df))\n#ques6_Ans\nques6_Ans = ques6_Ans.style.set_properties(**{'text-align': 'left'})\nques6_Ans","9876abcd":"ques7_Ans = article_search (6,corr,sentence_df,len(medtasks_df))\n#ques7_Ans\nques7_Ans = ques7_Ans.style.set_properties(**{'text-align': 'left'})\nques7_Ans","ee037f1b":"ques8_Ans = article_search (7,corr,sentence_df,len(medtasks_df))\n#ques8_Ans\nques8_Ans = ques8_Ans.style.set_properties(**{'text-align': 'left'})\nques8_Ans","f4762d30":"ques9_Ans = article_search (8,corr,sentence_df,len(medtasks_df))\n#ques9_Ans\nques9_Ans = ques9_Ans.style.set_properties(**{'text-align': 'left'})\nques9_Ans","1ad7fe96":"ques10_Ans = article_search (9,corr,sentence_df,len(medtasks_df))\n#ques10_Ans\nques10_Ans = ques10_Ans.style.set_properties(**{'text-align': 'left'})\nques10_Ans","8420b8a6":"ques11_Ans = article_search (10,corr,sentence_df,len(medtasks_df))\n#ques11_Ans\nques11_Ans = ques11_Ans.style.set_properties(**{'text-align': 'left'})\nques11_Ans","cd1a0e04":"ques12_Ans = article_search (11,corr,sentence_df,len(medtasks_df))\n#ques12_Ans\nques12_Ans = ques12_Ans.style.set_properties(**{'text-align': 'left'})\nques12_Ans","5143c199":"ques13_Ans = article_search (12,corr,sentence_df,len(medtasks_df))\n#ques13_Ans\nques13_Ans = ques13_Ans.style.set_properties(**{'text-align': 'left'})\nques13_Ans","96cd6654":"ques14_Ans = article_search (13,corr,sentence_df,len(medtasks_df))\n#ques14_Ans\nques14_Ans = ques14_Ans.style.set_properties(**{'text-align': 'left'})\nques14_Ans","cd051517":"ques15_Ans = article_search (14,corr,sentence_df,len(medtasks_df))\n#ques15_Ans\nques15_Ans = ques15_Ans.style.set_properties(**{'text-align': 'left'})\nques15_Ans","5c5e28d0":"ques16_Ans = article_search (15,corr,sentence_df,len(medtasks_df))\n#ques16_Ans\nques16_Ans = ques16_Ans.style.set_properties(**{'text-align': 'left'})\nques16_Ans","29145104":"## 10. Guidance on the simple things people can do at home to take care of sick people and manage disease","6d3662c8":"## 7. Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level","06f77054":"## 15. Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials","1658cb5f":"## 9. Best telemedicine practices, barriers and faciitators, and specific actions to remove\/expand them within and across state boundaries","d86244a9":"## 1. Resources to support skilled nursing facilities and long term care facilities","aa0a844d":"## 8. Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks","19c37c1c":"## 3. Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with\/without other organ failure \u2013 particularly for viral etiologies","6bad0140":"## 11. Oral medications that might potentially work","2292adcb":"## COVID-19 : What has been published about medical care?\n    -- Submitted By: Pradeep Joshi\n    \n### Top 10 articles for each sub question have been published    ","8afadb40":"## 2. Mobilization of surge medical staff to address shortages in overwhelmed communities","28acb018":"## 16. Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)","89d36446":"## 14. Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials","e7359028":"## 12. Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually","68f527fa":"## 13. Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes","7da55fec":"## 6. Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest","5ca8a8e5":"## 4. Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients","9102a422":"## 5. Outcomes data for COVID-19 after mechanical ventilation adjusted for age"}}