{"cell_type":{"ff4adeef":"code","04fb518c":"code","aabefe01":"code","70571954":"code","8e3815f8":"code","2a9674d6":"code","9eef0917":"code","ba85f537":"code","886ea6b2":"code","ea158e43":"code","281aee0f":"code","f72c7091":"code","a20b7602":"code","5af12918":"code","e185d7e7":"code","53686e6b":"code","a40a757f":"code","8ab4411a":"code","d2239337":"code","ca169d0a":"code","3459df65":"code","28c4ff80":"code","9c42dce2":"code","866e9956":"code","4e05cd6b":"code","e8fc07e7":"code","9b64674d":"code","ba6a0541":"code","c47c4886":"code","97ce7b89":"code","15aa30fc":"code","29fdce4c":"code","e7da6481":"code","8790d3a7":"code","6621267d":"code","e754c824":"code","22db3720":"code","db34b284":"code","96cf94bb":"code","6765b3fa":"code","35bf3ee4":"code","2eff9f51":"code","43b16710":"code","387d7fff":"code","74f39b70":"code","b19713c4":"code","b7c311f2":"code","10cb927d":"code","ae4a7ff0":"code","3ae03615":"code","0b082ef0":"code","a5523f26":"code","9cfae958":"code","8b2fe5a8":"code","77b218ed":"code","9a303bb8":"markdown","92666de3":"markdown","ef4025b9":"markdown","443e1919":"markdown","ea0cbd4a":"markdown","1f5b7f6d":"markdown","e33f429e":"markdown","ced69e71":"markdown","bf88a3ca":"markdown","4cf0f7b1":"markdown","791d7194":"markdown","a0fc6443":"markdown","6d19c4e4":"markdown","61debc8b":"markdown","ce7d2f53":"markdown"},"source":{"ff4adeef":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","04fb518c":"import seaborn as sns","aabefe01":"from sklearn.model_selection import train_test_split","70571954":"from sklearn.ensemble import IsolationForest","8e3815f8":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","2a9674d6":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler","9eef0917":"from sklearn.model_selection import GridSearchCV","ba85f537":"from sklearn.ensemble import RandomForestClassifier","886ea6b2":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier","ea158e43":"from sklearn.metrics import accuracy_score, recall_score, precision_score","281aee0f":"from sklearn.metrics import roc_auc_score, plot_roc_curve, precision_recall_curve, auc, f1_score, plot_confusion_matrix","f72c7091":"from imblearn.over_sampling import SMOTE","a20b7602":"from imblearn.pipeline import Pipeline as ImbalancedPipeline","5af12918":"from imblearn.over_sampling import RandomOverSampler","e185d7e7":"df = pd.read_csv(\"..\/input\/iba-ml1-mid-project\/train.csv\")\ndf.head()","53686e6b":"df = df.iloc[:,1:]\ndf.head(10)","a40a757f":"df['credit_line_utilization'] = pd.to_numeric(df['credit_line_utilization'], errors='coerce')","8ab4411a":"df['defaulted_on_loan'].value_counts()","d2239337":"df.describe()","ca169d0a":"df['log_monthly_income'] = np.log(df.loc[:, 'monthly_income'])","3459df65":"(df['log_monthly_income'] == np.NINF).value_counts()","28c4ff80":"df.loc[:, 'log_monthly_income'].replace(np.NINF, 0,inplace=True)\ndf.head()","9c42dce2":"df['log_number_of_credit_lines'] = np.log(df['number_of_credit_lines'])\ndf.loc[:, 'log_number_of_credit_lines'].replace(np.NINF, 0,inplace=True)","866e9956":"# A copy of our data with the necessary features\n\ndf_cpy = df[['age', 'number_dependent_family_members', 'log_monthly_income', 'log_number_of_credit_lines', 'real_estate_loans',\n             'ratio_debt_payment_to_income', 'credit_line_utilization', 'number_of_previous_late_payments_up_to_59_days',\n            'number_of_previous_late_payments_up_to_89_days', 'number_of_previous_late_payments_90_days_or_more', 'defaulted_on_loan']]","4e05cd6b":"test_df = pd.read_csv(\"..\/input\/iba-ml1-mid-project\/test.csv\")\ntest_id = test_df['Id']\ntest_df = test_df.iloc[:, 1:]\ntest_df.head()","e8fc07e7":"test_df['credit_line_utilization'] = pd.to_numeric(test_df['credit_line_utilization'], errors='coerce')","9b64674d":"test_df['log_monthly_income'] = np.log(test_df['monthly_income'])\ntest_df.loc[:, 'log_monthly_income'].replace(np.NINF, 0,inplace=True)","ba6a0541":"test_df['log_number_of_credit_lines'] = np.log(test_df['number_of_credit_lines'])\ntest_df.loc[:, 'log_number_of_credit_lines'].replace(np.NINF, 0,inplace=True)","c47c4886":"test_df_cpy = test_df[['age', 'number_dependent_family_members', 'log_monthly_income', 'log_number_of_credit_lines', 'real_estate_loans',\n             'ratio_debt_payment_to_income', 'credit_line_utilization', 'number_of_previous_late_payments_up_to_59_days',\n            'number_of_previous_late_payments_up_to_89_days', 'number_of_previous_late_payments_90_days_or_more']]","97ce7b89":"X = df_cpy.iloc[:, :-1]\nX.head()","15aa30fc":"y = df_cpy.iloc[:, -1]\ny.head()","29fdce4c":"over_sampler = RandomOverSampler(sampling_strategy='minority', random_state=0)","e7da6481":"# With Outlier Detection\n\nforest_pipeline_OD = Pipeline(steps=[\n    ('classifier', RandomForestClassifier(random_state=0))\n])","8790d3a7":"forest_pipeline_OD.get_params().keys()","6621267d":"# With Outlier Detection\n\nforest_param_space_OD = {\n    'classifier__max_depth': [3, 6, 9],\n    'classifier__n_estimators': [50, 150, 300],\n    'classifier__criterion': ['entropy', 'gini'],\n    'classifier__max_leaf_nodes': [4, 7, 10],\n    'classifier__n_jobs': [5]\n}","e754c824":"OD_imputer = SimpleImputer(strategy='most_frequent')","22db3720":"def forest_splits_OD(X, y, split_ratios, OD_contamination):\n    \n    params = {'model' : {},\n              'accuracy' : {},\n              'recall' : {},\n              'precision' : {},\n              'f1_score' : {},\n              'roc_auc_score' : {},\n              'pr_auc' : {}}\n    \n    for split_ratio in split_ratios:\n\n        fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(15,8))       \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split_ratio,\n                                                            random_state = 0)\n        \n        X_train = OD_imputer.fit_transform(X_train)\n        X_test = OD_imputer.fit_transform(X_test)\n        \n        iso = IsolationForest(contamination=OD_contamination, random_state=0)\n        y_hat = iso.fit_predict(X_train)\n        \n        mask = y_hat != -1\n        \n        X_train, y_train = X_train[mask], y_train[mask]\n\n        X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n        \n        X_train = pd.DataFrame(X_train)\n        \n        X_train.columns = list(X.columns)\n        \n        gridsearch = GridSearchCV(forest_pipeline_OD, forest_param_space_OD,\n                                  cv=5, )\n        gridsearch.fit(X_train, y_train)\n        \n        params['model'][split_ratio] = gridsearch\n        \n        y_pred = gridsearch.predict(X_test)\n        \n        precision = precision_score(y_test, y_pred)\n        params['precision'][split_ratio] = precision\n        \n        recall = recall_score(y_test, y_pred)\n        params['recall'][split_ratio] = recall\n                \n        accuracy = accuracy_score(y_test, y_pred)\n        params['accuracy'][split_ratio] = accuracy\n        \n        f1 = f1_score(y_test, y_pred)\n        params['f1_score'][split_ratio] = f1\n\n        roc_score = roc_auc_score(y_test, y_pred)\n        params['roc_auc_score'][split_ratio] = roc_score\n        \n        curve_precision, curve_recall, res = precision_recall_curve(y_test, y_pred)\n        \n        pr_auc = auc(curve_recall, curve_precision)\n        params['pr_auc'][split_ratio] = pr_auc\n        \n        \n        print(\"\\n>>> At split ratio \", split_ratio)\n        print(\"accuracy: \", accuracy)\n        print(\"ROC_AUC: \", roc_score)\n        print(\"Precision: \", precision)\n        print(\"Recall: \", recall)\n        print(\"PR AUC: \", pr_auc)\n        print(\"F1 score: \", f1)\n        print(\"Detected best params: \", gridsearch.best_params_)\n        \n        plot_confusion_matrix(gridsearch, X_test, y_test, ax=ax1)\n        plot_roc_curve(gridsearch, X_test, y_test, ax=ax2)\n        ax2.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10), '--', label = 'no skill classifier')\n        ax2.legend()\n        ax2.set_title(\"Split Ratio = {}\".format(split_ratio))\n        \n    plt.show()\n    \n    params = pd.DataFrame(params)\n    \n    return params","db34b284":"# With Outlier Detection\n\nforest_param_space_OD = {\n    'classifier__max_depth': [10],\n    'classifier__n_estimators': [200],\n    'classifier__criterion': ['gini'],\n    'classifier__max_leaf_nodes': [25],\n    'classifier__n_jobs': [5]\n}","96cf94bb":"%%time\n\nforests_OD_10 = forest_splits_OD(X, y, [0.1, 0.2, 0.3], 0.1)","6765b3fa":"forests_OD_10","35bf3ee4":"fig, ax = plt.subplots(figsize=(12,6))\n\nforests_OD_10.iloc[:, 1:].plot(kind='barh', ax=ax)\nax.grid()\nplt.show()","2eff9f51":"feature_trfm = Pipeline(steps=[\n    ('imputer', SimpleImputer()),\n    ('scaler', MinMaxScaler())\n])\n\ncol_trfm = ColumnTransformer(transformers=[\n    ('numeric', feature_trfm, list(X.columns))\n])","43b16710":"HistGradientBoost_pipeline = Pipeline(steps=[\n    ('preprocessing', col_trfm),\n    ('classifier', HistGradientBoostingClassifier(random_state=0))\n])","387d7fff":"HistGradientBoost_pipeline.get_params().keys()","74f39b70":"HistGradientBoost_param_space = {\n    'preprocessing__numeric__imputer__strategy': ['median'],\n    'classifier__max_depth': [10],\n    'classifier__max_leaf_nodes': [11]\n}","b19713c4":"HistBoost_imputer = SimpleImputer(strategy='median')","b7c311f2":"def HistGradientBoost_splits(X, y, split_ratios, OD_contamination):\n    \n    params = {'model' : {},\n              'accuracy' : {},\n              'recall' : {},\n              'precision' : {},\n              'f1_score' : {},\n              'roc_auc_score' : {},\n              'pr_auc' : {}}\n    \n    for split_ratio in split_ratios:\n\n        fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(15,8))       \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split_ratio,\n                                                            random_state = 0)\n        \n#         X_train = HistBoost_imputer.fit_transform(X_train)\n#         X_test = HistBoost_imputer.fit_transform(X_test)\n        \n#         iso = IsolationForest(contamination=OD_contamination, random_state=0)\n#         y_hat = iso.fit_predict(X_train)\n        \n#         mask = y_hat != -1\n        \n#         X_train, y_train = X_train[mask], y_train[mask]\n\n        X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n        \n        X_train = pd.DataFrame(X_train)\n        \n        X_train.columns = list(X.columns)\n        \n        gridsearch = GridSearchCV(HistGradientBoost_pipeline, HistGradientBoost_param_space,\n                                  cv=5, )\n        gridsearch.fit(X_train, y_train)\n        \n        params['model'][split_ratio] = gridsearch\n        \n        y_pred = gridsearch.predict(X_test)\n        \n        precision = precision_score(y_test, y_pred)\n        params['precision'][split_ratio] = precision\n        \n        recall = recall_score(y_test, y_pred)\n        params['recall'][split_ratio] = recall\n                \n        accuracy = accuracy_score(y_test, y_pred)\n        params['accuracy'][split_ratio] = accuracy\n        \n        f1 = f1_score(y_test, y_pred)\n        params['f1_score'][split_ratio] = f1\n\n        roc_score = roc_auc_score(y_test, y_pred)\n        params['roc_auc_score'][split_ratio] = roc_score\n        \n        curve_precision, curve_recall, res = precision_recall_curve(y_test, y_pred)\n        \n        pr_auc = auc(curve_recall, curve_precision)\n        params['pr_auc'][split_ratio] = pr_auc\n        \n        \n        print(\"\\n>>> At split ratio \", split_ratio)\n        print(\"accuracy: \", accuracy)\n        print(\"ROC_AUC: \", roc_score)\n        print(\"Precision: \", precision)\n        print(\"Recall: \", recall)\n        print(\"PR AUC: \", pr_auc)\n        print(\"F1 score: \", f1)\n        print(\"Detected best params: \", gridsearch.best_params_)\n        \n        plot_confusion_matrix(gridsearch, X_test, y_test, ax=ax1)\n        plot_roc_curve(gridsearch, X_test, y_test, ax=ax2)\n        ax2.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10), '--', label = 'no skill classifier')\n        ax2.legend()\n        ax2.set_title(\"Split Ratio = {}\".format(split_ratio))\n        \n    plt.show()\n    \n    params = pd.DataFrame(params)\n    \n    return params","10cb927d":"%%time\n\nHistGradientBoost_1 = HistGradientBoost_splits(X, y, [0.1, 0.2, 0.3], 0.1)","ae4a7ff0":"HistGradientBoost_1","3ae03615":"imputer = SimpleImputer(strategy='median')\ntest_df_processed = OD_imputer.fit_transform(test_df_cpy)","0b082ef0":"test_df_processed = pd.DataFrame(test_df_processed)\ntest_df_processed.columns = test_df_cpy.columns\ntest_df_processed.head()","a5523f26":"RF_test_pred_OD10 = pd.concat([test_id, pd.Series(forests_OD_10.loc[0.2, 'model'].predict_proba(test_df_processed)[:, -1])], axis=1)\nRF_test_pred_OD10.columns = ['Id', 'Predicted']\nRF_test_pred_OD10.head()","9cfae958":"HistGradientBoost_OS_OD_1 = pd.concat([test_id, pd.Series(HistGradientBoost_1.loc[0.2, 'model'].predict_proba(test_df_cpy)[:, -1])], axis=1)\nHistGradientBoost_OS_OD_1.columns = ['Id', 'Predicted']\nHistGradientBoost_OS_OD_1.head()","8b2fe5a8":"HistGradientBoost_OS_OD_1.iloc[:, -1].hist()","77b218ed":"RF_test_pred_OD10.iloc[:, -1].hist()","9a303bb8":"All of the models implement the below:\n- Missing Value Imputation\n- MinMax Scaling\n- Random Oversampling\n- Prediction\n\nFor RandomForests, we also have:\n- Outlier Detection using IsolationForests\n","92666de3":"### Testing our model on the test set and saving the results","ef4025b9":"# Model Fitting","443e1919":"# Fetching the Data","ea0cbd4a":"# Conclusions on the Model ","1f5b7f6d":"## Random Forests","e33f429e":"#### Random Forests","ced69e71":"# Importing the Libraries","bf88a3ca":"It seems like we can deal with the skewness of the 'number_of_credit_lines' feature like we did in the case of 'monthly_income'. Let's see how it goes","4cf0f7b1":"The metrics used as a tool of comparison were a comparison of which has been provided in the \"Model Fitting\" section:\n- Accuracy Score\n- Precision\n- Recall\n- ROC_AUC\n- F1 Score","791d7194":"Based on these metrics, the models with the best performing sets of parametres were chosen of each type of model mentioned previously and were juxtaposed against each other. At this point non-ensemble and bagging (Voted K-means) algorithms were eliminated alongside XGB. The remaining two models are the predictors the parameters of which are elaborated on above. Among these the best performing predictor in the Leader Board was the HistGradientBoost based model","a0fc6443":"During my testing different models of possible usefulness were tested out: LogisticRegression, DecisionTrees, Voted K-means, XGB, RandomForests and one of the in-development models of sklearn called HistGradientBoost which is a model developed mostly for the cases of continous datapoints with a relatively simpler multi-feature hyperplane distribution that predict a label. Through rigorous testing using different wide ranges of parametre spaces and test\/train ratios the two best performing models were:\n- RandomForests with:\n    - max_depth: 10\n    - n_estimators: 200\n    - criterion: gini\n    - max_leaf_nodes: 25,\n- HistGradientBoost:\n    - imputer__strategy: median,\n    - max_depth: 10\n    - max_leaf_nodes: 11","6d19c4e4":"## HistGradientBoostingClassifier","61debc8b":"## Preprocessing the Test Data","ce7d2f53":"#### Visual Inspection"}}