{"cell_type":{"23dd29cc":"code","75115f45":"code","3102bd64":"code","611b20d0":"code","1240bf43":"code","2ba3819b":"code","2c46e5f3":"code","4e92f49b":"code","08858101":"code","27a66486":"code","54b05dce":"code","39e00b8b":"code","0c48df62":"code","b9d390f1":"code","a68ed8c7":"code","1693cb78":"code","b831464f":"code","23415c62":"code","efb3f6cb":"code","24725a3d":"code","5b088968":"code","d6e6a61b":"code","88ba3aeb":"code","4c9966ad":"code","c65d0b23":"code","2e3c697b":"markdown","6f2cf10d":"markdown","d9073862":"markdown","11501dbc":"markdown","22eaf114":"markdown"},"source":{"23dd29cc":"## Making essential imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\n\n\n## defining a frame for image and mask storage\nframObjTrain = {'img' : [],\n           'mask' : []\n          }\n\n## defining data Loader function\ndef LoadData( frameObj = None, imgPath = None, maskPath = None, shape = 256):\n    imgNames = os.listdir(imgPath)\n    maskNames = []\n    \n\n    \n    imgAddr = imgPath + '\/'\n    maskAddr = maskPath + '\/'\n    \n    for i in range (len(imgNames)):\n        img = plt.imread(imgAddr + imgNames[i])\n        mask = plt.imread(maskAddr + imgNames[i])\n        \n        img = cv2.resize(img, (shape, shape)) \n        mask = cv2.resize(mask, (shape, shape))\n        \n        frameObj['img'].append(img[:,:,0:3])\n        frameObj['mask'].append(mask[:,:,0:3])\n        \n    return frameObj","75115f45":"framObjTrain = LoadData( framObjTrain, imgPath = '\/kaggle\/input\/paired-bald-and-not-bald-portrait-images\/Paired_Images_Bald_Not_Bald\/train_bald'\n                        , maskPath = '\/kaggle\/input\/paired-bald-and-not-bald-portrait-images\/Paired_Images_Bald_Not_Bald\/train_hair'\n                         , shape = 256)","3102bd64":"## displaying data loaded by our function\nplt.figure(figsize = (10, 8))\nplt.subplot(1,2,1)\nplt.imshow(framObjTrain['img'][1])\nplt.subplot(1,2,2)\nplt.imshow(framObjTrain['mask'][1])\nplt.show()","611b20d0":"# Defining our network now\n# this block essentially performs 2 convolution\n\ndef Conv2dBlock(inputTensor, numFilters, kernelSize = 3, doBatchNorm = True):\n    #first Conv\n    x = tf.keras.layers.Conv2D(filters = numFilters, kernel_size = (kernelSize, kernelSize),\n                              kernel_initializer = 'he_normal', padding = 'same') (inputTensor)\n    \n    if doBatchNorm:\n        x = tf.keras.layers.BatchNormalization()(x)\n        \n    x =tf.keras.layers.Activation('relu')(x)\n    \n    #Second Conv\n    x = tf.keras.layers.Conv2D(filters = numFilters, kernel_size = (kernelSize, kernelSize),\n                              kernel_initializer = 'he_normal', padding = 'same') (x)\n    if doBatchNorm:\n        x = tf.keras.layers.BatchNormalization()(x)\n        \n    x = tf.keras.layers.Activation('relu')(x)\n    \n    return x\n\n\n# Now defining Unet \ndef GiveMeUnet(inputImage, numFilters = 16, droupouts = 0.1, doBatchNorm = True):\n    # defining encoder Path\n    c1 = Conv2dBlock(inputImage, numFilters * 1, kernelSize = 3, doBatchNorm = doBatchNorm)\n    p1 = tf.keras.layers.MaxPooling2D((2,2))(c1)\n    p1 = tf.keras.layers.Dropout(droupouts)(p1)\n    \n    c2 = Conv2dBlock(p1, numFilters * 2, kernelSize = 3, doBatchNorm = doBatchNorm)\n    p2 = tf.keras.layers.MaxPooling2D((2,2))(c2)\n    p2 = tf.keras.layers.Dropout(droupouts)(p2)\n    \n    c3 = Conv2dBlock(p2, numFilters * 4, kernelSize = 3, doBatchNorm = doBatchNorm)\n    p3 = tf.keras.layers.MaxPooling2D((2,2))(c3)\n    p3 = tf.keras.layers.Dropout(droupouts)(p3)\n    \n    c4 = Conv2dBlock(p3, numFilters * 8, kernelSize = 3, doBatchNorm = doBatchNorm)\n    p4 = tf.keras.layers.MaxPooling2D((2,2))(c4)\n    p4 = tf.keras.layers.Dropout(droupouts)(p4)\n    \n    c5 = Conv2dBlock(p4, numFilters * 16, kernelSize = 3, doBatchNorm = doBatchNorm)\n    \n    # defining decoder path\n    u6 = tf.keras.layers.Conv2DTranspose(numFilters*8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n    \n    u6 = tf.keras.layers.concatenate([u6, c4])\n    u6 = tf.keras.layers.Dropout(droupouts)(u6)\n    c6 = Conv2dBlock(u6, numFilters * 8, kernelSize = 3, doBatchNorm = doBatchNorm)\n    \n    u7 = tf.keras.layers.Conv2DTranspose(numFilters*4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n    \n    u7 = tf.keras.layers.concatenate([u7, c3])\n    u7 = tf.keras.layers.Dropout(droupouts)(u7)\n    c7 = Conv2dBlock(u7, numFilters * 4, kernelSize = 3, doBatchNorm = doBatchNorm)\n    \n    u8 = tf.keras.layers.Conv2DTranspose(numFilters*2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n    u8 = tf.keras.layers.concatenate([u8, c2])\n    u8 = tf.keras.layers.Dropout(droupouts)(u8)\n    c8 = Conv2dBlock(u8, numFilters * 2, kernelSize = 3, doBatchNorm = doBatchNorm)\n    \n    u9 = tf.keras.layers.Conv2DTranspose(numFilters*1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n    u9 = tf.keras.layers.concatenate([u9, c1])\n    u9 = tf.keras.layers.Dropout(droupouts)(u9)\n    c9 = Conv2dBlock(u9, numFilters * 1, kernelSize = 3, doBatchNorm = doBatchNorm)\n    \n    output = tf.keras.layers.Conv2D(3, (1, 1), activation = 'sigmoid')(c9)\n    model = tf.keras.Model(inputs = [inputImage], outputs = [output])\n    return model","1240bf43":"## instanctiating model\ninputs = tf.keras.layers.Input((256, 256, 3))\nmyTransformer = GiveMeUnet(inputs, droupouts= 0.07)\nmyTransformer.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'] )","2ba3819b":"## trainign our model\nretVal = myTransformer.fit(np.array(framObjTrain['img']), np.array(framObjTrain['mask']), epochs = 400, verbose = 0)","2c46e5f3":"plt.plot(retVal.history['loss'], label = 'training_loss')\nplt.plot(retVal.history['accuracy'], label = 'training_accuracy')\nplt.legend()\nplt.grid(True)","4e92f49b":"## function for getting 16 predictions\ndef predict16 (valMap, model, shape = 256):\n    ## getting and proccessing val data\n    img = valMap['img']\n    mask = valMap['mask']\n    mask = mask[0:16]\n    \n    imgProc = img [0:16]\n    imgProc = np.array(img)\n    \n    predictions = model.predict(imgProc)\n    for i in range(len(predictions)):\n        predictions[i] = cv2.merge((predictions[i,:,:,0],predictions[i,:,:,1],predictions[i,:,:,2]))\n    \n    return predictions, imgProc, mask\n\ndef Plotter(img, predMask, groundTruth):\n    plt.figure(figsize=(20,10))\n    \n    plt.subplot(1,3,1)\n    plt.imshow(img)\n    plt.title('Image without hair')\n    \n    ## Adding Image sharpening step here\n    ## it is a sharpening filter\n    filter = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]) \n    imgSharpen = cv2.filter2D(predMask,-1,filter)\n    \n    plt.subplot(1,3,2)\n    plt.imshow(imgSharpen)\n    plt.title('Predicted Image with Hair')\n    \n    plt.subplot(1,3,3)\n    plt.imshow(groundTruth)\n    plt.title('actual Image with Hair')\n","08858101":"sixteenPrediction, actuals, masks = predict16(framObjTrain, myTransformer)\nPlotter(actuals[2], sixteenPrediction[2], masks[2])","27a66486":"Plotter(actuals[3], sixteenPrediction[3], masks[3])","54b05dce":"Plotter(actuals[4], sixteenPrediction[4], masks[4])","39e00b8b":"Plotter(actuals[5], sixteenPrediction[5], masks[5])","0c48df62":"Plotter(actuals[7], sixteenPrediction[7], masks[7])\n","b9d390f1":"Plotter(actuals[8], sixteenPrediction[8], masks[8])","a68ed8c7":"Plotter(actuals[10], sixteenPrediction[10], masks[10])","1693cb78":"Plotter(actuals[12], sixteenPrediction[12], masks[12])","b831464f":"framObjTrain = {'img' : [],\n           'mask' : []\n          }\n\nframObjTrain = LoadData( framObjTrain, imgPath = '\/kaggle\/input\/paired-bald-and-not-bald-portrait-images\/Paired_Images_Bald_Not_Bald\/train_hair'\n                        , maskPath = '\/kaggle\/input\/paired-bald-and-not-bald-portrait-images\/Paired_Images_Bald_Not_Bald\/train_bald'\n                         , shape = 256)\n\n## instanctiating model\ninputs = tf.keras.layers.Input((256, 256, 3))\nmyTransformer = GiveMeUnet(inputs, droupouts= 0.07)\nmyTransformer.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'] )\n\n## trainign our model\nretVal = myTransformer.fit(np.array(framObjTrain['img']), np.array(framObjTrain['mask']), epochs = 400, verbose = 0)","23415c62":"plt.plot(retVal.history['loss'], label = 'training_loss')\nplt.plot(retVal.history['accuracy'], label = 'training_accuracy')\nplt.legend()\nplt.grid(True)","efb3f6cb":"\ndef Plotter(img, predMask, groundTruth):\n    plt.figure(figsize=(20,10))\n    \n    plt.subplot(1,3,1)\n    plt.imshow(img)\n    plt.title('Image with hair')\n    \n    ## Adding Image sharpening step here\n    ## it is a sharpening filter\n    filter = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]) \n    imgSharpen = cv2.filter2D(predMask,-1,filter)\n    \n    plt.subplot(1,3,2)\n    plt.imshow(imgSharpen)\n    plt.title('Predicted Image without Hair')\n    \n    plt.subplot(1,3,3)\n    plt.imshow(groundTruth)\n    plt.title('actual Image without Hair')\n","24725a3d":"sixteenPrediction, actuals, masks = predict16(framObjTrain, myTransformer)\nPlotter(actuals[0], sixteenPrediction[0], masks[0])","5b088968":"Plotter(actuals[1], sixteenPrediction[1], masks[1])","d6e6a61b":"Plotter(actuals[2], sixteenPrediction[2], masks[2])","88ba3aeb":"Plotter(actuals[3], sixteenPrediction[3], masks[3])","4c9966ad":"Plotter(actuals[4], sixteenPrediction[4], masks[4])","c65d0b23":"Plotter(actuals[5], sixteenPrediction[5], masks[5])","2e3c697b":"# Introduction\nHi everyone bringing you another notebook on image transformation. This transformer will help you grow your hair backs, if you have lost them. We will be using modified Unet for this purpouse,I have already made many such applications, the only difference between them and this one is the explanation. Hope you will find this notebook useful.\n\n\n![](https:\/\/miro.medium.com\/max\/5998\/1*eKrh8FqJL3jodebYlielNg.png)","6f2cf10d":"Let's just say this works....\nAny how this is how image transformers can be built, lets try one more thing ie removing info from an image.","d9073862":"# The Idea behind:\nEncoder-Decoder (Auto-Encoders) is class of neural-networks, that can be very easily made to act image transformers, the idea behind is that the encoder network deconstructs the image forming a thought\/latent tensor, the decoder network than reconstructs the image from the latent\/thought vector in a way that satifies our need. Notice the interconnections between encoder and decoder layers, they help in reducing the feature loss along the way.","11501dbc":"The above function is simply for loading our data, nothing to fancy here.","22eaf114":"This is what we manage to do with 135 images, hopefully with more data we can improve much more.\nThanks.."}}