{"cell_type":{"e19c2626":"code","3827215d":"code","45356551":"code","9c0f0613":"code","a2c5c5b9":"code","1bd3b260":"code","0ea3b1b9":"code","0e126a33":"code","e5111ecb":"code","997e3d00":"code","ea9b5281":"code","822fa717":"code","c9e6ee6a":"code","6efb0c0e":"code","ab6b33de":"code","685ede91":"code","42a65dba":"markdown","2c1bf299":"markdown","07a8c0ff":"markdown","9849c1a0":"markdown","8ff39f20":"markdown","4753ec61":"markdown","029305bd":"markdown","201353be":"markdown","4d0fc091":"markdown","367d9b5b":"markdown","65f8275f":"markdown","e5267713":"markdown","035231a8":"markdown","38aa2503":"markdown","ab649442":"markdown","48dff6b3":"markdown","1c8a8ec0":"markdown","e67b67cc":"markdown","79514647":"markdown","beaf24db":"markdown","ce70c708":"markdown","caf55199":"markdown","bd00c622":"markdown","9887f4f6":"markdown"},"source":{"e19c2626":"# uncheck this cell to install the needed modules\n# !pip install pandas as pd #data structure\n# !pip install numpy as np #numerical computing\n# !pip install matplotlib.pyplot as plt #matlab based plotting\n# !pip install seaborn as sns #more pretty visulzation\n# !pip install warnings #warning messages eliminating\n!pip install dython #data analysis tools for python 3.x\n# !pip install math #mathematical functions\n# !pip install catboost #gradient boosting\n# !pip install tensorflow #keras backend\n!pip install scipy #math operations\n# !pip install graphviz #decision tree visualzations\n!pip install pydotplus #convert graphviz viz from svg to png\nimport warnings\nwarnings.filterwarnings('ignore')","3827215d":"#import modules\nimport pandas as pd #data structure\nimport numpy as np #numerical computing\nimport matplotlib.pyplot as plt #matlab based plotting\nimport seaborn as sns #more pretty visulzation\nimport dython #data analysis tools for python 3.x\nimport catboost #gradient boosting\nimport tensorflow #keras backend\nimport scipy.stats as ss #math operations\nimport pydotplus\nimport graphviz #decision tree visualzations\n#configurations#\n# %autosave 60\n%matplotlib inline\n# %config InlineBackend.figure_format ='retina'\nimport datetime\nprint('Last update on the nootebook was: \\n', datetime.datetime.now())","45356551":"# if you read data from local path\n# df = pd.read_csv('\/content\/mushrooms.csv')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\ndf = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\nprint('First 5 rows of all columns: \\n\\n',df.head().T)\nprint('\\nTotal number of columns: \\n',df.shape[1])\nprint('\\nTotal number of rows: \\n',df.shape[0])","9c0f0613":"print(df.describe().T)","a2c5c5b9":"import math #mathematical functions\nfrom collections import Counter\ndef conditional_entropy(x,y):\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] \/ total_occurrences\n        p_y = y_counter[xy[1]] \/ total_occurrences\n        entropy += p_xy * math.log(p_y\/p_xy)\n    return entropy\n\ndef theil_u(x,y):\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n\/total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) \/ s_x   \n#correlation viz\ntheilu = pd.DataFrame(index=['class'])\ncolumns = df.columns\nfor j in range(0,len(columns)):\n    u = theil_u(df['class'].tolist(),df[columns[j]].tolist())\n    theilu.loc[:,columns[j]] = u\ntheilu.fillna(value=np.nan,inplace=True)\nplt.figure(figsize=(20,1))\nsns.heatmap(theilu,annot=True,fmt='.2f')\nplt.show()","1bd3b260":"#viz per class\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nsns.countplot(x=\"cap-shape\", hue='class', data=df, ax=ax[0])\nsns.countplot(x=\"cap-surface\", hue='class', data=df, ax=ax[1])\nsns.countplot(x=\"cap-color\", hue='class', data=df, ax=ax[2])\nfig, ax = plt.subplots(1,2, figsize=(15,5))\nsns.countplot(x=\"bruises\", hue='class', data=df, ax=ax[0])\nsns.countplot(x=\"odor\", hue='class', data=df, ax=ax[1])\nfig, ax = plt.subplots(1,4, figsize=(20,5))\nsns.countplot(x=\"gill-attachment\", hue='class', data=df, ax=ax[0])\nsns.countplot(x=\"gill-spacing\", hue='class', data=df, ax=ax[1])\nsns.countplot(x=\"gill-size\", hue='class', data=df, ax=ax[2])\nsns.countplot(x=\"gill-color\", hue='class', data=df, ax=ax[3])\nfig, ax = plt.subplots(2,3, figsize=(20,10))\nsns.countplot(x=\"stalk-shape\", hue='class', data=df, ax=ax[0,0])\nsns.countplot(x=\"stalk-root\", hue='class', data=df, ax=ax[0,1])\nsns.countplot(x=\"stalk-surface-above-ring\", hue='class', data=df, ax=ax[0,2])\nsns.countplot(x=\"stalk-surface-below-ring\", hue='class', data=df, ax=ax[1,0])\nsns.countplot(x=\"stalk-color-above-ring\", hue='class', data=df, ax=ax[1,1])\nsns.countplot(x=\"stalk-color-below-ring\", hue='class', data=df, ax=ax[1,2])\nfig, ax = plt.subplots(2,2, figsize=(15,10))\nsns.countplot(x=\"veil-type\", hue='class', data=df, ax=ax[0,0])\nsns.countplot(x=\"veil-color\", hue='class', data=df, ax=ax[0,1])\nsns.countplot(x=\"ring-number\", hue='class', data=df, ax=ax[1,0])\nsns.countplot(x=\"ring-type\", hue='class', data=df, ax=ax[1,1])\nfig, ax = plt.subplots(1,3, figsize=(20,5))\nsns.countplot(x=\"spore-print-color\", hue='class', data=df, ax=ax[0])\nsns.countplot(x=\"population\", hue='class', data=df, ax=ax[1])\nsns.countplot(x=\"habitat\", hue='class', data=df, ax=ax[2])\nfig.tight_layout()\nfig.show()","0ea3b1b9":"# exclude any Na's which is represented by '?'\ndf = df[df['stalk-root'] != '?']\n# drop column veil-type becaue of 1 only unique observation\ndf = df.drop(['veil-type'],axis=1)\nprint('Unique columns from all data are: \\n\\n',np.unique(df.columns))\nprint('\\nUnique values from all columns: \\n',np.unique(df.values))\nprint('\\nTotal number of new columns: \\n',df.shape[1])\nprint('\\nTotal number of new rows: \\n',df.shape[0])\n# How many Na's count per column\n# df.isnull().sum().sort_values(ascending=False)\nprint('\\nCheck if we have na value in any column:\\n',df.isnull().any())","0e126a33":"print(df.describe().T)","e5111ecb":"#one hot label encoding\nfeatures = df.iloc[:,1:]\nfeatures = pd.get_dummies(features)\ntarget = df.iloc[:,0].replace({'p': 0, 'e': 1})\nprint('First 5 rows of new encoded feature columns:\\n',features.head())\nprint('First 5 rows of new encoded target class of mushroom poisonous = 0 edible = 1:\\n',target.head())\nX = features.values\ny = target.values","997e3d00":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                    test_size = 0.3,\n                                                    random_state=29)\ntarget_names = ['poisonous', 'edible']\nprint ('X_train Shape:', X_train.shape)\nprint ('X_test Shape:', X_test.shape)\nprint ('y_train Shape:', y_train.shape)\nprint ('y_test Shape:', y_test.shape)","ea9b5281":"#calling kmeans classifier from sklearn\nfrom sklearn.cluster import KMeans\n# setting the classifier parameters\nk_means=KMeans(n_clusters=2)\n#Fitting kmesnd to training set\nk_means.fit(X_train, y_train)\n#Predicting values on test set\nk_means_predict = k_means.predict(X_test)\n#report the results\nprint(\"\\nKmeans confusion matrix: \\n\",confusion_matrix(y_test, k_means_predict))\nprint(\"\\nKmeans Classifier report: \\n\",classification_report(y_test,k_means_predict,target_names=target_names))\n#testing\n# print(\"\\nAccuraccy score of the model is:\\n\",accuracy_score(k_means_predict, y_test)*100)","822fa717":"#calling the svm classifier from sklearn\nfrom sklearn.svm import SVC\n# setting the classifier parameters\nsvm = SVC(kernel= 'sigmoid',gamma='scale',probability=True)\n#Fitting SVM to training set\nsvm.fit(X_train, y_train)\n#Predicting values on test set\nsvm_predict = svm.predict(X_test)\n#report the results\nprint(\"\\nSVM confusion matrix: \\n\",confusion_matrix(y_test, svm_predict))\nprint(\"\\nSVM classification report: \\n\",classification_report(y_test,svm_predict,target_names=target_names))","c9e6ee6a":"#calling the decision tree classifier from sklearn and graphiz for visuals\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n# setting the classifier parameters\ndtree = tree.DecisionTreeClassifier(max_depth=3)\n#Fitting decision tree to training set\ndtree.fit(X_train, y_train)\n#Predicting values on test set\ndtree_predict = dtree.predict(X_test)\n#report the results\nprint(\"\\nDecision tree confusion matrix: \\n\",confusion_matrix(y_test, dtree_predict))\nprint(\"\\nDecision tree classification report: \\n\",classification_report(y_test,dtree_predict,target_names=target_names))\n#test\n# print(accuracy_score(y_test,dtree_predict)) #raw_score\ndtree_viz = export_graphviz(dtree, out_file=None, \n                         feature_names=features.columns,  \n                         filled=True, rounded=True,  \n                         special_characters=True,\n                         impurity=True,proportion=True,\n                         rotate=True,node_ids=True,\n                         class_names=['Poisonous','Edible'])  \nimport pydotplus #convert graphviz viz from svg to png\n# Draw graph\ngraph = pydotplus.graph_from_dot_data(dtree_viz)  \n\nfrom IPython.display import Image  \n# Show graph as png since it default output it as svg\nImage(graph.create_png())","6efb0c0e":"from catboost import CatBoostClassifier, Pool\n#catboost model, use_best_model params will make the model prevent overfitting\ncatboost = CatBoostClassifier(eval_metric='Accuracy',use_best_model=True)\ncatboost.fit(X_train,y_train,use_best_model=True,eval_set=(X_test,y_test),verbose=False)\ncatboost_predict = catboost.predict(X_test)\n#report the results\nprint(\"Catboost confusion matrix: \\n\",confusion_matrix(y_test, catboost_predict))\nprint(\"Catboost classification report: \\n\",classification_report(y_test,catboost_predict,target_names=target_names))","ab6b33de":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(50,),\n                    activation='logistic',\n                    max_iter=10,\n                    solver='adam',\n                    verbose=True)\nmlp.fit(X_train, y_train)\nmlp_predict = mlp.predict(X_test)\nprint(\"\\nMLP confusion matrix: \\n\",confusion_matrix(y_test, mlp_predict))\nprint(\"\\nMLP classification report: \\n\",classification_report(y_test,mlp_predict))","685ede91":"from tensorflow import keras\nmodel = keras.Sequential([keras.layers.Dense(50, input_shape=(97,)),\n                          keras.layers.Dense(2, activation='sigmoid')])\n# model.summary()\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['acc'])\nmodel.fit(X_train, y_train,epochs=5, verbose=1)\nkeras_pred = model.predict_classes(X_test)\n# keras_pred = np.argmax(keras_pred, axis=1)\nprint('\\nKeras confusion matrix:\\n',confusion_matrix(keras_pred, y_test))\nprint('\\nKeras classification Report:\\n',classification_report(keras_pred, y_test,target_names=target_names))","42a65dba":"# EXPLORATORY DATA ANALYSIS\n\nIn this part, exploratory data analysis which will performed by going through investigating  features to see how data is distributed, unique values, and if there are any missings values.","2c1bf299":"## DATA VISUALIZATION\nAs a final step to exploratory data analysis, we are going to visualize our features per the target feature 'class'  and to see how observations do contrasts from column to another.\n\n - Findings:\n \n >- The distribution of the 'class' feature is balanced and not distributed extremely in p or e label.\n \n >- Most poisonous mushrooms have a Buff='b' in the feature 'gill-color'.\n\n >- the feature **'veil-type'** has only have one unique observation, which does not add any value to our   analysis, therefore we will eliminate it.\n\n >- The feature **'stalk-root'** have a value of **'?'** which donated to be a missing value, from the main dataset source description.","07a8c0ff":"## ONE-HOT LABEL ENCODING\nOne-hot encoding is a technique which applied mostly for text-based categorical data, to transfer the values into boolean numerics of 1s or 0s, and each label will be added as a column and every time it occurs as a label it will get 1 as a value if exist, otherwise will get 0. for an example: the feature 'population' have values of v, y, s, a, n, and c, representing those labels by a different numeric value will give each one of them a higher effect more than the other labels. converting each into a binary will give all the labels the same effect across the dataset. Using one-hot encoding will increases the columns count to 97, which adds a huge sparsity into our data.\n\n>- NOTE: one-hot encoding can be implemented by calling (label encoder) from \"scikit-learn\" module or (get_dummies) from \"pandas\" module.","9849c1a0":"### SCIKIT-LEARN IMPLEMENTATION<BR>\n\nMLP classifier is a feed-forward neural network, which has an input layer, output layer, and two or more trainable weight layers (consisting of Perceptrons). scikit-learn MLP implementation is not intended for large-scale applications and offers no GPU support, this model is been set with a size of the hidden layer with a size of 50, and mode trained with 20 iterations. Adam optimizer has been chosen, Adam method intends to automatically adjust the amount to update parameters, Adam is also appropriate for non-stationary problems.\n\n\n\n#### **MODEL SUMMARY**:\n\n\n- MLP classifier achieved 99% accuracy with only 10 iterations.","8ff39f20":"## SVM CLASSIFIER\nSupport vector machine constructs a hyper-plane between data labels, which can be used for classification, and regression problems, data separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class, the larger the margin the lower the generalization error of the classifier.\n\n\n\n#### **MODEL SUMMARY**:\n\n\n- In the training  model, \"sigmoid' kernel\"  is being used, since the target values distributed as 0s, and 1s.","4753ec61":"### KERAS IMPLEMENTATION<BR>\n\n\nKeras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, and offers CPU and GPU computation support, widely used for text and image machine learning application, such as classification object detection......etc. In the following model, Keras has been designed in a similar setting as possible to MLP classifier from the Scikit-learn module, as a loss function, 'sparse_categorical_crossentropy' been selected for the categorical binary output. Adam optimizer selected since it required minimum tuning to work, and the number of iteration been reduced to the minimum.\n\n\n#### **MODEL SUMMARY**:\n\n\n- Keras implementation achieved 100% accuracy with half the number of iteration compared to the MLP classifier from the scikit-learn module.","029305bd":"# INTRODUCTION\nIn this project we will dive into mushroom classification problem which is represented by a tabular dataset that contains 23 features and 8124 observations, labeled as edible or poisonous mushroom. The goal is to classify mushrooms as either edible or poisonous.","201353be":"# CONCLUSION\n\n\nMushroom classification is one of the most famous examples of categorical machine learning problems, therefore it can be seen that most of the introduced models performed very good without any tuning for the parameters, on the other hand, Kmeans clustering as a classifier is not recommended for a such a sparse data since it misclassified many values and accuracy of the model is robust (between 13% & 85%) , on the other hand, support vector machine model achieved 94% accuracy, followed by decision tree classifier by a 97% accuracy.  Catboost algorithm which used manly in kaggle competitions for related problems achieved a 100% accuracy on our dataset. Deep learning introduced by two different modules, MLP classifier from scikit-learn which achieved 99% accuracy with an after 10 number of iterations, and Keras framework which selected the same model structure of MLP and half number of iterations, achieved a 100% accuracy, which means that gradient boosting and neural nets have good promising results in such cases.\n","4d0fc091":"## DECISION TREE\nDecision Trees algorithm uses a tree-like graph or model of decisions and their possible consequences, And used broadly in classification and regression problems, By predicting the value of a target variable by learning simple decision rules from data features, as an if-else in programming, decision tree gives a meaning of which features are important to the classifier decision, by stating how powerfull is every label related to the classifier accuracy.\n\n\n#### **MODEL SUMMARY**:\n\n- In the training model, the maximum depth of the tree set to be 3, since features are sparse and have less effect of the tree after this depth, other parameters been set at the default setting.\n\n- Most important features are: [spore-print-color_h, gill-size_n, and ring-number_o].\n\n- The model achieved 97% accuracy with the default setting.","367d9b5b":"# IMPORT MODULES\nIn this section, we are going to import the needed resources and configurations to the environment. Pandas will be used for data manipulation, numpy for linear algebric operations such as converting datashape into an array, matplotlib, seaborn and graphviz for visualizations, scikit-learn for preprosessing, as well for machine learning algorithms, other models and frameworks will be introduced such as keras (Deep-learning) and catboost (Gradient-boosting).","65f8275f":"# BUILDING, TRAINING & TESTING MACHINE LEARNING MODELS\nBuilding a machine learning model will start first by setting the train\/test data split, then build and evaluate several models and summarise each model score.","e5267713":"## CATBOOST\nCatBoost is a machine learning algorithm that uses gradient boosting on decision trees. It provides flexibility since it prevents the model of overfitting, Created by Yandex Technologies ,and use it for ranking tasks, forecasting and making recommendations.\n\n\n#### **MODEL SUMMARY**:\n\n\n- the model achieved 100% accuracy by the default parameters, and without any tunning.","035231a8":"# INTRODUCTION TO DATASET\nImport dataset from system path, and preview the first 5 entries for each column, it can be seen that all our 23 columns are categorical and a total of 8124 rows.\n\n- NOTE: Change the path of the dataset according to your local machine.","38aa2503":"\n\n## DATA DESCRIPTION:\n\n- Data dictionary of the used dataset.\n\n\n- Attribute Information: (classes: edible=e, poisonous=p)\n\n\n\n- Number of instances: (569).\n- Number of attributes: 32 (ID, diagnosis, 30 real-valued input features).\n- Class distribution: 357 benign, 212 malignant.\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n    \n    \n\n| Column                   | Description                                                                                        |\n|--------------------------|----------------------------------------------------------------------------------------------------|\n| cap-shape                | bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s                                               |\n| cap-surface              | fibrous=f,grooves=g,scaly=y,smooth=s                                                               |\n| cap-color                | brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y                    |\n| bruises                  | bruises=t,no=f                                                                                     |\n| odor                     | almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s                        |\n| gill-attachment          | attached=a,descending=d,free=f,notched=n                                                           |\n| gill-spacing             | close=c,crowded=w,distant=d                                                                        |\n| gill-size                | broad=b,narrow=n                                                                                   |\n| gill-color               | black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y |\n| stalk-shape              | enlarging=e,tapering=t                                                                             |\n| stalk-root               | bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?                                    |\n| stalk-surface-above-ring | fibrous=f,scaly=y,silky=k,smooth=s                                                                 |\n| stalk-surface-below-ring | fibrous=f,scaly=y,silky=k,smooth=s                                                                 |\n| stalk-color-above-ring   | brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y                                                                                           |\n| stalk-color-below-ring   | brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y                                                                                            |\n|veil-type | partial=p,universal=u|\n|veil-color|brown=n,orange=o,white=w,yellow=y|\n|ring-number| none=n,one=o,two=t|\n|ring-type| cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z|\n|spore-print-color| black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y|\n|population| abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y|\n|habitat| grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d|\n\n\n    \n    ","ab649442":"## **ABOUT THE DATASET**:\n- Data Set Information:<br>\n\n    This dataset is collected from the archive of University of California, Irvine, This dataset originally created by University of Wisconsin Clinical Sciences Center, and titled as Breast Cancer Wisconsin (Diagnostic) Data Set, according to Kaggle.com\/ this dataset was updated 3 years ago.\n\n\n- Creators: \n\n 1. Dr. William H. Wolberg, General Surgery Dept. \nUniversity of Wisconsin, Clinical Sciences Center \nMadison, WI 53792 \nwolberg '@' eagle.surgery.wisc.edu \n\n 2. W. Nick Street, Computer Sciences Dept. \nUniversity of Wisconsin, 1210 West Dayton St., Madison, WI 53706 \nstreet '@' cs.wisc.edu 608-262-6619 \n\n 3. Olvi L. Mangasarian, Computer Sciences Dept. \nUniversity of Wisconsin, 1210 West Dayton St., Madison, WI 53706 \nolvi '@' cs.wisc.edu \n\n\n- Source:<br>\n\n    This database is also available through the UW CS ftp server:\n ftp.cs.wisc.edu \ncd math-prog\/cpo-dataset\/machine-learning\/WDBC\/\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29","48dff6b3":"## INITIAL STATISTICAL SUMMARY OF OUR DATASET.\nHere we observe the distribution of our data and how values are distributed among the columns.","1c8a8ec0":"## DATA CLEANING\nIn this section we are going to remove the feature **'veil-type'**, and the **'?'** value from **'stalk-root'** feature, and return the new cleaned dataset. Checking out new features and values, the size of dataset labels and columns will decreases due to this process.\n\n- FINDING:\n\n>- The total number of rows decreased from 8124 to 5644 due to removing the '?' value from the 'stalk-root' feature.\n\n>- Our columns decreases from 23 to 22 due to clean the 'veil-type' feature.\n\n>- After cleaning it can be seen that there is no 'NA' values in our dataset.","e67b67cc":"# ENDING NOTE\nThanks for the many several kaggle notebooks to make this work possible, and for make acessing to information easier, please do not hesitate to add or comment. If the accuracy results were little bit diffrent due to kaggle enviroment, please note that the comments were written acording my local machine.","79514647":"# FEATURE ENGINEERING\nIn the feature engineering step, Data will be transformed into an acceptable format for machine learning models. Since all our features are all categorical, we will need to encode them to numerical values.","beaf24db":"## DATA TRAIN\/TEST SPLIT\nData train\/test split is the main step into test machine learning on unseen data, Here train\/test data split by a 70:30 ratio, which means that models will be trained by 70% of data, and tested on 30% of unseen data. Random_stat parameter ensures that the random numbers are generated in the same order for reproducibility.","ce70c708":"## CORRELATION BETWEEN CATEGORICAL FEATURES\nHere we are going to measure of association between two categorical features, to observe how features are correlated to the target feature ['class'], whether a mushroom is edible or poisonous. in the original dataset it can be seen that all features are donated by a character value, since we are going to perform one-hot label encoding to our feature its not wise to perform it after encoding our features since features will increase dramatically from our current number into 97 feature.\n\n\n**Uncertainty Coefficient:**\n\n\nthis method called Theil's U, also known as the Uncertainty Coefficient. Formally marked as U(x|y), this coefficient provides value in the range of [0,1], where 0 means that feature y provides no information about feature x, and 1 means that feature y provides full information about features x's value.\n\n - Findings:\n \n >- It can be seen how insignificant feature is 'veil-type' to target feature 'class', which needs to be removed from dataset.\n \n >- The feature odor is the most significant one for target feature.","caf55199":"## DEEP LEARNING\n\nIn this section two deep learning approaches will be selected, the first module is from sci-kit learn module, known as multilayer perception, and the second one is from Keras module, A neural network using the Sequential model of dense layers, all parameters in both models have been set nearly the same as possible.","bd00c622":"## KMEANS CLUSTERING\n\nk-means algorithm is a very fast algorithm. but the results can fall behind with a sparse data, That\u2019s why it can be useful to restart it several times. in our example, it can be seen how the sparsity of features is affecting the performance of the model with every run.\n\n\n#### **MODEL SUMMARY**:\n\n \n- In our example, we created a model with 2 clusters for our target data labels, which are 0s, and 1s. and left the other parameters on the default setting.\n\n>>- NOTE:\nKmeans clustering model is so robust, running it several times give a different accuracy (13%, and 85%)","9887f4f6":"## FINAL STATISTICAL SUMMARY OF OUR DATASET.\nHere we observe the distribution of our new cleaned data and how features are distributed among the columns.\n\n\n- **SUMMARY**:\n\n>- All features have at least two unique labels distributed by all labels in our target class.\n\n>- The new dataset has a total of 5644 labels."}}