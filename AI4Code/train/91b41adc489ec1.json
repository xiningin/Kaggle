{"cell_type":{"66de2bb8":"code","836106fa":"code","ab2d4dd6":"code","fa044370":"code","0338ba17":"code","46362cf3":"code","93e6ea95":"code","a3e6e469":"code","98a1da81":"code","e1b0cf53":"code","58e767fc":"code","53e9b37c":"code","33c0feec":"code","a7261779":"code","fd5304cb":"code","2817463d":"code","9f9e9c5e":"code","8b355117":"code","317d656c":"code","d666e559":"code","226830ad":"code","6a0be552":"code","a2598b4e":"code","cc6e6278":"code","dbe2c32b":"code","159446b3":"code","8211c003":"code","066cb4d3":"code","20d74e3a":"code","4c6aec2a":"code","327a558c":"code","d73911d7":"code","25331b36":"code","ed1be8d5":"code","a6f3cacc":"code","fdef7e23":"code","9e717b16":"code","da9db98d":"code","11d96d8e":"code","9ed04d16":"code","ae1e49f9":"code","a6966e21":"code","72aeb183":"code","d8d3ce3e":"code","1743970b":"code","080ed60d":"code","24aaa9b8":"code","b0a84ef5":"code","056bb204":"code","d675b693":"code","c28986eb":"code","f7b7153d":"code","210de441":"code","c7c65377":"code","c9bb5cf3":"code","4c8f2170":"code","186397ac":"code","042dcf3a":"code","910d5d18":"code","6f0cd014":"code","ed51d4d4":"code","6cb1b462":"code","0a7e4d27":"code","e246aadf":"code","f6861c58":"code","cd5fbf62":"code","e0173da2":"code","7a00998e":"code","a6c6750a":"code","a6909bfa":"code","150c7932":"code","f596b074":"code","0958aa87":"code","7e25270f":"code","da272863":"code","e3d9df30":"code","ec817336":"code","b600b20b":"code","d920b41e":"code","5f17547f":"code","45587051":"code","50d63e68":"code","2c3057d7":"code","49f7c0b0":"code","c2bfa556":"code","cae44e4e":"code","e68de028":"code","2e14658f":"code","fe8f45ef":"code","3b43bbfa":"code","6628f9fb":"code","15aecbf7":"code","4bde538e":"code","e9793c62":"code","de943f75":"code","505a107f":"code","53afdc56":"code","981982cd":"code","95b6903d":"code","7b20c916":"code","f5ea41ad":"code","c9416787":"code","2e138298":"code","0f5837ea":"code","b3ca77fc":"code","d5c14bde":"code","bb51de18":"markdown","4cdea2be":"markdown","4c78bd6b":"markdown","7f8dd507":"markdown","aaa8239d":"markdown","0d8e566e":"markdown","dc502f02":"markdown","de316094":"markdown","257a1aad":"markdown","3f5c3181":"markdown","538322cc":"markdown","26ac4e3f":"markdown","739ec1a5":"markdown","f6f69973":"markdown","812efba3":"markdown","865421e6":"markdown","557f9e74":"markdown","8e1bd4f0":"markdown","9877c5b0":"markdown","4b0e5253":"markdown","1b31f7bc":"markdown","6b0b0c00":"markdown","126163d2":"markdown","5e4e7c5e":"markdown","e4982685":"markdown","b24713c5":"markdown","ddd4bdf9":"markdown","9b90ddd1":"markdown","986bf3c3":"markdown","c0471d0b":"markdown","4c39f53b":"markdown"},"source":{"66de2bb8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","836106fa":"from matplotlib import pyplot as plt\nimport seaborn as sns\nfrom statsmodels.api import OLS\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nimport folium\nimport eli5\nimport datetime as dt\n%matplotlib inline","ab2d4dd6":"train = pd.read_csv('\/kaggle\/input\/diamonds-price\/diamonds_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/diamonds-price\/diamonds_test.csv')","fa044370":"train.head()","0338ba17":"test_df.head()","46362cf3":"train_desc = train.describe()\ntrain_desc","93e6ea95":"train.info()","a3e6e469":"train.isna().sum()","98a1da81":"#price iqr\ndesc = train.describe()\niqr = desc.iloc[6,1] - desc.iloc[4,1]\nupper_bound = desc.iloc[6,1] + (1.5*iqr)","e1b0cf53":"train_cut = train.groupby('cut')\ntrain_cut_desc = train_cut['price'].describe()\ntrain_cut_desc","58e767fc":"outliers = train[train['price'] > upper_bound]\noutliers","53e9b37c":"out_cut = outliers.groupby('cut')\nout_cut_desc = out_cut['price'].describe()\nout_cut_desc","33c0feec":"plt.figure(figsize=(16,10))\nplt.subplot(221)\nplt.title('Price Mean (Outliers)')\nplt.plot(out_cut_desc.index.tolist(), out_cut_desc['mean'])\nplt.subplot(222)\nplt.title('Sample (Outliers)')\nplt.plot(out_cut_desc.index.tolist(), out_cut_desc['count'], 'orange')\nplt.subplot(223)\nplt.title('Price Mean (All)')\nplt.plot(out_cut_desc.index.tolist(), train_cut_desc['mean'])\nplt.subplot(224)\nplt.title('Sample (All)')\nplt.plot(out_cut_desc.index.tolist(), train_cut_desc['count'], 'orange')","a7261779":"train_clean = train.drop(outliers.index.tolist(), axis=0)\ntrain_clean","fd5304cb":"train_clean_cut = train_clean.groupby('cut')","2817463d":"train","9f9e9c5e":"train_desc","8b355117":"train_clean_desc = train_clean.describe()\ntrain_clean_desc","317d656c":"#x = length\n#y = width\n#z = depth\ntrain[(train['x'] == 0) | (train['y'] == 0)]","d666e559":"train_clean[(train_clean['x'] == 0) | (train_clean['y'] == 0)]","226830ad":"train.drop(train[train['x'] == 0].index, axis=0, inplace=True)\ntrain_clean.drop(train_clean[train_clean['x'] == 0].index, axis=0, inplace=True)","6a0be552":"print('Length of train data: ', len(train))\nprint('Length of train_clean data: ', len(train_clean))","a2598b4e":"train.cut.unique()","cc6e6278":"#group again after dropping impossible value\ntrain_cut = train.groupby('cut')\ntrain_clean_cut = train_clean.groupby('cut')","dbe2c32b":"train_cut.first()","159446b3":"#distribution with outliers\nplt.figure(figsize=(16,10))\nplt.subplot(231)\nplt.title('All Price Distribution')\nsns.distplot(train.price)\nplt.subplot(232)\nplt.title('Fair Cut Price Distribution')\nsns.distplot(train_cut.get_group('Fair')['price'])\nplt.subplot(233)\nplt.title('Good Cut Price Distribution')\nsns.distplot(train_cut.get_group('Good')['price'])\nplt.subplot(234)\nplt.title('Very Good Cut Price Distribution')\nsns.distplot(train_cut.get_group('Very Good')['price'])\nplt.subplot(235)\nplt.title('Premium Cut Price Distribution')\nsns.distplot(train_cut.get_group('Premium')['price'])\nplt.subplot(236)\nplt.title('Ideal Cut Price Distribution')\nsns.distplot(train_cut.get_group('Ideal')['price'])","8211c003":"#distribution without outliers\nplt.figure(figsize=(16,10))\nplt.subplot(231)\nplt.title('All Price Distribution')\nsns.distplot(train_clean['price'])\nplt.subplot(232)\nplt.title('Fair Cut Price Distribution')\nsns.distplot(train_clean_cut.get_group('Fair')['price'])\nplt.subplot(233)\nplt.title('Good Cut Price Distribution')\nsns.distplot(train_clean_cut.get_group('Good')['price'])\nplt.subplot(234)\nplt.title('Very Good Cut Price Distribution')\nsns.distplot(train_clean_cut.get_group('Very Good')['price'])\nplt.subplot(235)\nplt.title('Premium Cut Price Distribution')\nsns.distplot(train_clean_cut.get_group('Premium')['price'])\nplt.subplot(236)\nplt.title('Ideal Cut Price Distribution')\nsns.distplot(train_clean_cut.get_group('Ideal')['price'])","066cb4d3":"plt.figure(figsize=(10,6))\nsns.boxplot(y='price', x='cut', hue='cut', data=train)","20d74e3a":"plt.figure(figsize=(10,6))\nsns.boxplot(y='price', x='cut', hue='cut', data=train_clean)","4c6aec2a":"#price correlation with carat and cutting quality (w\/ outliers)\nplt.figure(figsize=(10,6))\nsns.scatterplot(x='carat',y=\"price\",hue=\"cut\",palette=\"Set2\",data=train)","327a558c":"#price correlation with carat and cutting quality (w\/o outliers)\nplt.figure(figsize=(10,6))\nsns.scatterplot(x='carat',y=\"price\",hue=\"cut\",palette=\"Set2\",data=train_clean)","d73911d7":"#pearson correlation w\/ outliers\ntrain_cor = train.iloc[:,1:].corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(train_cor, annot=True)\nplt.show()","25331b36":"#pearson correlation w\/ outliers\ntrain_clean_cor = train_clean.iloc[:,1:].corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(train_cor, annot=True)\nplt.show()","ed1be8d5":"price_corr = train_cor['price']\nprice_clean_corr = train_clean_cor['price']\n\nplt.figure(figsize=(10,8))\nplt.plot(price_corr.index, price_corr)\nplt.plot(price_corr.index, price_clean_corr)\nplt.legend(['w\/ outliers', 'w\/o outliers'])\nplt.show()","a6f3cacc":"train_cut['cut'].describe()","fdef7e23":"colors=['#ff9999','#66b3ff','#E66032', '#99ff99','#ffcc99']\n\nplt.figure(figsize=(16,8))\nplt.subplot(121)\nplt.title('Cutting Percentage (w\/ outliers)')\nplt.pie(train_cut['cut'].describe()['count'], labels=train_cut['cut'].describe().index,\n        colors=colors, autopct='%1.1f%%', startangle=90)\n\nplt.subplot(122)\nplt.title('Cutting Percentage (w\/o outliers)')\nplt.pie(train_clean_cut['cut'].describe()['count'], labels=train_clean_cut['cut'].describe().index,\n        colors=colors, autopct='%1.1f%%', startangle=90)\n\nplt.show()","9e717b16":"train.head()","da9db98d":"train_clean.head()","11d96d8e":"train.drop(['depth', 'table'], axis=1, inplace=True)\ntrain_clean.drop(['depth', 'table'], axis=1, inplace=True)","9ed04d16":"train1 = train.copy()\ntrain1.head()","ae1e49f9":"train2 = train_clean.copy()\ntrain2.head()","a6966e21":"train1 = pd.get_dummies(train1, prefix=['cut', 'color', 'clarity'])\ntrain2 = pd.get_dummies(train2, prefix=['cut', 'color', 'clarity'])","72aeb183":"id_train = train['id']\nid_clean = train_clean['id']\ntrain.drop(['id'],axis=1,inplace=True)\ntrain_clean.drop(['id'],axis=1,inplace=True)\ntrain1.drop(['id'],axis=1,inplace=True)\ntrain2.drop(['id'],axis=1,inplace=True)","d8d3ce3e":"train1.head()","1743970b":"train2.head()","080ed60d":"#w\/ outliers\nX = train1.drop('price', axis=1)\ny = train1['price']\nols = sm.OLS(y, sm.add_constant(X))\nresults = ols.fit()\nresults.summary()","24aaa9b8":"sns.distplot(results.resid)","b0a84ef5":"# alpha= 0.05\n# Null-hypo: Autocorrelation is absent\n# Alternative Hypothesis: Autocorrelation is present\n\nfrom statsmodels.stats import diagnostic\n\ndiagnostic.acorr_ljungbox(results.resid, lags =1, return_df=True)","056bb204":"# Using Goldfeld Quandt we test for Heteroscedasticity\n# alpha= 0.05\n# Null Hypo: Error terms are homoscedastic\n# Alt. Hypo: Error terms are heteroscedastic\n\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(results.resid, results.model.exog)\nlzip(name, test)","d675b693":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = [variance_inflation_factor(X.values, j) for j in range(X.shape[1])]\npd.Series(vif, index = X.columns, name = 'VIF')","c28986eb":"#w\/ outliers\nX = train2.drop('price', axis=1)\ny = train2['price']\nols = sm.OLS(y, sm.add_constant(X))\nresults2 = ols.fit()\nresults2.summary()","f7b7153d":"sns.distplot(results2.resid)","210de441":"# alpha= 0.05\n# Null-hypo: Autocorrelation is absent\n# Alternative Hypothesis: Autocorrelation is present\n\nfrom statsmodels.stats import diagnostic\n\ndiagnostic.acorr_ljungbox(results2.resid, lags =1, return_df=True)","c7c65377":"# Using Goldfeld Quandt we test for Heteroscedasticity\n# alpha= 0.05\n# Null Hypo: Error terms are homoscedastic\n# Alt. Hypo: Error terms are heteroscedastic\n\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(results2.resid, results2.model.exog)\nlzip(name, test)","c9bb5cf3":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = [variance_inflation_factor(X.values, j) for j in range(X.shape[1])]\npd.Series(vif, index = X.columns, name = 'VIF')","4c8f2170":"def evaluate(model, test_features, test_labels):\n    vals = dict()\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    rmse = np.sqrt(mean_squared_error(test_labels, predictions))\n    r2 = r2_score(test_labels, predictions)\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    print('RMSE = {:0.2f}'.format(rmse))\n    print('R2 Score = {:0.2f}.'.format(r2))\n    vals['accuracy'] = accuracy\n    vals['rmse'] = rmse\n    vals['r2'] = r2\n    \n    return vals","186397ac":"#w\/ outliers\nX = train1.drop('price', axis=1)\ny = train1['price']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n\n#w\/o outliers\nX2 = train2.drop('price', axis=1)\ny2 = train2['price']\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2)","042dcf3a":"lr = LinearRegression()\nlr.fit(X_train, y_train)\nlr_eval1 = evaluate(lr, X_test, y_test)","910d5d18":"lr2 = LinearRegression()\nlr2.fit(X2_train, y2_train)\nlr_eval2 = evaluate(lr2,X2_test, y2_test)","6f0cd014":"# print(len(train2))\n# print(len(train_dum))\n# train_dum.tail(1)\ntrain2 = train2.set_index(np.arange(0,len(train2),1))\ntrain2.tail()","ed51d4d4":"scaler = MinMaxScaler()\nscaler.fit(train2)\nscaled = scaler.transform(train2)\ndfscaled = pd.DataFrame(scaled, columns=train2.columns)\ndfscaled['price'] = train2['price']\ndfscaled.head()","6cb1b462":"X = dfscaled.drop('price', axis=1)\ny = dfscaled['price']\npca = PCA(n_components=10)\npca.fit(X)\nxpca = pca.transform(X)\nxpca","0a7e4d27":"np.sum(pca.explained_variance_ratio_)","e246aadf":"dfpca = pd.DataFrame(xpca, columns=['pc'+str(i) for i in range(1,11)])\ndfpca['price'] = dfscaled['price']\ndfpca.head()","f6861c58":"print('train2 length: ', len(train2))\nprint('dfscaled length: ', len(dfscaled))\nprint('dfpca length: ', len(dfpca))","cd5fbf62":"print('train2 length: ', train2.isna().any().any())\nprint('dfscaled length: ', dfscaled.isna().any().any())\nprint('dfpca length: ', dfpca.isna().any().any())","e0173da2":"plt.figure(figsize=(16,8))\nsns.heatmap(dfpca.corr(), annot=True)\nplt.show()","7a00998e":"Xp = dfpca.drop('price',axis=1)\nyp = dfpca['price']\nXp_train, Xp_test, yp_train, yp_test = train_test_split(Xp, yp)","a6c6750a":"lr3 = LinearRegression()\nlr3.fit(Xp_train, yp_train)\nlr_eval3 = evaluate(lr3, Xp_test, yp_test)","a6909bfa":"scaledcon = dfscaled[['carat', 'x', 'y', 'z', 'price']]\nscaledcon.head()","150c7932":"scaledcon.describe()","f596b074":"Xcon = scaledcon.drop('price',axis=1)\nycon = scaledcon['price']\npcac = PCA(n_components=2)\npcac.fit(Xcon)\ncon_pca= pcac.transform(Xcon)\ncon_pca","0958aa87":"pcac.explained_variance_ratio_","7e25270f":"dfpcac = pd.DataFrame(con_pca, columns=['pc1', 'pc2'])\ndfpcac['price'] = scaledcon['price']\ndfpcac.head()","da272863":"# plt.figure(figsize=(10,8))\nsns.heatmap(dfpcac.corr(), annot=True)\nplt.show()","e3d9df30":"dfpcac['price']","ec817336":"Xpc = dfpcac.drop('price', axis=1)\nypc = dfpcac['price']\nXpc_train, Xpc_test, ypc_train, ypc_test = train_test_split(Xpc, ypc, test_size=0.2)","b600b20b":"lr4 = LinearRegression()\nlr4.fit(Xpc_train, ypc_train)\nlr_eval4 = evaluate(lr4, Xpc_test, ypc_test)","d920b41e":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 200, num = 20)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 200, num = 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 20]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4, 6, 8, 10]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\nrandom_grid = {\n    'n_estimators': n_estimators,\n    'max_features': max_features,\n    'max_depth': max_depth,\n    'min_samples_split': min_samples_split,\n    'bootstrap': bootstrap\n}\n\nprint(random_grid)","5f17547f":"rf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100,\n                               cv=5, verbose=2, random_state=41, n_jobs=-1)\n\nrf_random.fit(X2_train, y2_train)","45587051":"rf_random.best_params_","50d63e68":"base_rf = RandomForestRegressor()\nbase_rf.fit(X2_train, y2_train)\nbase_rf_eval1 = evaluate(base_rf, X2_test, y2_test)","2c3057d7":"best_rf = rf_random.best_estimator_\nbest_rf.fit(X2_train, y2_train)\nbest_rf_eval2 = evaluate(best_rf, X2_test, y2_test)","49f7c0b0":"best_rf_eval3 = evaluate(best_rf, X2_train, y2_train)","c2bfa556":"boost_grid = {\n    \"loss\": ['ls', 'lad', 'huber', 'quantile'],\n    \"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40],\n    \"n_estimators\": np.arange(100,201,20),\n    \"subsample\": np.linspace(0.5,1,6),\n    \"min_samples_split\": [2,4,6,8,10,12],\n    \"min_samples_leaf\": [1,2,3,4,5,6,7,8],\n    \"max_depth\": [ 3, 4, 5, 6, 8, 10, 12, 15],\n    \"max_features\": ['auto', 'sqrt', 'log2'],\n    \"alpha\": np.linspace(0.1,1,10),\n    \"max_leaf_nodes\": [10,20,30,40,50,60,70,'None'],\n    \"warm_start\": [True, False],\n    \"validation_fraction\": np.linspace(0,1,10)\n}\n\nboost_grid","cae44e4e":"gbr = GradientBoostingRegressor()\ngbr_random = RandomizedSearchCV(estimator=gbr, param_distributions=boost_grid, n_iter=100,\n                               cv=5, verbose=2, random_state=41, n_jobs=-1)\n\ngbr_random.fit(X2_train, y2_train)","e68de028":"gbr_random.best_params_","2e14658f":"base_gbr = GradientBoostingRegressor()\nbase_gbr.fit(X2_train, y2_train)\nbasegbr_eval = evaluate(base_gbr, X2_test, y2_test)","fe8f45ef":"best_gbr = gbr_random.best_estimator_\nbest_gbr.fit(X2_train, y2_train)\nbestgbr_eval = evaluate(best_gbr, X2_test, y2_test)","3b43bbfa":"bestgbr_eval_train = evaluate(best_gbr, X2_train, y2_train)","6628f9fb":"bestrf_scores = cross_val_score(best_rf, X2, y2, cv=5, scoring='neg_root_mean_squared_error')\nbestrf_scores","15aecbf7":"plt.plot(np.array(range(1,6)), np.abs(bestrf_scores), 'bo-')\nplt.show()","4bde538e":"bestgbr_scores = cross_val_score(best_gbr, X2, y2, cv=5, scoring='neg_root_mean_squared_error')\nbestgbr_scores","e9793c62":"plt.plot(np.array(range(1,6)), np.abs(bestgbr_scores), 'bo-')\nplt.show()","de943f75":"rmse_vals = [lr_eval2['rmse'], lr_eval3['rmse'], lr_eval4['rmse'], best_rf_eval2['rmse'], bestgbr_eval['rmse']]\nr2_vals = [lr_eval2['r2'], lr_eval3['r2'], lr_eval4['r2'], best_rf_eval2['r2'], bestgbr_eval['r2']]\ndf_eval = pd.DataFrame({'rmse': rmse_vals, 'r2': r2_vals}, index=['LinearRegression', 'LinearRegression w\/ pca', 'LinearRegression w\/ pca (only continous)', 'RandomForest', 'GradientBoost'])\ndf_eval","505a107f":"plt.figure(figsize=(10,5))\nplt.plot(df_eval.index, df_eval['rmse'])\nplt.xticks(rotation=45)\nplt.grid()\nplt.show()","53afdc56":"X2_test.columns","981982cd":"test_df.head()","95b6903d":"test_dum = pd.get_dummies(test_df)\ntest_dum","7b20c916":"test_id = test_dum['id']\ntest_dum.drop('id', axis=1, inplace=True)","f5ea41ad":"test_dum.drop(['depth', 'table'], axis=1, inplace=True)","c9416787":"print(X2_test.columns)\nprint(test_dum.columns)","2e138298":"test_dum['price'] = best_gbr.predict(test_dum)","0f5837ea":"test_dum['id'] = test_id","b3ca77fc":"output = test_dum[['id', 'price']]","d5c14bde":"output.to_csv('submission.csv', index=False)\noutput.head()","bb51de18":"## Heatmap: Pearson Correlation","4cdea2be":"## OLS Report\nDataset with outliers","4c78bd6b":"## Heteroscedasticity","7f8dd507":"## Multi-Collinearity","aaa8239d":"## OLS Report\nDataset without outliers","0d8e566e":"### GradientBoost","dc502f02":"## Data cleaning: Price outliers","de316094":"Above we can say that auto-correlation is absent (Null-hypo accepted)","257a1aad":"## RandomForest","3f5c3181":"## Auto-Correlation","538322cc":"## GradientBoosting","26ac4e3f":"## Scatterplot","739ec1a5":"## PCA Only continous","f6f69973":"## Boxplot","812efba3":"## Residual","865421e6":"## Distribution Plot","557f9e74":"## Auto-Correlation","8e1bd4f0":"## Training using dataset w\/ outliers","9877c5b0":"## Residual","4b0e5253":"## Function to evaluate","1b31f7bc":"## Price Distribution","6b0b0c00":"## Heteroscedasticity","126163d2":"Dari hasil evaluasi di atas, dapat diketahui bahwa tidak terjadi overfitting jika menggunakan algoritma `GradientBoostingRegressor`","5e4e7c5e":"### RandomForest","e4982685":"## Cross Validation Score (k=5)","b24713c5":"## Multi-collinearity","ddd4bdf9":"## Data cleaning: `length`, `width`, `depth` with impossible value (0.0)","9b90ddd1":"## MinMaxScaler & PCA\nKenapa saya melakukan PCA? Karena banyak kolom yang berkorelasi antar satu dengan yang lain.","986bf3c3":"## Prediction\n### Using LinearRegression","c0471d0b":"## Dummies","4c39f53b":"## Training using dataset w\/o outliers"}}