{"cell_type":{"48a5d565":"code","d9870496":"code","824fe3c1":"code","cbfd7b66":"code","fe867418":"code","b9909d61":"code","5299cf86":"code","4239eda3":"code","8f621217":"code","1eb8d638":"code","80c07a61":"code","98aa37d2":"code","0d390663":"code","965a2eb1":"code","5a4555ec":"code","ffac6af2":"code","b0d38b3f":"markdown","864b2a94":"markdown","e2d90f7c":"markdown","44b4de9c":"markdown","4f1c9d40":"markdown","c44afda2":"markdown"},"source":{"48a5d565":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9870496":"from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n","824fe3c1":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntrain_df.head()","cbfd7b66":"\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest_df.head()","fe867418":"train_df[train_df[\"target\"] == 0][\"text\"].values[2]","b9909d61":"train_df[train_df[\"target\"] == 1][\"text\"].values[1]","5299cf86":"count_vectorizer = feature_extraction.text.CountVectorizer()\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])","4239eda3":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","8f621217":"train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\n\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])","1eb8d638":"## Our vectors are really big, so we want to push our model's weights\n## toward 0 without completely discounting different words - ridge regression \n## is a good way to do this.\nclf = linear_model.RidgeClassifier()","80c07a61":"scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","98aa37d2":"clf.fit(train_vectors, train_df[\"target\"])","0d390663":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","965a2eb1":"sample_submission[\"target\"] = clf.predict(test_vectors)","5a4555ec":"sample_submission.head()","ffac6af2":"sample_submission.to_csv(\"submission.csv\", index=False)","b0d38b3f":"The Model","864b2a94":"Getting tweets based on whether they represent a disaster or not. 0 for Not a disaster tweet, 1 for disaster tweet","e2d90f7c":"Let's test our model and see how well it does on the training data. For this we'll use cross-validation - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n\nMetric: F1 score","44b4de9c":"We'll use scikit-learn's CountVectorizer to count the words in each tweet and turn them into data our machine learning model can process.\nA vector is, in this context, a set of numbers that a machine learning model can work with.","4f1c9d40":"The above tells us that:\n\nThere are 54 unique words (or \"tokens\") in the first five tweets.\nThe first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\nNow let's create vectors for all of our tweets.","c44afda2":"we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n\nWhat we're assuming here is a linear connection. So let's build a linear model and see!"}}