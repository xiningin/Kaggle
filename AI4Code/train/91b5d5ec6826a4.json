{"cell_type":{"07ea17fc":"code","409edc16":"code","744dd005":"code","d3b48f76":"code","a52dc82f":"code","01b79905":"code","4dc6eeef":"code","6d0cb9a3":"code","ec03c255":"code","bc038ab3":"code","ce77858e":"code","2980576d":"code","c779ce48":"code","7936d714":"code","6b372471":"code","a09d2245":"code","4324fb29":"code","f0026776":"code","976a48d6":"code","a0e2a6a0":"code","07ca79a5":"code","3ea24690":"code","dab73c90":"code","657e1534":"code","e30bc1a4":"code","f97e38e7":"code","367a6b4a":"code","e192a230":"code","5a360b7b":"code","268df7ef":"code","a80be201":"code","e146379d":"code","198c7606":"code","b34021fa":"code","f701bc27":"code","9ffeb5ce":"code","a3272312":"code","a28d4357":"code","945777df":"code","ca6bb797":"code","548bce0c":"code","70dc51ff":"code","d7f94473":"code","8e7b744a":"code","7910dada":"code","8ecac789":"code","4c20da5a":"code","da6efcbb":"code","b6c6c598":"code","791e7262":"code","d0c084b3":"code","27e98920":"code","b85497b3":"code","01d4e36d":"code","d710ad5e":"code","3dd16c03":"markdown","8cf379af":"markdown","0194fd52":"markdown","064c86b5":"markdown","5f4d8e1c":"markdown","a581957a":"markdown","0931e78d":"markdown","9c59c2c8":"markdown","65639289":"markdown"},"source":{"07ea17fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","409edc16":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nexample_df = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","744dd005":"full_data_df = pd.read_csv('https:\/\/www.openml.org\/data\/get_csv\/16826755\/phpMYEkMl')","d3b48f76":"test_df = pd.merge(test_df,full_data_df[['survived']],left_on='PassengerId',right_index=True,how='left').drop_duplicates()\ntest_df = test_df.dropna(subset=['survived'])\ny_test = test_df['survived']","a52dc82f":"import seaborn as sns\nsns.pairplot(train_df[['Sex','Age','Fare','Pclass','Survived']], hue=\"Survived\")","01b79905":"train_df['Survived'].unique()","4dc6eeef":"test_df.info()","6d0cb9a3":"train_df['Age'].unique()","ec03c255":"# Understand object-type summary statistics\ntrain_df.describe(include=['O'])","bc038ab3":"# First 5 rows\ntrain_df.head()","ce77858e":"# Do we have a categorical variable or do we have a floating variable with integer representation\ntrain_df['Parch'].unique()","2980576d":"# One-Hot Encoding of categorical variable\n# Each category gets its own column (think vectorization)\n\n# Pclass\n\nclasses_df = pd.get_dummies(train_df['Pclass'], prefix='class')\nmerge_train_df = pd.merge(train_df,classes_df,left_index=True,right_index=True,how='left')\n\nclasses_df = pd.get_dummies(test_df['Pclass'], prefix='class')\nmerge_test_df = pd.merge(test_df,classes_df,left_index=True,right_index=True,how='left')\n\n# Embarked\n\nmerge_train_df['Embarked'].fillna('No info', inplace=True)\nclasses_df = pd.get_dummies(merge_train_df['Embarked'], prefix='embarked')\nmerge_train_df = pd.merge(merge_train_df,classes_df,left_index=True,right_index=True,how='left')\n\nmerge_test_df['Embarked'].fillna('No info', inplace=True)\nclasses_df = pd.get_dummies(merge_test_df['Embarked'], prefix='embarked')\nmerge_test_df = pd.merge(merge_test_df,classes_df,left_index=True,right_index=True,how='left')\n\n# Sex\n\nclasses_df = pd.get_dummies(merge_train_df['Sex'], prefix='gender')\nmerge_train_df = pd.merge(merge_train_df,classes_df,left_index=True,right_index=True,how='left')\n\nclasses_df = pd.get_dummies(merge_test_df['Sex'], prefix='gender')\nmerge_test_df = pd.merge(merge_test_df,classes_df,left_index=True,right_index=True,how='left')\n\n","c779ce48":"# Fill NaN with the median value of the observation\n## Why can we do it?? Because only 891-714 observations are missing, and if we want to work with this features, we have to fill it up with a value that is not biasing the outcome, e.g. the median\nmerge_train_df[merge_train_df['Age'].isnull()]","7936d714":"merge_train_df['Age'].hist()\n# we use the median because the distribution is not normal","6b372471":"# Fill up missing values for age using the median\nmerge_train_df['Age'].fillna(merge_train_df['Age'].dropna().median(), inplace=True)\nmerge_test_df['Age'].fillna(merge_test_df['Age'].dropna().median(), inplace=True)\n\n# Fill up the missing value for fare using the mean of Pclass == 3\nmerge_test_df['Fare'].fillna(merge_train_df['Fare'][merge_train_df['Pclass']==3].mean(), inplace=True)","a09d2245":"merge_test_df.info()","4324fb29":"merge_train_df['Fare'].hist()","f0026776":"merge_train_df['Fare'].hist()","976a48d6":"merge_train_df['Fare'][merge_train_df['Survived']==1].hist()","a0e2a6a0":"merge_train_df['Fare'][merge_train_df['Survived']==0].hist()","07ca79a5":"ax = sns.displot(data=merge_train_df, x=\"Fare\", hue=\"Survived\",multiple='stack')","3ea24690":"merge_train_df['Fare'][merge_train_df['Survived']==0].max()","dab73c90":"# Winsorization helps you to reduce the impact of outliers on your model\n# Another way would be to use a quantitative threshold, such as the quantile of the distribution, such as merge_train_df['Fare'].quantile(.99), where your decision is to set the threshold at .99\n# In our example we set the threshold to the largest value of the category of the target with the relative smaller maximum value\nmerge_train_df['Fare_preprocessed'] = merge_train_df['Fare'].clip(upper=merge_train_df['Fare'][merge_train_df['Survived']==0].max())\nmerge_test_df['Fare_preprocessed'] = merge_test_df['Fare'].clip(upper=merge_train_df['Fare'][merge_train_df['Survived']==0].max())","657e1534":"merge_train_df['Fare_preprocessed'][merge_train_df['Survived']==1].hist()","e30bc1a4":"# Normalize continuous variables\nmerge_train_df['fare_normalized'] = (merge_train_df['Fare_preprocessed'] - merge_train_df['Fare_preprocessed'].mean())\/merge_train_df['Fare_preprocessed'].std()\nmerge_test_df['fare_normalized'] = (merge_test_df['Fare_preprocessed'] - merge_train_df['Fare_preprocessed'].mean())\/merge_train_df['Fare_preprocessed'].std()","f97e38e7":"merge_train_df['age_normalized'] = (merge_train_df['Age'] - merge_train_df['Age'].mean())\/merge_train_df['Age'].std()\nmerge_test_df['age_normalized'] = (merge_test_df['Age'] - merge_train_df['Age'].mean())\/merge_train_df['Age'].std()","367a6b4a":"merge_train_df.columns","e192a230":"X_train_df = merge_train_df[['class_1', 'class_2','class_3', 'embarked_C', 'embarked_Q', 'embarked_S', 'gender_female','gender_male','fare_normalized','age_normalized']].copy()\nX_test_df = merge_test_df[['class_1', 'class_2','class_3', 'embarked_C', 'embarked_Q', 'embarked_S', 'gender_female','gender_male','fare_normalized','age_normalized']].copy()\n","5a360b7b":"y_train = merge_train_df['Survived']","268df7ef":"from sklearn.linear_model import LogisticRegression\n\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train_df, y_train)\n#Y_pred = logreg.predict(X_test_df)\n# Check the accuracy based solely on the train dataset. There are NO test targets \/ label (for the moment)\nacc_log = round(logreg.score(X_train_df, y_train) * 100, 2)\nacc_log","a80be201":"import statsmodels.api as sm","e146379d":"logreg_sm = sm.Logit(y_train, X_train_df)\nlogit_res = logreg_sm.fit()\nprint(logit_res.summary())","198c7606":"y_pred = logreg.predict(X_test_df)","b34021fa":"from sklearn.metrics import confusion_matrix, accuracy_score\n\nconfusion_matrix(y_test,y_pred)\n","f701bc27":"accuracy_score(y_test,y_pred)","9ffeb5ce":"# Multicollinearity problems in classification models\nX_train_df","a3272312":"X_train_df","a28d4357":"# we create an interaction variable \nX_train_df['class_1_fare'] = X_train_df['class_1']*X_train_df['fare_normalized'][X_train_df['class_1']==True].mean()\nX_train_df['class_2_fare'] = X_train_df['class_2']*X_train_df['fare_normalized'][X_train_df['class_2']==True].mean()\nX_train_df['class_3_fare'] = X_train_df['class_3']*X_train_df['fare_normalized'][X_train_df['class_3']==True].mean()\n\nX_test_df['class_1_fare'] = X_test_df['class_1']*X_train_df['fare_normalized'][X_train_df['class_1']==True].mean()\nX_test_df['class_2_fare'] = X_test_df['class_2']*X_train_df['fare_normalized'][X_train_df['class_1']==True].mean()\nX_test_df['class_3_fare'] = X_test_df['class_3']*X_train_df['fare_normalized'][X_train_df['class_1']==True].mean()\n","945777df":"# Look-ahead Bias is to take aggregate information NOT from the test data because we assume that we have not seen it, yet","ca6bb797":"# check the new accuracy having introduced the interaction term \nlogreg = LogisticRegression()\nlogreg.fit(X_train_df, y_train)\n#Y_pred = logreg.predict(X_test_df)\n# Check the accuracy based solely on the train dataset. There are NO test targets \/ label (for the moment)\nacc_log = round(logreg.score(X_train_df, y_train) * 100, 2)\nacc_log","548bce0c":"# we merge class and fare normalized \nX_train_df['class_1_fare2'] = X_train_df['class_1']*X_train_df['fare_normalized']\nX_train_df['class_2_fare2'] = X_train_df['class_2']*X_train_df['fare_normalized']\nX_train_df['class_3_fare2'] = X_train_df['class_3']*X_train_df['fare_normalized']\n\nX_test_df['class_1_fare2'] = X_test_df['class_1']*X_test_df['fare_normalized']\nX_test_df['class_2_fare2'] = X_test_df['class_2']*X_test_df['fare_normalized']\nX_test_df['class_3_fare2'] = X_test_df['class_3']*X_test_df['fare_normalized']","70dc51ff":"X_train_df","d7f94473":"X_train2_df = X_train_df\nX_test2_df = X_test_df\nlogreg = LogisticRegression()\nlogreg.fit(X_train2_df, y_train)\n#Y_pred = logreg.predict(X_test_df)\n# Check the accuracy based solely on the train dataset. There are NO test targets \/ label (for the moment)\ny_pred = logreg.predict(X_test2_df)\n\nacc_log = round(logreg.score(X_train2_df, y_train) * 100, 2)\nacc_log\n","8e7b744a":"accuracy_score(y_test,y_pred)","7910dada":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train2_df,y_train).predict(X_test2_df)\naccuracy_score(y_test,y_pred)","8ecac789":"import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input","4c20da5a":"x","da6efcbb":"print('Building model sequentially 1...')\nmodel = Sequential()\nmodel.add(Input(shape=(X_train2_df.shape[1],))) # Input\nmodel.add(Dense(512, activation='relu')) # Encoder, we translate data into y = wx+b\n#model.add(Dropout(0.5, seed=42)) # Dropout Layer, to make sure you are not overfitting on x\nmodel.add(Dense(1))  # Representation Layer\nmodel.add(Activation('sigmoid')) # Outout Layer of normalized data","b6c6c598":"model.layers","791e7262":"opt = Adam(learning_rate=0.01)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])","d0c084b3":"# Translate labels into numpy representation\ny_train2 = np.asarray(y_train).astype('float32').reshape((-1,1))\ny_test2 = np.asarray(y_test).astype('float32').reshape((-1,1))","27e98920":"%%time\nbatch_size = 32\nepochs = 5\nhistory = model.fit([X_train2_df.values], y_train2,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=0,\n                    validation_split=0.2)","b85497b3":"y_pred = model.predict([X_test2_df.values])","01d4e36d":"from sklearn.metrics import classification_report, confusion_matrix\ny_pred = (y_pred>0.5)\nprint(classification_report(y_test, y_pred, target_names=['True','False']))","d710ad5e":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","3dd16c03":"# Why do we look at the accuracy instead of the loss itself?\n\n1. \"The means to the end\" - means: we maniulate \"loss\" directly, so it is a model \"parameter\"; end: the purpose of the loss minimization is to improve predictability, which is indirectly represented by accuracy - loss is used internally in the model, accuracy is used for inter-model comparison\n2. It enables comparability - I can use \"accuracy\" to compare different models\n","8cf379af":"## Why is the prediction result getting worse? - model complexity","0194fd52":"## Categorical Variables Preprocessing","064c86b5":"# Preprocessing","5f4d8e1c":"## Baseline Model","a581957a":"## Nota bene\nWhen you work with classification models, make sure your layer architecture takes into account the characteristics of your target variable. Is it a binary or a multi-class variable?\n","0931e78d":"# Deep Learning Model specification","9c59c2c8":"[Holistic Overview of data preprocessing and simple model specifications for the Titanic Dataset](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)","65639289":"## Feature Engineering to improve the base model"}}