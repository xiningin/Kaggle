{"cell_type":{"2b348f61":"code","c7a2bcf7":"code","a91a0418":"code","e22924ce":"code","d0c1274e":"code","1662c386":"code","0f591563":"code","23b8b9c9":"code","96fa1daf":"code","e11a6cff":"code","47274f52":"code","ecd875bd":"code","f755df64":"code","b45e2614":"code","de6a1d75":"code","39ba8ded":"code","b714b678":"code","2cf3011a":"code","8dbd272b":"code","05f909b9":"code","e8dd4e55":"code","07bff0ba":"code","459ca9f4":"code","f9cd92a3":"code","ca503f13":"code","2a4499a9":"code","cd2b9ad7":"code","afd3e9fd":"code","7d89c116":"code","199fcde6":"code","897d57db":"code","eaf6ea98":"code","d4d80169":"code","11864713":"code","9a4d87ff":"code","6fb6503e":"code","5df5b89b":"code","13d8cf72":"code","fa33e629":"code","67bd5162":"code","8fd0227d":"code","19265384":"code","913ddbfc":"code","476a7a22":"code","5e415302":"code","76de4892":"code","3f0c7b76":"code","9d9a4669":"code","e395fff8":"code","e593deb9":"code","fc713441":"code","0d3752f6":"code","047d31df":"code","e475dca3":"code","af7f112a":"code","131f5138":"code","3b9a00df":"code","e89fbd11":"code","3591d6d0":"code","c6db4a17":"markdown","735f0c80":"markdown","1bde501d":"markdown","21ec5f6f":"markdown","9760687d":"markdown","5a4a76a3":"markdown","c46a9ddc":"markdown","463d6c28":"markdown","47bfba26":"markdown","4e3c6545":"markdown","4479e61d":"markdown","1b710184":"markdown","f2cca4b4":"markdown","9b175766":"markdown","84a96a34":"markdown","d734cff8":"markdown","01b18551":"markdown","fe05a3c9":"markdown","20194356":"markdown","3e2fae21":"markdown","bc0ab2f7":"markdown","a8d63391":"markdown"},"source":{"2b348f61":"!pip install nmslib cdlib","c7a2bcf7":"import re\nimport nmslib\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport networkx as nx\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.sparse import csr_matrix, coo_matrix\nfrom scipy.sparse.csgraph import connected_components\nfrom cdlib.algorithms import infomap\nfrom sklearn.manifold import TSNE","a91a0418":"bot_users = pd.read_csv(\"\/kaggle\/input\/disqus-social-bot-networks\/potential_social_botnets.csv\", delimiter=',', encoding='ISO-8859-2')\nbot_users.head(3)","e22924ce":"users = pd.read_csv(\"\/kaggle\/input\/disqus-social-bot-networks\/anonymized_disqus_user_data.csv\", delimiter=',', encoding='ISO-8859-2')\nusers.head(3)","d0c1274e":"len(users) - len(users.drop_duplicates())","1662c386":"# De-duplicate the user data\n# drop duplicate user names by keeping only the first appearance\nusers.drop_duplicates(subset='username', keep='first', inplace=True, ignore_index=True)","0f591563":"# Are all potential bot users included in the users dataset?\nbot_usernames = set(bot_users.username.to_list())\nall_usernames = set(users.username.to_list())\nlen(bot_usernames) == len(all_usernames.intersection(bot_usernames))","23b8b9c9":"def find_first_k_sequential_digits_in_string(string, k=2):\n    \"\"\"\n    Function inspired by: https:\/\/www.geeksforgeeks.org\/python-first-k-consecutive-digits-in-string\/\n    \n    Example usage:\n        find_first_k_sequential_digits_in_string(\"allen1\", 2) --> \"\"\n        find_first_k_sequential_digits_in_string(\"allen123\", 2) --> \"12\"\n        find_first_k_sequential_digits_in_string(\"allen13\", 2) --> \"\"\n    \"\"\"\n    temp = re.search('\\d{% s}'% k, string)\n    digits_string = (temp.group(0) if temp else '')\n    if len(digits_string) == 1:\n        return digits_string\n    elif len(digits_string) > 1:\n        if all([int(digits_string[d]) + 1 == int(digits_string[d + 1]) for d in range(len(digits_string) - 1)]):\n            return digits_string\n        else:\n            return \"\"\n    else:\n        return \"\"\n\n\ndef check_for_2_to_4_sequential_digits(string):\n    condition = (\n        (find_first_k_sequential_digits_in_string(string, k=2) != \"\") or \n        (find_first_k_sequential_digits_in_string(string, k=3) != \"\") or \n        (find_first_k_sequential_digits_in_string(string, k=4) != \"\")\n    )\n    if condition:\n        return 1\n    else:\n        return 0","96fa1daf":"# Create some features that might help distinguish between bots and normal users\nusers[\"potential_bot\"] = np.where(users[\"username\"].isin(bot_usernames), 1, 0)\nusers[\"username\"] = users[\"username\"].astype(str)\nusers[\"username_ends_in_sequential_digits\"] = users[\"username\"].apply(lambda x: check_for_2_to_4_sequential_digits(x))","e11a6cff":"# Convert date string to datetime\nusers[\"creation_date\"] = pd.to_datetime(users.creation_date.str.split(' ').str[0])","47274f52":"# Might potential_bot be influenced by username ending in sequential digits?\npd.crosstab(users[\"potential_bot\"], users[\"username_ends_in_sequential_digits\"], normalize=\"columns\")","ecd875bd":"users.creation_date.value_counts().plot(kind=\"line\", title=\"Number of Accounts Created by Date\")","f755df64":"users.creation_date.value_counts().reset_index().max()","b45e2614":"users.loc[users.creation_date != datetime(2012, 7, 12), 'creation_date'].value_counts().mean()","de6a1d75":"pd.crosstab(users.potential_bot, users.is_major_email_provider, normalize=\"index\")","39ba8ded":"pd.crosstab(users.potential_bot, users.is_common_password, normalize=\"index\")","b714b678":"bot_comments = pd.read_csv(\"\/kaggle\/input\/disqus-social-bot-networks\/botnet_comments.csv\", delimiter=',', encoding='ISO-8859-2')","2cf3011a":"print(len(bot_comments), \"total rows\\n\")\nfor col_index, col in enumerate(list(bot_comments.columns)):\n    print(col_index, col)","8dbd272b":"bot_comments.head()","05f909b9":"# clean up comment date\nbot_comments.rename(columns={\"reatedAt\": \"created_at\"}, inplace=True)\nbot_comments[\"created_at\"] = pd.to_datetime(bot_comments.created_at.str.split(\"T\").str[0])\n\n# convert booleans to binary\nfor col in bot_comments.columns[bot_comments.dtypes==\"bool\"]:\n    bot_comments[col] = bot_comments[col].astype(int)","e8dd4e55":"pd.crosstab(bot_comments.isFlagged, bot_comments.numReports)","07bff0ba":"# drop fields I don't want - these are either almost all the same value or replaceable by another field\nbot_comments.drop(\n    [\n        \"canVote\", \"editableUntil\", \"isApproved\", \"isAtFlagLimit\", \n        \"isDeleted\", \"isDeletedByAuthor\", \"isHighlighted\", \n        \"isNewUserNeedsApproval\", \"isSpam\", \"media\", \"moderationLabels\", \n        \"sb\", \n    ], \n    axis=1, \n    inplace=True\n)","459ca9f4":"# Pull over some fields from the users data\n\n# bot_comments[\"potential_bot\"] = bot_comments[\"username\"].map(pd.Series(users.potential_bot.values, index=users.username).to_dict())\n# bot_comments[\"acct_creation_date\"] = bot_comments[\"username\"].map(pd.Series(users.creation_date.values, index=users.username).to_dict())\n\nbot_comments = pd.merge(\n    bot_comments, \n    users[[\"username\", \"potential_bot\", \"creation_date\", \"is_major_email_provider\", \"is_common_password\", \"username_ends_in_sequential_digits\"]], \n    how=\"left\", \n    on=\"username\"\n)\nbot_comments.rename(columns={\"creation_date\": \"acct_creation_date\"}, inplace=True)\nprint(len(bot_comments), \"total rows after merge\")","f9cd92a3":"# How many comments were made by a user who does not exist in users data?\nbot_comments.potential_bot.isna().sum()","ca503f13":"# Create new features that might help identify bots\nbot_comments[\"days_between_acct_creation_and_comment\"] = (bot_comments.created_at - bot_comments.acct_creation_date).dt.days\nbot_comments.loc[bot_comments.days_between_acct_creation_and_comment < 0, \"days_between_acct_creation_and_comment\"] = 0\nbot_comments[\"nbr_comments_in_forum\"] = bot_comments.groupby([\"username\", \"forum\"])[\"message\"].transform(\"count\")\nbot_comments.head()","2a4499a9":"# Inspect distribution of numeric variables by potential_bot\nfor col in [\"dislikes\", \"likes\", \"numReports\", \"points\", \"days_between_acct_creation_and_comment\", \"nbr_comments_in_forum\"]:\n    # sns.boxplot(x=\"potential_bot\", y=col, data=bot_comments)\n    sns.distplot(np.log(bot_comments.loc[bot_comments.potential_bot == 0, col]), hist=False, kde=True, color=\"blue\")\n    sns.distplot(np.log(bot_comments.loc[bot_comments.potential_bot == 1, col]), hist=False, kde=True, color=\"red\")\n    plt.title(f\"Log Scaled Distribution of {col} by Potential Bot\")\n    plt.legend(loc='upper right', labels=['Normal Users', 'Potential Bots'])\n    plt.show()","cd2b9ad7":"for col in [\n    \"isEdited\", \"isFlagged\", \"is_major_email_provider\", \n    \"is_common_password\",  \n]:\n    print(pd.crosstab(bot_comments[\"potential_bot\"], bot_comments[col], normalize=\"index\"), \"\\n\")","afd3e9fd":"n = 10\ntop_n_forums = bot_comments.forum.value_counts()[:n].reset_index().rename(columns={\"forum\": \"comments\", \"index\": \"forum\"})\ntop_n_forums_list = top_n_forums.forum.tolist()\ntop_n_forums","7d89c116":"# Plot the number of comments over time by the top n forums\n\ncomments_by_forum = bot_comments.loc[bot_comments[\"forum\"].isin(top_n_forums_list), [\"forum\", \"created_at\"]]\\\n    .value_counts()\\\n    .reset_index()\\\n    .rename(columns={0: \"nbr_comments\"})\ncomments_by_forum.sort_values(\"created_at\", inplace=True)\nmoving_average_nbr_days = 21\ncomments_by_forum[f\"nbr_comments_{moving_average_nbr_days}dma\"] = comments_by_forum.groupby(\"forum\")[\"nbr_comments\"]\\\n    .transform(lambda x: x.rolling(moving_average_nbr_days).mean())\n\n# Create a color map to be used for the plots by forum, so that each forum is colored the same across plots\ncm = plt.get_cmap('rainbow')\n# Get the RGBA value from the colormap, cm, so that each entity gets a distinct color\nc = [cm(1.0 * i\/len(set(comments_by_forum.forum))) for i in range(len(set(comments_by_forum.forum)))]\ncolormap = {i:j for i, j in zip(set(comments_by_forum.forum), c)}\n\nsns.set(rc={'figure.figsize':(17, 8)})\nsns.lineplot(\n    x=\"created_at\", \n    y=f\"nbr_comments_{moving_average_nbr_days}dma\", \n    hue=\"forum\", \n    palette = colormap, \n    data=comments_by_forum[comments_by_forum.created_at <= (bot_comments.acct_creation_date.max() + timedelta(days=moving_average_nbr_days*2))]\n)\nplt.title(\"Nbr Comments Over Time by Forum\")\nplt.show()","199fcde6":"# Compare the comment dates to account creations\n\nacct_creations_by_forum = bot_comments.loc[bot_comments[\"forum\"].isin(top_n_forums_list), [\"forum\", \"acct_creation_date\"]]\\\n    .value_counts()\\\n    .reset_index()\\\n    .rename(columns={0: \"nbr_accts_created\"})\nacct_creations_by_forum.sort_values(\"acct_creation_date\", inplace=True)\nmoving_average_nbr_days = 21\nacct_creations_by_forum[f\"nbr_accts_created_{moving_average_nbr_days}dma\"] = acct_creations_by_forum.groupby(\"forum\")[\"nbr_accts_created\"]\\\n    .transform(lambda x: x.rolling(moving_average_nbr_days).mean())\n\nsns.set(rc={'figure.figsize':(17, 8)})\nsns.lineplot(\n    x=\"acct_creation_date\", \n    y=f\"nbr_accts_created_{moving_average_nbr_days}dma\", \n    hue=\"forum\", \n    palette=colormap, \n    data=acct_creations_by_forum\n)\nplt.title(\"Account Creations Over Time by Forum\")\nplt.show()","897d57db":"# drop all but the desired forums\nbot_comments = bot_comments[bot_comments[\"forum\"].isin([\"ajblogs\", \"allkpop\", \"bdn\", \"foxnews\", \"worldstar\"])].copy().reset_index(drop=True)","eaf6ea98":"# make sure all messages are strings\nbot_comments[\"message\"] = bot_comments[\"message\"].astype(str)\n\n# remove tags from comments\nbot_comments[\"message\"] = bot_comments[\"message\"].str.replace(r\"<.*?>\", '', regex=True)\n\n# remove non ASCII characters from comments\n# del_chars =  \" \".join([chr(i) for i in list(range(32)) + list(range(127, 256))])\n# trans = str.maketrans(del_chars, \" \" * len(del_chars))\n# bot_comments[\"message\"] = bot_comments[\"message\"].apply(lambda x: x.translate(trans))","d4d80169":"# what percentage of the parent IDs are not null?\nround((len(bot_comments) - bot_comments.parent.dropna().count()) \/ len(bot_comments), 2) * 100","11864713":"def batch(iterable, batch_size=1):\n    \"\"\"\n    Used to create batches of equal size, batch_size\n    \n    Example usage:\n        data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # list of data\n        \n        for x in batch(data, 3):\n            print(x)\n        \n        # Output\n        \n        [0, 1, 2]\n        [3, 4, 5]\n        [6, 7, 8]\n        [9, 10]\n    \"\"\"\n    iterable_len = len(iterable)\n    for ndx in range(0, iterable_len, batch_size):\n        yield iterable[ndx:min(ndx + batch_size, iterable_len)]\n\n\ndef get_embeddings(text_input, model, batch_size=256):\n    \"\"\"\n    Runs text through the model and produces the embeddings\n    \n    :param text_input: a list where each item is a document (a comment from this dataset)\n    :param model: a TensorFlow embedding model\n    :param batch_size: integer representing how many samples to include in a batch\n    \"\"\"\n    embeddings = []\n    # helper variables to track progress\n    nbr_batches = int(np.ceil(len(text_input) \/ batch_size))\n    current_batch = 1\n    \n    for batch_indices in batch(iterable=range(len(text_input)), batch_size=batch_size):\n        progress = round(100 * current_batch \/ nbr_batches, 2)\n        if progress % 10 == 0:\n            print(f\"Embedding progress: {progress}%\")\n        \n        # grab the records for this batch\n        batch_records = [text_input[idx] for idx in batch_indices]\n        \n        # forward pass over the input\n        model_output = model(batch_records)\n        \n        # save the embeddings\n        embeddings.append(model_output.numpy())\n        \n        current_batch += 1\n        \n    # convert the list of embeddings to a numpy array, then scale it\n    print(\"Converting list to Numpy array and scaling to (0, 1) range...\")\n    scaler = MinMaxScaler()\n    embeddings = np.array(\n        [np.array(i) for i in scaler.fit_transform(np.vstack(embeddings)).tolist()]\n    )\n    \n    return embeddings\n","9a4d87ff":"# Load Universal Sentence Encoder (USE), which comes with pre-trained weights\nmodule_url = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\"\nmodel = hub.load(module_url)\nprint (f\"module {module_url} loaded\")","6fb6503e":"# Get an array of embeddings for the comments\nembeddings = get_embeddings(text_input=bot_comments['message'].tolist(), model=model, batch_size=256)","5df5b89b":"embeddings.shape","13d8cf72":"# Fast approximation of nearest neighbors, or pairwise distances\nindex_params = {\n    'M': 15,\n    'indexThreadQty': 4,\n    'efConstruction': 100,\n    'post': 0\n}\nindex = nmslib.init(method='hnsw', space='l2')\nindex.addDataPointBatch(embeddings)\nindex.createIndex(index_params, print_progress=True)","fa33e629":"# get nearest neighbors - list of tuples of (indices of neighbors, distances to neighbors)\nnbr_neighbors = 10\n# xn = index.knnQueryBatch(embeddings, k=nbr_neighbors, num_threads=4)\n# The commented line above would work, but the order gets scrambled.  The line below preserves ordering.\nxn = [index.knnQueryBatch(np.array(i).reshape(1,-1), k=nbr_neighbors, num_threads=4)  for i in embeddings.tolist()]","67bd5162":"len(xn)","8fd0227d":"# view nearest neighbors for a sample comment\npd.set_option('display.max_colwidth', None)  # allow full text to be printed\nbot_comments.loc[bot_comments.index.isin(xn[0][0][0].tolist()), \"message\"].head(10)","19265384":"# FYI - this is how the coo_matrix will look\n# notice there are 8 distinct IDs but the edge lists are 9 items long\n# the matrix will have 1 row and 1 column per unique ID\ntest_a = [0, 0, 0, 0, 1, 1, 1, 2, 3]\ntest_b = [3, 4, 5, 6, 2, 3, 7, 1, 6]\ntest_c = [0.1, 0.1, 0.3, 0.4, 0.5, 0.9, 0.8, 0.2, 0.1]\ntest_m = coo_matrix((test_c, (test_a, test_b)), shape=(8, 8))\ntest_m.todense()","913ddbfc":"# pre-compute a sparse matrix of distances\ni, j, distances = zip(*(([i] * len(neighbors[0][0]), neighbors[0][0].tolist(), neighbors[0][1].tolist()) for i, neighbors in enumerate(xn)))\n# not all comments have nbr_neighbors neighbors, so remember that when the cluster labels are created\ni = [m for n in i for m in n]\nj = [m for n in j for m in n]\n# invert the distances to get similarity, 0 distance set to -1 temporarily\ndistances = [-1 if m == 0 else 1 \/ m for n in distances for m in n]\n# now that distance is similarity, revert -1's to max score + 1 (perfect similarity)\nperfectly_similar = np.max(distances) + 1.0\ndistances = [perfectly_similar if d == -1 else d for d in distances]\nscaler = MinMaxScaler()\ndistances = scaler.fit_transform(np.array(distances).reshape(-1, 1)).tolist()\ndistances = [d[0] for d in distances]\nadj_matrix = coo_matrix((distances, (i, j)), shape=(len(bot_comments), len(bot_comments)))","476a7a22":"print(len(i), len(j), len(distances))","5e415302":"# Method 1: Infomap\n# convert the sparse matrix to a NetworkX graph, then do community detection with Infomap\ng = nx.convert_matrix.from_scipy_sparse_matrix(adj_matrix, parallel_edges=False, edge_attribute='weight')\ncomms = infomap(g)\ncluster_label_map = comms.to_node_community_map()\n\nbot_comments[\"cluster_label\"] = bot_comments.index.map(cluster_label_map)\nbot_comments[\"cluster_label\"] = [','.join(map(str, l)) for l in bot_comments[\"cluster_label\"]]\nbot_comments[\"cluster_label\"] = bot_comments[\"cluster_label\"].astype(int)","76de4892":"# Method 2: Connected Components\n# Get cluster labels from connected components in the graph - the transitive property will link similar comments\n# nbr_components, labels = connected_components(adj_matrix, directed=False, return_labels=True)\n# cluster_label_map_cc = {idx: labels[idx] for idx in i}  # dict will collapse duplicates in i so that there is a 1:1 mapping to label\n# print(\"Number of Clusters:\", nbr_components)\n# bot_comments[\"cluster_label\"] = bot_comments.index.map(cluster_label_map_cc)","3f0c7b76":"# Inspect the distribution of cluster sizes\n# ignore cluster label -1 and any clusters with only 1 member\ncluster_sizes = bot_comments.loc[bot_comments[\"cluster_label\"] != -1, \"cluster_label\"].value_counts().reset_index()\ncluster_sizes = cluster_sizes[cluster_sizes[\"cluster_label\"] > 1]\ncluster_sizes.rename(columns={\"index\": \"cluster_label\", \"cluster_label\": \"cluster_size\"}, inplace=True)\ncluster_size_freq = cluster_sizes.cluster_size.value_counts().reset_index()\nsns.barplot(data=cluster_size_freq, x=\"index\", y=\"cluster_size\")\nplt.title(\"Cluster Size Frequency\")\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Cluster Size\")\nplt.show()\n\n# Save cluster size as a new column\nbot_comments[\"cluster_size\"] = bot_comments.groupby([\"cluster_label\"])[\"id\"].transform(\"count\")","9d9a4669":"pd.set_option('display.max_colwidth', None)  # allow full text to be printed","e395fff8":"# Inspect a representative sentence for each cluster, for the first 20 clusters\nbot_comments.groupby(\"cluster_label\")[\"message\"].first().reset_index().head(20)","e593deb9":"# Inspect a single cluster: change the cluster label number to look at different clusters\nbot_comments.loc[bot_comments[\"cluster_label\"] == 1, \"message\"].head(20)","fc713441":"print(\"Number of distinct users:\", len(set(bot_comments.username)))\nprint(\"Number of distinct semantic clusters:\", len(set(bot_comments.cluster_label)))\nprint(\"Total rows:\", len(bot_comments))","0d3752f6":"user_features = [\n    \"days_between_acct_creation_and_comment\", \"is_major_email_provider\", \n    \"is_common_password\", \"username_ends_in_sequential_digits\", \"potential_bot\", \n]\n\n# scale features to ensure that the distances are not dominated by features with larger ranges\nbot_comments_scaled = bot_comments[user_features].copy()\n# log transform the features with long tails\nbot_comments_scaled[\"days_between_acct_creation_and_comment\"] = np.log(bot_comments[\"days_between_acct_creation_and_comment\"] + 1e-5)\nscaler = MinMaxScaler()\nbot_comments_scaled = pd.DataFrame(scaler.fit_transform(bot_comments_scaled.to_numpy()), columns=user_features)\n\nnon_bot_exemplar = bot_comments_scaled[bot_comments_scaled.potential_bot == 0]\\\n    .drop(\"potential_bot\", axis=1).mean().to_numpy()\n\nbot_exemplar = bot_comments_scaled[bot_comments_scaled.potential_bot == 1]\\\n    .drop(\"potential_bot\", axis=1).mean().to_numpy()","047d31df":"print(user_features[:-1], \"\\n\", \"Bot exemplar:\", bot_exemplar, \"\\n\", \"Non-Bot Exemplar:\", non_bot_exemplar)","e475dca3":"def calculate_l2_dist(feature_vector, exemplar):\n    # invert the distance to make it so that larger values = more similar\n    return 1 \/ np.linalg.norm(np.array([feature_vector, exemplar]))\n\n# calculate the distance from every sample to the exemplars\nnon_bot_dist = bot_comments_scaled.drop(\"potential_bot\", axis=1)\\\n    .apply(lambda x: calculate_l2_dist(feature_vector=x, exemplar=non_bot_exemplar), axis=1)\nbot_dist = bot_comments_scaled.drop(\"potential_bot\", axis=1)\\\n    .apply(lambda x: calculate_l2_dist(feature_vector=x, exemplar=bot_exemplar), axis=1)\n\n# subtract non-bot from bot and scale to range (0, 1)\n# this will give a number between 0 and 1, where being closer to 1 means being more similar to a bot\nbot_similarity = bot_dist - non_bot_dist\nscaler = MinMaxScaler()\nbot_similarity = scaler.fit_transform(bot_similarity.values.reshape(-1, 1))\nbot_comments[\"user_similarity_to_bot\"] = bot_similarity","af7f112a":"# find the semantic cluster with the highest proportion of bots\nbots_by_cluster = bot_comments.groupby([\"cluster_label\", \"username\"])[\"potential_bot\"].sum().reset_index()\nbots_by_cluster[\"potential_bot\"] = np.where(bots_by_cluster.potential_bot > 1, 1, 0)\nbots_by_cluster.drop(\"username\", axis=1, inplace=True)\nbots_by_cluster = bots_by_cluster.groupby(\"cluster_label\")[\"potential_bot\"].sum().reset_index()\nbots_by_cluster[bots_by_cluster.potential_bot > 1]","131f5138":"# inspect topic\nbot_comments[(bot_comments.cluster_label == 1) & (bot_comments.forum == \"ajblogs\")][[\"username\", \"forum\", \"message\", \"user_similarity_to_bot\"]]","3b9a00df":"# reduce embedding space to 2D using T-SNE so that they can be plotted\nembedding_indices = bot_comments[(bot_comments.cluster_label == 1) & (bot_comments.forum == \"ajblogs\")].index\nX = embeddings[embedding_indices]\ntsne = TSNE(n_components=2, perplexity=30.0, init='random', random_state=14, n_jobs=-1)\n\nembeddings_reduced = tsne.fit_transform(X)","e89fbd11":"graph_data = bot_comments[(bot_comments.cluster_label == 1) & (bot_comments.forum == \"ajblogs\")][[\"message\", \"potential_bot\", \"user_similarity_to_bot\"]]\ngraph_data[\"x\"] = embeddings_reduced[:,0]\ngraph_data[\"y\"] = embeddings_reduced[:,1]","3591d6d0":"fig = px.scatter(\n    graph_data, \n    x='x', y='y', \n    color='user_similarity_to_bot', #'potential_bot', \n    hover_data=['message'],\n    title=\"TSNE Plot of Comment Embeddings Colored by User Similarity to Bot\"\n)\nfig.show()","c6db4a17":"## Inspect Bot Comments","735f0c80":"## Embedding the Comments\n\nUniversal Sentence Encoder (USE) was chosen to embed the messages, because it was trained for semantic similarity.  Each comment\/message will be treated as a document and receive 1 embedding, even if there are multiple sentences within the comment.  This will make it easier to capture the semantics of entire comments, so that they will be easier to compare for similarity.","1bde501d":"If anyone reading this and following along would like the try the recommender system approach, it would be ideal to use the item-item matrix to get user recommendations, since the number of users > number of items.\n\nTo determine whether a user might be a bot, I am going to create exemplars.  There will be 1 that exhibits the average features of a potential bot, and another that exhibits the average features of a normal user.  Then I will calculate the distance from every sample to each of these exemplars to determine bot similarity.  This approach is not perfect, since there are no hard labels, only potential bots, but it is a good place to start.","21ec5f6f":"A note on duplicates:\n\nThere are duplicate users.  What likely has happened is that the same user created accounts using the same user name in different forums, or 2 different people chose the same user name in different forums.  Forum information is only available in the comments data, so there is no way to know for certain which user record corresponds to which forum comments.  I will remove duplicates by keeping on the first instance of a user name.  This will preserve the comments tied to each user name, but it will overlook the possibility that comments from the same user name could have come from 2 different people.  So it is important to point out that all of my future analysis will treat comments with the same user name as coming from the same person (or bot), regardless of whether that is actually true.","9760687d":"Of these features, only common password appears to be able to distinguish between bots and normal users.","5a4a76a3":"What is not shown here is the chi-square test I performed, which returned a p-value of 0.  The problem was that the sample size is so large, the p-value was unreliable (https:\/\/www.semanticscholar.org\/paper\/Research-Commentary-Too-Big-to-Fail%3A-Large-Samples-Lin-Lucas\/7241c748932deb734fff1681e951e50be0853a39?p2df).  So the table shows proportions by user name ending in sequential digits.  The table shows that a user name that does not end in sequential digits is almost 10x more likely to not be a bot.  A user name that ends in sequential digits is 99x more likely to not be a potential bot.  So it seems that potential bots avoid using sequential digits in their usernames more often than users who are not suspected of being bots.  That could be a pattern, but it goes in the opposite direction of what I suspected.\n\nNext I will explore the idea that bots that are working together likely create their accounts around the same time.  By plotting the number of accounts created by date, it may be possible to see spikes when large numbers of accounts were created.","c46a9ddc":"I will probably delete these rows, but I want to use the comments to build clusters of semantically similar text.  So I will keep them for now, but remove them after I get cluster labels.","463d6c28":"This graph shows how the comments related over and embedded space.  You can play around with the cluster label and forum to view different potential bot networks.  Note that this dataset is dominated by commented labeled as potential bots.  Ordinarily, one might expect data in the wild to be dominated by normal users.  So keep in mind that here, the majority class is reversed.","47bfba26":"The results show clear semantic similarity.","4e3c6545":"## Examining Bot Likelihood and Semantic Similarity\n\nSo far the messages have been clustered by semantic similarity.  This has resulted in clusters containing messages that share some theme.  Now it is time to link the messages back to the users by looking at which users made comments in which clusters.  \n\nThere are 2 ways to approach this.  The simple way, which is the way I will proceed with, is to find semantic clusters with the most potential bot users and see what they are commenting about.  The more complex way, would be to approach the problem as a recommender system.  For this approach, one might produce a user-item matrix, where the values of the matrix are the number of times a user wrote something in a message cluster.  \n\nAs mentioned before, I will focus my analysis on 5 forums: \n* ajblogs\n* allkpop\n* bdn\n* foxnews\n* worldstar \n\nI had previously identified the following features that showed the most promise in distinguishing between bots and normal users: \n* days between account creation and first post\n* common password\n* major email provider\n* user name ends in sequential digits\n\nThe plan is to find users who make semantically similar comments, then use these features to determine the likelihood that they are bots. ","4479e61d":"## Comparing User Data to Potential Bot User Data\n\nI will start off with some exploratory analysis on user data.  I am particularly interested in seeing if the user data for potential bots varies significantly from other users.","1b710184":"On average, 2,420 accounts are created each day.  On July 12, 2012, 11,175 accounts were created.  That definitely seems suspicious.  It would be interesting to compare this to the number of comments by forum.  There should be a spike in comments around that same time in 2012.  Now to compare creation date by potential bots, common password, and common email...","f2cca4b4":"### Message Clustering (Community Formation)\n\nI decided to try 2 methods of community formation, and after flushing them both out, decided to proceed only with Infomap.  But I will describe the alternative method that I tried here too.  The reason I decided on Infomap was that it produced communities faster without having to worry about overly large communities dominating the results.  Connected components was faster initially, but dealing with the large community problem by iteratively breaking them up was time consuming.\n\n#### Method 1: Infomap Community Detection\nThis method uses the Infomap algorithm to detect communities in a graph.  The graph is constructed from the sparse matrix, and messages are treated as nodes in the graph.  The edge weights, a.k.a. link weights, are determined by the distance values in the adjacency matrix.  For more on the Infomap algorithm, see: https:\/\/cdlib.readthedocs.io\/en\/latest\/reference\/cd_algorithms\/algs\/cdlib.algorithms.infomap.html and https:\/\/www.pnas.org\/content\/105\/4\/1118\/.  \n\n#### Method 2: Sparse Matrix Connected Components\nThis method takes the simple approach of seeing which nodes (messages) are connected by a weight in the sparse matrix.  The absence of a weight means the nodes are not connected.  The choice in K for the KNN search impacts this by determining connectivity.  If K is large, then all nodes will essentially be connected, resulting in 1 giant community.  This is bad.  If it happens, try shrinking K.  Alternativey, large communities could be broken up into smaller ones by iteratively rebuilding them with smaller and smaller K's until some threshold (like max community size) is satisfied.","9b175766":"ID is post ID, and parent is the post ID of the message responded to.  ","84a96a34":"Comparing these plots, there is clearly an overlap between account creation and number of comments.  Interestingly, I was expecting a surge of new accounts to precede a spike in comments for all forums, but that does not appear to be the case.  Some forums, like breitbarproduction, saw a surge in new accounts but no corresponding spike in comments.  There are 5 forums that exhibit the pattern I expected to see:\n* bdn\n* allkpop\n* ajblogs\n* worldstar\n* foxnews\n\nIt looks like there is data missing, as both the comments and the user accounts have gaps in different regions.  To make them comparable, I plotted them for the same time frame, ending a few weeks after the max account creation date.  \n\n**I will limit further anlaysis of the bot network to these 5 forums only, as they have the most data available with the least number of gaps in time.**  ","d734cff8":"## Bot Network Exploration\n\nNow that the bot labels and semantic clusters are solidified, it is time to explore potential bot networks lurking in these forums.","01b18551":"Here is an example of a semantic cluster about revolution in Syria, which contains a lot of potential bots.  If these bots are working together, they could be trying to influence the politics in Syria.  I will graph these below.\n\nYou can play around with the forum and cluster label in the cell above to view others.  Not all of them are well formed.  Some contain non-ascii jibberish, which could be a sign of bots, but more spam bots rather than bots trying to influence people's opinions.  ","fe05a3c9":"### Text Cleaning\n\nI'm going to remove HTML tags so that only the text (and any links) are embedded.","20194356":"I did not have time to address the following points in this notebook. \n\nOutstanding Questions:\n\n* Should comments by the same user be grouped, before making the embeddings?  - this would eliminate the risk of a user being matched with itself.  \n* Should comments be grouped by forum?  -  this would make it hard to find directed influence campaigns that span multiple forums\n* Should comments be grouped by thread?  -  this would be difficult for this dataset, as it would rely on chaining comment IDs\n* Features and exemplars at the comment level are influenced by the users who comment the most.  Should they be weighted equally instead? - this would require de-duplicating comments and re-building features as 1 per user.\n\nOther Ideas:\n\n* Try a separate model with reply IDs to graph who is talking to who.  ","3e2fae21":"So far, only the days between account creation and comment can distinguish between normal users and potential bots.  The potential bot distributions have some peculiar multi-modalities in the distributions of some of these features.  It is hard to say what they might mean.","bc0ab2f7":"## Calculating the Similarity (Distance) between the Embeddings\n\nClustering will require calculating the pairwise distances between embeddings.  This is hard with the size of the dataset, to a distance approximation algorithm will be used.  This will do a KNN search for the nearest neighbors and compile the results into a sparse similarity matrix.  The rows and colums of this matrix will be the IDs of the comments.  The values in the matrix will be the distances.  Every comment will have at most K distances.  The distances in the sparse matrix will be handed off to a clustering algorithm to produce cluster labels for each comment.  Thus, the cluster labels will capture the semantic content of the comments.\n\nThe distance calculations will be done with Non-Metric Space Library (NMSLib).  This is the same technology behind Amazon's Elasticsearch.  Read about it here: https:\/\/sefiks.com\/2020\/09\/19\/large-scale-face-recognition-with-nmslib\/.  The L2 metric, or Euclidean distance, will be used to determine similarity.  The space partitioning function will be the hierarchical navigable small world graph (HNSW).  \n\nNotes on the index parameters:\n* M: The size of the initial set of potential neighbors for indexing.  Smaller M -> faster indexing but less accurate NN approximation.  Larger M -> slower indexing but more accurate NN approximation.\n* indexThreadQty: The number of threads for multi-processing.\n* efConstruction: During the search for NN, candidates are queued for evaluation as long as they are closer the efConstruction-th closest point.  This defaults to a value of 1, meaning only the nearest 1 candidate is considered.  This parameter determines graph quality.  A larger value improves the quality at the expense of longer processing time.\nFor more on these parameters, the algorithm, and the indexing process, see the documentation: https:\/\/github.com\/nmslib\/nmslib\/blob\/master\/manual\/README.md","a8d63391":"A greater percentage of potential bots use obscure email providers than major providers.  Users who are not suspected bots largely use major email providers.  This is definitely a pattern in this dataset, but it might be an artificat of how potential bots were labeled.  It will nevertheless be helpful in identifying potential bot nets for this dataset.\n\nA greater percentage of potential bots use uncommon passwords.  That aligns with the dataset documentation that says that potential bots were labeled based on sharing the same uncommon password with 75+ other users.  Does that mean bots use more secure passwords than regular users?  That would be hilarious.  Maybe they are just less common.\n\n#### Other Areas of Exploration\n\nI have not touched the user meta data file that comes with this dataset.  However, looking at the summary stats on the data page, it looks like the fields of interest (about, disable 3rd party tracking, is anonymous, is power contributor, is primary, is private) are nearly all the same value, meaning they would not be helpful in identifying bots.  \n\nOne last idea that I will not test now, but may come back to, is that bots might use some pattern in the creation of their user names.  If they are working together, they might want to have a way of identifying each other.  User name conventions would be the easiest way to do that on a message board.  Unfortunately is is hard to code something that would find user name patterns.  Levenshtein distance might help, but a large number of legitimate users would likely be caught in that net too.  "}}