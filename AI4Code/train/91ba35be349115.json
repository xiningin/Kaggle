{"cell_type":{"65343533":"code","14c6e64b":"code","a2508f09":"code","d907d255":"code","dc884929":"code","06dd0bac":"code","73a9d087":"code","5bbd15ae":"code","53e3a5cf":"code","5a1e5c65":"code","25af10ed":"code","8d0b5fec":"code","1d21e0b7":"code","38a0d7e5":"code","963310eb":"code","b3a36b78":"code","c28f4bf1":"code","5576fe09":"code","16f939fd":"code","2eab551d":"code","9528a1ae":"code","5fe9676b":"code","e326bc2e":"code","1dbb9a8d":"code","0f9627d8":"code","6dd77496":"code","e9809ae9":"code","cbd97779":"code","c96db2b0":"code","e85bb996":"code","59fab025":"code","9e0bfec4":"code","69bad39b":"markdown"},"source":{"65343533":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \nfrom pandas import DataFrame\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nimport category_encoders as ce\n\n# Algorithms and models:\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Data categorization\/encoding:\n\nimport category_encoders as ce ","14c6e64b":"data_path = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\n\ntrain_df  = pd.read_csv(data_path + 'sales_train.csv')\ntest_df = pd.read_csv(data_path + 'test.csv')\n\ntrain_df.info()\ntest_df.info()","a2508f09":"train_df.head()","d907d255":"test_df.head()","dc884929":"# We put the date feature from train_df in the proper format:\n\ntrain_df['date']  =  pd.to_datetime(train_df['date'], format = '%d.%m.%Y')\n\n# Let's check if there are missing values in train_df:\n\ntotal = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","06dd0bac":"# There are no missing values, let's look for outliers:\n\nplt.scatter(train_df['item_price'].unique(),train_df.groupby(['item_price']).sum()['item_cnt_day'])","73a9d087":"# there is one outlier, we remove it\n\ntrain_df.drop(train_df[(train_df['item_price'] >200000)].index, inplace=True)","5bbd15ae":"# We check if there are nonsense item prices (minor than 0)\n\ntrain_df[(train_df['item_price']<0)]","53e3a5cf":"# We remove this outlier too \n\ntrain_df.drop(train_df[train_df['item_price'] < 0].index, inplace = True)\n\n# Sold items cannot be negative, we remove these outliers too\n\ntrain_df.drop(train_df[train_df['item_cnt_day'] < 0].index, inplace = True)","5a1e5c65":"train_df.isna().sum()","25af10ed":"train_df","8d0b5fec":"# Some feature engineering:\n\ntrain_df['year'] = train_df['date'].dt.year\ntrain_df['month'] = train_df['date'].dt.month\ntrain_df['day'] = train_df['date'].dt.day\ntrain_df['year'] = train_df['year'].replace({2013:1,2014:2,2015:3})\n\n# We add categories: \n\nitems_df = pd.read_csv(data_path + \"items.csv\")\ntrain_df = pd.merge(train_df, items_df[['item_id','item_category_id']], how='left', on=['item_id'])\n\ntrain_df","1d21e0b7":"train_df = train_df[['date','year','month','day','date_block_num','shop_id','item_id','item_price','item_category_id','item_cnt_day']]","38a0d7e5":"train_df.dtypes","963310eb":"# \"Shop_id\" is an integer number, but actually it is just a label. We should encode it in a more logical way.\n# For this we are going to encode it based on the item_price:\n\nencoder=ce.TargetEncoder(cols='shop_id') \ntrain_df['shop_id'] = encoder.fit_transform(train_df['shop_id'],train_df['item_price'])","b3a36b78":"# Same thing for the item_id:\n\nencoder=ce.TargetEncoder(cols='item_id') \ntrain_df['item_id'] = encoder.fit_transform(train_df['item_id'],train_df['item_price'])\ntrain_df","c28f4bf1":"# Train_df seems to be ready. Let's work with test_df:\n# We need to add \"year\", \"month\", \"date_block_num\" and \"item_price\" for it to be consistent with the features from \"train_df\".\n# \"Year\" is clear (2015), month is clear (11), \"date_block_num\" is clear (34).\n\ntest_df.insert(0,'year',2015,True)\ntest_df.insert(1,'month',11,True)\ntest_df.insert(2,'date_block_num',34,True)\ntest_df.drop(['ID'], axis=1, inplace=True)\ntest_df","5576fe09":"# For \"item_price\" we will start by adding the last price of the \"item_id\". It will not fill all the rows, but at least part of it:\n\nitem_price=dict(train_df.groupby('item_id')['item_price'].last('1D').reset_index().values)\ntest_df['item_price']=test_df['item_id'].map(item_price)\ntest_df","16f939fd":"# There are still NaN values that we will average. For this, we can set the item_price with the price of this item in other shops\n\ntest_df['item_price'].fillna(train_df.groupby(['item_id']).mean()['item_price'],inplace=True)\ntest_df","2eab551d":"test_df.isna().sum()","9528a1ae":"# There are still nan values in the price from \"test_df\", corresponding to items that whether are new or have been never sold. \n# We can fill these NaN values with the average of the products from his category. For this:\n\n# 1) We first read the items.csv file\n\nitems_df = pd.read_csv(data_path + \"items.csv\")\nitems_df","5fe9676b":"# 2) We add the category to each item from the test:\n\ntest_df = pd.merge(test_df, items_df[['item_id','item_category_id']], how='left', on=['item_id'])\ntest_df","e326bc2e":"# We average the price of every category in a new dataframe:\n\ncategory_prices_df = train_df[['item_category_id','item_price']].groupby(['item_category_id']).mean().reset_index()\ncategory_prices_df","1dbb9a8d":"# And we merge it with the NaN values from \"test_df\"\n\ntest_df = pd.merge(test_df, category_prices_df, how='left', on=['item_category_id'])\ntest_df['item_price_x'].fillna(test_df['item_price_y'], inplace=True)\ntest_df.drop('item_price_y', axis=1, inplace=True)\ntest_df=test_df.rename(columns = {'item_price_x':'item_price'})\ntest_df['year']=3\ntest_df","0f9627d8":"# Finally we confirm that there are no NaN values anymore in test_df\n\ntest_df.isna().sum()","6dd77496":"train_df","e9809ae9":"test_df","cbd97779":"# the feature \"shop_id\" needs to be encoded yet, to be consistent with \"train_df\"\n\nencoder=ce.TargetEncoder(cols='shop_id')\ntest_df['shop_id'] = encoder.fit_transform(test_df['shop_id'],test_df['item_price'])\ntest_df","c96db2b0":"# Same thing for \"item_id\":\n\nencoder=ce.TargetEncoder(cols='item_id')\ntest_df['item_id'] = encoder.fit_transform(test_df['item_id'],test_df['item_price'])\ntest_df","e85bb996":"X = train_df.drop(['date','day','item_cnt_day'], axis=1)\n\n# The target values from \"train_df\" are too broad. We will normalize it using log:\n\ny = np.log(train_df['item_cnt_day'])\ny = y.fillna(y.median())\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)","59fab025":"# HYPERPARAMETER OPTIMIZATION FOR XGBOOST:\n\n# Hyperparameters I tried with cross validation by means of XGBRegressor:\n\n# 'max_depth': [4,6,10],\n# 'learning_rate': [0.01, 0.1, 1],\n# 'n_estimators': [600, 1500, 2000],\n# 'colsample_bytree': [0.5,0.7,1],\n# 'tree_method': ['gpu_hist'],\n# 'alpha': [0,0.01,0.1],\n# 'gamma': [0, 0.1, 0.5],\n# 'eta': [0.01,0.1,0.2],\n# 'min_child_weight': [1,2,6]\n\n\n# The best hyperparameters turned out to be: Best parameters: {'alpha': 0, 'colsample_bytree': 0.5, 'eta': 0.01, 'gamma': 0.1, \n# 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 1500, 'tree_method': 'gpu_hist'}\n\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\nparams = { 'max_depth': [4],\n           'learning_rate': [0.1],\n           'n_estimators': [1500],\n           'colsample_bytree': [0.5],\n           'tree_method': ['gpu_hist'],\n           'alpha': [0],\n           'gamma': [0.1],\n           'eta': [0.01],\n           'min_child_weight': [1]\n            }\nxgbr = xgb.XGBRegressor(seed = 20)\nclf = GridSearchCV(estimator=xgbr, \n                   param_grid=params,\n                   scoring='neg_mean_squared_error', \n                   verbose=3)\nclf.fit(X, y)\nprint(\"Best parameters:\", clf.best_params_)\nprint(\"Lowest RMSE: \", (-clf.best_score_)**(1\/2.0))","9e0bfec4":"results_df = clf.predict(test_df)\n\n# We undo the normalized results by means of the log:\n\nresults_df = pow(10, results_df)\n\ndf = pd.DataFrame(data=results_df, columns=[\"item_cnt_month\"])\ndf['ID'] = np.arange(len(df))\ndf.to_csv('final_file', sep=',', index=False, index_label=False )","69bad39b":"Fitting 5 folds for each of 1 candidates, totalling 5 fits\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.079, total=  58.4s\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   58.4s remaining:    0.0s\n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.096, total=  55.4s\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.9min remaining:    0.0s\n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.080, total=  52.7s\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.093, total=  55.1s\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.087, total= 1.0min\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.7min finished\nBest parameters: {'alpha': 0, 'colsample_bytree': 0.5, 'eta': 0.01, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 1500, 'tree_method': 'gpu_hist'}\nLowest RMSE:  0.2946685995050872"}}