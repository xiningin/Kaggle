{"cell_type":{"2064686c":"code","a74d80be":"code","cd8e0e89":"code","437ccb9c":"code","e41a4694":"code","88ca97c8":"code","2e97d50c":"code","180cf371":"code","73579209":"code","2353aeaa":"code","dfb06176":"markdown","2998d961":"markdown","2932b7b2":"markdown","853bebeb":"markdown","4ad75101":"markdown","0a913251":"markdown","0edc64bf":"markdown","137ba3c2":"markdown","cfe706c2":"markdown","1e28810a":"markdown"},"source":{"2064686c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.style\n%matplotlib inline\nimport os\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n\nmatplotlib.style.use('ggplot')\nfrom cycler import cycler\ncolor_palette = sns.color_palette()\ncolor_palette[0], color_palette[1] = color_palette[1], color_palette[0]\nmatplotlib.rcParams['axes.prop_cycle'] = cycler(color=color_palette)\n\n# read the dataset and print five rows\noriginal_dataset = pd.read_csv('..\/input\/creditcard.csv')\n\ndataset = original_dataset.copy()\nprint(dataset.head(5))\n","a74d80be":"# count how many entry there are for every class\nclasses_count = pd.value_counts(dataset['Class'])\n\nprint(\"{} Bonafide examples\\n{} Fraud examples\".format(classes_count[0], classes_count[1]))\n\n# classes_count is a Series. \nclasses_count.plot(kind = 'bar')\nplt.xlabel('Classes')\nplt.ylabel('Frequencies')\nplt.title('Fraud Class Hist')","cd8e0e89":"# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import RobustScaler\n\n# RobustScaler is less prone to outliers.\nrob_scaler = RobustScaler()\ndataset['Amount'] = rob_scaler.fit_transform(dataset['Amount'].values.reshape(-1,1))\n\n# remove the Time Feature\ndataset.drop(['Time'], axis = 1, inplace = True)\n\ndataset.head(5)","437ccb9c":"X = dataset.loc[:, dataset.columns != 'Class' ]\ny = dataset.loc[:, dataset.columns == 'Class' ]\n\nfrom imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state = 0, sampling_strategy = 1.0)\n\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.20, stratify = y_resampled)\n\nassert len(y_train[y_train == 1]) + len(y_test[y_test == 1]) == len(dataset[dataset.Class == 1])\nprint(\"train_set size: {} - Class0: {}, Class1: {}\".format( len(y_train), len(y_train[y_train == 0]), len(y_train[y_train == 1]) ))\nprint(\"test_set size: {} - Class0: {}, Class1: {}\".format( len(y_test), len(y_test[y_test == 0]), len(y_test[y_test == 1]) ))\n","e41a4694":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import LinearSVC\n\nclassifier = LinearSVC(dual=False)\nclassifier.fit(X_train, y_train.ravel())","88ca97c8":"def plot_coefficients(classifier, feature_names, top_features=-1):\n if top_features == -1:\n    top_features = len(feature_names)\n    \n coef = classifier.coef_.ravel()\n abs_coef = np.abs(coef)\n top_coefficients = np.argsort(-abs_coef)[-top_features:]\n\n # create plot\n plt.clf()\n plt.figure(figsize=(15, 3))\n colors = [color_palette[c > 0] for c in coef[top_coefficients]]\n plt.bar(np.arange(top_features), coef[top_coefficients], color=colors)\n feature_names = np.array(feature_names)\n plt.xticks(np.arange(0, top_features), feature_names[top_coefficients], rotation=60, ha='right')\n plt.title(\"Feature coefficients\")\n plt.ylabel(\"Coefficient\")\n\nfeature_names = list(X.columns.values)\nplot_coefficients(classifier, feature_names)\nplt.show()","2e97d50c":"dataset.groupby(\"Class\")['V14', 'V15', 'V19'].describe(percentiles=[])","180cf371":"def remove_outliers(series):\n    return series[np.abs(series-series.mean()) <= (1*series.std())]\n\nfor feature in ['V14', 'V15', 'V19']:\n    ax = plt.subplot()\n    positive = dataset[feature][dataset.Class == 1]\n    negative = dataset[feature][dataset.Class == 0]\n\n    sns.distplot(positive, bins=50, label='Fraudulent')\n    sns.distplot(negative, bins=50, label='Bonafide')\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(feature))\n    plt.legend(loc='best')\n    plt.show()","73579209":"from sklearn.metrics import classification_report\ny_test_pred = classifier.predict(X_test) > 0.5\ntarget_names = [\"Bonafide\", \"Fraudulent\"]\nprint(classification_report(y_test, y_test_pred, target_names=target_names))","2353aeaa":"\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.clf()\nplt.grid('off')\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nclassNames = target_names\nplt.title('Fraud or Not Fraud Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nthresh = cm.max() \/ 2.\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\nplt.show()\n","dfb06176":"###  Undersampling with ratio 1","2998d961":"**Fraud Detection with SVM**\n\n","2932b7b2":"### Inspect the coefficients","853bebeb":"The Features `V1`..`V28` seem to be normalized. The creators of the dataset could not disclose what they represent, but did note that this features are already selected from a larger set using [PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis). The (transaction) `Amount` and `Time`  are not normalized, however. SVM algorithms are not scale invariant, so it is highly recommended to scale these 2 remaing features.\n\nThe `Time` feature represents the number of seconds since data recording started at the moment the transaction was performed. The total dataset covers a timespan of two days. It might be interesting to replace this feature by two new features `Day` (having value 0 or 1) and `TimeOfDay` (having values between 0 and 1, where 0 is 00:00  and 1 is 23:59)\n\nTodo: rescale the Time dimension. Removed for now.","4ad75101":"# Fraud detection principal components using SVM and undersampling for correcting unbalance\n\nThis Kernel is based on work by [Davide Vegliante](https:\/\/www.kaggle.com\/davidevegliante\/nn-for-fraud-detection#).\nInstead of a Neural Network, we will train an SVM, and later use a technique described by [Aneesha Bakharia](https:\/\/medium.com\/@aneesha\/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d) to inspect which of the unnamed features V1..V28 contributed most to the decision boundary \/ margin of the trained SVM. To increase our confidence that our results are sound, we use [Louis Headley's work](https:\/\/www.kaggle.com\/louish10\/anomaly-detection-for-fraud-detection\/notebook) to visualize the probability distributions of various features.\n\ntodo: compare to [Variance Threshold](https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html)\n\n## Dataset\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n","0a913251":"On first inspection, it seems that coefficient strength is indeed a good indicator for the importance of a feature. When the coefficient is positive, the feature is positively correlated with the output class. In other words: Examples with higher `V14` values are more likely to be fraudulent than lower values. Conversely, high values for `V19` are indicative for a bonafide transaction. However, the coefficient for `V19` is much weaker than the coefficient for `V14`, and thus we expect `V19` to have much less of an influence on our prediction. We also expect the probability distributions for Fraudulent and Bonafide transaction to overlap more for `V19` than for `V14`. The coefficient of `V15` is even weaker than that of `V19`. And indeed,, we see that there is larger overlap in the probabilities of fraudulent and bonafide transations for `V15` than for `V19`. All of these predictions were made based on the trained SVM coefficients, and all of them are in line with the probability distributions shown above.","0edc64bf":"### Confusion Matrix","137ba3c2":"Let's see how many examples and features our dataset contains. ","cfe706c2":"### Train SVM Structure\n","1e28810a":"## Report the classifier performance\n\nNote that **preciosion and recall are unreliable in an unbalanced dataset**, but we used undersampling to account for this."}}