{"cell_type":{"332d7a13":"code","c4f632bb":"code","89140923":"code","88a30e25":"code","810ed50b":"code","b6cf6e51":"code","68371704":"code","d096cc36":"code","88df660d":"code","aa3919ba":"code","80910096":"code","638d0432":"code","676cdbb5":"code","a7dee8ab":"code","4c0b266f":"code","0512b67c":"code","8e3e4e7f":"code","c9f065b8":"code","100c6e43":"code","c1f326c4":"code","f1c11d3b":"code","c6875e56":"code","92550f8a":"code","d6f7cca5":"code","0927241b":"code","b704194e":"code","cdda0742":"code","9677bb71":"code","7faa2af3":"code","3116f94c":"code","be3ac2a2":"code","465d1d7d":"code","a79247a9":"code","3493d54a":"code","d0631cd4":"code","aa2ab5da":"code","a26e76af":"code","5c2963e4":"code","a41786bd":"code","4f9316d0":"code","99d3d22b":"code","98c82624":"code","3413d060":"code","66de9968":"code","1f446d95":"code","0baef570":"code","6dbb23e3":"code","d88bc4e8":"code","98e64c2b":"code","cea7ca09":"code","d5381150":"code","495fe5c4":"code","dbb7555d":"code","f050cc3b":"code","588772e4":"markdown","89c65d23":"markdown","fa8ba9e8":"markdown","eb61dd86":"markdown","98f4370a":"markdown","0d96e401":"markdown","dc93c37e":"markdown","f7a304fb":"markdown","1b420c84":"markdown","86907e0f":"markdown","1eb44121":"markdown","240ad679":"markdown","f298f591":"markdown","e690d13c":"markdown","66a5f05c":"markdown","9c03b426":"markdown","4d4ca555":"markdown","9ce09ca3":"markdown","16bb67cc":"markdown","995e5bab":"markdown","9f544efa":"markdown","aa585a83":"markdown","faa4809e":"markdown","077cc8f9":"markdown","7637ac2d":"markdown","e6c27038":"markdown","5957a9db":"markdown","b985319a":"markdown","3f05a8b8":"markdown","c44a6737":"markdown","e465d057":"markdown","0ec44ea7":"markdown","8484ec7d":"markdown","b07af1f7":"markdown","328bc878":"markdown","438c2ea8":"markdown","a6d5b6ec":"markdown","bdaddcab":"markdown","94468e69":"markdown","8ec621c7":"markdown","00b50dfc":"markdown","0a76df98":"markdown","917e8304":"markdown","a1b2084d":"markdown","51e8eec4":"markdown","9ae0820c":"markdown","760cc8e5":"markdown","0dd95c99":"markdown","fbeb56dd":"markdown","a2357356":"markdown"},"source":{"332d7a13":"#data analysis libraries \nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","c4f632bb":"#import train and test CSV files\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","89140923":"#take a look at the training data\ntrain.describe(include=\"all\")","88a30e25":"#show a sample of the training data\ntrain.head()","810ed50b":"#get a list of the features within the training dataset\nprint(train.columns)","b6cf6e51":"#take a look at the test data\ntest.describe(include=\"all\")","68371704":"#show a sample of the test data\ntest.head()","d096cc36":"#get a list of the features within the test dataset\nprint(test.columns)","88df660d":"#count missing data in train.csv\nprint(pd.isnull(train).sum())","aa3919ba":"#count missing data in test.csv\nprint(pd.isnull(test).sum())","80910096":"#visualize the survival states\nf,ax=plt.subplots(1,2,figsize=(13,5))\ntrain['Survived'].value_counts().plot.pie(explode=[0,0.05],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=train,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","638d0432":"#draw a bar plot of survival by Pclass\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\nplt.show()\n\n#print percentage of people by Pclass that survived\n#print(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n#print(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n#print(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","676cdbb5":"#draw a bar plot of survival by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\nplt.show()\n\n#print percentages of females vs. males that survive\n#print(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n#print(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","a7dee8ab":"#sort the ages into logical categories\n'''\ntrain[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 18, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()\n'''\n#sort the ages into logical categories\n#train[\"Age\"] = train[\"Age\"].fillna(-0.5)\n#test[\"Age\"] = test[\"Age\"].fillna(-0.5)\nbins = [0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","4c0b266f":"train = train.drop(['AgeGroup'], axis = 1)\ntest = test.drop(['AgeGroup'], axis = 1)","0512b67c":"#draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\nplt.show()\n\n#I won't be printing individual percent values for all of these.\n#print(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n#print(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n#print(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)","8e3e4e7f":"#draw a bar plot for Parch vs. survival\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train)\nplt.show()","c9f065b8":"fare_not_survived = train['Fare'][train['Survived'] == 0]\nfare_survived = train['Fare'][train['Survived'] == 1]\n\naverage_fare = pd.DataFrame([fare_not_survived.mean(), fare_survived.mean()])\nstd_fare = pd.DataFrame([fare_not_survived.std(), fare_survived.std()])\naverage_fare.plot(yerr=std_fare, kind='bar', legend=False)\n\nplt.show()","100c6e43":"train[\"has_Cabin\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"has_Cabin\"] = (test[\"Cabin\"].notnull().astype('int'))\n\n#calculate percentages of CabinBool vs. survived\n#print(\"Percentage of CabinBool = 1 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\n\n#print(\"Percentage of CabinBool = 0 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\n#draw a bar plot of CabinBool vs. survival\nsns.barplot(x=\"has_Cabin\", y=\"Survived\", data=train)\nplt.show()","c1f326c4":"sns.barplot(x=\"Embarked\", y=\"Survived\", data=train)\n#sns.barplot('Embarked', 'Survived', data=train, size=3, aspect=2)\n#sns.factorplot('Embarked', 'Survived', data=train, size=3, aspect=2)\n#plt.title('Embarked and Survived rate')\nplt.show()","f1c11d3b":"#create a combined group of both datasets\ncombine = [train, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","c6875e56":"#replace various titles with more common names\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","92550f8a":"#map title\nfor dataset in combine:\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","d6f7cca5":"#to show the new column \"Title\"\ntrain.head()","0927241b":"#drop Name feature\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","b704194e":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","cdda0742":"#calculate the correlation between features\ntrain_corr = train.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ntrain_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ntrain_corr[train_corr['Feature 1'] == 'Age']","9677bb71":"#take the median value for Age feature based on 'Pclass' and 'Title'\ntrain['Age'] = train.groupby(['Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\ntest['Age'] = test.groupby(['Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))","7faa2af3":"#we can also drop the Ticket feature since it's unlikely to yield any useful information\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)","3116f94c":"#fill in missing Fare value in test set based on mean fare for that Pclass \n'''\nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n'''     \n#map Fare values into groups of numerical values\n#train['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\n#test['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\nfor dataset in combine:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n    \nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","be3ac2a2":"#drop the Cabin feature\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)","465d1d7d":"#replacing the missing values in the Embarked feature with S\ntrain = train.fillna({\"Embarked\": \"S\"})","a79247a9":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","3493d54a":"#count missing data in train.csv\nprint(pd.isnull(train).sum())","d0631cd4":"target = train[\"Survived\"]\npredictors = train.drop(['Survived', 'PassengerId'], axis = 1)","aa2ab5da":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.20, random_state = 0)","a26e76af":"predictors.head()","5c2963e4":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","a41786bd":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","4f9316d0":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","99d3d22b":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","98c82624":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","3413d060":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","66de9968":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","1f446d95":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","0baef570":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","6dbb23e3":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","d88bc4e8":"# XGBoost\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(x_train, y_train)\ny_pred = xgb.predict(x_val)\nacc_xgb = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_xgb)","98e64c2b":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier', 'XGBoost'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk, acc_xgb]})\nmodels.sort_values(by='Score', ascending=False)","cea7ca09":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom xgboost import XGBClassifier\n\nX = train.drop(['Survived', 'PassengerId'], axis=1)\nY = train[\"Survived\"]\n\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Support Vector Machines', 'K-Nearst Neighbor', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier', 'XGBoost']\nmodels=[SVC(), KNeighborsClassifier(), LogisticRegression(), RandomForestClassifier(), GaussianNB(), \n        Perceptron(), LinearSVC(), DecisionTreeClassifier(), SGDClassifier(), \n        GradientBoostingClassifier(), XGBClassifier()]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","d5381150":"#check missing data\nprint(pd.isnull(test).sum())","495fe5c4":"train.head()","dbb7555d":"test.head()","f050cc3b":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = xgb.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)\nprint(\"The submission was successfully saved!\")","588772e4":"### 8) Cabin Feature\nAs mentioned in the last section, we could see survival rate can be affected by the *Cabin*. However, there are too many data missing in the *Cabin* feature (more than 70%), it need more advanced \"domain knowledge\" (like knowledge about the ship) and data processing technique to deal with the missing data. Thus, at our beginer level, we will first drop the *Cabin* feature, but keep the *has_Cabin* feature.","89c65d23":"### 2.1 Data importing and brief analysing","fa8ba9e8":"### 3) Pclass Feature","eb61dd86":"## 1. Introduction\nOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this notebook, we will try to use the knowledge from data science to analysis the relation between features and survival rate, and to predict some passengers' survival rate based on the features.","98f4370a":"### 6)Ticket Feature\nWe can also drop the Ticket feature since it's unlikely to yield any useful information","0d96e401":"We could see that babies have higher survival rate, while seniors have lower survival rate. Babies are small and easy to be kept in boats. Seniors may be less strong to survive from the cold water. But anyway, it shows us that the *Age* feature is very important.","dc93c37e":"### 11) Cabin Feature\nAs more than 75% of the *Cabin* feature are missing, it is a bit hard to deal with it. Firstly, let's make two groups: one with the *Cabin* data recorded (marked as \"1\"), and the other with the *Cabin* data missing (marked as \"0\").","f7a304fb":"We use Pandas, a Python library, to read the \"train.csv\" and \"test.csv\", and to show a first look of the dataset.","1b420c84":"### 2.2 Data analysis and visualisation\nAs we stated above, the *Survived* column would be our target, and other columns are features. We already had an idea about what type of data do we have and how it looks like. Now let's try to visualize the data in *train.csv* to see whether the features really related to suvive rate.","86907e0f":"## 7. Further improvement\nAs for further improvement, we think there are lots of things can be done. For example,\n* ensemble models, like stacking\n* more carefully deal with features, such as the *Cabin* feature and the *Age* feature","1eb44121":"Let's compare the accuracies of each model.","240ad679":"### 4.1 Splitting the Training Data\nWe will use part of our training data (20% in this case) to test the accuracy of our different models.","f298f591":"From the description and samples shown above, we could see that there are 891 rows of data and 11 features in *train.csv*, while 418 rows of data and 10 features in *test.csv*. As the target, the *Survived* feature is only shown in *train.csv* instead of *test.csv*.\n\nAmong these features, there are numerical features, categorical features and alphanumeric features. The data type of each feature is stated below:\n* Passenger ID: int\n* Survived: int\n* Pclass: int\n* Name: string\n* Sex: string\n* Age: float\n* SibSp: int\n* Parch: int\n* Ticket: string\n* Fare: float\n* Cabin: string\n* Embarked: string","e690d13c":"### 4.2 Testing Different Models\nI will be testing the following models with my training data (got the list from [here](http:\/\/https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)):\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\n* XGBoost Classifier\n\nFor each model, we set the model, fit it with 80% of our training data, predict for 20% of the training data and check the accuracy.","66a5f05c":"### 7) Fare Feature\nIt's time separate the fare values into some logical groups as well as filling in the single missing value in the test dataset.","9c03b426":"## 8. Reference\nWhen writting this notebook, we refer a lot to the notebooks below:\n> [Nadin Tamer. Titanic Survival Predictions (Beginner)](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner)\n\n> [Anisotropic. Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)\n\n> [Gunes Evitan. Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial)\n\n> [Manav Sehgal. Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n\n> [Vikum Sri Wijesinghe. Beginners Basic Workflow Introduction](https:\/\/www.kaggle.com\/vikumsw\/beginners-basic-workflow-introduction)","4d4ca555":"The table above tells us , that correlation coefficient between *Age* and *Pclass* is about 0.41, while others are much lower. So we'll take *Pclass* into account for *Age* prediction.\n\nBesides, we think some title like \"Dr\", \"Miss\" or \"Mrs\" can also tell us some information about age. So we would take *Title* into account, too.","9ce09ca3":"### 2) Name Feature\nFrom the *Name* feature, we will extract the \"title\" information to be a new feature, because these title like \"Lady\", \"Sir\", \"Dr\" tells us a bit about the social economic situation or even approximate age range about the passengers. After extraction, we could drop the *Name* feature.","16bb67cc":"Overall, in *train.csv*, only about 38.4% people survived.","995e5bab":"### 10) Fare Feature\nAs there are lots of fare, so it is diffcult to visualize the *Fare* feature like before. Here we take another strategy to visualize it ---- we calculate the average fare of the survived passengers and unsurvived passengers.","9f544efa":"In general, people with more (>2) siblings or spouses aboard were less likely to survive. However, comparing to passengers with one or two siblings or spouses, people with no siblings or spouses were also less likely to survive.","aa585a83":"# Titanic: Machine Learning from Disaster\nThis notebook is written during Computational Materials Design Lecture as one of the semester training projects.\nThe notebook is the work on \"Titanic: Machine Learning from Disaster\".\n\n## Contents:\n1. Introduction\n2. Analysing Data\n3. Feature Engineering\n4. Modelling\n5. Cross Validation\n6. Creating Submission File\n7. Further Improvements\n8. Reference","faa4809e":"Besides, as shown below, some data in the dataset are missing. \nIn *train.csv*, 177 rows of the *Age* feature are missing (~19.9% missing); 687 rows of the *Cabin* feature are missing (~77.1% missing); 2 rows of the *Embarked* feature are missing (~0.2% missing).\nIn *test.csv*, 86 rows of the *Age* feature are missing (~20.6% missing); 327 rows of the *Cabin* feature are missing (~78.2% missing); 1 row of the *Fare* feature is missing (~0.2% missing).","077cc8f9":"### 6) Age Feature\nSo as to simply visualize the *Age* feature, we group people in to 8 groups (*AgeGroup*) based on their age:\n* Unknown: Age data missing\n* Baby: 0-5 years old\n* Child: 5-18 years old\n* Young Adult: 18-35 years old\n* Adult: 35-60 years old\n* Senior: above 60 years old","7637ac2d":"## 12) Embarked feature\nThere are three ports where people get on board: \n* Southampton (marked as \"S\")\n* Cherbourg (marked as \"C\")\n* Queenstown (marked as \"Q\")","e6c27038":"### 1) Overall survival states","5957a9db":"## 3. Feature Engineering\nAs we mentioned before, some data are missing, so in this section we'll decide what to do with the missing data: either fill them with some predictions or just drop them. \n\nBesides, we will do encoding for the features that are not numerical.","b985319a":"It shows us that people with a recorded Cabin number are more likely to survive. It might is because that collecting survived passengers' Cabin information is easier. \nBut as too many *Cabin* data are missing, we will discuss later in *Feature Engineering* about how to deal with this feature.","3f05a8b8":"### 4) Name Feature\nThe *Name* feature should not directly related to the survival rate. However, there are some titles included in the *Name* feature, which may give us some information later on.","c44a6737":"We could see that, *Gradient Boosting Classifier* and *XGBoost* give the most accurate result. Thus, we would choose *XGBoost Classifier* to predict the results for *test.csv*.","e465d057":"### 9) Embarked Feature\nThere are only two passenger's embarking information is missing in *train.csv*. By Google, we found that Mrs. George Nelson and her maid Miss. Amelie Icard embarked at Southampton, which can be seen from [this website](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html). Thus, we fill these two missing data with \"S\".\n\nThen we encode the *Embarked* feature.","0ec44ea7":"Survived passengers averagely have higher fare. ","8484ec7d":"## 5. Cross Validation\nIn the previous part, the accuracy actually highly depends on the splitting of the training set (we can try different seperation percentage, then results change a lot). To avoid that, we will use cross validation. Here, K-fold cross validation is used.","b07af1f7":"### 5) Sex Feature","328bc878":"From the boxplot above, we could see that passengers in *Pclass1* have highest survival rate, while passengers in *Pclass3* have lowest survival rate.","438c2ea8":"It tells us that people embarked at Cherbourg have highest chance to survive, while people embarked at Southampton are less likely to survive. This is probably because that people from different ports have some difference on socialeconamic class and may stayed in different part of the ship (not homogeneously).","a6d5b6ec":"### 5)SibSp and Parch Feature\nThese two features are already numerical and there is no missing, so let's keep them as they are.","bdaddcab":"### 1) Pclass Feature\nThe *Pclass* feature seems important in our previous analysis, and is already numerical without any data missing, so we will keep the *Pclass* feature as it is.","94468e69":"### 7) SibSp Feature\n*SibSp* means how many siblings and spouses the passenger has aboard.","8ec621c7":"The table above shows us that the survival rate obviously differs with different titles. So let's keep the title as an feature.\n\nThen, let's encode the *Title* feature.","00b50dfc":"### 3) Sex Feature\nThe *Sex* feature is not \"string\" type, so we need to encode it.","0a76df98":"It is shown that passengers with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children.","917e8304":"### 2) PassengerID Feature\nPassengerID obviously doesn't relate to survival rate. So we won't analyze it too much, and later on we will drop it in Feature Engineering.","a1b2084d":"## 2. Analysing Data\nIn this section, we will try to explore our data to know what kind of data we are dealing with and to have a brief idea of features.","51e8eec4":"## 6. Creating Submission File\nNow let's apply our model on *test.csv* to create a *submission.csv* file so that we can upload to the Kaggle competition.","9ae0820c":"### 8) Parch Feature\n*Parch* means how many parents or children the passenger has aboard.","760cc8e5":"## 4. Modelling","0dd95c99":"### 4) Age Feature\nNext we'll fill in the missing values in the *Age* feature. Since a higher percentage of values are missing, it would be illogical to fill all of them with the same value. Instead, we will try to predict the missing ages.","fbeb56dd":"It is shown that females have a much higher chance of survival than males. The Sex feature is essential in our predictions.","a2357356":"### 9) Ticket Feature\nIt seems *Ticket* feature does not give too much information. We may drop it later."}}