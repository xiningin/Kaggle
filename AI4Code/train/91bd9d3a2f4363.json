{"cell_type":{"b9026cb4":"code","b7c05c1a":"code","8cbee990":"code","f63bf2c8":"code","422de899":"code","1a3a412a":"code","d9f923c8":"code","43dea81e":"code","a3e9d046":"code","28865609":"code","4153b62e":"code","bf4e96c6":"code","0f160d15":"code","5df22723":"code","bcf3adcb":"code","6610bd99":"code","deaede45":"code","5fbd80ca":"code","5c0d2808":"code","466dd6e4":"code","c0ce8c59":"code","e3262a94":"code","8edbe8ef":"code","bf3d4af4":"code","81dbd1e9":"code","548a3a6a":"code","c1b9abe0":"code","91be379a":"code","1534b4f5":"code","db1f989e":"code","d72aeded":"code","aa9608e9":"code","9e305834":"code","6d6b5ba4":"code","846d086e":"code","bd1a813c":"code","3e78fdf7":"code","530c9735":"code","da037124":"code","454e629e":"code","9c92300a":"code","082ce767":"code","11584c80":"code","12eefc58":"code","1cbdd96e":"code","b96921a5":"code","d58050ae":"code","d92a26d0":"code","ae813099":"code","57207b3d":"code","8f8c8e66":"code","cff889c7":"code","8377cc19":"code","fede34d5":"code","5e48c180":"code","e528b333":"code","10a1d891":"code","7aa1dc8d":"code","9a557ac4":"code","2549ca5b":"code","5e2441fa":"code","ac5dbb15":"code","52fe0220":"code","b77390bf":"code","bccebbad":"code","88663123":"code","b43e6f86":"code","5e6a7a2e":"code","aeb6ea3c":"code","df525c64":"code","23227095":"code","08f4d485":"code","1d29bbd0":"code","c27fd2be":"code","add154b3":"code","3aabc136":"code","a42b9b6e":"code","55fcbc28":"code","d35e44a0":"code","45820747":"code","b5d75b04":"code","f01a9a56":"markdown","08dd7a80":"markdown","adcac971":"markdown","f18bcdc7":"markdown","c548ce51":"markdown","3cb3d32d":"markdown","d5ce9b07":"markdown","53736db8":"markdown","f6a8e5ac":"markdown","4758a0e1":"markdown","f6e8fd76":"markdown","5060fcce":"markdown","47d8d02b":"markdown","8e4e9953":"markdown","0d1b8c26":"markdown","ee83b0b4":"markdown","4ed714b5":"markdown","85dc7c62":"markdown","c7b1a694":"markdown","832a912b":"markdown","09c7fa7d":"markdown","9c0d6606":"markdown","11583798":"markdown","b07c13f5":"markdown","92ef2e3a":"markdown","05c749d6":"markdown","680171d4":"markdown","fe650d51":"markdown","d4f1a7d1":"markdown","6529ad3d":"markdown","03e2889c":"markdown","dcc28826":"markdown","3186cb5f":"markdown","26deecba":"markdown","abb12c8f":"markdown","45adbb7c":"markdown","472249bd":"markdown","e193cbcf":"markdown","e13519b0":"markdown","3196a0b9":"markdown","fff11173":"markdown","16ef6713":"markdown","96ddf2ff":"markdown","a34d4af3":"markdown","7a98a1bb":"markdown","47d4d88d":"markdown","d0dc83d4":"markdown","e5949547":"markdown","f425d4e7":"markdown","a0c63453":"markdown","c260a72e":"markdown","326699a5":"markdown","41183182":"markdown"},"source":{"b9026cb4":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport matplotlib.pyplot as plt\n\n#!pip install --upgrade altair\n#!pip install vega vega_datasets\nimport altair as alt\n#alt.renderers.enable('html')","b7c05c1a":"df = pd.read_excel('..\/input\/safe-driver-prediction\/IT_3.xlsx')\ndf.head(5)","8cbee990":"#Dropping null values\ndf.dropna(inplace = True)\ndf.head(5)","f63bf2c8":"df = df.iloc[:,list(range(1,11))+[16]]\ndf.head(5)","422de899":"df_copy = df.copy()","1a3a412a":"sns.countplot(df_copy['target'])","d9f923c8":"count_1, count_0 = df.target.value_counts()\n\ndf_0 = df[df['target'] == 0]\ndf_1 = df[df['target'] == 1]\n\ndf_1 = df_1.sample(count_0, random_state = 25)\n\ndf_copy = pd.concat([df_0,df_1], axis=0)","43dea81e":"sns.countplot(df_copy['target'])","a3e9d046":"fig, axes = plt.subplots(nrows=2, ncols=3)\n\nfor i, ax in zip(('EngineHP',\t'credit_history',\t'Years_Experience','Miles_driven_annually','annual_claims','size_of_family'), axes.flat):\n    sns.distplot(df_copy[i], kde=False, ax=ax)\n\nfig.set_size_inches(15,10)\nplt.show()","28865609":"sns.boxplot(df_copy['Gender'], df_copy['annual_claims'])","4153b62e":"sns.boxplot(df_copy['Gender'], df_copy['Years_Experience'])","bf4e96c6":"sns.boxplot(df_copy['Gender'], df_copy['credit_history'])","0f160d15":"df_copy.corr(method='pearson')","5df22723":"sns.heatmap(df_copy.corr(method='pearson'))","bcf3adcb":"sns.pairplot(df_copy, hue = \"target\", height=3)","6610bd99":"for col in df_copy.select_dtypes(include=object).columns:\n  print(f'{col}:')\n  print('Before Stripping')\n  print(df_copy[col].unique())\n  print('After Strippping')\n  df[col] = df_copy[col].str.strip() #Stripping white spaces\n  print(df_copy[col].unique())\n  print('')","deaede45":"df = df_copy.sample(n = 4000, random_state = 25)  #Getting the 4000 rows of sample of the dataset","5fbd80ca":"df.dtypes","5c0d2808":"categoricalColumn = df.columns[df.dtypes == np.object].tolist() #Getting all the categorical variables in a list\ncategoricalColumn\n\nfor col in categoricalColumn:\n    print(col + ':')\n    print(df[col].value_counts())\n    print()","466dd6e4":"target = df['target']\n\ndf= df.drop(columns = 'target')\n\ntarget.value_counts()","c0ce8c59":"df_oneHot = df.copy()\nfor col in categoricalColumn:\n    if (len(df_oneHot[col].unique()) == 2):\n        df_oneHot[col] = pd.get_dummies(df_oneHot[col], drop_first=True)\n        \n        \ndf_oneHot = pd.get_dummies(df_oneHot)       ","e3262a94":"print(df_oneHot.shape)\ndf_oneHot.head(5)","8edbe8ef":"#Normalizing the dataset and saving it into a different dataframe\n\nfrom sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\ndf_oneHot_numpy = scaler.fit_transform(df_oneHot)","bf3d4af4":"df_oneHot_numpy = pd.DataFrame(df_oneHot_numpy, columns=df_oneHot.columns)\n\ndf_oneHot_numpy.head()","81dbd1e9":"df_clean = df_oneHot_numpy.assign(target = target.values) #getting the target variable back into one-hot-encoded dataframe\ndf_clean.head()","548a3a6a":"Data = df_clean.drop(columns = 'target').values\ntarget = df_clean['target']","c1b9abe0":"#Splitting this dataset into Training and Test Set\n\nfrom sklearn.model_selection import train_test_split\n\nD_train, D_test, t_train, t_test = train_test_split(Data, target, test_size = 0.3, random_state = 25)","91be379a":"#Import classifiers\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n#KNN Classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=30, p=2)\nknn_classifier.fit(D_train, t_train)\nknN = knn_classifier.score(D_test, t_test)\n\n#Decision Tree Classifier\ndt_classifier = DecisionTreeClassifier(criterion='entropy', max_depth=7)\ndt_classifier.fit(D_train, t_train)\ndT = dt_classifier.score(D_test, t_test)\n\n\n#Random Forest Classifier\nrf_classifier = RandomForestClassifier(n_estimators=200)\nrf_classifier.fit(D_train, t_train)\nrF = rf_classifier.score(D_test, t_test)\n\n\n#Gaussian Naive Bayes Classifier\nnb_classifier = GaussianNB(var_smoothing=10**(-3))\nnb_classifier.fit(D_train, t_train)\nnB = nb_classifier.score(D_test, t_test)\n\n\n#Compare the classification scores\n\nprint('Knn Classifier : ', knN )\nprint('Random Forest : ', rF)\nprint('Gaussian Naive : ', nB)\nprint('Decision Tree : ', dT)","1534b4f5":"# Performance with all the features using Decision Tree \nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn import feature_selection as fs\n\ncv_method = StratifiedKFold(n_splits=7, shuffle=True, random_state=999)\n\ncv_results_full = cross_val_score(estimator=dt_classifier,\n                             X=D_test,\n                             y=t_test, \n                             cv=cv_method, \n                             scoring='accuracy')\n\ncv_results_full","db1f989e":"cv_perf_full = cv_results_full.mean().round(3)\ncv_perf_full","d72aeded":"num_features = 4\nscoring_metric = 'accuracy'\ndt_classifier = DecisionTreeClassifier(criterion='entropy', max_depth=7)","aa9608e9":"fscore = fs.SelectKBest(fs.f_classif, k=num_features) #Selecting the top 5 features among 9 \nfscore.fit_transform(Data, target)\nfscore_indices = np.argsort(fscore.scores_)[::-1][0:num_features] #[::-1] gives us all the features from the reverse order\nfscore_indices","9e305834":"#The best features using F scores are\nbest_features_fscore = df_clean.columns[fscore_indices].values\n\nfor x in range(len(best_features_fscore)):\n  print (best_features_fscore[x])","6d6b5ba4":"import altair as alt\n\ndef plot_imp(best_features, scores, method_name, color):\n    \n    df = pd.DataFrame({'features': best_features, \n                       'importances': scores})\n    \n    chart = alt.Chart(df, \n                      width=500, \n                      title=method_name + ' Feature Importances'\n                     ).mark_bar(opacity=0.75, \n                                color=color).encode(\n        alt.X('features', title='Feature', sort=None, axis=alt.AxisConfig(labelAngle=45)),\n        alt.Y('importances', title='Importance')\n    )\n    \n    return chart","846d086e":"feature_importances_fscore = fscore.scores_[fscore_indices]\nfeature_importances_fscore","bd1a813c":"plot_imp(best_features_fscore, feature_importances_fscore, 'F-Score', 'blue')","3e78fdf7":"cv_results_fscore = cross_val_score(estimator=dt_classifier,\n                             X=D_test[:, fscore_indices],\n                             y=t_test, \n                             cv=cv_method, \n                             scoring=scoring_metric)\ncv_perf_fscore = cv_results_fscore.mean().round(3)\ncv_perf_fscore","530c9735":"mutual_info = fs.SelectKBest(fs.mutual_info_classif, k=num_features) \nmutual_info.fit_transform(D_train, t_train)\nmutual_info_indices = np.argsort(mutual_info.scores_)[::-1][0:num_features]\n\n#Looking at the top 5 features - \nbest_features_mutual_info = df_clean.columns[mutual_info_indices].values\nbest_features_mutual_info","da037124":"feature_importances_mutual_info = mutual_info.scores_[mutual_info_indices]\nfeature_importances_mutual_info","454e629e":"plot_imp(best_features_mutual_info, feature_importances_mutual_info, 'Mutual Information', 'red')","9c92300a":"cv_results_mutual_info = cross_val_score(estimator=dt_classifier,\n                             X=D_test[:, mutual_info_indices],\n                             y=t_test, \n                             cv=cv_method, \n                             scoring=scoring_metric)\ncv_perf_mutual_info = cv_results_mutual_info.mean().round(3)\ncv_perf_mutual_info","082ce767":"from sklearn.ensemble import RandomForestClassifier\nrfi = RandomForestClassifier(n_estimators=200, random_state=25)\nrfi.fit(D_train, t_train)\nrfi_indices = np.argsort(rfi.feature_importances_)[::-1][0:num_features]\n# The top features are: \nbest_features_rfi = df_clean.columns[rfi_indices].values\nbest_features_rfi","11584c80":"feature_importances_rfi = rfi.feature_importances_[rfi_indices]\nfeature_importances_rfi","12eefc58":"plot_imp(best_features_rfi, feature_importances_rfi, 'Random Forest', 'yellow')","1cbdd96e":"cv_results_rfi = cross_val_score(estimator=dt_classifier,\n                             X=D_test[:, rfi_indices],\n                             y=t_test, \n                             cv=cv_method, \n                             scoring=scoring_metric)\ncv_perf_rfi = cv_results_rfi.mean().round(3)\ncv_perf_rfi","b96921a5":"print(f'Full Set of Features (with {D_train.shape[1]} Features):', cv_perf_full)\nprint(f'Feature Selection with {num_features} Features:')\nprint('F-Score:', cv_perf_fscore)\nprint('Mutual Information:', cv_perf_mutual_info)\nprint('RFI:', cv_perf_rfi)","d58050ae":"#Comparing P Values with all features and only 5 features \n\nfrom scipy import stats\nprint('P-Value for All Features vs F-Score:', stats.ttest_rel(cv_results_full, cv_results_fscore).pvalue.round(3))\nprint('P-Value for All Features vs Mutual Information:', stats.ttest_rel(cv_results_full, cv_results_mutual_info).pvalue.round(3))\nprint('P-Value for All Features vs RFI:', stats.ttest_rel(cv_results_full, cv_results_rfi).pvalue.round(3))","d92a26d0":"num_features = 12 #(All the features are being taken into account)\nscoring_metric = 'accuracy'\ndt_classifier = DecisionTreeClassifier(criterion='entropy', max_depth=7)","ae813099":"from sklearn.model_selection import StratifiedKFold, GridSearchCV\ncv_method_train = StratifiedKFold(n_splits=7, shuffle=True, random_state=25)\n\n\nKNN_neighbours = {'n_neighbors': [1, 5, 10, 15, 20, 25, 30], 'p': [1, 2, 5]}","57207b3d":"gs_KNN = GridSearchCV(estimator=KNeighborsClassifier(), \n                      param_grid=KNN_neighbours, \n                      cv=cv_method_train,\n                      verbose=1,\n                      scoring=scoring_metric)","8f8c8e66":"gs_KNN.fit(D_train, t_train);","cff889c7":"gs_KNN.best_params_","8377cc19":"gs_KNN.best_score_","fede34d5":"# To get more cross validation results, we are getting all the details of each field\ngs_KNN.cv_results_['mean_test_score']","5e48c180":"results_KNN = pd.DataFrame(gs_KNN.cv_results_['params'])\nresults_KNN['test_score'] = gs_KNN.cv_results_['mean_test_score']\nresults_KNN['metric'] = results_KNN['p'].replace([1,2,5], [\"Manhattan\", \"Euclidean\",\"Minlowski\"])\nresults_KNN","e528b333":"\nalt.Chart(results_KNN, \n          title='KNN Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('n_neighbors', title='Number of Neighbors'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n    color='metric'\n).interactive()","10a1d891":"dt_classifier = DecisionTreeClassifier(random_state=25)\n\nparams_DT = {'max_depth': [3, 4, 5, 7, 10, 12],\n             'min_samples_split': [2, 5, 15, 20, 25]}\n            \ngs_DT = GridSearchCV(estimator=dt_classifier, \n                     param_grid=params_DT, \n                     cv=cv_method_train,\n                     verbose=1,\n                     n_jobs = -2,\n                     scoring=scoring_metric)\n\ngs_DT.fit(D_train, t_train);","7aa1dc8d":"gs_DT.best_params_ #This is the best parameters which are obtained","9a557ac4":"gs_DT.best_score_","2549ca5b":"results_DT = pd.DataFrame(gs_DT.cv_results_['params'])\nresults_DT['test_score'] = gs_DT.cv_results_['mean_test_score']\nresults_DT.columns\n\nalt.Chart(results_DT, \n          title='DT Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('max_depth', title='Maximum Depth'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n    color='min_samples_split:N' \n).interactive()","5e2441fa":"from sklearn.ensemble import RandomForestClassifier\n\nrf_classifier = RandomForestClassifier(random_state=25)\n\nparams_RF = {\n    'max_depth': [5,7,10,12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n            \ngs_RF = GridSearchCV(estimator = rf_classifier, \n                     param_grid=params_RF, \n                     cv=cv_method_train,\n                     verbose=1,\n                     n_jobs=-2,\n                     scoring=scoring_metric)\n\ngs_RF.fit(D_train, t_train);","ac5dbb15":"gs_RF.best_params_","52fe0220":"gs_RF.best_score_","b77390bf":"results_RF = pd.DataFrame(gs_RF.cv_results_['params'])\nresults_RF['test_score'] = gs_RF.cv_results_['mean_test_score']\nresults_RF.columns\n\nalt.Chart(results_RF, \n          title='RF Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('max_depth', title='Maximum Depth'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n    color='n_estimators:N' # N is for nominal\n).interactive()","bccebbad":"from sklearn.preprocessing import PowerTransformer\nnb_classifier = GaussianNB()\n\nparams_NB = {'var_smoothing': np.logspace(1,-2, num=50)}\n\ngs_NB = GridSearchCV(estimator=nb_classifier, \n                     param_grid=params_NB, \n                     cv=cv_method_train,\n                     verbose=1, \n                     scoring=scoring_metric)\n\nD_train_transformed = PowerTransformer().fit_transform(D_train)\n\ngs_NB.fit(D_train, t_train);","88663123":"gs_NB.best_params_","b43e6f86":"gs_NB.best_score_","5e6a7a2e":"results_NB = pd.DataFrame(gs_NB.cv_results_['params'])\nresults_NB['test_score'] = gs_NB.cv_results_['mean_test_score']\n\nalt.Chart(results_NB, \n          title='NB Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('var_smoothing', title='Var. Smoothing'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False))\n).interactive()","aeb6ea3c":"from sklearn.model_selection import cross_val_score\ncv_method_test = StratifiedKFold(n_splits=5, shuffle=True, random_state=25)","df525c64":"cv_results_KNN = cross_val_score(estimator=gs_KNN.best_estimator_,\n                                 X=D_test,\n                                 y=t_test, \n                                 cv=cv_method_test, \n                                 scoring='roc_auc')\ncv_results_KNN.mean()","23227095":"cv_results_DT = cross_val_score(estimator=gs_DT.best_estimator_,\n                                 X=D_test,\n                                 y=t_test, \n                                 cv=cv_method_test, \n                                 scoring='roc_auc')\ncv_results_DT.mean()","08f4d485":"D_Test_fs_transformed = PowerTransformer().fit_transform(D_test)\n\ncv_results_NB = cross_val_score(estimator=gs_NB.best_estimator_,\n                                 X=D_test,\n                                 y=t_test, \n                                 cv=cv_method_test, \n                                 scoring='roc_auc')\ncv_results_NB.mean()","1d29bbd0":"cv_results_RF = cross_val_score(estimator=gs_RF.best_estimator_,\n                                 X=D_test,\n                                 y=t_test, \n                                 cv=cv_method_test, \n                                 n_jobs=-2,\n                                 scoring='roc_auc')\ncv_results_RF.mean()","c27fd2be":"print(stats.ttest_rel(cv_results_DT, cv_results_RF))\nprint(stats.ttest_rel(cv_results_DT, cv_results_KNN))\nprint(stats.ttest_rel(cv_results_DT, cv_results_NB))","add154b3":"t1_pred = gs_KNN.predict(D_test)\nt2_pred = gs_DT.predict(D_test)\nt3_pred = gs_NB.predict(D_test)\nt4_pred = gs_RF.predict(D_test)","3aabc136":"from sklearn import metrics\n\nprint('Accuracy of KNN score: ' , metrics.accuracy_score(t_test , t1_pred))\nprint('Accuracy of Decision Tree score: ' ,metrics.accuracy_score(t_test , t2_pred))\nprint('Accuracy of Gaussian Naive score: ' ,metrics.accuracy_score(t_test , t3_pred))\nprint('Accuracy of Random Forest score: ' ,metrics.accuracy_score(t_test , t4_pred))\n    ","a42b9b6e":"print(metrics.confusion_matrix(t_test, t1_pred))","55fcbc28":"print(metrics.confusion_matrix(t_test, t2_pred))","d35e44a0":"print(metrics.confusion_matrix(t_test, t3_pred))","45820747":"print(metrics.confusion_matrix(t_test, t4_pred))","b5d75b04":"print(\"KNN : \")\nprint(metrics.classification_report(t_test, t1_pred , labels = np.unique(t1_pred)))\nprint(\"------------------------------------------------------\")\n\nprint(\"Decision Tree : \")\nprint(metrics.classification_report(t_test, t2_pred , labels = np.unique(t2_pred)))\nprint(\"------------------------------------------------------\")\n\nprint(\"Gaussian Naive : \")\nprint(metrics.classification_report(t_test, t3_pred , labels = np.unique(t3_pred)))\nprint(\"------------------------------------------------------\")\n\nprint(\"Random Forest : \")\nprint(metrics.classification_report(t_test, t4_pred , labels = np.unique(t4_pred)))","f01a9a56":"We can see that the p-value is greater than 0.05 which indicates the value is not significantly better. We can say that Decision Trees is marginally better than the other classifiers. ","08dd7a80":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-About-the-dataset\" data-toc-modified-id=\"1.-About-the-dataset-1\">1. About the dataset<\/a><\/span><\/li><li><span><a href=\"#2.-Goals-and-Objective\" data-toc-modified-id=\"2.-Goals-and-Objective-2\">2. Goals and Objective<\/a><\/span><\/li><li><span><a href=\"#3.-Data-Pre-processing\" data-toc-modified-id=\"3.-Data-Pre-processing-3\">3. Data Pre-processing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Importing-Packages\" data-toc-modified-id=\"3.1-Importing-Packages-3.1\">3.1 Importing Packages<\/a><\/span><\/li><li><span><a href=\"#3.2-Importing-the-data\" data-toc-modified-id=\"3.2-Importing-the-data-3.2\">3.2 Importing the data<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#4-Data-exploration-and-visualization\" data-toc-modified-id=\"4-Data-exploration-and-visualization-4\">4 Data exploration and visualization<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#4.1-One-Variable-Plots\" data-toc-modified-id=\"4.1-One-Variable-Plots-4.1\">4.1 One-Variable Plots<\/a><\/span><\/li><li><span><a href=\"#4.2-Two-Variable-Plots\" data-toc-modified-id=\"4.2-Two-Variable-Plots-4.2\">4.2 Two Variable Plots<\/a><\/span><\/li><li><span><a href=\"#4.3-Three-Variable-plots\" data-toc-modified-id=\"4.3-Three-Variable-plots-4.3\">4.3 Three Variable plots<\/a><\/span><\/li><li><span><a href=\"#4.3-Data-preparation\" data-toc-modified-id=\"4.3-Data-preparation-4.4\">4.3 Data preparation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#5.-Predictive-Modelling\" data-toc-modified-id=\"5.-Predictive-Modelling-5\">5. Predictive Modelling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Fitting-Different-classifiers\" data-toc-modified-id=\"5.1-Fitting-Different-classifiers-5.1\">5.1 Fitting Different classifiers<\/a><\/span><\/li><li><span><a href=\"#5.2-Feature-Selection-and-Ranking\" data-toc-modified-id=\"5.2-Feature-Selection-and-Ranking-5.2\">5.2 Feature Selection and Ranking<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#a)--All-features\" data-toc-modified-id=\"a)--All-features-5.2.1\">a)  All features<\/a><\/span><\/li><li><span><a href=\"#b)--Using-F-Score\" data-toc-modified-id=\"b)--Using-F-Score-5.2.2\">b)  Using F-Score<\/a><\/span><\/li><li><span><a href=\"#c)-Mutual-Information\" data-toc-modified-id=\"c)-Mutual-Information-5.2.3\">c) Mutual Information<\/a><\/span><\/li><li><span><a href=\"#d)-Random-Forest-Importance\" data-toc-modified-id=\"d)-Random-Forest-Importance-5.2.4\">d) Random Forest Importance<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#5.3-Hyper-parameter-Tuning\" data-toc-modified-id=\"5.3-Hyper-parameter-Tuning-5.3\">5.3 Hyper-parameter Tuning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#a)-KNN-Classifier\" data-toc-modified-id=\"a)-KNN-Classifier-5.3.1\">a) KNN Classifier<\/a><\/span><\/li><li><span><a href=\"#b)--Decision-Tree\" data-toc-modified-id=\"b)--Decision-Tree-5.3.2\">b)  Decision Tree<\/a><\/span><\/li><li><span><a href=\"#c)-Random-Forest\" data-toc-modified-id=\"c)-Random-Forest-5.3.3\">c) Random Forest<\/a><\/span><\/li><li><span><a href=\"#d)-Gaussian-Naive\" data-toc-modified-id=\"d)-Gaussian-Naive-5.3.4\">d) Gaussian Naive<\/a><\/span><\/li><li><span><a href=\"#e)-Comparison-of-each-Hyper-parameter-using-T-tests\" data-toc-modified-id=\"e)-Comparison-of-each-Hyper-parameter-using-T-tests-5.3.5\">e) Comparison of each Hyper-parameter using T-tests<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#6.-Prediction-and-model-evaluation\" data-toc-modified-id=\"6.-Prediction-and-model-evaluation-6\">6. Prediction and model evaluation<\/a><\/span><\/li><li><span><a href=\"#7.-Critique\" data-toc-modified-id=\"7.-Critique-7\">7. Critique<\/a><\/span><\/li><li><span><a href=\"#8.-Summary-and-Conclusions\" data-toc-modified-id=\"8.-Summary-and-Conclusions-8\">8. Summary and Conclusions<\/a><\/span><\/li><li><span><a href=\"#9-Citations\" data-toc-modified-id=\"9-Citations-9\">9 Citations<\/a><\/span><\/li><\/ul><\/div>","adcac971":"\n#### b)  Decision Tree\n\n","f18bcdc7":"## 9 Citations","c548ce51":"Now fitting the tuned classifier with the best set of hyper parameter values on test data by cross validation. We can use five fold test CV on each of the tuned classifier. ","3cb3d32d":"#### c) Random Forest","d5ce9b07":"\n\n#### b)  Using F-Score\n\n\n","53736db8":"<br> Application to help insurance companies make better decisions before making the auto-insurance contract based on various parameters. \n\n<br>The objective is to test various classification algorithms and employing the best model for this application to get accurate predictions as to whether insurance can be given or not.  ","f6a8e5ac":"\n#### c) Mutual Information \n\n","4758a0e1":"### 5.3 Hyper-parameter Tuning","f6e8fd76":"Removing Redundant column : It can be observed that a few continuous features have been binned already. Therefore, such columns must be dropped.","5060fcce":"Now we can compare random forest to other classifiers because RF has faired the best. By doing so we can find : ","47d8d02b":"## 7. Critique","8e4e9953":"## 5. Predictive Modelling","0d1b8c26":"## 4 Data exploration and visualization\n\n### 4.1 One-Variable Plots\nHere, we explore the features as they are in isolation to understand how the observations are distributed.","ee83b0b4":"The years of experience for both men and women lies in the same bracket. There's no significant difference.","4ed714b5":"<br>Source : [GitHub - Dr.Vural Aksakalli](https:\/\/github.com\/vaksakalli)\n<br>Source : [Feature Ranking](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/)\n<br>Source : [Kaggle kernal](https:\/\/www.kaggle.com\/quekyaojing\/basic-data-overview-and-manipulation)\n<br>Source : [Towards Data Science](https:\/\/towardsdatascience.com\/a-z-of-exploratory-data-analysis-under-10-mins-aae0f598dfff)","85dc7c62":"<br>Now the one hot encoded data which is the form of numpy has no column names.\n<br>We can do this by getting the column names from df_oneHot.","c7b1a694":"### 5.2 Feature Selection and Ranking","832a912b":"## 2. Goals and Objective","09c7fa7d":"### 3.2 Importing the data\nImporting the data from the source.","9c0d6606":"## 6. Prediction and model evaluation ","11583798":"### 4.3 Data preparation","b07c13f5":"We found out from the previous section that Decision Tree has slightly better accurate test results compared to other models.","92ef2e3a":"We see a lot of overlapping in the data. Thus, we can say that no feature in itself is the best one to begin with.\n\nLet's go ahead with the rest of our data preparation.","05c749d6":"<br> 1) The data points had an almost zero correlation between the target and the variables. This means that our predictions cannot be made accurately based on recorded features. Therefore as expected, the models had an average accuracy. <br>2) This data could be made better by recording more relevant parameters which have a better correlation with the target feature. <br>3) The use of very few parameters while Hyper parameter tuning has contributed to a mediocre accuracy.  ","680171d4":"The distribution of the credit history of both the genders is fairly the same.\n\nHowever, to check for the best features that have a co-relation with the target, the easiest approach is to find the co-relation matrix.","fe650d51":"### 5.1 Fitting Different classifiers","d4f1a7d1":"Getting rid of all the missing values","6529ad3d":"We can see from the above target column split ratio is 34:66","03e2889c":"As seen from the results above, we can see that decision trees were giving the highest score and is hence used for all cross validation test results. \n\nWe will determine if selecting all the features is more accurate or selecting just 5 features is better. We can do that by selecting features by the follwing methods.","dcc28826":"## 1. About the dataset","3186cb5f":"There is a huge difference in the number of observations. Let's have an equal number of observations.","26deecba":" # <center>Safe Driver Prediction<\/center>","abb12c8f":"Getting the value counts of each of the existing columns to see if any anomolous values exist : ","45adbb7c":"#### d) Gaussian Naive ","472249bd":"It is therefore hard to say what features best describe the target. Paiplot is plotted below to give us an understanding of the data better.","e193cbcf":"Now making a copy of the original dataset so that we dont make changes to the dataset which is cleaned so far","e13519b0":"## 8. Summary and Conclusions","3196a0b9":"The annual claims appear to be between 1 and 2.","fff11173":"\n\n#### a)  All features\n\n\n","16ef6713":"From the above plots, we can observe that most of the features are skewed to the left or right. It might be that there is not a clear co-relation between the features an the target.\n\n### 4.2 Two Variable Plots","96ddf2ff":"\n\n####    a) KNN Classifier\n\n","a34d4af3":"It looks like years_Experience_bucket has two values 9-14' and 3-8' which needs to be replaced and binned more appropriately. Changing both of these to range of 3-15.","7a98a1bb":"<br>1) The data obtained was observed to be highly uncorrelated to the target feature. We can observe this from the data exploration phase (4.2 three variable plots)\n<br>2) As seen above, the precision is 50% for both the target values. \n<br>3) The recall for Knn, Random Forest model is more than 50% which indicates they would be a better model with respect to recall.  \n<br>4) Looking at F1 score, we can observe that all the classifiers except Gaussian Naive have similar performances. \n\nThus we can conclude that KNN and Random Forest give us similar accuracy, prediction, recall and F1 scores. ","47d4d88d":"\n#### d) Random Forest Importance\n\n","d0dc83d4":"<br>Dataset Name : Safe Driver Prediction \n<br>Source : [Safe Driver Prediction](https:\/\/www.kaggle.com\/mu202199\/safe-driver-prediction)\n\nThe dataset contains different parameters which the insurance company uses to charge the auto-insurance. This dataset also consists of a target feature, which predicts the probability that a driver will initiate an auto insurance claim in the following year. The target feature is already label encoded as 0 - the driver will not initiate the auto insurance and 1 - being the driver will initiate the auto insurance. \n\nThe dataset also consists of the some of the variables which are already binned and named as bucket.","e5949547":"#### e) Comparison of each Hyper-parameter using T-tests","f425d4e7":"We see a lot of negative co-relation. The heatmap for the above appears as below.\n\n### 4.3 Three Variable plots","a0c63453":"### 3.1 Importing Packages\nImporting the required libraries. Pandas and Numpy for data manipulation. Seaborne for plotting.","c260a72e":"We can now make a change to our parameters being passed. Since we dont need to take only some features, we can now test see what is the most suitable model to be used and the perfect parameters to be fed into the algorithm.","326699a5":"## 3. Data Pre-processing","41183182":"<br>Ho = Selecting all features \n<br>H1 = Selecting only 5 important features is better for prediction \n\nAs we can see above, the p-value between each of these test results are very high and thus indicated that the alternate hypothesis is very weak. It is better to stick with all the features rather than 5 of those features \n\nThus alternate hypothesis is rejected\n\n\n"}}