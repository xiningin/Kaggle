{"cell_type":{"e323e043":"code","89313ce5":"code","e3ac5108":"code","424e245a":"code","b6102f37":"code","e2c51420":"code","83a66486":"code","de96ff0e":"code","9549b9bb":"code","24dec2ee":"code","d1865048":"code","cf248e88":"code","24a94f51":"code","2a998ed7":"code","aadb4881":"code","cb54b3c3":"code","d87755e5":"code","7ad79553":"code","cb72daf8":"markdown","9c7cb91e":"markdown","baffe824":"markdown","0293a595":"markdown","153a9cfe":"markdown","fd75a99a":"markdown","bd4d0a93":"markdown","4dc6bdc3":"markdown","15bb42f6":"markdown"},"source":{"e323e043":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89313ce5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing, impute\nfrom IPython.display import display\n\nfrom sklearn.dummy import DummyRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict, cross_validate\nfrom sklearn.model_selection import train_test_split, GroupKFold\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.linear_model import Ridge, RidgeCV\n\nfrom fbprophet import Prophet\nimport optuna\nimport statsmodels.api as sm\nimport scipy.stats as stats\n\nplt.style.use('ggplot')","e3ac5108":"random_state = 42\nrng = np.random.RandomState(random_state)","424e245a":"X = pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/train.csv', index_col=0)\nX_test = pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/test.csv', index_col=0)","b6102f37":"print(X.columns.difference(X_test.columns))\ny = X.pop('pressure')","e2c51420":"# num_columns = ['R', 'C', 'u_in', 'u_out']\nds = 'time_step'\nts_id = 'breath_id'","83a66486":"def get_folds(X, y, n_splits=5, ts_id=ts_id):\n    # return fixed list of CV index (for XGB, LGB compability to groups)\n    folds = GroupKFold(n_splits=n_splits)\n    return list(folds.split(X, y, groups=X[ts_id]))\n\n\ndef score_model(model, X=X, y=y, folds=None, fit_as_score=True):\n    if folds is None:\n        folds = get_folds(X, y)\n        \n    sample_weight = X.mask(lambda x: x['u_out'] == 0).isna().any(axis=1).astype(int)\n    \n    def scorer(y_true, y_pred, sample_weight=sample_weight):\n        return mean_absolute_error(y_true.values, y_pred, sample_weight=sample_weight.loc[y_true.index].values)\n    scoring = make_scorer(scorer, greater_is_better=False, sample_weight=sample_weight)\n    \n    if fit_as_score:\n        fit_params = {'sample_weight':sample_weight}\n    else:\n        fit_params = {}\n        \n    d = cross_validate(model, X, y, scoring=scoring, cv=folds, n_jobs=1, fit_params=fit_params, return_train_score=True)\n    d['train_score'] = -d['train_score']\n    d['test_score'] = -d['test_score']\n    return pd.DataFrame(d).style.bar(subset=['fit_time', 'score_time'], vmin=0, vmax=30)\\\n                                .bar(subset=['test_score', 'train_score'], vmin=0, vmax=7.5)","de96ff0e":"def add_lag(X, k=1, columns=['u_in', 'time_step'], fill=0):\n    X = X.copy()\n    if isinstance(k, int):\n        k = list(range(1, k+1))\n    if columns is None:\n        columns = X.columns.tolist()\n    \n    for i in k:\n        X_lag = X[columns].groupby(X[ts_id]).shift(i)\n        if fill is not None:\n            X_lag = X_lag.fillna(fill)\n        X = X.join(X_lag.add_prefix(f'lag_{i}_'))\n        \n    return X\n\n\ndef proc(X):\n    pi = np.pi\n    X = X.eval('''RC=R*C\/1000\n                  tau=2 * @pi * RC\n                  t=time_step\/tau\n                  et=1-exp(-t)\n                  uet=u_in*et''')\\\n         .assign(time_diff=lambda x: x.groupby(ts_id)['time_step'].diff().fillna(0),\n                 int_u_in=lambda x: x.eval('u_in*time_diff').groupby(x[ts_id]).cumsum(),\n                 sum_u_in=lambda x: x.groupby(ts_id)['u_in'].cumsum(),\n                 diff_u_in=lambda x: x.groupby(ts_id)['u_in'].diff().fillna(0),\n                 max_u_in=lambda x: x.groupby(ts_id)['u_in'].transform('max'),)\n    return X","9549b9bb":"X_proc = add_lag(proc(X).query('u_out==0'), k=[2], columns=['u_in', 'time_step', 'int_u_in', 'sum_u_in'])\ny_proc = y.loc[X['u_out'] == 0]\n\n# select categorical as 1 value at each \"breath_id\"\ncategorical_feature = [ts_id] + (X_proc.groupby(ts_id).nunique().mean() == 1).loc[lambda x: x].index.tolist()\n# X_post[categorical_features] = pd.Categorical(X_post[categorical_features])","24dec2ee":"params = {\n    \"objective\": \"regression_l1\",\n    \"random_state\": random_state,\n    \"metric\": \"l1\",\n    \"verbosity\": -1,\n    #\"learning_rate\": 0.36, #leave it by default\n    \"num_threads\":4,\n}","d1865048":"# from optuna.integration import lightgbm as lgb_opt\n\n# dtrain = lgb_opt.Dataset(X_proc, y_proc)\n\n# study = optuna.create_study()\n# lgb_tuner = lgb_opt.LightGBMTunerCV(\n#     params=params,\n#     train_set=dtrain,\n#     folds=get_folds(X_proc, y_proc),\n#     shuffle=False,\n#     stratified=False,\n#     #return_cvbooster=True,\n#     verbose_eval=-1,\n#     show_progress_bar=True,\n#     optuna_seed=random_state,\n#     early_stopping_rounds=100,\n#     num_boost_round=500,#1_000,\n#     study=study,\n# )\n# lgb_tuner.run()","cf248e88":"# optuna.visualization.plot_optimization_history(study)","24a94f51":"# print(\"Best score:\", lgb_tuner.best_score)\n# best_params = lgb_tuner.best_params\n# print(\"Best params:\", best_params)\n# print(\"  Params: \")\n# for key, value in best_params.items():\n#     print(\"    {}: {}\".format(key, value))\n    \n# # print(\"best iteration: \", lgb_tuner.get_best_booster().best_iteration)","2a998ed7":"# params = lgb_tuner.best_params\nparams['n_estimators'] = 100_000 # int(lgb_tuner.get_best_booster().best_iteration*1.1)\nparams.pop('num_threads')\n\nprint(params)","aadb4881":"# score_model(lgb.LGBMRegressor(**params), X=X_proc, y=y_proc)","cb54b3c3":"X_test_proc = add_lag(proc(X_test).query('u_out==0'), k=[2], columns=['u_in', 'time_step', 'int_u_in', 'sum_u_in'])","d87755e5":"model = lgb.LGBMRegressor(**params).fit(X_proc, y_proc)\ny_pred = model.predict(X_test_proc)","7ad79553":"y_s = pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/sample_submission.csv', index_col=0)\ny_s.loc[X_test_proc.index] = y_pred\n\ny_s.to_csv(\"submission.csv\")","cb72daf8":"Best manually optimized learning rate","9c7cb91e":"# Baseline","baffe824":"# Processing","0293a595":"# Summary\n\nPrevious notebook with EDA, modelling and manual hyper-parameter tuning:  \n[Ventilator Pressure Prediction - LGB EDA](https:\/\/www.kaggle.com\/squarex\/ventilator-pressure-prediction-lgb-eda)","153a9cfe":"# Hyperparameter tuning","fd75a99a":"# Submission","bd4d0a93":"# Compare to baseline","4dc6bdc3":"Results with n_estimators = 10_000\n```\n \tfit_time\tscore_time\ttest_score\ttrain_score\n0\t902.430400\t190.816195\t0.734996\t0.497027\n1\t935.501294\t116.737442\t0.732414\t0.501023\n2\t877.202830\t106.261424\t0.739981\t0.495680\n3\t899.060446\t196.055113\t0.735223\t0.500615\n4\t897.823282\t133.426292\t0.736875\t0.495777\n```","15bb42f6":"Hyperparameter-tuning improve score less than just increasing the number of estimators to infinite.\n(The model struggles to overfit so we can push n_estimators.)"}}