{"cell_type":{"ae6f291f":"code","498df151":"code","007afb10":"code","0da8690c":"code","0b381a18":"code","95eb54b3":"code","00a191cf":"code","b7398900":"code","d0846956":"code","9db79d7d":"code","16fc3049":"code","cc88225a":"code","558d0292":"code","a2f4e863":"code","76b7bb35":"code","2e83ebc3":"code","31370405":"code","7a8889fe":"code","e3f6081e":"code","46762bb1":"code","280b0532":"code","7609f503":"code","f1ae5320":"code","fd1dffdc":"code","92d46361":"code","12d48861":"code","c0480f77":"code","c201c17a":"code","083b6453":"code","b4eb6c7e":"code","33684df1":"code","a1bb0006":"code","89efd7f6":"code","23c4ddc6":"code","035b2c01":"code","5a9d3389":"code","f6186aa2":"code","9ba05e0b":"code","110056be":"code","95835ad5":"code","1b31da17":"code","ba230a1e":"code","789e40d2":"code","20f3fafe":"code","eea54c33":"code","6436ceb0":"code","899c350a":"code","d427fade":"code","9fa286cb":"code","2ce58de6":"markdown","63e66223":"markdown","7e0a3a47":"markdown","0a7e6b2c":"markdown","7b9ccf7d":"markdown","d7f5f3ba":"markdown","ee8b82c7":"markdown","b7e4944e":"markdown","4a1e5b08":"markdown","0b876114":"markdown","6cb4ecde":"markdown","5c3e85b0":"markdown","d4cc3d01":"markdown","75e2781c":"markdown","b4f53bba":"markdown","ba81a354":"markdown","af4e0732":"markdown","769d43a5":"markdown","04c5502e":"markdown","c45a527e":"markdown","d43fdb5c":"markdown","4ac9fd71":"markdown","0ea3643d":"markdown","f8092371":"markdown","8f6dbe69":"markdown","98056080":"markdown"},"source":{"ae6f291f":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns\nfrom collections import Counter\n\n\nimport warnings \nwarnings.filterwarnings('ignore')","498df151":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","007afb10":"df.columns","0da8690c":"df.describe().T","0b381a18":"df.shape","95eb54b3":"df.info()","00a191cf":"# Encode the categorical data values 'diagnosis'\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_Y = LabelEncoder()\ndf.iloc[:,1] = labelencoder_Y.fit_transform(df.iloc[:,1].values)","b7398900":"# Create a pair plot\nsns.pairplot(df.iloc[:, 1:6], hue = 'diagnosis');","d0846956":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(df[variable],bins = 10)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with histogram\".format(variable))\n    plt.show()","9db79d7d":"selected_numericalVar = ['diagnosis','radius_mean', 'perimeter_mean', 'area_mean','concavity_mean', \"concave points_mean\"]\nfor n in selected_numericalVar:\n    plot_hist(n)","16fc3049":"# Radius Mean - Diagnosis\ndf[['radius_mean', 'diagnosis']].groupby(['radius_mean'], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","cc88225a":"# Perimeter  Mean - Diagnosis\ndf[['perimeter_mean', 'diagnosis']].groupby(['perimeter_mean'], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","558d0292":"# Area Mean - Diagnosis\ndf[['area_mean', 'diagnosis']].groupby(['area_mean'], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","a2f4e863":"# Concavity Mean - Diagnosis\ndf[['concavity_mean', 'diagnosis']].groupby(['concavity_mean'], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","76b7bb35":"# Concave Points Mean - Diagnosis\ndf[[\"concave points_mean\", 'diagnosis']].groupby([\"concave points_mean\"], as_index = False).mean().sort_values(by = 'diagnosis', ascending = False)","2e83ebc3":"def detect_outlier(df, features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3-Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # Detect Outlier and Their Indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # Store Indices\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","31370405":"df.loc[detect_outlier(df, ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n                           'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n                           'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n                           'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n                           'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n                           'fractal_dimension_se', 'radius_worst', 'texture_worst',\n                           'perimeter_worst', 'area_worst', 'smoothness_worst',\n                           'compactness_worst', 'concavity_worst', 'concave points_worst',\n                           'symmetry_worst', 'fractal_dimension_worst'])]","7a8889fe":"# drop outliers \ndf= df.drop(detect_outlier(df, ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n                           'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n                           'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n                           'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n                           'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n                           'fractal_dimension_se', 'radius_worst', 'texture_worst',\n                           'perimeter_worst', 'area_worst', 'smoothness_worst',\n                           'compactness_worst', 'concavity_worst', 'concave points_worst',\n                           'symmetry_worst', 'fractal_dimension_worst']), axis = 0).reset_index(drop = True)","e3f6081e":"df.columns[df.isnull().any()]","46762bb1":"df['Unnamed: 32']","280b0532":"del df['Unnamed: 32']","7609f503":"# Let's create a colorful correlation matrix \nsns.heatmap(df.iloc[:,1:12].corr(), annot = True);\n# as you can see, it is much better than the table\n# but if you want you can use\n# df.iloc[:,1:12].corr()","f1ae5320":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, 'radius_mean', bins = 2)\nplt.show()\n","fd1dffdc":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, 'perimeter_mean', bins = 5)\nplt.show()\n","92d46361":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, 'area_mean', bins = 50)\nplt.show()\n","12d48861":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, 'concavity_mean')\nplt.show()\n","c0480f77":"g = sns.FacetGrid(df, col = 'diagnosis')\ng.map(sns.distplot, \"concave points_mean\")\nplt.show()\n","c201c17a":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","083b6453":"#  Split the data set into independent (X) and dependent(Y) data sets\nX = df.iloc[:,2:31].values\ny = df.iloc[:,1].values","b4eb6c7e":"# Split the data set into 67% training and 33% testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state = 123)\n","33684df1":"# Scale tge data (Feature scaling)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc  = sc.fit_transform(X_test)\n\n#X_train","a1bb0006":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nacc_log_train = round(logreg.score(X_train, y_train)*100,2) \nacc_log_test = round(logreg.score(X_test,y_test)*100,2)\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","89efd7f6":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n              SVC(random_state = random_state),\n              RandomForestClassifier(random_state = random_state),\n              LogisticRegression(random_state = random_state),\n              KNeighborsClassifier()]","23c4ddc6":"dt_param_grid = {'min_samples_split': range(10,500,20),\n                'max_depth': range(1,20,2)}\nsvc_param_grid = {'kernel': ['rbf'],\n                  'gamma' : [0.001, 0.01, 0.1, 1],\n                  'C'     : [1,10,50,100,200,300,1000]}\nrf_param_grid = {\"max_features\": [1,3,10],\n                 \"min_samples_split\":[2,3,10],\n                 \"min_samples_leaf\":[1,3,10],\n                 \"bootstrap\":[False],\n                 \"n_estimators\":[100,300],\n                 \"criterion\":[\"gini\"]}\nlogreg_param_grid = {'C'      : np.logspace(-3,3,7),\n                     'penalty':['l1', 'l2']}\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                  \"weights\"    : [\"uniform\",\"distance\"],\n                  \"metric\"     :[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                    svc_param_grid,\n                    rf_param_grid,\n                    logreg_param_grid,\n                    knn_param_grid]","035b2c01":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","5a9d3389":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result,\n                           \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\"LogisticRegression\",\"KNeighborsClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","f6186aa2":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import MinMaxScaler","9ba05e0b":"df.head() # diagnosis part already encoded\n","110056be":"X = df.iloc[:,2:31].values\ny = df.iloc[:,1].values\n\nmin_max_scaler = MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(X)","95835ad5":"# Split our data 80% training \/ 10% testing \/ 10% validation\nX_train, X_val_and_test, y_train, y_val_and_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 123)\n# so let split val_and_test datas\nX_val, X_test, y_val, y_test = train_test_split(X_val_and_test, y_val_and_test, test_size = 0.5, random_state = 123)\nprint(\"X_train shape :\" ,X_train.shape, \"X_val shape :\", X_val.shape, \"X_test shape :\",X_test.shape)\nprint(\"y_train shape :\" ,y_train.shape, \"y_val shape :\", y_val.shape, \"y_test shape :\",y_test.shape)","1b31da17":"# Build the model and architecture of the deep neural network\nmodel = Sequential() # innitializes the NN\nmodel.add(Dense(units = 32, activation= 'relu',input_dim = 29))\nmodel.add(Dense(units = 32, activation= 'relu'))\nmodel.add(Dense(units = 32, activation= 'relu'))\nmodel.add(Dense(units = 1, activation= 'sigmoid'))","ba230a1e":"# Loss function measures how well the model did on training and then tries to improve on it using optimizer\nmodel.compile(optimizer='sgd',\n              loss = 'binary_crossentropy',\n              metrics = ['accuracy']\n              )","789e40d2":"# Train the model\nhist = model.fit(\n    X_train, y_train,\n    batch_size = 32,\n    epochs = 100,\n    validation_data = (X_val, y_val)\n)","20f3fafe":"model.evaluate(X_test, y_test)[1] # I want to see accuracy, thats the why I wrote [1]\n# It says 1 accurate, It's perfect","eea54c33":"# Make a prediction\nprediction = model.predict(X_test)\nprediction = [1 if y>=0.5 else 0 for y in prediction]\nprediction","6436ceb0":"# visualize the training loss and validation loss to see if the model is over fitting\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc = 'upper right');","899c350a":"# It seems not over fitted\nhist.history['val_accuracy']","d427fade":"# visualize the training accuracy and validation accuracy to see if the model is over fitting\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc = 'lower right');","9fa286cb":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, prediction))\nprint(accuracy_score(y_test, prediction))","2ce58de6":"<a id='4' a><\/r>\n# 4.Outlier Detection","63e66223":"<a id='6.4' a><\/r>\n# 6.4 Area Mean -- Diagnosis","7e0a3a47":"<a id='1.2' a><\/r>\n## 1.2 Load and Check Data","0a7e6b2c":"<a id='6.5' a><\/r>\n# 6.5 Concavity Mean -- Diagnosis","7b9ccf7d":"1. [Import Libraries and Data](#1)\n    * 1.1 [Import Libraries](#1.1)\n    * 1.2 [Load and Check Data](#1.2)\n2. [Variable Description](#2)\n    * 2.1 [Univariate Variable Analysis](#2.1)\n    * 2.2 [Selected Numerical Variable](#2.2)\n3. [Basic Data Analysis](#3)\n4. [Outlier Detection](#4)\n5. [Missing Value](#5)\n    * 5.1 [Find Missing Value](#5.1)\n6. [Visuzalization](#6)\n    * 6.1 [Correlation](#6.1)\n    * 6.2 [Radius Mean -- Diagnosis](#6.2)\n    * 6.3 [Perimeter Mean -- Diagnosis](#6.3)\n    * 6.4 [Area Mean -- Diagnosis](#6.4)\n    * 6.5 [Concavity Points Mean -- Diagnosis](#6.5)  \n7. [Modeling](#7)\n    * 7.1 [Train Test Split](#7.1)\n    * 7.2 [Scaling](#7.2)\n    * 7.3 [Training](#7.3)\n    * 7.4 [Hyperparameter Tuning -- Grid Search -- Cross Validation](#7.4)\n    * 7.5 [Lets try with NN](#7.5)","d7f5f3ba":"<a id='6.1' a><\/r>\n# 6.1 Correlation","ee8b82c7":"<a id='7.2' a><\/r>\n\n# 7.2 Scaling","b7e4944e":"<a id='2.1' a><\/r>\n## 2.1 Univariate Variable Analysis\n**Categorical Variable** : diagnosis  \n**Numerical Variable** : radius_mean, texture_mean, perimeter_mean, area_mean, smoothness_mean, compactness_mean, concavity_mean,    \n                      concave points_mean, symmetry_mean, fractal_dimension_mean, radius_se, texture_se, perimeter_se, area_se,  \n                      smoothness_se,compactness_se, concavity_se, concave points_se, symmetry_se, fractal_dimension_se,   radius_worst,\n                      texture_worst, perimeter_worst, area_worst, smoothness_worst, compactness_worst, concavity_worst, concave                             points_worst, symmetry_worst, fractal_dimension_worst, id  ","4a1e5b08":"<a id='7.3' a><\/r>\n\n# 7.3 Training","0b876114":"<a id='6.6' a><\/r>\n# 6.5 Concavity Points Mean -- Diagnosis","6cb4ecde":"<a id='3' a><\/r>\n# 3. Basic Data Analysis\n* Radius Mean - Diagnosis\n* Perimeter Mean - Diagnosis\n* Area Mean - Diagnosis\n* Concavity Mean - Diagnosis\n* Concave Points Mean - Diagnosis","5c3e85b0":"<a id='6' a><\/r>\n# 6. Visuzalization","d4cc3d01":"<a id='2' a><\/r>\n# 2. Variable Description\n  \n\n  \nAttribute Information:\n\n1) ID number  \n2) Diagnosis (M = malignant, B = benign)  \n3-32)  \n  \nTen real-valued features are computed for each cell nucleus:  \n  \na) radius (mean of distances from center to points on the perimeter)  \nb) texture (standard deviation of gray-scale values)  \nc) perimeter  \nd) area  \ne) smoothness (local variation in radius lengths)  \nf) compactness (perimeter^2 \/ area - 1.0)  \ng) concavity (severity of concave portions of the contour)  \nh) concave points (number of concave portions of the contour)  \ni) symmetry  \nj) fractal dimension (\"coastline approximation\" - 1)  ","75e2781c":"<a id='6.3' a><\/r>\n# 6.3 Perimeter Mean -- Diagnosis","b4f53bba":"<a id='7.1' a><\/r>\n\n# 7.1 Train Test Split","ba81a354":"<a id='6.2' a><\/r>\n# 6.2 Radius Mean -- Diagnosis","af4e0732":"<a id='7' a><\/r>\n\n# 7. Modeling","769d43a5":"<a id='7.5' a><\/r>\n\n## 7.5 Lets try with NN","04c5502e":"<a id='1.1' a><\/r>\n## 1.1 Import Libraries","c45a527e":"<a id='2.2' a><\/r>\n## 2.2 Selected Numerical Variable","d43fdb5c":"<a id='5' a><\/r>\n# 5. Missing Value","4ac9fd71":"<a id='5.1' a><\/r>\n# 5.1 Find Missing Value","0ea3643d":"<a id='1' a><\/r>\n# 1. Import Libraries, Load and Check Data","f8092371":"* Just because we have an unnecessary column, I directly delete it","8f6dbe69":"* So we have 83 rows outlier variables, lets drop them","98056080":"<a id='7.4' a><\/r>\n\n## 7.4 Hyperparameter Tuning -- Grid Search -- Cross Validation\nCompare 5 ML classifier and evaluate mean accuracy of each of them by stratified cross validation\n\n* Decision Tree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression"}}