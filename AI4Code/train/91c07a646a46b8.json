{"cell_type":{"db8afcd0":"code","848900cf":"code","a25b0c77":"code","f40af157":"code","2a513f1d":"code","6045fe54":"code","f5a664d6":"code","2ba1d3f7":"code","36bdc211":"code","8b23c5f4":"code","33eedf5b":"code","8681d40b":"code","810d5894":"code","99a09f86":"code","b2e299d3":"code","c78682cf":"code","2f63bef4":"code","e61e5e29":"code","41fd701e":"code","2d633318":"code","255d6297":"code","a25dc837":"code","976f8e29":"code","1939ff7f":"code","83cef6b5":"code","9db315cc":"code","aba112bd":"code","e994ff22":"code","57552f2a":"code","a420874f":"code","49eb6b9c":"code","86322d68":"code","e2b8c6bb":"code","2b6131ac":"code","753ef203":"code","2929266f":"code","1b63007a":"code","7b371ade":"code","da2878d0":"code","9f36e844":"code","1020cda9":"code","4e1d9383":"code","d3dd70a7":"code","6025f243":"code","2da6740c":"code","13444239":"code","ae838a9b":"code","731a3897":"code","304b6a27":"code","2ca014c3":"code","efe18bb5":"code","0042233f":"code","5f2f7c43":"code","fc1c41c8":"code","9a8281a6":"code","9c22f72c":"code","afbac795":"code","9c0d31f0":"code","d8727008":"code","c9995e37":"code","89ac319b":"code","671c2964":"code","063c06c9":"code","dd6ae64e":"code","b9c531ad":"code","3be74fb0":"code","001eed9e":"code","5fbab986":"code","46ef3914":"code","aeb4480d":"code","ef268f77":"code","c5dd971f":"code","e45c9f24":"code","d4eedada":"code","1560d82b":"code","8cee3566":"code","eef0be77":"code","f726c7c4":"code","15a4d033":"code","1d0cd246":"code","d74097f8":"code","693af6f5":"code","106a6e6a":"code","f0f96b93":"code","f84a0562":"code","d8f2753d":"code","73be6a85":"code","d3c470e1":"code","0e9b6739":"code","28f59586":"code","1f3ed3de":"code","b2966206":"code","6c6dc9ee":"code","bf5443fe":"code","bcc51c1c":"code","c6e67278":"code","458fb0d5":"code","2f906e67":"code","7913ada6":"code","01ada66b":"code","1e0d5f79":"code","be04c9be":"code","2ce0e448":"code","e386f94d":"code","f08d6b2b":"code","ec80b09d":"code","8191b88f":"code","5ec84a6a":"code","1906adcc":"code","176e6d47":"code","eae4ecf1":"code","72e7e2f3":"code","b7326ae9":"code","be47df6e":"code","985a65ef":"code","01a45998":"code","f09429b4":"code","2d3485ec":"code","2277700b":"code","01d93157":"code","1bf7197e":"code","cc5d7bb0":"code","050a015d":"code","49a42058":"code","e119ec0f":"markdown","80542dd3":"markdown","44c87dce":"markdown","4d45b8be":"markdown","3cba99b9":"markdown","d53669cb":"markdown","c955e50a":"markdown","bbd59517":"markdown","38613b2b":"markdown","866623ec":"markdown","5b6dbbcd":"markdown","03ab0e08":"markdown","a84c26d8":"markdown","25248edf":"markdown","b710e3b2":"markdown","0e7b2d51":"markdown","4667a76a":"markdown","03581826":"markdown","677e4cd2":"markdown","28e5f267":"markdown","3a3163cb":"markdown","eb463e87":"markdown","2f6dbecb":"markdown","6c8f26e5":"markdown","63ef9cdd":"markdown","1f843eea":"markdown","c39faea7":"markdown","99752342":"markdown","6283ed62":"markdown","04a7a2d0":"markdown","546b093f":"markdown","66cd98f6":"markdown","d4a4456c":"markdown","ff24d627":"markdown","c48ecd68":"markdown","7e632cad":"markdown","80cc8db9":"markdown","7e9dd9a9":"markdown","9fc786c2":"markdown","2859b131":"markdown","83e386cf":"markdown","c0d6f977":"markdown","05a8162a":"markdown","0ae7a1aa":"markdown","a5660d04":"markdown","e2bf1b25":"markdown","1bb8dabe":"markdown","1cfdfe8e":"markdown","dd77b991":"markdown","43ca9696":"markdown","468fc15e":"markdown","70eb89fa":"markdown","6a2a95d2":"markdown","75802dac":"markdown","14d49dd2":"markdown"},"source":{"db8afcd0":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\n# from plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter","848900cf":"# THIS IS FOR KAGGLE\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input\/donorschooseorg-application-screening\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nfor dirname, _, filenames in os.walk('..\/input\/glove-vector'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nproject_data = pd.read_csv(\"..\/input\/donorschooseorg-application-screening\/train.csv\")\nresource_data =  pd.read_csv('..\/input\/donorschooseorg-application-screening\/resources.csv')\n\n# project_data = pd.read_csv(\"..\/input\/donorschooseorg-application-screening\/train.csv\", nrows=5000)\n# resource_data =  pd.read_csv('..\/input\/donorschooseorg-application-screening\/resources.csv', nrows=5000)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a25b0c77":"# project_data = pd.read_csv('..\/LARGE_Datasets\/train.csv')\n# resource_data = pd.read_csv('..\/LARGE_Datasets\/resources.csv')","f40af157":"project_data.columns","2a513f1d":"print(\"Number of data points in train data\", project_data.shape)\nprint('-'*50)\nprint(\"The attributes of data :\", project_data.columns.values)","6045fe54":"print(\"Number of data points in train data\", resource_data.shape)\nprint(resource_data.columns.values)\nresource_data.head(2)","f5a664d6":"# this code is taken from \n# https:\/\/matplotlib.org\/gallery\/pie_and_polar_charts\/pie_and_donut_labels.html#sphx-glr-gallery-pie-and-polar-charts-pie-and-donut-labels-py\n\n\ny_value_counts = project_data['project_is_approved'].value_counts()\nprint(\"Number of projects thar are approved for funding \", y_value_counts[1], \", (\", (y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\nprint(\"Number of projects thar are not approved for funding \", y_value_counts[0], \", (\", (y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect=\"equal\"))\nrecipe = [\"Accepted\", \"Not Accepted\"]\n\ndata = [y_value_counts[1], y_value_counts[0]]\n\nwedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)\n\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1)\/2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                 horizontalalignment=horizontalalignment, **kw)\n\nax.set_title(\"Nmber of projects that are Accepted and not accepted\")\n\nplt.show()","2ba1d3f7":"# Pandas dataframe grouby count, mean: https:\/\/stackoverflow.com\/a\/19385591\/4084039\n\ntemp = pd.DataFrame(project_data.groupby(\"school_state\")[\"project_is_approved\"].apply(np.mean)).reset_index()\n# if you have data which contain only 0 and 1, then the mean = percentage (think about it)\ntemp.columns = ['state_code', 'num_proposals']\n\n# How to plot US state heatmap: https:\/\/datascience.stackexchange.com\/a\/9620\n\nscl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = temp['state_code'],\n        z = temp['num_proposals'].astype(float),\n        locationmode = 'USA-states',\n        text = temp['state_code'],\n        marker = dict(line = dict (color = 'rgb(255,255,255)',width = 2)),\n        colorbar = dict(title = \"% of pro\")\n    ) ]\n\nlayout = dict(\n        title = 'Project Proposals % of Acceptance Rate by US States',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)',\n        ),\n    )\n\nfig = go.Figure(data=data, layout=layout)\noffline.iplot(fig, filename='us-map-heat-map')","36bdc211":"# https:\/\/www.csi.cuny.edu\/sites\/default\/files\/pdf\/administration\/ops\/2letterstabbrev.pdf\ntemp.sort_values(by=['num_proposals'], inplace=True)\nprint(\"States with lowest % approvals\")\nprint(temp.head(5))\nprint('='*50)\nprint(\"States with highest % approvals\")\nprint(temp.tail(5))","8b23c5f4":"#stacked bar plots matplotlib: https:\/\/matplotlib.org\/gallery\/lines_bars_and_markers\/bar_stacked.html\ndef stack_plot(data, xtick, col2='project_is_approved', col3='total'):\n    ind = np.arange(data.shape[0])\n    \n    plt.figure(figsize=(20,5))\n    p1 = plt.bar(ind, data[col3].values)\n    p2 = plt.bar(ind, data[col2].values)\n\n    plt.ylabel('Projects')\n    plt.title('% of projects aproved state wise')\n    plt.xticks(ind, list(data[xtick].values))\n    plt.legend((p1[0], p2[0]), ('total', 'accepted'))\n    plt.show()","33eedf5b":"def univariate_barplots(data, col1, col2='project_is_approved', top=False):\n    # Count number of zeros in dataframe python: https:\/\/stackoverflow.com\/a\/51540521\/4084039\n    temp = pd.DataFrame(project_data.groupby(col1)[col2].agg(lambda x: x.eq(1).sum())).reset_index()\n\n    # Pandas dataframe grouby count: https:\/\/stackoverflow.com\/a\/19385591\/4084039\n    # https:\/\/stackoverflow.com\/questions\/60229375\/solution-for-specificationerror-nested-renamer-is-not-supported-while-agg-alo\n    \n    temp['total'] = pd.DataFrame(project_data.groupby(col1)[col2].agg(total='count')).reset_index()['total']\n    temp['Avg'] = pd.DataFrame(project_data.groupby(col1)[col2].agg(Avg='mean')).reset_index()['Avg']\n    \n    temp.sort_values(by=['total'],inplace=True, ascending=False)\n    \n    if top:\n        temp = temp[0:top]\n    \n    stack_plot(temp, xtick=col1, col2=col2, col3='total')\n    print(temp.head(5))\n    print(\"=\"*50)\n    print(temp.tail(5))","8681d40b":"univariate_barplots(project_data, 'school_state', 'project_is_approved', top=False)","810d5894":"univariate_barplots(project_data, 'teacher_prefix', 'project_is_approved' , top=False)","99a09f86":"univariate_barplots(project_data, 'project_grade_category', 'project_is_approved', top=False)","b2e299d3":"catogories = list(project_data['project_subject_categories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\ncat_list = []\nfor i in catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_') # we are replacing the & value into \n    cat_list.append(temp.strip())","c78682cf":"project_data['clean_categories'] = cat_list\nproject_data.drop(['project_subject_categories'], axis=1, inplace=True)\nproject_data.head(2)","2f63bef4":"univariate_barplots(project_data, 'clean_categories', 'project_is_approved', top=20)","e61e5e29":"# count of all the words in corpus python: https:\/\/stackoverflow.com\/a\/22898595\/4084039\nfrom collections import Counter\nmy_counter = Counter()\nfor word in project_data['clean_categories'].values:\n    my_counter.update(word.split())","41fd701e":"# dict sort by value python: https:\/\/stackoverflow.com\/a\/613218\/4084039\ncat_dict = dict(my_counter)\nsorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(sorted_cat_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(sorted_cat_dict.values()))\n\nplt.ylabel('Projects')\nplt.title('% of projects aproved state wise')\nplt.xticks(ind, list(sorted_cat_dict.keys()))\nplt.show()","2d633318":"for i, j in sorted_cat_dict.items():\n    print(\"{:20} :{:10}\".format(i,j))","255d6297":"sub_catogories = list(project_data['project_subject_subcategories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\n\nsub_cat_list = []\nfor i in sub_catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_')\n    sub_cat_list.append(temp.strip())","a25dc837":"project_data['clean_subcategories'] = sub_cat_list\nproject_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\nproject_data.head(2)","976f8e29":"univariate_barplots(project_data, 'clean_subcategories', 'project_is_approved', top=50)","1939ff7f":"# count of all the words in corpus python: https:\/\/stackoverflow.com\/a\/22898595\/4084039\nfrom collections import Counter\nmy_counter = Counter()\nfor word in project_data['clean_subcategories'].values:\n    my_counter.update(word.split())","83cef6b5":"# dict sort by value python: https:\/\/stackoverflow.com\/a\/613218\/4084039\nsub_cat_dict = dict(my_counter)\nsorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(sorted_sub_cat_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(sorted_sub_cat_dict.values()))\n\nplt.ylabel('Projects')\nplt.title('% of projects aproved state wise')\nplt.xticks(ind, list(sorted_sub_cat_dict.keys()))\nplt.show()","9db315cc":"for i, j in sorted_sub_cat_dict.items():\n    print(\"{:20} :{:10}\".format(i,j))","aba112bd":"#How to calculate number of words in a string in DataFrame: https:\/\/stackoverflow.com\/a\/37483537\/4084039\nword_count = project_data['project_title'].str.split().apply(len).value_counts()\nword_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(word_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(word_dict.values()))\n\nplt.ylabel('Numeber of projects')\nplt.title('Words for each title of the project')\nplt.xticks(ind, list(word_dict.keys()))\nplt.show()","e994ff22":"approved_word_count = project_data[project_data['project_is_approved']==1]['project_title'].str.split().apply(len)\napproved_word_count = approved_word_count.values\n\nrejected_word_count = project_data[project_data['project_is_approved']==0]['project_title'].str.split().apply(len)\nrejected_word_count = rejected_word_count.values","57552f2a":"# https:\/\/glowingpython.blogspot.com\/2012\/09\/boxplot-with-matplotlib.html\nplt.boxplot([approved_word_count, rejected_word_count])\nplt.xticks([1,2],('Approved Projects','Rejected Projects'))\nplt.ylabel('Words in project title')\nplt.grid()\nplt.show()","a420874f":"plt.figure(figsize=(10,3))\nsns.distplot(approved_word_count, hist=False, label=\"Approved Projects\")\nsns.distplot(rejected_word_count, hist=False, label=\"Not Approved Projects\")\nplt.legend()\nplt.show()","49eb6b9c":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","86322d68":"#How to calculate number of words in a string in DataFrame: https:\/\/stackoverflow.com\/a\/37483537\/4084039\nword_count = project_data['essay'].str.split().apply(len).value_counts()\nword_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(word_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(word_dict.values()))\n\nplt.ylabel('Number of projects')\nplt.xlabel('Number of words in each eassay')\nplt.title('Words for each essay of the project')\nplt.xticks(ind, list(word_dict.keys()))\nplt.show()","e2b8c6bb":"sns.distplot(word_count.values)\nplt.title('Words for each essay of the project')\nplt.xlabel('Number of words in each eassay')\nplt.show()\n","2b6131ac":"approved_word_count = project_data[project_data['project_is_approved']==1]['essay'].str.split().apply(len)\napproved_word_count = approved_word_count.values\n\nrejected_word_count = project_data[project_data['project_is_approved']==0]['essay'].str.split().apply(len)\nrejected_word_count = rejected_word_count.values","753ef203":"# https:\/\/glowingpython.blogspot.com\/2012\/09\/boxplot-with-matplotlib.html\nplt.boxplot([approved_word_count, rejected_word_count])\nplt.title('Words for each essay of the project')\nplt.xticks([1,2],('Approved Projects','Rejected Projects'))\nplt.ylabel('Words in project title')\nplt.grid()\nplt.show()","2929266f":"plt.figure(figsize=(10,3))\nsns.distplot(approved_word_count, hist=False, label=\"Approved Projects\")\nsns.distplot(rejected_word_count, hist=False, label=\"Not Approved Projects\")\nplt.title('Words for each essay of the project')\nplt.xlabel('Number of words in each eassay')\nplt.legend()\nplt.show()","1b63007a":"# we get the cost of the project using resource.csv file\nresource_data.head(2)","7b371ade":"# https:\/\/stackoverflow.com\/questions\/22407798\/how-to-reset-a-dataframes-indexes-for-all-groups-in-one-step\nprice_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)","da2878d0":"# join two dataframes in python: \nproject_data = pd.merge(project_data, price_data, on='id', how='left')","9f36e844":"approved_price = project_data[project_data['project_is_approved']==1]['price'].values\n\nrejected_price = project_data[project_data['project_is_approved']==0]['price'].values","1020cda9":"# https:\/\/glowingpython.blogspot.com\/2012\/09\/boxplot-with-matplotlib.html\nplt.boxplot([approved_price, rejected_price])\nplt.title('Box Plots of Cost per approved and not approved Projects')\nplt.xticks([1,2],('Approved Projects','Rejected Projects'))\nplt.ylabel('Words in project title')\nplt.grid()\nplt.show()","4e1d9383":"plt.figure(figsize=(10,3))\nsns.distplot(approved_price, hist=False, label=\"Approved Projects\")\nsns.distplot(rejected_price, hist=False, label=\"Not Approved Projects\")\nplt.title('Cost per approved and not approved Projects')\nplt.xlabel('Cost of a project')\nplt.legend()\nplt.show()","d3dd70a7":"# http:\/\/zetcode.com\/python\/prettytable\/\nfrom prettytable import PrettyTable\n\nx = PrettyTable()\nx.field_names = [\"Percentile\", \"Approved Projects\", \"Not Approved Projects\"]\n\nfor i in range(0,101,5):\n    x.add_row([i,np.round(np.percentile(approved_price,i), 3), np.round(np.percentile(rejected_price,i), 3)])\nprint(x)","6025f243":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\n\nfrom nltk.corpus import stopwords\nimport pickle\n\nfrom tqdm import tqdm\nimport os","2da6740c":"# project_data = pd.read_csv('..\/LARGE_Datasets\/train.csv', nrows=5000)\n# resource_data = pd.read_csv('..\/LARGE_Datasets\/resources.csv')\n\nproject_data = pd.read_csv(\"..\/input\/donorschooseorg-application-screening\/train.csv\")\nresource_data =  pd.read_csv('..\/input\/donorschooseorg-application-screening\/resources.csv')","13444239":"print(\"Number of data points in train data\", project_data.shape)\nprint('-'*50)\nprint(\"The attributes of data :\", project_data.columns.values)","ae838a9b":"print(\"Number of data points in train data\", resource_data.shape)\nprint(resource_data.columns.values)\nresource_data.head(2)","731a3897":"project_data['project_grade_category'].value_counts()","304b6a27":"# https:\/\/stackoverflow.com\/questions\/36383821\/pandas-dataframe-apply-function-to-column-strings-based-on-other-column-value\nproject_data['project_grade_category'] = project_data['project_grade_category'].str.replace(' ','_')\nproject_data['project_grade_category'] = project_data['project_grade_category'].str.replace('-','_')\nproject_data['project_grade_category'] = project_data['project_grade_category'].str.lower()\nproject_data['project_grade_category'].value_counts()","2ca014c3":"project_data['project_subject_categories'].value_counts()","efe18bb5":"project_data['project_subject_categories'] = project_data['project_subject_categories'].str.replace(' The ','')\nproject_data['project_subject_categories'] = project_data['project_subject_categories'].str.replace(' ','')\nproject_data['project_subject_categories'] = project_data['project_subject_categories'].str.replace('&','_')\nproject_data['project_subject_categories'] = project_data['project_subject_categories'].str.replace(',','_')\nproject_data['project_subject_categories'] = project_data['project_subject_categories'].str.lower()\nproject_data['project_subject_categories'].value_counts()","0042233f":"project_data['teacher_prefix'].value_counts()","5f2f7c43":"# check if we have any nan values are there\nprint(project_data['teacher_prefix'].isnull().values.any())\nprint(\"number of nan values\",project_data['teacher_prefix'].isnull().values.sum())","fc1c41c8":"project_data['teacher_prefix']=project_data['teacher_prefix'].fillna('Mrs.')","9a8281a6":"project_data['teacher_prefix'].value_counts()","9c22f72c":"project_data['teacher_prefix'] = project_data['teacher_prefix'].str.replace('.','')\nproject_data['teacher_prefix'] = project_data['teacher_prefix'].str.lower()\nproject_data['teacher_prefix'].value_counts()","afbac795":"project_data['project_subject_subcategories'].value_counts()","9c0d31f0":"project_data['project_subject_subcategories'] = project_data['project_subject_subcategories'].str.replace(' The ','')\nproject_data['project_subject_subcategories'] = project_data['project_subject_subcategories'].str.replace(' ','')\nproject_data['project_subject_subcategories'] = project_data['project_subject_subcategories'].str.replace('&','_')\nproject_data['project_subject_subcategories'] = project_data['project_subject_subcategories'].str.replace(',','_')\nproject_data['project_subject_subcategories'] = project_data['project_subject_subcategories'].str.lower()\nproject_data['project_subject_subcategories'].value_counts()","d8727008":"project_data['school_state'].value_counts()","c9995e37":"project_data['school_state'] = project_data['school_state'].str.lower()\nproject_data['school_state'].value_counts()","89ac319b":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","671c2964":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","063c06c9":"project_data['project_title'].head(5)","dd6ae64e":"print(\"printing some random reviews\")\nprint(9, project_data['project_title'].values[9])\nprint(34, project_data['project_title'].values[34])\nprint(147, project_data['project_title'].values[147])","b9c531ad":"# Combining all the above stundents \nfrom tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https:\/\/gist.github.com\/sebleier\/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text","3be74fb0":"preprocessed_titles = preprocess_text(project_data['project_title'].values)","001eed9e":"print(\"printing some random reviews\")\nprint(9, preprocessed_titles[9])\nprint(34, preprocessed_titles[34])\nprint(147, preprocessed_titles[147])","5fbab986":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","46ef3914":"print(\"printing some random essay\")\nprint(9, project_data['essay'].values[9])\nprint('-'*50)\nprint(34, project_data['essay'].values[34])\nprint('-'*50)\nprint(147, project_data['essay'].values[147])","aeb4480d":"preprocessed_essays = preprocess_text(project_data['essay'].values)","ef268f77":"print(\"printing some random essay\")\nprint(9, preprocessed_essays[9])\nprint('-'*50)\nprint(34, preprocessed_essays[34])\nprint('-'*50)\nprint(147, preprocessed_essays[147])","c5dd971f":"# https:\/\/stackoverflow.com\/questions\/22407798\/how-to-reset-a-dataframes-indexes-for-all-groups-in-one-step\nprice_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)","e45c9f24":"# join two dataframes in python: \nproject_data = pd.merge(project_data, price_data, on='id', how='left')","d4eedada":"project_data['price'].head()","1560d82b":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(project_data['price'].values.reshape(-1, 1))\nproject_data['std_price']=scaler.transform(project_data['price'].values.reshape(-1, 1) )","8cee3566":"project_data['std_price'].head()","eef0be77":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(project_data['price'].values.reshape(-1, 1))\nproject_data['nrm_price']=scaler.transform(project_data['price'].values.reshape(-1, 1))","f726c7c4":"project_data['nrm_price'].head()","15a4d033":"project_data.head()","1d0cd246":"import nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid    = SentimentIntensityAnalyzer()\n\nnegative_sentiments = []\npositive_sentiments = []\nneutral_sentiments = []\ncompound_sentiments = []\n\nfor i in tqdm(project_data['essay']):\n  sid_sentiments = sid.polarity_scores(i)\n  negative_sentiments.append(sid_sentiments['neg'])\n  positive_sentiments.append(sid_sentiments['pos'])\n  neutral_sentiments.append(sid_sentiments['neu'])\n  compound_sentiments.append(sid_sentiments['compound'])\n  \n# Now append these sentiments columns\/freatures to original preprocessed dataframe\nproject_data['negative_sent'] = negative_sentiments\nproject_data['positive_sent'] = positive_sentiments\nproject_data['neutral_sent'] = neutral_sentiments\nproject_data['compound_sent'] = compound_sentiments\n\nproject_data.head(1)","d74097f8":"project_data.columns","693af6f5":"# project_data.to_csv(\".\/preprocessed.csv\")","106a6e6a":"# !pip install wordcloud\n# from google.colab import drive\n# drive.mount('\/content\/gdrive')\n# root_path = '..\/LARGE_Datasets\/preprocessed_data.csv'\n# glove_vector_path = '..\/LARGE_Datasets\/glove_vectors'\nglove_vector_path = '..\/input\/glove-vector\/glove_vectors'\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.model_selection import GridSearchCV\nfrom prettytable import PrettyTable\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom scipy.sparse import hstack\nfrom sklearn.tree import DecisionTreeClassifier\nimport seaborn as sns\nsns.set()\n\nimport pickle\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon\n\norg_preprocessed = project_data\n\n\norg_preprocessed.head()\n","f0f96b93":"org_preprocessed.columns","f84a0562":"# please write all the code with proper documentation, and proper titles for each subsection\n# go through documentations and blogs before you start coding\n# first figure out what to do, and then think about how to do.\n# reading and understanding error messages will be very much helpfull in debugging your code\n# when you plot any graph make sure you use \n    # a. Title, that describes your plot, this will be very helpful to the reader\n    # b. Legends if needed\n    # c. X-axis label\n    # d. Y-axis label","d8f2753d":"Y = org_preprocessed['project_is_approved'].values\nX = org_preprocessed.drop(['project_is_approved'], axis=1)\nprint(Y.shape)\nprint(X.shape)","73be6a85":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","d3c470e1":"# As required for Task-1, applying TFIDF on the Essay column\nvectorizer_essay_tfidf = TfidfVectorizer(min_df=10)\n\n# Apply .fit() on this vectorizer on Train data\n# Note .fit() is applied only on the train data, as test and cv should not be fitted\nvectorizer_essay_tfidf.fit(X_train['essay'].values)\n\n# Now use the fitted TfidfVectorizer for converting 'essay' text to Vector form\nX_train_vectorized_tfidf_essay = vectorizer_essay_tfidf.transform(X_train['essay'].values)\nX_test_vectorized_tfidf_essay = vectorizer_essay_tfidf.transform(X_test['essay'].values)\n\nprint('After TFIDF on Essay column checking the shapes ')\nprint(X_train_vectorized_tfidf_essay.shape, y_train.shape)\nprint(X_test_vectorized_tfidf_essay.shape, y_test.shape)","0e9b6739":"count_vectorizer_school_state = CountVectorizer()\ncount_vectorizer_school_state.fit(X_train['school_state'].values)\n\n# Now use the fitted CountVectorizer for converting 'school_state' text to Vector form\nX_train_vectorized_ohe_school_state = count_vectorizer_school_state.transform(X_train['school_state'].values)\nX_test_vectorized_ohe_school_state = count_vectorizer_school_state.transform(X_test['school_state'].values)\n\nschool_state_features = count_vectorizer_school_state.get_feature_names()\nprint('school_state_features ', school_state_features)\n\n\nprint('After CountVectorizing on school_state column checking the shapes ')\nprint(X_train_vectorized_ohe_school_state.shape, y_train.shape)\nprint(X_test_vectorized_ohe_school_state.shape, y_test.shape)\n","28f59586":"count_vectorizer_teacher_prefix = CountVectorizer()\ncount_vectorizer_teacher_prefix.fit(X_train['teacher_prefix'].values)\n\n# Now use the fitted CountVectorizer for converting 'teacher_prefix' text to Vector form\nX_train_vectorized_ohe_teacher_prefix = count_vectorizer_teacher_prefix.transform(X_train['teacher_prefix'].values)\nX_test_vectorized_ohe_teacher_prefix = count_vectorizer_teacher_prefix.transform(X_test['teacher_prefix'].values)\n\nteacher_prefix_features = count_vectorizer_teacher_prefix.get_feature_names()\nprint('teacher_prefix_features ', teacher_prefix_features)\n\n\nprint('After CountVectorizing on teacher_prefix column checking the shapes ')\nprint(X_train_vectorized_ohe_teacher_prefix.shape, y_train.shape)\nprint(X_test_vectorized_ohe_teacher_prefix.shape, y_test.shape)","1f3ed3de":"count_vectorizer_project_grade_category = CountVectorizer()\ncount_vectorizer_project_grade_category.fit(X_train['project_grade_category'].values)\n\n# Now use the fitted CountVectorizer for converting 'project_grade_category' text to Vector form\nX_train_vectorized_ohe_project_grade_category = count_vectorizer_project_grade_category.transform(X_train['project_grade_category'].values)\nX_test_vectorized_ohe_project_grade_category = count_vectorizer_project_grade_category.transform(X_test['project_grade_category'].values)\n\nproject_grade_category_features = count_vectorizer_project_grade_category.get_feature_names()\nprint('project_grade_category_features ', project_grade_category_features)\n\n\nprint('After CountVectorizing on project_grade_category column checking the shapes ')\nprint(X_train_vectorized_ohe_project_grade_category.shape, y_train.shape)\nprint(X_test_vectorized_ohe_project_grade_category.shape, y_test.shape)","b2966206":"count_vectorizer_project_subject_categories = CountVectorizer()\ncount_vectorizer_project_subject_categories.fit(X_train['project_subject_categories'].values)\n\n# Now use the fitted CountVectorizer for converting 'project_subject_categories' text to Vector form\nX_train_vectorized_ohe_project_subject_categories = count_vectorizer_project_subject_categories.transform(X_train['project_subject_categories'].values)\nX_test_vectorized_ohe_project_subject_categories = count_vectorizer_project_subject_categories.transform(X_test['project_subject_categories'].values)\n\nproject_subject_categories_features = count_vectorizer_project_subject_categories.get_feature_names()\nprint('project_subject_categories_features ', project_subject_categories_features)\n\n\nprint('After CountVectorizing on project_subject_categories column checking the shapes ')\nprint(X_train_vectorized_ohe_project_subject_categories.shape, y_train.shape)\nprint(X_test_vectorized_ohe_project_subject_categories.shape, y_test.shape)","6c6dc9ee":"X_train.columns","bf5443fe":"count_vectorizer_project_subject_subcategories = CountVectorizer()\ncount_vectorizer_project_subject_subcategories.fit(X_train['project_subject_subcategories'].values)\n\n# Now use the fitted CountVectorizer for converting 'project_subject_subcategories' text to Vector form\nX_train_vectorized_ohe_project_subject_subcategories = count_vectorizer_project_subject_subcategories.transform(X_train['project_subject_subcategories'].values)\nX_test_vectorized_ohe_project_subject_subcategories = count_vectorizer_project_subject_subcategories.transform(X_test['project_subject_subcategories'].values)\n\nproject_subject_subcategories_features = count_vectorizer_project_subject_subcategories.get_feature_names()\n# print('project_subject_subcategories_features ', project_subject_subcategories_features)\n\n\nprint('After CountVectorizing on project_subject_subcategories column checking the shapes ')\nprint(X_train_vectorized_ohe_project_subject_subcategories.shape, y_train.shape)\nprint(X_test_vectorized_ohe_project_subject_subcategories.shape, y_test.shape)","bcc51c1c":"normalizer = Normalizer()\n\nnormalizer.fit(X_train['price'].values.reshape(-1, 1))\n\nX_train_normalized_price = normalizer.transform(X_train['price'].values.reshape(-1,1))\nX_test_normalized_price = normalizer.transform(X_test['price'].values.reshape(-1,1))\n\nprint('After Normalizing on price column checking the shapes ')\nprint(X_train_normalized_price.shape, y_train.shape)\nprint(X_test_normalized_price.shape, y_test.shape)\n","c6e67278":"sentiments_standardizer = StandardScaler()\n\n# First applying the .fit() on the train data to find Mean and SD\nsentiments_standardizer.fit(X_train['negative_sent'].values.reshape(-1,1))\n\n# Now applying .transform() to train, test and cv data\nX_train_negative_sent_standardized = sentiments_standardizer.transform(X_train['negative_sent'].values.reshape(-1,1))\nX_test_negative_sent_standardized = sentiments_standardizer.transform(X_test['negative_sent'].values.reshape(-1,1))\n\nprint('After Standardizing on negative_sent column checking the shapes ')\nprint(X_train_negative_sent_standardized.shape, y_train.shape)\nprint(X_test_negative_sent_standardized.shape, y_test.shape)","458fb0d5":"# First applying the .fit() on the train data to find Mean and SD\nsentiments_standardizer.fit(X_train['positive_sent'].values.reshape(-1,1))\n\n# Now applying .transform() to train, test and cv data\nX_train_positive_sent_standardized = sentiments_standardizer.transform(X_train['positive_sent'].values.reshape(-1,1))\nX_test_positive_sent_standardized = sentiments_standardizer.transform(X_test['positive_sent'].values.reshape(-1,1))\n\nprint('After Standardizing on positive_sent column checking the shapes ')\nprint(X_train_positive_sent_standardized.shape, y_train.shape)\nprint(X_test_positive_sent_standardized.shape, y_test.shape)","2f906e67":"# First applying the .fit() on the train data to find Mean and SD\nsentiments_standardizer.fit(X_train['neutral_sent'].values.reshape(-1,1))\n\n# Now applying .transform() to train, test and cv data\nX_train_neutral_sent_standardized = sentiments_standardizer.transform(X_train['neutral_sent'].values.reshape(-1,1))\nX_test_neutral_sent_standardized = sentiments_standardizer.transform(X_test['neutral_sent'].values.reshape(-1,1))\n\nprint('After Standardizing on neutral_sent column checking the shapes ')\nprint(X_train_neutral_sent_standardized.shape, y_train.shape)\nprint(X_test_neutral_sent_standardized.shape, y_test.shape)","7913ada6":"# First applying the .fit() on the train data to find Mean and SD\nsentiments_standardizer.fit(X_train['compound_sent'].values.reshape(-1,1))\n\n# Now applying .transform() to train, test and cv data\nX_train_compound_sent_standardized = sentiments_standardizer.transform(X_train['compound_sent'].values.reshape(-1,1))\nX_test_compound_sent_standardized = sentiments_standardizer.transform(X_test['compound_sent'].values.reshape(-1,1))\n\nprint('After Standardizing on compound_sent column checking the shapes ')\nprint(X_train_compound_sent_standardized.shape, y_train.shape)\nprint(X_test_compound_sent_standardized.shape, y_test.shape)","01ada66b":"X_train_s1_merged = hstack((X_train_vectorized_tfidf_essay, X_train_vectorized_ohe_school_state, X_train_vectorized_ohe_teacher_prefix, X_train_vectorized_ohe_project_grade_category, X_train_vectorized_ohe_project_subject_categories, X_train_vectorized_ohe_project_subject_subcategories, X_train_normalized_price, X_train_negative_sent_standardized, X_train_positive_sent_standardized, X_train_neutral_sent_standardized, X_train_compound_sent_standardized ))\n\nX_test_s1_merged = hstack((X_test_vectorized_tfidf_essay, X_test_vectorized_ohe_school_state, X_test_vectorized_ohe_teacher_prefix, X_test_vectorized_ohe_project_grade_category, X_test_vectorized_ohe_project_subject_categories, X_test_vectorized_ohe_project_subject_subcategories , X_test_normalized_price, X_test_negative_sent_standardized, X_test_positive_sent_standardized, X_test_neutral_sent_standardized, X_test_compound_sent_standardized ))","1e0d5f79":"# Shape of the data-matrix after mergeing as above\nprint('Shape of X_train_s1_merged ', X_train_s1_merged.shape, 'Shpae of y_train ', y_train.shape)\nprint('Shape of X_test_s1_merged ', X_test_s1_merged.shape, 'Shpae of y_test ', y_test.shape)","be04c9be":"depth_dt_gridsearch = [1, 5, 10, 50]\nsplit_dt_gridsearch = [5, 10, 100, 500]\n\nparameters_dict = { 'max_depth': depth_dt_gridsearch, 'min_samples_split': split_dt_gridsearch }\n\ndecision_tree = DecisionTreeClassifier(class_weight='balanced')\n\ngrid_search = GridSearchCV(decision_tree, parameters_dict, cv=3, scoring='roc_auc', return_train_score=True)\n\ngrid_search.fit(X_train_s1_merged, y_train)\n\nprint('Best Params from GridSearchCV ', grid_search.best_params_)","2ce0e448":"results_from_gridsearchcv = pd.DataFrame(grid_search.cv_results_)\n\nmax_auc_scores = results_from_gridsearchcv.groupby(['param_min_samples_split', 'param_max_depth']).max()\n\nmax_auc_scores = max_auc_scores.unstack()[['mean_test_score', 'mean_train_score']]\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\n\nsns.heatmap(max_auc_scores.mean_train_score, annot = True, fmt='.4g', ax=ax[0])\nsns.heatmap(max_auc_scores.mean_test_score, annot = True, fmt='.4g', ax=ax[1])\n\nax[0].set_title('Train Data')\nax[1].set_title('CV Data')\nplt.show()\n","e386f94d":"dt_clf = DecisionTreeClassifier(max_depth=10, min_samples_split=500, class_weight='balanced')\ndt_clf.fit(X_train_s1_merged, y_train )\n\ny_train_predicted = dt_clf.predict(X_train_s1_merged)\ny_test_predicted = dt_clf.predict(X_test_s1_merged)\n\ns1_train_fpr, s1_train_tpr, s1_train_threshold = roc_curve(y_train, y_train_predicted)\ns1_test_fpr, s1_test_tpr, s1_test_threshold = roc_curve(y_test, y_test_predicted)\n\nplt.plot(s1_train_fpr, s1_train_tpr, label=\"Train AUC = \"+str(auc(s1_train_fpr, s1_train_tpr)))\nplt.plot(s1_test_fpr, s1_test_tpr, label=\"Test AUC = \"+str(auc(s1_test_fpr, s1_test_tpr)))\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.grid()\nplt.title('ROC-AUC Curve after implementing Decition Tree')\nplt.show()","f08d6b2b":"# First the Function to get the Predicted Y_vector based on a given threshold and fpr & tpr\n# This will be needed for Calculating Confusion Matrix\n\ndef calculate_y_vector_from_threshold(proba, threshold, fpr, tpr):\n  \n    # Using argmax to return the position of the largest value.\n    # based on the calculated value of tpr*(1-fpr)\n    # tpr * (1-fpr) i.e. optimal_threshold is maximum when fpr is very low and tpr is very high\n    optimal_threshold = threshold[np.argmax(tpr * (1-fpr))]  \n\n  \n    predicted_y_vector = []\n    for i in proba:\n        if i >= optimal_threshold:\n            predicted_y_vector.append(1)\n        else:\n            predicted_y_vector.append(0)\n\n    return predicted_y_vector\n\nconfusion_matrix_s1_train = confusion_matrix(y_train, calculate_y_vector_from_threshold(y_train_predicted, s1_train_threshold, s1_train_fpr, s1_train_tpr ) )\n\nconfusion_matrix_s1_test = confusion_matrix(y_test, calculate_y_vector_from_threshold(y_test_predicted, s1_test_threshold, s1_test_fpr, s1_test_tpr ) )\n\nprint('confusion_matrix_s1_train ', confusion_matrix_s1_train)\n# Heatmap for Confusion Matrix: Train and SET 1\nheatmap_confusion_matrix_train_s1 = sns.heatmap(confusion_matrix_s1_train, annot=True, fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 24})\n\nplt.title('S1 Train Set: Confusion Matrix')\nplt.xlabel('Actual X values')\nplt.ylabel('Predicted Y Values')\nplt.show()\n\nheatmap_confusion_matrix_test_s1 = sns.heatmap(confusion_matrix_s1_test, annot=True, fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 24})\n\nplt.title('S1 Test Set: Confusion Matrix')\nplt.xlabel('Actual X values')\nplt.ylabel('Predicted Y Values')\nplt.show()","ec80b09d":"# Function to get the indices of the false-positive predictions\n# This indices array will later be used to extract the text and price\n# of only those rows whose predictions are found to be false-positive\ndef get_false_positive_indices_from_arr(y_actual, y_predicted):  \n  if_false_positives_list =  (y_actual == 0 ) & (y_predicted == 1 )\n  # \"if_false_positives_list\" is a list-of-booleans like so \n  # [True, False, True, False, ...., True]    \n  # Below will return me the y_actual values satisfying the above condtion \n  # i.e. ONLY those elements where if_false_positives_list has a True value\n  y_values_of_false_positives = y_actual[if_false_positives_list]\n  indices_of_false_positives = np.in1d(y_actual, y_values_of_false_positives).nonzero()[0]\n  return indices_of_false_positives\n\n# In above I am filtering-a-list-based-on-a-list-of-booleans\n# https:\/\/stackoverflow.com\/a\/18666622\/1902852\n# list_a = np.array([1, 2, 4, 6])\n# filter = np.array([True, False, True, False])\n# list_a[filter]\n# OUTPUT  =>  array([1, 4])\n\n\n# Function to create the whole text to be fed to wordcloud\ndef generate_word_cloud_from_text(text):\n  cloud_text = ''\n  for t in text:\n    cloud_text += ' ' + t\n    \n  return cloud_text\n\n# Function to create the plot of the wordcloud\ndef wordcloud_plotter(text):\n  # wordcloud = WordCloud(background_color = 'white', min_word_length=5)\n  wordcloud = WordCloud(width = 2000, height = 800, background_color =\"black\", min_font_size = 8)\n  image_of_word = wordcloud.generate(text)\n  plt.figure(figsize=(20, 12))\n  plt.imshow(wordcloud, interpolation='bilinear')\n  plt.axis('off')\n  plt.show()\n\n# X_train_vectorized_tfidf_essay","8191b88f":"row_indices_of_false_positives_test_data = get_false_positive_indices_from_arr(y_test, y_test_predicted)\n# print('row_indices_of_false_positives_test_data ', row_indices_of_false_positives_test_data.shape)\n\nX_test_essays_original_data = X_test['essay'].values\n# print('X_test_essays_original_data ', X_test_essays_original_data.shape)\n\nessays_text_of_false_positives_test_data = X_test_essays_original_data[row_indices_of_false_positives_test_data]\n# print('essays_text_of_false_positives_test_data ', essays_text_of_false_positives_test_data)\n\ntext_generated = generate_word_cloud_from_text(essays_text_of_false_positives_test_data)\n\nwordcloud_plotter(text_generated)","5ec84a6a":"X_test_price_original_data = X_test['price'].values\n# print('row_indices_of_false_positves_test_data ', row_indices_of_false_positves_test_data)\n\nprice_row_values_of_false_positive_test_data = X_test_price_original_data[row_indices_of_false_positives_test_data]\n# print('price_row_values_of_false_positive_test_data ', price_row_values_of_false_positive_test_data)\n\nsns.boxplot(price_row_values_of_false_positive_test_data)\n\nplt.title('Box plot with the `price` of `false positive data points`')\n\nplt.show()","1906adcc":"X_test_teacher_number_original_data = X_test['teacher_number_of_previously_posted_projects'].values\n\nteacher_number_values_of_false_positive_test_data = X_test_teacher_number_original_data[row_indices_of_false_positives_test_data]\n\nsns.distplot(teacher_number_values_of_false_positive_test_data)\n\nplt.title('PDF with `teacher_number` of `false positive data points` for X_test')\n\nplt.show()","176e6d47":"with open(glove_vector_path, 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())\n    \n# In the TF-IDF Word2Vec vectorization, we have to fit the TfidfVectorizer only on X_train['essay'] and \n# extract 'dictionary' (dictionary with features as the keys and IDF scores as the values) and \n# 'tfidf_words' (a set of all the features extracted from the vectorizer). \n# We have to use the same 'dictionary' and 'tfidf_words' in vectorizing both X_train['essay'] and X_test['essay'].\n\n# Now, at the very top section of this Notebook, we alrady have this code of Vectorizer on X_train data\n# vectorizer_essay_tfidf = TfidfVectorizer(min_df=10)\n# vectorizer_essay_tfidf.fit(X_train['essay'].values)\n\n# Hence we are now converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(vectorizer_essay_tfidf.get_feature_names(), list(vectorizer_essay_tfidf.idf_)))\ntfidf_words = set(vectorizer_essay_tfidf.get_feature_names())     \n\n    \n# Function to generate Word2Vec referencing \"4_Reference_Vectorization.ipynb\" given in the instruction\ndef generate_w2v_from_text(essays_text_arr):\n  # compute average word2vec for each review.\n    tfidf_w2v_vectors = []\n    # the avg-w2v for each sentence\/review is stored in this list\n\n    for sentence in tqdm(essays_text_arr):  # for each sentence\n        vector = np.zeros(300)  # as word vectors are of zero length\n        tf_idf_weight = 0\n        # num of words with a valid vector in the sentence\n        for word in sentence.split():  # for each word in a sentence\n            if (word in glove_words) and (word in tfidf_words):\n                vec = model[word]  # getting the vector for each word\n                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n                tf_idf = dictionary[word] * (\n                    sentence.count(word) \/ len(sentence.split())\n                )  # getting the tfidf value for each word\n                vector += vec * tf_idf  # calculating tfidf weighted w2v\n                tf_idf_weight += tf_idf\n        if tf_idf_weight != 0:\n            vector \/= tf_idf_weight\n        tfidf_w2v_vectors.append(vector)\n    return tfidf_w2v_vectors\n  \nX_train_vectorized_tfidf_w2v_essay = generate_w2v_from_text(X_train['essay'].values)\nX_test_vectorized_tfidf_w2v_essay = generate_w2v_from_text(X_test['essay'].values)\n","eae4ecf1":"X_train_s2_merged = hstack((X_train_vectorized_tfidf_w2v_essay, X_train_vectorized_ohe_school_state, X_train_vectorized_ohe_teacher_prefix, X_train_vectorized_ohe_project_grade_category, X_train_vectorized_ohe_project_subject_categories, X_train_vectorized_ohe_project_subject_subcategories, X_train_normalized_price, X_train_negative_sent_standardized, X_train_positive_sent_standardized, X_train_neutral_sent_standardized, X_train_compound_sent_standardized ))\n\n\nX_test_s2_merged = hstack((X_test_vectorized_tfidf_w2v_essay, X_test_vectorized_ohe_school_state, X_test_vectorized_ohe_teacher_prefix, X_test_vectorized_ohe_project_grade_category, X_test_vectorized_ohe_project_subject_categories, X_test_vectorized_ohe_project_subject_subcategories , X_test_normalized_price, X_test_negative_sent_standardized, X_test_positive_sent_standardized, X_test_neutral_sent_standardized, X_test_compound_sent_standardized ))\n\n\nprint('X_train_s2_merged.shape ', X_train_s2_merged.shape)\nprint('X_test_s2_merged.shape ', X_test_s2_merged.shape)","72e7e2f3":"depth_dt_gridsearch = [1, 5, 10, 50]\nsplit_dt_gridsearch = [5, 10, 100, 500]\n\nparameters_dict = { 'max_depth': depth_dt_gridsearch, 'min_samples_split': split_dt_gridsearch }\n\ndecision_tree = DecisionTreeClassifier(class_weight='balanced')\n\nclf_s2 = GridSearchCV(decision_tree, parameters_dict, cv=3, scoring='roc_auc', return_train_score=True)\n\nclf_s2.fit(X_train_s2_merged, y_train)\n\nprint('Best Params from GridSearchCV for Set S2 ', clf_s2.best_params_)\n# Best Params from GridSearchCV for Set S2  {'max_depth': 5, 'min_samples_split': 500}","b7326ae9":"results_from_gridsearchcv = pd.DataFrame(clf_s2.cv_results_)\n\nmax_auc_scores = results_from_gridsearchcv.groupby(['param_min_samples_split', 'param_max_depth']).max()\n\nmax_auc_scores = max_auc_scores.unstack()[['mean_test_score', 'mean_train_score']]\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\n\nsns.heatmap(max_auc_scores.mean_train_score, annot = True, fmt='.4g', ax=ax[0])\nsns.heatmap(max_auc_scores.mean_test_score, annot = True, fmt='.4g', ax=ax[1])\n\nax[0].set_title('Train Data')\nax[1].set_title('CV Data')\nplt.show()\n","be47df6e":"dt_clf_s2 = DecisionTreeClassifier(max_depth=5, min_samples_split=500, class_weight='balanced')\ndt_clf_s2.fit(X_train_s1_merged, y_train )\n\ny_train_predicted = dt_clf_s2.predict(X_train_s1_merged)\ny_test_predicted_s2 = dt_clf_s2.predict(X_test_s1_merged)\n\ns2_train_fpr, s2_train_tpr, s2_train_threshold = roc_curve(y_train, y_train_predicted)\ns2_test_fpr, s2_test_tpr, s2_test_threshold = roc_curve(y_test, y_test_predicted_s2)\n\nplt.plot(s2_train_fpr, s2_train_tpr, label=\"Train AUC TfIdf-W2V = \"+str(auc(s2_train_fpr, s2_train_tpr)))\nplt.plot(s2_test_fpr, s2_test_tpr, label=\"Test AUC TfIdf-W2V = \"+str(auc(s2_test_fpr, s2_test_tpr)))\nplt.legend()\nplt.xlabel('FPR S2')\nplt.ylabel('TPR S2')\nplt.grid()\nplt.title('ROC-AUC Curve for Set S2 after implementing Decition Tree')\nplt.show()","985a65ef":"confusion_matrix_s2_train = confusion_matrix(y_train, calculate_y_vector_from_threshold(y_train_predicted, s2_train_threshold, s2_train_fpr, s2_train_tpr ) )\n\nconfusion_matrix_s2_test = confusion_matrix(y_test, calculate_y_vector_from_threshold(y_test_predicted_s2, s2_test_threshold, s2_test_fpr, s2_test_tpr ) )\n\nprint('confusion_matrix_s2_train ', confusion_matrix_s2_train)\n# Heatmap for Confusion Matrix: Train and SET 1\nheatmap_confusion_matrix_train_s2 = sns.heatmap(confusion_matrix_s2_train, annot=True, fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 24})\n\nplt.title('S2 Train Set: Confusion Matrix')\nplt.xlabel('Actual X values')\nplt.ylabel('Predicted Y Values')\nplt.show()\n\nheatmap_confusion_matrix_test_s2 = sns.heatmap(confusion_matrix_s2_test, annot=True, fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 24})\n\nplt.title('S2 Test Set: Confusion Matrix')\nplt.xlabel('Actual X values')\nplt.ylabel('Predicted Y Values')\nplt.show()","01a45998":"row_indices_of_false_positives_test_data_s2 = get_false_positive_indices_from_arr(y_test, y_test_predicted_s2)\n\nX_test_essays_original_data = X_test['essay'].values\n# print('X_test_essays_original_data ', X_test_essays_original_data.shape)\n\nessays_text_of_false_positives_test_data = X_test_essays_original_data[row_indices_of_false_positives_test_data_s2]\n# print('essays_text_of_false_positives_test_data ', essays_text_of_false_positives_test_data)\n\ntext_generated = generate_word_cloud_from_text(essays_text_of_false_positives_test_data)\n\nwordcloud_plotter(text_generated)","f09429b4":"X_test_price_original_data = X_test['price'].values\n# print('row_indices_of_false_positves_test_data ', row_indices_of_false_positves_test_data)\n\nprice_row_values_of_false_positive_test_data = X_test_price_original_data[row_indices_of_false_positives_test_data_s2]\n# print('price_row_values_of_false_positive_test_data ', price_row_values_of_false_positive_test_data)\n\nsns.boxplot(price_row_values_of_false_positive_test_data)\n\nplt.title('Box plot with the `price` of `false positive data points`')\n\nplt.show()","2d3485ec":"X_test_teacher_number_original_data = X_test['teacher_number_of_previously_posted_projects'].values\n\nteacher_number_values_of_false_positive_test_data = X_test_teacher_number_original_data[row_indices_of_false_positives_test_data_s2]\n\nsns.distplot(teacher_number_values_of_false_positive_test_data)\n\nplt.title('PDF with `teacher_number` of `false positive data points` for X_test')\n\nplt.show()","2277700b":"dt_clf = DecisionTreeClassifier(class_weight='balanced')\n\n# As Indexing is not implemented for the coo format. \n# Hence using tocsr() - otherwise later can not access elements by index\n# So without it, X_train_s1_merged[:, important_features > 0 ] will throw below error\n# 'coo_matrix' object is not subscriptable\nX_train_s1_merged_csr = X_train_s1_merged.tocsr()\ndt_clf.fit(X_train_s1_merged_csr, y_train)\n\nX_test_s1_merged_csr = X_test_s1_merged.tocsr()\n\nimportant_features = np.array(dt_clf.feature_importances_)\n\n# print(\"important_features length \", len(important_features))\n# print('X_train_s1_merged ', X_train_s1_merged_csr.shape)\n\nX_train_s1_merged_important_features = X_train_s1_merged_csr[:, important_features > 0 ]\nX_test_s1_merged_important_features = X_test_s1_merged_csr[:, important_features > 0 ]","01d93157":"\nsvc = LinearSVC()\n\n# params_svc_gridsearchcv =  {\n#   'C': expon(loc=0, scale=4),\n# }\n\nhyperparams_svc_gridsearchcv =  {\n  \"C\": np.logspace(0, 4, 10)    \n}\n\ngridsearch_svc = GridSearchCV(svc, hyperparams_svc_gridsearchcv, cv=3)\n\ngridsearch_svc.fit(X_train_s1_merged_important_features, y_train )\n\nprint('Best Params from GridSearchCV with Important Features ', gridsearch_svc.best_params_)","1bf7197e":"svc = LinearSVC(C=1)\nsvc.fit(X_train_s1_merged_important_features, y_train )\n\ny_train_predicted = svc.predict(X_train_s1_merged_important_features)\ny_test_predicted = svc.predict(X_test_s1_merged_important_features)\n\n\ntrain_fpr_imp_features, train_tpr_imp_features, train_thresholds_imp_features = roc_curve(y_train, y_train_predicted)\n\ntest_fpr_imp_features, test_tpr_imp_features, test_thresholds_imp_features = roc_curve(y_test, y_test_predicted)\n\nplt.plot(train_fpr_imp_features, train_tpr_imp_features, label=\"Train AUC SVC Imp Features =\"+str(auc(train_fpr_imp_features, train_tpr_imp_features)))\n\nplt.plot(test_fpr_imp_features, test_tpr_imp_features, label=\"Test AUC SVC Imp Features =\"+str(auc(test_fpr_imp_features, test_tpr_imp_features)))\n\nplt.legend()\n\nplt.xlabel(\"False Positive Rate Imp_features\")\n","cc5d7bb0":"confusion_matrix(y_train, y_train_predicted)","050a015d":"confusion_matrix(y_test, y_test_predicted)","49a42058":"import pandas as pd\n\npd.DataFrame({\n  'Model':['TfIdf',  'Tfidf-W2V', 'LinearSVC'],\n  'Train AUC': [0.62, 0.58, 0.52],\n  'Test AUC': [0.60, 0.58, 0.52]\n})","e119ec0f":">numebr of missing values are very less in number, we can replace it with Mrs. as most of the projects are submitted by Mrs.","80542dd3":"# 1.2 Data Analysis","44c87dce":"<ol>\n    <li><strong>Applying Decision Tree Classifier(DecisionTreeClassifier) on the below feature sets<\/strong>\n        <ul>\n            <li><font color='red'>Set 1<\/font>: categorical, numerical features +  preprocessed_essay (TFIDF) + Sentiment scores(preprocessed_essay)<\/li>\n            <li><font color='red'>Set 2<\/font>: categorical, numerical features +  preprocessed_essay (TFIDF W2V) + Sentiment scores(preprocessed_essay)<\/li>        <\/ul>\n    <\/li>\n    <li><strong>The hyper paramter tuning (best `depth` in range [1, 5, 10, 50], and the best `min_samples_split` in range [5, 10, 100, 500])<\/strong>\n        <ul>\n    <li>Find the best hyper parameter which will give the maximum value<\/li>\n    <li>Finding the best hyper paramter using k-fold cross validation(use gridsearch cv or randomsearch cv)<\/li>\n        <\/ul>\n    <\/li>\n    <li>","4d45b8be":"> same process we did in project_subject_categories","3cba99b9":"# 6. Preprocessing Categorical Features: school_state","d53669cb":"## 8.1 applying StandardScaler","c955e50a":"## Heatmap - Plot the performance of model both on train data and cross validation data for each hyper parameter, with rows as min_sample_split, columns as max_depth, and values inside the cell representing AUC Score","bbd59517":"### 1.2.8 Univariate Analysis: Cost per project","38613b2b":"we need to remove the spaces, replace the '-' with '_' and convert all the letters to small","866623ec":"## Sentiment Analysis and adding positive\/neg\/neutral\/compound to the data matrix","5b6dbbcd":"### Vectorizing Price Column after Normalization\n\nIf I do the Normalization without reshaping first \n\n`normalizer.fit(X_train['price'].values)`\n\nThen will get below error\n\n```\nValueError: Expected 2D array, got 1D array instead:\narray=[ 388.99  151.15  944.92   17.74    6.81  479.94  147.02   75.19   64.89\n  565.   1418.08   13.99  184.95  393.83  379.98  193.53  278.88  351.54\n  145.99  299.99  149.97  529.95  628.42  279.98].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n```","03ab0e08":"# Now will be pre-processing the data","a84c26d8":"## Heatmap for Set S2 - Plot the performance of model both on train data and cross validation data for each hyper parameter, with rows as min_sample_split, columns as max_depth, and values inside the cell representing AUC Score","25248edf":"# 3. Preprocessing Categorical Features: project_subject_categories","b710e3b2":"Now we will do the below\n\n# <font color='red'><b> Calculating Non-zero feature importance on Set-1 Features <\/b><\/font>\n\n### Feature Importance on Set-1\n\n*  Selecting all the features which are having non-zero feature importance.You can get the feature importance using  'feature_importances_` \n   (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html), discard the all other remaining features and then apply any of the model of you choice i.e. (Dession tree, Logistic Regression, Linear SVM).\n*  Hyperparameter tuning corresponding to the model you selected\n  \n  **Note**: when you want to find the feature importance make sure you don't use max_depth parameter keep it None.\n  \nSummarize the results    ","0e7b2d51":"# 4. Preprocessing Categorical Features: teacher_prefix","4667a76a":"# 7. Preprocessing Categorical Features: project_title","03581826":"## About the DonorsChoose Data Set\n\nThe `train.csv` data set provided by DonorsChoose contains the following features:\n\nFeature | Description \n----------|---------------\n**`project_id`** | A unique identifier for the proposed project. **Example:** `p036502`   \n**`project_title`**    | Title of the project. **Examples:**<br><ul><li><code>Art Will Make You Happy!<\/code><\/li><li><code>First Grade Fun<\/code><\/li><\/ul> \n**`project_grade_category`** | Grade level of students for which the project is targeted. One of the following enumerated values: <br\/><ul><li><code>Grades PreK-2<\/code><\/li><li><code>Grades 3-5<\/code><\/li><li><code>Grades 6-8<\/code><\/li><li><code>Grades 9-12<\/code><\/li><\/ul>  \n **`project_subject_categories`** | One or more (comma-separated) subject categories for the project from the following enumerated list of values:  <br\/><ul><li><code>Applied Learning<\/code><\/li><li><code>Care &amp; Hunger<\/code><\/li><li><code>Health &amp; Sports<\/code><\/li><li><code>History &amp; Civics<\/code><\/li><li><code>Literacy &amp; Language<\/code><\/li><li><code>Math &amp; Science<\/code><\/li><li><code>Music &amp; The Arts<\/code><\/li><li><code>Special Needs<\/code><\/li><li><code>Warmth<\/code><\/li><\/ul><br\/> **Examples:** <br\/><ul><li><code>Music &amp; The Arts<\/code><\/li><li><code>Literacy &amp; Language, Math &amp; Science<\/code><\/li>  \n  **`school_state`** | State where school is located ([Two-letter U.S. postal code](https:\/\/en.wikipedia.org\/wiki\/List_of_U.S._state_abbreviations#Postal_codes)). **Example:** `WY`\n**`project_subject_subcategories`** | One or more (comma-separated) subject subcategories for the project. **Examples:** <br\/><ul><li><code>Literacy<\/code><\/li><li><code>Literature &amp; Writing, Social Sciences<\/code><\/li><\/ul> \n**`project_resource_summary`** | An explanation of the resources needed for the project. **Example:** <br\/><ul><li><code>My students need hands on literacy materials to manage sensory needs!<\/code<\/li><\/ul> \n**`project_essay_1`**    | First application essay<sup>*<\/sup>  \n**`project_essay_2`**    | Second application essay<sup>*<\/sup> \n**`project_essay_3`**    | Third application essay<sup>*<\/sup> \n**`project_essay_4`**    | Fourth application essay<sup>*<\/sup> \n**`project_submitted_datetime`** | Datetime when project application was submitted. **Example:** `2016-04-28 12:43:56.245`   \n**`teacher_id`** | A unique identifier for the teacher of the proposed project. **Example:** `bdf8baa8fedef6bfeec7ae4ff1c15c56`  \n**`teacher_prefix`** | Teacher's title. One of the following enumerated values: <br\/><ul><li><code>nan<\/code><\/li><li><code>Dr.<\/code><\/li><li><code>Mr.<\/code><\/li><li><code>Mrs.<\/code><\/li><li><code>Ms.<\/code><\/li><li><code>Teacher.<\/code><\/li><\/ul>  \n**`teacher_number_of_previously_posted_projects`** | Number of project applications previously submitted by the same teacher. **Example:** `2` \n\n<sup>*<\/sup> See the section <b>Notes on the Essay Data<\/b> for more details about these features.\n\nAdditionally, the `resources.csv` data set provides more data about the resources required for each project. Each line in this file represents a resource required by a project:\n\nFeature | Description \n----------|---------------\n**`id`** | A `project_id` value from the `train.csv` file.  **Example:** `p036502`   \n**`description`** | Desciption of the resource. **Example:** `Tenor Saxophone Reeds, Box of 25`   \n**`quantity`** | Quantity of the resource required. **Example:** `3`   \n**`price`** | Price of the resource required. **Example:** `9.95`   \n\n**Note:** Many projects require multiple resources. The `id` value corresponds to a `project_id` in train.csv, so you use it as a key to retrieve all resources needed for a project:\n\nThe data set contains the following label (the value you will attempt to predict):\n\nLabel | Description\n----------|---------------\n`project_is_approved` | A binary flag indicating whether DonorsChoose approved the project. A value of `0` indicates the project was not approved, and a value of `1` indicates the project was approved.","677e4cd2":"### 1.2.6 Univariate Analysis: Text features (Title)","28e5f267":"<font color='red'><b> TF-IDFW2V<\/b><\/font>\n\n<b>Tfidf w2v (w1,w2..) = (tfidf(w1) * w2v(w1) + tfidf(w2) * w2v(w2) + \u2026)  \/    (tfidf(w1) + tfidf(w2) + \u2026)<\/b>","3a3163cb":"## 8.2 applying MinMaxScaler","eb463e87":"![Imgur](https:\/\/imgur.com\/GZSklhL.png)\n\n# [DonorsChoose](https:\/\/en.wikipedia.org\/wiki\/DonorsChoose)\n\nDonorsChoose is a United States-based nonprofit organization that allows individuals to donate directly to public school classroom projects. The organization has been given Charity Navigator's highest rating every year since 2005.[4] In January 2018, they announced that 1 million projects had been funded. In 80% of public schools in the United States, at least one project has been requested on DonorsChoose. Schools from wealthy areas are more likely to make technology requests, while schools from less affluent areas are more likely to request basic supplies.[6] It has been noted that repeat donors on DonorsChoose typically donate to projects they have no prior relationship with, and most often fund projects serving financially challenged students.","2f6dbecb":"### Set S2 - A basic observtion on the above Heatmap\n\n- For training dataset the best score is coming max_depth = 50 and min_samples_split = 500\n- For validation dataset the best score is coming max_depth = 5 and min_samples_split = 500 as given in the GridSearchCV best_params_\n\n## Set S2 - ROC curve - After finding the best hyper parameter, training our model with it, and finding the AUC on test data and plot the ROC curve on both train and test.\n","6c8f26e5":"## 1. Reading Data","63ef9cdd":"### 1.2.2 Univariate Analysis: teacher_prefix","1f843eea":"## S2 - Confusion matrix with predicted and original labels of test data points","c39faea7":"### 1.2.7 Univariate Analysis: Text features (Project Essay's)","99752342":">convert all of them into small letters","6283ed62":"## Merging (with hstack) to construct S2 for TFIDF W2V","04a7a2d0":"# 5. Preprocessing Categorical Features: project_subject_subcategories","546b093f":"<font color='blue'><b>Hint for calculating Sentiment scores<\/b><\/font>","66cd98f6":"# 5. Set S1 - Merging (with hstack) all the above vectorized features that we created above  \n\nFirst a note on merging tith hstack from https:\/\/www.w3resource.com\/numpy\/manipulation\/hstack.php\n\n#### After vectorizing for all non-sparse features (like price, sentiments), I will get arrays of the form - [[3], [5], [7]]\n\n```py\n\nimport numpy as np\n\nx = np.array([[3], [5], [7]])\ny = np.array([[5], [7], [9]])\nnp.hstack((x,y))\n\n>>> array([[3, 5],\n           [5, 7],\n           [7, 9]])\n\n```\n\n![Imgur](https:\/\/imgur.com\/ypYNtUk.png)\n\n#### And similarly `scipy.sparse.hstack` concatenates the sparse tf-idf matrices (with the same number of rows) returned by TfidfVectorizer.fit_transform.\n\n---\n\n#### You can use the scipy.sparse.hstack to concatenate sparse matrices with the same number of rows (horizontal concatenation):\n\n`hstack((X1, X2))`\n\n### We need to merge all the numerical vectors i.e catogorical, text, once for Set S1 and then for S2 later","d4a4456c":"# 2. Preprocessing Categorical Features: project_grade_category","ff24d627":"# 8. Preprocessing Categorical Features: essay","c48ecd68":"# Set-1: categorical, numerical features +  essay (TFIDF) + Sentiment scores(preprocessed_essay)\n\n## Bag of Words for the \"essay\" column\n","7e632cad":"# <font color='red'> <b>Featurization - 1<\/b><\/font>","80cc8db9":"### 1.2.1 Univariate Analysis: School State","7e9dd9a9":"## Word-Cloud with all the `false positive data points`\n\n- Plot the WordCloud(https:\/\/www.geeksforgeeks.org\/generating-word-cloud-python\/) with the words of essay text of these `false positive data points`\n\n- Plot the box plot with the `price` of these `false positive data points`\n\n- Plot the pdf with the `teacher_number_of_previously_posted_projects` of these `false positive data points`","9fc786c2":"### The hyper paramter tuning with GridSearchCV\n\nbest `depth` in range [1, 5, 10, 50], and the best `min_samples_split` in range [5, 10, 100, 500]\n\nTo find the best hyper parameter which will give the maximum AUC","2859b131":"__Every state is having more than 80% success rate in approval__","83e386cf":"# 8. Preprocessing Numerical Values: price","c0d6f977":"### 1.2.3 Univariate Analysis: project_grade_category","05a8162a":"# Set-2: categorical, numerical features +  essay (TFIDF W2V) + Sentiment scores(preprocessed_essay)\n\nThe only change in Set-2 is the \"essay\" column - Now here I have to calculate the TFIDF W2V instead of just TFIDF\n\nThe rest of the Vectorized column will remain same, and hence I just need do the merging with hstack() this new TFIDF W2V column for 'text'","0ae7a1aa":"## Confusion matrix with predicted and original labels of test data points","a5660d04":"### Now will Standardize and then .fit() and .transform() all the Sentiments related Columns","e2bf1b25":"## 1.1 Reading Data","1bb8dabe":"### Notes on the Essay Data\n\n<ul>\nPrior to May 17, 2016, the prompts for the essays were as follows:\n<li>__project_essay_1:__ \"Introduce us to your classroom\"<\/li>\n<li>__project_essay_2:__ \"Tell us more about your students\"<\/li>\n<li>__project_essay_3:__ \"Describe how your students will use the materials you're requesting\"<\/li>\n<li>__project_essay_3:__ \"Close by sharing why your project will make a difference\"<\/li>\n<\/ul>\n\n\n<ul>\nStarting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:<br>\n<li>__project_essay_1:__ \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"<\/li>\n<li>__project_essay_2:__ \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"<\/li>\n<br>For all projects with project_submitted_datetime of 2016-05-17 and later, the values of project_essay_3 and project_essay_4 will be NaN.\n<\/ul>\n","1cfdfe8e":">remove spaces, 'the' <br>\nreplace '&' with '\\_', and ',' with '_'","dd77b991":"### 1.2.4 Univariate Analysis: project_subject_categories","43ca9696":"## Splitting the whole dataset into train and test\n\nImportant Note - Whenever you want to perform simple cross-validation, split the dataset into the train, CV and the test sets. If you want to perform K-fold CV(using 'for' loops) or GridSearch cross-validation or RandomSearch cross-validation, just split the dataset into the train and the test sets. The CV set gets created internally during the cross-validation. Even if you create a separate CV set while splitting the whole dataset, it goes waste without any usage.","468fc15e":"### A basic observtion on the above Heatmap\n\n- For training dataset the best score is coming max_depth = 50 and min_samples_split = 500\n- For validation dataset the best score is coming max_depth = 10 and min_samples_split = 500\n\n## ROC curve - After finding the best hyper parameter, training our model with it, and finding the AUC on test data and plot the ROC curve on both train and test.\n","70eb89fa":"## 1.1 Loading Data","6a2a95d2":">Remove '.' <br>\n>convert all the chars to small","75802dac":"<h1>1. Decision Tree <\/h1>","14d49dd2":"### 1.2.5 Univariate Analysis: project_subject_subcategories"}}