{"cell_type":{"eee12162":"code","b51e4047":"code","8638c79f":"code","43aaa73c":"code","a0c2a0d6":"code","7ddfc545":"code","c9c712f2":"code","eac08f2f":"code","7233f0a9":"code","4fd17769":"code","692675f1":"code","f322944c":"code","423d63c4":"code","5482eb44":"code","1c466120":"code","75fd1171":"code","7b5577e8":"code","7d1a1912":"code","d57a5ae0":"code","2be574a4":"code","d9cfc5c0":"code","c2f24eb8":"code","34e96c5a":"code","a37e088a":"code","116b8d76":"code","5838aaa3":"code","5e548667":"code","1671bd2e":"code","10504a19":"code","1248b5cc":"markdown"},"source":{"eee12162":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport random","b51e4047":"!pip install openpyxl","8638c79f":"pic1=pd.read_excel('..\/input\/peach-dataset\/Peach Dataset\/pictures.xlsx',sheet_name='Sheet1')\npic2=pd.read_excel('..\/input\/peach-dataset\/Peach Dataset\/pictures.xlsx',sheet_name='Sheet2')\npic3=pd.read_excel('..\/input\/peach-dataset\/Peach Dataset\/pictures.xlsx',sheet_name='Sheet3')\npic4=pd.read_excel('..\/input\/peach-dataset\/Peach Dataset\/pictures.xlsx',sheet_name='Sheet4')\npic5=pd.read_excel('..\/input\/peach-dataset\/Peach Dataset\/pictures.xlsx',sheet_name='Sheet5')\npic6=pd.read_excel('..\/input\/peach-dataset\/Peach Dataset\/pictures.xlsx',sheet_name='Sheet6')\npic=pd.concat([pic1,pic2,pic3,pic4,pic5,pic6])\npic","43aaa73c":"pic['fileName2']=pic['fileName'].apply(lambda x:x[2:])\npic","a0c2a0d6":"pic.info()","7ddfc545":"data_dir = '..\/input\/peach-dataset\/Peach Dataset'","c9c712f2":"path0='..\/input\/peach-dataset\/Peach Dataset\/1\/IMG_8729.JPG'\nimage=cv2.imread(path0)\nprint(image.shape)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","eac08f2f":"image3=cv2.resize(image,dsize=(150,100),interpolation=cv2.INTER_CUBIC)\nprint(image3.shape)\nprint(type(image3))\nplt.imshow(cv2.cvtColor(image3, cv2.COLOR_BGR2RGB))","7233f0a9":"cara_mapping={0:'a',1:'b'}","4fd17769":"import itertools\nfrom itertools import product\nClass=['1','2','3','4','5','6']\nCara=['a','b']\nName=[]\nA0=list(itertools.product(Class,Cara))\nfor item in A0:\n    Name+=[item[0]+item[1]]\nprint(Name)\nprint(len(Name))","692675f1":"N=list(range(len(Name)))\nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) ","f322944c":"dataX=[]\ndataY0=[]\nfor classname in Class:\n    path=os.path.join(data_dir,classname)\n    for im in os.listdir(path):\n        if im=='IMG_9374_recortada.jpg':\n            im='IMG_9374.JPG'\n        cara=pic[pic['fileName2']==im]['cara'].tolist()[0]\n        clase=pic[pic['fileName2']==im]['clase'].tolist()[0]\n        image=cv2.imread(os.path.join(path,im))\n        if type(image)==np.ndarray and image.shape[0]>10 and image.shape[1]>10:\n            image3=cv2.resize(image,dsize=(150,100),interpolation=cv2.INTER_CUBIC)\n            dataX+=[image3]\n            dataY0+=[str(clase)+cara_mapping[cara]]","423d63c4":"dataY1=pd.Series(dataY0).map(normal_mapping)","5482eb44":"dataX=np.array(dataX)\ndataY1=np.array(dataY1)","1c466120":"m=len(dataX)\nM=list(range(m))\nrandom.seed(2021)\nrandom.shuffle(M)","75fd1171":"trainX=dataX[M[0:(m\/\/4)*3]]\ntrainY0=dataY1[M[0:(m\/\/4)*3]]\n\ntestX=dataX[M[(m\/\/4)*3:]]\ntestY0=dataY1[M[(m\/\/4)*3:]]","7b5577e8":"labels1=to_categorical(trainY0)\ntrainY=np.array(labels1)","7d1a1912":"trainx,testx,trainy,testy=train_test_split(trainX,trainY,test_size=0.2,random_state=44)","d57a5ae0":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","2be574a4":"pretrained_model3 = tf.keras.applications.DenseNet201(input_shape=(100,150,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model3.trainable = False","d9cfc5c0":"inputs3 = pretrained_model3.input\nx3 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model3.output)\noutputs3 = tf.keras.layers.Dense(len(Name), activation='softmax')(x3)\nmodel = tf.keras.Model(inputs=inputs3, outputs=outputs3)","c2f24eb8":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","34e96c5a":"his=model.fit(trainx,trainy,batch_size=32,validation_data=(testx,testy),epochs=100)\n#his=model.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=2)","a37e088a":"y_pred=model.predict(testx)\npred=np.argmax(y_pred,axis=1)\nground = np.argmax(testy,axis=1)\nprint(classification_report(ground,pred))","116b8d76":"get_acc = his.history['accuracy']\nvalue_acc = his.history['val_accuracy']\nget_loss = his.history['loss']\nvalidation_loss = his.history['val_loss']\n\nepochs = range(len(get_acc))\nplt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\nplt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\nplt.title('Training vs validation accuracy')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","5838aaa3":"epochs = range(len(get_loss))\nplt.plot(epochs, get_loss, 'r', label='Loss of Training data')\nplt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\nplt.title('Training vs validation loss')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","5e548667":"pred2=model.predict(testX)\nprint(pred2.shape)\n\nPRED=[]\nfor item in pred2:\n    value2=np.argmax(item)      \n    PRED+=[value2]\nprint(pd.Series(PRED).value_counts())","1671bd2e":"ANS=testY0\nprint(pd.Series(ANS).value_counts())\naccuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","10504a19":"fig, axs = plt.subplots(3,3,figsize=(12,12))\nfor i in range(9):\n    r=i\/\/3\n    c=i%3\n    img1 = testX[i]\n    ax=axs[r][c].axis(\"off\")\n    actual=reverse_mapping[testY0[i]]\n    predict=reverse_mapping[PRED[i]]    \n    ax=axs[r][c].set_title(actual+'=='+predict)\n    ax=axs[r][c].imshow(img1)\nplt.show()","1248b5cc":"datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.2,\n                        width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")"}}