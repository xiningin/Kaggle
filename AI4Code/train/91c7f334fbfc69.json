{"cell_type":{"79fbad0e":"code","ca944486":"code","f63bdacc":"code","b2ccddcc":"code","1c4a7d34":"code","18e00ec1":"code","cbec9284":"code","81494b50":"code","c0f9cdc8":"code","8dd980de":"code","2e584cb2":"code","6fc90eef":"code","c42c801b":"code","6bfd9483":"code","081e7275":"code","e380463f":"code","de0d2f21":"code","084a1f7e":"code","3ffabde5":"code","c6723a3e":"code","f03a2681":"code","71d75c34":"code","362ba303":"code","973e1890":"code","b476380b":"markdown","ff4b1761":"markdown","177af953":"markdown","8ca2749b":"markdown","c1464a1f":"markdown","e534d933":"markdown","603bbbf4":"markdown","e1cb102a":"markdown","4d62ff32":"markdown","24609348":"markdown","350a790e":"markdown","e87a7d52":"markdown","9637297c":"markdown","61fe038b":"markdown","69b3bc90":"markdown","cca8c7c5":"markdown","be75cbf5":"markdown","757ff74d":"markdown"},"source":{"79fbad0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization\n\n# to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# train-test split\nfrom sklearn.model_selection import train_test_split\n\n# libraries for model building\nimport sklearn\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca944486":"# set the plot size\nplt.rcParams['figure.figsize'] = (10,5)","f63bdacc":"# read the data\ndf_mushroom = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\n\n# check the first five observations\ndf_mushroom.head()","b2ccddcc":"# check the dimension of the data\ndf_mushroom.shape","1c4a7d34":"# check the data type of each variable\ndf_mushroom.dtypes","18e00ec1":"df_mushroom['stalk-root'].value_counts()","cbec9284":"# replace '?' by 'b'\ndf_mushroom['stalk-root'] = df_mushroom['stalk-root'].replace(to_replace = '?', value ='b')","81494b50":"# check the count of each level in 'stalk-root'\ndf_mushroom['stalk-root'].value_counts()","c0f9cdc8":"# statistical summary of the dataset\ndf_mushroom.describe()","8dd980de":"# drop the variable 'veil-type'\ndf_mushroom = df_mushroom.drop('veil-type', axis=1)","2e584cb2":"# plot the count plot for each categorical variable \n# 'figsize' sets the figure size\n# pass the required number of rows and columns to plot the grid of subplots \nfig, ax = plt.subplots(4, 6, figsize=(28, 20))\nfor variable, subplot in zip(df_mushroom.columns, ax.flatten()):\n    sns.countplot(df_mushroom[variable], ax=subplot)\n\n# set the spacing between plots\nplt.tight_layout()\n\n# display the plot\nplt.show()","6fc90eef":"# use 'get_dummies' from pandas to create dummy variables\n# encode the independent variables\ndummy_var = pd.get_dummies(data = df_mushroom.drop('class', axis=1))\n\ndummy_var.head()","c42c801b":"# check the shape of the dummy encoded dataframe\ndummy_var.shape","6bfd9483":"# consider the target variable\ndf_target = df_mushroom['class']\n\n# split data into train and test set\n# set 'random_state' to generate the same dataset each time you run the code \n# 'test_size' returns the proportion of data to be included in the test set\nX_train, X_test, y_train, y_test = train_test_split(dummy_var, df_target, random_state = 1, test_size = 0.2)\n\n# check the dimensions of the train & test subset using 'shape'\n# print dimension of train set\nprint('X_train', X_train.shape)\nprint('y_train', y_train.shape)\n\n# print dimension of test set\nprint('X_test', X_test.shape)\nprint('y_test', y_test.shape)","081e7275":"# pass the required number of trees in the random forest to the parameter, 'n_estimators'\n# pass the 'random_state' to obtain the same samples for each time you run the code\nrf_classification = RandomForestClassifier(n_estimators = 35, random_state = 1)\n\n# use fit() to fit the model on the train set\nrf_model = rf_classification.fit(X_train, y_train)","e380463f":"# predict the target variable\ny_test_predicted = rf_model.predict(X_test)\n\n# print the classification report for test set\nprint(classification_report(y_test, y_test_predicted))","de0d2f21":"# create a dataframe that stores the feature names and their importance score\n# 'feature_importances_' returns the features based on the gini importance\nimportant_features = pd.DataFrame({'Features': X_train.columns, \n                                   'Importance': rf_model.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features[0:20])\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()","084a1f7e":"# consider the above variables as independent features and build a random forest model\ndf_imp_feat = df_mushroom[['odor', 'stalk-surface-below-ring', 'gill-size', 'ring-type', 'bruises', 'gill-spacing', 'stalk-root', 'population', \n                           'spore-print-color']]\n\n# encode the variables \ndummy_var = pd.get_dummies(data = df_imp_feat)\n\ndummy_var.head()","3ffabde5":"# consider the target variable\ndf_target = df_mushroom['class']\n\n# split data into train and test set\n# set 'random_state' to generate the same dataset each time you run the code \n# 'test_size' returns the proportion of data to be included in the test set\nX_train, X_test, y_train, y_test = train_test_split(dummy_var, df_target, random_state = 1, test_size = 0.2)\n\n# check the dimensions of the train & test subset using 'shape'\n# print dimension of train set\nprint('X_train', X_train.shape)\nprint('y_train', y_train.shape)\n\n# print dimension of test set\nprint('X_test', X_test.shape)\nprint('y_test', y_test.shape)","c6723a3e":"# pass the required number of trees in the random forest to the parameter, 'n_estimators'\n# pass the 'random_state' to obtain the same samples for each time you run the code\nrf_classification = RandomForestClassifier(n_estimators = 35, random_state = 1)\n\n# use fit() to fit the model on the train set\nrf_model = rf_classification.fit(X_train, y_train)","f03a2681":"# predict the target variable\ny_test_predicted = rf_model.predict(X_test)\n\n# print the classification report for test set\nprint(classification_report(y_test, y_test_predicted))","71d75c34":"# create a confusion matrix\nconf_mat = confusion_matrix(y_test, y_test_predicted)\n\n# label the confusion matrix  \nconf_matrix = pd.DataFrame(data = conf_mat,columns = ['Predicted:e','Predicted:p'], index = ['Actual:e','Actual:p'])\n\n# plot a heatmap to visualize the confusion matrix\nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Greens', cbar = False, linewidths = 0.1, annot_kws = {'size':20})\n\n# set the font size of x-axis ticks using 'fontsize'\nplt.xticks(fontsize = 15)\n\n# set the font size of y-axis ticks using 'fontsize'\nplt.yticks(fontsize = 15)\n\n# display the plot\nplt.show()","362ba303":"# initialize the SGBoost classifier\nxgb_model = XGBClassifier(max_depth = 8, learning_rate = 0.1, random_state = 5)\n\n# fit the model using fit() on train data\nxgb_model.fit(X_train, y_train)","973e1890":"# predict the target variable\ny_test_predicted = xgb_model.predict(X_test)\n\n# print the classification report for test set\nprint(classification_report(y_test, y_test_predicted))","b476380b":"The report shows that the accuracy, sensitivity, specificity and the f1-score is 1. This means that the random forest model has predicted all the mushrooms correctly.\n\nLet us look at the important features in the random forest. Here there are 115 dummy encoded features. Thus we will plot only the top 20 important features.","ff4b1761":"Now we have 115 independent features and one target variable.\n\n### Split the Data into Train and Test Set","177af953":"The above matrix shows that all the mushrooms are correctly classified. Thus, we build a model with only 43 features instead of 115 and obtained the 100% accuracy.\n\nLet us use these 43 features to build a XGBoost model.\n\n### Build the XGBoost Model\n\nLet us build the XGBoost model using the 43 dummy encoded features.","8ca2749b":"The above output shows that there is majority of edible mushrooms in the dataset. The variable `veil-type` contains only single value throughout the dataset; thus this variable will be redundant for the analysis. We remove `veil-type` before further analysis.\nAlso, there are no missing values in the data. ","c1464a1f":"### Distribution of Variables","e534d933":"The plot shows that the encoded variable 'odor_n' is the most important variable in the random forest. Now we consider these variables to again build a random forest.\n\nWe can not consider only few levels of a categorical variable. Either we need to consider all the dummy encoded levels of a variable or we will remove the variable entirely. Thus we use the distribution plot to choose the categorical variables.\n\nConsider the following variables: `odor, stalk-surface-below-ring, gill-size, ring-type, bruises, gill-spacing, stalk-root, population, spore-print-color`","603bbbf4":"### Read the Data","e1cb102a":"There are 2480 missing values in the variable `stalk-root`.  \n\nWe can not remove the observations with missing data, as it will reduce the dimension of the dataset significantly. Thus we need to replace '?' with the mode of the variable i.e. 'b'.","4d62ff32":"The XGBoost model gives the 100% accuracy.\n\n\nSo far we have build three models:\n1. Random forest on all the 115 dummy encoded features\n2. Random forest on the 43 dummy encoded significant features\n3. XGBoost on the 43 dummy encoded significant features\n\nAll the three models have 100% accuracy. i.e. all the models classified the poisonous\/ edible mushrooms correctly!! For the future prediction of a mushroom we can consider the random forest\/ XGBoost model with 43 dummy encoded features.\n\n### Thanks for reading!","24609348":"#### Split the data into train and test set","350a790e":"The accuracy and f-1 score of the random forest with significant variables is 100%.","e87a7d52":"We can see that the '?' is replaced by 'b'.\n\n### Statistical Summary","9637297c":"The dataset information shows that the variable `stalk-root` contains non-standard missing value. (i.e. '?') Check the count of different levels in this variable and treat the missing data before starting the analysis.","61fe038b":"Now there are 43 independent features in the dataset.\n\n### Build a Random Forest using Significant Variables","69b3bc90":"## Model Building\n\nWe first build the random forest on the training dataset and check the accuracy of the model.\n\n### Build a Random Forest Model","cca8c7c5":"Here `class` is the target variable that includes the edible or poisonous category of mushroom which is to be predicted.\n\n## Exploratory Data Analysis\n\n### Understand the Dataset","be75cbf5":"The algorithms like random forest, XGBoost will require the features in the numeric format. Thus, we dummy encode the independent variables.","757ff74d":"The plot shows that the levels of target variable 'class' are balanced. \n\n### Encode the Categorical Variables"}}