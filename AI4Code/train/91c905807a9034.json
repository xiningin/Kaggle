{"cell_type":{"7e16f68d":"code","7b52c422":"code","c4aa723a":"code","d3e2ee3f":"code","f2e74941":"code","93017636":"code","69d367e3":"code","22190c22":"code","fa8a4a15":"code","2af27972":"code","47e3cfed":"code","5a752e6a":"code","38753b65":"code","9a9438cd":"code","e05a6093":"code","bb5677ec":"code","4c0079da":"code","bb749bb3":"code","422a3bff":"code","de939089":"code","f3c1ca0e":"code","ef49cc2b":"code","421fc0e9":"code","c59dabb7":"code","65e135c6":"code","0b94ba4b":"code","ac2cee85":"code","dc62de81":"code","a6d64391":"code","9358c77d":"code","0741b728":"code","694c7743":"code","f7052c15":"code","d9f372cc":"code","aa5bdc78":"code","b100e67f":"code","39659336":"code","f41a5d2d":"code","8ca8523f":"code","b4ab8a7f":"code","e96df6c2":"code","6adf1d81":"code","002ce4e8":"code","d4e33613":"code","288480d2":"markdown","9aeece45":"markdown","9702cc71":"markdown","b7ec591c":"markdown","eb925a6a":"markdown","455ca82e":"markdown","37a8e299":"markdown","1b6a6391":"markdown","d3981115":"markdown","50051ec3":"markdown","b89199ee":"markdown","a014332d":"markdown"},"source":{"7e16f68d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7b52c422":"import pandas as pd \nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras.layers import Input, Add, Dense,  Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.models import  Model  , load_model\nfrom tensorflow.keras.initializers import  glorot_uniform\nfrom tensorflow.keras.optimizers import SGD","c4aa723a":"df = pd.read_csv(\"..\/input\/sample-sales-data\/sales_data_sample.csv\", encoding='unicode_escape')\ndf.head()","d3e2ee3f":"df.info()","f2e74941":"# Convert order date to datetime format\ndf['ORDERDATE'] = pd.to_datetime(df['ORDERDATE'])","93017636":"# check null\ndf.isnull().sum()","69d367e3":"# drop 'status' since it's unbalanced feature\n# drop 'addressline2', 'state', 'postal code' and 'territory because there are lots of null values.\n# drop 'city', 'address1', 'phone number', 'customer name', 'contact last_name' and 'contact first_name' since they are not required for the analysis and we already got 'Country' & 'City' for grographical information.\nto_drop  = ['STATUS','ADDRESSLINE1', 'ADDRESSLINE2', 'POSTALCODE', 'CITY', 'TERRITORY', 'PHONE', 'STATE', 'CONTACTFIRSTNAME', 'CONTACTLASTNAME', 'CUSTOMERNAME', 'ORDERNUMBER']\ndf = df.drop(to_drop, axis = 1)","22190c22":"# check null again\ndf.isnull().sum().sum()","fa8a4a15":"df.head()","2af27972":"labelencoder = LabelEncoder()","47e3cfed":"# label encode string values\ndf.loc[:, 'PRODUCTLINE'] = labelencoder.fit_transform(df.loc[:, 'PRODUCTLINE'])\ndf.loc[:, 'COUNTRY'] = labelencoder.fit_transform(df.loc[:, 'COUNTRY'])\ndf.loc[:, 'DEALSIZE'] = labelencoder.fit_transform(df.loc[:, 'DEALSIZE'])\ndf.loc[:, 'PRODUCTCODE'] = labelencoder.fit_transform(df.loc[:, 'PRODUCTCODE'])","5a752e6a":"df.head()","38753b65":"plt.figure(figsize=(10,50))\n\nc1,c2 = sns.color_palette('Set2',2)\n\nfor i in range(len(df.columns)):\n    plt.subplot(17,1,i+1)\n    sns.histplot(df[df.columns[i]])\n    plt.xlabel(' '.join(df.columns[i].split(sep='_')).title())\n    plt.title(f\"{' '.join(df.columns[i].split(sep='_')).title()} KDE and hist plot\")\n\nplt.tight_layout()","9a9438cd":"# drop 'ORDERDATE' since we have 'MONTH' and 'YEAR'\ndf.drop(\"ORDERDATE\", axis = 1, inplace = True)","e05a6093":"# Visualize the relationship between variables using pairplots\nfig = px.scatter_matrix(df, dimensions=df.columns, color='MONTH_ID')# fill color by months\nfig.update_layout(title_text='Sales Data',\n                  width=1100,\n                  height=1100)\nfig.show()","bb5677ec":"scale = StandardScaler().fit(df)\nscaled_data = scale.transform(df)","4c0079da":"scores_1 = []\n\nrange_of_cluster = range(1,10)\n\nfor i in range_of_cluster:\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(scaled_data)\n    print(f\"{i} Cluster score: {kmeans.inertia_}\")\n    scores_1.append(kmeans.inertia_)","bb749bb3":"#plot the scores of elbow method to  find the optimum number of clusters\nplt.figure(figsize=(15,8))\nplt.plot(scores_1, 'r*-')\n#fix the cluster representation on th eplot\nplt.xticks(np.arange(len(scores_1)), np.arange(1, len(scores_1) +1) )\nplt.title('Finding the right number of cluster')\nplt.xlabel('Cluster')\nplt.ylabel('Scores WCSS')\nplt.show()","422a3bff":"for num_clusters in range(2,10):\n    clusterer = KMeans(n_clusters=num_clusters)\n    preds = clusterer.fit_predict(scaled_data)\n    # centers = clusterer.cluster_centers_\n    score = silhouette_score (scaled_data, preds, metric='euclidean')\n    print (\"For n_clusters = {}, Kmeans silhouette score is {})\".format(num_clusters, score))","de939089":"# Fit KMeans and calculate sse,Silhouette_score for each k\nsse={}\nSilhouette_score={}\nfor k in range(2,11):\n    kmeans=KMeans(n_clusters=k,init='k-means++',random_state=0)\n    kmeans.fit(scaled_data)\n    sse[k]=kmeans.inertia_\n    \n    labels=kmeans.labels_\n    Silhouette_score[k]=silhouette_score(scaled_data,labels)\n    \nfig,(ax1,ax2)=plt.subplots(1,2)\n#Plot For The Elbow Method\nsns.pointplot(list(sse.keys()),list(sse.values()),ax=ax1)\nax1.set_xlabel('No of Cluster')\nax1.set_title('The Elbow Method')\n\n#Plot For The Silhouette Coefficient \nsns.pointplot(list(Silhouette_score.keys()),list(Silhouette_score.values()),ax=ax2)\nax2.set_title('Silhouette score Coefficient')\nax2.set_xlabel('No of Cluster')\nax2.set_ylabel('Silhouette score')\nfig.tight_layout()","f3c1ca0e":"num_cluster = 5\nkmeans = KMeans(n_clusters=num_cluster)\nkmeans.fit(scaled_data)\nlabels = kmeans.labels_\nlabels","ef49cc2b":"clusters_centers = pd.DataFrame(data=kmeans.cluster_centers_, columns=[df.columns])","421fc0e9":"y_kmeans = kmeans.fit_predict(scaled_data)\ny_kmeans","c59dabb7":"data_with_cluster = pd.concat([df, pd.DataFrame({'CLUSTER': labels})], axis=1)\ndata_with_cluster.head()","65e135c6":"for i in df.columns:\n    plt.figure(figsize=(35,5))\n    for j in range(num_cluster):\n        plt.subplot(1,num_cluster,j+1)\n        cluster = data_with_cluster[data_with_cluster['CLUSTER'] == j]\n        cluster[i].hist(bins=20)\n        plt.title(f\"Cluster: {j} \\n {i}\")\nplt.show()","0b94ba4b":"pca = PCA(n_components=2)\nprincipal_comp = pca.fit_transform(data_with_cluster)\nprincipal_comp","ac2cee85":"df_pca = pd.DataFrame(data = principal_comp, columns=['pca1','pca2'])\ndf_pca.head()","dc62de81":"#Concantenate the PCA and the clusters\ndf_pca = pd.concat([df_pca, pd.DataFrame({'cluster': labels})], axis=1)\ndf_pca.head()","a6d64391":"fig = px.scatter(df_pca, x='pca1', y='pca2', \n                    color='cluster', symbol='cluster', size_max=15, opacity=0.7)\nfig.update_layout(margin = dict(l = 0, r = 0, b = 0, t = 0))","9358c77d":"tf.keras.backend.clear_session()","0741b728":"inputs = tf.keras.Input(shape=(12,))\n\nx = tf.keras.layers.Dense(4,activation='relu')(inputs)\nx = tf.keras.layers.Dense(64,activation='relu',kernel_initializer = 'glorot_uniform')(x)\nx = tf.keras.layers.Dense(128,activation='relu',kernel_initializer = 'glorot_uniform')(x)\nx = tf.keras.layers.Dense(256,activation='relu',kernel_initializer = 'glorot_uniform')(x)\n\nencoded = tf.keras.layers.Dense(8,activation='relu',kernel_initializer = 'glorot_uniform')(x)\n\nx = tf.keras.layers.Dense(256,activation='relu',kernel_initializer = 'glorot_uniform')(encoded)\nx = tf.keras.layers.Dense(128,activation='relu',kernel_initializer = 'glorot_uniform')(x)\n\ndecoded = tf.keras.layers.Dense(12,kernel_initializer = 'glorot_uniform')(x)\n\nautoencoder = tf.keras.Model(inputs,decoded)\n\nencoder = tf.keras.Model(inputs,encoded)\n\nautoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')","694c7743":"autoencoder.fit(scaled_data, scaled_data, batch_size=128, epochs=50, verbose= 1)","f7052c15":"autoencoder.summary()","d9f372cc":"# taking only encoder part\npred = encoder.predict(scaled_data)\npred.shape","aa5bdc78":"#we have 8 features right now\n#we are going to use pred as out new data with 10 features\n\n #optimum number of cluster\n#calcaulated the score for Pred\nscores_2 = []\nfor i in range_of_cluster:\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(pred)\n    scores_2.append(kmeans.inertia_)\n  \n    print(f\"{i} Cluster score: {kmeans.inertia_}\")\n\nplt.figure(figsize=(15,8)) \nplt.plot(scores_2, 'r*-')\nplt.xticks(np.arange(len(scores_2)), np.arange(1, len(scores_2)+1))\nplt.title('Finding the right number of clusters')\nplt.xlabel('Cluaster')\nplt.ylabel('Scores')\nplt.show()","b100e67f":"plt.figure(figsize=(15,8)) \nplt.plot(scores_2, 'r*-')\nplt.plot(scores_1, 'b*-')\nplt.xticks(np.arange(len(scores_2)), np.arange(1, len(scores_2)+1))\nplt.title('Finding the right number of clusters')\nplt.xlabel('Cluaster')\nplt.ylabel('Scores')\nplt.show()","39659336":"#apply kmeans\n\nnumber_of_cluster = 5\n\nkmeans = KMeans(n_clusters=number_of_cluster)\nkmeans.fit(pred)\nlabels = kmeans.labels_\nlabels","f41a5d2d":"data_auto_with_cluster = pd.concat([df, pd.DataFrame({'CLUSTER': labels})], axis=1)\ndata_auto_with_cluster.head()","8ca8523f":"df","b4ab8a7f":"for i in df.columns:\n    plt.figure(figsize=(35,5))\n    \n    for j in range(number_of_cluster):\n        plt.subplot(1, number_of_cluster, j+1)\n        cluster = data_auto_with_cluster[data_auto_with_cluster['CLUSTER'] == j]\n        cluster[i].hist(bins=20)\n        plt.title(f\"Cluster {j} \\n {i} \")\nplt.show()","e96df6c2":"#PCA\n\npca = PCA(n_components=2)\nprincipal_comp_auto = pca.fit_transform(pred)\nprincipal_comp_auto","6adf1d81":"pca_auto_df = pd.DataFrame(data=principal_comp_auto, columns=['pca1','pca2'])","002ce4e8":"pca_auto_df = pd.concat([pca_auto_df, pd.DataFrame({'cluster':labels})], axis=1)\npca_auto_df.head()","d4e33613":"fig = px.scatter(pca_auto_df, x='pca1', y='pca2', \n                    color='cluster', symbol='cluster', size_max=15, opacity=0.7)\nfig.update_layout(margin = dict(l = 0, r = 0, b = 0, t = 0))","288480d2":"* A trend exists between 'SALES' and 'QUANTITYORDERED'\n* A trend exists between 'PRICEEACH' and 'SALES'\n* It seems that sales growth exists as we move from 2013 to 2014 to 2015 ('SALES' vs. 'YEAR_ID')","9aeece45":"### Encoding categorical features ","9702cc71":"## Scaling","b7ec591c":"## Preprocessing","eb925a6a":"## EDA","455ca82e":"* Clustor 0: High quantity, price mostly around 100, high in sales\n* Clustor 1: Low quantity (not more than 50), price mostly around 100, ordered in year 2003 and 2004\n* Clustor 2: High quantity, price mostly around 100, high in sales, ordered in year 2004 and 2005, high in MSRP\n* Clustor 3: Price varies, Lowest sales value, deal size mostly just 2, low in MSRP\n* Clustor 4: Price mostly around 100, high in sales, ordered in year 2003 and 2004, high in MSRP, deal size mostly just 1","37a8e299":"## Import Libraries","1b6a6391":"## Import dataset","d3981115":"## Principal Component Analysis (PCA)","50051ec3":"## Autoencoder","b89199ee":"## Clustering","a014332d":"* MSRP is the manufacturer's suggested retail price (MSRP) or sticker price represents the suggested retail price of products."}}