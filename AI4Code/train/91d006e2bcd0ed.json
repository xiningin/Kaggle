{"cell_type":{"f8f4b2a7":"code","f4b1b6bf":"code","77129fa1":"code","861b17aa":"code","443ea932":"code","37ad3a9e":"code","242956d6":"code","100cc1fc":"code","706bb474":"code","8c59504d":"code","43bc1b7c":"code","20a463f8":"code","2a354be8":"code","f83ea0d5":"code","7fefb51d":"code","8d9a56d3":"code","2af98ccd":"code","1d6f1399":"code","b14dd718":"code","359b941b":"code","b104fac6":"code","0be2b685":"code","191716de":"code","732686c3":"code","6dbe354d":"code","2a995961":"code","781ac440":"code","c18d37de":"markdown"},"source":{"f8f4b2a7":"import os\n!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/\n","f4b1b6bf":"from transformers import AdamW\nfrom transformers.optimization import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","77129fa1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","861b17aa":"import gc\nimport re\nimport os\nimport sys\nimport time\nimport pickle\nimport random\nimport unidecode\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom scipy.stats import spearmanr\nfrom gensim.models import Word2Vec\nfrom flashtext import KeywordProcessor\nfrom keras.preprocessing import text, sequence\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom transformers import (\n    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n    get_cosine_schedule_with_warmup,\n)\n\nfrom math import floor, ceil","443ea932":"train = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv')","37ad3a9e":"sub = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/sample_submission.csv')","242956d6":"PUNCTS = {\n            '\u300b', '\u301e', '\u00a2', '\u2039', '\u2566', '\u2551', '\u266a', '\u00d8', '\u2569', '\\\\', '\u2605', '\uff0b', '\u00ef', '<', '?', '\uff05', '+', '\u201e', '\u03b1', '*', '\u3030', '\uff5f', '\u00b9', '\u25cf', '\u3017', ']', '\u25be', '\u25a0', '\u3019', '\u2193', '\u00b4', '\u3010', '\u1d35',\n            '\"', '\uff09', '\uff40', '\u2502', '\u00a4', '\u00b2', '\u2021', '\u00bf', '\u2013', '\u300d', '\u2554', '\u303e', '%', '\u00be', '\u2190', '\u3014', '\uff3f', '\u2019', '-', ':', '\u2027', '\uff5b', '\u03b2', '\uff08', '\u2500', '\u00e0', '\u00e2', '\uff64', '\u2022', '\uff1b', '\u2606', '\uff0f', '\u03c0',\n            '\u00e9', '\u2557', '\uff3e', '\u25aa', ',', '\u25ba', '\/', '\u301a', '\u00b6', '\u2666', '\u2122', '}', '\u2033', '\uff02', '\u300e', '\u25ac', '\u00b1', '\u00ab', '\u201c', '\u00f7', '\u00d7', '^', '!', '\u2563', '\u25b2', '\u30fb', '\u2591', '\u2032', '\u301d', '\u201b', '\u221a', ';', '\u3011', '\u25bc',\n            '.', '~', '`', '\u3002', '\u0259', '\uff3d', '\uff0c', '{', '\uff5e', '\uff01', '\u2020', '\u2018', '\ufe4f', '\u2550', '\uff63', '\u3015', '\u301c', '\uff3c', '\u2592', '\uff04', '\u2665', '\u301b', '\u2264', '\u221e', '_', '[', '\uff06', '\u2192', '\u00bb', '\uff0d', '\uff1d', '\u00a7', '\u22c5', \n            '\u2593', '&', '\u00c2', '\uff1e', '\u3003', '|', '\u00a6', '\u2014', '\u255a', '\u3016', '\u2015', '\u00b8', '\u00b3', '\u00ae', '\uff60', '\u00a8', '\u201f', '\uff0a', '\u00a3', '#', '\u00c3', \"'\", '\u2580', '\u00b7', '\uff1f', '\u3001', '\u2588', '\u201d', '\uff03', '\u2295', '=', '\u301f', '\u00bd', '\u300f',\n            '\uff3b', '$', ')', '\u03b8', '@', '\u203a', '\uff20', '\uff5d', '\u00ac', '\u2026', '\u00bc', '\uff1a', '\u00a5', '\u2764', '\u20ac', '\u2212', '\uff1c', '(', '\u3018', '\u2584', '\uff07', '>', '\u20a4', '\u20b9', '\u2205', '\u00e8', '\u303f', '\u300c', '\u00a9', '\uff62', '\u2219', '\u00b0', '\uff5c', '\u00a1', \n            '\u2191', '\u00ba', '\u00af', '\u266b', '#'\n          }\n\n\nmispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n\"haven't\" : \"have not\", \"havent\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n\"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\",\n\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \n\"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n\"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n\"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n\"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n\"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\",\n\"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n\n\ndef clean_punct(text):\n  text = str(text)\n  for punct in PUNCTS:\n    text = text.replace(punct, ' {} '.format(punct))\n  \n  return text\n","100cc1fc":"kp = KeywordProcessor(case_sensitive=True)","706bb474":"for k, v in mispell_dict.items():\n    kp.add_keyword(k, v)","8c59504d":"def preprocessing(text):\n    text = text.lower()\n    text = re.sub(r'(\\&lt)|(\\&gt)', ' ', text)\n    \n    text = re.sub(r'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' url ', text)\n    text = kp.replace_keywords(text)\n    text = clean_punct(text)\n    text = re.sub(r'\\n\\r', ' ', text)\n    text = re.sub(r'\\s{2,}', ' ', text)\n    \n    return text","43bc1b7c":"train['clean_title'] = train['question_title'].apply(lambda x : preprocessing(x))\ntrain['clean_body'] = train['question_body'].apply(lambda x : preprocessing(x))\ntrain['clean_answer'] = train['answer'].apply(lambda x : preprocessing(x))\n\ntest['clean_title'] = test['question_title'].apply(lambda x : preprocessing(x))\ntest['clean_body'] = test['question_body'].apply(lambda x : preprocessing(x))\ntest['clean_answer'] = test['answer'].apply(lambda x : preprocessing(x))","20a463f8":"y_columns = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","2a354be8":"tokenizer = BertTokenizer.from_pretrained(\"..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt\")","f83ea0d5":"def trim_input(tokenizer, title, body, answer, max_seq_length=512):\n    all_title = []\n    all_body = []\n    all_answer = []\n    for t, b, a in tqdm(zip(title, body, answer), total=len(title)):\n        \n        tokenizer_t = tokenizer.tokenize(t)\n        tokenizer_b = tokenizer.tokenize(b)\n        tokenizer_a = tokenizer.tokenize(a)\n        \n        t_len = len(tokenizer_t)\n        b_len = len(tokenizer_b)\n        a_len = len(tokenizer_a)\n        \n        t_max_len=TITLE_MAX_LEN\n        b_max_len=BODY_MAX_LEN\n        a_max_len=ANSWER_MAX_LEN\n        \n        if (t_len+b_len+a_len+4) > max_seq_length:\n        \n            if t_max_len > t_len:\n                t_new_len = t_len\n                a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n                b_max_len = b_max_len + ceil((t_max_len - t_len)\/2)\n            else:\n                t_new_len = t_max_len\n\n            if a_max_len > a_len:\n                a_new_len = a_len \n                b_new_len = b_max_len + (a_max_len - a_len)\n            elif b_max_len > b_len:\n                a_new_len = a_max_len + (b_max_len - b_len)\n                b_new_len = b_len\n            else:\n                a_new_len = a_max_len\n                b_new_len = b_max_len\n\n\n            if t_new_len+a_new_len+b_new_len+4 != max_seq_length:\n                raise ValueError(\"New sequence length should be %d, but is %d\"%(max_seq_length, (t_new_len + a_new_len + b_new_len + 4)))\n\n\n\n            tokenizer_t = tokenizer_t[:t_new_len]\n            tokenizer_b = tokenizer_b[:b_new_len]\n            tokenizer_a = tokenizer_a[:a_new_len]\n\n        all_title.append(tokenizer_t)\n        all_body.append(tokenizer_b)\n        all_answer.append(tokenizer_a)\n        \n    return all_title, all_body, all_answer","7fefb51d":"def get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n        \n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    \n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\ndef convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = get_masks(stoken, max_sequence_length)\n    input_segments = get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef convert_lines(tokenizer, title, body, answer, max_seq_length=512):\n    title, body, answer = trim_input(tokenizer, title, body, answer)\n#     max_seq_length -= 4\n    all_tokens = []\n    longer = 0\n    \n    all_tokens = []\n    input_ids, input_masks, input_segments = [], [], []\n    for i, (t, b, a) in tqdm(enumerate(zip(title, body, answer)), total=len(title)):\n        stoken = [\"[CLS]\"] + t + [\"[SEP]\"] + b + [\"[SEP]\"] + a + [\"[SEP]\"]\n        \n        ids = get_ids(stoken, tokenizer, max_seq_length)\n        masks = get_masks(stoken, max_seq_length)\n        segments = get_segments(stoken, max_seq_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]\n\nTITLE_MAX_LEN = 30\nBODY_MAX_LEN = 239\nANSWER_MAX_LEN = 239\n# title, body, answer = trim_input(tokenizer, train['clean_title'], train['clean_body'], train['clean_answer'])\n# title, body, answer = trim_input(tokenizer, test['clean_title'], test['clean_body'], test['clean_answer'])\n\nx_train = convert_lines(tokenizer, train['clean_title'], train['clean_body'], train['clean_answer'])\nx_test = convert_lines(tokenizer, test['clean_title'], test['clean_body'], test['clean_answer'])\n\n# x_train_body = convert_lines(tokenizer, train['clean_body'], BODY_MAX_LEN)\n# x_train_answer = convert_lines(tokenizer, train['clean_answer'], ANSWER_MAX_LEN) ","8d9a56d3":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","2af98ccd":"class TextDataset(torch.utils.data.TensorDataset):\n\n    def __init__(self, x_train, idxs, targets=None):\n        self.input_ids = x_train[0][idxs]\n        self.input_masks = x_train[1][idxs]\n        self.input_segments = x_train[2][idxs]\n        self.targets = targets[idxs] if targets is not None else np.zeros((x_train[0].shape[0], 30))\n\n    def __getitem__(self, idx):\n        input_ids =  self.input_ids[idx]\n        input_masks = self.input_masks[idx]\n        input_segments = self.input_segments[idx]\n\n        target = self.targets[idx]\n\n        return input_ids, input_masks, input_segments, target\n\n    def __len__(self):\n        return len(self.input_ids)","1d6f1399":"accumulation_steps = 1\n\nSEED = 2020\nNFOLDS = 3\nBATCH_SIZE = 6\nEPOCHS = 4\nLR = 3e-5\nseed_everything(SEED)","b14dd718":"test_loader = torch.utils.data.DataLoader(TextDataset(x_test, test.index),\n                          batch_size=BATCH_SIZE, shuffle=False)","359b941b":"gc.collect()","b104fac6":"bert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\nbert_config = BertConfig.from_json_file(bert_model_config)\nbert_config.num_labels = 30","0be2b685":"y = train.loc[:, y_columns].values\n\noof = np.zeros((len(train), 30))\ntest_pred = np.zeros((len(test), 30))\n\n\nkf = list(KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED).split(train))\nk = 0\nfor i, (train_idx, valid_idx) in enumerate(kf):\n    print(f'fold {i+1}')\n    gc.collect()\n    \n    train_loader = torch.utils.data.DataLoader(TextDataset(x_train, train_idx, y),\n                          batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(TextDataset(x_train, valid_idx, y),\n                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n    \n    net = BertForSequenceClassification.from_pretrained('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/', config=bert_config)\n    \n    net.cuda()\n    \n    \n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    optimizer = AdamW(net.parameters(), lr=LR, eps=4e-5)\n    \n    for epoch in range(EPOCHS):  \n        start_time = time.time()\n        avg_loss = 0.0\n        net.train()\n        for data in train_loader:\n\n            # get the inputs\n            input_ids, input_masks, input_segments, labels = data\n            pred = net(input_ids = input_ids.long().cuda(),\n                             labels = None,\n                             attention_mask = input_masks.cuda(),\n                             token_type_ids = input_segments.cuda(),\n                            )[0]\n            loss = loss_fn(pred, labels.cuda())\n            # Before the backward pass, use the optimizer object to zero all of the\n            # gradients for the Tensors it will update (which are the learnable weights\n            # of the model)\n            \n\n            # Backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            avg_loss += loss.item()\n        \n        avg_val_loss = 0.0\n        net.eval()\n\n        valid_preds = np.zeros((len(valid_idx), 30))\n        true_label = np.zeros((len(valid_idx), 30))\n        for j, data in enumerate(val_loader):\n\n            # get the inputs\n#             body, answer, title, category, host, labels = data\n#             content, labels = data\n            input_ids, input_masks, input_segments, labels = data\n\n            ## forward + backward + optimize\n            pred = net(input_ids = input_ids.long().cuda(),\n                             labels = None,\n                             attention_mask = input_masks.cuda(),\n                             token_type_ids = input_segments.cuda(),\n                            )[0]\n            loss_val = loss_fn(pred, labels.cuda())\n            avg_val_loss += loss_val.item()\n\n            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = torch.sigmoid(pred).cpu().detach().numpy()\n            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n        \n        \n        score = 0\n        for i in range(30):\n            score += np.nan_to_num(\n                    spearmanr(true_label[:, i], valid_preds[:, i]).correlation \/ 30)\n        oof[valid_idx] = valid_preds\n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t spearman={:.2f} \\t time={:.2f}s'.format(\n            epoch + 1, EPOCHS, avg_loss \/ len(train_loader), avg_val_loss \/ len(val_loader), score, elapsed_time))\n        \n    test_pred_fold = np.zeros((len(test), 30))\n    k += 0\n    torch.save(net.state_dict(), \"bert_pytorch_folds_{}.pt\".format(k))\n    \n    with torch.no_grad():\n        for q, data in enumerate(test_loader):\n            input_ids, input_masks, input_segments, labels = data\n            y_pred = net(input_ids = input_ids.long().cuda(),\n                             labels = None,\n                             attention_mask = input_masks.cuda(),\n                             token_type_ids = input_segments.cuda(),\n                            )[0]\n            test_pred_fold[q * BATCH_SIZE:(q+1) * BATCH_SIZE] = torch.sigmoid(y_pred).cpu().detach().numpy()\n        \n    torch.cuda.empty_cache()\n    gc.collect()\n    test_pred += test_pred_fold\/NFOLDS\n        \n        ","191716de":"oof_score = 0\nfor i in range(30):\n    oof_score += np.nan_to_num(\n            spearmanr(y[:, i], oof[:, i]).correlation \/ 30)","732686c3":"oof_score","6dbe354d":"torch.cuda.empty_cache()\ngc.collect()","2a995961":"sub.loc[:, y_columns] = test_pred\nsub.to_csv('submission.csv', index=False)","781ac440":"sub.head()","c18d37de":"# Implementation of pytorch BERT\n\n## add some preprocessing and use simple pytorch BERT \n\n- next step, I will try customize BERT model that I re-write BERT module.\n\n\n**BTW: I train model in offline, and upload to inference is always scoring error... :(**"}}