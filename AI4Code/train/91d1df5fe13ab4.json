{"cell_type":{"966a4cc3":"code","51b77a12":"code","20ced860":"code","4c1fb851":"code","68dbf9bd":"code","b9f0b064":"code","98ab1c65":"code","88f79c5a":"code","da4fb581":"code","6912e231":"code","50b434c6":"code","3530e469":"code","0e88863c":"code","e664c72e":"code","4fca2419":"code","83906c56":"code","3b71e5f4":"code","922ed579":"code","76e9063a":"code","61374c72":"code","267f215d":"markdown","8166699e":"markdown","3981bcfe":"markdown","16ee087e":"markdown","1a20ebc1":"markdown","076b9862":"markdown","28815441":"markdown","7db24c28":"markdown","b99ebd65":"markdown","ce831736":"markdown","dfee0c9e":"markdown","6e31157e":"markdown","ef0e65ba":"markdown"},"source":{"966a4cc3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nimport copy","51b77a12":"data = pd.read_csv('..\/input\/fish-market\/Fish.csv')","20ced860":"data.head()","4c1fb851":"data.describe()","68dbf9bd":"data.drop(data[data[\"Weight\"] == 0].index, inplace = True)\ndata.describe()","b9f0b064":"sns.countplot(x='Species',data=data)","98ab1c65":"sns.heatmap(data.corr(), annot=True, cmap='YlGnBu');","88f79c5a":"data.drop(columns=['Length2', 'Length3'], inplace = True)\ndata.head()","da4fb581":"g = sns.FacetGrid(data, col=\"Species\")\ng.map(plt.scatter, \"Length1\", \"Weight\", alpha=.7)\ng.add_legend();","6912e231":"X = pd.get_dummies(data)\nX.drop(columns=['Weight','Length1','Height','Width'], inplace=True)\n\ni = 0\n\ncolumns_L = X.columns[i:(i+7)]\ncolumns_L = [s.replace('Species', 'L') for s in columns_L]\nX.rename(columns= {X.columns[i+0]:columns_L[0], \n                   X.columns[i+1]:columns_L[1], \n                   X.columns[i+2]:columns_L[2],\n                   X.columns[i+3]:columns_L[3],\n                   X.columns[i+4]:columns_L[4],\n                   X.columns[i+5]:columns_L[5],\n                   X.columns[i+6]:columns_L[6],\n                   }, inplace=True)\n\ncolumns_H = [s.replace('L', 'H') for s in columns_L]\ncolumns_W = [s.replace('L', 'W') for s in columns_L]\n\nfor k in range(7):\n    X[columns_H[k]] = X[columns_L[k]]\n    X[columns_W[k]] = X[columns_L[k]]\n    \n    X[columns_L[k]] *= data['Length1']\n    X[columns_H[k]] *= data['Height']\n    X[columns_W[k]] *= data['Width']\nX.head()","50b434c6":"X[X > 0] = np.log(X[X > 0])\ny = data['Weight']\ny = np.log(y)\nX.head()","3530e469":"model = LinearRegression()\nmodel.fit(X,y);","0e88863c":"y_pred = model.predict(X)\ny_exp = np.exp(y)\ny_pred = np.exp(y_pred)\nprint(\"R2: \", r2_score(y_exp, y_pred))\nerror = y_exp - y_pred\n\nplt.scatter(y_pred, error);\nplt.xlabel('weight')\nplt.ylabel('error')\nplt.style.use('_classic_test_patch')","e664c72e":"error = error\/y_pred","4fca2419":"sns.distplot(error);\nplt.title('Residual Graph');","83906c56":"th = 0.07\nq_max = error.quantile(1 - th)\nq_min = error.quantile(th)\nprint(\"max\", q_max)\nprint(\"min\", q_min)\nidx = error[(error > q_max) | (error < q_min)].index\nsns.distplot(error.drop(idx));","3b71e5f4":"X_cleared = X.drop(idx)\ny_cleared = y.drop(idx)","922ed579":"model = LinearRegression()\nmodel.fit(X_cleared, y_cleared)\n\ny_pred2 = model.predict(X_cleared)\ny_exp2 = np.exp(y_cleared)\ny_pred2 = np.exp(y_pred2)\n\nr2_score(y_exp2, y_pred2)","76e9063a":"#R2 Score for not cleared data\ny_pred2 = model.predict(X)\ny_exp2 = np.exp(y)\ny_pred2 = np.exp(y_pred2)\n\nr2_score(y_exp2, y_pred2)","61374c72":"y_dif2 = y_exp2 - y_pred2\nplt.scatter(y_pred2, y_dif2);\nplt.xlabel('weight')\nplt.ylabel('error');","267f215d":"Calculating the training error","8166699e":"Parameters Length1, Length2, Length2 parameters that depend on each other. \nSelect one of the three parameters: Length1.\nIndepended parameters:\n - Length1\n - Height\n - Width\n \n Target parameter: \n - Weight\n","3981bcfe":"Author notebook https:\/\/www.kaggle.com\/abolarinbukola\/weight-prediction-r-99-3 uses a logarithmic scale, which is a great idea.\nThe author is looking for a solution in the form:\n\n$log(Weight) = A_1 * log(Length1) + A_2 * log(Height) + A_3 * log(Width) + B_0 * Pike +... + \u0421$\n\nwhere **Pike, Perch,...** take values of either 0 or 1, thus adding a constant depending on fish species. For  fish species **Bream** all parameters are equal to 0, so the constant **C** is used.\nThis linear regression is equivalent to finding a solution in the form:\n\n$Weight = Bi * \u0421 * Length1^{A_1} * Height^{A_2} * Width^{A_3}$\n\nThis is a great idea, it perfectly displays the physical meaning, as it looks for the weight of the fish through calculating the volume of the fish.\nIt is worth noting that the author calculates the coefficient of determination for weight values in the logarithmic scale, which in my opinion is not correct, before calculating R2(coefficient of determination), you need to get rid of logarithms. If you convert the values from the logarithmic scale to the normal one, the coefficient of determination decreases to 0.99\n\nThe author's result is excellent, I will try to improve it ;)\n\nIn my opinion, regression is taking into account the dependence function of weight of fish species as different fish species have their characteristic form, therefore, a function depending on the volume of different sizes of fish for each fish species.\nLet's do it!","16ee087e":"# Introduction #\n### This notebook provides an example of linear regression for estimating the weight of fish by their size. ###\n","1a20ebc1":"\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0435\u0441\u0442\u044c \u0440\u044b\u0431\u044b \u0441 \u043d\u0443\u043b\u0435\u0432\u044b\u043c \u0432\u0435\u0441\u043e\u043c. \u0443\u0434\u0430\u043b\u0438\u043c \u0441\u0442\u0440\u043e\u043a\u0438 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0448\u0438\u0435 \u043d\u0443\u043b\u0435\u0432\u043e\u0439 \u0432\u0435\u0441","076b9862":"## 3. Multiple Linear Regression ##","28815441":"The graph above shows that the higher the value of **weight**, the greater the error","7db24c28":"Preparing data for training. We will not divide the dataset into train and test. We will train on all the data, because linear regression does not have hyperparameters and does not overfitting.","b99ebd65":"## 2. Analize data and remove outlier##","ce831736":"## 4.Remove outliers and fit again ##","dfee0c9e":"## 5.Summarize ##\n### R2(coefficient of determination): 99.5% ###\n### If you find this notebook useful don't forget to upvote. #Happycoding ###","6e31157e":"Some plots show a non-linear dependence of weight on length.","ef0e65ba":"## 1.Loading data"}}