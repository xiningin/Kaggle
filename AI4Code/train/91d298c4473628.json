{"cell_type":{"3098a22e":"code","731d2285":"code","eb7eb97f":"code","835a4869":"code","e41e6bed":"code","8184a2e1":"code","954a811e":"code","78a6a633":"code","34cb4411":"code","137d542d":"code","400d7286":"code","544a85f6":"code","4407e67e":"code","27d3b8bb":"code","f2465ed4":"code","67115e8e":"code","63c7a5da":"code","7f80d37f":"code","53adb2ae":"code","bd339080":"markdown","3678152b":"markdown","53887685":"markdown","175e3175":"markdown","ff8a52cb":"markdown","a88ec213":"markdown","62cc70e6":"markdown","f63420a4":"markdown","a1c3e8e9":"markdown","0f8b0072":"markdown","13dcaf53":"markdown","cfc303fe":"markdown"},"source":{"3098a22e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport os\nprint((os.listdir('..\/input\/')))","731d2285":"import numpy as np\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error , mean_absolute_error,roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor","eb7eb97f":"import warnings\nwarnings.filterwarnings('ignore')\nimport time \n!pip install lazypredict \nimport lazypredict\nfrom lazypredict.Supervised import LazyRegressor","835a4869":"!pip install pandas --upgrade\ndf = pd.read_csv('..\/input\/wecrec2021\/train.csv',parse_dates=['F3'])\ndf_test = pd.read_csv('..\/input\/wecrec2021\/test.csv',parse_dates=['F3'])\n\ntest_index=df_test['Id']\ndf.head()","e41e6bed":"# df_test['Date'] = \"2011-01-07\"\n# df_test.Date =  pd.to_datetime(df_test.F3) - pd.to_datetime(df_test.Date) \n# df_test.Date = df_test['Date'].astype('timedelta64[D]').astype(int)\n\n# df['Date'] = \"2011-01-07\"\n# df.Date =  pd.to_datetime(df.F3) - pd.to_datetime(df.Date) \n# df.Date = df['Date'].astype('timedelta64[D]').astype(int)","8184a2e1":"cor = df.corr()\ncor ","954a811e":"plt.figure(figsize=(20,16))\nsns.heatmap(cor, annot = True , cmap = plt.cm.CMRmap_r)\nplt.show()","78a6a633":"df.drop(['Id','F3','F9','F1','F4','F10','F6'], axis = 1, inplace = True)   #anotheronepart3\ndf_test.drop(['Id','F3','F9','F1','F4','F10','F6'], axis = 1, inplace = True)\n\ndf.info()","34cb4411":"X = df.drop(['Expected'], axis = 1)\ny = df['Expected']\n\nX_train , X_val , y_train, y_val = train_test_split(X, y,test_size = 0.2)\nX_train.shape , X_val.shape , y_train.shape, y_val.shape","137d542d":"rf = RandomForestRegressor(n_estimators= 800, min_samples_split= 2\n                           , max_features= 'auto',max_depth= 25, random_state= 43 ,\n                          bootstrap= True, max_samples = 400)\n\nrf.fit(X_train, y_train)\n\nmae = mean_absolute_error(rf.predict(X_train), y_train)\nmse = mean_squared_error(rf.predict(X_train), y_train)\n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}') \n\nmae = mean_absolute_error(rf.predict(X_val), y_val)\nmse = mean_squared_error(rf.predict(X_val), y_val)\n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}') ","400d7286":"for col,imp in zip(X_train.columns ,rf.feature_importances_):\n    print(f\"{col:<8} -> {imp*(10**5)}\")","544a85f6":"scaler_for_lazy_reg = StandardScaler()\nscaler_for_lazy_reg.fit(X_train)\nX_train_scaled = scaler_for_lazy_reg.transform(X_train)\nX_val_scaled = scaler_for_lazy_reg.transform(X_val)\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(2)\nX_train_scaled = poly.fit_transform(X_train_scaled)\nX_val_scaled = poly.transform(X_val_scaled)","4407e67e":"reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\nmodels, predictions = reg.fit(X_train_scaled, X_val_scaled, y_train, y_val)\nmodels","27d3b8bb":" predictions","f2465ed4":"from sklearn.linear_model import LassoCV\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndegree= 3\npolyreg=make_pipeline(PolynomialFeatures(degree),scaler,LassoCV(normalize=True,tol=1e-4))\npolyreg.fit(X_train,y_train)\n\nfrom sklearn.metrics import mean_squared_error\n\n\nmae = mean_absolute_error(polyreg.predict(X_train), y_train)\nmse = mean_squared_error(polyreg.predict(X_train), y_train)\nrmse = mean_squared_error(y_train, polyreg.predict(X_train), squared=False) \n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}\\t\\t{rmse:.5f}') \n\nmae = mean_absolute_error(polyreg.predict(X_val), y_val)\nmse = mean_squared_error(polyreg.predict(X_val), y_val)\nrmse = mean_squared_error(y_val, polyreg.predict(X_val), squared=False) \n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}\\t\\t{rmse:.5f}') \n\npolyreg.fit(X,y)\nmae = mean_absolute_error(polyreg.predict(X), y)\nmse = mean_squared_error(polyreg.predict(X), y)\nrmse = mean_squared_error(y, polyreg.predict(X), squared=False) \n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}\\t\\t{rmse:.5f}') ","67115e8e":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\n\ndegree= 3 \npolyreg=make_pipeline(PolynomialFeatures(degree),scaler,MLPRegressor(hidden_layer_sizes = (400,300,10),random_state=43, \n                                                                     batch_size = 100 ,max_iter = 3_500 ))\npolyreg.fit(X_train,y_train)\nfrom sklearn.metrics import mean_squared_error\n\nrms = mean_squared_error(y_train, polyreg.predict(X_train), squared=False) \n\nprint(rms)\n\nmae = mean_absolute_error(polyreg.predict(X_train), y_train)\nmse = mean_squared_error(polyreg.predict(X_train), y_train)\n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}') \n\nmae = mean_absolute_error(polyreg.predict(X_val), y_val)\nmse = mean_squared_error(polyreg.predict(X_val), y_val)\n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}') \n\npolyreg.fit(X,y)\nmae = mean_absolute_error(polyreg.predict(X), y)\nmse = mean_squared_error(polyreg.predict(X), y)\n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}') ","63c7a5da":"# anotheronepart3 with df.drop(['Id','F1','F3','F4','F6','F9','F10'], axis = 1, inplace = True)\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\n\ndegree= 3 \npolyreg=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\npolyreg.fit(X_train,y_train)\nfrom sklearn.metrics import mean_squared_error\n\nrms = mean_squared_error(y_train, polyreg.predict(X_train), squared=False) \n\nprint(rms)\n\nmae = mean_absolute_error(polyreg.predict(X_train), y_train)\nmse = mean_squared_error(polyreg.predict(X_train), y_train)\n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}') \n\nmae = mean_absolute_error(polyreg.predict(X_val), y_val)\nmse = mean_squared_error(polyreg.predict(X_val), y_val)\n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}') \n\npolyreg.fit(X,y)\nmae = mean_absolute_error(polyreg.predict(X), y)\nmse = mean_squared_error(polyreg.predict(X), y)\n\nprint(f'{mse:.5f}\\t\\t{mae:.5f}') ","7f80d37f":"pred = polyreg.predict(df_test)\n\nresult=pd.DataFrame()\nresult['Id'] = test_index\nresult['Expected'] = pd.DataFrame(pred)\nresult","53adb2ae":"# !pip install pandas --upgrade       ##run this if any error with pandas due to lazypredict installation\nresult.to_csv('anotheronepart3.csv', index=False)","bd339080":"## Trying Linear Regression with polynomial features ","3678152b":"## Added new column Date as days from earliest day in data\n- Turn's out its not any good ","53887685":"## We can see linear regression with polynomial features does well","175e3175":"## Trying LassoCV","ff8a52cb":"### Dropping Columns which aren't important","a88ec213":"### Train Test Split for validation ","62cc70e6":"## Importing all required libraries ","f63420a4":"## Importing lazypredict to check all models ","a1c3e8e9":"## Visualizing input data","0f8b0072":"## Using Standard Scaler before passing it into Lazypredict to get an idea of which model works the best ","13dcaf53":"## Trying MLP model with polynomial features","cfc303fe":"## Used this to check important features using the baseline model"}}