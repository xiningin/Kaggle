{"cell_type":{"319433cf":"code","507dd78a":"code","a3d50562":"code","deefe1a0":"code","ce925b48":"code","7f562d6e":"code","be465019":"code","9c36debe":"code","95a85dd0":"code","0339394e":"code","29b2e9ef":"code","4f8ff75b":"code","b07d5721":"code","6279a414":"code","ef737397":"code","486de7e4":"markdown"},"source":{"319433cf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm as tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import confusion_matrix\nimport json\nfrom pandas.io.json import json_normalize\n#from keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, LSTM, Dense,concatenate,TimeDistributed,SimpleRNN,GRU\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping \nfrom keras import regularizers\nimport keras.backend as K\nimport tensorflow as tf\nfrom numba import jit\nimport random\nimport collections\nimport gc\nimport copy\nimport sys\n\ndir='\/kaggle\/input\/data-science-bowl-2019\/'\n#dir='.\/'\nnbatch, nstep, nreset, maxlen, nfold = 1500, 70, 3, 300, 6\na,b,c=-1,-2,-4  # coefficients of the loss matrix; power of 2 \nrandom.seed(39)","507dd78a":"#quadratic_kappa\n@jit\ndef quadratic_kappa(actuals, preds, sample_weight, N=4):\n    #if len(sample_weight)==1:\n    #    sample_weight=np.ones(len(actuals),dtype='int32')\n    w, MO = np.zeros((N,N)), np.zeros((N,N))\n    for n in range(len(actuals)):\n        MO[actuals[n]][preds[n]]+=sample_weight[n]\n    for i in range(len(w)): \n        for j in range(len(w)):\n            w[i][j] = float(((i-j)**2))\n    act_hist, pred_hist = np.zeros(N), np.zeros(N)\n    for n in range(len(actuals)):\n        act_hist[actuals[n]]+=sample_weight[n]\n        pred_hist[preds[n]]+=sample_weight[n]   \n    ME = np.outer(act_hist, pred_hist);\n    ME = ME\/ME.sum()\n    MO = MO\/MO.sum()\n    \n    num, den = 0, 0\n    for i in range(len(w)):\n        for j in range(len(w)):\n            num+=w[i][j]*MO[i][j]\n            den+=w[i][j]*ME[i][j]\n    return (1 - (num\/den))","a3d50562":"#train = pd.read_csv(dir+\"train.csv\",nrows=3000000)\ntrain = pd.read_csv(dir+\"train.csv\")\ntrain_labels = pd.read_csv(dir+\"train_labels.csv\")\n#specs = pd.read_csv(\"specs.csv\")\nsample_submission = pd.read_csv(dir+\"sample_submission.csv\")\ndel train['timestamp']\ndel train['type']\ndel train['world']","deefe1a0":"#train.shape\nids = list(train_labels['installation_id'].unique())\ntrain=train[train['installation_id'].isin(ids)]\nlist_title = train['title'].unique()\nlist_event_code = train['event_code'].unique()\nlist_event_code.sort()\nlist_atitle = train_labels['title'].unique()","ce925b48":"def transform_df(train,nsplit=1):\n    split = np.array_split(np.arange(len(train)), nsplit)\n    for i in tqdm(range(len(split))):\n        tmp1 = train['event_data'].iloc[split[i]].apply(json.loads).values\n        for j in range(len(tmp1)):\n            tmp2={}\n            #if 'description' in tmp[j].keys():\n            #    tmp2['description'] = tmp[j]['description']\n            if 'round' in tmp1[j].keys():\n                tmp2['round'] = tmp1[j]['round']\n            if 'level' in tmp1[j].keys():\n                tmp2['level'] = tmp1[j]['level']\n            if 'correct' in tmp1[j].keys():\n                tmp2['correct'] = tmp1[j]['correct']\n            if 'misses' in tmp1[j].keys():\n                tmp2['misses'] = tmp1[j]['misses']\n            tmp1[j]=tmp2\n        if i ==0:\n            tmp=tmp1[0:]\n        else:\n            tmp=np.concatenate([tmp,tmp1])\n\n    tmp=json_normalize(tmp)\n    tmp.index=train.index\n    train = pd.merge(train, tmp,how='inner',left_index=True, right_index=True)\n    del tmp\n\n    train['event_code_label']=le_event_code.transform(train['event_code'])\n    del train['event_code']\n    train['title_label']=le_title.transform(train['title'])\n    \n    return train\n\nle_title = LabelEncoder()\nle_title.fit(list_title)\nle_atitle = LabelEncoder()\nle_atitle.fit(list_atitle)\nle_event_code = LabelEncoder()\nle_event_code.fit(list_event_code)\ntrain=transform_df(train,nsplit=10)","7f562d6e":"def make_set(train,fortest=False):\n    label_sessions = list(train_labels['game_session'].unique())\n    X,A,Y,T,ID=[],[],[],[],[]\n    for i, iid in tqdm(train.groupby('installation_id', sort=False)):\n        Xa=np.zeros((iid['game_session'].nunique(),len(list_title)+len(list_event_code)+8), dtype=np.float32)\n        total_n, total_t, total_r, total_l, total_s, total_f, total_m = 0.00001, 0, 0, 0, 0, 0, 0 \n        for n,(j, session) in enumerate(iid.groupby('game_session', sort=False)):\n            tmp=session['event_code_label'].value_counts(sort=False)\n            Xa[n,session['title_label'].iloc[0]]=1\n            Xa[n,tmp.index+len(list_title)]=tmp\/10\n            Xa[n,-1]=len(session)\/100\n            Xa[n,-2]=session['game_time'].iloc[-1]\/100000\n            Xa[n,-3]=sum(session[session['round'] != np.nan]['round'])\/100\n            if np.isnan(Xa[n,-3]):\n                Xa[n,-3]=0\n            Xa[n,-4]=sum(session[session['level'] != np.nan]['level'])\/100\n            if np.isnan(Xa[n,-4]):\n                Xa[n,-4]=0\n            Xa[n,-5]=max(session[session['level'] != np.nan]['level'])\/10\n            if np.isnan(Xa[n,-5]):\n                Xa[n,-5]=0\n            Xa[n,-6]=sum(session['correct']==True)\n            Xa[n,-7]=sum(session['correct']==False)\n            Xa[n,-8]=sum(session['misses']==True)\/10\n            #print(Xa[n,-8:])\n            if (fortest==False) and (j in label_sessions):\n                Atmp=[le_atitle.transform([session['title'].values[0]])[0],total_n\/100,total_t\/total_n\n                      , total_r\/total_n, total_l\/total_n, total_s\/total_n, total_f\/total_n, total_m\/total_n]\n                A.append(Atmp)\n                X.append(Xa[0:n].copy())\n                T.append(n)\n                ID.append(i)\n                Y.append(train_labels[train_labels['game_session']==j]['accuracy_group'].values[0])\n                label_sessions.remove(j)\n            if (fortest) and (n == iid['game_session'].nunique()-1):\n                Atmp=[le_atitle.transform([session['title'].values[0]])[0],total_n\/100,total_t\/total_n\n                      , total_r\/total_n, total_l\/total_n, total_s\/total_n, total_f\/total_n, total_m\/total_n]\n                A.append(Atmp)\n                X.append(Xa[0:n].copy())\n                T.append(n)\n                ID.append(i)\n            #total_n += 1\n            #total_t += Xa[n,-2]\n            #total_r += Xa[n,-3]\n            #total_l += Xa[n,-4]\n            #total_s += Xa[n,-5]\n            #total_f += Xa[n,-6]\n            #total_m += Xa[n,-7]\n    return X,A,Y,T,ID\n\nX,A,Y,T,ID=make_set(train)\ndel train\n#gc.collect()\n#print(len(X),len(A),len(Y),len(T))","be465019":"print(len(X),len(A[0]),len(Y),len(T),len(ID))","9c36debe":"#for padding of the time-series\ndef padding(X,A,Y=[],maxlen=maxlen):\n    X_train = pad_sequences(X, maxlen=maxlen, dtype='float32', padding='pre', truncating='post', value=0.0)\n    A_train = np.zeros((len(X),5), dtype=np.float32)\n    #A_train = np.zeros((len(X),len(A[0])+4), dtype=np.float32)\n    for i in range(len(X)):\n        A_train[i,A[i][0]] = 1\n        #A_train[i,5:] = A[i][1:]\n    if len(Y)>0:\n        Y_train = np.zeros((len(Y),4), dtype=np.int32)\n        for i in range(len(Y)):\n            Y_train[i,Y[i]] = 1\n        return X_train,A_train,Y_train\n    else:\n        return X_train,A_train        \n\nX_train,A_train,Y_train = padding(X,A,Y)\ndel X\ngc.collect()\nprint(X_train.shape,A_train.shape,Y_train.shape)","95a85dd0":"def make_network():\n    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n    atitle = Input(shape=(5,), name='atitle')\n    x=inputs\n    x = TimeDistributed(Dense(36,activation='tanh',kernel_regularizer=regularizers.l1(0.00001)))(x)\n    x = TimeDistributed(Dense(18,activation='tanh',kernel_regularizer=regularizers.l1(0.0000)))(x)\n    x = TimeDistributed(Dense(10,activation='tanh'))(x)\n    #x = TimeDistributed(Dense(10,activation='relu'))(x)\n    x = GRU(10,return_sequences=False)(x)\n    y = concatenate([x, atitle])\n    y = Dense(15,activation='tanh')(y)\n    y = Dense(10,activation='tanh')(y)\n    y = Dense(10,activation='tanh')(y)\n    #y = Dense(10,activation='relu')(y)\n    y = Dense(4,activation='softmax')(y)\n    model = Model(inputs=[inputs,atitle], outputs=y)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n    #model.summary()\n    return model","0339394e":"le_id = LabelEncoder()\nle_id.fit(ID)\nnID=le_id.transform(ID)\nnlabel=np.empty(len(ID))\nfor i in range(len(ID)):\n    nlabel[i]=sum(e==ID[i] for e in ID)\nid_sample_all=np.array(random.sample(range(len(set(ID))),len(set(ID))))\nW_train=1\/nlabel\n\nlossmat=np.array([[0,a,b,c],[a,0,a,b],[b,a,0,a],[c,b,a,0]])\nmodels = []\nfor ii in range(nfold):\n    i=ii\n    log, logs = np.zeros((nstep,8)), []    \n    id_sample = np.array_split(id_sample_all, nfold)[i]\n    test_sample = [e for e in range(len(ID)) if nID[e] in id_sample]\n    train_sample = [e for e in range(len(Y_train)) if e not in test_sample]\n    X_train1, A_train1, Y_train1, W_train1 = X_train[train_sample],A_train[train_sample],Y_train[train_sample], W_train[train_sample]\n    X_train2, A_train2, Y_train2, W_train2 = X_train[test_sample],A_train[test_sample],Y_train[test_sample], W_train[test_sample]\n    print(i,len(id_sample),len(set(test_sample)),len(set(train_sample)))\n    maxacc=-1\n    log2 = log.copy()\n    for j in range(nreset):\n        model = make_network()\n        for s in tqdm(range(nstep)):\n            history = model.fit([X_train1,A_train1], Y_train1, sample_weight=W_train1, batch_size=nbatch, \n                                epochs=1,shuffle=True,validation_data=([X_train2,A_train2],Y_train2),verbose=0)\n            Yp=(np.matmul(model.predict([X_train1,A_train1],batch_size=2000),lossmat)).argmax(axis=1).astype(np.int32)\n            log2[s,4] = quadratic_kappa(Y_train1.argmax(axis=1).astype(np.int32), Yp, sample_weight=W_train1)  \n            log2[s,6] = quadratic_kappa(Y_train1.argmax(axis=1).astype(np.int32), Yp, sample_weight=np.ones(len(Y_train1)))  \n            Yp=(np.matmul(model.predict([X_train2,A_train2],batch_size=2000),lossmat)).argmax(axis=1).astype(np.int32)\n            log2[s,5] = quadratic_kappa(Y_train2.argmax(axis=1).astype(np.int32), Yp, sample_weight=W_train2)  \n            log2[s,7] = quadratic_kappa(Y_train2.argmax(axis=1).astype(np.int32), Yp, sample_weight=np.ones(len(Y_train2)))  \n            log2[s,0], log2[s,1]= history.history['loss'][0], history.history['val_loss'][0]\n            log2[s,2], log2[s,3]= history.history['categorical_accuracy'][0], history.history['val_categorical_accuracy'][0]\n            #print(s,log2[s])\n            if -log2[s,1]+log2[s,3]+log2[s,4]+3*log2[s,5]>maxacc and s>20:\n                model0=copy.deepcopy(model)\n                maxacc=-log2[s,1]+log2[s,3]+log2[s,4]+3*log2[s,5]\n                #print(maxacc)\n        if j==0:\n            log=log2.copy()\n        else:\n            log=np.concatenate([log,log2])\n            \n    models.append(copy.deepcopy(model0))\n    print(i,'fold')\n    plt.figure(figsize=(10,5))\n    plt.plot(log[:,4])\n    plt.plot(log[:,5])\n    plt.ylim([0.50,0.60])\n    plt.xlabel('epoch')\n    plt.ylabel('kappa')\n    plt.show()","29b2e9ef":"def predicts(models,X,A):\n    Y=np.zeros((len(X),4))\n    for i in range(len(models)):\n        Y+=models[i].predict([X,A],batch_size=2000)\n    Y=Y\/len(models)\n    return Y","4f8ff75b":"Yp=predicts(models,X_train,A_train)\nprint(quadratic_kappa(Y_train.argmax(axis=1).astype(np.int32), Yp.argmax(axis=1).astype(np.int32),sample_weight=W_train))\nplt.hist([Y_train.argmax(axis=1),(Yp).argmax(axis=1)])#, stacked=True)\nplt.show()","b07d5721":"tmp=np.matmul(Yp,lossmat).argmax(axis=1).astype(np.int32)\nprint(quadratic_kappa(Y_train.argmax(axis=1).astype(np.int32), tmp ,sample_weight=W_train))\nplt.hist([np.array(Y_train.argmax(axis=1).astype(np.int32),dtype=np.int32),tmp])\nplt.show()","6279a414":"del X_train\ntest = pd.read_csv(dir+\"test.csv\")\ndel test['timestamp']\ndel test['type']\ndel test['world']\ntest=transform_df(test,nsplit=10)\nXt,At,Yt,Tt,IDt=make_set(test,fortest=True)\ndel test\ngc.collect()\nX_test,A_test = padding(Xt,At)\ndel Xt\ngc.collect()\n#y_pred = model0.predict([X_test,A_test],batch_size=2000)\ny_pred = predicts(models,X_test,A_test)\nysub=np.matmul(y_pred,lossmat).argmax(axis=1).astype(np.int32)\nsample_submission['accuracy_group'] = ysub\nsample_submission.to_csv('submission.csv', index=False)\nprint(sample_submission['accuracy_group'].value_counts(normalize=True))","ef737397":"#import pickle\n#with open('models.pickle', 'wb') as f:\n#    pickle.dump(models, f)   \n#with open('models.pickle', 'rb') as f:\n#    models = pickle.load(f)","486de7e4":"## LSTM : Learning from time-series of sessions \n\nNetwork in this notebook learns the conditional \nprobability, \n\nP(C | X, A)  .\n\nC:0,1,2,or 3  \nX=(x_0,x_1,x_2,...):time series of sessions  \n  for each session x_n includes   \n  title, event_code, durationtime, level, round, correct, missites, etc...  \nA:title of Assesment,0,1,2,3 or 4  \n\nThe predicted label, Cp, is decided by  \n  Cp = argmax_i { SUM_k M_ik P(C_i | X, A) },  \nwhere M_ik represents the loss matrix.\nSee the first code cell for the coefficients.  \n\nmodeifications:  \n\n(1)  I added weight to training samples for the loss function of the network and  \n    Cohen kappa coeffisient to adjust the training set to the test set, which is randamly truncated.  \n(2) I divided the set of \"instration_id\" and used the k-fold cross validation. \n"}}