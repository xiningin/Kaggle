{"cell_type":{"6e47b0c7":"code","b27e7148":"code","96d86609":"code","26b514ec":"code","9133e5ce":"code","eddbb9bf":"code","e0e87113":"code","b7873166":"code","5dfd1d6e":"code","813782f9":"code","de59245a":"code","d8cf2dcb":"code","db1b39cc":"code","4720c3c8":"code","3255b60b":"code","95e8eb13":"code","2b258e7a":"code","8063c318":"code","0cff76a0":"code","fdc1e44e":"code","8efaaf0a":"code","af9a376b":"code","8ef733b1":"code","19602245":"code","46b45528":"code","13e47a2c":"code","6b41c4b7":"code","7925c7eb":"code","24f5cb13":"code","1c80db7a":"code","290443cb":"code","9a44c8c6":"markdown","5274c9b8":"markdown","e6a48214":"markdown","02745c26":"markdown","09bea1d4":"markdown","e18688f0":"markdown","d50b22ca":"markdown","d458fe76":"markdown","8e0185b1":"markdown","2367d887":"markdown","d2f7af5c":"markdown","871aceda":"markdown","67c9ea3a":"markdown"},"source":{"6e47b0c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b27e7148":"data = pd.read_csv('\/kaggle\/input\/house-price-prediction-challenge\/train.csv')\ndata.head()","96d86609":"data.info()","26b514ec":"# extract city from address\ndata['ADDRESS'] = data['ADDRESS'].str.split(',').apply(lambda x: x[-1])\ndata['ADDRESS'].value_counts(ascending=False).head(25)","9133e5ce":"def map_city(city):\n    if city in ['Ahmedabad', 'Bangalore', 'Chennai', 'Delhi', 'Hyderabad', 'Kolkata', 'Mumbai', 'Pune', 'Maharashtra']:\n        return 'tier1'\n    elif city in ['Agra', 'Ajmer', 'Aligarh', 'Amravati', 'Amritsar', 'Asansol', 'Aurangabad', 'Bareilly', \n                  'Belgaum', 'Bhavnagar', 'Bhiwandi', 'Bhopal', 'Bhubaneswar', 'Bikaner', 'Bilaspur', 'Bokaro Steel City', \n                  'Chandigarh', 'Coimbatore', 'Cuttack', 'Dehradun', 'Dhanbad', 'Bhilai', 'Durgapur', 'Dindigul', 'Erode', \n                  'Faridabad', 'Firozabad', 'Ghaziabad', 'Gorakhpur', 'Gulbarga', 'Guntur', 'Gwalior', 'Gurgaon', 'Guwahati', \n                  'Hamirpur', 'Hubli\u2013Dharwad', 'Indore', 'Jabalpur', 'Jaipur', 'Jalandhar', 'Jammu', 'Jamnagar', 'Jamshedpur', \n                  'Jhansi', 'Jodhpur', 'Kakinada', 'Kannur', 'Kanpur', 'Karnal', 'Kochi', 'Kolhapur', 'Kollam', 'Kozhikode', \n                  'Kurnool', 'Ludhiana', 'Lucknow', 'Madurai', 'Malappuram', 'Mathura', 'Mangalore', 'Meerut', 'Moradabad', \n                  'Mysore', 'Nagpur', 'Nanded', 'Nashik', 'Nellore', 'Noida', 'Patna', 'Pondicherry', 'Purulia', 'Prayagraj', \n                  'Raipur', 'Rajkot', 'Rajahmundry', 'Ranchi', 'Rourkela', 'Ratlam', 'Salem', 'Sangli', 'Shimla', 'Siliguri', \n                  'Solapur', 'Srinagar', 'Surat', 'Thanjavur', 'Thiruvananthapuram', 'Thrissur', 'Tiruchirappalli', 'Tirunelveli', \n                  'Tiruvannamalai', 'Ujjain', 'Bijapur', 'Vadodara', 'Varanasi', 'Vasai-Virar City', 'Vijayawada', 'Visakhapatnam', \n                  'Vellore', 'Warangal']:\n        return 'tier2'\n    else:\n        return 'tier3'\n    \ndata['city_tier'] = data['ADDRESS'].apply(map_city)","eddbb9bf":"# check target value distribution\ndata['PRICE_IN_LACS'] = np.log(data['TARGET(PRICE_IN_LACS)'])\nsns.histplot(data['PRICE_IN_LACS'], bins=20)","e0e87113":"# taking log of square_ft as well\ndata['area'] = np.log(data['SQUARE_FT'])\nsns.histplot(data['area'], bins=20)","b7873166":"def plot_numerical(feature):\n    sns.lmplot(x=feature, y='TARGET(PRICE_IN_LACS)', data=data)\n    plt.show()\n    \ndef plot_categorical(feature, figsize=None):\n    df = data.groupby([feature])['TARGET(PRICE_IN_LACS)'].describe()[['mean', '50%', 'min', 'count']]\n\n    labels = df.index.values\n    x = np.arange(len(labels))\n    width = 0.9\n    fig, ax1 = plt.subplots(figsize=(8, 5))\n\n    # plot bars for min, median and mean house price\n    rects1 = ax1.bar(x-width\/2, df['50%'], width\/3, label='median')\n    rects2 = ax1.bar(x-width\/6, df['mean'], width\/3, label='mean')\n    rects3 = ax1.bar(x+width\/6, df['min'], width\/3, label='min')\n\n    ax1.set_ylabel('PRICE_IN_LACS', fontsize=12)\n    ax1.set_title(feature, fontsize=15)\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(labels, rotation=0)\n    ax1.legend()\n\n    # plot counts of data points\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Counts', fontsize=12)\n    ax2.plot(x-width\/2, df['count'], color='red', linestyle='dashed')\n\n    # annotate counts of data points\n    for i, rect in enumerate(rects2):\n        height = int(round(rect.get_height()))\n        ax1.annotate('{}'.format(int(df['count'].iloc[i])),\n                     xy=(rect.get_x() + rect.get_width()\/2, height),\n                     xytext=(0, 3), textcoords=\"offset points\",\n                     ha='center', va='bottom', color='red')\n    plt.show()","5dfd1d6e":"for feature in ['area', 'LONGITUDE', 'LATITUDE']:\n    plot_numerical(feature)","813782f9":"for feature in ['POSTED_BY', 'city_tier']:\n    plot_categorical(feature)","de59245a":"for feature in ['UNDER_CONSTRUCTION', 'RERA', 'BHK_OR_RK', 'READY_TO_MOVE', 'RESALE', 'BHK_NO.']:\n    plot_categorical(feature)","d8cf2dcb":"df = data.groupby(['ADDRESS'])['TARGET(PRICE_IN_LACS)'].describe()[['mean', 'count']]\ndf = df[df['count']>20]\n\nlabels = df.index.values\nx = np.arange(len(labels))\nfig, ax1 = plt.subplots(figsize=(20, 5))\nwidth = 0.9\n\n# plot bars for min, median and mean house price\nrects = ax1.bar(x, df['mean'], width, label='mean')\n\nax1.set_ylabel('PRICE_IN_LACS', fontsize=12)\nax1.set_title('ADDRESS', fontsize=15)\nax1.set_xticks(x)\nax1.set_xticklabels(labels, rotation=90)\nax1.legend()\n\n# plot counts of data points\nax2 = ax1.twinx()\nax2.set_ylabel('Counts', fontsize=12)\nax2.plot(x, df['count'], color='red', linestyle='dashed')\n\n# annotate counts of data points\nfor i, rect in enumerate(rects):\n    height = int(round(rect.get_height()))\n    ax1.annotate('{}'.format(int(df['count'].iloc[i])),\n                 xy=(rect.get_x() + rect.get_width()\/2, height),\n                 xytext=(0, 3), textcoords=\"offset points\",\n                 ha='center', va='bottom', color='red')\nplt.show()","db1b39cc":"categorical_features = ['POSTED_BY', 'BHK_OR_RK', 'city_tier', 'ADDRESS']\nnumerical_features = ['UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'SQUARE_FT', 'READY_TO_MOVE', 'RESALE', 'LONGITUDE', 'LATITUDE', 'area']","4720c3c8":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport joblib","3255b60b":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    le = LabelEncoder()\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    #print(feature)\n    \n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))","95e8eb13":"# Bivariate Analysis Correlation plot for numerical features\nplt.figure(figsize=(10, 5))\nsns.heatmap(round(data[numerical_features].corr(method='spearman'), 2), \n            annot=True, mask=None, cmap='GnBu')\nplt.show()","2b258e7a":"# Bivariate Analysis Correlation plot with the Categorical variables\nplt.figure(figsize=(10, 10))\nsns.heatmap(round(df[categorical_features+numerical_features+['TARGET(PRICE_IN_LACS)']].corr(method='spearman'), 2), annot=True,\n            mask=None, cmap='GnBu')\nplt.show()","8063c318":"from statsmodels.stats.outliers_influence import variance_inflation_factor","0cff76a0":"# Calculating VIF\nvif = pd.DataFrame()\nvif[\"variables\"] = [feature for feature in categorical_features+numerical_features if feature not in ['READY_TO_MOVE', 'area', \n                                                                                                      'RESALE', 'LATITUDE', 'LONGITUDE']]\nvif[\"VIF\"] = [variance_inflation_factor(df[vif['variables']].values, i) for i in range(len(vif[\"variables\"]))]\nprint(vif)","fdc1e44e":"NumericData = data[['BHK_NO.', 'TARGET(PRICE_IN_LACS)', 'SQUARE_FT']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","8efaaf0a":"# Percentage of outliers present in each variable\noutlier_percentage = {}\nfor feature in ['area', 'SQUARE_FT', 'BHK_NO.', 'TARGET(PRICE_IN_LACS)', 'PRICE_IN_LACS']:\n    tempData = data.sort_values(by=feature)[feature]\n    Q1, Q3 = tempData.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    Lower_range = Q1 - (1.5 * IQR)\n    Upper_range = Q3 + (1.5 * IQR)\n    outlier_percentage[feature] = round((((tempData<(Q1 - 1.5 * IQR)) | (tempData>(Q3 + 1.5 * IQR))).sum()\/tempData.shape[0])*100,2)\noutlier_percentage","af9a376b":"NumericData = data[['PRICE_IN_LACS', 'area']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","8ef733b1":"# Outlier treatment by removal or replacement\ndf_outlier = data.copy()\nfor feature in ['area']:\n    tempData = data.sort_values(by=feature)[feature]\n    Q1, Q3 = tempData.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    Lower_range = Q1 - (1.5 * IQR)\n    Upper_range = Q3 + (1.5 * IQR)    \n    # by replacing values\n    #df_outlier.loc[data[feature]>(Upper_range), feature] = Upper_range\n    #df_outlier.loc[data[feature]<(Lower_range), feature] = Lower_range\n    \n    # by dropping rows\n    df_outlier = data[(data[feature]>Lower_range) & (data[feature]<Upper_range)].reset_index(drop=True)","19602245":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    \n    le = LabelEncoder()\n    ohe = OneHotEncoder(sparse=False)\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))\n    # load classes\n    columns = joblib.load(\n        open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'rb'))\n\n    if len(le.classes_)>2 and feature!='ADDRESS':\n        # perform one hot encoding\n        ohe.fit(df[[feature]])\n        # save the encoder\n        joblib.dump(ohe, \n                    open(os.path.join(path, \"TextEncoding\/ohe_{}.sav\".format(feature)), 'wb'))\n\n        # transfrom training data\n        # removing first column of encoded data to elude from dummy variable trap\n        tempData = ohe.transform(df[[feature]])[:, 1:]\n\n        # create Dataframe with columns as classes\n        tempData = pd.DataFrame(tempData, columns=columns)\n    else:\n        tempData = df[[feature]]\n    \n    # create dataframe with all the label encoded categorical features along with hot encoding\n    if i==0:\n        encodedData = pd.DataFrame(data=tempData, columns=tempData.columns.values.tolist())\n    else:\n        encodedData = pd.concat([encodedData, tempData], axis=1)","46b45528":"# merge numerical features and categorical encoded features\ndf = df[numerical_features+['TARGET(PRICE_IN_LACS)', 'PRICE_IN_LACS']]\ndf = pd.concat([df, encodedData], axis=1)\ndf.info()","13e47a2c":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics, preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score","6b41c4b7":"train_data = df.copy()\nfeature_cols = [feature for feature in train_data.columns if feature not in(['READY_TO_MOVE', 'ADDRESS', 'TARGET(PRICE_IN_LACS)', \n                                                                             'SQUARE_FT', 'PRICE_IN_LACS'])]\nprint('features used: ', feature_cols)\n\n# RESCALING\n#scaler = MinMaxScaler()\n#scaler.fit(train_data[feature_cols])\n#train_data[feature_cols] = scaler.transform(train_data[feature_cols])","7925c7eb":"X = train_data[feature_cols]\ny = train_data['PRICE_IN_LACS']\n\nvalidation_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_size, random_state=0, \n                                                    stratify=X[['RESALE', 'UNDER_CONSTRUCTION', 'RERA']])\n\ny1 = train_data['TARGET(PRICE_IN_LACS)']\nvalidation_size = 0.2\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y1, test_size=validation_size, random_state=0, \n                                                    stratify=X[['RESALE', 'UNDER_CONSTRUCTION', 'RERA']])","24f5cb13":"model = XGBRegressor( \n    n_estimators = 500,\n    learning_rate=0.02, \n    #min_child_weight=3,\n    #max_depth = 3,\n    #subsample = 0.8,\n    seed=7)\n\n\nmodel = model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    #early_stopping_rounds=20,\n    #eval_set=[(X_test, y_test)],\n    verbose=False)","1c80db7a":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\n# rmse on actual scaled values\nprint('RMSE Actual: ', np.sqrt(mean_squared_error(y_train1, np.exp(y_pred))))\nprint('RMSE Scaled Data: ', np.sqrt(mean_squared_error(y_train, y_pred)))\nprint('r2_score: ', round(r2_score(y_train1, np.exp(y_pred))*100, 2))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\n# rmse on actual scaled values\nprint('RMSE Actual: ', np.sqrt(mean_squared_error(y_test1,  np.exp(y_pred))))\nprint('RMSE Scaled Data: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('r2_score: ', round(r2_score(y_test1, np.exp(y_pred))*100, 2))","290443cb":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_pred[-150:]))), y=np.exp(y_pred[-150:]),\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test[-150:]))), y=y_test1[-150:],\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","9a44c8c6":"# Handling Categorical Features (Label Encoding & One Hot Encoding)\u00b6","5274c9b8":"# Label encoding categorical features for correlation","e6a48214":"# EDA","02745c26":"# Model: XGB","09bea1d4":"# Training Model","e18688f0":"|Column|\tDescription|\n|---|---|\n|POSTED_BY|\tCategory marking who has listed the property|\n|UNDER_CONSTRUCTION|\tUnder Construction or Not|\n|RERA|\tRera approved or Not|\n|BHK_NO|\tNumber of Rooms|\n|BHKORRK|\tType of property|\n|SQUARE_FT|\tTotal area of the house in square feet|\n|READYTOMOVE|\tCategory marking Ready to move or Not|\n|RESALE|\tCategory marking Resale or not|\n|ADDRESS|\tAddress of the property|\n|LONGITUDE|\tLongitude of the property|\n|LATITUDE|\tLatitude of the property|","d50b22ca":"map all cities into tier1, tier2 and tier3 based on https:\/\/en.wikipedia.org\/wiki\/Classification_of_Indian_cities","d458fe76":"**Columns that are contributing towards high house prices-**\n* Area - +ve\n* BHK - +ve with some exceptions\n* UNDER_CONSTRUCTION - yes\n* RERA - yes\n* BHK_OR_RK - BHK\n* READY_TO_MOVE - no\n* RESALE - no\n* ADDRESS - Bangalore, Gurgaon, Mumbai, Mohali, Chennai, Goa, Maharashtra, Ranchi, Pune, Noida, Meerut\n* POSTED_BY - Builder has generally high while Owner has the least\n* city_tier - cities in 'tier1' have highest house prices","8e0185b1":"# CORRELATION","2367d887":"**Observations-**\n* UNDERCONSTRUCTION - READY_TO_MOVE - are exactly the same\n* BHK_NO. - SQUARE_FT","d2f7af5c":"# Looking at Outlier","871aceda":"**Observations-**\n* Taking log of *SQUARE_FT* reduces the outlier values from 6% to 4% in *area* column. We will remove rest of the outlier by dropping those rows\n* Taking log of *TARGET(PRICE_IN_LACS)* reduces the outlier from 10% to 3% in *PRICE_IN_LACS* column","67c9ea3a":"# Removing features using VIF"}}