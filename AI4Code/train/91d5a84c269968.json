{"cell_type":{"2c0a11b3":"code","ee860bc3":"code","01e38452":"code","1981de52":"code","afa94696":"code","ff33e54b":"code","6cfc6f7c":"code","e5b93b04":"code","11ae13a4":"code","d82ca915":"code","74aefcfc":"code","14707a0d":"code","ef50005e":"code","7cd6f9db":"code","39a6fa83":"code","3663742f":"code","a3b7bce3":"code","604d0eca":"code","6486f4b1":"code","9e26ce5e":"code","c3459a3a":"code","e8631934":"code","5b01e913":"code","d978df30":"code","9062ee29":"code","0bf56681":"code","db1d05a2":"code","77c3ba39":"code","2beaa033":"code","c798f90b":"code","fb709890":"code","633e06e8":"code","287d2ac5":"code","bf5ff53a":"code","aaace777":"code","a7150ee5":"code","86025516":"code","e818cee2":"code","d076986d":"code","564b03b0":"code","888f9785":"code","87a26c72":"code","1044481e":"code","28b2d599":"code","c5849c8d":"code","5c9a7666":"code","2930c8fe":"code","8de1543f":"code","a2c4de9a":"code","5c3f7e9e":"code","0b36f179":"code","cdd3b6c1":"code","479b2e31":"code","9e7c7e25":"code","729fd4ad":"code","59b5d360":"code","bab3bac4":"code","3fd95728":"code","e957cf98":"code","e2ef44f6":"code","e8863303":"markdown","73f86c97":"markdown","f8610a31":"markdown","3dc32e06":"markdown","f6acd4a9":"markdown","ef5c59b5":"markdown","daf9a81e":"markdown","0939206f":"markdown","9bf937d2":"markdown","90168b20":"markdown","6e476ac6":"markdown","db1db62d":"markdown","8edb57fc":"markdown","aeacfdf7":"markdown","f3731477":"markdown","186c0e10":"markdown","170882b2":"markdown","5cb7a703":"markdown","e0078861":"markdown","7362d4eb":"markdown","2e16552b":"markdown","723d7f3a":"markdown","ad760749":"markdown"},"source":{"2c0a11b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ee860bc3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import required library","01e38452":"#reading the dataset\n\ndf = pd.read_csv('..\/input\/heart-disease-dataset-uci\/HeartDiseaseTrain-Test.csv',index_col=False)\ndf","1981de52":"df.info()","afa94696":"##pairplot for the numerical variables\nsns.pairplot(data=df,hue='target')\nplt.show()","ff33e54b":"num = ['age','resting_blood_pressure' ,'cholestoral','Max_heart_rate','oldpeak']\n\nfig,ax=plt.subplots(1,5,figsize=(20,15))\nax=ax.ravel()\n\nfor index, col in enumerate(num):\n    sns.boxplot(x='target',y=col,data=df, ax=ax[index])","6cfc6f7c":"df.info()","e5b93b04":"cat = ['sex','chest_pain_type','fasting_blood_sugar','rest_ecg', 'exercise_induced_angina','slope','vessels_colored_by_flourosopy','thalassemia']\ndf[cat]","11ae13a4":"for col in cat:\n    print(f'For {col}, the unique values are: {df[col].unique()}')\n    print('\\n')\n    \n#to see each unique value in each categorical variable","d82ca915":"fig,ax=plt.subplots(4,2,figsize=(20,15))\nax=ax.ravel()\n\nfor index, col in enumerate(cat):\n    sns.countplot(x=col,hue='target',data=df,ax=ax[index])","74aefcfc":"sns.displot(data=df,x='cholestoral',kde=True)\nplt.show()","14707a0d":"def chol_level(x):\n    if x > 240:\n        return 2 #high\n    elif x > 200:\n        return 1 #medium (borderline high)\n    else:\n        return 0 #desirable","ef50005e":"df['cholestoral_level'] = df['cholestoral'].apply(chol_level)\ndf","7cd6f9db":"df[['cholestoral_level','cholestoral']]\n\n#looks like we have what we want","39a6fa83":"sns.countplot(x='cholestoral_level',hue='target',data=df)\nplt.title('Breakdown by cholestoral level')\nplt.show()","3663742f":"sns.displot(data=df,x='resting_blood_pressure',kde=True)\nplt.show()","a3b7bce3":"df['resting_blood_pressure'].describe()","604d0eca":"def get_blood_level(x):\n    if x > 140:\n        return 2 #high\n    elif x > 120:\n        return 1 #at risk\n    else:\n        return 0 #normal","6486f4b1":"df['blood_pressure_level'] = df['resting_blood_pressure'].apply(get_blood_level)\ndf[['blood_pressure_level','resting_blood_pressure']]","9e26ce5e":"sns.countplot(x='blood_pressure_level',hue='target',data=df)\nplt.title('Breakdown by blood pressure level')\nplt.show()","c3459a3a":"#import required library\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.preprocessing import LabelEncoder #for kNN\nfrom sklearn.preprocessing import StandardScaler #for kNN\nfrom sklearn.preprocessing import MinMaxScaler #for SVC","e8631934":"cat","5b01e913":"num","d978df30":"le = LabelEncoder()\nscaler = StandardScaler()","9062ee29":"df_knn = df.copy()","0bf56681":"scaler.fit(df_knn[num].values)\ndf_knn[num] = scaler.transform(df_knn[num].values)\n\n#scaled numerical values","db1d05a2":"for col in cat:  \n    le.fit(df_knn[col].values)\n    df_knn[col] = le.transform(df_knn[col].values)","77c3ba39":"X_features = ['age',\n              'sex',\n              'chest_pain_type',\n              'resting_blood_pressure',\n              'cholestoral',\n              'fasting_blood_sugar',\n              'rest_ecg',\n              'Max_heart_rate',\n              'exercise_induced_angina',\n              'oldpeak',\n              'slope',\n              'vessels_colored_by_flourosopy',\n              'thalassemia']\n\ny_outcome = 'target'\n\nX = df_knn[X_features]\ny = df_knn[y_outcome]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\nX_test, X_validate, y_test, y_validate = train_test_split(X_test,y_test,test_size=0.5,random_state=42)","2beaa033":"print(X_train.shape)\nprint(X_test.shape)\nprint(X_validate.shape)","c798f90b":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(f'Default parameter accuracy score {accuracy_score(y_test, y_pred)}')\nprint(f'\\nConfusion Matrix:\\n{confusion_matrix(y_test,y_pred)}')","fb709890":"#hyperparameter tuning\nknn = KNeighborsClassifier()\n\nparam = {'n_neighbors':[1,2,3,6,9,12,15],\n        }\n\ncv = GridSearchCV(estimator=knn, param_grid=param, scoring='accuracy',\n                  verbose=5,cv=5,n_jobs=-1)\n\ncv.fit(X_train, y_train)\n","633e06e8":"print(cv.best_params_)\nprint(cv.best_score_)","287d2ac5":"y_pred = cv.predict(X_validate)\nprint(f'Tuned parameter accuracy score {accuracy_score(y_validate, y_pred)}')\nprint(f'\\nConfusion Matrix:\\n{confusion_matrix(y_validate,y_pred)}')","bf5ff53a":"cat","aaace777":"df_rf = df.copy()\ndf_rf","a7150ee5":"for col in cat:  \n    le.fit(df_rf[col].values)\n    df_rf[col] = le.transform(df_rf[col].values)","86025516":"df_rf","e818cee2":"X_features = ['age',\n              'sex',\n              'chest_pain_type',\n              'resting_blood_pressure',\n              'cholestoral',\n              'fasting_blood_sugar',\n              'rest_ecg',\n              'Max_heart_rate',\n              'exercise_induced_angina',\n              'oldpeak',\n              'slope',\n              'vessels_colored_by_flourosopy',\n              'thalassemia']\n\ny_outcome = 'target'\n\nX = df_rf[X_features]\ny = df_rf[y_outcome]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\nX_test, X_validate, y_test, y_validate = train_test_split(X_test,y_test,test_size=0.5,random_state=42)","d076986d":"rf = RandomForestClassifier(n_estimators=50,\n                            max_features=round(np.sqrt(len(X_features))))","564b03b0":"rf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\nprint(f'Default parameter accuracy score {accuracy_score(y_test, y_pred)}')\nprint(f'\\nConfusion Matrix:\\n{confusion_matrix(y_test,y_pred)}')","888f9785":"#hyperparameter tuning\nknn = RandomForestClassifier()\n\nparam = {'n_estimators':[50,75,100,125],\n         'max_features':[3,4,5,6,8,10],\n         'max_depth':[5,10,15,20,25,30]\n        }\n\ncv = GridSearchCV(estimator=knn, param_grid=param, scoring='accuracy',\n                  verbose=5,cv=5,n_jobs=-1)\n\ncv.fit(X_train, y_train)","87a26c72":"print(cv.best_params_)\nprint(cv.best_score_)","1044481e":"y_pred = cv.predict(X_validate)\nprint(f'Tuned parameter accuracy score {accuracy_score(y_validate, y_pred)}')\nprint(f'\\nConfusion Matrix:\\n{confusion_matrix(y_validate,y_pred)}')","28b2d599":"y_pred = cv.predict_proba(X_validate)\ny_pred_1_proba = y_pred[:,1]\ny_pred_1_proba","c5849c8d":"#function to get class based on probability\ndef get_y_pred(p, proba_pred):\n    y_pred = []\n    for i in range(0,len(proba_pred)):\n        if proba_pred[i] > p:\n            y_pred.append(1)\n        else:\n            y_pred.append(0)\n    return pd.Series(y_pred)","5c9a7666":"prob = [0.1,0.3,0.4,0.6,0.75,0.8,0.875]\n\nfor p in prob:\n    y_pred = get_y_pred(p,y_pred_1_proba)\n    print(f'At P = {p}, Accuracy score = {accuracy_score(y_validate,y_pred)}')\n    print(confusion_matrix(y_validate,y_pred))\n    print('\\n')","2930c8fe":"rf = RandomForestClassifier(max_depth= 15, \n                            max_features= 3, \n                            n_estimators= 125) #best parameters\n\nrf.fit(X_train,y_train)\nplt.barh(X_features,rf.feature_importances_)\nplt.show()","8de1543f":"svc = LinearSVC()","a2c4de9a":"df_svc = df.copy()","5c3f7e9e":"dum_df = pd.get_dummies(df_svc,columns=cat)\ndum_df","0b36f179":"X_features = list(dum_df.columns)[8:] #saving dummies into new X_features","cdd3b6c1":"for col in num:\n    X_features.append(col)\n\nX_features #for svc","479b2e31":"scaler = MinMaxScaler()","9e7c7e25":"scaler.fit(dum_df[num].values)\ndum_df[num] = scaler.transform(dum_df[num].values)","729fd4ad":"dum_df[X_features]","59b5d360":"X = dum_df[X_features]\ny = dum_df[y_outcome]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\nX_test, X_validate, y_test, y_validate = train_test_split(X_test,y_test,test_size=0.5,random_state=42)","bab3bac4":"svc.fit(X_train,y_train)\ny_pred = svc.predict(X_test)\nprint(f'Default parameter accuracy score {accuracy_score(y_test, y_pred)}')\nprint(f'\\nConfusion Matrix:\\n{confusion_matrix(y_test,y_pred)}')","3fd95728":"#hyperparameter tuning\n\nsvc = LinearSVC()\n\nparam = {\n    'C':[0.1,1,10,100,1000],\n    'max_iter':[1000,2000]\n}\n\ncv = GridSearchCV(estimator=svc, param_grid=param, scoring='accuracy',\n                  verbose=5,cv=5,n_jobs=-1)\n\ncv.fit(X_train, y_train)","e957cf98":"print(cv.best_params_)\nprint(cv.best_score_)","e2ef44f6":"y_pred = cv.predict(X_validate)\nprint(f'Tuned parameter accuracy score {accuracy_score(y_validate, y_pred)}')\nprint(f'\\nConfusion Matrix:\\n{confusion_matrix(y_validate,y_pred)}')","e8863303":"## Ensemble trees\n\nTree models work well without scaling so we can just simply split our data and fit our model but it may still more appropriate to encode the categorical variables","73f86c97":"## 100% accuracy\nAt 0.3 probability, we get 100% accuracy","f8610a31":"## Probability Threshold adjustment\n\nThe above is our baseline results with the best parameteres, maybe we can use predict_proba and some function to output a class prediction according to some threshold to futher improve the scoring","3dc32e06":"With n_neighbor = 1, the model yields the best results of 96.7% accuracy.  Looking at the confusion matrix, there are no false negative but false positives.  Let's see what some more advanced models presents us","f6acd4a9":"## Numerical Values\n\n\n### From Pairplot\n- Seems that there are pretty obvious difference when it comes to heart rates and target -> *higher heart rate seems to have more heart attack*\n\n- Seems for oldpeak, most heart disease that occur are concentrated around 0\n\n- Other factors do not seem to provide much explanatory values\n\n### From boxplots\n- Older age median surprisingly results in less heart disease\n\n- Low oldpeak seems to be more common with heart disease\n\n- heart disease seems more common with higher max heart rate\n\n- No significance difference when it comes to resting blood pressure and chloesteral\n\n### Going forward\n\n- Some insights maybe derived if we split blood pressure and chloesteral into high, medium, low according to some medical guidelines","ef5c59b5":"Seems that higher cholestoral does make you less likely to have heart disease","daf9a81e":"## Initial Observation\n\n- No null data\n\n- A mix of numerical and categorical data","0939206f":"## Insights from categorical values\n\n- Female seems more at risk of heart disease\n\n- Slight increased risk of heart disease when fasting blood sugar is lower than 120mg\/ml\n\n- Increased risk of heart disease when exercise induced angina is not present\n\n- Highest risk of heart disease when vessels colored by flourosopy is zero\n\n- Typical angina has the lowest risk of heart disease\n\n- ST-T wave abnormality presents the highest heart disease risk\n\n- Downsloping and flat represents the highest and lowest risk of heart disease respectively\n\n- Fixed defect and reverseable defect of thalassemia presents the highest and lowest risk of heart disease respectively","9bf937d2":"We used LabelEncoder before, now let's try dummies since SVC is more sensitive to the scale of the feature values","90168b20":"## Conclusion\n\n- We have done some basic and advanced EDA to enhance our understanding of the problem\n\n- We have tried various models, with the most success with the RandomForestClassifier, reaching 100% accuracy on validation","6e476ac6":"## Feature Importance","db1db62d":"Even without tuning much, our RandomForestRegressor beats the kNN model","8edb57fc":"Which seems to confirm what we suspected during the EDA, most high importance feature are of categorical nature, with the exception of oldpeak and max heart rate","aeacfdf7":"## LinearSVC","f3731477":"## kNN","186c0e10":"Interestingly, it seems that for those with low blood pressure, there're more occurence of heart disease proportionally than high blood pressure levels","170882b2":"## Advanced EDA\n\n- See if we can extract more insights from some of the numerical variables when we categorize them as low, medium and high","5cb7a703":"## Categorical Values\n\n- Looking at the paiplot itself, the numerical variables do not seem to explain much and nothing except oldpeak and max heart rate\n\n- Let's have a look at categorical values","e0078861":"# EDA","7362d4eb":"Based on the warning and the results, it seems that SVC does not work very well with our problem","2e16552b":"## Blood pressure research\n\n### According to https:\/\/www.cdc.gov\/bloodpressure\/about.htm:\n\n- Normal \tsystolic: less than 120 mm Hg\n\n- At Risk (prehypertension) \tsystolic: 120\u2013139 mm Hg\t\n\n- High Blood Pressure (hypertension) \tsystolic: 140 mm Hg or higher","723d7f3a":"## Cholesterol Reseach\n\n### According to https:\/\/www.medicalnewstoday.com\/articles\/315900:\n\n\n*Cholesterol levels for adults*\n\n- *Total cholesterol levels less than 200 milligrams per deciliter (mg\/dL) are considered desirable for adults. A reading between 200 and 239 mg\/dL is considered borderline high and a reading of 240 mg\/dL and above is considered high.*\n\nAssuming the column is indeed referring to total cholesterol levels.  Let's try to explore this in relation to heart disease","ad760749":"# Classification Problem\n\nThere are some some classification models we can consider using:\n\n- Ensemble of trees (RandomForestClassifier).  From our EDA, there does not seem to be a strong trend among each of the variables.  D.Tree may help us discover some of the rules.\n\n- Linear SVC\n\n- kNearestNeighbor\n\n## Workflow\n\nFor each of the model, we will split the data into 70\/15\/15  for train, test, validation.  (as per stated in task).  We will use predict_proba and use a custom function to determine which probability threshold to classify.  My major scoring would be accuracy while keeping in mind of recall (since we want to capture all those at risk of heart disease)"}}