{"cell_type":{"bb7e780c":"code","36e6132e":"code","8954f75c":"code","aaa1de74":"code","c71578c7":"code","21c75100":"code","5c30b517":"code","a617b5ba":"code","a4b5df9c":"code","c881d2ab":"code","badb378a":"code","fc8c6096":"code","ed7015d8":"code","d7ef8c58":"code","99033f5f":"code","b56e6981":"code","3748c81d":"code","fc2e31d9":"code","2f040757":"code","1d16c9a2":"code","a99d5810":"code","59a9a3f2":"code","95a72c1f":"code","86320887":"code","dccb731a":"code","8d975ecc":"markdown"},"source":{"bb7e780c":"from sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nfrom catboost import CatBoostRegressor\n\nfrom skopt import gp_minimize, forest_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_convergence\nfrom skopt.plots import plot_objective, plot_evaluations\nfrom skopt.utils import use_named_args\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBRegressor\nfrom vecstack import stacking\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nimport os","36e6132e":"temp = pd.read_csv('..\/input\/santander-value-prediction-challenge\/test.csv')\ntest_ID = temp['ID'].values\ndel temp","8954f75c":"# give the path to where the training and test set are present\ntrain = pd.read_csv('..\/input\/46-features-lgb\/46feat_trainData.csv.gz',compression='gzip')\n# train['log_target'] = np.log1p(train['target'])\ntest = pd.read_csv('..\/input\/46-features-lgb\/46feat_testData.csv.gz',compression='gzip')","aaa1de74":"train.head(2)","c71578c7":"X_train, X_test = train_test_split(train, test_size=0.2, random_state=5)","21c75100":"y_train = X_train['log_target']\ny_test = X_test['log_target']\n\nt2 = 'log_target'\nfoo = train.columns\nfoo = list(foo)\nfoo.remove(t2)\ncolNames = foo","5c30b517":"# Caution! All models and parameter values are just \n# demonstrational and shouldn't be considered as recommended.\n# Initialize 1-st level models.\nmodels = [    \n    CatBoostRegressor(iterations=200,\n                            learning_rate=0.03,\n                            depth=4,\n                            loss_function='RMSE',\n                            eval_metric='RMSE',\n                            random_seed=99,\n                            od_type='Iter',\n                            od_wait=50,\n                     logging_level='Silent'),\n    \n    CatBoostRegressor(iterations=500,\n                            learning_rate=0.06,\n                            depth=3,\n                            loss_function='RMSE',\n                            eval_metric='RMSE',\n                            random_seed=99,\n                            od_type='Iter',\n                            od_wait=50,\n                     logging_level='Silent'),\n    \n    XGBRegressor(eta=0.1,reg_lambda=1,reg_alpha=10),\n    \n    XGBRegressor(eta=0.02,reg_lambda=1,reg_alpha=10,n_estimators=300),\n    \n    XGBRegressor(eta=0.002,max_depth=15,n_estimators=200),\n]","a617b5ba":"# mode 1 Compute stacking features to determine optimal parameters for 2nd level model\nS_train, S_test = stacking(models, X_train[colNames], y_train, X_test[colNames], \n    regression = True, metric = mean_absolute_error, n_folds = 5, \n    shuffle = True,random_state = 0, verbose = 2)","a4b5df9c":"print(X_train[colNames].shape)\nprint(X_test[colNames].shape)\nprint(S_test.shape)\nprint(S_train.shape)","c881d2ab":"dim_learning_rate = Real(low=1e-6, high=1e-1, prior='log-uniform',name='learning_rate')\ndim_estimators = Integer(low=50, high=500,name='n_estimators')\ndim_max_depth = Integer(low=1, high=6,name='max_depth')","badb378a":"dimensions = [dim_learning_rate,\n              dim_estimators,\n              dim_max_depth]","fc8c6096":"default_parameters = [1e-2,300,4]","ed7015d8":"def createModel(learning_rate,n_estimators,max_depth):       \n\n    model = XGBRegressor(n_estimators=n_estimators,\n                          learning_rate=learning_rate,\n                          max_depth=max_depth,\n                          random_state=0)\n   # Fit 2-nd level model\n    model = model.fit(S_train, y_train)\n    \n    # Predict\n    y_pred = model.predict(S_test)\n\n    # Final prediction score\n    lv = mean_absolute_error(y_test, y_pred)\n    \n    return lv\n    ","d7ef8c58":"@use_named_args(dimensions=dimensions)\ndef fitness(learning_rate,n_estimators,max_depth):\n    \"\"\"\n    Hyper-parameters:\n    learning_rate:     Learning-rate for the optimizer.\n    n_estimators:      Number of estimators.\n    max_depth:         Maximum Depth of tree.\n    \"\"\"\n\n    # Print the hyper-parameters.\n    print('learning rate: {0:.1e}'.format(learning_rate))\n    print('estimators:', n_estimators)\n    print('max depth:', max_depth)\n\n\n    \n    lv= createModel(learning_rate=learning_rate,\n                    n_estimators=n_estimators,\n                    max_depth = max_depth)\n    \n   # Print the rmse.\n    print()\n    print(\"Error: {}\".format(lv))\n    print()\n\n    return lv","99033f5f":"error = fitness(default_parameters)","b56e6981":"# use only if you haven't found out the optimal parameters for xgb. else comment this block.\nsearch_result = gp_minimize(func=fitness,\n                            dimensions=dimensions,\n                            acq_func='EI', # Expected Improvement.\n                            n_calls=20,\n                           x0=default_parameters)","3748c81d":"plot_convergence(search_result)\nplt.show()","fc2e31d9":"# optimal parameters found using scikit optimize. use these parameter to initialize the 2nd level model.\nsearch_result.x","2f040757":"# mode 2 Compute stacking features on the test set. \n# this mode assumes that you have already found out the best parameters using skopt for the 2nd level model\n# if not then uncomment the upper block (comment this one) and find the best paramters and \n# then uncomment this block and run on the test set.\n\nS_train, S_test = stacking(models, X_train[colNames], y_train, test[colNames], \n    regression = True, metric = mean_absolute_error, n_folds = 5, \n    shuffle = True, random_state = 0, verbose = 2)","1d16c9a2":"print(test.shape)\nprint(X_train[colNames].shape)\nprint(S_test.shape)\nprint(S_train.shape)","a99d5810":"# Initialize 2-nd level model\n\nmodel = XGBRegressor(n_estimators=search_result.x[1],\n                          learning_rate=search_result.x[0],\n                          max_depth=search_result.x[2],\n                          random_state=0)\n    \n# Fit 2-nd level model\nmodel = model.fit(S_train, y_train)","59a9a3f2":"# Predict\ny_pred = model.predict(S_test)","95a72c1f":"print(y_pred.shape)","86320887":"result = pd.DataFrame({'ID':test_ID\n                       ,'target':np.expm1(y_pred)})\n\nresult.to_csv('46feat_stacked_ensemble_regr_models.csv', index=False)","dccb731a":"result.tail()","8d975ecc":"**        This kernel uses [vecstack]( https:\/\/github.com\/vecxoz\/vecstack) ensemble for stacking different models.       \n        I use[ scikit optimize](https:\/\/scikit-optimize.github.io\/) to find the best parameters for 2nd level model (xgboost).         \n        If you don't want to use scikit optimize you can comment out the relevant portions and use your own\n        parameter values for the 2nd level model.\n        I created a train and test set based on  Olivier's 46 features for training the model. Please go through his excellent [post](https:\/\/www.kaggle.com\/ogrellier\/santander-46-features) for details\n**"}}