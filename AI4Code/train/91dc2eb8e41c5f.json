{"cell_type":{"b2ed4b1a":"code","9358ea44":"code","8d5f8bca":"code","16a58012":"code","fdf5bf06":"code","01e5360b":"code","7723a933":"code","7fee6b94":"code","52d2498c":"code","5e140e6a":"code","c13efd1b":"code","60f4d3ac":"code","ce4ca0be":"code","781f126a":"code","2211c747":"code","49452b4c":"markdown","93d7c70b":"markdown","e42e1038":"markdown","34b5016a":"markdown","8cd19383":"markdown","1babd790":"markdown","e395bb28":"markdown","694de987":"markdown","c2e9c740":"markdown","cfe3f3e4":"markdown","cc8cecc4":"markdown","775d7e14":"markdown","f844e229":"markdown"},"source":{"b2ed4b1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9358ea44":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","8d5f8bca":"train_data.describe()","16a58012":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef visualize_null_data(df,Title):\n    miss = df.isnull().sum()\/len(df)\n    miss = miss[miss> 0]*100\n    miss.sort_values(inplace=True)\n    print(miss)\n\n    #visualising missing values\n    miss = miss.to_frame()\n    miss.columns = ['count']\n    miss.index.names = ['Name']\n    miss['Name'] = miss.index\n\n    #plot the missing value count\n    sns.set(style=\"whitegrid\", color_codes=True)\n    sns.barplot(x = 'Name', y = 'count', data=miss)\n    plt.xticks(rotation = 90)\n    plt.title(Title)\n    plt.show()","fdf5bf06":"visualize_null_data(train_data,\"Titanic Training Data - Null Value %\")","01e5360b":"train_data.loc[(train_data.Sex=='male') & (train_data['Age'].isnull()),'Age']=train_data[train_data.Sex=='male']['Age'].median()\ntrain_data.loc[(train_data.Sex=='female') & (train_data['Age'].isnull()),'Age']=train_data[train_data.Sex=='female']['Age'].median()\ntrain_data['Embarked'].fillna(train_data['Embarked'].mode()[0],inplace=True)","7723a933":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","7fee6b94":"visualize_null_data(test_data,\"Titanic Test Data Null Values %\")","52d2498c":"test_data.loc[(test_data.Sex=='male') & (test_data['Age'].isnull()),'Age']=test_data[test_data.Sex=='male']['Age'].median()\ntest_data.loc[(test_data.Sex=='female') & (test_data['Age'].isnull()),'Age']=test_data[test_data.Sex=='female']['Age'].median()\ntest_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].mean())","5e140e6a":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\ndef build_model(train,test,y):\n    X = pd.get_dummies(train)\n    X_test = pd.get_dummies(test)\n    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n    model.fit(X,y)\n    \n    return X, X_test, model","c13efd1b":"features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX, X_test, model = build_model(train_data[features],test_data[features],y)\npredictions = model.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_with_4features.csv', index=False) #Score : 0.77511\nprint(\"Your submission was successfully saved!\")","60f4d3ac":"#Below one has given 0.78229\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Age\",\"Embarked\",\"Fare\"]\nX, X_test, model = build_model(train_data[features],test_data[features],y)\npredictions = model.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_with_7features.csv', index=False) #Score : 0.78468\nprint(\"Your submission was successfully saved!\")","ce4ca0be":"train_data['Infants']=0 #0 Means Non Infants and 1 means Infants\ntrain_data.loc[train_data.Age<=2,'Infants']=1\n\ntrain_data['Kids']=0 #0 Means Not Kids and 1 means Kids\ntrain_data.loc[(train_data.Age>2) & (train_data.Age<=18),'Kids']=1\n\ntrain_data['Adults']=0 #1 Means Adults\ntrain_data.loc[(train_data.Age>18) & (train_data.Age<=60),'Adults']=1\n\ntrain_data['SeniorCitizens']=0 #1 means SeniorCitizens\ntrain_data.loc[train_data.Age>60,'SeniorCitizens']=1","781f126a":"test_data['Infants']=0 #0 Means Non Infants and 1 means Infants\ntest_data.loc[test_data.Age<=2,'Infants']=1\n\ntest_data['Kids']=0 #0 Means Not Kids and 1 means Kids\ntest_data.loc[(test_data.Age>2) & (test_data.Age<=18),'Kids']=1\n\ntest_data['Adults']=0 #1 Means Adults\ntest_data.loc[(test_data.Age>18) & (test_data.Age<=60),'Adults']=1\n\ntest_data['SeniorCitizens']=0 #1 means SeniorCitizens\ntest_data.loc[test_data.Age>60,'SeniorCitizens']=1","2211c747":"#Below one has given 0.79186\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Age\",\"Embarked\",\"Fare\",'Infants','Kids','Adults','SeniorCitizens']\nX, X_test, model = build_model(train_data[features],test_data[features],y)\npredictions = model.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_with_11features.csv', index=False) #Score : 0.79186\nprint(\"Your submission was successfully saved!\")\n","49452b4c":"**Great!!!**\n\n**By doing this simple feature engineering we could increase our score to 0.79186, which is an increase of 1.7%.**\n\nIf you would like to understand more about how exactly RFC model predicts the outcome, based on the values of the factors then you can view my other Code. For example,\n* If passenger is female then possibility of survival is high\n* If passenger has travelled in 2nd or 1st class then possibility of survival is high etc.\n\nPlease view the Part1 & Part2 of the code(s) by clicking [here](https:\/\/www.kaggle.com\/rajamykaggle\/titanic-explainable-ml-code-part1) & [here](https:\/\/www.kaggle.com\/rajamykaggle\/titanic-explainable-ml-code-part2)\n\nHope you found this simple feature engineering technique useful.  Please give your comments.\n\n### If you like this kindly upvote!!!","93d7c70b":"You can submit above file & **you will get the score of 0.78468**. By adding 3 features, we could increase ~1% score.","e42e1038":"## Using Simple Feature Engineering To Improve the Score\n\nWe will discuss how simple feature engineering can be applied in the Titanic data to improve the accuracy.\n\n### Please up Vote if you like this and that will motivate me to continue sharing.\n\nLet us read the Training Data.","34b5016a":"In the above chart we can see that Embarked, Age and Cabin columns are having null values. \n\n* We can look into the imputation for Cabin data later as it is having huge amount of Null Values.\n* For Embarked column, let us impute using Mode.\n* For Age column, let us impute it using Median for Male and Female Separately.","8cd19383":"We have other columns also in our training data. **Let us use \"Age\",\"Embarked\",\"Fare\" columns also.**","1babd790":"Let us visualize the columns that has Null Values in the Training and Test Data. Let us write a method (visualize_null_data) to do the same.","e395bb28":"Let us build RFC model again with these features \"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Age\",\"Embarked\",\"Fare\",'Infants','Kids','Adults','SeniorCitizens'.","694de987":"Let us predict based on the \"Pclass\", \"Sex\", \"SibSp\", \"Parch\" features.","c2e9c740":"## Feature Engineering\n\nLet us Feature Engineering and additional Features.\n\nBased on the Age let us categorize the passengers as Infants, Kids, Adults & SeniorCitizens using below criteria:\n* If Age <=2 mark it as Infants\n* If Age > 2 and Age <=18 mark it as Kids\n* If Age > 18 and Age <=60 mark it as Adults\n* If Age > 60 mark it as SeniorCitizens\n\nLet us add these additional features in both Training & Test Data","cfe3f3e4":"You can submit above file & **you will get the score of 0.77511**","cc8cecc4":"Let us replace null values in both Fare & Age columns. **For Fare column let us impute it using Mean.**","775d7e14":"Let us build the Random Forest Classifier (RFC) using the \"Pclass\", \"Sex\", \"SibSp\", \"Parch\" features.\n\nLet us build RFC model using the method (as we will invoke this multiple times).","f844e229":"Let us read the Test data and populate missing values."}}