{"cell_type":{"eabc6005":"code","e5354647":"code","65faf267":"code","e1467b76":"code","bcecd960":"code","e4d8e42e":"code","1171e4b6":"code","02a1c175":"code","4d9c82c7":"code","6a48dce1":"code","4a6b8819":"code","cbac7bbd":"code","6157eb13":"code","779f1b80":"code","0694ad7d":"code","53a1d08e":"code","388921e6":"code","8373fe7d":"code","e88fe4d6":"code","f14fec16":"code","1d1053b7":"code","209ac5d9":"code","f3694987":"code","e2cdc3fc":"code","d66c0ddd":"code","21a5ae73":"code","f975ad43":"code","1bbff37e":"code","3614d115":"code","a2415301":"code","4269f27d":"code","d35e517e":"code","95c74882":"code","97d8bbe9":"code","225a9d76":"code","d0b02fd9":"code","8ffee005":"code","e6d81f06":"code","f6640a13":"code","fb8e26ac":"code","4d1c7f4f":"code","9637b41f":"code","38a0b6ac":"code","f4a77546":"code","6049c737":"markdown","1acd52bf":"markdown","9c93cf79":"markdown","33e44a53":"markdown","6128156f":"markdown","ce29a34a":"markdown","0e846af6":"markdown","a1d02f1d":"markdown","a38dce16":"markdown","1837d1ba":"markdown","d1a537b3":"markdown","d8e93275":"markdown","38495ed1":"markdown","4a4a67de":"markdown","54d2b646":"markdown","671e72bd":"markdown","e0ae570a":"markdown","4ac563db":"markdown","948bb49f":"markdown","6bf6c57d":"markdown","b7beb6a4":"markdown","fc7b96f9":"markdown","825dfa85":"markdown","5666e00d":"markdown","9eac50f8":"markdown","d3a34cf7":"markdown","33c23107":"markdown","5aaf74bb":"markdown","31e4a426":"markdown"},"source":{"eabc6005":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5354647":"#I will use excessive commenting to help me 1) practice commenting in a new language (python),\n#2) make sure I understand what the code I'm writing does, and \n#3) to help others who may happen upon my notebook.\n\n#import our libraries to get started\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42","65faf267":"#functions\ndef concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    #returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set'\n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)","e1467b76":"print(df_train.info())\ndf_train.sample(3)","bcecd960":"print(df_test.info())\ndf_test.sample(3)","e4d8e42e":"def display_missing(df):\n    for col in df.columns.tolist():\n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n    print('\/n')\n        \nfor df in dfs:\n    print('{}'.format(df.name))\n    display_missing(df)","1171e4b6":"age_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1,4):\n    for sex in ['female', 'male']:\n        print('Median age of pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\n    print('Median age of all passengers: {}'.format(df_all['Age'].median()))\n    \n#filling the missing values in Age with the medians of Sex and Pclass groups\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","02a1c175":"df_all[df_all['Embarked'].isnull()]","4d9c82c7":"#filling the missing values in Embarked with S\ndf_all['Embarked'] = df_all['Embarked'].fillna('S')","6a48dce1":"df_all[df_all['Embarked'].isnull()]","4a6b8819":"df_all[df_all['Fare'].isnull()]","cbac7bbd":"med_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n\n# fill the missing value in Fare with the median fare of a 3rd class lone passenger\ndf_all['Fare'] = df_all['Fare'].fillna(med_fare)\n\n# make sure there are no nulls once complete\ndf_all[df_all['Fare'].isnull()]","6157eb13":"#while I've written the other code myself, I am copying and pasting this to ensure no errors in my typing, again, to see the visual\n# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\ndf_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n\ndef get_pclass_dist(df):\n    \n    # Creating a dictionary for every passenger class count in every deck\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # Creating a dictionary for every passenger class percentage in every deck\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count \/ df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n\ndef display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\ndisplay_pclass_dist(all_deck_per)","779f1b80":"#Passenger in T deck is changed to A\nidx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'","0694ad7d":"df_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n                                                                                   'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\n\ndef get_survived_dist(df):\n    \n    # Creating a dictionary for every survival count in every deck\n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count \/ df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n\ndef display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85    \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Survived\")\n \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)","53a1d08e":"df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()","388921e6":"# Dropping the Cabin feature\ndf_all.drop(['Cabin'], inplace=True, axis=1)\n\ndf_train, df_test = divide_df(df_all)\ndfs = [df_train, df_test]\n\nfor df in dfs:\n    display_missing(df)","8373fe7d":"survived = df_train['Survived'].value_counts()[1]\nnot_survived = df_train['Survived'].value_counts()[0]\nsurvived_per = survived \/ df_train.shape[0] * 100\nnot_survived_per = not_survived \/ df_train.shape[0] * 100\n\nprint('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\nprint('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))\n\nplt.figure(figsize=(10, 8))\nsns.countplot(df_train['Survived'])\n\nplt.xlabel('Survival', size=15, labelpad=15)\nplt.ylabel('Passenger Count', size=15, labelpad=15)\nplt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(not_survived_per), 'Survived ({0:.2f}%)'.format(survived_per)])\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\n\nplt.title('Training Set Survival Distribution', size=15, y=1.05)\n\nplt.show()","e88fe4d6":"df_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\ndf_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n\ndf_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\ndf_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)","f14fec16":"#training set high correlations\ncorr = df_train_corr_nd['Correlation Coefficient'] > 0.1\ndf_train_corr_nd[corr]","1d1053b7":"#test set high correlations\ncorr = df_test_corr_nd['Correlation Coefficient'] > 0.1\ndf_test_corr_nd[corr]","209ac5d9":"fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()","f3694987":"cont_features = ['Age', 'Fare']\nsurv = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of survival in feature\n    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n    \n    # Distribution of feature in dataset\n    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()","e2cdc3fc":"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()","d66c0ddd":"df_all = concat_df(df_train, df_test)\ndf_all.head()","21a5ae73":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","f975ad43":"df_all['Age'] = pd.qcut(df_all['Age'], 10)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=df_all)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","1bbff37e":"df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()","3614d115":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')","a2415301":"fig, axs = plt.subplots(figsize=(12, 9))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all)\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()","4269f27d":"df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs.'] = 1","d35e517e":"fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()","95c74882":"def extract_surname(data):\n    \n    families = []\n    \n    for i in range(len(data)):\n        name = data.iloc[i]\n        \n        if '(' in name:\n            name_no_bracket = name.split('(')[0]\n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n        \n    return families\n\ndf_all['Family'] = extract_surname(df_all['Name'])\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","97d8bbe9":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]","225a9d76":"mean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","d0b02fd9":"for df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) \/ 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) \/ 2 ","8ffee005":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs:\n    for feature in non_numeric_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])","e6d81f06":"cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)","f6640a13":"df_all = concat_df(df_train, df_test)\ndrop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n\ndf_all.drop(columns=drop_cols, inplace=True)\n\ndf_all.head()","fb8e26ac":"X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))","4d1c7f4f":"single_best_model = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1)","9637b41f":"N = 5\noob = 0\nprobs = pd.DataFrame(np.zeros((len(X_test), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=df_all.columns)\nfprs, tprs, scores = [], [], []\n\nskf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    # Fitting the model\n    single_best_model.fit(X_train[trn_idx], y_train[trn_idx])\n    \n    # Computing Train AUC score\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_train[trn_idx], single_best_model.predict_proba(X_train[trn_idx])[:, 1])\n    trn_auc_score = auc(trn_fpr, trn_tpr)\n    # Computing Validation AUC score\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_train[val_idx], single_best_model.predict_proba(X_train[val_idx])[:, 1])\n    val_auc_score = auc(val_fpr, val_tpr)  \n      \n    scores.append((trn_auc_score, val_auc_score))\n    fprs.append(val_fpr)\n    tprs.append(val_tpr)\n    \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = single_best_model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = single_best_model.predict_proba(X_test)[:, 1]\n    importances.iloc[:, fold - 1] = single_best_model.feature_importances_\n        \n    oob += single_best_model.oob_score_ \/ N\n    print('Fold {} OOB Score: {}\\n'.format(fold, single_best_model.oob_score_))   \n    \nprint('Average OOB Score: {}'.format(oob))","38a0b6ac":"importances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 20))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)\n\nplt.show()","f4a77546":"class_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nprobs['1'] = probs[class_survived].sum(axis=1) \/ N\nprobs['0'] = probs.drop(columns=class_survived).sum(axis=1) \/ N\nprobs['pred'] = 0\npos = probs[probs['1'] >= 0.5].index\nprobs.loc[pos, 'pred'] = 1\n\ny_pred = probs['pred'].astype(int)\n\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test['PassengerId']\nsubmission_df['Survived'] = y_pred.values\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)","6049c737":"#### FEATURE TRANSFORMATION ####\n\n##### Label Encoding Non-Numerical Features #####\n\n`Embarked`, `Sex`, `Deck`, `Title`, and `Family_Size_Grouped` are object types, and `Age` and `Fare` features are category typed. These are converted to numberical types with `LabelEncoder`. `LabelEncoder` labeles the classes from 0 to N, this is a necessary process for models to learn from the features.","1acd52bf":"#### MISSING VALUES ####\n\nSome of the columns have missing values. Using the `display_missing` function will show the count of the missing value in every column of both datasets. **Definitely want to save this information for future data cleaning projects.**\n","9c93cf79":"##### Correlations #####\n\nFeatures are highly correlated with each other and also dependent to each other. While I'm not exactly sure how this part works, I am interested in again, seeing the information come to life, and also the visuals that the tutorial has put together.","33e44a53":"##### Embarked (Training set missing: 2, Test set missing: 0) #####\n\nLet's see the missing values","6128156f":"##### One-Hot Encoding the Categorical Features #####\n\nThe categorical features of `Pclass`, `Sex`, `Deck`, `Embarked`, `Title`, are converted to one-hot encoded features with `OneHotEncoder`. `Age` and `Fare` feautres are not converted because they are ordinal, unlike the others.","ce29a34a":"Thanks to the tutorial's research, I know that these women boarded together in Southampton.\n\nI'll fill the missing values of embarked with S","0e846af6":"Thanks again to the amazing tutorial, we have a lot of great information. Check out the tutorial for more information about the visualization, and what it means. The long and short are:\n\n* A, B, and C decks are labeled as ABC because they only have 1st class passengers\n* D and E decks are labeled DE because they have similar passenger distribuition and the same survial rate\n* F and G decks are labeled FG for the same reason above\n* M has the lowest survival rate, and is all the missing values of Cabin","a1d02f1d":"* `Survived` is what we're trying to predict, (0 = not survived, or 1 = survived)\n* `Pclass` (Passenger class) is the socio-economic status (1= upper class, 2= middle class, 3=lower class)\n* `Embarked` is the port of embarkation (C = Cherbourg, Q=Queenstown, S=Southampton)\n\n","a38dce16":"#### Target Distribution in Features ####\n\n##### Continuous Features #####\nDistribution of Age Feature shows that children younger than 15 have a high survial rate than other age groups\n\nThe below code is copied and pasted to view the visualizations of the data","1837d1ba":"I've gotten some values filled in, let's see if we have any others.","d1a537b3":"#### RANDOM FOREST ####\n\nUsing the `single_best_model` as recommended by tutorial because it is a good model to start experimenting and learning about decision trees.","d8e93275":"* 100% of A, B, and C decks are 1st class passengers\n* Deck D has 87% first class, 13% 2nd class passengers\n* Deck E has 83% 1st class, 10% second class, and 7% 3rd class passengers\n* Deck F has 62% 2nd class and 38% 3rd class passengers\n* Since there is only 1 person on T deck, and he's first class, we'll group him with A deck\n* Passengers labeled M have Missing values in Cabin, so M is used like a deck.","38495ed1":"All values are now filled. I'm not sure how we got to the next part of the tutorial, but it's marked as Target Distribution. I think that this is setting the targets we want for survival rates.\n\n#### Target Distribution ####\n* 38.38% (342\/891) of training set is Class 1\n* 61.62% (549\/891 of training set is Class 0\n\nThe following code is once again copied and pasted from the tutorial","4a4a67de":"##### Cabins (Training set missing:687, Test set missing:327) #####\n\nThe tutorial I'm following has a GREAT visual of the decks and how they're named. I'm not going to copy and paste the info here, but I am interested in viewing the visual shown in the tutorial.","54d2b646":"`Family_Survival_Rate` is calculated from the families in the training set since there is no Survived feature in the test set. Family names that occur in both sets (non_unique_families) is created. The survival rate is calculated for families with more than 1 member in the list, and stored in the Family_Survival_Rate feature.\n\n`Family_Survival_Rate_NA` is created for families that are unique to the test set. This is necessary because there is no way to know those families survival rates. This feature implies that the rate is not applicable to those passengers.\n\n`Ticket_Survival_Rate` and `Ticket_Survival_Rate_NA` features are created with the same method. `Ticket_Survival_Rate` and `Family_Survival_Rate` are averaged and become the `Survival_Rate`. The same is true of `Ticket_Survial_Rate_NA` and `Family_Survival_Rate_NA` to become `Survival_Rate_NA`.","671e72bd":"#### CONCLUSION ####\n\nI'm honestly not sure what binning is, but I'm following the tutorial here.\n\n`Age` and `Fare` are binned. binning helps deal with outliers and revealed some homogeneous groups in the features. ","e0ae570a":"#### FEATURE IMPORTANCE ####","4ac563db":"### VERSION 1 CODE FROM PYTHON COURSE TUTORIAL ###\n\n<pre>train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)*\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n \noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")<\/pre>\n\n<hr>\n\n### VERSION 2 AND BEYOND ###\nThe above code is from the competition tutorial in the Python Course. It was a great way to get my feet wet and submit my first competition entry.\n\nI will be attempting to follow the information from this notebook: [Titanic - Advanced Feature Engineering Tutorial](http:\/\/https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\/) moving forward. I'm hoping to further my overall understanding, and follow the tutorial myself.\n","948bb49f":"Now because I want to see the difference, I'll call for the null results once more.","6bf6c57d":"* `Parch` = total number of passengers' parents and children\n* `SibSP` = total number of siblings and spouse\n\nBecuase of this, the logical conclusion is that Mr. Storey was traveling alone and would have a males 3rd class ticket.","b7beb6a4":"#### Frequency Encoding ####\n\n`Family_Size` is created by adding `SibSp`, `Parch`, and `1`. `SibSp` is the count of siblings and spouse, and `Parch` is the count of parents and children. Adding these columns, plus 1 (for the passenger) gives the total size of the families. Graphs have shown that `Family_Size` is a predictor of survival because different values have different survivial rates.\n\n* Family Size of 1 = Alone\n* Family Size of 2-4 = Small\n* Family Size of 5-6 = Medium\n* Family Size of 7+ = Large","fc7b96f9":"#### Target Encoding ####\n\n`extract_surname` is used for extracting surnames (or last names) of the passengers from the `Name` feature. `Family` feature is created with extracted surnames. This is needed to group passengers in the same family.","825dfa85":"#### SUBMISSION ####","5666e00d":"Many passengers traveled in groups, with nannies, maids, friends, etc., and all used the same ticket. While this feature doesn't add any additonal information gain, it is neat to see.","9eac50f8":"##### Fare (Training set missing: 0, Test set missing: 1) #####\n\nLet's start by seeing who's missing the value.","d3a34cf7":"##### Categorical Features #####\n\nPassengers boarded from Southhampton have a lower survival rate than other ports. This could be related to the passenger class\nParch and SibSp features show that passengers with only one family member have a higher survival rate\n\nBelow code is copied\/pasted from tutorial for visualizations","33c23107":"#### MODEL ####","5aaf74bb":"Now we can see the survial percentage in the decks, using the same kind of table above, we can check the precentage. Again, this code will be directly copied and pasted from the tutorial for the visual.","31e4a426":"##### Age (Training set missing: 177, Test set missing: 86) #####\n\nI can use a median age to fill in the missing values, but the median age of the entire data set isn't the best idea, because the passenger's class would have bearing on their age, especially during the time of the Titanic, when higher survival rates were connected to be higher class.\n\nThe above linked tutorial really helps explain why this is the case, and goes on to explain that adding sex gives an even better median age to fill in the missing values."}}