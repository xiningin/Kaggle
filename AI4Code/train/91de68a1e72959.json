{"cell_type":{"98947009":"code","3ed0c7ee":"code","2ef296e4":"code","99cec766":"code","6fe7e187":"code","2811e554":"code","a8f97e68":"code","9c822931":"code","2cfd4807":"code","fd864849":"code","d3bc8599":"code","e80312b0":"code","67d9ff8a":"code","88d14a80":"code","c925d7a2":"code","ccd38927":"code","ef05f494":"code","35c901a6":"markdown","76eb84c0":"markdown","78be276d":"markdown","0dbf042a":"markdown","2e555f94":"markdown","5b85702a":"markdown","f1289a32":"markdown","2533b876":"markdown","ce3c4e05":"markdown","def98a73":"markdown","8ad4532e":"markdown","2221e422":"markdown","5b623f38":"markdown","369dfc1c":"markdown","97ea4b4b":"markdown"},"source":{"98947009":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #seaborn plotting lib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn import model_selection\nfrom skimage.transform import rotate,rescale, resize, downscale_local_mean,warp,SimilarityTransform,AffineTransform\nfrom skimage.morphology import thin,skeletonize\nfrom skimage.util import invert\n\n\nimport math\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3ed0c7ee":"# Utility functions\ndef extract_images_and_labels(data,is_test=False):\n    n = data.shape[0]\n    print(\"Number of images: %s\"%n)\n    data_array = data.to_numpy(dtype=np.int32)\n    if is_test:\n        X = data_array.reshape(-1,28,28)\n        print(\"Dimensions of images: {0}\".format(X.shape))\n        return X,n\n    else:\n        X = data_array[:,1::].reshape(-1,28,28)\n        y = data_array[:,0].reshape([data_array.shape[0],1])\n        print(\"Dimensions of labels: %s and images: %s\"%(y.shape,X.shape))\n        return X,y,n","2ef296e4":"training_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntesting_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","99cec766":"print(\"Number of training images: %d\"%training_data.shape[0])\nprint(\"Number of pixels per image: %d\"%(training_data.shape[1]-1))\ntraining_data.head()","6fe7e187":"# Reshape training data\nX,y,n = extract_images_and_labels(training_data)","2811e554":"# Display the first 25 images from the data set\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X[i],cmap=plt.cm.binary)\n    plt.xlabel(y[i][0])\nplt.show()","a8f97e68":"# Normalize pixel data for each image by --> (pixel_value - minimum_pixel_value)( maximum_pixel_value - minimum_pixel_value)\nX_normalized = X\/255.0\n# Re-render our previous 25 digits to make sure all is well\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X_normalized[i],cmap=plt.cm.binary)\n    plt.xlabel(y[i][0])\nplt.show()","9c822931":"# Build a multi-layer neural network (NN) to classify the images\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28,28)), # flatten images to single array of length 28x28 (784)\n    keras.layers.Dense(128*4,activation='relu'), # Add a hidden layer with 128*4 nodes, using RELU activation function\n    keras.layers.Dropout(0.5), # Dropout 0.5\n    keras.layers.Dense(128*2,activation='relu'), # Add a hidden layer with 128*2 nodes, using RELU activation function\n    keras.layers.Dense(10,activation='softmax') # # Add an output layer with 10 nodes, using Softmax activation function\n])","2cfd4807":"# Optimizer\noptimizer = 'adam'\n# Loss function\nloss = 'sparse_categorical_crossentropy'\n# Metrics \nmetrics = ['accuracy']\n# Compile the model\nmodel.compile(optimizer=optimizer,\n             loss=loss,\n             metrics=metrics)","fd864849":"# Split the monolithic training data into 'training' and 'validation' data set \nX_norm_train,X_norm_validation,y_train,y_validation = model_selection.train_test_split(X_normalized,y,test_size=0.33,random_state=42)","d3bc8599":"# Train the model\nepochs = 100\nvalidation_split = 0.33\n# Define an 'early stopping' callback\nearlystop_callback = EarlyStopping(\n  monitor='val_accuracy', min_delta=0.00001,\n  patience=3)\nmodel.fit(X_norm_train,y_train,epochs=epochs,validation_split=validation_split, callbacks=[earlystop_callback])","e80312b0":"# Evaluate the model on the validation data set\nvalidation_loss,validation_accuracy = model.evaluate(X_norm_validation,y_validation,verbose=2)\nprint(\"Accuracy: %.4f, Loss: %.4f\"%(validation_accuracy,validation_loss))","67d9ff8a":"# Generate modified images\nnum_repeats = 3\ndf = training_data.copy()\ndf_repeated = pd.concat([df] * num_repeats, ignore_index=True) \ndata_copy = df_repeated.copy().to_numpy(dtype=np.int32)\n# Handy utility to warp, thin and rotate an image randomly (within reasonable limits)\ndef warp_image(image):\n    image = (image\/255.0).reshape(28,28)\n    #image = warp(image,inverse_map=AffineTransform(shear=np.random.normal(0,0.2),translation=np.random.normal(-2,2,(1,2))))\n    image = thin(image,max_iter=np.random.randint(0,3))\n    image = rotate(image,angle=np.random.normal(-10,10))\n    return image.flatten()\n\nX_generated = np.apply_along_axis(lambda x: warp_image(x),1,data_copy[:,1::]).reshape(-1,28,28) \ny_generated = data_copy[:,0].reshape([data_copy.shape[0],1]) \n# Add generated images to training data\nX_augmented = np.concatenate((X_normalized,X_generated),axis=0)\ny_augmented = np.concatenate((y,y_generated),axis=0)","88d14a80":"# Show some examples of original & generated images \nnum_rows = 5\nnum_images_shown = num_repeats*num_rows\nfigure = plt.figure(\n    figsize=(15,20)\n)\nfor i in range(num_images_shown):\n    start_index = 1+i*(num_repeats+1)\n    for j in range(num_repeats+1):\n        plt_index =  start_index + j\n        plt.subplot(num_images_shown,num_repeats+1,plt_index)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        image_index = (j*n)+i\n        #print('{0} {1} image index {2} plt index {3}'.format(i,j,image_index,plt_index))\n        image_label = y_augmented[image_index][0]\n        plt.imshow(X_augmented[image_index],cmap=plt.cm.binary)\n        if j == 0: \n            plt.xlabel('Original {0} ({1},{2})'.format(image_label,i,j))\n        else:\n            plt.xlabel('Transformed {0} ({1},{2})'.format(image_label,i,j))\nfigure.suptitle('Each row shows the original digit and the {0} generated variations'.format(num_repeats),y=1.05,fontsize=16)\nplt.tight_layout()        \nplt.show()\n","c925d7a2":"\n# Split the monolithic training data into 'training' and 'validation' data set \nX_augmented_train,X_augmented_validation,y_augmented_train,y_augmented_validation = model_selection.train_test_split(X_augmented,y_augmented,test_size=0.33,random_state=42)\n# Train the model\nepochs = 100\nvalidation_split = 0.33\nmodel.fit(X_augmented_train,y_augmented_train,epochs=epochs,validation_split=validation_split,callbacks=[earlystop_callback])","ccd38927":"# Evaluate the model on the validation data set\nvalidation_loss,validation_accuracy = model.evaluate(X_augmented_validation,y_augmented_validation,verbose=2)\nprint(\"Accuracy: %.4f, Loss: %.4f\"%(validation_accuracy,validation_loss))","ef05f494":"# Submit predictions\nX_test,n_test = extract_images_and_labels(testing_data,is_test=True)\nX_test_normalized = X_test\/255.0\nresult = model.predict(X_test_normalized)\npredictions = np.apply_along_axis(lambda row: np.argmax(row),1,result)\npredictions.reshape([n_test,1])\ntesting_data['Label'] = predictions\ntesting_data['ImageId'] = list(range(1,n_test+1))\nsubmission = testing_data[['ImageId','Label']]\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.tail()","35c901a6":"### Evaluate performance <a class=\"anchor\" id=\"3.5\"><\/a>","76eb84c0":"### Prepare validation and training data sets <a class=\"anchor\" id=\"3.3\"><\/a>","78be276d":"### Utility functions <a class=\"anchor\" id=\"1.2\"><\/a>\n","0dbf042a":"### Prepare the data <a class=\"anchor\" id=\"2\"><\/a>\n","2e555f94":"## **Digit Recognition**\n* [Setting up](#1)\n * [Import libraries](#1.1)\n * [Utility functions](#1.2)\n* [Prepare the data](#2)\n* [Neural Network](#3)\n * [Build a multi-layer, fully connected neural network](#3.1)\n * [Configure loss function, cost function and metrics](#3.2)\n * [Prepare validation and training data sets](#3.3)\n * [Train the neural network](#3.4)\n * [Evaluate performance](#3.5)\n * [Enrich training data](#3.6)\n * [Retrain the neural network](#3.7)\n * [Reevaluate performance](#3.8)\n* [Submit predictions](#4)","5b85702a":"### Import libraries <a class=\"anchor\" id=\"1.1\"><\/a>\n","f1289a32":"### Enrich the training data <a class=\"anchor\" id=\"3.6\"><\/a>","2533b876":"### Train the neural network <a class=\"anchor\" id=\"3.4\"><\/a>","ce3c4e05":"### Build a multi-layer, fully connected neural network   <a class=\"anchor\" id=\"3.1\"><\/a>","def98a73":"## Setup <a class=\"anchor\" id=\"1\"><\/a>\n\n","8ad4532e":"### Retrain the neural network <a class=\"anchor\" id=\"3.7\"><\/a>","2221e422":"### Define loss function, cost function and metrics <a class=\"anchor\" id=\"3.2\"><\/a>","5b623f38":"## Neural Network <a class=\"anchor\" id=\"3\"><\/a>\n","369dfc1c":"## Submit predictions  <a class=\"anchor\" id=\"5\"><\/a>","97ea4b4b":"### Reevaluate performance <a class=\"anchor\" id=\"3.8\"><\/a>"}}