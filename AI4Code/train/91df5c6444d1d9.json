{"cell_type":{"2d8faa60":"code","da3a0ad6":"code","eab6041e":"code","b8636bf7":"code","893d8552":"code","07db84af":"code","90310e88":"code","76e06fbc":"code","e8f69f74":"code","f901b1a4":"code","99f80f38":"code","52889bff":"code","58b406aa":"code","30bc57d5":"code","25f45b6e":"code","ffe965f1":"code","c52d6781":"code","b5db06c8":"code","6d8f69a6":"code","4e364a51":"code","2805380b":"code","bba89ae3":"code","69c564f2":"code","4572fec8":"code","637478c7":"code","6036bd80":"markdown","db4918c9":"markdown","3c796df3":"markdown","623ecb1e":"markdown","8cf49c50":"markdown","791c52d1":"markdown","66e4e3b5":"markdown","2388a446":"markdown","dbc8d529":"markdown","1b9c3e84":"markdown","65da69ea":"markdown"},"source":{"2d8faa60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da3a0ad6":"#imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.tokenize import RegexpTokenizer\nimport string \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport wordcloud\npd.set_option('max_rows',1000000)\npd.set_option('max_columns',10000)\nsns.set(rc={'figure.figsize':(15,10)})\nimport spacy","eab6041e":"df = pd.read_csv('\/kaggle\/input\/mbti-type\/mbti_1.csv')\ndf.head()","b8636bf7":"df.info()","893d8552":"gr = df.groupby('type').count()\ngr.sort_values(\"posts\", ascending=False, inplace=True)\ngr","07db84af":"plt.figure(figsize=(15,10))\ngr['posts'].plot(kind='bar',title=\"Number of Posts per Personality type\")","90310e88":"sns.barplot(x=gr.index,y='posts',data=gr,palette='rocket')\nplt.title('Number of Posts per Personality type',fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","76e06fbc":"df[\"LenP\"] = df[\"posts\"].apply(len)\nsns.distplot(df[\"LenP\"]).set_title(\"Distribution of Lengths of all 50 Posts\");","e8f69f74":"df[\"NumPosts\"] = df[\"posts\"].apply(lambda x: len(x.split(\"|||\")))\n\nsns.distplot(df[\"NumPosts\"], kde=False).set_title(\"Number of Posts per User\")","f901b1a4":"df.head()","99f80f38":"#Split to posts\ndef extract(posts, new_posts):\n    for post in posts[1].split(\"|||\"):\n        new_posts.append((posts[0], post))\n\nposts = []\ndf.apply(lambda x: extract(x, posts), axis=1)\nprint(\"Number of users\", len(df))\nprint(\"Number of posts\", len(posts))","52889bff":"posts","58b406aa":"new_df = pd.DataFrame(posts, columns=[\"type\", \"posts\"])\nnew_df.head(100)","30bc57d5":"words = list(new_df[\"posts\"].apply(lambda x: x.split()))\nwords = [x for y in words for x in y]\nCounter(words).most_common(40)","25f45b6e":"wc = wordcloud.WordCloud(width=1200, height=500, \n                         collocations=False, background_color=\"white\", \n                         colormap=\"tab20b\").generate(\" \".join(words))\nplt.figure(figsize=(25,10))\nplt.imshow(wc, interpolation='bilinear')\n_ = plt.axis(\"off\")","ffe965f1":"def preprocess_text(df, remove_special=True):\n    #Remove links \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\\/\\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n    \n    #Keep EOS\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n    \n    #Strip Punctation\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n\n    #Remove Non-words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n\n    #To lower\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n\n    #Remove multiple letter repating words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n\n    #Remove short\/long words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n\n    #Remove Personality Types Words\n    #This is crutial in order to get valid model accuracy estimation for unseen data. \n    if remove_special:\n        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n        pers_types = [p.lower() for p in pers_types]\n        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: p.sub(' PTypeToken ',x))\n    return df","c52d6781":"#Preprocess Text\n#new_df = preprocess_text(new_df)","b5db06c8":"#Remove posts with less than X words\nmin_words = 15\nprint(\"Number of posts\", len(new_df)) \nnew_df[\"nw\"] = new_df[\"posts\"].apply(lambda x: len(re.findall(r'\\w+', x)))\nnew_df = new_df[new_df[\"nw\"] >= min_words]\nprint(\"Number of posts\", len(new_df))","6d8f69a6":"enc = LabelEncoder()\nnew_df['type_enc'] = enc.fit_transform(new_df['type'])\ntarget = new_df['type_enc']\ntarget.head()","4e364a51":"new_df.head(100)","2805380b":"nlp = spacy.load('en_core_web_lg')\n#train_vector = np.array([nlp(text).vector for text in new_df.posts])\n#print(train_vector.shape)","bba89ae3":"vect = CountVectorizer(stop_words='english') \ntrain =  vect.fit_transform(new_df[\"posts\"])","69c564f2":"train.shape","4572fec8":"X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.1, stratify=target, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","637478c7":"xgb = XGBClassifier()\nxgb.fit(X_train,y_train)\naccuracy_score(y_test,xgb.predict(X_test))","6036bd80":"Splitting the data into train and test sets.","db4918c9":"# MODELLING","3c796df3":"# EDA","623ecb1e":"plotting the most common words with WordCloud.","8cf49c50":"vectorizing the posts for the model","791c52d1":"encoding the personality types,","66e4e3b5":"lets plot the count.","2388a446":"Finding the most common words in all posts.","dbc8d529":"plotting the number of posts per user.","1b9c3e84":"lets see the distribution of length of all posts...","65da69ea":"Training the model."}}