{"cell_type":{"8a5377c3":"code","8c9c4978":"code","1c0da10c":"code","2388bac9":"code","3050b3ec":"code","d0a5a5e2":"code","77c26c4c":"code","4cd387cc":"code","c18eb23a":"code","940b0aa4":"code","d3566235":"code","50a1fa62":"code","55f62e8d":"code","c8c02966":"code","b8a3b538":"code","904322c8":"code","bb9b8c5a":"code","f38a3297":"code","c5652969":"code","aff0c929":"code","5c5459e5":"code","1f56b097":"code","226cf22f":"code","a4e9c8ad":"code","c92e1682":"code","b760a5af":"code","6e289824":"code","94d24e54":"code","a79611a5":"code","c2414132":"code","887830f4":"code","4bd2cbd1":"code","d64ad410":"code","82f6558c":"code","a03f886f":"code","ef4c822c":"code","42d9cf70":"code","faff1091":"code","c1f1e2c0":"code","bc9cc65d":"code","4a10d512":"code","a82ac4c8":"code","db898b97":"code","ca6b82d3":"code","3e73d33c":"code","e82e9fb6":"code","e58e3300":"code","774f11d4":"code","201fcb76":"code","60305825":"code","8e1030fc":"code","6a18fd54":"code","ca51b2ac":"code","8de56319":"code","ea9c9244":"markdown","94b7f80c":"markdown","43c83069":"markdown","846bef02":"markdown","0d7ebfac":"markdown","90ca2ce5":"markdown","35662a41":"markdown","0f28668f":"markdown","f3b28e55":"markdown","902111d9":"markdown","c7dedf98":"markdown","b2d3a2b8":"markdown","86f9f32e":"markdown","6df0e603":"markdown","171b8714":"markdown","8a121749":"markdown","33463ea7":"markdown","69f2187f":"markdown","4ec1b480":"markdown","924f894a":"markdown","934d31cf":"markdown","fc2f10f9":"markdown","d7a61fcd":"markdown","50c29706":"markdown","f1402e63":"markdown","ff8b203f":"markdown","a96fa3e2":"markdown","61ba22b6":"markdown","6ceb47d2":"markdown","a1eeb024":"markdown","06dd4630":"markdown","1f3032ea":"markdown","d6c31f28":"markdown","7a4e04d1":"markdown","a25b867f":"markdown","9ab98613":"markdown","a83fee14":"markdown","98e10dd2":"markdown","e8efe598":"markdown","b5f12b8f":"markdown","8328db7c":"markdown","8b0dfcb2":"markdown","1a92182d":"markdown","30fbcd3e":"markdown","0708b0e3":"markdown","39c6d525":"markdown","57901188":"markdown","14158c12":"markdown"},"source":{"8a5377c3":"#!pip install tensorflow","8c9c4978":"#importing tensorflow\nimport tensorflow as tf\nprint(tf.__version__)","1c0da10c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\nfrom sklearn.decomposition import PCA\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n","2388bac9":"# #mouting the drive\n# from google.colab import drive\n# drive.mount('\/content\/drive\/')","3050b3ec":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d0a5a5e2":"\n#Defining the path of the dataset\nproject_path = '\/kaggle\/input\/greatlearning-aimlmaycohort\/'\ndataset_file = project_path + 'creditcard.csv'","77c26c4c":"dataset_file","4cd387cc":"#reading dataset\ndata = pd.read_csv(dataset_file)","c18eb23a":"data.sample(10)","940b0aa4":"data.info()","d3566235":"#Number of distinct categories or classes i.e., Fraudulent and Genuine\ndata['Class'].nunique()","50a1fa62":"\n#checking the percentage of each class in the dataset\n(data.Class.value_counts())\/(data.Class.count())","55f62e8d":"print(\"*********Losses due to fraud:************\\n\")\nprint(\"Total amount lost to fraud\")\nprint(data.Amount[data.Class == 1].sum())\nprint(\"Mean amount per fraudulent transaction\")\nprint(data.Amount[data.Class == 1].mean())\nprint(\"Compare to normal transactions:\")\nprint(\"Total amount from normal transactions\")\nprint(data.Amount[data.Class == 0].sum())\nprint(\"Mean amount per normal transactions\")\nprint(data.Amount[data.Class == 0].mean())","c8c02966":"#visual representation of instances per class\ndata.Class.value_counts().plot.bar()","b8a3b538":"#PCA is performed for visualization only\n\npca= PCA(n_components=2)\ncreditcard_2d= pd.DataFrame(pca.fit_transform(data.iloc[:,0:30]))\ncreditcard_2d= pd.concat([creditcard_2d, data['Class']], axis=1)\ncreditcard_2d.columns= ['x', 'y', 'Class']\nsns.lmplot(x='x', y='y', data=creditcard_2d, fit_reg=False, hue='Class')","904322c8":"#Histrogram for feature Time\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nax1.hist(data[\"Time\"][data[\"Class\"] == 1], bins = 50)\nax1.set_title('Fraudulent')\n\nax2.hist(data[\"Time\"][data[\"Class\"] == 0], bins = 50)\nax2.set_title('Genuine')\n\nplt.xlabel('Seconds after transaction number zero')\nplt.ylabel('Number of Transactions')\nplt.show()","bb9b8c5a":"#Dropping time feature\ndata = data.drop(\"Time\", axis = 1)","f38a3297":"Vfeatures = data.iloc[:,0:29].columns\nprint(Vfeatures)","c5652969":"data","aff0c929":"\nf, ax1 = plt.subplots(figsize=(24,10))\n\ncorr = data.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix\", fontsize=14)","5c5459e5":"X_data = data.iloc[:,0:29]\ny_data = data.iloc[:, -1]","1f56b097":"#printing the shape of the data \nprint(y_data.shape)\nprint(X_data.shape)","226cf22f":"X_data","a4e9c8ad":"#Standardizing the Amount column (All other 'V' columns are already scaled as they've undergone PCA transformation).\nfrom sklearn.preprocessing import StandardScaler\nX_data['normalizedAmount'] = StandardScaler().fit_transform(X_data['Amount'].values.reshape(-1,1))  # Normalize 'Amount' in [-1,+1] range\nX_data= X_data.drop(['Amount'],axis=1)","c92e1682":"X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state = 7)","b760a5af":"\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","6e289824":"print(y_train.value_counts(normalize=True))\nprint(y_test.value_counts(normalize=True))","94d24e54":"from sklearn.ensemble import RandomForestClassifier","a79611a5":"random_forest = RandomForestClassifier(n_estimators=100)","c2414132":"# Pandas Series.ravel() function returns the flattened underlying data as an ndarray.\nrandom_forest.fit(X_train,y_train.values.ravel())    # np.ravel() Return a contiguous flattened array","887830f4":"y_pred = random_forest.predict(X_test)","4bd2cbd1":"random_forest.score(X_test,y_test)","d64ad410":"def make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()\/np.sum(cf)]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) \/ float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] \/ sum(cf[:,1])\n            recall    = cf[1,1] \/ sum(cf[1,:])\n            f1_score  = 2*precision*recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)","82f6558c":"cm1=confusion_matrix(y_test, y_pred)\nlabels = ['True Negative','False Positive','False Negative','True Positive']\ncategories = [ 'Not_Fraud','Fraud']\nmake_confusion_matrix(cm1, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='Blues')","a03f886f":"#initialize the model\nmodel = Sequential()","ef4c822c":"\n# This adds the input layer (by specifying input dimension) AND the first hidden layer (units)\nmodel.add(Dense(units=128, input_dim = 29,activation='relu'))   # input of 29 columns as shown above\nmodel.add(Dropout(0.20))\n# hidden layer\nmodel.add(Dense(units=96,activation='relu'))\n# Adding Dropout to prevent overfitting \nmodel.add(Dropout(0.20))\n# hidden layer\nmodel.add(Dense(units=48,activation='relu'))\nmodel.add(Dropout(0.20))\n# hidden layer\nmodel.add(Dense(units=24,activation='relu'))\nmodel.add(Dropout(0.20))\n# Adding the output layer\n# Notice that we do not need to specify input dim. \n# we have an output of 1 node, which is the the desired dimensions of our output (fraud or not)\n# We use the sigmoid because we want probability outcomes\nmodel.add(Dense(1,activation='sigmoid'))                        # binary classification fraudulent or not","42d9cf70":"# Create optimizer with default learning rate\n# Compile the model\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","faff1091":"# model.save(\"myfirstANNModel.mod\")","c1f1e2c0":"model.summary()","bc9cc65d":"X_train.shape","4a10d512":"227845\/5000","a82ac4c8":"#fitting the model\nhistory=model.fit(X_train,y_train,batch_size=1000,epochs=20,validation_split=0.2)","db898b97":"# Capturing learning history per epoch\nhist  = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\n\n# Plotting accuracy at different epochs\nplt.plot(hist['loss'])\nplt.plot(hist['val_loss'])\nplt.legend((\"train\" , \"valid\") , loc =0)","ca6b82d3":"X_test.shape","3e73d33c":"score = model.evaluate(X_test, y_test)","e82e9fb6":"print(score)","e58e3300":"## Confusion Matrix on unsee test set\nimport seaborn as sn\ny_pred1 = model.predict(X_test)\nfor i in range(len(y_test)):\n    if y_pred1[i]>0.5:\n        y_pred1[i]=1 \n    else:\n        y_pred1[i]=0\n\n\n\ncm2=confusion_matrix(y_test, y_pred1)\nlabels = ['True Negative','False Positive','False Negative','True Positive']\ncategories = [ 'Not_Fraud','Fraud']\nmake_confusion_matrix(cm2, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='Blues')","774f11d4":"#Training Multi-layer perceptron with 2 hidden layers\n\n#adding earlystopping callback\nes= keras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0.005,\n                              patience=5,\n \n                             verbose=0, mode='min', restore_best_weights= True)\nModel2 = Sequential()\n#Initializing the weights uisng hue_normal \nModel2.add(Dense(65, input_shape=(29, ), kernel_initializer='he_normal', activation='relu'))\nModel2.add(Dropout(0.5))\nModel2.add(Dense(65, kernel_initializer='he_normal', activation='relu'))\nModel2.add(Dropout(0.5))\nModel2.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n\nModel2.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    \nhis_mod2= Model2.fit(X_train, y_train, validation_split=0.2, batch_size=700, epochs=40, callbacks=[es], shuffle=True, verbose=1)\n","201fcb76":"# Capturing learning history per epoch\nhist  = pd.DataFrame(his_mod2.history)\nhist['epoch'] = his_mod2.epoch\n\n# Plotting accuracy at different epochs\nplt.plot(hist['loss'])\nplt.plot(hist['val_loss'])\nplt.legend((\"train\" , \"valid\") , loc =0)\n","60305825":"## Confusion Matrix on unsee test set\nimport seaborn as sn\ny_pred1 = Model2.predict(X_test)\nfor i in range(len(y_test)):\n    if y_pred1[i]>0.5:\n        y_pred1[i]=1 \n    else:\n        y_pred1[i]=0\n\n\n\ncm2=confusion_matrix(y_test, y_pred1)\nlabels = ['True Negative','False Positive','False Negative','True Positive']\ncategories = [ 'Not_Fraud','Fraud']\nmake_confusion_matrix(cm2, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='Blues')","8e1030fc":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), np.array([y_train.iloc[i] for i in range(len(y_train))]))\nclass_weights = dict(enumerate(class_weights))\nclass_weights","6a18fd54":"model.fit(X_train,y_train,batch_size=15,epochs=5, class_weight=class_weights, shuffle=True)","ca51b2ac":"score_weighted = model.evaluate(X_test, y_test)","8de56319":"## Confusion Matrix on unsee test set\nimport seaborn as sn\ny_pred1 = model.predict(X_test)\nfor i in range(len(y_test)):\n    if y_pred1[i]>0.5:\n        y_pred1[i]=1 \n    else:\n        y_pred1[i]=0\n\n\n\ncm2=confusion_matrix(y_test, y_pred1)\nlabels = ['True Negative','False Positive','False Negative','True Positive']\ncategories = [ 'Not_Fraud','Fraud']\nmake_confusion_matrix(cm2, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='Blues')","ea9c9244":" \n **Conclusion:** \n\n\n\n As you can see here the Recall is increased but the precision is very bad.  There is still  lot of scope of  improvements  as follows:\n\n1) Threshold can be tuned to get the optimal value \n\n2) Resampling techniques can be applied to balanced the data and then train the model \n\n3) Hyperparameter tuning can be applied to tune the different Hyperparameters\n\n\nWe can select the Model-1 as our final model based on the above analysis ","94b7f80c":"### Evaluation\nKeras model can be evaluated with evaluate() function\n\nEvaluation results are contained in a list\n\n","43c83069":"### Creating a model\n\nKeras model object can be created with Sequential class\n\nAt the outset, the model is empty per se. It is completed by adding additional layers and compilation\n","846bef02":"Above plot does not  give  bettter  visual representation of the class imbalance. The below plot after PCA gives a better visualization of the imbalance in the datasets. PCA helps to visualize the high dimensional data into lower dimensions ","0d7ebfac":"Plotting the train and validation loss","90ca2ce5":"Training the model","35662a41":"###Separating response variable and predictors ","0f28668f":"- The model achieves an accuracy of 99.95% ! Is this a good performance ?\n- Remember that our dataset is significantly composed of non fraudulent samples with only 172 fraudulent transactions per 100,000. Consequently, a model predicting every transaction as 'non fraudulent' would achieve 99.83% accuracy despite being unable to detect a single fraudulent case !","f3b28e55":"###Let's print the summary of the model ","902111d9":"## Introduction to Neural Networks - Fraud Detection\n\n**Context**\n\nNielsen reports that U.S. card fraud (credit, debt, etc) was reportedly 9 billion dollars in 2016 and expected to increase to 12 billion dollars by 2020. For perspective, in 2017 both PayPal's and Mastercard's revenue was only $10.8 billion each.\nTherefore, it is important that credit card companies should be able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n\n**Objective:** \n\nSuppose you are working as a Data scientist in a Credit Card company named **\"CCFraud\"** and  given the credit card transactions, you  need to  build a Model  (i.e. Multilayer perceptrons) for Fraud Detection using Keras.","c7dedf98":"### Adding layers [layers and activations]\n\nKeras layers can be added to the model\n\nAdding layers are like stacking lego blocks one by one\n\nIt should be noted that as this is a classification problem, sigmoid layer (softmax for multi-class problems) should be added\n","b2d3a2b8":"###Let's take a look at the V1,...,V28 features.","86f9f32e":"#Model-2","6df0e603":"**Conclusion:**\n\nAs you can see here the Recall of the model is not improved and it is worse than the Previous ANN model as well as the RandomForest but the precision is changed.\n\nLet's try weighted loss for imbalance dataset","171b8714":"* As you can see, PCA gives a better visualization of the imbalance in the datasets.","8a121749":"This notebook covers,\n\n1. Creating a Model\n\n2. Adding Layers\n\n3. Activations\n\n4. Optimizers and Loss functions\n\n5. Earlystopping\n\n6. Weight Initalization\n\n7. Dropout\n\n8. Model Evaluation","33463ea7":"##Let's try another architecture to get the better Recall \n\n\nThere are some basic Hyperparameters which can help to get the better model performance.","69f2187f":"##Import all necessary libraries ","4ec1b480":"Plotting confusion matrix ","924f894a":"Plotting the train and test loss","934d31cf":"Checking correlation between features and the likelihood of the transaction to be fraud on the unbalanced dataset","fc2f10f9":"Detection of fraudulent transactions did not improve compared to the previous machine learning model ( Randomforest).\n\n- There are 100 fraudulent transactions in the test data and  yet 15 fraudulent transactions are not identified (false negative) which remains an issue. Our objective must be to detect as many fraudulent transactions as possible since these can have a huge negative impact.\n\n- 15 regular transactions are detected as potentially fraudulent by the model. These are false positive. This number is negligible.\n\n**Conclusion:**\n\nWe must find ways to further reduce the number of false negative.","d7a61fcd":"* As expected, there are only 2 classes.","50c29706":"### Dataset Description\n\nThe dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the original features and more background information about the data is not provided. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. \n\n**Time** contains the seconds elapsed between each transaction and the first transaction in the dataset. \n\n\n**Amount** is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n\n\n**Class** is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n","f1402e63":"## Deep neural network\n\nModel-1 \n\n- We will use a simple NN made of 5 fully-connected layers with ReLu activation. The NN takes a vector of length 29 as input. This represents the information related to each transactions, ie each line with 29 columns from the dataset. For each transaction, the final layer will output a probability distribution (sigmoid activation function) and classify either as not fraudulent (0) or fraudulent (1).\n- a dropout step is included to prevent overfitting.\n\n\n\n**Dropout**\n\nDropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Dropout is a technique where randomly selected neurons are ignored during training. They are \u201cdropped-out\u201d randomly.","ff8b203f":"###Splitting the Data into train and test set","a96fa3e2":"## Model evaluation criterion\n\n### Model can make wrong predictions as:\n* Predicting a transaction is fraud  and the transaction  is not fraud\n* Predicting a transaction  is not fraud and  transaction is  fraud\n\n### Which case is more important? \n* Predicting that transaction is not fraud but it is Fraud. It might enable lot of criminal activities and heavy loss to the bank \n\n### How to reduce this loss i.e need to reduce False Negative?\n* Company  would want `Recall` to be maximized, greater the Recall higher the chances of minimizing false Negative. Hence, the focus should be on increasing Recall or minimizing the false Negative or in other words identifying the True Positive(i.e. Class 1) so that the Company can identify the fraud transaction.","61ba22b6":"**Early stopping:** \n\nDuring training, the model is evaluated on a holdout validation dataset after each epoch. If the performance of the model on the validation dataset starts to degrade or no improvement (e.g. loss begins to increase or accuracy begins to decrease), then the training process is stopped after the certian interations.The model at the time that training is stopped is then used and is known to have good generalization performance.\n\nThis procedure is called \u201cearly stopping\u201d and is perhaps one of the oldest and most widely used forms of neural network regularization.\n\n**Weight Initialization**\n\nWeight initialization is an important consideration in the design of a neural network model.\n\nThe nodes in neural networks are composed of parameters referred to as weights used to calculate a weighted sum of the inputs.\n\nNeural network models are fit using an optimization algorithm called stochastic gradient descent that incrementally changes the network weights to minimize a loss function, hopefully resulting in a set of weights for the mode that is capable of making useful predictions.\n\nThis optimization algorithm requires a starting point in the space of possible weight values from which to begin the optimization process. Weight initialization is a procedure to set the weights of a neural network to small random values that define the starting point for the optimization (learning or training) of the neural network model.\n\nThere are many WI techniques as follows:\n\n1) Random normal initialization\n\n2) Random Uniform initialization\n\n3) Xaviour Initialization\n\n4) He Initialization \n\n\n\n\n","6ceb47d2":"As seen, the correlations cannot be properly visualized because of the imbalance in the dataset. This is because the correlation matrix is affected by the high imbalance betwen the classes. But we can still analyse that there is high negative correlation between principle component ( v2 and v5 ) and Amount","a1eeb024":"Plotting confusion matrix ","06dd4630":"## Random Forest","1f3032ea":"* This shows a complete imbalance of classes. There are 99.82% 'Genuine' (0) instances and only 0.17% 'Fraudulent' (1) instances. This means that we are aiming to predict anomalous events.","d6c31f28":"##Importing data","7a4e04d1":"### Model compile [optimizers and loss functions]\n\nKeras model should be \"compiled\" prior to training\n\nTypes of loss (function) and optimizer should be designated\n","a25b867f":"###Data Pre-processing","9ab98613":"##Model Building","a83fee14":"##Let's Explore the data ","98e10dd2":"Let's Print confusion matrix ","e8efe598":"###Let's check the missing values ","b5f12b8f":"## Training [Forward pass and Backpropagation]\n\nTraining the model","8328db7c":"- while only 5 regular transactions are wrongly predicted as fraudulent, the model only detects 81% of the fraudulent transactions. As a consequence 19 fraudulent transactions are not detected (False Negatives).\n\n- Let's see if we can improve this performance with other machine learning \/ deep learning models in the rest of the notebook.","8b0dfcb2":"- The class 'Fraudulent' (y=1) is assigned a weight of 290 vs 0.5 for the class 'not fraudulent' due to the very low prevalence we detected during data exploration. This allows the model to give more importance to the errors made on fraudulent cases during training.","1a92182d":"* The transactions occur in a cyclic way. But the time feature does not provide any useful information as the time when the first transaction was initiated is not given. Thus, we'll drop this feature.","30fbcd3e":"* This shows that there are 284807 instances and 31 attributes including the class attribute.\n*  As you can see there are no null values in any of the column","0708b0e3":"### Weighted loss to account for large class imbalance in train dataset\n- we will adjust the class imbalance by giving additional weight to the loss associated to errors made on fraudulent transaction detection.\n\nWe will use our first ANN model and apply weighted loss\n\n\n Let's review the process:","39c6d525":"## Overview of Dataset","57901188":"**Conclusion:**","14158c12":"## Let's now explore Neural Network models"}}