{"cell_type":{"b99902d4":"code","7f630920":"code","8b5beb00":"code","c59e9300":"code","28cc2f56":"code","f341cb8a":"code","67c45f04":"code","0ebe5c10":"code","fac784f9":"code","7e1e4f5c":"code","ce40ed0e":"code","a1c3c97d":"code","4b8c4dfa":"code","900095c7":"code","56ddfe69":"code","ae96b7fd":"code","72905cce":"code","28baa432":"code","3069040c":"code","8fd09e50":"code","82669e59":"code","6274482a":"code","b21e2954":"code","5e44ff74":"code","cf1aa47a":"code","92ed16fb":"code","35e02f85":"code","0e1a0068":"markdown","352dc73d":"markdown","9fffdd49":"markdown","6f44ab51":"markdown","10b13d5c":"markdown","2ac182c9":"markdown","580f63e7":"markdown","43d4385f":"markdown","0ed1e0fa":"markdown","41ade532":"markdown","2a35c028":"markdown","7dc5b120":"markdown","e4caeb07":"markdown"},"source":{"b99902d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f630920":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n","8b5beb00":"df = pd.read_csv(\"\/kaggle\/input\/knn-data1\/KNN_Project_Data\")\ndf.head()","c59e9300":"##Gives us the information of the datatypes, memory usage etc\ndf.info()","28cc2f56":"df.describe()","f341cb8a":"print(df.isnull().sum())","67c45f04":"df.hist(figsize=(20,20))","0ebe5c10":"from pandas.plotting import scatter_matrix\np=scatter_matrix(df,figsize=(25, 25))","fac784f9":"##The pairs plot builds on two basic figures, the histogram and the scatter plot. The histogram on the diagonal allows us to see the distribution of a single variable while the scatter plots on the upper and lower triangles show the relationship (or lack thereof) between two variables.\n##For Reference: https:\/\/towardsdatascience.com\/visualizing-data-with-pair-plots-in-python-f228cf529166\nsns.pairplot(df, hue = 'TARGET CLASS')","7e1e4f5c":"#A heat map is a two-dimensional representation of information with the help of colors. \n#Heat maps can help the user visualize simple or complex information.\nplt.figure(figsize=(12,10)) \np=sns.heatmap(df.corr(), annot=True,cmap ='RdYlGn')","ce40ed0e":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(df.drop([\"TARGET CLASS\"],axis = 1),),\n        columns=['XVPM', 'GWTH', 'TRAT', 'TLLZ', 'IGGA',\n       'HYKR', 'EDFS', 'GUUB','MGJM','JHZC'])\n","a1c3c97d":"X.head()","4b8c4dfa":"y = df['TARGET CLASS']","900095c7":"#importing train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1\/3,random_state=42, stratify=y)","56ddfe69":"# Finding the best K value\nfrom sklearn.neighbors import KNeighborsClassifier\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","ae96b7fd":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","72905cce":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","28baa432":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')\n","3069040c":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(14)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","8fd09e50":"#Desicion Boundary\nfrom mlxtend.plotting import plot_decision_regions\nvalue = 20000\nwidth = 20000\nplot_decision_regions(X.values, y.values, clf=knn, legend=2, \n                      filler_feature_values={2: value, 3: value, 4: value, 5: value, 6: value, 7: value, 8: value, 9: value},\n                      filler_feature_ranges={2: width, 3: width, 4: width, 5: width, 6: width, 7: width, 8: width, 9: width},\n                      X_highlight=X_test.values)\n\nplt.title('KNN')\nplt.show()","82669e59":"from sklearn.metrics import confusion_matrix\n#let us get the predictions using the classifier we had fit above\ny_pred = knn.predict(X_test)\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","6274482a":"y_pred = knn.predict(X_test)\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","b21e2954":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","5e44ff74":"from sklearn.metrics import roc_curve\ny_pred_proba = knn.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)","cf1aa47a":"plt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Knn(n_neighbors=11) ROC curve')\nplt.show()","92ed16fb":"#Area under ROC curve\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_proba)","35e02f85":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","0e1a0068":"# Statistical analysis","352dc73d":"StandardScaler performs the task of Standardization. Usually a dataset contains variables that are different in scale. For example, the dataset will contain an insulin column with values on scale 20\u201370 and Glucose column with values on scale 80\u2013200. As these two columns are different in scale, they are standardized to have a common scale while building a machine learning model.","9fffdd49":"About Stratify : Stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.","6f44ab51":"# Building the model","10b13d5c":"The confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.","2ac182c9":"Reading the dataset","580f63e7":"Grid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.","43d4385f":"# Model Performance Analysis","0ed1e0fa":"Necessary Libraries","41ade532":"The best result is captured at k = 9 and k = 14 hence 11 is used for the final model. Since both have same accuracy score, we can consider any one","2a35c028":"ROC (Receiver Operating Characteristic) Curve tells us about how good the model can distinguish between two things (e.g If a patient has a disease or no). Better models can accurately distinguish between the two. Whereas, a poor model will have difficulties in distinguishing between the two","7dc5b120":"# Visualising the Result","e4caeb07":"# KNN (K- Nearest Neighbors) Algorithm\n\nKNN is one of the simplest forms of machine learning algorithms mostly used for classification. It classifies the data point on how its neighbor is classified.\n\nKNN classifies the new data points based on the similarity measure of the earlier stored data points. For example, if we have a dataset of cars and bikes. KNN will store similar measures like shape and build. When a new object comes it will check its similarity with the build and shape."}}