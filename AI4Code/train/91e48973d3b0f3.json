{"cell_type":{"8e7de949":"code","16624099":"code","f0ebd902":"code","fcd34049":"code","1471da38":"code","9cf8cc98":"code","c40341f4":"code","84b2f514":"code","9c862862":"code","92cfeca9":"code","f67fcc30":"code","6f322ae0":"markdown","4df5459e":"markdown","9f071307":"markdown","f16432b8":"markdown","5a1fbad0":"markdown","d0dd90fe":"markdown","ba881182":"markdown","a20f951d":"markdown","b5960ac7":"markdown","985dfa2e":"markdown","6f0c328f":"markdown","b053addd":"markdown"},"source":{"8e7de949":"!pip install utils\n!pip install livelossplot\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport utils\nimport os\n%matplotlib inline\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\nfrom tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\n\nfrom IPython.display import SVG, Image\nfrom livelossplot.tf_keras import PlotLossesCallback\nimport tensorflow as tf\nprint(\"Tensorflow version:\", tf.__version__)","16624099":"for expression in os.listdir(\"..\/input\/facial-expression-dataset\/train\/train\"):\n    print(str(len(os.listdir(\"..\/input\/facial-expression-dataset\/train\/train\/\" + expression))) + \" \" + expression + \" images\")","f0ebd902":"img_size = 48\nbatch_size = 64\n\ndatagen_train = ImageDataGenerator(horizontal_flip=True)\n\ntrain_generator = datagen_train.flow_from_directory(\"..\/input\/facial-expression-dataset\/train\/train\/\",\n                                                    target_size=(img_size,img_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=True)\n\ndatagen_validation = ImageDataGenerator(horizontal_flip=True)\nvalidation_generator = datagen_validation.flow_from_directory(\"..\/input\/facial-expression-dataset\/test\/test\/\",\n                                                    target_size=(img_size,img_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=False)","fcd34049":"# Initialising the CNN\nmodel = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(7, activation='softmax'))\n\nopt = Adam(lr=0.0005)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","1471da38":"plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\nImage('model.png',width=400, height=200)","9cf8cc98":"\n%%time\n\nepochs = 15\nsteps_per_epoch = train_generator.n\/\/train_generator.batch_size\nvalidation_steps = validation_generator.n\/\/validation_generator.batch_size\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=2, min_lr=0.00001, mode='auto')\ncheckpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy',\n                             save_weights_only=True, mode='max', verbose=1)\ncallbacks = [PlotLossesCallback(), checkpoint, reduce_lr]\n\nhistory = model.fit(\n    x=train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    validation_data = validation_generator,\n    validation_steps = validation_steps\n)","c40341f4":"model_json = model.to_json()\nmodel.save_weights('model_weights.h5')\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","84b2f514":"from tensorflow.keras.models import model_from_json\nimport numpy as np\n\nimport tensorflow as tf\n\n\nclass FacialExpressionModel(object):\n\n    EMOTIONS_LIST = [\"Angry\", \"Disgust\",\n                    \"Fear\", \"Happy\",\n                    \"Neutral\", \"Sad\",\n                    \"Surprise\"]\n\n    def __init__(self, model_json_file, model_weights_file):\n        # load model from JSON file\n        with open(model_json_file, \"r\") as json_file:\n            loaded_model_json = json_file.read()\n            self.loaded_model = model_from_json(loaded_model_json)\n\n        # load weights into the new model\n        self.loaded_model.load_weights(model_weights_file)\n        self.loaded_model.make_predict_function()\n\n    def predict_emotion(self, img):\n        self.preds = self.loaded_model.predict(img)\n        return FacialExpressionModel.EMOTIONS_LIST[np.argmax(self.preds)]\n","9c862862":"  \nimport cv2\nimport numpy as np\n\nfacec = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\nmodel = FacialExpressionModel(\".\/model.json\", \".\/model_weights.h5\")\nfont = cv2.FONT_HERSHEY_SIMPLEX\n\nclass VideoCamera(object):\n    def __init__(self):\n        self.video = cv2.VideoCapture(0)\n\n    def __del__(self):\n        self.video.release()\n\n    # returns camera frames along with bounding boxes and predictions\n    def get_frame(self):\n        _, fr = self.video.read()\n        gray_fr = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n        faces = facec.detectMultiScale(gray_fr, 1.3, 5)\n\n        for (x, y, w, h) in faces:\n            fc = gray_fr[y:y+h, x:x+w]\n\n            roi = cv2.resize(fc, (48, 48))\n            pred = model.predict_emotion(roi[np.newaxis, :, :, np.newaxis])\n\n            cv2.putText(fr, pred, (x, y), font, 1, (255, 255, 0), 2)\n            cv2.rectangle(fr,(x,y),(x+w,y+h),(255,0,0),2)\n\n        return fr","92cfeca9":"def gen(camera):\n    while True:\n        frame = camera.get_frame()\n        cv2.imshow('Facial Expression Recognization',frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    cv2.destroyAllWindows()","f67fcc30":"gen(VideoCamera())","6f322ae0":"## Represent Model as JSON String","4df5459e":"## Create CNN Model","9f071307":"## Getting frames and doing prediction","f16432b8":"## Class for loading model and weights","5a1fbad0":"## Generate Training and Validation Batches","d0dd90fe":"## Training and Evaluating Model","ba881182":"## Importing Libraries","a20f951d":"## Visualize Model Architecture","b5960ac7":"## Getting total number of images of each category","985dfa2e":"## Function for showing output video","6f0c328f":"<h2 align=center> Facial Expression Recognition<\/h2>","b053addd":"## Running the code"}}