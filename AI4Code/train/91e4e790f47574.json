{"cell_type":{"b8e727d7":"code","50b8f766":"code","117a5870":"code","4c8c8c84":"code","1e34a9a3":"code","0c6ff4a3":"code","57edcf0b":"code","6bec2c7d":"code","03434385":"code","40d8b514":"code","640eec9a":"code","36d286bb":"code","94e6e356":"code","0931abf2":"code","dbd44557":"markdown","9c00c67e":"markdown","26a44e15":"markdown","5519291c":"markdown","9b0b6c75":"markdown","57152446":"markdown","73ca634e":"markdown","e061921e":"markdown","6c794fe7":"markdown","f9110fe8":"markdown","34ce5343":"markdown","8ceac289":"markdown","20a4197e":"markdown","fdd8167f":"markdown","e489e2e7":"markdown","c364a4d5":"markdown","08473940":"markdown","022562fc":"markdown"},"source":{"b8e727d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n# \u0420\u0430\u0437\u0430\u0440\u0445\u0438\u0432\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0444\u0430\u0439\u043b\u043e\u0432\n\nimport zipfile\n\n# let's look at the contents of the archive\ndata_r = zipfile.ZipFile('..\/input\/platesv2\/plates.zip', 'r')\n# data_r.printdir()\n\n# extract the contents\ndata_r.extractall()\n\n# let's look at the working directory\nos.getcwd()\n\n# contents of the working directory\n#print(os.listdir(\"..\/working\"))\n\n# contents of our directory\ndata_root = '..\/working\/plates\/'\nprint(os.listdir(data_root))","50b8f766":"!rm -rf train val \n\nimport shutil\nfrom tqdm import tqdm\n\ntrain_dir = 'train'\nval_dir = 'val'\n\nclass_names = ['cleaned', 'dirty']\n\nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok = True)\n        \nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 5!= 0:\n            dest_dir = os.path.join(train_dir, class_name)\n            shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))\n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n            shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))\n            dest_dir = os.path.join(train_dir, class_name)\n            shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","117a5870":"import torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nimport time\nimport copy\n\nfrom torchvision import transforms, models\n\ntorch.manual_seed(17)\n\nf = 100\n# transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\ntrain_transforms_1 = transforms.Compose([\n    transforms.CenterCrop(f),\n    transforms.RandomHorizontalFlip(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_2 = transforms.Compose([\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.RandomVerticalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_3 = transforms.Compose([\n    transforms.RandomRotation(50),\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_4 = transforms.Compose([\n    transforms.RandomVerticalFlip(),\n    transforms.RandomHorizontalFlip(),\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_5 = transforms.Compose([\n    transforms.RandomVerticalFlip(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(50),\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_6 = transforms.Compose([\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_7 = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.09, p=0.75, interpolation=3, fill=255),\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_8 = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.09, p=0.75, interpolation=3, fill=255),\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.RandomVerticalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_9 = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.09, p=0.75, interpolation=3, fill=255),\n    transforms.RandomRotation(50),\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_10 = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.09, p=0.75, interpolation=3, fill=255),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomHorizontalFlip(),\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    #transforms.RandomRotation(50),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_11 = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.09, p=0.75, interpolation=3, fill=255),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(50),\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_transforms_12 = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.09, p=0.75, interpolation=3, fill=255),\n    transforms.CenterCrop(f),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\n\nval_transforms_1 = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.CenterCrop(f),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\n\ntrain_dataset_1 = torchvision.datasets.ImageFolder(train_dir, train_transforms_1)\ntrain_dataset_2 = torchvision.datasets.ImageFolder(train_dir, train_transforms_2)\ntrain_dataset_3 = torchvision.datasets.ImageFolder(train_dir, train_transforms_3)\ntrain_dataset_4 = torchvision.datasets.ImageFolder(train_dir, train_transforms_4)\ntrain_dataset_5 = torchvision.datasets.ImageFolder(train_dir, train_transforms_5)\ntrain_dataset_6 = torchvision.datasets.ImageFolder(train_dir, train_transforms_6)\ntrain_dataset_7 = torchvision.datasets.ImageFolder(train_dir, train_transforms_7)\ntrain_dataset_8 = torchvision.datasets.ImageFolder(train_dir, train_transforms_8)\ntrain_dataset_9 = torchvision.datasets.ImageFolder(train_dir, train_transforms_9)\ntrain_dataset_10 = torchvision.datasets.ImageFolder(train_dir, train_transforms_10)\ntrain_dataset_11 = torchvision.datasets.ImageFolder(train_dir, train_transforms_11)\ntrain_dataset_12 = torchvision.datasets.ImageFolder(train_dir, train_transforms_12)\n\n\nval_dataset_1 = torchvision.datasets.ImageFolder(val_dir, val_transforms_1)\n\ntrain_dataset = torch.utils.data.ConcatDataset([\n                                                train_dataset_1, train_dataset_2,\n                                                train_dataset_3, train_dataset_4,\n                                                train_dataset_5, train_dataset_6,\n                                                train_dataset_7, train_dataset_8,\n                                                train_dataset_9, train_dataset_10,\n                                                train_dataset_11, train_dataset_12,\n                                               ])\n\nval_dataset = torch.utils.data.ConcatDataset([val_dataset_1])\n\n\n# \u0412\u044b\u0431\u043e\u0440 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u0431\u0430\u0442\u0447\u0430\nbatch_size = 10\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size = batch_size, shuffle = True, num_workers=batch_size)\n\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size = batch_size, shuffle = True, num_workers=batch_size)\n\n# \u0412\u0430\u0436\u043d\u0430\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430. \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0434\u043e\u043b\u0436\u043d\u043e \u0434\u0435\u043b\u0438\u0442\u044c\u0441\u044f \u043d\u0430 \u0440\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nprint(len(train_dataloader), len(train_dataset))\nprint(len(val_dataloader), len(val_dataset))","4c8c8c84":"def show_input_image(input_tensor, title = ''):\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = image * std + mean\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n    \nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input_image(x_item, class_names[y_item])","1e34a9a3":"model = models.resnet152(pretrained=True)\n\n# \u0417\u0430\u043c\u043e\u0440\u043e\u0437\u0438\u0442\u044c \u0432\u0441\u0435 \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0435 \u0441\u043b\u043e\u0438\nfor param in model.parameters():\n    param.requires_grad = False\n    \n# \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0441\u043b\u043e\u0439\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)\n\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\n","0c6ff4a3":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\ndef train_model(model, loss, optimizer, scheduler, num_epochs):\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                #scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                dataloader = val_dataloader\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.\n            running_acc = 0.\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                # forward and backward\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss_value.item()\n                running_acc += (preds_class == labels.data).float().mean()\n\n            epoch_loss = running_loss \/ len(dataloader)\n            epoch_acc = running_acc \/ len(dataloader)\n            \n            if phase == 'val':\n                accuracies.append(epoch_acc)\n                losses.append(epoch_loss)\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n\n    return model","57edcf0b":"# \u0412\u044b\u0432\u043e\u0434 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\nsns.set(rc={'figure.figsize':(15, 6)});\n\n\ndef acc_loss_graph(accuracies, losses, net_list, save_file_name='plot.png', download=False):\n  fig, (ax1, ax2) = plt.subplots(1, 2)\n\n  for experiment_id in net_list:\n      ax1.plot(accuracies, label=experiment_id)\n  ax1.legend()\n  ax1.set_title('Validation Accuracy')\n  fig.tight_layout()\n\n  for experiment_id in net_list:\n\n      ax2.plot(losses, label=experiment_id)\n  ax2.legend()\n  ax2.set_title('Validation Loss');\n\n  fig.tight_layout()\n\n  if download:\n    fig.savefig(save_file_name)\n#     files.download(save_file_name) #\u0434\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u0441\u043a\u0438 \u0441 Colab","6bec2c7d":"optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n\n# \u0423\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u0435 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0439 step_size \u044d\u043f\u043e\u0445\u0435\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n\naccuracies = []\nlosses = []\n\ntrain_model(model, loss, optimizer, scheduler, num_epochs=100);","03434385":"acc_loss_graph(accuracies, losses, ['resnet152'])","40d8b514":"test_transforms = {\n                      '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 140 ': transforms.Compose([\n    transforms.CenterCrop(140),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                     '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 135': transforms.Compose([\n    transforms.CenterCrop(135),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]), \n                      '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 130': transforms.Compose([\n    transforms.CenterCrop(130),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 125': transforms.Compose([\n    transforms.CenterCrop(125),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 120': transforms.Compose([\n    transforms.CenterCrop(120),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 115': transforms.Compose([\n    transforms.CenterCrop(115),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 110': transforms.Compose([\n    transforms.CenterCrop(110),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 105': transforms.Compose([\n    transforms.CenterCrop(105),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 100': transforms.Compose([\n    transforms.CenterCrop(100),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                     '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 95': transforms.Compose([\n    transforms.CenterCrop(95),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                       '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 90': transforms.Compose([\n    transforms.CenterCrop(90),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                       '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 85': transforms.Compose([\n    transforms.CenterCrop(85),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                       '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 80': transforms.Compose([\n    transforms.CenterCrop(80),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 75': transforms.Compose([\n    transforms.CenterCrop(75),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),                                         \n                       '\u0412\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439 70': transforms.Compose([\n    transforms.CenterCrop(70),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),                                                           \n                     }","640eec9a":"test_dir = 'test'\n\nshutil.copytree(os.path.join(data_root, 'test'), \n                os.path.join(test_dir, 'unknown'))\n\n# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438 \u0441 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438\n# test_dir = 'test'\n# shutil.copytree(os.path.join(data_root, 'test'), \n#                 os.path.join(test_dir, 'unknown'))\n\n# \u041a\u043b\u0430\u0441\u0441, \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0432\u0435\u0440\u043d\u0435\u0442 turple \u0441 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\u043c \u0438 \u0441 \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435\u043c \u044d\u0442\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\nclass ImageFolderWithPaths(torchvision.datasets.ImageFolder): \n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index) \n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n    \n\ndf = pd.DataFrame\n# \u0414\u043b\u044f \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u043e \u0440\u0430\u0437\u043d\u044b\u043c \u0432\u044b\u0440\u0435\u0437\u043a\u0430\u043c \u0438\u0437 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f. \n# \u0412\u043f\u043e\u043b\u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u0441\u043b\u0443\u0447\u0438\u0442\u044c\u0441\u044f, \u0447\u0442\u043e \u043d\u0430 \u043c\u0435\u043d\u044c\u0448\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438 \u043d\u0435\u0442 \u0433\u0440\u044f\u0437\u0438, \u0430 \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0435\u043c \u0435\u0441\u0442\u044c. \n# \u0422\u043e\u0433\u0434\u0430 \u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u0438\u0435 \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u0438\u0441\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044e.\n\nfor (_,tranforms) in test_transforms.items():\n    test_dataset = ImageFolderWithPaths('\/kaggle\/working\/test', tranforms) \n    test_dataloader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    \n    # \u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0432 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 eval \n    model.eval()\n    test_predictions = []  \n    test_img_paths = [] \n    # \u0418 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\n    for inputs, labels, paths in tqdm(test_dataloader):\n        inputs = inputs.to(device) \n        labels = labels.to(device)  \n        with torch.set_grad_enabled(False):\n            preds = model(inputs) \n        test_predictions.append(\n            torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n        test_img_paths.extend(paths)\n    test_predictions = np.concatenate(test_predictions)\n    \n    # \u0417\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\n    submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})\n    submission_df['id'] = submission_df['id'].str.replace('\/kaggle\/working\/test\/unknown\/', '')\n    submission_df['id'] = submission_df['id'].str.replace('.jpg', '')\n    submission_df.set_index('id', inplace=True)\n    \n    try : df = df.merge(submission_df, how='inner', on='id') \n        \n    # \u0414\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438\n    except BaseException: \n        df = submission_df \ndf.head(8)","36d286bb":"# \u0417\u0430\u043f\u043e\u043c\u043d\u0438\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b, \u0447\u0442\u043e\u0431\u044b \u043d\u0430\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043f\u043e\u0440\u043e\u0433, \u043f\u0440\u0438 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043b\u0443\u0447\u0448\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b.\nnew_df = df.copy(deep=True)\n#df.head(14)\n","94e6e356":"!rm -rf submission.csv\nnew_df = df.copy(deep=True)\nnew_df['mean'] = new_df.mean(axis=1)\nnew_df.drop(new_df.columns[:-1], axis='columns', inplace=True)\nnew_df['label'] = new_df['mean'].map(lambda pred: 'dirty' if pred > 0.50 else 'cleaned')\nnew_df.drop(new_df.columns[:-1], axis='columns', inplace=True)\nnew_df.head(13)","0931abf2":"new_df.to_csv('submission.csv')","dbd44557":"## **\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445**","9c00c67e":"### \u0412\u044b\u0432\u043e\u0434 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432","26a44e15":"### \u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432","5519291c":"### \u0423\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0437\u0430 \u0441\u0447\u0435\u0442 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438","9b0b6c75":"\u0414\u043b\u044f \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u043e \u0440\u0430\u0437\u043d\u044b\u043c \u0432\u044b\u0440\u0435\u0437\u043a\u0430\u043c \u0438\u0437 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f. \n\u0412\u043f\u043e\u043b\u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u0441\u043b\u0443\u0447\u0438\u0442\u044c\u0441\u044f, \u0447\u0442\u043e \u043d\u0430 \u043c\u0435\u043d\u044c\u0448\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438 \u043d\u0435\u0442 \u0433\u0440\u044f\u0437\u0438, \u0430 \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0435\u043c \u0435\u0441\u0442\u044c. \n\u0422\u043e\u0433\u0434\u0430 \u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u0438\u0435 \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u0438\u0441\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044e.","57152446":"### \u041f\u0440\u043e\u0441\u043c\u043e\u0442\u0440 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435","73ca634e":"### \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0432\u044b\u0432\u043e\u0434\u0430 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432","e061921e":"## **\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435**","6c794fe7":"### \u0424\u0443\u043a\u043d\u0446\u0438\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","f9110fe8":"### \u0420\u0430\u0437\u0431\u0438\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","34ce5343":"### \u041f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u043c\u0443 \u0444\u043e\u0440\u043c\u0430\u0442\u0443('cleaned', 'dirty')","8ceac289":"### \u0417\u0430\u043f\u0443\u0441\u043a \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","20a4197e":"### \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f","fdd8167f":"### \u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","e489e2e7":"### \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445.","c364a4d5":"### \u0412\u044b\u0431\u043e\u0440 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438","08473940":"# **\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u0430\u0440\u0435\u043b\u043e\u043a \u043d\u0430 \u0433\u0440\u044f\u0437\u043d\u044b\u0435 \u0438 \u0447\u0438\u0441\u0442\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438. \u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 Transfer learning. ResNet152.**","022562fc":"## **\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445**"}}