{"cell_type":{"2e1aafd5":"code","5d210d2e":"code","fc61cfd9":"code","70e4feff":"code","b96961cf":"code","7798a549":"code","bea60542":"code","99725dac":"code","813694e0":"code","b1031a3b":"code","9b444feb":"code","11c5ff48":"code","6e84f065":"code","319e5a16":"code","bd05404a":"code","46a01335":"code","17e57783":"code","7526ede7":"code","76b65508":"code","4e718c41":"code","4dede925":"code","1e6afa7a":"code","7e1e8c64":"code","027dd160":"code","1850f15c":"markdown","34a31e62":"markdown"},"source":{"2e1aafd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d210d2e":"df_train = pd.read_csv('\/kaggle\/input\/mlub-learning-to-count\/train.txt',sep=' ',header=None)\n\n# take only the first 200 images\n#df_train = df_train.head(500)","fc61cfd9":"df_train","70e4feff":"df_train.describe()","b96961cf":"import imageio\nfrom skimage import transform,io\nimport matplotlib.pyplot as plt\n# read images and store into a np array\n\ndata_dir = '\/kaggle\/input\/mlub-learning-to-count\/train\/'\nim_size = 128\nN = df_train.shape[0]\nX = np.zeros((N, im_size,im_size))\ny = np.zeros((N))\ncont =0\n\nfor ind, item in df_train.iterrows():\n    im       = imageio.imread(data_dir + item[0])\/255. \n    small_im = transform.resize(im, (im_size,im_size), mode='symmetric', preserve_range=True)\n    X[cont, :,:] = small_im\n    y[cont] = item[1]\n    cont+=1\n    \nplt.imshow(small_im,cmap='gray')\nplt.show()","7798a549":"im = imageio.imread(data_dir + item[0])\nnum = item[1]\nprint(\"Tama\u00f1o de la imagen: \" + str(im.shape))\nprint(\"Etiqueta num de personas: \" + str(num))\n\n\nplt.imshow(im)\nplt.show()","bea60542":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range = 15,\n    width_shift_range = 0.1,\n    height_shift_range = 0.1,\n    shear_range = 0.1,\n    zoom_range = 0.1,\n    horizontal_flip = True,\n    vertical_flip = True,\n    fill_mode = \"nearest\")","99725dac":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val= train_test_split(X, y, test_size=0.33, random_state=66)\n\nX_train = X_train.reshape(X_train.shape + (1,))\nX_val = X_val.reshape(X_val.shape + (1,))","813694e0":"from sklearn.metrics import mean_squared_error\n\"\"\"\nplt.plot(rfc_predict,y_val,'.')\n\nprint(np.sqrt(mean_squared_error(rfc_predict,y_val)))\n\"\"\"","b1031a3b":"## With a Neural Network\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dropout, Flatten, Conv2D, MaxPool2D, Dense\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)","9b444feb":"model = keras.models.Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=5, padding=\"same\", activation=\"relu\", input_shape=[128, 128, 1]))\nmodel.add(Conv2D(filters=32, kernel_size=5, padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=2, strides=2, padding='valid'))\n\nmodel.add(Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=2, strides=2, padding='valid'))\n\nmodel.add(Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=2, strides=2, padding='valid'))\n\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation=\"relu\"))\nmodel.add(Dense(512, activation=\"relu\"))\nmodel.add(Dense(64, activation=\"relu\"))\nmodel.add(keras.layers.Dense(1, activation=\"linear\"))","11c5ff48":"model.layers","6e84f065":"model.summary()\n","319e5a16":"#model = keras.models.load_model('\/kaggle\/working\/model_counts.hdf5')","bd05404a":"adam = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad = True)\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"Adam\",metrics=[\"mean_squared_error\",tf.keras.metrics.MeanAbsoluteError()])","46a01335":"from keras.callbacks import ReduceLROnPlateau\nearlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 75)\n\n# Guardamos el mejor modelo con menor error de validaci\u00f3n \ncheckpointer = ModelCheckpoint(filepath = \"model_counts.hdf5\", verbose = 1, save_best_only=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', mode = 'auto', factor=0.2, verbose = 1, patience=15, min_lr=0.0001)","17e57783":"history = model.fit(train_datagen.flow(X_train, y_train, batch_size=64),steps_per_epoch=len(X_train) \/\/ 64, \n                    epochs=500, validation_data=(X_val, y_val),\n                    verbose=1,callbacks=[earlystopping, checkpointer,reduce_lr])","7526ede7":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","76b65508":"model_json = model.to_json()\nwith open(\"model.json\",\"w\") as json_file:\n    json_file.write(model_json)","4e718c41":"#model = keras.models.load_model('\/kaggle\/working\/model_counts.hdf5')","4dede925":"import pandas as pd\n\npd.DataFrame(history.history)[['loss','val_loss']].plot(figsize=(8, 5))\nplt.grid(True)\n#plt.gca().set_ylim(0.4, 0.8)\nplt.show()","1e6afa7a":"y_model1 = model.predict(X_val)\n\nplt.plot(y_model1,y_val,'.')\nnp.sqrt(mean_squared_error(y_model1,y_val))","7e1e8c64":"## evaluate test and generate submission\ntest_dir = '\/kaggle\/input\/mlub-learning-to-count\/test\/test\/'\nim_size = 128\nN = 500\nX_test = np.zeros((N, im_size,im_size))\ncont =0\n\nfor x in range(500):\n    im       = imageio.imread(test_dir + 'test_composite'+str(x).zfill(9) + '.png')\/255.\n    small_im = transform.resize(im, (im_size,im_size), mode='symmetric', preserve_range=True)\n    X_test[cont, :,:] = small_im\n    cont+=1\n    \nplt.imshow(small_im)\nplt.show()","027dd160":"X_test = X_test.reshape(X_test.shape + (1,))\n\n# create the file to make the sumbission\n\ny_test = model.predict(X_test)\ny_test = [int(x[0]) for x in y_test]\n\ndf_output = pd.DataFrame(y_test)\ndf_output.index.name = 'index'\ndf_output.columns = ['prediction']\ndf_output.to_csv('output.csv')\n","1850f15c":"# Aumentacion de las im\u00e1genes","34a31e62":"# Estructura del dataset"}}