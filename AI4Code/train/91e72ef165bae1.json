{"cell_type":{"cf115297":"code","3854e4d3":"code","223b2fc7":"code","8416bf31":"code","e3f714e6":"code","1af5ede2":"code","46609c08":"code","59795e65":"code","72645b92":"markdown","c47d542e":"markdown","e69c87ad":"markdown","1d7291d5":"markdown","b62fbb8d":"markdown"},"source":{"cf115297":"# import libraries\nimport os\nimport shutil\nimport numpy as np\nimport glob   \nimport keras.backend as K\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model, load_model\nfrom keras import optimizers\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nfrom keras.initializers import glorot_uniform\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nimport scipy.misc\n#from keras.applications import ResNet50\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.resnet50 import preprocess_input\n\n# for reading images\nfrom matplotlib.pyplot import imshow\n%matplotlib inline\n\n# channels last is the format used by tensorflow \nK.set_image_data_format('channels_last')\nK.set_learning_phase(1)","3854e4d3":"# set to where the 'flowers' directory is located\ndata_dir = '..\/input\/flowers-recognition\/flowers'\n\n# Training data dir\ntraining_dir = 'Train'\n\n# Test data dir\ntesting_dir = 'Test'\n\n# Ratio of training and testing data\ntrain_test_ratio = 0.8 \n\n\ndef split_dataset_into_test_and_train_sets(all_data_dir = data_dir, training_data_dir = training_dir, testing_data_dir=testing_dir, train_test_ratio = 0.8):\n\n    # recreate test and train directories if they don't exist\n    if not os.path.exists(training_data_dir):\n        os.mkdir(training_data_dir)\n\n    if not os.path.exists(testing_data_dir):\n        os.mkdir(testing_data_dir)               \n    \n    num_training_files = 0\n    num_testing_files = 0\n\n    # iterate through the data directory \n    for subdir, dirs, files in os.walk(all_data_dir):\n        \n        category_name = os.path.basename(subdir)\n\n        if category_name == os.path.basename(all_data_dir):\n            continue\n\n        training_data_category_dir = training_data_dir + '\/' + category_name\n        testing_data_category_dir = testing_data_dir + '\/' + category_name\n        \n        # creating subdirectory for each sub category\n        if not os.path.exists(training_data_category_dir):\n            os.mkdir(training_data_category_dir)   \n\n        if not os.path.exists(testing_data_category_dir):\n            os.mkdir(testing_data_category_dir)\n            \n        file_list = glob.glob(subdir + '\/*.jpg')\n\n        print(str(category_name) + ' has ' + str(len(files)) + ' images') \n        random_set = np.random.permutation((file_list))\n        \n        # copy percentage of data from each category to train and test directory\n        train_list = random_set[:round(len(random_set)*(train_test_ratio))] \n        test_list = random_set[-round(len(random_set)*(1-train_test_ratio)):]\n        \n        for lists in train_list : \n            shutil.copy(lists, training_data_dir + '\/' + category_name + '\/' )\n            num_training_files += 1\n  \n        for lists in test_list : \n            shutil.copy(lists, testing_data_dir + '\/' + category_name + '\/' )\n            num_testing_files += 1\n  \n\n    print(\"Processed \" + str(num_training_files) + \" training files.\")\n    print(\"Processed \" + str(num_testing_files) + \" testing files.\")","223b2fc7":"# split into train and test directories\nsplit_dataset_into_test_and_train_sets()","8416bf31":"# number of classes \nnum_classes = 5\n\ndef get_model():\n    \n    # Get base model: ResNet50 \n    base_model = ResNet50(weights='imagenet', include_top=False)\n    \n    # freeze the layers in base model\n    for layer in base_model.layers:\n        layer.trainable = False\n        \n    # Get the output from the base model \n    base_model_ouput = base_model.output\n    \n    # Adding our own layers at the end\n    # global average pooling: computes the average of all values in the feature map\n    x = GlobalAveragePooling2D()(base_model_ouput)\n    \n    # fully connected and 5-softmax layer\n    x = Dense(512, activation='relu')(x)\n    x = Dense(num_classes, activation='softmax', name='fcnew')(x)\n    \n    model = Model(inputs=base_model.input, outputs=x)\n    return model","e3f714e6":"# Get the model\nmodel = get_model()\n\n# compile it\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n# summary of model\nmodel.summary()","1af5ede2":"# Using ImageDataGenerator for pre-processing\n\nimage_size = 224\nbatch_size = 64\n\n# help(ImageDataGenerator)\ntrain_data_gen = ImageDataGenerator(preprocessing_function = preprocess_input, \n                                    shear_range=0.2, zoom_range=0.2, \n                                    horizontal_flip=True)\n\n# do only basic preprocessing for validation data (no data augmentation)\nvalid_data_gen = ImageDataGenerator(preprocessing_function = preprocess_input)\n\n# create data generator objects\ntrain_generator = train_data_gen.flow_from_directory(training_dir, (image_size,image_size), batch_size=batch_size, class_mode='categorical')\nvalid_generator = valid_data_gen.flow_from_directory(testing_dir, (image_size,image_size), batch_size=batch_size, class_mode='categorical')","46609c08":"# Training the newly added layers \nepochs = 10\n\n# flow data (in batches) from directories (while simultaneously preprocessing\/augmenting\nmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=train_generator.n\/\/batch_size,\n    validation_data=valid_generator,\n    validation_steps=valid_generator.n\/\/batch_size,\n    epochs=epochs,\n    verbose=1)","59795e65":"epochs = 10\n\n# training the model after 140 layers\nsplit_at = 140\nfor layer in model.layers[:split_at]: layer.trainable = False\nfor layer in model.layers[split_at:]: layer.trainable = True\n    \nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Choosing lower learning rate for fine-tuning\n# learning rate is generally 10-1000 times lower than normal learning rate when we are fine tuning the initial layers\nsgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\nmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=train_generator.n\/\/batch_size,\n    validation_data=valid_generator,\n    validation_steps=valid_generator.n\/\/batch_size,\n    epochs=epochs,\n    verbose=1)","72645b92":"### Importing the Pre-Trained Model \n\nLet's now import the pretrained ResNet model. In the first experiment, we will use the pretrained weights (from Imagenet) of ResNet. The argument `include_top = False` specifies that we do not want to import the top layers (the last ones, which are typically pooling, FC, softmax etc.). We'll add some of our own last layers (a global average poooling layer and a final softmax) and train just those.","c47d542e":"### Freezing the Initial-n Layers and Training the Rest\n\nLet's now try another variant of transfer learning. We will freeze the first 140 layers of ResNet (with the hypopthesis that they have learnt to extract some useful generic features from their ImageNet experience) and train the rest of the layers.","e69c87ad":"\nIn this notebook, we will implement transfer learning in Python using the pre-trained ResNet model. \nWe will run two experiments - \n1. **Freezing the base model weights**, adding a few layers to it at the end (fully connected etc.) and training the newly added layers, and \n2. **Freezing the first 140 layers of ResNet** and retraining the rest.\n\nApart from this, you will learn **two important practical preprocessing techniques** in this notebook - **data augmentation** and **data generators**. The notebook is divideded into the following sections:\n1. Importing libraries\n2. Splitting into train and test set\n3. Importing the pretrained ResNet model\n4. Data Generators: Preprocessing and Generating Batch-Wise Data (On the Fly)\n5. Training the Base Model (Using Batch-Wise Data Generation)\n6. Freezing the initial-n layers and training the rest","1d7291d5":"Data Generators: Preprocessing and Generating Batch-Wise Data (On the Fly)\nWe will now implement an incredibly useful preprocessing technique - data augmentation using data generators.\n\nYou will learn preprocessing techniques in detail in the next industry session, though they're quire easy to understand anyway. Here's a quick overview.\n\nData Augmentation is a commonly used technique in image processing used to 'create' more training data. It basically modifies the original training images a bit (e.g. rotates them by a few degrees, changes the colour shades a little, etc.) to 'create' (or augment) new training images. The basic reason to do this is to increase the amount of variance in the training data. It is possible to do this with images because if you rotate the image of (say) a dog (or change the colours a bit, stretch the image horizontally etc.), it stays a dog. Thus, you can create many images from each training image while the label stays the same.\n\nIn the code below, we have specified the augmentation techniques as shear_range=0.2, zoom_range=0.2, horizontal_flip=True. Shear 'stretches' the images, zoom_range zooms them in, and horizontal_flip 'flips' them around horizontally.\n\nNow, in the code below, you will notice that we have something called ImageDataGenerator - lets understand what it does.\n\nData generators are used to feed data points in batches to the model. The main reason to use them is that they are efficient (compared to feeding one data point at a time, or all of them at once which will require a lot of memory). What's cooler about them is that they (at least in keras) can preprocess the images and create augmented ones on the fly, i.e. as the batches are fed to the model, the augmented images are created and preprocessed. This eliminates the need to store the augmented images separately.\n\nAn important point to note is that you never augment the validation or test sets, only the training set. This is because test (and validation) sets are supposed to be representative of the real images you'll get in the real world. However, you do apply basic preprocessing to validation and test sets (scaling\/centering them etc.).\n\nIn the code below, the method flow_from_directory 'flows' the data in batches from the training and test directories. It is an instance of ImageDataGenerator where we specify the preprocessing and augmentation techniques that we want to use. In this case, we are just using the standard preprocessing techniques that come with the preprocess_input module in keras.","b62fbb8d":"### Training the Base Model (Using Batch-Wise Data Generation)\n\nLet's now train the model. When we use data generators, we use the `model.fit_generator` method rather than the usual `model.fit`."}}