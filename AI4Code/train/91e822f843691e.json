{"cell_type":{"2f2a6721":"code","a19660c7":"code","4c43fff7":"code","7bd5314e":"code","63ad13fe":"code","a1c37492":"code","35ac17e1":"code","0ae90703":"code","3fdcb4ac":"code","1efba2b5":"code","e3516136":"code","c24cc7fc":"code","2c280287":"code","e1b78dc2":"code","c3bd6486":"code","2c6aeb3e":"code","cc696e8c":"code","5447771c":"code","daaab671":"code","0c0bd950":"code","4a6b72d2":"code","f7b75732":"code","76cf897d":"code","17ef4ec5":"code","ac334bb8":"code","43e9e71e":"code","818f27a8":"code","ca055fc9":"code","14e6aff7":"code","f6523778":"code","872e90b9":"code","5ef3b87a":"code","298269e9":"code","f9fc4b5e":"code","d25fe53b":"code","57b09b73":"code","d3d52479":"code","88e22b99":"code","4abab18d":"code","770b5bc2":"code","694a5ef8":"code","870cd9a4":"code","85281a0f":"code","35b4f4b0":"code","625596fd":"code","dc16e8e2":"code","7ecf6eed":"code","3c19bd87":"code","5dc5f4ad":"code","48d8b1f5":"code","ab5fda15":"code","b1c78417":"code","ae13b697":"code","9ebdd39b":"code","82d17e32":"code","f3719a62":"code","eb1b8d23":"code","d29305fa":"code","992b49dd":"code","1509349c":"code","c35b7b5f":"markdown","d9d749ee":"markdown","079f5151":"markdown","179c20b6":"markdown","017d945f":"markdown","996619d9":"markdown","172daec4":"markdown","c16dc8f5":"markdown","354546f6":"markdown","795f2eea":"markdown","8a050377":"markdown","b1d772a1":"markdown","8104b892":"markdown","1683d69a":"markdown","e9582b1b":"markdown","ea5541cc":"markdown","6d7fa153":"markdown","b79eb93a":"markdown","b0f83937":"markdown","c1048dc0":"markdown","7047696a":"markdown","d78355a9":"markdown","0571555e":"markdown","547d8740":"markdown","7aa3cbc4":"markdown","d3033d81":"markdown","c44c0ca3":"markdown","b5ed291a":"markdown","bf8289b2":"markdown","a4fcfcdb":"markdown","d58dbbd2":"markdown","109b50bb":"markdown","0a30a857":"markdown"},"source":{"2f2a6721":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a19660c7":"df = pd.read_csv('\/kaggle\/input\/spotify-top-200-charts-20202021\/spotify_dataset.csv', index_col='Index')\n#df.head()\ndf1 = df","4c43fff7":"df1.head()","7bd5314e":"df1.columns","63ad13fe":"df1.iloc[0]","a1c37492":"df1.info()","35ac17e1":"df1['Song ID'].dropna().value_counts().head(30)","0ae90703":"# This songs havn't ID and features \n# df1[df1['Song ID'] == ' '].sort_values(by='Song Name')\ndf1[df1['Song ID'] == ' ']","3fdcb4ac":"song_names = df1['Song Name'][df1['Song ID'] == ' ']\nart_name = df1['Artist'][df1['Song ID'] == ' ']\nfor i, y  in zip(song_names, art_name):\n    print(f'Song', i)\n    print(f'____','by', y, 'haven\\'t ID')","1efba2b5":"df1[df1['Artist'] == 'The Kid LAROI'].sort_values(by=['Song Name'])\n# df1[df1['Artist'] == 'Polo G'].sort_values(by=['Song Name'])","e3516136":"df1[df1['Song Name'] == 'NOT SOBER (feat. Polo G & Stunna Gambino)'].sort_values(by=['Song Name'])","c24cc7fc":"df1[df1['Artist'] == 'Ariana Grande'].sort_values(by=['Song Name']).head(5)","2c280287":"df1[df1['Artist'] == 'Rod Wave'].sort_values(by=['Song Name'])","e1b78dc2":"df1[df1['Artist'] == 'Chris Rea'].sort_values(by=['Song Name'])","c3bd6486":"df1[df1['Artist'] == 'Queen'].sort_values(by=['Song Name'])","2c6aeb3e":"df1[df1['Artist'] == 'Tainy'].sort_values(by=['Song Name'])","cc696e8c":"df1[df1['Artist'] == 'Super Yei, Jone Quest'].sort_values(by=['Song Name'])","5447771c":"df1[df1['Artist'] == 'Dalex'].sort_values(by=['Song Name'])","daaab671":"df1[df1['Artist'] == 'AK AUSSERKONTROLLE, Bonez MC'].sort_values(by=['Song Name'])","0c0bd950":"df1[df1['Artist'] == 'Lauv'].sort_values(by=['Song Name'])","4a6b72d2":"# extract start and date of week\ndf1[['Week of Highest Charting_start','Week of Highest Charting_end']] = df1['Week of Highest Charting'].str.split('--', expand=True)","f7b75732":"cols = list(df.columns.values)\ncols = ['Highest Charting Position',\n 'Number of Times Charted',\n 'Week of Highest Charting',\n 'Week of Highest Charting_start',\n 'Week of Highest Charting_end',\n 'Song Name',\n 'Streams',\n 'Artist',\n 'Artist Followers',\n 'Song ID',\n 'Genre',\n 'Release Date',\n 'Weeks Charted',\n 'Popularity',\n 'Danceability',\n 'Energy',\n 'Loudness',\n 'Speechiness',\n 'Acousticness',\n 'Liveness',\n 'Tempo',\n 'Duration (ms)',\n 'Valence',\n 'Chord']","76cf897d":"dff = df1[cols]","17ef4ec5":"dff.head()","ac334bb8":"dff['Week of Highest Charting_start'] = pd.to_datetime(dff['Week of Highest Charting_start'], yearfirst=True)\ndff['Week of Highest Charting_end'] = pd.to_datetime(dff['Week of Highest Charting_end'], yearfirst=True)\ndff['Streams'] = dff['Streams'].replace(',','', regex=True)\ndff['Streams'] = dff['Streams'].astype('int32')\ndff.info()","43e9e71e":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\nsns.set_context(\"paper\")\n\ny = dff.groupby(by=['Week of Highest Charting_start'])['Streams'].sum()\nx = y.index\n\nfig, ax = plt.subplots(figsize=(20,10))\nax = sns.barplot(x=x, y=y\/100000000, data=y, order=y)\nax.axes.set_title('Distribution by week',size=20)\nx_dates = x.strftime('%y-%m-%d').sort_values()\nax.set_ylabel('Streams, 100M', size=15)\nax.set_xlabel('Date', size=15)\nax.set_xticklabels(labels=x_dates, rotation=90, ha='right')\nax.annotate('Maximum of streams on christmas',\n            xy=(50, 6), xycoords='data',\n            xytext=(-50, 0), textcoords='offset pixels',\n            horizontalalignment='right',\n            verticalalignment='center',\n            arrowprops=dict(facecolor='black', shrink=0.15), size=15)\nt = ax.text(\n    60, 6, \"Hello, Santa Claus!\", ha=\"center\", va=\"center\", rotation=0, size=15,\n    bbox=dict(boxstyle=\"sawtooth\", fc=\"white\", ec=\"b\", lw=2))\nbb = t.get_bbox_patch()\nbb.set_boxstyle(\"sawtooth\", pad=0.6)\n\nax.annotate('this is an incomprehensible outlier',\n            xy=(1, 4.5), xycoords='data',\n            xytext=(50, 0), textcoords='offset pixels',\n            horizontalalignment='left',\n            verticalalignment='center',\n            arrowprops=dict(facecolor='black', shrink=0.15), size=15)\nax.annotate('this is an incomprehensible outlier',\n            xy=(80, 4.5), xycoords='data',\n            xytext=(-50, 0), textcoords='offset pixels',\n            horizontalalignment='right',\n            verticalalignment='center',\n            arrowprops=dict(facecolor='black', shrink=0.15), size=15)\n\nfig.show()","818f27a8":"yf = pd.DataFrame(y)\nlabels = dff[['Artist','Song Name','Week of Highest Charting_start']]\nlabels.set_index('Week of Highest Charting_start', inplace=True)","ca055fc9":"z = yf.merge(labels, left_index=True, right_index=True)\nzf = z.sort_values(by='Streams', ascending=False)\nzf[zf['Streams'] == (483495649 and 456689174)]\n\n# list(zf['Streams'].unique())[:3]","14e6aff7":"# Christmas songs\nz = yf.merge(labels, left_index=True, right_index=True)\nzf = z.sort_values(by='Streams', ascending=False)\nzf[zf['Streams'] == 614846224].tail(5)\n# list(zf['Streams'].unique())[:3]","f6523778":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\ndates = ['2019-12-27','2020-12-18','2021-07-23']\nlabels = ['2019-12-27']\n\nfor i in dates:\n    df_max = dff[dff['Week of Highest Charting_start'] == i]\n    df_max['Artist and Song Name'] = df_max[['Artist', 'Song Name']].apply(lambda x: ' - '.join(x), axis=1)\n    df_max = df_max.sort_values(by=['Streams'],ascending=False).head(50)\n    fig, ax = plt.subplots(figsize = (25,20))\n    ax = sns.barplot(x='Artist and Song Name', y='Streams', data=df_max, alpha=0.5, log=False)\n    ax.set_xticklabels(labels=df_max['Artist and Song Name'], rotation=90,\n                       fontdict={\n                           'fontsize': 'large',\n                           'fontweight': 'bold',\n                           'verticalalignment': 'baseline',\n                           'horizontalalignment': 'center'});\n\n    ax.set_yticklabels(labels = [0,2,4,6,8,10], rotation=0, fontdict={\n                           'fontsize': 'large',\n                           'fontweight': 'bold',\n                           'verticalalignment': 'center_baseline',\n                           'horizontalalignment': 'right'});\n    ax.set_xlabel(xlabel='Artist and Song Name', loc='center', labelpad = 25, fontdict={\n                           'fontsize': 15,\n                           'fontweight': 'bold',\n                           'verticalalignment': 'center_baseline',\n                           'horizontalalignment': 'right'});\n    ax.set_ylabel(ylabel='Streams, in 100 Millions', loc='center',labelpad = 25, fontdict={\n                           'fontsize': 15,\n                           'fontweight': 'bold',\n                           'verticalalignment': 'center_baseline',\n                           'horizontalalignment': 'right'});\n    title = str('Barplot. ' + i + ' week')\n    ax.set_title(label = title, fontdict={'fontsize': 25,\n                        'fontweight': 'bold',\n                        'color': '#003366',\n                        'verticalalignment': 'baseline',\n                        'horizontalalignment': 'center'});","872e90b9":"duplicated_ID = df1['Song ID'].dropna().value_counts().head(29)\nduplicated_ID = duplicated_ID[1:29]\nprint(f'Duplicated Songs ID:','\\n', duplicated_ID)\nduplicated_ID = pd.DataFrame(duplicated_ID)\nduplicated_ID.reset_index(drop=False, inplace=True)\nprint(type(duplicated_ID))","5ef3b87a":"ddf= pd.DataFrame()\nfor i in duplicated_ID['index']:\n    j = pd.DataFrame(dff[dff['Song ID'] == i])\n    ddf = ddf.append(j)\nprint(dff.shape)\n# ddf","298269e9":"# ddf.drop_duplicates(['Song ID'])\nprint(r'Count of duplicated raws before dropping duplicates:',(ddf.count()[0]), 'raws')\nprint(r'Count of unique raws after dropping duplicates:',(ddf.drop_duplicates(['Song ID'])).count()[0], 'raws')","f9fc4b5e":"dff = dff.loc[dff['Song ID'] != ' ']\ndff.drop_duplicates('Song ID', inplace=True)\ndff.info()","d25fe53b":"dfff = dff[dff['Week of Highest Charting_start'] == '2020-12-18']\ndfff = dfff.reset_index()\n# dfff = dfff.drop(columns=['level_0','index'])","57b09b73":"crst = (dfff[dfff['Song Name'].str.contains('Christ|Snow|Santa|Xmas|Rudolph|Feliz|Fairytale|Year|Tree|Cold|Winter|Hallelujah|Mistletoe|Sleigh Ride|Joy To The World|One More Sleep|Little Saint Nick|Naughty List|I\\'ll Be Home|Do You Hear What I Hear?|My Kind Of Present')==True])\nChristmas_to_drop = [624,654,671,683]\ncrst = crst['Index']\ncrst = pd.DataFrame(crst)\nChristmas_to_drop = pd.DataFrame(Christmas_to_drop, columns=['Index'])\nChristmas_to_drop = Christmas_to_drop.append(crst)\nChristmas_to_drop.reset_index(inplace=True)\nctd = list(Christmas_to_drop['Index'])\nctd = np.array(ctd)\ncrst = np.array(crst['Index'])\nctdi = np.concatenate([ctd, crst])\ndfff[(dfff['Week of Highest Charting_start'] == '2020-12-18') & (~dfff['Index'].isin(ctdi))]\n# dfff[(~dfff['Index'].isin(ctdi))]","d3d52479":"dff","88e22b99":"dff = dff[(~dff.index.isin(ctdi))]","4abab18d":"dff.info()","770b5bc2":"sns.set_theme(style=\"whitegrid\")\nsns.set_context(\"paper\")\n\ny = dff.groupby(by=['Week of Highest Charting_start'])['Streams'].sum()\nx = y.index\n\nfig, ax = plt.subplots(figsize=[20,10])\nax = sns.barplot(x=x, y=y\/100000000, data=y, order=y)\nax.axes.set_title('Distribution by week',size=20)\nx_dates = x.strftime('%y-%m-%d').sort_values()\nax.set_ylabel('Streams, 100M', size=15)\nax.set_xlabel('Date', size=15)\nax.set_xticklabels(labels=x_dates, rotation=90, ha='right')\nax.annotate('It\\'s looks better',\n            xy=(50, 2), xycoords='data',\n            xytext=(-50, 0), textcoords='offset pixels',\n            horizontalalignment='right',\n            verticalalignment='center',\n            arrowprops=dict(facecolor='black', shrink=0.15), size=15)\n\nax.annotate('this is a normal peak',\n            xy=(1, 4.5), xycoords='data',\n            xytext=(50, 0), textcoords='offset pixels',\n            horizontalalignment='left',\n            verticalalignment='center',\n            arrowprops=dict(facecolor='black', shrink=0.15), size=15)\nax.annotate('this is a normal peak',\n            xy=(80, 4.5), xycoords='data',\n            xytext=(-50, 0), textcoords='offset pixels',\n            horizontalalignment='right',\n            verticalalignment='center',\n            arrowprops=dict(facecolor='black', shrink=0.15), size=15)\n\nfig.show()","694a5ef8":"df1 = dff","870cd9a4":"df1.drop(['Week of Highest Charting','Song Name','Release Date','Weeks Charted','Week of Highest Charting_end'], axis=1, inplace=True)\ndf1[['Artist','Chord']] = df1[['Artist','Chord']].astype('category')\ndf1['Streams'] = df1['Streams'].replace(',','', regex=True)\ndf1['Streams'] = df1['Streams'].astype('int16')\ndf1['Artist Followers'] = df1['Artist Followers'].astype('int16')","85281a0f":"df1[['Danceability','Energy','Loudness','Speechiness','Acousticness','Liveness','Tempo','Valence']] = df1[['Danceability','Energy','Loudness','Speechiness','Acousticness','Liveness','Tempo','Valence']].astype('float16')\ndf1['Popularity'] = df1['Popularity'].astype('int16')\ndf1['Duration (ms)'] = df1['Duration (ms)'].astype('int16')\ndf1.info()","35b4f4b0":"import warnings\nwarnings.filterwarnings('ignore')\n\ndf1['Genre'] = pd.Series(df1['Genre'])\ndf1['Genre'] = df1['Genre'].str.replace('[','')\ndf1['Genre'] = df1['Genre'].str.replace(']','')\ndf1['Genre'] = df1['Genre'].str.replace('\\'','')\ndf1['Genre'] = df1['Genre'].str.replace(',','')\nt = pd.Series(df1['Genre']).str.get_dummies(' ')\n# t.columns.value_counts().sort_values()\n# t.columns[201:]\n# looking for same Genres like latin, latina, latino\n# t[t['150'] == 1]\n# s = s.str.split(expand=True)\n# pd.get_dummies(s)\n# s.head(25)\n# s.to_csv()\ndf2 = pd.merge(df1, t, on='Index')","625596fd":"df2.drop(['Artist','Genre', 'Song ID'], axis=1, inplace=True)","dc16e8e2":"df2.describe()","7ecf6eed":"s = (df2.dtypes == 'category')\ncategory = list(s[s].index)\n\nprint(\"Categorical variables:\", category)","3c19bd87":"# One-Hot Encoder for Scales of songs\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\nnotes = df2['Chord']\nenc_df = pd.DataFrame(enc.fit_transform(df2[['Chord']]).toarray())\nenc_df.columns = enc.get_feature_names(['Chord'])\ndf2 = df2.join(enc_df)\ndf2.drop(['Chord'], axis=1, inplace=True)","5dc5f4ad":"df2.info()","48d8b1f5":"df2 = df2.dropna()\ndf2.head()","ab5fda15":"df3 = df2","b1c78417":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, ensemble\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","ae13b697":"y = df3['Popularity']\nX = df3.drop(['Popularity'], axis=1)\nprint('Full dataset shape is:',df3.shape)\nprint('Feature dataset shape is:',X.shape)\nprint('Target dataset shape is:',y.shape)","9ebdd39b":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = df3\nfeature_names = [i for i in X.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(n_estimators=100,\n                                  random_state=0).fit(train_X, train_y)","82d17e32":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","f3719a62":"# ['Number of Times Charted','rap','pop','post-teen','dance','german','girl','brooklyn','drill','melodic','latino','uk','hip','k-pop','trap','Highest Charting Position','en','group','r&b','puerto']\nfeatures = ['rap','pop','post-teen','dance','german','girl','brooklyn','drill','melodic','latino','uk','hip','k-pop','trap','en','group','r&b','puerto']\nX = X[features]","eb1b8d23":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=13)\n\nparams = {'n_estimators': 100,\n          'max_depth': 4,\n          'min_samples_split': 5,\n          'learning_rate': 0.01,\n          'loss': 'squared_error'}","d29305fa":"reg = ensemble.GradientBoostingRegressor()\nreg.fit(X_train, y_train)\n\nmse = mean_squared_error(y_test, reg.predict(X_test))\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))","992b49dd":"test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\nfor i, y_pred in enumerate(reg.staged_predict(X_test)):\n    test_score[i] = reg.loss_(y_test, y_pred)\n    \nfig = plt.figure(figsize=(6, 6))\nplt.subplot(1, 1, 1)\nplt.title('Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, reg.train_score_, 'b-', label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-', label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')\nfig.tight_layout()\nplt.show()","1509349c":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = df3\nfeature_names = [i for i in X.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(n_estimators=100,\n                                  random_state=0).fit(train_X, train_y)\nfeature_importance = reg.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(X.columns)[sorted_idx])\nplt.title('Feature Importance (MDI)')\n\nresult = permutation_importance(reg, X_test, y_test, n_repeats=10,\n                                random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\nplt.subplot(1, 2, 2)\nplt.boxplot(result.importances[sorted_idx].T,\n            vert=False, labels=np.array(X.columns)[sorted_idx])\nplt.title(\"Permutation Importance (test set)\")\nfig.tight_layout()\nplt.show()","c35b7b5f":"#### *Let's check the data for missing values and\/or duplicates.*\n#### *Looks not bad. But if we dig deeper we will find a lot of interesting things.*","d9d749ee":"### Song In meinem Benz - AK AUSSERKONTROLLE, Bonez MC\n","079f5151":"### There is the chart Distribution by week","179c20b6":"### When I was checking the data I noticed two songs without an ID with the word \"Christmas\" and from the data only a week of Highest Charting and Streams. I assume that the number of Christmas song auditions increases on Christmas Day. And this is quite natural.","017d945f":"### Lean (feat. Towy, Osquel, Beltito & Sammy & Falsetto) - Super Yei, Jone Quest","996619d9":"### Thank God It's Christmas - Non-Album Single - Queen","172daec4":"## We will decide which data to delete and which to leave.","c16dc8f5":"# Hello. \n#### I want to explore Spotify data set and try to understand something interesting from that.\n#### I will add my thoughts and findings and gradually bring the notebook into a structured form. It doesn't look good yet. If there are comments and suggestions, write in the comments-I will be glad and take into account the comments in future editions.","354546f6":"### Agua (with J Balvin) - Music From \"Sponge On The Run\" Movie - Tainy","795f2eea":"### fuck, i'm lonely (with Anne-Marie) - from \u201c13 Reasons Why: Season 3\u201d - Lauv\n* This song have a duplicate with Song ID - '09PGubKAMryhOWv1LHpCYz'","8a050377":"### now data set get to be training","b1d772a1":"### Richer (feat. Polo G)- Rod Wave","8104b892":"*Let's take a closer look at the data*","1683d69a":"##### We see three outliers on December 27, 2019; December 18, 2019; and July 23, 2021;","e9582b1b":"We can make assumptions and then confirm or refute them:\n1. The number of song plays is influenced by factors such as:\n    * Artist Followers\n    * Danceability\n    * Energy\n    * Loudness\n    * Speechiness\n    * Acousticness\n    * Liveness\n    * Tempo\n    * Duration (ms)\n    * Valence","ea5541cc":"### +Linda - Dalex","6d7fa153":"### We can see all duplicated entries","b79eb93a":"### We will be looking at the list of songs played during the week from December 18 to December 25, 2020","b0f83937":"### Driving Home for Christmas - 2019 Remaster - Chris Rea","c1048dc0":"# Preparating data for ML","7047696a":"### '34+35' and '34+35 Remix (feat. Doja Cat, Megan Thee Stalli...' - Arianda Grande\n* This song occurs twice in the original and the remix and both times from without ID\n* We will delete it later","d78355a9":"#### we can see that:\n* Eleven songs haven't ID\n* One song is counted three times\n* twenty-seven songs counted twice\n\nLet's figure it out with them all.","0571555e":"### NOT SOBER (feat. Polo G & Stunna Gambino) - The Kid LAROI\n* this song haven't ID\n* Modern songs are often performed with featuring of other artists. Therefore, it is necessary to check the presence of the song in the authorship of the guest artist.\n* It is not always clear in what shares the authorship belongs and musicians can place IDs on behalf of different artists.\n* We will delete it later.","547d8740":"### I decided that it was necessary to delete the lines with duplicates in the Song ID. Also delete lines without Song ID and without features instead of filling them with average values-because despite the similarity of the features of the music, the characteristics of Ariana Grande music and Chris Rea's music are very different. And also to highlight a separate feature from the name of the song-the presence of a mention in the song of Christmas, the words Jingle, Bell, etc.","7aa3cbc4":"# *Import and exploring data*","d3033d81":"# Eminem's Gnat was the only non-Christmas composition in the dataset for the 2020-12-18 week. Thank you Slim Shady!","c44c0ca3":"# **The dataset need a clearing.**","b5ed291a":"### Let's continue to deal with duplicates in the data.\n#### Let me remind you that we need to deal with the following duplicated positions.","bf8289b2":"## Now we can say something about dataset:\n* Highest Charting Position                                                     \n* Number of Times Charted                                                      \n* Week of Highest Charting                                \n* Song Name                                                              \n* Streams                                                             \n* Artist                                                               \n* Artist Followers                                                       \n* Song ID                                                 \n* Genre                                   \n* Release Date                                                        \n* Weeks Charted                \n* Popularity                                                                 \n* Tonality\n* Other features of song","a4fcfcdb":"# We are trying to understand wich feature better for predict song popularity","d58dbbd2":"### We will keep in mind the natural increase in listening to Christmas songs at the end of December","109b50bb":"### We see that on the Christmas week there are 6 times more auditions than usual ","0a30a857":"# Time for charts"}}