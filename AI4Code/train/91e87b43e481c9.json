{"cell_type":{"e50123d4":"code","b616e749":"code","2642189c":"code","ba387cb5":"code","f60379e2":"code","3ffcddce":"code","5c58487c":"code","e1b7b96d":"code","fcc4d841":"code","1db4c1a0":"code","09a21f03":"code","1aded8fb":"code","8dd43011":"code","57739c88":"code","e1f2ce4d":"code","f68e79f2":"code","2befb722":"code","599b61f1":"code","4235bf54":"code","7bce357c":"code","b364ca5e":"code","b1033faf":"code","a1f238de":"code","7fcb1b9e":"code","a05b7990":"code","3fb83a89":"code","ed2fb272":"code","5ac4f2d9":"markdown","e716728f":"markdown","e53e0569":"markdown","878755d0":"markdown"},"source":{"e50123d4":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.sparse import hstack\nimport gc\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","b616e749":"# Loading data\n\ntrain1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain1['lang'] = 'en'\n\ntrain_es = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-es-cleaned.csv')\ntrain_es['lang'] = 'es'\n\ntrain_fr = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-fr-cleaned.csv')\ntrain_fr['lang'] = 'fr'\n\ntrain_pt = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-pt-cleaned.csv')\ntrain_pt['lang'] = 'pt'\n\ntrain_ru = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-ru-cleaned.csv')\ntrain_ru['lang'] = 'ru'\n\ntrain_it = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-it-cleaned.csv')\ntrain_it['lang'] = 'it'\n\ntrain_tr = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-tr-cleaned.csv')\ntrain_tr['lang'] = 'tr'\n\n#train2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n#train2.toxic = train2.toxic.round().astype(int)\n#train2['lang'] = 'en'\n\ntrain = pd.concat([\n    \n    train1[['comment_text', 'lang', 'toxic']],\n    train_es[['comment_text', 'lang', 'toxic']],\n    train_tr[['comment_text', 'lang', 'toxic']],\n    train_fr[['comment_text', 'lang', 'toxic']],\n    train_pt[['comment_text', 'lang', 'toxic']],\n    train_ru[['comment_text', 'lang', 'toxic']],\n    train_it[['comment_text', 'lang', 'toxic']]\n    \n]).sample(n=300000).reset_index(drop=True)\n\ndel train1, train_es, train_fr, train_pt, train_ru, train_it, train_tr\ngc.collect()","2642189c":"#train = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\n#train1 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv')\n\n#valid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_valid_translated.csv')\n#valid1 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\n\n#test = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_test_translated.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\n\nsubm = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","ba387cb5":"train.head()","f60379e2":"#train = pd.concat([train,train1])\n#train = pd.concat([train,valid])\n#valid['comment_text'] = valid['translated'] #+' '+valid1['comment_text']\n#test['content'] = test['translated'] #+' '+test1['content']\n#train = pd.concat([train,valid])\n#train = valid.copy()","3ffcddce":"train.head()","5c58487c":"train['comment_text'][0]","e1b7b96d":"train['comment_text'][2]","fcc4d841":"lens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","1db4c1a0":"lens.hist();","09a21f03":"label_cols = ['toxic']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()","1aded8fb":"len(train),len(test)","8dd43011":"train['comment_text'].fillna(\"unknown\", inplace=True)\ntest['content'].fillna(\"unknown\", inplace=True)","57739c88":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","e1f2ce4d":"n = train.shape[0]\n\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\n\n\ntrn_term_doc = vec.fit_transform(train['comment_text'])\ntest_term_doc = vec.transform(test['content'])","f68e79f2":"import pickle\npickle.dump(vec, open(\"vectoriser.pkl\", \"wb\"))","2befb722":"trn_term_doc, test_term_doc","599b61f1":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","4235bf54":"x = trn_term_doc\ntest_x = test_term_doc","7bce357c":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    m = LogisticRegression(C=4, dual=False)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","b364ca5e":"for i, j in enumerate(label_cols):\n    print(i,j)","b1033faf":"preds = np.zeros((len(test), len(label_cols)))\n\n# for i, j in enumerate(label_cols):\nprint('fit', 'toxic')\nj = 'toxic'\ny = train[j]\ny = y.values\nr = np.log(pr(1,y) \/ pr(0,y))\nm = LogisticRegression(C=4, dual=False)\nx_nb = x.multiply(r)\nm.fit(x_nb, y)\n\nimport pickle\npickle.dump(m, open(\"model.pkl\", \"wb\"))","a1f238de":"preds[:,0] = m.predict_proba(test_x.multiply(r))[:,1]","7fcb1b9e":"print(r)","a05b7990":"m.predict_proba(vec.transform([\"i hate being hateful\"]).multiply(r))","3fb83a89":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n\nsubmission.to_csv('submission.csv', index=False)","ed2fb272":"submission.head(n=20)","5ac4f2d9":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-translated\/train_mic.csv\")\n#train2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n#train2.toxic = train2.toxic.round().astype(int)\n\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_valid_translated.csv')\n#valid1 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')","e716728f":"# Looking at the data\n## The training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict.","e53e0569":"train = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train1[['tr', 'toxic']].rename(columns={'tr': 'comment_text'}).dropna(),\n    train1[['ru', 'toxic']].rename(columns={'ru': 'comment_text'}).dropna(),\n    train1[['it', 'toxic']].rename(columns={'it': 'comment_text'}).dropna(),\n    train1[['fr', 'toxic']].rename(columns={'fr': 'comment_text'}).dropna(),\n    train1[['pt', 'toxic']].rename(columns={'pt': 'comment_text'}).dropna(),\n    train1[['es', 'toxic']].rename(columns={'es': 'comment_text'}).dropna(),\n    #train2[['comment_text', 'toxic']].query('toxic==1'),\n    #train2[['comment_text', 'toxic']].query('toxic==0'),\n    valid[['comment_text', 'toxic']]\n    #valid1[['comment_text', 'toxic']]\n]).reset_index(drop=True)","878755d0":"valid = pd.concat([\n    valid[['comment_text', 'toxic']],\n    valid[['translated', 'toxic']].rename(columns={'translated': 'comment_text'})])\n\n#valid['comment_text'] = valid['translated'] #+' '+valid1['comment_text']"}}