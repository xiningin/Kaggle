{"cell_type":{"584c753f":"code","969d418f":"code","8f7722f9":"code","f8c9d3f2":"code","a147392c":"code","8a678842":"code","f17e1ba3":"code","524dc097":"code","8100af1e":"code","a3e89a3d":"code","0d4f6bc6":"code","9559d3af":"code","471afb40":"code","7b80a21a":"code","ec869429":"code","b5d77ceb":"code","39e2e65a":"code","023a29af":"code","96161f8c":"code","8dba5c74":"code","b8368394":"code","2b148229":"code","635fe02e":"code","2b440954":"code","15c5a05f":"code","62976663":"code","829090f3":"code","494cd0e6":"code","c7dccd17":"code","4c450781":"code","66e1cf6f":"code","3b5cb2f6":"code","9781b807":"code","0017228c":"code","8436e5d1":"code","8893c0dd":"code","9f00bb09":"code","044044ec":"code","0abad7ff":"code","0abeda48":"code","0e5981cf":"code","46487a41":"code","69be32d2":"code","6ff5213e":"code","4bf9ff22":"code","6984b069":"code","4108e81f":"code","54356595":"code","c5b3ff22":"code","d689d77f":"code","d50486c8":"code","59b6e8ce":"code","b9cfbabc":"code","79f8c3fb":"code","94683955":"code","2f2f36d8":"code","043be9f8":"code","e228cdcb":"code","dc9becb1":"code","5fc6b691":"code","1c131a7c":"code","1d9134dd":"code","d555a4fb":"code","089d1c3f":"code","aabd56f2":"code","49542340":"code","12e2e3e0":"markdown","9a913bef":"markdown","738b5f11":"markdown","291f3ef6":"markdown","6d8ceda7":"markdown","1b26f3f4":"markdown","f80c6eec":"markdown","b8114f7a":"markdown","b8afbe9b":"markdown","4d3fa008":"markdown","5b65f784":"markdown","153c60fe":"markdown","c6997116":"markdown","6f7be834":"markdown","901df033":"markdown","b2dfd1a5":"markdown","478ba97b":"markdown","f92d8fb3":"markdown","81743524":"markdown","ec042073":"markdown","0aa73a6a":"markdown","f5eed6f6":"markdown","4b8232aa":"markdown","d85fa15b":"markdown","3e51f218":"markdown","23b2a534":"markdown","4cd9f325":"markdown","0561aa61":"markdown","46e0a509":"markdown","acae79a3":"markdown","3c5ae450":"markdown","bbe9b0de":"markdown","558d1db2":"markdown","242a2527":"markdown","85837957":"markdown","f44a3e3a":"markdown","e9e1b885":"markdown","2bde1e9e":"markdown","9ca1cdb4":"markdown","959a3efd":"markdown","ddb0ff4d":"markdown","c9df720c":"markdown","c7e86946":"markdown","de807a13":"markdown","7348ff85":"markdown","d790d018":"markdown","46f7e591":"markdown","cf120ff2":"markdown","89f24cba":"markdown","e609bca8":"markdown","297d8811":"markdown","47e2776a":"markdown","c7e58926":"markdown","76361e26":"markdown","9771b93a":"markdown","f622926d":"markdown","377cc107":"markdown","c8f9b92a":"markdown","c9353f5e":"markdown","09a36d37":"markdown","5647226b":"markdown","173b5e77":"markdown","cea09636":"markdown","b91fc32e":"markdown","3ff54268":"markdown","a50b84f8":"markdown","d0331505":"markdown","91b993c2":"markdown","3a422773":"markdown"},"source":{"584c753f":"import numpy as np #working with matrices, arrays, data science-friendly arrays\nimport pandas as pd #data processing, CSV file I\/O, preprocessing\nimport matplotlib.pyplot as plt  #data viz library\n#jupyter notebook magic function to make plots show in a notebook cell\n%matplotlib inline  \nplt.style.use('seaborn-whitegrid') #set my default matplotlib style to 'seaborn-whitegrid'\n\nimport seaborn as sns  #additional data viz helper library\nimport scipy.stats as st  #used to fit non-normal distributions with seaborn\nimport os  #working with the operating system, filepaths, folders,etc.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer #replace missing values with the mean\nfrom sklearn.feature_selection import mutual_info_classif #used to create a ranking with a feature utility metric \nfrom xgboost import XGBClassifier #first classifier model\nfrom sklearn.metrics import roc_auc_score","969d418f":"#This is default from Kaggle.  Basically uses os.walk to recursively \n#print the full filepath and filename for all files stored in the kaggle\/input folder.\n\n\n####### DEFAULT COMMENTS AND CODE FROM KAGGLE ###############\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        if 'train.csv' in os.path.join(dirname, filename):\n            train_df = pd.read_csv(os.path.join(dirname, filename), index_col = 0)  \n        elif 'test.csv' in os.path.join(dirname, filename):\n            test_df = pd.read_csv(os.path.join(dirname, filename), index_col = 0)\n        elif 'sample_solution.csv' in os.path.join(dirname, filename):\n            ss_df = pd.read_csv(os.path.join(dirname, filename), index_col = 0)\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f7722f9":"print('There are {} rows and {} columns in {}.'.format(train_df.shape[0],train_df.shape[1],'train_df'))","f8c9d3f2":"print('There are {} rows and {} columns in {}.'.format(test_df.shape[0],test_df.shape[1],'test_df'))","a147392c":"print('There are {} rows and {} columns in {}.'.format(ss_df.shape[0],ss_df.shape[1],'ss_df'))","8a678842":"#copy the training dataset\nX = train_df.copy()\nX['claim'] = X['claim'].astype('str')\ny = X.pop('claim')","f17e1ba3":"#split the dataset into a training\/validation set \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0,train_size=0.3, test_size=0.0125)","524dc097":"float_cols = [col for col in train_df if col != 'claim']\n#save the minmax scaler function as a variable\nmm_scaler = MinMaxScaler()\n\n#min-max scale the numeric columns only.  In this case, that is every column.\n\n#fit and transform the training df\nscaled_cols_train = pd.DataFrame(mm_scaler.fit_transform(X_train[float_cols]),index = X_train.index, columns = X_train.columns)\n\n#just transform the validation and test df.  \nscaled_cols_valid = pd.DataFrame(mm_scaler.transform(X_valid[float_cols]),index = X_valid.index, columns = X_valid.columns)\nscaled_cols_test = pd.DataFrame(mm_scaler.transform(test_df),index= test_df.index, columns = test_df.columns)","8100af1e":"#1.6% of the dataset is missing, however, 62% of the rows and 100% of the columns have at least 1 missing value.  This means\n#I will impute rather than drop.\n\n# Imputing AFTER min-max scaling so the mean imputation is on the same scale.\n\n#set simple imputer variable.  By default, this imputs using the mean to replace missing values\nmy_imputer = SimpleImputer()\n\n#fit and transform the training df\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(scaled_cols_train), index=X_train.index)\n\nimputed_X_valid = pd.DataFrame(my_imputer.transform(scaled_cols_valid), index=X_valid.index)\nimputed_X_test = pd.DataFrame(my_imputer.transform(scaled_cols_test), index=test_df.index)\n\n\n# Imputation removed column names; put them back\n\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\nimputed_X_test.columns = test_df.columns","a3e89a3d":"def make_mi_scores(X, y):\n    mi_scores = mutual_info_classif(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","0d4f6bc6":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","9559d3af":"mi_scores = make_mi_scores(imputed_X_train, y_train)","471afb40":"plt.figure(dpi=100, figsize=(10, 30))\nplot_mi_scores(mi_scores)","7b80a21a":"# deleting some variables to save memory\ndel X_train\ndel y_train\ndel X_valid\ndel y_valid\ndel scaled_cols_valid\ndel scaled_cols_train\ndel scaled_cols_test\ndel imputed_X_train\ndel imputed_X_valid\ndel imputed_X_test\ndel X\ndel y","ec869429":"#There are no null values in the target column so I shouldn't need to exclude it here.\ntrain_df['nullcount'] = train_df.isnull().sum(axis=1)\ntest_df['nullcount'] = test_df.isnull().sum(axis=1)","b5d77ceb":"train_df.groupby(['nullcount','claim']).size().unstack().plot(kind = 'bar', legend=True, title=\"Row Null Counts By Claim Group\")","39e2e65a":"not_claim_col = [col for col in train_df.columns if col != 'claim' and col != 'nullcount']","023a29af":"train_df[\"gt_count\"] = train_df[not_claim_col].gt(0).sum(axis=1)\ntest_df[\"gt_count\"] = test_df[not_claim_col].gt(0).sum(axis=1)","96161f8c":"train_df.groupby(['gt_count','claim']).size().unstack().plot(kind = 'bar', legend=True)","8dba5c74":"train_df[\"lt_count\"] = train_df[not_claim_col].lt(0).sum(axis=1)\ntest_df[\"lt_count\"] = test_df[not_claim_col].lt(0).sum(axis=1)","b8368394":"train_df.groupby(['lt_count','claim']).size().unstack().plot(kind = 'bar', legend=True)","2b148229":"train_df['avg'] = train_df[not_claim_col].mean(axis=1)\ntest_df['avg'] = test_df[not_claim_col].mean(axis=1)","635fe02e":"train_df[\"min_val\"] = train_df[not_claim_col].min(axis=1)\ntest_df[\"min_val\"] = test_df[not_claim_col].min(axis=1)","2b440954":"train_df[\"min_val\"] = train_df[not_claim_col].max(axis=1)\ntest_df[\"min_val\"] = test_df[not_claim_col].max(axis=1)","15c5a05f":"#iterating over all features columns and marking True or False if the value is missing (null).  Then converting that into an integer so it can be \n#used in our models universally.\nfor col in not_claim_col:\n    train_df[col + '_was_missing'] = train_df[col].isnull().astype(int)\n    test_df[col + '_was_missing'] = test_df[col].isnull().astype(int)","62976663":"counter=1\nnum_rows = len([col for col in train_df[not_claim_col].columns if train_df[col].skew() > 1][:20])\nplt.figure(1)\nplt.subplots(num_rows,3,figsize=(25,25))\n\n\nfor i, item in enumerate([col for col in train_df[not_claim_col].columns if train_df[col].skew() > 1][:20]):\n    \n    plt.subplot(num_rows,3,counter)\n    plt.hist(train_df[item]**(1\/3),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' **(1\/3)',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.subplot(num_rows,3,counter)\n    plt.hist(np.log10(train_df[item]),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' log10',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.subplot(num_rows,3,counter)\n    plt.hist(train_df[item]**(1\/2),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' **(1\/2)',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.grid(True)\nplt.subplots_adjust(top=1.5, bottom=0.2, left=0.10, right=0.95, hspace=0.45,\n        wspace=0.4)","829090f3":"counter=1\nnum_rows = len([col for col in train_df[not_claim_col].columns if train_df[col].skew() > 1][20:40])\nplt.figure(1)\nplt.subplots(num_rows,3,figsize=(20,20))\n\n\nfor i, item in enumerate([col for col in train_df[not_claim_col].columns if train_df[col].skew() > 1][20:40]):\n    \n    plt.subplot(num_rows,3,counter)\n    plt.hist(train_df[item]**(1\/3),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' **(1\/3)',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.subplot(num_rows,3,counter)\n    plt.hist(np.log10(train_df[item]),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' log10',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.subplot(num_rows,3,counter)\n    plt.hist(train_df[item]**(1\/2),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' **(1\/2)',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.grid(True)\nplt.subplots_adjust(top=1.5, bottom=0.2, left=0.10, right=0.95, hspace=0.45,\n        wspace=0.4)","494cd0e6":"counter=1\nnum_rows = len([col for col in train_df[not_claim_col].columns if train_df[col].skew() > 1][40:])\nplt.figure(1)\nplt.subplots(num_rows,3,figsize=(20,20))\n\n\nfor i, item in enumerate([col for col in train_df[not_claim_col].columns if train_df[col].skew() > 1][40:]):\n    \n    plt.subplot(num_rows,3,counter)\n    plt.hist(train_df[item]**(1\/3),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' **(1\/3)',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.subplot(num_rows,3,counter)\n    plt.hist(np.log10(train_df[item]),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' log10',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.subplot(num_rows,3,counter)\n    plt.hist(train_df[item]**(1\/2),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' **(1\/2)',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.grid(True)\nplt.subplots_adjust(top=1.5, bottom=0.2, left=0.10, right=0.95, hspace=0.45,\n        wspace=0.4)","c7dccd17":"counter=1\nnum_rows = len([col for col in train_df if train_df[col].skew() < -1])\nplt.figure(1)\nplt.subplots(num_rows,3,figsize=(20,20))\n\n\nfor i, item in enumerate([col for col in train_df if train_df[col].skew() < -1]):\n    \n    plt.subplot(num_rows,3,counter)\n    plt.hist(train_df[item]**(1\/3),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' **(1\/3)',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.subplot(num_rows,3,counter)\n    plt.hist(np.log10(train_df[item]),color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' log10',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.subplot(num_rows,3,counter)\n    plt.hist(train_df[item]**2,color='#7571B0',alpha=0.75)\n    plt.title(str(item)+' **2',fontsize=12,fontweight='bold')\n    counter+=1\n    plt.grid(True)\nplt.subplots_adjust(top=1.5, bottom=0.2, left=0.10, right=0.95, hspace=0.45,\n        wspace=0.4)","4c450781":"list_of_features_to_transform = ['f58','f110','f4','f8','f14','f30','f38','f39','f44','f52','f63','f64','f68','f78','f82','f87','f89','f95','f101','f102','f103','f118']","66e1cf6f":"for i, item in enumerate(list_of_features_to_transform):\n    train_df[str(item) + '_cubed_root'] = train_df[item] ** (1\/3)","3b5cb2f6":"for i, item in enumerate(list_of_features_to_transform):\n    test_df[str(item) + '_cubed_root'] = test_df[item] ** (1\/3)","9781b807":"#visually looking at each original feature's distribution, below is a list of features that may be financially-based.\n\nlist_of_possible_financials = train_df.iloc[:,:118].columns\n#list_of_possible_financials = ['f3','f7','f9','f10','f12','f16','f20','f25','f26','f27','f28','f32','f33','f35','f36','f37','f39','f41','f52','f62','f65','f67','f72','f73','f74','f77','f82','f83','f86','f89','f92','f96','f98','f102','f103','f104','f108','f114','f116','f117']","0017228c":"#define a visualization function.\ndef plot_bin_bars(num_bins,df):\n    \n    df2 = df.copy()\n    \n    num_rows = 24\n    num_cols = 5\n\n    row_ax_counter = 0\n    col_ax_counter = 0\n\n    plt.figure(1)\n\n    fig, ax = plt.subplots(num_rows,num_cols,figsize=(35,30))\n\n    width=0.35\n\n    for i, item in enumerate(list_of_possible_financials):\n\n        #bins_list = [bins for bins in range(int(train_df[item].min()),int(train_df[item].max()), int(round((train_df[item].max() - train_df[item].min())\/num_bins)))]\n        bins_list = list(np.arange(df2[item].min(),df2[item].max() + (df2[item].max() - df2[item].min())\/num_bins,(df2[item].max() - df2[item].min())\/num_bins))\n        \n        labels_test = ['bin'+str(lab+1) for lab in range(len(bins_list)-1)]\n\n        new_col_name = str(item) + '_binned'\n\n        df2[new_col_name] = pd.cut(df2[item], bins=bins_list, labels=labels_test)\n\n        #plt.subplot(num_rows,4,i+1)\n\n        groups_df = df2.groupby([new_col_name,'claim']).size().unstack()\n\n        rects1 = ax[row_ax_counter,col_ax_counter].bar(x=[ind for ind in range(len(groups_df.index))],height=groups_df[0],color='#4F66AF',width=0.35,label='0',alpha=0.75)\n        rects2 = ax[row_ax_counter,col_ax_counter].bar(x=[width + ind for ind in range(len(groups_df.index))],height=groups_df[1],color='#EDAC1A',width=0.35,label='1',alpha=0.75)\n        ax[row_ax_counter,col_ax_counter].set_title(new_col_name + ' Bins By Claim Status')\n        ax[row_ax_counter,col_ax_counter].legend()\n\n        ax[row_ax_counter,col_ax_counter].set_xticks([ind + width\/2 for ind in range(len(groups_df.index))])\n        ax[row_ax_counter,col_ax_counter].set_xticklabels(groups_df.index)\n\n        #fig.tight_layout()\n\n        if col_ax_counter == 4:\n            row_ax_counter+=1\n\n        if col_ax_counter < 4:\n            col_ax_counter+=1\n        else:\n            col_ax_counter = 0\n\n        #plt.show()\n\n        #train_df.groupby([new_col_name,'claim']).size().unstack().plot(kind = 'bar', legend=True, title=new_col_name + \" Counts By Claim Group\")\n\n        plt.grid(True)\n    plt.subplots_adjust(top=1.5, bottom=0.2, left=0.10, right=0.95, hspace=0.60,wspace=0.60)\n    del df2","8436e5d1":"plot_bin_bars(10,train_df)","8893c0dd":"plot_bin_bars(5,train_df)","9f00bb09":"plot_bin_bars(3,train_df)","044044ec":"def make_bin_cols(num_bins,bin_col_list,df):\n\n    df2=df.copy()\n    \n    for i, item in enumerate(bin_col_list):\n\n        bins_list = list(np.arange(df2[item].min(),df2[item].max() + (df2[item].max() - df2[item].min())\/num_bins,(df2[item].max() - df2[item].min())\/num_bins))[:-1]\n        \n        labels_test = [lab for lab in range(len(bins_list)-1)]\n\n        new_col_name = str(item) + '_binned'\n\n        df2[new_col_name] = pd.cut(df2[item], bins=bins_list, labels=labels_test)\n    \n    return(df2)","0abad7ff":"cols_to_bin = \\\n['f2',\n 'f32',\n 'f21',\n 'f86',\n 'f42',\n 'f110',\n 'f94',\n 'f103',\n 'f34',\n 'f102',\n 'f5',\n 'f40',\n 'f46',\n 'f12',\n 'f83',\n 'f111',\n 'f112',\n 'f36',\n 'f30',\n 'f57',\n 'f9',\n 'f95',\n 'f52',\n 'f107',\n 'f78',\n 'f90',\n 'f70',\n 'f91',\n 'f14',\n 'f35',\n 'f25',\n 'f3',\n 'f81',\n 'f65',\n 'f48',\n 'f31',\n 'f47',\n 'f71',\n 'f92',\n 'f24',\n 'f69',\n 'f56',\n 'f11',\n 'f28',\n 'f7',\n 'f23',\n 'f62',\n 'f104',\n 'f39',\n 'f16',\n 'f77',\n 'f96',\n 'f1',\n 'f43',\n 'f45',\n 'f4',\n 'f118',\n 'f8',\n 'f27',\n 'f53',\n 'f10',\n 'f38']","0abeda48":"#run the above function to create binned columns\ntrain_df = make_bin_cols(10,cols_to_bin,train_df)\ntest_df = make_bin_cols(10,cols_to_bin,test_df)","0e5981cf":"#copy the training dataset\nX = train_df.copy()\ny = X.pop('claim')","46487a41":"#split the dataset into a training\/validation set \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0,train_size=0.25, test_size=0.1)","69be32d2":"float_cols = [col for col in train_df if col != 'claim']\n#save the minmax scaler function as a variable\nmm_scaler = MinMaxScaler()\n\n#min-max scale the numeric columns only.  In this case, that is every column.\n\n#fit and transform the training df\nscaled_cols_train = pd.DataFrame(mm_scaler.fit_transform(X_train[float_cols]),index = X_train.index, columns = X_train.columns)\n\n#just transform the validation and test df.  \nscaled_cols_valid = pd.DataFrame(mm_scaler.transform(X_valid[float_cols]),index = X_valid.index, columns = X_valid.columns)\nscaled_cols_test = pd.DataFrame(mm_scaler.transform(test_df),index= test_df.index, columns = test_df.columns)","6ff5213e":"#removing variables to save memory\ndel X\ndel y\ndel X_train\ndel X_valid","4bf9ff22":"# Imputing AFTER min-max scaling so the mean imputation is on the same scale.\n\n#set simple imputer variable.  By default, this imputs using the mean to replace missing values\nmy_imputer = SimpleImputer()\n\n#fit and transform the training df\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(scaled_cols_train), index=scaled_cols_train.index)\n\nimputed_X_valid = pd.DataFrame(my_imputer.transform(scaled_cols_valid), index=scaled_cols_valid.index)\nimputed_X_test = pd.DataFrame(my_imputer.transform(scaled_cols_test), index=test_df.index)\n\n\n# Imputation removed column names; put them back\n\nimputed_X_train.columns = scaled_cols_train.columns\nimputed_X_valid.columns = scaled_cols_valid.columns\nimputed_X_test.columns = test_df.columns","6984b069":"#removing variables to save memory\ndel scaled_cols_valid\ndel scaled_cols_test","4108e81f":"!pip3 install BorutaShap\nfrom BorutaShap import BorutaShap","54356595":"confirmed_important = ['f86', 'f1', 'f71', 'f2', 'f95', 'f112', 'f10', 'f5', 'f70', 'f32', 'f16', 'f83', 'nullcount', 'f11', 'f14', 'f107', 'f12', 'f69', 'f3', 'f8', 'f62', 'f96', 'f102', 'f34', 'f24', 'f42', 'f21', 'f40', 'f65', 'f48', 'f43', 'f104', 'f25', 'f36', 'f77', 'f35', 'f47', 'f52', 'f9', 'f53', 'f31', 'f111', 'f45', 'f46', 'f78', 'f92', 'f27', 'f103', 'f57', 'f28', 'f38']\ntentative = ['f78_cubed_root', 'f109', 'f106', 'f81', 'f117', 'f73', 'f30', 'f7', 'f60', 'f75', 'f97', 'f116']","c5b3ff22":"#drop everything that was not important.  Keep the missing value flag columns.\ncols_to_drop = [col for col in imputed_X_train.columns if col not in confirmed_important + tentative and not col.endswith('missing') and not col.endswith('binned')]","d689d77f":"imputed_X_train.drop(columns = cols_to_drop, axis=1, inplace=True)","d50486c8":"test_df.drop(columns = cols_to_drop, axis=1, inplace=True)","59b6e8ce":"#baseline classifier model from Previous EDA notebook\nmodel = XGBClassifier(random_state=0, verbosity=0, tree_method='gpu_hist',use_label_encoder=False,n_estimators=500,learning_rate=0.05,n_jobs=4)\n\nFeature_Selector = BorutaShap(model = model, importance_measure='shap', classification=True)\n\nFeature_Selector.fit(X=imputed_X_train, y=y_train, n_trials=20, random_state=0)","b9cfbabc":"Feature_Selector.plot(which_features='all')","79f8c3fb":"Feature_Selector.Subset().columns","94683955":"\ntentative_features_final = ['f116', 'f75', 'f7', 'f11', 'f8', 'f117', 'f12', 'f102', 'f69', 'f78', 'f97', 'f30', 'f104', 'f10']\n","2f2f36d8":"features_to_keep = list(Feature_Selector.Subset().columns) + tentative_features_final","043be9f8":"print('Final feature set includes {} features.'.format(len(features_to_keep)))","e228cdcb":"#copy the training dataset\nX = train_df.copy()\ny = X.pop('claim')","dc9becb1":"X = X[features_to_keep]\ntest_df = test_df[features_to_keep]","5fc6b691":"#split the dataset into a training\/validation set \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0,train_size=0.8, test_size=0.2)","1c131a7c":"float_cols = [col for col in X_train if col != 'claim']\n#save the minmax scaler function as a variable\nmm_scaler = MinMaxScaler()\n\n#min-max scale the numeric columns only.  In this case, that is every column.\n\n#fit and transform the training df\nscaled_cols_train = pd.DataFrame(mm_scaler.fit_transform(X_train[float_cols]),index = X_train.index, columns = X_train.columns)\n\n#just transform the validation and test df.  \nscaled_cols_valid = pd.DataFrame(mm_scaler.transform(X_valid[float_cols]),index = X_valid.index, columns = X_valid.columns)\nscaled_cols_test = pd.DataFrame(mm_scaler.transform(test_df),index= test_df.index, columns = test_df.columns)","1d9134dd":"#removing variables to save memory\ndel X\ndel y\ndel X_train\ndel X_valid","d555a4fb":"# Imputing AFTER min-max scaling so the mean imputation is on the same scale.\n\n#set simple imputer variable.  By default, this imputs using the mean to replace missing values\nmy_imputer = SimpleImputer()\n\n#fit and transform the training df\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(scaled_cols_train), index=scaled_cols_train.index)\n\nimputed_X_valid = pd.DataFrame(my_imputer.transform(scaled_cols_valid), index=scaled_cols_valid.index)\nimputed_X_test = pd.DataFrame(my_imputer.transform(scaled_cols_test), index=test_df.index)\n\n\n# Imputation removed column names; put them back\n\nimputed_X_train.columns = scaled_cols_train.columns\nimputed_X_valid.columns = scaled_cols_valid.columns\nimputed_X_test.columns = test_df.columns","089d1c3f":"#removing variables to save memory\ndel scaled_cols_valid\ndel scaled_cols_test","aabd56f2":"final_model = XGBClassifier(random_state=0, verbosity=0, tree_method='gpu_hist',use_label_encoder=False,n_estimators=1550,learning_rate=0.05,n_jobs=4)\nfinal_model.fit(imputed_X_train, y_train,\n             verbose = False,\n             eval_set = [(imputed_X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\npreds_valid = final_model.predict_proba(imputed_X_valid)[:,1]\nprint(roc_auc_score(y_valid, preds_valid))","49542340":"predictions = final_model.predict_proba(imputed_X_test)[:,1]\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': imputed_X_test.index,\n                       'claim': predictions})\noutput.to_csv('submission.csv', index=False)","12e2e3e0":"<a id=\"1.1\"><\/a>\n\n## Load Libraries","9a913bef":"<a id=\"14\"><\/a>\n\n### Positively Skewed Features","738b5f11":"### BorutaShap One More Time with features selected during first iteration plus '_missing' engineered features and '_binned' features","291f3ef6":"***[back to top](#top)***\n\n<a id=\"p2\"><\/a>\n\n### Min-Max Scaling\n<br>\n<br>\n\nDuring EDA, I found all the feature columns are numeric (float64) but they are on several different scales.  I'm using min-max scaling to put all the numbers on the same scale.","6d8ceda7":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"24\"><\/a>\n\n### 10 Bins","1b26f3f4":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"7\"><\/a>\n## Creating Some Features","f80c6eec":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"18\"><\/a>\n\n## Preprocess Number 2\n\n<br>\n\nNow we have new features added to the dataset, I will re-preprocess the data with scaling and imputation","b8114f7a":"<br>\n\n<a id=\"33\"><\/a>\n\n### Filter Features And Final Train\/Test Split","b8afbe9b":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"30\"><a\/>\n\n#### An inital run of BorutaShap, found the following features to be important\n\n<br>\n\nIn my first run of BorutaShap, I included all default feature columns, plus the cubed_root engineered features, max, min, average and nullcount engineered features.  Using that feature set and XGBoost Classifier, the below features were found to be important or tentative.  In the next run of BorutaShap, I will exclude all unimportant features and test all the missing row flag features to see if those are important to the model.\n\n\n<br>\n\nconfirmed_important = ['f86', 'f1', 'f71', 'f2', 'f95', 'f112', 'f10', 'f5', 'f70', 'f32', 'f16', 'f83', 'nullcount', 'f11', 'f14', 'f107', 'f12', 'f69', 'f3', 'f8', 'f62', 'f96', 'f102', 'f34', 'f24', 'f42', 'f21', 'f40', 'f65', 'f48', 'f43', 'f104', 'f25', 'f36', 'f77', 'f35', 'f47', 'f52', 'f9', 'f53', 'f31', 'f111', 'f45', 'f46', 'f78', 'f92', 'f27', 'f103', 'f57', 'f28', 'f38']\n\n<br>\n\ntentative = ['f78_cubed_root', 'f109', 'f106', 'f81', 'f117', 'f73', 'f30', 'f7', 'f60', 'f75', 'f97', 'f116']","4d3fa008":"***[back to top](#top)***\n\n<a id=\"36\"><a\/>","5b65f784":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"22\"><a\/>\n\n## BorutaShap\n    \n<br>\n    \nFrom the [documentation site](https:\/\/pypi.org\/project\/BorutaShap\/)\n\n<br>\n    \n***BorutaShap is a wrapper feature selection method which combines both the Boruta feature selection algorithm with shapley values. This combination has proven to out perform the original Permutation Importance method in both speed, and the quality of the feature subset produced. Not only does this algorithm provide a better subset of features, but it can also simultaneously provide the most accurate and consistent global feature rankings which can be used for model inference too.***","153c60fe":"***[back to top](#top)***\n\n<a id=\"15\"><\/a>\n\n### Negatively Skewed Features\n\n<br>\n\nTry square, cube root, and log transformations","c6997116":"\n\n<a id=\"1\"><\/a>\n\n## Introduction And Findings From Exploratory Data Analysis","6f7be834":"***[back to top](#top)***","901df033":"Code Taken From Kaggle's Feature Engineering Course:  [Link To Kaggle's Feature Engineering Course Page Here](https:\/\/www.kaggle.com\/ryanholbrook\/mutual-information)","b2dfd1a5":"***[back to top](#top)***\n\n<a id=\"20\"><\/a>\n\n### Min-Max Scaling","478ba97b":"##### I think, based on the above charts, there are several features that could we could try transforming using a cubed root transformation.  ","f92d8fb3":"***[back to top](#top)***\n\n<a id=\"p1\"><\/a>\n\n### Train\/Test Split","81743524":"<a id=\"2\"><\/a>\n\n## Find Data Files In Input Folder and Read Them Into Pandas DataFrame","ec042073":"***[back to top](#top)***","0aa73a6a":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"28\"><\/a>\n\n### Binning Findings\/Conclusions and Bin Creation\n\n<br>\n\nIs there signal here?  Not sure.  The bars all seem very close throughout the bin distributions.  But, several of the features seem to have a little separation in bin1 so I'm just going to try those and see if it helps.","f5eed6f6":"\n<a id=\"3\"><\/a>\n\n## Dataset Dimensions\n\nJust as a check, make sure we read in our datasets successfully.","4b8232aa":"***[back to top](#top)***\n\n<br>\n\n<a id=\"6a\"><\/a>\n\n### Mutual Information Findings\/Conclusions","d85fa15b":"<a id=\"19\"><a\/>\n    \n### Train\/Test Split","3e51f218":"\n<a id=\"4\"><\/a>\n\n## Preprocess The Data (Same As EDA Notebook)","23b2a534":"#### Potential Choices:\n* f44:  **(1\/3)\n* f52:  **(1\/3)\n* f63:  **(1\/3)\n* f64:  **(1\/3)\n* f68:  **(1\/3)\n* f78:  **(1\/3)\n* f82:  **(1\/3)\n* f87:  **(1\/3)","4cd9f325":"***[back to top](#top)***\n\n<a id=\"21\"><a\/>\n    \n### Imputation","0561aa61":"#### In my first submission, including EDA, Preprocess and Baseline Model.  The best baseline model score was only ***0.79414 AUC***.  Hopefully, all this feature selection will improve the baseline model performance. [Link Here To My First Notebook](https:\/\/www.kaggle.com\/abrambeyer\/tps-sep21-eda-preprocess-baseline-model)\n\n<br>\n\n#### I am using the same parameters as my baseline model.  The only difference is the feature selection plus one engineered feature:  nullcount","46e0a509":"<a id=\"8\"><\/a>\n\n### Counting Missing Values Per Row\n\nPer @craigmthomas, he found counting missing values per row to be informative to the model.  [Link to discussion here](https:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/discussion\/274758)\n\n<br>\n\nStarting with that based on the suggestion!","acae79a3":"***[back to top](#top)***\n<a id=\"6\"><a\/>\n\n### Mutual Information Scores Plot And Findings","3c5ae450":"***[back to top](#top)***\n\n<a id=\"8a\"><\/a>\n\n### Flagging Missing Value Rows\n\n<br>\n\nWe now know counting all the columns with missing values per row appears to be pretty informative to the model (Confirmed by performing BorutaShap below).  The new 'nullcount' column is actually the most informative column in the dataset.  This was not difficult since none of the features were correlated with the target column and all of the features also had tiny mutual information scores.  \n\n<br>\n\nWhen I impute the missing values during pre-processing, I lose this signal because now all the missing value cells are imputed with a mean.  To extend upon this, what if we create new features flagging which rows were missing in the original dataset then impute.  This way, we can still see where things were missing in the original dataset.\n\nThe idea for this feature column came from Kaggle's and Alexis Cook's Intermediate Machine Learning Course \"Missing Values\" section.  [Link to the lesson here.](https:\/\/www.kaggle.com\/alexisbcook\/missing-values)","bbe9b0de":"##### All those features turned out to not be helpful in the model!  The only engineered feature that was helpful was 'nullcount.'  The rest turned out to be noise.  ","558d1db2":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"31\"><a\/>\n\n### Final Feature Set","242a2527":"### Drop Unimportant Columns","85837957":"1.  The default feature set includes 118 variables with no one feature having mutual information greater than 0.005.\n2.  There are no highly-correlated features in the default feature set.  None of the features are correlated with the target variable.\n3.  After trying binning, flagging missing values, averaging columns, max of columns, mins of columns, the best feature was simplying counting how many columns had a missing value in a row.  I called this feature \"nullcount.\"\n4.  Running BorutaShap on the final engineered dataset, the following features were found to be important to the XG Boost Classifier:\n    ['f27', 'f47', 'f45', 'f16', 'f34', 'f36', 'f62', 'f106', 'f57', 'f24',\n       'f103', 'f2', 'f5', 'f83', 'f21', 'f107', 'f3', 'f28', 'f96', 'f31',\n       'f40', 'f42', 'f95', 'nullcount', 'f77', 'f32', 'f92', 'f53', 'f65',\n       'f111', 'f38', 'f48', 'f35', 'f52', 'f70']\n5.  By creating the 'nullcount' column and using the above feature set, I was able to improve my baseline model performance from ***0.79414*** to ***0.81351***.","f44a3e3a":"##### According to the mutual information algorithm, no feature has a mutual information score of greater than 0.005.\n\n##### A Mutual Information Score of 0 indicates no dependencies with a score of 1 being the maximum.  All the features ***appear to have almost zero***\n##### relationship with the y variable.\n\n##### With Mutual Information scores so low, it doesn't appear super helpful.","e9e1b885":"<a id=\"12\"><\/a>\n\n### Max Across Features","2bde1e9e":"\n<br>\n<br>\n\n***[back to top](#top)***","9ca1cdb4":"***[back to top](#top)***\n\n<a id=\"9\"><\/a>\n\n### Counting Feature Values Greater Than Zero and Less Than Zero","959a3efd":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"32\"><a\/>\n\n### Final Preprocess","ddb0ff4d":"##### As @craigmthomas mention, the more missing values in a row, the more it seems it is likely a person made a claim.  Shout out to him for the suggestion.","c9df720c":"<br>\n<br>\n\n***[back to top](#top)***","c7e86946":"***[back to top](#top)***\n\n<a id=\"17\"><\/a>\n\n## Transforming Features","de807a13":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"23\"><\/a>\n\n### Looking For Binning Opportunities\n\n<br>\n\nThis dataset is supposed to be based on insurance claim data.  Even though it is simulated data, I would assume there's got to be some financial information about income or amounts previously claimed.  I would imagine someone's salary or amount previously claimed would be informative to the model.  Even though I already found none of the features are correlated with the target variable, perhaps from bin categorical variables would be.  Let's check.","7348ff85":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"27\"><\/a>\n\n### 3 Bins","d790d018":"***[back to top](#top)***\n\n<a id=\"38\"><a\/>\n\n### TLDR:  Summary\/Findings","46f7e591":"<br>\n<br>\n\n***[back to top](#top)***\n\n<a id=\"26\"><\/a>\n\n### 5 Bins","cf120ff2":"<br>\n<br>\n\n***[back to top](#top)***","89f24cba":"### Potential Choices:\n* f89:  **(1\/3)\n* f95:  **(1\/3)\n* f101: **(1\/3)\n* f102: **(1\/3)\n* f103: **(1\/3)\n* f118: **(1\/3)","e609bca8":"***[back to top](#top)***\n\n<a id=\"p3\"><a\/>\n    \n### Imputation","297d8811":"The goal of this notebook is to expand upon my earlier exporatory data analysis (EDA) of the Tabular Playground Series - Sep 2021 dataset, make some decisions about feature importance, possibly creates some new features, and train a model to beat my previous submissions.  [Link to my previous notebook covering EDA, preprocessing, and my baseline model](https:\/\/www.kaggle.com\/abrambeyer\/tps-sep21-eda-preprocess-baseline-model)\n\nIf you see any areas for improvement, I'm happy to hear about it.  Thanks!\n\n<br>\n\n***From the TPS September 2021 Competition Description Page:***\n\n*The dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.*\n\n*Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.*\n\n*For each id in the test set, you must predict a probability for the claim variable. The file should contain a header and have the following format:  id, claim*\n\n<br>\n\n***Findings From Previous EDA:***\n1. Training Dataset Shape:  ***(957919, 119)***\n2. Test Dataset Shape: ***(493474, 118)***\n3. This is a ***huge*** dataset that will likely test the default Kaggle CPU and RAM allocation.  GPU will likely be needed for faster iteration.  \n4. Due to necessity of GPU, models such as sklearn's RandomForest Classifier may not be appropriate due to its inability to work with GPU. Better choices might be XGBoost, Catboost, RAPIDS RandomForest, and other GPU-friendly classifier models\/packages.  Check out this discussion for more [tips on using GPU in Kaggle](https:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/discussion\/271900#1511854)\n5. The target column \"claim\" is a binary integer column with no missing values.  This is a probabilistic classification problem.  Models such as logistic regression, tree based models such as DecisionTrees, RandomForest, XGBoostClassifier, etc.\n6. All feature columns are float64 datatype\n7. All feature columns have at least one missing value with the most sparse column only missing 1.6% of its data.\n8. 62% of the rows have at least one missing value.\n9. The target \"claim\" column has balanced classes.  Only a 0.6% difference between class value counts.\n10. The training and test datasets have the same columns, column types and similar distributions of all feature columns.\n11. All 118 feature columns have at least one missing value.  Missing values appear randomly dispersed throughout the dataset.\n12. There are 67 feature columns with extremely skewed distributions.  Some distributions even look like categorical due to appeared binning of values with short ranges.\n13. Several feature columns have very negative kurtosis indicating possible outliers.\n14. None of the columns are correlated with each other (correlation is very small).\n15. None of the feature columns are correlated with the target column (correlation is very small).\n16. The feature columns (all numeric) appear to be on different scales.  For example, some features are on a 0-1 scale, some are in the 10,000s, some features have negative values.\n17. Ideas for feature engineering:  Lots of skewed numeric columns.  I'd like to try some transformations to see if those would improve the model.  Binning numeric columns.  Clustering the feature columns to created a new cluster feature.\n\n***Inspiration Acknowledgments:***\n\nI recently noted another Kaggler with similar (but much better) notebook organization style to myself.  Although I did not take the table of contents idea from his\/her notebooks, I am borrowing the idea to add links \"back to top.\" of the notebook and also to hide my code from view unless toggled.  I think it looks nicer and is a better experience for the reader.  [Link to awesome example notebook from Kaggler @dwin183287](https:\/\/www.kaggle.com\/dwin183287\/kagglers-seen-by-continents)","47e2776a":"***[back to top](#top)***\n\n<a id=\"37\"><a\/>\n\n### Final Baseline Predictions With Selected Features","c7e58926":"<a id=\"11\"><\/a>\n\n### Min Across Features","76361e26":"#### Potential choices:\n\n* f4:  **(1\/3)\n* f8:  **(1\/3)\n* f14:  **(1\/3)\n* f30:  **(1\/3)\n* f38: **(1\/3)\n* f39: **(1\/3)","9771b93a":"***[back to top](#top)***\n\n<a id=\"16\"><\/a>\n\n## Transformation Findings","f622926d":"\n\n### Baseline Model With Selected Features (Previous Best Score:  0.79414 AUC).","377cc107":"<br>\n<br>\n<br>\n\n\n***[back to top](#top)***","c8f9b92a":"***[back to top](#top)***\n\n<a id=\"10\"><\/a>\n\n### Average Across Features","c9353f5e":"<br>\n<br>\n\n***[back to top](#top)***","09a36d37":"### Potential Choices:\n* f58:  **(1\/3)\n* f110: **(1\/3)","5647226b":"***[back to top](#top)***\n\n<a id=\"35\"><a\/>\n    \n### Final Imputation","173b5e77":"\n<a id=\"5\"><\/a>\n\n## Baseline Mutual Information\n<br>\n<br>\nI saw in my EDA process there was very little correlation in the dataset between any columns.  None of the feature columns were even mildly correlated with the 'claim' column nor were any correlated with any other feature column.  Kind of a bummer.\n<br>\n<br>\nI'm going to try mutual information regression first to see if I can create some sort of ranking of feature importance so I can, hopefully, eliminate noise from the data.  \n<br>\n<br>\nPer Kaggle's Feature Engineering Course:\n\n***Mutual information describes relationships in terms of uncertainty. The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target?***","cea09636":"<a id=\"top\"><\/a>\n\n# Data Exploration Part 2:  Feature Selection\n\n<br>\n<br>\n\n## Table of Contents\n* [Introduction And Findings From Exploratory Data Analysis](#1)\n* [Load Libraries](#1.1)\n* [Find Data Files In Input Folder and Read Them Into Pandas DataFrame](#2)\n* [Dataset Dimensions](#3)\n* [Preprocess The Data (Same As EDA Notebook)](#4)\n    * [Train\/Test Split](#p1)\n    * [Min-Max Scaling](#p2)\n    * [Imputation](#p3)\n* [Baseline Mutual Information](#5)\n    * [Mutual Information Scores Plot And Findings](#6)\n    * [Mutual Information Findings\/Conclusions](#6a)\n* [Creating Some Features](#7)\n    * [Counting Missing Values Per Row](#8)\n    * [Flagging Missing Value Rows](#8a)\n    * [Counting Feature Values Greater Than Zero and Less Than Zero](#9)\n    * [Average Across Features](#10)\n    * [Min Across Features](#11)\n    * [Max Across Features](#12)\n    * [Check For Transformation Ideas](#13)\n        * [Positively Skewed Features](#14)\n        * [Negatively Skewed Features](#15)\n        * [ Transformation Findings](#16)\n        * [Transforming Features](#17)\n    * [Looking For Binning Opportunities](#23)\n        * [10 Bins](#24)\n        * [5 Bins](#25)\n        * [3 Bins](#27)\n        * [Binning Findings\/Conclusions and Bin Creation](#28)\n* [Preprocess Number 2](#18)\n    * [Train\/Test Split](#19)\n    * [Min-Max Scaling](#20)\n    * [Imputation](#21)\n* [BorutaShap](#22)\n    * [An inital run of BorutaShap, found the following features to be important](#30)\n    * [Final Feature Set](#31)\n* [Final Preprocess](#32)\n    * [Filter Features And Final Train\/Test Split](#33)\n    * [Final Min-Max Scaling](#34)\n    * [Final Imputation](#35)\n* [Baseline Model With Selected Features (Previous Best Score:  0.79414 AUC).](#36)\n    * [Final Baseline Predictions With Selected Features](#37)\n* [TLDR:  Summary\/Findings](#38)","b91fc32e":"<br>\n<br>\n\n\n***[back to top](#top)***","3ff54268":"#### The below list of columns to bin comes from the above bar charts.  I visually scanned all charts to look for charts with more visual separation between claim levels.  Also, during the first iteration of BorutaShap done previously on the default dataset, several features were tagged as important to the XG Boost Classifier model.  I'm going to bin those as well.","a50b84f8":"#### Now only include the important feature found above","d0331505":"<br>\n\n***[back to top](#top)***\n\n<a id=\"34\"><a\/>\n\n### Final Min-Max Scaling","91b993c2":"#### checking for any possible transformations of positively skewed features.  Checking Cubed Root, Squared Root, or Log transforms","3a422773":"***[back to top](#top)***\n\n<a id=\"13\"><\/a>\n\n### Check For Transformation Ideas"}}