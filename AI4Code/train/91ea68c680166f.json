{"cell_type":{"ef93056f":"code","59f63e53":"code","51889ee7":"code","7a695022":"code","1157767d":"code","e8991de6":"code","9dddb608":"code","b30b61b6":"code","ca18512e":"code","6bf2bce8":"code","9be6d5bb":"code","7fe94669":"code","6787ba34":"code","bed7fc3f":"code","01529f89":"code","a31ed10a":"code","3ed5dd4b":"code","a61343ca":"code","346c4c61":"code","4bacda65":"code","59e0eeff":"code","f8336dd6":"code","89275b56":"code","cf70aa76":"code","c0739659":"code","bc8f1c85":"code","7769fc28":"markdown","04fcbbc7":"markdown","f39c9a20":"markdown","017402c5":"markdown","530de85f":"markdown","5f3857a4":"markdown","4e1324c4":"markdown","4215b93b":"markdown","d2ac3388":"markdown","6ef3968e":"markdown","1da06701":"markdown"},"source":{"ef93056f":"!pip install pytorch-tabnet wget","59f63e53":"from pytorch_tabnet.tab_model import TabNetClassifier\n\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\n\n\nimport os\nimport wget\nfrom pathlib import Path\nimport shutil\nimport gzip\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline","51889ee7":"url = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/covtype\/covtype.data.gz\"\ndataset_name = 'forest-cover-type'\ntmp_out = Path('.\/data\/'+dataset_name+'.gz')\nout = Path(os.getcwd()+'\/data\/'+dataset_name+'.csv')","7a695022":"out.parent.mkdir(parents=True, exist_ok=True)\nif out.exists():\n    print(\"File already exists.\")\nelse:\n    print(\"Downloading file...\")\n    wget.download(url, tmp_out.as_posix())\n    with gzip.open(tmp_out, 'rb') as f_in:\n        with open(out, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n    \n","1157767d":"target = \"Covertype\"\n\nbool_columns = [\n    \"Wilderness_Area1\", \"Wilderness_Area2\", \"Wilderness_Area3\",\n    \"Wilderness_Area4\", \"Soil_Type1\", \"Soil_Type2\", \"Soil_Type3\", \"Soil_Type4\",\n    \"Soil_Type5\", \"Soil_Type6\", \"Soil_Type7\", \"Soil_Type8\", \"Soil_Type9\",\n    \"Soil_Type10\", \"Soil_Type11\", \"Soil_Type12\", \"Soil_Type13\", \"Soil_Type14\",\n    \"Soil_Type15\", \"Soil_Type16\", \"Soil_Type17\", \"Soil_Type18\", \"Soil_Type19\",\n    \"Soil_Type20\", \"Soil_Type21\", \"Soil_Type22\", \"Soil_Type23\", \"Soil_Type24\",\n    \"Soil_Type25\", \"Soil_Type26\", \"Soil_Type27\", \"Soil_Type28\", \"Soil_Type29\",\n    \"Soil_Type30\", \"Soil_Type31\", \"Soil_Type32\", \"Soil_Type33\", \"Soil_Type34\",\n    \"Soil_Type35\", \"Soil_Type36\", \"Soil_Type37\", \"Soil_Type38\", \"Soil_Type39\",\n    \"Soil_Type40\"\n]\n\nint_columns = [\n    \"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\",\n    \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n    \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n    \"Horizontal_Distance_To_Fire_Points\"\n]\n\nfeature_columns = (\n    int_columns + bool_columns + [target])\n","e8991de6":"train = pd.read_csv(out, header=None, names=feature_columns)\n\nn_total = len(train)\n\n# Train, val and test split follows\n# Rory Mitchell, Andrey Adinets, Thejaswi Rao, and Eibe Frank.\n# Xgboost: Scalable GPU accelerated learning. arXiv:1806.11248, 2018.\n\ntrain_val_indices, test_indices = train_test_split(\n    range(n_total), test_size=0.2, random_state=0)\ntrain_indices, valid_indices = train_test_split(\n    train_val_indices, test_size=0.2 \/ 0.6, random_state=0)\n","9dddb608":"categorical_columns = []\ncategorical_dims =  {}\nfor col in train.columns[train.dtypes == object]:\n    print(col, train[col].nunique())\n    l_enc = LabelEncoder()\n    train[col] = train[col].fillna(\"VV_likely\")\n    train[col] = l_enc.fit_transform(train[col].values)\n    categorical_columns.append(col)\n    categorical_dims[col] = len(l_enc.classes_)\n\nfor col in train.columns[train.dtypes == 'float64']:\n    train.fillna(train.loc[train_indices, col].mean(), inplace=True)","b30b61b6":"unused_feat = []\n\nfeatures = [ col for col in train.columns if col not in unused_feat+[target]] \n\ncat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n","ca18512e":"clf = TabNetClassifier(\n    n_d=64, n_a=64, n_steps=5,\n    gamma=1.5, n_independent=2, n_shared=2,\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    scheduler_params = {\"gamma\": 0.95,\n                     \"step_size\": 20},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15\n)","6bf2bce8":"if os.getenv(\"CI\", False):\n# Take only a subsample to run CI\n    X_train = train[features].values[train_indices][:1000,:]\n    y_train = train[target].values[train_indices][:1000]\nelse:\n    X_train = train[features].values[train_indices]\n    y_train = train[target].values[train_indices]\n\nX_valid = train[features].values[valid_indices]\ny_valid = train[target].values[valid_indices]\n\nX_test = train[features].values[test_indices]\ny_test = train[target].values[test_indices]","9be6d5bb":"max_epochs = 5 if not os.getenv(\"CI\", False) else 2","7fe94669":"clf.fit(\n    X_train=X_train, y_train=y_train,\n    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n    eval_name=['train', 'valid'],\n    max_epochs=max_epochs, patience=100,\n    batch_size=16384, virtual_batch_size=256\n) ","6787ba34":"# plot losses\nplt.plot(clf.history['loss'])","bed7fc3f":"# plot accuracy\nplt.plot(clf.history['train_accuracy'])\nplt.plot(clf.history['valid_accuracy'])","01529f89":"# To get final results you may need to use a mapping for classes \n# as you are allowed to use targets like [\"yes\", \"no\", \"maybe\", \"I don't know\"]\n\npreds_mapper = { idx : class_name for idx, class_name in enumerate(clf.classes_)}\n\npreds = clf.predict_proba(X_test)\n\ny_pred = np.vectorize(preds_mapper.get)(np.argmax(preds, axis=1))\n\ntest_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n\nprint(f\"BEST VALID SCORE FOR {dataset_name} : {clf.best_cost}\")\nprint(f\"FINAL TEST SCORE FOR {dataset_name} : {test_acc}\")","a31ed10a":"# or you can simply use the predict method\n\ny_pred = clf.predict(X_test)\ntest_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\nprint(f\"FINAL TEST SCORE FOR {dataset_name} : {test_acc}\")","3ed5dd4b":"# save state dict\nsaved_filename = clf.save_model('test_model')","a61343ca":"# define new model and load save parameters\nloaded_clf = TabNetClassifier()\nloaded_clf.load_model(saved_filename)","346c4c61":"loaded_preds = loaded_clf.predict_proba(X_test)\nloaded_y_pred = np.vectorize(preds_mapper.get)(np.argmax(loaded_preds, axis=1))\n\nloaded_test_acc = accuracy_score(y_pred=loaded_y_pred, y_true=y_test)\n\nprint(f\"FINAL TEST SCORE FOR {dataset_name} : {loaded_test_acc}\")","4bacda65":"assert(test_acc == loaded_test_acc)","59e0eeff":"clf.feature_importances_","f8336dd6":"explain_matrix, masks = clf.explain(X_test)","89275b56":"fig, axs = plt.subplots(1, 5, figsize=(20,20))\n\nfor i in range(5):\n    axs[i].imshow(masks[i][:50])\n    axs[i].set_title(f\"mask {i}\")","cf70aa76":"n_estimators = 1000 if not os.getenv(\"CI\", False) else 20","c0739659":"from xgboost import XGBClassifier\n\nclf_xgb = XGBClassifier(max_depth=8,\n    learning_rate=0.1,\n    n_estimators=n_estimators,\n    verbosity=0,\n    silent=None,\n    objective=\"multi:softmax\",\n    booster='gbtree',\n    n_jobs=-1,\n    nthread=None,\n    gamma=0,\n    min_child_weight=1,\n    max_delta_step=0,\n    subsample=0.7,\n    colsample_bytree=1,\n    colsample_bylevel=1,\n    colsample_bynode=1,\n    reg_alpha=0,\n    reg_lambda=1,\n    scale_pos_weight=1,\n    base_score=0.5,\n    random_state=0,\n    seed=None,)\n\nclf_xgb.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            early_stopping_rounds=40,\n            verbose=10)","bc8f1c85":"preds_valid = np.array(clf_xgb.predict_proba(X_valid, ))\nvalid_acc = accuracy_score(y_pred=np.argmax(preds_valid, axis=1) + 1, y_true=y_valid)\nprint(valid_acc)\n\npreds_test = np.array(clf_xgb.predict_proba(X_test))\ntest_acc = accuracy_score(y_pred=np.argmax(preds_test, axis=1) + 1, y_true=y_test)\nprint(test_acc)","7769fc28":"# Network parameters","04fcbbc7":"# Download ForestCoverType dataset","f39c9a20":"# Simple preprocessing\n\nLabel encode categorical features and fill empty cells.","017402c5":"# Load data and split\nSame split as in original paper","530de85f":"# Local explainability and masks","5f3857a4":"# XGB","4e1324c4":"# Training","4215b93b":"# Global explainability : feat importance summing to 1","d2ac3388":"# Save and load Model","6ef3968e":"### Predictions\n","1da06701":"# Define categorical features for categorical embeddings"}}