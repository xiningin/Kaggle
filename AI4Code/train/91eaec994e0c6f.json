{"cell_type":{"b8ae77b4":"code","396c1732":"code","49cb5074":"code","5af1de94":"code","70b28483":"code","7a0a6a0d":"code","528bb307":"code","960a257e":"code","7a418fd2":"code","48822ff1":"code","9e316b31":"code","b8b203b0":"code","076b54db":"code","fab271b8":"code","eaea5620":"code","49e5cd5f":"code","95d2c8d4":"code","0ef29b57":"code","c488ee1f":"code","47925b02":"code","c88e6f07":"code","6dc3787e":"code","7d8b027a":"code","3e850db2":"code","64b219aa":"code","2eb917c3":"code","1e0ca165":"code","20aa1227":"code","d4f06aef":"code","c8a6cdd1":"code","13935cef":"code","24124f14":"code","c2f3d348":"code","deca7368":"code","a7aacbe3":"code","707f47d8":"code","d57b7e6c":"code","3b5b34f1":"code","222c2c7d":"code","22c8c302":"code","4661b831":"code","ee3e3cdc":"code","265af6ad":"code","6b4b806b":"code","43888204":"code","8a6c08d6":"code","d551d00d":"code","be992092":"code","a254519c":"code","2d546fa1":"code","375b4fcf":"code","99728644":"code","c9fc4374":"code","4c99b087":"code","aebdc869":"code","71ba99af":"code","e748dee0":"code","28d58495":"code","2490d806":"code","7866ff21":"code","7dabc262":"code","459a8f5c":"code","57c5ca19":"code","3fe344a7":"code","d971535b":"code","4b9bc240":"code","f218a3c6":"code","a06b60dc":"code","a595569d":"code","5179b885":"code","5b42c02c":"code","126bc140":"code","aa10f58b":"code","bc4649f6":"code","7458f33d":"code","e9f6dd67":"code","3ae036a3":"code","586305ef":"code","5642a807":"code","8d03245f":"code","66335146":"code","d28b804f":"code","ebd430fb":"code","65c58e51":"code","3721fc77":"code","c1377e9c":"code","b73fd38f":"code","a0fa66a8":"code","ef1c6a9f":"code","63cee113":"code","5fe8c701":"code","fa499190":"code","ee2b3970":"code","0ee51687":"code","97e01013":"code","97ac6125":"code","561945eb":"code","111a3d63":"code","07591931":"code","cbabce0a":"code","08aa988b":"code","83e6ec26":"code","7f4f1642":"code","d35a7416":"code","846407c8":"code","ea32d2f2":"code","7f614e7e":"code","10d319e1":"markdown","c99384ec":"markdown","00509a32":"markdown","8a391540":"markdown","959e6a20":"markdown","66a5e4f0":"markdown","4609a8e0":"markdown","acce4e9b":"markdown","0b24a896":"markdown","c646841b":"markdown","5eacf533":"markdown","476dc48b":"markdown","675b7070":"markdown","00ecd64b":"markdown","6ad726ca":"markdown","55d455cb":"markdown","d700acc3":"markdown","73345816":"markdown","3c1e812f":"markdown","4579c562":"markdown","25a7f9d1":"markdown","e260ad95":"markdown","1f016756":"markdown","0b7b8053":"markdown","850b77e1":"markdown","ec050696":"markdown","8bb6ccac":"markdown","663717f8":"markdown","daa1ecb5":"markdown","cd8720bc":"markdown","d3eed99e":"markdown","f4fd2ff8":"markdown","c77d4804":"markdown","aa0fcb36":"markdown","47116c22":"markdown","5c3ddb94":"markdown","2c3ff275":"markdown","192535c9":"markdown","e47cb05e":"markdown","f4de24d2":"markdown","93c4b63b":"markdown","4773a1d0":"markdown","45359a18":"markdown"},"source":{"b8ae77b4":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport numpy as np\nfrom matplotlib.gridspec import GridSpec\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly, plot_components_plotly, add_changepoints_to_plot\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.api as sm\nimport calendar\n","396c1732":"import plotly.io as pio\npio.templates[\"draft\"] = go.layout.Template(\n    layout_annotations=[\n        dict(\n            textangle=-30,\n            opacity=0.1,\n            font=dict(color=\"black\", size=100),\n            xref=\"paper\",\n            yref=\"paper\",\n            x=0.5,\n            y=0.5,\n            showarrow=False,\n        )\n    ]\n)\npio.templates.default = \"draft\"","49cb5074":"# import plotly.offline as py\n# py.init_notebook_mode(connected=True)","5af1de94":"pd.set_option('display.max_columns', 2000)","70b28483":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7a0a6a0d":"calendar_data = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\ncalendar_data.head()","528bb307":"sales_train_validation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsales_train_validation.head()","960a257e":"sales_train_evaluation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nsales_train_evaluation.head()","7a418fd2":"train_data = sales_train_validation.copy()","48822ff1":"states = train_data.state_id.unique().tolist()\nprint(f\"States Present in The Dataset: {states} ({len(states)})\")","9e316b31":"stores = train_data.store_id.unique().tolist()\nprint(f\"Stores Present in The Dataset: {stores} ({len(stores)})\")","b8b203b0":"categories = train_data.cat_id.unique().tolist()\nprint(f\"Categories Present in The Dataset: {categories} ({len(categories)})\")","076b54db":"items = train_data.dept_id.unique().tolist()\nprint(f\"Items Present in The Dataset: {items} ({len(items)})\")","fab271b8":"print(f\"There are {len(train_data.item_id.unique())} Items in The Dataset\")","eaea5620":"print(f\"Total Numner of Time Series: {len(train_data.id.unique())} !\")","49e5cd5f":"train_data[:10]","95d2c8d4":"d_cols = train_data.columns.tolist()[6:] # Sales columns\nnon_d_cols = list(reversed(train_data.columns.tolist()[:6])) ","0ef29b57":"train_data.loc[train_data.d_100 == train_data.d_100.max()]","c488ee1f":"def merge_with_calendar(data, calendar_data):\n    # data should have a date column \"d\"\n    assert 'd' in data.columns, 'DataFrame should have a column \"d\" !'\n    # Merge With Calendar\n    cal = calendar_data[['d', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n                    'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']]\n    d = pd.merge(cal, data, on=\"d\")\n    # Fill Missing Event Values with None\n    for col in ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']:\n        d[col].fillna('None', inplace=True)\n    return d","47925b02":"def get_ts_example(data, d_cols, calendar_data, item_id, store_id, idx=None):\n    try:\n        if idx is None:\n            ts = data.loc[(data['item_id'] == item_id) & (data['store_id'] == store_id)]\n            ts = ts[d_cols].T.reset_index()\n            ts.columns = ['d', 'sales']\n        else:\n            ts = data.loc[idx][d_cols].reset_index()\n            ts.columns = ['d', 'sales']\n        # Make sure that sales column's type is int\n        ts[\"sales\"] = ts[\"sales\"].astype(\"int\")\n        return merge_with_calendar(ts, calendar_data)\n    except Exception as e:\n        print(f'Can not extract time series: {e}')","c88e6f07":"ts = get_ts_example(train_data, d_cols, calendar_data, item_id='FOODS_3_586', store_id='TX_3')","6dc3787e":"ts","7d8b027a":"print(f\"Length of the time series: {len(ts)} days.\")","3e850db2":"print(f\"Years in the TS: {ts.year.unique()}\")","64b219aa":"print(ts['event_type_2'].value_counts())","2eb917c3":"events_1_data = ts['event_type_1'].value_counts().iloc[1:]\nfig = plt.figure(figsize=(11, 5))\nax = fig.add_subplot()\nax.pie(x=events_1_data.values,\n       labels=events_1_data.index,\n       shadow=True,\n       radius=1,\n       autopct='%1.1f%%')\nax.set_title('Distribution of Type 1 Events')\nplt.show()","1e0ca165":"fig = plt.figure(figsize=(20, 6))\nax = fig.add_subplot()\nax.plot(ts.date, ts.sales)\nax.set_xticks(ts.date.values[::90])\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nax.grid()\nax.set_title(f'Sales of FOODS_3_586 in TX_3 Store')\nplt.show()","20aa1227":"# Area Plot\nfig = px.area(ts, \n              x='date',\n              y='sales',\n              title='Time Series Yearly Area Plot',\n              facet_row='year',\n              facet_row_spacing=0.05)\nfig.update_layout(width=900,\n                 height=900)\n             \nfig.show()","d4f06aef":"for year in ts.year.unique():\n    t = ts[ts.year == year]\n    fig = plt.figure(figsize=(20, 6))\n    ax = fig.add_subplot()\n    t_event_1 = t.loc[t.event_type_1 != 'None']\n    ax.plot(t.date, t.sales)\n    ax.scatter(t_event_1.date,\n               t_event_1.sales,\n               color='red',\n               label='Type 1 Event')\n    ax.set_xticks(t.date.values[::30])\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.grid()\n    ax.set_title(f'Sales of FOODS_3_586 in TX_3 store for {year}')\n    ax.legend()\n    plt.show()","c8a6cdd1":"fig = plt.figure(figsize=(20, 15))\nax1 = fig.add_subplot(311)\nsns.boxplot(data=ts, x='month', y='sales', ax=ax1)\nax2 = fig.add_subplot(312)\nsns.boxplot(data=ts, x='weekday', y='sales', ax=ax2)\nax3 = fig.add_subplot(313)\nsns.boxplot(data=ts, x='event_type_1', y='sales', ax=ax3)\nplt.show()","13935cef":"fig = px.histogram(ts,\n                   x='sales',\n                   marginal='box',\n                   title='Sales Distribution for FOODS_3_586 at TX_3 store')\nfig.show()","24124f14":"fig = px.line(data_frame=ts, \n              x='date', \n              y='sales', \n              color='year', \n              title='Sales of FOODS_3_586 at TX_3 Store',)\nfig.update_layout(legend=dict(x=1,\n                              y=1,\n                              title_font_family=\"Times New Roman\",\n                              bgcolor=\"snow\",\n                              bordercolor=\"Black\",\n                              borderwidth=1),\n                  font=dict(size=11)\n                 )\n# plot and legend are Interactive !\nfig.show()","c2f3d348":"for feature in ['wday', 'month', 'year', 'event_name_1']: #wday = weekday\n    data_feature = ts.groupby(feature).mean()['sales'].reset_index().sort_values(by='sales')\n    fig = px.bar(data_frame=data_feature,\n          x=feature,\n          y='sales',\n          title=f'Average Sales by {feature}')\n    fig.show()","deca7368":"d = train_data.groupby(['state_id','store_id']).count().reset_index()\nd['state_id'].value_counts().plot(kind=\"bar\", grid=True, title=\"(A) Number of Stores by State\", yticks=[0,1,2,3,4])\nplt.show()\nd = train_data.groupby(['state_id','id']).count().reset_index()\nd['state_id'].value_counts().plot(kind=\"bar\", grid=True, title=\"(B) Number of Items by State\")\nplt.show()\nd = train_data.groupby(['store_id','item_id']).count().reset_index()\nd['store_id'].value_counts().plot(kind=\"bar\", grid=True, title=\" (C) Number of Items by Store\")\nplt.show()","a7aacbe3":"train_data.head()","707f47d8":"# Transform Data Structure\ndata = train_data.set_index(non_d_cols)\n# the following will make one column for sales and one columns for \"d\" values (d_1 ... d_1913)\ndata = data.stack()\ndata = data.to_frame() \ndata.columns = [\"sales\"]\ndata.reset_index(inplace=True)\ndata.columns = non_d_cols + [\"d\", \"sales\"]","d57b7e6c":"data","3b5b34f1":"def plot_daily_data(df,level):\n    levels_dict = {'cat':'Category', 'dept':'Department', 'store':'Store', 'state':'State'}\n    fig = px.line(data_frame=df, \n                  x='date', \n                  y='sales', \n                  color=f'{level}_id', \n                  title=f'Sales by {levels_dict[level]}')\n    fig.update_layout(legend=dict(x=1,\n                                  y=1,\n                                  title_font_family=\"Times New Roman\",\n                                  bgcolor=\"snow\",\n                                  bordercolor=\"Black\",\n                                  borderwidth=1),\n                      font=dict(size=11)\n                     )\n    fig.show()","222c2c7d":"def plot_yearly_data(df,level):\n    levels_dict = {'cat':'Category', 'dept':'Department', 'store':'Store', 'state':'State'}\n    n_years = df.year.nunique()\n    years = df.year.unique()\n    level_elements = df[f'{level}_id'].unique().tolist()  \n    all_colors= ['red','green','blue','purple','cyan','orange','pink','yellow','black','magenta']\n    colors = all_colors[:len(level_elements)]\n    fig = plt.figure(figsize=(15,15))\n    gs = GridSpec(n_years, len(level_elements))\n    c_idx = 0\n    for l_e, color in zip(level_elements,colors):\n        r_idx = 0\n        for year in years:\n            ax = fig.add_subplot(gs[r_idx,c_idx], xticks=[], yticks=[])\n            df1 = df.loc[(df.year == year) & (df[f'{level}_id'] == l_e)]\n            ax.plot(df1.date, df1['sales'], color=color, linewidth=0.9)\n            ax.set_title(f'{l_e}: {year}')\n            r_idx += 1\n        c_idx+=1\n    fig.suptitle(f'Yearly Sales by {levels_dict[level]}')\n    plt.show()","22c8c302":"def plot_average_sales(df,level):\n    for feature in ['weekday', 'month', 'year', 'event_name_1']:\n        data_feature = df.groupby([f'{level}_id', feature]).mean()['sales'].reset_index()\n        fig = px.bar(data_frame=data_feature,\n              x=feature,\n              y='sales',\n              color=f'{level}_id',\n              title=f'Average Sales by {feature}')\n        fig .update_layout(legend=dict(x=1,\n                                       y=1,\n                                       title_font_family=\"Times New Roman\",\n                                       bgcolor=\"mintcream\",\n                                       bordercolor=\"black\",\n                                       borderwidth=1))\n        fig.show()","4661b831":"# Preparing state-level data\nstate_data = data.groupby([\"state_id\",\"d\"]).sum()[\"sales\"]\nstate_data = state_data.reset_index()\nstate_data = merge_with_calendar(state_data, calendar_data)","ee3e3cdc":"state_data.head()","265af6ad":"plot_daily_data(state_data,'state')","6b4b806b":"fig = px.histogram(state_data,\n                   x='sales',\n                   color='state_id',\n                   marginal='box',\n                   title='Sales Distribution By State')\nfig.show()","43888204":"dlow = state_data.loc[(state_data['sales']<20) & (state_data['month']==12)]\ndlow.style.applymap(lambda x:\"background-color:yellow\", subset=['event_name_1'])","8a6c08d6":"plot_yearly_data(state_data,'state')","d551d00d":"plot_average_sales(state_data,'state')","be992092":"# Preparing store-level data\nstore_data = data.groupby([\"store_id\",\"d\"]).sum()[\"sales\"]\nstore_data = store_data.reset_index()\nstore_data = merge_with_calendar(store_data, calendar_data)","a254519c":"store_data.head()","2d546fa1":"plot_daily_data(store_data,'store')","375b4fcf":"fig = px.histogram(store_data,\n                   x='sales',\n                   color='store_id',\n                   marginal='box',\n                   title='Sales Distribution By Store')\nfig.show()","99728644":"plot_yearly_data(store_data,'store')","c9fc4374":"plot_average_sales(store_data,'store')","4c99b087":"corr_data = pd.pivot_table(data=store_data,\n                           index='date',\n                           values='sales',\n                           columns='store_id')\ncorr_data.sort_values(by=\"date\", inplace=True)\nplt.figure(figsize=(12,5))\nheatmap = sns.heatmap(corr_data.corr(), annot=True, fmt='.2f')\nheatmap.set_yticklabels(heatmap.yaxis.get_ticklabels(), rotation=0)\nheatmap.set_title('Correlation Between Stores Sales')\nplt.show()","aebdc869":"train_data.groupby('cat_id').count()['id'].reset_index().plot(x='cat_id', \n                                                              kind='bar', \n                                                              figsize=(15,5),\n                                                              grid=True,\n                                                              title='Number of Items by Category')\nplt.show()","71ba99af":"# Preparing category-level data\ncat_data = data.groupby([\"cat_id\",\"d\"]).sum()[\"sales\"]\ncat_data = cat_data.reset_index()\ncat_data = merge_with_calendar(cat_data, calendar_data)","e748dee0":"cat_data.head()","28d58495":"plot_daily_data(cat_data,'cat')","2490d806":"fig = px.histogram(cat_data,\n                   x='sales',\n                   color='cat_id',\n                   marginal='box',\n                   title='Sales Distribution By Category')\nfig.show()","7866ff21":"plot_yearly_data(cat_data,'cat')","7dabc262":"plot_average_sales(cat_data,'cat')","459a8f5c":"years = cat_data.year.unique()\nfor year in [years[0],years[-1]]:\n    for cat in cat_data.cat_id.unique():\n        dyear = cat_data.loc[(cat_data.year == year) & (cat_data.cat_id == cat)] # & (cat_data.cat_id == cat)\n        fig = go.Figure(data=go.Heatmap(\n                z=dyear.sales, #z,\n                x=dyear.date, #dates,\n                y=dyear.cat_id, #programmers,\n                colorscale=px.colors.sequential.Plasma_r))\n\n        fig.update_layout(\n            title=f'{cat} Sales {year}',\n            xaxis_nticks=36)\n\n        fig.show()","57c5ca19":"train_data.groupby('dept_id').count()['id'].reset_index().plot(x='dept_id', \n                                                              kind='bar', \n                                                              figsize=(15,5),\n                                                              grid=True,\n                                                              title='Number of Items by Department')\nplt.show()","3fe344a7":"# Preparing department-level data\ndept_data = data.groupby([\"dept_id\",\"d\"]).sum()[\"sales\"]\ndept_data = dept_data.reset_index()\ndept_data = merge_with_calendar(dept_data, calendar_data)","d971535b":"dept_data.head()","4b9bc240":"plot_daily_data(dept_data,'dept')","f218a3c6":"fig = px.histogram(dept_data,\n                   x='sales',\n                   color='dept_id',\n                   marginal='box',\n                   title='Sales Distribution By Department')\nfig.show()","a06b60dc":"plot_yearly_data(dept_data,'dept')","a595569d":"plot_average_sales(dept_data,'dept')","5179b885":"corr_data = pd.pivot_table(data=dept_data,\n                           index='date',\n                           values='sales',\n                           columns='dept_id')\ncorr_data.sort_values(by=\"date\", inplace=True)\nplt.figure(figsize=(12,5))\nheatmap = sns.heatmap(corr_data.corr(), annot=True, fmt='.2f')\nheatmap.set_yticklabels(heatmap.yaxis.get_ticklabels(), rotation=0)\nheatmap.set_title('Correlation Between Department Sales')\nplt.show()","5b42c02c":"forecast_horizon = 28 # from d_1914 to d_1941\nd_fcst_columns = sales_train_evaluation.columns[-forecast_horizon:].tolist()","126bc140":"def get_ground_truth(idx, df, d_fcst_columns):\n    return df.loc[idx, d_fcst_columns].values  ","aa10f58b":"def plot_results(fcst, y_eval, rmse, algo, item):\n    fig = plt.figure(figsize=(11, 5))\n    ax = fig.add_subplot()\n    ax.plot(fcst, color='red', label='Forecast')\n    ax.plot(y_eval, color='blue', label='Ground Truth')\n    ax.set_title(f' {algo} for {item}, RMSE: {rmse}')\n    ax.grid()\n    ax.legend()\n    plt.show()","bc4649f6":"choice_data = train_data.copy()\nchoice_data['d_val'] = choice_data[d_cols].mean(axis=1)\nchoice_data.drop(columns=d_cols,inplace=True)","7458f33d":"print(f'Time Series Averages: Min {choice_data.d_val.min()} Max:{choice_data.d_val.max()} Median: {choice_data.d_val.median()}')","e9f6dd67":"idx1 = choice_data.loc[choice_data['d_val'] >= 50].sample(n=3, random_state=1).index\nidx2 = choice_data.loc[(choice_data['d_val'] <= 5) & (choice_data['d_val'] > 1)].sample(n=3, random_state=1).index\nidx = idx1.tolist() + idx2.tolist()\nts_test = train_data.iloc[idx]#.reset_index(drop=True)\ntest_items = ts_test.id.unique().tolist()\ntest_items = [x[:-11] for x in test_items]","3ae036a3":"test_items","586305ef":"# Dataframe for RMSE\nrmse_summary = pd.DataFrame({\"items\":test_items}, index=idx)","5642a807":"def choose_sarimax_order_and_forecast(ps,ds,qs, y_train, y_eval):\n    best_model, best_rmse, best_order, best_fcst = None, None, None, None\n    for p in ps:\n        for d in ds:\n            for q in qs:\n                order = (p,d,q)\n                model = sm.tsa.SARIMAX(y_train, \n                               order=order, \n                               trend='c',\n                               enforce_invertibility=False,\n                               enforce_stationarity=False).fit(disp=False, warn_convergence=False)\n                fcst = model.predict(start=len(y_train), end=len(y_train) - 1 + len(y_eval))\n                try:\n                    fcst = [round(x) for x in fcst]\n                    rmse = round(np.sqrt(mean_squared_error(fcst, y_eval)), 3)\n                    if (best_rmse is None) or (rmse < best_rmse):\n                        best_model, best_rmse, best_order, best_forecast= model, rmse, order, fcst\n                except Exception as e:\n                    print(f'For order={order}, model results are invalid: {e}')\n    print(f\"Best Order: {best_order}\")\n    return best_rmse, best_forecast","8d03245f":"df0 = get_ts_example(ts_test, d_cols, calendar_data, item_id=None, store_id=None, idx=idx[0])\ndf0['date'] = df0['date'].apply(lambda x : pd.to_datetime(x))\ndf0 = df0[['date','sales']]\ny_train = df0[\"sales\"].values\ndf0","66335146":"result = seasonal_decompose(y_train, model='additive', period=365)\nfig = result.plot()","d28b804f":"fig, ax = plt.subplots(2,1,figsize=(20,7))\nsm.tsa.graphics.plot_acf(y_train, lags=30, ax=ax[0])\nax[0].set_title('Autocorreation Function: lags=30')\nsm.tsa.graphics.plot_pacf(y_train, lags=30, ax=ax[1])\nax[1].set_title('Partial Autocorreation Function: lags=30')\nplt.show()","ebd430fb":"# Plotting Autocorrelation with pandas\nfig, ax = plt.subplots(1,1,figsize=(20,7))\npd.plotting.autocorrelation_plot(y_train, ax=ax)\nplt.show()","65c58e51":"sarimax_model = sm.tsa.SARIMAX(y_train, \n                               order=(7,1,7), \n                               trend='c',\n                               enforce_invertibility=False,\n                               enforce_stationarity=False).fit(disp=False, warn_convergence=False)\nsarimax_model.summary()","3721fc77":"y_eval = get_ground_truth(idx[0], sales_train_evaluation, d_fcst_columns)","c1377e9c":"fcst = sarimax_model.predict(start=len(y_train), end=len(y_train) - 1 + len(y_eval))\nrmse = round(np.sqrt(mean_squared_error(fcst, y_eval)), 3)","b73fd38f":"plot_results(fcst, y_eval, rmse, \"SARIMAX\", test_items[0])","a0fa66a8":"# SARIMAX Parameters Grid\nps = range(1,8)\nds = range(0,2)\nqs = range(0,8)","ef1c6a9f":"rmse_sarimax = []\nfor i,ix in enumerate(idx):\n    print(f\"Processing {test_items[i]}...\")\n    # Get Time Series (Train)\n    df0 = get_ts_example(ts_test, d_cols, calendar_data, item_id=None, store_id=None, idx=ix)\n    df0['date'] = df0['date'].apply(lambda x : pd.to_datetime(x))\n    df0 = df0[['date','sales']]\n    y_train = df0[\"sales\"].values\n    y_eval = get_ground_truth(ix, sales_train_evaluation, d_fcst_columns)\n    # Train SARIMAX model\n    rmse, fcst = choose_sarimax_order_and_forecast(ps,ds,qs, y_train, y_eval)\n    # Plot\n    plot_results(fcst, y_eval, rmse, \"SARIMAX\", test_items[i])\n    rmse_sarimax.append(rmse)","63cee113":"# Dataframe for RMSE\nrmse_summary = pd.DataFrame({\"items\":test_items}, index=idx)","5fe8c701":"rmse_summary[\"RMSE_SARIMAX\"] = rmse_sarimax\nrmse_summary","fa499190":"def generate_holidays(calendar_data, dates):\n    holidays = calendar_data.loc[calendar_data['d'].isin(dates), ['date','event_name_1']].dropna()\n    holidays['ds'] = holidays['date'].apply(lambda x : pd.to_datetime(x))\n    holidays['upper_window'] = 0\n    holidays['lower_window'] = 0\n    holidays.rename(columns={\"event_name_1\":\"holiday\"}, inplace=True)\n    holidays.drop(columns='date', inplace=True)\n    holidays.reset_index(drop=True, inplace=True)\n    return holidays","ee2b3970":"# holidays for FB prophet model\nholidays = generate_holidays(calendar_data, d_cols+d_fcst_columns)\nholidays","0ee51687":"df = get_ts_example(ts_test, d_cols, calendar_data, item_id=None, store_id=None, idx=idx[0])\ndf['ds'] = df['date'].apply(lambda x : pd.to_datetime(x))\ndf = df[['ds','sales']]\ndf = df.rename(columns={'sales':'y'})\ndf","97e01013":"df['cap'] = df['y'].max()\ndf['floor'] = df['y'].min()","97ac6125":"# Creating Model\nmodel = Prophet(daily_seasonality=True, \n                holidays=holidays,\n                holidays_prior_scale=0.2,\n                growth='logistic', # possible value: 'logistic', 'linear' or 'flat'\n                changepoint_range=0.8, # default value\n                changepoint_prior_scale=0.2) # default value\nmodel.add_seasonality(name='weekly', period=7, fourier_order=3)\nmodel.add_seasonality(name='yearly', period=364, fourier_order=10)\nmodel.fit(df)","561945eb":"# Forecasting\nfuture = model.make_future_dataframe(periods=forecast_horizon)\nfuture['cap'] = df['y'].max()\nfuture['floor'] = df['y'].min()\nforecast = model.predict(future)","111a3d63":"fig1 = model.plot(forecast)\na = add_changepoints_to_plot(fig1.gca(), model, forecast)","07591931":"# Interactive plots\nplot_plotly(model, forecast)","cbabce0a":"# Model Components\nplot_components_plotly(model, forecast)","08aa988b":"fcst = forecast['yhat'].values[-forecast_horizon:]\nfcst = [int(x) for x in fcst]\nground_truth = get_ground_truth(idx[0], sales_train_evaluation, d_fcst_columns)\nrmse = round(np.sqrt(mean_squared_error(ground_truth, fcst)), 3)","83e6ec26":"fig = plt.figure(figsize=(11, 5))\nax = fig.add_subplot()\nax.plot(fcst, color='red', label='Forecast')\nax.plot(ground_truth, color='blue', label='Ground Truth')\nax.set_title(f'FB Prophet for {test_items[i]}, RMSE: {rmse}')\nax.grid()\nax.legend()\nplt.show()","7f4f1642":"# FB Prophet Parameters Grid\nchangepoint_prior_scale = [0.0001, 0.001, 0.1, 0.5]\nseasonality_prior_scale = [0.01, 0.1, 1, 10]\nholiday_prior_scale = [0.1, 0.2, 0.5, 1]","d35a7416":"def tune_fbprophet_and_forecast(y_eval, df, forecast_horizon, chngp, sps, hps):\n    best_rmse, best_fcst = None, None\n    # Choosing the Best Parameters\n    for p1 in chngp:\n        for p2 in sps:\n            for p3 in hps:\n                # Training Model\n                model = Prophet(daily_seasonality=True, \n                                holidays=holidays,\n                                holidays_prior_scale=p3,\n                                seasonality_prior_scale=p2,\n                                growth='logistic', \n                                changepoint_range=0.8, \n                                changepoint_prior_scale=p1)\n                model.add_seasonality(name='weekly', period=7, fourier_order=3)\n                model.add_seasonality(name='yearly', period=364, fourier_order=10)\n                model.fit(df)\n                # Forecasting\n                future = model.make_future_dataframe(periods=forecast_horizon)\n                future['cap'] = df['y'].max()\n                future['floor'] = df['y'].min()\n                forecast = model.predict(future)\n                # Evaluating\n                fcst = forecast['yhat'].values[-forecast_horizon:]\n                fcst = [int(x) for x in fcst]\n                rmse = round(np.sqrt(mean_squared_error(y_eval, fcst)), 3)\n                if (best_rmse is None) or (rmse < best_rmse):\n                    best_rmse, best_fcst = rmse, fcst\n    return best_rmse, best_fcst","846407c8":"rmse_fbprophet = []\nfor i,ix in enumerate(idx):\n    print(f\"Processing {test_items[i]}...\")\n    # Get Time Series (Train)\n    df = get_ts_example(ts_test, d_cols, calendar_data, item_id=None, store_id=None, idx=ix)\n    df['ds'] = df['date'].apply(lambda x : pd.to_datetime(x))\n    df = df[['ds','sales']]\n    df = df.rename(columns={'sales':'y'})\n    # Add Cap and Floor Columns\n    df['cap'] = df['y'].max()\n    df['floor'] = df['y'].min()\n    # Get Ground Truth\n    y_eval = get_ground_truth(ix, sales_train_evaluation, d_fcst_columns)\n    # Train FBProphet model\n    rmse, fcst = tune_fbprophet_and_forecast(y_eval, df, forecast_horizon, changepoint_prior_scale, seasonality_prior_scale, holiday_prior_scale)\n    # Plot\n    plot_results(fcst, y_eval, rmse, \"FB Prophet\", test_items[i])\n    rmse_fbprophet.append(rmse)","ea32d2f2":"rmse_summary[\"RMSE_FBPROPHET\"] = rmse_fbprophet\nrmse_summary","7f614e7e":"r1 = round(rmse_summary.RMSE_FBPROPHET.mean(), 3)\nr2 = round(rmse_summary.RMSE_SARIMAX.mean(), 3)\nprint(f\"MEAN RMSE SARIMAX: {r2}, FBProphet: {r1}\")","10d319e1":"- A simple observation: from the first plot we can see that the biggest part of sales of this item (<i>FOODS_3_586<\/i>) in this store (<i>TX_3<\/i>) is during Month 8 (August).<br>\n- This very detailed level (<i>Item level<\/i>) won't generate many insights, aggregated levels will do such as <i>State<\/i>, <i>Store<\/i>, <i>Category<\/i> and <i>Department<\/i> levels.","c99384ec":"<b> Plots <\/b>","00509a32":"### 2.3.2 Store Level Analysis","8a391540":"An interesting pattern to observe is that, for this item example, sales decrease to the lowest level at the end of each year !","959e6a20":"- `calendar.csv` - Contains information about the dates on which the products are sold.\n- `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n- `sample_submission.csv` - The correct format for submissions. Reference the Evaluation tab for more info.\n- `sell_prices.csv` - Contains information about the price of the products sold per store and date.\n- `sales_train_evaluation.csv` - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)","66a5e4f0":"Let's take the example of item <b>FOODS_3_586<\/b> sales in California store <b>TX_3<\/b>","4609a8e0":"## 2.2 Example of Time Series","acce4e9b":"### 2.3.1 State Level Analysis","0b24a896":"Let's plot a Sales Calendar Heatmap of the first and last years (by Categroy).","c646841b":"<i>`sales_train_validation` Dataset is our train data set: [D1 - D1913].<\/i> <br>\n<i>`sales_train_evalutaion` Dataset is data used to evaluate our models, it contains [D1914 - D1941].<\/i>","5eacf533":"### 2.3.4 Department Level Analysis","476dc48b":"- Let's see Sales' correlations between different Stores.","675b7070":"## 2.3 Aggregated Level Analysis","00ecd64b":" - The Data hierarchy is:<br>\n <b>State<\/b> ==>  <b>Store<\/b> ==>  <b>Category<\/b> ==>  <b>Department<\/b> ==>  <b>Item<\/b>","6ad726ca":"order = (p,d,q)\n- p: AutoRegression (AR) order.\n- d: Trend Differncing order.\n- q: Moving Average (MA) order. <br>","55d455cb":"<b>For plotly plots, you can double click on legend to visualize data parts separately. You can also zoom in, zoom out and autoscale plots.<\/b>","d700acc3":"We will choose:\n- 3 Time Series with enough data and few zeros.\n- 3 Time Series with many zeros.","73345816":"<b> It was Christmas effect !<\/b>","3c1e812f":"# 2. Exploratory Data Analysis","4579c562":"Plotly offers some interesting plots and visualizations ! Let's try some of them at our time series.","25a7f9d1":"- We will use <b>pandas.DataFrame.stack<\/b> function to transform our data.\n- The result data should have as columns <b>['d', 'state_id','store_id','cat_id','dept_id','item_id', 'id', 'sales']<\/b> where \"d\" column has values in <br>[d_1,..., d_1913].\n- Initially we have data with 10 stores and 3049 items per store so as a result we have 30490 time series !\n- Each time series contains sales data of 1913 days. Final output data will have then 8 columns and 30490*1913 = 58327370 rows !","e260ad95":"Types:\n- <b>ARIMA<\/b>: Non-Seasonal. \n- <b>SARIMA<\/b>: Seasonal ARIMA.\n- <b>SARIMAX<\/b>: Seasonal ARIMA with eXogenous variables.","1f016756":"We can easily notice that the item's Average sale is at its maximum at Father's Day.","0b7b8053":"## 3.2 Statistical Method: ARIMA","850b77e1":"### 2.3.3 Catgeory Level Analysis","ec050696":"FB Prophet requires a column 'ds' (for dates) and a columns 'y' (target variable).","8bb6ccac":"- The highest correlation is between CA_1 and CA_3 stores, in the same state.\n- The lowest correlation is between WI_1 and WI_3 stores, in the same state!","663717f8":"- CA_3 is the store having the highest number of sales.","daa1ecb5":"- Now we'll go through more aggregated analysis.","cd8720bc":"- FOODS_3 and HOBBIES_2 have respectively the highest and lowest number of sales among all departments.","d3eed99e":"- In this part, we'll choose 6 Time Series from the 30490 ones we have and use them to test different forecasting approaches and evaluate them.","f4fd2ff8":"# 1. Read Data","c77d4804":"## 2.1 General Information","aa0fcb36":"## 3.1 Time Series Selection","47116c22":"- FOODS is the category having the highest number of sales, HOBBIES having the lowest one.","5c3ddb94":"## 3.3 FB Prophet","2c3ff275":"<i>The followoing are function that will be used for different analysis.<\/i>","192535c9":"<b>ARIMA<\/b> stands for <b>A<\/b>uto<b>R<\/b>egressive <b>I<\/b>ntegrated <b>M<\/b>oving <b>A<\/b>verage.","e47cb05e":"- Sales average is increasing over years.\n- Weekends correspond to the highest sales average.\n- Sales average is almost the same over different months.","f4de24d2":"<i>From 1913 days there are only 4 days with type 2 event! As a result we'll ignore that event type.<\/i>","93c4b63b":"After the analyzing and visualizing part comes the forecast part  !","4773a1d0":"# 3. Forecast","45359a18":"- California has the highest number of sales.\n- We observe again the sales decrease to their lowest level (previously observed with a single item time series) at the end of every year, let's try to get more details about this pattern !"}}