{"cell_type":{"aa09e51f":"code","388c102f":"code","f159c1ed":"code","d1786cbd":"code","d100156b":"code","ba1aa169":"code","c4e3ae7c":"code","d782156b":"code","0a0c5101":"code","5be41648":"code","5506236b":"code","ad4f357d":"code","80671274":"code","87fbdbb7":"code","27484ade":"code","5ea21c49":"code","7e871a58":"code","d6fe0e54":"code","a2f22d14":"code","e72714c6":"code","a460e754":"code","486826f4":"code","56ce1e30":"code","00f7b117":"code","46e6dbba":"code","4004d851":"code","0100990f":"code","a253521c":"code","e2796390":"code","ce77f8d4":"code","a8c2e19f":"code","ece9b4ab":"code","6e964509":"code","f58a6622":"code","138003d1":"code","4189cc91":"code","41f046fe":"code","2336111e":"code","91e55979":"code","bd68fc7b":"code","89d44d4f":"code","58020908":"code","2621f1df":"code","c40e910f":"code","1eecb126":"code","759863f3":"code","a41f846f":"code","3b3f6640":"code","d565e86d":"code","34c604a9":"code","b493a693":"code","3f875940":"code","3cb3bc94":"code","3f4e1c14":"code","c867c839":"code","5ba5014f":"code","ae7e3e0c":"code","42380c17":"code","53065576":"code","3a82a099":"code","e3eaa3c1":"code","f82f1fa7":"code","ccabe855":"code","ce04290e":"markdown","c8526404":"markdown","fa0c8979":"markdown","d39c1e9a":"markdown","5c764887":"markdown","48c85344":"markdown","835b968c":"markdown","db0bfb34":"markdown","9a287809":"markdown","4ecc1e4a":"markdown","47703149":"markdown","b5cd7ce3":"markdown","dcf504f7":"markdown","78d421e3":"markdown","564c4fa6":"markdown","87d737d1":"markdown","79feaaca":"markdown","53cb71a6":"markdown","2989136b":"markdown","044b51d8":"markdown","4410745b":"markdown","248eb9fe":"markdown","cd4dc44e":"markdown","591e003c":"markdown","f795e547":"markdown","2a0c4c01":"markdown","61dc0968":"markdown","6fa9a9df":"markdown","24f40ce6":"markdown","95cebc6a":"markdown","17afb815":"markdown","83186a0e":"markdown"},"source":{"aa09e51f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.utils import resample\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n%matplotlib inline","388c102f":"import keras\nfrom keras import backend as k\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy","f159c1ed":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf  =pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","d1786cbd":"df.head()","d100156b":"# Datset shape\ndf.shape","ba1aa169":"# let's check what different classes are present in our dataset\n\ndf['Class'].unique()","c4e3ae7c":"_ = sns.countplot(x='Class',data=df)","d782156b":"fig, ax = plt.subplots()\n_ = sns.kdeplot(df['Class'] , color='gray',bw=0.15)","0a0c5101":"df.describe()","5be41648":"scaler = MinMaxScaler()","5506236b":"df['Amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time'] = scaler.fit_transform(df['Time'].values.reshape(-1,1))","ad4f357d":"#what are the representation of each class in dataset, checking numbers\n\nfraudlent_df = df[df['Class']==1]\nnon_fraudlent_df = df[df['Class']==0]\n\nnum_fraudlent_df = len(df[df['Class']==1])\nnum_non_fraudlent_df = len(df[df['Class']==0])\n\nprint(f'number of fraudlent transaction : {num_fraudlent_df}')\nprint(f'number of non- fraudlent transaction : {num_non_fraudlent_df}')","80671274":"train_fraudlent_df = resample(fraudlent_df, \n                                 replace=False,    # sample without replacement\n                                 n_samples=num_fraudlent_df-100,     #  taking 392 out of 492 in train , 80%\n                                 random_state=123) # reproducible results","87fbdbb7":"train_non_fraudlent_df = resample(non_fraudlent_df, \n                                 replace=False,    # sample without replacement\n                                 n_samples=num_fraudlent_df-100,     # to match minority class, taking 392 out of 284315 in train\n                                 random_state=123) # reproducible results","27484ade":"train_non_fraudlent_df.head()","5ea21c49":"train_fraudlent_df.head()","7e871a58":"train_non_fraudlent_df.shape","d6fe0e54":"train_fraudlent_df.shape","a2f22d14":"train_df = pd.concat([train_non_fraudlent_df, train_fraudlent_df]) # final training set","e72714c6":"train_df = train_df.sample(frac=1)   # shuffling the whole train dataframe","a460e754":"train_df.head()","486826f4":"train_df_index = train_df.index","56ce1e30":"df.drop(train_df_index, inplace=True ) # after dropping the rows from train dataframe , the remaining will be used for testing","00f7b117":"train_df.reset_index(drop=True, inplace=True)","46e6dbba":"df.reset_index(drop=True, inplace=True)","4004d851":"x_train = train_df.values[:,:-1]","0100990f":"y_train = train_df.values[:,-1]","a253521c":"x_train.shape","e2796390":"y_train.shape","ce77f8d4":"x_test = df.values[:,:-1]","a8c2e19f":"y_test  = df.values[:,-1]","ece9b4ab":"x_test.shape","6e964509":"y_test.shape","f58a6622":"knn_model = KNeighborsClassifier()\nknn_model.fit(x_train, y_train)","138003d1":"predictions_knn = knn_model.predict(x_test)","4189cc91":"knn_model.score(x_test,y_test)","41f046fe":"logreg = LogisticRegression()\nlogreg.fit(x_train, y_train)","2336111e":"predictions_log = logreg.predict(x_test)","91e55979":"logreg.score(x_test,y_test)","bd68fc7b":"dec_tree = DecisionTreeClassifier(random_state=0)\ndec_tree.fit(x_train, y_train)\npredictions_dt = dec_tree.predict(x_test)","89d44d4f":"dec_tree.score(x_test,y_test)","58020908":"ran_for = RandomForestClassifier(random_state=0)\nran_for.fit(x_train, y_train)\npredictions_rf = ran_for.predict(x_test)","2621f1df":"ran_for.score(x_test,y_test)","c40e910f":"gb = GradientBoostingClassifier(random_state=0)\ngb.fit(x_train, y_train)\npredictions_gb = gb.predict(x_test)","1eecb126":"gb.score(x_test,y_test)","759863f3":"prediction_list = [predictions_knn, predictions_log,  predictions_dt, predictions_rf, predictions_gb ]\nname = ['KNN','LOGISTIC REGRESSION', 'DECISION TREE', 'RANDOM FORREST', 'GRADIENT BOOST']  ","a41f846f":"def plot_confusion_matrix(cm,classes,normalize=False,\n                          title='confusion_matrix',cmap=plt.cm.Blues):\n    plt.imshow(cm,interpolation='nearest',cmap= cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks,classes,rotation=30)\n    plt.yticks(tick_marks,classes)\n    if normalize:\n        cm = cm.astype('float')\/cm.sum(axis=1)[:,np.newaxis]\n        print('Normalized Confusion Matrix')\n    else:\n        print('confusion  matrix without normalization')\n    print(cm)\n    \n    thresh = cm.max()\/2\n    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(j,i,cm[i,j],\n                 horizontalalignment='center',\n                 color='white' if cm[i,j]>thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('true label')\n\n    plt.xlabel('predicted label')","3b3f6640":"plt.figure(figsize=(15,15))\nfor i in range(5):\n    plt.subplot(3,2,i+1)\n    cm = confusion_matrix(y_test, prediction_list[i])\n    cm_plot_labels = ['Non-Fraudlent', 'Fraudlent']\n    _ = plot_confusion_matrix(cm,cm_plot_labels,title=f'{name[i]}')","d565e86d":"activation_fn           = 'relu'\nnum_hidden_unit_layer_1 = 30\nnum_hidden_unit_layer_2 = 32\nnum_output_unit         = 2\nnum_epoch               = 20\nbatch_size              = 10\nlearning_rate           = 0.001","34c604a9":"nn=Sequential([\n    Dense(30, input_shape=(30,), activation=activation_fn),\n    Dense(32, activation=activation_fn),\n    Dense(num_output_unit, activation='softmax')\n])","b493a693":"nn.summary()","3f875940":"optimizer = Adam(lr=learning_rate)","3cb3bc94":"nn.compile(optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","3f4e1c14":"history =nn.fit(x_train, y_train, validation_split=0.2, batch_size=batch_size, epochs=num_epoch, shuffle=True, verbose=2)","c867c839":"# summarize history for accuracy\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n","5ba5014f":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n","ae7e3e0c":"score, acc = nn.evaluate(x_test, y_test,batch_size=100)","42380c17":"print(f'Test Score   : {score}')\nprint(f'Test Accuracy:   {acc}')","53065576":"predict =nn.predict(x_test)","3a82a099":"predictions = nn.predict_classes(x_test, batch_size=100, verbose=0)","e3eaa3c1":"cm = confusion_matrix(y_test, predictions)","f82f1fa7":"cm_plot_labels = ['Non-Fraudlent', 'Fraudlent']\n_ = plot_confusion_matrix(cm,cm_plot_labels,title='Confusion_matrix')","ccabe855":"# nn.save('credit_card_fraud_detection.h5')","ce04290e":"#### Define the model","c8526404":"#### Set hyperparameters","fa0c8979":"#### In above description table we can see that in case of Time and Amount columns the standard deviation, range, magnitude is very large, we need to scale those two columns","d39c1e9a":"### Random Forrest","5c764887":"#### Set optimizer ","48c85344":"<a id=\"intro\"><\/a>\n### 1. Introduction","835b968c":"# Credit Card Fraud Detection\n\n## Contents\n\n### [1. Introduction](#intro)\n\n### [2. Data Preparation](#data)\n   * **Import the required libraries**\n   * **Download and unzip the dataset**\n   * **Split the dataset**\n   \n### [3. Exploratory Analysis](#explore)\n\n### [4. Model Architecture](#cnn)\n   * **Set hyperparameters**\n   * **Define the model**\n   * **Set optimizer** \n   * **Compile model**\n   * **Train model**\n\n### [5. Model Evaluation](#eval)\n   * **Training Accuracy vs Validation Accuracy**\n   * **Training Loss vs Validation Loss**\n   * **Model Accuracy**\n   * **Observations**\n\n### [6. Prediction](#predict)\n\n### [7. Save Model to Disk](#save)\n  ","db0bfb34":"<a id=\"data\"><\/a>\n### 2. Data Preparation","9a287809":"<a id=\"save\"><\/a>\n### 7. Save Model to Disk","4ecc1e4a":"<a id=\"explore\"><\/a>\n### 3. Exploratory Analysis","47703149":"### DecisionTree","b5cd7ce3":"<a id=\"predict\"><\/a>\n### 6. Prediction","dcf504f7":"-----------------------------------------------------------------------------------------------------------------\n\n\n#### some basic ML model testing, before making  a neural net \n\n\n-----------------------------------------------------------------------------------------------------------------","78d421e3":"#### training set","564c4fa6":"### Gradient Boost","87d737d1":"### knn","79feaaca":"#### Training Loss vs Validation Loss","53cb71a6":"#### From the confusion matrix we can see that the imbalance issue has been properly handled as both the classes has pretty high test accuracy.","2989136b":"#### Import the required libraries","044b51d8":"#### Observations:\n\n96%  test accuracy is a decent value .Also, in the test set there was only 100 fraudlent transaction and remaining 283923 are non-fraudlent, so our main purpose will be to see how each label has been classified. It is important to note that if we hadn't done previous imbalance removal we may get an more accurate model , but there all fraudlent transaction would just be considered a noise. So , in that case the error would be more critical. ","4410745b":"### from kdeplot and barplot, one can easily see how this dataset is heavily imbalance ! The representation of non-fraudlent class is far more than fraudlent class. Their representation should be made almost equal , otherwise we would have an accuracy paradox !","248eb9fe":"<a id=\"cnn\"><\/a>\n### 4. Model Architecture","cd4dc44e":"#### Model Accuracy","591e003c":"#### Training Accuracy vs Validation Accuracy","f795e547":"#### our main aim here is to take equal representaion of Class 0 and Class 1 in our dataset to minimize data imabalance . I am taking almost 80% of 492 fraudlent transaction in train set.","2a0c4c01":"#### Compile model","61dc0968":"#### Train model","6fa9a9df":"\n-----------------------------------------------------\n\n\n### Split the dataset\n\n\n------------------------------------------------------","24f40ce6":"### lr","95cebc6a":"#### test set","17afb815":"#### About the dataset\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n\n#### Problem statement\nClassify the transactions as **fraud (1)** and **legitimate (0)**.\n\n#### Dataset link: https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\/","83186a0e":"<a id=\"eval\"><\/a>\n### 5. Model Evaluation"}}