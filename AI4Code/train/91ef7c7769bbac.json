{"cell_type":{"5209c27a":"code","b384d623":"code","7e5ab351":"code","04d0a52b":"code","a142a89c":"code","b360a362":"code","0c88aae4":"code","1823be0b":"code","c8fb8686":"code","fb1b3106":"markdown","1e556b24":"markdown","9a4eb4a8":"markdown","a4623cd6":"markdown","359dc16f":"markdown","06ab5d35":"markdown","d60d67ec":"markdown","45568bb9":"markdown","75ac9f5c":"markdown"},"source":{"5209c27a":"#Charles Averill, 2019","b384d623":"#Imports\nimport pandas as pd\n\nimport sklearn\n\nfrom itertools import count, islice\nfrom math import sqrt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split","7e5ab351":"#Training method\ndef train(features, labels):\n    #Separate features and labels into training and testing sets\n    ftrain, ftest, ltrain, ltest = train_test_split(features, labels, test_size=0.1)\n    #Create Random Forest Regressor with 100 trees\n    regressor = RandomForestRegressor(n_estimators=100)\n    #Fit model and get accuracy\n    model = regressor.fit(ftrain, ltrain)\n    accuracy = regressor.score(ftest, ltest) * 100\n\n    return model, accuracy","04d0a52b":"df = pd.read_csv(\"..\/input\/first-100000-prime-numbers\/output.csv\").drop(['Interval'], axis=1)","a142a89c":"features = df.drop('Num', axis=1).values\nlabels = df['Num'].values","b360a362":"model, accuracy = train(features, labels)\nprint(\"Accuracy:\", accuracy)","0c88aae4":"labels = ['Rank']\n\ncsv = df.values\n\n#num = int(input(\"What index prime do you want? \"))\nnum = 50","1823be0b":"def isPrime(n):\n    return n > 1 and all(n%i for i in islice(count(2), int(sqrt(n)-1)))","c8fb8686":"#Predicting\ninp = pd.DataFrame([[num]], columns = labels)\n\nprediction1 = int(round(model.predict(inp)[0] - .5))\nprediction2 = prediction1\n#Makes predictions more accurate\nwhile(not isPrime(prediction1) and not isPrime(prediction2)):\n    prediction1 += 1\n    prediction2 -= 1\n\n#Print values\nif(isPrime(prediction1)):\n    print(\"Prediction:\", prediction1)\nelse:\n    print(\"Prediction:\", prediction2)\n#Only prints actual value if the csv has it\nif(num < len(csv)):\n    print(\"Actual Value:\", csv[num - 1][1])","fb1b3106":"A method that checks primality","1e556b24":"Here we read in the first 100k primes from my dataset","9a4eb4a8":"Get the model's prediction for n. By adding the while loop, I change the prediction until it's prime.","a4623cd6":"Separating our data into features and labels","359dc16f":"Prompting for integer n to return the nth prime. Kaggle frontend doesn't support Python's input() function, so let's assume you asked for the 50th prime number.","06ab5d35":"Here we create our training method, using the sklearn's train_test_split to split features and labels into test and train data.","d60d67ec":"This is one of the first models I've built without a tutorial at hand. It's not too complicated, but needed improvement in the last cell with my primality while loop. However, given that the prediction is usually less than 5 away from the true value, it shouldn't impact performance time that much. This was a lot of fun to write, and I look forward to expanding the complexity of my future projects.\n\nUnfortunately, the models output by this code are about 500x larger than their training files (e.g. the 100k dataset is 1.6MB, and the model that trained on it is about 730MB), so there's no reason to use an AI for this instead of a simple program that loads primes from a CSV. It was an interesting experiment though!","45568bb9":"This is my code for a prime number predictor using SKLearn's Random Forest Regressor. My goal was to input a number n, and have the model return the nth prime number. I think it works pretty well, the predicted output is always within 5 of the true value.","75ac9f5c":"Call the train function and print the returned accuracy"}}