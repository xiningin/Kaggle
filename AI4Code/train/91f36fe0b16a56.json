{"cell_type":{"4bb520cb":"code","5429896e":"code","0c9dfac2":"code","b1e2a57c":"code","0066c549":"code","5ca855ab":"code","19534e5e":"code","3d6dad54":"code","7ea1f46d":"code","228e77ab":"code","aeb8479e":"code","d8089254":"markdown","000d8553":"markdown","4daa7ff4":"markdown","b1510738":"markdown","75cca472":"markdown","f8865ed7":"markdown","82d338ba":"markdown","465b8694":"markdown","cace0da5":"markdown","330c3023":"markdown","8f47e3aa":"markdown","c6aa61b3":"markdown"},"source":{"4bb520cb":"from datetime import datetime\nimport pytz\ntimeZone = pytz.timezone('Asia\/Kolkata') #Setting our local timezone , otherwise it will show americal timezone.\ntime_now = datetime.now(timeZone)\n# Now we will modify our time_now , to make it look prettier.\ntime_now = time_now.strftime('Date:%d-%b-%y  Time: %-I:%M %p') # for time we use %H:%M:%S for Hour\/min\/sec but it will give time in 24 hrs format ,\n                                                               #so with -- %-I:%M %p-- this we converted time into 12 hrs format\nprint(time_now)\n","5429896e":"import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nurl = 'https:\/\/en.wikipedia.org\/wiki\/List_of_countries_and_dependencies_and_their_capitals_in_native_languages'","0c9dfac2":"r = requests.get(url)\nprint(\"Request Status Code:\",r.status_code)","b1e2a57c":"Soup = BeautifulSoup(r.content,'html5lib')\n# print(Soup.prettify) #prettify() function  print the HTML code with proper indentation. But We won't need this","0066c549":"All_tables = Soup.find_all('table',class_='wikitable')","5ca855ab":"table_columns = ((All_tables[0]).find_all('tr')[0]).text\nprint(table_columns)\ntable_columns = table_columns.split('\\n')\nprint(table_columns) # we need to trim our List from whitespaces\n\nprint(table_columns[1::2])# that seems good option to give column header, so lets go\nTable_Header = table_columns[1::2]\nprint('\\n\\n\\n')\ndata= pd.DataFrame(columns=Table_Header)\ndata","19534e5e":"# From inspecting the webpage from browser we found Data we require lay in \"wikitable\" Class.\n# We will filter table we need from rest by -->find_all()<--method in BeautifulSoup.\n\nfor one_table in All_tables:\n    All_rows = one_table.find_all('tr')[1:]\n    for one_row in All_rows:\n        columns = one_row.find_all('td')\n        lxs = []\n        # Here Columns we got is in form of List so , we can appoint index and collect data from them\n            \n        Country1 = columns[0].text\n        Capital1 = columns[1].text\n        Country2 = columns[2].text\n        Capital2 = columns[3].text\n        Language5 = columns[-1].text\n#         print(Country1,'-',Country2,'-',Capital1,'-',Capital2,'-',Language5)\n        lxs = [Country1,Country2,Capital1,Capital2,Language5]\n#         dict = {'Country1':Country1,'Capital1':Capital1,'Country2':Country2,'Capital2':Capital2,'Language':Language5}\n        data.loc[len(data.index)] = lxs","3d6dad54":"# data = data[0:0]\ndata.iloc[1:,[4]]","7ea1f46d":"problem_name = data.columns[4]  # getting column name because its too big to write\ndata[problem_name] = data[problem_name].apply(lambda x:x[:-1]) # appling function to get everything from row except last entity i.e --\\n--\ndata.head()","228e77ab":"data.to_csv('wikitable.csv')","aeb8479e":"!pip install openpyxl\ndata.to_excel('wiki.xlsx')","d8089254":"**we have some problem in language column, and that problem is , there are \"\\n\" appearing as entity in Cell Value.**  \n# Cleaning DataFrame","000d8553":"# Setting-up Environment","4daa7ff4":"# Saving as CSV file","b1510738":"## Understand Status_code from Server:  \n* **1xx:**  *Informational \u2013 Communicates transfer protocol-level information.*\n* **2xx:**  *Success \u2013 Indicates that the client\u2019s request was accepted successfully.*\n* **3xx:**  *Redirection \u2013 Indicates that the client must take some additional action in order to complete their request.*\n* **4xx:**  *Client Error \u2013 This category of error status codes points the finger at clients.*\n* **5xx:**  *Server Error \u2013 The server takes responsibility for these error status codes.*","75cca472":"**The scenario:** We need to collect data about language, capital city, name and flags of different countries. So best web page we could use is Wikipedia and its about more than 200 countries detail outta there.\n![Wikipedia](https:\/\/www.frontlist.in\/wp-content\/uploads\/2021\/02\/Wikipeidia.jpg)\n","f8865ed7":"# Web Scrapping","82d338ba":"# Downloading HTML","465b8694":"*I am going to save in excel format also*","cace0da5":"# Checking DataFrame","330c3023":"# Traversing HTML","8f47e3aa":"# Parsing HTML","c6aa61b3":"# dataframe for Storage"}}