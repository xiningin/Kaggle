{"cell_type":{"ed7130f9":"code","da7cc40a":"code","d469896d":"code","87639b8c":"code","b074eb94":"code","e7ea8135":"code","a56366b5":"code","0a884ca4":"code","695c5824":"code","bf610a9d":"code","63094a64":"code","c3b341ab":"code","45550399":"code","be227ea5":"code","526e9532":"code","3a21ba97":"code","792a2876":"code","db771b0c":"code","bd07d1c0":"code","ec790c11":"code","eb1882e6":"code","de8a2a6c":"code","6e9b42e8":"code","66004a6f":"code","7d308833":"code","ba5ac385":"code","19ad7b9d":"code","b443484b":"code","c3dc1df9":"code","df35f8b4":"code","6e19ee03":"code","5c730bf1":"code","9f050ab5":"code","854e9966":"code","e5c67894":"code","2e8f4513":"code","07589dcb":"code","b1ba921a":"code","ccad7daf":"code","310f08b9":"code","2ea4e15d":"code","e09f7085":"code","30849a57":"code","80416342":"code","62b8ac90":"code","23d29343":"code","cfb97502":"code","320f8921":"code","c3fedf17":"code","5c25c4b8":"code","250d4aa8":"code","08106dbc":"code","b1fe0272":"code","93f3e5ed":"code","26e3a9f6":"code","65e42b7f":"code","dda170cd":"code","a5ff156b":"code","8716441f":"code","d3ceea4a":"code","ed49e379":"code","01abdf3d":"code","5727f6e4":"code","e0f8a5b9":"markdown","acb30196":"markdown","cacc6ce8":"markdown","3d791cf8":"markdown","5ac94a2e":"markdown","0ccf275f":"markdown","879450db":"markdown","2ac07a51":"markdown","7d07e4b0":"markdown","04ca2716":"markdown","d79884f7":"markdown","f4d3ce67":"markdown","92c316e6":"markdown","ad462ac2":"markdown","73390c4d":"markdown","9c72fde1":"markdown"},"source":{"ed7130f9":"!pip install -qqU fastai==2.1.7","da7cc40a":"import fastai; print(\"fastai:\", fastai.__version__)\nimport torch; print(\"torch:\", torch.__version__)","d469896d":"from fastai.vision.all import *\nimport torchvision ","87639b8c":"new_data_path = Path(\"..\/input\/cassava-leaf-disease-classification\/\/\")\nold_data_path = Path(\"..\/input\/cassavaold\/\")","b074eb94":"new_data_path.ls().map(lambda o: o.name)","e7ea8135":"train_images = get_image_files(new_data_path\/'train_images')\ntest_images = get_image_files(new_data_path\/'test_images')\ntrain_df = pd.read_csv(new_data_path\/'train.csv')","a56366b5":"len(train_images), len(test_images)","0a884ca4":"new_images = train_images","695c5824":"train_df['label'].value_counts()","bf610a9d":"labeldict = json.loads((new_data_path\/'label_num_to_disease_map.json').open().read())\nlabeldict = {int(k):v for k,v in labeldict.items()}","63094a64":"train_df['label'].map(labeldict).value_counts()","c3b341ab":"%%timeit\nimg1 = PILImage.create(train_images[0]) \nimg1 = ToTensor()(img1)","45550399":"%%timeit\nimg2 = torchvision.io.read_image(train_images[0].as_posix())","be227ea5":"old_train_images = get_image_files(old_data_path\/'train')\nold_test_images = get_image_files(old_data_path\/'test')\nold_unsup_images = get_image_files(old_data_path\/'extraimages')","526e9532":"old_images = old_train_images + old_test_images + old_unsup_images","3a21ba97":"len(old_images)","792a2876":"import imagehash, PIL","db771b0c":"def get_imagehash(path, hashfunc=imagehash.average_hash, hash_size=8):\n    img = PIL.Image.open(path)\n    return str(hashfunc(img, hash_size=hash_size))","bd07d1c0":"new_image_hashes = parallel(partial(get_imagehash, hashfunc=imagehash.phash, hash_size=8), train_images)\nold_image_hashes = parallel(partial(get_imagehash, hashfunc=imagehash.phash, hash_size=8), old_images)","ec790c11":"len(new_image_hashes), len(old_image_hashes)","eb1882e6":"common_hashes = list(set(new_image_hashes).intersection(set(old_image_hashes))); len(common_hashes)","de8a2a6c":"print(f\"Total of {len(common_hashes)}\/{len(old_image_hashes)} pairs might be same images\")","6e9b42e8":"common_hashes[:5]","66004a6f":"hash2new_images = defaultdict(list)\nhash2old_images = defaultdict(list)\n\nfor h, im in zip(new_image_hashes, train_images):\n    hash2new_images[h].append(im)\n\nfor h, im in zip(old_image_hashes, old_images):\n    hash2old_images[h].append(im)","7d308833":"hash2new_images[common_hashes[0]], hash2old_images[common_hashes[0]]","ba5ac385":"imgs = []\nfor h in common_hashes:\n    newimgs = [PILImage.create(o) for o in hash2new_images[h]]\n    oldimgs = [PILImage.create(o) for o in hash2old_images[h]]\n    imgs += newimgs\n    imgs += oldimgs\n    \n    if len(imgs) > 64: break","19ad7b9d":"show_images(imgs[:64], nrows=8, ncols=8)","b443484b":"duplicate_old_images = [hash2old_images[o] for o in common_hashes]\nduplicate_new_images = [hash2new_images[o] for o in common_hashes]","c3dc1df9":"len(duplicate_old_images), len(duplicate_new_images)","df35f8b4":"old_new_duplicate_pairs = list(zip(duplicate_old_images, duplicate_new_images))[0]","6e19ee03":"pd.to_pickle(old_new_duplicate_pairs, \"old_new_duplicate_pairs.pkl\")","5c730bf1":"new_image_dups = [v for k,v in hash2new_images.items() if len(v) > 1]","9f050ab5":"new_image_dups","854e9966":"old_image_dups = [v for k,v in hash2old_images.items() if len(v) > 1]","e5c67894":"len(old_image_dups), len(np.concatenate(old_image_dups))","2e8f4513":"dups = [(Path(o.parent.name)\/o.name, PILImage.create(o)) for o in np.random.choice(old_image_dups)]\ntitles, imgs = zip(*dups)\nshow_images(imgs, titles=titles)","07589dcb":"pd.to_pickle(old_image_dups, \"old_image_dups.pkl\")","b1ba921a":"len(old_images), len(new_images)","ccad7daf":"oldlabeldict = {'cbsd': 'Cassava Brown Streak Disease (CBSD)',\n                 'healthy': 'Healthy',\n                 'cmd': 'Cassava Mosaic Disease (CMD)',\n                 'cgm': 'Cassava Green Mottle (CGM)',\n                 'cbb': 'Cassava Bacterial Blight (CBB)',\n                 '0': 'Unsup', # test\n                 'extraimages': 'Unsup'}","310f08b9":"labeldict","2ea4e15d":"old_images2labels = dict(zip(old_images, [oldlabeldict[o] for o in old_images.map(lambda o: o.parent.name)]))\n\nnew_images2labels = dict(zip(train_df['image_id'], train_df['label']))\nnew_images2labels = {k:labeldict[v] for k,v in new_images2labels.items()}\nnew_images2labels = {o:new_images2labels[o.name] for o in new_images}","e09f7085":"Counter(old_images2labels.values())","30849a57":"Counter(new_images2labels.values())","80416342":"all_images2label = {**old_images2labels, **new_images2labels}","62b8ac90":"Counter(all_images2label.values())","23d29343":"len(all_images2label)","cfb97502":"label_vocab = {'Cassava Bacterial Blight (CBB)':0,\n             'Cassava Brown Streak Disease (CBSD)':1,\n             'Cassava Green Mottle (CGM)':2,\n             'Cassava Mosaic Disease (CMD)':3,\n             'Healthy':4, \n             'Unsup':5}","320f8921":"all_images = old_images + new_images; len(all_images)","c3fedf17":"# Torchvision\nsize = (224,224)\nbs = 64\ndef open_image(fn):    return TensorImage(torchvision.io.read_image(str(fn)))\n\ntfms = [[open_image, torchvision.transforms.Resize(size, )], \n        [lambda o: all_images2label[o], Categorize(label_vocab)]]\nbatch_tfms = [IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]\n\ndsets = Datasets(all_images, tfms=tfms, splits=None)\ndls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)","5c25c4b8":"show_image(dsets[0][0]);","250d4aa8":"%%time\ndls.show_batch(max_n=25)","08106dbc":"# # Fastai\n# size = (224,224)\n# bs = 64\n\n# tfms = [[PILImage.create, ToTensor, Resize(size, method='squish')], \n#         [lambda o: all_images2label[o], Categorize(label_vocab)]]\n\n# dsets = Datasets(all_images, tfms=tfms, splits=None)\n\n# batch_tfms = [IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]\n# dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms)","b1fe0272":"# show_image(dsets[0][0]);","93f3e5ed":"# %%time\n# dls.show_batch(max_n=25)","26e3a9f6":"model = create_cnn_model(resnet34, 1, pretrained=True)\nmodel = nn.Sequential(model[0], model[1][:2])\nlearner = Learner(dls, model, loss_func=CrossEntropyLossFlat)","65e42b7f":"# generate embeddings\nembedding_dl = dls.test_dl(all_images)\nembeddings, _ = learner.get_preds(dl=embedding_dl, act=noop)\ntorch.save(embeddings, \"embeddings.pth\")\npd.to_pickle(all_images, \"all_images_filenames.pkl\")","dda170cd":"# load\nembeddings = torch.load(\"embeddings.pth\")\nembeddings.shape, len(all_images)","a5ff156b":"gpu_kernel = torch.cuda.is_available()\nchunk_idxs = list(chunked(range(len(embeddings)), chunk_sz=1000))\n\nsims = []\nfor i, row_idxs in enumerate(progress_bar(chunk_idxs)):\n    row_sims = []\n    for col_idxs in progress_bar(chunk_idxs):\n        sim = F.cosine_similarity(embeddings[row_idxs].unsqueeze(0), embeddings[col_idxs].unsqueeze(1), dim=-1)    \n        row_sims.append(sim)\n    \n    if gpu_kernel: \n        if i == 2: break\n    \n    row_sims = torch.cat(row_sims, dim=0).T\n    sims.append(row_sims)","8716441f":"sims = torch.cat(sims)","d3ceea4a":"sims.shape","ed49e379":"sims = torch.triu(sims, diagonal=1)\nthresh = 0.95\nsimilar_idxs = [(i,j) for i,j in list(zip(torch.where(sims > thresh)[0].numpy(), torch.where(sims > thresh)[1].numpy())) if i != j]","01abdf3d":"print(f\"We found {len(similar_idxs)}\/{len(sims)} similar pairs\")","5727f6e4":"similar_files = []\n\nfor i,j in np.random.permutation(similar_idxs)[:20]:\n    fn1, fn2 = all_images[i], all_images[j]\n    similar_files.append((fn1, fn2))\n    \n    imgs = [open_image(fn1), open_image(fn2)]\n    titles = [all_images2label[fn1], all_images2label[fn2]]\n    titles = [all_images[i], all_images[j]]\n    show_images(imgs, titles=titles, imsize=10)","e0f8a5b9":"## Data","acb30196":"### Dups within new dataset or old dataset","cacc6ce8":"### 2) Get Embeddings ","3d791cf8":"### Dups between old and new datasets","5ac94a2e":"## 2) CNN Based Dedup","0ccf275f":"### Normalize labels ","879450db":"part 2 -> [notebook](https:\/\/www.kaggle.com\/keremt\/cassava-eda-part2-cnn-dedup-with-rapids\/)","2ac07a51":"### Up next: Rapids CuML clustering!","7d07e4b0":"You may check how average hash performs in this link: https:\/\/johannesbuchner.github.io\/imagehash\/art2.html. It looks like it has a high false positive rate and brings non-duplicates as similar when they are different. This is pretty good for us and gives us confidence that there is probably not any other duplicates.","04ca2716":"### a) New Data","d79884f7":"Let's plot first 64 sample due to notebook limit. We can see that these are indeed same images. But 4902 is not too bad if that's all the duplicates. I am not very experienced with image hash methods, so using a different hash function with different parameters might also change the result. For that reason I will let you be the judge on whether to use old competition data or not. Let me know what you think down in the comments, as I am also very interested to hear about it!\n\nYou may see new images are all horizontal images and old ones are resized in new dataset to make them horizontal too. It's probably due to how photo was originally take with the phone.","f4d3ce67":"### b) Old Data\n\nPlease upvote if you use: https:\/\/www.kaggle.com\/keremt\/cassavaold","92c316e6":"We might prefer to use either the old data or the new one for dups. It looks like old data is resized.","ad462ac2":"Let's plot a few random similar pairs","73390c4d":"CPU RAM on this GPU kernel is not enough, so we need to chunk embeddings in to rows and columns. Also it's pretty slow, so let me know down in the comments if you have a better solution for this!\n\nFor demonstration I will look at first 1000 images and to see if there are any dups for them.","9c72fde1":"## 1) Image Hash based Dedup\n\nLet's see if we can use old competition data or not...\n\nNote that below we are doing exact match of hash codes, so we are not looking at soft similarity scores, for that skip to cnn based dedup."}}