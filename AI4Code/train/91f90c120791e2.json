{"cell_type":{"95555d16":"code","2cc70464":"code","bdcc02e9":"code","5158cf9b":"code","258f100f":"code","8400ee26":"code","98ec4cab":"code","273838fb":"code","3814da57":"code","ba2b4c76":"code","5ef69af7":"code","0e4f9a3a":"code","0a2e30ca":"code","8c636334":"code","d1bae994":"code","d8d75872":"code","0ea3e192":"code","0f023866":"code","42f72eba":"code","c670db51":"code","9e686b6a":"code","83837435":"code","b0114575":"code","ec1373fc":"code","49f9362a":"code","1af0be42":"code","d16e83c0":"code","664da39a":"code","b12d8948":"code","eeb9bbaa":"code","d86626f9":"code","c7fabd7c":"code","c409b2ec":"code","7a70eee7":"code","45c097c3":"code","97a832b0":"code","ad586268":"code","e99cab52":"code","371f149a":"code","2aeeff5b":"code","fa3ad12f":"code","c45c1065":"code","e4f3017b":"code","e2b58635":"code","1d4debf7":"code","55f84f34":"markdown","6c57f579":"markdown","29a432b8":"markdown","6c891a02":"markdown","3aebd99e":"markdown","c4e1b42a":"markdown","76faadb0":"markdown","19b20722":"markdown","2a4e27e7":"markdown","c4e5b9ba":"markdown","b7bba22c":"markdown","03dd846e":"markdown","38301364":"markdown","3e0e39bb":"markdown","93b51adb":"markdown","1afc7e24":"markdown"},"source":{"95555d16":"import numpy as np\nimport pandas as pd\nfrom pprint import pprint\nimport gensim\nimport gensim.downloader","2cc70464":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = stopwords.words('english')","bdcc02e9":"##Importing Libraries for Neural Nets\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential, model_from_json\nfrom keras import layers\nfrom keras.layers import Input, Dense, Dropout, Activation, LSTM, GRU, GlobalAvgPool1D, GlobalMaxPool1D\nimport math\nfrom tqdm import tqdm\nimport pickle \nimport tensorflow as tf\nfrom keras.utils.np_utils import to_categorical\nfrom tqdm.keras import TqdmCallback","5158cf9b":"!pip install keras-self-attention\nfrom keras_self_attention import SeqSelfAttention","258f100f":"# download glove twitter embeddings\npprint(list(gensim.downloader.info()['models'].keys()))","8400ee26":"# Takes about 5 minutes to execute, for 100-dim twitter vectors\n# Takes about 10+ minutes to execute, for 200-dim twitter vectors\n# glove_vectors_100 = gensim.downloader.load('glove-twitter-100')\nglove_vectors = gensim.downloader.load('glove-twitter-100')\nglove = glove_vectors\nembedding_length = 100","98ec4cab":"path = '\/kaggle\/input\/nlp-getting-started\/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nprint(train.shape)\nprint(test.shape)\n\ntext_train = np.array(train['text'])\ntext_test = np.array(test['text'])\ntext = np.concatenate((text_train, text_test), axis = 0)\nprint(text.shape)","273838fb":"# word tokenization \nfor i in range(len(text)):\n    text[i] = word_tokenize(text[i])\nprint('After tokenization:')\nprint(text[0])\n\n# filter out punctuation\nfor i in range(len(text)):\n    text[i] = [word for word in text[i] if word.isalpha()]\nprint('After filtering out punctuation:')\nprint(text[0])\n\n# make words lowercase \nfor i in range(len(text)):\n    text[i] = [word.lower() for word in text[i]]\nprint('After making lowercase:')\nprint(text[0])\n\n# remove stopwords\nfor i in range(len(text)):\n    text[i] = [word for word in text[i] if not word in stop_words]\nprint('After removing stopwords:')\nprint(text[0])\n\n# concatenate list of words\nfor i in range(len(text)):\n    text_concat = ''\n    for word in text[i]:\n        text_concat += word + ' '\n    text[i] = text_concat\nprint('After concatenating words:')\ntext = np.array(text)","3814da57":"glove = glove_vectors","ba2b4c76":"embeddings = []\nfor sentence in text:\n    embedding = np.zeros(100)\n    word_count = 0\n    for word in sentence:\n#         if word in glove.vocab:\n        if word in glove.key_to_index:\n            embedding += glove.get_vector(word)\n        word_count += 1\n    if word_count != 0:\n        embedding \/= word_count\n    embeddings.append(embedding)\nembeddings = np.array(embeddings)","5ef69af7":"embeddings.shape","0e4f9a3a":"X_train = embeddings[:7613]\nX_test = embeddings[7613:] \n\ny_train = train['target'].values","0a2e30ca":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)","8c636334":"act = 'tanh'\nbatch_len = 32\nopt = 'adam'\nepoch = 10\nval_split = 0.2","d1bae994":"keras.backend.clear_session()\ninputs = keras.Input(shape = (X_train.shape[1]))\nx = layers.Dense(32, activation = act)(inputs)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'Deep-Averaging-Network')","d8d75872":"model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)","0ea3e192":"embeddings = []\nfor sentence in text:\n    embedding = []\n    for word in sentence.split():\n        if word in glove.key_to_index:\n            embedding.extend(glove.get_vector(word))\n        else:\n            embedding.extend(np.zeros(100).tolist())\n    # pad extra zeros to make length of each embedding = 2200\n    if len(embedding) < 2200:\n        padding_len = 2200 - len(embedding)\n        embedding.extend(np.zeros(padding_len).tolist())\n    embedding = np.array(embedding)\n    embeddings.append(embedding)\nembeddings = np.array(embeddings)","0f023866":"X_train = embeddings[:7613]\nX_test = embeddings[7613:] \ny_train = train['target'].values\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)","42f72eba":"act = 'tanh'\nbatch_len = 32\nopt = 'adam'\nepoch = 10\nval_split = 0.2","c670db51":"keras.backend.clear_session()\ninputs = keras.Input(shape = (X_train.shape[1]))\nx = layers.Dense(32, activation = act, input_dim = X_train.shape[1])(inputs)\nx = layers.Dropout(0.3)(x)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Dense(16, activation = act)(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'Glove-FFN-Concatenated')","9e686b6a":"model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)","83837435":"embeddings = []\nfor sentence in text:\n    embedding = []\n    for word in sentence.split():\n        if word in glove.key_to_index:\n            embedding.append(glove.get_vector(word).tolist())\n        else:\n            embedding.append(np.zeros(embedding_length).tolist())\n    # pad extra zeros to make length of each sentence = 22 \n    while len(embedding) < 22:\n        embedding.append(np.zeros(embedding_length).tolist())\n    embeddings.append(embedding)\nembeddings = np.array(embeddings)\nprint(embeddings.shape)","b0114575":"X_train = embeddings[:7613]\nX_test = embeddings[7613:]\ny_train = train['target'].values\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)","ec1373fc":"act = 'tanh'\nbatch_len = 32\nopt = 'adam'\nepoch = 10\nval_split = 0.2","49f9362a":"keras.backend.clear_session()\ninputs = keras.Input(shape = (22, embedding_length))\nx = layers.LSTM(64)(inputs)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'Glove-LSTM')","1af0be42":"model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)","d16e83c0":"act = 'tanh'\nbatch_len = 16\nopt = 'adam'\nepoch = 10\nval_split = 0.2","664da39a":"keras.backend.clear_session()\ninputs = keras.Input(shape = (22, embedding_length))\nx = layers.GRU(64)(inputs)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'Glove-GRU')","b12d8948":"model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)","eeb9bbaa":"act = 'tanh'\nbatch_len = 16\nopt = 'adam'\nepoch = 10\nval_split = 0.2","d86626f9":"keras.backend.clear_session()\ninputs = keras.Input(shape = (22, embedding_length))\nx = layers.GRU(64, return_sequences = True)(inputs)\nx = SeqSelfAttention(attention_activation = 'tanh')(x)\nx = layers.GlobalMaxPool1D()(x)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'GRU-self-attention')","c7fabd7c":"model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)","c409b2ec":"train['keyword'].unique()","7a70eee7":"## TRAIN DATA \"KEYWORDS\" PROCESSING\n# replace \"%20\" with \" \"\nkeywords = train['keyword'].fillna('none').replace('%20', ' ', regex = True).tolist()\n\n# replace keywords with embeddings\nkeyword_embeddings = []\nfor keyword in keywords:\n    embedding = np.zeros(embedding_length)\n    word_count = 0\n    for word in keyword.split():\n        if word in glove.key_to_index: \n            embedding += glove.get_vector(word)\n            word_count += 1\n    if word_count != 0:\n        embedding \/= word_count\n    keyword_embeddings.append(embedding)\n\nkeyword_embeddings_train = np.array(keyword_embeddings)\n\n## TEST DATA \"KEYWORDS\" PROCESSING\n# replace \"%20\" with \" \"\nkeywords = test['keyword'].fillna('none').replace('%20', ' ', regex = True).tolist()\n\n# replace keywords with embeddings\nkeyword_embeddings = []\nfor keyword in keywords:\n    embedding = np.zeros(embedding_length)\n    word_count = 0\n    for word in keyword.split():\n        if word in glove.key_to_index: \n            embedding += glove.get_vector(word)\n            word_count += 1\n    if word_count != 0:\n        embedding \/= word_count\n    keyword_embeddings.append(embedding)\n\nkeyword_embeddings_test = np.array(keyword_embeddings)","45c097c3":"print(keyword_embeddings_train.shape)\nprint(keyword_embeddings_test.shape)","97a832b0":"print(train['location'].nunique())\ntrain['location'].unique()","ad586268":"## TRAIN DATA \"KEYWORDS\" PROCESSING\nlocations = train['location'].fillna('none').replace('[^a-zA-Z ]', ' ', regex = True).tolist()\n\n# replace locations with embeddings\nlocation_embeddings = []\nfor location in locations:\n    embedding = np.zeros(embedding_length)\n    word_count = 0\n    for word in location.split():\n        if word in glove.key_to_index: \n            embedding += glove.get_vector(word)\n            word_count += 1\n    if word_count != 0:\n        embedding \/= word_count\n    location_embeddings.append(embedding)\n\nlocation_embeddings_train = np.array(location_embeddings)\n\n## TEST DATA \"KEYWORDS\" PROCESSING\nlocations = test['location'].fillna('none').replace('[^a-zA-Z ]', ' ', regex = True).tolist()\n\n# replace locations with embeddings\nlocation_embeddings = []\nfor location in locations:\n    embedding = np.zeros(embedding_length)\n    word_count = 0\n    for word in location.split():\n        if word in glove.key_to_index: \n            embedding += glove.get_vector(word)\n            word_count += 1\n    if word_count != 0:\n        embedding \/= word_count\n    location_embeddings.append(embedding)\n\nlocation_embeddings_test = np.array(location_embeddings)","e99cab52":"print(location_embeddings_train.shape)\nprint(location_embeddings_test.shape)","371f149a":"# concatenate \"keyword\" and location features\nnon_sequential_train = np.concatenate((keyword_embeddings_train, location_embeddings_train), axis = 1)\nnon_sequential_test = np.concatenate((keyword_embeddings_test, location_embeddings_test), axis = 1)\nprint(non_sequential_train.shape)\nprint(non_sequential_test.shape)","2aeeff5b":"act = 'tanh'\nbatch_len = 16\nopt = 'adam'\nepoch = 20\nval_split = 0.2","fa3ad12f":"keras.backend.clear_session()\nseq_input = keras.Input(shape = (22, embedding_length))\nnon_seq_input = keras.Input(shape = (2 * embedding_length))\nx = layers.GRU(64, return_sequences = True)(seq_input)\nx = SeqSelfAttention(attention_activation = 'tanh')(x)\nx = layers.GlobalMaxPool1D()(x)\nx = layers.concatenate([x, non_seq_input])\nx = layers.Dense(64, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutput = layers.Dense(1)(x)\nmodel = keras.Model(inputs = [seq_input, non_seq_input], outputs = output, name = 'complete_model')","c45c1065":"model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])","e4f3017b":"model.fit([X_train, non_sequential_train], y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)","e2b58635":"tf.keras.utils.plot_model(model, show_shapes = True)","1d4debf7":"# y_test = model.predict(X_test)\ny_test = model.predict([X_test, non_sequential_test])\ny_pred = []\nfor i in range(len(y_test)):\n    if y_test[i][0] > 0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\ntest['target'] = y_pred\nfinal = test[['id', 'target']]\nfinal.to_csv('pred.csv', index = False)\nfinal","55f84f34":"# **Create and train LSTM model**\n#### (Using only the text feature)","6c57f579":"# **Download embeddings**","29a432b8":"# **Get embeddings for each word (averaging)**\n\n\n\n","6c891a02":"# **Non-linear LSTM model (with both seq and non-seq inputs)**","3aebd99e":"Understanding input sizes for using LSTM: https:\/\/stackoverflow.com\/questions\/50418973\/how-lstm-work-with-word-embeddings-for-text-classification-example-in-keras","c4e1b42a":"# **LSTM model with self-attention**","76faadb0":"# **Import files**","19b20722":"# **Preparing \"keyword\" feature**","2a4e27e7":"# **Create and train GRU model**","c4e5b9ba":"# **Pre-process the text**","b7bba22c":"# **Get embeddings for each word (concatenate)**","03dd846e":"# **Create Model (Concatenated Embeddings Model)**","38301364":"# **Create Embeddings (for LSTM)**","3e0e39bb":"# **Get results for test set and generate CSV**","93b51adb":"# **Create Model**\n#### (Average the embeddings for each word of the tweet and learn to classify using a feedforward NN model)","1afc7e24":"# **Preparing \"location\" feature**"}}