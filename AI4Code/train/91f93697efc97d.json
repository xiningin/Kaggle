{"cell_type":{"845cbf7a":"code","4eba05f7":"code","e5e72d7f":"code","0cbe684e":"code","557d495a":"code","86a1464b":"code","420e9875":"code","418b9ace":"code","862ec224":"code","1c0a2a2d":"code","0ad98c0e":"code","2183d5bb":"code","7684d9db":"code","86e27946":"code","09707869":"code","6df86046":"code","3e59ac6e":"code","a851bf1c":"code","9d2cd1f2":"code","ba6dcc6f":"code","c076289e":"code","3e12b195":"code","8cc1e8b6":"markdown","fbbe6b17":"markdown","0cd9f9a7":"markdown","d6142655":"markdown","31a87454":"markdown","f457da06":"markdown","a1613711":"markdown","56f1a4dc":"markdown","e54cbd6c":"markdown","49f25dd5":"markdown","13288d68":"markdown","136e8286":"markdown","69ec475d":"markdown","4e0de057":"markdown","bff9afbe":"markdown","4ab8c2e6":"markdown","48171940":"markdown","b564b6a5":"markdown","b74d40fb":"markdown","b39c069b":"markdown"},"source":{"845cbf7a":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport re\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom time import time\n\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, optimizers, losses, metrics, utils, applications, callbacks\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\n# For removing stopwords\nfrom nltk.corpus import stopwords\nstopwords_eng = set(stopwords.words('english'))\ndef remove_stopwords(txt):\n    for stopword in stopwords_eng:\n        while stopword in txt:\n            txt = txt.replace(stopword,'')\n    return txt\n\n# For convenient train:val:test splitting\nfrom sklearn.model_selection import train_test_split as tts\ndef tvt_split(X, y, split_sizes=[8,1,1], stratify=True):\n    split_sizes = np.array(split_sizes)\n    if stratify:\n        train_X, test_X, train_y, test_y = tts(X, y, test_size=split_sizes[2]\/split_sizes.sum(), stratify=y)\n        train_X, val_X, train_y, val_y = tts(train_X, train_y, test_size=split_sizes[1]\/(split_sizes[0]+split_sizes[1]), stratify=train_y)\n    else:\n        train_X, test_X, train_y, test_y = tts(X, y, test_size=split_sizes[2]\/split_sizes.sum())\n        train_X, val_X, train_y, val_y = tts(train_X, train_y, test_size=split_sizes[1]\/(split_sizes[0]+split_sizes[1]))\n    return train_X, val_X, test_X, train_y, val_y, test_y\n\n\ninput_dir = '..\/input\/enron-email-dataset'\ndata = pd.read_csv(input_dir+'\/emails.csv')\ndata.head()","4eba05f7":"print(data.message[0])","e5e72d7f":"sender_pattern = re.compile('(?<=From:\\s).*') # Because we do not pass the re.S flag, the dot does not match the newline char (\\n) and the match is cut at the end of the line\nreceiver_pattern = re.compile('(?<=To:\\s).*')\n\nprint(sender_pattern.search(data.message[0]).group())\nprint(sender_pattern.search(data.message[123123]).group())\nprint(sender_pattern.search(data.message[9999]).group())","0cbe684e":"xfn_pattern = re.compile('X-FileName:.*')\ncontent_pattern = re.compile('[^\\n].*\\Z', re.S)\n\nprint(data.message[0])\nxfn_end = xfn_pattern.search(data.message[0]).end()\nmatch = content_pattern.search(data.message[0], pos=xfn_end)\n\nprint(\"Message:\")\nprint(match.group())","557d495a":"data['Sender'] = data.message.apply(lambda x: sender_pattern.search(x).group())\ndata['Receiver'] = data.message.apply(lambda x: receiver_pattern.search(x).group())\ndata['Content'] = data.message.apply(\n    lambda x: content_pattern.search(\n        x,\n        pos = xfn_pattern.search(x).end()\n    ).group())\ndata['Length'] = data.Content.apply(lambda x: len(x))\n\nprint(data.isnull().any())","86a1464b":"senders_list = data.Sender.value_counts().index[:10].tolist() # 1)\nn_per_class = 1280\n\ntexts = [] # 2)\nlabels = [] # 3)\nfor i, sender in tqdm(enumerate(senders_list)):\n    sender_texts = data.query(' `Sender` == @sender ').sample(frac=1)['Content'][:n_per_class].apply(lambda x: remove_stopwords(x)).values.tolist() # 2)\n    texts += sender_texts # 2)\n    labels += (np.ones(shape=(n_per_class,))*i).tolist() # 3)\n    \n","420e9875":"max_features = 512  # Maximum number of words, we are going to embed\nembed_dim = 128      # Number of embedding dimensions in the word embedding space constructed by the embedding layer\nmaxlen = 200         # The length of the sequence (message) - The longer messages will be cropped to 256, while the shorter ones will be padded with zeros to be exactly 256 tokens long\n\n\ntokenizer = Tokenizer(num_words=max_features) # 4)\ntokenizer.fit_on_texts(texts)\nword_index = tokenizer.word_index\n\nseqs = tokenizer.texts_to_sequences(texts) # 5)\nseqs = pad_sequences(seqs, maxlen=maxlen) # 6)\n\nlabels = np.array(labels)\n\ntrain_X, val_X, test_X, train_y, val_y, test_y = tvt_split(seqs, labels, stratify=True) # 7)","418b9ace":"# Sanity check for proper shapes and the equal distribution of classes\nfor X, y in [train_X, train_y], [val_X, val_y], [test_X, test_y]:\n    print(X.shape, y.shape)\n    print(np.bincount(y.astype(np.int32)))","862ec224":"model = models.Sequential(layers=[\n    layers.Embedding(input_dim=max_features, output_dim=embed_dim, input_length=maxlen),\n    layers.Bidirectional(layers.GRU(32, activation='relu', return_sequences=True, dropout=.1, recurrent_dropout=.1)),\n    layers.Bidirectional(layers.GRU(32, activation='relu', return_sequences=False, dropout=.1, recurrent_dropout=.1)),\n    layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n    layers.BatchNormalization(),\n    layers.Dropout(.2),\n    layers.Dense(32, activation='relu', kernel_regularizer='l2'),\n    layers.BatchNormalization(),\n    layers.Dropout(.1),\n    layers.Dense(10, activation='softmax')\n])\nmodel.summary()","1c0a2a2d":"def lr_scheduler(epoch, lr):\n    if epoch==8 or epoch==12:\n        return lr\/8\n    else:\n        return lr\n        \n\ncallbacks_list = [\n    callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, save_freq='epoch'),\n    callbacks.LearningRateScheduler(lr_scheduler)\n]\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='sparse_categorical_crossentropy',\n    metrics=['acc']\n)\n\nEPOCHS = 32\n\nhistory = model.fit(\n    train_X, train_y,\n    validation_data = (val_X, val_y),\n    epochs = EPOCHS, batch_size=64,\n    shuffle = True,\n    verbose = 1,\n    callbacks = callbacks_list\n)\n\npd.DataFrame(history.history).to_csv('history.csv')\nmodel.save('last_model.h5')","0ad98c0e":"train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\ntrain_acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nlr = history.history['lr']\n\nx = np.arange(1, EPOCHS+1)\n\nplt.plot(x, train_loss, 'r--', label='Training loss')\nplt.plot(x, val_loss, 'g-', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n\nplt.plot(x, train_acc, 'r--', label='Training accuracy')\nplt.plot(x, val_acc, 'g-', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(x, lr, 'b-', label='Learning Rate')\nplt.yscale('log')\nplt.title('Learning rate')\nplt.legend()\nplt.show()","2183d5bb":"model.load_weights('last_model.h5')\nmodel.compile(\n    optimizer='rmsprop',\n    loss='sparse_categorical_crossentropy',\n    metrics=['acc']\n)\nprint(\"\\t\\tLAST MODEL:\")\nprint('Training set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(train_X, train_y, verbose=0)))\nprint('Validation set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(val_X, val_y, verbose=0)))\nprint('Testing set evaluation:\\t\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(test_X, test_y, verbose=0)))\n\nbest_model = models.load_model('best_model.h5')\nbest_model.compile(\n    optimizer='rmsprop',\n    loss='sparse_categorical_crossentropy',\n    metrics=['acc']\n)\nprint(\"\\n\\t\\tBEST MODEL:\")\nprint('Training set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(best_model.evaluate(train_X, train_y, verbose=0)))\nprint('Validation set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(best_model.evaluate(val_X, val_y, verbose=0)))\nprint('Testing set evaluation:\\t\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(best_model.evaluate(test_X, test_y, verbose=0)))","7684d9db":"cm = np.zeros(shape=(10,10))\n\nfor X, y in tqdm(zip(test_X, test_y)):\n    pred = model.predict(X.reshape(1,-1)).argmax().astype(np.int32)\n    y = int(y)\n    cm[pred,y] += 1","86e27946":"cm = pd.DataFrame(cm, columns=senders_list, index=senders_list)\ncm","09707869":"cl = []\nfor row in cm.index:\n    for col in cm.columns:\n        cl.append([cm.loc[col, row], row, col])\n        \ncl = pd.DataFrame(cl, columns=['Confusions', 'Predicted', 'Actual']).sort_values(by='Confusions', ascending=False)\ncl = cl.query('`Predicted`!=`Actual`').reset_index(drop=True)\ncl.head()","6df86046":"cl['Reverse'] = cl.apply(lambda x: cl.query('`Predicted`==@x.Actual & `Actual`==@x.Predicted').values[0][0] , axis=1)\ncl.iloc[:10]","3e59ac6e":"# Sanity check\ncl.query('Confusions==2')","a851bf1c":"receivers_list = data.Receiver.value_counts().index[:10].tolist() # 1)\nn_per_class = 1280\n\ntexts = [] # 2)\nlabels = [] # 3)\nfor i, receiver in tqdm(enumerate(receivers_list)):\n    receiver_texts = data.query(' `Receiver` == @receiver ').sample(frac=1)['Content'][:n_per_class].apply(lambda x: remove_stopwords(x)).values.tolist() # 2)\n    texts += receiver_texts # 2)\n    labels += (np.ones(shape=(n_per_class,))*i).tolist() # 3)\n    \nmax_features = 512  # Maximum number of words, we are going to embed\nembed_dim = 128      # Number of embedding dimensions in the word embedding space constructed by the embedding layer\nmaxlen = 200         # The length of the sequence (message) - The longer messages will be cropped to 256, while the shorter ones will be padded with zeros to be exactly 256 tokens long\n\n\ntokenizer = Tokenizer(num_words=max_features) # 4)\ntokenizer.fit_on_texts(texts)\nword_index = tokenizer.word_index\n\nseqs = tokenizer.texts_to_sequences(texts) # 5)\nseqs = pad_sequences(seqs, maxlen=maxlen) # 6)\n\nlabels = np.array(labels)\n\ntrain_X, val_X, test_X, train_y, val_y, test_y = tvt_split(seqs, labels, stratify=True) # 7)","9d2cd1f2":"for X, y in [train_X, train_y], [val_X, val_y], [test_X, test_y]:\n    print(X.shape, y.shape)\n    print(np.bincount(y.astype(np.int32)))","ba6dcc6f":"model = models.Sequential(layers=[\n    layers.Embedding(input_dim=max_features, output_dim=embed_dim, input_length=maxlen),\n    layers.Bidirectional(layers.GRU(32, activation='relu', return_sequences=True, dropout=.1, recurrent_dropout=.1)),\n    layers.Bidirectional(layers.GRU(32, activation='relu', return_sequences=False, dropout=.1, recurrent_dropout=.1)),\n    layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n    layers.BatchNormalization(),\n    layers.Dropout(.2),\n    layers.Dense(32, activation='relu', kernel_regularizer='l2'),\n    layers.BatchNormalization(),\n    layers.Dropout(.1),\n    layers.Dense(10, activation='softmax')\n])\n\ndef lr_scheduler(epoch, lr):\n    if epoch==8 or epoch==12:\n        return lr\/8\n    else:\n        return lr\n        \n\ncallbacks_list = [\n    callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, save_freq='epoch'),\n    callbacks.LearningRateScheduler(lr_scheduler)\n]\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='sparse_categorical_crossentropy',\n    metrics=['acc']\n)\n\nEPOCHS = 32\n\nhistory = model.fit(\n    train_X, train_y,\n    validation_data = (val_X, val_y),\n    epochs = EPOCHS, batch_size=64,\n    shuffle = True,\n    verbose = 1,\n    callbacks = callbacks_list\n)\n\npd.DataFrame(history.history).to_csv('history.csv')\nmodel.save('last_model.h5')","c076289e":"train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\ntrain_acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nlr = history.history['lr']\n\nx = np.arange(1, EPOCHS+1)\n\nplt.plot(x, train_loss, 'r--', label='Training loss')\nplt.plot(x, val_loss, 'g-', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n\nplt.plot(x, train_acc, 'r--', label='Training accuracy')\nplt.plot(x, val_acc, 'g-', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(x, lr, 'b-', label='Learning Rate')\nplt.yscale('log')\nplt.title('Learning rate')\nplt.legend()\nplt.show()","3e12b195":"model.load_weights('last_model.h5')\nmodel.compile(\n    optimizer='rmsprop',\n    loss='sparse_categorical_crossentropy',\n    metrics=['acc']\n)\nprint(\"\\t\\tLAST MODEL:\")\nprint('Training set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(train_X, train_y, verbose=0)))\nprint('Validation set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(val_X, val_y, verbose=0)))\nprint('Testing set evaluation:\\t\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(test_X, test_y, verbose=0)))\n\nbest_model = models.load_model('best_model.h5')\nbest_model.compile(\n    optimizer='rmsprop',\n    loss='sparse_categorical_crossentropy',\n    metrics=['acc']\n)\nprint(\"\\n\\t\\tBEST MODEL:\")\nprint('Training set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(best_model.evaluate(train_X, train_y, verbose=0)))\nprint('Validation set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(best_model.evaluate(val_X, val_y, verbose=0)))\nprint('Testing set evaluation:\\t\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(best_model.evaluate(test_X, test_y, verbose=0)))","8cc1e8b6":"Two callbacks:\n\n* ModelCheckpoint - if this is the best model (having the smallest validation loss so far) it saves it as 'best_model.h5' file. Validation loss is checked after each epoch (save_freq='epoch').\n* LearningRateScheduler - divides the learning rate by a factor of 8 before the 8-th and 12-th epoch (using lr_scheduler function below).\n","fbbe6b17":"A sequential model, similar to the one [I used for binary classification](http:\/\/https:\/\/www.kaggle.com\/mateuszbagiski\/enron-feature-extraction-eda-sender-prediction)","0cd9f9a7":"1) Find the 10 people who sent the most e-mails\n\n2) Extract the contents of 1280 messages from each one of them\n\n3) Make a list of labels corresponding to each sender","d6142655":"# Results inspection","31a87454":"Sure we can, although the same results in much lower accuracy.","f457da06":"Apparently the content of the message (at least processed in the way presented here) is a much better predictor of the sender of the message (test set accuracy ~= 84.5%) than its receiver (test set accuracy ~=80.5%).","a1613711":"4) Create the tokenizer and fit it on the texts of all the messages\n\n5) Convert these messages into sequences of integers corresponding to each of 512 most prevalent words\n\n6) Pad the sequences so that they are exactly 200 tokens long\n\n7) Split the sequences and labels into training, validation, and testing sets in 8:1:1 proportions, respectively","56f1a4dc":"Plotting the training trajectory","e54cbd6c":"Sort the confusions: which senders are most often confused?","49f25dd5":"Rows: predicted sender\n\nColumns: actual sender","13288d68":"Model's confusion matrix","136e8286":"# Feature Extraction","69ec475d":"# Build and train the model","4e0de057":"Extract the relevant data","bff9afbe":"It seems Steven Kean and Jeff Dasovich are often confused with each other (10 times in both directions). The same is true for Sara Shackleton and Tana Jones (7 and 8). On the other hand, 10 times the model predicted Jeff Dasovich when the real sender was Chris Germany, while the reverse error was made only 2 times.","4ab8c2e6":"Comparing the best model and the final model of the training","48171940":"# Can we do the same for receivers of the e-mails?","b564b6a5":"# Import libs and load the data","b74d40fb":"Are these relations even slightly symmetric?","b39c069b":"# Prepare the data for the model"}}