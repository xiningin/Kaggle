{"cell_type":{"1c193f4c":"code","318f0b25":"code","a5cc5f51":"code","6495c2fe":"code","6e33a25f":"code","0ea55e8f":"code","0723d63c":"code","db753c74":"code","7e43f2b4":"code","a37f3346":"code","1f8dde5a":"code","238b7c88":"code","12c7ad94":"code","09004673":"code","b9e445c5":"code","9493ef44":"code","4bd57988":"code","c240f1c8":"code","ced71fa6":"code","37bdf896":"code","ceb4e70b":"code","f865551a":"code","10b3ce46":"code","69021da4":"code","026fc013":"code","8d959904":"code","164f25f0":"code","a5accd25":"code","84df1c3a":"code","9cfb9441":"code","bef8dadc":"code","16ee9f04":"code","cf736877":"code","86307152":"code","1dbdbfc5":"code","35be30de":"code","e403149d":"code","5a03991a":"code","1eaab9af":"code","365aa751":"code","488d109e":"markdown","182d3a71":"markdown","38d32d66":"markdown","8d9d1ca4":"markdown","876e660b":"markdown","ef202f81":"markdown","c8bd5408":"markdown","2bd242a7":"markdown","986770dd":"markdown","61b38f6a":"markdown","5a0af3aa":"markdown","9249b798":"markdown","5f3ade75":"markdown","e8881e35":"markdown","f6649424":"markdown","ec46423d":"markdown","44bbe58f":"markdown","92daa98a":"markdown","f4e37f02":"markdown","06e34160":"markdown","8a856403":"markdown","38cb2742":"markdown","48964ec3":"markdown"},"source":{"1c193f4c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# visualiation tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# sci-kit learn tools\nfrom scipy.stats import skew\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","318f0b25":"# given data imports\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# copies of DS for manipulation (we will use this for remainder of project)\ntrain_df = train.copy()\ntest_df = test.copy()\n\n# copies for EDA purposes\nEDA_train = train.copy()\nEDA_test = test.copy()\n\n# combine data sets to avoid dimension misalignment\nall_data = pd.concat((train_df.loc[:,'MSSubClass':'SaleCondition'],\n                      test_df.loc[:,'MSSubClass':'SaleCondition']))\n\nprint(train_df.shape, test_df.shape, all_data.shape)","a5cc5f51":"# drop target (dependent variable) from training dataframe\nactual_y = train_df['SalePrice']\n#train_df = train_df.drop('SalePrice', axis=1)\n\ntrain_df.shape","6495c2fe":"# from Abhinand \"Predicting HousingPrices: Simple Approach\" Kernel\ndef show_all(df):\n    #This fuction lets us view the full dataframe\n    with pd.option_context('display.max_rows', 100, 'display.max_columns', 100):\n        display(df)","6e33a25f":"show_all(train_df.head())","0ea55e8f":"f, ax = plt.subplots(figsize=(12, 6))\nsns.distplot(actual_y)\nprint(\"Skew is: \", actual_y.skew())","0723d63c":"log_actual_y = np.log(actual_y)\nf, ax = plt.subplots(figsize=(12, 6))\nsns.distplot(log_actual_y)\n\nprint(\"Skew is: \", log_actual_y.skew())","db753c74":"quant = train.select_dtypes(include=[np.number])\nquant.dtypes","7e43f2b4":"corr = quant.corr()\nprint(corr['SalePrice'].sort_values(ascending=False)[:5])\nprint(corr['SalePrice'].sort_values(ascending=False)[-5:])","a37f3346":"corr_map = train_df.corr()\nfig, ax = plt.subplots(figsize=(20,16))\nsns.heatmap(corr_map, vmax=.8, square=True, annot=True, fmt='.1f')\nplt.show();","1f8dde5a":"# Top 10 high correlation to SalePrice matrix\nn = 10\ncols = corr_map.nlargest(n, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nfig, ax = plt.subplots(figsize=(10,8))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","238b7c88":"train_df.OverallQual.unique()","12c7ad94":"# pivot table to further investigate relationship between 'OverallQual' and 'SalePrice'\nquality_pivot = train.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)\nquality_pivot","09004673":"f, ax = plt.subplots(figsize=(8, 4))\nsns.lineplot(x='OverallQual', y = train_df.SalePrice, color='green',data=train_df)","b9e445c5":"plt.figure(figsize=(8, 6), dpi=80)\nplt.scatter(x = train_df['GrLivArea'], y = log_actual_y)\nplt.ylabel('LogSalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","9493ef44":"# remove outliers and update EDA_train\nEDA_train = EDA_train[EDA_train['GrLivArea'] < 4000]\n\nplt.figure(figsize=(8, 6), dpi=80)\nplt.scatter(x = EDA_train['GrLivArea'], y = np.log(EDA_train.SalePrice))\nplt.xlim(-200,6000) # keeps same scale as first scatter plot\nplt.ylabel('LogSalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","4bd57988":"# lets do the same for all data\n#all_data = all_data[all_data['GrLivArea'] < 4000]","c240f1c8":"plt.figure(figsize=(8, 6), dpi=80)\nplt.scatter(x = train_df['GarageArea'], y = np.log(train_df.SalePrice))\nplt.ylabel('LogSalePrice')\nplt.xlabel('GarageArea')\nplt.show()","ced71fa6":"# remove outliers and update train_df\nEDA_train = EDA_train[EDA_train['GarageArea'] < 1200]\n\nplt.figure(figsize=(8, 6), dpi=80)\nplt.scatter(x = EDA_train['GarageArea'], y = np.log(EDA_train.SalePrice))\nplt.xlim(-50,1475)\nplt.ylabel('LogSalePrice')\nplt.xlabel('GarageArea')\nplt.show()","37bdf896":"# instead of removing outlier rows, lets try to impute them with a value\n# dropping rows with outliers is misaligning my data and preventing submission\n#all_data = all_data[all_data['GarageArea'] < 1200]","ceb4e70b":"# Number of missing values in each column of training data\nmissing_vals = (train_df.isnull().sum())\nprint(missing_vals[missing_vals > 0])","f865551a":"# GrLivArea\nf, ax = plt.subplots(figsize=(8, 4))\nsns.distplot(train_df['GrLivArea'])\nprint(\"Skew is: \", train_df['GrLivArea'].skew())","10b3ce46":"train_df['GrLivArea'] = np.log(train_df['GrLivArea'])\n\nf, ax = plt.subplots(figsize=(8, 4))\nsns.distplot(train_df['GrLivArea'])\nprint(\"Skew is: \", train_df['GrLivArea'].skew())","69021da4":"# TotalBsmtSF\nf, ax = plt.subplots(figsize=(8, 4))\nsns.distplot(train_df['TotalBsmtSF'])\nprint(\"Skew is: \", train_df['TotalBsmtSF'].skew())","026fc013":"all_data.shape","8d959904":"# we will begin by applying log transformation to skewed numeric features\nnum_data = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskew_data = all_data[num_data].apply(lambda x: skew(x.dropna()))\nskew_data = skew_data[skew_data > 0.75]\nskew_data = skew_data.index\n\nall_data[skew_data] = np.log1p(all_data[skew_data])","164f25f0":"all_data.shape","a5accd25":"# drop all features with missing values, noted above : keep electrical\nall_data = all_data.drop((missing_vals[missing_vals > 1]).index,1)\n#all_data = all_data.drop(all_data.loc[all_data['Electrical'].isnull()].index)\n\n# fix few number of missing vals in test set\nall_data = all_data.fillna(all_data.mean())","84df1c3a":"all_data.shape","9cfb9441":"all_data = pd.get_dummies(all_data)","bef8dadc":"# drop variables noted in EDA section\ndrop_me = ['GarageArea', '1stFlrSF', 'TotRmsAbvGrd']\nall_data = all_data.drop(drop_me, axis=1)","16ee9f04":"# quick look under the hood\nshow_all(all_data.head())\nprint(all_data.shape)","cf736877":"# split concatonated data into train and test dataframes\n\ny = np.log1p(train_df[\"SalePrice\"])\ntrain_df = train_df.drop('SalePrice', axis=1)\nX_train = all_data[:train_df.shape[0]]\nX_test = all_data[train_df.shape[0]:]\n\n\nX_train.shape","86307152":"# Root-Mean-Squared-Error (RMSE) evaluation metric\nfrom sklearn.model_selection import cross_val_score\n\n# from \"Regularized Linear Models\" w\/ cross validation\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","1dbdbfc5":"# Linear Regression !\nfrom sklearn import linear_model\n\nlinear_model = linear_model.LinearRegression()\nlr_model = linear_model.fit(X_train, y)\n\nrmse_cv(lr_model).mean()","35be30de":"# LassoCV !\nfrom sklearn.linear_model import LassoCV\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nlasso_model = LassoCV(alphas = [1, 0.1, 0.001, 0.0005, 0.005, 0.0001, 0.5, 0.2]).fit(X_train, y)\nrmse_cv(lasso_model).mean()","e403149d":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(random_state=42, max_depth = 6, n_jobs = 5)\nrf_model.fit(X_train, y)\n\nrmse_cv(rf_model).mean()","5a03991a":"linear_pred = np.expm1(lr_model.predict(X_test))\nlasso_pred = np.expm1(lasso_model.predict(X_test))","1eaab9af":"lasso_pred.shape","365aa751":"#submit!\noutput = pd.DataFrame({\"id\":test.Id, \"SalePrice\":lasso_pred})\noutput.to_csv(\"lasso_solution.csv\", index = False)","488d109e":"We will drop all variables with a high amount of missing values. Why? None of these variables seem to be important or considered when deciding to buy a house (and that's probably why they have so many missing values). \n> Drop: 'MiscFeature', 'Fence', 'PoolQC', 'FireplaceQu', 'Alley'\n\nIn regards to the Garage-related variables with missing values, we already have a garage variable with a high correlation to SalePrice. That variable alone will do the trick so we will delete all Garage variables with missing data.\n> Drop: 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond'\n\nThe same logic applies to Bsmt variables. Also MasVnr variables correlate heavily with 'OverallQual' so we will delete those.\n\nWe will delete everything except for Electrical.\n\n> General intution here was gathered from [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n","182d3a71":"* 'GarageCars' and 'GarageArea' are like twins. So we just need one. We will choose 'GarageCars' because it has a stronger correlation to 'SalePrice'!\n* 'TotalBsmtSF' and '1stFlrSF' are also twins. We will choose 'TotalBsmtSF' because of higher correlation to SP\n* 'TotRmsAbvGrd' and 'GrLivArea' are also twins. We will choose 'GrLivArea'\n\n> * (Drop 'GarageArea')\n> * (Drop '1stFlrSF')\n> * (Drop 'TotRmsAbvGrd')","38d32d66":"We will handle quantitative and qualitative features seperately. We will begin with Quantitative features. We will examine correlation between actual SalePrice and quantitative features.","8d9d1ca4":"> Examine correlations between SalePrice and target","876e660b":"Now we will take a look at garage area.","ef202f81":"Submit!","c8bd5408":"> We can see that 'GrLivArea' and 'TotRmsAbvGrd', 'TotalBsmtSF' and '1stFlrSF', 'YearBuilt' and 'GarageYrBlt' have high correlations. **These correlations are so strong that it can indicate a situation of multicollinearity**.\n\nLets take a moment to visualize these highly correlated numeric features (and later trim outliers).\n","2bd242a7":"> We can see that as the overall quality of the house increases so does the price of the house. This is an excellent variable for our model.\n\nNow we will take a look at 'GrLivArea'","986770dd":"For the sake of working quick, we will encode all qualitative variables with dummy representations. At a later point we will re-visit qualitative variables with a more granular approach.","61b38f6a":"> Outliers can affect a regression model by pulling our estimated regression line further away from the true population regression line. So, we\u2019ll remove those observations from our data.","5a0af3aa":"> We can see that there exists many qualitative and missing values^\n> \n> Let's take a look at the skewness of SalePrice to see if a log transformation will be necessary for linear regression.","9249b798":"*We will now begin an analysis on the normality of some of our very important features. Let's note our most important variables thus far:*\n> * OverallQual\n> * GarageCars (recall all other garage variables have been dropped)\n> * TotalBsmtSF (recall all other Bsmt variables have been dropped)\n> * GrLivArea","5f3ade75":"**Build the Model**\n>  We will attempt to apply the following models:\n> * Linear Regression\n> * Lasso Regression\n> * Random Forests","e8881e35":"This Kernel is meant to solve the Housing Prices Challenge in a simple way and produce an acceptable leaderboard score. We will apply Linear Regression and machine learning models known for handling structured data.\n\n> Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.\n\nResources used to guide my personal learning and application throughout this House Price project:\n* [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n* [Getting Started with Kaggle: House Prices Competition](https:\/\/www.dataquest.io\/blog\/kaggle-getting-started\/)\n* [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models)","f6649424":"We can do more investigation on other variable outliers at a later date. Now we will take a look at missing values and begin imputation process. (Do not forget it will soon be time to combine two data sets to avoid dimension misalignment)","ec46423d":"> A skew value closer to 0 means that we have improved the skewness of the data. You can see from the plot that the logged data resembles a normal distribution!","44bbe58f":"**Exploratory Data Analysis**","92daa98a":"**Transforming and Engineering Features**\n> Start with applying all drops and transformations noted in EDA section. \n> For this section I reference: [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models)","f4e37f02":"**Notebook Content:**\n1. Imports\n2. Exploratory Data Analysis\n3. Transforming and Engineering Feautures","06e34160":"Holy skew! Let's transform.","8a856403":"> You can see that the data is skewed. We will attempt to log-transform the data to bring the skew number closer to 0.","38cb2742":"We can now see the top 5 most postiviely correlated features with SalePrice. \n> If your dataset has perfectly positive or negative attributes then there is a high chance that the performance of the model will be impacted by a problem called\u200a\u2014\u200a\u201cMulticollinearity\u201d\n\nLets take a look at a correlation heatmap.","48964ec3":"> There does exists methods to log-transform complicated variables that contain: values equal to zero. We will visit this at a later date."}}