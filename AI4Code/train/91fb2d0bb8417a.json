{"cell_type":{"6a607531":"code","58426bbe":"code","4804e8a6":"code","63d67f8d":"code","dd9c2044":"code","adc4ffc6":"code","def58ee1":"code","3536e07c":"code","0f0cce24":"code","070e95a7":"code","4b098f45":"code","9243aecf":"code","c846570a":"code","61283d1f":"code","4d977c4c":"code","425d10af":"code","910f7d8f":"code","e0fd85fd":"code","9ce0bf43":"code","33239dac":"code","f355859d":"markdown","f62aa7a9":"markdown","e6e32414":"markdown","c5ac6c9c":"markdown","168f45e2":"markdown","a67a0694":"markdown","e7080b63":"markdown","dbe14f1c":"markdown","c8b86d39":"markdown","7c3c128a":"markdown","0903c9ae":"markdown","f1ef7242":"markdown","b42e00d3":"markdown","ac356e71":"markdown","aa4187e8":"markdown","fa99ea83":"markdown","7bd91753":"markdown","1f1313fd":"markdown","ff5ce823":"markdown","6f101dd3":"markdown","db10cce7":"markdown","31322ea6":"markdown","363b4713":"markdown","646c9b76":"markdown","6d8668ba":"markdown","ce8d109e":"markdown","a233fcab":"markdown","fc726e49":"markdown","3cfc22a2":"markdown","3db075a0":"markdown","ca49340e":"markdown","c289e098":"markdown","74c0fe43":"markdown","8c79c1d6":"markdown","47916e02":"markdown","472c417e":"markdown","5637fb28":"markdown","8af81e11":"markdown","da0675d7":"markdown","dd22d21a":"markdown"},"source":{"6a607531":"import pandas as pd\nimport dask.dataframe as dd\nimport numpy as np\nimport math\nimport gc\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# import math\n# from scipy import stats # For outlier handling\n\n'''''''''\ntorch\n'''''''''\nimport torch\nfrom torch.utils.data import Dataset, DataLoader,TensorDataset\nimport torch.optim \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nprint(torch.__version__)\n\n'''''''''\noptuna\n'''''''''\nimport optuna\nfrom optuna.trial import TrialState\n\n'''''''''\nSklearn \n'''''''''\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\nfrom sklearn.metrics import accuracy_score,roc_auc_score\n\n'''''''''\nLight Gradient Boosting Machine\n'''''''''\n!pip install lightgbm\nimport lightgbm as lgb\n\n'''''''''\nXGb\n'''''''''\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n'''''''''''''''\nFeature Engine\n'''''''''''''''\n!pip install feature_engine\nfrom feature_engine.selection import SmartCorrelatedSelection","58426bbe":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\nprint(f'train {train.shape}')","4804e8a6":"train.head()","63d67f8d":"plt.figure(figsize=(8,4))\nmean = (train.claim.values == 1).mean()\nax = sns.barplot(['Claim', 'Not Claim'], [mean, 1-mean])\nax.set(ylabel='Proportion', title='Calim vs Not Claim')\nfor p, uniq in zip(ax.patches, [mean, 1-mean]):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height+0.01,\n            '{}%'.format(round(uniq * 100, 2)),\n            ha=\"center\")\nplt.show()    \nSummarydf = train.groupby('claim')['id'].count().reset_index()\nSummarydf.columns = ['claim','count']\nSummarydf['total'] = len(train)\nSummarydf['perc']  = round((Summarydf['count']\/Summarydf['total']),2)*100\nSummarydf","dd9c2044":"def summary(df):\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Missing'] = df.isna().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary\n\nsummary(train)","adc4ffc6":"def plot_feature_distribution(df, features):\n    fig,axes = plt.subplots(40,3,figsize=(30,150))\n    plt.style.use('ggplot')\n    \n    for col,ax in zip(features,axes.flat):\n\n        sns.kdeplot(df.loc[(df['claim']==1),col], color='purple', shade=False,ax=ax, label='Claim',alpha=0.5)\n        sns.kdeplot(df.loc[(df['claim']==0),col], color='green', shade=False,ax=ax, label='Not Claim',alpha=0.5)\n        ax.legend(loc='upper right')\n        plt.xlabel(col, fontsize=18)\n        plt.tight_layout()\n\nfeatures = [feature for feature in train.columns if feature not in ['id','claim']]\nplot_feature_distribution(train, features)","def58ee1":"# Row wise Missing count for each columns - feature generation\ntrain['nan_count'] = train.isnull().sum(axis=1)\n# impute simple mean imputation\ntrain.fillna(train.mean(),inplace=True)\n\n# set up the selector\ntr = SmartCorrelatedSelection(\n    variables=None,\n    method=\"pearson\",\n    threshold=0.8,\n    missing_values=\"raise\",\n    selection_method=\"variance\",\n    estimator=None,\n)\n\nXt = tr.fit_transform(train.drop(['id','claim'],axis=1))\ntr.correlated_feature_sets_","3536e07c":"class dataProcessing:\n    \n    def __init__(self,data,feature):\n        self.X        = data.copy()\n        self.feature  = feature\n\n    def robustScaling(self):\n        # Scaling data\n        scaler = RobustScaler()\n        scaler.fit(self.X[self.feature])\n        X = scaler.transform(self.X[self.feature])\n        scalerfile = 'robust_scaler.sav'\n        pickle.dump(scaler, open(scalerfile, 'wb'))\n        return X\n    def standardScaling(self):\n        # Scaling data\n        scaler = StandardScaler()\n        scaler.fit(self.X[self.feature])\n        X = scaler.transform(self.X[self.feature])\n        scalerfile = 'standard_scaler.sav'\n        pickle.dump(scaler, open(scalerfile, 'wb'))\n        return X\n    def minMaxScaling(self):\n        # Scaling data\n        scaler = MinMaxScaler()\n        scaler.fit(self.X[self.feature])\n        X = scaler.transform(self.X[self.feature])\n        scalerfile = 'minmax_scaler.sav'\n        pickle.dump(scaler, open(scalerfile, 'wb'))        \n        return X","0f0cce24":"# Drop target features\nX = train.drop(['id','claim'],axis=1)\n# move target features\ny = train['claim']\nX","070e95a7":"# LGBM parameters\nbest_params = {'n_estimators': 1000, \n               'subsample': 0.4, \n               'learning_rate': 0.01, \n               'num_leaves': 70, \n               'is_unbalance':False,\n               'device': 'gpu'}\n    \ngbm = lgb.LGBMClassifier(**best_params)\n\n# OOF predictions\nscores = list()\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n\n# enumerate splits\nfor train_idx, test_idx in kfold.split(X,y):\n\n    # get train fold data\n    train_X, test_X = X.loc[train_idx,:], X.loc[test_idx,:]\n    train_y, test_y = y.loc[train_idx], y.loc[test_idx]\n\n    # fit model on train fold data\n    gbm.fit(train_X, train_y)\n    # predict on test fold data\n    pred = gbm.predict_proba(test_X)[:, 1] # This grabs the positive class prediction\n    # get accuracy score\n    cv_score  = roc_auc_score(test_y, pred)\n    # store score\n    scores.append(cv_score)\n    print('> ', cv_score)\n# summarize model performance\nmean_s, std_s = np.mean(scores), np.std(scores)\nprint('Mean: %.5f, Standard Deviation: %.5f' % (mean_s, std_s))","4b098f45":"features = [var for var in X.columns]\n\n# Scaling Data\ndataObj   = dataProcessing(X,features)\nX_scaled  = pd.DataFrame(dataObj.robustScaling())\nX_scaled.columns = features\nX_scaled","9243aecf":"print(train.shape,X_scaled.shape)","c846570a":"class swapDataset:\n    def __init__(self, features, noise):\n        self.features = features\n        self.noise = noise\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        \n        sample = self.features[idx, :].copy()\n        sample = self.swap_sample(sample)\n        dct = {\n            'x' : torch.tensor(sample, dtype=torch.float),\n            'y' : torch.tensor(self.features[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \n    def swap_sample(self,sample):\n            num_samples = self.features.shape[0]\n            num_features = self.features.shape[1]\n            if len(sample.shape) == 2:\n                batch_size = sample.shape[0]\n                random_row = np.random.randint(0, num_samples, size=batch_size)\n                for i in range(batch_size):\n                    random_col = np.random.rand(num_features) < self.noise\n                    sample[i, random_col] = self.features[random_row[i], random_col]\n            else:\n                batch_size = 1\n                random_row = np.random.randint(0, num_samples, size=batch_size)\n                random_col = np.random.rand(num_features) < self.noise\n                sample[ random_col] = self.features[random_row, random_col]\n                \n            return sample","61283d1f":"'''''''''''''''\nOptimizer\n'''''''''''''''\ndef get_optimizer(model):\n    '''\n    To keep the weights small and avoid exploding gradient. Because the L2 norm of the weights are added to the \n    loss, each iteration of your network will try to optimize\/minimize the model weights in addition to the loss. \n    This will help keep the weights as small as possible, preventing the weights to grow out of control, and thus \n    avoid exploding gradient\n    '''\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=False)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2,factor=0.001,threshold=1e2)    \n  \n    return optimizer,scheduler\n\n'''''''''''''''\nWeights\n'''''''''''''''\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        '''\n        This is an example i created as instructed by Andrew Ng course\n        torch.ones([10, 6])*torch.sqrt(torch.Tensor([2.])\/torch.Tensor([1.4142]))\n        '''\n        m.weight.data = torch.randn(m.weight.size())*torch.sqrt(torch.Tensor([1.])\/m.weight.size()[1])\n      \n'''''''''''''''\nAutoencoder\n'''''''''''''''\nclass Autoencoder(nn.Module):\n    # instantiate the object created using this class\n    def __init__(self):\n        super().__init__()\n        m = nn.Dropout(p=0.2)\n        \n        '''\n        Techniques like alternate weight initialization schemes and alternate activation functions can be used to reduce the impact of \n        the vanishing gradients problem for feed-forward neural networks. \n        '''\n        # FC (Fully Connected Layer) -> BatchNorm -> ReLu(or other activation) -> Dropout -> FC ->\n        self.encoder = nn.Sequential(\n            nn.Linear(119,480),\n            nn.BatchNorm1d(480),            \n            nn.ReLU(),\n\n            nn.Linear(480,95),\n            nn.BatchNorm1d(95),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(95,480),\n            nn.BatchNorm1d(480),                        \n            nn.ReLU())\n            \n        self.decoder_recon = nn.Linear(480,119)\n        self.decoder_classifier = nn.Linear(480,119)\n\n    \n    def forward(self,x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        reconstruction = self.decoder_recon(x)\n        logits = self.decoder_classifier(x)\n        \n        return reconstruction,logits\n    \n    def get_encoder_state(self,x):\n        encoded = self.encoder(x)\n        return encoded","4d977c4c":"'''''''''''''''''''''''''''''''''''''''''''''\nSwapping Dataset and create data loader\n'''''''''''''''''''''''''''''''''''''''''''''\ndevice   = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrain_dataset = swapDataset(X_scaled.values,0.2) # Swapping data \ndata_loader   = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n\n'''''''''''''''''''''''''''''''''''''''''''''\nDefine Model\n'''''''''''''''''''''''''''''''''''''''''''''\nautoencoder = Autoencoder()\n\n'''''''''''''''''''''''''''\nAttach Weights to model\n'''''''''''''''''''''''''''\nautoencoder.apply(init_weights)\n\n'''''''''''''''''''''\nDefine Loss function\n'''''''''''''''''''''\ncriterion = nn.MSELoss()\n\n'''''''''''''''''''''''''''\nLoad Optimizer and scheduler\n'''''''''''''''''''''''''''\noptimizer,scheduler = get_optimizer(autoencoder)\nautoencoder.to(device)","425d10af":"seed = 4\ntorch.manual_seed(seed)\n\nnum_epochs = 200\noutputs = []\nrunning_loss = 0.0\n\nfor epoch in range(num_epochs):\n    for data in data_loader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        recon,logits = autoencoder(inputs)\n        loss_recon  = criterion(recon,targets) # calculate loss\n        loss_classifier     = criterion(logits,inputs)\n        loss = loss_classifier \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step() # update weights \n        if not  scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n            scheduler.step(loss)\n        running_loss += loss.item()\n    \n    if (epoch+1) % 2 ==0:\n        print(f'Epoch: {epoch+1}, Loss:{loss_classifier}')","910f7d8f":"# Save model for future use\n# torch.save(autoencoder.state_dict(),'DAE_Auteoncoder.pth')","e0fd85fd":"# Load model if you want to use best model base on your checkpoint\n# autoencoder.load_state_dict(torch.load('..\/input\/daeautoencoderbest\/DAE_Auteoncoder_best.pth'))\n# autoencoder","9ce0bf43":"torch.cuda.empty_cache()\ngc.collect()\n# Convert to tensor\nX_train = torch.from_numpy(pd.DataFrame(X_scaled).to_numpy(np.float32)).to(device)\n# Encoded using autoencoder\nX_train_encoded = autoencoder.get_encoder_state(X_train)\n# transfer from tensor to dataframe\nX = pd.DataFrame(X_train_encoded.cpu().detach().numpy())\nX.shape","33239dac":"best_params = {'n_estimators': 1000, \n               'subsample': 0.4, \n               'learning_rate': 0.01, \n               'num_leaves': 70, \n               'is_unbalance':False,\n               'device': 'gpu'}\n    \ngbm = lgb.LGBMClassifier(**best_params)\n\n# OOF predictions\nscores = list()\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n\n\n# enumerate splits\nfor train_idx, test_idx in kfold.split(X,y):\n\n    # get data\n    train_X, test_X = X.loc[train_idx,:], X.loc[test_idx,:]\n    train_y, test_y = y.loc[train_idx], y.loc[test_idx]\n\n    # fit model\n    gbm.fit(train_X, train_y)\n\n    pred = gbm.predict_proba(test_X)[:, 1] # This grabs the positive class prediction\n    cv_score  = roc_auc_score(test_y, pred)\n    \n    # store score\n    scores.append(cv_score)\n    print('> ', cv_score)\n# summarize model performance\nmean_s, std_s = np.mean(scores), np.std(scores)\nprint('Mean: %.5f, Standard Deviation: %.5f' % (mean_s, std_s))","f355859d":"- Given data set is for classification\n- It has 118 features from \"f1\" to \"f118\" excluding \"id\" and target feature \"claim\"\n- Target feature is claim (1 or 0)","f62aa7a9":"#### **2.2) Missing Data**","e6e32414":"##  **9) Create Dataset & Data Loader Using Swapping Class**","c5ac6c9c":"#### **2.4) Correlated Features**","168f45e2":"#### **Findings**","a67a0694":"![Swapped Data-3.png](attachment:f74a2fe2-7777-440d-a39a-296ab0a15c04.png)","e7080b63":"- Features are too much overlapped for the given  target claim '1' and '0'\n- Some features are bimodal and some are multi modal, it means features are from different subgroups\n- Some features are too much left or right skewed\n- Its quite challenging to generate new features which seperate given data from claim '1' to '0'\n- Autoencoder may help , it needs to be explored","dbe14f1c":"#### **2.1) Target Feature Distribution**","c8b86d39":"# **12) Downstream Models**","7c3c128a":"#### **Findings**","0903c9ae":"## **4) Base Line Models**","f1ef7242":"- With all given features and generated new features \"nan_count\" , model is giving 0.81351 accuracy score which is a good starting point\n- We will try to reduce feature space first using Autoencoder technique.Reduced feature space may lead to enhance model performance using hyperparameter tuning","b42e00d3":"### **Problem Statement**","ac356e71":"## **11) Encoding**","aa4187e8":"- Before understanding Swap Denoising Autoencoder (DAE) , we must understand what is noise and why it is useful. In mathematics there is an identity function , a function always return the same value as its an input , for example f(x) = x for the domain of integer {1,2,3,4}. This identity function will always yield f(x) --> 1,2,3,4.In simple Autoencoder hidden layers learn generate complex relations between the given inputs ,generated noise try to decrease dependency of these learned relations so it can avoid identity.Noises randomly apply so the same input at different time can learn something different rather than depend on one unified relations.\n\n- There are different techniques to generate noise, it depends from one data to another data. Image data and tabular data has different proximity.We have tabular data,we will use swap noise where we will swap some defined percentage of input data in each batch,for example if we have 5 rows with 3 columns data as show below then in a batch 20% swapping noise will effect one column in each batch iterations as 20% of 5 rows is 1.For more clarity see example below","fa99ea83":"* Threshold for given dataset feature correlation is 0.8 , none of any features are highly correlated to each other\n* If the input features were each independent of one another, this compression and subsequent reconstruction would be a very difficult task for autoencoder","7bd91753":"## **3) Data Processing**","1f1313fd":"## **6) Scaling Data**","ff5ce823":"## **7) Swap Data Set Class**","6f101dd3":"### **Conclusion**","db10cce7":"#### **Findings**","31322ea6":"- Claim vs not Claim rate are almost 50%\n- Given data is balanced\n- We dont need to balance data using any sampling technique , or associate weights with classes during training ","363b4713":"#### **2.3) Density plots features w.r.t (claim) feature**","646c9b76":"![image.png](attachment:a4abadd1-e8a5-4410-a527-8d6b635450ac.png)","6d8668ba":"##  **10) Training Network**","ce8d109e":"### **12.1) LGBM**","a233fcab":"## **8) Autoencoder Class**","fc726e49":"- Any neural network works best on scaled data to calculate gradeints","3cfc22a2":"## **2) EDA**","3db075a0":"* In summary,latent space model overall score on unseen data is 0.80658.While without autoencoder,base line model is giving 0.81351. There is a loss of 0.006 with autoencoder technique which is not bad.Moving forward i will implement some other swapping techniques or some other type of autoencoder to break this 0.81351 benchmark with autoencoder.There could be some manifold which can exploit this data insights and boost overall score of this challenge.There could be some latent space which can be revealed using some other noise technique or using some other autoencoder types Convolutional Autoencoder or Deep Autoencoder for the same overlapped and uncorrelated data features.Please give any feedback to improve this approach and upvote if it helps you out to understand dimension reduction technique","ca49340e":"## **1) Read Data**","c289e098":"#### **Findings**","74c0fe43":"## **5) Autoencoder**","8c79c1d6":"The dataset is used for this problem is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.The real challenge with this data is it has 119 features and almost one million records for training\n\nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/data\n\nMy objective in this notebook to reduce the size of features atleast 20% using unsupervised learning technique autoencoders without losing any significant model performance.Too many features leads to over-fitting when learning a model, reducing feature size with some compression technique helps to generalize model well","47916e02":"### **5.1) What is Autoencoder**","472c417e":"#### **Findings**","5637fb28":"- Most of the features data are missing \n- As given data is synthetically generated we can't judge which column has to keep and impute  \n- For single missing values or for values missing less than 5% simple imputation is fine,simple imputation is mean \/ median \/ mode. But if it is more than 5% then here a MICE helps , it is a multiple imputation by chain equation. In this notebook i will use simple imputation technique\n- Based on during Kaggle competition most of the competitors used this missing as a new feature which boosted score by 1% which is a lot to climb on public\/private leaderboard. \n- On the backend i tried to use MICE imputation and gave up as missingness is not in one column , it is almost in all columns so it deos not make sense to use MICE technique in this situation , if you want to see further MICE detail then have a look here https:\/\/stats.stackexchange.com\/questions\/421545\/multiple-imputation-by-chained-equations-mice-explained","8af81e11":"- Autoencoders are an unsupervised learning technique in which we use neural network that attempts to do two things\n\n    1- It compresses input data knowledge into a lower dimnesion\n    \n    2- Then it uses this lower dimensional representation of the data to recreate the original input\n    \n- the difference between the attempted recreation and the original input is called reconstruction error , by training the network to minimize this reconstruction error on given dataset the network learns to exploit the natural structure (Latent Representation) in your data to find an efficient lower dimensional representation\n\n- Left part of the network is called encoder, its job to transform the original input into a lower dimensional represnetation (Latent Representation). Our real data is not random but instead it has structure and sturcture means we do'nt need every part of our full input space to represent our data , its the encoders job to map it from that full input space into a meaningful lower dimension\n\n- Right part of the network is called decoder, it recreate the original input using the output of the encoder, it tries to reverse the encoding process. It tries to recreate a higher dimensional thing using a lower dimensional thing (middle layer) by training the whole network to minimze the reconstruction error.Training network with different epochs find the weights that minimize the reconstruction error\n\n- The whole process is to force encoder and decoder to work together to find the most efficient way to condense the input data into lower dimension\n\n- As we have 118 features , we will try to compress it first by 95 features and will see how above base model LGBM score \n  on these new representation of data\n\n- We will not use simple Autoencoder,we will use swap noise Autoencoder.What is it and why to use it? see next section below","da0675d7":"### **5.2) Swap Denoising Autoencoder**","dd22d21a":"#### **Findings**"}}