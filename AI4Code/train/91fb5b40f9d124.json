{"cell_type":{"1a9a7c75":"code","5bc5c2d7":"code","313f092f":"code","86ed084b":"code","bc586db0":"code","eba9f42d":"code","839e5f6d":"code","41fbe99a":"code","fa8abb94":"code","3471e4b1":"code","b2460afa":"code","c0eccffc":"code","bf59e58c":"code","6f01984e":"code","b58d55ba":"code","a41dd236":"code","6ab81bc9":"code","5336e981":"code","6cae8963":"code","a7ae5bd4":"code","025bb5aa":"code","0a1a2b5f":"code","58c381b8":"code","6e733d00":"code","1259a314":"code","6e96571b":"code","c1511fdd":"code","14750118":"code","0df3bb46":"code","34a36b21":"code","b484e9af":"code","344752df":"code","0717d662":"code","93543e42":"code","5d780e5b":"code","a68140ba":"code","ae6051c7":"code","761fdd6f":"code","01497e37":"code","e29b2025":"code","66cd62c0":"code","7444d9aa":"code","3ac392ac":"code","e3bf2ead":"code","63cf0637":"code","84b8ab21":"code","c6e6955c":"markdown","c287b7df":"markdown","17e6bc8f":"markdown","1ae5730d":"markdown","f9e2563e":"markdown","31344a23":"markdown","233d7e8f":"markdown","f8d681e1":"markdown","44acf332":"markdown","9d398eb7":"markdown","fc523310":"markdown","ae17fd4d":"markdown","9e0b7114":"markdown","96afc03e":"markdown","ed897393":"markdown","2586c057":"markdown","019633cb":"markdown","e64dab84":"markdown","0f3df9c6":"markdown","946d2ee8":"markdown","7f51d50c":"markdown","616597c4":"markdown","c68ff82a":"markdown","3b8fbbee":"markdown","29109b6a":"markdown"},"source":{"1a9a7c75":"import numpy as np\nimport pandas as pd\nimport math\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom scipy import stats\nfrom scipy.stats import norm, skew \nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    cross_validate,\n    train_test_split,\n)\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap\n\nimport optuna\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.style as style\nimport seaborn as sns\nstyle.use('seaborn')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5bc5c2d7":"df= pd.read_csv(\"..\/input\/quality-prediction-in-a-mining-process\/MiningProcess_Flotation_Plant_Database.csv\",\n                decimal=\",\",\n                parse_dates=[\"date\"],\n                infer_datetime_format=True)\ndf.head()","313f092f":"df.info(),df.isna().sum().sum()","86ed084b":"df.describe().T","bc586db0":"df= df.drop(df.columns[[0]], axis=1)\ntrain, test = train_test_split(df,test_size=0.2,random_state=42)\ntrain_split,val_split=train_test_split(train,test_size=0.2,random_state=42)\ntrain.shape,test.shape,train_split.shape,val_split.shape","eba9f42d":"plt.figure(figsize=(12,12))\ng = sns.heatmap(train.corr() ,\n                fmt='.2f',\n                annot=True, \n                annot_kws={'size': 8} ,\n                cmap=sns.diverging_palette(20, 220, as_cmap=True)\n            )","839e5f6d":"def plotting_3_chart(df, feature):\n\n    fig = plt.figure(constrained_layout=True, figsize=(10,5))\n    grid = gridspec.GridSpec(ncols=1, nrows=3, figure=fig)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :3])\n    ax1.set_title('Histogram')\n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :3])\n    ax2.set_title('QQ_plot')\n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[2,:3])\n    ax3.set_title('Box Plot')\n    sns.boxplot(df.loc[:,feature], ax = ax3 )","41fbe99a":"plotting_3_chart(train, \"% Silica Concentrate\")","fa8abb94":"features = train.columns[:]\ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 5,figsize=(20, 20))\nfor feature in features:\n    plt.subplot(5, 5,i)\n    sns.distplot(train[feature],color=\"blue\", kde=True, bins=120, label='train')\n    sns.distplot(test[feature],color=\"red\", kde=True, bins=120, label='test')\n    plt.ylabel(\"\");plt.xlabel(feature, fontsize=9);plt.legend()\n    i += 1\nplt.show()","3471e4b1":"X_train_split= train_split.drop(['% Silica Concentrate','% Iron Concentrate'], axis=1)\ny_train_split= train_split['% Silica Concentrate']\nX_valid_split= val_split.drop(['% Silica Concentrate','% Iron Concentrate'], axis=1)\ny_valid_split= val_split['% Silica Concentrate']","b2460afa":"model = xgb.XGBRegressor(n_estimators=3000,random_state=42, tree_method=\"gpu_hist\").fit(\n    X_train_split, y_train_split\n)","c0eccffc":"preds = model.predict(X_valid_split)\nrmse = mean_squared_error(y_valid_split, preds, squared=False)\nrmse","bf59e58c":"_, val_subset = train_test_split(val_split,test_size=0.2,random_state=42)\nX_valid_subset= val_subset.drop(['% Silica Concentrate','% Iron Concentrate'], axis=1)\ny_valid_subset= val_subset['% Silica Concentrate']\n","6f01984e":"%%time\n\nperm = PermutationImportance(model, random_state=42).fit(X_valid_subset, y_valid_subset)\neli5.show_weights(perm, feature_names = X_valid_subset.columns.tolist())","b58d55ba":"%%time\n\npdp_goals = pdp.pdp_isolate(model=model, \n                            dataset=X_valid_subset, \n                            model_features=X_valid_subset.columns.tolist(), \n                            feature='Amina Flow')\npdp.pdp_plot(pdp_goals, 'Amina Flow')\nplt.show()","a41dd236":"%%time\n\nfnames = ['% Silica Feed', 'Ore Pulp pH']\nlongitudes_partial_plot  =  pdp.pdp_interact(model=model, dataset=X_valid_subset,\n                                            model_features=X_valid_subset.columns.tolist(), features=fnames)\npdp.pdp_interact_plot(pdp_interact_out=longitudes_partial_plot,\n                      feature_names=fnames, plot_type='contour')\nplt.show()","6ab81bc9":"xgb_explainer = shap.TreeExplainer(\n    model, X_train_split, feature_names=X_train_split.columns.tolist()\n)","5336e981":"%%time\n\nbooster_xgb = model.get_booster()\nshap_values_xgb = booster_xgb.predict(xgb.DMatrix(X_train_split, y_train_split), pred_contribs=True)","6cae8963":"shap_values_xgb = shap_values_xgb[:, :-1]\npd.DataFrame(shap_values_xgb, columns=X_train_split.columns.tolist()).head()","a7ae5bd4":"shap.summary_plot(\n    shap_values_xgb, X_train_split, feature_names=X_train_split.columns, plot_type=\"bar\"\n)","025bb5aa":"shap.summary_plot(shap_values_xgb, X_train_split, feature_names=X_train_split.columns);","0a1a2b5f":"shap.dependence_plot(\"% Silica Feed\", shap_values_xgb, X_train_split, interaction_index=\"% Iron Feed\");","58c381b8":"target_plot = [col for col in train.columns.to_list() if col not in [\"date\",'% Silica Concentrate','% Iron Concentrate']]\n\nfor target in target_plot:\n    shap.dependence_plot(target, shap_values_xgb, X_train_split, interaction_index=\"auto\");","6e733d00":"%%time\n\ninteractions_xgb = booster_xgb.predict(\n    xgb.DMatrix(X_train_split, y_train_split), pred_interactions=True\n)","1259a314":"def get_top_k_interactions(feature_names, shap_interactions, k):\n    # Get the mean absolute contribution for each feature interaction\n    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n    interactions = []\n    for i in range(aggregate_interactions.shape[0]):\n        for j in range(aggregate_interactions.shape[1]):\n            if j < i:\n                interactions.append(\n                    (\n                        feature_names[i] + \"-\" + feature_names[j],\n                        aggregate_interactions[i][j] * 2,\n                    )\n                )\n    # sort by magnitude\n    interactions.sort(key=lambda x: x[1], reverse=True)\n    interaction_features, interaction_values = map(tuple, zip(*interactions))\n\n    return interaction_features[:k], interaction_values[:k]\n\n\ntop_10_inter_feats, top_10_inter_vals = get_top_k_interactions(\n    X_train_split.columns, interactions_xgb, 10\n)\n\n\ndef plot_interaction_pairs(pairs, values):\n    plt.bar(pairs, values)\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show();","6e96571b":"top_10_inter_feats, top_10_inter_vals = get_top_k_interactions(\n    X_train_split.columns, interactions_xgb, 10\n)\n\nplot_interaction_pairs(top_10_inter_feats, top_10_inter_vals)","c1511fdd":"row_to_show = 5\ndata_for_prediction = X_train_split.iloc[row_to_show]\nshap_values = xgb_explainer.shap_values(data_for_prediction)","14750118":"shap.initjs()\nshap.force_plot(xgb_explainer.expected_value, shap_values, data_for_prediction)","0df3bb46":"row_to_show = 200\ndata_for_prediction = X_train_split.iloc[row_to_show]\nshap_values = xgb_explainer.shap_values(data_for_prediction)","34a36b21":"shap.initjs()\nshap.force_plot(xgb_explainer.expected_value, shap_values, data_for_prediction)","b484e9af":"columns = [col for col in train.columns.to_list() if col not in [\"date\",'% Silica Concentrate','% Iron Concentrate']]\n","344752df":"data=train[columns]\ntarget=train['% Silica Concentrate']\ndata","0717d662":"def objective(trial,data=data,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n\n    param = {\n        'max_depth': trial.suggest_int('max_depth', 2, 15),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1500, 50),\n        'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n        'reg_alpha': trial.suggest_int('reg_alpha', 1, 50),\n        'reg_lambda': trial.suggest_int('reg_lambda', 5, 100),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 20),\n#         'learning_rate': trial.suggest_discrete_uniform('leaning_rate', 0.01, 1, 0.01)\n    }\n    \n    model = xgb.XGBRegressor(tree_method=\"gpu_hist\",**param )  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","93543e42":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=30)\n#study.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","5d780e5b":"study.trials_dataframe()","a68140ba":"optuna.visualization.plot_optimization_history(study)","ae6051c7":"optuna.visualization.plot_parallel_coordinate(study)","761fdd6f":"optuna.visualization.plot_slice(study)","01497e37":"#plot_contour: plots parameter interactions on an interactive chart. You can choose which hyperparameters you would like to explore.\noptuna.visualization.plot_contour(study, params=['max_depth',\n                            #'max_depth',\n                            'eta',\n                            'subsample',\n                            \"n_estimators\",\n                            'min_child_weight',\n                            'subsample',\n                                                \"reg_alpha\",'reg_lambda'\n                                                ])","e29b2025":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","66cd62c0":"#Visualize empirical distribution function\noptuna.visualization.plot_edf(study)","7444d9aa":"params={'max_depth': 12, 'subsample': 0.9, 'n_estimators': 4450, 'eta': 0.04, 'reg_alpha': 2, 'reg_lambda': 33, 'min_child_weight': 19}\nmodel = xgb.XGBRegressor(**params, tree_method=\"gpu_hist\",random_state=42).fit(\n   X_train_split, y_train_split\n)","3ac392ac":"preds = model.predict(X_valid_split)\nrmse = mean_squared_error(y_valid_split, preds, squared=False)\nrmse","e3bf2ead":"preds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,random_state=42,shuffle=True)\nrmse=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train[columns],train['% Silica Concentrate']):\n    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n    y_tr,y_val=train['% Silica Concentrate'].iloc[trn_idx],train['% Silica Concentrate'].iloc[test_idx]\n    model = xgb.XGBRegressor(**params, tree_method=\"gpu_hist\",random_state=42)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(test[columns])\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(n+1,rmse[n])\n    n+=1","63cf0637":"np.mean(rmse)","84b8ab21":"# Compute rmse_test\nrmse_test = mean_squared_error(test['% Silica Concentrate'], preds)**0.5\n\n# Print rmse_test\nprint('Test score: {:.3f}'.format(rmse_test))","c6e6955c":"<div style='text-align: justify' ><font size =\"3\">\n-The vertical axis on the left indicates the names of the features, arranged according to their importance, from top to bottom.\n\/-The horizontal axis represents the magnitude of the SHAP values for the predictions.\n\/-The right vertical axis represents the actual magnitude of a feature as it appears in the data set and colors the points. \n From the graph of% Silica Feed, as we see in the graph that the more value this characteristic has, the more it impacts the SAHP value, therefore it affects the model more, which makes sense. From the graph of Flotation Column 03 Air Flow, from this graph we can see that while the values are higher the shap value decreases and while the value is lower the shap value increases. If we keep observing we can draw more conclusions. A good understanding of these graphs is important to understand the model.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">     \n- El eje vertical de la izquierda indica los nombres de las caracter\u00edsticas, ordenados seg\u00fan su importancia, de arriba hacia abajo.\n\/- El eje horizontal representa la magnitud de los valores SHAP para las predicciones.\n\/- El eje vertical derecho representa la magnitud real de una caracter\u00edstica tal como aparece en el conjunto de datos y colorea los puntos.\n  Del grafico d\u00e9 % Silica Feed , como vemos en el grafico que mientras m\u00e1s valor tenga esta caracter\u00edstica m\u00e1s impacta en el valor SAHP por lo que afecta m\u00e1s al modelo, lo cual tiene l\u00f3gica.Del grafico d\u00e9  Flotation Column 03 Air Flow, de este grafico podemos sacar que mientras los valores son m\u00e1s altos el valor shap disminuye y mientras el valor es m\u00e1s bajo el valor shap aumenta. Si seguimos observando podemos sacar m\u00e1s conclusiones. Entender bien estos gr\u00e1ficos es importante para comprender el modelo.\n<\/div>\n    <p><\/p>","c287b7df":"\n <b><p style=\"text-align:center;\">\n    <font size =\"6\" color =#1B3873>\n        Mining Process SHAP Explainability XGBoost\n    <\/font>\n<\/b>","17e6bc8f":"<div style='text-align: justify' ><font size =\"3\">For these first 2 examples we are going to use a subset since if we use the train it would take a long time to run, for that reason we will use SHAP later that our GPU uses to be able to work with datasets of large sizes.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Para estos 2 primeros ejemplos vamos a usar un subset ya que si usamos el train demorar\u00eda mucho en correr, por esa raz\u00f3n usaremos SHAP m\u00e1s adelante que utiliza nuestra GPU para poder trabajar con dataset de tama\u00f1os grandes.<\/div>\n    <p><\/p>","1ae5730d":"### **Exploratory Data Analysis**\n","f9e2563e":"<div style='text-align: justify' ><font size =\"3\">In this type of graph we can see how 2 characteristics are interrelated with the target variable.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">En este tipo de gr\u00e1ficos podemos observar c\u00f3mo se interrelacionan 2 caracter\u00edsticas con la variable objetivo.\n<\/div>\n    <p><\/p>","31344a23":"<div style='text-align: justify' ><font size =\"3\">\n- The vertical axis on the left indicates the shap value of the characteristic\n\/- The horizontal axis represents the magnitude of the characteristic\n\/- The right vertical axis represents the real magnitude of the most correlated characteristic or the one that we choose.\n  In this case we can see in the graph of% Silica Feed the values that give us the highest shap values are in the range of 15-25, so this range provides the highest SAHP values, with this information the metallurgical engineer can make decisions, it can also be seen in the graph that low silica feed values are related to high% iron feed values and vice versa which also makes sense. in this case we have chosen the value with coloring, but it can also be set to auto and shap will choose the most related feature automatically.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\"> \n-El eje vertical de la izquierda indica el valor shap de la caracter\u00edstica\n\/-El eje horizontal representa la magnitud de la caracter\u00edstica\n\/-El eje vertical derecho representa la magnitud real de la caracter\u00edstica m\u00e1s correlaciona o la que nosotros escojamos\n En este caso podemos ver en la grafico de % Silica Feed los valores que nos dan los valores shap m\u00e1s altos est\u00e1 en el rango de 15-25, por lo que este rango entrega los mayores valores de SAHP, con esta informaci\u00f3n el ingeniero metal\u00fargico puede tomar decisiones, tambi\u00e9n se puede ver en el grafico que valores de silica feed bajos estan relacionados a altos valores de % iron feed y viceversa que tambi\u00e9n tiene sentido. en este caso nosotros hemos elegido el valor con colorear, pero tambi\u00e9n se puede poner en auto y shap elegir\u00e1 la caracter\u00edstica m\u00e1s relacionada autom\u00e1ticamente.\n<\/div>\n    <p><\/p>","233d7e8f":"<div style='text-align: justify' ><font size =\"3\">We can see that some columns are highly negatively correlated as in the case of % Iron Concentrate and % Silica Concentrate or % Iron Feed and % Silica Feed, we also see that we do not have null values and we have some abnormal values, this time I will not modify the abnormal values to be able to work with them in a next notebook where I will deal with them and I will also make assembled models to improve the results.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Podemos ver que algunas columnas est\u00e1n altamente correlacionados negativamente como el caso de % Iron Concentrate y % Silica Concentrate o % Iron Feed y % Silica Feed , tambi\u00e9n vemos que no tenemos valores nulos y tenemos algunos valores anormales, en esta oportunidad no trabajare los valores anormales para poder trabajar con ellos en un siguiente notebook donde los tratare y tambi\u00e9n realizar\u00e9 modelos ensamblados para mejorar los resultados.<\/div>\n    <p><\/p>","f8d681e1":"<div style='text-align: justify' ><font size =\"3\">SHAP helps us better understand the model and breaks down a prediction to show the impact of each characteristic, if you want more information you can visit its website  <a style='color:#1B3873; font-size:110% ;' href='https:\/\/shap.readthedocs.io\/en\/latest\/index.html' target='_blank'> SHAP. <\/a>\nRemember to turn on your GPU to run this part of the Notebook.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">SHAP nos ayuda a entender de mejor manera el modelo y desglosa una predicci\u00f3n para mostrar el impacto de cada caracter\u00edstica, si quiere m\u00e1s informaci\u00f3n puede visitar su p\u00e1gina web  <a style='color:#1B3873; font-size:110% ;' href='https:\/\/shap.readthedocs.io\/en\/latest\/index.html' target='_blank'> SHAP. <\/a>\nRecude prender su GPU para correr esta parte del Notebook.\n<\/div>\n    <p><\/p>","44acf332":"<div style='text-align: justify' ><font size =\"3\">This graph shows us which characteristics are most interrelated, which can give us an idea to look for patterns.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Este grafico nos muestra que caracter\u00edsticas esta m\u00e1s interrelacionadas, lo que nos puede dar una idea para buscar patrones.\n<\/div>\n    <p><\/p>","9d398eb7":"# Modelo optimizado y predict en test","fc523310":"# Cross-Validation","ae17fd4d":"# **Introduction**\n\n\n    \n<div style='text-align: justify' ><font size =\"3\">Hello everyone, since 1 year and a half ago that I started learning about Deep Learning and Machine Learning in my free time, I am a mining engineer from Peru, I share this Notebook where I use SHAP, Optuna and XGBoost, if you have any questions, queries or correction, do not hesitate to leave it in the comments or contact me through <a style='color:#1B3873; font-size:110% ;' href='https:\/\/www.linkedin.com\/in\/cristiancartagenamatos\/' target='_blank'> LinkedIn.<\/a> <\/div>\n    <p><\/p>\n\n<div style='text-align: justify' ><font size =\"3\">In the mining industry a large amount of data is generated which is collected every day, this time we are going to see a Dataset of a mineral flotation process in a concentration plant. Machine Learning is a very good tool to use the data collected in mining to make predictions, but sometimes it is difficult to understand the logic behind it, for this reason I created this Notebook where I am going to use SHAP to better understand the data and being able to make informed decisions.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Hola a todos desde hace 1 a\u00f1o y medio que empec\u00e9 a aprender sobre Deep Learning y Machine Learning en mis tiempos libres, soy ingeniero de minas de Per\u00fa, les comparto este Notebook donde utilizo SHAP, Optuna y XGBoost, si tienes alguna duda, consulta o correcci\u00f3n no dudes en dejarlo en los comentarios o contactarte conmigo a trav\u00e9s de <a style='color:#1B3873; font-size:110% ;' href='https:\/\/www.linkedin.com\/in\/cristiancartagenamatos\/' target='_blank'> LinkedIn.<\/a> <\/div>\n    <p><\/p>\n\n<div style='text-align: justify' ><font size =\"3\">En la industria minera se genera una gran cantidad de datos los cuales son recopilados todos los d\u00edas, en esta oportunidad vamos a ver un Dataset de un proceso de flotaci\u00f3n de minerales en una planta de concentraci\u00f3n. El Machine Learning es una muy buena herramienta para utilizar los datos recopilados en la miner\u00eda para hacer predicciones, pero a veces es dif\u00edcil entender la l\u00f3gica detr\u00e1s de esta, por esta raz\u00f3n cree este Notebook donde voy a utilizar SHAP para poder entender de mejor manera los datos y poder tomar decisiones informadas.<\/div> \n    \n","9e0b7114":"\n\n\n<div style='text-align: justify' ><font size =\"3\">For this case I am going to put the values that optuna gave me with a n_trials = 50, this value can be improved by fine-tuning the optuna parameters looking for a better result.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Para este caso voy a poner los valores que me entrego optuna con un n_trials=50, se puede mejorar este valor al afinar los par\u00e1metros de optuna buscando un mejor resultado.\n<\/div>\n    <p><\/p>","96afc03e":"<div style='text-align: justify' ><font size =\"3\">We will create a Baseline Model using xgboost to predict the% Silica Concentrate, so it is necessary to remove the '% Silica Concentrate', '% Iron Concentrate' columns as mentioned in the problem statement.\nIs it possible to predict % Silica in Concentrate whitout using % Iron Concentrate column (as they are highly correlated)?\n    <\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Crearemos un Modelo de L\u00ednea base utilizando xgboost para predecir el % Silica Concentrate, por lo que es necesario eliminar las columnas '% Silica Concentrate','% Iron Concentrate' como se menciona en el enunciado del problema.\n\u00bfEs posible predecir el% de s\u00edlice en el concentrado sin usar la columna de% de concentrado de hierro (ya que est\u00e1n altamente correlacionados)?\n<\/div>\n    <p><\/p>","ed897393":"# Local interpretability","2586c057":"#  Summary\n\n<div style='text-align: justify' ><font size =\"3\">In this notebook we have seen how to use SHAP and Optuna to understand and improve our Machine Learning models, in the following weeks I will upload another post where I will work on outliers, optimize and tune the hyperparameters of different models and use an assembled model to reduce the RMSE.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">En este notebook hemos visto c\u00f3mo usar SHAP y Optuna para poder entender y mejorar nuestros modelos de Machine Learning, en las siguientes semanas voy a subir otro post donde trabajare los outliers, optimizare y afinare los hyperparametros de diferentes modelos y utilizare un modelo ensamblado para disminuir el RMSE.\n<\/div>\n    <p><\/p>","019633cb":"<div style='text-align: justify' ><font size =\"3\">This graph indicates that the values towards the top are the most important characteristics, and those towards the bottom are the ones that matter the least, giving us a high-level general description of the model, so more care should be taken in the plant in these important characteristics, paying more attention to Flotation Column 03 Air Flow, we should also take care that the process of Ore Pulp pH and Amina Flow are correct since they greatly affect our objective.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Este grafico nos indica que los valores hacia la parte superior son las caracter\u00edsticas m\u00e1s importantes, y aquellos hacia la parte inferior son los que menos importan d\u00e1ndonos una descripci\u00f3n general de alto nivel del modelo por lo que se deber\u00eda tener m\u00e1s cuidado en la planta en estas caracter\u00edsticas importantes, prestando m\u00e1s atenci\u00f3n a Flotation Column 03 Air Flow, tambi\u00e9n se deber\u00eda cuidar que el proceso de Ore Pulp pH y Amina Flow sean los correctos ya que afectan en gran medida en nuestro objetivo.\n<\/div>\n    <p><\/p>","e64dab84":"<div style='text-align: justify' ><font size =\"3\">Local interpretability helps us understand an individual prediction of the model, showing how each characteristic affects the prediction.\nThat is, the SHAP values of all the features are added together to explain why my prediction was different from the baseline (the mean). This allows us to decompose a prediction into a graph that shows which characteristics contribute positively, which characteristics contribute negatively, and how much those characteristics mattered. In the case of a metallurgical plant, it will help us understand why the% Silica Concentrate varied at a certain time and what this variation was due to, so that the engineers pay attention to these points.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Local interpretability nos ayuda a comprender una predicci\u00f3n individual del modelo, mostrando como afecta cada caracter\u00edstica a la predicci\u00f3n.\nEs decir, los valores SHAP de todas las caracter\u00edsticas se suman para explicar por qu\u00e9 mi predicci\u00f3n fue diferente de la l\u00ednea de base (la media). Esto nos permite descomponer una predicci\u00f3n en un gr\u00e1fico que muestra qu\u00e9 caracter\u00edsticas contribuyen positivamente, qu\u00e9 caracter\u00edsticas contribuyeron negativamente y cu\u00e1nto importaron esas caracter\u00edsticas. En el caso de una planta metal\u00fargica nos ayudara a entender porque en un tiempo determinado vario el % Silica Concentrate y a que se debi\u00f3 esta variaci\u00f3n, para que los ingenieros pongan atenci\u00f3n en estos puntos.\n\n<\/div>\n    <p><\/p>\n","0f3df9c6":"# Model Baseline\n","946d2ee8":"# Explainability","7f51d50c":"# Hyperparameter Optimization \n\n\n\n<div style='text-align: justify' ><font size =\"3\">Now we are going to optimize the hyperparameters to improve the XGBoost model, for this we are going to use the optuna tool that helps us find the best hyperparameters.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Ahora vamos a optimizar los hyperparametros para mejorar el modelo de XGBoost, para esto vamos a utilizar la herramienta de optuna que nos ayuda a encontrar los mejores hyperparametros.\n<\/div>\n    <p><\/p>","616597c4":"<div style='text-align: justify' ><font size =\"3\">We see that we have 737453 rows, of which we are going to divide into train and test, the test must not be manipulated and it will be used only at the end of the process to measure how successful our machine Learning process is, the train must be to divide into training and validation or to work with cross-validation. now we are going to do a quick scan of each column for abnormal values and do an exploratory data analysis.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Vemos que tenemos 737453 filas, de las cuales vamos a dividir en train y test, el test no debe de manipularse y va a ser utilizado solo al final del proceso para medir que tan acertado es nuestro proceso de machine Learning, en cuanto al train este se debe de dividir en entrenamiento y validaci\u00f3n o trabajar con validaci\u00f3n cruzada. ahora vamos a hacer una exploraci\u00f3n r\u00e1pida de cada columna en b\u00fasqueda de valores anormales y hacer un an\u00e1lisis exploratorio de datos.<\/div>\n    <p><\/p>","c68ff82a":"## Partial Plots\n\n<div style='text-align: justify' ><font size =\"3\">Partial dependency plots show how a characteristic affects predictions.\n  The y-axis is interpreted as a change in the prediction from what would be predicted at the baseline or the leftmost value.\nAn area shaded in blue indicates the confidence level.\nWe can see that Amina Flow does not affect the amount of silica in the concentrate until 380, from then on it affects the model giving a peak of affectation around 580. This gives us an idea of how a characteristic affects the result as it increases in magnitude, which to a metallurgical expert could help you better understand the process.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Las gr\u00e1ficas de dependencia parcial muestran c\u00f3mo una caracter\u00edstica afecta las predicciones.\n El eje y se interpreta como un cambio en la predicci\u00f3n de lo que se predecir\u00eda en la l\u00ednea de base o en el valor m\u00e1s a la izquierda.\nUn \u00e1rea sombreada en azul indica el nivel de confianza.\nPodemos ver que la Amina Flow no afecta a la cantidad de s\u00edlice en el concentrado hasta los 380, de ah\u00ed en adelante afecta en el modelo dando un pico de afectaci\u00f3n alrededor de 580. Esto nos da una idea de c\u00f3mo afecta una caracter\u00edstica a el resultado a medida que se incrementa de magnitud, lo cual para un experto de metalurgia podr\u00eda ayudarlo a entender mejor el proceso.\n<\/div>\n    <p><\/p>","3b8fbbee":"## Permutation Importance\n\n\n<div style='text-align: justify' ><font size =\"3\">Values towards the top are the most important features, and those towards the bottom matter least, giving us a high-level overview of the model.\nThe first number in each row shows how much the performance of the model decreased with a random mix (in this case, using \"precision\" as the performance metric). this shows us the importance in terms of the column in general and how it affects the model, but has the disadvantage of taking too long when using the CPU.<\/div>\n    <p><\/p>\n<div style='text-align: justify' ><font size =\"3\">Los valores hacia la parte superior son las caracter\u00edsticas m\u00e1s importantes, y aquellos hacia la parte inferior son los que menos importan, d\u00e1ndonos una descripci\u00f3n general de alto nivel del modelo. El primer n\u00famero de cada fila muestra cu\u00e1nto disminuy\u00f3 el rendimiento del modelo con una mezcla aleatoria (en este caso, utilizando la \"precisi\u00f3n\" como m\u00e9trica de rendimiento). esto nos da a conocer la importancia en cuanto a la columna en general y como afecta al modelo, pero tiene la desventaja de tardar demasiado al utilizar la CPU.<\/div>\n    <p><\/p>\n\n","29109b6a":"# SHAP"}}