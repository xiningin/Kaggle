{"cell_type":{"b4a30b61":"code","9df333ae":"code","64b423d1":"code","9ec48490":"code","a554026c":"code","0282ed57":"code","1a7df6d7":"code","be34ef55":"code","cbea510e":"code","e155576e":"code","64f6a060":"code","c2b60076":"code","e81ae34c":"code","deb28828":"code","b237a058":"code","3788f385":"code","14af6d0e":"code","0f8e1283":"code","0188867f":"markdown","79071abf":"markdown","bfcdead3":"markdown","28e099db":"markdown","ff128b21":"markdown","0a5c7c39":"markdown","b96e8295":"markdown","5809a5f0":"markdown","2f421b99":"markdown","66f3209b":"markdown","2cf5a505":"markdown","c26c76eb":"markdown","3e07fd63":"markdown","ff68c883":"markdown","f9a231de":"markdown","7a8027a2":"markdown","cd7a6b94":"markdown","b40f5f37":"markdown","4ad5231b":"markdown","a745639d":"markdown","c1bbcae9":"markdown","19237417":"markdown","9194726e":"markdown","23acccb7":"markdown","0f785f7e":"markdown","a759ba81":"markdown","8ded10f1":"markdown","84b75bd7":"markdown","ec85b43f":"markdown","77b55db7":"markdown","ad973d46":"markdown","a74242fa":"markdown"},"source":{"b4a30b61":"# Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport math\n\n# Reading the dataset (Tennis-dataset)\ndata = pd.read_csv('\/kaggle\/input\/playtennis\/PlayTennis.csv')","9df333ae":"def highlight(cell_value):\n    '''\n    Highlight yes \/ no values in the dataframe\n    '''\n    color_1 = 'background-color: pink;'\n    color_2 = 'background-color: lightgreen;'\n    \n    if cell_value == 'no':\n        return color_1\n    elif cell_value == 'yes':\n        return color_2  \n\ndata.style.applymap(highlight)\\\n    .set_properties(subset=data.columns, **{'width': '100px'})\\\n    .set_table_styles([{'selector': 'th', 'props': [('background-color', 'lightgray'), ('border', '1px solid gray'),\n                                                    ('font-weight', 'bold')]},\n     {'selector': 'tr:hover', 'props': [('background-color', 'white'), ('border', '1.5px solid black')]}])","64b423d1":"def find_entropy(data):\n    \"\"\"\n    Returns the entropy of the class or features\n    formula: - \u2211 P(X)logP(X)\n    \"\"\"\n    entropy = 0\n    for i in range(data.nunique()):\n        x = data.value_counts()[i]\/data.shape[0] \n        entropy += (- x * math.log(x,2))\n    return round(entropy,3)\n\n\n\ndef information_gain(data, data_):\n    \"\"\"\n    Returns the information gain of the features\n    \"\"\"\n    info = 0\n    for i in range(data_.nunique()):\n        df = data[data_ == data_.unique()[i]]\n        w_avg = df.shape[0]\/data.shape[0]\n        entropy = find_entropy(df.play)\n        x = w_avg * entropy\n        info += x\n    ig = find_entropy(data.play) - info\n    return round(ig, 3)   \n\n\n\ndef entropy_and_infogain(datax, feature):\n    \"\"\"\n    Grouping features with the same class and computing their \n    entropy and information gain for splitting\n    \"\"\"\n    for i in range(data[feature].nunique()):\n        df = datax[datax[feature]==data[feature].unique()[i]]\n        if df.shape[0] < 1:\n            continue\n        \n        display(df[[feature, 'play']].style.applymap(highlight)\\\n                .set_properties(subset=[feature, 'play'], **{'width': '80px'})\\\n                .set_table_styles([{'selector': 'th', 'props': [('background-color', 'lightgray'), \n                                                                ('border', '1px solid gray'), \n                                                                ('font-weight', 'bold')]},\n                                   {'selector': 'td', 'props': [('border', '1px solid gray')]},\n                                   {'selector': 'tr:hover', 'props': [('background-color', 'white'), \n                                                                      ('border', '1.5px solid black')]}]))\n        \n        print(f'Entropy of {feature} - {data[feature].unique()[i]} = {find_entropy(df.play)}')\n    print(f'Information Gain for {feature} = {information_gain(datax, datax[feature])}')","9ec48490":"print(f'Entropy of the entire dataset: {find_entropy(data.play)}')","a554026c":"entropy_and_infogain(data, 'outlook')","0282ed57":"entropy_and_infogain(data, 'temp')","1a7df6d7":"entropy_and_infogain(data, 'humidity')","be34ef55":"entropy_and_infogain(data, 'windy')","cbea510e":"sunny = data[data['outlook'] == 'sunny']\nsunny.style.applymap(highlight)\\\n    .set_properties(subset=data.columns, **{'width': '100px'})\\\n    .set_table_styles([{'selector': 'th', 'props': [('background-color', 'lightgray'), ('border', '1px solid gray'),\n                                                    ('font-weight', 'bold')]},\n     {'selector': 'tr:hover', 'props': [('background-color', 'white'), ('border', '1.5px solid black')]}])","e155576e":"print(f'Entropy of the Sunny dataset: {find_entropy(sunny.play)}')","64f6a060":"entropy_and_infogain(sunny, 'temp')","c2b60076":"entropy_and_infogain(sunny, 'humidity')","e81ae34c":"entropy_and_infogain(sunny, 'windy')","deb28828":"rainy = data[data['outlook'] == 'rainy'] \nrainy.style.applymap(highlight)\\\n    .set_properties(subset=data.columns, **{'width': '100px'})\\\n    .set_table_styles([{'selector': 'th', 'props': [('background-color', 'lightgray'), ('border', '1px solid gray'),\n                                                    ('font-weight', 'bold')]},\n     {'selector': 'tr:hover', 'props': [('background-color', 'white'), ('border', '1.5px solid black')]}])","b237a058":"print(f'Entropy of the Rainy dataset: {find_entropy(rainy.play)}')","3788f385":"entropy_and_infogain(rainy, 'temp')","14af6d0e":"entropy_and_infogain(rainy, 'humidity')","0f8e1283":"entropy_and_infogain(rainy, 'windy')","0188867f":"**Making a decision tree node using the feature which has the maximum Information Gain.**","79071abf":"![](https:\/\/miro.medium.com\/max\/1400\/1*Qy8wtdBnnlP3gz9gG6olvQ.jpeg)","bfcdead3":"# <center><span style=\"font-family:cursive;\">Decision Tree from scratch<\/span><\/center>","28e099db":"**Temp**","ff128b21":"![](https:\/\/developers.bloomreach.com\/binaries\/original\/content\/gallery\/developer\/blog\/decision-tree.jpg)","0a5c7c39":"# Step 3\n<h3 style='font-family: serif;'>Repeat until we run out of all features, or the decision tree has all leaf nodes.<\/h3>","b96e8295":"# <span style=\"font-family:cursive;\">What is Decision tree ?<\/span>","5809a5f0":"**Outlook**","2f421b99":"**Windy**","66f3209b":"<div class=\"alert alert-block alert-info\" style=\"text-align:center\">\n<b>Outlook has the highest Information Gain - 0.246, so it is used as the root node.<\/b><br>\n<b>In Overcast every data belongs to same class (Yes), which makes it as leaf node.<\/b><br>  \n<b>Sunny and Rainy has different class (Yes & no). So we find the feature with maximum Information gain for sunny and rainy for further splitting.<\/b>\n<\/div>","2cf5a505":"**Calculating the Information gain for each feature**\n\n**Temp**","c26c76eb":"<div class=\"alert alert-block alert-info\" style=\"text-align:center\">\n<b>Humidity has the highest Information Gain - 0.971, so it is used as the decision node.<\/b><br>\n<b>In Overcast and Humidity - high, normal every data classified to same class, which makes them as leaf node.<\/b><br>  \n<b>Now, only Rainy has different class (Yes & no). So again we find the feature with maximum Information gain for\nand rainy for further splitting.<\/b> \n<\/div>","3e07fd63":"# Step 1\n<h3 style='font-family: serif;'>Calculate the Information Gain for each feature.<\/h3>","ff68c883":"**Calculating the Information gain for each feature**\n\n**Temp**","f9a231de":"**Computing entropy for the entire dataset**","7a8027a2":"![Untitled Workspace (3).jpg](attachment:af92e3ee-9af5-46ed-9360-bcec6cd3cbb7.jpg)","cd7a6b94":"# <span style=\"font-family:cursive;\">Information Gain<\/span>\nInformation gain is used to decide which feature to split on at each step in building the tree. It works on the concept of the entropy. Entropy is used for calculating the purity of a node. Lower the value of entropy, higher is the purity of the node. \n\n#### Entropy = $- \\sum\\limits_{i=1}^{n} p_i \\space log_2 \\space p_i $\nwhere,\n> n = number of class in the target variable\n\n#### Information Gain = $ entropy(parent) - \\sum\\limits_{i=1}^{n} weighted \\space average_i*entropy(child)_i$\nwhere,\n> n = number of class in the selected feature","b40f5f37":"Types of decision tree : **Classification Tree** and **Regression Tree**.<br>\n\nThere are 4 different types of splitting criteria : \n* Categorical Target Variable (classification) : **Information Gain** , **Gini Impurity** and **Chi-Square** \n* Continuous Target Variable (regression) : **Reduction in Variance**.","4ad5231b":"<p>Before learning more about decision trees let\u2019s get familiar with some of the terminologies.<br>\n    \n<b>Root Nodes<\/b> \u2013 It is the node present at the beginning of a decision tree from this node the population starts dividing according to various features.<br>\n\n<b>Decision Nodes<\/b> \u2013 the nodes we get after splitting the root nodes are called Decision Node<br>\n\n<b>Leaf Nodes<\/b> \u2013 the nodes where further splitting is not possible are called leaf nodes or terminal nodes<br>\n\n<b>Sub-tree<\/b> \u2013 just like a small portion of a graph is called sub-graph similarly a sub-section of this decision tree is called sub-tree.<br>\n\n<b>Pruning<\/b> \u2013 is nothing but cutting down some nodes to stop overfitting.<\/p><br>","a745639d":"**Humidity**","c1bbcae9":"# Step 2\n<h3 style='font-family: serif;'>Make a decision tree node using the feature with the maximum Information Gain.<\/h3>","19237417":"**Windy**","9194726e":"![Untitled Workspace (10).jpg](attachment:2c6fd6ff-0511-4b0e-acf0-26a35e10aeb4.jpg)","23acccb7":"<div class=\"alert alert-block alert-info\" style=\"text-align:center\">    \n<b>Windy has the highest Information Gain - 0.971, so it is used as the decision node.<\/b><br>\n<b>This decision tree has all leaf node.<\/b><br>\n<\/div>","0f785f7e":"Decision trees can be used for classification as well as regression problems ( C A R T ). The name itself suggests that it uses a flowchart like a tree structure to show the predictions that result from a series of feature-based splits. It starts with a root node and ends with a decision made by leaves.\n\nProgramatically, Decision trees are nothing but a gaint structure of nested if - else condition.\n\nMathematically, Decisio trees use hyperplanes which run parallel to any one of the axes to cut your co-ordinate system into hyper cuboids.","a759ba81":"**Making a decision tree node using the feature which has the maximum Information Gain.**","8ded10f1":"![Untitled Workspace (2).jpg](attachment:f60313a4-95e0-4411-9794-5070e6b5aed8.jpg)","84b75bd7":"**Windy**","ec85b43f":"**Humidity**","77b55db7":"**Humidity**","ad973d46":"### **Outlook - Rainy**","a74242fa":"### **Outlook - Sunny**"}}