{"cell_type":{"7acd1b97":"code","8a6da6f0":"code","0e0df5c6":"code","ace32e86":"code","ef89f213":"code","b751a23e":"code","9ee95acb":"code","5179e5fa":"code","c9cd41cf":"code","5df3031a":"code","e63a933e":"code","9713266f":"code","191d8c10":"code","4e704ddb":"code","6d3bf07c":"code","e5dce8a6":"code","ea2d84e5":"code","5e76038d":"code","8addc9ce":"code","3a0705a1":"code","bf25b8b0":"code","8287d2f0":"code","def72566":"code","14b8f5d1":"code","f36a4741":"code","8ab90dc8":"code","ce7cda28":"code","0ef60855":"code","00d7e2b8":"code","b105d82c":"code","7d100192":"code","eb34337e":"code","f088cb9e":"code","24edbeb1":"code","ee977f73":"code","f85d9652":"code","f3e0de5b":"code","7fa2ef01":"code","a0fe8ea0":"code","9134d373":"code","cf6328c9":"code","bc62a797":"code","b5834e9c":"code","02d0e1d3":"code","e84e04c4":"code","ecff0163":"code","0b26115c":"code","f83c821c":"code","ae493062":"code","b330ee92":"code","10d925da":"code","4e39630b":"code","cc878b5d":"code","9c3ab8e0":"code","799faaa6":"code","9dc237e4":"code","c18db494":"code","3019c427":"code","74b5fa26":"code","dedcbf7f":"code","7bbd9ba6":"code","eb977219":"code","d510aa74":"code","89f38c97":"code","a9728274":"code","50e49b45":"code","c9a5d53c":"code","67276cb7":"code","7ffe6ec4":"code","210fd912":"code","364881ce":"code","ec7e26d3":"markdown","b8bf6be0":"markdown","2e3bf3e4":"markdown","0e86a632":"markdown","80058783":"markdown","dda0bcba":"markdown","208d6bbd":"markdown","7d03fa66":"markdown"},"source":{"7acd1b97":"#predicting if news is fake (0) or real (1) based on text using Machine Learning Algorithms\n#Random Forest Classifier\n#Gaussian Naive Bayes\n#Bernoulli Naive Bayes\n#Logistic Regression\n#XGBoost Classifier","8a6da6f0":"import numpy as np\nimport pandas as pd\nimport math \nimport matplotlib.pyplot as plt \nimport string\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","0e0df5c6":"fake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\ntrue = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","ace32e86":"fake.describe()","ef89f213":"true.describe()","b751a23e":"fake.head(5)","9ee95acb":"true.head(5)","5179e5fa":"fake.subject.unique()","c9cd41cf":"fake['subject'].value_counts().plot(kind='bar', figsize=(10,15))\n","5df3031a":"fake.isnull().sum()","e63a933e":"#some text is in date column, dropping the rows\nfake = fake.drop(columns = ['date'])","9713266f":"true.subject.unique()","191d8c10":"true['subject'].value_counts().plot(kind='bar', figsize=(10,15))","4e704ddb":"true.isnull().sum()","6d3bf07c":"true = true.drop(columns = ['date'])","e5dce8a6":"true.head(5)","ea2d84e5":"fake.head(5)","5e76038d":"#cleaning fake dataset title column\ntitles_fake = []\nlowered_fake = []\nfor i in fake.title:\n    \n    titles_fake.append(re.sub(r'[^\\w\\s]','',i))\n\nfor i in titles_fake:\n    lowered_fake.append(i.lower())","8addc9ce":"len(lowered_fake)","3a0705a1":"fake['title'] = lowered_fake","bf25b8b0":"#cleaning fake dataset text column\ntexts_fake = []\nlowered_text_fake = []\nfor i in fake.text:\n    \n    texts_fake.append(re.sub(r'[^\\w\\s]','',i))\n\nfor i in texts_fake:\n    lowered_text_fake.append(i.lower())","8287d2f0":"fake['text'] = lowered_text_fake","def72566":"fake.head()","14b8f5d1":"#adding column for type, FAKE = 0\nfor i in range(len(fake)):\n    fake['type'] = 0","f36a4741":"fake.head(5)","8ab90dc8":"#cleaning true dataset title column\ntitles_true = []\nlowered_true = []\nfor i in true.title:\n    \n    titles_true.append(re.sub(r'[^\\w\\s]','',i))\n\nfor i in titles_true:\n    lowered_true.append(i.lower())","ce7cda28":"true['title'] = lowered_true","0ef60855":"#removing city names from text   \ntrue['text_new'] = true['text'].str.split(')').str[1]","00d7e2b8":"true = true.drop(columns = ['text'])\ntrue = true.rename( columns = {'text_new': 'text'})","b105d82c":"##cleaning true dataset title column\ntext_true = []\nlowered_true = []\nfor i in true.text:\n    \n    text_true.append(re.sub(r'[^\\w\\s]','',str(i)))\n\nfor i in text_true:\n    lowered_true.append(i.lower())","7d100192":"true['text'] = lowered_true","eb34337e":"true.head(5)","f088cb9e":"#adding column for type TRUE = 1\nfor i in range(len(true)):\n    true['type'] = 1","24edbeb1":"true.head(5)","ee977f73":"len(true)","f85d9652":"len(fake)","f3e0de5b":"merged_data = pd.concat([fake, true], axis = 0)","7fa2ef01":"merged_data.head(5)","a0fe8ea0":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer","9134d373":"#shuffling merged data to balance type of news\nmerged_data = merged_data.sample(frac=1)","cf6328c9":"merged_data.head(5)","bc62a797":"x = merged_data\nx = x.drop(columns = ['type'])\ny = merged_data[['type']]","b5834e9c":"x.head(2)","02d0e1d3":"y.head(2)","e84e04c4":"import nltk \nfrom nltk.corpus import stopwords\n#using english library for stopwords and setting max_features to 1000 unique words\nvectorizer = CountVectorizer(max_features=1000, stop_words=stopwords.words('english'))\n\n#fitting and transforming on text column of dataset\nX = vectorizer.fit_transform(x['text']).toarray()","ecff0163":"#split train test models\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","0b26115c":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=1000, random_state=42)\n","f83c821c":"rfc.fit(x_train, y_train.values.ravel()) ","ae493062":"y_pred_rfc = rfc.predict(x_test)","b330ee92":"probs_rfc = rfc.predict_proba(x_test)[:,1] ","10d925da":"print('Confusin Matrix: ')\nprint(confusion_matrix(y_test,y_pred_rfc))\nprint('\\n')\nprint('Classification Report: ')\nprint('-------------------------------------------------')\nprint(classification_report(y_test,y_pred_rfc))\nprint('-------------------------------------------------')\nprint('\\n')\nprint('Accuracy is: ',accuracy_score(y_test, y_pred_rfc))","4e39630b":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()","cc878b5d":"y_pred_gnb = gnb.fit(x_train, y_train.values.ravel()).predict(x_test)","9c3ab8e0":"probs_gnb = gnb.predict_proba(x_test)[:,1] ","799faaa6":"print('Confusin Matrix: ')\nprint(confusion_matrix(y_test,y_pred_gnb))\nprint('\\n')\nprint('Classification Report: ')\nprint('-------------------------------------------------')\nprint(classification_report(y_test,y_pred_gnb))\nprint('-------------------------------------------------')\nprint('\\n')\nprint('Accuracy is: ',accuracy_score(y_test, y_pred_gnb))","9dc237e4":"from sklearn.naive_bayes import BernoulliNB\nbnb = BernoulliNB()","c18db494":"y_pred_bnb = bnb.fit(x_train, y_train.values.ravel()).predict(x_test)","3019c427":"probs_bnb = bnb.predict_proba(x_test)[:,1] ","74b5fa26":"print('Confusin Matrix: ')\nprint(confusion_matrix(y_test,y_pred_bnb))\nprint('\\n')\nprint('Classification Report: ')\nprint('-------------------------------------------------')\nprint(classification_report(y_test,y_pred_bnb))\nprint('-------------------------------------------------')\nprint('\\n')\nprint('Accuracy is: ',accuracy_score(y_test, y_pred_bnb))","dedcbf7f":"from sklearn.linear_model import LogisticRegression\nlgr = LogisticRegression(max_iter = 200)","7bbd9ba6":"y_pred_lgr = lgr.fit(x_train, y_train.values.ravel()).predict(x_test)","eb977219":"probs_lgr = lgr.predict_proba(x_test)[:,1] ","d510aa74":"print('Confusin Matrix: ')\nprint(confusion_matrix(y_test,y_pred_lgr))\nprint('\\n')\nprint('Classification Report: ')\nprint('-------------------------------------------------')\nprint(classification_report(y_test,y_pred_lgr))\nprint('-------------------------------------------------')\nprint('\\n')\nprint('Accuracy is: ',accuracy_score(y_test, y_pred_lgr))","89f38c97":"!pip install xgboost","a9728274":"from xgboost import XGBClassifier\nxgboost = XGBClassifier()","50e49b45":"y_pred_xgbc = xgboost.fit(x_train, y_train.values.ravel()).predict(x_test)","c9a5d53c":"probs_xgbc = xgboost.predict_proba(x_test)[:,1] ","67276cb7":"print('Confusin Matrix: ')\nprint(confusion_matrix(y_test,y_pred_xgbc))\nprint('\\n')\nprint('Classification Report: ')\nprint('-------------------------------------------------')\nprint(classification_report(y_test,y_pred_xgbc))\nprint('-------------------------------------------------')\nprint('\\n')\nprint('Accuracy is: ',accuracy_score(y_test, y_pred_xgbc))","7ffe6ec4":"from sklearn.metrics import roc_curve, roc_auc_score","210fd912":"#calculate AUC scores\n\nlgr_auc = roc_auc_score(y_test, probs_lgr)\ngnb_auc = roc_auc_score(y_test, probs_gnb)\nbnb_auc = roc_auc_score(y_test, probs_bnb)\nxgbc_auc = roc_auc_score(y_test, probs_xgbc)\nrfc_auc = roc_auc_score(y_test, probs_rfc)\n\n\n# summarize scores\nprint('Logistic: ROC AUC=%.3f' % (lgr_auc))\nprint('Guassian NB: ROC AUC=%.3f' % (gnb_auc))\nprint('Bernoulli NB: ROC AUC=%.3f' % (bnb_auc))\nprint('XGBoost: ROC AUC=%.3f' % (xgbc_auc))\nprint('Random Forest: ROC AUC=%.3f' % (rfc_auc))\n\n# calculate ROC curves\nlgr_fpr, lgr_tpr, _ = roc_curve(y_test, probs_lgr)\ngnb_fpr, gnb_tpr, _ = roc_curve(y_test, probs_gnb)\nbnb_fpr, bnb_tpr, _ = roc_curve(y_test, probs_bnb)\nxgbc_fpr, xgbc_tpr, _ = roc_curve(y_test, probs_xgbc)\nrfc_fpr, rfc_tpr, _ = roc_curve(y_test, probs_rfc)\n","364881ce":"lw =2\nplt.figure(figsize = (15,10))\nplt.plot(lgr_fpr, lgr_tpr, lw = lw, label='Logistic ROC curve (area = %0.2f)' % lgr_auc)\nplt.plot(gnb_fpr, gnb_tpr, lw = lw, label='Gaussian NB ROC curve (area = %0.2f)' % gnb_auc)\nplt.plot(bnb_fpr, bnb_tpr,  lw = lw, label='Bernoulli NB ROC curve (area = %0.2f)' % bnb_auc)\nplt.plot(xgbc_fpr, xgbc_tpr, lw=lw,  label='XGBoost ROC curve (area = %0.2f)' % xgbc_auc)\nplt.plot(rfc_fpr, rfc_tpr,  lw = lw, label='Random Forest ROC curve (area = %0.2f)' % rfc_auc)\nplt.plot([0, 1], [0, 1], color='navy',  linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","ec7e26d3":"# Merging Data","b8bf6be0":"## BERNOULLI NAIVE BAYES","2e3bf3e4":"# Cleaning Strings","0e86a632":"## ROC-AUC CURVE","80058783":"## XGBoost Classifier","dda0bcba":"## RANDOM FOREST CLASSIFIER","208d6bbd":"## LOGISTIC REGRESSION","7d03fa66":"# GUASSIAN NAIVE BAYES "}}