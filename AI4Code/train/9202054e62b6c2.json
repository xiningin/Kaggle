{"cell_type":{"2b270d36":"code","9be98fa6":"code","0bad04e9":"code","20ec9914":"code","22706846":"code","71a8ab8d":"code","3f237100":"code","ad101ba5":"code","22ca5865":"code","2ea19ee1":"code","cb955bdd":"code","06de9f4c":"code","36d3df19":"code","6b2a947a":"code","22a1739e":"code","22a7ceb2":"code","89c09ed9":"markdown","7005e3f9":"markdown","464fb136":"markdown","efe15d61":"markdown","8507b30e":"markdown","ece5859d":"markdown","c12395bb":"markdown","2674e3bd":"markdown"},"source":{"2b270d36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport matplotlib.pyplot as plt # import matplotlib\n%matplotlib inline\nimport seaborn as sns # seaborn data visualizer\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9be98fa6":"BASE_DIR = Path('..\/input\/pima-indians-diabetes-database')","0bad04e9":"df = pd.read_csv(BASE_DIR \/ 'diabetes.csv')\ndf.head(10)","20ec9914":"features_list = list(df.drop(columns='Outcome').columns)\ncolumns = list(df.columns)\nprint(features_list)","22706846":"isnull = df.isnull().sum()\nisnull","71a8ab8d":"dup = df.duplicated(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\ndup.value_counts()","3f237100":"df.drop_duplicates(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])","ad101ba5":"def standardize_var(x):\n    mean = np.mean(x)\n    std = np.sqrt(np.sum(np.square(x-mean))\/(len(x)-1))\n    return ((x-mean)\/std)\/np.sqrt(len(x)-1)\n \nsdf = df.apply(standardize_var) ## \ub370\uc774\ud130 \ud45c\uc900\ud654\nsdf_X = sdf[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\ncorr = np.array(sdf_X.corr()) ## \uc0c1\uad00\uacc4\uc218 \ud589\ub82c\ncorr_inv = np.linalg.inv(corr) ## \uc0c1\uad00\uacc4\uc218\ud589\ub82c\uc758 \uc5ed\ud589\ub82c\n \nfit = ols('Outcome~Pregnancies+Glucose+BloodPressure+SkinThickness+Insulin+BMI+DiabetesPedigreeFunction+Age',data=sdf).fit()\n \nvariables = [] ## \ubcc0\uc218 \uc774\ub984\nreg_coef = [] ## \ud574\ub2f9 \ubcc0\uc218\uc758 \ud68c\uadc0 \uacc4\uc218\nvif = [] ## \ud574\ub2f9 \ubcc0\uc218\uc758 \ubd84\uc0b0\ud33d\ucc3d\uc778\uc790\nfor i in range(len(sdf_X.columns)):\n    col_name = sdf_X.columns[i]\n    variables.append(col_name)\n    reg_coef.append(fit.params[col_name])\n    vif.append(corr_inv[i][i])\n    \ndf_res = pd.DataFrame()\ndf_res['Variable'] = variables\ndf_res['Estimate'] = reg_coef\ndf_res['VIF'] = vif\n\ndf_res","22ca5865":"colormap = plt.cm.PuBu \nplt.figure(figsize=(10, 8)) \nplt.title(\"Diabetes Correlation of Features\", y = 1.05, size = 15) \nsns.heatmap(df.astype(float).corr(), linewidths = 0.1, vmax = 1.0, \n            square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 16})","2ea19ee1":"model2 = ols('Outcome ~ Pregnancies+Glucose+BloodPressure+SkinThickness+Insulin+BMI+DiabetesPedigreeFunction+Age', df).fit()\ntable2 = sm.stats.anova_lm(model2, type=2)\ntable2","cb955bdd":"df2 = table2[table2['PR(>F)'] < 0.05]\ndf2","06de9f4c":"features = df[['Pregnancies', 'Glucose', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction']]\nY = df['Outcome']","36d3df19":"train_features, test_features, train_labels, test_labels = train_test_split(features, Y)","6b2a947a":"scaler = StandardScaler()\ntrain_features = scaler.fit_transform(train_features)\ntest_features = scaler.transform(test_features)","22a1739e":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(train_features, train_labels)","22a7ceb2":"print(model.score(train_features, train_labels))","89c09ed9":"We will first remove the variable by using the T-test.","7005e3f9":"# Logistic Regression\n**I wanted to use Backward Elimination, but gave up because the number of features was too small.**","464fb136":"Multicollinearity, which is the most problematic when performing LR, is checked.\n\nWhen multicollinearity exists, the explanatory power of the model decreases, and the model breaks when other variables are added.\n\nAfter checking multicollinearity, features with VIF value of 10 or higher are removed.\n\n\nThere are several methods to remove multicollinearity. The main methods are PCA and VIF. I will use VIF\n\nNote that you have to remove them one by one using 'loop'. Remove one and check the VIF value again.","efe15d61":"# T-test","8507b30e":"**Fortunately, there is no feature with a VIF greater than 10.**","ece5859d":"# Multicollinearity","c12395bb":"**Visualize and confirm the correlation between features. Although seemingly trivial, statistical analysis is a very important task.**\n\n\nIf you have time, draw a scatter plot between features as well.\n\nYou can get something out of a visualized graph.","2674e3bd":"# HEATMAP"}}