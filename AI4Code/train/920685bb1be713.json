{"cell_type":{"50fd09ed":"code","d5fff94f":"code","d879c007":"code","794ac4c1":"code","ef5ea29f":"code","6c7a9a67":"code","4ff45459":"code","9cb2a7e1":"code","f09ddccf":"code","da60471a":"code","0d5501d3":"code","bf4d6abb":"code","8443dfe8":"code","75a93b98":"code","a8e0e9b4":"code","3287d232":"code","dfaf6610":"code","3a769568":"code","de56a713":"code","bc2f213a":"code","c66af3ca":"code","587fe329":"code","2b12aa3d":"code","0987520a":"code","64d46f3e":"code","252b3be4":"code","bf70e430":"code","33d58ee4":"code","772e1a24":"code","5470345c":"code","4fefb4f5":"code","fe4dc700":"code","e7a349a8":"markdown","81ff6e3c":"markdown","d6f9e482":"markdown","292bcab8":"markdown","259b44e9":"markdown","b14651bb":"markdown","2b23af1a":"markdown","d91800f9":"markdown","516ae1ca":"markdown","54c42fc7":"markdown","4da8adc4":"markdown","2fbe298d":"markdown","5039340a":"markdown","b06b66a4":"markdown","1b82a7c7":"markdown","8768099c":"markdown","ad46d070":"markdown","52a55c0e":"markdown","aa75c09c":"markdown","b9b57ff3":"markdown","8eaf2cfc":"markdown","94fbc1f6":"markdown","11003854":"markdown","763de243":"markdown","5333ab65":"markdown","5211f5b5":"markdown"},"source":{"50fd09ed":"# !pip install 'kaggle-environments>=0.1.6'","d5fff94f":"from kaggle_environments import evaluate, make, utils\nfrom kaggle_environments import agent as KAgent\nfrom tqdm.notebook import tqdm\nfrom random import choice\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport gym\nimport inspect\nimport os\nimport sys","d879c007":"class connectX(gym.Env):\n    def __init__(self, switch_prob=0.5):\n        \n        # create environment\n        self.env = make('connectx', debug=True)\n        \n        # create opponent\n        self.pair = [None, 'negamax']\n        self.trainer = self.env.train(self.pair)\n        \n        self.switch_prob = switch_prob\n        \n        # initialize action space and observation space\n        config = self.env.configuration\n        \n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n    \n    # to switch trainer\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n    \n    # do the action against trainer\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    # reset trainer\n    def reset(self):\n        if random.uniform(0, 1) < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    # render environment\n    def render(self, **kwargs):\n        return self.env.render(**kwargs)","794ac4c1":"# create environment \nenv = connectX()","ef5ea29f":"state = env.reset()","6c7a9a67":"type(state)","4ff45459":"class model(nn.Module):\n    def __init__(self, num_states, hidden_units, num_actions):\n        super(model, self).__init__()\n        \n        # initialize hidden layers\n        self.hidden_layers = nn.ModuleList([])\n        \n        # add more layers \n        for i in range(len(hidden_units)):\n            if i == 0:\n                self.hidden_layers.append(nn.Linear(num_states, hidden_units[i]))\n            else:\n                self.hidden_layers.append(nn.Linear(hidden_units[i-1], hidden_units[i]))\n        \n        # initialize output layer\n        self.output_layer = nn.Linear(hidden_units[-1], num_actions)\n    \n    # create forward function\n    def forward(self, x):\n        # pass the input through hidden layers\n        for layer in self.hidden_layers:\n            x = torch.sigmoid(layer(x))\n        \n        # pass through output layer\n        x = self.output_layer(x)\n        \n        return x","9cb2a7e1":"class DQN:\n    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n        # initialize hyperparameters\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.gamma = gamma\n        \n        # initialize the agent model\n        self.model = model(num_states, hidden_units, num_actions)\n        \n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n        \n        # create the experience replay buffer\n        self.experience = {'s':[], 'a':[], 'r':[], 's2':[], 'done':[]}\n        \n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n        \n    # predict the q values for different action using model\n    def predict(self, inputs):\n        return self.model(torch.from_numpy(inputs).float())\n    \n    # create method to train the model\n    def train(self, targetNet):\n        \n        # check if we have min experience in the buffer\n        if len(self.experience['s']) < self.min_experiences:\n            return 0\n        \n        # select random indices\n        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n        \n        # get batch of states, action, rewards\n        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n        actions = np.asarray([self.experience['a'][i] for i in ids])\n        rewards = np.asarray([self.experience['r'][i] for i in ids])\n        \n        # get next states values\n        states_next = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids])\n        dones = np.asarray([self.experience['done'][i] for i in ids])\n        value_next = np.max(TargetNet.predict(states_next).detach().numpy(), axis=1)\n        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n        \n        # one hot encoding for actions\n        actions = np.expand_dims(actions, axis=1)\n        actions_one_hot = torch.FloatTensor(self.batch_size, self.num_actions).zero_()\n        actions_one_hot = actions_one_hot.scatter_(1, torch.LongTensor(actions), 1)\n        \n        # get the q values for each (state, action)\n        selected_action_values = torch.sum(self.predict(states) * actions_one_hot, dim=1)\n        actual_values = torch.FloatTensor(actual_values)\n        \n        # change the weights\n        self.optimizer.zero_grad()\n        \n        loss = self.criterion(selected_action_values, actual_values)\n        loss.backward()\n        \n        self.optimizer.step()\n        \n    # choose action using epsilon-greedy method\n    def get_action(self, state, epsilon):\n        \n        # choose random action\n        if np.random.random() < epsilon:\n            return int(np.random.choice([c for c in range(self.num_actions) if state['board'][c] == 0]))\n        \n        # return best action\n        else:\n            prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].detach().numpy()\n            \n            # iterate all possible actions\n            for i in range(self.num_actions):\n                if state['board'][i] != 0:\n                    prediction[i] = -1e7\n            \n            return int(np.argmax(prediction))\n    \n    # define method to add experience to buffer\n    def add_experience(self, exp):\n        \n        # if have required experiences\n        if len(self.experience['s']) >= self.max_experiences:\n            \n            # remove last values \n            for key in self.experience.keys():\n                self.experience[key].pop(0)\n        \n        for key, value in exp.items():\n            self.experience[key].append(value)\n    \n    # extra functions\n    def copy_weights(self, TrainNet):\n        self.model.load_state_dict(TrainNet.state_dict())\n\n    def save_weights(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load_weights(self, path):\n        self.model.load_state_dict(torch.load(path))\n    \n    # Each state will consist of the board and the mark\n    # in the observations\n    def preprocess(self, state):\n        result = state['board'][:]\n        result.append(state.mark)\n\n        return result","f09ddccf":"def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n    rewards = 0\n    iter = 0\n    done = False\n    observations = env.reset()\n    while not done:\n        # Using epsilon-greedy to get an action\n        action = TrainNet.get_action(observations, epsilon)\n\n        # Caching the information of current state\n        prev_observations = observations\n\n        # Take action\n        observations, reward, done, _ = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else:           # Draw\n                reward = 10\n        else:\n#             reward = -0.05 # Try to prevent the agent from taking a long move\n            reward = 0.5\n\n        rewards += reward\n\n        # Adding experience into buffer\n        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n        TrainNet.add_experience(exp)\n\n        # Train the training model by using experiences in buffer and the target model\n        TrainNet.train(TargetNet)\n        iter += 1\n        if iter % copy_step == 0:\n            # Update the weights of the target model when reaching enough \"copy step\"\n            TargetNet.copy_weights(TrainNet)\n    \n    return rewards","da60471a":"gamma = 0.99\ncopy_step = 25\nhidden_units = [128, 128, 128, 128, 128]   # no of hidden layers\n# hidden layers = 5\nmax_experiences = 10000\nmin_experiences = 100\nbatch_size = 32\nlr = 1e-2\nepsilon = 0.5\ndecay = 0.9999\nmin_epsilon = 0.1\nepisodes = 20000\n\nprecision = 7","0d5501d3":"# number of possible states and actions\nnum_states = env.observation_space.n + 1\nnum_actions = env.action_space.n\n\n# create empty array to store results\nall_total_rewards = np.empty(episodes)\nall_avg_rewards = np.empty(episodes) # Last 100 steps\nall_epsilons = np.empty(episodes)","bf4d6abb":"# initialize the models\nTrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\nTargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)","8443dfe8":"for i in tqdm(range(episodes)):\n    # get a epsilon value\n    epsilon = max(min_epsilon, epsilon * decay)\n    \n    # train the training model using target model (for one episode)\n    total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n    \n    # append the results\n    all_total_rewards[i] = total_reward\n    avg_reward = all_total_rewards[max(0, i - 100):(i + 1)].mean()\n    all_avg_rewards[i] = avg_reward\n    all_epsilons[i] = epsilon","75a93b98":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","a8e0e9b4":"plt.plot(all_epsilons)\nplt.xlabel('Episode')\nplt.ylabel('Epsilon')\nplt.show()","3287d232":"# save the weights\n\nTrainNet.save_weights('\/weights.pth')","dfaf6610":"model_layers = []","3a769568":"# for each hidden layer\nfor i in range(len(hidden_units)):\n    model_layers.extend([\n        \n        # add the weights and biases\n        TrainNet.model.hidden_layers[i].weight.T.tolist(),  \n        TrainNet.model.hidden_layers[i].bias.tolist()\n    ])","de56a713":"# output layer\nmodel_layers.extend([\n    \n    # add the weight and bias\n    TrainNet.model.output_layer.weight.T.tolist(),\n    TrainNet.model.output_layer.bias.tolist()\n])","bc2f213a":"# reshape the model\nmodel_layers = np.reshape(model_layers, (-1, 2))","c66af3ca":"def my_agent(observation, configuration):\n    \n    # create list of hidden and output layers\n    hl_w = [] * len(model_layers)  # n hidden layers\n    hl_b = [] * len(model_layers)\n    ol_w = []    # 1 output layer\n    ol_b = []\n    \n    # add hidden layers's weights and biases\n    for i, (w, b) in enumerate(model_layers[:-1]):\n        hl_w.append(np.array(w, dtype=np.float32))\n        hl_b.append(np.array(b, dtype=np.float32))\n    \n    # add output layer's weights and biases\n    ol_w = np.array(model_layers[-1][0], dtype=np.float32)\n    ol_b = np.array(model_layers[-1][1], dtype=np.float32)\n\n    # get current state of environment\n    # board \n    state = observation['board'][:]\n    state.append(observation.mark)\n    \n    # create result array\n    res = np.array(state, dtype=np.float32)\n    \n    # for each hidden layer\n#     for i in range(model_layers[:-1]):  # use the enumerate method\n    for i, (w, b) in enumerate(model_layers[:-1]):\n        # add weights and biases \n        res = np.matmul(res, hl_w[i]) + hl_b[i]\n        \n        # apply sigmoid function\n        res = 1 \/ (1 + np.exp(-res))\n    \n    # add weights and biases of output layer\n    res = np.matmul(res, ol_w) + ol_b\n    \n    # for unfilled columns set to min\n    for i in range(configuration.columns):\n        if observation['board'][i] != 0:\n            res[i] = 1e-7\n    \n    # return best action\n    return int(np.argmax(res))","587fe329":"# reset environment\nenv.reset()\n\n# get the opponent\ntrainer = env.trainer\n\n# get starting configurations\nobservation = trainer.reset()\nconfiguration = env.env.configuration","2b12aa3d":"done = False\n\n# while episode is not finished\nwhile not done:\n    my_action = my_agent(observation, env.env.configuration)\n    print(\"My Action\", my_action)\n    \n    # keep playing\n    observation, reward, done, info = trainer.step(my_action)\n\nenv.render(mode=\"ipython\")","0987520a":"env.reset()\n\n# play\nenv.env.run([my_agent, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","64d46f3e":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ float(len(rewards))","252b3be4":"# run multiple episodes\nprint(\"My agent vs random agent: \", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))","bf70e430":"print(\"My agent vs Negamax agent: \", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","33d58ee4":"# create agent\n\nmy_agent = '''def my_agent(observation, configuration):\n    # import required libraries\n    import numpy as np\n    \n'''\n    \n# NOTE - NO INTENDS\n# create list of hidden and output layers\n\n# add hidden layers's weights and biases\nfor i, (w, b) in enumerate(model_layers[:-1]):\n        \n    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n    my_agent +=  '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n    \n# add output layer's weights and biases\nmy_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(model_layers[-1][0])\nmy_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(model_layers[-1][1])\n\n# get current state of environment\n# board \nmy_agent += '''\n    state = observation['board'][:]\n    state.append(observation.mark)\n    \n    # create result array\n    res = np.array(state, dtype=np.float32)\n    \n'''\n    \n    \n    \n# for each hidden layer\n#     for i in range(model_layers[:-1]):  # use the enumerate method\nfor i, (w, b) in enumerate(model_layers[:-1]):\n    # add weights and biases \n    my_agent += '    res = np.matmul(res, hl{0}_w) + hl{0}_b \\n'.format(i+1)\n        \n    # apply sigmoid function\n    my_agent += '    res = 1 \/ (1 + np.exp(-res)) \\n'\n    \n# add weights and biases of output layer\nmy_agent += '    res = np.matmul(res, ol_w) + ol_b\\n'\n    \nmy_agent += '''\n    # for unfilled columns set to min\n    for i in range(configuration.columns):\n        if observation['board'][i] != 0:\n            res[i] = 1e-7\n    \n    # return best action\n    return int(np.argmax(res)) \n    '''","772e1a24":"\n# save our agent in a python file\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)","5470345c":"import sys\nfrom kaggle_environments import agent\n\nout = sys.stdout\nsubmission = utils.read_file(\"\/kaggle\/working\/submission.py\")\na = agent.get_last_callable(submission, path=submission)\nsys.stdout = out","4fefb4f5":"out = sys.stdout\n\n# read the file\nsubmission = utils.read_file(\"\/kaggle\/working\/submission.py\")\n\n# get the agent\nagent = KAgent.get_last_callable(submission, path=submission)\nsys.stdout = out","fe4dc700":"# play aginst itself\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\n\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","e7a349a8":"Since we will use the trained weights for creating an agent. Hence we have to save the weights for future use.","81ff6e3c":"**Define hyperparameters**","d6f9e482":"**DQN Class**","292bcab8":"## Create Environment","259b44e9":"We will use the Deep Q-learning method. Create a Neural network model and we will train the weights to choose correct action.","b14651bb":"Create an envrionment class to do all the things:-\n- reset\n- render\n- step\n- initialize","2b23af1a":"Play against itself.","d91800f9":"We have to write it as a string because the dictionary will not able to save.","516ae1ca":"Import our agent in the way it will be used by the tester.","54c42fc7":"## Create an Agent","4da8adc4":"## Write Submission File","2fbe298d":"**Play games Class**","5039340a":"Since we are using the Double DQn algorithm, hence we will create a training network and a target network.","b06b66a4":"**Against random**","1b82a7c7":"**Deep Learning Model**","8768099c":"## Debug\/Train Agent","ad46d070":"**Against negamax**","52a55c0e":"## Evaluate Agent","aa75c09c":"# Doule DQN\n\n## Installs & Imports","b9b57ff3":"## Test Agent","8eaf2cfc":"## Validate Submission","94fbc1f6":"**Training**","11003854":"Extract different layers weights and biases from network to store them in a list.","763de243":"Now, create an agent that will use the above calculated models (or the trained model weights and biases).","5333ab65":"Train the model for episodes. We will use the trained weights for our submission agent.\n\nUsing Double DQN algorithm.","5211f5b5":"**Plots**\n\nCreate some plots regarding the training work."}}