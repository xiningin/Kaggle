{"cell_type":{"2cb5f0f0":"code","b1bde4c6":"code","e21fe630":"code","d2963d57":"code","d5e1698c":"code","de5ca72d":"code","1919efc1":"code","8ad01b81":"code","c098a8a4":"code","3ca36260":"code","362391f4":"code","b93acd7f":"code","54380fb1":"code","86ec9f12":"code","c984b1eb":"code","bc217955":"code","f6b3935c":"code","c984851d":"code","4ad0dac2":"code","1bc81db1":"code","417a4d1b":"code","a7ed5841":"code","79c36d7e":"code","e89151af":"code","64a308b5":"code","5d936735":"code","473b4df5":"code","75d339db":"code","c4dade2e":"code","74ab5ed8":"code","49ef406f":"code","5780e297":"code","6571b91f":"code","938c9f76":"code","0a7aa295":"code","3a22b6dd":"code","527137a8":"code","8dd99a30":"code","c44f96a7":"code","db76e023":"code","a2ec5f03":"code","d2c0a550":"code","e0bafbb1":"markdown","df6dfca6":"markdown","b63fd590":"markdown","e6a0536a":"markdown","bdc0f840":"markdown","ee177558":"markdown","4cbe4c33":"markdown","b3faa6d0":"markdown","6bc00778":"markdown","7e132eca":"markdown","dbfbda0d":"markdown","47af2e2f":"markdown","deb43ab1":"markdown","daa619d1":"markdown","d9ce035f":"markdown","400322cd":"markdown","e6c9f81a":"markdown","69287b57":"markdown","766797af":"markdown","9cea5c83":"markdown","64e20be5":"markdown","9d68d55e":"markdown"},"source":{"2cb5f0f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1bde4c6":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport missingno as msno\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, precision_recall_curve\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dense\n#from sklearn.model_selection import GridSearchCV\n#from sklearn.model_selection import RandomizedSearchCV\nfrom kerastuner.tuners import RandomSearch\n","e21fe630":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","d2963d57":"df.head()","d5e1698c":"df.shape","de5ca72d":"df.info()","1919efc1":"df.describe()","8ad01b81":"df.isna().mean()","c098a8a4":"msno.bar(df)","3ca36260":"#plt.rcParams['figure.figsize'] = (20.0, 10.0)\n#plt.rcParams['font.family'] = \"serif\"\n#plt.rcParams['font.family'] = \"cursive\"\nbg_color = (.23,.34,.29)\nsns.set(rc={\"font.style\":\"normal\",\n            \"axes.facecolor\":bg_color,\n            \"figure.facecolor\":bg_color,\n            \"text.color\":\"white\",\n            \"xtick.color\":\"white\",\n            \"ytick.color\":\"white\",\n            \"axes.labelcolor\":\"white\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(20.0, 10.0),\n            'xtick.labelsize':25,\n            'ytick.labelsize':20})\n\n\nf, axes = plt.subplots(2,4, figsize=(30,25))\n\nax = sns.countplot(data=df,\n                  x = 'gender',\n                  hue = 'stroke',\n             \n                  edgecolor=(0,0,0),\n                  linewidth=5,\n                  palette=\"prism\",ax=axes[0,0]\n                  )\nax.set_xticklabels(ax.get_xticklabels(),rotation=30)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width()  \/ 2,rect.get_height()+ 0.25,rect.get_height(),horizontalalignment='center', fontsize = 20.0)\n\nax1 = sns.countplot(data=df,\n                  x = 'hypertension',\n                  hue = 'stroke',\n             \n                  edgecolor=(0,0,0),\n                  linewidth=5,\n                  palette=\"cool\",ax=axes[0,1]\n                  )\nfor rect in ax1.patches:\n    ax1.text (rect.get_x() + rect.get_width()  \/ 2,rect.get_height()+ 0.25,rect.get_height(),horizontalalignment='center', fontsize = 20.0)\n\nax2 = sns.countplot(data=df,\n                  x = 'heart_disease',\n                  hue = 'stroke',\n             \n                  edgecolor=(0,0,0),\n                  linewidth=5,\n                  palette=\"Accent\",ax=axes[0,2]\n                  )\nfor rect in ax2.patches:\n    ax2.text (rect.get_x() + rect.get_width()  \/ 2,rect.get_height()+ 0.25,rect.get_height(),horizontalalignment='center', fontsize = 20.0)\n   \nax3 = sns.countplot(data=df,\n                  x = 'ever_married',\n                  hue = 'stroke',\n             \n                  edgecolor=(0,0,0),\n                  linewidth=5,\n                  palette=\"Accent_r\",ax=axes[0,3]\n                  )\nfor rect in ax3.patches:\n    ax3.text (rect.get_x() + rect.get_width()  \/ 2,rect.get_height()+ 0.25,rect.get_height(),horizontalalignment='center', fontsize = 20.0)\n   \n\nax4 = sns.countplot(data=df,\n                  x = 'work_type',\n                  hue = 'stroke',\n             \n                  edgecolor=(0,0,0),\n                  linewidth=5,\n                  palette=\"twilight_shifted_r\",ax=axes[1,0]\n                  )\nax4.set_xticklabels(ax4.get_xticklabels(),rotation=30)\nfor rect in ax4.patches:\n    ax4.text (rect.get_x() + rect.get_width()  \/ 2,rect.get_height()+ 0.25,rect.get_height(),horizontalalignment='center', fontsize = 20.0)\n\nax5 = sns.countplot(data=df,\n                  x = 'Residence_type',\n                  hue = 'stroke',\n             \n                  edgecolor=(0,0,0),\n                  linewidth=5,\n                  palette=\"twilight_shifted\",ax=axes[1,1]\n                  )\nfor rect in ax5.patches:\n    ax5.text (rect.get_x() + rect.get_width()  \/ 2,rect.get_height()+ 0.25,rect.get_height(),horizontalalignment='center', fontsize = 20.0)\n\nax6 = sns.countplot(data=df,\n                  x = 'smoking_status',\n                  hue = 'stroke',\n             \n                  edgecolor=(0,0,0),\n                  linewidth=5,\n                  palette=\"Accent_r\",ax=axes[1,2]\n                  )\nax6.set_xticklabels(ax6.get_xticklabels(),rotation=30)\nfor rect in ax6.patches:\n    ax6.text (rect.get_x() + rect.get_width()  \/ 2,rect.get_height()+ 0.25,rect.get_height(),horizontalalignment='center', fontsize = 20.0)\n   \nf.suptitle('Features distribution based on Stroke ',fontsize=40)","362391f4":"#Let's analyze the Numerical Columns.\n\nsns.set_style(style = 'darkgrid')\nf, axes = plt.subplots(1,3, figsize = (15,5))\n\nsns.kdeplot(df['age'],shade = True ,ax=axes[0])\nsns.kdeplot(df['avg_glucose_level'],shade = True ,ax=axes[1])\nsns.kdeplot(df['bmi'],shade = True ,ax=axes[2])\nf.suptitle('Numerical Features distribution',fontsize=40)\n\n","b93acd7f":"g = sns.kdeplot(df['age'][(df[\"stroke\"] == 0)] , color=\"Red\", shade = True)\ng = sns.kdeplot(df['age'][(df[\"stroke\"] == 1)], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('Age Distribution')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"No,\",\"Yes\"],fontsize = 30)","54380fb1":"g = sns.kdeplot(df['avg_glucose_level'][(df[\"stroke\"] == 0)] , color=\"Red\", shade = True)\ng = sns.kdeplot(df['avg_glucose_level'][(df[\"stroke\"] == 1)], ax =g, color=\"Green\", shade= True)\ng.set_xlabel('Avg_glucose_level Distribution')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"No,\",\"Yes\"],fontsize = 30)","86ec9f12":"sns.set_palette(palette = 'YlGn')\ng = sns.kdeplot(df['bmi'][(df[\"stroke\"] == 0)] , color=\"Red\", shade = True)\ng = sns.kdeplot(df['bmi'][(df[\"stroke\"] == 1)], ax =g, color=\"Yellow\", shade= True)\ng.set_xlabel('BMI Distribution')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"No,\",\"Yes\"],fontsize = 30)","c984b1eb":"plt.Figure(figsize = (10,15))\ng = sns.FacetGrid(df, col=\"stroke\",height=8, aspect=1,palette = 'blue', sharey = False)\ng.map(sns.scatterplot, \"age\", \"bmi\")","bc217955":"plt.Figure(figsize = (10,15))\ng = sns.FacetGrid(df, col=\"stroke\",height=8, aspect=1,palette = 'BuPu', sharey = False)\ng.map(sns.scatterplot, \"age\", \"avg_glucose_level\")","f6b3935c":"\nplt.Figure(figsize = (10,15))\ng = sns.FacetGrid(df, col=\"stroke\",hue = 'stroke', height=8, aspect=1,palette = 'BuPu', sharey = False)\ng.map(sns.scatterplot, \"bmi\", \"avg_glucose_level\")","c984851d":"df.dropna(inplace = True)","4ad0dac2":"df = pd.get_dummies(df,\n                    columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'],\n                    drop_first=True)","1bc81db1":"X = df.drop(['stroke'], axis=1)\ny = df['stroke']","417a4d1b":"from sklearn.model_selection import train_test_split","a7ed5841":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=42)","79c36d7e":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using SMOTE\nsmt = SMOTE()\n#X_train, y_train = smt.fit_resample(X_train, y_train)\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_sm)\nprint('After',counter)","e89151af":"#ADASYN\nfrom imblearn.over_sampling import ADASYN\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using ADASYN\nada = ADASYN(random_state=130)\nX_train_ada, y_train_ada = ada.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_ada)\nprint('After',counter)\n","64a308b5":"#SMOTE + TOMEK\nfrom imblearn.combine import SMOTETomek\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using SMOTE + Tomek\nsmtom = SMOTETomek(random_state=139)\nX_train_smtom, y_train_smtom = smtom.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smtom)\nprint('After',counter)\n","5d936735":"# SMOTE+ ENN\nfrom imblearn.combine import SMOTEENN\n\ncounter = Counter(y_train)\nprint('Before',counter)\n#oversampling the train dataset using SMOTE + ENN\nsmenn = SMOTEENN()\nX_train_smenn, y_train_smenn = smenn.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smenn)\nprint('After',counter)","473b4df5":"#SMOTE + UNDER SAMPLING\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nsm = SMOTE(sampling_strategy = .3)\nrus =  RandomUnderSampler(sampling_strategy=.4)\n\npipeline = Pipeline(steps = [('smote', sm),('under',rus)])\n\ncounter = Counter(y_train)\nprint('Before',counter)\n#over and undersampling the train dataset using SMOTE + RandomUnderSampler\nX_train_smrus, y_train_smrus = pipeline.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smrus)\nprint('After',counter)","75d339db":"sampled_data = {\n    'ACTUAL':[X_train, y_train],\n    'SMOTE':[X_train_sm, y_train_sm],\n    'ADASYN':[X_train_ada, y_train_ada],\n    'SMOTE_TOMEK':[X_train_smtom, y_train_smtom],\n    'SMOTE_ENN':[X_train_smenn, y_train_smenn],\n    'UNDERSAMPLING': [X_train_smrus, y_train_smrus]\n}","c4dade2e":"def test_eval(clf_model, X_test, y_test, algo=None, sampling=None):\n    # Test set prediction\n    y_prob=clf_model.predict_proba(X_test)\n    y_pred=clf_model.predict(X_test)\n\n    print('Confusion Matrix')\n    print('='*60)\n    #plot_confusion_matrix(clf_model, X_test, y_test)  \n    #plt.show() \n    print(confusion_matrix(y_test,y_pred),\"\\n\")\n    print('Classification Report')\n    print('='*60)\n    print(classification_report(y_test,y_pred),\"\\n\")\n    print('AUC-ROC')\n    print('='*60)\n    print(roc_auc_score(y_test, y_prob[:,1]))\n    \n    #x = roc_auc_score(y_test, y_prob[:,1])\n    f1 = f1_score(y_test, y_pred, average='binary')\n    recall = recall_score(y_test, y_pred, average='binary')\n    precision = precision_score(y_test, y_pred, average='binary')\n          \n    \n    return algo,precision,recall,f1,sampling\n    ","74ab5ed8":"model_params = {\n\n    'random-forest' : {\n        'model' : RandomForestClassifier(),\n        'params' : {\n             \"n_estimators\": [5, 10, 15, 20, 25], \n             'max_depth': [i for i in range(5,16,2)],\n             'min_samples_split': [2, 5, 10, 15, 20, 50, 100],\n             'min_samples_leaf': [1, 2, 5],\n             'criterion': ['gini', 'entropy'],\n             'max_features': ['log2', 'sqrt', 'auto']\n\n         }\n    },\n    'logisticregression': {\n        'model' : LogisticRegression(),\n        'params' : {\n            'C' : [1,6,10],\n            'penalty': ['l1', 'l2']\n        }\n    },\n    'decision_tree' :{\n        'model' :  DecisionTreeClassifier(),\n        'params' : {\n             'max_depth': [i for i in range(5,16,2)],\n             'min_samples_split': [2, 5, 10, 15, 20, 50, 100],\n             'min_samples_leaf': [1, 2, 5],\n             'criterion': ['gini', 'entropy'],\n             'max_features': ['log2', 'sqrt', 'auto']\n        }\n        \n    }\n    \n}","49ef406f":"cv = StratifiedKFold(n_splits=5, random_state=100, shuffle=True)\noutput = []\nfor model , model_hp in model_params.items():\n    for resam , data in sampled_data.items():\n        clf = RandomizedSearchCV(model_hp['model'], model_hp['params'],cv = cv, scoring='roc_auc', n_jobs=-1 )\n        clf.fit(data[0], data[1])\n        clf_best = clf.best_estimator_\n       \n        print(model+' with ' + resam)\n        print('='*60)\n        output.append(test_eval(clf_best, X_test, y_test, model, resam))","5780e297":"def build_model(hp):\n    model = keras.models.Sequential()\n    \n    model.add(layers.Dense(units=hp.Int('Input_units',min_value = 6, max_value = 12),\n                    activation =hp.Choice('input_act', ['relu', 'sigmoid']), input_dim = 17))\n    \n    for i in range(hp.Int('num_layers', 2, 20)):\n        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n                                            min_value=32,\n                                            max_value=512,\n                                            step=32),\n                               activation=hp.Choice('act_' + str(i), ['relu', 'sigmoid'])))\n        \n    model.add(layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n        loss='binary_crossentropy',\n         metrics='accuracy')\n    return model","6571b91f":"tuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3,\n    directory='my_dir2',\n    project_name='StrokePrediction')","938c9f76":"def train_test(X_train,y_train,X_test, y_test,algo,sampling = None ):\n    tuner.search(X_train, y_train,\n                 epochs=5,\n                 validation_data=(X_test, y_test))\n    best_hp = tuner.get_best_hyperparameters()[0]\n    model = tuner.hypermodel.build(best_hp)\n    model.fit(X_train, y_train, epochs = 5, validation_split = .1 )\n    y_p = model.predict(X_test)\n    y_pred = []\n    for element in y_p:\n        if element > .5:\n            y_pred.append(1)\n        else:\n            y_pred.append(0)\n\n    f1 = f1_score(y_test, y_pred, average='binary')\n    recall = recall_score(y_test, y_pred, average='binary')\n    precision = precision_score(y_test, y_pred, average='binary')\n\n    print(classification_report(y_test,y_pred),\"\\n\")\n    \n    return algo,precision,recall,f1,sampling","0a7aa295":"output.append(train_test(X_train, y_train, X_test, y_test, 'ANN', 'ACTUAL'))","3a22b6dd":"output.append(train_test(X_train_sm, y_train_sm, X_test, y_test, 'ANN', 'SMOTE'))","527137a8":"output.append(train_test(X_train_ada, y_train_ada, X_test, y_test, 'ANN', 'SMOTE'))","8dd99a30":"output.append(train_test(X_train_smtom, y_train_smtom, X_test, y_test, 'ANN', 'SMOTE_TOMEK'))","c44f96a7":"output.append(train_test(X_train_smenn, y_train_smenn, X_test, y_test, 'ANN', 'SMOTE_ENN'))","db76e023":"output.append(train_test(X_train_smrus, y_train_smrus, X_test, y_test, 'ANN', 'UNDERSAMPLING'))","a2ec5f03":"result = pd.DataFrame(output, columns = ['Model','Precision','Recall', 'F1-score','Resample'])","d2c0a550":"result","e0bafbb1":"Age. Stroke risk increases with age. Three-quarters of strokes occur in people ages 65 and older.\n\nGeography. The highest U.S. death rates from stroke occur in the southeastern United States.\n\nGender. Men are more likely than women to have a stroke.\n\nCertain lifestyle factors and conditions also increase the risk for stroke. The most important of these include:\n\nHigh blood pressure\n\nDiabetes\n\nHeart disease (such as atrial fibrillation)\n\nPrevious stroke or transient ischemic attack\n\nCigarette smoking\n\nAdditional risk factors include:\n\nPhysical inactivity\n\nOverweight or obesity\n\nHigh cholesterol\n\nSickle cell disease\n\nDrinking too much alcohol\n\nFamily history of stroke\n\nDrug abuse\n\nGenetic conditions, such as blood-clotting or vascular disorders (for example, Factor V Leiden or CADASIL)\n\nCertain medications (such as hormonal birth control pills)\n\nBeing pregnant\n\nMenopause\n\nLesser risk factors include:\n\nHead and neck injuries\n\nRecent viral or bacterial infections","df6dfca6":"# ACTUALL:","b63fd590":"# Objective:\n\nDon\u2019t deprive me learning from your valuable feedback.Advance thanks for your time.","e6a0536a":"# ADASYN:","bdc0f840":"Observations:\n    \n1.People do have stroke mostly are Old and ranging from around 60 to 100 ","ee177558":"Let's try it with neural network ","4cbe4c33":"# SMOTE + TOMEK:","b3faa6d0":"Here we are seeing that 7 Categorical features.So let's start the EDA with those features.\n\n","6bc00778":"# Let's Understand the problem","7e132eca":"# Model Building","dbfbda0d":"# SMOTE:","47af2e2f":"# UNDER_SAMPLING:","deb43ab1":"Work in progress.","daa619d1":"# Handling imbalance data using SMOTE based techniques\n","d9ce035f":"\nA stroke occurs when part of the brain loses its blood supply and stops working. This causes the part of the body that the injured brain controls to stop working.\nA stroke also is called a cerebrovascular accident, CVA, or \"brain attack.\"\n\nThe types of strokes include:\n\nIschemic stroke (part of the brain loses blood flow)\n\nHemorrhagic stroke (bleeding occurs within the brain)\n\nTransient ischemic attack, TIA, or mini-stroke (The stroke symptoms resolve within minutes, but may take up to 24 hours on their own without treatment. This is a warning sign that a stroke may occur in the near future.)\n\n","400322cd":"# Causes of Stroke:","e6c9f81a":"Make Observations:\n    \n    Gender:\n        \n1. Male = (108+2007) = .054 and Female = (141\/2853) = .045 \n\n2. Male has more porne to Stroke\n\n    Hypertension:\n        \n1. No =((183\/4429)) =.0413 and Yes = 66\/432 = .152\n\n2. This clearly shows that people with hypertension are more porne to stroke.054\n\n    Heart Disease:\n        \n1. No = ((202\/4632)) = .044 and Yes = (47.\/229) = .20\n\n2. This also shows people having heart disease are more porne to stroke.054\n\n    Married:\n        \n1. Yes = (220\/3133) = .07 NO = (29\/1728) = 0.016\n\n2. It indaicates Unmarried people are safer then then married.","69287b57":"Obserbations:\n    \n1. People have stroke having avg_glucose_level around 100 and 200.\n\n2. People  don't have stroke having avg_glucose_level around 75.\n","766797af":"surprisingly in every Smote technique\nin ANN i got alomost same type of\nresult.Can anyone help me \nin fixing this problem. ","9cea5c83":"# Feature Engineering","64e20be5":"Here we cann't compare category of each features among them, because they are not same in number.So we have to calculate the ratio.\n\n","9d68d55e":"# SMOTE + ENN:"}}