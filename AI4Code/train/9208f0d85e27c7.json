{"cell_type":{"946aff76":"code","74e13eea":"code","afea991c":"code","598351a2":"code","e9c26b0e":"code","bb989254":"code","05b4dc1a":"code","2302a4de":"code","77ea8ec0":"code","8e618db1":"code","749f07c5":"code","ca94d2e3":"code","05804301":"code","e5b72af5":"code","44342f76":"code","ac151982":"code","0ebd0b4d":"markdown","c1bd359b":"markdown"},"source":{"946aff76":"import os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport random\nimport math\nimport warnings\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","74e13eea":"# Set some parameters\nIMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNELS = 3\nTRAIN_PATH = '..\/input\/stage1_train\/'\nTEST_PATH = '..\/input\/stage1_test\/'\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","afea991c":"# Get train and test IDs\ntrain_ids = next(os.walk(TRAIN_PATH))[1]\ntest_ids = next(os.walk(TEST_PATH))[1]","598351a2":"images = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nimages","e9c26b0e":"# Get and resize train images and masks\nimages = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nlabels = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\nprint('Getting and resizing train images and masks ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_\n    img = imread(path + '\/images\/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    images[n] = img\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n    for mask_file in next(os.walk(path + '\/masks\/'))[2]:\n        mask_ = imread(path + '\/masks\/' + mask_file)\n        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n                                      preserve_range=True), axis=-1)\n        mask = np.maximum(mask, mask_)\n    labels[n] = mask\n\nX_train = images\nY_train = labels\n\n# Get and resize test images\nX_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nsizes_test = []\nprint('Getting and resizing test images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = TEST_PATH + id_\n    img = imread(path + '\/images\/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Done!')","bb989254":"labels","05b4dc1a":"def shuffle():\n    global images, labels\n    p = np.random.permutation(len(X_train))\n    images = X_train[p]\n    labels = Y_train[p]","2302a4de":"def next_batch(batch_s, iters):\n    if(iters == 0):\n        shuffle()\n    count = batch_s * iters\n    return images[count:(count + batch_s)], labels[count:(count + batch_s)]","77ea8ec0":"def deconv2d(input_tensor, filter_size, output_size, out_channels, in_channels, name, strides = [1, 1, 1, 1]):\n    dyn_input_shape = tf.shape(input_tensor)\n    batch_size = dyn_input_shape[0]\n    out_shape = tf.stack([batch_size, output_size, output_size, out_channels])\n    filter_shape = [filter_size, filter_size, out_channels, in_channels]\n    w = tf.get_variable(name=name, shape=filter_shape)\n    h1 = tf.nn.conv2d_transpose(input_tensor, w, out_shape, strides, padding='SAME')\n    return h1","8e618db1":"def conv2d(input_tensor, depth, kernel, name, strides=(1, 1), padding=\"SAME\"):\n    return tf.layers.conv2d(input_tensor, filters=depth, kernel_size=kernel, strides=strides, padding=padding, activation=tf.nn.relu, name=name)","749f07c5":"X = tf.placeholder(tf.float32, [None, 128, 128, 3])\nY_ = tf.placeholder(tf.float32, [None, 128, 128, 1])\nlr = tf.placeholder(tf.float32)","ca94d2e3":"net = conv2d(X, 32, 1, \"Y0\") #128\n\nnet = conv2d(net, 64, 3, \"Y2\", strides=(2, 2)) #64\n\nnet = conv2d(net, 128, 3, \"Y3\", strides=(2, 2)) #32\n\n\nnet = deconv2d(net, 1, 32, 128, 128, \"Y2_deconv\") # 32\nnet = tf.nn.relu(net)\n\nnet = deconv2d(net, 2, 64, 64, 128, \"Y1_deconv\", strides=[1, 2, 2, 1]) # 64\nnet = tf.nn.relu(net)\n\nnet = deconv2d(net, 2, 128, 32, 64, \"Y0_deconv\", strides=[1, 2, 2, 1]) # 128\nnet = tf.nn.relu(net)\n\nlogits = deconv2d(net, 1, 128, 1, 32, \"logits_deconv\") # 128\n\nloss = tf.losses.sigmoid_cross_entropy(Y_, logits)\noptimizer = tf.train.AdamOptimizer(lr).minimize(loss)","05804301":"# init\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\nbatch_count = 0\ndisplay_count = 1\nfor i in range(10000):\n    # training on batches of 10 images with 10 mask images\n    if(batch_count > 67):\n        batch_count = 0    \n\n    batch_X, batch_Y = next_batch(10, batch_count)\n\n    batch_count += 1\n\n    feed_dict = {X: batch_X, Y_: batch_Y, lr: 0.0005}\n    loss_value, _ = sess.run([loss, optimizer], feed_dict=feed_dict)\n\n    if(i % 500 == 0):\n        print(str(display_count) + \" training loss:\", str(loss_value))\n        display_count +=1\n        \nprint(\"Done!\")","e5b72af5":"ix = 3 #random.randint(0, 64) #len(X_test) - 1 = 64\ntest_image = X_test[ix].astype(float)\nimshow(test_image)\nplt.show()","44342f76":"def sigmoid(x):\n    return 1 \/ (1 + math.exp(-x))","ac151982":"#print(ix)\ntest_image = np.reshape(test_image, [-1, 128 , 128, 3])\ntest_data = {X:test_image}\n\ntest_mask = sess.run([logits],feed_dict=test_data)\ntest_mask = np.reshape(np.squeeze(test_mask), [IMG_WIDTH , IMG_WIDTH, 1])\nfor i in range(IMG_WIDTH):\n    for j in range(IMG_HEIGHT):\n            test_mask[i][j] = int(sigmoid(test_mask[i][j])*255)\nimshow(test_mask.squeeze().astype(np.uint8))\nplt.show()","0ebd0b4d":"#Intro\nFirst of all thanks to Kjetil \u00c5mdal-S\u00e6vik for providing excellent code for data preparation.\nBeing a novice python programmer, my code may not be that much efficient but it may serve as a starting point for using TensorFlow.","c1bd359b":"**Test on the data that is not seen by the network during training:**"}}