{"cell_type":{"74a93756":"code","4d379fc0":"code","c03728a0":"code","d67e32ae":"code","b318e3ae":"code","20a38900":"code","bd46ddd2":"code","5e98554c":"code","834cc336":"code","9677bf38":"code","ed01bdbd":"code","38329fb3":"code","f7f01f9a":"code","a0b50049":"code","4db1e49f":"code","67c030ee":"code","db3ad8d5":"code","915a85b2":"code","68c04671":"code","c22a5e65":"code","56dbdf24":"code","8e81892b":"code","e403f7b6":"code","644b5499":"code","fa301ba0":"code","5190b5ce":"code","8d994b9a":"code","e998ee47":"code","fe61dfab":"code","61dc9ec9":"code","1223c145":"code","1dced4a4":"code","e5746a7d":"code","c6295bff":"code","b08e1b57":"code","338bcbd2":"code","ef7eed24":"code","ded82e5c":"code","7eeaa453":"code","42ef7ac5":"code","460a7d3f":"code","12c265fe":"code","d4bd0b61":"code","7221c695":"code","950cb156":"code","bb2bd49c":"code","5bda7c2c":"code","5e5dab01":"code","1e1c7c5b":"code","2788e5a8":"code","5cffd108":"code","9912eb23":"code","cf2bc810":"code","ef508812":"code","4e800ab3":"code","e3671845":"code","6528b55f":"code","f1007ae2":"code","c92eab52":"code","c56ca331":"code","a3956699":"code","e70471f0":"code","dc726fba":"code","9efac00f":"code","bd55abf6":"code","df46b3d2":"code","a0f6c13b":"code","9843de57":"code","7ba57d7a":"code","7d981fa6":"code","573fa0e3":"code","94486abb":"code","8f91b9e9":"code","9cf55b09":"code","1188515d":"code","c2fe54ab":"code","edd85f3f":"code","d6b78c5e":"code","cbc8ff5d":"code","526a04f4":"code","ba08b694":"code","cd44c28e":"code","93473817":"code","b5ffd03d":"code","73bc5588":"code","ad062a81":"code","bd21fc7e":"code","ecd36de4":"code","6dc7823d":"code","983731f4":"code","5fba52e0":"code","0c48a1ac":"code","7cc1cc37":"code","e1e4d98b":"code","7285fe6a":"code","767c02ba":"code","a3bc6d27":"code","216bd608":"code","e5b8b572":"code","e0ee75ec":"code","d0460a4f":"code","aa41d8b3":"code","f71c3e61":"code","66e9f4ee":"code","49b8db5b":"code","d7c411a3":"code","2d735dae":"code","9aa5c7ce":"code","199d3cb7":"code","44f5ca4c":"code","b2381dc8":"code","b287d0f4":"code","4874c803":"code","b5492788":"code","81ff920a":"code","5391208c":"code","a280a77c":"code","67322aee":"code","8749f4fc":"code","8801f2f9":"code","e625cafa":"code","418be987":"code","676580d8":"code","7202435e":"code","2d9c269d":"code","7e9af9b9":"code","b4d2ad72":"code","03d9eddb":"code","9f09d9f6":"code","db1863d1":"code","6d639e78":"code","2a8cec45":"code","58df29b1":"code","6417ac0e":"code","8be4b441":"code","f23125a5":"code","3fd2908f":"code","b9706e2a":"code","e1d5aff0":"code","8162a25c":"code","68e947a0":"code","942f8c32":"code","1fb2c1ff":"code","5a372c2c":"code","510eb0cb":"code","32003eed":"code","eeb692bb":"code","4f36f193":"code","65ce0521":"code","dca9575f":"code","a37eacf4":"code","779644e8":"code","3391b135":"code","60345013":"code","c1dbb802":"code","f827faae":"code","46226245":"code","e851997c":"code","0b9a6587":"code","48b26fb5":"code","d32eefae":"code","3bd65695":"code","2dee1676":"code","489b879f":"code","047277e3":"code","bed06992":"code","d23147d9":"code","8ce12584":"code","4b23bbf8":"code","77d8db48":"code","b4768378":"code","25f93539":"code","50010cdd":"code","0f0ceed7":"code","f4a3ea6f":"code","0fa0de20":"code","0a35db3b":"code","ee5fddcb":"code","4abbee3d":"code","efe01d04":"code","aa933e09":"code","70b0aa1d":"code","855b59d1":"code","fadcd621":"code","f4abfbb7":"code","a0bddf58":"code","ce70782d":"code","83b4c7fe":"code","e74f7b67":"code","f9dd0f6a":"code","b4b358d6":"code","d96aeaf1":"code","94ebe110":"code","ed460a6b":"code","115f08ef":"code","95d6bf9e":"code","cbf5c789":"code","a3f34fa1":"code","0f68df9e":"code","4c54b5af":"code","5e59989c":"markdown","957264ae":"markdown","f3d9742b":"markdown","82b04b7e":"markdown","4812e7cb":"markdown","f21a4c97":"markdown","641bbdcc":"markdown","a853d66f":"markdown","82627214":"markdown","15a02645":"markdown","65388600":"markdown","0f473d88":"markdown","e6933019":"markdown","96e48c97":"markdown","747bcf4d":"markdown","2d70909c":"markdown","2018086e":"markdown","90b7d266":"markdown","dc6b2116":"markdown","a5c2ce89":"markdown","06a5d320":"markdown","4c9afd26":"markdown","f2fc3018":"markdown","7e6a6a1b":"markdown","16e0a507":"markdown","a792c360":"markdown","d4f75722":"markdown","03277644":"markdown","f53acc48":"markdown","177c9fbd":"markdown","734f3de9":"markdown","20f067b9":"markdown","8450bc27":"markdown","0f32a4fc":"markdown","22e48974":"markdown","f76524be":"markdown","92a8fb95":"markdown","756e21cc":"markdown","4d98812d":"markdown","9a4d1402":"markdown","56327cd9":"markdown","dfa12c20":"markdown","1059c7a8":"markdown","6b5904ce":"markdown","84403529":"markdown","4e00dd38":"markdown"},"source":{"74a93756":"# Importing the libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport datetime as dt\n\nimport warnings\nwarnings.filterwarnings('ignore')","4d379fc0":"# Reading the dataset\nos.chdir('\/kaggle\/input\/wallmart\/')\ncal = pd.read_csv('calendar.csv')\nsales = pd.read_csv('sales_train_validation.csv')\nsell_prices = pd.read_csv('sell_prices.csv')","c03728a0":"cal['date']=pd.to_datetime(cal['date'])\ncal = cal.rename(columns = {'d':'id'})\ncal_1913 = cal[:1913]\ncal_1913.head()","d67e32ae":"sales_new = sales.drop(columns = ['item_id','dept_id','cat_id','store_id','state_id'])\nsales_new = sales_new.sort_values(['id'])\nsales_new = sales_new.set_index(['id'])\nsales_new = sales_new.transpose()\nsales_new = sales_new.reset_index()\nsales_new.head()","b318e3ae":"cal_sales = pd.concat([cal_1913,sales_new],axis=1)\ncal_sales.head()","20a38900":"cal_sales.tail()","bd46ddd2":"sell_prices['state_id'] = sell_prices.item_id.map(str) \\\n                          + '_'  + sell_prices.store_id.map(str) + '_validation' \\\n\nsell_prices.head()            ","5e98554c":"sell_prices[(sell_prices['state_id'] == 'HOBBIES_1_001_CA_1_validation') & (sell_prices['wm_yr_wk']>11613)]","834cc336":"main_set = set(cal_sales.columns[15:])         # Set of all items\nprices_arr = np.zeros((1,30490))               # Initializing prices with zeros as the first observation\nfor week_no in cal_sales['wm_yr_wk'].unique() :      \n    single_row_sell = sell_prices[sell_prices['wm_yr_wk'] == week_no][['sell_price','state_id']]  # Retrieving prices of all items in the week\n    differ_set = main_set.difference(set(single_row_sell['state_id'])) # Finding missing items with no price tag in the week \n    data = {'sell_price':[np.nan]*len(differ_set),'state_id':list(differ_set)} # Assigning NaN values to those missing items\n    dfl = pd.DataFrame.from_dict(data) # Converting missing items (with NaN tags) to a dataframe\n    new_df = pd.concat([single_row_sell,dfl],axis=0).sort_values('state_id').reset_index().iloc[:,1:] # Concatinating items with and without prices in the week\n    prices_arr = np.vstack((prices_arr,np.array(list(new_df.set_index('state_id').T.values)*7))) # Stacking prices of each week\n\nprices_arr = prices_arr[1:-5] # Removing the intial zeros we initialized with and the extra prices in the last week\nprices_arr","9677bf38":"total_value = cal_sales.iloc[:,15:] * prices_arr","ed01bdbd":"df2 = total_value\ndf2['date'] = cal['date'].iloc[:1913]\ndf2 = df2.fillna(0)\ndf2.head()","38329fb3":"df2.shape","f7f01f9a":"df2['Total'] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    df2['Total'] += df2[i]\ndf2['Total'].head()","a0b50049":"fig = px.line(df2, x='date', y='Total', title='Wallmart Sales 2011-2016\/10 stores',width=1200)\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","4db1e49f":"for i in range(30490):\n    i = df2.columns[i]\n    state = i.split('_')[3]\n    if state not in df2.columns:\n        df2[state] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    state = i.split('_')[3]\n    df2[state] += df2[i]\ndf2.head()","67c030ee":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['CA'].values,\n                    mode='lines',\n                    name='CA'))\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['TX'].values,\n                    mode='lines',\n                    name='TX'))\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['WI'].values,\n                    mode='lines',\n                    name='WI'))\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=700,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n    title=\"Walmart statewise sales\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#042a30\"\n    )\n)\n\nfig.update_xaxes(rangeslider_visible=True,)\nfig.show()","db3ad8d5":"for i in range(30490):\n    i = df2.columns[i]\n    category = i.split('_')[0]\n    if category not in df2.columns:\n        df2[category] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    category = i.split('_')[0]\n    df2[category] += df2[i]\ndf2.head()","915a85b2":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['FOODS'].values,\n                    mode='lines',\n                    name='FOODS'))\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['HOBBIES'].values,\n                    mode='lines',\n                    name='HOBBIES'))\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['HOUSEHOLD'].values,\n                    mode='lines',\n                    name='HOUSEHOLD'))\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=700,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n    title=\"Walmart category wise sales\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#042a30\"\n    )\n)\n\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","68c04671":"df2.columns","c22a5e65":"for i in range(30490):\n    i = df2.columns[i]\n    store = i.split('_')[3] + '_' + i.split('_')[4]\n    if store not in df2.columns:\n        df2[store] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    store = i.split('_')[3] + '_' + i.split('_')[4]\n    df2[store] += df2[i]\ndf2.head()","56dbdf24":"for i in range(30490):\n    i = df2.columns[i]\n    item = i.split('_')[3] + '_' + i.split('_')[4] + '_' + i.split('_')[0]\n    if item not in df2.columns:\n        df2[item] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    item = i.split('_')[3] + '_' + i.split('_')[4] + '_' + i.split('_')[0]\n    df2[item] += df2[i]\ndf2.head()","8e81892b":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfor i in range(30498,30502): \n    i = df2.columns[i]\n    fig.add_trace(go.Scatter(x=df2['date'], y=df2[i].values,\n                        mode='lines',\n                        name=i.split('_')[0] + ' store ' +i.split('_')[1]))\n    \nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=700,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n    title=\"Walmart California store wise sales\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#042a30\"\n    )\n)\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","e403f7b6":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfor i in range(30510,30531,10): \n    i = df2.columns[i]\n    fig.add_trace(go.Scatter(x=df2['date'], y=df2[i].values,\n                        mode='lines',\n                        name=i.split('_')[2]))\n    \nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=700,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n    title=\"Walmart California store 3 category wise sales\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#042a30\"\n    )\n)\n\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","644b5499":"df2.columns","fa301ba0":"cal = pd.read_csv('calendar.csv')\nevents = cal[['date','event_name_1','event_type_1','event_name_2','event_type_2']]\nevents = events.fillna(0)\nevents = events[(events['event_name_1'] != 0) | (events['event_name_2'] != 0)]\nevents.shape","5190b5ce":"l = []\nc = 0\nfor x in events['date'].values:\n    c +=1\n    l.append(\n    dict(\n        type=\"line\",\n        yref='paper',\n        y0=0,\n        y1=1,\n        xref='x1',\n        x0=x,\n        x1=x,\n        line=dict(\n            color=\"Red\",\n            width=2,\n            dash=\"dashdot\",\n    )))\nprint(c)\nfig = px.line(df2, x='date', y='CA_3')\nfig.update_layout(shapes=l)\nfig.show()","8d994b9a":"\nevents[events['date'] == dt.datetime(2015, 4 , 12)]\nevents[(events['date'].apply(lambda a : dt.datetime.strptime(a, \"%Y-%m-%d\").month) == 5) | (events['date'].apply(lambda a : dt.datetime.strptime(a, \"%Y-%m-%d\").month) == 6)]","e998ee47":"df2[['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA', 'snap_TX', 'snap_WI']] = cal[['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA', 'snap_TX', 'snap_WI']]\ndf2[['date','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA', 'snap_TX', 'snap_WI']]\n","fe61dfab":"dis = pd.read_csv('us_disasters_m5.csv')\ndis.head()","61dc9ec9":"dis_ca = dis[dis['state'] == 'CA']\nprint(dis_ca.shape)\ndis_ca.head()","1223c145":"dis_ca['declaration_date'] = pd.to_datetime(dis_ca['declaration_date'].apply(lambda x : x[:10]))\ndis_ca['declaration_date'].head()","1dced4a4":"dis_ca_timeline = dis_ca[['incident_type','declaration_date']]\ndis_ca_timeline['declaration_date'] = pd.to_datetime(dis_ca_timeline['declaration_date'])\ndis_ca_timeline['declaration_date'] = dis_ca_timeline['declaration_date'].apply(lambda x : x.strftime(\"%Y-%m-%d\"))","e5746a7d":"\ndis_ca_timeline = dis_ca_timeline.reset_index()\ndis_ca_timeline = dis_ca_timeline.drop(columns='index') \ndis_ca_timeline.head()","c6295bff":"dis_ca_timeline.head()","b08e1b57":"df2[['date','Total']].set_index('date').head()","338bcbd2":"l = []\nfor i in range(54):\n    x = dis_ca_timeline['declaration_date'].iloc[i]\n    l.append(\n    dict(\n        type=\"line\",\n        yref='paper',\n        y0=0,\n        y1=1,\n        xref='x1',\n        x0=x,\n        x1=x,\n        line=dict(\n            color=\"Red\",\n            width=2,\n            dash=\"dashdot\",\n    )))\nfig = px.line(df2, x='date', y='Total')\n\ni=40\nx = dis_ca_timeline['declaration_date'].iloc[i]\nfig.update_layout(shapes=l)    ","ef7eed24":"dis_ca_timeline['declaration_date'].unique()","ded82e5c":"dis_ca_timeline['Threat level'] = 'Minor'\n\nd = dis_ca_timeline['incident_type'] == 'Tsunami'\n\ndis_ca_timeline.loc[d.values,'Threat level'] = 'Major'","7eeaa453":"dis_ca_timeline[dis_ca_timeline['incident_type'] != 'Fire']","42ef7ac5":"dis[dis['state'] == 'CA']\ndis.iloc[327:329,:]","460a7d3f":"dis_ca_timeline1 = dis_ca[['incident_type','incident_begin_date']]\ndis_ca_timeline1['incident_begin_date'] = pd.to_datetime(dis_ca_timeline1['incident_begin_date'])\ndis_ca_timeline1['incident_begin_date'] = dis_ca_timeline1['incident_begin_date'].apply(lambda x : x.strftime(\"%Y-%m-%d\"))\ndis_ca_timeline1 = dis_ca_timeline1.reset_index()\ndis_ca_timeline1 = dis_ca_timeline1.drop(columns='index') \ndis_ca_timeline1.set_index('incident_begin_date')\n#dis_ca_timeline1 = dis_ca_timeline1[dis_ca_timeline1['incident_type'] != 'Fire']\ndis_ca_timeline1.head()","12c265fe":"dis_ca_timeline2 = pd.read_excel('dis_ca_timeline1_modified.xlsx')\ndis_ca_timeline2[dis_ca_timeline2['Threat level'] == 'Medium']['incident_begin_date'].values\ndis_ca_timeline2.head()","d4bd0b61":"l = []\nfor x in dis_ca_timeline2[dis_ca_timeline2['Threat level'] == 'Medium']['incident_begin_date'].values:\n    l.append(\n    dict(\n        type=\"line\",\n        yref='paper',\n        y0=0,\n        y1=1,\n        xref='x1',\n        x0=x,\n        x1=x,\n        line=dict(\n            color=\"Red\",\n            width=2,\n            dash=\"dashdot\",\n    )))\n    \nfig = px.line(df2, x='date', y='CA_3')\nfig.update_layout(shapes=l)\nfig.show()","7221c695":"dataset = pd.concat([cal,total_value['HOBBIES_1_001_CA_1_validation']],1)","950cb156":"# Head of the dataset\ndataset.head()","bb2bd49c":"# Shape of the dataset\ndataset.shape","5bda7c2c":"# Info of the dataset\ndataset.info()","5e5dab01":"# Summary statistics\ndataset.describe()","1e1c7c5b":"# Null values check\ndataset.isnull().sum()","2788e5a8":"dataset.drop(['wm_yr_wk', 'weekday','d', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_TX', 'snap_WI'],1,inplace=True)","5cffd108":"dataset['event_name_1'] = dataset['event_name_1'].fillna(0)\ndataset['event_name_1'] = np.where(dataset['event_name_1'] != 0,1,0)","9912eb23":"dataset.info()","cf2bc810":"dataset.head()","ef508812":"dataset['date'] = pd.to_datetime(dataset['date'])\ndataset['dayofmonth'] = dataset['date'].dt.day\ndom = pd.get_dummies(dataset['dayofmonth'],prefix='dayofmonth_',drop_first=True)\nmonth = pd.get_dummies(dataset['month'],prefix='month_',drop_first=True)\nyear = pd.get_dummies(dataset['year'],prefix='year_',drop_first=True)\nwday = pd.get_dummies(dataset['wday'],prefix='wday_',drop_first=True)\ndataset.drop(['month','year','dayofmonth','wday'],1,inplace=True)\ndataset = pd.concat([dataset,month,year,dom,wday],axis=1)","4e800ab3":"dataset = dataset.iloc[896:-28,:]\ndataset.head()","e3671845":"split_date = '2016-04-24'\nTrain = dataset.loc[dataset['date'] <= split_date].copy()\nTest = dataset.loc[dataset['date'] > split_date].copy()","6528b55f":"Train.drop(['date'],1,inplace=True)\nTest.drop(['date'],1,inplace=True)","f1007ae2":"Train.tail()","c92eab52":"x_train = Train.drop(['HOBBIES_1_001_CA_1_validation'],1)\ny_train = Train['HOBBIES_1_001_CA_1_validation']\nx_test = Test.drop(['HOBBIES_1_001_CA_1_validation'],1)","c56ca331":"os.chdir('\/kaggle\/input\/wallmart-sales\/')\nevaluation_df = pd.read_csv('sales_train_evaluation.csv')","a3956699":"y_test = pd.Series(evaluation_df.iloc[0,-28:].values)","e70471f0":"x = pd.concat([x_train,x_test],0)\ny = pd.concat([y_train,y_test],0)","dc726fba":"x.shape","9efac00f":"from sklearn.decomposition import PCA","bd55abf6":"pca = PCA(n_components=54)\npca.fit_transform(x)","df46b3d2":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative variance explained')\nplt.show()","a0f6c13b":"pca = PCA(n_components=20)\npca.fit(x)\nx = pca.transform(x)","9843de57":"x = pd.DataFrame(x)","7ba57d7a":"x_train = x.iloc[:-28,:]\nx_test = x.iloc[-28:,:]","7d981fa6":"y_test *= 8.38","573fa0e3":"# Importing Linear Regression and fitting the model\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train,y_train)\nlr_pred = lr.predict(x_test)","94486abb":"# Importing metrics and evaluating the model\nfrom sklearn import metrics","8f91b9e9":"y_test","9cf55b09":"# RMSE score \nlr_rmse = np.sqrt(metrics.mean_squared_error(lr_pred,y_test))\nlr_rmse","1188515d":"# R2 score\nlr_r2score = metrics.r2_score(lr_pred,y_test)\nlr_r2score","c2fe54ab":"# Train score\nlr_train = lr.score(x_train,y_train)\nlr_train","edd85f3f":"# Test score\nlr_test = lr.score(x_test,y_test)\nlr_test","d6b78c5e":"# Importing Decision Tree and performing decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\ndt.fit(x_train,y_train)\ndt_pred = dt.predict(x_test)","cbc8ff5d":"# RMSE score for Decision Tree\ndt_rmse = np.sqrt(metrics.mean_squared_error(dt_pred,y_test))\ndt_rmse","526a04f4":"# R2 score for Decision Tree\ndt_r2score = metrics.r2_score(dt_pred,y_test)\ndt_r2score","ba08b694":"# Train score for Decision Tree\ndt_train = dt.score(x_train,y_train)\ndt_train","cd44c28e":"# Test score for Decision Tree\ndt_test = dt.score(x_test,y_test)\ndt_test","93473817":"# Importing Randomizedsearchcv and finding out optimal parameters for Decision Tree\nfrom sklearn.model_selection import RandomizedSearchCV\nparams = {'max_depth': np.arange(1,20),'criterion':['mse','mae']}\ndt = DecisionTreeRegressor()\ntree = RandomizedSearchCV(dt, params, cv=3 , return_train_score = True) # RandomizedSearchCV\ntree.fit(x,y)# Fit","b5ffd03d":"# optimal parameters\ntree.best_params_","73bc5588":"# Fitting the model and training and testing after parameter tuning\ndtr = DecisionTreeRegressor(criterion='mse',max_depth=1)\ndtr.fit(x_train,y_train)\ndtr_pred = dtr.predict(x_test)","ad062a81":"# RMSE score for DT after parameter tuning\ndt_tune_rmse = np.sqrt(mean_squared_error(dtr_pred,y_test))\ndt_tune_rmse","bd21fc7e":"# R2 score for DT after parameter tuning\ndt_tune_r2score = r2_score(dtr_pred,y_test)\ndt_tune_r2score","ecd36de4":"# Train score for DT after parameter tuning\ndt_tune_train = dtr.score(x_train,y_train)\ndt_tune_train","6dc7823d":"# Test score for DT after parameter tuning\ndt_tune_test = dtr.score(x_test,y_test)\ndt_tune_test","983731f4":"# Importing Random Forest Regressor and fitting the model\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(x_train,y_train)\nrf_pred = rf.predict(x_test)","5fba52e0":"# RMSE score for Random Forest\nrf_rmse = np.sqrt(mean_squared_error(rf_pred,y_test))\nrf_rmse","0c48a1ac":"# R2 Score for Random Forest\nrf_r2score = r2_score(rf_pred,y_test)\nrf_r2score","7cc1cc37":"# Train score for Random Forest\nrf_train = rf.score(x_train,y_train)\nrf_train","e1e4d98b":"# Test score for Random Forest\nrf_test = rf.score(x_test,y_test)\nrf_test","7285fe6a":"# Using Randomized SearchCV and finding optimal parameters\nrf = RandomForestRegressor()\nparams1 = {'n_estimators': np.arange(1,20),'criterion':['mse','mae']}\nforest = RandomizedSearchCV(rf, params, cv=3 , return_train_score = True) # GridSearchCV\nforest.fit(x,y)# Fit","767c02ba":"# optimal parameters\nforest.best_params_","a3bc6d27":"# Random Forest after parameter tuning\nrfr = RandomForestRegressor(criterion='mse',max_depth=2)\nrfr.fit(x_train,y_train)\nrfr_pred = rfr.predict(x_test)","216bd608":"# RMSE score  for Random Forest after parameter tuning\nrf_tune_rmse = np.sqrt(metrics.mean_squared_error(rfr_pred,y_test))\nrf_tune_rmse","e5b8b572":"# R2 score for Random Forest after parameter tuning\nrf_tune_r2score = metrics.r2_score(rfr_pred,y_test)\nrf_tune_r2score","e0ee75ec":"# Train score for Random Forest after parameter tuning\nrf_tune_train = rfr.score(x_train,y_train)\nrf_tune_train","d0460a4f":"# Test score for Random Forest after parameter tuning\nrf_tune_test = rfr.score(x_test,y_test)\nrf_tune_test","aa41d8b3":"# Importing Support Vector Regressor and fitting the model\nfrom sklearn.svm import SVR\nsvm = SVR()\nsvm.fit(x_train,y_train)\nsvm_pred = svm.predict(x_test)","f71c3e61":"# RMSE score for SVM\nsvm_rmse = np.sqrt(metrics.mean_squared_error(svm_pred,y_test))\nsvm_rmse","66e9f4ee":"# R2 score for SVM\nsvm_r2score = metrics.r2_score(svm_pred,y_test)\nsvm_r2score","49b8db5b":"# Train score for SVM\nsvm_train = svm.score(x_train,y_train)\nsvm_train","d7c411a3":"# Test score for SVM\nsvm_test = svm.score(x_test,y_test)\nsvm_test","2d735dae":"# Using Randomized Search cv to find the optimal parameters\nparams2 = {'kernel':['linear','rbf'],'C': [0.01, 0.1, 1, 10],'gamma': [0.01,0.1,1,10]}\nsvr = SVR()\nsupport = RandomizedSearchCV(svr, params2, cv=3 , return_train_score = True) # RandomizedSearchCV\nsupport.fit(x,y)# Fit","9aa5c7ce":"# optimal parameters\nsupport.best_params_","199d3cb7":"# Fitting the model and training and testing\nsvrr = SVR(C = 10,gamma = 10,kernel = 'rbf')\nsvrr.fit(x_train,y_train)\nsvrr_pred = svrr.predict(x_test)","44f5ca4c":"# RMSE score for SVM after parameter tuning\nsvm_tune_rmse = np.sqrt(metrics.mean_squared_error(svrr_pred,y_test))","b2381dc8":"svm_tune_rmse","b287d0f4":"# R2 score for SVM after parameter tuning\nsvm_tune_r2score = metrics.r2_score(svrr_pred,y_test)\nsvm_tune_r2score","4874c803":"# Train score for SVM after parameter tuning\nsvm_tune_train = svrr.score(x_train,y_train)\nsvm_tune_train","b5492788":"# Test score for SVM after parameter tuning\nsvm_tune_test = svrr.score(x_test,y_test)\nsvm_tune_test","81ff920a":"# Importing KNearest Neighbors and fitting the model\nfrom sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor()\nknn.fit(x_train,y_train)\nknn_pred = knn.predict(x_test)","5391208c":"# RMSE score for KNN\nknn_rmse = np.sqrt(metrics.mean_squared_error(knn_pred,y_test))\nknn_rmse","a280a77c":"# R2 score for KNN\nknn_r2score = metrics.r2_score(knn_pred,y_test)\nknn_r2score","67322aee":"# Train score for KNN\nknn_train = knn.score(x_train,y_train)\nknn_train","8749f4fc":"# Test score for KNN\nknn_test = knn.score(x_test,y_test)\nknn_test","8801f2f9":"# Finding optimal parameters using Randomized Search CV\nparams4 = {'leaf_size':np.arange(1,50),'n_neighbors':np.arange(1,30),'p':[1,2]}\nknn = KNeighborsRegressor()\nneighbor = RandomizedSearchCV(knn, params4, cv=3 , return_train_score = True) # RandomizedSearchCV\nneighbor.fit(x,y)","e625cafa":"# Optimal parameters\nneighbor.best_params_","418be987":"# Fitting the model and training and testing\nknn = KNeighborsRegressor(n_neighbors=21,p=1,leaf_size=38)\nknn.fit(x_train,y_train)\nknnr_pred = knn.predict(x_test)","676580d8":"# RMSE score for KNN after parameter tuning\nknn_tune_rmse = np.sqrt(metrics.mean_squared_error(knnr_pred,y_test))\nknn_tune_rmse","7202435e":"# R2 score for KNN after parameter tuning\nknn_tune_r2score = metrics.r2_score(knnr_pred,y_test)\nknn_tune_r2score","2d9c269d":"# Train score for KNN after parameter tuning\nknn_tune_train = knn.score(x_train,y_train)\nknn_tune_train","7e9af9b9":"# Test score for KNN after parameter tuning\nknn_tune_test = knn.score(x_test,y_test)\nknn_tune_test","b4d2ad72":"# Fitting the Ada Boost model\nfrom sklearn.ensemble import AdaBoostRegressor\nab = AdaBoostRegressor()\nab.fit(x_train,y_train)\nab_pred = ab.predict(x_test)","03d9eddb":"# RMSE score for Ada Boost\nab_rmse = np.sqrt(metrics.mean_squared_error(ab_pred,y_test))\nab_rmse","9f09d9f6":"# R2 score for Ada Boost\nab_r2score = metrics.r2_score(ab_pred,y_test)\nab_r2score","db1863d1":"# Train score for Ada Boost\nab_train = ab.score(x_train,y_train)\nab_train","6d639e78":"# Test score for Ada Boost\nab_test = ab.score(x_test,y_test)\nab_test","2a8cec45":"# Finding the optimal parameters for Ada Boost Regressor using Randomized Search CV\n\nparam_grid1 = {\"n_estimators\": range(5,20,2) ,  \n              \"learning_rate\": [0.01,0.05,0.1,0.5,1],'loss':['linear','square','exponential']}\n \n\nAB = RandomizedSearchCV(ab,param_distributions=param_grid1,\n                           cv = 5,\n                           n_jobs=-1,\n                           verbose=2)\nAB.fit(x,y)","58df29b1":"# optimal parameters\nAB.best_params_","6417ac0e":"# Fitting the model after parameter tuning\nabr = AdaBoostRegressor(n_estimators=9,learning_rate=0.05,loss='linear')\nabr.fit(x_train,y_train)\nabr_pred = abr.predict(x_test)","8be4b441":"# RMSE score for Ada Boost after parameter tuning\nab_tune_rmse = np.sqrt(metrics.mean_squared_error(abr_pred,y_test))\nab_tune_rmse","f23125a5":"# R2 score for Ada Boost after parameter tuning\nab_tune_r2score = metrics.r2_score(abr_pred,y_test)\nab_tune_r2score","3fd2908f":"# Train score for Ada Boost after parameter tuning\nab_tune_train = abr.score(x_train,y_train)\nab_tune_train","b9706e2a":"# Test score for Ada Boost after parameter tuning\nab_tune_test = abr.score(x_test,y_test)\nab_tune_test","e1d5aff0":"# Fitting the Gradient Boost model\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor()\ngb.fit(x_train,y_train)\ngb_pred = gb.predict(x_test)","8162a25c":"# RMSE score for Gradient Boosting\ngb_rmse = np.sqrt(metrics.mean_squared_error(gb_pred,y_test))\ngb_rmse","68e947a0":"# R2 score for Gradient Boosting\ngb_r2score = metrics.r2_score(gb_pred,y_test)\ngb_r2score","942f8c32":"# Train score for Gradient Boosting\ngb_train = gb.score(x_train,y_train)\ngb_train","1fb2c1ff":"# Test score for Gradient Boosting\ngb_test = gb.score(x_test,y_test)\ngb_test","5a372c2c":"# Finding the optimal parameters For Gradient Boosting Regressor using Randomized Search CV\n\nparam_grid1 = {\"n_estimators\": range(5,20,2) ,  \n              \"learning_rate\": [0.01,0.05,0.1,0.5,1]}\n \n\nGB = RandomizedSearchCV(gb,param_distributions=param_grid1,\n                           cv = 5,\n                           n_jobs=-1,\n                           verbose=2)\nGB.fit(x,y)","510eb0cb":"# optimal parameters\nGB.best_params_","32003eed":"# Fitting the Gradient model after parameter tuning\ngbr = GradientBoostingRegressor(n_estimators=9,learning_rate=0.01)\ngbr.fit(x_train,y_train)\ngbr_pred = gbr.predict(x_test)","eeb692bb":"# RMSE score for Gradient Boosting after parameter tuning\ngb_tune_rmse = np.sqrt(metrics.mean_squared_error(gbr_pred,y_test))\ngb_tune_rmse","4f36f193":"# R2 score for Gradient Boosting after parameter tuning\ngb_tune_r2score = metrics.r2_score(gbr_pred,y_test)\ngb_tune_r2score","65ce0521":"# Train score for Gradient Boosting after parameter tuning\ngb_tune_train = gbr.score(x_train,y_train)\ngb_tune_train","dca9575f":"# Test score for parameter tuning\ngb_tune_test = gbr.score(x_test,y_test)\ngb_tune_test","a37eacf4":"from xgboost.sklearn import XGBRegressor","779644e8":"# Fitting the model\nxgb = XGBRegressor()","3391b135":"# Training the model\nxgb.fit(x_train, y_train)","60345013":"# Testing the model\nxgb_pred=xgb.predict(x_test)","c1dbb802":"# RMSE score for XG Boost\nxgb_rmse = np.sqrt(metrics.mean_squared_error(xgb_pred,y_test))\nxgb_rmse","f827faae":"# R2 score for XG Boost\nxgb_r2score = metrics.r2_score(xgb_pred,y_test)\nxgb_r2score","46226245":"# Train score for XG Boost\nxgb_train = xgb.score(x_train,y_train)\nxgb_train","e851997c":"# Test score for XG Boost\nxgb_test = xgb.score(x_test,y_test)\nxgb_test","0b9a6587":"# Finding optimal parameters for XG Boost Regressor using Randomized Search CV\nparam_grid1 = {\"max_depth\": [10,15,20,30],\n              \"n_estimators\": range(5,20,2) , \n              \"gamma\": [0.03,0.05], \n              \"learning_rate\": [0.01,0.05]}\n \n\nXGB = RandomizedSearchCV(xgb,param_distributions=param_grid1,\n                           cv = 5)\nXGB.fit(x,y)","48b26fb5":"# optimal parameters\nXGB.best_params_","d32eefae":"# Fitting the model after parameter tuning\nxgbr = XGBRegressor(n_estimators=9,max_depth=10,learning_rate=0.05,gamma=0.03)","3bd65695":"# Training the model after parameter tuning\nxgbr.fit(x_train,y_train)","2dee1676":"# Testing the model after parameter tuning\nxgbr_pred = xgbr.predict(x_test)","489b879f":"# RMSE score for XG Boost after parameter tuning\nxgb_tune_rmse = np.sqrt(metrics.mean_squared_error(xgbr_pred,y_test))\nxgb_tune_rmse","047277e3":"# R2 score for XG Boost after parameter tuning\nxgb_tune_r2score = metrics.r2_score(xgbr_pred,y_test)\nxgb_tune_r2score","bed06992":"# Train score for XG Boost after parameter tuning\nxgb_tune_train = xgbr.score(x_train,y_train)\nxgb_tune_train","d23147d9":"# Test score for XG Boost after parameter tuning\nxgb_tune_test = xgbr.score(x_test,y_test)\nxgb_tune_test","8ce12584":"df_arima_train = Train['HOBBIES_1_001_CA_1_validation']","4b23bbf8":"!python3.7 -m pip install --upgrade pip","77d8db48":"!pip install pmdarima","b4768378":"from pmdarima.arima import auto_arima\nstepwise_model = auto_arima(df_arima_train,start_p=1,start_q=1,max_p=3,max_q=3,m=7,start_P=0,seasonal=True,d=1,D=1,trace=True,error_action='ignore',suppress_warnings=True,stepwise=True)","25f93539":"ar_day_pred = stepwise_model.predict(n_periods=28)\nar_day_rmse = np.sqrt(metrics.mean_squared_error(ar_day_pred, y_test))\nar_day_rmse","50010cdd":"stepwise_model1 = auto_arima(df_arima_train,start_p=1,start_q=1,max_p=3,max_q=3,m=12,start_P=0,seasonal=True,d=1,D=1,trace=True,error_action='ignore',suppress_warnings=True,stepwise=True)\nar_month_pred = stepwise_model1.predict(n_periods=28) ","0f0ceed7":"ar_month_rmse = np.sqrt(metrics.mean_squared_error(ar_month_pred,y_test))\nar_month_rmse","f4a3ea6f":"# Creating dictionary for all the metrics and models\nmetrics_dict = {'Metrics': ['Before Parameter Tune Train Score','Before Parameter Tune Test Score','After Parameter Tune Train Score','After Parameter Tune Test Score','Before Parameter Tune RMSE Score','After Parameter Tune RMSE Score','Before Parameter Tune R2 Score','After Parameter Tune R2 Score'],'Linear Regression':[lr_train,lr_test,'NA','NA',lr_rmse,'NA',lr_r2score,'NA'],\n          'Decision Tree Regressor':[dt_train,dt_test,dt_tune_train,dt_tune_train,dt_rmse,dt_tune_rmse,dt_r2score,dt_tune_r2score],'Ramdom Forest Regressor':[rf_train,rf_test,rf_tune_train,rf_tune_test,rf_rmse,rf_tune_rmse,rf_r2score,rf_tune_r2score],'Support Vector Regressor':[svm_train,svm_test,'NA','NA',svm_rmse,'NA',svm_r2score,'NA'],\n          'KNearestNeighbor Regressor':[knn_train,knn_test,knn_tune_train,knn_tune_test,knn_rmse,knn_tune_rmse,knn_r2score,knn_tune_r2score],\n          'XG Boost Regressor':[xgb_train,xgb_test,xgb_tune_train,xgb_tune_test,xgb_rmse,xgb_tune_rmse,xgb_r2score,xgb_tune_r2score],\n          'Ada Boost Regressor':[ab_train,ab_test,ab_tune_train,ab_tune_test,ab_rmse,ab_tune_rmse,ab_r2score,ab_tune_r2score],\n          'Gradient Boosting Regressor':[gb_train,gb_test,gb_tune_train,gb_tune_test,gb_rmse,gb_tune_rmse,gb_r2score,gb_tune_r2score]}","0fa0de20":"# Converting dictionary to dataframe\nmetrics_df = pd.DataFrame(metrics_dict)","0a35db3b":"# Dataframe of metrics\nmetrics_df","ee5fddcb":"# Assigning estimator models for voting classifier\nvote_est = [('lr',lr),('ab',ab),('dt',dt)]","4abbee3d":"# Importing Voting Regressor\nfrom sklearn.ensemble import VotingRegressor\nvote = VotingRegressor(estimators=vote_est)","efe01d04":"# Fitting the model\nvote.fit(x_train,y_train)","aa933e09":"# Testing the model\nvote_pred = vote.predict(x_test)","70b0aa1d":"# Importing metrics\nfrom sklearn import metrics","855b59d1":"# RMSE score for Voting Regressor\nvote_rmse = np.sqrt(metrics.mean_squared_error(vote_pred,y_test))\nvote_rmse","fadcd621":"# R2 score for Voting Regressor\nvote_r2score = metrics.r2_score(vote_pred,y_test)\nvote_r2score","f4abfbb7":"# Train score for Voting Regressor\nvote_train = vote.score(x_train,y_train)\nvote_train","a0bddf58":"# Test score for Voting Regressor\nvote_test = vote.score(x_test,y_test)\nvote_test","ce70782d":"# mlxtend regressor\nfrom mlxtend.regressor import StackingRegressor","83b4c7fe":"# Assigning individual models to variables\nxgb = XGBRegressor()\nada = AdaBoostRegressor()\ngrad = GradientBoostingRegressor()","e74f7b67":"# Fitting the model\nst = StackingRegressor(regressors=[dt,ab,rf,xgb,ada,grad],meta_regressor=lr)","f9dd0f6a":"# Training the model\nst.fit(x_train,y_train)","b4b358d6":"# Testing the model\nst_pred = st.predict(x_test)","d96aeaf1":"# RMSE score for Stacking Regressor\nst_rmse = np.sqrt(metrics.mean_squared_error(st_pred,y_test))\nst_rmse","94ebe110":"# R2 score for Stacking Regressor\nst_r2score = metrics.r2_score(st_pred,y_test)\nst_r2score","ed460a6b":"# Train score for Stacking Regressor\nst_train = st.score(x_train,y_train)\nst_train","115f08ef":"# Test score for Stacking Regressor\nst_test = st.score(x_test,y_test)\nst_test","95d6bf9e":"# Creating dictionary for all the metrics and converting it to dataframe\nmetrics_stack = {'Models': ['Voting Regressor','Stacking Regressor'],'RMSE score':[vote_rmse,st_rmse],'R2 Score':[vote_rmse,st_rmse],'Train score':[vote_train,st_train],'Test score':[vote_test,st_test]}\n\nmetrics_stack = pd.DataFrame(metrics_stack)","cbf5c789":"# Dataframe\nmetrics_stack","a3f34fa1":"# Importing Vecstack\nfrom vecstack import stacking","0f68df9e":"#1st level model\nmodels = [lr,ab,dt,svm]\nS_train, S_test = stacking(models, x_train, y_train, x_test, \n    regression = True, metric = metrics.r2_score, n_folds = 4 , \n    shuffle = True, random_state = 0, verbose = 2)","4c54b5af":"#2nd level model\nmodels = [knn,xgb,grad,ada]\nS_train, S_test = stacking(models, x_train, y_train, x_test, \n    regression = True, metric = metrics.r2_score, n_folds = 4 , \n    shuffle = True, random_state = 0, verbose = 2)","5e59989c":"**Parameter Tuning**","957264ae":"The best model being Voting Regressor","f3d9742b":"###5.2.7 Gradient Boosting <a id = 'grad'>","82b04b7e":"### 5.2.9 Arima models","4812e7cb":"#### b. Total sales per day per state","f21a4c97":"### 5.2.2 Decision Tree  <a id='dt'>","641bbdcc":"## 5.2.6 Ada Boost Classifier <a id = 'ada'>","a853d66f":"### 3.3 Multiplying quantities and prices","82627214":"### 5.2.10 Comparison Table","15a02645":"## 5. Extracting single item ","65388600":"### 4.3 Events analysis","0f473d88":"#### d. Total sales per day per state per store","e6933019":"### 5.2.3 Random Forest  <a id='rf'>","96e48c97":"#### 5.3.2 Mlxtend Stacking Regressor","747bcf4d":"### 5.4 Create a dataframe with model Stacking model names and metric scores and compare along with the first dataframe and give inference","2d70909c":"**Parameter Tuning**","2018086e":"### 5.2.5 KNearest Neighbors <a id = 'knn'>","90b7d266":"### 5.5 Performing Vecstack","dc6b2116":"## 4. EDA","a5c2ce89":"### 3.2 Calculating prices","06a5d320":"The downward lines in the above chart corresponds to no sales on the Eve of Christmas holiday","4c9afd26":"###  5.2.4 Support Vector Machine <a id = 'svm'>","f2fc3018":"### 5.2.8 XG Boost","7e6a6a1b":"### 3.1 Calculating quantities","16e0a507":"**As we could see the max and min are almost same for all the columns so no need of scaling**","a792c360":"### 5.2.1 Linear Regression","d4f75722":"## 1. Importing packages","03277644":"### 5.1 Data preprocessing","f53acc48":"#### 4.4.1 Disaster is Medium","177c9fbd":"### 5.2 Building models","734f3de9":"### 4.2 Analysis of California stores","20f067b9":"## 3. Preprocessing data","8450bc27":"As there are 4 stores in California compared to 3 stores each in Texas and Wisconsin, California is on top the chart","0f32a4fc":"### 5.3 Perform the Stacking models Voting and Mlxtend and analyze the metrics","22e48974":"#### c. Total sales per day per category","f76524be":"**Parameter Tuning**","92a8fb95":"**Parameter Tuning**","756e21cc":"**Parameter tuning**","4d98812d":"**Parameter Tuning**","9a4d1402":"## 2. Reading data","56327cd9":"### 4.4 Disaster analysis","dfa12c20":"**Parameter Tuning**","1059c7a8":"#### 4.2.1 Analysis of California Store 3","6b5904ce":"#### 5.3.1 Voting Regressor ","84403529":"### 4.1 Adding new features","4e00dd38":"#### a. Total sales of 10 stores per day"}}