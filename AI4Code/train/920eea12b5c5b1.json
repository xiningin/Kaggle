{"cell_type":{"0dafcfb4":"code","873f4a1c":"code","6ca5c5c2":"code","ec05f453":"code","e2ea5c0f":"code","c104dde5":"code","ad1e892c":"code","eb31809f":"code","8a9201be":"code","ad9c4f43":"code","74f672e8":"code","f9848124":"code","0f18c4dd":"code","8690c42a":"code","fe46efee":"code","d3b96bf4":"code","55ac8fbf":"code","1eed541c":"code","bc83274f":"code","30840661":"code","5d2a0d33":"code","39fa0415":"code","ce64998c":"code","2c8b24d2":"code","de87e0cc":"code","056e46c6":"code","65030ec8":"code","9c310aa9":"code","1fa5d959":"code","fbac6bed":"markdown","003370f3":"markdown","b7e2ec5c":"markdown","340bf8bf":"markdown","b123d633":"markdown","87dc1db3":"markdown","f45859c2":"markdown","a666f31a":"markdown","a41e25a0":"markdown","1284f152":"markdown","fa3c2fca":"markdown","7b0ae398":"markdown","477bc511":"markdown"},"source":{"0dafcfb4":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport re\nimport os\n\nwarnings.filterwarnings(\"ignore\")","873f4a1c":"%%HTML\n\n# Thanks to https:\/\/www.kaggle.com\/masumrumi\n<div class='tableauPlaceholder' id='viz1516349898238' style='position: relative'><noscript><a href='#'><img alt='Titanic Training Dataset ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1_rss.png' style='border: none' \/><\/a><\/noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/> <param name='embed_code_version' value='3' \/> <param name='site_root' value='' \/><param name='name' value='Titanic_data_mining&#47;Dashboard1' \/><param name='tabs' value='no' \/><param name='toolbar' value='yes' \/><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1.png' \/> <param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/><param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/><param name='display_count' value='yes' \/><param name='filter' value='publish=yes' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1516349898238');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>","6ca5c5c2":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_index = test.PassengerId.copy()","ec05f453":"train.drop([\"PassengerId\"], axis = 1, inplace = True) # Drop \ntest.drop([\"PassengerId\"], axis = 1, inplace = True) # Drop ","e2ea5c0f":"train.head()","c104dde5":"def eda_object(df,feature):\n    a = len(df[feature].unique())\n    plt.figure(figsize = [15,min(max(8,a),5)])\n\n    plt.subplot(1,2,1)\n    x_ = df.groupby([feature])[feature].count()\n    x_.plot(kind='pie')\n    plt.title(feature)\n\n    plt.subplot(1,2,2)\n    cross_tab = pd.crosstab(df['Survived'],df[feature],normalize=0).reset_index()\n    x_ = cross_tab.melt(id_vars=['Survived'])\n    x_['value'] = x_['value']\n\n    sns.barplot(x=feature,y='value',hue ='Survived',data=x_,palette = ['b','r','g'],alpha =0.7)\n    plt.xticks(rotation='vertical')\n    plt.title(feature + \" - Survived\")\n\n    plt.tight_layout()\n    plt.legend()\n    plt.show()\n\nrm_list = ['PassengerId','Name','Ticket', \"Cabin\"]\ntype_list = ['object']\nfeature_list = []\n\nfor feature in train.columns:\n    if (feature not in rm_list) & (train[feature].dtypes in type_list):\n        feature_list.append(feature)","ad1e892c":"for feature in feature_list:\n    eda_object(train,feature)","eb31809f":"train[[\"Survived\",\"Pclass\", \"Sex\", \"Cabin\", \"Embarked\"]] = train[[\"Survived\",\"Pclass\", \"Sex\", \"Cabin\", \"Embarked\"]].astype(\"object\")\ntest[[\"Pclass\", \"Sex\", \"Cabin\", \"Embarked\"]] = test[[\"Pclass\", \"Sex\", \"Cabin\", \"Embarked\"]].astype(\"object\")","8a9201be":"def eda_numeric(df,feature):\n    x_ = df[feature]\n    y_ = df['Survived']\n    data = pd.concat([x_,y_],1)\n    plt.figure(figsize=[15,5])\n\n    ax1 = plt.subplot(1,2,1)\n    sns.boxplot(x='Survived',y=feature,data=data)\n    plt.title(feature+ \" - Boxplot\")\n    upper_0 = data[data['Survived']==0][feature].quantile(q=0.75)\n    upper_1 = data[data['Survived']==1][feature].quantile(q=0.75)\n    lower_0 = data[data['Survived']==0][feature].quantile(q=0.25)\n    lower_1 = data[data['Survived']==1][feature].quantile(q=0.25)\n\n    ax1.set(ylim=(min(lower_0,lower_1),max(upper_0,upper_1)))\n\n    ax2 = plt.subplot(1,2,2)\n    plt.title(feature+ \" - Density with Log\")\n    \n    p1=sns.kdeplot(data[data['Survived']==0][feature].apply(np.log), color=\"b\",legend=False)\n    p2=sns.kdeplot(data[data['Survived']==1][feature].apply(np.log), color=\"r\",legend=False)\n    plt.legend(loc='upper right', labels=['0', '1'])\n\n    plt.tight_layout()\n    plt.show()\n    \nrm_list = ['PassengerId','Name','Ticket', \"Cabin\"]\ntype_list = ['int64','float']\nfeature_list = []\n\nfor feature in train.columns:\n    if (feature not in rm_list) & (train[feature].dtypes in type_list) & (len(train[feature].unique()) > 5):\n        feature_list.append(feature)\n","ad9c4f43":"for feature in feature_list:\n    eda_numeric(train,feature)","74f672e8":"%config InlineBackend.figure_format = \"retina\"\nsns.catplot(x=\"Embarked\",y=\"Age\", data = train, hue = \"Sex\");","f9848124":"sns.boxplot(\"SibSp\",\"Age\", data = train);","0f18c4dd":"sns.boxplot(\"SibSp\",\"Fare\", hue = \"Survived\",data = train, palette=\"ch:.25\");","8690c42a":"tmp = pd.crosstab(train['Embarked'], train['Survived'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'Survived', 1:'NoSurvived'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Embarked Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='Embarked', data=train)\n\ng.set_title(\"Embarked Distribution\", fontsize=19)\ng.set_xlabel(\"Embarked Name\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\n\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/len(train)*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(222)\ng1 = sns.countplot(x='Embarked', hue='Survived', data=train)\nplt.legend(title='Survived', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='Embarked', y='Survived', data=tmp, color='black', order=['S', 'C', 'Q'], legend=False)\ngt.set_ylabel(\"% of Survived \", fontsize=16)\n\ng1.set_title(\"Embarked  by Target(Survived)\", fontsize=19)\ng1.set_xlabel(\"Embarked Name\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='Embarked', y='Age', hue='Survived', \n              data=train[train['Age'] <= 100] )\ng3.set_title(\"Age Distribuition by Embarked and Target\", fontsize=20)\ng3.set_xlabel(\"Embarked Name\", fontsize=17)\ng3.set_ylabel(\"Age Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","fe46efee":"sns.pairplot(data=train[[\"Fare\",\"Survived\",\"Age\",\"Pclass\"]],\n             hue=\"Survived\", dropna=True);","d3b96bf4":"pd.crosstab([train.Embarked, train.Pclass], [train.Sex, train.Survived], margins=True).style.background_gradient(cmap='cubehelix_r')","55ac8fbf":"sns.catplot(\"Survived\",\"SibSp\", kind = \"violin\", hue = \"Sex\", data = train);","1eed541c":"    def missing_zero_values_table(df):\n            zero_val = (df == 0.00).astype(int).sum(axis=0)\n            mis_val = df.isnull().sum()\n            mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n            mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n            mz_table = mz_table.rename(\n            columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})\n            mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']\n            mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] \/ len(df)\n            mz_table['Data Type'] = df.dtypes\n            mz_table = mz_table[\n                mz_table.iloc[:,1] != 0].sort_values(\n            '% of Total Values', ascending=False).round(1)\n            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n                \"There are \" + str(mz_table.shape[0]) +\n                  \" columns that have missing values.\")\n            return mz_table\n\n\n    missing_zero_values_table(train)\n","bc83274f":"fig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(train.isnull(), cmap=\"cubehelix_r\", yticklabels='')\nplt.show()","30840661":"train[\"Embarked\"] = train[\"Embarked\"].fillna('S')\ntest[\"Embarked\"] = test[\"Embarked\"].fillna('S')\n\ndata = [train, test]\n\nfor dataset in data:\n    mean = train[\"Age\"].mean()\n    std = test[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train[\"Age\"].astype(int)\n    \ntrain[\"Fare\"] = train[\"Fare\"].fillna(train[\"Fare\"].median())\ntest[\"Fare\"] = test[\"Fare\"].fillna(test[\"Fare\"].median())","5d2a0d33":"%%time\n\ntrain.Cabin.fillna(\"Miss\", inplace=True)\ntrain.Cabin = [i[0] for i in train.Cabin.astype(\"str\")]\n\n\n\ntest.Cabin.fillna(\"Miss\", inplace=True)\ntest.Cabin = [i[0] for i in test.Cabin.astype(\"str\")]\n\n\n\ntrain.Ticket = [i[0] for i in train.Ticket.astype(\"str\")]\ntest.Ticket = [i[0] for i in test.Ticket.astype(\"str\")]\n\n\n\n\n\n\ntrain.Fare_group = pd.DataFrame()\ntrain['Fare_group'] = np.where(train['Fare'] < 7, 'Low_Fare',\n                              np.where(train[\"Fare\"] < 14, \"low_mid_Fare\",\n                                      np.where(train[\"Fare\"] < 21, \"mid_fare\", \n                                              np.where(train[\"Fare\"] < 28, \"mid_high_fare\", \n                                                      np.where(train[\"Fare\"] < 35, \"high_low_fare\",\n                                                              np.where(train[\"Fare\"] < 42, \"high_mid_fare\",\n                                                                      np.where(train[\"Fare\"] < 49, \"high_high_fare\",\n                                                                              np.where(train[\"Fare\"] < 56, \"top_fare\",\n                                                                                      np.where(train[\"Fare\"] < 63, \"max_fare\", \"out_top_fare\")))))))))\n\n\n\ntest.Fare_group = pd.DataFrame()\ntest['Fare_group'] = np.where(test['Fare'] < 7, 'Low_Fare',\n                              np.where(test[\"Fare\"] < 14, \"low_mid_Fare\",\n                                      np.where(test[\"Fare\"] < 21, \"mid_fare\", \n                                              np.where(test[\"Fare\"] < 28, \"mid_high_fare\", \n                                                      np.where(test[\"Fare\"] < 35, \"high_low_fare\",\n                                                              np.where(test[\"Fare\"] < 42, \"high_mid_fare\",\n                                                                      np.where(test[\"Fare\"] < 49, \"high_high_fare\",\n                                                                              np.where(test[\"Fare\"] < 56, \"top_fare\",\n                                                                                      np.where(test[\"Fare\"] < 63, \"max_fare\", \"out_top_fare\")))))))))\n\n\ntrain['Age_group'] = 0\ntrain.loc[train['Age']  <= 15, 'Age_group'] = 0\ntrain.loc[(train['Age'] > 15) & (train['Age'] <= 30), 'Age_group']=1\ntrain.loc[(train['Age'] > 30) & (train['Age'] <= 45), 'Age_group']=2\ntrain.loc[(train['Age'] > 45) & (train['Age'] <= 60), 'Age_group']=3\ntrain.loc[train['Age']  > 60, 'Age_group'] = 4\n\n\ntest['Age_group'] = 0\ntest.loc[test['Age']  <= 15, 'Age_group'] = 0\ntest.loc[(test['Age'] > 15) & (test['Age'] <= 30), 'Age_group']=1\ntest.loc[(test['Age'] > 30) & (test['Age'] <= 45), 'Age_group']=2\ntest.loc[(test['Age'] > 45) & (test['Age'] <= 60), 'Age_group']=3\ntest.loc[test['Age']  > 60, 'Age_group'] = 4\n\n\n\ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n\ntrain.Alone = pd.DataFrame()\ntest.Alone = pd.DataFrame()\n\ntrain[\"Alone\"] = np.where(train[\"FamilySize\"] < 2, 1, 0)\ntest[\"Alone\"] = np.where(test[\"FamilySize\"] < 2, 1, 0)\n\n\ndef title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\ntrain['Title'] = train['Name'].apply(title)\ntest['Title'] = test['Name'].apply(title)\n\ntrain['Title'] = train['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')\n\ntest['Title'] = test['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')\n\ntrain['Fare_Per_Person'] = train['Fare']\/(train['FamilySize'])\ntest['Fare_Per_Person'] = test['Fare']\/(test['FamilySize'])\n\ntrain.drop([\"Name\"], axis = 1, inplace = True)\ntest.drop([\"Name\"], axis = 1, inplace = True)\n\ntrain[\"Cabin_Ticket\"] = train[\"Cabin\"]+\"_\"+train[\"Ticket\"]\ntest[\"Cabin_Ticket\"] = test[\"Cabin\"]+\"_\"+test[\"Ticket\"]\n\ntrain['Cabin_Ticket_number'] = train['Cabin_Ticket'].apply(lambda x: len(x))\ntrain['Title_grup_count'] = train.groupby(['Cabin_Ticket', 'Title'])['Cabin_Ticket_number'].transform('count')\n\ntest['Cabin_Ticket_number'] = test['Cabin_Ticket'].apply(lambda x: len(x))\ntest['Title_grup_count'] = test.groupby(['Cabin_Ticket', 'Title'])['Cabin_Ticket_number'].transform('count')\n\ntrain['Pclass_Alone_count'] = train.groupby(['Pclass', 'Alone'])['Age'].transform('count')\ntest['Pclass_Alone_count'] = test.groupby(['Pclass', 'Alone'])['Age'].transform('count')\n\ntrain['Pclass_Alone_mean'] = train.groupby(['Pclass', 'Alone'])['Age'].transform('mean')\ntest['Pclass_Alone_mean'] = test.groupby(['Pclass', 'Alone'])['Age'].transform('mean')\n\ntrain['Pclass_Alone_SibSp_count'] = train.groupby(['Pclass', 'Alone'])['SibSp'].transform('count')\ntest['Pclass_Alone_SibSp_count'] = test.groupby(['Pclass', 'Alone'])['SibSp'].transform('count')\n\ntrain['Pclass_Alone_SibSp_mean'] = train.groupby(['Pclass', 'Alone'])['SibSp'].transform('mean')\ntest['Pclass_Alone_SibSp_mean'] = test.groupby(['Pclass', 'Alone'])['SibSp'].transform('mean')\n\ntrain['Fare_group_Cabin_Age_count'] = train.groupby(['Fare_group', 'Cabin'])['Age'].transform('count')\ntest['Fare_group_Cabin_Age_count'] = test.groupby(['Fare_group', 'Cabin'])['Age'].transform('count')\n\ntrain['Fare_group_Cabin_Age_mean'] = train.groupby(['Fare_group', 'Cabin'])['Age'].transform('mean')\ntest['Fare_group_Cabin_Age_mean'] = test.groupby(['Fare_group', 'Cabin'])['Age'].transform('mean')\n\ntrain['Fare_group_Cabin_Fare_Per_Person_mean'] = train.groupby(['Fare_group', 'Cabin'])['Fare_Per_Person'].transform('mean')\ntest['Fare_group_Cabin_Fare_Per_Person_mean'] = test.groupby(['Fare_group', 'Cabin'])['Fare_Per_Person'].transform('mean')\n\ntrain['Fare_group_Cabin_Fare_Per_Person_count'] = train.groupby(['Fare_group', 'Cabin'])['Fare_Per_Person'].transform('count')\ntest['Fare_group_Cabin_Fare_Per_Person_count'] = test.groupby(['Fare_group', 'Cabin'])['Fare_Per_Person'].transform('count')\n\ntrain['Cabin_Ticket_Alone_count'] = train.groupby(['Cabin_Ticket', 'Alone'])['Age'].transform('count')\ntest['Cabin_Ticket_Alone_count'] = test.groupby(['Cabin_Ticket', 'Alone'])['Age'].transform('count')\n\ntrain['Cabin_Ticket_Alone_mean'] = train.groupby(['Cabin_Ticket', 'Alone'])['Age'].transform('mean')\ntest['Cabin_Ticket_Alone_mean'] = test.groupby(['Cabin_Ticket', 'Alone'])['Age'].transform('mean')\n\ntrain['Title_FamilySize_count'] = train.groupby(['Title', 'FamilySize'])['Fare'].transform('count')\ntest['Title_FamilySize_count'] = test.groupby(['Title', 'FamilySize'])['Fare'].transform('count')\n\ntrain['Title_FamilySize_Parch_count'] = train.groupby(['Title', 'FamilySize'])['Parch'].transform('count')\ntest['Title_FamilySize_Parch_count'] = test.groupby(['Title', 'FamilySize'])['Parch'].transform('count')\n\ntrain['Title_FamilySize_mean'] = train.groupby(['Title', 'FamilySize'])['Fare'].transform('mean')\ntest['Title_FamilySize_mean'] = test.groupby(['Title', 'FamilySize'])['Fare'].transform('mean')\n\ntrain['Title_FamilySize_Parch_mean'] = train.groupby(['Title', 'FamilySize'])['Parch'].transform('mean')\ntest['Title_FamilySize_Parch_mean'] = test.groupby(['Title', 'FamilySize'])['Parch'].transform('mean')\n\n\n\ncolumns=['Title_FamilySize_Parch_mean','Fare_group_Cabin_Age_mean', 'Age', 'Parch', 'SibSp', 'Title_FamilySize_mean', 'FamilySize', 'Fare_Per_Person', 'Fare_group_Cabin_Age_mean',\n         'Cabin_Ticket_number', 'Fare_group_Cabin_Fare_Per_Person_count','Fare','Title_grup_count','Pclass_Alone_mean','Pclass_Alone_SibSp_count','Cabin_Ticket_Alone_mean','Title_FamilySize_count','Fare_group_Cabin_Fare_Per_Person_mean']\n\n\nobj_cols=['Alone','Cabin_Ticket','Title', 'Fare_group','Cabin','Pclass','Age_group', \"Embarked\", \"Sex\"]\n\n\nfor col in columns:\n    for feat in obj_cols:\n        train[f'{col}_mean_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('mean')\n        train[f'{col}_max_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('max')\n        train[f'{col}_min_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('min')\n        train[f'{col}_skew_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('skew')\n        train[f'{col}_skew_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('count')\n        \n        \n        \nfor col in columns:\n    for feat in obj_cols:\n        test[f'{col}_mean_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('mean')\n        test[f'{col}_max_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('max')\n        test[f'{col}_min_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('min')\n        test[f'{col}_skew_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('skew')\n        test[f'{col}_skew_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('count')","39fa0415":"def get_redundant_pairs(df):\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations !\")\nprint(get_top_abs_correlations(train.select_dtypes(include=['int32','int64']), 30))","ce64998c":"train.drop([\"Title_FamilySize_Parch_count\", \"Fare_group_Cabin_Fare_Per_Person_count\", \"Pclass_Alone_SibSp_count\"], axis = 1, inplace = True)\ntest.drop([\"Title_FamilySize_Parch_count\", \"Fare_group_Cabin_Fare_Per_Person_count\", \"Pclass_Alone_SibSp_count\"], axis = 1, inplace = True)","2c8b24d2":"train.shape, test.shape","de87e0cc":"categorical_1 = [\"Pclass\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\", \"Fare_group\", \"Age_group\", \"Alone\", \"Title\", \"Cabin_Ticket\"]\n\nfrom sklearn.preprocessing import LabelEncoder\nfor col in train[categorical_1].columns:\n        \n    train[col] = train[col].astype(str)\n    test[col] = test[col].astype(str)\n        \n    le = LabelEncoder()\n    le.fit(list(train[col])+list(test[col]))\n    train[col] = le.transform(train[col])\n    test[col]  = le.transform(test[col])\n        \n    train[col] = train[col].astype('category')\n    test[col] = test[col].astype('category')","056e46c6":"y = train['Survived']\ntrain = train.drop(['Survived'],axis=1)","65030ec8":"nfolds = 15\n\nfolds = KFold(n_splits = nfolds, shuffle=True, random_state=4590)\n\n\nparams = {\n    'num_leaves': 2**5 - 1,\n    'max_depth': 8,\n    'min_data_in_leaf': 50,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 1,\n    'metric': 'auc',\n    'num_threads': 4,\n    'verbose': -1,\n    'objective': 'binary',\n    'metric': 'auc',\n    \"boosting_type\": \"gbdt\"\n}\n\nnum_round = 10000","9c310aa9":"feature_importance_df = np.zeros((train.shape[1], nfolds))\nmvalid = np.zeros(len(train))\npredictions  = np.zeros(len(test))\n\n\naucs = list()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, train.values)):\n    print('----')\n    print(\"fold n\u00b0{}\".format(fold_))\n    \n    x0,y0 = train.iloc[trn_idx], y[trn_idx]\n    x1,y1 = train.iloc[val_idx], y[val_idx]\n    \n    trn_data = lgb.Dataset(x0, label= y0); val_data = lgb.Dataset(x1, label= y1)\n    \n    clf = lgb.train(params, trn_data, \n                    num_round, valid_sets = [trn_data, val_data], \n                    verbose_eval=2000, \n                    early_stopping_rounds = 1000, \n                    categorical_feature = [\"Pclass\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\", \"Fare_group\", \"Age_group\", \"Alone\", \"Title\", \"Cabin_Ticket\"])\n    \n    mvalid[val_idx] = clf.predict(x1, num_iteration=clf.best_iteration)\n    \n    feature_importance_df[:, fold_] = clf.feature_importance()\n    \n    predictions += clf.predict(test, num_iteration=clf.best_iteration) \/ folds.n_splits\n    aucs.append(clf.best_score['valid_1']['auc'])\n    \n    \n    \nprint('-' * 80)\nprint('-' * 70)\nprint('-' * 60)\nprint('-' * 50)\nprint('-' * 40)\n\nprint('Mean AUC:', np.mean(aucs).round(3))","1fa5d959":"ximp = pd.DataFrame()\nximp['feature'] = train.columns\nximp['importance'] = feature_importance_df.mean(axis = 1)\n\nplt.figure(figsize=(10,8))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=ximp.sort_values(by=\"importance\",\n                                           ascending=False).head(45))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","fbac6bed":"**Thank You**","003370f3":"# Missing Values","b7e2ec5c":"## Numerical EDA","340bf8bf":"# Feature Engineering","b123d633":"Please upvote if you find it educational :)","87dc1db3":"# Import libraries","f45859c2":"# Drop Correlated Features","a666f31a":"# EDA","a41e25a0":"# Label Encoding","1284f152":"## Categorical EDA","fa3c2fca":"## Handling Missing Values - Imputation","7b0ae398":"# LGBM Model","477bc511":"# Import Data"}}