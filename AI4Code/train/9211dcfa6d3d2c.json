{"cell_type":{"02b65786":"code","2150c554":"code","6e14668d":"code","f738154d":"code","351d4541":"code","fa106432":"code","95c0c95c":"code","90f035d2":"code","5a6c76dc":"code","dcce9c47":"code","2d4d33b4":"code","d3b7edad":"code","f001e8ef":"code","8fd06909":"code","a2876a12":"code","744f5234":"code","a443541a":"code","70729d48":"code","88473de6":"code","bdf02a6e":"code","f6fe5303":"code","68c66207":"code","2dda3984":"code","900d9d1c":"code","e793ebb2":"code","77b7e283":"code","f0a0b65b":"code","03799d70":"code","e7fc5724":"code","42a63e0a":"code","74ca7208":"code","1b901387":"code","05101f7b":"code","0fbeb993":"code","5c304353":"markdown","1a251483":"markdown","31bd2c3c":"markdown","60b98caf":"markdown","ebe5feed":"markdown","379478bd":"markdown","59db8af3":"markdown","932e03c9":"markdown","490660a7":"markdown","f67ad544":"markdown","f2fab456":"markdown","3625ed45":"markdown","33bf2d2b":"markdown","d5d5375b":"markdown","c26ad014":"markdown","eab9033f":"markdown","37592d51":"markdown"},"source":{"02b65786":"import pandas as pd\nimport numpy as np\nimport ast\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')","2150c554":"taxonomy = pd.read_csv('\/kaggle\/input\/identifying-potential-online-customers\/taxonomy.csv')\nd = {\n    'user_uid': [],\n    'attempt_id': [],\n    'attempt_answers': []\n}\n\nfor i in taxonomy.Tag:\n    P = str(i)+'_P'\n    A = str(i)+'_A'\n    d[P] = []\n    d[A] = []","6e14668d":"data = pd.read_csv('\/kaggle\/input\/identifying-potential-online-customers\/data.csv')\n\nfor index, i in data.iterrows():\n    d['user_uid'].append(i.user_uid)\n    d['attempt_answers'].append(i.attempt_answers)\n    d['attempt_id'].append(i.attempt_id)\n    score = ast.literal_eval(i.score)[0]\n    for tag in taxonomy.Tag:\n        P = str(tag)+'_P'\n        A = str(tag)+'_A'\n        d[P].append(score.get(P, 0))\n        d[A].append(score.get(A, 0))","f738154d":"df = pd.DataFrame(d)","351d4541":"df.head()","fa106432":"df_ = []\nfor j in range(len(data)):\n    Smalltrunk = []\n    for i in json.loads(data.score[j]):\n        Smalltrunk.append(pd.DataFrame.from_dict(i,orient='index'))\n        df_.append(Smalltrunk[0].index.values)\n\nflatList = [val for sublist in df_ for val in sublist]","95c0c95c":"Test = pd.DataFrame(flatList)\nTest.columns = [\"Test\"]\nTest.Test.value_counts().head()","90f035d2":"Test.Test.value_counts()","5a6c76dc":"df.groupby('attempt_answers').size()","dcce9c47":"CountStatus = pd.value_counts(Test['Test'].values, sort=True)\n\nCountStatus.plot.barh()","2d4d33b4":"taxonomy = pd.read_csv(\"\/kaggle\/input\/identifying-potential-online-customers\/taxonomy.csv\")\n\ntable_taxonomy = pd.concat([taxonomy.Tag, pd.DataFrame(taxonomy.Label.str.split(\"\u00a7\").tolist())], axis=1)","d3b7edad":"table_taxonomy","f001e8ef":"taxonomy_ = pd.concat([taxonomy.Tag, pd.DataFrame(taxonomy.Label.str.split(\"\u00a7\").tolist())], axis=1)\n\nCountStatus = pd.value_counts(taxonomy_.iloc[:,2].values, sort=True)\n\nCountStatus.plot.bar()","8fd06909":"#### An experiment to reduce dimentionality\n\n#If you look at the chart above, several questions are being asked directly about holidays. Perhaps \n#that can be leveraged by perhaps considering (primarily) those variables that related to holidays. \n#Of the Tags in taxonomy I created a list of tags manually into a separate csv file, select_taxonomy.csv.\n#The idea is to consider those variables which explicitly gauge the interest of users in taking a holiday. \n#I then created this code below to extract from our dataframe (variable table) only those variables whose\n#tags are listed in select_taxonomy.csv. \n#Unfortunately I did not have time to test dimensionality reduction based on this assumption.\n\n\n#taxonomy = pd.read_csv('select_taxonomy.csv')\n#stringP = \"_P\"\n#stringA = \"_A\"\n\n#selected_variables = ['user_uid']\n#for i in taxonomy.Tag:\n    #selected_variables.append(str(i)+stringP)\n    #selected_variables.append(str(i)+stringA)\n","a2876a12":"##### Tried dimensionality reduction - PCA but it gave worse results\n##### So using all the varilables as they are\n\n#from sklearn.decomposition import PCA\n#pca = PCA(n_components=2)\n#principalComponents = pca.fit_transform(totalMerged)\n#totalMerged = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])","744f5234":"conversion = pd.read_csv(\"\/kaggle\/input\/identifying-potential-online-customers\/conversion.csv\")\nconversion.head()","a443541a":"conversion.shape","70729d48":"MergedConvertedData = pd.merge(df, conversion, on='user_uid')\n\nMergedConvertedData.head()","88473de6":"MergedConvertedData.shape\n# this is 535 and conversion is 520 rows. means 15 are either duplicates or they are users taking quiz again. deal with this","bdf02a6e":"MergedConvertedData = MergedConvertedData.drop_duplicates(subset = 'user_uid',keep=\"first\")","f6fe5303":"MergedConvertedData.shape\n# fixed now","68c66207":"totalMerged = MergedConvertedData.drop(['attempt_id','user_uid','attempt_answers','conv'],axis=1)","2dda3984":"MergedConvertedData.head()","900d9d1c":"test_threshold = int(len(totalMerged)*0.60)\nX_train = totalMerged.iloc[:test_threshold,]\nX_test = totalMerged.iloc[test_threshold:,]\ny_test = MergedConvertedData.iloc[test_threshold:,-1]","e793ebb2":"X_train.shape","77b7e283":"X_test.shape","f0a0b65b":"from sklearn import svm\n\noneclass = svm.OneClassSVM(nu=0.1, kernel='linear')\n\noneclass.fit(X_train)\ny_pred = oneclass.predict(X_test)\n\ny_pred = np.where(y_pred==-1, 0, y_pred)\n\nfrom sklearn.metrics import accuracy_score\n\nprint('Accuracy of One class SVM model is ',np.round(accuracy_score(y_test, y_pred)*100,2))\n","03799d70":"pd.Series(abs(oneclass.coef_[0]), index=X_train.columns).nlargest(10).plot(kind='barh', title='OneClassSVM: Feature Importances')","e7fc5724":"from sklearn.ensemble import IsolationForest\nclf = IsolationForest(n_estimators=10, contamination=0.2, warm_start=True)\nclf.fit(X_train)\n\ny_pred = clf.predict(X_test)\n\ny_pred = np.where(y_pred==-1, 0, y_pred)\n\nprint('Accuracy of Isolation forest model is ',np.round(accuracy_score(y_test, y_pred)*100,2))","42a63e0a":"submission = pd.read_csv(\"\/kaggle\/input\/identifying-potential-online-customers\/submission.csv\")\nsubm = pd.merge(submission, df, on='user_uid')\nsubm.head()","74ca7208":"testSubmission = subm.drop(['attempt_id','user_uid','attempt_answers','p_conv'],axis=1)\n\nX_test_ = pd.DataFrame({\"p_conv\":oneclass.predict(testSubmission)})\nX_test_['user_uid'] = subm.user_uid\n\n#X_test_.to_csv('submission_oneclass.csv',index=False)","1b901387":"testSubmission_IF = subm.drop(['attempt_id','attempt_answers','user_uid','p_conv'],axis=1)\n\nX_test_IF = pd.DataFrame({\"p_conv\":clf.predict(testSubmission_IF)})\nX_test_IF['user_uid'] = subm.user_uid\n\n#X_test_IF.to_csv('submission_IF.csv',index=False)","05101f7b":"from sklearn.neighbors import LocalOutlierFactor\n\nlof = LocalOutlierFactor(n_neighbors=5, contamination=0.1, novelty=True) \n\nlof.fit(X_train)\n\ny_pred=lof.predict(X_test)\n\nprint('Accuracy of LocalOutlierFactor model is ',np.round(accuracy_score(y_test, y_pred)*100,2))","0fbeb993":"testSubmissionLOF = subm.drop(['attempt_id','attempt_answers','user_uid','p_conv'],axis=1)\n\nX_test_LOF = pd.DataFrame({\"p_conv\":lof.predict(testSubmissionLOF)})\nX_test_LOF['user_uid'] = subm.user_uid\n\n#X_test_LOF.to_csv('submission_LOF.csv',index=False)","5c304353":"merging conversion column to dataframe of segment and user id, by unique identifier \"user id\"","1a251483":"### JSON to Python Dataframe\nWe have data in JSON like format, so we need to handle it using JSON library,\n\nFirstly we are converting class string to json format and then we are appending the key and values in new lists and then we are creating and puting into new dataframe.\n\nThen for count oprtation we are putting it to flatList","31bd2c3c":"### Explorartory Data Analysis","60b98caf":"### Splitting dataset to train and test","ebe5feed":"### Model 2: One Class IsolationForest model","379478bd":"Extracting segment id from taxonomny tag","59db8af3":"### Data transformation","932e03c9":"### Data Loading","490660a7":"spliting data for training and testing so that we can validate \nthe model and predict on test set to get the accuracy of model","f67ad544":"# Anomaly detection for identifying potential customers","f2fab456":"#### Feature Importance","3625ed45":"### Model 1: One class SVM  model","33bf2d2b":"Handling the deleminiter and spliting data to Pandas dataframe","d5d5375b":"transforming json type data format to pandas dataframe to get the count and for further\ndata analysis. ","c26ad014":"## Model 3: Local Outlier Factor model","eab9033f":"### Submission file","37592d51":"### Holiday operator"}}