{"cell_type":{"2785dcf7":"code","98139cc1":"code","52a0fc1e":"code","43882af5":"code","c6cb7f00":"code","748461ca":"code","87046c1b":"code","973c700d":"code","80bd73b6":"code","5dc35e02":"code","31db3a5f":"code","1887a93b":"code","0f00bbf1":"code","8b50b8cf":"code","6f2522e5":"code","2c8779c1":"code","eb961e73":"code","f6b097ab":"code","601f8752":"markdown"},"source":{"2785dcf7":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FormatStrFormatter\nimport datatable as dt\n\n","98139cc1":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","52a0fc1e":"%%time\ntrain = dt.fread('..\/input\/song-popularity-prediction\/train.csv').to_pandas().drop('id', axis=1)\ntrain = reduce_memory_usage(train)\ntest = dt.fread('..\/input\/song-popularity-prediction\/test.csv').to_pandas().drop('id', axis=1)\ntest = reduce_memory_usage(test)\nss = dt.fread('..\/input\/song-popularity-prediction\/sample_submission.csv').to_pandas()\nss = reduce_memory_usage(ss)","43882af5":"print(train.shape)\nprint(test.shape)","c6cb7f00":"display(train.isna().sum().sum())\ndisplay(test.isna().sum().sum())","748461ca":"train.describe().T","87046c1b":"train.head(10)","973c700d":"#Lets find traget variable distribution\ntarget_varibale=train['song_popularity']\npal = ['#5ddef4','#8e99f3']\nplt=sns.countplot(x=train.song_popularity,palette=pal)\nplt.set_title('Target  distribution', fontsize=20, y=1.05)","80bd73b6":"categorical_features=[]\nnumerical_features=[]\nfor i in train.columns:\n    if train[i].dtype=='float16':\n#         print('yes')\n        numerical_features.append(i)\n    elif i!='target':\n        categorical_features.append(i)\n#         print('no')","5dc35e02":"display(len(categorical_features))\ndisplay(len(numerical_features))","31db3a5f":"df_num_features = pd.concat([train[numerical_features], train['song_popularity']], axis=1) # still writing even though train and this frame will be same","1887a93b":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(16 , 16))\ncorr = df_num_features.sample(10000, random_state=2021).corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax, square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(0,255,sep=77, as_cmap=True),vmax=0.5, vmin=-0.5,\n        cbar_kws={\"shrink\": .85}, mask=mask )\nax.set_title('Correlation heatmap: Numerical features', fontsize=24, y= 1.05)\nplt.show()","0f00bbf1":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nX = train.drop('song_popularity',axis=1)\ny = train['song_popularity']\n#In general a good idea is to scale the data\nscaler = StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X)    ","8b50b8cf":"!pip install  autoxgb ","6f2522e5":"OUTPUT='.\/'","2c8779c1":"!autoxgb train \\\n--train_filename ..\/input\/song-popularity-prediction\/train.csv \\\n--test_filename ..\/input\/song-popularity-prediction\/test.csv \\\n--id IDX \\\n--target song_popularity \\\n--task classification \\\n--num_folds 5 \\\n--time_limit 3600 \\\n--output OUTPUT \\\n--use_gpu","eb961e73":"pred_df=dt.fread('.\/OUTPUT\/test_predictions.csv').to_pandas()\n\nfinal_class=[]\nfor i in pred_df.iterrows():\n    current_class=1\n    if i[1][1] >i[1][2]:\n        current_class=0\n        final_class.append(current_class)\n    else:\n         final_class.append(current_class)\n        \n    ","f6b097ab":"ss = dt.fread('..\/input\/song-popularity-prediction\/sample_submission.csv').to_pandas()\nss['song_popularity'] = final_class\nss.to_csv('sub.csv', index=False)\nss.head()","601f8752":"# \ud83d\udd25Song Popularity Prediction \n## This Notebook is purely the implementation of the autoxgb written by [Abhishek](https:\/\/www.kaggle.com\/abhishek)\n### I have done a simple EDA and trained using Autoxgb, loaded the model and created the submission file.\n### Note- No FE was done and it's a single model.\n### Huge thanks to [Abhishek](https:\/\/www.kaggle.com\/abhishek) for creating the competition, optuna hp tune notebook out soon along with 14model baseline."}}