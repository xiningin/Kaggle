{"cell_type":{"7a2751d4":"code","084be93d":"code","bc21364c":"code","4344f384":"code","c3c6922c":"code","6d698d98":"code","d48f37c8":"code","0e86db4d":"code","71f78424":"code","8bfbe04d":"code","3e0b3e02":"code","2f085d33":"code","c1555ef3":"code","53079484":"code","0cd9231c":"code","1380584e":"code","316d8713":"code","b039d04c":"code","010e0c55":"code","35b1b3a8":"code","c67badd1":"code","e6be2f9a":"code","be05662a":"code","d91767fc":"code","1a0d10a4":"code","df9761ea":"code","1b214836":"code","f8290a55":"code","07585144":"code","896b9d30":"code","60d89e13":"code","ed8430a3":"code","8f6820a0":"code","a5f6c802":"code","435046bb":"code","afe1c5fd":"code","18d23631":"code","147b6492":"code","08b5a315":"code","e35b7b02":"code","955202e8":"code","22b0193f":"code","8c1a58c5":"code","04ab5e58":"code","5f3359eb":"code","d5cdefcb":"code","b56ce56c":"code","6f19b082":"code","ffe56d2b":"code","4549d515":"code","100444e0":"code","c26728ea":"code","46d7d4a0":"code","ff607c15":"code","b69f7e23":"code","7b6b6a6c":"code","701bd1ae":"code","1441fd47":"code","06e7416a":"code","a2f337ac":"code","425494f5":"code","c0e21509":"code","0959db14":"code","bf2dc588":"code","81ef8bf5":"code","41cef522":"code","73dad39d":"code","c1107305":"code","bf99e952":"code","47445877":"code","4637afe3":"code","84cca58f":"code","2acd43d4":"code","d19e1c36":"code","4613baeb":"code","7a5cf5f4":"code","0ec66999":"code","a1fc2fbf":"code","6808f713":"code","67ab4c3d":"code","60c427c0":"code","d4da3679":"code","9a97aa8b":"code","389e06c7":"code","64078dc4":"code","34427c06":"code","a025c1c4":"code","33959ec8":"code","dbab5ce9":"code","4b47078b":"code","bba0cddc":"code","9d5a6b0e":"code","d1befff1":"code","62a6a287":"code","bd9c4fa4":"code","dc012880":"code","1c131098":"code","19fc5683":"code","b2b2b535":"code","b21a9337":"code","3179537a":"code","b726d5f1":"markdown","0af2221d":"markdown","7a17023b":"markdown","7f92349a":"markdown","abbc36c9":"markdown"},"source":{"7a2751d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNetCV, Lasso\nfrom sklearn.metrics import mean_squared_error, SCORERS\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost.sklearn import XGBRegressor\nfrom tqdm.notebook import tqdm\nimport os\n\n\nimport warnings\nfrom datetime import datetime\n    \n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n","084be93d":"pd.set_option('max_columns', None)\npd.set_option('max_rows', None)","bc21364c":"print(os.listdir(\"\/kaggle\/input\/willow-house-pricing-v2\"))","4344f384":"# Import train & test data \ndf_train = pd.read_csv('\/kaggle\/input\/willow-house-pricing-v2\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/willow-house-pricing-v2\/test (1).csv')\n#sample_submission = pd.read_csv('\/kaggle\/input\/willow-real-estate\/sample_submission.csv')\n\ndf_train['df_train_test'] = 1\ndf_test['df_train_test'] = 0\nall_data = pd.concat([df_train,df_test])\n","c3c6922c":"df_train.info()","6d698d98":"df_train = df_train[df_train['price'] > 0]","d48f37c8":"missingno.matrix(df_train, figsize = (30,10))","0e86db4d":"df_train = df_train.dropna(subset=['coord_X', 'coord_Y']) ","71f78424":"df_train.info()","8bfbe04d":"rows, cols = df_train.shape\nprint(f'Training Dataset\\n-------\\ncolumns: {cols}\\nrows: {rows}')\ncat_cols = df_train.loc[:, df_train.columns != 'price'].select_dtypes(include=['object']).columns\nnum_cols = df_train.loc[:, df_train.columns != 'price'].select_dtypes(exclude=['object']).columns\nprint(f'categorical columns: {len(cat_cols)}\\nnumeric columns: {len(num_cols)}\\n\\n=================\\n')\n\nrows, cols = df_test.shape\nprint(f'Test Dataset\\n-------\\ncolumns: {cols}\\nrows: {rows}')\ncat_cols = df_test.loc[:, df_test.columns != 'price'].select_dtypes(include=['object']).columns\nnum_cols = df_test.loc[:, df_test.columns != 'price'].select_dtypes(exclude=['object']).columns\nprint(f'categorical columns: {len(cat_cols)}\\nnumeric columns: {len(num_cols)}')","3e0b3e02":"####################  NULLS AND DROPPING COLUMNS ############################","2f085d33":"nulls = {}\n\nfor col in df_train.columns:\n    nulls[col] = (1-(len(df_train[df_train[col].isna()][col]) \/ df_train.shape[0]))\n\nlabels = []\nvals = []\n\nfor k, v in nulls.items():\n    if v < 1.0:\n        labels.append(k)\n        vals.append(v)\n\n_, ax = plt.subplots(figsize=(12,5))\n\nsns.barplot(y=vals, x=labels, color='lightskyblue')\nax.set_xticklabels(labels=labels, rotation=45)\nplt.title('% non-null values by columns')\nax.set_xlabel('columns')\nax.set_ylabel('%')\nplt.show()","c1555ef3":"df_train.columns","53079484":"for i in num_cols:\n    sns.boxplot(df_train[i])\n    plt.title(i)\n    plt.show()","0cd9231c":"Q1 = df_train[\"living_m2\"].quantile(0.25)\n\nQ3 = df_train[\"living_m2\"].quantile(0.75)\n\nIQR = Q3 - Q1\n\nLower_Fence = Q1 - (3 * IQR)\n\nUpper_Fence = Q3 + (3 * IQR)","1380584e":"df_train = df_train[~((df_train[\"living_m2\"] < Lower_Fence) |(df_train[\"living_m2\"] > Upper_Fence))]","316d8713":"Q1 = df_train[\"living_vs_neighbors\"].quantile(0.25)\n\nQ3 = df_train[\"living_vs_neighbors\"].quantile(0.75)\n\nIQR = Q3 - Q1\n\nLower_Fence = Q1 - (3 * IQR)\n\nUpper_Fence = Q3 + (3 * IQR)","b039d04c":"df_train = df_train[~((df_train[\"living_vs_neighbors\"] < Lower_Fence) |(df_train[\"living_vs_neighbors\"] > Upper_Fence))]","010e0c55":"Q1 = df_train[\"lot_vs_neighbors\"].quantile(0.25)\n\nQ3 = df_train[\"lot_vs_neighbors\"].quantile(0.75)\n\nIQR = Q3 - Q1\n\nLower_Fence = Q1 - (3 * IQR)\n\nUpper_Fence = Q3 + (3 * IQR)","35b1b3a8":"df_train = df_train[~((df_train[\"lot_vs_neighbors\"] < Lower_Fence) |(df_train[\"lot_vs_neighbors\"] > Upper_Fence))]","c67badd1":"Q1 = df_train[\"lot_m2\"].quantile(0.25)\n\nQ3 = df_train[\"lot_m2\"].quantile(0.75)\n\nIQR = Q3 - Q1\n\nLower_Fence = Q1 - (3 * IQR)\n\nUpper_Fence = Q3 + (3 * IQR)","e6be2f9a":"df_train = df_train[~((df_train[\"lot_m2\"] < Lower_Fence) |(df_train[\"lot_m2\"] > Upper_Fence))]","be05662a":"Q1 = df_train[\"baths\"].quantile(0.25)\n\nQ3 = df_train[\"baths\"].quantile(0.75)\n\nIQR = Q3 - Q1\n\nLower_Fence = Q1 - (3 * IQR)\n\nUpper_Fence = Q3 + (3 * IQR)","d91767fc":"df_train = df_train[(df_train[\"baths\"] <= 4)]","1a0d10a4":"df_train = df_train[~((df_train[\"bedrooms\"] == 0)|(df_train[\"bedrooms\"] == 8))]","df9761ea":"df_train.info()","1b214836":"# should I remove variables without coordinates?\n# Plot graphic of missing values\nx = list(range(1,24))\nmissingno.matrix(df_train.iloc[:,x], figsize = (20,3))","f8290a55":"# find which columns are missing values, and how many\ndef find_missing_values(df,columns):\n    missing_vals = {}\n    df_length = len(df)\n    for i in df.columns:\n        total_columns_values = df[i].value_counts().sum()\n        missing_vals[i] = df_length - total_columns_values\n    return missing_vals     \n\nmissing_values = find_missing_values(df_train,columns = df_train.columns)\nmissing_values","07585144":"to_drop = []\n\nfor k, v in nulls.items():\n    if v < 0.6:\n        to_drop.append(k)\n        \n#make a copy\ndf_train_c = df_train.drop(to_drop, axis=1)\n\nrows, cols = df_train_c.shape\nprint(f'columns: {cols}\\nrows: {rows}')\ncat_cols = df_train_c.loc[:, df_train_c.columns != 'price'].select_dtypes(include=['object']).columns\nnum_cols = df_train_c.loc[:, df_train_c.columns != 'price'].select_dtypes(exclude=['object']).columns\nprint(f'categorical columns: {len(cat_cols)}\\nnumeric columns: {len(num_cols)}')","896b9d30":"df_train_c = df_train.copy()","60d89e13":"X = df_train_c.loc[:, df_train_c.columns != 'price']\ny = df_train_c.loc[:, df_train_c.columns == 'price']\n\n\nX = X.loc[:, X.columns != 'id']","ed8430a3":"# replace df_train_c! missing values with most frequent\nsi = SimpleImputer(strategy='most_frequent')\n\nfor k,v in nulls.items():\n    if (v < 1) and (k not in to_drop):\n        df_train_c[k] = si.fit_transform(df_train_c[k].values.reshape(-1,1))","8f6820a0":"# split between numeric and \ndf_train_num = df_train_c.select_dtypes(include=[np.number])\ndf_train_cat = df_train_c.select_dtypes(exclude=[np.number])","a5f6c802":"# distributions for all variables \nfor i in df_train_c.columns:\n    plt.hist(df_train_c[i])\n    plt.title(i)\n    plt.show()\n    \n    # remove id","435046bb":"# charts for categorical variables\nfor i in df_train_cat.columns:\n    sns.barplot(df_train_cat[i].value_counts().index,df_train_cat[i].value_counts()).set_title(i)\n    plt.show()\n    \n    # remove city","afe1c5fd":"#####################   Data Skewness #############","18d23631":"fig, ax = plt.subplots(1,2, figsize=(15,5))\n\nsns.distplot(y, fit=stats.norm, ax=ax[0], kde=False)\nstats.probplot(y.price,  plot=ax[1])\nplt.show()\nprint(f'Fisher-Pearson coeficient of skewness: {stats.skew(y.price.values):.2f}')","147b6492":"numerical_columns = X.loc[:, ~X.columns.isin(['city', 'house_state_index', 'basement', 'viewsToPOI','view_quality','dow','house_quality_index', 'floors', 'bedrooms', 'baths', 'id'])].select_dtypes(include=['int', 'float']).columns\nsk = X[numerical_columns].apply(lambda x: stats.skew(x.dropna())).to_frame('Fisher-Pearson Coef')\nskw_cols = list(sk[abs(sk['Fisher-Pearson Coef']) > 0.5].index)\nsk[abs(sk['Fisher-Pearson Coef']) > 0.5]","08b5a315":"lmbda = 0.0\nX[skw_cols] = X[numerical_columns].loc[:, X[numerical_columns].columns.isin(skw_cols)].apply(lambda x: stats.boxcox(1+x, lmbda=lmbda))","e35b7b02":"y = y.apply(lambda x: stats.boxcox(1+x, lmbda=lmbda))","955202e8":"sk['Fisher-Pearson Coef (After)'] = X[numerical_columns].apply(lambda x: stats.skew(x))\nsk[sk.index.isin(skw_cols)]","22b0193f":"fig, ax = plt.subplots(1,2, figsize=(15,5))\n\nsns.distplot(y, fit=stats.norm, ax=ax[0], kde=False)\nstats.probplot(y.price,  plot=ax[1])\nplt.show()\nprint(f'Fisher-Pearson coeficient of skewness: {stats.skew(y.price.values):,.2f}')","8c1a58c5":"# we filtered the num variables that were skewed  (> 0.5) and transformed them","04ab5e58":"# ########## CONTINUOUS VARIABLES ##########","5f3359eb":"X_disc = X.loc[:,~(X.columns.isin([ 'house_quality_index', 'floors', 'bedrooms','baths', 'living_vs_neighbors','dow','month','df_train_test'])) &\n                  (X.columns.isin(numerical_columns))]\n\nX_disc['y'] = y\n\n_, ax = plt.subplots(figsize=(25,15))\n\nsns.heatmap(X_disc.corr(), annot=True, cbar=False, cmap='YlGnBu')\nplt.show()","d5cdefcb":"mask = (abs(X_disc.corr()['y'] >= 0.15))\ncorr_variables = X_disc.corr()['y'][mask]\ncorr_variables = list(corr_variables[corr_variables.index != 'y'].index)\n\ncorr_variables","b56ce56c":"_, ax = plt.subplots(figsize=(15,8))\n\nsns.heatmap(X_disc.loc[:, corr_variables].corr(), annot=True, cbar=True, cmap='YlGnBu')\nplt.show()","6f19b082":"mask = ((abs(X_disc.loc[:, corr_variables].corr()) > 0.8) & \n        (X_disc.loc[:, corr_variables].corr() != 1.0))\ncols = list(X_disc.loc[:, corr_variables].corr()[mask].dropna(how='all', axis=1).columns)\n\nto_remove = []\n\nfor i in range(0,len(cols),2):\n    to_remove.append(cols[i])\n    \ncontinous_features = list(set(corr_variables) - set(to_remove))\ncontinous_features","ffe56d2b":"####### DISCRETE ###############","4549d515":"X_discrete = X.loc[:, X.columns.isin(['house_quality_index', 'floors', 'bedrooms','baths', 'living_vs_neighbors','month'])]\nX_discrete['y'] = y\n\n_, ax = plt.subplots(figsize=(15,8))\n\nsns.heatmap(X_discrete.corr('spearman'), annot=True, cmap='YlGnBu')\nplt.show()","100444e0":"mask = (((abs(X_discrete.corr('spearman')['y']) >= 0.2) &\n       (X_discrete.corr('spearman')['y'] != 1.0)))\nX_discrete_cols = list(X_discrete.corr('spearman')['y'][mask].index)\ndiscrete_features = list(set(X_discrete_cols))","c26728ea":"discrete_features","46d7d4a0":"X_num = X.loc[:, X.columns.isin(continous_features + discrete_features)]\nX_num['y'] = y","ff607c15":"\nsns.pairplot(x_vars=continous_features[:int(len(continous_features)\/2)],\n             y_vars=['y'],\n            data=X_num,\n            height=3.5)\n\nsns.pairplot(x_vars=continous_features[int(len(continous_features)\/2):],\n             y_vars=['y'],\n            data=X_num,\n            height=3.5)\n\nsns.pairplot(x_vars=discrete_features,\n             y_vars=['y'],\n            data=X_num,\n            height=3.5)\n\nplt.show()","b69f7e23":"X_num.head()","7b6b6a6c":"df_train.house_state_index.value_counts()","701bd1ae":"X.head()","1441fd47":"le = LabelEncoder()","06e7416a":"X['viewsToPOI']  = X[['viewsToPOI']].apply(lambda x: le.fit_transform(x.values))\nX['house_state_index']  = X[['house_state_index']].apply(lambda x: le.fit_transform(x.values))\nX['basement']  = X[['basement']].apply(lambda x: le.fit_transform(x.values))\nX['view_quality']  = X[['view_quality']].apply(lambda x: le.fit_transform(x.values))\nX['dow']  = X[['dow']].apply(lambda x: le.fit_transform(x.values))","a2f337ac":"#dico >>  'house_quality_index', 'floors', 'bedrooms','baths', 'living_vs_neighbors','dow','month'","425494f5":"#dico = ['house_quality_index', 'floors', 'bedrooms','baths', 'living_vs_neighbors','month']","c0e21509":"dico = []","0959db14":"#categoricals = ['viewsToPOI','house_state_index','basement','view_quality']","bf2dc588":"## Categorical","81ef8bf5":"categoricals_columns = ['viewsToPOI','house_state_index','basement','view_quality','dow']","41cef522":"X_categoricals = X.loc[:, X.columns.isin(['viewsToPOI','house_state_index','basement','view_quality','dow'])]\n","73dad39d":"X_categoricals.head()","c1107305":"X_categoricals['y'] = y","bf99e952":"corr = []\n\n\nfor col in tqdm(X_categoricals.columns):\n    cat = X_categoricals[col].unique()\n   \n    y_avg = []\n    n_cat = []\n    for c in cat:\n        y_avg.append(X_categoricals[X_categoricals[col] == c].y.mean())\n        n_cat.append(len(X_categoricals[X_categoricals[col] == c]))\n    \n    y_total_avg = np.sum(np.multiply(y_avg,n_cat) \/ np.sum(n_cat))\n\n    numerator = np.sum((np.multiply(n_cat,np.power(np.subtract(y_avg, y_total_avg),2))))\n    denominator = np.sum(np.power(np.subtract(X_categoricals.y, y_total_avg),2))\n\n    if denominator == 0:\n        eta = 0.0\n        corr.append((col, eta))\n    else:\n        eta = np.sqrt(numerator\/denominator)\n        corr.append((col, eta))\n        \nprint(corr)","47445877":"categoricals_columns = []\n\nfor el in corr:\n    if el[1] >= 0.1:\n        categoricals_columns.append(el[0])\n\ncategoricals_columns.pop(len(categoricals_columns)-1)\ncategoricals_columns","4637afe3":"X_cat = X_categoricals[categoricals_columns]\nX_cat['y'] = y","84cca58f":"sns.pairplot(x_vars=categoricals_columns[:int(len(categoricals_columns)\/2)],\n             y_vars=['y'],\n            data=X_cat,\n            height=3.5)\nsns.pairplot(x_vars=categoricals_columns[int(len(categoricals_columns)\/2):],\n             y_vars=['y'],\n            data=X_cat,\n            height=3.5)\nplt.show()","2acd43d4":"discrete_features","d19e1c36":"continous_features","4613baeb":"categoricals_columns","7a5cf5f4":"dico","0ec66999":"features = categoricals_columns + continous_features + discrete_features + dico\nfeatures","a1fc2fbf":"X = X[features]\ny = y\nX_train, X_test, y_train, y_test = train_test_split(X.loc[:,X.columns != 'y'], y, test_size=0.3)\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3)","6808f713":"X_train.info()","67ab4c3d":"n_fold = 5\n\ndef rmseModel(m):\n    kf = KFold(n_splits=n_fold, random_state=0, shuffle=True).get_n_splits()\n    rmse = np.sqrt(-cross_val_score(m, X, y, scoring='neg_mean_squared_error', cv=kf))\n    return rmse","60c427c0":"XGBBaselie = XGBRegressor(objective='reg:squarederror')\nXGBBaselie.fit(X_test, y_test)\npred = XGBBaselie.predict(X_val)\n\nrmseBaseline = np.sqrt(mean_squared_error(pred, y_val.values))\nprint(f'Baseline RMSE: {rmseBaseline}')","d4da3679":"######### OLS ##########","9a97aa8b":"ols_reg = LinearRegression()\nols_rge_scores = rmseModel(ols_reg)\nprint(f'OLS Reg RMSE, mean: {np.mean(ols_rge_scores)}, stdv: {np.std(ols_rge_scores)}')","389e06c7":"######### RIDGE ##########","64078dc4":"GD = False","34427c06":"if GD:\n    print('Running Grid Search for model tunning')\n    params = {'alpha': [0.1,0.3,0.5,0.7,0.9],\n             'solver': ['auto', 'svd', 'cholesky', 'lsqr']}\n    ridge_reg = Ridge()\n    gs = GridSearchCV(ridge_reg, params, cv=5)\n    gsf = gs.fit(X_train, y_train).best_params_\nelse:\n    gsf = {'alpha': 0.9, 'solver': 'auto'}\n\nridge_reg = Ridge(**gsf)\nridge_reg_scores = rmseModel(ridge_reg)\nprint(f'Ridge Reg RMSE, mean: {np.mean(ridge_reg_scores)}, stdv: {np.std(ridge_reg_scores)}')","a025c1c4":"######## ELASTIC NET ########","33959ec8":"if GD:\n    print('Running Grid Search for model tunning')\n    params = {'l1_ratio': [.1, .5, .7, .9, .92, .95, .99, 1],\n             'n_alphas': [10,15,50, 100],\n              'normalize': [True, False],\n              'max_iter': [5,10,50,100],\n              'tol': [0.001, 0.0001, 0.00001]\n                }\n    el_reg = ElasticNetCV()\n    gs = GridSearchCV(el_reg, params, cv=5, n_jobs=-1, verbose=1)\n    gsf = gs.fit(X_train, y_train).best_params_\nelse:\n    gsf = {'l1_ratio': 0.9,\n         'max_iter': 50,\n         'n_alphas': 50,\n         'normalize': True,\n         'tol': 0.0001}\n\n    \nel_reg = ElasticNetCV(**gsf)\nel_reg_scores = rmseModel(el_reg)\nprint(f'Elastic Net Reg RMSE, mean: {np.mean(el_reg_scores)}, stdv: {np.std(el_reg_scores)}')","dbab5ce9":"### GRADIENT BOOST #########","4b47078b":"if GD:\n    print('Running Grid Search for model tunning')\n    params = {'min_samples_split': [80],\n             'min_samples_leaf': [25],\n             'max_depth':[9],\n            'max_features': [4],\n             'subsample': [0.8],\n             'n_estimators': [2500],\n             'learning_rate': [0.005],\n             'subsample':[0.87]}\n    GB = GradientBoostingRegressor()\n    gs = GridSearchCV(GB, param_grid=params, cv=5, n_jobs=-1, verbose=1)\n    gsf = gs.fit(X_train, y_train).best_params_\nelse:\n    gsf = {'learning_rate': 0.005,\n         'max_depth': 9,\n         'max_features': 4,\n         'min_samples_leaf': 25,\n         'min_samples_split': 80,\n         'n_estimators': 2500,\n         'subsample': 0.87}\n\nGB_reg = GradientBoostingRegressor(**gsf)\nGB_reg_scores = rmseModel(GB_reg)\nprint(f'GB Reg, mean: {np.mean(GB_reg_scores)}, stdv: {np.std(GB_reg_scores)}')","bba0cddc":"#### EXTREME GRAD BOOST REG ######","9d5a6b0e":"if GD:\n    print('Running Grid Search for model tunning')\n    params = {'max_depth ': [1],\n              'min_child_weight': [2],\n              'gamma ': [0.0],\n              'subsample':[0.7],\n              'reg_alpha':[1e-5, 1e-4, 1e-6],\n              'colsample_bytree': [0.87],\n              'scale_pos_weight':[1],\n                }\n\n    xgb_reg = XGBRegressor()\n    gs = GridSearchCV(xgb_reg, params, cv=5, n_jobs=-1, verbose=1)\n    gsf = gs.fit(X_train, y_train).best_params_\nelse:\n    gsf = {'colsample_bytree': 0.87,\n         'gamma ': 0.0,\n         'max_depth ': 1,\n         'min_child_weight': 2,\n         'reg_alpha': 1e-06,\n         'scale_pos_weight': 1,\n         'subsample': 0.7}\n    \nxgb_reg = XGBRegressor(**gsf, objective='reg:squarederror', nthread=4, learning_rate=0.005, n_estimators=10000)\nxgb_reg_scores = rmseModel(xgb_reg)\nprint(f'XGB Reg, mean: {np.mean(xgb_reg_scores)}, stdv: {np.std(xgb_reg_scores)}')","d1befff1":"olsM = ols_reg.fit(X_train, y_train)\nelM = el_reg.fit(X_train, y_train)\nRidgeM = ridge_reg.fit(X_train, y_train)\nGBregM = GB_reg.fit(X_train, y_train)\nXGBoostM = xgb_reg.fit(X_train, y_train)","62a6a287":"ensembleOutput = np.hstack((olsM.predict(X_test), RidgeM.predict(X_test), elM.predict(X_test).reshape(-1,1), GBregM.predict(X_test).reshape(-1,1)))\nstackedReg = LinearRegression()\nsackedM = stackedReg.fit(ensembleOutput, y_test)","bd9c4fa4":"valEnsembleOutput = np.hstack((olsM.predict(X_val), RidgeM.predict(X_val), elM.predict(X_val).reshape(-1,1),GBregM.predict(X_val).reshape(-1,1)))\nstackedPred = sackedM.predict(valEnsembleOutput)","dc012880":"pred = (np.expm1(stackedPred).reshape(1,-1)[0]*0.55 +\\\nnp.expm1(XGBoostM.predict(X_val))*0.45)\n\nrmse_test = np.sqrt(mean_squared_error(np.log(pred), y_val.values))\nprint(f'rmse for test data: {rmse_test}')","1c131098":"######  SUBMITTING","19fc5683":"df_test['viewsToPOI_enc']  = df_test[['viewsToPOI']].apply(lambda x: le.fit_transform(x.values))\n\ndfPred = df_test[features]","b2b2b535":"nulls = {}\n\nfor col in dfPred.columns:\n    nulls[col] = (1-(len(dfPred[dfPred[col].isna()][col]) \/ dfPred.shape[0]))\n    \nfor k, v in nulls.items():\n    if v < 1.0:\n        dfPred[k] = si.fit_transform(dfPred[k].values.reshape(-1,1))\n        \ndfPred[list(set(skw_cols).intersection(set(dfPred.columns)))] = dfPred[list(set(skw_cols).intersection(set(dfPred.columns)))].\\\n                                                                                apply(lambda x: stats.boxcox(1+x, lmbda=lmbda))\n\ndfPred[categoricals_columns] = dfPred[categoricals_columns].apply(lambda x: le.fit_transform(x))","b21a9337":"outputPred = np.hstack((olsM.predict(dfPred), RidgeM.predict(dfPred), elM.predict(dfPred).reshape(-1,1), GBregM.predict(dfPred).reshape(-1,1)))\nstackedPred = sackedM.predict(outputPred)\nfinalPred = (np.expm1(stackedPred).reshape(1,-1)[0]*0.55 +\\\nnp.expm1(XGBoostM.predict(dfPred))*0.45)","3179537a":"dff = pd.DataFrame({\n    'id': df_test.id,\n    'price':finalPred\n})\n\ndff.to_csv(f\"submission_{datetime.today().strftime('%Y%m%d')}_v2.csv\", index=False)","b726d5f1":"*categorical 'house_state_index','view_quality',\n\n*dicotomous 'basement','viewsToPOI' (e)","0af2221d":"# outliers","7a17023b":"*numeric 'coord_X','coord_Y','dist','bearing','age_since_construction','age_since_renovation','living_m2','lot_m2','lot_vs_neighbors'\n\n*categorical 'house_state_index','view_quality','dow'\n\n*dicotomous 'basement','viewsToPOI' (e)\n\n*discrete 'house_quality_index', 'floors', 'bedrooms','baths', 'living_vs_neighbors','month'","7f92349a":"# Categorical","abbc36c9":"df_train.head()"}}