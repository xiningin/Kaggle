{"cell_type":{"c8620038":"code","c8397648":"code","d90b8f28":"code","aa8311cf":"code","107e840e":"code","2229196f":"code","dabc6e97":"code","5207700b":"code","a0acb881":"code","8856729a":"code","d3100dc7":"markdown","b0f6b9fc":"markdown","fda9c482":"markdown","34824e69":"markdown","2a04f84a":"markdown","e42f2426":"markdown","eaf239e5":"markdown","07333150":"markdown","8ded8ff9":"markdown","e622c5c1":"markdown"},"source":{"c8620038":"# import th\u01b0 vi\u1ec7n\nfrom __future__ import print_function\nimport zipfile \n#%matplotlib inline\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n","c8397648":"# Root directory for dataset\n# dataroot = \"..\/input\/celeba-dataset\"\ndataroot = \"..\/input\/celeba-dataset\"\n# Number of workers for dataloader\nworkers = 2\n\n# Batch size during training\nbatch_size = 128\n\n# Spatial size of training images. All images will be resized to this\n#   size using a transformer.\nimage_size = 64\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Number of training epochs\nnum_epochs = 5\n\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")","d90b8f28":"# We can use an image folder dataset the way we have it setup.\n# Create the dataset\ndataset = dset.ImageFolder(root=dataroot,\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n\n\n# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","aa8311cf":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","107e840e":"# Generator Code\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            self.make_layer(nz, ngf * 8, 4, 1, 0, bias=False),\n            # state size. (ngf*8) x 4 x 4\n            self.make_layer(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            # state size. (ngf*4) x 8 x 8\n            self.make_layer(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            # state size. (ngf*2) x 16 x 16\n            self.make_layer(ngf * 2, ngf, 4, 2, 1, bias=False),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n    def make_layer(self,n_input, n_output, k_size, stride, padding, bias):\n        return nn.Sequential(\n            nn.ConvTranspose2d(n_input, n_output, k_size, stride, padding, bias=bias),\n            nn.BatchNorm2d(n_output),\n            nn.ReLU(True)\n        )\n    \n    \nclass Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            self.make_layer(ndf, ndf * 2, 4, 2, 1, bias=False),\n            # state size. (ndf*2) x 16 x 16\n            self.make_layer(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            # state size. (ndf*4) x 8 x 8\n            self.make_layer(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n    def make_layer(self,n_input, n_output, k_size, stride, padding, bias):\n        return nn.Sequential(\n            nn.Conv2d(n_input, n_output, k_size, stride, padding, bias=bias),\n            nn.BatchNorm2d(n_output ),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n    def forward(self, input):\n        return self.main(input)\n    \n\n    \n# Create the generator\nG = Generator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu > 1):\n    G = nn.DataParallel(G, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.02.\nG.apply(weights_init)\n\n# Print the model\nprint(G)\n\n# Create the Discriminator\nD = Discriminator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu > 1):\n    D = nn.DataParallel(D, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nD.apply(weights_init)\n\n# Print the model\nprint(D)\n","2229196f":"# Calculate losses\ndef real_loss(D_out, smooth=False):\n    # label smoothing\n    b_size = D_out.size(0)\n    if smooth:\n        # smooth, real labels = 0.9\n        labels = torch.full((b_size,), 1, dtype=torch.float, device=device)*0.9\n    else:\n        labels = torch.full((b_size,), 1, dtype=torch.float, device=device) # real labels = 1\n        \n    # numerically stable loss\n    criterion = nn.BCELoss()\n    # calculate loss\n    loss = criterion(D_out, labels)\n    return loss\n\ndef fake_loss(D_out):\n    b_size = D_out.size(0)\n    labels  = torch.full((b_size,), 0, dtype=torch.float, device=device) # fake labels = 0\n    criterion = nn.BCELoss()\n    # calculate loss\n    loss = criterion(D_out, labels)\n    return loss\n\n# Learning rate for optimizers\nlr = 0.0002\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n# Setup Adam optimizers for both G and D\nd_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\ng_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n\n","dabc6e97":"# Training Loop\nlosses = []\niters = 0\nprint_every = 800\nsamples = []\n# Create batch of latent vectors that we will use to visualize\n#  the progression of the generator\nfixed_z = torch.randn(64, nz, 1, 1, device=device)\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for batch_i, data in enumerate(dataloader, 0):\n        \n        ## Important rescaling step ## \n        # ============================================\n        #            TRAIN THE DISCRIMINATOR\n        # ============================================\n        D.zero_grad()\n        \n        real_img = data[0].to(device)\n        b_size = real_img.size(0)\n        \n        # 1. Train with real images\n\n        # Compute the discriminator losses on real images \n        # smooth the real labels\n        D_real = D(real_img).view(-1)\n        d_real_loss = real_loss(D_real, smooth=True).to(device)\n        d_real_loss.backward()\n\n        # 2. Train with fake images\n\n        # Generate fake images\n        # gradients don't have to flow during this step\n\n        z = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake images\n        fake_img = G(z)\n        # Compute the discriminator losses on fake images  \n        D_fake = D(fake_img.detach()).view(-1)\n        d_fake_loss = fake_loss(D_fake)\n        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n        # add up loss and perform backprop\n        d_fake_loss.backward()\n        d_optimizer.step()\n        \n        d_loss = d_real_loss + d_fake_loss\n        \n        \n\n       \n        # =========================================\n        #            TRAIN THE GENERATOR\n        # =========================================\n        G.zero_grad()\n        # Compute the discriminator losses on fake images \n        # using flipped labels!\n        D_fake = D(fake_img).view(-1)\n        g_loss = real_loss(D_fake) # use real loss to flip labels\n        \n        # perform backprop\n        g_loss.backward()\n        \n        g_optimizer.step()\n        \n        # Print some loss stats\n        if batch_i % print_every == 0:\n#             if epoch % 4 ==0:\n          # print discriminator and generator loss\n                print('Epoch [{:5d}\/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n                epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n                samples_z = G(fixed_z).detach().cpu()\n                samples.append(samples_z)\n                \n                with torch.no_grad():\n                    fake_img = G(fixed_z).detach().cpu()\n                    fake_img = vutils.make_grid(fake_img, padding=2, normalize=True)\n                    samples.append(fake_img)\n                    plt.figure(figsize=(10,10))\n                    plt.imshow(samples[-1].permute(1,2,0))\n                    plt.show()\n        # append discriminator loss and generator loss\n        losses.append((d_loss.item(), g_loss.item()))\n        \n    ## AFTER EACH EPOCH##\n    \n    # generate and save sample, fake images\n    G.eval() # eval mode for generating samples\n    G.train() # back to train mode\n","5207700b":"fig, ax = plt.subplots()\nlosses = np.array(losses)\nplt.plot(losses.T[0], label='Discriminator')\nplt.plot(losses.T[1], label='Generator')\nplt.title(\"Training Losses\")\nplt.legend()","a0acb881":"# Grab a batch of real images from the dataloader\nreal_batch = next(iter(dataloader))\n\n# Plot the real images\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# Plot the fake images from the last epoch\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(samples[-1],(1,2,0)))\nplt.show()","8856729a":"list_images = []\ndef interpolate(z1,z2,n_step = 8):\n    p = torch.linspace(0,1,steps = n_step)\n    for i in p:\n        noise__z = (1.0-i)*z1 + i*z2\n        noise__z = noise__z.unsqueeze(0)\n        fake = G(noise__z).detach().cpu()\n        list_images.append(vutils.make_grid(fake, padding=2, normalize=True))\n        \n\nfor i in range (0,8):\n    noise = torch.randn(2, 100,1,1, device=device)\n    interpolated = interpolate(noise[0],noise[1])\n\nplt.figure(figsize=(16,16))\nplt.axis(\"off\")\nplt.title(\"interpolated\")\nplt.imshow(np.transpose(vutils.make_grid(list_images[:64], padding=2, normalize=True).cpu(),(1,2,0)))","d3100dc7":"Implementation\n--------------","b0f6b9fc":"Input Data\n----\n\n\n\n","fda9c482":"# Input\n\n\n-  **dataroot** - Th\u01b0 m\u1ee5c d\u1eabn data \n-  **workers** - s\u1ed1 lu\u1ed3ng CPU \u0111\u1ec3 load data \n-  **batch_size** - the batch size used in training. The DCGAN paper\n   uses a batch size of 128\n-  **image_size** - the spatial size of the images used for training.\n   This implementation defaults to 64x64. If another size is desired,\n   the structures of D and G must be changed. See\n    <a href = \"https:\/\/github.com\/pytorch\/examples\/issues\/70\" > here<\/a>\n   details\n-  **nc** - s\u1ed1 k\u00eanh m\u00e0u trong h\u00ecnh \u1ea3nh \u0111\u1ea7u v\u00e0o. \u0110\u1ed1i v\u1edbi m\u00e0u s\u1eafc h\u00ecnh \u1ea3nh \u0111\u00e2y l\u00e0 3 = kernel\n-  **nz** - chi\u1ec1u d\u00e0i vecto \u1ea9n\n-  **ngf** - li\u00ean quan depth of feature maps c\u1ee7a b\u1ed9 sinh d\u1eef li\u1ec7u\n-  **ndf** - sets the depth of feature maps propagated through the\n   discriminator\n-  **num_epochs** - s\u1ed1 l\u01b0\u1ee3ng epochs \u0111\u1ec3 hu\u1ea5n luy\u1ec7n.\n-  **lr** - learning rate for training = 0.002\n-  **beta1** - si\u00eau tham s\u1ed1 cho Adam optimizers. Theo paper l\u00e0 0.5\n-  **ngpu** - s\u1ed1 l\u01b0\u1ee3ng GPU c\u00f3 s\u1eb5n\n\n","34824e69":"Samples epoch\n\n","2a04f84a":"### DCGAN\n\nB\u00e0i n\u00e0y em l\u00e0m v\u1edbi d\u1eef li\u1ec7u CIFAR10 . Em c\u00f3 \u0111\u1ecdc v\u00e0 tham kh\u1ea3o t\u1ea1i b\u00e0i b\u00e1o <a href = \"https:\/\/arxiv.org\/pdf\/1511.06434.pdf\" > Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks <\/a>\n","e42f2426":"**Real Images vs.\u00a0Fake Images**\n\n\n\n\n","eaf239e5":"# N\u1ed9i suy","07333150":"# Generator\n\n\n---\n\n H\u00ecnh \u1ea3nh c\u1ee7a b\u1ed9 sinh (G) t\u1eeb paper DCGAN : \n<img src=\"https:\/\/pytorch.org\/tutorials\/_images\/dcgan_generator.png\">\n\n\n\n\n","8ded8ff9":"K\u1ebft qu\u1ea3\n-------\n\n\n","e622c5c1":"T\u1eeb paper DCGAN, t\u1ea5t c\u1ea3 c\u00e1c tr\u1ecdng s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh s\u1ebd \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o ng\u1eabu nhi\u00ean t\u1eeb ph\u00e2n ph\u1ed1i Chu\u1ea9n v\u1edbi gi\u00e1 tr\u1ecb trung b\u00ecnh mean = 0, stdev = 0,02. H\u00e0m weights_init l\u1ea5y m\u1ed9t m\u00f4 h\u00ecnh \u0111\u00e3 kh\u1edfi t\u1ea1o l\u00e0m \u0111\u1ea7u v\u00e0o v\u00e0 kh\u1edfi \u0111\u1ed9ng l\u1ea1i t\u1ea5t c\u1ea3 c\u00e1c l\u1edbp t\u00edch ch\u1eadp chuy\u1ec3n v\u1ecb v\u00e0 chu\u1ea9n h\u00f3a h\u00e0ng lo\u1ea1t \u0111\u1ec3 \u0111\u00e1p \u1ee9ng ti\u00eau ch\u00ed n\u00e0y. Ch\u1ee9c n\u0103ng n\u00e0y \u0111\u01b0\u1ee3c \u00e1p d\u1ee5ng cho c\u00e1c m\u00f4 h\u00ecnh ngay sau khi kh\u1edfi t\u1ea1o.\n\n\n"}}