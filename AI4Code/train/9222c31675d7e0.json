{"cell_type":{"627eddc6":"code","1ce78adf":"code","b0817eb3":"code","dcf5365b":"code","2587829f":"code","abd79aa1":"code","81fcacfb":"code","048e5497":"code","376b6f21":"code","d08f7043":"code","472a0c93":"code","70df036c":"code","5212e963":"code","c349b3c9":"code","29633bc6":"code","0e9173be":"code","e1c43e7b":"code","2e2fa1d9":"code","8828a31a":"code","bfab2a6d":"code","5e96e795":"code","7351a0c0":"code","57614fad":"code","25b4868e":"code","5d99dd22":"code","4f73403f":"code","3cde4d47":"code","ab0aaf3c":"code","be3789e2":"code","cee0e627":"code","3ac47c58":"code","7133b246":"code","356487fc":"code","f84b226f":"code","cd4179f3":"code","cd703072":"code","0fa40bcd":"code","e8e156dd":"code","4fe82d7a":"code","c9ca51f6":"code","e8bcbaa0":"code","a6d87567":"markdown","d152fffc":"markdown","8b0739ae":"markdown","42f68c9d":"markdown","9b0d8b64":"markdown","7f81620c":"markdown","a2790ad5":"markdown","44782c16":"markdown","dc42ec9a":"markdown","dc91dd32":"markdown","956b7631":"markdown","3860d23f":"markdown","f06b5141":"markdown","58f2b554":"markdown"},"source":{"627eddc6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ce78adf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport seaborn as sn\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom xgboost import XGBClassifier\n\nfrom itertools import chain ","b0817eb3":"path = '\/kaggle\/input\/pokemon-challenge\/'\npokemon_stats = pd.read_csv(os.path.join(path,'pokemon.csv'))\nbattles = pd.read_csv(os.path.join(path,'combats.csv'))","dcf5365b":"battles","2587829f":"pokemon_stats","abd79aa1":"#We have 18 unique pokemon types\npokemon_types = pokemon_stats['Type 1'].unique().tolist()\nlen(pokemon_types)","81fcacfb":"#We dont need Name of pokemon, we can drop it\npokemon_stats = pokemon_stats.drop(columns=['Name'])\n\n#convert categorical to one hot encoding\npokemon_stats =  pd.get_dummies(pokemon_stats)\n\npokemon_stats","048e5497":"#combining 'Type 1' and 'Type 2' columns.\nfor cols in pokemon_types:\n    pokemon_stats['{}'.format(cols)]=pokemon_stats['Type 1_{}'.format(cols)] | pokemon_stats['Type 2_{}'.format(cols)]\n    pokemon_stats = pokemon_stats.drop(columns=['Type 1_{}'.format(cols),'Type 2_{}'.format(cols)],axis=1)\n    \npokemon_stats.columns","376b6f21":"#converting Legendary column to int\npokemon_stats[\"Legendary\"] = pokemon_stats[\"Legendary\"].astype(int)\npokemon_stats","d08f7043":"pokemon_stats.isnull().sum()","472a0c93":"pokemon_stats.to_csv('\/kaggle\/working\/pokemon_stats_edited.csv',index=False)","70df036c":"#check correlation of each variable\ncorrMatrix = pokemon_stats.corr()\n\nplt.figure(figsize=(15,15))\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","5212e963":"#first merge pokemon stats for 1st pokemon\ncombat_df  = pd.merge(battles,pokemon_stats,left_on='First_pokemon',right_on='#')\ncombat_df","c349b3c9":"#merge pokemon stats for second pokemon\ncombat_df = pd.merge(combat_df,pokemon_stats,left_on='Second_pokemon',right_on='#')\ncombat_df","29633bc6":"#Create new winner_x column\n# if first pokemon wins, winner_x is 1 else it is 0\n\ncombat_df['Winner_x'] = np.NaN\ncombat_df.loc[(combat_df['First_pokemon']==combat_df['Winner']),'Winner_x'] = 1\ncombat_df.loc[(combat_df['Second_pokemon']==combat_df['Winner']),'Winner_x'] = 0\ncombat_df","0e9173be":"#drop unwanted columns\ncombat_df = combat_df.drop(columns=['First_pokemon','Second_pokemon','#_x','#_y','Winner'])\ny = combat_df['Winner_x'].values\ntrain_df = combat_df.drop(columns=['Winner_x'])\ntrain_df","e1c43e7b":"train_df['Winner_x'] = y\ntrain_df.to_csv('train.csv',index=False)\ntrain_df","2e2fa1d9":"y = train_df['Winner_x'].values\n\ntrain_df = train_df.drop(columns=['Winner_x'])\nscaler = StandardScaler()\nscaled_df = pd.DataFrame(scaler.fit_transform(train_df))\nscaled_df","8828a31a":"x=scaled_df.values  # Features\n\nX_train, X_test, y_train, y_test = train_test_split(x , y, test_size=0.3) # 70% training and 30% test\n\nX_train[0], y_train[0]","bfab2a6d":"EPOCHS = 20\nBATCH_SIZE = 16\nLEARNING_RATE = 0.001","5e96e795":"## train data\nclass trainData(Dataset):\n    \n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n\n\ntrain_data = trainData(torch.FloatTensor(X_train), \n                       torch.FloatTensor(y_train))\n## test data    \nclass testData(Dataset):\n    \n    def __init__(self, X_data):\n        self.X_data = X_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n    \n\ntest_data = testData(torch.FloatTensor(X_test))","7351a0c0":"train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE)","57614fad":"class binaryClassification(nn.Module):\n    def __init__(self):\n        super(binaryClassification, self).__init__()\n        # Number of input features is 52.\n        self.layer_1 = nn.Linear(52, 100) \n        self.layer_2 = nn.Linear(100, 500)\n        self.layer_3 = nn.Linear(500,100) \n        self.layer_4 = nn.Linear(100, 50) \n        self.layer_out = nn.Linear(50, 1) \n\n        self.out_act = nn.Sigmoid()\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.1)\n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.relu(self.layer_2(x))\n        x = self.relu(self.layer_3(x))\n        x = self.relu(self.layer_4(x))\n        x = self.dropout(x)\n        x = self.layer_out(x)\n        x = self.out_act(x)\n        \n        return x","25b4868e":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","5d99dd22":"model = binaryClassification()\nmodel.to(device)\nprint(model)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)","4f73403f":"def binary_acc(y_pred, y_test):\n    y_pred_tag = torch.round(y_pred)\n\n    correct_results_sum = (y_pred_tag == y_test).sum().float()\n    acc = correct_results_sum\/y_test.shape[0]\n    acc = torch.round(acc * 100)\n    \n    return acc","3cde4d47":"model.train()\nfor e in range(0, EPOCHS):\n    epoch_loss = 0\n    epoch_acc = 0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        y_pred = model(X_batch)\n\n        loss = criterion(y_pred, y_batch.unsqueeze(1))\n        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n\n    print(f'Epoch {e+0:03}: | Loss: {epoch_loss\/len(train_loader):.5f} | Acc: {epoch_acc\/len(train_loader):.3f}')\n\ntorch.save(model,'\/kaggle\/working\/model.pt')","ab0aaf3c":"from itertools import chain \ny_pred_list = []\nmodel.eval()\nwith torch.no_grad():\n    for X_batch in test_loader:\n        X_batch = X_batch.to(device)\n        y_test_pred = model(X_batch)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.cpu().numpy())\n\ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n#flatten 2d list to 1d list\ny_pred_list =  list(chain.from_iterable(y_pred_list))  \ny_pred_list = np.array(y_pred_list)","be3789e2":"acc = (y_test == y_pred_list)\ntest_acc = acc.sum()\/len(acc)\nprint(\"Test Acc : \",test_acc)","cee0e627":"model = torch.load('model.pt')","3ac47c58":"pokemon_stats = pd.read_csv('pokemon_stats_edited.csv')\ntests = pd.read_csv(os.path.join(path,'tests.csv'))\ntests","7133b246":"#lets convert test data\ntest_df  = pd.merge(tests,pokemon_stats,left_on='First_pokemon',right_on='#')\ntest_df = pd.merge(test_df,pokemon_stats,left_on='Second_pokemon',right_on='#')\ntest_df","356487fc":"test_df = test_df.drop(columns=['First_pokemon','Second_pokemon','#_x','#_y'])\ntest_df","f84b226f":"scaler = StandardScaler()\ntest_df = pd.DataFrame(scaler.fit_transform(test_df))\ntest_df","cd4179f3":"test_df = test_df.values\ntest_data = testData(torch.FloatTensor(test_df))\nscaled_test_tensor = DataLoader(dataset=test_data, batch_size=BATCH_SIZE)\nscaled_test_tensor","cd703072":"y_pred_list = []\nmodel.eval()\nwith torch.no_grad():\n    for X_batch in scaled_test_tensor:\n        X_batch = X_batch.to(device)\n        y_test_pred = model(X_batch)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.cpu().numpy())\n\ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n#flatten 2d list to 1d list\ny_pred_list =  list(chain.from_iterable(y_pred_list))  ","0fa40bcd":"tests['Predicted']=y_pred_list\ntests","e8e156dd":"tests.loc[(tests['Predicted']==1),'Predicted'] = tests.First_pokemon\ntests.loc[(tests['Predicted']==0),'Predicted'] = tests.Second_pokemon\ntests['Predicted'] = tests['Predicted'].astype(int)\ntests","4fe82d7a":"tests.to_csv('Predicted_tests.csv',index=False)","c9ca51f6":"#one example of battle\ntests[9998:9999]","e8bcbaa0":"pokemon_stats = pd.read_csv(os.path.join(path,'pokemon.csv'))\npokemon_stats = pokemon_stats.loc[((pokemon_stats['#']==643) | (pokemon_stats['#']==259))]\npokemon_stats","a6d87567":"**We have 18 different types of pokemon**\n\n**Name is not needed to predict the battles**\n\n**We can convert 'Type 1' and 'Type 2' categorical to numeric value**","d152fffc":"**Converting 'Legendary' column to numeric**","8b0739ae":"# First lets look at data","42f68c9d":"**Adding prediction column to tests.csv**","9b0d8b64":"**Here 1 means, first pokemon is winner**\n\n**And 0 means, second pokemon is winner**","7f81620c":"**We now have inputs and labels ready**","a2790ad5":"**We see that battle is about Elekid vs Swanna**\n\n\n**Elekid is electric type pokemon, Swanna is water and flying type**\n\n**Elekid should win this battle even though its stats are less because, electric is strong against water and flying in pokemon battle**\n\n**Hence, model is doing well**","44782c16":"**Battle.csv lists two pokemon ids and the winner id**\n\n**We need to replace the pokemon id with pokemon stats**\n\n**The input to the model will be stats of 2 pokemons**","dc42ec9a":"**Checking for null values**","dc91dd32":"# Predicting battles in tests.csv","956b7631":"**Verifing battles with pokemon to see if the model has learnt it properly**","3860d23f":"**Checking correlation among variables**","f06b5141":"**Convert Winner column to binary for training**","58f2b554":"# Building a Neural network model"}}