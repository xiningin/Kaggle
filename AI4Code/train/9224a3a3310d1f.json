{"cell_type":{"f9e180de":"code","fa1a0d7a":"code","c796a65e":"code","2148b9cc":"code","55b4d20c":"code","682cfe50":"code","3fc63977":"code","3707d0f5":"code","036bf188":"code","ae573f54":"code","bf8813e8":"code","edc9e69e":"code","aeea99a8":"code","db360b59":"code","8d96052b":"code","3cb606a0":"code","cc4c6f66":"code","fa2990e0":"code","5556d4fc":"code","b3a3eb36":"code","fddb7e6c":"code","96578fee":"code","160f9f82":"code","06f9f2b7":"code","05dd6e1a":"code","e24da46d":"code","72c1ad08":"code","5d4bd6a3":"code","9fdd5746":"code","085141b8":"code","77a644b2":"code","70989e2b":"code","7446986a":"code","ed14548f":"code","d0e71e90":"code","5d0203d5":"code","8d6415e6":"code","ffdb8d15":"code","5141d6b8":"code","f19cbe7c":"code","f45b1c6c":"code","37c64dca":"code","515a9003":"code","8bdbc4b7":"markdown","7aa69a06":"markdown","d1e454cd":"markdown","ec450ac3":"markdown","46a09b02":"markdown","543b31ba":"markdown","82271397":"markdown","88b00398":"markdown","ec3194dc":"markdown","f24017b0":"markdown","4306727a":"markdown","35c83fca":"markdown","47ec2f50":"markdown","4e79f05d":"markdown","28a4f184":"markdown","7c13ae98":"markdown","0957e795":"markdown"},"source":{"f9e180de":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport keras\nimport nltk\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\n\nimport re\nimport string\nfrom tqdm import tqdm\n\nfrom wordcloud import WordCloud","fa1a0d7a":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col = 'id')","c796a65e":"df.sample(5)","2148b9cc":"df.info()","55b4d20c":"df.isnull().sum()","682cfe50":"df = df.drop(['location', 'keyword'], axis = 1)","3fc63977":"sns.countplot(df['target'])","3707d0f5":"df.shape","036bf188":"stop_words = stopwords.words('english')\nps = PorterStemmer()","ae573f54":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","bf8813e8":"df['text']=df['text'].apply(lambda x : remove_URL(x))","edc9e69e":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","aeea99a8":"df['text']=df['text'].apply(lambda x : remove_html(x))","db360b59":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","8d96052b":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","3cb606a0":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","cc4c6f66":"df['text']=df['text'].apply(lambda x : remove_punct(x))","fa2990e0":"!pip install pyspellchecker","5556d4fc":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","b3a3eb36":"# df['text']=df['text'].apply(lambda x : correct_spellings(x))","fddb7e6c":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop_words))]\n        corpus.append(words)\n    return corpus\n        ","96578fee":"corpus=create_corpus(df)","160f9f82":"def preprocess(text):\n\n    text=text.lower()\n    # remove hyperlinks\n    text = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', text)\n    text = re.sub(r'http?:\\\/\\\/.*[\\r\\n]*', '', text)\n    #Replace &amp, &lt, &gt with &,<,> respectively\n    text=text.replace(r'&amp;?',r'and')\n    text=text.replace(r'&lt;',r'<')\n    text=text.replace(r'&gt;',r'>')\n    #remove hashtag sign\n    #text=re.sub(r\"#\",\"\",text)   \n    #remove mentions\n    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n    #text=re.sub(r\"@\",\"\",text)\n    #remove non ascii chars\n    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n    #remove some puncts (except . ! ?)\n    text=re.sub(r'[:\"#$%&\\*+,-\/:;<=>@\\\\^_`{|}~]+','',text)\n    text=re.sub(r'[!]+','!',text)\n    text=re.sub(r'[?]+','?',text)\n    text=re.sub(r'[.]+','.',text)\n    text=re.sub(r\"'\",\"\",text)\n    text=re.sub(r\"\\(\",\"\",text)\n    text=re.sub(r\"\\)\",\"\",text)\n    \n    text=\" \".join(text.split())\n    return text\n\ndf['text'] = df['text'].apply(preprocess)\ndf=df[df[\"text\"]!='']","06f9f2b7":"df['target'].value_counts()","05dd6e1a":"plt.figure(figsize=(20,20))\nwordCloud = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df[\"target\"] == 1][\"text\"]))\nplt.imshow(wordCloud, interpolation = 'bilinear')","e24da46d":"plt.figure(figsize=(20,20))\nwordCloud = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df[\"target\"] == 0][\"text\"]))\nplt.imshow(wordCloud, interpolation = 'bilinear')","72c1ad08":"X = df['text']\ny = df['target']","5d4bd6a3":"vocab_size = 40000\nembedding_dim = 200\ntrunc_type = 'post'\npad_type = 'post'\nmax_len = 80\n\ntokenizer = Tokenizer(num_words = vocab_size)\ntokenizer.fit_on_texts(X)\nX_sequences = tokenizer.texts_to_sequences(X)\n\nX_paded = pad_sequences(X_sequences, truncating=trunc_type, padding=pad_type, maxlen=max_len)","9fdd5746":"model = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","085141b8":"model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","77a644b2":"history = model.fit(X_paded, y, batch_size = 32, validation_split = 0.1, epochs = 8)","70989e2b":"df_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\ndf_test['text'] = df_test['text'].apply(preprocess)\n\ndf_test.head()","7446986a":"test_id = df_test['id']","ed14548f":"df_test = df_test.drop(['id', 'location', 'keyword'], axis = 1)","d0e71e90":"df_test.head()","5d0203d5":"df_test.info()","8d6415e6":"X_test = df_test['text']","ffdb8d15":"test_sequences = tokenizer.texts_to_sequences(X_test)\npadded_test = pad_sequences(test_sequences,maxlen = max_len, truncating = trunc_type, padding=pad_type) ","5141d6b8":"pred = model.predict_classes(padded_test)\n\npred","f19cbe7c":"sub=[]\nfor i in pred:\n    sub.append(i[0])","f45b1c6c":"submission = pd.DataFrame({'id':test_id, 'target':sub})\nsubmission.shape","37c64dca":"submission.head()","515a9003":"submission.to_csv('submission.csv',index=False)","8bdbc4b7":"In the following notebook, we will develop a machine learning model to identify whether the tweet is a real disaster tweet or not. We will be using Bidirectional LSTM to develop this model.","7aa69a06":"## Data Pre-Processing","d1e454cd":"## Contents","ec450ac3":"Removing HTML tags","46a09b02":"Removing Punctuations","543b31ba":"## Dataset","82271397":"Removing URLs","88b00398":"## Submission","ec3194dc":"## Testing","f24017b0":"WordCloud for Real Disaster Tweets","4306727a":"Removing Emojis","35c83fca":"## Visualization","47ec2f50":"## Introduction","4e79f05d":"- `id` - a unique identifier for each tweet\n- `text` - the text of the tweet\n- `location` - the location the tweet was sent from (may be blank)\n- `keyword` - a particular keyword from the tweet (may be blank)\n- `target` - in train.csv only, this denotes whether a tweet is about a real disaster or not\n        Real = 1\n        Fake = 0","28a4f184":"## Importing Libraries","7c13ae98":"- Introduction\n- Dataset\n- Importing Libraries\n- Reading Dataset\n- Data Pre-Processing\n- Visualization\n- Building Model\n- Training Model\n- Testing Data Pre-Processing\n- Submission File\n- Conclusion","0957e795":"Word Cloud for Fake Disaster Tweets"}}