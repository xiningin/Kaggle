{"cell_type":{"ab5f188c":"code","3fe68ac8":"code","b5ad0c82":"code","b1b500e2":"code","09fff608":"code","c5177de6":"code","48cd33fe":"code","dc5d2210":"code","cb32562d":"code","1323e594":"code","41ec8eba":"code","70ff0815":"code","4b8fda03":"code","1caf76ab":"code","52e53ccf":"code","31853447":"code","d22a1e0c":"code","ef827178":"code","f8250ed2":"code","674e527a":"code","0a7dd821":"code","f26cc236":"code","05ba9fba":"code","38a10020":"code","c1fb77c3":"code","11d4d86a":"code","2b4b1c10":"code","ea3f6671":"code","c0f27e0b":"code","e8210628":"code","7ff610d9":"code","3a0d60e9":"code","9495cde2":"code","0e58c7f0":"code","fdf7612a":"code","b68eb91f":"code","88e7d9c7":"code","1847afd2":"code","fa566248":"code","39f774d3":"code","d5217a6d":"markdown","3a962b69":"markdown","24e4c886":"markdown","79a51af3":"markdown","45166091":"markdown","b8e96299":"markdown","bcb60a8c":"markdown","43ae172f":"markdown","5874d420":"markdown","0f4a43c3":"markdown","f35fb31d":"markdown","b6307163":"markdown","2b12c515":"markdown","8bc4ab23":"markdown","5e07cafd":"markdown","1e040f7a":"markdown","5249bc3f":"markdown","430cbf29":"markdown","8362e5a7":"markdown","2d5968a5":"markdown","18b6899e":"markdown","48ea8f72":"markdown","0877f0fe":"markdown","369ce628":"markdown","d98f6b1d":"markdown","5d1d4c4e":"markdown","74166abb":"markdown","cf465ae8":"markdown","ccb0eb01":"markdown","3328a7e5":"markdown","b4948d1e":"markdown","e0e2522b":"markdown","9d901118":"markdown","1810e23b":"markdown","ba6ceebf":"markdown","e1171e04":"markdown","fc9d6df4":"markdown","49a26fa2":"markdown","562f9b98":"markdown","26d24d8a":"markdown","39d2ae07":"markdown","93026b71":"markdown","54a853ba":"markdown","21ee9cb8":"markdown","a342a64e":"markdown","011b3308":"markdown","7519e418":"markdown","07b27061":"markdown"},"source":{"ab5f188c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3fe68ac8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom scipy import stats\nfrom scipy.stats import rankdata, norm\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR, NuSVR, LinearSVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import *\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nimport time, os, warnings, random, string, re, gc, sys\n\nimport category_encoders as ce\n\nimport lightgbm as lgb\nimport catboost as cb\n\n\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \n\ndef set_seed(seed=4242):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()\n\nplt.style.use('fivethirtyeight')","b5ad0c82":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv')\ndel train['id']\ndel test['id']\n\ntrain.shape, test.shape","b1b500e2":"train.info()","09fff608":"cats = [c for c in train.columns if train[c].dtypes=='object']\ncats","c5177de6":"nums = [\n 'cont0',\n 'cont1',\n 'cont2',\n 'cont3',\n 'cont4',\n 'cont5',\n 'cont6',\n 'cont7',\n 'cont8',\n 'cont9',\n 'cont10',\n 'cont11',\n 'cont12',\n 'cont13',\n ]\n","48cd33fe":"tr_orig = train.copy()\nts_orig = test.copy()\nplt.figure(figsize=(10, 6))\nsns.distplot(train.target,bins=50,  fit=norm,kde=True,kde_kws={\"shade\": True}, norm_hist=True,  color='darkcyan')","dc5d2210":"for c in nums:\n    fig, axs = plt.subplots(ncols=2, figsize=(25, 7))\n    sns.distplot(train[c],bins=100,  fit=norm, norm_hist=True,  color='darkcyan', ax=axs[0])\n    sns.distplot(test[c],bins=100,  fit=norm, norm_hist=True,  color='coral', ax=axs[0])\n    axs[0].set_title('Train Vs Test')\n    \n    sns.boxenplot(train[c], color='grey', ax=axs[1])\n    axs[1].set_title('Train Box Plot')","cb32562d":"import matplotlib.style as style\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (20,12))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 25);","1323e594":"\n\n\ndef analyse_cats(df, cat_cols):\n    d = pd.DataFrame()\n    cl = [];u = [];s =[]; nans =[]\n    for c in cat_cols:\n        #print(\"column:\" , c ,\"--Uniques:\" , train[c].unique(), \"--Cardinality:\", train[c].unique().size)\n        cl.append(c); u.append(df[c].unique());s.append(df[c].unique().size);nans.append(df[c].isnull().sum())\n        \n    d['\"feat\"'] = cl;d[\"uniques\"] = u; d[\"cardinality\"] = s; d[\"nans\"] = nans\n    return d\n\nplt.style.use('fivethirtyeight')\ncatanadf = analyse_cats(train, cats)\ncatanadf","41ec8eba":"for c in cats:\n    le = LabelEncoder()\n    le.fit(list(train[c].astype(str)) + list(test[c].astype(str)))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))\n    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    print('target mapping :  ',c ,  le_name_mapping)","70ff0815":"target = train.pop('target')\n\nss = RobustScaler()\ntrain[nums]= ss.fit_transform(train[nums])\ntest[nums]= ss.fit_transform(test[nums])","4b8fda03":"score = []\n\noof_rg = np.zeros(len(train))\npred_rg = np.zeros(len(test))\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    rg = Ridge(alpha=0.1, random_state=2021)\n    rg.fit(trn_data, y_train)\n    oof_rg[val_ind] = rg.predict(val_data)\n    y = rg.predict(trn_data)\n    print('train rmse:' , np.sqrt(mean_squared_error(y_train, y)),'val rmse:' , np.sqrt(mean_squared_error(y_val, oof_rg[val_ind])))\n    \n    score.append(np.sqrt(mean_squared_error(y_val, oof_rg[val_ind])))\n    pred_rg += rg.predict(test)\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model rmse:  ', np.mean(score))\n","1caf76ab":"train = tr_orig.copy()\ntest = ts_orig.copy()\ntarget = train.pop('target')\ndata = pd.concat([train, test], axis=0)\ndata = pd.get_dummies(data)\ntrain = data.iloc[:len(train), ]\ntest = data.iloc[:len(test), ]\n\n","52e53ccf":"\ntrain.head()","31853447":"ss = RobustScaler()\ntrain[nums]= ss.fit_transform(train[nums])\ntest[nums]= ss.fit_transform(test[nums])","d22a1e0c":"train.shape, test.shape","ef827178":"\nscore = []\n\noof_svm = np.zeros(len(train))\npred_svm = np.zeros(len(test))\n\nfolds = KFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target[train_ind], target[val_ind]\n    \n    lsvm =   LinearSVR(C=1, random_state=2021)\n    lsvm.fit(trn_data, y_train)\n    oof_svm[val_ind] = lsvm.predict(val_data)\n    y = lsvm.predict(trn_data)\n    print('train rmse:' , np.sqrt(mean_squared_error(y_train, y)))\n    print('val rmse:' , np.sqrt(mean_squared_error(y_val, oof_svm[val_ind])))\n    score.append(np.sqrt(mean_squared_error(y_val, oof_svm[val_ind])))\n                           \n    pred_svm += lsvm.predict(test)\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model rmse:  ', np.mean(score))\n","f8250ed2":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\ntarget = train.pop('target')\n\n","674e527a":"daset = pd.concat([train, test], axis=0)\n\n\nfor c in (cats):\n    daset[c+'_freq'] = daset[c].map(daset.groupby(c).size() \/ daset.shape[0])\n    indexer = pd.factorize(daset[c], sort=True)[1]\n    daset[c] = indexer.get_indexer(daset[c])\n\ntrain= daset.iloc[:len(train) , ]\ntest= daset.iloc[len(train): , ]\ncols=train.columns\ntrain.shape, test.shape","0a7dd821":"ss = StandardScaler()\ntrain = ss.fit_transform(train)\ntest  = ss.transform(test)","f26cc236":"score = []\n\noof_rg = np.zeros(len(train))\npred_rg = np.zeros(len(test))\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train[train_ind], train[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    rg = Ridge(alpha=0.1, random_state=2021)\n    rg.fit(trn_data, y_train)\n    oof_rg[val_ind] = rg.predict(val_data)\n    y = rg.predict(trn_data)\n    print('train rmse:' , np.sqrt(mean_squared_error(y_train, y)),'val rmse:' , np.sqrt(mean_squared_error(y_val, oof_rg[val_ind])))\n    \n    score.append(np.sqrt(mean_squared_error(y_val, oof_rg[val_ind])))\n    pred_rg += rg.predict(test)\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model rmse:  ', np.mean(score))\n","05ba9fba":"train = tr_orig.copy()\ntest = ts_orig.copy()\ntarget = train.pop('target')\n","38a10020":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","c1fb77c3":"\nsmoothing =1\nimport category_encoders as ce\noof = pd.DataFrame([])\nfrom sklearn.model_selection import KFold\nfor tr_idx, oof_idx in KFold(n_splits=5, random_state=2020, shuffle=True).split(train, target):\n    ce_target_encoder = ce.TargetEncoder(cols = cats, smoothing=smoothing)\n    ce_target_encoder.fit(train.iloc[tr_idx, :], target.iloc[tr_idx])\n    oof = oof.append(ce_target_encoder.transform(train.iloc[oof_idx, :]), ignore_index=False)\nce_target_encoder = ce.TargetEncoder(cols = cats, smoothing=smoothing)\nce_target_encoder.fit(train, target);  train = oof.sort_index(); test = ce_target_encoder.transform(test)\n","11d4d86a":"ss = StandardScaler()\ntrain[nums] = ss.fit_transform(train[nums])\ntest[nums]  = ss.transform(test[nums])","2b4b1c10":"score = []\n\noof_rg = np.zeros(len(train))\npred_rg = np.zeros(len(test))\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    rg = Ridge(alpha=0.1, random_state=2021)\n    rg.fit(trn_data, y_train)\n    oof_rg[val_ind] = rg.predict(val_data)\n    y = rg.predict(trn_data)\n    print('train rmse:' , np.sqrt(mean_squared_error(y_train, y)),'val rmse:' , np.sqrt(mean_squared_error(y_val, oof_rg[val_ind])))\n    \n    score.append(np.sqrt(mean_squared_error(y_val, oof_rg[val_ind])))\n    pred_rg += rg.predict(test)\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model rmse:  ', np.mean(score))\n","ea3f6671":"score = []\n\noof_rf = np.zeros(len(train))\npred_rf = np.zeros(len(test))\n\nfolds = KFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    rf = RandomForestRegressor(n_estimators=150,max_depth=5, n_jobs=-1,random_state=2021)\n    rf.fit(trn_data, y_train)\n    oof_rf[val_ind] = rf.predict(val_data)\n    y = rf.predict(trn_data)\n    print('train rmse:' , np.sqrt(mean_squared_error(y_train, y)),'val rmse:' , np.sqrt(mean_squared_error(y_val, oof_rf[val_ind])))\n    \n    score.append(np.sqrt(mean_squared_error(y_val, oof_rf[val_ind])))\n    pred_rf += rf.predict(test)\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model rmse:  ', np.mean(score))\n","c0f27e0b":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\ntarget = train.pop('target')\ndata = pd.concat([train, test], axis=0)\n","e8210628":"#Create object for hash encoder\nencoder=ce.HashingEncoder(cols=cats,n_components=6)\ndata = encoder.fit_transform(data)\ntrain = data.iloc[:len(train), ]\ntest = data.iloc[len(train):, ]\n","7ff610d9":"ss = StandardScaler()\ntrain = ss.fit_transform(train)\ntest  = ss.transform(test)","3a0d60e9":"score = []\n\noof_rg = np.zeros(len(train))\npred_rg = np.zeros(len(test))\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train[train_ind], train[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    rg = Ridge()\n    rg.fit(trn_data, y_train)\n    oof_rg[val_ind] = rg.predict(val_data)\n    y = rg.predict(trn_data)\n    print('train rmse:' , np.sqrt(mean_squared_error(y_train, y)),'val rmse:' , np.sqrt(mean_squared_error(y_val, oof_rg[val_ind])))\n    \n    score.append(np.sqrt(mean_squared_error(y_val, oof_rg[val_ind])))\n    pred_rg += rg.predict(test)\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model rmse:  ', np.mean(score))\n","9495cde2":"import gc\nimport numpy as np \nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n    \nclass BetaEncoder(object):\n        \n    def __init__(self, group):\n        \n        self.group = group\n        self.stats = None\n        \n    # get counts from df\n    def fit(self, df, target_col):\n        self.prior_mean = np.mean(df[target_col])\n        stats = df[[target_col, self.group]].groupby(self.group)\n        stats = stats.agg(['sum', 'count'])[target_col]    \n        stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n        stats.reset_index(level=0, inplace=True)           \n        self.stats = stats\n        \n    # extract posterior statistics\n    def transform(self, df, stat_type, N_min=1):\n        \n        df_stats = pd.merge(df[[self.group]], self.stats, how='left')\n        n = df_stats['n'].copy()\n        N = df_stats['N'].copy()\n        \n        # fill in missing\n        nan_indexs = np.isnan(n)\n        n[nan_indexs] = self.prior_mean\n        N[nan_indexs] = 1.0\n        \n        # prior parameters\n        N_prior = np.maximum(N_min-N, 0)\n        alpha_prior = self.prior_mean*N_prior\n        beta_prior = (1-self.prior_mean)*N_prior\n        \n        # posterior parameters\n        alpha = alpha_prior + n\n        beta =  beta_prior + N-n\n        \n        # calculate statistics\n        if stat_type=='mean':\n            num = alpha\n            dem = alpha+beta\n                    \n        elif stat_type=='mode':\n            num = alpha-1\n            dem = alpha+beta-2\n            \n        elif stat_type=='median':\n            num = alpha-1\/3\n            dem = alpha+beta-2\/3\n        \n        elif stat_type=='var':\n            num = alpha*beta\n            dem = (alpha+beta)**2*(alpha+beta+1)\n                    \n        elif stat_type=='skewness':\n            num = 2*(beta-alpha)*np.sqrt(alpha+beta+1)\n            dem = (alpha+beta+2)*np.sqrt(alpha*beta)\n\n        elif stat_type=='kurtosis':\n            num = 6*(alpha-beta)**2*(alpha+beta+1) - alpha*beta*(alpha+beta+2)\n            dem = alpha*beta*(alpha+beta+2)*(alpha+beta+3)\n\n        else:\n            num = self.prior_mean\n            dem = np.ones_like(N_prior)\n            \n        # replace missing\n        value = num\/dem\n        value[np.isnan(value)] = np.nanmedian(value)\n        return value\n        \n\n\n\ncat_cols = cats\ntrain = tr_orig.copy()\ntest = ts_orig.copy()\n\nlgb_params = {\n    'learning_rate': 0.02,\n    'application': 'regression',\n    \n    'num_leaves': 10,\n    'verbosity': -1,\n    'metric': 'rmse',\n    'data_random_seed': 3,\n    'bagging_fraction': 0.7,\n    'feature_fraction': 0.5,\n    'bagging_frequency': 1,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n   # 'min_data_in_leaf': 40,\n}\n\nn_folds = 5\nn_rounds = 400\n\nfor N_min in [10, 100, 1000, 10000, -1]: \n\n    print('label encoding')\n    for col in cat_cols:\n        le = LabelEncoder()\n        le.fit(np.concatenate([train[col], test[col]]))\n        train[col] = le.transform(train[col])\n        test[col] = le.transform(test[col])\n        \n            \n    scores = []\n    cbe_lgb_pred = np.zeros(len(test))\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n    for i, (dev_index, val_index) in enumerate(kf.split(train.index.values)):\n        \n        print(f'Fold {i}:')\n        \n        # split data\n        dev = train.loc[dev_index].reset_index(drop=True)\n        val = train.loc[val_index].reset_index(drop=True)\n    \n        # built-in categorical encoding\n        if N_min==-1: \n            \n            feature_cols = cat_cols\n            \n            # setup lightgbm data\n            d_dev = lgb.Dataset(dev[cat_cols],\n                                label=dev.target,\n                                feature_name=cat_cols,\n                                categorical_feature=cat_cols)\n            d_val = lgb.Dataset(val[cat_cols],\n                                label=val.target,\n                                feature_name=cat_cols,\n                                categorical_feature=cat_cols)\n    \n        # target encoding\n        else: \n            \n            # encode variables\n            feature_cols = []\n            for var_name in cat_cols:\n        \n                # fit encoder\n                be = BetaEncoder(var_name)\n                be.fit(dev, 'target')\n        \n                # mean\n                feature_name = f'{var_name}_mean'\n                dev[feature_name]  = be.transform(dev,  'mean', N_min)\n                val[feature_name]  = be.transform(val,  'mean', N_min)\n                test[feature_name] = be.transform(test, 'mean', N_min)        \n                feature_cols.append(feature_name)\n                \n            # setup lightgbm data\n            d_dev = lgb.Dataset(dev[feature_cols], label=dev.target)\n            d_val = lgb.Dataset(val[feature_cols], label=val.target)\n        \n        # fit model\n        mdl = lgb.train(lgb_params,\n                          train_set = d_dev,\n                          num_boost_round = n_rounds,\n                          valid_sets = [d_dev, d_val],\n                          verbose_eval = n_rounds\/\/5)\n        scores.append(mdl.best_score['valid_1']['rmse'])\n        \n        # make predictions on test set\n        cbe_lgb_pred += mdl.predict(test[feature_cols])\/n_folds    \n        \n    # clean up\n    del d_dev, d_val, mdl\n    gc.collect()\n    \n    # print results\n    if N_min==-1:\n        print(f'baseline: {np.mean(scores):0.2f}')\n    else: \n        print(f'N_min={N_min}: {np.mean(scores):0.2f}')\n        \n    # save data\n    #subm['deal_probability'] = np.clip(subm['deal_probability'], 0, 1)\n    #if N_min==-1:\n    #    subm.to_csv(f'submission-baseline.csv', index=False)\n    #else: \n    #    subm.to_csv(f'submission-{N_min}.csv', index=False)","0e58c7f0":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\ntarget = train.pop('target')\ncategorical_features_indices = np.where(train.dtypes == 'object')[0]\ncategorical_features_indices","fdf7612a":"\ncat_score = []\n# Split data with kfold\nkfolds = KFold(n_splits=3, shuffle=True, random_state=2018)\ntrain_features = train.columns\n# Make importance dataframe\nimportances = pd.DataFrame()\n\noof_preds = np.zeros(train.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(train, target)):\n    X_train, y_train = train.iloc[trn_idx], target.iloc[trn_idx]\n    X_valid, y_valid = train.iloc[val_idx], target.iloc[val_idx]\n    \n    # CatBoost Regressor estimator\n    model = cb.CatBoostRegressor(\n        learning_rate = 0.1,\n        iterations = 2000,\n        eval_metric = 'RMSE',\n        allow_writing_files = False,\n        od_type = 'Iter',\n        bagging_temperature = 0.8,\n        depth = 6,\n        od_wait = 20,\n        silent = False\n    )\n    \n    # Fit\n    model.fit(\n        X_train, y_train,\n        cat_features=categorical_features_indices,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        verbose=100,\n        early_stopping_rounds=100\n    )\n    \n    # Feature importance\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = model.get_feature_importance()\n    imp_df['fold'] = n_fold + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_preds[val_idx] = model.predict(X_valid)\n    cat_score.append(np.sqrt(mean_squared_error(y_valid, oof_preds[val_idx])))\n    test_preds = model.predict(test)\n    sub_preds += test_preds \/ kfolds.n_splits\n    \nprint(np.mean(cat_score))\n","b68eb91f":"importances['gain_log'] = importances['gain']\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(20, 8))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False), palette='vlag')","88e7d9c7":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\ntarget = train.pop('target')\n","1847afd2":"for c in cats:\n    le=LabelEncoder()\n    le.fit(list(train[c].astype('str')) + list(test[c].astype('str')))\n    train[c] = le.transform(list(train[c].astype(str))) \n    test[c] = le.transform(list(test[c].astype(str))) \ntrain.head()","fa566248":"\nlgb_params = {\n    \n 'objective': 'rmse', \n 'boosting': 'gbdt', \n 'bagging_fraction': 0.7,\n 'bagging_frequency': 1,\n 'cat_smooth': 200,\n 'feature_fraction': 0.7,\n 'learning_rate': 0.01,\n 'min_child_samples': 50,\n 'min_data_per_group': 200,\n 'num_leaves': 10,\n 'reg_alpha': 2.,\n 'reg_lambda': 3., \n 'metric':'rmse', \n }\n    \n    \n    \n    \n\n\noof_lgb = np.zeros(len(train))\npred_lgb = np.zeros(len(test))\n\nscores = []\n\nfeature_importances_gain = pd.DataFrame()\nfeature_importances_gain['feature'] = train.columns\n\nfeature_importances_split = pd.DataFrame()\nfeature_importances_split['feature'] = train.columns\n\n\nfolds = KFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print(\"fold : ---------------------------------------\", fold_)\n    trn_data = lgb.Dataset(train.iloc[train_ind], label=target.iloc[train_ind], categorical_feature=cats) #-------> Specify Categorical feature for lgb\n    val_data= lgb.Dataset(train.iloc[val_ind], label=target.iloc[val_ind], categorical_feature=cats)  #-------> Specify Categorical feature for lgb\n    \n    lgb_clf = lgb.train(lgb_params, trn_data, num_boost_round=3000, valid_sets=(trn_data, val_data), verbose_eval=100, early_stopping_rounds=100)\n    oof_lgb[val_ind] = lgb_clf.predict(train.iloc[val_ind], num_iteration= lgb_clf.best_iteration)\n    \n    scores.append(np.sqrt(mean_squared_error(target.iloc[val_ind], oof_lgb[val_ind])))\n    \n    feature_importances_gain['fold_{}'.format(fold_ + 1)] = lgb_clf.feature_importance(importance_type='gain')\n    feature_importances_split['fold_{}'.format(fold_ + 1)] = lgb_clf.feature_importance(importance_type='split')\n    \n    pred_lgb += lgb_clf.predict(test, num_iteration=lgb_clf.best_iteration)\/folds.n_splits\n    \nprint(np.mean(scores))\n    \n","39f774d3":"feature_importances_gain['average'] = feature_importances_gain[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\nfeature_importances_gain.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(20, 8))\nsns.barplot(data=feature_importances_gain.sort_values(by='average', ascending=False).head(100),palette='Reds_r',  x='average', y='feature');\nplt.title('TOP n feature importance over {} folds average'.format(folds.n_splits));","d5217a6d":"Label encoding  includes replacing the categories with digits from 1 to n (or 0 to n-1, depending on the implementation),where n is the number of the variable\u2019s distinct categories (the cardinality), and these numbers are assigned arbitrarily.","3a962b69":"Advantages of Mean encoding\n\n- Does not expand the feature space.\n- Creates a monotonic relationship between categories and the target.\n\nLimitations of Mean encoding\n- May lead to overfitting.\n- May lead to a possible loss of value if two categories have the same mean as the target\u2014in these cases, the same number replaces the original.","24e4c886":"using https:\/\/contrib.scikit-learn.org\/category_encoders\/index.html#","79a51af3":"When running machine learning algorithms, simply assigning numbers to categorical variables work if a category has only two levels. This is the case for gender (male\/female), bought a product (yes\/no), attended a course (yes\/no). When a category has several levels, as with nationality, assigning numbers to each level implies an order of the levels. This means that one level of the category has a lower rank than another level. While this makes sense for ordinal variables (e.g., preferences of food items or educational degree), it is a wrong assumption for nominal variables such as color preferences, nationality, residential city specially when we use linear Algorithms. Algorithms like CatBoost have different perspective to solve this problem.  \n\nWe can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features.\n\nIn detail , Catboost calculates for every category of a nominal variable , a value (target-based statistic). This is done using a number of steps:\nWe begin with one categorical feature (e.g., Nationality). This is called x.\nIn one randomly chosen row (k-th row in the training data set), we exchange one random level of this categorical feature (i-th level of x) with a number (e.g., Dutch by 5)\nThis number (in our example 5) is usually based on the target variable (the one we want to predict) conditional on the category level. In other words, the target number is based on the expected outcome variable.\nA splitting attribute is used to create two sets of the training data: One set that has all categories (e.g., German, French, Indian etc) who will have greater target variable than the one computed in step 3, and the other set with smaller target variables.\n\n![catb](https:\/\/developer-blogs.nvidia.com\/wp-content\/uploads\/2018\/12\/catboost_hero.png)\n\nIn their [paper](http:\/\/learningsys.org\/nips17\/assets\/papers\/paper_11.pdf) authors describe how catboost is dealing with categorical features. The standard way is to compute some statistics, such as median, based on the label values of the category. However, this creates problems if there is only one example for a label value. In this case, the numerical value of the category would be the same than the label value. For example if in our example with nationalities, the category Belgian is assigned the value 2, and there is only 1 Belgian student, this student would get the value 2 for nationality. This can create problems of overfitting.\n\nTo avoid this problem, the authors designed a solution which involves randomly changing the order of rows in the complete data set. We perform a random permutation of the data set and for each example we compute average label value for the example with the same category value placed before the given one in the permutation .In their paper they also describe how different features are combined to create a new feature. Think about it, every individual observations of categorical and numerical data points describe one observation. The chances that two observations are exactly identical is slim. Hence, different categorical values and numerical values could be combined to create a unique merged categorical variable which contains all the different individual choices. While this might sound easy, doing this for all potential types of combinations will be computational intensive.\nAnother way to combine different features is to do a greedy search at every tree split. Catboost does this by combining all categorical and numerical values at the current tree with all categorical values in the data set.\n\nTransforming categorical features to numerical features methods are:\n\n\n- Borders\n- Buckets\n- BinarizedTargetMeanValue\n- Counter\n\n\n You can read more about it [here](https:\/\/catboost.ai\/docs\/concepts\/algorithm-main-stages_cat-to-numberic.html).","45166091":"According to: \n - https:\/\/maxhalford.github.io\/blog\/target-encoding\/\n - https:\/\/medium.com\/@pouryaayria\/k-fold-target-encoding-dfe9a594874b \n \nwe have better implement target encoding through KFold and  with smoothing.\n\nmin_samples_leaf define a threshold where prior and target mean (for a given category value) have the same weight. Below the threshold prior becomes more important and above mean becomes more important.How weight behaves against value counts is controlled by smoothing parameter","b8e96299":"## **6. Bayesian Target Encoding and Lightgbm**\n\n","bcb60a8c":"Lgb sorts the categories according to the training objective at each split. More specifically, LightGBM sorts the histogram (for a categorical feature) according to its accumulated values (sum_gradient \/ sum_hessian) and then finds the best split on the sorted histogram. So the split can be made based on the variable being of one specific level or any subset of levels, so you have 2^N splits available in comparision with e.g of 4 for OHE.\n\nThe algorithm behind above mechanism is Fisher (1958) to find the optimal split over categories. http:\/\/www.csiss.org\/SPACE\/workshops\/2004\/SAC\/files\/fisher.pdf","43ae172f":"### **LinearSVR**","5874d420":"# **Cats on a hot Tin Roof: An Overview of Categorical Encoding Methods**","0f4a43c3":">## **Content:**\n>    \n>#### 1. Quick EDA\n>#### 2. Label Encoding\n>#### 3. One-Hot Encoding, Dummy Encoding\n>#### 4. Frequency Encoding\n>#### 5. Target Encoding\n>#### 6. Hash Encoding\n>#### 7. Bayesian Encoding\n>#### 8. CatBoost and Cats\n>#### 9. LightGBM and Cats\n","f35fb31d":"Better  score in comparision with Ridge with Label Encoding","b6307163":"So how should we select encoding methods is depends algorithm(s) we apply :\n        \n- Some algorithms can work with categorical data directly e.g CatBoost , or For example, a decision tree can be learned directly from categorical data with no data transform required (this depends on the specific implementation).\n\n- Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric.\n\n- Some implementations of machine learning algorithms require all data to be numerical. For example, scikit-learn has this requirement.\n\n- If we categorize algorithms to linear and tree based models we sholuld consider that generally linear models are sensitive to order of ordinal data so we should select appropriate encoding methods. \n\n\n\n","2b12c515":"Here we will add frequency encoded features to labeled encoded features:","8bc4ab23":"The performance of a machine learning model not only depends on the model and the hyperparameters but also on how we process and feed different types of variables to the model. Since most machine learning models only accept numerical variables, preprocessing the categorical variables becomes a necessary step. We need to convert these categorical variables to numbers such that the model is able to understand and extract valuable information.\n\n\nIn the 1940s, Stanley Smith Stevens introduced four scales of measurement: nominal, ordinal, interval, and ratio. These are still widely used today as a way to describe the characteristics of a variable. Knowing the scale of measurement for a variable is an important aspect in choosing the right statistical analysis.\n#  \n\n![](https:\/\/lh5.googleusercontent.com\/7jyxzQ2ObysJGLFcGB6Zc25AHAswexk68SbOh_KYa4if2P9yRe7lIC8NDUgZEcSGspqpRIGQcMx_qCmrG6sjHegFHy9Sqhp_1z3PFido6d19TKYFq0pMTHDs4OV9l6pP-MTNmeKu)\n[image ref: What is the difference between ordinal, interval and ratio variables? Why should I care?](https:\/\/www.graphpad.com\/support\/faq\/what-is-the-difference-between-ordinal-interval-and-ratio-variables-why-should-i-care\/)\n\n#  \n\nUsually there are 2 kinds of categorical data:\n\n- Ordinal Data: The categories have an inherent order\nlike: socio economic status (\u201clow income\u201d,\u201dmiddle income\u201d,\u201dhigh income\u201d), education level (\u201chigh school\u201d,\u201dBS\u201d,\u201dMS\u201d,\u201dPhD\u201d), income level (\u201cless than 50K\u201d, \u201c50K-100K\u201d, \u201cover 100K\u201d), satisfaction rating (\u201cextremely dislike\u201d, \u201cdislike\u201d, \u201cneutral\u201d, \u201clike\u201d, \u201cextremely like\u201d)\n- Nominal Data: The categories do not have an inherent order \nlike: blood type, zip code, gender, race, ethnicity\n\nAlso binary data could be nominal or ordinal.\n\nGenerally, In Ordinal data, while encoding, one should retain the information regarding the order in which the category is provided. While encoding Nominal data, we have to consider the presence or absence of a feature. In such a case, no notion of order is present. \n\n\n\n","5e07cafd":"## **3. Frequency Encoding**","1e040f7a":"### **Ridge**","5249bc3f":"## **1. Label Encoding**","430cbf29":"### **Ridge**","8362e5a7":"![dum](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/08\/Screenshot-from-2020-08-12-18-28-24-768x452.png)","2d5968a5":"## **Introduction**","18b6899e":"To understand Hash encoding it is necessary to know about hashing. Hashing is the transformation of arbitrary size input in the form of a fixed-size value. We use hashing algorithms to perform hashing operations i.e to generate the hash value of an input. Further, hashing is a one-way process, in other words, one can not generate original input from the hash representation.\n\nHashing has several applications like data retrieval, checking data corruption, and in data encryption also. We have multiple hash functions available for example Message Digest (MD, MD2, MD5), Secure Hash Function (SHA0, SHA1, SHA2), and many more.\n\nJust like one-hot encoding, the Hash encoder represents categorical features using the new dimensions. Here, the user can fix the number of dimensions after transformation using n_component argument. Here is what we mean \u2013 A feature with 5 categories can be represented using N new features similarly, a feature with 100 categories can also be transformed using N new features. \n\nBy default, the Hashing encoder uses the md5 hashing algorithm but a user can pass any algorithm of his choice. ","48ea8f72":"Mean encoding means replacing the category with the mean target value for that category. We start by grouping each category alone, and for each group, we calculate the mean of the target in the corresponding observations. Then we assign that mean to that category. Thus, we encoded the category with the mean of the target.\nHere\u2019s a detailed illustration of mean encoding:\n\n\n![te](https:\/\/miro.medium.com\/max\/653\/1*gbKFmnAGdnaatRm011RLxA.png)\n\n\n","0877f0fe":"The main motivation of Bayesian Target Encoding is to use iner-category variance in addition to the target mean in encoding categorical variables. \nThe main insight is that we should compute mean, variance and higher moments of the posterior distribution.\nIn Bayesian learning the parameters of the distribution are themselves considered random variables. The distribution of parameters before seeing any data is called prior distribution. This distribution is updated based on the new data to become a posterior distribution. The prediction on the unseen data can be derived by marginalizing over the parameter space. To avoid intractable integrals, Bayesian practitioners often use Conjugate Priors. The beauty of them is that they have very simple update rules, that makes it possible to quickly compute posterior distribution based on the training examples.\n\nMore information:\nhttps:\/\/towardsdatascience.com\/target-encoding-and-bayesian-target-encoding-5c6a6c58ae8c","369ce628":"## **Quick EDA**","d98f6b1d":"![oh1](https:\/\/www.renom.jp\/notebooks\/tutorial\/preprocessing\/onehot\/ConvertCategorical_en.png)","5d1d4c4e":"Label encoding nominals:","74166abb":"Advantages of integer (label) encoding\n- Straightforward to implement.\n- Does not expand the feature space.\n- Can work well enough with tree-based algorithms.\n- Allows agile benchmarking of machine learning models.\n\nLimitations of integer (label) encoding\n- Does not add extra information while encoding.\n- Not suitable for linear models.\n- Does not handle new categories in the test set automatically.\n- Creates an order relationship between the categories.","cf465ae8":"I will use dummy encoding:","ccb0eb01":"### **Ridge**","3328a7e5":"## **4. Target Encoding - Mean Likelihood Encoding ,\"The Right Way !\"**","b4948d1e":"### **RandomForest**","e0e2522b":"In One-Hot method, we map each category to a vector that contains 1 and 0 denoting the presence of the feature or not. The number of vectors depends on the categories which we want to keep. For high cardinality features, this method produces a lot of columns that slows down the learning significantly. There is a buzz between one hot encoding and dummy encoding and when to use one. They are much alike except one hot encoding produces the number of columns equal to the number of categories and dummy producing is one less. This should ultimately be handled by the modeler accordingly in the validation process.\n\n","9d901118":"Best result of Ridge gained with Target Encoding","1810e23b":"## **2. One-Hot Encoding, Dummy Encoding**","ba6ceebf":"It is a way to utilize the frequency of the categories as labels. In the cases where the frequency is related somewhat with the target variable, it helps the model to understand and assign the weight in direct and inverse proportion, depending on the nature of the data.\nReplace the categories by the count of the observations that show that category in the dataset. Similarly, we can replace the category by the frequency -or percentage- of observations in the dataset. \n\n","e1171e04":">In this notebook we will review some encoding techniques along side with applying some algorithms and base models. All models are as base and boosting is possible by applying advanced feat eng, feat selection, model tuning and so on.\n\n","fc9d6df4":"## **5. Hash Encoding**","49a26fa2":"## **7. CatBoost and Cats**","562f9b98":"Folowing we specified categorical feats for lgb .  As lgb is using target encoding i used 2 additive parameters 'min_data_per_group' and 'cat_smooth' and changed default values. Thease parameters help to prevent overfitting, similar to what we did with target encoding for Ridge throug KFold. \n\n","26d24d8a":"As lgb is inherently tree based we can convert ordinals by LabelEncoder like nominals and then specify them for lgb: ","39d2ae07":"### Numerics","93026b71":"### Cats","54a853ba":"Dummy encocoding scheme is similar to one-hot encoding. This categorical data encoding method transforms the categorical variable into a set of binary variables (also known as dummy variables). In the case of one-hot encoding, for N categories in a variable, it uses N binary variables. The dummy encoding is a small improvement over one-hot-encoding. Dummy encoding uses N-1 features to represent N labels\/categories.\n\nTo understand this better let\u2019s see the image below. Here we are coding the same data using both one-hot encoding and dummy encoding techniques. While one-hot uses 3 variables to represent the data whereas dummy encoding uses 2 variables to code 3 categories.","21ee9cb8":"Since Hashing transforms the data in lesser dimensions, it may lead to loss of information. Another issue faced by hashing encoder is the collision. Since here, a large number of features are depicted into lesser dimensions, hence multiple values can be represented by the same hash value, this is known as a collision.\n\nMoreover, hashing encoders have been very successful in some Kaggle competitions. It is great to try if the dataset has high cardinality features.","a342a64e":"Advantages of one-hot encoding\n\n- Does not assume the distribution of categories of the categorical variable.\n- Keeps all the information of the categorical variable.\n- Not so Suitable for tree based models.\n\nLimitations of one-hot encoding\n- Expands the feature space.\n- Does not add extra information while encoding.\n- Many dummy variables may be identical, and this can introduce redundant information.","011b3308":"## **8. LightGBM and Cats**","7519e418":"Advantages of Count or Frequency encoding\n- Straightforward to implement.\n- Does not expand the feature space.\n- Can work well with tree-based algorithms.\n\nLimitations of Count or Frequency encoding\n\n- Does not handle new categories in the test set automatically.\n- We can lose valuable information if there are two different categories with the same amount of observations count\u2014this is because we replace them with the same number.","07b27061":"### **Ridge**"}}