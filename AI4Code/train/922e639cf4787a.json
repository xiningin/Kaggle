{"cell_type":{"78b5832b":"code","a5f0c8e2":"code","d13d59ec":"code","4987ed9a":"code","eccd3dc4":"code","e4951cc9":"code","fbed6773":"code","7dc44614":"code","f9fdd238":"code","923b4ba4":"code","52727b8a":"code","6f9e2a73":"code","234cb216":"code","88a5146c":"code","b588e1e0":"code","c6e63e6b":"code","82b9b5c2":"code","236e1840":"code","48fa7473":"code","e6a3f1f3":"code","d5300b8a":"code","153b16a4":"code","0f7a50fb":"code","5ec9b1b5":"code","0ba11f51":"code","dc7bdcac":"code","9710c310":"code","a2b44434":"code","da519b8c":"code","323376d6":"code","5c5360c1":"markdown","fd576c9d":"markdown","91c8d8e2":"markdown","dc441451":"markdown","8352685a":"markdown","136954ab":"markdown","56ac4d8c":"markdown","37ed747f":"markdown","beb9906f":"markdown","8501b1a6":"markdown","874d9e77":"markdown","63afdca9":"markdown","2d93523c":"markdown","af84b35b":"markdown","f0abab8a":"markdown","5d6413ec":"markdown","c58bcda1":"markdown"},"source":{"78b5832b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","a5f0c8e2":"train = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsub = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","d13d59ec":"train.shape, test.shape, sub.shape","4987ed9a":"train.head()","eccd3dc4":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","e4951cc9":"print('train: ')\ntrain.describe().T.style.bar(subset=['mean'], color='#606ff2')\\\n                            .background_gradient(subset=['std'], cmap='PuBu')\\\n                            .background_gradient(subset=['50%'], cmap='PuBu')","fbed6773":"print('test: ')\ntest.describe().T.style.bar(subset=['mean'], color='#606ff2')\\\n                            .background_gradient(subset=['std'], cmap='PuBu')\\\n                            .background_gradient(subset=['50%'], cmap='PuBu')","7dc44614":"plt.figure(figsize=(14,5))\ntarget_values = train['loss'].value_counts()\nsns.barplot(x=target_values.index, y=target_values.values,linewidth=1.5, facecolor=(1, 1, 1, 0),\n                 errcolor=\".2\", edgecolor=\".2\")\nplt.title(\"Target unique values\", fontdict={'fontsize':20})\nplt.show()","f9fdd238":"# plot the boxplot of area distribution\nplt.figure(figsize=(14,5))\nsns.boxplot(train.loss,color = 'white',linewidth=2.5)\nplt.title('loss Distribution')\nplt.xlabel('loss')\nplt.show()","923b4ba4":"fig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.kdeplot(train[train.columns.tolist()[:100][i]], shade = True, alpha = 0.9, linewidth = 1.5, facecolor=(1, 1, 1, 0), edgecolor=\".2\")\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\nplt.show()","52727b8a":"fig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.boxplot(train[train.columns.tolist()[:100][i]], linewidth = 2.5,color = 'white')\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\nplt.show()","6f9e2a73":"y = train['loss']\ntrain.drop('loss',axis=1,inplace=True)","234cb216":"features = []\nfor feature in train.columns:\n    features.append(feature)\nprint(features)","88a5146c":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\ntrain[features] = mm.fit_transform(train[features])\ntest[features] = mm.transform(test[features])\nX = train","b588e1e0":"def fit_lgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.47 , 0.5),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 0.32 , 0.33),\n        'num_leaves' : trial.suggest_int('num_leaves' , 50 , 70),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.03 , 0.04),\n        'max_depth' : trial.suggest_int('max_depth', 30 , 40),\n        'n_estimators' : trial.suggest_int('n_estimators', 100 , 6100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.015 , 0.02),\n        'subsample' : trial.suggest_uniform('subsample' , 0.9 , 1.0), \n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.52 , 1),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 76, 80),\n        'metric' : 'rmse',\n        'device_type' : 'gpu',\n    }\n    \n    \n    model = LGBMRegressor(**params, random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","c6e63e6b":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_lgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","82b9b5c2":"lgb_params = {'reg_alpha': 0.4972562469417825, 'reg_lambda': 0.3273637203281044, \n          'num_leaves': 50, 'learning_rate': 0.032108486615557354, \n          'max_depth': 40, 'n_estimators': 4060, \n          'min_child_weight': 0.0173353329222102,\n          'subsample': 0.9493343850444064, \n          'colsample_bytree': 0.5328221263825876, 'min_child_samples': 80,'device':'gpu'}","236e1840":"def cross_val(X, y, model, params, folds=10):\n\n    kf = KFold(n_splits=folds, shuffle=True, random_state=2021)\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.values[train_idx], y.values[train_idx]\n        x_test, y_test = X.values[test_idx], y.values[test_idx]\n\n        alg = model(**params,random_state = 2021)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=400,\n                verbose=False)\n        pred = alg.predict(x_test)\n        error = mean_squared_error(y_test, pred,squared = False)\n        print(f\" mean_squared_error: {error}\")\n        print(\"-\"*50)\n    \n    return alg","48fa7473":"lgb_model = cross_val(X, y, LGBMRegressor, lgb_params)","e6a3f1f3":"def fit_xgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0, 2.0, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 6, 10), # Extremely prone to overfitting!\n        'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400), # Extremely prone to overfitting!\n        'eta': trial.suggest_float('eta', 0.007, 0.013), # Most important parameter.\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), # I've had trouble with LB score until tuning this.\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4), # L2 regularization\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), # L1 regularization\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4)\n    } \n    \n    \n    model = XGBRegressor(**params,tree_method='gpu_hist', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","d5300b8a":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_xgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","153b16a4":"xgb_params = {'tweedie_variance_power': 2.0,\n 'max_depth': 9,\n 'n_estimators': 4000,\n 'eta': 0.01200085275863839,\n 'subsample': 0.8,\n 'colsample_bytree': 0.7,\n 'colsample_bylevel': 0.4,\n 'min_child_weight': 2.824928835841522,\n 'reg_lambda': 67.43522142240646,\n 'reg_alpha': 0.00012103217663028774,\n 'gamma': 0.012432559904494572,'tree_method':'gpu_hist'}","0f7a50fb":"xgb_model = cross_val(X, y, XGBRegressor, xgb_params)","5ec9b1b5":"def fit_cat(trial, x_train, y_train, x_test, y_test):\n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n              'task_type':\"GPU\",\n              'eval_metric':'RMSE',\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.03 , 0.04),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.32 , 0.33),\n              'subsample': trial.suggest_uniform('subsample',0.9,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n               }\n    \n    \n    model = CatBoostRegressor(**params,task_type='GPU', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","0ba11f51":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_cat(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","dc7bdcac":"cat_params = {'iterations': 1224,\n 'od_wait': 1243,\n 'learning_rate': 0.03632022350716054,\n 'reg_lambda': 0.3257139588327784,\n 'subsample': 0.9741256425198503,\n 'random_strength': 41.06792107841663,\n 'depth': 12,\n 'min_data_in_leaf': 27,\n 'leaf_estimation_iterations': 10,'task_type':'GPU'}","9710c310":"cat_model = cross_val(X, y, CatBoostRegressor, cat_params)","a2b44434":"cat = CatBoostRegressor(**cat_params)\nlgb = LGBMRegressor(**lgb_params)\nxgb = XGBRegressor(**xgb_params)","da519b8c":"from sklearn.ensemble import VotingRegressor\nfolds = KFold(n_splits = 10, random_state = 2021, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    print(f\"Fold: {fold}\")\n    X_train, X_val = X.values[trn_idx], X.values[val_idx]\n    y_train, y_val = y.values[trn_idx], y.values[val_idx]\n\n    model = VotingRegressor(\n            estimators = [\n                ('lgbm', lgb),\n                ('xgb', xgb)\n            ],\n            weights = [0.15, 0.65]\n        )\n   \n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    error = mean_squared_error(y_val, pred,squared = False)\n    print(f\" mean_squared_error: {error}\")\n    print(\"-\"*50)\n    \n    predictions += model.predict(test) \/ folds.n_splits ","323376d6":"sub['loss'] = lgb_model.predict(test)\nsub.to_csv(f'lgb.csv',index = False)\n\nsub['loss'] = xgb_model.predict(test)\nsub.to_csv(f'xgb.csv',index = False)\n\nsub['loss'] = cat_model.predict(test)\nsub.to_csv(f'cat.csv',index = False)\n\nsub['loss'] = predictions\nsub.to_csv(f'vote.csv',index = False)","5c5360c1":"* these are the best params recovered from **Optuna**.","fd576c9d":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Importing Libraries<\/center><\/h1>\n<\/div>","91c8d8e2":"Reference:[https:\/\/www.kaggle.com\/dmitryuarov\/falling-below-7-87-voting-cb-xgb-lgbm](https:\/\/www.kaggle.com\/dmitryuarov\/falling-below-7-87-voting-cb-xgb-lgbm)","dc441451":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>xgboost<\/center><\/h2>\n<\/div>","8352685a":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>lightgbm<\/center><\/h2>\n<\/div>","136954ab":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>catboost<\/center><\/h2>\n<\/div>","56ac4d8c":"* these are the best params recovered from **Optuna**.","37ed747f":"### **Feature Distribution:**","beb9906f":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>Final Voting<\/center><\/h2>\n<\/div>","8501b1a6":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Data Preprocessing<\/center><\/h1>\n<\/div>","874d9e77":"### **Target Distribution:**","63afdca9":"* these are the best params recovered from **Optuna**.","2d93523c":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h2><center>Prediction and submission<\/center><\/h2>\n<\/div>","af84b35b":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Data Exploration<\/center><\/h1>\n<\/div>","f0abab8a":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Data Visualization<\/center><\/h1>\n<\/div>","5d6413ec":"<div class=\"alert alert-warning\">\n<h4>If you like this notebook, a upvote would be amazing :)<\/h4>\n<\/div>","c58bcda1":"<div style=\"background-color:rgba(255, 99, 71, 0.5);\">\n    <h1><center>Model Building+Optuna<\/center><\/h1>\n<\/div>"}}