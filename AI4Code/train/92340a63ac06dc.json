{"cell_type":{"ab2b9d05":"code","d0c43a00":"code","1729e592":"code","ef8b00a2":"code","29e818b1":"code","f5dee625":"code","36fd5f67":"code","80715a71":"code","a9e3c4d9":"code","fe33dd66":"code","b2cded22":"code","fd01ff49":"code","47c75d14":"code","74759244":"code","f1467843":"code","dd40cf17":"code","700118f8":"code","9948a406":"code","5659478f":"code","9e8c1e1d":"code","e3e05b2f":"code","cc7251f4":"code","ebf1c68d":"code","c9f98a39":"code","debd2f24":"code","76d5670a":"code","65b39806":"code","e6dd1931":"code","fa507b81":"code","1720371b":"code","783284dc":"code","0ce5c857":"code","86f8f67e":"code","1afc7d28":"markdown","e797d197":"markdown","48cc57dd":"markdown","ac4e13a3":"markdown","347e56b3":"markdown","74f51999":"markdown","09b4669e":"markdown","b134d280":"markdown","30d1d7ad":"markdown","9dbeea47":"markdown","d5ca8344":"markdown","eab515b5":"markdown","ff70c6fb":"markdown","99baaae5":"markdown","d18b72a9":"markdown","fcac24cf":"markdown","17166912":"markdown","943f8ab4":"markdown","eee55080":"markdown","512c398a":"markdown"},"source":{"ab2b9d05":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, log_loss\nimport operator\nimport json\nfrom IPython import display\nimport os\nimport warnings\n\nimport random\n\nnp.random.seed(0)\nwarnings.filterwarnings(\"ignore\")\nTHRESHOLD = 4","d0c43a00":"# read data from file\ntrain = pd.read_csv(\"..\/input\/train.csv\") \ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n# check the number of features and data points in train\nprint(\"Number of data points in train: %d\" % train.shape[0])\nprint(\"Number of features in train: %d\" % train.shape[1])\n\n# check the number of features and data points in test\nprint(\"Number of data points in test: %d\" % test.shape[0])\nprint(\"Number of features in test: %d\" % test.shape[1])","1729e592":"def data_clean(data):\n    \n    # Let's first remove all missing value features\n    columns_to_remove = ['Also Known As','Applications','Audio Features','Bezel-less display'\n                         'Browser','Build Material','Co-Processor','Browser'\n                         'Display Colour','Mobile High-Definition Link(MHL)',\n                         'Music', 'Email','Fingerprint Sensor Position',\n                         'Games','HDMI','Heart Rate Monitor','IRIS Scanner', \n                         'Optical Image Stabilisation','Other Facilities',\n                         'Phone Book','Physical Aperture','Quick Charging',\n                         'Ring Tone','Ruggedness','SAR Value','SIM 3','SMS',\n                         'Screen Protection','Screen to Body Ratio (claimed by the brand)',\n                         'Sensor','Software Based Aperture', 'Special Features',\n                         'Standby time','Stylus','TalkTime', 'USB Type-C',\n                         'Video Player', 'Video Recording Features','Waterproof',\n                         'Wireless Charging','USB OTG Support', 'Video Recording','Java']\n\n    columns_to_retain = list(set(data.columns)-set(columns_to_remove))\n    data = data[columns_to_retain]\n\n    #Features having very low variance \n    columns_to_remove = ['Architecture','Audio Jack','GPS','Loudspeaker','Network','Network Support','VoLTE']\n    columns_to_retain = list(set(data.columns)-set(columns_to_remove))\n    data = data[columns_to_retain]\n\n    # Multivalued:\n    columns_to_remove = ['Architecture','Launch Date','Audio Jack','GPS','Loudspeaker','Network','Network Support','VoLTE', 'Custom UI']\n    columns_to_retain = list(set(data.columns)-set(columns_to_remove))\n    data = data[columns_to_retain]\n\n    # Not much important\n    columns_to_remove = ['Bluetooth', 'Settings','Wi-Fi','Wi-Fi Features']\n    columns_to_retain = list(set(data.columns)-set(columns_to_remove))\n    data = data[columns_to_retain]\n    \n    return data","ef8b00a2":"train = data_clean(train)\ntest = data_clean(test)","29e818b1":"train = train[(train.isnull().sum(axis=1) <= 15)]\n# You shouldn't remove data points from test set\n#test = test[(test.isnull().sum(axis=1) <= 15)]","f5dee625":"# check the number of features and data points in train\nprint(\"Number of data points in train: %d\" % train.shape[0])\nprint(\"Number of features in train: %d\" % train.shape[1])\n\n# check the number of features and data points in test\nprint(\"Number of data points in test: %d\" % test.shape[0])\nprint(\"Number of features in test: %d\" % test.shape[1])","36fd5f67":"def for_integer(test):\n    try:\n        test = test.strip()\n        return int(test.split(' ')[0])\n    except IOError:\n           pass\n    except ValueError:\n        pass\n    except:\n        pass\n\ndef for_string(test):\n    try:\n        test = test.strip()\n        return (test.split(' ')[0])\n    except IOError:\n        pass\n    except ValueError:\n        pass\n    except:\n        pass\n\ndef for_float(test):\n    try:\n        test = test.strip()\n        return float(test.split(' ')[0])\n    except IOError:\n        pass\n    except ValueError:\n        pass\n    except:\n        pass\ndef find_freq(test):\n    try:\n        test = test.strip()\n        test = test.split(' ')\n        if test[2][0] == '(':\n            return float(test[2][1:])\n        return float(test[2])\n    except IOError:\n        pass\n    except ValueError:\n        pass\n    except:\n        pass\n\n    \ndef for_Internal_Memory(test):\n    try:\n        test = test.strip()\n        test = test.split(' ')\n        if test[1] == 'GB':\n            return int(test[0])\n        if test[1] == 'MB':\n#             print(\"here\")\n            return (int(test[0]) * 0.001)\n    except IOError:\n           pass\n    except ValueError:\n        pass\n    except:\n        pass\n    \ndef find_freq(test):\n    try:\n        test = test.strip()\n        test = test.split(' ')\n        if test[2][0] == '(':\n            return float(test[2][1:])\n        return float(test[2])\n    except IOError:\n        pass\n    except ValueError:\n        pass\n    except:\n        pass\n","80715a71":"def data_clean_2(x):\n    data = x.copy()\n    \n    data['Capacity'] = data['Capacity'].apply(for_integer)\n\n    data['Height'] = data['Height'].apply(for_float)\n    data['Height'] = data['Height'].fillna(data['Height'].mean())\n\n    data['Internal Memory'] = data['Internal Memory'].apply(for_Internal_Memory)\n\n    data['Pixel Density'] = data['Pixel Density'].apply(for_integer)\n\n    data['Internal Memory'] = data['Internal Memory'].fillna(data['Internal Memory'].median())\n    data['Internal Memory'] = data['Internal Memory'].astype(int)\n\n    data['RAM'] = data['RAM'].apply(for_integer)\n    data['RAM'] = data['RAM'].fillna(data['RAM'].median())\n    data['RAM'] = data['RAM'].astype(int)\n\n    data['Resolution'] = data['Resolution'].apply(for_integer)\n    data['Resolution'] = data['Resolution'].fillna(data['Resolution'].median())\n    data['Resolution'] = data['Resolution'].astype(int)\n\n    data['Screen Size'] = data['Screen Size'].apply(for_float)\n\n    data['Thickness'] = data['Thickness'].apply(for_float)\n    data['Thickness'] = data['Thickness'].fillna(data['Thickness'].mean())\n    data['Thickness'] = data['Thickness'].round(2)\n\n    data['Type'] = data['Type'].fillna('Li-Polymer')\n\n    data['Screen to Body Ratio (calculated)'] = data['Screen to Body Ratio (calculated)'].apply(for_float)\n    data['Screen to Body Ratio (calculated)'] = data['Screen to Body Ratio (calculated)'].fillna(data['Screen to Body Ratio (calculated)'].mean())\n    data['Screen to Body Ratio (calculated)'] = data['Screen to Body Ratio (calculated)'].round(2)\n\n    data['Width'] = data['Width'].apply(for_float)\n    data['Width'] = data['Width'].fillna(data['Width'].mean())\n    data['Width'] = data['Width'].round(2)\n\n    data['Flash'][data['Flash'].isna() == True] = \"Other\"\n\n    data['User Replaceable'][data['User Replaceable'].isna() == True] = \"Other\"\n\n    data['Num_cores'] = data['Processor'].apply(for_string)\n    data['Num_cores'][data['Num_cores'].isna() == True] = \"Other\"\n\n\n    data['Processor_frequency'] = data['Processor'].apply(find_freq)\n    #because there is one entry with 208MHz values, to convert it to GHz\n    data['Processor_frequency'][data['Processor_frequency'] > 200] = 0.208\n    data['Processor_frequency'] = data['Processor_frequency'].fillna(data['Processor_frequency'].mean())\n    data['Processor_frequency'] = data['Processor_frequency'].round(2)\n\n    data['Camera Features'][data['Camera Features'].isna() == True] = \"Other\"\n\n    #simplifyig Operating System to os_name for simplicity\n    data['os_name'] = data['Operating System'].apply(for_string)\n    data['os_name'][data['os_name'].isna() == True] = \"Other\"\n\n    data['Sim1'] = data['SIM 1'].apply(for_string)\n\n    data['SIM Size'][data['SIM Size'].isna() == True] = \"Other\"\n\n    data['Image Resolution'][data['Image Resolution'].isna() == True] = \"Other\"\n\n    data['Fingerprint Sensor'][data['Fingerprint Sensor'].isna() == True] = \"Other\"\n\n    data['Expandable Memory'][data['Expandable Memory'].isna() == True] = \"No\"\n\n    data['Weight'] = data['Weight'].apply(for_integer)\n    data['Weight'] = data['Weight'].fillna(data['Weight'].mean())\n    data['Weight'] = data['Weight'].astype(int)\n\n    data['SIM 2'] = data['SIM 2'].apply(for_string)\n    data['SIM 2'][data['SIM 2'].isna() == True] = \"Other\"\n    \n    return data","a9e3c4d9":"train = data_clean_2(train)\ntest = data_clean_2(test)\n\n# check the number of features and data points in train\nprint(\"Number of data points in train: %d\" % train.shape[0])\nprint(\"Number of features in train: %d\" % train.shape[1])\n\n# check the number of features and data points in test\nprint(\"Number of data points in test: %d\" % test.shape[0])\nprint(\"Number of features in test: %d\" % test.shape[1])","fe33dd66":"def data_clean_3(x):\n    \n    data = x.copy()\n\n    columns_to_remove = ['User Available Storage','SIM Size','Chipset','Processor','Autofocus','Aspect Ratio','Touch Screen',\n                        'Bezel-less display','Operating System','SIM 1','USB Connectivity','Other Sensors','Graphics','FM Radio',\n                        'NFC','Shooting Modes','Browser','Display Colour' ]\n\n    columns_to_retain = list(set(data.columns)-set(columns_to_remove))\n    data = data[columns_to_retain]\n\n\n    columns_to_remove = [ 'Screen Resolution','User Replaceable','Camera Features',\n                        'Thickness', 'Display Type']\n\n    columns_to_retain = list(set(data.columns)-set(columns_to_remove))\n    data = data[columns_to_retain]\n\n\n    columns_to_remove = ['Fingerprint Sensor', 'Flash', 'Rating Count', 'Review Count','Image Resolution','Type','Expandable Memory',\\\n                        'Colours','Width','Model']\n    columns_to_retain = list(set(data.columns)-set(columns_to_remove))\n    data = data[columns_to_retain]\n\n    return data","b2cded22":"train = data_clean_3(train)\ntest = data_clean_3(test)\n\n# check the number of features and data points in train\nprint(\"Number of data points in train: %d\" % train.shape[0])\nprint(\"Number of features in train: %d\" % train.shape[1])\n\n# check the number of features and data points in test\nprint(\"Number of data points in test: %d\" % test.shape[0])\nprint(\"Number of features in test: %d\" % test.shape[1])","fd01ff49":"# one hot encoding\n\ntrain_ids = train['PhoneId']\ntest_ids = test['PhoneId']\n\ncols = list(test.columns)\ncols.remove('PhoneId')\ncols.insert(0, 'PhoneId')\n\ncombined = pd.concat([train.drop('Rating', axis=1)[cols], test[cols]])\nprint(combined.shape)\nprint(combined.columns)\n\ncombined = pd.get_dummies(combined)\nprint(combined.shape)\nprint(combined.columns)\n\ntrain_new = combined[combined['PhoneId'].isin(train_ids)]\ntest_new = combined[combined['PhoneId'].isin(test_ids)]","47c75d14":"train_new = train_new.merge(train[['PhoneId', 'Rating']], on='PhoneId')","74759244":"# check the number of features and data points in train\nprint(\"Number of data points in train: %d\" % train_new.shape[0])\nprint(\"Number of features in train: %d\" % train_new.shape[1])\n\n# check the number of features and data points in test\nprint(\"Number of data points in test: %d\" % test_new.shape[0])\nprint(\"Number of features in test: %d\" % test_new.shape[1])","f1467843":"#Check the shape of the training data and testing data\ntrain_new.shape, test_new.shape","dd40cf17":"train_new.head()\n# There is a PhoneID column which is an indicator column and does not play any role decision making.\n# We see categorical variables coded as dummy variables which is an important observation and will be \n# used later on in the model","700118f8":"test_new.head()\n# There is a PhoneID column which is an indicator column and does not play any role decision making\n# There is no Rating column in test_new since this you will have to predict by building a working model\n# on the train_data","9948a406":"# Create X_train which will hold all columns except PhoneID and Rating using train_new\n# Create Y_train which will only hold the Rating column present in train_new, note that the dataframe maintains integrity of the PhoneID \n# which is very essential\n\n# Create X_test which will hold all columns except PhoneID\n# There is no Y_test for obvious reasons as this is what you will be predicting \n\nX_train = train_new.drop(['PhoneId','Rating'],axis=1)\nY_train = train_new['Rating'].map(lambda x: 1 if x >= 4 else 0) # Notice that in the Perceptron model the output should be binary\n\nX_test = test_new.drop(['PhoneId'],axis=1)","5659478f":"# Checking for correlation only for the first 10 discrete variables\nX_train_corr = X_train.iloc[:,[0,1,2,3,4,5,6,7,8,9]]\nX_train_corr.head()","9e8c1e1d":"correlations = X_train_corr.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\n\ncorrelations.tail(10)\n\n# The tail of the dataframe has the information about highest correlation between features,\n# where as the head has details about features that have least correlation\n\n# Observe that Screen Size and Screen to Body Ratio are highly correlated, We don't know if they are\n# positively or negatively correlated yet. We will find that out visually\n# Similarly Height and Screen Size are correlated.\n\n# Intution tells me that they will be positvely correlated, meaning any increase in Height of the \n# phone will result in an increase of the Screen Size and vice versa. \n\n#A bigger phone screen (Screen Size) means the phone is lenghtier (Height). ","e3e05b2f":"corr = X_train.corr()\nfig = plt.figure()\nfig.set_size_inches(20,20)\nax = fig.add_subplot(111)\ncax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(X_train.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(X_train.columns)\nax.set_yticklabels(X_train.columns)\nplt.show()\n\n# We can clearly see that Screen Size is positively correlated with Height and Screen to body Ratio\n# There are other positively correlated variables too. Another good easy to understand example is \n# Brand_Apple and os_name_iOS.\n\n# Look against the line of 'Brand_Apple' and compare it with all the top columns \n# It's obvious isn't it that an Apple iPhone \/ product user will have an iOS operating \n# system on his\/her device. He\/she cannot have an Android on his Apple iPhone. \n# This now introduces to you negative correlation. Notice how Brand_Apple and os_name_Andriod are\n# negatively correlated (dark blue), which means Apple folks cannot have Android OS","cc7251f4":"X_train = np.array([\nX_train['Weight'], \n#X_train['Height'],# Column removed due to correlation\nX_train['Screen to Body Ratio (calculated)'],\nX_train['Pixel Density'], \nX_train['Processor_frequency'], \n#X_train['Screen Size'],# Column removed due to correlation\nX_train['RAM'], \nX_train['Resolution'], \nX_train['Internal Memory'], \n#X_train['Capacity'],# Column removed due to correlation\nX_train['Brand_10.or'],\nX_train['Brand_Apple'],\nX_train['Brand_Asus'], \n#X_train['Brand_Billion'], # No Rows found, Removed to increase train accuracy due to scope of assignment\nX_train['Brand_Blackberry'], \nX_train['Brand_Comio'], \nX_train['Brand_Coolpad'], \n#X_train['Brand_Do'], # No Rows found, Removed to increase train accuracy due to scope of assignment\nX_train['Brand_Gionee'], \nX_train['Brand_Google'], \nX_train['Brand_HTC'], \nX_train['Brand_Honor'], \nX_train['Brand_Huawei'], \nX_train['Brand_InFocus'], \nX_train['Brand_Infinix'], \nX_train['Brand_Intex'], \nX_train['Brand_Itel'], \nX_train['Brand_Jivi'], \nX_train['Brand_Karbonn'], \nX_train['Brand_LG'], \nX_train['Brand_Lava'], \nX_train['Brand_LeEco'], \nX_train['Brand_Lenovo'], \nX_train['Brand_Lephone'], \nX_train['Brand_Lyf'], \nX_train['Brand_Meizu'], \nX_train['Brand_Micromax'], \nX_train['Brand_Mobiistar'], \nX_train['Brand_Moto'], \nX_train['Brand_Motorola'], \nX_train['Brand_Nokia'], \nX_train['Brand_Nubia'], \nX_train['Brand_OPPO'], \nX_train['Brand_OnePlus'],\nX_train['Brand_Oppo'], \nX_train['Brand_Panasonic'], \nX_train['Brand_Razer'], \nX_train['Brand_Realme'], \n#X_train['Brand_Reliance'],# Removed due to correlation \nX_train['Brand_Samsung'], \nX_train['Brand_Sony'], \n#X_train['Brand_Spice'],# No Rows found, Removed to increase train accuracy due to scope of assignment \nX_train['Brand_Tecno'], \nX_train['Brand_Ulefone'], \nX_train['Brand_VOTO'], \nX_train['Brand_Vivo'], \nX_train['Brand_Xiaomi'], \nX_train['Brand_Xiaomi Poco'], \nX_train['Brand_Yu'], \nX_train['Brand_iVooMi'], \nX_train['SIM Slot(s)_Dual SIM, GSM+CDMA'], \nX_train['SIM Slot(s)_Dual SIM, GSM+GSM'], \nX_train['SIM Slot(s)_Dual SIM, GSM+GSM, Dual VoLTE'], \n# X_train['SIM Slot(s)_Single SIM, GSM'],# Removed due to correlation\nX_train['Num_cores_312'], \nX_train['Num_cores_Deca'], \nX_train['Num_cores_Dual'], \nX_train['Num_cores_Hexa'], \nX_train['Num_cores_Octa'], \nX_train['Num_cores_Other'],# Food for thought column - Retained to prove correlation theory, If I remove this column my 71% data gives \nX_train['Num_cores_Quad'],# 85% accuracy compared to my current 83.33 % accuracy. I sacrificed my leaderboard rank for this !!\nX_train['Num_cores_Tru-Octa'], \n#X_train['Sim1_2G'],# Column removed due to correlation\nX_train['Sim1_3G'], \n#X_train['Sim1_4G'],# Column removed due to correlation\nX_train['SIM 2_2G'], \nX_train['SIM 2_3G'], # Food for thought columns - What happens if I remove this and retain SIM 2_4G ?  \nX_train['SIM 2_4G'], # Food for thought columns - What happens if I remove this and retain SIM 2_3G ?  \nX_train['SIM 2_Other'], \nX_train['os_name_Android'], \n#X_train['os_name_Blackberry'], # Removed due to correlation  \n#X_train['os_name_KAI'], # Removed due to correlation  \nX_train['os_name_Nokia'], \n#X_train['os_name_Other'],# Removed due to correlation\nX_train['os_name_Tizen'],  \n#X_train['os_name_iOS'],# Removed due to correlation\n    \n]) \n\nX_train = X_train.T","ebf1c68d":"X_test = np.array([\nX_test['Weight'], \n#X_test['Height'],# Column removed due to correlation\nX_test['Screen to Body Ratio (calculated)'], \nX_test['Pixel Density'], \nX_test['Processor_frequency'], \n#X_test['Screen Size'],# Column removed due to correlation \nX_test['RAM'], \nX_test['Resolution'], \nX_test['Internal Memory'], \n#X_test['Capacity'], # Column removed due to correlation\nX_test['Brand_10.or'],\nX_test['Brand_Apple'],\nX_test['Brand_Asus'], \n#X_test['Brand_Billion'], # No Rows found, Removed to increase train accuracy due to scope of assignment\nX_test['Brand_Blackberry'], \nX_test['Brand_Comio'], \nX_test['Brand_Coolpad'], \n#X_test['Brand_Do'], # No Rows found, Removed to increase train accuracy due to scope of assignment\nX_test['Brand_Gionee'], \nX_test['Brand_Google'], \nX_test['Brand_HTC'], \nX_test['Brand_Honor'], \nX_test['Brand_Huawei'], \nX_test['Brand_InFocus'], \nX_test['Brand_Infinix'], \nX_test['Brand_Intex'], \nX_test['Brand_Itel'], \nX_test['Brand_Jivi'], \nX_test['Brand_Karbonn'], \nX_test['Brand_LG'], \nX_test['Brand_Lava'], \nX_test['Brand_LeEco'], \nX_test['Brand_Lenovo'], \nX_test['Brand_Lephone'], \nX_test['Brand_Lyf'], \nX_test['Brand_Meizu'], \nX_test['Brand_Micromax'], \nX_test['Brand_Mobiistar'], \nX_test['Brand_Moto'], \nX_test['Brand_Motorola'], \nX_test['Brand_Nokia'], \nX_test['Brand_Nubia'], \nX_test['Brand_OPPO'], \nX_test['Brand_OnePlus'],\nX_test['Brand_Oppo'], \nX_test['Brand_Panasonic'], \nX_test['Brand_Razer'], \nX_test['Brand_Realme'], \n#X_test['Brand_Reliance'],# Removed due to correlation \nX_test['Brand_Samsung'], \nX_test['Brand_Sony'], \n#X_test['Brand_Spice'],# No Rows found, Removed to increase train accuracy due to scope of assignment \nX_test['Brand_Tecno'], \nX_test['Brand_Ulefone'], \nX_test['Brand_VOTO'], \nX_test['Brand_Vivo'], \nX_test['Brand_Xiaomi'], \nX_test['Brand_Xiaomi Poco'], \nX_test['Brand_Yu'], \nX_test['Brand_iVooMi'], \nX_test['SIM Slot(s)_Dual SIM, GSM+CDMA'], \nX_test['SIM Slot(s)_Dual SIM, GSM+GSM'], \nX_test['SIM Slot(s)_Dual SIM, GSM+GSM, Dual VoLTE'], \n# X_test['SIM Slot(s)_Single SIM, GSM'],# Removed due to correlation\nX_test['Num_cores_312'], \nX_test['Num_cores_Deca'], \nX_test['Num_cores_Dual'], \nX_test['Num_cores_Hexa'], \nX_test['Num_cores_Octa'], \nX_test['Num_cores_Other'],# Food for thought column - Retained to prove correlation theory, If I remove this column my 71% data gives \nX_test['Num_cores_Quad'],# 85% accuracy compared to my current 83.33 % accuracy. I sacrificed my leaderboard rank for this !!\nX_test['Num_cores_Tru-Octa'], \n#X_test['Sim1_2G'],# Column removed due to correlation\nX_test['Sim1_3G'], \n#X_test['Sim1_4G'],# Column removed due to correlation\nX_test['SIM 2_2G'], \nX_test['SIM 2_3G'], # Food for thought columns - What happens if I remove this and retain SIM 2_4G ?  \nX_test['SIM 2_4G'], # Food for thought columns - What happens if I remove this and retain SIM 2_3G ?  \nX_test['SIM 2_Other'], \nX_test['os_name_Android'], \n#X_test['os_name_Blackberry'], # Removed due to correlation  \n# X_test['os_name_KAI'], # Removed due to correlation  \nX_test['os_name_Nokia'], \n#X_test['os_name_Other'],# Removed due to correlation\nX_test['os_name_Tizen'],  \n#X_test['os_name_iOS'],# Removed due to correlation\n    \n]) \n\nX_test = X_test.T","c9f98a39":"from sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nprint(scaler.fit(X_train))\nMinMaxScaler(copy=True, feature_range=(0, 1))\n\nX_train = scaler.transform(X_train)\n\nscaler = MinMaxScaler()\nprint(scaler.fit(X_test))\nMinMaxScaler(copy=True, feature_range=(0, 1))\n\nX_test = scaler.transform(X_test)","debd2f24":"X_train[0:1]","76d5670a":"class Perceptron:\n    def __init__ (self):\n        self.w = None\n        self.b = None\n \n    def model(self, x):\n        return 1 if (np.dot(self.w, x) >= self.b) else 0\n    \n    def predict(self,X):\n        Y = []\n        for x in X:\n            result=self.model(x)\n            Y.append(result)\n        return np.array(Y)\n    \n    def fit(self,X, Y, epochs = 1, lr = 1):\n        #Weights = 0\n        self.w = np.ones(X.shape[1]) #np.random.rand(73)\n        self.b = 0 #random.randint(0,1)\n        \n        accuracy = {}\n        max_accuracy = 0\n        \n        for i in range(epochs):\n            for x,y in zip(X, Y):\n                y_pred = self.model(x)\n                if y==1 and y_pred ==0 :\n                    self.w = self.w + lr * x\n                    self.b = self.b - lr * 1\n                #elif y==1 and y_pred == 1:\n                    #self.w = self.w\n                    #self.b = self.b\n                #elif y==0 and y_pred == 0:\n                    #self.w = self.w\n                    #self.b = self.b\n                elif y==0 and y_pred ==1:\n                    self.w = self.w - lr * x\n                    self.b = self.b + lr * 1\n            accuracy[i] = accuracy_score(self.predict(X),Y)\n            if (accuracy[i] > max_accuracy):\n                max_accuracy = accuracy[i]\n                chkptw = self.w\n                chkptb = self.b\n            #    self.var = self.var - 1\n            #elif (accuracy[i] < max_accuracy):\n            #    self.var = self.var + 1\n                \n        self.w = chkptw\n        self.b = chkptb\n        \n        print(max_accuracy)\n        #print(self.var)\n        print(self.w)\n        plt.plot(accuracy.values())\n        plt.ylim([0,1])\n        plt.show()","65b39806":"perceptron = Perceptron()","e6dd1931":"perceptron.fit(X_train,Y_train,10000,.01)","fa507b81":"plt.plot(perceptron.w)\nplt.show()","1720371b":"Y_pred_train = perceptron.predict(X_train)","783284dc":"Y_pred_test = perceptron.predict(X_test)\nY_pred_test = list(Y_pred_test)\nprint(Y_pred_test)","0ce5c857":"submission = pd.DataFrame({'PhoneId':test_new['PhoneId'], 'Class':Y_pred_test})\nsubmission = submission[['PhoneId', 'Class']]\nsubmission.head()","86f8f67e":"submission.to_csv(\"submission.csv\", index=False)","1afc7d28":"#### Some Key Points which we need to understand with respect to building a model\n\n##### 1. The idea is to get the Train accuracy close to Test accuracy so that the model follows a generalized behaviour and I can use it on any type of data that comes in, if at all I happen to deploy it.\n##### 2. The model is more advanced than the primitive MP Neuron model built earlier on the same dataset, Perceptron is now able to take discrete as well as binary variables as input and output a binary value ( 0 or 1 )\n##### 3. We use wiehgts and learning rate mechanisim in the Perceptron model to acheive model stabilty \n##### 4.\n##### 5. I have even managed to get a better accuracy compared to my MP Neuron model on 71 % of the test data","e797d197":"### X_train Standardized","48cc57dd":"### Understanding Correlation ","ac4e13a3":"# Removing features","347e56b3":"#### Visual Interpretation of correlation  ","74f51999":"# Filling Missing values","09b4669e":"removing all those data points in which more than 15 features are missing ","b134d280":"### The Perceptron Model","30d1d7ad":"##### I highly encourage you to visit my MP Neuron Public Kernel for more insights on plots with respect to correlation. I haven't included the bar charts \/ scatter plots in this kernel as the dataset that I am working on is exactly the same","9dbeea47":"#### Perform the head operation to get a feel for the test & train data and make basic observations","d5ca8344":"### Min Max Scalar\n#### We know through the describe() function that the scale ( range of values ) for all the features are not the same, some features like Capacity have a higher range of values where as some features like RAM have a smaller range. We have to sort this out before you pass your np.array to the model. We can \" standardize \" the scale using a Python MinMaxScalar function as shown below. Note that we are standardizing the input variables to fit into a discrete range of 0 to 1. Some of them get a question at this stage whether or not we have to Normalize the data, Normalization is not required ( however Normalization is an important aspect in feature engineering )","eab515b5":"### Create staging DataFrames","ff70c6fb":"<b>Missing values:<\/b><br>\n'Also Known As'(459),'Applications'(421),'Audio Features'(437),'Bezel-less display'(266),'Browser'(449),'Build Material'(338),'Co-Processor'(451),'Display Colour'(457),'Mobile High-Definition Link(MHL)'(472),'Music'(447)\n'Email','Fingerprint Sensor Position'(174),'Games'(446),'HDMI'(454),'Heart Rate Monitor'(467),'IRIS Scanner'(467),\n'Optical Image Stabilisation'(219),'Other Facilities'(444),'Phone Book'(444),'Physical Aperture'(87),'Quick Charging'(122),'Ring Tone'(444),'Ruggedness'(430),SAR Value(315),'SIM 3'(472),'SMS'(470)', 'Screen Protection'(229),'Screen to Body Ratio (claimed by the brand)'(428),'Sensor'(242),'Software Based Aperture'(473),\n'Special Features'(459),'Standby time'(334),'Stylus'(473),'TalkTime'(259), 'USB Type-C'(374),'Video Player'(456),\n'Video Recording Features'(458),'Waterproof'(398),'Wireless Charging','USB OTG Support'(159), 'Video ,'Recording'(113),'Java'(471),'Browser'(448)\n\n<b>Very low variance:<\/b><br>\n'Architecture'(most entries are 64-bit),'Audio Jack','GPS','Loudspeaker','Network','Network Support','Other Sensors'(28),'SIM Size', 'VoLTE'\n\n\n<b>Multivalued:<\/b><br>\n'Colours','Custom UI','Model'(1),'Other Sensors','Launch Date'\n\n<b>Not important:<\/b><br>\n'Bluetooth', 'Settings'(75),'Wi-Fi','Wi-Fi Features'\n\n<b>Doubtful:<\/b><br>\n'Aspect Ratio','Autofocus','Brand','Camera Features','Fingerprint Sensor'(very few entries are missing),\n'Fingerprint Sensor Position', 'Graphics'(multivalued),'Image resolution'(multivalued),'SIM Size','Sim Slot(s)', 'User Available Storage', 'SIM 1', 'SIM 2','Shooting Modes', 'Touch Screen'(24), 'USB Connectivity'\n    \n<b>To check:<\/b><br>\n'Display Type','Expandable Memory','FM Radio'\n\n<b>High Correlation with other features<\/b><br>\n'SIM Slot(s)' high correlation with SIM1\n'Weight' has high high correlation with capacity , screen-to-body ratio\n'Height' - screen size is also there\n    \n<b>Given a mobile, we can't directly get these features<\/b><br>\n'Rating Count', 'Review Count'\n\n<b>Keeping:<\/b><br>\n'Capacity','Flash'(17),'Height'(22),'Internal Memory'(20, require cleaning),'Operating System'(25, require cleaning), 'Pixel Density'(1, clean it),'Processor'(22, clean it), 'RAM'(17, clean), 'Rating','Resolution'(cleaning), 'Screen Resolution','Screen Size', 'Thickness'(22), 'Type','User Replaceable','Weight'(cleaning),'Sim Size'(), 'Other Sensors'(28), 'Screen to Body Ratio (calculated)','Width',\n","99baaae5":"#### Check the shape of the test and train data","d18b72a9":"Task: To predict whether the user likes the mobile phone or not. <br>\nAssumption: If the average rating of mobile >= threshold, then the user likes it, otherwise not.","fcac24cf":"### Key Factors & Observation for High Accuracy Score\n\n#### 1. Dealing with correlation - \n#####  I removed one of the columns in the group of columns which were correlated, but I have tried to retain more discrete variable columns as much as I can. You can see that I removed Weight, Height, Screen to Body Ratio & Resolution in MP Neuron, but here I have removed Weight, Screen Size and Capacity just to prove the point on correlation. I have even removed the ones that are negatively correlated for perceptron model, If this is not done, then the accuracy cannot be improved.\n\n#### 2. Accounting for the direction using Scatter plot - \n##### There is no need to worry about direction in Perceptron as weights and learning rate will account for it.\n\n#### 3. Binary Columns -\n##### If you notice I have removed correlation between binary categorical columns, the reason being I am confident that my wiehgts will take care of balancing the function of the model. In MP Neuron columns Apple_Brand and os_name_iOS both seemed to be important, where as in Perceptron I was able to remove one of the columns and yet maintain good accuracy. \n\n#### 4. Food for thought -\n##### Num_cores_Other & Num_cores_Quad - Retained both columns to prove correlation theory, If I remove any one of the columns my 71% data gives **85% accuracy ( private score )**, If I retain my 71% data gives me **83.33 % accuracy ( private score ) **, the public score ( trian and test ) however is constant **85%**. You can try this out and also try removing one of the columns SIM 2_3G and SIM 2_4G and see what happens and let me know in the comments \/ through your Kernel !\n","17166912":"### Epochs and Learning Rate\n#### Best practise is to adjust Epochs and Learning rate in steps to improve accuracy. I used a high epoch value and a lower learning rate to stablize the model. You can try with other epoch values and observe the curve.","943f8ab4":"Not very important feature","eee55080":"### Key Mentions & References\n\n##### 1. https:\/\/www.91mobiles.com for providing padhai.onefourthlabs.in the dataset to analyze and work upon\n##### 2. The entire team of https:\/\/padhai.onefourthlabs.in for their supreme training efforts, dedication, data cleansing and quality time in helping the newbies and pros scale higher in their pursuit of becoming a better data scientist  \n##### 3. https:\/\/www.stackoverflow.com - for most of the doubts, errors & shortcuts in python \n##### 4. My Ex Manager & Mentor for life, Arulvel Kumar - https:\/\/au.linkedin.com\/in\/arulvel-kumar who has been a great motivation and driving force of my data science career. ","512c398a":"### Acheivements\n\n##### 1. The above logic has worked best for me resulting in a train accuracy of **85.63%** and a test accuracy of **85.74 %** on **29%** of the data followed by **83.33 %** on **71% **of the data.\n##### 2. I feel the model is consistent and will not vary over a varience of **5%** which is my cut off for accuracy on any given dataset which is able to fit into the model\n##### 3. Feature addition & handling can be re-engineerd, hence the model is scalable with few tweaks\n##### 4. Kaggle rank **42** out of **1080** ( **Top 4%** )\n##### 5. Possible & Proven improvement of Kaggle rank to **14** out of **1080** ( **Top 2%** ) with code tweaks as explained above and taking full advantage of eliminating correlating variables."}}