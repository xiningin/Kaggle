{"cell_type":{"0120e717":"code","9fd2d691":"code","bf73e15d":"code","6b601426":"code","efcc1b40":"code","e3032dc1":"code","b28a9fcc":"code","2b1e553d":"code","c8d23c6b":"code","4ad15868":"code","57f968b1":"code","7449f309":"code","ce024dba":"code","aae99e6c":"code","f869e8a9":"code","d291f6bc":"code","1788918f":"code","c9831abd":"code","ef6712c8":"code","60385f98":"code","114b5e46":"code","bdb78514":"code","d2b56547":"code","12211a5c":"code","b252d2c9":"code","369b949c":"code","e0dbaeb2":"code","af2c81ac":"code","8422e338":"code","a8104dba":"code","3867a550":"code","d5989846":"code","4b0c134a":"code","0a7a570c":"code","d40b06f1":"code","9af431a2":"code","97d6cf37":"code","d59b4fa0":"code","66856aa7":"code","79d08264":"code","fe10e3f5":"code","315b41f5":"code","9e6f5773":"code","da7cf9d9":"code","6ff7a3d2":"code","04741932":"code","48b14e24":"code","6a416b18":"code","da76c1a5":"code","4a02891c":"markdown","0ee6b47e":"markdown","4e6848b4":"markdown","01366bb4":"markdown","aadb705d":"markdown","7edb0c85":"markdown"},"source":{"0120e717":"! pip install impyute","9fd2d691":"import numpy as np\nimport pandas as pd\nimport os\nfrom impyute.imputation.cs import mice\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.feature_selection import SequentialFeatureSelector\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bf73e15d":"train = pd.read_csv('..\/input\/prohack-hackathon\/train.csv')\ntest = pd.read_csv('..\/input\/prohack-hackathon\/test.csv')\nsubmission = pd.read_csv('..\/input\/prohack-hackathon\/sample_submit.csv')","6b601426":"train.head()","efcc1b40":"train.info()","e3032dc1":"# Imputation runs for a very long time, so we comment out the code below and upload the final result\n\n# train_to_mice = train.drop('galaxy', axis=1)\n\n# imputed_training = mice(train_to_mice.values)\n# train_mice = pd.DataFrame(data=imputed_training, columns=train_to_mice.columns.tolist())","b28a9fcc":"train_mice = pd.read_csv('..\/input\/prohack-mice-imputed\/df_mice.csv')","2b1e553d":"# Return the column with galaxies\n\ntrain_mice['galaxy'] = train['galaxy']","c8d23c6b":"# Let's make the order of the columns as in the original frame\n\nnew_list = ['galactic year',\n 'galaxy',\n 'existence expectancy index',\n 'existence expectancy at birth',\n 'Gross income per capita',\n 'Income Index',\n 'Expected years of education (galactic years)',\n 'Mean years of education (galactic years)',\n 'Intergalactic Development Index (IDI)',\n 'Education Index',\n 'Intergalactic Development Index (IDI), Rank',\n 'Population using at least basic drinking-water services (%)',\n 'Population using at least basic sanitation services (%)',\n 'Gross capital formation (% of GGP)',\n 'Population, total (millions)',\n 'Population, urban (%)',\n 'Mortality rate, under-five (per 1,000 live births)',\n 'Mortality rate, infant (per 1,000 live births)',\n 'Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))',\n 'Population, ages 15\u201364 (millions)',\n 'Population, ages 65 and older (millions)',\n 'Life expectancy at birth, male (galactic years)',\n 'Life expectancy at birth, female (galactic years)',\n 'Population, under age 5 (millions)',\n 'Young age (0-14) dependency ratio (per 100 creatures ages 15-64)',\n 'Adolescent birth rate (births per 1,000 female creatures ages 15-19)',\n 'Total unemployment rate (female to male ratio)',\n 'Vulnerable employment (% of total employment)',\n 'Unemployment, total (% of labour force)',\n 'Employment in agriculture (% of total employment)',\n 'Labour force participation rate (% ages 15 and older)',\n 'Labour force participation rate (% ages 15 and older), female',\n 'Employment in services (% of total employment)',\n 'Labour force participation rate (% ages 15 and older), male',\n 'Employment to population ratio (% ages 15 and older)',\n 'Jungle area (% of total land area)',\n 'Share of employment in nonagriculture, female (% of total employment in nonagriculture)',\n 'Youth unemployment rate (female to male ratio)',\n 'Unemployment, youth (% ages 15\u201324)',\n 'Mortality rate, female grown up (per 1,000 people)',\n 'Mortality rate, male grown up (per 1,000 people)',\n 'Infants lacking immunization, red hot disease (% of one-galactic year-olds)',\n 'Infants lacking immunization, Combination Vaccine (% of one-galactic year-olds)',\n 'Gross galactic product (GGP) per capita',\n 'Gross galactic product (GGP), total',\n 'Outer Galaxies direct investment, net inflows (% of GGP)',\n 'Exports and imports (% of GGP)',\n 'Share of seats in senate (% held by female)',\n 'Natural resource depletion',\n 'Mean years of education, female (galactic years)',\n 'Mean years of education, male (galactic years)',\n 'Expected years of education, female (galactic years)',\n 'Expected years of education, male (galactic years)',\n 'Maternal mortality ratio (deaths per 100,000 live births)',\n 'Renewable energy consumption (% of total final energy consumption)',\n 'Estimated gross galactic income per capita, male',\n 'Estimated gross galactic income per capita, female',\n 'Rural population with access to electricity (%)',\n 'Domestic credit provided by financial sector (% of GGP)',\n 'Population with at least some secondary education, female (% ages 25 and older)',\n 'Population with at least some secondary education, male (% ages 25 and older)',\n 'Gross fixed capital formation (% of GGP)',\n 'Remittances, inflows (% of GGP)',\n 'Population with at least some secondary education (% ages 25 and older)',\n 'Intergalactic inbound tourists (thousands)',\n 'Gross enrolment ratio, primary (% of primary under-age population)',\n 'Respiratory disease incidence (per 100,000 people)',\n 'Interstellar phone subscriptions (per 100 people)',\n 'Interstellar Data Net users, total (% of population)',\n 'Current health expenditure (% of GGP)',\n 'Intergalactic Development Index (IDI), female',\n 'Intergalactic Development Index (IDI), male',\n 'Gender Development Index (GDI)',\n 'Intergalactic Development Index (IDI), female, Rank',\n 'Intergalactic Development Index (IDI), male, Rank',\n 'Adjusted net savings ',\n 'Creature Immunodeficiency Disease prevalence, adult (% ages 15-49), total',\n 'Private galaxy capital flows (% of GGP)',\n 'Gender Inequality Index (GII)',\n 'y']","4ad15868":"train_mice = train_mice[new_list]","57f968b1":"train_mice.head()","7449f309":"train['galaxy'].value_counts()","ce024dba":"test['galaxy'].value_counts()","aae99e6c":"train_mice['galaxy'].nunique()","f869e8a9":"test['galaxy'].nunique()","d291f6bc":"# The names of some galaxies are not in the test dataset\n\nset(train_mice['galaxy'].tolist()) ^ set(test['galaxy'].tolist())","1788918f":"# Delete galaxies that are not in the test dataset\n\ndf_train = train_mice.loc[~train_mice['galaxy'].isin(['Andromeda XII',\n 'Andromeda XIX[60]',\n 'Andromeda XVIII[60]',\n 'Andromeda XXII[57]',\n 'Andromeda XXIV',\n 'Hercules Dwarf',\n 'NGC 5253',\n 'Triangulum Galaxy (M33)',\n 'Tucana Dwarf'])]","c9831abd":"# Let's check\n\nset(df_train['galaxy'].tolist()) ^ set(test['galaxy'].tolist())","ef6712c8":"# Create datasets with one-hot encoded galaxy names\n\ntrain_dummies = pd.get_dummies(df_train['galaxy'])\ntest_dummies = pd.get_dummies(test['galaxy'])","60385f98":"# Let's see how much data is missing in the training dataset\n\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","114b5e46":"missing_data.loc[missing_data['Percent'] > 0.7]","bdb78514":"list_of_missing_data = missing_data.loc[missing_data['Percent'] > 0.7].index.tolist()","d2b56547":"# delete columns with missing data more than 70%\n# delete from the MICE IMPUTED set!\n\ndf_train = df_train.drop(list_of_missing_data, axis=1)","12211a5c":"df_train.shape","b252d2c9":"impute_data = df_train.drop(['galaxy'], axis=1)","369b949c":"num_colums = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_columns = list(impute_data.select_dtypes(include=num_colums).columns)\nimpute_data = impute_data[numerical_columns]\nimpute_data.shape\n\ntrain_features, test_features, train_labels, test_labels = train_test_split(\n    impute_data.drop(labels=['y'], axis=1),\n    impute_data['y'],\n    test_size=0.2,\n    random_state=41)\n\ncorrelated_features = set()\ncorrelation_matrix = impute_data.corr()\nfor i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\n\n\ntrain_features.drop(labels=correlated_features, axis=1, inplace=True)\ntest_features.drop(labels=correlated_features, axis=1, inplace=True)\n\ntrain_features.shape, test_features.shape","e0dbaeb2":"# We will use the Random Forest regressor to find the most optimal parameters. Set the number of features to 5 (5 gave the best result)\n\n# from sklearn.ensemble import RandomForestRegressor\n# from mlxtend.feature_selection import SequentialFeatureSelector\n\nfeature_selector = SequentialFeatureSelector(RandomForestRegressor(n_jobs=-1),\n           k_features=5,\n           forward=True,\n           verbose=2,\n           scoring='neg_mean_squared_error',\n           cv=4)","af2c81ac":"features = feature_selector.fit(np.array(train_features.fillna(0)), train_labels)","8422e338":"filtered_features= train_features.columns[list(features.k_feature_idx_)]\nfiltered_features","a8104dba":"best_features = filtered_features.tolist()","3867a550":"best_features","d5989846":"best_features.append('y')","4b0c134a":"# We get the final data set for the predictive model\n\ndf = impute_data[best_features]","0a7a570c":"X = df.drop(['y'], axis=1)\ny = df['y']","d40b06f1":"# Similar to the training dataset, I used MICE imputation. Download the finished data set\n\ntest_mice = pd.read_csv('..\/input\/prohack-mice-imputed\/df_mice_test.csv')","9af431a2":"test_mice.head()","97d6cf37":"# list of columns left for prediction:\n\ndata_columns = X.columns.tolist()","d59b4fa0":"df_test = test_mice[data_columns]","66856aa7":"df_test.head()","79d08264":"# Join datasets with encoded galaxy names\n\nX_joined_dummies = X.join(train_dummies)\ndf_test_joined_dummies = df_test.join(test_dummies)","fe10e3f5":"X = X_joined_dummies\n\ndf_test_pred = df_test_joined_dummies","315b41f5":"# rename columns with galaxies from alphabetic names to numbers\n\ngalaxy_rename_list = train_dummies.columns.tolist()\n\ni = 1\nfor name in galaxy_rename_list:\n    X.rename(columns={name: i}, inplace=True)\n    df_test_pred.rename(columns={name: i}, inplace=True)\n    i = i + 1","9e6f5773":"# I used the code below to pick up the parameters\n\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# from sklearn.model_selection import train_test_split\n# from sklearn.model_selection import GridSearchCV\n# from catboost import CatBoostRegressor\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# model = CatBoostRegressor()\n# parameters = {'depth'         : [3, 4, 5],\n#               'learning_rate' : [0.05, 0.1, 0.2],\n#               'iterations'    : [8000, 12000],\n#               'subsample'     : [0.3, 0.5, 1]\n#             }\n# grid = GridSearchCV(estimator=model, param_grid = parameters, cv = 2, n_jobs=-1)\n# grid.fit(X_train, y_train)    \n\n# # Results from Grid Search\n# print(\"\\n========================================================\")\n# print(\" Results from Grid Search \" )\n# print(\"========================================================\")    \n    \n# print(\"\\n The best estimator across ALL searched params:\\n\",\n#     grid.best_estimator_)\n    \n# print(\"\\n The best score across ALL searched params:\\n\",\n#     grid.best_score_)\n    \n# print(\"\\n The best parameters across ALL searched params:\\n\",\n#     grid.best_params_)\n    \n# print(\"\\n ========================================================\")","da7cf9d9":"# ========================================================\n#  Results from Grid Search \n# ========================================================\n\n#  The best estimator across ALL searched params:\n#  <catboost.core.CatBoostRegressor object at 0x000000000C956348>\n\n#  The best score across ALL searched params:\n#  0.9482559923204557\n\n#  The best parameters across ALL searched params:\n#  {'depth': 3, 'iterations': 12000, 'learning_rate': 0.1, 'subsample': 0.3}\n\n#  ========================================================","6ff7a3d2":"model = CatBoostRegressor(iterations=12000,\n                          learning_rate=0.1,\n                          subsample=0.3,\n                          depth=3)","04741932":"# Fit model\n\nmodel.fit(X_train,y_train)","48b14e24":"# Get predictions\n\npreds = model.predict(X_test)","6a416b18":"rmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","da76c1a5":"# Predict 'y' on test case\n\ny_pred_test = model.predict(df_test_pred)","4a02891c":"#### Take a look at the names of the galaxies in the original frames.","0ee6b47e":"## Preparing a test data set","4e6848b4":"## CatBoost Algorithm\n#### (my best result)","01366bb4":"#### So we see a lot of missing values. Let's use a MICE imputation.","aadb705d":"### Now make a selection of the best features\n\n#### Drop features with a correlation greater than 0.9","7edb0c85":"### Final datasets for work"}}