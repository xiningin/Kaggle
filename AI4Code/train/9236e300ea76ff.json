{"cell_type":{"22a675c4":"code","27c7a7b3":"code","c440c823":"code","0ba17022":"code","d46d51cb":"code","410e4489":"code","db33befd":"code","05416847":"code","14df0bda":"code","3f699afd":"code","38d7feba":"code","d4af1feb":"code","4988b76b":"code","6bd919f1":"code","c3beac5b":"code","09390b0a":"code","0e9bf12f":"code","47969a5b":"code","a55bf499":"code","1f2ae59d":"code","2b4025a4":"code","11657199":"code","7782f910":"code","88cf016a":"code","31f1ef02":"code","4e0d1030":"code","05f65e87":"code","f51ac557":"code","c571d12a":"code","5b237810":"code","bd08f095":"code","d52d02c0":"code","be8baab1":"code","8da321ed":"code","b8febb75":"code","aa29c233":"code","d174225a":"code","5ebe6436":"code","6db91e9e":"code","e46db9a6":"code","056d08ba":"code","25a98f13":"code","cee8291c":"code","13146e9d":"code","e68d3db1":"code","df7441b1":"code","ab44d251":"code","db5405c2":"code","f01104db":"code","a324ae08":"markdown","89f4260b":"markdown","a1e549b1":"markdown","cd8d2593":"markdown","d3c75a66":"markdown","531ae9bf":"markdown","aa3f47d4":"markdown","db1b112f":"markdown","fc8310f3":"markdown","d9ff7493":"markdown","8cc631eb":"markdown","8fd7cf36":"markdown","17feaade":"markdown","65d2cde8":"markdown","f106bba6":"markdown","d2b9ad8e":"markdown","ce8cd2cc":"markdown","ee56f4c6":"markdown","c4dd232c":"markdown","caff6b7f":"markdown","e8378214":"markdown","5525290a":"markdown","799b7c4a":"markdown","19218fb4":"markdown","61936617":"markdown","b715183b":"markdown"},"source":{"22a675c4":"from sklearn import model_selection, preprocessing, metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom pandas import get_dummies\nimport lightgbm as lgb\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\n\n%matplotlib inline","27c7a7b3":"#Loading Train and Test Data\ndf_train = pd.read_csv('..\/input\/train.csv', parse_dates=[\"first_active_month\"] )\ndf_test = pd.read_csv('..\/input\/test.csv' ,parse_dates=[\"first_active_month\"] )\ndf_merchants=pd.read_csv('..\/input\/merchants.csv')\ndf_new_merchant_transactions=pd.read_csv('..\/input\/new_merchant_transactions.csv')\ndf_historical_transactions = pd.read_csv(\"..\/input\/historical_transactions.csv\")","c440c823":"sample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")","0ba17022":"sample_submission.head()","d46d51cb":"pp.ProfileReport(df_train)","410e4489":"pp.ProfileReport(df_test)","db33befd":"pp.ProfileReport(df_merchants)","05416847":"pp.ProfileReport(df_new_merchant_transactions)","14df0bda":"#Missing Data Analysis on train set\ntotal = df_train.isnull().sum().sort_values(ascending = False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percentage'])\nmissing_data.head(20)","3f699afd":"#Missing Data Analysis on test set\ntotal = df_test.isnull().sum().sort_values(ascending = False)\npercent = (df_test.isnull().sum()\/df_test.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percentage'])\nmissing_data.head(20)","38d7feba":"df_train.target.plot.hist()","d4af1feb":"# Code reference from https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo\n# SRK - Simple Exploration Notebook\n\nimport seaborn as sns\n\ncnt_srs = df_train['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in train set\")\nplt.show()\n\ncnt_srs = df_test['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in test set\")\nplt.show()","4988b76b":"df_train.corr()","6bd919f1":"df_train = pd.get_dummies(df_train, columns=['feature_1', 'feature_3'])\ndf_test = pd.get_dummies(df_test, columns=['feature_1', 'feature_3'])\ndf_train.head()","c3beac5b":"df_card_history  = df_historical_transactions['card_id'].value_counts().head(10)\ndf_card_history","09390b0a":"df_card_history_authorized  = df_historical_transactions['authorized_flag'].value_counts()\ndf_card_history_authorized","0e9bf12f":"df_non_authorized = df_historical_transactions[df_historical_transactions['authorized_flag'] == \"N\"]\ndf_non_authorized","47969a5b":"df_card_unauth  = df_non_authorized['card_id'].value_counts()\ndf_card_unauth.head(10)","a55bf499":"df_card_1_unauth= df_non_authorized[df_non_authorized['card_id']==\"C_ID_5ea401d358\"]\ndf_card_2_unauth = df_non_authorized[df_non_authorized['card_id']==\"C_ID_3d3dfdc692\"]","1f2ae59d":"df_card_1_merch = df_card_1_unauth['merchant_id'].value_counts()\ndf_card_1_merch.head(10)","2b4025a4":"df_card_1_merch = df_card_2_unauth['merchant_id'].value_counts()\ndf_card_1_merch.head(10)","11657199":"df_card_new  = df_new_merchant_transactions['card_id'].value_counts().head(10)\ndf_card_new","7782f910":"df_non_authorized = df_new_merchant_transactions[df_new_merchant_transactions['authorized_flag'] == \"N\"]\ndf_non_authorized","88cf016a":"df_new_merchant_transactions.head()","31f1ef02":"df_new_merchant_transactions['purchase_amount_integer'] = df_new_merchant_transactions.purchase_amount.apply(lambda x: x == np.round(x))\nprint(df_new_merchant_transactions.groupby('purchase_amount_integer')['card_id'].count())","4e0d1030":"df_new_merchant_transactions['purchase_amount_new'] = np.round(df_new_merchant_transactions['purchase_amount'] \/ 0.00150265118 + 497.06,8)","05f65e87":"#rounding off two decimal places\ndf_new_merchant_transactions['purchase_amount_new'] = np.round(df_new_merchant_transactions['purchase_amount'] \/ 0.00150265118 + 497.06,2)\n\ndf_new_merchant_transactions['purchase_amount_integer'] = df_new_merchant_transactions.purchase_amount_new.apply(lambda x: x == np.round(x))\nprint(df_new_merchant_transactions.groupby('purchase_amount_integer')['card_id'].count())","f51ac557":"df_new_merchant_transactions.groupby('purchase_amount_new')['card_id'].count().reset_index(name='count').sort_values('count',ascending=False).head(100)","c571d12a":"df_historical_transactions = pd.get_dummies(df_historical_transactions, columns=['category_2', 'category_3'])\ndf_historical_transactions['authorized_flag'] = df_historical_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\ndf_historical_transactions['category_1'] = df_historical_transactions['category_1'].map({'Y': 1, 'N': 0})\n\ndf_historical_transactions.head()","5b237810":"df_historical_transactions['purchase_amount_integer'] = df_historical_transactions.purchase_amount.apply(lambda x: x == np.round(x))\nprint(df_historical_transactions.groupby('purchase_amount_integer')['card_id'].count())","bd08f095":"#df_historical_transactions['purchase_amount_new'] = np.round(df_historical_transactions['purchase_amount'] \/ 0.00150265118 + 497.06,8)\ndf_historical_transactions['purchase_amount_new'] = np.round(df_historical_transactions['purchase_amount'] \/ 0.00150265118 + 497.06,2)\ndf_historical_transactions['purchase_amount_integer'] = df_historical_transactions.purchase_amount_new.apply(lambda x: x == np.round(x))\nprint(df_historical_transactions.groupby('purchase_amount_integer')['card_id'].count())","d52d02c0":"df_historical_transactions.groupby('purchase_amount_new')['card_id'].count().reset_index(name='count').sort_values('count',ascending=False).head(100)","be8baab1":"del df_historical_transactions['purchase_amount_integer']","8da321ed":"df_historical_transactions.dtypes","b8febb75":"\ndef aggregate_transactions(trans, prefix):  \n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'category_3_A': ['mean'],\n        'category_3_B': ['mean'],\n        'category_3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'purchase_amount_new': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans","aa29c233":"###### Aggregate transaction function on the historical transaction ######\n\nimport gc\nmerch_hist = aggregate_transactions(df_historical_transactions, prefix='hist_')\n\ndel df_historical_transactions\ngc.collect()\ndf_train = pd.merge(df_train, merch_hist, on='card_id',how='left')\ndf_test = pd.merge(df_test, merch_hist, on='card_id',how='left')\n\ndel merch_hist\ngc.collect()\ndf_train.head()\n","d174225a":"df_new_merchant_transactions = pd.get_dummies(df_new_merchant_transactions, columns=['category_2', 'category_3'])\ndf_new_merchant_transactions['authorized_flag'] = df_new_merchant_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\ndf_new_merchant_transactions['category_1'] = df_new_merchant_transactions['category_1'].map({'Y': 1, 'N': 0})\n\ndf_new_merchant_transactions.head()","5ebe6436":"######Aggregate transaction function on the new merchant transaction ######\n\nmerch_new = aggregate_transactions(df_new_merchant_transactions, prefix='new_')\ndel df_new_merchant_transactions\ngc.collect()\ndf_train = pd.merge(df_train, merch_new, on='card_id',how='left')\ndf_test = pd.merge(df_test, merch_new, on='card_id',how='left')\n\ndel merch_new\ngc.collect()\ndf_train.head()","6db91e9e":"###### Dropping 3 columns ######\n\ntarget = df_train['target']\ndrops = ['card_id', 'first_active_month', 'target']\nuse_cols = [c for c in df_train.columns if c not in drops]\nfeatures = list(df_train[use_cols].columns)\ndf_train[features].head()","e46db9a6":"print(df_train[features].shape)\nprint(df_test[features].shape)","056d08ba":"df_train.corr()","25a98f13":"#### Utilizing KFold Cross Validator ####\n\nfrom sklearn.model_selection import KFold","cee8291c":"###LGB model detals for different parameters referenced from  the below notebook\n####https:\/\/www.kaggle.com\/peterhurford\/you-re-going-to-want-more-categories-lb-3-737\n\nparam = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\n################# 5  fold cross validation #################\n\n\n### Splitting the dataset into 5 folds for validation\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_5 = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\n\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=100)\n    oof_5[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits","13146e9d":"print('5 Fold Cross Validation - lgb - ', np.sqrt(mean_squared_error(oof_5, target)))\n","e68d3db1":"###LGB model detals for different parameters referenced from  the below notebook\n####https:\/\/www.kaggle.com\/peterhurford\/you-re-going-to-want-more-categories-lb-3-737\n\nparam = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\n\n\n################# 10  fold cross validation #################\n\n\n\n### Splitting the dataset into 10 folds for validation\nfolds = KFold(n_splits=10, shuffle=True, random_state=15)\noof_10 = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\n\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=100)\n    oof_10[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits","df7441b1":"print('10 Fold Cross Validation - lgb - ', np.sqrt(mean_squared_error(oof_10, target)))\n","ab44d251":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] =  predictions #0.5 * predictions_lgb + 0.5 * predictions_xgb\nsub_df.to_csv(\"submission.csv\", index=False)","db5405c2":"sub_df.tail(5)","f01104db":"pp.ProfileReport(df_historical_transactions)","a324ae08":"The above analysis signifies that there are no missing values in the train and test set","89f4260b":"* Similar to the historical transaction dataset, the categorical variables from the new merchant dataset are also converted with the below code snippet with the help of **get_dummies** function from the pandas library\n\n* Since there are only 2 levels in **authorized flag** and **category_1** , henceforth they have been mapped to binary values","a1e549b1":"Since the above data has some **integer values** and some  with \"** 0.55 and 0.99 **\" after the decimal places, henceforth the \"**purchase amount**\" can be considered as the money spent on the transaction\n","cd8d2593":"The above code depicts that none of the purchase amount values are integers. \n\nSome modification is required to make the data consistent which can then be consumed in the model for better predictions.\n\nThe below code snippet has been referenced from :\nhttps:\/\/www.kaggle.com\/raddar\/towards-de-anonymizing-the-data-some-insights","d3c75a66":"**Basic EDA on Historical and New_merchant files**","531ae9bf":"***All the transactions in the New Merchant transactions are authorized transactions. **","aa3f47d4":"* Converting the categorical data to numerical values with the help of **get_dummies** function from the pandas library\n\n* Since there are only 2 levels in **authorized flag** and **category_1** , henceforth they have been mapped to binary values","db1b112f":"**Top 10 cards with highest number of transactions in new merchant transactions dataset**","fc8310f3":"Since the above data too has some **integer values** and some  with \"**.50 and 0.99**\" after the decimal places, henceforth the \"**purchase amount**\" can be confirmed as the money spent on the transaction\n","d9ff7493":"                                                Elo Merchant Category Recommendation\n![image.png](attachment:image.png)\n\n","8cc631eb":"Only Feature 2 and Feature 1 are being converted to numerical values as Feature 3 will not impact the target score much because of negative correlation","8fd7cf36":"**Pandas Profiling package has been used below for taking the profile report of the datasets**","17feaade":"Light GBM is a gradient boosting framework that uses tree based learning algorithm.\n\nLight GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n\nLight GBM works this way\n![image.png](attachment:image.png)\n\nOther algorithms works this way\n![image.png](attachment:image.png)\n\n\nReferences : https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n\n","65d2cde8":"Elo is a pure domestic card brand in Brazil and only allows for transactions in Brazilian Real currency. Elo results from a partnership of three of the largest banks in Brazil: Banco do Brasil, Bradesco and CAIXA. The Elo brand supports credit, debit, and prepaid card transactions. More than 50 million Elo cards have been issued to date. The Elo credit card supports so-called 'installments' that allows the shopper to spread their payments for online purchases over a certain period of time. Cielo, one of the largest merchant acquirers in Brazil, allows ecommerce merchants to accept Elo in their online store. Most of the Brazilian and international Payment Service Providers are connected to Cielo to allow its customer to accept Elo online. Please find below all Payment Service Providers that enable merchants to accept Elo in their webshop.\n\n\nThe competition circulates around the loyalty of customers and the precise prediction of the same is the main judging criteria.\n\nThe notebook here comprises of basic Exploratory Data Analysis and then some deep dive into particular customer transactions and then the final prediction of the loyalty score on basis of the various features from the different datasets in consideration","f106bba6":"* The final recommendation is the above target value or the loyalty score for the Elo Platform.","d2b9ad8e":"\"C_ID_3d3dfdc692\" has the most unauthorized transactions at  \"M_ID_79692349d6\"    ","ce8cd2cc":"Importing Libraries","ee56f4c6":"Below are the findings for the merchants where these cards have done the  most unauthorized transactions","c4dd232c":"*** From the above correlation matrix , it is apparent that the features have very less correlation with target value.\nFeature 3 has the least correlation as it is closest to 0 and the Feature_1 and Feature_2 have a little negative correlation**","caff6b7f":"The below 2 card_ids have the most unauthorized transactions\n\n\n**C_ID_5ea401d358 ** --------- \n** C_ID_3d3dfdc692  **","e8378214":"**Calculating the mean squared error with the help light gradient booster model**","5525290a":"\"C_ID_5ea401d358\" has the most unauthorized transactions at \" M_ID_9fa00da7b2 \"    ","799b7c4a":"Top 10 cards with highest number of transactions in historical dataset","19218fb4":"The above plot signifies that most of the values of the target variable are between -9 to +9 .\n\nThere is an outlier value around -30","61936617":"Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n\nThe general procedure is as follows:\n1. Shuffle the dataset randomly.\n2. Split the dataset into k groups\n3. For each unique group:\n    1. Take the group as a hold out or test data set\n    2. Take the remaining groups as a training data set\n    3. Fit a model on the training set and evaluate it on the test set\n4. Retain the evaluation score and discard the model\n5. Summarize the skill of the model using the sample of model evaluation scores\n\n\nReferences: https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/","b715183b":"Moving on to the Historical and new merchant transactions table\nThe most interesting feature in transactions table seems to be the `purchase_amount`for a particular card_id \n\nIdeally the values should have been positive , but because the values have been normalized the data is different from expectation . Also,there should have been some integers in it's distribution which is obviously not the case here!"}}