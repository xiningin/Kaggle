{"cell_type":{"2448264a":"code","7cd540d3":"code","3ce96846":"code","28641f39":"code","5ec99911":"code","19019b8e":"code","e7edbdc7":"code","69fc2e3e":"code","9febb81b":"code","30757c3f":"code","9bbf52d0":"code","37be3750":"code","ff0fefba":"code","859f9a16":"code","6ea38591":"code","2d69de1f":"code","f3134b8b":"code","05b8579d":"code","aa6baade":"code","3fde23db":"markdown","252efd04":"markdown","358b90a7":"markdown","2078aaf1":"markdown","9a54f7d9":"markdown","6206f85f":"markdown","9ec620e7":"markdown","d8809d89":"markdown","a608f033":"markdown","7d6ff5f4":"markdown","6680f421":"markdown","9163ca61":"markdown","357c58bd":"markdown","cea826cf":"markdown","5d1d43a5":"markdown","55af33ee":"markdown","cc1d2271":"markdown","f4c84b0b":"markdown","96c70bb4":"markdown","1d08631b":"markdown","1b9b4146":"markdown","d29d837e":"markdown","d76dbd33":"markdown","07955903":"markdown","ef125492":"markdown","4c653fdc":"markdown","5eb62d43":"markdown","ce85e88a":"markdown","fca49711":"markdown"},"source":{"2448264a":"# regular expression operations\nimport re    \n# string operation \nimport string  \n# shuffle the list\nfrom random import shuffle\n\n# linear algebra\nimport numpy as np \n# data processing\nimport pandas as pd \n\n# NLP library\nimport nltk\n# download twitter dataset\nfrom nltk.corpus import twitter_samples                          \n\n# module for stop words that come with NLTK\nfrom nltk.corpus import stopwords          \n# module for stemming\nfrom nltk.stem import PorterStemmer        \n# module for tokenizing strings\nfrom nltk.tokenize import TweetTokenizer   \n\n# scikit model selection\nfrom sklearn.model_selection import train_test_split\n\n# smart progressor meter\nfrom tqdm import tqdm","7cd540d3":"# Download the twitter sample data from NLTK repository\nnltk.download('twitter_samples')","3ce96846":"# read the positive and negative tweets\npos_tweets = twitter_samples.strings('positive_tweets.json')\nneg_tweets = twitter_samples.strings('negative_tweets.json')\nprint(f\"positive sentiment \ud83d\udc4d total samples {len(pos_tweets)} \\nnegative sentiment \ud83d\udc4e total samples {len(neg_tweets)}\")","28641f39":"# Let's have a look at the data\nno_of_tweets = 3\nprint(f\"Let's take a look at first {no_of_tweets} sample tweets:\\n\")\nprint(\"Example of Positive tweets:\")\nprint('\\n'.join(pos_tweets[:no_of_tweets]))\nprint(\"\\nExample of Negative tweets:\")\nprint('\\n'.join(neg_tweets[:no_of_tweets]))","5ec99911":"# helper class for doing preprocessing\nclass Twitter_Preprocess():\n    \n    def __init__(self):\n        # instantiate tokenizer class\n        self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                                       reduce_len=True)\n        # get the english stopwords \n        self.stopwords_en = stopwords.words('english') \n        # get the english punctuation\n        self.punctuation_en = string.punctuation\n        # Instantiate stemmer object\n        self.stemmer = PorterStemmer() \n        \n    def __remove_unwanted_characters__(self, tweet):\n        \n        # remove retweet style text \"RT\"\n        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n\n        # remove hyperlinks\n        tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n     \n        # remove hashtags\n        tweet = re.sub(r'#', '', tweet)\n        \n        #remove email address\n        tweet = re.sub('\\S+@\\S+', '', tweet)\n        \n        # remove numbers\n        tweet = re.sub(r'\\d+', '', tweet)\n        \n        ## return removed text\n        return tweet\n    \n    def __tokenize_tweet__(self, tweet):        \n        # tokenize tweets\n        return self.tokenizer.tokenize(tweet)\n    \n    def __remove_stopwords__(self, tweet_tokens):\n        # remove stopwords\n        tweets_clean = []\n\n        for word in tweet_tokens:\n            if (word not in self.stopwords_en and  # remove stopwords\n                word not in self.punctuation_en):  # remove punctuation\n                tweets_clean.append(word)\n        return tweets_clean\n    \n    def __text_stemming__(self,tweet_tokens):\n        # store the stemmed word\n        tweets_stem = [] \n\n        for word in tweet_tokens:\n            # stemming word\n            stem_word = self.stemmer.stem(word)  \n            tweets_stem.append(stem_word)\n        return tweets_stem\n    \n    def preprocess(self, tweets):\n        tweets_processed = []\n        for _, tweet in tqdm(enumerate(tweets)):        \n            # apply removing unwated characters and remove style of retweet, URL\n            tweet = self.__remove_unwanted_characters__(tweet)            \n            # apply nltk tokenizer\n            tweet_tokens = self.__tokenize_tweet__(tweet)            \n            # apply stop words removal\n            tweet_clean = self.__remove_stopwords__(tweet_tokens)\n            # apply stemmer \n            tweet_stems = self.__text_stemming__(tweet_clean)\n            tweets_processed.extend([tweet_stems])\n        return tweets_processed","19019b8e":"# initilize the text preprocessor class object\ntwitter_text_processor = Twitter_Preprocess()\n\n# process the positive and negative tweets\nprocessed_pos_tweets = twitter_text_processor.preprocess(pos_tweets)\nprocessed_neg_tweets = twitter_text_processor.preprocess(neg_tweets)","e7edbdc7":"pos_tweets[:no_of_tweets], processed_pos_tweets[:no_of_tweets]","69fc2e3e":"# BOW frequency represent the (word, y) and frequency of y class\ndef build_bow_dict(tweets, labels):\n    freq = {}\n    ## create zip of tweets and labels\n    for tweet, label in list(zip(tweets, labels)):\n        for word in tweet:\n            freq[(word, label)] = freq.get((word, label), 0) + 1\n        \n    return freq","9febb81b":"# create labels of the tweets\n# 1 for positive labels and 0 for negative labels\nlabels = [1 for i in range(len(processed_pos_tweets))]\nlabels.extend([0 for i in range(len(processed_neg_tweets))])\n\n# combine the positive and negative tweets\ntwitter_processed_corpus = processed_pos_tweets + processed_neg_tweets\n\n# build Bog of words frequency \nbow_word_frequency = build_bow_dict(twitter_processed_corpus, labels)","30757c3f":"# extract feature for tweet\ndef extract_features(processed_tweet, bow_word_frequency):\n    # feature array\n    features = np.zeros((1,3))\n    # bias term added in the 0th index\n    features[0,0] = 1\n    \n    # iterate processed_tweet\n    for word in processed_tweet:\n        # get the positive frequency of the word\n        features[0,1] = bow_word_frequency.get((word, 1), 0)\n        # get the negative frequency of the word\n        features[0,2] = bow_word_frequency.get((word, 0), 0)\n    \n    return features","9bbf52d0":"# shuffle the positive and negative tweets\nshuffle(processed_pos_tweets)\nshuffle(processed_neg_tweets)\n\n# create positive and negative labels\npositive_tweet_label = [1 for i in processed_pos_tweets]\nnegative_tweet_label = [0 for i in processed_neg_tweets]\n\n# create dataframe\ntweet_df = pd.DataFrame(list(zip(twitter_processed_corpus, positive_tweet_label+negative_tweet_label)), columns=[\"processed_tweet\", \"label\"])","37be3750":"# train and test split\ntrain_X_tweet, test_X_tweet, train_Y, test_Y = train_test_split(tweet_df[\"processed_tweet\"], tweet_df[\"label\"], test_size = 0.20, stratify=tweet_df[\"label\"])\nprint(f\"train_X_tweet {train_X_tweet.shape}, test_X_tweet {test_X_tweet.shape}, train_Y {train_Y.shape}, test_Y {test_Y.shape}\")","ff0fefba":"# train X feature dimension\ntrain_X = np.zeros((len(train_X_tweet), 3))\n\nfor index, tweet in enumerate(train_X_tweet):\n    train_X[index, :] = extract_features(tweet, bow_word_frequency)\n\n# test X feature dimension\ntest_X = np.zeros((len(test_X_tweet), 3))\n\nfor index, tweet in enumerate(test_X_tweet):\n    test_X[index, :] = extract_features(tweet, bow_word_frequency)\n\nprint(f\"train_X {train_X.shape}, test_X {test_X.shape}\")","859f9a16":"train_X[0:5]","6ea38591":"def sigmoid(z): \n    \n    # calculate the sigmoid of z\n    h = 1 \/ (1+ np.exp(-z))\n    \n    return h","2d69de1f":"# implementation of gradient descent algorithm  \n\ndef gradientDescent(x, y, theta, alpha, num_iters):\n\n    # get the number of samples in the training\n    m = x.shape[0]\n    \n    for i in range(0, num_iters):\n        \n        # find linear regression equation value, X and theta\n        z = np.dot(x, theta)\n        \n        # get the sigmoid of z\n        h = sigmoid(z)\n \n        # calculate the cost function, log loss\n        J = (-1\/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h)))\n\n        # update the weights theta\n        theta = theta - (alpha \/ m) * np.dot((x.T), (h - y))\n   \n    J = float(J)\n    return J, theta","f3134b8b":"# set the seed in numpy\nnp.random.seed(1)\n# Apply gradient descent of logistic regression\nJ, theta = gradientDescent(train_X, np.array(train_Y).reshape(-1,1), np.zeros((3, 1)), 1e-7, 1000)\nprint(f\"The cost after training is {J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")","05b8579d":"# predict for the features from learned theata values\ndef predict_tweet(x, theta):\n    \n    # make the prediction for x with learned theta values\n    y_pred = sigmoid(np.dot(x, theta))\n    \n    return y_pred","aa6baade":"# predict for the test sample with the learned weights for logistics regression\npredicted_probs = predict_tweet(test_X, theta)\n# assign the probability threshold to class\npredicted_labels = np.where(predicted_probs > 0.5, 1, 0)\n# calculate the accuracy\nprint(f\"Accuracy is {len(predicted_labels[predicted_labels == np.array(test_Y).reshape(-1,1)]) \/ len(test_Y)*100:.2f}\")","3fde23db":"The preprocessing is one of the important step in the pipeline. It includes cleaning and removing the uncessary data before building a machine learning model. The following are simple and basic preprocessing steps will do for our tweets:\n\n* Tokenizing the string\n   - Convert the tweet into lowercase and split the tweets into tokens(words)\n* Removing stop words and punctuation\n    - Will remove some strings commonly used on the twitter platform like the hashtag, retweet marks, hyperlinks, numbers and email address\n* Stemming\n    - This is the process of converting a word to it's most general form. This helps in reducing the size of our vocabulary. For example, the stemmed word `engag` will have the following different words,    \n        - <strong>engag<\/strong>ement\n        - <strong>engag<\/strong>ed\n        - <strong>engag<\/strong>ing\n\nLet's see how we can implement this.","252efd04":"It is time to test our logistic regression function on test data that model has not seen before. \n\nPredict whether a tweet is positive or negative. \n\n* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n\n$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$","358b90a7":"<img src=\"https:\/\/www.equiskill.com\/wp-content\/uploads\/2018\/07\/WhatsApp-Image-2020-02-11-at-8.30.11-PM.jpeg\" title=\"logistic regression intuition\">\n\nThe goal of this kernal is to implement the logistic regression from scratch for sentiment analysis using twitter dataset. Will be mainly focusing on building blocks of logistic regression at own. This kernal can provides the in-depth understanding of <strong><i>how logistic regression works internally<\/i><\/strong>.\n\nGiven a tweet, Will be classifying if it has <strong>positive sentiment \ud83d\udc4d or negative sentiment \ud83d\udc4e<\/strong>. It is very useful for beginners and others as well.\n\n<font size=-1><i>Leave your comments or thought about this kernal for improvements. This is my first notebook. I am learning by doing it. Your feedback is highly appreciated to boost my confidence.<\/i><\/font>","2078aaf1":"### <font color=\"#007bff\"><b>4.1 Overview<\/b><\/font><br><a id=\"4.1\"><\/a>","9a54f7d9":"Take a look at sample train features. \n* 0th index is bias term added. \n* 1st index is representing positive word frequency\n* 2nd index is representing negative word frequency","6206f85f":"Now, we have the various methods to represent features for our twitter corpus. Some of the basic and powerful techniques are\n\n    - 1. CountVectorizer \n    - 2. TF-IDF feature\n\n<h4>1. CountVectorizer<\/h4>\nCount vectorizer indicates the sparse matrix and the value can be <strong>frequncy of the word<\/strong>. The each column is unique token in our corpus. \n\n> The sparse matrix dimension would be `no of unique tokens in the corpus * no of sample tweets`. \n\nExample: `\ncorpus = [\n     'This is the first document.',\n     'This document is the second document.',\n     'And this is the third one.',\n     'Is this the first document?',\n ]\n` and the CountVectorizer representation is\n\n`[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n`\n\n<h4>2. TF-IDF (Term Frequency - Inverse Document Frequence)<\/h4>\nTF-IDF statistical measure that evaluates how relevant a word is to a document in a collection of documents. TF-IDF is computed as follows:\n\n$$ tiidf(t, d, D) = tf(t,d)*idf(t,D) $$\n\n<strong>Term Frequency:<\/strong> term frequency $ tf(t,d) $, the simplest choice is to use the frequency of a term (word) in a document.\n<strong>Inverse Document Frequency:<\/strong> $idf(t,D)$ measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the <strong> logarthmic scaled <\/strong> of inverse fraction of the document that contain word. Definition is as per [Wiki](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).\n","9ec620e7":"### <font color=\"#007bff\"><b>2.1 Preprocess the text<\/b><\/font><br><a id=\"2.1\"><\/a>","d8809d89":"Let's take a look at what output got after preprocessed tweets. It's good that we able to process the tweets successfully.","a608f033":"## <font color=\"#007bff\"><b>4. Implementation of Logistic Regression<\/b><\/font><br><a id=\"4\"><\/a>","7d6ff5f4":"Shuffle the corpus and will split the train and test set.","6680f421":"* The `twitter_samples` contains 5,000 positive tweets and 5,000 negative tweets. Total count of 10,000 tweets.  \n* This is highly balanced dataset since we have same no of data samples in each class.","9163ca61":"## <font color=\"#007bff\"><b>2. Load the data<\/b><\/font><br><a id=\"2\"><\/a>","357c58bd":"* Tweets may have URL, numbers and special characters. Hence, we need to preprocess the text. ","cea826cf":"## <font color=\"#007bff\"><b>5. Train model<\/b><\/font><br><a id=\"5\"><\/a>","5d1d43a5":"The cost function used in logistic regression is:\n$$ -\\frac{1}{M}(ylog(p) - (1-y) log(1-p))  \\tag{3}$$\n\nThis is the <strong>Log loss of binary classification.<\/strong> The average of the log loss across all training samples is calculated in logistic regression, the equation ${3}$ modified for all the training samples as follows:\n\n$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\tag{4}$$\n* $m$ is the number of training examples\n* $y^{(i)}$ is the actual label of the i-th training example.\n* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n\nThe loss function for a single training example is\n$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n\n* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n* When the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. \n* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss.\n","55af33ee":"### <font color=\"#007bff\"><b>4.3 Cost function<\/b><\/font><br><a id=\"4.3\"><\/a>","cc1d2271":"* Given the text, It is very important to represent into `features (numeric values)` such a way that we can feed into the model.\n\n### <font color=\"#007bff\"><b>3.1 Create a Bag Of Words (BOW) representation<\/b><\/font><br><a id=\"3.1\"><\/a>\n\nBOW represents the word and it's frequency with respect to the class. Will create a `dict` for storing frequency of `positive` and `negative` class for each word. Let assume `positive` tweet is `1` and `negative` tweet is `0`.\nThe `dict` key is tuple containing the`(word, y)` pair. The `word` is processed word and `y` indicates the label of the class. The dict value represent the `frequency of the word` with respect to `y` class. \n\nExample:\n    # word good occurs 32 time in the 1.0 (positive) class \n    {(\"good\", 1.0) : 32}\n    \n    # word bad occurs 45 time in the 0 (negative) class \n    {(\"bad\", 0) : 45}","f4c84b0b":"### <font color=\"#007bff\"><b>4.4 Gradient Descent<\/b><\/font><br><a id=\"4.4\"><\/a>","96c70bb4":"## Table of Content\n\n* [1. Objective](#1) \n* [2. Load the data](#2)\n    - [2.1 Preprocess the text](#2.1)\n* [3. Extract features from text](#3)\n    - [3.1 Create a Bag Of Words (BOW) representation](#3.1)\n    - [3.2. Extracting simple features for our model](#3.2)\n    - [3.3 Train and Test split](#3.3)\n* [4. Implementation of Logistic Regression](#4)\n    - [4.1 Overview](#4.1)\n    - [4.2 Sigmoid](#4.2)\n    - [4.3 Cost function](#4.3)\n    - [4.4 Gradient Descent](#4.4)\n* [5. Train model](#5)\n* [6. Test our logistic regression](#6)\n\n","1d08631b":"Gradient Descent is a algorithm used for <strong> updating the weights $\\theta$<\/strong> iteratively to minimize the objective function (cost). Why we need to update the weigths iteratively because,\n\n> At initial random weights, model doesn't learn anything much. To imporve the prediction we need to learn the from the data with multiple iteration and tune the random weights accordingly.\n \nThe gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n\n$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n* 'i' is the index across all 'm' training examples.\n* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n\n* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n","1b9b4146":"## <font color=\"#007bff\"><b>1. Objective<\/b><\/font><br><a id=\"1\"><\/a>","d29d837e":"Lets keep the 80% data for training and 20% data samples for testing","d76dbd33":"### <font color=\"#007bff\"><b>4.2 Sigmoid<\/b><\/font><br><a id=\"4.2\"><\/a>\n\n* The sigmoid function is defined as: \n\n$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{2}$$\n\nIt maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a <strong>probability<\/strong>. ","07955903":"## Import necessary Packages","ef125492":"Now, Lets see how logistic regression works and the implementation.\n\nMost of the time, when you hear about logistic regression you may think, it is a regression problem. No it is not, <strong>Logistic regression<\/strong> is classification problem and it is a non-linear model.\n\n\n![overview of logistic regression](attachment:image.png)\n\nAs shown in the above picture, there are 4 stages for most of the ML algorithms,\n* Step 1. Initialize the weights\n    - Random weights initialized\n* Step 2. Apply function \n    - Calculate the sigmoid\n* Step 3. Calculate the cost (objective of the algorithm)\n    - Calculate the log-loss for binary classification\n* Step 4. Gradient Descent \n    - Update the weights iteratively till finding the minimum cost\n\nLogistic regression takes a linear regression and applies a <strong>sigmoid<\/strong> to the output of the linear regression. So, It produces the probability of each class and it sums up to 1.\n\n<h6><strong>Regression:<\/strong><\/h6>\nSingle linear regression equation as follows:\n\n$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N \\tag{1}$$\n* Note that the $\\theta$ values are \"weights\" \n* $x_0, x_1, x_2,... x_N$ is input features\n\nYou may think of how complicated equation it is. We need to multiply all the weigths with each feature at `ith` position then sums up all.\n\n> Fortunately, <strong>Linear algebra<\/strong> brings this equation with ease of operation. Yes, It is a matrix `dot` product. You can apply the dot product of features and weights to find the $z$.\n> \n<h6><strong>Logistic regression:<\/strong><\/h6>\nApplying the <strong>sigmoid<\/strong> function to equation $1$ becames <strong>logits<\/strong> function. \n\n$$ h(z) = \\frac{1}{1+\\exp^{-z}} $$\n","4c653fdc":"## <font color=\"#007bff\"><b>3. Extract features from text<\/b><\/font><br><a id=\"3\"><\/a>","5eb62d43":"### <font color=\"#007bff\"><b>3.2. Extracting simple features for our model<\/b><\/font><br><a id=\"3.2\"><\/a>\n\n\n* Given a list of tweets, will be extracting two features.\n    * The first feature is the number of positive words in a tweet.\n    * The second feature is the number of negative words in a tweet. \n    \nThis seems to be simple isn't it? Perhaps yes. We are not representing our features to sparse matrix. Will use simplest features for our analysis.\n","ce85e88a":"## <font color=\"#007bff\"><b>6. Test our logistic regression<\/b><\/font><br><a id=\"6\"><\/a>","fca49711":"\n### <font color=\"#007bff\"><b>3.3 Train and Test split<\/b><\/font><br><a id=\"3.3\"><\/a>"}}