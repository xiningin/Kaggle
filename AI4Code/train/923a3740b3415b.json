{"cell_type":{"31d74329":"code","a6425752":"code","a2658046":"code","2778fba0":"code","284e1498":"code","b0b17987":"code","d5c320eb":"code","c2cee681":"code","da38f83d":"code","78c3a5e5":"code","30e850ad":"code","6553c11c":"code","4b5bb7c8":"code","c385da9c":"code","12dcf150":"code","a464931d":"code","2dcc9d1d":"code","f6db600d":"code","dab733b5":"markdown","0daf2300":"markdown","782d8a07":"markdown","e6faccea":"markdown","3ff49dc7":"markdown","06191336":"markdown","bb5deb68":"markdown","48974592":"markdown","f0bee722":"markdown","0e0056c3":"markdown"},"source":{"31d74329":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.applications import EfficientNetB3","a6425752":"\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","a2658046":"train.head()","2778fba0":"train.shape","284e1498":"Y_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"],axis = 1)\nX_train = X_train \/ 255.0\nX_test = test \/ 255.0\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\nY_train = to_categorical(Y_train, num_classes = 10)","b0b17987":"import matplotlib\nimport matplotlib.pyplot as plt\n# EXTRA\ndef plot_digits(instances, images_per_row=10, **options):\n    size = 28\n    images_per_row = min(len(instances), images_per_row)\n    images = [instance.reshape(size,size) for instance in instances]\n    n_rows = (len(instances) - 1) \/\/ images_per_row + 1\n    row_images = []\n    n_empty = n_rows * images_per_row - len(instances)\n    images.append(np.zeros((size, size * n_empty)))\n    for row in range(n_rows):\n        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n        row_images.append(np.concatenate(rimages, axis=1))\n    image = np.concatenate(row_images, axis=0)\n    plt.imshow(image, cmap = matplotlib.cm.binary, **options)\n    plt.axis(\"off\")\n    \nplt.figure(figsize=(9,9))\nexample_images = np.r_[X_train[:12000:600], X_train[13000:30600:600], X_train[30600:60000:590]]\nplot_digits(example_images, images_per_row=10)\nplt.show()","d5c320eb":"datagen = ImageDataGenerator(\n        rotation_range=10,  \n        zoom_range = 0.10,  \n        width_shift_range=0.1, \n        height_shift_range=0.1)","c2cee681":"# PREVIEW AUGMENTED IMAGES\nX_train3 = X_train[9,].reshape((1,28,28,1))\nY_train3 = Y_train[9,].reshape((1,10))\nplt.figure(figsize=(15,4.5))\nfor i in range(30):  \n    plt.subplot(3, 10, i+1)\n    X_train2, Y_train2 = datagen.flow(X_train3,Y_train3).next()\n    plt.imshow(X_train2[0].reshape((28,28)),cmap=plt.cm.binary)\n    plt.axis('off')\n    if i==9: X_train3 = X_train[11,].reshape((1,28,28,1))\n    if i==19: X_train3 = X_train[18,].reshape((1,28,28,1))\nplt.subplots_adjust(wspace=-0.1, hspace=-0.1)\nplt.show()","da38f83d":"X_train2.shape","78c3a5e5":"#https:\/\/www.kaggle.com\/ateplyuk\/tpu-tensor-processing-unit-mnist-efficientnet\nenet = EfficientNetB3(input_shape=(32, 32, 3), weights='imagenet',include_top=False) ","30e850ad":"X_train = np.pad(X_train, ((0,0), (2,2), (2,2), (0,0)), mode='constant')\nX_train.shape","6553c11c":"X_train = np.squeeze(X_train, axis=-1)\nX_train = stacked_img = np.stack((X_train,)*3, axis=-1)\nX_train.shape","4b5bb7c8":"nets = 2\nmodel = [0] *nets\nfor j in range(nets):\n    model[j] = Sequential(enet)\n    model[j].add(Flatten())\n    model[j].add(Dense(units=1024, use_bias=True, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dense(units=512, use_bias=True, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(.4))\n    model[j].add(Dense(units=256, use_bias=True, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(.4))\n    model[j].add(Dense(units=10, use_bias=True, activation='softmax'))\n    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","c385da9c":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\nhistory = [0] * nets\nepochs = 45\nfor j in range(nets):\n    X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.1)\n    history[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64),\n        epochs = epochs, steps_per_epoch = X_train2.shape[0]\/\/64,  \n        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n    print(\"EffNet {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j+1,epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","12dcf150":"X_test = np.pad(X_test, ((0,0), (2,2), (2,2), (0,0)), mode='constant')\nX_test = np.squeeze(X_test, axis=-1)\nX_test = stacked_img = np.stack((X_test,)*3, axis=-1)\nX_test.shape","a464931d":"results = np.zeros( (X_test.shape[0],10) ) \nfor j in range(nets):\n    results = results + model[j].predict(X_test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"MNIST_EffNet_Ensemble.csv\",index=False)","2dcc9d1d":"X_test1a = test \/ 255.0\nX_test1a = X_test1a.values.reshape(-1,28,28,1)","f6db600d":"plt.figure(figsize=(15,6))\nfor i in range(40):  \n    plt.subplot(4, 10, i+1)\n    plt.imshow(X_test1a[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"predict=%d\" % results[i],y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","dab733b5":"## Prepare Training Data for EfficientNet B3\n\nhttps:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/","0daf2300":"## Architecture\n\nUsing the AutoML MNAS framework which optimizes for accuracy and efficiency a new baseline network was created.  The baseline network of the EfficientNet B0 can be seen below:\n![image.png](attachment:image.png)","782d8a07":"## View our Predictions","e6faccea":"# EfficientNet 0.99646 Accuracy Using Data Augmentation\n\nFor the purpose of illustration, this notebook ensembles only 2 networks.  If 20 networks are used then the above results are acheived.\n\nThis notebook builds upon the [25 Million Images!](http:\/\/https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\/output) notebook but uses EfficientNet B3.  Having never used EfficeintNet before and being fairly new to image classification I thought this would be a good place to give it a try.  \n\nGoogle, in a 2019 paper, proposed a novel model scaling method, and with recent progress in AutoML, developed EfficientNets.  The entire premise behind EfficientNet is in regards to the following question:  What if we could find a more principled method to scale up a CNN to obtain better accuracy and efficiency?\n\n## Model Scaling\n\n![image.png](attachment:image.png)\n\nScaling different dimesnions of models will lead to different results.  Google found that when balancing all dimensions of a network (width, depth, resolution) leads to the best overall performance.  EfficientNet operates by performing a grid search to find relationships between the different scaling dimensions while having the constraint of fixed reasources.  This grid search determines the approrpiate scaling coefficient for each of the scaling dimensions.  ","3ff49dc7":"Here are the results when running 20 networks (done locally because of time constraint):\n\n![image.png](attachment:image.png)","06191336":"## Exploratory Data Analysis","bb5deb68":"## Build Networks and Train","48974592":"## Prepare Test Data and Submit Results","f0bee722":"## Performance\n\nUsing ImageNet it was found that EfficientNet models were able to be more accurate and more efficient than existing CNN's.  Below is the performance comparision of each EfficientNet model:\n![image.png](attachment:image.png)\n\nSource: [EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling ](http:\/\/ai.googleblog.com\/2019\/05\/efficientnet-improving-accuracy-and.html)\n\n## Load Libraries and Data","0e0056c3":"## Data Augmentation"}}