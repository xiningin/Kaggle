{"cell_type":{"d24e9d9c":"code","a8a64865":"code","ad4fd4be":"markdown","6bf6b95e":"markdown","dc5cc30f":"markdown","5f3f5232":"markdown","984ffa37":"markdown","f4a00a6a":"markdown"},"source":{"d24e9d9c":"import numpy as np\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\nY = np.array([1, 1, 1, 2, 2, 2])\n\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X, Y)\nprint(clf.predict([[-0.8, -1]]))\n\nclf_pf = GaussianNB()\nclf_pf.partial_fit(X, Y, np.unique(Y))\nprint(clf_pf.predict([[-0.8, -1]]))\n\nX = np.random.randint(5, size=(6, 100))\ny = np.array([1, 2, 3, 4, 5, 6])\n\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(X, y)\nprint(clf.predict(X[2:3]))","a8a64865":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\n# make_classification : Generate a random n-class classification problem\nX, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=0, shuffle=False)\nclf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\nclf.fit(X, y)  \nprint(clf.feature_importances_)\nprint(clf.predict([[0, 0, 0, 0]]))","ad4fd4be":"This is part 3 of series ML Algos and how to use them. <br>\nIf you have not checked out part 1, kindly visit \n- [ML Algos Part 1](https:\/\/www.kaggle.com\/nee2shaji\/for-novices-ml-algos-how-to-use-them-part-1)\n- [ML Algos Part 2](https:\/\/www.kaggle.com\/nee2shaji\/for-novices-ml-algos-how-to-use-them-part-2)\n<br><br>This Part 3 consists of following models<br>\n- Na\u00efve Bayes Classifiers\n- Random Forests\n- Gradient Boosted Decision Trees\n- Neural Networks","6bf6b95e":"**Random Forests**<br>\n- An ensemble of trees, not just one tree. Widely used, very good results on many problems.\n- Make a prediction for every tree in the forest and combine individual predictions\n    - Regression: mean of individual tree predictions.\n    - Classification: Each tree gives probability for each class, Probabilities averaged across trees, Predict the class with highest probability.\n- One decision tree \u2192 Prone to overfitting. Many decision trees \u2192 More stable, better generalization\n- Ensemble of trees should be diverse: introduce random variation into tree-building.\n<br><br>\n**Parameters to be looked at:**<br>\n- max_features : has a strong effect on performance. Influences the diversity of trees in the forest. Setting max_features = 1 leads to forests with diverse, more complex trees. Setting max_features = close to number of features will lead to similar forests with simpler trees. Default works well in practice, but adjusting may lead to some further gains. \n- n_estimators: number of trees to use in ensemble (default: 10). Should be larger for larger datasets to reduce overfitting (but uses more computation).\n- max_depth: controls the depth of each tree (default: None. Splits until all leaves are pure).\n- n_jobs: How many cores to use in parallel during training.\n- Choose a fixed setting for the random_state parameter if you need reproducible results.\n<br><br>\n**Remarks:**<br>\n- Doesn't require careful normalization of features or extensive parameter tuning. Like decision trees, handles a mixture of feature types.\n- Easily parallelized across multiple CPUs.\n- Like decision trees, random forests may not be a good choice for very high- dimensional tasks (e.g. text classifiers) compared to fast, accurate linear models.\n- The resulting models are often difficult for humans to interpret.","dc5cc30f":"References:<br>\n* https:\/\/scikit-learn.org\/\n* Applied Machine Learning by Kevyn Collins-Thompson","5f3f5232":"**Gradient Boosted Decision Trees**<br>\n- Training builds a series of small decision trees.\n- Each tree attempts to correct errors from the previous stage.\n<br><br>\n**Parameters to be looked at:**<br>\n- n_estimators: sets # of small decision trees to use (weak learners) in the ensemble.\n- learning_rate: controls how hard each new tree tries to correct remaining mistakes from previous round. High learning rate: more complex trees. Low learning rate: simpler trees\n- The above two are typically tuned together. n_estimators is adjusted first, to best exploit memory and CPUs during training, then other parameters.\n- max_depth is typically set to a small value (e.g. 3-5) for most applications.\n<br><br>\n**Remarks:**<br>\n- Often best off-the-shelf accuracy on many problems. Using model for prediction requires only modest memory and is fast.\n- Doesn't require careful normalization of features to perform well. Like decision trees, handles a mixture of feature types.\n- Like random forests, the models are often difficult for humans to interpret.\n- Requires careful tuning of the learning rate and other parameters.\n- Training can require significant computation.\n- Like decision trees, not recommended for text classification and other problems with very high dimensional sparse features, for accuracy and computational cost reasons.","984ffa37":"**Neural Networks**<br>\n- They form the basis of state-of-the-art models and can be formed into advanced architectures that effectively capture complex features given enough data and computation. (MLPClassifier and MLPRegressor)\n- Larger, more complex models require significant training time, data, and customization.\n- Careful preprocessing of the data is needed.\n- A good choice when the features are of similar types, but less so when features of very different types.\n\n<br><br>\n**Paramters to be looked at:**<br>\n- hidden_layer_sizes: sets the number of hidden layers (number of elements in list), and number of hidden units per layer (each list element). Default: (100).\n- alpha: controls weight on the regularization penalty that shrinks weights to zero. Default: alpha = 0.0001.\n- activation: controls the nonlinear function used for the activation function, including: 'relu' (default), 'logistic', 'tanh'.","f4a00a6a":"**Naive Bayes Classifier**<br>\n- These classifiers are called 'Na\u00efve' because they assume that features are conditionally independent, given the class.\n- In other words: they assume that, for all instances of a given class, the features have little\/no correlation with each other.\n- Highly efficient learning and prediction. But generalization performance may worse than more sophisticated learning methods.\n- Classifier types:\n    - Bernoulli: binary features (e.g. word presence\/absence)\n    - Multinomial: discrete features (e.g. word counts)\n    - Gaussian: continuous\/real-valued features\n<br><br>\n**Parameters to be looked at:**<br>\n- For GaussianNB\n    - priors : Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n    - var_smoothing : optional (default=1e-9) Portion of the largest variance of all features that is added to variances for calculation stability.\n- For MultinomialNB\n    - alpha : optional (default=1.0). Additive (Laplace\/Lidstone) smoothing parameter (0 for no smoothing).\n    - fit_prior : optional (default=True). Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n    - class_prior : Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n<br><br>\n**Remarks:**<br>\n- Simple, efficient parameter estimation\n- Works well with high- dimensional data\n- Often useful as a baseline comparison against more sophisticated methods\n- Assumption that features are conditionally independent given the class is not realistic. As a result, other classifier types often have better generalization performance."}}