{"cell_type":{"af50167c":"code","3c7c9994":"code","2b73b6af":"code","7556249b":"code","4c3cedab":"code","8322b0d0":"code","3f022ea4":"code","394921c5":"code","8f308c3f":"code","dfb0d4f3":"code","70f935bd":"code","20c30e0e":"markdown","44a01147":"markdown","17902363":"markdown","7cfb8ed3":"markdown","616a016c":"markdown","163aa589":"markdown","e0833dc8":"markdown","596c2cde":"markdown","a28b4d73":"markdown","30efc03b":"markdown","33e3ea1a":"markdown","6d0cab15":"markdown","96e67c10":"markdown"},"source":{"af50167c":"# This Python 3 environment is this kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np # linear algebrac\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom fastai.vision import *\nimport csv\nimport glob\nimport os\n\ncurrPath = os.getcwd()\nos.mkdir(currPath+\"\/data\")\nos.mkdir(currPath+\"\/data\/positive\")\nos.mkdir(currPath+\"\/data\/negative\")\n\nprint('Great! You clicked on it correctly.')","3c7c9994":"finding = \"atelectasis\"","2b73b6af":"# https:\/\/www.kaggle.com\/rtatman\/import-functions-from-kaggle-script [todo]\n# search csv for finding, and return arrays of positive and negative cases\ndef lookfor(finding):\n    reader = csv.reader(open(\"..\/input\/sample\/sample_labels.csv\"), delimiter=\",\")\n    positive, negative = [], []\n    PREFIX = \"..\/input\/sample\/sample\/images\/\"\n    next(reader) # skip header line\n    for row in reader:\n        if finding.lower() in row[1].lower(): positive.append(PREFIX+row[0]) # row[0] is filename\n        else: negative.append(PREFIX+row[0])\n    print(\"positive: \"+str(len(positive))+\" images\")\n    print(\"negative: \"+str(len(negative))+\" images\")\n    size = min(len(positive), len(negative)) # take the smaller of two numbers\n    print()\n    if size > 100: print(\"Great! We'll use \"+str(size)+\" positive and \"+str(size)+\" negative cases in our set.\")\n    else: print(\"We don't have enough data to work with. Let's try another finding.\")\n    return positive, negative, size\n\n# use arrays of filenames to rearrange positive and negative cases into respective folders\ndef moveimages(positive, negative, size):\n    # wipe directories first\n    for file in glob.glob('\/kaggle\/working\/data\/positive\/*.png'):\n        os.unlink(file)\n    for file in glob.glob('\/kaggle\/working\/data\/negative\/*.png'):\n        os.unlink(file)\n    for x in range(size):\n        shutil.copy(positive[x], \"\/kaggle\/working\/data\/positive\")\n        shutil.copy(negative[x], \"\/kaggle\/working\/data\/negative\")","7556249b":"positive, negative, size = lookfor(finding)","4c3cedab":"path = Path('\/kaggle\/working\/data\/')\nmoveimages(positive, negative, size)","8322b0d0":"np.random.seed(42)\ndata = ImageDataBunch.from_folder(path, train=\".\", valid_pct=0.2,\n        ds_tfms=get_transforms(), size=256, num_workers=4).normalize(imagenet_stats)\ndata.show_batch(rows=3, figsize=(16,16))","3f022ea4":"learn = create_cnn(data, models.resnet34, metrics=[accuracy, AUROC()])\n# this line is necessary to save the model (usually read-only access)\nlearn.model_dir='\/kaggle\/working\/'","394921c5":"learn.lr_find(start_lr=1e-5, end_lr=5e-1)\nlearn.recorder.plot()","8f308c3f":"learn.fit_one_cycle(25, max_lr=slice(5e-4,5e-3))\nlearn.recorder.plot_losses()\nlearn.save('stage-1')","dfb0d4f3":"learn.load('stage-1');\ninterp = ClassificationInterpretation.from_learner(learn)","70f935bd":"interp.plot_confusion_matrix()\ninterp.plot_top_losses(16, figsize=(24,24), heatmap=False)\ninterp.plot_top_losses(16, figsize=(24,24), heatmap=True, alpha=0.3)","20c30e0e":"We'll be using a subset (5%) of the NIH CXR14 dataset for this project, but this principles here apply to any project. The NIH CXR14 dataset only has a few selected findings within its dataset, but let's pretend that we're creating a dataset from scratch (using Montage, e.g.)\n\nWe'll need to see how many examples there are of the finding we typed in. Let's run the cell below to see.","44a01147":"Now, we've figured out what we want our model to take a look at. Behind the scenes, we just need to sort the data into two folders: one with negative cases and one with positive cases. We can click the code below to get this done for the purpose of this presentaion. If we're creating our own model, we would use another workflow to get it done (via montage, e.g.). Let's click the run button below to get this done.","17902363":"Great -- we got it all moved. Now, we will start to use **fast.ai**. This next bit of code loads the images in fast.ai, resizes them to be 256 x 256 pixels, and shows them to us. If we have a more powerful computer we can make the images a bit larger. We'll use 80% of our data for training and 20% for validation.","7cfb8ed3":"# **\"A Deep Learning Radiology Cookbook\"**: using fast.ai on the NIH CXR14 dataset\n\nWelcome to this kernel! This kernel is adapated\/inspired from the very excellent \"Hello World Deep Learning\" [kernel](https:\/\/www.kaggle.com\/wfwiggins203\/hello-world-deep-learning-siim) by Walter Wiggins (2018-2019). The goal of this presentation is to show radiologists \\[**without** technical backgrounds\\] how it's simple to create AI models that can find patterns in medical images, and hopefully empower radiologists to make their own.\n\nWe will use **fast.ai** and the **NIH CXR14 [dataset](https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC)** to come up with our own world class machine learning models in 60 minutes or less.\n\n# Overview\n\n![image.png](https:\/\/i.imgur.com\/WebTYSX.jpg)\n\nFor this presentation, we will break down the process into four steps (image credit [ACR AI-LAB](https:\/\/ailab.acr.org\/)):\n\n1. **Defining a problem:** what do we want our cxr model to look for? For this we'll need to think of pathology you might find in a chest x-ray.\n2. **Data preparation:** for this we'll talk about how to manipulate our data in a way the model can understand.\n3. **Configuring a model:** we'll use a technology called fast.ai to create our model.\n4. **Train and test:** we'll talk about some important considerations when creating a model like this.\n\n# How to use this kernel\n\nTo explore this model and data set, please <kbd>Copy and Edit<\/kbd> the notebook in the menu above (i.e. create a copy under your Kaggle profile). \n\n>When you get into the draft environment, please ensure that you see **\"GPU on\"** and **\"Internet on\"** under **Settings** so you can utilize the cloud GPU for faster model training.\n\nIn this Notebook editing environment, each block of text is referred to as a **cell**.  Cells containing formatted text are **Markdown** cells, as they use the *Markdown* formatting language. Similarly, cells containing code are **code** cells.\n\nClicking within a cell will allow you to edit the content of that cell (a.k.a. enter **edit mode**). You can also navigate between cells with the arrow keys. Note that the appearance of Markdown cells will change when you enter edit mode.\n\nYou can **run code cells** (and format Markdown cells) as you go along by clicking within the cell and then clicking the **blue button with *one* arrow** next to the cell or at the bottom of the window. You can also use the keyboard shortcut <kbd>SHIFT<\/kbd> + <kbd>ENTER<\/kbd> (press both keys at the same time).\n\nLet's try this out by **running** the cell below. This will help us load the technologies we need to power our model.","616a016c":"In order to get the best results, we want to make sure our model isn't learning too quickly (jumping to conclusions) or learning too slowly (not making any conclusions at all). Fast.ai contains a method for us to assess that. When we begin to actually train our model, we want to plug in something called <code>max_lr<\/code>, which stands for maximum learning rate. We want to type in a range of values in which the loss is increasing the most quickly \\[*where the derivative is the highest*\\]. \n\nAs an example, in the graph below, we'd want our learning rate to be around that red dot. We'll type two numbers in exponential format below where it says <code>max_lr=slice(number,number)<\/code>. Continuing with the example, we'd want to type in two numbers slightly less than and greater than 1e-01, so we'd go with <code>max_lr=slice(5e-2,5e-1)<\/code>\n\n![image](https:\/\/i.imgur.com\/nDdgl6R.png)\n\nNow, we will train our model! Every time it looks through the data is called an epoch. Let's run 25 epochs -- this might take a few minutes.\n>My test set of 500 positive and 500 negative cases takes ~30s per epoch, for 5 minutes total. Yours may take more or less time depending on (1) how many images it's looking through and (2) the resolution of the images.","163aa589":"## 4. Evaluating training results and model performance\n\nOnce training is finished, **evaluating the training curves** can help us **tune our hyperparameters** in subsequent experiments by telling us if we're **underfitting** or by showing us when the model begins to **overfit**.\n\nFinally, we will demonstrate the model output on the test data set (in this case, one of each type of radiograph).\n\n>At the end of your experiments, you want to evaluate network performance on an **independent, *held-out* test data set**, as the model will be *indirectly* exposed to the validation data set over successive experiments, potentially **biasing the model hyperparameters** toward overfitting to the validation data.","e0833dc8":"## 1. Defining a Problem\n\nGreat! Let's start by thinking of something we can look for in a chest xray.\n\nOnce we think of something, let's type it in below, in between the quotes <kbd>\"<\/kbd><kbd>\"<\/kbd>, and run the cell. I'll put in \"atelectasis\" by default, but let's see if we can think of something else.","596c2cde":"If you liked this presentation, welcome to follow me on twitter at <a href=\"https:\/\/twitter.com\/AdlebergJason?ref_src=twsrc%5Etfw\" class=\"twitter-follow-button\" data-show-count=\"false\">@AdlebergJason<\/a><script async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"><\/script>!","a28b4d73":"How does that look? If there are a sufficient amount of training examples (at least 100), then we can move onto the next step! \n\nIf not, let's re-type something into the <code>finding = \"\"<\/code> field, run that cell again, and then run the above cell again.","30efc03b":"# 2. Preparing the data","33e3ea1a":"For fast.ai, we want to use this call to find the best learning rate for our model.\n\nThis will likely take **5-10 minutes**. Underneath this section we'll explain what it's doing.","6d0cab15":"## 3. Training the model\nNow, we will **train the model**. This is the cool part.\n\nWe will be using a technology called **resnet34** to look at our images. This is a model that Google created to do image analysis, and has released to the general public. It's pretty smart -- let's see how smart it is on our images.","96e67c10":"This method below will show up heatmaps to make sure that the model is looking at the right thing! It is also worth stating that this particular dataset is not without [criticism](https:\/\/lukeoakdenrayner.wordpress.com\/2017\/12\/18\/the-chestxray14-dataset-problems\/), but the principles here can be applied to anyone hoping to develop a machine learning model that examines medical images.\n\n>We can see that the model is often misclassifying images by looking at the wrong thing. In a subsequent kernel we can look into how we can fix this."}}