{"cell_type":{"6afbba6b":"code","6512e5f7":"code","6a1857be":"code","a1581638":"code","d6f42863":"code","6ca12744":"code","47f813ab":"code","f44d4952":"code","9495f001":"code","70b799b9":"code","5c82140d":"code","93f52e65":"code","73e70dcb":"code","6c6ca5fd":"code","8515d777":"code","0f57c588":"code","4fcee8bf":"code","0e7e6d38":"code","5c361bcb":"code","875f1b91":"code","d157f1a5":"code","258e886b":"code","697e7537":"code","7cd2963d":"code","1d1eb1a5":"code","39bd7a70":"code","3308021f":"code","3219b434":"code","227b41e0":"code","e90d49c7":"code","2561a6dd":"code","f7585031":"code","d0550680":"code","507541f4":"code","0b5c720d":"code","ece2d3f8":"markdown","5b73d1a5":"markdown","2c66ade9":"markdown","e09115b2":"markdown","8a7f5a54":"markdown","cc5971b8":"markdown","9a67d663":"markdown","06895529":"markdown","231ef8de":"markdown","992e8f08":"markdown","73a7d5ef":"markdown","d41c0bb4":"markdown","b2a20165":"markdown","d61152fb":"markdown","43509983":"markdown","4f6ecef3":"markdown","182643eb":"markdown","6e4390db":"markdown","123bc2e8":"markdown","26f59c54":"markdown","ec1f24cc":"markdown","bf8468f5":"markdown","2bbe68d5":"markdown","22fe3294":"markdown","8b6e994d":"markdown"},"source":{"6afbba6b":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","6512e5f7":"%config IPCompleter.greedy=True","6a1857be":"df_mitbih_train = pd.read_csv('..\/input\/heartbeat\/mitbih_train.csv', header=None)\ndf_mitbih_test = pd.read_csv('..\/input\/heartbeat\/mitbih_test.csv', header=None)\ndf_mitbih = pd.concat([df_mitbih_train, df_mitbih_test], axis=0)\n\ndf_ptbdb_normal = pd.read_csv('..\/input\/heartbeat\/ptbdb_normal.csv', header=None)\ndf_ptbdb_abnormal = pd.read_csv('..\/input\/heartbeat\/ptbdb_abnormal.csv', header=None)\ndf_ptbdb = pd.concat([df_ptbdb_normal, df_ptbdb_abnormal], axis=0)\n\nprint(df_mitbih.info())\nprint(df_ptbdb.info())","a1581638":"# ptbdb\nM_ptbdb = df_ptbdb.values\nX_ptbdb = M_ptbdb[:,:-1]\ny_ptbdb = M_ptbdb[:,-1]\n\n# mitbih\nM_mitbih = df_mitbih.values\nX_mitbih = M_mitbih[:,:-1]\ny_mitbih = M_mitbih[:,-1]","d6f42863":"classes={0:\"Normal\",\n         1:\"Artial Premature\",\n         2:\"Premature ventricular contraction\",\n         3:\"Fusion of ventricular and normal\",\n         4:\"Fusion of paced and normal\"}\nplt.figure(figsize=(15,4))\nfor i in range(0,5):\n    plt.subplot(2,3,i + 1)\n    all_samples_indexes = np.where(y_mitbih == i)[0]\n    rand_samples_indexes = np.random.randint(0, len(all_samples_indexes), 3)\n    rand_samples = X_mitbih[rand_samples_indexes]\n    plt.plot(rand_samples.transpose())\n    plt.title(\"Samples of class \" + classes[i], loc='left', fontdict={'fontsize':8}, x=0.01, y=0.85)\n","6ca12744":"classes={0:\"Normal\", 1:\"Abnormal (MI)\"}\nplt.figure(figsize=(10,2))\nfor i in range(0,2):\n    plt.subplot(1,2,i + 1)\n    all_samples_indexes = np.where(y_ptbdb == i)[0]\n    rand_samples_indexes = np.random.randint(0, len(all_samples_indexes), 3)\n    rand_samples = X_ptbdb[rand_samples_indexes]\n    plt.plot(rand_samples.transpose())\n    plt.title(\"Samples of class \" + classes[i], loc=\"left\", fontdict={'fontsize':8})","47f813ab":"repartition = df_mitbih[187].astype(int).value_counts()\nprint(repartition)","f44d4952":"plt.figure(figsize=(5,5))\ncircle=plt.Circle( (0,0), 0.8, color='white')\nplt.pie(repartition, labels=['n','q','v','s','f'], colors=['red','green','blue','skyblue','orange'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(circle)\nplt.show()","9495f001":"from tensorflow.keras.layers import Input, Conv1D, MaxPool1D, Activation, Add, Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import LearningRateScheduler","70b799b9":"input_shape = (187, 1)\n\ndef make_model(final_layer_size=5):\n    I = Input(input_shape)\n    C = Conv1D(filters=32, kernel_size=5)(I)\n\n    C11 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(C)\n    C12 = Conv1D(filters=32, kernel_size=5, padding='same')(C11)\n    A11 = Add()([C, C12])\n    R11 = Activation(activation='relu')(A11)\n    M11 = MaxPool1D(pool_size=5, strides=2)(R11)\n\n    C21 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(M11)\n    C22 = Conv1D(filters=32, kernel_size=5, padding='same')(C21)\n    A21 = Add()([M11, C22])\n    R21 = Activation(activation='relu')(A21)\n    M21 = MaxPool1D(pool_size=5, strides=2)(R21)\n\n    C31 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(M21)\n    C32 = Conv1D(filters=32, kernel_size=5, padding='same')(C31)\n    A31 = Add()([M21, C32])\n    R31 = Activation(activation='relu')(A31)\n    M31 = MaxPool1D(pool_size=5, strides=2)(R31)\n\n    C41 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(M31)\n    C42 = Conv1D(filters=32, kernel_size=5, padding='same')(C41)\n    A41 = Add()([M31, C42])\n    R41 = Activation(activation='relu')(A41)\n    M41 = MaxPool1D(pool_size=5, strides=2)(R41)\n\n    C51 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(M41)\n    C52 = Conv1D(filters=32, kernel_size=5, padding='same')(C51)\n    A51 = Add()([M41, C52])\n    R51 = Activation(activation='relu')(A51)\n    M51 = MaxPool1D(pool_size=5, strides=2)(R51)\n\n    F1 = Flatten()(M51)\n    D1 = Dense(32)(F1)\n    R1 = Activation(activation='relu')(D1)\n    D2 = Dense(32)(R1)\n    D3 = Dense(final_layer_size)(D2)\n\n    O = Activation(activation='softmax')(D3)\n\n    return Model(inputs=I, outputs=O)","5c82140d":"lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.001, decay_steps=10000, decay_rate=0.75)\nadam = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, amsgrad=False)","93f52e65":"n_classes = len(np.unique(y_ptbdb))\nmodel_ptbdb = make_model(n_classes)\nmodel_ptbdb.summary()","73e70dcb":"X_train_ptbdb, X_test_ptbdb, y_train_ptbdb, y_test_ptbdb = train_test_split(X_ptbdb, y_ptbdb, test_size=0.15)","6c6ca5fd":"model_ptbdb.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","8515d777":"history = model_ptbdb.fit(np.expand_dims(X_train_ptbdb, axis=2), \n                          y_train_ptbdb, \n                          validation_split=0.15,\n                          epochs=30,\n                          batch_size=256,\n                          verbose=0)","0f57c588":"def plot_learning(history):\n    plt.subplot(211)\n    plt.plot(history.history['accuracy'])\n    plt.legend([\"accuracy\"])\n    plt.subplot(212)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'], label = \"val_loss\")\n    plt.legend([\"loss\", \"val_loss\"])\n    plt.show()\n    ","4fcee8bf":"plot_learning(history)","0e7e6d38":"unique, counts = np.unique(y_test_ptbdb, return_counts=True)\nprint(f\"The testing set contains {counts[0]} normal recordings and {counts[1]} with myocardial infarction.\\nLet's compute the confusion matrix.\")","5c361bcb":"results = model_ptbdb.evaluate(np.expand_dims(X_test_ptbdb, axis=2), y_test_ptbdb, batch_size=128)\nprint(f\"The accuracy on the testing set is {np.round(results[1]*100,1)}%\")","875f1b91":"y_pred_ptbdb = model_ptbdb.predict(np.expand_dims(X_test_ptbdb, axis=2))\ny_pred_ptbdb_bool = np.argmax(y_pred_ptbdb, axis=1)\nprint(classification_report(y_test_ptbdb, y_pred_ptbdb_bool))\n\nconfusion_matrix = tf.math.confusion_matrix(y_test_ptbdb, y_pred_ptbdb_bool)\nprint(f\"Confusion matrix :\\n {confusion_matrix}\")","d157f1a5":"print(f\"{confusion_matrix[0][0]}\/{counts[0]} MI were correctly classified\")\n\nprint(f\"{confusion_matrix[1][1]}\/{counts[1]} normal beats were correctly classified\")\n\nprint(f\"{confusion_matrix[1][0]} beats were classified as MI\")\n\nprint(f\"{confusion_matrix[0][1]} MI were classified as normal\")\n","258e886b":"X_train_mitbih, X_test_mitbih, y_train_mitbih, y_test_mitbih = train_test_split(X_mitbih, y_mitbih, test_size=0.15)","697e7537":"n_classes_mitbih = len(np.unique(y_mitbih))\nmodel_mitbih = make_model(n_classes_mitbih)\nmodel_mitbih.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","7cd2963d":"history = model_mitbih.fit(np.expand_dims(X_train_mitbih, axis=2), \n                           y_train_mitbih, \n                           validation_split=0.15,\n                           epochs=30,\n                           batch_size=256,\n                           verbose=0)","1d1eb1a5":"plot_learning(history)","39bd7a70":"results = model_mitbih.evaluate(np.expand_dims(X_test_mitbih, axis=2), y_test_mitbih, batch_size=128)\nprint(f\"The accuracy on the testing set is {np.round(results[1]*100,1)}%\")","3308021f":"predictions = model_mitbih.predict(np.expand_dims(X_test_mitbih, axis=2))\nconfusion_matrix = tf.math.confusion_matrix(y_test_mitbih, np.argmax(predictions[:], axis=1))\nprint(confusion_matrix)","3219b434":"y_pred_mitbih = model_mitbih.predict(np.expand_dims(X_test_mitbih, axis=2))\ny_pred_mitbih_bool = np.argmax(y_pred_mitbih, axis=1)\nprint(classification_report(y_test_mitbih, y_pred_mitbih_bool))\n\nconfusion_matrix = tf.math.confusion_matrix(y_test_mitbih, y_pred_mitbih_bool)\nprint(f\"Confusion matrix :\\n {confusion_matrix}\")","227b41e0":"D1 = Dense(32)(model_mitbih.output)\nD2 = Dense(32)(D1)\nO = Dense(2, activation='softmax')(D2)\nmodel = Model(inputs=model_mitbih.input, outputs=O)\n\nfor layer in model.layers[:-3]:\n    layer.trainable = False\n\nfor layer in model.layers[-3:]:\n    layer.trainable = True\n    \nmodel.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","e90d49c7":"history = model.fit(np.expand_dims(X_train_ptbdb, axis=2), \n                    y_train_ptbdb, \n                    validation_split=0.15,\n                    epochs=5,\n                    batch_size=128,\n                    verbose=0)","2561a6dd":"plot_learning(history)","f7585031":"print(\"Trainability of the layers \\n\")\nfor layer in model.layers:\n    config = layer.get_config()\n    print(f\"{config['name']} : {config.get('trainable')}\")","d0550680":"D1 = Dense(32)(model_mitbih.layers[-3].output)\nD2 = Dense(32)(D1)\nO = Dense(2, activation='softmax')(D2)\nmodel = Model(inputs=model_mitbih.input, outputs=O)\nmodel.summary()","507541f4":"for layer in model.layers[:-3]:\n    layer.trainable = False\n\nfor layer in model.layers[-3:]:\n    layer.trainable = True\n    \nmodel.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(np.expand_dims(X_train_ptbdb, axis=2), \n                    y_train_ptbdb, \n                    validation_split=0.15,\n                    epochs=100,\n                    batch_size=128,\n                    verbose=0)\nplot_learning(history)","0b5c720d":"D1 = Dense(32)(model_mitbih.layers[-3].output)\nD2 = Dense(32)(D1)\nO = Dense(2, activation='softmax')(D2)\nmodel = Model(inputs=model_mitbih.input, outputs=O)\nmodel.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(np.expand_dims(X_train_ptbdb, axis=2), \n                    y_train_ptbdb, \n                    validation_split=0.15,\n                    epochs=100,\n                    batch_size=128,\n                    verbose=0)\nplot_learning(history)","ece2d3f8":"The MITBIH dataset is constituted of 109446 beats, labeled with 5 different classes : \n\n'N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4\n\n    N : Non-ecotic beats (normal beat) \n\n    S : Supraventricular ectopic beats \n\n    V : Ventricular ectopic beats\n\n    F : Fusion Beats \n\n    Q : Unknown Beats\n\nThe PTBHB dataset is constituted of 14552 beats, labeled with two different classes : \n\n    '0' for normal beat\n    '1' for abnormal beat (Myocardial infarction)\n\nAll the beats are recorded with 187 points. The shorter beats are padded with zeros to reach 187.","5b73d1a5":"The model converges at around 7-10 epochs on the validation set.","2c66ade9":"## Model","e09115b2":"The results for detecting MI by training directly with the ptbdb dataset are excellent : precision, recall, f1 and accuracy are all at around 0,99 on the testing set.","8a7f5a54":"The loss converges at around 15 epochs.","cc5971b8":"### Detecting MI with ptbdb dataset only","9a67d663":"### Fetching data","06895529":"The research article suggests a training on the mitbih dataset with 5 output classes to detect high level features.\n\nThen, this model is frozen and followed by two dense layers of size 32 trained on the ptbdb dataset. Let's try to implement this.","231ef8de":"### Transfer learning","992e8f08":"Now, let's add the two dense layers with transfer learning.\nThis implies a bottleneck since the last layers will follow these sizes :\n\n[...] --> 32 --> **5** --> 32 --> 32 --> 2","73a7d5ef":"The data repartition is very imbalanced : 83% of the data are normal beats.","d41c0bb4":"## Detecting Myocardial Infarction","b2a20165":"\"For training the networks, we used Adam optimization method with the learning rate, beta-1, and beta-2 of 0.001, 0.9, and 0.999, respectively. Learning rate is decayed exponentially with the decay factor of 0.75 every 10000 iterations.\"","d61152fb":"Let's try on the testing set.","43509983":"The model is converging at 76% accuracy, this is way below the 99% reached without transfer learning. I wonder how they managed to reach 95.6% accuracy with transfer learning in the article, and even though they did it would still be below the score reached without TL.","4f6ecef3":"We can try again by removing the bottleneck.","182643eb":"### Training on mitbih","6e4390db":"The method and model are based on this article : https:\/\/arxiv.org\/abs\/1805.00794","123bc2e8":"### Let's start with some imports","26f59c54":"Even without the bottleneck, the results are pretty bad. Let's try one last time without bottleneck and with the whole model trainable.","ec1f24cc":"## Detecting MI with the mitbih dataset and transfert learning","bf8468f5":"This dataset has 5 classes as outputs.","2bbe68d5":"The results seem to be drastically worse than without transfer learning. The validate accuracy is constant after one epoch only. I'm not sure how they reached 95,9% accuracy on the article. The model designed above seems correct, with only the last layers set as trainable, as we can see below.","22fe3294":"## Visualization of the dataset","8b6e994d":"The results for the classification with 5 classes are quite good although it struggles a bit for the classes 1 and 3 with a recall of 0.74-0.78 and an f1-score of 0.80-0.84."}}