{"cell_type":{"08c0a0e8":"code","5531c311":"code","3d14034d":"code","f29a0f91":"code","f9fc744d":"code","2e32903d":"code","1b63019a":"code","15de558b":"code","ac7472cd":"code","dbff61e2":"code","50659095":"code","3e6f07a6":"code","28ceffed":"code","d6a526f8":"code","becaa2b5":"code","559069d8":"code","294627c6":"markdown","e03e7024":"markdown","7d3c7833":"markdown","d4ea3202":"markdown","5edcc442":"markdown","6ca0f1de":"markdown","c8444b83":"markdown","305471c8":"markdown","f63a3965":"markdown","6562a4b4":"markdown","b1aa33f6":"markdown","c8a6b1ab":"markdown","6d991ec2":"markdown"},"source":{"08c0a0e8":"import numpy as np\nimport pandas as pd\nimport gc\nfrom matplotlib import pyplot as plt\nfrom scipy.signal import lombscargle\nimport math\nfrom tqdm import tqdm","5531c311":"# some help functions\n# angular frequency to period\ndef freq2Period(w):\n    return 2 * math.pi \/ w\n# period to angular frequency\ndef period2Freq(T):\n    return 2 * math.pi \/ T","3d14034d":"gc.enable()\ntrain = pd.read_csv('..\/input\/training_set.csv')\nprint(train['object_id'].unique())\ngc.collect()","f29a0f91":"# get data and normalize bands\ndef processData(train, object_id):\n    \n    #load data for given object\n    X = train.loc[train['object_id'] == object_id]\n    x = np.array(X['mjd'].values)\n    y = np.array(X['flux'].values)\n    passband = np.array(X['passband'].values)\n    \n    # normalize bands\n    for i in np.unique(passband):\n        yy = y[np.where(passband==i)]\n        mean = np.mean(yy)\n        std = np.std(yy)\n        y[np.where(passband==i)] = (yy - mean)\/std\n    \n    return x, y, passband","f9fc744d":"x, y, passband = processData(train, 615)\nplt.scatter(x, y, c=passband)\nplt.xlabel('time (MJD)')\nplt.ylabel('Normalized flux')\nplt.show()","2e32903d":"# calculate periodogram\ndef getPeriodogram(x, y, steps = 10000, minPeriod = None, maxPeriod = None):\n    if not minPeriod:\n        minPeriod = 0.1 # for now, let's ignore very short periodic objects\n    if not maxPeriod:\n        maxPeriod = (np.max(x) - np.min(x))\/2 # you cannot detect P > half of your observation period\n\n    maxFreq = np.log2(period2Freq(minPeriod))\n    minFreq = np.log2(period2Freq(maxPeriod))\n    f = np.power(2, np.linspace(minFreq,maxFreq, steps))\n    p = lombscargle(x,y,f,normalize=True)\n    return f, p","1b63019a":"%%time\nf,p = getPeriodogram(x, y, steps=20000)","15de558b":"plt.semilogx(freq2Period(f),p)\nplt.xlabel('Period (days)')\nplt.ylabel('Power')\nplt.show()","ac7472cd":"def findBestPeaks(x, y, F, P, threshold=0.3, n=5):\n    \n    # find peaks above threshold\n    indexes = np.where(P>threshold)[0]\n    # if nothing found, look at the highest peaks anyway\n    if len(indexes) == 0:\n        q = np.quantile(P, 0.9995)\n        indexes = np.where(P>q)[0]\n    \n    peaks = []\n    start = 0\n    end = 0\n    for i in indexes:\n        if i - end > 10:\n            peaks.append((start, end))\n            start = i\n            end = i\n        else:\n            end = i\n    \n    peaks.append((start, end))\n        \n    \n    # increase accuracy on the found peaks\n    results = []\n    for start, end in peaks:\n        if end > 0:\n            minPeriod = freq2Period(F[min(F.shape[0]-1, end+1)])\n            maxPeriod = freq2Period(F[max(start-1, 0)])\n            steps = int(100 * np.sqrt(end-start+1)) # the bigger the peak width, the more steps we want - but sensible (linear increase leads to long computation)\n            f, p = getPeriodogram(x, y, steps = steps, minPeriod=minPeriod, maxPeriod=maxPeriod)\n            results.append(np.array([freq2Period(f[np.argmax(p)]), np.max(p)]))\n\n    # sort by normalized periodogram score and return first n results\n    if results:\n        results = np.array(results)\n        results = results[np.flip(results[:,1].argsort())]\n    else:\n        results = np.array([freq2Period(F[np.argmax(P)]), np.max(P)]).reshape(1,2)\n    return results[0:n]","dbff61e2":"%%time\nresults = findBestPeaks(x, y, f, p)\nprint('Period(days) Power')\nprint(results)","50659095":"plt.figure(figsize=(20,25))\n\nfor i in range(results.shape[0]):\n    plt.subplot(results.shape[0],2,i+1)\n    phase = x\/results[i][0] % 1\n    plt.scatter(phase, y, c = passband, s=4)\n    plt.xlabel('Phase')\n    plt.ylabel('Normalized flux')\n    plt.title('Period: {:.4f}, power: {:.2f}'.format(results[i][0], results[i][1]))\n\nplt.show()","3e6f07a6":"from multiprocessing import Pool\nimport multiprocessing as mp\n\nCORES = mp.cpu_count() #4\n\ndef getFeatures(object_id):\n    \n    x, y, passband = processData(train, object_id)\n    f,p = getPeriodogram(x, y)\n    peaks = findBestPeaks(x, y, f, p)\n    features = np.zeros((5,2))\n    features[:peaks.shape[0],:peaks.shape[1]] = peaks\n    \n    return np.append(np.array([object_id]), features.reshape(5*2))","28ceffed":"object_ids = train['object_id'].unique()[0:100]","d6a526f8":"%%time\nfeatures = []\nfor object_id in object_ids:\n    results = getFeatures(object_id)\n    features.append(results)","becaa2b5":"%%time\np = Pool(CORES)\n\nresults = p.map(getFeatures, object_ids)","559069d8":"object_ids = train['object_id'].unique()\ncolumns = np.array(['id'])\nfor i in range(5):\n    period_str = 'period_'+str(i+1)\n    power_str = 'power_'+str(i+1)\n    columns = np.append(columns, np.array([period_str, power_str]))\n\nresults = p.map(getFeatures, object_ids)\n\noutput = pd.DataFrame(results, columns=columns)\noutput['id'] = output['id'].astype(np.int32)\noutput.to_csv('.\/train-periods.csv')","294627c6":"Let's get light curve for first object in dataset and plot it.","e03e7024":"# Further development \/ ideas:\n\n* *the speed is a key.  The process to compute periodograms and features for 3M+ dataset cannot take ages. I.e. we need paralelization and smart optimization of the number steps in periodogram search*\n* sort the observations by phase and band and feed the phase curve into RNN\n* feed the phase curve into CNN with channels = number of bands\n* look for the functions, that fit the curves well - some classes of variable objects can be fitted very precisely by a specific function, which can then help to identify 99 class (increase your chance to have it right).","7d3c7833":"You can see how a typical periodogram of noisy data looks like. There is huge noise and several peak of different size. We use only 10000 steps for first pass, which means we may not always hit the peak exactly. Let's take all peak candidates of certain height (let's say power > 0.3) and examine them further.","d4ea3202":"# Multiprocessing\n\nIn this section, I will try to develop speed optimized technique to precompute basic periodogram features.","5edcc442":"Light curve can tell you a lot about the type of variable object, especially when the object is periodic. This kernel is about to examine, how to extract additional features from the lightcurves using periodograms and phase curves.\n\nWe will use scipy.signal.lobscargle which is something like fourier analysis for unevenly distributed data. Also known as [Least-squares spectral analysis](https:\/\/en.wikipedia.org\/wiki\/Least-squares_spectral_analysis) (LSSA).","6ca0f1de":"We can see that only first period makes sense and the rest are false positives. This one looks like some short-period variable star with nicely periodic changes.\n\nYou can use the found periods and their respective powers as a new feature.\n\nWith phase curve, you can also try to extract other features, e.g.:\n* shape\n* symmetry \/ assymetry of the curve\n* humps, double minimas \/ maximas\n* fit the phase curve with sin function and calculate residuals for each band - in combination with flux errors, it's a measure of how strong the periodicity is (some objects are nicely periodic, like this one, some are semi-periodic with each minimum\/maximum slightly different, which makes the phase curve more noisy)","c8444b83":"Big mess, right? Let's make some sense in it by periodograms.\nLoosely speaking, periodogram shows you something like the probability for each of possible periods (well, not exactly probability, that's why I call it power, but it's enough for basic understanding).\n\nThe computation takes ages and time is our most valuable resource (we have more than 3M objects in test set). Therefore I look for 5 most \"probable\" periods above some \"probability\" threshold and use them as new features.","305471c8":"My basic idea is simple: take lightcurves in different bands and normalize them, so that they fit together. Most periodic changes happen in all bands. The difference is, bands are usually shifted from each other (they have different **mean**), and the amplitude of changes can be also different (they have different **standard deviation**). Normalizing bands will make most of the well-behaved variable object fit together.\n\n*Note: this is just some rough approximation, which may not work for exotic (mainly extra-galactic) or extreme objects (like active black holes or whatever) - in general any object, whose light curves are not similar enough in different bands. But the information about such a mismatch is valuable for classifications by itself. For such objects, you could do periodogram for each band and then merge the results. But not know, maybe in future versions...*","f63a3965":"We found 4 peaks - one with a very high power, three others with much lower one. Let's check the results visually:","6562a4b4":"First, let's try to calculate without multi-cpu speed-up.","b1aa33f6":"That's 0.100 seconds per star. Training set will take around 780 seconds. Testing set 300 000 seconds (~3,5 days). With 4x more CPU power, we got roughly 2,8x speedup.\nIf this relation holds linearly with CPU power, we could get testing set computed in 10 hours on 32 CPU on google cloud.\nI will definitelly try and will make it public, if the new features prove to be benefitial on train\/validation set.","c8a6b1ab":"That's 0.284 second per star. Training set will then take 2200 seconds to calculate. Testing set would take 852000 seconds, ~10 days. Not good.\nLet's try 4 cores available in Kaggle Kernels.","6d991ec2":"# Calculating training set"}}