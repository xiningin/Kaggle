{"cell_type":{"8fdfc342":"code","a074d99e":"code","dd776137":"code","ba71bcf6":"code","15381de0":"code","a5111809":"code","4f5281b4":"code","9b34da02":"code","a1f8f0b0":"code","e834146b":"code","8f31b5c1":"code","675691a0":"code","444c19e2":"code","e6bdd84e":"code","ce306124":"code","f12f07f9":"code","ce7b4b62":"code","3de87763":"code","f8bc61dd":"code","dc65ecc4":"code","53a1f327":"code","acb36d20":"code","6c1c6fd4":"code","a59f61b4":"code","671a0259":"code","c1808f7a":"code","9a3ed048":"code","cd4280e1":"code","c3b20c07":"code","0e3fca7b":"code","78e68f90":"code","ca9829be":"code","2ea356a6":"code","86787b5b":"code","3a523871":"code","e9bd52d1":"code","3c60d857":"code","e595e07f":"code","4806c9d3":"code","87a68175":"code","6f3308b2":"code","c46167d1":"code","2f844c24":"code","03a73f8e":"code","19b77071":"code","bda9cfa9":"code","92e8b57f":"code","9eb68f08":"code","58d04bf6":"code","4dc42777":"code","7c783581":"code","e0606a24":"code","0e90403a":"code","85ffefdb":"code","478ed7b2":"code","9673e29f":"code","f53c8ff0":"code","7d0eb64e":"code","aa5d2eae":"code","c60b9b2d":"code","636c7a70":"code","995bf1fa":"code","85d5b613":"code","2ae53edd":"code","90ca3346":"code","7960218c":"code","a08b5658":"code","d42ec394":"code","fe2f633e":"code","8b181b06":"code","713d80c0":"code","6f5fc12c":"code","ffd0ef3c":"code","40a0fe23":"code","4161cb3a":"code","785b20f5":"code","d629a748":"code","20212c59":"code","bb72688b":"code","3e951edc":"code","5ec4935f":"code","5fd58260":"code","80de97e4":"code","38808787":"code","71f138db":"code","3007c585":"code","5784dd7e":"code","46a102f3":"code","a7fa64fe":"code","591ead62":"code","5f4188e1":"code","db780653":"code","9780eb79":"code","661b0e9c":"code","5c2f83f0":"code","9b8842d9":"code","c84b5885":"code","1daac070":"code","67e6a7b3":"code","e20f4d04":"code","e33cf9a2":"code","e420812f":"code","1c268b61":"code","19745542":"code","4725307d":"code","584347d7":"code","c150cf4a":"code","2f458fb5":"code","20bae398":"code","50022832":"code","dc7a42e8":"code","0e197e44":"code","1b2afac0":"code","6766916c":"code","2b9480b9":"code","895a9f30":"code","b5a2b3cf":"code","7209847c":"code","485884c4":"code","4ee19f1d":"code","f1eaacb4":"code","7a1e7bfc":"code","71b59cae":"code","eac331e0":"code","222ebdf2":"code","8b49d5e3":"code","7194cb18":"code","70928d4c":"code","52ff2c94":"code","0a0556b6":"code","139852c9":"code","d25aba81":"code","e6bbd63a":"code","05611cd1":"code","99619bed":"code","705ba521":"code","02cc572e":"code","69081210":"code","d127c5be":"code","54ef46a2":"code","a97d9beb":"code","06386468":"code","ab6c6263":"code","94d35ca2":"markdown","c59c24e2":"markdown","87983597":"markdown","d7ac77d5":"markdown","d5da57d7":"markdown","e193af5e":"markdown","4abeeb82":"markdown","abce6406":"markdown","673a42cb":"markdown","05dea815":"markdown","15e96210":"markdown","426aafc4":"markdown","c60b3019":"markdown","ba561d5c":"markdown","31b7253a":"markdown","a2ddbf6e":"markdown","3db81fd3":"markdown","1a33c4e4":"markdown","84237301":"markdown","c264152c":"markdown","6446d581":"markdown","11f2c4ef":"markdown","b6cb2b48":"markdown","2b9fe77e":"markdown","9dbf63d3":"markdown","0b57ff8b":"markdown","d2686b62":"markdown","1b738d91":"markdown","21bc4414":"markdown","d8240cc1":"markdown","8c83dd4a":"markdown","f132e278":"markdown","b375e2f7":"markdown","fb0dbec7":"markdown","82d17361":"markdown","58bdca32":"markdown","973c5c5e":"markdown","8ad44d2f":"markdown","61d6e1e9":"markdown","f81855f8":"markdown","8c88fe5e":"markdown","b686741e":"markdown","444c4a1f":"markdown","386082d9":"markdown","f75a12ee":"markdown","57ceb101":"markdown","5d914e15":"markdown","6acbeb21":"markdown","16ab010d":"markdown","326bb4e5":"markdown","30955d5d":"markdown","fd644330":"markdown","05f600cb":"markdown","6a34fff1":"markdown","fba2fb1c":"markdown","fc610fdc":"markdown","29976cb2":"markdown","02f498de":"markdown","4155e1a1":"markdown","62cdc4b3":"markdown","6a4e85d9":"markdown","fc05ddf5":"markdown","12ed763c":"markdown","b5faaf85":"markdown","252476e8":"markdown","39c85d3e":"markdown","595a7ca0":"markdown","29ec8f06":"markdown","810b2aa2":"markdown","a7e55461":"markdown","ca01ff5e":"markdown","f7102055":"markdown","597ecdee":"markdown","0089a92a":"markdown","9bb7ed37":"markdown","4b92f78a":"markdown","17b28243":"markdown","57adadb4":"markdown","25cb1c30":"markdown","48dbb3f2":"markdown","49976134":"markdown","573a93d8":"markdown","019306ee":"markdown","ecb4c288":"markdown"},"source":{"8fdfc342":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n# for plotting graphs\nimport matplotlib.pyplot as plt\n# to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","a074d99e":"train=pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ntest=pd.read_csv('..\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv')","dd776137":"# copies of the data \ntrain_original = train.copy()\ntest_original = test.copy()","ba71bcf6":"train.shape","15381de0":"train.head()\n# to have a look on the dataset","a5111809":"train.info()","4f5281b4":"train.describe()","9b34da02":"print(train.dtypes)","a1f8f0b0":"#we will change the type of Credit_History to object becaues we can see that it is 1 or 0\n\ntrain['Credit_History'] = train['Credit_History'].astype('O')","e834146b":"train.describe(include='O')","8f31b5c1":"# we will drop ID because it's not important for our model and it will just mislead the model\n\ntrain.drop('Loan_ID', axis=1, inplace=True)","675691a0":"train.duplicated().any()\n","444c19e2":"\nplt.figure(figsize=(3,5))\nsns.countplot(train['Loan_Status']);\n\nprint('The percentage of Y class : %.2f' % (train['Loan_Status'].value_counts()[0] \/ len(train)))\nprint('The percentage of N class : %.2f' % (train['Loan_Status'].value_counts()[1] \/ len(train)))","e6bdd84e":"plt.figure(figsize=(3,5))\nsns.barplot(train['Gender'].value_counts().index, train['Gender'].value_counts().values\/614, alpha=0.9 )\nplt.title('Gender')","ce306124":"plt.figure(figsize=(3,5))\nsns.barplot(train['Married'].value_counts().index, train['Married'].value_counts().values\/614, alpha=0.9 )\nplt.title('Married')","f12f07f9":"plt.figure(figsize=(3,5))\nsns.barplot(train['Self_Employed'].value_counts().index, train['Self_Employed'].value_counts().values\/614, alpha=0.9 )\nplt.title('Self_Employed')","ce7b4b62":"plt.figure(figsize=(3,5))\nsns.barplot(train['Credit_History'].value_counts().index, train['Credit_History'].value_counts().values\/614 , alpha=0.9)\nplt.title('Credit_History')","3de87763":"plt.figure(figsize=(3,5))\nsns.barplot(train['Dependents'].value_counts().index, train['Dependents'].value_counts().values\/614, alpha=0.9 )\nplt.title('Dependents')","f8bc61dd":"plt.figure(figsize=(3,5))\nsns.barplot(train['Education'].value_counts().index, train['Education'].value_counts().values\/614, alpha=0.9 )\nplt.title('Education')","dc65ecc4":"plt.figure(figsize=(4,6))\nsns.barplot(train['Property_Area'].value_counts().index, train['Property_Area'].value_counts().values\/614, alpha=0.9 )\nplt.title('Property Area')","53a1f327":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train['ApplicantIncome'], color='m')\n\nplt.subplot(122)\ntrain['ApplicantIncome'].plot(kind='box',figsize=(14,5))\n","acb36d20":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train['CoapplicantIncome'], color='m')\n\nplt.subplot(122)\ntrain['CoapplicantIncome'].plot(kind='box',figsize=(14,5))\n","6c1c6fd4":"train.boxplot(column='ApplicantIncome',by='Education', figsize=(5,5))\n","a59f61b4":"plt.figure(1)\nplt.subplot(121)\ndf_train = train.dropna() # to remove 'cannot convert float NaN to integer' error\nsns.distplot(df_train['LoanAmount'], color='m')\n\nplt.subplot(122)\ndf_train['LoanAmount'].plot(kind='box',figsize=(14,5))\n","671a0259":"'''plt.figure(1)\nplt.subplot(121)\nsns.distplot(train['Loan_Amount_Term'],color='m')'''\n\n\n","c1808f7a":"sns.set_style=(\"whitegrid\")\nsns.FacetGrid(train,hue=\"Loan_Status\",size=4).map(plt.scatter,\"Credit_History\",\"ApplicantIncome\").add_legend();\nplt.show()","9a3ed048":"# Married\nplt.figure(figsize=(5,5))\nsns.countplot(x='Married', hue='Loan_Status', data=train)\n\n\n","cd4280e1":"#gender\nplt.figure(figsize=(5,5))\nsns.countplot(x='Gender', hue='Loan_Status', data=train)","c3b20c07":"#Education\nplt.figure(figsize=(5,5))\nsns.countplot(x='Education', hue='Loan_Status', data=train)","0e3fca7b":"#dependents\nplt.figure(figsize=(5,5))\nsns.countplot(x='Dependents', hue='Loan_Status', data=train)","78e68f90":"#property area\nplt.figure(figsize=(5,5))\nsns.countplot(x='Property_Area', hue='Loan_Status', data=train)","ca9829be":"#credit history\nplt.figure(figsize=(5,5))\nsns.countplot(x='Credit_History', hue='Loan_Status', data=train)","2ea356a6":"#self employed\nplt.figure(figsize=(5,5))\nsns.countplot(x='Self_Employed', hue='Loan_Status', data=train)","86787b5b":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='ApplicantIncome', data=train , palette='Set3')\n","3a523871":"max(train['ApplicantIncome']), min(train['ApplicantIncome'])","e9bd52d1":"bins=[0,2500,4000,6000,81000] \ngroup=['Low','Average','High', 'Very high'] \ntrain['Income_bin']=pd.cut(train['ApplicantIncome'],bins,labels=group)\n\nIncome_bin=pd.crosstab(train['Income_bin'],train['Loan_Status']) \nIncome_bin.div(Income_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,figsize=(5,5)) \nplt.xlabel('ApplicantIncome') \nP = plt.ylabel('Percentage')","3c60d857":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='CoapplicantIncome', data=train , palette='Set3')","e595e07f":"max(train['CoapplicantIncome']), min(train['CoapplicantIncome'])","4806c9d3":"bins=[0,1000,3000,42000]\ngroup=['Low','Average','High']\ntrain['Coapplicant_Income_bin']=pd.cut(train['CoapplicantIncome'],bins,labels=group)\n\nCoapplicant_Income_bin=pd.crosstab(train['Coapplicant_Income_bin'],train['Loan_Status'])\nCoapplicant_Income_bin.div(Coapplicant_Income_bin.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(5,5))\nplt.xlabel('CoapplicantIncome') \nP=plt.ylabel('Percentage')","87a68175":"train['Total_Income']=train['ApplicantIncome'] + train['CoapplicantIncome']","6f3308b2":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='Total_Income', data=train , palette='Set3')","c46167d1":"max(train['Total_Income'])","2f844c24":"bins=[0,2500,4000,6000,81000]\ngroup=['Low','Average','High','Very High']\ntrain['Total_Income_bin']=pd.cut(train['Total_Income'],bins,labels=group)\n\nTotal_Income_bin=pd.crosstab(train['Total_Income_bin'],train['Loan_Status'])\nTotal_Income_bin.div(Total_Income_bin.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(5,5))\nplt.xlabel('Total Income') \nP=plt.ylabel('Percentage')","03a73f8e":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='LoanAmount', data=train , palette='Set3')","19b77071":"bin2 = [0,100,300,700]\ngroup2 = ['Low','Average','High']\n\ntrain['LoanAmount_bin'] = pd.cut(train['LoanAmount'],bin2,labels= group2)\nLoanAmount_bin = pd.crosstab(train['LoanAmount_bin'],train['Loan_Status'])\nLoanAmount_bin = LoanAmount_bin.div(LoanAmount_bin.sum(1).astype(float),axis = 0).plot(kind='bar',stacked=True,figsize=(5,5))\nplt.xlabel('LoanAmount')\nP = plt.ylabel(\"Percentages\")","bda9cfa9":"train=train.drop(['Income_bin','Coapplicant_Income_bin','LoanAmount_bin','Total_Income_bin','Total_Income'], axis=1)","92e8b57f":"train.shape\ntrain.head()","9eb68f08":"train.isnull().any()","58d04bf6":"train.isnull().sum()","4dc42777":"colours = ['#000099', '#ffff00'] # specify the colours - yellow is missing. blue is not missing.\nsns.heatmap(train.isnull(), cmap=sns.color_palette(colours))","7c783581":"train['Gender'].fillna(train['Gender'].mode()[0],inplace = True)\ntrain['Dependents'].fillna(train['Dependents'].mode()[0],inplace = True)\ntrain['Married'].fillna(train['Married'].mode()[0],inplace = True)\ntrain['Self_Employed'].fillna(train['Self_Employed'].mode()[0],inplace = True)\ntrain['Credit_History'].fillna(train['Credit_History'].mode()[0],inplace = True)","e0606a24":"train['LoanAmount'].fillna(train['LoanAmount'].median(),inplace = True)","0e90403a":"train['Loan_Amount_Term'].median()","85ffefdb":"train['Loan_Amount_Term'].mode()[0]","478ed7b2":"train['Loan_Amount_Term'].fillna(360, inplace =True)","9673e29f":"train.isnull().any()","f53c8ff0":"test.isnull().any()","7d0eb64e":"test.isnull().sum()","aa5d2eae":"colours = ['#000099', '#ffff00'] # specify the colours - yellow is missing. blue is not missing.\nsns.heatmap(test.isnull(), cmap=sns.color_palette(colours))","c60b9b2d":"test['Gender'].fillna(test['Gender'].mode()[0],inplace = True)\ntest['Dependents'].fillna(test['Dependents'].mode()[0],inplace = True)\ntest['Married'].fillna(test['Married'].mode()[0],inplace = True)\ntest['Self_Employed'].fillna(test['Self_Employed'].mode()[0],inplace = True)\ntest['Credit_History'].fillna(test['Credit_History'].mode()[0],inplace = True)","636c7a70":"test['LoanAmount'].fillna(test['LoanAmount'].median(),inplace = True)","995bf1fa":"test['Loan_Amount_Term'].median()","85d5b613":"test['Loan_Amount_Term'].mode()[0]","2ae53edd":"test['Loan_Amount_Term'].fillna(test['Loan_Amount_Term'].mode()[0],inplace=True)","90ca3346":"test.isnull().any()","7960218c":"plt.figure(1)\nplt.subplot(121)\nsns.boxplot(train['LoanAmount'], color='m')\n\nplt.subplot(122)\ntrain['LoanAmount'].hist(bins=100,figsize=(14,5), color='m')\n\n","a08b5658":"train['LoanAmount_log'] = np.log(train['LoanAmount'])","d42ec394":"plt.figure(1)\nplt.subplot(121)\nsns.boxplot(train['LoanAmount_log'])\n\nplt.subplot(122)\ntrain['LoanAmount_log'].hist(bins=20,figsize=(14,5))","fe2f633e":"#doing the same for loan amount in test set\ntest['LoanAmount_log'] = np.log(test['LoanAmount'])","8b181b06":"plt.figure(1)\nplt.subplot(121)\ntest['LoanAmount'].hist(bins=100,figsize=(14,5),color='m')\n\nplt.subplot(122)\ntest['LoanAmount_log'].hist(bins=20,figsize=(14,5))","713d80c0":"plt.figure(1)\nplt.subplot(121)\nsns.boxplot(train['ApplicantIncome'], color='m')\n\nplt.subplot(122)\ntrain['ApplicantIncome'].hist(bins=100,figsize=(14,5), color='m')\n","6f5fc12c":"train['ApplicantIncome_log'] = np.log(train['ApplicantIncome'])","ffd0ef3c":"plt.figure(1)\nplt.subplot(121)\nsns.boxplot(train['ApplicantIncome_log'])\n\nplt.subplot(122)\ntrain['ApplicantIncome_log'].hist(bins=20,figsize=(14,5))\n","40a0fe23":"plt.figure(1)\nplt.subplot(121)\ntest['ApplicantIncome'].hist(bins=100,figsize=(14,5),color='m')\n","4161cb3a":"test['ApplicantIncome_log'] = np.log(test['ApplicantIncome'])","785b20f5":"plt.figure(1)\nplt.subplot(121)\nsns.boxplot(train['CoapplicantIncome'], color='m')\n\nplt.subplot(122)\ntrain['CoapplicantIncome'].hist(bins=100,figsize=(14,5), color='m')\n","d629a748":"#Making a new variable for total income\ntrain[\"TotalIncome\"]=train[\"ApplicantIncome\"]+train[\"CoapplicantIncome\"]\ntest[\"TotalIncome\"]=test[\"ApplicantIncome\"]+test[\"CoapplicantIncome\"]","20212c59":"train.head()","bb72688b":"x_train_original = train.drop('Loan_Status',1)\ny_train_original = train['Loan_Status']","3e951edc":"x_train_original.head()","5ec4935f":"y_train_original.head()","5fd58260":"x_train_original = pd.get_dummies(x_train_original)","80de97e4":"y_train_original.replace('N', 0,inplace=True)\ny_train_original.replace('Y', 1,inplace=True)","38808787":"x_train_original","71f138db":"y_train_original","3007c585":"data_corr = pd.concat([x_train_original, y_train_original], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(13,9))\nsns.heatmap(corr, annot=True)","5784dd7e":"data_corr.corr()","46a102f3":"from sklearn.model_selection import train_test_split\nx_train, x_cv, y_train, y_cv = train_test_split(x_train_original,y_train_original,test_size = 0.3)","a7fa64fe":"x_train.shape","591ead62":"x_cv.shape","5f4188e1":"y_train.shape","db780653":"y_cv.shape","9780eb79":"from sklearn.linear_model import LogisticRegression\nclassifier1=LogisticRegression(random_state=0)\nclassifier1.fit(x_train,y_train)","661b0e9c":"#Let us predict the results\ny_pred=classifier1.predict(x_cv)","5c2f83f0":"#et us understand it using the confusion matrix which is a tabular representation of Actual vs Predicted values.\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_cv,y_pred)\nprint(cm)\n","9b8842d9":"#check accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_cv,y_pred)*100","c84b5885":"from sklearn.model_selection import cross_val_score\naccuracies=cross_val_score(estimator=classifier1,X=x_train,y=y_train,cv=10)\nprint(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))","1daac070":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\n\nfpr, tpr, threshold = metrics.roc_curve(y_cv, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","67e6a7b3":"from sklearn.svm import SVC\nclassifier2=SVC(kernel='rbf')\nclassifier2.fit(x_train,y_train)","e20f4d04":"y_pred_svm=classifier2.predict(x_cv)","e33cf9a2":"cm=confusion_matrix(y_cv,y_pred_svm)\nprint(cm)\naccuracy_score(y_cv,y_pred_svm)*100","e420812f":"accuracies=cross_val_score(estimator=classifier2,X=x_train,y=y_train,cv=10)\nprint(\"Accuracy of svm: {:.2f}%\".format(accuracies.mean()*100))","1c268b61":"from sklearn.neighbors import KNeighborsClassifier\nclassifier3 = KNeighborsClassifier(n_neighbors = 19, metric = 'minkowski', p = 2)\nclassifier3.fit(x_train, y_train)\ny_pred_knn = classifier3.predict(x_cv)\n# Creating confusion matrix and calculating the accuracy score\ncm_knn = confusion_matrix(y_cv, y_pred_knn)\nprint(cm_knn)\naccuracy_score(y_cv, y_pred_knn)","19745542":"from sklearn.ensemble import RandomForestClassifier\nclassifier4=RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=0)\nclassifier4.fit(x_train,y_train)\ny_pred_RandomF = classifier4.predict(x_cv)\naccuracy_score(y_cv,y_pred_RandomF)","4725307d":"sns.distplot(x_train_original['TotalIncome'])","584347d7":"#converting to log as it is not normally distributed\nx_train_original['TotalIncomeLog'] = np.log(x_train_original['TotalIncome'])\nsns.distplot(x_train_original['TotalIncomeLog'])","c150cf4a":"test['TotalIncome'] = test['ApplicantIncome'] + test['CoapplicantIncome']\ntest['TotalIncomeLog'] = np.log(test['TotalIncome'])","2f458fb5":"x_train_original['EMI'] = x_train_original['LoanAmount'] \/ x_train_original['Loan_Amount_Term']\ntest['EMI'] = test['LoanAmount'] \/ test['Loan_Amount_Term']","20bae398":"sns.distplot(x_train_original['EMI'])","50022832":"x_train_original['EMI_log'] = np.log(x_train_original['EMI'])\nsns.distplot(x_train_original['EMI_log'])","dc7a42e8":"test['EMI_log'] = np.log(test['EMI'])","0e197e44":"x_train_original['Balance_Income'] = x_train_original['TotalIncome'] - (x_train_original['EMI']*1000) # *1000 is done to make the units equal for subtraction\ntest['Balance_Income'] = test['TotalIncome'] - (test['EMI']*1000)","1b2afac0":"sns.distplot(x_train_original['Balance_Income'])","6766916c":"x_train_original['Balance_Income_log'] = np.log(x_train_original['Balance_Income'])\ntest['Balance_Income_log'] = np.log(test['Balance_Income'])","2b9480b9":"x_train_original = x_train_original.drop(['ApplicantIncome', 'CoapplicantIncome', \n                        'LoanAmount', 'Loan_Amount_Term' , 'EMI','TotalIncome','Balance_Income'],axis = 1)\ntest = test.drop(['ApplicantIncome', 'CoapplicantIncome', \n                        'LoanAmount', 'Loan_Amount_Term','EMI','TotalIncome','Balance_Income'],axis = 1)","895a9f30":"x_train_original.head(3)","b5a2b3cf":"from sklearn.model_selection import train_test_split\nx_train1, x_cv1, y_train1, y_cv1 = train_test_split(x_train_original,y_train_original,test_size = 0.3)","7209847c":"from sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier","485884c4":"x_train1.isnull().sum()","4ee19f1d":"x_train1['Balance_Income_log'].fillna(x_train1['Balance_Income_log'].median(),inplace = True)\n","f1eaacb4":"x_cv1.isnull().sum()","7a1e7bfc":"x_cv1['Balance_Income_log'].fillna(test['Balance_Income_log'].median(),inplace = True)","71b59cae":"\"\"\" LOGISTIC REGRESSION \"\"\"\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(class_weight='balanced', random_state = 13)\nlogreg.fit(x_train1, y_train1)\ny_pred_logreg = logreg.predict(x_cv1)\n# Creating confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm_logreg = confusion_matrix(y_cv1, y_pred_logreg)\nas_logreg=accuracy_score(y_cv1, y_pred_logreg)\nprint(as_logreg)","eac331e0":"x_cv1.isnull().sum()","222ebdf2":"from numpy import sqrt\nfrom numpy import argmax\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = logreg.predict_proba(x_cv1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_cv1, preds)\nroc_auc_logreg = metrics.auc(fpr, tpr)\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (threshold[ix], gmeans[ix]))\n\nplt.figure()\nplt.title('Receiver Operating Characterstics')\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc_logreg))\nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n \n# create the axis of thresholds (scores)\nax2 = plt.gca().twinx()\nax2.plot(fpr, threshold, markeredgecolor='r',linestyle='dashed', color='r')\nax2.scatter(fpr[ix], threshold[ix], marker='o', color='r', label='Best1')\nax2.set_ylabel('Threshold',color='r')\nax2.set_ylim([threshold[-1],threshold[0]])\nax2.set_xlim([fpr[0],fpr[-1]])\n \nplt.show()","8b49d5e3":"\"\"\" K-NEAREST NEIGHBORS \"\"\"\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors = 19, metric = 'minkowski', p = 2)\nKNN.fit(x_train1, y_train1)\ny_pred = KNN.predict(x_cv1)\n# Creating confusion matrix and calculating the accuracy score\ncm_knn = confusion_matrix(y_cv1, y_pred)\nas_knn=accuracy_score(y_cv1, y_pred)\n","7194cb18":"print(as_knn)","70928d4c":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = KNN.predict_proba(x_cv1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_cv1, preds)\nroc_auc_knn = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_knn)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","52ff2c94":"\"\"\"Naive Bayes Classifier\"\"\"\nfrom sklearn.naive_bayes import GaussianNB\nNaiveB = GaussianNB()\nNaiveB.fit(x_train1, y_train1)\ny_pred1 = NaiveB.predict(x_cv1)\n# Creating confusion matrix and calculating the accuracy score\ncm_nb = confusion_matrix(y_cv1, y_pred1)\nas_nb = accuracy_score(y_cv1, y_pred1)\nprint(cm_nb)\nprint(as_nb)","0a0556b6":"from numpy import sqrt\nfrom numpy import argmax\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = NaiveB.predict_proba(x_cv1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_cv1, preds)\nroc_auc_nb = metrics.auc(fpr, tpr)\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (threshold[ix], gmeans[ix]))\n\nplt.figure()\nplt.title('Receiver Operating Characterstics')\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc_nb))\nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n \n# create the axis of thresholds (scores)\nax2 = plt.gca().twinx()\nax2.plot(fpr, threshold, markeredgecolor='r',linestyle='dashed', color='r')\nax2.scatter(fpr[ix], threshold[ix], marker='o', color='r', label='Best1')\nax2.set_ylabel('Threshold',color='r')\nax2.set_ylim([threshold[-1],threshold[0]])\nax2.set_xlim([fpr[0],fpr[-1]])\n \nplt.show()","139852c9":"\"\"\" SVM GAUSSIAN \"\"\"\nfrom sklearn.svm import SVC\nSVCG = SVC(kernel = 'rbf', class_weight='balanced',random_state = 42, probability = True)\nSVCG.fit(x_train1, y_train1)\ny_pred = SVCG.predict(x_cv1)\n# Creating confusion matrix and calculating the accuracy score\ncm_svm_gaussian = confusion_matrix(y_cv1, y_pred)\nas_svm_gaussian = accuracy_score(y_cv1, y_pred)\nprint(as_svm_gaussian)","d25aba81":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = SVCG.predict_proba(x_cv1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_cv1, preds)\nroc_auc_svg = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_svg)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","e6bbd63a":"\"\"\" DECISION TREE CLASSIFICATION \"\"\"\nfrom sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)\nDTC.fit(x_train1, y_train1)\ny_pred = DTC.predict(x_cv1)\n# Creating confusion matrix and calculating the accuracy score\ncm_dtc = confusion_matrix(y_cv1, y_pred)\nas_dtc = accuracy_score(y_cv1, y_pred)\nprint(as_dtc)","05611cd1":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = DTC.predict_proba(x_cv1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_cv1, preds)\nroc_auc_dt = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_dt)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","99619bed":"\"\"\" RANDOM FOREST CLASSIFIER \"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\nRanForest = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 1, max_depth = 5)\nRanForest.fit(x_train1, y_train1)\ny_pred = RanForest.predict(x_cv1)\n# Creating confusion matrix and calculating the accuracy score\ncm_rfc = confusion_matrix(y_cv1, y_pred)\nas_rfc = accuracy_score(y_cv1, y_pred)\nprint(as_rfc)","705ba521":"from numpy import sqrt\nfrom numpy import argmax\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = RanForest.predict_proba(x_cv1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_cv1, preds)\nroc_auc_rf = metrics.auc(fpr, tpr)\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (threshold[ix], gmeans[ix]))\n\nplt.figure()\nplt.title('Receiver Operating Characterstics')\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc_rf))\nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n \n# create the axis of thresholds (scores)\nax2 = plt.gca().twinx()\nax2.plot(fpr, threshold, markeredgecolor='r',linestyle='dashed', color='r')\nax2.scatter(fpr[ix], threshold[ix], marker='o', color='r', label='Best1')\nax2.set_ylabel('Threshold',color='r')\nax2.set_ylim([threshold[-1],threshold[0]])\nax2.set_xlim([fpr[0],fpr[-1]])\n \nplt.show()","02cc572e":"# Evaluating the best method to use in this loan prediction case wrt AUROC\nscore={'auc_logreg':roc_auc_logreg, 'auc_knn':roc_auc_knn, 'auc_svm_gaussian':roc_auc_svg, 'auc_nb':roc_auc_nb, 'auc_dtc':roc_auc_dt, 'auc_rfc':roc_auc_rf}\nscore_list=[]\nfor i in score:\n    score_list.append(score[i])\n    u=max(score_list)\n    if score[i]==u:\n        v=i  \n    print(f\"{i}={score[i]}\");   \nprint(f\"The best AUROC score in this case is {v} with accuracy score {u}\")","69081210":"# Evaluating the best method to use in this loan prediction case wrt accuracy\nscore={'as_logreg':as_logreg, 'as_knn':as_knn, 'as_svm_gaussian':as_svm_gaussian, 'as_nb':as_nb, 'as_dtc':as_dtc, 'as_rfc':as_rfc}\nscore_list=[]\nfor i in score:\n    score_list.append(score[i])\n    u=max(score_list)\n    if score[i]==u:\n        v=i  \n    print(f\"{i}={score[i]}\");   \nprint(f\"The best accuracy score in this case is {v} with accuracy score {u}\")","d127c5be":"importances=pd.Series(RanForest.feature_importances_, index=x_train1.columns) \nimportances.plot(kind='barh', figsize=(12,8))","54ef46a2":"pip install xgboost","a97d9beb":"from xgboost import XGBClassifier\nclassifier=XGBClassifier()\nclassifier.fit(x_train1,y_train1)\ny_pred=classifier.predict(x_cv1)\ncm=confusion_matrix(y_cv1,y_pred)\naccuracy_score(y_cv1,y_pred)","06386468":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = x_train1, y = y_train1, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","ab6c6263":"from numpy import sqrt\nfrom numpy import argmax\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = classifier.predict_proba(x_cv1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_cv1, preds)\nroc_auc_rf = metrics.auc(fpr, tpr)\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (threshold[ix], gmeans[ix]))\n\nplt.figure()\nplt.title('Receiver Operating Characterstics')\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc_rf))\nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n \n# create the axis of thresholds (scores)\nax2 = plt.gca().twinx()\nax2.plot(fpr, threshold, markeredgecolor='r',linestyle='dashed', color='r')\nax2.scatter(fpr[ix], threshold[ix], marker='o', color='r', label='Best1')\nax2.set_ylabel('Threshold',color='r')\nax2.set_ylim([threshold[-1],threshold[0]])\nax2.set_xlim([fpr[0],fpr[-1]])\n \nplt.show()","94d35ca2":"Not many outliners, therefore leave it as it is.","c59c24e2":"From the above graph we can infer that applicants having low coapplicant incomes have a higher chance of loan approval than those whose coapplicant income is higher. This doesnt fit logically into the problem as CoapplicantIncome should be higher to help repay the loan.\n\nThe possible explanation can be that not many applicants have a coapplicant , therefore the coapplicant income for them is 0 and hence the loan approval is not dependent on it.So we will make a new variable to see the combined effect on Loan status.","87983597":"### 2) APPLICANT INCOME\n\n#### TRAIN SET","d7ac77d5":"We observe that there are few missing values in test data set and now we will impute the new values , the same way we did for the train dataset above.","d5da57d7":"# DATA CLEANING","e193af5e":"#### CONTINIOUS VARIABLES","4abeeb82":"1. Married vs loan status","abce6406":"### CORRELATION ANALYSIS","673a42cb":"### UNIVARIATE ANALYSIS\nThis is the simplest from of statistical analysis.Only one variable is studied at a time.\nIt helps us to formulate conclusions such as-\n\n1.Outlier detection\n\n2.Concentrated points\n\n3.Pattern recognition\n\n4.Required transformations","05dea815":"#### RANDOM FOREST ","15e96210":"Due to the outliers bulk of the data in the loan amount is at the left and the right tail is longer. This is called right skewness. One way to remove the skewness is by doing the log transformation. As we take the log transformation, it does not affect the smaller values much, but reduces the larger values. So, we get a distribution similar to normal distribution.","426aafc4":"All the missing data has been taken care of in our train datset. Using the similar approach, we will fill all the missing values in our test dataset.","c60b3019":"## Simple look on the data","ba561d5c":"## FINAL MODEL BUILDING","31b7253a":"customers with 0 or 2 dependents have a higher proportion of loan approval.","a2ddbf6e":"This indicates no duplicate rows.","3db81fd3":"For modelling in Python, we have an inbuilt library called scikit-learn(sklearn).\nSklearn requires the Independent variables and the target variable (Loan Status) in separate datasets. So, we will drop our target variable from the train dataset and save it in another dataset.\n\nWe will save the x_train dataset without the target variable and save the target variable into y_train.","1a33c4e4":"Replacing by the most frequesnt value,mode.","84237301":"The Loan_Amount_Term is a continuous variable here. So instead of directly going with the median here, we would like to get an idea of what data represents by seeing the most occuring value.\n\nBut, here we get both the mode and median as 360. So we are choosing 360 to replace which just validates our point.","c264152c":"# Let us have a look at the target variable of the train set.","6446d581":"### CATEGORICAL FEATURES","11f2c4ef":"We can infer that percentage of married people who have got their loan approved is higher when compared to non- married people.","b6cb2b48":"#### TEST SET","2b9fe77e":"The process of model building is not complete without evaluation of model\u2019s performance. Suppose we have the predictions from the model, how can we decide whether the predictions are accurate? We can plot the results and compare them with the actual values, i.e. calculate the distance between the predictions and actual values. Lesser this distance more accurate will be the predictions. Since this is a classification problem, we can evaluate our models using any one of the following evaluation metrics:\n\n    1. True Positive - Targets which are actually true(Y) and we have predicted them true(Y)\n    2. True Negative - Targets which are actually false(N) and we have predicted them false(N)\n    3. False Positive - Targets which are actually false(N) but we have predicted them true(T)\n    4. False Negative - Targets which are actually true(T) but we have predicted them false(N)\n\n\n1)ACCURACY: (True Positive + True Negative) \/ (True Positive + False Positive + True Negative + False Positive)\n\n2)PRECISION: It is a measure of correctness achieved in true prediction i.e. of observations labeled as true, how many are actually labeled true. = TP \/ (TP + FP)\n\n3)RECALL(Sensitivity) - It is a measure of actual observations which are predicted correctly i.e. how many observations of true class are labeled correctly. It is also known as \u2018Sensitivity\u2019. = TP \/ (TP + FN)\n\n3)SPECIFICITY - It is a measure of how many observations of false class are labeled correctly. = TN \/ (TN + FP)\n\nSpecificity and Sensitivity plays a crucial role in deriving ROC curve.\n\nReceiver Operating Characteristic(ROC) summarizes the model\u2019s performance by evaluating the trade offs between true positive rate (sensitivity) and false positive rate(1- specificity). The area under curve (AUC), referred to as index of accuracy(A) or concordance index, is a perfect performance metric for ROC curve. Higher the area under curve, better the prediction power of the model.","9dbf63d3":"#### CATEGORICAL VARIABLES","0b57ff8b":"### K-FOLD\n\nThe dataset is divided into K grpups(k=10).First, group 1 is taken as test set and all others as train set.Then group 2 is taken as test set and all others as train set. This process repeats till all groups are taken as test set, therefore we get 10 accuracies. This is done because sometimes we get lucky on 1 test set and not others, this way different test sets are tested.","d2686b62":"#### CONCLUSION:\n\n1) 80% males and 20% females\n\n2) 65% married and 35% not married\n\n3) 80% not self employed and 10% self employed\n\n4) 80% have credit history and 20% do not have\n\n5) Less than 10% of the applicants have 3+ dependents and more than 50% have 0 dependents.","1b738d91":"#### XGBOOST","21bc4414":"#### NUMERIC VARIABLES","d8240cc1":"### 2) CO-APPLICANT INCOME\n\n#### TRAIN SET","8c83dd4a":"#### BALANCE INCOME","f132e278":"We make bins for the applicant income varaiable based on the values in it and analyse the corresponding loan status for each bin.","b375e2f7":"Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided.","fb0dbec7":"Graduates have a higher proportion of loan approvals than not Graduates.","82d17361":"The dataset has been divided into training and validation part.\n\n70% data will use for train the model and rest of the 30% data will use for checking validation of the model.","58bdca32":"We can treat the missing values according to the following:\n\n1. Categorical Variables- imputation using mode\n2. Numerical Variables-imputation using mean or median","973c5c5e":"The yellow bars indicates missing values while blue bars indicate no missing values.","8ad44d2f":"## HYPOTHESIS\nBelow are some factors which I think can affect the loan approval-\n\n1) Greater the applicant income, greater the loan approval chances.\n\n2) Lesser the loan amount,greater the loan approval chances.\n\n3) Property Area = Urban, greater chances.\n","61d6e1e9":"## RANDOM FOREST ","f81855f8":"## SVM","8c88fe5e":"Proportion of approved loans is higher for low and average Loan Amount as compared to that of High loan amount which supports our our initial hypothesis that approval chance is more for less loan amount.\n\nNow we will drop the bins we made for the exploration part and move on to the next section of data cleaning .","b686741e":"#### NUMERIC VARIABLES\nReplacing by median","444c4a1f":"## LOGISTIC REGRESSION\n\nLet us make our first model to predict the target variable. We will start with Logistic Regression which is used for predicting binary outcome.\n\n\nLogistic Regression is a classification algorithm. It is used to predict a binary outcome (1 \/ 0, Yes \/ No, True \/ False) given a set of independent variables. Logistic regression is an estimation of Logit function. Logit function is simply a log of odds in favor of the event. This function creates a s-shaped curve with the probability estimate. We use this curve to predict the probability of the outcome of the event.\nIf the probabilty is below 0.5, predicted target variable will be NO\/0\/False.\nIf the probabilty is above 0.5, predicted target variable will be YES\/1\/True.","386082d9":"## K-NEAREST NEIGHBORS ","f75a12ee":"There is hardly any correlation between Loan_Status and Self_Employed applicants. So in short we can say that it doesn\u2019t matter whether the applicant is self employed or not.","57ceb101":"Let\u2019s visualize the Loan amount variable.","5d914e15":"## MISSING VALUES TREATMENT","6acbeb21":"#### CATEGORICAL VARIABLES\nReplacing by mode","16ab010d":"#### TOTAL INCOME","326bb4e5":"### NUMERIC FEATURES","30955d5d":"#### With this we have completed our exploratory data analysis on the training data.","fd644330":"Now it is normalized.\n\n#### TEST SET","05f600cb":"Reading the Data\n\nTrain Data - for training the model. It contains all the independent variables and the target variable.\n\nTest Data - for testing the model. We will apply the model to predict the target variable for the test data.\n\nTo determine the accuracy of our model after training it, we will test on the unseen data called as test data.","6a34fff1":"The above results show that ApplicantIncome has no effect on Loan_Status which contradicts our hypothesis. Let's analyze the effect of coapplicantIncome on Loan_Status","fba2fb1c":"We can see that Proportion of loans getting approved for applicants having low Total_Income is very less as compared to that of applicants with Average, High and Very High Income.","fc610fdc":"#### CONTINIOUS VARIABLES\n","29976cb2":"From the above visualization we can say that applicants with income below 20,000 with no prior credit history and are not granted a loan. But this approach also does not make sense as loan status depends on various other factors.","02f498de":"As the mean is adversely affected by outliners and our data has a lot of outliners, we replace numeric values with the MEDIAN.","4155e1a1":"#### EMI","62cdc4b3":"# Exploratory Data Analysis or EDA \nThis is the process where the data is intensely studied.It helps us understand the mystery and insights behind the data which may not make sense in the first glance.\nEDA employs three primary statistical techniques to go about this exploration:\n\n1)Univariate Analysis\n\n2)Bivariate Analysis\n\n3)Multivariate Analysis","6a4e85d9":"Describe categorical data","fc05ddf5":"Males have a higher proportion of loan approval than females.","12ed763c":"We will copy our train and test dataset to a new dataset called train_new and test_new. (We are basically making copies of our dataset to retain its contents and ensure that no changes are made to the original. All changes will be made in the new copy.)\n\nFrom the new copies of our train and test datasets we will drop all those features which were used to create these new features - EMI, Balance_Income and Total_Income.\n\nWe will drop 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term' as the correlation between these features and the new features will be very high and the linear regression model assumes that the correlation between variables is not too high i.e they are not highly correlated. We also wish to remove noise from our dataset and removing these features will help in noise reduction as well.","b5faaf85":"This indicates the presence of missing values. We will take care of them as we proceed.","252476e8":"#### CATEGORICAL VARIABLE against TARGET VARIABLE(loan status)","39c85d3e":"This gives a description of the numeric data","595a7ca0":"### ORDINAL FEATURES","29ec8f06":"Missing values in Gender,Married,Dependents,Self employed,Loan amount,Loan amount term and Credit history.","810b2aa2":"# MODEL BUILDING\n\n","a7e55461":"We are visulaising different types of variables :\n\n1.Categorical features: These features have categories (Gender, Married, Self_Employed, Credit_History, Loan_Status)\n\n2.Ordinal features: Variables in categorical features having some order involved (Dependents, Education, Property_Area)\n\n3.Numerical features: These features have numerical values (ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term)","ca01ff5e":"## Feature Engineering \n\nLet us make an attempt to better our score.\nWe can do this by feature engineering. Based on our domain knowledge, we can say that there are some more factors which may affect our results.\nThese features can be\n\n1)Total Income: ApplicantIncome + CoapplicantIncome\nIf the total income is high, the chance of loan approval is also high.\n\n2)EMI: It is the monthly amount to be paid by the applicant to repay the loan. Applicants with higher EMIs might find it difficult to pay back their loans. We calculate this feature by taking ratio of loan_amount with respect to the loan_amount_term.\n\n3)Balance Income: This is the income left after the EMI has been paid. Idea behind creating this variable is that if the value is high, the chances are that they might be able to pay their upcoming EMIs timely and hence loan approval chances go up.","f7102055":"# Problem Statement\nThis company deals in all home loans. Customer first apply for home loan,after that company validates the customer eligibility for loan. Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers.\nWe are gonna categorize the customers on the basis of details like Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others.\nWe are gonna perform binary classification and assign 0 for customers not eligible for loan and 1 for customers who are.\n","597ecdee":"Next is the training of our model based on the training dataset and make predictions on the test data. We also need to provide validation for our model i.e. confirm whether our model is predicting correctly or not. For this we can split our training dataset into training data and validation data. We cannot use our test dataset for this purpose as our test data doesn't provide us with values of the target variable to validate our model. \n\nThus to split our train data we use the function train_test_split() from sklearn library.","0089a92a":"Customers with prior credit history have a very high chance of loan approval.","9bb7ed37":"This shows that theres are higher number of graduates who have higher incomes which seem to be the majority outliers in the boxplot.","4b92f78a":"## BIVARIATE ANAYLYSIS\nWe need to see how each variable correlates with Loan Status. This is done using bivariate analysis.","17b28243":"An important point to remember is that Logistic Regression only takes numerical inputs. In our dataset we only have few numerical variables and rest are categorical variables. These categorical variables have to be converted to numeric values to be able to feed them into our regression model.\n\nWe achieve this by using pd.get_dummies() function. The .get_dummies() turns categorical variables into a series of 0 and 1, making them lot easier to quantify and compare.\n\nFor example, Once we apply dummies to this variable, it will convert the \u201cGender\u201d variable into two variables(Gender_Male and Gender_Female), one for each class, i.e. Male and Female. Gender_Male will have a value of 0 if the gender is Female and a value of 1 if the gender is Male.","57adadb4":"Loan approval for Semiurban areas is higher","25cb1c30":"It can be inferred that most of the data in the distribution of applicant income, co- applicant income and loan amount is towards left and of loan amount term is towards right which means they are not normally distributed. We will try to make it normal in later sections as algorithms works better if the data is normally distributed.\n\nThe boxplot confirms the presence of a lot of outliers\/extreme values. ","48dbb3f2":"As we can see, our data is imbalanced since there 70-30 distribution between approved and unapproved applicants.","49976134":"#### NUMERIC VARIABLE against TARGET VARIABLE(loan status)","573a93d8":"This indicates 614 rows(customers) and 13 columns(variables)","019306ee":"## OUTLINER TREATMENT\n\nchecking for outliners in numeric data-\n\n### 1) LOAN AMOUNT\n\n#### TRAIN SET","ecb4c288":"82% of our predictions are correct"}}