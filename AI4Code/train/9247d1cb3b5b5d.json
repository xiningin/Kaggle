{"cell_type":{"d6b09e7e":"code","80f28f67":"code","cc1f0a04":"code","a95f3096":"code","90e3682b":"code","9476fe99":"code","ef3d0e23":"code","e753bd72":"markdown"},"source":{"d6b09e7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80f28f67":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\nimport random\nfrom sklearn import preprocessing\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nif device == 'cuda':\n  torch.cuda.manual_seed_all(777)\n\nrandom.seed(777)\ntorch.manual_seed(777)\n\nScaler=preprocessing.StandardScaler()","cc1f0a04":"\nxy_data=pd.read_csv('train.csv')\nprint(xy_data.info())\nprint(xy_data.isna().sum())\nxy_data=xy_data.fillna(0)\nxy_data.head()\nxy_data.mean()['solar']\ntest_data=pd.read_csv('test.csv')\ntest_data\nimport datetime as dt\n\nxy_data=pd.read_csv('train.csv')\nxy_data['solar'] = xy_data['solar'].interpolate()\nxy_data.fillna(value={'rain':0}, inplace=True)\n\nxy_data['date'] = pd.to_datetime(xy_data['date'])\nxy_data['date'] = xy_data['date'].map(dt.datetime.toordinal)\nxy_data\ntest_data = pd.read_csv('test.csv')\ntest_data['solar'] = test_data['solar'].interpolate()\ntest_data.fillna(value={'rain':0}, inplace=True)\n\ntest_data['date'] = pd.to_datetime(test_data['date'])\ntest_data['date'] = test_data['date'].map(dt.datetime.toordinal)\ntest_data\nxy_data.isna().sum()\ntest_data.isna().sum()\nx_data=xy_data.loc[:,'aveTemp':'solar']\ny_data=xy_data.loc[:,'Category']\nx_data=np.array(x_data,dtype=float)\ny_data=np.array(y_data,dtype=float)\n\nx_data=Scaler.fit_transform(x_data)\nx_train=torch.FloatTensor(x_data).to(device)\ny_train=torch.LongTensor(y_data).to(device)\ntrain_dataset=torch.utils.data.TensorDataset(x_train, y_train)","a95f3096":"# \ud559\uc2b5 \ud30c\ub77c\ubbf8\ud130 \uc124\uc815\nlearning_rate = 0.1\nbatch_size = 100\ndata_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=True,\n                                          drop_last=True)","90e3682b":"linear1=torch.nn.Linear(5,6,bias=True)\nlinear2=torch.nn.Linear(6,6,bias=True)\nlinear3=torch.nn.Linear(6,6,bias=True)\nlinear4=torch.nn.Linear(6,6,bias=True)\nlinear5=torch.nn.Linear(6,4,bias=True)\nrelu=torch.nn.ELU()\ntorch.nn.init.kaiming_normal_(linear1.weight)\ntorch.nn.init.kaiming_normal_(linear2.weight)\ntorch.nn.init.kaiming_normal_(linear3.weight)\ntorch.nn.init.kaiming_normal_(linear4.weight)\ntorch.nn.init.kaiming_normal_(linear5.weight)\nmodel=torch.nn.Sequential(linear1,relu,\n                          linear2,relu,\n                          linear3,relu,\n                          linear4,relu,\n                          linear5\n                          ).to(device)\nloss = torch.nn.CrossEntropyLoss().to(device) # softmax \ub0b4\ubd80\uc801\uc73c\ub85c \uacc4\uc0b0\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) ","9476fe99":"total_batch = len(data_loader)\nfor epoch in range(350+1):\n    avg_cost = 0\n\n    for X, Y in data_loader:\n\n        X = X.to(device)\n        Y = Y.to(device)\n      \n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = loss(hypothesis, Y)\n        cost.backward()\n        optimizer.step()\n\n        avg_cost += cost \/ total_batch\n\n    if epoch%100 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n\nprint('Learning finished')","ef3d0e23":"with torch.no_grad():\n    test=test_data.loc[:,'aveTemp':'solar']\n    test=np.array(test,dtype=float)\n    test=Scaler.transform(test)\n    test=torch.from_numpy(test).float().to(device)\n    prediction = model(test)\n    correct_prediction = torch.argmax(prediction,dim=1)\n","e753bd72":"* \ud65c\uc131\ud568\uc218: ReLU(\uae30\uc874 \ubca0\uc774\uc2a4\ub77c\uc778) -> LeakyReLU(\ubca0\uc774\uc2a4\ub77c\uc778\uacf5\uaca9\uc131\uacf5\ud558\uc2e0\ubd84)->ELU\ub85c \ubcc0\uacbd\n* \ucd08\uae30\ud654\ubc29\ubc95: xavier initialization -> kaiming initialization "}}