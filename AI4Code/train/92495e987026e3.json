{"cell_type":{"8c7d62be":"code","047df3f4":"code","51cb919e":"code","ed64d6a9":"code","4f9b5928":"code","2f23ee77":"code","520a0715":"code","d83c70e6":"code","c950e2a4":"code","97895c13":"code","bbb6e85a":"code","a90e2f3a":"code","4dd02a89":"code","c4bfd0f1":"code","a64f78cf":"code","ef3df03b":"code","89b8cd7e":"code","ce678a8d":"code","e86ae4b1":"code","06b5e718":"code","8cbfcc51":"code","7fdb6d81":"code","1047470e":"code","7601a65c":"code","23fd8cd6":"code","dc73f42a":"code","309a7571":"markdown","9b75ab15":"markdown","39891f92":"markdown","ef6aa7fe":"markdown","00127917":"markdown","b384aca8":"markdown","f7ece9b5":"markdown","a8e1fec6":"markdown","1c953860":"markdown","2f599359":"markdown","cbe47169":"markdown","af69e65c":"markdown","6c1959e9":"markdown","e7f4ad1a":"markdown","45fb6002":"markdown"},"source":{"8c7d62be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom sklearn.metrics import accuracy_score\n\n# Any results you write to the current directory are saved as output.","047df3f4":"data= pd.read_csv(\"..\/input\/diabetes.csv\")","51cb919e":"data.head()","ed64d6a9":"data.info()","4f9b5928":"data.shape","2f23ee77":"data.describe()","520a0715":"# correlation between features\ndata.Outcome =[\"D\" if each == 1 else \"ND\" for each in data.Outcome]\n\nsns.pairplot(data=data,palette=\"Set2\",hue=\"Outcome\")\nplt.show()","d83c70e6":"data.Outcome =[\"1\" if each == \"D\" else \"0\" for each in data.Outcome]\n# we find out number of zeros in each feature\nzeros = (data == 0)\nzeros.sum(axis=0)","c950e2a4":"# we replace zeros of each column ex. pregnancies ,age  and outcome with their column's mean \nfor each in data.columns[1:6]:\n    data[each] = data[each].replace(0, data[each].median())\ndata.head()\n","97895c13":"y=data.Outcome.values                                  \nx_data=data.drop(['Outcome'],axis=1)\nprint(y.shape,x_data.shape)\nx_data.head()","bbb6e85a":"#normalization: to get a value between 0 and 1 for each feature to prevent  some features from being dominant\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values\nx.head()","a90e2f3a":"diabet = data[\"BMI\"]\nsimilarity_with_other_col = data.corrwith(diabet) #correlation of BMI with other features\nsimilarity_with_other_col","4dd02a89":"#Seaborn Heatmap to find out correlation between each feature\nf,ax = plt.subplots(figsize=(12,10))\ncmap=sns.diverging_palette(150, 275, s=80, l=55,n=9)\nsns.heatmap(\ndata.corr(), \nannot=True, annot_kws={'size':12},\nlinewidths=.8,linecolor=\"blue\", fmt= '.2f',ax=ax,square=True,cmap=cmap)\n\nplt.show()","c4bfd0f1":"from sklearn.model_selection import train_test_split\n#we split our data in 80% train and 20% test data\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n#random state is important to get the same values after each forward_backward_propagation\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","a64f78cf":"#Logistic Regression with sklearn\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(C=100,penalty=\"l2\",solver=\"saga\",class_weight=None)\n#C : float, default value: 1.0\n#Inverse of regularization strength should be  positive, \n#small values>stronger regularization.\n#solver : For small datasets choose \u2018liblinear\u2019 ,\u2018sag\u2019 and \u2018saga\u2019 are faster for large datasets.\n#For multiclass problems choose only \u2018newton-cg\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018lbfgs\u2019 for multinomial loss\n#\u2018newton-cg\u2019, \u2018lbfgs\u2019 and \u2018sag\u2019 with L2 penalty,\u2018liblinear\u2019 and \u2018saga\u2019 with L1 penalty.\nlogreg.fit(x_train, y_train)\ny_pre_lr = logreg.predict(x_test)\n\ntest_acc= logreg.score(x_test,y_test) \n\nprint(\"LR accuracy :  \",test_acc)\nlr_acc=logreg.score(x_test,y_test)\n","ef3df03b":"from sklearn.model_selection import GridSearchCV\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}  \n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,grid,cv = 10)\nlogreg_cv.fit(x_train,y_train)\ny_pre_lrcv = logreg_cv.predict(x_test)\nprint(\"tuned hyperparameters: \",logreg_cv.best_params_)\nprint(\"lr_accuracy: \",logreg_cv.best_score_)\n\nlogreg2 = LogisticRegression(C=100.0,penalty=\"l1\")\nlogreg2.fit(x_train,y_train)\nprint(\"lr_score: \", logreg2.score(x_test,y_test))\n","89b8cd7e":"# knn \nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=None, n_neighbors=16, p=2,\n           weights='uniform')\n#\u2018distance\u2019 : weight points, closer neighbors  have  greater influence than neighbors far away.\n#\u2018uniform\u2019 : uniform weights, all points are weighted equally\n#algorithm :\u2018auto\u2019  to decide the most appropriate algorithm \n# n_neighbors = k\nknn.fit(x_train,y_train)\ny_pre_knn = knn.predict(x_test)\nprint(\" With KNN (K= {}) accuracy is: {} \".format(16,knn.score(x_test,y_test)))\nknn_acc=knn.score(x_test,y_test)","ce678a8d":"# find k value\nk_list = []\nfor each in range(1,25):\n    knn_2 = KNeighborsClassifier(n_neighbors = each)\n    knn_2.fit(x_train,y_train)\n    k_list.append(knn_2.score(x_test,y_test))\n    \n\nf = plt.subplots(figsize=(18,8))\nplt.plot(range(1,25),k_list)\n   \nplt.xlabel('k values',fontsize = 15,color='black')             \nplt.ylabel('accuracy',fontsize = 15,color='black')\nplt.title('K values-Accuracy Plot',fontsize = 20,color='black')\nplt.xticks(range(1,25))\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(k_list),1+k_list.index(np.max(k_list))))","e86ae4b1":"from sklearn.svm import SVC\n\nsvm = SVC(C=100.0, cache_size=200, class_weight=\"balanced\", kernel='rbf',max_iter=-1)\n#Penalty parameter C ,default = 1.0\nsvm.fit(x_train,y_train)\ny_pre_svm = svm.predict(x_test)\nprint(\"SVM accuracy is: \",accuracy_score(y_test, y_pre_svm))\nsvm_acc=accuracy_score(y_test, y_pre_svm)","06b5e718":" # %% Naive bayes \nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pre_nb = nb.predict(x_test)\n\nprint(\"NB accuracy is: \",nb.score(x_test,y_test))\nnb_acc=nb.score(x_test,y_test)","8cbfcc51":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(class_weight=\"balanced\",max_leaf_nodes=100)\ndt.fit(x_train,y_train)\n\ny_pre_dt = dt.predict(x_test)\n\nprint(\"DT accuracy is: \", dt.score(x_test,y_test))\ndt_acc= dt.score(x_test,y_test)","7fdb6d81":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 500,max_depth=200)\nrf.fit(x_train,y_train)\n\ny_pre_rf = rf.predict(x_test)\nprint(\"RF accuracy is: \",rf.score(x_test,y_test))\nrf_acc=rf.score(x_test,y_test)","1047470e":"import numpy  as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm_lr = confusion_matrix(y_test,y_pre_lr)        #logistic regression\ncm_dt = confusion_matrix(y_test,y_pre_dt)        #decision tree\ncm_knn = confusion_matrix(y_test,y_pre_knn)      #nearest neighbors\ncm_nb = confusion_matrix(y_test,y_pre_nb)        #naine bayes\ncm_rf = confusion_matrix(y_test,y_pre_rf)        #random forest\ncm_svm = confusion_matrix(y_test,y_pre_svm)      #support vector machine\n\ncm= np.array([cm_lr,cm_dt,cm_knn,cm_nb,cm_rf,cm_svm])\n  \n\nplt.figure(figsize=(20,10))\nplt.suptitle(\"Confusion Matrix\",fontsize=24,color=\"b\") \nclassification = np.array([\"Logistic Regression\",\"Decision Tree\",\"Random Forest\",\"K Nearest Neighbors\",\"Naive Bayes\",\"Support Vector Machine\"])\n\ni=0\nk=1\nwhile i < len(classification):\n    plt.subplot(2,3,k)\n    plt.title(classification[i],fontsize=14,color=\"b\")\n    sns.heatmap(cm[i],cbar=False,annot=True,cmap=\"PuBuGn\",fmt=\"d\",linewidths=.8,linecolor=\"red\")\n    i=i+1\n    k=k+1\n\nplt.show()\n \n        \n\n      ","7601a65c":"dictionary = {\"Class\":[\"Logistic Regression\",\"Decision Tree\",\"Random Forest\",\"K Nearest Neighbors\",\"Naive Bayes\",\"Support Vector Machine\"],\n              \"Accuracy\":[lr_acc,dt_acc,rf_acc,knn_acc,nb_acc,svm_acc]} \ndataFrame1 = pd.DataFrame(dictionary)\ndataFrame1","23fd8cd6":"\nfig, ax = plt.subplots(figsize=(15,10))\nN = 6  # number of groups\nind = np.arange(N)  # group positions\nwidth = 0.2  # bar width\n\nsns.barplot(x=dataFrame1['Class'], y=dataFrame1[\"Accuracy\"])\n\nax.set_xticks(ind + width)\nax.set_xticklabels(['LogisticRegression\\n',\n                    \"Decision Tre\\n\",\n                    'RandomForest\\n',\n                    \"K Nearest Neighbors\\n\",\n                    'Naive Bayes\\n',\n                    'Support Vector Machine\\n'],\n                   rotation=40,\n                   ha='right',fontsize = 13,color='magenta')\nplt.xlabel('Class',fontsize = 18,color='blue')\nplt.ylabel('Accuracy',fontsize = 18,color='blue')\nplt.ylim(0.65,0.85)\nplt.title('Class-Accuracy Diagram',fontsize = 20,color='blue')\n\nplt.savefig('graph.png')\nplt.grid()  ","dc73f42a":"\ntrace1 = go.Bar(\n                x = dataFrame1['Class'],\n                y = dataFrame1[\"Accuracy\"],\n                name = \"Accuracy\",\n                marker = dict(color = ['rgba(160, 200, 155, 0.7)','rgba(60, 20, 155, 0.7)','rgba(16, 200, 55, 0.7)','rgba(90, 2, 155, 0.7)',\n                              'rgba(33, 234, 155, 0.7)','rgba(67, 56, 155, 0.7)'],\n                             line=dict(color='rgba(0,0,0)',width=2)))\ndt = [trace1]\nlayout = go.Layout(barmode = \"relative\",title = 'Class-Accuracy Diagram',hovermode='closest',font=dict(family='Arial', size=14,color=\"rgba(123,34,121,0.7)\"),\n         xaxis= dict(title= 'Class',ticklen= None,zeroline= False,gridwidth=2,tickangle=-20), \n         yaxis= dict(title= 'Accuracy',ticklen= None,zeroline= False,gridwidth=2))\n\n\n\nfig = go.Figure(data = dt, layout = layout)\n\niplot(fig)","309a7571":"<a id=\"3\"><\/a> <br>\n#  K-Nearest Neighbors (KNN)\n","9b75ab15":"<a id=\"4\"><\/a> <br>\n# Support Vector Machine (SVM) Classification","39891f92":"<a id=\"2\"><\/a> <br>\n# Logistic Regression with sklearn","ef6aa7fe":"<a id=\"1\"><\/a> <br>\n# Exploratory Data analysis","00127917":"<a id=\"18\"><\/a> <br>\n# Conclusion:\n* After adjusting each method with their parameters we get K-Nearest Neighbors as the best  regression method for our dataset ! \nbut as we see in our Confusion Matrix none of the methods achieved a satisfactory result !","b384aca8":"<a id=\"15\"><\/a> <br>\n# Seaborn Heatmap","f7ece9b5":"<a id=\"16\"><\/a> <br>\n# Supervised Learning:","a8e1fec6":"<a id=\"9;\"><\/a> <br>\n# Gridsearch with Logistic Regression","1c953860":"<a id=\"8\"><\/a> <br>\n# Confusion Matrix","2f599359":"<a id=\"5\"><\/a> <br>\n# Naive Bayes Classification\n","cbe47169":"<a id=\"6\"><\/a> <br>\n# Decision Tree Classification","af69e65c":"<a id=\"18\"><\/a> <br>\n# Class-Accuracy Diagram","6c1959e9":"<a id=\"17\"><\/a> <br>\n# Class-Accuracy Barplot","e7f4ad1a":"<a id=\"7\"><\/a> <br>\n# Random Forest Classification","45fb6002":"# This kernel is about Logistic Regression of 'Pima Indians Diabetes Database'\n\n1.[Exploratory Data analysis](#1)\n\n2.[Seaborn Heatmap](#15)\n\n3.[Supervised Learning:](#16)\n\n4.[Logistic Regression with sklearn](#2) \n\n5.[Gridsearch with Logistic Regression](#9)\n\n6.[K-Nearest Neighbors (KNN)](#3)\n\n7.[Support Vector Machine (SVM) Classification](#4)\n\n8.[Naive Bayes Classification](#5)\n\n9.[Decision Tree Classification](#6)\n\n10.[Random Forest Classification](#7)\n\n11.[Confusion Matrix](#8)\n\n12.[Class-Accuracy Barplot](#17)\n\n13.[Class-Accuracy Diagram](#18)\n\n13.[Conclusion:](#18)\n\n\n\n\n"}}