{"cell_type":{"111864c4":"code","0e3c09af":"code","216a2873":"code","26f33954":"code","1b85bfa4":"code","825b8842":"code","d35f19ca":"code","dfdfd03d":"code","ea2e1248":"code","badd2b33":"code","6890c7a4":"code","2c07e6d2":"code","a67274c7":"code","b01357f5":"code","60586757":"code","852391b7":"code","00ebb548":"code","476724f6":"code","ff782151":"code","022fc877":"code","d7ff7926":"code","ba3eb959":"markdown","fddf5829":"markdown","3f0a786f":"markdown","26da1ef1":"markdown","819946a1":"markdown","e38092b9":"markdown","5bd78639":"markdown","0d6ea6e8":"markdown","0ce2a7da":"markdown","a278592e":"markdown","d44d5c7d":"markdown"},"source":{"111864c4":"import warnings       \nwarnings.filterwarnings('ignore')\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error","0e3c09af":"df = pd.read_csv('..\/input\/google-stock-data\/GOOG.csv', \n                 index_col=['Date'], parse_dates=['Date'])\ndf.head()","216a2873":"#data size\ndf.shape","26f33954":"#N\/A values\ndf.isnull().values.any()","1b85bfa4":"df['Close'].plot(figsize=(20, 8), linewidth=1, label = 'Closing price');\nplt.legend(fontsize = 15)","825b8842":"result = adfuller(df['Close'])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","d35f19ca":"diff1 = pd.Series(df['Close'].diff()).dropna()\ndiff1.plot(figsize=(20, 8), label = '1st difference');\nplt.legend(fontsize = 15)","dfdfd03d":"result = adfuller(diff1)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %e' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","ea2e1248":"#ACF and PACF of the initial data\nplt.rcParams[\"figure.figsize\"] = (15,6)\nplot_acf(df.Close);\nplot_pacf(df.Close);","badd2b33":"#ACF and PACF of the first difference\nplt.rcParams[\"figure.figsize\"] = (15,6)\nplot_acf(diff1);\nplot_pacf(diff1);","6890c7a4":"df['Close'].index = pd.DatetimeIndex(df['Close'].index).to_period('D')\nsize = int(len(df) * 0.8)\ntrain_df, test_df = df[0:size], df[size:len(df)]","2c07e6d2":"#function for selecting the best model based on the Bayesian information criterion (BIC)\ndef select_mod(p_values, d_values, q_values, train):    \n    mod_eval = []\n    mod_eval = pd.DataFrame(mod_eval)\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    model = ARIMA(train, order=order)\n                    model_fit = model.fit()\n                    mod_eval = mod_eval.append({'Order':order, 'BIC':model_fit.bic}, \n                                               ignore_index=True)\n                except:\n                    continue\n    return mod_eval","a67274c7":"#ARIMA parameters\np = range(0, 2)\nd = range(0, 2)\nq = range(0, 2)\nmodels = select_mod(p,d,q,train_df['Close'])\nmodels","b01357f5":"#choosing the best model\nprint(models[models.BIC == models.BIC.min()])","60586757":"model = ARIMA(train_df['Close'], order=(0,1,0))\nmodel_fit = model.fit()\nresid = model_fit.resid\n#evaluaring the residuals\nplt.figure(figsize=(15,5))\nplt.subplot(1, 2, 1)\nresid.plot.kde();\nplt.subplot(1, 2, 2)\nresid.plot()","852391b7":"k2, p = stats.normaltest(resid)\nprint(\"p = {:g}\".format(p))","00ebb548":"plt.rcParams[\"figure.figsize\"] = (15,6)\nplot_acf(resid);\nplot_pacf(resid);","476724f6":"history = [x for x in train_df['Close']]\npredictions = list()\nfor t in range(len(test_df['Close'])):\n    model = ARIMA(history, order=(0,1,0))\n    model_fit = model.fit()\n    yhat = model_fit.forecast()[0]\n    predictions.append(yhat)\n    history.append(test_df['Close'][t])","ff782151":"#Root Mean Square Error\nrmse = np.sqrt(mean_squared_error(test_df['Close'], predictions))\n#Mean Absolute Percentage Error\nmape = np.round(np.mean(np.abs(test_df['Close']-predictions)\/test_df['Close'])*100,2)\nprint('Test RMSE: %.3f' % rmse)\nprint('Test MAPE: %.3f' % mape)","022fc877":"plt.figure(figsize=(20,10))\nplt.plot(test_df['Close'].index, test_df['Close'], label = 'Actual values')\nplt.plot(test_df['Close'].index, predictions, color='red', label = 'Predicted values')\nplt.legend(fontsize = 15)","d7ff7926":"plt.figure(figsize=(20,10))\nplt.plot(train_df['Close'].index, train_df['Close'], \n         label = 'Training set')\nplt.plot(test_df['Close'].index, predictions, color='red', \n         label = 'Predicted values')\nplt.legend(fontsize = 15)","ba3eb959":"# 3. Checking the data","fddf5829":"The residuals are not correlated and have zero mean, so the model is built adequately.","3f0a786f":"The residuals belong to a normal distribution.","26da1ef1":"# 1. Importing libraries","819946a1":"# 2. Loading data","e38092b9":"# 6. Forecasting.","5bd78639":"# 4. Description of the data.","0d6ea6e8":"This time-series is definitely non-stationary, since it has a visible trend. We can confirm this hypothesis by using the Augmented Dickey-Fuller unit root test.","0ce2a7da":"P-value is 1, so the null hypothesis about the presence of a unit root cannot be rejected.\n\nLet's see if can get rid of non-stationarity by taking the first difference.","a278592e":"P-value is close to zero, so the data is stationary.","d44d5c7d":"# 5. Building the model"}}