{"cell_type":{"61d46367":"code","bee563dc":"code","551ff915":"code","c2838d27":"code","0cb6b22d":"code","572eac12":"code","da499cf8":"code","bcb2811a":"code","bb342a43":"code","0df9c983":"code","7f8abdfc":"code","f1ea9ddb":"code","79f872fd":"code","63608224":"code","2d0b73a0":"code","05378b1a":"code","c46df483":"code","54582dc5":"code","270b2b8d":"code","543f422b":"code","f8d68270":"code","d2940ef3":"code","1e0cc0af":"code","47d78eca":"markdown","214d813f":"markdown","c43181d6":"markdown","911fe3f4":"markdown","bf5f55e9":"markdown","a38721f8":"markdown","ece3a948":"markdown","f3eb273b":"markdown","51cb15c9":"markdown","8fbaca06":"markdown","9df515bd":"markdown","02c411a8":"markdown","2819f358":"markdown","257a7f78":"markdown","daed08c1":"markdown","54f64e4a":"markdown","3ef1ca8b":"markdown","d33317a0":"markdown","177e73a0":"markdown","b0a9d000":"markdown","625b2bca":"markdown"},"source":{"61d46367":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBRegressor\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\n\nseed = 47","bee563dc":"train_start = pd.read_csv('\/kaggle\/input\/prohack-all-data\/train_preproc_ploxa_interpol.csv')\ntest_start = pd.read_csv('\/kaggle\/input\/prohack-all-data\/test_preproc_ploxa_interpol.csv')\nsample_submit = pd.read_csv('\/kaggle\/input\/prohack-all-data\/sample_submit.csv')","551ff915":"df = pd.concat([train_start,test_start], ignore_index=True)\n\nfor col in test_start.iloc[:,:-2].columns.values:\n    mean = df[col].mean()\n    df[col] = df[col].fillna(mean)","c2838d27":"print('Initial years: ', df['galactic year'].unique())\n\ndf=df.replace(df['galactic year'].unique(), list(np.arange(1, 28)))\n\nprint('Modified years: ', df['galactic year'].unique())\n\ntotal_df = df.copy()","0cb6b22d":"col_interp = ['existence expectancy index', \n       'existence expectancy at birth',\n       'Gross income per capita', \n       'Income Index',\n       'Expected years of education (galactic years)',\n       'Mean years of education (galactic years)', \n       'Education Index',\n       'Population using at least basic sanitation services (%)',\n       'Gross capital formation (% of GGP)',\n       'Population, urban (%)',\n       'Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))',\n       'Population, ages 15\u201364 (millions)',\n       'Population, ages 65 and older (millions)',\n       'Life expectancy at birth, male (galactic years)',\n       'Total unemployment rate (female to male ratio)',\n       'Employment to population ratio (% ages 15 and older)',\n       'Exports and imports (% of GGP)',\n       'Estimated gross galactic income per capita, male',\n       'Estimated gross galactic income per capita, female',\n       'Domestic credit provided by financial sector (% of GGP)',\n       'Remittances, inflows (% of GGP)',\n       'Population with at least some secondary education (% ages 25 and older)',\n       'Gross enrolment ratio, primary (% of primary under-age population)',\n       'Current health expenditure (% of GGP)',\n       'Intergalactic Development Index (IDI), female',\n       'Intergalactic Development Index (IDI), male',\n       'Gender Development Index (GDI)',\n       'Gender Inequality Index (GII)']","572eac12":"unique_gal = list(df.galaxy.unique())\n\ntotal_df_new = pd.DataFrame()\nfor gal in unique_gal:\n    temp = total_df[total_df.galaxy==gal]\n    temp = temp.sort_values(by=['galactic year'], ascending=True).reset_index()\n    for c in col_interp:\n        for numb in range(2):\n            pr = temp[c].diff()\/temp[c].mean()\n            low_diff = pr[(pr>0.15) | (pr<-0.15)]\n            for ind in (low_diff.index.values):\n                if (ind!=0) and (ind!=(len(temp))):\n                    temp.loc[ind, c] = np.nan\n                    temp.loc[ind-1:ind+1, c] = temp.loc[ind-1:ind+1, c].interpolate()\n    total_df_new = pd.concat([total_df_new, temp])","da499cf8":"temp=df[df['galaxy']=='Hercules Dwarf'].sort_values(by=['galactic year'], ascending=True)\nsns.lineplot(x=temp['galactic year'], y=temp['Population, ages 15\u201364 (millions)'], label='Before fixing outliers')\ntemp1=total_df_new[total_df_new['galaxy']=='Hercules Dwarf'].sort_values(by=['galactic year'], ascending=True)\nsns.lineplot(x=temp1['galactic year'], y=temp1['Population, ages 15\u201364 (millions)'], label='After fixing outliers').set_title('Fixing outliers')\nplt.legend(loc=3)\nplt.show()","bcb2811a":"total_df_new = total_df_new.sort_values(by='index').reset_index()\ntotal_df_new = total_df_new.drop(labels=['level_0', 'index'], axis=1)\n\ntrain = total_df_new[:len(train_start.index)]\ntest = total_df_new[len(train_start.index):]","bb342a43":"scaler = preprocessing.RobustScaler(quantile_range=(15,70))\n\ntarget = train['y']\n\ntrain_num = train.drop(labels=['y', 'galaxy'], axis=1)\ntest_num = test.drop(labels=['galaxy', 'y'], axis=1)\n\ntrain_num_cols = list(train_num.columns)\ntest_num_cols  = list(test_num.columns)\n\ntrain_num = scaler.fit_transform(train_num)\ntest_num = scaler.transform(test_num)\n\ntrain_num = pd.DataFrame(train_num, columns=train_num_cols)\ntest_num = pd.DataFrame(test_num, columns=test_num_cols)","0df9c983":"cat_target = target.copy()\nfor i, num in enumerate(target):\n    if num>0.2:\n        cat_target[i]='high'\n    elif num<0.1:\n        cat_target[i]='low'\n    else:\n        cat_target[i]='mid'","7f8abdfc":"print('Distribution of classes: ')\nprint(cat_target.value_counts())","f1ea9ddb":"def feature_selection(train_num, model):\n\n    stkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n    train_copy = train_num.copy()\n    res_base=[]\n\n    for i, (tdx, vdx) in enumerate(stkfold.split(train_copy, cat_target)):\n        X_train, X_valid, y_train, y_valid = train_copy.iloc[tdx], train_copy.iloc[vdx], target.iloc[tdx], target.iloc[vdx]\n        model.fit(X_train, y_train)\n        preds=model.predict(X_valid)\n        res_base.append(mean_squared_error(y_valid, preds, squared=False))\n\n    baseline = np.mean(res_base)\n\n    while True:\n        cols = list(train_copy.columns)\n        drop_rmse = []\n        for drop_col in tqdm(cols):\n\n            train_drop = train_copy.drop(labels=[drop_col], axis=1)\n            res=[]\n\n            for i, (tdx, vdx) in enumerate(stkfold.split(train_copy, cat_target)):\n                X_train, X_valid, y_train, y_valid = train_drop.iloc[tdx], train_drop.iloc[vdx], target.iloc[tdx], target.iloc[vdx]\n                model.fit(X_train, y_train)\n                preds=model.predict(X_valid)\n                res.append(mean_squared_error(y_valid, preds, squared=False))\n\n            drop_rmse.append(np.mean(res))\n        drop_df = pd.DataFrame(list(zip(drop_rmse, cols)), columns =['RMSE', 'Column'])\n        drop_df=drop_df.sort_values(by=['RMSE'], ascending=True)\n        print(drop_df.head())\n\n        if baseline > drop_df.iloc[0, 0]:\n            baseline = drop_df.iloc[0, 0]\n            train_copy = train_copy.drop(labels=[drop_df.iloc[0, 1]], axis=1)\n            print(f\"RMSE: {round(baseline,8)}, num_cols:{len(train_copy.columns)}, dropped:{list(set(train_num.columns) - set(train_copy.columns))}\") \n        else:\n            print('nothing better')\n            break","79f872fd":"#feature_selection(train_num, xgb)","63608224":"col_xgb = ['existence expectancy at birth',\n           'Gross income per capita', \n           'Expected years of education (galactic years)',\n           'Mean years of education (galactic years)', \n           'Education Index',\n           'Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))',\n           'Population, ages 15\u201364 (millions)',\n           'Population, ages 65 and older (millions)',\n           'Total unemployment rate (female to male ratio)',\n           'Exports and imports (% of GGP)',\n           'Estimated gross galactic income per capita, female',\n           'Domestic credit provided by financial sector (% of GGP)',\n           'Population with at least some secondary education (% ages 25 and older)',\n           'Gross enrolment ratio, primary (% of primary under-age population)',\n           'Current health expenditure (% of GGP)',\n           'Gender Development Index (GDI)',\n           'Gender Inequality Index (GII)', \n           'galactic year',\n           'galaxy_cb']","2d0b73a0":"train_num_xgb_def = train_num.copy()\ntest_num_xgb_def = test_num.copy()\n\ncol = col_xgb\n\ntrain_num_xgb_def = train_num_xgb_def[col]\ntest_num_xgb_def = test_num_xgb_def[col]\n\nxgb=XGBRegressor(verbosity=1, random_state=seed)\nstkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)","05378b1a":"res=[]\nprobs = pd.DataFrame()\n\nfor i, (tdx, vdx) in enumerate(stkfold.split(train_num_xgb_def, cat_target)):\n    X_train, X_valid, y_train, y_valid = train_num_xgb_def.iloc[tdx], train_num_xgb_def.iloc[vdx], target[tdx], target[vdx]\n    xgb.fit(X_train, y_train)\n    preds=xgb.predict(X_valid)\n    oof_predict = xgb.predict(test_num_xgb_def)\n    probs['fold_%i'%i] = oof_predict\n    res.append(mean_squared_error(y_valid, preds, squared=False))\n\nprint('cv: ', round(np.mean(res),8))  \n\nprobs['res_xgb'] = probs.mean(axis=1)\nsns.distplot(probs['res_xgb']).set_title('Predictions distribution')\nplt.show()","c46df483":"from xgboost import plot_importance\n\nplot_importance(xgb)\nplt.show()","54582dc5":"from lightgbm import LGBMRegressor\n\nlgbm = LGBMRegressor(objective='regression', verbose=-1, random_state=seed, \n                     learning_rate=0.2,n_estimators=900, boosting='dart',num_leaves=5)","270b2b8d":"df_opt = pd.DataFrame()\n\ndf_opt['pred'] = probs['res_xgb']\ndf_opt['smpl_opt_pred'] = sample_submit['opt_pred']\ndf_opt['existence expectancy index'] = test['existence expectancy index']\ndf_opt['const'] = df_opt['pred'].apply(lambda x: ((3-np.log(x+0.01))**2)\/1000)\n\ndf_opt = df_opt.sort_values(by='existence expectancy index')","543f422b":"sns.lineplot(x=df_opt['pred'],y=df_opt['const'])\nplt.xlabel('Predictions')\nplt.ylabel('Constant')\nplt.show()","f8d68270":"from scipy.optimize import minimize\n\nx0 = np.array(df_opt['smpl_opt_pred'])\nconsts = np.array(df_opt['const'])\n\ndef objective(x):\n    return -x.dot(consts)\n\ndef constraint1(x):\n    return 49999.99-sum(x)\n\ndef constraint2(x):\n    return sum(x[:67])-4999.99\n\nb = (0.0, 100.0)\nbnds = (b,)*890\n\ncon1 = {'type': 'ineq', 'fun': constraint1}\ncon2 = {'type': 'ineq', 'fun': constraint2}\ncons = ([con1,con2])\nsolution = minimize(objective,x0,method='SLSQP',\n                    bounds=bnds,constraints=cons)\n\nx = solution.x\nprint('result:',-1*objective(x))","d2940ef3":"f, axes = plt.subplots(2,1, figsize=(7, 7))\nsns.distplot(target,ax=axes[0]).set_title('No Log-transformation')\nsns.distplot(np.log(target),ax=axes[1]).set_title('With Log-transformation')\nplt.show()","1e0cc0af":"from sklearn.model_selection import cross_val_predict\n\ny_pred = cross_val_predict(xgb, train_num_xgb_def, target, cv=5)\n\nfig, ax = plt.subplots()\nax.scatter(y=target,x=y_pred)\nax.set_xlabel('Predicted value')\nax.set_ylabel('True value')\n\nminimum = np.min([ax.get_xlim(), ax.get_ylim()])\nmaximum = np.max([ax.get_xlim(), ax.get_ylim()])\n\nlims = [minimum, maximum]\nk=0.1\n\nax.plot(lims, lims, 'k-', alpha=0.7)\nax.plot([minimum, maximum-k], [minimum+k, maximum], 'g-')\nax.plot([minimum+k, maximum], [minimum, maximum-k], 'g-')\n\nmask = np.absolute(y_pred-target)>k\nprint('Is row an outlier?')\nprint(mask.value_counts())","47d78eca":"We also used RobustScaler for fixing outliers.\n> This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n> \n> Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method.\n> \n> Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean \/ variance in a negative way. In such cases, the median and the interquartile range often give better results.","214d813f":"Code for optimization below won't give the best score for 2nd task. \n\nFew reasons:\n* we empirically found x0 - initial vector\n* we used another parameters in constraints","c43181d6":"# Modeling\n<b><font size=\"4\">Cross validation<\/font><\/b>\n\nAs I said earlier, we used StratifiedKFold cross validation, which works as it shown on photo.\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png\" width=\"500px\"> \n\nMoreover we predict test set with this technique. Model fits on each X_train, predict test set and final result is made by averaging of all predictions on test set.","911fe3f4":"The second part is over! If you missed first part, click [here](https:\/\/www.kaggle.com\/mrmorj\/prohack-hackathon-part-1-eda-preprocessing).","bf5f55e9":"<b><font size=\"4\">Fixing outliers<\/font><\/b>\n\nWe found that some columns have got outliers which degrades model performance. So we choose columns which have got lots of outliers and deleted them(we made sections with outliers more smoother).","a38721f8":"Also quite interesting thing that if we renumber 'galactic years' from sorted initial values to numbers from 1 to 27, we'll got better score.","ece3a948":"<b><font size=\"4\">Categorical target<\/font><\/b>\n\nAs you see, we made 3 classes of our target to use StratifiedKFold. This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.","f3eb273b":"<b><font size=\"4\">Importing libraries<\/font><\/b>","51cb15c9":"<b><font size=\"4\">Normalization of the target<\/font><\/b>","8fbaca06":"<b><font size=\"4\">Remove rows-outliers from train set<\/font><\/b>\n\nWe tried to detect outliers in train set by cross_val_predict and drop them. If the true value and the predicted difference in absolute value greater than 0.1, then this row is considered to be an outlier. But maybe cause our model a little bit overfitted, we didn't get improvement.","9df515bd":"# Stacking\nWe used small stack with LightGBM in proportions 80\/20 with respect to XGBoost.\n\n<img src=\"https:\/\/github.com\/imgremlin\/Photos\/blob\/master\/Untitled%20Diagram.png?raw=true\" width=\"400px\"> \n\nLightGBM has got next parameters:","02c411a8":"# Feature engeneering\n<b><font size=\"4\">Some tips<\/font><\/b>\n\nAfter preprocessing part, where we tried to handle with missing data, there were still some NaN's. It was decided to insert the average value with respect to each columns. ","2819f358":"# Feature selection\nWe used this function to select the columns with which our model works best. Feature selection was performed using cross validation. This function is not the most optimal in its speed, but it was enough for this task. It drops one column at a time and perform cross validation, then chose a set with the best result and repeat steps below up to the moment when cross validation fall down.","257a7f78":"We counted and found out that exists 67 countries with 'existence expectancy index' < 0.7. So it was made up a system of equations:\n<img src=\"https:\/\/github.com\/imgremlin\/Photos\/blob\/master\/Screenshot%20(21).png?raw=true\" width=\"350px\"> ","daed08c1":"# Optimization\nWe used SciPy for 2nd optimization task. ","54f64e4a":"We tried to normalize target to make it less skewed, and to shorten the right tail.\nFor this approach we used:\n* Square Root Transformation\n* Log Transformation\n* Box Cox Transformation\n\nBut any of them didn't bring good result. In my opinion because our dataset was also skewed in some way. And because of this, the normalized target was discordant with the skewed dataset.","3ef1ca8b":"# What didn't help\n<b><font size=\"4\">CatBoost<\/font><\/b>\n<img src=\"https:\/\/camo.githubusercontent.com\/1ba204e6a09e6f13c919dcf961fe5a9a7f2d6e30\/687474703a2f2f73746f726167652e6d64732e79616e6465782e6e65742f6765742d646576746f6f6c732d6f70656e736f757263652f3235303835342f636174626f6f73742d6c6f676f2e706e67\" width=\"300px\"> \nWe tried to add CatBoost to stack but it didn't give score improvement.","d33317a0":"<img src=\"https:\/\/kenya.ai\/wp-content\/uploads\/2020\/05\/Mckinsey-prohack.png\" width=\"1000px\"> \n# International Data Science Hackathon by McKinsey & Company\n\n\n\n**by team GORNYAKI (Tsepa Oleksii and Samoshin Andriy [Ukraine, KPI, IASA])**\n\nThanks to the organizers for this hackathon and everyone for participating! In this notebook you will find:\n\n* **feature engeneering**\n* **feature selection**\n* **modeling**\n* **optimization part**\n* **which things didn't help us to improve score**\n\nFor those who hear about this competition for the first time, you can familiarize yourself with the condition [here](https:\/\/prohack.org\/). All data is located [here](https:\/\/www.kaggle.com\/mrmorj\/prohack-hackathon).\n\n[The firs part, where we talk about our EDA & Preprocessing.](https:\/\/www.kaggle.com\/mrmorj\/prohack-hackathon-part-1-eda-preprocessing)","177e73a0":"After using this function, we dropped about 15 columns. Then with help of correlation matrix were dropped some high corellated columns, because it's not the best idea to have few linear combinations of one or few columns in dataset. This trick didn't bring score increase in cross validation, but it gives in leaderboard.\n\nSo after two steps bellow we got next columns:","b0a9d000":"<b><font size=\"4\">Feature importance<\/font><\/b>","625b2bca":"<b><font size=\"4\">Rollings Statistics<\/font><\/b>\n\nBecause we have got not so big time window for each galaxy(about 20-30 values), it was rather hard to implement rolling statistics for each galaxy. But in truth, the rolling statistics gave an increase, but we decided not to use them."}}