{"cell_type":{"106acf2d":"code","a69490d7":"code","ce4c1999":"code","8c8148e0":"code","606f7339":"code","748c8dc1":"code","c123d23a":"code","df26df33":"markdown","0ea86dae":"markdown","54455563":"markdown","f7618fe6":"markdown","ca383a16":"markdown","49fb7fa1":"markdown","42abe292":"markdown","f59bed50":"markdown","735b8c8c":"markdown"},"source":{"106acf2d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport sys\n!{sys.executable} -m pip install plotly\n\n!{sys.executable} -m pip install wordcloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport random\nplt.rc('figure',figsize=(17,13))\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","a69490d7":"\nf_data = pd.read_csv('..\/input\/vaccination-tweets\/vaccination_tweets.csv')\nf_data.text =f_data.text.str.lower()\n\n#Remove twitter handlers\nf_data.text = f_data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n\n#remove hashtags\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\B#\\S+','',x))\n\n\n# Remove URLS\nf_data.text = f_data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n\n# Remove all the special characters\nf_data.text = f_data.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n\n#remove all single characters\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n\n# Substituting multiple spaces with single space\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","ce4c1999":"\nsid = SIA()\nf_data['sentiments']           = f_data['text'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nf_data['Positive Sentiment']   = f_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nf_data['Neutral Sentiment']    = f_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nf_data['Negative Sentiment']   = f_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\nf_data.drop(columns=['sentiments'],inplace=True)","8c8148e0":"plt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],bw=0.1)\nsns.kdeplot(f_data['Positive Sentiment'],bw=0.1)\nsns.kdeplot(f_data['Neutral Sentiment'],bw=0.1)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],bw=0.1,cumulative=True)\nsns.kdeplot(f_data['Positive Sentiment'],bw=0.1,cumulative=True)\nsns.kdeplot(f_data['Neutral Sentiment'],bw=0.1,cumulative=True)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.show()","606f7339":"\n#Sorting And Feature Engineering\nf_data = f_data.sort_values(by='date')\nft_data=f_data.copy()\nft_data['date'] = pd.to_datetime(f_data['date']).dt.date\n\nft_data['year']         = pd.DatetimeIndex(ft_data['date']).year\nft_data['month']        = pd.DatetimeIndex(ft_data['date']).month\nft_data['day']          = pd.DatetimeIndex(ft_data['date']).day\nft_data['day_of_year']  = pd.DatetimeIndex(ft_data['date']).dayofyear\nft_data['quarter']      = pd.DatetimeIndex(ft_data['date']).quarter\nft_data['season']       = ft_data.month%12 \/\/ 3 + 1\n\nplt.subplot(2,1,1)\nplt.title('Selecting A Cut-Off For Most Positive\/Negative Tweets',fontsize=19,fontweight='bold')\n\nax0 = sns.kdeplot(f_data['Negative Sentiment'],bw=0.1)\n\nkde_x, kde_y = ax0.lines[0].get_data()\nax0.fill_between(kde_x, kde_y, where=(kde_x>0.25) , \n                interpolate=True, color='b')\n\nplt.annotate('Cut-Off For Most Negative Tweets', xy=(0.25, 0.5), xytext=(0.4, 2),\n            arrowprops=dict(facecolor='red', shrink=0.05),fontsize=16,fontweight='bold')\n\nax0.axvline(f_data['Negative Sentiment'].mean(), color='r', linestyle='--')\nax0.axvline(f_data['Negative Sentiment'].median(), color='tab:orange', linestyle='-')\nplt.legend({'PDF':f_data['Negative Sentiment'],r'Mean: {:.2f}'.format(f_data['Negative Sentiment'].mean()):f_data['Negative Sentiment'].mean(),\n            r'Median: {:.2f}'.format(f_data['Negative Sentiment'].median()):f_data['Negative Sentiment'].median()})\n\nplt.subplot(2,1,2)\n\nax1 = sns.kdeplot(f_data['Positive Sentiment'],bw=0.1,color='green')\n\nplt.annotate('Cut-Off For Most Positive Tweets', xy=(0.4, 0.43), xytext=(0.4, 2),\n            arrowprops=dict(facecolor='red', shrink=0.05),fontsize=16,fontweight='bold')\nkde_x, kde_y = ax1.lines[0].get_data()\nax1.fill_between(kde_x, kde_y, where=(kde_x>0.4) , \n                interpolate=True, color='green')\nax1.set_xlabel('Sentiment Strength',fontsize=18)\n\n\nax1.axvline(f_data['Positive Sentiment'].mean(), color='r', linestyle='--')\nax1.axvline(f_data['Positive Sentiment'].median(), color='tab:orange', linestyle='-')\nplt.legend({'PDF':f_data['Positive Sentiment'],r'Mean: {:.2f}'.format(f_data['Positive Sentiment'].mean()):f_data['Positive Sentiment'].mean(),\n            r'Median: {:.2f}'.format(f_data['Positive Sentiment'].median()):f_data['Positive Sentiment'].median()})\n\nplt.show()","748c8dc1":"Most_Positive = f_data[f_data['Positive Sentiment'].between(0.4,1)]\nMost_Negative = f_data[f_data['Negative Sentiment'].between(0.25,1)]\n\nMost_Positive_text = ' '.join(Most_Positive.text)\nMost_Negative_text = ' '.join(Most_Negative.text)\n\n\npwc = WordCloud(width=600,height=400,collocations = False).generate(Most_Positive_text)\nnwc = WordCloud(width=600,height=400,collocations = False).generate(Most_Negative_text)\n\nplt.subplot(1,2,1)\nplt.title('Common Words Among Most Positive Tweets',fontsize=16,fontweight='bold')\nplt.imshow(pwc)\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.title('Common Words Among Most Negative Tweets',fontsize=16,fontweight='bold')\nplt.imshow(nwc)\nplt.axis('off')\n\nplt.show()","c123d23a":"Most_Positive = f_data[f_data['Positive Sentiment'].between(0.4,1)]\nMost_Negative = f_data[f_data['Negative Sentiment'].between(0.25,1)]\n\nMost_Positive_text = ' '.join(Most_Positive.text)\nMost_Negative_text = ' '.join(Most_Negative.text)\n\n\npwc = WordCloud(width=600,height=400,collocations = False).generate(Most_Positive_text)\nnwc = WordCloud(width=600,height=400,collocations = False).generate(Most_Negative_text)\n\nplt.subplot(1,2,1)\nplt.title('Common Words Among Most Positive Tweets',fontsize=16,fontweight='bold')\nplt.imshow(pwc)\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.title('Common Words Among Most Negative Tweets',fontsize=16,fontweight='bold')\nplt.imshow(nwc)\nplt.axis('off')\n\nplt.show()","df26df33":"# Covid 19 Vaccine VADER Sentiment Analysis","0ea86dae":"# Visualization of the most negative and the most positive sentiments","54455563":"![image.png](attachment:e655adc8-1b6f-4d98-9305-e3e133058475.png)","f7618fe6":"# Analysis of Sentiments","ca383a16":"# Importing Libraries","49fb7fa1":"# Data Preprocessing","42abe292":"# Top 10 most negative and most positve sentiments","f59bed50":"# Exploratory Data Analysis","735b8c8c":"# \ud83d\udc89 Covid19 Vaccine Sentiment Analysis"}}