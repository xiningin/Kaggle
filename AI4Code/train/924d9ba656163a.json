{"cell_type":{"fef95ac3":"code","75827a13":"code","3f0f5539":"code","aa21f372":"code","b13ae5c8":"code","e57987eb":"code","b6cca041":"code","f667edbb":"code","d88df466":"code","180824aa":"code","8847aba8":"code","37d371f8":"code","8bf98073":"code","e6ee788d":"code","18e019d7":"code","f034cf67":"code","ff22a82c":"code","502a1712":"code","43519bec":"code","2be4aab0":"code","fcbe06fa":"code","02d7c537":"code","68d423a7":"code","7ad583c5":"code","cf93e5d6":"code","7f456736":"code","17a569be":"markdown","f1ca1f10":"markdown","c4176833":"markdown","5b92df5b":"markdown","af05ada6":"markdown","938f034a":"markdown","05a5e0b6":"markdown","a48492c6":"markdown","b1f63d7f":"markdown","fd8f16e4":"markdown","0d5f2667":"markdown","d9362477":"markdown","0e3dcead":"markdown","5a6c6031":"markdown","5538e411":"markdown","dfee3c8b":"markdown","d352e4aa":"markdown","394071ff":"markdown","a5a90539":"markdown","f5a5f4ce":"markdown","5d7b8626":"markdown","670557f1":"markdown","2402fda9":"markdown","9351024a":"markdown","cc1bd4a7":"markdown","14d516f9":"markdown","63e2102d":"markdown","c1f44401":"markdown","1f2c79ac":"markdown","cc98f382":"markdown","bcc11326":"markdown","ee4435f7":"markdown","78958e8a":"markdown","6e43234e":"markdown","f5dfe778":"markdown","af6d6ac1":"markdown","3e12ca6b":"markdown","b8c5c0db":"markdown","830f90b0":"markdown","76fa9277":"markdown","119aeb61":"markdown","2e39dfb7":"markdown","33fdcdb1":"markdown","d152cecc":"markdown","24627f39":"markdown","a9cbdee1":"markdown","d4c425ab":"markdown","8766fc9e":"markdown","bda45f5b":"markdown","0c97733b":"markdown","8c5c2288":"markdown","dde62e3d":"markdown","afd8b2e6":"markdown","4f388612":"markdown","c9c2ff2d":"markdown","3f5d75d7":"markdown","32a3a83d":"markdown","fedc8828":"markdown","024cf51e":"markdown","725076d7":"markdown","005538af":"markdown","c6c0e236":"markdown","10cce669":"markdown","f1090437":"markdown","cbadef0e":"markdown","2fcbd670":"markdown","e22b75fb":"markdown","a756e476":"markdown","fe8e07ef":"markdown","e22dfc1e":"markdown","9f58f708":"markdown","8ef9300b":"markdown","6c179e5e":"markdown","6dcef5a6":"markdown","d083ff9d":"markdown","bf5eeee0":"markdown","b98e4654":"markdown","bd26465c":"markdown","74ec5d4e":"markdown","97eb64bb":"markdown","7c7a9bcd":"markdown","8baad188":"markdown","66779993":"markdown","463ab3f7":"markdown","bca8814b":"markdown","a3963fa0":"markdown","47056f75":"markdown","2faa7b32":"markdown","06201111":"markdown","052702d7":"markdown","45ef34ef":"markdown","d452f95b":"markdown","cf4af105":"markdown","0a25e195":"markdown","39e6d64c":"markdown","9ad6173f":"markdown","80a68b7b":"markdown","cc1a13f6":"markdown","14cc2288":"markdown","a6ac8f67":"markdown","72bccecb":"markdown","26c05a7a":"markdown","061da1dc":"markdown","f6cb48d1":"markdown","217bb145":"markdown"},"source":{"fef95ac3":"import pandas as pd","75827a13":"import numpy as np\nimport re as re #regular expressions","3f0f5539":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aa21f372":"%xmode Verbose","b13ae5c8":"traindata = pd.read_csv ('\/kaggle\/input\/titanic\/train.csv')","e57987eb":"traindata","b6cca041":"testdata = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","f667edbb":"testdata","d88df466":"genderdata = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","180824aa":"genderdata","8847aba8":"combine = (traindata, testdata)","37d371f8":"combine","8bf98073":"import matplotlib.pyplot as plt\nimport seaborn as sns","e6ee788d":"corrmat = traindata.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","18e019d7":"print (traindata[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())\n","f034cf67":"print (traindata[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())","ff22a82c":"print (traindata[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean())","502a1712":"print (traindata[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean())","43519bec":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 #\u2190Got to know the logic of this code\nprint (traindata[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","2be4aab0":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\nprint (traindata[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","fcbe06fa":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nprint (traindata[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())","02d7c537":"for dataset in combine:\n    dataset['Fare'] = dataset['Fare'].fillna(traindata['Fare'].median())\ntraindata['CategoricalFare'] = pd.qcut(traindata['Fare'], 4)\nprint (traindata[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())","68d423a7":"for dataset in combine:\n    age_avg \t   = dataset['Age'].mean()\n    age_std \t   = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntraindata['CategoricalAge'] = pd.cut(traindata['Age'], 5)\n\nprint (traindata[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())","7ad583c5":"def get_title(name):\n\ttitle_search = re.search(' ([A-Za-z]+)\\.', name)\n\t# If the title exists, extract and return it.\n\tif title_search:\n\t\treturn title_search.group(1)\n\treturn \"\"\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\nprint(pd.crosstab(traindata['Title'], traindata['Sex']))","cf93e5d6":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nprint (traindata[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())","7f456736":"for dataset in combine:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4\n\n# Feature Selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n                 'Parch', 'FamilySize']\ntrain = traindata.drop(drop_elements, axis = 1)\ntrain = traindata.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n\ntest  = testdata.drop(drop_elements, axis = 1)\n\nprint (traindata.head(10))\n\ntrain = traindata.values\ntest  = testdata.values\n","17a569be":"# \u2191Statistical Analysis","f1ca1f10":"\u2191That means, having a family size of 4 has the highest survival rate with a staggering 72%.","c4176833":"# \u2191Code Analysis","5b92df5b":"\"Parch\" means Parent\/Children. For example, if a person is with a parent or his child\/children, the number of people that he is with is listed. If his relationship with that\/those person\/s is his parent or child\/ren, it will be listed in this category. ","af05ada6":"\u2191And apparently, being female is way more favorable than being a man.","938f034a":"# \u2705Age also has missing values so it has to be fixed.","05a5e0b6":"\u2191This is as simple as, if the fare is more expensive, that means higher survival rate.","a48492c6":"Note to everyone: Here, I used my own Jupyter Notebook to tackle this data analysis.\n\n\n\nIn order to read this project carefully, there are explanations in every code, statistical result, and environment setup, which are:\n\n\u2192Environmental Analysis - the explanations of the libraries and modules used to analyze data.\n\n\u2192Statistical Analysis - the explanations of results that appear statistically. This is the raw analyzation of the data itself.\n\n\u2192Code Analysis - this is the explanation behind every code used for achieving the desired output.","b1f63d7f":"The gender data was imported here. The info that was given is the passender id column and if they survived. This dataset was organized to work as a guide to know if the created machine learning predictions will be accurate.","fd8f16e4":"# \u2191Statistical Analysis","0d5f2667":"The libraries that were first imported are numpy, and pandas. Pandas is a data manipulation tool that is pre-installed already in Python 3, although some older versions require a separate installation. According to learnpython.org, \"Pandas is a high-level data manipulation tool developed by Wes McKinney. It is built on the Numpy package and its key data structure is called the DataFrame. DataFrames allow you to store and manipulate tabular data in rows of observations and columns of variables.\" For more info about pandas, visit pandas.pydata.org.\n\n\nNext library that was imported is Numpy and it is used for numerical computing. See https:\/\/numpy.org\/about\/ for more info.\n\nRe is a Python module that allows you to work with Regular Expressions or RegEx. RegEx is a sequence of characters that defines a search pattern(https:\/\/www.programiz.com\/python-programming\/regex). This will be used later when searching for titles of the passengers of Titanic.\n\nThe %xmode Verbose is a python magic function that allows you to debug easily, especially if there will be errors in the code that you're doing. Reasons for errors will easily appear especifically when using Verbose, which adds some extra information, including the arguments to any functions that are called (https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/01.06-errors-and-debugging.html).\n\nAccording to geekforgeeks.com, \"The OS module in python provides functions for interacting with the operating system. OS, comes under Python\u2019s standard utility modules. This module provides a portable way of using operating system dependent functionality. The *os* and *os.path* modules include many functions to interact with the file system.\" For more info, look at https:\/\/www.geeksforgeeks.org\/os-module-python-examples\/.","d9362477":"\u2193\u2193The feature 'fare'  has a missing value, so we will handle it.","0e3dcead":"# \u2705Combining SibSp and Parch to create Family Size","5a6c6031":"For this test dataset, the code logic is similar to what was used in the train data.","5538e411":"# \u2705PClass and Survived Correlation","dfee3c8b":"# \u2191Code Analysis","d352e4aa":"The first code which is dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int) is used to transform the binomial category male and female to a numerical variable which is 0 to 1. According to geeksforgeeks.com, \"map() function returns a map object(which is an iterator) of the results after applying the given function to each item of a given iterable (list, tuple etc.)\". The .astype() allows you to input a pandas object (Or the dataset that you used, pandas is the library for it) to a dtype that you want. For more info about the .astype(), check this explanation at https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-astype\/\n\n\nThe second code is all about mapping the titles, turning them into numerical values. The titles Mr, Miss, Mrs, Master, and Rare are turned into 1, 2, 3, 4, 5 respectively. And the .fillna() was used to fill out missing values. \n\nThe third code logic is just the same as the first code.\n\nIn the 4th code, which is the mapping of the fare, the .loc() was used to show the numerical outputs of the fare category. As you can see, numerical values were attributed to the fares. Less than or equal 7.91 dollars = 0, between 7.91 - 14.454 dollars = 1, between  14.454 - 31 dollars = 2, and 31 and above dollars = 3. The .astype() was assigned to integer so that the output will be an integer.\n\nThe same code logic applies to the 5th code coming from the 4th one. However, the one that is manipulated here was the 'Age' category.\n\nIn the feature selection code, the .drop() was used to remove some unimportant variables. 'PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'FamilySize' are the variables that were removed. I think these are irrelevant in the Machine Learning model. ","394071ff":"With regards to the code logic, again it's the same thing as the former codes above it.","a5a90539":"\u2191I think this code means the feature 'IsAlone' was created from the dataset 'FamilySize'. By using the .loc function, all of the data that is in 'FamilySize' that are considered alone on the Titanic trip was acquired. And then, the .groupby() was used to group the 'IsAlone' feature to the 'Survived' feature, which is the same code logic above it that corresponds with the .groupby().","f5a5f4ce":"\u2193Now, let's analyze the correlation of the survivors to the other variables of the train data by creating a heatmap. To do that, let us import matplotlib and seaborn.","5d7b8626":"# \u2191Code Analysis","670557f1":"# \u2705Quick Correlation Analysis","2402fda9":"# \u2705Now, in working on the 'name' feature, this code below will show the title of the passengers.","9351024a":"# \u2191Statistical Analysis","cc1bd4a7":"# \u2191Statistical Analysis","14d516f9":"# \u2191No statistical analysis needed above because the dataset was just mapped to numerical features for machine learning purposes.","63e2102d":"# \u2191Statistical Analysis","c1f44401":"# \u2705Feature Engineering","1f2c79ac":"# \u2705 Feature Engineering by the usage of .groupby()","cc98f382":"# \u2705Test Data","bcc11326":"\u2191People that embarked on Cherbourg has the highest survival rate of 55%. 38% if from Queenstown, and it's not good to embark in Southampton, only 34% chance of survival in that port of embarkation.","ee4435f7":"\u2191The code started by creating a function as you can see in the def header. And then below the header, there is this code title_search = re.search(' ([A-Za-z]+)\\.', name). The re.search() is a regular expression and it is used to match text patterns. According to https:\/\/www.w3schools.com\/python\/python_regex.asp, \"re.search() Returns a Match object if there is a match anywhere in the string\". \n\nThe code return title_search.group(1) was used to retrieve the captured string. More info about it at https:\/\/www.xspdf.com\/help\/50186815.html\n\nThe .apply() was used in the code dataset['Title'] = dataset['Name'].apply(get_title) .\"Pandas.apply allow the users to pass a function and apply it on every single value of the Pandas series. It comes as a huge improvement for the pandas library as this function helps to segregate data according to the conditions required due to which it is efficiently used in data science and machine learning\"(https:\/\/www.geeksforgeeks.org\/python-pandas-apply\/). \n\nThe pd.crosstab() was used in the very last code here print(pd.crosstab(traindata['Title'], traindata['Sex'])) . This method is used to compute a simple cross-tabulation of two (or more) factors. By default, computes a frequency table of the factors unless an array of values and an aggregation function are passed (https:\/\/www.geeksforgeeks.org\/pandas-crosstab-function-in-python\/). \n","78958e8a":"The datasets were combined just to see the semi-full overview of the original dataset.","6e43234e":"# \u2191Environmental Analysis","f5dfe778":"I just have to note this one. Upon checking datasets in Kaggle that has most votes, what they mostly did is feature engineering, what is it? This is where you will categorize, analyze, fix, and clean the features or variables of your dataset (Just my own explanation, you can search other explanations on your own). Aside from that, missing values was also worked on. Very few regression analysis was done in this dataset, unlike the House Prices dataset here in Kaggle that is mostly regression. That means, it's time for feature engineering.","af6d6ac1":"# \ud83d\udcaf The goal of this project","3e12ca6b":"# \u2705Embarked and Survived Correlation","b8c5c0db":"# \u2191Statistical Analysis","830f90b0":"Again, the test data is the one that will be used for prediction that's why the survived column is not present. A good practice here is to examine the data first on excel and try to do a short scan on it.","76fa9277":"# \u2191Statistical Analysis","119aeb61":"# \u2191Code Analysis","2e39dfb7":"\u2191 Look at the output above, Pclass means the ticket class, I think this is the equivalent of the 1st class, business class, and economy class in an airplane. As you can see, the mean or average of the survivors per Pclass was shown here and we can infer that Pclass 1 has the most survivors with almost 63%. Pclass 2 has the second highest survival rate, and being in the Pclass 3 is quite unfortunate, as you can see in the mean. ","33fdcdb1":"The .corr() was used for correlation purposes. According to geekforgeeks.com, \"Pandas dataframe.corr() is used to find the pairwise correlation of all columns in the dataframe.\" For more info about the .corr(), check it here https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-corr\/. \n\nNext, the plt.subplots was used to create the plot that you are seeing above, with the figsize (figure size) = 12 by 9.\n\nAnd the last code the sns.heatmap which is to make the plot a heatmap. Inside the parenthesis, the variable corrmat was inputted, the vmax is the measurement of the boxes, and square=True means that it will have a square shape.","d152cecc":"Now, the code showed all the Titles of people that were aboard on the ship. The highest title is Mr., which is 517 total. Followed by Miss, which is 182, and the 3rd one which comprises of 125 people is the title Mrs. Some notable title like the 'Master' comprises of 40 males. Others are very few like Sir,  Rev, Dr., and we can regard them as the Elites. Of course, the Capt is 1 because he is the captain of the ship.","24627f39":"# \u2191Statistical Analysis","a9cbdee1":"# \u2705Train Data","d4c425ab":"# \u2705Data Cleaning","8766fc9e":"The title \"Mrs.\" has a staggering 79% survival rate followed by \"Miss\" which entails 70%. The next one would be the title \"Master\" with a survival rate that is almost 58%. Being in the title \"Rare\" means the titles that has few people in it garnered 35% survival rate. And having the title of \"Mr.\" is quite undesirable because of the 16% survival rate. Maybe because they are the ones who sacrificed their lives in this tragedy or they are the least priority when the rescue arrived.","bda45f5b":"\n\n\nAccording to sinakhorami, it is better to combine the SibSp and Parch because SibSp stands for Sibling\/Spouse and Parch stands for Parent\/Children. When these two features\/variables are combined, they can be called \"Family Size\"","0c97733b":"\u2193Now, time to clean the data and map features into numerical variables. We do this so the data is prepared for machine learning.","8c5c2288":"# \u2191Statistical Analysis","dde62e3d":"# \u2191Code Analysis","afd8b2e6":"# \u2191Statistical Analysis","4f388612":"# \u2191Code Analysis","c9c2ff2d":"# \u2191Statistical Analysis","3f5d75d7":"\u2193In doing this project, it is a must to set up the environment first, which means importing the relevant libraries. ","32a3a83d":"# \u2705 SibSp and Survived Correlation","fedc8828":"# \u2191Code Analysis","024cf51e":"# \u2191Statistical Analysis","725076d7":"Let's proceed to the analysis of the train dataset. If you're going to look at the legend on the right side, the lightest color means the most correlated to the variable that we are going to look at. And the variable that we are going to focus on is 'Survived'. Basing on the observation, the lightest color that is on the same row of 'Survived' is 'Fare'. Now, from this, we can infer that passenger fare has a quite a big correlation to the survival rate of the passengers. The next nearest variable that has a slight correlation is 'Parch', which means the number of Parent and Children Aboard. The other variables seem to have very low correlation. In line with that, I still have to know why the columns 'Sex', 'Cabin' and 'Embarked' are not included in this heatmap.","005538af":"\u2191Here, the median of the 'Fare' category was acquired by first, filling values with no data in the 'Fare' category by using .fillna() and second, by using the .median() function. ","c6c0e236":"# \u2191Code Analysis","10cce669":"\u2191SibSp means siblings and spouse. Now, if you're going to look at the data, having a spouse or 1 sibling gives the highest survival rate, and it gets so low when you have more people with you. It's not good to be alone also in this tragedy as you can see that people that are by themselves only has a 35% survival rate.","f1090437":"# \u2191Code Analysis","cbadef0e":"The inspiration of this code is this, https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier. ","2fcbd670":"That's it for my simple analysis. Again, this will be updated with a Machine Learning code as soon as possible. ","e22b75fb":"# \u2191Code Analysis","a756e476":"The goal of this project is to predict what sorts of people survived in the Titanic tragedy on a separate but connected dataset which is from the past record of the Titanic passenger dataset.","fe8e07ef":"# \ud83d\udcaf Thank you for reading this code! And please upvote, thanks!\ud83d\udcaf","e22dfc1e":"# \u2193Like any other data analysis, we always need to have an overview of the data first.","9f58f708":"\u2191The next line of code which is the traindata['CategoricalFare'] = pd.qcut(traindata['Fare'], 4) will be explained here. From the traindata, a new feature which is 'Categorical Fare' was created. The pd.qcut() was used to bin the categorical datasets into 4 segregations or bins. Binning is organizing a dataset and segregating them based on their characteristics. You can look at Data Binning at https:\/\/docs.tibco.com\/pub\/spotfire\/7.0.1\/doc\/html\/bin\/bin_what_is_binning.htm#:~:text=Binning%20is%20a%20way%20to,smaller%20number%20of%20age%20intervals. or https:\/\/en.wikipedia.org\/wiki\/Data_binning","8ef9300b":"# \u2191Code Analysis:","6c179e5e":"A person that is by himself has a survival rate of 30% and if he is with someone else, atleast 51% survival chance.","6dcef5a6":"\u2191Again, the code has the same logic.","d083ff9d":"According to geekforgeeks.com, \".replace() is an inbuilt function in Python programming language that returns a copy of the string where all occurrences of a substring is replaced with another substring.\" In short, the substrings, or the categories\/words that you see in the code, some of it will be replaced for another substring. The string in the dataset is 'Title', and the substrings are 'Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Mlle', 'Ms', 'Mme', 'Miss', and 'Rare'.\n\n\n'Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona' were replaced with 'Rare'. 'Mlle' and 'Ms' were replaced by Miss, and 'Mme' was replaced by Mrs.\n\nThis was done to organize the titles and to correct some title misspelling.","bf5eeee0":"\u2191The 'Embarked' feature means port of embarkation. The letters C, Q, and S corresponds to the places where the ship embarked or go on board. C means that the ship emarked at Cherbourg, Q in Queenstown, and S in Southampton. ","b98e4654":"In the code above, we used a process called .groupby(). According to pandas.pydata.org, \"By \u201cgroup by\u201d we are referring to a process involving one or more of the following steps:\n\n1. Splitting the data into groups based on some criteria.\n\n2. Applying a function to each group independently.\n\n3. Combining the results into a data structure.\"\n\nReference at https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/groupby.html . Check the site for more info about the groupby process.\n\nAt the same time, we are looking at the mean or average values of the \"Survived\" category by using the .mean().\n\nThe .groupby() here combines the P class and Survived and the meaning of as_index=False is that Pclass is not intended to be the index. For more info, look at https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-groupby\/.","bd26465c":"# \u2191Statistical Analysis","74ec5d4e":"# \u2191Statistical Analysis","97eb64bb":"# \u2705Handling of Missing Values","7c7a9bcd":"# \u2705Now, let's see if a person's title has a correlation to his survival.","8baad188":"The data that was imported is the train data. The train data is the dataset that will be used for analyzation and feature engineering. Here you can see that there is the 'Survived' category and later you will see in the test dataset that this category doesn't appear. Why is that? It's because the test dataset is the one that will be used for the Machine Learning model, that is the dataset where the prediction of who and what type of people can survive in the tragedy.\n\nWhat you can also observe here is that the train data is the first half of the full Titanic data and the test data is the 2nd half. Machine learning will not make sense if the the dataset wasn't cut into half.\n\nNow, what are your observations in the train data above? Observe it for a minute and then get back here and continue reading.\n\n\u2192Observation 1: For me, one of my observations is that the whole data is not shown. It started from row 0 - row 4 and skipped all the rows obviously present and immediately jumped right into row 886 - row 890. This is one of the limitation of the usage of pandas. That's why it is advisable to look at the file in MS Excel first.\n\n\u2192Observation 2: Started at row 0. If you're still a beginner in python, this just means that python counts from 0 onwards.\n\n\u2192Observation 3: What does 0 and 1 mean in the 'Survived' column? For those who still don't know, 0 means the person did not survive and 1 means the person survived. This is considered as a binomial category. For more info about this, check https:\/\/stattrek.com\/probability-distributions\/binomial.aspx#:~:text=A%20binomial%20experiment%20is%20a,and%20the%20other%2C%20a%20failure.\n\n\u2192Observation 4: Are there categories from the dataset that seem unfamiliar to you? Of course, the category name, age, and sex is very obvious already. But what about the others? Here, you can see the meanings of the categories at https:\/\/www.kaggle.com\/c\/titanic\/data. And these categories are also called 'Features'. Thus they are highly involved in Feature Engineering.\n\n","66779993":"This is one of the \"powers\" of pandas, which is importing excel files here in the notebook. traindata is the variable name and pd.read_csv() is used to import the train data file here. As you can see, \/kaggle\/input\/titanic\/train.csv' was the file location. You have to input the file location so that pandas can import it here.","463ab3f7":"# \u2191Code Analysis","bca8814b":"# \u2705 Parch and Survived Correlation","a3963fa0":"# \u2191Code Analysis","47056f75":"\u2191Lastly, in the line of code where the print function is located, same code logic from the preceding codes.","2faa7b32":"# \u2191Code Analysis","06201111":"Here, we are going to check the survival rate of each subcategories of every features in the dataset. To do that, the code .groupby() was used","052702d7":"\u2191 The logic of this code above is the same as the code before it. The only thing that is different is the category. ","45ef34ef":"# \u2191Code Analysis","d452f95b":"Here, pandas was also used to import the gender dataset. And it is the same code logic from the preceeding codes.","cf4af105":"Being young means the highest possible chance of survival. People that were 16 years old and below survived 51% of the time. While the 48 - 64 year old people has a survival rate of 43%. 37% chances of survival for those age bracket between 16 - 32 years old and for the ages 32 - 48, 36% survival chance. Being old is not good for they only have 9% survival rate for those ages 64 - 80.","0a25e195":"# \u2705Combining Train and Test Data","39e6d64c":"# \u2705Now, let's check the survival rate of people that are alone and that are not.","9ad6173f":"Here, a for loop was used for iteration purposes. For more info about for loops, check this https:\/\/www.w3schools.com\/python\/python_for_loops.asp. The new feature 'Family Size' was created as a new dataset from the combination of 'SibSp' and 'Parch'. I got to know more the meaning of the + 1 though.\n\nAnd on the last code line, same code logic on the preceeding codes earlier.","80a68b7b":"# \u2191Statistical Analysis","cc1a13f6":"# \u2191Code Analysis","14cc2288":"This project is all about analyzing and predicting the survivors of Titanic based on past data. The scope of my work is all about data analysis. I will update this notebook and include a machine learning algorithm soon.","a6ac8f67":"# \u2191Statistical Analysis","72bccecb":"\u2191Here a for loop was used so that the age category of the combined train and test dataset will be acquired. The mean, standard deviation, missing values, and the sum of the ???missing values will be acquired. The corresponding code to do it is below:\n   \n    age_avg \t   = dataset['Age'].mean()\n    age_std \t   = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n\nThe next line of code here is this:\n\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \n\u2191And in this code here, np.random.randint is all about random sampling. According to geeksforgeeks.org, \"numpy.random.randint() is one of the function for doing random sampling in numpy. It returns an array of specified shape and fills it with random integers from low (inclusive) to high (exclusive), i.e. in the interval (low, high)\". I think this code tends to create a random sample of the mean - standard deviation and mean + standard deviation with this code (age_avg - age_std, age_avg + age_std, size=age_null_count). That's how I understand the code but got to deeply know this further. Programming has a lot of deep logic. \n\nNow, let's go to the second line which is the     dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list  . Here, the np.isnan is used to check whether the category 'Age' has a NaN(Not a Number) value. The logic of this code is usually, it shows boolean values (True or False). But it wasn't shown because it was combined with other codes.\n\nNow, let's head to the last line of code above, which is the     dataset['Age'] = dataset['Age'].astype(int)  . Here, the category 'Age' was just assigned as an integer due to the .astype(int) function. The .astype() allows you to input a pandas object (Or the dataset that you used, pandas is the library for it) to a dtype that you want. For more info about the .astype(), check this explanation at https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-astype\/. \n\nAnd then, the code traindata['CategoricalAge'] = pd.cut(traindata['Age'], 5) explanation is this. The category 'age' was divided into 5 segregations or bins using the pd.cut() function. The difference of pd.cut() to pd.qcut() is that pd.cut() segregates by making the bins equal but the frequency of samples unequal. The pd.qcut() segregates by making the bins unequal but the frequency of samples equal. For more info about this, refer to https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n\nAnd then the print (traindata[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean()) is the same code logic with the .groupby() above it.\n\n\n\n\n","26c05a7a":"# \u2705Sex and Survived Correlation","061da1dc":"\u2191The .fillna function is to put values on the variables on the category 'S' or Southampton. Then, on the line of the print function, the code logic was the same as the preceeding codes.","f6cb48d1":"# \u2191Code Analysis","217bb145":"One of the easiest codes because just create a list on the 2 datasets will combine them "}}