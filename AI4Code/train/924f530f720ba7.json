{"cell_type":{"c7050677":"code","808646bb":"code","75f04d5f":"code","af20ab7d":"code","4dcba33e":"code","b8ee2b90":"code","ad97ef6f":"code","5a32e940":"code","be8e68da":"code","8839f71b":"code","a04d01a1":"code","d2f53051":"code","6049448f":"code","7fcdcaea":"code","178f284e":"code","90480ec1":"code","97344da5":"code","bdf87f8b":"code","d48f2adb":"code","5d65b4ab":"code","5a258035":"code","ce14fcd3":"code","4c30e185":"code","86ad3dd1":"code","cdfa8a25":"code","6b78929e":"code","0777a75d":"code","ed14e9f3":"code","9c0aa71f":"code","93d2841d":"code","3d4bf090":"code","bdf7e2ea":"code","3451e2ee":"code","e57d4f8e":"code","be696be2":"code","b18c1c96":"code","ba03e801":"code","975bd50b":"code","29a8d18c":"code","e339bf5c":"code","0d2de624":"code","ec368b2e":"code","b779ae6d":"code","d5d8eb27":"code","953ad7e7":"code","80dc7e83":"code","cd8f06a2":"code","747c59d6":"code","3c209778":"code","1d2627f1":"code","4ceba18d":"code","cceb5e19":"code","86c49efd":"code","ef15475c":"code","3da67b6d":"code","5a834301":"code","7fb1c4a8":"code","2e52c156":"code","cbbfe756":"code","44ee5b8c":"code","eb06bb7e":"code","00654962":"code","c6439b6f":"code","2398f025":"code","0ba401f6":"code","0cd6bbdb":"code","bd59c2db":"code","39fd5097":"code","aedef205":"code","7a8872a3":"markdown","534c418b":"markdown","34de27b2":"markdown","42842a5f":"markdown","3341e605":"markdown","c23e5627":"markdown","c82158b6":"markdown","96dee989":"markdown"},"source":{"c7050677":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","808646bb":"# This dataset is about heart disease, more specifically it's about coronary artery disease and angina. I will go through this dataset. First I want to test my Python skills. Second, I want to test some ideas with machine learning. ","75f04d5f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","af20ab7d":"HD=pd.read_csv ('..\/input\/heart.csv')","4dcba33e":"HD.shape","b8ee2b90":"HD.sample(3)","ad97ef6f":"# the dataset is very clean, which makes it easy to go next step.\nHD.isnull().sum()","5a32e940":"HD.info()","be8e68da":"hd=HD.copy()","8839f71b":"# from the data, we can see that there seems no clear 'outcome' information. So test several basic features and also learn the number meanings from data description.\nprint('cp values are', hd['cp'].unique())\nprint('thal values are', hd['thal'].unique())\nprint('restecg values are', hd['restecg'].unique())\nprint('ca values are', hd['ca'].unique())","a04d01a1":"# to make the column name more readable, such as 'target' means how narrow the coronary artery is.\nhd=hd.rename(columns={'target': 'narrow', 'thal': 'duration', 'thalach':'max_HR', 'fbs':'high_fasting_sugar', 'exang':'exercise_angina','ca': 'coronary_num'})","d2f53051":"hd.sample(3)","6049448f":"# Correlation test\nsns.heatmap(hd.corr(),annot=True,cmap='RdYlGn') \nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show","7fcdcaea":"# two factors correlation\nfig, ax=plt.subplots( figsize=(6,6))\nsns.regplot(x=\"cp\", y=\"age\", data=hd,)","178f284e":"sns.distplot(HD[\"ca\"] )","90480ec1":"# distribution. \nfig, ax=plt.subplots(1,3, figsize=(15,5))\nsns.kdeplot(hd['age'], ax=ax[0])\nsns.kdeplot(hd['trestbps'], ax=ax[1])\nsns.kdeplot(hd['max_HR'], ax=ax[2])","97344da5":"from sklearn.ensemble import RandomForestClassifier","bdf87f8b":"# I set chest pain as the disease outcome\nx=hd.drop(['cp'] , axis=1)\ny=hd['cp']","d48f2adb":"# look at which features are important for the 'Outcome'\nmodel= RandomForestClassifier(n_estimators=100,random_state=0)\nmodel.fit(x,y)\npd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)\n\n#remember that 'max_HR' has the highest score","5d65b4ab":"# Since we already set the 'outcome', some supervised models are considered\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","5a258035":"Xtrain,Xtest, Ytrain, Ytest=train_test_split(x, y, test_size=0.2,random_state=42,stratify=y) # split data to train and test parts","ce14fcd3":"# evaluate the accuracies of the models\nabc=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    model.fit(Xtrain,Ytrain)\n    prediction=model.predict(Xtest)\n    abc.append(metrics.accuracy_score(prediction,Ytest))\n\nmodels_dataframe=pd.DataFrame(abc,index=classifiers)   \nmodels_dataframe","4c30e185":"# Notice that the highest score is only 0.6. Based on the feature score list, to 8 features have scores above 0.5. To see whether these important features will produce more accurate prediction\nhd_8=hd.loc[:, ['max_HR', 'chol', 'age', 'trestbps', 'oldpeak', 'narrow', 'exercise_angina', 'coronary_num']]\nXtrain,Xtest, Ytrain, Ytest=train_test_split(hd_8, y, test_size=0.2,random_state=42,stratify=y)\nabc8=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    model.fit(Xtrain,Ytrain)\n    prediction=model.predict(Xtest)\n    abc8.append(metrics.accuracy_score(prediction,Ytest))\n\nmodels_df8=pd.DataFrame(abc8,index=classifiers)   \nmodels_df8","86ad3dd1":"# surprisingly, top 8 important features are less accurate than the total features. How about the top 4?\nhd_4=hd.loc[:, ['max_HR', 'chol', 'age', 'trestbps']]\nXtrain,Xtest, Ytrain, Ytest=train_test_split(hd_4, y, test_size=0.2,random_state=42,stratify=y)\nabc4=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    model.fit(Xtrain,Ytrain)\n    prediction=model.predict(Xtest)\n    abc4.append(metrics.accuracy_score(prediction,Ytest))\n\nmodels_df4=pd.DataFrame(abc4,index=classifiers)   \nmodels_df4","cdfa8a25":"# Even worse!","6b78929e":"# Maybe standarlization could help?\nfrom sklearn.preprocessing import StandardScaler","0777a75d":"features=x[x.columns[:13]]\nfeatures_standard=StandardScaler().fit_transform(features)","ed14e9f3":"hd_std=pd.DataFrame(features_standard,columns=[[  'age', 'sex', 'trestbps', 'chol', 'high_fasting_sugar', 'restecg', 'max_HR','exercise_angina', 'oldpeak', 'slope', 'coronary_num','duration','narrow' ]])\n","9c0aa71f":"hd_std.head(1)","93d2841d":"# try again with standarlizaed data","3d4bf090":"Xtrain,Xtest, Ytrain, Ytest=train_test_split(hd_std, y, test_size=0.2,random_state=42,stratify=y)\nabc_std=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    model.fit(Xtrain,Ytrain)\n    prediction=model.predict(Xtest)\n    abc_std.append(metrics.accuracy_score(prediction,Ytest))\n\nmodels_df_std=pd.DataFrame(abc_std,index=classifiers)   \nmodels_df_std","bdf7e2ea":"hd_8std=hd_std.loc[:, ['max_HR', 'chol', 'age', 'trestbps', 'oldpeak', 'narrow', 'exercise_angina', 'coronary_num']]\nXtrain,Xtest, Ytrain, Ytest=train_test_split(hd_8std, y, test_size=0.2,random_state=42,stratify=y)\nabc_8std=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    model.fit(Xtrain,Ytrain)\n    prediction=model.predict(Xtest)\n    abc_8std.append(metrics.accuracy_score(prediction,Ytest))\n\nmodels_df_8std=pd.DataFrame(abc_8std,index=classifiers)   \nmodels_df_8std","3451e2ee":"# For other models, it seems improve a little. For the LInear Svm, not better.\n# So look back at the data, whether we miss something?\n# For 'cp' column, it records chest pain type. Number 3 mean no chest pain, number 0-2 means different tyoe of angina. So is it possible that this kind of recording makes the problem too complicated?\n# To simplify it, I will group the number0-2 together as disease positive, number 3 as disease negative","e57d4f8e":"hd['cp'].isin([0,1,2]).value_counts()","be696be2":"# The dataset contain 23 ' disease negative' patient","b18c1c96":"hd_mod=HD.copy()","ba03e801":"number=[0,1,2]\nfor col in hd.itertuples():\n\n    if col.cp in number:\n        hd_mod['cp'].replace(to_replace=col.cp, value=1, inplace=True)\n","975bd50b":"hd_mod['cp'].value_counts()","29a8d18c":"y_mod=hd_mod['cp']","e339bf5c":"x.sample(3)","0d2de624":"model= RandomForestClassifier(n_estimators=100,random_state=0)\nmodel.fit(x,y_mod)\npd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)","ec368b2e":"# Notice that after this change, the feature with highest score became 'the rest blood pressure', instead of 'max_HR'","b779ae6d":"hd_std.sample(3)","d5d8eb27":"Xtrain,Xtest, Ytrain, Ytest=train_test_split(hd_std, y_mod, test_size=0.2,random_state=42,stratify=y)\nabc_std=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    model.fit(Xtrain,Ytrain)\n    prediction=model.predict(Xtest)\n    abc_std.append(metrics.accuracy_score(prediction,Ytest))\n\nmodels_df_std=pd.DataFrame(abc_std,index=classifiers)   \nmodels_df_std","953ad7e7":"# Wow! The accuracy is more than 90%?!\n# IS that too good to be true?\n# we look at the percentage of so called 'disease positve' patient, after we grouped number 0-2 together","80dc7e83":"280\/303","cd8f06a2":"# the patient percentage is 92%. That means the model just keep saying the prediction is '1', it still has more than 90% to be correct...\n# Let's do a test","747c59d6":"hd.head(1)","3c209778":"model=svm.SVC(kernel='linear')\nmodel.fit(Xtrain,Ytrain)\nprint(model.predict([[0.952197,\n0.681005,\n0.763956,\n-0.256334,\n2.394438,\n-1.005832,\n0.015443,\n-0.696631,\n1.087338,\n-2.274579,\n-0.714429,\n-2.148873,\n0.914529]]))","1d2627f1":"# Use the row 1 data to do the test and the answer should be 3, but the prediction is 1...as I thought.\n# So go back to look at the data more carefully..","4ceba18d":"sns.violinplot(x=HD['cp'], y=HD['target'])","cceb5e19":"# Typical angina usually have less than 50% narrow artery.","86c49efd":"sns.violinplot(x=HD['cp'], y=HD['thalach'])","ef15475c":"sns.violinplot(x=HD['cp'], y=HD['age'])","3da67b6d":"# look at data with 'cp' grouped","5a834301":"sns.violinplot(x=hd_mod['cp'], y=hd_mod['chol'])","7fb1c4a8":"sns.violinplot(x=hd_mod['cp'], y=hd_mod['thalach'])","2e52c156":"sns.violinplot(x=hd_mod['cp'], y=hd_mod['ca'])","cbbfe756":"# first check the value of 'narrow' column\nhd['narrow'].value_counts()","44ee5b8c":"hd_narrow=HD.copy()","eb06bb7e":"hd_narrow=hd_narrow.rename(columns={'target': 'narrow','pred_attribute': 'narrow', 'thal': 'duration', 'thalach':'max_HR', 'fbs':'high_fasting_sugar', 'exang':'exercise_angina','ca': 'coronary_num'})","00654962":"x_narrow=hd_narrow.drop(['narrow'] , axis=1)\ny_narrow=hd_narrow['narrow']","c6439b6f":"model= RandomForestClassifier(n_estimators=100,random_state=0)\nmodel.fit(x_narrow,y_narrow)\npd.Series(model.feature_importances_,index=x_narrow.columns).sort_values(ascending=False)","2398f025":"Xtrain_n,Xtest_n, Ytrain_n, Ytest_n=train_test_split(x_narrow, y_narrow, test_size=0.2,random_state=42,stratify=y_narrow)\nabc_narrow=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    model.fit(Xtrain_n,Ytrain_n)\n    prediction=model.predict(Xtest_n)\n    abc_narrow.append(metrics.accuracy_score(prediction,Ytest_n))\n\nmodels_df_narrow=pd.DataFrame(abc_narrow,index=classifiers, columns=['Score'])   \nmodels_df_narrow","0ba401f6":"model=svm.SVC(kernel='linear')\nmodel.fit(Xtrain_n,Ytrain_n)\nprint(model.predict([[61,\n1,\n0,\n140,\n207,\n0,\n0,\n138,\n1,\n1.9,\n2,\n1,\n3]]))","0cd6bbdb":"model=svm.SVC(kernel='linear')\nmodel.fit(Xtrain_n,Ytrain_n)\nprint(model.predict([[43,\n1,\n0,\n110,\n211,\n0,\n1,\n161,\n0,\n0.0,\n2,\n0,\n3]]))","bd59c2db":"hd_8nar=hd_narrow.loc[:, ['max_HR', 'chol', 'age', 'trestbps', 'oldpeak', 'cp', 'duration', 'coronary_num']]\nXtrain,Xtest, Ytrain, Ytest=train_test_split(hd_8nar, y_narrow, test_size=0.2,random_state=42,stratify=y_narrow)\nabc_8nar=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    model.fit(Xtrain,Ytrain)\n    prediction=model.predict(Xtest)\n    abc_8nar.append(metrics.accuracy_score(prediction,Ytest))\n\nmodels_df_8nar=pd.DataFrame(abc_8nar,index=classifiers, columns=['Scores'])   \nmodels_df_8nar","39fd5097":"fig, ax=plt.subplots(1, 4, figsize=(25,5))\nsns.violinplot(x=\"narrow\", y=\"cp\", data=hd_narrow, ax=ax[0])\nax[0].set_xlabel('narrow', fontsize=20)\nax[0].set_ylabel('cp', fontsize=20)\nsns.violinplot(x='narrow', y='oldpeak', data=hd_narrow, ax=ax[1])\nax[1].set_xlabel('narrow', fontsize=20)\nax[1].set_ylabel('ST depression', fontsize=20)\nsns.violinplot(x='narrow', y='duration', data=hd_narrow, ax=ax[2])\nax[2].set_xlabel('narrow', fontsize=20)\nax[2].set_ylabel('duration of excercise', fontsize=20)\nsns.violinplot(x='narrow', y='max_HR', data=hd_narrow, ax=ax[3])\nax[3].set_xlabel('narrow', fontsize=20)\nax[3].set_ylabel('max_HR', fontsize=20)","aedef205":"# I want to thank Kaggle for providing the wonderful place to learn and to communicate. Many code in here are learned from other kaggle members, such as I,coder.","7a8872a3":"This time the top important values are very different with previous one that 'chest pain' was set as the outcome.","534c418b":"Bingo! Try another one. Record 113. The 'narrow' number should be 1.... even though the 'cp' number is the same as Record 284","34de27b2":"very nice! How about the old question: top 8 features make more accurate prediction?","42842a5f":"From the graphs, we can tell the distribution differences between two groups of Narrow feature. So finally we have a reasonable perspective to read this dataset.\n\n","3341e605":"Good! This parameter contain half-half amount of two values. Let's try it again!","c23e5627":"This time, the highest score is over 0.8! That means if we set the 'artery narrowness' as the outcome, it makes the whole data predictable!\nLet's try some examples and see whether the model is reliable.\nfirst, number 284 record, the 'narrow' number should be 0....","c82158b6":"Let's take a look at the data from another angle. If the chest pain type is not a proper outcome feature, then how about asking\" the relationship of coronary artery narrowness with other factors\", because the narrow level of the coronary artery will determine the prognosis of the disease.","96dee989":"not improved at all! So more features, better prediction. Lets look at the relationships of those top features with the 'narrow' feature. "}}