{"cell_type":{"14c58f66":"code","4ce49477":"code","a9466ba4":"code","6b4001eb":"code","89e97fcd":"code","bc72b07a":"code","7f30280b":"code","2ab25e10":"code","f24b6b93":"code","c0781b49":"code","220f7172":"code","2bb98617":"code","7f2568b4":"code","42f49262":"code","497e2645":"code","bd5177af":"code","f83fec4f":"markdown","32ff656d":"markdown","7daffa80":"markdown","c3254264":"markdown","6a97c147":"markdown","8b7b3fe1":"markdown","a8a092b2":"markdown","8527218e":"markdown","e8cbeb57":"markdown","399873a6":"markdown","ad5c5453":"markdown","2280d362":"markdown","9e137752":"markdown"},"source":{"14c58f66":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf  \nfrom tensorflow import keras  \nfrom tensorflow.keras import Sequential \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense\nimport matplotlib.pyplot as plt\n","4ce49477":"#get the train and test data from the below link respectively\nbank_note_data = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/bank_note_data\/training_set_label.csv\" )\ntest_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/bank_note_data\/testing_set_label.csv')","a9466ba4":"bank_note_data.head()","6b4001eb":"X = bank_note_data.drop('Class', axis=1) #Input variables\n# axis=1 indicates that a column will be dropped\ny = bank_note_data['Class']  # Target variable","89e97fcd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX=pd.DataFrame(scaler.fit_transform(X),columns=X.columns)\ntest_data=pd.DataFrame(scaler.fit_transform(test_data),columns=test_data.columns)","bc72b07a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","7f30280b":"# Building the model\nmodel = Sequential()\nmodel.add(Dense(8, activation='relu', input_shape=(X_train.shape[1],)))   \nmodel.add(Dense(1, activation='sigmoid'))","2ab25e10":"# Compiling the model\nfrom tensorflow.keras.optimizers import Adam\nmodel.compile(loss='binary_crossentropy', optimizer= 'adam' , metrics=['accuracy'])","f24b6b93":"history = model.fit(X_train, y_train, validation_split=0.2, epochs=200, batch_size=10, verbose=1)","c0781b49":"model.evaluate(X_test, y_test)","220f7172":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'])\nplt.show()","2bb98617":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'])\nplt.show()","7f2568b4":"predictions=model.predict(test_data)","42f49262":"predictions","497e2645":"x = np.round(predictions)\nx","bd5177af":"res = pd.DataFrame(x) \nres.index = test_data.index \nres.columns = [\"prediction\"]\nres.to_csv(\"Banknote_submitted.csv\")\n","f83fec4f":"### Fitting the model with the validation data of 20%  for 200 epocs and batch size of 10 ","32ff656d":"### Using the above model to predict the unseen data","7daffa80":"### Building the sequential model with one input, hidden and output layer","c3254264":"### Evaluated the model and got the accuracy of 100% in validation data","6a97c147":"### Standardizing all the numerical column values in both training and testing data","8b7b3fe1":"### plotting the accuracy and loss curves for training and validation data","a8a092b2":"# BANKNOTE AUTHENTICATION\nProblem to solve : To Predict whether the note is genuine or not","8527218e":"### Libraries to import","e8cbeb57":"### Achieved 100% accuracy in test data set as well","399873a6":"### Assigning input and Output variables to X and y respectively","ad5c5453":"### Spliting the data into 80\/20 ratio for training and testing","2280d362":"### Compling the model with binary_crossentrophy loss and adam optimizer","9e137752":"### Reading data from csv file using pandas read_csv function"}}