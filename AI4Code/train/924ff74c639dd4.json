{"cell_type":{"b7100cc2":"code","3b383ffb":"code","d761f20c":"code","a43e19b0":"code","8a61cf71":"code","b97368d3":"code","a75d33dd":"code","d3fe5d7d":"code","a17efbb9":"code","b9aa7516":"code","67cb30de":"code","39044aef":"code","35521bdb":"code","b1517480":"code","a6b4b552":"code","edeaabc6":"code","c53b16e9":"code","87ad3389":"code","e6d38645":"code","034d5ef1":"code","44bb3be1":"code","6f5dd566":"code","5d685b75":"code","ed169a1a":"code","deed9c45":"code","8eb48e23":"code","a263cffa":"code","e811651a":"code","5633ddac":"code","95844350":"code","87b853a0":"code","dfbed340":"code","907c49de":"code","5b3d6097":"code","44b4d459":"code","3bff47e7":"code","c035f1cf":"code","fa11a9f2":"code","aa19fb59":"code","cc0ca543":"code","475db4c3":"code","f7107f09":"code","bb489782":"code","ea61d3c9":"code","2c98ad4d":"code","99dfe8c5":"code","03d981eb":"code","a84b8e3b":"code","885ce9dc":"code","6eb42361":"code","70e89c10":"code","98b90b8f":"code","60bd6491":"code","2928bb1f":"code","0f830173":"code","cf06b506":"code","e44e3170":"code","2e51c131":"code","c660f7a9":"code","408da528":"code","022176f4":"markdown","5b5bc463":"markdown"},"source":{"b7100cc2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly import tools\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('max_columns',150)\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b383ffb":"dataset = pd.read_csv('..\/input\/lending-club-loan-data\/loan.csv')\n\noriginal_df = dataset.copy()","d761f20c":"dataset.info()","a43e19b0":"dataset.head()","8a61cf71":"dataset = dataset.rename(columns={\"loan_amnt\": \"loan_amount\", \"funded_amnt\": \"funded_amount\", \"funded_amnt_inv\": \"investor_funds\",\n                       \"int_rate\": \"interest_rate\", \"annual_inc\": \"annual_income\"})\n\n# Drop irrelevant columns\ndataset.drop(['id', 'member_id', 'emp_title', 'url', 'desc', 'zip_code', 'title'], axis=1, inplace=True)","b97368d3":"# Distribution of different type of amount\nfig , ax = plt.subplots(1,3,figsize = (12,5))\n\nloan_amount = dataset.loan_amount.values\nfunded_amount = dataset.funded_amount.values\ninvestor_funds = dataset.investor_funds.values\n\nsns.distplot(loan_amount , ax = ax[0] , color = 'blue').set_title('Loan Applied by the Borrower' , fontsize = 14)\nsns.distplot(funded_amount , ax = ax[1] , color = 'cyan').set_title('Amount funded by the Lender' , fontsize = 14)\nsns.distplot(investor_funds , ax = ax[2] , color = 'purple').set_title('Total Commited by Investors' , fontsize = 14)\n\nplt.show()","a75d33dd":"dataset.issue_d","d3fe5d7d":"dataset['year'] = dataset.issue_d.apply(lambda x:x[len(x)-4:])\ndataset['year'].value_counts()","a17efbb9":"dx = dataset.groupby(['year'])['loan_amount'].mean().reset_index()\ndx.columns = ['Year' ,'Average']\ndx","b9aa7516":"plt.figure(figsize = (15,6))\nsns.barplot(x = dx.Year , y = dx.Average).set_title('Average Loam Amount in each year')\nplt.show()","67cb30de":"dataset['loan_status'].value_counts()","39044aef":"good_loan = ['Fully Paid','Current','Does not meet the credit policy. Status:Fully Paid']\n\ndef loan_condition(status):\n    if status not in good_loan:\n        return 'Bad Loan'\n    else:\n        return 'Good Loan'\n\ndataset['Loan_Condition_x'] = dataset['loan_status'].apply(lambda x:loan_condition(x))","35521bdb":"dataset['Loan_Condition_x'].value_counts()","b1517480":"mean1 = float('%.2f' % dataset[dataset.Loan_Condition_x == 'Good Loan']['loan_amount'].mean())\nstd1 = float('%.2f' % dataset[dataset.Loan_Condition_x == 'Good Loan']['loan_amount'].std())\n\nmean2 = float('%.2f' % dataset[dataset.Loan_Condition_x == 'Bad Loan']['loan_amount'].mean())\nstd2 = float('%.2f' % dataset[dataset.Loan_Condition_x == 'Bad Loan']['loan_amount'].std())\n\nprint((mean1,std1),(mean2 , std2))","a6b4b552":"n1 = dataset[dataset.Loan_Condition_x == 'Good Loan']['loan_amount'].count()\nn2 = dataset[dataset.Loan_Condition_x == 'Bad Loan']['loan_amount'].count()\n(n1,n2)","edeaabc6":"point_estimate = mean1 - mean2\ntstar = 1.96\nstd_error = np.sqrt(std1**2\/n1 + std2**2\/n2) #Pooled Approach \nlcb = point_estimate - tstar*std_error\nucb = point_estimate + tstar*std_error\nprint('Pooled Approach ',(lcb , ucb))\nstd_error = np.sqrt((n1-1)*std1**2 + (n2-1)*std2**2)\/(n1+n2-1)\nlcb = point_estimate - tstar*std_error\nucb = point_estimate + tstar*std_error\nprint('UnPooled Approach ',(lcb , ucb))","c53b16e9":"f,ax = plt.subplots(1,2,figsize = (16,8))\n\ncolors = ['blue','red']\nlabels = ['Good Loan', 'Bad Loan']\nplt.suptitle('Information on Loan Condition',fontsize = 20)\n\ndataset['Loan_Condition_x'].value_counts().plot.pie(explode = [0,0.25], autopct = \"%1.2f%%\" , ax = ax[0],\n                                                 labels = labels , colors = colors ,fontsize = 12 , startangle = 70)\n\nax[0].set_ylabel('% of condition of Loans')\n\npalette = [\"Blue\", \"Red\"]\n\nsns.barplot(x = 'year', y = 'loan_amount',hue = 'Loan_Condition_x',data = dataset,palette = palette,\n           estimator = lambda x: len(x)\/len(dataset) * 100)\n\nax[1].set(ylabel='%')\n","87ad3389":"# Make a list with each of the regions by state.\n\nwest = ['CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID']\nsouth_west = ['AZ', 'TX', 'NM', 'OK']\nsouth_east = ['GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN' ]\nmid_west = ['IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND']\nnorth_east = ['CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME']\n\ndef regions(state):\n    if state in west:\n        return 'West'\n    elif state in south_west:\n        return 'South West'\n    elif state in south_east:\n        return 'South East'\n    elif state in mid_west:\n        return 'Mid West'\n    else:\n        return 'North East'\n    \ndataset['region'] = dataset.addr_state.apply(lambda x:regions(x))\ndataset.region.head()","e6d38645":"dataset['complete_date'] = pd.to_datetime(dataset.issue_d)\ngroup_dates = dataset.groupby(['complete_date','region'] , as_index = False).sum()\n\ngroup_dates['issue_d'] = [month.to_period('M') for month in group_dates['complete_date']]\n\ngroup_dates = group_dates.groupby(['issue_d' , 'region'] , as_index = False).sum()\n\ngroup_dates['loan_amount'] = group_dates.loan_amount\/1000\n\ndf_dates = pd.DataFrame(data = group_dates[['issue_d','region','loan_amount']])\n\ndf_dates.head()","034d5ef1":"plt.style.use('dark_background')\ncmap = plt.cm.Set3\ndf_dates.groupby(['issue_d','region'])['loan_amount'].sum().unstack().plot(figsize = (15,6))\nplt.title('Loan issued by region')","44bb3be1":"dataset.emp_length.value_counts()","6f5dd566":"dataset['emp_length_int'] = dataset.emp_length.replace({'10+ years':10,'2 years':2,\n                                                              '< 1 year':0.5,'3 years':3,'4 years':4,'5 years':5,\n                                                              '6 years':6,'7 years':7,'8 years':8,'9 years':9,'1 year':1})\n\ndataset.emp_length_int.value_counts()","5d685b75":"f , ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)\nsns.set_style('whitegrid')\ncmap = plt.cm.inferno\ninterest_rate = dataset.groupby(['year','region']).interest_rate.mean()\ninterest_rate.unstack().plot(kind = 'area',ax = ax1 , figsize = (16,12) , colormap = cmap , grid = False)\nax1.set_title('Average Interest Rate by region')\nax1.set_xlabel('Year')\n\nemp_length = dataset.groupby(['year','region']).emp_length_int.mean().unstack().plot(kind = 'area',ax = ax2 ,colormap = cmap, figsize = (16,12),grid = False)\nax2.set_title('Average Employment Length by region')\nax2.set_xlabel('Year')\ndebt_ti = dataset.groupby(['year','region'])['dti'].mean().unstack().plot(kind = 'area',ax = ax3 , figsize = (16,12) ,colormap = cmap, grid = False)\nax3.set_title('Average Debt to income by region')\nax3.set_xlabel('Year')\n\nincome = dataset.groupby(['year','region']).annual_income.mean().unstack().plot(kind = 'area',ax = ax4 , figsize = (16,12),colormap = cmap,grid = False)\nax4.set_title('Average Income by region')\nax4.set_xlabel('Year')\nplt.show()","ed169a1a":"df_badloans = dataset[dataset['Loan_Condition_x'] == 'Bad Loan']\ndf_badloans.shape","deed9c45":"loan_status_cross = pd.crosstab(df_badloans.region , df_badloans.loan_status).apply(lambda x:x\/x.sum()*100)\n\nnumber_of_loans = pd.crosstab(df_badloans.region , df_badloans.loan_status)\n\nloan_status_cross['Charged Off'] = loan_status_cross['Charged Off'].apply(lambda x: round(x,2))\nloan_status_cross['Default'] = loan_status_cross['Default'].apply(lambda x: round(x,2))\nloan_status_cross['Does not meet the credit policy. Status:Charged Off'] = loan_status_cross['Does not meet the credit policy. Status:Charged Off'].apply(lambda x:round(x,2))\nloan_status_cross['In Grace Period'] = loan_status_cross['In Grace Period'].apply(lambda x: round(x,2))\nloan_status_cross['Late (16-30 days)'] = loan_status_cross['Late (16-30 days)'].apply(lambda x: round(x,2))\nloan_status_cross['Late (31-120 days)'] = loan_status_cross['Late (31-120 days)'].apply(lambda x: round(x,2))\n\nloan_status_cross","8eb48e23":"number_of_loans['total'] = number_of_loans.sum(axis = 1)\nnumber_of_loans","a263cffa":"charged_off = loan_status_cross['Charged Off'].values.tolist()\ndefault = loan_status_cross['Default'].values.tolist()\ndoes_not_meet = loan_status_cross['Does not meet the credit policy. Status:Charged Off'].values.tolist()\nin_grace_period = loan_status_cross['In Grace Period'].values.tolist()\nshort_pay = loan_status_cross['Late (16-30 days)'].values.tolist()\nlong_pay = loan_status_cross['Late (31-120 days)'].values.tolist()\n\ndefaults = go.Bar(\n            x = ['MidWest','NorthEast','SouthEast','SouthWest','West'],\n            y = default,\n            name = 'Defaulters',\n            marker = dict(\n                color = 'rgb(176, 26, 26)'),\n            text = '%'\n)\n\ncredit_policy = go.Bar(\n            x = ['MidWest','NorthEast','SouthEast','SouthWest','West'],\n            y = does_not_meet,\n            name = 'Credit Policy',\n            marker = dict(\n                color = 'purple'),\n            text = '%'\n)\ngrace_period = go.Bar(\n            x = ['MidWest','NorthEast','SouthEast','SouthWest','West'],\n            y = in_grace_period,\n            name = 'Grace Period',\n            marker = dict(\n                color = 'rgb(147, 147, 147)'),\n            text = '%'\n)\nshort_payment = go.Bar(\n            x = ['MidWest','NorthEast','SouthEast','SouthWest','West'],\n            y = short_pay,\n            name = 'Short Pay',\n            marker = dict(\n                color = 'rgb(246, 157, 135)'),\n            text = '%'\n)\nlong_payment = go.Bar(\n            x = ['MidWest','NorthEast','SouthEast','SouthWest','West'],\n            y = long_pay,\n            name = 'Long Pay',\n            marker = dict(\n                color = 'rgb(238, 76, 73)'),\n            text = '%'\n)\ncharged = go.Bar(\n            x = ['MidWest','NorthEast','SouthEast','SouthWest','West'],\n            y = charged_off,\n            name = 'Charged Off',\n            marker = dict(\n                color = 'rgb(192, 148, 245)'),\n            text = '%'\n)\ndata = [charged , defaults , credit_policy , grace_period , short_payment , long_payment]\nlayout = go.Layout(barmode = 'stack' , \n                  title = '% of Bad Loan status by Region',\n                  xaxis = dict(title = 'US Region'))\n\nfig = go.Figure(data = data , layout = layout)\niplot(fig)","e811651a":"(dataset['interest_rate'].mean(),dataset['annual_income'].mean())","5633ddac":"by_loan_amount = dataset.groupby(['region','addr_state'] , as_index = False).loan_amount.sum()\nby_interest_rate = dataset.groupby(['region' , 'addr_state'],as_index = False).interest_rate.mean()\nby_income = dataset.groupby(['region','addr_state'] , as_index = False).annual_income.mean()\n\nby_income.head()","95844350":"states = by_loan_amount['addr_state'].values.tolist()\naverage_loan_amounts = by_loan_amount['loan_amount'].values.tolist()\naverage_interest_rate = by_interest_rate['interest_rate'].values.tolist()\naverage_annual_income = by_income['annual_income'].values.tolist()\n\nmetrics_data = {'State_Codes':states , 'Loan_Amounts':average_loan_amounts,\n                'Interest_Rate':average_interest_rate , 'Annual_Income':average_annual_income}\n\nmetrics_dataset = pd.DataFrame.from_dict(metrics_data)\nmetrics_dataset = metrics_dataset.round(decimals = 2)\nmetrics_dataset.head()","87b853a0":"def category(x):\n    if x < 100000:\n        return 'Low'\n    elif x < 200000:\n        return 'Medium'\n    else:\n        return 'High'\n    \ndataset['income_category'] = dataset.annual_income.apply(lambda x:category(x))\ndataset.income_category.value_counts()","dfbed340":"dataset['Loan_Condition'] = dataset['Loan_Condition_x'].replace({'Good Loan':1 , 'Bad Loan':0})\ndataset['Loan_Condition'].value_counts()","907c49de":"fig , ((ax1,ax2),(ax3,ax4)) = plt.subplots(nrows = 2, ncols = 2, figsize = (14,6))\n\nsns.violinplot(x = 'income_category' , y = 'loan_amount' , data = dataset , ax = ax1 , palette = 'Set2')\nsns.violinplot(x = 'income_category' , y = 'Loan_Condition' , data = dataset , ax = ax2 , palette = 'Set2')\nsns.boxplot(x = 'income_category' , y = 'emp_length_int', data = dataset, ax = ax3 , palette = 'Set2')\nsns.boxplot(x = 'income_category',y = 'interest_rate', data = dataset, ax = ax4, palette = 'Set2')","5b3d6097":"f , (ax1,ax2) = plt.subplots(1,2,figsize = (15,6))\ncmap = plt.cm.coolwarm\n\nby_credit_score = dataset.groupby(['year','grade']).loan_amount.mean()\nby_credit_score.unstack().plot(ax = ax1 , colormap = cmap)\nax1.set_title('Loan Issued by Credit Score')\n\nby_inc = dataset.groupby(['year','grade']).interest_rate.mean()\nby_inc.unstack().plot( ax = ax2 , colormap = cmap)\nax2.set_title('Interest Rate by Credit Score')\n\n#ax2.legend(bbox_to_anchor=(-1.0, -0.3, 1.7, 0.1), loc=5, prop={'size':12},\n #          ncol=7, mode=\"expand\", borderaxespad=0.)","44b4d459":"fig = plt.figure(figsize = (16,12))\n\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(212)\n\ncmap = plt.cm.coolwarm_r\n\nloans_by_region = dataset.groupby(['grade','Loan_Condition_x']).size()\nloans_by_region.unstack().plot(kind = 'bar', ax = ax1 , stacked = True , colormap = cmap , grid = False)\nax1.set_title('Type of Loans by Grade',fontsize = 14)\n\nloans_by_grade = dataset.groupby(['sub_grade','Loan_Condition_x']).size().unstack().plot(kind = 'bar',ax = ax2, stacked = True, colormap = cmap, grid = False)\nax2.set_title('Type of Loans by sub_grade', fontsize = 14)\n\nby_interest = dataset.groupby(['year', 'Loan_Condition_x']).interest_rate.mean().unstack().plot(ax = ax3, colormap = cmap)\nax3.set_title('Average Interest Rate by Loan Condition', fontsize = 14)\nax3.set_ylabel('Interest Rate (%)',fontsize = 12)","3bff47e7":"fig = plt.figure(figsize = (15,12))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\ndataset_bad = dataset[dataset['Loan_Condition'] == 0]\n\n\nsns.boxplot(x = 'home_ownership',y = 'loan_amount',data = dataset_bad , ax = ax1)\n\nsns.boxplot(x = 'year',y = 'loan_amount',hue = 'home_ownership',data = dataset_bad , ax = ax2)\nax2.legend(bbox_to_anchor=(0.1, -0.3, 0.7, 0.1), loc=5, prop={'size':12},\n          ncol=7, mode=\"expand\", borderaxespad=0.)","c035f1cf":"x = dataset.interest_rate.mean()\n\ndef interest(y):\n    if y < x:\n        return 'Low'\n    else:\n        return 'High'\n    \ndataset['interest_payments'] = dataset['interest_rate'].apply(lambda x:interest(x))\ndataset['interest_payments'].value_counts()\n    ","fa11a9f2":"dataset['term'].value_counts()","aa19fb59":"fig = plt.figure(figsize = (20,10))\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(212)\nsns.countplot(x = 'interest_payments', hue = 'Loan_Condition_x',data = dataset , ax = ax1 )\nax1.set_title('Impact of Interest Rate on Loan Condition')\n\nsns.countplot(x = 'interest_payments', hue = 'term',data = dataset , ax = ax2 )\nax2.set_title('Impact of Maturity date on interest rates')\n\nsns.distplot(dataset[dataset.interest_payments == 'Low']['loan_amount'] , ax = ax3 , label = 'Low Interest Rate',color = 'blue')\nsns.distplot(dataset[dataset.interest_payments == 'High']['loan_amount'] , ax = ax3 , label = 'High Interest Rate' , color = 'red')\n\nplt.legend()\nplt.show()","cc0ca543":"plt.figure(figsize = (15,8))\ndx = dataset.purpose.value_counts().reset_index()\ndx.columns = ['Purpose','Values']\nsns.barplot(x = 'Values' , y = 'Purpose' , data = dx)","475db4c3":"dx = round(pd.crosstab(dataset['Loan_Condition_x'] , dataset['purpose']).apply(lambda x:x\/x.sum()*100).reset_index(),2)\n\npurpose_bad = dx.values[0].tolist()[1:]\npurpose_good = dx.values[1].tolist()[1:]\npurpose = dx.columns[1:]\n\nbad_plot = go.Bar(\n            x = purpose,\n            y = purpose_bad,\n            name = 'Bad Loans',\n            text = '%'\n            )\n\ngood_plot = go.Bar(\n            x = purpose,\n            y = purpose_good,\n            name = 'Good Loans',\n            text = '%'\n            )\n\ndata = [bad_plot , good_plot]\n\nlayout = go.Layout(\n        title = 'Condition of Loan by Purpose',\n        paper_bgcolor='#FFF8DC',\n        plot_bgcolor='#FFF8DC',\n        showlegend = True)\n\nfig = dict(data = data, layout = layout)\niplot(fig)","f7107f09":"plt.figure(figsize = (20,10))\nsns.countplot(x = 'purpose' , hue = 'Loan_Condition_x' , data = dataset)","bb489782":"dx = dataset.groupby(['income_category','purpose'])['interest_rate'].mean().reset_index()\ndy = dataset.groupby(['income_category','purpose'])['loan_amount'].mean().reset_index()\ndx['loan_amount_mean'] = dy['loan_amount']\ndy = dataset[dataset.Loan_Condition_x == 'Good Loan'].groupby(['income_category','purpose'])['Loan_Condition_x'].count().reset_index()\ndx['good_loans_count'] = dy['Loan_Condition_x']\ndy =  dataset[dataset.Loan_Condition_x == 'Bad Loan'].groupby(['income_category','purpose'])['Loan_Condition_x'].count().reset_index()\ndx['bad_loans_count'] = dy[\"Loan_Condition_x\"]\ndx","ea61d3c9":"dx['total_loans_issued'] = dx.good_loans_count + dx.bad_loans_count\ndx['bad\/good(%)'] = (dx.bad_loans_count\/dx.good_loans_count)*100\ndx.style.background_gradient('coolwarm')","2c98ad4d":"final_df = dx.sort_values(by = 'purpose' , ascending = False)\nfinal_df","99dfe8c5":"purpose_labels = final_df.purpose.unique()\n\nhigh_income = final_df[final_df.income_category == 'High']['interest_rate'].values.tolist()\nmedium_income = final_df[final_df.income_category == 'Medium']['interest_rate'].values.tolist()\nlow_income = final_df[final_df.income_category == 'Low']['interest_rate'].values.tolist()\n\n\nhigh_list = ['%.2f' % val for val in high_income]\nmed_list = ['%.2f' % val for val in medium_income]\nlow_list = ['%.2f' % val for val in low_income]\n\ntrace1 = {'x' : high_list,\n          'y':purpose_labels,\n           'marker':{'color':\"#0040FF\", \"size\": 16},\n          'mode' : 'markers',\n          'name' : 'High Income',\n          'type' : 'scatter'\n         }\n\ntrace2 = {\"x\": med_list,\n          \"y\": purpose_labels,\n          \"marker\": {\"color\": \"#FE9A2E\", \"size\": 16},\n          \"mode\": \"markers\",\n          \"name\": \"Medium Income\",\n          \"type\": \"scatter\",\n}\n\ntrace3 = {\"x\": low_list,\n          \"y\": purpose_labels,\n          \"marker\": {\"color\": \"#FE2E2E\", \"size\": 16},\n          \"mode\": \"markers\",\n          \"name\": \"Low Income\",\n          \"type\": \"scatter\",\n}\n\ndata = [trace1 , trace2 , trace3]\nlayout = {\"title\": \"Average Purpose Interest Rate <br> <i> by Income Category <\/i> \",\n          \"xaxis\": {\"title\": \"Average Interest Rate\", },\n          \"yaxis\": {\"title\": \"\"}}\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","03d981eb":"plt.figure(figsize = (20,10))\ndx = final_df.sort_values(by = 'good_loans_count' , ascending = False)\nsns.barplot(y = dx.purpose , x = dx.good_loans_count , hue = dx.income_category).set(title = 'Amount of Good Loans issued by purpose')","a84b8e3b":"plt.figure(figsize = (20,10))\ndx = final_df.sort_values(by = 'bad_loans_count' , ascending = False)\nsns.barplot(y = dx.purpose , x = dx.bad_loans_count , hue = dx.income_category).set(title = 'Amount of Bad Loans issued by purpose')","885ce9dc":"complete_df = dataset.copy()\n\nfor col in ('dti_joint', 'annual_inc_joint', 'il_util', 'mths_since_rcnt_il','open_acc_6m', 'open_act_il', 'open_il_12m',\n           'open_il_24m', 'inq_last_12m', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',\n           'mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_last_delinq', 'total_bal_il', 'tot_coll_amt',\n           'tot_cur_bal', 'total_rev_hi_lim', 'revol_util', 'collections_12_mths_ex_med', 'open_acc', 'inq_last_6mths',\n           'verification_status_joint', 'acc_now_delinq'):\n    complete_df[col] = complete_df[col].fillna(0)","6eb42361":"complete_df.next_pymnt_d = complete_df.groupby(['region'])['next_pymnt_d'].transform(lambda x:x.fillna(x.mode))\ncomplete_df.last_pymnt_d = complete_df.groupby(['region'])['last_pymnt_d'].transform(lambda x:x.fillna(x.mode))\ncomplete_df[\"last_credit_pull_d\"] = complete_df.groupby([\"region\"])[\"last_credit_pull_d\"].transform(lambda x: x.fillna(x.mode))\ncomplete_df[\"earliest_cr_line\"] = complete_df.groupby([\"region\"])[\"earliest_cr_line\"].transform(lambda x: x.fillna(x.mode))\ncomplete_df[\"pub_rec\"] = complete_df.groupby(\"region\")[\"pub_rec\"].transform(lambda x: x.fillna(x.median()))\n\ncomplete_df[\"annual_income\"] = complete_df.groupby(\"region\")[\"annual_income\"].transform(lambda x: x.fillna(x.mean()))\n\ncomplete_df[\"total_acc\"] = complete_df.groupby(\"region\")[\"total_acc\"].transform(lambda x: x.fillna(x.median()))\ncomplete_df[\"delinq_2yrs\"] = complete_df.groupby(\"region\")[\"delinq_2yrs\"].transform(lambda x: x.fillna(x.mean()))","70e89c10":"complete_df.drop(['issue_d', 'income_category', 'region', 'year', 'emp_length', 'Loan_Condition_x',\n                 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d', \n                 'verification_status_joint', 'emp_length_int', 'total_rec_prncp', 'funded_amount', 'investor_funds', \n                 'sub_grade', 'complete_date', 'loan_status', 'interest_payments', \n                 'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n               'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee', 'recoveries',\n               'collection_recovery_fee', 'last_pymnt_amnt',\n               'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n               'policy_code', 'application_type', 'annual_inc_joint', 'dti_joint',\n               'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m',\n               'open_acc_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il',\n               'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc',\n               'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m'], axis=1, inplace=True)","98b90b8f":"complete_df = complete_df[['loan_amount', 'term', 'interest_rate', 'installment', 'grade',\n       'home_ownership', 'annual_income', 'verification_status', 'pymnt_plan',\n       'purpose', 'addr_state', 'dti', 'delinq_2yrs', 'inq_last_6mths',\n       'mths_since_last_delinq', 'mths_since_last_record', 'open_acc',\n       'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n       'Loan_Condition']]\ncomplete_df['dti'] = complete_df['dti'].transform(lambda x:x.fillna(x.mean()))\ncomplete_df.isnull().sum().max()","60bd6491":"complete_df['Loan_Condition'].value_counts(normalize = True)","2928bb1f":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n        \n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","0f830173":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","cf06b506":"from sklearn.model_selection import StratifiedShuffleSplit\ncomplete_df['term'] = complete_df['term'].replace({' 36 months':0 , ' 60 months':1})\nstratified = StratifiedShuffleSplit(n_splits = 1 , test_size = 0.2 , random_state = 42)\nfor train_set,test_set in stratified.split(complete_df , complete_df['Loan_Condition']):\n    stratified_train = complete_df.loc[train_set]\n    stratified_test = complete_df.loc[test_set]\n\nprint('Train data ratio\\n',stratified_train['Loan_Condition'].value_counts(normalize = True))\nprint('\\nTest data ratio\\n',stratified_test['Loan_Condition'].value_counts(normalize = True))","e44e3170":"train_df = stratified_train\ntest_df = stratified_test\n\n\n# Let's Shuffle the data\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ntest_df = test_df.sample(frac=1).reset_index(drop=True)\n\n\n# Train set (Normal training dataset)\nX_train = train_df.drop('Loan_Condition', axis=1)\ny_train = train_df['Loan_Condition']\n\n\n# Test Dataset\nX_test = test_df.drop('Loan_Condition', axis=1)\ny_test = test_df['Loan_Condition']","2e51c131":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\n\n# Columns to delete or fix: earliest_cr_line, last_pymnt_d, next_pymnt_d, last_credit_pull_d, verification_status_joint\n\nnumeric = X_train.select_dtypes(exclude=[\"object\"])\ncategorical = X_train.select_dtypes([\"object\"])\n\nnumeric_pipeline = Pipeline([\n    ('selector', DataFrameSelector(numeric.columns.tolist())),\n    ('scaler', StandardScaler()),\n])\n\ncategorical_pipeline = Pipeline([\n    ('selector', DataFrameSelector(categorical.columns.tolist())), # We will have to write the categorical columns manually and see if it works.\n    ('encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n])\n# Combine both Pipelines into one array\ncombined_pipeline = FeatureUnion(transformer_list=[\n    ('numeric_pipeline', numeric_pipeline),\n    ('categorical_pipeline', categorical_pipeline)\n])\n\nX_train = combined_pipeline.fit_transform(X_train)\nX_test = combined_pipeline.fit_transform(X_test)\n","c660f7a9":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\n# log_reg_sm = LogisticRegression()\n_=log_reg.fit(X_train, y_train)","408da528":"from sklearn.metrics import accuracy_score\n\nnormal_ypred = log_reg.predict(X_test)\nprint(accuracy_score(y_test, normal_ypred))","022176f4":"**Research Question**: Is there a significant difference between the average loan amount for bad and good loans","5b5bc463":"Using both the approaches we get almost the same result\nOn the basis of sample with 95% confidence we estimate that the difference in average loan amount for bad loans is 719.94 to 789.04 units more than good loans."}}