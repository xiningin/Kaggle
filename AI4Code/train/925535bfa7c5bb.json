{"cell_type":{"1baffad0":"code","cce824f2":"code","33d55941":"code","d3e6503a":"code","8c7c605d":"code","01ae01c4":"code","ead1d87e":"code","d42bbf5d":"code","369f69d8":"code","70984738":"code","3285a248":"code","d989619c":"code","6719247f":"code","7290d27d":"code","17256efc":"code","82a45eaa":"code","33e01f50":"code","0696e73b":"code","2db167da":"code","fd6105b7":"code","309e790d":"code","7ce461e4":"code","f361401a":"code","d55f7636":"code","611fa2cd":"code","d8847fde":"markdown","ea8e50d0":"markdown","163a2f7c":"markdown","05d9f8ea":"markdown","4f8afa7d":"markdown","2e76df97":"markdown","478ec0ec":"markdown","795e2e96":"markdown","c8a22650":"markdown","77cee7a5":"markdown","cbf7b263":"markdown","572b5745":"markdown","a7bd801c":"markdown","503c3c69":"markdown","0b056aee":"markdown","013c5af5":"markdown","f2f226b1":"markdown","64dfd1f1":"markdown","3bece227":"markdown","87c3a976":"markdown","4079c034":"markdown","50058a59":"markdown","10f5b9d4":"markdown","bc936e8b":"markdown","f99d95a8":"markdown","55f2deb1":"markdown","3ecbc36c":"markdown","aca1230d":"markdown","e44441a3":"markdown","191725f4":"markdown","d3ea447b":"markdown","34606135":"markdown","4018e87f":"markdown","ace07cb8":"markdown","38c2bfea":"markdown","e587098a":"markdown"},"source":{"1baffad0":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Input\n\ntqdm.pandas()\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","cce824f2":"os.listdir('..\/input\/nlp-getting-started')","33d55941":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","d3e6503a":"train_data.head(10)","8c7c605d":"train_data[\"tweet_length\"] = train_data[\"text\"].progress_apply(len)\ntrain_data[\"tweet_words\"] = train_data[\"text\"].progress_apply(lambda x: len(x.split()))\ntrain_data[\"average_word_length\"] = train_data[\"tweet_length\"]\/train_data[\"tweet_words\"]","01ae01c4":"train_data.head(10)","ead1d87e":"sns.distplot(train_data[\"tweet_words\"], color=\"deeppink\")\nplt.show()","d42bbf5d":"sns.distplot(train_data[\"tweet_length\"], color=\"teal\")\nplt.show()","369f69d8":"sns.distplot(train_data[\"average_word_length\"], color=\"darkorchid\")\nplt.show()","70984738":"sns.boxplot(data=train_data, x=\"target\", y=\"tweet_words\", palette=[\"turquoise\", \"hotpink\"])\nplt.show()","3285a248":"sns.boxplot(data=train_data, x=\"target\", y=\"tweet_length\", palette=[\"turquoise\", \"hotpink\"])\nplt.show()","d989619c":"sns.boxplot(data=train_data, x=\"target\", y=\"average_word_length\", palette=[\"turquoise\", \"hotpink\"])\nplt.show()","6719247f":"count_vectorizer = feature_extraction.text.CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train_data[\"text\"]).todense()","7290d27d":"X = train_vectors\/train_vectors.max(axis=1)\ny = train_data[\"target\"].values.reshape((len(train_data), 1))","17256efc":"train_X, val_X, train_y, val_y = train_test_split(X, y)","82a45eaa":"model = Sequential()\nmodel.add(Dropout(0.85))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])","33e01f50":"model.build(input_shape=(None, 21637))\nmodel.summary()","0696e73b":"model.fit(x=train_X, y=train_y, validation_data=(val_X, val_y), epochs=100)","2db167da":"test_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","fd6105b7":"test_vectors = count_vectorizer.transform(test_data[\"text\"]).todense()\nX_test = test_vectors\/test_vectors.max(axis=1)\nX_test[np.isnan(X_test)] = 0","309e790d":"predictions = np.round(model.predict(X_test)).reshape((len(X_test)))","7ce461e4":"sample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","f361401a":"sample_submission[\"target\"] = np.int32(predictions)","d55f7636":"sample_submission.head(10)","611fa2cd":"sample_submission.to_csv('submission.csv', index=False)","d8847fde":"# Contents","ea8e50d0":"### Dealing with the test data","163a2f7c":"### Preliminary Steps","05d9f8ea":"A look at the train data - ","4f8afa7d":"Average Word Length Distribution - ","2e76df97":"Since sample_submission.csv is of the format in which our submission is supposed to be made, I'm first importing it and converting it into a pandas dataframe -","478ec0ec":"Tweet Length Distribution - ","795e2e96":"Creating 3 new columns - tweet_length, tweet_words and average_word_length","c8a22650":"A look at the train data with the new columns - ","77cee7a5":"Splitting the training data into training data and validation data -","cbf7b263":"# Introduction","572b5745":"### Visualizing the data","a7bd801c":"Converting the CSV file into a pandas dataframe - ","503c3c69":"Importing the necessary libraries - ","0b056aee":"Target vs. Average Tweet Length -","013c5af5":"I made this kernel for Kaggle's 'NLP with Disaster Tweets' competition. In this kernel, given thousands of tweets, I tried to identify whether the tweet talks about a disaster or not.","f2f226b1":"* Preliminary steps\n    * Importing the necessary libraries\n    * Converting the CSV file into a pandas dataframe\n* Creating new columns\n* Visualizing the data\n* Encoding the features of the train data\n* Defining the features and prediction target\n* Creating the model\n* Fitting the model\n* Dealing with the test data\n    * Encoding the features of the test data\n* Prediction\n* Ending Note","64dfd1f1":"### Prediction","3bece227":"Converting the CSV file into a pandas dataframe - ","87c3a976":"Target vs. Tweet Words - ","4079c034":"### Encoding the features of the train data","50058a59":"A final look at the dataframe with our predictions -","10f5b9d4":"Converting the dataframe into a csv file without the index column -","bc936e8b":"Encoding the features of the test data and defining a new variable to hold the features - ","f99d95a8":"Converting the tweets to vectors - ","55f2deb1":"Providing the input size to the model -","3ecbc36c":"Tweet Words Distributions - ","aca1230d":"### Defining the features and prediction target","e44441a3":"### Encoding the features of the test data","191725f4":"### Creating new columns","d3ea447b":"Replacing the 'target' column in the dataframe with the values we got -","34606135":"Through this project, I learnt about the conversion of text to vectors. I really enjoyed it, and look forward to learning more in the future. This being only my third ml model, I really appreciate feedback to help me improve both the accuracy and efficiency of my model :)","4018e87f":"### Ending Note","ace07cb8":"Target vs. Tweet Length -","38c2bfea":"### Creating the model","e587098a":"### Fitting the model"}}