{"cell_type":{"91a9f601":"code","2b889c27":"code","70f0933e":"code","dd919ea9":"code","def459ce":"code","43633cb0":"code","1476de4f":"code","6a7f40e1":"code","b4ec1846":"code","6491ab85":"code","7d7ca07b":"code","121a1291":"code","54b77801":"code","ac1722b5":"code","399ba2f4":"code","a8048b4b":"code","0d6403a1":"code","3e98f3ac":"code","c216605b":"code","15f5ad36":"markdown"},"source":{"91a9f601":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2b889c27":"from scipy.stats import randint\nimport seaborn as sns # used for plot interactive graph. \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom io import StringIO\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom IPython.display import display\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","70f0933e":"df = pd.read_csv('\/kaggle\/input\/legal-citation-text-classification\/legal_text_classification.csv')\ndf.shape","dd919ea9":"df.head(2).T","def459ce":"df1 = df[['case_outcome', 'case_text']].copy()\n\n# Remove missing values (NaN)\ndf1 = df1[pd.notnull(df1['case_text'])]\n\n# Renaming second column for a simpler name\ndf1.columns = ['Outcome', 'Text'] \n\ndf1.shape","43633cb0":"total = df1['Text'].notnull().sum()\nround((total\/len(df)*100),1)","1476de4f":"pd.DataFrame(df.case_outcome.unique()).values","6a7f40e1":"df2 = df1.sample(5000, random_state=1).copy()","b4ec1846":"df2['category_id'] = df2['Outcome'].factorize()[0]\ncategory_id_df = df2[['Outcome', 'category_id']].drop_duplicates()\n\n\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'Outcome']].values)\n\ndf2.head()","6491ab85":"fig = plt.figure(figsize=(8,6))\ncolors = ['grey','grey','grey','grey','grey','grey','grey','darkblue','darkblue','darkblue']\ndf2.groupby('Outcome').Text.count().sort_values().plot.barh(ylim=0, color=colors, title= 'COUNT OF EACH CATEGRIES')\nplt.xlabel('Number of ocurrences', fontsize = 10)","7d7ca07b":"tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 2), \n                        stop_words='english')\n\n\nfeatures = tfidf.fit_transform(df2.Text).toarray()\n\nlabels = df2.category_id\n\nprint(\"Each of the %d Text is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))","121a1291":"N = 3\nfor Outcome, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"\\n==> %s:\" %(Outcome))\n  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))","54b77801":"X = df2['Text']\ny = df2['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state = 0)","ac1722b5":"models = [\n    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\n\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\n\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])","399ba2f4":"mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\nstd_accuracy = cv_df.groupby('model_name').accuracy.std()\n\nacc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n          ignore_index=True)\nacc.columns = ['Mean Accuracy', 'Standard deviation']\nacc","a8048b4b":"plt.figure(figsize=(8,5))\nsns.boxplot(x='model_name', y='accuracy', \n            data=cv_df, \n            color='lightblue', \n            showmeans=True)\nplt.title(\"MEAN ACCURACY (cv = 5)\\n\", size=14)","0d6403a1":"X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n                                                               labels, \n                                                               df2.index, test_size=0.25, \n                                                               random_state=1)\nmodel = LogisticRegression(random_state=0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","3e98f3ac":"print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\nprint(metrics.classification_report(y_test, y_pred, \n                                    target_names= df2['Outcome'].unique()))","c216605b":"conf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n            xticklabels=category_id_df.Outcome.values, \n            yticklabels=category_id_df.Outcome.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title(\"CONFUSION MATRIX - LogisticRegression\\n\", size=16);","15f5ad36":"# This time, I tryed to compare 'RandomForestClassifier', LinearSVC, MultinomialNB and LogisticRegression. Accuracy is a little bit improved, but still around 50%.....\n# \n# I refered to below notebook to try TFIDF and ML classification model\n# https:\/\/www.kaggle.com\/selener\/multi-class-text-classification-tfidf"}}