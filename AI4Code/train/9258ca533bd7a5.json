{"cell_type":{"d182a464":"code","496c6dd2":"code","facc7c16":"code","fcbd7132":"code","bb405939":"code","046d4068":"code","fee1bff5":"code","70f04f00":"code","e54a7907":"code","51084aa9":"code","f8d0d9ae":"code","4a6d65de":"code","4c81fd76":"code","4fe1424c":"code","bd7cab6d":"code","b95ef481":"code","f1f52d55":"code","3bd5602b":"code","f79e89b9":"code","cedb149d":"code","337621fc":"code","43d20916":"code","2915a75b":"code","45286ebe":"code","27fe5945":"code","006cfcab":"code","8e7faef0":"code","350c833d":"code","abd54648":"code","b5a48e67":"code","d9c34eec":"code","0382cda5":"code","bd9efdb2":"code","4bdca166":"code","3520e9a6":"code","4e2f47fd":"code","5a764990":"code","38d58082":"code","d7b07c12":"code","7b0214a0":"code","5eb7abe9":"code","80eeb950":"code","00aa1573":"code","d72afe70":"code","ef7a2bca":"code","f3cb9ff1":"code","a2787edf":"code","b8d6afc8":"code","6e09eb00":"code","e9141c2f":"code","01569e2d":"code","9abc5b2b":"markdown","b0a80771":"markdown","a6368d95":"markdown","c0af713c":"markdown","33d17abd":"markdown","69f1e7e6":"markdown","fc6229b8":"markdown","ab10cfac":"markdown","6bfd27e7":"markdown","4cfbb3df":"markdown","f153e351":"markdown","0b88dc42":"markdown","7be33ff5":"markdown","fed94726":"markdown","e26d20ba":"markdown"},"source":{"d182a464":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport statistics\n\nwarnings.filterwarnings('ignore')\nsns.set_style('darkgrid')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","496c6dd2":"train= pd.read_csv('\/kaggle\/input\/av-healthcare-analytics-ii\/healthcare\/train_data.csv')\ntrain.head()","facc7c16":"train.info()","fcbd7132":"train.describe()","bb405939":"train.isna().sum()","046d4068":"train['Bed Grade'].fillna(statistics.mode(train['Bed Grade']),inplace=True)\ntrain['City_Code_Patient'].fillna(statistics.mode(train['City_Code_Patient']),inplace=True)","fee1bff5":"train.isna().sum()","70f04f00":"train.drop(['case_id', 'patientid'], axis=1, inplace=True)","e54a7907":"cat_cols=[]\nnum_cols=[]\n\nfor col in train.columns:\n    if train[col].dtypes=='object':\n        cat_cols.append(col)\n        \nfor col in train.columns:\n    if train[col].dtypes!='object':\n        num_cols.append(col)\n        \nprint(cat_cols)\nprint(num_cols)","51084aa9":"i=1\nplt.figure(figsize=(15,20))\nfor col in cat_cols:\n    plt.subplot(5,2,i)\n    sns.countplot(train[col])\n    i=i+1\nplt.show()","f8d0d9ae":"train['Stay'].value_counts()","4a6d65de":"train['Stay'].replace('More than 100 Days', '>100', inplace=True)","4c81fd76":"train['Stay'].value_counts()","4fe1424c":"train['Stay']= train['Stay'].replace({'0-10':0, '11-20':0, '21-30':1, '31-40':1, '41-50':1, '51-60':2,'61-70':2,'71-80':2,'81-90':2,'91-100':2,'>100':2})","bd7cab6d":"train['Stay'].value_counts()","b95ef481":"i=1\nplt.figure(figsize=(15,20))\nfor col in num_cols:\n    plt.subplot(4,2,i)\n    sns.distplot(train[col])\n    i=i+1\n    \nplt.show()\n   ","f1f52d55":"from sklearn.preprocessing import LabelEncoder\n\nle= LabelEncoder()\ncat_cols.append('Bed Grade')\ncat_cols.append('City_Code_Hospital')\ncat_cols.append('City_Code_Patient')\nfor col in cat_cols:\n    train[col]= le.fit_transform(train[col])\n    ","3bd5602b":"train[cat_cols]","f79e89b9":"train['City_Code_Hospital'].value_counts()","cedb149d":"train['City_Code_Patient'].value_counts()","337621fc":"num_cols.remove('Bed Grade')\nnum_cols.remove('City_Code_Hospital')\nnum_cols.remove('City_Code_Patient')\nnum_cols","43d20916":"plt.figure(figsize=(12,12))\nsns.heatmap(train.corr(), annot=True, cmap='coolwarm')","2915a75b":"from sklearn.preprocessing import StandardScaler\n\nss= StandardScaler()\n\ntrain[num_cols]= ss.fit_transform(train[num_cols].values)\n","45286ebe":"train","27fe5945":"from sklearn.model_selection import train_test_split\n\ny= train['Stay']\nX= train.drop('Stay', axis=1)","006cfcab":"X","8e7faef0":"y","350c833d":"X_train, X_test, y_train,y_test= train_test_split(X,y,test_size= 0.2, stratify=y, random_state=42)","abd54648":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import RandomizedSearchCV","b5a48e67":"value= [LogisticRegression(), RandomForestClassifier(), DecisionTreeClassifier(),  KNeighborsClassifier(), CatBoostClassifier(), XGBClassifier()]\n\nkey= ['LogisticRegression', 'RandomForsetClassifier', 'DecisionTreeClassifier',  'KNeighborsClassifier', 'CatBoostClassifier', 'XGBClassifier']\n\nmodels= dict(zip(key,value))\naccuracy_scores=[]\nfor key,value in models.items():\n    value.fit(X_train,y_train)\n    y_pred= value.predict(X_test)\n    accuracy= accuracy_score(y_test, y_pred)\n    accuracy_scores.append(accuracy)\n    print(key)\n    print(accuracy)","d9c34eec":"sns.barplot(x= ['LR','RFC','DT','KNN','CB','XGB'],y=accuracy_scores)","0382cda5":"params= {'objective':['binary:logistic'],\n              'max_depth': [3,4,5,6],\n              'min_child_weight': [1,5,10,12],\n              'subsample': [0.6,0.8,1.0],\n              'colsample_bytree': [0.6,0.8,1.0], 'gamma': [0.5,1,1.5,2]}\n\nxgb= XGBClassifier(n_estimators=600)\n\ngrid= RandomizedSearchCV(xgb, cv=3, verbose=3,param_distributions= params, n_iter=5)\ngrid.fit(X,y)","bd9efdb2":"grid.best_score_","4bdca166":"grid.best_estimator_","3520e9a6":"y_pred= grid.best_estimator_.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","4e2f47fd":"test= pd.read_csv('\/kaggle\/input\/av-healthcare-analytics-ii\/healthcare\/test_data.csv')\ntest.head()","5a764990":"test.isna().sum()","38d58082":"test['Bed Grade'].fillna(statistics.mode(test['Bed Grade']),inplace=True)\ntest['City_Code_Patient'].fillna(statistics.mode(test['City_Code_Patient']),inplace=True)","d7b07c12":"test.info()","7b0214a0":"test.drop(['case_id', 'patientid'], axis=1, inplace=True)","5eb7abe9":"cat_cols.remove('Stay')\nfor col in cat_cols:\n    \n    test[col]= le.fit_transform(test[col])","80eeb950":"test[num_cols]= ss.transform(test[num_cols].values)","00aa1573":"test","d72afe70":"predictions= grid.best_estimator_.predict(test)","ef7a2bca":"np. set_printoptions(threshold=np. inf)\nprint(predictions)","f3cb9ff1":"sns.countplot(predictions)","a2787edf":"submission= pd.read_csv('\/kaggle\/input\/av-healthcare-analytics-ii\/healthcare\/sample_sub.csv')","b8d6afc8":"submission.head()","6e09eb00":"submission['Stay']= predictions","e9141c2f":"submission['Stay']= submission['Stay'].replace({0:'Less than 20 days', 1:'21-50 days', 2:'51-100+ days'})","01569e2d":"submission","9abc5b2b":"# Dividing the Labels\n**Here, I divide the stay duration into 3 categories**\n0: 0-20 days\n1: 21-60 days\n2: 61-100+ days\n\nYou might say why a disproportionate distribution. This is to balance the number of patients in each category\n\nPlus the model will not give good prediction accuracy if there are 11 classes to predict. It would have given good results if there were a proportionate number of rows give, example 1,000,000 but with the amount of data we have, we can get a decent accuracy with 3 labels to predict.","b0a80771":"**Dividing the columns into categorical and numerical for EDA**","a6368d95":"# Encoding Categorical Columns","c0af713c":"# Building our Model","33d17abd":"**I found that some columns in numerical category were actually categorical columns. So I shifted them to the category**","69f1e7e6":"# Using the test data","fc6229b8":"# Load Data","ab10cfac":"# Hyperparameter Tuning","6bfd27e7":"# More EDA","4cfbb3df":"**These columns are of no use**","f153e351":"# Upvote and Comment if you liked my Notebook","0b88dc42":"# Exploratory Data Analysis","7be33ff5":"# Scaling Numerical Columns","fed94726":"**It is clear that CatBoostClassifier and XGBClassifier are best for the data.**\n\n**But, now for hyperparameter tuning, CatBoostClassifier takes a lot of time. So, I decided to go with only tuning XGBClassifier model fot the sake of my old laptop xD**","e26d20ba":"# Final Predictions"}}