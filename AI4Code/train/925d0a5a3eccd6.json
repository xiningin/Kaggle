{"cell_type":{"291615a4":"code","815c57e1":"code","ab600f35":"code","ca168cbd":"code","afcaf729":"code","5172c022":"code","99b27d99":"code","1a18e777":"code","549724db":"code","3e6e768b":"code","fdac2e8c":"code","7cdbac75":"code","452167e1":"code","4a145bbe":"code","100cb5b7":"code","102ca8e8":"code","907d04af":"code","df96a046":"code","9e7a9137":"code","1c5ad65f":"markdown","24e2f547":"markdown","9e5bb02f":"markdown","f5e48d53":"markdown","ad9697b0":"markdown","aa20d1c2":"markdown","5afe8225":"markdown","4519ae58":"markdown"},"source":{"291615a4":"!pip install timm","815c57e1":"import os\nimport pandas as pd\nimport torch\nimport numpy as np\nimport random\n\nfrom torch.utils.data import DataLoader\nimport cv2\n\nimport timm\nfrom pprint import pprint\n# model_names = timm.list_models(pretrained=True)\n# pprint(model_names)","ab600f35":"def seed_everything(seed=99):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()","ca168cbd":"cfg = {\n    'version':'version1',\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'train_img':'..\/input\/bms-molecular-translation\/train',\n    'train_anno':'..\/input\/bms-molecular-translation\/train_labels.csv',\n    'sample_ratio': 0.01,\n    'backbone':'efficientnet_b0',\n    'pretrianed':True,\n    'vocab_size':38, # 36 unique char + [SOS, EOS]\n    'max_len':150,\n    'embed_size':16,\n    'hidden_size':64,\n    'image_size':[128, 128],\n    'batch_size':64,\n    'num_workers':6,\n    'n_epochs':10,\n    'lr':1e-3,\n    'min_lr':1e-6,\n    'patience':3,\n    'TTA':5,\n}\n\nif not os.path.isdir(cfg['version']):\n    os.mkdir(cfg['version'])\n    print('create dir')\ncfg","afcaf729":"class Lang():\n    \"\"\"\n    seq: chemical structure, shape like: 'InChI=1S...'\n    \"\"\"\n    def __init__(self):\n        start_end_token = ['SOS', 'EOS']\n        self.vocab = start_end_token + ['(', ')', '+', ',', '-', '\/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'B', 'C', 'D', 'F', 'H', 'I', 'N', 'O', 'P', 'S', 'T', 'b', 'c', 'h', 'i', 'l', 'm', 'r', 's', 't']\n        self.token_to_idx = {token:idx for idx, token in enumerate(self.vocab)}\n        \n    def seq_to_idx(self, seq):\n        idxs = []\n        seq = seq.replace('InChI=1S', '')  # remove head\n        for token in seq:\n            idxs += [self.token_to_idx[token]]\n        source = [0] + idxs\n        target = idxs + [1]\n        return source, target      # return source and target\n    \n    def idx_to_seq(self, idxs):\n        idxs = self.rm_re_idxs(idxs)  # remove repeated text\n        # add head, remove SOS and EOS\n        seq = 'InChI=1S'+''.join([self.vocab[idx] for idx in idxs])\n        \n        return seq.replace('SOS','').replace('EOS','')\n    \n    def rm_re_idxs(self, idxs):\n        # remove repeated text\n        new_idxs = []\n        for index, idx in enumerate(idxs):\n            if idx == 1:\n                break\n            new_idxs += [idx]\n        return new_idxs\n    \nlang = Lang()\ncfg['vocab_size'] = len(lang.vocab)","5172c022":"sentence = 'InChI=1S\/C13H20OS\/c1-9(2)8-15-13-6-5-10(3)7-12(13)11(4)14\/h5-7,9,11,14H,8H2,1-4H3'\nsource, target = lang.seq_to_idx(sentence)\nlang.idx_to_seq(source)","99b27d99":"from sklearn.model_selection import train_test_split\n\nfull_df = pd.read_csv(cfg['train_anno'])\n\nsample_df = full_df.sample(frac=cfg['sample_ratio']).reset_index(drop=True)\n\nX = list(range(len(sample_df)))\ntrain_indexs, val_indexs = train_test_split(X, test_size=0.33, random_state=42)\ntrain_df, val_df = sample_df.loc[train_indexs], sample_df.loc[val_indexs]\ntrain_df.head()","1a18e777":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ntransform = A.Compose([\n    A.Resize(cfg['image_size'][0], cfg['image_size'][1]),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])","549724db":"def convert_image_id_2_path(img_dir:str, image_id: str) -> str:\n    return \"{}\/{}\/{}\/{}\/{}.png\".format(img_dir,\n        image_id[0], image_id[1], image_id[2], image_id \n    )\n\ndef data_process(df, img_dir):\n    data = []\n    for idx in range(len(df)):\n        image_id = df.iloc[idx].image_id\n        img_path = convert_image_id_2_path(img_dir, image_id)\n        seq = df.iloc[idx].InChI\n        data += [(img_path, seq)]\n        \n    return data\n    \ndef generate_batch(data_batch, tfs=transform, train=True):\n    \n    img_batch, source_batch, target_batch = [], [], []\n    for (img_path, seq) in data_batch:\n        img = cv2.imread(img_path)\n        if tfs:\n            img = tfs(image=img)['image']\n            \n        img_batch += [img]\n        source, target = lang.seq_to_idx(seq)\n        source_batch += [torch.tensor(source, dtype=torch.long)]\n        target_batch += [torch.tensor(target, dtype=torch.long)]\n    return img_batch, source_batch, target_batch\n","3e6e768b":"train_data = data_process(train_df, img_dir=cfg['train_img'])\nval_data = data_process(val_df, img_dir=cfg['train_img'])\ntrain_iter = DataLoader(train_data, batch_size=cfg['batch_size'],\n                        shuffle=True, collate_fn=generate_batch, num_workers=cfg['num_workers'])\nvalid_iter = DataLoader(val_data, batch_size=cfg['batch_size'],\n                        shuffle=False, collate_fn=generate_batch, num_workers=cfg['num_workers'])","fdac2e8c":"import matplotlib.pyplot as plt\n\ndef visualize_batch(image, labels):\n    plt.figure(figsize=(16, 12))\n    for ind, (image, label) in enumerate(zip(image, labels)):\n        plt.subplot(3, 3, ind + 1)\n        plt.imshow(image.permute(1, 2, 0))\n        plt.title(f\"{label[:10]}...\", fontsize=10)\n        plt.axis(\"off\")\n    \n    plt.show()\n\nfor imgs, source, target in train_iter:\n    visualize_batch(imgs[:3], target[:3])\n    break","7cdbac75":"from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pack_sequence\nimport torch.nn as nn\n\n# Conv + LSTM\nclass Generator(nn.Module):\n    \"\"\"\n    Conv encoder LSTM decoder\n    x: imgs\n    seqs: padded sequence of idxs that is a batch of InChl\n    lengths: batch of seqs length\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.backbone = timm.create_model(cfg['backbone'], pretrained=cfg['pretrianed'], num_classes=cfg['hidden_size'])\n        self.emb_layer = nn.Embedding(cfg['vocab_size'], cfg['embed_size'])\n        self.lstm = nn.LSTM(cfg['embed_size'], cfg['hidden_size'])\n        self.out_layer = nn.Linear(cfg['hidden_size'], cfg['vocab_size'])\n        \n    def forward(self, x, source_padded, lengths):\n        batch_size = x.size(0)\n        features = self.backbone(x)\n        (h, c) = self.init_state(batch_size)\n        states = (features.unsqueeze(0), c) \n        input_embedding = self.emb_layer(source_padded)    # embedding: len, batch, size\n        packed = pack_padded_sequence(input_embedding, lengths, enforce_sorted=False)\n        out_packed, _ = self.lstm(packed, states)\n        outputs = self.out_layer(out_packed[0])\n        return outputs   # packed len, batch\n    \n    def greedy_decode(self, x):\n        \"\"\"Greedy search\"\"\"\n        sampled_ids = []\n        batch_size = x.size(0)\n        features = self.backbone(x)\n        (h, c) = self.init_state(batch_size)\n        states = (features.unsqueeze(0), c)\n        # create start \n        SOS = torch.zeros((1, batch_size), dtype=torch.long, device=self.cfg['device']) # create SOS\n        last_emb = self.emb_layer(SOS) \n        for i in range(self.cfg['max_len']):                      # maximum sampling length\n            hiddens, states = self.lstm(last_emb, states)         # (1, batch, hidden_size), \n            outputs = self.out_layer(hiddens.squeeze())           # (batch_size, vocab_size)\n            curr = torch.argmax(outputs, dim=-1, keepdim=True)    # to idxs\n            curr = curr.reshape(1, batch_size)\n            last_emb = self.emb_layer(curr)  # (1, batch, embed_size)\n            \n            sampled_ids.append(curr)\n        sampled_ids = torch.cat(sampled_ids)                    \n        return sampled_ids  # (seq, batch)\n    \n    def init_state(self, batch_size):\n        return (\n            torch.zeros(1, batch_size, self.cfg['hidden_size']).to(self.cfg['device']),\n            torch.zeros(1, batch_size, self.cfg['hidden_size']).to(self.cfg['device'])\n        )","452167e1":"import Levenshtein\n\n# metric\ndef get_score(y_pred, y_true):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        score = Levenshtein.distance(true, pred)\n        scores.append(score)\n    avg_score = np.mean(scores)\n    return avg_score\n\ndef get_accuracy(y_pred, y_true):\n    return (y_pred == y_true).sum() \/ len(y_pred)","4a145bbe":"# preprocess data\ndef load_data(data):\n    imgs_batch, source_batch, target_batch = data\n    imgs_batch = torch.stack(imgs_batch, dim=0).to(cfg['device'])\n    lens = torch.tensor([len(item) for item in source_batch])\n    source_padded = pad_sequence(source_batch).to(cfg['device']) # padding\n    return imgs_batch, source_padded, lens, target_batch","100cb5b7":"import math\n\nclass Eearly_Stopping():\n    def __init__(self, mode: str='min', patience: int=10, apply=True):\n        self.best = math.inf\n        self.mode = mode\n        self.base_patience = patience\n        self.patience = patience\n        self.apply=apply\n        if self.patience <= 0:\n            raise Exception(\"Invalid patience!\", patience)\n            \n    def step(self, model, monitor):\n        if monitor < self.best:\n            self.best = monitor\n            torch.save(model, f'model.pth') # save model\n            self.patience = self.base_patience\n        else:\n            self.patience -= 1 if self.apply else -1\n        \n        return True if self.patience==0 else False","102ca8e8":"import torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau \n\n# training setting\ngenerator = Generator(cfg).to(cfg['device'])\noptimizer = optim.Adam(generator.parameters(), lr=cfg['lr'])\ncriterion = nn.CrossEntropyLoss()\nscheduler = ReduceLROnPlateau(optimizer, mode='min', patience=cfg['patience'], factor=0.5, verbose=True)\nes = Eearly_Stopping(mode='min', patience=5, apply=True)","907d04af":"def evaluate(val_loader, model):\n    pred_text = []\n    gts_text = []\n    bar = tqdm(val_loader, desc='eval')\n    model.eval()\n    with torch.no_grad():\n        for data in bar:\n            imgs_batch, source_padded, lens, target_batch = load_data(data)\n            pred_batch = model.greedy_decode(imgs_batch)\n            pred_batch = pred_batch.permute(1, 0)  # (seq, batch) -> (batch, seq)\n            preds = [lang.idx_to_seq(item) for item in pred_batch.cpu().numpy()]\n            gts = [lang.idx_to_seq(item) for item in target_batch] \n            pred_text += preds\n            gts_text += gts\n        \n    return pred_text, gts_text\n\ndef train(train_iter, epoch):\n    generator.train()\n    bar = tqdm(train_iter, desc='training')\n    for data in bar:\n        imgs_batch, source_padded, lens, target_batch  = load_data(data)\n        outs = generator(imgs_batch, source_padded, lens)\n        target_packed = pack_sequence(target_batch, enforce_sorted=False)[0].to(cfg['device'])\n        loss = criterion(outs, target_packed)\n\n        ### Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        ## log\n        preds = torch.argmax(outs, dim=-1)\n        preds = preds.cpu().detach().numpy()\n        target_packed = target_packed.cpu().numpy()\n        pred_text = lang.idx_to_seq(preds)\n        gts_text = lang.idx_to_seq(target_packed)\n        score = get_score(pred_text, gts_text)\n        acc = get_accuracy(preds, target_packed)\n        bar.set_description(f'epoch: {epoch+1} loss: {loss.item():.3f} score: {score:.3f} acc: {acc:.3f}')\n        \n    return generator","df96a046":"from tqdm import tqdm\n\nfor epoch in range(cfg['n_epochs']):\n    ## train\n    generator = train(train_iter, epoch)\n    ## evaluate\n    pred_text, gts_text = evaluate(valid_iter, generator)\n    score = get_score(pred_text, gts_text)\n    print('val score:', score)\n    scheduler.step(score)\n    if es.step(generator, score):\n        print('early stop!')\n        break;\n","9e7a9137":"model = torch.load('model.pth')\ncheckpoint = {}\ncheckpoint['net'] = model.state_dict()\ncheckpoint['train_cfg'] = cfg\ntorch.save(checkpoint, f\"{cfg['version']}\/checkpont.pth\")  # save model","1c5ad65f":"dataloader pipeline","24e2f547":"augmentation pipeline","9e5bb02f":"sampling data, then split it into train and test dataset","f5e48d53":"sentence to idxs, then restore","ad9697b0":"#### build vocab\nadd start token 'SOS 'and end token 'EOS' into vocab","aa20d1c2":"This is a baseline model that using CNN and LSTM to translate image into chemical structure. The total training images are 2424186, that's enormous. So I decided to sample 5% of full training data for training.\n![](https:\/\/github.com\/yunjey\/pytorch-tutorial\/raw\/master\/tutorials\/03-advanced\/image_captioning\/png\/model.png)","5afe8225":"This notebook borrowed many codes from\uff1a \n1. [PyTorch Tutorial](https:\/\/pytorch.org\/tutorials\/beginner\/torchtext_translation.html)\n2. [Blog: solving-an-image-captioning-task-using-deep-learning](https:\/\/www.analyticsvidhya.com\/blog\/2018\/04\/solving-an-image-captioning-task-using-deep-learning\/)","4519ae58":"display samples"}}