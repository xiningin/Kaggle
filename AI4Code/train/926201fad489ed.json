{"cell_type":{"4e19365f":"code","e8036c6c":"code","90d8c26b":"code","b26cb968":"code","c5474b19":"code","4b872c5c":"code","898ed72e":"code","2287ffb4":"code","3d5e969a":"code","228f8b19":"code","54235aa3":"code","f5c8ed56":"code","49009420":"code","d5f47659":"code","b5a71f7a":"code","d28cdc21":"code","2a2e5f9e":"code","7c924520":"code","f938253c":"code","a4ec520e":"code","b43384c0":"code","ea85718a":"code","d53a82df":"code","19c25f2e":"code","4ff5ded6":"markdown","0b6c6574":"markdown","ebb8eabf":"markdown","0ae70dbb":"markdown","9de0bd58":"markdown","39a1f740":"markdown","05e982e6":"markdown","7f053839":"markdown","3a2b3fb4":"markdown","6069a13d":"markdown","c7bc6682":"markdown","b0179907":"markdown","d315e8c7":"markdown","6878c4f6":"markdown","9968abba":"markdown","0b8a9cd3":"markdown","edc8d442":"markdown","35485aae":"markdown","1cd7cfac":"markdown","38dddbe8":"markdown","feaa49a9":"markdown","e8a42384":"markdown","613d8f1e":"markdown","6ac34682":"markdown","d7c1bffb":"markdown","0e8ffcdf":"markdown","20dc117c":"markdown","e4be8055":"markdown","72d83af8":"markdown","d1ec3e5f":"markdown","fb871239":"markdown","9aeacfa8":"markdown","64347e0d":"markdown","af309a0e":"markdown","2b27cf39":"markdown","0b88f4d6":"markdown","6a48274e":"markdown","fa4d9fb1":"markdown","6fee62f7":"markdown","412314f7":"markdown","c2f37b2f":"markdown","cb885574":"markdown","15d2c9ea":"markdown","121e0fba":"markdown","45a5c3a7":"markdown","27e07449":"markdown","5e35ec15":"markdown"},"source":{"4e19365f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8036c6c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.style as style\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nimport random\n\npd.set_option(\"display.max_rows\",None)\npd.set_option(\"display.max_columns\",None)","90d8c26b":"train_data = pd.read_csv(\"\/kaggle\/input\/ghouls-goblins-and-ghosts-boo\/train.csv.zip\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/ghouls-goblins-and-ghosts-boo\/test.csv.zip\")","b26cb968":"train_data.head()","c5474b19":"test_data.head()","4b872c5c":"train_data.describe().T","898ed72e":"test_data.describe().T","2287ffb4":"train_data.isnull().sum()","3d5e969a":"test_data.isnull().sum()","228f8b19":"graph = sns.countplot(x=\"color\", data=train_data) #x is color column, data is the dataset\nfor p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","54235aa3":"graph = sns.countplot(x=\"color\", data=test_data)\nfor p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","f5c8ed56":"#bone_length and has_soul are float columns from tarin_data\nsns.jointplot(train_data.loc[:,'bone_length'], train_data.loc[:,'has_soul'], kind=\"regg\", color=\"#ce1414\")","49009420":"#bone_length and has_soul are float columns from tarin_data\nsns.jointplot(train_data.loc[:,'hair_length'], train_data.loc[:,'rotting_flesh'], kind=\"regg\", color=\"#ce1414\")","d5f47659":"# drop the original color column and create color_category (color_white, color_clear, etc.) columns with \"prefix\"\ntrain_data = pd.get_dummies(train_data, columns=[\"color\"], prefix=[\"color\"])","b5a71f7a":"# Approach 1\n#le = LabelENcoder ()\n#train_data[\"type\"] = label_encoder.fit_transform(train_data[\"type\"].astype(str)) #you need to code .astype(str) in Label Encoding\n\n#Approach 2\n\nmap_type = {\"Ghoul\":1, \"Goblin\":2, \"Ghost\":0} # change ghoul, goblin, ghost to 1, 2 and 0\ntrain_data.loc[:, \"type\"] = train_data.type.map(map_type)","d28cdc21":"train_data = train_data.set_index('id')\ntrain_data.head()","2a2e5f9e":"X = train_data.drop([\"type\"],axis=1)\ny = train_data.type\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)","7c924520":"# These parameters will be used in gridsearch and will be found optimum parameters\nparam_grid = {\n    'class_weight': [None, 'balanced'],\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(30, 150)),\n    'learning_rate': [0.01,0.1,0.5],\n    'subsample_for_bin': [20000,50000,100000,120000,150000],\n    'min_child_samples': [20,50,100,200,500],\n    'colsample_bytree': [0.6,0.8,1],\n    \"max_depth\": [5,10,50,100]\n}","f938253c":"lgbm = LGBMClassifier() # Shortname the LGBMClassifier()\n\nlgbm.fit(X_train, y_train) # Train the lgbm on train sets","a4ec520e":"#%%time\n#lgbm_cv = GridSearchCV(lgbm, param_grid, cv=5, n_jobs=-1, verbose=2) #5 fold cross validation\n#lgbm_cv.fit(X_train, y_train) #try all parameters\n#lgbm_cv.best_params_ #print optimum parameters","b43384c0":"# Here are the optimum values of the parameters\n{'boosting_type': 'gbdt',\n 'class_weight': None,\n 'colsample_bytree': 0.6,\n 'learning_rate': 0.01,\n 'max_depth': 10,\n 'min_child_samples': 20,\n 'num_leaves': 30,\n 'subsample_for_bin': 20000}","ea85718a":"lgbm_tuned = LGBMClassifier(boosting_type = 'gbdt',\n                            class_weight = None,\n                            min_child_samples = 20,\n                            num_leaves = 30,\n                            subsample_for_bin = 20000,\n                            learning_rate=0.01, \n                            max_depth=10, \n                            n_estimators=40, \n                            colsample_bytree=0.6) # LightGBM Classifier with optimum paramteres\nlgbm_tuned.fit(X_train, y_train) #Fit the model with the optimum parameters","d53a82df":"y_test_pred = lgbm_tuned.predict(X_test) #Predicting X_test to find the soluti\u0131n\n\nscore = round(accuracy_score(y_test, y_test_pred), 3) # Find the accuracy of y_test and predicitons, and round the result\nprint(score)","19c25f2e":"sns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\n\nfi = pd.DataFrame()\nfi['features'] = X.columns.values.tolist()\nfi['importance'] = lgbm_tuned.booster_.feature_importance(importance_type='gain')\n\nsns.barplot(x='importance', y='features', data=fi.sort_values(by='importance', ascending=True))","4ff5ded6":"### **Observations**\n\nThere isn't any null row in both sets. So we don't have to use filling techiques on the datasets. But there will be a notebook about missing values about in the future, not only about filling with **mean, median, mode** but also **advanced techniques** .. **Keep calm and wait for the notebook!**","0b6c6574":"![asd1.png](attachment:asd1.png)","ebb8eabf":"### **Observations**\n\nThere is an \"id\" column that can be set as index in both sets and as we see the test set had been chosen from the train set. Look at the id columns.\n\n\"bone_length\" column looks like normalized between 0 and 1. It measures the length of the bones of the creatures.\n\n\"rotting_flesh\" also column looks like normalized between 0 and 1. It measures the percentages of the fresh meat of the creatures.\n\n\"hair_length\" column looks like normalized between 0 and 1. It measures the length of the hair of the creatures.\n\n\"has_soul\" column looks like normalized between 0 and 1. It measures the percentages of the soul of the creatures.\n\n\"color\" column is a categorical column and tells us the color of the creatures.\n\nTest data has more rows (529) than train data has (371)","0ae70dbb":"## ***I will develop this notebook further with your advices.***","9de0bd58":"![unnamed%20%281%29.jpg](attachment:unnamed%20%281%29.jpg)","39a1f740":"# **Some EDA**\n\nAlthough EDA is not our topic, I will mention it briefly, because we need to take a look at the features to determine who is an ally and who is an enemy. **Keep calm, we will also work on a EDA and how to approach almost any kind of columns in the future, wait for the another notebook**","05e982e6":"# ***Motivation***","7f053839":"# ***SOME SOURCES***\n\n[Here is the winning solutions of kaggle competitions with LightGBM](https:\/\/github.com\/microsoft\/LightGBM\/tree\/master\/examples#machine-learning-challenge-winning-solutions)\n\n[For further information in LightGBM CLassifier and parameters](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier)","3a2b3fb4":"The accuracy score is 0.718. Now you can apply the last mdoel to the test set and upload it to see your place in the LeaderBoard","6069a13d":"# **FASTER... BETTER...**\n\nIf you want to win a war, then, you have to be fast, and furious. If you want to be fast, you must learn using your weapon faster. To use your weapon fasster you must use GPU. Every warrior doesn't have water cooling RTX 3090 unfortunately, so if you have a GPU and want to use your GPU, here is the documentation [for Windows;](https:\/\/lightgbm.readthedocs.io\/en\/latest\/GPU-Windows.html).. For us, it will be subject of another warbook. We will continue with CPU, poor us...","c7bc6682":"## ***It took about 1 week to prepare the notebook, I would appreciate it if you upvote.*** ","b0179907":"Independent features are appointed to \"X\", and dependent feature is appointed to \"y\". 40% of the train_data is allocated for testing.","d315e8c7":"Don't forget to change id column to the index! And final look to our dataset!","6878c4f6":"# ***FEATURES of the WEAPON***","9968abba":"![Battle-of-Thermopylae-in-300.jpg](attachment:Battle-of-Thermopylae-in-300.jpg)","0b8a9cd3":"# ***Last but not Least***","edc8d442":"## ***LightGBM Classifier***","35485aae":"We all read dozens of notebooks here every day when we are at school or at work or in a war, of course, when we need something we run to Kaggle, Medium or TowardsDataScience. We write algorithms of unknown origin to our war books with copy-paste and change the variable names. So what about when there is a problem, when it fails?\n\nWhat would you think if I promised you that I would end all these problems with the series I started with this war book and will write in the future? It would be very assertive but I can still soothe your pain but never your tears...","1cd7cfac":"The parameters below are used for both classification and regression. Specific classification parameters will be discussed at the SOME SOURCES part.\n\n**max_depth:** This parameter is an integer that controls the maximum distance between the root node of each tree and a leaf node. ***Decrease max_depth to reduce training time, but decreasing can reduce training accuracy also***\n\n**num_leaves:** This is the main parameter to control the complexity of the tree model. We can set num_leaves = 2^(max_depth)-1 to obtain the same number of leaves as depth-wise tree. If max_depth = 7, then num_leaves = 127 but this equality may cause ovefitting. ***Decrease num_leave to prevent overfitting and reduce training time, but decreasing can reduce training accuracy also***\n\n**min_data_in_leaf:** This is a very important parameter to prevent over-fitting in a leaf-wise tree. Its optimal value depends on the number of training samples and \"num_leaves\". Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. ***In practice, setting it to hundreds or thousands is enough for a large dataset.***\n\n**num_threads:** By default, this will defer to the default behavior of OpenMP (one thread per real CPU core or the value in environment). ***For best performance, set this to the number of real CPU cores available.***\n\n**min_gain_to_split:**  Gain is basically the reduction in training loss that results from adding a split point. By default, LightGBM sets min_gain_to_split to 0.0. ***Increase min_gain_to_split to reduce training time.***\n\n**num_iterations:** Parameter controls the number of boosting rounds that will be performed. Since LightGBM uses decision trees as the learners, this can also be thought of as \u201cnumber of trees\u201d. ***Decrease num_iterations to reduce training time.***\n\n\n**learning_rate:** Correlated parameter with \"num_iterations\". learning_rate will not have any impact on training time, but it will impact the training accuracy. ***As a general rule, if you reduce num_iterations, you should increase learning_rate.***\n\n**early_stopping_rounds:** That \u201cnumber of consecutive rounds\u201d is controlled by the parameter early_stopping_rounds. For example, early_stopping_rounds=1 says \u201cthe first time accuracy on the validation set does not improve, stop training\u201d. ***Set early_stopping_rounds and provide a validation set to possibly reduce training time.*** If you don't use early_stopping_rounds, model will stop after tried all parameters.\n\n**Dataset:** When a LightGBM \"Dataset\" object is constructed, some features will be filtered out based on the value of \"min_data_in_leaf\". For a simple example, consider a 1000-observation dataset with a feature called **feature_1**. feature_1 takes on only two values: 25.0 (995 observations) and 50.0 (5 observations). If min_data_in_leaf = 10, there is no split for this feature which will result in a valid split at least one of the leaf nodes will only have 5 observations. Instead of reconsidering this feature and then ignoring it every iteration, LightGBM filters this feature out at before training, when the \"Dataset\" is constructed. **If this default behavior has been overridden by setting feature_pre_filter=False, set feature_pre_filter=True to reduce training time.**\n\n***The hyperparameters below are used with \"Dataset\" parameter.***\n\n**max_bin** or **max_bin_by_feature When Creating \"Dataset\":** Reducing the number of bins per feature can reduce the number of splits that need to be evaluated. max_bin is controls the maximum number of bins that features will bucketed into. It is also possible to set this maximum feature-by-feature, by passing max_bin_by_feature. **Reduce max_bin or max_bin_by_feature to reduce training time.**\n\n\n**min_data_in_bin:** Some bins might contain a small number of observations, which might mean that the effort of evaluating that bin\u2019s boundaries as possible split points isn\u2019t likely to change the final model very much. You can control the granularity of the bins by setting min_data_in_bin. **Increase min_data_in_bin to reduce training time.**\n\n**feature_fraction:** By default, LightGBM considers all features in a Dataset during the training process. This behavior can be changed by setting feature_fraction to a value > 0 and <= 1.0. Setting feature_fraction to 0.5, for example, tells LightGBM to randomly select 50% of features at the beginning of constructing each tree. This reduces the total number of splits that have to be evaluated to add each tree node. **Decrease feature_fraction to reduce training time.**\n\n**max_cat_threshold:**  In this process, LightGBM explores splits that break a categorical feature into two groups. These are sometimes called \u201ck-vs.-rest\u201d splits. Higher max_cat_threshold values correspond to more split points and larger possible group sizes to search. **Decrease max_cat_threshold to reduce training time.**\n\n***By default, LightGBM uses all observations in the training data for each iteration. It is possible to instead tell LightGBM to randomly sample the training data. This process of training over multiple random samples without replacement is called \u201cbagging\u201d.***\n\n**bagging_freq:** Set bagging_freq to an integer greater than 0 to control how often a new sample is drawn. Set bagging_fraction to a value > 0.0 and < 1.0 to control the size of the sample. For example, {\"bagging_freq\": 5, \"bagging_fraction\": 0.75} tells LightGBM \u201cre-sample without replacement every 5 iterations, and draw samples of 75% of the training data\u201d. **Decrease bagging_fraction to reduce training time.**\n\n**save_binary:** If you pass parameter save_binary, the training dataset and all validations sets will be saved in a binary format understood by LightGBM. **This can speed up training next time, because binning and other work done when constructing a Dataset does not have to be re-done.**\n\n### **SUMMARY TO WIN A WAR!**\n\n**For Better Accuracy**\n\n1. Use large max_bin (may be slower)\n\n2. Use small learning_rate with large num_iterations\n\n3. Use large num_leaves (may cause over-fitting)\n\n4. Use bigger training data\n\n5. Try \"dart\"\n\n**Deal with Over-fitting**\n\n1. Use small \"max_bin\"\n\n2. Use small \"num_leaves\"\n \n3. Use \"min_data_in_leaf\" and \"min_sum_hessian_in_leaf\"\n \n4. Use bagging by set \"bagging_fraction\" and \"bagging_freq\"\n \n5. Use feature sub-sampling by set \"feature_fraction\"\n \n6. Use bigger training data\n\n7. Try \"lambda_l1\", \"lambda_l2\" and \"min_gain_to_split\" for regularization\n\n8. Try \"max_depth\" to avoid growing deep tree\n\n9. Try \"extra_trees\"\n\n10. Try increasing \"path_smooth\"\n\n\n\n","38dddbe8":"**In order not to break with the main subject, we can now return to our main topic, LightGBM. As I mentioned above, I will create and share all kinds of notebooks with you in the future with all the details and how to approach a problem! Please don't forget to upvote and follow, I'm open to all comments.**","feaa49a9":"LightGBM, a member of the Boosting family, is the most common model in Kaggle recently and we use it to quickly build base models. Should I say it's a weapon? So if the idea of boosting is \"can we create a strong student out of weak students?\" Did you know that it was first proposed by Michael Kearns in 1988  based on the question? Didn't you know? Now you know, and you can start this war stronger. [The Hypothesis](https:\/\/www.cis.upenn.edu\/~mkearns\/papers\/boostnote.pdf)","e8a42384":"Importing necessary libraries;","613d8f1e":"I will teach you what your weapons are to prepare for war, which weapon to use in this warbook. Later in a series I'll explain other weapons and food, drinks and of course what your lover is. Let's start with the first weapon called LightGBM which will be the main subject of this warbook.","6ac34682":"**First look into the train set and test set**","d7c1bffb":"### **Observations**\n\nNumber of the white colored creatures are the highest in the both sets\n\nClear colored cretures are following the white colored creatures.\n\nThe other colors nearly have same frequncies. \n\nSo, the model will probably learn white and clear colors better.","0e8ffcdf":"# **SHALL WE BEGIN?**\n\n**With all this theory, I see the fear I don't want to see in your eyes.** \n\n**I hear the roar of battle axes in your ears.** \n\n**Today is not that day. Today is not the day you will fear, today is the day you will fight...**\n\n**For all you have, for your freedom, SPARTANS, draw your swords, we go!**","20dc117c":"# **APPLICATIONS and METRICS**\n\nYour weapon works with the applications and metrics below. You can use whatever you need. For this warbook, we will learn to use it for classification problem.\n\nLightGBM supports the following applications:\n\n* regression, the objective function is L2 loss\n\n* binary classification, the objective function is logloss\n\n* multi classification\n\n* cross-entropy, the objective function is logloss and supports training on non-binary labels\n\n* lambdarank, the objective function is lambdarank with NDCG\n\nLightGBM supports the following metrics:\n\n* L1 loss\n\n* L2 loss\n\n* Log loss\n\n* Classification error rate\n\n* AUC\n\n* NDCG\n\n* MAP\n\n* Multi-class log loss\n\n* Multi-class error rate\n\n* AUC-mu (new in v3.0.0)\n\n* Average precision (new in v3.1.0)\n\n* Fair\n\n* Huber\n\n* Poisson\n\n* Quantile\n\n* MAPE\n\n* Kullback-Leibler\n\n* Gamma\n\n* Tweedie","e4be8055":"Let's call these features hyperparameters, and as your hyperparameters change, that is, as the characteristics of your weapon change, the result of the war changes. For this reason, it is necessary to choose hyperparameters carefully. There are many ways to choose the optimum features, but they will be the subject of another warbook. For now, let's see what these hyperparameters do ... Let's see that before we know the enemy, let's get to know the weapon we have!","72d83af8":"Hello beginners, I know you intend to go on a long journey. This journey you will embark on does not promise you a peaceful life in rose gardens, if you are told that it is and you think it will, you can return at the beginning of the road. The rest, you, too, prepare yourself for blood and tears and hug your loved ones for the last time. Be sure to put on all your weapons, get enough food and drink, and look into your lover's eyes for the last time on this journey. **SPARTANS! SHALL WE BEGIN?!**","d1ec3e5f":"Jointplot is used to show the distribution between 2 integer or float columns and and also is used to show the distribution of a column within itself.","fb871239":"2 approaches are below, I usually prefer mapping, so, I will use mapping function here again","9aeacfa8":"### **Observations**\n\nhas_soul and bone_length columns are positive correlated columns. While bone_length increases has_soul also increases. Both columns look they have normal distribution.\n\nhair_length and rotting_flesh columns are negative correlated columns. While hair_length decreases rotting_flesh also decreases. Both columns look they have normal distribution.\n\nYou can try all float and integer columns one by one and discover the relations between the columns. **Don't forget to use same codes on the test data! If you discover a hidden pattern you can win a competition!**","64347e0d":"If you wonder the importance of the futures, take a look at the importance plot","af309a0e":"# from ZERO to HERO LightGBM Classifier","2b27cf39":"LightGBM is a weapon that can be shaped according to the war you will enter. If your battle is regression, you should use the LightGBM Regressor model, if your battle is classification, you should use the LightGBM Classifier model. Nevertheless, real war is one's war with oneself, luck with you ... There are some features that really give a weapon its strength, such as being unbreakable, light, and if possible made of dragon bone. LightGBM also has some features that give it its power, and I will show on a data set what these features are, how they should be chosen and how they should be used in a war.","0b88f4d6":"Let's check the null rows","6a48274e":"# **PRACTICE ON A DATASET**\n\nWe will use all we learnt up to now on a dataset. The dataset is a classification dataset about to classify a creature. There are 3 classes in the target feature, ghouls - goblins - ghosts. We will tyr to predict the class of the creature according to the independent features.","fa4d9fb1":"First we need to define the parameters and intervals. Parameters should be defined in a dictionary. The model will try all the intervals we defined and try to find the optimum parameters to get the best score. **You can add more parameters if you want, but don't forget, more parameters mean more time!**","6fee62f7":"**Uploading datasets.. There are 2 datasets, one of them is train set and the other is test set which doesn't contain the target feature. We will train our weapon on the train set, we will sharp our weapon on validation set which is a part of the train set, and finally we will use our sharpen weapon on the test set and see the results!**","412314f7":"## ***I will publish other notebooks as soon as possible.***","c2f37b2f":"### ***Thanks in advance for your comments and advice.***","cb885574":"# ***LIGHTGBM***\n\nThe topic of this notebook is LightGBM CLassifier, **I'll also prepare LightGBM Regressor and simple LightGBM with Hyperopt or Optuna and Bayesian Optimization!**\n\nLet's prepare the tarin_data for the LightGBM Classifier.\n\nWe will change the type of the \"color\" categorical column to the integer, and also change the target column, \"type\", to the integer.\n\nColor column is a nominal column, so we will use one-hot-encoding, but in pandas there is \"get_dummies\" function which is very useful, easy to use and do the same thing with one-hot-encoding.\n\nType column is also a nominal column but due to be the target column we should use Label Encoder. I will show you 2 approach to convert categorical columns. One of them is LabelEncoder and the other is map function. You can use whichever you want.\n\n**I will also share a notebook about Encoding Types which are game changer in the competitions. I will share the differences, the usages and the results about encoders. Keep calm and wait for the notebook!**","15d2c9ea":"# ***LightGBM Classifier [main page](https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html)***","121e0fba":"The code above lasts too long so I changed it to comment rows, if you want to run just delete the # symbols. You can also reduce the number of parameters and then run the code above. But be careful, it really lasts :)","45a5c3a7":"Countplot is used to count and group the categorical columns to observe the number of categories in the categorical columns","27e07449":"Total 3240000 fits were applied to the train set, processing lasted 8 hours 46 minutes. A very long time, and if you add more parameter the time will be longer. There are any other methods to find optimum parameters but they will be a story of another notebook.","5e35ec15":"# **IMPORTANCE is IMPORTANT**\n\nSpartans! Our weapon show us our allies and our enemies. To win a war you must know your allies and you must take care your enemies. **plot_importance** parameter plots the importance of the features. The most important features are our allies and the least important features our enemies, the main point here is that, try not to delete your enemies! If you want to get a better score, first try to solve the mystery of the least important features, try to clean the noise to see the sunset! Many of you realized that when you delete the least important features your score get worse, so, try not to delete your enemies firstly, try to make peace!"}}