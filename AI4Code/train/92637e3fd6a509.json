{"cell_type":{"41031a3e":"code","56addafd":"code","fba61b02":"code","76fd5663":"code","ef878616":"code","591dd479":"code","2ad216fd":"code","5b81e44e":"code","17cba527":"code","e25e9410":"code","cf3e54f3":"code","6f37c7de":"code","0d24f270":"code","ab7ebe53":"code","df7f7f94":"code","3f62327d":"code","de133c4f":"code","308cc70c":"code","fac6220a":"code","cc2da424":"markdown","a3092f5f":"markdown","f36088dd":"markdown","6eb1d29a":"markdown","27eeaa0a":"markdown","0bfcb124":"markdown","9ba111e7":"markdown","d11ef91b":"markdown"},"source":{"41031a3e":"import numpy as np\nimport lmdb\nimport msgpack\nimport logging\nimport msgpack_numpy\nimport pickle\nimport psutil\nimport os \nimport gc\nimport math\n\nfrom time import time\nfrom contextlib import contextmanager\nfrom tqdm.auto import tqdm","56addafd":"# custom utility for reporting mem usage for each chunk of code wrapped with the timer utility below...\n\nclass Colors:\n    \"\"\"Defining Color Codes to color the text displayed on terminal.\"\"\"\n\n    blue = \"\\033[94m\"\n    green = \"\\033[92m\"\n    yellow = \"\\033[93m\"\n    red = \"\\033[91m\"\n    end = \"\\033[0m\"\n\n\ndef color(string: str, color: Colors = Colors.yellow) -> str:\n    return f\"{color}{string}{Colors.end}\"\n\n\n@contextmanager\ndef timer(label: str) -> None:\n    \"\"\"\n    Compute the time the code block takes to run.\n    \"\"\"\n\n    p = psutil.Process(os.getpid())\n    start = time()  # Setup - __enter__\n    m0 = p.memory_info()[0] \/ 2.0 ** 30\n    print(color(f\"{label}: Start at {start}; RAM USAGE AT START {m0}\"))\n    try:\n        yield  # yield to body of `with` statement\n    finally:  # Teardown - __exit__\n        m1 = p.memory_info()[0] \/ 2.0 ** 30\n        delta = m1 - m0\n        sign = \"+\" if delta >= 0 else \"-\"\n        delta = math.fabs(delta)\n        end = time()\n        print(\n            color(\n                f\"{label}: End at {end} ({end - start} elapsed); RAM USAGE AT END {m1:.3f}GB ({sign}{delta:.3f}GB)\",\n                color=Colors.red,\n            )\n        )","fba61b02":"# sample usage for the above timer\n\nwith timer(\"Hey! You Are Awesome! Thanks for passing by!\"):\n    pass","76fd5663":"# We can even use pickle and other things mentioned above as well, just be careful with what you are using and how it works, adjust the things accordingly!\n\nclass MsgpackSerializer:\n\n    def __init__(self, raw : bool = False):\n        \"\"\"If true, unpack msgpack raw to Python bytes. \n        Otherwise, unpack to Python str by decoding with UTF-8 encoding.\n        Ref-: https:\/\/msgpack-python.readthedocs.io\/en\/latest\/api.html#msgpack.Unpacker\n        \"\"\"\n        self._raw = raw\n\n    @staticmethod\n    def serialize(vector : np.array) -> bytes:\n        \"\"\"serialize's a numpy-vector using msgpack to bytes.\n        \"\"\"\n        return msgpack.packb(vector, default = msgpack_numpy.encode)\n\n    def deserialize(self, serialized_vector : msgpack.packb) -> np.ndarray:\n        \"\"\"de-serialize's a vector using msgpack from bytes back to it'sorignal\n        serialized_vector state.\n        \"\"\"\n        return msgpack.unpackb(\n            serialized_vector,\n            object_hook = msgpack_numpy.decode,\n            raw = self._raw\n        )\n\n# let's create an object for the same.\nser = MsgpackSerializer()","ef878616":"# sample usage of the above MsgpackSerializer...\n\nwith timer(\"Using msgpack as our serialiser\"):\n    \n    # let's create a dummy vec\n    dummy_vec = np.random.rand(100)\n    \n    # let's serialise it using the util above\n    serialised_dummy_vec = ser.serialize(vector = dummy_vec)\n    \n    # let's get back our original vector\n    deserialised_dummy_vec = ser.deserialize(serialized_vector = serialised_dummy_vec)\n    assert np.allclose(dummy_vec, deserialised_dummy_vec) == True, \"Vectors aren't same, Please Stop and Check\"\n    \n    # cleanup..\n    del dummy_vec, serialised_dummy_vec, deserialised_dummy_vec\n    gc.collect()","591dd479":"# This is what's writing to the lmdB.\n\nfrom typing import Iterator, Tuple\n\nclass LmdbEmbeddingsWriter:\n    \n    def __init__(self, embeddings_generator: Iterator[Tuple[str, float]], serializer : MsgpackSerializer = MsgpackSerializer.serialize):\n        \"\"\"Pass in the embeddings_generator func (a callable) and a vector serializer defined, it\n        will write the embedding's content to the lmdb db by using the generator on fly.\n        \"\"\"\n        self.embeddings_generator = embeddings_generator\n        self.serializer = serializer\n        self._batch_size = 2**12\n        self._map_size = 100 * 1024 * 1024 * 1024\n\n    def write(self, path):\n        \"\"\"Let's write the embeddings to the given file path.\n        Ref -> https:\/\/lmdb.readthedocs.io\/en\/release\/#environment-class\n        \"\"\"\n        \n        environment = lmdb.open(path, map_size = self._map_size)\n        transaction = environment.begin(write = True)\n\n        for i, (word, vector) in enumerate(self.embeddings_generator):\n\n            encoded_word = word.encode(encoding = 'UTF-8')\n            transaction.put(encoded_word, self.serializer(vector))\n\n            if i % self._batch_size == 0:\n                # https:\/\/lmdb.readthedocs.io\/en\/release\/#lmdb.Transaction\n                # we commit the pending transactions to db in chunks.\n                transaction.commit()\n                transaction = environment.begin(write = True)\n        \n        # commit any remaining transactions as well. (leftovers if bs is not perfectly-divisible)\n        transaction.commit()","2ad216fd":"# This is what's going go read the data from the lmdB.\n\nclass LmdbEmbeddingsReader:\n\n    def __init__(self, path : str, deserializer : MsgpackSerializer = ser.deserialize, **kwargs):\n        \"\"\"for the args passed to the env, please refer to the lmdb docs.\n        Rrf https:\/\/lmdb.readthedocs.io\/en\/release\/#environment-class\n        \"\"\"\n        self.MAX_READERS = 2**12\n        self.deserializer = deserializer\n        self.environment = lmdb.open(\n            path,\n            readonly = True,\n            max_readers = self.MAX_READERS,\n            max_spare_txns = 2,\n            lock = kwargs.pop('lock', False),\n            readahead=False,\n            **kwargs,\n        )\n\n    def get_word_vector(self, word : str) -> np.ndarray:\n        \"\"\"fetch the given word vector from the lmdB database.\n        \"\"\"\n        with self.environment.begin() as transaction:\n            word_vector = transaction.get(word.encode(encoding = 'UTF-8'))\n            if word_vector is None:\n                raise '\"%s\" does not exist in the database.' % word\n            return self.deserializer(serialized_vector=word_vector)","5b81e44e":"# let's load the mammoth emebddings file in one shot...\n# we could have streamed it directly ss well in case it was a txt file etc!\n\nwith timer(\"Reading Embeddings dumped as pkl.. Way faster than loading using bin\/vec... (Thanks @authman)\"):\n    with open('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', 'rb') as fp:\n        embeddings = pickle.load(fp)","17cba527":"# let's just check it got loaded properly...\nlen(embeddings.keys()) # cool, so we have 2196008 tokens here!","e25e9410":"embeddings[','] # embedding representation for the \",\"","cf3e54f3":"# let's dump the words the above dict's keys i.e our embeddings words into a txt file.\nfrom tqdm.auto import tqdm\n\noutput_dir = \".\/\"\nwith timer(\"Dumping the words into vocab.txt file...\"):\n    with open(os.path.join(output_dir, \"vocabulary.txt\"), \"w\", encoding='utf-8') as f:\n        for word in tqdm(embeddings.keys()):\n            f.write(word+\"\\n\")","6f37c7de":"!ls -lh *.txt # let's just check the file is there","0d24f270":"! head -n 5 vocabulary.txt # first 5 words are...","ab7ebe53":"def embeddings_generator(embeddings:dict) -> Tuple[str, np.ndarray]:\n    \"\"\"we are kinda streaming our embedding matrix here\n    to write it the the lmdb.\n    \"\"\"\n    for idx, vector in tqdm(enumerate(embeddings.values())):\n        yield str(idx), vector","df7f7f94":"%%time\n\n# let's write the matrix to lmdb. (will take some while ~ 60 secs, hang on..)\n\nwith timer(\"writing to lmdb database\"):\n    lmdb_writer = LmdbEmbeddingsWriter(embeddings_generator=embeddings_generator(embeddings))\n    lmdb_writer.write(path=\"lmdb_dir\")","3f62327d":"# cool, let's define the lmdb reader now, since we want to retrieve what we wrote.\nlmdb_reader = LmdbEmbeddingsReader(path=\"\/kaggle\/working\/lmdb_dir\")","de133c4f":"# let's check for a sample word, vectors we receive should be same from both i.e lmdb and the original matrix.\n# sanity check\nwith timer(\"checking for same vectors\"):\n    assert np.allclose(embeddings[\",\"], lmdb_reader.get_word_vector('0')) == True, \"Data is not closeby range, Please Stop and verify the same\"\n    assert np.allclose(embeddings[\".\"], lmdb_reader.get_word_vector('1')) == True, \"Data is not closeby range, Please Stop and verify the same\"\n    assert np.allclose(embeddings[\"the\"], lmdb_reader.get_word_vector('2')) == True, \"Data is not closeby range, Please Stop and verify the same\"","308cc70c":"embeddings[\",\"]","fac6220a":"lmdb_reader.get_word_vector('0')","cc2da424":"# Numpy vector serialising strategy\n\n- Commonly used modules are (not specific to numpy only)\n    - msgpack (msgpack-numpy)\n    - Pickle \/ c-Pickle\n    - Apache Thrift\n    - JSON\n    - Apache Avro\n    - numpy's to string\/buffer\/fromstring etc\n    \nBut before proceeding further, What do you think serialization and de-serialization means? From the wiki!\n\n- **Serialization** -> Serialization is the process of translating a data structure or object state into a format that can be stored (for example, in a file or memory data buffer) or transmitted (for example, over a computer network) and reconstructed later (possibly in a different computer environment).\n\n- **De-serialization** -> De-serialization is the reverse of the above process, taking data structured from some format and rebuilding it into an object.","a3092f5f":"# utility\n\n- timer -> wrap any chunk of code to see the mem it takes","f36088dd":"# lmdb section\n\n- Below we have defined two self explainable functions which are simple wrapper for our task over the underlying modules so that it's easy to sue for the next set of steps.\n    - LmdbEmbeddingsWriter -> Helps us to write the serialized numpy vector's to the dB.\n    - LmdbEmbeddingsReader -> Helps us to get back the numpy vecotr's from the dB.\n\nRef-: https:\/\/lmdb.readthedocs.io\/en\/release\/","6eb1d29a":"# Imports\n\n- Let's import some useful stuffs here.","27eeaa0a":"## Writing to the lmdb","0bfcb124":"Thanks for passing by! \n\nLet me kow if you have any questions, i will try to answer them to the best of my knowledge!\n\nBest,\n\nAditya.\n\nadityaecdrid@gmail.com","9ba111e7":"# Main Section Goes Here","d11ef91b":"# Intro\n\n**The code below is for the post that i made on LinkedIn. Read [here](https:\/\/www.linkedin.com\/posts\/aditya-ecdrid_nlp-ml-dl-activity-6804976554435866624-abBM)**\n\nI hope it's useful! Let me know your thoughts about the drawbacks as well and how it helepd you!**\n\nBest,\n\nAditya.\n\n[my-linkedin](https:\/\/in.linkedin.com\/in\/aditya-ecdrid), [post_link](https:\/\/www.linkedin.com\/posts\/aditya-ecdrid_nlp-ml-dl-activity-6804976554435866624-abBM)"}}