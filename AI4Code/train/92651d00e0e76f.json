{"cell_type":{"621de5bf":"code","bbe6e5c7":"code","92653acd":"code","ad9c833e":"code","a4a17071":"code","88c12e0d":"code","204694df":"code","e40e5df0":"code","30b72c4c":"code","0e65cfc8":"code","04575f04":"code","62a01324":"code","7815ef0c":"code","5bffff47":"code","81c234c5":"code","95126c9b":"code","95e11c82":"code","6d65a100":"code","e24b93a4":"code","f90967d8":"code","d226c669":"code","51252c93":"code","f0a6fcf9":"code","587df1f9":"code","9f144f3c":"code","5559171e":"code","49018b0b":"code","53632c46":"code","d2de3ebc":"code","838594ea":"code","c5ac9a23":"code","c01d6b9c":"code","22f5c401":"code","185e1dca":"code","0e83f304":"code","518f2672":"code","adc02166":"code","1aef199a":"markdown","ccd33595":"markdown","049818fd":"markdown","df867cb5":"markdown","70a87695":"markdown","5f2a9d47":"markdown","5e66837f":"markdown","fcbaaec2":"markdown","6d75b35a":"markdown","22c33dea":"markdown","9b779806":"markdown","c2964492":"markdown","0d7b43e5":"markdown"},"source":{"621de5bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bbe6e5c7":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier, plot_importance\nfrom lightgbm import LGBMClassifier\n\nimport optuna","92653acd":"%%time\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","ad9c833e":"train","a4a17071":"test","88c12e0d":"train.target.value_counts()","204694df":"enc = {'Class_1':0,'Class_2':1,'Class_3':2,'Class_4':3}\ntrain.target.replace(enc,inplace=True)\ntrain.head(5)","e40e5df0":"train.drop(columns='id',inplace=True)\ntest.drop(columns='id',inplace=True)","30b72c4c":"train.info()","0e65cfc8":"test.info()","04575f04":"%%time\n\nig, ax = plt.subplots(figsize=(20 , 20))\n\ncorr_mat = train.corr()\n\nmask = np.zeros_like(corr_mat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n\nsns.heatmap(corr_mat,\n        square=True, center=0, linewidth=0.2,\n        cmap='inferno',\n        mask=mask, ax=ax) \n\nax.set_title('Correlation Matrix')\nplt.show()","62a01324":"fig, ax = plt.subplots()\nplt.grid()\nsns.countplot(x='target', data=train, ax=ax)\nax.set_ylim(0, 60000)\nax.set_title('Target Distribution')\n\nplt.show()","7815ef0c":"train.describe().T","5bffff47":"plt.figure(figsize=(20,8))\nplt.grid()\nplt.xticks(rotation=90)\nplt.stem(train.drop(columns='target').columns, \n         train.drop(columns='target').describe().T['mean']-test.describe().T['mean'])","81c234c5":"#train.drop(columns='feature_19',inplace=True)\n#test.drop(columns='feature_19',inplace=True)","95126c9b":"cols = train.columns\n\nn_col = 2\nn_row = round(len(cols)\/2)\nsize = (n_col * 10, n_row * 4.5)\n\n#Create figure\nplt.subplots(n_row,n_col,figsize=size)\n\nfor  i ,feature  in enumerate(cols , 1):\n    plt.subplot(n_row, n_col , i)\n    sns.countplot(x = feature, data = train)\n    plt.xlabel(feature, fontsize=9); plt.legend()\nplt.show()","95e11c82":"#Create figure\nplt.subplots(n_row,n_col,figsize=size)\n\nfor  i ,feature  in enumerate(cols , 1):\n    plt.subplot(n_row, n_col , i)\n    sns.boxenplot(x = feature, data = train)\n    plt.xlabel(feature, fontsize=9); plt.legend()\nplt.show()","6d65a100":"sns.violinplot(x = 'feature_13', y='target', data = train)","e24b93a4":"from sklearn.manifold import TSNE\nimport umap\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA","f90967d8":"X = train.drop(columns='target').to_numpy()\ny = train['target'].to_numpy()","d226c669":"reducer = umap.UMAP(random_state=42,n_components=2)\nembedding = reducer.fit_transform(X)","51252c93":"plt.figure(figsize=(10,10))\nplt.grid()\nsns.scatterplot(x = reducer.embedding_[:, 0], y = reducer.embedding_[:, 1], \n                hue='target', data=train)","f0a6fcf9":"model = XGBClassifier(tree_method = 'gpu_hist' ,\n                      use_label_encoder=False)    #using xgboost default parameters\nmodel.fit(X,y)","587df1f9":"fig, ax = plt.subplots(figsize=(10,10))\nplot_importance(model,\n                height=0.5,\n               max_num_features=None,\n               title='Feature importance',\n                xlabel='F score', \n                ylabel='Features',\n               ax=ax)","9f144f3c":"train.shape, test.shape","5559171e":"from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import log_loss","49018b0b":"def objective(trial):\n    X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.3,random_state=42) \n    \n    params = {\n        'booster' : 'gbtree',\n        'tree_method' : 'gpu_hist' , \n        'use_label_encoder' : False , \n        'eval_metric': 'mlogloss',\n        'lambda' : trial.suggest_loguniform('lambda' , 1e-5 , 1.0),  #L2 regularisation\n        #'alpha' : trial.suggest_loguniform('alpha' , 1e-5 , 1.0),   #L1 regularisation\n        'colsample_bytree' : 0.2,\n        'subsample' : 0.8,\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.001 , 0.01),\n        'n_estimators' : trial.suggest_int('n_estimators' , 5000 , 30000),\n        'max_depth' : trial.suggest_int('max_depth' , 3 , 25),\n        'random_state' : 42,\n        'min_child_weight' : trial.suggest_int('min_child_weight' , 1 , 300),\n        'gamma' : trial.suggest_loguniform('gamma' , 1e-5 , 1.0)\n    }\n    \n    model = XGBClassifier(**params)\n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 200, verbose = False)\n    y_pred = model.predict_proba(X_val)\n        \n    return log_loss(y_val, y_pred)","53632c46":"study = optuna.create_study(direction='minimize', study_name = 'xgbclassifier') \nstudy.optimize(objective, n_trials=15)","d2de3ebc":"print('XGBoost Score')\nprint()\nprint('Best trial params: ' , study.best_trial.params)\nprint()\nprint('Best CV: ' , study.best_value)","838594ea":"X_test = test.to_numpy()","c5ac9a23":"import gc","c01d6b9c":"del X\ndel y\ngc.collect()","22f5c401":"preds = np.zeros((X_test.shape[0],4))\nkf = StratifiedKFold(n_splits = 5 , random_state = 42 , shuffle = True)\n\nbest_params = study.best_trial.params\nbest_params['eval_metric'] = 'mlogloss'\nbest_params['tree_method'] = 'gpu_hist'\nbest_params['booster'] = 'gbtree'\nbest_params['use_label_encoder'] = False\nbest_params['colsample_bytree'] =0.2\nbest_params['subsample'] = 0.8\nbest_params['random_state'] = 42\n\nmodel = XGBClassifier(**best_params)\nmulti_logloss =[]\nn=0\n\ncols = train.drop(columns='target').columns\n\nfor train_idx, test_idx in kf.split(train[cols], train['target']):\n    \n    X_train, X_val = train[cols].iloc[train_idx], train[cols].iloc[test_idx]\n    y_train, y_val = train['target'].iloc[train_idx], train['target'].iloc[test_idx]\n    \n    model.fit(X_train,y_train,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    \n    preds += model.predict_proba(X_test)\/kf.n_splits\n    multi_logloss.append(log_loss(y_val, model.predict_proba(X_val)))\n    print(n+1,multi_logloss[n])\n    n+=1","185e1dca":"test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","0e83f304":"xgb_submission = pd.DataFrame(preds,columns=['Class_1','Class_2','Class_3','Class_4'])\nxgb_submission['id']  = test['id']\nxgb_submission = xgb_submission[['id','Class_1','Class_2','Class_3','Class_4']]","518f2672":"xgb_submission","adc02166":"output = xgb_submission.to_csv('submission_xgboost',index=False)","1aef199a":"# Feature Importance using XGBoost default Parameters","ccd33595":"# Visualisation in Reduced Dimension using UMAP, LDA & t-SNE ","049818fd":"# Boxenplots or Letter Value Plots","df867cb5":"# Studying feature wise difference in distribution ","70a87695":"# Violin Plots","5f2a9d47":"Optuna is an open source hyperparameter optimization framework to automate hyperparameter search. It runs trials by choosing hyperparmaters according to the distribution (uniform, categorical, integer)  and hence acclerating our search for near optimal hyperparameters.","5e66837f":"# Distribution Study using Countplots","fcbaaec2":"This is the UMAP 2D projection of the 50 dimensional original data. UMAP is considered better than t-SNE in terms of computational power as well as better clustering. t-SNE uses KL divergence as a loss function for optimisation which gives emphasis to conserve local structure i.e. intra-cluster data points are guaranteed to be similar while inter-cluster data points are not.","6d75b35a":"# Submission","22c33dea":"# Best Parameters","9b779806":"**UMAP**","c2964492":"# Optimisation using Optuna framework","0d7b43e5":"Conventional boxplots are useful displays for conveying rough information about the central 50% and the extent of data. For small-sized data sets (n < 200), detailed estimates of tail behavior beyond the quartiles may not be trustworthy. Larger data sets (n ~ 10,000-100,000) afford more precise estimates of quantiles beyond the quartiles. Boxenplot (or letter value plot) conveys more detailed information in the tails using letter values, but only to the depths where the letter values are reliable estimates of their corresponding quantiles."}}