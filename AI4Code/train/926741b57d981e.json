{"cell_type":{"409629c2":"code","8c5949e9":"code","5edb4b6d":"code","9c4f1271":"code","d42dc025":"code","2e078730":"code","4c7f7dcf":"code","9972582f":"code","945f5809":"code","aad8b0dd":"code","a14325ea":"code","043c83e6":"code","63f83452":"code","181dd543":"code","59e5f21b":"code","491e97ce":"code","7e31b2d0":"code","60fec16e":"code","f7f83a1e":"code","87dc1a6d":"code","6ea59274":"code","0296b5c2":"code","de3f5316":"code","1d83df0c":"code","88077c7d":"code","d1f5c7b2":"code","1a3d2c94":"code","1e085439":"code","79aa6fac":"code","c8a85839":"code","996f8735":"code","b9b42775":"code","43bad057":"code","c304260f":"code","f1e20d41":"code","49590ab2":"code","4540e258":"code","4b593216":"code","909bbfe8":"code","1b431630":"markdown","5eee7888":"markdown","16794bd9":"markdown","c1a5e1b0":"markdown","4303fe40":"markdown","773348ba":"markdown","6ef4ddb8":"markdown","108383fc":"markdown","8d6faa99":"markdown","d6e1e109":"markdown","70c922dd":"markdown","e50b3c9d":"markdown","00e01e04":"markdown","419e4553":"markdown","1409c823":"markdown","0a65248b":"markdown","4378aefa":"markdown","7e99ace2":"markdown","54b05028":"markdown","6156b37c":"markdown","48f70a3c":"markdown"},"source":{"409629c2":"# supress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Importing Libraries\nimport numpy as np\nimport pandas as pd\n\n# For Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To Scale our data\nfrom sklearn.preprocessing import scale\n\n# To perform KMeans clustering \nfrom sklearn.cluster import KMeans\n\n# To perform Hierarchical clustering\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","8c5949e9":"retail = pd.read_csv(\"..\/input\/Online Retail.csv\", sep = ',',encoding = \"ISO-8859-1\", header= 0)\nretail.head()","5edb4b6d":"retail.shape","9c4f1271":"retail.describe()","d42dc025":"retail.info()","2e078730":"# parse date\nretail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'], format = \"%d-%m-%Y %H:%M\")\nretail.head()","4c7f7dcf":"# Let's look top 5 rows\nretail.head()","9972582f":"#Na Handling\nretail.isnull().values.any()","945f5809":"retail.isnull().sum()*100\/retail.shape[0]","aad8b0dd":"#dropping the na cells\nretail = retail.dropna()","a14325ea":"retail.isnull().sum()*100\/retail.shape[0]","043c83e6":"#RFM implementation\n\n# Extracting amount by multiplying quantity and unit price and saving the data into amount variable.\nretail[\"Amount\"]  = retail.Quantity * retail.UnitPrice","63f83452":"# Monetary Function\n\n# Finding total amount spent per customer\nmonetary = retail.groupby(\"CustomerID\").Amount.sum()\nmonetary = monetary.reset_index()\nmonetary.head()","181dd543":"#Frequency function\n\n# Getting the count of orders made by each customer based on customer ID.\nfrequency = retail.groupby(\"CustomerID\").InvoiceNo.count()\nfrequency = frequency.reset_index()\nfrequency.head()","59e5f21b":"#creating master dataset\nmaster = monetary.merge(frequency, on = \"CustomerID\", how = \"inner\")\nmaster.head()","491e97ce":"# Finding max data\nmaximum = max(retail.InvoiceDate)","7e31b2d0":"# Adding one more day to the max data, so that the max date will have 1 as the difference and not zero.\nmaximum = maximum + pd.DateOffset(days = 1)","60fec16e":"retail['diff'] = maximum - retail.InvoiceDate\nretail.head()","f7f83a1e":"#Dataframe merging by recency\nrecency = retail.groupby('CustomerID').diff.min()\nrecency = recency.reset_index()\nrecency.head()","87dc1a6d":"#Combining all recency, frequency and monetary parameters\nRFM = master.merge(recency, on = \"CustomerID\")\nRFM.columns = ['CustomerID','Amount','Frequency','Recency']\nRFM.head()","6ea59274":"RFM.info()","0296b5c2":"# outlier treatment for Amount\nfig, axs = plt.subplots(1,3, figsize = (15,5))\n\nsns.boxplot(RFM.Amount, ax = axs[0])\nsns.boxplot(RFM.Frequency, ax = axs[1])\nsns.boxplot(RFM.Recency.dt.days, ax = axs[2])\n\nplt.tight_layout\nplt.show()","de3f5316":"# outlier treatment for Amount\nQ1 = RFM.Amount.quantile(0.25)\nQ3 = RFM.Amount.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Amount >= Q1 - 1.5*IQR) & (RFM.Amount <= Q3 + 1.5*IQR)]","1d83df0c":"# outlier treatment for Frequency\nQ1 = RFM.Frequency.quantile(0.25)\nQ3 = RFM.Frequency.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Frequency >= Q1 - 1.5*IQR) & (RFM.Frequency <= Q3 + 1.5*IQR)]","88077c7d":"# outlier treatment for Recency\nQ1 = RFM.Recency.quantile(0.25)\nQ3 = RFM.Recency.quantile(0.75)\nIQR = Q3 - Q1\nRFM = RFM[(RFM.Recency >= Q1 - 1.5*IQR) & (RFM.Recency <= Q3 + 1.5*IQR)]","d1f5c7b2":"fig, axs = plt.subplots(1,3, figsize = (15,5))\n\nsns.boxplot(RFM.Amount, ax = axs[0])\nsns.boxplot(RFM.Frequency, ax = axs[1])\nsns.boxplot(RFM.Recency.dt.days, ax = axs[2])\n\nplt.tight_layout\nplt.show()","1a3d2c94":"RFM.head()","1e085439":"# standardise all parameters\nRFM_norm1 = RFM.drop(\"CustomerID\", axis=1)\nRFM_norm1.Recency = RFM_norm1.Recency.dt.days\n\nfrom sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\nRFM_norm1 = standard_scaler.fit_transform(RFM_norm1)","79aa6fac":"RFM_norm1 = pd.DataFrame(RFM_norm1)\nRFM_norm1.columns = ['Frequency','Amount','Recency']\nRFM_norm1.head()","c8a85839":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","996f8735":"hopkins(RFM_norm1)","b9b42775":"from sklearn.metrics import silhouette_score\nsse_ = []\nfor k in range(2, 15):\n    kmeans = KMeans(n_clusters=k).fit(RFM_norm1)\n    sse_.append([k, silhouette_score(RFM_norm1, kmeans.labels_)])","43bad057":"plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1]);","c304260f":"# sum of squared distances\nssd = []\nfor num_clusters in list(range(1,21)):\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=100)\n    model_clus.fit(RFM_norm1)\n    ssd.append(model_clus.inertia_)\n\nplt.plot(ssd)","f1e20d41":"# Kmeans with K=5\nmodel_clus5 = KMeans(n_clusters = 5, max_iter=50)\nmodel_clus5.fit(RFM_norm1)","49590ab2":"# analysis of clusters formed\nRFM.index = pd.RangeIndex(len(RFM.index))\nRFM_km = pd.concat([RFM, pd.Series(model_clus5.labels_)], axis=1)\nRFM_km.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency', 'ClusterID']\nRFM_km.head()","4540e258":"RFM_km.Recency = RFM_km.Recency.dt.days\nkm_clusters_amount = pd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Amount.mean())\nkm_clusters_frequency = pd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Frequency.mean())\nkm_clusters_recency = pd.DataFrame(RFM_km.groupby([\"ClusterID\"]).Recency.mean())","4b593216":"df = pd.concat([pd.Series([0,1,2,3,4]), km_clusters_amount, km_clusters_frequency, km_clusters_recency], axis=1)\ndf.columns = [\"ClusterID\", \"Amount_mean\", \"Frequency_mean\", \"Recency_mean\"]\ndf.head()","909bbfe8":"fig, axs = plt.subplots(1,3, figsize = (15,5))\n\nsns.barplot(x=df.ClusterID, y=df.Amount_mean, ax = axs[0])\nsns.barplot(x=df.ClusterID, y=df.Frequency_mean, ax = axs[1])\nsns.barplot(x=df.ClusterID, y=df.Recency_mean, ax = axs[2])\nplt.tight_layout()            \nplt.show()","1b431630":"### Extracting R (Recency), F (Frequency), M (Monetary) columns form the data.","5eee7888":"#### Frequency Value","16794bd9":"## K-Means with some K","c1a5e1b0":"## RFM","4303fe40":"## K- Means Clustering ","773348ba":"## Hopkins Statistics:\nThe Hopkins statistic, is a statistic which gives a value which indicates the cluster tendency, in other words: how well the data can be clustered.\n\n- If the value is between {0.01, ...,0.3}, the data is regularly spaced.\n\n- If the value is around 0.5, it is random.\n\n- If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster.","6ef4ddb8":"### Problem Statement:\nTo identify different segments of customers on the basis of their shopping behaviour to run targeted marketing campaign.\n\n### Data:\nOnline retail is a transnational data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\n### Approach:\nWe will build a RFM clustering to identify different customers.","108383fc":"### Data quality check and cleaning","8d6faa99":"### RFM combined DataFrame","d6e1e109":"## Inferences\n1. On RFM analysis it was found that Customers with Cluster ID 4 are the best and loyal customers.\n2. Cluster Id 3 are customers who are not interested in retail store.\n3. People with cluster ID 0,1,2 needs targeted marketing marketing based on their demographics, buying pattern etc. ","70c922dd":"### Data Cleaning","e50b3c9d":"#### Monetary Value","00e01e04":"### Scaling the RFM data","419e4553":"### Outlier Treatment","1409c823":"## Silhouette Analysis\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","0a65248b":"# Customer Segmentation","4378aefa":"### Data Inspection","7e99ace2":"### Recency Value","54b05028":"##### Merging Amount and Frequency columns","6156b37c":"## Sum of Squared Distances","48f70a3c":"### Data Reading and Understanding"}}