{"cell_type":{"28d74402":"code","29cb6955":"code","f87fec6e":"code","b33e46b7":"code","3c4c21f0":"code","9de1f697":"code","d788b2af":"code","cd583d1e":"code","d4be3039":"code","7d113e19":"code","758974f9":"code","0ee5bd4d":"code","cb13dc93":"code","aeedc797":"code","1d64edcd":"code","5f3563be":"code","21fb4f6f":"code","ab925320":"code","1d84871b":"code","1d2e8fc9":"code","423b5938":"code","17a0523b":"code","df88b9fe":"code","31892bf1":"code","53a88b52":"code","50a62bd8":"code","3237006c":"code","08515848":"code","e571ab94":"code","0e4bc1ff":"code","a8288280":"code","c11c5ff5":"code","599c80d7":"code","fc251e5c":"code","e80501b1":"code","fdb10b74":"code","d501eba1":"code","5faceb45":"code","08a1c4cf":"code","67c4a48b":"code","e1c3690c":"code","08d95d1e":"code","c814a96f":"code","1bb7441d":"code","156e05ad":"code","5e457e2f":"code","10494b29":"code","fb596ddc":"code","e9396cd4":"code","7c49bda4":"code","bf6a4e7c":"code","6e8c331f":"code","d4715974":"code","28606046":"code","395573fe":"code","33afcf1d":"code","fb17439b":"code","a3fc50b5":"code","75aaf166":"code","58210bae":"code","7741033c":"code","d1ce6079":"code","d5212872":"code","3bbc087a":"code","9dc77b0d":"code","5305e4cd":"code","b4f3e4dc":"code","037ef657":"code","883ced29":"code","af10eca1":"code","89d30eef":"code","99b1a850":"code","23784ef4":"code","9fcd5303":"code","68946436":"code","a684929d":"code","e0f8a2e7":"code","e655855a":"code","ddab7d8d":"code","0765f27d":"code","6bd61e49":"code","16bcde20":"code","35b855c4":"code","251dbc57":"code","bd57aa59":"code","513c79e8":"code","f7cfe317":"code","3b02d308":"code","41461b57":"code","5330d741":"code","ede5cd6e":"code","e0b6bbfb":"code","2cf06534":"code","5c6b5997":"code","d970bec0":"code","30f2f12e":"code","33680565":"code","1454ebe7":"code","6c02d217":"code","db77dcc8":"code","15f29442":"code","2d7df091":"code","ce597fb1":"code","3f9a6a81":"code","73b68719":"code","ff6074cc":"code","b7f513bb":"code","82c31c9b":"code","f17d7b35":"code","ac2ed7ed":"code","da38ad4d":"code","b0bab000":"code","4eda059c":"code","8f9c0bc5":"code","1c7b3815":"code","65eae9de":"code","6538e6a8":"code","a7f08fc2":"code","0393c8d9":"code","e632c666":"code","a5b15c75":"code","ca800ca4":"code","739dfb2d":"code","bd8835a6":"code","bcc364fe":"code","fdb1eb94":"code","e10e2c5a":"code","d400c25e":"code","b3497751":"code","4ed45da4":"code","4a1cdec9":"code","6db0e012":"code","dcdd63dc":"markdown","b7f2a206":"markdown","1605efeb":"markdown","c43d7fdb":"markdown","5b91e08a":"markdown","2da3562b":"markdown","d57fa1f1":"markdown","f48cc621":"markdown","80d0b201":"markdown","947e90e3":"markdown","3d3c8172":"markdown","61339bd9":"markdown","dec10ae1":"markdown","5f16937d":"markdown","bcac0b59":"markdown","8487dfd6":"markdown","a08b6673":"markdown","63dbbe96":"markdown","2b181d45":"markdown","8a1939fb":"markdown","461c48ee":"markdown","c9922ae5":"markdown","14d30aeb":"markdown","2a671e52":"markdown","ffe2b6b7":"markdown","6756be31":"markdown","3794d8dd":"markdown","21ac0f0a":"markdown","ae0d9ff1":"markdown","eb44a681":"markdown","a2272ee5":"markdown","3f8bf159":"markdown","715842f5":"markdown","7ad63386":"markdown","be555321":"markdown","7f2dc62c":"markdown","2ef13fcc":"markdown","ea17c683":"markdown","e3959d6f":"markdown","58b27ffb":"markdown","7b745631":"markdown","b850a43a":"markdown","8904be8d":"markdown","f8ebfa39":"markdown","8e8aad53":"markdown","6a74216b":"markdown","c0035825":"markdown"},"source":{"28d74402":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom category_encoders.one_hot import OneHotEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, RandomizedSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.feature_selection import RFE\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import StackingClassifier\nfrom scipy.optimize import fmin\nfrom hyperopt import hp, tpe, fmin, Trials\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n#import pydotplus as pdot\nfrom IPython.display import Image\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","29cb6955":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f87fec6e":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","b33e46b7":"train.head()","3c4c21f0":"test.head()","9de1f697":"submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\", index_col = 0)\nsubmission.head()","d788b2af":"print(plt.style.available)","cd583d1e":"sns.set(font_scale = 1.2)\nplt.style.use(['ggplot', 'bmh'])\nplt.figure(figsize=(6, 7))\nax = sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data = train)\nax.set_title(\"Barplot Showing Survival Rate of Male and Female\", y = 1.05)\nax.set_xlabel(\"Gender\", fontsize=16)\nax.set_ylabel(\"Survival Rate\", fontsize=16)\n\nplt.show()","d4be3039":"women = train.loc[train.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women*100)\n","7d113e19":"men = train.loc[train.Sex == 'male']['Survived']\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men*100)","758974f9":"sns.set(font_scale = 1.1)\nplt.style.use('dark_background')\nplt.figure(figsize=(14, 6))\nplt.subplot(121)\nmissing_train = train.isnull().sum()\nmissing_train.sort_values(ascending=False, inplace=True)\nmissing_train.plot(kind = 'bar')\nplt.xlabel(\"Features\", fontsize = 14)\nplt.ylabel(\"Count\", fontsize = 14)\nplt.title(\"Missing Values of Train Dataset\", y = 1.05)\nplt.subplot(122)\nmissing_test = test.isnull().sum()\nmissing_test.sort_values(ascending=False, inplace=True)\nmissing_test.plot(kind = 'bar')\nplt.xlabel(\"Features\", fontsize = 14)\nplt.ylabel(\"Count\", fontsize = 14)\nplt.title(\"Missing Values of Test Dataset\", y = 1.05)\nplt.show()\nplt.show()","0ee5bd4d":"missing_train[missing_train>0]","cb13dc93":"missing_test[missing_test>0]","aeedc797":"train.info()","1d64edcd":"plt.style.use('seaborn-whitegrid')\ncount, bin_edges = np.histogram(train.Fare)\ntrain['Fare'].plot(kind = 'hist', xticks = bin_edges, figsize = (8, 5))\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Fare\", y = 1.04)\nplt.show()","5f3563be":"plt.figure(figsize = (8, 5))\nsns.distplot(train.Age)\nplt.title(\"Distribution of Age\", y = 1.04)\nplt.xlabel(\"Age of Passengers\")\nplt.ylabel(\"Frequency\")\nplt.show()","21fb4f6f":"sns.set(font_scale = 1.2)\nplt.style.use('ggplot')\nplt.figure(figsize=(4, 5))\nax = sns.barplot(x = 'Embarked', y = 'Survived', data = train)\nax.set_title(\"Barplot Showing Survival Rate with Respect to Port of Embarkation\", y = 1.05)\nax.set_xlabel(ax.get_xlabel(), fontsize=16)\nax.set_ylabel(ax.get_ylabel(), fontsize=16)\nplt.show()","ab925320":"plt.figure(figsize = (8, 5))\nsns.countplot(train.Embarked)\nplt.show()","1d84871b":"sns.heatmap(train.corr(), annot = True, cmap = 'BrBG')\nplt.show()","1d2e8fc9":"full = pd.concat([train, test])\nfull.head()","423b5938":"from statistics import mode\n\nfull['Age'] = full.groupby('Pclass')['Age'].transform(lambda x: x.fillna(x.median()))\nfull.Fare = full.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\nfull[\"Embarked\"] = full[\"Embarked\"].fillna(mode(full[\"Embarked\"]))\nfull['Cabin'].fillna('U', inplace=True)\nfull.isnull().sum()","17a0523b":"full.Cabin.unique().tolist()","df88b9fe":"#Let's engineer the Cabin feature a little bit. We will go through the other steps of feature engineering later\nimport re\n\n# Extract (first) letter!\nfull['Cabin'] = full['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\nfull.Cabin.unique().tolist()","31892bf1":"plt.style.use('seaborn-whitegrid')\ncount, bin_edges = np.histogram(full.Fare)\nfull['Fare'].plot(kind = 'hist', xticks = bin_edges, figsize = (8, 5))\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Fare\", y = 1.04)\nplt.show()","53a88b52":"plt.figure(figsize = (8, 5))\nsns.distplot(full.Age)\nplt.title(\"Distribution of Age\", y = 1.04)\nplt.xlabel(\"Age of Passengers\")\nplt.ylabel(\"Frequency\")\nplt.show()","50a62bd8":"sns.set(font_scale = 1.2)\nplt.style.use('ggplot')\nplt.figure(figsize=(4, 5))\nax = sns.barplot(x = 'Cabin', y = 'Survived', data = full)\nax.set_title(\"Barplot Showing Survival Rate with Respect to Cabin Type\", y = 1.05)\nax.set_xlabel(ax.get_xlabel(), fontsize=14)\nax.set_ylabel(ax.get_ylabel(), fontsize=14)\nplt.show()","3237006c":"sns.countplot(full.Cabin)\nplt.show()","08515848":"full.head()","e571ab94":"print(\"train shape:\", train.shape, \" test shape:\", test.shape)","0e4bc1ff":"# Recover test dataset\ntest = full[full['Survived'].isna()].drop(['Survived'], axis = 1)\n\n# Recover train dataset\ntrain = full[full['Survived'].notna()]","a8288280":"print(\"train shape:\", train.shape, \" test shape:\", test.shape)","c11c5ff5":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(4, 5))\nax = sns.barplot(x = 'Pclass', y = 'Survived', data = train)\nax.set_title(\"Barplot Showing Survival Rate with Respect to Passenger Class\", y = 1.05)\nax.set_xlabel(ax.get_xlabel(), fontsize=14)\nax.set_ylabel(ax.get_ylabel(), fontsize=14)\nplt.show()","599c80d7":"plt.figure(figsize=(4, 5))\nsns.countplot(train.Pclass)\nplt.show()","fc251e5c":"plt.style.use('seaborn-poster')\nplt.figure(figsize=(4, 5))\nax = sns.barplot(x = 'SibSp', y = 'Survived', data = train)\nax.set_title(\"Barplot Showing Survival Rate of Passengers having Siblings\/Spouses\", y = 1.05)\nax.set_xlabel(ax.get_xlabel(), fontsize=14)\nax.set_ylabel(ax.get_ylabel(), fontsize=14)\nplt.show()","e80501b1":"sns.set(font_scale = 1.3)\nplt.style.use('seaborn-ticks')\nplt.figure(figsize = (4, 5))\nsns.countplot(train['SibSp'])\nplt.show()","fdb10b74":"full.Parch.unique().tolist()","d501eba1":"sns.set()\nplt.style.use('fast')\nplt.figure(figsize=(4, 5))\nax = sns.barplot(x = 'Parch', y = 'Survived', data = train)\nax.set_title(\"Barplot Showing Survival Rate of Passengers having Parents\/Children\", y = 1.05)\nax.set_xlabel(ax.get_xlabel(), fontsize=14)\nax.set_ylabel(ax.get_ylabel(), fontsize=14)\nplt.show()","5faceb45":"sns.set(font_scale = 1.2)\nplt.figure(figsize = (4, 5))\nsns.countplot(train['Parch'])\nplt.show()","08a1c4cf":"df_fare_pc = train.groupby('Pclass').mean().sort_values('Fare', ascending=False)['Fare']\ndf_fare_pc.head()","67c4a48b":"df_age_pc = train.groupby('Pclass')['Age'].mean()\ndf_age_pc.sort_values(ascending = False, inplace = True)\ndf_age_pc.head()","e1c3690c":"sns.set(font_scale = 2)\nplt.style.use('seaborn-talk')\nfig = plt.figure()\nax0 = fig.add_subplot(1, 2, 1) # add subplot 1 \nax1 = fig.add_subplot(1, 2, 2) # add subplot 2\n\n# Subplot1 = Barplot of Pclass and Fare\ndf_fare_pc.plot(kind = 'barh', figsize=(20, 6), ax=ax0, color='crimson')\nax0.set_title(\"Barplot Of Average Fare of each Passenger Class\", y = 1.04, fontsize = 17)\nax0.set_xlabel('Average Fare', fontsize = 17)\nax0.set_ylabel(ax0.get_ylabel(), fontsize = 17)\nfor index, value in enumerate(df_fare_pc):\n    label = format(int(value), ',')\n    ax0.annotate(label, color='white', xy = (value - 5, index - 0.05), fontsize = 15)\n\n# Subplot2 = Barplot of Pclass and Age\ndf_age_pc.plot(kind = 'barh', figsize=(20, 6), ax=ax1, color='green')\nax1.set_title(\"Barplot Of Average Age in each Passenger Class\", y = 1.04, fontsize = 17)\nax1.set_xlabel('Average Age', fontsize = 17)\nax1.set_ylabel(ax1.get_ylabel(), fontsize = 17)\nfor index, value in enumerate(df_age_pc):\n    label = format(int(value), ',')\n    ax1.annotate(label, color='white', xy = (value - 2.3, index - 0.05), fontsize = 15, )\n\nplt.show()","08d95d1e":"plt.style.use('ggplot')\ntrain.plot(kind='scatter', x='Age', y='Fare', figsize=(10, 6), color='darkblue')\n\nplt.title('Scatter Plot of Fare vs Age')\nplt.xlabel('Age of Passengers')\nplt.ylabel('Fare')\n\nplt.show()","c814a96f":"train.info()","1bb7441d":"df_1 = train[train.Pclass == 1].loc[:, ['Fare', 'Survived']]\ndf_2 = train[train.Pclass == 2].loc[:, ['Fare', 'Survived']]\ndf_3 = train[train.Pclass == 3].loc[:, ['Fare', 'Survived']]","156e05ad":"df_3.head()","5e457e2f":"df_1 = df_1.groupby('Survived')['Fare'].median()\ndf_2 = df_2.groupby('Survived')['Fare'].median()\ndf_3 = df_3.groupby('Survived')['Fare'].median()","10494b29":"df_4 = 100 * df_1 \/ df_1.sum()\ndf_5 = 100 * df_2 \/ df_2.sum()\ndf_6 = 100 * df_3 \/ df_3.sum()","fb596ddc":"sns.set(font_scale = 1.1)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure()\nax_0 = fig.add_subplot(1, 3, 1) # add subplot 1 \nax_1 = fig.add_subplot(1, 3, 2) # add subplot 2\nax_2 = fig.add_subplot(1, 3, 3) # add subplot 2\n\n# Subplot1 = 1st class passengers\ndf_1.plot(kind = 'bar', ax=ax_0, color='crimson', sort_columns = True)\nax_0.set_title(\"1st Class Passengers\", y = 1.04, fontsize = 17)\nax_0.set_xlabel('Survived', fontsize = 17)\nax_0.set_ylabel('Fare', fontsize = 17)\n\n# Subplot2 = 2nd class passengers\ndf_2.plot(kind = 'bar', ax=ax_1, color='blue', sort_columns = True)\nax_1.set_title(\"2st Class Passengers\", y = 1.04, fontsize = 17)\nax_1.set_xlabel('Survived', fontsize = 17)\n\n# Subplot2 = 3rd class passengers\ndf_3.plot(kind = 'bar', ax=ax_2, color='green', sort_columns = True)\nax_2.set_title(\"3rd Class Passengers\", y = 1.04, fontsize = 17)\nax_2.set_xlabel('Survived', fontsize = 17)\n \nprint(\"\\t\\t\\tAmount of Fare Paid by each Passenger in each Class\")    \nplt.show()","e9396cd4":"sns.set(font_scale = 1.1)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure()\nax_0 = fig.add_subplot(1, 3, 1) # add subplot 1 \nax_1 = fig.add_subplot(1, 3, 2) # add subplot 2\nax_2 = fig.add_subplot(1, 3, 3) # add subplot 2\n\n# Subplot1 = 1st class passengers\ndf_4.plot(kind='pie',\n          figsize=(15, 6),\n          autopct='%1.1f%%', \n          startangle=90,    \n          shadow=True,       \n          labels=None,         \n          pctdistance=0.5,     \n          colors=['gold', 'yellowgreen'],  \n          explode=[0.1, 0],\n          ax = ax_0,\n          sharex = True,\n          legend = True\n          )\nax_0.set_title(\"1st Class Passengers\", y = 1.04, fontsize = 17)\nax_0.axis('equal')\n\n# Subplot2 = 2nd class passengers\ndf_5.plot(kind='pie',\n          figsize=(15, 6),\n          autopct='%1.1f%%', \n          startangle=90,    \n          shadow=True,       \n          labels=None,         \n          pctdistance=0.5,     \n          colors=['gold', 'yellowgreen'],  \n          explode=[0.1, 0],\n          ax = ax_1,\n          sharex = True,\n          legend = True\n          )\nax_1.set_title(\"2nd Class Passengers\", y = 1.04, fontsize = 17)\nax_1.axis('equal')\n\n# Subplot2 = 3rd class passengers\ndf_6.plot(kind='pie',\n          figsize=(15, 6),\n          autopct='%1.1f%%', \n          startangle=90,    \n          shadow=True,       \n          labels=None,         \n          pctdistance=0.5,     \n          colors=['gold', 'yellowgreen'],  \n          explode=[0.1, 0],\n          ax = ax_2,\n          sharex = True,\n          legend = True\n          )\nax_2.set_title(\"3rd Class Passengers\", y = 1.04, fontsize = 17)\nax_2.axis('equal')\n\nprint(\"\\t\\t\\t% of Fare paid by Passengers who survived in each class\")\n\nplt.show()","7c49bda4":"df_7 = full[full.Pclass == 1].groupby('Survived')['Fare'].count()\ndf_8 = full[full.Pclass == 2].groupby('Survived')['Fare'].count()\ndf_9 = full[full.Pclass == 3].groupby('Survived')['Fare'].count()","bf6a4e7c":"sns.set(font_scale = 1.1)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure()\nax_0 = fig.add_subplot(1, 3, 1) # add subplot 1 \nax_1 = fig.add_subplot(1, 3, 2) # add subplot 2\nax_2 = fig.add_subplot(1, 3, 3) # add subplot 2\n\n# Subplot1 = 1st class passengers\ndf_7.plot(kind = 'bar', ax=ax_0, color='crimson', sort_columns = True)\nax_0.set_title(\"1st Class Passengers\", y = 1.04, fontsize = 17)\nax_0.set_xlabel('Survived', fontsize = 17)\nax_0.set_ylabel('Count', fontsize = 17)\n\n# Subplot2 = 2nd class passengers\ndf_8.plot(kind = 'bar', ax=ax_1, color='blue', sort_columns = True)\nax_1.set_title(\"2st Class Passengers\", y = 1.04, fontsize = 17)\nax_1.set_xlabel('Survived', fontsize = 17)\n\n# Subplot2 = 3rd class passengers\ndf_9.plot(kind = 'bar', ax=ax_2, color='green', sort_columns = True)\nax_2.set_title(\"3rd Class Passengers\", y = 1.04, fontsize = 17)\nax_2.set_xlabel('Survived', fontsize = 17)\n \nprint(\"\\t\\t\\tNumber of Passengers who paid fare in each class\")    \nplt.show()","6e8c331f":"count, bins = np.histogram(train['Age'])\ncount, bins","d4715974":"for i in range(train.shape[0]):\n    if train.loc[i, 'Age'] <= 8:\n        train.loc[i, 'Age Binned'] = '0 - 8'\n    elif train.loc[i, 'Age'] <= 16:\n        train.loc[i, 'Age Binned'] = '9 - 16'\n    elif train.loc[i, 'Age'] <= 24:\n        train.loc[i, 'Age Binned'] = '17 - 24'\n    elif train.loc[i, 'Age'] <= 32:\n        train.loc[i, 'Age Binned'] = '25 - 32'\n    elif train.loc[i, 'Age'] <= 40:\n        train.loc[i, 'Age Binned'] = '33 - 40'\n    elif train.loc[i, 'Age'] <= 48:\n        train.loc[i, 'Age Binned'] = '41 - 48'\n    elif train.loc[i, 'Age'] <= 56:\n        train.loc[i, 'Age Binned'] = '49 - 56'\n    elif train.loc[i, 'Age'] <= 64:\n        train.loc[i, 'Age Binned'] = '57 - 64'\n    elif train.loc[i, 'Age'] <= 72:\n        train.loc[i, 'Age Binned'] = '65 - 72'\n    elif train.loc[i, 'Age'] <= 80:\n        train.loc[i, 'Age Binned'] = '73 - 80'\n    else:\n        train.loc[i, 'Age Binned'] = '80+'","28606046":"for i in range(test.shape[0]):\n    if test.loc[i, 'Age'] <= 8:\n        test.loc[i, 'Age Binned'] = '0 - 8'\n    elif test.loc[i, 'Age'] <= 16:\n        test.loc[i, 'Age Binned'] = '9 - 16'\n    elif test.loc[i, 'Age'] <= 24:\n        test.loc[i, 'Age Binned'] = '17 - 24'\n    elif test.loc[i, 'Age'] <= 32:\n        test.loc[i, 'Age Binned'] = '25 - 32'\n    elif test.loc[i, 'Age'] <= 40:\n        test.loc[i, 'Age Binned'] = '33 - 40'\n    elif test.loc[i, 'Age'] <= 48:\n        test.loc[i, 'Age Binned'] = '41 - 48'\n    elif test.loc[i, 'Age'] <= 56:\n        test.loc[i, 'Age Binned'] = '49 - 56'\n    elif test.loc[i, 'Age'] <= 64:\n        test.loc[i, 'Age Binned'] = '57 - 64'\n    elif test.loc[i, 'Age'] <= 72:\n        test.loc[i, 'Age Binned'] = '65 - 72'\n    elif test.loc[i, 'Age'] <= 80:\n        test.loc[i, 'Age Binned'] = '73 - 80'\n    else:\n        test.loc[i, 'Age Binned'] = '80+'","395573fe":"train.head()","33afcf1d":"for i in range(train.shape[0]):\n    if train.loc[i, 'Fare'] <= 51:\n        train.loc[i, 'Fare Binned'] = '0 - 51'\n    elif train.loc[i, 'Fare'] <= 103:\n        train.loc[i, 'Fare Binned'] = '52 - 103'\n    elif train.loc[i, 'Fare'] <= 154:\n        train.loc[i, 'Fare Binned'] = '104 - 154'\n    elif train.loc[i, 'Fare'] <= 205:\n        train.loc[i, 'Fare Binned'] = '155 - 205'\n    elif train.loc[i, 'Fare'] <= 256:\n        train.loc[i, 'Fare Binned'] = '206 - 256'\n    elif train.loc[i, 'Fare'] <= 307:\n        train.loc[i, 'Fare Binned'] = '257 - 307'\n    elif train.loc[i, 'Fare'] <= 359:\n        train.loc[i, 'Fare Binned'] = '308 - 359'\n    elif train.loc[i, 'Fare'] <= 410:\n        train.loc[i, 'Fare Binned'] = '360 - 410'\n    elif train.loc[i, 'Fare'] <= 461:\n        train.loc[i, 'Fare Binned'] = '411 - 461'\n    elif train.loc[i, 'Fare'] <= 512:\n        train.loc[i, 'Fare Binned'] = '462 - 512'\n    else:\n        train.loc[i, 'Fare Binned'] = '512+'\n        \nfor i in range(test.shape[0]):\n    if test.loc[i, 'Fare'] <= 51:\n        test.loc[i, 'Fare Binned'] = '0 - 51'\n    elif test.loc[i,'Fare'] <= 103:\n        test.loc[i, 'Fare Binned'] = '52 - 103'\n    elif test.loc[i, 'Fare'] <= 154:\n        test.loc[i, 'Fare Binned'] = '104 - 154'\n    elif test.loc[i, 'Fare'] <= 205:\n        test.loc[i, 'Fare Binned'] = '155 - 205'\n    elif test.loc[i, 'Fare'] <= 256:\n        test.loc[i, 'Fare Binned'] = '206 - 256'\n    elif test.loc[i, 'Fare'] <= 307:\n        test.loc[i, 'Fare Binned'] = '257 - 307'\n    elif test.loc[i, 'Fare'] <= 359:\n        test.loc[i, 'Fare Binned'] = '308 - 359'\n    elif test.loc[i, 'Fare'] <= 410:\n        test.loc[i, 'Fare Binned'] = '360 - 410'\n    elif test.loc[i, 'Fare'] <= 461:\n        test.loc[i, 'Fare Binned'] = '411 - 461'\n    elif test.loc[i, 'Fare'] <= 512:\n        test.loc[i, 'Fare Binned'] = '462 - 512'\n    else:\n        test.loc[i, 'Fare Binned'] = '512+'","fb17439b":"train.head()","a3fc50b5":"sns.set(font_scale = 1.2)\nplt.style.use(['ggplot', 'bmh'])\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 7))\nsns.barplot(x = 'Age Binned', y = 'Survived', data = train, ax = ax[0])\nax[0].set_title(\"Barplot Showing Survival Rate w.r.t Different Age Groups\", y = 1.05)\nax[0].set_xlabel(\"Age Groups\", fontsize=16)\nax[0].set_ylabel(\"Survival Rate\", fontsize=16)\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45)\n\nsns.barplot(x = 'Fare Binned', y = 'Survived', data = train, ax = ax[1])\nax[1].set_title(\"Barplot Showing Survival Rate w.r.t Different Fare Types\", y = 1.05)\nax[1].set_xlabel(\"Fare Types\", fontsize=16)\nax[1].set_ylabel(\"Survival Rate\", fontsize=16)\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45)\nplt.show()","75aaf166":"train['FamilySize'] = train.Parch + train.SibSp + 1\ntest['FamilySize'] = test.Parch + test.SibSp + 1\ntrain.drop(columns = ['Age', 'Fare'], axis = 1, inplace = True)","58210bae":"train.set_index('PassengerId', inplace = True)\ntest.set_index('PassengerId', inplace = True)\ntrain.head()","7741033c":"train.FamilySize = train.FamilySize.astype(str)\ntrain.info()","d1ce6079":"test.drop(columns = ['Age', 'Fare'], axis = 1, inplace = True)\ntrain.FamilySize = train.FamilySize.astype(str)","d5212872":"train.info()","3bbc087a":"train.tail()","9dc77b0d":"# Extract the salutation!\ntrain['Title'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\ntest['Title'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)","5305e4cd":"test_sal = list(set(test.Title.unique().tolist()) - set(train.Title.unique().tolist()))\ntest_sal","b4f3e4dc":"train_sal = set(train.Title.unique().tolist()) - set(test.Title.unique().tolist())","037ef657":"test.Title.unique().tolist()","883ced29":"train.Title.unique().tolist()","af10eca1":"train['Title'].value_counts(normalize = True) * 100","89d30eef":"test.Title.value_counts(normalize = True) * 100","99b1a850":"# Bundle rare salutations: 'Other' category\ntrain['Title'] = train['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')","23784ef4":"# Bundle rare salutations: 'Other' category\ntest['Title'] = test['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')","9fcd5303":"train.Title.value_counts(normalize=True) * 100","68946436":"test.Title.value_counts(normalize=True) * 100","a684929d":"test.isnull().sum()","e0f8a2e7":"train.head()","e655855a":"# Embarked, Sex: OneHotEncoder\n# Cabin, Title: TargetEncoder\n# Name, Ticket: To be droped\ntrain.drop(columns=['Name', 'Ticket'], inplace=True)\ntest.drop(columns=['Name', 'Ticket'], inplace=True)\ntrain.head()","ddab7d8d":"# We will also drop Parch and SibSp\ntrain.drop(columns=['Parch', 'SibSp'], inplace=True)\ntest.drop(columns=['Parch', 'SibSp'], inplace=True)","0765f27d":"test.head()","6bd61e49":"y = pd.DataFrame(train.Survived, index = train.index, columns = ['Survived'])\ny.head()","16bcde20":"train.drop(columns='Survived', inplace = True)","35b855c4":"train.Pclass = train.Pclass.astype(str)\ntest.Pclass = test.Pclass.astype(str)\ntrain.info()","251dbc57":"X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.20, random_state = 42)\ny_train.head()","bd57aa59":"from category_encoders.one_hot import OneHotEncoder\n\n# First doing One Hot Encoding\nOH_encoder = OneHotEncoder(handle_unknown='ignore', cols = ['Embarked', 'Sex'], use_cat_names = True)\nnum_encoder = TargetEncoder(handle_unknown = 'value', cols=['Cabin', 'Title', 'Age Binned', 'Fare Binned', 'FamilySize', 'Pclass'])\n\npipeline = Pipeline([('one_hot', OH_encoder),\n                    ('target', num_encoder)])","513c79e8":"num_X_train = pd.DataFrame(pipeline.fit_transform(X_train, y_train))\nnum_X_val = pd.DataFrame(pipeline.transform(X_test))\nnum_X_test = pd.DataFrame(pipeline.transform(test))\nnum_X_train.drop(columns = ['Embarked_S', 'Sex_male'], axis = 1, inplace = True)\nnum_X_val.drop(columns = ['Embarked_S', 'Sex_male'], axis = 1, inplace = True)\nnum_X_test.drop(columns = ['Embarked_S', 'Sex_male'], axis = 1, inplace = True)\nnum_X_test.head()","f7cfe317":"num_X_test.isnull().sum()","3b02d308":"num_X_val.isnull().sum()","41461b57":"num_X_train.describe()","5330d741":"from sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.model_selection import train_test_split\n\n# Keep 5 features\nselector = SelectKBest(f_classif, k=6)\n\nX_new = selector.fit_transform(num_X_train, y_train)\nX_new","ede5cd6e":"# Get back the features we've kept, zero out all other features\nselected_features_train = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=num_X_train.index, \n                                 columns=num_X_train.columns)\nselected_features_train.head()","e0b6bbfb":"# Dropped columns have values of all 0s, so var is 0, drop them\nselected_columns = selected_features_train.columns[selected_features_train.var() != 0]\n\n# Get the valid dataset with the selected features.\nnum_X_val[selected_columns].head()\n","2cf06534":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\n# Set the regularization parameter C=1\nlogistic = LogisticRegression(C=1, penalty=\"l1\", random_state=7, solver = 'liblinear').fit(num_X_train,y_train) # In later models, it was found out Age Binned and Fare Binned didn't contribute much to model building. So, we are dropping it here to avoid data leakage.\nmodel = SelectFromModel(logistic, prefit=True)\n\nX_new = model.transform(num_X_train)\nX_new","5c6b5997":"# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features_train2 = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=num_X_train.index,\n                                 columns=num_X_train.columns)\n\n# Dropped columns have values of all 0s, keep other columns \nselected_columns2 = selected_features_train2.columns[selected_features_train2.var() != 0]\n\nnum_X_val[selected_columns2].head()","d970bec0":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 1000).fit(num_X_train, y_train)\n\nmodel = SelectFromModel(rf, prefit=True)\n\nX_new = model.transform(num_X_train)\nX_new","30f2f12e":"# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features_train3 = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=num_X_train.index,\n                                 columns=num_X_train.columns)\n\n# Dropped columns have values of all 0s, keep other columns \nselected_columns3 = selected_features_train3.columns[selected_features_train3.var() != 0]\n\nnum_X_val[selected_columns3].head()","33680565":"rf = RandomForestClassifier(n_estimators = 1000)\n\nrfecv = RFE(estimator=rf, step=1)\nrfecv.fit(num_X_train, y_train)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)","1454ebe7":"selected_columns4 = num_X_val.columns[rfecv.support_]\n\npd.DataFrame({'Ranking' : rfecv.ranking_}, index=num_X_val.columns)","6c02d217":"import lightgbm as lgb\nfrom sklearn import metrics\n\ndef train_model(X_train, y_train, X_test, y_test, feature_cols):\n    dtrain = lgb.Dataset(X_train[feature_cols], label=y_train)\n    dvalid = lgb.Dataset(X_test[feature_cols], label=y_test)\n\n    param = {'num_leaves': 64, 'objective': 'binary', \n             'metric': 'auc', 'seed': 7}\n    print(\"Training model!\")\n    bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid], \n                    early_stopping_rounds=10, verbose_eval=False)\n\n    valid_pred = bst.predict(X_test[feature_cols])\n    valid_score = metrics.roc_auc_score(y_test, valid_pred)\n    print(f\"Validation AUC score: {valid_score:.4f}\")\n    return bst","db77dcc8":"# Dataset1\n\n_ = train_model(num_X_train, y_train, num_X_val, y_test, selected_columns)","15f29442":"# Dataset2\n\n_ = train_model(num_X_train, y_train, num_X_val, y_test, selected_columns2)","2d7df091":"# Dataset3\n\n_ = train_model(num_X_train, y_train, num_X_val, y_test, selected_columns3)","ce597fb1":"# Dataset4\n\n_ = train_model(num_X_train, y_train, num_X_val, y_test, selected_columns4)","3f9a6a81":"param = {\n    'min_samples_split': hp.uniform('min_samples_split', 0.1, 0.95),\n    'max_depth': hp.quniform('max_depth', 2, 15, 2),\n    'ccp_alpha': hp.uniform('ccp_alpha', 0.001,0.9), \n}\n\nnp.random.seed(0)\nrstate = np.random.RandomState(42)\n# Set up objective function\ndef objective(params):\n    params = {'max_depth': int(params['max_depth']),\n              'ccp_alpha': params['ccp_alpha'], \n              'min_samples_split' : params['min_samples_split']\n              }\n    tree_clf = DecisionTreeClassifier(**params) \n    tree_clf.fit(num_X_train[selected_columns], y_train.Survived)\n    y_pred = tree_clf.predict(num_X_val[selected_columns])\n    score = accuracy_score(y_test.Survived, y_pred)\n    loss = 1 - score\n    return loss\n\n# Run the algorithm\ntrials = Trials()\nbest1 = fmin(fn=objective, space=param, max_evals=1000, rstate=rstate, algo=tpe.suggest, trials = trials)\nprint(best1)","73b68719":"tree = DecisionTreeClassifier(min_samples_split = best1['min_samples_split'],\n                             max_depth = best1['max_depth'],\n                             ccp_alpha = best1['ccp_alpha'])\ntree.fit(num_X_train[selected_columns], y_train.Survived)\n","ff6074cc":"y_tr = tree.predict(num_X_train[selected_columns])\ntr_score = accuracy_score(y_train.Survived.to_numpy(), y_tr)\ny_val = tree.predict(num_X_val[selected_columns])\nval_score = accuracy_score(y_test.Survived.to_numpy(), y_val)\nprint('Validation Results:\\n Training Accuracy =', tr_score, ', Validation Score =', val_score)","b7f513bb":"y_pred = tree.predict(num_X_test[selected_columns])\npredictions = pd.DataFrame({'Survived': y_pred}, index = num_X_test.index)\npredictions.Survived = predictions.Survived.astype('int64')\npredictions.info()","82c31c9b":"predictions.to_csv('submission1.csv')","f17d7b35":"pd.DataFrame(tree.feature_importances_, index = selected_columns).plot.bar(legend=False)\nplt.show()","ac2ed7ed":"param = {\n    'n_neighbors': hp.quniform('n_neighbors', 2, 15, 2),\n    'weights' : hp.choice('weights', ('uniform', 'distance'))\n}\n\nnp.random.seed(0)\nrstate = np.random.RandomState(42)\n# Set up objective function\ndef objective(params):\n    params = {'n_neighbors': int(params['n_neighbors']),\n              'weights': params['weights']\n              }\n    knn = KNeighborsClassifier(**params) \n    knn.fit(num_X_train[selected_columns], y_train.Survived)\n    y_pred = knn.predict(num_X_val[selected_columns])\n    score = accuracy_score(y_test.Survived, y_pred)\n    loss = 1 - score\n    return loss\n\n# Run the algorithm\ntrials = Trials()\nbest2 = fmin(fn=objective, space=param, max_evals=1000, rstate=rstate, algo=tpe.suggest, trials = trials)\nprint(best2)","da38ad4d":"w = ('uniform', 'distance')\nknn = KNeighborsClassifier(n_neighbors = int(best2['n_neighbors']), weights = w[best2['weights']])\nknn.fit(num_X_train[selected_columns], y_train.Survived)","b0bab000":"y_tr = knn.predict(num_X_train[selected_columns])\ntr_score = accuracy_score(y_train.Survived.to_numpy(), y_tr)\ny_val = knn.predict(num_X_val[selected_columns])\nval_score = accuracy_score(y_test.Survived.to_numpy(), y_val)\nprint('Validation Results:\\n Training Accuracy =', tr_score, ', Validation Score =', val_score)","4eda059c":"y_pred = knn.predict(num_X_test[selected_columns])\npredictions = pd.DataFrame({'Survived': y_pred}, index = num_X_test.index)\npredictions.Survived = predictions.Survived.astype('int64')\npredictions.info()","8f9c0bc5":"predictions.to_csv('submission2.csv')","1c7b3815":"param = {\n    'degree': hp.quniform('degree', 1, 20, 1),\n    'C': hp.quniform('C', 1, 1000, 1),\n    'gamma': hp.uniform('gamma', 0.0001, 0.9),\n}\n\nnp.random.seed(0)\nrstate = np.random.RandomState(42)\n# Set up objective function\ndef objective(params):\n    params = {'degree' : int(params['degree']),\n              'C' : int(params['C']),\n              'gamma' : params['gamma'],\n              }\n    svm = SVC(**params) \n    svm.fit(num_X_train[selected_columns], y_train.Survived)\n    y_pred = svm.predict(num_X_val[selected_columns])\n    score = accuracy_score(y_test.Survived, y_pred)\n    loss = 1 - score\n    return loss\n\n# Run the algorithm\ntrials = Trials()\nbest3 = fmin(fn=objective, space=param, max_evals=1000, rstate=rstate, algo=tpe.suggest, trials = trials)\nprint(best3)","65eae9de":"svm = SVC(C = int(best3['C']), degree = int(best3['degree']), gamma = best3['gamma'])\nsvm.fit(num_X_train[selected_columns], y_train.Survived)","6538e6a8":"y_tr = svm.predict(num_X_train[selected_columns])\ntr_score = accuracy_score(y_train.Survived.to_numpy(), y_tr)\ny_val = svm.predict(num_X_val[selected_columns])\nval_score = accuracy_score(y_test.Survived.to_numpy(), y_val)\nprint('Validation Results:\\n Training Accuracy =', tr_score, ', Validation Score =', val_score)","a7f08fc2":"y_pred = svm.predict(num_X_test[selected_columns])\npredictions = pd.DataFrame({'Survived': y_pred}, index = num_X_test.index)\npredictions.Survived = predictions.Survived.astype('int64')\npredictions.info()","0393c8d9":"predictions.to_csv('submission3.csv')","e632c666":"naive = GaussianNB()\nnaive.fit(num_X_train[selected_columns], y_train.Survived)\ny_tr = naive.predict(num_X_train[selected_columns])\ntr_score = accuracy_score(y_train.Survived.to_numpy(), y_tr)\ny_val = naive.predict(num_X_val[selected_columns])\nval_score = accuracy_score(y_test.Survived.to_numpy(), y_val)\nprint('Validation Results:\\n Training Accuracy =', tr_score, ', Validation Score =', val_score)","a5b15c75":"y_pred = naive.predict(num_X_test[selected_columns])\npredictions = pd.DataFrame({'Survived': y_pred}, index = num_X_test.index)\npredictions.Survived = predictions.Survived.astype('int64')\npredictions.info()","ca800ca4":"predictions.to_csv('submission4.csv')","739dfb2d":"# Define get_stacking():\ndef get_stacking():\n    \n\t# Create an empty list for the base models called layer1\n    layer1 = []\n    w = ('uniform', 'distance')\n  # Append tuple with classifier name and instantiations (no arguments) for DecisionTreeClassifier, KNeighborsClassifier, SVC, and GaussianNB base models\n  # Hint: layer1.append(('ModelName', Classifier()))\n    layer1.append(('DT', DecisionTreeClassifier(min_samples_split = best1['min_samples_split'],\n                             max_depth = best1['max_depth'],\n                             ccp_alpha = best1['ccp_alpha'])))\n    layer1.append(('KNN', KNeighborsClassifier(n_neighbors = int(best2['n_neighbors']), weights = w[best2['weights']])))\n    layer1.append(('SVM', SVC(C = int(best3['C']), degree = int(best3['degree']), gamma = best3['gamma'])))\n    layer1.append(('Bayes', GaussianNB()))\n\n  # Instantiate Logistic Regression as meta learner model called layer2\n    layer2 = LogisticRegression()\n\n\t# Define StackingClassifier() called model passing layer1 model list and meta learner with 5 cross-validations\n    model = StackingClassifier(estimators = layer1, final_estimator = layer2, cv = 5)\n\n  # return model\n    return model","bd8835a6":"# Define get_models():\ndef get_models():\n\n  # Create empty dictionary called models\n    models = dict()\n\n  # Add key:value pairs to dictionary with key as ModelName and value as instantiations (no arguments) for DecisionTreeClassifier, KNeighborsClassifier, SVC, and GaussianNB base models\n  # Hint: models['ModelName'] = Classifier()\n    models['DT'] = DecisionTreeClassifier(min_samples_split = best1['min_samples_split'],\n                             max_depth = best1['max_depth'],\n                             ccp_alpha = best1['ccp_alpha'])\n    models['KNN'] = KNeighborsClassifier(n_neighbors = int(best2['n_neighbors']), weights = w[best2['weights']])\n    models['SVM'] = SVC(C = int(best3['C']), degree = int(best3['degree']), gamma = best3['gamma'])\n    models['Bayes'] = GaussianNB()\n    \n  # Add key:value pair to dictionary with key called Stacking and value that calls get_stacking() custom function\n    models['Stacking'] = get_stacking()\n\n  # return dictionary\n    return models","bcc364fe":"# Define evaluate_model:\ndef evaluate_model(model):\n    model.fit(num_X_train[selected_columns], y_train.Survived)\n    y_val = model.predict(num_X_val[selected_columns])\n    val_score = accuracy_score(y_test.Survived.to_numpy(), y_val)\n\n  # return scores\n    return val_score","fdb1eb94":"# Assign get_models() to a variable called models\nmodels = get_models()","e10e2c5a":"# Evaluate the models and store results\n# Create an empty list for the results\nresults = list()\n\n# Create an empty list for the model names\nnames = list()\n\n# Create a for loop that iterates over each name, model in models dictionary \nfor name, model in models.items():\n\n\t# Call evaluate_model(model) and assign it to variable called scores\n\tscores = evaluate_model(model)\n \n  # Append output from scores to the results list\n\tresults.append(scores)\n \n  # Append name to the names list\n\tnames.append(name)\n \n  # Print name, mean and standard deviation of scores:\n\tprint('>%s %.3f' % (name, scores))\n\n# Plot model performance for comparison using names for x and results for y and setting showmeans to True\nsns.barplot(x=names, y=results)","d400c25e":"def plot_loss(loss,val_loss):\n  plt.figure()\n  plt.plot(loss)\n  plt.plot(val_loss)\n  plt.title('Model loss')\n  plt.ylabel('Loss')\n  plt.xlabel('Epoch')\n  plt.legend(['Train', 'Test'], loc='upper right')\n  plt.show()","b3497751":"def plot_accuracy(acc,val_acc):\n  # Plot training & validation accuracy values\n  plt.figure()\n  plt.plot(acc)\n  plt.plot(val_acc)\n  plt.title('Model accuracy')\n  plt.ylabel('Accuracy')\n  plt.xlabel('Epoch')\n  plt.legend(['Train', 'Test'], loc='upper left')\n  plt.show()","4ed45da4":"n = len(selected_columns)\n\n# Instantiate a Sequential Model\nmodel = Sequential()\n\n# Add input and hidden layer\nmodel.add(Dense(6, input_shape=(n, ), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(7, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# Define a callback to monitor val_acc\nmonitor_val_acc = EarlyStopping(monitor='val_accuracy', patience=5)\n# Train model\nh_callback = model.fit(num_X_train[selected_columns], y_train.Survived, epochs=30, validation_data=(num_X_val[selected_columns], y_test.Survived),\n                      callbacks=[monitor_val_acc])","4a1cdec9":"# Plot train vs test loss during training\nplot_loss(h_callback.history['loss'], h_callback.history['val_loss'])","6db0e012":"# Plot train vs test accuracy during training\nplot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])","dcdd63dc":"Distribution of Fare is right-skewed i.e. passengers on titanic have mostly paid between (0 - 51.2) currency units.","b7f2a206":"Distribution of Fare has not changed.","1605efeb":"Survival rate does depend on Age Groups and Fare Types. The above plots show that children belonging to age group and people who paid high fare had higher survival rate which we have already discussed above.","c43d7fdb":"# Problem Statement:\n![The Titanic](https:\/\/thumbs-prod.si-cdn.com\/PHhFx_cNxoSff8oyBOaXfk9mUPw=\/fit-in\/1072x0\/https:\/\/public-media.si-cdn.com\/filer\/Titanic-sinking-illustration-1.jpg)\n              \n    Predict whether the other 418 passengers on board (found in test.csv) survived or not","5b91e08a":"Age and Fare have highest correlation with Pclass","2da3562b":"### 1.1.2. <I>Treating Missing Values<\/I>","d57fa1f1":"### 1.1.3 <I>After Treating Missing Values<\/I>","f48cc621":"## 4.2. K-Nearest Neighbors","80d0b201":"## 3.4. Feature Set Evaluation","947e90e3":"Distribution of Age is a little bit right-skewed, which can be made normal by using box-cox. We will check it after treating missing values.","3d3c8172":"## 4.4. Gaussian Naive Bayes","61339bd9":"# 3) <B><U>Feature Selection<\/U><\/B>\n\n## 3.1. <b>Univariate Feature Selection<\/b>","dec10ae1":"Source:\n1. [41 secrets you never knew about the Titanic and the people aboard it](https:\/\/www.insider.com\/titanic-secrets-facts-2018-4)\n2. [Titanic: History, Sinking, Rescue, Survivors and Facts](https:\/\/www.britannica.com\/topic\/Titanic)","5f16937d":"In the gender_submission file, it is assumed that all the females have survived and all males died. Let's check if this assumption is valid.","bcac0b59":"Majority of Passenger are those who don't have any spouse \/ siblings; but their survival rate is low (Though not the lowest).","8487dfd6":"As can be seen from above plot, port of Embarkation does effect survival rate of passengers.<br>\nC = Cherbourg,<br> Q = Queenstown, <br>S = Southampton","a08b6673":"## 4.3. Support Vector Machine","63dbbe96":"# 2) <B><U>Feature Engineering<\/U><\/B>","2b181d45":"Passengers having 4, or 6 number of parents \/ children aboard the Titanic have almost zero survival rate. Even Passengers with 5 number of children\/parents have lowest survival rate.","8a1939fb":"From above plots we can infer that since 1st class passengers pay more fare, the contribution to fare of passengers who survived in this class are more as compared to those who died. There are more survivors too in this class. But when we see for 3rd class passengers, both survivors and the unfortunate have almost equal contribution to the total fare paid passengers in this class. The median fare paid by a 3rd class passenger is lowest and there are more survivors than dead in this class. Also, from above plot, we can infer that most passengers belong to this class; lowest number of passengers belong to class 1. (Therefore, we can assume that at the time of titanic crisis, Class 1 passengers were given more priority while saving the passengers)<br>\n<br>\nSource:<br>\n1. [Katie Vernon, \"Titanic Revelations: Bodies of Third-Class Passengers were Tossed Back into the Sea\", Aug 1, 2018](https:\/\/www.thevintagenews.com\/2018\/08\/01\/titanic-3rd-class-passengers\/)\n2. Piouffre, G\u00e9rard (2009). Le Titanic ne r\u00e9pond plus (in French). W. W. Norton & Company. p. 317. ISBN 978-2-03-584196-4\n3. [David Aaronovitch, \"Did the third class passengers on the Titanic have a fair chance?\", Saturday 11 April 1998 00:02 \n](https:\/\/www.independent.co.uk\/voices\/did-the-third-class-passengers-on-the-titanic-have-a-fair-chance-1155678.html)\n4. [Titanic: History, Sinking, Rescue, Survivors and Facts](https:\/\/www.britannica.com\/topic\/Titanic)","461c48ee":"## 4.5. Stacking Classifier\n\n### 4.5.1 Create custom functions\n1. get_stacking() - This function will create the layers of our `StackingClassifier()`.\n2. get_models() - This function will create a dictionary of models to be evaluated.\n3. evaluate_model() - This function will evaluate each of the models to be compared.\n\n### 4.5.2. Custom function # 1: get_stacking()\n1. `StackingClassifier()` arguments:\n - `estimators`: List of baseline classifiers\n - `final_estimator`: Defined meta classifier \n - `cv`: Number of cross validations to perform.","c9922ae5":"### 4.5.4. Custom function # 3: evaluate_model(model)","14d30aeb":"Most Passengers are those who have no parents\/children aboard Titanic. Their survival rate is also third lowest. So, family size can be a good feature for model building. I have created this feature at last before feature engineering.","2a671e52":"## <b>3.4. Feature Selection through Recursive Feature Elimination<\/b>","ffe2b6b7":"Univariate Feature Selection tells that Cabin, Fare Binned, FamilySize, Pclass, Sex_female and Title are the 5 most significant features in explaining the survival of passengers. ","6756be31":"As we can see from above plot, survival rate of Men is quite low as compared to Women which does support the above assumption of gender_submission.csv file. But still, we can see that around 19% Men did survived. Also, Men from the First Class died at a higher rate than women from the Third Class.<br> Source: \n1. [Here is an article I Found on Business Insider that proves this asumption](https:\/\/www.businessinsider.in\/slideshows\/miscellaneous\/12-famous-people-who-died-on-the-titanic-160and-11-who-survived\/slidelist\/63762130.cms#slideid=63762131)\n2. [Titanic: History, Sinking, Rescue, Survivors and Facts](https:\/\/www.britannica.com\/topic\/Titanic)\n3. [Howells, Richard (1999). The Myth of the Titanic. United Kingdom: MacMillan Press. ISBN 978-0-333-72597-9.](https:\/\/link.springer.com\/chapter\/10.1057\/9780230510845_3)","3794d8dd":"Both Univariate Feature Selection and RFE yielded good score. But I will select Dataset1 for modelling as it has slightly higher score that Dataset4.","21ac0f0a":"From above two plots, we can infer the following:\n1. Though E, D and B have almost same survival rate, the type of cabin passengers are residing in does affect their survival rate.\n2. Most Passengers reside in a cabin that is not mentioned or is undefined. These passengers have lowest survival rate.\n<br>","ae0d9ff1":"So, Support Vector Machine has higher accuracy than all models.","eb44a681":"L1 regularization selected all features for modelling except Fare Binned.\n\n## 3.3. <b>Feature Selection through Random Forest Classication<\/b>","a2272ee5":"The above feature selection method tells that Age Binned, Family Size, Sex_female, Title will give better output.","3f8bf159":"## 1.2. <b>Exploring Missing Values<\/b>","715842f5":"Passengers mostly embarked from Southampton, but their survival rate is low when we compare the countplot and barplot of feature 'Embarked'.<br>\n<br>\nSource:\n1. [Titanic: History, Sinking, Rescue, Survivors and Facts](https:\/\/www.britannica.com\/topic\/Titanic)\n","7ad63386":"## 4.2. Deep Learning Sequential Model","be555321":"### 4.5.3. Custom function # 2: get_models()","7f2dc62c":"Class 1 Passengers pay highest average fare for boarding Titanic. And also, they have highest average age.","2ef13fcc":"## 3.2. L1 Regularization","ea17c683":"Distribution of Age too has not changed.","e3959d6f":"### 1.1.1 <I> Exploring Age, Fare and Embarked<\/I>","58b27ffb":"Cabin: Object<br>\nAge: float64","7b745631":"In train dataset, set of features that have missing values = (Cabin, Age, Embarked)<br>\nIn test dataset, set of features that have missing values = (Cabin, Age, Fare)","b850a43a":"1st Class Passengers have higher survival rate while 3rd Class Passengers have lower survival rate.","8904be8d":"# 4. **Modelling**\n## 4.1. Decision Tree","f8ebfa39":"From the above scatter plot, there is no trend between fare and age of passengers.","8e8aad53":"We will impute the 'Cabin' missing values with 'U' for Undefined.","6a74216b":"Majority of passengers boarding Titanic belong to 3rd class","c0035825":"# 1) <B><u>Exploratory Data Analysis<\/u><\/B>"}}