{"cell_type":{"136eb3c8":"code","57b56e3b":"code","671353dc":"code","10664a91":"code","a93e4d55":"code","4c986418":"code","db072fa3":"code","b9a2639f":"code","0f20f819":"code","00bb88b6":"code","edca1e86":"code","0ce8f1ad":"code","afb61a1c":"code","3d6c1a56":"code","1feb09a1":"code","6f5c40a1":"code","f9760ae8":"code","f1c187a2":"code","c5c31aa3":"code","352fa60b":"markdown","1730a25e":"markdown","c30e578a":"markdown","c5dcb795":"markdown"},"source":{"136eb3c8":"import pandas as pd\nimport numpy as np\nimport json,ast\nfrom scipy.sparse import csr_matrix as csr\n# SKLEARN\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, pairwise_distances\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import NearestNeighbors","57b56e3b":"movies = pd.read_csv('..\/input\/movies\/movies_data.csv')\nprint(movies.head(10))\nratings = pd.read_csv('..\/input\/movies\/ratings.csv')\nprint(ratings.head(10))","671353dc":"print(movies.shape,\"\\n\")\nmovies.info()","10664a91":"# movies['movieId'] = [int(eval(s)[0]['id']) if len(eval(s))>0 else 0 for s in movies['genres'].values]\n# Extracting the genres\ndef extractGenre(s):\n  lst = eval(s)\n  if len(lst) > 0:\n    return lst[0]['name']\n  else:\n    return ''\n# Removing NaN\ndef delNan(cell):\n  if type(cell) == float:\n    return ''\n  else:\n    return cell\n# Test run\n'''print(extractGenre([{'id': 14, 'name': 'Fantasy'}, {'id': 35, 'name': 'Comedy'}, {'id': 18, 'name': 'Drama'}]))'''\n\nmovies['genres'] = movies['genres'].apply(extractGenre)\nmovies['title'] = movies['title'].apply(delNan)\nmovies['overview'] = movies['overview'].apply(delNan)\nmovies","a93e4d55":"# Movies without titles have no relevance in recommendation systems. So, since the number of movies with missing title are only 6 out of the 45466 movies, we remove them\nmovies.drop(index=[45460,45461,45462,45463,45464,45465],inplace=True)\nmovies['id'] = movies['id'].astype(int)\n# Merging genres and overview together so that our recommender system is based on both the overview and genre\nmovies['genre+overview'] = [\",\".join([movies.iloc[i,0].lower(),movies.iloc[i,2].lower()]) for i in range(movies.shape[0])]\n# Counting the number of movies under each genre\nprint(movies['genres'].value_counts(),\"\\n\")\nmov_enc = movies.copy()\n# Label Encoding the genres\nle = LabelEncoder()\nmov_enc['genres'] = le.fit_transform(mov_enc['genres'])\nprint(\"ORIGINAL :\\n\",movies,\"\\n\\nENCODED GENRES :\\n\",mov_enc)","4c986418":"# Training the tf-idf model on the genre+overview column for a subset of the data\nmovies_new = movies.iloc[:35001,:]\ntfidf = TfidfVectorizer(analyzer='word',ngram_range=(1,3),stop_words='english',lowercase=True,encoding='utf-8')\ntfidf_matrix = tfidf.fit_transform(movies_new['genre+overview'])\nsim_scores = linear_kernel(tfidf_matrix,tfidf_matrix)","db072fa3":"# Function for recommending movies\nindices = pd.Series(movies_new.index,index=movies_new['id']).drop_duplicates()\ndef rec_movie(movieId,similarity=sim_scores):\n  # User input is mapped to the corresponding movie id\n  index = indices[movieId]\n  # Pairwise score evaluation\n  similar = list(enumerate(similarity[index]))\n  # Sort in decreasing order\n  similar = sorted(similar, key = lambda x: x[1], reverse=True)\n  # Fetch the top 10 movies\n  mov10_ind = [i[0] for i in similar[1:11]]\n  recommended = pd.DataFrame()\n  recommended['movieId'],recommended['title'],recommended['genre+overview'] = movies_new['id'].iloc[mov10_ind],movies_new['title'].iloc[mov10_ind],movies_new['genre+overview'].iloc[mov10_ind]\n  return recommended\n\n# Test for Despicable Me\nprint(\"ORIGINAL MOVIE : \", movies_new['title'].iloc[indices[20352]],\"\\nGENRE + OVERVIEW : \", movies_new['genre+overview'].iloc[indices[20352]])\nprint(\"\\n\\n10 RECOMMENDED MOVIES :\\n\")\nrec_movie(20352)","b9a2639f":"print(ratings.shape,\"\\n\")\nratings.info()","0f20f819":"# Using a subset of the data due to RAM limitations\nratings_new = ratings.iloc[:20001,:]\n# Splitting to train and test data\ntrain,test = ratings_new.iloc[:(int(0.8*ratings_new.shape[0])+1),:],ratings_new.iloc[(int(0.8*ratings_new.shape[0])+1):,:]\nprint(\"TRAIN :\\n\",train,\"\\n\\nTEST :\\n\",test)","00bb88b6":"userID,movieId = 2,222\ntest1 = rec_movie(movieId)\ntest1","edca1e86":"final1 = ratings.merge(test1,on='movieId')\nfinal1","0ce8f1ad":"pred = final1['rating'].median()\npred","afb61a1c":"test2 = ratings.merge(movies_new,left_on='movieId',right_on='id')\ntest2","3d6c1a56":"temp = train.merge(movies_new,left_on='movieId',right_on='id')\ntemp","1feb09a1":"pvt = ratings_new.pivot_table(index='userId',columns='movieId',values='rating')\npvt.fillna(0,inplace=True)\npvt","6f5c40a1":"cosine_sim = 1-pairwise_distances(pvt, metric=\"cosine\")\ncosine_sim","f9760ae8":"pearson_sim = 1-pairwise_distances(pvt, metric=\"correlation\")\npearson_sim","f1c187a2":"# Get 10 similar users by nearest neighbors algorithm, defaulting to Pearson correlation coefficient metric\ndef sim10_users(user_id,pvt,metric=\"correlation\",k=10):\n  indices_sim = []\n  knn = NearestNeighbors(metric = metric, algorithm = 'brute') \n  knn.fit(pvt)\n  distances, indices_sim = knn.kneighbors(pvt.iloc[user_id - 1, :].values.reshape(1, -1), n_neighbors = k+1)\n  sims = 1 - distances.flatten()\n  return sims,indices_sim\n\n# Predict ratings\n'''ALGORITHM :\n1. Get 10 simialr users\n2. Get the mean of all user ratings for that userId\n3. Find sum of all ratings of the similar users obtained.\n4. for all similar users:\n    --> Find rating_diff = (Rating by user j on movie i) - (mean of all user ratings by that user)\n    --> Get updated_rating = updated_rating + (rating_diff * similarity_score(of user j))\n5. Predicted rating = (((step 2.)*updated_rating + step 3.) + 1)\n'''\ndef predict_rating(user_id, movie_id, pvt, metric = \"correlation\", k = 10):\n    pred = 0\n    indices_mov = list(pvt.columns)\n    indexm = indices_mov.index(movie_id)\n    # STEP 1\n    sims, indices = sim10_users(user_id, pvt, metric, k)\n    # STEP 2\n    mean_rating = pvt.loc[user_id,:].mean()     # Adjusting for zero based indexing\n    # STEP 3\n    rtSum = np.sum(sims) - 1\n    pdt,updated_rating = 1,0           # Initializing product and updated rating\n    # STEP 4\n    for i in range(0, len(indices.flatten())):\n        if (indices.flatten()[i] + 1) == user_id:\n            continue\n        else: \n            rating_diff = abs(pvt.iloc[indices.flatten()[i],indexm]-np.mean(pvt.iloc[indices.flatten()[i],:]))\n            pdt = rating_diff * (sims[i])\n            updated_rating = updated_rating + pdt\n    \n    pred = pred + ((mean_rating*updated_rating + rtSum) + 1)\n    return pred\n# Test\npredict_rating(77, 4499, pvt)","c5c31aa3":"def predictRating_combined(userId,movieId,pvt=pvt,metric=\"correlation\",k=10):\n  testdf1 = rec_movie(movieId)\n  finaldf1 = ratings.merge(testdf1,on='movieId')\n  if len(list(finaldf1.index)) > 0 :\n    pred = final1['rating'].median()\n    return pred,testdf1\n  else:\n    pred2 = predict_rating(userId,movieId,pvt=pvt,metric=\"correlation\",k=10)\n    return pred2,testdf1\n\n# Final function for predicting user rating and recommended movies\ndef recommend(userId,movieId,movies_new = movies_new):\n  indices = pd.Series(movies_new.index,index=movies_new['id']).drop_duplicates()\n  predicted_rating,rec = predictRating_combined(userId,movieId)\n  print('\\nRECOMMENDED MOVIES for the movie {} : \\n\\n{}\\n\\n=== Predicted rating for user {} -> movie {}: {:.1f} ==='.format(movies_new['title'].iloc[indices[movieId]],rec,userId,movieId,predicted_rating))\n\n# Test run 1\nrecommend(2,222)\nprint('-'*90)\n# Test run 2\nrecommend(77,4499)","352fa60b":"Training the model for content based recommendation","1730a25e":"Predicting ratings","c30e578a":"Final test runs for :\n\n1. Berlin : Symphony of a Great City\n2. Pepi, Luci, Bom","c5dcb795":"Preprocessing"}}