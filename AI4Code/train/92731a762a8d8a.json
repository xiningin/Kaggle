{"cell_type":{"5f2f0e15":"code","3bf12fc3":"code","e3127b51":"code","417497b6":"code","5cb2c916":"code","909cd825":"code","5a79d527":"code","6bb23493":"code","9e376c9a":"code","a14dc14b":"code","0c2220f6":"code","5fce93fe":"code","5840b822":"code","37883803":"code","478d3a2b":"code","8a4ffa5e":"code","913119c8":"code","4ba2955b":"code","c893da60":"code","aa0e77a9":"code","84b036b3":"code","ff34712f":"code","b9170aed":"code","9dd42e1c":"code","366e96a5":"code","7a5e7399":"code","a3cb48da":"code","8883b0b7":"code","4c5a0839":"code","74c3081a":"code","a2d6f111":"code","bc3e9472":"code","1d7a6438":"code","478c4c96":"code","a1df3534":"markdown","797e09c8":"markdown","f44834c9":"markdown","41c0bbff":"markdown","5a23bb7b":"markdown","a66b8b60":"markdown","6700e2ab":"markdown","4bdca2d0":"markdown","5e25b945":"markdown","0daf86f6":"markdown","9066a990":"markdown","b55e7ba4":"markdown","00687b1a":"markdown","b74a6967":"markdown","b77e807a":"markdown","48f6573e":"markdown","8ece17ab":"markdown","6777ed28":"markdown","e564b377":"markdown","ec52fb75":"markdown"},"source":{"5f2f0e15":"#packages\n#basics\nimport os\nimport numpy as np\nimport pandas as pd\n\n#images\nimport cv2\n\n#modeling\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.client import device_lib\n\n#visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt","3bf12fc3":"#Check to see\ntf.config.get_visible_devices()","e3127b51":"#check if GPU available to use - works well in google colab but kaggle varies. Can use just CPU, GPU, or TPU or mix. \nif 'GPU' in str(device_lib.list_local_devices()):\n    config = tf.compat.v1.ConfigProto(device_count = {'GPU': 0})\n    sess = tf.compat.v1.Session(config=config) ","417497b6":"#get the data\ntrain = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')","5cb2c916":"#Modify the Id such that each Id is the full image path. In the form\ndef train_id_to_path(x):\n    return '..\/input\/petfinder-pawpularity-score\/train\/' + x + \".jpg\"\ndef test_id_to_path(x):\n    return '..\/input\/petfinder-pawpularity-score\/test\/' + x + \".jpg\"\n\n#Read in the data and drop unnecessary columns\ntrain = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntrain = train.drop(['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'],axis=1)\n\ntest = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\ntest = test.drop(['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'],axis=1)\n\n#Add the .jpg extensions to the image file name ids\ntrain[\"img_path\"] = train[\"Id\"].apply(train_id_to_path)\ntest[\"img_path\"] = test[\"Id\"].apply(test_id_to_path)","909cd825":"#binning columns to test models\ntrain['two_bin_pawp'] = pd.qcut(train['Pawpularity'], q=2, labels=False)\ntrain = train.astype({\"two_bin_pawp\": str})\n\ntrain['four_bin_pawp'] = pd.qcut(train['Pawpularity'], q=4, labels=False)\ntrain = train.astype({\"four_bin_pawp\": str})\n\ntrain['ten_bin_pawp'] = pd.qcut(train['Pawpularity'], q=10, labels=False)\ntrain = train.astype({\"ten_bin_pawp\": str})","5a79d527":"train2bin_stats = train.groupby('two_bin_pawp')\ntrain2bin_stats.describe()","6bb23493":"train4bin_stats = train.groupby('four_bin_pawp')\ntrain4bin_stats.describe()","9e376c9a":"train10bin_stats = train.groupby('ten_bin_pawp')\ntrain10bin_stats.describe()","a14dc14b":"#show the full training dataframe now\ntrain.head()","0c2220f6":"#show the full testing dataframe now. Notice how we don't have anything besides the image file paths\/ids. \n#These are what we will use to make our submissions. \ntest.head()","5fce93fe":"#Set the size image you want to use\nimage_height = 128\nimage_width = 128\n\n#define a function that accepts an image url and outputs an eager tensor\ndef path_to_eagertensor(image_path):\n    raw = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(raw, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    #image = tf.image.resize_with_pad(image, image_height, image_width) #optional with padding to retain original dimensions\n    image = tf.image.resize(image, (image_height, image_width))\n    return image","5840b822":"#show the image file path for the first image in the training data\nprint(train['img_path'][0])","37883803":"#let's plot that first image:\n#use plt.imread() to read in that image file\nog_example_image = plt.imread('..\/input\/petfinder-pawpularity-score\/train\/0007de18844b0dbbb5e1f607da0606e0.jpg') \nprint(og_example_image.shape)\n\n#then plt.imshow() can display it for you\nplt.imshow(og_example_image)\nplt.title('First Training Image') \nplt.axis('off') #turns off the gridlines\nplt.show()","478d3a2b":"%%capture\n#run the function to show the pre-processing on the first training image only\nexample_image = path_to_eagertensor('..\/input\/petfinder-pawpularity-score\/train\/0007de18844b0dbbb5e1f607da0606e0.jpg')","8a4ffa5e":"#show the type \nprint('type: ', type(example_image),'\\n shape: ',example_image.shape)\nplt.imshow(example_image)\nplt.title('First Training Image - with preprocessing done by path_to_eagertensor()') \nplt.show()","913119c8":"#get all the images in the training folder and put their tensors in a list\nX = []\nfor img in train['img_path']:\n    new_img_tensor = path_to_eagertensor(img)\n    X.append(new_img_tensor)\n    \nprint(type(X),len(X))\nX = np.array(X)\nprint(type(X),X.shape)","4ba2955b":"#get all the images in the test folder and put their tensors in a list\nX_submission = []\nfor img in test['img_path']:\n    new_img_tensor = path_to_eagertensor(img)\n    X_submission.append(new_img_tensor)\n    \nprint(type(X_submission),len(X_submission))\nX_submission = np.array(X_submission)\nprint(type(X_submission),X_submission.shape)\n","c893da60":"#grab the target variable. In our case, Pawpularity\ny = train['Pawpularity']\nprint(type(y))","aa0e77a9":"#generate train - test splits  90% train - 10% test\n#You usually don't want to do a 90-10 split unless you have a lot of data, \n#but we get to evaluate performance using the leaderboard submissions as well\n#So I really want this model to see as many pets as possible in trianing\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=7)","84b036b3":"#Show the shape of each of the new arrays\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","ff34712f":"#define the inputs to your model. Basically the shape of the incoming data\ninputs = tf.keras.Input(shape=(image_height,image_width,3))\n\n#start off with x just being those inputs\nx = inputs\n\nx = tf.keras.layers.Conv2D(filters = 16, kernel_size = (7,7), strides = (2,2), padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), activation = 'relu')(x)\nx = tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), padding='same', activation = 'relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), strides = (2,2), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), activation = 'relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.25)(x)\n\n\nx = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0002), activation = 'relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), strides = (2,2), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), activation = 'relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.25)(x)\n\nx = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0002), activation = 'relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3),padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0002), activation = 'relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.25)(x)\n\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dense(512, activation = \"relu\")(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\noutput = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.Model(inputs = inputs, outputs = output)","b9170aed":"model.summary()","9dd42e1c":"#compile the model\nmodel.compile(\n    loss = 'mse', \n    optimizer = 'Adam', \n    metrics = [tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\", \"mape\"])","366e96a5":"#you can use this to provide your model with different variations of the same images. \n# It doesn't actually make and save a bunch of new images, just defines how new images can be created. \n# These transformations will randomly be applied to the training images as they get used in training.\ndata_augmentation = ImageDataGenerator(\n    rotation_range = 15, \n    zoom_range = 0.15,\n    width_shift_range = 0.2, \n    height_shift_range = 0.2, \n    shear_range = 0.1,\n    horizontal_flip = True, \n    fill_mode = \"nearest\")","7a5e7399":"history = model.fit(\n    data_augmentation.flow(x_train,y_train,batch_size=32),\n    validation_data = (x_test,y_test),\n    steps_per_epoch = len(x_train) \/\/ 32,\n    epochs = 10\n)","a3cb48da":"plt.figure()\nplt.plot(history.history[\"rmse\"], label=\"train_rmse\")\nplt.plot(history.history[\"val_rmse\"], label=\"val_rmse\")\n#plt.xticks(range(0,60))\nplt.title(\"RMSE train\/validation by Epoch\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"RMSE\")\nplt.legend(loc=\"upper right\")","8883b0b7":"#predict on the submission data\ncnn_pred = model.predict(X_submission)\nprint(X_submission.shape, type(X_submission))\nprint(cnn_pred.shape, type(cnn_pred))","4c5a0839":"#put the submission predictions alongside their associated Ids\ncnn = pd.DataFrame()\ncnn['Id'] = test['Id']\ncnn['Pawpularity'] = cnn_pred\ncnn.to_csv('submission.csv',index=False)","74c3081a":"cnn.head(10)","a2d6f111":"testing_example_image = plt.imread('..\/input\/petfinder-pawpularity-score\/test\/4128bae22183829d2b5fea10effdb0c3.jpg') \nprint(testing_example_image.shape)\n#then plt.imshow() can display it for you\nplt.imshow(testing_example_image)\nplt.title('First Testing Image \\n Predicted Pawpularity = {}'.format(cnn['Pawpularity'].iloc[0])) \nplt.axis('off') #turns off the gridlines\nplt.show()","bc3e9472":"#Empty List\nSample_image_prediction = []\n#Preprocess with our function\nsample_new_img_tensor = path_to_eagertensor('..\/input\/petfinder-pawpularity-score\/train\/0007de18844b0dbbb5e1f607da0606e0.jpg')\nSample_image_prediction.append(sample_new_img_tensor)\nSample_image_prediction = np.array(Sample_image_prediction)\n#Show data type is good to input into model\nprint(type(Sample_image_prediction),Sample_image_prediction.shape)","1d7a6438":"sample_cnn_pred = model.predict(Sample_image_prediction)\nprint(sample_cnn_pred,sample_cnn_pred.shape, type(sample_cnn_pred))","478c4c96":"sample_example_image = plt.imread('..\/input\/petfinder-pawpularity-score\/train\/0007de18844b0dbbb5e1f607da0606e0.jpg') \nprint(sample_example_image.shape)\n#then plt.imshow() can display it for you\nplt.imshow(sample_example_image)\nplt.title('First Training Image \\n Predicted Pawpularity = {}'.format(sample_cnn_pred[0][0])) \nplt.axis('off') #turns off the gridlines\nplt.show()","a1df3534":"## 3. Model Building\nNow that we have our data, building the models themselves is a pretty straightforward process. \n\nAt a basic level, you can think of model building with images as having 5 steps:\n1. Split up your data into training and testing (split up the X numpy array)\n2. Define a model architecture\n3. Compile your model \n4. Fit your model to the training data you have split off in your train_test_split. \n5. Use your now trained model to predict on new data (X_submission)","797e09c8":"All that for our model to just predict the Pawpularity scores near the median Pawpularity score every single time! \ud83e\udd26\ud83c\udffb\u200d\u2642\ufe0f Well it was seeing random noise, so this is actually a good thing! Yeah that's right, the sample images in the ..\/test\/ subdirectory are all like that. So if you're getting super specific predictions for random noise, something is probably off here lol. That said, the model did just learn to basically guess the median for everything.\n\nStill, better to switch to classification and use the bins we created earlier. Feel free to modify the model architecture, data augmentation, preprocessing, image size, etc to see if you can improve the score. To keep things clean, I'll share the classification and binning methods in a new notebook Tutorial Part 3: CNN Image Modeling 2","f44834c9":"### 3.1 Split up you data into training and testing","41c0bbff":"## 2. Get the data\nThe trickiest part of modeling using images in my opinion is often getting them in the right format to be used by your ML model architectures. You need to know where your images are, and then turn them into the data types that can be accepted by your models. If your images are already in directories with their associated classes, there are some straighforward ways to read in the data. Imagine the file paths: \/train\/26\/1239581345.jpg and \/train\/93\/1239581345.jpg where the classes would be a pawpularity score of 26 in the first case and 93 in the second case. That is NOT the case for this competition.\n\nFor our purposes, an easy way to get our data is to:\n1. Get the image paths from train.csv and test.csv files. The image files are named the same as the Ids in the Id column of the csv's.\n2. Then, create a function that will preprocess all the images and put then into an array\/tensor. At a basic level, you want to turn the all training images into one super big array. That is what will be fed into your ML model. So you don't feed the images in based on their image paths\/URLs, you have to turn them all into a single array\/tensor before the model can be fit to the images.\n3. Preprocess the images using your function. Display the result.","5a23bb7b":"Usually you want to use CNNs for classification tasks. And the Pawpularity score is a poor metric to classify given the minimal visual difference between the images at varying Pawpularity scores. A pet scored at 26 pawpularity is not so different from a pet scored at a 27 - it's not like a dog and a car. But we'll do the basics here to show this as a regression type task.","a66b8b60":"## 1. Load in the packages","6700e2ab":"### 3.3 Compile your model","4bdca2d0":"For the pre-processing function that will get our images, these are some useful documentation links. For our first run through, we'll use the full rgb images. So 3 channels. If you wanted you could change the images to grayscale in this function. We read in the file, then decode the image, normalize, and resize.\nFunction path_to_eagertensor() documentation\n* https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/read_file\n* https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/decode_jpeg\n* https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/cast\n* https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/image\/resize\n* https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/image\/resize_with_pad","5e25b945":"# Tutorial Part 3: CNN Image Modeling 1\nHello and welcome to Part 3 of this Pawpularity Contest Tutorial Series. \n\n**In this 'Tutorial Part 3: CNN Image Modeling 1', you'll learn:**\n* How to read in and pre-process the images for modeling\n* How to perform data augmentation in 1 line of code\n* How to build a basic Convolutional Neural Network (CNN)\n* How to predict RMSE using your CNN architecture\n* How to evaluate your models\n* How to submit your predictions for the competition\n\n**Other Tutorials in this series:**\n* In [Tutorial Part 1: EDA for Beginners](https:\/\/www.kaggle.com\/alexteboul\/tutorial-part-1-eda-for-beginners), we covered the exploratory data analysis process from start to finish for the PetFinder.my Pawpularity Contest. \n* In [Tutorial Part 2: Model Building using the Metadata](https:\/\/www.kaggle.com\/alexteboul\/tutorial-part-2-model-building-using-the-metadata), we built models using the metadata (.csv data) provided by the competition hosts. Specifically, we tried Decision Tree Classification, Decision Tree Regression, Ordinary Least Squares Regression, Ridge Regression, Bernoulli Naive Bayes Classification, Random Forest Regression, and Histogram-based Gradient Boosting Regression (LightGBM).\n\nTLDR on the first 2 Tutorials, all the metadata models suck because the metadata isn't strongly predictive of \/ correlated with the target class \"Pawpularity\". You could just guess the mean Pawpularity value for every image and get a similar RMSE score - which is what many of the models learned to do. Sometimes it's just not possible to build highly predictive models. Not because of the data you have collected, but because the target class may simply be a random distribution. Pawpularity may just not have much to do with the images themselves. That said, today we are going to see if we can lower our RMSE score by building models using the images. Specifically, we're going to explore basic Convolutional Neural Networks (CNNs) and Transformers.\n\n**Score to beat: 20.52**\n\n**Index:**\n1. Load in the packages\n2. Get the data\n3. Model Building","0daf86f6":"### 3.2 Define your model architecture\nThis is the secret sauce of many ML models. It can get quite complicated when you have different layers of your network interacting with eachother. For our example, we'll stick to a basic architecture and explain each step. Basically:\n* [tf.keras.layers.Conv2D](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Conv2D) - defines a convolutional layer and you can mess with parameters\n* [tf.keras.layers.BatchNormalization](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/BatchNormalization) - Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n* [tf.keras.layers.Dropout](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dropout) - The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n* [tf.keras.layers.MaxPooling2D](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/MaxPool2D) - a way to perform dimensionality reduction by taking the max value in the pool along strides.\n* **strides** - The strides parameter is a 2-tuple of integers, specifying the \u201cstep\u201d of the convolution along the x and y axis of the input volume. This can sometimes be used instead of doing max pooling. Reduces volume.\n* **kernel_size** - The size of the kernel. A kernel is a filter that is used to extract the features from the images. The kernel is a matrix that moves over the input data, performs the dot product with the sub-region of input data, and gets the output as the matrix of dot products\n* **padding** - Can be \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding with zeros evenly to the left\/right or up\/down of the input such that output has the same height\/width dimension as the input.\n* *kernel_regularizer=l2(0.0002)* - performing regularization can help prevent overfitting. That's when your model is to attuned to the training data and doesn't generalize well. So you might be really good on your training metrics, but then your predictions could be totally off. The smaller the value applied here, the less impact it'll have. \n\nYou may also see this look different, where some people like to use .add() to add layers to their model architecture. So you might see something like:\n\n* model_a = tf.keras.models.Sequential()\n* model_a.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128,128,3), data_format=\"channels_last\"))\n* model_a.add(BatchNormalization())\n* model_a.add(MaxPooling2D(pool_size=(2, 2)))\n* model_a.add(Dropout(0.5))\n* model_a.add(Conv2D(32, (3, 3), activation='relu'))\n* model_a.add(Flatten())\n* model_a.add(Dense(32, activation='relu'))\n* model_a.add(BatchNormalization())\n* model_a.add(Dropout(0.5))\n* model_a.add(Dense(1, activation='linear')) \n\nThis is acceptable too in most cases. My personal preference is to use this format you'll see coded below. \n","9066a990":"**Use model.summary() to actually show the model architecture**","b55e7ba4":"When we fit our model here, note that we're using the real-time data augmentation. Also note that we validation_data is the x_test and y_test from our train_test_split before.","00687b1a":"Make note below of the data type that the example_image was turned into after applying our pre-processing funciton. Also note that the shape of the image has been resized.","b74a6967":"#### For Fun - see prediction on an image of your choice - here training image #1","b77e807a":"It may also be helpful for us to build models where we try to turn this into a classification problem. So instead of the model predicting Pawpularity scores between 1-100, it could predict low v. high, or some other arrangement of bins. To do this, you can use pd.qcut(). The q value you specify determins the number of bins\/classes the target column will get split up into fairly equally. You can specify labels in a list as well.\n\nFor the sake of clarity, I moved this to Tutorial Part 4: CNN Image Modeling 2","48f6573e":"**We've now finished the steps to get our data. As a result, we have:**\n* 2 numpy arrays\n * Array 1 is our training data array composed of all our training preprocessed images (from ..\/train\/*.jpg): X\n * Array 2 is our testing data array composed of the test preprocessed images (from ..\/test\/*.jpg): X_submission\n* We will use X for our model building \/ training \/ evaluation of our models\n* We will use X_submission for our submission predictions","8ece17ab":"### 3.4 Fit your model using the training data\n\nJust fit the model to the training data. A helpful addition here is usually to do some data augmentation.\n* [tf.keras.preprocessing.image.ImageGenerator](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator) - This generates batches of tensor image data for real-time data augmentation.\n * Just call your generator.flow() when you .fit() your model to use this real time data augmentation.\n* [tf.keras.Model](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model) - groups layers into an object with training and inference features.","6777ed28":"### 3.5 Now use your newly trained model on unseen data + make your submission!","e564b377":"Here we can see exactly how the data is split up. If we model on this, we'll still need to at some point turn the predictions back into scores between 1-100 for evaluation though.","ec52fb75":"Now, with a simple for loop, you can run the preprocessing function on all the image paths in the training dataframe and next the testing dataframe. By training and testing I mean the data from the training.csv and testing.csv."}}