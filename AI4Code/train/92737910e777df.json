{"cell_type":{"77ac5a06":"code","8dcff9a0":"code","6af137f9":"code","243417e1":"code","70654182":"code","9cafae1e":"code","716f3565":"code","fe9e622b":"code","e529e848":"code","a38ea686":"code","27e31b35":"code","f564904a":"code","ea23e3a6":"code","fa1b57c2":"markdown","a656976b":"markdown","124c8c55":"markdown","ce96a215":"markdown","fab691e3":"markdown","4098fb51":"markdown","a4933c43":"markdown","f0230fed":"markdown","a0c7782e":"markdown","665592d4":"markdown","ca0aa2c0":"markdown","e9ca6f3d":"markdown","043b15aa":"markdown"},"source":{"77ac5a06":"import warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport xgboost as xgb\n\nimport category_encoders as ce\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, normalize, Normalizer,MinMaxScaler\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier","8dcff9a0":"df = pd.read_csv('..\/input\/webclubrecruitment2019\/TRAIN_DATA.csv')\ndf_test = pd.read_csv('..\/input\/webclubrecruitment2019\/TEST_DATA.csv')","6af137f9":"df.head()","243417e1":"df_test.head()","70654182":"#Creating the object for onehot encoding\nonehot_encoder = OneHotEncoder()\n\n#function to return the onehot encoded dataframe\ndef one_hot_encodingg(temp_col):\n    integer_encoded = temp_col.reshape(len(temp_col), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n    Mc=onehot_encoded.tocoo()\n    mc = Mc.toarray()\n    return mc\n\n#function for renaming the columns\ndef rename_col(new_df,string):    \n    new_df.columns = [string+str(i) for i in range(len(new_df.columns))]\n    return new_df\n    \n#converting categorical dataframe to one hot encoded dataframes    \ndef convert_to_one_hot(temp_col,string):\n    temp_col = np.array(list(temp_col))\n    temp_col = temp_col.reshape(-1,1)\n    new_df = one_hot_encodingg(temp_col)\n    new_df = pd.DataFrame(new_df)\n    new_df = rename_col(new_df,string)\n    return new_df","9cafae1e":"#concating the two dataframes\ndef concatenate_dataframes(df1,df2):\n    df1 = df1.reset_index()\n    df2 = df2.reset_index()\n    df_c =  pd.concat([df1,df2] , axis = 1)\n    df_c = df_c.drop(columns = ['index'])\n    return df_c\n\n#concating onehot encoded dataframes with the original dataframe\ndef encode_concate(df, df_col, string):\n    df_one_hot = convert_to_one_hot(df_col, string)\n    df_concat = concatenate_dataframes(df,df_one_hot)\n    df_concat = df_concat.drop(columns = [string])\n    return df_concat \n\n#extracting class from the original dataframe and dropping that column\ndef Y_train_extract(df):\n    y_train = df.Class\n    df1 = df.drop(columns = 'Class')\n    return y_train,df1\n\n#doing explorating data analysis\ndef eda(df):\n    #encoding into onehot of the columns 'V2', 'V3', 'V4', 'V9'\n    df = encode_concate(df,df.V2,'V2')\n    df = encode_concate(df,df.V3,'V3')\n    df = encode_concate(df,df.V4,'V4')\n    df = encode_concate(df,df.V9,'V9')\n    \n    #dropping the column 'V11' as some of its category are not there in training data\n    df = df.drop(columns = 'V11')\n\n    #dropping columns of 'V11' from test data which are not there in training data\n    try:\n        df = df.drop(columns = ['V119','V1110','V1111'])\n    except:\n        pass\n    \n#     std = StandardScaler()\n#     df_for_x = df.drop(columns=['Class'])\n#     normalize = Normalizer().fit(df)\n#     X = normalize.transform(df)\n#     X = std.fit_transform(df)\n#     return X, df\n    return df\n\n#dropping the unnamed column from the data\ndef drop_unecessary(df):\n    df = df.drop(columns = ['Unnamed: 0'])\n    return df","716f3565":"y_train, df_new = Y_train_extract(df)\n\ndf_new = drop_unecessary(df_new)\n# X_train, df_new = eda(df_new)\nX_train = eda(df_new)\ny_train = np.array(y_train)\n\ndf_test_prime = drop_unecessary(df_test)\n# test, df_test_new = eda(df_test_prime)\ntest = eda(df_test_prime)\ntest = np.array(test)","fe9e622b":"sm = SMOTE(random_state=12, ratio = 1)\nX_train, y_train = sm.fit_sample(X_train, y_train)","e529e848":"seed = 7\ntest_size = 0.25\nX_train_prime, X_test_prime, y_train_prime, y_test_prime = train_test_split(X_train, y_train, test_size=test_size, random_state=seed, shuffle = True)","a38ea686":"model = xgb.XGBClassifier(n_estimators=320, n_jobs=-1)\nmodel.fit(X_train_prime, y_train_prime)","27e31b35":"y_pred = model.predict(X_test_prime)\npredictions = [round(value) for value in y_pred]\n\naccuracy = accuracy_score(y_test_prime, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n\ny_true = y_test_prime\ny_scores = predictions\nroc_auc_score(y_true, y_scores)","f564904a":"test_pred = model.predict_proba(test)\ntest_prime = pd.DataFrame(test_pred)\ntest_prime.columns = ['a', 'b']\nsample = pd.read_csv('..\/input\/webclubrecruitment2019\/SAMPLE_SUB.csv')\nsample.PredictedValue = test_prime['b']\nsample.to_csv('predfinal_5.csv',index= False)\nsample.head()","ea23e3a6":"# model_1 = pd.read_csv('..\/input\/f-ka-baap\/f_ka_baap.csv')\n# model_2 = pd.read_csv('..\/input\/f-ka-baap-2\/f_ka_baap_2.csv')\n# model_3 = pd.read_csv('..\/input\/final-pred\/predfinal_5.csv')\n# model_4 = pd.read_csv('..\/input\/final-pred\/predfinal_2.csv')\n# model_5 = pd.read_csv('..\/input\/final-pred\/predfinal_3.csv')\n# model_6 = pd.read_csv('..\/input\/final-pred\/predfinal_4.csv')\n\n# ensemble_model = model_1.copy()\n# ensemble_model['PredictedValue'] = (model_1['PredictedValue'] + model_2['PredictedValue'] + model_3['PredictedValue'] + model_4['PredictedValue'] + model_5['PredictedValue'] + model_6['PredictedValue'])\/6\n# ensemble_model.to_csv('final_pred.csv', index = False)\n\n# ensemble_model.head()","fa1b57c2":"## Doing the ensemble of different models, tuning the same model","a656976b":"## Extracting the final data from the above function","124c8c55":"## Seeing the test Data","ce96a215":"## Importing Data","fab691e3":"# Solution Notebook for the Contest\n### Here I had used the ensemble of some models and tuning of hyperparameter like n_estimators used are 300, 315, 320, 330 and some other model like AdaBoost, Random Forest, Voting Classifier, GBM, LightGBM etc...  but I had done on some other notebook also and this is the clean and documented version so here is the final algorithms which I used threfore I had included my prediction files which are the prediction of other models.","4098fb51":"## Upsampling the data using smote algorithm as data is highly unbalanced","a4933c43":"## Applying XGBoost as machine learning model for training","f0230fed":"## Seeing the training Data","a0c7782e":"## Data Preprocessing","665592d4":"## Predicting on the test dataset and converting that to csv file","ca0aa2c0":"## Predicting on validation set and calculating the Accuracy,ROCscore and F1 score","e9ca6f3d":"## Spliting the data into training and validation set","043b15aa":"## Importing Dependencies"}}