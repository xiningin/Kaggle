{"cell_type":{"5decf7aa":"code","63ff971e":"code","97f72aac":"code","e05e30b7":"code","e1693b4d":"code","28c524a0":"code","486a277e":"code","2bdafdcc":"code","ba344274":"code","920252bb":"code","b5691d4f":"code","68fdd6c2":"code","b9aa9ab4":"markdown","dfd279ff":"markdown","545df4d0":"markdown","8cae2252":"markdown","28f86310":"markdown","fd76e9a6":"markdown","3e0ca2dd":"markdown","1eda61d7":"markdown","05ed9a92":"markdown","198316b5":"markdown","f774238e":"markdown"},"source":{"5decf7aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63ff971e":"from tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, GRU, Embedding,CuDNNGRU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nimport string\nfrom textblob import TextBlob\nimport spacy\nnlp=spacy.load('en_core_web_sm')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, precision_score,recall_score\nimport seaborn as sns","97f72aac":"def read_data():\n    dataset=pd.read_csv('..\/input\/sentiment-analysis-for-financial-news\/all-data.csv',\n                header=None,\n                 names=['Sentiment','News'])\n    return dataset\n\ndef convert_to_lower_case():\n    def lower(input_text):\n        return input_text.lower()\n    dataset['News']=dataset['News'].apply(lower)\n    \ndef remove_punctuation():\n    def remove_punctuation_from_text(input_text):\n        output_list=[word for word in input_text.split() if word.isalpha()]\n        return ' '.join(output_list)    \n    dataset['News']=dataset['News'].apply(remove_punctuation_from_text)\n    \ndef correct_words():\n    def correct_text(input_text):\n        list_1=[str(TextBlob(word).correct()) for word in input_text.split()]\n        output_text= ' '.join(list_1)\n        return output_text\n    dataset['News']=dataset['News'].apply(correct_text)\n    \ndef lemmatize():\n    def lematize_text(input_text):\n        doc=nlp(input_text)\n        lemmas=[token.lemma_ for token in doc]\n        output_text=' '.join(lemmas)\n        return output_text\n    dataset['News']=dataset['News'].apply(lematize_text)\n    \ndef remove_stopwords():\n    def remove_stopwords_from_text(input_text):\n        stopwords=spacy.lang.en.stop_words.STOP_WORDS\n        output_list=[word for word in input_text.split() if word not in stopwords and not(word=='-PRON-') ]\n        return ' '.join(output_list)\n    dataset['News']=dataset['News'].apply(remove_stopwords_from_text)\n\ndef filter_the_neutral_news():\n    return dataset[dataset['Sentiment']!='neutral']\n\ndef create_target_and_input():\n    target=dataset['Sentiment'].values.tolist()\n    target=[1 if sentiment=='positive' else 0 for sentiment in target]\n    data=dataset['News'].values.tolist()\n    return data,target\n\ndef split_train_test():\n    x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42,stratify=target)\n    y_train=np.array(y_train)\n    y_test=np.array(y_test)\n    return x_train, x_test, y_train, y_test ","e05e30b7":"#reading the data\ndataset=read_data()\n#Preprocessing the text\nconvert_to_lower_case()\nremove_punctuation()\nlemmatize()\nremove_stopwords()\n#Preparing data for model\ndataset=filter_the_neutral_news()\ndata, target=create_target_and_input()\nx_train, x_test, y_train, y_test =split_train_test()","e1693b4d":"#setting a threshold for the number of words that we are going to use\n\nnum_words=1000 # number of words that we are going to use. It takes top 1k words with the highest frequency\ntokenizer=Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(data)\n\n#tokenization\nx_train_tokens=tokenizer.texts_to_sequences(x_train)\nx_test_tokens=tokenizer.texts_to_sequences(x_test)\n\n#setting a threshold for the number of words in each text\nnum_tokens=[len(tokens) for tokens in x_train_tokens+x_test_tokens]\nnum_tokens=np.array(num_tokens)\nmax_tokens=np.mean(num_tokens)+2*np.std(num_tokens)\nmax_tokens=int(max_tokens)\n\n#padding\nx_train_pad=pad_sequences(x_train_tokens,\n                              maxlen=max_tokens)\nx_test_pad=pad_sequences(x_test_tokens,\n                         maxlen=max_tokens)\n","28c524a0":"print('{:.2f}'.format(np.sum(num_tokens<max_tokens)\/len(num_tokens)))","486a277e":"#creating model\nmodel=Sequential()\nembedding_size=50  # we will create a 50 size vector for each word.\n#At the beginning we will use random word vectors and each optimization step these vectors will be  \nmodel.add(Embedding(input_dim=num_words,\n                    output_dim=embedding_size,\n                    input_length=max_tokens,\n                    name='embedding_layer')\n) # this Embedding layer will take a text as an input, convert it to a vector as an output\n\nmodel.add(GRU(units=16, # number of neurons \n              return_sequences=True) # if true this layer odel creates multiple outputs. If the following layer has one neuron, which means the following layer creates the output. \n)\nmodel.add(GRU(units=8, return_sequences=True))\nmodel.add(GRU(units=4))\nmodel.add(Dense(1,activation='sigmoid'))#with the sigmoid activation function, we receive an output between 0 and 1.\noptimizer=Adam(lr=1e-3)","2bdafdcc":"model.compile(loss='binary_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])","ba344274":"model.summary()","920252bb":"model.fit(x_train_pad,\n          y_train,\n          epochs=5)","b5691d4f":"result=model.evaluate(x_test_pad,\n                      y_test)","68fdd6c2":"#model success on the test dataset\ny_test_pred=model.predict(x=x_test_pad)\ny_test_pred=y_test_pred.T[0]\ny_test_pred=np.array([1.0 if p>0.5 else 0.0 for p in y_test_pred])\n\nprecision_scr=precision_score(y_test, y_test_pred)\nrecall_scr=recall_score(y_test, y_test_pred)\n\nprint('Precision Score: {:.2f}'.format(precision_scr))\nprint('Recall Score: {:.2f}'.format(recall_scr))","b9aa9ab4":"Loading dependencies","dfd279ff":"Reading data, preprocessing text, splitting dataset as train and test","545df4d0":"After setting the max tokens threshold value, let's find the ratio of the text data which doesn't require to be modified. ","8cae2252":"# Model Results\n\nWith this model, I am able to predict %82 of the sentiment successfuly.\n\nOur shooting ratio is %89, which means when the model makes 100 precition as positive, 89 of them are true.\n\nOur catch ratio is %84, which means the model is able to predict sucessfully 84 of the all positive cases.\n\nIf you want to increase those ratios and have a better predictor, you should more focus on the preprocessing step and the model parameters tuning step.","28f86310":"Building the model","fd76e9a6":"Testing the model performance","3e0ca2dd":"Defining functions for loading and preprocessing","1eda61d7":"Preparing dataset for modelling.","05ed9a92":"Fitting the model","198316b5":"Model summary","f774238e":"In this notebook, I worked on a news dataset to create a model to predict the sentiment in those news headlines.\nThe dataset has 2 features and 4.8k rows. The features are news headlines and the sentiment label. \n\nIn text mining analysis, the most important part is data preprocessing. Analysis with raw data or nearly raw data might mislead you. After completing this step carefully, you may be lucky to build a successful model and make correct estimations.\n\nThere are a couple of steps in text preprocessing phase: standardization, data cleaning.\nLowering letters, removing numbers and punctuations, lemmatization and removing stopwords are the substeps to completing standardization and data cleaning part.\n\nLowering letters is the first step of standardization and cleaning. Same words with upper and lower case letters are processed as different words in text analysis.Converting all charachers to lowercase prevents us from this situation.\n\nRemoving numbers and punctuations is the another step however you should be careful about it. This step could be modified or skipped according to the  answers you try to get.\n\nLemmatization is a important step in text analysis. With this step, words are reduced to their base forms. \nRemoving stop words is the last step. Like other types of the datasets, text data contains noisy words. After applying this step, the words that might have valuable information will remain.\n\nAfter the text preprocessing steps, I try to build a classifier for sentiment prediction and review the model outputs at the end of this notebook."}}