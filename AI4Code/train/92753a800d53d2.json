{"cell_type":{"c9ed65a8":"code","54deecb7":"code","f240fbd9":"code","df2a7e8d":"code","361ded6f":"code","2664a92d":"code","45235fee":"code","127b2d29":"code","048e7bde":"code","dd3873e4":"code","079563ff":"code","9a5a5a8d":"code","b180abb4":"code","de25a996":"code","76388d13":"code","83c72a10":"code","e0c20faa":"code","f07094ea":"code","53221c30":"code","8f8155da":"code","2bac1529":"code","51292397":"code","8ba6dae9":"code","30b46400":"code","82c5b204":"code","28a4179e":"code","27677d8a":"markdown","d144ada6":"markdown","eb7d4651":"markdown","d0362485":"markdown","6f2f78d6":"markdown","5fa42831":"markdown","490d86fa":"markdown","eb976d8c":"markdown","20aca976":"markdown","9dfb8f1b":"markdown","b545d630":"markdown","8404b995":"markdown","6998ae60":"markdown","e505bf63":"markdown","76ef6ef0":"markdown","3a6ff4f4":"markdown","4fffbe3b":"markdown","eff7fd85":"markdown","df305a2f":"markdown","deb24a5b":"markdown","580e6f0f":"markdown","d030d9dc":"markdown"},"source":{"c9ed65a8":"!pip install pytorch-tabnet\n\nimport os\nimport gc\n\nimport numpy  as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 18})\nplt.style.use('ggplot')\nimport seaborn as sns\nfrom scipy import stats\n\nimport wandb\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","54deecb7":"df_train = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ndf_test  = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\ndf_sample = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')","f240fbd9":"df_train = df_train[:10000]\ndf_test = df_test[:10000]\ncommon_features = ['breath_id','R','C','time_step','u_in','u_out']\nnumerical_features = ['time_step','u_in']\ncategorical_features = ['R','C','u_out','breath_id']\n","df2a7e8d":"plt.figure(figsize = (25,11))\nsns.heatmap(df_train.isna().values, cmap = ['#ffd514','#ff355d'], xticklabels=df_train.columns)\nplt.title(\"Missing values in training Data\", size=20);","361ded6f":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","2664a92d":"CONFIG = dict(competition = 'VentilatorPressurePrediction',_wandb_kernel = 'tensorgirl')","45235fee":"# Save train data to W&B Artifacts\nrun = wandb.init(project='GoogleBrainVentilatorPressurePrediction', name='training_data', anonymous=anony,config=CONFIG) \nartifact = wandb.Artifact(name='training_data',type='dataset')\nartifact.add_file(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","127b2d29":"# basic stats of features\ndf_train.describe().style.background_gradient(cmap=\"Pastel1\")","048e7bde":"def kdeplot_features(df_train,df_test, feature, title):\n    '''Takes a column from the dataframe and plots the distribution (after count).'''\n    \n    values_train = df_train[feature].to_numpy()\n    values_test = df_test[feature].to_numpy()  \n     \n    plt.figure(figsize = (18, 3))\n    \n    sns.kdeplot(values_train, color = '#ffd514')\n    sns.kdeplot(values_test, color = '#ff355d')\n    \n    plt.title(title, fontsize=15)\n    plt.legend()\n    plt.show();\n    \n    del values_train , values_test\n    gc.collect()\n    \ndef countplot_features(df_train, feature, title):\n    '''Takes a column from the dataframe and plots the distribution (after count).'''\n    \n           \n    plt.figure(figsize = (10, 5))\n    \n    sns.countplot(df_train[feature], color = '#ff355d')\n        \n    plt.title(title, fontsize=15)    \n    plt.show();\n    \n        \ndef create_wandb_hist(x_data=None, x_name=None, title=None, log=None):\n    '''Create and save histogram in W&B Environment.\n    x_data: Pandas Series containing x values\n    x_name: strings containing axis name\n    title: title of the graph\n    log: string containing name of log'''\n    \n    data = [[x] for x in x_data]\n    table = wandb.Table(data=data, columns=[x_name])\n    wandb.log({log : wandb.plot.histogram(table, x_name, title=title)})","dd3873e4":"# plot distributions of features\nfor feature in common_features:\n    kdeplot_features(df_train,df_test, feature=feature, title = feature + \" distribution\")","079563ff":"# Log Plots to W&B environment\ntitle = \"Distribution of features\"\nrun = wandb.init(project='GoogleBrainVentilatorPressurePrediction', name=title,anonymous=anony,config=CONFIG)\nfor feature in common_features:\n    title = \"Distribution of \"+feature    \n    create_wandb_hist(x_data=df_train[feature],x_name=feature , title=title,log=\"hist\")    \nwandb.finish()\n\ntitle = \"Countplot Distribution\"\nrun = wandb.init(project='GoogleBrainVentilatorPressurePrediction', name=title,anonymous=anony,config=CONFIG)    \nfor feature in categorical_features:\n    fig = countplot_features(df_train, feature=feature, title = feature + \" countplot distribution\")\n    wandb.log({feature + \" countplot distribution\": fig})\nwandb.finish()","9a5a5a8d":"# plot distributions of categorical features\nfor feature in categorical_features:\n    countplot_features(df_train, feature=feature, title = \"Frequency of \"+ feature)","b180abb4":"#histogram\nsns.distplot(df_train['pressure'],color = '#ff355d');\nfig = plt.figure()\nres = stats.probplot(df_train['pressure'], plot=plt)","de25a996":"#Target vs Numerical Features\nfor feature in numerical_features:\n    sns.jointplot(df_train['pressure'],df_train[feature],color = '#ff355d', kind = \"kde\")     \n    plt.show()","76388d13":"#Target vs Categorical Features\nfor feature in categorical_features:\n    sns.boxplot(df_train[feature],df_train['pressure'] ,color = '#ff355d')     \n    plt.show()","83c72a10":"#code copied from https:\/\/www.kaggle.com\/vincenttu\/google-vent-eda\ntrain_breath_id_2 = df_train[df_train.breath_id == 2] \ntrain_breath_id_2.style.background_gradient(cmap=\"Pastel1\")\n","e0c20faa":"x = range(80)\nplt.figure(figsize = (10, 5))\ny1 = train_breath_id_2.u_in\ny2 = train_breath_id_2.u_out\n\nplt.xlabel(\"Time\")\nplt.ylabel(\"u_in\/u_out range\")\nsns.lineplot(x, y1, label=\"u_in\",color = '#ffd514')\nsns.lineplot(x, y2, label=\"u_out\",color = '#ff355d')\nplt.legend()","f07094ea":"plt.figure(figsize = (10, 5))\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pressure\")\nplt.plot(x, train_breath_id_2.pressure.values, label=\"pressure\")\nplt.ylabel(\"pressure\/u_in\/u_out\")\nsns.lineplot(x, y1, label=\"u_in\",color = '#ffd514')\nsns.lineplot(x, y2, label=\"u_out\",color = '#ff355d')\nplt.legend()\nplt.legend()","53221c30":"plt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(df_train.corr().values,linewidths=0.1,vmax=1.0,square=True, cmap=\"Pastel1\", linecolor='white', annot=True)","8f8155da":"sns.pairplot(df_train , height = 2.5 , hue = \"R\")","2bac1529":"#Code copied from https:\/\/www.kaggle.com\/ryanbarretto\/tensorflow-lstm-baseline\n\ndf_train['u_in_cumsum'] = (df_train['u_in']).groupby(df_train['breath_id']).cumsum()\ndf_test['u_in_cumsum'] = (df_test['u_in']).groupby(df_test['breath_id']).cumsum()\ndf_train['u_in_lag'] = df_train['u_in'].shift(2)\ndf_train = df_train.fillna(0)\ndf_test['u_in_lag'] = df_test['u_in'].shift(2)\ndf_test = df_test.fillna(0)","51292397":"# Code copied from https:\/\/www.kaggle.com\/tolgadincer\/tensorflow-bidirectional-lstm-0-234\n\ndf_train['area'] = df_train['time_step'] * df_train['u_in']\ndf_train['area'] = df_train.groupby('breath_id')['area'].cumsum()\n\ndf_test['area'] = df_test['time_step'] * df_test['u_in']\ndf_test['area'] = df_test.groupby('breath_id')['area'].cumsum()","8ba6dae9":"X      = df_train[common_features]\ny      = df_train[\"pressure\"]\nX_test = df_test[common_features]","30b46400":"X      = X.to_numpy()\ny      = y.to_numpy().reshape(-1, 1)\nX_test = X_test.to_numpy()","82c5b204":"regressor = TabNetRegressor(verbose=0,seed=42)\nregressor.fit(X_train=X, y_train=y,max_epochs=5,eval_metric=['mae'])","28a4179e":"output = regressor.predict(X_test)","27677d8a":"# **<span style=\"color:#F7B2B0;\">Correlation of Features<\/span>**","d144ada6":"# Work in progress \ud83d\udea7","eb7d4651":"Logging plots to W&B dashboard","d0362485":"# **<span style=\"color:#F7B2B0;\">W & B Artifacts<\/span>**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https:\/\/docs.wandb.ai\/guides\/artifacts)\n\n![](https:\/\/drive.google.com\/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)","6f2f78d6":"# **<span style=\"color:#F7B2B0;\">Analysis for single breath_id<\/span>**","5fa42831":"![](https:\/\/drive.google.com\/uc?id=1ubiwsZtL3GcfnrMhJI_6Ls_73qrnRwPH)","490d86fa":" # <h1 style='background:#F7B2B0; border:0; color:black'><center>Google Brain - Ventilator Pressure Prediction<\/center><\/h1> ","eb976d8c":"# **<span style=\"color:#F7B2B0;\">References<\/span>**\n\nhttps:\/\/arxiv.org\/pdf\/1908.07442.pdf\n\nhttps:\/\/towardsdatascience.com\/tabnet-e1b979907694\n\n@karnikakapoor Header styles \n\n@debarshichanda Wandb Content","20aca976":"# **<span style=\"color:#F7B2B0;\">Numerical Variables Vs Target<\/span>**","9dfb8f1b":"What do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier.\n\n# **<span style=\"color:#F7B2B0;\">Goal<\/span>**\n \nThe goal is to simulate a ventilator connected to a sedated patient's lung by taking lung attributes compliance and resistance into account.\n\n# **<span style=\"color:#F7B2B0;\">Data<\/span>**\n\nEach time series represents an approximately 3-second breath. The files are organized such that each row is a time step in a breath and gives the two control signals, the resulting airway pressure, and relevant attributes of the lung, described below.\n\n**Files**\n> - ``` train.csv``` - the training set\n> - ```test.csv``` - the test set\n> - ```sample_submission.csv``` - a sample submission file in the correct format\n\n**Columns**\n> - ```id``` - globally-unique time step identifier across an entire file\n> - ```breath_id``` - globally-unique time step for breaths\n> - ```R``` - lung attribute indicating how restricted the airway is (in cmH2O\/L\/S). Physically, this is the change in pressure per change in flow (air volume per time). Intuitively, one can imagine blowing up a balloon through a straw. We can change R by changing the diameter of the straw, with higher R being harder to blow.\n> - ```C``` - lung attribute indicating how compliant the lung is (in mL\/cmH2O). Physically, this is the change in volume per change in pressure. Intuitively, one can imagine the same balloon example. We can change C by changing the thickness of the balloon\u2019s latex, with higher C having thinner latex and easier to blow.\n> - ```time_step``` - the actual time stamp.\n> - ```u_in``` - the control input for the inspiratory solenoid valve. Ranges from 0 to 100.\n> - ```u_out``` - the control input for the exploratory solenoid valve. Either 0 or 1.\n> - ```pressure``` - the airway pressure measured in the respiratory circuit, measured in cmH2O\n\n# **<span style=\"color:#F7B2B0;\">Evaluation Metric<\/span>**\n\nThe competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored. The score is given by:\n\n                                        |X-Y|\n\nwhere  is the vector of predicted pressure and  is the vector of actual pressures across all breaths in the test set.\n","b545d630":"# **<span style=\"color:#F7B2B0;\">Missing Values<\/span>**","8404b995":"**Observations:**\n\nAll R and C pairs are the same for any given Breath ID.","6998ae60":"# **<span style=\"color:#F7B2B0;\">TabNet<\/span>**\n\nTabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. \n\nThe main features of TabNet are \n\nThe main contributions are summarized as:\n\n\ud83d\udccc TabNet inputs raw tabular data without any preprocessing\n\n\ud83d\udccc TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and better learning as the learning capacity\n\n\ud83d\udccc TabNet outperforms or is on par with other tabular learning models on various datasets for classification and regression problems from different domains\n\n\ud83d\udccc TabNet shows significant performance improvements by using unsupervised pre-training to predict masked features \n\n![](https:\/\/drive.google.com\/uc?id=1snKduiQHakIeulnr7jKwt2uQvmv8rDcl)\n\n[Source](https:\/\/arxiv.org\/pdf\/1908.07442.pdf)\n\n# **<span style=\"color:#F7B2B0;\">TabNet for Timeseries Data<\/span>**\n\nSome resources using TabNet for timeseries data\n\n[Github](https:\/\/github.com\/AlbertoCastelo\/tabnet-timeseries-spike)\n\nShort Term Load Forecasting using TabNet - MDPI\n\nRainfall Forecast using TabNet - MDPI\n\n\nThe below explanation is taken from medium article [here](https:\/\/towardsdatascience.com\/tabnet-e1b979907694)\n\n# **<span style=\"color:#F7B2B0;\">Steps:<\/span>**\n\nEach Step is a block of components. The number of Steps is a hyperparameter option when training the model. Increasing the number of steps will increase the learning capacity of the model, but will also increase training time, memory usage and the chance of overfitting.Each Step gets its own vote in the final classification and these votes are equally weighted. This mimics an ensemble classification.\n\n# **<span style=\"color:#F7B2B0;\">Feature Transformer:<\/span>**\n\nThe Feature Transformer is a network which has an architecture of its own.It has multiple layers, some of which are shared across every Step while others are unique to each Step. Each layer contains a fully connected layer, batch normalisation and a Gated Linear Unit activiation.\n\nSharing some layers between decision Steps leads to \u201cparameter-efficient and robust learning with high capacity\u201d and that normalization with root 0.5 \u201chelps to stabilize learning by ensuring that the variance throughout does not change dramatically\u201d. The output of the feature transformer uses a ReLU activation function.\n\n![](https:\/\/drive.google.com\/uc?id=1iuVE-7hkmh2ZMFfY3FdrZ1UbptidK-mI)\n\n# **<span style=\"color:#F7B2B0;\">Feature Selection :<\/span>**\n\nOnce features have been transformed, they are passed to the Attentive Transformer and the Mask for feature selection.The Attentive Transformer is comprised of a fully connected layer, batch normalisation and Sparsemax normalisation. It also includes prior scales, meaning it knows how much each feature has been used by the previous steps. This is used to derive the Mask using the processed features from the previous Feature Transformer.\n\n![](https:\/\/drive.google.com\/uc?id=12PNJHZqt7bso16m0H8NZ0wrDq9uLdX0U)\n\nThe Mask ensures the model focuses on the most important features and is also used to derive explainability. It essentially covers up features, meaning the model is only able to use those that have been considered important by the Attentive Transformer.We can also understand feature importance by looking at how much a feature has been masked for all decisions and and an individual prediction.\nTabNet employs soft feature selection with controllable sparsity in end-to-end learning\nThis means one model jointly performs feature selection and output mapping, which leads to better performance.TabNet uses instance-wise feature selection, which means features are selected for each input and each prediction can use different features.\nThis feature selection is essential as it allows decision boundaries to be generalised to a linear combination of features, where coefficients determine the proportion of each feature, which in the end leads to the model\u2019s interpretability","e505bf63":"# **<span style=\"color:#F7B2B0;\">Distribution of Target Variable - Pressure<\/span>**","76ef6ef0":"# **<span style=\"color:#F7B2B0;\">Distribution of Features<\/span>**\n\n","3a6ff4f4":"<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts!\n> \n> [Google Brain - Ventilator Pressure Prediction Project on W&B Dashboard](https:\/\/wandb.ai\/usharengaraju\/GoogleBrainVentilatorPressurePrediction)\n> \n> - To get the API key, create an account in the [website](https:\/\/wandb.ai\/site) .\n> - Use secrets to use API Keys more securely \n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\"> Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. <strong>Kaggle competitions require fast-paced model development and evaluation<\/strong>. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.<\/span>\n\n> <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">\u23f3 Lots of components = Lots of places to go wrong = Lots of time spent debugging<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">W&B can be useful for Kaggle competition with it's lightweight and interoperable tools:<\/span>\n\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Quickly track experiments,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Version and iterate on datasets, <br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Evaluate model performance,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Reproduce models,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Visualize results and spot regressions,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Share findings with colleagues.<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">To learn more about Weights and Biases check out this <strong><a href=\"https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases\">kernel<\/a><\/strong>.<\/span>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","4fffbe3b":" # **<span style=\"color:#F7B2B0;\">Feature Engineering<\/span>**","eff7fd85":"Snapshot of the artifacts created  \n\n![](https:\/\/drive.google.com\/uc?id=16ROHOYdW3ewGESfCwewUWW8X3mvNbFKT)","df305a2f":"# **<span style=\"color:#F7B2B0;\">Categorical Variables Vs Target<\/span>**","deb24a5b":"To reduce the running I have taken only 10,000 rows for visualization and modelling . If you wish to take entire dataset , comment out the below code ","580e6f0f":"# **<span style=\"color:#F7B2B0;\">Frequency Distribution of Categorical Features<\/span>**\n\n","d030d9dc":"The code below is inspired from @vincenttu Brilliant EDA notebook . Kindly upvote his work [here](https:\/\/www.kaggle.com\/vincenttu\/google-vent-eda)"}}