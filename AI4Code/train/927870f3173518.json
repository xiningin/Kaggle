{"cell_type":{"4b9fc389":"code","d55d2594":"code","e1c3f70c":"code","9eaceec1":"code","701e8af5":"code","94e7a365":"code","8fa06419":"code","deb40240":"code","839922c9":"code","03437329":"code","2b87983f":"code","2f588286":"code","1fa5cf72":"code","c6ff233a":"code","1eb33509":"code","a6212173":"code","8dfc80c3":"code","61b814b1":"code","c66d81cf":"code","370d4390":"code","88c8480b":"code","38fd7409":"code","107fea16":"code","08ecf620":"code","5c1b7978":"code","4ed81c04":"code","f4d74c79":"code","7dbf25d9":"code","10687ac2":"code","38e446d9":"code","f7723cb4":"code","d76e7455":"code","2a27057a":"code","7f863f72":"code","8ee14d6f":"code","1998a01d":"code","1784f419":"code","5623eb99":"code","87db0442":"code","0784aced":"code","b507a069":"code","16aa9f3f":"code","5e29875a":"code","52063315":"code","e833a90a":"code","814d3b1e":"code","3279f3ce":"code","0dbe3dff":"code","2653dd4f":"code","5a03cc21":"code","ef227287":"code","2ee9a378":"code","0fcf3f40":"code","e3f7204a":"code","6a87e2b3":"code","2541ae6e":"code","ce879588":"code","809da79f":"code","6b2bb466":"code","efa3548c":"code","6eee3576":"code","72c75369":"code","f419f3a6":"code","4fa30410":"code","8d0ae849":"code","94a1c619":"code","fa4533eb":"code","77c2b52a":"code","21394a10":"code","6222ad4f":"code","b53233b5":"code","d7d1e39c":"code","55f819c4":"code","037fdeb4":"code","a7d390ba":"code","9ece6dcf":"code","0a2adc2e":"code","fb15fa32":"code","056efc67":"code","d54b87a5":"code","94fafcdf":"code","2cccb4c2":"markdown","7525c70a":"markdown","4065b113":"markdown","fc66a898":"markdown","6cbc46fc":"markdown","8762c1e8":"markdown","28ec437b":"markdown","22c19a04":"markdown","10c5b0d3":"markdown","4b4e556e":"markdown","2342a5d7":"markdown","d7c2a8a1":"markdown","ac610b45":"markdown","1ea7cc99":"markdown","aeb5f526":"markdown","02b41642":"markdown","c2f2c433":"markdown","855db2fe":"markdown","5925318e":"markdown","82506ec3":"markdown","81ef9b8d":"markdown","3f4bd824":"markdown","095701f6":"markdown","5f279b43":"markdown","a6762922":"markdown","f94e5ddd":"markdown","08f8c0ac":"markdown","79adc4b3":"markdown","38f63a08":"markdown","ee7c0784":"markdown","ed1206ee":"markdown","b303e792":"markdown","1f9b0f7f":"markdown","9bbb673a":"markdown","6b0ee665":"markdown","5253f591":"markdown","c3932981":"markdown","2d749400":"markdown","1a4c0aa3":"markdown","e59d1ce9":"markdown","f1c7503d":"markdown","e5be5f15":"markdown","461583a2":"markdown","9daa4277":"markdown","b47fb2a5":"markdown","124e07dc":"markdown","dfce4ae6":"markdown","f2f423c4":"markdown","16c2b734":"markdown","c8b4d34f":"markdown","35d0d99c":"markdown","635e52cd":"markdown","7ac69f09":"markdown","7cec57e2":"markdown","6788ca6e":"markdown","187647d5":"markdown","06462062":"markdown","b44854a3":"markdown","d46cd751":"markdown","a7f7c3de":"markdown","01c30769":"markdown","9730a1f5":"markdown","6b704bd5":"markdown","ced487a2":"markdown","c352e13b":"markdown","65b83482":"markdown","f5681d07":"markdown","837c10a2":"markdown","55bea779":"markdown","d70b26a9":"markdown","591b375d":"markdown","5301be78":"markdown","48d9db0f":"markdown","05e5abfb":"markdown","6ca42473":"markdown","140d5d54":"markdown","1f2b9f6c":"markdown"},"source":{"4b9fc389":"%matplotlib inline","d55d2594":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter","e1c3f70c":"# Text processing library\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","9eaceec1":"# Deep Learning library\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.layers import Conv1D, Dropout, MaxPooling1D\n\nfrom tensorflow.keras import backend as K","701e8af5":"train_data = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/train.txt\", header = None)\nvalidation_data = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/val.txt\", header = None)\ntest_data = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/test.txt\", header = None)","94e7a365":"train_data.head()","8fa06419":"validation_data.head()","deb40240":"test_data.head()","839922c9":"train_data.shape, validation_data.shape, test_data.shape","03437329":"def rename_column(dataset):\n    \"\"\"\n    Function: Renames a column of a dataset\n    \n    Arguments: dataset\n    \n    Returns: dataset with renamed column\n    \"\"\"\n    dataset.rename(columns = {0: \"description\"}, inplace = True)\n    \n    return dataset","2b87983f":"rename_column(train_data)\nrename_column(validation_data)\nrename_column(test_data)","2f588286":"def separate_emotion(dataset):\n    \"\"\"\n    Function: Separates label from features\n    \n    Arguments: dataset\n    \n    Returns: A dataset with separated features and labels\n    \"\"\"\n    dataset[[\"text\", \"emotion\"]] = dataset[\"description\"].str.split(\";\", expand = True)\n    dataset = dataset.drop(\"description\", axis = 1)\n    \n    return dataset","1fa5cf72":"train_data = separate_emotion(train_data)\nvalidation_data = separate_emotion(validation_data)\ntest_data = separate_emotion(test_data)","c6ff233a":"train_data.head()","1eb33509":"print(\"Unique emotions in training set: \", train_data.emotion.unique())\nprint(\"Unique emotions in validation set: \", validation_data.emotion.unique())\nprint(\"Unique emotions in testing set: \", test_data.emotion.unique())","a6212173":"train_data.dtypes, validation_data.dtypes, test_data.dtypes","8dfc80c3":"train_data.isna().any(), validation_data.isna().any(), test_data.isna().any()","61b814b1":"def plot_emotion_counts(dataset, title = None):\n    \"\"\"\n    Function: Displays class distribution\n    \n    Arguments: dataset, title (optional)\n    \"\"\"\n    plt.figure(figsize = (6, 4))\n    plt.bar((dataset.groupby(\"emotion\").size()).index, dataset.groupby(\"emotion\").size(), \n           color= [\"orangered\", \"salmon\", \"royalblue\", \"lightseagreen\", \"gold\", \"seagreen\"])\n    if title is not None:\n        plt.title(title)\n    plt.show()","c66d81cf":"plot_emotion_counts(train_data, \"Emotion types in training data\")","370d4390":"plot_emotion_counts(validation_data, \"Emotion types in validation data\")","88c8480b":"plot_emotion_counts(test_data, \"Emotion types in testing data\")","38fd7409":"emotions_encoded = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"love\": 3, \"sadness\": 4, \"surprise\": 5}","107fea16":"def encode_emotions(dataset):\n    \"\"\"\n    Function: Encode labels\n    \n    Arguments: dataset\n    \n    Returns: Encoded labels\n    \"\"\"\n    dataset.replace(emotions_encoded, inplace = True)\n    \n    return dataset","08ecf620":"encode_emotions(train_data)\nencode_emotions(validation_data)\nencode_emotions(test_data)","5c1b7978":"for i in range(20):\n    print(train_data.text[i])","4ed81c04":"words_list = []\nfor value in train_data.text.str.split(' '):\n    words_list.extend(value)","f4d74c79":"len(words_list)","7dbf25d9":"count_words = Counter(words_list)","10687ac2":"len(count_words)","38e446d9":"words_items =  count_words.items()","f7723cb4":"first_words = list(words_items)[:50]","d76e7455":"print(first_words)","2a27057a":"print(count_words.most_common(50))","7f863f72":"stop_words = set(stopwords.words(\"english\"))","8ee14d6f":"meaningful_words = [word for word in words_list if word not in stop_words]","1998a01d":"print(meaningful_words[:50])","1784f419":"len(meaningful_words)","5623eb99":"def remove_stopwords(input_text):\n    \"\"\"\n    Function: Removes stopwords from text\n    \n    Arguments: text\n    \n    Returns: text without stopwords\n    \"\"\"\n    words = input_text.split()\n    clean_words = [word for word in words if word not in stopwords.words(\"english\")]\n    clean_words = \" \".join(clean_words)\n    \n    return clean_words","87db0442":"train_data[\"text\"] = train_data[\"text\"].apply(remove_stopwords)\nvalidation_data[\"text\"] = validation_data[\"text\"].apply(remove_stopwords)\ntest_data[\"text\"] = test_data[\"text\"].apply(remove_stopwords)","0784aced":"train_data.head()","b507a069":"train_data.isna().any(), validation_data.isna().any(), test_data.isna().any()","16aa9f3f":"print(pd.Series(train_data[\"text\"]).str.len().mean())\nprint(pd.Series(train_data[\"text\"]).str.len().max())","5e29875a":"strings_len = pd.Series(train_data[\"text\"]).str.len()","52063315":"strings_len.plot.box()\nplt.title(\"Distribution of strings length\")\nplt.ylabel(\"Number of strings\")\nplt.show()","e833a90a":"strings_len.describe()","814d3b1e":"def separate_features_and_labels(dataset):\n    \"\"\"\n    Function: Separates features and labels\n    \n    Arguments: dataset\n    \n    Returns: Tuple of features and labels in NumPy format\n    \"\"\"\n    features = dataset[\"text\"].to_numpy()\n    labels = dataset[\"emotion\"].to_numpy()\n    \n    return (features, labels)","3279f3ce":"(train_features, train_labels) = separate_features_and_labels(train_data)\n(validation_features, validation_labels) = separate_features_and_labels(validation_data)\n(test_features, test_labels) = separate_features_and_labels(test_data)","0dbe3dff":"train_labels","2653dd4f":"VOCABULARY_SIZE = 16000\nEMBEDDING_DIM = 32\nMAX_LENGTH = 80\nPADDING_TYPE = \"post\"\nTRUNC_TYPE = \"post\"","5a03cc21":"tokenizer = Tokenizer(num_words = VOCABULARY_SIZE)","ef227287":"tokenizer.fit_on_texts(train_features)","2ee9a378":"word_index = tokenizer.word_index","0fcf3f40":"train_sequences = tokenizer.texts_to_sequences(train_features)","e3f7204a":"for i in range (10):\n    print(train_sequences[i])","6a87e2b3":"np.array(train_sequences[42])","2541ae6e":"train_padded = pad_sequences(train_sequences, maxlen = MAX_LENGTH, padding = PADDING_TYPE, truncating = TRUNC_TYPE)","ce879588":"train_padded[42]","809da79f":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])","6b2bb466":"def decode_sentence(text):\n    \"\"\"\n    Function: Reverse tokenization\n    \n    Arguments: List of tokens\n    \n    Returns: A list of strings\n    \"\"\"\n    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])","efa3548c":"print(decode_sentence(train_padded[42]))\nprint(train_features[42])","6eee3576":"validation_sequences = tokenizer.texts_to_sequences(validation_features)\ntest_sequences = tokenizer.texts_to_sequences(test_features)","72c75369":"validation_padded = pad_sequences(validation_sequences, maxlen = MAX_LENGTH, padding = PADDING_TYPE, truncating = TRUNC_TYPE)\ntest_padded = pad_sequences(test_sequences, maxlen = MAX_LENGTH, padding = PADDING_TYPE, truncating = TRUNC_TYPE)","f419f3a6":"OUTPUT_SHAPE = len(set(train_labels))","4fa30410":"tf.keras.backend.clear_session()","8d0ae849":"model = Sequential([\n        Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, input_length = MAX_LENGTH),\n        Dropout(0.2),\n        Conv1D(filters = 256, kernel_size = 3, activation = \"relu\"),\n        MaxPooling1D(pool_size = 3),\n        Conv1D(filters = 128, kernel_size = 3, activation = \"relu\"),\n        MaxPooling1D(pool_size = 3),\n        LSTM(128),\n        Dense(128, activation = \"relu\"),\n        Dropout(0.2),\n        Dense(64, activation = \"relu\"),\n        Dense(OUTPUT_SHAPE, activation = \"softmax\")\n])","94a1c619":"model.summary()","fa4533eb":"plt.hist(model.layers[2].get_weights()[0].ravel(), color = \"coral\")\nplt.xlabel(\"weights of Conv1D layer\")\nplt.ylabel(\"count\")\nplt.title(\"Distribution of weights in Conv1D before training\")\nplt.show()","77c2b52a":"OPTIMIZER = tf.keras.optimizers.Adam(learning_rate = 0.0001)","21394a10":"model.compile(loss = \"sparse_categorical_crossentropy\",\n              optimizer = OPTIMIZER,\n              metrics = [\"accuracy\"])","6222ad4f":"history = model.fit(train_padded, train_labels,\n                    epochs = 150,\n                    validation_data = (validation_padded, validation_labels),\n                    batch_size = 64)","b53233b5":"def plot_graphs(history, string):\n    \"\"\"\n    Function: Displays changes in monitored loss and metrics during model training\n    \n    Arguments: monitoring logs\n    \n    Returns: A line graph with changes of loss and metrics over time\n    \"\"\"\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()","d7d1e39c":"plot_graphs(history, \"loss\")\nplot_graphs(history, \"accuracy\")","55f819c4":"plt.hist(model.layers[2].get_weights()[0].ravel(), color = \"coral\")\nplt.xlabel(\"weights of Conv1D layer\")\nplt.ylabel(\"count\")\nplt.title(\"Distribution of weights in Conv1D after training\")\nplt.show()","037fdeb4":"plt.hist(model.layers[2].get_weights()[1].ravel(), color = \"royalblue\")\nplt.xlabel(\"bias of Conv1D layer\")\nplt.ylabel(\"count\")\nplt.title(\"Distribution of bias in Conv1D after training\")\nplt.show()","a7d390ba":"loss, accuracy = model.evaluate(test_padded, test_labels)","9ece6dcf":"print(f\"Model accuracy is {accuracy * 100}%\")","0a2adc2e":"predicted_proba = np.argmax(model.predict(test_padded), axis =-1)","fb15fa32":"predicted_proba","056efc67":"test_labels","d54b87a5":"confusion_matrix = tf.math.confusion_matrix(test_labels, predicted_proba).numpy()","94fafcdf":"figure = plt.figure(figsize = (6, 5))\nsns.heatmap(confusion_matrix, annot = True, fmt = \".0f\", cmap = plt.cm.Blues)\nplt.tight_layout()\nplt.ylabel(\"Actual class\")\nplt.xlabel(\"Predicted class\")\nplt.title(\"Confusion Matrix of classified emotions\")\nplt.show()","2cccb4c2":"### II. 2. Separate emotion from text","7525c70a":"The output above shows encoding was successful. Now, instead of \"anger\", the value is \"0\".","4065b113":"A brief check shows that more than half of the words used in the training data for describing feelings were stopwords.","fc66a898":"As expected, all values in each dataset are of `object` type. Since this is a text classification problem, it doesn't require any specific actions at the moment.","6cbc46fc":"### II.9 Check strings lenght ","8762c1e8":"Then, text are turned into sequences of numbers. The first 10 rows in the training data are displayed below.","28ec437b":"Optimizer's learning rate is reduced for better performance.","22c19a04":"Most strings are up to 150 words. There are \"outliers\", i.e. longer sentences. Calling `describe` over the data shows that 75% of strings are no longer than 81 words. This is taken into account in the next Chapter. ","10c5b0d3":"To avoid clutter from existing models and layers (when model is being fune-tuned several times), especially when memory is limited, clear_session() resets all prior state generated by Keras.","4b4e556e":"It is interesting to see if and how weights changed after forward and backward propagation. To enable comparisons, weights of the same layer are examined below.","2342a5d7":"Global variables are used in text pre-processing and modelling. Vocabulary size indicates the number of most frequent words the tokenizer is going to get from the text. 16000 is more than the unique words left after removing stopwords, which means that the net will get all words used for describing emotions. Embedding dimension is passed to the Embedding layer of the Neural Network. The maximum length shows the number of tokens the matrix will have. If a sentece is shorter, a function below will add zeroes until its length reaches 80. Padding type indicates where these zeroes to be added - before the first or after the last token; in this case - at the end of the sentence. Truncation type tells the function where to remove tokens from if the row is longer than 80 elements.","d7c2a8a1":"Diagonal elements represent the number of points for which the predicted class is equal to the true class, while off-diagonal elements are those that were mislabelled by the model.\n\nThe emotion against each feeling description was properly recognised but still there are 290 sentences (~ 15% of the training data), which were wrongly classified.","ac610b45":"Thereafter, all words which are not stopwords are stored in `meaningful_words`.","1ea7cc99":"Text pre-processing stage tokenizes words and makes each row the same length. The first step is initializing the Tokenizer. It is instruncted to use as many words as defined in the VOCABULARY_SIZE variable above.","aeb5f526":"`evaluate` method applied over the model checks its performance on unseen data (e.g., the testing sets). In this case, accuracy on the testing data is 85% which is not great but not too bad as well.","02b41642":"### II.6 Explore texts","c2f2c433":"It is nice to see how many times each emotion type (class) is recorded in the respective dataset. The function below helps for displaying emotions distributions.","855db2fe":"For convenience, the only column is renamed. This is performed with a function since the operation should be applied three times.","5925318e":"The confusion matrix below shows how many sentences and the emotions they describe were wrongly classified by the model. To build it, the model computes the probability a given sentence (sequence) to \"respond\" to a given class.","82506ec3":"Various architectures (including Bidirectional layers with LSTM and GRU layers) were tried and tested. Most of them reached accuracy around 96-98% on the training data and no more than 82-83% on the validation set. The one used below returned the best results. It stards with an Embedding layer followed Dropout layer and two blocks of 1-dimentional convolutional and max pooling layers. A LSTM layer is placed on top of the last MaxPooling layer. It is followed by Dense layers, first of which slightly regularized due to model's overfitting. Non-linearity in convolutional and dense layers is performed by \"relu\" activation.","81ef9b8d":"A brief check shows that the feelings in the training data are expressed with more than 300 000 words.","3f4bd824":"A brief check shows that stopwords were successfully removed.","095701f6":"All these figures provide insight as to what happened during training. However, learning process would be more understandable if visualized. Thus, the function below helps for displaying how loss diminished over each epoch, and how much accuracy improved over time.","5f279b43":"## III. Data pre-processing","a6762922":"People use various words and symbols to express emotions. Many of them are redundant or might be considred a noise by a Neural Network. For this reason, before to proceed further, it is important to know how people described their feelings. The code line below displays the first 20 rows in the training data.","f94e5ddd":"Applying `shape` over the datasets shows their dimentions.","08f8c0ac":"`sparse_categorical_crossentropy` is the appropriate function for multiclass classification tasks, where the labels are not one-hot encoded. Model performance is evaluated against its accuracy.","79adc4b3":"Both Machine Learning and Deep Learning algorithms work with numeric values. They cannot handle booleans and strings. For this reason, the words used for expressing emotions are replaced with integers. The dictionary and the function below help for encoding labels (replacing strings with numbers).","38f63a08":"Now, the 42-nd row is 80 tokens long; \"empty\" spaces were filled in with zeroes.","ee7c0784":"Now, weights have more or less Gaussian rather than uniform distribution. If initially most had values very close to 0, their range now is slightly larger.\n\nBias values were also updated (they are all initialized with zeroes). After training, bias terms tend to be negative, and only a tiny number got positive values.","ed1206ee":"It is a little bit early but checking strings length should be done before further preprocessing. It will be used for determining the maximum length of all sentences. The Neural Network expects an n-dimentional matrix with rows and columns of one and the same length.\n\nThe code lines below finds the longest string (232 words) and the average number of words used in all observations (62 words).","b303e792":"The last (Dense) layer of a Classification model should have as many neurons as the number of classes. For this reason, this value is computed as the unique items ih a labels list and stored as a global variable.","1f9b0f7f":"The word \"i\" is used 25859 times; next is \"didnt\", which appears 272 times. People used \"go\" 393 times, and so forth. These words, however, are not ordered in descending order. The most commont words (in descending order) are displayed below.","9bbb673a":"It is interesting to see how sequences are decoded, i.e., if these numbers could be reversed to words again. The dictionary and the function below helps for this exercise.","6b0ee665":"### II.4. Visualize emotions per type","5253f591":"### III.3. Text pre-processing","c3932981":"### III.1. Separate features and labels","2d749400":"### II.3. Explore unique emotions, dtypes and missing values","1a4c0aa3":"### II.8 Check for missing values after removing stopwords","e59d1ce9":"Length extention is performed with `pad_sequences`, which needs to get the data, their expected length, padding and truncating type (the latter stored as global variables above).","f1c7503d":"Classification tasks require features and labels. An algorithm would not be able to find the relationships between them if both are stored together. For this reason, the function below helps for separating them. It places feature and labels in new columns and removes the existing one.","e5be5f15":"However, these 300 000 are not unique words. `Counter` function checks how many are the unique words in the list.","461583a2":"### II.1. Rename column","9daa4277":"Fortunately, there are not missing values in neither dataset.","b47fb2a5":"Above, stopwords were counted and removed from a list of all words. The function below will help for removimg them row by row.","124e07dc":"A brief check shows that the weights in the first convolutional layer were properly initialized (they have uniform distribution). ","dfce4ae6":"It is important to know how many classes (emotion types) each dataset has. The code lines below shows that here are 6 unique classes in each dataset.","f2f423c4":"The plots above show that the 6 classes are equally distributed in all three datasets. This is important for precondition for classification tasks since models trained on imbalanced datasets most likely will not perform well on unseen data.","16c2b734":"Twenty times less unique words present in the list. The latter means that people use similar and repeating words to describe their emotions.","c8b4d34f":"The code lines below display the confusion matrix. It shows the actual and predicted class of an instance. ","35d0d99c":"### II.7. Remove stopwords","635e52cd":"Counting how many times each used is used and displaying usage frequence requires some computations. Each pair of word and its frequency are stored in `words_items`. Thereafter, the first 50 words are stored in a list, which is displayed below.","7ac69f09":"\"i\" is the most common word followed by \"feel\" (no surprise). The next places are taken by words (prepositions, articles, etc.) like \"and\", \"to\", \"the\", \"a\", \"that\", \"to\", and so on. These are important language elements but do not hold meaningfull information out of the context. In Natural Language Processing are known as \"stopwords\". \n\nStopwords are [defined](https:\/\/medium.com\/@saitejaponugoti\/stop-words-in-nlp-5b248dadad47) as the \"words in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence.\" It is explained that if the task is text classification (as in this case) or sentiment analysis, the stopwords should be removed since they would not provide any information to the model. However, if the task is language translation, the stopwords should be kept since they would be useful and are being translated along with the other words.\n\nSo, the code line below loads the stopwords in the English language as they are stored in the NLTK corpus.","7cec57e2":"It might turn out that the function removed all words in a row if an emotion was described with words in the stopwords list. Neural Networks, however, cannot handle datasets with missing values. For this reason, it is important to check if there are empty cells in the datasets. The code line below confirms that this is not the case.","6788ca6e":"It could be seen that all words are lowercase. This suggests that the datasets might have undergone some preliminary preprocessing. Numbers and special signs are not visible either. However, it seems the texts still contain words that bear no meaningfull value. \n\nThe code line below takes all words in the training data and places them in a list.","187647d5":"The code line below confirms that separation was successful.","06462062":"The three `csv` datasets are loaded and stored in variables. It is explicitly indicated that the DataFrames do not have titles.","b44854a3":"The 42-nd row has 12 words (tokens). It's length should be extended to 80 to meed the soon-to-be-built model expectations.","d46cd751":"#### Imports","a7f7c3de":"Loading all three datasets show that the information is stored in only one column. Feelings description is separated from respective emotions by a semicolon.","01c30769":"# Classifying emotions with a Recurrent Neural Network","9730a1f5":"Then, the tokenizer gets the train features to assign a number (index) against each word. `word_index` variable stores the index for each word.","6b704bd5":"A brief check confirms that labels are stored as an array in separate variables.","ced487a2":"### II.5 Encode labels","c352e13b":"## I. Load datasets","65b83482":"## V. Model evaluation","f5681d07":"## II. Exploratory Data Analysis","837c10a2":"The plots above shows that the model overfits the training data. Loss reached negligible values and accuracy reached almost 100%. However, it doesn't perform well on the validation data. For 150 epochs, loss fell down to 0.8 units and accuracy barely reached 85%. One possible reason for overfitting could be the noise in the data (meaningless words).","55bea779":"Among the first 50 meaningful words are \"didnt\", \"im\", \"ive\", and \"wasnt\", which were not removed since all are spelled wrongly (perhaps after removing special characters). This might cause problems during modelling since these are not meaningful words but present in the text.","d70b26a9":"## IV. Building and training a Neural Network","591b375d":"This is a slightly complex model but simpler ones didn't return better results.","5301be78":"The outputs above shows that reverse conversiong was successful. The funtion got the right word and replaced padded 0-s with question mark (as told by decoding function).\n\nNow, both validation and testing sets undergo the same transformations.","48d9db0f":"The ouput above (first and the last five rows) shows that columns were renamed successfully.","05e5abfb":"### III.2 Define global variables","6ca42473":"This information is useful but does not tell how many rows are close to the maximum length and how many to the minimum or the mean. For this reason, it is better to show strings distribution on a boxplot. The code line below stores the length of all string in a variable, used for displaying their distribution.","140d5d54":"Data pre-processing means to prepare the datasets for modelling. This means separating labels from features, defining global variables and text tokenization.","1f2b9f6c":"This Notebook presents how the Emotions dataset for NLP (available [here](https:\/\/www.kaggle.com\/praveengovi\/emotions-dataset-for-nlp)) could be classified with a Recurrent Neural Network. The dataset is stored in three separate files with training, validation, and testing data. There are 20000 samples - 16000 for training, 2000 for validation, and another 2000 for testing. Texts describing each feeling are stored in DataFrames followed by the emotion associated with them.\n\nThe task is building a model which could recognise the emotion a person experience based on person's feelings. The work is organised in several Chapters."}}