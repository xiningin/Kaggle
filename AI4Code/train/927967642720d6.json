{"cell_type":{"d91ffcba":"code","3d35ab78":"code","c5f5bac0":"code","336f3796":"code","caa85d8d":"code","6757fee7":"code","2a50056a":"code","d556076e":"code","1a271652":"code","6a6ed810":"code","5515f12f":"code","2a362b02":"code","98025dcd":"code","71c1ff67":"code","6182b4eb":"code","40efc61b":"code","5e93a389":"code","9d085425":"code","720c9815":"code","7bc65cac":"code","e8b94d8d":"code","3a48a823":"code","073a67f0":"code","3f35fb4b":"code","2bee74ef":"code","af5f3f0f":"code","43674e94":"code","fac1a40b":"code","3a710402":"code","b4b53888":"code","3f0c91ff":"code","6eab0f9f":"code","eac4c8a0":"code","5d638e9b":"code","ed5e90a3":"code","e4803435":"code","9a53baae":"code","6a4ec294":"code","ed2d470a":"code","7ca0ff42":"code","144bbb35":"code","0c32212a":"code","b051f917":"code","3e715898":"code","aaf3a283":"code","bb865c3c":"code","56a93953":"code","a5b0373c":"code","649912fe":"code","3a415d31":"code","1ec9cc25":"code","5b95211e":"code","0721fab7":"markdown","8115eab9":"markdown","dbe590f0":"markdown","46cb24f7":"markdown","b0b8d9ab":"markdown","408a6e8f":"markdown","69f1916e":"markdown"},"source":{"d91ffcba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d35ab78":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom catboost import Pool\nimport xgboost as XGB\nimport lightgbm as lgbm\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold","c5f5bac0":"sample_submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nf = open('\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt', 'r')\ndata_description = f.read()\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","336f3796":"train.head(10)","caa85d8d":"test.head(10)","6757fee7":"train.YrSold.value_counts()","2a50056a":"test.YrSold.value_counts()","d556076e":"years_lst = list(train.YrSold.value_counts().index)\nmean_per_year = {}\nfor year in years_lst:\n    mean_per_year[year] = train.SalePrice[train.YrSold == year].mean()\n    print(f\"For year: {year} mean value: {train.SalePrice[train.YrSold == year].mean()}\\\n    max value: {train.SalePrice[train.YrSold == year].max()} min value {train.SalePrice[train.YrSold == year].min()}\")","1a271652":"test['target'] = 0\nfor year in years_lst:\n    test.loc[test.YrSold == year, 'target'] = mean_per_year[year]","6a6ed810":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': test['target']})\nmy_submission.to_csv('submission.csv', index=False)","5515f12f":"train.dtypes.unique()","2a362b02":"test.dtypes.unique()","98025dcd":"train_float = train.select_dtypes(include=['float64']).fillna(-999.0).astype(str)\ntrain_int = train.select_dtypes(include=['int64']).fillna(-999)\ntrain_obj = train.select_dtypes(include=['object']).fillna(\"NAN\").astype(str)\npreproccesed_train = pd.concat([train_float, train_int, train_obj], axis=1)","71c1ff67":"test_float = test.select_dtypes(include=['float64']).fillna(-999.0).astype(str)\ntest_int = test.select_dtypes(include=['int64']).fillna(-999)\ntest_obj = test.select_dtypes(include=['object']).fillna(\"NAN\").astype(str)\npreproccesed_test = pd.concat([test_float, test_int, test_obj], axis=1)","6182b4eb":"X_train, X_val, y_train, y_val = train_test_split(preproccesed_train.drop(['SalePrice'], axis=1), preproccesed_train['SalePrice'], test_size=0.2, random_state=42)","40efc61b":"train_data = Pool(X_train, y_train, cat_features=list(train.select_dtypes(include=['object']).columns))\nval_data = Pool(X_val, y_val, cat_features=list(train.select_dtypes(include=['object']).columns))\ntest_data = Pool(preproccesed_test, cat_features=list(test.select_dtypes(include=['object']).columns))","5e93a389":"model = CatBoostRegressor(\n    learning_rate = 0.1,\n    iterations = 1000,\n    random_seed=42,\n    verbose=50\n)","9d085425":"model.fit(\n    train_data,\n    eval_set=val_data,\n    logging_level='Verbose'\n)","720c9815":"test_target = model.predict(test_data)","7bc65cac":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': test_target})\nmy_submission.to_csv('submission.csv', index=False)","e8b94d8d":"importances = model.get_feature_importance(prettified=True)\nimportances","3a48a823":"sns.displot(train[\"SalePrice\"])\nsns.displot(np.log1p(train[\"SalePrice\"]))","073a67f0":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","3f35fb4b":"corr = train.corr()\nhighest_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5]\nf, ax = plt.subplots(figsize=(12, 9))\ng = sns.heatmap(train[highest_corr_features].corr(),annot=True,cmap=\"mako\", vmax=.8, square=True)","2bee74ef":"corr[\"SalePrice\"].sort_values(ascending=False)[:25]","af5f3f0f":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols])","43674e94":"corrmat_df = pd.get_dummies(pd.concat([train[train.dtypes[train.dtypes == 'object'].index], train.SalePrice], axis=1))\ncorrmat_categorical = corrmat_df.corr()\nhighest_corr_features_cat = corrmat_categorical.index[abs(corrmat_categorical[\"SalePrice\"])>0.5]\nf, ax = plt.subplots(figsize=(12, 9))\ng = sns.heatmap(corrmat_df[highest_corr_features_cat].corr(),annot=True,cmap=\"mako\", vmax=.8, square=True)","fac1a40b":"train['SalePrice'] = np.log1p(train[\"SalePrice\"])","3a710402":"target_train = train['SalePrice']\ntest_id = test['Id']\ndata = pd.concat([train, test], axis=0, sort=False)\ndata = data.drop(['Id', 'SalePrice'], axis=1)","b4b53888":"null_values = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum() \/ data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([null_values, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","3f0c91ff":"data.drop((missing_data[missing_data['Total'] > 5]).index, axis=1, inplace=True)","6eab0f9f":"nulls = data.isnull().sum().sort_values(ascending=False)\nnulls.head(20)","eac4c8a0":"data[list(nulls[nulls > 0].index)].dtypes","5d638e9b":"numerical_feat = ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFinSF1', 'GarageArea']\ncategorical_feat = ['MSZoning', 'Utilities', 'Functional', 'KitchenQual', 'Exterior2nd', 'Electrical', 'Exterior1st', 'SaleType']","ed5e90a3":"for cat_feat in categorical_feat:\n    data[cat_feat] = data[cat_feat].fillna(str(data[cat_feat][:len(train)].value_counts().index[0]))","e4803435":"for num_feet in numerical_feat:\n    data[num_feet] = data[num_feet].fillna(data[num_feet][:len(train)].mean())","9a53baae":"data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata['SumOverAll'] = data['OverallQual'] + data['OverallCond']","6a4ec294":"numeric_feats = data.dtypes[data.dtypes != 'object'].index\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\nhigh_skew","ed2d470a":"for feat in high_skew.index:\n    data[feat] = np.log1p(data[feat])","7ca0ff42":"def get_target_encoded_feat(data, feat_name, target_name):\n    kf = KFold(n_splits=5, shuffle=False)\n    for train_ind, val_ind in kf.split(data):\n        train_data, val_data =  data.iloc[train_ind], data.iloc[val_ind]\n        data.loc[data.index[val_ind], f\"{feat_name}_enc\"] = val_data[feat_name].map(train_data.groupby(feat_name)[target_name].mean())\n    return data","144bbb35":"def get_target_encoded_df(feat_list, data=data, target_name='SalePrice', len_target_train=len(target_train), target_train=target_train):\n    target_enc_df = pd.concat([data[:len_target_train], target_train], axis=1)\n    for feat in feat_list:\n        target_enc_df = get_target_encoded_feat(target_enc_df, feat, target_name)\n    feat_list_extended = feat_list.copy()\n    for el in feat_list:\n        feat_list_extended.append(f\"{el}_enc\")\n    target_enc_df = target_enc_df[feat_list_extended]\n    for el in feat_list:\n        data[f\"{el}_enc\"] = data[el].map(target_enc_df.groupby(el)[f\"{el}_enc\"].mean())\n    data.drop(feat_list, axis=1, inplace=True)\n    return data","0c32212a":"cat_most_corr_feat = list(data.dtypes[data.dtypes == 'object'].index)\ndata = get_target_encoded_df(cat_most_corr_feat)\ndata = data.fillna(\"None\") ","b051f917":"cat_feats = list(data.select_dtypes(include=['object']).columns)\nfor feat in cat_feats:\n    lbl = LabelEncoder() \n    lbl.fit(list(data[feat].values)) \n    data[feat] = lbl.transform(list(data[feat].values))\ntrain_prep = data[:len(train)]\ntest_prep = data[len(train):]","3e715898":"dummie_data = pd.get_dummies(data)\ntrain_prep_dummie = dummie_data[:len(train)]\ntest_prep_dummie = dummie_data[len(train):]","aaf3a283":"print(train_prep.isna().sum().sum())\nprint(test_prep.isna().sum().sum())","bb865c3c":"xgb_model = XGB.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=4000,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, random_state=42, nthread=-1)\nxgb_model.fit(train_prep, target_train, eval_set=[(train_prep, target_train)], verbose=300)","56a93953":"lgbm_model = lgbm.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=3000,\n                              max_bin=55, bagging_fraction=0.8,\n                              bagging_freq=5, feature_fraction=0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf=6, min_sum_hessian_in_leaf=11, \n                              n_jobs=-1)\nlgbm_model.fit(train_prep, target_train, eval_set=(train_prep, target_train), verbose=100)","a5b0373c":"xgb_pred_train = np.expm1(xgb_model.predict(train_prep))\nlgbm_pred_train = np.expm1(lgbm_model.predict(train_prep))","649912fe":"fig, axs = plt.subplots(ncols=3, figsize=(15,5))\nfig.tight_layout(pad=5.0)\nsns.scatterplot(x=xgb_pred_train, y=np.expm1(target_train), ax=axs[0])\nsns.scatterplot(x=lgbm_pred_train, y=np.expm1(target_train), ax=axs[1])\nsns.scatterplot(x=(xgb_pred_train + lgbm_pred_train) \/ 2, y=np.expm1(target_train), ax=axs[2])","3a415d31":"xgb_target = np.expm1(xgb_model.predict(test_prep))\nlgbm_target = np.expm1(lgbm_model.predict(test_prep))","1ec9cc25":"prediction = (xgb_target + lgbm_target) \/ 2","5b95211e":"my_submission = pd.DataFrame({'Id': test_id, 'SalePrice': prediction})\nmy_submission.to_csv('submission.csv', index=False)","0721fab7":"# 3. Constant prediction","8115eab9":"# 1. Import data","dbe590f0":"# 5.Simple prediction Catboost","46cb24f7":"# 4. Data preprocess for simple Catboost solution","b0b8d9ab":"# 6. EDA","408a6e8f":"# 2. Start analysis","69f1916e":"# 7. Feature preprocessing and generation"}}