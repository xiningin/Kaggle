{"cell_type":{"f1d7f044":"code","77a4f01c":"code","ea519891":"code","a6edb5b7":"code","6531a22a":"code","68835684":"markdown","47a60475":"markdown","989ac29a":"markdown","3520879c":"markdown","2737b676":"markdown","7f2f2216":"markdown","eb11dd4c":"markdown","aeac8bc0":"markdown","c171cee9":"markdown","0b5f6a50":"markdown","cbdd3bb6":"markdown","9d2ded9f":"markdown","eba3fc43":"markdown","1b770c84":"markdown","d6045153":"markdown","be01dd0f":"markdown","ac46d398":"markdown","0c5b6bed":"markdown","49d16229":"markdown","a2c27835":"markdown","a0621c2f":"markdown","26d85699":"markdown"},"source":{"f1d7f044":"!pip install sklearn_relief","77a4f01c":"# Base Libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n# Transformation\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import power_transform\nfrom sklearn.pipeline import Pipeline\n# Feature Selection\nimport sklearn_relief as sr\n# Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n","ea519891":"df_amp = pd.read_csv('..\/input\/wind-power-generation\/Amprion.csv')\ndates = df_amp['Date']\ndf_amp = df_amp.T\ndf_amp = df_amp[1:]\ndf_amp.columns = [dates]\n\ndf_amp.reset_index(drop=True, inplace=True)\n\nscaler = MinMaxScaler()\n\ndf_amp = power_transform(df_amp, method='yeo-johnson')\ndf_amp = scaler.fit_transform(df_amp)\n\nX = df_amp[:,0:396]\ny = df_amp[:,396]\n\n# (optional) plot train & test\nfig, ax=plt.subplots(1,2,figsize=(30, 6))\nsns.distplot(X, ax=ax[0])\nsns.distplot(y, ax=ax[1])\n\n","a6edb5b7":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\nr = sr.RReliefF(n_features = 20)\nprint(r.fit_transform(X_train,y_train))","6531a22a":"nof_list=np.arange(1,20)            \nhigh_score=0\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    fs = sr.RReliefF(n_features = nof_list[n])\n    relief = Pipeline([('fs', fs), ('m', RandomForestRegressor())])\n    relief.fit(X_train,y_train)\n    score = relief.score(X_test,y_test)\n    score_list.append(score)\n    print(f'NOF: {nof_list[n]}, Score: {score}')\n    if(score > high_score):\n        high_score = score\n        nof = nof_list[n]\n\nprint (print(f'High Score: NOF: {nof}, Score: {high_score}'))","68835684":"The main focus of this kernel is the RReliefF algorithm, but let's spend some time on the data preprocessing, to make our job easier.","47a60475":"## 4.1 Filter Method","989ac29a":"A large variety of feature selection methodologies have been proposed and research continues to support the claim that there is no universal \u201cbest\u201d method for all tasks. In this section, the RReliefF is described in Filter and Wrapped Methods","3520879c":"This method uses a \u2018proxy measure\u2019 calculated from the general characteristics of the training data to score features or feature subsets as a processing step prior to modeling. Filters are generally **much faster and function independently** of the induction algorithm, meaning that selected features can then be passed to any modeling algorithm. Filter methods can be roughly classified further by the filtering measures they employ, i.e. information, distance, dependence, consistency, similarity, and statistical measures.","2737b676":"# 2. Base Libraries","7f2f2216":"During my studies, I tried to find a good example (or notebook) about RReliefF (Relief for Regression). Unfortunately, I couldn't find anything that suited my needs, as there were many examples for Classification, and almost none for Regression. So, I decided to collaborate with the Kaggle community and create an example of how to use RReliefF. To achieve this goal, I used the sklearn_relief library (https:\/\/gitlab.com\/moongoal\/sklearn-relief)\n\nRelief is an algorithm developed by Kira and Rendell in 1992 that takes a filter-method approach to feature selection that is notably sensitive to feature interactions. It was originally designed for application to binary classification problems with discrete or numerical features. Relief calculates a feature score for each feature which can then be applied to rank and select top scoring features for feature selection. \n\nRobnik-\u0160ikonja and Kononenko propose further updates to ReliefF, making it appropriate for regression (RReliefF, the focus of this notebook)","eb11dd4c":"# 4. Feature Selection","aeac8bc0":"# 3. Data Preprocessing","c171cee9":"First, the basic libraries are imported: **pandas**, **matplotlib**, **seaborn**, **numpy** to Dataframes, Graphs and numeric operations; **MinMaxScaler** to normalize our data between 0 and 1, **train_test_split** to help split the dataset (usually 70% training\/ 30% testing). We'll use **power_transform** to make data more Gaussian-like, **RandomForestRegressor** to wrap the RReliefF algorithm on the second example, and **sklearn_relief** for obvious reasons","0b5f6a50":"The best definition to this method: It **employs any stand-alone modeling algorithm to train a predictive model using a candidate feature subset**. The testing performance on a hold-out set is typically used to score the feature set. Alternatively in a modeling algorithm like a random forest (that I used in this example) , estimated feature importance scores can be applied to select a feature subset. In any wrapper method, a new model must be trained to test any subsequent feature subset, therefore wrapper methods are typically iterative and computationally intensive, but **can identify the best performing features set for that specific modeling algorithm**. Each iteration of the wrapper, the feature subset is generated based on the selected search strategy, e.g. forward or backward selection or a heuristic feature subset selection.","cbdd3bb6":"### Table of Contents","9d2ded9f":"## 5. Conclusions","eba3fc43":"In this case, I wrapped the RReliefF algorithm in an Random Forest Regressor pipeline. The RF hyperparametrization wasn't done, but of course it can be used to improve even more the accuracy of this model. Here, the model is tested with n between 1 and 20 features, giving us the best values for this dataset.","1b770c84":"Here, the data will be imported, transposed so that we can use the days of the year as parameters, transformed using the **yeo-johnson** method and presented to see if they are resembling a Gaussian pattern.","d6045153":"Accordingly to the original implementation, a RReliefF used in filter method will rank all the features, based on importance. ","be01dd0f":"## 4.2 Wrapper Method","ac46d398":"![](https:\/\/www.iberdrola.com\/wcorp\/gc\/prod\/pt_BR\/comunicacion\/machine_learning_mult_1_res\/machine_learning_746x419.jpg)","0c5b6bed":"## 6. References","49d16229":"1. [Introduction](#1.-Introduction)\n\n2. [Base Libraries](#2.-Base-Libraries)\n\n3. [Data Preprocessing](#3.-Data-Preprocessing)\n\n4. [Feature Selection](#4.-Feature-Selection)\n\n5. [Conclusions](#5.-Conclusions)\n\n6. [References](#6.-References)","a2c27835":"Accordinly to the original paper, RReliefF can **discover strong dependencies between attributes**, while in domains without such dependencies it **performs the same as the mean squared error**. It is also robust and noise tolerant. Its intrinsic contextual nature allows it to recognize contextual attributes. From paper's original experimental results, it's concluded that learning regression trees with RReliefF is promising especially in combination with linear models in the leaves of the tree.","a0621c2f":"# 1. Introduction","26d85699":"[1] Bolon-Canedo, V., Sanchez-Marono, N., Alonso-Betanzos, A., 2013. A Review of Feature Selection\nMethods on Synthetic data. Knowledge and Information Systems 34 (3), 483\u2013519.\n\n[2] Dash, M., Liu, H., 1997. Feature Selection for Classification. Intelligent Data Analysis 1 (1-4), 131\u2013156.\n\n[3] Guyon, I., Elisseeff, A., 2003. An Introduction to Variable and Feature Selection. Journal of Machine\nLearning Research 3 (Mar), 1157\u20131182.\n\n[4] Holland, J. H., 1992. Adaptation in Natural and Artificial Systems. 1975. Ann Arbor, MI: University\nof Michigan Press.\n\n[5] Jovic, A., Brkic, K., Bogunovic, N., 2015. A Review of Feature Selection Methods with Applications. In: Information and Communication Technology, Electronics and Microelectronics (MIPRO), 2015 38th\nInternational Convention on. IEEE, pp. 1200\u20131205.\n\n[6] Kira, Kenji and Rendell, Larry (1992). The Feature Selection Problem: Traditional Methods and a New Algorithm. AAAI-92 Proceedings.\n\n[7] Kira, Kenji and Rendell, Larry (1992) A Practical Approach to Feature Selection, Proceedings of the Ninth International Workshop on Machine Learning, p249-256\n\n[8] Kittler, J., 1978. Feature set Search algorithms. Pattern Recognition and Signal Processing.\n\n[9] Langley, P., 1994. Selection of Relevant Features in Machine Learning. In: Proceedings of the AAAI Fall Symposium on Relevance. Vol. 184. pp. 245\u2013271\n\n[10] Menze, B. H., Kelm, B. M., Masuch, R., Himmelreich, U., Bachert, P., Petrich, W., Hamprecht, F. A., 2009. A Comparison of Random Forest and its Gini Importance with Standard Chemometric Methods for the Feature Selection and Classification of Spectral Data. BMC Bioinformatics 10 (1), 213\n\n[11] Ni, W., 2012. A Review and Comparative Study on Univariate Feature Selection Techniques. Ph.D. thesis, University of Cincinnati.\n\n[12] Robnik-\u0160ikonja, Marko, and Kononenko, Igor (1997). An Adaptation of Relief for Attribute Estimation in Regression. Machine Learning: Proceedings of the Fourteenth International Conference (ICML\u201997) (p296-304)\n\n[13] Van Laarhoven, P. J., Aarts, E. H., 1987. Simulated Annealing. In: Simulated Annealing: Theory and\nApplications. Springer, pp. 7\u201315."}}