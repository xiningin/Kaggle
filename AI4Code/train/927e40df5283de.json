{"cell_type":{"c19321f8":"code","f483de68":"code","370c1fe3":"code","b6c35beb":"code","d69afc11":"code","7a083305":"code","9ab54434":"code","848added":"code","9c70bd28":"code","f813c331":"code","29ccfa75":"code","791a2dac":"code","1265bcfa":"code","23a515ba":"code","10b85570":"code","5935434b":"code","4f564cde":"code","ea6b5639":"code","26589868":"code","0e965765":"code","8e91b93e":"code","570d7f97":"code","8181884f":"code","0292191f":"code","e35be439":"code","51c7e664":"code","dbaa6eaf":"code","c0ac7e37":"code","38a096b6":"code","fc5e9ced":"code","107b8c81":"code","516c58f7":"code","77208951":"code","bde1faf0":"code","280da3ce":"code","277a6016":"code","5b32b622":"code","dfc52b2e":"code","b3ae5375":"code","59e0991b":"code","5d02bc1e":"code","27ee09fb":"code","569dec3d":"code","710adafb":"code","4abd9cbd":"markdown","16c0a000":"markdown","e371bb55":"markdown","88e2ec75":"markdown","b13160b2":"markdown","a60a5ac3":"markdown","e2b2dea7":"markdown"},"source":{"c19321f8":"#Action\n\n#Import Libraries\n\nimport datetime\nimport math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport json # to convert json in df\nfrom pandas.io.json import json_normalize # to normalize the json file","f483de68":"# peak at the dataset\ntrain_head = pd.read_csv(\"..\/input\/train.csv\",nrows=5)\n#show train data \ntrain_head","370c1fe3":"#peak at the dataset\ntest_head = pd.read_csv(\"..\/input\/test.csv\",nrows=5)\n#show train data \ntest_head\n","b6c35beb":"#load train dataset\ntrain = pd.read_csv(\"..\/input\/train.csv\", low_memory=False)","d69afc11":"#shape and column names train\nprint (train.shape)\nprint (train.columns)","7a083305":"# load test dataset\ntest = pd.read_csv(\"..\/input\/test.csv\", low_memory=False)","9ab54434":"#shape and column names of test\nprint (test.shape)\nprint (test.columns)","848added":"sampleSubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\n#shape and column names of submission file\nprint (sampleSubmission.shape)\nprint (sampleSubmission.columns)\n","9c70bd28":"#Train:\n# sessionId = fullVisitorId + visitId\nprint(len(train))\nprint(train.sessionId.nunique())\nprint(train.fullVisitorId.nunique())\nprint(train.visitId.nunique())\n\n# sessionid is not unique for somereason, duplicates do exist.\n","f813c331":"#test:\nprint(len(test))\nprint(test.sessionId.nunique())\nprint(test.fullVisitorId.nunique())\nprint(test.visitId.nunique())","29ccfa75":"#Understand the data types \ntrain.dtypes","791a2dac":"#Action\n#borrowed code to parse JSON objects, big query another alternative, need to review this code\n# removed sampling and added check, takes time to run\n\ncolumns = ['device', 'geoNetwork', 'totals', 'trafficSource'] # Columns that have json format\n\ndir_path = \"..\/input\/\" # you can change to your local \n\n# p is a fractional number to skiprows and read just a random sample of the our dataset. \n#p = 0.07 # *** In this case we will use 50% of data set *** #\n\n#Code to transform the json format columns in table\ndef json_read(df):\n    #joining the [ path + df received]\n    data_frame = dir_path + df\n    \n    #Importing the dataset\n    df = pd.read_csv(data_frame, \n                     converters={column: json.loads for column in columns}, # loading the json columns properly\n                     dtype={'fullVisitorId': 'str'}) # transforming this column to string\n        \n    for column in columns: #loop to finally transform the columns in data frame\n        #It will normalize and set the json to a table\n        column_as_df = json_normalize(df[column]) \n        # here will be set the name using the category and subcategory of json columns\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns] \n        # after extracting the values, let drop the original columns\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n        print(\"check\")\n \n    # Printing the shape of dataframes that was imported     \n    print(f\"Loaded {os.path.basename(data_frame)}. Shape: {df.shape}\")\n    return df # returning the df after importing and transforming\n\n","1265bcfa":"%%time\n#Action\n\n#Loading the train again with new function\ndf_train = json_read(\"train.csv\")\nprint (\"executed time:\",datetime.datetime.now())","23a515ba":"#peak the dataset\ndf_train.head()","10b85570":"#Garbage collection, will error if no train and test\nimport gc\ndel [[train,test]]\ngc.collect()","5935434b":" %who\n    # what variables exist in the program, to ensure it is deleted","4f564cde":"#what datatypes ?\ndf_train.dtypes\n","ea6b5639":"# find the null values \ntotal=df_train.isnull().sum()\ntotal \n#df_train[\"channelGrouping\"].value_counts()\n#Describe the Data\n#df_train.describe()","26589868":"# what numeric variables \nnumeric_features = df_train.select_dtypes(include=[np.number])\nnumeric_features.columns\n\n","0e965765":"# what non numeric variables \nnumeric_features = df_train.select_dtypes(include=[np.object])\nnumeric_features.columns","8e91b93e":"#Action\n#let fill the missing values\n# my priority is address few fields i like, not to replace all the missing fields\n\n\ndef FillingNaValues(df):    # fillna numeric feature\n    df['totals.pageviews'].fillna(1, inplace=True) #filling NA's with 1\n    df['totals.newVisits'].fillna(0, inplace=True) #filling NA's with 0\n    df['totals.bounces'].fillna(0, inplace=True)   #filling NA's with 0\n    df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].fillna(0.0) #filling NA with zero\n    \n    return df #return the transformed dataframe","570d7f97":"#Action\n# replace missing values using the above fucntion\ndf_train = FillingNaValues(df_train)\nprint (\"executed time:\",datetime.datetime.now())","8181884f":"#Action\n#change date formating using function (borrowed,quote source)\n\nfrom datetime import datetime\ndef date_process(df):\n    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\") # seting the column as pandas datetime\n    df[\"_weekday\"] = df['date'].dt.weekday #extracting week day\n    df[\"_day\"] = df['date'].dt.day # extracting day\n    df[\"_month\"] = df['date'].dt.month # extracting day\n    df[\"_year\"] = df['date'].dt.year # extracting day\n    df['_visitHour'] = (df['visitStartTime'].apply(lambda x: str(datetime.fromtimestamp(x).hour))).astype(int)\n    \n    return df #returning the df after the transformations","0292191f":"#Action\n#Call the function for date formating\ndf_train = date_process(df_train) #calling the function that we created above\n\ndf_train.head(2) #printing the first 2 rows of our dataset","e35be439":"#Action\n# find the constant columns and remove them \nconstant_columns = []\nfor col in df_train.columns:\n    if len(df_train[col].value_counts()) == 1:\n        constant_columns.append(col)\n\n#print column names \nconstant_columns        ","51c7e664":"#Action\n#delete the columns which has empty values \nfor x in constant_columns:\n    df_train.drop(x,axis=1, inplace=True)\n","dbaa6eaf":"df_train.shape\ndf_train.columns","c0ac7e37":"#Action\n# Data type conversion object to Int\ndf_train['totals.bounces'] = df_train['totals.bounces'].astype(str).astype(int)\ndf_train['totals.hits'] = df_train['totals.hits'].astype(str).astype(int)\ndf_train['totals.newVisits'] = df_train['totals.newVisits'].astype(str).astype(int)\ndf_train['totals.pageviews'] = df_train['totals.pageviews'].astype(str).astype(float)\ndf_train['totals.transactionRevenue'] = df_train['totals.transactionRevenue'].astype(str).astype(float)","38a096b6":"df_train.groupby('channelGrouping')['totals.transactionRevenue'].agg('sum')","fc5e9ced":"# Natural log issue pad by 1 \n%matplotlib inline\nplt.subplot(211)\ndf_train[\"totals.transactionRevenue\"].hist(bins =5)\nplt.ylabel('transactionRevenu')\nplt.title('transactionRevenu histogram')\n\nplt.subplot(212)\nLogRevenue = np.log(df_train[\"totals.transactionRevenue\"]+1)\nLogRevenue.hist(bins =5)\nplt.ylabel('log(transactionRevenu)')\nplt.show()","107b8c81":"# distribution of numercial values \n%matplotlib inline\nimport matplotlib.pyplot as plt\nattributes = [\"totals.bounces\", \"totals.hits\",\"totals.newVisits\",\n              \"totals.pageviews\",\"visitNumber\",\"_visitHour\"]\n\nloc[:,attributes].hist(bins =20,figsize =(20,15))\nplt.show()","516c58f7":"%matplotlib inline\nchannel_df = df_train['channelGrouping'].value_counts()\nchannel_df.index.name = 'channelGrouping'\nchannel_df.sort_index(inplace=True)\nchannel_df.plot(kind='bar',rot=20, title= 'Channel Distribution -count not revenue',figsize=(14,5))\nplt.show()\n","77208951":"# sample visitor\nsample_visitor = df_train[df_train['fullVisitorId'] == '7813149961404844386'].sort_values(by='visitNumber')\n\n#sample_visitor[['channelGrouping','date','visitId','visitNumber','totals.hits','totals.pageviews','totals.transactionRevenue']].head(30)\nsample_visitor","bde1faf0":"#Action\ndf_train['target'] = np.log(df_train[\"totals.transactionRevenue\"]+1)","280da3ce":"#verify if the transformation happended\ndf_train[df_train['fullVisitorId'] == '7813149961404844386'].sort_values(by='visitNumber')['target'].head()","277a6016":"#cross tab on device\npd.crosstab(df_train['device.deviceCategory'], df_train['device.isMobile'], margins=False)","5b32b622":"#revenue by mobile device \ng1 = df_train.groupby('device.isMobile')['target'].sum()\ng1.plot.bar()\nplt.show()\n\n# seems like non mobile device generate more revenue","dfc52b2e":"#revenue by mobile device \ng1 = df_train.groupby('device.deviceCategory')['target'].sum()\ng1.plot.bar()\nplt.show()\n\n# Again Desktop makes more revenue","b3ae5375":"#revenue by browser \ng1 = df_train.groupby('device.browser')['target'].sum().sort_values()\ndf =pd.DataFrame(g1)\ndf =  df[df['target']>0]\ndf.plot.barh()\nplt.show()\n\n# too many browsers, \n# chrome leads the way","59e0991b":"#revenue by Operating System \ng1 = df_train.groupby('device.operatingSystem')['target'].sum().sort_values()\ndf =pd.DataFrame(g1)\ndf =  df[df['target']>0]\ndf.plot.barh()\nplt.show()\ndf.plot(kind='bar', stacked=True)\nchannel_df.plot(kind='bar',rot=20, title= 'Channel Distribution -count not revenue',figsize=(14,5))\n\n\n# too many browsers, \n# chrome leads the way","5d02bc1e":"df.plot(kind='bar',rot=20, title= 'revenue by OS',figsize=(14,5))\nplt.show()","27ee09fb":"#revenue by date \ng1 = df_train.groupby('date')['target'].sum().sort_values()\ndf =pd.DataFrame(g1)\ndf.plot(figsize=(40,5))\nplt.show()\n# definitely some seaonality going on","569dec3d":"#revenue by year \ng1 = df_train.groupby('_year')['target'].sum().sort_values()\ndf =pd.DataFrame(g1)\ndf.plot(kind='bar',rot=20, title= 'revenue by year')\nplt.show()\n# renevue increasing each year","710adafb":"#revenue by year \ng1 = df_train.groupby(['_month'])['target'].sum()\ndf =pd.DataFrame(g1)\ndf.plot()\nplt.show()\n# Dec month has higher revenue","4abd9cbd":"**Missing value analysis and any field transformation**\n1. find null values, analyze them\n2. way to replace missing values\n3. Date Transformation\n4. Many values are not numberical should be converted ?\n5. Not available in demo dataset  - is not NAN or NULL counted, only NAN** - can be removed","16c0a000":"**fullVisitorId 617242 matches between test and submission.**","e371bb55":"**Action: unwrap the test data once the analysis is over in the train**","88e2ec75":"Convert these Object datatype to Numeric to perform aggregation\n* totals.bounces \n* totals.hits\n* totals.newVisits\n* totals.pageviews\n* totals.transactionRevenue\n","b13160b2":"**EDA and Plots**\n1. graphs, histograms, tables","a60a5ac3":"**4 fields in nest Jason format**\n*  device\n* geoNetwork\n* totals\n* trafficSource\n","e2b2dea7":"totals.transactionRevenue\tfield has the revenue for each session."}}