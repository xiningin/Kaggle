{"cell_type":{"82b5f605":"code","c9a2b22c":"code","361f14d5":"code","1e151742":"code","8aa9b5d3":"code","69558f8f":"code","1450108a":"code","00a27c98":"code","e6b0eea2":"code","a99a6c06":"code","e9ec0e5f":"code","13586459":"code","062f2c0d":"code","182a4880":"code","5a636861":"code","dcf01f26":"code","afbb6773":"code","4a450cf1":"code","e206e0b4":"code","420848e6":"code","522f6063":"code","52f2486b":"code","e77c49be":"code","f649a323":"code","c180dab5":"code","0ee0e0b2":"code","70e2e0cf":"code","372ad05c":"code","387371d9":"code","af8395af":"code","9494ff35":"code","e2182bc9":"code","73884255":"code","e5ad8bee":"code","16f5c119":"code","0e721e20":"code","f9860b3f":"code","95c83f3d":"code","75e0255a":"code","39685bf4":"code","fbe64bb4":"code","1a334e35":"code","9c4b3ebc":"code","9a9657d4":"code","7e2660c0":"code","ba7a2a3f":"code","b126d140":"code","e47edb36":"code","1daaeb1f":"code","611690a6":"markdown","101c525e":"markdown","6c5c7c2d":"markdown","096ba551":"markdown","39f953fd":"markdown","86de3543":"markdown","44541b13":"markdown","c5775f53":"markdown","205e61c7":"markdown","73fe635a":"markdown"},"source":{"82b5f605":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as pt\nimport seaborn as sns\nsns.set()\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c9a2b22c":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","361f14d5":"train_data.head()","1e151742":"pd.isnull(train_data).any()","8aa9b5d3":"#we can see that the Age and Cabin and Embarked are contaning NULL values so now lets handle them\n#First by plotting them and then imputing them\n\nsns.heatmap(train_data.isnull(), yticklabels = False, color='darkred')","69558f8f":"#Lets handle the age and visualize with the relation with pclass\npt.figure(figsize = (10,5))\nsns.boxplot(x = 'Pclass', y = 'Age', data=train_data)","1450108a":"def in_age(cols):\n    age = cols[0]\n    pclass = cols[1]\n    if pd.isnull(age):\n        if pclass == 1:\n            return 38\n        elif pclass ==2:\n            return 29\n        else:\n            return 24\n    else:\n        return age","00a27c98":"#Applying the above function\ntrain_data['Age'] = train_data[['Age', 'Pclass']].apply(in_age, axis=1)","e6b0eea2":"#checking heatmap of nullity\nsns.heatmap(train_data.isnull(),  yticklabels = False, cbar=False)\n\n#So below we can see that er have removed the nullity from age column","a99a6c06":"#Lets check survival based on Age\nsns.distplot(train_data['Age'])","e9ec0e5f":"#Lets check the class wise survival of passenger\nsns.countplot(train_data['Survived'],hue=train_data['Pclass'])","13586459":"women = train_data.loc[train_data['Sex']=='female']['Survived']\nwom_sur = sum(women)\/len(women)*100\n\nprint(\"% of women survived : \", wom_sur)","062f2c0d":"men = train_data.loc[train_data['Sex']=='male']['Survived']\nmen_sur = sum(men)\/len(men)*100\n\nprint(\"% of women survived : \", men_sur)","182a4880":"#Lets visualize the data\nsns.countplot(train_data['Survived'], hue=train_data['Sex'])","5a636861":"train_data.describe()","dcf01f26":"#Now we will drop PassengerId, Name, Ticket, Fare, Cabin\ntrain_data.drop(['PassengerId', 'Name', 'Ticket','Fare', 'Cabin'], inplace=True, axis=1)","afbb6773":"train_data.head()","4a450cf1":"em = pd.get_dummies(train_data['Embarked'], drop_first = True)\ntrain_data.drop(\"Embarked\", axis=1, inplace=True)","e206e0b4":"train_data = train_data.join(em)\ntrain_data.head()","420848e6":"#Separating dependent and independent variables\nx = train_data.iloc[:,1:8].values\ny = train_data.iloc[:,0].values","522f6063":"#Handling the categorical features\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabel = LabelEncoder()\nx[:,1] = label.fit_transform(x[:,1])\nx[:,0] = label.fit_transform(x[:,0])","52f2486b":"onehot = OneHotEncoder(categorical_features = [0])\nx = onehot.fit_transform(x).toarray()\nx","e77c49be":"#Avoiding dummy variable trap\nx =x[:,1:]","f649a323":"#Splitting the training set and test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .33, random_state = 0)","c180dab5":"def conf_mat(classifier, y_real, y_pred):\n    from sklearn.metrics import confusion_matrix, accuracy_score\n    cm = confusion_matrix(y_real, y_pred)\n    acc = accuracy_score(y_real, y_pred)*100\n    return cm, acc","0ee0e0b2":"#Building our models\n\n# ---- LOGISTIC REGRESSION ----\n\nfrom sklearn.linear_model import LogisticRegression\nlog_classifier = LogisticRegression(random_state = 0)\nlog_classifier.fit(x_train, y_train)\n\ny_pred = log_classifier.predict(x_test)\n\nlog_cm, log_acc = conf_mat(log_classifier, y_test, y_pred)\n\nprint(\"Confusion Matrix is  : \",log_cm)\nprint(\"Accuracy Score is : \", log_acc)","70e2e0cf":"# ---- K NEAREST NEIGHBORS ----\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 7, metric = 'minkowski', p=2)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\nknn_cm, accuracy_knn = conf_mat(classifier, y_test, y_pred)\n\nprint(\"Confusion Matrix is  : \",knn_cm)\nprint(\"Accuracy Score is : \", accuracy_knn)","372ad05c":"# ---- SUPPORT VECTOR MACHINE ---\n\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\n\nlin_svm_cm, accuracy_svm = conf_mat(classifier, y_test, y_pred)\n\nprint(\"Confusion Matrix is  : \",lin_svm_cm)\nprint(\"Accuracy Score is : \", accuracy_svm)","387371d9":"# ---- GAUSSIAN SVM ----\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0, gamma='auto')\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\n\nsvm_cm, accuracy_gsvm = conf_mat(classifier, y_test, y_pred)\n\nprint(\"Confusion Matrix is  : \",svm_cm)\nprint(\"Accuracy Score is : \", accuracy_gsvm)","af8395af":"# ---- NAIVE BAYES ALGORITHM ----\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\n\nbayes_cm, accuracy_bayes = conf_mat(classifier, y_test,  y_pred)\n\nprint(\"Confusion Matrix is  : \",bayes_cm)\nprint(\"Accuracy Score is : \", accuracy_bayes)","9494ff35":"# ---- DECISION TREE ----\n\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\n\ndecision_cm, accuracy_decision = conf_mat(classifier, y_test, y_pred)\n\nprint(\"Confusion Matrix is  : \" ,decision_cm)\nprint(\"Accuracy Score is : \", accuracy_decision)","e2182bc9":"# ---- RANDOM FOREST ----\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 30, criterion = 'entropy', random_state=0)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\n\nrandom_cm, accuracy_random = conf_mat(classifier, y_test, y_pred)\n\nprint(\"Confusion Matrix is  : \" ,random_cm)\nprint(\"Accuracy Score is : \", accuracy_random)\n","73884255":"Score_Sheet = pd.DataFrame(\n    {\"Accuracy Score\":[log_acc,accuracy_knn,accuracy_svm,accuracy_gsvm,accuracy_bayes\n                       ,accuracy_decision, accuracy_random]},\n    index = [\"Logisic_Regression\",\"K_Nearest\",\"Support_Vector\",\"Gaussian_SVM\",\"Naive_Bayes\",\"Decision_Tree\", \n                           \"Random_Forest\"]\n        )","e5ad8bee":"Score_Sheet","16f5c119":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","0e721e20":"test_data.head()","f9860b3f":"temp = test_data.copy()\ntest_data.drop([\"PassengerId\", 'Name', 'Ticket', 'Fare', 'Cabin'],axis = 1, inplace=True)","95c83f3d":"pd.isnull(test_data).any()","75e0255a":"sns.heatmap(test_data.isnull(), yticklabels = False, cbar = False)","39685bf4":"test_data['Age'] = test_data[['Age', 'Pclass']].apply(in_age, axis = 1)\n","fbe64bb4":"sns.heatmap(test_data.isnull(), yticklabels = False, cbar = False)","1a334e35":"pclass = pd.get_dummies(test_data['Pclass'], drop_first = True)\nsex = pd.get_dummies(test_data['Sex'], drop_first = True)\nemb = pd.get_dummies(test_data['Embarked'], drop_first = True)\ntest_data.drop(['Pclass', 'Sex', 'Embarked'], axis = 1, inplace=True)","9c4b3ebc":"test_data = test_data.join([pclass, sex, emb])","9a9657d4":"test_data = test_data[[2,3,'male','Age', 'SibSp', 'Parch', 'Q', 'S']]\ntest_data.head()","7e2660c0":"x = test_data.iloc[:,:].values\nx","ba7a2a3f":"y_pred = log_classifier.predict(x)","b126d140":"x1 = pd.Series(temp['PassengerId'])\nx2 = pd.Series(y_pred, name='Survived')\nsubmission = pd.DataFrame(x1)\nsubmission = submission.join(x2)","e47edb36":"submission.to_csv(\"Result.csv\")","1daaeb1f":"submission","611690a6":"Making Submission Data Frame","101c525e":"Lets Check If any column has NULL data or Not","6c5c7c2d":"Using LabelEncoder and OneHotEncoding technique\n","096ba551":"Handling Categorical Variables","39f953fd":"We can clearly see that the Logistic Regression is outperforming all the classifications based on\n\nsklearn.metrics['accuracy score']. So we will use this model to predict our data.","86de3543":"Importing Test Data for our predicitions","44541b13":"Building our all Classifications models","c5775f53":"Lets Check the Impact of Gender on Survival","205e61c7":"Final Prediciton","73fe635a":"So Here Age and Cabin Columns are having NaN values, as we can say intutively that Age can impact survival\nbut dont think the same for Cabin. So we will handle age but leave the Cabin column"}}