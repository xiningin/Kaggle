{"cell_type":{"5446a535":"code","79dd80f3":"code","9713f266":"code","53e9d116":"code","b19b4e96":"code","4c81fec8":"code","1da60193":"code","eecc86f9":"code","0c62b49c":"code","1013e515":"code","fcad91a2":"code","aa1a320e":"code","09efd6dc":"code","61c4e39f":"code","3b818551":"code","53aea660":"code","33a59df6":"code","cbe82d98":"code","61319d58":"code","816bc80f":"markdown","9a482eb3":"markdown","cce3cc88":"markdown","7f24dd07":"markdown","e89765a8":"markdown","973f35d8":"markdown","c8c4101f":"markdown","6f8a2f00":"markdown","a3d6073b":"markdown","79b0f18c":"markdown","fe9ca660":"markdown"},"source":{"5446a535":"import pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')","79dd80f3":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\n\ncolumn_names = [ 'age', 'workclass', 'fnlwgt', 'education', 'educational-num', \n                'marital-status', 'occupation', 'relationship', 'race', \n                'gender', 'capital-gain', 'capital-loss', 'hours-per-week', \n                'native-country', 'income' ]\n\ncolumns_to_encoding = [ 'workclass', 'marital-status', 'occupation',\n                        'relationship', 'race', 'gender' ]\n\ncolumns_to_normalize = [ 'age', 'educational-num', 'hours-per-week', \n                         'capital-gain', 'capital-loss' ]\n\nle = LabelEncoder()\nscaler = StandardScaler()\npl = PolynomialFeatures(2, include_bias=False)\n\ndef feature_engineering(filename, train=True):\n    df = pd.read_csv(filename, index_col=False)        \n    df.drop(['fnlwgt', 'education', 'native-country'], axis=1, inplace=True)\n    df = pd.get_dummies(df, columns=columns_to_encoding)\n    df[\"income\"] = le.fit_transform(df['income'])\n    if train:\n        X_temp = pl.fit_transform(df[columns_to_normalize])\n        X_temp = scaler.fit_transform(X_temp)\n        df.drop(columns_to_normalize, axis=1, inplace=True)\n        X_train = np.hstack((df.values, X_temp))\n        y_train = df['income']\n        columns_names = pl.get_feature_names(df.columns)\n        return np.hstack((df.columns.values, columns_names)), X_train, y_train\n    else:\n        X_temp = pl.transform(df[columns_to_normalize])\n        X_temp = scaler.transform(X_temp)\n        df.drop(columns_to_normalize, axis=1, inplace=True)\n        X_test = np.hstack((df.values, X_temp))\n        y_test = df['income']\n        columns_names = pl.get_feature_names(df.columns)\n        return np.hstack((df.columns.values, columns_names)), X_test, y_test","9713f266":"columns_names, X, y = feature_engineering('..\/input\/adult.csv', train=True)","53e9d116":"from sklearn.model_selection import train_test_split\ndef rmnan(X, y):\n    X_, y_ = [], []    \n    for x, yt in zip(X, y):                \n        if np.isnan(x).any() or np.isnan(yt).any():\n            continue\n        X_.append(x)\n        y_.append(yt)        \n    return np.array(X_), np.array(y_)\n\nX, y = rmnan(X, y)","b19b4e96":"X, X_test, y, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\ny.shape, y_test.shape","4c81fec8":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nparam_distribution = {\n    'max_depth': np.arange(1, 15),\n}\n\nscoring = {    \n    'Accuracy': make_scorer(accuracy_score),\n    'F1_Score': make_scorer(fbeta_score, beta=1, average='micro'),    \n}","1da60193":"result = []\nkf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2)\nfor fold, (train_index, test_index) in enumerate(kf.split(X, y)):\n    X_tr, X_tst = X[train_index], X[test_index]\n    y_tr, y_tst = y[train_index], y[test_index]\n\n    for i in range(1, 20):\n        # train\n        pca = PCA(i)\n        X_t = pca.fit_transform(X_tr)\n        search_cv = RandomizedSearchCV(DecisionTreeClassifier(), param_distribution,\n                                       scoring=scoring, n_jobs=-1, \n                                       cv=RepeatedStratifiedKFold(n_splits=2, n_repeats=2), \n                                       refit='F1_Score') \n        search_cv.fit(X_t, y_tr)\n        model = search_cv.best_estimator_        \n\n        # test\n        X_t = pca.transform(X_tst)\n        y_pred = model.predict(X_t)\n\n        # model evaluation\n        f1 = fbeta_score(y_tst, y_pred, beta=1)\n        acc = accuracy_score(y_tst, y_pred)\n        \n        print(f\"fold: {fold} - cp:{i} train: {search_cv.best_score_} test: f1={f1}, acc={acc}\")\n\n        result.append((fold, i, acc, f1, pca, model))","eecc86f9":"best_f1 = 0\nbest_model = None\nfor fold, n, acc, f1, pca, model in result:\n    if best_f1 < f1:\n        best_f1 = f1\n        best_model=(fold, n, acc, f1, pca, model)\npca_components = best_model[1]\n\npca_components","0c62b49c":"result, metrics_ = [], []\nkf = RepeatedStratifiedKFold(n_splits=10, n_repeats=1)\nfor fold, (train_index, test_index) in enumerate(kf.split(X, y)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # train\n    pca = PCA(pca_components)\n    X_t = pca.fit_transform(X_train)\n    search_cv = RandomizedSearchCV(DecisionTreeClassifier(), param_distribution,\n                                   scoring=scoring, n_jobs=-1, \n                                   cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=1), \n                                   refit='F1_Score') \n    search_cv.fit(X_t, y_train)\n    model = search_cv.best_estimator_        \n\n    # test\n    X_t = pca.transform(X_test)\n    y_pred = model.predict(X_t)\n\n    # model evaluation\n    f1 = fbeta_score(y_test, y_pred, beta=1)\n    acc = accuracy_score(y_test, y_pred)\n\n    print(f\"fold: {fold} - cp:{pca_components} train: {search_cv.best_score_} test: f1={f1}, acc={acc}\")\n\n    result.append((X_train, y_train, X_test, y_test, fold, i, acc, f1, pca, model))\n    metrics_.append((f1, acc))","1013e515":"best_f1 = 0\nbest_model = None\nfor X_train, y_train, X_test, y_test, fold, n, acc, f1, pca, model in result:\n    if best_f1 < f1:\n        best_f1 = f1\n        best_model=(X_train, y_train, X_test, y_test, fold, n, acc, f1, pca, model)\n\nX_train, y_train, X_test, y_test = X, y, X_test, y_test #best_model[:4]","fcad91a2":"from sklearn import metrics\n\npca, model = best_model[-2], best_model[-1]\nprobs = model.predict_proba(pca.transform(X_test))\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","aa1a320e":"f1_r, acc_r = [], []\nfor f1, acc in metrics_:\n    f1_r.append(f1)\n    acc_r.append(acc)\n\nf1_r, acc_r = np.array(f1_r), np.array(acc_r)\nl = f1_r.shape[0]\nplt.title(f'F1 Score in Folds(PCA components = {pca_components})')\nplt.plot(range(l), f1_r, 'r', label = 'F1 Score')\nplt.plot(range(l), acc_r, 'b', label = 'Accuracy')\nplt.legend(loc = 'lower right')\nplt.xticks(range(l))\nplt.xlim([0, l - 1])\nplt.ylim([0.95, 1])\nplt.ylabel('F1 Score')\nplt.xlabel('Fold')\nplt.grid()\nplt.show()","09efd6dc":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n#     https:\/\/www.kaggle.com\/grfiv4\/plotting-feature-importances\n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    X_train = pd.DataFrame(data=X_train, columns=[f\"PC{i}\" for i in range(1, X_train.shape[1] + 1)])\n    \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp\n\npca, clf = best_model[-2], best_model[-1]\nfeature_importance = plot_feature_importances(clf, pca.transform(X_train), top_n=X_train.shape[1], title=clf.__class__.__name__)","61c4e39f":"# https:\/\/stackoverflow.com\/questions\/22348668\/pca-decomposition-with-python-features-relevances\npca, clf = best_model[-2], best_model[-1]\nindex_components = [int(x[2:]) for x in feature_importance.index.values]\ndef features_used_to_generate_pca_components(index_components, pca, clf, columns_names):    \n    for i in index_components:\n        index_features = np.abs(pca.components_[i - 1]).argsort()[:4]\n        features = columns_names[index_features]\n        print(f'PC{i}')\n        print(f'Features:')\n        for f in features:\n            print(\"\\t\" + f)\n        print()\n        \nfeatures_used_to_generate_pca_components(index_components, pca, clf, columns_names)","3b818551":"from sklearn.metrics import confusion_matrix\n\npca, clf = best_model[-2], best_model[-1]\n\ny_pred = clf.predict(pca.transform(X_test))\n\ncm = confusion_matrix(y_test, y_pred)\ncm","53aea660":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \nplot_confusion_matrix(cm, [0, 1], True)","33a59df6":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","cbe82d98":"from sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(best_model[-1], out_file='tree.dot', \n                #feature_names = iris.feature_names,\n                class_names = [\">= 50K\", \"< 50K\"],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","61319d58":"from sklearn.externals import joblib\n\njoblib.dump(best_model, 'lgr.joblib')","816bc80f":"# Find Best number of components to PCA","9a482eb3":"## Confusion Matrix","cce3cc88":"## Plot feature importances","7f24dd07":"# Import Base Packages","e89765a8":"## Classification Report","973f35d8":"# Analyse Model Result","c8c4101f":"# Load Data","6f8a2f00":"## Get Features Used to Generate PCA Components","a3d6073b":"# Interface function to feature engineering data","79b0f18c":"# Get best model with best pca_components number","fe9ca660":"# Save Best Model"}}