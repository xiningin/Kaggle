{"cell_type":{"c8fc7603":"code","a7014b25":"code","1ddc7a08":"code","8a4c9e94":"code","c1435d4a":"code","b4d39003":"code","2e6e3aa5":"code","99dfd2d3":"code","7e2c99a9":"code","c782ede2":"code","57fdd191":"code","7284df90":"code","bdf84243":"code","7773215d":"code","411c3292":"code","d8ed48e2":"code","584c105f":"code","2d41daea":"code","e5f3efb6":"markdown","ff59e133":"markdown","a34f6103":"markdown","f51ae315":"markdown","bb20b819":"markdown","8b58074a":"markdown","46731956":"markdown","71f6d681":"markdown","123f086c":"markdown","e939c82e":"markdown","f2ad8427":"markdown","a10eb466":"markdown"},"source":{"c8fc7603":"# Load the necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')","a7014b25":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1ddc7a08":"# Load the data\n\ndf = pd.read_csv('\/kaggle\/input\/imdb_movie_data.csv')","8a4c9e94":"print('*'*10 + 'First 10 rows' + '*'*10)\nprint(df.head(10))\nprint(\"\")\nprint('*'*10 + 'Information' + '*'*10)\nprint(df.info())\nprint(\"\")\nprint('*'*10 + 'Null values' + '*'*10)\nprint(df.isnull().any())\nprint(\"\")\n","c1435d4a":"print('*'*10 + 'Duplicate values' + '*'*10)\nprint(df.duplicated(subset='review').value_counts())\n\nsns.heatmap(df.isnull(),cmap='viridis',cbar=False,yticklabels=False)","b4d39003":"df.drop_duplicates(subset='review', inplace=True)","2e6e3aa5":"sns.distplot(df.sentiment,kde=False)","99dfd2d3":"# Let us check one data row\n\ndf.loc[0,'review']","7e2c99a9":"def cleaner(text):\n    # Remove html objects\n    text = re.sub('<[^<]*>','',text)\n    \n    # Temporarily store emoticons\n    emoticons = ''.join(re.findall('[:;=]-+[\\)\\(pPD]+',text))\n    \n    # Remove non-word characters and combine back the emoticons\n    text = re.sub('\\W+',' ',text.lower()) + emoticons.replace('-','')\n    \n    return text","c782ede2":"# let us check the function if it works\n\ncleaner(df.loc[0,'review'])","57fdd191":"# Apply the function to whole dataset\n\ndf['review'] = df['review'].apply(cleaner)\ndf.head(10)","7284df90":"porter = PorterStemmer()\n\ndef token_porter(text):\n    return [porter.stem(word) for word in text.split()]\n\n# We will also tokenize without porter\ndef token(text):\n    return text.split()\n\n# We will pass the 2 functions in our GridSearchCV","bdf84243":"tfidf = TfidfVectorizer(lowercase=False)\n\n# Also load the stopwords from nltk library\nstop = stopwords.words('english')","7773215d":"X = df.iloc[:,0].to_numpy()\ny = df.iloc[:,1].to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, stratify=y)","411c3292":"# Initialize parameters\nparam_grid = [{'vect__stop_words':[stop, None],\n               'vect__tokenizer':[token, token_porter],\n               'clf__penalty':['l2'],\n               'clf__C':[1, 10, 100]},\n              {'vect__use_idf':[False],\n               'vect__stop_words':[stop, None], \n               'vect__tokenizer':[token, token_porter],\n               'clf__penalty':['l2'],\n               'clf__C':[1, 10, 100]}\n             ]\n\n# Use pipeline to build composite estimator\nlr_tfidf = Pipeline([('vect', tfidf),\n                     ('clf', LogisticRegression(tol=0.01, random_state=0))])\n\ngs = GridSearchCV(lr_tfidf, \n                  param_grid, \n                  scoring='accuracy',\n                  cv=5,\n                  n_jobs=1,\n                  verbose=0)","d8ed48e2":"# Fit our model to the train dataset\ngs.fit(X_train, y_train)","584c105f":"print('Best parameter settings: %s' % gs.best_params_)\nprint('CV Accuracy:%.3f' % gs.best_score_)","2d41daea":"# Get our best classifier settings\nclf = gs.best_estimator_\n\nprint('Test Accuracy: %.3f' % clf.score(X_test, y_test))","e5f3efb6":"# 5. Classification using LogisticRegression and GridSearchCV","ff59e133":"### Missing values will appear as yellow line in the plot. So this means there are no missing values in our data.\n### Also there are 418 duplicate values, so let us drop these.","a34f6103":"# 1. Exploratory Data Analysis","f51ae315":"### We see that there is 50-50 distribution of our sentiment classes, therefore we can use accuracy scoring in our model.","bb20b819":"# 2. Cleaning the Data","8b58074a":"# Sentiment Analysis using Logistic Regression\n### In this case study, we will use the dataset from imbd to create a model which will classify if a movie review is a positive or negative sentiment.","46731956":"# 6. Model Accuracy","71f6d681":"### We see that there are 50,000 rows and our data has no null values. Next let us check if there are duplicate and missing values.","123f086c":"# 3. Tokenization\n\n### We use the nltk library to tokenize the documents.","e939c82e":"### We also need to check the distribution of the dependent variable to determine the scoring method to use.","f2ad8427":"# 4. Transform into feature vectors and Data splitting\n### We will use the TfidfVectorizer to transform the words into numbers and give weights to each word.","a10eb466":"### We see that our data contains html objects so we need to remove these."}}