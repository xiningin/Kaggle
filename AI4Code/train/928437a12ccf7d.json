{"cell_type":{"f2375512":"code","72698cd1":"code","8fe5b9bc":"code","409f2a84":"code","fe03adfb":"code","d5fa0aff":"code","cce9a795":"markdown","af5a36b6":"markdown","7dd3ee71":"markdown","5c58b47f":"markdown","d1332797":"markdown","b1621017":"markdown"},"source":{"f2375512":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split","72698cd1":"x = pd.read_csv(\"\/kaggle\/input\/fish-market\/Fish.csv\", usecols=[\"Height\", \"Length1\", \"Length2\", \"Length3\", \"Width\"]).to_numpy()\ny = pd.read_csv(\"\/kaggle\/input\/fish-market\/Fish.csv\", usecols=[\"Species\"]).to_numpy()\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=30)\n\nsc = StandardScaler()\nsc.fit(x_train)\n\nx_train = sc.transform(x_train)\nx_test = sc.transform(x_test)\n\nencoder = OneHotEncoder()\nencoder.fit(y_train)\n\ny_train = encoder.transform(y_train).toarray()\ny_test = encoder.transform(y_test).toarray()","8fe5b9bc":"model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100, 100), solver=\"lbfgs\", max_iter=1000) # This is may take too much processing power\nmodel.fit(x_train, y_train)","409f2a84":"print(\"Accuracy: \", model.score(x_test, y_test))","fe03adfb":"import pickle\n\nwith open(\"model.pkl\", \"wb+\") as f:\n    pickle.dump(model, f)\nprint(\"Model successfully saved!\")","d5fa0aff":"vertical_length = float(input(\"Vertical Length: \"))\ndiagonal_length = float(input(\"Diagonal Length: \"))\ncross_length = float(input(\"Cross Length: \"))\nheight = float(input(\"Height: \"))\nwidth = float(input(\"Width: \"))\n\nscaled = sc.transform([[vertical_length, diagonal_length, cross_length, height, width]])\nprediction_raw = model.predict(scaled)\n\nprint(encoder.inverse_transform(prediction_raw)) ## Returns 2D Array you will have to convert more from their.","cce9a795":"So now that we have imported the neccessary libraries, we have to load and preprocess the data.","af5a36b6":"# Creating the Model\nNow that we have loaded and preprocessed the data, it is time to create the model.","7dd3ee71":"## Measuring Accuracy\nNow that we have our model trained, we should measure the accuracy to see how well it went.","5c58b47f":"# Using the Model\n**NOTE: This code is written in the context of the previous code. This will most likely not be the case if you are using this. You would have to save the model like stated above and save the `OneHotEncoder` and the `StandardScaler` so you can use them for utilizing the model.**\nwWe have trained the model so now it is time to use it for something.","d1332797":"## Saving the Model\nNow that we have trained the model and tested it against our datasets, lets save it so we don't have to train another one later.","b1621017":"# Importing Libraries"}}