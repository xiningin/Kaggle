{"cell_type":{"01d0d253":"code","aab855c2":"code","8d632de2":"code","794185a5":"code","856515ad":"code","2cfee45e":"code","99520ab7":"code","470f4c93":"code","a240c030":"code","c79e92cc":"code","11229f69":"code","845c152f":"code","7a1dfbd6":"code","a0d65ec1":"code","a187c842":"code","2c3f36cb":"code","7380853c":"code","f8101b22":"code","89838e9d":"code","138bff55":"code","ef9dbe97":"code","1db336a9":"markdown","cbc179f7":"markdown","f27099db":"markdown","26ebfda4":"markdown","7294a112":"markdown","7de62d65":"markdown","d9d9558d":"markdown","2d1c5cbb":"markdown","4be99f94":"markdown","d42599a0":"markdown","517ba30f":"markdown","a60cce7f":"markdown","ab22325b":"markdown","9f202389":"markdown","779cbead":"markdown","a5045c6c":"markdown","7e51cb6d":"markdown","f70197a8":"markdown","e3ccbbdb":"markdown","6f18ddfd":"markdown","c9bbb1ac":"markdown","3c71f742":"markdown","b54101fe":"markdown","727ecedc":"markdown","214c8acc":"markdown"},"source":{"01d0d253":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aab855c2":"import torch\nimport torchvision\ndata_directory = \"\/kaggle\/input\/face-mask-lite-dataset\/\"\nclasses = os.listdir(data_directory)\nprint(classes)","8d632de2":"mask_files = os.listdir(data_directory+\"with_mask\")\nwithout_mask_files = os.listdir(data_directory+\"without_mask\")\nprint(\"Mask Images: {}\".format(len(mask_files)))\nprint(\"Without_mask Images: {}\".format(len(without_mask_files)))\nprint(\"Balanced: {}\".format(len(mask_files)==len(without_mask_files)))","794185a5":"from PIL import Image\nfrom tqdm import tqdm\nworking_directory = \"\/kaggle\/working\/\"\n#pull original images and convert them to 50, 50 images\nif os.path.isdir(\"model_data\"):\n    os.mkdir(\"model_data\")\n    os.mkdir(\"model_data\/with_mask\")\n    os.mkdir(\"model_data\/without_mask\")\n    #use tqdm to track progress but it can spam the output\n    print(\"Starting to convert 10000 images.\")\n    for i in tqdm(range(10000)):\n        # Opens a image in RGB mode, then resizes it to 50, 50\n        mask = Image.open(r\"{}\".format(data_directory + \"with_mask\/\" + mask_files[i]))\n        without_mask = Image.open(r\"{}\".format(data_directory + \"without_mask\/\" + without_mask_files[i]))\n        mask = mask.resize((50,50))\n        without_mask = without_mask.resize((50,50))\n        mask.save(r\"{}\".format(working_directory + \"\/model_data\/with_mask\/\" + mask_files[i]))\n        without_mask.save(r\"{}\".format(working_directory + \"\/model_data\/without_mask\/\" + without_mask_files[i]))\n    print(\"Finished converting 10000 images.\")\n#check that an image has been resized\nimage = Image.open(r\"{}\".format(working_directory + \"model_data\/with_mask\/\" + mask_files[0]))\nprint(image.size)","856515ad":"from torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\ntransformed = tt.Compose([tt.RandomHorizontalFlip(), \n                         tt.RandomResizedCrop(50, scale=(0.6,1)), \n                         tt.RandomRotation((-30,30), fill=255),\n                         tt.RandomPerspective(distortion_scale=0.4, p=0.2),\n                         tt.Grayscale(num_output_channels=1),\n                         tt.ToTensor(),])\nnot_transformed = tt.Compose([\n                         tt.Grayscale(num_output_channels=1),\n                         tt.ToTensor(),])\ntransformed_dataset = ImageFolder(working_directory+\"model_data\", transform=transformed)\nnot_transformed_dataset = ImageFolder(working_directory+\"model_data\", transform=not_transformed)","2cfee45e":"image, label = transformed_dataset[900]\nprint(label)\nprint(image.shape)\nprint(transformed_dataset.classes)","99520ab7":"import matplotlib.pyplot as plt\n\nplt.imshow(image.permute(1, 2, 0), cmap='gray',)","470f4c93":"#the split indicies for the training, validation, and test set\ndef split_indices(n, validation_pct=0.1, test_pct=0.1, seed=99):\n    test_val = int(test_pct*n)\n    validation_val = int(validation_pct*n+test_val)\n    np.random.seed(seed)\n    indices = np.random.permutation(n)\n    return indices[:test_val], indices[test_val:validation_val], indices[validation_val:]\nvalidation_pct = 0.2\ntest_pct = 0.1\nrand_seed = 42\ntest_indicies, validation_indicies, train_indicies = split_indices(len(transformed_dataset), validation_pct, test_pct, rand_seed)\nprint(len(train_indicies),len(validation_indicies),len(test_indicies))\nprint(\"Sample validation indices: {0}\".format(validation_indicies[:10]))","a240c030":"from torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data.dataloader import DataLoader\n\nbatch_size = 100\n#dataloader for training\ntrain_sampler = SubsetRandomSampler(train_indicies)\ntrain_dataloader = DataLoader(transformed_dataset, batch_size, sampler=train_sampler, num_workers=4, pin_memory=True)\n\n#dataloader for validation\nvalidation_sampler = SubsetRandomSampler(validation_indicies)\nvalidation_dataloader = DataLoader(not_transformed_dataset, batch_size, sampler = validation_sampler, num_workers=4, pin_memory=True)\n\n#dataloader for testing\ntest_sampler = SubsetRandomSampler(test_indicies)\ntest_dataloader = DataLoader(not_transformed_dataset, batch_size, sampler = test_sampler, num_workers=4, pin_memory=True)","c79e92cc":"from torchvision.utils import make_grid\n\ndef show_batch(dataloader):\n    for images, labels in dataloader:\n        fig, ax = plt.subplots(figsize=(10,10))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, 10).permute(1,2,0))\n        break\nshow_batch(train_dataloader)\nshow_batch(test_dataloader)","11229f69":"import torch.nn as nn\nimport torch.nn.functional as F\nclass MaskModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(1, 50, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(50, 100, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(100, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Flatten(),\n            nn.Linear(1024, 2)\n            )\n        \n    def forward(self, xb):\n        return self.network(xb)\nmodel = MaskModel()\n#check to make sure shapes are correct\nfor images, labels in train_dataloader:\n    print(images.shape)\n    out = model(images)\n    print(labels[0])\n    print(out[0])\n    break","845c152f":"def get_default_device():\n    #Pick GPU if available, else CPU\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    #Move tensor(s) to chosen device\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device)\n\nclass DeviceDataLoader():\n    def __init__(self, dl, device):\n        #Wrap a dataloader to move data to a device\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        #Yield a batch of data after moving it to device\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        #Number of batches\n        return len(self.dl)","7a1dfbd6":"device = get_default_device()\ntrain_dataloader = DeviceDataLoader(train_dataloader, device)\nvalidation_dataloader = DeviceDataLoader(validation_dataloader, device)\ntest_dataloader = DeviceDataLoader(test_dataloader, device)\n#redefine our model on the GPU\nmodel = MaskModel()\nmodel.to(device)","a0d65ec1":"def loss_batch(model, loss_function, images, labels, opt=None, metric=None):\n    preds = model(images)\n    loss_value = loss_function(preds, labels)\n    \n    if opt is not None:\n        #calculate gradients\n        loss_value.backward()\n        #optimize model parameters\n        opt.step()\n        #zero out the gradients\n        opt.zero_grad()\n    metric_result = None\n    if metric is not None:\n        metric_result = metric(preds, labels)\n    return loss_value.item(), len(images), metric_result\ndef evaluate(model, loss_fn, dataloader, metric=None):\n    model.eval()\n    with torch.no_grad():\n        # Pass each batch through the model\n        results = [loss_batch(model, loss_fn, images, labels, metric=metric)\n                   for images, labels in dataloader]\n        # Separate losses, counts and metrics\n        losses, nums, metrics = zip(*results)\n        # Total size of the dataset\n        total = np.sum(nums)\n        # Avg. loss across batches \n        avg_loss = np.sum(np.multiply(losses, nums)) \/ total\n        avg_metric = None\n        if metric is not None:\n            # Avg. of metric across batches\n            avg_metric = np.sum(np.multiply(metrics, nums)) \/ total\n    return avg_loss, total, avg_metric\ndef fit(epochs, model, loss_function, train_dataloader, validation_dataloader, \n        opt_fn=None, lr=None, metric=None):\n    train_losses, val_losses, val_metrics = [], [], []\n    \n    # Instantiate the optimizer\n    if opt_fn is None: opt_fn = torch.optim.SGD\n    opt = opt_fn(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        for images,labels in train_dataloader:\n            train_loss,_,_ = loss_batch(model, loss_function, images, labels, opt)\n\n        # Evaluation\n        result = evaluate(model, loss_function, validation_dataloader, metric)\n        val_loss, total, val_metric = result\n        \n        # Record the loss & metric\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_metrics.append(val_metric)\n        \n        # Print progress\n        if metric is None:\n            print('Epoch [{}\/{}], train_loss: {:4f}, val_loss: {:.4f}'\n                  .format(epoch+1, epochs, train_loss, val_loss))\n        else:\n            print('Epoch [{}\/{}], train_loss: {:.4f}, val_loss: {:.4f}, val_{}: {:.4f}'\n                  .format(epoch+1, epochs, train_loss, val_loss, \n                          metric.__name__, val_metric))\n    return train_losses, val_losses, val_metrics","a187c842":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.sum(preds == labels).item() \/ len(preds)","2c3f36cb":"val_loss, _, val_acc = evaluate(model, F.cross_entropy, \n                                    validation_dataloader, metric=accuracy)\nprint('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss, val_acc))","7380853c":"num_epochs = 10\nopt_fn = torch.optim.Adam\nlr = 1e-6\n\nhistory = fit(num_epochs, model, F.cross_entropy, train_dataloader, validation_dataloader, opt_fn=opt_fn, lr=lr, metric = accuracy)","f8101b22":"def plot_accuracies(history):\n    plt.plot(history[2], '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\nplot_accuracies(history)","89838e9d":"for images, labels in test_dataloader:\n    predictions = model(images)\n    plt.imshow(images[0].cpu().permute(1,2,0), cmap=\"gray\")\n    predictions = torch.argmax(predictions,1)\n    print(images[0].shape)\n    print(\"Prediction: {}\".format(transformed_dataset.classes[predictions[0]]))\n    print(\"Actual: {}\".format(transformed_dataset.classes[labels[0]]))\n    break","138bff55":"test_accuracies = []\nfor images, labels in test_dataloader:\n    test_output = model(images)\n    test_accuracies.append(accuracy(test_output, labels))\naverage_accuracy = sum(test_accuracies)\/len(test_accuracies)\nprint(\"Test Images Accuracy: {}\".format(round(average_accuracy,2)))","ef9dbe97":"import cv2\nforeign_data_input = \"\/kaggle\/input\/outside-dataset\/\"\nforeign_data_output = \"\/kaggle\/working\/outside-dataset\/\"\n# Load the cascade\nface_cascade = cv2.CascadeClassifier('\/kaggle\/input\/opencv\/haarcascade_frontalface_default.xml')\nimages = os.listdir(foreign_data_input)\nmodel.eval()\nif not os.path.isdir(\"outside-dataset\"):\n    os.mkdir(\"outside-dataset\")\nfor image in images:\n    whole_img = Image.open(r\"{}\".format(foreign_data_input+image))\n    whole_img_arr = np.asarray(whole_img)\n    fig = plt.figure()\n    fig.suptitle(\"Image Name: {}\".format(image))\n    plt.imshow(whole_img_arr)\n    img = cv2.imread(r\"{}\".format(foreign_data_input+image))\n    faces = face_cascade.detectMultiScale(img, 1.1, 20)\n    for (x, y, w, h) in faces:\n        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n        cv2.imwrite(r\"{}\".format(foreign_data_output+image), img[y:y+h, x:x+w])\n    img = Image.open(r\"{}\".format(foreign_data_output+image))\n    img = img.resize((50,50))\n    img_arr = np.asarray(img)\n    image_tensors = np.array(tt.Compose([tt.ToTensor(),tt.Grayscale()])(img_arr))\n    image_tensors = torch.from_numpy(image_tensors).unsqueeze(0)\n    image_tensors = image_tensors.to(device)\n    predictions = model(image_tensors)\n    pred = torch.argmax(predictions,1)\n    fig = plt.figure()\n    fig.suptitle(\"Predicted: {}\".format(transformed_dataset.classes[pred[0]]))\n    plt.imshow(img_arr)","1db336a9":"### Some of the changes to our images are drastic, and others not so much. This variability is useful in real world applications. Now we will create our training set, our validation set, and the test set. Let's start by defining the training and validation indicies.","cbc179f7":"### Now that the model is on the GPU, let's define functions to calculate loss, fit, and evaluate our model.","f27099db":"### That testing data that the model had never before seen was mostly accurately classified. Now let's try testing on some data that is completely foreign to the dataset. To isolate the faces in the image before we run our model, we will use opencv to detect faces and then pull the faces from that data.","26ebfda4":"### Model and Annotation by Yanni Kouloumbis","7294a112":"### In order to **decrease reliance** on mask size, as all the masks in the dataset look the same, I am going to apply data augmentation techniques to the dataset to vary the data. This will help in real world application as masks can be of all different shapes and sizes. I am also going to convert the images to grayscale to **reduce**, but not eliminate, reliance on color. This is because in the real world, masks can be of all colors, not just a white mask as in our training data.","7de62d65":"### Throughout PyTorch there are ways to handle images ordered in a folder to class ratio. We will be using Image Folder to apply transforms to our data.","d9d9558d":"### As expected, the initialized model doesn't do too hot on the validation dataset(Two classes, and the accuracy is just about the probability of randomly guessing heads or tails on a coin. Isn't that cool!). Let's start to train it.","2d1c5cbb":"### Awesome! Now let's define our convolutional neural network.","4be99f94":"### Conclusion: Although the model achieves a very high accuracy which is often indicative of overfitting, it is able to handle real world data to an impressive degree. Further adjustments to hyper-parameters may still be made, and may increase the reliablity of the model. Overall, this model achieves its purpose and does so in an efficent manner. Thanks for checking out my model!","d42599a0":"### Our model is passing data correcly. As expected, our initialized model still has a ways to go. Now let's define functions for working on the GPU.","517ba30f":"### Now let's test out the data on an image from the test set.","a60cce7f":"### Nice! Now let's see the results of the entire test set.","ab22325b":"### Now let's take a look at our data again. Each image has one channel since it is a greyscale image, and the size of the images is 50 by 50.","9f202389":"### Nice! Just training for 10 epochs has already vastly increased our accuracy to 98%. Normally, when we hit high accuracies, that indicates something nefarious(possibly overfitting) is going on. Let's visualize our improvements and look deeper into our results.","779cbead":"### Before training, lets see how it preforms on the validation set.","a5045c6c":"### We now create PyTorch dataloaders for handling our data.","7e51cb6d":"### So we can see our two classes, with_mask and without_mask. Let's take a deeper look at our data and how it is structured, while also checking for balance in our dataset.","f70197a8":"### Now let's move our dataloaders and model to our GPU.","e3ccbbdb":"#### Data augmentation techniques used:\n* Randomly flip the image horizontally\n* Randomly resize a crop on the image\n* Randomly rotate the image between -30 and 30 degress, and fill in extra space with white(255)\n* Randomly alter the perspective of the image\n* Grayscale the image","6f18ddfd":"### Our images are currently 1024 by 1024, which is far too large. Let's resize them to 50 by 50, and let's only do it once. ","c9bbb1ac":"### Hello! My name is Yanni Kouloumbis and I am currently a junior at Troy High School in Fullerton, California. Today I wanted to expand upon my ability in combining real world events with deep learning. With that said, today we are going to create a model that can tell if someone is wearing a mask or not. I will be using a dataset of GAN-generated faces with a mask image attatched onto the person. For our project, the first step is to make sure our data is there.","3c71f742":"*Database Credits:\nface-mask-lite-dataset\nall images in outside-dataset were compiled by me, Yanni Kouloumbis, using google images, with credit to respective owners of the images used*","b54101fe":"### Now let's define our metric.","727ecedc":"### Let's show the data in a viewable manner. We do this by moving the data into the format that is accepted by matplotlib's plotting function.","214c8acc":"### Let's make sure that our data is set up correctly and let's view our data in the dataloader format."}}