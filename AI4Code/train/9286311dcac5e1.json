{"cell_type":{"884c757d":"code","cbed477b":"code","a1efb60a":"code","c3465e57":"code","64604b9e":"code","d80c7ee6":"code","08a8d9a2":"code","0143a2a3":"code","f8297c55":"code","d87ac379":"code","b913a263":"code","d52a1a09":"code","a8eefa37":"code","95c734cc":"code","a7fcc0cc":"code","da328f5b":"code","c3ef2f5a":"code","11283255":"code","6ae2776f":"code","9a221c7a":"code","cbc1c989":"code","06c040b5":"code","63ed4f06":"code","6bcfe6d6":"code","c96a61f3":"code","149801f9":"code","d06bc79a":"code","f75ea0f1":"code","03262d80":"code","a47239be":"code","7149cbd5":"code","ae40719a":"code","fc453306":"code","cd85c547":"code","98a28252":"code","7e3402ac":"code","2a075f7f":"code","986e2878":"code","dee6ffa5":"code","db091185":"code","c92c92a0":"code","6103185a":"code","0bbb24f4":"code","cf70ca1f":"code","88056934":"code","c4efc8bb":"code","ce44675e":"code","d11ef552":"code","b8be7c94":"code","3b90b8d7":"code","fb044e68":"code","81eba56e":"code","927a8ffd":"code","b183d035":"code","ed9831cf":"code","d0bdcc7a":"code","cbb5dde9":"code","e5703256":"code","6ae37261":"code","2763de18":"code","eabd4a6b":"code","ee9d7d2d":"code","78bfab79":"code","358ec031":"code","659eae2e":"code","70378fe2":"code","f6746672":"code","af3f716d":"code","7c31e7bb":"code","c7c0aff8":"code","a6797438":"code","ef52a901":"code","f70d4154":"code","31aeed80":"code","e4616226":"code","44f4582b":"code","027d3f18":"code","7b62af81":"code","08c26f31":"code","e609992d":"code","613cae18":"code","999374d2":"code","b869b1e1":"code","9ed3cd0b":"code","594c52f9":"code","284c6e27":"code","6a9310a3":"code","91f922ca":"code","4ab0b7d3":"code","8676faaf":"code","e60f17b3":"code","fc856aac":"code","3e9e0c61":"code","0ad1a41b":"code","1e8eb2d1":"code","a76257b4":"code","027b0726":"code","ad3ac065":"code","e0cc3eb0":"code","3000288a":"code","5c0ab97b":"code","ba850621":"code","a69f558c":"code","e969047d":"code","712551d1":"code","9efecf83":"code","a02e95b7":"code","a2d86e6c":"code","36585f79":"markdown","5f33a1f1":"markdown","7700498b":"markdown","c38ff4d7":"markdown","eb1d6fa6":"markdown","a3d849b1":"markdown","97356728":"markdown","4a2e191a":"markdown","450a96c5":"markdown","fefe02fb":"markdown","2e90807b":"markdown","176252e8":"markdown","9c7deb6a":"markdown","6b32049a":"markdown","1e50d15f":"markdown","d603bb9f":"markdown","5c77c091":"markdown","7f891434":"markdown","43724023":"markdown","17636a07":"markdown","c52b00f4":"markdown","7d65647e":"markdown","cf46e9e3":"markdown","3c6c014a":"markdown","13243e3a":"markdown","49c75ff6":"markdown","f36e7424":"markdown","7c0a874f":"markdown","aea2c848":"markdown","9c4339d4":"markdown","c60deec3":"markdown","62bb724c":"markdown","561e8104":"markdown","0d50312d":"markdown","7050d0f7":"markdown","72485254":"markdown","090bd9e9":"markdown","6a82cc73":"markdown","095bd40d":"markdown","300524ae":"markdown","b715f1bc":"markdown","0c76e21e":"markdown","160d6f06":"markdown","48c9c364":"markdown"},"source":{"884c757d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px","cbed477b":"Application_data=pd.read_csv('..\/input\/google-play-store-apps\/googleplaystore.csv')","a1efb60a":"Application_data.head()","c3465e57":"Application_data.columns","64604b9e":"Application_data.describe()","d80c7ee6":"#Checking whether there are null values in the Ratings column\nnullcheck_ratings=pd.isnull(Application_data[\"Rating\"])\nApplication_data[nullcheck_ratings]","08a8d9a2":"#Replacing the NaN values with the mean rating value\nApplication_data[\"Rating\"].fillna(value=Application_data[\"Rating\"].mean(),inplace=True)\nApplication_data[\"Rating\"]\n","0143a2a3":"# Checking the unique values in the Rating column,we find there is an inconsistent value of 19.\nApplication_data[\"Rating\"].unique()","f8297c55":"# Replacing the inconsistent value with the mean value of ratings\nApplication_data[\"Rating\"].replace(19.,4.1,inplace=True)","d87ac379":"# Checking the unique values of the number of reviews column, we find there are no unrelated values.\nlen(Application_data[\"Reviews\"].unique())","b913a263":"# Checking the Null values of the number of reviews column, we find there are no null values.\nnullcheck_reviews=pd.isnull(Application_data[\"Reviews\"])\nApplication_data[nullcheck_reviews]","d52a1a09":"# Checking for any special character that might prevent numeric conversion, 3.0M is replaced with its real value to make the data consistent.\nApplication_data[\"Reviews\"].replace(\"3.0M\",\"3000000\",inplace=True)","a8eefa37":"# Finally converting the datatype of Reviews column from Object type(String) to Numeric type(float or int)\nApplication_data[\"Reviews\"]=pd.to_numeric(Application_data[\"Reviews\"])","95c734cc":"# Checking for the unique values of the Size column, it is observed it has values appended with M,k and \"Varies with device\"\nApplication_data[\"Size\"].unique()","a7fcc0cc":"# Replacing the \"Varies with device\" field with NaN entry, so that later on these can be replaced with mean values.\nApplication_data['Size'].replace('Varies with device', np.nan, inplace = True )\nApplication_data['Size'].replace('1,000+', np.nan, inplace = True )\n","da328f5b":"# Checking for null values which we will find, since in the above line we have added few null values.\nnullcheck_size=pd.isnull(Application_data[\"Size\"])\nApplication_data[nullcheck_size]\n","c3ef2f5a":"Application_data.Size = (Application_data.Size.replace(r'[kM]+$','', regex=True).astype(float) *\n                         Application_data.Size.str.extract(r'[\\d\\.]+([kM]+)', expand=False).fillna(1).replace(['k','M'], [10**3, 10**6]).astype(int))","11283255":"# Finally replacing the NaN values with the mean value.\nApplication_data[\"Size\"].fillna(value=\"21516530\",inplace=True)","6ae2776f":"# After removing the special characters, lets convert it to numeric data type for finding the mean value.\nApplication_data[\"Size\"]=pd.to_numeric(Application_data[\"Size\"])","9a221c7a":"# Checking the unique values of the column Installs, we observe that there is a type called \"free\", which is inconsistent and non numeric, so it should be replaced.\nApplication_data[\"Installs\"].unique()","cbc1c989":"# Removing the \"+\" symbol to make the column numeric.\nApplication_data[\"Installs\"]=Application_data[\"Installs\"].map(lambda x: x.rstrip('+'))","06c040b5":"# Removing the \",\" from the digits to make it easier.\nApplication_data[\"Installs\"]=Application_data[\"Installs\"].str.replace(\",\",\"\")","63ed4f06":"# There was no null entries found in this column\nnullcheck_installs=pd.isnull(Application_data[\"Installs\"])\nApplication_data[nullcheck_installs]","6bcfe6d6":"# Replacing the inconsistent label value with the mean value of the column.\nApplication_data[\"Installs\"].replace(\"Free\",\"15462910\",inplace=True)","c96a61f3":"# Converting the Datatype to the numeric type for analysis\nApplication_data[\"Installs\"]=pd.to_numeric(Application_data[\"Installs\"])","149801f9":"# Checking for the unique values, we found nan and 0 which should be replaced with Free.\nApplication_data[\"Type\"].unique()","d06bc79a":"# Replacing 0 with Free\nApplication_data[\"Type\"].replace(\"0\",\"Free\",inplace=True)\n","f75ea0f1":"# Filling the missing values with Free, since most of the applications are free on Google play.\nApplication_data[\"Type\"].fillna(value=\"Free\",inplace=True)","03262d80":"# Addding the dummy columns for this, so that it can contribute to our model.\ndummy_type=pd.get_dummies(Application_data[\"Type\"])","a47239be":"#Concatenating the dummy columns with the main dataframe.\nApplication_data=pd.concat([Application_data,dummy_type],axis=1)","7149cbd5":"# Finally dropping the type column.\nApplication_data.drop([\"Type\"],axis=1,inplace=True)","ae40719a":"Application_data.head()","fc453306":"# By checking the unique values we observe that \"Everyone\" is an inconsistent value that should be removed.\nApplication_data[\"Price\"].unique()","cd85c547":"# Removing the dollar symbol\nApplication_data[\"Price\"]=Application_data[\"Price\"].map(lambda x: x.lstrip('$'))","98a28252":"# Removing the non essential row value.\nApplication_data.drop(Application_data[Application_data[\"Price\"] == \"Everyone\"].index, inplace=True)","7e3402ac":"# By checking there were no null values found\nnullcheck_Prices=pd.isnull(Application_data[\"Price\"])\nApplication_data[nullcheck_Prices]","2a075f7f":"# Finally converting to numeric type for analysis\nApplication_data[\"Price\"]=pd.to_numeric(Application_data[\"Price\"])","986e2878":"# Checking the unique values, we found \nApplication_data[\"Category\"].unique()","dee6ffa5":"Application_data[\"Category\"].replace(\"1.9\",\"MISCELLANEOUS\",inplace=True)","db091185":"# Checking for null values, there were no null values found for this column\nnullcheck=pd.isnull(Application_data[\"Category\"])\nApplication_data[nullcheck]","c92c92a0":"# Importing the required library\nfrom sklearn.preprocessing import LabelEncoder","6103185a":"# Instantiating the encoder\nlabelencoder2 = LabelEncoder()","0bbb24f4":"#Encoding the Ctegory column using scikit learn\nApplication_data['Categories_encoded'] = labelencoder2.fit_transform(Application_data['Category'])","cf70ca1f":"# finally dropping the type column, since it is already splitted.\nApplication_data.drop([\"Category\"],axis=1,inplace=True)","88056934":"Application_data.head()","c4efc8bb":"# Checking for unique values\nApplication_data[\"Content Rating\"].unique()","ce44675e":"# Null check for Content Rating\nnullcheck_contentrating=pd.isnull(Application_data[\"Content Rating\"])\nApplication_data[nullcheck_contentrating]","d11ef552":"# importing the required package\nfrom sklearn.preprocessing import LabelEncoder","b8be7c94":"#instantiating the encoder\nlabelencoder = LabelEncoder()","3b90b8d7":"# encoding the column\nApplication_data['Content_Rating_encoded'] = labelencoder.fit_transform(Application_data['Content Rating'])","fb044e68":"# finally removing the content ratig column after encoding\nApplication_data.drop([\"Content Rating\"],axis=1,inplace=True)","81eba56e":"Application_data.head()","927a8ffd":"# Checking the datatypes of the columns to ensure that we have successfully gathered all the numerical columns.\nApplication_data.dtypes","b183d035":"# Finding the mean of all the numerical columns\nApplication_data.mean()","ed9831cf":"sns.pairplot(Application_data)","d0bdcc7a":"colorassigned=Application_data[\"Rating\"]\nfig = px.histogram(Application_data, x=\"Rating\", marginal=\"rug\",\n                   hover_data=Application_data.columns,nbins=30,color=colorassigned)\nfig.show()","cbb5dde9":"fig = px.histogram(Application_data, x=\"Reviews\", marginal=\"rug\",\n                   hover_data=Application_data.columns,nbins=30)\nfig.show()","e5703256":"colorassigned=Application_data[\"Size\"]\nfig = px.histogram(Application_data, x=\"Size\", marginal=\"rug\",\n                   hover_data=Application_data.columns,nbins=30,color=colorassigned)\nfig.show()","6ae37261":"colorassigned=Application_data[\"Installs\"]\nfig = px.histogram(Application_data, x=\"Installs\", marginal=\"rug\",\n                   hover_data=Application_data.columns,nbins=30,color=colorassigned)\nfig.show()","2763de18":"colorassigned=Application_data[\"Price\"]\nfig = px.histogram(Application_data, x=\"Price\", marginal=\"rug\",\n                   hover_data=Application_data.columns,nbins=30,color=colorassigned)\nfig.show()","eabd4a6b":"# Calculating the Correlation and plotting the heatmap to know the relations.\ncors=Application_data.corr()\nfig = px.imshow(cors,labels=dict(color=\"Pearson Correlation\"), x=['Rating', 'Reviews', 'Size', 'Installs', 'Price','Paid','Free','Content_Rating_encoded','Categories_encoded'],\n                y=['Rating', 'Reviews', 'Size','Installs','Price','Paid','Free','Content_Rating_encoded','Categories_encoded'])\nfig.show()","ee9d7d2d":"# Plotting scatter plot with a line of fit between Installs and Reviews, these two have the highest Correlation between them.\nfrom scipy.stats import pearsonr \ncorryu,_ =pearsonr(Application_data[\"Installs\"],Application_data[\"Reviews\"])\ncolorassigned=Application_data[\"Reviews\"]\nfig = px.scatter(Application_data, x=\"Installs\", y=\"Reviews\",trendline=\"ols\",color=colorassigned)\nfig.show()\nprint(\"Pearson Correlation: %.3f\" % corryu)\nprint(\"P-value: %.8f\" % _)","78bfab79":"# Plotting scatter plot with a line of fit between Rating and Reviews, these two have very less correlation between them. \nfrom scipy.stats import pearsonr \ncorryu,_ =pearsonr(Application_data[\"Rating\"],Application_data[\"Reviews\"])\ncolorassigned=Application_data[\"Reviews\"]\nfig = px.scatter(Application_data, x=\"Rating\", y=\"Reviews\",trendline=\"ols\",color=colorassigned)\nfig.show()\nprint(\"Pearson Correlation: %.3f\" % corryu)\nprint(\"P-value: %.8f\" % _)","358ec031":"# Plotting scatter plot with a line of fit between Size and Reviews, these two have very less correlation between them. \nfrom scipy.stats import pearsonr \ncorryu,_ =pearsonr(Application_data[\"Size\"],Application_data[\"Reviews\"])\ncolorassigned=Application_data[\"Reviews\"]\nfig = px.scatter(Application_data, x=\"Size\", y=\"Reviews\",trendline=\"ols\",color=colorassigned)\nfig.show()\nprint(\"Pearson Correlation: %.3f\" % corryu)\nprint(\"P-value: %.8f\" % _)","659eae2e":"from scipy.stats import pearsonr \ncorryu,_ =pearsonr(Application_data[\"Installs\"],Application_data[\"Categories_encoded\"])\ncolorassigned=Application_data[\"Categories_encoded\"]\nfig = px.scatter(Application_data, x=\"Installs\", y=\"Categories_encoded\",trendline=\"ols\",color=colorassigned)\nfig.show()\nprint(\"Pearson Correlation: %.3f\" % corryu)\nprint(\"P-value: %.8f\" % _)","70378fe2":"# Splitting the target variable and the feature matrix\nX=Application_data[[\"Reviews\",\"Size\",\"Rating\",\"Price\",\"Paid\",\"Free\",\"Categories_encoded\",\"Content_Rating_encoded\"]]\ny=Application_data[\"Installs\"]","f6746672":"# importing train test set\nfrom sklearn.model_selection import train_test_split","af3f716d":"# splitting the training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","7c31e7bb":"# importing linear regressor\nfrom sklearn.linear_model import LinearRegression","c7c0aff8":"# Instantiating linear regressor\nlm=LinearRegression()","a6797438":"# Fitting the model\nlm.fit(X_train,y_train)","ef52a901":"# making predictions on the test set\npredictions=lm.predict(X_test)","f70d4154":"# displaying predictions\npredictions","31aeed80":"# Accuracy score for Linear regressor\nlinearregressionscore=lm.score(X_test,y_test)\nlinearregressionscore","e4616226":"# The coefficient for Linear regressor per feature.\nlm.coef_","44f4582b":"# Importing the metrics\nfrom sklearn import metrics","027d3f18":"# Mean absolute error on test data\nmetrics.mean_absolute_error(y_test,predictions)","7b62af81":"# Mean squared error on test data\nmetrics.mean_squared_error(y_test,predictions)","08c26f31":"# Root mean squared error on test data\nrmelinear=np.sqrt(metrics.mean_absolute_error(y_test,predictions))\nrmelinear","e609992d":"# Defining the feature matrix and the target variable\nX=Application_data[[\"Reviews\",\"Size\",\"Rating\",\"Price\",\"Paid\",\"Free\",\"Categories_encoded\",\"Content_Rating_encoded\"]]\ny=Application_data[\"Installs\"]","613cae18":"# Importing the train test split\nfrom sklearn.model_selection import train_test_split","999374d2":"# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","b869b1e1":"# Importing the regressor\nfrom sklearn.tree import DecisionTreeRegressor","9ed3cd0b":"# Instantiating the regressor\ndecisiontreereg=DecisionTreeRegressor()","594c52f9":"# Fitting the model\ndecisiontreereg.fit(X_train,y_train)","284c6e27":"# Gettting the predicted values\ny_prediction=decisiontreereg.predict(X_test)","6a9310a3":"# The accuracy score for decision tree regressor\ndecisiontreescore=decisiontreereg.score(X_test,y_test)\ndecisiontreescore","91f922ca":"from sklearn import metrics","4ab0b7d3":"# Mean absolute error\nmetrics.mean_absolute_error(y_test,y_prediction)","8676faaf":"# Root mean square error\nrmetree=np.sqrt(metrics.mean_absolute_error(y_test,y_prediction))\nrmetree","e60f17b3":"# Separating the feature matrix and target variable\nX=Application_data[[\"Reviews\",\"Size\",\"Rating\",\"Price\",\"Paid\",\"Free\",\"Categories_encoded\",\"Content_Rating_encoded\"]]\ny=Application_data[\"Installs\"]","fc856aac":"# Importing the train test split\nfrom sklearn.model_selection import train_test_split","3e9e0c61":"# Splitting the training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","0ad1a41b":"# Importing the random forest regressor\nfrom sklearn.ensemble import RandomForestRegressor","1e8eb2d1":"# Instantiating with giving the value of number of sub trees to be created.\nRandomforestreg=RandomForestRegressor(n_estimators = 100,n_jobs = -1,oob_score = True, bootstrap = True,random_state=42)","a76257b4":"# fitting the model\nRandomforestreg.fit(X_train,y_train)","027b0726":"# Predicting the number of installs\ny_prediction_randomforest=Randomforestreg.predict(X_test)","ad3ac065":"# Using barplot from seaborn to show importance of features in sorted manner.\nfeature_imp=pd.DataFrame(sorted(zip(Randomforestreg.feature_importances_,Application_data[[\"Reviews\",\"Size\",\"Rating\",\"Price\",\"Paid\",\"Free\",\"Categories_encoded\",\"Content_Rating_encoded\"]])),columns=[\"Significance\",\"Features\"])\nfig=plt.figure(figsize=(6,6))\nsns.barplot(x=\"Significance\",y=\"Features\",data=feature_imp.sort_values(by=\"Significance\",ascending=False),dodge=False)\nplt.title(\"Important features for predicting the number of installs\")\nplt.tight_layout()\nplt.show()","e0cc3eb0":"\nfrom sklearn.metrics import r2_score,mean_squared_error","3000288a":"# The performance of random forest.\nprint('R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Validation Score: {:.2f}'.format(Randomforestreg.score(X_train, y_train), \n                                                                                             Randomforestreg.oob_score_,\n                                                                                             Randomforestreg.score(X_test, y_test)))","5c0ab97b":"# Accuracy score for random forest\nrandomforestscore=Randomforestreg.score(X_test,y_test)\nrandomforestscore","ba850621":"# Importing the performance metrics\nfrom sklearn import metrics","a69f558c":"# Mean absolute error\nmetrics.mean_absolute_error(y_test,y_prediction_randomforest)","e969047d":"# Root mean squared error\nrmerandom=np.sqrt(metrics.mean_absolute_error(y_test,y_prediction_randomforest))\nrmerandom","712551d1":"# Creating the dataframe that has accuracy score and root mean squared error for all the 3 models.\ndict={\"Linear Regressor\":[linearregressionscore,rmelinear],\"DecisionTree Regressor\":[decisiontreescore,rmetree],\"RandomForest Regressor\":[randomforestscore,rmerandom]}\ndf_comparison_models=pd.DataFrame(dict,[\"Score\",\"Root Mean Square Error\"])","9efecf83":"df_comparison_models.head()","a02e95b7":"# Plotting the accuracy of all the 3 models\n%matplotlib inline\nmodel_accuracy = pd.Series(data=[linearregressionscore,decisiontreescore,randomforestscore], \n        index=['Linear_Regressor','DecisionTree Regressor','RandomForest Regressor'])\nfig= plt.figure(figsize=(8,8))\nmodel_accuracy.sort_values().plot.barh()\nplt.title('Model Accuracy')","a2d86e6c":"# Plotting the Root Mean Squared Error comaparison\n%matplotlib inline\nmodel_accuracy = pd.Series(data=[rmelinear,rmetree,rmerandom], \n        index=['Linear_Regressor','DecisionTree Regressor','RandomForest Regressor'])\nfig= plt.figure(figsize=(8,8))\nmodel_accuracy.sort_values().plot.barh()\nplt.title('Model Root Mean Squared Error')","36585f79":"## The final step to compare our models accuracy for this task. We will be comparing the accuracy score and the root mean square error for all the 3 models. For that to achieve, we need to create a dataframe that consists of the accuracy score and root mean square error for each model and then we can plot that dataframe.","5f33a1f1":"# RANDOM FOREST REGRESSOR","7700498b":"# DECISION TREE REGRESSOR","c38ff4d7":"## It is very fruitful if we know how much significant a feature is for predicting our target variable. Below is the plot showing the importance of various features in predicting the number of installs.","eb1d6fa6":"#### Here a pairplot is shown between all the numerical columns of the data. This gives a high level of intuition between the relationships between the various features. Now firstly, histograms will be drawn for all the numerical columns just to know thier counts and distribution. Plotly is used here for graphical representations.","a3d849b1":"#### For this column, we will perform the label encoding and not the dummies, since by making dummies there will be too many extra columns added to our feature matrix that is not required, so label encoding is done by providing numerical values to each and every category of application.","97356728":"#### Here we have completed the cleaning of the Size column by following all the 6 steps which were required, since this column was very uncleaned.","4a2e191a":"#### We need to remove the \"free\" with the average number of installs for the applications, but for calculating the average, we need to remove the \"+\" and \",\" from the values. After removing them, we will have to convert these into numeric type and then we can calculate the mean and finally substitute the mean value in place of \"Free\".","450a96c5":"## Following inferences can be drawn from this heatmap:\n### CORRELATION VALUE                                FEATURES INVOLVED                                              VERDICT\n\n###             -0.020                                                      Price vs Rating                                                    No Correlation\n\n###             -0.009                                                      Price vs Reviews                                                 No Correlation\n\n###             -0.022                                                      Price vs Size                                                        No Correlation\n\n###             -0.011                                                      Price vs Installs                                                   No Correlation\n\n###              0.051                                                      Installs vs Rating                                                 No Correlation\n\n###              0.643                                                      Installs vs Reviews                                              Great Correlation\n\n###              0.082                                                      Installs vs Size                                                     No Correlation\n\n###             -0.011                                                      Installs vs Price                                                   No Correlation\n\n###              0.074                                                      Size vs Rating                                                      No Correlation\n\n###              0.128                                                      Size vs Reviews                                                 Very less Correlation\n\n###              0.082                                                      Size vs Installs                                                    No Correlation\n\n###             -0.022                                                      Size vs Price                                                        No Correlation\n\n###              0.067                                                      Reviews vs Rating                                              No Correlation\n\n\n\n## We will be plotting only those relations whose correlation value is greater than 0.1, rest all do not have any correlation, so plotting will not be fruitful. ","fefe02fb":"# \"REVIEWS\" column cleaning by following the 6 steps","2e90807b":"# \"PRICE\" column cleaning by following the 6 steps","176252e8":"# \"RATING\" column cleaning by following the 6 steps","9c7deb6a":"# \"CONTENT RATING\" Column cleaning by 6 Steps","6b32049a":"#### There was no special character found in the ratings column, so step 4 is not required. Moreover, the datatype of ratings column is already float, so no need for the conversion. So now our Rating column is ready for analysis.","1e50d15f":"#### In this way we have removed the Type categorical column, used dummy columns to make our feature space more accurate.","d603bb9f":"#  \"SIZE\" column cleaning by following the 6 steps","5c77c091":"#### Evaluating the metrics for linear regresssor, the mean absolute error, mean squared error and finally root mean square error.","7f891434":"# MODEL BUILDING AND EVALUATION USING SKLEARN","43724023":"#### All the steps have been completed for the reviews column and it is also ready for the analysis.","17636a07":"## It is observed that we have a good fit to the data points, since the correlation between these 2 columns is significant. As it is visible, as the number of installs increases, the number of reviews are also increasing which makes sense, since if the user has installed the application, then only they can give feedback to it. Without using an application, reviews cannot be given. If we get a new data point, we can predict its number of installs based on the number of reviews. By hovering on the red line, the equation of the straight line can be seen. Hovering on each data point gives its installs and reviews at that point.","c52b00f4":"#### Here to get the mean of the values, the datatype of the column should be numeric and for that to happen we need to remove the dollar symbol from the values and drop the everyone row, since it contains redundant data that will compromise the performance of our model.","7d65647e":"# CLEANING THE DATASET\n### The dataset will have redundant values like NaN, or some columns will not have any value at all, some columns will have unrelated values, some will be having some special charcters which cannot be feeded to our machine learning model. So these inconsistencies will be resolved in this section using basic python skills and pandas tricks.","cf46e9e3":"# \"TYPE\" column cleaning by following the 6 steps","3c6c014a":"#### For this categorical column also, we are doing the label encoding similarly we did for the Category column.","13243e3a":"### This was all about this dataset, where we performed all the process from the scratch. We loaded the data, cleaned the features, did a thorough exploratory data analysis to understand which will be the key features that will be vital for predicting our target variable, finally created the models and made some predictions. At the last compared the accuracy of all the models on which the analysis was performed. ","49c75ff6":"# \"CATEGORY\" column cleaning by following the 6 steps","f36e7424":"#### This is an histogram to show the distribution of number of reviews for each application. It is quite clearly visible that, 90% of the applications on Google play store have reviews less than 5 million. 138 applications have reviews between 5 Million to 10 Million. Only 47 android applications have reviews between 10 Million to 15 Million. So majority of the applications have less then 5 Million reviews.","7c0a874f":"#### We have cleaned the Price column by following all the 6 steps as per the requirement, now this column is ready for the analysis.","aea2c848":"#### In this way, we have made our Installs column ready for the analysis by following all the 6 steps again.","9c4339d4":"#### The above Graph is an Histogram, that shows the distribution of ratings of various android applications. The histogram is divided into colors based on the values of the rating. The color scale is given on the right side. The count of rating 4.1 is maximum(1474) as can be found by hovering on the graph. Moreover, the count of rating uniformly increases from 3.4(128) to going to maximum of 4.1(1474), and then again going up and down. This means most of the applications on google play have their ratings between 4 to 4.5.","c60deec3":"## LINEAR REGRESSOR ","62bb724c":"# \"INSTALL\" column cleaning by following the 6 steps","561e8104":"## With this we have completed the individual analysis of all the numerical columns of our dataset. Now we will find the relation between each column to analyse deeply. The step that is followed below is:\n### 1)- Calculate the correlation value and draw a heatmap to know the correlation between different columns.\n### 2)- Once we find the correlation, then we know which columns are affecting one another, then we start plotting columns in pair of two based on thier correlation values. If the correlation is negative or very less, there is no point in plotting those columns.\n### 3)- After plotting we will fit a linear regression line to our data points. The more the correlation value, better line of fit we get.\n","0d50312d":"### The final step is creating the model that will predict the number of installs for an android application on Google play. We will be using 3 regressors for this purpose, Linear, DecisonTree and RandomForest. Finally the performance for all the 3 will be compared in the graphical format.","7050d0f7":"#### Now we need to replace the NaN values with the mean size of all the applications, but we cannot calculate mean since our column is of Object type String, so we need to convert it into a numeric type. Moreover, we need to remove \"M\", \"k\" from the values of the column, since we cannot convert them to numeric without handling these special symbols.","72485254":"## As can be observed from this graph, we can see that the applications that have ratings between 4 to 4.7 have maximum number of reviews. However, we cannot say that as the ratings increases the reviews increases, this happens just for a particular range of 4 to 4.7 where Reviews increase as the ratings increase but before 4 and after 4.7 there is different trend. It is observed that after rating 4.7, the count of number of reviews have reduced, that is the applications to which review is given is reduced. The apps having 5 star rating have only 4 reviews. However, the apps having rating less then 4 have been rated by many users.","090bd9e9":"## There is no general trend observed in this graph, since there is very less correlation observed in these two columns. There are applications of size 21 MB getting 80 Million reviews, and there are applications with larger size like 98 MB, getting 45 million reviews. So there is no trend observed here.","6a82cc73":"#### The above graph shows the number of installs of the android applications. It can be observed that majority of the applications have less than 10 Million installs. Moreover, there are only 58 applications that have more than 1 billion installs on Google play.","095bd40d":"#### The above Graph is an Histogram, that shows the distribution of size of various android applications. It can be observed that most of the applications have lesser size, since as the size increases on the x-axis, the bars are getting shorter and shorter, which means the count of such types of apps is decreasing. So we have more applications on Google playstore that are smaller in size than the larger ones. Most of the applications have the size of around 21.5 MB.","300524ae":"# FINAL THOUGHTS","b715f1bc":"## Lets move from left to right in the columns of the dataset, we start from \"RATING\" column, and move till \"PRICE\" column, since these are the numeric columns and are neccessary features for our model. We will do following process on each of these columns:\n#### 1)- Checking all the unique values in the column.\n#### 2)- If there are some unrelated unique values which are not significant, these will be replaced.\n#### 3)- The null check is performed on each numerical column, if null entries are found, they are replaced with the mean values.\n#### 4)- The values in the columns contain some special characters, that needs to be removed in order to perform aggregations, so those will be removed like \"+\" and \",\" in Installs column, \"M\", \"Varies with Device\" and \"k\" from Size column etc.\n#### 5)- The columns that are in object type will be converted to their numerical counterparts for analysis and trends.\n#### 6)- Final filtration will be done to make sure there is no inconsistency in any column that may affect the performance of our model.","0c76e21e":"#### This histogram shows the price distribution of various android applications on Google play. Majority of the applications are free of cost. There are 12 android applications that are the most expensive costing 400 bucks. ","160d6f06":"# READING THE DATASET\n### The first step in any machine learning problem is reading the data from a given file format, in this case we have a csv file from where we will read the data.","48c9c364":"# EXPLORATORY DATA ANALYSIS\n## Below there is a complete analysis of various relationships between the features of our data. This is required so that we can understand what all features will play a significant role when predicting the number of installs for any application."}}