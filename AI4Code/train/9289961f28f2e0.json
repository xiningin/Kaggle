{"cell_type":{"2f7c6132":"code","3b4345ed":"code","c9adf440":"code","3a7e31b8":"code","e52cd442":"code","475e2583":"code","9fa665a2":"code","4099f347":"code","b15560d0":"code","7afca3bd":"code","8378268b":"code","b7961980":"code","ad6300b4":"code","349d29c3":"code","27994ad5":"code","6e836734":"code","4a75a84e":"code","6300204f":"code","787345f1":"code","0b5cc3f5":"code","cfe66b05":"code","975c18e5":"code","a4b92c06":"code","aa36f63e":"code","4ad92580":"code","f5be159f":"code","30f6ce53":"code","8309f792":"code","1fc51717":"code","b9877255":"code","1797e46f":"code","2544be25":"code","692460e4":"code","77fa991d":"code","961be665":"code","a950a807":"code","adef69d6":"code","7b3580c7":"code","14a518e1":"code","f5e9d92a":"code","aa939346":"markdown","3b06c224":"markdown","e3f75a6d":"markdown","40b3f37c":"markdown","0776a0af":"markdown","ef457e6f":"markdown","92341247":"markdown","c4bd6740":"markdown","9d26b211":"markdown","ecc58d07":"markdown","ab944db3":"markdown","10f6bd1b":"markdown","4907c7cf":"markdown","b8922bdf":"markdown","e6018c2e":"markdown","689dae7f":"markdown"},"source":{"2f7c6132":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b4345ed":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","c9adf440":"train","3a7e31b8":"train.head()","e52cd442":"test.head()","475e2583":"resume = train.describe()\nresume","9fa665a2":"test.describe()","4099f347":"data_missing_value = train.isnull().sum().reset_index()\ndata_missing_value.columns = ['feature','missing_value']\ndata_missing_value['percentage'] = round((data_missing_value['missing_value']\/len(train))*100,2)\ndata_missing_value = data_missing_value.sort_values('percentage', ascending=False).reset_index(drop=True)\ndata_missing_value = data_missing_value[data_missing_value['percentage']>0]\n# df_i.export(data_missing_value, 'data_missing_value.png')\ndata_missing_value","b15560d0":"data_missing_value = test.isnull().sum().reset_index()\ndata_missing_value.columns = ['feature','missing_value']\ndata_missing_value['percentage'] = round((data_missing_value['missing_value']\/len(test))*100,2)\ndata_missing_value = data_missing_value.sort_values('percentage', ascending=False).reset_index(drop=True)\ndata_missing_value = data_missing_value[data_missing_value['percentage']>0]\n# df_i.export(data_missing_value, 'data_missing_value.png')\ndata_missing_value","7afca3bd":"train.duplicated().sum()","8378268b":"test.duplicated().sum()","b7961980":"numb = train.select_dtypes(include='number').columns\nnumb\n","ad6300b4":"obj = train.select_dtypes(include='object').columns\ntrain[obj].nunique()\nfor i in obj:\n    print(i,' berisi >>>',len(train[i].unique()), 'unik')","349d29c3":"# Replace Null in Age\ntrain['Age']=train['Age'].fillna(train['Age'].mean())\ntest['Age']=train['Age'].fillna(train['Age'].mean())\n# Replace Null in Cabin\ntrain['Cabin']=train['Cabin'].fillna('None')\ntest['Cabin']=test['Cabin'].fillna('None')\n\n# Change Cabin to fisrt char\ntrain['Cabin'] = train['Cabin'].str.slice(stop=1)\ntest['Cabin'] = test['Cabin'].str.slice(stop=1)\n\n#Encode Alternarive Cabin as 1 if have cabin else 0\n# train['Cabin'] = train['Cabin'].apply(lambda x: 0 if x=='N' else 1 )\n# test['Cabin'] = test['Cabin'].apply(lambda x: 0 if x=='N' else 1 )\n\n# Replace Null in Fare\ntest['Fare']=test['Fare'].fillna(train['Fare'].mean())\ntrain = train.dropna()\n","27994ad5":"obj = train.select_dtypes(include='object').columns\ntrain[obj].nunique()\nfor i in obj:\n    print(i,' berisi >>>',len(train[i].unique()), 'unik')","6e836734":"import matplotlib.pyplot as plt\nimport seaborn as sns\nk = len(train.columns)\nn = 3\nm = (k - 1) \/\/ n + 1\nfig, axes = plt.subplots(m, n, figsize=(n * 8, m * 5))\nsp=0\nfor i in train.columns:\n    r, c = sp \/\/ n, sp % n\n    sns.histplot(ax=axes[r,c] , data = train, x = i,hue = 'Survived')\n    sp=sp+1\nplt.show()","4a75a84e":"for i in obj:\n    dummy = train[[i,'Survived']].groupby(i).agg(['mean']).reset_index(drop=False)\n    awal = list(dummy[dummy.columns[0]])\n    akhir = list(dummy[dummy.columns[1]])\n    EF = 'AVGSurvived'+i\n    train[EF] = train[i].replace(awal,akhir)\n    test[EF] = test[i].replace(awal,akhir)\n    print('Fitur ',i,' encoded')\n#     for j in range(len(awal)):\n#         print('*',awal[j],' \\t -->> ', akhir[j])\ntrain","6300204f":"numb = train.select_dtypes(include='number').columns\nnumb","787345f1":"X = train[numb].copy().drop(['PassengerId','Survived', 'AVGSurvivedName','AVGSurvivedTicket'],axis =1)\nY = train[ 'Survived'].copy()\nfor i in X.columns:\n    X[i] = (X[i]-X[i].min())\/(X[i].max()-X[i].min())","0b5cc3f5":"X","cfe66b05":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# print(train.columns)\ny = Y\nchi2_selector = SelectKBest(chi2,k=len(X.columns))\nchi2_selector.fit(X,y)\n# X_kbest = chi2_selector.fit_transform(X, y)\nfeature = X.columns\nnilai = (chi2_selector.scores_).tolist()\nprint(chi2_selector.get_support(indices=True))\nchi2_selector.scores_\nChis = pd.DataFrame({'Feature':feature,\n             'chi': nilai}).sort_values(by= 'chi', ascending=False)\nimport seaborn\nseaborn.barplot(y=Chis['Feature'],x=Chis['chi'],orient='h')\n","975c18e5":"numb = train.select_dtypes(include='number').columns\nnumb","a4b92c06":"x = train[['Pclass', \n           'Age', \n           'SibSp', \n           'Parch', \n           'Fare',\n           'AVGSurvivedSex',\n           'AVGSurvivedCabin', \n           'AVGSurvivedEmbarked'\n          ]].copy()\ny = train[ 'Survived'].copy()\n\n# x = X\n# y =Y\n\nfrom sklearn.model_selection import train_test_split \nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 8\n                                                  , stratify = y, shuffle = True\n                                                 )","aa36f63e":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve, auc","4ad92580":"\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\n\n\nMODEL = {xgb.XGBClassifier(),\n         LogisticRegression(random_state=8, solver='liblinear'),\n         KNeighborsClassifier(n_neighbors=350),\n         tree.DecisionTreeClassifier(random_state=8),\n         RandomForestClassifier(random_state=8),\n         AdaBoostClassifier(random_state=8)\n}\n\n\nfor MODEL in MODEL:\n    MODEL.fit(x_train, y_train)\n    y_pred = MODEL.predict(x_test)\n    print(MODEL)\n    print('\\nconfustion matrix') # generate the confusion matrix\n    print(confusion_matrix(y_test, y_pred))\n\n    print('\\naccuracy')\n    print(accuracy_score(y_test, y_pred))\n\n\n    print('\\nclassification report')\n    print(classification_report(y_test, y_pred))\n    y_pred = MODEL.predict_proba(x_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1) # pos_label: label yang kita anggap positive\n   \n    print('Area Under ROC Curve (AUC):', auc(fpr, tpr))","f5be159f":"test","30f6ce53":"from sklearn.model_selection import cross_val_score\n\nMODEL = {\n         xgb.XGBClassifier(),\n#          LogisticRegression(random_state=8, solver='liblinear'),\n#          KNeighborsClassifier(n_neighbors=350),\n#          tree.DecisionTreeClassifier(random_state=8),\n#          RandomForestClassifier(random_state=8),\n         AdaBoostClassifier(\n             learning_rate=0.1,\n             n_estimators=1500,\n#              random_state=8\n             random_state=8)\n}\n\n\nfor MODEL in MODEL:\n    clf = MODEL\n    scores = cross_val_score(clf, x, y, cv=5,scoring='accuracy')\n    print(MODEL)\n    print(scores)\n    print(scores.mean())\n    print(scores.std())","8309f792":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nimport numpy as np\n\nMODEL= AdaBoostClassifier(random_state=8)\n\n# list of hyperparameter\nn_estimators =[50,100,200,400,800,1500]\nlearning_rate= [0.01,0.1,0.5,1,2,5,10]\n\n# max_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree\n# min_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node\n# min_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node\n# max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n\n#Menjadikan ke dalam bentuk dictionary\nhyperparameters = {'n_estimators' : n_estimators,\n                   'learning_rate' : learning_rate\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'max_features': max_features\n                }\n\n# Init Logres dengan Gridsearch, cross validation = 5\n# dt2 = DecisionTreeClassifier(random_state=42)\nclf = RandomizedSearchCV(MODEL, hyperparameters, cv=5)\n\n#Fitting Model\nbest_model = clf.fit(x, y)\n\n#Prediksi menggunakan model baru\n# y_pred = best_model.predict(X_test)#Check performa dari model\n# print(classification_report(y_test, y_pred))\nbest_model.best_estimator_","1fc51717":"MODEL= xgb.XGBClassifier()\n\n# list of hyperparameter AdaBoost\nn_estimators =[50,100,200,400,800,1500]\nlearning_rate= [0.01,0.1,0.5,1,2,5,10]\n\n# list of hyperparameter XGBoost\nmax_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree\nmin_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node\nmax_features = ['auto', 'sqrt'] # Number of features to consider at every split\n\n#Menjadikan ke dalam bentuk dictionary\nhyperparameters = {\n                #ADABOOST\n#                 'n_estimators' : n_estimators,\n#                 'learning_rate' : learning_rate\n                #XGBOOST\n                'max_depth': max_depth,\n                'min_samples_split': min_samples_split,\n                'min_samples_leaf': min_samples_leaf,\n                'max_features': max_features\n                }\n\n# Init Logres dengan Gridsearch, cross validation = 5\n# dt2 = DecisionTreeClassifier(random_state=42)\nclf = RandomizedSearchCV(MODEL, hyperparameters, cv=5)\n\n#Fitting Model\nbest_model = clf.fit(x, y)\n\n#Prediksi menggunakan model baru\n# y_pred = best_model.predict(X_test)#Check performa dari model\n# print(classification_report(y_test, y_pred))\nbest_model.best_estimator_","b9877255":"x = train[['Pclass', \n           'Age', \n           'SibSp', \n           'Parch', \n           'Fare',\n           'AVGSurvivedSex',\n           'AVGSurvivedCabin', \n           'AVGSurvivedEmbarked'\n          ]].copy()\ny = train[ 'Survived'].copy()","1797e46f":"TOPMODEL = [\n    xgb.XGBClassifier(min_samples_split = 100,\n   min_samples_leaf = 2,\n   max_features = 'sqrt',\n   max_depth = 64), \n#     LogisticRegression(random_state=8, solver='liblinear'),\n#     KNeighborsClassifier(n_neighbors=350),\n#     tree.DecisionTreeClassifier(random_state=8),\n#     RandomForestClassifier(random_state=8),\n    AdaBoostClassifier(learning_rate=0.1, n_estimators=1500, random_state=8)\n    ]\nfor i in range (len(TOPMODEL)):\n    exec(f'model{i} = TOPMODEL[i]')\n    exec(f'model{i}.fit(x, y)')\n#     exec(f'y_pred = model{i}.predict(x_test)')\n#     print('\\n*************************************************************************')\n    exec(f'print(model{i})')\n#     print('\\nconfustion matrix') # generate the confusion matrix\n#     print(confusion_matrix(y_test, y_pred))\n\n#     print('\\naccuracy')\n#     print(accuracy_score(y_test, y_pred))\n\n\n#     print('\\nclassification report')\n#     print(classification_report(y_test, y_pred))\n#     y_pred2 = exec(f'model{i}.predict_proba(x_test)[:,1]')\n#     fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1) # pos_label: label yang kita anggap positive\n   \n#     print('Area Under ROC Curve (AUC):', auc(fpr, tpr))","2544be25":"# x = test[['Pclass', \n#            'Age', \n#            'SibSp', \n#            'Parch', \n#            'Fare',\n#            'AVGSurvivedSex',\n#            'AVGSurvivedCabin', \n#            'AVGSurvivedEmbarked'\n#           ]].copy()\n# out0 = model0.predict(x)\n# out1 = model1.predict(x)\n# out01 = out0+out1\n# out = out01\n# for i in range(len(out01)):\n#     if out01[i] == 2:\n#         out[i] = 1\n#     if out01[i] == 0:\n#         out[i] = 0\n#     if out01[i] == 1:\n#         if out0[i] == 1:\n#             out[i] = 1\n#         else:\n#             out[i] = 0\n# out","692460e4":"x = test[['Pclass', \n           'Age', \n           'SibSp', \n           'Parch', \n           'Fare',\n           'AVGSurvivedSex',\n           'AVGSurvivedCabin', \n           'AVGSurvivedEmbarked'\n          ]].copy()\nout0 = model0.predict_proba(x)[:,1]\nout1 = model1.predict_proba(x)[:,1]\nout = out0*0.5 + out1*0.5","77fa991d":"out = out.round().astype(int)\n","961be665":"out","a950a807":"submiss = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","adef69d6":"submiss['Survived'] = out","7b3580c7":"submiss","14a518e1":"round(0.5000000000001)","f5e9d92a":"submiss.to_csv(\"submiss10.csv\", index=False)","aa939346":"Perlu buang null di embarked 0.22% data\n\nCabin dan age perlu diisi. ","3b06c224":"# Load and Describe Data","e3f75a6d":"# Model Selection","40b3f37c":"# EDA","0776a0af":"# Missing Value Handling","ef457e6f":"# Missing Value Check","92341247":"# Encode Object","c4bd6740":"# Ensembling and Predict Using 2 best Model","9d26b211":"# Hyper Parameter Tuning","ecc58d07":"# Cross Val","ab944db3":"# Train 2 Best Model","10f6bd1b":"Fare missing berbeda dengan di train perlu diurus juga","4907c7cf":"# Number and Object Data Separation","b8922bdf":"# Submission","e6018c2e":"# Duplicate Check","689dae7f":"# Feature Selection"}}