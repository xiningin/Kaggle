{"cell_type":{"bbd84801":"code","7d564241":"code","f1c1aad6":"markdown"},"source":{"bbd84801":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# load the meta data from the CSV file using 3 columns (abstract, title, authors),\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['title','abstract','authors','doi','publish_time'])\nprint (df.shape)\n#drop duplicates\n#df=df.drop_duplicates()\ndf = df.drop_duplicates(subset='abstract', keep=\"first\")\n#drop NANs \ndf=df.dropna()\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()\n#show 5 lines of the new dataframe\nprint (df.shape)\ndf.head()","7d564241":"import functools\nfrom IPython.core.display import display, HTML\nfrom nltk import PorterStemmer\n\n#tell the system how many sentences are needed\nmax_sentences=5\n\n# function to stem keywords into a common base word\ndef stem_words(words):\n    stemmer = PorterStemmer()\n    singles=[]\n    for w in words:\n        singles.append(stemmer.stem(w))\n    return singles\n\n# list of lists for topic words realting to tasks\ndisplay(HTML('<h1>COVID-19 summary page vaccines and therapeutics<\/h1>'))\ndisplay(HTML('<h3>Table of Contents (ctrl f and search the hash tag and words below to find table<\/h3>'))\ntasks = [['prophylaxis'],['drug','study'],['vaccine'],['fda','approved','drugs' ],['drug','repurposing'],['clarithromycin'],['chloroquine'],['antibody','dependent', 'enhancement'],['therapeutic'],['animal','models'],['process','development']]\nz=0\nfor terms in tasks:\n    stra=' '\n    stra=' '.join(terms)\n    k=str(z)\n    #display(HTML('<a href=\"#'+k+'\">'+stra+'<\/a>'))\n    display(HTML('# '+stra))\n    z=z+1\n# loop through the list of lists\nz=0\nfor search_words in tasks:\n    df_table = pd.DataFrame(columns = [\"pub_date\",\"authors\",\"title\",\"excerpt\"])\n    str1=''\n    # a make a string of the search words to print readable search\n    str1=' '.join(search_words)\n    search_words=stem_words(search_words)\n    # add cov to focus the search the papers and avoid unrelated documents\n    search_words.append(\"covid\")\n    # search the dataframe for all the keywords\n    dfa=df[functools.reduce(lambda a, b: a&b, (df['abstract'].str.contains(s) for s in search_words))]\n    search_words.pop()\n    search_words.append(\"-cov-\")\n    dfb=df[functools.reduce(lambda a, b: a&b, (df['abstract'].str.contains(s) for s in search_words))]\n    # remove the cov word for sentence level analysis\n    search_words.pop()\n    #combine frames with COVID and cov and drop dups\n    frames = [dfa, dfb]\n    df1 = pd.concat(frames)\n    df1=df1.drop_duplicates()\n    \n    display(HTML('<h3>Task Topic: '+str1+'<\/h3>'))\n    \n    display(HTML('# '+str1+' <a><\/a>'))\n    z=z+1\n    # record how many sentences have been saved for display\n    # loop through the result of the dataframe search\n    for index, row in df1.iterrows():\n        pub_sentence=''\n        sentences_used=0\n        #break apart the absracrt to sentence level\n        sentences = row['abstract'].split('. ')\n        #loop through the sentences of the abstract\n        for sentence in sentences:\n            # missing lets the system know if all the words are in the sentence\n            missing=0\n            #loop through the words of sentence\n            for word in search_words:\n                #if keyword missing change missing variable\n                if word not in sentence:\n                    missing=missing+1\n            # after all sentences processed show the sentences not missing keywords limit to max_sentences\n            if missing<len(search_words) and sentences_used < max_sentences and len(sentence)<1000 and sentence!='':\n                sentence=sentence.capitalize()\n                if sentence[len(sentence)-1]!='.':\n                    sentence=sentence+'.'\n                pub_sentence=pub_sentence+'<br><br>'+sentence\n        if pub_sentence!='':\n            sentence=pub_sentence\n            sentences_used=sentences_used+1\n            authors=row[\"authors\"].split(\" \")\n            link=row['doi']\n            title=row[\"title\"]\n            linka='https:\/\/doi.org\/'+link\n            linkb=title\n            sentence='<p align=\"left\">'+sentence+'<\/p>'\n            final_link='<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka,linkb)\n            to_append = [row['publish_time'],authors[0]+' et al.',final_link,sentence]\n            df_length = len(df_table)\n            df_table.loc[df_length] = to_append\n    filename=str1+'.csv'\n    df_table.to_csv(filename,index = False)\n        #display(HTML('<b>'+sentence+'<\/b> - <i>'+title+'<\/i>, '+'<a href=\"https:\/\/doi.org\/'+link+'\" target=blank>'+authors[0]+' et al.<\/a>'))\n    df_table=HTML(df_table.to_html(escape=False,index=False))\n    display(df_table)\nprint (\"done\")","f1c1aad6":"# **COVID-19 summary page vaccines and therapeutics**\n\n![](https:\/\/sportslogohistory.com\/wp-content\/uploads\/2018\/09\/georgia_tech_yellow_jackets_1991-pres-1.png)\n\n***IF YOU FIND THIS USEFUL, PLEASE UPVOTE IT.***\n\n**PROBLEM:** When a new virus is discovered and causes a pandemic, it is important for scientists to get information coming from all scientific sources that may help them combat the pandemic.  The challenege, however, is that the number of scientific papers created is large and the papers are published very rapidly, making it nearly impossible for scientists to digest and understand important data in this mass of data.\n\n**SOLUTION:** Create an unsupervised scientific literature understanding system that can take in common terms and analyze a very large corpus of scientific papers and return highly relevant text excerpts from papers containing topical data relating to the common text inputed, allowing a single researcher or small team to gather targeted information and quickly and easily locate relevant text in the scientific papers to answer important questions about the new virus from a large corpus of documents.\n\n**APPROACH:** The current implementation uses Pandas built in search technology to search all paper abstracts for the keywords realting to topics where specific answers are desired.  Once the dataframe slice is returned, the abstracts are then parsed into sentence and word levels to understand which of the abstracts likley contain the most relevant answers to the keyword topics.\n\n**Enhancements:** 2020-03-23 - added NLTK stemming for search terms. 2020-03-24 - dropping duplicate rows from the dataframe\n\n**Enhancements: (COMING SOON)** The system currenlty requires some human thought regarding the crafting of keyword combinations to return the desired information from the tasks. We are working on updates to the system that will take in natrual languge questions and provide very detailed responses.  We are working on some word adjacencey, synonym and probablisitc scoring analysis so the system will be able learn from the corpus on its own how to ferret out relevant and responsive information despite vague or incomplete natural language inputs.\n\n**Pros:** Currently the system is a very simple **(as Einstein said \"make it as simple as possible, but no simpler\")**, but quite effective solutiuon to providing insight on topical queries.\n\n**Cons:** Currently the system requires some human understanding in crafting keyword combinations and the systems brute force approach may miss relevant documents because of its very specific approach to \"relevance\" of document and sentence content that requires the presence of all keywords.\n"}}