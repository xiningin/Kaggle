{"cell_type":{"e0b8cae1":"code","b642170d":"code","c33ffefb":"code","3a792305":"code","048e6b5d":"code","50744d37":"code","273f2c24":"code","db56283f":"code","95937e5a":"code","45eea65c":"code","605b8284":"code","5cc03ffb":"code","22f67878":"code","c308ccdc":"code","f870d3cd":"code","a7ed02c8":"code","d8803b18":"code","ac1e1552":"code","e17ac1a3":"code","2c47ca47":"code","93c35abd":"code","18e83e9c":"code","48eb83b3":"code","3d121b6b":"code","90846a08":"code","478c2032":"code","9377265a":"code","3de004fa":"code","5be8ba35":"code","81cfc001":"code","55ce65ec":"code","d69c5d1f":"code","f2318b25":"code","efc5c326":"code","94b0c599":"code","86d7b57e":"code","cc13ef27":"code","d8b96053":"code","9f875fd7":"code","e981137c":"code","ce85a329":"code","0651f6a9":"code","b5eb7a18":"code","d8b5009b":"code","54f89ef2":"code","6c71b669":"code","d1e725d1":"code","b4504928":"code","6a60c71c":"code","e79311da":"code","cf2f693c":"code","f2f1a753":"code","bd960e5d":"code","1e717491":"code","712b1564":"code","d18a591b":"code","e28910d8":"code","46fdb75c":"code","2f1c1c43":"code","e977ebc9":"code","82bc44a7":"code","b929ac35":"code","45face49":"code","25393052":"code","beb38a55":"code","eb6f4a37":"code","25cd1ad5":"code","b43eb3d9":"code","630f90f3":"code","02bd3b03":"code","1fa9089f":"code","fd29ce03":"code","229ec784":"code","50ffd869":"code","e5cf6714":"code","713d5f15":"code","396803a0":"code","d2b59053":"code","3ab1760f":"code","78ad0774":"code","47d3f8cd":"code","2feea92e":"code","830a2e65":"code","6785f37a":"code","696deaaf":"code","77bbb271":"code","6882c315":"code","fb7088e3":"code","c988ed1e":"code","2301ba1f":"code","9c290a45":"code","490633ba":"code","76e0ec6e":"code","ca50742b":"code","4543d3e7":"code","ba7fc248":"code","072a34d3":"code","36e23953":"code","91d5b1c0":"code","d2ee7ceb":"code","054071e5":"code","7d9bdc12":"code","1785e84a":"code","198f04ad":"code","37e7ccde":"code","f46fa405":"code","9f39c11b":"code","0fe7378b":"code","b02e7897":"code","7d514e65":"code","3b90fd5e":"code","15bf47b2":"code","12a732d3":"code","8be6f119":"code","1cc8a7d5":"code","9d65f055":"code","2278af73":"code","b654e879":"code","600ee155":"code","e5d23829":"code","183d544c":"code","c03c7d44":"code","a5d04b8c":"code","a39be8f4":"code","89bc6130":"code","c5875dc1":"code","331d398a":"code","a4898c79":"code","17c794d2":"code","cfd4db95":"code","9a280a33":"code","b7f1eab5":"code","61b23399":"code","17f37ba4":"code","7cd1a8f4":"code","45c29bd3":"code","c38e3899":"code","eae04bcf":"code","d2a466c0":"code","c2cf46dd":"code","107e0fed":"code","434ea06a":"code","32baa9f6":"code","38386931":"code","e5a9e2f4":"code","39ddf7a7":"code","c46430e5":"code","e25a9dd5":"code","9e435050":"code","4d65824a":"code","19f6a9c3":"code","e6e0bb29":"code","0655cc2a":"code","eab8ed3e":"code","7f825f0a":"code","c4c8cc14":"code","c44f33d8":"code","83a5c31c":"code","cbb29fc0":"code","9f90e3cb":"code","f6a04dd7":"code","260d3e78":"code","bd9b2679":"code","6339f6b2":"code","c6154a8a":"code","15ecd8be":"code","7f730c46":"code","cd1b0a5b":"code","f9ad0c01":"code","47d56559":"code","282303d0":"code","4062ba11":"code","a3f2e3f4":"code","d65ef85b":"code","978aecad":"code","92e5bf9d":"code","a224b3ca":"code","4621f126":"code","cd626711":"code","d073a83b":"code","dff17a52":"code","cee54b91":"code","c20931d0":"code","5ada2b0f":"code","bb54fb22":"code","2736442e":"code","79a9ad42":"code","6b206ec3":"code","f4531f5f":"code","4df2ddb1":"code","c28f9a00":"markdown","c4957dda":"markdown","9dd15277":"markdown","2e0dd2c4":"markdown","4d4ccf21":"markdown","7a26662f":"markdown","032cb006":"markdown","df169297":"markdown","913d622f":"markdown","73c7e69c":"markdown","d321cea4":"markdown","113f8f4e":"markdown","6533dd6f":"markdown","1ed3f3cc":"markdown","31eac205":"markdown","ace52cbb":"markdown","4ca8a423":"markdown","652ad7ee":"markdown","605b7269":"markdown","8305b14f":"markdown","f07b2f19":"markdown","7611a1c5":"markdown","98fbbb45":"markdown","600d248d":"markdown","a0304b0e":"markdown","7321d4a5":"markdown","f9f3b448":"markdown","073910b7":"markdown","94181ae0":"markdown","3aac313f":"markdown","9e7cf67d":"markdown","8986fe2f":"markdown","90591f3f":"markdown","5ab1b88f":"markdown","6dc5c11d":"markdown","64373c73":"markdown","bf074a31":"markdown","023b67cb":"markdown","ec889ca8":"markdown","9ddafe2d":"markdown","2f0a82ba":"markdown","e89f3b83":"markdown","87917531":"markdown","8278886b":"markdown","c0251b2c":"markdown","d5367486":"markdown","7d3f746d":"markdown","95edf7b6":"markdown","3977bed5":"markdown","a2dde94a":"markdown","d037503a":"markdown","067afbf1":"markdown","9d06dae7":"markdown","b5ac06ab":"markdown","b7c7bdae":"markdown","c2f4ece3":"markdown","7f10196f":"markdown","db28370d":"markdown","43daa286":"markdown","171f79df":"markdown","e28c807d":"markdown","352763de":"markdown","06129069":"markdown","abdb5d94":"markdown","e8546dd2":"markdown","c0b2b461":"markdown","075af991":"markdown","3ff8b4d1":"markdown","c329872f":"markdown","d2b3d6e2":"markdown","d6c3b5df":"markdown","f3d137d8":"markdown","3001e5d9":"markdown","b3109238":"markdown","f0a345f1":"markdown","7296799f":"markdown","69f5fc4f":"markdown","d3bcc82f":"markdown","c509514c":"markdown"},"source":{"e0b8cae1":"#Importing Libraries\n# please do go through this python notebook: \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport csv\nimport pandas as pd#pandas to create small dataframes \nimport datetime #Convert to unix time\nimport time #Convert to unix time\n# if numpy is not installed already : pip3 install numpy\nimport numpy as np#Do aritmetic operations on arrays\n# matplotlib: used to plot graphs\nimport matplotlib\nimport matplotlib.pylab as plt\nimport seaborn as sns#Plots\nfrom matplotlib import rcParams#Size of plots  \nfrom sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\nimport math\nimport pickle\nimport os\n# to install xgboost: pip3 install xgboost\nimport xgboost as xgb\n\nimport warnings\nimport networkx as nx\nimport pdb\nimport pickle\nfrom pandas import HDFStore,DataFrame\nfrom pandas import read_hdf\nfrom scipy.sparse.linalg import svds, eigs\nimport gc\nfrom tqdm import tqdm","b642170d":"#reading graph\nif not os.path.isfile('data\/after_eda\/train_woheader.csv'):\n    traincsv = pd.read_csv('data\/train.csv')\n    print(traincsv[traincsv.isna().any(1)])\n    print(traincsv.info())\n    print(\"Number of diplicate entries: \",sum(traincsv.duplicated()))\n    traincsv.to_csv('data\/after_eda\/train_woheader.csv',header=False,index=False)\n    print(\"saved the graph into file\")\nelse:\n    g=nx.read_edgelist('data\/after_eda\/train_woheader.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n    print(nx.info(g))","c33ffefb":"if not os.path.isfile('train_woheader_sample.csv'):\n    pd.read_csv('data\/train.csv', nrows=50).to_csv('train_woheader_sample.csv',header=False,index=False)\n    \nsubgraph=nx.read_edgelist('train_woheader_sample.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n# https:\/\/stackoverflow.com\/questions\/9402255\/drawing-a-huge-graph-with-networkx-and-matplotlib\n\npos=nx.spring_layout(subgraph)\nnx.draw(subgraph,pos,node_color='#A0CBE2',edge_color='#00bb5e',width=1,edge_cmap=plt.cm.Blues,with_labels=True)\nplt.savefig(\"graph_sample.pdf\")\nprint(nx.info(subgraph))","3a792305":"# No of Unique persons \nprint(\"The number of unique persons\",len(g.nodes()))","048e6b5d":"indegree_dist = list(dict(g.in_degree()).values())\nindegree_dist.sort()\nplt.figure(figsize=(10,6))\nplt.plot(indegree_dist)\nplt.xlabel('Index No')\nplt.ylabel('No Of Followers')\nplt.show()","50744d37":"indegree_dist = list(dict(g.in_degree()).values())\nindegree_dist.sort()\nplt.figure(figsize=(10,6))\nplt.plot(indegree_dist[0:1500000])\nplt.xlabel('Index No')\nplt.ylabel('No Of Followers')\nplt.show()","273f2c24":"plt.boxplot(indegree_dist)\nplt.ylabel('No Of Followers')\nplt.show()","db56283f":"### 90-100 percentile\nfor i in range(0,11):\n    print(90+i,'percentile value is',np.percentile(indegree_dist,90+i))","95937e5a":"### 99-100 percentile\nfor i in range(10,110,10):\n    print(99+(i\/100),'percentile value is',np.percentile(indegree_dist,99+(i\/100)))","45eea65c":"%matplotlib inline\nsns.set_style('ticks')\nfig, ax = plt.subplots()\nfig.set_size_inches(11.7, 8.27)\nsns.distplot(indegree_dist, color='#16A085')\nplt.xlabel('PDF of Indegree')\nsns.despine()\n#plt.show()","605b8284":"outdegree_dist = list(dict(g.out_degree()).values())\noutdegree_dist.sort()\nplt.figure(figsize=(10,6))\nplt.plot(outdegree_dist)\nplt.xlabel('Index No')\nplt.ylabel('No Of people each person is following')\nplt.show()","5cc03ffb":"indegree_dist = list(dict(g.in_degree()).values())\nindegree_dist.sort()\nplt.figure(figsize=(10,6))\nplt.plot(outdegree_dist[0:1500000])\nplt.xlabel('Index No')\nplt.ylabel('No Of people each person is following')\nplt.show()","22f67878":"plt.boxplot(indegree_dist)\nplt.ylabel('No Of people each person is following')\nplt.show()","c308ccdc":"### 90-100 percentile\nfor i in range(0,11):\n    print(90+i,'percentile value is',np.percentile(outdegree_dist,90+i))","f870d3cd":"### 99-100 percentile\nfor i in range(10,110,10):\n    print(99+(i\/100),'percentile value is',np.percentile(outdegree_dist,99+(i\/100)))","a7ed02c8":"sns.set_style('ticks')\nfig, ax = plt.subplots()\nfig.set_size_inches(11.7, 8.27)\nsns.distplot(outdegree_dist, color='#16A085')\nplt.xlabel('PDF of Outdegree')\nsns.despine()","d8803b18":"print('No of persons those are not following anyone are' ,sum(np.array(outdegree_dist)==0),'and % is',\n                                sum(np.array(outdegree_dist)==0)*100\/len(outdegree_dist) )","ac1e1552":"print('No of persons having zero followers are' ,sum(np.array(indegree_dist)==0),'and % is',\n                                sum(np.array(indegree_dist)==0)*100\/len(indegree_dist) )","e17ac1a3":"count=0\nfor i in g.nodes():\n    if len(list(g.predecessors(i)))==0 :\n        if len(list(g.successors(i)))==0:\n            count+=1\nprint('No of persons those are not not following anyone and also not having any followers are',count)","2c47ca47":"from collections import Counter\ndict_in = dict(g.in_degree())\ndict_out = dict(g.out_degree())\nd = Counter(dict_in) + Counter(dict_out)\nin_out_degree = np.array(list(d.values()))","93c35abd":"in_out_degree_sort = sorted(in_out_degree)\nplt.figure(figsize=(10,6))\nplt.plot(in_out_degree_sort)\nplt.xlabel('Index No')\nplt.ylabel('No Of people each person is following + followers')\nplt.show()","18e83e9c":"in_out_degree_sort = sorted(in_out_degree)\nplt.figure(figsize=(10,6))\nplt.plot(in_out_degree_sort[0:1500000])\nplt.xlabel('Index No')\nplt.ylabel('No Of people each person is following + followers')\nplt.show()","48eb83b3":"### 90-100 percentile\nfor i in range(0,11):\n    print(90+i,'percentile value is',np.percentile(in_out_degree_sort,90+i))","3d121b6b":"### 99-100 percentile\nfor i in range(10,110,10):\n    print(99+(i\/100),'percentile value is',np.percentile(in_out_degree_sort,99+(i\/100)))","90846a08":"print('Min of no of followers + following is',in_out_degree.min())\nprint(np.sum(in_out_degree==in_out_degree.min()),' persons having minimum no of followers + following')","478c2032":"print('Max of no of followers + following is',in_out_degree.max())\nprint(np.sum(in_out_degree==in_out_degree.max()),' persons having maximum no of followers + following')","9377265a":"print('No of persons having followers + following less than 10 are',np.sum(in_out_degree<10))","3de004fa":"print('No of weakly connected components',len(list(nx.weakly_connected_components(g))))\ncount=0\nfor i in list(nx.weakly_connected_components(g)):\n    if len(i)==2:\n        count+=1\nprint('weakly connected components wit 2 nodes',count)","5be8ba35":"%%time\n###generating bad edges from given graph\nimport random\nif not os.path.isfile('data\/after_eda\/missing_edges_final.p'):\n    #getting all set of edges\n    r = csv.reader(open('data\/after_eda\/train_woheader.csv','r'))\n    edges = dict()\n    for edge in r:\n        edges[(edge[0], edge[1])] = 1\n        \n        \n    missing_edges = set([])\n    while (len(missing_edges)<9437519):\n        a=random.randint(1, 1862220)\n        b=random.randint(1, 1862220)\n        tmp = edges.get((a,b),-1)\n        if tmp == -1 and a!=b:\n            try:\n                if nx.shortest_path_length(g,source=a,target=b) > 2: \n\n                    missing_edges.add((a,b))\n                else:\n                    continue  \n            except:  \n                    missing_edges.add((a,b))              \n        else:\n            continue\n    pickle.dump(missing_edges,open('data\/after_eda\/missing_edges_final.p','wb'))\nelse:\n    missing_edges = pickle.load(open('data\/after_eda\/missing_edges_final.p','rb'))","81cfc001":"len(missing_edges)","55ce65ec":"from sklearn.model_selection import train_test_split\nif (not os.path.isfile('data\/after_eda\/train_pos_after_eda.csv')) and (not os.path.isfile('data\/after_eda\/test_pos_after_eda.csv')):\n    #reading total data df\n    df_pos = pd.read_csv('data\/train.csv')\n    df_neg = pd.DataFrame(list(missing_edges), columns=['source_node', 'destination_node'])\n    \n    print(\"Number of nodes in the graph with edges\", df_pos.shape[0])\n    print(\"Number of nodes in the graph without edges\", df_neg.shape[0])\n    \n    #Trian test split \n    #Spiltted data into 80-20 \n    #positive links and negative links seperatly because we need positive training data only for creating graph \n    #and for feature generation\n    X_train_pos, X_test_pos, y_train_pos, y_test_pos  = train_test_split(df_pos,np.ones(len(df_pos)),test_size=0.2, random_state=9)\n    X_train_neg, X_test_neg, y_train_neg, y_test_neg  = train_test_split(df_neg,np.zeros(len(df_neg)),test_size=0.2, random_state=9)\n    \n    print('='*60)\n    print(\"Number of nodes in the train data graph with edges\", X_train_pos.shape[0],\"=\",y_train_pos.shape[0])\n    print(\"Number of nodes in the train data graph without edges\", X_train_neg.shape[0],\"=\", y_train_neg.shape[0])\n    print('='*60)\n    print(\"Number of nodes in the test data graph with edges\", X_test_pos.shape[0],\"=\",y_test_pos.shape[0])\n    print(\"Number of nodes in the test data graph without edges\", X_test_neg.shape[0],\"=\",y_test_neg.shape[0])\n\n    #removing header and saving\n    X_train_pos.to_csv('data\/after_eda\/train_pos_after_eda.csv',header=False, index=False)\n    X_test_pos.to_csv('data\/after_eda\/test_pos_after_eda.csv',header=False, index=False)\n    X_train_neg.to_csv('data\/after_eda\/train_neg_after_eda.csv',header=False, index=False)\n    X_test_neg.to_csv('data\/after_eda\/test_neg_after_eda.csv',header=False, index=False)\nelse:\n    #Graph from Traing data only \n    del missing_edges","d69c5d1f":"if (os.path.isfile('data\/after_eda\/train_pos_after_eda.csv')) and (os.path.isfile('data\/after_eda\/test_pos_after_eda.csv')):        \n    train_graph=nx.read_edgelist('data\/after_eda\/train_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n    test_graph=nx.read_edgelist('data\/after_eda\/test_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n    print(nx.info(train_graph))\n    print(nx.info(test_graph))\n\n    # finding the unique nodes in the both train and test graphs\n    train_nodes_pos = set(train_graph.nodes())\n    test_nodes_pos = set(test_graph.nodes())\n\n    trY_teY = len(train_nodes_pos.intersection(test_nodes_pos))\n    trY_teN = len(train_nodes_pos - test_nodes_pos)\n    teY_trN = len(test_nodes_pos - train_nodes_pos)\n\n    print('no of people common in train and test -- ',trY_teY)\n    print('no of people present in train but not present in test -- ',trY_teN)\n\n    print('no of people present in test but not present in train -- ',teY_trN)\n    print(' % of people not there in Train but exist in Test in total Test data are {} %'.format(teY_trN\/len(test_nodes_pos)*100))","f2318b25":"#final train and test data sets\nif (not os.path.isfile('data\/after_eda\/train_after_eda.csv')) and \\\n(not os.path.isfile('data\/after_eda\/test_after_eda.csv')) and \\\n(not os.path.isfile('data\/train_y.csv')) and \\\n(not os.path.isfile('data\/test_y.csv')) and \\\n(os.path.isfile('data\/after_eda\/train_pos_after_eda.csv')) and \\\n(os.path.isfile('data\/after_eda\/test_pos_after_eda.csv')) and \\\n(os.path.isfile('data\/after_eda\/train_neg_after_eda.csv')) and \\\n(os.path.isfile('data\/after_eda\/test_neg_after_eda.csv')):\n    \n    X_train_pos = pd.read_csv('data\/after_eda\/train_pos_after_eda.csv', names=['source_node', 'destination_node'])\n    X_test_pos = pd.read_csv('data\/after_eda\/test_pos_after_eda.csv', names=['source_node', 'destination_node'])\n    X_train_neg = pd.read_csv('data\/after_eda\/train_neg_after_eda.csv', names=['source_node', 'destination_node'])\n    X_test_neg = pd.read_csv('data\/after_eda\/test_neg_after_eda.csv', names=['source_node', 'destination_node'])\n\n    print('='*60)\n    print(\"Number of nodes in the train data graph with edges\", X_train_pos.shape[0])\n    print(\"Number of nodes in the train data graph without edges\", X_train_neg.shape[0])\n    print('='*60)\n    print(\"Number of nodes in the test data graph with edges\", X_test_pos.shape[0])\n    print(\"Number of nodes in the test data graph without edges\", X_test_neg.shape[0])\n\n    X_train = X_train_pos.append(X_train_neg,ignore_index=True)\n    y_train = np.concatenate((y_train_pos,y_train_neg))\n    X_test = X_test_pos.append(X_test_neg,ignore_index=True)\n    y_test = np.concatenate((y_test_pos,y_test_neg)) \n    \n    X_train.to_csv('data\/after_eda\/train_after_eda.csv',header=False,index=False)\n    X_test.to_csv('data\/after_eda\/test_after_eda.csv',header=False,index=False)\n    pd.DataFrame(y_train.astype(int)).to_csv('data\/train_y.csv',header=False,index=False)\n    pd.DataFrame(y_test.astype(int)).to_csv('data\/test_y.csv',header=False,index=False)","efc5c326":"print(\"Data points in train data\",X_train.shape)\nprint(\"Data points in test data\",X_test.shape)\nprint(\"Shape of traget variable in train\",y_train.shape)\nprint(\"Shape of traget variable in test\", y_test.shape)","94b0c599":"#Importing Libraries\n# please do go through this python notebook: \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport csv\nimport pandas as pd#pandas to create small dataframes \nimport datetime #Convert to unix time\nimport time #Convert to unix time\n# if numpy is not installed already : pip3 install numpy<p style=\"font-size:32px;text-align:center\"> <b>Social network Graph Link Prediction - Facebook Challenge<\/b> <\/p>\n\n#Importing Libraries\n# please do go through this python notebook: \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport csv\nimport pandas as pd#pandas to create small dataframes \nimport datetime #Convert to unix time\nimport time #Convert to unix time\n# if numpy is not installed already : pip3 install numpy\nimport numpy as np#Do aritmetic operations on arrays\n# matplotlib: used to plot graphs\nimport matplotlib\nimport matplotlib.pylab as plt\nimport seaborn as sns#Plots\nfrom matplotlib import rcParams#Size of plots  \nfrom sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\nimport math\nimport pickle\nimport os\n# to install xgboost: pip3 install xgboost\nimport xgboost as xgb\n\nimport warnings\nimport networkx as nx\nimport pdb\nimport pickle\nfrom pandas import HDFStore,DataFrame\nfrom pandas import read_hdf\nfrom scipy.sparse.linalg import svds, eigs\nimport gc\nfrom tqdm import tqdm\n\n# 1. Reading Data\n\nif os.path.isfile('data\/after_eda\/train_pos_after_eda.csv'):\n    train_graph=nx.read_edgelist('data\/after_eda\/train_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n    print(nx.info(train_graph))\nelse:\n    print(\"please run the FB_EDA.ipynb or download the files from drive\")\n\n# 2. Similarity measures\n\n## 2.1 Jaccard Distance:\nhttp:\/\/www.statisticshowto.com\/jaccard-index\/\n\n\\begin{equation}\nj = \\frac{|X\\cap Y|}{|X \\cup Y|} \n\\end{equation}\n\n#for followees\ndef jaccard_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (len(set(train_graph.successors(a)).union(set(train_graph.successors(b)))))\n    except:\n        return 0\n    return sim\n\n#one test case\nprint(jaccard_for_followees(273084,1505602))\n\n#node 1635354 not in graph \nprint(jaccard_for_followees(273084,1505602))\n\n#for followers\ndef jaccard_for_followers(a,b):\n    try:\n        if len(set(train_graph.predecessors(a))) == 0  | len(set(g.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                 (len(set(train_graph.predecessors(a)).union(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0\n\nprint(jaccard_for_followers(273084,470294))\n\n#node 1635354 not in graph \nprint(jaccard_for_followees(669354,1635354))\n\n## 2.2 Cosine distance\n\n\\begin{equation}\nCosineDistance = \\frac{|X\\cap Y|}{|X|\\cdot|Y|} \n\\end{equation}\n\n#for followees\ndef cosine_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (math.sqrt(len(set(train_graph.successors(a)))*len((set(train_graph.successors(b))))))\n        return sim\n    except:\n        return 0\n\nprint(cosine_for_followees(273084,1505602))\n\nprint(cosine_for_followees(273084,1635354))\n\ndef cosine_for_followers(a,b):\n    try:\n        \n        if len(set(train_graph.predecessors(a))) == 0  | len(set(train_graph.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                     (math.sqrt(len(set(train_graph.predecessors(a))))*(len(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0\n\nprint(cosine_for_followers(2,470294))\n\nprint(cosine_for_followers(669354,1635354))\n\n## 3. Ranking Measures\n\nhttps:\/\/networkx.github.io\/documentation\/networkx-1.10\/reference\/generated\/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html\n\nPageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links.\n\n<img src='PageRanks-Example.jpg'\/>\n\nMathematical PageRanks for a simple network, expressed as percentages. (Google uses a logarithmic scale.) Page C has a higher PageRank than Page E, even though there are fewer links to C; the one link to C comes from an important page and hence is of high value. If web surfers who start on a random page have an 85% likelihood of choosing a random link from the page they are currently visiting, and a 15% likelihood of jumping to a page chosen at random from the entire web, they will reach Page E 8.1% of the time. <b>(The 15% likelihood of jumping to an arbitrary page corresponds to a damping factor of 85%.) Without damping, all web surfers would eventually end up on Pages A, B, or C, and all other pages would have PageRank zero. In the presence of damping, Page A effectively links to all pages in the web, even though it has no outgoing links of its own.<\/b>\n\n## 3.1 Page Ranking\n\nhttps:\/\/en.wikipedia.org\/wiki\/PageRank\n\n\nif not os.path.isfile('data\/fea_sample\/page_rank.p'):\n    pr = nx.pagerank(train_graph, alpha=0.85)\n    pickle.dump(pr,open('data\/fea_sample\/page_rank.p','wb'))\nelse:\n    pr = pickle.load(open('data\/fea_sample\/page_rank.p','rb'))\n\nprint('min',pr[min(pr, key=pr.get)])\nprint('max',pr[max(pr, key=pr.get)])\nprint('mean',float(sum(pr.values())) \/ len(pr))\n\n#for imputing to nodes which are not there in Train data\nmean_pr = float(sum(pr.values())) \/ len(pr)\nprint(mean_pr)\n\n# 4. Other Graph Features\n\n## 4.1 Shortest path:\n\nGetting Shortest path between twoo nodes, if nodes have direct path i.e directly connected then we are removing that edge and calculating path. \n\n#if has direct edge then deleting that edge and calculating shortest path\ndef compute_shortest_path_length(a,b):\n    p=-1\n    try:\n        if train_graph.has_edge(a,b):\n            train_graph.remove_edge(a,b)\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n            train_graph.add_edge(a,b)\n        else:\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n        return p\n    except:\n        return -1\n\n#testing\ncompute_shortest_path_length(77697, 826021)\n\n#testing\ncompute_shortest_path_length(669354,1635354)\n\n## 4.2 Checking for same community\n\n#getting weekly connected edges from graph \nwcc=list(nx.weakly_connected_components(train_graph))\ndef belongs_to_same_wcc(a,b):\n    index = []\n    if train_graph.has_edge(b,a):\n        return 1\n    if train_graph.has_edge(a,b):\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if (b in index):\n                train_graph.remove_edge(a,b)\n                if compute_shortest_path_length(a,b)==-1:\n                    train_graph.add_edge(a,b)\n                    return 0\n                else:\n                    train_graph.add_edge(a,b)\n                    return 1\n            else:\n                return 0\n    else:\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if(b in index):\n                return 1\n            else:\n                return 0\n\nbelongs_to_same_wcc(861, 1659750)\n\nbelongs_to_same_wcc(669354,1635354)\n\n## 4.3 Adamic\/Adar Index:\nAdamic\/Adar measures is defined as inverted sum of degrees of common neighbours for given two vertices.\n$$A(x,y)=\\sum_{u \\in N(x) \\cap N(y)}\\frac{1}{log(|N(u)|)}$$\n\n#adar index\ndef calc_adar_in(a,b):\n    sum=0\n    try:\n        n=list(set(train_graph.successors(a)).intersection(set(train_graph.successors(b))))\n        if len(n)!=0:\n            for i in n:\n                sum=sum+(1\/np.log10(len(list(train_graph.predecessors(i)))))\n            return sum\n        else:\n            return 0\n    except:\n        return 0\n\ncalc_adar_in(1,189226)\n\ncalc_adar_in(669354,1635354)\n\n## 4.4 Is persion was following back:\n\ndef follows_back(a,b):\n    if train_graph.has_edge(b,a):\n        return 1\n    else:\n        return 0\n\nfollows_back(1,189226)\n\nfollows_back(669354,1635354)\n\n## 4.5 Katz Centrality:\nhttps:\/\/en.wikipedia.org\/wiki\/Katz_centrality\n\nhttps:\/\/www.geeksforgeeks.org\/katz-centrality-centrality-measure\/\n Katz centrality computes the centrality for a node \n    based on the centrality of its neighbors. It is a \n    generalization of the eigenvector centrality. The\n    Katz centrality for node `i` is\n \n$$x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta,$$\nwhere `A` is the adjacency matrix of the graph G \nwith eigenvalues $$\\lambda$$.\n\nThe parameter $$\\beta$$ controls the initial centrality and \n\n$$\\alpha < \\frac{1}{\\lambda_{max}}.$$\n\nif not os.path.isfile('data\/fea_sample\/katz.p'):\n    katz = nx.katz.katz_centrality(train_graph,alpha=0.005,beta=1)\n    pickle.dump(katz,open('data\/fea_sample\/katz.p','wb'))\nelse:\n    katz = pickle.load(open('data\/fea_sample\/katz.p','rb'))\n\nprint('min',katz[min(katz, key=katz.get)])\nprint('max',katz[max(katz, key=katz.get)])\nprint('mean',float(sum(katz.values())) \/ len(katz))\n\nmean_katz = float(sum(katz.values())) \/ len(katz)\nprint(mean_katz)\n\n## 4.6 Hits Score\nThe HITS algorithm computes two numbers for a node. Authorities estimates the node value based on the incoming links. Hubs estimates the node value based on outgoing links.\n\nhttps:\/\/en.wikipedia.org\/wiki\/HITS_algorithm\n\nif not os.path.isfile('data\/fea_sample\/hits.p'):\n    hits = nx.hits(train_graph, max_iter=100, tol=1e-08, nstart=None, normalized=True)\n    pickle.dump(hits,open('data\/fea_sample\/hits.p','wb'))\nelse:\n    hits = pickle.load(open('data\/fea_sample\/hits.p','rb'))\n\nprint('min',hits[0][min(hits[0], key=hits[0].get)])\nprint('max',hits[0][max(hits[0], key=hits[0].get)])\nprint('mean',float(sum(hits[0].values())) \/ len(hits[0]))\n\n# 5. Featurization\n\n## 5. 1 Reading a sample of Data from both train and test\n\nimport random\nif os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/train_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 15100030\n    # n_train = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_train =  15100028\n    s = 100000 #desired sample size\n    skip_train = sorted(random.sample(range(1,n_train+1),n_train-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039\n\nif os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/test_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 3775008\n    # n_test = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_test = 3775006\n    s = 50000 #desired sample size\n    skip_test = sorted(random.sample(range(1,n_test+1),n_test-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039\n\nprint(\"Number of rows in the train data file:\", n_train)\nprint(\"Number of rows we are going to elimiate in train data are\",len(skip_train))\nprint(\"Number of rows in the test data file:\", n_test)\nprint(\"Number of rows we are going to elimiate in test data are\",len(skip_test))\n\ndf_final_train = pd.read_csv('data\/after_eda\/train_after_eda.csv', skiprows=skip_train, names=['source_node', 'destination_node'])\ndf_final_train['indicator_link'] = pd.read_csv('data\/train_y.csv', skiprows=skip_train, names=['indicator_link'])\nprint(\"Our train matrix size \",df_final_train.shape)\ndf_final_train.head(2)\n\ndf_final_test = pd.read_csv('data\/after_eda\/test_after_eda.csv', skiprows=skip_test, names=['source_node', 'destination_node'])\ndf_final_test['indicator_link'] = pd.read_csv('data\/test_y.csv', skiprows=skip_test, names=['indicator_link'])\nprint(\"Our test matrix size \",df_final_test.shape)\ndf_final_test.head(2)\n\n## 5.2 Adding a set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>jaccard_followers<\/li>\n<li>jaccard_followees<\/li>\n<li>cosine_followers<\/li>\n<li>cosine_followees<\/li>\n<li>num_followers_s<\/li>\n<li>num_followees_s<\/li>\n<li>num_followers_d<\/li>\n<li>num_followees_d<\/li>\n<li>inter_followers<\/li>\n<li>inter_followees<\/li>\n<\/ol>\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    #mapping jaccrd followers to train and test data\n    df_final_train['jaccard_followers'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followers'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['jaccard_followees'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followees'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    \n\n        #mapping jaccrd followers to train and test data\n    df_final_train['cosine_followers'] = df_final_train.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followers'] = df_final_test.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['cosine_followees'] = df_final_train.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followees'] = df_final_test.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)\n\ndef compute_features_stage1(df_final):\n    #calculating no of followers followees for source and destination\n    #calculating intersection of followers and followees for source and destination\n    num_followers_s=[]\n    num_followees_s=[]\n    num_followers_d=[]\n    num_followees_d=[]\n    inter_followers=[]\n    inter_followees=[]\n    for i,row in df_final.iterrows():\n        try:\n            s1=set(train_graph.predecessors(row['source_node']))\n            s2=set(train_graph.successors(row['source_node']))\n        except:\n            s1 = set()\n            s2 = set()\n        try:\n            d1=set(train_graph.predecessors(row['destination_node']))\n            d2=set(train_graph.successors(row['destination_node']))\n        except:\n            d1 = set()\n            d2 = set()\n        num_followers_s.append(len(s1))\n        num_followees_s.append(len(s2))\n\n        num_followers_d.append(len(d1))\n        num_followees_d.append(len(d2))\n\n        inter_followers.append(len(s1.intersection(d1)))\n        inter_followees.append(len(s2.intersection(d2)))\n    \n    return num_followers_s, num_followers_d, num_followees_s, num_followees_d, inter_followers, inter_followees\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    df_final_train['num_followers_s'], df_final_train['num_followers_d'], \\\n    df_final_train['num_followees_s'], df_final_train['num_followees_d'], \\\n    df_final_train['inter_followers'], df_final_train['inter_followees']= compute_features_stage1(df_final_train)\n    \n    df_final_test['num_followers_s'], df_final_test['num_followers_d'], \\\n    df_final_test['num_followees_s'], df_final_test['num_followees_d'], \\\n    df_final_test['inter_followers'], df_final_test['inter_followees']= compute_features_stage1(df_final_test)\n    \n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage1.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'test_df',mode='r')\n\n## 5.3 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>adar index<\/li>\n<li>is following back<\/li>\n<li>belongs to same weakly connect components<\/li>\n<li>shortest path between source and destination<\/li>\n<\/ol>\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage2.h5'):\n    #mapping adar index on train\n    df_final_train['adar_index'] = df_final_train.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n    #mapping adar index on test\n    df_final_test['adar_index'] = df_final_test.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping followback or not on train\n    df_final_train['follows_back'] = df_final_train.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping followback or not on test\n    df_final_test['follows_back'] = df_final_test.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping same component of wcc or not on train\n    df_final_train['same_comp'] = df_final_train.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n\n    ##mapping same component of wcc or not on train\n    df_final_test['same_comp'] = df_final_test.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n    \n    #--------------------------------------------------------------------------------------------------------\n    #mapping shortest path on train \n    df_final_train['shortest_path'] = df_final_train.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n    #mapping shortest path on test\n    df_final_test['shortest_path'] = df_final_test.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage2.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'test_df',mode='r')\n\n## 5.4 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>Weight Features\n    <ul>\n        <li>weight of incoming edges<\/li>\n        <li>weight of outgoing edges<\/li>\n        <li>weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges * weight of outgoing edges<\/li>\n        <li>2*weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges + 2*weight of outgoing edges<\/li>\n    <\/ul>\n<\/li>\n<li>Page Ranking of source<\/li>\n<li>Page Ranking of dest<\/li>\n<li>katz of source<\/li>\n<li>katz of dest<\/li>\n<li>hubs of source<\/li>\n<li>hubs of dest<\/li>\n<li>authorities_s of source<\/li>\n<li>authorities_s of dest<\/li>\n<\/ol>\n\n#### Weight Features\n\nIn order to determine the similarity of nodes, an edge weight value was calculated between nodes. Edge weight decreases as the neighbor count goes up. Intuitively, consider one million people following a celebrity on a social network then chances are most of them never met each other or the celebrity. On the other hand, if a user has 30 contacts in his\/her social network, the chances are higher that many of them know each other. \n`credit` - Graph-based Features for Supervised Link Prediction\nWilliam Cukierski, Benjamin Hamner, Bo Yang\n\n\\begin{equation}\nW = \\frac{1}{\\sqrt{1+|X|}}\n\\end{equation}\n\nit is directed graph so calculated Weighted in and Weighted out differently\n\n#weight for source and destination of each link\nWeight_in = {}\nWeight_out = {}\nfor i in  tqdm(train_graph.nodes()):\n    s1=set(train_graph.predecessors(i))\n    w_in = 1.0\/(np.sqrt(1+len(s1)))\n    Weight_in[i]=w_in\n    \n    s2=set(train_graph.successors(i))\n    w_out = 1.0\/(np.sqrt(1+len(s2)))\n    Weight_out[i]=w_out\n    \n#for imputing with mean\nmean_weight_in = np.mean(list(Weight_in.values()))\nmean_weight_out = np.mean(list(Weight_out.values()))\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    #mapping to pandas train\n    df_final_train['weight_in'] = df_final_train.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_train['weight_out'] = df_final_train.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n    #mapping to pandas test\n    df_final_test['weight_in'] = df_final_test.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_test['weight_out'] = df_final_test.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n\n    #some features engineerings on the in and out weights\n    df_final_train['weight_f1'] = df_final_train.weight_in + df_final_train.weight_out\n    df_final_train['weight_f2'] = df_final_train.weight_in * df_final_train.weight_out\n    df_final_train['weight_f3'] = (2*df_final_train.weight_in + 1*df_final_train.weight_out)\n    df_final_train['weight_f4'] = (1*df_final_train.weight_in + 2*df_final_train.weight_out)\n\n    #some features engineerings on the in and out weights\n    df_final_test['weight_f1'] = df_final_test.weight_in + df_final_test.weight_out\n    df_final_test['weight_f2'] = df_final_test.weight_in * df_final_test.weight_out\n    df_final_test['weight_f3'] = (2*df_final_test.weight_in + 1*df_final_test.weight_out)\n    df_final_test['weight_f4'] = (1*df_final_test.weight_in + 2*df_final_test.weight_out)\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    \n    #page rank for source and destination in Train and Test\n    #if anything not there in train graph then adding mean page rank \n    df_final_train['page_rank_s'] = df_final_train.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_train['page_rank_d'] = df_final_train.destination_node.apply(lambda x:pr.get(x,mean_pr))\n\n    df_final_test['page_rank_s'] = df_final_test.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_test['page_rank_d'] = df_final_test.destination_node.apply(lambda x:pr.get(x,mean_pr))\n    #================================================================================\n\n    #Katz centrality score for source and destination in Train and test\n    #if anything not there in train graph then adding mean katz score\n    df_final_train['katz_s'] = df_final_train.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_train['katz_d'] = df_final_train.destination_node.apply(lambda x: katz.get(x,mean_katz))\n\n    df_final_test['katz_s'] = df_final_test.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_test['katz_d'] = df_final_test.destination_node.apply(lambda x: katz.get(x,mean_katz))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and test\n    #if anything not there in train graph then adding 0\n    df_final_train['hubs_s'] = df_final_train.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_train['hubs_d'] = df_final_train.destination_node.apply(lambda x: hits[0].get(x,0))\n\n    df_final_test['hubs_s'] = df_final_test.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_test['hubs_d'] = df_final_test.destination_node.apply(lambda x: hits[0].get(x,0))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and Test\n    #if anything not there in train graph then adding 0\n    df_final_train['authorities_s'] = df_final_train.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_train['authorities_d'] = df_final_train.destination_node.apply(lambda x: hits[1].get(x,0))\n\n    df_final_test['authorities_s'] = df_final_test.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_test['authorities_d'] = df_final_test.destination_node.apply(lambda x: hits[1].get(x,0))\n    #================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage3.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'test_df',mode='r')\n\n## 5.5 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>SVD features for both source and destination<\/li>\n<\/ol>\n\ndef svd(x, S):\n    try:\n        z = sadj_dict[x]\n        return S[z]\n    except:\n        return [0,0,0,0,0,0]\n\n#for svd features to get feature vector creating a dict node val and inedx in svd vector\nsadj_col = sorted(train_graph.nodes())\nsadj_dict = { val:idx for idx,val in enumerate(sadj_col)}\n\nAdj = nx.adjacency_matrix(train_graph,nodelist=sorted(train_graph.nodes())).asfptype()\n\nU, s, V = svds(Adj, k = 6)\nprint('Adjacency matrix Shape',Adj.shape)\nprint('U Shape',U.shape)\nprint('V Shape',V.shape)\nprint('s Shape',s.shape)\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage4.h5'):\n    #===================================================================================================\n    \n    df_final_train[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_train[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_train[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_train[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_test[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_test[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n\n    #===================================================================================================\n    \n    df_final_test[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_test[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage4.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\n\n# prepared and stored the data from machine learning models\n# pelase check the FB_Models.ipynb<p style=\"font-size:32px;text-align:center\"> <b>Social network Graph Link Prediction - Facebook Challenge<\/b> <\/p>\n\n#Importing Libraries\n# please do go through this python notebook: \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport csv\nimport pandas as pd#pandas to create small dataframes \nimport datetime #Convert to unix time\nimport time #Convert to unix time\n# if numpy is not installed already : pip3 install numpy\nimport numpy as np#Do aritmetic operations on arrays\n# matplotlib: used to plot graphs\nimport matplotlib\nimport matplotlib.pylab as plt\nimport seaborn as sns#Plots\nfrom matplotlib import rcParams#Size of plots  \nfrom sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\nimport math\nimport pickle\nimport os\n# to install xgboost: pip3 install xgboost\nimport xgboost as xgb\n\nimport warnings\nimport networkx as nx\nimport pdb\nimport pickle\nfrom pandas import HDFStore,DataFrame\nfrom pandas import read_hdf\nfrom scipy.sparse.linalg import svds, eigs\nimport gc\nfrom tqdm import tqdm\n\n# 1. Reading Data\n\nif os.path.isfile('data\/after_eda\/train_pos_after_eda.csv'):\n    train_graph=nx.read_edgelist('data\/after_eda\/train_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n    print(nx.info(train_graph))\nelse:\n    print(\"please run the FB_EDA.ipynb or download the files from drive\")\n\n# 2. Similarity measures\n\n## 2.1 Jaccard Distance:\nhttp:\/\/www.statisticshowto.com\/jaccard-index\/\n\n\\begin{equation}\nj = \\frac{|X\\cap Y|}{|X \\cup Y|} \n\\end{equation}\n\n#for followees\ndef jaccard_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (len(set(train_graph.successors(a)).union(set(train_graph.successors(b)))))\n    except:\n        return 0\n    return sim\n\n#one test case\nprint(jaccard_for_followees(273084,1505602))\n\n#node 1635354 not in graph \nprint(jaccard_for_followees(273084,1505602))\n\n#for followers\ndef jaccard_for_followers(a,b):\n    try:\n        if len(set(train_graph.predecessors(a))) == 0  | len(set(g.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                 (len(set(train_graph.predecessors(a)).union(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0\n\nprint(jaccard_for_followers(273084,470294))\n\n#node 1635354 not in graph \nprint(jaccard_for_followees(669354,1635354))\n\n## 2.2 Cosine distance\n\n\\begin{equation}\nCosineDistance = \\frac{|X\\cap Y|}{|X|\\cdot|Y|} \n\\end{equation}\n\n#for followees\ndef cosine_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (math.sqrt(len(set(train_graph.successors(a)))*len((set(train_graph.successors(b))))))\n        return sim\n    except:\n        return 0\n\nprint(cosine_for_followees(273084,1505602))\n\nprint(cosine_for_followees(273084,1635354))\n\ndef cosine_for_followers(a,b):\n    try:\n        \n        if len(set(train_graph.predecessors(a))) == 0  | len(set(train_graph.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                     (math.sqrt(len(set(train_graph.predecessors(a))))*(len(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0\n\nprint(cosine_for_followers(2,470294))\n\nprint(cosine_for_followers(669354,1635354))\n\n## 3. Ranking Measures\n\nhttps:\/\/networkx.github.io\/documentation\/networkx-1.10\/reference\/generated\/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html\n\nPageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links.\n\n<img src='PageRanks-Example.jpg'\/>\n\nMathematical PageRanks for a simple network, expressed as percentages. (Google uses a logarithmic scale.) Page C has a higher PageRank than Page E, even though there are fewer links to C; the one link to C comes from an important page and hence is of high value. If web surfers who start on a random page have an 85% likelihood of choosing a random link from the page they are currently visiting, and a 15% likelihood of jumping to a page chosen at random from the entire web, they will reach Page E 8.1% of the time. <b>(The 15% likelihood of jumping to an arbitrary page corresponds to a damping factor of 85%.) Without damping, all web surfers would eventually end up on Pages A, B, or C, and all other pages would have PageRank zero. In the presence of damping, Page A effectively links to all pages in the web, even though it has no outgoing links of its own.<\/b>\n\n## 3.1 Page Ranking\n\nhttps:\/\/en.wikipedia.org\/wiki\/PageRank\n\n\nif not os.path.isfile('data\/fea_sample\/page_rank.p'):\n    pr = nx.pagerank(train_graph, alpha=0.85)\n    pickle.dump(pr,open('data\/fea_sample\/page_rank.p','wb'))\nelse:\n    pr = pickle.load(open('data\/fea_sample\/page_rank.p','rb'))\n\nprint('min',pr[min(pr, key=pr.get)])\nprint('max',pr[max(pr, key=pr.get)])\nprint('mean',float(sum(pr.values())) \/ len(pr))\n\n#for imputing to nodes which are not there in Train data\nmean_pr = float(sum(pr.values())) \/ len(pr)\nprint(mean_pr)\n\n# 4. Other Graph Features\n\n## 4.1 Shortest path:\n\nGetting Shortest path between twoo nodes, if nodes have direct path i.e directly connected then we are removing that edge and calculating path. \n\n#if has direct edge then deleting that edge and calculating shortest path\ndef compute_shortest_path_length(a,b):\n    p=-1\n    try:\n        if train_graph.has_edge(a,b):\n            train_graph.remove_edge(a,b)\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n            train_graph.add_edge(a,b)\n        else:\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n        return p\n    except:\n        return -1\n\n#testing\ncompute_shortest_path_length(77697, 826021)\n\n#testing\ncompute_shortest_path_length(669354,1635354)\n\n## 4.2 Checking for same community\n\n#getting weekly connected edges from graph \nwcc=list(nx.weakly_connected_components(train_graph))\ndef belongs_to_same_wcc(a,b):\n    index = []\n    if train_graph.has_edge(b,a):\n        return 1\n    if train_graph.has_edge(a,b):\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if (b in index):\n                train_graph.remove_edge(a,b)\n                if compute_shortest_path_length(a,b)==-1:\n                    train_graph.add_edge(a,b)\n                    return 0\n                else:\n                    train_graph.add_edge(a,b)\n                    return 1\n            else:\n                return 0\n    else:\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if(b in index):\n                return 1\n            else:\n                return 0\n\nbelongs_to_same_wcc(861, 1659750)\n\nbelongs_to_same_wcc(669354,1635354)\n\n## 4.3 Adamic\/Adar Index:\nAdamic\/Adar measures is defined as inverted sum of degrees of common neighbours for given two vertices.\n$$A(x,y)=\\sum_{u \\in N(x) \\cap N(y)}\\frac{1}{log(|N(u)|)}$$\n\n#adar index\ndef calc_adar_in(a,b):\n    sum=0\n    try:\n        n=list(set(train_graph.successors(a)).intersection(set(train_graph.successors(b))))\n        if len(n)!=0:\n            for i in n:\n                sum=sum+(1\/np.log10(len(list(train_graph.predecessors(i)))))\n            return sum\n        else:\n            return 0\n    except:\n        return 0\n\ncalc_adar_in(1,189226)\n\ncalc_adar_in(669354,1635354)\n\n## 4.4 Is persion was following back:\n\ndef follows_back(a,b):\n    if train_graph.has_edge(b,a):\n        return 1\n    else:\n        return 0\n\nfollows_back(1,189226)\n\nfollows_back(669354,1635354)\n\n## 4.5 Katz Centrality:\nhttps:\/\/en.wikipedia.org\/wiki\/Katz_centrality\n\nhttps:\/\/www.geeksforgeeks.org\/katz-centrality-centrality-measure\/\n Katz centrality computes the centrality for a node \n    based on the centrality of its neighbors. It is a \n    generalization of the eigenvector centrality. The\n    Katz centrality for node `i` is\n \n$$x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta,$$\nwhere `A` is the adjacency matrix of the graph G \nwith eigenvalues $$\\lambda$$.\n\nThe parameter $$\\beta$$ controls the initial centrality and \n\n$$\\alpha < \\frac{1}{\\lambda_{max}}.$$\n\nif not os.path.isfile('data\/fea_sample\/katz.p'):\n    katz = nx.katz.katz_centrality(train_graph,alpha=0.005,beta=1)\n    pickle.dump(katz,open('data\/fea_sample\/katz.p','wb'))\nelse:\n    katz = pickle.load(open('data\/fea_sample\/katz.p','rb'))\n\nprint('min',katz[min(katz, key=katz.get)])\nprint('max',katz[max(katz, key=katz.get)])\nprint('mean',float(sum(katz.values())) \/ len(katz))\n\nmean_katz = float(sum(katz.values())) \/ len(katz)\nprint(mean_katz)\n\n## 4.6 Hits Score\nThe HITS algorithm computes two numbers for a node. Authorities estimates the node value based on the incoming links. Hubs estimates the node value based on outgoing links.\n\nhttps:\/\/en.wikipedia.org\/wiki\/HITS_algorithm\n\nif not os.path.isfile('data\/fea_sample\/hits.p'):\n    hits = nx.hits(train_graph, max_iter=100, tol=1e-08, nstart=None, normalized=True)\n    pickle.dump(hits,open('data\/fea_sample\/hits.p','wb'))\nelse:\n    hits = pickle.load(open('data\/fea_sample\/hits.p','rb'))\n\nprint('min',hits[0][min(hits[0], key=hits[0].get)])\nprint('max',hits[0][max(hits[0], key=hits[0].get)])\nprint('mean',float(sum(hits[0].values())) \/ len(hits[0]))\n\n# 5. Featurization\n\n## 5. 1 Reading a sample of Data from both train and test\n\nimport random\nif os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/train_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 15100030\n    # n_train = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_train =  15100028\n    s = 100000 #desired sample size\n    skip_train = sorted(random.sample(range(1,n_train+1),n_train-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039\n\nif os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/test_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 3775008\n    # n_test = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_test = 3775006\n    s = 50000 #desired sample size\n    skip_test = sorted(random.sample(range(1,n_test+1),n_test-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039\n\nprint(\"Number of rows in the train data file:\", n_train)\nprint(\"Number of rows we are going to elimiate in train data are\",len(skip_train))\nprint(\"Number of rows in the test data file:\", n_test)\nprint(\"Number of rows we are going to elimiate in test data are\",len(skip_test))\n\ndf_final_train = pd.read_csv('data\/after_eda\/train_after_eda.csv', skiprows=skip_train, names=['source_node', 'destination_node'])\ndf_final_train['indicator_link'] = pd.read_csv('data\/train_y.csv', skiprows=skip_train, names=['indicator_link'])\nprint(\"Our train matrix size \",df_final_train.shape)\ndf_final_train.head(2)\n\ndf_final_test = pd.read_csv('data\/after_eda\/test_after_eda.csv', skiprows=skip_test, names=['source_node', 'destination_node'])\ndf_final_test['indicator_link'] = pd.read_csv('data\/test_y.csv', skiprows=skip_test, names=['indicator_link'])\nprint(\"Our test matrix size \",df_final_test.shape)\ndf_final_test.head(2)\n\n## 5.2 Adding a set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>jaccard_followers<\/li>\n<li>jaccard_followees<\/li>\n<li>cosine_followers<\/li>\n<li>cosine_followees<\/li>\n<li>num_followers_s<\/li>\n<li>num_followees_s<\/li>\n<li>num_followers_d<\/li>\n<li>num_followees_d<\/li>\n<li>inter_followers<\/li>\n<li>inter_followees<\/li>\n<\/ol>\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    #mapping jaccrd followers to train and test data\n    df_final_train['jaccard_followers'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followers'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['jaccard_followees'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followees'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    \n\n        #mapping jaccrd followers to train and test data\n    df_final_train['cosine_followers'] = df_final_train.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followers'] = df_final_test.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['cosine_followees'] = df_final_train.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followees'] = df_final_test.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)\n\ndef compute_features_stage1(df_final):\n    #calculating no of followers followees for source and destination\n    #calculating intersection of followers and followees for source and destination\n    num_followers_s=[]\n    num_followees_s=[]\n    num_followers_d=[]\n    num_followees_d=[]\n    inter_followers=[]\n    inter_followees=[]\n    for i,row in df_final.iterrows():\n        try:\n            s1=set(train_graph.predecessors(row['source_node']))\n            s2=set(train_graph.successors(row['source_node']))\n        except:\n            s1 = set()\n            s2 = set()\n        try:\n            d1=set(train_graph.predecessors(row['destination_node']))\n            d2=set(train_graph.successors(row['destination_node']))\n        except:\n            d1 = set()\n            d2 = set()\n        num_followers_s.append(len(s1))\n        num_followees_s.append(len(s2))\n\n        num_followers_d.append(len(d1))\n        num_followees_d.append(len(d2))\n\n        inter_followers.append(len(s1.intersection(d1)))\n        inter_followees.append(len(s2.intersection(d2)))\n    \n    return num_followers_s, num_followers_d, num_followees_s, num_followees_d, inter_followers, inter_followees\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    df_final_train['num_followers_s'], df_final_train['num_followers_d'], \\\n    df_final_train['num_followees_s'], df_final_train['num_followees_d'], \\\n    df_final_train['inter_followers'], df_final_train['inter_followees']= compute_features_stage1(df_final_train)\n    \n    df_final_test['num_followers_s'], df_final_test['num_followers_d'], \\\n    df_final_test['num_followees_s'], df_final_test['num_followees_d'], \\\n    df_final_test['inter_followers'], df_final_test['inter_followees']= compute_features_stage1(df_final_test)\n    \n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage1.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'test_df',mode='r')\n\n## 5.3 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>adar index<\/li>\n<li>is following back<\/li>\n<li>belongs to same weakly connect components<\/li>\n<li>shortest path between source and destination<\/li>\n<\/ol>\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage2.h5'):\n    #mapping adar index on train\n    df_final_train['adar_index'] = df_final_train.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n    #mapping adar index on test\n    df_final_test['adar_index'] = df_final_test.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping followback or not on train\n    df_final_train['follows_back'] = df_final_train.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping followback or not on test\n    df_final_test['follows_back'] = df_final_test.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping same component of wcc or not on train\n    df_final_train['same_comp'] = df_final_train.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n\n    ##mapping same component of wcc or not on train\n    df_final_test['same_comp'] = df_final_test.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n    \n    #--------------------------------------------------------------------------------------------------------\n    #mapping shortest path on train \n    df_final_train['shortest_path'] = df_final_train.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n    #mapping shortest path on test\n    df_final_test['shortest_path'] = df_final_test.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage2.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'test_df',mode='r')\n\n## 5.4 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>Weight Features\n    <ul>\n        <li>weight of incoming edges<\/li>\n        <li>weight of outgoing edges<\/li>\n        <li>weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges * weight of outgoing edges<\/li>\n        <li>2*weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges + 2*weight of outgoing edges<\/li>\n    <\/ul>\n<\/li>\n<li>Page Ranking of source<\/li>\n<li>Page Ranking of dest<\/li>\n<li>katz of source<\/li>\n<li>katz of dest<\/li>\n<li>hubs of source<\/li>\n<li>hubs of dest<\/li>\n<li>authorities_s of source<\/li>\n<li>authorities_s of dest<\/li>\n<\/ol>\n\n#### Weight Features\n\nIn order to determine the similarity of nodes, an edge weight value was calculated between nodes. Edge weight decreases as the neighbor count goes up. Intuitively, consider one million people following a celebrity on a social network then chances are most of them never met each other or the celebrity. On the other hand, if a user has 30 contacts in his\/her social network, the chances are higher that many of them know each other. \n`credit` - Graph-based Features for Supervised Link Prediction\nWilliam Cukierski, Benjamin Hamner, Bo Yang\n\n\\begin{equation}\nW = \\frac{1}{\\sqrt{1+|X|}}\n\\end{equation}\n\nit is directed graph so calculated Weighted in and Weighted out differently\n\n#weight for source and destination of each link\nWeight_in = {}\nWeight_out = {}\nfor i in  tqdm(train_graph.nodes()):\n    s1=set(train_graph.predecessors(i))\n    w_in = 1.0\/(np.sqrt(1+len(s1)))\n    Weight_in[i]=w_in\n    \n    s2=set(train_graph.successors(i))\n    w_out = 1.0\/(np.sqrt(1+len(s2)))\n    Weight_out[i]=w_out\n    \n#for imputing with mean\nmean_weight_in = np.mean(list(Weight_in.values()))\nmean_weight_out = np.mean(list(Weight_out.values()))\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    #mapping to pandas train\n    df_final_train['weight_in'] = df_final_train.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_train['weight_out'] = df_final_train.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n    #mapping to pandas test\n    df_final_test['weight_in'] = df_final_test.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_test['weight_out'] = df_final_test.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n\n    #some features engineerings on the in and out weights\n    df_final_train['weight_f1'] = df_final_train.weight_in + df_final_train.weight_out\n    df_final_train['weight_f2'] = df_final_train.weight_in * df_final_train.weight_out\n    df_final_train['weight_f3'] = (2*df_final_train.weight_in + 1*df_final_train.weight_out)\n    df_final_train['weight_f4'] = (1*df_final_train.weight_in + 2*df_final_train.weight_out)\n\n    #some features engineerings on the in and out weights\n    df_final_test['weight_f1'] = df_final_test.weight_in + df_final_test.weight_out\n    df_final_test['weight_f2'] = df_final_test.weight_in * df_final_test.weight_out\n    df_final_test['weight_f3'] = (2*df_final_test.weight_in + 1*df_final_test.weight_out)\n    df_final_test['weight_f4'] = (1*df_final_test.weight_in + 2*df_final_test.weight_out)\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    \n    #page rank for source and destination in Train and Test\n    #if anything not there in train graph then adding mean page rank \n    df_final_train['page_rank_s'] = df_final_train.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_train['page_rank_d'] = df_final_train.destination_node.apply(lambda x:pr.get(x,mean_pr))\n\n    df_final_test['page_rank_s'] = df_final_test.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_test['page_rank_d'] = df_final_test.destination_node.apply(lambda x:pr.get(x,mean_pr))\n    #================================================================================\n\n    #Katz centrality score for source and destination in Train and test\n    #if anything not there in train graph then adding mean katz score\n    df_final_train['katz_s'] = df_final_train.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_train['katz_d'] = df_final_train.destination_node.apply(lambda x: katz.get(x,mean_katz))\n\n    df_final_test['katz_s'] = df_final_test.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_test['katz_d'] = df_final_test.destination_node.apply(lambda x: katz.get(x,mean_katz))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and test\n    #if anything not there in train graph then adding 0\n    df_final_train['hubs_s'] = df_final_train.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_train['hubs_d'] = df_final_train.destination_node.apply(lambda x: hits[0].get(x,0))\n\n    df_final_test['hubs_s'] = df_final_test.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_test['hubs_d'] = df_final_test.destination_node.apply(lambda x: hits[0].get(x,0))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and Test\n    #if anything not there in train graph then adding 0\n    df_final_train['authorities_s'] = df_final_train.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_train['authorities_d'] = df_final_train.destination_node.apply(lambda x: hits[1].get(x,0))\n\n    df_final_test['authorities_s'] = df_final_test.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_test['authorities_d'] = df_final_test.destination_node.apply(lambda x: hits[1].get(x,0))\n    #================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage3.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'test_df',mode='r')\n\n## 5.5 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>SVD features for both source and destination<\/li>\n<\/ol>\n\ndef svd(x, S):\n    try:\n        z = sadj_dict[x]\n        return S[z]\n    except:\n        return [0,0,0,0,0,0]\n\n#for svd features to get feature vector creating a dict node val and inedx in svd vector\nsadj_col = sorted(train_graph.nodes())\nsadj_dict = { val:idx for idx,val in enumerate(sadj_col)}\n\nAdj = nx.adjacency_matrix(train_graph,nodelist=sorted(train_graph.nodes())).asfptype()\n\nU, s, V = svds(Adj, k = 6)\nprint('Adjacency matrix Shape',Adj.shape)\nprint('U Shape',U.shape)\nprint('V Shape',V.shape)\nprint('s Shape',s.shape)\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage4.h5'):\n    #===================================================================================================\n    \n    df_final_train[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_train[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_train[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_train[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_test[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_test[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n\n    #===================================================================================================\n    \n    df_final_test[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_test[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage4.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\n\n# prepared and stored the data from machine learning models\n# pelase check the FB_Models.ipynb\nimport numpy as np#Do aritmetic operations on arrays\n# matplotlib: used to plot graphs\nimport matplotlib\nimport matplotlib.pylab as plt\nimport seaborn as sns#Plots\nfrom matplotlib import rcParams#Size of plots  \nfrom sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\nimport math\nimport pickle\nimport os\n# to install xgboost: pip3 install xgboost\nimport xgboost as xgb\n\nimport warnings\nimport networkx as nx\nimport pdb\nimport pickle\nfrom pandas import HDFStore,DataFrame\nfrom pandas import read_hdf\nfrom scipy.sparse.linalg import svds, eigs\nimport gc\nfrom tqdm import tqdm","86d7b57e":"if os.path.isfile('data\/after_eda\/train_pos_after_eda.csv'):\n    train_graph=nx.read_edgelist('data\/after_eda\/train_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n    print(nx.info(train_graph))\nelse:\n    print(\"please run the FB_EDA.ipynb or download the files from drive\")","cc13ef27":"#for followees\ndef jaccard_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (len(set(train_graph.successors(a)).union(set(train_graph.successors(b)))))\n    except:\n        return 0\n    return sim","d8b96053":"#one test case\nprint(jaccard_for_followees(273084,1505602))","9f875fd7":"#node 1635354 not in graph \nprint(jaccard_for_followees(273084,1505602))","e981137c":"#for followers\ndef jaccard_for_followers(a,b):\n    try:\n        if len(set(train_graph.predecessors(a))) == 0  | len(set(g.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                 (len(set(train_graph.predecessors(a)).union(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0","ce85a329":"print(jaccard_for_followers(273084,470294))","0651f6a9":"#node 1635354 not in graph \nprint(jaccard_for_followees(669354,1635354))","b5eb7a18":"#for followees\ndef cosine_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (math.sqrt(len(set(train_graph.successors(a)))*len((set(train_graph.successors(b))))))\n        return sim\n    except:\n        return 0","d8b5009b":"print(cosine_for_followees(273084,1505602))","54f89ef2":"print(cosine_for_followees(273084,1635354))","6c71b669":"def cosine_for_followers(a,b):\n    try:\n        \n        if len(set(train_graph.predecessors(a))) == 0  | len(set(train_graph.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                     (math.sqrt(len(set(train_graph.predecessors(a))))*(len(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0","d1e725d1":"print(cosine_for_followers(2,470294))","b4504928":"print(cosine_for_followers(669354,1635354))","6a60c71c":"if not os.path.isfile('data\/fea_sample\/page_rank.p'):\n    pr = nx.pagerank(train_graph, alpha=0.85)\n    pickle.dump(pr,open('data\/fea_sample\/page_rank.p','wb'))\nelse:\n    pr = pickle.load(open('data\/fea_sample\/page_rank.p','rb'))","e79311da":"print('min',pr[min(pr, key=pr.get)])\nprint('max',pr[max(pr, key=pr.get)])\nprint('mean',float(sum(pr.values())) \/ len(pr))","cf2f693c":"#for imputing to nodes which are not there in Train data\nmean_pr = float(sum(pr.values())) \/ len(pr)\nprint(mean_pr)","f2f1a753":"#if has direct edge then deleting that edge and calculating shortest path\ndef compute_shortest_path_length(a,b):\n    p=-1\n    try:\n        if train_graph.has_edge(a,b):\n            train_graph.remove_edge(a,b)\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n            train_graph.add_edge(a,b)\n        else:\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n        return p\n    except:\n        return -1","bd960e5d":"#testing\ncompute_shortest_path_length(77697, 826021)","1e717491":"#testing\ncompute_shortest_path_length(669354,1635354)","712b1564":"#getting weekly connected edges from graph \nwcc=list(nx.weakly_connected_components(train_graph))\ndef belongs_to_same_wcc(a,b):\n    index = []\n    if train_graph.has_edge(b,a):\n        return 1\n    if train_graph.has_edge(a,b):\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if (b in index):\n                train_graph.remove_edge(a,b)\n                if compute_shortest_path_length(a,b)==-1:\n                    train_graph.add_edge(a,b)\n                    return 0\n                else:\n                    train_graph.add_edge(a,b)\n                    return 1\n            else:\n                return 0\n    else:\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if(b in index):\n                return 1\n            else:\n                return 0","d18a591b":"belongs_to_same_wcc(861, 1659750)","e28910d8":"belongs_to_same_wcc(669354,1635354)","46fdb75c":"#adar index\ndef calc_adar_in(a,b):\n    sum=0\n    try:\n        n=list(set(train_graph.successors(a)).intersection(set(train_graph.successors(b))))\n        if len(n)!=0:\n            for i in n:\n                sum=sum+(1\/np.log10(len(list(train_graph.predecessors(i)))))\n            return sum\n        else:\n            return 0\n    except:\n        return 0","2f1c1c43":"calc_adar_in(1,189226)","e977ebc9":"calc_adar_in(669354,1635354)","82bc44a7":"def follows_back(a,b):\n    if train_graph.has_edge(b,a):\n        return 1\n    else:\n        return 0","b929ac35":"follows_back(1,189226)","45face49":"follows_back(669354,1635354)","25393052":"if not os.path.isfile('data\/fea_sample\/katz.p'):\n    katz = nx.katz.katz_centrality(train_graph,alpha=0.005,beta=1)\n    pickle.dump(katz,open('data\/fea_sample\/katz.p','wb'))\nelse:\n    katz = pickle.load(open('data\/fea_sample\/katz.p','rb'))","beb38a55":"print('min',katz[min(katz, key=katz.get)])\nprint('max',katz[max(katz, key=katz.get)])\nprint('mean',float(sum(katz.values())) \/ len(katz))","eb6f4a37":"mean_katz = float(sum(katz.values())) \/ len(katz)\nprint(mean_katz)","25cd1ad5":"if not os.path.isfile('data\/fea_sample\/hits.p'):\n    hits = nx.hits(train_graph, max_iter=100, tol=1e-08, nstart=None, normalized=True)\n    pickle.dump(hits,open('data\/fea_sample\/hits.p','wb'))\nelse:\n    hits = pickle.load(open('data\/fea_sample\/hits.p','rb'))","b43eb3d9":"print('min',hits[0][min(hits[0], key=hits[0].get)])\nprint('max',hits[0][max(hits[0], key=hits[0].get)])\nprint('mean',float(sum(hits[0].values())) \/ len(hits[0]))","630f90f3":"import random\nif os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/train_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 15100030\n    # n_train = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_train =  15100028\n    s = 100000 #desired sample size\n    skip_train = sorted(random.sample(range(1,n_train+1),n_train-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039","02bd3b03":"if os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/test_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 3775008\n    # n_test = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_test = 3775006\n    s = 50000 #desired sample size\n    skip_test = sorted(random.sample(range(1,n_test+1),n_test-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039","1fa9089f":"print(\"Number of rows in the train data file:\", n_train)\nprint(\"Number of rows we are going to elimiate in train data are\",len(skip_train))\nprint(\"Number of rows in the test data file:\", n_test)\nprint(\"Number of rows we are going to elimiate in test data are\",len(skip_test))","fd29ce03":"df_final_train = pd.read_csv('data\/after_eda\/train_after_eda.csv', skiprows=skip_train, names=['source_node', 'destination_node'])\ndf_final_train['indicator_link'] = pd.read_csv('data\/train_y.csv', skiprows=skip_train, names=['indicator_link'])\nprint(\"Our train matrix size \",df_final_train.shape)\ndf_final_train.head(2)","229ec784":"df_final_test = pd.read_csv('data\/after_eda\/test_after_eda.csv', skiprows=skip_test, names=['source_node', 'destination_node'])\ndf_final_test['indicator_link'] = pd.read_csv('data\/test_y.csv', skiprows=skip_test, names=['indicator_link'])\nprint(\"Our test matrix size \",df_final_test.shape)\ndf_final_test.head(2)","50ffd869":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    #mapping jaccrd followers to train and test data\n    df_final_train['jaccard_followers'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followers'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['jaccard_followees'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followees'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    \n\n        #mapping jaccrd followers to train and test data\n    df_final_train['cosine_followers'] = df_final_train.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followers'] = df_final_test.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['cosine_followees'] = df_final_train.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followees'] = df_final_test.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)","e5cf6714":"def compute_features_stage1(df_final):\n    #calculating no of followers followees for source and destination\n    #calculating intersection of followers and followees for source and destination\n    num_followers_s=[]\n    num_followees_s=[]\n    num_followers_d=[]\n    num_followees_d=[]\n    inter_followers=[]\n    inter_followees=[]\n    for i,row in df_final.iterrows():\n        try:\n            s1=set(train_graph.predecessors(row['source_node']))\n            s2=set(train_graph.successors(row['source_node']))\n        except:\n            s1 = set()\n            s2 = set()\n        try:\n            d1=set(train_graph.predecessors(row['destination_node']))\n            d2=set(train_graph.successors(row['destination_node']))\n        except:\n            d1 = set()\n            d2 = set()\n        num_followers_s.append(len(s1))\n        num_followees_s.append(len(s2))\n\n        num_followers_d.append(len(d1))\n        num_followees_d.append(len(d2))\n\n        inter_followers.append(len(s1.intersection(d1)))\n        inter_followees.append(len(s2.intersection(d2)))\n    \n    return num_followers_s, num_followers_d, num_followees_s, num_followees_d, inter_followers, inter_followees","713d5f15":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    df_final_train['num_followers_s'], df_final_train['num_followers_d'], \\\n    df_final_train['num_followees_s'], df_final_train['num_followees_d'], \\\n    df_final_train['inter_followers'], df_final_train['inter_followees']= compute_features_stage1(df_final_train)\n    \n    df_final_test['num_followers_s'], df_final_test['num_followers_d'], \\\n    df_final_test['num_followees_s'], df_final_test['num_followees_d'], \\\n    df_final_test['inter_followers'], df_final_test['inter_followees']= compute_features_stage1(df_final_test)\n    \n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage1.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'test_df',mode='r')","396803a0":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage2.h5'):\n    #mapping adar index on train\n    df_final_train['adar_index'] = df_final_train.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n    #mapping adar index on test\n    df_final_test['adar_index'] = df_final_test.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping followback or not on train\n    df_final_train['follows_back'] = df_final_train.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping followback or not on test\n    df_final_test['follows_back'] = df_final_test.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping same component of wcc or not on train\n    df_final_train['same_comp'] = df_final_train.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n\n    ##mapping same component of wcc or not on train\n    df_final_test['same_comp'] = df_final_test.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n    \n    #--------------------------------------------------------------------------------------------------------\n    #mapping shortest path on train \n    df_final_train['shortest_path'] = df_final_train.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n    #mapping shortest path on test\n    df_final_test['shortest_path'] = df_final_test.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage2.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'test_df',mode='r')","d2b59053":"#weight for source and destination of each link\nWeight_in = {}\nWeight_out = {}\nfor i in  tqdm(train_graph.nodes()):\n    s1=set(train_graph.predecessors(i))\n    w_in = 1.0\/(np.sqrt(1+len(s1)))\n    Weight_in[i]=w_in\n    \n    s2=set(train_graph.successors(i))\n    w_out = 1.0\/(np.sqrt(1+len(s2)))\n    Weight_out[i]=w_out\n    \n#for imputing with mean\nmean_weight_in = np.mean(list(Weight_in.values()))\nmean_weight_out = np.mean(list(Weight_out.values()))","3ab1760f":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    #mapping to pandas train\n    df_final_train['weight_in'] = df_final_train.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_train['weight_out'] = df_final_train.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n    #mapping to pandas test\n    df_final_test['weight_in'] = df_final_test.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_test['weight_out'] = df_final_test.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n\n    #some features engineerings on the in and out weights\n    df_final_train['weight_f1'] = df_final_train.weight_in + df_final_train.weight_out\n    df_final_train['weight_f2'] = df_final_train.weight_in * df_final_train.weight_out\n    df_final_train['weight_f3'] = (2*df_final_train.weight_in + 1*df_final_train.weight_out)\n    df_final_train['weight_f4'] = (1*df_final_train.weight_in + 2*df_final_train.weight_out)\n\n    #some features engineerings on the in and out weights\n    df_final_test['weight_f1'] = df_final_test.weight_in + df_final_test.weight_out\n    df_final_test['weight_f2'] = df_final_test.weight_in * df_final_test.weight_out\n    df_final_test['weight_f3'] = (2*df_final_test.weight_in + 1*df_final_test.weight_out)\n    df_final_test['weight_f4'] = (1*df_final_test.weight_in + 2*df_final_test.weight_out)","78ad0774":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    \n    #page rank for source and destination in Train and Test\n    #if anything not there in train graph then adding mean page rank \n    df_final_train['page_rank_s'] = df_final_train.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_train['page_rank_d'] = df_final_train.destination_node.apply(lambda x:pr.get(x,mean_pr))\n\n    df_final_test['page_rank_s'] = df_final_test.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_test['page_rank_d'] = df_final_test.destination_node.apply(lambda x:pr.get(x,mean_pr))\n    #================================================================================\n\n    #Katz centrality score for source and destination in Train and test\n    #if anything not there in train graph then adding mean katz score\n    df_final_train['katz_s'] = df_final_train.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_train['katz_d'] = df_final_train.destination_node.apply(lambda x: katz.get(x,mean_katz))\n\n    df_final_test['katz_s'] = df_final_test.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_test['katz_d'] = df_final_test.destination_node.apply(lambda x: katz.get(x,mean_katz))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and test\n    #if anything not there in train graph then adding 0\n    df_final_train['hubs_s'] = df_final_train.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_train['hubs_d'] = df_final_train.destination_node.apply(lambda x: hits[0].get(x,0))\n\n    df_final_test['hubs_s'] = df_final_test.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_test['hubs_d'] = df_final_test.destination_node.apply(lambda x: hits[0].get(x,0))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and Test\n    #if anything not there in train graph then adding 0\n    df_final_train['authorities_s'] = df_final_train.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_train['authorities_d'] = df_final_train.destination_node.apply(lambda x: hits[1].get(x,0))\n\n    df_final_test['authorities_s'] = df_final_test.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_test['authorities_d'] = df_final_test.destination_node.apply(lambda x: hits[1].get(x,0))\n    #================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage3.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'test_df',mode='r')","47d3f8cd":"def svd(x, S):\n    try:\n        z = sadj_dict[x]\n        return S[z]\n    except:\n        return [0,0,0,0,0,0]","2feea92e":"#for svd features to get feature vector creating a dict node val and inedx in svd vector\nsadj_col = sorted(train_graph.nodes())\nsadj_dict = { val:idx for idx,val in enumerate(sadj_col)}","830a2e65":"Adj = nx.adjacency_matrix(train_graph,nodelist=sorted(train_graph.nodes())).asfptype()","6785f37a":"U, s, V = svds(Adj, k = 6)\nprint('Adjacency matrix Shape',Adj.shape)\nprint('U Shape',U.shape)\nprint('V Shape',V.shape)\nprint('s Shape',s.shape)","696deaaf":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage4.h5'):\n    #===================================================================================================\n    \n    df_final_train[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_train[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_train[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_train[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_test[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_test[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n\n    #===================================================================================================\n    \n    df_final_test[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_test[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage4.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()","77bbb271":"<p style=\"font-size:32px;text-align:center\"> <b>Social network Graph Link Prediction - Facebook Challenge<\/b> <\/p>\n\n#Importing Libraries\n# please do go through this python notebook: \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport csv\nimport pandas as pd#pandas to create small dataframes \nimport datetime #Convert to unix time\nimport time #Convert to unix time\n# if numpy is not installed already : pip3 install numpy\nimport numpy as np#Do aritmetic operations on arrays\n# matplotlib: used to plot graphs\nimport matplotlib\nimport matplotlib.pylab as plt\nimport seaborn as sns#Plots\nfrom matplotlib import rcParams#Size of plots  \nfrom sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\nimport math\nimport pickle\nimport os\n# to install xgboost: pip3 install xgboost\nimport xgboost as xgb\n\nimport warnings\nimport networkx as nx\nimport pdb\nimport pickle\nfrom pandas import HDFStore,DataFrame\nfrom pandas import read_hdf\nfrom scipy.sparse.linalg import svds, eigs\nimport gc\nfrom tqdm import tqdm\n\n# 1. Reading Data\n\nif os.path.isfile('data\/after_eda\/train_pos_after_eda.csv'):\n    train_graph=nx.read_edgelist('data\/after_eda\/train_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n    print(nx.info(train_graph))\nelse:\n    print(\"please run the FB_EDA.ipynb or download the files from drive\")\n\n# 2. Similarity measures\n\n## 2.1 Jaccard Distance:\nhttp:\/\/www.statisticshowto.com\/jaccard-index\/\n\n\\begin{equation}\nj = \\frac{|X\\cap Y|}{|X \\cup Y|} \n\\end{equation}\n\n#for followees\ndef jaccard_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (len(set(train_graph.successors(a)).union(set(train_graph.successors(b)))))\n    except:\n        return 0\n    return sim\n\n#one test case\nprint(jaccard_for_followees(273084,1505602))\n\n#node 1635354 not in graph \nprint(jaccard_for_followees(273084,1505602))\n\n#for followers\ndef jaccard_for_followers(a,b):\n    try:\n        if len(set(train_graph.predecessors(a))) == 0  | len(set(g.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                 (len(set(train_graph.predecessors(a)).union(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0\n\nprint(jaccard_for_followers(273084,470294))\n\n#node 1635354 not in graph \nprint(jaccard_for_followees(669354,1635354))\n\n## 2.2 Cosine distance\n\n\\begin{equation}\nCosineDistance = \\frac{|X\\cap Y|}{|X|\\cdot|Y|} \n\\end{equation}\n\n#for followees\ndef cosine_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (math.sqrt(len(set(train_graph.successors(a)))*len((set(train_graph.successors(b))))))\n        return sim\n    except:\n        return 0\n\nprint(cosine_for_followees(273084,1505602))\n\nprint(cosine_for_followees(273084,1635354))\n\ndef cosine_for_followers(a,b):\n    try:\n        \n        if len(set(train_graph.predecessors(a))) == 0  | len(set(train_graph.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                     (math.sqrt(len(set(train_graph.predecessors(a))))*(len(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0\n\nprint(cosine_for_followers(2,470294))\n\nprint(cosine_for_followers(669354,1635354))\n\n## 3. Ranking Measures\n\nhttps:\/\/networkx.github.io\/documentation\/networkx-1.10\/reference\/generated\/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html\n\nPageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links.\n\n<img src='PageRanks-Example.jpg'\/>\n\nMathematical PageRanks for a simple network, expressed as percentages. (Google uses a logarithmic scale.) Page C has a higher PageRank than Page E, even though there are fewer links to C; the one link to C comes from an important page and hence is of high value. If web surfers who start on a random page have an 85% likelihood of choosing a random link from the page they are currently visiting, and a 15% likelihood of jumping to a page chosen at random from the entire web, they will reach Page E 8.1% of the time. <b>(The 15% likelihood of jumping to an arbitrary page corresponds to a damping factor of 85%.) Without damping, all web surfers would eventually end up on Pages A, B, or C, and all other pages would have PageRank zero. In the presence of damping, Page A effectively links to all pages in the web, even though it has no outgoing links of its own.<\/b>\n\n## 3.1 Page Ranking\n\nhttps:\/\/en.wikipedia.org\/wiki\/PageRank\n\n\nif not os.path.isfile('data\/fea_sample\/page_rank.p'):\n    pr = nx.pagerank(train_graph, alpha=0.85)\n    pickle.dump(pr,open('data\/fea_sample\/page_rank.p','wb'))\nelse:\n    pr = pickle.load(open('data\/fea_sample\/page_rank.p','rb'))\n\nprint('min',pr[min(pr, key=pr.get)])\nprint('max',pr[max(pr, key=pr.get)])\nprint('mean',float(sum(pr.values())) \/ len(pr))\n\n#for imputing to nodes which are not there in Train data\nmean_pr = float(sum(pr.values())) \/ len(pr)\nprint(mean_pr)\n\n# 4. Other Graph Features\n\n## 4.1 Shortest path:\n\nGetting Shortest path between twoo nodes, if nodes have direct path i.e directly connected then we are removing that edge and calculating path. \n\n#if has direct edge then deleting that edge and calculating shortest path\ndef compute_shortest_path_length(a,b):\n    p=-1\n    try:\n        if train_graph.has_edge(a,b):\n            train_graph.remove_edge(a,b)\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n            train_graph.add_edge(a,b)\n        else:\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n        return p\n    except:\n        return -1\n\n#testing\ncompute_shortest_path_length(77697, 826021)\n\n#testing\ncompute_shortest_path_length(669354,1635354)\n\n## 4.2 Checking for same community\n\n#getting weekly connected edges from graph \nwcc=list(nx.weakly_connected_components(train_graph))\ndef belongs_to_same_wcc(a,b):\n    index = []\n    if train_graph.has_edge(b,a):\n        return 1\n    if train_graph.has_edge(a,b):\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if (b in index):\n                train_graph.remove_edge(a,b)\n                if compute_shortest_path_length(a,b)==-1:\n                    train_graph.add_edge(a,b)\n                    return 0\n                else:\n                    train_graph.add_edge(a,b)\n                    return 1\n            else:\n                return 0\n    else:\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if(b in index):\n                return 1\n            else:\n                return 0\n\nbelongs_to_same_wcc(861, 1659750)\n\nbelongs_to_same_wcc(669354,1635354)\n\n## 4.3 Adamic\/Adar Index:\nAdamic\/Adar measures is defined as inverted sum of degrees of common neighbours for given two vertices.\n$$A(x,y)=\\sum_{u \\in N(x) \\cap N(y)}\\frac{1}{log(|N(u)|)}$$\n\n#adar index\ndef calc_adar_in(a,b):\n    sum=0\n    try:\n        n=list(set(train_graph.successors(a)).intersection(set(train_graph.successors(b))))\n        if len(n)!=0:\n            for i in n:\n                sum=sum+(1\/np.log10(len(list(train_graph.predecessors(i)))))\n            return sum\n        else:\n            return 0\n    except:\n        return 0\n\ncalc_adar_in(1,189226)\n\ncalc_adar_in(669354,1635354)\n\n## 4.4 Is persion was following back:\n\ndef follows_back(a,b):\n    if train_graph.has_edge(b,a):\n        return 1\n    else:\n        return 0\n\nfollows_back(1,189226)\n\nfollows_back(669354,1635354)\n\n## 4.5 Katz Centrality:\nhttps:\/\/en.wikipedia.org\/wiki\/Katz_centrality\n\nhttps:\/\/www.geeksforgeeks.org\/katz-centrality-centrality-measure\/\n Katz centrality computes the centrality for a node \n    based on the centrality of its neighbors. It is a \n    generalization of the eigenvector centrality. The\n    Katz centrality for node `i` is\n \n$$x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta,$$\nwhere `A` is the adjacency matrix of the graph G \nwith eigenvalues $$\\lambda$$.\n\nThe parameter $$\\beta$$ controls the initial centrality and \n\n$$\\alpha < \\frac{1}{\\lambda_{max}}.$$\n\nif not os.path.isfile('data\/fea_sample\/katz.p'):\n    katz = nx.katz.katz_centrality(train_graph,alpha=0.005,beta=1)\n    pickle.dump(katz,open('data\/fea_sample\/katz.p','wb'))\nelse:\n    katz = pickle.load(open('data\/fea_sample\/katz.p','rb'))\n\nprint('min',katz[min(katz, key=katz.get)])\nprint('max',katz[max(katz, key=katz.get)])\nprint('mean',float(sum(katz.values())) \/ len(katz))\n\nmean_katz = float(sum(katz.values())) \/ len(katz)\nprint(mean_katz)\n\n## 4.6 Hits Score\nThe HITS algorithm computes two numbers for a node. Authorities estimates the node value based on the incoming links. Hubs estimates the node value based on outgoing links.\n\nhttps:\/\/en.wikipedia.org\/wiki\/HITS_algorithm\n\nif not os.path.isfile('data\/fea_sample\/hits.p'):\n    hits = nx.hits(train_graph, max_iter=100, tol=1e-08, nstart=None, normalized=True)\n    pickle.dump(hits,open('data\/fea_sample\/hits.p','wb'))\nelse:\n    hits = pickle.load(open('data\/fea_sample\/hits.p','rb'))\n\nprint('min',hits[0][min(hits[0], key=hits[0].get)])\nprint('max',hits[0][max(hits[0], key=hits[0].get)])\nprint('mean',float(sum(hits[0].values())) \/ len(hits[0]))\n\n# 5. Featurization\n\n## 5. 1 Reading a sample of Data from both train and test\n\nimport random\nif os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/train_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 15100030\n    # n_train = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_train =  15100028\n    s = 100000 #desired sample size\n    skip_train = sorted(random.sample(range(1,n_train+1),n_train-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039\n\nif os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/test_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 3775008\n    # n_test = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_test = 3775006\n    s = 50000 #desired sample size\n    skip_test = sorted(random.sample(range(1,n_test+1),n_test-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039\n\nprint(\"Number of rows in the train data file:\", n_train)\nprint(\"Number of rows we are going to elimiate in train data are\",len(skip_train))\nprint(\"Number of rows in the test data file:\", n_test)\nprint(\"Number of rows we are going to elimiate in test data are\",len(skip_test))\n\ndf_final_train = pd.read_csv('data\/after_eda\/train_after_eda.csv', skiprows=skip_train, names=['source_node', 'destination_node'])\ndf_final_train['indicator_link'] = pd.read_csv('data\/train_y.csv', skiprows=skip_train, names=['indicator_link'])\nprint(\"Our train matrix size \",df_final_train.shape)\ndf_final_train.head(2)\n\ndf_final_test = pd.read_csv('data\/after_eda\/test_after_eda.csv', skiprows=skip_test, names=['source_node', 'destination_node'])\ndf_final_test['indicator_link'] = pd.read_csv('data\/test_y.csv', skiprows=skip_test, names=['indicator_link'])\nprint(\"Our test matrix size \",df_final_test.shape)\ndf_final_test.head(2)\n\n## 5.2 Adding a set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>jaccard_followers<\/li>\n<li>jaccard_followees<\/li>\n<li>cosine_followers<\/li>\n<li>cosine_followees<\/li>\n<li>num_followers_s<\/li>\n<li>num_followees_s<\/li>\n<li>num_followers_d<\/li>\n<li>num_followees_d<\/li>\n<li>inter_followers<\/li>\n<li>inter_followees<\/li>\n<\/ol>\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    #mapping jaccrd followers to train and test data\n    df_final_train['jaccard_followers'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followers'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['jaccard_followees'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followees'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    \n\n        #mapping jaccrd followers to train and test data\n    df_final_train['cosine_followers'] = df_final_train.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followers'] = df_final_test.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['cosine_followees'] = df_final_train.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followees'] = df_final_test.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)\n\ndef compute_features_stage1(df_final):\n    #calculating no of followers followees for source and destination\n    #calculating intersection of followers and followees for source and destination\n    num_followers_s=[]\n    num_followees_s=[]\n    num_followers_d=[]\n    num_followees_d=[]\n    inter_followers=[]\n    inter_followees=[]\n    for i,row in df_final.iterrows():\n        try:\n            s1=set(train_graph.predecessors(row['source_node']))\n            s2=set(train_graph.successors(row['source_node']))\n        except:\n            s1 = set()\n            s2 = set()\n        try:\n            d1=set(train_graph.predecessors(row['destination_node']))\n            d2=set(train_graph.successors(row['destination_node']))\n        except:\n            d1 = set()\n            d2 = set()\n        num_followers_s.append(len(s1))\n        num_followees_s.append(len(s2))\n\n        num_followers_d.append(len(d1))\n        num_followees_d.append(len(d2))\n\n        inter_followers.append(len(s1.intersection(d1)))\n        inter_followees.append(len(s2.intersection(d2)))\n    \n    return num_followers_s, num_followers_d, num_followees_s, num_followees_d, inter_followers, inter_followees\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    df_final_train['num_followers_s'], df_final_train['num_followers_d'], \\\n    df_final_train['num_followees_s'], df_final_train['num_followees_d'], \\\n    df_final_train['inter_followers'], df_final_train['inter_followees']= compute_features_stage1(df_final_train)\n    \n    df_final_test['num_followers_s'], df_final_test['num_followers_d'], \\\n    df_final_test['num_followees_s'], df_final_test['num_followees_d'], \\\n    df_final_test['inter_followers'], df_final_test['inter_followees']= compute_features_stage1(df_final_test)\n    \n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage1.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'test_df',mode='r')\n\n## 5.3 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>adar index<\/li>\n<li>is following back<\/li>\n<li>belongs to same weakly connect components<\/li>\n<li>shortest path between source and destination<\/li>\n<\/ol>\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage2.h5'):\n    #mapping adar index on train\n    df_final_train['adar_index'] = df_final_train.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n    #mapping adar index on test\n    df_final_test['adar_index'] = df_final_test.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping followback or not on train\n    df_final_train['follows_back'] = df_final_train.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping followback or not on test\n    df_final_test['follows_back'] = df_final_test.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping same component of wcc or not on train\n    df_final_train['same_comp'] = df_final_train.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n\n    ##mapping same component of wcc or not on train\n    df_final_test['same_comp'] = df_final_test.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n    \n    #--------------------------------------------------------------------------------------------------------\n    #mapping shortest path on train \n    df_final_train['shortest_path'] = df_final_train.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n    #mapping shortest path on test\n    df_final_test['shortest_path'] = df_final_test.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage2.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'test_df',mode='r')\n\n## 5.4 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>Weight Features\n    <ul>\n        <li>weight of incoming edges<\/li>\n        <li>weight of outgoing edges<\/li>\n        <li>weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges * weight of outgoing edges<\/li>\n        <li>2*weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges + 2*weight of outgoing edges<\/li>\n    <\/ul>\n<\/li>\n<li>Page Ranking of source<\/li>\n<li>Page Ranking of dest<\/li>\n<li>katz of source<\/li>\n<li>katz of dest<\/li>\n<li>hubs of source<\/li>\n<li>hubs of dest<\/li>\n<li>authorities_s of source<\/li>\n<li>authorities_s of dest<\/li>\n<\/ol>\n\n#### Weight Features\n\nIn order to determine the similarity of nodes, an edge weight value was calculated between nodes. Edge weight decreases as the neighbor count goes up. Intuitively, consider one million people following a celebrity on a social network then chances are most of them never met each other or the celebrity. On the other hand, if a user has 30 contacts in his\/her social network, the chances are higher that many of them know each other. \n`credit` - Graph-based Features for Supervised Link Prediction\nWilliam Cukierski, Benjamin Hamner, Bo Yang\n\n\\begin{equation}\nW = \\frac{1}{\\sqrt{1+|X|}}\n\\end{equation}\n\nit is directed graph so calculated Weighted in and Weighted out differently\n\n#weight for source and destination of each link\nWeight_in = {}\nWeight_out = {}\nfor i in  tqdm(train_graph.nodes()):\n    s1=set(train_graph.predecessors(i))\n    w_in = 1.0\/(np.sqrt(1+len(s1)))\n    Weight_in[i]=w_in\n    \n    s2=set(train_graph.successors(i))\n    w_out = 1.0\/(np.sqrt(1+len(s2)))\n    Weight_out[i]=w_out\n    \n#for imputing with mean\nmean_weight_in = np.mean(list(Weight_in.values()))\nmean_weight_out = np.mean(list(Weight_out.values()))\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    #mapping to pandas train\n    df_final_train['weight_in'] = df_final_train.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_train['weight_out'] = df_final_train.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n    #mapping to pandas test\n    df_final_test['weight_in'] = df_final_test.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_test['weight_out'] = df_final_test.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n\n    #some features engineerings on the in and out weights\n    df_final_train['weight_f1'] = df_final_train.weight_in + df_final_train.weight_out\n    df_final_train['weight_f2'] = df_final_train.weight_in * df_final_train.weight_out\n    df_final_train['weight_f3'] = (2*df_final_train.weight_in + 1*df_final_train.weight_out)\n    df_final_train['weight_f4'] = (1*df_final_train.weight_in + 2*df_final_train.weight_out)\n\n    #some features engineerings on the in and out weights\n    df_final_test['weight_f1'] = df_final_test.weight_in + df_final_test.weight_out\n    df_final_test['weight_f2'] = df_final_test.weight_in * df_final_test.weight_out\n    df_final_test['weight_f3'] = (2*df_final_test.weight_in + 1*df_final_test.weight_out)\n    df_final_test['weight_f4'] = (1*df_final_test.weight_in + 2*df_final_test.weight_out)\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    \n    #page rank for source and destination in Train and Test\n    #if anything not there in train graph then adding mean page rank \n    df_final_train['page_rank_s'] = df_final_train.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_train['page_rank_d'] = df_final_train.destination_node.apply(lambda x:pr.get(x,mean_pr))\n\n    df_final_test['page_rank_s'] = df_final_test.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_test['page_rank_d'] = df_final_test.destination_node.apply(lambda x:pr.get(x,mean_pr))\n    #================================================================================\n\n    #Katz centrality score for source and destination in Train and test\n    #if anything not there in train graph then adding mean katz score\n    df_final_train['katz_s'] = df_final_train.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_train['katz_d'] = df_final_train.destination_node.apply(lambda x: katz.get(x,mean_katz))\n\n    df_final_test['katz_s'] = df_final_test.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_test['katz_d'] = df_final_test.destination_node.apply(lambda x: katz.get(x,mean_katz))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and test\n    #if anything not there in train graph then adding 0\n    df_final_train['hubs_s'] = df_final_train.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_train['hubs_d'] = df_final_train.destination_node.apply(lambda x: hits[0].get(x,0))\n\n    df_final_test['hubs_s'] = df_final_test.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_test['hubs_d'] = df_final_test.destination_node.apply(lambda x: hits[0].get(x,0))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and Test\n    #if anything not there in train graph then adding 0\n    df_final_train['authorities_s'] = df_final_train.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_train['authorities_d'] = df_final_train.destination_node.apply(lambda x: hits[1].get(x,0))\n\n    df_final_test['authorities_s'] = df_final_test.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_test['authorities_d'] = df_final_test.destination_node.apply(lambda x: hits[1].get(x,0))\n    #================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage3.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'test_df',mode='r')\n\n## 5.5 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>SVD features for both source and destination<\/li>\n<\/ol>\n\ndef svd(x, S):\n    try:\n        z = sadj_dict[x]\n        return S[z]\n    except:\n        return [0,0,0,0,0,0]\n\n#for svd features to get feature vector creating a dict node val and inedx in svd vector\nsadj_col = sorted(train_graph.nodes())\nsadj_dict = { val:idx for idx,val in enumerate(sadj_col)}\n\nAdj = nx.adjacency_matrix(train_graph,nodelist=sorted(train_graph.nodes())).asfptype()\n\nU, s, V = svds(Adj, k = 6)\nprint('Adjacency matrix Shape',Adj.shape)\nprint('U Shape',U.shape)\nprint('V Shape',V.shape)\nprint('s Shape',s.shape)\n\nif not os.path.isfile('data\/fea_sample\/storage_sample_stage4.h5'):\n    #===================================================================================================\n    \n    df_final_train[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_train[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_train[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_train[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_test[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_test[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n\n    #===================================================================================================\n    \n    df_final_test[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_test[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage4.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\n\n# prepared and stored the data from machine learning models\n# pelase check the FB_Models.ipynb","6882c315":"if os.path.isfile('data\/after_eda\/train_pos_after_eda.csv'):\n    train_graph=nx.read_edgelist('data\/after_eda\/train_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n    print(nx.info(train_graph))\nelse:\n    print(\"please run the FB_EDA.ipynb or download the files from drive\")","fb7088e3":"#for followees\ndef jaccard_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (len(set(train_graph.successors(a)).union(set(train_graph.successors(b)))))\n    except:\n        return 0\n    return sim","c988ed1e":"#one test case\nprint(jaccard_for_followees(273084,1505602))","2301ba1f":"#node 1635354 not in graph \nprint(jaccard_for_followees(273084,1505602))","9c290a45":"#for followers\ndef jaccard_for_followers(a,b):\n    try:\n        if len(set(train_graph.predecessors(a))) == 0  | len(set(g.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                 (len(set(train_graph.predecessors(a)).union(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0","490633ba":"print(jaccard_for_followers(273084,470294))","76e0ec6e":"#node 1635354 not in graph \nprint(jaccard_for_followees(669354,1635354))","ca50742b":"#for followees\ndef cosine_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (math.sqrt(len(set(train_graph.successors(a)))*len((set(train_graph.successors(b))))))\n        return sim\n    except:\n        return 0","4543d3e7":"print(cosine_for_followees(273084,1505602))","ba7fc248":"print(cosine_for_followees(273084,1635354))","072a34d3":"def cosine_for_followers(a,b):\n    try:\n        \n        if len(set(train_graph.predecessors(a))) == 0  | len(set(train_graph.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                     (math.sqrt(len(set(train_graph.predecessors(a))))*(len(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0","36e23953":"print(cosine_for_followers(2,470294))","91d5b1c0":"print(cosine_for_followers(669354,1635354))","d2ee7ceb":"if not os.path.isfile('data\/fea_sample\/page_rank.p'):\n    pr = nx.pagerank(train_graph, alpha=0.85)\n    pickle.dump(pr,open('data\/fea_sample\/page_rank.p','wb'))\nelse:\n    pr = pickle.load(open('data\/fea_sample\/page_rank.p','rb'))","054071e5":"print('min',pr[min(pr, key=pr.get)])\nprint('max',pr[max(pr, key=pr.get)])\nprint('mean',float(sum(pr.values())) \/ len(pr))","7d9bdc12":"#for imputing to nodes which are not there in Train data\nmean_pr = float(sum(pr.values())) \/ len(pr)\nprint(mean_pr)","1785e84a":"#if has direct edge then deleting that edge and calculating shortest path\ndef compute_shortest_path_length(a,b):\n    p=-1\n    try:\n        if train_graph.has_edge(a,b):\n            train_graph.remove_edge(a,b)\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n            train_graph.add_edge(a,b)\n        else:\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n        return p\n    except:\n        return -1","198f04ad":"#testing\ncompute_shortest_path_length(77697, 826021)","37e7ccde":"#testing\ncompute_shortest_path_length(669354,1635354)","f46fa405":"#getting weekly connected edges from graph \nwcc=list(nx.weakly_connected_components(train_graph))\ndef belongs_to_same_wcc(a,b):\n    index = []\n    if train_graph.has_edge(b,a):\n        return 1\n    if train_graph.has_edge(a,b):\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if (b in index):\n                train_graph.remove_edge(a,b)\n                if compute_shortest_path_length(a,b)==-1:\n                    train_graph.add_edge(a,b)\n                    return 0\n                else:\n                    train_graph.add_edge(a,b)\n                    return 1\n            else:\n                return 0\n    else:\n            for i in wcc:\n                if a in i:\n                    index= i\n                    break\n            if(b in index):\n                return 1\n            else:\n                return 0","9f39c11b":"belongs_to_same_wcc(861, 1659750)","0fe7378b":"belongs_to_same_wcc(669354,1635354)","b02e7897":"#adar index\ndef calc_adar_in(a,b):\n    sum=0\n    try:\n        n=list(set(train_graph.successors(a)).intersection(set(train_graph.successors(b))))\n        if len(n)!=0:\n            for i in n:\n                sum=sum+(1\/np.log10(len(list(train_graph.predecessors(i)))))\n            return sum\n        else:\n            return 0\n    except:\n        return 0","7d514e65":"calc_adar_in(1,189226)","3b90fd5e":"calc_adar_in(669354,1635354)","15bf47b2":"def follows_back(a,b):\n    if train_graph.has_edge(b,a):\n        return 1\n    else:\n        return 0","12a732d3":"follows_back(1,189226)","8be6f119":"follows_back(669354,1635354)","1cc8a7d5":"if not os.path.isfile('data\/fea_sample\/katz.p'):\n    katz = nx.katz.katz_centrality(train_graph,alpha=0.005,beta=1)\n    pickle.dump(katz,open('data\/fea_sample\/katz.p','wb'))\nelse:\n    katz = pickle.load(open('data\/fea_sample\/katz.p','rb'))","9d65f055":"print('min',katz[min(katz, key=katz.get)])\nprint('max',katz[max(katz, key=katz.get)])\nprint('mean',float(sum(katz.values())) \/ len(katz))","2278af73":"mean_katz = float(sum(katz.values())) \/ len(katz)\nprint(mean_katz)","b654e879":"if not os.path.isfile('data\/fea_sample\/hits.p'):\n    hits = nx.hits(train_graph, max_iter=100, tol=1e-08, nstart=None, normalized=True)\n    pickle.dump(hits,open('data\/fea_sample\/hits.p','wb'))\nelse:\n    hits = pickle.load(open('data\/fea_sample\/hits.p','rb'))","600ee155":"print('min',hits[0][min(hits[0], key=hits[0].get)])\nprint('max',hits[0][max(hits[0], key=hits[0].get)])\nprint('mean',float(sum(hits[0].values())) \/ len(hits[0]))","e5d23829":"import random\nif os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/train_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 15100030\n    # n_train = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_train =  15100028\n    s = 100000 #desired sample size\n    skip_train = sorted(random.sample(range(1,n_train+1),n_train-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039","183d544c":"if os.path.isfile('data\/after_eda\/train_after_eda.csv'):\n    filename = \"data\/after_eda\/test_after_eda.csv\"\n    # you uncomment this line, if you dont know the lentgh of the file name\n    # here we have hardcoded the number of lines as 3775008\n    # n_test = sum(1 for line in open(filename)) #number of records in file (excludes header)\n    n_test = 3775006\n    s = 50000 #desired sample size\n    skip_test = sorted(random.sample(range(1,n_test+1),n_test-s))\n    #https:\/\/stackoverflow.com\/a\/22259008\/4084039","c03c7d44":"print(\"Number of rows in the train data file:\", n_train)\nprint(\"Number of rows we are going to elimiate in train data are\",len(skip_train))\nprint(\"Number of rows in the test data file:\", n_test)\nprint(\"Number of rows we are going to elimiate in test data are\",len(skip_test))","a5d04b8c":"df_final_train = pd.read_csv('data\/after_eda\/train_after_eda.csv', skiprows=skip_train, names=['source_node', 'destination_node'])\ndf_final_train['indicator_link'] = pd.read_csv('data\/train_y.csv', skiprows=skip_train, names=['indicator_link'])\nprint(\"Our train matrix size \",df_final_train.shape)\ndf_final_train.head(2)","a39be8f4":"df_final_test = pd.read_csv('data\/after_eda\/test_after_eda.csv', skiprows=skip_test, names=['source_node', 'destination_node'])\ndf_final_test['indicator_link'] = pd.read_csv('data\/test_y.csv', skiprows=skip_test, names=['indicator_link'])\nprint(\"Our test matrix size \",df_final_test.shape)\ndf_final_test.head(2)","89bc6130":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    #mapping jaccrd followers to train and test data\n    df_final_train['jaccard_followers'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followers'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['jaccard_followees'] = df_final_train.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followees'] = df_final_test.apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    \n\n        #mapping jaccrd followers to train and test data\n    df_final_train['cosine_followers'] = df_final_train.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followers'] = df_final_test.apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['cosine_followees'] = df_final_train.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followees'] = df_final_test.apply(lambda row:\n                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)","c5875dc1":"def compute_features_stage1(df_final):\n    #calculating no of followers followees for source and destination\n    #calculating intersection of followers and followees for source and destination\n    num_followers_s=[]\n    num_followees_s=[]\n    num_followers_d=[]\n    num_followees_d=[]\n    inter_followers=[]\n    inter_followees=[]\n    for i,row in df_final.iterrows():\n        try:\n            s1=set(train_graph.predecessors(row['source_node']))\n            s2=set(train_graph.successors(row['source_node']))\n        except:\n            s1 = set()\n            s2 = set()\n        try:\n            d1=set(train_graph.predecessors(row['destination_node']))\n            d2=set(train_graph.successors(row['destination_node']))\n        except:\n            d1 = set()\n            d2 = set()\n        num_followers_s.append(len(s1))\n        num_followees_s.append(len(s2))\n\n        num_followers_d.append(len(d1))\n        num_followees_d.append(len(d2))\n\n        inter_followers.append(len(s1.intersection(d1)))\n        inter_followees.append(len(s2.intersection(d2)))\n    \n    return num_followers_s, num_followers_d, num_followees_s, num_followees_d, inter_followers, inter_followees","331d398a":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    df_final_train['num_followers_s'], df_final_train['num_followers_d'], \\\n    df_final_train['num_followees_s'], df_final_train['num_followees_d'], \\\n    df_final_train['inter_followers'], df_final_train['inter_followees']= compute_features_stage1(df_final_train)\n    \n    df_final_test['num_followers_s'], df_final_test['num_followers_d'], \\\n    df_final_test['num_followees_s'], df_final_test['num_followees_d'], \\\n    df_final_test['inter_followers'], df_final_test['inter_followees']= compute_features_stage1(df_final_test)\n    \n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage1.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage1.h5', 'test_df',mode='r')","a4898c79":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage2.h5'):\n    #mapping adar index on train\n    df_final_train['adar_index'] = df_final_train.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n    #mapping adar index on test\n    df_final_test['adar_index'] = df_final_test.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping followback or not on train\n    df_final_train['follows_back'] = df_final_train.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping followback or not on test\n    df_final_test['follows_back'] = df_final_test.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping same component of wcc or not on train\n    df_final_train['same_comp'] = df_final_train.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n\n    ##mapping same component of wcc or not on train\n    df_final_test['same_comp'] = df_final_test.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n    \n    #--------------------------------------------------------------------------------------------------------\n    #mapping shortest path on train \n    df_final_train['shortest_path'] = df_final_train.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n    #mapping shortest path on test\n    df_final_test['shortest_path'] = df_final_test.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage2.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage2.h5', 'test_df',mode='r')","17c794d2":"#weight for source and destination of each link\nWeight_in = {}\nWeight_out = {}\nfor i in  tqdm(train_graph.nodes()):\n    s1=set(train_graph.predecessors(i))\n    w_in = 1.0\/(np.sqrt(1+len(s1)))\n    Weight_in[i]=w_in\n    \n    s2=set(train_graph.successors(i))\n    w_out = 1.0\/(np.sqrt(1+len(s2)))\n    Weight_out[i]=w_out\n    \n#for imputing with mean\nmean_weight_in = np.mean(list(Weight_in.values()))\nmean_weight_out = np.mean(list(Weight_out.values()))","cfd4db95":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    #mapping to pandas train\n    df_final_train['weight_in'] = df_final_train.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_train['weight_out'] = df_final_train.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n    #mapping to pandas test\n    df_final_test['weight_in'] = df_final_test.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_test['weight_out'] = df_final_test.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n\n    #some features engineerings on the in and out weights\n    df_final_train['weight_f1'] = df_final_train.weight_in + df_final_train.weight_out\n    df_final_train['weight_f2'] = df_final_train.weight_in * df_final_train.weight_out\n    df_final_train['weight_f3'] = (2*df_final_train.weight_in + 1*df_final_train.weight_out)\n    df_final_train['weight_f4'] = (1*df_final_train.weight_in + 2*df_final_train.weight_out)\n\n    #some features engineerings on the in and out weights\n    df_final_test['weight_f1'] = df_final_test.weight_in + df_final_test.weight_out\n    df_final_test['weight_f2'] = df_final_test.weight_in * df_final_test.weight_out\n    df_final_test['weight_f3'] = (2*df_final_test.weight_in + 1*df_final_test.weight_out)\n    df_final_test['weight_f4'] = (1*df_final_test.weight_in + 2*df_final_test.weight_out)","9a280a33":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage3.h5'):\n    \n    #page rank for source and destination in Train and Test\n    #if anything not there in train graph then adding mean page rank \n    df_final_train['page_rank_s'] = df_final_train.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_train['page_rank_d'] = df_final_train.destination_node.apply(lambda x:pr.get(x,mean_pr))\n\n    df_final_test['page_rank_s'] = df_final_test.source_node.apply(lambda x:pr.get(x,mean_pr))\n    df_final_test['page_rank_d'] = df_final_test.destination_node.apply(lambda x:pr.get(x,mean_pr))\n    #================================================================================\n\n    #Katz centrality score for source and destination in Train and test\n    #if anything not there in train graph then adding mean katz score\n    df_final_train['katz_s'] = df_final_train.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_train['katz_d'] = df_final_train.destination_node.apply(lambda x: katz.get(x,mean_katz))\n\n    df_final_test['katz_s'] = df_final_test.source_node.apply(lambda x: katz.get(x,mean_katz))\n    df_final_test['katz_d'] = df_final_test.destination_node.apply(lambda x: katz.get(x,mean_katz))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and test\n    #if anything not there in train graph then adding 0\n    df_final_train['hubs_s'] = df_final_train.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_train['hubs_d'] = df_final_train.destination_node.apply(lambda x: hits[0].get(x,0))\n\n    df_final_test['hubs_s'] = df_final_test.source_node.apply(lambda x: hits[0].get(x,0))\n    df_final_test['hubs_d'] = df_final_test.destination_node.apply(lambda x: hits[0].get(x,0))\n    #================================================================================\n\n    #Hits algorithm score for source and destination in Train and Test\n    #if anything not there in train graph then adding 0\n    df_final_train['authorities_s'] = df_final_train.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_train['authorities_d'] = df_final_train.destination_node.apply(lambda x: hits[1].get(x,0))\n\n    df_final_test['authorities_s'] = df_final_test.source_node.apply(lambda x: hits[1].get(x,0))\n    df_final_test['authorities_d'] = df_final_test.destination_node.apply(lambda x: hits[1].get(x,0))\n    #================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage3.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()\nelse:\n    df_final_train = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'train_df',mode='r')\n    df_final_test = read_hdf('data\/fea_sample\/storage_sample_stage3.h5', 'test_df',mode='r')","b7f1eab5":"def svd(x, S):\n    try:\n        z = sadj_dict[x]\n        return S[z]\n    except:\n        return [0,0,0,0,0,0]","61b23399":"#for svd features to get feature vector creating a dict node val and inedx in svd vector\nsadj_col = sorted(train_graph.nodes())\nsadj_dict = { val:idx for idx,val in enumerate(sadj_col)}","17f37ba4":"Adj = nx.adjacency_matrix(train_graph,nodelist=sorted(train_graph.nodes())).asfptype()","7cd1a8f4":"U, s, V = svds(Adj, k = 6)\nprint('Adjacency matrix Shape',Adj.shape)\nprint('U Shape',U.shape)\nprint('V Shape',V.shape)\nprint('s Shape',s.shape)","45c29bd3":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage4.h5'):\n    #===================================================================================================\n    \n    df_final_train[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_train[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_train[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_train.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_train[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_train.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n    \n    df_final_test[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n    \n    df_final_test[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n\n    #===================================================================================================\n    \n    df_final_test[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n    df_final_test.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n\n    df_final_test[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n    df_final_test.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n    #===================================================================================================\n\n    hdf = HDFStore('data\/fea_sample\/storage_sample_stage4.h5')\n    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n    hdf.close()","c38e3899":"#Importing Libraries\n# please do go through this python notebook: \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport csv\nimport pandas as pd#pandas to create small dataframes \nimport datetime #Convert to unix time\nimport time #Convert to unix time\n# if numpy is not installed already : pip3 install numpy\nimport numpy as np#Do aritmetic operations on arrays\n# matplotlib: used to plot graphs\nimport matplotlib\nimport matplotlib.pylab as plt\nimport seaborn as sns#Plots\nfrom matplotlib import rcParams#Size of plots  \nfrom sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\nimport math\nimport pickle\nimport os\n# to install xgboost: pip3 install xgboost\nimport xgboost as xgb\n\nimport warnings\nimport networkx as nx\nimport pdb\nimport pickle\nfrom pandas import HDFStore,DataFrame\nfrom pandas import read_hdf\nfrom scipy.sparse.linalg import svds, eigs\nimport gc\nfrom tqdm import tqdm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score","eae04bcf":"from google.colab import drive\ndrive.mount('\/content\/drive')","d2a466c0":"#reading\nfrom pandas import read_hdf\ndf_final_train = read_hdf('\/content\/drive\/My Drive\/Facebook\/data\/fea_sample\/storage_sample_stage4.h5', 'train_df',mode='r')\ndf_final_test = read_hdf('\/content\/drive\/My Drive\/Facebook\/data\/fea_sample\/storage_sample_stage4.h5', 'test_df',mode='r')","c2cf46dd":"df_final_train.columns","107e0fed":"y_train = df_final_train.indicator_link\ny_test = df_final_test.indicator_link","434ea06a":"df_final_train.drop(['source_node', 'destination_node','indicator_link'],axis=1,inplace=True)\ndf_final_test.drop(['source_node', 'destination_node','indicator_link'],axis=1,inplace=True)","32baa9f6":"estimators = [10,50,100,250,450]\ntrain_scores = []\ntest_scores = []\nfor i in estimators:\n    clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=5, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=52, min_samples_split=120,\n            min_weight_fraction_leaf=0.0, n_estimators=i, n_jobs=-1,random_state=25,verbose=0,warm_start=False)\n    clf.fit(df_final_train,y_train)\n    train_sc = f1_score(y_train,clf.predict(df_final_train))\n    test_sc = f1_score(y_test,clf.predict(df_final_test))\n    test_scores.append(test_sc)\n    train_scores.append(train_sc)\n    print('Estimators = ',i,'Train Score',train_sc,'test Score',test_sc)\nplt.plot(estimators,train_scores,label='Train Score')\nplt.plot(estimators,test_scores,label='Test Score')\nplt.xlabel('Estimators')\nplt.ylabel('Score')\nplt.title('Estimators vs score at depth of 5')","38386931":"depths = [3,9,11,15,20,35,50,70,130]\ntrain_scores = []\ntest_scores = []\nfor i in depths:\n    clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=i, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=52, min_samples_split=120,\n            min_weight_fraction_leaf=0.0, n_estimators=115, n_jobs=-1,random_state=25,verbose=0,warm_start=False)\n    clf.fit(df_final_train,y_train)\n    train_sc = f1_score(y_train,clf.predict(df_final_train))\n    test_sc = f1_score(y_test,clf.predict(df_final_test))\n    test_scores.append(test_sc)\n    train_scores.append(train_sc)\n    print('depth = ',i,'Train Score',train_sc,'test Score',test_sc)\nplt.plot(depths,train_scores,label='Train Score')\nplt.plot(depths,test_scores,label='Test Score')\nplt.xlabel('Depth')\nplt.ylabel('Score')\nplt.title('Depth vs score at depth of 5 at estimators = 115')\nplt.show()","e5a9e2f4":"from sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform\n\nparam_dist = {\"n_estimators\":sp_randint(105,125),\n              \"max_depth\": sp_randint(10,15),\n              \"min_samples_split\": sp_randint(110,190),\n              \"min_samples_leaf\": sp_randint(25,65)}\n\nclf = RandomForestClassifier(random_state=25,n_jobs=-1)\n\nrf_random = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=5,cv=10,scoring='f1',random_state=25)\n\nrf_random.fit(df_final_train,y_train)\nprint('mean test scores',rf_random.cv_results_['mean_test_score'])\nprint('mean train scores',rf_random.cv_results_['mean_train_score'])","39ddf7a7":"print(rf_random.best_estimator_)","c46430e5":"clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=14, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=28, min_samples_split=111,\n            min_weight_fraction_leaf=0.0, n_estimators=121, n_jobs=-1,\n            oob_score=False, random_state=25, verbose=0, warm_start=False)","e25a9dd5":"clf.fit(df_final_train,y_train)\ny_train_pred = clf.predict(df_final_train)\ny_test_pred = clf.predict(df_final_test)","9e435050":"from sklearn.metrics import f1_score\nprint('Train f1 score',f1_score(y_train,y_train_pred))\nprint('Test f1 score',f1_score(y_test,y_test_pred))","4d65824a":"from sklearn.metrics import confusion_matrix\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    \n    B =(C\/C.sum(axis=0))\n    plt.figure(figsize=(20,4))\n    \n    labels = [0,1]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","19f6a9c3":"print('Train confusion_matrix')\nplot_confusion_matrix(y_train,y_train_pred)\nprint('Test confusion_matrix')\nplot_confusion_matrix(y_test,y_test_pred)","e6e0bb29":"from sklearn.metrics import roc_curve, auc\nfpr,tpr,ths = roc_curve(y_test,y_test_pred)\nauc_sc = auc(fpr, tpr)\nplt.plot(fpr, tpr, color='navy',label='ROC curve (area = %0.2f)' % auc_sc)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic with test data')\nplt.legend()\nplt.show()","0655cc2a":"features = df_final_train.columns\nimportances = clf.feature_importances_\nindices = (np.argsort(importances))[-25:]\nplt.figure(figsize=(10,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='r', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","eab8ed3e":"df_final_train.head()","7f825f0a":"df_final_test.head()","c4c8cc14":"train_graph=nx.read_edgelist('\/content\/drive\/My Drive\/Facebook\/data\/after_eda\/train_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\ntest_graph=nx.read_edgelist('\/content\/drive\/My Drive\/Facebook\/data\/after_eda\/test_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n","c44f33d8":"from tqdm import tqdm\ndef compute_features_stage1(df,graph):\n  num_followers_s =[]\n  num_followees_s =[]\n  num_followers_d =[]\n  num_followees_d = []\n  inter_followers= []\n  inter_followees =[]\n  for i,row in tqdm(df.iterrows()):\n    try:\n      s1 = set(graph.predecessors(row['source_node']))\n      s2 = set(graph.successors(row['source_node']))\n    except:\n      s1 = set()\n      s2 = set()\n    try: \n      d1 = set(graph.predecessors(row[\"destination_node\"]))\n      d2 = set(graph.successors(row['destination_node']))\n    except:\n      d1 = set()\n      d2 = set()\n    num_followers_s.append(len(s1))\n    num_followees_s.append(len(s2))\n    num_followers_d.append(len(d1))\n    num_followees_d.append(len(d2))\n\n    inter_followees.append(len(s1.intersection(d1)))\n    inter_followers.append(len(s2.intersection(d2)))\n\n  return num_followers_d,num_followees_d,num_followers_s,num_followees_s,inter_followers,inter_followees       \n","83a5c31c":"train_num_followers_d,train_num_followees_d,train_num_followers_s,train_num_followees_s,train_inter_followers,train_inter_followees = compute_features_stage1(df_final_train,train_graph)\ntest_num_followers_d,test_num_followees_d,test_num_followers_s,test_num_followees_s,test_inter_followers,test_inter_followees = compute_features_stage1(df_final_test,test_graph)","cbb29fc0":"len(train_num_followers_d)","9f90e3cb":"len(test_num_followers_d)","f6a04dd7":"df_final_train[\"num_followers_d\"] = train_num_followers_d\ndf_final_train.head()","260d3e78":"df_final_test['num_followers_d'] = test_num_followees_d\ndf_final_test.head()","bd9b2679":"source = np.array(df_final_train[\"num_followers_s\"])\ndestination = np.array(df_final_train[\"num_followers_d\"])\nfollowers_preferential = []\nfor i in range(len(source)):\n  followers_preferential.append(source[i]*destination[i])\ndf_final_train[\"followers_preferential_attach\"]  = followers_preferential \ndf_final_train.head()","6339f6b2":"source = np.array(df_final_train[\"num_followees_s\"])\ndestination = np.array(df_final_train[\"num_followees_s\"])\nfollowees_preferential = []\nfor i in range(len(source)):\n  followees_preferential.append(source[i]*destination[i])\ndf_final_train[\"followees_preferential_attach\"]  = followees_preferential \ndf_final_train.head()","c6154a8a":"source = np.array(df_final_test[\"num_followers_s\"])\ndestination = np.array(df_final_test[\"num_followers_d\"])\nfollowers_preferential = []\nfor i in range(len(source)):\n  followers_preferential.append(source[i]*destination[i])\ndf_final_test[\"followers_preferential_attach\"]  = followers_preferential \ndf_final_test.head()","15ecd8be":"source = np.array(df_final_test[\"num_followees_s\"])\ndestination = np.array(df_final_test[\"num_followees_s\"])\nfollowees_preferential = []\nfor i in range(len(source)):\n  followees_preferential.append(source[i]*destination[i])\ndf_final_test[\"followees_preferential_attach\"]  = followees_preferential \ndf_final_test.head()","7f730c46":"sorted(df_final_train.columns)","cd1b0a5b":"svd_dot_U =[]\nrange_value = len(np.array(df_final_train[\"svd_u_d_1\"]))\nfor i in range(range_value):\n  u_s = []\n  u_d = []\n  u_s.append(np.array(df_final_train[\"svd_u_s_1\"][i]))\n  u_s.append(np.array(df_final_train[\"svd_u_s_2\"][i]))\n  u_s.append(np.array(df_final_train[\"svd_u_s_3\"][i]))\n  u_s.append(np.array(df_final_train[\"svd_u_s_4\"][i]))\n  u_s.append(np.array(df_final_train[\"svd_u_s_5\"][i]))\n  u_s.append(np.array(df_final_train[\"svd_u_s_6\"][i]))\n\n  u_d.append(np.array(df_final_train[\"svd_u_d_1\"][i]))\n  u_d.append(np.array(df_final_train[\"svd_u_d_2\"][i]))\n  u_d.append(np.array(df_final_train[\"svd_u_d_3\"][i]))\n  u_d.append(np.array(df_final_train[\"svd_u_d_4\"][i]))\n  u_d.append(np.array(df_final_train[\"svd_u_d_5\"][i]))\n  u_d.append(np.array(df_final_train[\"svd_u_d_6\"][i]))\n\n  svd_dot_U.append(np.dot(u_s,u_d))\n  ","f9ad0c01":"df_final_train[\"svd_dot_U\"] = svd_dot_U\ndf_final_train.head()","47d56559":"svd_dot_V = []\nrange_value = len(np.array(df_final_train[\"svd_v_d_1\"]))\nfor i in range(range_value):\n  v_s = []\n  v_d = []\n  v_s.append(np.array(df_final_train[\"svd_v_s_1\"][i]))\n  v_s.append(np.array(df_final_train[\"svd_v_s_2\"][i]))\n  v_s.append(np.array(df_final_train[\"svd_v_s_3\"][i]))\n  v_s.append(np.array(df_final_train[\"svd_v_s_4\"][i]))\n  v_s.append(np.array(df_final_train[\"svd_v_s_5\"][i]))\n  v_s.append(np.array(df_final_train[\"svd_v_s_6\"][i]))\n\n  v_d.append(np.array(df_final_train[\"svd_v_d_1\"][i]))\n  v_d.append(np.array(df_final_train[\"svd_v_d_2\"][i]))\n  v_d.append(np.array(df_final_train[\"svd_v_d_3\"][i]))\n  v_d.append(np.array(df_final_train[\"svd_v_d_4\"][i]))\n  v_d.append(np.array(df_final_train[\"svd_v_d_5\"][i]))\n  v_d.append(np.array(df_final_train[\"svd_v_d_6\"][i]))\n\n  svd_dot_V.append(np.dot(v_s,v_d))","282303d0":"df_final_train[\"svd_dot_V\"] = svd_dot_V\ndf_final_train.head()","4062ba11":"svd_dot_U =[]\nrange_value = len(np.array(df_final_test[\"svd_u_d_1\"]))\nfor i in range(range_value):\n  u_s = []\n  u_d = []\n  u_s.append(np.array(df_final_test[\"svd_u_s_1\"][i]))\n  u_s.append(np.array(df_final_test[\"svd_u_s_2\"][i]))\n  u_s.append(np.array(df_final_test[\"svd_u_s_3\"][i]))\n  u_s.append(np.array(df_final_test[\"svd_u_s_4\"][i]))\n  u_s.append(np.array(df_final_test[\"svd_u_s_5\"][i]))\n  u_s.append(np.array(df_final_test[\"svd_u_s_6\"][i]))\n\n  u_d.append(np.array(df_final_test[\"svd_u_d_1\"][i]))\n  u_d.append(np.array(df_final_test[\"svd_u_d_2\"][i]))\n  u_d.append(np.array(df_final_test[\"svd_u_d_3\"][i]))\n  u_d.append(np.array(df_final_test[\"svd_u_d_4\"][i]))\n  u_d.append(np.array(df_final_test[\"svd_u_d_5\"][i]))\n  u_d.append(np.array(df_final_test[\"svd_u_d_6\"][i]))\n\n  svd_dot_U.append(np.dot(u_s,u_d))\n  ","a3f2e3f4":"df_final_test[\"svd_dot_U\"] = svd_dot_U\ndf_final_test.head()","d65ef85b":"svd_dot_V = []\nrange_value = len(np.array(df_final_test[\"svd_v_d_1\"]))\nfor i in range(range_value):\n  v_s = []\n  v_d = []\n  v_s.append(np.array(df_final_test[\"svd_v_s_1\"][i]))\n  v_s.append(np.array(df_final_test[\"svd_v_s_2\"][i]))\n  v_s.append(np.array(df_final_test[\"svd_v_s_3\"][i]))\n  v_s.append(np.array(df_final_test[\"svd_v_s_4\"][i]))\n  v_s.append(np.array(df_final_test[\"svd_v_s_5\"][i]))\n  v_s.append(np.array(df_final_test[\"svd_v_s_6\"][i]))\n\n  v_d.append(np.array(df_final_test[\"svd_v_d_1\"][i]))\n  v_d.append(np.array(df_final_test[\"svd_v_d_2\"][i]))\n  v_d.append(np.array(df_final_test[\"svd_v_d_3\"][i]))\n  v_d.append(np.array(df_final_test[\"svd_v_d_4\"][i]))\n  v_d.append(np.array(df_final_test[\"svd_v_d_5\"][i]))\n  v_d.append(np.array(df_final_test[\"svd_v_d_6\"][i]))\n\n  svd_dot_V.append(np.dot(v_s,v_d))","978aecad":"df_final_test[\"svd_dot_V\"] = svd_dot_V\ndf_final_test.head()","92e5bf9d":"x_train  =  df_final_train\nx_test = df_final_test","a224b3ca":"x_train.shape","4621f126":"y_train.shape","cd626711":"x_test.shape","d073a83b":"y_test.shape","dff17a52":"from sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import RandomizedSearchCV\n","cee54b91":"def plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    print(\"Number of misclassified points \",(len(test_y)-np.trace(C))\/len(test_y)*100)\n    A =(((C.T)\/(C.sum(axis=1))).T)\n    B =(C\/C.sum(axis=0))\n    labels = [0,1]\n    cmap=sns.light_palette(\"green\")\n    print(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*50, \"Precision matrix\", \"-\"*50)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    print(\"Sum of columns in precision matrix\",B.sum(axis=0))\n\n    print(\"-\"*50, \"Recall matrix\"    , \"-\"*50)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    print(\"Sum of rows in precision matrix\",A.sum(axis=1))","c20931d0":"params = {\n        'min_child_weight': [1, 5, 10],\n        'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n        'n_estimators' :[100,200,500,1000,2000],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n          }\nxgboost = XGBClassifier()          \nrandom_cfl1=RandomizedSearchCV(xgboost,param_distributions = params,verbose=10,n_jobs=-1)\nrandom_cfl1.fit(x_train,y_train)","5ada2b0f":"print(random_cfl1.best_params_)","bb54fb22":"from sklearn.calibration import CalibratedClassifierCV\nbest_xgb = XGBClassifier(sub_sample=0.8,learning_rate = 0.2,colsample_bytree=0.6,max_depth=5,n_estimators=1000,min_child_weight=5,gamma=5)\nbest_xgb.fit(x_train,y_train)\nclf = CalibratedClassifierCV(best_xgb,method=\"sigmoid\")\nclf.fit(x_train,y_train)","2736442e":"from sklearn.metrics import log_loss\npredict_y = clf.predict_proba(x_train)\nprint(\"the train log loss is\",log_loss(y_train,predict_y,labels=best_xgb.classes_))\npredict_y = clf.predict_proba(x_test)\nprint(\"the test log loss is\",log_loss(y_test,predict_y,labels = best_xgb.classes_))\nplot_confusion_matrix(y_test,clf.predict(x_test))","79a9ad42":"from sklearn.metrics import f1_score\nprint(\"f1 score for train data\",f1_score(y_train,clf.predict(x_train)))\nprint(\"f1 score for test data\",f1_score(y_test,clf.predict(x_test)))","6b206ec3":"from sklearn.metrics import roc_curve, auc\nfpr,tpr,ths = roc_curve(y_test,clf.predict(x_test))\nauc_sc = auc(fpr, tpr)\nplt.plot(fpr, tpr,label = ('auc=',auc_sc))\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.legend()\nplt.show()","f4531f5f":"features = df_final_train.columns\nimportances = best_xgb.feature_importances_\nindices = (np.argsort(importances))[-25:]\nplt.figure(figsize=(10,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='r', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","4df2ddb1":"from prettytable import PrettyTable\nt = PrettyTable()\nt.field_names=[\"model\",\"train_f1_Score\",\"test_f1_score\"]\nt.add_row([\"XGBoost\",\"0.9920553921274338\",\"0.9124241511382708\"])\nprint(t)","c28f9a00":"it is directed graph so calculated Weighted in and Weighted out differently","c4957dda":"## 5.5 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>SVD features for both source and destination<\/li>\n<\/ol>","9dd15277":"> Displaying a sub graph","2e0dd2c4":"## 4.4 Is persion was following back:","4d4ccf21":"<p style=\"font-size:32px;text-align:center\"> <b>Social network Graph Link Prediction - Facebook Challenge<\/b> <\/p>","7a26662f":"## 5.4 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>Weight Features\n    <ul>\n        <li>weight of incoming edges<\/li>\n        <li>weight of outgoing edges<\/li>\n        <li>weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges * weight of outgoing edges<\/li>\n        <li>2*weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges + 2*weight of outgoing edges<\/li>\n    <\/ul>\n<\/li>\n<li>Page Ranking of source<\/li>\n<li>Page Ranking of dest<\/li>\n<li>katz of source<\/li>\n<li>katz of dest<\/li>\n<li>hubs of source<\/li>\n<li>hubs of dest<\/li>\n<li>authorities_s of source<\/li>\n<li>authorities_s of dest<\/li>\n<\/ol>","032cb006":"\\begin{equation}\nj = \\frac{|X\\cap Y|}{|X \\cup Y|} \n\\end{equation}","df169297":"## 2.1 Jaccard Distance:\nhttp:\/\/www.statisticshowto.com\/jaccard-index\/","913d622f":"For followers\n","73c7e69c":"## 3.1 Page Ranking\n\nhttps:\/\/en.wikipedia.org\/wiki\/PageRank\n","d321cea4":"# 4. Other Graph Features","113f8f4e":"\\begin{equation}\nj = \\frac{|X\\cap Y|}{|X \\cup Y|} \n\\end{equation}","6533dd6f":"## 2.2 Training and Test data split:  \nRemoved edges from Graph and used as test data and after removing used that graph for creating features for Train and test data","1ed3f3cc":"99% of data having followers of 40 only.","31eac205":"#### Weight Features","ace52cbb":"## 4.5 Katz Centrality:\nhttps:\/\/en.wikipedia.org\/wiki\/Katz_centrality\n\nhttps:\/\/www.geeksforgeeks.org\/katz-centrality-centrality-measure\/\n Katz centrality computes the centrality for a node \n    based on the centrality of its neighbors. It is a \n    generalization of the eigenvector centrality. The\n    Katz centrality for node `i` is\n \n$$x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta,$$\nwhere `A` is the adjacency matrix of the graph G \nwith eigenvalues $$\\lambda$$.\n\nThe parameter $$\\beta$$ controls the initial centrality and \n\n$$\\alpha < \\frac{1}{\\lambda_{max}}.$$","4ca8a423":"https:\/\/networkx.github.io\/documentation\/networkx-1.10\/reference\/generated\/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html\n\nPageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links.\n\n<img src='PageRanks-Example.jpg'\/>\n\nMathematical PageRanks for a simple network, expressed as percentages. (Google uses a logarithmic scale.) Page C has a higher PageRank than Page E, even though there are fewer links to C; the one link to C comes from an important page and hence is of high value. If web surfers who start on a random page have an 85% likelihood of choosing a random link from the page they are currently visiting, and a 15% likelihood of jumping to a page chosen at random from the entire web, they will reach Page E 8.1% of the time. <b>(The 15% likelihood of jumping to an arbitrary page corresponds to a damping factor of 85%.) Without damping, all web surfers would eventually end up on Pages A, B, or C, and all other pages would have PageRank zero. In the presence of damping, Page A effectively links to all pages in the web, even though it has no outgoing links of its own.<\/b>","652ad7ee":"Getting Shortest path between twoo nodes, if nodes have direct path i.e directly connected then we are removing that edge and calculating path. ","605b7269":"## 5.5 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>SVD features for both source and destination<\/li>\n<\/ol>","8305b14f":"## 5.3 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>adar index<\/li>\n<li>is following back<\/li>\n<li>belongs to same weakly connect components<\/li>\n<li>shortest path between source and destination<\/li>\n<\/ol>","f07b2f19":"## 5.4 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>Weight Features\n    <ul>\n        <li>weight of incoming edges<\/li>\n        <li>weight of outgoing edges<\/li>\n        <li>weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges * weight of outgoing edges<\/li>\n        <li>2*weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges + 2*weight of outgoing edges<\/li>\n    <\/ul>\n<\/li>\n<li>Page Ranking of source<\/li>\n<li>Page Ranking of dest<\/li>\n<li>katz of source<\/li>\n<li>katz of dest<\/li>\n<li>hubs of source<\/li>\n<li>hubs of dest<\/li>\n<li>authorities_s of source<\/li>\n<li>authorities_s of dest<\/li>\n<\/ol>","7611a1c5":"## 2.2 Cosine distance","98fbbb45":"## 4.5 Katz Centrality:\nhttps:\/\/en.wikipedia.org\/wiki\/Katz_centrality\n\nhttps:\/\/www.geeksforgeeks.org\/katz-centrality-centrality-measure\/\n Katz centrality computes the centrality for a node \n    based on the centrality of its neighbors. It is a \n    generalization of the eigenvector centrality. The\n    Katz centrality for node `i` is\n \n$$x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta,$$\nwhere `A` is the adjacency matrix of the graph G \nwith eigenvalues $$\\lambda$$.\n\nThe parameter $$\\beta$$ controls the initial centrality and \n\n$$\\alpha < \\frac{1}{\\lambda_{max}}.$$","600d248d":"\\begin{equation}\nCosineDistance = \\frac{|X\\cap Y|}{|X|\\cdot|Y|} \n\\end{equation}","a0304b0e":"# **Preferential Attachments**","7321d4a5":"## 3. Ranking Measures","f9f3b448":"### Business objectives and constraints:  \n- No low-latency requirement.\n- Probability of prediction is useful to recommend ighest probability links","073910b7":"> we have a cold start problem here","94181ae0":"<p style=\"font-size:32px;text-align:center\"> <b>Social network Graph Link Prediction - Facebook Challenge<\/b> <\/p>","3aac313f":"## 5.2 Adding a set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>jaccard_followers<\/li>\n<li>jaccard_followees<\/li>\n<li>cosine_followers<\/li>\n<li>cosine_followees<\/li>\n<li>num_followers_s<\/li>\n<li>num_followees_s<\/li>\n<li>num_followers_d<\/li>\n<li>num_followees_d<\/li>\n<li>inter_followers<\/li>\n<li>inter_followees<\/li>\n<\/ol>","9e7cf67d":"# 4. Other Graph Features","8986fe2f":"## 5.2 Adding a set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>jaccard_followers<\/li>\n<li>jaccard_followees<\/li>\n<li>cosine_followers<\/li>\n<li>cosine_followees<\/li>\n<li>num_followers_s<\/li>\n<li>num_followees_s<\/li>\n<li>num_followers_d<\/li>\n<li>num_followees_d<\/li>\n<li>inter_followers<\/li>\n<li>inter_followees<\/li>\n<\/ol>","90591f3f":"\\begin{equation}\nW = \\frac{1}{\\sqrt{1+|X|}}\n\\end{equation}","5ab1b88f":"### Problem statement: \nGiven a directed social graph, have to predict missing links to recommend users (Link Prediction in graph)","6dc5c11d":"### Mapping the problem into supervised learning problem:\n- Generated training samples of good and bad links from given directed graph and for each link got some features like no of followers, is he followed back, page rank, katz score, adar index, some svd fetures of adj matrix, some weight features etc. and trained ml model based on these features to predict link. \n- Some reference papers and videos :  \n    - https:\/\/www.cs.cornell.edu\/home\/kleinber\/link-pred.pdf\n    - https:\/\/www3.nd.edu\/~dial\/publications\/lichtenwalter2010new.pdf\n    - https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/2594\/supervised_link_prediction.pdf\n    - https:\/\/www.youtube.com\/watch?v=2M77Hgy17cg","64373c73":"## 4.4 Is persion was following back:","bf074a31":"# 5. Featurization","023b67cb":"## 2.1 Jaccard Distance:\nhttp:\/\/www.statisticshowto.com\/jaccard-index\/","ec889ca8":"## 4.6 Hits Score\nThe HITS algorithm computes two numbers for a node. Authorities estimates the node value based on the incoming links. Hubs estimates the node value based on outgoing links.\n\nhttps:\/\/en.wikipedia.org\/wiki\/HITS_algorithm","9ddafe2d":"In order to determine the similarity of nodes, an edge weight value was calculated between nodes. Edge weight decreases as the neighbor count goes up. Intuitively, consider one million people following a celebrity on a social network then chances are most of them never met each other or the celebrity. On the other hand, if a user has 30 contacts in his\/her social network, the chances are higher that many of them know each other. \n`credit` - Graph-based Features for Supervised Link Prediction\nWilliam Cukierski, Benjamin Hamner, Bo Yang","2f0a82ba":"## 5. 1 Reading a sample of Data from both train and test","e89f3b83":"https:\/\/networkx.github.io\/documentation\/networkx-1.10\/reference\/generated\/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html\n\nPageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links.\n\n<img src='PageRanks-Example.jpg'\/>\n\nMathematical PageRanks for a simple network, expressed as percentages. (Google uses a logarithmic scale.) Page C has a higher PageRank than Page E, even though there are fewer links to C; the one link to C comes from an important page and hence is of high value. If web surfers who start on a random page have an 85% likelihood of choosing a random link from the page they are currently visiting, and a 15% likelihood of jumping to a page chosen at random from the entire web, they will reach Page E 8.1% of the time. <b>(The 15% likelihood of jumping to an arbitrary page corresponds to a damping factor of 85%.) Without damping, all web surfers would eventually end up on Pages A, B, or C, and all other pages would have PageRank zero. In the presence of damping, Page A effectively links to all pages in the web, even though it has no outgoing links of its own.<\/b>","87917531":"# 2. Similarity measures","8278886b":"## 3.1 Page Ranking\n\nhttps:\/\/en.wikipedia.org\/wiki\/PageRank\n","c0251b2c":"# 1. Exploratory Data Analysis","d5367486":"# 1. Reading Data","7d3f746d":"## 4.1 Shortest path:","95edf7b6":"<p style=\"font-size:32px;text-align:center\"> <b>Social network Graph Link Prediction - Facebook Challenge<\/b> <\/p>","3977bed5":"Training","a2dde94a":"## 1.1 No of followers for each person","d037503a":"# 2. Similarity measures","067afbf1":"## 5. 1 Reading a sample of Data from both train and test","9d06dae7":"## 4.2 Checking for same community","b5ac06ab":"# 2. Posing a problem as classification problem ","b7c7bdae":"\n1. Preferential Attachment \nhttp:\/\/be.amazd.com\/link-prediction\/ <br>\n2. svd_dot\nhttps:\/\/storage.googleapis.com\/kaggle-forum-message-attachments\/2594\/supervised_link_prediction.pdf<br>\n","c2f4ece3":"### Data Overview\nTaken data from facebook's recruting challenge on kaggle https:\/\/www.kaggle.com\/c\/FacebookRecruiting  \ndata contains two columns source and destination eac edge in graph \n    - Data columns (total 2 columns):  \n    - source_node         int64  \n    - destination_node    int64  ","7f10196f":"Computing num_follwers_d","db28370d":"#### Weight Features","43daa286":"\n# Conclution\n\n","171f79df":"Getting Shortest path between twoo nodes, if nodes have direct path i.e directly connected then we are removing that edge and calculating path. ","e28c807d":"# 5. Featurization","352763de":"## 2.2 Cosine distance","06129069":"## 1.3 both followers + following ","abdb5d94":"## 5.3 Adding new set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>adar index<\/li>\n<li>is following back<\/li>\n<li>belongs to same weakly connect components<\/li>\n<li>shortest path between source and destination<\/li>\n<\/ol>","e8546dd2":"it is directed graph so calculated Weighted in and Weighted out differently","c0b2b461":"## 2.1 Generating some edges which are not present in graph for supervised learning  \nGenerated Bad links from graph which are not in graph and whose shortest path is greater than 2. ","075af991":"## 4.3 Adamic\/Adar Index:\nAdamic\/Adar measures is defined as inverted sum of degrees of common neighbours for given two vertices.\n$$A(x,y)=\\sum_{u \\in N(x) \\cap N(y)}\\frac{1}{log(|N(u)|)}$$","3ff8b4d1":"adding SV_DOT feature","c329872f":"# 1. Reading Data","d2b3d6e2":"## 4.6 Hits Score\nThe HITS algorithm computes two numbers for a node. Authorities estimates the node value based on the incoming links. Hubs estimates the node value based on outgoing links.\n\nhttps:\/\/en.wikipedia.org\/wiki\/HITS_algorithm","d6c3b5df":"### Performance metric for supervised learning:  \n- Both precision and recall is important so F1 score is good choice\n- Confusion matrix","f3d137d8":"## 4.1 Shortest path:","3001e5d9":"\\begin{equation}\nW = \\frac{1}{\\sqrt{1+|X|}}\n\\end{equation}","b3109238":"## 4.3 Adamic\/Adar Index:\nAdamic\/Adar measures is defined as inverted sum of degrees of common neighbours for given two vertices.\n$$A(x,y)=\\sum_{u \\in N(x) \\cap N(y)}\\frac{1}{log(|N(u)|)}$$","f0a345f1":"## 1.2 No of people each person is following","7296799f":"\\begin{equation}\nCosineDistance = \\frac{|X\\cap Y|}{|X|\\cdot|Y|} \n\\end{equation}","69f5fc4f":"## 3. Ranking Measures","d3bcc82f":"## 4.2 Checking for same community","c509514c":"In order to determine the similarity of nodes, an edge weight value was calculated between nodes. Edge weight decreases as the neighbor count goes up. Intuitively, consider one million people following a celebrity on a social network then chances are most of them never met each other or the celebrity. On the other hand, if a user has 30 contacts in his\/her social network, the chances are higher that many of them know each other. \n`credit` - Graph-based Features for Supervised Link Prediction\nWilliam Cukierski, Benjamin Hamner, Bo Yang"}}