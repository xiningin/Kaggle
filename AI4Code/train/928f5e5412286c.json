{"cell_type":{"7ab2a395":"code","4deec59e":"code","c52ca675":"code","cfdff2a2":"code","2cf170af":"code","2fb17330":"code","6097b6b0":"code","a3c0c141":"code","41a3e43a":"code","0a66f89a":"code","42a3be1b":"code","af2c330a":"code","0cad9c64":"code","2bd11df7":"code","e07c5b3f":"code","7ed45270":"code","ac69d925":"code","a7514b43":"code","e6612bc5":"code","56556c0c":"code","b4ae8bb5":"code","237303c3":"code","9602fa64":"code","2deca80c":"code","2369df66":"code","5807a784":"code","79bb9ad3":"code","0241b281":"code","e1b99815":"markdown","dca63b18":"markdown","66a14ff8":"markdown","d4bdba80":"markdown","23c5f81a":"markdown","3d3aa369":"markdown","0593cd59":"markdown","5f8a7fd3":"markdown","85bc97fd":"markdown","348ccace":"markdown"},"source":{"7ab2a395":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4deec59e":"import pandas_profiling\nimport sys\nimport math\nimport numpy.random as nr\nimport scipy.stats as ss\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.preprocessing as skpe\nimport sklearn.model_selection as ms\nimport sklearn.metrics as sklm\nimport sklearn.linear_model as lm\nfrom sklearn import tree\nfrom sklearn import neighbors\nfrom sklearn import ensemble\nimport statsmodels.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std","c52ca675":"path=\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\"\ndf=pd.read_csv(path)\ndf.head()","cfdff2a2":"df.info()","2cf170af":"df.describe()","2fb17330":"# A brief overview and detailed EDA of this dataset\npandas_profiling.ProfileReport(df)","6097b6b0":"# Histogram for univariate analysis\nfig, axs = plt.subplots(ncols=3, nrows=4, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in df.items():\n    sns.distplot(v,kde=False,rug=True,ax=axs[index]) # rug is used to see the frequency density on the x-scale\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","a3c0c141":"# Box Plots to detect outliers\nfig, axs = plt.subplots(ncols=6, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in df.items():\n    sns.boxplot(y=k, data=df, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","41a3e43a":"# Checking correlation bw different variables\nplt.figure(figsize=(18,18))\nsns.heatmap(df.corr(),vmax=.7,cbar=True,annot=True)  ","0a66f89a":"fig, (axis1,axis2) = plt.subplots(2,1,figsize=(10,8))\n\nsns.barplot(x='quality', y='fixed acidity', data=df, ax=axis1)\nsns.barplot(x='quality', y='volatile acidity', data=df, ax=axis2)","42a3be1b":"fig, (axis1,axis2) = plt.subplots(2,1,figsize=(10,8))\n\nsns.barplot(x='quality', y='citric acid', data=df, ax=axis1)\nsns.barplot(x='quality', y='residual sugar', data=df, ax=axis2)","af2c330a":"fig, (axis1,axis2) = plt.subplots(2,1,figsize=(10,8))\n\nsns.barplot(x='quality', y='chlorides', data=df, ax=axis1)\nsns.barplot(x='quality', y='total sulfur dioxide', data=df, ax=axis2)","0cad9c64":"fig, (axis1,axis2) = plt.subplots(2,1,figsize=(10,8))\n\nsns.barplot(x='quality', y='free sulfur dioxide', data=df, ax=axis1)\nsns.barplot(x='quality', y='total sulfur dioxide' , data=df, ax=axis2)","2bd11df7":"fig,(axis1,axis2) = plt.subplots(2,1,figsize=(10,8))\n\nsns.barplot(x='quality', y='pH', data=df, ax=axis1)\nsns.barplot(x='quality', y='sulphates', data=df, ax=axis2)","e07c5b3f":"sns.barplot(x='quality', y='alcohol' , data=df)","7ed45270":"# Making bins in order to classify these bins as good or bad (binary classificaion)\nbins = (2, 6, 8)\ngroups = ['Bad', 'Good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = groups)\n\n\n# Encoding these binary gruoups into 0, 1, 2, etc.(categorical to numerical, because most of the machine learning models are not able to interpret categorical varaibles)\n# For this purpose we use label encoder\nle = skpe.LabelEncoder()\n\n\n# Fitting and transforming these features\ndf['quality'] = le.fit_transform(df['quality'])\n\nsns.countplot(df['quality'])","ac69d925":"# Seperating dependent and independent variables \nX = df.drop('quality', axis = 1)\ny = df['quality']","a7514b43":"# Splitting the data into train and test set\nX_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size = 0.25, random_state = 111,stratify=y)","e6612bc5":"# Feature scaling\nscaler=skpe.StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","56556c0c":"clf_rfc = ensemble.RandomForestClassifier(n_estimators=300)\nclf_rfc.fit(X_train, y_train)\npred_rfc = clf_rfc.predict(X_test)\n\n# Model performance\nprint(sklm.classification_report(y_test, pred_rfc))","b4ae8bb5":"def print_metrics(labels, scores):\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n\n\n    \nprint_metrics(y_test, pred_rfc) ","237303c3":"log_reg=lm.LogisticRegression(penalty='l2')\nlog_reg.fit(X_train,y_train)\nprobabilities = log_reg.predict_proba(X_test)  # Predicting probablities of the quality of wine","9602fa64":"# Function to classify the wine as good or bad based on the probability (if it is less than 0.5, then it is bad otherwise it is good)\ndef score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\nscores = score_model(probabilities, 0.5)\nprint(np.array(scores[:15]))\nprint(np.array(y_test[:15]))","2deca80c":"def print_metrics(labels, scores):\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n\n\n    \nprint_metrics(y_test, scores)   ","2369df66":"# Plotting ROC-AUC curve\n# This curve is used measure the performance of the model by area under the curve.\ndef plot_auc(labels, probs):\n    ## Compute the false positive rate, true positive rate\n    ## and threshold along with the AUC\n    fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1])\n    auc = sklm.auc(fpr, tpr)\n    \n    ## Plot the result\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \nplot_auc(y_test, probabilities) ","5807a784":"# We can also use GridSearchCV but it takes a lot of time and searches through all hyperparameters even though they are not necessarily required.\n# Hence, RandomizedSearchCV is possibly the best method you could use\nparam_dist={'n_estimators':[100,200,300,400,500,600],'criterion':['gini','entropy'],'max_depth':ss.randint(1,15),'max_features':ss.randint(1,9),'min_samples_leaf':ss.randint(1,9)}\nclf=ensemble.RandomForestClassifier()\nclf_cv=ms.RandomizedSearchCV(clf,param_distributions=param_dist,cv=5)\nclf_cv.fit(X_train,y_train)\nprint(\"Tuned Random Forest Parameters: {}\".format(clf_cv.best_params_)) \nprint(\"Best score is {}\".format(clf_cv.best_score_)) ","79bb9ad3":"# Putting these hyperparameters into our model\nclf=ensemble.RandomForestClassifier(criterion='entropy',max_depth=12,max_features=2,min_samples_leaf=5,n_estimators=300)\nclf.fit(X_train,y_train)","0241b281":"# Now let's see how this model works on an unseen data.For this we are using K-Fold CrossValidation\nrfc_cv = ms.cross_val_score(estimator = clf , X = X_train, y = y_train, cv = 10)\nrfc_cv.mean()","e1b99815":"**Logistic regression gives an accuracy of 86%**","dca63b18":"# Random Forest Classifier","66a14ff8":"# Logistic Regression","d4bdba80":"**Bivariate Analysis**","23c5f81a":"# Cross-Validation using Random Forest as estimator","3d3aa369":"**Hyperparamter Tuning **","0593cd59":"**Accuracy improved from 89% to 91% using Random Forest**","5f8a7fd3":"**Univariate Analysis**","85bc97fd":"**Random Forest gives the accuracy of 89% **","348ccace":"**Hence, our model is generalizing well on unseen data**"}}