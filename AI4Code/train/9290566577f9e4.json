{"cell_type":{"9ef961f5":"code","c453365a":"code","489cb73b":"code","ae37b5c9":"code","bd1f1e4b":"code","a0e55b18":"code","86ebc655":"code","c13ffb26":"code","64a9273c":"code","5229df95":"code","bb916c85":"code","4518dc2b":"code","4564ab2b":"code","08479208":"code","79f35906":"code","cf1f99d7":"markdown","4e7f945a":"markdown","94cfd48c":"markdown","bf8bd776":"markdown","3cd0e659":"markdown"},"source":{"9ef961f5":"import pandas as pd\nimport numpy as np\npd.set_option('max_rows', 10000)\npd.set_option('max_columns', 20000)\nfrom IPython.display import display\n# reading data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndisplay(train.head())\ndisplay(train.describe())\nprint(train.shape)\ntrain.info()","c453365a":"# create copy before messing\ntrain_cleaned = train.copy()\ntest_cleaned = test.copy()\n# after inspection, we can notice high std condition like square feet, mostly areas, while some need manual tuning like years\n# additionally, log transform Sale Price might help since its variance is huge\n# but rmb to transform back when prediction is done\nrescale_SF = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n               'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n              'GrLivArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n              'ScreenPorch', 'PoolArea', 'MiscVal']\ndef SF_rescale(data):\n    for col in rescale_SF:\n        data[col] = data[col].apply(lambda x: np.log(1+x))\n    return data\ndef train_SalePrice_rescale(data):\n    data['SalePrice'] = data['SalePrice'].apply(lambda x: np.log(1+x))\n    return data\n# minus 1900 to data.GarageYrBlt since min is 1900\n# minus 1872 to data.YearBuilt \n# minus 1950 to data.YearRemodAdd\n# minus 2006 to data.YrSold\ndef year_rescale(data):\n    data['GarageYrBlt'] = data.GarageYrBlt - 1900\n    data['YearBuilt'] = data.YearBuilt - 1872\n    data['YearRemodAdd'] = data.YearRemodAdd - 1950\n    data['YrSold'] = data.YrSold - 2006\n    return data\n# auto data cleaning since data contains too many columns\ndef data_cleaning(data):\n    for col_n, row_ind in data.iteritems():\n        # convert categorical to numeric\n        if data[col_n].dtypes == 'object':\n            data[col_n] = data[col_n].factorize()[0]\n        # check nos of NAN to decide what to fill\n        if data[col_n].count() \/ data.shape[0] > 0.1:\n            data[col_n].fillna('-1', inplace=True)\n        else:\n            median = data[col_n].median()\n            data[col_n].fillna(median, inplace=True)\n        # one hot encode non-ordinal and non-binary categorical columns\n        # however, its better to use groupby and plot to manually extract signal before creating\n        # too much dimensionality \n    print(data.shape)\n    return data\n\ntrain_cleaned = SF_rescale(train_cleaned)\ntrain_cleaned = train_SalePrice_rescale(train_cleaned)\ntest_cleaned = SF_rescale(test_cleaned)\n\ntrain_cleaned = year_rescale(train_cleaned)\ntest_cleaned = year_rescale(test_cleaned)\n\ntrain_cleaned = data_cleaning(train_cleaned)\ntest_cleaned = data_cleaning(test_cleaned)\n\ntrain_cleaned = train_cleaned.astype(dtype=np.float64)\ntest_cleaned = test_cleaned.astype(dtype=np.float64)","489cb73b":"train_cleaned.info()","ae37b5c9":"# lets apply minMax scaler to train & test before PCA\nfor col in train_cleaned.columns.tolist()[1:-1]:\n    maxi = max(max(train_cleaned[col]), max(test_cleaned[col]))\n    mini = min(min(train_cleaned[col]), min(test_cleaned[col]))\n    RANGE = maxi- mini\n    train_cleaned[col] = (train_cleaned[col] - mini) \/ RANGE\n    test_cleaned[col] = (test_cleaned[col] - mini) \/ RANGE","bd1f1e4b":"test_cleaned.describe()","a0e55b18":"# apply PCA to reduce dimensionality, since nos of rows is much less than that\nfrom sklearn.decomposition import PCA\nbins = 5\nnos_col = train_cleaned.shape[1]\nn_trials = np.arange(nos_col\/\/5, nos_col-2, nos_col\/\/5)\nn_explained_variance = []\n# rmb pca is applied to both train and test set so make sure u fit both\nall_data = np.concatenate((train_cleaned.drop(['SalePrice', 'Id'], axis=1), test_cleaned.drop(['Id'], axis=1)))\nfor n_components in n_trials:\n    pca = PCA(n_components=n_components)\n    pca.fit(all_data)\n    n_explained_variance.append(sum(pca.explained_variance_ratio_)*100)\n# plot pca against n_components\nimport matplotlib.pyplot as plt\nplt.plot(n_trials, n_explained_variance)\nprint(n_trials)\nprint(n_explained_variance)","86ebc655":"# apply PCA\npca = PCA(n_components=40)\nall_data = np.concatenate((train_cleaned.drop(['SalePrice', 'Id'], axis=1), test_cleaned.drop(['Id'], axis=1)))\npca.fit(all_data)\ntrain_PCA = pca.transform(train_cleaned.drop(['SalePrice', 'Id'], axis=1))\ntest_PCA = pca.transform(test_cleaned.drop(['Id'], axis=1))\ntrain_PCA = pd.DataFrame(train_PCA)\ntest_PCA = pd.DataFrame(test_PCA)\n# add back SalePrice after PCA\ntrain_PCA['SalePrice'] = train_cleaned.SalePrice\n# lets check for outliners and scale of our data set after PCA\nprint(train_PCA.shape)\ntrain_PCA.describe()","c13ffb26":"# create validation from train set for performance testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(train_cleaned.iloc[:, 1:-1], train_cleaned.iloc[:, -1], test_size=0.3, random_state=42, shuffle=True) \n# print(X_train.shape, X_val.shape)\n# lets get a baseline accuracy and get feature importance for GBT \nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error as mse\nGBR = GradientBoostingRegressor()\nGBR.fit(X_train, Y_train)\ntrain_mse_error = mse(Y_train, GBR.predict(X_train))\nval_mse_error = mse(Y_val, GBR.predict(X_val))\nprint('Inverse-transform back it to the unit of Price since we used log(1+x).\\n')\nprint('original_training RMSE: {}'.format(train_mse_error**0.5))\nprint('original_validation RMSE: {}'.format(val_mse_error**0.5))","64a9273c":"# plot feature importance for feature engineering\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20,10))\nplt.title('feature importance of GBR')\nplt.bar(train_cleaned.columns.tolist()[1:-1], GBR.feature_importances_)\nplt.show()","5229df95":"X_train, X_val, Y_train, Y_val = train_test_split(train_PCA.iloc[:, :-1], train_PCA.iloc[:, -1], test_size=0.3, random_state=42, shuffle=True) \n# print(X_train.shape, X_val.shape)\n# lets get a baseline accuracy and get feature importance for GBT \nGBR_PCA = GradientBoostingRegressor()\nGBR_PCA.fit(X_train, Y_train)\ntrain_mse_error = mse(Y_train, GBR_PCA.predict(X_train))\nval_mse_error = mse(Y_val, GBR_PCA.predict(X_val))\nprint('pca_training RMSE: {}'.format(train_mse_error**0.5))\nprint('pca_validation RMSE: {}'.format(val_mse_error**0.5))","bb916c85":"# plot feature importance for feature engineering\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20,10))\nplt.title('feature importance of GBR_PCA')\nplt.bar(train_PCA.columns.tolist()[:-1], GBR_PCA.feature_importances_)\nplt.show()","4518dc2b":"picked_PCA_features = [0, 3, 24]\ntrain_PCA[picked_PCA_features].hist()","4564ab2b":"# also prepare a set of input that its SalePrice is not scaled\ntrain_NS = train_cleaned.copy()\ntrain_NS.SalePrice = train_NS.SalePrice.apply(lambda x: np.exp(x)-1)","08479208":"# 1st level: GBT_R, RF_R, SVR, LR. KNN_R \n# 2nd level: Linear Model\nimport sklearn.ensemble as ensemble\nimport sklearn.neighbors as neighbors\nimport sklearn.linear_model as linear\nimport sklearn.svm as svm\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn import tree\nfrom sklearn import neural_network\n\nmodels = [[ensemble.GradientBoostingRegressor(),\n           ensemble.RandomForestRegressor(),\n           svm.SVR(gamma='auto'),\n           linear.BayesianRidge(),\n           GaussianProcessRegressor(),\n           tree.DecisionTreeRegressor(),\n           neural_network.MLPRegressor(),\n           neighbors.KNeighborsRegressor()], \n         linear.BayesianRidge()]\n\n# creates dataframe to store first level result\nfirst_level_B = pd.DataFrame()\nfirst_level_C = pd.DataFrame()\ni = 0\nfor AB_C in [(train_PCA[picked_PCA_features], test_PCA[picked_PCA_features]),\n            (train_PCA.iloc[:,:-1], test_PCA.iloc[:,:]),\n            (train_cleaned.iloc[:,1:-1], test_cleaned.iloc[:,1:]),\n            (train_NS.iloc[:,1:-1], test_cleaned.iloc[:,1:])]:\n    AB, C = AB_C[0], AB_C[1]\n    A_x, B_x, A_y, B_y = train_test_split(AB, train_PCA.iloc[:,-1], test_size=0.1, random_state=42)\n    for model in models[0]:\n        model.fit(A_x, A_y)\n        first_level_B[type(model).__name__+'_'+str(i)] = model.predict(B_x)\n        first_level_C[type(model).__name__+'_'+str(i)] = model.predict(C)\n    i += 1\n    \nfirst_level_B['B_y'] = B_y.to_numpy()\ndisplay(first_level_B.head())\nfirst_level_C.head()","79f35906":"# perform second level stacking\nfinal_model = models[1]\nfinal_model.fit(first_level_B.iloc[:,:-1], first_level_B.iloc[:,-1])\n# mse\ntraining_prediction = final_model.predict(first_level_B.iloc[:,:-1])\ntraining_mse = mse(first_level_B.iloc[:,-1], training_prediction)\nprint('2nd level model RMSE: {}'.format(training_mse**0.5))\n\nsubmission = final_model.predict(first_level_C.iloc[:,:])\n# transform it back\nsubmission = np.exp(submission) - 1\nsubmission_df = pd.DataFrame({'Id':test_cleaned.Id.astype(np.int32) , 'SalePrice':submission})\nsubmission_df.to_csv('.\/submission.csv', index=False)\nfrom IPython.display import FileLink\nFileLink(r'.\/submission.csv')","cf1f99d7":"# **Step 1: EDA**\n1. cleaning the dataset e.g scale, encoding, Null, duplicate features\n2. checking train vs test distribution as this can affect our validation setup\n3. explore features and feature interaction by plots","4e7f945a":"* It seems that just with 40 principal components we can already explained over 90 percentage of variance in train & test data.*","94cfd48c":"# **Step 2: Advanced Feature engineering**\n1. Engineering based on domain knowledge\n2. decompose dense feature and more feature interaction\n3. mean encoding or target encoding (optional)\n4. apply PCA or tsne to model with high dimension (if dimensionality is high)","bf8bd776":"*As we can see from the above graph after PCA is done, if we just pick top 3 features, its already very significant.*\n*lets try this simplest model ever with just 3 PCA features.*","3cd0e659":"# **3. Stacking multiple classes of models with different input**"}}