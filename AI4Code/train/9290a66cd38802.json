{"cell_type":{"df2c78b2":"code","3eccd167":"code","d19ae401":"code","8d17e2bc":"code","d1adaaa6":"code","21725b7a":"code","c345d064":"code","ece4f3b1":"code","ccd3e2ba":"code","61fefede":"code","7c14fda2":"code","237a8587":"code","77946b00":"code","c5b751b7":"code","99aee721":"code","a17aaa1e":"code","911dc90e":"code","a7056493":"code","5f958825":"code","823f2a13":"code","1378826e":"code","67dd98ba":"markdown","60f8da8d":"markdown","10495d05":"markdown","af2b9443":"markdown","8aed1544":"markdown","bad581cb":"markdown","469815e6":"markdown","e58ff87e":"markdown","50f5c4eb":"markdown","97e69297":"markdown","19077afc":"markdown","3b023f15":"markdown","bfbf6f83":"markdown","1eaab8e3":"markdown","b3f4ed2f":"markdown"},"source":{"df2c78b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3eccd167":"data =  pd.read_csv('\/kaggle\/input\/malicious-and-benign-websites\/dataset.csv')\ndata.head()","d19ae401":"data.describe(include='all')","8d17e2bc":"data = data.drop(labels=['URL','WHOIS_REGDATE','WHOIS_UPDATED_DATE'], axis='columns')","d1adaaa6":"print(data.isnull().sum())\n\ndata[pd.isnull(data).any(axis='columns')]","21725b7a":"processed_data =  data.interpolate()\nprint(processed_data.isnull().sum())","c345d064":"max_value = processed_data['SERVER'].value_counts().idxmax()\n\nprint('Highest frequency value:',max_value)\n\nprocessed_data['SERVER'].fillna(max_value, inplace=True)\n\nprint(processed_data.isnull().sum())","ece4f3b1":"knn_data = pd.get_dummies(processed_data, prefix_sep=\"_\")\nknn_data.head()","ccd3e2ba":"X = knn_data.drop(labels='Type', axis='columns')\ny = knn_data['Type']\nX.head()","61fefede":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nprint(\"Training size: %d\" % len(y_train))\nprint(\"Test size    : %d\" % len(y_test))\n","7c14fda2":"from sklearn.neighbors import KNeighborsClassifier\nk=10\n\nclf = KNeighborsClassifier(n_neighbors=k, p=2, weights='distance')\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Print results for 30 test data points:\")\nprint(\"Predicted labels: \", y_pred[30:60])\nprint(\"Ground truth    : \", y_test.to_numpy()[30:60])\n\nprint(\"Accuracy of %d NN: %.2f %%\" % (k, 100 * accuracy_score(y_test.to_numpy(), y_pred)))\nprint('Classification Report:\\n{}\\n'.format(classification_report(y_test.to_numpy(),clf.predict(X_test))))\n","237a8587":"y_pred_proba = clf.predict_proba(X_test)\nprint(y_pred_proba[30:60]*100)","77946b00":"nb_data = pd.get_dummies(processed_data, prefix_sep=\"_\")\nnb_data.head()","c5b751b7":"X = nb_data.drop(labels='Type', axis='columns')\ny = nb_data['Type']\nX.head()","99aee721":"X_train, X_test, y_train, y_test = train_test_split(nb_data, y, test_size=0.3)\n\nprint(\"Training size: %d\" % len(y_train))\nprint(\"Test size    : %d\" % len(y_test))\n","a17aaa1e":"from sklearn.naive_bayes import GaussianNB\nnb_clf= GaussianNB()\nnb_clf.fit(X_train, y_train)\n\ny_pred = nb_clf.predict(X_test)\n\nprint(\"Print results for 30 test data points:\")\nprint(\"Predicted labels: \", y_pred[30:60])\nprint(\"Ground truth    : \", y_test.to_numpy()[30:60])\n\nprint(\"Accuracy of GNB: %.2f %%\" % ( 100 * accuracy_score(y_test.to_numpy(), y_pred)))\nprint('Classification Report:\\n{}\\n'.format(classification_report(y_test.to_numpy(),nb_clf.predict(X_test))))","911dc90e":"from sklearn.naive_bayes import MultinomialNB\nnb_clf= MultinomialNB()\nnb_clf.fit(X_train, y_train)\n\ny_pred = nb_clf.predict(X_test)\n\nprint(\"Print results for 30 test data points:\")\nprint(\"Predicted labels: \", y_pred[30:60])\nprint(\"Ground truth    : \", y_test.to_numpy()[30:60])\n\nprint(\"Accuracy of MNB: %.2f %%\" % ( 100 * accuracy_score(y_test.to_numpy(), y_pred)))\nprint('Classification Report:\\n{}\\n'.format(classification_report(y_test.to_numpy(),nb_clf.predict(X_test))))","a7056493":"from sklearn.naive_bayes import BernoulliNB\nnb_clf= BernoulliNB()\nnb_clf.fit(X_train, y_train)\n\ny_pred = nb_clf.predict(X_test)\n\nprint(\"Print results for 30 test data points:\")\nprint(\"Predicted labels: \", y_pred[30:60])\nprint(\"Ground truth    : \", y_test.to_numpy()[30:60])\n\nprint(\"Accuracy of BNB: %.2f %%\" % ( 100 * accuracy_score(y_test.to_numpy(), y_pred)))\nprint('Classification Report:\\n{}\\n'.format(classification_report(y_test.to_numpy(),nb_clf.predict(X_test))))","5f958825":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_data  = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.4)\n\nprint(\"Training size: %d\" % len(y_train))\nprint(\"Test size    : %d\" % len(y_test))","823f2a13":"from sklearn.naive_bayes import MultinomialNB\nnb_clf= MultinomialNB()\nnb_clf.fit(X_train, y_train)\n\ny_pred = nb_clf.predict(X_test)\n\nprint(\"Print results for 30 test data points:\")\nprint(\"Predicted labels: \", y_pred[30:60])\nprint(\"Ground truth    : \", y_test.to_numpy()[30:60])\n\nprint(\"Accuracy of MNB: %.2f %%\" % ( 100 * accuracy_score(y_test.to_numpy(), y_pred)))\nprint('Classification Report:\\n{}\\n'.format(classification_report(y_test.to_numpy(),nb_clf.predict(X_test))))","1378826e":"import matplotlib.pyplot as plt\nimport time\nfrom sklearn.model_selection import KFold\n\nmodels = []\nmodels.append(('KNN', KNeighborsClassifier(n_neighbors=k, p=2, weights='distance'), 0))\nmodels.append(('GNB', GaussianNB(), 0))\nmodels.append(('MNB', MultinomialNB(), 0))\nmodels.append(('BNB', BernoulliNB(), 0))\n# models with normalization on numerical columns\nmodels.append(('KNN-S', KNeighborsClassifier(n_neighbors=k, p=2, weights='distance'), 1))\nmodels.append(('GNB-S', GaussianNB(), 1))\nmodels.append(('MNB-S', MultinomialNB(), 1))\nmodels.append(('BNB-S', BernoulliNB(), 1))\n\nresults = []\nnames = []\nrun_times = []\nscoring = 'accuracy'\nfor name, model, scaler in models:\n    start = time.time()\n    kfold = KFold(n_splits=10, random_state=7)\n    if(scaler==1):\n        scaler = MinMaxScaler()\n        scaled_X  = scaler.fit_transform(X)\n        cv_results = cross_val_score(model, scaled_X, y, cv=10, scoring=scoring)\n    else:\n        cv_results = cross_val_score(model, X, y, cv=10, scoring=scoring)\n    stop= time.time()\n    run_times.append(stop-start)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n\nprint( \"Run times: %s\" % (run_times))\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Accuracy Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()\n\n\ny_pos = np.arange(len(names))\nplt.bar(y_pos, run_times, align='center', alpha=0.5)\nplt.xticks(y_pos, names)\nplt.title('Time Comparison')\nplt.show()","67dd98ba":"For missing values, there are 2 ways to treat them:\n\n* Assign a specific value that you think it is reasonable such as NaN = 0 in `CONTENT_LENGTH`\n* Using [interpolate method](https:\/\/en.wikipedia.org\/wiki\/Interpolation) to fill missing values\n\nI choose the second one to fill missing data","60f8da8d":"After the processing, there is still 1 missing value. The reason is: `interpolate()` function in `pd.dataframe` only support `linear` function that only apply in numerical data so for categorial data in `SERVER`column it cannot apply.\n\nSo our solution is we will fill the last one by the highest frequency value in `SERVER`","10495d05":"The accuracy of this one is quite slow. \n\nSo let do the second try with other modals: MultinomialNB and BernoulliNB","af2b9443":"# k-NN\nThe first algorithm we wanna try that is k-NN. For the numerical data, it is easy to calculate a distance by using Euclide measure. However, there is many columns containing categorical data so we cannot apply Euclide measure on those columns.\n\nFor solving this problem, we will apply a simple logic that is:\n* If value on x of 2 vector are not different -> distance between them will be 0\n* Else distance will be 1\n\nAn idea to conduct it that is we try to split a column A with n unique value to n columns with value is 0 or 1\n\nFor example:\n* A = {a1,a2,a3} \n* Data: {id=1, A=a1}\n\nThen we will have\n* Data: {id=1, A_a1=1, A_a2=0, A_a3=0}\n\nAfter spliting, we can apply Euclide measure to calculate a distance for all columns.\n","8aed1544":"First of all,  we do some basic statistics to give an overview","bad581cb":"Then we first try with K = 10 and Euclide measure","469815e6":"First, I will try with Gauusian Naive Bayes modal","e58ff87e":"Next, we have to prepare a training set and test set","50f5c4eb":"Let's see now MultinomialNB is also has accuracy similar to BernoulliNB :)\n\n# Conclusion\n\nBetween 2 algorithm k-NN and Naive Bayes, NB has accuracy is higher than k-NN (especially BernoulliNB) but we have to try different model to choose the suitable one.\n\nBelow is comparison between modals","97e69297":"Next, we will check loss data in our data set by printing out all lines that contain any missing value","19077afc":"From the table, we can see that there are some columns which contain unique values, especially URL that is totally unique.\n\nFor example:\n* URL (100% unique)\n* WHOIS_REGDATE (50% unique) \n* WHOIS_UPDATED_DATE (33% unique) \n\n\nThose columns can make noises and decrease the accuracy because the difference between them will increase a distance in our modal without any reason.\n\nTherefore we have to drop those column to make data more clean","3b023f15":"Accuracy of k=10 is quite good (80%-90%).\n\nAdditional, we want to show a result in probability so that we can try to use exist function as follows","bfbf6f83":"# Naive Bayes\n\nIt becomes more complicated with Naive Bayes\n\nFirst of all, we have to start with same data processing as k-NN \n","1eaab8e3":"# Preprocessing data\n\nImport data from csv file and show 5 first lines as an example","b3f4ed2f":"It seem BernoulliNB has the highest accuracy. I guess it because our data contains large of categorical data so Gaussian is not suiltable in this case \n\nThen we compare between 2 modals BernoulliNB and MultinomialNB:\n\n MultinomialNB care about how many time X value appear in our dataset while BernoulliNB only care about whether X value appear or not.\n So I think some numerical columns make MultinomialNB accurracy decrease by the difference between their unit (for example character vs bytes)\n\nSo the idea to increase the perfomance of MultinomialNB is trying to scale all numerical columns into range (0,1) (normalization) and run MultinomialNB again"}}