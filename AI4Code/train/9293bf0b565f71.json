{"cell_type":{"8bec7a90":"code","bb12d22c":"code","b1ae7621":"code","f92d581c":"code","a8c3fccf":"code","74421c4e":"code","612a5861":"code","5032288b":"code","e45e02db":"code","c275d42a":"code","ccb7dbe6":"code","7456c0c7":"code","2563f3c9":"code","65bea551":"code","46e5f82f":"code","8cfe115b":"code","545edd76":"code","541a66f2":"code","74f1af33":"code","a6ce6d60":"code","dcbc4f58":"code","8a8fde63":"markdown","226319e6":"markdown"},"source":{"8bec7a90":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import initializers\n\nfrom keras.models import Model","bb12d22c":"from tensorflow import keras","b1ae7621":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')","f92d581c":"target = pd.get_dummies(train['target'])","a8c3fccf":"\"\"\"\nAdapted from : \nhttps:\/\/keras.io\/examples\/structured_data\/deep_neural_decision_forests\/\n\"\"\"\nclass Decision_Tree(keras.Model):\n    def __init__(self, depth, num_features, used_features_rate, num_classes):\n        super(Decision_Tree, self).__init__()\n        self.depth = depth\n        self.num_leaves = 2 ** depth\n        self.num_classes = num_classes\n        num_used_features = int(num_features * used_features_rate)    \n        one_hot = np.eye(num_features)                                \n        sampled_feature_indicies = np.random.choice(\n            np.arange(num_features), num_used_features, replace=False\n        )                                                            \n        self.used_features_mask = one_hot[sampled_feature_indicies]   \n        self.pi = tf.Variable(\n            initial_value = tf.random_normal_initializer()(\n            shape = [self.num_leaves, self.num_classes]\n            ),\n            dtype=\"float32\",\n            trainable=True,\n        )\n        \n        self.decision_fn = layers.Dense(\n            units=self.num_leaves, \n            activation=\"sigmoid\", \n            name=\"decision\"\n            )\n\n    def call(self, features):\n        batch_size = tf.shape(features)[0]\n        features = tf.matmul(\n            features, \n            self.used_features_mask, \n            transpose_b=True\n            )  \n        decisions = tf.expand_dims(\n            self.decision_fn(features),\n            axis=2\n            )  \n        decisions = layers.concatenate(\n            [decisions, 1 - decisions],\n            axis=2\n            ) \n\n        mu = tf.ones([batch_size, 1, 1]) \n        begin_idx = 1\n        end_idx = 2\n\n        for level in range(self.depth):\n            mu = tf.reshape(mu, [batch_size, -1, 1])  \n            mu = tf.tile(mu, (1, 1, 2))  \n            level_decisions = decisions[:, begin_idx:end_idx, :]  \n            mu = mu * level_decisions  \n            begin_idx = end_idx\n            end_idx = begin_idx + 2 ** (level + 1)\n\n        mu = tf.reshape(mu, [batch_size, self.num_leaves])  \n        probabilities = keras.activations.softmax(self.pi)  \n        outputs = tf.matmul(mu, probabilities) \n        \n        return outputs","74421c4e":"class Decision_Forest(keras.Model):\n    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n        super(Decision_Forest, self).__init__()\n        self.ensemble = []\n        self.num_classes = num_classes\n        \n        for _ in range(num_trees):\n            self.ensemble.append(\n                Decision_Tree(depth, \n                              num_features,\n                              used_features_rate,\n                              self.num_classes)\n                                )\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        outputs = tf.zeros([batch_size, \n                            num_classes])\n        \n        for tree in self.ensemble:\n            outputs += tree(inputs)\n            \n        outputs \/= len(self.ensemble)\n        \n        return outputs\n\nnum_trees = 20\ndepth = 5\nused_features_rate = 0.5\nnum_classes = 9\nnum_features = 20\nforest_model = Decision_Forest(\n                        num_trees,\n                        depth, \n                        num_features,\n                        used_features_rate,\n                        num_classes\n                        )","612a5861":"metrics = [tf.keras.metrics.CategoricalCrossentropy()]\nloss = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0.0000001, patience=2, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=2, verbose=0,\n    mode='min', min_delta=0.0000001, cooldown=0, min_lr=10e-7)","5032288b":"N_FOLDS = 10\nSEED = 2021\noof_embedding = np.zeros((train.shape[0],9))\npred_embedding = np.zeros((test.shape[0],9))\noof_forest = np.zeros((train.shape[0],9))\npred_forest = np.zeros((test.shape[0],9))\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(train,train.iloc[:,-1])):\n    print(f\"\\n===== TRAINING FOLD {fold} =====\\n\")\n       \n    X_train = train.iloc[:,1:-1].iloc[tr_idx]\n    y_train = target.iloc[tr_idx]\n    X_test = train.iloc[:,1:-1].iloc[ts_idx]\n    y_test = target.iloc[ts_idx]\n    \n\n    #----------NN Model definition ----------\n    \n    inp = layers.Input(shape = (75,))\n    x = layers.Embedding(400, 8, input_length = 256)(inp)\n    x = layers.Flatten()(x)\n    # API is the future imput layer for decision forest :\n    API = layers.Dense(20, \n                       activation='relu',\n                       kernel_initializer='random_uniform',\n                       bias_initializer=initializers.Constant(0.1))(x)\n    x = layers.Dropout(0.3)(API)\n    x = layers.Dense(50, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(20, activation = 'relu')(x)\n    output = layers.Dense(9, activation = 'softmax')(x)\n\n    #----------Models instantiation ---------\n\n    model_embedding = Model(inp,output)\n    model_embedding_without_head = tf.keras.models.Model(inputs=model_embedding.inputs,outputs=API)\n    model_forest = Model(inp,forest_model(API))\n\n    #----------NN Model training ------------\n\n    model_embedding.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n            loss = loss ,\n            metrics = metrics)\n\n    model_embedding.fit(X_train,y_train,\n            validation_data=(X_test,y_test),\n            epochs=50,\n            verbose=0,\n            batch_size = 256,\n            callbacks=[es,plateau])\n     \n    #----------NN Model prediction------------\n    \n    oof_embedding[ts_idx] = model_embedding.predict(X_test)\n    score_embedding = log_loss(y_test, oof_embedding[ts_idx])\n    print(f\"\\nFOLD {fold} Score for NN model {score_embedding}\\n\")\n    pred_embedding += model_embedding.predict(test.iloc[:,1:]) \/ N_FOLDS\n    \n    \n    #----------Model forest training -----------\n    \n    model_forest.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n            loss = loss,\n            metrics = metrics)\n    \n    model_forest.fit(X_train,y_train,\n                    validation_data = (X_test,y_test),\n                    batch_size = 256,\n                    epochs = 50,\n                    verbose = 0,\n                    callbacks = [es,plateau])\n    \n    #----------Model forest prediction------------ \n        \n    oof_forest[ts_idx] = model_forest.predict(X_test)\n    score_forest = log_loss(y_test, oof_forest[ts_idx])\n    print(f\"\\nFOLD {fold} Score for decision forest : {score_forest}\\n\")\n    \n    pred_forest += model_forest.predict(test.iloc[:,1:]) \/ N_FOLDS\n    \nscore_embedding = log_loss(target, oof_embedding)\nprint(f\"\\n=== FINAL SCORE FOR NN MODEL : {score_embedding}===\\n\")   \n\nscore_forest = log_loss(target, oof_forest)\nprint(f\"\\n=== FINAL SCORE FOR DECISION FOREST : {score_forest}===\\n\")  \n","e45e02db":"pred_forest","c275d42a":"import numpy as np\nimport pandas as pd \n\nfrom catboost import CatBoostClassifier, Pool\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","ccb7dbe6":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nId = test['id']","7456c0c7":"le = LabelEncoder()\nencoded = le.fit_transform(train.target)\ntrain = train.assign(target = encoded)\nfor i,c in enumerate(le.classes_):\n    print(i,c)","2563f3c9":"features = train.columns[:75]\ntrain= train.drop_duplicates(subset = features)\ny = train['target']","65bea551":"col = []\nfor cols in train.columns[1:-1]:\n    col.append(cols)","46e5f82f":"X = train.drop(['id','target'],axis=1)","8cfe115b":"cat_params ={\n    'iterations': 10143, \n    'od_wait': 1115, \n    'learning_rate': 0.02248589308956038, \n    'reg_lambda': 86.12583478104304, \n    'subsample': 0.08594672381075155, \n    'random_strength': 29.926327447041192, \n    'depth': 6, \n    'min_data_in_leaf': 30, \n    'leaf_estimation_iterations': 3,\n    'loss_function' : 'MultiClass',\n    'eval_metric' : 'MultiClass',\n    'bootstrap_type' : 'Bernoulli',\n    'leaf_estimation_method' : 'Newton',\n    'random_state' : 42\n    }\n","545edd76":"preds = None\nskf = StratifiedKFold(n_splits=10,random_state=42,shuffle=True)\nl=[]\nn=0\nfor tr_idx, test_idx in skf.split(X.values,y.values):\n    \n    X_tr, X_val = X.values[tr_idx], X.values[test_idx]\n    y_tr, y_val = y.values[tr_idx], y.values[test_idx]\n    \n    model = CatBoostClassifier(**cat_params)\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    if preds is None:\n        preds = model.predict_proba(test[col].values)\n    else:\n        preds += model.predict_proba(test[col].values)\n    preds = preds\/skf.n_splits\n    l.append(log_loss(y_val, model.predict_proba(X_val)))\n    print(n+1,l[n])\n    n+=1","541a66f2":"params_lgbm = {\n                    'learning_rate': 0.03193398814609538, \n                    'max_depth': 100, \n                    'min_child_samples': 263, \n                    'min_child_weight': 0.00038121415013974824,\n                    'objective':'multiclass',\n                    'random_state': 42,\n                    'n_estimators': 10000,\n                    'metric': 'multi_logloss',\n                   }","74f1af33":"preds2 = None\nskf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nll =[]\nn = 0\nfor tr_idx, test_idx in skf.split(X.values,y.values):\n    X_tr, X_val = X.values[tr_idx], X.values[test_idx]\n    y_tr, y_val = y.values[tr_idx], y.values[test_idx] \n    \n    model = LGBMClassifier(**params_lgbm)\n    \n    model.fit(X_tr,y_tr,eval_set = [(X_val,y_val)],early_stopping_rounds = 200,verbose = False)\n    \n    if preds2 is None:\n        preds2 = model.predict_proba(test[col].values)\n    else:\n        preds2 += model.predict_proba(test[col].values)\n    preds2 \/= skf.n_splits\n    ll.append(log_loss(y_val, model.predict_proba(X_val)))\n    print(n+1,ll[n])\n    n += 1","a6ce6d60":"preds_combined = 0.5*preds + 0.2*preds2 + 0.3*pred_forest\ndf_combined = pd.DataFrame(preds_combined,columns=['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\ndf_combined['id'] = Id\ndf_combined = df_combined[['id','Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']]\nfinal_output = df_combined.to_csv('submission.csv',index=False)","dcbc4f58":"preds_combined = 0.5*preds + 0.1*preds2 + 0.4*pred_forest\ndf_combined = pd.DataFrame(preds_combined,columns=['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\ndf_combined['id'] = Id\ndf_combined = df_combined[['id','Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']]\nfinal_output = df_combined.to_csv('submission2.csv',index=False)","8a8fde63":"<h3> Decision Forest building (\"concatenation\" of decision trees)","226319e6":"<h3> Definition model and training + prediction"}}