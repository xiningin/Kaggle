{"cell_type":{"1100261c":"code","087c90f3":"code","11ba97e2":"code","d7250d5b":"code","684907a3":"code","08a3b918":"code","f3a138e7":"code","240def2b":"code","f2c06b34":"code","28a67f75":"code","9e38abb7":"code","a87e851c":"code","9847517c":"code","df942dc2":"code","80806250":"code","74ee6b72":"code","d9767d7f":"code","36aa86d8":"code","0a4fb302":"code","30bfa5ce":"code","b43f01ce":"code","1d1f86d6":"code","5a831fe2":"code","7024ffbf":"code","b5b7bbcc":"code","05ae00bc":"code","b3c07924":"code","4ef59140":"code","1be5c180":"code","3a1c66ef":"code","26e042d9":"code","12767271":"code","ff6b0138":"code","c53c506d":"code","37cfbc2d":"code","0b1e50b0":"code","1ccf30bf":"code","50c0739e":"code","1c4f1398":"code","d0f57790":"code","70d5cf3d":"code","c21272b3":"code","6a269977":"code","42d5d720":"code","cb62ed57":"code","e72b79b9":"code","1c6f64d6":"code","787f25b4":"code","90df4f68":"code","af7106d6":"code","819afc40":"code","79406346":"markdown","c27cf94e":"markdown","cbb2d72c":"markdown","104c041a":"markdown","12e0c2a7":"markdown","2234ad4b":"markdown","a54e131f":"markdown","9d8c4e14":"markdown","3aa7e920":"markdown","aec8063c":"markdown","5fd47b15":"markdown","25e32f62":"markdown","5d39d5b7":"markdown","0a009ff2":"markdown","8dd9f84c":"markdown","93c818d7":"markdown","2077474e":"markdown","018b5dff":"markdown","0a53a1d3":"markdown","58c6e36a":"markdown","f8defc64":"markdown","6f12a2b3":"markdown","3b1bd950":"markdown"},"source":{"1100261c":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report, confusion_matrix","087c90f3":"df=pd.read_csv('..\/input\/titanic\/train.csv')","11ba97e2":"df_test=pd.read_csv('..\/input\/titanic\/test.csv')","d7250d5b":"df.describe(include='all')","684907a3":"df_test.describe(include='all')","08a3b918":"df.info()","f3a138e7":"pd.isnull(df).sum()","240def2b":"df_test.info()","f2c06b34":"pd.isnull(df_test).sum()","28a67f75":"def graficar(var):\n    plt.figure(figsize=(15,5))\n    \n    plt.subplot(1,2,1)\n    plt.title(var)\n    plt.hist(df[var]);\n    \n    plt.subplot(1,2,2)\n    plt.title(var)\n    plt.boxplot(df[var], vert=False);","9e38abb7":"graficar('Pclass')","a87e851c":"graficar('Fare')","9847517c":"df[df['Fare']>500]","df942dc2":"graficar('SibSp')","80806250":"cor=df.corr()","74ee6b72":"sns.heatmap(cor, annot=True)","d9767d7f":"df[['Sex','Survived']].groupby(['Sex']).mean()","36aa86d8":"df[['Survived','Pclass']].groupby(['Pclass']).mean()","0a4fb302":"df[['Fare','Pclass']].groupby(['Pclass']).mean()","30bfa5ce":"df[['Pclass', 'Age']].groupby(['Pclass']).mean()","b43f01ce":"df=df.drop(['Cabin', 'Ticket','Name'], axis=1)\n#testing data also\ndf_test=df_test.drop(['Cabin', 'Ticket','Name'], axis=1)","1d1f86d6":"sex=pd.get_dummies(df['Sex'],drop_first=True)\nembarked=pd.get_dummies(df['Embarked'], drop_first=True)\n#testing data also\nsex2=pd.get_dummies(df_test['Sex'],drop_first=True)\nembarked2=pd.get_dummies(df_test['Embarked'], drop_first=True)","5a831fe2":"df.drop(['Sex','Embarked'],axis=1, inplace=True)\n#testing data also\ndf_test.drop(['Sex','Embarked'],axis=1, inplace=True)","7024ffbf":"df=pd.concat([df,sex,embarked], axis=1)\n#testing data also\ndf_test=pd.concat([df_test,sex2,embarked2], axis=1)","b5b7bbcc":"df.head()","05ae00bc":"df_test.head()","b3c07924":"cor=df.corr()\nsns.heatmap(cor, annot=True);","4ef59140":"df.info()","1be5c180":"df_test.info()","3a1c66ef":"# Funci\u00f3n que recibe el dataframe y cambia los valores nulos de la columna 'Age' por los valores promedio de edad dependiendo de\n# la 'Pclass' del pasajero.\ndef fill_age(df):    \n    cont2=0\n    for i in pd.isna(df.loc[:,'Age']):\n        \n        if i:\n            \n            if df.loc[cont2,'Pclass']==1:\n                df.loc[cont2,'Age']=38\n            \n            elif df.loc[cont2,'Pclass']==2:\n                df.loc[cont2,'Age']=30\n                \n            else:\n                df.loc[cont2,'Age']=25\n            \n        cont2+=1   \n    return df","26e042d9":"df=fill_age(df)\ndf_test=fill_age(df_test)","12767271":"df_test[df_test['Fare'].isna()==True]","ff6b0138":"df_test.loc[152,'Fare']=13.675550","c53c506d":"df.info()","37cfbc2d":"df_test.info()","0b1e50b0":"X=df.drop(['Survived'],axis=1)\ny=df['Survived']","1ccf30bf":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=150)","50c0739e":"modelo_Reg=LogisticRegression()","1c4f1398":"modelo_Reg_E=modelo_Reg.fit(X_train,y_train)","d0f57790":"y_pred=modelo_Reg_E.predict(X_test)","70d5cf3d":"print(confusion_matrix(y_pred,y_test))","c21272b3":"print(classification_report(y_pred,y_test))","6a269977":"modelo_NB=GaussianNB()\nmodelo_NB_E=modelo_NB.fit(X_train,y_train)\ny_pred_NB=modelo_NB_E.predict(X_test)","42d5d720":"print(classification_report(y_test,y_pred_NB))","cb62ed57":"print(confusion_matrix(y_test,y_pred_NB))","e72b79b9":"y_pred2=modelo_Reg_E.predict(df_test)","1c6f64d6":"column=pd.DataFrame(y_pred2,columns=['Survived'])","787f25b4":"final=df_test[['PassengerId']]","90df4f68":"final=pd.concat([final,column],axis=1)    ","af7106d6":"compression_opts = dict(method='zip',\n                     archive_name='out.csv')  \nfinal.to_csv('out.zip', index=False,\n          compression=compression_opts)  ","819afc40":"final","79406346":"Now that we have the information of 'Sex' in a new column 'male' and 'Embarqued' with their new respective columns, we drop 'Sex' and 'Embarqued' and append the new columns.","c27cf94e":"# Logistic Regression\nNow we just have to train our modelo with logistic regression algorithm","cbb2d72c":"Also 62.9630% of the passengers in Pclass 1 , 47.2826% of the passengers in Pclass 2 and 24.2363% of the passengers in Pclass 3 survived.","104c041a":"Before we start with the cleaning of the data we will do some visualization and find paterns.","12e0c2a7":"We use .get_dummies() to separate one column in diferent columns with the information and characteristics of the initial column. So we will apply .get_dummies() to the columns 'Sex' and 'Embarqued' so we can have that information in numbers.","2234ad4b":"We confirm there are no missing values.$$$$ Now we will split the data into training and testing.We will use train_test_split()","a54e131f":"Model is making good predictions.$$$$ We will proceed to create a new model but with the Naive-Bayes algorithm.","9d8c4e14":"From .info() we notice a lot of null data.","3aa7e920":"As we can see, Logistic Regression performs better than Naive-Bayes. So we are going to use Logistic Regression to predict in df_test","aec8063c":"Is the passenger 1044, corresponding to Pclass 3. We got the average fare by Pclass with the .groupby().mean() analysis, so for Pclass 3 the average fare is 13.675550. Using .loc[] we fill that missing value.","5fd47b15":"Now that we fill the null values from the 'Age' column, we will fill the missing value in the 'Fare' column of the df_test dataframe. $$$$ First we detect the passenger with the mising value.","25e32f62":"This tells us that 74.2038% of women and 18.8908% of male in this dataset survived.","5d39d5b7":"Also from the Fare plots and the df.describe(), 75% of the passengers paid a fee less than 31 dolars.","0a009ff2":"Using Seaborn we can plot a heatmap. in this heatmap we notice that 'Survived' is somehow related with 'Fare' giving us to think that those who paid a more expensive fare\nare the ones who were more likely to survive.","8dd9f84c":"Now that we have done the data analysis we will proced with data cleaning; that is, give the best possible data to feed our model.$$$$\nWe sill start by droping the columns that don\u00b4t give us usefull information.\n","93c818d7":"We will now use .groupby().mean() to determine the mean of the passengers in some groups.","2077474e":"With a simple manipulation in pandas we can observe who are the ones paying more tha 500 dolars of fare.","018b5dff":"From the Pclass histogram and boxplot we notice that the largest number of passengers traveled in Pclass 3 ","0a53a1d3":"At this point we are almost done with the cleaning process but notice that there are a lot of null data in the 'Age' column, so we will create a function that fill the null data with the average age of the passenger respective Pclass. The avarege age of the Pclass is:$$$$ 38 for Pclass 1 $$$$ 30 for Pclass 2 $$$$ 25 for Pclass 3 $$$$ We got that information in the .groupby().mean() analysis.$$$$ Also in df_test there is one null value in the 'Fare' column. To fix this we will get the Pclass of the passegner and fill the null value with the averege of the Fare corresponding to the passegners Pclass.","58c6e36a":"New heatmap to visualize the new columns and how they relate","f8defc64":"# Naive Bayes","6f12a2b3":"# Titanic - Machine Learning from Disaster\n## Logistic Regression and Naive Bayes Analysis\n\nWe will start the notebook by importing our work enviroment and reading the traning,testing data.","3b1bd950":"For visualization we will use matplotlib and seaborn libraries."}}