{"cell_type":{"a213e657":"code","abaa33fd":"code","b1dc7071":"code","8da0420a":"code","6acef4d0":"code","50ed63a9":"code","6c1f24f6":"code","1b058c5a":"code","7a787942":"code","d029004c":"code","237bf224":"code","649a6fea":"code","d05160c7":"code","a3db70fb":"code","dff96d1e":"code","4853c2c9":"code","fa589028":"code","33a49a02":"code","07c6af73":"code","9ee83463":"code","8301cf6f":"code","6de56625":"code","f5bd85be":"code","a254a0c0":"code","a1367615":"code","f94e9b91":"code","00ea86c1":"code","d2518153":"code","c0e5d33a":"code","c4f01ce9":"code","0aa012c3":"code","e43f448b":"code","e9613b95":"code","0100e357":"code","fa2c6546":"code","e2706346":"code","4e383208":"markdown","41c011f4":"markdown","8f3c700c":"markdown","e72f0a8c":"markdown","b3a5ac21":"markdown","03876efb":"markdown","708f5343":"markdown","c64a1539":"markdown","246b9821":"markdown","3b0a4b0a":"markdown","81aae7d7":"markdown","44614d4f":"markdown","81511ab4":"markdown","f401ffbc":"markdown","833fca62":"markdown","4ad6f552":"markdown","2a868353":"markdown","460959c9":"markdown","2cecca91":"markdown","7d2de553":"markdown","37e4e98c":"markdown","34937930":"markdown","f211b309":"markdown"},"source":{"a213e657":"import seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","abaa33fd":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf.head(5)","b1dc7071":"df.info()","8da0420a":"df.isnull().sum()\n","6acef4d0":"df.dtypes","50ed63a9":"df.corr()","6c1f24f6":"df.corr()['Survived'].sort_values()","1b058c5a":"df.describe()","7a787942":"ax = sns.catplot(kind='point',data=df)\nax.fig.autofmt_xdate()","d029004c":"sns.countplot(data=df)","237bf224":"df.hist(figsize = (20,20))","649a6fea":"sns.pairplot(data=df, hue=\"Survived\", corner=True,kind='reg')\nplt.show()","d05160c7":"sns.heatmap(df.iloc[:,1:10].corr(), annot=True, cmap='RdBu_r')\nplt.show()","a3db70fb":"sns.boxplot(data=df.iloc[:,1:10], orient=\"v\", palette=\"Set2\")\nplt.show()","dff96d1e":"sns.relplot(\n      data=df,x='Fare',y='Pclass',hue=\"Parch\",style=\"Parch\",col='Survived', palette='tab10',\n    facet_kws=dict(sharex=False),\n)","4853c2c9":"sns.lmplot(data=df,x='Fare',y='Pclass',hue='Survived',palette='tab10')","fa589028":"sns.displot(data=df, kind=\"ecdf\", rug=True,palette='tab10')","33a49a02":"sns.jointplot(data=df,palette='tab10')","07c6af73":"df[['Age', 'Cabin',  'Embarked']]=df[['Age', 'Cabin',  'Embarked']].replace(np.NaN,\"0\")\ndf","9ee83463":"y=df.iloc[:,1].values\ndf.drop(columns=['Survived','PassengerId','Name'],inplace=True)\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\ndf['Sex']=LabelEncoder().fit_transform(df['Sex'])\ndf['Ticket']=LabelEncoder().fit_transform(df['Ticket'])\ndf['Cabin']=LabelEncoder().fit_transform(df['Cabin'])\ndf['Embarked']=LabelEncoder().fit_transform(df['Embarked'])\ndf","8301cf6f":"df[['Age', 'Cabin',  'Embarked']]=df[['Age', 'Cabin',  'Embarked']].replace(\"0\",np.NaN)\nfrom sklearn.impute import  SimpleImputer\nx=SimpleImputer().fit_transform(df)\npd.DataFrame(x).isnull().sum()","6de56625":"pd.DataFrame(x)","f5bd85be":"x_train,y_train = x,y","a254a0c0":"x_test,y_test =pd.read_csv(\"..\/input\/titanic\/test.csv\"),pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")['Survived']\nx_test.drop(columns=['PassengerId'],inplace=True)\nx_test","a1367615":"x_test[['Age', 'Cabin',  'Embarked']]=x_test[['Age', 'Cabin',  'Embarked']].replace(np.NaN,\"0\")\nx_test","f94e9b91":"\nx_test.drop(columns=['Name'],inplace=True)\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nx_test['Sex']=LabelEncoder().fit_transform(x_test['Sex'])\nx_test['Ticket']=LabelEncoder().fit_transform(x_test['Ticket'])\nx_test['Cabin']=LabelEncoder().fit_transform(x_test['Cabin'])\nx_test['Embarked']=LabelEncoder().fit_transform(x_test['Embarked'])\nx_test\n","00ea86c1":"x_test[['Age', 'Cabin',  'Embarked']]=x_test[['Age', 'Cabin',  'Embarked']].replace(\"0\",np.NaN)\nfrom sklearn.impute import  SimpleImputer\nx_test=SimpleImputer().fit_transform(x_test)\npd.DataFrame(x_test)","d2518153":"from sklearn.naive_bayes import  GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train)\ny_pred=nb.predict(x_test)\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Score for Naive Bayes : \", accuracy_score(y_pred,y_test))","c0e5d33a":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test,y_pred), annot=True)","c4f01ce9":"from xgboost import XGBClassifier\nXGB=XGBClassifier(use_label_encoder=False)\nXGB.fit(x_train,y_train)\ny_pred=XGB.predict(x_test)\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Score for  XGB : \", accuracy_score(y_pred,y_test))","0aa012c3":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test,y_pred), annot=True)","e43f448b":"from sklearn.tree import DecisionTreeClassifier\nDTC=DecisionTreeClassifier(random_state=42)\nDTC.fit(x_train,y_train)\ny_pred=DTC.predict(x_test)\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Score for DecisionTreeClassifier : \", accuracy_score(y_pred,y_test))","e9613b95":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test,y_pred), annot=True)","0100e357":"from sklearn.ensemble import RandomForestClassifier\nRFC= RandomForestClassifier(random_state=42,n_estimators=250)\nRFC.fit(x_train,y_train)\ny_pred=RFC.predict(x_test)\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Score for DecisionTreeClassifier : \", accuracy_score(y_pred,y_test))","fa2c6546":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test,y_pred), annot=True)","e2706346":"from sklearn.svm import SVC\nfor i in ['linear', 'poly', 'rbf', 'sigmoid']:\n    svc= SVC(random_state=42,kernel=i)\n    svc.fit(x_train,y_train)\n    y_pred=svc.predict(x_test)\n\n    from sklearn.metrics import accuracy_score\n    print(f\"Accuracy Score for Support Vector Classification {i} :  {accuracy_score(y_pred,y_test)}\")","4e383208":"#  **Load Data**","41c011f4":"# ** Overview**\n# ***The data has been split into two groups:***\n\n# ***training set (train.csv)***\n# ** test set (test.csv) **\n# ***The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.*** \n\n#***The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.***\n\n#***We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.***","8f3c700c":"Check null values","e72f0a8c":"# 4-Random Forests","b3a5ac21":"# 3-DecisionTreeClassifier\n","03876efb":"describe","708f5343":"Check the corr for Prediction","c64a1539":"\nSurvived       1.000000\nPclass        -0.338481\nSex           -0.543351\nSibSp         -0.035322\nParch          0.081629\nTicket        -0.164549\nFare           0.257307\nCabin          0.276235\nEmbarked      -0.176509\nName: Survived, dtype: float64\n```\n","246b9821":"corr() is used to find the pairwise correlation of all columns in the dataframe. Any na values are automatically excluded. For any non-numeric data type columns in the dataframe it is ignored.\n\u0645\u0646 0 \u0627\u0644\u0649 1 \u062a\u0627\u062b\u064a\u0631 \u0627\u064a\u062c\u0627\u0628\u064a \u0637\u0631\u062f\u064a \u064a\u0639\u0646\u064a \u0643\u0644\u0645\u0627 \u0627\u0642\u062a\u0631\u0628 \u0627\u0644\u0639\u062f\u062f \u0644\u06441 \u0643\u0644\u0645\u0627 \u0643\u0627\u0646\u062a \u0627\u0644\u0639\u0644\u0627\u0642\u0629 \u0627\u0642\u0648\u0649\n\u0645\u0646 0 \u0627\u0644\u0649 1- \u062a\u0627\u062b\u064a\u0631 \u0627\u064a\u062c\u0627\u0628\u064a \u0639\u0643\u0633\u064a\u0629 \u064a\u0639\u0646\u064a \u0643\u0644\u0645\u0627 \u0627\u0642\u062a\u0631\u0628 \u0627\u0644\u0639\u062f\u062f \u0644\u0644-1 \u0643\u0644\u0645\u0627 \u0643\u0627\u0646\u062a \u0627\u0644\u0639\u0644\u0627\u0642\u0629 \u0627\u0642\u0648\u0649\n\u0643\u0644\u0645\u0627 \u0643\u0627\u0646 \u0627\u0644\u0639\u062f\u062f \u0627\u0642\u0631\u0628 \u0644\u0644\u0635\u0641\u0631 \u0643\u0644\u0645\u0627 \u0643\u0627\u0646 \u0644\u0627 \u064a\u0648\u062c\u062f \u062a\u0627\u062b\u064a\u0631 \u0628\u064a\u0646 \u0627\u0644\u0639\u0644\u0627\u0642\u062a\u064a\u0646","3b0a4b0a":" # **Test data**","81aae7d7":"# **Train and fit**","44614d4f":"count","81511ab4":"# 1-naive_bayes","f401ffbc":"boxplot","833fca62":"heatmap","4ad6f552":"Check info","2a868353":"**Kernel precomputed should be square**","460959c9":"Point","2cecca91":"pairplot","7d2de553":"# 5- Support Vector Classification ","37e4e98c":"# ***Preproccing & Splite***","34937930":"Check data type","f211b309":"# 2-XGBClassifier"}}