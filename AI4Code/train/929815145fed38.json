{"cell_type":{"a5daa981":"code","bf0b6f7d":"code","9959d2dc":"code","6c5c09ef":"code","0b164697":"code","e606835e":"code","ecdda8d5":"code","091b8d07":"code","66c232b2":"code","f1109790":"code","37ec35ad":"code","691199f4":"code","215c8a86":"code","ec3a4521":"code","542b04f4":"code","3051542f":"code","d9657fc9":"code","4c85ebc3":"code","023f8aab":"code","40a30f93":"code","c7b19c9a":"code","5aefefdc":"code","9f545dfb":"code","679ab4b3":"code","d613239c":"code","5c621a15":"code","23f97523":"code","3c595890":"code","6b0a3bb0":"code","f2fd818d":"code","13ee9b75":"code","58752f9f":"code","8c4ba49b":"code","e979423f":"code","c8b08d92":"code","491e39b9":"code","dacc416e":"markdown"},"source":{"a5daa981":"!pip install Pillow==5.3.0\n!pip install image\n!pip install --pre torch torchvision -f https:\/\/download.pytorch.org\/whl\/nightly\/cu92\/torch_nightly.html  \n","bf0b6f7d":"!pip install --upgrade pip\n","9959d2dc":"import PIL\nprint(PIL.PILLOW_VERSION)","6c5c09ef":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport time\nimport numpy as np\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nimport torchvision\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nfrom PIL import Image\nfrom torch.optim import lr_scheduler\nimport copy\nimport json\nimport os\nfrom os.path import exists\n# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","0b164697":"#Organizing the dataset\ndata_dir = '..\/input\/dataset-breks\/data set'\ntrain_dir = data_dir + '\/train'\nvalid_dir = data_dir + '\/valid'\nbatch_size = 32\nuse_gpu = torch.cuda.is_available()","e606835e":"import json\nwith open('..\/input\/cat-to-name-1json\/cat_to_name (1).json', 'r') as f:\n    cat_to_name = json.load(f)\n\n","ecdda8d5":"# D\u00e9finissez vos transformations pour les ensembles de formation et de validation\n# Augmentation et normalisation des donn\u00e9es pour la formation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomRotation(30),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'valid': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n#Charger les jeux de donn\u00e9es avec ImageFolder\ndata_dir = '..\/input\/dataset-breks\/data set'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])for x in ['train', 'valid']}\n#print(image_datasets)\n# \u00c0 l'aide des jeux de donn\u00e9es d'images et des trains, d\u00e9finissez les chargeurs de donn\u00e9es\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'valid']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\nprint(dataset_sizes)\n\nclass_names = image_datasets['valid'].classes\n\nprint(class_names)\n\n","091b8d07":"model = models.alexnet(pretrained=True)\n# Geler les param\u00e8tres\nfor param in model.parameters():\n    param.requires_grad = False","66c232b2":"from collections import OrderedDict\n\n\n# Remplacement du classificateur de mod\u00e8le pr\u00e9-form\u00e9 par notre classificateur\n\nmodel.classifier [6] = nn.Sequential ( \n                      nn.Linear (4096, 256), \n                      nn.ReLU (), \n                      nn.Dropout (0.5), \n                      nn.Linear (256, 2),                    \n                      nn.LogSoftmax ( dim = 1))\nprint(model)\n","f1109790":"def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc_v = 0.0\n    best_acc_T = 0.0\n    best_loss_v= 1.0\n    best_loss_T= 1.0\n    loss_dict = {'train': [], 'valid': []}\n    acc_dict = {'train': [], 'valid': []}\n\n    for epoch in range(1, num_epochs+1):\n        print('Epoch {}\/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs, labels = inputs.to(device), labels.to(device)\n              \n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward() #L'appel de .backward()plusieurs fois accumule le gradient (par addition) pour chaque param\u00e8tre. C'est pourquoi vous devez appeler optimizer.zero_grad()apr\u00e8s chaque .step()appel.\n                        optimizer.step()#est effectue une mise \u00e0 jour des param\u00e8tres bas\u00e9e sur le gradient actuel SGD\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            if phase == 'train':\n                scheduler.step()\n                #print(labels)\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n            loss_dict[phase].append(epoch_loss)\n            acc_dict[phase].append(epoch_acc)\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            \n\n           # copier en profondeur le mod\u00e8le\n            if phase == 'valid' :\n                if epoch_acc > best_acc_v  :\n                   best_acc_v = epoch_acc\n                   best_model_wts = copy.deepcopy(model.state_dict())\n                if best_loss_v > epoch_loss:\n                   best_loss_v = epoch_loss    \n            if phase == 'train' :\n                if epoch_acc > best_acc_T:\n                   best_acc_T = epoch_acc\n                if best_loss_T > epoch_loss:\n                   best_loss_T = epoch_loss    \n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best valid accuracy: {:4f}'.format(best_acc_v))\n    print('Best train  accuracy: {:4f}'.format(best_acc_T))\n    print('valid losss: {:4f}'.format(best_loss_v))\n    print('train losss: {:4f}'.format(best_loss_T))\n\n\n    #   charger les meilleurs poids de mod\u00e8le\n    model.load_state_dict(best_model_wts)\n    return model,loss_dict, acc_dict,time_elapsed\n\n    ","37ec35ad":"ress_loss = {'train': [], 'valid': []}\nress_acc = {'train': [], 'valid': []}\ntime_elapse=0","691199f4":"# Train a model with a pre-trained network\nres_loss = {'train': [], 'valid': []}\nres_acc = {'train': [], 'valid': []}\nif use_gpu:\n    print (\"Using GPU: \"+ str(use_gpu))\n    model = model.cuda()\n# NLLLoss because our output is LogSoftmax\ncriterion = nn.NLLLoss()\n# Adam optimizer with a learning rate\noptimizer = optim.SGD(model.classifier.parameters(), lr=0.006, momentum=0.9)\n# Decay LR by a factor of 0.1 every 5 epochs 15\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n\nmodel_ft,loss_dict, acc_dict,time_elapsed = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=50)\nres_loss = loss_dict\nres_acc = acc_dict\n\n","215c8a86":"\n\n\ntime_elapse=time_elapse+time_elapsed\nprint('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapse \/\/ 60, time_elapse % 60))","ec3a4521":"ress_loss['train'].extend(loss_dict['train'])\nress_loss['test'].extend(loss_dict['test'])\nress_acc['train'].extend(acc_dict['train'])\nress_acc['test'].extend(acc_dict['test'])\nprint(ress_loss)\nprint(ress_acc)","542b04f4":"\nfrom torch.utils.data import DataLoader\nplt.rcParams[\"figure.figsize\"] = (13,13)\n\nres_loss = loss_dict\nres_acc = acc_dict\nplt.title(\"Loss\")\n\nplt.plot(ress_loss['train'],label='Training Loss')  \nplt.plot(ress_loss['test'],label='Validation Loss')  \n\nplt.legend()  \nplt.show()  ","3051542f":"from torch.utils.data import DataLoader\nplt.rcParams[\"figure.figsize\"] = (11,11)\n\nres_loss = loss_dict\nres_acc = acc_dict\nplt.title(\"Accuracy\")\n\nplt.plot(ress_acc['train'],label='Training acc')  \nplt.plot(ress_acc['test'],label='Validation acc')\nplt.plot(ress_loss['test'],label='Validation Loss') \n\nplt.legend()  \nplt.show()  \n\ntime_elapse=time_elapse+time_elapsed\nprint('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapse \/\/ 60, time_elapse % 60))","d9657fc9":"# Save the checkpoint \nnum_epochs=100\nmodel.class_to_idx = dataloaders['train'].dataset.class_to_idx\nmodel.epochs = num_epochs\ncheckpoint = {'input_size': [3, 224, 224],\n                 'batch_size': dataloaders['train'].batch_size,\n                  'output_size': 2,\n                  'state_dict': model.state_dict(),\n                  'data_transforms': data_transforms,\n                  'optimizer_dict':optimizer.state_dict(),\n                  'class_to_idx': model.class_to_idx,\n                  'epoch': model.epochs,\n                  'ress_loss': ress_loss,\n                  'ress_acc': ress_acc \n             }\ntorch.save(checkpoint, 'resnet152.pth')","4c85ebc3":"\ndef load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = models.resnet152()\n    \n    # Our input_size matches the in_features of pretrained model\n    input_size = 2048\n    output_size = 2\n    \n    classifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(2048, 512)),\n                          ('relu', nn.ReLU()),\n                          #('dropout1', nn.Dropout(p=0.2)),\n                          ('fc2', nn.Linear(512, 2)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\n\n# Replacing the pretrained model classifier with our classifier\n    model.fc = classifier\n    \n    \n    model.load_state_dict(checkpoint['state_dict'])\n    \n    return model, checkpoint['class_to_idx']\n# Get index to class mapping\nloaded_model, class_to_idx = load_checkpoint('8960_checkpoint10.pth')\n\nidx_to_class = { v : k for k,v in class_to_idx.items()}\n","023f8aab":"\ndef process_image(image):\n    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n        returns an Numpy array\n    '''\n    \n    # Process a PIL image for use in a PyTorch model\n\n    size = 256, 256\n    image.thumbnail(size, Image.ANTIALIAS)\n    image = image.crop((128 - 112, 128 - 112, 128 + 112, 128 + 112))\n    npImage = np.array(image)\n    npImage = npImage\/255.\n        \n    imgA = npImage[:,:,0]\n    imgB = npImage[:,:,1]\n    imgC = npImage[:,:,2]\n    \n    imgA = (imgA - 0.485)\/(0.229) \n    imgB = (imgB - 0.456)\/(0.224)\n    imgC = (imgC - 0.406)\/(0.225)\n        \n    npImage[:,:,0] = imgA\n    npImage[:,:,1] = imgB\n    npImage[:,:,2] = imgC\n    \n    npImage = np.transpose(npImage, (2,0,1))\n    \n    return npImage","40a30f93":"def imshow(image, ax=None, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    \n    # PyTorch tensors assume the color channel is the first dimension\n    # but matplotlib assumes is the third dimension\n    image = image.numpy().transpose((1, 2, 0))\n    \n    # Undo preprocessing\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = std * image + mean\n    \n    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n    image = np.clip(image, 0, 1)\n    \n    ax.imshow(image)\n    \n    return ax","c7b19c9a":"def predict(image_path, model, topk=2):\n    ''' Predict the class (or classes) of an image using a trained deep learning model.\n    '''\n    \n    # Implement the code to predict the class from an image file\n    \n    image = torch.FloatTensor([process_image(Image.open(image_path))])\n    model.eval()\n    output = model.forward(Variable(image))\n    pobabilities = torch.exp(output).data.numpy()[0]\n    \n\n    top_idx = np.argsort(pobabilities)[-topk:][::-1] \n    top_class = [idx_to_class[x] for x in top_idx]\n    top_probability = pobabilities[top_idx]\n\n    return top_probability, top_class","5aefefdc":"# Display an image along with the top 2 classes\ndef view_classify(img, probabilities, classes, mapper):\n    ''' Function for viewing an image and it's predicted classes.\n    '''\n    img_filename = img.split('\/')[-2]\n    img = Image.open(img)\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,10), ncols=1, nrows=2)\n    cancer_type = mapper[img_filename]\n    \n    ax1.set_title(cancer_type)\n    ax1.imshow(img)\n    ax1.axis('off')\n    \n    y_pos = np.arange(len(probabilities))\n    ax2.barh(y_pos, probabilities)\n    ax2.set_yticks(y_pos)\n    ax2.set_yticklabels([mapper[x] for x in classes])\n    ax2.invert_yaxis()","9f545dfb":"img = '..\/input\/data-breakhis\/data images Breakhis Bresil\/valid\/MALIGNANT\/SOB_M_DC-14-2523-100-023.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)","679ab4b3":"img = '..\/input\/data-breakhis\/data images Breakhis Bresil\/valid\/MALIGNANT\/SOB_M_DC-14-2523-100-028.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)","d613239c":"img = '..\/input\/data-breakhis\/data images Breakhis Bresil\/train\/BENIGN\/SOB_B_A-14-22549CD-100-007.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)","5c621a15":"img = '..\/input\/data-breakhis\/data images Breakhis Bresil\/train\/BENIGN\/SOB_B_A-14-22549CD-100-030.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\n","23f97523":"img = '..\/input\/data-breakhis\/data images Breakhis Bresil\/train\/MALIGNANT\/SOB_M_DC-14-10926-200-009.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\n","3c595890":"img = '..\/input\/breakhis-400x\/BreaKHis 400X\/test\/benign\/SOB_B_A-14-22549AB-400-001.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\n","6b0a3bb0":"img = '..\/input\/breakhis-400x\/BreaKHis 400X\/test\/benign\/SOB_B_A-14-22549AB-400-011.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\n","f2fd818d":"img = '..\/input\/breakhis-400x\/BreaKHis 400X\/test\/benign\/SOB_B_A-14-22549CD-400-009.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\n","13ee9b75":"img = '..\/input\/breakhis-400x\/BreaKHis 400X\/test\/benign\/SOB_B_F-14-14134-400-020.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\n","58752f9f":"img = '..\/input\/breakhis-400x\/BreaKHis 400X\/test\/benign\/SOB_B_F-14-21998EF-400-007.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\n","8c4ba49b":"validation_img_paths = [\"..\/input\/breakhis-400x\/BreaKHis 400X\/test\/malignant\/SOB_M_DC-14-16601-400-002.png\",\n                        \"..\/input\/breakhis-400x\/BreaKHis 400X\/test\/malignant\/SOB_M_DC-14-16716-400-018.png\",\n                        \"..\/input\/breakhis-400x\/BreaKHis 400X\/test\/malignant\/SOB_M_DC-14-17901-400-011.png\",\n                        \"..\/input\/breakhis-400x\/BreaKHis 400X\/test\/benign\/SOB_B_F-14-21998EF-400-017.png\",\n                        \"..\/input\/breakhis-400x\/BreaKHis 400X\/test\/benign\/SOB_B_F-14-23060AB-400-006.png\",\n                        \"..\/input\/breakhis-400x\/BreaKHis 400X\/test\/benign\/SOB_B_F-14-23222AB-400-008.png\"]\nimg_list = [Image.open(img_path) for img_path in validation_img_paths]","e979423f":"\nvalidation_batch = torch.stack([data_transforms['valid'](img).to(device)\n                                for img in img_list])","c8b08d92":"pred_logits_tensor = model(validation_batch)\npred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n","491e39b9":"fig, axs = plt.subplots(1, len(img_list), figsize=(20, 5))\nfor i, img in enumerate(img_list):\n    ax = axs[i]\n    ax.axis('off')\n    ax.set_title(\"{:.0f}% BENIGN, {:.0f}% MALIGNANT\".format(100*pred_probs[i,0],\n                                                          100*pred_probs[i,1]))\n    ax.imshow(img)","dacc416e":"# benign"}}