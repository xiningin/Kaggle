{"cell_type":{"b17f827c":"code","6814cf0c":"code","82851567":"code","eca7e816":"code","001ec834":"code","0917d9d0":"code","6ad2ea30":"code","16f24d9f":"code","98267397":"code","791298d7":"code","253a4145":"code","ccf30300":"code","9afdf9e5":"code","a159be93":"code","2ee17a85":"code","5a0e2d3a":"code","364e9106":"code","f3b9235b":"code","930ddfb0":"code","9e9b5eda":"code","2d09a7ac":"code","33783154":"code","f02043da":"code","82f2c41f":"code","6ef1c347":"code","28ee23f5":"code","eedc5c92":"code","15c5f510":"code","e7f3046e":"code","6d696467":"code","a7f9cfc7":"code","edbdc0f3":"code","79a009e7":"code","5e2797ca":"code","a70b3941":"code","c66d24bc":"code","a97c0eaa":"code","77913e34":"code","67187731":"code","bc0a6d42":"code","8a8124fc":"code","a5cf8250":"code","6742bbc5":"code","8490f76e":"code","82ae2381":"code","fd159f4b":"code","c55c3b79":"code","f271d872":"code","67dbeaef":"code","66eb546e":"code","a8b1b7ab":"code","07aa3275":"code","937972cc":"code","2d49f4db":"code","530eb687":"code","b6355fe8":"code","f4123de3":"code","221bb478":"code","9c183614":"code","14d25552":"code","79ce9b0e":"code","7bd19852":"code","fda9ebaf":"code","9e8c4c6b":"code","90466da7":"code","5c010fc3":"code","156af17a":"markdown","fdcbbf83":"markdown","18a28f93":"markdown","e27de8f0":"markdown","b6e7e352":"markdown","8015afdb":"markdown","b5c91d07":"markdown","17087d88":"markdown","c52b146f":"markdown","c5baf028":"markdown","7e1066b2":"markdown","bea98436":"markdown","1f7e1eac":"markdown","45c417ed":"markdown","e58a9525":"markdown","20c8f859":"markdown","27a191bf":"markdown","81a7438a":"markdown","457f374f":"markdown","a781eb14":"markdown","5631c89d":"markdown","09fea340":"markdown","860fb10e":"markdown","f554b514":"markdown","5061c0c5":"markdown","d1b6f9a9":"markdown","9dd4ba17":"markdown","c2065960":"markdown","40a6eb45":"markdown","f06d4d8d":"markdown","667af904":"markdown","31a27bd9":"markdown","1862f0ad":"markdown","26590fad":"markdown","e4a81f56":"markdown","1607280b":"markdown","94ad6d75":"markdown","b4d5b06d":"markdown","5310720f":"markdown","b1c3da86":"markdown","0be33236":"markdown"},"source":{"b17f827c":"import sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as sk\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics         import accuracy_score\nfrom sklearn.metrics         import confusion_matrix\nfrom sklearn.linear_model    import LogisticRegression\nfrom sklearn.neighbors       import KNeighborsClassifier\nfrom sklearn.tree            import DecisionTreeClassifier\nfrom sklearn.naive_bayes     import GaussianNB\nfrom sklearn.svm             import SVC\nfrom sklearn.ensemble        import RandomForestClassifier\nfrom sklearn.ensemble        import ExtraTreesClassifier\nfrom sklearn.ensemble        import AdaBoostClassifier\nfrom sklearn.ensemble        import GradientBoostingClassifier\nfrom sklearn.ensemble        import VotingClassifier\n\nprint('*'*50)\n#print('Python Version    : ', sys.version)\nprint('Pandas Version    : ', pd.__version__)\nprint('Numpy Version     : ', np.__version__)\nprint('Matplotlib Version: ', mpl.__version__)\nprint('Seaborn Version   : ', sns.__version__)\nprint('SKLearn Version   : ', sk.__version__)\nprint('*'*50)","6814cf0c":"sns.set_style('whitegrid')\n\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\n\n#Reproducibility!\nseed      = 42\nv_size    = 0.33\nnum_folds = 10\nscoring   = 'accuracy'\n\n#random seeds\nnp.random.seed(seed)","82851567":"def missingData(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    md = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    md = md[md[\"Percent\"] > 0]\n    plt.figure(figsize = (8, 4))\n    plt.xticks(rotation='90')\n    sns.barplot(md.index, md[\"Percent\"],color=\"g\",alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n    return md\n\ndef valueCounts(dataset, features):\n    \"\"\"Display the features value counts \"\"\"\n    for feature in features:\n        vc = dataset[feature].value_counts()\n        print(vc)\n\ndef correlationHeatmap(df, title):\n    plt.figure(figsize =(20, 14))\n    sns.heatmap(df.corr(),annot=True,cmap='coolwarm',linewidths=0.2)\n    plt.title(title, size=25)\n    plt.xticks(size=15)\n    plt.yticks(size=15)\n    plt.show()\n\n#Spot-Check functions\n\ndef algoSpotCheck(models, X_train, y_train, num_folds, scoring, seed):\n    \"\"\"Makes a spot-check of the models\"\"\"\n    results = []\n    names   = []\n    for name, model in models:\n        kfold = KFold(n_splits=num_folds, random_state=seed)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        print(\"*\"*60)\n    return names, results\n\ndef boxplotCompare(names, results, title):\n    \"\"\"Generate a boxplot of the models\"\"\"\n    fig = plt.figure(figsize=(20, 14)) \n    ax = fig.add_subplot(111)\n    sns.boxplot(data=results)\n    ax.set_xticklabels(names) \n    plt.title('Comparison between Algorithms', size = 40, color='k')\n    plt.xlabel('Percentage',size = 20,color=\"k\")\n    plt.ylabel('Algorithm',size = 20,color=\"k\")\n    plt.xticks(size=15)\n    plt.yticks(size=15)\n    plt.show()\n\n#Hyperparameter Tuning Function\n\ndef algoGridTune(model, param_grid, X_train, y_train, num_folds, scoring, seed):\n    \"\"\"Makes the hyperparameter tuning of the chosen model\"\"\"\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold, n_jobs=-1)\n    grid_result = grid.fit(X_train, y_train)\n    best_estimator = grid_result.best_estimator_\n    print(\"BestScore: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    print(\"*\"*60)\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"Score: %f (%f) with: %r\" % (mean, stdev, param))\n        print(\"*\"*60)\n    return best_estimator\n\ndef plotLearningCurve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure(figsize=(15,10))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std  = np.std(train_scores, axis=1)\n    test_scores_mean  = np.mean(test_scores, axis=1)\n    test_scores_std   = np.std(test_scores, axis=1)\n    \n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, \n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    plt.show()\n\ndef plotFeatureImportance(best_model, datacols, title):\n    plt.figure(figsize=(10,4))\n    pd.Series(best_model.feature_importances_,datacols).sort_values(ascending=True).plot.barh(width=0.8)\n    plt.title(title)\n    plt.show()","eca7e816":"path_train = '..\/input\/train.csv'\npath_test = '..\/input\/test.csv'\n\ndf_train = pd.read_csv(path_train)\ndf_test  = pd.read_csv(path_test)","001ec834":"df_train.head(5)","0917d9d0":"df_test.head(5)","6ad2ea30":"df_train.describe()","16f24d9f":"df_test.describe()","98267397":"df_train.info()","791298d7":"df_test.info() #Since this is the test set, the Survivors class is missing","253a4145":"print('Datasets shapes: ')\nprint(\"Training shape: \", df_train.shape)\nprint(\"Test shape    : \", df_test.shape)","ccf30300":"missingData(df_train)","9afdf9e5":"missingData(df_test)","a159be93":"df_train.drop(\"Cabin\", axis=1, inplace = True)\ndf_test.drop(\"Cabin\", axis=1, inplace = True)","2ee17a85":"df_train[\"Age\"].fillna(df_train[\"Age\"].median(), inplace = True)\ndf_test[\"Age\"].fillna(df_test[\"Age\"].median(),  inplace = True)","5a0e2d3a":"df_test[\"Fare\"].fillna(df_test['Fare'].median(), inplace = True)","364e9106":"df_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].mode()[0])","f3b9235b":"def displayNanValues():\n    print(\"Check the NaN value in train data\")\n    print(df_train.isnull().sum())\n    print(\"---\"*30)\n    print(\"Check the NaN value in test data\")\n    print(df_test.isnull().sum())\n    print(\"---\"*30)","930ddfb0":"displayNanValues()","9e9b5eda":"all_data = [df_train, df_test]","2d09a7ac":"def featureExtraction(all_data):\n    \n    # Create new feature FamilySize as a combination of SibSp and Parch\n    for dataset in all_data:\n        dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 #+1 because it indicates the person of the i-th row\n        \n    # Create bin for age features\n    for dataset in all_data:\n        dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,18,60,120], labels=['Children','Adult','Elder'])\n        \n    #Create a Title feature ...\n    for dataset in all_data:\n        dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \n    #And replaces old titles with new ones\n    for dataset in all_data:\n        dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n        dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n        dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n        dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","33783154":"featureExtraction(all_data)","f02043da":"dfTrain = df_train.copy()\ndfTest  = df_test.copy()","82f2c41f":"traindf = pd.get_dummies(dfTrain, columns = [\"Pclass\",\"Title\",'FamilySize',\"Sex\",\"Age_bin\",\"Embarked\"],\n                             prefix=[\"Pclass\",\"Title\",'FamilySize',\"Sex\",\"Age_type\",\"Em_type\"])\n\ntestdf = pd.get_dummies(dfTest, columns = [\"Pclass\",\"Title\",'FamilySize',\"Sex\",\"Age_bin\",\"Embarked\"],\n                             prefix=[\"Pclass\",\"Title\",'FamilySize',\"Sex\",\"Age_type\",\"Em_type\"])","6ef1c347":"allData = [traindf, testdf]","28ee23f5":"traindf.head()","eedc5c92":"for dataset in allData:\n    drop_column = [\"Age\",\"Fare\",\"Name\",\"Ticket\",\"SibSp\",\"Parch\"]\n    dataset.drop(drop_column, axis=1, inplace = True)\n\ntraindf.drop([\"PassengerId\"], axis=1, inplace = True)","15c5f510":"correlationHeatmap(traindf, 'Pearson Correlation of Features')","e7f3046e":"#Random Shuffle\ndf_data_shuffled = traindf.iloc[np.random.permutation(len(traindf))]","6d696467":"df_data_shuffled.head()","a7f9cfc7":"df_data_shuffled.describe()","edbdc0f3":"array = df_data_shuffled.values #convert into array the train set\n\nfeatures = array[:,1:].astype(float)\ntargeted = array[:,0].astype(float)","79a009e7":"X_train,X_test,y_train,y_test = train_test_split(features,targeted,test_size=v_size,random_state=seed)\n\nprint('Data shapes: ')\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape : \", X_test.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"y_test shape : \", y_test.shape)","5e2797ca":"# Spot-Check Algorithms\n\nmodels = [('LR', LogisticRegression(solver='liblinear')),\n          ('KNN', KNeighborsClassifier()),\n          ('CART', DecisionTreeClassifier()),\n          ('NB', GaussianNB()),\n          ('SVM', SVC(gamma='auto')),     \n         ]\n\nnames,results = algoSpotCheck(models,X_train,y_train,num_folds,scoring,seed)\nboxplotCompare(names, results, 'Comparison_beetween_Algorithms0')","a70b3941":"lr_clf = LogisticRegression()\nlr_param_grid = {'solver' : ['liblinear', 'lbfgs'],'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n#lr_param_grid = {'solver' : ['liblinear'],'C': [1] }\nbest_lr = algoGridTune(lr_clf, lr_param_grid, X_train, y_train,num_folds, scoring, seed)","c66d24bc":"best_lr","a97c0eaa":"knn_clf = KNeighborsClassifier()\nknn_param_grid = {'n_neighbors':[3,5,7,9,11],\n              'leaf_size':[1,2,3,5],\n              'weights':['uniform', 'distance'],\n              'algorithm':['auto', 'ball_tree','kd_tree','brute']\n             }\n\n\nbest_knn = algoGridTune(knn_clf, knn_param_grid, X_train, y_train, num_folds, scoring, seed)","77913e34":"best_knn","67187731":"dt_clf = DecisionTreeClassifier()\ndt_param_grid = {'max_depth' : [3,4,5,6,7,8,9,10],\n              'max_features': ['sqrt', 'log2'],\n              'min_samples_split': [3,5,7,9,11], \n              'min_samples_leaf':[1,3,5,7,9,11]\n             }\n\nbest_dt = algoGridTune(dt_clf, dt_param_grid, X_train, y_train,num_folds, scoring, seed)","bc0a6d42":"best_dt","8a8124fc":"svc_clf = SVC()\n\nsvc_param_grid = [{\"kernel\": [\"rbf\"], \n                   \"gamma\": [10 ,1, 0.1, 1e-2, 1e-3],\n                   \"C\": [0.1,1,10],\n                   \"random_state\" : [seed]},\n                  {\"kernel\": [\"linear\"], \"C\": [0.1,1,10,100]}\n                 ]\n\nbest_SVC = algoGridTune(svc_clf, svc_param_grid, X_train, y_train, num_folds, scoring, seed)","a5cf8250":"best_SVC","6742bbc5":"# Spot-Check Algorithms\n\nmodels = [('RFC', RandomForestClassifier(n_estimators=100)),\n          ('ETC', ExtraTreesClassifier(n_estimators=100)),\n          ('ABC', AdaBoostClassifier(n_estimators=100)),\n          ('GBC', GradientBoostingClassifier(n_estimators=100))\n           ]\n\nnames,results = algoSpotCheck(models,X_train,y_train,num_folds,scoring,seed)\nboxplotCompare(names, results, 'Comparison_beetween_Algorithms1')","8490f76e":"# Adaboost\nDTC = DecisionTreeClassifier()\nABC_clf = AdaBoostClassifier(DTC, random_state=seed)\n\nABC_param_grid = {\"base_estimator__criterion\" : [\"gini\"],\n                  \"base_estimator__splitter\" :   [\"best\"],\n                  \"algorithm\" : [\"SAMME\"],\n                  \"n_estimators\" :[30, 100],\n                  \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\n\nbest_ABC = algoGridTune(ABC_clf, ABC_param_grid, X_train, y_train, num_folds, scoring, seed)","82ae2381":"best_ABC","fd159f4b":"#ExtraTrees \nETC_clf = ExtraTreesClassifier()\n\n #Search grid for optimal parameters\nETC_param_grid = {\"max_depth\": [30],\n                  \"max_features\": ['sqrt'],\n                  \"min_samples_split\": [2, 3, 5],\n                  \"min_samples_leaf\": [1, 3, 5],\n                  \"bootstrap\": [True],\n                  \"n_estimators\" :[300],\n                  \"criterion\": [\"gini\"]}\n\n\nbest_ETC = algoGridTune(ETC_clf, ETC_param_grid, X_train, y_train, num_folds, scoring, seed)","c55c3b79":"best_ETC","f271d872":"\n\nGBC_clf = GradientBoostingClassifier()\nGBC_param_grid = {'loss' : [\"deviance\"],\n                  'n_estimators' : [100, 200, 300],\n                  'learning_rate': [0.1, 0.05, 0.01],\n                  'max_depth': [3, 5, 7],\n                  'min_samples_leaf': [1, 5, 9],\n                  'min_samples_split': [2, 6, 10],\n                  'max_features': ['sqrt', 'log2'] \n                 }\n\nbest_GBC = algoGridTune(GBC_clf, GBC_param_grid, X_train, y_train, num_folds, scoring, seed)","67dbeaef":"best_GBC","66eb546e":"RFC_clf = RandomForestClassifier()\n\nRFC_param_grid = {\"max_depth\": [None],\n                 \"max_features\": [1, 3, 10],\n                 \"min_samples_split\": [2, 3, 10],\n                 \"min_samples_leaf\": [1, 3, 10],\n                 \"bootstrap\": [True, False],\n                 \"n_estimators\" :[100,300],\n                 \"criterion\": [\"gini\"]}\n\n\n\nbest_RFC = algoGridTune(RFC_clf, RFC_param_grid, X_train, y_train, num_folds, scoring, seed)","a8b1b7ab":"best_RFC\n","07aa3275":"kfold = KFold(n_splits=num_folds, random_state=seed)","937972cc":"plotLearningCurve(best_lr,\"LogisticRegression learning curves\",X_train,y_train,cv=kfold)\nplotLearningCurve(best_knn,\"K-NearestNeighbor learning curves\",X_train,y_train,cv=kfold)\nplotLearningCurve(best_dt,\"DecisionTree learning curves\",X_train,y_train,cv=kfold)\nplotLearningCurve(best_SVC,\"SupportVectorClassifier learning curves\",X_train,y_train,cv=kfold)","2d49f4db":"plotLearningCurve(best_RFC,\"RandomForest learning curves\",X_train,y_train,cv=kfold)\nplotLearningCurve(best_ETC,\"EtraTrees learning curves\",X_train,y_train,cv=kfold)\nplotLearningCurve(best_ABC,\"AdaBoost learning curves\",X_train,y_train,cv=kfold)\nplotLearningCurve(best_GBC,\"GradientBoosting learning curves\",X_train,y_train,cv=kfold)","530eb687":"datacols = list(traindf.drop(\"Survived\", axis=1))","b6355fe8":"plotFeatureImportance(best_RFC,datacols, 'Feature Importance for Best_RFC')","f4123de3":"plotFeatureImportance(best_ETC,datacols, 'Feature Importance for Best_ETC')","221bb478":"plotFeatureImportance(best_ABC,datacols, 'Feature Importance for Best_ABC')","9c183614":"plotFeatureImportance(best_GBC,datacols, 'Feature Importance for Best_GBC')","14d25552":"passengerIds = testdf[\"PassengerId\"].copy()\ntestdf.drop([\"PassengerId\"], axis=1, inplace = True)","79ce9b0e":"test = testdf.values","7bd19852":"test_Survived_lr = pd.Series(best_lr.predict(test), name=\"LR\")\ntest_Survived_knn = pd.Series(best_knn.predict(test), name=\"KNN\")\ntest_Survived_dt = pd.Series(best_dt.predict(test), name=\"DT\")\ntest_Survived_SVC = pd.Series(best_SVC.predict(test), name=\"SVC\")\ntest_Survived_ABC = pd.Series(best_ABC.predict(test), name=\"ABC\")\ntest_Survived_RFC = pd.Series(best_RFC.predict(test), name=\"RFC\")\ntest_Survived_ETC = pd.Series(best_ETC.predict(test), name=\"ETC\")\ntest_Survived_GBC = pd.Series(best_GBC.predict(test), name=\"GBC\")\n\n# Concatenate all classifier results\n\nensemble_results = pd.concat([test_Survived_lr, test_Survived_knn, \n                              test_Survived_dt, test_Survived_SVC,\n                              test_Survived_ABC, test_Survived_RFC,\n                              test_Survived_ETC, test_Survived_GBC],axis=1)\n\n\ncorrelationHeatmap(ensemble_results, 'Correlation beetween models results')","fda9ebaf":"votingC = VotingClassifier(estimators=[('lr', best_lr),\n                                       ('knn', best_knn),\n                                       ('dt', best_dt),\n                                       ('svc', best_SVC),\n                                       ('abc', best_ABC),\n                                       ('rfc', best_RFC),\n                                       ('etc', best_ETC),\n                                       ('gbc', best_GBC)], \n                           voting='hard', n_jobs=-1)\n\nvotingC = votingC.fit(X_train, y_train)","9e8c4c6b":"votingC","90466da7":"predictions = votingC.predict(test)\ntest_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\ntest_Survived = test_Survived.apply(int)\nresults = pd.concat([passengerIds,test_Survived],axis=1)\nresults.to_csv('submission.csv', columns=['PassengerId', 'Survived'], index=False)","5c010fc3":"results.head()","156af17a":"## 10.1 Plot Learning Curves of standart Algorithms","fdcbbf83":"# 8. Spot-check Algorithms","18a28f93":"## 9.1 Ensemble Hyperparameter Tuning ","e27de8f0":"# 12. Combining Models with VotingClassifier","b6e7e352":"# 1. Setup","8015afdb":"# 9. Spot-Check Ensemble Methods","b5c91d07":"#### Fill the Fare Feature in the test set with median","17087d88":"# 2. Functions","c52b146f":"#### The 'cabin' feature is full of missing values (> 75%), so I remove it, as it is difficult to guess which cabins used certain people.","c5baf028":"#### ok, after several tests I decided how to do the feature extraction. So I wrote a function that encapsulates all the others I used for this purpose.","7e1066b2":"# 3. Importing Data","bea98436":"#### Now I verify that the handling of the missing data has gone correctly.","1f7e1eac":"# 7. Preparing the datasets for Machine Learning","45c417ed":"# 10. Plot learning curves","e58a9525":"#### 7.2 Split into train and test dataset.\n#### Note: Do not confuse this test dataset with the one processed at the beginning!","20c8f859":"### 8.1.2 K-NearestNeighbors","27a191bf":"### 9.1.4 RandomForest","81a7438a":"### 8.1.3 Decision Tree","457f374f":"#### I delete these features because I created the dummy variables","a781eb14":"### 9.1.1 AdaBoost","5631c89d":"# The kernel ends here. Thanks for watching. Anyone who wants to give me advice and criticism to improve the work is welcome! If you liked the kernel, please rate it. This will give me the motivation to improve myself. Thank you.","09fea340":"### 8.1.4 Support Vector Machines","860fb10e":"# 11. Ensemble Feature Importances","f554b514":"## 8.1 Algorithms Hyperparameter Tuning ","5061c0c5":"# 6. Feature Extraction","d1b6f9a9":"#### 7.1 Split into features and class","9dd4ba17":"<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fd\/RMS_Titanic_3.jpg'>","c2065960":"#### Now I print a heatmap with the correlation coefficients between the features.","40a6eb45":"#### Fill the Age Feature with median","f06d4d8d":"# 5. Handling Missing Data","667af904":"The data has been split into two groups:\ntraining set (train.csv) test set (test.csv) The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n#### Data Dictionary\n|Target|Description|\n|-------|-----------|\n|survival|Survival 0 = No, 1 = Yes|\n\n|Feature|Description|\n|-------|-----------|\n|pclass|Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd|\n|sex|Male or Female|\n|Age|Age in years|\n|sibsp|# of siblings \/ spouses aboard the Titanic|\n|parch|# of parents \/ children aboard the Titanic|\n|ticket|Ticket number|\n|fare|Passenger fare|\n|cabin|Cabin number|\n|embarked|Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton|\n\nVariable Notes pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them","31a27bd9":"### This is my first competition. I use it so that I can test all the functions I have created. The functions are defined in the specific section, so don't worry if you don't find them immediately before they are used. I accept all your criticisms,and, if you liked the kernel, please rate it. This will give me the motivation to improve myself. Thank you.","1862f0ad":"### 8.1.1 Logistic Regression","26590fad":"#### Fill the Embarked Feature in the train setwith 0","e4a81f56":"### 9.1.2 ExtraTrees","1607280b":"### The Learning Curve refers to a plot of the prediction accuracy\/error vs. the training set size (ie: how better does the model get at predicting the target as you the increase number of instances used to train it)","94ad6d75":"#### In order not to slow down loading the kernel, I commented on the parameter grids used by me and replaced them with parameter grids with the best models.","b4d5b06d":"#### I Create a copy of datasets","5310720f":"# 4. First Data Exploration","b1c3da86":"# 13. Apply the model on test dataset and submit on Kaggle","0be33236":"### 9.1.3 GradientBoosting"}}