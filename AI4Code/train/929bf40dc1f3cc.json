{"cell_type":{"fa515239":"code","6cdce560":"code","5c74e291":"code","8ff65d4f":"code","7602c6c0":"code","70e680c5":"code","dbffda25":"code","9342b841":"code","fd099c1b":"code","779c156f":"markdown","9075703b":"markdown","3952f8e9":"markdown","84d8fa20":"markdown","5ae52a76":"markdown","8b5c8aa6":"markdown","1e881ec8":"markdown","dc8e2b28":"markdown","7d57c7c6":"markdown","014514ef":"markdown","c8f01a4d":"markdown","58f570d1":"markdown","5064e297":"markdown","633c8a42":"markdown","c5e1b189":"markdown","2455019e":"markdown","a8e50529":"markdown","97646161":"markdown"},"source":{"fa515239":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the diabetes dataset\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n# Use only one feature\ndiabetes_X = diabetes_X[:, np.newaxis, 2]\n\n# Split the data into training\/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n\n# Split the targets into training\/testing sets\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint('Mean squared error: %.2f'\n      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.2f'\n      % r2_score(diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","6cdce560":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n\n# Our dataset and targets\nX = np.c_[(.4, -.7),\n          (-1.5, -1),\n          (-1.4, -.9),\n          (-1.3, -1.2),\n          (-1.1, -.2),\n          (-1.2, -.4),\n          (-.5, 1.2),\n          (-1.5, 2.1),\n          (1, 1),\n          # --\n          (1.3, .8),\n          (1.2, .5),\n          (.2, -2),\n          (.5, -2.4),\n          (.2, -2.3),\n          (0, -2.7),\n          (1.3, 2.1)].T\nY = [0] * 8 + [1] * 8\n\n# figure number\nfignum = 1\n\n# fit the model\nfor kernel in ('linear', 'poly', 'rbf'):\n    clf = svm.SVC(kernel=kernel, gamma=2)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n                facecolors='none', zorder=10, edgecolors='k')\n    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,\n                edgecolors='k')\n\n    plt.axis('tight')\n    x_min = -3\n    x_max = 3\n    y_min = -3\n    y_max = 3\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.figure(fignum, figsize=(4, 3))\n    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    plt.xticks(())\n    plt.yticks(())\n    fignum = fignum + 1\nplt.show()","5c74e291":"from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nfeatures, true_labels = make_blobs(n_samples=250, centers=4, cluster_std=1.1, random_state=40)\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(features)\n\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(scaled_features)\ny_kmeans = kmeans.predict(scaled_features)\n\nplt.scatter(scaled_features[:,0], scaled_features[:,1], c=y_kmeans)\nplt.show()","8ff65d4f":"from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nfeatures, true_labels = make_moons(n_samples=250, noise=0.05, random_state=40)\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(features)\n\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(scaled_features)\ny_kmeans = kmeans.predict(scaled_features)\n\nplt.scatter(scaled_features[:,0], scaled_features[:,1], c=y_kmeans)\nplt.show()","7602c6c0":"#!pip install scikit-surprise #if it is not already installed then use this line","70e680c5":"import pandas as pd\nfrom surprise import Dataset\nfrom surprise import Reader\n\nratings_dict = {\n    \"item\": [1, 2, 1, 2, 1, 2, 1, 2, 1],\n    \"user\": ['A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'E'],\n    \"rating\": [1, 2, 2, 4, 2.5, 4, 4.5, 5, 3],\n}\n\ndf = pd.DataFrame(ratings_dict)\nreader = Reader(rating_scale=(1, 5))\n\n# Loads Pandas dataframe\ndata = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n# Loads the builtin Movielens-100k data\nmovielens = Dataset.load_builtin('ml-100k')","dbffda25":"from surprise import KNNWithMeans\n\ntrainingSet = data.build_full_trainset() #step neded in surprise, fits algorithm to whole dataset\n#for a simple random train and test set, with the test set being 25% of the ratings you can use:\n#trainset, testset = train_test_split(data, test_size=.25)\n\n#to use item-based cosine similarity\nsim_options = {\n    \"name\": \"cosine\",\n    \"user_based\": False, #compute similarities between items\n}\n\nalgo = KNNWithMeans(sim_options = sim_options)\n\nalgo.fit(trainingSet)\n\nprediction = algo.predict('E',2) #predicting what user E would rate movie 2\nprediction.est","9342b841":"from surprise import KNNWithMeans\nfrom surprise import Dataset\nfrom surprise.model_selection import GridSearchCV\n\ndata = Dataset.load_builtin(\"ml-100k\")\nsim_options = {\n    \"name\": [\"msd\", \"cosine\"],\n    \"min_support\": [3, 4, 5],\n    \"user_based\": [False, True],\n}\n\nparam_grid = {\"sim_options\": sim_options}\n\ngs = GridSearchCV(KNNWithMeans, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\ngs.fit(data)\n\nprint(gs.best_score[\"rmse\"])\nprint(gs.best_params[\"rmse\"])","fd099c1b":"from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import GridSearchCV\n\ndata = Dataset.load_builtin(\"ml-100k\")\n\nparam_grid = {\n    \"n_epochs\": [5, 10],\n    \"lr_all\": [0.002, 0.005],\n    \"reg_all\": [0.4, 0.6]\n}\ngs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n\ngs.fit(data)\n\nprint(gs.best_score[\"rmse\"])\nprint(gs.best_params[\"rmse\"])","779c156f":"# Random forest\n\nRandom forests are covered in the introduction to machine learning Kaggle course https:\/\/www.kaggle.com\/dansbecker\/random-forests but here is a breif summary of what is says:\n\n* decision trees leave you with a difficult decision \n* either have a deep tree with loads of leaves that overfit\n* or a shallow tree with few leaves performs poorly because it fails to capture distinctions in the raw data\n* even today's sophisticated modelling techniques struggle with underfitting and overfitting\n* random forests are a way to combat this\n\n\n* random forests make predictions by averaging the predictions of each component tree\n* and all the trees have a low correlation which protects them from individual errors\n* the trees have better predictive accuracy together in the random forest\n* works well with default parameters\n* the more modelling, the better the performance\n\n\n* the kaggle course has example code too!\n\nfor random forests to have some predictive power:\n* we need features to have some predictive power\n* the trees of the forest and more importantly their predictions to be uncorrelated ","9075703b":"Some problems can't be solved using linear hyperplanes, in this case SVM uses the kernel trick to transform the input space to a higher dimensional space like the image below. \n\n![image.png](attachment:image.png)\n\nThis is called the kernel trick and can be 'linear', 'poly' or 'rbf' which stands for radial basis function and is mapped to an infinite dimensional space, it is pretty common to use.\n\nThere is an example on the website where this explanation and the images were taken from: https:\/\/www.datacamp.com\/community\/tutorials\/svm-classification-scikit-learn-python \n\nAnother example from https:\/\/scikit-learn.org\/stable\/auto_examples\/svm\/plot_svm_kernels.html:","3952f8e9":"# Linear regression\n\nLinear regression is a linear approach to modelling the relationship between a scalar response (dependant variable) and one or more explanatory variables (independent variables). When there is only one dependant variable we use the term 'simple linear regression'. \n\nLinear regression is usually used to predict, forecast or error reduce - linear regression can be used to fit a predictive model to an observed data set. Linear regression is also used to quantify a relationship between the response and explanatory variables. \n\nLinear regression assumes:\n* a the relationship between the scalar and explanatory variables are linear\n* that the variance is constant (homoscedasticity), this means the same variance of errors\n* independent errors\n* lack of perfect multicollinearity in the predictors - the predictors are independent\n\nMore info on: https:\/\/en.wikipedia.org\/wiki\/Linear_regression#:~:text=In%20statistics%2C%20linear%20regression%20is,is%20called%20simple%20linear%20regression. \nincluding information on the mathematical notation and side of things\n\nExample using scikit learn:\n\n","84d8fa20":"# PCA\n\nThe following is taken from: https:\/\/towardsdatascience.com\/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c and https:\/\/www.datacamp.com\/community\/tutorials\/principal-component-analysis-in-python?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034349&utm_targetid=aud-299261629574:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=1006886&gclid=CjwKCAjw-5v7BRAmEiwAJ3DpuMQ7fHjJw_dg4wgp4g5g2Dv4WIAB7vQ6lvRqi8xZqLdXsmO5QpNyZRoC-KsQAvD_BwE\n\nPCA stands for Principal Component Analysis is a technique for reducing the dimensionality for large datasets with loads and loads of variables where we don't fully understand the relationship between each variable and are worried about overfitting. For example predicting the GDP of the UK, there are variables like the inflation rate, unemployment rate, etc.\n\nWhen we say we want to reduce the dimensionality, we just want to focus on a few of these variables to help reduce overfitting. \n\nThere are two broad techniques that are used to achieve dimensionality reduction:\n* feature elimination - self explanatory, disadvantage is that we don't use some of the variables\n* feature extraction - combine the variables into new variables and drop some of them. Feature elimination problem is solved becuase the new variables are a combination of all the variables\n\nPCA is a technique for feature extraction, all the new variables after PCA are independent which fits the assumptions of a linear model - hence we can do linear regression on the model then.\n\nWhen should you use PCA?\n1. when you want to reduce the number of variables, but don't know which ones\n2. ensure independent variables\n3. most importantly: are you okay with making your independent variables less interpretable \n\nIt should be noted that PCA is an unsupervised dimensionality reduction technique that uses orthogonal transformation to convert a set of possibly correlated variables into a set of linearly correlated variables - called principal components.\n\nWhere can you apply PCA?\n* data visualisation - like in the GDP example, where there is a lot of data and you need a way to find out how the variables are correlated\n* speeding up machine learning algorithms - can speed up testing and training time of ML algorithms that are processing a lot of data \n\nThe datacamp article has a worked example of both of these applications and is an excellent reference point:\nhttps:\/\/www.datacamp.com\/community\/tutorials\/principal-component-analysis-in-python?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034349&utm_targetid=aud-299261629574:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=1006886&gclid=CjwKCAjw-5v7BRAmEiwAJ3DpuMQ7fHjJw_dg4wgp4g5g2Dv4WIAB7vQ6lvRqi8xZqLdXsmO5QpNyZRoC-KsQAvD_BwE\n\nA key point that the examples showed is that there is data loss when performing PCA, we can try and retain a larger variance by having PCA(0.9) - hence the principal components outputted producing a variance of 0.9. We could do this instead of using PCA(n_components = ...)\n\nIt should be noted that the principal components are not a list of variables, like in a neural network. They are a collection of components.\n\n","5ae52a76":"# Collaborative filtering\n\nCollaborative filtering is using data from many users to predict what a user will like - hence giving a recomendation. An example of this would be Netflix or Amazon recommendations.\n\nThe following explanation summarised from https:\/\/realpython.com\/build-recommendation-engine-collaborative-filtering\/\n\nThere are three main questions involved in collaborative filtering:\n* how do you determine which users are similar?\n* given that you know which users are similar, how do you determine the rating that a user would give an item, based on the ratings of similar users\n* how do you measure the accuracy of the ratings you calculate\n\nThere are tough raters and average raters, for example, two users can have the same preferences, but one could be more stingy with their rating system. Hence before running collaborative filtering you need to average out the magnitude of the recommendations. You can make the average 0 for ease and have negative reviews in the vector you use for analysis.\nYou can fill in missing values on the vector with the centered average (in this case zero).\n\nEuclidean distance and cosine similarity are two approaches to finding similar users:\n* Euclidean distance - how far apart the points are\n* cosine similarity - drawing a line from (0,0) to the points on a graph. The smaller the angle between those lines, the closer the users are to each other. This method helps with the tough raters\nThese can be calculated using 'spacial.distance.euclidean' and 'spacial.distance.cosine'. 'from scipy import spacial'\n\n## User-based vs Item-based collaborative filtering\n* user-based: the matrix is used to find similar users based on the ratings they give\n* item-based: the rating matrix is used to find similar items based on the ratings given to them by users\n\nItem-based collaborative filtering was developed by Amazon\n* because there are more users than items\n* which makes item based filtering more stable than user based\n* it is effective because usually, the average rating recieved by an item doesn't change as quickly as the average rating given by a user to different items\n* user-based filtering performs better when the ratings matrix is sparse\n\n## Model based approaches\nAs the ratings matirx is mostly empty, reducing the number of dimensions can improve the performance of the algorithm both in terms of space and time\n\n*Matrix factorisation* is factorising, but for matricies - like breaking down a number into its primes, but breaking down a matrix into a product of smaller matricies. \n\nOne of the most popular algorithms to factorise a matrix is singular value decomposition (SVD) - SVD got popular when it performed well in the Netflix prize competition. Other algorithms include PCA and its variations like NMF.\n\nIf you want to use a neural network then an autoencoder is a neural net that is typically used for dimensionality reduction.\n\nThe Real Python article recommends Surprise, a Python SciKit while learning about recommender systems. Here is the example they gave:","8b5c8aa6":"An incredible example using plenty of data cleaning, manipulation, visualisation and python skills: https:\/\/www.kaggle.com\/johnosorio\/retail-rocket-ecommerce-recommender-system","1e881ec8":"This example shows that k-means clustering does not work as well for data that is not in blobs, a density based clustering method like DBSCAN will be more useful for these half moons.","dc8e2b28":"# Gradient boosting\n\nXGBoost is a model that provides better performance than random forests when fine tuned to the right parameters. It does require some skill to get the right parameters though.\n\nThere is a Kaggle tutorial on this: https:\/\/www.kaggle.com\/alexisbcook\/xgboost which I have summarised:\n\nThe random forest method is an \"ensemble method\" which means it combines the predictions of several models. Gradient boosting is another ensemble method\n\nGradient boosting goes through cycles to iteratively add models to an ensemble\n\nXGBoost stands for extreme gradient boosting \n\nParameter tuning:\nXGBoost has a few parameters which can drastically affect accuracy and training speed\n* n_estimators\n    * how many times you go through the modelling cycle \n    * too low = underfitting\n    * too high = overfitting\n    * typical values are from 100-1000, but depends on the learning_rate\n* early_stopping_rounds\n    * is a way to automatically find the ideal value for n_estimators\n    * causes the model to stop iterating when the validation score stops improving\n    * it is smart to set a high n_estimators and then use early_stopping_rounds\n    * early_stopping_rounds = 5 is a sensible choice\n* learning_rate\n    * instead of adding up the predictions from each component model, we can multiply the predictions from each model by a small number\n    * this small number would be the learning_rate\n    * we do this so that adding another tree to the model helps us less\n    * to avoid overfitting\n    * default learning_rate = 0.1\n* n_jobs\n    * for larger datasets, it is common to set n_jobs as the number of cores on your machine\n    * on smaller datasets this won't help\n\nThe Kaggle course has an example of XGBoost in action.","7d57c7c6":"## Issues with collaborative filtering:\n\n* not good for new products as no one has rated them yet\n* data sparsity can affect recommendations\n* scaling can be a problem. Item-based recommenders are better for large datasets \n\nMultiple algorithms workiing together or a data pipeline can help you set up more accurate recommenders","014514ef":"From: https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_ols.html\n\nThe Mean Squared Error (MSE) is the squared difference between the predictive line and the values, it is often the measure of success with linear regression. We want to reduce it","c8f01a4d":"So the MovieLens 100k dataset works best if you go for the item-based approach, use the msd as the simiarity matrix with minimum support 3.\n\nThis algorithm also works for SVM:","58f570d1":"Here is a list of the 10 algorithms I will cover:\n* linear regression\n* logistic regression\n* SVM\n* random forest\n* gradient boosting\n* PCA\n* K-means\n* collaborative filtering\n* kNN\n* ARIMA \n\n\n\n\n\n\n\n","5064e297":"# k-means\n\nK-means clustering is a unsupervised machine leaning technique used to identify clusters of data objects in a dataset. Clusters are groups of data points that are similar to each other. There are many approaches to cluster a dataset, which include:\n\n* partitional clustering - partitioning the data objects into non-overlapping groups, this is not good for when there are clusters with complex shapes, sizes and various densities. K-means clustering is an example of partitional clustering\n* heirachical clustering - either iteratively merges points that are most similar or starts with one big cluster and iteratively splits the least similar clusters\n* density based clsutering - makes clusters based on the density of data points. An exmaple of this is DBSCAN\n\nK-means clustering works by selecting k points on the graph, these are the k centroids. \n\nIt then assigns each data point to the closest centriod, hence forming k clusters. Calculates the mean of each cluster and reassigns the centroid to that mean.\n\nK-means then repeats these steps using the newly assigned centroids, hence iterating towards a stationary point for the centroids. These steps are repeated until the centroids stop moving. \n\nWhere the centroids initially are affect the outcome of the final clusters that the k-means algorithm produces. This value is fed into the algorithm and is normally set to 'random'.\n\nThe number of clusters k is set by the user, there are computational\/statistical methods to help you chose the number k. \n\nThe maximum number of iterations can be set by the user, this is useful in case the algorithm diverges.\n\nHere is a link to a k-means clustering example: https:\/\/realpython.com\/k-means-clustering-python\/\n\nBelow are two k-means clustering examples:","633c8a42":"# KNN\n\nOne of the simplest of all machine learning algorithms\n\nIt calculates the distance of a new data point to all other data points, then selects the K nearest data points, then assigns the data point to the class where the majority of the K data points belong\n\nIt is a super easy learning algorithm and requires no training data (hence a lazy learning algorithm) - hence making it a fast algorithm.\n\nKNN doesn't work well for high dimensional data because of the large number of dimensions, it takes a while for the algorithm to calculate all the distances. ","c5e1b189":"# Logistic regression\n\nLogistic regression is used to model the probability of binary pass\/fail classes, this can be extended to several classes like determining if something is a shoe, sock, etc. \n\nA binary logitic model has a dependent variable with two possible options. \n\nHere is a useful article that explains how to build a logistic regression model: \n\nhttps:\/\/towardsdatascience.com\/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n\nin this example, Li is predicting if a customer will take out a certain policy or not, Li does this by creating dummy variables - this is variables with two values, zero and one and can be thought of as numeric stand ins for qualitative variables. \n\nLi then oversamples - adds more samples to a variable with fewer samples than other ones, hence creating new tweaked observations. \n\nThen Recursive Feature Elimination (RFE) which \"repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.\"\n\nThen implement and remove variables with a high p-value as they are not good predictors of y\n\nThen logistic model fitting, with a ROC curve to model the results \"The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).\"\n\n![image.png](attachment:image.png)\n\n\n\n\nAnother example of logistic regression, this time on the MNIST dataset: https:\/\/towardsdatascience.com\/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n","2455019e":"## Chosing parameters\n\nWith GridSearchCV you can check which similarity metric works best for your data:","a8e50529":"# ARIMA\n\nThe following explanation is a brief summary of the incredible notebook: https:\/\/www.kaggle.com\/freespirit08\/time-series-for-beginners-with-arima\n\nARIMA is a time series forecasting algorithm\n\nARIMA stands for Auto Regressive Integrated Moving Average and is a combination of 2 models: AR (Auto Regressive) & MA (Moving Average)\n\nIt has 3 hyperparameters (hyperparamters is a machine learning term that means 'a paramter whose value is used to control the learning process'):\n* P - auto regressive lags \n* d - order of differentiation\n* Q - moving average\nwhich respectively come from the AR, I & MA components \n\nThe AR part is correlation between previous and current time periods\nMA is used to smooth out the noise\nand I binds AR and MA together\n\n## Stationary Data\n\nBefore applying any statistical model to a time series, the time series has to be stationary:\n* it should have a constant mean\n* constant variance (or standard deviation)\n* the degree to which the dependent and independant variables correlate (autocovariance) should not depend on time\n\nThe data can be treated to ensure this, the Kaggle notebook example does this. It first shows that the data is not statoinary by using .rolling(window=12).mean() and .rolling(window=12).std() to calculate the 12 month rolling average and standard deviation of the data. \n\nThe notebook then does the ADCF (Augmented Dickey-Fuller test) to show that the data has a large p-value and that the critical values are not close to the test statistic. Hence, the time series is not stationary at the moment. \n\nThere are many ways to  make the data stationary like taking $log_{10}$, $log_{e}$, square, square root, cube, cube root, etc. You can try different ones and use the best result - this was done in the example.\n\n\n## ARIMA ins and outs\n\nTo get P for ARIMA you calculate x at y=0 on a PACF (Partial Auto Correlation Function) plot \nand to get Q you calculate x at y=0 on a ACF (Auto Correlation Function)  graph\n\n\nThe calculations are long and intense, I would recommend https:\/\/www.kaggle.com\/sumi25\/understand-arima-and-tune-p-d-q if you would like more information on the ins and outs of ARIMA. However for a brief introduction, this summary is sufficient.\n","97646161":"# SVM\n\nSVM stands for Support Vector Machines and is a supervised learning algorithm that allows you to classify data that is linearly separable. If it isn't linearly separable then you can use the kernel trick to make it work. \n\n![image.png](attachment:image.png)\n\nSVM pretty much generates hyperplanes which segregates classes in the best way, with the objective to set a hyperplane (line) with the maximum possible margins between support vectors (the vectors closes to the hyperplane).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary from:\nhttps:\/\/www.datacamp.com\/community\/tutorials\/svm-classification-scikit-learn-python\n"}}