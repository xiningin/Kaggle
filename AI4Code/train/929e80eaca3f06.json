{"cell_type":{"f0ab736f":"code","3a6ed235":"code","675e57ce":"code","9f9d641f":"code","e907706f":"code","f1ad6381":"code","dd8f210a":"code","1d1cf22a":"code","898618fa":"code","d4294fb8":"code","457cd2e9":"code","0de8fc83":"code","8ea70d2e":"code","605a290a":"code","f45ffe54":"code","8abd515d":"code","3cd28031":"code","65e44917":"code","f86a685a":"code","ce2b5ed2":"code","22568f96":"code","acb6862b":"code","0399364b":"markdown","5087f894":"markdown","7e2debf7":"markdown","6764afb8":"markdown","3da41c8b":"markdown","5b460f69":"markdown","6498d842":"markdown","adb0fb44":"markdown","0f6d15d9":"markdown","c11b1427":"markdown","b677d144":"markdown","c50c4d89":"markdown","b587854e":"markdown","e89c398a":"markdown","99e14f3a":"markdown","0861fff7":"markdown","ac4b289a":"markdown","519be102":"markdown","8f6d637b":"markdown","27df13c6":"markdown","dfbabf6f":"markdown","d6533448":"markdown"},"source":{"f0ab736f":"# ! pip3 install transformers","3a6ed235":"import pandas as pd\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport copy\nfrom tqdm.notebook import tqdm\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import (\n    accuracy_score, \n    f1_score, \n    classification_report\n)\n\nfrom transformers import (\n    AutoTokenizer, \n    AutoModel,\n    get_linear_schedule_with_warmup\n)\n\nproject_dir = '..\/input\/avjanatahackresearcharticlesmlc\/av_janatahack_data\/'","675e57ce":"! nvidia-smi","9f9d641f":"train_df = pd.read_csv(project_dir + 'train.csv')\ntrain_df.head()","e907706f":"# preprocessing\ndef clean_abstract(text):\n    text = text.split()\n    text = [x.strip() for x in text]\n    text = [x.replace('\\n', ' ').replace('\\t', ' ') for x in text]\n    text = ' '.join(text)\n    text = re.sub('([.,!?()])', r' \\1 ', text)\n    return text\n    \n\ndef get_texts(df):\n    texts = df['ABSTRACT'].apply(clean_abstract)\n    texts = texts.values.tolist()\n    return texts\n\n\ndef get_labels(df):\n    labels = df.iloc[:, 3:].values\n    return labels\n\ntexts = get_texts(train_df)\nlabels = get_labels(train_df)\n\nfor text, label in zip(texts[:5], labels[:5]):\n    print(f'TEXT -\\t{text}')\n    print(f'LABEL -\\t{label}')\n    print()","f1ad6381":"# no. of samples for each class\ncategories = train_df.columns.to_list()[3:]\nplt.figure(figsize=(6, 4))\n\nax = sns.barplot(categories, train_df.iloc[:, 3:].sum().values)\nplt.ylabel('Number of papers')\nplt.xlabel('Paper type ')\nplt.xticks(rotation=90)\nplt.show()","dd8f210a":"# no of samples having multiple labels\nrow_sums = train_df.iloc[:, 3:].sum(axis=1)\nmultilabel_counts = row_sums.value_counts()\n\nplt.figure(figsize=(6, 4))\nax = sns.barplot(multilabel_counts.index, multilabel_counts.values)\nplt.ylabel('Number of papers')\nplt.xlabel('Number of labels')\nplt.show()","1d1cf22a":"# lengths\ny = [len(t.split()) for t in texts]\nx = range(0, len(y))\nplt.bar(x, y)","898618fa":"class Config:\n    def __init__(self):\n        super(Config, self).__init__()\n\n        self.SEED = 42\n        self.MODEL_PATH = 'allenai\/scibert_scivocab_uncased'\n        self.NUM_LABELS = 6\n\n        # data\n        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n        self.MAX_LENGTH = 320\n        self.BATCH_SIZE = 16\n        self.VALIDATION_SPLIT = 0.25\n\n        # model\n        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.FULL_FINETUNING = True\n        self.LR = 3e-5\n        self.OPTIMIZER = 'AdamW'\n        self.CRITERION = 'BCEWithLogitsLoss'\n        self.N_VALIDATE_DUR_TRAIN = 3\n        self.N_WARMUP = 0\n        self.SAVE_BEST_ONLY = True\n        self.EPOCHS = 1\n\nconfig = Config()","d4294fb8":"class TransformerDataset(Dataset):\n    def __init__(self, df, indices, set_type=None):\n        super(TransformerDataset, self).__init__()\n\n        df = df.iloc[indices]\n        self.texts = get_texts(df)\n        self.set_type = set_type\n        if self.set_type != 'test':\n            self.labels = get_labels(df)\n\n        self.tokenizer = config.TOKENIZER\n        self.max_length = config.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index], \n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        if self.set_type != 'test':\n            return {\n                'input_ids': input_ids.long(),\n                'attention_mask': attention_mask.long(),\n                'labels': torch.Tensor(self.labels[index]).float(),\n            }\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }","457cd2e9":"# train-val split\n\nnp.random.seed(config.SEED)\n\ndataset_size = len(train_df)\nindices = list(range(dataset_size))\nsplit = int(np.floor(config.VALIDATION_SPLIT * dataset_size))\nnp.random.shuffle(indices)\n\ntrain_indices, val_indices = indices[split:], indices[:split]","0de8fc83":"train_data = TransformerDataset(train_df, train_indices)\nval_data = TransformerDataset(train_df, val_indices)\n\ntrain_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE)\nval_dataloader = DataLoader(val_data, batch_size=config.BATCH_SIZE)\n\nb = next(iter(train_dataloader))\nfor k, v in b.items():\n    print(f'{k} shape: {v.shape}')","8ea70d2e":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n        self.transformer_model = AutoModel.from_pretrained(\n            config.MODEL_PATH\n        )\n        self.dropout = nn.Dropout(0.3)\n        self.output = nn.Linear(768, config.NUM_LABELS)\n\n    def forward(\n        self,\n        input_ids, \n        attention_mask=None, \n        token_type_ids=None\n        ):\n\n        _, o2 = self.transformer_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        x = self.dropout(o2)\n        x = self.output(x)\n        \n        return x","605a290a":"device = config.DEVICE\ndevice","f45ffe54":"def val(model, val_dataloader, criterion):\n    \n    val_loss = 0\n    true, pred = [], []\n    \n    # set model.eval() every time during evaluation\n    model.eval()\n    \n    for step, batch in enumerate(val_dataloader):\n        # unpack the batch contents and push them to the device (cuda or cpu).\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        b_labels = batch['labels'].to(device)\n\n        # using torch.no_grad() during validation\/inference is faster -\n        # - since it does not update gradients.\n        with torch.no_grad():\n            # forward pass\n            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            \n            # calculate loss\n            loss = criterion(logits, b_labels)\n            val_loss += loss.item()\n\n            # since we're using BCEWithLogitsLoss, to get the predictions -\n            # - sigmoid has to be applied on the logits first\n            logits = torch.sigmoid(logits)\n            logits = np.round(logits.cpu().numpy())\n            labels = b_labels.cpu().numpy()\n            \n            # the tensors are detached from the gpu and put back on -\n            # - the cpu, and then converted to numpy in order to -\n            # - use sklearn's metrics.\n\n            pred.extend(logits)\n            true.extend(labels)\n\n    avg_val_loss = val_loss \/ len(val_dataloader)\n    print('Val loss:', avg_val_loss)\n    print('Val accuracy:', accuracy_score(true, pred))\n\n    val_micro_f1_score = f1_score(true, pred, average='micro')\n    print('Val micro f1 score:', val_micro_f1_score)\n    return val_micro_f1_score\n\n\ndef train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch):\n    \n    # we validate config.N_VALIDATE_DUR_TRAIN times during the training loop\n    nv = config.N_VALIDATE_DUR_TRAIN\n    temp = len(train_dataloader) \/\/ nv\n    temp = temp - (temp % 100)\n    validate_at_steps = [temp * x for x in range(1, nv + 1)]\n    \n    train_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader, \n                                      desc='Epoch ' + str(epoch))):\n        # set model.eval() every time during training\n        model.train()\n        \n        # unpack the batch contents and push them to the device (cuda or cpu).\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        b_labels = batch['labels'].to(device)\n\n        # clear accumulated gradients\n        optimizer.zero_grad()\n\n        # forward pass\n        logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n        \n        # calculate loss\n        loss = criterion(logits, b_labels)\n        train_loss += loss.item()\n\n        # backward pass\n        loss.backward()\n\n        # update weights\n        optimizer.step()\n        \n        # update scheduler\n        scheduler.step()\n\n        if step in validate_at_steps:\n            print(f'-- Step: {step}')\n            _ = val(model, val_dataloader, criterion)\n    \n    avg_train_loss = train_loss \/ len(train_dataloader)\n    print('Training loss:', avg_train_loss)","8abd515d":"def run():\n    # setting a seed ensures reproducible results.\n    # seed may affect the performance too.\n    torch.manual_seed(config.SEED)\n\n    criterion = nn.BCEWithLogitsLoss()\n    \n    # define the parameters to be optmized -\n    # - and add regularization\n    if config.FULL_FINETUNING:\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n\n    num_training_steps = len(train_dataloader) * config.EPOCHS\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n\n    max_val_micro_f1_score = float('-inf')\n    for epoch in range(config.EPOCHS):\n        train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n        val_micro_f1_score = val(model, val_dataloader, criterion)\n\n        if config.SAVE_BEST_ONLY:\n            if val_micro_f1_score > max_val_micro_f1_score:\n                best_model = copy.deepcopy(model)\n                best_val_micro_f1_score = val_micro_f1_score\n\n                model_name = 'scibertfft_best_model'\n                torch.save(best_model.state_dict(), model_name + '.pt')\n\n                print(f'--- Best Model. Val loss: {max_val_micro_f1_score} -> {val_micro_f1_score}')\n                max_val_micro_f1_score = val_micro_f1_score\n\n    return best_model, best_val_micro_f1_score","3cd28031":"model = Model()\nmodel.to(device);","65e44917":"best_model, best_val_micro_f1_score = run()","f86a685a":"test_df = pd.read_csv(project_dir + 'test.csv')\ndataset_size = len(test_df)\ntest_indices = list(range(dataset_size))\n\ntest_data = TransformerDataset(test_df, test_indices, set_type='test')\ntest_dataloader = DataLoader(test_data, batch_size=config.BATCH_SIZE)","ce2b5ed2":"def predict(model):\n    val_loss = 0\n    test_pred = []\n    model.eval()\n    for step, batch in enumerate(test_dataloader):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n\n        with torch.no_grad():\n            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            logits = torch.sigmoid(logits)\n            logits = np.round(logits.cpu().numpy())\n            test_pred.extend(logits)\n\n    test_pred = np.array(test_pred)\n    return test_pred\n\ntest_pred = predict(best_model)","22568f96":"sample_submission = pd.read_csv(project_dir + 'sample_submission.csv')\nids = sample_submission['ID'].values.reshape(-1, 1)\n\nmerged = np.concatenate((ids, test_pred), axis=1)\nsubmission = pd.DataFrame(merged, columns=sample_submission.columns).astype(int)\n\nsubmission","acb6862b":"submission_fname = f'submission_scibertfft_microf1-{round(best_val_micro_f1_score, 4)}.csv'\nsubmission.to_csv(submission_fname, index=False)","0399364b":"Checking the GPU configurations. Kaggle's Tesla P100 GPU proves to be much faster for finetuning SciBERT on this dataset as compared to Google Colab's Tesla K80.","5087f894":"# Model","7e2debf7":"Here we'll initialize PyTorch DataLoader objects for the training & validation data.<br>\nThese dataloaders allow us to iterate over them during training, validation or testing and return a batch of the Dataset class outputs.","6764afb8":"## Dataset & Dataloader","3da41c8b":"# Submission","5b460f69":"# Data","6498d842":"Load the test dataset, and initialize and DataLoader object for it.","adb0fb44":"# Run","0f6d15d9":"### Loss function used<br>\n- **BCEWithLogitsLoss** - Most commonly used loss function for Multi Label Classification tasks. Note that, PyTorch's BCEWithLogitsLoss is numerically stable than BCELoss.\n<br>\n\n### Optimizer used <br>\n- **AdamW** - Commonly used optimizer. Performs better than Adam.\n<br>\n\n### Scheduler used <br>\n- **get_linear_scheduler_with_warmup** from the **transformers** library.\n<br>","c11b1427":"Now, we'll create a custom Dataset class inherited from the PyTorch Dataset class. We'll be using the **SciBERT tokenizer** that returns **input_ids** and **attention_mask**.<br>\nThe custom Dataset class will return a dict containing - <br>\n\n- input_ids\n- attention_mask\n- labels\n<br>\n\nAll three of these are inputs required by BERT models.","b677d144":"# Imports","c50c4d89":"## Exploratory Data Analysis","b587854e":"Coming to the most interesting part - the model architecture! We'll create a class named **Model**, inherited from **torch.nn.Module**.<br><br>\n\n### Flow\n- Get **768** dimensional features from the SciBERT model.\n- Pass them through a **dropout** layer.\n- Pass the dropout layer output through a Linear layer with **input_features=768** and **output_features=6**. (6 is the number of classes)","e89c398a":"Here we define a Config class, which contains all the fixed parameters & hyperparameters required for **Dataset** creation as well as **Model** training.","99e14f3a":"### **[SciBERT](http:\/\/github.com\/allenai\/scibert)** \n* It is a BERT model trained on scientific text.<br>\n* SciBERT is trained on papers from the corpus of semanticscholar.org. Corpus size is 1.14M papers, 3.1B tokens. We use the full text of the papers in training, not just abstracts.\n\n### Multi Class vs Multi Label Classification\n* **Multi Class** - There are multiple categories but each instance is assigned only one, therefore such problems are known as multi-class classification problem.\n* **Multi Label** - There are multiple categories and each instance can be assigned with multiple categories, so these types of problems are known as multi-label classification problem, where we have a set of target labels.","0861fff7":"The entire code is written using **PyTorch**.<br>\nWe'll be using the **transformers** library by [huggingface](https:\/\/github.com\/huggingface\/transformers) as they provide wrappers for multiple Transformer models.","ac4b289a":"# Engine","519be102":"Our engine consists of the training and validation step functions.","8f6d637b":"From the plot above we can infer that, **320** seems like a good choice for **MAX_LENGTH**.","27df13c6":"# Config","dfbabf6f":"We're using just the abstract text, but concatenating the title text along with it performed better on the leaderboard. \n<br><br>\n## Preprocessing\n- Stripping extra whitespaces around the text.\n- Replacing escape characters with whitespace.\n- Padding all punctuations with whitespaces on both sides.\n\n#### Additional Tips:\n- Replacing Latex equations with a special token.\n- Data Augmentation. \n\n","d6533448":"Our **TransformerDataset** Class takes as input the **dataframe**, **indices** & **set_type**. We calculate the train \/ val set indices beforehand, pass it to **TransformerDataset** and slice the dataframe using these indices."}}