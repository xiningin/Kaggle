{"cell_type":{"a4732a84":"code","82ff6fce":"code","798e7ac1":"code","e7ad53ed":"code","63aa8a25":"code","64ce75d0":"code","29825ee7":"code","e11f4324":"code","aa63b317":"code","ded25e26":"code","d5367f41":"code","1f997736":"code","50ab44ba":"code","e89f4275":"code","6ae59b3b":"code","ac95cdf1":"code","f668452f":"markdown","3393c352":"markdown","e722d625":"markdown","45e988f2":"markdown","76bc18da":"markdown"},"source":{"a4732a84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82ff6fce":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, OrthogonalMatchingPursuit\n\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport gc\n\n\nimport sys, os\nimport random \n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nfrom IPython import display, utils\n\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_columns', 300)\n\n\n\ndef set_seed(seed=4242):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()","798e7ac1":"data = pd.read_csv('\/kaggle\/input\/uplift-modeling\/criteo-uplift-v2.1.csv',\n                   dtype={'f0': 'float32',\n                          'f1': 'float32',\n                          'f2': 'float32',\n                          'f3': 'float32',\n                          'f4': 'float32',\n                          'f5':'float32',\n                          'f6': 'float32',\n                          'f7': 'float32',\n                          'f8': 'float32',\n                          'f9': 'float32',\n                          'f10': 'float32',\n                          'f11': 'float32',\n                          'treatment': 'int8',\n                          'conversion': 'int8',\n                          'visit': 'int8',\n                          'exposure':'int8'})\n                          # ,nrows=10000000)\n","e7ad53ed":"data.shape","63aa8a25":"\ndata.head(20)","64ce75d0":"data.isnull().sum()","29825ee7":"from scipy.stats import norm\nfeats = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11']\ndef plot_numerical(feature):\n    sns.set()\n    plt.style.use('fivethirtyeight')\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n    sns.distplot(data[feature], ax=axes[0], label='train', color='k', kde_kws={\"bw\":0.01},bins = 40, fit=norm)\n    sns.distplot(np.log1p(data[feature]),  ax=axes[1], color=\"darkcyan\",kde_kws={\"bw\":0.01}, bins=40, fit=norm)\n    \n    axes[0].set_title('train distribution')\n    axes[1].set_title('log1p')\n    \n    \n   \n","e11f4324":"for c in feats:\n    plot_numerical(c)    ","aa63b317":"cat_cols = [c for c in data.columns if data[c].dtypes =='int8']\n\ndef analyse_cats(df, cat_cols):\n    \n    for c in cat_cols:\n                      \n        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n        sns.countplot(df[c], ax=axes[0], label='data counts', palette='bone');\n        sns.distplot(df[c],  ax=axes[1], kde_kws={\"bw\":0.01},color=\"darkcyan\");\n\n\n        fig.suptitle(c, fontsize=18);\n        axes[0].set_title('data');\n        print(c ,': ', df[c].value_counts())\n        \nanalyse_cats(data, cat_cols)\n","ded25e26":"import matplotlib.style as style\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (20,15))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(data.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(data.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 25);","d5367f41":"plt.style.use('fivethirtyeight')\nsns.clustermap(data.corr(), center=0, cmap=\"vlag\", method='single', linewidths=.75, figsize=(14, 12))  ","1f997736":"plt.figure(figsize=(7, 4))\nsns.countplot(data.conversion)","50ab44ba":"target = data.pop('conversion')\n\ntrain, test, tar_train, tar_test = train_test_split(data, target, test_size=0.2)\ntrain.shape, test.shape, tar_train.shape, tar_test.shape","e89f4275":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport gc\n","6ae59b3b":"n_splits = 3\nkf = StratifiedKFold(n_splits=n_splits, random_state=2020)\ntrain_oof = np.zeros((train.shape[0],))\ntest_preds = 0\ntrain_oof.shape\n\nfor fold_, (train_index, val_index) in enumerate(kf.split(train, tar_train)):\n    print(\"Fitting fold\", fold_)\n    train_features = train.iloc[train_index]\n    train_target = target.iloc[train_index]\n    \n    val_features = train.iloc[val_index]\n    val_target = target.iloc[val_index]\n    \n    model = HistGradientBoostingClassifier(max_iter=4000, learning_rate=0.01)\n    model.fit(train_features, train_target)\n    val_pred = model.predict_proba(val_features)\n    train_oof[val_index] = val_pred[:,1]\n    print(\"Fold AUC:\", roc_auc_score(val_target, val_pred[:,1]))\n    test_preds += model.predict_proba(test)[:,1]\/n_splits\n    del train_features, train_target, val_features, val_target\n    gc.collect()","ac95cdf1":"from sklearn.metrics import confusion_matrix\noof_r = np.where(train_oof > 0.5, 1, 0)\ncf_matrix = confusion_matrix(tar_train, (oof_r)) \ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.4)\nplt.style.use('seaborn-poster')\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='vlag')","f668452f":"### HistgradientBoosting","3393c352":"### Memory Reduction","e722d625":"### Build unseen data","45e988f2":"### HistGradientBoosting","76bc18da":"### Confusion Matrix"}}