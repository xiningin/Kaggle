{"cell_type":{"45e9ede0":"code","c7d7d6d4":"code","aad0d74e":"code","c38f8cbc":"code","39542287":"code","0963e733":"code","94ddbe80":"code","b874ad92":"code","e01f6fad":"code","0518d98c":"code","378d1d77":"markdown","851ce6d2":"markdown","9d579730":"markdown","d9a9366c":"markdown","bbc7d3df":"markdown","8604ad46":"markdown","d2234b37":"markdown","471d3c86":"markdown","c4988b2d":"markdown","bce12095":"markdown","3dd21599":"markdown","ccb8f3e9":"markdown","58e5fb7f":"markdown","6b27f978":"markdown","e707de32":"markdown","5c44c210":"markdown","37e5d288":"markdown"},"source":{"45e9ede0":"!pip install sentence_transformers\n!pip install nltk","c7d7d6d4":"import pandas as pd\nimport nltk\nfrom nltk import sent_tokenize,word_tokenize\nnltk.download('punkt')\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport math\nsbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\nimport time","aad0d74e":"df = pd.read_csv(\"..\/input\/wine-reviews\/winemag-data-130k-v2.csv\")\ndf.head()","c38f8cbc":"def cosine(u, v):\n    return np.dot(u, v) \/ (np.linalg.norm(u) * np.linalg.norm(v))","39542287":"def convert_to_embeddings(hypothesis):\n  text = hypothesis\n  hypothesis_embeddings = {}\n  hypothesis = sent_tokenize(hypothesis)\n  sum = 0\n  for sent in hypothesis:\n    var = sbert_model.encode([sent])[0]\n    sum+=var\n    hypothesis_embeddings[sent] = var\n\n  centroid = sum\/len(hypothesis)\n  content_relevance_score = content_relevance(centroid,hypothesis_embeddings)\n  sentence_novelty_score = sentence_novelty(content_relevance_score,hypothesis_embeddings)\n  sentence_position_score = sentence_position(hypothesis)\n  return total_score(content_relevance_score,sentence_novelty_score,sentence_position_score,text)\n  ","0963e733":"def content_relevance(centroid,hypothesis_embeddings):\n  d = {}\n  for sentence,embed in hypothesis_embeddings.items():\n    d[sentence] = cosine(centroid,embed)\n  return d","94ddbe80":"def sentence_novelty(content_relevance_score,hypothesis_embeddings):\n  novel_sentences = {}\n  TAU = 0.95\n  for sent1,embed1 in hypothesis_embeddings.items():\n    d = {}\n    for sent2,embed2 in hypothesis_embeddings.items():\n      if sent1!=sent2:\n        d[sent2] = cosine(embed1,embed2)\n\n    all_values = d.values()\n    max_similarity = max(all_values) #getting max similarity\n    sent2 = max(d, key=d.get)   #getting sentence with max similarity\n\n    if max_similarity<TAU:\n      novel_sentences[sent1] = 1\n\n    if max_similarity>TAU:\n      if content_relevance_score[sent1]>content_relevance_score[sent2]:\n        novel_sentences[sent1] = 1\n\n      else:\n        novel_sentences[sent1] = 1-max_similarity\n  #print(\"novel sentences\",len(novel_sentences))\n  #print(novel_sentences)\n  return novel_sentences","b874ad92":"def sentence_position(hypothesis):\n  score_sent = {}\n  for i,sent in enumerate(hypothesis):\n    score_sent[sent] = max(0.5,math.exp(-(i+1)\/(len(hypothesis)**(1\/3))))\n  return score_sent","e01f6fad":"def total_score(content_relevance_score,sentence_novelty_score,sentence_position_score,text):\n  ALPHA = 0.6\n  BETA = 0.2\n  GAMMA = 0.2\n  final_score = {}\n  for sent in content_relevance_score:\n    final_score[sent] = ALPHA*content_relevance_score[sent]+BETA*sentence_novelty_score[sent]+GAMMA*sentence_position_score[sent]\n  final_score = {k: v for k, v in sorted(final_score.items(), key=lambda item: item[1],reverse=True)}\n  summary = \"\"\n  for sent in final_score:\n    if len(summary)<len(word_tokenize(text))\/\/2:   #making summary of approx 50% length\n      summary+=sent\n  return summary","0518d98c":"for i in range(0,10):\n    text = df['description'][i]\n    summary = convert_to_embeddings(text)\n    print(\"TEXT \\n\",text)\n    print(\"SUMMARY \\n\",summary)","378d1d77":"# **Imports**","851ce6d2":"# **Sentence selection**","9d579730":"**The proposed algorithm is from Salima Lamsiyah\u2019s paper \"An Unsupervised Method for Extractive Multi-Document Summarization based on Centroid Approach and Sentence Embeddings\"**\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S0957417420308952\n\n**If you like this notebook, please dont forget to upvote and follow me for more upcoming research implementations**","d9a9366c":"# **Import Wine Dataset**","bbc7d3df":"# **Libraries to be installed**","8604ad46":"# **Content relevance**","d2234b37":"# **Sentence Postion score**","471d3c86":"# **Cosine similarity**","c4988b2d":"1. Split the review into sentences.\n2. Convert the sentence to embedding using pretrained models such as bert (sentence transformer in this case).\n3. Compute the centroid embedding using the all the sentence's embeddings.\n4. Compute the content relevance score for each sentence(more similar to centroid) means higher content relevance.\n5. Compute the novelty score of each sentence. Higher novelty score defines lesser similarity with other sentences, hence removing redundant sentences.\n6. Calculate the position of sentence in the text. Sentence appearing earlier in the text are more important than later.\n7. Sum up the total score based on above factors giving more weightage to content relevance(0.6).\n8. Form up the summary based on score and length of summary needed.","bce12095":"# **Sentence novelty**","3dd21599":"Sentence relevance score defines the similarity between the embeddings of the documents and the centroid embedding. As I said earlier, sentences whose embeddings are more similar to the centroid embedding(cosine similarity in this case), the more relevant they are than other sentences. The value is between 0 and 1. Higher the sentence relevance score of a sentence, the more relevant it is.","ccb8f3e9":"Sentence novelty score filters the sentences with similar information as they don\u2019t impart any distinct information to the document. It specifies a low score to the sentence whose context is similar to other sentences and a high score to sentences with novel content.","58e5fb7f":"According to Edmundson, sentences at the beginning of a document are more relevant than the later ones. The sentence position score metric used in this algorithm was introduced by Joshi in 2019.","6b27f978":"<img src= \"https:\/\/images.unsplash.com\/photo-1532153975070-2e9ab71f1b14?ixlib=rb-1.2.1&ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&auto=format&fit=crop&w=750&q=80\" alt =\"document\" style='width: 800px;'>","e707de32":"After the calculation of all the scores for all the sentences in the doc, the scores are then linearly added with a weighted sum of 3 scores. Sentences with higher final scores are selected to be a part of the summary (keeping in mind the length of the summary).","5c44c210":"# **Convert the sentences to embeddings & centroid computation**","37e5d288":"# Procedure for unsupervised summarization using centroid & embedding "}}