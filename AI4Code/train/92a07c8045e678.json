{"cell_type":{"c320f39b":"code","455063cb":"code","81e43b57":"code","ab069e3a":"code","2b315cc3":"code","6600b46c":"code","f8274042":"code","96b6f774":"code","3f48f6f9":"code","f980ae0e":"code","3f911cdf":"code","d63932b7":"code","09320229":"code","be361e68":"code","565433ff":"code","918d5d7e":"code","3fa7e09a":"code","2cc77074":"code","da75c774":"markdown","939e0574":"markdown"},"source":{"c320f39b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom skimage.filters import threshold_otsu\nimport lightgbm as lgb\nimport gc\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","455063cb":"df_input = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv\",index_col=0)\ndf_test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv\",index_col=0)","81e43b57":"print(df_input.shape)\ndf_input.head()","ab069e3a":"print(df_test.shape)\ndf_test.head()","2b315cc3":"df_input.isnull().sum().sum()","6600b46c":"df_input.target.value_counts()","f8274042":"features = [x for x in df_input.columns.values if x[0]==\"f\"]","96b6f774":"df_input['abs_sum'] = df_input[features].abs().sum(axis=1)\ndf_input['sem'] = df_input[features].sem(axis=1)\ndf_input['std'] = df_input[features].std(axis=1)\ndf_input['avg'] = df_input[features].mean(axis=1)\ndf_input['max'] = df_input[features].max(axis=1)\ndf_input['min'] = df_input[features].min(axis=1)","3f48f6f9":"df_test['abs_sum'] = df_test[features].abs().sum(axis=1)\ndf_test['sem'] = df_test[features].sem(axis=1)\ndf_test['std'] = df_test[features].std(axis=1)\ndf_test['avg'] = df_test[features].mean(axis=1)\ndf_test['max'] = df_test[features].min(axis=1)\ndf_test['min'] = df_test[features].min(axis=1)","f980ae0e":"X = df_input.drop([\"target\"], axis=1)\ny = df_input[\"target\"]","3f911cdf":"scaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X))\ntest_data = pd.DataFrame(scaler.transform(df_test))","d63932b7":"print(X.shape)\nprint(y.shape)\nprint(test_data.shape)","09320229":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","be361e68":"del df_test, df_input, scaler\ngc.collect()","565433ff":"# helper functions\ndef get_auc(y_true, y_hat):\n    fpr, tpr, _ = roc_curve(y_true, y_hat)\n    score = auc(fpr, tpr)\n    return score","918d5d7e":"# best params\nlgbm1_params = {\n    'metric' : 'auc',\n    'max_depth' : 3,\n    'num_leaves' : 7,\n    'n_estimators' : 5000,\n    'colsample_bytree' : 0.3,\n    'subsample' : 0.5,\n    'random_state' : 42,\n    'reg_alpha' : 18,\n    'reg_lambda' : 17,\n    'learning_rate' : 0.095,\n    'device' : 'cpu',\n    'objective' : 'binary'\n}\n\nlgbm2_params = {\n    'metric' : 'auc',\n    'objective': 'binary',\n    'n_estimators': 10000,\n    'random_state': 42,\n    'learning_rate': 0.095,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'device' : 'cpu',\n    'max_depth' : 3,\n    'num_leaves' : 7\n}\n\nlgbm3_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n    'device_type': 'cpu', \n    'n_estimators': 10000, \n    'learning_rate': 0.12230165751633416, \n    'num_leaves': 1400, \n    'max_depth': 8, \n    'min_child_samples': 3100, \n    'reg_alpha': 10, \n    'reg_lambda': 65, \n    'min_split_gain': 5.157818977461183, \n    'subsample': 0.5, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.2\n}\n\ncatb1_params = {\n    'eval_metric' : 'AUC',\n    'iterations': 15585, \n    'objective': 'CrossEntropy',\n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'CPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\ncatb2_params = {\n    'eval_metric' : 'AUC',\n    'depth' : 5,\n    'grow_policy' : 'SymmetricTree',\n    'l2_leaf_reg' : 3.0,\n    'random_strength' : 1.0,\n    'learning_rate' : 0.1,\n    'iterations' : 10000,\n    'loss_function' : 'CrossEntropy',\n    'task_type' : 'CPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\nxgb1_params = {\n    'eval_metric' : 'auc',\n    'lambda': 0.004562711234493688, \n    'alpha': 7.268146704546314, \n    'colsample_bytree': 0.6468987558386358, \n    'colsample_bynode': 0.29113878257290376, \n    'colsample_bylevel': 0.8915913499148167, \n    'subsample': 0.37130229826185135, \n    'learning_rate': 0.021671163563123198, \n    'grow_policy': 'lossguide', \n    'max_depth': 18, \n    'min_child_weight': 215, \n    'max_bin': 272,\n    'n_estimators': 10000,\n    'random_state': 0,\n    'use_label_encoder': False,\n    'objective': 'binary:logistic',\n    'tree_method': 'hist',\n    'predictor': 'cpu_predictor'\n}\n\nxgb2_params = dict(\n    eval_metric='auc',\n    max_depth=3,\n    subsample=0.5,\n    colsample_bytree=0.5,\n    learning_rate=0.01187431306013263,\n    n_estimators=10000,\n    n_jobs=-1,\n    use_label_encoder=False,\n    objective='binary:logistic',\n    tree_method='hist',\n    predictor='cpu_predictor'\n)\n\nxgb3_params = {\n    'eval_metric': 'auc', \n    'objective': 'binary:logistic', \n    'tree_method': 'hist', \n    'predictor': 'cpu_predictor', \n    'n_estimators': 10000, \n    'learning_rate': 0.01063045229441343, \n    'gamma': 0.24652519525750877, \n    'max_depth': 4, \n    'min_child_weight': 366, \n    'subsample': 0.6423040816299684, \n    'colsample_bytree': 0.7751264493218339, \n    'colsample_bylevel': 0.8675692743597421, \n    'lambda': 0, \n    'alpha': 10\n}","3fa7e09a":"%%time\nmodels = [\n    ('lgbm1', LGBMClassifier(**lgbm1_params))\n    #('lgbm2', LGBMClassifier(**lgbm2_params)),\n    #('lgbm3', LGBMClassifier(**lgbm3_params)),\n    #('catb1', CatBoostClassifier(**catb1_params)),\n    #('catb2', CatBoostClassifier(**catb2_params)),\n    #('xgb1', XGBClassifier(**xgb1_params)),\n    #('xgb2', XGBClassifier(**xgb2_params)),\n    #('xgb3', XGBClassifier(**xgb3_params))\n]\n\noof_pred_tmp = dict()\ntest_pred_tmp = dict()\nscores_tmp = dict()\n\n#kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nkf = StratifiedKFold(n_splits=1, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    for name, model in models:\n        if name not in scores_tmp:\n            oof_pred_tmp[name] = list()\n            oof_pred_tmp['y_valid'] = list()\n            test_pred_tmp[name] = list()\n            scores_tmp[name] = list()\n     \n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_valid,y_valid)],\n            verbose=0\n        )\n        \n        pred_valid = model.predict_proba(X_valid)[:,1]\n        score = get_auc(y_valid, pred_valid)\n        \n        scores_tmp[name].append(score)\n        oof_pred_tmp[name].extend(pred_valid)\n        \n        print(f\"Fold: {fold + 1} Model: {name} Score: {score}\")\n        print('--'*20)\n        \n        y_hat = model.predict_proba(test_data)[:,1]\n        test_pred_tmp[name].append(y_hat)\n    \n    oof_pred_tmp['y_valid'].extend(y_valid)\n        \nfor name, model in models:\n    print(f\"Overall Validation Score | {name}: {np.mean(scores_tmp[name])}\")\n    print('::'*20)","2cc77074":"#submit_df = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\n#submit_df['target'] = y_pred_final_lgb.ravel()\n#submit_df.to_csv(\"LGB_Submission.csv\", index=False)\n#submit_df.head()","da75c774":"Classes are balanced with almost 50-50. No need to handle class imbalance.","939e0574":"No null value present in dataset, hence no need to impute missing value through any means."}}