{"cell_type":{"22646298":"code","6de45dfd":"code","34b5a8bf":"code","d05b53cc":"code","5525df50":"code","c40da8a0":"code","6bddf44d":"code","79cdb104":"code","025ec3a8":"code","7fb9ad26":"code","89ac8472":"code","0c0bed85":"code","0acbd978":"code","4c8dd3fd":"code","2dc6db25":"code","53c69764":"code","12a114f9":"code","16c08e7d":"code","57898e87":"code","23329357":"code","c44fc6f8":"code","707e9682":"code","62464db1":"code","9088aac0":"code","f58ce890":"code","12671250":"code","5a808342":"code","438af729":"code","04159cd6":"code","0191b27f":"code","5d6be284":"code","c8410106":"code","8617aedd":"code","ee872547":"code","040e991e":"code","ebf769fd":"code","f6766e0f":"code","d580a740":"code","6e044cb3":"code","887f7a27":"code","78e93fd2":"code","3cbd0739":"code","0cfe9760":"code","4241e147":"code","c4a41ce3":"code","0fcd4f20":"code","4d736905":"code","3490df9e":"code","49d73cca":"code","a8c1bb09":"code","e59ea0a8":"code","c8b01382":"code","2cacb487":"code","0a295bb8":"code","5c978c7c":"code","24a14fe9":"code","4f3ce24f":"code","c8a17836":"code","2aca2062":"code","861b712b":"code","bb6970d9":"code","de29883e":"code","2b0a14f3":"markdown","6a8e8046":"markdown","b153044b":"markdown","76438413":"markdown","4f5bbd34":"markdown","9e7a5be9":"markdown","bc6817c9":"markdown","f7b14c02":"markdown","627189b6":"markdown","336754a7":"markdown","867e5595":"markdown","a0ee79b3":"markdown","ce126424":"markdown","132c176d":"markdown","663ee800":"markdown","69f227dc":"markdown","f4c14f80":"markdown","1008e551":"markdown","53dfe90f":"markdown"},"source":{"22646298":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime as dt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom lifetimes import BetaGeoFitter\nfrom lifetimes import GammaGammaFitter\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6de45dfd":"from IPython.display import Image\n!ls ..\/input\/crmanalyticsfigures\/","34b5a8bf":"Image(\"..\/input\/crmanalyticsfigures\/KPI.jpeg\")","d05b53cc":"Image(\"..\/input\/crmanalyticsfigures\/Cohort.png\")","5525df50":"Image(\"..\/input\/crmanalyticsfigures\/RFM.png\")","c40da8a0":"Image(\"..\/input\/crmanalyticsfigures\/RFMSegmentation.png\")","6bddf44d":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)","79cdb104":"df_ = pd.read_csv(\"..\/input\/online-retail-ii-uci\/online_retail_II.csv\")\n# Let's copy the dataset for furher changes\ndf = df_.copy()\ndf.head()","025ec3a8":"df.shape","7fb9ad26":"# InvoiceNo column has some features that needs to be clean\n# If the InvoiceNo data has \"C\" letter that means there was an cancellation\n# So, let's get rid of that unwanted data\ndf = df[~df[\"Invoice\"].str.contains(\"C\", na=False)]","89ac8472":"# To define recency, we need to define analyzing date\ndf[\"InvoiceDate\"].max()\ntoday_date = dt.datetime(2011, 12, 11)\ndf['InvoiceDate']=pd.to_datetime(df['InvoiceDate'])","0c0bed85":"rfm = df.groupby('Customer ID').agg({'InvoiceDate': lambda date: (today_date - date.max()).days,\n                                     'Invoice': lambda num: num.nunique(),\n                                     'Price': lambda Price: Price.sum()})\nrfm.columns = ['recency', 'frequency', 'monetary']\nrfm.head()","0acbd978":"rfm.describe().T","4c8dd3fd":"# Cancellation processes affected our dataset as -41451 which shouldn\u2019t include in our dataset\nrfm = rfm[rfm[\"monetary\"] > 0]","2dc6db25":"# Recency\nrfm[\"recency_score\"] = pd.qcut(rfm['recency'], 5, labels=[5, 4, 3, 2, 1])\n# Frequency\nrfm[\"frequency_score\"] = pd.qcut(rfm['frequency'].rank(method=\"first\"), 5, labels=[1, 2, 3, 4, 5])\n# Monetary\nrfm[\"monetary_score\"] = pd.qcut(rfm['monetary'], 5, labels=[1, 2, 3, 4, 5])","53c69764":"rfm.head()","12a114f9":"# Let's use Recency and Frequency for calculating the RFM Score\n\nrfm[\"RFM_SCORE\"] = (rfm['recency_score'].astype(str) +\n                    rfm['frequency_score'].astype(str))\n\nrfm.head()","16c08e7d":"rfm[rfm[\"RFM_SCORE\"] == \"55\"].head()  # champions","57898e87":"rfm[rfm[\"RFM_SCORE\"] == \"11\"].head()  # hibernating","23329357":"# RFM naming\nseg_map = {\n    r'[1-2][1-2]': 'hibernating',\n    r'[1-2][3-4]': 'at_Risk',\n    r'[1-2]5': 'cant_loose',\n    r'3[1-2]': 'about_to_sleep',\n    r'33': 'need_attention',\n    r'[3-4][4-5]': 'loyal_customers',\n    r'41': 'promising',\n    r'51': 'new_customers',\n    r'[4-5][2-3]': 'potential_loyalists',\n    r'5[4-5]': 'champions'\n}","c44fc6f8":"# Segmentation using RFM scores and RFM naming\nrfm['segment'] = rfm['RFM_SCORE'].replace(seg_map, regex=True) ","707e9682":"# Group by each segment fro RFM score as mean and count\nrfm[[\"segment\", \"recency\", \"frequency\", \"monetary\"]].groupby(\"segment\").agg([\"mean\", \"count\"])","62464db1":"# Show the need attantion segment\nrfm[rfm[\"segment\"] == \"need_attention\"].head()","9088aac0":"# Listing the new customers segment indexes\nrfm[rfm[\"segment\"] == \"new_customers\"].index ","f58ce890":"# Creating new customers dataframe\nnew_df = pd.DataFrame()\nnew_df[\"new_customer_id\"] = rfm[rfm[\"segment\"] == \"new_customers\"].index\nnew_df.head()","12671250":"# Saving the new customers dataframe to csv file\nnew_df.to_csv(\"new_customers.csv\")","5a808342":"df.head()","438af729":"# Data Preparation\ndf = df[~df[\"Invoice\"].str.contains(\"C\", na=False)]\ndf = df[(df['Quantity'] > 0)]\ndf.dropna(inplace=True)\ndf[\"TotalPrice\"] = df[\"Quantity\"] * df[\"Price\"]","04159cd6":"cltv_c = df.groupby('Customer ID').agg({'Invoice': lambda x: x.nunique(),\n                                        'Quantity': lambda x: x.sum(),\n                                        'TotalPrice': lambda x: x.sum()})","0191b27f":"cltv_c.columns = ['total_transaction', 'total_unit', 'total_price']\ncltv_c.head()","5d6be284":"# Average Order Value (average_order_value = total_price \/ total_transaction)\ncltv_c['avg_order_value'] = cltv_c['total_price'] \/ cltv_c['total_transaction']","c8410106":"# Purchase Frequency (total_transaction \/ total_number_of_customers)\ncltv_c[\"purchase_frequency\"] = cltv_c['total_transaction'] \/ cltv_c.shape[0]","8617aedd":"# Repeat Rate & Churn Rate\nrepeat_rate = cltv_c[cltv_c.total_transaction > 1].shape[0] \/ cltv_c.shape[0]\nchurn_rate = 1 - repeat_rate","ee872547":"# Profit Margin (profit_margin =  total_price * 0.10)\ncltv_c['profit_margin'] = cltv_c['total_price'] * 0.10","040e991e":"# Customer Value\ncltv_c['customer_value'] = (cltv_c['avg_order_value'] * cltv_c[\"purchase_frequency\"]) \/ churn_rate","ebf769fd":"# Customer Lifetime Value (CLTV = (customer_value \/ churn_rate) x profit_margin)\ncltv_c['cltv'] = cltv_c['customer_value'] * cltv_c['profit_margin']\ncltv_c.head()","f6766e0f":"# But there is no scale for the output. So let's scale it \nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler.fit(cltv_c[[\"cltv\"]])\ncltv_c[\"scaled_cltv\"] = scaler.transform(cltv_c[[\"cltv\"]])\ncltv_c.sort_values(by=\"scaled_cltv\", ascending=False).head()","d580a740":"# Segmentation based on scaled customer lifetime value\ncltv_c[\"segment\"] = pd.qcut(cltv_c[\"scaled_cltv\"], 4, labels=[\"D\", \"C\", \"B\", \"A\"])\ncltv_c.head()","6e044cb3":"cltv_c[[\"total_transaction\", \"total_unit\", \"total_price\", \"cltv\", \"scaled_cltv\"]].sort_values(by=\"scaled_cltv\",ascending=False).head()","887f7a27":"# Group by segments\ncltv_c.groupby(\"segment\")[[\"total_transaction\", \"total_unit\", \"total_price\", \"cltv\", \"scaled_cltv\"]].agg(\n    {\"count\", \"mean\", \"sum\"})","78e93fd2":"Image(\"..\/input\/crmanalyticsfigures\/CustomerLifetime.png\")","3cbd0739":"Image(\"..\/input\/crmanalyticsfigures\/BuyTillYouDie.png\")","0cfe9760":"#Poison Process\nImage(\"..\/input\/crmanalyticsfigures\/Poison-Process.png\")","4241e147":"Image(\"..\/input\/crmanalyticsfigures\/Gamma-Distrubution.png\")","c4a41ce3":"Image(\"..\/input\/crmanalyticsfigures\/Geometric-Distribution.png\")","0fcd4f20":"Image(\"..\/input\/crmanalyticsfigures\/Shifted-.png\")","4d736905":"Image(\"..\/input\/crmanalyticsfigures\/Beta-Distribution.png\")","3490df9e":"Image(\"..\/input\/crmanalyticsfigures\/BG-NBD-Model-Formula.png\")","49d73cca":"df.head()","a8c1bb09":"# Data Preprocessing\ndef outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.01)\n    quartile3 = dataframe[variable].quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","e59ea0a8":"df.dropna(inplace=True)\ndf = df[~df[\"Invoice\"].str.contains(\"C\", na=False)]\ndf = df[df[\"Quantity\"] > 0]\ndf = df[df[\"Country\"] == \"United Kingdom\"]\n\nreplace_with_thresholds(df, \"Quantity\")\nreplace_with_thresholds(df, \"Price\")\n\n# df.shape  # before: (541910, 10) after: (397925, 10)\n\ndf[\"TotalPrice\"] = df[\"Quantity\"] * df[\"Price\"]","c8b01382":"today_date = dt.datetime(2011, 12, 11)\n\ncltv_df = df.groupby('Customer ID').agg({'InvoiceDate': [lambda date: (date.max() - date.min()).days,\n                                                        lambda date: (today_date - date.min()).days],\n                                        'Invoice': lambda num: num.nunique(),\n                                        'TotalPrice': lambda TotalPrice: TotalPrice.sum()})\n\ncltv_df.columns = cltv_df.columns.droplevel(0)\ncltv_df.columns = ['recency', 'T', 'frequency', 'monetary']\ncltv_df[\"frequency\"] = cltv_df[\"frequency\"].astype(int)","2cacb487":"# Let's assign monetary value to the mean value for each transaction frequency\ncltv_df[\"monetary\"] = cltv_df[\"monetary\"] \/ cltv_df[\"frequency\"]\n\n# Picking the positive monetary value\ncltv_df = cltv_df[cltv_df[\"monetary\"] > 0]\ncltv_df.head()","0a295bb8":"# Defining Recency value as weekly for BG\/NB Model\ncltv_df[\"recency\"] = cltv_df[\"recency\"] \/ 7\n# Defining T value as weekly for BG\/NB Model\ncltv_df[\"T\"] = cltv_df[\"T\"] \/ 7\n# Frequency needs to higher than 0\ncltv_df = cltv_df[(cltv_df['frequency'] > 1)]","5c978c7c":"!pip install lifetimes","24a14fe9":"##############################################################\n# BG\/NBD Model and Gamma-Gamma Model\n##############################################################\nfrom lifetimes import BetaGeoFitter\nfrom lifetimes import GammaGammaFitter\n\nbgf = BetaGeoFitter(penalizer_coef=0.001)\n\nbgf.fit(cltv_df['frequency'],\n        cltv_df['recency'],\n        cltv_df['T'])\n\nggf = GammaGammaFitter(penalizer_coef=0.01)\nggf.fit(cltv_df['frequency'], cltv_df['monetary'])","4f3ce24f":"cltv = ggf.customer_lifetime_value(bgf,\n                                   cltv_df['frequency'],\n                                   cltv_df['recency'],\n                                   cltv_df['T'],\n                                   cltv_df['monetary'],\n                                   time=6,  # 6 months\n                                   freq=\"W\",  # T's frequency information.\n                                   discount_rate=0.01)\ncltv.head()","c8a17836":"cltv = cltv.reset_index()\ncltv.sort_values(by=\"clv\", ascending=False).head(50)\n\ncltv_final = cltv_df.merge(cltv, on=\"Customer ID\", how=\"left\")\ncltv_final.sort_values(by=\"clv\", ascending=False).head(10)","2aca2062":"from sklearn.preprocessing import MinMaxScaler\n# Standardization of CLTV\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler.fit(cltv_final[[\"clv\"]])\ncltv_final[\"scaled_clv\"] = scaler.transform(cltv_final[[\"clv\"]])\n\n# Sorting the  Scaled CLV results\ncltv_final.sort_values(by=\"scaled_clv\", ascending=False).head()","861b712b":"# define 4 different segment for the CLTV 6 months\ncltv_final[\"expected_purc_1_month\"] = bgf.predict(4*6, cltv_final['frequency'], cltv_final['recency'], cltv_final['T'])\n\ncltv_final[\"expected_average_profit\"] = ggf.conditional_expected_average_profit(cltv_final['frequency'],\n                                       cltv_final['monetary'])\n\ncltv_final.sort_values(by= \"expected_purc_1_month\", ascending=False).head(20)","bb6970d9":"# Defining 4 different segmentation\ncltv_final[\"segment\"] = pd.qcut(cltv_final[\"scaled_clv\"], 4, labels=[\"D\", \"C\", \"B\", \"A\"])\ncltv_final.head()\n\ncltv_final.sort_values(by=\"scaled_clv\", ascending=False).head(50)","de29883e":"# Describing the segments:\ncltv_final.groupby(\"segment\").agg(\n    {\"count\", \"mean\", \"sum\"})","2b0a14f3":"# CRM Analytics\n\nCRM analytics refers to the processing of data that resides in your CRM database to uncover useful insights about customers that businesses can act upon. Integration with analytics makes CRM systems more intelligent in comprehending customers and empowers you to make data-driven decisions.\n\n![image.png](attachment:213a504d-04ef-4183-b05d-b815ced4050b.png)\n\nBesides that, CRM analytics contains different subtitles as below:\n\n- Customer lifecycle\/journey\/funnel\n- Communication with the customers (language, business culture, colors, campaign etc.)\n- Finding new customers\n- Customer retention or desertion\n- Cross-sell \/ up-sell\n\n![image.png](attachment:aa441ec4-8f2d-4c14-834e-63a1bbafe0a3.png)","6a8e8046":"##### Assumption 2\n\nHeterogeneity in transaction rates among customers follows a Gamma distribution. This is equivalent to saying that each customer has its own buy coin (with its very own probability of head and tail).\n\n__Gamma Distribution__\n\nIn probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-square distribution are special cases of the gamma distribution. There are two different parameterizations in common use:\n\nWith a shape parameter k and a scale parameter \u03b8.\nWith a shape parameter \u03b1 = k and an inverse scale parameter \u03b2 = 1\/\u03b8, called a rate parameter.\nIn each of these forms, both parameters are positive real numbers.\n\nThe gamma distribution is the maximum entropy probability distribution (both with respect to a uniform base measure and with respect to a 1\/x base measure) for a random variable X\n","b153044b":"### Calculating RFM Metrics\n\n- Recency\n\n- Frequency\n\n- Monetary","76438413":"### Customer Lifetime Value\n\n- CLTV = (Customer Value \/ Churn Rate) * Profit Margin\n\n- Customer Value = Average Order Value * Purchase Frequency\n\n- Average Order Value = Total Price \/ Total Transaction\n\n- Purchase Frequency = Total Transaction \/ Total Number of Customers\n\n- Churn Rate = 1 - Repeat Rate\n\n- Repeat Rate = Customers that buy products several times \/ Total Customers\n\n- Profit Margin = Total Price * 0.10 (0.10 can change depends on the industry standarts)","4f5bbd34":"## 3) What is RFM ?\n\nRFM stands for Recency, Frequency, and Monetary value. RFM represents a method used for measuring customer value.\n\nThe RFM model implies analyzing past transactional data and using that research to identify different segments of customers based on their purchase history.\n\nRFM specialists usually use it in database marketing and direct marketing, but lately, RFM Analysis has received big attention and is widely used in eCommerce.\n\nThat\u2019s why the main benefit of an RFM analysis consists of being able to address each segment separately, according to what you know about them based on their score on Recency, Frequency, and Monetary value indicators.","9e7a5be9":"### Calculating RFM Scores","bc6817c9":"##### Assumption 3\n\nAfter any transaction, a customer becomes inactive with probability p.\nTherefore the point at which the customer \u201cdrops out\u201d is distributed across transactions according to a (shifted) Geometric distribution.\n\nAfter every transaction, each customer will toss the second coin, the die coin.\nGiven that p is the probability of \u201cdying\u201d, then we can define P(Alive) = 1-p.\nOnce again, let\u2019s plot a random customer probability distribution to better grasp the meaning of this assumption.\n\nAssuming that our customer becomes inactive with probability p = 0.52, then the probability that he becomes inactive after the 2nd transaction is 25%, and the probability that he becomes inactive after the 3rd transaction is 12%.\nAs you see the more the customer purchases the higher his probability of being alive.","f7b14c02":"#### Time to Practice Customer Lifetime Prediction \ud83d\ude80\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb\n\n - Recency(derived from tx) : the age of the customer at the moment of his last purchase, which is equal to the duration between a customers first purchase and their last purchase\n \n- Frequency(x): the number of periods in which  the customer has made a repeat purchase\n\n- Age of the customer(T): the age of the customer at the end of the period under study, which is equal to the duration between a customer's first purchase and the last day in the dataset.\n","627189b6":"### Customer Lifetime Value Prediction\n\nWhat is a customer worth? How many more times a customer will purchase before churning? How likely is he to churn within the next 3 months? And above all, how long should we expect a customer to be \u201calive\u201d for?\n\nIn non-contractual business settings, where customers can end their relationship with a retailer at any moment and without notice, this can be even trickier.\n\nAmazon for books (or any other of its product categories without subscription), Zalando for clothing, and Booking.com for hotels are all examples of non-contractual businesses settings. For all these three E-commerces we cannot look at the end date of a customer\u2019s contract to know if he\u2019s \u201calive\u201d (will purchase in the future) or \u201cdead\u201d (will never purchase again). We can only rely on a customer\u2019s past purchases and other less characterizing events (website visits, reviews, etc.).\nBut how do we decide in this scenario if a customer is going to come back or he\u2019s gone for good?\n\n- \u201cBuy \u2018Til You Die\u201d probabilistic models help us in quantifying the lifetime value of a customer by assessing the expected number of his future transactions and his probability of being \u201calive\u201d.\n\n\nCustomer Lifetime Value Prediction\n\n- CLTV = Expected Number of Transaction * Expected Average Profit\n\n- CLTV = BG\/NBM Model * Gamma-Gamma Submodel\n","336754a7":"## 2) Cohort Analysis\n\nA cohort is a group of people who share a common characteristic over a certain period of time, such as users that have become customers at approximately the same time, a graduating class of students, or contact tracing individuals during a pandemic.\n\nCohort analysis is a study that concentrates on the activities of a specific cohort type. A cohort analysis table is used to visually display cohort data in order to help analysts compare different groups of users at the same stage in their lifecycle, and to see the long-term relationship between the characteristics of a given user group.\n\nSource: [Cohort](https:\/\/www.omnisci.com\/technical-glossary\/cohort-analysis#:~:text=Cohort%20analysis%20is%20an%20analytical,common%20characteristics%20prior%20to%20analysis)\n","867e5595":"### Data Understanding","a0ee79b3":"## Let's Code and Practice \ud83d\ude80\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb\n\n- We will use open source Online Retail Dataset\n\nLet's check the dataset for first insight\n\n- InvoiceNo: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'C', it indicates a cancellation.\n\n- StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n\n- Description: Product (item) name. Nominal.\n\n- Quantity: The quantities of each product (item) per transaction. Numeric.\n\n- InvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated.\n\n- UnitPrice: Unit price. Numeric. Product price per unit in sterling (\u00a3).\n\n- CustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n\n- Country: Country name. Nominal. The name of the country where a customer resides.\n\nSource: [Online Retail Dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail+II#)","ce126424":"## 1) KPI (Key Performance Indicator)\n\nA Key Performance Indicator is a measurable value that demonstrates how effectively a company is achieving key business objectives. Organizations use KPIs at multiple levels to evaluate their success at reaching targets. High-level KPIs may focus on the overall performance of the business, while low-level KPIs may focus on processes in departments such as sales, marketing, HR, support and others.\n\n__Oxford's Dictionary definition of KPI:__ A quantifiable measure used to evaluate the success of an organization, employee, etc. in meeting objectives for performance.\n\nKPI contains different subtitles for the business. For instance:\n\n- Customer Acquisition Rate\n- Customer Retention Rate\n- Customer Churn Rate\n- Growth Rate\n\nSource: [KPI](https:\/\/ceaksan.com\/tr\/kpi-nedir)","132c176d":"#### BG\/NBD Model\n\nIn particular, to predict future transactions the model treats the customer purchasing behaviour as a coin tossing game.\n\nEach customer has 2 coins: a buy coin that controls the probability of a customer to purchase, and a die coin that controls the probability of a customer to quit and never purchase again.\n\n##### Assumption 1: \n\n- while active, the number of transactions made by a customer follows a Poisson Process with transaction rate \u03bb (=expected number of transactions in a time interval).\n\nPoison Process:\n\nA Poisson Process is a model for a series of discrete event where the average time between events is known, but the exact timing of events is random. The arrival of an event is independent of the event before (waiting time between events is memoryless). For example, suppose we own a website which our content delivery network (CDN) tells us goes down on average once per 60 days, but one failure doesn\u2019t affect the probability of the next. All we know is the average time between failures. This is a Poisson process that looks like:\n\nThe important point is we know the average time between events but they are randomly spaced (stochastic). We might have back-to-back failures, but we could also go years between failures due to the randomness of the process.\nA Poisson Process meets the following criteria (in reality many phenomena modeled as Poisson processes don\u2019t meet these exactly):\n\n- Events are independent of each other. The occurrence of one event does not affect the probability another event will occur.\n\n- The average rate (events per time period) is constant.\n\n- Two events cannot occur at the same time.\n\nAt every sub-period (1 month) of a specific time interval (12 months) each customer tosses his buy coin and, depending on the result, he purchases or not.\nThe number of transactions (heads) we observe in the period depends on each customer\u2019s probability distribution around \u03bb.\nLet\u2019s plot below a customer\u2019s Poisson probability distribution to visualize what we just said.","663ee800":"### Customer Lifetime Value\n\nCustomer lifetime value (CLV) is one of the key stats to track as part of a customer experience program. CLV is a measurement of how valuable a customer is to your company, not just on a purchase-by-purchase basis but across the whole relationship.\n\n\nWe also can formulaze the Customer Lifetime Value as below:\n\n- CLTV = (Customer Value \/ Churn Rate) * Profit Margin\n\n- Customer Value = Average Order Value * Purchase Frequency\n\n- Average Order Value = Total Price \/ Total Transaction\n\n- Purchase Frequency = Total Transaction \/ Total Number of Customers\n\n- Churn Rate = 1 - Repeat Rate\n\n- Repeat Rate = Customers that buy products several times \/ Total Customers\n\n- Profit Margin = Total Price * 0.10 (0.10 can change depends on the industry standarts)\n\nSource:\n- [Customer Life Time](https:\/\/www.qualtrics.com\/uk\/experience-management\/customer\/customer-lifetime-value\/?rid=ip&prevsite=en&newsite=uk&geo=TR&geomatch=uk)\n\n- [CVT](https:\/\/clevertap.com\/blog\/customer-lifetime-value\/)","69f227dc":"### What are Recency, Frequency and Monetary?\n\n- Recency \u2013 How recent was a customer\u2019s latest purchase from you?\n\n- Frequency \u2013 How often does a customer purchase from you?\n\n- Monetary \u2013 How much does a customer usually spend?\n\n### What is the RFM analysis?\n\nAfteridentifying the three metrics and listed some questions they might provide the answer for, it\u2019s time to learn how to do the RFM analysis.\n\nAn RFM analysis can show you who are the most valuable customers for your business. The ones who buy most frequently, most often, and spend the most.\n\nAnd it all starts with proper segmentation.\n\n- Frequency: very frequent buyers, medium frequency buyers, one transaction only buyers.\n\n- Recency: most recent customers, medium recency customers, least recent customers\n\n- Monetary: customers who spend the most, above-average monetary value, average, low monetary value\n\nCombining the above segments you\u2019ll get more advanced ones, such as:\n\n- Clients who come back frequently but spend very little (high frequency, low monetary, maybe deal hunters)\n\n- Clients who only ordered once but spent above average (they could help you identify pain points that prevented them from ordering again and their monetary value \u2013 higher than average \u2013 is worth digging for insights)\n\n- VIP clients (those who have a high RFM score overall, especially the monetary value, and who bring your business the most revenue)\n\n- Clients who used to have high frequency and monetary value but have stopped ordering from you, so they have low recency (a sign that they might have switched to the competition although they had been loyal to you, and it\u2019s worth finding out why\n\n### RFM Segmentation\n\nThe recency frequency monetary model can help you take segmentation to a whole new level.\n\nThrough an RFM customer segmentation analysis you\u2019ll be able to see:\n\n- Who is included in the 1% of customers that bring most of your revenue\n\n- Which of the loyal customers return the most often\n\n- Who are the customers with big monetary value who placed big orders in the past, but have a low recency score, meaning they haven\u2019t ordered in a long time\n\nBased on these segments you\u2019ll be able to ask yourself questions such as:\n\n- How can I make the high monetary value but low recency customers come back?\n\n- What makes the high recency and frequency customers so loyal? What is it they appreciate in my products or services?\n\n- What other traits (geographical, demographical, behavioral) do my high monetary value customers have in common? How can I use those to create a segment of potential new customers and address them accordingly?\n\n\nSource: \n- [RFM](https:\/\/www.omniconvert.com\/blog\/what-is-rfm\/)\n\n- [RFM Segmentation](https:\/\/medium.com\/@raviatgrowlytics?source=rss-e4b11989be06------2)","f4c14f80":"##### Assumption 5\n\nThe transaction rate \u03bb and the dropout probability p vary independently across customers.\n\n##### Model Output\n\nEventually, by fitting the previously mentioned distributions on the historical customers data we are able to derive a model that for each customer provides:\n\n- P(X(t) = x | \u03bb, p)- the probability of observing x transactions in a time period of length t\n\n- E(X(t) | \u03bb, p)- the expected number of transactions in a time period of length t\n\n- P(\u03c4>t) - the probability of a customer becoming inactive at period \u03c4\n\nThe fitted distributions parameters are then used in the forward-looking customer-base analysis to find the expected number of transactions in a future period of length t for an individual with past observed behavior defined by x, t\u2093, T \u2014 where x = number of historical transactions, t\u2093 = time of last purchase and T = Age of a customer.\n\n__The Shape of the Data:__\n\n- Recency(derived from tx) : the age of the customer at the moment of his last purchase, which is equal to the duration between a customers first purchase and their last purchase\n\n- Frequency(x): the number of periods in which the customer has made a repeat purchase\n\n- Age of the customer(T): the age of the customer at the end of the period under study, which is equal to the duration between a customer's first purchase and the last day in the dataset.","1008e551":"##### Assumption 4\n\nHeterogeneityin p follows a Beta Distribution.\n\nAs for the buy coin, each customer has his own die coin with its own probability of being alive after a specific amount of transactions.\n\nBeta Distribution\n\nIn probability theory and statistics, the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parameterized by two positive shape parameters, denoted by \u03b1 and \u03b2, that appear as exponents of the random variable and control the shape of the distribution.\n\nThe beta distribution has been applied to model the behavior of random variables limited to intervals of finite length in a wide variety of disciplines.\n\nIn Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. The beta distribution is a suitable model for the random behavior of percentages and proportions.\n\n\n[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Beta_distribution)","53dfe90f":"### Creating and Analyzing RFM Segments"}}