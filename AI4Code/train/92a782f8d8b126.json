{"cell_type":{"b67df9b8":"code","25c5dcfd":"code","8fee80d6":"code","939163bd":"code","446f104b":"code","09cee863":"code","81cabc15":"code","0425d2ec":"code","41a9311c":"code","97e71ba4":"code","444b0060":"code","a7ac8ad7":"code","39d3629a":"code","958dc927":"code","0db2d316":"code","adc11d3f":"code","8ec71fb9":"code","d741d852":"code","04a1401d":"code","604248a0":"code","6ac9bd8b":"code","2d1cd87b":"code","a2843610":"code","94dd199e":"code","58d3ee74":"code","8f46d231":"code","42687727":"code","1d35f0dc":"code","62213be7":"code","817b65b4":"code","d68726dc":"code","52b11d68":"code","ef7f7899":"code","76581352":"code","8e3050b7":"code","ec587cde":"code","0d255195":"code","55863d58":"code","0a5ce07a":"code","a55977d7":"code","b95facc5":"code","add1c6b3":"code","a02773d5":"code","495a47ec":"code","ea1c74fa":"code","da6e1398":"code","376fa9e6":"code","f183d051":"code","7d169235":"code","7ab22c4a":"code","2dd871ec":"markdown","ed5b02f0":"markdown","0a91151b":"markdown","d4854801":"markdown","11cc6646":"markdown","e5f75d69":"markdown","7cd4a25f":"markdown","e1bb5f8f":"markdown","d1b38226":"markdown","fa4d98f7":"markdown","6ecb12de":"markdown","41ec30b1":"markdown","36843f6a":"markdown","d0051ead":"markdown","9f480e80":"markdown","1d1897a4":"markdown","879cb702":"markdown","9fcea81c":"markdown"},"source":{"b67df9b8":"# Import Required Python Packages :\n\n# Scientific and Data Manipulation Libraries :\n\nimport numpy as np\nimport pandas as pd\n\n# Data Viz & Regular Expression Libraries :\n\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n# Scikit-Learn ML Libraries :\n\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\n\n# Garbage Collection Libraries :\n\nimport gc\n\n# Boosting Algorithm Libraries :\n\nfrom xgboost                          import XGBClassifier\nfrom catboost                         import CatBoostClassifier\nfrom lightgbm                         import LGBMClassifier\nfrom sklearn.ensemble                 import VotingClassifier","25c5dcfd":"# Viewing data files present in Default Path \"\/kaggle\/input\" :\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8fee80d6":"# Loading data from the default Path \"\/kaggle\/input\/\" + Created data Repository \"health\" :\n# Import Data from Excel Files in .csv format and store in Table format called DataFrame using Pandas :\n\ntrain           = pd.read_csv('\/kaggle\/input\/health\/AV_Train.csv')\nss              = pd.read_csv('\/kaggle\/input\/health\/AV_sample_submmission.csv')\ntest            = pd.read_csv('\/kaggle\/input\/health\/AV_Test.csv')\n\nhealth_camp     = pd.read_csv('\/kaggle\/input\/health\/AV_Health_Camp_Detail.csv')\ncamp_1          = pd.read_csv('\/kaggle\/input\/health\/AV_First_Health_Camp_Attended.csv')\ncamp_2          = pd.read_csv('\/kaggle\/input\/health\/AV_Second_Health_Camp_Attended.csv')\ncamp_3          = pd.read_csv('\/kaggle\/input\/health\/AV_Third_Health_Camp_Attended.csv')\n\npatient_profile = pd.read_csv('\/kaggle\/input\/health\/AV_Patient_Profile.csv')\n\n# data_dict       = pd.read_excel('https:\/\/www.kaggle.com\/vin1234\/janatahack-healthcare-analytics\/Train\/Data_Dictionary.xlsx')","939163bd":"# Python Method 1 : Displays Data Information :\n\ndef display_data_information(data, data_types, dataframe_name):\n    print(\" Information of \",dataframe_name,\": Rows = \",data.shape[0],\"| Columns = \",data.shape[1],\"\\n\")\n    data.info()\n    print(\"\\n\")\n    for VARIABLE in data_types :\n        data_type = data.select_dtypes(include=[ VARIABLE ]).dtypes\n        if len(data_type) > 0 :\n            print(str(len(data_type))+\" \"+VARIABLE+\" Features\\n\"+str(data_type)+\"\\n\"  )        \n\n# Display Data Information of \"patient_profile\" :\n\ndata_types  = [\"float32\",\"float64\",\"int32\",\"int64\",\"object\",\"category\",\"datetime64[ns]\"]\ndisplay_data_information(patient_profile, data_types, \"patient_profile\")","446f104b":"# Python Method 2 : Displays Data Head (Top Rows) and Tail (Bottom Rows) of the Dataframe (Table) :\n\ndef display_head_tail(data, head_rows, tail_rows):\n    display(\"Data Head & Tail :\")\n    display(data.head(head_rows).append(data.tail(tail_rows)))\n#     return True\n\n# Displays Data Head (Top Rows) and Tail (Bottom Rows) of the Dataframe (Table)\n# Pass Dataframe as \"patient_profile\", No. of Rows in Head = 3 and No. of Rows in Tail = 2 :\n\ndisplay_head_tail(patient_profile, head_rows=3, tail_rows=2)","09cee863":"# Python Method 3 : Displays Data Description using Statistics :\n\ndef display_data_description(data, numeric_data_types, categorical_data_types):\n    \n    print(\"Data Description :\")\n    display(data.describe( include = numeric_data_types))\n    print(\"\")\n    display(data.describe( include = categorical_data_types))\n\n# Display Data Description of \"patient_profile\" :\n\ndisplay_data_description(patient_profile, data_types[0:4], data_types[4:7])","81cabc15":"# Display Data Information of \"train\" :\n\ndisplay_data_information(train, data_types, \"train\")","0425d2ec":"# Displays Data Head (Top Rows) and Tail (Bottom Rows) of the Dataframe (Table)\n# Pass Dataframe as \"train\", No. of Rows in Head = 3 and No. of Rows in Tail = 2 :\n\ndisplay_head_tail( train, head_rows=3, tail_rows=2 )","41a9311c":"# Display Data Description of \"train\" :\n\ndisplay_data_description(train, data_types[0:4], data_types[4:7])","97e71ba4":"# Display Data Information of \"test\" :\n\ndisplay_data_information(test, data_types, \"test\")","444b0060":"# Displays Data Head (Top Rows) and Tail (Bottom Rows) of the Dataframe (Table)\n# Pass Dataframe as \"test\", No. of Rows in Head = 3 and No. of Rows in Tail = 2 :\n\ndisplay_head_tail( test, head_rows=3, tail_rows=2 )","a7ac8ad7":"# Display Data Description of \"test\" :\n\ndisplay_data_description(test, data_types[0:4], data_types[4:7])","39d3629a":"# Display Data Information of \"health_camp\" :\n\ndisplay_data_information(health_camp, data_types, \"health_camp\")","958dc927":"# Displays Data Head (Top Rows) and Tail (Bottom Rows) of the Dataframe (Table)\n# Pass Dataframe as \"health_camp\", No. of Rows in Head = 3 and No. of Rows in Tail = 2 :\n\ndisplay_head_tail(health_camp, head_rows=3, tail_rows=2)","0db2d316":"# Display Data Description of \"health_camp\" :\n\ndisplay_data_description(health_camp, data_types[0:4], data_types[4:7])","adc11d3f":"# Checking Percentage(%) of Common Patient_ID's  between train and test data using Unique train values :\n\nnp.intersect1d(train['Patient_ID'], test['Patient_ID']).shape[0]\/train['Patient_ID'].nunique()","8ec71fb9":"# Python Method 4 : Removes Data Duplicates while Retaining the First one - Similar to SQL DISTINCT :\n\ndef remove_duplicate(data):\n    \n    print(\"BEFORE REMOVING DUPLICATES - No. of Rows = \",data.shape[0])\n    data.drop_duplicates(keep=\"first\", inplace=True) \n    print(\"AFTER REMOVING DUPLICATES  - No. of Rows = \",data.shape[0])\n    \n    return data\n\n# Remove Duplicates from \"train\" data :\n\ntrain = remove_duplicate(train)","d741d852":"# Python Method 5 : Fills or Imputes Missing values with Various Methods : \n\ndef fill_missing_values(data, fill_value, fill_types, columns, dataframe_name):\n    \n    print(\"Missing Values BEFORE REMOVAL in \",dataframe_name,\" data\")\n    display(data.isnull().sum())\n    for column in columns :\n        \n        # Fill Missing Values with Specific Value :\n        if \"Value_Fill\" in fill_types :\n            data[ column ] = data[ column ].fillna(fill_value)\n#             print(\"Value_Fill\")\n\n        # Fill Missing Values with Forward Fill  (Previous Row Value as Current Row in Table) :\n        if \"Forward_Fill\" in fill_types :\n            data[ column ] = data[ column ].ffill(axis = 0)\n#             print(\"Forward_Fill\")\n\n        # Fill Missing Values with Backward Fill (Next Row Value as Current Row in Table) :\n        if \"Backward_Fill\" in fill_types :\n            data[ column ] = data[ column ].bfill(axis = 0)\n#             print(\"Backward_Fill\")\n    \n    print(\"Missing Values AFTER REMOVAL in \",dataframe_name,\" data\")\n    display(data.isnull().sum())\n    \n    return data\n\nfill_types = [ \"Forward_Fill\"]\nfill_value = 0\n# Fills or Imputes Missing values in \"Registration_Date\" Column with \"Forward_Fill\" Method in \"train\" : \ntrain = fill_missing_values(train, fill_value, fill_types, [\"Registration_Date\"],\"train\")\n\n# Fills or Imputes Missing values in \"Registration_Date\" Column with \"Forward_Fill\" Method in \"train\" :\ntest  = fill_missing_values(test, fill_value, fill_types, [\"Registration_Date\"],\"test\")","04a1401d":"# Python Method 6 : Displays Unique Values in Each Column of the Dataframe(Table) :\n\ndef display_unique(data):\n    for column in data.columns :\n        \n        print(\"No of Unique Values in \"+column+\" Column are : \"+str(data[column].nunique()))\n        print(\"Actual Unique Values in \"+column+\" Column are : \"+str(data[column].sort_values(ascending=True,na_position='last').unique() ))\n        print(\"NULL Values :\")\n        print(data[ column ].isnull().sum())\n        print(\"Value Counts :\")\n        print(data[column].value_counts())\n        print(\"\")\n        \n# Displays Unique Values in Each Column of \"patient_profile\" :\n\ndisplay_unique(patient_profile)\n\n# Display this info in a Table Format - Improvements coming In Part 2","604248a0":"# Replace all 'None' Values as 'NaN' and Convert to float data-type in \n# 3 Columns - 'Income', 'Education_Score', 'Age' of \"patient_profile\" Dataframe (Table) :\n\npatient_profile[['Income', 'Education_Score', 'Age']] = patient_profile[['Income', 'Education_Score', 'Age']].apply(lambda x: x.replace('None', 'NaN').astype('float'))","6ac9bd8b":"# Replace all Missing Values in \n# 4 Columns - 'City_Type', 'Income', 'Education_Score', 'Age' of \"patient_profile\" Dataframe (Table) : \n\n# Replace \"City_Type\" missing valuea with Specific Value \"J\" :\npatient_profile['City_Type'].fillna('J', inplace=True)\n\n# Replace \"Education_Score\" missing values with Specific Value \"0\" :\npatient_profile['Education_Score']=patient_profile['Education_Score'].replace('NaN',0)\npatient_profile['Education_Score']=patient_profile['Education_Score'].astype(float)\npatient_profile['Education_Score']=np.log1p(patient_profile['Education_Score'])\n\n# Replace \"Age\" missing values with Specific Value \"25\" :\npatient_profile['Age']=patient_profile['Age'].replace('NaN',25)\npatient_profile['Age']=np.log1p(patient_profile['Age'])\n\n# Replace \"Income\" missing values with Specific Value \"7\" :\npatient_profile['Income']=patient_profile['Income'].replace('NaN',7)","2d1cd87b":"# Convert 2 Categorical(String) Columns 'City_Type','Employer_Category' using Label Encode Technique :\n# Docs : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor col in ['City_Type','Employer_Category']:\n    patient_profile[col]= patient_profile[col].astype('str')\n    patient_profile[col]= le.fit_transform(patient_profile[col])","a2843610":"# Create New Feature 1 : \"Health_Camp_ID_freq\" based on Frequency(Value Counts) of \n# Existing Feature(Column) \"Health_Camp_ID\" in \"train\" and \"test\" data :\n\nfor df_tmp in [train, test]:\n  for c in ['Health_Camp_ID']:\n    # mapper = train\n    df_tmp[c + '_freq'] = df_tmp[c].map(df_tmp[c].value_counts(normalize=True))","94dd199e":"# Merge(Combine) train and patient_profile based on 'Patient_ID' with left outer join similar to SQL :\n# Docs 1 : https:\/\/www.geeksforgeeks.org\/sql-join-set-1-inner-left-right-and-full-joins\/\n# Docs 2 : https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/03.07-merge-and-join.html\n\ntrain = pd.merge(train, patient_profile, on = 'Patient_ID', how = 'left')\ntest  = pd.merge(test , patient_profile, on = 'Patient_ID', how = 'left')","58d3ee74":"# Map : 2 Categorical Features(String Columns) - \"Category1\" & \"Category2\" with Numerical Values :\n\nhealth_camp['Category1'] = health_camp['Category1'].map({'First': 1, 'Second': 2, 'Third': 3})\nhealth_camp['Category2'] = pd.factorize(health_camp['Category2'])[0]","8f46d231":"# Technique 4.1 : \n\n# Python Method 7 : Calculates the Time Difference between 2 Given Dates :\n\ndef timediff(duration):\n    duration_in_s = duration.total_seconds()\n    days = divmod(duration_in_s, 86400)[0]\n    return days\n\n# Create New Feature 2 : \"Camp_Duration\" using Time Difference between \n# \"Camp_Start_Date\" & \"Camp_End_Date\" :\nhealth_camp['Camp_Duration']=pd.to_datetime(health_camp['Camp_End_Date'])-pd.to_datetime(health_camp['Camp_Start_Date'])\nhealth_camp['Camp_Duration']=health_camp['Camp_Duration'].apply(timediff)\n\n# Create 3 New Features 3,4,5 : \"camp_start_year\",\"camp_start_month\",\"camp_start_day\" \n# using \"Camp_Start_Date\" :\nhealth_camp['camp_start_year'] = pd.to_datetime(health_camp['Camp_Start_Date']).dt.year\nhealth_camp['camp_start_month'] = pd.to_datetime(health_camp['Camp_Start_Date']).dt.month\nhealth_camp['camp_start_day'] = pd.to_datetime(health_camp['Camp_Start_Date']).dt.day\n\n# Create 3 New Features 6,7,8 : \"camp_end_year\",\"camp_end_month\",\"camp_end_day\" \n# using \"Camp_End_Date\" :\nhealth_camp['camp_end_year'] = pd.to_datetime(health_camp['Camp_End_Date']).dt.year\nhealth_camp['camp_end_month'] = pd.to_datetime(health_camp['Camp_End_Date']).dt.month\nhealth_camp['camp_end_day'] = pd.to_datetime(health_camp['Camp_End_Date']).dt.day","42687727":"# Merge(Combine) train and health_camp based on 'Health_Camp_ID' with left outer join similar to SQL :\n\ntrain = pd.merge(train, health_camp, on = 'Health_Camp_ID', how = 'left')\ntest  = pd.merge(test, health_camp, on = 'Health_Camp_ID', how = 'left')","1d35f0dc":"# Pass : Dataframe as \"camp_1\", No. of Rows in Head = 3 and No of Rows in Tail = 2 :\n\ndisplay_head_tail(camp_1, head_rows=3, tail_rows=2)","62213be7":"# Pass : Dataframe as \"camp_2\", No of Rows in Head = 3 and No of Rows in Tail = 2 :\n\ndisplay_head_tail(camp_2, head_rows=3, tail_rows=2)","817b65b4":"# Pass - Dataframe as \"camp_3\", No of Rows in Head = 3 and No of Rows in Tail = 2 :\n\ndisplay_head_tail(camp_3, head_rows=3, tail_rows=2)","d68726dc":"# Convert Date format to Date-Time Format in \"train\" and \"test\" data :\n\nreg_date          = 'Registration_Date'\ncamp_str_date     = 'Camp_Start_Date' \ncamp_end_date     = 'Camp_End_Date' \nfirst_interaction = 'First_Interaction'\n\nfor df_tmp in [train, test]:\n  df_tmp[reg_date]          = pd.to_datetime(df_tmp[reg_date])\n  df_tmp[camp_str_date]     = pd.to_datetime(df_tmp[camp_str_date])\n  df_tmp[camp_end_date]     = pd.to_datetime(df_tmp[camp_end_date])\n  df_tmp[first_interaction] = pd.to_datetime(df_tmp[first_interaction])\n\ntest_min_date = test[reg_date].min()","52b11d68":"# Create 3 New Features 9,10,11 : \"Days_Between_Reg_and_Start\", \"Days_Between_Reg_and_End\",\n# \"Interaction_Date\" using Difference of \"Registration_Date\",\"Camp_Start_Date\",\"Camp_End_Date\"\n# and \"First_Interaction\" :\n\ntrain['Days_Between_Reg_and_Start'] = (train['Registration_Date'] - train['Camp_Start_Date']).astype('str')\ntest['Days_Between_Reg_and_Start'] = (test['Registration_Date'] - test['Camp_Start_Date']).astype('str')\n\ntrain['Days_Between_Reg_and_End'] = (train['Camp_End_Date'] - train['Registration_Date']).astype('str')\ntest['Days_Between_Reg_and_End'] = (test['Camp_End_Date'] - test['Registration_Date']).astype('str')\n\ntrain['Interaction_Date'] = (train['Registration_Date'] - train['First_Interaction']).astype('str')\ntest['Interaction_Date'] = (test['Registration_Date'] - test['First_Interaction']).astype('str')\n\nEncoder = LabelEncoder()\n\ntrain['Days_Between_Reg_and_Start'] = Encoder.fit_transform(tuple(train['Days_Between_Reg_and_Start']))\ntest['Days_Between_Reg_and_Start']  = Encoder.fit_transform(tuple(test['Days_Between_Reg_and_Start']))\n\ntrain['Days_Between_Reg_and_End'] = Encoder.fit_transform(tuple(train['Days_Between_Reg_and_End']))\ntest['Days_Between_Reg_and_End']  = Encoder.fit_transform(tuple(test['Days_Between_Reg_and_End']))\n\ntrain['Interaction_Date'] = Encoder.fit_transform(tuple(train['Interaction_Date']))\ntest['Interaction_Date'] = Encoder.fit_transform(tuple(test['Interaction_Date']))","ef7f7899":"# Technique 2 : Online_Activity_Score\n# Create New Feature 12 : \"Online_Activity_Score\" by combining all Online and Social Media\n# 'Online_Follower', 'LinkedIn_Shared', 'Twitter_Shared', 'Twitter_Shared', 'Facebook_Shared' :\n\ntrain['Online_Activity_Score']= train['Online_Follower']+train['LinkedIn_Shared']+train['Twitter_Shared']+train['Facebook_Shared']\ntest['Online_Activity_Score'] = test['Online_Follower']+test['LinkedIn_Shared']+test['Twitter_Shared']+test['Facebook_Shared']\n\ndel train['Online_Follower']\ndel train['LinkedIn_Shared']\ndel train['Twitter_Shared']\ndel train['Facebook_Shared']\n\ndel test['Online_Follower']\ndel test['LinkedIn_Shared']\ndel test['Twitter_Shared']\ndel test['Facebook_Shared']","76581352":"# Display Data Information of \"train\" :\n\ndisplay_data_information(train, data_types, \"train\")","8e3050b7":"# Display Data Information of \"test\" :\n\ndisplay_data_information(test, data_types, \"test\")","ec587cde":"# Checking Common No. of rows using Patient_ID's  between train and test data :\n\nnp.intersect1d(train['Patient_ID'], test['Patient_ID']).shape","0d255195":"# Technique 4.2 - Unique Patient Details :\n\n# Create 3 New Features 13,14,15 : \"Unique_camp_year_per_patient\",\"Unique_camp_months_per_patient\",\n# \"Unique_camp_day_per_patient\" using Group by and Unique in \"train\" and \"test\" - SQL GROUP BY & DISTINCT :\n# Reference : https:\/\/stackoverflow.com\/questions\/164319\/is-there-any-difference-between-group-by-and-distinct\n\ntrain['Unique_camp_year_per_patient']=train.groupby(['Patient_ID'])['camp_start_year'].transform('nunique')\ntrain['Unique_camp_months_per_patient']=train.groupby(['Patient_ID'])['camp_start_month'].transform('nunique')\ntrain['Unique_camp_day_per_patient']=train.groupby(['Patient_ID'])['camp_start_day'].transform('nunique')\n\ntest['Unique_camp_year_per_patient']=test.groupby(['Patient_ID'])['camp_start_year'].transform('nunique')\ntest['Unique_camp_months_per_patient']=test.groupby(['Patient_ID'])['camp_start_month'].transform('nunique')\ntest['Unique_camp_day_per_patient']=test.groupby(['Patient_ID'])['camp_start_day'].transform('nunique')","55863d58":"# Create Target(Dependent) Variable - \"Outcome\" based on 3 Conditions given in Problem Statement :\n# Create New Feature 16 : \"id\" using 'Patient_ID', 'Health_Camp_ID' :\n\nfor c in [camp_1, camp_2, camp_3, train]:\n  c['id'] = c['Patient_ID'].astype('str') + c['Health_Camp_ID'].astype('str')\ncamp_3 = camp_3[camp_3['Number_of_stall_visited'] > 0]\n\n# Get the List of All Unique Patients in all 3 Camps \"camp_1\",\"camp_2\",\"camp_3\" :\nall_patients_in_camp = pd.Series(camp_1['id'].tolist() + camp_2['id'].tolist() + camp_3['id'].tolist()).unique()\n\ntrain['target'] = 0\n\n# Set value as 1 : if List of All Unique Patients in Train 'id' feature are present in \"train\" data :\ntrain.loc[train['id'].isin(all_patients_in_camp), 'target'] = 1\n\n# Check Value of 0 and 1 in Target :\ntrain['target'].value_counts(normalize=True)","0a5ce07a":"# Split \"train\" data(Rows) into 2 Parts train - \"trn\" and validation - \"val\"\n# Based on reg_date = \"Registration_Date\" in \"train\" data :\n\ntrn = train[train[reg_date] <  test_min_date]\nval = train[train[reg_date] >= test_min_date]","a55977d7":"# Check Train | Validation | Test SHAPE = (Rows,Columns) :\n \nprint(\"Train SHAPE : \",trn.shape,\" | Validation SHAPE : \",val.shape,\" | Test SHAPE : \",test.shape)","b95facc5":"TARGET_COL = 'target'\n\n# Technique 3 : \"Category3\" , \"target\"  - DROP ID-based and Date-based Columns :\nfeatures = [c for c in trn.columns if c not in ['Category3','Patient_ID', 'Health_Camp_ID', 'Registration_Date', TARGET_COL, 'id', 'Camp_Start_Date', 'Camp_End_Date', 'First_Interaction']]\n\nprint( str(len(features)) +\" Predictors Feature List : \" )\ndisplay( str(features) )","add1c6b3":"# Skipped for Now - Will explain in JanataHack HealthCare 10 Step Solution : Part-2 \ud83d\ude0a","a02773d5":"# Algorithm 0 : Baseline - LightGBM \n# Refer Official Documentation : https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-Intro.html\n# ------------------------------------------------------------------------------------------\n\n# 1. Create a Baseline - Machine Learning Model :\nlgb_0 = LGBMClassifier(\n                      # No Hyper-Parameters to be passed. \n                      # Default Values will be set to all Hyper-Parameters\n                      )\n\n# 2. Fit the created Machine Learning Model on \"train\" data :\nlgb_0.fit(\n          trn[features], trn[TARGET_COL], \n          eval_set=[(val[features], val[TARGET_COL])], \n          verbose=50,         # Show auc value for Every 50\n          eval_metric = 'auc' # Set Evaluation Metric to 'auc' based on Hackathon\n         )\n\n# 3. Predict the Probability Values of Target \"Outcome\" for \"test\" data :\npreds_0 = lgb_0.predict_proba(test[features])[:, 1]","495a47ec":"# Algorithm 1 : LightGBM \n# Refer Official Documentation : https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-Intro.html\n# ------------------------------------------------------------------------------------------\n\n# 1. Create a Fine_Tuned - Machine Learning Model using Hyper-Parameters :\nlgb_1 = LGBMClassifier(\n                       n_estimators=5500, \n                       learning_rate=0.01, \n                       random_state=1, \n                       colsample_bytree=0.5, \n                       reg_alpha=2, \n                       reg_lambda=2\n                      )\n\n# 2. Fit the created Machine Learning Model on \"train\" data :\nlgb_1.fit(\n          trn[features], trn[TARGET_COL], \n          eval_set=[(val[features], val[TARGET_COL])], \n          verbose=50,\n          eval_metric = 'auc',        # Set Evaluation Metric to 'auc' based on Hackathon\n          early_stopping_rounds = 200 # Set early_stopping to stop training if auc has not improved \n                                      # for \"200\" rounds\n         )\n\n\n# 3. Predict the Probability Values of Target \"Outcome\" for \"test\" data :\npreds_1 = lgb_1.predict_proba(test[features])[:, 1]","ea1c74fa":"# Algorithm 2 : XGBoost \n# Refer Official Documentation : https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_intro.html\n# ------------------------------------------------------------------------------------------------\n\n# 1. Create a Fine_Tuned - Machine Learning Model using Hyper-Parameters :\nxgb_2 = XGBClassifier(learning_rate=0.05, \n                      max_depth=5,\n                      n_estimators=1000, \n                      objective='binary:logistic', \n                      subsample=0.9, \n                      verbose = False, \n                      colsample_bytree=0.9, \n                      min_child_weight=2,\n                      seed = 420\n                     )\n\n# 2. Fit the created Machine Learning Model on \"train\" data :\nxgb_2.fit(\n          trn[features], trn[TARGET_COL], \n          eval_set=[(val[features], val[TARGET_COL])], \n          verbose=100,\n          eval_metric = 'auc', \n          early_stopping_rounds = 200\n         )\n\n# 3. Predict the Probability Values of Target \"Outcome\" for \"test\" data :\npreds_2 = xgb_2.predict_proba(test[features])[:, 1]","da6e1398":"# Algorithm 3 : CatBoost \n# Refer Official Documentation : https:\/\/catboost.ai\/docs\/concepts\/python-quickstart.html\n# ---------------------------------------------------------------------------------------\n\n# 1. Create a Fine_Tuned - Machine Learning Model using Hyper-Parameters :\ncat_3 = CatBoostClassifier(\n                           custom_metric=['AUC'], \n                           learning_rate=0.16, \n                           n_estimators=1000, \n                           subsample=0.085, \n                           max_depth=5, \n                           scale_pos_weight=6.5,  \n                           verbose = False\n                          )\n\n# 2. Fit the created Machine Learning Model on \"train\" data :\ncat_3.fit(\n          train[features], train[TARGET_COL], \n          verbose=50\n         )\n\n# 3. Predict the Probability Values of Target \"Outcome\" for \"test\" data :\npreds_3 = cat_3.predict_proba(test[features])[:, 1]\n\n# 4. Get the Best Score of 'auc'\nprint(cat_3.get_best_score())","376fa9e6":"# Make Submission of Baseline Model :\n\n# Private LB : 0.5973202162114066   | Rank : 176 - Good to Start with !!!\n\nss['Outcome'] = preds_0\nss.to_csv('0. LightGBM_Tree_Baseline.csv',index = False)","f183d051":"# Make Submission of Fine-Tuned Models :\n\n# LightGBM Submission - Good Result \n# Nan Ffill & Bfill   - Private LB : 0.6995712323810263   | Rank : 95 - Improved over Baseline !!!\n# ROC_AUC             - Public  LB : 0.7775599304105607\n# Technique 5         - Private LB : 0.7251274208164504   | Rank : 81 - Poor !!!\n\nss['Outcome'] = preds_1\nss.to_csv('1. LGBM_TUNED_FE.csv',index = False)\n\n# XGBoost Submission  - Ok\n# Nan Ffill & Bfill   - Private LB : 0.7350070361178844   | Rank : 71 - Great Increase over Baseline !!!\n# ROC_AUC             - Public LB  : 0.6215435218767659\n# Technique 5         - Private LB : 0.7330105893486812   | Rank : 73 - No Use !!!\nss['Outcome'] = preds_2\nss.to_csv('2. XGBOOST_TUNED_FE.csv',index = False)\n\n# Catboost Submission - Good Result \n# Nan Ffill & Bfill   - Private LB : 0.7051866242484782  | Rank : 94 - Improved over Baseline !!!\n# ROC_AUC             - Public LB  : 0.7643767548329479\n# Technique 4.1 & 4.2 - Private LB : 0.7463279477304313  | Rank : 61 - Stable BEST Public Score !!!\n# Technique 5         - Private LB : 0.753060169726327   | Rank : 55 - Stable BEST Public Score !!!\n\nss['Outcome'] = preds_3\nss.to_csv('3. CATBOOST_TUNED_FE.csv',index = False)\n","7d169235":"# Combination \/ Ensemble of All 3 - LightGBM + CatBoost works well !!!\n\n# Leaderboard Closed             - Private LB : 0.736838716723437  | Rank : 69\n# Nan Ffill & Bfill              - Private LB : 0.7277982253004195 | Rank : 74 - Great Increase over Baseline !!!\n# Ensemble + Technique 2         - Private LB : 0.737794725875573  | Rank : 68\n# Ensemble + Technique 1         - Private LB : 0.7411696443950796 | Rank : 66\n# Ensemble + Technique 3         - Private LB : 0.743839306084929  | Rank : 64\n# Ensemble + Technique 4.1       - Private LB : 0.7196429698624955 | Rank : 84 - Score DROPPED No USE !!!\n# Ensemble + Technique 4.1 & 4.2 - Private LB : 0.742139803813258  | Rank : 66 - Stable BEST Public Score !!!\n# Ensemble + Technique 5         - Private LB : 0.7401701071611877 | Rank : 66 - Reduced a Bit !\nss['Outcome'] = (preds_1 +preds_2+ preds_3) \/ 3\nss.to_csv('4.1 ALL_3_ENSEMBLE_LGBM_XGBOOST_CATBOOST_TUNED_FE.csv',index = False)\n\n# Ensemble + Technique 4.1 & 4.2 - Private LB : 0.74581535725462    | Rank : 62 - Stable Private Score !!!\nss['Outcome'] = (preds_1+ preds_3) \/ 2\nss.to_csv('4.2 ALL_2_ENSEMBLE_LGBM_CATBOOST_TUNED_FE.csv',index = False)\n\n# Ensemble + Technique 4.1 & 4.2 - Private LB : 0.7427467730696796  | Rank : 66 - No USE !!!\nss['Outcome'] = (preds_2+ preds_3) \/ 2\nss.to_csv('4.3 ALL_2_ENSEMBLE_XGBOOST_CATBOOST_TUNED_FE.csv',index = False)\n\n# Ensemble + Technique 4.1 & 4.2 - Private LB : 0.7321659917047996  | Rank : 73 - Score DROPPED No USE !!!\nss['Outcome'] = (preds_1+ preds_2) \/ 2\nss.to_csv('4.4 ALL_2_ENSEMBLE_LGBM_XGBOOST_TUNED_FE.csv',index = False)","7ab22c4a":"preds_4 = 0\n\n# Nan Ffill & Bfill   - Private LB : 0.7292682224855065 | Rank : 73 - Great Increase over Baseline !!!\n# Technique 6 - Top 3 - Private LB : 0.7543231683179386 | Rank : 54 - Stable Private Score !!!\nfor seed_val in [0,3,1997]: # Combo of 3 SEEDS\n  \n    m = CatBoostClassifier(random_state=seed_val, custom_metric=['AUC'], learning_rate=0.16, n_estimators=1000, subsample=0.085, max_depth=5, \n                           scale_pos_weight=6.5,  verbose = False)\n    \n    m.fit(train[features],train[TARGET_COL])    \n    predict = m.predict_proba(test[features])[:,1]    \n    print (seed_val)\n    preds_4 += predict\n    \npreds_4 = preds_4\/3\n\nss[\"Outcome\"] =  preds_4\nss.to_csv(\"5. TOP_3_PRIORITY_SEED_CatBoost.csv\", index=False)","2dd871ec":"## Steps for Applied Machine Learning (ML) for Hackathons :\n\n1.  Understand the Problem Statement & Import Packages and Datasets.  \n\n2.  Perform EDA (Exploratory Data Analysis) - Understanding the Datasets :\n\n       *       Explore Train and Test Data and get to know what each Column \/ Feature denotes.\n       *       Check for Imbalance of Target Column in Datasets.\n       *       Visualize Count Plots & Unique Values to infer from Datasets.\n            \n3.  Remove Duplicate Rows from Train Data if present.\n\n4.  Fill\/Impute Missing Values Continuous - Mean\/Median\/Any Specific Value & Categorical - Others\/ForwardFill\/BackFill.\n\n5.  Feature Engineering \n\n      *       Feature Selection - Selection of Most Important Existing Features.\n      *       Feature Creation  - Creation  of New Feature from the Existing Features.\n      \n6.  Split Train Data into Train and Validation Data with Predictors(Independent) & Target(Dependent).      \n7.  Data Encoding - Label Encoding, OneHot Encoding and Data Scaling - MinMaxScaler, StandardScaler, RobustScaler\n8.  Create Baseline ML Model\n9.  Improve ML Model,Fine Tune with MODEL Evaluation METRIC - ROC_AUC and Predict Traget \"Outcome\"\n10. Result Submission, Check Leaderboard & Improve ROC_AUC","ed5b02f0":"### 1. Download the .csv file generated in Kaggle from below code - refresh right side output section (\/kaggle\/working) for downloaded files.\n\n### 2. Go to Analytics Vidhya Link below to make Submissions :\n\n### https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-healthcare-analytics\/#SolutionChecker","0a91151b":"![AV_HC_2-720.jpg](attachment:AV_HC_2-720.jpg)","d4854801":"![AV_HC_1.jpg](attachment:AV_HC_1.jpg)","11cc6646":"## 6.  Split Train Data into Train and Validation Data with Predictors(Independent) & Target(Dependent) :\n\n### Target : Favorable outcome\n\n**1. For the first 2 formats, a favourable outcome is defined as getting a health_score, while in the third format it is defined as visiting at least a stall.**\n\n**2. You need to predict the chances (probability) of having a favourable outcome.**","e5f75d69":"![3.%20Public_Private_Score.png](attachment:3.%20Public_Private_Score.png)","7cd4a25f":"## 4.  Fill\/Impute Missing Values Continuous - Mean\/Median\/Any Specific Value & Categorical - Others\/ForwardFill\/BackFill :","e1bb5f8f":"## 8.  Create Baseline ML Model :","d1b38226":"## Link to the Analytics Vidhya HACKATHON - JanataHack HealthCare\n\n### https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-healthcare-analytics\/#ProblemStatement","fa4d98f7":"## 3.  Remove Duplicate Rows from Train data if present :","6ecb12de":"### **<center>\ud83d\ude0a Happy to Build a Generalised and Robust Model to Finish in 69th Position in Private Leaderboard Moving UP by 73 Positions from 142nd Position in Public Leaderboard in the 2 Days Weekend Hackathon held on 17,18th July 2020. If this was my actual code, would have reached 54th Position in Private Leaderboard - leant a few things later after the Competition ended \ud83d\ude0a<\/center>**\n\n### **<center>\ud83d\ude0a For Learning Purpose - You can still participate in your free time to see your Public and Private Scores & Private Rank, though it won't reflect on Leaderboard \ud83d\ude0a<\/center>**\n\n### **<center>\ud83d\ude0a Ask your doubts & Share your thoughts, ideas & feedbacks in Comments below \ud83d\ude0a<\/center>**\n\n### **<center>\ud83d\ude0a Thanks Friends - Stay Tuned for Part 2 for more Analysis and Modelling - UPVOTE & ENCOURAGE if you liked this Notebook \ud83d\ude0a<\/center>**","41ec30b1":"- Duplicates are NOT Found, lets move on with Filling \/ Imputing Missing Values for each columns.","36843f6a":"## 2. Perform EDA (Exploratory Data Analysis) - Understanding the Datasets :    \n\n### 2.1 Explore Train and Test Data and get to know what each Column \/ Feature denotes :","d0051ead":"## 10. Result Submission, Check Leaderboard & Improve ROC_AUC :","9f480e80":"## 7. Data Encoding - Label Encoding, OneHot Encoding &       Data Scaling - MinMaxScaler, StandardScaler, RobustScaler :","1d1897a4":"## 1.  Understand the Problem Statement & Import Packages and Datasets :","879cb702":"## 5.  Feature Engineering\n\n### 5.1 Feature Selection - Selection of Most Important Existing Features\n### 5.2 Feature Creation  - Creation  of New Features from the Existing Features \/ Predictors :","9fcea81c":"## 9. Improve ML Model,Fine Tune with MODEL Evaluation METRIC - ROC_AUC and Predict Traget \"Outcome\" :"}}