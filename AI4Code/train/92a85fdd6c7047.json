{"cell_type":{"b6b98fcd":"code","a13a51af":"code","bed8cb59":"code","8fee182b":"code","946d1b32":"code","5e2d3ab5":"code","7daecfdc":"code","d4f794a9":"code","7363e2c1":"code","50f916d6":"code","478f1102":"code","11a6e028":"code","2c5e9ecd":"code","10b1a0c9":"code","848a41ab":"code","f21b1527":"markdown","cdc99005":"markdown","da306160":"markdown","1caeb037":"markdown","523ce93c":"markdown","2e39a76d":"markdown","40bc428e":"markdown","f0690647":"markdown","a9ce4212":"markdown","c2e3c91d":"markdown","849290c9":"markdown","37e1bcc2":"markdown"},"source":{"b6b98fcd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","a13a51af":"data_train = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")","bed8cb59":"data_train","8fee182b":"data_test","946d1b32":"# M\u00f4 ph\u1ecfng v\u1ec1 \u0111\u1ed9 t\u01b0\u01a1ng quan gi\u1eefa c\u00e1c c\u00e2u insincere v\u00e0 sincere d\u01b0\u1edbi d\u1ea1ng bi\u1ec3u \u0111\u1ed3:\nplt.figure(figsize=(7,5))\nax = sns.countplot(x='target', data=data_train)\ntotal = data_train.shape[0]\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2., height + 3, '{0:.0%}'.format(height\/total), ha=\"center\")\nplt.suptitle('Number of Sincere and Insincere Questions')\nplt.show()","5e2d3ab5":"from sklearn.utils import resample\n\nsincere = data_train[data_train.target == 0]\ninsincere = data_train[data_train.target == 1]\ndata_train_sampled = pd.concat([resample(sincere, replace = True, n_samples = len(insincere)*4), insincere])\ndata_train_sampled","7daecfdc":"y = data_train_sampled['target']\ny.value_counts().plot(kind='bar', rot=0)","d4f794a9":"import re, string\n\ndef clean_text(text):\n\n  # Remove HTML Tags\n  text = re.sub(re.compile('<.*?>'), '', text)\n\n  # Remove [\\], ['], [\"]\n  text = re.sub(r'\\\\', '', text)\n  text = re.sub(r'\\\"', '', text)\n  text = re.sub(r'\\'', '', text)\n\n  # Remove number\n  text = re.sub('[0-9]{5,}','#####', text);\n  text = re.sub('[0-9]{4,}','####', text);\n  text = re.sub('[0-9]{3,}','###', text);\n  text = re.sub('[0-9]{2,}','##', text);\n\n  ## Remove Roman words\n  roman = re.compile(r'^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$');\n  text = roman.sub(r'', text);\n\n  # Convert all text to lowercase\n  text = text.strip().lower()\n\n  # Replace punctuation chars with spaces\n  filters = '!\"\\'#$%@&*()+_-;:<=>.?{}|`\\\\^\\t\\n'\n  translate_dict = dict((c, \" \") for c in filters)\n  translate_map = str.maketrans(translate_dict)\n  text = text.translate(translate_map)\n\n  return text","7363e2c1":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words=\"english\",\n                             preprocessor=clean_text,\n                             ngram_range=(1, 3))\nX = vectorizer.fit_transform(data_train_sampled['question_text'])\nx = vectorizer.transform(data_test['question_text'])","50f916d6":"# Chia dataset th\u00e0nh c\u00e1c t\u1eadp train v\u00e0 test theo t\u1ec9 l\u1ec7 80%, 20%\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, data_train_sampled['target'], test_size=0.2, random_state=42)","478f1102":"from sklearn.metrics import f1_score, accuracy_score, classification_report\n# T\u00ednh f1-score\ndef get_f1(model, name):\n  y_train_pred, y_pred = model.predict(X_train), model.predict(X_test)\n  print(classification_report(y_test, y_pred), '\\n')\n\n  print('{} model with F1 score = {}'.format(name, f1_score(y_test, y_pred)))","11a6e028":"import xgboost as xgb\nxgb = xgb.XGBClassifier()\nxgb.fit(X_train, y_train)","2c5e9ecd":"get_f1(xgb, 'XGBClassifier')","10b1a0c9":"model = xgb\nsubmission = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/sample_submission.csv')\npreds = model.predict(x)\nsubmission.loc[:, 'prediction'] = preds\nsubmission","848a41ab":"submission.to_csv('submission.csv', index=False)","f21b1527":"## **2.Vector h\u00f3a d\u1eef li\u1ec7u**\n* S\u1eed d\u1ee5ng CountVectorizer \u0111\u1ec3 tr\u00edch xu\u1ea5t c\u00e1c t\u1eeb, bi\u1ebfn words th\u00e0nh d\u1ea1ng vectors \u1edf d\u1ea1ng Bag-of-Words b\u1eb1ng c\u00e1ch \u0111\u1ebfm s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c t\u1eeb trong b\u1ed9 d\u1eef li\u1ec7u.\n* TF-IDF (Term Frequency \u2013 Inverse Document Frequency) l\u00e0 1 k\u0129 thu\u1eadt s\u1eed d\u1ee5ng trong khai ph\u00e1 d\u1eef li\u1ec7u v\u0103n b\u1ea3n. Tr\u1ecdng s\u1ed1 n\u00e0y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 t\u1ea7m quan tr\u1ecdng c\u1ee7a m\u1ed9t t\u1eeb trong m\u1ed9t v\u0103n b\u1ea3n. Gi\u00e1 tr\u1ecb cao th\u1ec3 hi\u1ec7n \u0111\u1ed9 quan tr\u1ecdng cao v\u00e0 n\u00f3 ph\u1ee5 thu\u1ed9c v\u00e0o s\u1ed1 l\u1ea7n t\u1eeb xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n nh\u01b0ng b\u00f9 l\u1ea1i b\u1edfi t\u1ea7n su\u1ea5t c\u1ee7a t\u1eeb \u0111\u00f3 trong t\u1eadp d\u1eef li\u1ec7u.\n\nTa chuy\u1ec3n \u0111\u1ed5i d\u1eef li\u1ec7u t\u1eeb text sang th\u1ec3 hi\u1ec7n vector th\u00f4ng qua TfidfVectorizer.","cdc99005":"Do ta \u0111ang l\u00e0m vi\u1ec7c tr\u00ean dataset thi\u1ebfu c\u00e2n b\u1eb1ng => s\u1eed d\u1ee5ng \u0111\u1ea1i l\u01b0\u1ee3ng **F1-score** trung b\u00ecnh \u0111i\u1ec1u h\u00f2a gi\u1eefa precision (\u0111\u1ed9 ch\u00ednh x\u00e1c) v\u00e0 recall (\u0111\u1ed9 bao ph\u1ee7)\n\nF1-score \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n nh\u01b0 sau:\n\n![image.png](attachment:96032297-0608-481f-9dce-0d1877277df6.png)\n\nC\u00e1ch t\u00ednh F1-score s\u1eed d\u1ee5ng sklearn:","da306160":"T\u1eadp d\u1eef li\u1ec7u test g\u1ed3m h\u01a1n 300 ngh\u00ecn d\u00f2ng","1caeb037":"**T\u00ednh c\u00e2n b\u1eb1ng c\u1ee7a t\u1eadp d\u1eef li\u1ec7u**\n\nTa x\u00e9t t\u1ef7 l\u1ec7 c\u00e2u sincere v\u00e0 insincere trong t\u1eadp d\u1eef li\u1ec7u train","523ce93c":"# **Submition**","2e39a76d":"Ta th\u1ea5y t\u1ef7 l\u1ec7 gi\u1eefa 2 lo\u1ea1i c\u00e2u h\u1ecfi kho\u1ea3ng 15:1. \u0110\u00e2y l\u00e0 hi\u1ec7n t\u01b0\u1ee3ng m\u1ea5t c\u00e2n b\u1eb1ng nghi\u00eam tr\u1ecdng d\u1eabn t\u1edbi k\u1ebft qu\u1ea3 d\u1ef1 b\u00e1o k\u00e9m ch\u00ednh x\u00e1c tr\u00ean nh\u00f3m thi\u1ec3u s\u1ed1.\n\n**X\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng d\u1eef li\u1ec7u**\n\nKhi m\u1ed9t b\u00ean chi\u1ebfm \u0111a s\u1ed1 v\u00e0 b\u00ean c\u00f2n l\u1ea1i thi\u1ec3u s\u1ed1, hi\u1ec3n nhi\u00ean s\u1ebd c\u00f3 2 c\u00e1ch \u0111\u1ec3 l\u00e0m c\u00e2n b\u1eb1ng ch\u00fang:\n* T\u0103ng s\u1ed1 l\u01b0\u1ee3ng b\u00ean thi\u1ec3u s\u1ed1 (Over sampling): thu th\u1eadp th\u00eam d\u1eef li\u1ec7u ho\u1eb7c ch\u1ea5p nh\u1eadn m\u1eabu c\u00f3 tr\u00f9ng l\u1eb7p.\n* Gi\u1ea3m s\u1ed1 l\u01b0\u1ee3ng b\u00ean \u0111a s\u1ed1 (Under sampling): c\u00e2n b\u1eb1ng m\u1eabu m\u1ed9t c\u00e1ch nhanh ch\u00f3ng, d\u1ec5 d\u00e0ng ti\u1ebfn h\u00e0nh th\u1ef1c hi\u1ec7n nh\u01b0ng k\u00edch th\u01b0\u1edbc m\u1eabu s\u1ebd gi\u1ea3m \u0111\u00e1ng k\u1ec3.\n\nTa s\u1eed d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p **Under sampling**: gi\u1ea3m b\u1edbt s\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng (1:4) sao cho kh\u00f4ng \u1ea3nh h\u01b0\u1edfng \u0111\u00e1ng k\u1ec3 t\u1edbi kh\u1ea3 n\u0103ng d\u1ef1 b\u00e1o c\u1ee7a m\u00f4 h\u00ecnh.","40bc428e":"# **M\u00f4 t\u1ea3 thu\u1eadt to\u00e1n**\n## **1.Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u**\nB\u01b0\u1edbc \u0111\u1ea7u ti\u00ean ta ph\u1ea3i lo\u1ea1i b\u1ecf nh\u1eefng t\u00e1c nh\u00e2n g\u00e2y nhi\u1ec5u ho\u1eb7c kh\u00f4ng c\u1ea7n thi\u1ebft, c\u00f3 th\u1ec3 g\u00e2y \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn k\u1ebft qu\u1ea3:\n* Lo\u1ea1i b\u1ecf c\u00e1c tag HTML\n* Lo\u1ea1i b\u1ecf c\u00e1c d\u1ea5u nh\u00e1y \u0111\u01a1n, nh\u00e1y k\u00e9p\n* Lo\u1ea1i b\u1ecf c\u00e1c s\u1ed1 th\u1ef1c, s\u1ed1 ph\u1ee9c\n* Lo\u1ea1i b\u1ecf c\u00e1c k\u00ed t\u1ef1 La M\u00e3\n* Thay th\u1ebf c\u00e1c d\u1ea5u, c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t v\u1edbi kho\u1ea3ng tr\u1eafng\n* \u0110\u01b0a t\u1ea5t c\u1ea3 c\u00e1c ch\u1eef c\u00e1i vi\u1ebft hoa, vi\u1ebft th\u01b0\u1eddng v\u1ec1 d\u1ea1ng chung l\u00e0 vi\u1ebft th\u01b0\u1eddng","f0690647":"# **Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh**\n","a9ce4212":"Sau khi chuy\u1ec3n \u0111\u1ed5i t\u1eadp d\u1eef li\u1ec7u text th\u00e0nh c\u00e1c vector, c\u00e1c bi\u1ec3u di\u1ec5n d\u1ea1ng s\u1ed1, ta ph\u00e2n chia b\u1ed9 d\u1eef li\u1ec7u th\u00e0nh t\u1eadp train (d\u00f9ng \u0111\u1ec3 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh) v\u00e0 t\u1eadp test (d\u00f9ng \u0111\u1ec3 ki\u1ec3m tra m\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n) theo t\u1ec9 l\u1ec7 80%, 20%","c2e3c91d":"# **Ph\u00e2n t\u00edch d\u1eef li\u1ec7u**\n","849290c9":"T\u1eadp d\u1eef li\u1ec7u train g\u1ed3m 3 tr\u01b0\u1eddng, trong \u0111\u00f3:\n* **qid**: m\u00e3 c\u00e2u h\u1ecfi\n* **question_text**: n\u1ed9i dung c\u00e2u h\u1ecfi\n* **target**: g\u1ed3m hai gi\u00e1 tr\u1ecb \"insincere\" mang gi\u00e1 tr\u1ecb l\u00e0 1 c\u00f2n \"sincere\" l\u00e0 0","37e1bcc2":"# **M\u00f4 t\u1ea3 b\u00e0i to\u00e1n**\n**Input**: D\u1eef li\u1ec7u d\u1ea1ng text c\u00e2u h\u1ecfi\n\n**Output**: Ph\u00e2n lo\u1ea1i c\u00e1c c\u00e2u insincere v\u00e0 sincere c\u00f3 tr\u00ean h\u1ec7 th\u1ed1ng c\u1ee7a Quora"}}