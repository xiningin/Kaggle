{"cell_type":{"16bdddf2":"code","54c7a27c":"code","d99854d8":"code","a11b52d3":"code","9ef867a4":"code","9cd43ce2":"code","8182b5fe":"code","d57d829d":"code","7bbc6af4":"code","263add7d":"code","4c287bb2":"code","3f8eaf34":"code","7c87bced":"code","da6de2a7":"code","453f155d":"code","cb0a7ce4":"code","38b6eb28":"code","52cf7b5c":"code","227cf783":"code","7e7b5e42":"code","2057be74":"code","57a6e1ae":"code","c5d5ca80":"code","6db43865":"code","4b476254":"code","b5031e5c":"code","31043e5c":"code","c4b1a492":"code","e0cd3200":"code","a45fad6d":"code","51fb2241":"code","e12f6109":"code","6e947efa":"code","13723d77":"code","496d4c4e":"code","14d9a0ed":"code","a04b1f78":"code","377d4c37":"code","3754eeb0":"code","4c20b035":"code","7572745e":"code","b48a4999":"code","98c3a2ad":"code","4814443e":"code","0dea9ce2":"code","64386001":"code","a28c07a4":"code","f373e519":"code","1d3358b2":"code","c54a8cc4":"code","8d653dfe":"code","2a2a1905":"code","c42ee68c":"code","892bdbc1":"code","d8787e50":"code","27e9e2bb":"code","9b6844bb":"code","89a73517":"code","8a6f97d1":"code","cfc69300":"code","d284cbf0":"code","d343e0d3":"code","da4c5d47":"code","f5331782":"code","4d07f9d5":"code","24672acc":"code","678254fb":"code","29dace53":"code","70ecdb7d":"code","4e6e00b8":"code","f88fd164":"code","516e073f":"code","8f0a0d90":"code","786d3ad8":"code","68ec7e8b":"code","936f5be9":"code","f2e7096f":"code","964b0d69":"code","bc619c11":"code","af4bf4bf":"code","aba93db2":"code","8129514f":"code","3ddf13ea":"code","d4307485":"code","10d2fd7a":"code","8ac092f1":"code","f0edf88e":"code","ab9f8e0d":"code","c3c3112b":"code","0fb9b9d2":"code","f274e375":"code","4cea3002":"code","370969e5":"code","518faee9":"markdown","20795e24":"markdown","4efeba50":"markdown","36c6bf9e":"markdown","af5d5eee":"markdown","90f31619":"markdown","57299076":"markdown","5904722d":"markdown","94243a60":"markdown","1eeb007d":"markdown","a08097ca":"markdown","ec316b17":"markdown","d16733e4":"markdown","0f8c2363":"markdown","ef285f41":"markdown","eb6ff138":"markdown","b2374528":"markdown","a94d9b51":"markdown","5cc415f5":"markdown","4faf1a0f":"markdown","df0807ae":"markdown","3d41915c":"markdown","e96a52b4":"markdown","02b55f8f":"markdown","a76912a1":"markdown","cf3d2781":"markdown","f67f556f":"markdown","885e1b02":"markdown","c3bd15db":"markdown","9ff85023":"markdown","23874395":"markdown","19b64637":"markdown","a78d9b63":"markdown","9f9f440c":"markdown","1e7b6a14":"markdown","7e3ac514":"markdown","f8dc5c75":"markdown","a178e68c":"markdown","880462f0":"markdown","185e5f46":"markdown","af26b15b":"markdown","1d0a3905":"markdown","a8156e53":"markdown","451c2d3e":"markdown","70ace548":"markdown","d69a3ffd":"markdown","a6ae244a":"markdown","ad1db533":"markdown","4ce22323":"markdown","5cefc3cc":"markdown","efaf047b":"markdown","081e5cc8":"markdown","71825849":"markdown","24b5f454":"markdown","3f069509":"markdown","fefc2a6a":"markdown","50ed9a83":"markdown","4095c613":"markdown","7b52978f":"markdown"},"source":{"16bdddf2":"# Import libraries and tools\n# Data preprocessing and linear algebra\nimport os, re, random\nimport pandas as pd\nimport numpy as np\nimport zipfile\nnp.random.seed(2)\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\n# Tools for cross-validation, error calculation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom keras.utils.np_utils import to_categorical\n\n# Machine Learning\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import MaxPooling2D, GlobalMaxPooling2D, Activation\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras import optimizers\n# from keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.models import Model\nfrom keras.applications import VGG16\nfrom keras.applications.resnet50 import ResNet50","54c7a27c":"# Save datasets path\nPATH = \"..\/input\/dogs-vs-cats-redux-kernels-edition\"\nPATH_TRAIN = \"train_images\/\"\nPATH_TEST = \"test_images\/\"","d99854d8":"# Check datasets in file system\nos.listdir(PATH)","a11b52d3":"# Save archives paths and names\ntrain_image_path = os.path.join(PATH, \"train.zip\")\ntest_image_path = os.path.join(PATH, \"test.zip\")","9ef867a4":"# Create subfolder for train dataset\n# Disclaimer. It is need because ImageDataGenerator() which we will use later correctly reads images only from subfolders\nos.mkdir('\/kaggle\/working\/train_images')","9cd43ce2":"# Create subfolder for test dataset\nos.mkdir('\/kaggle\/working\/test_images')","8182b5fe":"# Unzip train dataset\narchive = zipfile.ZipFile(train_image_path,\"r\")\nfor file in archive.namelist():\n        archive.extract(file, 'train_images\/')","d57d829d":"# Unzip test dataset\narchive = zipfile.ZipFile(test_image_path,\"r\")\nfor file in archive.namelist():\n        archive.extract(file, 'test_images\/')","7bbc6af4":"# Check if our kittens and puppies are in right place\n# os.listdir('\/kaggle\/working\/train_images\/train')","263add7d":"# If you prefer work with file system, unzip can be done using bash commands.\n# Just leave this code here, it will give same result - unzipped images in two folders.\n# Unzip train dataset\n# !unzip ..\/input\/dogs-vs-cats-redux-kernels-edition\/train.zip\n# Check unzipped dataset in file system\n#! ls -l train\/\n# Unzip test dataset\n# !unzip ..\/input\/dogs-vs-cats-redux-kernels-edition\/test.zip","4c287bb2":"# Save images names to variable\ntrain_images = os.listdir(f'{PATH_TRAIN}\/train\/')","3f8eaf34":"# Then extract images names and save them into Numpy array\nimagenames = np.array([f'{f}' for f in train_images])","7c87bced":"# Check our image names array\nimagenames","da6de2a7":"# Assign labels to images according to competitions task (0-cat, 1-dog)\n# Implement array of image categories\ncategories = []\nfor imagename in imagenames:\n    # Loop through data and split our images names\n    split_category = imagename.split('.')[0]\n    # Assign labels\n    if split_category == 'cat':\n        categories.append(str(0))\n    else:\n        categories.append(str(1))","453f155d":"# Save our filenames \nanimals = pd.DataFrame({\n    'Image name': imagenames,\n    'Category': categories\n})\nanimals.head(5)","cb0a7ce4":"# Check total amount of 0 and 1 labels\nanimals['Category'].value_counts()","38b6eb28":"# Draw a cat\n# Don't forget to install 'pillow' module (conda install pillow) to give a 'pyplot' ability of working with '.jpg'\nimg = plt.imread(f'{PATH_TRAIN}\/train\/{imagenames[1]}')\nplt.imshow(img);","52cf7b5c":"# Split data on train and validation subsets\n# Using 10% or 20% from train data is classical approach\nX_train, X_val = train_test_split(animals, test_size=0.2,  random_state=2)\nX_train = X_train.reset_index()\nX_val = X_val.reset_index()\n\n# We may want use only 1800 images because of CPU computational reasons. If so, this code should be run\n# X_train = X_train.sample(n=1800).reset_index()\n# X_val = X_val.sample(n=100).reset_index()","227cf783":"# Count\ntotal_X_train = X_train.shape[0]\ntotal_X_val = X_val.shape[0]","7e7b5e42":"total_X_train","2057be74":"total_X_val","57a6e1ae":"# By default, the VGG16 model expects images as input with the size 224 x 224 pixels with 3 channels (e.g. color).\nimage_size = 224\ninput_shape = (image_size, image_size, 3)","c5d5ca80":"# Define CNN model constants\nepochs = 5\nbatch_size = 16","6db43865":"# Define our pre-trained model\npre_trained_model = VGG16(input_shape=input_shape, include_top=False, weights=\"imagenet\")","4b476254":"# Print models summary table\n# Note that it expects input pictures in 224 size and 3 channels, as we mensioned before. So we didn't lie.\nprint(pre_trained_model.summary())","b5031e5c":"# Use this if want to see a Big Bang. Downloads VGG16 with total defaults (Total params: 138,357,544).\n# Very huge.\n# model = VGG16()\n# print(model.summary())","31043e5c":"# Add some micro-tuning \n# Set above layers to be not traianble since using pre-trained model - they are already trained\nfor layer in pre_trained_model.layers[:15]:\n    layer.trainable = False\n\nfor layer in pre_trained_model.layers[15:]:\n    layer.trainable = True\n\n# Specify networks output    \nlast_layer = pre_trained_model.get_layer('block5_pool')\nlast_output = last_layer.output\n    \n# Flatten the output layer to one dimension\nx = GlobalMaxPooling2D()(last_output)\n\n# Add a fully connected layer with 512 hidden units and ReLU activation\nx = Dense(512, activation='relu')(x)\n\n# Add a dropout rate of 0.5\nx = Dropout(0.5)(x)\n\n# Add a final sigmoid layer for classification\nx = layers.Dense(1, activation='sigmoid')(x)\n\n# Form our model\nmodel_mod = Model(pre_trained_model.input, x)","c4b1a492":"# Compile model\nmodel_mod.compile(loss='binary_crossentropy',\n                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n                  metrics=['accuracy']\n                 )","e0cd3200":"# Implement train ImageDataGenerator and specify some small preprocessing\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)","a45fad6d":"# Upload images from file system using flow_from_dataframe() method and use our datagen\n# to make parallel preprocessing. We obtain uploaded and preprocessed images.\ntrain_generator = train_datagen.flow_from_dataframe(\n    X_train, \n    \"\/kaggle\/working\/train_images\/train\",\n    x_col='Image name',\n    y_col='Category',\n    class_mode='binary',\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    #validate_filenames = False\n)","51fb2241":"# Implement validation ImageDataGenerator\nvalidation_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)","e12f6109":"# Upload and peprocess images\nvalidation_generator = validation_datagen.flow_from_dataframe(\n    X_val, \n    \"\/kaggle\/working\/train_images\/train\",\n    x_col='Image name',\n    y_col='Category',\n    class_mode='binary',\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    #validate_filenames = False\n)","6e947efa":"# Check one sample generated image\n# Create generator for test sample image\ngenerated_example_df = X_train.sample(n=1).reset_index(drop=True)\nexample_generator = train_datagen.flow_from_dataframe(\n    generated_example_df, \n    \"\/kaggle\/working\/train_images\/train\", \n    x_col='Image name',\n    y_col='Category',\n    class_mode='categorical',\n    #validate_filenames = False\n)","13723d77":"# Plot sample\nplt.figure(figsize=(10, 10))\nfor i in range(0, 9):\n    plt.subplot(3, 3, i+1)\n    for X_batch, Y_batch in example_generator:\n        image = X_batch[0]\n        plt.imshow(image)\n        break\nplt.tight_layout()\nplt.show()","496d4c4e":"earlystop = EarlyStopping(patience=10, \n                          verbose=1, \n                          mode='auto'\n                         )","14d9a0ed":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001\n                                           )","a04b1f78":"# Save our callbacks\ncallbacks = [earlystop, learning_rate_reduction]","377d4c37":"callbacks","3754eeb0":"# Just leave it here\n# def fixed_generator(generator):\n#     for batch in generator:\n#         yield (batch, batch)","4c20b035":"# Fit the model\nhistory = model_mod.fit_generator(\n    train_generator, \n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=total_X_val\/\/batch_size,\n    steps_per_epoch=total_X_train\/\/batch_size,\n    callbacks=callbacks\n)","7572745e":"# Epoch 1\/5\n# 1250\/1250 [==============================] - 404s 323ms\/step - loss: 0.3144 - accuracy: 0.8517 - val_loss: 0.1308 - val_accuracy: 0.9451 - lr: 1.0000e-04\n# Epoch 2\/5\n# 1250\/1250 [==============================] - 385s 308ms\/step - loss: 0.1555 - accuracy: 0.9362 - val_loss: 0.1066 - val_accuracy: 0.9527 - lr: 1.0000e-04\n# Epoch 3\/5\n# 1250\/1250 [==============================] - 382s 305ms\/step - loss: 0.1305 - accuracy: 0.9467 - val_loss: 0.1012 - val_accuracy: 0.9599 - lr: 1.0000e-04\n# Epoch 4\/5\n# 1250\/1250 [==============================] - 381s 305ms\/step - loss: 0.1125 - accuracy: 0.9553 - val_loss: 0.0866 - val_accuracy: 0.9619 - lr: 1.0000e-04\n# Epoch 5\/5\n# 1250\/1250 [==============================] - 381s 305ms\/step - loss: 0.1051 - accuracy: 0.9577 - val_loss: 0.0886 - val_accuracy: 0.9649 - lr: 1.0000e-04","b48a4999":"# Save calculated weigthts (approx. 60 Mb)\nmodel_mod.save_weights('model_wieghts.h5')\nmodel_mod.save('model_keras.h5')","98c3a2ad":"# Plot accuracy and loss curves\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 7))\n\nax1.plot(history.history['loss'], color='r', label=\"Train loss\")\nax1.plot(history.history['val_loss'], color='b', label=\"Validation loss\")\nax1.set_xticks(np.arange(1, epochs, 1))\nlegend = ax1.legend(loc='best', shadow=True)\n\nax2.plot(history.history['accuracy'], color='r', label=\"Train accuracy\")\nax2.plot(history.history['val_accuracy'], color='b',label=\"Validation accuracy\")\nax2.set_xticks(np.arange(1, epochs, 1))\nlegend = ax2.legend(loc='best', shadow=True)\n\nplt.tight_layout()\nplt.show()","4814443e":"# Prepare Y_val\nY_val = X_val['Category']\n# Predict on validation data\nY_pred =  model_mod.predict_generator(validation_generator)","0dea9ce2":"# Define treshold\nthreshold = 0.5\n# Convert\nY_pred_conv = np.where(Y_pred > threshold, 1,0)","64386001":"Y_pred_conv[:,0]","a28c07a4":"# Plot probability histogram\npd.Series(Y_pred_conv[:,0]).hist()","f373e519":"# Check datatypes\nY_pred_conv.dtype","1d3358b2":"Y_val.dtype","c54a8cc4":"# Convert to int\nY_val_str = Y_val.astype(int)","8d653dfe":"# Compute and plot the Confusion matrix\nconfusion_mtx = confusion_matrix(Y_val_str, Y_pred_conv) \n\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Validation (aka True) Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","2a2a1905":"test_images = os.listdir('\/kaggle\/working\/test_images\/test')\nX_test = pd.DataFrame({\n    'test_imagename': test_images\n})\nsamples = X_test.shape[0]","c42ee68c":"X_test.count()","892bdbc1":"# If we use only 1800 images because of CPU computational reasons:\n# X_test_cpu = X_test.sample(n=1800).reset_index()\n# X_test_cpu.count()","d8787e50":"test_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)","27e9e2bb":"test_generator = test_datagen.flow_from_dataframe(\n    X_test, \n    \"\/kaggle\/working\/test_images\/test\", \n    x_col='test_imagename',\n    y_col=None,\n    class_mode=None,\n    batch_size=batch_size,\n    target_size=(image_size, image_size),\n    shuffle=False\n)","9b6844bb":"test_generator","89a73517":"predict = model_mod.predict_generator(test_generator, steps=np.ceil(samples\/batch_size))","8a6f97d1":"predict.shape","cfc69300":"X_test.shape","d284cbf0":"threshold = 0.5\nX_test['Category'] = np.where(predict > threshold, 1,0)","d343e0d3":"# Save results using competitions format\nsubmit = X_test.copy()\nsubmit['id'] = submit['test_imagename'].str.split('.').str[0]\nsubmit['label'] = submit['Category']\nsubmit.drop(['test_imagename', 'Category'], axis=1, inplace=True)\nsubmit.to_csv('submission_vgg16.csv', index=False)","da4c5d47":"# Check how our answer looks\nplt.figure(figsize=(10,5))\nsns.countplot(submit['label'])\nplt.title(\"(Final answer on test data (Model - VGG16))\")","f5331782":"# Define model constants\n# image_size = 224 like in previous model\nnum_classes = 2\nnum_epochs = 5\nnum_batch_size = 64\nWEIGHTS_PATH = \"..\/input\/resnet-weights-uploaded\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"","4d07f9d5":"# Define model\nmodel_resnet = Sequential()\n# Add pre-trained weights\nmodel_resnet.add(ResNet50(include_top=False, pooling='max', weights='imagenet'))\n# The last dense layer must specify the number of labels (or classes) and activation f-n\nmodel_resnet.add(Dense(num_classes, activation='softmax'))\n# Since we load pre-trained model we must specify first layer as non-trainable\nmodel_resnet.layers[0].trainable = True","24672acc":"# Compile model\nmodel_resnet.compile(\n    optimizer='sgd', \n    loss='categorical_crossentropy', \n    metrics=['accuracy']\n)","678254fb":"# Print models summary\nmodel_resnet.summary()","29dace53":"# Before we begin lets save our train and validation subsets into threir copies\nX_train_resnet = X_train\nX_val_resnet = X_val\nY_val_resnet = Y_val\nY_train_resnet = animals['Category']","70ecdb7d":"total_X_train_resnet = X_train_resnet.shape[0]\ntotal_X_val_resnet = X_val_resnet.shape[0]","4e6e00b8":"X_train_resnet.head()","f88fd164":"total_X_train_resnet","516e073f":"X_train_resnet.shape","8f0a0d90":"X_val_resnet.shape","786d3ad8":"train_resnet_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n)","68ec7e8b":"train_resnet_generator = train_resnet_datagen.flow_from_dataframe(\n    X_train_resnet,\n    \"\/kaggle\/working\/train_images\/train\",\n    x_col='Image name',\n    y_col='Category',\n    class_mode='categorical',\n    target_size=(image_size, image_size),\n    batch_size=num_batch_size\n)","936f5be9":"validation_resnet_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)","f2e7096f":"validation_resnet_generator = validation_resnet_datagen.flow_from_dataframe(\n    X_val_resnet, \n    \"\/kaggle\/working\/train_images\/train\", \n    x_col='Image name',\n    y_col='Category',\n    class_mode='categorical',\n    target_size=(image_size, image_size),\n    batch_size=num_batch_size\n)","964b0d69":"# Fit the model\nhistory_resnet = model_resnet.fit_generator(\n        train_resnet_generator,\n        epochs = num_epochs,\n        validation_data = validation_resnet_generator,\n        validation_steps = total_X_val_resnet\/\/num_batch_size,\n        steps_per_epoch = total_X_train_resnet\/\/num_batch_size,\n        callbacks = callbacks\n)","bc619c11":"# Leave this output here to outline that we obtain low accuracy when train on 5 epochs and\n# first layer trainable = False param\n# Epoch 1\/5\n# 312\/312 [==============================] - 190s 611ms\/step - loss: 5.4424 - accuracy: 0.5646 - val_loss: 3.1316 - val_accuracy: 0.6012 - lr: 0.0050\n# Epoch 2\/5\n# 312\/312 [==============================] - 195s 624ms\/step - loss: 5.1490 - accuracy: 0.5693 - val_loss: 5.3533 - val_accuracy: 0.5655 - lr: 0.0050\n# Epoch 3\/5\n# 312\/312 [==============================] - ETA: 0s - loss: 5.5350 - accuracy: 0.5592\n# Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n# 312\/312 [==============================] - 194s 622ms\/step - loss: 5.5350 - accuracy: 0.5592 - val_loss: 5.5042 - val_accuracy: 0.5615 - lr: 0.0050\n# Epoch 4\/5\n# 312\/312 [==============================] - 193s 617ms\/step - loss: 1.8847 - accuracy: 0.6557 - val_loss: 1.7388 - val_accuracy: 0.6354 - lr: 0.0025\n# Epoch 5\/5\n# 312\/312 [==============================] - 192s 615ms\/step - loss: 2.0500 - accuracy: 0.6221 - val_loss: 5.0342 - val_accuracy: 0.5294 - lr: 0.0025","af4bf4bf":"# Save calculated weigthts\nmodel_resnet.save_weights('model_resnet_wieghts.h5')\nmodel_resnet.save('model_resnet_keras.h5')","aba93db2":"# Plot accuracy and loss curves\nfig, (ax3, ax4) = plt.subplots(2, 1, figsize=(7, 7))\nhistory_resnet = history\n\nax3.plot(history.history['loss'], color='r', label=\"Train loss\")\nax3.plot(history.history['val_loss'], color='b', label=\"Validation loss\")\nax3.set_xticks(np.arange(1, epochs, 1))\nlegend = ax3.legend(loc='best', shadow=True)\n\nax4.plot(history.history['accuracy'], color='r', label=\"Train accuracy\")\nax4.plot(history.history['val_accuracy'], color='b',label=\"Validation accuracy\")\nax4.set_xticks(np.arange(1, epochs, 1))\nlegend = ax4.legend(loc='best', shadow=True)\n\nplt.tight_layout()\nplt.show()","8129514f":"# Predict on validation X_val_resnet\nY_pred_resnet =  model_resnet.predict_generator(validation_resnet_generator)","3ddf13ea":"# Define treshold\nthreshold = 0.5\n# Convert\nY_pred_conv_res = np.where(Y_pred_resnet > threshold, 1,0)","d4307485":"Y_pred_conv_res[:,0]","10d2fd7a":"# Plot probability histogram\npd.Series(Y_pred_conv_res[:,0]).hist()","8ac092f1":"# Prepare Y_val in str dtype\n# Convert to int\nY_val_resnet_str = Y_val_resnet.astype(int)","f0edf88e":"# Compute and plot the Confusion matrix\nconfusion_mtx_resnet = confusion_matrix(Y_val_resnet_str, Y_pred_conv_res[:,0]) \n\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx_resnet, annot=True)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Validation (aka True) Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","ab9f8e0d":"# Add X_test_resnet\nX_test_resnet = X_test","c3c3112b":"test_resnet_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)","0fb9b9d2":"test_resnet_generator = test_resnet_datagen.flow_from_dataframe(\n    X_test_resnet, \n    \"\/kaggle\/working\/test_images\/test\", \n    x_col='test_imagename',\n    y_col=None,\n    class_mode=None,\n    batch_size=num_batch_size,\n    target_size=(image_size, image_size),\n    shuffle=False\n)","f274e375":"predict_resnet = model_resnet.predict_generator(test_resnet_generator, steps=np.ceil(samples\/num_batch_size))","4cea3002":"# Save predictions\nsubmit_resnet = X_test_resnet.copy()\nsubmit_resnet['id'] = submit_resnet['test_imagename'].str.split('.').str[0]\nsubmit_resnet['label'] = submit_resnet['Category']\nsubmit_resnet.drop(['test_imagename', 'Category'], axis=1, inplace=True)\nsubmit_resnet.to_csv('submission_resnet_50.csv', index=False)","370969e5":"# Check answer\nplt.figure(figsize=(10,5))\nsns.countplot(submit_resnet['label'])\nplt.title(\"(Final answer on test data (Model - ResNet))\")","518faee9":"#### Learning Rate Reduction #### \nLets reduce the learning rate when accuracy, for example, not increase for two continuous steps. Use well-known ReduceLROnPlateau() method, whoes library was imported at the first stage. We can use its default parameters.","20795e24":"#### Early Stop ####\nTo prevent overfitting lets stop model learning after 10 epochs when val_loss value not decreased. Lets use EarlyStopping() method, since it is simple and good working.","4efeba50":"### Train model ###","36c6bf9e":"Isn't it pretty? Our efforts were not in vain. Move on.","af5d5eee":"### Callbacks ###","90f31619":"# Data load #","57299076":"Before we start teaching our model we should care about avoiding of model overfitting. In general, such techniques are known as **Callbacks**. There are two useful ones: **early stop** and **learning rate reduction**.  \nIf we cut from our 12500 dataset a small 1800 part, we probably will not fall in overfitting,\nbut for general purpose lets take care about it.  \n**Nota Bene**  \nIn Kaggle enviroment we should import EarlyStop and ReduceLROnPlateau from tensorflow.keras.callbacks, not from keras.callbacks, since the last doesn't accepted in model.fit(). What shold we do without StackOverflow?","5904722d":"VGG16 [2] CNN architectire proposed in 2014 by K. Simonyan and A. Zisserman (University of Oxford) in their paper \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d [5]. The model achieves 92.7% top-5 test accuracy in ImageNet - a dataset of over 14 million images belonging to 1000 classes.\nIts principal scheme [2] is shown below:\n\n![image.png](attachment:image.png)\n\nAs we can see, this network consists of 16 layers, which are:   \n- 13 stacked Convolutional layers;\n- 3 Dense layers.\n\nConvolution layers extract features from the image, fully connected layers classify the image using extracted features.  \nThe number of filters in the convolution layers follow an increasing pattern.  \nThe informative features are obtained by Maxpooling layers applied at different steps in the architecture.\nThe dense layers comprises of 4096, 4096, and 1000 nodes each.  \nModels full configuration table may be found in [2].\n\nThe choice of this model is made due to its provenness and ease of implementation. But, it should be noted that this model, as we have already said, consumes a lot of resources, in other words, it works very slowly. However, the Keras library has a **pre-trained** model that will allow us to save our time and resources. And, as we know, time is money.\n\nLets proceed with importing and implementation of our pre-trained model.","94243a60":"### Visualize accuracy and loss curves ###","1eeb007d":"We begin with some small preparations. We should extract our images filemames and labels. They will be used in model training.  \n**Note**. For now we will not work with test data in order to prevent mistakenly using it instead of validation.\nAs we know, test data must be shown to algorhytm only once - at final prediction. So, we will work on test subset at pre-final - prediction step.","a08097ca":"# Conclusion #\nEvery great road has great finish. We completed our task of teaching machine how to recognise cats and dogs on pretty photos. We did it twice: using VGG16 and ResNet-50 CNN models. This was in learning purpose. And to be continuous we choosed absolutely different models: one deep and huge in computing and second more complex in structure but less resource-taking.  \nSo what we obtain?  \n1. Model implementation process (as well as data preparation) is very close in both cases. (Remember, we used Transfer learning paradigm - pre-trained models, so we just connected models implementation from Keras library, downloaded pre-trained weights, and specified models few layers, in particular, outputs).\n2. Models gave approximately a similar level of accuracy (ResNet-50 gave better result).  \n3. CPU and memory usage is very sensitive on old-school PC's like ours (with Core i5 from 2010-s and 4 Gb RAM). Our goal was not trying to benchmark since our models are very famous and probably already have many benchmarks. Therefore, the goal was only to become familiar with the capabilities and implementation of models. And what we can denote? Taking into account paragraphs one and two we can conclude that in all else being equal case it is adecuately to use more quick and modern net at least in order to save our time for new discoveries. Its complicity is not big problem if we use transfer learning concept.","ec316b17":"In order to compare predicted and validation data we need to convert predictions (which are probabiliies of classifying an animal as dog or cat, i.e 0,05% prob) into actual class predictions (which are 0 and 1).  \nTo do it we should define a threshold. Let it be default 0.5.","d16733e4":"In the problems of image recognition, when we get to the part of machine learning itself, question No. 1 arises.  \n\n**Which neural network to choose?**  \nWell, we know from the literature that convolutional network architecture is suitable for this task. Already good. But which architecture to choose, because there are dozens of them? Or may be we need to build our own - of course, empirically?  \nTruth be sayed, if we study the comparison of models in terms of accuracy and consumed computing power, for example [3], then it becomes clear that the vast majority of architects at one time won various competitions for image recognition. Almost any of them will suit us.\n\nWe proceed as follows.  \nBased on a comparison of networks [3], lets choose two architectures: **VGG16** and **Resnet-50**.  \nWhy them?    \nBoth are famous enough, both have approximately the same accuracy. But they need different computing power: VGG16 requires significantly more.   \nWe will choose them for comparison.\n\nWe will use Transfer Learning (TL).  \nIt is a paradigm in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.\nThe essence of the TL is that when we build a network, we import the weights of the same network trained on a very large dataset. thus, we save time and resources.","0f8c2363":" ### Predict on validation data ###","ef285f41":"### Validation DataGenerator ###","eb6ff138":"### Implementation ###","b2374528":"**Nota Bene**  \nLet us leave a few comments right here at the top of paragraph just before we start our model fit.  \nIf we obtain error like above when call fit_generator() method:\n\"ValueError: Error when checking target: expected block5_pool to have 4 dimensions, but got array with shape (16, 1)\"\nwe can change class_mode in generators implemented before from 'binary' to None.  \n\nAccording to the Keras docs, setting the class_mode to 'binary' will return a 1-D array.  \nBy changing the class_mode to None, no labels are returned and the generator will only yield batches of image data. This is what you are expecting with your model, and this works well with the fit_generator() method.  \nHowever, when using class_mode None, the data still needs to reside in a subdirectory of directory for it to work correctly.  \n\nIf we do make such change, we also should make second thing - define a fixed_generator method (code is in comments below). It fixes known generator issue.  \nBut actually we can avoid this steps if we little tune VGG16 models outputs, as we did before.\n\nGod bless Stackoverflow.","a94d9b51":"For model building clarity, lets repeat the process with image generation instead of using from previous step. It is quick. Omit comments, since actions are identical to the previous paragraph.","5cc415f5":"# Data exploration #","4faf1a0f":"### Model fit ###","df0807ae":"Keras has very useful tools for images preprocessing (i.e. rotation, flip, crop, resize etc). Lets use one of them - **ImageDataGenerator** class and perform some actions which are actually almost standard.","3d41915c":"### Test DataGenerator (for ResNet) ###","e96a52b4":"### Training DataGenerator ###","02b55f8f":"[1] https:\/\/towardsdatascience.com\/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d7  \n[2] https:\/\/neurohive.io\/en\/popular-networks\/vgg16\/  \n[3] https:\/\/medium.com\/analytics-vidhya\/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5  \n[4] https:\/\/gist.github.com\/baraldilorenzo\/07d7802847aaad0a35d3  \n[5] https:\/\/arxiv.org\/abs\/1409.1556  \n[6] https:\/\/towardsdatascience.com\/keras-data-generators-and-how-to-use-them-b69129ed779c  \n[7] https:\/\/keras.io\/preprocessing\/image\/  \n[8] https:\/\/neurohive.io\/en\/popular-networks\/resnet\/  \n[9] https:\/\/arxiv.org\/abs\/1512.03385  \n[10] https:\/\/towardsdatascience.com\/understanding-and-coding-a-resnet-in-keras-446d7ff84d33  \n[11] https:\/\/www.kaggle.com\/keras\/resnet50\/  \n[12] https:\/\/machinelearningmastery.com\/difference-between-a-batch-and-an-epoch\/","a76912a1":"## Model 2. ResNet-50 ##","cf3d2781":"As we see, our tasks initial 12500 images dataset is balanced: both 12500 dogs and cats. Good.  \nOk. Now we can take a look at one little cute kitten.","f67f556f":"### Predict on test data ###","885e1b02":"### Train DataGenereator (for ResNet) ###","c3bd15db":"### Load and unzip data ###","9ff85023":"### Predict on validation data ###","23874395":"### Upload test data ###","19b64637":"**Nota Bene**  \nPay attention on weights path. It is manually downloaded from ResNet's Keras Dataset [11] and then manually uploaded into \/input directory. First we tryed the most logical way - to attach weights from [11] butdoing so will not work in model definition because of defect mensioned here: https:\/\/www.kaggle.com\/learn-forum\/124381 and https:\/\/stackoverflow.com\/questions\/60119041\/failed-to-load-keras-resnet50-model-offline-using-weight-file. So there was nothing better for us than to save the files manually, taking into account the fact that they should be saved forever, since we will need many times (download every time from Imagenet external source seems traffic-wasting unefficiant way). Don't know why, but it worked for one time, so we will skip it and use weights from Imagenet like we did in first model.","a78d9b63":"### Save data to NumPy and Pandas arrays ###","9f9f440c":"A few notes about fit_generator() parameters. We pass into it our train DataGenerator, number of epochs, validation DataGenerator and two interesting ones: validation_steps and steps_per_epoch. How to choose them?  \nThere is a traditional approach. Traditionally, the steps per epoch is calculated as train_length \/\/ batch_size and validation steps follow same logic for validation data.  \n\nRemember, that an epoch means one iteration over all of the training data. Thus, if we have 10,000 images and a batch size of 100 then the epoch should contain 10,000 \/ 100 = 100 steps.  \nA training step is one gradient update. In one step batch_size many examples are processed.  \n\nA good article about difference between batch, epoch and step can be found in [12].\n","1e7b6a14":"### Test DataGenerator ###","7e3ac514":"ResNet (aka Residual Networks) is a classic neural network used for many computer vision real-world tasks. Was implemented by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun [9]. This model won ImageNet challenge in 2015. Scheme of net [10]: ![image.png](attachment:image.png)","f8dc5c75":"Choosing models hyperparameters (as well as their tuning) is almost empyrical process. In addition, it should be noted that it is a kind of \"cherry on the cake\" ie performed at the very end of the construction, validation and use of the working model. Therefore, we will for now set the above parameters based on the fact that their combination is known and has proven itself well. After building the model and testing it, we can easily return to this point and tune them if necessary.","a178e68c":"We will again use pre-trained network, staying at transfer learning paradigm.\nOur steps. Initialize the ResNet-50 model, add an additional last Dense-layer, with softmax activation function and set the first layer of the model to be not trainable, since our model is pre-trained.  \n\nLets denote a few things about model constants.  \nImage size is equal 224 as it was in VGG16 model, number of epochs due to computtional reasons let equal to 5; number of classes is equal to 2, it is self-descriptive; let batch size here be equal to 64.\n\nDownload pre-trained weights from Resnet-50 officical datasource on Kaggle [11]. Notice that there are two pre-trained weights files:  \na) ..._tf_kernels.h5;  \nb) ..._tf_kernels_NOTOP.h5.  \n\nWhat are they?   \nThe _tf_kernels.h5 weights is useful for 'pure' prediction of test image and this prediction will rely completely on ResNet-50 pre-trained weights, it does not expected any training from our side.  \n\nSince we use a Transfer Learning paradigm, we should use models pre-trained weights except its TOP layer, so we will use second (aka notop) weights as initial weight for training new layer using train images.","880462f0":"### Predict on test data ###","185e5f46":"### Save submission results ###","af26b15b":"In order to compute a Confusion matrix we should have our predicted and validation sets in same datatype.","1d0a3905":"### Implementation ###","a8156e53":"A good VGG16 implementtaion (including pre-trained weights) is present in [4]. We can refer to it for example, or just import model from Keras libs (imported in notebooks libraries section).","451c2d3e":"### Visualize prediction on validation data ###","70ace548":"# Literature #","d69a3ffd":"### Validation DataGenereator (for ResNet) ###","a6ae244a":"### Split data on train and validation subsets ###","ad1db533":"### Description ###","4ce22323":"### Save submission results (Model - ResNet) ###","5cefc3cc":"Count amount of rows. Need for two reasons.  \nFirst: will be used in model fit process as parameter devided by batch size.  \nSecond: if we cut 1800-images subset for comp. reasons we should check if there are exact 1800\nimages present.","efaf047b":"### Visualize model accuracy and loss ###","081e5cc8":"# Machine learning #","71825849":"First, lets outline our goal. \"Our goal is to correctly distinguish dogs from cats.\"","24b5f454":"# Import libraries and tools #","3f069509":"Since we created notebook from \"Dogs vs Cats\" page, data will load automatically and may be found in \/input directory after session starts.","fefc2a6a":"### Description ###","50ed9a83":"## Model 1. VGG16 ##","4095c613":"# Intro #\nRefer to the famous Dogs and Cats classification problem. Goal is simple: we need to teach machine distinguish dogs from cats. In this notebook we will try to address it by using neural networks.\n\n**General plan**:  \n1.Explore dataset, make some preprocessing if need.   \n2.Choose two different neural network types, implement and train it on validation and train data.  \n3.Make predictions on test data.  \n4.Outline conclusions.\n\n**Why neural networks?**  \nThey have proven themselves in practice of real-word problem solving and competitions, in particular in\ncomputer vision problem. Thus, \u0421onvolution Neural Networks (CNN) perfectly proved themselves especially in matters of computer vision. Many CNN architectures were specially designed for that purpose. An overview of CNN concepts, their architectures and implementations is beyond the scope of this article. We will choose two of them, give short overview and describe in detail their implementation and training. The rationale for their choice is given in the Machining Learning section.\n\n**Some comments**  \nWe have reviewed many laptops on this topic, the authors of which have done a great job, for which many thanks can be given to them. But unfortunately, many authors paid little attention to the comments of their actions, and also did not highlight such subtleties as loading datasets, unzipping them, connecting external weights. In the process of this work, we were faced with many nuances and glitches in these \"auxiliary\", but inevitable issues. Therefore, we tried to comment on almost every action, in particular, on \"auxiliary\" issues. So some comments with direct links to Stack Overflow may be found in text.","7b52978f":"### Visualize prediction on validation data ###"}}