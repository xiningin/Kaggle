{"cell_type":{"2b4dcc83":"code","8bc1f00d":"code","7737e2d3":"code","879c323a":"code","fcd4a1ce":"code","0594543f":"code","9c36a708":"code","81be3916":"code","6194cf22":"code","23d4024a":"code","15c53119":"code","2c425aa6":"code","9f64b094":"code","b782cfb8":"code","3ee144cb":"code","6a43e4a3":"code","eb722032":"code","613e3adc":"code","ef1113f2":"code","afc2ad6a":"code","5416caa1":"code","91bdf795":"code","0dd5e698":"code","d254dd6a":"code","0bd9d226":"code","2c68548d":"code","3b3e6919":"code","088d85be":"code","437eb1c3":"code","2b49d7d1":"code","5dc585b8":"markdown","c08cec8f":"markdown","95079501":"markdown","c4f0630f":"markdown","16e85601":"markdown","6f981a21":"markdown","a090e462":"markdown","f1be971f":"markdown","79614db8":"markdown","9bedd514":"markdown","587d5d02":"markdown","b07619e5":"markdown","19ac852a":"markdown","1fdadafd":"markdown","bfa6ed98":"markdown","16f14a59":"markdown","864b346b":"markdown","4cb950ba":"markdown","788178e8":"markdown","09e2eb5f":"markdown","99cafadb":"markdown","6a910c5b":"markdown","46d100d0":"markdown","1f93a881":"markdown","f83a387d":"markdown"},"source":{"2b4dcc83":"# Basic module\nimport os, glob, cv2, re\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\n# To choose random sample in Brain Sequence data\nimport tensorflow as tf \nfrom random import seed as random_seed\nrandom_seed(50)\n\nfrom numpy.random import seed as np_random_seed\nnp_random_seed(50)\ntf.random.set_seed(50)\n\n# Dicom image preprocess\nimport pydicom as dicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\n# Deep learning\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.preprocessing import image","8bc1f00d":"'''\nMake Data Frame for BraTS Train labels \nand sample submission labels data\n\nchange directory for your file directory!\n'''\n\ndata_directory =  '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/'\n\ntrain_df = pd.read_csv(data_directory+\"train_labels.csv\")\ntrain_df['BraTS21ID5'] = [format(x, '05d') for x in train_df.BraTS21ID] # -> ZFILL 5\ntrain_df","7737e2d3":"test = pd.read_csv(\n    data_directory+'sample_submission.csv')\n\ntest['BraTS21ID5'] = [format(x, '05d') for x in test.BraTS21ID]\ntest.head(3)","879c323a":"IMAGE_SIZE = 299 # Xception default input size \nSCALE = .8\nNUM_IMAGES = 64\nMRI_TYPE = \"FLAIR\"\n# MRI_TYPE = [\"FLAIR\",\"T1w\",\"T1Gd\",\"T2\"]\n\n''' \nMRI_TYPE(multi-parametric MRI (mpMRI)) scans has four category.\nwe train for FLAIR in MRI_TYPE.\n- Fluid Attenuated Inversion Recovery (FLAIR)\n- T1-weighted pre-contrast (T1w)\n- T1-weighted post-contrast (T1Gd)\n- T2-weighted (T2)\n'''","fcd4a1ce":"# Preprocess Image Fucntion\n\n'''\n****************************************************\nPARAMETERS\n****************************************************\n  - path : String\n      Path to the DCIM image file to load.\n  - img_size : Integer\n      Image size desired for resizing.\n  - scale : Float\n      Desired scale for the cropped image\n  - prep : Bool\n      True for a full preprocessing with denoising.\n'''\n\ndef load_dicom_image(\n    path,\n    img_size = IMAGE_SIZE,\n    scale = SCALE,\n    rotate=0):\n\n    # Load single image\n    img = dicom.read_file(path).pixel_array\n    \n    # Crop image\n    center_x, center_y = img.shape[1] \/ 2, img.shape[0] \/ 2\n    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale # center \ud655\uc778\n    left_x, right_x = center_x - width_scaled \/ 2, center_x + width_scaled \/ 2\n    top_y, bottom_y = center_y - height_scaled \/ 2, center_y + height_scaled \/ 2 # scale \uc801\uc6a9\ubc94\uc704 \ud655\uc778\n    img = img[int(top_y):int(bottom_y), int(left_x):int(right_x)] # \ubc94\uc704\uc5d0 \ub9de\ucdb0 crop\n    \n    # rotate image\n    if rotate>0:\n        rot_choices=[0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n        img-cv2.rotate(img, rot_choices[rotate])\n    \n    # Resize image\n    img = cv2.resize(img, (img_size, img_size), interpolation=cv2.INTER_AREA) ## interpolation \uc9c0\uc815\uc774 \uc548\ub3fc\uc788\uc74c\n    \n    # img normaliztaion\n    img = img.astype(np.float32)\n    img = img\/img.max() * 255\n    \n    # Convert in 3D array\n    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    img=np.nan_to_num(img)\n\n#     print(img.dtype) # float32\n#     print(img.max(),\"||\",img.min()) #  255.0 || 0.0\n    \n    return preprocess_input(img)  ","0594543f":"# Test sample image loading & plot\n\nsample_img = dicom.read_file(data_directory+\"train\/00052\/FLAIR\/Image-50.dcm\").pixel_array\npreproc_img = load_dicom_image(data_directory+\"train\/00052\/FLAIR\/Image-50.dcm\")\n\n\nfig = plt.figure(figsize=(12,8))\nax1 = plt.subplot(1,2,1)\nax1.imshow(sample_img, cmap=\"gray\")\nax1.set_title(f\"Original image shape = {sample_img.shape}\")\nax2 = plt.subplot(1,2,2)\nax2.imshow(preproc_img[:,:], cmap=\"gray\")\nax2.set_title(f\"Preproc image shape = {preproc_img.shape}\")\nplt.show()","9c36a708":"def load_dicom_images_3d(\n    scan_id, \n    num_imgs=NUM_IMAGES, \n    img_size=IMAGE_SIZE, \n    mri_type=MRI_TYPE, \n    split=\"train\",\n    rotate=0):\n    '''\n    This function allows loading an ordered sequence \n    of x preprocessed images starting from the central \n    image of each folder.\n    ****************************************************\n    PARAMETERS\n    ****************************************************\n    - scan_id : String\n        ID of the patient to load.\n    - num_imgs : Integer\n        Number of desired images of the \n        sequence.\n    - img_size : Integer\n        Image size desired for resizing.\n    - scale : Float\n        Desired scale for the cropped image\n    - mri_type : String\n        Type of scan to load (FLAIR, T1w, \n        T1wCE, T2).\n    - split : String\n        Type of split desired : Train or Test\n    '''\n    files = sorted(glob.glob(f\"{data_directory}{split}\/{scan_id}\/{mri_type}\/*.dcm\"), \n               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n\n    middle = len(files)\/\/2\n    num_imgs2 = num_imgs\/\/2 # 64\/\/2 =32\n    p1 = max(0, middle - num_imgs2)  \n    p2 = min(len(files), middle + num_imgs2)\n    img3d = np.stack([load_dicom_image(f) for f in files[p1:p2]]) \n    \n    if img3d.shape[-1] < num_imgs:\n        n_zero = np.zeros((num_imgs \n\n- img3d.shape[0], img_size, img_size, 3))\n        img3d = np.concatenate((img3d,  n_zero), axis = 0)\n            \n    return np.expand_dims(img3d,0)","81be3916":"# Test sample image loading & plot\n\nsample_seq = load_dicom_images_3d(\"00046\")\nprint(\"Shape of the sequence is:\", sample_seq.shape)\nprint(\"Dimension of the 15th image in sequence is:\", sample_seq[0,15].shape)\nfig = plt.figure(figsize=(5,5))\nplt.imshow(np.squeeze(sample_seq[0][15][:,:,0]), cmap=\"gray\")\nplt.show()","6194cf22":"train = train_df[['BraTS21ID5','MGMT_value']]\nX_train = train['BraTS21ID5'].values\ny_train = train['MGMT_value'].values","23d4024a":"base_resnet = Xception(    \n    weights=None,\n    pooling='avg',\n    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n    include_top=False)\n\nbase_resnet.trainable = False","15c53119":"'''\nFeature Extract\nwe use convnet model(InceptionResNetV2) for Feature Extraction\n'''\n\nlistMatrix = []\nfor patient in tqdm(X_train):\n    listVectors = []\n    sequence = load_dicom_images_3d(scan_id=str(patient),mri_type=MRI_TYPE)\n    for j in range(sequence.shape[1]):\n        img = sequence[:,j,...]\n#         img = np.expand_dims(img, axis=0)\n#         img = tf.keras.applications.resnet50.preprocess_input(img)\n        img_vector = base_resnet.predict(img)\n        listVectors.append(np.array(img_vector))\n    \n    PatientMatrix = np.stack(listVectors)\n    listMatrix.append(PatientMatrix)","2c425aa6":"listMatrix = np.array(listMatrix)","9f64b094":"# np.save('.\/listMatrix',listMatrix)\n# listMatrix=np.load('.\/listMatrix.npy')","b782cfb8":"print(f\"Number of Patient matrix: {len(listMatrix)}\")\nprint(f\"Patient matrix shape: {listMatrix[0].shape}\")","3ee144cb":"model_input_dim = listMatrix[0].shape[2]\nmodel_input_dim=2048","6a43e4a3":"# Create a function for lstm model\ndef get_sequence_model():\n\n    '''Define the LSTM architecture'''\n\n    model = keras.models.Sequential()\n    model.add(keras.layers.LSTM(512, input_shape=(NUM_IMAGES, model_input_dim), return_sequences=True))\n    model.add(keras.layers.Dropout(0.4))\n    model.add(keras.layers.Dense(64, activation='relu'))\n    model.add(keras.layers.Dropout(0.4))\n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    return model","eb722032":"model = get_sequence_model()\nmodel.summary()","613e3adc":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import optimizers\n\nversion=\"xception_6\"\noutput_path=os.path.join(\".\/result\", version)\n\ninputs2 = np.array(listMatrix)\n# inputs2 = (inputs2-inputs2.min())\/(inputs2.max()-inputs2.min())\ninputs=inputs2[:,:,0,:]\ntargets = np.array(y_train).astype('float32').reshape((-1,1))\n\nnum_folds = 5\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nhistory = {}\nfold_no = 1\nfor train_idx, valid_idx in kfold.split(inputs, targets):\n    \n    print(\"train: \",len(train_idx), \"val: \", len(valid_idx))\n    \n    train_dataset = tf.data.Dataset.from_tensor_slices((inputs[train_idx], targets[train_idx]))\n    valid_dataset = tf.data.Dataset.from_tensor_slices((inputs[valid_idx], targets[valid_idx]))\n    \n    train_dataset = train_dataset.cache().shuffle(1000, reshuffle_each_iteration=True).batch(32, drop_remainder=True)\n    valid_dataset = valid_dataset.cache().shuffle(1000, reshuffle_each_iteration=True).batch(32, drop_remainder=True)\n    \n    \n    model = get_sequence_model()\n    model.compile(loss='mse', \n                  optimizer=optimizers.RMSprop(lr=1e-3), \n                  metrics=['accuracy'])\n    \n    output_path_weight=os.path.join(output_path, str(fold_no)+\"_fold\")\n    if os.path.isdir(output_path_weight)==False:\n        os.makedirs(output_path_weight)\n        \n    # Define callbacks.\n    checkpoint = ModelCheckpoint(os.path.join(output_path_weight, 'weights_{epoch:08d}_{val_loss:.4g}.hdf5'), save_weights_only=True)\n    csv_logger = CSVLogger(os.path.join(output_path_weight, r'log.csv'), append=True)\n#     early_stop = EarlyStopping(monitor = 'val_accuracy', \n#                                patience = 25, mode = 'max', verbose = 1,\n#                                restore_best_weights = True)\n    \n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n    \n    epochs = 100\n    history[fold_no] = model.fit(\n        train_dataset, \n        validation_data=valid_dataset,\n        epochs=epochs, \n        verbose=1,\n        callbacks = [checkpoint, csv_logger])\n    \n    # Increase fold number\n    fold_no += 1","ef1113f2":"# Plot result for training\n\nfig, ax = plt.subplots(5, 2, figsize=(20, 50))\nax = ax.ravel()\n\nfor j,fold in enumerate(history):\n    ax[j*2].plot(history[fold].history[\"accuracy\"], label=\"train \"+str(fold))\n    ax[j*2].plot(history[fold].history[\"val_accuracy\"], linestyle=\"dotted\", label=\"val \"+str(fold))\n    ax[j*2].set_title(\"Model accuracy\")\n    ax[j*2].set_xlabel(\"epochs\")\n    ax[j*2].set_ylim(0,1)\n#     ax[j*2].set_ylabel(metric)\n    ax[j*2].legend()\n    \n    ax[j*2+1].plot(history[fold].history[\"loss\"], label=\"train \"+str(fold))\n    ax[j*2+1].plot(history[fold].history[\"val_loss\"], linestyle=\"dotted\", label=\"val \"+str(fold))\n    ax[j*2+1].set_title(\"Model loss\")\n    ax[j*2+1].set_xlabel(\"epochs\")\n    ax[j*2+1].set_ylim(0,1)\n#     ax[j*2+1].set_ylabel(metric)\n    ax[j*2+1].legend()","afc2ad6a":"from sklearn.model_selection import KFold\n\ninputs2 = np.array(listMatrix)\ninputs=inputs2[:,:,0,:]\nnum_folds = 5\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\ntotal_info =[]\n\nfor fold_no, (train_idx, valid_idx) in enumerate(kfold.split(inputs, targets)):\n    \n    train_idx, test_idx = train_test_split(train_idx, test_size=0.2, random_state=42)\n#     print(\"test: \",len(train_idx))\n\n    test_dataset = tf.data.Dataset.from_tensor_slices((inputs[test_idx], targets[test_idx]))\n    test_dataset = test_dataset.cache().shuffle(1000, reshuffle_each_iteration=True).batch(1, drop_remainder=True)\n    \n    log=pd.read_csv(f\".\/result\/xception_6\/{fold_no+1}_fold\/log.csv\")\n    \n    fold_model = get_sequence_model()\n#     print(log['val_loss'].idxmin())\n    fold_model.load_weights(glob.glob(f\".\/result\/xception_6\/{fold_no+1}_fold\/weights_{log['val_loss'].idxmin():08d}*.hdf5\")[0])\n    predict = fold_model.predict(test_dataset)\n    \n    predict=predict[:,:,0]\n    predict=np.mean(predict, axis=-1)\n    predict=np.round(predict)\n    \n    train_df2=train_df[[\"BraTS21ID\",\"MGMT_value\"]]\n    inference_df=train_df2.iloc[test_idx]\n    inference_df[\"MGMT_predicted\"]=predict\n    total_info.append(inference_df)\n    \n\ntotal_df = pd.concat(total_info)","5416caa1":"total_df","91bdf795":"from sklearn.metrics import roc_auc_score\n\nauc = roc_auc_score(\n    total_df.MGMT_value,\n    total_df.MGMT_predicted,\n)\nprint(f\"Validation AUC={auc}\")","0dd5e698":"from sklearn.metrics import confusion_matrix\n\nlabel = total_df['MGMT_value']\nprediction =  total_df['MGMT_predicted']\n\nlabel = np.array(label)\nprediction=np.array(prediction,dtype = int)\n\ncm = confusion_matrix(label, prediction, labels=[0, 1])\n\nprint(cm)\nprint(\"\")\nprint(f'Accuracy: {(cm[0, 0] + cm[1, 1]) \/ cm.sum():.3f}')","d254dd6a":"from sklearn import metrics\nprint(metrics.classification_report(label,prediction))","0bd9d226":"X_test = test['BraTS21ID5'].values\ntest_listMatrix = []\nfor i, patient in enumerate(tqdm(X_test)):\n    test_listVectors = []\n    test_sequence = load_dicom_images_3d(scan_id=str(patient),mri_type=MRI_TYPE,split=\"test\")\n    for j in range(len(test_sequence)):\n        img = test_sequence[j]\n        img_vector = base_resnet.predict(img)\n        test_listVectors.append(np.array(img_vector))\n    \n    test_PatientMatrix = np.stack(test_listVectors)\n    test_listMatrix.append(test_PatientMatrix)","2c68548d":"print(f\"Number of test patient matrix: {len(test_listMatrix)}\")\nprint(f\"Test patient matrix shape: {test_listMatrix[0].shape}\")","3b3e6919":"test_dataset = tf.data.Dataset.from_tensor_slices(test_listMatrix)\nlen(test_dataset)","088d85be":"final_model = get_sequence_model()\nfinal_model.load_weights(glob.glob(f\".\/result\/xception_6\/5_fold\/weights_{log['val_loss'].idxmin():08d}*.hdf5\")[0])\n  \npredict = final_model.predict(test_dataset)\nprint(predict.shape)","437eb1c3":"test = pd.read_csv(\n    data_directory+'sample_submission.csv')\ntest['BraTS21ID5'] = [format(x, '05d') for x in test.BraTS21ID]","2b49d7d1":"predict2 = predict[:,:,0]\nfinal_predict = []\nfor i in range(len(test_listMatrix)):\n    i+=1\n    final_predict.append(round(predict2[i-1].mean(),2))\n\nsubmission = test[[\"BraTS21ID\",\"MGMT_value\"]]\nsubmission[\"MGMT_value\"] = final_predict\nsubmission.to_csv('.\/submission.csv', index=False)\nsubmission","5dc585b8":"## Data condition\n\n[ pretrained model: Xception] \n\n1. default input image size : 299 x 299\n\n2.  valiable data format\n\n- 'channels_first' data format(channels, height, width).\n- 'channels_last' data format(height, width, channels).","c08cec8f":"# ConvNet Feature Extracture","95079501":"# DATA LOAD","c4f0630f":"# DATA","16e85601":"## Test","6f981a21":"In order to further emphasize the feature of the common area shared by each frame, \n\nwe use model which has strong cross-channels correlation and spatial correlation, in tShe feature extracurricular process","a090e462":"## validation set Inference","f1be971f":"### confusion matrix","79614db8":"![image.png](attachment:1904aed5-afd5-4e88-b583-b0268115d9d0.png)","9bedd514":"In this part of Transfer Learning, predictions are made only for each image of each patient's sequence without training the Xception model.\n\nTherefore, we will obtain a matrix of model weights to be integrated into the list to reproduce patient sequences for each image.","587d5d02":"## Data processing function\n\n1. Load a DCIM(Data Center Infrastructure Management) type image \n\n- method of loading & building data by accessing the id path using a generator.\n-  After loading individual data, crop and rotate augmentation are performed.\n-  Finally, data type, color is mediated, and scale normalize.\n\n2. Apply preprocess steps such as crop & resize & rotate & normalization.\n\n[Data Normalization]\n\n- convert gray scale to rgb (color emd),\n- `preprocess_input` function: sample-wise normalization\n- scale pixels 0 ~ 255 into -1 ~ 1\n\n[Data Preprocess] \n\n- resize\n    \n    Xception default size: 299 x 299\n    \n- frame of samples are all different, so All samples were unified 60 frame.\n    \n    if frame num is smaller than 60, add image padding with 0(zero) for last frame","b07619e5":"Each independent case has a dedicated folder identified by a five-digit number. \n\nWithin each of these \u201ccase\u201d folders, there are four sub-folders, each of them corresponding to each of the structural multi-parametric MRI (mpMRI) scans, in DICOM format. The exact mpMRI scans included are:","19ac852a":"- Fluid Attenuated Inversion Recovery (FLAIR)\n- T1-weighted pre-contrast (T1w)\n- T1-weighted post-contrast (T1Gd)\n- T2-weighted (T2)","1fdadafd":"# MODULE","bfa6ed98":"![image.png](attachment:eb1af0dc-dc4b-4860-b72f-8f604cd1d57b.png)","16f14a59":"- Pre-trained model Chracter\n\n1) Transfer learning\n\nTransfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem.\n\n2) fine-tuning\n\nfine-tuning consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate.\n\n\u2192 we keep freezing entire model layers.","864b346b":"# Basic Information\n\n- Pre-trained model(feature extraction) : Xception","4cb950ba":"# PREPROCESS DICOM DATA\n\n- Use weights Pre-trained on ImageNet\n- feature extractor model : Xception\n- Train model : LSTM","788178e8":"## MODEL TRAIN  PARAMETER\n\n- Loss function : Mean Square Error(mse)\n- Optimizer : RMSprop, , learning rate= 1e-3\n- Metrics : accuracy","09e2eb5f":"## Feature Extract","99cafadb":"## EVALUATION\n\n- 5-fold cross validation method\n- Train data num: 468\n- Validation data num: 117","6a910c5b":"Finally, we will create a global matrix that groups sequences of Xception for all patients.","46d100d0":"for test set predict, we use weights which has smallest validation loss in 5th fold ","1f93a881":"# Result","f83a387d":"# TRAIN\n\nfor sequence data, chain structure model is used"}}