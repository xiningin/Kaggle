{"cell_type":{"bc00ddf6":"code","49bd9556":"code","341d6f20":"code","2faf9494":"code","54cce4ef":"code","561611d2":"code","ce8cef01":"code","6b295a5e":"code","05ba7a70":"code","6c18815f":"code","0349c752":"code","4ee0fad7":"code","a9021298":"code","5311689c":"code","435c85c6":"code","c702359e":"code","eaf88156":"code","1e808d51":"code","aa1b71b1":"code","062105d7":"code","b9ba95f2":"code","abb6973d":"code","21bebe05":"code","24b80c50":"code","bbddd6d6":"code","0bab0c29":"code","7d9ca74f":"code","f0aa4e52":"code","e3b5589e":"code","9feafb0c":"code","cceefa0f":"code","22b4dfa9":"code","37d820c0":"code","b5a48a0f":"code","552e6500":"code","8ea552ac":"code","957d8a28":"code","818db8f1":"code","f83d4008":"code","e0bc991e":"markdown","232ca233":"markdown","7e520824":"markdown","21f3d3ff":"markdown","617a70e0":"markdown","d18a46d0":"markdown","22bb3a60":"markdown","fd74b55c":"markdown","117a00b3":"markdown","b6ae551a":"markdown","946bcfe4":"markdown","cbb56744":"markdown","f4f5c3aa":"markdown","75bdd977":"markdown","84b9cdae":"markdown","1e21df9d":"markdown","f42519f6":"markdown","306f22fa":"markdown","fd35e079":"markdown","676d214c":"markdown","5c9b48ce":"markdown"},"source":{"bc00ddf6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","49bd9556":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","341d6f20":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","2faf9494":"train.head()","54cce4ef":"test.head()","561611d2":"train.describe()","ce8cef01":"test.describe()","6b295a5e":"train.shape","05ba7a70":"test.shape","6c18815f":"train_missing=[]\nfor col in train.columns:\n    if train[col].isna().sum()!=0:\n        missing=train[col].isna().sum()\n        print(f\"{col: <{20}} {missing}\")\n        if missing>train.shape[0]\/3:\n              train_missing.append(col)","0349c752":"test_missing=[]\nfor col in test.columns:\n    if test[col].isna().sum()!=0:\n        missing=test[col].isna().sum()\n        print(f\"{col: <{20}} {missing}\")\n        if missing>test.shape[0]\/3:\n              test_missing.append(col)","4ee0fad7":"cols= train.columns\nno_of_cols = len(cols)\nprint(\"Number of columns: {}\".format(no_of_cols))\n\nnumeric_cols = [col for col in train.columns if (train[col].dtype in (\"int32\", \"int64\", \"float64\"))]\nprint(\"Numeric columns: {}\".format(numeric_cols))\nprint(\"No of numeric columns: {}\".format(len(numeric_cols)))\n      \ncategorical_cols = [col for col in train.columns if (train[col].dtype == 'object')]\nprint(\"Categorical columns: {}\".format(categorical_cols))\nprint(\"No of categorical columns: {}\".format(len(categorical_cols)))","a9021298":"cols= test.columns\nno_of_cols = len(cols)\nprint(\"Number of columns: {}\".format(no_of_cols))\n\nnumeric_cols = [col for col in test.columns if (test[col].dtype in (\"int32\", \"int64\", \"float64\"))]\nprint(\"Numeric columns: {}\".format(numeric_cols))\nprint(\"No of numeric columns: {}\".format(len(numeric_cols)))\n      \ncategorical_cols = [col for col in test.columns if (test[col].dtype == 'object')]\nprint(\"Categorical columns: {}\".format(categorical_cols))\nprint(\"No of categorical columns: {}\".format(len(categorical_cols)))","5311689c":"plt.figure(figsize=(14,12))\nsns.heatmap(train.corr(), vmax=.8, square=True, cmap='Blues')","435c85c6":"corr_cols= train.corr().nlargest(15, 'SalePrice')['SalePrice'].index\nplt.figure(figsize=(12,10))\nsns.heatmap(train[corr_cols].corr(), annot=True, vmax=.8, square=True, cmap='Blues')","c702359e":"for col in train[categorical_cols]:\n    if train[col].isnull().sum()>0:\n        print(col, train[col].unique())","eaf88156":"col_fillna=('Alley','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n            'BsmtFinType2','Electrical','FireplaceQu','GarageType','GarageFinish',\n            'GarageQual','GarageCond','PoolQC','Fence', 'MiscFeature')","1e808d51":"for col in col_fillna:\n    train[col].fillna('no',inplace=True)\n    test[col].fillna('no',inplace=True)","aa1b71b1":"total= train.isnull().sum().sort_values(ascending=False)\npercent= (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\ntotal_missing=pd.concat([total,percent], axis=1, keys=['Total','Percent'])\ntotal_missing","062105d7":"total= test.isnull().sum().sort_values(ascending=False)\npercent= (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\ntotal_missing=pd.concat([total,percent], axis=1, keys=['Total','Percent'])\ntotal_missing","b9ba95f2":"train.fillna(train.median(), inplace=True)\ntest.fillna(test.median(), inplace=True)","abb6973d":"print(train.isnull().sum().sum())\nprint(test.isnull().sum().sum())","21bebe05":"test.isnull().sum().sort_values(ascending=False)[:8]","24b80c50":"for col in test.columns:\n    if test[col].dtype=='object' and test[col].isnull().sum():\n        test.fillna('no', inplace=True)","bbddd6d6":"test.isnull().sum().sum()","0bab0c29":"from sklearn.preprocessing import OrdinalEncoder\noe= OrdinalEncoder()\ntrain[categorical_cols]= oe.fit_transform(train[categorical_cols])\ntest[categorical_cols]= oe.fit_transform(test[categorical_cols])","7d9ca74f":"x= train.drop('SalePrice', axis=1)\ny= train.SalePrice.copy()","f0aa4e52":"from sklearn.feature_selection import mutual_info_classif\nimportant= mutual_info_classif(x, y)\nimportant_features= pd.Series( important, train.columns[:len(train.columns)-1])\nimportant_features","e3b5589e":"feature_cols=[]\nfor key, val in important_features.items():\n    if np.abs(val)>0.5:\n        feature_cols.append(key)\n        \nprint(feature_cols)\nprint(len(feature_cols))","9feafb0c":"x=x[feature_cols]\nxtest= test[feature_cols]","cceefa0f":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test= train_test_split(x, y, test_size=0.2, random_state=0) ","22b4dfa9":"from sklearn.svm import SVR\nregressor= SVR(kernel='rbf')\nregressor.fit(x_train, y_train)\n\nfrom sklearn.metrics import mean_squared_error\ny_pred1= regressor.predict(x_test)\nmean_squared_error(y_test, y_pred1, squared=False)","37d820c0":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(random_state=2, n_estimators=200)\nrf_model.fit(x_train, y_train)\ny_pred2 = rf_model.predict(x_test)\nmean_squared_error(y_test, y_pred2, squared=False)","b5a48a0f":"from sklearn.linear_model import LinearRegression\nlr= LinearRegression()\nlr.fit(x_train, y_train)\n\ny_pred3= lr.predict(x_test)\nmean_squared_error(y_test, y_pred3, squared=False)","552e6500":"from sklearn.tree import DecisionTreeRegressor\ndt= DecisionTreeRegressor()\ndt.fit(x_train, y_train)\n\ny_pred4= lr.predict(x_test)\nmean_squared_error(y_test, y_pred4, squared=False)","8ea552ac":"import xgboost as xg\nxgb_model = xg.XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\nxgb_model.fit(x_train, y_train)\n\ny_pred5= xgb_model.predict(x_test)\nmean_squared_error(y_test, y_pred5, squared=False)","957d8a28":"submission_csv = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","818db8f1":"yPred = xgb_model.predict(xtest)\nmean_squared_error(submission_csv['SalePrice'], yPred, squared=False)","f83d4008":"submission_csv.SalePrice = yPred\nsubmission_csv.to_csv('output.csv', index=False)","e0bc991e":"# Choosing X and Y","232ca233":"# Test and train split","7e520824":"# Classifying into numeric and categotical columns","21f3d3ff":"# Random Forest Regression","617a70e0":"# Encoding the Categorical Columns","d18a46d0":"# **Submitting the prediction**","22bb3a60":"# Decision Tree Regression","fd74b55c":" Choosing XG Boost Regression over other models as it has the minimum mean squared error","117a00b3":"# Multiple Linear Regression","b6ae551a":"# **HOUSE PRICE PREDICTION**","946bcfe4":"# Taking only the important features into consideration","cbb56744":"# Filling the missing values","f4f5c3aa":"# **Regression Models**","75bdd977":"# SVM model","84b9cdae":"# Data Visualization","1e21df9d":"# Importing Dataset","f42519f6":"# XG Boost Regression","306f22fa":"# Getting the correlation between features and label","fd35e079":"# Importing Libraries","676d214c":"# Taking the new features","5c9b48ce":"# Finding the missing values"}}