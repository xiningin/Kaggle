{"cell_type":{"f89c0672":"code","5a965288":"code","0ba52e1e":"code","cbde4df5":"code","90dab80c":"code","02d98ecc":"code","f439c7a5":"code","59dfd230":"code","9769e92d":"code","57396df4":"code","f9d46733":"code","d9acba0f":"code","ce749158":"code","f3d82d7f":"code","00c4ed67":"code","849e0621":"code","67d0f292":"code","125a12f8":"code","5d1cba7a":"code","9e348df1":"code","486b0761":"code","b2e3cb0a":"code","6352b04d":"markdown","15bc64bf":"markdown","e1fd002f":"markdown","23115b7a":"markdown","390a834a":"markdown","7fe77f7b":"markdown"},"source":{"f89c0672":"!pip install transformers==2.10.0\n\n!apt install aptitude -y\n!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n!pip install mecab-python3\n\n!pip install unidic-lite","5a965288":"import gc\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\n\nimport MeCab\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.utils.data\n\nfrom tqdm.autonotebook import tqdm\nimport pickle\n\nfrom transformers import *\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda')","0ba52e1e":"VALID_BATCH_SIZE = 2\nMODEL_DIR = \"\/kaggle\/input\/aio-best-for-ensemble\/\"\nPREDICT_DICT = {}","cbde4df5":"def get_preds(model, data_loader):\n    tk = tqdm(data_loader, total=len(data_loader))\n    predict = []\n    with torch.no_grad():\n        for bi, d in enumerate(tk):\n            ids = d[\"ids\"].to(device, dtype=torch.long)\n            mask = d[\"mask\"].to(device, dtype=torch.long)\n            try:\n                token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n            except KeyError:\n                token_type_ids = None\n\n            if len(ids.shape) == 2:\n                ids = ids.unsqueeze(0)\n                mask = mask.unsqueeze(0)\n                if token_type_ids is not None:\n                    token_type_ids = token_type_ids.unsqueeze(0)\n\n            if token_type_ids is not None:\n                pred = model(ids, mask, token_type_ids)\n            else:\n                pred = model(ids, mask)\n            pred = pred.cpu().detach().numpy()\n            pred = pred + (-99999 * (d[\"in_q\"]!=0).int().numpy())\n            \n            predict.append(pred)\n    predict = np.vstack(predict)\n    return predict","90dab80c":"ORG_DIR = \"\/kaggle\/input\/aio-org-csv\"\n\ntest_df = pd.read_csv(f\"{ORG_DIR}\/aio_leaderboard.csv\")\nqid = test_df[\"qid\"].values\nanswer_candidates =  test_df[\"answer_candidates\"].map(lambda x: eval(x)).values\n\ndev1_questions = pd.read_csv(f\"{ORG_DIR}\/dev1_questions.csv\")\ndev2_questions = pd.read_csv(f\"{ORG_DIR}\/dev2_questions.csv\")\n\ndef get_in_q(questions):\n    lst = []\n    for idx in range(len(questions)):\n        candidate = eval(questions.loc[idx, \"answer_candidates\"])\n        question = questions.loc[idx, \"question\"]\n        in_q = [c in question for c in candidate]\n        lst.append(in_q)\n    return lst\n\ndev1_in_q = get_in_q(dev1_questions)\ndev2_in_q = get_in_q(dev2_questions)\ntest_in_q = get_in_q(test_df)","02d98ecc":"class BertForAIO(nn.Module):\n    def __init__(self):\n        super(BertForAIO, self).__init__()\n\n        bert_conf = BertConfig(\"bert-base-japanese\")\n        bert_conf.output_hidden_states = True\n        bert_conf.vocab_size = 32000\n\n        self.n_use_layer = 1\n        self.dropuot_rate = 0.2\n        self.dropout_sample = 5\n\n        self.bert = AutoModel.from_pretrained(\"bert-base-japanese\", config=bert_conf)\n        \n        self.dropout = nn.Dropout(self.dropuot_rate)\n        n_weights = bert_conf.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n        \n        self.dense1 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        self.dense2 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        \n        self.dropouts = nn.ModuleList([nn.Dropout(self.dropuot_rate) for _ in range(self.dropout_sample)])\n    \n        self.fc = nn.Linear(bert_conf.hidden_size*self.n_use_layer, 1)\n\n\n    def forward(self, ids, mask, token_type_ids):\n        n_choice = ids.shape[1]\n        \n        ids = ids.view(-1, ids.size(-1))\n        mask = mask.view(-1, mask.size(-1))\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n        _, _, h = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n\n        cat_output = torch.stack([self.dropout(layer[:, 0, :]) for layer in h], dim=2)\n        cat_output = (torch.softmax(self.layer_weights, dim=0) * cat_output).sum(-1)\n\n        cat_output = self.dense1(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n        cat_output = self.dense2(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n\n        logits = sum([self.fc(dropout(cat_output)) for dropout in self.dropouts])\/self.dropout_sample\n\n        logits = logits.view(-1, n_choice)\n\n        return logits\n    \nclass JaqketDataset:\n    def __init__(self, data, optinons=20, in_q=None):\n        self.data = data\n        self.optinons = optinons\n        self.negative_sample = list(range(1, 20))\n        self.in_q = in_q\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        d = self.data[item]\n        if self.in_q is not None:\n            return {\n                'ids': torch.tensor(d[\"input_ids\"][:self.optinons], dtype=torch.long),\n                'mask': torch.tensor(d[\"attention_mask\"][:self.optinons], dtype=torch.long),\n                'token_type_ids': torch.tensor(d[\"token_type_ids\"][:self.optinons], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long),\n                'in_q': torch.tensor(self.in_q[item], dtype=torch.long),\n            }\n        else:\n            return {\n                'ids': torch.tensor(d[\"input_ids\"][:self.optinons], dtype=torch.long),\n                'mask': torch.tensor(d[\"attention_mask\"][:self.optinons], dtype=torch.long),\n                'token_type_ids': torch.tensor(d[\"token_type_ids\"][:self.optinons], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long)}","f439c7a5":"DATA_DIR = \"\/kaggle\/input\/aio-bert-tokenids-nnlm\"\nwith open(f\"{DATA_DIR}\/dev1.pkl\", \"rb\") as f:\n    dev1 = pickle.load(f)\nwith open(f\"{DATA_DIR}\/dev2.pkl\", \"rb\") as f:\n    dev2 = pickle.load(f)\nwith open(f\"{DATA_DIR}\/test.pkl\", \"rb\") as f:\n    test = pickle.load(f)","59dfd230":"dev1_dataset = JaqketDataset(dev1, optinons=20, in_q=dev1_in_q)\ndev1_data_loader = torch.utils.data.DataLoader(\n        dev1_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        drop_last=False,\n        num_workers=1\n  )\n\ndev2_dataset = JaqketDataset(dev2, optinons=20, in_q=dev2_in_q)\ndev2_data_loader = torch.utils.data.DataLoader(\n        dev2_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        drop_last=False,\n        num_workers=1\n  )\n\ntest_dataset = JaqketDataset(test, optinons=20, in_q=test_in_q)\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=VALID_BATCH_SIZE,\n    drop_last=False,\n    num_workers=1\n)","9769e92d":"model = BertForAIO()\nmodel.load_state_dict(torch.load(f\"{MODEL_DIR}\/bert_base_best.bin\", map_location=torch.device('cpu')))\nmodel.to(device)\nmodel.eval()\n\ndev1_preds = get_preds(model, dev1_data_loader)\ndev2_preds = get_preds(model, dev2_data_loader)\ntest_preds = get_preds(model, test_dataset)\n\nPREDICT_DICT[\"bert_base\"] = {\n    \"dev1\":dev1_preds,\n    \"dev2\":dev2_preds,\n    \"test\":test_preds,\n}","57396df4":"del BertForAIO, JaqketDataset, model\ndel DATA_DIR, dev1, dev2, test, dev1_dataset, dev1_data_loader, dev2_dataset, dev2_data_loader, test_dataset, test_data_loader\ngc.collect()","f9d46733":"class XLMRoBertaForAIO(nn.Module):\n    def __init__(self):\n        super(XLMRoBertaForAIO, self).__init__()\n\n        xlm_roberta_conf = XLMRobertaConfig(\"xlm-roberta-base\")\n        xlm_roberta_conf.output_hidden_states = True\n        xlm_roberta_conf.vocab_size = 250002\n        xlm_roberta_conf.pad_token_id = 1\n        xlm_roberta_conf.max_position_embeddings = 514\n        xlm_roberta_conf.type_vocab_size = 1\n\n        self.n_use_layer = 1\n        self.dropout_sample = 5\n\n        self.xlm_roberta = AutoModel.from_pretrained(\"xlm-roberta-base\", config=xlm_roberta_conf)\n                \n        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(self.dropout_sample)])\n    \n        self.fc = nn.Linear(xlm_roberta_conf.hidden_size*self.n_use_layer, 1)\n\n\n    def forward(self, ids, mask):\n        n_choice = ids.shape[1]\n        \n        ids = ids.view(-1, ids.size(-1))\n        mask = mask.view(-1, mask.size(-1))\n\n        _, _, h = self.xlm_roberta(ids, attention_mask=mask)\n\n        cat_output = h[-1][:, 0, :]\n        logits = sum([self.fc(dropout(cat_output)) for dropout in self.dropouts])\/self.dropout_sample\n\n        logits = logits.view(-1, n_choice)\n\n        return logits\n    \n    \nclass JaqketDataset:\n    def __init__(self, data, optinons=20, in_q=None):\n        self.data = data\n        self.optinons = optinons\n        self.negative_sample = list(range(1, 20))\n        self.in_q = in_q\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        d = self.data[item]\n        if self.in_q is not None:\n            return {\n                'ids': torch.tensor(d[\"input_ids\"][:self.optinons], dtype=torch.long),\n                'mask': torch.tensor(d[\"attention_mask\"][:self.optinons], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long),\n                'in_q': torch.tensor(self.in_q[item], dtype=torch.long),\n            }\n        else:\n            return {\n                'ids': torch.tensor(d[\"input_ids\"][:self.optinons], dtype=torch.long),\n                'mask': torch.tensor(d[\"attention_mask\"][:self.optinons], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long)}","d9acba0f":"DATA_DIR = \"\/kaggle\/input\/aio-bert-tokenids-nnlm-multi-lang\"\nwith open(f\"{DATA_DIR}\/dev1.pkl\", \"rb\") as f:\n    dev1 = pickle.load(f)\nwith open(f\"{DATA_DIR}\/dev2.pkl\", \"rb\") as f:\n    dev2 = pickle.load(f)\nwith open(f\"{DATA_DIR}\/test.pkl\", \"rb\") as f:\n    test = pickle.load(f)","ce749158":"dev1_dataset = JaqketDataset(dev1, optinons=20, in_q=dev1_in_q)\ndev1_data_loader = torch.utils.data.DataLoader(\n        dev1_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        drop_last=False,\n        num_workers=1\n  )\n\ndev2_dataset = JaqketDataset(dev2, optinons=20, in_q=dev2_in_q)\ndev2_data_loader = torch.utils.data.DataLoader(\n        dev2_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        drop_last=False,\n        num_workers=1\n  )\n\ntest_dataset = JaqketDataset(test, optinons=20, in_q=test_in_q)\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=VALID_BATCH_SIZE,\n    drop_last=False,\n    num_workers=1\n  )","f3d82d7f":"!ls {MODEL_DIR}","00c4ed67":"model = XLMRoBertaForAIO()\nmodel.load_state_dict(torch.load(f\"{MODEL_DIR}\/xlmroberta_best.bin\", map_location=torch.device('cpu')))\nmodel.to(device)\nmodel.eval()\n\ndev1_preds = get_preds(model, dev1_data_loader)\ndev2_preds = get_preds(model, dev2_data_loader)\ntest_preds = get_preds(model, test_dataset)\n\nPREDICT_DICT[\"xlm_roberta\"] = {\n    \"dev1\":dev1_preds,\n    \"dev2\":dev2_preds,\n    \"test\":test_preds,\n}","849e0621":"del XLMRoBertaForAIO, JaqketDataset, model\ndel DATA_DIR, dev1, dev2, test, dev1_dataset, dev1_data_loader, dev2_dataset, dev2_data_loader, test_dataset, test_data_loader\ngc.collect()","67d0f292":"class BertForAIO(nn.Module):\n    def __init__(self):\n        super(BertForAIO, self).__init__()\n\n        bert_conf = BertConfig(\"bert-base-japanese-char-whole-word-masking\")\n        bert_conf.output_hidden_states = True\n        bert_conf.vocab_size = 4000\n\n        self.n_use_layer = 1\n        self.dropuot_rate = 0.2\n        self.dropout_sample = 5\n\n        self.ksizes = [3, 4, 5]\n        self.num_filters = 100\n        \n        \n        self.bert = AutoModel.from_pretrained(\"bert-base-japanese-char-whole-word-masking\", config=bert_conf)\n        \n        self.convs = nn.ModuleList([nn.Conv1d(bert_conf.hidden_size, self.num_filters, k) for k in self.ksizes])\n        \n        self.dropouts = nn.ModuleList([nn.Dropout(self.dropuot_rate) for _ in range(self.dropout_sample)])\n        \n        self.fc = nn.Linear(self.num_filters, 1)\n\n\n    def forward(self, ids, mask, token_type_ids):\n        n_choice = ids.shape[1]\n        \n        ids = ids.view(-1, ids.size(-1))\n        mask = mask.view(-1, mask.size(-1))\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n        _, _, h = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        \n        cat_output = h[-1]\n        \n        lst = []\n        for k, conv in zip(self.ksizes, self.convs):\n            sequence_output = cat_output.transpose(1, 2)\n            sequence_output = F.pad(sequence_output, (0, k-1))\n            h = sum([conv(dropout(sequence_output)).transpose(1, 2) for dropout in self.dropouts])\/self.dropout_sample\n            lst.append(h)\n        h = sum(lst)\n        h, _ = h.max(1)\n        logits = self.fc(h)\n\n        logits = logits.view(-1, n_choice)\n\n        return logits\n    \n\nclass JaqketDataset:\n    def __init__(self, data, optinons=20, in_q=None):\n        self.data = data\n        self.optinons = optinons\n        self.negative_sample = list(range(1, 20))\n        self.in_q = in_q\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        d = self.data[item]\n        if self.in_q is not None:\n            return {\n                'ids': torch.tensor(d[\"input_ids\"][:self.optinons], dtype=torch.long),\n                'mask': torch.tensor(d[\"attention_mask\"][:self.optinons], dtype=torch.long),\n                'token_type_ids': torch.tensor(d[\"token_type_ids\"][:self.optinons], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long),\n                'in_q': torch.tensor(self.in_q[item], dtype=torch.long),\n            }\n        else:\n            return {\n                'ids': torch.tensor(d[\"input_ids\"][:self.optinons], dtype=torch.long),\n                'mask': torch.tensor(d[\"attention_mask\"][:self.optinons], dtype=torch.long),\n                'token_type_ids': torch.tensor(d[\"token_type_ids\"][:self.optinons], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long)}","125a12f8":"DATA_DIR = \"\/kaggle\/input\/aio-bert-tokenids-char\"\nwith open(f\"{DATA_DIR}\/dev1.pkl\", \"rb\") as f:\n    dev1 = pickle.load(f)\nwith open(f\"{DATA_DIR}\/dev2.pkl\", \"rb\") as f:\n    dev2 = pickle.load(f)\nwith open(f\"{DATA_DIR}\/test.pkl\", \"rb\") as f:\n    test = pickle.load(f)","5d1cba7a":"dev1_dataset = JaqketDataset(dev1, optinons=20, in_q=dev1_in_q)\ndev1_data_loader = torch.utils.data.DataLoader(\n        dev1_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        drop_last=False,\n        num_workers=1\n  )\n\ndev2_dataset = JaqketDataset(dev2, optinons=20, in_q=dev2_in_q)\ndev2_data_loader = torch.utils.data.DataLoader(\n        dev2_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        drop_last=False,\n        num_workers=1\n  )\n\ntest_dataset = JaqketDataset(test, optinons=20, in_q=test_in_q)\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=VALID_BATCH_SIZE,\n    drop_last=False,\n    num_workers=1\n  )","9e348df1":"model = BertForAIO()\nmodel.load_state_dict(torch.load(f\"{MODEL_DIR}\/bert_char_best.bin\", map_location=torch.device('cpu')))\nmodel.to(device)\nmodel.eval()\n\ndev1_preds = get_preds(model, dev1_data_loader)\ndev2_preds = get_preds(model, dev2_data_loader)\ntest_preds = get_preds(model, test_dataset)\n\nPREDICT_DICT[\"bert_char\"] = {\n    \"dev1\":dev1_preds,\n    \"dev2\":dev2_preds,\n    \"test\":test_preds,\n}","486b0761":"del BertForAIO, JaqketDataset, model\ndel DATA_DIR, dev1, dev2, test, dev1_dataset, dev1_data_loader, dev2_dataset, dev2_data_loader, test_dataset, test_data_loader\ngc.collect()","b2e3cb0a":"with open(f\"ensemble_predicts.pkl\", \"wb\") as f:\n    pickle.dump(PREDICT_DICT, f)","6352b04d":"\u7d50\u679c\u4fdd\u5b58","15bc64bf":"### BERT base","e1fd002f":"### BERT char","23115b7a":"\u5171\u901a\u8a2d\u5b9a","390a834a":"### XLM RoBERTa","7fe77f7b":"\u5143\u30b3\u30f3\u30da\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f"}}