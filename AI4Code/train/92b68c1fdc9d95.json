{"cell_type":{"aa06893e":"code","4903e20b":"code","46bb71b5":"code","959b2408":"code","fd1a293a":"code","5bc930bb":"code","35c34cee":"code","3b92e301":"code","41f3b6ec":"code","c1538490":"code","c487feee":"code","3b79bc49":"code","927ee63c":"code","37cb7e39":"code","23fb0113":"code","5397ade4":"code","d60e1336":"code","a2960135":"code","43baefef":"code","ccbd6834":"code","c1559ff0":"code","03b677f4":"code","949edb83":"code","748ed296":"code","4b236ad2":"markdown","1562c48e":"markdown","2806566d":"markdown","7c14442c":"markdown","894cf0fb":"markdown","511de93c":"markdown","751bd9c7":"markdown","7334f9b4":"markdown","982332fe":"markdown","ff75290a":"markdown","70521868":"markdown","5d23038b":"markdown","1a19d910":"markdown","63670c13":"markdown","796ebe96":"markdown","60392caa":"markdown","ed0fd719":"markdown","4fad4c90":"markdown","b7d9127d":"markdown","fe2c33d6":"markdown"},"source":{"aa06893e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))","4903e20b":"# Reading Train Data\ndfTrain = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\nprint(\"Shape of Train Data: \" + str(dfTrain.shape))","46bb71b5":"# First 5 rows of train data\ndfTrain.head()","959b2408":"# Reading Test Data\ndfTest = pd.read_csv(\"..\/input\/fashion-mnist_test.csv\")\nprint(\"Shape of Test Data: \" + str(dfTest.shape))","fd1a293a":"# First 5 rows of test data\ndfTest.head()","5bc930bb":"Y_train = dfTrain.label\nX_train = dfTrain.drop([\"label\"], axis=1)\nX_test = dfTest.drop([\"label\"], axis=1)\nY_test = dfTest.label","35c34cee":"plt.figure(figsize=(16,5))\nsns.countplot(Y_train, palette=\"terrain\")\nplt.title(\"Number of Classes\")\nplt.show()","3b92e301":"plt.figure(figsize=(20,5))\n\nfor i in range(10):\n    plt.subplot(2,5,i+1)\n    img = dfTrain[dfTrain.label==i].iloc[0,1:].values\n    img = img.reshape((28,28))\n    plt.imshow(img, cmap='gray')\n    plt.title(\"Class: \" + str(i))\n    plt.axis('off')\n    \nplt.show()","41f3b6ec":"# Normalization\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","c1538490":"# Reshape\nX_train = X_train.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)\nprint(\"X_train Shape: \", X_train.shape)\nprint(\"X_test Shape: \", X_test.shape)","c487feee":"# Label Encoding\nfrom keras.utils.np_utils import to_categorical\n\nY_train = to_categorical(Y_train, num_classes=10)","3b79bc49":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 42)\n\nprint(\"x_train shape\",x_train.shape)\nprint(\"x_test shape\",x_val.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"y_test shape\",y_val.shape)","927ee63c":"from sklearn.metrics import confusion_matrix\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(10, activation='softmax'))\n","37cb7e39":"optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)","23fb0113":"model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])","5397ade4":"epochs = 50\nbatchSize = 300","d60e1336":"# Data Augmentation\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=0.1,  # randomly rotate images in the range\n        zoom_range = 0.1, # Randomly zoom image\n        width_shift_range=0.1,  # randomly shift images horizontally\n        height_shift_range=0.1,  # randomly shift images vertically\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(x_train)","a2960135":"cnn = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batchSize), epochs=epochs, validation_data=(x_val, y_val), steps_per_epoch=x_train.shape[0] \/\/ batchSize)","43baefef":"print(\"Accuracy after fitting: {:.2f}%\".format(cnn.history['acc'][-1]*100))","ccbd6834":"plt.figure(figsize=(18,6))\n\nplt.subplot(1,2,1)\nplt.plot(cnn.history['loss'], color=\"blue\", label = \"Loss\")\nplt.plot(cnn.history['val_loss'], color=\"orange\", label = \"Validation Loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(cnn.history['acc'], color=\"green\", label = \"Accuracy\")\nplt.plot(cnn.history['val_acc'], color=\"red\", label = \"Validation Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Number of Epochs\")\nplt.legend()\nplt.show()","c1559ff0":"Y_test = to_categorical(Y_test, num_classes=10) # One-Hot Encoding","03b677f4":"score = model.evaluate(X_test, Y_test)\nprint(\"Test Loss: {:.4f}\".format(score[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(score[1]*100))","949edb83":"Y_pred = model.predict(X_test)\nY_pred_classes = np.argmax(Y_pred, axis = 1)\nY_true = np.argmax(Y_test, axis = 1)\nconfusionMatrix = confusion_matrix(Y_true, Y_pred_classes)\n\nf,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(confusionMatrix, annot=True, linewidths=0.1, cmap = \"gist_yarg_r\", linecolor=\"black\", fmt='.0f', ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","748ed296":"for i in range(len(confusionMatrix)):\n    print(\"Class:\",str(i))\n    print(\"Number of Wrong Prediction:\", str(sum(confusionMatrix[i])-confusionMatrix[i][i]), \"out of 1000\")\n    print(\"Percentage of True Prediction: {:.2f}%\".format(confusionMatrix[i][i] \/ 10))\n    print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")","4b236ad2":"Let's find out score by giving test data we imported before.","1562c48e":"As you can see above we should focus on especially Class 0, Class 2 and Class 6 to improve our score.\n\nThanks!\n\n**If you like it please upvote and I will be happy to hear your comments!**        ","2806566d":"### Normalization\n<br>\nWe'll use normalization to reduce effect of illumination's differences. Also it contributes to works CNN faster.","7c14442c":"### Implementing Convolutional Neural Network Algorithm with Keras\n<br>\n<font size=4.5 color=\"blue\">Create Model<\/font>","894cf0fb":"### Label Encoding\n<br>\nWe turn our classes into one-hot encoding label.\n<br>\n<img src=\"https:\/\/i.stack.imgur.com\/Fcdj4.jpg\"\/>","511de93c":"### Example Images","751bd9c7":"### Train-Test Split\n<br>\nWe'll split our train data 30% of data will be validation data and 70% of data will be train data.","7334f9b4":"<font size=4.5 color=\"blue\">Compile Model<\/font>\n<br>\nSince we have 10 classes we'll use categorical crossentropy.","982332fe":"We'll use 'Adam Optimizer'. Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. It is different to classical stochastic gradient descent. SGD pursues a single learning rate for all weights updates and learning rate(alpha) doesn't change during training process. However in adam optimizer we can say adam optimizer updates leraning rate dynamically.\n<br>\n<img src=\"https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/05\/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png\" \/>","ff75290a":"<font size=4.5 color=\"blue\">Fit The Model<\/font>","70521868":"<font size=4.5 color=\"blue\">Epoch and Batch Size<\/font>\n<br>\nEpoch is the number of times the algorithm sees the entire data set. If one epoch is too big to run to the computer at once we divide it smaller parts and number of this parts is called batch.","5d23038b":"<font size=4.5 color=\"blue\">Optimizer<\/font>","1a19d910":"### Import Libraries and Read Data","63670c13":"# CONVOLUTIONAL NEURAL NETWORK\n<br><br>\n<font size=4.5 color=\"red\"> What is Convolutional Neural Network (CNN or ConvNet) ?<\/font>\n<br>\nConvolutional Neural Network is a deep learning algorithm which is used for recognizing images. This algorithm clusters images by similarity and perform object recognition within scenes. CNN uses unique feature of images (e.g. cat's tail and ears, airplane's wing and engine etc.) to identify object that is placed on the image. Actually this process is very similar with what our brain does to identify objects. <br>\nTraditional neural networks are not ideal for image processing that's why we are using CNN. Altough CNN is not too different from ANN. Because in the end CNN algorithm uses Artificial Neural Networks but before that CNN uses some layers to gather information and determine some features from the image.\n<br>\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*N4h1SgwbWNmtrRhszM9EJg.png\" \/>\n<br>\nWhat are these layers or processes? How CNN works?\n-  Convolution\n-  Padding\n-  Pooling\n-  Flattening\n-  Full Connection\n\n<br>\n<br>\n<font size=4.5 color=\"red\"> Convolutional Layer (Convolutional Operation)<\/font>\n<br>\nThis process is main process for CNN. In this operation there is a feature detector or filter. This filter detects edges or specific shapes. Filter is placed top left of image and multiplied with value on same indices. After that all results are summed and this result is written to output matrix. Then filter slips to right to do this whole processes again and again. Usually filter slips one by one but it can be change according to your model and this slipping process is called 'stride'. Bigger stride means smaller output. Sometimes stride value is increased to decrease output size and time.\n<br><br>\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQoNDQAgN5M94MXU7K6sjqSW4FkVmAvY18DQGKf9ux27wA1w9Sm\"\/>\n<br>\nLet's try to understand process above with a visualization.\n<br>\n<img src=\"https:\/\/i2.wp.com\/tuncerergin.com\/wp-content\/uploads\/2018\/02\/giphy.gif?fit=480%2C272\"\/>\n<br>\nAfter convolutional operations we will use an activation function to break up linearity. We want to increase non-linearity otherwise algorithm can't understand image and act like it is a linear function. In other algorithms we usually use sigmoid and tanh functions as activation functions but in Convolutional Neural Network we are using **ReLU** becuase ReLU function is better for time-efficiency.\n<br>\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*XxxiA0jJvPrHEJHD4z893g.png\"\/>\n<br><br>\n<font size=4.5 color=\"red\">Padding<\/font>\n<br>\nWe have to keep as much information we can in early processes of CNN. But convolutional operations we mentioned above decrease size of image that's why we apply <font color=\"blue\">**Padding**<\/font> to preserve our input size.\n<br>\nFor example our input size is 36x36x3 but after convolutional operations we have output with size of 32x32x3 we will fix it by adding some padding like below.\n<img src=\"https:\/\/i2.wp.com\/tuncerergin.com\/wp-content\/uploads\/2018\/02\/XzUSmeu.png?w=263\"\/>\n<br><br>\n<font size=4.5 color=\"red\">Pooling<\/font>\n<br>\nThis layer is used for reducing parameters and computating process. Also by using this layer features invariant to scale or orientation changes are detected and it prevents overfitting. There are some pooling process like average pooling, max pooling etc. But mostly **max pooling** is used. Let's say we have a 2x2 filter and an 4x4 input(image) results of max pooling and average pooling will be like that:\n(stride = 2)\n<br>\n<img src=\"https:\/\/www.researchgate.net\/profile\/Zenghui_Wang2\/publication\/317496930\/figure\/fig1\/AS:551445004066816@1508486142257\/Average-versus-max-pooling_W840.jpg\" \/>\n<br>\n<br>\n<font size=4.5 color=\"red\">Flattening<\/font>\n<br>\nBasically flattening is taking matrix came from convolutional and pooling processes and turn it into one dimensional array. This is important because input of fully-connected layer -or let's say Artifical Neural Networks- consist of one dimensional array.\n<br>\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*Lzx2pNLpHjGTKcofsaSH1g.png\" width=400\/>\n<br><br>\n<font size=4.5 color=\"red\">Full Connection<\/font>\n<br>\nThis layer takes data from one dimension array we saw above and starts learning process.\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*Gh5PS4R_A5drl5ebd_gNrg@2x.png\" width=600\/>","796ebe96":"## Convolutional Neural Network with Keras","60392caa":"<font size=4.5 color=\"red\">Dropout<\/font>\n<br>\nDropout is a regularizaton technique for reducing overfitting. It is called \"dropout\" because it drops out visible or hidden units in neural network.\n<br>\n<img src=\"http:\/\/perso.mines-paristech.fr\/fabien.moutarde\/ES_MachineLearning\/TP_convNets\/drop.png\"\/>\n<br>\n<br>\nLet's look dataset we'll use.\n<br><br><br>\n<font size=4.5 color=\"blue\">About Dataset<\/font>\n<br>\nNow we\u2019ll try to use this algorithm with a dataset contains images of 10 different classes of clothing.\nDataset consists a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n-  Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\n-  Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.\n-  The training and test data sets have 785 columns. The first column consists of the class labels, and represents class of clothing. The rest of the columns contain the pixel-values of the associated image.\n<br>\n<br>\nEach training and test example is assigned to one of the following labels:\n<br>\n-  0 T-shirt\/top\n-  1 Trouser\n-  2 Pullover\n-  3 Dress\n-  4 Coat\n-  5 Sandal\n-  6 Shirt\n-  7 Sneaker\n-  8 Bag\n-  9 Ankle boot","ed0fd719":"<font size=4.5 color=\"blue\">Data Augmentation<\/font>\n<br>\nBy using \"data augmentation\" we can create new data with different orientations. It prevents overfitting.\n<img src=\"https:\/\/developers.google.com\/machine-learning\/practica\/image-classification\/images\/data_augmentation.png\"\/>","4fad4c90":"### Reshape\n<br>\nOur images are 28x28 but to using Keras they have to be 3D matrices. That's why we reshape them as 28x28x1, we'll use 1 channel because our images are gray scaled. (e.g. grayscale images has only one channel, rgb image has three channels)","b7d9127d":"### Evaluate The Model","fe2c33d6":"**For better accuracy you can increase number of epochs or you can change parameters on layers or you can add additional layer to the model.For the moment I won\u2019t do that because fitting process takes a lot of time.**"}}