{"cell_type":{"8b3859f4":"code","c2f32c01":"code","9f4fc120":"code","e21b241f":"code","06312c7f":"code","2f5ea6c4":"code","254c0e97":"code","99d064a3":"code","b29cc07e":"code","90ff0276":"code","5c1a5c44":"code","d35bcdd1":"code","8868d1ec":"code","a3f9ca68":"code","9f46f2fa":"code","f47bf550":"code","ffa6d89d":"code","18579d3f":"code","0b97dd82":"code","d96437d2":"code","1da8aa32":"code","3f350218":"code","418718fd":"code","408eb78e":"code","b4b07c9d":"code","e03d9799":"code","12e4d512":"code","6dcab995":"code","cf39f08c":"code","688f11cb":"code","37d4c2dd":"code","51f6a402":"markdown","9ec98dc9":"markdown","dc0d1720":"markdown","cdff4f16":"markdown","6b5dfbc7":"markdown","af802101":"markdown","7911d970":"markdown","b38e5398":"markdown","ae6bb190":"markdown","0cf45390":"markdown","3a5e9562":"markdown","44fd6760":"markdown","62fdbd51":"markdown","c4ebcd4a":"markdown","42590c2e":"markdown","b426931a":"markdown","6470deca":"markdown","585085ba":"markdown","79648a2f":"markdown","096f7637":"markdown","87027f04":"markdown","dc7bdd8a":"markdown","408f4173":"markdown"},"source":{"8b3859f4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, fbeta_score\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport random","c2f32c01":"names = ['age', 'year_operation', 'axillary_nodes', 'survival']\ndf = pd.read_csv(\"\/kaggle\/input\/habermans-survival-data-set\/haberman.csv\", names=names)","9f4fc120":"print(df.shape)\n\ndf.head()","e21b241f":"df.info()","06312c7f":"df.describe()","2f5ea6c4":"sns.pairplot(df, hue=\"survival\")","254c0e97":"scaler = MinMaxScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(df.iloc[:,:-1]))\nX_train.columns = df.iloc[:,:-1].columns\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\nsns.boxplot(data=X_train)","99d064a3":"df.hist()","b29cc07e":"pd.DataFrame({'survived_qty': df.survival.value_counts(), 'survived_pct': round(df.survival.value_counts()\/306,3)})","90ff0276":"# split into input and target\nX, y = df.values[:, :-1], df.values[:, -1]","5c1a5c44":"# ensure all data are floating point\nX = X.astype('float32')\n\n# label encode strings to 0\/1\ny = LabelEncoder().fit_transform(y)","d35bcdd1":"# split into train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=3)\n\n# set number of input features\nn_features = X_train.shape[1]","8868d1ec":"def create_model():\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_features,)))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='adam', \n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model","a3f9ca68":"model = create_model()\n\nhistory = model.fit(X_train, \n                    y_train, \n                    epochs=200, \n                    batch_size=16, \n                    verbose=0, \n                    validation_data=(X_test,y_test))","9f46f2fa":"losses = pd.DataFrame(model.history.history)","f47bf550":"losses[['loss','val_loss']].plot()","ffa6d89d":"losses[['accuracy','val_accuracy']].plot()","18579d3f":"model.evaluate(X_test, y_test)","0b97dd82":"predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n# classification_report\nprint(classification_report(y_test, predictions))\n\n# confusion matrix\npd.DataFrame(confusion_matrix(y_test, predictions))","d96437d2":"# set 10-fold pertinent objects\nkfold = StratifiedKFold(10)\nscores = list()\nn_features = X.shape[1]\n\n# perform kfold\nfor fold, (train_K, test_K) in enumerate(kfold.split(X, y)):\n    # split data\n    X_train, X_test, y_train, y_test = X[train_K], X[test_K], y[train_K], y[test_K]\n\n    # define model (same as before)\n    model = create_model()\n\n    # fit model\n    model.fit(X_train, y_train, epochs=200, batch_size=16, verbose=0)\n\n    # predict in test set\n    predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n    # evaluate predictions\n    score = accuracy_score(y_test, predictions)\n\n    print(f\"fold {fold+1}, score: {round(score,2)}\")\n    scores.append(score)\n    # summarize all scores\nprint(f\"Mean Accuracy: {round(np.mean(scores),2)} ({round(np.std(scores),2)})\")","1da8aa32":"model = KerasClassifier(build_fn=create_model)","3f350218":"# set metric scores to monitor in gridsearch (need to use scikit-learn make_scorer function)\n\nmy_scores = {'accuracy' :make_scorer(accuracy_score),\n             'recall'   :make_scorer(recall_score),\n             'precision':make_scorer(precision_score),\n             'f1'       :make_scorer(fbeta_score, beta = 1)}","418718fd":"# set dict with parameter to grid\nparam_grid = dict(epochs=[100,150,200,250], batch_size=[8,16,32,64])\n\ngrid = GridSearchCV(estimator=model,\n                    param_grid=param_grid,\n                    n_jobs=-1,\n                    refit='accuracy',\n                    scoring = my_scores,\n                    cv=3,\n                    verbose=0)\n\ngrid_result = grid.fit(X, y)","408eb78e":"print(grid_result.best_score_)\nprint(grid_result.best_params_)","b4b07c9d":"pd.DataFrame(grid_result.cv_results_).columns.to_list()","e03d9799":"# setting a dataframe with top 5 results from gridsearchCV\n\npd.DataFrame(grid_result.cv_results_)[['params',\n                                       'mean_test_accuracy',\n                                       'mean_test_recall',\n                                       'mean_test_precision',\n                                       'mean_test_f1',\n                                       'rank_test_accuracy']].sort_values('rank_test_accuracy').head(5)","12e4d512":"X, y = df.values[:, :-1], df.values[:, -1]\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y)\nn_features = X_train.shape[1]","6dcab995":"model = create_model()","cf39f08c":"BS = grid_result.best_params_['batch_size']\nep = grid_result.best_params_['epochs']\n\nmodel.fit(X,y, epochs=ep, batch_size=BS, verbose=0)","688f11cb":"random_ind = random.randint(0,len(X))\n\nnew_data = X[random_ind]\nexp_out = y[random_ind]\npred = (model.predict(new_data.reshape(1,3)) > 0.5).astype('int32')[0][0]","37d4c2dd":"print(f\"\\n new_data: {new_data}\\n \\n expected output: {exp_out} \\n \\n predicted output: {pred}\\n\")","51f6a402":"# <a id=\"grid\"> Grid Search CV <\/a>","9ec98dc9":"Now, we are in a position to apply the trained model to make predictions on new data.\n\nTo simulate a new data input, let's choose an aleatory sample from our dataset:","dc0d1720":"### <a id=\"cross\">Cross validation<\/a>","cdff4f16":"_age_ seems to have a gaussian like distribution, and _auxiliary nodes_ looks more like an exponential one.\n\nAlso, our target feature - _survival_ - is clearly imbalanced: there is much more samples from class 1 than class 2. Let's check how imbalanced it is.","6b5dfbc7":"There are indeed only 3 predictive features and 306 sample rows. Probably, we can deal with this problem using a small network, with a small batch size and using some regularization strategy, in order to avoid overfitting.","af802101":"Let's build a simple MLP, just to have a feeling on the behaviour of such a model applied to our problem, with architecture and parameters chosen arbitrarily.","7911d970":"Our mean accuracy, according to the applied cross validation strategy, is now about ~77%.\n\nA deeper discussion with domain experts should provide a guidance on tha acceptance of this performance level. Assuming that this result is acceptable, the next step is to train our verified model on full dataset, using the optimized parameters found ('batch_size': 8, 'epochs': 250).","b38e5398":"# <a id=\"intro\"> Introduction <\/a>","ae6bb190":"Let's apply a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html\" target='_blank'>stratified k-fold cross-validation<\/a>, to have a more reliable estimate of our model performance. i.e. we are going to fit k models to our data and check mean accuracy.","0cf45390":"# <a id=\"load\">Loading data<\/a>","3a5e9562":"### <a id=\"first\"> First experiment <\/a>","44fd6760":"# <a id=\"final\">Final training<\/a>","62fdbd51":"This simple test, without any tunning, achieved 77% accuracy on test set and an acceptable learning curve. As the accuracy obtained is above the percentage of 73% (class 1 full dataset share), there is an indicative that our approach is promising at solving this task.","c4ebcd4a":"# <a id=\"exp\"> Exploring Data <\/a>","42590c2e":"the best parameters and score (based on refit argument) can now be accessed:","b426931a":"And in a more detailed analysis:","6470deca":"In order to optimize the hyperparameters of our model, let's use grid search capability from scikit-learn library (<a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\" target=\"_blank\">GridSearchCV class<\/a>) to tune Keras deep learning models.\n\nKeras models can be used along scikit-learn by wrapping them with the <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/wrappers\/scikit_learn\/KerasClassifier\" target=\"_blank\">KerasClassifier<\/a>. We just have to define a Keras model function and pass it to the build_fn KerasClassifier argument. Let's go and build the framework to optimize epochs and batch_size:","585085ba":"_Dataset information:_ https:\/\/archive.ics.uci.edu\/ml\/datasets\/haberman's+survival\n\nThe dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\n\n\n_Attribute Information:_\n\n1. Age of patient at time of operation (numerical)\n2. Patient's year of operation (year - 1900, numerical)\n3. Number of positive axillary nodes detected (numerical)\n4. Survival status (class attribute)\n   * 1 = the patient survived 5 years or longer\n   * 2 = the patient died within 5 year\n   \nOur task here will be to build a model to predict if a patient will survive to breast cancer surgery, given these 3 available features.\n\nIMPORTANT NOTE: By any chance I am trying to solve a \"real world\" breast cancer survival prediction, and this notebook should not be used for any medical application. This is just a brief classification problem exploration.","79648a2f":"1. <a href=\"#intro\"> Introduction <\/a>\n2. <a href=\"#load\"> Loading data <\/a>\n3. <a href=\"#exp\"> Exploring Data <\/a>\n4. <a href=\"#prep\"> Data preparation <\/a>\n5. <a href=\"#model\"> Build Model <\/a>\n    * <a href=\"#first\"> First experiment <\/a>\n    * <a href=\"#cross\"> Cross validation <\/a>\n    * <a href=\"#grid\"> Grid Search Cross Validation<\/a>\n    * <a href=\"#final\"> Final training <\/a>\n6. <a href=\"#eval\"> Evaluate Model <\/a>","096f7637":"# <a id=\"model\"> Build Model <\/a>","87027f04":"# <a id=\"prep\">Data Preparation<\/a>","dc7bdd8a":"# <a id=\"eval\"> Evaluate Model <\/a>","408f4173":"~73% percent (225 samples) are from class 1, i.e. patients that survived 5 years or longer."}}