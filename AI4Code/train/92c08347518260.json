{"cell_type":{"e415556d":"code","3109b72a":"code","8a41f0d3":"code","0c23cfc1":"code","d3218561":"code","67184387":"code","564f3307":"code","73767c86":"code","60fcbebd":"code","3014b5fe":"code","367a0c5b":"code","a1bea763":"code","0a0fb37e":"code","22c8c5bf":"code","875ec84c":"code","d479df92":"code","1f8ea4a0":"code","83d262f9":"code","c02e0daa":"code","a9967e2e":"code","0ca48aec":"code","659e137b":"code","1e8ed320":"code","c4a24b6a":"markdown","1bddaa97":"markdown","efa264bd":"markdown","b5518c5c":"markdown","66880edf":"markdown","cab599c4":"markdown","f61a96d9":"markdown","fdbc848f":"markdown","f91c37ee":"markdown"},"source":{"e415556d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","3109b72a":"import keras.backend as K","8a41f0d3":"def quadratic_kappa_coefficient(y_true, y_pred):\n    y_true = K.cast(y_true, \"float32\")\n    n_classes = K.cast(y_pred.shape[-1], \"float32\")\n    weights = K.arange(0, n_classes, dtype=\"float32\") \/ (n_classes - 1)\n    weights = (weights - K.expand_dims(weights, -1)) ** 2\n\n    hist_true = K.sum(y_true, axis=0)\n    hist_pred = K.sum(y_pred, axis=0)\n\n    E = K.expand_dims(hist_true, axis=-1) * hist_pred\n    E = E \/ K.sum(E, keepdims=False)\n\n    O = K.transpose(K.transpose(y_true) @ y_pred)  # confusion matrix\n    O = O \/ K.sum(O)\n\n    num = weights * O\n    den = weights * E\n\n    QWK = (1 - K.sum(num) \/ K.sum(den))\n    return QWK\n\ndef quadratic_kappa_loss(scale=2.0):\n    def _quadratic_kappa_loss(y_true, y_pred):\n        QWK = quadratic_kappa_coefficient(y_true, y_pred)\n        loss = -K.log(K.sigmoid(scale * QWK))\n        return loss\n        \n    return _quadratic_kappa_loss","0c23cfc1":"from keras.applications.vgg16 import VGG16\nfrom keras import models, Model\nfrom keras.layers import Input,Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import Adam\nfrom keras.losses import categorical_crossentropy","d3218561":"input_shape = (256, 256, 3)\n\nbase_net = VGG16(weights='..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_shape=input_shape)\nfor layer in base_net.layers:\n    layer.trainable = False","67184387":"model = models.Sequential()\nmodel.add(base_net)\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(6, activation = \"softmax\"))\nmodel.summary()","564f3307":"model = Model(inputs = model.input, outputs = model.output)","73767c86":"#loss = categorical_crossentropy,\nmodel.compile(optimizer = Adam(lr=1e-3), loss = quadratic_kappa_loss(scale=6.0), \\\n             metrics = ['accuracy',quadratic_kappa_coefficient])","60fcbebd":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport skimage.io\nimport cv2\n\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom matplotlib.pyplot import imshow","3014b5fe":"HOME = Path(\"..\/input\/prostate-cancer-grade-assessment\")\nTRAIN = Path(\"train_images\")","367a0c5b":"train_ann = pd.read_csv(HOME\/'train.csv')\ntrain_ann['image_path'] = [str(HOME\/TRAIN\/image_name) + \".tiff\" \\\n                           for image_name in train_ann['image_id']]\ntrain_ann.head()","a1bea763":"enc = OneHotEncoder(handle_unknown = 'ignore')","0a0fb37e":"enc_labels = pd.DataFrame(enc.fit_transform(train_ann[['isup_grade']]).toarray())\n\ntrain_ann = pd.merge(train_ann, enc_labels, left_index=True, right_index=True)\ntrain_ann.head(8)","22c8c5bf":"# Function to get one image\n\ndef get_image(image_location):\n    image = skimage.io.MultiImage(image_location)\n    # take the smallest image size\n    image = image[-1]\n    # resize the image to the desired size\n    image = cv2.resize(image, (input_shape[0], input_shape[1]))\n    \n    return image","875ec84c":"# Function that shuffles annotation rows and chooses batch_size samples\n#sequence = range(len(annotation_file))\n\ndef get_batch_ids(sequence, batch_size):\n    sequence = list(sequence)\n    random.shuffle(sequence)\n    batch = random.sample(sequence, batch_size)\n    return batch","d479df92":"# Basic data generator -> Next: add augmentation = False\n\ndef data_generator(data, batch_size):\n    while True:\n        data = data.reset_index(drop=True)\n        indices = list(data.index)\n\n        batch_ids = get_batch_ids(indices, batch_size)\n        batch = data.iloc[batch_ids]['image_path']\n\n        X = [get_image(x) for x in batch]\n        Y = data[[0, 1, 2, 3, 4, 5]].values[batch_ids]\n\n        # Convert X and Y to arrays\n        X = np.array(X)\n        Y = np.array(Y)\n\n        yield X, Y\n\n# data: should be a pandas DF (train or val) obtained from train_test_split\n# batch_size: is the size of the number of images passed through the net in one step","1f8ea4a0":"# Train -  Validation Split function\ntrain, val = train_test_split(train_ann, \\\n                              test_size = 0.1, \\\n                              random_state = 42)","83d262f9":"# Some checkpoints\nmodel_checkpoint = ModelCheckpoint('.\/model_01.h5', monitor = 'val_loss', verbose=0, save_best_only=True, save_weights_only=True)\nearly_stop = EarlyStopping(monitor='val_loss',patience=5,verbose=True)","c02e0daa":"EPOCHS = 30 \nBS = 100\n\nhistory = model.fit_generator(generator = data_generator(train, BS),\n                              validation_data = data_generator(val, BS),\n                              epochs = EPOCHS,\n                              verbose = 1,\n                              #steps_per_epoch = len(train)\/\/ BS,\\\n                              steps_per_epoch = 20,\n                              validation_steps = 20, \n                              #validation_steps = len(val)\/\/ BS,\\\n                              callbacks =[model_checkpoint, early_stop])","a9967e2e":"import matplotlib.pyplot as plt","0ca48aec":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","659e137b":"initial_sample_submission = pd.read_csv('..\/input\/prostate-cancer-grade-assessment\/sample_submission.csv')\nTEST = Path(\"test_images\")\ntest_ann = pd.read_csv(HOME\/'test.csv')","1e8ed320":"if os.path.exists(f'..\/input\/prostate-cancer-grade-assessment\/test_images\/'):\n    print('inference!')\n\n    predictions = []\n    for img_id in test_ann['image_id']:\n        img = str(HOME\/TEST\/img_id) + \".tiff\"\n        print(img)\n        image = get_image(img)\n        image = image[np.newaxis,:]\n        prediction = model.predict(image)\n        # if we have 1 at multiple locations\n        ind = np.where(prediction == np.amax(prediction))\n        final_prediction = random.sample(list(ind[1]), 1)[0].astype(int)\n        predictions.append(final_prediction)\n\n    sample_submission = pd.DataFrame()\n    sample_submission['image_id'] = test_ann['image_id']\n    sample_submission['isup_grade'] = predictions\n    sample_submission\n\n    sample_submission.to_csv('submission.csv', index=False)\n    sample_submission.head()\nelse:\n    print('Test Images folder does not exist! Save the sample_submission.csv!')\n    initial_sample_submission.to_csv('submission.csv', index=False)","c4a24b6a":"### Split the data set in train and validation","1bddaa97":"### Fit the model (Train)","efa264bd":"### Data Encoder for the labels\n\n... as we need them to be represented as dummy variables. Each response will be an array of length 6. Eg. class 3 will be represented as [0,0,0,1,0,0]","b5518c5c":"## Predict on the Test Data \n- sample submission","66880edf":"### Loss Function - Quadratic Weighted Kappa\n","cab599c4":"We choose a softmax activation on the last layer because our classes are mutually exclusive. We want the algorithm to choose only one class, the one with the highest probability, therefore the probabilities must sum up to 1. \n\n(in contrast, the sigmoid activation function will output independent probabilities and can be used when Eg. a patient might have multiple diseases - the output might be multiple classes)","f61a96d9":"### Data Generator\n\n- First of all, take either train of val pandaDF. \n- Shuffle the rows and randomly select a number, equal to your batch size.\n- Read the selected images (from the path column) and resize them in get_image()\n- Output the data array as well as the labels corresponding to that data.","fdbc848f":"### Create a data generator\n\n1. Create a DF which contains the image path + the label of that image. We will not use the masks at all at this stage.\n2. Use the 3rd version o the data (smallest size array) to speed up the process\n3. Create a labels array (Y) and a data array (X)\n4. Split the data in train & validation (use validation to also test at this stage)","f91c37ee":"### Build the model\n\nPerform transfer learning from VGG16 with imagenet weights. \nIn order to load the weights and use it without activating the internet, in the notebook, go to \"Data\" -> \"Add data\" and search \"keras\" then select \"Keras Pretrained Models\".\n\nFreeze the conv layers and only train the top layers."}}