{"cell_type":{"4dbcf175":"code","218dea74":"code","9a322d54":"code","ec6053ae":"code","a8df9b73":"code","8bf606eb":"code","db48540a":"code","f4c61e6e":"code","42632166":"code","9d1dd682":"code","a980f128":"code","08f8416d":"code","13b9261e":"code","c48fcc16":"markdown","af069ea0":"markdown","8f3d361e":"markdown","6ccdeac9":"markdown","cac8b687":"markdown","9493641e":"markdown","74d5e07a":"markdown","75bc3d96":"markdown","0ab71019":"markdown","5a63c537":"markdown"},"source":{"4dbcf175":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","218dea74":"custs = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_customers_dataset.csv')\nsellers = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv')\nreviews = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv')\nprod_order = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv')\nprod = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_products_dataset.csv')\nlocations = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv')\norders = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_orders_dataset.csv')\ncats = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/product_category_name_translation.csv')","9a322d54":"### Step 1: Join Orders on customers\nnew_orders = orders.merge(custs,on='customer_id',how='left')\n\n### Step 2: Select only the columns you want\nnew_orders = new_orders[['order_id','customer_unique_id','order_purchase_timestamp','customer_state']]\n\n### Step 3: Get Year month\nnew_orders['order_purchase_timestamp'] = pd.to_datetime(new_orders['order_purchase_timestamp'],format='%Y-%m-%d %H:%M:%S')\nnew_orders['yearmonth'] = new_orders['order_purchase_timestamp'].dt.year*100 + new_orders['order_purchase_timestamp'].dt.month\n\n### Step 4: Check for NA values or duplicate values\nif not len(new_orders)-len(new_orders.drop_duplicates()):\n    print('No Duplicates')\nelse:\n    print('Duplicates Exist')\nnew_orders.info()","ec6053ae":"### Step 1: Join table\nnew_po = prod_order.merge(new_orders,on='order_id',how='left')\nnew_po = new_po.merge(prod,on='product_id',how='left')\n\n### Step 2: Select Columns we want and drop duplicates\nnew_po = new_po[['order_id','product_id','price','customer_unique_id','order_purchase_timestamp','yearmonth',\n                 'customer_state','product_category_name','order_item_id']]\n\n### Replace Portuguese (I hope) category name with English one and rename column for convenience\nnew_po = new_po.set_index('product_category_name').join(cats.set_index('product_category_name')).reset_index()\nnew_po = new_po.drop(columns=['product_category_name'])\nnew_po = new_po.rename(columns={'product_category_name_english':'category_name'})\n\n### Step 3: Check for NA values or duplicate values\nif not len(new_po)-len(new_po.drop_duplicates()):\n    print('No Duplicates')\nelse:\n    print('Duplicates Exist')\nnew_po.info()","a8df9b73":"### We see a few cases of products with unknown category, lets fill in the category name as 'unknown'\nnew_po['category_name'] = new_po['category_name'].fillna('unknown')\n\n### Now we have all the data we need to create our required table\nstate = new_po.groupby(['customer_state'])['price','order_id','customer_unique_id']\\\n.agg({'price':'sum','order_id':'nunique','customer_unique_id':'nunique'}).reset_index()\n\ncategory = new_po.groupby(['category_name'])['price','order_id','customer_unique_id']\\\n.agg({'price':'sum','order_id':'nunique','customer_unique_id':'nunique'}).reset_index()\n\nyearmonth = new_po.groupby(['yearmonth'])['price','order_id','customer_unique_id']\\\n.agg({'price':'sum','order_id':'nunique','customer_unique_id':'nunique'}).reset_index()","8bf606eb":"fig, (ax1,ax2,ax3) = plt.subplots(3,figsize=(15,15))\nfig.suptitle('Yearmonth Performance')\nx = yearmonth['yearmonth']\nax1.plot(x, yearmonth['price'],label='Sales')\nax1.set_title('Sales')\nax2.plot(x, yearmonth['order_id'],label='#Orders')\nax2.set_title('#Orders')\nax3.plot(x,yearmonth['customer_unique_id'],label='#Customers')\nax3.set_title('#Customers')","db48540a":"### avg_ts stands for average ticket size\nyearmonth['avg_ts'] = yearmonth['price']\/yearmonth['order_id']\n\nyearmonth['freq'] = yearmonth['order_id']\/yearmonth['customer_unique_id']\n\nprint('The average ticket size per order is: {}\\nwith a standard deviation of: {}\\n'\\\n      .format(round(yearmonth['avg_ts'].mean(),2),round(yearmonth['avg_ts'].std(),2)))\n\nprint('The average frequency per month is: {}\\nwith a standard deviation of: {}\\n'\\\n      .format(round(yearmonth['freq'].mean(),2),round(yearmonth['freq'].std(),2)))","f4c61e6e":"category['contribution'] = category['price']\/category['price'].sum()\n\n### I'll use 4% contribution as the threshold, this is an arbitary number\n### It will result in 9 categories + miscellaneous category we will have a total of 10 categories\n\ncategory['category'] = category.apply(lambda x: x['category_name'] if x['contribution']>=0.04 else 'miscellaneous',axis=1)\n\nnew_cat = category.groupby('category')['price','order_id','customer_unique_id'].sum().reset_index()","42632166":"fig, (ax1,ax2,ax3) = plt.subplots(3,figsize=(20,20))\nfig.suptitle('Category Performance')\nx = new_cat['category']\nax1.pie(new_cat['price'],labels=x,autopct='%1.2f%%')\nax1.set_title('Sales')\nax2.pie(new_cat['order_id'],labels=x,autopct='%1.2f%%')\nax2.set_title('#Orders')\nax3.pie(new_cat['customer_unique_id'],labels=x,autopct='%1.2f%%')\nax3.set_title('#Customers')","9d1dd682":"new_cat['price_rank'] = new_cat['price'].rank(ascending=False)\nnew_cat['order_rank'] = new_cat['order_id'].rank(ascending=False)\nnew_cat['customer_rank'] = new_cat['customer_unique_id'].rank(ascending=False)\n\nnew_cat","a980f128":"state['contribution'] = state['price']\/state['price'].sum()\n\nstate['state'] = state.apply(lambda x: x['customer_state'] if \\\n                             x['contribution'] >= state['contribution'].quantile(0.7)\\\n                             else 'SS',axis=1)\n\nnew_state = state.groupby('state')['price','order_id','customer_unique_id'].sum().reset_index()","08f8416d":"fig, (ax1,ax2,ax3) = plt.subplots(3,figsize=(20,20))\nfig.suptitle('State Performance')\nx = new_state['state']\nax1.pie(new_state['price'],labels=x,autopct='%1.2f%%')\nax1.set_title('Sales')\nax2.pie(new_state['order_id'],labels=x,autopct='%1.2f%%')\nax2.set_title('#Orders')\nax3.pie(new_state['customer_unique_id'],labels=x,autopct='%1.2f%%')\nax3.set_title('#Customers')","13b9261e":"new_state['price_rank'] = new_state['price'].rank(ascending=False)\nnew_state['order_rank'] = new_state['order_id'].rank(ascending=False)\nnew_state['customer_rank'] = new_state['customer_unique_id'].rank(ascending=False)\n\nnew_state","c48fcc16":"#### Category Performance\n\nNext I'd like to see how each category contributes to the sales of the company, unfortunately we have too many categories so any chart we create will be too cluttered, I'll start by combining the categories that dont contribute a lot to the sales as _miscellaneous_ ","af069ea0":"### State performance","8f3d361e":"## Performance\n\n#### Yearmonth Performance","6ccdeac9":"As observed, our orders table dont have duplicate or null values, our next step will be to join the orders table with the product orders and then join the prod orders with the products table on product id.","cac8b687":"We'll follow the same logic we did, only selecting major states and renaming all others to SS which stands for small states","9493641e":"## Exploratory Data Analysis:\n\nIn this exercise I'd like to evaluate the performance of products and categories across different areas and year month combinations in Sales Value, #Unique Customers, #Unique Orders. To do this our table will need to have the following columns:\n\n1. Product ID\n2. Order ID\n3. Customer ID\n4. Geolocation State\n5. Category\n6. Purchase Date\n7. Sales Value\n\nOur main table here will be prod_orders, this is the table that contains records of product sales order, but our first step will be on the orders table, we will join the orders table with the customers (custs) table on customer_id, then we will keep the columns we want, which will be order_id, order_purchase_timestamp, customer_unique_id, customer_state. Finally we will join the product order table (prod_orders) with the new orders table on order_id","74d5e07a":"# Part 1\n\nIn this part I'll just do an exploratory EDA across 3 different measures, first the performance of each month, then the performance of the categories and finally the performance of the states.","75bc3d96":"Our top in everything here is the smaller categories, combined they make about 41% of the sales,orders and customers.\nThe rest of the pie is divided into the top 9 categories, I'd say our 2nd best category here is the _health_beauty_ as it has the 2nd most sales and 3rd best #Orders and #Customers, after that our 3rd best would be _bed_bath_table_ since it has 4th best sales and 2nd best #Orders and #Customers. A cool observation here is that while _watches_gifts_ are low in the order and customer rank, they are 3rd best in sales value.","0ab71019":"Our best performing state here is _SP_, it's the top across all the metrics contributing around 42% in the #Orders and #Customers and 38% of the sales. The 2nd spot goes to our customer made state _SS_ which is basically a combination of all the small states, they have contributed to about 16% of the sales and our 3rd best is _RJ_, contributing to around 13% of the total sales of the company. Unlike the category analysis we dont observe states with different ranks across the metrics but we do see some variation between the order and the sales contribution in some states.\n\n# End of Part 1\n-----\n---------\n\nThis brings us to the end of this section, we can go deeper into each measure and we can combine them to get more insights, we can check the performance of individual categories\/states month over month and check for abnormal change (for example category grew when overall company performance was dropping or vice versa), we can check which categories have the best performance in terms of not just sale value but order value, meaning users who buy the category buy more items along with said category items. We can investigate more metrics, for example we can group products based on volume and check the sales of products based on volumes, for a large dataset like this there's a lot of analysis to be done and insights to be gathered.\n\nI'm not sure what I'll do in part 2, I'm thinking predicting weekly category sales since I want to build a model but nothing is set in stone yet. \n\n**Thank you reading this far and I appreciate constructive feedback.**","5a63c537":"Looking at the graphs above we observe that all 3 are very similar, an insight we can gather from this is that on average customers will make 1 order per month and that the average order value has been somewhat consistent. Lets verify below by looking at the mean order value and avg number of orders per customer every month"}}