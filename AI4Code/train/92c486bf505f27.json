{"cell_type":{"c30102f8":"code","65bc81ca":"code","f475f9c3":"code","174115b1":"code","10fb8254":"code","c189f45d":"markdown","248fd20e":"markdown","32a3cf69":"markdown","c828dec3":"markdown","45defd78":"markdown","ac18fa12":"markdown","adae0f65":"markdown","c0e3b176":"markdown"},"source":{"c30102f8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n%matplotlib inline\nsns.set(font_scale=1.3)","65bc81ca":"# Generate data\nn = 100_000\nt = np.linspace(-2 * np.pi, 2 * np.pi, int(np.sqrt(n)))\nx, z = np.meshgrid(t, t)\ny = np.sin(x**2 + z**2) \/ (x**2 + z**2) + np.cos(x) * 0.5 - 0.5\n\n# Turn to DataFrame\ndata = pd.DataFrame({\n    'x': x.flatten(), \n    'z': z.flatten(),\n    'y': y.flatten()\n})\n\nfig, ax = plt.subplots(1, 2, figsize=(11, 5))\n\n# Response distribution\nsns.distplot(data.y, ax=ax[0])\nax[0].set(title=\"Distribution of y\")\n\n# Response profile\ndef nice_heatmap(data, v, ax):\n    sns.heatmap(data.pivot('z', 'x', v), \n                xticklabels=False, yticklabels=False, \n                cmap='coolwarm', vmin=-1, vmax=1, ax=ax)\n    ax.set_title(\"Heatmap of \" + v)\n    return None\n\nnice_heatmap(data, v='y', ax=ax[1])\nfig.tight_layout()","f475f9c3":"X_train, X_test, y_train, y_test = train_test_split(\n    data[[\"x\", \"z\"]], data[\"y\"], \n    test_size=0.33, random_state=63\n)\n\nprint(\"All shapes:\")\nfor dat in (X_train, X_test, y_train, y_test):\n    print(dat.shape)","174115b1":"# Parameters\nparams = {\n    'objective': 'regression',\n    'num_leaves': 63,\n    'metric': 'l2_root',\n    'learning_rate': 0.3,\n    'bagging_fraction': 1,\n    'min_sum_hessian_in_leaf': 0.01\n}\n\n# Data interface\nlgb_train = lgb.Dataset(X_train, label=y_train)\n                 \n# Fitting the model\nif False:\n    # Find good parameter set by cross-validation\n    gbm = lgb.cv(params,\n                 lgb_train,\n                 num_boost_round=20000,\n                 early_stopping_rounds=1000,\n                 stratified=False,\n                 nfold=5,\n                 verbose_eval=1000, \n                 show_stdv=False)\nelse: \n    # Fit with parameters\n    gbm = lgb.train(params,\n                    lgb_train,\n                    num_boost_round=5000)\n","10fb8254":"# Add predictions to test data\ndata_eval = pd.DataFrame(np.c_[X_test, y_test], columns=['x', 'z', 'y'])\ndata_eval[\"predictions\"] = gbm.predict(X_test)\ndata_eval[\"residuals\"] = data_eval[\"y\"] - data_eval[\"predictions\"]\n\n# Plot the results\nfig, ax = plt.subplots(1, 3, figsize=(21, 6))\nfor i, v in enumerate(['y', 'predictions', 'residuals']):\n    nice_heatmap(data_eval, v=v, ax=ax[i])\nfig.tight_layout()","c189f45d":"## Wrap up\n\nCongratulation to the gradient boosted tree! The approximation error is extremely small. Imagine how long you would have had to generate relevant features for a linear model...","248fd20e":"## Imports and settings","32a3cf69":"# The Unreasonable Effectiveness of Tree Boosting\n\nThis notebook illustrates how gradient boosting can learn even the most complex statistical relationships, at least if you feed enough data to the beast!","c828dec3":"The white scatter in the image are due to plotting only the 33% hold-out sample.","45defd78":"## Evaluation\n\nAfter fitting the models, we are ready to apply the model on the factory fresh hold-out data set. Was gradient boosting able to well approximate our crazy function? To do so, we look at heatmaps of\n\n- the response y (i.e. the ground truth)\n\n- the predictions, as well as\n\n- the out-of-sample residuals.","ac18fa12":"## Generate data\n\nWe first sample 100'000 data points from the following two-dimensional function:\n$$\n    y = f(x, z) = \\frac{\\sin(x^2 + z^2)}{x^2 + z^2} + 0.5 \\cos(x) - 0.5,\n$$\n$x, z \\in [-2\\pi, 2\\pi]$.\n\nIts profile looks as follows.","adae0f65":"## Train\/Test split\n\nIn order to not fall into the trap of overfitting, we set aside 33% of the data lines for testing only. The model is trained on the remaining 67%.","c0e3b176":"## Modelling\n\nNow we fit a gradient boosting tree with [LightGBM](https:\/\/github.com\/microsoft\/LightGBM), besides [XGBoost](https:\/\/github.com\/dmlc\/xgboost) and [CatBoost](https:\/\/github.com\/catboost\/catboost) one of the tree major implementations of gradient boosting. \n\nThe parameters have been manually chosen by five-fold cross-validation on the training data in order to minimize (root-)mean squared error."}}