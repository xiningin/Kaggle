{"cell_type":{"088e7068":"code","5934cd8a":"code","7ac83596":"code","6221df94":"code","2c9a7d65":"code","f076b0d1":"code","f8f213c0":"code","e34347c6":"code","cd3b8519":"code","c0f2cd2a":"code","5dd8b244":"code","3276a575":"code","729bd2fd":"code","11cf3ef9":"code","13d78e27":"code","e33bbeb2":"code","d6004a4c":"code","ef92b6ed":"code","b7e0a467":"code","6e3eff15":"code","620a5b0c":"code","ed8e6c95":"code","f0e5f094":"code","fdd11e3c":"code","2803461c":"code","2c003e8b":"code","63358df1":"code","ef4906c4":"code","3f645686":"code","dec3e287":"code","9aa7e76b":"code","1c5643f3":"code","10ba24d2":"code","29f486c9":"code","747d2395":"code","a49f03ef":"code","3c39603c":"code","c068fd1e":"markdown","201c722a":"markdown","d56e372b":"markdown","a4e00810":"markdown","f9690c18":"markdown","32ab27e4":"markdown","1cd32e81":"markdown","e1dedb69":"markdown","69d5e218":"markdown","d14a38e7":"markdown","a702d3ae":"markdown","07778505":"markdown","c1ade241":"markdown","97749d91":"markdown","68e21439":"markdown","2f1c08d6":"markdown","13dd4db9":"markdown","3667ac66":"markdown","4e41cc8b":"markdown","6c7eee51":"markdown","ab143d8c":"markdown","f904af9d":"markdown","d6f102d2":"markdown","7b47f2f8":"markdown","e5ee2a63":"markdown","46cea510":"markdown","9abae7b2":"markdown","b4ce73af":"markdown","79390b56":"markdown","b3601a3d":"markdown","0dba8eda":"markdown","1dcabb05":"markdown","a0412afb":"markdown","39d9d5ad":"markdown","1b75be24":"markdown","f159432b":"markdown","7087c404":"markdown","6cfe488b":"markdown","cf5feded":"markdown","fcaecece":"markdown"},"source":{"088e7068":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5934cd8a":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","7ac83596":"df_train.head()","6221df94":"df_test.head()","2c9a7d65":"round((df_train.isna().sum()\/len(df_train)*100), 1)","f076b0d1":"round((df_test.isna().sum()\/len(df_test)*100), 1)","f8f213c0":"df_train.info()","e34347c6":"df_test.info()","cd3b8519":"plt.figure()\nsns.barplot(x='Sex', y='Survived', hue='Pclass', data=df_train)\nplt.show()","c0f2cd2a":"plt.figure()\nsns.barplot(x='Pclass', y='Survived', data=df_train)\nplt.show()","5dd8b244":"plt.figure()\nsns.violinplot(x='Survived', y='Age', hue='Sex', data=df_train)\nplt.show()","3276a575":"ages = (0, 15, 25, 60, 100)\ngrp_names = ['Child', 'Teenager', 'Adult', 'Senior']\nage_grp = pd.cut(df_train['Age'], ages, labels=grp_names)\ndf_train['AgeGrp'] = age_grp","729bd2fd":"plt.figure()\nsns.barplot(x='AgeGrp', y='Survived', hue='Sex', data=df_train)\nplt.show()","11cf3ef9":"df_train.drop(['AgeGrp'], axis=1, inplace=True)","13d78e27":"plt.figure()\nsns.barplot(x='Embarked', y='Survived', data=df_train)\nplt.show()","e33bbeb2":"df_train.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1, inplace=True)\ndf_test.drop(['Name', 'Cabin', 'Ticket'], axis=1, inplace=True)","d6004a4c":"df_train.head()","ef92b6ed":"df_test.head()","b7e0a467":"def replaceAgeValues(df):\n    df_age = df['Age'].copy()\n    df_mean = df_age.mean()\n    df_std = df_age.std()\n    df_isna = df_age.isna().sum()\n    df_rand = np.random.randint(df_mean-df_std, df_mean+df_std, size=df_isna)\n    df_age[np.isnan(df_age)] = df_rand\n    df['Age'] = df_age\n\nreplaceAgeValues(df_train)\nreplaceAgeValues(df_test)","6e3eff15":"df_train['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace=True)","620a5b0c":"df_test['Fare'].fillna(df_test['Fare'].mean(), inplace=True)","ed8e6c95":"df_train.isna().any()","f0e5f094":"df_test.isna().any()","fdd11e3c":"corr_matrix = df_train.corr()\nsns.heatmap(corr_matrix, annot=True)\nplt.show()","2803461c":"def combineFeatures(df):\n    df['FamMbrs'] = df['SibSp'] + df['Parch']\n    df.drop(['SibSp'], axis=1, inplace=True)\n    df.drop(['Parch'], axis=1, inplace=True)\n\ncombineFeatures(df_train)\ncombineFeatures(df_test)","2c003e8b":"df_train.head()","63358df1":"df_train.info()","ef4906c4":"def convertFeatures(df):\n    df['Sex'] = df['Sex'].map({\"male\": 0, \"female\": 1})\n    df['Age'] = df['Age'].astype('int64')\n    df['Fare'] = df['Fare'].astype('int64')\n    df['Embarked'] = df['Embarked'].map({\"S\": 0, \"C\": 1, \"Q\": 2})\n\nconvertFeatures(df_train)\nconvertFeatures(df_test)","3f645686":"df_train.info()","dec3e287":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score","9aa7e76b":"x = df_train.drop(['Survived'], axis=1)\ny = df_train['Survived']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=50)","1c5643f3":"rdf_0 = RandomForestClassifier()\nrdf_0.fit(x_train, y_train)\n\nrdf_0_acc = round(rdf_0.score(x_train, y_train)*100, 2)\n\ny_pred_0 = rdf_0.predict(x_test)\npred_acc_0 = round(accuracy_score(y_test, y_pred_0)*100, 2)\n\nprint(f'Mean accuracy: {rdf_0_acc} %')\nprint(f'Prediction accuracy: {pred_acc_0} %')","10ba24d2":"feat_imp = pd.DataFrame({\n    'features': x_train.columns,\n    'importances': rdf_0.feature_importances_\n})\n\nfeat_imp = feat_imp.sort_values('importances', ascending=False).set_index('features')\nfeat_imp.plot.bar()","29f486c9":"easy_training_params = {\n    'max_features': 'auto',\n    'n_jobs': -1,\n    'oob_score': True,\n    'random_state': 20\n}\n\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [3, 6, 9],\n    'min_samples_leaf': [1, 2, 4],\n    'min_samples_split': [2, 4, 6, 8, 10],\n    'n_estimators': [100, 500, 1000, 1500]\n}\n\nrdf_1 = RandomForestClassifier()\nrdf_1.set_params(**easy_training_params)\n\nrdf_1_grid = GridSearchCV(estimator=rdf_1, param_grid=param_grid, n_jobs=-1)\nrdf_1_grid.fit(x_train, y_train)\nrdf_1_grid.best_params_","747d2395":"easy_training_params.update(rdf_1_grid.best_params_)\n\nrdf_2 = RandomForestClassifier()\nrdf_2.set_params(**easy_training_params)\nrdf_2.fit(x_train, y_train)\n\nrdf_2_acc = round(rdf_2.score(x_train, y_train)*100, 2)\n\ny_pred_2 = rdf_2.predict(x_test)\npred_acc_2 = round(accuracy_score(y_test, y_pred_2)*100, 2)\n\nprint(f'Mean accuracy: {rdf_2_acc} %')\nprint(f'Prediction accuracy: {pred_acc_2} %')","a49f03ef":"scores = cross_val_score(rdf_2, x_train, y_train, cv=10, scoring='accuracy')\n\nprint(f'Scores: {scores}')\nprint(f'Min score: {min(scores)}')\nprint(f'Max score: {max(scores)}')\nprint(f'Accuracy: {round(scores.mean()*100, 2)} %')\nprint(f'Error: {round(scores.std()*100, 2)} %')","3c39603c":"y_pred_final = rdf_2.predict(df_test.drop(['PassengerId'], axis=1))\ny_pred_final = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': y_pred_final})\ny_pred_final.to_csv('submission.csv', index=False)","c068fd1e":"> ### 3. Plotting and first observations","201c722a":"> ### 1. Data visualization","d56e372b":"First of all, we notice that the `Survived` column is missing in the testing set. We also notice that the `Cabin` column has many null values. We can confirm this intuition by displaying for each column the percentage of the missing values.","a4e00810":"Like we said previously, we can remove the `Cabin` column. We can also drop the `Ticket` column because `Fare` and `Pclass` are enough to differentiate passengers. We will do the same for the `PassengerId` and `Name` columns because they are not really pertinent in our case. However, we will keep the passenger's ID in the testing set for the submission file.","f9690c18":"This is even more evident on this graph. Women and children were evacuated first. We can drop the `AgeGrp` column now.","32ab27e4":"**Dealing with** `Age` **:** To make it simple, we will replace missing values by a random value from the mean \u00b1 standard deviation interval. This will be sufficient for our study case.","1cd32e81":"> ### 1. Import basic libraries","e1dedb69":"> ### 3. Dealing with unbalanced features","69d5e218":"We have almost **80%** of missing values for the `Cabin` column in both sets. This column describes the passenger's cabin number. In our case, it is not a very useful information. We can therefore remove this column from both sets in the data cleaning part.","d14a38e7":"In this section, we will convert all our features into numeric (**int64**) in order to use Scikit-Learn models.","a702d3ae":"We notice on this first graph that men had less chances of survival than women : \u00ab Women and children first ! \u00bb. We will confirm the second part of this sentence in the following graph. Ticket class also has a significant impact on survival rate. Only **25%** of the third class passengers survived. They were located in the lower levels of the Titanic. Therefore, they were the first to be flooded. Based on this observation, we can deduce that the ticket price will take an important place in the predictions of our future model.","07778505":"> ### 5. Model validation","c1ade241":"I have intentionally reduced the number of possibilities for each parameter for a faster execution. Indeed, we already have $2*3*3*5*4=360$ combinations of settings !","97749d91":"As we expected, the `Age` and `Sex` columns are the most important features. Our deduction about the importance of the ticket price (`Fare`) was correct as it comes in third place.","68e21439":"> ### 1. Import Scikit-Learn library","2f1c08d6":"> ### 2. Dealing with missing values","13dd4db9":"Age has a strong impact on the survival rate. However, we have to keep in mind in our analysis that there are **177 missing values**. We can create age groups for a better visualization. For this purpose, we will create a new column : `AgeGrp`","3667ac66":"> ### 4. Hyperparameter Tuning","4e41cc8b":"**Dealing with** `Fare` **:** There is only 1 missing value in the testing set. So we can replace it by the mean.","6c7eee51":"## IV- Data conversion","ab143d8c":"> ### 2. Column characteristics","f904af9d":"To deal with missing values we have multiple strategies :\n* drop rows with missing values (data loss)\n* replace missing values with random values\n* replace missing values by 0\n* replace missing values by the mean\n* replace missing values by the mode (most recurrent value) for categorical values\n* ...","d6f102d2":"## V- Model building","7b47f2f8":"## III- Data cleaning","e5ee2a63":"The graph shows that the port of embarkation has only a minor influence on the survival rate.","46cea510":"Last check...","9abae7b2":"# Titanic - Machine Learning from disaster\n\nThe purpose of this notebook is to create a model capable of predicting whether a person could have survived the night of April 15, 1912, during the sinking of the Titanic.\n\n## I- Data extraction","b4ce73af":"> ### 2. Splitting","79390b56":"Everything's good !","b3601a3d":"It is crucial to understand the data we are going to manipulate. So we need to clarify any ambiguity about the dataset. For this purpose, it is interesting to visualize the data.","0dba8eda":"## II- Understanding the data","1dcabb05":"We use 20% of the training set for validation.","a0412afb":"> ### 1. Removal of unnecessary features","39d9d5ad":"As we can see in the correlation matrix, two features are unbalanced :\n* `SibSp` **:** number of siblings \/ spouses aboard the Titanic\n* `Parch` **:** number of parents \/ children aboard the Titanic\n\nThese two columns could be combined into a single one. This column would represent the number of family members aboard the Titanic : `FamMbrs`","1b75be24":"> ### 6. Final prediction","f159432b":"In this section, we will use correlation matrices to identify and modify highly correlated features. The objective is to avoid overfitting. Indeed, two strongly correlated features do not bring more information for the performance of our model.","7087c404":"> ### 3. Fitting","6cfe488b":"> ### 2. Data reading","cf5feded":"**Dealing with** `Embarked` **:** There are 2 missing values for this column. Precedently, we saw that the port of embarkation does not have a significant impact on survival rate. So we can replace missing values by the mode.","fcaecece":"As we can see, the features are not of the same type. To use the Scikit-Learn models, they must be numeric (**int64**). So we will have to convert them later."}}