{"cell_type":{"0347e2a0":"code","fb00bead":"code","887d65ab":"code","eff411e8":"code","d750511f":"code","901242ae":"code","bc2a6377":"code","6436a8d2":"code","9e3c1b4e":"code","692b9eb0":"code","b0f9da16":"code","5204354d":"code","cbdeb9ba":"code","61d74b56":"code","f0c4fb85":"code","e0d0fbeb":"code","3e910c66":"code","def77ee9":"code","8f6a71f7":"markdown","84c8536c":"markdown","ebd150cf":"markdown"},"source":{"0347e2a0":"# =======================================================\n# TPS October 2021 - EDA\n# =======================================================\n# Name: B\u00e1rbara Sulpis\n# Date: 11-oct-2021\n# Description: I will analyze TPS data to have an idea of following steps...\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.stats as st # statistical functions\nimport os\n\nfrom sklearn.model_selection import train_test_split\n\n#Lgbm\nimport lightgbm as lgb\n\n# roc\nimport sklearn.metrics as metrics   # Para la curva ROC\nimport matplotlib.pyplot as plt     # Para la curva ROC\n\n# for hystograms\nimport seaborn as sns\n\n\n# ---------------------------\n# Input data:\n# Go to file -> add or upload data -> \"Competition\" data tab and select the commpetition which you want to add the csv data data \"\n# files are available in the read-only \"..\/input\/\" directory\n# ---------------------------\n\nlist =  os. getcwd()\nprint(list) # shoud be in \"kaggle\" directory\n\n# I left this commented if you want to check that the files are there\n# i = 0\n# for subdir, dirs, files in os.walk('.\/'):\n#     for file in files:\n#         print(file)\n#         i+= 1\n#         if i>20: \n#             break\n\n\ndata = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")        \nsubm = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")  \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb00bead":"# Size of the dataset\ndata.shape","887d65ab":"# With this setting we can see all rows of the dataset\npd.set_option(\"display.max_columns\", 300)\n# We have a look to the data\ndata","eff411e8":"# Before working with the data, we reduce the use of memory, so we can improve performance\n# REFERENCE: https:\/\/www.kaggle.com\/smiles28\/tps10-optuna-xgb-catb-lgbm-stacking\n\n# What the function does is to deduce the data types and cast each column to its most performant type\n\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n    \n            # test if column can be converted to an integer\n            asint = props[col].astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props\n","d750511f":"data = reduce_mem_usage(data)","901242ae":"subm = reduce_mem_usage(subm)","bc2a6377":"# ------------------------------------------------------------\n#   Search for MISSING values\n# ------------------------------------------------------------\n# First we make a dataframe with the number of not-null values for each column\ncount = pd.DataFrame(data.count(), columns=['count'])\n# Then we get the fields that has a number smaller than 1M (the number of rows in train set)\ncount.query(\"count < 1000000\")\n\n# As we can see there are not null values in the dataset. \n# That's good because we don't have to spend time working with missing values","6436a8d2":"# We can make the same check for the submission dataset (\"test.csv dataset\")\ncount = pd.DataFrame(subm.count(), columns=['count'])\ncount.query(\"count < 500000\")\n\n# As expected, there are not null values in test dataset neither.","9e3c1b4e":"# ------------------------------------------------------------\n#   Variable CORRELATION\n# ------------------------------------------------------------\n# Correlation matrix\n# --------------------\n# We make a correlation matrix to check if there are relations between the different fields.\ncorrmat = data.corr()","692b9eb0":"# Let's draw the corrmat\nf, ax = plt.subplots(figsize =(40, 40))\nsns.heatmap(corrmat, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1)\n# Explanation of the graph: The blue diagonal \"line\" represents a 100% of correlation between each feature and itself\n#   the other points, as the right vertical correlation rule indicates, seems not to have correlation with other features except of itself. \n# f22 seems to have no correlation at all with target, that could leat to the feature removal","b0f9da16":"# Distribution of the target:\ndata.groupby('target').count()\n# 499515\n# 500485\n# The data is quite perfectly balanced","5204354d":"# ------------------------------------------------------------\n#   Variable DISTRIBUTIONS\n# ------------------------------------------------------------\n# I will draw the hystograms for all variables\ndata.hist(grid = False, figsize=(25,80), layout=(29, 10), bins=50)","cbdeb9ba":"# ------------------------------------------------------------------------------\n#  CARDINALITY OF VARIABLES\n# ------------------------------------------------------------------------------\n# After watching the output we can appreciate that there are plenty of features that seems to be binaries\n# So, let's see theyr cardinality\npd.set_option(\"display.max_rows\", 300)\n\ndata.nunique()","61d74b56":"# What we can see below is that there are 45 binary features . \n# This is useful for example in case we use a LightGBM algorithm, we can specify the categorical features in order to improve results\n\n# Example usage (LightGBM): \n# categorical = ['f22', 'f43', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248',\n#                'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257',\n#                'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266',\n#                'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275',\n#                'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284']\n\n# fit_params={... \n#             'categorical_feature': categorical\n#             ...\n#            }  ","f0c4fb85":"# We can find handy this other histogram plot, that makes two plots overlapped\n# Superposition of the two graphs: target==1 and target==0\n# We will only plot the first 10 features\n\n# REFERENCE: https:\/\/stackoverflow.com\/questions\/37911731\/seaborn-histogram-with-4-panels-2-x-2-in-python\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndata_hist = pd.melt(data[['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'target']], \"target\", var_name=\"target distributions\")\ng = sns.FacetGrid(data_hist, hue=\"target\", col=\"target distributions\", col_wrap=5, sharex=False, sharey=False)\ng.map(plt.hist, \"value\", bins=20, alpha=.4)\n","e0d0fbeb":"# ------------------------------------------------------------\n#  Checking SKEWNESS for continuous data\n# ------------------------------------------------------------\n# Last of all, the following code is to calculate the skewed data. In this example left skewed data.\n# This could be used to correct skewness with log or exponential transformations \n\ndata_skewed = pd.concat([pd.DataFrame(data.columns), pd.DataFrame(st.skew(data))], axis=1)\ndata_skewed.columns = ['names', 'skewness']\n# I only get fields that has a skewness bigger than 3\nskewed = data_skewed.query('skewness > 3')['names']","3e910c66":"skewed","def77ee9":"# ------------------------------------------------------------\n#  Best performing algorithms\n# ------------------------------------------------------------\n# As part of the EDA I can add the output of the LazyPredict used in other of my notebooks.\n# REFERENCE: https:\/\/www.kaggle.com\/brbarasulpis\/tps-2021-oct-automl-lazypredict-lazyclassifier\n\n# In the ouptut we can see that the best option got is LightGBM Classifier.\n# If we perform an ensemble algorithm, we could use for example LightGBM+BernoulliNB+AdaBoostClassifier.","8f6a71f7":"![](http:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1642245\/2696798\/LazyClassifier_Output.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211011%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211011T220846Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=36028cc47ab8ddc360a12680533209103048444e19d16eb41c8996032e865b1f9c28b5cac8432bcc9ca25c2333a710e25c651844ea159e0790b4f7bd780bc9d28eea421e2a9c3850c83ba20c921048dd25845f904ae90603e0bc226eceab6b9b141d66ea9d588ec88d64ceb5f3e25064e147bdb930fea38ba3841c3839d76a0a7f539b01fb3ab49ccfa49e1b007e64139500c00586fd494d98a4d0cb451869b5193e36262075617bfbbcf3a922a3d593542c911a259ef46c51ae45eaa66b71c1620131fbbb6ac544ade9204ea65e2fabc352c244a42a4336b721e2ecf94d6b70993ebd1e25c99603f64aeba92384c1ce88dac421a2a1a185e618e0aa8666b484)","84c8536c":"# Exploratory Data Analisys for Tabular Playground Series (Oct 2021)","ebd150cf":"# CONCLUSIONS\nAfter this exploratory data analisys we now know that:\n* There are no missing values in the dataset\n* The target is balanced (nearly half values in 1 and half in 0)\n* There is no correlations between the variables\n* There are 45 categorical features\n* Many continuous features are left skewed\n* Best algorithm suggested for the problem is LightGBM"}}