{"cell_type":{"52987452":"code","5aa61b5b":"code","b4e354e6":"code","0f98fb75":"code","580b6cce":"code","a1f4664e":"code","7b03cb53":"code","e7cce84a":"code","1dcb0b76":"code","76c293d3":"code","e3084a51":"code","29beb50e":"code","a7f5e979":"code","8595653f":"code","b68d7b99":"code","52f5c51e":"code","2d46b791":"code","d133da05":"code","dc5c97d1":"code","8ee34691":"code","4f323c1d":"code","9bfff423":"code","a6c14581":"code","02fb6819":"code","7e56bb4f":"code","a3d277c2":"code","99331b2b":"code","f644786e":"code","d64180af":"code","d6a823ab":"code","1f68b32c":"code","02ee602a":"code","c66dc957":"code","1c3ae9b2":"code","30c50075":"code","011a37de":"markdown","94c800e2":"markdown","d319ca3e":"markdown","795cc659":"markdown","ee3e98ca":"markdown","7cf5b7fe":"markdown","6c0e0c8a":"markdown","eaee5ff8":"markdown","806a71f9":"markdown","5c6c8ff0":"markdown","91bd0442":"markdown","b278c54f":"markdown","a5762247":"markdown","183b5497":"markdown","a3636aaf":"markdown","94d75c0a":"markdown","b0c67fdb":"markdown","781e6fbe":"markdown","f2c7535a":"markdown","f7c9de61":"markdown","e50a1b7f":"markdown","8c9c77ad":"markdown","47540280":"markdown","ad7becc2":"markdown","97ef76ae":"markdown","9ebf3dda":"markdown","4ab31fa0":"markdown","c7a91593":"markdown","13ec44c8":"markdown","4041fddb":"markdown","f1c73832":"markdown","720d16be":"markdown","56e72412":"markdown","cf42a98a":"markdown","c89caccb":"markdown","8be2187f":"markdown"},"source":{"52987452":"# Importando bibliotecas:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score\n","5aa61b5b":"adult_train = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\", index_col=['Id'], na_values=\"?\")\nadult_train.head()","b4e354e6":"adult_train.info()","0f98fb75":"above_50k = adult_train[adult_train[\"income\"] == \">50K\"]\nabove_50k.head()","580b6cce":"below_50k = adult_train[adult_train[\"income\"] == \"<=50K\"]\nbelow_50k.head()","a1f4664e":"adult_train.describe().T","7b03cb53":"# Fun\u00e7\u00e3o gaussiana. Recebe as bins, m\u00e9dia e desvio padr\u00e3o e devolve a gaussiana.\n\ndef gaussian(x, mu, sig):\n    return 1 \/ (np.sqrt(2 * np.pi) * sig) * np.exp(-(((x - mu) \/ sig)**2) \/ 2)\n","e7cce84a":"# Fun\u00e7\u00e3o para plotar gaussiana, recebe lista dos conjuntos de dados, lista de suas labels, nome dos dados e valor m\u00e1x e m\u00edn. Devolve gr\u00e1fico da gaussiana\ndef plot_gaus(dados, lbl, name, min_val, max_val):\n  # Bins que ser\u00e3o usados para plotar a gaussiana:\n  bins = np.linspace(min_val, max_val, 1000)\n\n  gaus = []  # Dados das gaussianas\n  \n  for data in dados:\n    # Calculando a m\u00e9dia e o desvio padr\u00e3o dos dados:\n    mean = np.mean(data)\n    sdev = np.std(data)\n\n    # Mapeando a densidade de probabilidade normal (gaussiana):\n    gaus.append(gaussian(bins, mean, sdev))\n\n  # Montando o gr\u00e1fico\n  i = 0\n  for y in gaus:\n    labels = lbl[i]\n    plt.plot(bins, y, label=labels)\n    i += 1\n  \n  # Plotando o gr\u00e1fico\n  plt.xlabel(name)  # NOME DOS DADOS ANALISADOS\n  plt.ylabel('Densidade de probabilidade normal')\n  plt.legend()\n  plt.show()\n","1dcb0b76":"d = [above_50k[\"age\"].values, below_50k[\"age\"].values]\nl = [\">50K\", \"<=50K\"]\nplot_gaus(d, l, \"Idade\", 0, 100)","76c293d3":"d = [above_50k[\"hours.per.week\"].values, below_50k[\"hours.per.week\"].values]\nl = [\">50K\", \"<=50K\"]\nplot_gaus(d, l, \"Horas de trabalho semanais\", 0, 100)","e3084a51":"d = [above_50k[\"education.num\"].values, below_50k[\"education.num\"].values]\nl = [\">50K\", \"<=50K\"]\nplot_gaus(d, l, \"N\u00edvel escolar\", 0, 20)","29beb50e":"# Feature Engineering: Juntando capital gain e capital loss\nadult_train[\"capital.change\"] = adult_train[\"capital.gain\"] - adult_train[\"capital.loss\"]\nabove_50k[\"capital.change\"] = above_50k[\"capital.gain\"] - above_50k[\"capital.loss\"]\nbelow_50k[\"capital.change\"] = below_50k[\"capital.gain\"] - below_50k[\"capital.loss\"]","a7f5e979":"d = [above_50k[\"capital.change\"].values, below_50k[\"capital.change\"].values]\nl = [\">50K\", \"<=50K\"]\nplot_gaus(d, l, \"Varia\u00e7\u00e3o de Capital\", -10000, 10000)","8595653f":"d = [above_50k[\"fnlwgt\"].values, below_50k[\"fnlwgt\"].values]\nl = [\">50K\", \"<=50K\"]\nplot_gaus(d, l, \"Peso Final\", -1000000, 1000000)","b68d7b99":"adult_train.describe(include=['object']).T","52f5c51e":"# Fazendo gr\u00e1fico de barras:\nsns.countplot(y=\"workclass\", hue=\"income\", data=adult_train)","2d46b791":"# Fazendo gr\u00e1fico de barras:\nsns.countplot(y=\"marital.status\", hue=\"income\", data=adult_train)","d133da05":"# Fazendo gr\u00e1fico de barras:\nsns.countplot(y=\"occupation\", hue=\"income\", data=adult_train)","dc5c97d1":"# Fazendo gr\u00e1fico de barras:\nsns.countplot(y=\"relationship\", hue=\"income\", data=adult_train)","8ee34691":"# Fazendo gr\u00e1fico de barras:\nsns.countplot(y=\"race\", hue=\"income\", data=adult_train)","4f323c1d":"# Fazendo gr\u00e1fico de barras:\nsns.countplot(y=\"sex\", hue=\"income\", data=adult_train)","9bfff423":"# Fazendo gr\u00e1fico de barras:\nsns.countplot(y=\"native.country\", hue=\"income\", data=adult_train)","a6c14581":"adult_train.isnull().sum()","02fb6819":"print(\"workclass: \")\nprint((1836\/32560)*100)\nprint(\"occupation: \")\nprint((1843\/32560)*100)\nprint(\"native country: \")\nprint((583\/32560)*100)","7e56bb4f":"drop_cols = [\"capital.gain\", \"capital.loss\"]","a3d277c2":"drop_cols.append(\"fnlwgt\")\ndrop_cols.append(\"education\")","99331b2b":"X = adult_train.drop(drop_cols, axis=1)\n#X[\"USA\"]= (X[\"native.country\"] == \"United-States\").astype(int)\nX.drop(\"native.country\", axis=1, inplace=True)","f644786e":"X.head()","d64180af":"# Separando em valores de treinamento e o classificador\ny = X[\"income\"]\nX = X.drop(\"income\", axis=1)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)","d6a823ab":"# Selecionando valores n\u00famericos, categ\u00f3ricos e esparsos\n\ncat_col = list(X_train.columns[X_train.dtypes == object])\nnum_col = list(X_train.columns[X_train.dtypes == int])\nspa_col = [num_col.pop(3)]","1f68b32c":"# Fazendo um pipeline:\ncat = Pipeline(\n    steps=[\n        ('cimputer', SimpleImputer(strategy='most_frequent')),\n        ('cencoder', OneHotEncoder()),\n    ]\n)\n\nnum = Pipeline(\n    steps=[\n        ('nimputer', SimpleImputer(strategy='most_frequent')),\n        ('nscaler', StandardScaler()),\n    ]\n)\n\nspa = Pipeline(\n    steps=[\n        ('rscaler', RobustScaler())\n    ]\n)\n\npreprocess = ColumnTransformer(\n    transformers=[\n                  ('numerical', num, num_col),\n                  ('sparse', spa, spa_col),\n                  ('categorical', cat, cat_col)\n    ]\n)","02ee602a":"X_train = preprocess.fit_transform(X_train)\nX_valid = preprocess.fit_transform(X_valid)","c66dc957":"def knn_cross_val(X_t, y_t, folds):\n  neighbors = [3,5,7,11,13,15,17,21,23,25,27,29,30]\n  neighbor_score = []\n  best_score = 0\n  best_k = 0\n  for k in neighbors:\n    score = cross_val_score(KNeighborsClassifier(n_neighbors=k, metric='euclidean'), X_t, y_t, cv = folds, scoring='accuracy').mean()\n    neighbor_score.append(score)\n    if score > best_score:\n      best_score = score\n      best_k = k\n\n  print(f\"Best score: {best_score} for k = {best_k}\")\n\n  return best_k, best_score\n\nwin_k, win_score = knn_cross_val(X_train, y_train, 5)","1c3ae9b2":"# Treinando o KNN:\nknn = KNeighborsClassifier(win_k,metric='euclidean' )\nknn.fit(X_train, y_train)\ny_predict_v = knn.predict(X_valid)\naccuracy = accuracy_score(y_valid, y_predict_v)\nprint(accuracy)","30c50075":"# Abrindo arquivo\n\nadult_test = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\", index_col=['Id'], na_values=\"?\")\n\n# Deletando Atributos:\nadult_test[\"capital.change\"] = adult_test[\"capital.gain\"] - adult_test[\"capital.loss\"]\ndrop_cols = [\"capital.gain\", \"capital.loss\", \"fnlwgt\", \"education\"]\nX_test = adult_test.drop(drop_cols, axis=1)\n#X_test[\"USA\"]= (X_test[\"native.country\"] == \"United-States\").astype(int)\nX_test.drop(\"native.country\", axis=1, inplace=True)\n\n# Passando pelo pipeline\nX_test = preprocess.fit_transform(X_test)\n\n#Usando KNN pra prever a label:\nY = knn.predict(X_test)\n\nanswer = pd.DataFrame()\nanswer[0] = adult_test.index\nanswer[1] = Y\nanswer.columns = [\"Id\", \"Income\"]\nanswer.to_csv('PMR3508-2021-71-ADULT-KNN.csv', index = False)\n\n\nanswer.head()","011a37de":"O esperado nesse tipo de situa\u00e7\u00e3o seria que profiss\u00e3o que necessitam de maior qualifica\u00e7\u00e3o ou que possuem escassez de profissionais geram maior renda, e no geral esse \u00e9 o padr\u00e3o que podemos observar no gr\u00e1fico acima, com o maior n\u00famero de indiv\u00edduos com alta renda se concentrando em posi\u00e7\u00f5es executivas e especializadas.\n\n**Rela\u00e7\u00f5es**\n\nSe refere ao papel do ind\u00edviduo em sua fam\u00edlia.","94c800e2":"# 1: An\u00e1lise dos Dados\nAntes mesmo de importar a nossa base de dados iremos tentar compreend\u00ea-la melhor de forma anal\u00f3gica, ou seja, iremos ler o arquivo descritivo que veio junto com a base de dados e usar nosso melhor julgamento para interpretar o significado de cada vari\u00e1vel.\n\nDe acordo com o arquivo descritivo incluso a base de dados adult foi extra\u00edda do censo de 1994 dos Estados Unidos. \n\nEla possu\u00ed 48842 pontos de dados se considerarmos os que possuem valores faltantes e 45222 se os exclu\u00edrmos (do que podemos concluir que existem 3620 inst\u00e2ncias com dados faltantes).\n\nTemos 14 atributos (sem contar o identificador).\n\nA base Adult foi separada de forma que 2\/3 s\u00e3o dados de treino e 1\/3 s\u00e3o dados de teste. \n\nVamos importar a base treino para podermos discuti-la melhor com apoio de vizualiza\u00e7\u00f5es:\n\nComo j\u00e1 sabemos pelo arquivo descritivo que a base possu\u00ed uma coluna de identificadores, iremos utiliz\u00e1-los como nosso \u00edndice e tamb\u00e9m sabemos que dados faltantes est\u00e3o indicados como \"?\".","d319ca3e":"**Profiss\u00e3o**\n\nRepresenta a categoria da profiss\u00e3o do ind\u00edviduo.","795cc659":"Vale notar que pessoas cujo papel \u00e9 ser filho (provavelmente mais jovens) tem renda mais baixa no geral e que maridos e esposas tem mais inst\u00e2ncias de alta renda.\n\n**Ra\u00e7a**\n\nIndica a etnia do \u00edndividuo.","ee3e98ca":"# 1.2. Vari\u00e1veis Qualitativas\n\nAgora vamos analizar as vari\u00e1veis de valor n\u00e3o-n\u00famerico (categ\u00f3ricas ou qualitativas). Como mencionado anteriormente a \u00fanica que seria ordinal e n\u00e3o nominal seria educa\u00e7\u00e3o, que n\u00e3o ser\u00e1 inclusa nessa an\u00e1lise pois ela foi observada de forma num\u00e9rica na sess\u00e3o acima. \n\nLevando isso em considera\u00e7\u00e3o os atributos qualitativos s\u00e3o: \n\nworkclass, marital-status, occupation, relationship, race, sex e native-country.\n\nPodemos ter uma primeira an\u00e1lise com o pandas:","7cf5b7fe":"# 3. Pipeline e Cross Validation\n\nAgora podemos come\u00e7ar a construir nosso k-vizinhos. Primeiro iremos organizar os dados em colunas categ\u00f3ricas e num\u00e9ricas e depois dividir em dados de treino e teste.\n","6c0e0c8a":"Existem diversas formas de abordar esse problema mas vamos focar em duas: \n  - Remover os dados faltantes.\n  - Substitu\u00ed-los.\n\nGeralmente se remove os dados quando uma porcentagem muito grande do total estiver faltando. Temos que:\n\n","eaee5ff8":"\u00c9 claro nesse caso que indiv\u00edduos casados tendem a ter renda maior. Vale a pena olhar a gaussiana desses dados uma vez que eles forem codificados para valores num\u00e9ricos. ","806a71f9":"Nesse caso como a renda vem de investimentos separados do sal\u00e1rio \u00e9 esperado que a mudan\u00e7a mais dr\u00e1stica venha das pessoas com alta renda, j\u00e1 que sabemos que essas tendem a ter mais investimentos. E como podemos ver pelo gr\u00e1fico isso \u00e9 o que aconteceu. \n\nAqui tamb\u00e9m os dois gr\u00e1ficos est\u00e3o se sobrepondo, mas como a diferen\u00e7a entre eles \u00e9 dr\u00e1stica \u00e9 poss\u00edvel que essa vari\u00e1vel seja de boa utilidade.\n\n**Peso**\n\nFinalmente chegamos no atributo \"fnlwgt\" ou \"\"final weight\", um indicador utilizado pelo censo americano para amenizar qualquer vi\u00e9s que possa ter surgido a partir da forma que o censo \u00e9 realizado. Podemos ver a descri\u00e7\u00e3o de como ele \u00e9 calculado e utilizado no pr\u00f3prio [site do censo](https:\/\/www.census.gov\/programs-surveys\/sipp\/methodology\/weighting.html) ou [nesse documento](https:\/\/pages.nyu.edu\/jackson\/design.of.social.research\/Readings\/Johnson%20-%20Introduction%20to%20survey%20weights%20%28PRI%20version%29.pdf) produzido pela Pennsilvania State University. \n\nPelo que podemos ver esse indicador parece considerar diversos aspectos socioecon\u00f4micos que poderiam influenciar a renda, por\u00e9m n\u00e3o sabemos ainda como ele poderia influenciar o knn. Vamos ver sua gaussiana:","5c6c8ff0":"Em todos eles \u00e9 uma porcentagem relativamente pequena de dados faltantes, ent\u00e3o podemos considerar substitu\u00edlos. \n\nIsso pode ser feito de v\u00e1rias formas como usar o modo ou a mediana como os novos par\u00e2metros. Iremos testar diversas formas quando construirmos o pipeline.","91bd0442":"Tamb\u00e9m vimos que education e education.num tem a mesma fun\u00e7\u00e3o ent\u00e3o podemos remover um deles. ","b278c54f":"Por fim podemos rodar o algor\u00edtimo de k-vizinhos utilizado valida\u00e7\u00e3o cruzada, em que separamos a base de dados em v\u00e1rias folds para conseguirmos ter uma melhor vis\u00e3o da acur\u00e1cia do modelo.","a5762247":"Como seria esperado pessoas com maior n\u00edvel escolar tendem a ter maior renda.\n\nNovamente os dados se sobrep\u00f5e, n\u00e3o havendo um cluster claro. Isso n\u00e3o significa que os dados n\u00e3o devem ser utilizados, j\u00e1 que existe uma correla\u00e7\u00e3o clara em todos os casos tratados acima, mas significa que nenhum deles ser\u00e1 **sozinho** um grande contribuidor para a classifica\u00e7\u00e3o.\n\n**Ganho e Perda de Capital**\n\nAmbos os atributos \"capital.gain\" e \"capital.loss\" se referem a movimenta\u00e7\u00e3o de capital que vai al\u00e9m da renda f\u00edsica e est\u00e3o relacionados um com o outro. De tal modo n\u00e3o iremos utilizalos separadamente mas utilizar \"feature engineering\" para as juntarmos em uma \u00fanica vari\u00e1vel representando a varia\u00e7\u00e3o total de capital:","183b5497":"Nesse caso \u00e9 mais dif\u00edcil interpretar os dados pois existem muito mais inst\u00e2ncias de ind\u00edviduos brancos na base de dados do que de outras etinias. Isso \u00e9 provavelmente um dos motivos para os pesos terem sido inclu\u00eddos, j\u00e1 que esses dados n\u00e3o representariam a popula\u00e7\u00e3o americana real. Por\u00e9m para os objetivos desse exerc\u00edcio esses dados devem ser suficientes.\n\n**Sexo**\n\nIndica o g\u00eanero do indiv\u00edduo entre feminino e masculino.","a3636aaf":"O comportamento visto nesse gr\u00e1fico \u00e9 condizente com o esperado e h\u00e1 uma rela\u00e7\u00e3o viz\u00edvel entre os dados, mesmo que n\u00e3o muito expressiva, ent\u00e3o este atributo pode ser utilizado.\n\n**Estado Civil**\n\nO estado civil se refere a se o indiv\u00edduo \u00e9 casado, separado, solteiro, vi\u00favo, etc. \n\n","94d75c0a":"Pelo arquivo descritivo sabemos quais vari\u00e1veis s\u00e3o qualitativas e quais s\u00e3o quantitativas, por\u00e9m, tamb\u00e9m podemos verificar buscando a informa\u00e7\u00e3o geral da nossa base de dados a partir do pandas:","b0c67fdb":"# 2.2. Removendo atributos\n\nPara conseguirmos um modelo melhor removemos atributos desnecess\u00e1rios.\n\nOs primeiros que iremos remover ser\u00e3o os valores capital.gain e capital.loss j\u00e1 que eles n\u00e3o s\u00e3o mais necess\u00e1rios pois capital.change descreve os dois:","781e6fbe":"A partir disso percebemos que, como conseguimos obter a m\u00e9dia e o desvio padr\u00e3o de cada um desses atributos, podemos calcular e plotar a densidade de probabilidade normal (gaussiana) de cada uma delas.\n\nIsso \u00e9 muito \u00fatil pois o k-vizinhos parte do princ\u00edpio de que existe uma base de dados organizada em clusters (grupos) de dados de forma que, dado um objeto  teste seus vizinhos provavelmente ter\u00e3o a mesma classe que ele. Isso significa que se conseguirmos identificar que atributos criam uma separa\u00e7\u00e3o mais not\u00e1vel entre os dois classificadores, conseguimos saber quais s\u00e3o as vari\u00e1veis mais significantes para nosso modelo.\n\n**Idade (age)**\n\nO atributo \"age\" representa a idade de cada ind\u00edviduo em nossa base de dados. Vemos pela an\u00e1lise acima que o seu valor m\u00ednimo s\u00e3o 17 anos e o m\u00e1ximo seria 90, o que \u00e9 consistente com idades de seres humanos com renda e segue as condi\u00e7\u00f5es de extra\u00e7\u00e3o apresentadas no arquivo descritivo. \nSendo assim, n\u00e3o parece haver nenhuma discrep\u00e2ncia \u00f3bvia, mas se prestarmos aten\u00e7\u00e3o a faixa de valores \u00e9 bem diferente entre cada atributo, ent\u00e3o talvez seja uma boa ideia normalizar os dados mais tarde. \nFazendo a gaussiana:","f2c7535a":"Como podemos ver nesse caso, os dados est\u00e3o sobrepostos ent\u00e3o n\u00e3o \u00e9 t\u00e3o bom para identificar clusters, apesar de ainda ser um dado \u00fatil. \n\nPodemos perceber tamb\u00e9m que \u00e9 mais prov\u00e1vel que pessoas mais jovens recebam menos que 50k.\n\n**Hours per week**\n\nO atributo hours.per.week representa as horas de trabalho semanais de cada indiv\u00edduo. Observando sua gaussiana:\n","f7c9de61":"Como vimos que o peso n\u00e3o cria clusters e \u00e9 dif\u00edcil de diferenciar entre as categorias podemos exclu\u00ed-lo tamb\u00e9m:","e50a1b7f":"\u00c9 claro ver que a maior parte dos \u00edndividuos de alta renda se indentificou como homem. Esse pode ser um bom dado para ajudar a separar os clusters. \n\n**Pa\u00eds de Origem**\n\nObserva o pa\u00eds em que o \u00edndividuo nasceu.","8c9c77ad":"Agora n\u00f3s podemos aplicar boa parte das mudan\u00e7as que mencionamos enquanto estudavamos os dados atrav\u00e9s de um pipeline. \nVamos usar um simple imputer para substituir os valores faltantes pelo valor mais frequente, iremos transformar os valores categ\u00f3ricos em num\u00e9ricos com o OneHotEncoder e iremos normalizar os dados n\u00famericos com o StandardScalar.  Quanto \u00e0 mudan\u00e7a de capital especificamente, como os dados s\u00e3o muito desigualmente distribu\u00eddos (uns muito pequenos e outros de valor muito mais alto que qualquer outro atributo) ser\u00e1 utilizado o RobustScaler, para que sejam reduzidos os outliers.","47540280":"# 4. Usando na base de teste\n\nPor fim carregamos a base de teste e realizamos os mesmos procedimentos que realizamos sobre os dados de treino:\n\n","ad7becc2":"Novamente \u00e9 viz\u00edvel que algumas dessas categorias possuem dados faltantes, mas podemos fazer a an\u00e1lise por enquanto e ent\u00e3o utiliz\u00e1-la para decidir como resolver esse problema.\n\nObservando os atributos um por um:\n\n**Tipo de emprego**\n\nEsse atributo descreve se a pessoa trabalha para uma empresa privada, para o governo, se \u00e9 auto-empregada, nunca teve um emprego, etc.\n\nPodemos ver a influ\u00eancia dessa vari\u00e1vel na renda dos indiv\u00edduos fazendo um histograma:","97ef76ae":"# 1.1. Vari\u00e1veis Quantitativas\n\nPrimeiro vamos olhar as vari\u00e1veis quantitativas, ou seja, aquelas de valores num\u00e9ricos, que no nosso caso s\u00e3o:\n\nage, fnlwgt, education.num, capital.gain, capital.loss, hours.per.week\n\nPodemos obter algumas an\u00e1lises r\u00e1pidamente com o pandas:","9ebf3dda":"Outros dados que podemos considerar remover s\u00e3o os pa\u00edses de origem e a varia\u00e7\u00e3o de capital, por\u00e9m como \u00e9 mais dif\u00edcil perceber o impacto que estes tem sobre o modelo \u00e9 poss\u00edvel criarmos subsets do banco de dados durante o cross validation, para verificar se seria mais eficiente remov\u00ea-los ou n\u00e3o.","4ab31fa0":"\n# 2.3. Normalizando dados\n\nPrimeiramente podemos pegar o pa\u00eds de origem e reduz\u00ed-lo para duas categorias apenas, 0 se eimigrante e 1 se nativo o que tamb\u00e9m j\u00e1 transforma a categoria em num\u00e9rica:\n(A linha comentada aqui n\u00e3o est\u00e1 mais ativa, dado a uma an\u00e1lise que ser\u00e1 feita posteriormente no documento)\n","c7a91593":"# 2. Tratando os dados\n\nAgora que entendemos o que cada atributo representa e sua signific\u00e2ncia para o modelo podemos ajustar a base de dados para atender melhor nossas necessidades. \n\n# 2.1. Dados Faltantes\n\nComo mencionado, existem 3 categorias com dados faltantes. Podemos ver exatamente quantos elementos est\u00e3o nulos em cada uma:","13ec44c8":"Vemos aqui que existem um desequil\u00edbrio muito grande entre pesoas nascidas nos Estados Unidos e pessoas nascidas em outros pa\u00edses. Isso \u00e9 de se esperar considerando que os dados foram extra\u00eddos do censo americano, ent\u00e3o seria razo\u00e1vel assumir que maior parte de seus participantes estavam morando no pa\u00eds na \u00e9poca e muito provavelmente eram nativos. \nAssim talvez seja ben\u00e9fico reclassificar essa categoria entre \"nativos\" e \"imigrantes\", se n\u00e3o exclu\u00ed-la por completo. Podemos explorar mais nossas op\u00e7\u00f5es quando estivermos implementando o k-vizinhos.","4041fddb":"Assim podemos ver que a melhor op\u00e7\u00e3o \u00e9 remover a vari\u00e1vel de nacionalidade de agora em diante. Pode-se perceber algumas linhas de c\u00f3digo comentadas nesse notebook que s\u00e3o essencialmente remanescentes desse teste. Outros testes que podem ser feitos \u00e9 testar que combina\u00e7\u00e3o de par\u00e2metros podemos usar para o algor\u00edtimo do Knn por exemplo.\n\nSe vamos usar dist\u00e2ncias com peso, ou se vamos calcular de forma euclidiana, etc.\n\nN\u00e3o faremos isso hoje porque para os objetivos desse exerc\u00edcio o nosso modelo j\u00e1 est\u00e1 satisfat\u00f3rio, mas \u00e9 bom ter isso em mente como uma possibilidade.\n\nVamos ao inv\u00e9s disso ir direto ao treinamento de nosso classificador:\n","f1c73832":"Infelizmente nesse gr\u00e1fico vemos que a sobreposi\u00e7\u00e3o entre as duas curvas \u00e9 quase perfeita. Nesse caso seria muito dif\u00edcil esse atributo ser significativo para o m\u00e9todo de k-vizinhos ent\u00e3o \u00e9 poss\u00edvel considerar excluir o atributo como um todo. \n\nTalvez existam outros modelos em que o peso final fa\u00e7a maior diferen\u00e7a ou existam outras formas de utiliz\u00e1-los desconhecidas \u00e0 compiladora.","720d16be":"N\u00e3o \u00e9 um resultado ruim, mas existem formas de verificar se ele poderia ser melhor. Inicialmente seria aplicado um GridSearch para observar os melhores par\u00e2metros poss\u00edveis, infelizmente isso n\u00e3o foi poss\u00edvel pois o equipamento dispon\u00edvel n\u00e3o aguentou o peso computacional, ent\u00e3o vamos tentar fazer alguns experimentos manualmente:\n\n**Sele\u00e7\u00e3o de Atributos**\n\nComo foi dito durante a an\u00e1lise dos dados, existem duas features (capital e pa\u00eds de origem) que talvez sejam mais ben\u00e9ficas se forem exclu\u00eddas, ent\u00e3o podemos fazer alguns experimetos para verificar essa possibilidade.","56e72412":"Como podemos ver, temos tanto atributos qualitativos (dos quais apenas \"education\" \u00e9 ordinal) e quantitativos (que s\u00e3o todos cont\u00ednuos). \n\nTamb\u00e9m notamos que os atributos \"workclass\", \"ocupation\" e \"native.country\" tem dados faltantes. Vamos trataar deles depois de entendermos melhor o que os atributos representam.\n\nPodemos analizar nossos atributos um a um para realmente entendermos a signific\u00e2ncia das vari\u00e1veis dispon\u00edveis.\n\nPara facilitar um pouco a an\u00e1lise vamos criar dois subgrupos de dados, um para cada classificador:","cf42a98a":"# KNN for Dataset Adult\n\nBeatriz C. - hash: 71\n\nPMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es 2021\n\nNesse programa iremos aplicar o m\u00e9todo de k-vizinhos na base de dados Adult disponibilizada pela [University of California: Irvine](https:archive.ics.uci.edu\/ml\/datasets\/adult) com o objetivo de classificar os dados teste entre aqueles que recebem <=50k e >50k d\u00f3lares.","c89caccb":"Novamente temos grande sobreposi\u00e7\u00e3o, e vemos tamb\u00e9m que n\u00e3o h\u00e1 grande diferen\u00e7a entre as horas de trabalho entre as pessoas que ganham mais ou menos. J\u00e1 que o expediente de trabalho tradicional \u00e9 40 horas isso \u00e9 esperado, por\u00e9m se percebe que as pessoas que trabalham mais de 40 horas tendem a ganhar mais.\n\n**Educa\u00e7\u00e3o**\n\nLendo o arquivo descritivo vemos que \"education.num\" na verdade esse \u00e9 um atributo redundante, pois \u00e9 a representa\u00e7\u00e3o n\u00famerica da vari\u00e1vel qualitativa ordinal \"education\" que representa o n\u00edvel de escolaridade dos ind\u00edviduos. \n\nComo para aplicar k-vizinhos seria necess\u00e1rio que os dados fossem encodificados em dados n\u00famericos posteriormente de qualquer maneira \u00e9 mais simples se a partir desse ponto trat\u00e1ssemos apenas com o atributo \"education.num\" em que 1 \u00e9 o menor n\u00edvel de escolaridade (pr\u00e9-escola) e 16 \u00e9 o maior (doutorado).\n\nObservando a gaussiana:\n","8be2187f":"Foi assim que essa combina\u00e7\u00e3o de features em particular foi escolhida, foi-se excluindo cada uma e testando para os maiores valores:\n\n**Quando tinha tanto a var\u00edavel de capital quanto a nacionalidade:**\nBest score: 0.8655437443952966 for k = 15\n\n**Sem Mudan\u00e7a de Capital:** Best score: 0.8336610762374009 for k = 30\n\n**Sem Nacionalidade:** Best score: 0.8659534545124791 for k = 23\n\n**Sem Mudan\u00e7a de Capital e Nacionalidade:** Best score: 0.8346027846027846 for k = 30\n\n"}}