{"cell_type":{"e82c6849":"code","d249656c":"code","7047a2cd":"code","2eb33540":"code","7f01c2fd":"code","829b0fcb":"code","3815d15b":"code","8a823b84":"code","308fca23":"code","b611955b":"code","de73cf00":"code","734176a9":"code","f6d9f889":"code","8cc60a08":"code","9615031b":"code","8ac9d3d9":"code","c3bb6700":"markdown","c743f07e":"markdown","3fc5fb6e":"markdown","8cef2798":"markdown","c6b0df8f":"markdown","3d2903c0":"markdown","6518fa17":"markdown","0f5f1244":"markdown"},"source":{"e82c6849":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nimport string\nimport unidecode\nimport random\nimport torch","d249656c":"# Check if GPU is available\ntrain_on_gpu = torch.cuda.is_available()\nif(train_on_gpu):\n    print('Training on GPU!')\nelse: \n    print('No GPU available, training on CPU; consider making n_epochs very small.')","7047a2cd":"train_df = pd.read_csv('..\/input\/train.csv')\nauthor = train_df[train_df['author'] == 'EAP'][\"text\"]\nauthor[:5]","2eb33540":"text = list(author[:100])\ndef joinStrings(text):\n    return ' '.join(string for string in text)\ntext = joinStrings(text)\n# text = [item for sublist in author[:5].values for item in sublist]\nlen(text.split())","7f01c2fd":"stop = set(nltk.corpus.stopwords.words('english'))\nexclude = set(string.punctuation) \nlemma = nltk.stem.wordnet.WordNetLemmatizer()\ndef clean(doc):\n        stop_free = \" \".join([i for i in doc.split() if i not in stop])\n        punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n        return normalized\ntest_sentence = clean(text).lower().split()","829b0fcb":"trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n            for i in range(len(test_sentence) - 2)]\nchunk_len=len(trigrams)\nprint(trigrams[:3])","3815d15b":"vocab = set(test_sentence)\nvoc_len=len(vocab)\nword_to_ix = {word: i for i, word in enumerate(vocab)}\n","8a823b84":"inp=[]\ntar=[]\nfor context, target in trigrams:\n        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n        inp.append(context_idxs)\n        targ = torch.tensor([word_to_ix[target]], dtype=torch.long)\n        tar.append(targ)","308fca23":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(RNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        \n        self.encoder = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers,batch_first=True,\n                          bidirectional=False)\n        self.decoder = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, input, hidden):\n        input = self.encoder(input.view(1, -1))\n        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n        output = self.decoder(output.view(1, -1))\n        return output, hidden\n\n    def init_hidden(self):\n        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))","b611955b":"def train(inp, target):\n    hidden = decoder.init_hidden().cuda()\n    decoder.zero_grad()\n    loss = 0\n    \n    for c in range(chunk_len):\n        output, hidden = decoder(inp[c].cuda(), hidden)\n        loss += criterion(output, target[c].cuda())\n\n    loss.backward()\n    decoder_optimizer.step()\n\n    return loss.data.item() \/ chunk_len","de73cf00":"import time, math\n\ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)","734176a9":"n_epochs = 300\nprint_every = 100\nplot_every = 10\nhidden_size = 100\nn_layers = 1\nlr = 0.015\n\ndecoder = RNN(voc_len, hidden_size, voc_len, n_layers)\ndecoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nstart = time.time()\nall_losses = []\nloss_avg = 0\nif(train_on_gpu):\n    decoder.cuda()\nfor epoch in range(1, n_epochs + 1):\n    loss = train(inp,tar)       \n    loss_avg += loss\n\n    if epoch % print_every == 0:\n        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch \/ n_epochs * 50, loss))\n#         print(evaluate('ge', 200), '\\n')\n\n    if epoch % plot_every == 0:\n        all_losses.append(loss_avg \/ plot_every)\n        loss_avg = 0","f6d9f889":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline\n\nplt.figure()\nplt.plot(all_losses)","8cc60a08":"def evaluate(prime_str='this process', predict_len=100, temperature=0.8):\n    hidden = decoder.init_hidden().cuda()\n\n    for p in range(predict_len):\n        \n        prime_input = torch.tensor([word_to_ix[w] for w in prime_str.split()], dtype=torch.long).cuda()\n        inp = prime_input[-2:] #last two words as input\n        output, hidden = decoder(inp, hidden)\n        \n        # Sample from the network as a multinomial distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n        \n        # Add predicted word to string and use as next input\n        predicted_word = list(word_to_ix.keys())[list(word_to_ix.values()).index(top_i)]\n        prime_str += \" \" + predicted_word\n#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n\n    return prime_str","9615031b":"print(evaluate('this process', 40, temperature=1))","8ac9d3d9":"print(evaluate('i might', 30, temperature=1))","c3bb6700":"**Generating the text**","c743f07e":"> **Beginners Guide to Text Generation using GRUs**\n\nText Generation is a type of Language Modelling problem. Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. In this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network.","3fc5fb6e":"> **N-Gram Language Modeling**\n\nRecall that in an n-gram language model, given a sequence of words w, we want to compute.\n                                      * P(wi|wi\u22121,wi\u22122,\u2026,wi\u2212n+1)                                                     \nWhere wi is the ith word of the sequence.                                                                              here we will take n=2.","8cef2798":"**GRU model for Text Generation**","c6b0df8f":"**Load the dataset**","3d2903c0":"**Dataset cleaning**","6518fa17":"> **Improvement Ideas**\n\nAs we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n\n* Adding more data\n* Fine Tuning the network architecture\n* Fine Tuning the network parameters\n\nThanks for going through the notebook, please upvote if you liked.","0f5f1244":"**Import the libraries**"}}