{"cell_type":{"e4e2cc21":"code","783ebef7":"code","37400c94":"code","ca4fab37":"code","845af9bd":"code","0548549e":"code","26e83516":"code","8506cca5":"code","4e900722":"code","ac8f59fa":"code","941a3b0e":"code","4b0615c5":"code","a904ab22":"code","f1cff108":"code","0d7efd8f":"code","c91887a2":"code","a8448831":"code","5809d9f7":"code","cae2bdf2":"code","ca8bd482":"code","42e540df":"markdown","6ce526d4":"markdown","7a99f603":"markdown","f55b30ae":"markdown","6077b3ad":"markdown","6f15877e":"markdown","8c97e898":"markdown","a5375370":"markdown","631c0c7f":"markdown"},"source":{"e4e2cc21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","783ebef7":"df = pd.read_csv(\"\/kaggle\/input\/beginners-classification-dataset\/classification.csv\")","37400c94":"df.head() # only three variables. \ndf.info() ","ca4fab37":"df.shape # 297 rows, 3 columns (small size)","845af9bd":"df.isnull().sum() # no missing","0548549e":"df[df.duplicated()] # no duplicated values. ","26e83516":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)","8506cca5":"px.histogram(df, x=\"age\", color=\"success\", marginal = \"box\")","4e900722":"px.histogram(df, x=\"interest\", color=\"success\", marginal='box')","ac8f59fa":"from sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\ndf = shuffle(df)\nX = df.drop('success', axis=1).values\ny = df['success'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)","941a3b0e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n\n# make get_score function\ndef get_scores(y, y_pred):\n    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n    'Precision':np.round(precision_score(y, y_pred),2),\n    'Recall':np.round(recall_score(y, y_pred),2),\n    'F1':np.round(f1_score(y, y_pred),2),\n    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n    scores_df = pd.Series(data).to_frame(' ').transpose()\n    return scores_df","4b0615c5":"model1 = LogisticRegression()\npred = cross_val_predict(model1, X_train, y_train, cv=5)\nget_scores(y_train, pred)","a904ab22":"import xgboost as xgb\nfrom xgboost import plot_importance\nfrom xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GridSearchCV\n","f1cff108":"xgb_model = xgb.XGBClassifier()\n\nparam = {'nthread':[4], \n         'objective':['binary:logistic'],\n         'learning_rate': [0.03, 0.05, 0.07], \n         'max_depth': [6, 8, 10],\n         'min_child_weight': [7, 9, 11],\n         'subsample': [0.5, 0.7, 0.9],\n         'colsample_bytree': [0.7, 0.9, 1.0],\n         'n_estimators': [100],\n         'seed': [122]}\nclf = GridSearchCV(xgb_model, param_grid = param, scoring = 'accuracy', refit = True, verbose = 0)\n","0d7efd8f":"xgbfit = clf.fit(X_train, y_train)","c91887a2":"clf.best_params_","a8448831":"pred2 = clf.predict(X_train)","5809d9f7":"get_scores(y_train, pred2) # great result. ","cae2bdf2":"# logistic regression\nmodel1 = LogisticRegression().fit(X_train, y_train)\npred = model1.predict(X_test)\nget_scores(y_test, pred)","ca8bd482":"# XGBoost\npred2 = clf.predict(X_test)\nget_scores(y_test, pred2)","42e540df":"Interest seems to be more influential factor! ","6ce526d4":"### 1. Logistic Regression\nLet's use logistic regression as the baseline. ","7a99f603":"For logistic regression, accuracy = 0.88, precision=0.92, recall=0.86, F1=0.89, roc-auc=0.88. ","f55b30ae":"I would practice XGBoost with new data. \nData source: https:\/\/www.kaggle.com\/sveneschlbeck\/beginners-classification-dataset","6077b3ad":"Let's test the result using test dataset. ","6f15877e":"### 2. XGBoost","8c97e898":"### 0. Split data into train & test. \n- considering the small datasize, I divided data with the ratio of 8:2 (8 for train, 2 for test). ","a5375370":"Interestingly, as one is younger, success rate seems to be lower. ","631c0c7f":"When I test the result, the recall rate of tuned XGBoost was much worser than basic logistic regression. \nOther indices were slightly better with tuned XGBoost. "}}