{"cell_type":{"9926d878":"code","e64a938f":"code","d27b3301":"code","99d9fd3e":"code","3d06b32f":"code","286639d0":"code","b55ef258":"code","01d44a6a":"code","fe6ce2ee":"code","87ef9387":"code","ec919a6b":"code","4a4bf947":"code","ddaaf1d6":"code","60bf9e18":"code","cfd1e6bb":"code","e9cc7ee2":"code","f737929f":"code","460e4489":"code","bb3b36fb":"markdown"},"source":{"9926d878":"!unzip '\/kaggle\/input\/carvana-image-masking-challenge\/train_hq.zip' -d '.\/'\n!unzip '\/kaggle\/input\/carvana-image-masking-challenge\/train_masks.zip' -d '.\/'\n!unzip '\/kaggle\/input\/carvana-image-masking-challenge\/train.zip' -d '.\/'\n!unzip '\/kaggle\/input\/carvana-image-masking-challenge\/train_masks.csv.zip' -d '.\/'","e64a938f":"!pip install git+https:\/\/github.com\/tensorflow\/examples.git\n!pip install -U tfds-nightly\nimport tensorflow as tf\nfrom tensorflow_examples.models.pix2pix import pix2pix\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport pandas as pd \nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nimport glob\nfrom PIL import Image\nfrom keras.preprocessing import image\nimport cv2","d27b3301":"glob.glob('.\/*')","99d9fd3e":"mask_csv=pd.read_csv('.\/train_masks.csv')","3d06b32f":"glob.glob('.\/train\/*')[0]","286639d0":"fig=plt.figure(figsize=(15, 15))\nimg=cv2.imread('.\/train\/d46244bc42ed_04.jpg')\nmask=Image.open('.\/train_masks\/d46244bc42ed_04_mask.gif')\nfiles=[img,mask]\nfor i in range(len(files)):\n    plt.subplot(1, 2 , i+1)\n    plt.imshow(files[i])","b55ef258":"img.shape,image.img_to_array(mask).shape","01d44a6a":"def preprocess_image(img,mask,train=True):\n    input_img=cv2.resize(img,(128,128))\/255.0\n    input_mask=cv2.resize(mask,(128,128))\n    return input_img,input_mask","fe6ce2ee":"def load_imgs(name):\n    input_img=cv2.imread('.\/train\/'+name+'.jpg')\n    input_mask=image.img_to_array(Image.open('.\/train_masks\/'+name+'_mask.gif'))\n    input_img,input_mask=preprocess_image(input_img,input_mask)\n    return input_img,input_mask","87ef9387":"x_data=[]\ny_data=[]\nimgs_path=glob.glob('.\/train\/*')\nfor i in range(len(imgs_path)):\n    input_img,input_mask=load_imgs(imgs_path[i][8:-4])\n    x_data.append(input_img)\n    y_data.append(input_mask)","ec919a6b":"x_d=np.array(x_data)\ny_d=np.array(y_data)\ntrain_data=int((x_d.shape[0]*0.80))\nx_train=x_d[:train_data]\ny_train=y_d[:train_data]\nx_test=x_d[train_data:]\ny_test=y_d[train_data:]\nprint(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n\ny_train=y_train[...,np.newaxis]\ny_test=y_test[...,np.newaxis]\nprint(x_train.shape,y_train.shape,x_test.shape,y_test.shape)","4a4bf947":"import keras","ddaaf1d6":"OUTPUT_CHANNELS = 2\nbase_model = keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n\n# Use the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nlayers = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\ndown_stack = keras.Model(inputs=base_model.input, outputs=layers)\n\ndown_stack.trainable = False","60bf9e18":"up_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]","cfd1e6bb":"def unet_model(output_channels):\n    inputs = keras.layers.Input(shape=[128, 128, 3])\n    x = inputs\n\n  # Downsampling through the model\n    skips = down_stack(x)\n    x = skips[-1]\n    skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        concat = keras.layers.Concatenate()\n        x = concat([x, skip])\n\n  # This is the last layer of the model\n    last = keras.layers.Conv2DTranspose(output_channels, 3, strides=2,padding='same')  #64x64 -> 128x128\n\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","e9cc7ee2":"model = unet_model(OUTPUT_CHANNELS)\nmodel.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","f737929f":"model_history = model.fit(x_train,y_train, epochs=1,validation_data=(x_test,y_test))","460e4489":"i=580\npred_mask = model.predict(x_test[i:i+1])\nprint(pred_mask.shape)\ny_pred=np.argmax(pred_mask[0],-1)\ny_pred=y_pred[...,np.newaxis]\nprint(y_pred.shape)\nfig=plt.figure(figsize=(15, 15))\nimg=x_test[i]\ntrue_mask=keras.preprocessing.image.array_to_img(y_test[i])\npred_mask=keras.preprocessing.image.array_to_img(y_pred)\nfiles=[img,true_mask,pred_mask]\nfor i in range(len(files)):\n    plt.subplot(1, len(files) , i+1)\n    plt.imshow((files[i]))","bb3b36fb":"# Model"}}