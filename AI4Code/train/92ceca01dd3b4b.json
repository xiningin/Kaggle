{"cell_type":{"8dde9afd":"code","36680aa0":"code","1b986f48":"code","f41cccb5":"code","588d0532":"code","0faa6125":"code","3e42e446":"code","1bec5e18":"code","5df10690":"code","76602062":"code","a40f56d2":"code","ad571c72":"code","66710f90":"code","b5b25ca7":"code","38e32d93":"code","3b08019c":"code","025df967":"code","b6f62aca":"code","79937821":"code","ee776504":"code","7898d8a9":"code","8f5ef2a5":"code","c56cfcf9":"code","8d51e607":"code","f7e551ca":"code","5ac2111e":"code","c26849a5":"code","7c875c7b":"code","6ec5fa13":"code","b49d4f7b":"code","021fef22":"code","76c75397":"code","1c9587a9":"code","0b9f3a73":"code","b3757b3f":"markdown","4e01f529":"markdown","084c0bd1":"markdown","c6008c90":"markdown","aa19322e":"markdown","c3e5a3a2":"markdown","1e35801e":"markdown","a79f4dca":"markdown","dfb60367":"markdown","235ca1b4":"markdown","30e0d6c5":"markdown","619b6918":"markdown","afba67bc":"markdown","0df1ca25":"markdown","9253269b":"markdown","6a827f5e":"markdown","226b3eae":"markdown","ab0573f4":"markdown","50dab71d":"markdown","8deb29ab":"markdown","c26aef54":"markdown","7ed2dd3c":"markdown","88c3a25e":"markdown","e72b21e1":"markdown","3ed1f3f2":"markdown","dcff17e7":"markdown","7ead0d61":"markdown","83691764":"markdown","a71d3b7a":"markdown","2b12f174":"markdown"},"source":{"8dde9afd":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n","36680aa0":"import numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport string\nimport plotly.express as px\nfrom collections import defaultdict\nimport operator\nimport re\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score,recall_score,f1_score\nfrom tensorflow.keras.callbacks import Callback,EarlyStopping\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Dense,Dropout,BatchNormalization,Input\nfrom tensorflow.keras.models import Model\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow_addons.optimizers import RectifiedAdam\nimport tensorflow_hub as hub\nimport tokenization\nfrom tensorflow.keras.optimizers import Adam,SGD","1b986f48":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\") # Load the train data\ntest_df  = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")  # Load the test data","f41cccb5":"# Let's take a look at the datas.\ntrain_df.head()","588d0532":"# Display the missing values of the train and the test datas.\nmissing_values = pd.DataFrame({c:[(train_df[c].isna().sum()\/len(train_df))*100,\\\n                                  (test_df[c].isna().sum()\/len(test_df))*100] for c in \\\n                             [\"keyword\",\"location\"]},index=[\"train\",\"test\"])\nmissing_values","0faa6125":"fig , ax = plt.subplots(nrows=1,ncols=2,figsize=(15,3))\nsns.barplot(x=list(missing_values.columns),y=[missing_values.loc[\"train\",\"keyword\"],missing_values.loc[\"train\",\"location\"]],ax=ax[0])\nsns.barplot(x=list(missing_values.columns),y=[missing_values.loc[\"test\",\"keyword\"],missing_values.loc[\"test\",\"location\"]],ax=ax[1])\nax[0].set_ylabel(\"Missing Value count\",size=15)\nax[0].tick_params(axis=\"x\",labelsize=15)\nax[0].tick_params(axis=\"y\",labelsize=15)\nax[1].tick_params(axis=\"x\",labelsize=15)\nax[1].tick_params(axis=\"y\",labelsize=15)\n\nax[0].set_title(\"Training Set\", fontsize=13)\nax[1].set_title(\"Test Set\",fontsize=13)\n\nplt.show()","3e42e446":"train_df[\"keyword\"].fillna(\"no_keywords\",inplace = True)\ntest_df[\"keyword\"].fillna(\"no_keywords\",inplace = True)\ntrain_df[\"location\"].fillna(\"no_location\",inplace=True)\ntest_df[\"location\"].fillna(\"no_location\",inplace=True)","1bec5e18":"train_df[\"target_mean\"] = train_df.groupby(\"keyword\")[\"target\"].transform(\"mean\")\n\nplt.figure(figsize=(8,72))\n\nsns.countplot(y=train_df.sort_values(\"target\",ascending=False)[\"keyword\"],hue=\\\n              train_df.sort_values(\"target\",ascending=False)[\"target\"])\nplt.tick_params(axis=\"x\",labelsize=15)\nplt.tick_params(axis=\"y\",labelsize=15)\nplt.title(\"Target Distribution in Keywords\")\nplt.show()","5df10690":"# word_count\ntrain_df[\"word_count\"] = train_df[\"text\"].map(lambda x: len(str(x).split()))\ntest_df[\"word_count\"] = test_df[\"text\"].map(lambda x: len(str(x).split()))\n\n# unique_word_count \ntrain_df[\"unique_word_count\"] = train_df[\"text\"].map(lambda x:len(set(str(x).split())))\ntest_df[\"unique_word_count\"] = test_df[\"text\"].map(lambda x:len(set(str(x).split())))\n\n# stop_word_count\ntrain_df[\"stop_word_count\"] = train_df[\"text\"].map(lambda x: len([elt for elt in str(x).lower().split() \\\n                                                                  if elt in stopwords.words(\"english\")]))\ntest_df[\"stop_word_count\"] = test_df[\"text\"].map(lambda x:len([elt for elt in str(x).lower().split()\\\n                                                              if elt in stopwords.words(\"english\")]))\n# url_count \ntrain_df[\"url_count\"] = train_df[\"text\"].map(lambda x : len([w for w in str(x).lower().split()\\\n                                                            if 'http' or 'https' in w]))\ntest_df[\"url_count\"] = test_df[\"text\"].map(lambda x : len([w for w in str(x).lower().split()\\\n                                                          if \"http\" or \"https\" in w ]))\n# mean_word_length\ntrain_df[\"mean_word_length\"] = train_df[\"text\"].map(lambda x : np.mean([len(word) for word in x.split()]))\ntest_df[\"mean_word_length\"] = test_df[\"text\"].map(lambda x : np.mean([len(word) for word in x.split()]))\n\n#char_count \ntrain_df[\"char_count\"] = train_df[\"text\"].map(lambda x:len(str(x)))\ntest_df[\"char_count\"] = test_df[\"text\"].map(lambda x:len(str(x)))\n\n#punctuation_count\ntrain_df[\"punctuation_count\"] = train_df[\"text\"].map(lambda x: len([elt for elt in str(x) if elt in string.punctuation]))\ntest_df[\"punctuation_count\"] = test_df[\"text\"].map(lambda x:len([elt for elt in str(x) if elt in string.punctuation]))\n#hashtag_count\ntrain_df[\"hashtag_count\"] = train_df[\"text\"].apply(lambda x:len([c for c in str(x) if c==\"#\"]))\ntest_df[\"hashtag_count\"] = test_df[\"text\"].apply(lambda x:len([c for c in str(x) if c==\"#\"]))\n\n#mention_count\ntrain_df[\"mention_count\"] = train_df[\"text\"].map(lambda x: len([c for c in str(x) if c==\"@\"]))\ntest_df[\"mention_count\"] = test_df[\"text\"].map(lambda x: len([c for c in str(x) if c==\"@\"]))","76602062":"# Heureunder, we will explore the distribution of each mega-feature per target and per dataset\n# (train & test)\nMETA_FEATURES = [\"word_count\",\"unique_word_count\",\"stop_word_count\",\"url_count\",\\\n                 \"mean_word_length\",\"char_count\",\"punctuation_count\",\"hashtag_count\",\"mention_count\"]\nfig,ax = plt.subplots(nrows=len(META_FEATURES),ncols=2,figsize=(20,50),dpi=100)\nmask = train_df[\"target\"]==1\nfor i,feature in enumerate(META_FEATURES):\n\n    \n   sns.distplot(train_df[mask][feature],ax=ax[i,0],label=\"Disaster\",kde=False)\n   sns.distplot(train_df[~mask][feature],ax=ax[i,0],label=\"Not Disaster\",kde=False)\n   ax[i,0].set_title(\"{} target distribution in trainning dataset\".format(feature),fontsize=13)\n \n   sns.distplot(train_df[feature],ax=ax[i,1],label=\"Train Dataset\",kde=False)\n   sns.distplot(test_df[feature],ax=ax[i,1],label=\"Test Dataset\",kde=False)\n   ax[i,1].set_title(\"{} training and test dataset distributions \".format(feature),fontsize=13)\n   for j in range(2):\n        ax[i,j].set_xlabel(\" \")\n        ax[i,j].tick_params(axis=\"x\",labelsize=13)\n        ax[i,j].tick_params(axis=\"y\",labelsize=13)\n        ax[i,j].legend()\nplt.show()\n    ","a40f56d2":"fig,ax = plt.subplots(1,2,figsize=(20,6))\n\ntrain_df.groupby(\"target\").count()[\"id\"].plot(kind=\"pie\",labels=[\"Not Disaster\",\"Disaster\"],\\\n                                              autopct=\"%1.1f pourcents\",ax=ax[0])\nsns.countplot(x=train_df[\"target\"],hue=train_df[\"target\"],ax=ax[1])\nax[1].set_xticklabels([\"Non Disaster\",\"Disaster\"])\nax[0].tick_params(axis=\"x\",labelsize=15)\nax[0].tick_params(axis=\"y\",labelsize=15)\nax[0].set_ylabel(\"\")\nax[1].tick_params(axis=\"x\",labelsize=15)\nax[1].tick_params(axis=\"y\",labelsize=15)\nax[0].set_title(\"Target distribution in training set\",fontsize=13)\nax[1].set_title(\"Target count in training set\",fontsize=13)","ad571c72":"# create the function which will be able to generate n_grams for each row of dataset.\ndef gen_n_grams(text,n_grams=1):\n    \"\"\" This function allow to extract the n_gram in the introduced text.\n    \n      @param text(str): the text that the function, will use to extract features (n_grams).\n      @param n_grams(int): the length of n_gram, that we will use.\n      @return ngrams(list): list of the ngrams in the intriduced text.\n    \"\"\"\n    tokens = [token for token in str(text).lower().split() if token not in stopwords.words(\"english\")]\n    ngrams = zip(*[tokens[i:] for i in range(n_grams)])\n    \n    return [\" \".join(gram) for gram in ngrams]\n\n# create the function which will be able to generate dataframe of n_gram features for disaster\n# and non disaster tweets.\ndef gen_df_ngrams(n_grams=1):\n    \"\"\" This function, allow to generate dataframes for n_grams in disaster tweets and non \n        disaster tweet\n    \"\"\"\n    mask = train_df[\"target\"]==1\n    disaster_unigrams = defaultdict(int)\n    non_disaster_unigrams = defaultdict(int)\n    \n    for tweet in train_df.loc[mask,\"text\"].values:\n        for gram in gen_n_grams(tweet,n_grams=n_grams):\n            disaster_unigrams[gram] +=1\n    for tweet in train_df.loc[~mask,\"text\"].values:\n        for gram in gen_n_grams(tweet,n_grams=n_grams):\n            non_disaster_unigrams[gram] +=1\n    df_disaster_n_grams = pd.DataFrame(sorted(disaster_unigrams.items(),reverse=True,key=\\\n                                              lambda item:item[1]))\n    df_non_disaster_n_grams = pd.DataFrame(sorted(non_disaster_unigrams.items(),reverse=True,key=\\\n                                                  lambda item:item[1]))\n    return df_disaster_n_grams,df_non_disaster_n_grams\n\n# Define function, which allow to plot the N most occured n_gram in disaster tweet \n# and non disaster tweet.\n\ndef plot_ngrams(df_disaster_n_grams,df_non_disaster_unigrams,N=100,n_grams=1):\n    \"\"\"This function,allow to plot the top most n_grams in disaster tweet and non disaser tweet.\n    \"\"\"\n    fig,ax = plt.subplots(1,2,figsize=(18,50))\n    sns.barplot(y=df_disaster_n_grams[0].values[:N],x=df_disaster_n_grams[1].values[:N],ax=ax[0],\\\n               color=\"red\")\n    for i in range(2):\n        ax[i].tick_params(axis=\"x\",labelsize=15)\n        ax[i].tick_params(axis=\"y\",labelsize=15)\n        ax[i].set_xlabel(\"Occurences\")\n        ax[i].spines[\"right\"].set_visible(False)\n    sns.barplot(y=df_non_disaster_unigrams[0].values[:N],x=df_non_disaster_unigrams[1].values[:N],\\\n               ax=ax[1],color=\"green\")\n    ax[0].set_title(\"Top most {} {}_grams for disaster tweets\".format(N,n_grams),size=15)\n    ax[1].set_title(\"Top most {} {}_grams for non disaster tweets\".format(N,n_grams),size=15)","66710f90":"# implement the previous functions dor case n_gram = 1.\ndf_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams()\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams)","b5b25ca7":"# extract the most 100 bigrams per target (Disaster and not Disaster)\ndf_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams(n_grams=2)\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams,n_grams=2)","38e32d93":"# extract the most 100 bigrams per target (Disaster and not Disaster) when n_grams=3\ndf_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams(n_grams=3)\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams,n_grams=3)","3b08019c":"%%time\nglove_embeddings = np.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', allow_pickle=True)\nfasttext_embeddings = np.load('..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl', allow_pickle=True)","025df967":"def build_vocab(X):\n    \"\"\" This  function allow to generate a dict, which summarize each the frequency of each\n        word in the introduced corpus.\n        \n        @ params X(Series) : the corpus that the function, will analyse.\n        @ return vocab(dict) : dictionnary, which give each word in the corpus with his frequency\n                               occurrence.\n    \"\"\"\n    tweets = X.apply(lambda x : x.split()).values\n    vocab = {}\n    for tweet in tweets:\n        \n        for word in tweet:\n            try:\n               vocab[word] +=1\n            except KeyError:\n               vocab[word] = 1\n    return vocab","b6f62aca":"def check_embedding_coverage(X,embedding):\n    \"\"\"This function allow to check the coverage of the input corpus by the introduced embedding\n        \n        \n       @param  X(Series) : the introduced corpus that the function, check his coverage by the embedding\n       @param embedding(dict) : the words represented in the introduced embedding with their weights.\n       \n       @return oov(dict) : A dict of words in the vocab, which not presented the embedding technic.\n       @return coverage(int) : the percentil of word, hich had been presented in the embedding technic\n       @return text_coverage (int): the percentage of text which can presented by the embedding.\n    \n    \"\"\"\n    vocab = build_vocab(X)\n    covered = {}\n    oov ={}\n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab :\n        try:\n            covered[word] = embedding[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n    coverage = len(covered) \/ len(vocab)\n    text_coverage = n_covered \/ (n_covered + n_oov)\n    oov = sorted(oov.items(),key = operator.itemgetter(1),reverse = True)\n    \n    return  oov,coverage,text_coverage","79937821":"# Glove technic\ntrain_glove_oov,train_glove_coverage,train_glove_text = check_embedding_coverage(train_df[\"text\"],glove_embeddings)\ntest_glove_oov,test_glove_coverage,test_glove_text = check_embedding_coverage(test_df[\"text\"],glove_embeddings)\nprint(\"Glove embedding cover {} of vocabulary and {} of text in the training dataset\".format(train_glove_coverage,train_glove_text))\nprint(\"Glove embedding cover \u00a0{} of vocabularly and {} of text in the test dataset\".format(test_glove_coverage,test_glove_text))","ee776504":"# FasText technic\ntrain_fastext_oov,train_fastext_coverage,train_fastext_text = check_embedding_coverage(train_df[\"text\"],fasttext_embeddings)\ntest_fastext_oov,test_fastext_coverage,test_fastext_text = check_embedding_coverage(test_df[\"text\"],fasttext_embeddings)\nprint(\"FastText embedding cover {} of vocabulary and {} of text in the training dataset\".format(train_fastext_coverage,train_fastext_text))\nprint(\"FastText embedding cover {} of vocabularly and {} of text in the test dataset\".format(test_fastext_coverage,test_fastext_text))","7898d8a9":"\ndef clean(tweet): \n            \n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen\/Buy\", \"Listen \/ Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\n    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n    tweet = re.sub(r\"andword\", \"and word\", tweet)\n    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n           \n    # Urls\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n        \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n        \n    # ... and ..\n    tweet = tweet.replace('...', ' ... ')\n    if '...' not in tweet:\n        tweet = tweet.replace('..', ' ... ')      \n        \n    # Acronyms\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"m\u00cc\u00bcsica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n    \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n    \n    return tweet","8f5ef2a5":"# Cleaning text\ntrain_df[\"text_cleaned\"] = train_df[\"text\"].apply(lambda s:clean(s))\ntest_df[\"text_cleaned\"] = test_df[\"text\"].apply(lambda s:clean(s))","c56cfcf9":"# Gloves coverage after cleaning text.\ntrain_glove_oov,train_glove_coverage,train_glove_text = check_embedding_coverage(train_df[\"text_cleaned\"],glove_embeddings)\ntest_glove_oov,test_glove_coverage,test_glove_text = check_embedding_coverage(test_df[\"text_cleaned\"],glove_embeddings)\nprint(\"Glove embedding cover {} of vocabulary and {} of text in the training dataset\".format(train_glove_coverage,train_glove_text))\nprint(\"Glove embedding cover \u00a0{} of vocabularly and {} of text in the test dataset\".format(test_glove_coverage,test_glove_text))","8d51e607":"# FastText coverage after cleaning text.\ntrain_fastext_oov,train_fastext_coverage,train_fastext_text = check_embedding_coverage(train_df[\"text_cleaned\"],fasttext_embeddings)\ntest_fastext_oov,test_fastext_coverage,test_fastext_text = check_embedding_coverage(test_df[\"text_cleaned\"],fasttext_embeddings)\nprint(\"FastText embedding cover {} of vocabulary and {} of text in the training dataset\".format(train_fastext_coverage,train_fastext_text))\nprint(\"FastText embedding cover {} of vocabularly and {} of text in the test dataset\".format(test_fastext_coverage,test_fastext_text))","f7e551ca":"del(train_fastext_oov)\ndel(test_fastext_oov)\ndel(train_glove_oov)\ndel(test_glove_oov)\ndel(glove_embeddings)\ndel(fasttext_embeddings)","5ac2111e":"missalabeled_text = train_df.groupby(\"text\").nunique().sort_values(by=\"target\",ascending =False)\ndf =missalabeled_text[missalabeled_text[\"target\"] > 1]\ndf.index.tolist()","c26849a5":"train_df[\"rebuild_target\"] = train_df[\"target\"].copy()\ntrain_df.loc[train_df[\"text\"]==\"like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"Hellfire! We don\\x89\u00db\u00aat even want to think about it or mention it so let\\x89\u00db\u00aas not do anything that leads to it #islam!\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"To fight bioterrorism sir.\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"Hellfire is surrounded by desires so be careful and don\\x89\u00db\u00aat let your desires control you! #Afterlife\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"Caution: breathing may be hazardous to your health.\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\",\"rebuild_target\"]=0\ntrain_df.loc[train_df[\"text\"]==\"that horrible sinking feeling when you\\x89\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\",\"rebuild_target\"]=0","7c875c7b":"k=2\nSEED= 1337\nsk = StratifiedKFold(n_splits=k,random_state=SEED,shuffle=True)\n\nDisaster = train_df[\"target\"] == 1\nprint(\"Whole Training Set Shape = {}\".format(train_df.shape))\nprint(\"Whole Training Set Unique keyword Count = {}\".format(train_df[\"keyword\"].nunique()))\nprint(\"Whole training Set Target Rate (Disaster) {}\/{}\".format(train_df[Disaster][\"rebuild_target\"].count(),\\\n                                                              train_df[~Disaster][\"rebuild_target\"].count()))\nfor fold,(tr_idx,val_idx) in enumerate(sk.split(train_df[\"text_cleaned\"],train_df[\"rebuild_target\"]),\\\n                                       1):\n    print(\"\\nFold {} Training Set Shape={} - Validation Set Shape={}\".format(fold,train_df.loc[tr_idx,\"text_cleaned\"].shape,\\\n                                                                            train_df.loc[val_idx,\"text_cleaned\"].shape))\n    print(\"Fold {} Training Set unique keywords count{} - Validation Set unique keywords count{}\".format(fold,\\\n                                                                                                     train_df.loc[tr_idx,\"keyword\"].nunique(),train_df.loc[val_idx,\"keyword\"]\\\n                                                                                                    .nunique()))\n    ","6ec5fa13":"class ClassificationReport(Callback):\n    \n    def __init__(self,train_data=(),val_data=()):\n        super(Callback,self).__init__()\n        self.X_train,self.y_train = train_data\n        self.train_precision_scores = []\n        self.train_recall_scores = []\n        self.train_f1_scores = []\n        \n        self.X_val,self.y_val = val_data\n        self.val_precision_scores = []\n        self.val_recall_scores = []\n        self.val_f1_scores = []\n    \n    def on_epoch_end(self,epoch,logs={}):\n        \n        train_prediction = np.round(self.model.predict(self.X_train,verbose=0))\n        train_precision = precision_score(self.y_train,train_prediction,average=\"macro\")\n        train_recall = recall_score(self.y_train,train_prediction,average=\"macro\")\n        train_f1 = f1_score(self.y_train,train_prediction,average=\"macro\")\n        self.train_precision_scores.append(train_precision)\n        self.train_recall_scores.append(train_recall)\n        self.train_f1_scores.append(train_f1)\n        \n        val_prediction = np.round(self.model.predict(self.X_val,verbose=0))\n        val_precision = precision_score(self.y_val,val_prediction,average=\"macro\")\n        val_recall = recall_score(self.y_val,val_prediction,average=\"macro\")\n        val_f1 = f1_score(self.y_val,val_prediction,average=\"macro\")\n        self.val_precision_scores.append(val_precision)\n        self.val_recall_scores.append(val_recall)\n        self.val_f1_scores.append(val_f1)\n        \n        print(\"\\n Epoch: {} - Training Precision: {} - Training Recall: {} - Training F1: {}\".format(\\\n                                                                                                   epoch+1,train_precision,train_recall,train_f1))\n        \n        print(\"\\n Epoch: {} - Validation Precision: {} - Validation Recall: {} - Validation F1: {}\".format(\\\n                                                                                                          epoch+1,val_precision,val_recall,val_f1))\n        \n        ","b49d4f7b":"class BertDisasterDetecter:\n    \n    def __init__(self,max_seq_length=128,epoch=100,batch_size=128,lr=1e-3):\n        \n        #Bert layer\n        \n        self.bert=hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=True)\n        self.epoch = epoch\n        self.lr = lr\n        self.max_seq_length = max_seq_length\n        self.batch_size = batch_size\n        \n        \n        vocab_file = self.bert.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = self.bert.resolved_object.do_lower_case.numpy()\n        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n        self.models = []\n        self.scores = {}\n        \n    def encode(self,texts):\n\n        all_tokens = []\n        all_mask = []\n        all_segment = []\n        for text in texts:\n            tx= self.tokenizer.tokenize(text)\n            tx = ['[CLS]']+tx[:self.max_seq_length - 2] + ['[SEP]']\n            pad_len = self.max_seq_length - len(tx) \n            tokens = self.tokenizer.convert_tokens_to_ids(tx)\n            tokens += [0] * pad_len\n            mask_ids = [1]*len(tx) + [0]*pad_len\n            segments_ids = [0] * self.max_seq_length\n            \n            all_tokens.append(tokens)\n            all_mask.append(mask_ids)\n            all_segment.append(segments_ids)\n            \n        return np.array(all_tokens),np.array(all_mask),np.array(all_segment)\n    \n    def build_model(self):\n        \n        input_word_ids = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n                                       name=\"input_word_ids\")\n        input_mask = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n                                   name=\"input_mask\")\n        segment_ids = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n                                    name=\"segment_ids\")\n        \n        pooled_output, sequence_output = self.bert([input_word_ids, input_mask, segment_ids])\n        x = sequence_output[:,0,:]\n        output = Dense(1,activation=\"sigmoid\")(x)\n        \n        model =Model(inputs=[input_word_ids,input_mask,segment_ids],outputs=output)\n        \n        model.compile(optimizer =SGD(self.lr,momentum=0.8),loss=\"binary_crossentropy\",metrics=['accuracy'])\n        \n        return model\n    def train(self):\n        \n        for fold,(train_ind,test_ind) in enumerate(sk.split(train_df[\"text_cleaned\"],train_df[\"keyword\"])):\n            \n            print(\"\\nFold {}\\n\".format(fold))\n            \n            Xtr = train_df.loc[train_ind,\"text_cleaned\"].str.lower()\n            Ytr = train_df.loc[train_ind,\"rebuild_target\"]\n            Xval = train_df.loc[test_ind,\"text_cleaned\"].str.lower()\n            Yval = train_df.loc[test_ind,\"rebuild_target\"]\n            \n            Xtr_encoded = self.encode(Xtr)\n            Xval_encoded = self.encode(Xval)\n            \n            model = self.build_model()\n            \n            metrics = ClassificationReport(train_data=(Xtr_encoded,Ytr),val_data=(Xval_encoded,Yval))\n            \n            model.fit(Xtr_encoded,Ytr,callbacks=metrics,batch_size=self.batch_size,epochs=self.epoch,\\\n                     validation_data=(Xval_encoded,Yval))\n            \n            self.models.append(model)\n            self.scores[fold] = {\n                'train':{\n                    \"precision\" : metrics.train_precision_scores,\n                    \"recall\":metrics.train_recall_scores,\n                    \"f1\":metrics.train_f1_scores},\n                \"validation\":{\n                    \"precision\":metrics.val_precision_scores,\n                    \"recall\":metrics.val_precision_scores,\n                    \"f1\":metrics.val_f1_scores\n                }\n                }\n        \n    def plot_curve(self):\n        \n        fig,axes = plt.subplots(nrows=k,ncols=2,figsize=(20,k*6),dpi=100)\n        \n        for i in range(k):\n           # Classification Reports\n           sns.lineplot(x=np.arange(1,self.epoch+1),y=self.models[i].history.history[\"val_accuracy\"],\\\n                       ax=axes[i][0],label=\"val_accuracy\")\n           sns.lineplot(x=np.arange(1,self.epoch+1),y=self.scores[i][\"validation\"][\"precision\"],\\\n                       ax=axes[i][0],label=\"val_precision\")\n           sns.lineplot(x=np.arange(1,self.epoch+1),y=self.scores[i][\"validation\"][\"recall\"],\\\n                       ax=axes[i][0],label=\"val_recall\")\n           sns.lineplot(x=np.arange(1,self.epoch+1),y=self.scores[i][\"validation\"][\"f1\"],\\\n                       ax=axes[i][0],label=\"val_f1\")\n           \n           axes[i][0].legend()\n           axes[i][0].set_title(\"Fold {} Validation classification Reports\".format(i),fontsize=14)\n           \n           # Loss Curve\n           \n           sns.lineplot(x=np.arange(1,self.epoch+1),y=self.models[i].history.history[\"loss\"],\\\n                       ax=axes[i][1],label=\"train_loss\")\n           sns.lineplot(x=np.arange(1,self.epoch+1),y=self.models[i].history.history[\"val_loss\"],\\\n                       ax =axes[i][1],label=\"val_loss\")\n            \n           axes[i][1].legend()\n           axes[i][1].set_title(\"Fold {} Train \/ Validation Loss\".format(i),fontsize=14)\n            \n           for j in range(2):\n             axes[i][j].set_xlabel('Epoch',size=12)\n             axes[i][j].tick_params(axis=\"x\",labelsize=12)\n             axes[i][j].tick_params(axis=\"y\",labelsize=12)\n        plt.show()\n        \n    def predict(self,X):\n        \n        encoded_X = self.encode(X)\n        prediction=[]\n        for i in range(k):\n            prediction.append(self.models[i].predict(encoded_X).T)\n        return np.mean(prediction,axis=0).T","021fef22":"clf = BertDisasterDetecter(max_seq_length=128,lr=1e-3,epoch=2,batch_size=32)\n\nclf.train()","76c75397":"clf.plot_curve()","1c9587a9":"ypred= clf.predict(test_df[\"text_cleaned\"].values)","0b9f3a73":"model_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nmodel_submission['target'] = np.round(ypred).astype('int')\nmodel_submission.to_csv('model_submission.csv', index=False)\nmodel_submission.describe()","b3757b3f":"# 3.Meta Features:","4e01f529":"# 1.Loading Datas:","084c0bd1":"# 4.Class imbalance:","c6008c90":"Distribution of meta features in classes and dataset can help to identify disaster tweets.\nHereunder,the meta features that we will use for our analysis:\n* word_count : number of words in text.\n* unique_word_count : number of unique words in text.\n* stop_word_count : number of stop words in text.\n* url_count : number of url in text.\n* mean_word_length : average character count in words.\n* char_count : number of characters in text.\n* punctuation_count : number of punctuation in text.\n* hashtag_count : number of hashtags (#) in text.\n* mention_count : number of mention tags (@) in text.\n","aa19322e":"## 2.1 Missing values :","c3e5a3a2":"## 5.2 Bigrams:","1e35801e":"# 0.Import required librairies:","a79f4dca":"# 6. Embedding and Text Cleaning:","dfb60367":"Hereunder, we will check the tweets wich are mentionned more than once in our training set with differents labels. This miselabeling could be come from various persons who can interpret differently the tweet, or an unclear tweets or just an human error.","235ca1b4":"==> It's obvious from the chart above , that keywords have signal because the distribution of each one is dominated by a label. Keyword can be used as feature by itself or added to each text.","30e0d6c5":"# 5.n_grams:","619b6918":"## 5.1 unigrams:","afba67bc":"We will do text cleaning based in the flowing embedding technics:\n\n* Glove-300d-840B\n* FastText-Crawl-300d-2M","0df1ca25":"# 9.2 Bert Model:","9253269b":"The percentil of missing values for the keyword and the location feature, are the barely the same in the train and the test datas. This means that the train and the test datas are provided from the same sample.","6a827f5e":"Tweets require lots of cleaning . i think it is inneficient to clean all the tweets in our corpus. We will made the following approach , in order to enhance the embedding coverage of vocab and text :\n* The most common type of words that require cleaning in oov have punctuations at the start or end. Those words doesn't have embeddings because of the trailing punctuations. Punctuations #, @, !, ?, +, &, -, $, =, <, >, |, {, }, ^, ', (, ),[, ], *, %, ..., ', ., :, ; are separated from words\n* Special characters that are attached to words are removed completely\nContractions are expanded\n* Urls are removed\n* Character entity references are replaced with their actual symbols\n* Typos and slang are corrected, and informal abbreviations are written in their long forms\n* Some words are replaced with their acronyms and some words are grouped into one\n* Finally, hashtags and usernames contain lots of information about the context but they are written without spaces in between words so they don't have embeddings. Informational usernames and hashtags should be expanded but there are too many of them. I expanded as many as I could, but it takes too much time to run clean function after adding those replace calls.","226b3eae":"## 5.3 Trigrams :","ab0573f4":"## 6.2 Text Cleaning :","50dab71d":"# 7.Miselabeling Text:","8deb29ab":"# 9.Modeling:","c26aef54":"## 9.1 Metric:","7ed2dd3c":"henceforth, we replace keyword missed values by \"no_keywords\" and location missing values by \"no_location\".","88c3a25e":"The chart above, show that the non disaster text represent about 57% of the whole tweet text in the training dataset. So we can not say that there is class imbalance, then no specific processing should be required to treat class imbalance in modeling step.","e72b21e1":"## 6.1 Embedding coverage:","3ed1f3f2":"# Training,evaluation and prediction:","dcff17e7":"# 2.Keyword and location :","7ead0d61":"# 8. Cross Validation:","83691764":"## 2.2 Target Distribution :","a71d3b7a":"BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. It was originally published by\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", 2018.\nThis TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at tensorflow\/models\/official\/nlp\/bert. It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads.\n\nThis model has been pre-trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs have been \"uncased\", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. For training, random input masking has been applied independently to word pieces (as in the original BERT paper).\n\nAll parameters in the module are trainable, and fine-tuning all parameters is the recommended ","2b12f174":"Based on the above chart, we can conculde the following statements:\n\n* All meta features, have the same distribution for training set and test set, which confirm that datas for the both set, are provided from the same sample.\n* All meta features, seem also have same distributions for both target (Disaster and Not Disaster), which mean that theses meta features seem not have informations about the target."}}