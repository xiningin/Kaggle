{"cell_type":{"264b71c0":"code","e844c32c":"code","d2365341":"code","15c5c5e3":"code","a0ede210":"code","00962649":"code","1993d32f":"code","f6e6373d":"code","597d632d":"code","84ee471a":"code","1627d5b6":"code","e980aa0f":"code","d66e54fd":"code","b81e9530":"code","d4bd5790":"code","9599e002":"code","24b2f50e":"code","cab3bf5b":"code","10393b99":"code","f08dc964":"code","6e2c4de5":"code","d12284ad":"code","75e6c399":"code","76aa361f":"code","f571965a":"code","eaab64e6":"code","cd8c0d4d":"code","b4b4136c":"code","63ef1ae5":"code","0075ab48":"code","63c04aaa":"code","ed985714":"code","e76c67fe":"code","7106b50e":"code","2b7ee261":"code","2ecbe0ac":"markdown","ba052292":"markdown","dc2f3f44":"markdown","9cfb2008":"markdown","9601fcfd":"markdown","16cad653":"markdown","63d070a5":"markdown","326c3d6a":"markdown","ac6a9908":"markdown","396007ae":"markdown","db78d2a0":"markdown","477f6947":"markdown","ea3e063a":"markdown","483ea839":"markdown","d69a9778":"markdown","213b116d":"markdown","9c2d8e5a":"markdown","6e63d4aa":"markdown","d8ed2218":"markdown","cf3b9225":"markdown","6cb78827":"markdown","d92ec20e":"markdown","c6040ac8":"markdown","91558d3b":"markdown","4df78232":"markdown","be226bed":"markdown","65178a00":"markdown","17722230":"markdown"},"source":{"264b71c0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n%matplotlib inline","e844c32c":"train = pd.read_csv('..\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip')","d2365341":"train.head()","15c5c5e3":"train.info()","a0ede210":"# Missing data per day\n\ndays = [r for r in range(train.shape[1] - 1)]\nfig, ax = plt.subplots(figsize = (10,7))\nplt.xlabel('Day')\nplt.ylabel('# of null data')\nax.axvline(x=123, c = 'red',  lw = 0.5)\nplt.plot(days, train.iloc[:,1:].isnull().sum())","00962649":"train.columns[123]","1993d32f":"# Histogram of pages with their number of data missing. \n\ntrain.isnull().sum(axis = 1).hist()","f6e6373d":"train = train.fillna(0)","597d632d":"train.Page","84ee471a":"import re\ndef split_page(page):\n  w = re.split('_|\\.', page)\n  return ' '.join(w[:-5]), w[-5], w[-2], w[-1]\n\nli = list(train.Page.apply(split_page))\ndf = pd.DataFrame(li)\ndf.columns = ['Title', 'Language', 'Access_type','Access_origin']\ndf = pd.concat([train, df], axis = 1)\ndel df['Page']","1627d5b6":"df.iloc[:, -4:]","e980aa0f":"df[df.Language == 'de'].iloc[:,-4:]","d66e54fd":"df.Language.value_counts().plot(kind = 'bar')","b81e9530":"df.Access_type.value_counts().plot(kind = 'bar')","d4bd5790":"df.Access_origin.value_counts().plot(kind = 'bar', color = 'orange')","9599e002":"sum_all = df.iloc[:,:-4].sum(axis = 0)\n\ndays = list(r for r in range(sum_all.shape[0]))\n\nfig = plt.figure(figsize = (10, 7))\nplt.xlabel('Days')\nplt.ylabel('Views')\nplt.title('Page View of All Pages')\nplt.plot(days, sum_all)\n","24b2f50e":"summap = {}\nlang_list = [\"en\", \"ja\", \"de\", \"fr\", \"zh\", \"ru\", \"es\", \"commons\", \"www\"]\nfor l in lang_list:\n  summap[l] = df[df.Language == l].iloc[:,:-4].sum(axis = 0)\/df[df.Language == l].shape[0]\n\nfig = plt.figure(figsize = (15, 7))\nplt.xlabel('Days')\nplt.ylabel('Views')\nplt.title('Average Page View by Language')\n\nfor key in summap:\n  plt.plot(days, summap[key], label = key)\nplt.legend()\nplt.show()\n\n\n","cab3bf5b":"from scipy.fftpack import fft\n\n#data = df.iloc[idx,0:-4]\n\nfig, ax = plt.subplots(figsize = (15, 7))\n\nfftmean = {}\nfftxvals = {}\n\nfor key in summap:\n  fftval = fft(df[df.Language == key].iloc[:, :-6])\n\n#calculate magnitude\n  fftmag = [np.sqrt(np.real(x)*np.real(x)+\n                    np.imag(x)*np.imag(x)) for x in fftval]\n  arr = np.array(fftmag)\n#calculate mean\n  fftmean[key] = np.mean(arr,axis=0)\n\n  fftxvals[key] = [day\/fftmean[key].shape[0] for day in range(fftmean[key].shape[0])]\n\n  npts = len(fftxvals[key])\/\/2 + 1\n  fftmean[key] = fftmean[key][:npts]\/fftmean[key].shape[0]\n  fftxvals[key] = fftxvals[key][:npts]\n  ax.plot(fftxvals[key][1:], fftmean[key][1:], label = key)\n\nplt.axvline(x = 1\/7, color = 'black', lw = 0.5)\nplt.axvline(x = 2\/7, color = 'black', lw = 0.5)\nplt.axvline(x = 3\/7, color = 'black', lw = 0.5)\n\nplt.xlabel('Frequency')\nplt.ylabel('Views')\nplt.title('Fourier Transform of Average View by Language')\n\nplt.legend()\nplt.show()","10393b99":"sums = pd.concat([df.iloc[:,-4:], df.iloc[:,:-4].sum(axis = 1)], axis = 1)\nsums.columns = ['Title', 'Language', 'Access_type', 'Access_origin', 'sumvalues']","f08dc964":"max_list = {}\nfor l in lang_list:\n  lang_sums = sums[sums.Language == l]\n  max_list[l] = lang_sums.sumvalues.idxmax()","6e2c4de5":"df[df.index.isin(max_list.values())].iloc[:,-4:]","d12284ad":"import matplotlib as mpl\nmpl.rcParams['font.family'] = 'AppleGothic'\n\ndef plot_trend(lang, idx):\n    fig = plt.figure(1,figsize=(10,5))\n    plt.plot(days, df.iloc[idx,:-4])\n    plt.xlabel('day')\n    plt.ylabel('views')\n    plt.title('Most Viewed Pages ({})'.format(lang))  \n    plt.show()","75e6c399":"for key in max_list:\n  plot_trend(key, max_list[key])","76aa361f":"sums2 = sums.drop(labels = max_list.values(), axis = 0)\nmax_list2 = {}\nfor l in lang_list:\n  lang_sums = sums2[sums2.Language == l]\n  max_list2[l] = lang_sums.sumvalues.idxmax()\n  \ndf[df.index.isin(max_list2.values())].iloc[:,-4:]","f571965a":"main_titles = dict(zip(list(df[df.index.isin(max_list.values())].Language), list(df[df.index.isin(max_list.values())].Title)))\n\nall_access = {}\nmobile_access = {}\ndesktop_access = {}\n\nfor l in lang_list:\n  all_access[l] = df.index[(df.Language == l) & (df.Title == main_titles[l]) & (df.Access_type == 'all-access')]\n  mobile_access[l] = df.index[(df.Language == l) & (df.Title == main_titles[l]) & (df.Access_type == 'mobile-web')]\n  desktop_access[l] = df.index[(df.Language == l) & (df.Title == main_titles[l]) & (df.Access_type == 'desktop')]","eaab64e6":"def plot_trend_access_type(lang):\n\n    plt.figure(figsize=(15,4))\n\n    plt.subplot(1,3, 1)\n    plt.plot(days, df.iloc[all_access[l][0],:-4])\n    plt.title('All Access ({})'.format(lang))\n    plt.subplot(1,3, 2)\n    plt.plot(days, df.iloc[mobile_access[l][0],:-4])\n    plt.title('Mobile-web Access ({})'.format(lang))\n    plt.subplot(1,3, 3)\n    plt.plot(days, df.iloc[desktop_access[l][0],:-4])\n    plt.title('Desktop Access ({})'.format(lang))\n    plt.show()","cd8c0d4d":"for l in lang_list:\n  plot_trend_access_type(l)","b4b4136c":"def plot_trend_access_origin(lang):\n\n    plt.figure(figsize=(10,3))\n\n    plt.subplot(1,2, 1)\n    plt.plot(days, df.iloc[all_access[l][0],:-4])\n    plt.title('{} ({})'.format(df.iloc[all_access[l][0],:].Access_origin,lang))\n    plt.subplot(1,2, 2)\n    plt.plot(days, df.iloc[all_access[l][1],:-4])\n    plt.title('{} ({})'.format(df.iloc[all_access[l][1],:].Access_origin, lang))\n    plt.show()\n  \nfor l in lang_list:\n    plot_trend_access_origin(l)","63ef1ae5":"# Split the data into train and test\n\nseries = df.iloc[:, 0:-4]\n\nfrom sklearn.model_selection import train_test_split\n\nX = series.iloc[:,:500]\ny = series.iloc[:,500:]\n\nX_train, X_val, y_train, y_val = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n","0075ab48":"from statsmodels.tsa.arima_model import ARIMA\n\ntrain, test = X_train[86431], y_train[86431]\nrecord = [x for x in train]\npredictions = list()\n# walk-forward validation\nfor t in range(len(test)):\n\t# fit model\n\tmodel = ARIMA(record, order=(4,1,0))\n\tmodel_fit = model.fit(disp=False)\n\t# forecast one step\n\tyhat = model_fit.forecast()[0]\n\t# store the result\n\tpredictions.append(yhat)\n\trecord.append(test[t])\n","63c04aaa":"from math import sqrt\nfrom sklearn.metrics import mean_squared_error\n# evaluate forecasts\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('RMSE: %.3f' % rmse)\n# plot forecasts against actual outcomes\nfig = plt.subplots(figsize=(10,7))\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.legend(['test', 'prediction'])\nplt.title('ARIMA with Walk-foward validation')\nplt.show()","ed985714":"\n# evaluate an ARIMA model for a given order (p,d,q) with MSE\ndef evaluate_arima_model(train, test, arima_order):\n\t# prepare training dataset\n\trecord = [x for x in train]\n\t# make predictions\n\tpredictions = list()\n\tfor t in range(len(test)):\n\t\tmodel = ARIMA(record, order=arima_order)\n\t\tmodel_fit = model.fit(disp=0)\n\t\tyhat = model_fit.forecast()[0]\n\t\tpredictions.append(yhat)\n\t\trecord.append(test[t])\n\t# calculate out of sample error\n\terror = mean_squared_error(test, predictions)\n\taic_score= model_fit.aic\n\treturn error, aic_score\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(train, test, p_values, d_values, q_values):\n\ttrain, test = train.astype('float32'), test.astype('float32')\n\tbest_score, best_cfg = float(\"inf\"), None\n\tfor p in p_values:\n\t\tfor d in d_values:\n\t\t\tfor q in q_values:\n\t\t\t\torder = (p,d,q)\n\t\t\t\ttry:\n\t\t\t\t\tmse, aic = evaluate_arima_model(train, test, order)\n\t\t\t\t\tif mse < best_score:\n\t\t\t\t\t\tbest_score, best_cfg = mse, order\n\t\t\t\t\t\taic_out = aic\n\t\t\t\t\tprint('ARIMA  ', order,    'MSE=%.3f   AIC=%.3f' % ( mse, aic))\n\t\t\t\texcept:\n\t\t\t\t\tcontinue\n\t#print('Best ARIMA:    ', best_cfg,  'MSE=%.3f  AIC=%.3f' % (best_cfg, best_score))\n","e76c67fe":"\np_values = [0, 5]\nd_values = range(0, 2)\nq_values = range(4, 8)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(X_train[86431], y_train[86431], p_values, d_values, q_values)","7106b50e":"p_values = [0, 1]\nd_values = [0,1]\nq_values = [5,7]\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(X_train[86431], y_train[86431], p_values, d_values, q_values)","2b7ee261":"train, test = X_train[86431], y_train[86431]\nrecord = [x for x in train]\npredictions = list()\n# walk-forward validation\nfor t in range(len(test)):\n\t# fit model\n\tmodel = ARIMA(record, order=(1,0,5))\n\tmodel_fit = model.fit(disp=False)\n\t# forecast one step\n\tyhat = model_fit.forecast()[0]\n\t# store the result\n\tpredictions.append(yhat)\n\trecord.append(test[t])\n \n# evaluate forecasts\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('RMSE: %.3f' % rmse)\n# plot forecasts against actual outcomes\nfig = plt.subplots(figsize=(10,7))\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.plot(predictions, color='red')\nplt.legend(['test', 'prediction'])\nplt.title('ARIMA with Walk-foward validation 2')\nplt.show()","2ecbe0ac":"* There are three types of access: all-access, mobike, and desktop","ba052292":"- The most viewed pages are the main portal pages of Wikipedia. \n","dc2f3f44":"* OK, so we have quite lots of null data in the beggining, but decreases as the time goes.\n* Perhaps, some pages were newly created?\n* However, many pages have no missing data as seen from the second diagram.","9cfb2008":"* There are clear peaks at 1\/7, 2\/7 and 3\/7. They are likely to be the weekly trends as we have 7 days per week. \n* Trends in longer terms (smaller frequency) depend on the language. ","9601fcfd":"- So in general,  MSE is smaller at p=0, d=0 and d around 5 to 7.\n- In deed, the best combination was when p=0,d = 0 and q = 5.","16cad653":"\n- The shapes of the graphs differ a lot depending on the access type. \n- In most of the languages, there are peaks around 300th days, which could be because of the 2016 summer Olympics from Rio. \n- For the case of English, Japanese and French, there are overall decrease in Mobile Access but increase in Desktop Access for some reason... \n- Possible reasons could be... mobile users shifted to desktop? Was there new macbook released?  Or mobile wikipedia degraded? \n","63d070a5":"Okay, so now we have global categorical features. Let's take a look at the time series data.","326c3d6a":"Okay, so now the Page name is split into \n* Title (title of the page)\n* Language (languaeg written with)\n* Access_type (the type of access)\n*Acceess_origin (the type of access) \n","ac6a9908":"- So compared to the previous model, the model is not cpaturing the large spikes, but did not cause any huge error either, which is why this model had the lowest MSE.","396007ae":"## Missing Data","db78d2a0":"- Why don't we compare the access type to see if there is any difference?","477f6947":"# Prediction\nI will use the first 500 days to predict the last 50 days.\n\n## ARIMA\nLet's try ARIMA to predict the views from the time series data.","ea3e063a":"So, our data consist of...\n* Page:  The name of each wikipedia page.\n* yyyy-mm-dd :  The days views were recorded. (2015-07-01 - 2016-12-31,  550 days in total) \n","483ea839":"- Spider seem to be more constant than all-agent. Perhaps, they are less events driven than normal access.","d69a9778":"- Each page has unique trend features. \n- There are some weird spikes as well.\n- OK, so what about the second largest?","213b116d":"# <center>Time Series Data with ARIMA Model<\/center>\n\n## Table of contents\n> ### Overview of Data\n> ### Feature Engineering\n> ### Prediction\n","9c2d8e5a":"## Feature Engineering\nFirst I will take a look at each page name. The general format is \n\n```\nSPECIFIC NAME _ LANGUAGE.wikipedia.org _ ACESS TYPE _ ACCESS ORIGIN\n``` \n\nSo we can split them with the underscore and dots.","6e63d4aa":"* There are actually pages with unspecific languages: commons and www.\n* Others are English, Japanese, German, French, Chinese, Russian, Spanish","d8ed2218":"Okay, so now I will take a look at individual page.\nLet's check the most viewed pages for each language.","cf3b9225":"I will fill them with 0 as it makes it easier to continue.","6cb78827":"Let's see how the parameters p, d, q affect our ARIMA model.\n- p: Determines the time range to obtain the auto regression (AR)\n- q: Determines the time range to obtain moving avereage (MV)\n- d: Determines the level of differentiation.","d92ec20e":"* The overall sum is largelly affected by the English trend. \n* It is difficult to see the trends of minor languages so I will use Forier Transform.","c6040ac8":"So the best combination was actually 1, 0 ,5.\n\nLet's visualize and compare to the previous result.\n","91558d3b":"## Overview of Data","4df78232":"* The acess origins are either all-agents or spider. ","be226bed":"- The second most viewed ones are from main pages in different access types. \n- Let's compare the trend by their access type then.\n","65178a00":"First, let's take a look at the data we will use.","17722230":"Let's use walk-forward validation technique to see how the ARIMA model learns and predicts. It will take a lot of time to predict all dataset so I will just use the main page in Japanese."}}