{"cell_type":{"81db939b":"code","5c552bad":"code","e4f79dfc":"code","ffe66478":"code","5523bd30":"code","5af3704a":"code","2f819c1c":"code","633c0534":"code","6698292b":"code","bee733bd":"code","ecf5c1a4":"code","ddb6598e":"code","23d98d3a":"code","9b9aff4b":"code","0548d7d4":"code","03ba88d0":"code","233223d5":"code","f044cf75":"code","dea4151b":"code","bd567248":"code","b92d5cf5":"code","ed23cd83":"code","087dd4e3":"code","96be8d55":"code","44b9157f":"code","98c919cf":"code","86640a91":"code","68feae91":"code","11764ca6":"code","456ed660":"code","a742423c":"code","82bac0fd":"code","5bcc48dc":"code","43c615d9":"markdown","df4e42a1":"markdown","0cdf672c":"markdown","a818ef23":"markdown","103c84c2":"markdown","cf39dd7b":"markdown","0dc6934a":"markdown","01e83fef":"markdown"},"source":{"81db939b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c552bad":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn import preprocessing \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report \nfrom sklearn.metrics import accuracy_score","e4f79dfc":"sample_prediction= pd.read_csv('..\/input\/loan-prediction-based-on-customer-behavior\/Sample Prediction Dataset.csv')\nsample_prediction.head()","ffe66478":"test_data = pd.read_csv('..\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv')\ntest_data.head()","5523bd30":"training_data= pd.read_csv('..\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')\ntraining_data.head()","5af3704a":"training_data.head()","2f819c1c":"training_data.shape","633c0534":"training_data[\"Risk_Flag\"].unique()","6698292b":"training_data.describe()","bee733bd":"training_data.isnull().sum() #no null values ","ecf5c1a4":"plt.subplot(1,4,1)\nplt.subplots_adjust(left=0,right=3,bottom=1,top=2,wspace=0.2,hspace=0.4)\nlabel_age=['single','married']\nplt.title(\"Married\/Single\")\nplt.pie(training_data.groupby(training_data[\"Married\/Single\"]).size(),labels=label_age)\n\nplt.subplot(1,4,2)\nlabel_ed=['rented','norent_noown','owned']\nplt.title(\"House_Ownership\")\nplt.pie(training_data.groupby(training_data[\"House_Ownership\"]).size(),labels=label_ed)\n\nplt.subplot(1,4,3)\nlabel_car=['yes','no']\nplt.title(\"car_ownership\")\nplt.pie(training_data.groupby(training_data[\"Car_Ownership\"]).size(),labels=label_car)\n\nplt.subplot(1,4,4)\nplt.title(\"Risk_Flag\")\nlabel_r=[0,1]\nplt.pie(training_data.groupby(training_data[\"Risk_Flag\"]).size(),labels=label_r)","ddb6598e":"import pandas as pd \ntrain=pd.read_csv(\"..\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv\")\ncorrelations=train.corr()\nprint(correlations)","23d98d3a":"train.corr()","9b9aff4b":"print(correlations['Risk_Flag'])","0548d7d4":"## always remember to use magic function in jupyter notebook :)\n%matplotlib inline \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nsns.heatmap(correlations)\nplt.show()","03ba88d0":"test= pd.read_csv(\"..\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv\")\ncorrelation2= test.corr()\nprint(correlation2)","233223d5":"test.corr()","f044cf75":"%matplotlib inline \nsns.heatmap(correlation2)\nplt.show()","dea4151b":"sample_prediction= pd.read_csv(\"..\/input\/loan-prediction-based-on-customer-behavior\/Sample Prediction Dataset.csv\")\ncorrelation3= sample_prediction.corr()\nprint(correlation3)","bd567248":"sns.heatmap(correlation3)\nplt.show()","b92d5cf5":"#our target feature is Risk_Flag lets first find EDA of Risk_Flag \nimport pandas as pd \ntraining_data=pd.read_csv(\"..\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv\")\nprint(training_data['Risk_Flag'].describe())\nplt.figure(figsize=(15,5))\nsns.distplot(training_data['Risk_Flag'],color='g',bins=100,hist_kws={'alpha':0.4});","ed23cd83":"training_data.hist(figsize=(16,20),bins=50,xlabelsize=8,ylabelsize=8)","087dd4e3":"for i in range(0, len(training_data.columns), 5):\n    sns.pairplot(data=training_data,\n                x_vars=training_data.columns[i:i+5],\n                y_vars=['Risk_Flag'])","96be8d55":"test_data.hist(figsize=(16,20),bins=50,xlabelsize=8,ylabelsize=8)","44b9157f":"for i in range(0, len(test_data.columns), 5):\n    sns.pairplot(data=test_data,\n                x_vars=test_data.columns[i:i+5],\n                y_vars=['Car_Ownership'])","98c919cf":"for i in range(0, len(test_data.columns), 5):\n    sns.pairplot(data=test_data,\n                x_vars=test_data.columns[i:i+5],\n                y_vars=['House_Ownership'])","86640a91":"import seaborn as sns \nfrom sklearn import preprocessing \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","68feae91":"#label encoding \nlabel_ms=preprocessing.LabelEncoder()\ntraining_data[\"Married\/Single\"]=label_ms.fit_transform(training_data[\"Married\/Single\"])\n\nlabel_ho=preprocessing.LabelEncoder()\ntraining_data[\"House_Ownership\"]=label_ho.fit_transform(training_data[\"House_Ownership\"])\n\nlabel_co=preprocessing.LabelEncoder()\ntraining_data[\"Car_Ownership\"]=label_co.fit_transform(training_data[\"Car_Ownership\"])\n","11764ca6":"training_data.head()","456ed660":"training_data.columns","a742423c":"#splitting of data\nX=training_data[['Income','Age','Experience','Married\/Single','House_Ownership','Car_Ownership','CURRENT_JOB_YRS','CURRENT_HOUSE_YRS']]\ny=training_data['Risk_Flag']","82bac0fd":"train_x, test_x, train_y, test_y=train_test_split(X, y, test_size=0.30, random_state=0)\nprint(train_x.shape)\nprint(test_x.shape)\nprint(train_y.shape)\nprint(test_y.shape)","5bcc48dc":"import numpy as  np\nimport pandas as pd \nfrom sklearn.metrics import confusion_matrix\n#from sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\ndef printdata():\n    print(\"Dataset Length: \",len(training_data))\n    print(\"Dataset shape: \",training_data.shape)\n\n    #printing dataset observations \n    print(\"dataset: \",training_data.head())\n    return training_data\n\n#function to split dataset \ndef splitdataset(training_data):\n    #seperating the target variable\n    X=training_data[['Income','Age','Experience','Married\/Single','House_Ownership','Car_Ownership','CURRENT_JOB_YRS','CURRENT_HOUSE_YRS']]\n    Y=training_data['Risk_Flag']\n    \n    #splitting dataset into train and test \n    X_train,X_test,y_train,y_test = train_test_split(X,Y,random_state=100)\n    \n    return X,Y,X_train,X_test,y_train,y_test\n\n#function to perform training with gini_index\ndef train_using_gini(X_train,X_test,y_train):\n    #creating the classifier object \n    clf_gini= DecisionTreeClassifier(criterion=\"gini\",random_state=100, max_depth=3,min_samples_leaf=5)\n    \n    #performing training \n    clf_gini.fit(X_train,y_train)\n    return clf_gini\n\n#function to perform training with entropy \ndef train_using_entropy(X_train,X_test,y_train):\n    \n    #decision tree with entropy\n    clf_entropy=DecisionTreeClassifier(criterion=\"entropy\",random_state=100,max_depth=3,min_samples_leaf=5)\n    \n    #performing training \n    clf_entropy.fit(X_train,y_train)\n    return clf_entropy\n\n#function to make predictions \ndef prediction(X_test,clf_object):\n    \n    #prediction on test with ginniIndex\n    y_pred = clf_object.predict(X_test)\n    print(\"Predicted values:\")\n    print(y_pred)\n    return y_pred\n      \n    # Function to calculate accuracy\ndef cal_accuracy(y_test, y_pred):\n      \n    print(\"Confusion Matrix: \",\n        confusion_matrix(y_test, y_pred))\n      \n    print (\"Accuracy : \",\n    accuracy_score(y_test,y_pred))\n    print ( \"Roc_Auc score %.2f: \", roc_auc_score ( y_test, y_pred ))\n      \n    print(\"Report : \",\n    classification_report(y_test, y_pred))\n  \n# Driver code\ndef main():\n      \n    # Building Phase\n    data = training_data\n    X, Y, X_train, X_test, y_train, y_test = splitdataset(data)\n    clf_gini = train_using_gini(X_train, X_test, y_train)\n    clf_entropy = train_using_entropy(X_train, X_test, y_train)\n      \n    # Operational Phase\n    print(\"Results Using Gini Index:\")\n      \n    # Prediction using gini\n    y_pred_gini = prediction(X_test, clf_gini)\n    cal_accuracy(y_test, y_pred_gini)\n      \n    print(\"Results Using Entropy:\")\n    # Prediction using entropy\n    y_pred_entropy = prediction(X_test, clf_entropy)\n    cal_accuracy(y_test, y_pred_entropy)\n      \n\n        # Calling main function\nif __name__==\"__main__\":\n    main()\n       ","43c615d9":"### target varaible is risk flag","df4e42a1":"### now we will do label enconding of train dataset ","0cdf672c":"### Exploratory Data Analysis ","a818ef23":"### Finding correlation through heatmap of all tha datasets ","103c84c2":"### Finding out number of null values ","cf39dd7b":"### first we will do data cleaning ","0dc6934a":"### Now find the correlation for dataset sample prediction and test data ","01e83fef":"### Finding Accuracy Score and roc_auc_score of train dataset from Decision Tree Classifier "}}