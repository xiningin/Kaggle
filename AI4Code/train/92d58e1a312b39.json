{"cell_type":{"e96c4769":"code","447e780a":"code","ac9b713e":"code","bec08256":"code","3a823f5e":"code","207d64bf":"code","777fea79":"code","4c71b9a3":"code","5665745c":"code","43c4533f":"code","5b9cc829":"code","984aa5b9":"code","b1c1b096":"code","82d3b2e8":"code","46b2364f":"code","6cab82e1":"code","4ebd08ee":"code","c32716cd":"code","d18d2346":"code","105faa0c":"code","984301ed":"code","7c97b6d2":"markdown","5ccba023":"markdown","3ec0677d":"markdown","d7de6a5d":"markdown","d2e323f4":"markdown","4d6a4ac0":"markdown","b21e75e3":"markdown","1a44eb6a":"markdown","f44c70ad":"markdown","aafbe25d":"markdown","dbc0f7f1":"markdown","9a874da0":"markdown","08a1ec80":"markdown","1d82c0bb":"markdown","94e0ca9e":"markdown","dd133fb7":"markdown","6ce1e123":"markdown","15d6c7c4":"markdown","2e79a21a":"markdown","06ca7a70":"markdown","cb4fa56d":"markdown","5d7c27c0":"markdown","5bf5f12f":"markdown"},"source":{"e96c4769":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\n\n\nsns.set(style='white', context='notebook', palette='deep')","447e780a":"# Load the data\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","ac9b713e":"Y_train = train[\"label\"]\n\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\n# free some space\ndel train \n\ng = sns.countplot(Y_train)","bec08256":"# Check the data\nX_train.isnull().any().describe()","3a823f5e":"test.isnull().any().describe()","207d64bf":"X_train = X_train \/ 255.0\ntest = test \/ 255.0","777fea79":"# Observe the data\nX_train.head","4c71b9a3":"# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1 (as it is gray scale))\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","5665745c":"#observe the labels\nY_train.head(10)","43c4533f":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(Y_train, num_classes = 10)","5b9cc829":"# Set the random seed\nrandom_seed = 2","984aa5b9":"# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)","b1c1b096":"# Some examples\ng = plt.imshow(X_train[0][:,:,0])","82d3b2e8":"# Set the CNN model \n# my CNN architechture is In -> [[Conv2D->relu] -> MaxPool2D -> Dropout]*3 -> Flatten -> Dense -> Dropout -> Out\n# If you want to increase accuracy then you can increase the number of CNNs\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","46b2364f":"# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","6cab82e1":"# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","4ebd08ee":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","c32716cd":"# Try experimenting different Epoch and Batch_size values and fix it to the one which gives maximum accuracy\nepochs = 6\nbatch_size = 128","d18d2346":"# Without data augmentation i obtained an accuracy of 0.98114\nhistory = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n          validation_data = (X_val, Y_val), verbose = 2)","105faa0c":"from pandas         import DataFrame\npredicted_value = model.predict(test)\nclasses=[0,1,2,3,4,5,6,7,8,9]\nlist1=[]\nfor index in range(0, len(predicted_value)):\n    list1.append(classes[np.argmax(predicted_value[index])])\nresults= DataFrame(list1, columns=['Label'])","984301ed":"#change the index value and check the results\nindex=25\ng = plt.imshow(test[index][:,:,0])\nprint(list1[index])","7c97b6d2":"It is always good to check for uniformity before moving forward. We have similar counts for the 10 digits.","5ccba023":"## 2.5 Label Encoding","3ec0677d":"## 2.3 Normalization","d7de6a5d":"## 3.2 Set the optimizer and annealer\n\nOnce our layers are added to the model, we need to set up:\n1. A loss function: Its used to measure how poorly our model performs on images with known labels. It is the error rate between the oberved labels and the predicted ones. We use a specific form for categorical classifications (>2 classes) called the \"categorical_crossentropy\".\n2. A score function: The metric function \"accuracy\" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation). \n3. An optimisation algorithm: This is the most important function, as this function will iteratively improve parameters (such as filters kernel values, weights and bias of neurons) in order to minimise the loss. I chose RMSprop. The RMSProp update adjusts the Adamgrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. ","d2e323f4":"We can get a better sense for one of these examples by visualising the image and looking at the label.","4d6a4ac0":"##  4.1 Prediction on test set","b21e75e3":"Make sure to also check for missing values.\nThere is no missing values in the train and test dataset. So we can safely go ahead.","1a44eb6a":"## 2.6 Split to Training and Valdiation set ","f44c70ad":"Since, we have a training set of balanced labels, a random split of the training set doesn't cause some labels to be over represented in the validation set.\nAnd, as we have only 42000 images, I chose to split the set in two parts : a small fraction (10%) became the validation set which the model is evaluated on and the rest (90%) is used to train the model. ","aafbe25d":"# 3. CNN\n## 3.1 Define the model","dbc0f7f1":"Here, we have to encode the labels from, the number the image is depicting to a matrix(vector) of 1x10 (0 to 9 numbers), where one cell value is 1, showing that this number is the number depicted, and rest all are 0's. These vectors are known as one hot vectors.\nex : 2 -> [0,0,1,0,0,0,0,0,0,0]","9a874da0":"# 1. Introduction\n\nThis is a 4 layers Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. I have build it with keras API (Tensorflow backend) which is very intuitive. I have achieved 99.10% accuracy in less than 5 mins. \n\nThis Notebook follows three main parts:\n\n* Preparing the Data\n* CNN Modeling and Evaluation\n* Results- Prediction on test set","08a1ec80":"# ***Thank-you!***","1d82c0bb":"In order to make the optimizer converge faster and closer to the global minimum of the loss function, I used an annealing method of the Learning Rate (LR).\n\nThe amount that the weights are updated during training is referred to as the step size or the \u201clearning rate.\u201d Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value. Because, the sampling is very poor with a high Learning Rate and the optimizer could probably fall into a local minima.\n\nIts better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function. For this reason I have used the ReduceLROnPlateau function from Keras.callbacks, I chose to reduce the LR by half if the accuracy is not improved after 3 epochs.","94e0ca9e":"Keras requires an extra dimension in the end which correspond to channels. MNIST images are gray scaled so it use only one channel. For RGB images, there is 3 channels, we would have reshaped 784px vectors to 28x28x3 3D matrices. ","dd133fb7":"Looking at the output, we know that train images have been stocked as a 1D vector of 784 values. We have to reshape all the data to 28x28x1, i.e., 3D matrices","6ce1e123":"# 4. Results","15d6c7c4":"## 2.2 Check for null and missing values","2e79a21a":"I used the Keras Sequential API, where you have to just add one layer at a time, starting from the input.\n\nThe first is the convolutional (Conv2D) layer. It is like a set of learnable filters. I chose to set 32 filters of size 5x5 for the first conv2D layer and 64 filters of same size for the next conv2D layer, followed by a 32 filters of size 5x5. Each filter transforms a part of the image using the kernel filter. Filters can be seen as a transformation of the image.\n\nThe CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n\nThe second important layer in CNNs is the pooling (MaxPool2D) layer. This layer simply acts as a down sampling filter. I chose a MaxPool layer, which simply looks at the pixels in the matrix size defined (I fixed it to 2x2) and picks the maximum value. These are used to reduce computational cost, and to some extent also reduce overfitting. \n\nCombining convolutional and pooling layers, CNN are able to combine local features and learn more global features of the image.\n\nDropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their weights to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting. \n\n'relu' is the rectifier (which equals max(0,x)). The rectifier activation function is used to add non linearity to the network. \n\nThe Flatten layer is use to convert the final feature maps into a single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional\/maxpool layers. It combines all the found local features of the previous convolutional layers.\n\nIn the end I passed these features into two fully-connected (Dense) layers which is just artificial neural networks (ANN) classifier. In the last layer(Dense(10,activation=\"softmax\")) the net outputs distribution of probability of each class.","06ca7a70":"# Introduction to CNN Keras - Acc 0.99+ \n\n* **1. Introduction**\n* **2. Data preparation**\n    * 2.1 Load data\n    * 2.2 Check for null and missing values\n    * 2.3 Normalization\n    * 2.4 Reshape\n    * 2.5 Label encoding\n    * 2.6 Split training and valdiation set\n* **3. CNN**\n    * 3.1 Define the model\n    * 3.2 Set the optimizer and annealer\n* **4. Results**\n    * 4.1 Predict on Test Set","cb4fa56d":"Then, perform a grayscale normalization. Moreover the CNNs converge faster on [0..1] data.","5d7c27c0":"# 2. Data preparation\n## 2.1 Load data","5bf5f12f":"## 2.4 Reshape"}}