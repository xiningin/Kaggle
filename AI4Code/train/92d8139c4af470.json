{"cell_type":{"825c1e13":"code","c6d58e22":"code","ea25f8d3":"code","95c82ea8":"code","559fa3cd":"code","0f72f6a3":"code","e8d19333":"code","0abe3fd6":"code","d31e4218":"code","d000c771":"code","f20fea41":"code","ec093bfd":"code","a7e4e396":"code","e9b6e076":"code","955c7475":"code","be18aa68":"markdown","9951c9ce":"markdown","1cc59267":"markdown","6c8ba7c2":"markdown","740ff5b6":"markdown","a9aa9a7d":"markdown","63877392":"markdown","20b9eb26":"markdown","2f288088":"markdown","0a041824":"markdown","e58ca103":"markdown","660ddce7":"markdown","26bf1525":"markdown","b68365a8":"markdown","c78607bd":"markdown","c1ebf0d2":"markdown"},"source":{"825c1e13":"import pandas as pd\nimport tensorflow as tf\nimport glob,os\nfrom osgeo import gdal\nimport cv2\nimport PIL\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nfrom PIL import Image as PILImage\nfrom IPython.display import Image\nimport gc\nplt.rcParams[\"figure.figsize\"] = (30,20)","c6d58e22":"def saveRGB(ds,showpic=False):\n    R1 = ds.GetRasterBand(1).ReadAsArray()\n    G1 = ds.GetRasterBand(2).ReadAsArray()\n    B1 = ds.GetRasterBand(3).ReadAsArray()\n    I1 = ds.GetRasterBand(4).ReadAsArray()\n    R2 = ds.GetRasterBand(5).ReadAsArray()\n    G2 = ds.GetRasterBand(6).ReadAsArray()\n    B2 = ds.GetRasterBand(7).ReadAsArray()\n    I2 = ds.GetRasterBand(8).ReadAsArray()\n\n\n    im8ch=np.array([R1,G1,B1,I1,R2,G2,B2,I2]).astype(np.float32).transpose(1, 2, 0)\n    dataX.append(im8ch)","ea25f8d3":"dataX = []\ndataY = {}\nfilenames = {}\ndataset = glob.glob(os.path.join('..\/input\/roscosmos-rucode\/Images_composit\/Images_composit\/8_ch\/', '*.tif'), recursive=True)\nn_im = 0\nfor filename in dataset:\n\n    ds = gdal.Open(filename)\n    saveRGB(ds, False)\n    train = os.listdir('..\/input\/roscosmos-rucode\/mask\/mask\/')\n    \n    filename = filename.split('\/')[6]\n       \n    filename = filename[:-4]\n    \n    if filename+'.tif' in train:\n\n        mask2 = gdal.Open('..\/input\/roscosmos-rucode\/mask\/mask\/'+filename+'.tif')\n        mask2 = mask2.GetRasterBand(1).ReadAsArray()\n        dataY[n_im] = mask2\n    filenames[n_im] = filename\n    n_im += 1 \n    \ngc.collect();","95c82ea8":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_probability as tfp\nimport os, random, json, PIL, shutil, re, gc\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nfrom scipy import linalg\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ntf.__version__\n\n","559fa3cd":"for num_s in range(46):\n  im = dataX[num_s]\n  \n  bgr_im1=im[:,:,:3]\n  bgr_im2=im[:,:,4:7]\n  G1=im[:,:,1]\n  G2=im[:,:,5] \n  mean_rgb1=bgr_im1\/np.median(bgr_im1[bgr_im1>0])\n  mean_rgb2=bgr_im2\/np.median(bgr_im2[bgr_im2>0])\n    \n  maskzero = (((mean_rgb1[:,:,0]>1.55)*(mean_rgb1[:,:,1]>1.55)*(mean_rgb1[:,:,2]>1.55) + (mean_rgb2[:,:,0]>1.55)*(mean_rgb2[:,:,1]>1.55)*(mean_rgb2[:,:,2]>1.55))>0).astype(np.float32)\n  kernel = np.ones((5, 5), np.uint8)\n  maskzero = cv2.erode(maskzero, kernel, iterations=1) \n  maskzero = np.expand_dims(1-cv2.dilate(np.float32(maskzero), kernel, iterations=20),axis=-1)\n  maskborder = ((im[:,:,2] * im[:,:,6]) == 0 ).astype(np.float32)\n  maskborder = cv2.erode(maskborder, kernel, iterations=1) \n  maskborder = np.expand_dims(1-cv2.dilate(np.float32(maskborder), kernel, iterations=6),axis=-1)\n  dataX[num_s] = np.concatenate((dataX[num_s],maskzero * maskborder),axis=-1)\n  im1m = np.median(G1[G1>0])\n  im2m = np.median(G2[G2>0])\n  att = np.abs((G2\/im2m - G1\/im1m)>0.16).astype(np.float32)\n  att = cv2.erode(att, kernel, iterations=1) \n  att = np.expand_dims(cv2.dilate(np.float32(att), kernel, iterations=4),axis=-1)\n  dataX[num_s] = np.concatenate((dataX[num_s],att),axis=-1)\n  ","0f72f6a3":"OUTPUT_CHANNELS = 3\n\ndef down_sample(filters, size, apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    layer = keras.Sequential()\n    layer.add(layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        layer.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    layer.add(layers.LeakyReLU())\n\n    return layer","e8d19333":"def up_sample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    layer = keras.Sequential()\n    layer.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer,use_bias=False))\n    layer.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        layer.add(layers.Dropout(0.5))\n\n    layer.add(layers.ReLU())\n\n    return layer","0abe3fd6":"def Encoder():\n    inputs = layers.Input(shape=[256,256,4])\n    down_stack = [\n        down_sample(64, 4, apply_instancenorm=False),# (size, 128, 128, 64)\n        down_sample(128, 4),                         # (size, 64, 64, 128)\n        down_sample(256, 4),                         # (size, 32, 32, 256)\n        down_sample(512, 4),                         # (size, 16, 16, 512)\n        down_sample(512, 4),                         # (size, 8, 8, 512)\n        down_sample(512, 4),                         # (size, 4, 4, 512)\n        down_sample(512, 4),                         # (size, 2, 2, 512)\n        down_sample(512, 4),                         # (size, 1, 1, 512)\n    ]\n\n    up_stack = [\n        up_sample(512, 4, apply_dropout=False),       # (size, 2, 2, 1024)\n        up_sample(512, 4, apply_dropout=False),       # (size, 4, 4, 1024)\n        up_sample(512, 4, apply_dropout=False),       # (size, 8, 8, 1024)\n        up_sample(512, 4),                           # (size, 16, 16, 1024)\n        up_sample(256, 4),                           # (size, 32, 32, 512)\n        up_sample(128, 4),                           # (size, 64, 64, 256)\n        up_sample(64, 4),                            # (size, 128, 128, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(4, 4, strides=2, padding='same', kernel_initializer=initializer) \n    # (size, 256, 256, 1)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","d31e4218":"def Generator():\n        inputs = layers.Input(shape=[256,256,4])\n        down_stack = [\n            down_sample(64, 4, apply_instancenorm=False),# (size, 128, 128, 64)\n            down_sample(128, 4),                         # (size, 64, 64, 128)\n            down_sample(256, 4),                         # (size, 32, 32, 256)\n            down_sample(512, 4),                         # (size, 16, 16, 512)\n            down_sample(512, 4),                         # (size, 8, 8, 512)\n            down_sample(512, 4),                         # (size, 4, 4, 512)\n            down_sample(512, 4),                         # (size, 2, 2, 512)\n            down_sample(512, 4),                         # (size, 1, 1, 512)\n        ]\n\n        up_stack = [\n            up_sample(512, 4, apply_dropout=False),       # (size, 2, 2, 1024)\n            up_sample(512, 4, apply_dropout=False),       # (size, 4, 4, 1024)\n            up_sample(512, 4, apply_dropout=False),       # (size, 8, 8, 1024)\n            up_sample(512, 4),                           # (size, 16, 16, 1024)\n            up_sample(256, 4),                           # (size, 32, 32, 512)\n            up_sample(128, 4),                           # (size, 64, 64, 256)\n            up_sample(64, 4),                            # (size, 128, 128, 128)\n        ]\n\n        initializer = tf.random_normal_initializer(0., 0.02)\n        last = layers.Conv2DTranspose(4, 4, strides=2, padding='same', kernel_initializer=initializer) \n        # (size, 256, 256, 3)\n\n        x = inputs\n\n        # Downsampling through the model\n        skips = []\n        for down in down_stack:\n            x = down(x)\n            skips.append(x)\n\n        skips = reversed(skips[:-1])\n\n        # Upsampling and establishing the skip connections\n        for up, skip in zip(up_stack, skips):\n            x = up(x)\n            x = layers.Concatenate()([x, skip])\n\n        x = last(x)\n\n        return keras.Model(inputs=inputs, outputs=x)","d000c771":"with strategy.scope():\n    generator1 = Generator() \n    encoder1 = Encoder() \n\n    generator2 = Generator() \n    encoder2 = Encoder() \n","f20fea41":"class FreeI2IGan(keras.Model):\n    def __init__(\n        self,\n        generator1,\n        encoder1,\n        generator2,\n        encoder2,\n\n        lambda_id=10\n    ):\n        super(FreeI2IGan, self).__init__()\n        self.generator1 = generator1\n        self.encoder1 = encoder1\n        self.generator2 = generator2\n        self.encoder2 = encoder2\n        self.lambda_id = lambda_id\n\n\n        \n    def compile(\n        self,\n        gen_optimizer1,\n        enc_optimizer1,\n        gen_optimizer2,\n        enc_optimizer2,\n    ):\n        super(FreeI2IGan, self).compile()\n        self.gen_optimizer1 = gen_optimizer1\n        self.enc_optimizer1 = enc_optimizer1\n        self.gen_optimizer2 = gen_optimizer2\n        self.enc_optimizer2 = enc_optimizer2\n\n   \n    \n    def train_step(self, batch_data):\n        with tf.GradientTape(persistent=True) as tape:\n            real_1d = batch_data[:,:,:,:4]\n            real_2d = batch_data[:,:,:,4:8]\n            maskzero = batch_data[:,:,:,8:9]\n            att = batch_data[:,:,:,9:]\n\n            msk_1 = self.encoder1(real_1d, training=True)\n            msk_2 = self.encoder2(real_2d, training=True)\n            \n          \n            fake_1 = self.generator1(msk_1, training=True)\n            fake_2 = self.generator2(msk_2, training=True)\n\n            fake_a1 = self.generator1(msk_2, training=True)\n            fake_a2 = self.generator2(msk_1, training=True)\n\n\n            fake_r2 = self.generator2(self.encoder1(self.generator1(msk_2, training=True), training=True), training=True)\n            fake_r1 = self.generator1(self.encoder2(self.generator2(msk_1, training=True), training=True), training=True)\n\n            # MAE instead of MSE \n\n            pic1_loss = tf.reduce_sum(tf.abs(fake_1 - real_1d)*maskzero)\/70225\n            pic2_loss = tf.reduce_sum(tf.abs(fake_2 - real_2d)*maskzero) \/70225\n            rec1_loss = tf.reduce_sum(tf.abs(fake_r1 - real_1d)*maskzero)\/70225\n            rec2_loss = tf.reduce_sum(tf.abs(fake_r2 - real_2d)*maskzero) \/70225\n            att1_loss = tf.reduce_sum(tf.abs(fake_a1 - real_1d)*maskzero*(1-att))\/70225\n            att2_loss = tf.reduce_sum(tf.abs(fake_a2 - real_2d)*maskzero*(1-att)) \/70225\n\n            total_encoder1_loss =  pic1_loss + rec1_loss +  rec2_loss +  att2_loss\n            total_encoder2_loss =  pic2_loss + rec1_loss +  rec2_loss + att1_loss\n            total_generator1_loss =  pic1_loss + rec1_loss +  rec2_loss + att1_loss \n            total_generator2_loss =  pic2_loss + rec1_loss +  rec2_loss +  att2_loss\n\n            \n    \n\n        generator1_gradients = tape.gradient(total_generator1_loss,\n                                                      self.generator1.trainable_variables)\n\n        generator2_gradients = tape.gradient(total_generator2_loss,\n                                                      self.generator2.trainable_variables)\n        \n        encoder1_gradients = tape.gradient(total_encoder1_loss,\n                                                      self.encoder1.trainable_variables)\n\n        encoder2_gradients = tape.gradient(total_encoder2_loss,\n                                                      self.encoder2.trainable_variables)\n\n\n        self.enc_optimizer1.apply_gradients(zip(encoder1_gradients,\n                                                     self.encoder1.trainable_variables))\n        self.gen_optimizer1.apply_gradients(zip(generator1_gradients,\n                                                     self.generator1.trainable_variables))\n\n        self.enc_optimizer2.apply_gradients(zip(encoder2_gradients,\n                                                     self.encoder2.trainable_variables))\n        self.gen_optimizer2.apply_gradients(zip(generator2_gradients,\n                                                     self.generator2.trainable_variables))\n\n        \n        return {\n            \"pic1_loss\": pic1_loss,\n            \"pic2_loss\": pic2_loss,\n            \"rec1_loss\": rec1_loss,\n            \"rec2_loss\": rec2_loss,\n            \"att1_loss\": att1_loss,\n            \"att2_loss\": att2_loss,\n\n        }\n    \n    @tf.function\n    def call(self, inputs, training=None, mask=None):\n\n\n        return inputs\n    ","ec093bfd":"with strategy.scope():\n    generator_optimizer = tf.keras.optimizers.Adam(1e-3)\n    encoder_optimizer = tf.keras.optimizers.Adam(1e-3)\n\n\n\n    free_gan_model = FreeI2IGan(\n        generator1, encoder1,  generator2, encoder2\n    )\n\n    free_gan_model.compile(\n        gen_optimizer1 = generator_optimizer,\n        enc_optimizer1 = encoder_optimizer,\n        gen_optimizer2 = generator_optimizer,\n        enc_optimizer2 = encoder_optimizer,\n    )","a7e4e396":"def compue_mask(num_s):\n    \n    IMG_SIZE = 256\n    x_size = dataX[num_s].shape[0]\n    y_size = dataX[num_s].shape[1]\n    thebatchRGB1 = np.zeros((1,IMG_SIZE,IMG_SIZE,4))\n    thebatchRGB2 = np.zeros((1,IMG_SIZE,IMG_SIZE,4))\n    mymask1=np.zeros((x_size,y_size,4))\n    mymask2=np.zeros((x_size,y_size,4))\n    for ix in range(x_size\/\/IMG_SIZE+1):\n        for iy in range(y_size\/\/IMG_SIZE+1):\n                x_f = min(IMG_SIZE * (ix + 1), x_size)\n                y_f = min(IMG_SIZE * (iy + 1), y_size)\n                thebatchRGB1[0,:,:,:] = dataX[num_s][x_f - IMG_SIZE:x_f, y_f - IMG_SIZE:y_f,:4].astype(np.float32)\n                thebatchRGB2[0,:,:,:] = dataX[num_s][x_f - IMG_SIZE:x_f, y_f - IMG_SIZE:y_f,4:8].astype(np.float32)\n                m_msk1 = generator1(encoder2(thebatchRGB2))\n                m_msk2 = generator2(encoder1(thebatchRGB1))\n                mymask1[x_f - IMG_SIZE:x_f, y_f - IMG_SIZE:y_f,:] = m_msk1[0,:,:,:]\n                mymask2[x_f - IMG_SIZE:x_f, y_f - IMG_SIZE:y_f,:] = m_msk2[0,:,:,:]\n    np.save('m1sk'+str(num_s)+'.npy', mymask1)  \n    np.save('m2sk'+str(num_s)+'.npy', mymask2)  \n    return ","e9b6e076":"with strategy.scope():\n    def augment(tensor):\n        tensor = tf.cast(x=tensor, dtype=tf.float32)\n        tensor = tf.image.random_crop(value=tensor, size=(256, 256, 10))\n        return tensor\n    def create_ds(image_idx):\n        tf_ds = tf.data.Dataset.from_tensor_slices(np.expand_dims(dataX[image_idx], axis=0)).repeat(1024)\n        tf_ds = tf_ds.map(augment).batch(8).prefetch(8)\n        return tf_ds\n    ","955c7475":"%%time\n\nfor image_idx in range(46):\n\n    print(\"image_number = \", image_idx, dataX[image_idx].shape)\n\n    ds=create_ds(image_idx)\n\n    free_gan_model.fit(ds, epochs= 14  )  \n    compue_mask(image_idx)\n \n\n","be18aa68":"# \u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 4-\u0445 \u043a\u0430\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0445\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u043d\u0438\u043c\u043a\u043e\u0432 \u0438\u0437 \u043e\u0434\u043d\u043e\u0439 \u0434\u0430\u0442\u044b \u0432 \u0434\u0440\u0443\u0433\u0443\u044e \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0438\u0445 \u0432 npy","9951c9ce":"# \u041e\u0431\u0443\u0447\u0430\u0435\u043c, \u0434\u0432\u0435 \u043b\u043e\u0441\u0441-\u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043a\u0430\u043a \u0432 \u0441\u0442\u0430\u0442\u044c\u0435 \u0441\u0442\u0440\u043e\u0433\u043e, \u0442\u0440\u0435\u0442\u044c\u044f \u0431\u0435\u0437 \u0438\u0442\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f, \u0430 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u043e\u0439 \u043d\u0435\u0442\u0443...","1cc59267":"# \u042d\u043d\u043a\u043e\u0434\u0435\u0440 \u0438\u0437 4-\u0445 \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u0432 4 \u043a\u0430\u043d\u0430\u043b\u0430 \u043d\u0435 \u0441\u0436\u0438\u043c\u0430\u044f \u0441\u043e\u0432\u0441\u0435\u043c\n","6c8ba7c2":"# \u0421\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c 8 \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u043a\u0430\u043a \u0435\u0441\u0442\u044c \u0431\u0435\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438","740ff5b6":"# \u0412\u0441\u0435 \u043f\u043e \u0441\u0442\u0430\u0442\u044c\u0435 https:\/\/arxiv.org\/abs\/2004.07011 \u043a\u0440\u043e\u043c\u0435 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e MAE \u0432\u043c\u0435\u0441\u0442\u043e MSE, \u043d\u0435\u0442 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u043e\u0439 \u043b\u043e\u0441\u0441, \u0430 \u0442\u0440\u0435\u0442\u044c\u044f \u043b\u043e\u0441\u0441 \u043d\u0435 \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0438\u0442\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e, \u0430 \u0432\u0437\u044f\u0442\u0430 \u0441 \u0440\u0430\u0437\u043d\u0438\u0446 \u0437\u0435\u043b\u0435\u043d\u044b\u0445 \u043a\u0430\u043d\u0430\u043b\u043e\u0432.","a9aa9a7d":"# \u0415\u0449\u0435 \u043f\u0430\u043a\u0435\u0442\u044b \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u043d\u0430 TPU \u0433\u043e\u0442\u043e\u0432\u0438\u043c","63877392":"# \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0442\u0430\u043a\u043e\u0439 \u0436\u0435, \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043b \u0441\u043e \u0441\u0436\u0430\u0442\u0438\u0435\u043c \u043f\u0440\u043e\u0441\u0442\u043e, \u0430 \u0442\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0435\u043d\u043a\u043e\u0434\u0435\u0440 \u043e\u043f\u044f\u0442\u044c \u0431\u044b\u043b\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\n","20b9eb26":"# \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043a\u0430\u043d\u0430\u043b \u0441 \u043e\u0431\u043b\u0430\u043a\u0430\u043c\u0438 \u0438 \u0433\u0440\u0430\u043d\u0438\u0446\u0430\u043c\u0438 \u0441\u043d\u0438\u043c\u043a\u0430 maskborder * maskzero, \u0438 \u043a\u0430\u043d\u0430\u043b att - \u0430\u0442\u0442\u0435\u043d\u0448\u0438\u043d \u0434\u043b\u044f \u0442\u0440\u0435\u0442\u044c\u0435\u0439 \u043b\u043e\u0441\u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u0438","2f288088":"# \u041e\u0431\u0443\u0447\u0430\u0435\u043c.... \u0432 \u043f\u0435\u0440\u0432\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0438 \u043a\u0435\u0440\u043d\u0435\u043b\u0430 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043a\u0430\u043a \u0443\u0447\u0438\u043b\u043e\u0441\u044c ","0a041824":"# \u0422\u0430\u043a\u0436\u0435 \u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0438 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0433\u0434\u0435 \u0435\u0441\u0442\u044c \u0438 \u0438\u043c\u0435\u043d\u0430 \u0444\u0430\u0439\u043b\u043e\u0432","e58ca103":"# \u0410 \u0434\u0430\u043b\u044c\u0448\u0435 \u0432 Colab \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043b npy, dataX, dataY, filenames \u0438 \u0441\u0434\u0435\u043b\u0430\u043b \u0440\u0430\u0437\u043d\u0438\u0446\u0443 MSE \u0438\u0445, \u043f\u043e\u0434\u043e\u0431\u0440\u0430\u043b \u043f\u043e\u0440\u043e\u0433 \u0438 \u0441\u0434\u0430\u043b \u043d\u0430 10-\u0435 \u043c\u0435\u0441\u0442\u043e. \u0417\u0434\u0435\u0441\u044c \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f \u0437\u0430\u043a\u043e\u043d\u0447\u0438\u043b\u043e\u0441\u044c. \u041d\u043e \u0442\u0430\u043c \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e\u0433\u043e \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435\u0442\u0443","660ddce7":"# \u0414\u0432\u0430 \u044d\u043d\u043a\u043e\u0434\u0435\u0440\u0430 \u0438 \u0434\u0432\u0430 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u0430, \u043a\u0430\u043a \u0432 \u0441\u0442\u0430\u0442\u044c\u0435","26bf1525":"# U-NET","b68365a8":"# \u041f\u0430\u043a\u0435\u0442\u044b...","c78607bd":"## Your own personal Autoencoder\n\nSomeone to know your latent space\n\nSomeone who distinguishes\n\n\n\n\n\n\nAutoencoder is trained with each pair of images personally to detect differencies \n\n\n ","c1ebf0d2":"# \u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u0432\u044b\u0440\u0435\u0437\u0430\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e 256 \u0445 256"}}