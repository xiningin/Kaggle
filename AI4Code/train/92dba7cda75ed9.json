{"cell_type":{"86377309":"code","8d2f12bc":"code","4a835dc4":"code","b73f05fd":"code","65294bba":"code","92a61cd4":"code","e18a3498":"code","e271d912":"code","62594223":"code","11bbbd84":"code","57980a0e":"code","6d6b76d1":"code","3f9f8539":"code","4288e891":"code","0886baa1":"code","4be84440":"code","7ed833d1":"code","968b4f5d":"code","d336a32a":"code","68054783":"code","d22854da":"code","16a1f74a":"code","d07a4556":"code","5c8481eb":"markdown","14da03e2":"markdown"},"source":{"86377309":"import pandas as pd\nimport numpy as np\nfrom calendar import monthrange","8d2f12bc":"pd.set_option('display.max_columns', None)","4a835dc4":"path = \"..\/input\/globalterrorism-old.csv\"\ndf = pd.read_csv(path, low_memory= False)","b73f05fd":"df.info(memory_usage='deep')","65294bba":"# =============================================================================\n# SUMMARY - BEFORE\n# dtypes: datetime64[ns](1), float64(55), int64(22), object(57)\n# memory usage: 590.0 MB\n# =============================================================================","92a61cd4":"df.head(2)","e18a3498":"# Deleting unnecessary columns\ncols =  ['eventid','approxdate', 'resolution', 'country','region', 'provstate', 'vicinity', 'multiple', 'alternative','attacktype1', 'attacktype2',\n         'attacktype3','weaptype1','weapsubtype1','weaptype2','weapsubtype2', 'targtype1', 'targsubtype1','natlty1','targtype2','targsubtype2',\n         'natlty2','targtype3', 'targsubtype3', 'natlty3','claimmode','propextent','hostkidoutcome','weaptype3','weaptype3_txt', 'weapsubtype3',\n         'weapsubtype3_txt','weaptype4','weaptype4_txt','weapsubtype4','weapsubtype4_txt', 'guncertain1', 'guncertain2', 'guncertain3', 'gname3', \n         'propcomment', 'ishostkid', 'nhostkid', 'nhostkidus', 'nhours', 'ndays', 'divert', 'kidhijcountry', 'ransom', 'ransomamt', 'ransomamtus', \n         'ransompaid', 'ransompaidus', 'ransomnote', 'hostkidoutcome_txt', 'attacktype3_txt', 'targtype2_txt', 'targsubtype2_txt', 'corp2', 'target2', \n         'natlty2_txt', 'targtype3_txt', 'targsubtype3_txt', 'corp3', 'target3', 'natlty3_txt', 'claim2', 'claimmode2', 'claimmode2_txt', 'claim3', \n         'claimmode3', 'claimmode3_txt', 'compclaim', 'motive', 'gsubname', 'gname2', 'gsubname2', 'gsubname3', 'nperps', 'nperpcap', 'attacktype2_txt', \n         'weapsubtype1_txt', 'weaptype2_txt', 'weapsubtype2_txt', 'weapdetail', 'nkillus', 'nwoundus', 'nwoundte', 'nreleased', 'addnotes', 'scite1', \n         'scite2', 'scite3', 'dbsource', 'INT_LOG', 'INT_IDEO', 'INT_MISC', 'INT_ANY', 'related']\n\n# Dropping columns function\ndef deleting_columns(col):\n    df.drop(col, axis=1, inplace=True)\n\n# Dropping columns in the cols list\nfor col in cols:\n    try:\n        deleting_columns(col)  \n    except:\n        print(\"Error in deleting columns:  {}\".format(col))","e271d912":"# Calculating memory usage function\ndef mem_usage(pandas_obj):\n    \n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    \n    usage_mb = usage_b \/ 1024 ** 2 # convert bytes to megabytes\n    \n    return \"{:03.2f} MB\".format(usage_mb)","62594223":"# Test type int columns\ndf_int = df.select_dtypes(include=['int'])\nconverted_int = df_int.apply(pd.to_numeric,downcast='unsigned')\n\nprint('Memory usage of columns type INT before optimization is: ', mem_usage(df_int)) #15.99 MB\nprint('Memory usage of columns type INT after optimization is: ', mem_usage(converted_int)) #3.29 MB\n\ndf_int = converted_int\n\n# Re-run the cell - text below is not correct","11bbbd84":"# Test type float columns\ndf_float = df.select_dtypes(include=['float'])\nconverted_float = df_float.apply(pd.to_numeric,downcast='float')\n\nprint('Memory usage of columns type FLOAT before optimization is: ', mem_usage(df_float)) #37.69 MB\nprint('Memory usage of columns type FLOAT after optimization is: ', mem_usage(converted_float)) #18.85 MB\n\ndf_float = converted_float","57980a0e":"# Optimize all object columns by converting them to type category if the unique values < 0.5\ndf_obj_col = df.select_dtypes(include=['object'])\n\nfor col in df_obj_col:\n    try:\n        num_unique_values = len(df[col].unique())\n        num_total_values = len(df[col])\n        if num_unique_values \/ num_total_values < 0.5:\n            df.loc[:,col] = df[col].astype('category')\n        else:\n            df.loc[:,col] = df[col]\n    except:\n        print(\"{} , Error converting object columns to type category\".format(col))","6d6b76d1":"# Check if null value is more than 90% in all columns\nfor col in df.columns:\n    if df[col].isnull().sum() \/ len(df[col]) > .90:\n        print(\"Columns more than 90% :  {}\".format(col))","3f9f8539":"# Check if the -9 value is more than 90% in INT columns\nx = df.select_dtypes(include=['int'])\n\nfor col in x:\n    if len(df.loc[df[col] == -9, col]) \/ len(df[col]) > .90:\n        print(\"Columns with -9 more than 90% :  {}\".format(col))","4288e891":"len(df.loc[df['location'] == -9, 'location']) \/ len(df['location'])","0886baa1":"for col in df.columns:\n    if df[col].isnull().sum() \/ len(df[col]) > .70:\n        print(\"Columns more than 70% :  {}\".format(col))","4be84440":"df.drop(['alternative_txt', 'claimmode_txt', 'propvalue'], axis= 1, inplace= True)","7ed833d1":"# Renaming rest of columns\nnew_cols_names = {  'country_txt' : 'country_name',\n                    'region_txt' : 'region_name',\n                    'provstate' : 'province_state',\n                    'crit1' : 'criteria_pol_eco_rel_goals',\n                    'crit2' : 'criteria_coerce',\n                    'crit3' : 'criteria_outside_hum_law',\n                    'doubtterr' : 'criteria_unsure',\n                    'attacktype1_txt' : 'attack_type',\n                    'success' : 'attack_status',\n                    'suicide' : 'attack_suicide',\n                    'weaptype1_txt' : 'weapon_first_type_general',\n                    'weapsubtype1_txt' : 'weapon_first_type_specefic',\n                    'targtype1_txt' : 'target_victim_type',\n                    'targsubtype1_txt' : 'target_victim_subtype',\n                    'corp1' : 'name_of_targeted_entity',\n                    'target1' : 'name_of_targeted_entity_specific',\n                    'natlty1_txt' : 'nationality_of_target',\n                    'gname' : 'group_name',\n                    'individual' : 'individual_not_groups',\n                    'claimed' : 'claim_of_responsibility',\n                    'nkill' : 'total_number_killed',\n                    'nkillter' : 'total_number_terrorists_killed',\n                    'property' : 'property_damage',\n                    'propextent_txt' : 'property_damage_category',\n                    'nwound': 'total_number_injured',\n                    'iyear': 'year', # make it ready for the to_datetime function\n                    'imonth': 'month', # make it ready for the to_datetime function\n                    'iday': 'day' # make it ready for the to_datetime function \n                 }\n\ndf.rename(columns = new_cols_names, inplace=True)","968b4f5d":"# Solution: Convert 0 to np.nan and then use ffill method to get numbers in previous cells\ndf.loc[df.month == 0, 'month'] = np.nan\ndf.loc[df.day == 0, 'day'] = np.nan\n\ndf.loc[:, ['month', 'day']] = df.loc[:, ['month', 'day']].fillna(method= 'ffill')  # inplace = True argument doesnt work here. I had to assign it to the \"optimized_df.loc[:, ['month', 'day']]\"","d336a32a":"# Check if the day exceed the month range\nfor i in df.index.tolist():\n    \n    prev_month = 1\n    y, m, d = df.year[i].astype(int), df.month[i].astype(int), df.day[i].astype(int)\n    \n    if d > 28:        \n        if prev_month != m:\n            d = 1\n            df.loc[i, 'day'] = d\n        elif m == 2:\n            d = monthrange(y,m)[1]\n            df.loc[i, 'day'] = d\n\n    if (d == 30) | (d == 31):\n        if d > monthrange(y,m)[1]:\n            d = monthrange(y,m)[1]\n            df.loc[i, 'day'] = d\n        else:\n            d = 1\n            df.loc[i, 'day'] = d","68054783":"# Creating datetime column\ndf['dates'] = pd.to_datetime(df[['year', 'month', 'day']].astype(int))","d22854da":"# Drop year, month, day columns and re arrange all columns starting with date column\ndf.drop(['year', 'month', 'day'], axis= 1, inplace= True)\n\ndf = df [[ 'dates', 'extended', 'country_name', 'region_name', 'city', 'latitude', 'longitude', 'specificity', 'location',\n                               'criteria_pol_eco_rel_goals', 'criteria_coerce', 'criteria_outside_hum_law', 'criteria_unsure', 'attack_status',\n                               'attack_suicide', 'attack_type', 'target_victim_type', 'target_victim_subtype', 'name_of_targeted_entity',\n                               'name_of_targeted_entity_specific', 'nationality_of_target', 'group_name', 'individual_not_groups', \n                               'claim_of_responsibility', 'weapon_first_type_general', 'total_number_killed', 'total_number_terrorists_killed', \n                               'total_number_injured', 'property_damage', 'property_damage_category', 'summary']]","16a1f74a":"# Check memory usage\ndf.info(memory_usage='deep')","d07a4556":"# =============================================================================\n# SUMMARY - AFTER\n# dtypes: category(13), datetime64[ns](1), float64(8), int64(8), object(1)\n# memory usage: 87.6 MB\n# =============================================================================\n\n\n# =============================================================================\n# SUMMARY - BEFORE\n# dtypes: datetime64[ns](1), float64(55), int64(22), object(57)\n# memory usage: 590.0 MB\n# =============================================================================","5c8481eb":"# Reducing Memory Consumption of Pandas Dataframes for Quicker ML Predictions\n\nMemory consumption is an aspect many Data Scientists (including me, until now) neglect while running Machine Learning algorithms. With the availability of heavy RAMs which come built-in with GPUs on Cloud servers, memory is something that does not bother anyone much.\n\nReducing the memory consumption of data (involving dataframes) can prove to be beneficial in many ways:\n\n- Reduces loading time\n- Faster run time\n- Quicker predictions\n- Useful when running ML algorithms on memory limited systems\n- And especially useful when running kernels on Kaggle\n","14da03e2":"# Checks\ndf.select_dtypes(include=['int']).sum()\ndf.select_dtypes(include=['float']).sum()\ndf.select_dtypes(include=['object']).sum()\ndf.select_dtypes(include=['category']).sum()"}}