{"cell_type":{"a70320be":"code","37ed0907":"code","20f0b979":"code","2f698b78":"code","4bfbd3ba":"code","43c8c5b6":"code","b5319b42":"code","d7faf5a9":"code","11ffc8e5":"code","5db7a569":"code","de0e3659":"code","62a38d4c":"code","bc1d4680":"code","849fe360":"code","9cc61a66":"code","5d53b388":"code","770069e0":"code","179c04c8":"code","88f3e64f":"code","799c94c0":"code","d83f657b":"code","f5f34e27":"code","7ccb9c4a":"code","8657b879":"code","e14fd90a":"code","1f346b08":"code","28b9523a":"code","60d47b1e":"code","d0660ffb":"code","d03e8d87":"code","a95534a3":"code","1655960d":"code","4959c303":"code","26b9eff4":"code","96b38c06":"code","8b69d2cc":"code","70f0497b":"code","de98af84":"code","784f90c6":"code","85e5ed7d":"code","6db78b1b":"code","0b09ccc3":"markdown","cff5b268":"markdown","e88980af":"markdown","aa71497a":"markdown","1753ee5d":"markdown","6eb1cfd1":"markdown","dfb25f62":"markdown","b359e68b":"markdown","fdf73c96":"markdown","4e597181":"markdown","8b4e4b08":"markdown","33f78b02":"markdown"},"source":{"a70320be":"import os\nfrom tqdm.auto import tqdm\nimport time, gc\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport cv2\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model, Input\nfrom keras.layers import Activation, Concatenate, AveragePooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.initializers import RandomNormal\n\nfrom sklearn.model_selection import train_test_split\n\nstart_time = time.time()","37ed0907":"Kaggle = True\n\nif Kaggle:\n    DIR = '..\/input\/bengaliai-cv19'\nelse:\n    DIR = '.\/bengaliai-cv19'    # local PC","20f0b979":"# read CSV files\ntrain_df = pd.read_csv(os.path.join(DIR,'train.csv'))\ntest_df = pd.read_csv(os.path.join(DIR,'test.csv'))\nclass_map_df = pd.read_csv(os.path.join(DIR,'class_map.csv'))\nsample_sub_df = pd.read_csv(os.path.join(DIR,'sample_submission.csv'))\n                            \n# read Parquet Format Image file (example)\nimg_df = pd.read_parquet(os.path.join(DIR,'train_image_data_0.parquet'))","2f698b78":"print(train_df.shape)\ntrain_df.head()","4bfbd3ba":"print(test_df.shape)\ntest_df.head()","43c8c5b6":"print(class_map_df.shape)\nclass_map_df.head()","b5319b42":"print(sample_sub_df.shape)\nsample_sub_df.head()","d7faf5a9":"print(img_df.shape)\nimg_df.head()","11ffc8e5":"# target columns\ntgt_cols = ['grapheme_root','vowel_diacritic','consonant_diacritic']","5db7a569":"def get_top(tgt, n, top=True):\n    top_df = train_df.groupby([tgt]).size().reset_index(name='counts') \\\n                            ['counts'].sort_values(ascending=not top)[:n].copy()\n    top_ids = top_df.index\n    top_vals = top_df.values\n    \n    top_df = class_map_df.iloc[top_ids].copy()\n    top_df.drop(['component_type', 'label'], axis=1, inplace=True)\n    top_df['count'] = top_vals\n    return top_df","de0e3659":"def disp_img(df, ids):\n    r_n = len(ids)  # character count\n    c_n = 5         # num of examples for each character\n    plt.figure()\n    fig, ax = plt.subplots(r_n, c_n, figsize=(12, 10))\n    for r, id in enumerate(ids[:r_n]):\n        sumple_ids = train_df[train_df['grapheme_root'] == id].index\n        for c, sumple_id in enumerate(sumple_ids[:c_n]):\n            flattened_image = df.iloc[sumple_id].drop('image_id').values.astype(np.uint8)\n            ax[r, c%c_n].imshow(flattened_image.reshape([137, 236]))\n            ax[r, c%c_n].set_title(str(id)+'(Train_'+str(sumple_id)+')')","62a38d4c":"desc_df = train_df[tgt_cols].astype('str').describe()\ndesc_df","bc1d4680":"# Number of unique types\ntypes = desc_df.loc['unique',:]","849fe360":"top_roots = get_top('grapheme_root', 10)\ntop_roots","9cc61a66":"# Top 5 example\ndisp_img(img_df, top_roots.index[:5])","5d53b388":"top_vowels = get_top('vowel_diacritic', 10)\ntop_vowels","770069e0":"# Top 5 example\ndisp_img(img_df, top_vowels.index[:5])","179c04c8":"top_consonants = get_top('consonant_diacritic', 10)\ntop_consonants","88f3e64f":"disp_img(img_df, top_consonants.index[:5])","799c94c0":"SIZE = 64    # input image size\nN_ch = 1","d83f657b":"# Dense block\ndef dense_block(input_tensor, input_channels, n_block, growth_rate):\n    x = input_tensor\n    n_ch = input_channels\n    for i in range(n_block):\n        # main root\n        main = x\n        # DenseBlock root\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        # Bottle-Neck 1x1 Convolution\n        x = Conv2D(128, (1, 1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        # 3x3 Convolution\n        x = Conv2D(growth_rate, (3, 3), padding=\"same\")(x)\n        # Concatenate\n        x = Concatenate()([main, x])\n        n_ch += growth_rate\n    return x, n_ch","f5f34e27":"# Transition Layer\ndef transition_layer(input_tensor, input_channels, compression):\n    n_ch = int(input_channels * compression)\n    # 1x1 compression\n    x = Conv2D(n_ch, (1, 1))(input_tensor)\n    # AveragePooling\n    x = AveragePooling2D((2, 2))(x)\n    return x, n_ch","7ccb9c4a":"def build_denseNet(input_shape, growth_rate, blocks, compression):\n    input = Input(shape = input_shape)\n\n    n = growth_rate\n    x = Conv2D(n, (1,1))(input)\n\n    for i in range(len(blocks)):\n        # Transition layer\n        if i != 0:\n            x, n = transition_layer(x, n, compression)\n        # DenseBlock\n        x, n = dense_block(x, n, blocks[i], growth_rate)\n    x = GlobalAveragePooling2D()(x)\n\n    # multi output\n    grapheme_root = Dense(types['grapheme_root'],\n                      activation = 'softmax', name='root')(x)\n    vowel_diacritic = Dense(types['vowel_diacritic'],\n                        activation = 'softmax', name='vowel')(x)\n    consonant_diacritic = Dense(types['consonant_diacritic'],\n                            activation = 'softmax', name='consonant')(x)\n\n    # model\n    model = Model(input,\n              [grapheme_root, vowel_diacritic, consonant_diacritic])\n\n    return model","8657b879":"model = build_denseNet(input_shape = (SIZE, SIZE, N_ch),\n                       growth_rate = 16,        # 32\n                       blocks = [2,4,8,5],      # [6,12,24,16]\n                       compression = 0.5)","e14fd90a":"# compile\nmodel.compile(Adam(lr=0.002),\n              loss={'root': 'categorical_crossentropy',\n                    'vowel': 'categorical_crossentropy',\n                    'consonant': 'categorical_crossentropy'},\n              loss_weights={'root': 0.333,        ## Set weights\n                            'vowel': 0.333,\n                            'consonant': 0.333},\n              metrics={'root': 'accuracy',\n                       'vowel': 'accuracy',\n                       'consonant': 'accuracy'}\n             )","1f346b08":"model.summary()","28b9523a":"# Clip horizontally\ndef clip(img):\n    cols = np.any(img < 200, axis=0)\n    xleft, xright = np.where(cols)[0][[0, -1]]\n    width = xright - xleft\n    center = int((xleft + xright) \/ 2)\n    \n    if width < 137:\n        img = img[:, max(0, center - 70):min(center + 70, 236)]\n    else:\n        img = img[:, max(0, xleft - 2):min(xright + 2, 236)]\n        \n    return img","60d47b1e":"# Resize image size\ndef resize(df, size=64):\n    resized = {}\n    for i in range(df.shape[0]):\n        img = clip(df.loc[df.index[i]].values.reshape(137,236))\n        image = cv2.resize(img,(size,size))\n        resized[df.index[i]] = image.reshape(-1)\n    resized_df = pd.DataFrame(resized).T\n    return resized_df","d0660ffb":"# prepare X\nimg_df = img_df.drop(['image_id'], axis = 1)\nX_df = (resize(img_df, SIZE) \/ 255.).astype('float32')\ndel img_df\ngc.collect()\nfor i in tqdm(range(1,4)):\n    img_df = pd.read_parquet(os.path.join(\n                DIR, 'train_image_data_'+str(i)+'.parquet'))\n    img_df = img_df.drop(['image_id'], axis = 1)\n    img_df = (resize(img_df, SIZE) \/ 255.).astype('float32')\n    X_df = pd.concat([X_df, img_df], axis = 0)\n    del img_df\n    gc.collect()\n    \nX_train = X_df.values.reshape(-1, SIZE, SIZE, N_ch)\ndel X_df\ngc.collect()","d03e8d87":"# prepare Y\ntrain_df = train_df[tgt_cols].astype('uint8')\nfor col in tgt_cols:\n    train_df[col] = train_df[col].map('{:03}'.format)\nY_train = pd.get_dummies(train_df)\n\ndel train_df\ngc.collect()","a95534a3":"# Divide the data into train and val set\nx_train, x_test, y_train, y_test = train_test_split(X_train, Y_train,\n                                                test_size=0.2, random_state=42)\ny_train_root = y_train.iloc[:,0:types['grapheme_root']]\ny_train_vowel = y_train.iloc[:,types['grapheme_root']:types['grapheme_root']+types['vowel_diacritic']]\ny_train_consonant = y_train.iloc[:,types['grapheme_root']+types['vowel_diacritic']:]\ny_test_root = y_test.iloc[:,0:types['grapheme_root']]\ny_test_vowel = y_test.iloc[:,types['grapheme_root']:types['grapheme_root']+types['vowel_diacritic']]\ny_test_consonant = y_test.iloc[:,types['grapheme_root']+types['vowel_diacritic']:]\n    \ndel X_train, Y_train\ngc.collect()","1655960d":"print(time.time() - start_time,'[sec]')","4959c303":"batch_size = 128\nepochs = 14           ## set epochs","26b9eff4":"# Learning Rate annealer\nreduceLR = ReduceLROnPlateau(monitor = 'val_root_loss',\n                             patience = 2,\n                             factor = 0.5,\n                             min_lr = 1e-5,\n                             verbose = 1)","96b38c06":"history = model.fit(x_train,\n                    {'root': y_train_root,\n                     'vowel': y_train_vowel,\n                     'consonant': y_train_consonant},\n                    batch_size = batch_size,\n                    epochs = epochs,\n                    shuffle = True,\n                    validation_data = (x_test,\n                                       {'root': y_test_root,\n                                        'vowel': y_test_vowel,\n                                        'consonant': y_test_consonant}),\n                    callbacks = [reduceLR],\n                    verbose = 1)\n\ndel x_train, x_test, y_train, y_test\ngc.collect()","8b69d2cc":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(1, 2, figsize = (12, 4))\ndf = pd.DataFrame(history.history)\n\nax[0].plot(df[['root_loss','vowel_loss','consonant_loss',\n               'val_root_loss','val_vowel_loss','val_consonant_loss']])\nax[0].set_ylim(0, 2)\nax[0].set_title('Loss')\nax[0].legend(['train_root_loss','train_vowel_loss','train_conso_loss',\n              'val_root_loss','val_vowel_loss','val_conso_loss'],\n             loc='upper right')\nax[0].grid()\nax[1].plot(df[['root_accuracy','vowel_accuracy','consonant_accuracy',\n               'val_root_accuracy','val_vowel_accuracy','val_consonant_accuracy']])\nax[1].set_ylim(0.5, 1)\nax[1].set_title('Accuracy')\nax[1].legend(['train_root_acc','train_vowel_acc','train_conso_acc',\n              'val_root_acc','val_vowel_acc','val_conso_acc'],\n             loc='lower right')\nax[1].grid()","70f0497b":"del history\ngc.collect()\n\nprint(time.time() - start_time, '[sec]')","de98af84":"row_ids = []\ntargets = []      # prediction result\nid = 0\nfor i in range(4):\n    img_df = pd.read_parquet(os.path.join(\n                            DIR, 'test_image_data_'+str(i)+'.parquet'))\n    img_df = img_df.drop('image_id', axis = 1)\n    img_df = resize(img_df, SIZE) \/ 255.\n    X_test = img_df.values.reshape(-1, SIZE, SIZE, N_ch)\n\n    preds = model.predict(X_test)\n    for j in range(len(X_test)):\n        for k in range(3):\n            row_ids.append('Test_'+str(id)+'_'+tgt_cols[k])\n            targets.append(np.argmax(preds[k][j]))\n        id += 1","784f90c6":"submit_df = pd.DataFrame({'row_id':row_ids,'target':targets},\n                         columns = ['row_id','target'])\nsubmit_df.head(10)","85e5ed7d":"print(time.time() - start_time,'[sec]')","6db78b1b":"submit_df.to_csv('submission.csv',index=False)","0b09ccc3":"### Summarize training data","cff5b268":"## Observe each data","e88980af":"### Top used Vowel Diacritic","aa71497a":"## Bengali.Ai : Multi_Output_DenseNet (keras)","1753ee5d":"## Training\n### Prepare Training Data","6eb1cfd1":"## Predict and Submit","dfb25f62":"![image.png](attachment:image.png)<BR>\n<BR>\n    I referred to [kaushl Shah : Starter EDA+ Multi Output CNN](https:\/\/www.kaggle.com\/kaushal2896\/bengali-graphemes-starter-eda-multi-output-cnn). Thanks.<BR>\n- DenseNet is built by stacking Dense blocks.\n- To keep the GPU time limit, I keep the number of layers of DenseNet low. In addition, the number of epochs is modest.","b359e68b":"## Multi Output DenseNet Models\nPrepare DenceNet models with three-outputs (for 'grapheme root', 'vowel diacritic' and 'consonant_diacritic')","fdf73c96":"### Top used Consonant Diacritic","4e597181":"### Top used Grapheme Root","8b4e4b08":"### Fit","33f78b02":"`Column['0': '32331']` of img_df has flattened image size `137 * 236 (= 32332)`"}}