{"cell_type":{"76d9c132":"code","f878b43a":"code","2f124028":"code","2d57b116":"code","c4b5010e":"code","697ffe93":"code","050cae32":"code","f37012ee":"code","44a37d4b":"code","5acd2a6f":"code","e00049c2":"code","202977ee":"code","18a4332c":"code","85b204fd":"code","29ccbaee":"code","cc2b13a0":"code","ee668224":"code","497d4585":"code","b66a4840":"code","dd74f534":"code","8c5485bd":"code","d79e879f":"code","307e83c2":"code","54e70ad8":"code","bf52bc62":"code","cf7a6d16":"code","ba7df7d6":"code","2af4499a":"markdown","a5a415df":"markdown","f61bdc23":"markdown","556922fc":"markdown","d5a18014":"markdown","6a2f2906":"markdown","8e231793":"markdown","25f9e5db":"markdown","2bd4c9b5":"markdown","80a37929":"markdown","a008d105":"markdown"},"source":{"76d9c132":"timm_path = \"..\/input\/timm-pytorch-image-models\/pytorch-image-models-master\"\nimport sys\nsys.path.append(timm_path)\nimport timm\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nimport os\nfrom tqdm.notebook import tqdm\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import optim\n\nimport numpy as np, pandas as pd, gc\nimport cv2, matplotlib.pyplot as plt\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\n# from tensorflow.keras.applications import EfficientNetB0\nprint('RAPIDS',cuml.__version__)\nprint('TF',tf.__version__)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","f878b43a":"import math\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, train_test_split\n\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim import Adam, lr_scheduler\n\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n\n# transformer_model = 'sentence-transformers\/paraphrase-xlm-r-multilingual-v1'\n# TOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\n# transformer_model = '..\/input\/shopee-embedding-df\/paraphrase-xlm-r-multilingual-v1\/0_Transformer'\n# TOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\n","2f124028":"COMPUTE_CV = False\n\ntest = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\nif len(test)>3: COMPUTE_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')","2d57b116":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score","c4b5010e":"if COMPUTE_CV:\n    test = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n    test_gf = cudf.DataFrame(test)\n    print('Using train as test to compute CV (since commit notebook). Shape is', test_gf.shape )\nelse:\n    test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    test_gf = cudf.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    print('Test shape is', test_gf.shape )\ntest_gf.head()","697ffe93":"BASE = '..\/input\/shopee-product-matching\/test_images\/'\nif COMPUTE_CV: BASE = '..\/input\/shopee-product-matching\/train_images\/'\n\n# image_size = 192#256\nvalid_batch_size = 64\n\n\n\nclass Shopee(Dataset):\n    def __init__(self, df, augs=None):\n        self.df = df\n        self.augs = augs\n\n    def __len__(self):\n        return(len(self.df))\n\n    def __getitem__(self,idx):\n        img_src = self.df.loc[idx, 'image']\n        image = cv2.imread(BASE + img_src)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n\n        if (self.augs):\n            transformed = self.augs(image=image)\n            image = transformed['image']\n\n        return image\n    \n# \u30c6\u30b9\u30c8\u7528\u95a2\u6570\ndef test_predict(model, dataloader, device):\n    model.eval()\n    embeds = []\n\n    with torch.no_grad():    \n        for i, inputs in enumerate(tqdm(dataloader)):\n            inputs = inputs.to(device)\n            features = model(inputs).detach()\n            if len(features.shape) != 2:\n                features = torch.nn.AdaptiveAvgPool2d(1)(features).cpu().view(-1,features.shape[1]).detach().numpy()\n            else:\n                features = features.detach().cpu().numpy()\n\n            metric = features.reshape(features.shape[0], features.shape[1])\n            embeds.append(metric)\n\n    return np.concatenate(embeds)\n\n\ndef get_image_embeddings(model_name, weights_path, image_size):\n\n    valid_aug = A.Compose([\n        A.LongestMaxSize(max_size=image_size*1.2, p=1.0),\n        A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=0, p=1.0),\n        A.Normalize(p=1.0),\n        A.CenterCrop (image_size, image_size, always_apply=False, p=1.0),\n        ToTensorV2(p=1.0)\n        ])\n    \n    valid_data = test.copy()\n    valid_data = Shopee(valid_data.reset_index(drop=True), augs = valid_aug)\n    test_loader = DataLoader(valid_data,\n                              shuffle=False,\n                              num_workers=4,\n                              batch_size=valid_batch_size)\n\n\n    num_embeddings = 512#256\n    model = timm.create_model(model_name, pretrained=False)\n\n    if \"efficientnet\" in model_name: \n        num_features = model.classifier.in_features\n        model.classifier = nn.Linear(num_features, num_embeddings)\n    elif \"densenet\" in model_name: \n        num_features = model.classifier.in_features\n        model.classifier = nn.Linear(num_features, num_embeddings)\n    elif \"swin\" in model_name:\n        num_features = model.head.in_features\n        model.head = nn.Linear(num_features, num_embeddings)\n    elif \"vit\" in model_name:\n        num_features = model.head.in_features\n        model.head = nn.Linear(num_features, num_embeddings)\n    else:\n        num_features = model.head.fc.in_features\n        model.head.fc = nn.Linear(num_features, num_embeddings)           \n    _ = model.to(device)\n\n\n    try:\n        load_weghts = torch.load(weights_path)\n        model.load_state_dict(load_weghts)\n    except:\n        from collections import OrderedDict\n        def fix_model_state_dict(state_dict):\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k\n                if name.startswith('module.'):\n                    name = name[7:]  # remove 'module.' of dataparallel\n                new_state_dict[name] = v\n            return new_state_dict\n\n        state_dict = torch.load(weights_path)\n        model.load_state_dict(fix_model_state_dict(state_dict))\n        print(\"except\")\n\n\n    # newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))\n    image_embeddings = test_predict(model, test_loader, device)\n    print('image embeddings shape',image_embeddings.shape)\n\n    del model\n    _ = gc.collect()\n    return image_embeddings","050cae32":"def get_similar(image_embeddings,threshold=0.36,KNN = 50):\n    if len(test)==3: KNN = 2\n    model = NearestNeighbors(n_neighbors=KNN,metric='cosine')\n    model.fit(image_embeddings)\n    \n    preds = []\n    \n    CHUNK = 1024*4\n\n    print('Finding similar images...')\n    CTS = len(image_embeddings)\/\/CHUNK\n    if len(image_embeddings)%CHUNK!=0: CTS += 1\n    for j in tqdm(range( CTS )):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(image_embeddings))\n        distances, indices = model.kneighbors(image_embeddings[a:b,])\n\n#         x_avg=np.mean(distances) # \u5e73\u5747\u5024(\u5b9a\u7fa9\u4e0a\u306f0)\n#         x_std=np.std(distances) # \u6a19\u6e96\u504f\u5dee(\u5b9a\u7fa9\u4e0a\u306f1)\n#         lsl_1=x_avg-x_std*1.55\n\n        for k in range(b-a):\n            IDX = np.where(distances[k,]<threshold)[0]\n            IDS = indices[k,IDX]\n            o = test.iloc[IDS].posting_id.values\n            preds.append(o)\n\n#     print(lsl_1)\n    print(f\"embed={image_embeddings.shape[1]}_KNN={KNN}_distances={threshold}\")\n#     plt.hist(distances.flatten(),bins=100)\n#     plt.show()\n    del model, distances, indices, image_embeddings#, embeds\n    _ = gc.collect()\n    \n    return preds\n\ndef get_similar_two(image_embeddings,threshold=100):\n    KNN = 2\n    model = NearestNeighbors(n_neighbors=KNN,metric='cosine')\n    model.fit(image_embeddings)\n    \n    preds = []\n    \n    CHUNK = 1024*4\n\n    print('Finding similar images...')\n    CTS = len(image_embeddings)\/\/CHUNK\n    if len(image_embeddings)%CHUNK!=0: CTS += 1\n    for j in tqdm(range( CTS )):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(image_embeddings))\n        distances, indices = model.kneighbors(image_embeddings[a:b,])\n\n\n        for k in range(b-a):\n            IDX = np.where(distances[k,]<threshold)[0]\n            IDS = indices[k,IDX]\n            o = test.iloc[IDS].posting_id.values\n            preds.append(o)\n\n#     print(lsl_1)\n    print(f\"embed={image_embeddings.shape[1]}_KNN={KNN}_distances={threshold}\")\n#     plt.hist(distances.flatten(),bins=100)\n#     plt.show()\n    del model, distances, indices, image_embeddings#, embeds\n    _ = gc.collect()\n    \n    return preds","f37012ee":"def get_bret_embeddings(weights_path,transformer_model='..\/input\/shopee-embedding-df\/paraphrase-xlm-r-multilingual-v1\/0_Transformer'):\n    TOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n    \n    BASE = '..\/input\/shopee-product-matching\/test_images\/'\n    if COMPUTE_CV: BASE = '..\/input\/shopee-product-matching\/train_images\/'\n\n\n    valid_batch_size = 64\n\n    class ShopeeDataset(Dataset):\n        def __init__(self, csv):\n            self.csv = csv.reset_index()\n\n        def __len__(self):\n            return self.csv.shape[0]\n\n        def __getitem__(self, index):\n            row = self.csv.iloc[index]\n\n            text = row.title\n\n            text = TOKENIZER(text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n            input_ids = text['input_ids'][0]\n            attention_mask = text['attention_mask'][0]  \n\n            return input_ids, attention_mask\n\n    # \u30c6\u30b9\u30c8\u7528\u95a2\u6570\n    def test_predict(model, dataloader, device):\n\n        model.eval()\n        _predicted_metrics = []\n        _true_labels = []\n\n        with torch.no_grad():    \n            for i, (inputs, attention_masks) in enumerate(tqdm(dataloader)):\n                inputs,attention_masks = inputs.to(device),attention_masks.to(device)\n                features = model(inputs,attention_masks)[0][:,0,:].detach()\n\n                if len(features.shape) != 2:\n                    features = torch.nn.AdaptiveAvgPool2d(1)(features).cpu().view(-1,features.shape[1]).detach().numpy()\n                else:\n                    features = features.detach().cpu().numpy()\n\n                metric = features.reshape(features.shape[0], features.shape[1])\n                _predicted_metrics.append(metric)\n\n        return np.concatenate(_predicted_metrics)\n\n    valid_data = test.copy()\n    valid_data = ShopeeDataset(valid_data.reset_index(drop=True))\n    test_loader = DataLoader(valid_data,\n                              shuffle=False,\n                              num_workers=4,\n                              batch_size=valid_batch_size)\n\n\n    num_embeddings = 768#256\n\n    model = transformers.AutoModel.from_pretrained(transformer_model)\n    num_features = model.config.hidden_size\n    if transformer_model=='..\/input\/shopee-embedding-df\/paraphrase-xlm-r-multilingual-v1\/0_Transformer':\n        model.fc = nn.Linear(num_features, num_embeddings)\n\n    _ = model.to(device)\n\n\n\n    load_weghts = torch.load(weights_path)\n    model.load_state_dict(load_weghts)\n\n\n\n    bert_embeddings = test_predict(model, test_loader, device)\n    print('bert embeddings shape',bert_embeddings.shape)\n\n    del model\n    _ = gc.collect()\n    \n    return bert_embeddings\n","44a37d4b":"!ls ..\/input\/shopee-embedding-df\/","5acd2a6f":"# from nltk.tokenize import word_tokenize\n# test_title_token = test['title'].apply(lambda x: word_tokenize(x))\n# test[\"test_title_token\"] = test_title_token\n\n# from gensim.test.utils import common_texts\n# from gensim.models import Word2Vec\n\n    \n# def get_word_embeddings():\n#     model = Word2Vec(sentences=test_title_token, size=512, window=5, min_count=1, workers=4)\n#     model.train(test_title_token,total_examples=len(test_title_token),epochs=100)\n    \n#     def vectors_test(train_title_token): #test_df\n#         word_embeddings_test = []\n#         # Reading the each book description \n#         for line in train_title_token:\n#             avgword2vec = None\n#             count = 0\n#             for word in line:\n#                 if word in model.wv:\n#                     count += 1\n#                     if avgword2vec is None:\n#                         avgword2vec = model.wv[word]\n#                     else:\n#                         avgword2vec = avgword2vec + model.wv[word]\n\n#             if avgword2vec is not None:\n#                 avgword2vec = avgword2vec \/ count\n#                 word_embeddings_test.append(avgword2vec)\n#             else:\n#                 word_embeddings_test.append(np.array([0]*512, dtype='float32'))\n\n#             return word_embeddings_test[0]\n\n#     title_emb=[]\n#     title_embs = test['test_title_token'].apply(lambda x: vectors_test(x))\n\n#     for metric in title_embs:\n#         title_emb.append([metric])\n        \n#     word_embeddings = np.concatenate(title_emb)\n#     print('word embeddings shape',word_embeddings.shape)\n        \n#     del model\n#     _ = gc.collect()\n\n#     return word_embeddings","e00049c2":"# word_embeddings_0 = get_word_embeddings()\n# test['word_preds0'] = get_similar(word_embeddings_0,0.22)\n# # # test['word_preds1'] = get_similar(word_embeddings_0,0.36)\n\n# del word_embeddings_0","202977ee":"image_embeddings_0 = get_image_embeddings('dm_nfnet_f0',\"..\/input\/shopee-embedding-df\/F002_ArcFaceLoss_tfold_5_T_False_dm_nfnet_f0_imgsize_192_nume_512_epoch_20_CV_0.9456.pth\",192)\ntest['image_preds0'] = get_similar(image_embeddings_0,0.27)\ntest['image_preds3'] = get_similar(image_embeddings_0,0.36)\ntest['image_predsl1'] = get_similar(image_embeddings_0,0.45)\ntest['image_predsl4'] = get_similar(image_embeddings_0,0.50)\n\ntest['image_preds6'] = get_similar(image_embeddings_0,10,13)\ntest['image_preds7'] = get_similar(image_embeddings_0,10,30)\ntest['image_preds8'] = get_similar(image_embeddings_0,10,40)\n\n\n\nimage_embeddings_1 = get_image_embeddings('swin_small_patch4_window7_224',\"..\/input\/shopee-embedding-df\/F005_ArcFaceLoss_tfold_5_T_False_swin_small_patch4_window7_224_imgsize_224_nume_512_epoch_7_CV_0.9372.pth\",224)\ntest['image_preds1'] = get_similar(image_embeddings_1,0.27)\ntest['image_preds4'] = get_similar(image_embeddings_1,0.36)\ntest['image_predsl2'] = get_similar(image_embeddings_1,0.45)\ntest['image_predsl5'] = get_similar(image_embeddings_1,0.50)\n\n# test['image_preds_13_1'] = get_similar(image_embeddings_1,10,13)\n# test['image_preds_30_1'] = get_similar(image_embeddings_1,10,30)\n# test['image_preds_50_1'] = get_similar(image_embeddings_1,10,40)\n\n\n\nimage_embeddings_2 = get_image_embeddings('efficientnet_b0',\"..\/input\/shopee-embedding-df\/F003_ArcFaceLoss_tfold_5_T_False_efficientnet_b0_imgsize_224_nume_512_epoch_20_CV_0.9304.pth\",224)\ntest['image_preds2'] = get_similar(image_embeddings_2,0.27)\ntest['image_preds5'] = get_similar(image_embeddings_2,0.36)\ntest['image_predsl3'] = get_similar(image_embeddings_2,0.45)\ntest['image_predsl6'] = get_similar(image_embeddings_2,0.50)\n\n# test['image_preds_13_2'] = get_similar(image_embeddings_2,10,13)\n# test['image_preds_30_2'] = get_similar(image_embeddings_2,10,30)\n# test['image_preds_50_2'] = get_similar(image_embeddings_2,10,40)\n\n\n\ntest['image_predsb1'] = get_similar(image_embeddings_0,0.60)\ntest['image_predsb2'] = get_similar(image_embeddings_1,0.60)\ntest['image_predsb3'] = get_similar(image_embeddings_2,0.60)\n\n\n\ndel image_embeddings_0,image_embeddings_1,image_embeddings_2","18a4332c":"bert_embeddings_1 = get_bret_embeddings(\"..\/input\/shopee-embedding-df\/G002_bert_indonesian_tfold_5_T_False_algo_ArcFaceLoss_nume_768_epoch_20_CV_0.9464.pth\",'..\/input\/shopee-embedding-df\/distilbert-base-indonesian')\ntest['bert_preds5'] = get_similar(bert_embeddings_1,0.27)\ntest['bert_preds6'] = get_similar(bert_embeddings_1,0.36)\ntest['bert_predsl1'] = get_similar(bert_embeddings_1,0.45)\ntest['bert_predsl3'] = get_similar(bert_embeddings_1,0.50)\n\n\ntest['bert_under'] = get_similar_two(bert_embeddings_1)\ntest['bert_preds2'] = get_similar(bert_embeddings_1,10,13)\ntest['bert_preds3'] = get_similar(bert_embeddings_1,10,30)\ntest['bert_preds4'] = get_similar(bert_embeddings_1,10,40)\n\ntest['bert_predsb1'] = get_similar(bert_embeddings_1,0.60)\n\n\n# image_bert_embeddings_0 = np.concatenate([image_embeddings_0,bert_embeddings_1],axis=1)\n# test['image_bert_preds0'] = get_similar(bert_embeddings_1,0.27)\n\n\ndel bert_embeddings_1","85b204fd":"berta_embeddings_0 = get_bret_embeddings(\"..\/input\/shopee-embedding-df\/G003_bert_hasa-cased_tfold_5_T_False_algo_ArcFaceLoss_nume_768_epoch_20_CV_0.9524.pth\",'..\/input\/shopee-embedding-df\/albert-base-bahasa-cased')\ntest['berta_preds1'] = get_similar(berta_embeddings_0,0.27)\ntest['berta_preds2'] = get_similar(berta_embeddings_0,0.36)\ntest['berta_predsl1'] = get_similar(berta_embeddings_0,0.45)\ntest['bert_predsl4'] = get_similar(berta_embeddings_0,0.50)\n\ntest['bert_predsb3'] = get_similar(berta_embeddings_0,0.60)\n\n# test['bert_preds_13_1'] = get_similar(berta_embeddings_0,10,13)\n# test['bert_preds_30_1'] = get_similar(berta_embeddings_0,10,30)\n# test['bert_preds_50_1'] = get_similar(berta_embeddings_0,10,50)\n\ndel berta_embeddings_0","29ccbaee":"bert_embeddings_0 = get_bret_embeddings(\"..\/input\/shopee-embedding-df\/G001_bert_tfold_5_T_False_nume_768_epoch_19_CV_0.9541.pth\",'..\/input\/shopee-embedding-df\/paraphrase-xlm-r-multilingual-v1\/0_Transformer')\ntest['bert_preds0'] = get_similar(bert_embeddings_0,0.27)\ntest['bert_preds1'] = get_similar(bert_embeddings_0,0.36)\ntest['bert_predsl2'] = get_similar(bert_embeddings_0,0.45)\ntest['bert_predsl5'] = get_similar(bert_embeddings_0,0.50)\n\ntest['bert_predsb2'] = get_similar(bert_embeddings_0,0.60)\n\n# test['bert_preds_13_2'] = get_similar(bert_embeddings_0,10,13)\n# test['bert_preds_30_2'] = get_similar(bert_embeddings_0,10,30)\n# test['bert_preds_50_2'] = get_similar(bert_embeddings_0,10,40)\n\ndel  bert_embeddings_0","cc2b13a0":"import string\n\ndef removePunctuation(text):\n    punc_translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n    return text.translate(punc_translator)\n\ntest['title_clean'] = test['title'].apply(removePunctuation)\ntitle_to_use = cudf.DataFrame(test).title_clean\n\nprint('Computing text embeddings...')\n# tfidf_vec = TfidfVectorizer(stop_words='english', \n#                             binary=True, \n#                             max_features=21500)\ntfidf_vec = TfidfVectorizer(stop_words=None, \n                            binary=True, \n                            max_features=21500)\ntext_embeddings = tfidf_vec.fit_transform(title_to_use).toarray()\nprint('text embeddings shape',text_embeddings.shape)","ee668224":"def get_text_simier(text_embeddings,threshold=0.75):\n    preds = []\n    CHUNK = 1024\n\n    print('Finding similar titles...')\n    CTS = len(test)\/\/CHUNK\n    if len(test)%CHUNK!=0: CTS += 1\n    for j in tqdm(range( CTS )):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(test))\n    #     print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.65)[0]\n            o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n            \n    del text_embeddings\n    _ = gc.collect()\n    \n    return preds\n\n\n_ = gc.collect()\n\ntest['text_preds'] = get_text_simier(text_embeddings,0.75)\n# test['text_preds2'] = get_text_simier(text_embeddings,0.65)\n# test['text_preds3'] = get_text_simier(text_embeddings,0.55)\n# test.head()\n\ndel tfidf_vec, text_embeddings","497d4585":"# tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\n# test['o_image_phash_preds'] = test.image_phash.map(tmp)\n\n# tmp = test.groupby('image').posting_id.agg('unique').to_dict()\n# test['o_image_preds'] = test.image.map(tmp)\n\n# tmp = test.groupby('title').posting_id.agg('unique').to_dict()\n# test['o_title_preds'] = test.title.map(tmp)\n# test.head()","b66a4840":"from functools import reduce\ndef intersect(*args):\n    return reduce(np.intersect1d, args)\n\n\ndef higher(f,*args):\n    res = {}\n    keys = np.unique(np.concatenate(args))\n    for k in keys: \n        res[k] = np.count_nonzero(np.concatenate(args) == k)\n    output_dict = dict(filter(lambda item: item[1] >= f, res.items()))\n    \n    return np.array(list(output_dict.keys()))\n\ndef combine_for_sub(row):\n#     base2 = intersect(row.text_preds,row.image_predsl4,row.image_predsl5,row.image_predsl6,row.bert_predsl3,row.bert_predsl5)\n    base3 = higher(5,row.text_preds,row.image_predsb1,row.image_predsb2,row.image_predsb3,row.bert_predsb1,row.bert_predsb2,row.bert_predsb3)\n    # image+bert 0.18\n#     base = np.concatenate([base2,base3,row.image_predsb1,row.image_predsb2,row.image_predsb3,row.bert_predsb1,row.bert_predsb2])\n    \n    # image+bert 0.18\n    base = np.concatenate([base3])#,row.image_predsb1,row.image_predsb2,row.image_predsb3,row.bert_predsb1,row.bert_predsb2])\n    \n    \n    # 0.27 zone\n    x = np.concatenate([row.image_preds0]) # image dm_nfnet_f0 0.27\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n        \n    x = np.concatenate([row.bert_preds5]) # bert indnesia 0.27\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.bert_preds0]) # bert english 0.27\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n        \n    x = np.concatenate([row.image_preds1]) # image swin 0.27\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.image_preds2]) # image efficientnet_b0 0.27\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n        \n        \n        \n    # 0.36 zone\n    x = np.concatenate([row.image_preds3]) # image dm_nfnet_f0 0.36\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.bert_preds6]) # bert indnesia 0.36\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )    \n\n    x = np.concatenate([row.bert_preds1]) # bert eglish 0.36\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.image_preds4]) # image swin 0.36\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.image_preds5]) # image efficientnet_b0 0.36\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n        \n        \n    # commone zone\n    x = np.intersect1d(row.image_preds6 ,row.bert_preds2) # 13\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n        \n    x = np.intersect1d(row.image_preds7 ,row.bert_preds3) # 30\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n        \n    x = np.intersect1d(row.image_preds8 ,row.bert_preds4) # 50\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n#     # word zone\n#     x = np.concatenate([row.word_preds0]) # word2vec 0.18\n#     if len(np.unique(x)) > 1:\n#         x = np.concatenate([row.text_preds,x,base])\n#         return ' '.join( np.unique(x)[:51] )\n        \n        \n    # 0.45 zone\n    x = np.concatenate([row.image_predsl1]) # image dm_nfnet_f0 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.bert_predsl1]) # bert indnesia 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.bert_predsl2]) # bert eglish 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.image_predsl2]) # image swin 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.image_predsl3]) # image eff 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    #berta\n    x = np.concatenate([row.berta_preds1]) # berta english 0.27\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.berta_preds2]) # berta english 0.36\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n\n    # 0.45 zone\n    x = np.concatenate([row.image_predsl4]) # image dm_nfnet_f0 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.bert_predsl3]) # bert indnesia 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.bert_predsl5]) # bert eglish 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.image_predsl5]) # image swin 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    x = np.concatenate([row.image_predsl6]) # image eff 0.45\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n    \n    x = np.concatenate([base]) # base\n    if len(np.unique(x)) > 1:\n        x = np.concatenate([x,base])\n        return ' '.join( np.unique(x)[:51] )\n    \n\n    \n    # cout zone\n#     all_model = pd.Series(np.concatenate([row.image_preds6 ,row.bert_preds2,row.image_preds_13_1,row.image_preds_13_2,row.image_preds_13_3,row.bert_preds_13_1,row.bert_preds_13_2])) # 13\n#     df_image = all_model.value_counts()\n#     df_image = pd.DataFrame(df_image.rename_axis('posting_id').reset_index(name='num'))\n#     x = df_image[df_image[\"num\"]>2].posting_id.to_list()\n#     if len(np.unique(x)) > 1:\n#         x = np.concatenate([row.text_preds,x,base])\n#         return ' '.join( np.unique(x)[:51] )\n    \n    \n    \n    \n    # under zone\n    x = np.concatenate([row.bert_under])\n    return ' '.join( np.unique(x)[:51] )","dd74f534":"test['matches'] = test.apply(combine_for_sub,axis=1)\ntest","8c5485bd":"# def split_data(x):\n#     return x.split(\" \")\n\n# test[\"matches\"] = test[\"matches\"].map(split_data)\n# matches_dict = test.set_index('posting_id').to_dict()['matches']\n\n# def get_other(x,matches_dict):\n#     matches_set = set()\n#     for posting_id in matches_dict.keys():\n#         sample = matches_dict[posting_id]\n#         for xx in x:\n#             if xx in sample:\n#                 matches_set = matches_set | set(sample)\n#     # marage other predict\n#     return list(matches_set)[:50]\n\n\n# for posting_id in tqdm(matches_dict.keys()):\n#     x = matches_dict[posting_id]\n#     if len(x) == 1:\n#         y = get_other(x,matches_dict)\n#         matches_dict[posting_id] = y[:51]\n\n# for posting_id in tqdm(matches_dict.keys()):\n#     matches_dict[posting_id] = \" \".join(matches_dict[posting_id])\n    \n# test2 = pd.DataFrame.from_dict(matches_dict, orient='index').reset_index()\n# test2.columns=['posting_id','matches']","d79e879f":"# test2[\"matches\"] = test2[\"matches\"].map(split_data)\n# test2 = pd.merge(test2, test[[\"posting_id\",\"bert_under\"]])\n# test2","307e83c2":"# def combine_for_sub2(row):\n#     x = np.concatenate([row.matches])\n#     if len(np.unique(row.matches)) == 1:\n#         x = np.concatenate([row.matches,row.bert_under])\n#     return ' '.join( np.unique(x)[:50] )","54e70ad8":"# test2['matches'] = test2.apply(combine_for_sub2,axis=1)","bf52bc62":"test[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","cf7a6d16":"def count_num(x):\n    return len(x.split(\" \"))\nsub[\"num\"] = sub[\"matches\"].map(count_num)\nmax(sub[\"num\"].to_list())","ba7df7d6":"min(sub[\"num\"].to_list())","2af4499a":"# use word2vec","a5a415df":"1. # Use Image Embeddings model1","f61bdc23":"# Compute CV Score","556922fc":"# use Bert","d5a18014":"# other ","6a2f2906":"# Use Text Embeddings","8e231793":"# Load Libraries","25f9e5db":"# Load Train Data","2bd4c9b5":"# Write Submission CSV","80a37929":"# use cnn","a008d105":"# Compute RAPIDS Model CV and Infer Submission"}}