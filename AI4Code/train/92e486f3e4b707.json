{"cell_type":{"b3430a87":"code","474cd51e":"code","671b7306":"code","20377327":"code","af4a0c76":"code","5f08e9ae":"code","e6877f82":"code","472bb2bf":"code","edd3172c":"code","09f28d95":"code","92b489fe":"code","743aac8d":"code","ddc85a15":"code","d9997c59":"code","41bd0fd2":"code","048c9ea8":"code","799b0149":"code","452d9c9d":"code","200af545":"code","68823af3":"code","88da69b7":"code","2df05546":"code","b3a4f0ee":"code","eed0b6dc":"code","43c449cb":"code","73ae230e":"code","a8701111":"code","9ba07e03":"code","eef32df8":"code","928edb76":"code","a3c24286":"code","188c7a36":"code","ab72957f":"code","9185cc0c":"code","0315fcf6":"code","c5da1a8a":"code","517d6ce9":"code","5c4293de":"code","d11609e5":"code","48affe69":"code","16a54d8f":"code","7887de61":"code","669d9eda":"code","013a5cad":"code","f016395e":"code","75977731":"code","96893d5c":"code","68d86200":"code","cf9b34c9":"code","d75c4c11":"code","64c1b40b":"code","72929184":"code","2670ea67":"code","c5a28c59":"code","c5c1fd78":"code","9d4cdd58":"code","8c8046f8":"code","44ae0ba2":"code","5ed39261":"code","9cd6cb07":"code","3c31c859":"code","781363bc":"code","4f0404c6":"code","6d6e089c":"code","93a550e3":"code","03e9d4e5":"code","85ebdbb6":"code","14f41bd3":"code","99d8d78b":"code","8f3b2da1":"code","8237a879":"code","03d4b870":"code","b55543c1":"code","d72711fb":"code","bc43cfce":"code","f3d7b097":"code","39f1a254":"code","4d1fd190":"code","34bb8487":"code","4dfcd4d6":"code","37b864c8":"code","c25b6255":"code","38f2a565":"code","3e36ee70":"code","b4855af1":"code","460a5594":"code","e4f342d2":"code","5b7d3279":"code","b64335c8":"code","a968ca65":"code","3fafe4f5":"code","bf46e364":"code","17ca165c":"code","d1a8ea27":"markdown","bbc1c985":"markdown","627b15bd":"markdown","3cba39ee":"markdown","00b4b8e3":"markdown","e8991ad5":"markdown","bc9c3909":"markdown","d6cad59c":"markdown","dd981fe8":"markdown","f94b9cb9":"markdown","c9a0cc24":"markdown","63e158a0":"markdown","dc5a61cd":"markdown","a73f9abe":"markdown","1445a980":"markdown","11cce62d":"markdown","4b0b0a51":"markdown","1e2d8103":"markdown","4ce5d4f8":"markdown","ec82dc4d":"markdown","cb7d0cbf":"markdown","adf26f22":"markdown","0d71be9b":"markdown","312ad2af":"markdown"},"source":{"b3430a87":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","474cd51e":"import matplotlib.pyplot as plt\n\n# PEP 484 type hints are easier with this\nfrom typing import Tuple","671b7306":"fname_train = '..\/input\/covid19-global-forecasting-week-2\/train.csv'\nfname_test = '..\/input\/covid19-global-forecasting-week-2\/test.csv'\nfname_sub = '..\/input\/covid19-global-forecasting-week-2\/submission.csv'","20377327":"df_train = pd.read_csv(fname_train,parse_dates=True)\ndf_test  = pd.read_csv(fname_test, parse_dates=True)\ndf_sub   = pd.read_csv(fname_sub)","af4a0c76":"# Week 2 globals\n# Training dates are from 2020-01-22 to 2020-03-25 ... but the end date can extend each day as more data is added\n#                                                  ... possibly up to 2020-03-31\n# Week 2: Training index goes 29364 and contains 294 locations\nSTART_TRAIN_DATE = df_train['Date'].min()   # Week 2: '2020-01-22'\nEND_TRAIN_DATE   = df_train['Date'].max()   # Week 2: '2020-03-25'\n\n# Prediction dates are from 3\/19\/2020 to 4\/30\/2020 (43 dates per region)\n# Prediction index goes to 12642\nSTART_PREDICTION_DATE = df_test['Date'].min() # Week 2: '2020-03-19'\nEND_PREDICTION_DATE   = df_test['Date'].max() # Week 2: '2020-04-30'","5f08e9ae":"# Tanu's population by country\nfname_pop = '..\/input\/population-by-country-2020\/population_by_country_2020.csv'\ndf_pop = pd.read_csv(fname_pop)","e6877f82":"df_pop.head()\n","472bb2bf":"GLOBAL_FILL_STRING = 'not_provided'","edd3172c":"prediction_scores = {}  # create a dictionary of prediction scores so that I know where to consider improvements","09f28d95":"def check_first_ConfirmedCase_exists(df_in:pd.DataFrame,\n                                     region_list:set)->int:\n    '''Confirm that the data for the region contains at least the first ConfirmedCase\n    '''\n    count_no_cases = 0\n    \n    for sname in sname_set:       \n        df1 = df_in[df_in['SimpleName'] == sname]   # grab the entries for this location only        \n        total_cc = df1['ConfirmedCases'].max()        \n        if (total_cc < 1):\n            print(\"{} as 0 ConfirmedCases\".format(sname))\n            count_no_cases += 1\n            \n    return count_no_cases","92b489fe":"def evaluate_predictions(df_golden:pd.DataFrame,df_predictions:pd.DataFrame,\\\n                         num_wh=0, num_extended=0,\\\n                         model_name=\"no-name\",verbose=False)->Tuple[(float,float)]:\n    '''Compare the predicted values in df_predictions to the values in df_golden for where the data was withheld.\n        Data in the extended region is ignored.\n        \n       Note: DataFrames MUST only contain a single prediction series for ONE location.\n    '''\n    round_places = 4\n    \n    # changed names df_train  --> df_golden\n    #               df_pred   --> df_predictions\n    \n    last_golden_date = df_golden['DateTime'].max()\n    \n    last_predicted_date = df_predictions['DateTime'].max()\n    first_withheld_date = last_golden_date - (num_wh)*pd.to_timedelta('1D')\n    if (verbose):\n        print(\"last_golden_date: {}\".format(last_golden_date))\n        print(\"last_predicted_date: {}\".format(last_predicted_date))\n        print(\"first_withheld_date: {}\".format(first_withheld_date))\n    # comparison period spans the first_withheld_date to the last_golden_date inclusive\n    \n    start_eval_time = first_withheld_date\n    end_eval_time   = last_golden_date\n     \n    df_pred_for_eval = df_predictions[(df_predictions['DateTime'] >= start_eval_time) & \n                                      (df_predictions['DateTime'] <= end_eval_time)]\n    df_answers       = df_golden[(df_golden['DateTime'] >= start_eval_time) & \n                                 (df_golden['DateTime'] <= end_eval_time)]\n    \n    if (verbose):\n        print(\"df_answers is {}\".format(df_answers))\n        print(\"df_pred_for_eval is {}\".format(df_pred_for_eval))\n        \n    \n    c_rmsle = round(rmsle(list(df_answers['ConfirmedCases']),\n                          list(df_pred_for_eval['ConfirmedCases'])),\n                    round_places)\n    f_rmsle = round(rmsle(list(df_answers['Fatalities']),\n                          list(df_pred_for_eval['Fatalities'])),\n                    round_places)\n    \n    if (verbose):\n        print(\"Model {} for {} through {}\".format(model_name,start_eval_time, end_eval_time))\n        print(\"   evaluation (ideal --> 0):\\n    RMSLE confirmed: {}\\n    RMSLE fatalities: {}\".format(c_rmsle,f_rmsle))\n    \n    return c_rmsle, f_rmsle","743aac8d":"def evaluate_predictions2(df_golden:pd.DataFrame,df_predictions:pd.DataFrame,\\\n                         num_wh=0, num_extended=0,\\\n                         model_name=\"no-name\",verbose=False)->Tuple[(float,float)]:\n    '''Compare the predicted values in df_predictions to the values in df_golden for where the data was withheld.\n        Data in the extended region is ignored.\n        \n       Note: DataFrames MUST only contain a single prediction series for ONE location.\n    '''\n    round_places = 4\n    \n    # changed names df_train  --> df_golden\n    #               df_pred   --> df_predictions\n    \n    last_golden_date = df_golden['DateTime'].max()\n    \n    last_predicted_date = df_predictions['DateTime'].max()\n    first_withheld_date = last_golden_date - (num_wh)*pd.to_timedelta('1D')\n    if (verbose):\n        print(\"last_golden_date: {}\".format(last_golden_date))\n        print(\"last_predicted_date: {}\".format(last_predicted_date))\n        print(\"first_withheld_date: {}\".format(first_withheld_date))\n    # comparison period spans the first_withheld_date to the last_golden_date inclusive\n    \n    start_eval_time = first_withheld_date\n    end_eval_time   = last_golden_date\n     \n    df_pred_for_eval = df_predictions[(df_predictions['DateTime'] >= start_eval_time) & \n                                      (df_predictions['DateTime'] <= end_eval_time)]\n    df_answers       = df_golden[(df_golden['DateTime'] >= start_eval_time) & \n                                 (df_golden['DateTime'] <= end_eval_time)]\n    \n    if (verbose):\n        print(\"df_answers is {}\".format(df_answers))\n        print(\"df_pred_for_eval is {}\".format(df_pred_for_eval))\n        \n    \n    c_rmsle = round(rmsle(list(df_answers['ConfirmedCases']),\n                          list(df_pred_for_eval['maxConfirmedCases'])),\n                    round_places)\n    f_rmsle = round(rmsle(list(df_answers['Fatalities']),\n                          list(df_pred_for_eval['Fatalities'])),\n                    round_places)\n    \n    if (verbose):\n        print(\"Model {} for {} through {}\".format(model_name,start_eval_time, end_eval_time))\n        print(\"   evaluation 2 (ideal --> 0):\\n    RMSLE confirmed: {}\\n    RMSLE fatalities: {}\".format(c_rmsle,f_rmsle))\n    \n    return c_rmsle, f_rmsle","ddc85a15":"# From https:\/\/www.kaggle.com\/marknagelberg\/rmsle-function\nimport math\n\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(terms_to_sum) * (1.0\/len(y))) ** 0.5","d9997c59":"# unit test for evaluate_predictions, withheld = 1, extended = 2\ngolden_data    = {'DateTime':pd.to_datetime(['2020-06-01','2020-06-02','2020-06-03']),\n                  'ConfirmedCases':[2,4,8],\n                  'Fatalities':[0,0,1],\n                  'Province_State':['PS_TBD','PS_TBD','PS_TBD'],\n                  'Country_Region':['CR_TBD','CR_TBD','CR_TBD'],\n                  'Id':[201,202,203],\n                  'SimpleName':['TBD','TBD','TBD']}\npredicted_data = {'DateTime':pd.to_datetime(['2020-06-01','2020-06-02','2020-06-03','2020-06-04','2020-06-05']),\n                  'ConfirmedCases':[2,4,6,8,10],\n                  'Fatalities':[0,0,0,1,2],\n                  'SimpleName':['TBD','TBD','TBD','TBD','TBD']}\ndf_golden = pd.DataFrame.from_dict(golden_data)\ndf_predicted = pd.DataFrame.from_dict(predicted_data)\n\n\nunit_c, unit_f = evaluate_predictions(df_golden,df_predicted,1,2,'unit test',False)\nassert((unit_c == 0.1777) & (unit_f == 0.4901)),'Failed unit test for evaluate_predictions'","41bd0fd2":"def create_parameter_database():\n    '''Creates a database of parameters for use in the model.\n       Note that this uses \n           global parameter sname_set \n           default values   for all parameters (intent to replace later)\n    '''\n    # Create a database of BETA, GAMMA, i_factor, r_factor for each SimpleName\n    #  Initially, just use the same values I used for CA\n    #  Next step will be to customize based on the region, e.g. temperature, proximity to international airports, etc.\n\n\n    # defaults   beta=0.38  gamma=0.14   ifx=30   rfx=1000\n    ones_column = np.ones((len(sname_set),))\n    beta_column = 0.38 * ones_column\n    gamma_column = 0.14 * ones_column\n    ifx_column = 30.0 * ones_column\n    rfx_column = 1000.0 * ones_column  \n\n    pop_column = 1000000 * ones_column  ## population - default to 1 million\n\n    data = {'beta':beta_column,'gamma':gamma_column,'ifx':ifx_column,'rfx':rfx_column,'population':pop_column}\n    return data","048c9ea8":"def plot_train_predictions(df_pred_in,df_train_in,title_str,ylim_factor=1.2):\n    fig, axes = plt.subplots(3,2,figsize=(12,6))\n    plt.tight_layout()    \n    \n    title_cc = title_str + ' ConfirmedCases'\n    axes[0,0].set_title(title_cc)\n    df_pred_in.plot(ax=axes[0,0],legend=True,x='DateTime',y='ConfirmedCases',label='p_Confirmed',c='lightgreen')\n    df_train_in.plot(ax=axes[0,0],legend=True,x='DateTime',y='ConfirmedCases',label='Confirmed',c='orange')\n\n    title_f = title_str + ' Fatalities'\n    axes[0,1].set_title(title_f)\n    df_pred_in.plot(ax=axes[0,1],legend=True,x='DateTime',y='Fatalities',label='p_Fatalities',c='green')\n    df_train_in.plot(ax=axes[0,1],legend=True,x='DateTime',y='Fatalities',label='Fatalities',c='darkblue')\n    \n    #Scale y-axis to be the known ConfirmedCases + 20% to ensure model dovetails with known data\n    ylim_cc = ylim_factor * df_train_in['ConfirmedCases'].max()\n    axes[1,0].set_title(title_cc)\n    axes[1,0].set_ylim(0,ylim_cc)\n    df_pred_in.plot(ax=axes[1,0],legend=True,x='DateTime',y='ConfirmedCases',label='p_Confirmed',c='lightgreen')\n    df_train_in.plot(ax=axes[1,0],legend=True,x='DateTime',y='ConfirmedCases',label='Confirmed',c='orange')\n    \n    #title_f = title_str + ' Fatalities'\n    ylim_f = ylim_factor * df_train_in['Fatalities'].max()\n    axes[1,1].set_title(title_f)\n    axes[1,1].set_ylim(0,ylim_f)\n    df_pred_in.plot(ax=axes[1,1],legend=True,x='DateTime',y='Fatalities',label='p_Fatalities',c='green')\n    df_train_in.plot(ax=axes[1,1],legend=True,x='DateTime',y='Fatalities',label='Fatalities',c='darkblue')\n    \n    try:\n        df_pred_in['maxConfirmedCases'].shape\n        title_cc = title_str + ' maxConfirmedCases'\n        axes[2,0].set_title(title_cc)\n        df_pred_in.plot(ax=axes[2,0],legend=True,x='DateTime',y='maxConfirmedCases',label='p_maxConfirmed',c='lightgreen')\n        df_train_in.plot(ax=axes[2,0],legend=True,x='DateTime',y='ConfirmedCases',label='Confirmed',c='orange')\n    except:\n        pass","799b0149":"try: \n    df_sname['maxConfirmedCases'].shape\n    print('yeah')\nexcept:\n    print('nah')","452d9c9d":"df_train.head(n=2)","200af545":"df_train.tail(n=2)","68823af3":"df_test.head(n=2)","88da69b7":"df_test.tail(n=2)","2df05546":"df_sub.tail(n=2)","b3a4f0ee":"df_train[['Province_State','Country_Region']] = df_train[['Province_State','Country_Region']].fillna(GLOBAL_FILL_STRING)\ndf_test[['Province_State','Country_Region']] = df_test[['Province_State','Country_Region']].fillna(GLOBAL_FILL_STRING)","eed0b6dc":"df_train['SimpleName'] = df_train['Province_State'] + '__' + df_train['Country_Region']   # for fetching data\ndf_test['SimpleName'] = df_test['Province_State'] + '__' + df_test['Country_Region']      # for storing predictions","43c449cb":"df_train['DateTime'] = pd.to_datetime(df_train['Date'])   #format = '%Y-%m-%d'\ndf_test['DateTime']  = pd.to_datetime(df_test['Date'])   #format = '%Y-%m-%d'","73ae230e":"df_test.tail(n=3)","a8701111":"df_train.tail(n=3)","9ba07e03":"df_test.dtypes","eef32df8":"df_train['SimpleName'].nunique()","928edb76":"sname_set = list(set(df_train['SimpleName']))    # another handy global","a3c24286":"# Make sure there aren't any locations with no ConfirmedCases reported yet\nassert(0 == check_first_ConfirmedCase_exists(df_train,sname_set)),\"Data contains locations with NO ConfirmedCases!\"","188c7a36":"# Start with default values for all locations\n\nparameter_data = create_parameter_database()\ndb_parameters = pd.DataFrame(parameter_data, index=sname_set,\\\n                            columns=['beta','gamma','ifx','rfx','population'])","ab72957f":"db_parameters.head(n=2)","9185cc0c":"ctry_col = r'Country (or dependency)'\npop_col  = r'Population (2020)'","0315fcf6":"df_pop[df_pop[ctry_col] == 'France'][pop_col].values[0]  # test","c5da1a8a":"df_pop.head(n=2)","517d6ce9":"# TODO: Once the basic functionality generates a valid submission, this is where the creative model tweaks will happen\n# UPDATE the db_parameters pd.DataFrame\n\n# populate the parameters with the data from df_pop\n\n\nfor index,row in db_parameters.iterrows():\n    state, ctry = index.split('__')   # ctry is ['not_provided', 'Jersey']\n    if (state == GLOBAL_FILL_STRING):\n        try:\n            pop = df_pop[df_pop[ctry_col] == ctry][pop_col].values[0]\n        except:\n            # 8 or so locations use other names in the population database\n            if (ctry == 'Czechia'):\n                pop = df_pop[df_pop[ctry_col] == r'Czech Republic (Czechia)'][pop_col].values[0]\n            elif (ctry == r'Congo (Kinshasa)'):\n                pop = df_pop[df_pop[ctry_col] == r'DR Congo'][pop_col].values[0]\n            elif (ctry == r'Congo (Brazzaville)'):\n                pop = df_pop[df_pop[ctry_col] == r'Congo'][pop_col].values[0]\n            elif (ctry == r'Saint Vincent and the Grenadines'):\n                pop = df_pop[df_pop[ctry_col] == r'St. Vincent & Grenadines'][pop_col].values[0]                \n            elif (ctry == r'Korea, South'):\n                pop = df_pop[df_pop[ctry_col] == r'South Korea'][pop_col].values[0]                                \n            elif (ctry == r\"Cote d'Ivoire\"):\n                pop = df_pop[df_pop[ctry_col] == r\"C\u00f4te d'Ivoire\"][pop_col].values[0]   \n            elif (ctry == r'Saint Kitts and Nevis'):\n                pop = df_pop[df_pop[ctry_col] == r'Saint Kitts & Nevis'][pop_col].values[0]    \n            elif (ctry == r'Taiwan*'):\n                pop = df_pop[df_pop[ctry_col] == r'Taiwan'][pop_col].values[0]   \n            elif (ctry == r'Diamond Princess'):\n                pop = 3711   # from Wikipedia  \n            else:\n                pop=500000   # set a default population of 500k\n                print(\"No population data available for {}, {}\".format(state,ctry))\n        #print('population is {}'.format(pop))\n        #print(\"{} has population {}\".format(ctry,pop))\n        db_parameters.loc[index,'population'] = pop\n    else:\n        # Need to figure out how to handle state populations!\n        print(\"State missing population data: {}, {}\".format(state,ctry))","5c4293de":"# Create placeholders for ConfirmedCases and Fatalities\ncol_len = df_test.shape[0]\nzeros_column = np.zeros((col_len,))\ndf_test['ConfirmedCases'] = zeros_column\ndf_test['Fatalities'] = zeros_column\n\n# Create a placeholder for max_confirmed -- dataset does not reduce the number of ConfirmedCases to reflect\n#   only the active cases\ndf_test['maxConfirmedCases'] = zeros_column","d11609e5":"df_train_preserve = df_train.copy()  # for debugging and testing\ndf_test_preserve  = df_test.copy()","48affe69":"# df_train ready to accept predictions\ndf_test.tail(n=4)","16a54d8f":"# check on France data in df_test -- sanity check of randomly selected location to confirm all 0\ndf_test[df_test['SimpleName'] == 'not_provided__France'].head(n=22)","7887de61":"def prepare_prediction_template(df_in,withhold=0,extend_to_date = END_PREDICTION_DATE,\n                                verbose=False):\n    '''Prepare a pd.DataFrame that has the known data filled in and the dates ready for the predictions.\n        prepare_prediction_template(df_training,withhold=0)   <-- use ALL data, then extend dates\n        prepare_prediction_template(df_training,withhold=13)  <-- withhold 13 days, then extend dates\n        \n       Use withhold=0 for final submission only.\n    '''\n    # GLOBAL reminders\n    # start_train_date = '2020-01-22'\n    # end_train_date   = '2020-03-25'\n    # start_prediction_date = '2020-03-19'\n    # end_prediction_date   = '2020-04-30'\n    \n    # avoid stupid errors with + or -\n    withhold = abs(withhold)\n    \n    if (withhold != 0):\n        # calculate the last date in the training set, then calculate which date to go back to in order\n        #    withhold info for training\n        # df_train['DateTime'].max() - pd.to_timedelta('13D')     # pattern\n        wth_str = str(withhold) + 'D'\n        last_date = df_in['DateTime'].max() - pd.to_timedelta(wth_str)\n        df_local = df_in[df_in['DateTime']<=last_date].copy()\n        if (verbose):\n            print(\"pptemp withhold={} so last_date to copy was {}\".format(withhold,last_date))\n    else:\n        df_local = df_in.copy()\n        if (verbose):\n            print(\"pptemp withhold=0 so made complete copy before extension.\")\n        \n    # When extending data, extend these info columns, too\n    keep_ps =  df_in.iloc[1]['Province_State']\n    keep_cr =  df_in.iloc[1]['Country_Region']\n    keep_id =  df_in.iloc[1]['Id'] # not likely to be needed, TBD\n    keep_sn =  df_in.iloc[1]['SimpleName']\n    \n    # Extend the predictions through end_prediction_date; first, find out how far it goes right now\n    max_date = df_local['DateTime'].max()\n\n    num_extensions = (pd.to_datetime(extend_to_date) - max_date).days\n    if (verbose):\n        print(\"Max date was {} so we need {} extensions.\".format(max_date,num_extensions))\n    \n    # Prepare to extend the dates\n    next_date = max_date + pd.to_timedelta('1D')\n    #print(\"types are {} and {}\".format(type(next_date),type(end_prediction_date)))\n    end_date_datetime = pd.to_datetime(extend_to_date)\n    while (next_date <= end_date_datetime):\n        #print(\"Adding a row for new date {}\".format(next_date))\n        df_local = df_local.append({'Id':0,\n                                    'Province_State':keep_ps,\n                                    'Country_Region':keep_cr,\n                                    'DateTime':next_date,\n                                    'ConfirmedCases':0,\n                                    'maxConfirmedCases':0,\n                                    'Fatalities':0,\n                                    'SimpleName':keep_sn},\n                                   ignore_index=True)\n        next_date = next_date + pd.to_timedelta('1D')\n    \n    return df_local,num_extensions","669d9eda":"# unit test for prepare_predictions_template\ndf_golden","013a5cad":"df_1, num_e = prepare_prediction_template(df_golden,withhold=1,extend_to_date='2020-06-09',verbose=True)\nprint(\"Output num_e={} and df is \\n {}\".format(num_e,df_1))","f016395e":"def calculate_SIR_next_step(S,I,R,N,beta,gamma):\n    '''Given the current status of S,I,R, N the total population, and the transition factors, \n        the next step for each category is calculated and returned.\n    '''   \n    # Governing equations:\n    #  dS\/dt = (-1*BETA*S*I)\/N\n    #  dI\/dt = (BETA*S*I)\/N - GAMMA*I\n    #  dR\/dt = GAMMA*I\n    \n    drdt = gamma*I\n    dsdt = -1*beta*S*I\/N\n    didt = -1*dsdt - drdt\n    \n    # print(\"dsdt {}  didt {}  drdt {}\".format(dsdt,didt,drdt))\n    \n    S = S + dsdt\n    I = I + didt\n    R = R + drdt\n    \n    # Saturate the values\n    S = max(S,0)  # max because it's heading to 0\n    R = min(R,N)  # min because it's heading to N\n    \n    if (I<0):\n        I = 0\n    elif (I>N):\n        I = N\n    \n    return S,I,R  ","75977731":"def sir_model(df_in,N=100000,beta=.2,gamma=.4,i_factor=1,r_factor=1,verbose=False):\n    '''Simple SIR model assuming \n            S = population, N\n            I = ConfirmedCases\n            R = Fatalities\n    '''\n    df_local = df_in.copy()   \n    first_row = True\n    first_CC = False\n    \n    for index, data_row in df_local.iterrows(): \n        r          = index\n        r_previous = index - 1     \n        \n        if (verbose):\n            print(\"Working on index {} and DateTime {}\".format(index,df_local.loc[r,'DateTime']))\n            \n        c = data_row['ConfirmedCases']\n        f = data_row['Fatalities']\n\n        # No changes needed for the first timestamp\n        if (first_row):\n            first_row = False\n            df_local.loc[r,'maxConfirmedCases'] = df_local.loc[r,'ConfirmedCases'] # copy over\n        else:\n            I = df_local.loc[r_previous,'ConfirmedCases'] * i_factor  # factor to guess at under-estimate\n            R = df_local.loc[r_previous]['Fatalities']    * r_factor  # factor to guess at under-estimate\n            S = N - (I + R) \n        \n            # Start predictions only after the first ConfirmedCase has been found\n            if (c > 0):\n                first_CC = True   # next time we get c==0, we'll start predicting\n                df_local.loc[r,'maxConfirmedCases'] = df_local.loc[r,'ConfirmedCases'] # copy over\n            if (first_CC & (c == 0)):    # no prediction for ConfirmedCases\n                nS,nI,nR = calculate_SIR_next_step(S,I,R,N,beta,gamma)\n\n                # Resolving the chained indexing issue\n                df_local.loc[r,'ConfirmedCases'] = round(nI\/i_factor)\n                df_local.loc[r,'Fatalities'] = round(nR\/r_factor)\n                \n                # Keep track of the maxConfirmedCases\n                df_local.loc[r,'maxConfirmedCases'] = max(df_local.loc[r,'ConfirmedCases'],\n                                                          df_local.loc[r_previous,'maxConfirmedCases'])\n                if (verbose):\n                    print(\"{} SIR previous {} {} {} is changing to {} {} {}\".format(df_local.loc[r,'DateTime'],S,I,R,\n                                                    round(nS),round(nI),round(nR)))\n            else:       \n                if (verbose):\n                    print(\"{} SIR current {} {} {} no prediction needed\".format(df_local.loc[r,'DateTime'],S,I,R))\n    return df_local","96893d5c":"# Week 2's dates are\n# start_prediction_date = '2020-03-19'\n# end_prediction_date   = '2020-04-30'\n\nmdates = pd.date_range('2020-03-19', '2020-04-30', freq='D')\ndate_list = list(mdates.strftime('%Y-%m-%d'))\n\ndate_list2 = ['2020-03-19','2020-03-20','2020-03-21','2020-03-22','2020-03-23','2020-03-24','2020-03-25',\\\n             '2020-03-26','2020-03-27',\\\n             '2020-03-28','2020-03-29','2020-03-30','2020-03-31','2020-04-01','2020-04-02','2020-04-03','2020-04-04',\\\n             '2020-04-05','2020-04-06','2020-04-07','2020-04-08','2020-04-09','2020-04-10','2020-04-11','2020-04-12',\\\n             '2020-04-13','2020-04-14','2020-04-15','2020-04-16','2020-04-17','2020-04-18','2020-04-19','2020-04-20',\\\n             '2020-04-21','2020-04-22','2020-04-23','2020-04-24','2020-04-24','2020-04-25','2020-04-26','2020-04-27',\\\n             '2020-04-28','2020-04-29','2020-04-30'\n            ]\n    \ndef copy_preditions_to_final_list(location, df_predictions, df_final_list,verbose=False):\n    '''Copy the prediction results into the final predictions DataFrame'''\n    \n    if (verbose):\n        print(\"Copying predictions for {}:\".format(location))\n        \n    assert(100==df_predictions.shape[0]),'Error 1 - Passed in an incorrect predictions DataFrame'    \n    df_small = df_predictions[df_predictions['SimpleName'] == location]\n    assert(df_small.shape[0]==df_predictions.shape[0]),'Error 2 - Passed in an incorrect predictions DataFrame'  \n    # in theory, the length of df_predictions and the length of df_small should be the same  \n    \n    for date1 in date_list:\n\n        date1_dt = pd.to_datetime(date1)\n        \n        # create masks to get the correct items lined up\n        mask_location = df_final_list['SimpleName'] == location\n        mask_date = df_final_list['DateTime'] == date1_dt        \n        idx_final = df_final_list[mask_location & mask_date].index[0]\n        \n        mask_date2 = df_small['DateTime'] == date1_dt\n        idx_small = df_small[mask_date2].index[0]        \n        \n        if (verbose):\n            print('idx: final={}  small={}'.format(idx_final, idx_small))\n        df_final_list.at[idx_final,'ConfirmedCases'] = df_small.at[idx_small,'ConfirmedCases']\n        df_final_list.at[idx_final,'maxConfirmedCases'] = df_small.at[idx_small,'maxConfirmedCases']\n        df_final_list.at[idx_final,'Fatalities']     = df_small.at[idx_small,'Fatalities']\n        \n        #break","68d86200":"# for each SimpleName, do an SIR forecast with a fixed set of parameters\n\n    # GLOBAL reminders\n    # start_train_date = '2020-01-22'\n    # end_train_date   = '2020-03-25'\n    # start_prediction_date = '2020-03-19'\n    # end_prediction_date   = '2020-04-30'\n    \ndef single_model(location, practice_withhold=0, verbose=False):\n\n    this_name = location\n    # practice_withhold \n        #  0 means withhold  0 days of data -- use only for final submission!\n        # 13 means withhold 13 days of data\n\n    df_sname = df_train[df_train['SimpleName'] == this_name].copy()\n    \n    df_sname['maxConfirmedCases'] = 0\n\n    df_sname_template,num_extended = prepare_prediction_template(df_sname,(-1*practice_withhold),verbose=verbose)     \n\n    \n    df_sname.head()\n\n    # defaults   beta=0.38  gamma=0.14   ifx=30   rfx=1000 N = 500000\n    this_beta, this_gamma, this_ifx, this_rfx, this_pop = db_parameters.loc[this_name]\n\n    df_SIR_predictions = sir_model(df_sname_template,N=this_pop,\\\n                                    beta=this_beta,gamma=this_gamma,\\\n                                    i_factor = this_ifx, r_factor = this_rfx)\n\n    # Remember that we can ONLY evaluate versus the original training data, so it can only go from start to\n    #    end of training data at most.\n    if (verbose):\n        starter = practice_withhold + num_extended\n        print(\"ends to compare are \\n {} \\n ***AND*** \\n {}\".format(df_sname,\n                                                                      df_SIR_predictions.iloc[:(-1*num_extended)]))\n    rmsle_c, rmsle_f = evaluate_predictions(df_sname,df_SIR_predictions,\n                                          practice_withhold,num_extended,\n                                          'simple SIR',verbose)\n    rmsle_c2, rmsle_f2 = evaluate_predictions2(df_sname,df_SIR_predictions,\n                                          practice_withhold,num_extended,\n                                          'simple SIR',verbose)\n    plot_title = this_name + \"  v1 SIR plots\"\n    #plot_train_predictions(df_SIR_practice,df_sname,plot_title,ylim_factor = 6)\n\n    print(\"Finished evaluating...{}: {} {}\".format(this_name,rmsle_c,rmsle_f))\n    print(\"Finished evaluating 2...{}: {} {}\".format(this_name,rmsle_c2,rmsle_f2))\n    copy_preditions_to_final_list(this_name, df_SIR_predictions, df_test)\n    \n    # update dictionary of prediction scores\n    prediction_scores[this_name] = [rmsle_c2, rmsle_f2]\n    \n    return df_SIR_predictions, df_sname, plot_title","cf9b34c9":"mdf_SIR_practice, mdf_sname, mplot_title = single_model('Hubei__China',practice_withhold=24,verbose=False)","d75c4c11":"prediction_scores","64c1b40b":"practice_withhold = 7\nverbose = True\nthis_name = 'Hubei__China'\n    # practice_withhold \n    #  0 means withhold  0 days of data -- use only for final submission!\n    # 13 means withhold 13 days of data\n\ndf_sname = df_train[df_train['SimpleName'] == this_name].copy()\n    \ndf_sname['maxConfirmedCases'] = 0\n\ndf_sname_template,num_extended = prepare_prediction_template(df_sname,(-1*practice_withhold),verbose=verbose)     \n\n\ndf_sname.head()\n\n# defaults   beta=0.38  gamma=0.14   ifx=30   rfx=1000 N = 500000\nthis_beta, this_gamma, this_ifx, this_rfx, this_pop = db_parameters.loc[this_name]\n\ndf_SIR_predictions = sir_model(df_sname_template,N=this_pop,\\\n                                beta=this_beta,gamma=this_gamma,\\\n                                i_factor = this_ifx, r_factor = this_rfx)","72929184":"df_sname","2670ea67":"df_sname_template","c5a28c59":"df_SIR_predictions","c5c1fd78":"    rmsle_c, rmsle_f = evaluate_predictions(df_sname,df_SIR_predictions,\n                                          practice_withhold,num_extended,\n                                          'simple SIR',verbose)\n    rmsle_c2, rmsle_f2 = evaluate_predictions2(df_sname,df_SIR_predictions,\n                                          practice_withhold,num_extended,\n                                          'simple SIR',verbose)\n    plot_title = this_name + \"  v1 SIR plots\"\n    #plot_train_predictions(df_SIR_practice,df_sname,plot_title,ylim_factor = 6)\n\n    print(\"Finished evaluating...{}: {} {}\".format(this_name,rmsle_c,rmsle_f))\n    print(\"Finished evaluating 2...{}: {} {}\".format(this_name,rmsle_c2,rmsle_f2))","9d4cdd58":"##### END single country investigation ###################","8c8046f8":"df_sname = df_train[df_train['SimpleName'] == 'not_provided__Germany']","44ae0ba2":"df_sname_template_1,num_extended = prepare_prediction_template(df_sname,-5)","5ed39261":"num_extended","9cd6cb07":"# df_sname_template_1.tail(n=40)","3c31c859":"df_SIR_practice = sir_model(df_sname_template_1) # use all defaults","781363bc":"# df_SIR_practice.tail(n=40)","4f0404c6":"# df_sname.tail(n=40)","6d6e089c":"\n# mdf_SIR_practice, mdf_sname, mplot_title = single_model('Alberta__Canada',practice_withhold=24,verbose=True)\n# mdf_SIR_practice, mdf_sname, mplot_title = single_model('not_provided__Italy',practice_withhold=24,verbose=True)\nmdf_SIR_practice, mdf_sname, mplot_title = single_model('Hubei__China',practice_withhold=24,verbose=False)","93a550e3":"mdf_sname.head()","03e9d4e5":"plot_train_predictions(mdf_SIR_practice,mdf_sname,mplot_title,ylim_factor = 4)","85ebdbb6":"selected_country_region = \"US\"\nselected_province_state = \"Washington\"     # can also use GLOBAL_FILL_STRING\n\nsname = selected_province_state + '__' + selected_country_region\nprint(\"Sanity check for {}\".format(sname))","14f41bd3":"mdf_SIR_practice, mdf_sname, mplot_title = single_model(sname,7)","99d8d78b":"plot_train_predictions(mdf_SIR_practice,mdf_sname,mplot_title,ylim_factor = 2)","8f3b2da1":"db_parameters.loc['Isle of Man__United Kingdom']","8237a879":"mdf_SIR_practice, mdf_sname, mplot_title = single_model('Isle of Man__United Kingdom',7)\nplot_train_predictions(mdf_SIR_practice,mdf_sname,mplot_title,ylim_factor = 2)","03d4b870":"mdf_sname.tail(n=15)","b55543c1":"sname_set\nmini_set = ['not_provided__Togo',\n 'not_provided__Cambodia',\n 'Connecticut__US',\n 'Jilin__China',\n 'Greenland__Denmark',\n 'Maine__US',\n 'Pennsylvania__US',\n 'Quebec__Canada',\n 'New Jersey__US',\n 'Manitoba__Canada',\n 'not_provided__Haiti']","d72711fb":"# full set runs about 3 minutes\n# start with clean copies\ndf_train = df_train_preserve.copy()\ndf_test = df_test_preserve.copy()","bc43cfce":"# Run all models \nfor sname in sname_set:\n    mdf_SIR_practice, mdf_sname, mplot_title = single_model(sname,0)   # omit last 7 days to check score","f3d7b097":"# Sanity check that all models are complete\nlast_day = df_test[df_test['DateTime'] == pd.to_datetime('2020-04-30')]\nprint(\"Final prediction for 2020-04-30 has {} ConfirmedCases and {} Fatalities\".format(last_day['ConfirmedCases'].sum(),\\\n                                                                                          last_day['Fatalities'].sum()))","39f1a254":"last_day","4d1fd190":"prediction_scores","34bb8487":"df_scores = pd.DataFrame.from_dict(prediction_scores,orient='index', columns=['cc_score','f_score'])","4dfcd4d6":"df_scores['cc_score'].idxmax()","37b864c8":"df_scores['f_score'].idxmax()","c25b6255":"df_test.shape","38f2a565":"df_test.head(n=3)","3e36ee70":"# write out my results so that I can find issues\n# df_test.to_csv('df_test_results2.csv')","b4855af1":"df_sub = df_test.drop(['Province_State','SimpleName','Country_Region','Date','DateTime'],axis=1)","460a5594":"# final dates 3\/19\/2020 to 4\/30\/2020","e4f342d2":"df_sub.head()","5b7d3279":"df_sub_mod = df_sub.drop('ConfirmedCases', axis=1)","b64335c8":"df_sub_mod.head()","a968ca65":"df_sub_mod.columns = ['ForecastId','Fatalities','ConfirmedCases']\ndf_sub_mod.head()","3fafe4f5":"df_sub = df_sub_mod","bf46e364":"df_sub.tail()","17ca165c":"df_sub.to_csv('submission.csv',index=False)\n\n## turned out to have 12642 index lines, which is correct","d1a8ea27":"### Add 2 columns to train.csv and also test.csv:\n - SimpleName = Province_State  __ Country_Region\n - DateTime = Date converted to pandas DateTime format","bbc1c985":"df_ccp = df_cc.pivot(index='SimpleName',columns='DateTime')\ndf_ccp.iloc[6].plot()","627b15bd":"### Globals","3cba39ee":"Predictions made by these models will be evaluated by Kaggle using RMSLE metrics. \n\nBy way of doing a sanity check, the existing data and the modelled data should be plotted and compared. ","00b4b8e3":"### *** Single example ***","e8991ad5":"<a id='section_background'><\/a>\n## Notebook setup\nThis section contains imports, globals, and functions that will be used to work with the data.","bc9c3909":"Visual representation of how the DataFrames compare:\n    Train - this is the input data from Kaggle and represents golden data\n         n n n\n         n n n \n         n n n\n         m m m\n         m m m\n        \ndef prepare_prediction_template\n         This function takes parameters for the user to decide \n             how many days to withhold\n             to what date to extend\n     \n                                 in (golden_data from Train)               out (template_for_predictions)\n                                         n n n                                       n n n\n                                         n n n                                       n n n\n                                         n n n                                       n n n\n                                         m m m                       withheld |      0 0 0        \n                                         m m m                       withheld |      0 0 0\n                                                                     extended |      0 0 0\n           \ndef sir_model\n    This function runs the input template through the model and fills in the missing predictions\n              in - template_for_predictions\n             out - predictions_all_filled_in\n        \ndef evaluate_model\n              in - golden_data from Train\n              in - predictions_all_filled_in\n               w - number withheld\n               e - number extended\n      Do the evaluation ONLY on the data that was withheld\n             -- data prior to that will be 100% correct\n             -- data in the extended section cannot be evaluated","d6cad59c":"## Table of Contents\n - [SIR Model](#section_sir)\n - [Notebook setup](#section_background)\n - [EDA](#section_EDA)\n - [Models](#section_models)\n - [Model Evaluations](#section_model_evaluations)\n - [Conclusion](#section_conclusion)","dd981fe8":"##### BEGIN single country investigation ###################","f94b9cb9":"### Functions","c9a0cc24":"# sanity check with plots\ndf_cc = df_test.copy()\ndf_cc = df_cc.drop(['ForecastId','Country_Region','Province_State','Date','Fatalities'],axis=1)\ndf_f  = df_test.copy()\ndf_f = df_f.drop(['ForecastId','Country_Region','Province_State','Date','ConfirmedCases'],axis=1)\n\n","63e158a0":"Modify parameters and customize by location (TODO)","dc5a61cd":"<a id='section_models'><\/a>\n## Models","a73f9abe":"### Create a list of the unique locations","1445a980":"<a id='section_conclusion'><\/a>\n## Conclusion","11cce62d":"### Finish complete preparation for all locations","4b0b0a51":"# COVID-19 World (Week 2) - modified SIR\n\nThe 2019-2020 COVID-19 pandemic is both a devastating virus and an opportunity for us to join together to explore our understanding of disease, disease transmission, and the factors that we can observe and control to address the spread of disease and its impact.\n\nIn this notebook, the primary focus is on the challenges with the available data. Given the high number of anecdotes regarding under-counting, lack of testing, and an overwhelmed medical system, which pieces of the data can we use reliably? If we rely on those, do we end up with reasonable predictions of future data?","1e2d8103":"<a id='section_EDA'><\/a>\n## EDA\n\nThe format and length of the input and output files changes slightly between weeks 1 and 2:\n\nWeek 1:\n - Final predictions for 12,212 ForecastId; forecast only ConfirmedCases and Fatalities\n - ForecastID are broken down into\n - 284 forecast locations X 43 dates 3\/12\/2020 - 4\/23\/2020 inclusive = 12212 predictions\n \nWeek 2:\n\n - Final predictions for 12,642 ForecastId; forecast only ConfirmedCases and Fatalities\n - ForecastID are broken down into\n  - 294 forecast locations x 43 dates 3\/19\/2020 - 4\/30\/2020 inclusive\n - TRAIN data has been modified from Week 1:\n  - Dropped: \n    - latitude\n    - longitude\n  - Changed:  \n    - Province\/State -> Province_State\n    - Country\/Region -> Country_Region","4ce5d4f8":"### Week 1:\n - *Notebook for COVID-19 CA* was developed first. That notebook explored \n  - *Extended data* back to 2020-01-22 (manually added from JHU csse time series)\n  - *Initial models*\n    - 1) *NAIVE model*, akin to \"same as previous day\" but adding +5 for ConfirmedCases and +1 for Fatalities. This model performed \n    very poorly (as expected) but was an easy way to get the code set up and the first prediction submitted. \n    I considered this a \"pipe cleaner.\"\n    - 2) *SIR modified model*\n      - a simple calculation based on per-day-deltas (e.g. dS\/dt is the change from one day to the next -- avoided complex math)\n      - believing that US testing is still not representative of the full extent of the infected cases (I), and further that \n        R = recoverd + fatalities while only fatalities are reported, the model was modified to SCALE both of these to estimate I and R, as in\n        - I = i_factor * ConfirmedCases\n        - R = r_factor * Fatalities\n - *Extend to WORLD*. \n   - Kept the modified SIR model with a country-keyed table of parameters. \n   - Ran out of time to customize the parameter table.\n - *Ran out of time* to upload the correctly functioning version, so the results were poor. Fortunately, I learned a lot.\n\n### Week 2:\n\n - Fit to fatalities. As in the earlier data, I believe that the data for Fatalities is more accurate than the data for ConfirmedCases, \nso the models will be built to be more accurate for the Fatalities at the potential risk of incorrectly \nestimating the ConfirmedCases.\n - SIR parameter prediction. Week 1 was focused (for me) on extending my CA notebook to WORLD data. Week 2 is \n    focused on exploring the parameter predictions and determining which input factors (population total, population density, etc.) \n    provide the best parameter fit. In theory, the parameters that provide the best fit should be CLUES to the \n    true inputs to the system. Note that I refer to these as only clues; population density may really be an indicator of cultural social distancing, housing structure, climate, or some other factor that is the true input.","ec82dc4d":"<a id='section_model_evaluations'><\/a>\n## Model Evaluations","cb7d0cbf":"<a id='section_sir'><\/a>\n## Classic SIR model \nThe classic SIR model describes a disease progression for each individual as\n\n**S**usceptible -> **I**nfected -> **R**esolved\n\nand has 3 governing equations\n* **dS\/dt** = (-1 x BETA x S x I)\/N\n* **dI\/dt** = (BETA x S x I)\/N - (GAMMA x I)\n* **dR\/dt** = GAMMA x I\n\nwhere **N** is the total population and the two factors \n\n* **BETA** (transmission factor that moves individuals from S to I)\n* **GAMMA** (recovery factor that moves individuals from I to R)\n\nare specific to the disease being modelled.\n\n\n## Modified SIR model\n\nThe model used here is a modified SIR where the modifications are factors to estimate the true counts for I and R, namely \n* I = i_factor * ConfirmedCases\n* R = r_factor * Fatalities\n\n### S = the number of susceptible people \n  - Since there has not been a vaccine, this study assumes that the starting value for S is the total population of the country or region.\n  - The total population of a country is probably not the best estimator. The number of susceptible people in each densely populated and infected region would likely be a better predictor, and perhaps the density of the population would further improve that. Unfortunately, this level of granularity (cities, towns) is not currently available in the data.\n   * In the SIR model, S + I + R = N = total_population at all points in time. We can calculate S after calculating changes to I and R.\n  - __Conclusion__: The total population of each region will be used keeping in mind the limitations (assumptions) that this introduces to the model.\n  \n### I = the number of infected people\n * If we had perfect data, we would have I = ActiveCases.\n * Instead, we have ConfirmedCases which is believed to be under-estimated for a variety of reasons (silent carriers, inability to test all symptomatic individuals, other reporting issues).\n * __Conclusion__: The number of ConfirmedCases will be used to estimate the number of actual Infected cases, using an estimated correction factor, INFECTED = i_factor * ConfirmedCases. \n \n### R = the number of people who are resolved\n * R includes the individuals who are no longer susceptible and no longer actively infected.\n * Resolved = Recovered + Fatalities\n * Cases that were never counted as ConfirmedCases will also not be counted as recoveries.\n * Fatalities are most likely to be accurate as testing is likely to occur when individuals require a diagnostic that provides clarity into the necessary interventions, and many countries require a \"Cause of Death\" to be recorded on death certificates.\n * __Conclusion__: The number of Fatalities will be used to estimate the number of actual Resolved cases, using an estimated correction factor, RESOLVED = r_factor * Fatalities.\n     \n","adf26f22":"Start with default values for all locations","0d71be9b":"## Hypothesis and Model\n\nEssentially, our hypothesis and assumptions are:\n * SIR model is appropriate:\n  * No vaccinated or immune individuals.\n  * Exposure data is not available so the existence of an \"E\" state as in SEIR is a hidden node. S->E->I can be approximated by S->I.\n  * Quarantines are 'leaky' in that movement and interaction are reduced but not completely halted, and that unconfirmed \/ silent carriers are the most likely to be moving about, so SEIQR would also contain a hidden node without visibility or data.\n * Number of Fatalities is the most accurate statistic.\n * The amount of under-estimation in historic data is a good predictor of future under-estimation.\n * Factors calculated to correct for under-estimation can be used to predict future reported data.\n * The derivatives (dS\/dt, dI\/dt, dR\/dt where ideally dt->0) can be replaced by delta-S\/delta-t etc. with minimal loss of accuracy compared to other error sources:\n  * The best available dt at this point is a time delta of 1 day (resolution of the data). \n  * dS, dI, and dR are confined to integers (resolution of counting people).\n \nThis notebook implements a modified SIR as follows:\n  * BETA and GAMMA are estimated based on comparison to the seasonal flu, measles, and estimations made in the public disclosures (news and other reports).\n  * i_factor and r_factor are based on EDA of historical data.\n  * Prior to calculating (forecasting) the next values, the correction factor is applied to I and R.\n  * After calculating the next 'true' step, only the uncorrected values are saved as ConfirmedCases and Fatalities; these form the predictions.\n        \nAdditional data sources:\n 1.  Population by country from https:\/\/www.kaggle.com\/tanuprabhu\/population-by-country-2020). \n 1.  Population of 3711 for the Diamond Princess is from https:\/\/en.wikipedia.org\/wiki\/Diamond_Princess_(ship).\n\n*Note: This notebook has room for improvement in several areas; please improve on this if you copy code! For example, some regions did not correctly get population data from the external source and are using the default value instead. i_factor and r_factor EDA could be replaced with an ML search for the best factors.*","312ad2af":"### Prepare a table of parameters for each of the unique locations for use in the model"}}