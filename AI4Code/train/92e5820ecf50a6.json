{"cell_type":{"616a5bc5":"code","0c6646b5":"code","403ad2ca":"code","55cf3d70":"code","d198c163":"code","d8e9c24b":"code","ceede8f4":"code","51112967":"code","3948ad0a":"code","60f449f7":"code","1d8f6614":"code","637931d8":"code","77cd2669":"code","88f8baf1":"code","78b7bfea":"code","84d5ecb4":"code","47fc5169":"code","3b4b9f96":"code","09ee003c":"code","e571d838":"code","296297b8":"code","0a8fa7ce":"code","e7e8c481":"code","5bed1373":"code","32a34c5d":"code","23a209d0":"code","d0a3f041":"code","c764015d":"code","a96557cb":"code","79a157df":"code","776c9812":"code","65f5937c":"code","8811e04b":"code","05e56845":"code","59d06ad1":"code","0d4d7401":"code","9745aa3b":"code","4871ff72":"code","3caba1fa":"code","86d6a176":"code","67dd08f0":"code","80a03596":"code","95b0acf5":"code","36615a4d":"code","561d11a7":"code","9b4c5abf":"code","cd887409":"code","d0adaf79":"code","1d3357bc":"code","e6558d6f":"code","be5ff2bd":"code","e9505fd5":"code","de9358d5":"code","e0f5d48d":"code","9defbf2e":"code","198d0391":"code","6d2f2a70":"code","f0e002b5":"code","eb394ad0":"code","00cff8c7":"code","8284a5f3":"code","67e03937":"code","45dcf00f":"code","ca2b17ec":"code","11d14b4e":"code","36a8d425":"code","94cff625":"code","66f03d1f":"code","749683dc":"code","6de8323e":"code","42160ba4":"code","9c07bee2":"code","35430115":"code","38128b85":"code","750d71db":"code","318d143f":"code","9d9b93ee":"code","07e14af6":"code","1845600f":"markdown","f7cfa67c":"markdown","be760483":"markdown","626d7b71":"markdown","9826874b":"markdown","92169c38":"markdown","755a2bb8":"markdown","3dc80c30":"markdown","5ce87410":"markdown","1288d223":"markdown","f07f5d73":"markdown","56a81c5e":"markdown","46e3262c":"markdown","81cc2b13":"markdown","cdb3166d":"markdown","938c6856":"markdown"},"source":{"616a5bc5":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.utils import shuffle\nimport string\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten, Conv2D, Conv1D, LeakyReLU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers","0c6646b5":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer","403ad2ca":"from sklearn.linear_model import LogisticRegression","55cf3d70":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler","d198c163":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","d8e9c24b":"from sklearn.model_selection import GridSearchCV","ceede8f4":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler","51112967":"import tensorflow as tf\ntf.__version__ # newest version","3948ad0a":"from wordcloud import WordCloud","60f449f7":"import matplotlib.pyplot as plt\nimport seaborn as sns","1d8f6614":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","637931d8":"data_path = '..\/input\/iba-ml1-final-project\/train.csv'\ndata = pd.read_csv(data_path)","77cd2669":"df = data[['Review_Title', 'Review', 'Rating', 'Recommended']]\n\ndf = df.fillna('Like')\ndf['Reviews'] = df['Review_Title'] + ' ' + df['Review']\ndf = df[['Reviews', 'Rating', 'Recommended']]\n\ndf = shuffle(df, random_state=2021)\ndf[\"Reviews\"] = df['Reviews'].str.replace('[{}]'.format(string.punctuation), '')\n\n# lower-case everything\ndf['Reviews'] = df['Reviews'].str.lower()\n\ndf['Reviews'] = df['Reviews'].str.replace('[^\\w\\s]','')\n\ndf['Reviews'] = df['Reviews'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\ndf['Reviews'] = df['Reviews'].str.replace('\\d+', '')\n\nlemmatizer = WordNetLemmatizer()\ndf['Reviews'] = [lemmatizer.lemmatize(row) for row in df['Reviews']]","88f8baf1":"data.head()","78b7bfea":"data.isnull().sum()","84d5ecb4":"sns.heatmap(data.drop(columns=['Id']).corr())","47fc5169":"def countplot(x, data):\n    plt.figure(figsize=(12,5))\n    plt.xticks(rotation=45)\n\n    sns.countplot(x=x, data=data, palette='RdBu')","3b4b9f96":"def barplot(x, y, data):\n    plt.figure(figsize=(12,5))\n    plt.xticks(rotation=45)\n\n    sns.barplot(x=x, y=y, data=data, palette='viridis')","09ee003c":"countplot('Product_Category', data)","e571d838":"countplot('Department', data)","296297b8":"countplot('Division', data)","0a8fa7ce":"countplot('Rating', data)","e7e8c481":"def combined_cat_box_plot(x, y, data):\n    \n    fig, [ax, ax1] = plt.subplots(2, 1, figsize=(12, 12))\n    category = data.groupby(x).agg({y: np.average})\n\n    sns.barplot(x=category.index, y=y, data=category, ax=ax)\n    sns.boxplot(x=x, y=y, data=data, ax=ax1)\n    \n    plt.setp(ax.get_xticklabels(), rotation=45)\n    plt.setp(ax1.get_xticklabels(), rotation=45)\n    plt.show","5bed1373":"combined_cat_box_plot('Product_Category', 'Rating', data)","32a34c5d":"combined_cat_box_plot('Product_Category', 'Age', data)","23a209d0":"combined_cat_box_plot('Rating', 'Age', data)","d0a3f041":"sns.set()\nsns.boxplot(x='Rating', y='Age', data=data)","c764015d":"reviews = '\\n'.join(df['Reviews'])\n\nfig, ax = plt.subplots(figsize=(12, 8), facecolor='black')\n\nwc = WordCloud(background_color='black',\n               width=1280, height=720).generate(reviews)\nax.imshow(wc, interpolation='bilinear')\nplt.show()","a96557cb":"y_1 = df[['Rating']]\ny_1","79a157df":"ft_df = data.drop(['Id', 'Review_Title', 'Review', 'Rating', 'Recommended'], axis=1)\nft_df","776c9812":"num_trfm = Pipeline(steps=[\n    ('imputer', SimpleImputer()),    \n#     ('scaler', MinMaxScaler()),\n])\n\ncat_trfm = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),  \n    ('encoder', OneHotEncoder())\n])\n\ncol_trfm = ColumnTransformer(transformers=[\n    ('numeric', num_trfm, ['Age', 'Pos_Feedback_Cnt']),\n    ('categorical', cat_trfm, ['Division', 'Department', 'Product_Category'])    \n])","65f5937c":"X_non_txt = col_trfm.fit_transform(ft_df).toarray()\nX_non_txt.shape","8811e04b":"X_txt = df['Reviews']","05e56845":"y_ml = np.zeros((y_1.shape[0], 5))\ny_ml[np.arange(y_1.shape[0]), y_1['Rating']-1] = 1\ny_ml = pd.DataFrame(y_ml,columns=['1','2','3','4','5'],dtype='int64')","59d06ad1":"y_ml.head()","0d4d7401":"y_1.value_counts()","9745aa3b":"X_txt_train, X_txt_test, X_non_txt_train, X_non_txt_test, y_train, y_test = train_test_split(X_txt, X_non_txt, y_1, test_size = 0.2,\n                                                    random_state = 0)\n\ncvec = CountVectorizer(stop_words='english').fit(X_txt_train)\nX_txt_train = pd.DataFrame(cvec.transform(X_txt_train).toarray(), columns=cvec.get_feature_names())\nX_txt_train","4871ff72":"X_txt_test = pd.DataFrame(cvec.transform(X_txt_test).toarray(), columns=cvec.get_feature_names())","3caba1fa":"print(X_txt_train.shape)\nprint(X_txt_test.shape)","86d6a176":"X_train = pd.concat([X_txt_train, pd.DataFrame(X_non_txt_train)], axis=1)\nX_train.shape","67dd08f0":"X_test = pd.concat([X_txt_test, pd.DataFrame(X_non_txt_test)], axis=1)\nX_test.shape","80a03596":"lr = LogisticRegression(multi_class='ovr')","95b0acf5":"from sklearn.svm import SVC","36615a4d":"svm = SVC()","561d11a7":"lr.fit(X_train, y_train)\nlr.score(X_test, y_test)  ","9b4c5abf":"svm.fit(train_df, y_train)\nsvm.score(test_df, y_test)","cd887409":"logistic_play_pipeline = Pipeline(steps=[\n    ('classifier', LogisticRegression())\n])","d0adaf79":"logistic_play_param_space = {\n    'classifier__penalty': ['l1', 'l2'],\n    'classifier__max_iter': [100, 300, 500],\n    'classifier__solver': ['liblinear'],\n    'classifier__class_weight': [None, 'balanced']\n}","1d3357bc":"from sklearn.metrics import classification_report","e6558d6f":"y_train = y_train.values.reshape(len(y_train))","be5ff2bd":"y_train.shape","e9505fd5":"over_sampler = RandomOverSampler(random_state=0)","de9358d5":"X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n\ngridsearch = GridSearchCV(logistic_play_pipeline, logistic_play_param_space)\ngridsearch.fit(X_train, y_train)\n\ny_pred = gridsearch.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","e0f5d48d":"from imblearn.over_sampling import RandomOverSampler","9defbf2e":"sampler = RandomOverSampler()","198d0391":"import tensorflow_addons as tfa","6d2f2a70":"tqdm_callback = tfa.callbacks.TQDMProgressBar()","f0e002b5":"def target_binarizer(y):\n    unique = np.unique(y)\n    y_final = np.zeros((y.shape[0], len(unique)))\n    y_final[np.arange(y.shape[0]), y-1] = 1\n    y_final = pd.DataFrame(y_final,columns=[str(i) for i in unique],dtype='int64')\n    \n    return y_final","eb394ad0":"def model_loss_accuracy_plot(model_history):\n\n    # Plot training & validation accuracy values\n    fig, [ax, ax1] = plt.subplots(1, 2, figsize=(15,6))\n\n    ax.plot(model_history.history['accuracy'], 'o-')\n    ax.plot(model_history.history['val_accuracy'], 'o-')\n    ax.set_title('Model accuracy')\n    ax.set_ylabel('Accuracy')\n    ax.set_xlabel('Epoch')\n    ax.legend(['Train', 'Test'], loc='upper left')\n    # Plot training & validation loss values\n    ax1.plot(model_history.history['loss'], 'o-')\n    ax1.plot(model_history.history['val_loss'], 'o-')\n    ax1.set_title('Model loss')\n    ax1.set_ylabel('Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.legend(['Train', 'Test'], loc='upper left')\n    \n    plt.show()","00cff8c7":"with tpu_strategy.scope():\n    \n    max_features = 5000\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(pd.concat([df['Reviews'], df_test['Reviews']]))\n    list_tokenized_train = tokenizer.texts_to_sequences(df['Reviews'])\n\n    maxlen = 120\n#     X_t = np.hstack([pad_sequences(list_tokenized_train, maxlen=maxlen), X_non_txt])\n    X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X_t, y_1['Rating'], test_size=0.1,\n                                                        random_state=0, stratify=y_1['Rating'])\n\n#     X_train, y_train = sampler.fit_resample(X_train, y_train.values)\n    \n    y_train = target_binarizer(y_train)\n    y_test = target_binarizer(y_test)\n    \n#     y2 = np.zeros((y_resampled.shape[0], 5))\n#     y2[np.arange(y_resampled.shape[0]), y_resampled-1] = 1\n#     y2 = pd.DataFrame(y2,columns=['1','2','3','4','5'],dtype='int64')\n    \n#     y2 = np.zeros((y_train.shape[0], 5))\n#     y2[np.arange(y_train.shape[0]), y_train-1] = 1\n#     y2 = pd.DataFrame(y2,columns=['1','2','3','4','5'],dtype='int64')\n    \n\n    embed_size = 150\n    model = Sequential()\n    model.add(Embedding(max_features, embed_size))\n    model.add(Bidirectional(LSTM(embed_size*2, return_sequences = True)))\n#     model.add(Conv1D(1000, 1))\n#     model.add(LeakyReLU())\n    model.add(Conv1D(embed_size*2, 1))\n    model.add(LeakyReLU())\n    model.add(Dropout(0.1))\n    \n    model.add(Conv1D(embed_size, 1))\n    model.add(LeakyReLU())\n    \n    model.add(GlobalMaxPool1D())\n#     model.add(Dropout(0.1))\n\n    model.add(Dense(50))\n#     model.add(Dropout(0.05))\n    \n    model.add(LeakyReLU())\n    model.add(Dense(5, activation=\"softmax\"))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'MeanSquaredError'])\n\nbatch_size = 32 * tpu_strategy.num_replicas_in_sync\nepochs = 25 \n\ncallback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy', min_delta=0.0001, patience=10, verbose=0,\n    mode='auto', baseline=None, restore_best_weights=True\n)\n\ntrial_history_32 = model.fit(X_train,y_train, batch_size=batch_size, \n          epochs=epochs, validation_split=0.1,\n          use_multiprocessing=True,\n          callbacks = [callback, tqdm_callback],\n          verbose=0)\n\nmodel_loss_accuracy_plot(trial_history_32)","8284a5f3":"trial_history_32.model.evaluate(X_t, target_binarizer(y_1['Rating']))","67e03937":"ratings_final = tf.keras.models.load_model('..\/input\/lstm-model\/LSTM-exp_1_6_ratings.h5')","45dcf00f":"ratings_final.summary()","ca2b17ec":"with tpu_strategy.scope():\n\n    max_features = 5000\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(pd.concat([df['Reviews'], df_test['Reviews']]))\n    list_tokenized_train = tokenizer.texts_to_sequences(df['Reviews'])\n\n    maxlen = 150\n    X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n    y_2 = df['Recommended']\n\n    embed_size = 150\n    trial_model_2 = Sequential()\n    trial_model_2.add(Embedding(max_features, embed_size))\n    trial_model_2.add(Bidirectional(LSTM(embed_size*2, return_sequences = True)))\n#     model.add(Conv1D(1000, 1))\n#     model.add(LeakyReLU())\n    trial_model_2.add(Conv1D(embed_size, 1))\n    trial_model_2.add(LeakyReLU())\n\n    trial_model_2.add(GlobalMaxPool1D())\n    trial_model_2.add(Dropout(0.1))\n\n    trial_model_2.add(Dense(30))\n    trial_model_2.add(Dropout(0.1))\n    \n    trial_model_2.add(LeakyReLU())\n    trial_model_2.add(Dense(1, activation=\"sigmoid\"))\n    trial_model_2.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'MeanSquaredError'])\n\nbatch_size = 32 * tpu_strategy.num_replicas_in_sync\nepochs = 15\n\ncallback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy', min_delta=0.0001, patience=10, verbose=0,\n    mode='auto', baseline=None, restore_best_weights=True\n)\n\ntrial_model_2_history_02 = trial_model_2.fit(X_t,y_2, batch_size=batch_size, \n          epochs=epochs, validation_split=0.1,\n          use_multiprocessing=True,\n          callbacks = [callback])\n\nmodel_loss_accuracy_plot(trial_model_2_history_02)","11d14b4e":"recommendations_final = tf.keras.models.load_model('..\/input\/lstm-model\/LSTM-exp_1_6_recommendation.h5')","36a8d425":"df_test=pd.read_csv('..\/input\/iba-ml1-final-project\/test.csv')","94cff625":"X_non_txt_test = df_test.drop(['Id', 'Review_Title', 'Review'], axis=1)\nX_non_txt_test = col_trfm.transform(X_non_txt_test).toarray()\nX_non_txt_test.shape","66f03d1f":"df_test = df_test[['Id', 'Review_Title', 'Review']]\n\n# fill the nan in features with ''(empty string)\ndf_test = df_test.fillna('like')\n\n# concatenate\ndf_test['Reviews'] = df_test['Review_Title'] + ' ' + df_test['Review']\n\ndf_test = df_test[['Id','Reviews']]\n\n\n# remove punctuation\ndf_test[\"Reviews\"] = df_test['Reviews'].str.replace('[{}]'.format(string.punctuation), '')\n\n# lower-case everything\ndf_test['Reviews'] = df_test['Reviews'].str.lower()\n\ndf_test['Reviews'] = df_test['Reviews'].str.replace('[^\\w\\s]','')\n\ndf_test['Reviews'] = df_test['Reviews'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\nlist_tokenized_test = tokenizer.texts_to_sequences(df_test['Reviews'])\n\nmaxlen = 200\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)","749683dc":"result_= trial_history_29.model.predict(X_test)","6de8323e":"pd.DataFrame(result_).to_csv('ratings.csv', index=False)","42160ba4":"STAR_RATING = result_.argmax(axis=1)+1\nSTAR_RATING","9c07bee2":"result = trial_model_2_history_02.model.predict(X_test)\nresult","35430115":"result = result.round()\nresult = result.astype('int64')\nresult","38128b85":"starred = df_test","750d71db":"starred.columns","318d143f":"starred['Rating'] =STAR_RATING\nstarred['Recommended'] = result\nstarred = starred.drop(columns=['Reviews'])","9d9b93ee":"starred","07e14af6":"starred.to_csv('exp_1_7.csv', index=False)","1845600f":"## Initiating a TPU environment","f7cfa67c":"# Retrieving and Transforming data ","be760483":"We can easily see some outlierness in the above plots. Removing the outliers was also tried out but no matter how low the proportion ","626d7b71":"## Recommendation","9826874b":"### Trial Models","92169c38":"# EDA","755a2bb8":"This does not provide much info over the intuitively evident info of the ratings being related to recommendation. One could easily make the case that higher rating score would mean that the reviewer also recommends the product in most cases. Let's now explore the non-text data","3dc80c30":"***Rating Prediction Bow***","5ce87410":"In all of my trials, the non-text features only served to increase the training times with virtually no added benefit to the accuracy and performance of the different types of models I used. Thus, they are left out in the model input, while still being explored at the EDA stage","1288d223":"We will need to deal with the missing data","f07f5d73":"At this stage the Review Title and Review features are combined to acquire a final text input. Punctuations, upper, lower case letters, white spaces are dealt with and the test is lemmatized. Lemmatization was preferred over Stemming as it had increased precision over Stemming ","56a81c5e":"Let's see if there are any direct correlations between features that can easily be spotted","46e3262c":"# Test","81cc2b13":"# Results","cdb3166d":"We can see an imbalance regarding the ratings in the favor of 5 star ratings. Imbalanced learning was one of the methods that were tried out but it did not add any gains in the score upon submission","938c6856":"# Iimporting Libraries"}}