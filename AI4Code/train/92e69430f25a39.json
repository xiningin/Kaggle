{"cell_type":{"078be4bb":"code","c10157a0":"code","a3735cc3":"code","f57af3c4":"code","2a46f368":"code","1a2f206a":"markdown","29625049":"markdown"},"source":{"078be4bb":"import numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score","c10157a0":"from sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if  X[cname].dtype != \"object\"]\n\n# Columns that nan represent that the item does not exist\nimportent_nan = {'Condition2', 'Exterior2nd', 'BsmtFinType1', 'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature'}\nimportent_nan = set(categorical_cols).intersection(importent_nan)\nunimp_nan = set(categorical_cols) - importent_nan\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols","a3735cc3":"def get_model(numeric_imput, learning_rate, n_estimators):\n    # Preprocessing for numerical data\n    numerical_transformer = ColumnTransformer(transformers=[\n        ('GarageYrBlt', SimpleImputer(strategy='constant', fill_value=1899), ['GarageYrBlt']),\n        ('others', SimpleImputer(strategy=numeric_imput), list(set(numerical_cols) - set(['GarageYrBlt'])))\n    ])\n    \n    categorical_imputer = ColumnTransformer(transformers=[\n        ('importent_nan', SimpleImputer(strategy='constant', fill_value='not_exist'), list(importent_nan)),\n        ('others', SimpleImputer(strategy='most_frequent'), list(unimp_nan))\n    ])\n    \n                                              \n    # Preprocessing for categorical data\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', categorical_imputer),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n    my_cols = categorical_cols + numerical_cols\n    # Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numerical_transformer, numerical_cols),\n            ('cat', categorical_transformer, categorical_cols)\n        ])\n    \n    # Define model\n    model = XGBRegressor(learning_rate=learning_rate, n_estimators=n_estimators, random_state=0,)\n    \n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                  ('model', model)\n                                 ])\n    return my_pipeline","f57af3c4":"model = get_model('mean', 0.08, 110)\nmodel.fit(X, y)\npred = model.predict(X_test)","2a46f368":"# Save test predictions to file\noutput = pd.read_csv('..\/input\/sample_submission.csv')\noutput['SalePrice'] = pred\noutput.to_csv('submission.csv', index=False)","1a2f206a":"In the previus versions (versions 5-10) we did fine tuning on the hyper-parameters. We found that the best results are from mean imputer, learning_rate=0.08 and n_estimators=1100.\n\nNow we train and predict by those parameters.\n","29625049":"This is an exercise I managed for myself to summerise the Intermediate Machine Learning minicourse.\n\nMy Goal is to recieve the best possible grade using the technologies learnt in the minicourse."}}