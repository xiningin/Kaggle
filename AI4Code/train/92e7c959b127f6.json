{"cell_type":{"b619dc19":"code","b7ad5a70":"code","599ed40f":"code","5fdceb4d":"code","e74ef82f":"code","d3256ff4":"code","91a6b2cb":"code","2694cf70":"code","d4c81258":"code","e1c32751":"code","7a95a1ec":"code","9b503794":"code","907c6ab2":"code","7a64db7d":"code","876d83ff":"code","ec2460ec":"code","2f0826c8":"code","a53623b5":"code","9086c897":"code","58865070":"code","d7996379":"code","16be21b6":"code","4b389f38":"code","d49e38f7":"code","d1f5c028":"code","1ece0828":"code","d7a5446f":"code","b2eb6b80":"code","535c0569":"code","c8fa419a":"code","f8817619":"code","4bcbfb4c":"code","64d13f64":"code","50ade5d3":"code","960a299b":"code","1732cf4e":"code","95475d5b":"code","51f51af7":"code","24186f26":"code","d4de584d":"code","591cc39e":"code","5591efab":"code","a139c02f":"code","315f7acd":"code","0e80d79b":"code","6b2f115c":"code","58221c64":"code","b9e19335":"code","62141b16":"code","a8d02076":"code","aec9e284":"code","dfe1888e":"code","05b95dc1":"code","a3e60876":"code","ffcd9197":"code","bd12dd95":"code","c6463672":"code","fc8a719a":"code","ee6c3da6":"code","1ca00710":"code","3021e10e":"code","33dceb36":"code","cda3749f":"code","21ad8388":"code","1bf55ff0":"code","e9647986":"code","332802b3":"code","bc0239f9":"code","fdbbf229":"code","d903d3c5":"code","f56784a6":"code","97f009bf":"code","3d4a779e":"markdown"},"source":{"b619dc19":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b7ad5a70":"df = pd.read_csv('\/kaggle\/input\/voicegender\/voice.csv')","599ed40f":"df.head()","5fdceb4d":"df.tail()","e74ef82f":"df.shape","d3256ff4":"df.describe()","91a6b2cb":"df.columns","2694cf70":"df.isnull().sum()","d4c81258":"# no missing values","e1c32751":"def check_out(col):\n    q1, q3 = df[col].quantilee([0.25,0.75])\n    iqr=q3-q1\n    rang = 1.5*iqr\n    return(q1-rang, q3+rang)","7a95a1ec":"def plot(col):\n    fig,axes=plt.subplots(1,2)\n    sns.boxplot(data=df,x=col,ax=axes[0])\n    sns.distplot(a=df[col],ax=axes[1],color='#ff4125')\n    fig.set_size_inches(15,5)\n    #lower,upper = check_outliers(col)\n    #l=[df[col] for i in df[col] if i>lower and i<upper] \n    #print(\"Number of data points remaining if outliers removed : \",len(l))","9b503794":"plot('meanfreq')","907c6ab2":"plot('sd')","7a64db7d":"plot('median')","876d83ff":"plot('Q25')","ec2460ec":"plot('Q75')","2f0826c8":"plot('skew')","a53623b5":"plot('kurt')","9086c897":"plot('sp.ent')","58865070":"plot('sfm')","d7996379":"plot('meanfun')","16be21b6":"sns.countplot(data=df, x = 'label')","4b389f38":"df['label'].count()","d49e38f7":"df['label'] = df['label'].replace({'male': 1, 'female' : 0})","d1f5c028":"df.head()","1ece0828":"df.tail()","d7a5446f":"df.dtypes","b2eb6b80":"df['label'].value_counts()","535c0569":"correlation = df.corr()\n#print(correlation)\n\nfig=plt.gcf()\nfig.set_size_inches(30,15)\nsns.heatmap(data=correlation, annot=True)","c8fa419a":"def plot_against_tar(featurevar):\n    sns.factorplot(data = df, y = featurevar, x='label', kind='box')\n    fig=plt.gcf()\n    fig.set_size_inches(6,7)","f8817619":"plot_against_tar('meanfreq')","4bcbfb4c":"plot_against_tar('sd')","64d13f64":"plot_against_tar('median')","50ade5d3":"plot_against_tar('Q25')","960a299b":"plot_against_tar('Q75')","1732cf4e":"plot_against_tar('IQR')","95475d5b":"plot_against_tar('meanfun')","51f51af7":"plot_against_tar('sp.ent')","24186f26":"sns.pairplot(data = df, hue = 'label')","d4de584d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaleraer_df = scaler.fit_transform(df.drop('label', axis=1))","591cc39e":"X = scaleraer_df\ny = df['label'].values","5591efab":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 42)","a139c02f":"# linear regression\nfrom sklearn.linear_model import LogisticRegression\nlin_rig = LogisticRegression()\nlin_rig.fit(X_train, y_train)","315f7acd":"lin_rig.score(X_train, y_train)","0e80d79b":"lin_rig.score(X_test, y_test)","6b2f115c":"y_hat = lin_rig.predict(X_test)","58221c64":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_hat))","b9e19335":"from sklearn.svm import SVC\nsvm_classifier = SVC()\nsvm_classifier.fit(X_train, y_train)","62141b16":"svm_classifier.score(X_train, y_train)","a8d02076":"svm_classifier.score(X_test, y_test)","aec9e284":"y_hat = svm_classifier.predict(X_test)","dfe1888e":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_hat))","05b95dc1":"from sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_train, y_train)","a3e60876":"knn_classifier.score(X_train, y_train)","ffcd9197":"knn_classifier.score(X_test, y_test)","bd12dd95":"y_hat = knn_classifier.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_hat))","c6463672":"from sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier()\nDTC.fit(X_train, y_train)","fc8a719a":"DTC.score(X_train, y_train)","ee6c3da6":"DTC.score(X_test, y_test)","1ca00710":"y_hat = DTC.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_hat))","3021e10e":"from sklearn.ensemble import RandomForestClassifier\nRFC = RandomForestClassifier()\nRFC.fit(X_train, y_train)","33dceb36":"RFC.score(X_train, y_train)","cda3749f":"RFC.score(X_test, y_test)","21ad8388":"y_hat = RFC.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_hat))","1bf55ff0":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)","e9647986":"gbc.score(X_train, y_train)","332802b3":"gbc.score(X_test, y_test)","bc0239f9":"y_hat = gbc.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_hat))","fdbbf229":"from sklearn.model_selection import GridSearchCV\n \n# defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf','linear']}\n \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n \n# fitting the model for grid search\ngrid.fit(X_train, y_train)","d903d3c5":"grid.best_params_","f56784a6":"grid.best_estimator_","97f009bf":"grid.best_score_","3d4a779e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session"}}