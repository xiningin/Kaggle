{"cell_type":{"320bab8e":"code","4c2da881":"code","66b48e51":"code","b06c76a8":"code","c4c7e0b6":"code","c25468c5":"code","ea60684b":"code","ccf98d0d":"code","c2316e1f":"code","69219782":"code","347f21f9":"code","354c48ed":"code","75531421":"code","04745062":"code","9f4fd24c":"code","f7832db5":"code","cf0c1247":"code","7bf94deb":"code","396d8f88":"code","4ea98494":"code","1303450f":"code","72cddc4e":"code","fac49b1b":"code","ac909e86":"code","b276aca2":"code","17e1aca1":"code","35b3fa85":"code","3abfbff0":"code","073150ca":"code","fce5e5a7":"code","4b6ffa94":"code","30baa86a":"code","1a4251c6":"code","4e640044":"code","a086e575":"code","28ac82bc":"code","71834837":"markdown","1c9a1ecb":"markdown","5be5f69d":"markdown","20fb42ab":"markdown","b369f4c0":"markdown","f99c876c":"markdown","ffac8036":"markdown","bec49012":"markdown","3e249139":"markdown","99506aab":"markdown","d59a4f3b":"markdown","ed505702":"markdown","3e715a72":"markdown","5e902528":"markdown","69e31142":"markdown","80d72718":"markdown","51ae93ae":"markdown"},"source":{"320bab8e":"import os\nimport shutil\nimport random\nimport json\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\n\n%matplotlib inline\n\npd.set_option(\"display.precision\", 3)\npd.set_option(\"display.expand_frame_repr\", False)\npd.set_option(\"display.max_rows\", 50)\npd.set_option('max_colwidth',100)\npd.get_option('min_rows')","4c2da881":"DATA_ROOT = '\/kaggle\/input\/retail-product-checkout-dataset'\nWORK_ROOT = '\/kaggle\/working\/retail-product-checkout-dataset'","66b48e51":"!ls -l $DATA_ROOT\n!mkdir -p $WORK_ROOT","b06c76a8":"def load_json(jfile):\n    with open(jfile, 'rb') as f:\n        return json.load(f)\nval_data = load_json(os.path.join(DATA_ROOT, 'instances_val2019.json'))\nval_data.keys()","c4c7e0b6":"val_data['info']","c25468c5":"val_data['licenses']","ea60684b":"val_categories_df = pd.DataFrame(val_data['categories'])\nval_categories_df.tail()","ccf98d0d":"val_categories_grp = val_categories_df.groupby('supercategory', as_index=False)\nval_categories_grp.groups","c2316e1f":"val_rawcat_df = pd.DataFrame(val_data['__raw_Chinese_name_df'])\nval_rawcat_df.groupby(['sku_class', 'clas'], as_index=False).size()","69219782":"pd.set_option(\"display.max_rows\", None)\nval_rawcat_df.groupby(['sku_class', 'clas', 'sku_name']).first()","347f21f9":"val_rawcat_df[val_rawcat_df['sku_class'] == 'drink']","354c48ed":"pd.reset_option(\"display.min_rows\")\npd.reset_option(\"display.max_rows\")","75531421":"val_images_df = pd.DataFrame(val_data['images'])\nprint(len(val_images_df))\nval_images_df[17:23]","04745062":"val_images_df[['width', 'height']].describe()","9f4fd24c":"val_images_df.groupby(['level'], as_index=False).size().plot.pie(figsize=(8,8), y='size', autopct='%.2f', title='LEVEL');","f7832db5":"val_anns_df = pd.DataFrame(val_data['annotations'])\nprint(len(val_anns_df))\nval_anns_df[17:23]","cf0c1247":"val_images_0824_df = val_images_df[val_images_df['file_name'].str.contains('20180824')]\nval_images_0824_df.head()","7bf94deb":"val_sample_img0_info = val_images_0824_df.iloc[0]\nval_sample_img0_info","396d8f88":"val_sample_img0_fpath = f'{DATA_ROOT}\/val2019\/{val_sample_img0_info.file_name}'\nif not os.path.exists(val_sample_img0_fpath):\n    raise RuntimeError(f'Not found {val_sample_img0_fpath}')\nval_sample_img0_fpath","4ea98494":"val_sample_img0_anns = val_anns_df[val_anns_df['image_id'] == val_sample_img0_info.id]\nval_sample_img0_anns","1303450f":"def annotate_image(image_path, anns, resize=True):\n    img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n    for i, a in anns.iterrows():\n        xmin, ymin, width, height = a.bbox\n        xmax, ymax = xmin + width, ymin + height\n        xpoint, ypoint = a.point_xy\n        xpoint, ypoint = int(xpoint), int(ypoint)\n        cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (255, 255, 255), 2)\n        cv2.rectangle(img, (xpoint-20, ypoint-20), (xpoint + 20, ypoint + 20), (0, 255, 0), 5)\n\n    if not resize:\n        return img\n\n    return cv2.resize(img, (200, 200), interpolation = cv2.INTER_AREA)","72cddc4e":"val_sample_img0_cv2 = annotate_image(val_sample_img0_fpath, val_sample_img0_anns, True)\nplt.figure(figsize=(12, 8))\nplt.xticks([])\nplt.yticks([])\nplt.imshow(val_sample_img0_cv2);","fac49b1b":"random3x3_rows = random.sample(range(1, len(val_images_0824_df)), 9)\nval_images_0824_df.iloc[random3x3_rows]","ac909e86":"plt.figure(figsize=(16,16))\ni = 1\nfor _, item in val_images_0824_df.iloc[random3x3_rows].iterrows():\n    img_path = f'{DATA_ROOT}\/val2019\/{item.file_name}'\n    if not os.path.exists(img_path):\n        raise RuntimeError(f'Not found {img_path}')\n    anns = val_anns_df[val_anns_df['image_id'] == item.id]\n    plt.subplot(3, 3, i)\n    plt.xticks([])\n    plt.yticks([])\n    img_cv2 = annotate_image(img_path, anns, False)\n    plt.imshow(img_cv2)\n    i += 1","b276aca2":"sample_ids = [34, 71, 74, 77, 78, 83]","17e1aca1":"sample_rawcat = val_rawcat_df[val_rawcat_df['category_id'].isin(sample_ids)].set_index('category_id')\nsample_rawcat","35b3fa85":"sample_classes = sample_rawcat['name'].values.tolist()\nsample_classes","3abfbff0":"list(zip(sample_ids, sample_classes))","073150ca":"sample_anns = val_anns_df[val_anns_df['category_id'].isin(sample_ids)].set_index('id')\nsample_anns.head()","fce5e5a7":"sample_image_ids = sample_anns['image_id'].unique().tolist()\nlen(sample_image_ids), sample_image_ids[-10:]","4b6ffa94":"sample_images = val_images_df[val_images_df['id'].isin(sample_image_ids)]\nsample_images.tail()","30baa86a":"SAMPLE_IN_IMAGES_PATH = f'{DATA_ROOT}\/val2019'\n\nSAMPLE_OUT_IMAGES_PATH = f'{WORK_ROOT}\/images\/val'\nSAMPLE_OUT_LABELS_PATH = f'{WORK_ROOT}\/labels\/val'\n\nos.makedirs(SAMPLE_OUT_IMAGES_PATH, exist_ok=True)\nos.makedirs(SAMPLE_OUT_LABELS_PATH, exist_ok=True)\n\nfor _, item in sample_images[:3].iterrows():\n    imgw, imgh = item.width, item.height\n    dw, dh = 1.0 \/ imgw, 1.0 \/ imgh\n    img_src_path = os.path.join(SAMPLE_IN_IMAGES_PATH, item.file_name)\n    if not os.path.exists(img_src_path):\n        continue\n    img_dst_path = os.path.join(SAMPLE_OUT_IMAGES_PATH, item.file_name)\n    lab_dst_path = os.path.join(SAMPLE_OUT_LABELS_PATH, item.file_name.replace('.jpg', '.txt'))\n    anns = sample_anns[sample_anns['image_id'] == item.id]\n    labs = []\n    for _, ann in anns.iterrows():\n        cls_id = sample_ids.index(ann.category_id)\n        cx, cy = dw * ann.point_xy[0], dh  * ann.point_xy[1]\n        sw, sh = dw * ann.bbox[2], dh * ann.bbox[3]\n        labs.append('%d %.6f %.6f %.6f %.6f' % (cls_id, cx, cy, sw, sh))\n    with open(lab_dst_path, 'w') as fw:\n        fw.write('\\n'.join(labs))\n    shutil.copyfile(img_src_path, img_dst_path)","1a4251c6":"!apt install tree","4e640044":"!tree {WORK_ROOT}","a086e575":"def convert_annotation(phase, in_dir, out_dir, sample_ids):\n    in_images_path  = f'{in_dir}\/{phase}2019'\n    out_images_path = f'{out_dir}\/images\/{phase}'\n    out_labels_path = f'{out_dir}\/labels\/{phase}'\n    \n    os.makedirs(out_images_path, exist_ok=True)\n    os.makedirs(out_labels_path, exist_ok=True)\n    \n    # 1. load json desc\n    with open(os.path.join(f'{in_dir}\/instances_{phase}2019.json'), 'rb') as f:\n        data = json.load(f)\n        \n    imgs_df = pd.DataFrame(data['images'])\n    anns_df = pd.DataFrame(data['annotations'])\n    \n    # 2. filter category id\n    anns_df = anns_df[anns_df['category_id'].isin(sample_ids)]\n    \n    # 3. filter image id\n    img_ids = anns_df['image_id'].unique()\n    imgs_df = imgs_df[imgs_df['id'].isin(img_ids)]\n    \n    # 4. covert yolov5 format\n    for _, item in imgs_df.iterrows():\n        imgw, imgh = item.width, item.height\n        ## 4.1 normalize scale\n        dw, dh = 1.0 \/ imgw, 1.0 \/ imgh\n        img_src_path = os.path.join(in_images_path, item.file_name)\n        if not os.path.exists(img_src_path):\n            continue\n        img_dst_path = os.path.join(out_images_path, item.file_name)\n        lab_dst_path = os.path.join(out_labels_path, item.file_name.replace('.jpg', '.txt'))\n        ## 4.2 annotation in this image\n        anns = anns_df[anns_df['image_id'] == item.id]\n        labs = []\n        for _, ann in anns.iterrows():\n            ## 4.3 convert bbox\n            cls_id = sample_ids.index(ann.category_id)\n            cx, cy = dw * ann.point_xy[0], dh * ann.point_xy[1]\n            sw, sh = dw * ann.bbox[2], dh * ann.bbox[3]\n            labs.append('%d %.6f %.6f %.6f %.6f' % (cls_id, cx, cy, sw, sh))\n        ## 4.4 save to file\n        with open(lab_dst_path, 'w') as fw:\n            fw.write('\\n'.join(labs))\n        ## 4.5 copy image to out dir\n        shutil.copyfile(img_src_path, img_dst_path)","28ac82bc":"# convert_annotation('train', DATA_ROOT, os.path.join(WORK_ROOT, 'out'), sample_ids)\n# convert_annotation('val', DATA_ROOT, os.path.join(WORK_ROOT, 'out'), sample_ids)\n# convert_annotation('test', DATA_ROOT, os.path.join(WORK_ROOT, 'out'), sample_ids)","71834837":"### Data.RawChinese","1c9a1ecb":"### Data.Licenses","5be5f69d":"### Data.Info","20fb42ab":"### Filter Sample Annotations","b369f4c0":"### Data.Images","f99c876c":"## Yolov5(l) Configure Files\n\n### dataset.yaml\n****\n```json\ntrain: \/kaggle\/working\/retail-product-checkout-dataset\/images\/test\nval: \/kaggle\/working\/retail-product-checkout-dataset\/images\/val\ntest: \/kaggle\/working\/retail-product-checkout-dataset\/images\/train\n\nnc: 6\n\nnames: ['0', '1', '2', '3', '4', '5']\n# names: ['\u4f18\u4e50\u7f8e\u7ea2\u8c46\u5976\u8336', '\u519c\u592b\u5c71\u6cc9\u77ff\u6cc9\u6c34', '\u53ef\u53e3\u53ef\u4e50', '\u82ac\u8fbe\u6a59\u5473', '\u96ea\u78a7', '\u738b\u8001\u5409']\n\n```\n\n### yolov5l.yaml\n\n```json\n# parameters\nnc: 6  # number of classes\ndepth_multiple: 1.0  # model depth multiple\nwidth_multiple: 1.0  # layer channel multiple\n\n# anchors\nanchors:\n  - [10,13, 16,30, 33,23]  # P3\/8\n  - [30,61, 62,45, 59,119]  # P4\/16\n  - [116,90, 156,198, 373,326]  # P5\/32\n\n# YOLOv5 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Focus, [64, 3]],  # 0-P1\/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2\/4\n   [-1, 3, BottleneckCSP, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3\/8\n   [-1, 9, BottleneckCSP, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4\/16\n   [-1, 9, BottleneckCSP, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5\/32\n   [-1, 1, SPP, [1024, [5, 9, 13]]],\n   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n  ]\n\n# YOLOv5 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, BottleneckCSP, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3\/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4\/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5\/32-large)\n\n   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n  ]\n```\n\n### hyperparameters.yaml\n\n```json\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum\/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+\/- deg)\ntranslate: 0.1  # image translation (+\/- fraction)\nscale: 0.5  # image scale (+\/- gain)\nshear: 0.0  # image shear (+\/- deg)\nperspective: 0.0  # image perspective (+\/- fraction), range 0-0.001\nflipud: 0.0  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 1.0  # image mosaic (probability)\nmixup: 0.0  # image mixup (probability)\n```","ffac8036":"## Sample Display","bec49012":"### Filter Sample Images","3e249139":"### Covert Yolov5 Annotation","99506aab":"### Data.Annotations","d59a4f3b":"### Filter Sample Categories\n","ed505702":"## Global Constants","3e715a72":"## Yolov5 Format (Sample)","5e902528":"### Data.Categories","69e31142":"## Covert Yolov5 Format Function\n","80d72718":"## Yolov5 Train Script\n\n```sh\n#!\/bin\/bash\n\npython3 \/kaggle\/working\/retail-product-checkout-dataset\/yolov5\/train.py \\\n    --img-size 640 --batch-size 32 --epochs 40  --device 0 \\\n    --workers 4 --project \/kaggle\/working\/retail-product-checkout-dataset\/out --name rpc_l --exist-ok \\\n    --data dataset.yaml --cfg yolov5l.yaml --hyp hyperparameters.yaml \\\n    --weights \/kaggle\/working\/retail-product-checkout-dataset\/yolov5\/weights\/yolov5l.pt\n```","51ae93ae":"## Prepare Data"}}