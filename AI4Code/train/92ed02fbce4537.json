{"cell_type":{"20ac4d34":"code","b5be4a5c":"code","9b392256":"code","7acdb882":"code","0b921ee5":"code","f0749e4c":"code","3b57c63d":"code","59c5b3d0":"code","beeccd73":"code","fb2eaba2":"code","a53b33f3":"code","2a4170f2":"code","9ba8981e":"code","ea573677":"code","3fdbacfb":"code","fa6bb33e":"code","0500db03":"code","c11bae43":"code","1aafe293":"code","80d372c2":"code","2692ab53":"code","5d276a8d":"code","f177fd38":"code","d6f0826d":"code","b57df796":"code","f464897f":"code","9d3c6323":"code","69a74bc1":"code","1ed56543":"code","803cd15d":"markdown","fc17ac4c":"markdown","4f077463":"markdown","f81ae5a9":"markdown","d11f3b99":"markdown","3aaa7b97":"markdown","43c1e77a":"markdown","0576150d":"markdown","61251010":"markdown","fec057b8":"markdown"},"source":{"20ac4d34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5be4a5c":"df = pd.read_csv(\"\/kaggle\/input\/logistic-regression\/Social_Network_Ads.csv\")\ndf.head()","9b392256":"df.Gender.replace(to_replace=[\"Male\" , \"Female\"] , value = [1,0] , inplace=True)\ndf.head()","7acdb882":"df.info()","0b921ee5":"df.Gender.value_counts()","f0749e4c":"df.Purchased.value_counts()","3b57c63d":"sns.countplot(x = df.Gender)\nplt.xlabel(\"Gender\" , fontsize=15)\nplt.ylabel(\"Frequency\" , fontsize=15)\nplt.title(\"Frequency of Gender\" , color = \"red\" , fontsize=15)\nplt.show()","59c5b3d0":"sns.countplot(x=df.Purchased)\nplt.xlabel(\"Purchased or Not\" , fontsize=15)\nplt.ylabel(\"Frequency\" , fontsize=15)\nplt.title(\"Frequency of Purchased\" , color = \"red\" , fontsize=15)\nplt.show()","beeccd73":"countNoPurchased = len(df[df.Purchased==0])\ncountPurchased = len(df[df.Purchased==1])\nprint(\"percentage of people purchased these product : {}\".format(countPurchased\/len(df.Purchased)*100))\nprint(\"Percentage of people not purchased of these product : {}\".format(countNoPurchased\/len(df.Purchased)*100))","fb2eaba2":"countMale = len(df[df.Gender==1])\ncountFemale = len(df[df.Gender==0])\nprint(\"percentage of Male customers : {}\".format(countMale\/len(df.Gender)*100))\nprint(\"percentage of Female customers : {}\".format(countFemale\/len(df.Gender)*100))","a53b33f3":"df.groupby(\"Purchased\").mean()","2a4170f2":"pd.crosstab(df.Gender , df.Purchased).plot(kind=\"bar\",figsize=(10,7))\nplt.show()","9ba8981e":"pd.crosstab(df.Age , df.Purchased).plot(kind=\"bar\" , figsize=(15,7))\nplt.show()","ea573677":"df.drop([\"User ID\"] , axis=1 , inplace=True)","3fdbacfb":"df.head()","fa6bb33e":"y = df.Purchased.values\nx_data = df.drop([\"Purchased\"], axis=1)","0500db03":"x = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nx.head()","c11bae43":"x_train , y_train , x_test , y_test = train_test_split(x,y,test_size=0.2 ,random_state=42)","1aafe293":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","80d372c2":"x_train=x_train.T\n\ny_test=y_test.T\ny_train=y_train.T","2692ab53":"x_test=x_test.T","5d276a8d":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","f177fd38":"def initialize_weight_and_bias(dimension):\n    w=np.full((dimension,1) , 0.01)\n    b=0\n    return w,b","d6f0826d":"def sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head","b57df796":"def forward_backward_propagaion(w,b,x_train,y_train):\n    #Forward\n    z=np.dot(w.T,x_train)+b\n    y_head=sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    #Backward\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost , gradients","f464897f":"def update(w,b,x_train,y_train,learning_rate,number_of_iterations):\n    cost_list=[]\n    index=[]\n    \n    for i in range(number_of_iterations):\n        cost,gradients = forward_backward_propagaion(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        \n        index.append(i)\n        \n        parameters= {\"weight\" : w , \"bias\" : b}\n        plt.plot(index,cost_list)\n        plt.xlabel(\"Number of Iterations\")\n        plt.ylabel(\"Costs\")\n        plt.show()\n        return parameters , gradients , cost_list","9d3c6323":"def prediction(w,b,x_test):\n    z= np.dot(w.T, x_test) + b\n    y_head = sigmoid(z)\n    \n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_head.shape[1]):\n        if y_head[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction","69a74bc1":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    dimension = x_train.shape[0]\n    w,b = initialize_weight_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    y_prediction_test = prediction(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","1ed56543":"logistic_regression(x_train, y_train, x_test, y_test,1, 300)","803cd15d":"# Sigmoid Function","fc17ac4c":"# Dataset Train-Test Split","4f077463":"Gender--> 1=Male and 0=Female\n\nAge --> Age of customer\n\nEstimatedSalary --> Salary of customer\n\nPurchased --> 0=not Purchased and 1=Purchased\n","f81ae5a9":"# Normalization","d11f3b99":"# Forward and Backward Propagation","3aaa7b97":"# Initializing Parameters","43c1e77a":"# Implementing Update Parameters","0576150d":"# Implementing Prediction","61251010":"# Look Data","fec057b8":"# Implementing Logistic Regression"}}