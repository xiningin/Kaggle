{"cell_type":{"0c88880b":"code","bbfefd97":"code","e3081f16":"code","02ffdf1a":"code","fd760772":"code","e2de97f9":"code","f63763bb":"code","f4def6c6":"code","b453e87f":"code","567309b2":"code","ba6195b2":"code","fbf66f8d":"code","542f7195":"code","db13149f":"code","a655aeca":"code","24ed6086":"code","e5e56032":"code","e3b0f4d8":"code","a7a5e2af":"code","53d1cc51":"code","a5d5b8a9":"code","030c8008":"markdown","437d8f22":"markdown","fc0d653a":"markdown","a26fa5a9":"markdown","681b5b80":"markdown","af18a176":"markdown","55fbbd54":"markdown","9d914d9d":"markdown","49139b25":"markdown","3c76d409":"markdown","5e8bc8f6":"markdown"},"source":{"0c88880b":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport dask\nimport dask.dataframe as dd\n\nimport cv2\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import iplot\n\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline = False, world_readable = True)\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.style.use('seaborn-whitegrid')\nsns.set_palette('Set2')\n\nimport glob as glob\nimport gc\n\nimport requests\nimport urllib\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter('ignore')","bbfefd97":"def tsv_to_feather(tsv_files):\n    for i, tsv in enumerate(tsv_files):\n        print(f\"Processingc file: {i + 1} ...\")\n        df = dd.read_csv(tsv, sep = '\\t', quoting = 3, escapechar = '\\n', \n                         on_bad_lines = 'skip', dtype = 'string')\n        df = df.dropna()\n        df = df.compute() #Convert to pandas df\n        df = df.reset_index(drop = True)\n        #Save as feature to save disk space and read faster\n        df.to_feather(tsv.split('\/')[-1].split('.')[0])\n        print(f\"Tsv file {tsv.split('\/')[-1].split('.')[0]} stored as feather file\")\n\n        del df\n        gc.collect()","e3081f16":"%%time\ntrain_tsvs = glob.glob('\/kaggle\/input\/wikipedia-image-caption\/train*.tsv')\n#tsv_to_feather(train_tsvs)","02ffdf1a":"train_external = '\/kaggle\/input\/train-tsv-file-to-feather-files\/'","fd760772":"test_df = pd.read_csv('\/kaggle\/input\/wikipedia-image-caption\/test.tsv', sep = '\\t')\nprint(test_df.shape)\ntest_df.head()","e2de97f9":"sub = pd.read_csv('\/kaggle\/input\/wikipedia-image-caption\/sample_submission.csv')\nprint(sub.shape)\nsub.head()","f63763bb":"train_feathers = glob.glob(train_external + 'train*')\nprint(train_feathers)","f4def6c6":"train_df = pd.DataFrame()\nfor file in train_feathers:\n    df = pd.read_feather(file)\n    train_df = pd.concat([train_df, df])\nprint(f\"Before removing duplicate rows: {train_df.shape}\")\ntrain_df = train_df.drop_duplicates() #Drop duplicate rows if any\nprint(f\"After removing duplicate rows: {train_df.shape}\")\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True)\ntrain_df.head()","b453e87f":"train_df.isna().any()","567309b2":"train_df['language'].unique()","ba6195b2":"train_df['language'].value_counts()","fbf66f8d":"def url_to_images(df: pd.DataFrame, num_images: int, flag: str) -> tuple:\n    images_to_display = []\n    #Get random 'num_images' url from df\n    if flag == 'train':\n        sample_df = df[['image_url', 'caption_title_and_reference_description']].sample(num_images)\n    else:\n        sample_df = df[['image_url']].sample(num_images)\n    URLS = sample_df['image_url'].tolist()\n    for img_url in URLS:\n        try:\n            with urllib.request.urlopen(img_url) as url:\n                with open('.\/temp.jpg', 'wb') as f:\n                    f.write(url.read())\n            img = cv2.imread('.\/temp.jpg')\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n            images_to_display.append(img)\n        except:\n            continue\n    if flag == 'train':\n        return images_to_display, sample_df['caption_title_and_reference_description'].tolist()\n    else:\n        return images_to_display","542f7195":"from textwrap import wrap\n\ndef display_images(df: pd.DataFrame, rows: int, cols: int, flag: str = 'train') -> None:\n    if flag == 'train':\n        images, captions = url_to_images(df, num_images = rows * cols, flag = flag)\n    else:\n        images = url_to_images(df, num_images = rows * cols, flag = flag)\n    \n    fig, ax = plt.subplots(rows, cols, figsize = (20, 12))\n    ax = ax.flatten()\n    for p in range(rows * cols):\n        try:\n            ax[p].imshow(images[p])\n            ax[p].grid(False)\n            ax[p].axis('off')\n        except:\n            continue\n        if flag == 'train':\n            ax[p].set_title('\\n'.join(wrap(captions[p], 30)))\n    fig.tight_layout()\n    plt.show()\n    return None","db13149f":"display_images(train_df, 3, 3, 'train')","a655aeca":"display_images(train_df, 3, 3, 'train')","24ed6086":"display_images(test_df, 3, 3, 'test')","e5e56032":"def clean_format(x):\n    if (x.lower() == 'jpg') or (x.lower() == 'jpeg'):\n        return 'jpg'\n    elif (x.lower() == 'png'):\n        return 'png'\n    elif (x.lower() == 'tif') or (x.lower() == 'tiff'):\n        return 'tif'\n    elif (x.lower() == 'svg'):\n        return 'svg'\n    elif (x.lower() == 'gif'):\n        return 'gif'","e3b0f4d8":"test_df['format'] = test_df['image_url'].apply(lambda x: x.split('.')[-1])\ntest_df['format'] = test_df['format'].apply(clean_format)\nax = sns.countplot(test_df['format'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\nplt.title('Test Image Formats Counts');","a7a5e2af":"train_df['target_len'] = train_df['caption_title_and_reference_description'].apply(lambda x: len(x.split()))\ntrain_df['target_len'].hist();","53d1cc51":"print(f\"Max no. of words in target: {train_df['target_len'].max()}\")\nprint(f\"Min no. of words in target: {train_df['target_len'].min()}\")\nprint(f\"Avg. no. of words in target: {train_df['target_len'].mean()}\")","a5d5b8a9":"train_df = train_df[train_df['target_len'] != 0]\nprint(f\"Max no. of words in target: {train_df['target_len'].max()}\")\nprint(f\"Min no. of words in target: {train_df['target_len'].min()}\")\nprint(f\"Avg. no. of words in target: {train_df['target_len'].mean()}\")\ntrain_df.shape","030c8008":"## Target Text","437d8f22":"## Train Image and Caption Visualization","fc0d653a":"- Looks like there are GIF, SVG, TIF, PNG formats as well in addition to JPG","a26fa5a9":"# WIP\nMore to come...","681b5b80":"__Missing Values__","af18a176":"- Read all the feather files and make it into one dataframe","55fbbd54":"## Display Test Images","9d914d9d":"There is a sample with no target caption, remove this from train dataset","49139b25":"Use dask to read the dataframes, its fast!","3c76d409":"# Competition\n\nIn this competition, you\u2019ll build a model that automatically retrieves the text closest to an image. Specifically, you'll train your model to associate given images with article titles or complex captions, in multiple languages. The best models will account for the semantic granularity of Wikipedia images.\n\nThe objective is to predict the target *'caption_title_and_reference_description'* given information about an images\n# Evaluation\n\nSubmissions will be evaluated using NDCG@5 (Normalized Discounted Cumulative Gain).","5e8bc8f6":"This Notebook uses external dataset [feather format](https:\/\/www.kaggle.com\/msafi04\/train-tsv-file-to-feather-files) created from the train tsv files"}}