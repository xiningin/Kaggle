{"cell_type":{"a660edbf":"code","3cc5daea":"code","b61104f2":"code","92d151a6":"code","1747f0a4":"code","723fee60":"code","1a81072e":"code","4ef4e059":"code","c43c7d78":"code","4b48ddca":"code","705e2a59":"code","6b07c976":"code","7762e311":"code","3eb4d418":"code","492596a4":"code","f0dba305":"code","3e1545eb":"code","46c793e4":"code","1a4831d9":"code","823ff2a0":"code","61c34b08":"code","0b1e1148":"code","dda8e85f":"code","1ce31260":"code","25233ba5":"code","22166657":"markdown","64216f71":"markdown","42df4858":"markdown","df8eda3d":"markdown","5f30d2e4":"markdown","b25c6701":"markdown","b6e87deb":"markdown","74c3c8f2":"markdown","cfc74045":"markdown","990a3a1e":"markdown","539782d7":"markdown","2559d765":"markdown"},"source":{"a660edbf":"import re\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nimport missingno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import LSTM\n\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntensorflow.__version__","3cc5daea":"nltk.download(\"stopwords\")","b61104f2":"# Reading train and test data\ndf_train = pd.read_csv(\"..\/input\/fake-news\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/fake-news\/test.csv\")","92d151a6":"df_train.shape, df_test.shape","1747f0a4":"df_train.head(3)","723fee60":"# Viewing all the train data\nmissingno.matrix(df_train)","1a81072e":"# Viewing all the test data\nmissingno.matrix(df_test)","4ef4e059":"fig = plt.figure(figsize=(20,1))\nsns.countplot(y=\"label\", data=df_train)","c43c7d78":"df_train.fillna(\"\", inplace=True)","4b48ddca":"X = df_train.drop(\"label\", axis=1)\ny = df_train[\"label\"]","705e2a59":"X.shape, y.shape","6b07c976":"ps = PorterStemmer()\ntrain_titles = []\nfor i in range(0, len(X)):\n    title = re.sub(\"[^a-zA-Z]\", \" \", X[\"title\"][i]).lower().split()\n    title = [ps.stem(word) for word in title if word not in stopwords.words(\"english\")]\n    train_titles.append(\" \".join(title))\n\nprint(len(train_titles))","7762e311":"# One hot encoding the data\nvocab_size = 5000\ntrain_onehot = [one_hot(title, vocab_size) for title in train_titles]\n\n# Padding the sentences\nsent_length = 20\nembeded_docs = pad_sequences(train_onehot, padding=\"pre\", maxlen=sent_length)","3eb4d418":"embedding_vector_features = 40\n\n# 1: Building the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_vector_features, input_length=sent_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(64, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\n# 2: Comipling the model\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","492596a4":"model.summary()","f0dba305":"history = model.fit(embeded_docs, y, epochs=10, batch_size=64)","3e1545eb":"pd.DataFrame(history.history).plot()","46c793e4":"# Mean of the accuracy\npd.Series(history.history['accuracy']).mean(), pd.Series(history.history['accuracy']).max()","1a4831d9":"df_test.fillna(\"\", inplace=True)","823ff2a0":"missingno.matrix(df_test)","61c34b08":"ps = PorterStemmer()\ntest_titles = []\n\nfor i in range(0, len(df_test)):\n    title = re.sub(\"[^a-zA-Z]\", \" \", df_test[\"title\"][i]).lower().split()\n    title = [word for word in title if word not in stopwords.words(\"english\")]\n    test_titles.append(\" \".join(title))\nlen(test_titles)","0b1e1148":"# One hot encoding the data\nvocab_size = 5000\ntest_onehot = [one_hot(title, vocab_size) for title in test_titles]\n\n# Padding the titles\nsent_length = 20\ntest_embeded_docs = pad_sequences(test_onehot, padding=\"pre\", maxlen=sent_length)","dda8e85f":"y_pred = model.predict_classes(test_embeded_docs);","1ce31260":"submit = pd.read_csv(\"..\/input\/fake-news\/submit.csv\")\nsubmit[\"label\"] = y_pred","25233ba5":"submit.to_csv(\"submission.csv\" ,index=False)","22166657":"## 5: Feature Engineering and Predicting Classed on Test File","64216f71":"## 1: Importing libraries and dataset","42df4858":"## 3: Feature Engineering\n\nSince there are a lot of missing values, we will be replacing the missing values with empty strings","df8eda3d":"## 4: Model Building and Evaluating","5f30d2e4":"Since the mean acuracy is ~ 97% and the max is ~99%, we will be using the same model to prepare the submission file","b25c6701":"Steps in feautre engineering the training data\n1. Preprocessing the text, removing all the other characters, converting it to lower cases, and removing the stop words\n2. Converting the title one hot encoding\n3. Adding a padding to bring the sentences to a common length","b6e87deb":"There are equal distribution of labels on fake news and not so fake news","74c3c8f2":"**Feature Engineering the Training data**","cfc74045":"# Fake News Classification Using LSTM\nLink to challenge - https:\/\/www.kaggle.com\/c\/fake-news\/data#\n\nOverview: \n- This notebooks aims at classifying a news as either fake or not using its titles\n- Exploratory Data Analysis is performed to understand if there are any missing values, and replace them with empty strings if there are any\n- Basic feature engineering is performed on the data to normalize the text and to remove stop words\n- Along with normalizing text, one hot encoding matrix is built to be used by the embedding layer. This matrix is then padded with 0's to bring each title to a common shape\n- The pre-processed text is trained on a LSTM network which produced a mean accuracy of 97%\n- The same model is used to predict the test data\n","990a3a1e":"Splitting `df_train` to features (X) and labels (y)","539782d7":"## 2: EDA","2559d765":"## 6: Preparing the Submission File"}}