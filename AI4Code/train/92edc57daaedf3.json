{"cell_type":{"46225db0":"code","d7cb41f2":"code","a349d0ce":"code","249970cb":"code","7aa16cba":"code","2a27c9b0":"code","22c04db7":"code","70e80d9c":"code","3dce7009":"code","bf6b1235":"code","a88a799f":"code","f1bfaeb3":"code","9d4345e9":"code","e121be6b":"code","32efc1f0":"code","e319022b":"code","f3be986a":"code","df77962e":"code","ebc059dc":"code","7bf41b5f":"code","f2655f4f":"code","7ddfabfb":"code","5330a4f8":"code","5e072011":"code","8825a59c":"code","c2360e29":"code","d8748dbf":"code","e3384240":"code","c9eb4010":"code","461829bc":"code","43ec6e97":"code","9b7fc166":"code","504b3478":"code","7c9474f2":"code","1eb6c8df":"code","2ce940a5":"code","0ab79995":"code","0c16be21":"code","247e87a0":"code","203d7395":"code","cafefd34":"code","b87c7292":"code","ef44c82c":"code","3dfd303d":"code","e7845ab6":"code","9252dd90":"code","d485f530":"code","a25cad11":"code","0aa69c86":"code","33bb66e5":"code","a51356d7":"code","f4471f24":"code","5ed72a53":"code","3dff83f0":"code","2a4c3e59":"code","4202dd1d":"code","21580b7b":"code","22a586cd":"code","025cad4a":"code","12f07ffb":"code","5c5f90e3":"code","f108d75b":"code","b92b6ff1":"code","52fdca1a":"code","7528871a":"code","4063ecd6":"code","ddf35dc2":"code","99ba5aa3":"code","6fbc9965":"code","73cfb8d5":"code","c1ef38bd":"code","f3e43508":"code","17b9f01b":"code","d9b7377f":"code","4e7448d1":"code","d53b39ca":"code","59ad05cd":"code","7ecd3d1c":"code","0b4b75a9":"code","e080e1c6":"code","3b39bdff":"code","39916179":"code","98b49d21":"code","c4748a46":"code","12fea2da":"code","86417692":"code","df2c7d84":"code","1d4817d7":"code","c0ab41ab":"code","ababd35a":"code","b84cd42b":"code","3702a862":"code","9095326a":"code","27a503a5":"markdown","c1725b8b":"markdown","a3e47c6d":"markdown","d4608a49":"markdown","0523b8d0":"markdown","c3bb8867":"markdown","ac95c45c":"markdown","841472e1":"markdown","ad6a607c":"markdown","b6ef2809":"markdown","93ee217d":"markdown","45974c92":"markdown","b39f1f52":"markdown","beda1f24":"markdown","adeabafb":"markdown","e9117a07":"markdown","ea7f9b52":"markdown","a7f7d4cd":"markdown","651c0bb3":"markdown","9628e45a":"markdown","d5555ec1":"markdown","5c69ec55":"markdown","b28a18c6":"markdown","dcaac02c":"markdown","fb4f409d":"markdown","34ffc9b8":"markdown","247ee7ee":"markdown","cbfec166":"markdown","9cd3687b":"markdown","55f87a73":"markdown","a8bc467f":"markdown","19f82bbe":"markdown"},"source":{"46225db0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7cb41f2":"# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","a349d0ce":"#Importing the Dataset\ntrain_data = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv')\ntest_data = pd.read_csv('..\/input\/santander-customer-satisfaction\/test.csv')","249970cb":"print(train_data.shape, test_data.shape)","7aa16cba":"train_data.head()","2a27c9b0":"train_data.describe()","22c04db7":"train_data.dtypes","70e80d9c":"train_data.info()","3dce7009":"sns.countplot(x = train_data[\"TARGET\"])","bf6b1235":"train_data[\"TARGET\"].value_counts()\/len(train_data)","a88a799f":"train_data.isnull().sum()","f1bfaeb3":"train_data['var3'].describe()","9d4345e9":"train_data['var3'].hist(bins = 14)","e121be6b":"train_data['var15'].describe()","32efc1f0":"train_data['var15'].hist(by = train_data['TARGET'], bins = 50)\nplt.xlabel('Age')\nplt.ylabel('No Of Customers')","e319022b":"sns.FacetGrid(train_data, hue=\"TARGET\", size=6).map(plt.hist, \"var15\",edgecolor='w').add_legend()\nplt.xlabel(\"var15(Age)\")\nplt.ylabel(\"No of Customers\")\nplt.title('var 15 impact on Target value')\nplt.show()","f3be986a":"train_data['var38'].hist(by=train_data['TARGET'], bins = 25)","df77962e":"# Removing the data with constant features (i.e zero variance where std=0)\nconstant_features = [features for features in train_data.columns if train_data[features].std() == 0]\nlen(constant_features)\ntrain_data[constant_features]","ebc059dc":"# Drop the constant features from traina and test dataset\ntrain_data.drop(labels = constant_features, axis = 1, inplace=True)\ntest_data.drop(labels = constant_features, axis = 1, inplace = True)","7bf41b5f":"print(train_data.shape,test_data.shape)","f2655f4f":"# Find the columns where most of the values are equal( approx to 99.9% )\napprox_constants = []\nfor feature in train_data.columns:\n    approx_value = (train_data[feature].value_counts()\/ np.float(len(train_data))).sort_values(ascending=False).values[0]\n    if approx_value > 0.999:\n      approx_constants.append(feature)\nprint(len(approx_constants))\ntrain_data[approx_constants]","7ddfabfb":"train_data_ac=train_data.copy()\ntest_data_ac=test_data.copy()","5330a4f8":"train_data_ac.drop(labels = approx_constants, axis = 1, inplace = True)\ntest_data_ac.drop(labels = approx_constants, axis = 1, inplace = True)","5e072011":"print(train_data.shape, train_data_ac.shape, test_data.shape, test_data_ac.shape)","8825a59c":"#Remove Duplicate Data, Columns having smae values\nduplicate_features = []\nfor i in range(0, len(train_data.columns)):\n    col_1 = train_data.columns[i]\n    for col_2 in train_data.columns[i + 1:]:\n        if train_data[col_1].equals(train_data[col_2]):\n            duplicate_features.append(col_2)\nlen(duplicate_features)\ntrain_data[duplicate_features]","c2360e29":"# Dropping the duplicate features from the dataset as their contribute towards prediction of target is neligible \ntrain_data.drop(labels = duplicate_features, axis = 1, inplace=True)\ntest_data.drop(labels = duplicate_features, axis = 1, inplace = True)","d8748dbf":"# Dropping the duplicate features from the dataset as their contribute towards prediction of target is neligible \nfor feature in duplicate_features:\n  if feature in train_data_ac and feature in test_data_ac:\n    train_data_ac.drop(labels = feature, axis = 1, inplace=True)\n    test_data_ac.drop(labels = feature, axis = 1, inplace = True)","e3384240":"print(train_data.shape,test_data.shape,train_data_ac.shape,test_data_ac.shape)","c9eb4010":"from sklearn.feature_selection import VarianceThreshold\ndef features_wo_low_variance(data):\n  threshold_n = 0.98  \n  sel = VarianceThreshold(threshold = (threshold_n* (1 - threshold_n) ))\n  sel_var = sel.fit_transform(data)\n  return data.columns[sel.get_support(indices = True)]","461829bc":"(train_data.var() < 0.02).value_counts()","43ec6e97":"train_data_hv = train_data[features_wo_low_variance(train_data)].copy()\ntest_data_hv = test_data[features_wo_low_variance(train_data)[:-1]].copy()","9b7fc166":"print(train_data_hv.shape,test_data_hv.shape)","504b3478":"# Dataset without accurate constants\ntrain_data_ac_hv = train_data_ac[features_wo_low_variance(train_data_ac)].copy()\ntest_data_ac_hv = test_data_ac[features_wo_low_variance(train_data_ac)[:-1]].copy()","7c9474f2":"print(train_data_ac_hv.shape,test_data_ac_hv.shape)","1eb6c8df":"def find_correlated_features(data):\n  # Create correlation matrix\n  corr_matrix = data.corr().abs()\n\n  # Select upper triangle of correlation matrix\n  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n\n  # Find features with correlation greater than 0.95\n  to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n  print(len(to_drop))\n  return to_drop","2ce940a5":"plt.figure(figsize = (20,25))\nsns.heatmap(train_data_ac_hv.corr())","0ab79995":"# Drop features having correlation greater than 0.95\ntrain_data_hv_co= train_data_hv.drop(find_correlated_features(train_data_hv), axis=1)\ntest_data_hv_co= test_data_hv.drop(find_correlated_features(train_data_hv), axis=1)","0c16be21":"# shape of the train_data after dropping high correlated features\nprint(train_data_hv_co.shape, test_data_hv_co.shape)","247e87a0":"# shape of the train_data after dropping high correlated features\nprint(train_data_hv_co.shape,test_data_hv_co.shape)","203d7395":"# Drop features having correlation greater than 0.95\ntrain_data_ac_hv_co= train_data_ac_hv.drop(find_correlated_features(train_data_ac_hv), axis=1)\ntest_data_ac_hv_co= test_data_ac_hv.drop(find_correlated_features(train_data_ac_hv), axis=1)","cafefd34":"# shape of the train_data after dropping high correlated features\nprint(train_data_ac_hv_co.shape,test_data_ac_hv_co.shape)","b87c7292":"for feature in train_data[train_data_ac_hv_co.var().sort_values(ascending=False).index[0:10]]:\n    \n  plt.figure(figsize = (12, 8))\n  data = train_data.copy()\n    \n  if 0 in data[feature].unique():\n    pass\n  else:\n    data[feature]=np.log(data[feature])\n    sns.boxplot(y = feature, x = 'TARGET', data = data)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.yticks()\n    plt.show()","ef44c82c":"# Calculating IQR\nQ1 = train_data_ac_hv_co.quantile(0.25)\nQ3 = train_data_ac_hv_co.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","3dfd303d":"# Finding outliers in the dataset\nprint(train_data_ac_hv_co < (Q1 - 1.5 * IQR)) or (train_data_ac_hv_co > (Q3 + 1.5 * IQR))","e7845ab6":"# Removing the outliers\ntrain_data_ol = train_data_ac_hv_co.copy()\ntrain_data_out = train_data_ol[((train_data_ol >= (Q1 - 1.5 * IQR)) & (train_data_ol <= (Q3 + 1.5 * IQR))).all(axis = 1)]","9252dd90":"train_data_out.shape","d485f530":"train_data_out[\"TARGET\"].value_counts()","a25cad11":"# Replacing value \"-999999\" in var3 column with most occuring value(75%) 2\ntrain_data.var3 = train_data.var3.replace(-999999,2)","0aa69c86":"test_data.var3 = test_data.var3.replace(-999999,2)","33bb66e5":"test_data_hv_co.var3 = test_data_hv_co.var3.replace(-999999,2)\ntrain_data_hv_co.var3 = train_data_hv_co.var3.replace(-999999,2)","a51356d7":"test_data_ac.var3 = test_data_ac.var3.replace(-999999,2)\ntrain_data_ac.var3 = train_data_ac.var3.replace(-999999,2)","f4471f24":"\ntest_data_ac_hv.var3 = test_data_ac_hv.var3.replace(-999999,2)\ntrain_data_ac_hv.var3 = train_data_ac_hv.var3.replace(-999999,2)","5ed72a53":"\ntest_data_ac_hv_co.var3 = test_data_ac_hv_co.var3.replace(-999999,2)\ntrain_data_ac_hv_co.var3 = train_data_ac_hv_co.var3.replace(-999999,2)","3dff83f0":"# Removing Target and ID columns to scale the data across all columns between -1 to 1\ntrain_data_scaled2= train_data_hv_co.drop([\"ID\",\"TARGET\"],axis=1)\ntrain_data_scaled1= train_data_ac_hv_co.drop([\"ID\",\"TARGET\"],axis=1)\ntrain_data_scaled= train_data.drop([\"ID\",\"TARGET\"],axis=1)","2a4c3e59":"print(train_data_scaled.shape,train_data_scaled1.shape,train_data_scaled2.shape)","4202dd1d":"from sklearn.decomposition import PCA\ndef find_pca_components(data):\n  pca = PCA().fit(data)\n  plt.rcParams[\"figure.figsize\"] = (18,6)\n\n  fig, ax = plt.subplots()\n  xi = np.arange(1, data.shape[1]+1, step=1)\n  y = np.cumsum(pca.explained_variance_ratio_)\n\n  plt.ylim(0.0,1.1)\n  plt.plot(xi\/2, y, marker='o', linestyle='--', color='b')\n\n  plt.xlabel('Number of Components')\n  plt.xticks(np.arange(0, data.shape[1]\/2, step=2)) #change from 0-based array index to 1-based human-readable label\n  plt.ylabel('Cumulative variance (%)')\n  plt.title('The number of components needed to explain variance')\n  plt.axhline(y=0.98, color='r', linestyle='-')\n  plt.text(0.7, 0.85, '98% cut-off threshold', color = 'red', fontsize=16)\n  print(\"Pca component prediciton:\")\n  ax.grid(axis='x')\n  plt.show()","21580b7b":"from sklearn.preprocessing import StandardScaler\nfor data in [train_data_scaled,train_data_scaled1,train_data_scaled2]:\n  scaler = StandardScaler()\n  find_pca_components(scaler.fit_transform(data))","22a586cd":"def pca_analysis(n_co,data):\n  pca = PCA(n_components=n_co)\n  data_transformed = pca.fit_transform(data)\n  print(\"PCA Analysis for %s pca components\"%(n_co))\n  print(\"Eigen vector for each principal component : \",pca.components_)\n  print(\"Amount of variance by each PCA : \", pca.explained_variance_)\n  print(\"Percentage of variance by each PCA : \", pca.explained_variance_ratio_)\n  print(\"number of features in training data : \", pca.n_features_)\n  print(\"number of samples in training data: \", pca.n_samples_)\n  print(\"noise variance of the data : \",pca.noise_variance_)\n  return pd.DataFrame(data_transformed)","025cad4a":"data_transformed=[]\nfor no, data in [(48,train_data_scaled),(44,train_data_scaled1),(60,train_data_scaled2)]:\n  scaler = StandardScaler()\n  data_transformed.append(pca_analysis(no,scaler.fit_transform(data)))","12f07ffb":"from sklearn.utils import resample\ndef upsampling_dataset(data):\n  data_majority=data[data.TARGET==0] \n  data_minority=data[data.TARGET==1]  \n\n  data_minority_upsampled=resample(data_minority,replace=True,n_samples=73012)\n  data_upsampled=pd.concat([data_minority_upsampled,data_majority])\n\n  data_upsampled.info()\n  print(data_upsampled['TARGET'].value_counts())\n  return data_upsampled","5c5f90e3":"datasets = [train_data_ac,train_data_ac_hv_co,train_data_hv_co]","f108d75b":"test_datasets = [test_data_ac,test_data_ac_hv_co,test_data_hv_co]","b92b6ff1":"upsampled_data = []\nfor data in datasets:\n  upsampled_data.append(upsampling_dataset(data))","52fdca1a":"if 'ID' not in data_transformed[2]:\n    data_transformed[2].insert(1,'ID',train_data['ID'])\nx_pca = data_transformed[2]\ny_pca = train_data[\"TARGET\"]","7528871a":"x_upsample = upsampled_data[2].drop(\"TARGET\",axis=1)\ny_upsample = upsampled_data[2][\"TARGET\"]","4063ecd6":"x = datasets[2].drop(\"TARGET\",axis=1)\ny = datasets[2][\"TARGET\"]","ddf35dc2":"print(x_pca.shape, y_pca.shape, x_upsample.shape, y_upsample.shape, x.shape, y.shape)","99ba5aa3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=44)\nX_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(x_pca, y_pca, test_size=0.30, random_state=44)\nX_upsample_train, X_upsample_test, y_upsample_train, y_upsample_test = train_test_split(x_upsample, y_upsample, test_size =0.30, random_state=44)","6fbc9965":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel","73cfb8d5":"sel_ = SelectFromModel(RandomForestClassifier(n_estimators=200))\nsel_.fit(X_train.fillna(0), y_train)","c1ef38bd":"selected_features = X_train.columns[(sel_.get_support())]\nlen(selected_features)","f3e43508":"train_data[selected_features]","17b9f01b":"train_data[selected_features]","d9b7377f":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC","4e7448d1":"# Model preparation\nmodels = []\nmodels.append(('LR', LogisticRegression(class_weight='balanced')))\nmodels.append(('Bagging Classifier',BaggingClassifier()))\nmodels.append(('KNN', KNeighborsClassifier(weights='distance')))\nmodels.append(('RandomForest', RandomForestClassifier(class_weight='balanced')))\nmodels.append(('DecisionTree', DecisionTreeClassifier(class_weight='balanced')))\nmodels.append(('GradientBoosting', GradientBoostingClassifier()))\nmodels.append(('xgb', XGBClassifier(missing=np.nan, max_depth=6, \nn_estimators=350, learning_rate=0.025, nthread=4, subsample=0.95,\ncolsample_bytree=0.85, seed=4242)))","d53b39ca":"from sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.metrics import f1_score,roc_curve,roc_auc_score,precision_score,recall_score,accuracy_score","59ad05cd":"def model_comparison_plot(model_metrics):\n  plt.figure(figsize = (12,4))\n  sns.heatmap(model_metrics, annot=True, cmap=sns.light_palette((210, 90, 60), input=\"husl\"),linewidth=2)\n  plt.title('Metrics comparison for diff models')\n  plt.show()","7ecd3d1c":"def plot_roc_curve(y_test, prob_dict):\n  sns.set_style('whitegrid')\n  plt.figure()\n  i=0\n  fig, ax = plt.subplots(4,2,figsize=(16,30))\n  for key,prob in prob_dict.items():\n    fpr, tpr, thresholds = metrics.roc_curve( y_test, prob,\n                                                  drop_intermediate = False )\n    roc_auc = metrics.roc_auc_score( y_test, prob)\n    i+= 1\n    plt.subplot(4,2,i)\n    plt.plot( fpr, tpr, color='red',label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.axis('tight')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(key)\n  plt.show()","0b4b75a9":"def model_analysis(title,x_train,y_train,x_test,y_test):\n  df_scores=pd.DataFrame()\n  pred_dict={}\n  for name,model in models:\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    pred_dict[name] = y_pred\n    confusion = confusion_matrix(y_test,y_pred)\n    TP = confusion[1, 1]\n    TN = confusion[0, 0]\n    FP = confusion[0, 1]\n    FN = confusion[1, 0]\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    error = 1-accuracy\n    sensitivity = TP \/ float(FN + TP)\n    specificity = TN \/ (TN + FP)\n    False_positive_rate = 1-specificity\n    precision = TP \/ float(TP + FP)\n    bal_acc = metrics.balanced_accuracy_score(y_test, y_pred)\n    Null_accuracy = max(y_test.mean(), (1 - y_test.mean()))\n    f1 = metrics.f1_score(y_test,y_pred)\n    auc_score = metrics.roc_auc_score(y_test,y_pred)\n    clf_score = pd.DataFrame(\n        {name: [accuracy, bal_acc, Null_accuracy,precision,sensitivity,f1,error,specificity,auc_score]},\n        index=['Accuracy', 'Balanced accuracy','Null_accuracy','precision','recall','f1 score','error','specificity','auc_score']\n    )\n   \n    df_scores = pd.concat([df_scores, clf_score], axis=1).round(decimals=3)\n  print(\"Roc_curve for all models\")\n  plot_roc_curve(y_test,pred_dict)\n  print(title,end='\\n\\n')\n  print(df_scores.to_markdown(),end='\\n\\n')\n  model_comparison_plot(df_scores)","e080e1c6":"model_analysis(\"Model with normal data\", X_train, y_train, X_test, y_test)","3b39bdff":"model_analysis(\"Model with upsampled data\", X_upsample_train, y_upsample_train, X_upsample_test, y_upsample_test)","39916179":"model_analysis(\"Model with pca data\", X_pca_train, y_pca_train, X_pca_test, y_pca_test)","98b49d21":"final_model = RandomForestClassifier(class_weight = 'balanced', random_state = 42)","c4748a46":"final_model.fit(X_upsample_train, y_upsample_train)","12fea2da":"probs = final_model.predict_proba(test_data_hv_co)","86417692":"submission = pd.DataFrame({\"ID\":test_data_hv_co.ID, \"TARGET\": probs[:,1]})","df2c7d84":"submission.to_csv(\"santander_solution.csv\")","1d4817d7":"submission","c0ab41ab":"from sklearn.calibration import CalibratedClassifierCV\nxgb_classifier = XGBClassifier(missing=np.nan, max_depth=6, \nn_estimators=350, learning_rate=0.025, nthread=4, subsample=0.95,\ncolsample_bytree=0.85, seed=4242)\nxgb_mdl = CalibratedClassifierCV(xgb_classifier, method='isotonic', cv=10)\nxgb_mdl.fit(X_upsample_train,y_upsample_train)","ababd35a":"probs_xgb = xgb_mdl.predict_proba(test_data_hv_co)","b84cd42b":"submission1 = pd.DataFrame({\"ID\":test_data_hv_co.ID, \"TARGET\": probs_xgb[:, 1]})","3702a862":"submission1.to_csv(\"submission_xgb.csv\", index = False)","9095326a":"submission1.to_csv(\"\/kaggle\/working\/submission.csv\", index = False)","27a503a5":"We can remove the low variance(such as 0.02) features, as they might not have impact on target value\n\n","c1725b8b":"from the above output, it's clear that we don't have any categorical features. We have features with dtypes as below : \n1. float64 : 111 features\n2. int 64 : 260 features","a3e47c6d":"# Problem Statement - Identify dissatisfied customers for Santander Bank","d4608a49":"# Data Analysis","0523b8d0":"Observation Fro Above Result : \n1. No Missing Value in the Dataset","c3bb8867":"#  PCA Analysis","ac95c45c":"It's clear that the dataset is unbalanaced dataset","841472e1":"Most of the customers (75%) have the same value which means it represents a feature which is common among the customers which might be Gender or Country of the customer.","ad6a607c":"From the above Data few observations as below :","b6ef2809":"**Feature Variance Analysis**","93ee217d":"#  Model Building and Analysis","45974c92":"**Constant and Duplicate features handling**","b39f1f52":"Observations from outlier analysis:\n\n1. We are loosing valuable information if we are removing outliers\n1. We can say that outliers are important in predicting the target value\n1. Dataset reduced from (76020,135) to (27125,135) -- loosing valuable information.\n1. So, we are not considering to remove the outliers","beda1f24":"From the above visualization, we can say that we need to have 48,44, and 60 pca components for each dataset respectively","adeabafb":"**Feature Outlier Analysis**","e9117a07":"**Individual Feature Analysis**","ea7f9b52":"**Missing\/garbage value treatment**","a7f7d4cd":"**Drop the Duplicate Features**","651c0bb3":"**Removing low variance features**","9628e45a":"# Predicting the probability of every customer is unhappy","d5555ec1":"1. we can see that var3 feature column having unknown value -999999, need to impute this value.\n2. var 15 has values ranging from 5 to 105 (var3 may be age)\n3. var38 has min value as 5163.75000 and max value as 22034740.000 (may be relationship value between the bank and customer.)","5c69ec55":"Observations from the above point : \n1. Most Unhappy customers ae in the range of 25 - 50\n2. Most Happy Customers are in the range 21 - 24\n3. Most Customers are in the age range (25 - 35) (45, 000 Customers)","b28a18c6":"# Upsampling the Data","dcaac02c":"Upsampling the data\nAs data is very unbalanced dataset, we will try to upsample the data with minority target","fb4f409d":"* Dataset info , datatypes, etc.\n* Identifying Categorical variables if any\n* Null value\/Missing value check\n* Target value distribution\n* Individual feature Analysis\n* Finding constant features, duplicates features, and approx constant features\n* Feature variance analysis\n* Feature correlation analysis\n* Feature outlier analysis","34ffc9b8":"In this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.","247ee7ee":"1. As correlation matrix is symmetric, it is reduandant to consider the whole matrix.\n1. Instead we can consider either upper or lower triangle of the correaltion matrix.","cbfec166":"**Feature ariance Analysis**","9cd3687b":"# Feature Selection","55f87a73":"* We have created different datasets such as : original dataset, dataset without constant and duplicate features, dataset without constant,duplicate features,low variance,and correlated features and their PCA transformed data, upsampled datasets\n* We will use above datasets to train the model using below algorithms\n\n1. Logistic regression\n2. Random Forest\n3. Decision Tree\n4. Bagging classifier\n5. Support vector classifier\n6. Gradient Boosting classifier\n7. kNearest neighbors classifier","a8bc467f":"**Check Missing Value or Null Vaue**","19f82bbe":"Observations from above model results : \n1. Among all the datasets, upsampled dataset is performing good acorss all models except logistic regression.\n2. Bagging classifier, Decision Tree, Random Forest, K nearest neighboirs, and XGB classifier performing well across all metrics\n3. Bagging classifier and Decision Tree seems to overfit the model"}}