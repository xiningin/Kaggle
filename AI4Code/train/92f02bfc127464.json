{"cell_type":{"1e92bd8d":"code","3e0ebf0a":"code","f6752589":"code","2cd0a91d":"code","066710f9":"code","3cd3199a":"code","ea26cc02":"code","2db65186":"code","92da38e7":"code","aa70f8c0":"code","ae75dd6e":"code","bf40f298":"code","5575852f":"code","2802a87a":"code","94042dc3":"code","085e560b":"code","ac4675f5":"code","d1f5515d":"code","a2b33bcb":"code","59d191b7":"code","3dca59f9":"code","35d31a5a":"code","c7138138":"code","a70ac646":"code","eae562d2":"code","91adb7dc":"code","06cdb095":"code","fb0a2231":"code","934f91f0":"code","4a19dc01":"code","04ba7f42":"code","58858069":"code","53136124":"code","fb847595":"code","f56e906a":"code","65942cab":"code","ea71c9b9":"code","9dbfb6c2":"code","1c15cf06":"code","8cfa89d5":"code","116bd1a5":"code","0762d1ed":"markdown","16e77446":"markdown","3d1e1bed":"markdown","c41bfb81":"markdown","383a08ab":"markdown","dbf49314":"markdown","22d06cdd":"markdown","36c114a1":"markdown","fd813184":"markdown","a514282c":"markdown","51f3d1a5":"markdown","bab430aa":"markdown","ac808032":"markdown","56065b47":"markdown","f8b11d63":"markdown","310763d3":"markdown","0d1a2c71":"markdown","0a9e7781":"markdown","d706f744":"markdown","8eb51a86":"markdown","7ab972c9":"markdown"},"source":{"1e92bd8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # visualization\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e0ebf0a":"raw_train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nraw_test_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_df = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","f6752589":"raw_train_df.head(5)","2cd0a91d":"raw_test_df.head(5)","066710f9":"raw_train_df.shape","3cd3199a":"raw_test_df.shape","ea26cc02":"merged_df = pd.concat([raw_train_df, raw_test_df], sort = False).reset_index(drop=True)\n\nmerged_df.head(5)","2db65186":"merged_df.shape","92da38e7":"interesting_cols = [# 'PassengerId', # lets not use this for now but make a note of it\n                    'Survived',\n                    'Pclass', \n                    'Name', # Going to use this to extract title and inpute missing ages. Will make sense later\n                    'Sex', \n                    'Age',\n                    'SibSp',\n                    'Parch', \n                    # 'Ticket', # lets not use this for now but make a note of it\n                    'Fare', \n                    # 'Cabin', # lets not use this for now but make a note of it\n                    'Embarked'\n                   ]","aa70f8c0":"selected_df = merged_df.copy()","ae75dd6e":"selected_df = selected_df[interesting_cols]","bf40f298":"selected_df","5575852f":"def custom_one_hot_encode(df, column, categories):\n    for category in categories:\n        df[f'{column}_{category}'] = (df[column] == category) * 1\n    del df[column]\n            ","2802a87a":"preprocessed_df = selected_df.copy()\n\n\n","94042dc3":"\nsns.heatmap(preprocessed_df.isnull(), cbar=False)\n","085e560b":"preprocessed_df.isnull().sum()","ac4675f5":"custom_one_hot_encode(preprocessed_df , 'Embarked', preprocessed_df.Embarked.unique())","d1f5515d":" custom_one_hot_encode(preprocessed_df , 'Sex', ['male', 'female'])","a2b33bcb":"preprocessed_df","59d191b7":"preprocessed_df.isnull().sum()","3dca59f9":"preprocessed_df['Title'] = preprocessed_df['Name'].apply(lambda st: st[st.find(\",\")+1:st.find(\".\")])","35d31a5a":"preprocessed_df","c7138138":"preprocessed_df[preprocessed_df['Age'].isnull() & preprocessed_df['Title'].isnull()]","a70ac646":"preprocessed_df['Age'] = preprocessed_df.groupby('Title')['Age'].transform(lambda x: x.fillna(x.mean()))","eae562d2":"preprocessed_df[preprocessed_df['Fare'].isnull()]","91adb7dc":"preprocessed_df[\"Fare\"].fillna(preprocessed_df[\"Fare\"].mean(), inplace=True) ","06cdb095":"preprocessed_df.isnull().sum()","fb0a2231":"del preprocessed_df['Title']\ndel preprocessed_df['Name']","934f91f0":"preprocessed_df.head(5)","4a19dc01":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler","04ba7f42":"train_df = preprocessed_df.iloc[:891, :]\ntest_df  = preprocessed_df.iloc[891:, :]","58858069":"train_df","53136124":"test_df","fb847595":"\ndel test_df['Survived']\n\n\"\"\"Extract data sets as input and output for machine learning models.\"\"\"\nxTrain = train_df.drop(columns = [\"Survived\"], axis = 1) # Input matrix as pandas dataframe (dim:891*47).\nyTrain = train_df['Survived'] # Output vector as pandas series (dim:891*1)\n\n\"\"\"Extract test set\"\"\"\nxTest  = test_df.copy()\n\n\"\"\"See the dimensions of input and output data set.\"\"\"\nprint(f\"Input Matrix Dimension: {xTrain.shape}\")\nprint(f\"Output Vector Dimension: {yTrain.shape}\")\nprint(f\"Test Data Dimension: {xTest.shape}\")\n\n","f56e906a":"# scaled_preprocessed_df = preprocessed_df.copy()\nscaler = StandardScaler()\n\nscaled_xTrain = pd.DataFrame(scaler.fit_transform(xTrain), columns=xTrain.columns)\nscaled_xTest = pd.DataFrame(scaler.fit_transform(xTest), columns=xTest.columns)\n","65942cab":"%%time\n\nparameters = {'max_depth': [3,5,7,10],\n              'min_samples_split': [1, 5, 10, 20, 50, 100]\n             }\n\n         \n\ndtc = DecisionTreeClassifier()\ngridsearch_cv = GridSearchCV(dtc, parameters, scoring='accuracy')\nresult = gridsearch_cv.fit(scaled_xTrain, yTrain)\nresult","ea71c9b9":"pd.DataFrame(result.cv_results_).sort_values(by='rank_test_score')","9dbfb6c2":"gridsearch_cv.best_params_","1c15cf06":"predictions = gridsearch_cv.predict(xTest)","8cfa89d5":"predictions.shape","116bd1a5":"\nsubmissionDTC = pd.DataFrame({\n        \"PassengerId\": raw_test_df[\"PassengerId\"],\n        \"Survived\": predictions})\nsubmissionDTC.to_csv(\"Submission.csv\", index = False)","0762d1ed":"1. Great there are none. Lets delete the Title and Name now as we dont need them anymore","16e77446":"here we are grouping by title and taking the average age. Then this average age is used to fill in the NaN values based on the title.","3d1e1bed":"The bestr parameters for the decision tree are:","c41bfb81":"there are 263 missing age values. We can probably infer this from the name by taking the average age of the title of each person.\nSo for example the average age of miss, Mrs, Mr etc. \n","383a08ab":"Lets check is there are any rows with both the Age and Title empty. Hopefully there wont be any. If there was that means something is not right.","dbf49314":"Lets take a look at both dfs ","22d06cdd":"Here we extract everthing after the first comma and before the period. In other words the title of the person and create a new column called title","36c114a1":"#scratchpad\n1. 'PassengerId' check if this can be useful\n2. 'Name' check if this can be useful\n3. 'Ticket' check if this can be useful\n4. 'Cabin' check if this can be useful\n\n","fd813184":"Checking to see if any NaN now","a514282c":"splitting back into train and test","51f3d1a5":"Although Max depth 5 has a higher mean score I would go with max depth 3 as it has a lower standard deviation with not much difference in the mean.","bab430aa":"Its best to merge train and test data together. This eliminates the hassle of handling train and test data seperately for various analysis and transformations.\nWe'll then split them later into train and test once all the transformations are complete.\n","ac808032":"Loading the data into dataframes to analyse","56065b47":"Here im using GridSearch to run different hyperameters","f8b11d63":"here is what the final preprocessed_df looks like now:","310763d3":"So its a 60.5 year old Male that Embarked from S and has a ticket class of 3. Can we infer his fare somehow?\nThe easiest option is to just take the average which is what I'll do now.\n\nOther complex methods may be to do some group by methods to see if there is any markable difference in mean fare depending on your age and ticket class.","0d1a2c71":"There was also one missing 'Fare'. Lets deal with that","0a9e7781":"Lets make some predictions now:","d706f744":"Lets scale these as the features have different scales","8eb51a86":"Lets create a very naive model that we can then use to improve upon. For this naive model I'm simply going to pick some features, ensure they are numerical and have no missing values and so a decision tree.","7ab972c9":"Lets view the results sorted by score"}}