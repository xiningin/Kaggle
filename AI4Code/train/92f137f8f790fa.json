{"cell_type":{"d6ecea43":"code","55943d68":"code","10c17f3a":"code","f22784b4":"code","95616bb9":"code","f148313e":"code","5d73d50c":"code","9065ae02":"code","ee19fd7e":"code","15e0473a":"code","f6dde05e":"code","8cef96a3":"code","9b71531e":"code","cec62c27":"code","da227429":"code","4b225110":"code","3a6a4aa8":"code","131f1a32":"code","c2bc25af":"code","5a8ddf93":"code","e4a4b02c":"code","e5124ae9":"code","0070d10a":"code","0edb8d95":"code","2458acfd":"code","b5f418fd":"code","1417eb2d":"code","1fc735a7":"markdown","3d7b5842":"markdown","0ea13345":"markdown","4d16d844":"markdown","76d2d217":"markdown","4699b003":"markdown","412cdf11":"markdown","83bfec69":"markdown","a0da58aa":"markdown","38b805f5":"markdown","a09e1bdb":"markdown","ef874f85":"markdown","4930c047":"markdown","e9b0b957":"markdown","6a6f6bba":"markdown","d62d1852":"markdown","08cbf644":"markdown","ac7ee839":"markdown","af3bdc4d":"markdown","c4af0498":"markdown","1c736eb7":"markdown","2e98569a":"markdown","3618dac6":"markdown","d0f278f2":"markdown","1e61e361":"markdown","c2693666":"markdown","34356df8":"markdown","09ea6059":"markdown","849aab2e":"markdown","00df325e":"markdown"},"source":{"d6ecea43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 1000)\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nimport lightgbm as lgb\nimport plotly.figure_factory as ff\nimport gc\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nimport json\nfrom keras.preprocessing import text, sequence\nfrom sklearn.feature_extraction.text import CountVectorizer","55943d68":"path = '\/kaggle\/input\/tensorflow2-question-answering\/'\ntrain_path = 'simplified-nq-train.jsonl'\ntest_path = 'simplified-nq-test.jsonl'\nsample_submission_path = 'sample_submission.csv'\n\ndef read_data(path, sample = True, chunksize = 30000):\n    if sample == True:\n        df = []\n        with open(path, 'rt') as reader:\n            for i in range(chunksize):\n                df.append(json.loads(reader.readline()))\n        df = pd.DataFrame(df)\n        print('Our sampled dataset have {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n    else:\n        df = pd.read_json(path, orient = 'records', lines = True)\n        print('Our dataset have {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n        gc.collect()\n    return df\n\ntrain = read_data(path+train_path, sample = True)\ntest = read_data(path+test_path, sample = False)\ntrain.head()","10c17f3a":"sample_submission = pd.read_csv(path + sample_submission_path)\nprint('Our sample submission have {} rows'.format(sample_submission.shape[0]))\nsample_submission.head()","f22784b4":"def missing_values(df):\n    df = pd.DataFrame(df.isnull().sum()).reset_index()\n    df.columns = ['features', 'n_missing_values']\n    return df\nmissing_values(train)","95616bb9":"missing_values(test)","f148313e":"question_text_0 = train.loc[30, 'question_text']\nquestion_text_0","5d73d50c":"document_text_0 = train.loc[30, 'document_text'].split()\n\" \".join(document_text_0[:800])","9065ae02":"long_answer_candidates_0 = train.loc[30, 'long_answer_candidates']\nlong_answer_candidates_0[0:10]","ee19fd7e":"annotations_0 = train['annotations'][30][0]\nannotations_0","15e0473a":"print('Our question is : ', question_text_0)\nprint('Our short answer is : ', \" \".join(document_text_0[annotations_0['short_answers'][0]['start_token']:annotations_0['short_answers'][0]['end_token']]))\nprint('Our long answer is : ', \" \".join(document_text_0[annotations_0['long_answer']['start_token']:annotations_0['long_answer']['end_token']]))","f6dde05e":"yes_no_answer = []\nfor i in range(len(train)):\n    yes_no_answer.append(train['annotations'][i][0]['yes_no_answer'])\nyes_no_answer = pd.DataFrame({'yes_no_answer': yes_no_answer})\n    \ndef bar_plot(df, column, title, width, height, n, get_count = True):\n    if get_count == True:\n        cnt_srs = df[column].value_counts(normalize = True)[:n]\n    else:\n        cnt_srs = df\n        \n    trace = go.Bar(\n        x = cnt_srs.index,\n        y = cnt_srs.values,\n        marker = dict(\n            color = '#1E90FF',\n        ),\n    )\n\n    layout = go.Layout(\n        title = go.layout.Title(\n            text = title,\n            x = 0.5\n        ),\n        font = dict(size = 14),\n        width = width,\n        height = height,\n    )\n\n    data = [trace]\n    fig = go.Figure(data = data, layout = layout)\n    py.iplot(fig, filename = 'bar_plot')\nbar_plot(yes_no_answer, 'yes_no_answer', 'Yes No Answer Distribution', 800, 500, 3)","8cef96a3":"# this function extract the short answers and fill a dataframe\ndef extract_target_variable(df, short = True):\n    if short:\n        short_answer = []\n        for i in range(len(df)):\n            short = df['annotations'][i][0]['short_answers']\n            if short == []:\n                yes_no = df['annotations'][i][0]['yes_no_answer']\n                if yes_no == 'NO' or yes_no == 'YES':\n                    short_answer.append(yes_no)\n                else:\n                    short_answer.append('EMPTY')\n            else:\n                short = short[0]\n                st = short['start_token']\n                et = short['end_token']\n                short_answer.append(f'{st}'+':'+f'{et}')\n        short_answer = pd.DataFrame({'short_answer': short_answer})\n        return short_answer\n    else:\n        long_answer = []\n        for i in range(len(df)):\n            long = df['annotations'][i][0]['long_answer']\n            if long['start_token'] == -1:\n                long_answer.append('EMPTY')\n            else:\n                st = long['start_token']\n                et = long['end_token']\n                long_answer.append(f'{st}'+':'+f'{et}')\n        long_answer = pd.DataFrame({'long_answer': long_answer})\n        return long_answer\n        \nshort_answer = extract_target_variable(train)\nshort_answer.head()","9b71531e":"short_answer['type'] = short_answer['short_answer'].copy()\nshort_answer.loc[(short_answer['short_answer']!='EMPTY') & (short_answer['short_answer']!='YES') & (short_answer['short_answer']!='NO'), 'type'] =  'TEXT'\nbar_plot(short_answer, 'type', 'Short Answer Distribution', 800, 500, 10)","cec62c27":"long_answer = extract_target_variable(train, False)\nlong_answer.head()","da227429":"long_answer['type'] = long_answer['long_answer'].copy()\nlong_answer.loc[(long_answer['long_answer']!='EMPTY'), 'type'] =  'TEXT'\nbar_plot(long_answer, 'type', 'Long Answer Distribution', 800, 500, 10)","4b225110":"def count_word_frequency(series, top = 0, bot = 20):\n    cv = CountVectorizer()   \n    cv_fit = cv.fit_transform(series)    \n    word_list = cv.get_feature_names(); \n    count_list = cv_fit.toarray().sum(axis=0)\n    frequency = pd.DataFrame({'Word': word_list, 'Frequency': count_list})\n    frequency.sort_values(['Frequency'], ascending = False, inplace = True)\n    frequency['Percentage'] = frequency['Frequency']\/frequency['Frequency'].sum()\n    frequency.drop('Frequency', inplace = True, axis = 1)\n    frequency['Percentage'] = frequency['Percentage'].round(3)\n    frequency = frequency.iloc[top:bot]\n    frequency.set_index('Word', inplace = True)\n    bar_plot(pd.Series(frequency['Percentage']), 'Percentage', 'Question Text Word Frequency Distribution', 800, 500, 20, False)\n    return frequency\n    \nfrequency = count_word_frequency(train['question_text'])","3a6a4aa8":"frequency = count_word_frequency(train['question_text'], 20, 40)","131f1a32":"frequency = count_word_frequency(test['question_text'])","c2bc25af":"frequency = count_word_frequency(test['question_text'], 20, 40)","5a8ddf93":"frequency = count_word_frequency(test['document_text'])","e4a4b02c":"from collections import defaultdict\nfrom wordcloud import WordCloud, STOPWORDS\n\nlong_answer = extract_target_variable(train, False)\nshort_answer = extract_target_variable(train)\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    punct = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\"--\u2026\"\"'\n    token = [token for token in text.lower().split(\" \") if token != \"\" \n             if token != \" \" if token not in STOPWORDS if \"<\" not in token if token not in punct] #modified\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train['question_text']:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in test['question_text']:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words train\", \n                                          \"Frequent words test\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n\n#plt.figure(figsize=(10,16))\n#sns.barplot(x=\"ngram_count\", y=\"ngram\", data=fd_sorted.loc[:50,:], color=\"b\")\n#plt.title(\"Frequent words for Insincere Questions\", fontsize=16)\n#plt.show()","e5124ae9":"freq_dict = defaultdict(int)\nfor sent in train['question_text']:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in test['question_text']:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams train\", \n                                          \"Frequent bigrams test\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","0070d10a":"freq_dict = defaultdict(int)\ncnt = 0\nfor sent in test['document_text']:\n    if cnt < 10:\n        #print(cnt)\n        #print(sent)\n        cnt += 1\n    else:\n        break\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\ncnt = 0\nfor sent in test['document_text']:\n    if cnt < 30:\n        #print(cnt)\n        #print(sent)\n        cnt += 1\n    else:\n        break\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams test\", \n                                          \"Frequent bigrams test\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","0edb8d95":"def build_train_test_long(df, train = True):\n    final_long_answer_frame = pd.DataFrame()\n    if train == True:\n        # get long answer\n        long_answer = extract_target_variable(df, False)\n        \n        # iterate over each row to get the possible answers\n        for index, row in df.iterrows():\n            start_end_tokens = []\n            questions = []\n            responds = []\n            for i in row['long_answer_candidates']:\n                start_token = i['start_token']\n                end_token = i['end_token']\n                start_end_token = str(i['start_token']) + ':' + str(i['end_token'])\n                question = row['question_text']\n                respond = \" \".join(row['document_text'].split()[start_token : end_token])\n                start_end_tokens.append(start_end_token)\n                questions.append(question)\n                responds.append(respond)\n\n            long_answer_frame = pd.DataFrame({'question': questions, 'respond': responds, 'start_end_token': start_end_tokens})\n            long_answer_frame['answer'] = long_answer.iloc[index][0]\n            long_answer_frame['target'] = long_answer_frame['start_end_token'] == long_answer_frame['answer']\n            long_answer_frame['target'] = long_answer_frame['target'].astype('int16')\n            long_answer_frame.drop(['answer'], inplace = True, axis = 1)\n            final_long_answer_frame = pd.concat([final_long_answer_frame, long_answer_frame])\n        return final_long_answer_frame\n    else:\n         # iterate over each row to get the possible answers\n        for index, row in df.iterrows():\n            start_end_tokens = []\n            questions = []\n            responds = []\n            for i in row['long_answer_candidates']:\n                start_token = i['start_token']\n                end_token = i['end_token']\n                start_end_token = str(i['start_token']) + ':' + str(i['end_token'])\n                question = row['question_text']\n                respond = \" \".join(row['document_text'].split()[start_token : end_token])\n                start_end_tokens.append(start_end_token)\n                questions.append(question)\n                responds.append(respond)\n\n            long_answer_frame = pd.DataFrame({'question': questions, 'respond': responds, 'start_end_token': start_end_tokens})\n            final_long_answer_frame = pd.concat([final_long_answer_frame, long_answer_frame])\n        return final_long_answer_frame\n        \n\n\ndef build_train_test_short(df, train = True):\n    \n    final_short_answer_frame = pd.DataFrame()\n    \n    if train == True:\n        # get short answer\n        short_answer = extract_target_variable(df, True)\n\n        # iterate over each row to get the possible answer\n        for index, row in df.iterrows():\n            start_tokens = []\n            end_tokens = []\n            start_end_tokens = []\n            questions = []\n            responds = []\n            for i in row['long_answer_candidates']:\n                start_token = i['start_token']\n                end_token = i['end_token']\n                start_end_token = str(i['start_token']) + ':' + str(i['end_token'])\n                question = row['question_text']\n                respond = \" \".join(row['document_text'].split()[int(start_token) : int(end_token)])\n                start_tokens.append(start_token)\n                end_tokens.append(end_token)\n                start_end_tokens.append(start_end_token)\n                questions.append(question)\n                responds.append(respond)\n\n            short_answer_frame = pd.DataFrame({'question': questions, 'respond': responds, 'start_token': start_tokens, 'end_token': end_tokens, 'start_end_token': start_end_tokens})\n            short_answer_frame['answer'] = short_answer.iloc[index][0]\n            short_answer_frame['start_token_an'] = short_answer_frame['answer'].apply(lambda x: x.split(':')[0] if ':' in x else 0)\n            short_answer_frame['end_token_an'] = short_answer_frame['answer'].apply(lambda x: x.split(':')[1] if ':' in x else 0)\n            short_answer_frame['start_token_an'] = short_answer_frame['start_token_an'].astype(int)\n            short_answer_frame['end_token_an'] = short_answer_frame['end_token_an'].astype(int)\n            short_answer_frame['target'] = 0\n            short_answer_frame.loc[(short_answer_frame['start_token_an'] >= short_answer_frame['start_token']) & (short_answer_frame['end_token_an'] <= short_answer_frame['end_token']), 'target'] = 1\n            short_answer_frame.drop(['answer', 'start_token', 'end_token', 'start_token_an', 'end_token_an'], inplace = True, axis = 1)\n            final_short_answer_frame = pd.concat([final_short_answer_frame, short_answer_frame])\n        return final_short_answer_frame\n    else:\n        # iterate over each row to get the possible answer\n        for index, row in df.iterrows():\n            start_end_tokens = []\n            questions = []\n            responds = []\n            for i in row['long_answer_candidates']:\n                start_token = i['start_token']\n                end_token = i['end_token']\n                start_end_token = str(i['start_token']) + ':' + str(i['end_token'])\n                question = row['question_text']\n                respond = \" \".join(row['document_text'].split()[int(start_token) : int(end_token)])\n                start_end_tokens.append(start_end_token)\n                questions.append(question)\n                responds.append(respond)\n\n            short_answer_frame = pd.DataFrame({'question': questions, 'respond': responds, 'start_end_token': start_end_tokens})\n            final_short_answer_frame = pd.concat([final_short_answer_frame, short_answer_frame])\n        return final_short_answer_frame","2458acfd":"sh = build_train_test_long(train.head())\nsh.head()","b5f418fd":"sh[sh['target']==1]","1417eb2d":"long_answer.head()","1fc735a7":"# Evaluation\u00b6\nSubmissions are evaluated using micro F1 between the predicted and expected answers. Predicted long and short answers must match exactly the token indices of one of the ground truth labels ((or match YES\/NO if the question has a yes\/no short answer). There may be up to five labels for long answers, and more for short. If no answer applies, leave the prediction blank\/null.\n\nThe metric in this competition diverges from the original metric in two key respects: 1) short and long answer formats do not receive separate scores, but are instead combined into a micro F1 score across both formats, and 2) this competition's metric does not use confidence scores to find an optimal threshold for predictions.\n\n","3d7b5842":"* Top words are repeated","0ea13345":"* 98.7% is None\n* The amount of observations that are YES and NO only sum 1.3%!","4d16d844":"# Document URL\n\nThis is the url of the document text. Maybee we can use this for something but i will not analyze this variable because i believe we will not get any insight from it.","76d2d217":"For some reason, after the 30th document text, the word chart is dominated by blanks. Maybe someone can help fix.","4699b003":"* We can see some differences between train and test.","412cdf11":"# Data fields\n* document_text - the text of the article in question (with some HTML tags to provide document structure). The text can be tokenized by splitting on whitespace.\n* question_text - the question to be answered\n* long_answer_candidates - a JSON array containing all of the plausible long answers.\n* annotations - a JSON array containing all of the correct long + short answers. Only provided for train.\n* document_url - the URL for the full article. Provided for informational purposes only. This is NOT the simplified version of the article so indices from this cannot be used directly. The content may also no longer match the html used to generate document_text. Only provided for train.\n* example_id - unique ID for the sample.","83bfec69":"# Load Data\n\nThe dataset is huge, for exploration purpose we are going perform the exploratory analysis over a sample of the dataset. Let's read the training data and extract a sample (hopefully the dataset is shuffled so that the first records are random)","a0da58aa":"Combining info from two fantastic kernels:\n\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc\n\nhttps:\/\/www.kaggle.com\/ragnar123\/exploratory-data-analysis-and-baseline","38b805f5":"# Logic\n\nThis extructure is not easy to understand, let's explore the first line of our train set to understand the logic of this dataset. (changed to 30th entry)","a09e1bdb":"# Question Explorations\n\nLet's explore our question_text column which tell us the question that we want to answer with a segment of the document text\n\n* Count the number of words and check distribution\n* Most common words","ef874f85":"Let's check the submission file to understand better what we need to predict\n\n# Submission File\nFor each ID in the test set, you must predict a) a set of start:end token indices, b) a YES\/NO answer if applicable (short answers ONLY), or c) a BLANK answer if no prediction can be made. The file should contain a header and have the following format:\n\n* -7853356005143141653_long,6:18\n* -7853356005143141653_short,YES\n* -545833482873225036_long,105:200\n* -545833482873225036_short,\n* -6998273848279890840_long,\n* -6998273848279890840_short,NO\n\nInteresting :).","4930c047":"# Missing Values","e9b0b957":"So we have some interesting words like many, world, played, name, song, movie and a lot more.\n\nLet\u00b4s check the test set and see if we have the same behaviour.","6a6f6bba":"* Now that we understand the main logic let's check the distributions of our target variable. Remeber that our test it's going to expand to 692 rows because for each question we need to answer the long and short answer.\n\n# Target Variable Exploration","d62d1852":"Great we don't have missing values.","08cbf644":"> So in the first column we have a huge wikipedia text. This is where we need to find the answer for the previous question","ac7ee839":"# Document Text Exploration\n\nWe are only going to analyze the test set because this documents are very big.","af3bdc4d":"* This is a sample of our training set for long answers.\n* We have 4 text answer for the first 5 question\n* For long answers we will get a probability for each question response combination. We will filter this result by each question to check if there is a hight probability for one of the answers. \n* For short answer we are going to do the same, but is harder because we only will know the long tokens. We need to figure how we can extract the short token indices.\n* The dataframe is very large so maybee we will need to remake this function so they work with a batching process.\n* We can make 2 models to resolve this problem (long answer and short answer)","c4af0498":"I believe this is the main question you need to respond.","1c736eb7":"This are all the possibles long answers ranges. In other words they give you the start indices and last indices of all the possibles long answers in the document text columns that could answer the question.","2e98569a":"First let's explore if we have missing values","3618dac6":"* We need to clean this column to have a better idea.\n* Leaving this part for the future notebooks because i believe it\u00b4s not easy to clean it.","d0f278f2":"* This is our target variable. In this case this is telling us that our long answer starts in indices 1952 and end at indices 2019.\n* Also, we have a short answer that starts at indices 1960 and end at indices 1969.\n* In this example we dont have a yes or no answer\n* If you check the submission file we have 692 rows, this means that for each row in the test set we have to predict the short and long answer\n* Sometime long and short answer are not available, in this case it's possible that we have a Yes or No answer for the short answer.\n\nLets check the entire logic of the first line of our train set (30th)","1e61e361":"# Short Answer Results\n\n* We have 63.47% of the observations with a empty text\n* We have 35.23% of the observations with a start and end token result\n* We have the same distribution for YES and NO from the previous plot","c2693666":"* \"the\" word corresponds to 9.5% of the words in question_text column\n\n> Let\u00b4s check the next 20 words to check if we have some common topic.","34356df8":"# File Description\n\n* simplified-nq-train.jsonl - the training data, in newline-delimited JSON format.\n* simplified-nq-kaggle-test.jsonl - the test data, in newline-delimited JSON format.\n* sample_submission.csv - a sample submission file in the correct format","09ea6059":"# Long Answer Results\n* We have 50.16% of the observations empty\n* We have 49.84% of the observarions with a start and end token result","849aab2e":"# Work on progess!!...","00df325e":"# Preprocess and Model\n\n* In this section we are going to build a baseline model\n* First, we need to create a preprocess function to pass the data and get a training and testing set with the correct format.\n* The main idea of my preprocessing is making a train were each long answer candidate text is going to be a row, in other words we are going to use the indices of the annotation to extract from the document text the answer. Next we can use the extracted segment and the question as features and label a ground truth variable y with 1 and 0.\n* Short answer have 4 possible answers, we are going to transform it to a binary classification problem were YES and NO are going to be empty answers.\n* We have a nice sample dataset to try this preprocessing function, let's start coding."}}