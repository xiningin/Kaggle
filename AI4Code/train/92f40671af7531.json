{"cell_type":{"69846889":"code","fc151f5c":"code","176ae6fc":"code","19f6ef8d":"code","f4cb74bc":"code","0fc6b332":"code","1a51484e":"code","a2d600e3":"code","0b8f5e70":"code","76989568":"code","b34acbd3":"code","08a5e06d":"code","5064a9a7":"code","8db29892":"code","4493bc55":"code","b3c06534":"code","93495c70":"code","d32d2ae7":"code","99b7c8fd":"code","dac35f58":"code","29d846ac":"code","fe945fc9":"markdown","20650088":"markdown","ae6b4c53":"markdown","d7b712e6":"markdown","94f93014":"markdown","e15e3952":"markdown","24ff7cc8":"markdown","c0729e62":"markdown","05a2e4b0":"markdown"},"source":{"69846889":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc151f5c":"df= pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","176ae6fc":"df.head()","19f6ef8d":"df.tail()","f4cb74bc":"df.info()","0fc6b332":"num_cols= ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\nclass_cols= ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']","1a51484e":"plt.figure(figsize= (15,10))\nsns.heatmap(df.corr(),annot= True, cmap= 'coolwarm')","a2d600e3":"df.hist(figsize=(20,20))\nplt.show()","0b8f5e70":"plt.figure(figsize=(20,20))\n\nplt.subplot(4,2,1)\nsns.boxplot(x= df['output'], y=df['cp'] )\nplt.title('Chest Pain vs Output')\n\nplt.subplot(4,2,2)\nsns.boxplot(x= df['output'], y= df['thalachh'])\nplt.title('Maximum Heart Rate vs Output')\n\nplt.subplot(4,2,3)\nsns.boxplot(x= df['output'], y= df['oldpeak'])\nplt.title('Old Peak vs Output')\n\nplt.subplot(4,2,4)\nsns.boxplot(x= df['output'], y= df['exng'])\nplt.title('exercise induced angina vs Output')\n\nplt.subplot(4,2,5)\nsns.boxplot(x= df['output'], y= df['sex'])\nplt.title('Sex vs Output')\n\nplt.subplot(4,2,6)\nsns.boxplot(x= df['output'], y= df['age'])\nplt.title('Age vs Output')\n\nplt.subplot(4,2,7)\nsns.boxplot(x= df['output'], y= df['trtbps'])\nplt.title('Blood Pressure vs Output')\n\nplt.subplot(4,2,8)\nsns.boxplot(x= df['output'], y= df['slp'])\nplt.title('Slope vs Output')\n\n","76989568":"X= df.drop('output', axis=1)\ny= df['output']","b34acbd3":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV","08a5e06d":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= 0.2, stratify=y, random_state= 100)\n\nss= StandardScaler()\nX_train= ss.fit_transform(X_train)\nX_test= ss.transform(X_test)","5064a9a7":"key= ['LogisticRegression', 'DecisionTreeRegressor', 'DecisionTreeClassifier',  'RandomForestClassifier', 'KNeighborsClassifier', ]\n\nvalue= [LogisticRegression(), DecisionTreeRegressor() , DecisionTreeClassifier() ,  RandomForestClassifier() ,  KNeighborsClassifier() ]\nmodels= dict(zip(key, value))","8db29892":"models","4493bc55":"scores= []\nfor keys, value in models.items():\n    score= -1*cross_val_score(value, X, y,  cv=5, scoring= 'neg_mean_absolute_error' )\n    scores.append(score)\n    print(value, score.mean())\n    ","b3c06534":"accuracy_scores= []\nfor key, value in models.items():\n    value.fit(X_train, y_train)\n    y_pred= value.predict(X_test)\n    accuracy= accuracy_score(y_test, y_pred)\n    accuracy_scores.append(accuracy)\n    print(key, accuracy)","93495c70":"lr= LogisticRegression()\n\nlr.fit(X_train, y_train)\ny_pred= lr.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","d32d2ae7":"rfc= RandomForestClassifier(random_state= 50)\nrfc.fit(X_train, y_train)\ny_pred= rfc.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","99b7c8fd":"params= {'max_depth':np.arange(2,10,1),\n        'n_estimators': [10,100,200],\n        'max_features':[10,100,500],\n        }\n\ngrid= GridSearchCV(rfc, param_grid= params, cv= 5)\ngrid.fit(X_train, y_train)\n","dac35f58":"grid.best_params_","29d846ac":"grid.best_estimator_.fit(X_train, y_train)\ny_predcv= grid.best_estimator_.predict(X_test)\n\nprint(classification_report(y_test, y_predcv))\nprint(confusion_matrix(y_test, y_predcv))\n","fe945fc9":"# Creating Models","20650088":"* Heart Attack is highly positively co-related to Chest Pain(cp)\n* Heart Attack is positively co-related to Maximum Heart Rate achieved(thalachh)\n* Heart Attack is negatively co-related to OldPeak and EXNG\n* People between Age 50-70 more prone to heart attack\n* People with chest pain type 0 have highest risk of heart attack\n* People with cholestrol level 200-300 have very high risk\n* People with max heart rate 150-175 have high risk\n* People with thaal rate 2 and 3 have high risk","ae6b4c53":"**Dividing dataset into numeric and class dataset**","d7b712e6":"# Creating BoxPlot to check outliers in highly co-related data","94f93014":" **Logistic Regression and RandomForest Classifier are the best models to predict **","e15e3952":"**Checking the Distribution of Data**","24ff7cc8":"# Hence, we get an accuracy of 87% on test set with RandomForestClassifier","c0729e62":"**IT IS CLEAR THAT RANDOMFORESTCLASSIFIER IS BEST MODEL FOR THIS PROBLEM**","05a2e4b0":"# Now we do some Hyperparameter Tuning"}}