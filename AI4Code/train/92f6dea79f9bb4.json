{"cell_type":{"47e535b9":"code","78a81498":"code","0edd340e":"code","485a4564":"code","81ec45f9":"code","31272258":"code","29dc4fc1":"code","06374cf6":"code","5727049e":"code","512de9a8":"code","36b6c9c6":"code","d5d0e8df":"code","ae2f0131":"code","ade2448b":"code","01033c01":"code","c1a7f38c":"code","62a1d4af":"code","d0c45594":"code","c07c9540":"code","be9f5157":"code","7a468b67":"code","f0ecbaf5":"code","b732e3b6":"code","645e4e9b":"code","5a2bba74":"code","ce98e9d6":"code","83b618d6":"code","b90b89e9":"code","e3027dea":"code","c6906960":"code","a139551a":"code","f7dc7248":"code","cd6f1120":"code","a3a2ebed":"code","ad88691e":"code","a135e724":"code","e831967e":"code","06849188":"code","f2a70bd1":"code","d2b7260f":"code","11c8065b":"code","25655496":"code","784df53f":"code","1297c5c3":"code","03645cbb":"code","20da987b":"code","b73ed1df":"markdown","58079e59":"markdown","591f8888":"markdown","a97be754":"markdown","4ccc4cb6":"markdown","831e8dc9":"markdown","1de211f2":"markdown","464d1294":"markdown","aafe61ee":"markdown","ded2c9e1":"markdown","59ad1734":"markdown","2e433cdb":"markdown","01323595":"markdown","f2f8bfc7":"markdown","5a9c27ff":"markdown","7d5a6732":"markdown","aa1ae671":"markdown","3f7a9923":"markdown","daca02f3":"markdown","0fb27935":"markdown","e88a0311":"markdown","215686fe":"markdown","11f8ac1d":"markdown","4688720d":"markdown"},"source":{"47e535b9":"pip install tensorflow_datasets","78a81498":"!pip install -q git+https:\/\/github.com\/tensorflow\/examples.git","0edd340e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\nimport urllib\n\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n\n\n\nimport tensorflow_datasets as tfds\nimport PIL.Image\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12, 5)\nimport numpy as np\n\n\n\n\n\n\n# Any results you write to the current directory are saved as output.","485a4564":"\nIMAGE_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/images\/\"\nTEST_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/test.csv\"\nTRAIN_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/train.csv\"\nSUB_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\"\n\nsub = pd.read_csv(SUB_PATH)\ntest_data = pd.read_csv(TEST_PATH)\nplant_data = pd.read_csv(TRAIN_PATH)","81ec45f9":"\ndef format_path(st):\n    return IMAGE_PATH + st+'.jpg'\n\n#taking the file path of train and test images\ntrain_paths = plant_data.image_id.apply(format_path).values\ntest_paths =test_data.image_id.apply(format_path).values\n\n#Taking  the labels of train images\nlabels = np.float32(plant_data.loc[:, 'healthy':'scab'].values)\n\n#splitting int train and validation \ntrain_path,val_path,train_lab,val_label=train_test_split(train_paths,labels, test_size=0.3, random_state=42)","31272258":"IMG_SIZE = 160 # All images will be resized to 160x160\n#Resize the images to a fixed input size, and rescale the input channels to a range of [-1,1]\ndef aug_format_example(image, label=None):\n    \n    image = tf.io.read_file(image)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32)\n    image = (image\/255) \n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n\n   \n   \n    if label is None :\n        return image\n    else:\n        return image, label\n\ndef format_example(image, label=None):\n    \n    image = tf.io.read_file(image)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32)\n    image = (image\/255) \n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    if label is None :\n        return image\n    else:\n        return image, label\n","29dc4fc1":"BATCH_SIZE = 32\nSHUFFLE_BUFFER_SIZE = 500\n#Apply this function to each item in the dataset using the map method:\ntrain = tf.data.Dataset.from_tensor_slices((train_path, train_lab)).map(aug_format_example)\nval=tf.data.Dataset.from_tensor_slices((val_path, val_label)).map(format_example)\ntest=tf.data.Dataset.from_tensor_slices((test_paths)).map(format_example)","06374cf6":"#Now shuffle and batch the data.\ntrain_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\nvalidation_batches =val.batch(BATCH_SIZE)\ntest_batches=test.batch(BATCH_SIZE)","5727049e":"#Inspect a batch of data:\nfor image_batch, label_batch in train_batches.take(1):\n    pass\n\nimage_batch.shape","512de9a8":"IMG_SIZE=160\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n\n# Create the base model from the pre-trained model MobileNet V2\nbase_model =tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\n","36b6c9c6":"\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","d5d0e8df":"base_model.trainable = False","ae2f0131":"# Let's take a look at the base model architecture\nbase_model.summary()","ade2448b":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","01033c01":"prediction_layer = tf.keras.layers.Dense(4,activation='sigmoid')\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","c1a7f38c":"model = tf.keras.models.Sequential([base_model,\n                                 global_average_layer ,\n                            prediction_layer,\n                                ])","62a1d4af":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])","d0c45594":"model.summary()","c07c9540":"len(model.trainable_variables)","be9f5157":"initial_epochs = 10\nvalidation_steps=20\n\nloss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)","7a468b67":"print(\"initial loss: {:.2f}\".format(loss0))\nprint(\"initial accuracy: {:.2f}\".format(accuracy0))","f0ecbaf5":"# Define the checkpoint directory to store the checkpoints\n\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")","b732e3b6":"def decay(epochs):\n    if epochs < 3:\n        return 1e-3\n    elif epochs >= 3 and epochs < 7:\n        return 1e-4\n    else:\n        return 1e-5","645e4e9b":"# Callback for printing the LR at the end of each epoch.\nclass PrintLR(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n                                                      model.optimizer.lr.numpy()))","5a2bba74":"callbacks = [\n    \n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n                                       save_weights_only=True),\n    tf.keras.callbacks.LearningRateScheduler(decay),\n    PrintLR()\n]","ce98e9d6":"\nSTEPS_PER_EPOCH = labels.shape[0] \/\/ BATCH_SIZE\nhistory = model.fit(train_batches,\n                    epochs=initial_epochs,\n                    callbacks=callbacks,\n              \n                    validation_data=validation_batches)","83b618d6":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","b90b89e9":"base_model.trainable = True","e3027dea":"# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(base_model.layers))","c6906960":"# Fine-tune from this layer onwards\nfine_tune_at = 100\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","a139551a":"\nmodel.compile(optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate\/10),\n               loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])","f7dc7248":"model.summary()","cd6f1120":"len(model.trainable_variables)","a3a2ebed":"fine_tune_epochs = 10\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = model.fit(train_batches,\n                         epochs=total_epochs,\n                          callbacks=callbacks,\n                      \n                         initial_epoch =  history.epoch[-1],\n                         validation_data=validation_batches)","ad88691e":"acc += history_fine.history['accuracy']\nval_acc += history_fine.history['val_accuracy']\n\nloss += history_fine.history['loss']\nval_loss += history_fine.history['val_loss']","a135e724":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","e831967e":"\nx_val=[]\ny_val=[]\nfor image_batch, label_batch in validation_batches.take(1):\n    x_val.append(image_batch)\n    y_val.append(label_batch)\ny_val=np.array(y_val).reshape(32,4)    \n  ","06849188":"#predicting on validation input\nc=x_val\nY_pred = model.predict(c)\n\nY_pred\n","f2a70bd1":"for row in range(len(Y_pred)):\n    for col in range(4):\n        if Y_pred[row][col] == max(Y_pred[row]):\n            Y_pred[row][col] = 1\n        else:\n            Y_pred[row][col] = 0\n","d2b7260f":"#funtion to create confition _matrix\nimport sklearn\nimport itertools\nfrom sklearn.metrics import confusion_matrix\ndict_characters = {0: 'No', 1: 'Yes'}\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.figure(figsize = (5,5))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","11c8065b":"#Y_pred_classes = np.argmax(y_p,axis=0) \nconfusion_mtx = confusion_matrix(y_val[:,0],np.array(Y_pred)[:,0]) \nplot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values()))","25655496":"#Y_pred_classes = np.argmax(y_p,axis=0) \nconfusion_mtx = confusion_matrix(y_val[:,1],np.array(Y_pred)[:,1]) \nplot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values()))","784df53f":"#Y_pred_classes = np.argmax(y_p,axis=0) \nconfusion_mtx = confusion_matrix(y_val[:,2],np.array(Y_pred)[:,2]) \nplot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values()))","1297c5c3":"#Y_pred_classes = np.argmax(y_p,axis=0) \nconfusion_mtx = confusion_matrix(y_val[:,3],np.array(Y_pred)[:,3]) \nplot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values()))","03645cbb":"t=test_batches\nY_pred1 = model.predict(t)\n\nY_pred1\n\n\n","20da987b":"Y_pred1=pd.DataFrame(Y_pred1 ,columns=['healthy',\t'multiple_diseases','rust','scab'])\nY_pred1.to_csv('submission1.csv')","b73ed1df":"To understand how my model working on Validation Data","58079e59":"![image.png](attachment:image.png)","591f8888":"  ![image.png](attachment:image.png)\n","a97be754":"![image.png](attachment:image.png)","4ccc4cb6":"Here iam using a pretrained model (Mobilenet ) with fine tunning:\n\nA pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task.\n\nThe intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n\n\n**Feature Extraction:** Use the representations learned by a previous network to extract meaningful features from new samples. You simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for the dataset.\n\nYou do not need to (re)train the entire model. The base convolutional network already contains features that are generically useful for classifying pictures. However, the final, classification part of the pretrained model is specific to the original classification task, and subsequently specific to the set of classes on which the model was trained.\n\n**Fine-Tuning**: Unfreeze a few of the top layers of a frozen model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows us to \"fine-tune\" the higher-order feature representations in the base model in order to make them more relevant for the specific task","831e8dc9":"Feature extraction\nIn this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier.\n\nFreeze the convolutional base\nIt is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all the layers.","1de211f2":"*Healthy plant leaf*\n\n\n![image.png](attachment:image.png)    \n\n\n\n","464d1294":"To generate predictions from the block of features, average over the spatial 5x5 spatial locations, using a tf.keras.layers.GlobalAveragePooling2D layer to convert the features to a single 1280-element vector per image.","aafe61ee":"**Leaf which scab**\n![image.png](attachment:image.png)\n","ded2c9e1":"![image.png](attachment:image.png)","59ad1734":"**Leaf which having multiple disease**\n\n\n![image.png](attachment:image.png)","2e433cdb":"1. This feature extractor converts each 160x160x3 image into a 5x5x1280 block of features. See what it does to the example batch of images:","01323595":"![image.png](attachment:image.png)","f2f8bfc7":"**Label Distributions**\n\n\n\n![image.png](attachment:image.png)","5a9c27ff":"![image.png](attachment:image.png)","7d5a6732":"![image.png](attachment:image.png)","aa1ae671":"# **Fine tuning**\nIn the feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were not updated during training.\n\nOne way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.\n\n\nUn-freeze the top layers of the model\nAll you need to do is unfreeze the base_model and set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training.","3f7a9923":"![image.png](attachment:image.png)","daca02f3":"![image.png](attachment:image.png)","0fb27935":"![image.png](attachment:image.png)","e88a0311":"**Leaf  having rust**","215686fe":"![image.png](attachment:image.png)","11f8ac1d":"First, you need to pick which layer of MobileNet V2 you will use for feature extraction. The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final\/top layer.\n\nFirst, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.","4688720d":"![image.png](attachment:image.png)"}}