{"cell_type":{"aa8adb5d":"code","8c3559ac":"code","4a91f3c9":"code","62c76075":"code","1e863a1d":"code","0c08a09e":"code","c70a35bb":"code","0162b952":"code","5f32f787":"code","7844450e":"code","1ca924d3":"code","060af8de":"code","26560a72":"code","cb1edca6":"code","9ed27e9f":"code","17f90fdb":"code","76b38732":"code","3bfd1c80":"code","df28e722":"code","f137de88":"code","a12b3203":"code","ec8877ba":"code","83c74d4a":"code","45b27f99":"code","a8e28731":"code","d3b04860":"code","6474343e":"code","035e3264":"code","7d4bb898":"code","a11819bc":"code","3ca54e59":"markdown","ce3c5451":"markdown","238f5961":"markdown","e8df8d2a":"markdown","19e0b310":"markdown","5c4a37cf":"markdown","ba235e7e":"markdown","f6e18c31":"markdown","a885b67c":"markdown","04beb8ca":"markdown","25055492":"markdown","fe08495b":"markdown","8b4da252":"markdown","54ce0cc3":"markdown","c6f64b70":"markdown","759b95e8":"markdown","2ef3148c":"markdown","465621ab":"markdown","33ff9b2c":"markdown","e8200b31":"markdown"},"source":{"aa8adb5d":"#!pip install pycaret\nimport os \nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sb \nfrom pycaret.datasets import get_data\nfrom tpot import TPOTClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom pycaret.classification import *\nfrom sklearn.metrics import classification_report, accuracy_score,f1_score \nfrom sklearn.metrics import precision_score, recall_score,confusion_matrix \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nprint(os.listdir('..\/input\/creditcardfraud'))","8c3559ac":"# getting data \ndata=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","4a91f3c9":"data.info()","62c76075":"data.describe().transpose()","1e863a1d":"#separate data values per class \nfraud=data[data.Class==1]\nnonfraud=data[data.Class==0]\nper_fraud=round(len(fraud)\/len(data)*100,2)\nper_nonfraud=round(len(nonfraud)\/len(data)*100,2)\nprint(f'Percentage of Fraud transactions is : {per_fraud} %')\nprint('---------')\nprint(f'Percentage of Non Fraud transactions is : {per_nonfraud} %')","0c08a09e":"#Mention that : 1 = Fradulent and 0 = not Fradulent\ndata.Class.value_counts()","c70a35bb":"sb.set_style('whitegrid')\nsb.set(rc={'figure.figsize':(8,5)})\nsb.countplot(x='Class',data=data).set_title('Number of Transactions per Class')","0162b952":"print('Amount Details of Fraudulent Transactions :\\n',fraud.Amount.describe())\nprint('**********')\nprint('Amount Details of Valid Transactions :\\n',nonfraud.Amount.describe())\n","5f32f787":"# Correlation matrix \ncorrmatrix = data.corr() \nfig = plt.figure(figsize = (10, 8)) \nsb.heatmap(corrmatrix, vmax = .8, square = True) \nplt.show() ","7844450e":"# splitting data into  X features  and  y target \nX = data.drop(['Class'], axis = 1) \ny = data[\"Class\"] \nprint('X: Features || y:Class Labels: \\n')\nprint(X.shape,y.shape) \n# Converting to arrays   \nX = X.values \ny = y.values \n# Split the data into training set and testing set \nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state = 42) \nprint('After Splitting : \\n')\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","1ca924d3":"# Building the Random Forest Classifier (RANDOM FOREST) \nrfc = RandomForestClassifier() \nrfc.fit(X_train, y_train) \n# predictions \nrfc_preds = rfc.predict(X_test) ","060af8de":"#DEFINE A HELPER FUNCTION TO SHOW EVALUATION METRICS \ndef evaluate(y_test,preds):\n    print('****EVALUATIONS METRICS****')\n    accuracy = round(accuracy_score(y_test, preds),2)\n    print(\"Accuracy == {}\".format(accuracy)) \n    #precision\n    precision = round(precision_score(y_test, preds),2)\n    print(\"Precision == {}\".format(precision)) \n    #recall\n    recall = round(recall_score(y_test, preds),2)\n    print(\"Recall == {}\".format(recall)) \n    #F1 score\n    f1 = round(f1_score(y_test, rfc_preds),2)\n    print(\"F1-Score == {}\".format(f1)) ","26560a72":"# Show results on Random Forest Classifier \nevaluate(y_test,rfc_preds)   ","cb1edca6":"# printing the confusion matrix \nconf_matrix = confusion_matrix(y_test, rfc_preds) \nplt.figure(figsize =(8, 8)) \nsb.heatmap(conf_matrix, xticklabels =['Valid', 'Fraud'], \n           yticklabels = ['Valid', 'Fraud'], annot = True, fmt =\"d\")\nplt.title(\"Confusion matrix\") \nplt.ylabel('True Class') \nplt.xlabel('Predicted Class') \nplt.show() ","9ed27e9f":"print('Training Score :',rfc.score(X_train,y_train))\nprint('Testing Score :',rfc.score(X_test,y_test))","17f90fdb":"#let's shuffle the dataset before getting into undersampling \ndata=data.sample(frac=1)\n#separate fraud and non fraud transactions \nfraud=data[data.Class==1]\nnonfraud=data[data.Class==0][:500]\n# create a balanced datafrme of fraud and non fraud transactions and worn on it \nbalanced_df = pd.concat([fraud, nonfraud])\n# Shuffle dataframe rows\nbalanced_df= balanced_df.sample(frac=1, random_state=101)\n#show \nbalanced_df.head()","76b38732":"print(\"Before Undersampling :\\n\",data.Class.value_counts())\nprint('________-----________')\nprint(\"After Undersampling :\\n\",balanced_df.Class.value_counts())","3bfd1c80":"sb.countplot(data=balanced_df,x='Class')","df28e722":"balanced_df.shape","f137de88":"df=balanced_df.copy()\n# dividing the X  and the Y  from the balanced dataset \nX_balanced = df.drop(['Class'], axis = 1) \ny_balanced = df[\"Class\"] \nprint('X_balanced: Features || y_balanced:Class Labels: \\n')\nprint(X_balanced.shape,y_balanced.shape) \n# getting just the values for the sake of processing  \n# (its a numpy array with no columns) \nX_balanced = X_balanced.values \ny_balanced = y_balanced.values \n# Split the data into training and testing sets \nX_train, X_test, y_train, y_test = train_test_split( X_balanced,y_balanced , test_size = 0.2, random_state = 101) \nprint('\\n')\nprint('After Splitting : \\n')\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","a12b3203":"# Random Forest Classifier (RANDOM FOREST) \nrfc = RandomForestClassifier()\n# Decison Tree Classifier \ndt=DecisionTreeClassifier()\n# Logistic Regression Classifier\nlr=LogisticRegression()\n#put those alorithms into a dictionnary \nmodels={'Random Forest Classifier': rfc,'Decision Tree':dt,'Logistic Regression':lr}\npredictions={}\nfor name,model in models.items():\n    print('***********')\n    print(f'TRAINING {name} Model ')\n    model.fit(X_train,y_train)\n    print('FINISHED TRAINING \\n')\n    print(f'TRAIN SCORE : {model.score(X_train,y_train)}\\n')\n    print(f'TEST SCORE : {model.score(X_test,y_test)}\\n')\n    predictions[name] = model.predict(X_test) \n    print(f'DONE WITH {name} MODEL')\n","ec8877ba":"#evaluate the models \nrfc_preds=predictions['Random Forest Classifier']\ndt_preds=predictions['Decision Tree']\nlr_preds=predictions['Logistic Regression']\nfor model,preds in predictions.items():\n    print(f'{model} RESULTS :\\n')\n    evaluate(y_test,preds)\n    print('*************')  ","83c74d4a":"# Setup the dataset and preprocess \npycaret_clf=setup(data=balanced_df,target='Class',train_size=0.8)","45b27f99":"#let's compare models  \ncompare_models()","a8e28731":"# Create the XGBoost Classifier for prediction \nxgboost=create_model('xgboost')\n","d3b04860":"tuned_xgboost=tune_model(xgboost,n_iter=200)","6474343e":"#plot the learning curve \nplot_model(tuned_xgboost,plot='learning')","035e3264":"#plot the Confusion Matrix \nplot_model(tuned_xgboost,plot='confusion_matrix')","7d4bb898":"# plot the classification report \nplot_model(tuned_xgboost,plot='class_report')","a11819bc":"#plot the target prediction error \nplot_model(tuned_xgboost,plot='error')","3ca54e59":"### Import the Usual Suspects. ","ce3c5451":"* Comparing these results with the non tuned XGBoost model, we can notice that all the metrics have  increased, and the model is well tuned. ","238f5961":"### Comparing Results","e8df8d2a":"* Pycaret's setup() function performs some basic preprocessing tasks, like ignoring insigniicant variables such as  IDs and Date Columns, imputing the missing values, encoding the categorical variables, and splitting the dataset into the train-test split for the rest of the modeling steps. When you run the setup function, it will first confirm the data types, and then if you press enter, it will create the environment for you to go ahead. Here we will train our model on size of 80 % of the whole dataset and we will leave the rest 20 % for testing. ","19e0b310":"### Working with Pycaret Library ","5c4a37cf":"#### 3. Hyperparameter Tuning of XGBoost Model:","ba235e7e":"#### 2. Compare Models and get the Best of them : ","f6e18c31":"#### 1. Setup the Dataset","a885b67c":"* There is a high propability saying that the model is overfitting, due to unbalanced nature of the dataset. Now, let's try to balance data before getting to modeling section. ","04beb8ca":"* In this section, we will evaluate and compare the performance of standard machine learning models on our balanced dataset using compare_model() method.\n* By default, this function will evaluate models using 10-fold cross-validation, sort results by classification accuracy, and return the single best model to choose for prediction section.\n* Calling the compare_models() method will also report a table of results with all  the models that were evaluated and their performance.\n ","25055492":"* PyCaret is an open source, low-code auto machine learning library in Python that allows you to go from preparing your data to deploying your model within seconds in your choice of notebook environment. Which makes this library more productive and effecient to use. With less time spent coding, you and your team can now focus on business problems.\n* PyCaret is simple and easy to use machine learning library that will help you to perform end-to-end ML experiments with less lines of code. It allows you to do prototyping quickly and efficiently from your choice of notebook environment. For more documentation you can check the website: https:\/\/pycaret.org\n* To install Pycaret library go to command prompt and type : pip install pycaret Or !pip install pycaret on your kernel. ","fe08495b":"#### undersampling data \nOver sampling and under sampling are techniques used in data mining and data analytics to modify unequal data classes to create balanced data sets. If a class of data is the overrepresented majority class, under sampling may be used to balance it with the minority class. Under sampling is used when the amount of collected data is sufficient. Common methods of under sampling include cluster centroids and Tomek links, both of which target potential overlapping characteristics within the collected data sets to reduce the amount of majority data.","8b4da252":"### Working on Unbalanced Data","54ce0cc3":"### Working on Balanced Data.","c6f64b70":"### If you Find this Notebook Useful to you , please Upvote..\n### Thanks for Reading.. ","759b95e8":" V2 and V5 are negatively correlated with  Amount.\nWe also notice a correlation between V20 and Amount. \nThis gives us a deeper understanding of our data. ","2ef3148c":"* The tune_model() method we will perfom a random search and tune  the hyperparameters of our selected machine learning model (XGBoost Classifier in our case) to increase its performance on predicting. The funcyion takes many parameters such as number of iterations and k-folds. We will do the tuning over 200 iterations and 10 cross validation folds(default value). ","465621ab":"* AS WE CAN SE FROM THE PLOT ABOVE, OUR DATA IS HIGHLY UNBALANCED.","33ff9b2c":"#### 4. Visualizing Results ","e8200b31":"From the results table above we can see that XGBClassifier(xgboost) has the highest accuracy and Recall, and F1 score amongst all other models. Let's notice that Naive Bayes (nb) has also the highest Precision score. "}}