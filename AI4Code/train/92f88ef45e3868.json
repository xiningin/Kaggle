{"cell_type":{"45aa277d":"code","818fdd5c":"code","63bb1ff2":"code","da0a85d3":"code","3e3a41e7":"code","0e26a527":"code","a14b8a20":"code","43006c54":"code","fd6c6682":"code","f5ce5986":"code","e3117349":"code","aaeba687":"code","e0ee1566":"code","1d6aac1f":"code","fe3487fa":"code","84dcdb4a":"code","b1247236":"code","2467f0d9":"code","fae1d995":"markdown","09b0d420":"markdown","66f92e61":"markdown","7f4dbfda":"markdown","e7def089":"markdown","d5ae640e":"markdown","34c1b85f":"markdown","6675fc45":"markdown","863f9dd8":"markdown","f9c097a3":"markdown","ed5f3617":"markdown","98218320":"markdown","d6e2b823":"markdown","479d1580":"markdown","c2533137":"markdown"},"source":{"45aa277d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport math\n%matplotlib inline","818fdd5c":"final_data = pd.read_csv('..\/input\/football-data-top-5-leagues\/combined_data.csv')","63bb1ff2":"final_data.drop(final_data.columns[[0, 1, ]], axis=1, inplace=True) \ndf1 = final_data","da0a85d3":"X = final_data[['Match Excitement', 'Home Team Rating', 'Away Team Rating',\n       'Home Team Possession %', 'Away Team Possession %',\n       'Home Team Off Target Shots', 'Home Team On Target Shots',\n       'Home Team Total Shots', 'Home Team Blocked Shots', 'Home Team Corners',\n       'Home Team Throw Ins', 'Home Team Pass Success %',\n       'Home Team Aerials Won', 'Home Team Clearances', 'Home Team Fouls',\n       'Home Team Yellow Cards', 'Home Team Second Yellow Cards',\n       'Home Team Red Cards', 'Away Team Off Target Shots',\n       'Away Team On Target Shots', 'Away Team Total Shots',\n       'Away Team Blocked Shots', 'Away Team Corners', 'Away Team Throw Ins',\n       'Away Team Pass Success %', 'Away Team Aerials Won',\n       'Away Team Clearances', 'Away Team Fouls', 'Away Team Yellow Cards',\n       'Away Team Second Yellow Cards', 'Away Team Red Cards', 'year']]\ny = final_data['Home Points']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)","3e3a41e7":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\naccuracies = []\n\n# Code for finding the best value of n_neighbors\n# for i in range(0,1000,1):\n#     if(i!=0):\n#         model = KNeighborsClassifier(n_neighbors=i) # 103 is perfect\n#         model.fit(X_train, y_train)\n#         y_pred = model.predict(X_test)\n#     from sklearn import metrics\n# #     print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n#     if(i == 0):\n#         accuracies.append(0)\n#     else:\n#         accuracies.append(metrics.accuracy_score(y_test, y_pred))\n        \nmodel = KNeighborsClassifier(n_neighbors=103) # 103 is perfect\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint('Accuracy : ',metrics.accuracy_score(y_test, y_pred))\nprint('Confusion Matrix : ')\nprint(confusion_matrix(y_test, y_pred))","0e26a527":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(max_depth=6)\nmodel.fit(X_train,y_train)\npred = model.predict(X_test)\nprint('Accuracy : ',metrics.accuracy_score(y_test, pred))\nprint('Confusion Matrix : ')\nprint(confusion_matrix(y_test, pred))","a14b8a20":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 250, criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\nprint(f'Accuracy : {metrics.accuracy_score(y_test, predictions):.3f}')\nprint(confusion_matrix(y_test, predictions))","43006c54":"# import optuna\n# import sklearn\n# def objective(trial):\n#     data = X\n#     target = y\n#     n_estimators = trial.suggest_int('n_estimators', 2, 300)\n# #     max_depth = int(trial.suggest_loguniform('max_depth', 1, 32))\n#     clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators, criterion = 'entropy', random_state = 42)\n#     return sklearn.model_selection.cross_val_score(clf, data, target, \n#        n_jobs=-1, cv=3).mean()","fd6c6682":"# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=100)","f5ce5986":"# trial = study.best_trial\n# print('Accuracy: {}'.format(trial.value))","e3117349":"from sklearn.ensemble import GradientBoostingClassifier\ngb=GradientBoostingClassifier(n_estimators=400,learning_rate=0.1)\ngb.fit(X_train,y_train)\npred = gb.predict(X_test)\nprint(f'Accuracy : {metrics.accuracy_score(y_test, pred):.3f}')","aaeba687":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators' : [100,50,150,200],\n    'learning_rate' : [ 0.1,0.01,]\n}\n\ngrid = GridSearchCV(GradientBoostingClassifier(), param_grid, refit = True, verbose = 3,n_jobs=-1)\ngrid.fit(X_train, y_train) \n\nprint(grid.best_params_) \ngrid_predictions = grid.predict(X_test)\n\nprint(f'Accuracy : {metrics.accuracy_score(y_test, grid_predictions):.3f}')","e0ee1566":"fig = px.scatter(x = X.columns, y = grid.best_estimator_.feature_importances_)\nfig.show()","1d6aac1f":"import xgboost as xb\n!ignore warnings\n\nmodel = xb.XGBClassifier()\nmodel.fit(X_train,y_train)\npred = model.predict(X_test)\n\nprint(f'Accuracy : {metrics.accuracy_score(y_test, pred):.3f}')","fe3487fa":"import optuna\nimport sklearn\ndef objective(trial):\n    data = X\n    target = y\n    n_estimators = trial.suggest_int('n_estimators', 2, 401)\n#     max_depth = int(trial.suggest_loguniform('max_depth', 1, 32))\n    learning_rate =  trial.suggest_categorical('learning_rate', [0.01,0.1, 0.05])\n    clf = xb.XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate,  tree_method='gpu_hist', predictor='gpu_predictor')\n    return sklearn.model_selection.cross_val_score(clf, data, target, \n       n_jobs=-1, cv=3).mean()","84dcdb4a":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)","b1247236":"trial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))","2467f0d9":"fig = px.scatter(x = X.columns, y = gb.feature_importances_)\nfig.show()","fae1d995":"# Data loading","09b0d420":"## Random Forest Vs Decision Tree\n\nRandom Forest is suitable for situations when we have a large dataset, and interpretability is not a major concern.\nDecision trees are much easier to interpret and understand. Since a random forest combines multiple decision trees, it becomes more difficult to interpret. ","66f92e61":"## Grid Search CV","7f4dbfda":"## Conclusion\n\nBest Classifier for this particular data is Gradient Boosting Classifier.\n\nThe three methods are similar, with a significant amount of overlap. \nIn a nutshell:\n\n* A decision tree is a simple, decision making-diagram.\n* Random forests are a large number of trees, combined (using averages or \"majority rules\") at the end of the process.\n* Gradient boosting machines also combine decision trees, but start the combining process at the beginning, instead of at the end\n\nFinally, the match winning preiction depends upon team rating and team shots","e7def089":"# KNN Classification","d5ae640e":"# Gradient Boosting Classifier","34c1b85f":"# Optuna Hyperparameter Tuning","6675fc45":"# Decision Tree","863f9dd8":"# Preparing Data","f9c097a3":"### Feature Importance","ed5f3617":"## Feature Importance","98218320":"# Import Statements","d6e2b823":"# Random Forest Classifier","479d1580":"# Optuna Hyperparameter Tuning","c2533137":"# XG Boost Classifier"}}