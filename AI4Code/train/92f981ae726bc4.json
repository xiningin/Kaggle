{"cell_type":{"69f53240":"code","2bb672a9":"code","2b705e8e":"code","66d2ab7e":"code","3a8546ee":"code","bcbcef11":"code","71748db7":"code","f879271b":"code","f96643a1":"code","3471652a":"markdown"},"source":{"69f53240":"import gc\nimport os\nimport random\nfrom contextlib import contextmanager\nfrom glob import glob\nfrom pathlib import Path\nfrom time import time\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import layers as L\nfrom tqdm.notebook import tqdm","2bb672a9":"ll ..\/input","2b705e8e":"DEBUG = True # if False, make sure gpu ON","66d2ab7e":"ROOT = Path.cwd().parent\nINPUT_ROOT = ROOT \/ \"input\"\nORG_ROOT = ROOT \/ \"input\" \/ \"stanford-covid-vaccine-new-sequences-augmentation\" \/ \"original\"\nOUTPUT_ROOT = ROOT \/ \"working\" \/ \"output\" \nOUTPUT_ROOT.mkdir(exist_ok=True)\nAUG_ROOTS = [\n    INPUT_ROOT \/ \"stanford-covid-vaccine-new-sequences-augmentation\" \/ \"contrafold\",\n    INPUT_ROOT \/ \"stanford-covid-vaccine-new-sequences-augmentation\" \/ \"vienna_2\",\n    INPUT_ROOT \/ \"stanford-covid-vaccine-new-sequences-augmentation\" \/ \"rnasoft\",\n]\n\nMODEL_ROOT = INPUT_ROOT \/ \"stanford-covid-vaccine-onodera-models\"\n","3a8546ee":"COLS_TARGET = [\"reactivity\", \"deg_Mg_pH10\", \"deg_pH10\", \"deg_Mg_50C\", \"deg_50C\"]\nCOLS_EVALUATE = [\"reactivity\", \"deg_Mg_pH10\", \"deg_pH10\", \"deg_Mg_50C\", \"deg_50C\"]\n\n\nPARAMS_FEATURE = [\n    # expXXX, [loop, structure, cat_channel]\n    [\"exp201\", [1, 0, 0]],\n    [\"exp202\", [1, 0, 0]],\n    [\"exp203\", [1, 1, 0]],\n    [\"exp204\", [1, 1, 1]],\n    [\"exp205\", [0, 1, 0]],\n    [\"exp206\", [0, 1, 1]],\n    [\"exp207\", [1, 0, 1]],\n    [\"exp208\", [1, 0, 1]],\n    [\"exp209\", [1, 1, 0]],\n    [\"exp210\", [1, 1, 0]],\n    [\"exp211\", [1, 1, 0]],\n    [\"exp212\", [1, 1, 0]],\n]\n\ntest = pd.read_json(ORG_ROOT \/ \"dataset.json\", lines=True)\n\n# replace and concat\ncols = [\"sequence\", \"structure\", \"predicted_loop_type\"]\nli_te = []\nfor aug in AUG_ROOTS:\n    df = pd.read_json(aug \/ \"dataset.json\", lines=True)\n    li_te.append(\n        pd.merge(test.drop(cols, axis=1), df[[\"id\"] + cols], how=\"left\", on=\"id\")\n    )\n\ntest = pd.concat([test] + li_te, ignore_index=True)\n\nif test[cols].isnull().sum().sum() == 0:\n    print(\"test.shape:\", test.shape)\nelse:\n    raise Exception(\"Merge was not success\")\n\n","bcbcef11":"# =============================================================================\n# def\n# =============================================================================\ndef get_structure_adj(test, ixs):\n    Ss = []\n    for i in ixs:\n        seq_length = test[\"seq_length\"].iloc[i]\n        structure = test[\"structure\"].iloc[i]\n        sequence = test[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = {\n            (\"A\", \"U\"): np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\"): np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\"): np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\"): np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\"): np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\"): np.zeros([seq_length, seq_length]),\n        }\n        a_structure = np.zeros([seq_length, seq_length])\n        for i in range(seq_length):\n            if structure[i] == \"(\":\n                cue.append(i)\n            elif structure[i] == \")\":\n                start = cue.pop()\n                #                 a_structure[start, i] = 1\n                #                 a_structure[i, start] = 1\n                a_structures[(sequence[start], sequence[i])][start, i] = 1\n                a_structures[(sequence[i], sequence[start])][i, start] = 1\n\n        a_strc = np.stack([a for a in a_structures.values()], axis=2)\n        a_strc = np.sum(a_strc, axis=2, keepdims=True)\n        Ss.append(a_strc)\n\n    Ss = np.array(Ss)\n    # print(Ss.shape)\n    return Ss\n\n\ndef get_distance_matrix(As):\n    idx = np.arange(As.shape[1])\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1 \/ Ds\n    Ds = Ds[None, :, :]\n    Ds = np.repeat(Ds, len(As), axis=0)\n\n    Dss = []\n    for i in [1, 2, 4]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis=3)\n    # print(Ds.shape)\n    return Ds\n\n\n## sequence\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\n\ndef get_input(test, ixs, codes):\n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_seq = np.stack(\n        test.iloc[ixs][\"sequence\"].apply(\n            lambda x: list(map(lambda y: mapping[y], list(x)))\n        )\n    )\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(\n        test.iloc[ixs][\"predicted_loop_type\"].apply(\n            lambda x: list(map(lambda y: mapping[y], list(x)))\n        )\n    )\n\n    mapping = {}\n    vocab = [\".\", \"(\", \")\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_structure = np.stack(\n        test.iloc[ixs][\"structure\"].apply(\n            lambda x: list(map(lambda y: mapping[y], list(x)))\n        )\n    )\n\n    li = [X_seq]\n    if codes[0] == 1:\n        li.append(X_loop)\n    if codes[1] == 1:\n        li.append(X_structure)\n    X_node = np.concatenate(li, axis=2)\n\n    ## interaction\n    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis=2)\n\n    if codes[:2] == [1, 0]:\n        vocab = [17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136,\n                 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n    elif codes[:2] == [1, 1]:\n        vocab = [2081, 2082, 2084, 2088, 2113, 2114, 2116, 2120, 2177, 2178, 2180,\n                 2184, 2305, 2306, 2308, 2312, 2561, 2562, 2564, 2568, 3073, 3074,\n                 3076, 3080, 4113, 4114, 4116, 4120, 8209, 8210, 8212, 8216]\n    elif codes[:2] == [0, 1]:\n        vocab = [17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72]\n\n    ohes = []\n    for v in vocab:\n        ohes.append(a == v)\n    ohes = np.stack(ohes, axis=2)\n    X_node = np.concatenate([X_node, ohes], axis=2).astype(np.float32)\n\n    # print(X_node.shape)\n    return X_node\n\n\ndef attention(x_inner, x_outer, n_factor, dropout):\n    x_Q = L.Conv1D(\n        n_factor,\n        1,\n        activation=\"linear\",\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"glorot_uniform\",\n    )(x_inner)\n    x_K = L.Conv1D(\n        n_factor,\n        1,\n        activation=\"linear\",\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"glorot_uniform\",\n    )(x_outer)\n    x_V = L.Conv1D(\n        n_factor,\n        1,\n        activation=\"linear\",\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"glorot_uniform\",\n    )(x_outer)\n    x_KT = L.Permute((2, 1))(x_K)\n    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) \/ np.sqrt(n_factor))([x_Q, x_KT])\n    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n    return att\n\n\ndef multi_head_attention(x, y, n_factor, n_head, dropout):\n    if n_head == 1:\n        att = attention(x, y, n_factor, dropout)\n    else:\n        n_factor_head = n_factor \/\/ n_head\n        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n        att = L.Concatenate()(heads)\n        att = L.Dense(\n            n_factor,\n            kernel_initializer=\"glorot_uniform\",\n            bias_initializer=\"glorot_uniform\",\n        )(att)\n    x = L.Add()([x, att])\n    x = L.LayerNormalization()(x)\n    if dropout > 0:\n        x = L.Dropout(dropout)(x)\n    return x\n\n\ndef res(x, unit, kernel=3, rate=0.1):\n    h = L.Conv1D(unit, kernel, 1, padding=\"same\", activation=None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.LeakyReLU()(h)\n    h = L.Dropout(rate)(h)\n    return L.Add()([x, h])\n\n\ndef forward(x, unit, kernel=3, rate=0.1):\n    #     h = L.Dense(unit, None)(x)\n    h = L.Conv1D(unit, kernel, 1, padding=\"same\", activation=None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.Dropout(rate)(h)\n    #         h = tf.keras.activations.swish(h)\n    h = L.LeakyReLU()(h)\n    h = res(h, unit, kernel, rate)\n    return h\n\n\ndef adj_attn(x, adj, unit, n=2, rate=0.1):\n    x_a = x\n    x_as = []\n    for i in range(n):\n        x_a = forward(x_a, unit)\n        x_a = tf.matmul(adj, x_a)\n        x_as.append(x_a)\n    if n == 1:\n        x_a = x_as[0]\n    else:\n        x_a = L.Concatenate()(x_as)\n    x_a = forward(x_a, unit)\n    return x_a\n\n\ndef get_base(config):\n    node = tf.keras.Input(shape=(None, X_CHANNEL), name=\"node\")\n    adj = tf.keras.Input(shape=(None, None, A_CHANNEL), name=\"adj\")\n    # node = tf.keras.Input(shape=(None, None), name=\"node\")\n    # adj = tf.keras.Input(shape=(None, None, None), name=\"adj\")\n\n    adj_learned = L.Dense(1, \"relu\")(adj)\n    adj_all = L.Concatenate(axis=3)([adj, adj_learned])\n\n    xs = []\n    xs.append(node)\n    x1 = forward(node, 128, kernel=3, rate=0.0)\n    x2 = forward(x1, 64, kernel=6, rate=0.0)\n    x3 = forward(x2, 32, kernel=15, rate=0.0)\n    x4 = forward(x3, 16, kernel=30, rate=0.0)\n    x = L.Concatenate()([x1, x2, x3, x4])\n\n    for unit in [64, 32]:\n        x_as = []\n        for i in range(adj_all.shape[3]):\n            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate=0.0)\n            x_as.append(x_a)\n        x_c = forward(x, unit, kernel=30)\n\n        x = L.Concatenate()(x_as + [x_c])\n        x = forward(x, unit)\n        x = multi_head_attention(x, x, unit, 4, 0.0)\n        xs.append(x)\n\n    x = L.Concatenate()(xs)\n\n    model = tf.keras.Model(inputs=[node, adj], outputs=[x])\n    return model\n\n\ndef get_ae_model(base, config):\n    node = tf.keras.Input(shape=(None, X_CHANNEL), name=\"node\")\n    adj = tf.keras.Input(shape=(None, None, A_CHANNEL), name=\"adj\")\n    # node = tf.keras.Input(shape=(None, None), name=\"node\")\n    # adj = tf.keras.Input(shape=(None, None, None), name=\"adj\")\n\n    x = base([L.SpatialDropout1D(0.3)(node), adj])\n    x = forward(x, 64, rate=0.3)\n    p = L.Dense(X_CHANNEL, \"sigmoid\")(x)\n\n    loss = -tf.reduce_mean(\n        20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4)\n    )\n    model = tf.keras.Model(inputs=[node, adj], outputs=[loss])\n\n    opt = get_optimizer()\n    model.compile(optimizer=opt, loss=lambda t, y: y)\n    return model\n\n\ndef get_model(base, config):\n    node = tf.keras.Input(shape=(None, X_CHANNEL), name=\"node\")\n    adj = tf.keras.Input(shape=(None, None, A_CHANNEL), name=\"adj\")\n    # node = tf.keras.Input(shape=(None, None), name=\"node\")\n    # adj = tf.keras.Input(shape=(None, None, None), name=\"adj\")\n\n    x = base([node, adj])\n    x = forward(x, 128, rate=0.4)\n    x = L.Dense(len(COLS_EVALUATE), None)(x)\n\n    model = tf.keras.Model(inputs=[node, adj], outputs=[x])\n\n    opt = get_optimizer()\n    model.compile(optimizer=opt, loss=lambda t, y: y)\n    return model\n\n\ndef get_optimizer():\n    adam = tf.optimizers.Adam()\n    return adam\n\n\ndef get_subformat(df: pd.DataFrame, length):\n    df_id = df.reset_index(drop=True)\n    df_id[\"key\"] = 0\n    df_index = pd.DataFrame(range(length), columns=[\"index\"])\n    df_index[\"key\"] = 0\n    df_id = df_id.merge(df_index, how=\"outer\")\n    df_id[\"id_seqpos\"] = df_id[\"id\"] + \"_\" + df_id[\"index\"].map(str)\n    del df_id[\"key\"]\n    return df_id\n\n\ndef get_feature(ixs, params):\n\n    exp = params[0]\n    codes = params[1]\n\n    # ixs = [0, 233, 466, 699]\n    As = []\n    ix = ixs[0]\n    As.append(np.load(ORG_ROOT \/ f\"bpps\/{test.iloc[ix]['id']}.npy\"))\n    for aug in AUG_ROOTS:\n        As.append(np.load(aug \/ f\"bpps\/{test.iloc[ix]['id']}.npy\"))\n    As = np.array(As)\n\n    Ss = get_structure_adj(test, ixs)\n    Ds = get_distance_matrix(As)\n\n    # concat adjecent\n    As = np.concatenate([As[:, :, :, None], Ss, Ds], axis=3).astype(np.float32)\n    del Ss, Ds\n\n    X_node = get_input(test, ixs, codes).astype(np.uint8)\n\n    if codes[2]:\n        a = len(AUG_ROOTS) + 1\n\n        length = int(len(X_node) \/ a)\n        X_node = np.concatenate(\n            [X_node[length * i : length * (i + 1)] for i in range(a)], axis=-1\n        )\n\n        length = int(len(As) \/ a)\n        As = np.concatenate(\n            [As[length * i : length * (i + 1)] for i in range(a)], axis=-1\n        )\n\n    return X_node, As\n\n\n@contextmanager\ndef timer(name):\n    t0 = time()\n    yield\n    print(f\"[{name}] done in {time() - t0:.0f} s\")","71748db7":"# =============================================================================\n# main\n# =============================================================================\n\n\norg_len = len(test) \/ (len(AUG_ROOTS) + 1)\nindex_list = [\n    list(map(int, np.arange(org_len) + (org_len * i))) for i in range(len(AUG_ROOTS) + 1)\n]\nindex_list = list(zip(*index_list))\n\n\nfor params in PARAMS_FEATURE:\n\n    exp = params[0]\n    codes = params[1]\n\n    with timer(exp):\n        fe_path = OUTPUT_ROOT \/ \"\".join(map(str, params[1]))\n        fe_path.mkdir(exist_ok=True)\n        sub_path = OUTPUT_ROOT \/ f\"{exp}.csv\"\n        if sub_path.exists():\n            print(f\"[{exp}] Done\")\n            continue\n\n        X_node, As = get_feature(list(index_list[0]), params)\n        X_CHANNEL = X_node.shape[2]\n        A_CHANNEL = As.shape[3]\n        base = get_base({})\n        model = get_model(base, {})\n\n        sub = pd.read_csv(INPUT_ROOT \/ \"stanford-covid-vaccine\" \/ \"post_deadline_files\" \/ \"new_sequences_submission.csv\").set_index('id_seqpos')\n\n        for fold in range(8):\n            model.load_weights(MODEL_ROOT \/ exp \/ f\"model{fold}\")\n            print('loaded', fold)\n            df_rows = []\n            for ixs in tqdm(index_list, total=org_len):\n\n                ixs = list(ixs)\n                id_ = ixs[0]\n\n                if (fe_path \/ f\"{id_}_X.npy\").exists() and (fe_path \/ f\"{id_}_A.npy\").exists():\n                    X_node = np.load(fe_path \/ f\"{id_}_X.npy\")\n                    As = np.load(fe_path \/ f\"{id_}_A.npy\")\n                else:\n                    X_node, As = get_feature(ixs, params)\n                    np.save(fe_path \/ f\"{id_}_X.npy\", X_node)\n                    np.save(fe_path \/ f\"{id_}_A.npy\", As)\n\n                pred = model.predict([X_node, As])\n                df = pd.DataFrame(np.array(pred).mean(0),\n                                  columns=COLS_EVALUATE)\n                df[\"id_seqpos\"] = list(map(lambda x: f\"{id_}_{x}\", range(len(df))))\n                df_rows.append(df.set_index('id_seqpos'))\n                \n                if DEBUG:\n                    break\n                \n            df_rows = pd.concat(df_rows)\n            sub += df_rows\n        \n            if DEBUG:\n                break\n\n        sub \/= 8\n        sub.to_csv(sub_path)\n        \n        if DEBUG:\n            break","f879271b":"ll output","f96643a1":"pd.read_csv(OUTPUT_ROOT \/ 'exp201.csv')","3471652a":"### output is [here](https:\/\/www.kaggle.com\/onodera\/covid-233-onodera-outputs-v2)"}}