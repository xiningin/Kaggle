{"cell_type":{"ef643eb2":"code","9413d8c4":"code","c6fc3fd3":"code","78b0c957":"code","c9833d66":"code","c09fde9c":"code","a07ba6da":"code","5d98ef03":"code","d19830b7":"code","117edff4":"code","61c4cd94":"code","eb447b55":"code","ac8a19c8":"code","b8c6829c":"code","fb2cc3aa":"code","26575538":"code","f79a907c":"code","276a426f":"code","ec8d4815":"code","5d6c0991":"code","aba061c1":"code","02214f89":"code","d6727b74":"code","aa8a1140":"code","b8231ddf":"code","0a5ee68c":"code","e3cfa979":"code","251b4dc3":"code","40f2cd89":"markdown","9ea63812":"markdown","b22160cc":"markdown","445c7b39":"markdown","9e1ca259":"markdown","25a3376d":"markdown","100b66ff":"markdown","d879214d":"markdown","fd053d74":"markdown","ef3f2dc8":"markdown"},"source":{"ef643eb2":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom time import time\nimport seaborn as sns\nimport os\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nfrom sklearn.metrics import accuracy_score\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, Activation\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\n%matplotlib inline","9413d8c4":"toy=False","c6fc3fd3":"# This competition settings\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","78b0c957":"(market_train_df, news_train_df) = env.get_training_data()","c9833d66":"market_train_df.shape","c09fde9c":"news_train_df.shape","a07ba6da":"# We will reduce the number of samples for memory reasons\nif toy:\n    market_train_df = market_train_df.tail(100_000)\n    news_train_df = news_train_df.tail(300_000)\nelse:\n    market_train_df = market_train_df.tail(750_000)\n    news_train_df = news_train_df.tail(1500_000)","5d98ef03":"news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","d19830b7":"def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n    gc.collect()\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n    gc.collect()\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    gc.collect()\n    \n    return market_train_df","117edff4":"market_df = join_market_news(market_train_df, news_train_df)","61c4cd94":"market_df.tail()","eb447b55":"market_df.shape","ac8a19c8":"market_df.columns","b8c6829c":"    # Features\n    cat_cols = ['assetCode']\n    time_cols=['year', 'week', 'day', 'dayofweek']\n    mkt_numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n    \n    news_numeric_cols = [\n#        'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n#        'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n#        'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n#        'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n#        'returnsOpenNextMktres10', 'universe', 'urgency_min', 'urgency_count',\n#        'takeSequence_max', 'bodySize_min', 'bodySize_max', 'bodySize_mean',\n#        'bodySize_std', 'wordCount_min', 'wordCount_max', 'wordCount_mean',\n#        'wordCount_std', 'sentenceCount_min', 'sentenceCount_max',\n#        'sentenceCount_mean', 'sentenceCount_std', 'companyCount_min',\n#        'companyCount_max', 'companyCount_mean', 'companyCount_std',\n#        'marketCommentary_min', 'marketCommentary_max', 'marketCommentary_mean',\n#        'marketCommentary_std', \n        'relevance_min', 'relevance_max',\n        'relevance_mean', 'relevance_std', 'sentimentNegative_min',\n        'sentimentNegative_max', 'sentimentNegative_mean',\n        'sentimentNegative_std', 'sentimentNeutral_min', 'sentimentNeutral_max',\n        'sentimentNeutral_mean', 'sentimentNeutral_std',\n        'sentimentPositive_min', 'sentimentPositive_max',\n        'sentimentPositive_mean', 'sentimentPositive_std',\n        'sentimentWordCount_min', 'sentimentWordCount_max',\n        'sentimentWordCount_mean', 'sentimentWordCount_std',\n        'noveltyCount12H_min', 'noveltyCount12H_max', 'noveltyCount12H_mean',\n        'noveltyCount12H_std', 'noveltyCount24H_min', 'noveltyCount24H_max',\n        'noveltyCount24H_mean', 'noveltyCount24H_std', 'noveltyCount3D_min',\n        'noveltyCount3D_max', 'noveltyCount3D_mean', 'noveltyCount3D_std',\n        'noveltyCount5D_min', 'noveltyCount5D_max', 'noveltyCount5D_mean',\n        'noveltyCount5D_std', 'noveltyCount7D_min', 'noveltyCount7D_max',\n        'noveltyCount7D_mean', 'noveltyCount7D_std']\n#         'volumeCounts12H_min',\n#        'volumeCounts12H_max', 'volumeCounts12H_mean', 'volumeCounts12H_std',\n#        'volumeCounts24H_min', 'volumeCounts24H_max', 'volumeCounts24H_mean',\n#        'volumeCounts24H_std', 'volumeCounts3D_min', 'volumeCounts3D_max',\n#        'volumeCounts3D_mean', 'volumeCounts3D_std', 'volumeCounts5D_min',\n#        'volumeCounts5D_max', 'volumeCounts5D_mean', 'volumeCounts5D_std',\n#        'volumeCounts7D_min', 'volumeCounts7D_max', 'volumeCounts7D_mean',\n#        'volumeCounts7D_std']\n    \n    numeric_cols = mkt_numeric_cols + news_numeric_cols\n    \n    feature_cols = cat_cols + time_cols + numeric_cols\n    \n    # Labels\n    label_cols = ['returnsOpenNextMktres10']","fb2cc3aa":"print (np.unique(feature_cols).shape)\nprint (len(feature_cols))\nprint (numeric_cols)","26575538":"# Split to train, validation and test.\n# ToDo: remove shuffle, use generator.\n#market_train_df, market_test_df = train_test_split(market[market.time > '2009'].sample(100000, random_state=42), shuffle=True, random_state=24)\nmarket_train_df, market_test_df = train_test_split(market_df, shuffle=True, random_state=24)\nmarket_train_df, market_val_df = train_test_split(market_train_df, shuffle=True, random_state=24)\n\n# Look at min\/max and quantiles\nmarket_train_df.describe()","f79a907c":"class Prepro:\n    \"\"\"\n    Bring all preprocessing here: scale, encoding\n    Should be fit on train data and called on train, validation or test data\n    \"\"\"\n    \n    def __init__(self, feature_cols, cat_cols, time_cols, numeric_cols, label_cols):\n        self.feature_cols = feature_cols\n        self.cat_cols = cat_cols\n        self.time_cols = time_cols\n        self.numeric_cols = numeric_cols\n        self.label_cols = label_cols\n        self.cats={}\n    \n    def transformXy(self, df):\n        \"\"\"\n        Preprocess and return X,y\n        \"\"\"\n        df = df.copy()\n        # Scale, encode etc. features\n        X = self.transform(df)\n        # Scale labels\n        df[self.label_cols] = self.y_scaler.transform(df[self.label_cols])\n        y = df[self.label_cols]\n        return(X,y)\n    \n    def transform(self, df):\n        \"\"\"\n        Preprocess and return X\n        \"\"\"\n        # Add day, week, year\n        df = self.prepare_time_cols(df)\n        # Fill nans\n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        # Preprocess categorical features\n        for col in cat_cols:\n            df[col] = df[col].apply(lambda cat_name: self.prepare_cat_cols(cat_name, col))\n        # Scale numeric features and labels\n        df[self.numeric_cols+self.time_cols] = self.numeric_scaler.transform(df[self.numeric_cols+self.time_cols])\n        # Return X\n        return df[self.feature_cols]\n    \n    def fit(self, df):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df\n        To be called on train df only\n        \"\"\"\n        # Extract day, week, year from time\n        df = df.copy()\n        df = self.prepare_time_cols(df)\n        # Handle strange cases, impute etc.\n        df = self.prepare_train_df(df)\n        # Use QuantileTransformer to handle outliers\n        # Fit for labels\n        self.y_scaler = QuantileTransformer()\n        self.y_scaler.fit(df[self.label_cols])\n        # Fit for numeric and time\n        self.numeric_scaler = QuantileTransformer()\n        self.numeric_scaler.fit(df[self.numeric_cols + self.time_cols])\n        # Fit for categories\n        # Organize dictionary, each category column has list with values\n        self.cats=dict()\n        for col in cat_cols:\n            self.cats[col] = list(df[col].unique())\n\n    def prepare_train_df(self, train_df):\n        \"\"\"\n        Clean na, remove strange cases.\n        For train dataset only. \n        \"\"\"\n        # Handle nans\n        train_df = train_df.copy()\n        # Need better imputer\n        # for col in numeric_cols:\n        #     market_train_df[col] = market_train_df[col].fillna(market_train_df[col].mean())\n        train_df.tail()\n        train_df[self.numeric_cols] = train_df[self.numeric_cols].fillna(0)\n\n        # # Remove strange cases with close\/open ratio > 2\n        max_ratio  = 2\n        train_df = train_df[np.abs(train_df['close'] \/ train_df['open']) <= max_ratio]\n        return(train_df)\n\n    def prepare_time_cols(self, df):\n        \"\"\" \n        Extract time parts, they are important for time series \n        \"\"\"\n        df = df.copy()\n        df['year'] = df['time'].dt.year\n        # Maybe remove month because week of year can handle the same info\n        df['day'] = df['time'].dt.day\n        # Week of year\n        df['week'] = df['time'].dt.week\n        df['dayofweek'] = df['time'].dt.dayofweek\n        return(df)\n        \n    def prepare_cat_cols(self, cat_name, col):\n        \"\"\"\n        Encode categorical features to numbers\n        \"\"\"\n        try:\n            # Transform to index of name in stored names list\n            index_value = self.cats[col].index(cat_name)\n        except ValueError:\n            # If new value, add it to the list and return new index\n            self.cats[col].append(cat_name)\n            index_value = len(self.cats[col])\n        index_value = 1.0\/(index_value+1.0)\n        return(index_value)\n","276a426f":"# Preprocess and split to X_train, X_val, X_test, y_train ...\nprepro = Prepro(feature_cols, cat_cols, time_cols, numeric_cols, label_cols)\nprepro.fit(market_train_df)\n\n# Clean train df,handle strange cases\nmarket_train_df = prepro.prepare_train_df(market_train_df)\n\nX_train, y_train = prepro.transformXy(market_train_df)\nX_val, y_val = prepro.transformXy(market_val_df)\nX_test, y_test = prepro.transformXy(market_test_df)\n\n# Display for visual check. \npd.concat([X_train,y_train], axis=1).describe()\n#X_train.head()","ec8d4815":"# Initialize the constructor\nmodel = Sequential()\n\n# Add an input layer \ninput_size = X_train.shape[1]\n\n# Add layers - no worries which are.\n# ToDo: find a good architecture of NN\nmodel.add(Dense(256, input_shape=(input_size,), kernel_initializer='glorot_normal'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\n# Add an output layer \nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])\nmodel.summary()","5d6c0991":"weights_file='best_weights.h5'\n\n# We'll stop training if no improvement after some epochs\nearlystopper = EarlyStopping(patience=5, verbose=1)\n\n# Low, avg and high scor training will be saved here\n# Save the best model during the traning\ncheckpointer = ModelCheckpoint(weights_file\n    ,verbose=1\n    ,save_best_only=True\n    ,save_weights_only=True)\n\nreduce_lr = ReduceLROnPlateau(factor=0.2,\n                              patience=5, min_lr=0.001)\n\n# Train\ntraining = model.fit(X_train,y_train\n                                ,batch_size=512\n                                ,epochs=100\n                                ,validation_data=[X_val, y_val]\n                                #,steps_per_epoch=100\n                                 #, validation_steps=100\n                                ,callbacks=[earlystopper, checkpointer, reduce_lr])\n# Load best weights saved\nmodel.load_weights(weights_file)","aba061c1":"# f, axs = plt.subplots(1,2, figsize=(12,4))\n# axs[0].plot(training.history['loss'])\n# axs[0].set_xlabel(\"Epoch\")\n# axs[0].set_ylabel(\"Loss\")\n# axs[0].set_title(\"Loss\")\n# axs[1].plot(training.history['val_loss'])\n# axs[1].set_xlabel(\"Epoch\")\n# axs[1].set_ylabel(\"val_loss\")\n# axs[1].set_title(\"Validation loss\")\n# plt.tight_layout()\n# plt.show()\nplt.plot(training.history['loss'])\nplt.plot(training.history['val_loss'])\nplt.title(\"Loss and validation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Value\")\nplt.show()\n","02214f89":"pred_size=100\nX_test2 = X_test.values[:pred_size]\ny_pred2 = model.predict(X_test2) [:,0]*2-1\ny_test2 = y_test.values[:pred_size][:,0]*2-1\n\nax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\nax1.plot(y_test2, linestyle='none', marker='.', color='darkblue')\nax1.plot(y_pred2, linestyle='none', marker='.', color='darkred')\nax1.legend([\"Ground truth\",\"Predicted\"])\nax1.set_title(\"Both\")\nax1.set_xlabel(\"Epoch\")\nax2 = plt.subplot2grid((2, 2), (0, 1), colspan=1,rowspan=1)\nax2.plot(y_test2, linestyle='none', marker='.', color='darkblue')\nax2.set_title(\"Ground truth\")\nax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1,rowspan=1)\nax3.plot(y_pred2, linestyle='none', marker='.', color='darkred')\nax3.set_title(\"Predicted\")\nplt.tight_layout()\nplt.show()","d6727b74":"def predict_random_asset():\n    \"\"\"\n    Get random asset from test set, predict on it, plot ground truth and predicted value\n    \"\"\"\n    # Get any asset\n    ass = market_test_df.assetName.sample(1, random_state=24).iloc[0]\n    test_ass_df = market_test_df[market_test_df['assetName'] == ass]\n    # Preprocess\n    X,y = prepro.transformXy(test_ass_df)\n    y.index = test_ass_df.time\n    # Predict\n    pred = pd.DataFrame(model.predict(X)*2 -1)\n    pred.index = test_ass_df.time\n    # Plot\n    plt.plot(y*2-1, linestyle='none', marker='.', color='darkblue')\n    plt.plot(pred, linestyle='none', marker='.', color='orange')\n    plt.title(ass)\n    plt.legend([\"Ground truth\", \"predicted\"])\n    plt.show()\n    \npredict_random_asset()","aa8a1140":"# Accuracy metric\nconfidence = model.predict(X_test)*2 -1\nprint(\"Accuracy: %f\" % accuracy_score(y_test > 0, confidence > 0))\n\n# Show distribution of confidence that will be used as submission\nplt.hist(confidence, bins='auto')\nplt.xlabel(\"Confidence\")\nplt.ylabel(\"Count\")\nplt.title(\"predicted confidence\")\nplt.show()","b8231ddf":"def make_predictions(market_obs_df, news_obs_df, predictions_template_df):\n    \"\"\"\n    Predict confidence for one day and update predictions_template_df['confidenceValue']\n    @param market_obs_df: market_obs_df returned from env\n    @param predictions_template_df: predictions_template_df returned from env.\n    @return: None. prediction_template_df updated instead. \n    \"\"\"\n    # Preprocess the data\n    market_obs_df = join_market_news(market_obs_df, news_obs_df)\n    X = prepro.transform(market_obs_df)\n    # Predict\n    y_pred = model.predict(X)\n    confidence_df=pd.DataFrame(y_pred*2-1, columns=['confidence'])\n\n    # Merge predicted confidence to predictions template\n    pred_df = pd.concat([predictions_template_df, confidence_df], axis=1).fillna(0)\n    predictions_template_df.confidenceValue = pred_df.confidence","0a5ee68c":"##########################\n# Submission code\n\n# Save data here for later debugging on it\ndays_saved_data = []\n\n# Store execution info for plotting later\npredicted_days=[]\npredicted_times=[]\nlast_predictions_template_df = None\n\n# Predict day by day\ndays = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    # Store the data for later debugging on it\n    days_saved_data.append((market_obs_df, news_obs_df, predictions_template_df))\n    # For later plotting\n    predicted_days.append(market_obs_df.iloc[0].time.strftime('%Y-%m-%d'))\n    time_start = time()\n\n    # Call prediction func\n    make_predictions(market_obs_df, news_obs_df, predictions_template_df)\n    #!!!\n    env.predict(predictions_template_df)\n    \n    # For later plotting\n    last_predictions_template_df = predictions_template_df\n    predicted_times.append(time()-time_start)\n    #print(\"Prediction completed for \", predicted_days[-1])","e3cfa979":"# Plot execution time \nsns.barplot(np.array(predicted_days), np.array(predicted_times))\nplt.title(\"Execution time per day\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Execution time, seconds\")\nplt.show()\n\n# Plot predicted confidence for last day\nlast_predictions_template_df.plot(linestyle='none', marker='.')\nplt.title(\"Predicted confidence for last observed day: %s\" % predicted_days[-1])\nplt.xlabel(\"Observation No.\")\nplt.ylabel(\"Confidence\")\nplt.show()","251b4dc3":"# We've got a submission file!\n# !!! Write submission after all days are predicted\nenv.write_submission_file()\nprint([filename for filename in os.listdir('.') if '.csv' in filename])","40f2cd89":"# Submission","9ea63812":"## Preprocess the data\nScale numeric columns, encode categorical. Split to X, y for training.","b22160cc":"### Predict on random asset","445c7b39":"## Evaluate market model\n\n### Loss function by epoch ","9e1ca259":"## Create the model","25a3376d":"### Predict on test data","100b66ff":"This is heavily based on following two kernels - \nhttps:\/\/www.kaggle.com\/bguberfain\/a-simple-model-using-the-market-and-news-data\nhttps:\/\/www.kaggle.com\/dmitrypukhov\/eda-and-nn-for-market-and-news","d879214d":"# 5. Base model\nNN with Dense layers.\n","fd053d74":"## Split to train, validation and test","ef3f2dc8":"## Train market model"}}