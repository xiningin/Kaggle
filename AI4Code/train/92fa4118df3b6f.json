{"cell_type":{"9752969d":"code","8acecbe7":"code","9f3670bf":"code","4babb580":"code","bec8c482":"code","6205d820":"code","a1073e84":"code","7926de9e":"code","922efc85":"code","1a4c9140":"code","653787ae":"code","b515f2aa":"code","d470d543":"code","cbd551b9":"code","be5747ba":"code","8aa6f791":"code","d78689a3":"code","8c059f2e":"code","ea9af000":"code","04e41da4":"code","99aa0e0c":"code","db8bea8a":"code","9e7d7d62":"code","2aad7bc4":"code","74b45b4b":"code","3e516e30":"code","31cf652e":"code","0261c0f0":"code","07060a59":"code","050858e6":"code","65cbf822":"code","0672d758":"code","2a7a39d4":"code","bda3ead0":"code","a90bb551":"code","6313af29":"code","f1dfed2b":"code","f5a4d362":"code","9fa48b57":"code","626887e6":"code","ceb9126d":"code","8f20e34a":"code","9264a722":"code","4f4cbbdf":"code","73e19179":"code","2c86cf1c":"code","4cdb9039":"code","a959cc7b":"code","b2f851c7":"code","b36973ab":"code","9fc81742":"code","69dd2c09":"code","2e68ce41":"code","4bab37e8":"code","7f5a97d0":"code","9992a803":"code","2cf2dad9":"code","e471cd76":"code","6afb2f02":"code","ab4e6ff7":"code","11bcbfcb":"code","fbbe7657":"code","d66e8afb":"code","ee4468d0":"code","85fb4cf6":"code","90b474ce":"code","c736c581":"code","c56673cc":"code","6fadbe8a":"code","b4812215":"code","87e68bfb":"code","dedf42f2":"code","f0e6fe8a":"code","821225c7":"code","74e3b055":"code","f63170a1":"code","04343982":"code","f6bb8e02":"code","56ac505a":"code","cd8d802e":"code","59cea226":"code","4097a644":"code","b1d2f9e6":"code","17174676":"code","a3dda424":"code","b622c5fa":"code","cafbac92":"code","e18978d6":"code","1487f665":"code","cf3f8ec0":"code","8fee0949":"code","07334111":"code","c576840a":"code","cbd5a887":"code","c82c9e87":"code","b28c52f2":"code","e18aa605":"code","19558a74":"code","de9e4162":"code","4fbd3cf3":"code","32d26481":"code","9189d986":"code","7f216401":"code","04234395":"code","203cba55":"code","69c000d1":"code","a53dc51d":"code","63175851":"code","b9d64a2e":"code","b0dce0d1":"code","6acfccee":"code","84c77dfd":"code","7c7bfcd3":"code","7310e8b9":"code","e109f297":"code","da1f7714":"code","91e1d1b3":"code","11a1574f":"code","d3983312":"code","0c681125":"code","83e1f1bc":"code","65a3665f":"code","d24822a1":"code","4f3edd17":"code","b9192623":"code","560f1c6f":"code","15bf254f":"code","1df36e55":"code","4e7d4784":"code","98718bb8":"code","c3e44e86":"code","a7e8b5b7":"code","f5c0d275":"code","92aee9f2":"code","d43a9532":"markdown","2afcd62b":"markdown","cf3a72b9":"markdown","20458f2f":"markdown","26cfd79c":"markdown","c88cc23b":"markdown","a6f22814":"markdown","16494a34":"markdown","6d237f5c":"markdown","8e0b5465":"markdown","3dff4dfe":"markdown","295b06d6":"markdown","8c9f9fea":"markdown","9e498663":"markdown","973a2d69":"markdown","67d0f661":"markdown","656d4555":"markdown","8fdec61f":"markdown","a1b8cda8":"markdown","cc52ac5c":"markdown","53958451":"markdown","b6f5c671":"markdown","5c7b84a6":"markdown","ca3086c1":"markdown","1acca36e":"markdown","ac7cf89d":"markdown","a2cf3ecc":"markdown","54bcf356":"markdown","76849def":"markdown","31eea470":"markdown","f4606dea":"markdown","2b439ca1":"markdown","4a820c63":"markdown","e9712dc4":"markdown","3a5a6f72":"markdown","d3bbd123":"markdown","32e0afa3":"markdown","878dab4a":"markdown","32bd870d":"markdown","fcfe30f4":"markdown","ca5cc0fc":"markdown","b9c4ceea":"markdown","540a642c":"markdown","10f9610f":"markdown","4c0a07ff":"markdown","43cc8a14":"markdown","356731dc":"markdown","871b2927":"markdown","d80aa475":"markdown","be46baee":"markdown","ca21c381":"markdown","bcb4bf23":"markdown","54a48fc7":"markdown","868a0dd6":"markdown","52303a7c":"markdown","e742d4e0":"markdown","1252ec36":"markdown","9dc4ea1d":"markdown","10ff90bf":"markdown","df3154c4":"markdown","c30cb012":"markdown","18aa7f3e":"markdown","444954a9":"markdown","acf9a9e8":"markdown","5aa86343":"markdown","0d417e69":"markdown","b53ee991":"markdown","4f1471be":"markdown","523ea927":"markdown","003b9b43":"markdown","fabff3c9":"markdown","b062ea5a":"markdown","f5a0fd4e":"markdown","1ea4f5ef":"markdown","5e2703ef":"markdown","0db46988":"markdown","b486639c":"markdown","ffbb164b":"markdown","3a0c29be":"markdown","afc46e3d":"markdown","e57d8ea4":"markdown","b533b7f9":"markdown","7be9c04b":"markdown","5432e48b":"markdown","81860f85":"markdown","e120a24d":"markdown","23371f09":"markdown","6b1c328a":"markdown","524c211d":"markdown","6c2c79ce":"markdown","e32a2e80":"markdown","76d0ca35":"markdown","115a1ba2":"markdown"},"source":{"9752969d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8acecbe7":"import ast # used to make columns of dictionnary's keys \n# used for safely evaluating strings containing Python values from untrusted sources without the need to parse the values oneself\n\nimport matplotlib.pyplot as plt # Library for data visualization\nimport seaborn as sns\nimport plotly.graph_objs as go\n\nimport re # Library to use Regular Expressions\n\n\nimport nltk\nfrom nltk.corpus import stopwords # Nltk is used to remove stopwords from our descriptions data\nimport gensim # open-source library for unsupervised topic modeling and NLP\nfrom gensim.utils import simple_preprocess # Convert a document into a list of tokens\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer # Library used to apply TF-IDF word embedding\n\nfrom sklearn.model_selection import train_test_split # Used to split the dataset (train\/val)\n\n\n## Training models\n\n# Statistical models\nfrom skmultilearn.problem_transform import LabelPowerset\nfrom sklearn.linear_model import LogisticRegression\n\nfrom skmultilearn.adapt import MLkNN # Implementation of KNN for multi label classification tasks\nfrom scipy.sparse import csr_matrix, lil_matrix\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nfrom skmultilearn.problem_transform import ClassifierChain\n\n# EVALUATION METRICS\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import hamming_loss\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score","9f3670bf":"aliens_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_aliens.csv')\n\nalternate_history_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_alternate_history.csv')\n\nalternate_universe_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_alternate_universe.csv')\n\napocalyptic_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_apocalyptic.csv')\n\ncyberpunk_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_cyberpunk.csv')\n\ndystopia_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_dystopia.csv')\n\nhard_sf_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_hard.csv')\n\nmilitary_sf_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_military.csv')\n\nrobots_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_robots.csv')\n\nspace_opera_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_space_opera.csv')\n\nsteampunk_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_steampunk.csv')\n\ntime_travel_df = pd.read_csv('..\/input\/science-fiction-books-subgenres\/sf_time_travel.csv')","4babb580":"def extract_genres(raw_df):\n    raw_df['Genres'] = raw_df['Genres'].map(lambda d : ast.literal_eval(d))\n    genres_df = pd.DataFrame(raw_df[\"Genres\"].to_dict()).T\n    genres_df = genres_df.fillna(0)\n    return genres_df","bec8c482":"# We transform every value in genres dataset > 0 to 1 \n\ndef binary_genres(genres_df):\n    df_cols = list(genres_df.columns)\n    for i in df_cols:\n        genres_df.loc[genres_df[i] <= 5, i] = 0\n        genres_df.loc[genres_df[i] > 5, i] = 1\n\n    return genres_df","6205d820":"def select_binary_sf_subgenres(raw_df, sf_cols):\n    genres_df = extract_genres(raw_df)\n    genres_df = genres_df[sf_cols].copy()\n    genres_df = binary_genres(genres_df)\n    return genres_df","a1073e84":"sf_cols = ['Science Fiction (Dystopia) ',\n         'Science Fiction (Time Travel) ',\n         'Science Fiction (Cyberpunk) ',\n         'Science Fiction (Robots) ',\n         'Science Fiction (Aliens) ',\n         'Science Fiction (Apocalyptic) ',\n         'Science Fiction (Alternate History) ',\n         'Science Fiction (Hard Science Fiction) ',\n         'Science Fiction (Steampunk) ',\n         'Science Fiction (Military Science Fiction) ',\n         'Space (Space Opera) ']\n\nsf_cols_apo = ['Science Fiction (Dystopia) ',\n         'Science Fiction (Time Travel) ',\n         'Science Fiction (Cyberpunk) ',\n         'Science Fiction (Robots) ',\n         'Science Fiction (Aliens) ',\n         'Science Fiction (Apocalyptic) ',\n         'Science Fiction (Alternate History) ',\n         'Science Fiction (Hard Science Fiction) ',\n         'Science Fiction (Steampunk) ',\n         'Space (Space Opera) ']\n\nsf_cols_alt_u = ['Science Fiction (Dystopia) ',\n         'Science Fiction (Time Travel) ',\n         'Science Fiction (Alternate Universe) ',\n         'Science Fiction (Cyberpunk) ',\n         'Science Fiction (Robots) ',\n         'Science Fiction (Aliens) ',\n         'Science Fiction (Apocalyptic) ',\n         'Science Fiction (Alternate History) ',\n         'Science Fiction (Hard Science Fiction) ',\n         'Science Fiction (Steampunk) ',\n         'Science Fiction (Military Science Fiction) ',\n         'Space (Space Opera) ']\n\n\n\nsf_cols_dyst = ['Science Fiction (Dystopia) ',\n 'Science Fiction (Time Travel) ',\n 'Science Fiction (Cyberpunk) ',\n 'Science Fiction (Robots) ',\n 'Science Fiction (Aliens) ',\n 'Science Fiction (Apocalyptic) ',\n 'Science Fiction (Alternate History) ',\n 'Science Fiction (Steampunk) ',\n 'Space (Space Opera) ']\n\nsf_cols_tt = ['Science Fiction (Dystopia) ',\n         'Science Fiction (Time Travel) ',\n         'Science Fiction (Cyberpunk) ',\n         'Science Fiction (Aliens) ',\n         'Science Fiction (Apocalyptic) ',\n         'Science Fiction (Alternate History) ',\n         'Science Fiction (Hard Science Fiction) ',\n         'Science Fiction (Steampunk) ',\n         'Science Fiction (Military Science Fiction) ',\n         'Space (Space Opera) ']\n\n\n\nsf_cols_None = ['Science Fiction (Dystopia) ',\n 'Science Fiction (Time Travel) ',\n 'Science Fiction (Cyberpunk) ',\n 'Science Fiction (Robots) ',\n 'Science Fiction (Aliens) ',\n 'Science Fiction (Apocalyptic) ',\n 'Science Fiction (Alternate History) ',\n 'Science Fiction (Hard Science Fiction) ',\n 'Science Fiction (Steampunk) ',\n 'Science Fiction (Military Science Fiction) ',\n 'Space (Space Opera) ',\n               'None']","7926de9e":"aliens_subgenres_df = select_binary_sf_subgenres(aliens_df, sf_cols)\nplt.figure(figsize=(16,8))\naliens_subgenres_df.sum(axis=0).plot.bar()","922efc85":"alternate_history_subgenres_df = select_binary_sf_subgenres(alternate_history_df, sf_cols)\nplt.figure(figsize=(16,8))\nalternate_history_subgenres_df.sum(axis=0).plot.bar()","1a4c9140":"alternate_universe_subgenres_df = select_binary_sf_subgenres(alternate_universe_df, sf_cols_alt_u)\nplt.figure(figsize=(16,8))\nalternate_universe_subgenres_df.sum(axis=0).plot.bar()","653787ae":"apocalyptic_subgenres_df = select_binary_sf_subgenres(apocalyptic_df, sf_cols_apo)\nplt.figure(figsize=(16,8))\napocalyptic_subgenres_df.sum(axis=0).plot.bar()","b515f2aa":"cyberpunk_subgenres_df = select_binary_sf_subgenres(cyberpunk_df, sf_cols)\nplt.figure(figsize=(16,8))\ncyberpunk_subgenres_df.sum(axis=0).plot.bar()","d470d543":"dystopia_subgenres_df = select_binary_sf_subgenres(dystopia_df, sf_cols_dyst)\nplt.figure(figsize=(16,8))\ndystopia_subgenres_df.sum(axis=0).plot.bar()","cbd551b9":"hard_sf_subgenres_df = select_binary_sf_subgenres(hard_sf_df, sf_cols)\nplt.figure(figsize=(16,8))\nhard_sf_subgenres_df.sum(axis=0).plot.bar()","be5747ba":"military_sf_subgenres_df = select_binary_sf_subgenres(military_sf_df, sf_cols)\nplt.figure(figsize=(16,8))\nmilitary_sf_subgenres_df.sum(axis=0).plot.bar()","8aa6f791":"robots_subgenres_df = select_binary_sf_subgenres(robots_df, sf_cols)\nplt.figure(figsize=(16,8))\nrobots_subgenres_df.sum(axis=0).plot.bar()","d78689a3":"space_opera_subgenres_df = select_binary_sf_subgenres(space_opera_df, sf_cols)\nplt.figure(figsize=(16,8))\nspace_opera_subgenres_df.sum(axis=0).plot.bar()","8c059f2e":"steampunk_subgenres_df = select_binary_sf_subgenres(steampunk_df, sf_cols_dyst)\nplt.figure(figsize=(16,8))\nsteampunk_subgenres_df.sum(axis=0).plot.bar()","ea9af000":"time_travel_subgenres_df = select_binary_sf_subgenres(time_travel_df, sf_cols_tt)\nplt.figure(figsize=(16,8))\ntime_travel_subgenres_df.sum(axis=0).plot.bar()","04e41da4":"# We only keep the SF subgenres we identified as plain subgenres.\n# We add None to exclude the books classified in no subgenres (by the website) in the future.\n\nsf_cols = ['Science Fiction (Dystopia) ',\n         'Science Fiction (Time Travel) ',\n         'Science Fiction (Cyberpunk) ',\n         'Science Fiction (Robots) ',\n         'Science Fiction (Aliens) ',\n         'Science Fiction (Apocalyptic) ',\n         'Science Fiction (Alternate History) ',\n         'Science Fiction (Steampunk) ',\n         'Science Fiction (Military Science Fiction) ',\n         'Space (Space Opera) ',\n          'None']","99aa0e0c":"real_subgenres = [aliens_df, alternate_history_df, apocalyptic_df, cyberpunk_df, dystopia_df, military_sf_df,\n                  robots_df, space_opera_df, steampunk_df, time_travel_df]\n\n# We concat all subgenres datasets to create a new dataset with all identified subgenres\nsf_df = pd.concat(real_subgenres)\n\nsf_df","db8bea8a":"sf_df = sf_df.drop_duplicates(subset='url', keep='first', inplace=False)\nsf_df","9e7d7d62":"sf_df = sf_df.reset_index().drop('index', axis=1)\nsf_df","2aad7bc4":"sf_df = sf_df.reset_index().drop('index', axis=1)\nsf_df","74b45b4b":"sf_df = sf_df.reset_index().drop('index', axis=1)\n\nsf_subgenres_df = pd.DataFrame(sf_df[\"Genres\"].to_dict()).T\nsf_subgenres_df = sf_subgenres_df.fillna(0)\nsf_subgenres_df = sf_subgenres_df[sf_cols].copy()\nsf_subgenres_df = binary_genres(sf_subgenres_df)\n\nplt.figure(figsize=(16,8))\nsf_subgenres_df.sum(axis=0).plot.bar()","3e516e30":"sf_full = pd.concat([sf_df, sf_subgenres_df], axis=1)","31cf652e":"table = pd.pivot_table(sf_full, values=sf_cols, index=['Year_published'],\n                    aggfunc=np.sum)","0261c0f0":"table_2000 = table[95:105]\ntable_2010 = table[105:115]\ntable_2000_2020 = table[95:115]","07060a59":"plt.figure(figsize=(40,20))\ntable_2000_2020.plot.line()","050858e6":"sf_full.isna().sum()","65cbf822":"# Getting rid of NaNs in book description\nsf_full.dropna(subset = [\"Book_Description\"], inplace=True)\n# Replacing NaNs\nsf_full.Edition_Language = sf_full.Edition_Language.fillna('None')","0672d758":"# Verifying that Nans have successfully been processed\nsf_full.isna().sum()","2a7a39d4":"sf_full = sf_full[sf_full['Edition_Language'].isin(['English', 'None'])]\nsf_full","bda3ead0":"#pd.options.display.max_rows = 999\n#sf_full[sf_full['Book_Description'].str.contains(' el ')]\nsf_full = sf_full.drop([410, 3202, 7421, 8079], axis=0)\n# 410 fr\n# 3202 nl\n# 7421 es\n# 8079 it","a90bb551":"sf_full = sf_full[sf_full['None'] < 1]\nsf_full =  sf_full.drop('None', axis = 1)\nsf_cols.pop()\nsf_full","6313af29":"sf_full = sf_full.drop([1838, 5461], axis=0)\n# Dropping books with no sf genres assigned\nsf_full = sf_full[sf_full[sf_cols].sum(axis=1) > 0]\nsf_full = sf_full.reset_index()","f1dfed2b":"# Index of the book\nj = 0\n\nprint(sf_df.Book_Title[j])\nprint(sf_df.Book_Description[j])","f5a4d362":"print('Book: ' +  str(sf_df['Book_Title'][j]))\n\nfor i in sf_cols:\n    if sf_full[i][j] > 0:\n        print(i + \": \" + str(sf_full[i][j]))\n        \nfor i in sf_cols:\n    if sf_full[i][j] == 0:\n        print(i + \": \" + str(sf_full[i][j]))","9fa48b57":"def preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence","626887e6":"books_descriptions = []\nsentences = list(sf_full[\"Book_Description\"].copy())\nfor sen in sentences:\n    books_descriptions.append(preprocess_text(sen))","ceb9126d":"# Storing a list of stopwords\nnltk.download('stopwords') # start the NLTK Downloader and download the stopwords\n\nstop_words = stopwords.words('english') # Selecting english stopwords\n\n# We add our own list of stopwords for our specific usecase\nstop_words.extend(['from', 'story', 'novel', 'series', 'author', 'written', 'book', 'science_fiction', 'u', 'story', 'tale', 'writer', 'volume', 'classic',\n                 'collection', 'el', 'novelette', 'shortstory', 'novella', 'story', 'wikipedia', 'essay']) \n\n\n# Defining a function to convert our descriptions to word tokens and remove ponctuation\ndef sent_to_words(descriptions):\n    i = 0\n    for description in descriptions:\n        # deacc=True removes punctuations\n        yield(gensim.utils.simple_preprocess(str(description), deacc=True))  # Convert a document into a list of tokens\n        \n# Defining a function to remove stopwords    \ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) \n             if word not in stop_words] for doc in texts]","8f20e34a":"books_descriptions = list(sent_to_words(books_descriptions))\nbooks_descriptions = remove_stopwords(books_descriptions)\n\nprint(books_descriptions[0]) #Showing our first description tokens","9264a722":"from nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import wordnet\n# Init the Wordnet Lemmatizer\nlemmatizer = WordNetLemmatizer()\n\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n\ndef lemmatize(data_words):\n    data_lemmatized = list()\n    for tokens in data_words:\n        sentence = ' '.join(word for word in tokens)\n        sentence = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]\n        data_lemmatized.append(sentence)\n    return data_lemmatized\n        \n\nbooks_descriptions = lemmatize(books_descriptions)","4f4cbbdf":"print(books_descriptions[0])","73e19179":"from nltk.tokenize.treebank import TreebankWordDetokenizer\npreprocessed_texts = []\nfor i in books_descriptions:\n    j = TreebankWordDetokenizer().detokenize(i)\n    preprocessed_texts.append(j)","2c86cf1c":"sf_full['Book_Description_preprocessed'] = preprocessed_texts","4cdb9039":"sf_full","a959cc7b":"y = sf_full[sf_cols].copy()","b2f851c7":"vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), norm='l2', max_features = 10000)\n# analyzer =\"word\", means that the features extracted should be made of words n-grams (opposed to character n-grams)\n# ngram_range=(1,3) means that the features extracted can be made from 1 to 3 words association.\n# norm='l2' means that the sum of squares of vector elements is 1.\n# max_features = 100000 , means that only the 10K top features will be taken into account - ordered by term frequency across the corpus.\nvectorizer.fit(sf_full['Book_Description_preprocessed'].values)\nX = vectorizer.transform(sf_full['Book_Description_preprocessed'].values)","b36973ab":"# pickle.dump(vectorizer, open(\"lr2.sav\", 'wb'))","9fc81742":"sf_full.Book_Description_preprocessed.isna().sum()","69dd2c09":"sf_full","2e68ce41":"from wordcloud import WordCloud,STOPWORDS\n\nplt.figure(figsize=(40,25))\ntext = sf_full.Book_Description_preprocessed.values\ncloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\nplt.axis('off')\nplt.title(\"Common words in the description\",fontsize=40)\nplt.imshow(cloud)","4bab37e8":"bar_plot = pd.DataFrame()\nbar_plot['subgenre'] = y.columns[:]\nbar_plot['count'] = y.iloc[:,:].sum().values\nbar_plot.sort_values(['count'], inplace=True, ascending=False)\nbar_plot.reset_index(inplace=True, drop=True)\nbar_plot.head()","7f5a97d0":"plt.figure(figsize=(15,8))\nsns.set(font_scale = 1.5)\nsns.set_style('whitegrid') \n\n\npal = sns.color_palette(\"Blues_r\", len(bar_plot))\nrank = bar_plot['count'].argsort().argsort()  \n\nsns.barplot(bar_plot['subgenre'], bar_plot['count'], palette=np.array(pal[::-1])[rank])\nplt.title(\"Most commons subgenres\", fontsize=24)\nplt.ylabel('Number of books', fontsize=18)\nplt.xlabel('Genre', color=\"white\", fontsize=18)\nplt.xticks(rotation='vertical')\n\nplt.show()","9992a803":"rowSums = y.iloc[:,:].sum(axis=1)\nmultiLabel_counts = rowSums.value_counts()","2cf2dad9":"sns.set(font_scale = 1.5)\nsns.set_style('whitegrid') \nplt.figure(figsize=(10,6))\n\nsns.barplot(multiLabel_counts.index, multiLabel_counts.values)\nplt.title(\"Number of subgenres per book\", fontsize=24)\nplt.ylabel('Number of books', fontsize=18)\nplt.xlabel('Number of subgenres', fontsize=18)\n\nplt.show()","e471cd76":"boxplot = sf_full.copy()\nboxplot['len'] = sf_full.Book_Description.apply(lambda x: len(x))","6afb2f02":"sns.set(style=\"whitegrid\",rc={\"font.size\":13,\"axes.labelsize\":13})\n\nplt.figure(figsize=(9, 4))\n\nax = sns.boxplot(x='len', data=boxplot, orient=\"h\", palette=\"Set2\")\nplt.ylabel('')\nplt.xlabel('Words')\nplt.title(\"Distribution of the word frequency\", fontsize=13)\nplt.tight_layout(h_pad=3)","ab4e6ff7":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.20, shuffle=True)","11bcbfcb":"y.sum(axis=0)\/y.shape[0]","fbbe7657":"y_train.sum(axis=0)\/y_train.shape[0]","d66e8afb":"y_val.sum(axis=0)\/y_val.shape[0]","ee4468d0":"words_list = vectorizer.get_feature_names()\nimportance_df = pd.DataFrame()\nimportance_df[\"Words\"] = words_list","85fb4cf6":"from sklearn.model_selection import cross_val_score","90b474ce":"# import pickle","c736c581":"# Creating null matrix that will recieve the predictions of each model\nprd_proba = np.zeros((X_val.shape[0],y.shape[1]))\nprd = np.zeros((X_val.shape[0],y.shape[1]))\n\n\ni = 0\nfor sf_col in sf_cols:\n    lr = LogisticRegression(C=3,random_state = 0,class_weight = 'balanced')\n    # C :Inverse of regularization strength; must be a positive float. Like in support vector machines,\n    #   smaller values specify stronger regularization.\n    \n    # The \u201cbalanced\u201d mode make the error of the underrepresented subgenre weight more\n    # n_samples \/ (n_classes * np.bincount(y)).\n    lr.fit(X_train,y_train.iloc[:, i])\n    lr.predict(X_val)\n    print(sf_col)\n    \n#     filename = \"lr_\" + re.search(\"(\\()(.*)(\\))\", sf_col)[2].replace(\" \", \"_\") +\"2\" +\".sav\"\n#     print(filename)\n#     # save the model to disk\n#     pickle.dump(lr, open(filename, 'wb'))\n#     text = vectorizer.transform(text_dystopia)\n\n#     print(lr.predict_proba(text))\n    \n    prd_proba[:,i] = lr.predict_proba(X_val)[:,1]  \n    prd[:,i] = lr.predict(X_val)\n    \n    print(\"Accuracy = \",accuracy_score(y_val[sf_col],prd[:,i]))\n    print(\"F1 = \",f1_score(y_val[sf_col],prd[:,i], average=\"micro\"))\n    print(\"Hamming loss = \",hamming_loss(y_val[sf_col],prd[:,i]))\n    print(\"\\n\")\n    \n    ## Cross val\n    #scores = cross_val_score(lr, X_train, y_train.iloc[:, i], cv=5)\n    #print('Cross-Validation Accuracy Scores', scores)\n    \n    # Getting the importance of each word in the decisions of the model\n    words_importances = lr.coef_[0]\n    importance_df[sf_col] = words_importances\n    \n    i += 1","c56673cc":"print(\"Accuracy = \",accuracy_score(y_val,prd))\nprint(\"Recall = \", recall_score(y_val,prd, average=\"micro\"))\nprint(\"F1 = \",f1_score(y_val,prd, average=\"micro\"))\nprint(\"Hamming loss = \",hamming_loss(y_val,prd))\nprint('AUC ROC score is ', roc_auc_score(y_val,prd_proba))","6fadbe8a":"all_models_results = pd.DataFrame(columns=['Model', 'Accuracy', 'F1', 'Hamming_loss', 'AUC_ROC'])","b4812215":"lr_result = [\"Logistic Regression\", accuracy_score(y_val,prd), f1_score(y_val,prd, average=\"micro\"), hamming_loss(y_val,prd), roc_auc_score(y_val,prd_proba)]\nall_models_results.loc[0] = lr_result\nall_models_results","87e68bfb":"\naccuracy_list=[]\nf1_list=[]\nhl_list=[]\nauc_list=[]\n\nfor i in range(0,10):\n    accuracy_list.append(accuracy_score(y_val.iloc[:,i],prd[:,i]))\n    f1_list.append(f1_score(y_val.iloc[:,i],prd[:,i], average=\"micro\"))\n    hl_list.append(hamming_loss(y_val.iloc[:,i],prd[:,i]))\n    auc_list.append(roc_auc_score(y_val.iloc[:,i],prd_proba[:,i]))\n    ","dedf42f2":"np.array(accuracy_list).mean()","f0e6fe8a":"lr_result = [\"Logistic Regression_binary_class\", np.array(accuracy_list).mean(), np.array(f1_list).mean(), np.array(hl_list).mean(), np.array(auc_list).mean()]\nall_models_results.loc[1] = lr_result\nall_models_results","821225c7":"importance_df","74e3b055":"fig=go.Figure()\nfig.layout.xaxis.fixedrange = True\nfig.layout.yaxis.fixedrange = True\n\nfig.update_layout(title_text=\"TOP 30 Feature Importances\")\n\n\nbuttons=[]\n\ndefault_state = \"Science Fiction (Dystopia) \"\n\nfor sf_col in sf_cols:\n    test_df = importance_df.sort_values(by=sf_col, ascending=False)[:30]\n    subgenre_importance = test_df[[\"Words\", sf_col]].copy()\n    \n    # We have two traces we're plotting per state: a boxplot of the submission quartiles, and a line with the current data to-date\n    fig.add_trace(go.Bar(x=subgenre_importance[\"Words\"], y=subgenre_importance[sf_col], visible=(sf_col==default_state)))\n\n    \nfor sf_col in sf_cols:\n    buttons.append(dict(method='update',\n                        label=sf_col,\n                        args = [{'visible': [sf_col==r for r in sf_cols]}]))\n\n# Add dropdown menus to the figure\nfig.update_layout(showlegend=False, updatemenus=[{\"buttons\": buttons, \"direction\": \"down\", \"active\": sf_cols.index(default_state), \"showactive\": True, \"x\": 1, \"y\": 1.15}])\nfig.show()","f63170a1":"fig=go.Figure()\nfig.layout.xaxis.fixedrange = True\nfig.layout.yaxis.fixedrange = True\n\nfig.update_layout(title_text=\"FLOP 30 Feature Importances\")\n\n\nbuttons=[]\n\ndefault_state = \"Science Fiction (Dystopia) \"\n\nfor sf_col in sf_cols:\n    test_df = importance_df.sort_values(by=sf_col, ascending=True)[:30]\n    subgenre_importance = test_df[[\"Words\", sf_col]].copy()\n    \n    # We have two traces we're plotting per state: a boxplot of the submission quartiles, and a line with the current data to-date\n    fig.add_trace(go.Bar(x=subgenre_importance[\"Words\"], y=subgenre_importance[sf_col], visible=(sf_col==default_state)))\n    fig.update_traces(marker_color='red')\n\n    \nfor sf_col in sf_cols:\n    buttons.append(dict(method='update',\n                        label=sf_col,\n                        args = [{'visible': [sf_col==r for r in sf_cols]}]))\n\n# Add dropdown menus to the figure\nfig.update_layout(showlegend=False, updatemenus=[{\"buttons\": buttons, \"direction\": \"down\", \"active\": sf_cols.index(default_state), \"showactive\": True, \"x\": 1, \"y\": 1.15}])\nfig.show()","04343982":"from sklearn.metrics import recall_score\n\nth = []\nf = []\nham = []\nac = []\nrec = []\n\n# We make iterations to discover the best threshold value\n# Threshold says how to distinguish between 0\/1 label assignment.\nfor t in range (5,70): # threshold value\n y_pred_new = (prd_proba >= t\/100).astype(int)\n#  print(\"t =\" ,t\/100)\n#  print(\"Accuracy = \",accuracy_score(y_val,y_pred_new))\n#  print(\"F1 = \",f1_score(y_val,y_pred_new, average=\"micro\"))\n#  print(\"Hamming loss = \",hamming_loss(y_val,y_pred_new))\n th.append(t)\n ac.append(accuracy_score(y_val,y_pred_new))\n f.append(f1_score(y_val,y_pred_new, average=\"micro\"))\n ham.append(hamming_loss(y_val,y_pred_new))\n rec.append(recall_score(y_val,y_pred_new, average=\"micro\"))\nplt.rcParams[\"figure.figsize\"] = (12,6)\nwith plt.style.context('ggplot'):\n plt.plot(th, f)\n plt.plot(th, ham)\n plt.plot(th, ac)\n plt.plot(th,rec)\n plt.legend(['F1', 'Hamming loss', 'Accuracy', 'Recall'], loc='center left', fontsize = 14)\n plt.ylabel(\"metrics\", fontsize = 14)\n plt.xlabel(\"probability threshold\", fontsize = 14)\n plt.title(\"Logistic Regression Model\", fontsize = 18)\nplt.show()","f6bb8e02":"# #Label Powerset\n# lp_classifier = LabelPowerset(LogisticRegression())\n# lp_classifier.fit(X_train, y_train)\n# lp_predictions = lp_classifier.predict(X_val)\n# print(\"Accuracy = \",accuracy_score(y_val,lp_predictions))\n# print(\"F1 score = \",f1_score(y_val,lp_predictions, average=\"micro\"))\n# print(\"Hamming loss = \",hamming_loss(y_val,lp_predictions))\n# print('AUC ROC score is ', roc_auc_score(y_val,lp_classifier.predict_proba(X_val).toarray()))","56ac505a":"lp_result = [\"Label Powerset\", 0.6025477707006369, 0.7352309344790549, 0.06280254777070064, 0.9611438821255189]\nall_models_results.loc[2] = lp_result\nall_models_results","cd8d802e":"# # #MLkNN\n# ml_classifier = MLkNN(k=10)\n# # to prevent errors when handling sparse matrices.\n# X_train = lil_matrix(X_train).toarray()\n# y_train = lil_matrix(y_train).toarray()\n# X_val = lil_matrix(X_val).toarray()\n# y_val = lil_matrix(y_val).toarray()\n# ml_classifier.fit(X_train, y_train)\n# # predict\n# ml_predictions = ml_classifier.predict(X_val)\n# # accuracy metrics\n# print(\"Accuracy = \",accuracy_score(y_val,ml_predictions))\n# print(\"F1 score = \",f1_score(y_val,ml_predictions, average=\"micro\"))\n# print(\"Hamming loss = \",hamming_loss(y_val,ml_predictions))\n# print(\"AUC ROC = \", roc_auc_score(y_val, ml_classifier.predict_proba(X_val).toarray()))","59cea226":"mlknn_result = [\"Multi KNN\", 0.4732484076433121, 0.653293575494714, 0.08146496815286625, 0.8549072590225327]\nall_models_results.loc[3] = mlknn_result\nall_models_results","4097a644":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.20, shuffle=True)","b1d2f9e6":"\n#for the next classifier we need to remove from y-train, y-test categories which equal 0 for all train samples\nselected_labels = y_train.columns[y_train.sum(axis = 0, skipna = True) > 0].tolist()\ny_train = y_train.filter(selected_labels, axis=1)\ny_val = y_val.filter(selected_labels, axis=1)\n\ncc_classifier = ClassifierChain(LogisticRegression(C=3,random_state = 0, class_weight=\"balanced\", solver=\"saga\"))\ncc_classifier.fit(X_train, y_train)\ncc_predictions_proba = cc_classifier.predict_proba(X_val)\n#for plotting metrics as a function of threashold\nth = []\nf = []\nham = []\nac = []\n\n# We make iterations to discover the best threshold value\n# Threshold says how to distinguish between 0\/1 label assignment.\nfor t in range (5,70): # threshold value\n y_pred_new = (cc_predictions_proba >= t\/100).astype(int)\n#  print(\"t =\" ,t\/100)\n#  print(\"Accuracy = \",accuracy_score(y_val,y_pred_new))\n#  print(\"F1 = \",f1_score(y_val,y_pred_new, average=\"micro\"))\n#  print(\"Hamming loss = \",hamming_loss(y_val,y_pred_new))\n th.append(t)\n ac.append(accuracy_score(y_val,y_pred_new))\n f.append(f1_score(y_val,y_pred_new, average=\"micro\"))\n ham.append(hamming_loss(y_val,y_pred_new))\nplt.rcParams[\"figure.figsize\"] = (12,6)\nwith plt.style.context('ggplot'):\n plt.plot(th, f)\n plt.plot(th, ham)\n plt.plot(th, ac)\n plt.legend(['F1', 'Hamming loss', 'Accuracy'], loc='center left', fontsize = 14)\n plt.ylabel(\"metrics\", fontsize = 14)\n plt.xlabel(\"threshold\", fontsize = 14)\n plt.title(\"Classfier Chain Model\", fontsize = 18)\nplt.show()","17174676":"y_pred_new = (cc_predictions_proba >= 61\/100).astype(int)\nprint(\"Accuracy = \",accuracy_score(y_val,y_pred_new))\nprint(\"F1 = \",f1_score(y_val,y_pred_new, average=\"micro\"))\nprint(\"Hamming loss = \",hamming_loss(y_val,y_pred_new))\nprint(\"AUC ROC = \", roc_auc_score(y_val, cc_predictions_proba.toarray()))\n","a3dda424":"cc_result = [\"Classifier Chain\", accuracy_score(y_val,y_pred_new), f1_score(y_val,y_pred_new, average=\"micro\"), hamming_loss(y_val,y_pred_new), roc_auc_score(y_val, cc_predictions_proba.toarray())]\nall_models_results.loc[4] = cc_result\nall_models_results","b622c5fa":"# import pickle\n# filename = \"cc.sav\"\n# pickle.dump(cc_classifier, open(filename, 'wb'))","cafbac92":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\n\n\nLR_pipeline = Pipeline([('clf', OneVsRestClassifier(LogisticRegression(C= 3, class_weight='balanced', solver='sag'), n_jobs=-1))])\naccuracy = 0\nf1 = 0\nhl = 0\nauc_roc = 0\nfor category in sf_cols[:]:\n#     print('**Processing {} titles...**'.format(category))\n    \n    # Training logistic regression model on train data\n    LR_pipeline.fit(X_train, y_train[category])\n    \n    # calculating test accuracy\n    prediction = LR_pipeline.predict(X_val)\n    accuracy = accuracy + accuracy_score(y_val[category],prediction)\n    f1 = f1 + f1_score(y_val[category],prediction, average=\"micro\")\n    hl = hl + hamming_loss(y_val[category],prediction)\n    auc_roc = auc_roc + roc_auc_score(y_val[category], prediction)\n#     print(\"Accuracy = \",accuracy_score(y_val[category],prediction))\n#     print(\"F1 = \",f1_score(y_val[category],prediction, average=\"micro\"))\n#     print(\"Hamming loss = \",hamming_loss(y_val[category],prediction))\n#     print('AUC-ROC is {}'.format(roc_auc_score(y_val[category],prediction)))\n#     print(\"\\n\")\n\nprint('Test averaged Accuracy is {}'.format(accuracy\/len(sf_cols[:])))\nprint('Test averaged F1 is {}'.format(f1\/len(sf_cols[:])))\nprint('Test averaged Hamming Loss is {}'.format(hl\/len(sf_cols[:])))\nprint('Test averaged AUC-ROC is {}'.format(auc_roc\/len(sf_cols[:])))","e18978d6":"ovr_lr_result = [\"One vs. Rest (LR)\", accuracy\/len(sf_cols[:]), f1\/len(sf_cols[:]), hl\/len(sf_cols[:]), auc_roc\/len(sf_cols[:])]\nall_models_results.loc[5] = ovr_lr_result\nall_models_results","1487f665":"from sklearn.svm import LinearSVC\n\nSVC_pipeline = Pipeline([('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\"), n_jobs=1))])\n\naccuracy = 0\nf1 = 0\nhl = 0\nauc_roc = 0\n\nfor category in sf_cols[:]:\n#     print('**Processing {} titles...**'.format(category))\n    \n    # Training logistic regression model on train data\n    SVC_pipeline.fit(X_train, y_train[category])\n    \n    # calculating test accuracy\n    prediction = SVC_pipeline.predict(X_val)\n    accuracy = accuracy + accuracy_score(y_val[category],prediction)\n    f1 = f1 + f1_score(y_val[category],prediction, average=\"micro\")\n    hl = hl + hamming_loss(y_val[category],prediction)\n    auc_roc = auc_roc + roc_auc_score(y_val[category], prediction)\n#     print(\"Accuracy = \",accuracy_score(y_val[category],prediction))\n#     print(\"F1 = \",f1_score(y_val[category],prediction, average=\"micro\"))\n#     print(\"Hamming loss = \",hamming_loss(y_val[category],prediction))\n#     print('AUC-ROC is {}'.format(roc_auc_score(y_val[category],prediction)))\n#     print(\"\\n\")\n\nprint('Test averaged Accuracy is {}'.format(accuracy\/len(sf_cols[:])))\nprint('Test averaged F1 is {}'.format(f1\/len(sf_cols[:])))\nprint('Test averaged Hamming Loss is {}'.format(hl\/len(sf_cols[:])))\nprint('Test averaged AUC-ROC is {}'.format(auc_roc\/len(sf_cols[:])))","cf3f8ec0":"ovr_lsvc_result = [\"One vs. Rest (LSVC)\", accuracy\/len(sf_cols[:]), f1\/len(sf_cols[:]), hl\/len(sf_cols[:]), auc_roc\/len(sf_cols[:])]\nall_models_results.loc[6] = ovr_lsvc_result\nall_models_results","8fee0949":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF_pipeline = Pipeline([('clf', OneVsRestClassifier(RandomForestClassifier(), n_jobs=1))])\n\naccuracy = 0\nf1 = 0\nhl = 0\nauc_roc = 0\n\nfor category in sf_cols[:]:\n#     print('**Processing {} titles...**'.format(category))\n    \n    # Training logistic regression model on train data\n    RF_pipeline.fit(X_train, y_train[category])\n    \n    # calculating test accuracy\n    prediction = RF_pipeline.predict(X_val)\n    accuracy = accuracy + accuracy_score(y_val[category],prediction)\n    f1 = f1 + f1_score(y_val[category],prediction, average=\"micro\")\n    hl = hl + hamming_loss(y_val[category],prediction)\n    auc_roc = auc_roc + roc_auc_score(y_val[category], prediction)\n#     print(\"Accuracy = \",accuracy_score(y_val[category],prediction))\n#     print(\"F1 = \",f1_score(y_val[category],prediction, average=\"micro\"))\n#     print(\"Hamming loss = \",hamming_loss(y_val[category],prediction))\n#     print('AUC-ROC is {}'.format(roc_auc_score(y_val[category],prediction)))\n#     print(\"\\n\")\n\nprint('Test averaged Accuracy is {}'.format(accuracy\/len(sf_cols[:])))\nprint('Test averaged F1 is {}'.format(f1\/len(sf_cols[:])))\nprint('Test averaged Hamming Loss is {}'.format(hl\/len(sf_cols[:])))\nprint('Test averaged AUC-ROC is {}'.format(auc_roc\/len(sf_cols[:])))","07334111":"ovr_rf_result = [\"One vs. Rest (RF)\", accuracy\/len(sf_cols[:]), f1\/len(sf_cols[:]), hl\/len(sf_cols[:]), auc_roc\/len(sf_cols[:])]\nall_models_results.loc[7] = ovr_rf_result\nall_models_results","c576840a":"from skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = BinaryRelevance(GaussianNB())\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_val)\naccuracy_score(y_val,predictions)\nprint(\"Accuracy = \",accuracy_score(y_val,predictions.toarray()))\nprint(\"F1 = \",f1_score(y_val,predictions.toarray(), average=\"micro\"))\nprint(\"Hamming loss = \",hamming_loss(y_val,predictions.toarray()))\nprint('AUC ROC is {}'.format(roc_auc_score(y_val,predictions.toarray())))","cbd5a887":"br_result = [\"Binary Relevance\", accuracy_score(y_val,predictions.toarray()), f1_score(y_val,predictions.toarray(), average=\"micro\"), hamming_loss(y_val,predictions.toarray()), roc_auc_score(y_val,predictions.toarray())]\nall_models_results.loc[8] = br_result\nall_models_results","c82c9e87":"X_bert = sf_full['Book_Description']","b28c52f2":"y","e18aa605":"X_train, X_val, y_train, y_val = train_test_split(X_bert.values, y['Space (Space Opera) '].values, random_state=0, test_size=0.20, shuffle=True)","19558a74":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","de9e4162":"    import torch\n    torch.cuda.empty_cache()","4fbd3cf3":"# !pip install transformers","32d26481":"import re\ndef text_preprocessing(text):\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n    text = re.sub(r'[0-9]+' , '' ,text)\n    text = re.sub(r'\\s([@][\\w_-]+)', '', text).strip()\n    text = re.sub(r'&amp;', '&', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    text = text.replace(\"#\" , \" \")\n    encoded_string = text.encode(\"ascii\", \"ignore\")\n    decode_string = encoded_string.decode()\n    return decode_string","9189d986":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ndef preprocessing_for_bert(data):\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in data:\n        encoded_sent = tokenizer.encode_plus(\n            text=text_preprocessing(sent),  # Preprocess sentence\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n            max_length=MAX_LEN,                  # Max length to truncate\/pad\n            pad_to_max_length=True,         # Pad sentence to max length\n            #return_tensors='pt',           # Return PyTorch tensor\n            truncation = True,\n            return_attention_mask=True      # Return attention mask\n            )\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n    \n    return input_ids, attention_masks\n","7f216401":"MAX_LEN = 300\n\ntoken_ids = list(preprocessing_for_bert([X_bert.values[0]])[0].squeeze().numpy())\nprint('Original: ', X_bert.values[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)\n","04234395":"\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 32\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","203cba55":"%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in, H, D_out = 768, 50, 2\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        # self.LSTM = nn.LSTM(D_in,D_in,bidirectional=True)\n        # self.clf = nn.Linear(D_in*2,2)\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            # nn.LSTM(D_in,D_in)\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","69c000d1":"from transformers import AdamW, get_linear_schedule_with_warmup\n\ndef initialize_model(epochs=4):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False)\n\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","a53dc51d":"import random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n            #b_labels = b_labels.long().squeeze(1)\n            #p#rint(b_labels)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            #loss = criterion(logits, target)\n            loss = loss_fn(logits, b_labels.long())\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss \/ batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss \/ len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels.long())\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy ","63175851":"import gc\n#del variables\ngc.collect()","b9d64a2e":"set_seed(42)    # Set seed for reproducibility\nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","b0dce0d1":"import torch.nn.functional as F","6acfccee":"def bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","84c77dfd":"from sklearn.metrics import accuracy_score, roc_curve, auc\n\ndef evaluate_roc(probs, y_true):\n    \"\"\"\n    - Print AUC and accuracy on the test set\n    - Plot ROC\n    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n    \"\"\"\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    print(\"F1 = \",f1_score(y_true, y_pred, average=\"micro\"))\n    print(\"Hamming loss = \",hamming_loss(y_true, y_pred))\n    \n    # Plot ROC AUC\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","7c7bfcd3":"probs = bert_predict(bert_classifier, val_dataloader)\nevaluate_roc(probs, y_val)\n","7310e8b9":"# PATH = 'bert_spaceopera_2.pth'","e109f297":"# torch.save(bert_classifier, PATH)","da1f7714":"#      from IPython.display import FileLink\n#      FileLink(r'bert_spaceopera_2.pth')","91e1d1b3":"sf_cols","11a1574f":"PATHS = ['..\/input\/bert-models-sf-books\/bert_dystopia.pth','..\/input\/bert-models-sf-books\/bert_timetravel.pth','..\/input\/bert-models-sf-books\/bert_cyberpunk.pth','..\/input\/bert-models-sf-books\/bert_robots.pth','..\/input\/bert-models-sf-books\/bert_aliens.pth','..\/input\/bert-models-sf-books\/bert_apocalyptic.pth','..\/input\/bert-models-sf-books\/bert_alternatehistory.pth','..\/input\/bert-models-sf-books\/bert_steampunk.pth','..\/input\/bert-models-sf-books\/bert_militarysf.pth','..\/input\/bert-models-sf-books\/bert_spaceopera.pth']","d3983312":"# Creating null matrix that will recieve the predictions of each model\nprd = np.zeros((X_val.shape[0],y.shape[1]))\nprd_proba = np.zeros((X_val.shape[0],y.shape[1]))\n\ni=0\n\nfor PATH in PATHS:\n    gc.collect()\n    model = torch.load(PATHS[i])\n    model.eval()\n\n    probs = bert_predict(model, val_dataloader)\n\n    preds = probs[:,1]\n    prd_proba[:,i] = preds\n    prd[:,i] = np.where(preds >= 0.5, 1, 0)\n    gc.collect()\n#     print(i)\n    i += 1\n\n","0c681125":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.20, shuffle=True)","83e1f1bc":"print(\"Accuracy = \",accuracy_score(y_val,prd))\nprint(\"F1 = \",f1_score(y_val,prd, average=\"micro\"))\nprint(\"Hamming loss = \",hamming_loss(y_val,prd))\nprint('AUC ROC score is ', roc_auc_score(y_val,prd_proba))","65a3665f":"th = []\nf = []\nham = []\nac = []\nrec = []\n\n# We make iterations to discover the best threshold value\n# Threshold says how to distinguish between 0\/1 label assignment.\nfor t in range (5,90): # threshold value\n y_pred_new = (prd_proba >= t\/100).astype(int)\n#  print(\"t =\" ,t\/100)\n#  print(\"Accuracy = \",accuracy_score(y_val,y_pred_new))\n#  print(\"F1 = \",f1_score(y_val,y_pred_new, average=\"micro\"))\n#  print(\"Hamming loss = \",hamming_loss(y_val,y_pred_new))\n th.append(t)\n ac.append(accuracy_score(y_val,y_pred_new))\n f.append(f1_score(y_val,y_pred_new, average=\"micro\"))\n ham.append(hamming_loss(y_val,y_pred_new))\n rec.append(recall_score(y_val,y_pred_new, average=\"micro\"))\nplt.rcParams[\"figure.figsize\"] = (12,6)\nwith plt.style.context('ggplot'):\n plt.plot(th, f)\n plt.plot(th, ham)\n plt.plot(th, ac)\n plt.plot(th,rec)\n plt.legend(['F1', 'Hamming loss', 'Accuracy', 'Recall'], loc='center left', fontsize = 14)\n plt.ylabel(\"metrics\", fontsize = 14)\n plt.xlabel(\"threshold\", fontsize = 14)\n plt.title(\"Classfier Chain Model\", fontsize = 18)\nplt.show()","d24822a1":"filtered_prd = (prd_proba >= 70\/100).astype(int)\nbert_result = [\"Bert Model_overall\", accuracy_score(y_val,filtered_prd), f1_score(y_val,filtered_prd, average=\"micro\"),  hamming_loss(y_val,filtered_prd), roc_auc_score(y_val,prd_proba)]\nall_models_results.loc[9] = bert_result\nall_models_results","4f3edd17":"     from IPython.display import FileLink\n     FileLink(r'bert_cyberpunk_2.pth')","b9192623":"# text = text_cyberpunk\n\n# paths = ['bert_timetravel.pth', 'bert_dystopia.pth', 'bert_cyberpunk.pth']\n\n# for path in paths:\n#     print (path)\n\n\n#     # Model class must be defined somewhere\n#     model = torch.load(path)\n#     model.eval()\n\n#     tt_inputs, tt_masks  = preprocessing_for_bert(text_dystopia)\n#     tt_inputs = tt_inputs.to(device)\n#     tt_masks = tt_masks.to(device)\n#     logits = model(tt_inputs, tt_masks)\n#     print(F.softmax(logits, dim=1).cpu().detach().numpy()[:, 1][0])","560f1c6f":"# test_inputs, test_masks = preprocessing_for_bert(test_csv.comment_text)\n\n# # Create the DataLoader for our test set\n# test_dataset = TensorDataset(test_inputs, test_masks)\n# test_sampler = SequentialSampler(test_dataset)\n# test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)","15bf254f":"test_average_accuracy = (87.27 + 95.57 + 94.95 + 97.16 + 93.30 + 96.44 + 93.87 + 95.21 + 96.55 + 94.02)\/1000\ntest_average_auc = (0.9286 + 0.9665 + 0.9477 + 0.9584 + 0.9584 + 0.9714 + 0.9531 + 0.9683 + 0.9828 + 0.9726)\/10\ntest_average_f1 = (0.872680412371134 + 0.9556701030927836 + 0.9494845360824742 + 0.9716494845360825 + 0.9329896907216495 + 0.9644329896907217 + 0.938659793814433 + 0.952061855670103 + 0.9654639175257732 + 0.9402061855670103)\/10\ntest_average_hamming_loss = (0.12731958762886597 + 0.0443298969072165 + 0.050515463917525774 + 0.028350515463917526 + 0.06701030927835051 + 0.03556701030927835 + 0.06134020618556701 + 0.04793814432989691 + 0.034536082474226806 + 0.05979381443298969)\/10","1df36e55":"bert_result = [\"Bert Model_binary_class_mean\", test_average_accuracy, test_average_f1,  test_average_hamming_loss, test_average_auc]\nall_models_results.loc[10] = bert_result\nall_models_results","4e7d4784":"text_dystopia = [\"To Health. To Life. To the Future. We are The Key. 'No touching today for a healthy tomorrow.' Elodie obeys The Key. Elodie obeys the rules. Elodie trusts in the system. At least, Elodie used to... Aidan is a rebel. Aidan doesn't do what he's told. Aidan just wants to be free. Aidan is on his last chance... After a pandemic wiped out most of the human race, The Key took power. The Key dictates the rules. They govern in order to keep people safe. But as Elodie and Aidan begin to discover there is another side to The Key, they realise not everything is as it seems. Rather than playing protector, The Key are playing God.\"]\n\ntext_time_travel = [\"At the gates between worlds. In a war outside of time. They fight for us. The man they took prisoner was not the Jonathan Tibbs they remembered. He said that time was running out. That the world faced an enemy it could not defeat. He said everything depended on whether or not they could make a leap of faith. Then, he asked them for five nuclear warheads. The Never Army is the final installment in The Chronicles of Jonathan Tibbs -- a mind-bending, genre crossing, multiple award winning, action-adventure trilogy.\"]\n\ntext_cyberpunk = [\"A corporate mercenary wakes in a reconstructed body, a beautiful woman by his side. Then Hosaka Corporation reactivates him, for a mission more dangerous than the one he\u2019s recovering from: to get a defecting chief of R&D\u2014and the biochip he\u2019s perfected\u2014out intact. But this proves to be of supreme interest to certain other parties\u2014some of whom aren\u2019t remotely human...\"]\n\ntext_robots = [\"The critically acclaimed author of the \u201cbold, innovating, and thrilling\u201d (Stephen King) novel The Twenty-Year Death and the \u201cbrilliant\u201d (Booklist, starred review) novel Barren Cove returns with a dark and compelling mystery set in the near future. Decimated by plague, the human population is now a minority. Robots\u2014complex AIs almost indistinguishable from humans\u2014are the ruling majority. Nine months ago, in a controversial move, the robot government opened a series of preserves, designated areas where humans can choose to live without robot interference. Now the preserves face their first challenge: someone has been murdered. Chief of Police Jesse Laughton on the SoCar Preserve is assigned to the case. He fears the factions that were opposed to the preserves will use the crime as evidence that the new system does not work. As he digs for information, robots in the outside world start turning up dead from bad drug-like programs that may have originated on SoCar land. And when Laughton learns his murder victim was a hacker who wrote drug-programs, it appears that the two cases might be linked. Soon, it\u2019s clear that the entire preserve system is in danger of collapsing. Laughton\u2019s former partner, a robot named Kir, arrives to assist on the case, and they soon uncover shocking secrets revealing that life on the preserve is not as peaceful as its human residents claim. But in order to protect humanity\u2019s new way of life, Laughton must solve this murder before it\u2019s too late. The Preserve is a fresh and futuristic mystery that is perfect for fans of Westworld and Blade Runner.\"]\n\ntext_aliens = [\"Peacetime can be a rough adjustment for the battle-hardened Merry Band of Pirates. Especially when aliens don\u2019t get the memo that the shooting is over.\"]\n\ntext_apocalyptic = [\"Raised in isolation at Heavenly Shepherd, her family\u2019s trailer-dealership-turned-survival compound, Ami Miles knows that she was lucky to be born into a place of safety after the old world ended and the chaos began. But when her grandfather arranges a marriage to a cold-eyed stranger, she realizes that her \u201cdestiny\u201d as one of the few females capable of still bearing children isn\u2019t something she\u2019s ready to face. With the help of one of her aunts, she flees the only life she\u2019s ever known, and sets off on a quest to find her long-lost mother (and hopefully a mate of her own choosing). But as she journeys, Ami discovers many new things about the world... and about herself. \"]\n\ntext_alternate_history = [\"AN INSTANT NEW YORK TIMES BESTSELLER! Marie Benedict, the beloved New York Times bestselling author of The Only Woman in the Room, uncovers the untold story of Agatha Christie\u2019s mysterious eleven day disappearance. In December 1926, Agatha Christie goes missing. Investigators find her empty car on the edge of a deep, gloomy pond, the only clues some tire tracks nearby and a fur coat left in the car\u2014strange for a frigid night. Her husband and daughter have no knowledge of her whereabouts, and England unleashes an unprecedented manhunt to find the up-and-coming mystery author. Eleven days later, she reappears, just as mysteriously as she disappeared, claiming amnesia and providing no explanations for her time away. The puzzle of those missing eleven days has persisted. With her trademark exploration into the shadows of history, acclaimed author Marie Benedict brings us into the world of Agatha Christie, imagining why such a brilliant woman would find herself at the center of such a murky story. What is real, and what is mystery? What role did her unfaithful husband play, and what was he not telling investigators? A master storyteller whose clever mind may never be matched, Agatha Christie\u2019s untold history offers perhaps her greatest mystery of all.\"]\n\ntext_hard_sf = [\"Humanity is struggling to hold out against a hostile takeover by an alien race that claims to be on a religious mission to bring all sentient life to its God at the End of Time. But while things may look grim in the immediate aftermath of the attack, mankind is busy playing an even longer game than the aliens may have expected. Will a strategy millennia in the making finally be enough to defeat this seemingly unstoppable enemy? And what secrets are the aliens truly hiding in their most deeply protected stronghold?With his trademark optimism about humanity's tenacity and capacity for greatness, Peter F. Hamilton ends this brilliant saga with a bang--and reveals a few startling surprises along the way.\"]\n\ntext_steampunk = [\"The letter was short. A name, a time, a place. Marion Lane and the Midnight Murder plunges readers into the heart of London, to the secret tunnels that exist far beneath the city streets. There, a mysterious group of detectives recruited for Miss Brickett\u2019s Investigations & Inquiries use their cunning and gadgets to solve crimes that have stumped Scotland Yard. Late one night in April 1958, a filing assistant for Miss Brickett\u2019s named Michelle White receives a letter warning her that a heinous act is about to occur. She goes to investigate but finds the room empty. At the stroke of midnight, she is murdered by a killer she can\u2019t see\u2014her death the only sign she wasn\u2019t alone. It becomes chillingly clear that the person responsible must also work for Miss Brickett\u2019s, making everyone a suspect. Almost unwillingly, Marion Lane, a first-year Inquirer-in-training, finds herself being drawn ever deeper into the investigation. When her friend and mentor is framed for the crime, to clear his name she must sort through the hidden alliances at Miss Brickett\u2019s and secrets dating back to WWII. Masterful, clever and deliciously suspenseful, Marion Lane and the Midnight Murder is a fresh take on the Agatha Christie\u2014style locked-room mystery with an exciting new heroine detective at the helm.\"]\n\ntext_military_sf = [\"The battle against the Lankies has been won. Earth seems safe. Peacetime military? Not on your life. It\u2019s been four years since Earth threw its full military prowess against the Lanky incursion. Humanity has been yanked back from the abyss of extinction. The solar system is at peace. For now. The future for Major Andrew Grayson of the Commonwealth Defense Corps and his wife, Halley? Flying desk duty on the front. No more nightmares of monstrous things. No more traumas to the mind and body. But when an offer comes down from above, Andrew has to make a choice: continue pushing papers into retirement, or jump right back into the fight? What\u2019s a podhead to do? The remaining Lankies may have retreated in fear, but the threat isn\u2019t over. They need to be wiped out for good before they strike again. That\u2019ll take a new offensive deployment. Aboard an Avenger warship, Andrew and the special tactics team under his command embark on the ultimate search-and-destroy mission. This time, it\u2019ll be on Lanky turf. No big heroics. No unnecessary risks. Just a swift hit-and-run raid in the hostile Capella system. Blow the alien seed ships into oblivion and get the hell back to Earth. At least, that\u2019s the objective. But when does anything in war go according to plan?\"]\n\ntext_space_opera = ['Queen Samara Rani knows she\u2019s likely walking into a trap, but agreeing to meet with Commander Adams is the fastest way to get within striking distance of the Quint Confederacy\u2019s biggest traitor and her sworn enemy. Adams attacked her home and destroyed her ship, and if he\u2019s not stopped, he will ruin the tentative peace between the Kos Empire and the Quint Confederacy\u2014and Samara\u2019s chance at future with Emperor Valentin Kos. Samara is determined to serve up some well-deserved payback, but she is no longer a lone assassin, and despite her protests, her friends and allies refuse to let her undertake such a perilous mission without them. Even Valentin, usually the voice of reason, refuses to stay behind. Samara is loath to put her friends in danger, and taking a team carries its own risks, so she makes plans to keep them safe, no matter what the cost. When Adams threatens that safety, and everything she holds dear, Samara vows to show him exactly how she earned her deadly reputation\u2014and why one should never cross the Rogue Queen.']\n\ntexts = [text_dystopia, text_time_travel, text_cyberpunk, text_robots, text_aliens, text_apocalyptic, text_alternate_history,  text_steampunk, text_military_sf, text_space_opera]\n\ntexts_category = ['dystopia', 'time_travel', 'cyberpunk', 'robots', 'aliens', 'apocalyptic', 'alternate_history', 'steampunk', 'military_sf', 'space_opera']\n","98718bb8":"#sf_full[sf_full.Book_Description.str.contains(text_space_opera[0])]","c3e44e86":"\n# n = 0\n\n# for text in texts:\n#     ### Give your book summary to the text variable:\n    \n\n\n#     text_vector = vectorizer.transform(text)\n#     preds = lp_classifier.predict(text_vector).toarray()\n    \n#     print('Genre: ' + '\\n' +  str(texts_category[n]) + '\\n')\n    \n#     print('Description: ' + '\\n' +  str(text[0]) + '\\n')\n\n#     i = 0\n#     for col in sf_cols:\n#         if preds[0][i] > 0:        \n#             print(col + \": \" + str(preds[0][i]))\n\n#         if preds[0][i] == 0:        \n#             print(col + \": \" + str(preds[0][i]))\n\n\n#         i =  i + 1\n#     print('\\n')\n    \n#     n += 1","a7e8b5b7":"n = 0\nt=61\n\nfor text in texts:\n    ### Give your book summary to the text variable:\n    \n    text_vector = vectorizer.transform(text)\n\n    cc_predictions_proba = cc_classifier.predict_proba(text_vector)\n    \n    y_pred_new = (cc_predictions_proba >= t\/100).astype(int)\n\n    preds = y_pred_new.toarray()\n    \n    print('Genre: ' + '\\n' +  str(texts_category[n]) + '\\n')\n    \n    print('Description: ' + '\\n' +  str(text[0]) + '\\n')\n\n    i = 0\n    for col in sf_cols:\n        if preds[0][i] > 0:        \n            print(col + \": \" + str(preds[0][i]))\n\n        if preds[0][i] == 0:        \n            print(col + \": \" + str(preds[0][i]))\n\n\n        i =  i + 1\n    print('\\n')\n    \n    print(preds[0][9])\n    print(n)\n    if preds[0][n] == 1:\n        print('Main Genre detected', '\\n')\n    else:\n        print('Missed the main genre', '\\n')\n        \n    \n    n += 1","f5c0d275":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.20, shuffle=True)\n\nn = 0\n\n\n\nfor text in texts:\n    ### Give your book summary to the text variable:\n    LR_pipeline = Pipeline([('clf', OneVsRestClassifier(LogisticRegression(C= 3, class_weight='balanced', solver='sag'), n_jobs=-1))])\n    \n    text_vector = vectorizer.transform(text)\n    \n    preds = []\n    \n    for category in sf_cols[:]:\n        #print('**Processing {} titles...**'.format(category))\n\n        # Training logistic regression model on train data\n        LR_pipeline.fit(X_train, y_train[category])\n\n        # calculating test accuracy\n        prediction = LR_pipeline.predict(text_vector)\n        \n        preds.append(int(prediction[0]))\n    \n        \n    \n    print('Genre: ' + '\\n' +  str(texts_category[n]) + '\\n')\n    \n    print('Description: ' + '\\n' +  str(text[0]) + '\\n')\n    \n\n    i = 0\n    for col in sf_cols:\n        if preds[i] > 0:        \n            print(col + \": \" + str(preds[i]))\n\n        if preds[i] == 0:        \n            print(col + \": \" + str(preds[i]))\n\n\n        i =  i + 1\n    print('\\n')\n    \n    if preds[n] == 1:\n        print('Main Genre detected', '\\n')\n    else:\n        print('Missed the main genre', '\\n')\n        \n    \n    n += 1","92aee9f2":"j=0\nfor text in texts:\n    i=0\n    print(texts_category[j])\n    tt_inputs, tt_masks  = preprocessing_for_bert(text)\n    tt_inputs = tt_inputs.to(device)\n    tt_masks = tt_masks.to(device)\n    for PATH in PATHS:\n        # # Model class must be defined somewhere\n        model = torch.load(PATHS[i])\n        model.eval()\n        \n        logits = model(tt_inputs, tt_masks)\n\n        probs = F.softmax(logits, dim=1).cpu().detach().numpy()\n        preds = probs[:, 1]\n        y_pred = np.where(preds >= 0.5, 1, 0)\n        print(sf_cols[i], y_pred)\n        i += 1\n    print(\"\\n\")\n    j += 1","d43a9532":"Classifier chains is a machine learning method for problem transformation in multi-label classification. It combines the computational efficiency of the Binary Relevance method while still being able to take the label dependencies into account for classification.\n\nEach model makes a prediction in the order specified by the chain using all of the available features provided to the model plus the predictions of models that are earlier in the chain.","2afcd62b":"### Classifier Chain","cf3a72b9":"Multiclass classification means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time.\n\nMultilabel classification assigns to each sample a set of target labels. This can be thought of as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might be about any of religion, politics, finance or education at the same time or none of these.\n\nTaken from http:\/\/scikit-learn.org\/stable\/modules\/multiclass.html","20458f2f":"### Test the model","26cfd79c":"### Binary Relevance\n\nTransforms a multi-label classification problem with L labels into L single-label separate binary classification problems using the same base classifier provided in the constructor. The prediction output is the union of all per label classifiers\n\nThe problem with binary relevnce is that it doesn't take label dependency into account.\n\nhttps:\/\/www.youtube.com\/watch?v=vnObmjMWVd0","c88cc23b":"Let\u2019s apply models described above and compare the results. For selecting the best model we will measure:\n* **Accuracy** \u2014 accuracy is the most common metric. However, it can be largely contributed by a large number of True Negatives which we have in this case and in the most of businesss cases.\n\n>Accuracy (A): Accuracy for each instance is defined as the proportion of the predicted correct labels\nto the total number (predicted and actual) of labels for that instance. Overall accuracy is the average\nacross all instances.\n\n\n\n* **F1 score** \u2014 F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall and there is an uneven class distribution (large number of Actual Negatives) as in our example\n\n>Precision (P): Precision is the proportion of predicted correct labels to the total number of actual\nlabels, averaged over all instances\n\n>Recall (R): Recall is the proportion of predicted correct labels to the total number of predicted labels,\naveraged over all instances.\n\n* **Hamming loss** \u2014 the fraction of labels that are incorrectly predicted. Brings additional information in case of unbalanced datasets.\n\n>Hamming Loss (HL): Hamming Loss reports how many times on average, the relevance of an\nexample to a class label is incorrectly predicted [44]. Therefore, hamming loss takes into account the\nprediction error (an incorrect label is predicted) and the missing error (a relevant label not predicted),\nnormalized over total number of classes and total number of examples.\n\n**The higher the value of accuracy, precision, recall and F1- score, the better the performance of the learning algorithm.**\n\n**Ideally, we would expect hamming loss, HL = 0, which would\nimply no error; practically the smaller the value of hamming loss, the better the performance of the learning algorithm.**\n\n\nhttps:\/\/www.researchgate.net\/profile\/Mohammad_Sorower\/publication\/266888594_A_Literature_Survey_on_Algorithms_for_Multi-label_Learning\/links\/58d1864392851cf4f8f4b72a\/A-Literature-Survey-on-Algorithms-for-Multi-label-Learning.pdf","a6f22814":"#### Exploring the feature importance of our binary classifiers","16494a34":"### Alternate History df subgenres distribution","6d237f5c":"### Alternate Universe df subgenres distribution","8e0b5465":"### BERT Model\n\n\ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone.\n\n\ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets then share them with the community on our model hub. At the same time, each python module defining an architecture can be used as a standalone and modified to enable quick research experiments.\n\n\ud83e\udd17 Transformers is backed by the two most popular deep learning libraries, PyTorch and TensorFlow, with a seamless integration between them, allowing you to train your models with one then load it for inference with the other.\n\nhttps:\/\/github.com\/huggingface\/transformers","3dff4dfe":"### Removing stopwords\nStopwords are the words in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on.","295b06d6":"## Robots df subgenres distribution","8c9f9fea":"## Printing an example of the data we will use","9e498663":"Robots look like a small subgenre of SF but still a subgenre in itself, though heavily linked to dystopia.\n\nAccording to goodreads:\n> A robot is a machine\u2014especially one programmable by a computer\u2014capable of carrying out a complex series of actions automatically. Robots can be guided by an external control device or the control may be embedded within. Robots may be constructed to take on human form but most robots are machines designed to perform a task with no regard to how they look.\n","973a2d69":"### We define a function that use the two others and select a subset of genres","67d0f661":"#### Optimizing the probabilistic model with a probability threshold","656d4555":"\n#### Evaluating each binary classifier independently","8fdec61f":"### Multilabel k Nearest Neighbours\nMLkNN module uses kNN classification method adapted for multi-label classification.\n![image.png](attachment:image.png)\n\nMLkNN builds uses k-NearestNeighbors find nearest examples to a test class and uses Bayesian inference to select assigned labels.\n>Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.\n\nAdvantages:\n- Often a better accuracy than other methods\n- Correlations between labels are considered\n\nDrawbacks:\n- Data distributions for some labels are unbalanced\n","a1b8cda8":"#### One vs. Rest with Logistic Regression","cc52ac5c":"The Dystopia subgenre dataset mainly contains Dystopia assigned books, it can be a main SF subgenre in itself. \n\nAccording to goodreads:\n>Dystopia is a form of literature that explores social and political structures. It is a creation of a nightmare world - unlike its opposite, Utopia, which is an ideal world.\nDystopia is often characterized by an authoritarian or totalitarian form of government. It often features different kinds of repressive social control systems, a lack or total absence of individual freedoms and expressions, and a state of constant warfare or violence.\nMany novels combine both Dystopia and Utopia, often as a metaphor for the different directions humanity can take in its choices, ending up with one of the two possible futures.\nDystopia is very similar to False Utopia, but but instead of the often visibly oppressive and\/or anarchic \"true\" Dystopia, a False Utopia appears inviting at first and indeed may well be a nice place to live in but hides a dark and often terrible secret beneath its innocent exterior.","53958451":"The Cyberpunk subgenre dataset mainly contains Cyberpunk assigned books, it can be a main SF subgenre in itself. \nDystopia is heavy linked with Cyberpunk, this makes sense.\n\nAccording to goodreads:\n>Cyberpunk is a subgenre of science fiction in a future setting that tends to focus on society as \"high tech low life\" featuring advanced technological and scientific achievements, such as information technology and cybernetics, juxtaposed with a degree of breakdown or radical change in the social order.\nCyberpunk plots often center on conflict among artificial intelligences, hackers, and among megacorporations, and tend to be set in a near-future Earth, rather than in the far-future settings or galactic vistas found in novels such as Isaac Asimov's Foundation or Frank Herbert's Dune. The settings are usually post-industrial dystopias but tend to feature extraordinary cultural ferment and the use of technology in ways never anticipated by its original inventors (\"the street finds its own uses for things\"). Much of the genre's atmosphere echoes film noir, and written works in the genre often use techniques from detective fiction.\nClassic cyberpunk characters were marginalized, alienated loners who lived on the edge of society in generally dystopic futures where daily life was impacted by rapid technological change, an ubiquitous datasphere of computerized information, and invasive modification of the human body","b6f5c671":"The Space Opera subgenre dataset mainly contains Space Opera assigned books, it can be a main SF subgenre in itself. \nSometimes Military SF and\/or Aliens are also assigned, this makes sense.\n\n>Space opera is a subgenre of speculative fiction or science fiction that emphasizes romantic, often melodramatic adventure, set mainly or entirely in space, generally involving conflict between opponents possessing powerful (and sometimes quite fanciful) technologies and abilities. Perhaps the most significant trait of space opera is that settings, characters, battles, powers, and themes tend to be very large-scale.","5c7b84a6":"While exploring the different genres SF subsets, we will need to select some genres and sometimes exclude others.","ca3086c1":"### Use regex to preprocess the text","1acca36e":"### Plotting the distribution of subgenres of our dataset","ac7cf89d":"Looking at the representation of each subgenres in our dataset, we could try to implement stratified sampling later.\nhttp:\/\/scikit.ml\/stratification.html","a2cf3ecc":"### Military SF df subgenres distribution","54bcf356":"### Apocalyptic df subgenres distribution","76849def":"#### Bert Model","31eea470":"### Print an example of description","f4606dea":"### Label Powerset\nThis method is one of the least used in the field but it has some advantages.\n\n>Label Powerset is a problem transformation approach to multi-label classification that transforms a multi-label problem to a multi-class problem with 1 multi-class classifier trained on all unique label combinations found in the training data.\n\n>The method maps each combination to a unique combination id number, and performs multi-class classification using the classifier as multi-class classifier and combination ids as classes.\n\nThe advantage of this method is that it is the only one that enable us to make a ranking between the labels.\n\nOn the other hand, this method has several drawbacks:\n\n* The problem with this technique is its computational complexity (for instance, on our dataset, it is very long to compute).\n* This cannot predict unseen labels (the model naturally overfits).\n\nhttps:\/\/www.youtube.com\/watch?v=g1xUgdCozJ8","2b439ca1":"## Datasets SF Subgenres Exploration","4a820c63":"## Cleaning our new dataset","e9712dc4":"## Aliens df subgenres distribution","3a5a6f72":"### Defining a function to make binaries from every genre data","d3bbd123":"#### Label Powerset Test","32e0afa3":"### Getting rid of foreing edition books (resulting in non-english book description)","878dab4a":"The Alternate History subgenre dataset mainly contains Alternate History assigned books, it can be a main SF subgenre in itself. \nSometimes dystopia and\/or time-travel and\/or Steampunk are also assigned, this makes sense.\n\nAccording to goodreads:\n> Alternate history is a subgenre of speculative fiction (or science fiction) and historical fiction that is set in a world in which history has diverged from the actual history of the world. Alternate history literature asks the question, \"What if history had developed differently?\" Most works in this genre are based on real historical events, yet feature social, geopolitical, or industrial circumstances that developed differently than our own. ","32bd870d":"### Lemmatization\n\nWhile stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always.\n\n**Lemmatization**, on the other hand, takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries (like WordNetLemmatizer) which the algorithm can look through to link the form back to its lemma.","fcfe30f4":"The Apocalyptic subgenre dataset mainly contains Dystopia and Apocalyptic assigned books, since apocalyptic contains dystopia but dystopia doesn't imperatively contains it, so it can be a sub SF subgenre in itself. \n\nAccording to goodreads:\n>Apocalyptic is from the word apocalypse, referring to the end of the world.\nApocalyptic fiction focuses on the end of civilization either through nuclear war, plague, or other global catastrophic risk.\nApocalyptic literature is a genre of religious writing centered on visions of the end of time.\nMany apocalyptic stories focus on stories that are on the brink of the end of the world of the civilization. Post-apocalyptic fiction is set in a world or civilization after such a disaster. \n","ca5cc0fc":"### Save the models","b9c4ceea":"### Loading the different subgenres dataset","540a642c":"### Getting rid of reluctant foreign language book descriptions","10f9610f":"## Results of BERT for different subgenres\n\n### Dystopia:\n\n``\nAUC: 0.9286\nAccuracy: 87.27%\nF1 =  0.872680412371134\nHamming loss =  0.12731958762886597\n``\n\n### Time Travel:\n\n``\nAUC: 0.9665\nAccuracy: 95.57%\nF1 =  0.9556701030927836\nHamming loss =  0.0443298969072165\n`` \n\n### Cyberpunk:\n``\nAUC: 0.9477\nAccuracy: 94.95%\nF1 =  0.9494845360824742\nHamming loss =  0.050515463917525774\n``\n\n### Robots:\n``\nAUC: 0.9584\nAccuracy: 97.16%\nF1 =  0.9716494845360825\nHamming loss =  0.028350515463917526\n``\n\n### Aliens:\n``\nAUC: 0.9584\nAccuracy: 93.30%\nF1 =  0.9329896907216495\nHamming loss =  0.06701030927835051\n``\n\n### Apocalyptic:\n``\nAUC: 0.9714\nAccuracy: 96.44%\nF1 =  0.9644329896907217\nHamming loss =  0.03556701030927835\n``\n\n### Alternate History:\n``\nAUC: 0.9531\nAccuracy: 93.87%\nF1 =  0.938659793814433\nHamming loss =  0.06134020618556701\n``\n\n### Steampunk:\n``\nAUC: 0.9683\nAccuracy: 95.21%\nF1 =  0.952061855670103\nHamming loss =  0.04793814432989691\n``\n\n### Military SF:\n``\nAUC: 0.9828\nAccuracy: 96.55%\nF1 =  0.9654639175257732\nHamming loss =  0.034536082474226806\n``\n\n### Space Opera:\n``\nAUC: 0.9726\nAccuracy: 94.02%\nF1 =  0.9402061855670103\nHamming loss =  0.05979381443298969\n``\n\n","4c0a07ff":"#### One vs. Rest with Random Forest Classifier","43cc8a14":"### We extract subgenres from our new dataset","356731dc":"We will not compute it because it would take too much time (approximately 40 min), but here are the results:\n\n`````\nAccuracy =  0.6025477707006369\nF1 score =  0.7352309344790549\nHamming loss =  0.06280254777070064\nAUC ROC score is  0.9611438821255189\n`````","871b2927":"### Plotting the number of subgenres per books for our dataset","d80aa475":"I'll first create a dataframe that will register the words of the bag of words created with the tf-idf vectorizer. We will later complete this dataframe with the importance of each words for a specific subgenre.","be46baee":"### Dropping url (meaning books) that appeared several times over the different subgenres dataset ","ca21c381":"![image.png](attachment:image.png)","bcb4bf23":"### WORD CLOUD","54a48fc7":"* A chain of binary classifiers C0, C1, . . . , Cn is constructed, where a classifier Ci uses the predictions of all the classifier Cj , where j < i. This way the method, also called classifier chains (CC), can take into account label correlations.\n* The total number of classifiers needed for this approach is equal to the number of classes, but the training of the classifiers is more involved.\n* Following is an illustrated example with a classification problem of three categories {C1, C2, C3} chained in that order.\n\nAdvantages :\n- label correlation taken into consideration \n- acceptable computational complexity \n\nDrawbacks :\n- accuracy heavily depends on the order\n- for n labels there are n! possible orders \nhttps:\/\/towardsdatascience.com\/journey-to-the-center-of-multi-label-classification-384c40229bff","868a0dd6":"To use Transformers, we need to:\n\n1. Add special tokens to the start and end of each sentence.\n2. Pad & truncate all sentences to a single constant length.\n3. Explicitly differentiate real tokens from padding tokens with the \u201cattention mask\u201d.","52303a7c":"### We combine the two datasets in one","e742d4e0":"Military sf is heavily linked to space opera but we will try to consider it as a subgenre in itself.\n\nAccording to goodreads:\n>Military science fiction is a subgenre of science fiction that features the use of science fiction technology, mainly weapons, for military purposes and usually principal characters that are members of a military organization involved in military activity; occurring sometimes in outer space or on a different planet or planets. It exists in literature, comics, film and video games.\nA detailed description of the conflict, the tactics and weapons used for it, and the role of a military service and the individual members of that military organization forms the basis for a typical work of military science fiction. The stories often use features of actual past or current Earth conflicts, with countries being replaced by planets or galaxies of similar characteristics, battleships replaced by space battleships and certain events changed so that the author can extrapolate what might have occurred.","1252ec36":"## Preprocessing the data","9dc4ea1d":"#### Classifier Chain test","10ff90bf":"SETTING UP THE GPU IF POSSIBLE","df3154c4":"With only ~250 books assigned to Hard SF and a lot more to space opera, I will not retain Hard SF as a main subgenre of SF.\n\nAccording to goodreads:\n> Hard science fiction is a category of science fiction characterized by an emphasis on scientific or technical detail, or on scientific accuracy, or on both. It is characterized by rigorous attention to accurate detail in the natural sciences, especially physics, astrophysics, and chemistry, or on accurately depicting worlds that more advanced technology may make possible. Hard science fiction is driven more by ideas than characterization. Plausible science and technology are central to the plot.","c30cb012":"#### Installing the Hugging Face Library","18aa7f3e":"### Getting rid of Nones genres\nAs indicated in the metadata of the dataset, the books with the \"None\" key in the genres dictionnary are books that are assigned to no genres, meaning that they could in reality belong to any genre. \n\nThus, keeping them will just add noise to our model and that's not what we want. ","444954a9":"### One VS Rest CLassifier (same as One vs All)\nWith One vs Rest classifier we make a binary classification of each class (considered our first class) vs. all the other class grouped together (considered our second class)\n![image.png](attachment:image.png)\nExplanation of the model here : https:\/\/www.youtube.com\/watch?v=V8fS0T_ktn4","acf9a9e8":"Steampunk seems to be a main subgenre of SF in itself, sometimes linked to alternate-history and dystopia.\n\nAccording to goodreads:\n>Steampunk is a subgenre of speculative fiction, usually set in an anachronistic Victorian or quasi-Victorian alternate history setting. It could be described by the slogan \"What the past would look like if the future had happened sooner.\" The term denotes works set in an era or world where STEAM POWER is still widely used\u2014usually the 19th century, and often set in Victorian era England\u2014but with prominent elements of either science fiction or fantasy, such as fictional technological inventions like those found in the works of H. G. Wells and Jules Verne, or real technological developments like the computer occurring at an earlier date.\nIt is a subgenre of fantasy and speculative fiction that came into prominence in the 1980s and early 1990s.\nDieselpunk takes over where Steampunk leaves off. These are stories that take over as we usher in the machine-heavy eras of WWI and WWII. The use of diesel-powered machines plays heavily. In this (like its steam counterpart), the focus is on the technology.","5aa86343":"## Steam Punk df subgenres distribution","0d417e69":"### We split our dataset into a train and validation dataset (80\/20)","b53ee991":"````````\nAccuracy =  0.4732484076433121\nF1 score =  0.653293575494714\nHamming loss =  0.08146496815286625\nAUC ROC =  0.8549072590225327\n````````","4f1471be":"#### One vs. Rest (LR)","523ea927":"Time travel seems well suited to be a subgenre in itself.\n\nAccording to goodreads:\n>Time travel is the concept of moving between different points in time in a manner analogous to moving between different points in space. Time travel could hypothetically involve moving backward in time to a moment earlier than the starting point, or forward to the future of that point without the need for the traveler to experience the intervening period (at least not at the normal rate). Time travel can form the central theme of a book or it can simply be a plot device to drive a story. Time travel in fiction can ignore the possible effects of the time traveler's actions or it can explote its repercussions (ie: paradoxes).","003b9b43":"### Classic Multi Label Text Classification methods","fabff3c9":"### Let's make some visualizations of our features and targets","b062ea5a":"We filter the subgenres by a minimum number of 5 people that tagged (we collected this information before) a book to reduce the bias of subjectivity introduced by human filled data.","f5a0fd4e":"### Print the genres associated with this description","1ea4f5ef":"## Space Opera df subgenres distribution","5e2703ef":"![image.png](attachment:image.png)","0db46988":"### Make some data exploration about the subgenres published these 20 last years","b486639c":"### First, we\u2019ll use TF-IDF for word embedding. \n\n#### Word embedding\n>Word embedding helps capture the semantic, syntactic context or a word\/term and helps understand how similar\/dissimilar it is to other terms in an article, blog, etc.\n\n>Word embedding implements language modeling and feature extraction based techniques to map a word to vectors of real numbers.\n\n#### TF-IDF\n>TF is the term abbreviation of Term Frequency, defined as the total number of times a term occurs in a document. TF is calculated using the number of times the term occurs in a document divided by the total number of terms. The method is quite intuitive. The more a term occurs in a document, the more significance this term holds against the document.\n\n>Since TF tends to emphasize the wrong term sometimes, IDF is introduced to balance the term weight. IDF, short for inverse document frequency, defined as how frequently a term occurs in the entire document. It is used to balance the weight of terms that occur in the entire document set. In other words, IDF reduces the weight of terms that occur frequently and increases the weight of terms that occur infrequently.\n\n>To get the final TF-IDF score, we need to multiply the results of TF and IDF. The larger the TF-IDF score is, the more relevant the term is in the documents.\n\n\nWe could have try with other word embedding methods like BOW, GloVe, Word2Vec, ELMo...","ffbb164b":"## Trying the best models\nWe are going to try the MLkNN on texts it has never seen (new releases from 2021) on different categories:","3a0c29be":"#### One vs. Rest with Linear SVC","afc46e3d":"## Time Travel df subgenres distribution","e57d8ea4":"### Hard SF df subgenres distribution","b533b7f9":"### LOGISTIC REGRESSION","7be9c04b":"## We untokenize our text to make some visualizations","5432e48b":"### Cyberpunk df subgenres distribution","81860f85":"The Aliens subgenre dataset mainly contains Aliens assigned books, it can be a main SF subgenre in itself. \nSometimes dystopia and\/or space opera are also assigned, this makes sense.\n\nAccording to goodreads:\n>Aliens are extraterrestrial life that does not originate from Earth. The sentient beings can range from simple bacteria-life organisms to complex species and individuals. Aliens are used in a variety of genres, most notably Science-Fiction.","e120a24d":"### Dystopia df subgenres distribution","23371f09":"Looking at the distribution plot of the subgenres, we can not conclude that alternate universe is a subgenre of SF in itself.\nBy nature Alternate Universe is only a composition of other subgenres (mainly dystopia, alternate-history, steampunk and time-travel).\n\nThis can be explained by the fact that alternate universe is mainly used in fantasy books while in SF we prefer to speak about alternate history.","6b1c328a":"# Test it with your own text description ! \nI created a web application so that you can directly test it with your text and get more information about the project.\nYou may have to wait a little since the application goes to sleep when nobody uses it.\n\nhttps:\/\/sf-books-analyser.herokuapp.com\/","524c211d":"### For a text","6c2c79ce":"### Conclusion of this subgenre exploration:\n* Aliens is a genre in itself\n* Alternate History is a genre in itself\n* **Alternate Universe is not a genre in itself**\n* Apocalyptic is a genre in itself\n* Cyberpunk is a genre in itself\n* Dystopia is a genre in itself\n* **Hard SF is not a genre in itself**\n* Military SF is a genre in itself\n* Robots is a genre in itself\n* Space Opera is a genre in itself\n* Steampunk is a genre in itself\n* Time Travel is a genre in itself","e32a2e80":"## Preprocessing the texts","76d0ca35":"### Plotting the number of words per book description","115a1ba2":"### Defining a function to extract the genres from the \"Genres\" column of a dataset"}}