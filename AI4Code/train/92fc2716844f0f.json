{"cell_type":{"ce550ff0":"code","ecad173e":"code","e136c25b":"code","fbfba929":"code","7a70db8c":"code","767b6731":"code","d66cfdd4":"code","218e9365":"code","29b31ed8":"code","697736cd":"code","e1878b7e":"code","6e4488f9":"code","b0b8ba5c":"code","929cb92c":"code","f6bab45b":"code","46f230d4":"code","66d3e1e8":"code","dbcade43":"code","73eada11":"code","41565be4":"code","4fcefcbf":"code","453f8143":"code","4da1e856":"code","8cb32bec":"code","300c7888":"code","d307b6e5":"code","1bd4bc52":"code","e7706aae":"code","1e50a5f6":"code","df6505be":"code","4e44176b":"code","6af7ac33":"code","c741819a":"code","9d282862":"code","852f72cd":"code","279f1356":"code","120fa54e":"code","e096ad6a":"markdown","5489fb70":"markdown","5aa42262":"markdown","bde1b937":"markdown","1cb332c3":"markdown"},"source":{"ce550ff0":"import numpy as np\nimport pandas as pd\nimport seaborn as sns    # For graphical representation\n\nimport itertools\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\n\nfrom sklearn import preprocessing\n%matplotlib inline\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\ndf_titanic = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', error_bad_lines=False, engine=\"python\", sep=\",\")\ndf_titanic.set_index(\"PassengerId\", inplace = True)\nprint(df_titanic.shape)\nprint(df_titanic.ndim)","ecad173e":"# There are three main methods of selecting columns in pandas:\n#   1. using a dot notation, e.g. data.column_name,\n#   2. using square braces and the name of the column as a string, e.g. data['column_name'] or\n#   3. using numeric indexing and the iloc selector data.iloc[:, <column_number>]\n\n# For selecting rows:\n#   1. numeric row selection using the iloc selector, e.g. data.iloc[0:10, :] \u2013 select the first 10 rows.\n#   2. label-based row selection using the loc selector \n#              (this is only applicably if you have set an \u201cindex\u201d on your dataframe. e.g. data.loc[44, :]\n#   3. logical-based row selection using evaluated statements, e.g. data[data[\"Area\"] == \"Ireland\"]\n#               \u2013 select the rows where Area value is \u2018Ireland\u2019.\ndf_titanic.iloc[0:3, :]","e136c25b":"print(df_titanic.isna().sum())","fbfba929":"print(df_titanic.dtypes)","7a70db8c":"print(df_titanic['Survived'].unique())","767b6731":"print(df_titanic['Sex'].unique())","d66cfdd4":"dict_gender_map = {\n  'male': 0,\n  'female': 1\n}\n\ndef gender_to_numeric(gender):\n  return dict_gender_map[gender]\n\ndf_titanic['Sex'] = df_titanic['Sex'].apply(gender_to_numeric)","218e9365":"print(df_titanic['Embarked'].unique())","29b31ed8":"dict_embarked_map = {\n  'S': 0,\n  'C': 1,\n  'Q': 2,\n  np.nan: -1\n}\n\ndef embarked_to_numeric(embarked):\n  return dict_embarked_map[embarked]\n\ndf_titanic['Embarked'] = df_titanic['Embarked'].apply(embarked_to_numeric)","697736cd":"df_titanic['Age'].fillna(np.mean(df_titanic['Age'][:]), inplace = True)","e1878b7e":"# To use scikit-learn library, we have to convert the Pandas data frame to a Numpy array:\nX = df_titanic.drop(['Survived', 'Name', 'Ticket', 'Cabin'], axis=1).values\nY = df_titanic['Survived']","6e4488f9":"print(X[0:10])\nprint(Y[0:10])","b0b8ba5c":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=4)\nprint ('Train set:', X_train.shape,  Y_train.shape)\nprint ('Test set:', X_test.shape,  Y_test.shape)","929cb92c":"from sklearn import metrics","f6bab45b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix","46f230d4":"LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train, Y_train)\nY_LR_predicted = LR.predict(X_test)\nY_LR_predicted_prob = LR.predict_proba(X_test)\nY_LR_predicted[0:5]","66d3e1e8":"print(\"Train set Accuracy: \", metrics.accuracy_score(Y_train, LR.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(Y_test, Y_LR_predicted))","dbcade43":"from sklearn.neighbors import KNeighborsClassifier","73eada11":"def bestK(max_number_of_Ks):\n  mean_acc = np.zeros((max_number_of_Ks-1))\n  std_acc = np.zeros((max_number_of_Ks-1))\n\n  ConfusionMatrix = [];\n  for n in range(1, max_number_of_Ks):\n      neigh         = KNeighborsClassifier(n_neighbors = n).fit(X_train, Y_train)\n      Y_predicted   = neigh.predict(X_test)\n      mean_acc[n-1] = metrics.accuracy_score(Y_test, Y_predicted)\n      std_acc[n-1]  = np.std(Y_predicted == Y_test)\/np.sqrt(Y_predicted.shape[0])\n  \n  return (mean_acc.argmax() + 1, mean_acc.max())","41565be4":"k = bestK(25)[0]\nprint(\"k =\", k)\n# Train Model and Predict with the best K\nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train, Y_train)\nY_bestK_predicted = neigh.predict(X_test)\nY_bestK_predicted[0:5]","4fcefcbf":"print(\"Train set Accuracy: \", metrics.accuracy_score(Y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(Y_test, Y_bestK_predicted))","453f8143":"from sklearn.tree import DecisionTreeClassifier","4da1e856":"def bestMax_Depth(max_number_of_Max_Depth):\n  mean_acc = np.zeros((max_number_of_Max_Depth-1))\n  std_acc = np.zeros((max_number_of_Max_Depth-1))\n\n  ConfusionMatrix = [];\n  for n in range(1, max_number_of_Max_Depth):\n      tree          = DecisionTreeClassifier(criterion=\"entropy\", max_depth = n).fit(X_train, Y_train)\n      Y_predicted   = tree.predict(X_test)\n      mean_acc[n-1] = metrics.accuracy_score(Y_test, Y_predicted)\n      std_acc[n-1]  = np.std(Y_predicted == Y_test)\/np.sqrt(Y_predicted.shape[0])\n  \n  return (mean_acc.argmax() + 1, mean_acc.max())","8cb32bec":"max_depth = bestMax_Depth(10)[0]\nprint(\"max_depth =\", max_depth)\n# Train Model and Predict with the best K\ntree = DecisionTreeClassifier(criterion=\"gini\", max_depth = max_depth).fit(X_train, Y_train)\nY_bestMax_Depth_predicted = tree.predict(X_test)\nY_bestMax_Depth_predicted[0:5]","300c7888":"print(\"Train set Accuracy: \", metrics.accuracy_score(Y_train, tree.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(Y_test, Y_bestMax_Depth_predicted))","d307b6e5":"from sklearn import svm\nsvm_model = svm.SVC().fit(X_train, Y_train)\nY_svm_predicted = svm_model.predict(X_test)","1bd4bc52":"print(\"Train set Accuracy: \", metrics.accuracy_score(Y_train, svm_model.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(Y_test, Y_svm_predicted))","e7706aae":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss","1e50a5f6":"print(\"LogReg Accuracy train set:  %.4f\" % metrics.accuracy_score(Y_train, LR.predict(X_train)))\nprint(\"LogReg Accuracy test set:   %.4f\" % metrics.accuracy_score(Y_test, Y_LR_predicted))\nprint(\"LogReg F1-score test set:   %.4f\" % f1_score(Y_test, Y_LR_predicted, average='weighted'))\nprint(\"LogReg LogLoss:             %.4f\" % log_loss(Y_test, Y_LR_predicted_prob))\n\nprint(\"KNN Accuracy train set:  %.4f\" % metrics.accuracy_score(Y_train, neigh.predict(X_train)))\nprint(\"KNN Accuracy test set:   %.4f\" % metrics.accuracy_score(Y_test, Y_bestK_predicted))\nprint(\"KNN F1-score test set:   %.4f\" % f1_score(Y_test, Y_bestK_predicted, average='weighted'))\n\nprint(\"DT Accuracy train set:   %.4f\" % metrics.accuracy_score(Y_train, tree.predict(X_train)))\nprint(\"DT Accuracy test set:    %.4f\" % metrics.accuracy_score(Y_test, Y_bestMax_Depth_predicted))\nprint(\"DT F1-score test set:    %.4f\" % f1_score(Y_test, Y_bestMax_Depth_predicted, average='weighted'))\n\nprint(\"SVM Accuracy train set:  %.4f\" % metrics.accuracy_score(Y_train, svm_model.predict(X_train)))\nprint(\"SVM Accuracy test set:   %.4f\" % metrics.accuracy_score(Y_test, Y_svm_predicted))\nprint(\"SVM F1-score test set:   %.4f\" % f1_score(Y_test, Y_svm_predicted, average='weighted'))","df6505be":"df_titanic_prediction = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', error_bad_lines=False, engine=\"python\", sep=\",\")\ndf_titanic_prediction.set_index(\"PassengerId\", inplace = True)\nprint(df_titanic_prediction.shape)\nprint(df_titanic_prediction.ndim)","4e44176b":"print(df_titanic_prediction.isna().sum())","6af7ac33":"df_titanic_prediction['Sex'] = df_titanic_prediction['Sex'].apply(gender_to_numeric)","c741819a":"df_titanic_prediction['Embarked'] = df_titanic_prediction['Embarked'].apply(embarked_to_numeric)","9d282862":"df_titanic_prediction['Age'].fillna(np.mean(df_titanic_prediction['Age'][:]), inplace = True)\ndf_titanic_prediction['Fare'].fillna(np.mean(df_titanic_prediction['Fare'][:]), inplace = True)","852f72cd":"# To use scikit-learn library, we have to convert the Pandas data frame to a Numpy array:\nX_evaluation = df_titanic_prediction.drop(['Name', 'Ticket', 'Cabin'], axis=1).values\nX_evaluation[0:10]","279f1356":"Y_bestMax_Depth_predicted_evaluation = tree.predict(X_evaluation)\nY_bestMax_Depth_predicted_evaluation[0:5]","120fa54e":"submission = pd.DataFrame({\\\n                           'Survived': Y_bestMax_Depth_predicted_evaluation\\\n                          }, index = df_titanic_prediction.index)\nsubmission.to_csv('decision_tree_submission.csv')\nsubmission.head()","e096ad6a":"Let's see now how many different kind of `'Survived'` values there are defined in the data set. For that purpose, we will use the function `unique()`.","5489fb70":"So, we will check if there are missing values that may affect our study. For that purpose, we will use the `isna()` function.","5aa42262":"| Algorithm          | Jaccard (train) | Jaccard (test) | F1-score |\n|--------------------|---------|---------|----------|\n| LogReg                | 0.7219    | 0.7654    | 0.7336     |\n| KNN                | 0.7598    | 0.7765    | 0.7722     |\n| **Decision Tree**      | **0.8272**    | **0.8492**    | **0.8458**     |\n| SVM                | 0.6798    | 0.6983    | 0.6614     |","bde1b937":"**77.99% of right predictions** achieved using a with a DecisionTreeClassifier.","1cb332c3":"In this case, we do not need to apply any transformation over the values."}}