{"cell_type":{"03f4293d":"code","d161af55":"code","0fa06dc8":"code","82794cef":"code","f8927f0b":"code","68a28ba3":"code","70ec078e":"code","83f8203c":"code","4ef15b38":"code","b5b5c7be":"code","d8e53db4":"code","293db0c1":"code","01da3880":"code","e1324fa1":"code","02f259f4":"code","2273261d":"code","f4e74f6c":"code","b4c3e99b":"code","e72901e8":"code","3bfff047":"code","1c67686c":"code","c52af6da":"code","48895207":"code","858a2b96":"code","1b929255":"code","dc5e3794":"code","ea9572f3":"code","59118498":"code","9ef49ced":"code","7957fa4c":"code","c287f967":"code","c0de6ec0":"code","79f7b107":"code","161d0d75":"code","10424397":"code","d04e113e":"code","491d3080":"markdown","da03cad9":"markdown","b7720cb7":"markdown","1e26362e":"markdown","9b65f49d":"markdown","d57dd645":"markdown","5eb6cf4c":"markdown","2c5ddae2":"markdown","4d3499c1":"markdown","c40c08e4":"markdown","f13d5758":"markdown","11f67966":"markdown","eccc8f53":"markdown","d89e56c0":"markdown","e4e714b5":"markdown","9e95980c":"markdown","1f274f83":"markdown","88fae0e6":"markdown","3ebc1634":"markdown","86908dfa":"markdown","7c0ed699":"markdown","7a982ea9":"markdown","7886982a":"markdown","3ca6a20e":"markdown","b3390987":"markdown","63091725":"markdown","0eb3ffcd":"markdown","c4b79800":"markdown","1bccb60f":"markdown","8818d8ea":"markdown","2776f35e":"markdown","6f076d26":"markdown","bb1b09d2":"markdown","1e092bad":"markdown","cfc93072":"markdown","d6b2ea9a":"markdown","174c409e":"markdown"},"source":{"03f4293d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_context('poster')\n\nimport matplotlib.pyplot as plt\nfrom pandas_profiling import ProfileReport\nfrom IPython.display import HTML\n\nfrom tqdm import tqdm\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (16,12)\nplt.rcParams['axes.titlesize'] = 16\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d161af55":"train_sales = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsubmission_file = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","0fa06dc8":"print(train_sales.shape)\ntrain_sales.sample(2)","82794cef":"CREATE_TIDY_DF = False  # We will not create the tidy DF here.  Offline instead due to memory constraints.\nLOAD_TIDY_DF = True  # When done, it will load externally for ease instead of processing in the kernel.","f8927f0b":"if CREATE_TIDY_DF:\n    # Create a list comprehension for all the date columns to melt.\n    d_cols = ['d_' + str(i + 1) for i in range(1913)]\n\n    # Melt columns into rows so that each row is a separate and discrete entry with one target\n    tidy_df = pd.melt(frame = train_sales, \n                     id_vars = ['id', 'item_id', 'cat_id', 'store_id'],\n                     var_name = 'd',\n                     value_vars = d_cols,\n                     value_name = 'sales')\n\n    # This has duplicate ID's now.  We should add the date to the id to make each row unique.\n    new_ids = tidy_df['id'] + '_' + tidy_df['d']\n    tidy_df['id'] = new_ids\n\n    # Check this turned out ok so far.\n    tidy_df.head()","68a28ba3":"if CREATE_TIDY_DF:\n    # Merge the prices.  \n    # NOTE - For now we are aggregating on the mean price of each item.\n    # TO DO: We will want to set the price with the week or run some statistics on price volatility over time.\n\n    price_means = sell_prices.groupby(['item_id']).mean()\n    \n    # Now, merge this and the date col\n    with_prices_df = pd.merge(left = tidy_df, right = calendar,\n                            on = 'd')\n\n    with_prices_df.head(10)\n    # Let's see the results.","70ec078e":"if CREATE_TIDY_DF:\n    with_date_info_df = pd.merge(left = with_prices_df, right = price_means,\n                            on = 'item_id')\n    \n    total_tidy_df = with_date_info_df\n    total_tidy_df.columns\n    \n    # Drop d and drop item_id (price is an informative proxy)\n    total_tidy_df.drop(['d', 'wday', 'item_id'], axis = 1, inplace = True)\n    \n    # fill categorical NaNs with 0's.\n    total_tidy_df = total_tidy_df.fillna(0)\n    \n    print(with_date_info_df.iloc[0])\n\n","83f8203c":"if CREATE_TIDY_DF:\n    \n    # Categorical encoded column helper function.\n    def categorically_encode_col(df, col):\n        encoded_df = pd.get_dummies(df[col], \n                                    prefix = str(col),\n                                   drop_first = False)\n\n        return encoded_df\n    \n    total_tidy_df.columns\n\n# Categorically encode the categorical columns and then drop the originals.\n# This makes them ML ready.\n\nif CREATE_TIDY_DF:\n    \n    # Categorically encode categorical columns\n    cols_to_encode = ['cat_id', 'store_id', 'weekday', 'event_type_1', 'event_type_2' ]\n    \n    for col in cols_to_encode:\n        new_cols = pd.DataFrame(categorically_encode_col(total_tidy_df, col))\n        total_tidy_df = pd.concat([total_tidy_df, new_cols], axis = 1)\n        # total_tidy_df.drop(col, inplace = True)  # Drop the un-encoded column","4ef15b38":"if CREATE_TIDY_DF:\n    total_tidy_df.columns","b5b5c7be":"if CREATE_TIDY_DF:\n    # Export if necessary\n    total_tidy_df.to_csv('total_tidy_df.csv')","d8e53db4":"def disp_boxplot(data, title, xlabel, ylabel):\n    sns.set_style('whitegrid')\n    sns.set_context('poster')\n    palette = sns.color_palette(\"mako_r\", 6)\n    \n    ax = sns.boxplot(data=data, palette = palette)\n    ax.set(title = title,\n          xlabel = xlabel,\n          ylabel = ylabel)\n    \n    try:\n        ax.axhline(y = data.mean().mean(), color = 'b', label = 'Mean of all datapoints', linestyle = '--', linewidth = 1.5)\n        ax.ahline(y = data.median().median(), color = 'g', label = 'Median of all datapoints', linestyle = '--', linewidth = 1.5)\n    except:\n        pass\n    \n    ax.set_xticklabels(ax.get_xticklabels(), rotation = 45)\n    \n    plt.legend()\n    plt.show()","293db0c1":"dept_sales = train_sales.groupby(['dept_id']).mean().mean()\ndept_sum = train_sales.groupby(['dept_id']).sum().T.reset_index(drop = True)","01da3880":"disp_boxplot(data = dept_sum, title = 'Total Sales by Department',\n            xlabel = \"Department\", ylabel = \"Total Sales\")","e1324fa1":"dept_storeloc_cross = pd.crosstab(train_sales['dept_id'], train_sales['store_id'])\nprint(dept_storeloc_cross)\nax = sns.heatmap(dept_storeloc_cross,\n                linewidths = 0.4,\n                cmap=\"BuGn\")\nax.set(title = 'Number of items in each category per store - Uniform')","02f259f4":"n_items_dept = train_sales['dept_id'].value_counts()\nmean_of_total_sales_per_dept = dept_sum.mean(axis = 0)\n\nax = sns.regplot(n_items_dept, mean_of_total_sales_per_dept)\nax.set(title = 'Do departments with more items sell more? - No',\n      xlabel = 'Number of items Per Department',\n      ylabel = 'Mean total sales per department.')\nplt.show()","2273261d":"cat_sum = train_sales.groupby(['cat_id']).sum().T.reset_index(drop = True)\ndisp_boxplot(data = cat_sum, title = 'Total Sales by Category',\n            xlabel = \"Category\", ylabel = \"Total Sales\")","f4e74f6c":"state_sum = train_sales.groupby(['state_id']).sum().T.reset_index(drop = True)\nstate_mean = train_sales.groupby(['state_id']).mean().T.reset_index(drop = True)\n\ndisp_boxplot(data = state_sum, title = 'Total Sales by State ID',\n            xlabel = \"State ID\", ylabel = \"Total Sales\")\n\ndisp_boxplot(data = state_mean, title = 'Mean Sales by State ID',\n            xlabel = \"State ID\", ylabel = \"Mean Sales\")","b4c3e99b":"train_sales.head()","e72901e8":"store_sum = train_sales.groupby(['store_id']).sum().T.reset_index(drop = True)\nstore_mean = train_sales.groupby(['store_id']).mean().T.reset_index(drop = True) \n\ndisp_boxplot(data = store_sum, title = 'Total Sales by Store ID',\n            xlabel = \"Store ID\", ylabel = \"Total Sales\")\n\n\ndisp_boxplot(data = store_mean, title = 'Mean Sales Per Day by Store ID',\n            xlabel = \"Store ID\", ylabel = \"Total Sales\")","3bfff047":"ax = sns.regplot(x = np.arange(dept_sales.shape[0]), y = dept_sales,\n                 scatter_kws = {'color':'blue', 'alpha': 0.1},\n                 order = 3, line_kws = {'color':'green'},)\n\nax.set(title = \"Mean Total Sales Per Item Per Day Over Time\",\n      xlabel = 'Day ID', ylabel = 'Total sale per item per day')\n\nplt.show()","1c67686c":"from statsmodels.tsa.seasonal import seasonal_decompose\nweeks_per_year = 365\n\ntime_series = store_sum[\"CA_1\"]\nsj_sc = seasonal_decompose(time_series, period = weeks_per_year)\nsj_sc.plot()\n\nplt.show()","c52af6da":"from statsmodels.tsa.seasonal import seasonal_decompose\ndays_per_week = 7\n\ntime_series = store_sum[\"CA_1\"]\nsj_sc = seasonal_decompose(time_series, period = days_per_week)\nsj_sc.plot()\n\nplt.show()","48895207":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndef sarima_train_test(t_series, p = 2, d = 1, r = 2, NUM_TO_FORECAST = 56, do_plot_results = True):\n    NUM_TO_FORECAST = NUM_TO_FORECAST  # Similar to train test splits.\n    dates = np.arange(t_series.shape[0])\n\n    model = SARIMAX(t_series, order = (p, d, r), trend = 'c')\n    results = model.fit()\n    results.plot_diagnostics(figsize=(18, 14))\n    plt.show()\n\n    forecast = results.get_prediction(start = -NUM_TO_FORECAST)\n    mean_forecast = forecast.predicted_mean\n    conf_int = forecast.conf_int()\n\n    print(mean_forecast.shape)\n\n    # Plot the forecast\n    plt.figure(figsize=(14,16))\n    plt.plot(dates[-NUM_TO_FORECAST:],\n            mean_forecast.values,\n            color = 'red',\n            label = 'forecast')\n\n\n    plt.plot(dates[-NUM_TO_FORECAST:],\n            t_series.iloc[-NUM_TO_FORECAST:],\n            color = 'blue',\n            label = 'actual')\n    plt.legend()\n    plt.title('Predicted vs. Actual Values')\n    plt.show()\n    \n    residuals = results.resid\n    mae_sarima = np.mean(np.abs(residuals))\n    print('Mean absolute error: ', mae_sarima)\n    print(results.summary())\n","858a2b96":"sarima_train_test(time_series)","1b929255":"USE_SARIMA_PREDS = True","dc5e3794":"if USE_SARIMA_PREDS:\n    # Clean this code up.\n    \n    sarima_preds = pd.read_csv('\/kaggle\/input\/m5-untuned-sarima-preds\/Sarima_preds_submission.csv')\n    sarima_preds[sarima_preds < 0] = 0  # Convert all negative numbers into 0.\n    sarima_preds['id']= submission_file['id']\n    \n    # sarima_preds.to_csv('submission.csv', index = False)\n    \n    submission_df = sarima_preds\n    \n    #Cleaning\n    submission_df = submission_df.iloc[:,:29]\n    submission_df = submission_df.drop(['Unnamed: 0'], axis = 1)\n    submission_df.index = submission_file['id']\n    submission_df.reset_index(inplace = True)\n    submission_df.columns = submission_file.columns\n    submission_df.head()\n    \n    sarima_df = submission_df.copy()","ea9572f3":"train_sales.shape","59118498":"def subset_validation_set(df):\n    is_validation_subset = df['id'].str.contains('validation')\n    valid_subset = df[is_validation_subset]\n    return valid_subset\n\ndef create_evaluation_rows(df):\n    val_idx = df['id']\n    \n\ndef prepare_submission_file(df, i, val_or_eval):\n    ########################################################\n    # This function does several things:\n    #  It aggregates data from all the files and makes them\n    #  inputtable to traditional ML algorithms, such as trees.\n    \n    #  It will output the a dataframe for each 'day'\n    \n    # Returns a submission_like dataframe that is ready to put\n    # in an estimator.\n    # Please cite and upvote this kernel if you use this code\n    #########################################################\n    \n    # Extract the validation samples\n    # valid_subset = subset_validation_set(df)\n    valid_subset = df  # Fix this.\n    \n    # assert valid_subset.shape[0] == train_sales.shape[0], \"The rows are not equal\"\n    \n    # To do: validate the indices as well.\n    \n    # Collect date information 2016-04-25 to 2016-05-22 <-- The validation set days.\n    # First denote which columns you care about.\n    # + i is so that I can loop through to get all 28 dates.\n    \n    if val_or_eval == 'val':\n        # Validation range: 2016-04-25 to 2016-05-22\n        \n        # d_87 - d_114\n        i1 = 87 + i\n        i1_str = \"d_\" + str(i1)\n\n        # d_453 - d_480  <-- Leap year.  366 days after.\n        i2 = 453 + i\n        i2_str = \"d_\" + str(i2)\n\n        # d_818 - d_845\n        i3 = 818 + i\n        i3_str = \"d_\" + str(i3)\n\n        # d_1183 - d_1210\n        i4 = 1183 + i\n        i4_str = \"d_\" + str(i4)\n\n        # d_1548 - d_1575\n        i5 = 1548 + i\n        i5_str = \"d_\" + str(i5)\n\n        # d_1914 - d_1941  <-- Not in validation set\n        # i6 = 1941 + i\n        # i6_str = \"d_\" + str(i6)\n        \n    elif val_or_eval == 'eval':\n        # Evaluation range: 2016-05-23 to 2016-06-19\n        # d_87 - d_114\n        eval_num_ahead = 28\n        i1 = 87 + eval_num_ahead + i\n        i1_str = \"d_\" + str(i1)\n\n        # d_453 - d_480  <-- Leap year.  366 days after.\n        i2 = 453 + eval_num_ahead + i\n        i2_str = \"d_\" + str(i2)\n\n        # d_818 - d_845\n        i3 = 818 + eval_num_ahead + i\n        i3_str = \"d_\" + str(i3)\n\n        # d_1183 - d_1210\n        i4 = 1183 + eval_num_ahead + i\n        i4_str = \"d_\" + str(i4)\n\n        # d_1548 - d_1575\n        i5 = 1548 + eval_num_ahead + i\n        i5_str = \"d_\" + str(i5)\n\n        # d_1914 - d_1941  <-- Not in validation set\n        # i6 = 1941 + eval_num_ahead + i\n        # i6_str = \"d_\" + str(i6)\n        \n    \n    all_important_days = [i1_str, i2_str, i3_str, i4_str, i5_str]\n    col_names = ['this_day_1', 'this_day_2', 'this_day_3', 'this_day_4', 'this_day_5' ]\n    \n    # Extract the data from just these rows.\n    important_days_subset = valid_subset[all_important_days]\n    important_days_subset.columns = col_names\n    \n    # TO DO:\n    # Rename them F1-F28?\n    \n    # Add the cost of the item.\n    \n    # create is_special_event\n    \n    # categorically encode special_event.\n    \n    # encode_is_snap\n    \n    return important_days_subset\n\n","9ef49ced":"DO_BASELINE_MEAN_PRED = False","7957fa4c":"if DO_BASELINE_MEAN_PRED:\n    \n    # Do a loop for every F_DAYNUM\n    # Retrieve the information for that.\n    # Create a submission based on that information.\n\n    ####################################################\n    # TO DO: CLEAN THIS UP.\n    # Do the validation set.\n    final_submission_df = train_sales.copy()\n    final_cols = ['id']  # This will just be the columns we want in the final submission file.\n\n    for i in tqdm(range(28)):\n\n        important_days_df = prepare_submission_file(train_sales, i, val_or_eval = 'val')\n\n        # Round or don't roud here?\n        mean_of_days = important_days_df.mean(axis = 1)   # Shouldn't this be axis = 0?  Works only with ax = 1.\n\n        this_col = \"F\" + (str(i + 1))\n        final_cols.append(this_col)\n\n        final_submission_df[this_col] = mean_of_days\n\n    # Do the same thing for the evaluation set for now.\n    # Update this later to include calendar and other information.\n\n    ####################################################\n    # Do the evaluation set.\n    final_submission_df_eval = train_sales.copy()\n    final_cols = ['id']\n    for i in tqdm(range(28)):\n\n        important_days_df_eval = prepare_submission_file(train_sales, i, val_or_eval = 'eval')\n\n        mean_of_days = important_days_df_eval.mean(axis = 1)  # Shouldn't this be axis = 0?  Works only with ax = 1.\n\n        this_col = \"F\" + (str(i + 1))\n        final_cols.append(this_col)\n\n        final_submission_df_eval[this_col] = mean_of_days\n\n    final_submission_df_eval['id'] = final_submission_df_eval['id'].str.replace('validation', 'evaluation')\n\n    # CONCATENATE THE VAL AND EVAL SETS\n    submission_df = pd.concat([final_submission_df, final_submission_df_eval])\n\n    def clean_submission_file(df):\n        df = df[final_cols]\n        return df\n    \n    mean_of_days_df = submission_df.copy()\n    mean_of_days_df = clean_submission_file(mean_of_days_df)\n    ","c287f967":"if DO_BASELINE_MEAN_PRED:\n    print(submission_df.shape)\n    submission_df = clean_submission_file(submission_df)\n    print(submission_df.shape)\n","c0de6ec0":"if DO_BASELINE_MEAN_PRED:\n    submission_df.head(5)","79f7b107":"# Understanding the forecast metric.\n# https:\/\/robjhyndman.com\/papers\/foresight.pdf","161d0d75":"submission_df.to_csv('submission.csv', index = False)\nprint(submission_df.shape)\nprint(\"Submission file created\")","10424397":"# For Viewing\neval_start = int(60980 \/ 2)\neval_head_end = eval_start + 2\n\nsubmission_df.iloc[eval_start:eval_head_end, :]  # Take a look at the first few of the eval set.","d04e113e":"lineplot_data = submission_df.iloc[eval_start:eval_head_end, 1:].T\nlineplot_data.index = range(len(lineplot_data.index))  # Clean up the F1 naming.\n\nax = sns.scatterplot(data = lineplot_data, legend = False)\nax2 = sns.lineplot(data = lineplot_data, legend = False)\n\nax.set(title = 'Predicted Sales of 2 Different Products over 28 days',\n      xlabel = 'Days',\n      ylabel = 'Mean Products Sold')\n\nplt.xticks(rotation = 90)\nplt.show()","491d3080":"### Preparing the submission file ###\n* This section is in progress.","da03cad9":"**The dips are Christmas - the only day the store is closed** - Thanks https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda","b7720cb7":"## How do sales differ by department? ##","1e26362e":"## Do Sales Differ by Category? ##","9b65f49d":"### Baseline based on Means on the same day F1-F28 - Not Active ###\n* This code did quite bad compared to the SARIMA model and is therefore going to be deactivated and erased.","d57dd645":"## Seasonal Decomposition of One California Store: CA_1 ##\n\nThe seasonal decomposition shows how much a time series shows seaonality and trend.  It is a great way to think about how many sales are due to seaonality, trend, or one-off events like holidays.\n\nWe'll use a yearly frequency for now until we do a more detailed analysis.","5eb6cf4c":"**Takeaways:**\n* Since the mean sales are not significantly higher in CA, this may show that the total sales being higher in CA are simply due to there being 4 stores in the dataset instead of 3 like with TX and WI.","2c5ddae2":"**Plotting Helper Functions**","4d3499c1":"1. ## Weekly cycle decompose of CA_1 store ##","c40c08e4":"## Are there significant outliers on holidays and special days? ##\n* In progress...","f13d5758":"### Yearly Cycle Decompose of CA_1 store ###","11f67966":"* ## Sarima Predictions ##\n* These were done offline in about 16 hours.  Uploaded here and will submit.  Basic strategy was the above cell.","eccc8f53":"## Creating 'Tidy' Dataframes Capable of being fed into Models ##\nAs it stands, this code makes the Kernel crash due to memory constraints.  Therefore, I created these dataframes offline\nand will upload them when they are complete.  However, in order to see the code I used, I have included it below.\n\n### Reveal Code To See How to Create a Tidy ML Ready DF ###","d89e56c0":"### How Do Sales Differ By Department?  Foods and household supplies are highest. ###","e4e714b5":"## Calendar Dataset and its relationship with the sales dataset ##\n![https:\/\/www.thomasmeli.com\/wp-content\/uploads\/2020\/03\/Calendar-Dataset-Explanation.png](https:\/\/www.thomasmeli.com\/wp-content\/uploads\/2020\/03\/Calendar-Dataset-Explanation.png)","9e95980c":"**Read in the data for analysis**","1f274f83":"## Baseline Sarima Model ##\n\nSarima models use both a moving average approach and an autorgressive approach to time series predictions.  To analyze time series, we must make the series stationary, which means modifying the series to remove its trend and keeping its variance constant.  As we saw above, there is variance and trend, so the SARIMA model has a parameter to take care of this for us (the d or diff parameter).  By taking the difference between each time step, we are able to make the time series stationary.\n\nThe SARIMA model seems to be doing extremely well, but it takes a very long time to do the predictions on a kaggle kernel.  I'm doing the processing of the SARIMA model offline.\n\nAfter I work on Sarima optimizations, I will attempt to put this in ther Kernel, but due to time constraints I may need to keep it offline.","88fae0e6":"### Baseline with Tree Ensemble ###\n\nWe will be using the tidy dataframe tidy_df we created above to feed into this model.  Each row represents a distinct prediction, so this should go over well with regular model.\n","3ebc1634":"## Understanding the Submission ##\n* For those of you who haven't read \"Heads or Tails\"'s amazing kernel where the author explains this, [check it out here](https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda)","86908dfa":"## Question based EDA of m5 Competition - Beginners Guide ##\n\nThis guide will walk you through each aspect of the dataset and begin exploring questions to generate hypotheses for your ML workflow.  \n\nRemember to implement these best practices:\n* We will be going over it here, but [check out the detailed pdf on this competition.](https:\/\/mk0mcompetitiont8ake.kinstacdn.com\/wp-content\/uploads\/2020\/02\/M5-Competitors-Guide_Final-1.pdf)\n* Glean insights from previous competitions that are similar to this one.\n* Read the discussion board for ideas and insights from those generous souls publishing their ideas.\n* Get to know the data inside and out.\n* Check out the pandas profiles I put up in this kernel [https:\/\/www.kaggle.com\/tpmeli\/simple-pandas-profiling-eda-for-starting-out](https:\/\/www.kaggle.com\/tpmeli\/simple-pandas-profiling-eda-for-starting-out)\n\n### *This is just getting started - please upvote and link to this kernel if you use a portion of my work or if you'd like me to continue making it public.  Thanks!* ###","7c0ed699":"## What is intermittancy in a time series and why does it matter? ##\n\nIntermittancy matters for several reasons:\n* Sparse data sets have high dimensionality and this influences the effectiveness of various models.\n* The evaluation metric (\ud835\udc79\ud835\udc74\ud835\udc7a\ud835\udc7a\ud835\udc6c - discussed soon) will take this into account so we can't get a high acccuracy by guessing a lot of 0's.","7a982ea9":"**Price should be a good proxy for item_id and gives it a numerical value instead of a categorical value**\nOnce price is merged, we can drop item_id.  \n\nWe can also categorically encode store ID and cat_id.","7886982a":"## How do sales differ by store? ##","3ca6a20e":"## Baseline Submissions - In Progress ##\n\nI will be trying three baseline submissions:\n\n* One will be a simple mean submission of sales on that day for that item throughout the last few years.\n\n* One will be a tree regressor which will have to run a separate regression for each day.\n\n* One will be a SARIMA prediction.  This is far easier, but takes very long to process for all the rows.\n","b3390987":"## How do stores differ by State? ##","63091725":"### Thanks for reading - If you like my Kernel and want me to continue making my work public, let me know by upvoting and commenting.  It encourages me to continue making it visible to other Kagglers. ###","0eb3ffcd":"## Ensemble the above ##\n* The mean submission dropped the ensemble quite a bit.  So just doing SARIMA prediction for now.","c4b79800":"### How do SNAP days affect sales? ###\n","1bccb60f":"## Is there seasonality to the sales? ##","8818d8ea":"## Question Based Exploratory Data Analysis ##","2776f35e":"**Sarima helper function**","6f076d26":"## What is Hierarchical Data? ##\n\nHierarchical data refers to data being aggregated at different levels and scales.  \n\nKnowing the data is hierarchical is important because it leads to a natural question - do the data tell us something different at different scales and hierarchies?\n","bb1b09d2":"## Do total sales correlate with the number of items in a department? ##\n\nDo departments sell more items simply because they HAVE more items to sell?  Or is there a weaker correlation?","1e092bad":"**Takeaways:**\n* While CA_3 made the most sales, CA_2 was similar to other stores and CA_4 was fairly low.  Even though CA does have more overall mean sales, this may be because CA_3 is significantly higher than all the others.","cfc93072":"### Visualize sample rows of the submission predictions ###","d6b2ea9a":"## The Train_Sales Dataset ##\n\n![https:\/\/www.thomasmeli.com\/wp-content\/uploads\/2020\/03\/Annotated-ID.png](https:\/\/www.thomasmeli.com\/wp-content\/uploads\/2020\/03\/Annotated-ID.png)\n\n\"Contains the historical daily unit sales data per product and store\"\nIt looks like it has 1907 days of store sale records.\n","174c409e":"### Do stores sell different kinds of items? Nope - All stores have the same kind of items ###\nAbove we can see that Foods_3 category and household_1 make the most total sales.  If stores have more of one type of item than another, their sales might be influenced by what items they stock the stores with.  Let's check to see if they have different amounts of items in each store.\n\nLet's just make sure that all stores have the same categories and aren't selling more of one category or another."}}