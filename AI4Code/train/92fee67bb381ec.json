{"cell_type":{"866e3905":"code","0d94f6c8":"code","07940bd9":"code","16166478":"code","08c97da9":"code","92babcaa":"code","f4b0303f":"code","16b0e01f":"code","627e237b":"code","0bfa80c1":"code","6224b510":"code","f02f8f8a":"code","27522f55":"code","a1034af6":"code","40a19aa3":"code","5f329d8f":"code","e79d41db":"code","72546ed0":"code","7a32ffbb":"code","2f2f360a":"code","dfaf865a":"code","9debf8d1":"code","5153f8f6":"code","2449cbbc":"code","a1cd900e":"code","9face11d":"code","f651ccd3":"code","23c56d67":"code","ae27daba":"code","d67b8878":"code","d9e2656d":"code","e4a8c764":"code","4f9074bd":"code","18c7fc40":"code","d2902d39":"code","fc8d16a8":"code","37d9bb9e":"code","c6aec9ff":"code","966d54bc":"code","6eca1d4d":"code","30f9b11c":"code","28826bb8":"code","9f92cc6d":"markdown","23ba7ad8":"markdown","3f59bd3d":"markdown","fb505303":"markdown","da6a7fba":"markdown","17c89a3d":"markdown","17e95634":"markdown","bed68536":"markdown","aa07589a":"markdown","b0fd3769":"markdown","0ff267ff":"markdown","34c76073":"markdown","a75cf132":"markdown","507e5823":"markdown","d1bfd7e9":"markdown","e3b75e45":"markdown","8fbcbbef":"markdown","d37b2587":"markdown","5a5ac172":"markdown","9a577c55":"markdown"},"source":{"866e3905":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d94f6c8":"# importing libraries\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.ticker as mtick\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom imblearn.combine import SMOTEENN # imbalanced-learn Python library,\n\nimport warnings\nwarnings.filterwarnings('ignore')","07940bd9":"# to get a view of all the columns\npd.set_option(\"display.max_columns\", None)","16166478":"# Importing data\ndataset = pd.read_csv(\"..\/input\/employeeattrition-analyticsvidya-jobathon\/train_MpHjUjU.csv\")\ndataset.head(10)","08c97da9":"# selecting those columns where LastWorkingDate is set to some date\n# i.e., employees who have left (or going to leave) the organization\ndata_attr = dataset[dataset['LastWorkingDate'].notnull()]\ndata_attr.info()","92babcaa":"dataset.info()\n# to get familier with column datatypes and number of data points","f4b0303f":"# prining the  unique count of employees\nlen(dataset.Emp_ID.unique())","16b0e01f":"# finding the emp_id corresponding to whom we have max rows\ntemp_df = dataset.groupby(['Emp_ID']).size().reset_index(name='Count')\ntemp_df","627e237b":"# values corresponding to max values\ntemp_df.loc[temp_df['Count'].idxmax()]","0bfa80c1":"# all Emp_ID whose count is 24 (which is the max count for an employee)\ntemp_df[temp_df['Count'] == 24]","6224b510":"# checking if for a given employee there exists more than 1 city\ntemp_df = dataset.groupby(['Emp_ID','City']).size().reset_index(name='Count')\nprint(temp_df)\nprint()\nprint(\"checking for duplicate City values for a given employee\")\nprint(temp_df[temp_df.duplicated(['Emp_ID', 'City'])] )","f02f8f8a":"# checking if for a given employee there exists more than 1 Education\ntemp_df = dataset.groupby(['Emp_ID','Education_Level']).size().reset_index(name='Count')\nprint(temp_df )\nprint()\nprint(\"checking for duplicate EducationLevel values for a given employee\")\nprint(temp_df[temp_df.duplicated(['Emp_ID', 'Education_Level'])] )","27522f55":"# Checking how salary varies with Designation using count of salary\ntemp_df = dataset.groupby(['Designation', 'Salary']).count()\ntemp_df['Emp_ID']","a1034af6":"# Checking how salary varies with Designation using mode of salary\ntemp_df = dataset.groupby(['Designation']).agg(lambda x:x.value_counts().index[0]).reset_index()\ntemp_df[['Designation', 'Salary']]","40a19aa3":"# Checking how salary varies with Designation\ntemp_df = dataset.groupby(['Designation']).mean().reset_index()\ntemp_df[['Designation', 'Salary']]\n# type(temp_df) --> pandas.core.frame.DataFrame\n# temp_df.columns","5f329d8f":"# Checking how Designation and 'Joining Designation' varies for a given Emp_ID\ntemp_df = dataset.groupby(['Emp_ID'])['Designation'].apply(lambda x: list(np.unique(x))).reset_index()\nprint(temp_df)\nprint()\nprint(\"Records with more than one Designation for a given employee\")\nprint(temp_df[~(temp_df.Designation.str.len() < 2)]) # selecting rows having more than 1 designation for a given employee","e79d41db":"temp_df = dataset.groupby(['Emp_ID'])['Joining Designation'].apply(lambda x: list(np.unique(x))).reset_index()\nprint(temp_df)\nprint()\nprint(\"Records with more than one Designation for a given employee\")\nprint(temp_df[~(temp_df['Joining Designation'].str.len() < 2)]) # selecting rows having more than 1 designation for a given employee","72546ed0":"# Checking for which emp_ID we have end date\ntemp_df = dataset.groupby(['Emp_ID']).count().reset_index()\ntemp_df[['Emp_ID','LastWorkingDate']]\n# type(temp_df) --> pandas.core.frame.DataFrame\n# temp_df.columns","7a32ffbb":"# for each emp_id upating the end-date using reverse mode operation\ntemp_df = dataset.copy()\ntemp_df.LastWorkingDate.fillna(0, inplace=True)\ntemp_df = temp_df.groupby(['Emp_ID']).agg(lambda x:x.value_counts().index[-1]).reset_index() # as there can be onely 1 lastWorking Date\ntemp_df.LastWorkingDate.replace(0,np.nan,inplace=True) # replacing back all the 0s to NaN\ntemp_df[['Emp_ID', 'LastWorkingDate']]","2f2f360a":"dataset.info()","dfaf865a":"# features: generation and giving them a groupby value\n\n## mean of age for employee\ndf_age = dataset.groupby(['Emp_ID']).mean().reset_index()[['Age']]\ndf_age['Age'] = df_age.astype({\"Age\": int})\ndf_age\n\n# mode of age for each employee\ndf_gender = dataset.groupby(['Emp_ID'])['Gender'].apply(lambda x:x.value_counts().index[-1]).reset_index()\ndf_gender.drop(['Emp_ID'], inplace=True, axis=1)\ndf_gender\n\n# mode of city value for each employee\ndf_city = dataset.groupby(['Emp_ID'])['City'].apply(lambda x:x.value_counts().index[-1]).reset_index()\ndf_city.drop(['Emp_ID'], inplace=True, axis=1)\ndf_city\n\n# mode of Education level for each employee\ndf_education = dataset.groupby(['Emp_ID'])['Education_Level'].apply(lambda x:x.value_counts().index[-1]).reset_index()\ndf_education.drop(['Emp_ID'], inplace=True, axis=1)\ndf_education\n\n# mean salary for each employee\ndf_salary = dataset.groupby(['Emp_ID']).mean().reset_index()[['Salary']]\ndf_salary\n\n\n# keep only one date of joining\ndf_dateOfJoining = dataset.groupby(['Emp_ID'])['Dateofjoining'].apply(lambda x:x.value_counts().index[-1]).reset_index()\ndf_dateOfJoining.drop(['Emp_ID'], inplace=True, axis=1)\ndf_dateOfJoining['Dateofjoining'] = pd.to_datetime(df_dateOfJoining['Dateofjoining'])\ndf_dateOfJoining\n\n# for each emp_id upating the end-date using reverse mode operation\ndf_lastWorkingDate = dataset.copy()\ndf_lastWorkingDate.LastWorkingDate.fillna(0, inplace=True) # replacing NaN to 0, to perform aggregate function\ndf_lastWorkingDate = df_lastWorkingDate.groupby(['Emp_ID']).agg(lambda x:x.value_counts().index[-1]).reset_index()\ndf_lastWorkingDate.LastWorkingDate.replace(0, np.nan, inplace=True) # replacing 0 back to NaN\ndf_lastWorkingDate = df_lastWorkingDate[['LastWorkingDate']]\ndf_lastWorkingDate['LastWorkingDate'] = pd.to_datetime(df_lastWorkingDate['LastWorkingDate']) # converting to datetime\ndf_lastWorkingDate\n\n# With the help of lastWorkingDate, we are creating our target variable which is attrition\ndf_attr = df_lastWorkingDate.copy()\ndf_attr['Attrition'] = df_attr.apply(lambda x: 0 if x['LastWorkingDate'] is pd.NaT else 1, axis=1)\ndf_attr.drop(['LastWorkingDate'], inplace=True, axis=1)\ndf_attr\n\n# We are also creating new variable Tenure using the dateOFJoining and lastWorkingDate\ndf_tenure = pd.concat([df_dateOfJoining, df_lastWorkingDate], axis=1)\ndf_tenure['Tenure'] = df_tenure['LastWorkingDate'] - df_tenure['Dateofjoining']\ndf_tenure['Tenure_days'] = df_tenure['Tenure'].dt.days # converting the datetime variable to integer\nmax_tenure_days = df_tenure['Tenure_days'].max()\ndf_tenure.drop(['LastWorkingDate', 'Dateofjoining', 'Tenure'], axis=1, inplace=True)\ndf_tenure.fillna(max_tenure_days+10, axis=1, inplace=True)\ndf_tenure\n\n\n# mode joining designation\ndf_joiningDesignation = dataset.groupby(['Emp_ID'])['Joining Designation'].apply(lambda x:x.value_counts().index[-1]).reset_index()\ndf_joiningDesignation.drop(['Emp_ID'], inplace=True, axis=1)\ndf_joiningDesignation['Joining Designation'] = df_joiningDesignation['Joining Designation'].apply(str)\ndf_joiningDesignation\n\n# mode of designation\ndf_Designation = dataset.groupby(['Emp_ID'])['Designation'].apply(lambda x:x.value_counts().index[-1]).reset_index()\ndf_Designation.drop(['Emp_ID'], inplace=True, axis=1)\ndf_Designation['Designation'] = df_Designation['Designation'].apply(str)\ndf_Designation\n\n# creating a new variable, named promotion, for those employees who have more than one designation during their tenure\ndf_promotion = dataset.groupby(['Emp_ID'])['Designation'].apply(lambda x: list(np.unique(x))).reset_index()\nprint(\"Records with more than one Designation for a given employee\")\ndf_promotion['Promotion'] = df_promotion.Designation.str.len() >= 2\ndf_promotion['Promotion'] = df_promotion['Promotion'].apply(str)\ndf_promotion.drop(['Emp_ID', 'Designation'], inplace=True, axis=1)\ndf_promotion\n\n\n# mean of total business value\ndf_totalBusinessValue = dataset.groupby(['Emp_ID']).mean().reset_index()[['Total Business Value']]\ndf_totalBusinessValue\n\n# mode of quarterly ratings\ndf_quarterlyRating = dataset.groupby(['Emp_ID'])['Quarterly Rating'].apply(lambda x:x.value_counts().index[-1]).reset_index()\ndf_quarterlyRating.drop(['Emp_ID'], inplace=True, axis=1)\ndf_quarterlyRating['Quarterly Rating'] = df_quarterlyRating['Quarterly Rating'].apply(str)\ndf_quarterlyRating","9debf8d1":"# concatinating all the features together\ndataset_FE = pd.concat([df_age, df_gender, df_city, df_education, df_salary, df_tenure, df_joiningDesignation,  df_Designation, \n                        df_promotion, df_totalBusinessValue, df_quarterlyRating, df_attr], axis=1)\n# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\ndataset_FE","5153f8f6":"X = dataset_FE.drop(['Attrition'], axis=1)\ny = dataset_FE.Attrition","2449cbbc":"X.info()","a1cd900e":"OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\ncat_cols = ['Gender', 'City', 'Education_Level', 'Joining Designation',  'Designation', 'Promotion', 'Quarterly Rating']\nX_encoded = pd.DataFrame(OH_encoder.fit_transform(X[cat_cols]), index = X.index)\n\nX_num = X.drop(cat_cols, axis=1)\nX_encoded = pd.concat([X_encoded, X_num], axis=1)","9face11d":"X_encoded.shape","f651ccd3":"X_encoded.head()","23c56d67":"cols = X_encoded.columns # keeping the names of all columns\n\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\nx_scaled = scaler.fit_transform(X_encoded)\nX_encoded_scaled = pd.DataFrame(x_scaled, columns=cols)","ae27daba":"# Finding out optimum k-value using elbow method\nfrom sklearn.neighbors import KNeighborsClassifier\ndef find_k_KNN(x_train, x_test, y_train, y_test):\n    error_rate = []\n\n    # calculating error rate\n    for i in range(1,40):\n        knn = KNeighborsClassifier(n_neighbors=i)\n        knn.fit(x_train, y_train)\n        pred_i = knn.predict(x_test)\n        error_rate.append(np.mean(pred_i != y_test))\n\n\n    # Plotting elbow graph\n    plt.figure(figsize=(10,6))\n    plt.plot(range(1,40), error_rate, color=\"green\", linestyle=\"dashed\", marker=\"o\",\n             markerfacecolor=\"red\", markersize=10)\n    plt.title(\"Error Rate vs. K Value\")\n    plt.xticks(range(1,40))\n    plt.xlabel(\"K\")\n    plt.ylabel(\"Error Rate\")","d67b8878":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV # grid search CV was taking hours to calculate all the combinations\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\ndef train_models(x_train, y_train, k_values):\n    # defining models\n    models = [LogisticRegression(penalty = 'l2'), RandomForestClassifier(), \n              XGBClassifier(use_label_encoder=False, verbosity = 0, eval_metric='logloss', tree_method = 'gpu_hist', \n                            predictor = 'gpu_predictor'),\n              KNeighborsClassifier(), tree.DecisionTreeClassifier(), GaussianNB(), svm.SVC()]\n\n    # predictor = 'gpu_predictor', tree_method = 'gpu_hist' --> to use gpu for XGBoost\n    \n    \n    # defining model names\n    model_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting Classifier', 'KNN', 'Decision Tree', \n                   'Naive Bayes', 'Support Vector Machines']\n\n    # defining parameters\n\n    parameters = [{'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}, # logistic regression\n\n                 {'n_estimators':[100, 300, 500, 600, 700, 1000], 'criterion':['gini', 'entropy'], \n                  'max_depth' : [10, 20, 25, 30, 35, 40], 'min_samples_split': [100, 200, 50, 25]}, # random forest classifier\n\n                 {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] , \"max_depth\": [ 3, 4, 5, 6, 8, 10, 12, 15],\n                  \"min_child_weight\" : [ 1, 3, 5, 7 ], \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ], \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ], \n                  \"n_estimators\": [100, 120, 135, 150, 165, 200]}, # xgb classifier\n\n                 {'n_neighbors' : k_values}, # kNN\n\n                {'criterion':['gini', 'entropy'], 'max_depth' : [10, 20, 25, 30, 35, 40], 'min_samples_split': [100, 200, 50, 25]}, # decision tree\n\n                {}, # naive bayer\n\n                {'kernel': ['linear', 'poly', 'rbf', 'sigmoid']} # svm\n                 ]\n    \n    \n    # training the models\n\n    for model_idx in range(len(models)):\n        \n        \n        if model_names[model_idx] in ['Logistic Regression', 'KNN', 'Naive Bayes', 'Support Vector Machines']:\n            classifier = GridSearchCV(estimator = models[model_idx], param_grid = parameters[model_idx], n_jobs=-1)\n        else:\n            classifier = RandomizedSearchCV(estimator = models[model_idx], param_distributions = parameters[model_idx], random_state=0, n_jobs=-1)\n\n        classifier.fit(x_train, y_train)\n\n        print(model_names[model_idx])\n        print(classifier.best_estimator_)\n        models[model_idx] = classifier.best_estimator_ # updating the model with best hyperparameters as per training data\n        \n        print(f'Best train score: { classifier.best_score_}')\n        print(classifier.best_params_)\n        print(\"\\n******************************************************************************************************************\\n\")\n    \n    return (models, model_names)","d9e2656d":"from sklearn.metrics import classification_report\n\ndef test_models(x_train, x_test, y_train, y_test, models, model_names):\n    for classifier_idx in range(len(models)):\n        models[classifier_idx].fit(x_train, y_train)\n        y_pred = models[classifier_idx].predict(x_test)\n        \n        print(model_names[classifier_idx])\n        print(f'{models[classifier_idx].score(x_test, y_test)*100}')\n        print(\"\\n####################\")\n        print(classification_report(y_test, y_pred))\n        print(\"\\n\\n******************************************************************************************************************\\n\\n\")","e4a8c764":"%%time\n# dependent and independent variables\nX_encoded_scaled.shape, y.shape\n\n# train - test split\nx_train, x_test, y_train, y_test = train_test_split(X_encoded_scaled, y, test_size = 0.10, stratify=y)\n\nfind_k_KNN(x_train, x_test, y_train, y_test) # decide using the pictorial graph","4f9074bd":"%%time\nk_values = [12]\n\n# for training the models to find the best estimators based on training score\nmodels, model_names = train_models(x_train, y_train, k_values)","18c7fc40":"# for testing the models to find the best test score using the best estimated parameters for each algorithm\ntest_models(x_train, x_test, y_train, y_test, models, model_names)","d2902d39":"# choosing the final model depending on best train and test scores\n\nclassifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.3,\n              enable_categorical=False, eval_metric='logloss', gamma=0.2,\n              gpu_id=0, importance_type=None, interaction_constraints='',\n              learning_rate=0.15, max_delta_step=0, max_depth=4,\n              min_child_weight=7, missing=np.nan, monotone_constraints='()',\n              n_estimators=135, n_jobs=2, num_parallel_tree=1,\n              predictor='gpu_predictor', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='gpu_hist', use_label_encoder=False,\n              validate_parameters=1, verbosity=0)\n\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\n# classification report for imbalanced data\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","fc8d16a8":"test_data = pd.read_csv(\"..\/input\/employeeattrition-analyticsvidya-jobathon\/test_hXY9mYw.csv\")\ntest_data.info()","37d9bb9e":"# df_merge = test_data.merge(dataset, how='left', on=['Emp_ID'])\n# df_merge.info()","c6aec9ff":"!pip install pycaret","966d54bc":"# checking the pycaret version\nimport pycaret\nfrom pycaret.classification import setup\nfrom pycaret.classification import compare_models\nfrom pycaret.classification import tune_model\n\nprint(pycaret.__version__)","6eca1d4d":"X = dataset_FE.drop(['Attrition'], axis=1)\ny = dataset_FE.Attrition","30f9b11c":"gird_search = setup(data=dataset_FE, target='Attrition', html=False, silent=True, verbose=False) # to setup the dataset\nbest_classifiers = compare_models()# to evaluate and compare models\nprint(best_classifiers) # to report the best model","28826bb8":"# running hyperparameter tuning on the best selected classifier\ngird_search = setup(data=dataset_FE, target='Attrition', html=False, silent=True, verbose=False) # to setup the dataset\nbest_classifier_ = tune_model(RandomForestClassifier(), n_iter=200, choose_better=True) # tuning model hyperparameters\nprint(best_classifier_) # to report the best model","9f92cc6d":"##### Observations on Joining Designation and Designation\n* we have multiple values for Designation for some employees\n* but we always have unique values for Joining Designation (as expected) for all employees","23ba7ad8":"#### 2.3 checking if City & Education levels are changing for a given employee","3f59bd3d":"#### 2.5 For each employee, checking if designation changes?","fb505303":"#### Observation: \n* For each employee we have multiple rows.\n* For some employee the last row holds their LastWorkingDate, which indicates whether they have already churned","da6a7fba":"### 1. About Variable\n* MMMM-YY --> Reporting Date (Monthly)\n* Emp_ID --> Unique id for employees\n* Age --> Age of the employee\n* Gender --> Gender of the employee\n* City --> City Code of the employee\n* Education_Level --> Education level : Bachelor, Master or College\n* Salary --> Salary of the employee\n* Dateofjoining --> Joining date for the employee\n* LastWorkingDate --> Last date of working for the employee\n* Joining Designation --> Designation of the employee at the time of joining\n* Designation --> Designation of the employee at the time of reporting\n* Total_Business_Value --> The total business value acquired by the employee in a month (negative business indicates cancellation\/refund of sold insurance policies)  \n* Quarterly Rating --> Quarterly rating of the employee: 1,2,3,4 (higher is better)","17c89a3d":"### One Hot encoding","17e95634":"##### Observation: (City, Education_Level)\n* City is unique for any employees\n* Education_Level is unique for employees","bed68536":"#### 2.2 Employees with highest row counts!","aa07589a":"## Use of PyCaret to Solve the problem\n* setup() - pycaret library for defining the data transforms to perform\n* compare_models() - pycaret library for evaluating and comparing standard models\n* tune_model() - library to tune hyperparameters for a selected model\n* **PyCaret require that the dataset does have column names**","b0fd3769":"### Model Building","0ff267ff":"### Feature Engineering\n- Will combine the rows, based on specific criterion on each column. Aim is to make it a classification problem, instead of time series\n\n1. Age: [mean] of age, based on emp_id\n2. Gender: [mode] of gender, cause gender won't change\n3. City: in given dataset, city is not changing, so we can use [mode]\n4. Education: [mode], cause education is not changing for employees\n5. Salary: [mean],\n6. DateOfJoining: [mode] cause it will be same for a given employee\n7. LastWorkingDay: [reverse mode]\n8. [new variable] Attrition: 0: if the employee does not leave the organization, 1: if the employee leaves the organization\n9. Joining Designation: [mode] cause it is fixed for a given employee\n10. Designation: [mode] taking that designation where they remained for longer period of time\n11. [new variable] promotion: is Designation.str.len()> 1, yes else no\n12. Total Business Value: [mean] cause it involves -ve, 0 and +ve values\n13. Quarter Rating: [mode]","34c76073":"##### Observations: \n* Out of 2381 employees, 1616 employees have left\n* 765 employees still working","a75cf132":"### Splitting X,Y ","507e5823":"#### 2.6 Employee and their lastWorkingDate","d1bfd7e9":"### Feature Scaling - MinMaxScaler","e3b75e45":"#### 2.1 How many records per employee?","8fbcbbef":"##### Observations: \n* Salary is not dependent only on Designation, it is being affected by other parameters too, i.e., for a given designation we can have multiple salary values","d37b2587":"##### Observation: Decisition Tree classifier giving the best results. But as there is high chances of overfitting, we will go for RandomForest","5a5ac172":"#### 2.4 Checking if Salary is unique for a given Designation?","9a577c55":"### 2. Getting Familier with Data"}}