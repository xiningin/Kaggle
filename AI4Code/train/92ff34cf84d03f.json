{"cell_type":{"0f91c64e":"code","1f9c51d8":"code","eb48545a":"code","e6e2541e":"code","c658996b":"code","49c8e5d3":"code","96396388":"code","d1e86240":"code","6cb153f8":"code","7161d4ec":"code","bc8151be":"code","710ca740":"code","1d334031":"code","04e6a022":"code","f426ffec":"code","36bc04d8":"code","dea5f70b":"markdown","03574bc1":"markdown","a68b751f":"markdown","04335895":"markdown"},"source":{"0f91c64e":"from numpy.random import seed\nseed(8)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, model_selection \nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import BatchNormalization\n%matplotlib inline\nimport tensorflow as tf\n\nfrom sklearn.datasets import load_iris\niris_data = load_iris()\nprint(iris_data['DESCR'])","1f9c51d8":"### load and prepreocess the data\ndef read_in_and_split_data(iris_data):\n    \n    data = iris_data['data']\n    target = iris_data['target']\n    \n    train_data, test_data, train_targets, test_targets = train_test_split(data, target, test_size = 0.1)\n    return train_data, test_data, train_targets, test_targets","eb48545a":"# Generate the test and training data.\niris_data = datasets.load_iris()\ntrain_data, test_data, train_targets, test_targets = read_in_and_split_data(iris_data)","e6e2541e":"# Convert targets to a one-hot encoding\ntrain_targets = tf.keras.utils.to_categorical(np.array(train_targets))\ntest_targets = tf.keras.utils.to_categorical(np.array(test_targets))","c658996b":"# the model \ndef get_model(input_shape):\n    \"\"\"\n    This function should build a Sequential model according to the above specification. Ensure the \n    weights are initialised by providing the input_shape argument in the first layer, given by the\n    function argument.\n    Your function should return the model.\n    \"\"\"\n    model = Sequential([\n        Dense(64, activation = 'relu', \n              #initializer = tf.keras.initializers.HeUniform(), \n              bias_initializer=tf.keras.initializers.Ones(),\n              input_shape = input_shape),\n        Dense(128, activation = 'relu'),\n        Dense(128, activation = 'relu'),\n        Dense(128, activation = 'relu'),\n        Dense(128, activation = 'relu'),\n        Dense(64, activation = 'relu'),\n        Dense(64, activation = 'relu'),\n        Dense(64, activation = 'relu'),\n        Dense(64, activation = 'relu'),\n        Dense(3,  activation = 'softmax')    \n    ])\n    return model","49c8e5d3":"# get the model\nmodel = get_model(train_data[0].shape)","96396388":"# compile the model\ndef compile_model(model):\n    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics = ['accuracy'])","d1e86240":"# fit the model to the training data\ndef train_model(model, train_data, train_targets, epochs):\n    history = model.fit(train_data, train_targets, epochs = epochs, validation_split = 0.15, \n                        batch_size = 40 )\n    return history","6cb153f8":"# compile the model\ncompile_model(model)","7161d4ec":"# Run to train the model\nhistory = train_model(model, train_data, train_targets, epochs=800)","bc8151be":"#plot the learning curves \ntry:\n        plt.plot(history.history['accuracy'])\n        plt.plot(history.history['val_accuracy'])\nexcept KeyError:\n        plt.plot(history.history['acc'])\n        plt.plot(history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show()","710ca740":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","1d334031":"\n# regularized model \ndef get_regularised_model(input_shape, dropout_rate, weight_decay):\n    \n    model = Sequential([\n        Dense(128, activation=\"relu\", kernel_regularizer = regularizers.l2(weight_decay), input_shape=input_shape),\n        Dense(128, activation=\"relu\", kernel_regularizer = regularizers.l2(weight_decay)),\n        Dense(128, activation=\"relu\", kernel_regularizer = regularizers.l2(weight_decay)),\n        Dropout(dropout_rate),\n        Dense(128, activation=\"relu\", kernel_regularizer = regularizers.l2(weight_decay)),\n        Dense(128, activation=\"relu\", kernel_regularizer = regularizers.l2(weight_decay)),\n        BatchNormalization(),\n        Dense(64, activation=\"relu\", kernel_regularizer = regularizers.l2(weight_decay)),\n        Dense(64, activation=\"relu\", kernel_regularizer = regularizers.l2(weight_decay)),\n        Dropout(dropout_rate),\n        Dense(64, activation=\"relu\", kernel_regularizer = regularizers.l2(weight_decay)),\n        Dense(64, activation=\"relu\", kernel_regularizer = regularizers.l2(weight_decay)),\n        Dense(3, activation=\"softmax\")\n    ])\n    return model","04e6a022":"# Let's now instantiate the model, using a dropout rate of 0.3 and weight decay coefficient of 0.001\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras import regularizers\n\n#istantiate\nreg_model = get_regularised_model(train_data[0].shape, 0.3, 0.001)\n\n#compile\ncompile_model(reg_model)\n\n# train \nreg_history = train_model(reg_model, train_data, train_targets, epochs=800)","f426ffec":"try:\n    plt.plot(reg_history.history['accuracy'])\n    plt.plot(reg_history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(reg_history.history['acc'])\n    plt.plot(reg_history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","36bc04d8":"plt.plot(reg_history.history['loss'])\nplt.plot(reg_history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","dea5f70b":"#### As can be seen from the graphics, the regularisation process has helped to reduce the overfitting of the network. Now there can be employed other tecniques like EarlyStopping. ","03574bc1":"### Plot the improved learning curves","a68b751f":"### Let's insert now regularization to redice overfitting","04335895":"The goal of this notebook is to show how a neural network that classifies Iris dataset samples can improve its performance by using validation and regularisation techniques."}}