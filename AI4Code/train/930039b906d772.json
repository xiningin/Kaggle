{"cell_type":{"2bf4bfe7":"code","52fae507":"code","db37d24d":"code","dcd0b27f":"code","1a4d6def":"code","34207dc5":"code","f59a57cb":"code","e04349f4":"code","bf1a7378":"code","d6b84f9e":"code","d797c7b4":"code","62cb48a1":"code","6acc44f3":"code","72ab28c1":"code","2860be8a":"code","5e4d8e18":"code","21efc20c":"code","38922deb":"code","1ff802d0":"code","dc22adfd":"code","17034b15":"code","b1150710":"code","5d736b24":"code","a6c5c369":"code","4e211ff5":"code","cac2e2d6":"code","ac429fb9":"code","1fb0e0f7":"code","e7fdddb3":"code","f034656a":"code","1a779c1f":"code","edfa3083":"code","035cf795":"code","f0de9736":"code","fd443d2e":"code","74ddd92d":"code","d0714bfc":"code","afbd659d":"code","cc8ff93f":"code","bfaf2d64":"code","767f9ddd":"code","617b26ae":"code","7a8361c7":"code","36a18c31":"code","37aca95a":"code","3af584dc":"code","bdc2cd73":"code","6aa2069b":"markdown","5a174dce":"markdown","b19b53b1":"markdown","ada9da21":"markdown","fe82b2bc":"markdown","b1f6996f":"markdown","8114c72a":"markdown","d2c1a423":"markdown","3fce5888":"markdown","8a3e99a6":"markdown","ad90a0b8":"markdown","9529a2f6":"markdown","41939e05":"markdown","3c320c6d":"markdown","752abc1b":"markdown","6bc45240":"markdown","232c94aa":"markdown","b626c558":"markdown","b678688e":"markdown","8192bbb1":"markdown","5d4ec42d":"markdown","c43e5a61":"markdown","10fe1427":"markdown","a1205aa1":"markdown","6670c893":"markdown","1ab5771c":"markdown","5a00b489":"markdown"},"source":{"2bf4bfe7":"import numpy as np\nimport pandas as pd\n\n# Modelling\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\n\n# KNeighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Perceptron\nfrom sklearn.linear_model import Perceptron\n\n# Support Vector Machines\nfrom sklearn.svm import SVC\n\n# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\n# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# XGBoost\nfrom xgboost import XGBClassifier\n\n# LightGBM\nfrom lightgbm import LGBMClassifier","52fae507":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","db37d24d":"train_data","dcd0b27f":"train_data.describe()","1a4d6def":"print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))","34207dc5":"missing_values = train_data.isna().any()\nprint('Columns which have missing values: \\n{0}'.format(missing_values[missing_values == True].index.tolist()))","f59a57cb":"print(\"Percentage of missing values in `Age` column: {0:.2f}\".format(100.*(train_data.Age.isna().sum()\/len(train_data))))\nprint(\"Percentage of missing values in `Cabin` column: {0:.2f}\".format(100.*(train_data.Cabin.isna().sum()\/len(train_data))))\nprint(\"Percentage of missing values in `Embarked` column: {0:.2f}\".format(100.*(train_data.Embarked.isna().sum()\/len(train_data))))","e04349f4":"duplicates = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates))","bf1a7378":"categorical = train_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical))","d6b84f9e":"def clean_data(data):\n    # Too many missing values\n    data.drop(['Cabin'], axis=1, inplace=True)\n    \n    # Probably will not provide some useful information\n    data.drop(['Name', 'Ticket', 'Fare', 'Embarked'], axis=1, inplace=True)\n    \n    return data\n    \ntrain_data = clean_data(train_data)\ntest_data = clean_data(test_data)","d797c7b4":"train_data.tail()","62cb48a1":"train_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\ntest_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\n\n# Merge two data to get the average Age and fill the column\nall_data = pd.concat([train_data, test_data])\naverage = all_data.Age.median()\nprint(\"Average Age: {0}\".format(average))\ntrain_data.fillna(value={'Age': average}, inplace=True)\ntest_data.fillna(value={'Age': average}, inplace=True)","6acc44f3":"train_data.tail()","72ab28c1":"# Set X and y\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\ny = train_data['Survived']\ntest_X = test_data.drop(['PassengerId'], axis=1)","2860be8a":"# To store models created\nbest_models = {}\n\n# Split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\ndef print_best_parameters(hyperparameters, best_parameters):\n    value = \"Best parameters: \"\n    for key in hyperparameters:\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n    if hyperparameters:\n        print(value[:-2])\n\ndef get_best_model(estimator, hyperparameters, fit_params={}):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparameters, n_jobs=-1, cv=cv, scoring=\"accuracy\")\n    best_model = grid_search.fit(train_X, train_y, **fit_params)\n    best_parameters = best_model.best_estimator_.get_params()\n    print_best_parameters(hyperparameters, best_parameters)\n    return best_model\n\ndef evaluate_model(model, name):\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n    best_models[name] = model","5e4d8e18":"print(\"Features: \\n{0} \".format(X.columns.tolist()))","21efc20c":"# https:\/\/machinelearningmastery.com\/hyperparameters-for-classification-machine-learning-algorithms\/\nhyperparameters = {\n    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n    'penalty' : ['l2'],\n    'C'       : [100, 10, 1.0, 0.1, 0.01]\n}\nestimator = LogisticRegression(random_state=1)\nbest_model_logistic = get_best_model(estimator, hyperparameters)","38922deb":"evaluate_model(best_model_logistic.best_estimator_, 'logistic')","1ff802d0":"# https:\/\/www.analyticsvidhya.com\/blog\/2021\/01\/gaussian-naive-bayes-with-hyperpameter-tuning\/\nhyperparameters = {\n    'var_smoothing': np.logspace(0, -9, num=100)\n}\nestimator = GaussianNB()\nbest_model_gaussian_nb = get_best_model(estimator, hyperparameters)","dc22adfd":"evaluate_model(best_model_gaussian_nb.best_estimator_, 'gaussian_nb')","17034b15":"# https:\/\/medium.com\/@kocur4d\/hyper-parameter-tuning-with-pipelines-5310aff069d6\nhyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = MultinomialNB()\nbest_model_multinominal_nb = get_best_model(estimator, hyperparameters)","b1150710":"evaluate_model(best_model_multinominal_nb.best_estimator_, 'multinominal_nb')","5d736b24":"hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n    'norm'      : [True, False]\n}\nestimator = ComplementNB()\nbest_model_complement_nb = get_best_model(estimator, hyperparameters)","a6c5c369":"evaluate_model(best_model_complement_nb.best_estimator_, 'complement_nb')","4e211ff5":"hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = BernoulliNB()\nbest_model_bernoulli_nb = get_best_model(estimator, hyperparameters)","cac2e2d6":"evaluate_model(best_model_bernoulli_nb.best_estimator_, 'bernoulli_nb')","ac429fb9":"# https:\/\/medium.datadriveninvestor.com\/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f\nhyperparameters = {\n    'n_neighbors' : list(range(1,5)),\n    'weights'     : ['uniform', 'distance'],\n    'algorithm'   : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size'   : list(range(1,10)),\n    'p'           : [1,2]\n}\nestimator = KNeighborsClassifier()\nbest_model_kneighbors = get_best_model(estimator, hyperparameters)","1fb0e0f7":"evaluate_model(best_model_kneighbors.best_estimator_, 'kneighbors')","e7fdddb3":"# https:\/\/machinelearningmastery.com\/perceptron-algorithm-for-classification-in-python\/\n# https:\/\/machinelearningmastery.com\/manually-optimize-hyperparameters\/\nhyperparameters = {\n    'penalty'  : ['l1', 'l2', 'elasticnet'],\n    'eta0'     : [0.0001, 0.001, 0.01, 0.1, 1.0],\n    'max_iter' : list(range(50, 200, 50))\n}\nestimator = Perceptron(random_state=1)\nbest_model_perceptron = get_best_model(estimator, hyperparameters)","f034656a":"evaluate_model(best_model_perceptron.best_estimator_, 'perceptron')","1a779c1f":"# https:\/\/www.geeksforgeeks.org\/svm-hyperparameter-tuning-using-gridsearchcv-ml\/\n# https:\/\/towardsdatascience.com\/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167\nhyperparameters = {\n    'C'      : [0.1, 1, 10, 100],\n    'gamma'  : [0.0001, 0.001, 0.01, 0.1, 1],\n    'kernel' : ['rbf']\n}\nestimator = SVC(random_state=1)\nbest_model_svc = get_best_model(estimator, hyperparameters)","edfa3083":"evaluate_model(best_model_svc.best_estimator_, 'svc')","035cf795":"# https:\/\/towardsdatascience.com\/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4\n# https:\/\/www.knowledgehut.com\/tutorials\/machine-learning\/hyperparameter-tuning-machine-learning\nhyperparameters = {\n    'loss'    : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n    'penalty' : ['l1', 'l2', 'elasticnet'],\n    'alpha'   : [0.01, 0.1, 1, 10]\n}\nestimator = SGDClassifier(random_state=1, early_stopping=True)\nbest_model_sgd = get_best_model(estimator, hyperparameters)","f0de9736":"evaluate_model(best_model_sgd.best_estimator_, 'sgd')","fd443d2e":"# https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/\nhyperparameters = {\n    'loss'          : ['deviance', 'exponential'],\n    'learning_rate' : [0.01, 0.1, 0.2, 0.3],\n    'n_estimators'  : [50, 100, 200],\n    'subsample'     : [0.1, 0.2, 0.5, 1.0],\n    'max_depth'     : [2, 3, 4, 5]\n}\nestimator = GradientBoostingClassifier(random_state=1)\nbest_model_gbc = get_best_model(estimator, hyperparameters)","74ddd92d":"evaluate_model(best_model_gbc.best_estimator_, 'gbc')","d0714bfc":"# https:\/\/medium.com\/@chaudhurysrijani\/tuning-of-adaboost-with-computational-complexity-8727d01a9d20\nhyperparameters = {\n    'n_estimators'  : [10, 50, 100, 500],\n    'learning_rate' : [0.001, 0.01, 0.1, 1.0]\n}\nestimator = AdaBoostClassifier(random_state=1)\nbest_model_adaboost = get_best_model(estimator, hyperparameters)","afbd659d":"evaluate_model(best_model_adaboost.best_estimator_, 'adaboost')","cc8ff93f":"# https:\/\/towardsdatascience.com\/how-to-tune-a-decision-tree-f03721801680\n# https:\/\/www.kaggle.com\/gauravduttakiit\/hyperparameter-tuning-in-decision-trees\nhyperparameters = {\n    'criterion'         : ['gini', 'entropy'],\n    'splitter'          : ['best', 'random'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = DecisionTreeClassifier(random_state=1)\nbest_model_decision_tree = get_best_model(estimator, hyperparameters)","bfaf2d64":"evaluate_model(best_model_decision_tree.best_estimator_, 'decision_tree')","767f9ddd":"# https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n# https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/beginners-guide-random-forest-hyperparameter-tuning\/\nhyperparameters = {\n    'n_estimators'      : list(range(10, 50, 10)),\n    'max_features'      : ['auto', 'sqrt', 'log2'],\n    'criterion'         : ['gini', 'entropy'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = RandomForestClassifier(random_state=1)\nbest_model_random_forest = get_best_model(estimator, hyperparameters)","617b26ae":"evaluate_model(best_model_random_forest.best_estimator_, 'random_forest')","7a8361c7":"# https:\/\/towardsdatascience.com\/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d\nhyperparameters = {\n    'learning_rate' : [0.3, 0.4, 0.5],\n    'gamma'         : [0, 0.4, 0.8],\n    'max_depth'     : [2, 3, 4],\n    'reg_lambda'    : [0, 0.1, 1],\n    'reg_alpha'     : [0.1, 1]\n}\nfit_params = {\n    'verbose'               : False,\n    'early_stopping_rounds' : 40,\n    'eval_metric'           : 'logloss',\n    'eval_set'              : [(val_X, val_y)]\n}\nestimator = XGBClassifier(seed=1, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False)\nbest_model_xgb = get_best_model(estimator, hyperparameters, fit_params)","36a18c31":"evaluate_model(best_model_xgb.best_estimator_, 'xgb')","37aca95a":"# https:\/\/towardsdatascience.com\/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\nhyperparameters = {\n    'boosting_type' : ['gbdt', 'dart', 'goss'],\n    'num_leaves'    : [4, 8, 16, 32],\n    'learning_rate' : [0.01, 0.1, 1],\n    'n_estimators'  : [25, 50, 100],\n    'reg_alpha'     : [0, 0.1, 1],\n    'reg_lambda'    : [0, 0.1, 1],\n}\nestimator = LGBMClassifier(random_state=1, device='gpu')\nbest_model_lgbm = get_best_model(estimator, hyperparameters)","3af584dc":"evaluate_model(best_model_lgbm.best_estimator_, 'lgbm')","bdc2cd73":"# Get predictions for each model and create submission files\nfor model in best_models:\n    predictions = best_models[model].predict(test_X)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('submission_' + model + '.csv', index=False)","6aa2069b":"## [Random Forest Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)\n\n* **n_estimators: int, default=100**  \n    The number of trees in the forest.\n\n\n* **max_features: {\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}, int or float, default=\u201dauto\u201d**  \n    * The number of features to consider when looking for the best split:\n        * If int, then consider max_features features at each split.\n        * If float, then max_features is a fraction and round(max_features * n_features) features are considered at each split.\n        * If \u201cauto\u201d, then max_features=sqrt(n_features).\n        * If \u201csqrt\u201d, then max_features=sqrt(n_features) (same as \u201cauto\u201d).\n        * If \u201clog2\u201d, then max_features=log2(n_features).\n        * If None, then max_features=n_features.\n\n> Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n\n* **criterion: {\u201cgini\u201d, \u201centropy\u201d}, default=\u201dgini\u201d**  \n    The function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain. Note: this parameter is tree-specific.\n\n* **max_depth: int, default=None**  \n    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n    \n    \n* **min_samples_split: int or float, default=2**  \n    * The minimum number of samples required to split an internal node:\n        * If int, then consider min_samples_split as the minimum number.\n        * If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n\n\n* **min_samples_leaf: int or float, default=1**  \n    The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.  \n     * If int, then consider min_samples_leaf as the minimum number.\n     * If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","5a174dce":"## Check for duplicates","b19b53b1":"### [Multinomial Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html)\n\n* **alpha: float, default=1.0**  \n    Additive (Laplace\/Lidstone) smoothing parameter (0 for no smoothing).\n    \n* **fit_prior: bool, default=True**  \n    Whether to learn class prior probabilities or not. If false, a uniform prior will be used.","ada9da21":"# 2. Explore data","fe82b2bc":"## [LGBMClassifier](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html)\n\n* **boosting_type: (str, optional (default='gbdt'))**  \n    * \u2018gbdt\u2019, traditional Gradient Boosting Decision Tree.\n    * \u2018dart\u2019, Dropouts meet Multiple Additive Regression Trees.\n    * \u2018goss\u2019, Gradient-based One-Side Sampling.\n    * \u2018rf\u2019, Random Forest.\n\n\n* **num_leaves: (int, optional (default=31))**  \n    Maximum tree leaves for base learners.\n\n* **learning_rate: (float, optional (default=0.1))**  \n    Boosting learning rate. You can use callbacks parameter of fit method to shrink\/adapt learning rate in training using reset_parameter callback. Note, that this will ignore the learning_rate argument in training.\n\n* **n_estimators: (int, optional (default=100))**  \n    Number of boosted trees to fit.\n\n* **reg_alpha: (float, optional (default=0.))**  \n    L1 regularization term on weights.\n\n* **reg_lambda: (float, optional (default=0.))**  \n    L2 regularization term on weights.","b1f6996f":"# 6. Modelling\n\nTry different models with different parameters to understand which models give better results.","8114c72a":"# 5. Feature engineering\n\nAlthough I have eliminated most of the columns for simplicity, in the future I am planning to recover those columns. They may contain some useful information.  \nFor now encoding the `Sex` column and filling `Age` column is enough to run a model.","d2c1a423":"# WORK IN PROGRESS","3fce5888":"## [Support Vector Machines](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)\n\n* **C: float, default=1.0**  \n    Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n\n* **kernel: {\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019**  \n    Specifies the kernel type to be used in the algorithm. It must be one of \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019 or a callable. If none is given, \u2018rbf\u2019 will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).\n\n\n* **gamma{\u2018scale\u2019, \u2018auto\u2019} or float, default=\u2019scale\u2019**  \n    Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019.\n    * if gamma='scale' (default) is passed then it uses 1 \/ (n_features * X.var()) as value of gamma,\n    * if \u2018auto\u2019, uses 1 \/ n_features.","8a3e99a6":"## [GradientBoostingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)\n\n* **loss: {\u2018deviance\u2019, \u2018exponential\u2019}, default=\u2019deviance\u2019**  \n    The loss function to be optimized. \u2018deviance\u2019 refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss \u2018exponential\u2019 gradient boosting recovers the AdaBoost algorithm.\n\n* **learning_rate: float, default=0.1**  \n    Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n\n* **n_estimators: int, default=100**  \n    The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n\n* **subsample: float, default=1.0**  \n    The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n\n* **max_depth: int, default=3**  \n    The maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.","ad90a0b8":"# 1. Importing libraries and loading datasets","9529a2f6":"# 4. Data cleaning","41939e05":"# 7. Submission","3c320c6d":"## [Decision Tree Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)\n\nTune decision tree classifier model by changing some of its parameters.\n\n* **criterion: {\u201cgini\u201d, \u201centropy\u201d}, default=\u201dgini\u201d**  \n    The function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain.\n\n* **splitter: {\u201cbest\u201d, \u201crandom\u201d}, default=\u201dbest\u201d**  \n    The strategy used to choose the split at each node. Supported strategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose the best random split.\n\n* **max_depth: int, default=None**  \n    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n    \n\n* **min_samples_split: int or float, default=2**  \n    * The minimum number of samples required to split an internal node:\n        * If int, then consider min_samples_split as the minimum number.\n        * If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n\n\n* **min_samples_leaf: int or float, default=1**  \n   The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.  \n    * If int, then consider min_samples_leaf as the minimum number.\n    * If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","752abc1b":"## [Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html)","6bc45240":"## [Stochastic Gradient Descent](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html)\n\n* **loss: str, default=\u2019hinge\u2019**  \n    The loss function to be used. Defaults to \u2018hinge\u2019, which gives a linear SVM.  \n    The possible options are \u2018hinge\u2019, \u2018log\u2019, \u2018modified_huber\u2019, \u2018squared_hinge\u2019, \u2018perceptron\u2019, or a regression loss: \u2018squared_error\u2019, \u2018huber\u2019, \u2018epsilon_insensitive\u2019, or \u2018squared_epsilon_insensitive\u2019.  \n    The \u2018log\u2019 loss gives logistic regression, a probabilistic classifier. \u2018modified_huber\u2019 is another smooth loss that brings tolerance to outliers as well as probability estimates. \u2018squared_hinge\u2019 is like hinge but is quadratically penalized. \u2018perceptron\u2019 is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.\n\n* **penalty: {\u2018l2\u2019, \u2018l1\u2019, \u2018elasticnet\u2019}, default=\u2019l2\u2019**  \n    The penalty (aka regularization term) to be used. Defaults to \u2018l2\u2019 which is the standard regularizer for linear SVM models. \u2018l1\u2019 and \u2018elasticnet\u2019 might bring sparsity to the model (feature selection) not achievable with \u2018l2\u2019.\n    \n* **alpha: float, default=0.0001**  \n    Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when set to learning_rate is set to \u2018optimal\u2019.","232c94aa":"## [AdaBoost Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html)\n\n* **n_estimators: int, default=50**  \n    The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n    \n* **learning_rate: float, default=1.0**  \n    Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learning_rate and n_estimators parameters.","b626c558":"## Missing values","b678688e":"## Categorical variables","8192bbb1":"## [K-nearest neighbors](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)\n\nTune k-nearest neighbors model by changing some of its parameters.\n\n* **n_neighbors: int, default=5**  \n    Number of neighbors to use by default for kneighbors queries.\n\n\n* **weights: {\u2018uniform\u2019, \u2018distance\u2019} or callable, default=\u2019uniform\u2019**  \n    * Weight function used in prediction. Possible values:\n        * \u2018uniform\u2019 : uniform weights. All points in each neighborhood are weighted equally.\n        * \u2018distance\u2019 : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n        * [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n\n\n* **algorithm: {\u2018auto\u2019, \u2018ball_tree\u2019, \u2018kd_tree\u2019, \u2018brute\u2019}, default=\u2019auto\u2019**  \n    * Algorithm used to compute the nearest neighbors:  \n        * \u2018ball_tree\u2019 will use BallTree\n        * \u2018kd_tree\u2019 will use KDTree\n        * \u2018brute\u2019 will use a brute-force search.\n        * \u2018auto\u2019 will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n        \n> Note: fitting on sparse input will override the setting of this parameter, using brute force.\n\n\n* **leaf_size: int, default=30**  \n    Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n    \n* **p: int, default=2**  \n    Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n* **n_neighbors: int, default=5**  \n    Number of neighbors to use by default for kneighbors queries.","5d4ec42d":"## [Logistic Regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n\nTune the logistic regression model by changing some of its parameters.\n\nLogistic regression parameters:  \n\n* **solver: {\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019}, default=\u2019lbfgs\u2019**  \n    * Algorithm to use in the optimization problem. Default is \u2018lbfgs\u2019. To choose a solver, you might want to consider the following aspects:\n        * For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones;\n        * For multiclass problems, only \u2018newton-cg\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018lbfgs\u2019 handle multinomial loss;\n        * \u2018liblinear\u2019 is limited to one-versus-rest schemes.\n\n> **Warning**  \n> The choice of the algorithm depends on the penalty chosen: Supported penalties by solver:  \n> * \u2018newton-cg\u2019 - [\u2018l2\u2019, \u2018none\u2019]  \n> * \u2018lbfgs\u2019 - [\u2018l2\u2019, \u2018none\u2019]  \n> * \u2018liblinear\u2019 - [\u2018l1\u2019, \u2018l2\u2019]  \n> * \u2018sag\u2019 - [\u2018l2\u2019, \u2018none\u2019]  \n> * \u2018saga\u2019 - [\u2018elasticnet\u2019, \u2018l1\u2019, \u2018l2\u2019, \u2018none\u2019]  \n\n* **penalty: {\u2018l1\u2019, \u2018l2\u2019, \u2018elasticnet\u2019, \u2018none\u2019}, default=\u2019l2\u2019**  \n    * Specify the norm of the penalty:\n        * 'none': no penalty is added;\n        * 'l2': add a L2 penalty term and it is the default choice;\n        * 'l1': add a L1 penalty term;\n        * 'elasticnet': both L1 and L2 penalty terms are added.\n\n> **Warning**  \n> Some penalties may not work with some solvers. See the parameter solver below, to know the compatibility between the penalty and solver. \n\n* **C: float, default=1.0**  \n    Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n","c43e5a61":"### [Gaussian Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html)\n\n* **var_smoothing: float, default=1e-9**  \n    Portion of the largest variance of all features that is added to variances for calculation stability.","10fe1427":"## [Perceptron](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Perceptron.html)\n\n* **penalty: {\u2018l2\u2019,\u2019l1\u2019,\u2019elasticnet\u2019}, default=None**  \n    The penalty (aka regularization term) to be used.\n\n* **max_iter: int, default=1000**  \n    The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit method.\n    \n* **eta0: double, default=1**  \n    Constant by which the updates are multiplied.","a1205aa1":"### [Complement Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.ComplementNB.html)\n\n* **alpha: float, default=1.0**  \n    Additive (Laplace\/Lidstone) smoothing parameter (0 for no smoothing).\n\n* **fit_prior: bool, default=True**  \n    Only used in edge case with a single class in the training set.\n\n* **norm: bool, default=False**  \n    Whether or not a second normalization of the weights is performed. The default behavior mirrors the implementations found in Mahout and Weka, which do not follow the full algorithm described in Table 9 of the paper.","6670c893":"### [Bernoulli Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html)\n\n* **alpha: float, default=1.0**  \n    Additive (Laplace\/Lidstone) smoothing parameter (0 for no smoothing).\n    \n* **fit_prior: bool, default=True**  \n    Whether to learn class prior probabilities or not. If false, a uniform prior will be used.","1ab5771c":"# 3. Basic data check","5a00b489":"## [XGBClassifier](https:\/\/xgboost.readthedocs.io\/en\/stable\/parameter.html)\n\n* **eta [default=0.3, alias: learning_rate]**  \n    * Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n    * range: [0,1]\n\n\n* **gamma [default=0, alias: min_split_loss]**  \n    * Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n    * range: [0,\u221e]\n\n\n* **max_depth [default=6]**  \n    * Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 is only accepted in lossguide growing policy when tree_method is set as hist or gpu_hist and it indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree.\n    * range: [0,\u221e] (0 is only accepted in lossguide growing policy when tree_method is set as hist or gpu_hist)\n\n\n* **lambda [default=1, alias: reg_lambda]**  \n    L2 regularization term on weights. Increasing this value will make model more conservative.\n\n* **alpha [default=0, alias: reg_alpha]**  \n    L1 regularization term on weights. Increasing this value will make model more conservative."}}