{"cell_type":{"3a89780a":"code","45916aff":"code","68062314":"code","24a3e855":"code","b3140ec1":"code","c905d4b0":"code","23dccb95":"code","7422bdd1":"code","5cd8e7e6":"code","bc248d80":"code","5e2c99b5":"code","f79c29d5":"code","680b93f5":"code","15907792":"code","bc8cb522":"code","bed8ef9b":"code","7f27917c":"code","0f710bf7":"code","4b2b999e":"code","7f82c9e1":"code","537d8dc8":"code","f33a2ea2":"code","d8ddf901":"code","21353570":"code","85c4289f":"code","06f16e79":"code","e464173d":"code","5ad2c32d":"code","8d8ad4b2":"code","6362e972":"code","3c768c79":"code","f7e197ec":"code","40a5f70b":"code","7e177a3a":"code","048a003e":"code","1f39e205":"code","5eaa06ad":"code","49470e72":"code","fb8aa179":"code","e54a6a0f":"code","08558e92":"code","2359c1c0":"code","0a2ec715":"code","8cee4b43":"code","316cdb24":"code","50f0e9a8":"code","4630f8ad":"code","53f6e389":"code","a108a692":"code","91996d2c":"markdown","5950d5cb":"markdown","b76481c0":"markdown","e459326d":"markdown","a99dfeee":"markdown","6f2ef986":"markdown","f50536fc":"markdown","aee342e3":"markdown","63d32df1":"markdown","57a90864":"markdown","91d44e7f":"markdown","ff9aac93":"markdown","0c218d67":"markdown","33928a81":"markdown","209bc8eb":"markdown","5d072b8e":"markdown","a87d0f8f":"markdown","c309f261":"markdown","79c32208":"markdown","3946a762":"markdown"},"source":{"3a89780a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45916aff":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.head()","68062314":"df_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","24a3e855":"df = pd.concat([df_train,df_test], axis=0, sort=False)\ndf.shape","b3140ec1":"df = df.set_index('Id')","c905d4b0":"train_id = df_train.shape[0]\nSEED = 66","23dccb95":"for i in df.columns:\n    if df[i].isna().sum() > 0:\n        print(i + ' : ' + str(round(df[i].isna().sum() \/ df.shape[0] * 100, 2)) + ' %')","7422bdd1":"to_fill_none = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature', 'Exterior2nd']\nto_fill_0 = ['LotFrontage', 'MasVnrArea', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageArea', 'BsmtFinSF1', 'BsmtFinSF2', 'GarageCars']\nto_fill_freq = ['GarageYrBlt', 'Electrical', 'MSZoning', 'Utilities', 'KitchenQual', 'Functional', 'Exterior1st', 'SaleType']","5cd8e7e6":"for i in to_fill_none:\n    df[i].fillna('None', inplace=True)","bc248d80":"for i in to_fill_0:\n    df[i].fillna(0, inplace=True)","5e2c99b5":"for i in to_fill_freq:\n    df[i].fillna(df[i][:train_id].mode().item(), inplace=True)","f79c29d5":"# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n# Estimators\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n# Metrics\nfrom sklearn.metrics import mean_squared_error","680b93f5":"# We'll use label encoder by default\ndef encoder(dataset, encoder=LabelEncoder()):\n    var_obj = dataset.select_dtypes(include='object').columns\n    dataset_cop = dataset.copy()\n    le = encoder\n    for i in var_obj:\n        dataset_cop[i] = le.fit_transform(dataset_cop[i])\n    return dataset_cop","15907792":"def get_data(dataset):\n    \n    dataset = encoder(dataset)\n    \n    df_test = dataset[dataset.index > train_id].copy()\n    X = dataset.loc[set(dataset.index) - set(df_test.index)].copy()\n    y = X.pop('SalePrice')\n    \n    return X, y, df_test","bc8cb522":"def split(X, y, seed=SEED):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n    return X_train, X_test, y_train, y_test","bed8ef9b":"def scaler(X_train, X_test, scaler_func, df_test=None):\n    scal = scaler_func\n    X_train = scal.fit_transform(X_train)\n    X_test = scal.transform(X_test)\n    if df_test is not None:\n        df_test.drop('SalePrice', axis=1, inplace=True)\n        df_test = scal.transform(df_test)\n    return X_train, X_test, df_test","7f27917c":"# we'll use standard scaler by default\ndef get_score(dataset, estimator, scaler_func=StandardScaler()):\n    \n    dataset = encoder(dataset)\n    \n    X, y, _ = get_data(dataset)\n    \n    X_train, X_test, y_train, y_test = split(X, y)\n    \n    X_train, X_test, _ = scaler(X_train, X_test, scaler_func)\n    \n    model = estimator.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    # we use log to be on the same scale than the leaderboard\n    score = mean_squared_error(np.log(y_test), np.log(pred), squared=False)\n    return score","0f710bf7":"models = [RandomForestRegressor(), LGBMRegressor(), XGBRegressor(), CatBoostRegressor(verbose=0), ElasticNet(alpha=0.1, l1_ratio=0, random_state=6)]\nfor i in models:\n    print(str(i) + ' : ' + str(get_score(df, i)))","4b2b999e":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nvar_num = df.select_dtypes(include=['float', 'int']).columns\nc = 1 \nnrows = round(len(var_num) \/ 3) + 1\nfig = plt.figure(figsize=(20,30))\nfor i in var_num:\n    plt.subplot(nrows, 3, c)\n    sns.boxplot(df[i])\n    c += 1 \nplt.tight_layout()\nplt.show()","7f82c9e1":"df[df['GarageYrBlt'] > 2100]","537d8dc8":"df.loc[2593, 'GarageYrBlt'] = df.loc[2593, 'YearBuilt']","f33a2ea2":"contain_outliers = ['MSSubClass', 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']","d8ddf901":"def outliers(rate):\n    \n    train = df[:train_id]\n    limit = {}\n    \n    for i in contain_outliers:\n        limit[i] = train[i].quantile(rate)\n        \n    for i, j in limit.items():\n        train = train[train[i] < j]\n        \n    df_all = pd.concat([train, df[train_id:]])\n    \n    return str(rate) + ' : ' + str(get_score(df_all, CatBoostRegressor(verbose=0))) + ' : ' + str(df[:train_id].shape[0] - train.shape[0])","21353570":"rate = [0.994, 0.995, 0.996, 0.997, 0.998, 0.999]","85c4289f":"for i in rate:\n    print(outliers(i))","06f16e79":"train = df[:train_id]\nlimit = {}\nfor i in contain_outliers:\n    limit[i] = train[i].quantile(0.999)","e464173d":"for i, j in limit.items():\n    train = train[train[i] < j]","5ad2c32d":"df = pd.concat([train, df[train_id:]])","8d8ad4b2":"c = 1 \nnrows = round(len(var_num) \/ 3) + 1\nfig = plt.figure(figsize=(20,30))\nfor i in var_num:\n    plt.subplot(nrows, 3, c)\n    sns.histplot(df[i])\n    c += 1 \nplt.tight_layout()\nplt.show()","6362e972":"to_bin = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'GarageYrBlt', 'GarageArea']\nto_binarize = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', '2ndFlrSF', 'LowQualFinSF', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea']","3c768c79":"from sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import Binarizer","f7e197ec":"bins = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')","40a5f70b":"df_copy = df.copy()\ndf_copy[to_bin] = bins.fit_transform(df_copy[to_bin])","7e177a3a":"get_score(df_copy, CatBoostRegressor(verbose=0))","048a003e":"scoring = get_score(df, CatBoostRegressor(verbose=0))\nbetter_feat = []\nfor i in to_bin:\n    df_copy = df.copy()\n    df_copy[i] = bins.fit_transform(np.array(df_copy[i]).reshape(-1, 1))\n    scoring_new = get_score(df_copy, CatBoostRegressor(verbose=0))\n    if scoring_new < scoring:\n        better_feat.append(i)","1f39e205":"better_feat","5eaa06ad":"to_bin = ['LotArea', 'YearRemodAdd', 'BsmtUnfSF', 'GarageYrBlt']\ndf[to_bin] = bins.fit_transform(df[to_bin])\nget_score(df, CatBoostRegressor(verbose=0))","49470e72":"binarizer = Binarizer(threshold=0.5)","fb8aa179":"df_copy = df.copy()\ndf_copy[to_binarize] = binarizer.fit_transform(df_copy[to_binarize])","e54a6a0f":"get_score(df_copy, CatBoostRegressor(verbose=0))","08558e92":"scoring = get_score(df, CatBoostRegressor(verbose=0))\nbetter_feat = []\nfor i in to_binarize:\n    df_copy = df.copy()\n    df_copy[i] = binarizer.fit_transform(np.array(df_copy[i]).reshape(-1, 1))\n    scoring_new = get_score(df_copy, CatBoostRegressor(verbose=0))\n    if scoring_new < scoring:\n        better_feat.append(i)","2359c1c0":"better_feat","0a2ec715":"to_binarize = ['PoolArea']\ndf[to_binarize] = binarizer.fit_transform(df[to_binarize])\nget_score(df, CatBoostRegressor(verbose=0), StandardScaler())","8cee4b43":"import optuna\nfrom optuna.samplers import TPESampler","316cdb24":"def objective(trial):\n    param = {\n        \"loss_function\": trial.suggest_categorical(\"loss_function\", [\"RMSE\", \"MAE\"]),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e0),\n        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 1e0),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 20),\n        \"one_hot_max_size\": trial.suggest_int(\"one_hot_max_size\", 2, 20),    \n        'random_state': trial.suggest_categorical('random_state', [6, 9, 12, 15])\n    }\n    # Conditional Hyper-Parameters\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    reg = CatBoostRegressor(**param)\n    reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)\n    y_pred = reg.predict(X_test)\n    score = mean_squared_error(y_test, y_pred, squared=False)        \n    return score           ","50f0e9a8":"X, y, df_test = get_data(df)\nX_train, X_test, y_train, y_test = split(X, y)                                                                      ","4630f8ad":"\"\"\" \n\nThis will take a lot of time to run\n\nstudy = optuna.create_study(sampler=TPESampler(), direction=\"minimize\")\nstudy.optimize(objective, n_trials=10000, timeout=3600) # Run for 60 minutes     \nprint(\"Number of completed trials: {}\".format(len(study.trials)))   \nprint(\"Best trial:\")\ntrial = study.best_trial      \n     \nprint(\"\\tBest Score: {}\".format(trial.value))                                                                 \nprint(\"\\tBest Params: \")           \nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))     \n    \n\"\"\"","53f6e389":"def get_sub(dataset, model, scaler_func):\n    \n    X, y, df_test = get_data(dataset)\n    \n    X_train, X_test, y_train, y_test = split(X, y)\n    \n    X_train, X_test, df_test = scaler(X_train, X_test, scaler_func, df_test)\n    \n    model.fit(X_train, y_train)\n    \n    pred = model.predict(df_test)\n    sub['SalePrice'] = pred\n    \n    return sub","a108a692":"sub = get_sub(df, \n    CatBoostRegressor(loss_function='RMSE',\n                      learning_rate=0.07632340853297458,\n                      l2_leaf_reg=0.015170633257026214,\n                      colsample_bylevel=0.0956372937135792,\n                      depth=4,\n                      boosting_type='Plain',\n                      bootstrap_type='MVS',\n                      min_data_in_leaf=8,\n                      one_hot_max_size=12,\n                      verbose=0,\n                      random_state=12), \n    RobustScaler())\nsub.to_csv('submission.csv', index=False)","91996d2c":"I'll chose Catboost as estimator","5950d5cb":"# **Feature enginneering**","b76481c0":"To avoid data leakage we'll take the most frequent values on the train set","e459326d":"Two possible transformations here depending on the distribution of the features : \n* Bin the values\n* Transform the feature in a binary feature (for example, has or hasn't a pool)","a99dfeee":"The rate 0.999 seems to lead at the best result\n\nWe lost about fifty observations ","6f2ef986":"Some cleans due to visual inspection ","f50536fc":"# **Hyper parameter optimisation**","aee342e3":"Thanks to : \nhttps:\/\/www.kaggle.com\/saurabhshahane\/catboost-hyperparameter-tuning-with-optuna\n","63d32df1":"Using the bins transformation on these features leads to a worse result\n\nLet see the impact on the performance of each transformed feature","57a90864":"From the distribution of each features i selected those where dropping outliers could benefit the model","91d44e7f":"# **Submission**","ff9aac93":"# **Missing values**","0c218d67":"I'll use Optuna rather than grid search","33928a81":"# **Outliers**","209bc8eb":"Let's define some functions to automate some tasks\n","5d072b8e":"Several imputation strategies here depending on the data documentation","a87d0f8f":"# **Baseline model**","c309f261":"I define a function to test different rate where the observations above the rate will be deleted\n\nWe'll drop outliers only from the train set","79c32208":"Same process for the features to binarize","3946a762":"Let's take a look at the distribution of numerical variables"}}