{"cell_type":{"7aaa62fa":"code","b11f16f1":"code","6110b204":"code","84dae3f1":"code","5f69db08":"code","483be439":"code","02d96ebe":"code","76e87474":"code","5b61406b":"code","7ed490f2":"code","63f66a75":"code","d6312d50":"code","f881f5f1":"code","24101ef0":"code","9b37ce04":"code","53ffac73":"code","fc2afbbd":"code","cd7e5ec6":"code","dce2d019":"code","82e5ca09":"code","b90a32ee":"code","55227dda":"code","b4ab6ae4":"code","7dd819db":"code","635c8f8b":"code","d2937aa8":"code","fa54dcb7":"code","00f0a0b1":"code","78ab31a4":"code","88bc5739":"code","f3fbdeda":"code","5c59efe5":"code","fa64d0e3":"markdown","0dd527a3":"markdown","f8315128":"markdown","7c055468":"markdown","0f1a538f":"markdown"},"source":{"7aaa62fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b11f16f1":"# Read both train and test datasets as a DataFrame\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","6110b204":"numerical_train = train.select_dtypes(include=['int', 'float'])\nnumerical_train.drop(['Id', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'MoSold', 'YrSold'], axis=1, inplace=True)","84dae3f1":"train.head()","5f69db08":"sns.distplot(train['SalePrice']);","483be439":"# Statistics about the data\ntrain.describe()","02d96ebe":"train.info()","76e87474":"# Seperate out the numerical data\nnumerical_train = train.select_dtypes(include=['int', 'float'])\nnumerical_train.head()","5b61406b":"# Plot the distributions of all numerical data. \ni = 1\nfig = plt.figure(figsize=(40,50))\nfor item in numerical_train:\n    axes = fig.add_subplot(8,5,i)\n    axes = numerical_train[item].plot.hist(rot=0, subplots=True)\n    plt.xticks(rotation=45)\n    i += 1","7ed490f2":"# Seperate out the categorical data\ncategorical_train = train.select_dtypes(include=['object'])\ncategorical_train.head()","63f66a75":"# Plot the counts of all categorical data. \ni = 1\nfig = plt.figure(figsize=(40,50))\nfor item in categorical_train:\n    axes = fig.add_subplot(9,5,i)\n    axes = categorical_train[item].value_counts().plot.bar(rot=0, subplots=True)\n    plt.xticks(rotation=45)\n    i += 1","d6312d50":"# Boxplot all categorical data with SalePrice\ni = 1\nfig = plt.figure(figsize=(40,50))\nfor item in categorical_train:\n    data = pd.concat([train['SalePrice'], categorical_train[item]], axis=1)\n    axes = fig.add_subplot(9,5,i)\n    axes = sns.boxplot(x=item, y=\"SalePrice\", data=data)\n    plt.xticks(rotation=45)\n    i += 1","f881f5f1":"# Correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)","24101ef0":"# Correlation matrix with strong correlations with SalePrice\nsorted_corrs = train.corr()['SalePrice'].abs().sort_values()\nstrong_corrs = sorted_corrs[sorted_corrs > 0.5]\ncols = strong_corrs.index\ncorrmat = train[strong_corrs.index].corr()\nsns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)","9b37ce04":"#\u00a0The histogram on the diagonal is the distribution of a single variable \n#\u00a0The scatter plots represent the relationships between two variables\nsns.set()\ncols = strong_corrs.index\nsns.pairplot(numerical_train[cols], height=2.5)\nplt.show()","53ffac73":"df = train","fc2afbbd":"def transform_features(df):\n    # Count number of missing values in each numerical column\n    num_missing = df.isnull().sum()\n    # Drop the columns where at least 5% of the values are missing\n    drop_missing_cols = num_missing[(num_missing > len(df)\/20)].sort_values()\n    df = df.drop(drop_missing_cols.index, axis=1)\n    # Count number of missing values in each categorical column\n    text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n    # Drop the columns where at least 1 missing value\n    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n    df = df.drop(drop_missing_cols_2.index, axis=1)\n    # For numerical columns with missing values calcualate number of missing values\n    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n    fixable_numeric_cols = num_missing[(num_missing <= len(df)\/20) & (num_missing > 0)].sort_values()\n    # Calcualte the most common value for each column\n    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n    # For numerial columns with missing values fill with most common value in that column\n    df = df.fillna(replacement_values_dict)\n    # Compute two new columns by combining other columns which could be useful\n    years_sold = df['YrSold'] - df['YearBuilt']\n    years_since_remod = df['YrSold'] - df['YearRemodAdd']\n    df['YearsBeforeSale'] = years_sold\n    df['YearsSinceRemod'] = years_since_remod\n    # Drop the no longer needed original year columns\n    df = df.drop([\"YearBuilt\", \"YearRemodAdd\"], axis = 1)\n    # Remove irrelevant data\n    df = df.drop([\"Id\"], axis=1)\n    return df","cd7e5ec6":"transform_features(df)","dce2d019":"def select_features(df, uniq_threshold):\n    # Create list of all column names that are supposed to be categorical\n    nominal_features = [\"Id\", \"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \n                    \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \n                    \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \n                    \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n    # Check which categorical columns we have carried with us\n    transform_cat_cols = []\n    for col in nominal_features:\n        if col in df.columns:\n            transform_cat_cols.append(col)\n    # Check how many unique values in each categorical column\n    uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n    # For each item that has more than the defined unique threshold values, create category 'Other'\n    for item in uniqueness_counts.iteritems():\n        if item[1] >= uniq_threshold:\n            # Count unique values in the column\n            unique_val = df[item[0]].value_counts()\n            # Select the 10th least common index and the rest lower than that\n            other_index = unique_val.loc[unique_val < unique_val.iloc[uniq_threshold - 2]].index\n            df.loc[df[item[0]].isin(list(other_index)), item[0]] = 'Other'\n    # Select the text columns and convert to categorical\n    text_cols = df.select_dtypes(include=['object'])\n    for col in text_cols:\n        df[col] = df[col].astype('category')\n    # Create dummy columns\n    df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=['category']))], axis=1).drop(text_cols,axis=1)\n    return df","82e5ca09":"def drop_features(df, coeff_threshold):\n    # Select numerical columns\n    numerical_df = df.select_dtypes(include=['int', 'float'])\n    #print(numerical_df)\n    # Compute the absolute correlation between the numerical columns and SalePrice\n    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n    #print(abs_corr_coeffs)\n    # Drop the columns that have a coefficient lower than than the defined threshold\n    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n    return df","b90a32ee":"#split the data to train the model \n#y = train.SalePrice\n#X_train,X_test,y_train,y_test = train_test_split(train_df.drop(['SalePrice'], axis=1) ,y ,test_size=0.2 , random_state=0)","55227dda":"# Scale the data\n#scaler = StandardScaler().fit(train_df[features])\n#rescaled_train_df = scaler.transform(train_df[features])\n#rescaled_test_df = scaler.transform(test_df[features])\n\n#model = linear_model.LinearRegression()\n#model.fit(train_df[features], train[\"SalePrice\"])\n#predictions = model.predict(test_df[features])","b4ab6ae4":"transform_train_df = transform_features(train)\ntransform_test_df = transform_features(test)\ntrain_df = select_features(transform_train_df, uniq_threshold=100)\ntest_df = select_features(transform_test_df, uniq_threshold=100)","7dd819db":"train_features = drop_features(train_df, coeff_threshold=0.01)\ntest_features = test_df.columns\nfeatures = pd.Series(list(set(train_features) & set(test_features)))","635c8f8b":"X_train = train_df[features]\ny_train = train[\"SalePrice\"]\nX_test = test_df[features]\nX_train","d2937aa8":"#rfgs_parameters = {\n#    'n_estimators': [50],\n#    'max_depth'   : [n for n in range(2, 16)],\n#    'max_features': [n for n in range(2, 16)],\n#    \"min_samples_split\": [n for n in range(2, 8)],\n#    \"min_samples_leaf\": [n for n in range(2, 8)],\n#    \"bootstrap\": [True,False]\n#}\n#rfr_cv = GridSearchCV(RandomForestRegressor(), rfgs_parameters, cv=8, scoring='neg_mean_squared_log_error')","fa54dcb7":"#rfr_cv.fit(X_train, y_train)","00f0a0b1":"#predictions = rfr_cv.predict(X_test)","78ab31a4":"model_rf = RandomForestClassifier(n_estimators=1000, oob_score=True, random_state=42)","88bc5739":"model_rf.fit(X_train, y_train)","f3fbdeda":"predictions = model_rf.predict(X_test)","5c59efe5":"# Output the predictions into a csv\nsubmission = pd.DataFrame(test.Id)\npredictions = pd.DataFrame({'SalePrice': predictions})\noutput = pd.concat([submission,predictions],axis=1)\noutput.to_csv('submission.csv', index=False)","fa64d0e3":"**Algorithm Selection**\n\nWe will use Regression.","0dd527a3":"**Exploratory Analysis**\n\nThe aim is to get a good understanding of the data set and have ideas for data cleaning and feature engineering.\n\n1. Start by analysing the data to get a feel for what we have. \n    * Look at the features, their data type, the target variable.\n    * Check for missing data and the scale of the data. \n    * Make sure to understand the dataset. \n    \n\n2. Plot Numerical Distributions. \n    * Use histogram or scatter plots. \n    * Check if anything looks out of place.\n    \n    \n3. Plot Categorical Distributions. \n    * Use bar plots. \n    * Check for sparse classes, these can lead to overfitting. \n    \n    \n4. Plot Segmentations. \n    * Use box plots. \n    * Check the relationship between categorical and numeric features.\n    \n    \n5. Study Correlations. \n    * Use heat maps. \n    * Correlation is a value between -1 and 1. \n    * Close to -1 or 1 means strong negative or positive correlation. \n    * 0 means no correlation. \n    * Check which features are strongly correlated with the target. ","f8315128":"**Model Training**\n\n1. Tune and Fit Hyperparameters.\n    * Gradient descent algorithm\n    * k-fold cross validation\n \n \n2. Check error with performance metrics such as MSE.\n\n\n3. Select Winning Model.","7c055468":"**Feature Engineering**\n\nThe aim is to transform the data into a analytical base table. \n\n1. Combine Numerical Features.\n    * Sum\/multiply\/subtract features to create a new feature that could be more useful.\n\n\n2. Combine Sparse Categorical Classes.\n\n\n3. Add Knowledge.\n    * Create my own features indicate other useful information from my own knowledge. \n\n\n4. Add Dummy Variables.\n\n\n5. Remove Unused or RedundantFeatures.","0f1a538f":"**Data Cleaning**\n\nThe aim is to leave a clean data set that will avoid errors later on. \n\n1. Remove Unwanted Observations.\n    * Remove duplicated data.    \n    * Remove any data that is irrelevant for the task. \n\n\n2. Fix Structural Errors.\n    * Check for typos, inconsistent capitalisation and mislabeled classes.\n\n\n3. Filter Unwanted Outliers.\n    * Remove any data that is obviously wrong. \n\n\n4. Handle Missing Data.\n    * Dropping the data - sub-optimal because you lose all the information associated.\n    * Imputing the data e.g. using the mean - sub-optimal because your reinforcing patterns from other features.\n    * Flag observation with indicator that observation is missing and label numerical data as 0\n    * label categorical data as 'missing'.\n\n"}}