{"cell_type":{"b4e61f3d":"code","68cea427":"code","6d8e085e":"code","769c23ea":"code","a5e72b72":"code","708cb2e3":"code","f41dfa31":"code","53a0a3f7":"code","6d68fdfc":"code","84de1bb0":"code","ad67348c":"code","6bd8f627":"code","350f1cb3":"code","61ed1af6":"code","203b3f48":"code","11b9f878":"code","4c6014a8":"code","30c31f0a":"code","5105ebce":"code","c4b6dd6a":"code","2c7e9de5":"code","edea2ad3":"code","3606c0b0":"code","5b1f41ae":"code","92fffcd7":"code","28bc1d6f":"code","3792225d":"code","87b815cd":"code","61de79ae":"code","d7939d57":"code","da90817d":"code","6ab455ee":"code","3f0d70f4":"code","333d361a":"code","f2e0c5a2":"code","e164ced7":"code","2115141b":"code","363107f2":"code","730321c9":"code","3dc37c5c":"code","9822f961":"code","e9b7d3dd":"code","795efc99":"code","ab56f7f7":"code","ffe290e4":"code","db3f60f0":"code","1460741f":"code","5e666b6e":"code","87fbb8f2":"code","549271e6":"code","194830ff":"code","6167a3b1":"code","ebab9cc1":"code","e5324d82":"code","78cbc74e":"code","101029db":"code","a19ad5f8":"code","8b6fd3bc":"code","65660488":"code","214970ed":"code","997056b7":"code","8d488f15":"code","38e03538":"code","8d783b91":"code","f1997a5c":"code","b4ae1e1d":"code","76e64975":"code","31ffd5db":"code","6da62996":"code","dbc40adf":"code","5a9cec95":"code","466637e3":"code","a2a923e2":"code","6dfe08d4":"code","24575ad6":"code","c613486a":"code","9d208092":"code","f7479c5d":"code","94e86a06":"code","ec1bc81b":"code","8899adee":"code","791b2dc7":"code","5f824dff":"code","1144adff":"code","52584675":"code","79f4239b":"code","7abca150":"code","59ed2f75":"code","d394bc20":"code","57950914":"code","a06aaf4c":"code","eb18bbc2":"code","a35c6db8":"code","8a6a0c60":"code","25f36088":"markdown","4312fc87":"markdown","af0a51ea":"markdown","e4d77d66":"markdown","07740c56":"markdown","2b5dd444":"markdown","03a48774":"markdown","7840fc22":"markdown","c42f24a2":"markdown","90fd2892":"markdown","56f71b24":"markdown","c9e88b0a":"markdown","5848e80e":"markdown","40c42fd8":"markdown","48c0c7b1":"markdown","92941358":"markdown","cfbbf368":"markdown","3dd7357c":"markdown","f31949b9":"markdown","5c95f7b3":"markdown","1c0b468f":"markdown","80eeb754":"markdown","f0409dd6":"markdown","183225c9":"markdown","036c8742":"markdown","901b3db7":"markdown","13afd495":"markdown","b91ad055":"markdown","92ccebdb":"markdown","48152553":"markdown"},"source":{"b4e61f3d":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom scipy.stats.mstats import winsorize\n\nimport re\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\n\n%matplotlib inline\n\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 1000\npd.options.display.max_colwidth = 1000","68cea427":"# from google.colab import drive\n# drive.mount('\/content\/drive')","6d8e085e":"data = pd.read_csv('..\/input\/telecom-customer\/Telecom_customer churn.csv')\n# data = pd.read_csv('Telecom_customer churn.zip')\ntelco= data.copy()\ntelco.head()","769c23ea":"features=pd.read_csv(\"..\/input\/description\/description.csv\", index_col = 0)\nfeatures","a5e72b72":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Median = df.quantile(0.5)\n    Mean = df.mean()\n    Mode = df.mode().loc[0]\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max', 'Mean', 'Median','Mode']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max, Mean, Median, Mode], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    return str","708cb2e3":"!pip install colorama\ndef column_details(regex, df):\n  # We will focus on each column in detail\n  # Uniqe Values, DTYPE, NUNIQUE, NULL_RATE\n  global columns\n  columns=[col for col in df.columns if re.search(regex, col)]\n\n  from colorama import Fore, Back, Style\n\n  print('Unique Values of the Features:\\nfeature: DTYPE, NUNIQUE, NULL_RATE\\n')\n  for i in df[columns]:\n      color = Fore.RED if df[i].dtype =='float64' else Fore.BLUE if df[i].dtype =='int64' else Fore.GREEN\n      print(f'{i}: {color} {df[i].dtype}, {df[i].nunique()}, %{round(df[i].isna().sum()\/len(df[i])*100,2)}\\n{Style.RESET_ALL}{pd.Series(df[i].unique()).sort_values().values}\\n')\n      ","f41dfa31":"def null_values(df, rate=0):\n    \"\"\"a function to show null values with percentage\"\"\"\n    nv=pd.concat([df.isnull().sum(), 100 * df.isnull().sum()\/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n    return nv[nv['Percentage (%)']>rate].sort_values('Percentage (%)', ascending=False)","53a0a3f7":"def labels(ax, df, xytext=(0, 0)):\n    for bar in ax.patches: \n        ax.annotate('%{:.2f}\\n{:.0f}'.format(100*bar.get_height()\/len(df),bar.get_height()), (bar.get_x() + bar.get_width() \/ 2,  \n                    bar.get_height()), ha='center', va='center', \n                    size=11, xytext=xytext, \n                    textcoords='offset points')\n\ndef plot_col(col, df, target='Churn', figsize=(20,6)):\n\n    fig, ax = plt.subplots(1,2,figsize=figsize, sharey=True)\n\n    plt.subplot(121)\n    tmp = pd.crosstab(df[col], df[target], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NotChurn', 1:'Churn'}, inplace=True)\n\n    ax[0] = sns.countplot(x=col, data=df, hue=target, \n                  order=np.sort(df[col].dropna().unique()),\n                  )\n    ax[0].tick_params(axis='x', rotation=90)\n    labels(ax[0],df[col].dropna(),(0, 0))\n    \n    ax_twin = ax[0].twinx()\n    # sns.set(rc={\"lines.linewidth\": 0.7})\n    ax_twin = sns.pointplot(x=col, y='Churn', data=tmp, color='black', legend=False, \n                  order = np.sort(df[col].dropna().unique()), \n                  linewidth=0.1)\n    \n\n    ax[0].grid()\n\n    plt.subplot(122)\n    ax[1] = sns.countplot(x=df[col].dropna(),\n                  order= np.sort(df[col].dropna().unique()),\n                  )\n    ax[1].tick_params(axis='x', rotation=90)\n    labels(ax[1],df[col].dropna())\n    plt.show()\n","6d68fdfc":"def plot_cols(regex, figsize, target, df):\n  columns=[col for col in df.columns if re.search(regex, col)]\n  nrow, ncolumn = len(columns),1\n\n  fig, ax = plt.subplots(nrow, ncolumn,figsize=figsize)\n\n  for i,col in enumerate(columns):\n      order = np.sort(df[col].dropna().unique())\n      \n      plt.subplot(nrow,ncolumn,i+1)\n      ax[i] = sns.countplot(x=df[col], data= df, hue=target, order = order)\n  #     labels(ax[i],df[col].dropna(),(0,0))\n      \n      tmp = pd.crosstab(df[col], df[target], normalize='index') * 100\n      tmp = tmp.reset_index()\n      tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n      \n      ax_twin = ax[i].twinx()\n  #     sns.set(rc={\"lines.linewidth\": 1})\n      ax_twin = sns.pointplot(x=tmp[col], y=tmp['Fraud'],color='black', order = order)\n      ax[i].grid();","84de1bb0":"def hist_countplot(regex, figsize, nrow=10, ncolumn = 4, target='isFraud', df=pd.DataFrame()):\n  plt.figure(figsize=figsize)\n  columns=[col for col in df.columns if re.search(regex, col)]\n\n  for i,col in enumerate(columns):\n      plt.subplot(nrow,ncolumn,i+1)\n      if df[col].dtype!='O':\n          sns.histplot(x=df[col], data= df, hue=target)\n      else:\n          sns.countplot(x=df[col], data= df, hue=target) ","ad67348c":"def box_countplot(regex, figsize, nrow=10, ncolumn = 4, target='isFraud', df=pd.DataFrame()):\n  plt.figure(figsize=figsize)\n\n  columns=[col for col in df.columns if re.search(regex, col)]\n\n  for i,col in enumerate(columns):\n      plt.subplot(nrow,ncolumn,i+1)\n      if df[col].dtype!='O':\n          sns.boxplot(y=df[col], data= df, x=target)\n      else:\n          sns.countplot(x=df[col])","6bd8f627":"def box_labels(ax, df, col1,col2):\n    medians = df.groupby([col1])[col2].median().round(2)\n    vertical_offset = df[col2].median() * 0.05 # offset from median for display\n\n    for xtick in ax.get_xticks():\n        ax.text(xtick,medians[xtick] + vertical_offset,medians[xtick], \n                horizontalalignment='center',size='small',color='w',weight='semibold')","350f1cb3":"def stripplot(regex, figsize, nrow=10, ncolumn = 4, target='churn', df=pd.DataFrame()):\n  plt.figure(figsize=figsize)\n\n  columns=[col for col in df.columns if re.search(regex, col)]\n\n  for i,col in enumerate(columns):\n      plt.subplot(nrow,ncolumn,i+1)\n      sns.stripplot(y=df[col], data= df, x=target)","61ed1af6":"# Remove the highly collinear features from data\ndef remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model \n        to generalize and improves the interpretability of the model.\n\n    Inputs: \n        x: features dataframe\n        threshold: features with correlations greater than this value are removed\n\n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n\n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i+1):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n\n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n#                 print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns=drops)\n\n    return drops","203b3f48":"def corrank(X, threshold=0):\n    import itertools\n    df = pd.DataFrame([[i,j,X.corr().abs().loc[i,j]] for i,j in list(itertools.combinations(X.corr().abs(), 2))],columns=['Feature1','Feature2','corr'])    \n    df = df.sort_values(by='corr',ascending=False).reset_index(drop=True)\n    return df[df['corr']>threshold]","11b9f878":"import scipy.stats as sts\n\n# References:\n# https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\n# https:\/\/en.wikipedia.org\/wiki\/Cram%C3%A9r%27s_V\n\ndef cramers_v(x, y):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = sts.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))","4c6014a8":"def outliers(s):\n  # summary of the outliers\n  iqr = (np.quantile(s, 0.75))-(np.quantile(s, 0.25))\n  upper_bound = np.quantile(s, 0.75)+(1.5*iqr)\n  lower_bound = np.quantile(s, 0.25)-(1.5*iqr)\n  f = []\n  for i in s:\n      if i > upper_bound:\n          f.append(i)\n      elif i < lower_bound:\n          f.append(i)\n  sums = len(f)\n  pros = len(f)\/len(s)*100\n  d = {'IQR':iqr,\n        'Upper Bound':upper_bound,\n      'Lower Bound':lower_bound,\n      'Sum outliers': sums,'percentage outliers':pros}\n  d = pd.DataFrame(d.items(),columns = ['sub','values'])\n  return(d)\n","30c31f0a":"def capping_outliers(col, whisker=1.5):\n  # replace outliers with upper_bound and lower_bound values\n\n  iqr = (np.quantile(df[col], 0.75))-(np.quantile(df[col], 0.25))\n  upper_bound = np.quantile(df[col], 0.75)+(whisker*iqr)\n  lower_bound = np.quantile(df[col], 0.25)-(whisker*iqr)\n\n  df[col] = np.where(df[col] < upper_bound, \n                                upper_bound,df[col])\n  df[col] = np.where(df[col] > lower_bound, \n                                lower_bound,df[col])\n  plt.figure(figsize=(12,6))\n  sns.boxplot(x= df[col]);\n  return df[col]","5105ebce":"def replace_outliers(col, replaced_value='median',whisker=1.5):\n  # replace outliers with 'median','mean','mode' or an assigned value.\n\n  iqr = (np.quantile(df[col], 0.75))-(np.quantile(df[col], 0.25))\n  upper_bound = np.quantile(df[col], 0.75)+(whisker*iqr)\n  lower_bound = np.quantile(df[col], 0.25)-(whisker*iqr)\n  if replaced_value=='median':\n    df[col] = df[col].mask(df[col] > upper_bound, df[col].median())\n    df[col] = df[col].mask(df[col] < lower_bound, df[col].median())\n  elif replaced_value=='mean':\n    df[col] = df[col].mask(df[col] > upper_bound, df[col].mean())\n    df[col] = df[col].mask(df[col] < lower_bound, df[col].mean())\n  elif replaced_value=='mode':\n    df[col] = df[col].mask(df[col] > upper_bound, df[col].mode()[0])\n    df[col] = df[col].mask(df[col] < lower_bound, df[col].mode()[0])\n  else:\n    df[col] = df[col].mask(df[col] > upper_bound, replaced_value)\n    df[col] = df[col].mask(df[col] < lower_bound, replaced_value)   \n\n  plt.figure(figsize=(12,6))\n  sns.boxplot(x= df[col]);\n\n  return df[col]","c4b6dd6a":"def col_plot(df,col_name):\n    plt.figure(figsize=(15,6))\n    \n    plt.subplot(141) # 1 satir x 4 sutun dan olusan ax in 1. sutununda calis\n    plt.hist(df[col_name], bins = 20)\n    f_sqrt=lambda x:(np.sqrt(x) if x>1 else -np.sqrt(-x) if x<-1 else x)\n    f_log=lambda x:(np.log(x)+1 if x>1 else -np.log(-x)-1 if x<-1 else x)\n    \n    # \u00fc\u00e7 sigma aralikta(verinin %99.7 sini icine almasi beklenen bolum) iki kirmizi cizgi arasinda\n    plt.axvline(x=df[col_name].mean() + 3*df[col_name].std(),color='red')\n    plt.axvline(x=df[col_name].mean() - 3*df[col_name].std(),color='red')\n    plt.xlabel(col_name)\n    plt.tight_layout\n    plt.xlabel(\"Histogram \u00b13z\")\n    plt.ylabel(col_name)\n\n    plt.subplot(142)\n    plt.boxplot(df[col_name]) # IQR katsayisi, defaultu 1.5\n    plt.xlabel(\"IQR=1.5\")\n\n    plt.subplot(143)\n    plt.boxplot(df[col_name].apply(f_sqrt), whis = 1.5)\n    plt.xlabel(\"ROOT SQUARE - IQR=1.5\")\n\n    plt.subplot(144)\n    plt.boxplot(df[col_name].apply(f_log), whis = 1.5)\n    plt.xlabel(\"LOGARITMIC - IQR=1.5\")\n    plt.show()","2c7e9de5":"def plot_winsorize(df,col_name,down=0, up=0.1):\n    plt.figure(figsize = (15, 6))\n\n    winsor=winsorize(df[col_name], (down,up))\n    f_sqrt=lambda x:(np.sqrt(x) if x>1 else -np.sqrt(-x) if x<-1 else x)\n    root_winsor=winsorize(df[col_name].apply(f_sqrt), (down,up))\n\n    plt.subplot(141)\n    plt.hist(winsor, bins = 22)\n    plt.axvline(x=winsor.mean()+3*winsor.std(),color='red')\n    plt.axvline(x=winsor.mean()-3*winsor.std(),color='red')\n    plt.xlabel('Winsorize_Histogram')\n    plt.ylabel(col_name)\n    plt.tight_layout\n\n    plt.subplot(142)\n    plt.boxplot(winsor, whis = 1.5)\n    plt.xlabel('Winsorize - IQR:1.5')\n    \n    plt.subplot(143)\n    plt.hist(root_winsor, bins=22)\n    plt.axvline(x=root_winsor.mean()+3*root_winsor.std(),color='red')\n    plt.axvline(x=root_winsor.mean()-3*root_winsor.std(),color='red')\n    plt.xlabel('root_winsor_col_name')\n\n    plt.subplot(144)\n    plt.boxplot(root_winsor, whis = 1.5)\n    plt.xlabel(\"Root & Winsorize - IQR=1.5\")\n    plt.show() ","edea2ad3":"def plot_log_winsorize(df,col_name,up=0.1,down=0):\n    plt.figure(figsize = (15, 6))\n\n    winsor=winsorize(df[col_name], (down,up))\n    f_log=lambda x:(np.log(x)+1 if x>1 else -np.log(-x)-1 if x<-1 else x)\n    log_winsor=winsorize(df[col_name].apply(f_log), (down,up))\n\n    plt.subplot(141)\n    plt.hist(winsor, bins = 22)\n    plt.axvline(x=winsor.mean()+3*winsor.std(),color='red')\n    plt.axvline(x=winsor.mean()-3*winsor.std(),color='red')\n    plt.xlabel('Winsorize_Histogram')\n    plt.ylabel(col_name)\n    plt.tight_layout\n\n    plt.subplot(142)\n    plt.boxplot(winsor, whis = 1.5)\n    plt.xlabel('Winsorize - IQR:1.5')\n    \n    plt.subplot(143)\n    plt.hist(log_winsor, bins=22)\n    plt.axvline(x=log_winsor.mean()+3*log_winsor.std(),color='red')\n    plt.axvline(x=log_winsor.mean()-3*log_winsor.std(),color='red')\n    plt.xlabel('log_winsor_col_name')\n\n    plt.subplot(144)\n    plt.boxplot(log_winsor, whis = 1.5)\n    plt.xlabel(\"Log & Winsorize - IQR=1.5\")\n    plt.show()","3606c0b0":"def simplify_column(col, df, threshold=0.005, value='mode'):\n  df[col] = df[col].replace(df[col].value_counts(dropna=True)[df[col].value_counts(dropna=True, normalize=True)<threshold].index,df[col].mode()[0] if value=='mode' else 'other')\n  return df[col]","5b1f41ae":"# Memory Reduction\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","92fffcd7":"# Label Encoding\n\nfrom sklearn.preprocessing import LabelEncoder\n# def label_encoder(df):\n#   for col in df.columns:\n#     if df[col].dtype=='object':\n#       le = LabelEncoder()\n#       df[col] = le.fit_transform(df[col])\n#   return df\n\n\n# from sklearn.preprocessing import LabelEncoder\ndef label_encoder(cat_cols, df):\n  for col in cat_cols:\n    # if df[col].dtype=='object':\n    if col in df.columns:\n      le = LabelEncoder()\n      # le.fit(list(df[col].astype(str).values))\n      df[col] = le.fit_transform(list(df[col].astype(str).values))\n  return df","28bc1d6f":"# Frequency Encoding\n\ndef frequency_encoder(cat_cols, df):\n  for col in cat_cols:\n    if col in df.columns:\n      df= df.join(df[col].map(df[col].value_counts(normalize=True)).to_frame().add_suffix('_freq'))\n  return df","3792225d":"# Frequency Encoding\n\ndef frequency_encoder(cat_cols, df):\n  for col in cat_cols:\n    if col in df.columns:\n      df= df.join(df[col].map(df[col].value_counts(normalize=True)).to_frame().add_suffix('_freq'))\n  return df","87b815cd":"def plot_feature_importances(model, num=10, figsize=(20,10)):\n  feature_imp = pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)[:num]\n  plt.figure(figsize=figsize)\n  sns.barplot(x=feature_imp, y=feature_imp.index)\n  plt.title(\"Feature Importance\")\n  plt.show()","61de79ae":"telco.info()","d7939d57":"null_values(telco)","da90817d":"telco.columns","6ab455ee":"telco = telco.drop('Customer_ID', axis=1)","3f0d70f4":"cat_cols = [col for col in telco.columns if telco[col].dtype=='object']","333d361a":"columns=[]\ncolumn_details(regex='', df=telco[cat_cols])","f2e0c5a2":"# # Total number of kids of a customer\n# kid_cols = ['kid0_2', 'kid3_5', 'kid6_10', 'kid11_15', 'kid16_17']\n# telco['total_kid']=telco[kid_cols].replace({'Y':1,'U':0}).apply(sum, axis=1)\n\n# # Average number of blocked (failed) voice calls \/ Average number of voice call attempts made \n# telco['vce_blk_rate'] = (telco['blck_vce_Mean'] \/ telco['plcd_vce_Mean']).fillna(0)\n\n# # Average number of dropped (failed) voice calls \/ Average number of voice call attempts made\n# telco['vce_drp_rate'] = (telco['drop_vce_Mean'] \/ telco['plcd_vce_Mean']).fillna(0)\n\n# # Average number of blocked (failed) data calls \/ Average number of data call attempts made\n# telco['dat_blk_rate'] = (telco['blck_dat_Mean'] \/ telco['plcd_dat_Mean']).fillna(0)\n\n# # Average number of dropped (failed) data calls \/ Average number of data call attempts made\n# telco['dat_drp_rate'] = (telco['drop_dat_Mean'] \/ telco['plcd_dat_Mean']).fillna(0)\n\n# # Average number of completed voice calls \/ Average number of voice call attempts made \n# telco['vce_cmpt_rate'] = (telco['comp_vce_Mean'] \/ telco['plcd_vce_Mean']).fillna(0)\n\n# # Average number of completed data calls \/ Average number of data call attempts made\n# telco['dat_cmpt_rate'] = (telco['comp_dat_Mean'] \/ telco['plcd_dat_Mean']).fillna(0)\n\n# # Average number of completed searches \/ Average number of attempted calls\n# telco['tot_cmpt_rate'] = (telco['complete_Mean'] \/ telco['attempt_Mean']).fillna(0)\n\n# # Average number of dropped or blocked calls \/ Average number of attempted calls\n# telco['tot_drp_blk_rate'] = (telco['drop_blk_Mean'] \/ telco['attempt_Mean']).fillna(0)\n\n# # Average number of voice call attempts made  \/ Average number of voice and data call attempts made\n# telco['vce_dat_ratio'] = (telco['plcd_vce_Mean'] \/  (telco['plcd_vce_Mean'] + telco['plcd_dat_Mean'])).fillna(0)\n\n# # (Average monthly usage minutes in the previous three months - Average monthly usage minutes over the customer's lifetime)  \/ Average monthly usage minutes over the customer's lifetime\n# telco['diff_3mon_overall_mou'] = ((telco['avg3mou'] - telco['avgmou']) \/ telco['avgmou']).fillna(0)\n\n# # (Average monthly searches over the previous three months - Average monthly calls over the customer's lifetime)  \/ Average monthly calls over the customer's lifetime\n# telco['diff_3mon_overall_qty'] = ((telco['avg3qty'] - telco['avgqty']) \/ telco['avgqty']).fillna(0)\n\n# # (Average monthly income over the previous three months - Average monthly income over the customer's lifetime)  \/ Average monthly income over the customer's lifetime\n# telco['diff_3mon_overall_rev'] = ((telco['avg3rev'] - telco['avgrev']) \/ telco['avgrev']).fillna(0)\n\n# # (Average monthly usage minutes in the previous six months - Average monthly usage minutes over the customer's lifetime)  \/ Average monthly usage minutes over the customer's lifetime\n# telco['diff_6mon_overall_mou'] = ((telco['avg6mou'] - telco['avgmou']) \/ telco['avgmou']).fillna(0)\n\n# # (Average monthly searches over the previous six months - Average monthly calls over the customer's lifetime)  \/ Average monthly calls over the customer's lifetime\n# telco['diff_6mon_overall_qty'] = ((telco['avg6qty'] - telco['avgqty']) \/ telco['avgqty']).fillna(0)\n\n# # (Average monthly income over the previous six months - Average monthly income over the customer's lifetime)  \/ Average monthly income over the customer's lifetime\n# telco['diff_6mon_overall_rev'] = ((telco['avg6rev'] - telco['avgrev']) \/ telco['avgrev']).fillna(0)\n\n# # Number of missing values in every observation\n# telco['total_nulls'] = data.isnull().sum(axis=1)\n\n# # analog to digital transformation\n# telco['eqpdays_digitized'] = np.digitize(telco['eqpdays'], bins=list(range(-30,901,30))+[telco['eqpdays'].max()+1])\n\n# telco.replace([np.inf, -np.inf], np.nan, inplace=True)","e164ced7":"cols_FE = ['vce_blk_rate','vce_drp_rate','dat_blk_rate','dat_drp_rate','vce_cmpt_rate','dat_cmpt_rate','tot_cmpt_rate',\n          'tot_drp_blk_rate','vce_dat_ratio','diff_3mon_overall_mou','diff_3mon_overall_qty','diff_3mon_overall_rev',\n          'diff_6mon_overall_mou','diff_6mon_overall_qty','diff_6mon_overall_rev']","2115141b":"num_cols = [col for col in telco.columns if telco[col].dtype!='object']\nsummary(telco[num_cols])","363107f2":"import missingno as msno\nmsno.matrix(telco.sample(200));","730321c9":"drop_col = remove_collinear_features(telco[num_cols], 0.9)\nprint(drop_col)","3dc37c5c":"telco = telco.drop(drop_col, axis=1)","9822f961":"plt.figure(figsize=(30,20))\nsns.heatmap(telco.corr(), cmap='coolwarm',annot=False);","e9b7d3dd":"cramers_v(telco.dwlltype,telco.dwllsize)","795efc99":"summary(telco[['dwlltype','dwllsize']])","ab56f7f7":"# plot_col('dwlltype', df=telco, target='churn')\n# plot_col('dwllsize', df=telco, target='churn')","ffe290e4":"telco = telco.drop('dwlltype', axis=1)","db3f60f0":"cat_cols = [col for col in telco.columns if telco[col].dtype=='object']\nnum_cols = [col for col in telco.columns if telco[col].dtype!='object']","1460741f":"telco[cat_cols].describe(include=['O']).T","5e666b6e":"telco = frequency_encoder(cat_cols, telco)\ntelco = telco.drop(cat_cols, axis=1)","87fbb8f2":"telco.shape","549271e6":"import gc\n\ntelco = reduce_mem_usage(telco)\ngc.collect()","194830ff":"# Iterative Imputer default=BayesianRidge()\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nidf = telco.copy()\n\nimp_median = IterativeImputer(missing_values=np.nan, initial_strategy='median', random_state=42)\ndf_imputed_bayesian = pd.DataFrame(imp_median.fit_transform(idf), index=idf.index, columns=idf.columns)\nnull_values(df_imputed_bayesian)","6167a3b1":"df_imputed_bayesian.to_pickle('.\/clean_dataset_11a.pkl')","ebab9cc1":"telco = df_imputed_bayesian.copy()","e5324d82":"# from sklearn.preprocessing import MinMaxScaler\n# PCA_columns = [col for col in cols_FE if col in telco.columns]\n# sc = MinMaxScaler()\n# telco[PCA_columns] = sc.fit_transform(telco[PCA_columns])","78cbc74e":"# plt.figure(figsize=(30,6))\n# pca = PCA().fit(telco[PCA_columns])\n# x = range(1,len(PCA_columns)+1)\n# plt.plot(x,np.cumsum(pca.explained_variance_ratio_), \"bo-\")\n# plt.xlabel(\"Component Count\")\n# plt.ylabel(\"Variance Ratio\")\n# plt.xticks(range(1,telco[PCA_columns].shape[1]+1))\n# plt.grid()\n# plt.show()","101029db":"# pca = PCA(n_components = 3)\n# pca.fit(telco[PCA_columns])\n# pca_telco = pca.transform(telco[PCA_columns])\n\n# np.cumsum(pca.explained_variance_ratio_)","a19ad5f8":"# pca_telco = pd.DataFrame(data = pca_telco).add_prefix('pca_')\n# telco = pd.concat([telco, pca_telco], ignore_index=False, sort=False, axis=1)\n# telco.drop(PCA_columns, axis=1, inplace=True)","8b6fd3bc":"telco.to_pickle('.\/clean_dataset_11b.pkl')","65660488":"outliers_cols= ['drop_dat_Mean','blck_dat_Mean', \n               'unan_dat_Mean', 'plcd_dat_Mean','recv_sms_Mean', \n               'mou_cdat_Mean','mou_pead_Mean','callfwdv_Mean','churn']\n\nstripplot('', figsize=(25,70), nrow=23, ncolumn = 4, target='churn', df=telco[outliers_cols])","214970ed":"outliers_cols.remove('churn')\nfor col in outliers_cols:\n  telco[col] = telco[col].replace({telco[col].max():telco[col].median()})\n  telco[col] = telco[col].replace({telco[col].max():telco[col].median()})\n  telco[col] = telco[col].replace({telco[col].max():telco[col].median()})","997056b7":"telco.to_pickle('.\/clean_dataset_11c.pkl')","8d488f15":"# telco= pd.read_pickle('\/content\/drive\/MyDrive\/Colab Notebooks\/Telecom_customer\/clean_dataset_11c.pkl')","38e03538":"import pandas as pd\nimport numpy as np\nfrom numpy import percentile\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, scale, LabelEncoder\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.metrics import confusion_matrix, roc_curve, classification_report, plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, roc_auc_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nimport multiprocessing\nfrom IPython.core.pylabtools import figsize\nfont_title = {'family': 'times new roman', 'color': 'darkred', \n              'weight': 'bold', 'size': 14}\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")\n\nplt.rcParams['figure.dpi'] = 100","8d783b91":"lgb = LGBMClassifier(learning_rate= 0.05, \n                    max_depth= 12, \n                    n_estimators= 1000, \n                    subsample= 0.1)\n\nrfe = RFECV(estimator=lgb, step=10, cv=KFold(n_splits=5, shuffle=False), scoring='accuracy', verbose=2)\n\nX = telco.drop(['churn'], axis=1)\ny = telco['churn']\n\nrfe.fit(X, y)","f1997a5c":"print('Optimal number of features:', rfe.n_features_)","b4ae1e1d":"plt.figure(figsize=(6,4))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score\")\nplt.plot(range(1, len(rfe.grid_scores_) + 1), rfe.grid_scores_)\nplt.show()","76e64975":"rfecv_cols = [col for col in X.columns[rfe.ranking_ == 1]]\nlen(rfecv_cols)","31ffd5db":"rfecv_cols+=['churn']","6da62996":"telco[rfecv_cols].to_pickle('.\/clean_dataset_11d.pkl')","dbc40adf":"telco= pd.read_pickle('.\/clean_dataset_11d.pkl')","5a9cec95":"print(\"Percentage of Churned Customer:%\",\n      round(telco.churn.mean(),5))","466637e3":"X = telco.drop(['churn'], axis=1)\ny = telco['churn']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state =42)","a2a923e2":"sc = RobustScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc= sc.transform(X_test)","6dfe08d4":"cv_acc_train = {}\ncv_acc_test = {}\ncv_precision = {}\ncv_recall = {}\ncv_fallout = {}\ncv_f1 = {}\ncv_AUC = {}","24575ad6":"def plot_result(model, name:str):\n  global X_train, X_test, y_train, y_test\n  if name=='lr':\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test= sc.transform(X_test)\n\n  model.fit(X_train, y_train)\n  y_pred = model.predict(X_test)\n\n  # Evaluation based on a 10-fold cross-validation\n  scores = cross_validate(model, X_test, y_test, \n                        scoring=['balanced_accuracy','precision','recall','f1','roc_auc'], cv=10)\n\n  df_scores = pd.DataFrame(scores, index = range(1,11))\n  cv_acc_train[name] = cross_val_score(model, X_train, y_train, cv=10, scoring = 'balanced_accuracy').mean()\n  cv_acc_test[name] = df_scores.mean()[2:].iloc[0]\n  cv_precision[name] = df_scores.mean()[2:].iloc[1]\n  cv_recall[name] = df_scores.mean()[2:].iloc[2]\n  cv_fallout[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())\n  cv_f1[name] = df_scores.mean()[2:].iloc[3]\n  cv_AUC[name] = df_scores.mean()[2:].iloc[4]\n\n  # accuracy scores\n  print('Average Balanced Accuracy (CV=10), Test Set:', cv_acc_test[name])  \n  print('Average Balanced Accuracy (CV=10), Training Set: ', cv_acc_train[name])\n\n  # print classification report\n  print(classification_report(y_test, y_pred, zero_division=0))\n\n  # Plot Confusion Matrix\n  plot_confusion_matrix(model, X_test, y_test, values_format='d')\n  plt.grid(False)\n  plt.show()\n\ndef get_metrics():\n  df_eval = pd.DataFrame(data={'model': list(cv_acc_test.keys()), \n                              'bal_acc_train':list(cv_acc_train.values()),\n                              'bal_acc_test': list(cv_acc_test.values()), \n                              'precision': list(cv_precision.values()), \n                              'recall': list(cv_recall.values()), \n                              'fallout':list(cv_fallout.values()), \n                              'f1': list(cv_f1.values()), \n                              'AUC': list(cv_AUC.values())}).round(3)\n  return df_eval","c613486a":"from lightgbm import LGBMClassifier","9d208092":"# lgb_params = {\"n_estimators\": [500,1000],\n#              \"subsample\":[0.1],\n#              \"max_depth\":[12,15],\n#              \"learning_rate\":[0.1,0.05]}","f7479c5d":"# lgb_grid= GridSearchCV(lgb, lgb_params, cv = 5, \n#                             n_jobs = -1, verbose = 2).fit(X_train, y_train)","94e86a06":"# lgb_grid.best_params_","ec1bc81b":"lgb = LGBMClassifier(learning_rate= 0.05, \n                    max_depth= 12, \n                    n_estimators= 1000, \n                    subsample= 0.1)\n\nplot_result(lgb, \"lgb\")","8899adee":"# Cross Validation Kfold=10\nget_metrics()","791b2dc7":"from xgboost import XGBClassifier\nxgb= XGBClassifier()\nplot_result(xgb, \"xgb\")","5f824dff":"get_metrics()","1144adff":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nplot_result(rfc, \"rfc\")","52584675":"get_metrics()","79f4239b":"lr=LogisticRegression()\nplot_result(lr, \"lr\")","7abca150":"get_metrics()","59ed2f75":"feature_imp = pd.Series(xgb.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(18,12))\nsns.barplot(x=feature_imp[:50], y=feature_imp[:50].index)\nplt.title(\"Feature Importance\")\nplt.show()","d394bc20":"feature_imp = pd.Series(rfc.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(18,12))\nsns.barplot(x=feature_imp[:50], y=feature_imp[:50].index)\nplt.title(\"Feature Importance\")\nplt.show()","57950914":"feature_imp = pd.Series(lgb.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(18,12))\nsns.barplot(x=feature_imp[:50], y=feature_imp[:50].index)\nplt.title(\"Feature Importance\")\nplt.show()","a06aaf4c":"df_eval = get_metrics()\ndf_eval","eb18bbc2":"def labels(ax):\n    for p in ax.patches:\n        width = p.get_width()    # get bar length\n        ax.text(width,       # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n                '{:1.3f}'.format(width), # set variable to display, 2 decimals\n                ha = 'left',   # horizontal alignment\n                va = 'center')  # vertical alignment\n\nfor i,col in enumerate(['bal_acc_test','recall','f1','AUC']):\n  plt.subplot(4,1,i+1)\n  ax = sns.barplot(x=col, y=\"model\", data=df_eval.sort_values(by=col, ascending=False), palette=\"Blues_d\")\n  labels(ax)\n  plt.show()","a35c6db8":"sns.relplot(x=\"f1\", y=\"AUC\", hue=\"model\", size=\"bal_acc_test\", sizes=(40, 400), \n            alpha=1, palette=\"bright\", height=4, legend='full', data=get_metrics());","8a6a0c60":"import pickle\nimport pandas as pd\npickle.dump(lgb,open(\".\/LightGBM.pkl\",\"wb\"), protocol=4)","25f36088":"## FUNCTIONS","4312fc87":"LightGBM is the best model. Lets save it.","af0a51ea":"## EDA","e4d77d66":"### Feature Importance for Random Forest","07740c56":"### Random Forest Classifier","2b5dd444":"#### Correlation Functions","03a48774":"### Logistic Regression","7840fc22":"### Feature Importance for LightGBM","c42f24a2":"### Saving Model","90fd2892":"### Memory Reduction Functions","56f71b24":"## Building Models","c9e88b0a":"### Modeling","5848e80e":"### Understanding Data","40c42fd8":"### LightGBM Classifier","48c0c7b1":"### Multivariate Imputation","92941358":"### Encoders","cfbbf368":"### Feature Importance for XGBoost","3dd7357c":"### PCA","f31949b9":"## Telecom customer churn prediction","5c95f7b3":"#### Outlier Functions","1c0b468f":"## Compare Models","80eeb754":"#### Plot Functions","f0409dd6":"### Multicolliniarity","183225c9":"### XGBoost Classifer","036c8742":"#### Description Functions","901b3db7":"##RFECV","13afd495":"### Frequency Encoding","b91ad055":"### Feature Engineering","92ccebdb":"This data set consists of 100 variables and approx 100 thousand records. This data set contains different variables explaining the attributes of telecom industry and various factors considered important while dealing with customers of telecom industry. The target variable here is churn which explains whether the customer will churn or not. We can use this data set to predict the customers who would churn or who wouldn't churn depending on various variables available.","48152553":"### Handling Outliers"}}