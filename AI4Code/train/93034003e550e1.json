{"cell_type":{"3042a97e":"code","d6d4e090":"code","7ca69257":"code","793ffc4f":"code","92fc46ec":"code","05f80ba9":"code","803d5c36":"code","937189a0":"code","a4f76b43":"code","9f1f014d":"code","83899470":"code","3c377548":"markdown","f53a40d1":"markdown","34f91a99":"markdown","3c8b480b":"markdown","0a63d26c":"markdown"},"source":{"3042a97e":"from catboost import Pool, CatBoostRegressor\n# Import order: data manipulating -> machine\/deep learning -> utilities\/helpers\/improvement -> configuration\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nfrom joblib import Parallel, delayed\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n\nfrom hpsklearn import HyperoptEstimator\nfrom hpsklearn import any_regressor\nfrom hpsklearn import any_preprocessing\nfrom hyperopt import tpe\nfrom  hyperopt import hp","d6d4e090":"# Define data_directory and data_read func\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","7ca69257":"# Calculate 1st WAP\ndef calc_wap1(df):\n    wap = (df['ask_price1'] * df['bid_size1'] + df['bid_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n# Calculate 2nd WAP\ndef calc_wap2(df):\n    wap = (df['ask_price2'] * df['bid_size2'] + df['bid_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Calculate Log Return\ndef log_return(series):\n    return np.log(series).diff() # log(x \/ y) = log(x) - log(y), ref[2]\n\n# Realized Volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Count Unique Elements of Series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Calculate features as specifized feature_dict\ndef calc_features(df, feature_dict):\n    # Calculate STATs (sum, mean, std) for different time-window (seconds in bucket)\n    def calc_certain_window(window, add_suffix=False):\n        # Filter by time-window, Groupy by time_id, then Apply feature_dict\n        df_feature = df[df['seconds_in_bucket'] >= window].groupby(['time_id']).agg(feature_dict).reset_index()\n        # Rename features\/columns by joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix for different time-window\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(window))\n        return df_feature\n    \n    windows = [0, 150, 300, 450]\n    df_feature = pd.DataFrame()\n    \n    for window in windows:\n        if window == 0:\n            df_feature = calc_certain_window(window=window, add_suffix=False)\n        else:\n            df_feature_tmp = calc_certain_window(window=window, add_suffix=True)\n            df_feature = df_feature.merge(df_feature_tmp, how='left', left_on='time_id_', right_on='time_id__'+str(window))\n        \n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n        \n    return df_feature\n\n\n# Preprocess book-data (applied for each stock_id)\ndef preprocess_book(file_path):\n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate Log-Return\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate Wap-Balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate Various-Spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Feature (Generating) Dict for aggregated operations\n    feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    df_feature = calc_features(df, feature_dict=feature_dict)\n    \n    # Generate row_id (for later merge)\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    # Drop the left time_id_ (after using for generating row_id)\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n# Preprocess trade-Data (applied for each stock_id)\ndef preprocess_trade(file_path):\n    df = pd.read_parquet(file_path)\n    \n    # Calculate Log-Return\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Feature (Generating) Dict for aggregated operations\n    feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = calc_features(df, feature_dict=feature_dict)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\n# Preprocess\/Feature-Engineering in parallel (applied for each stock_id)\ndef preprocess(list_stock_ids, is_train=True):\n    \n    def preprocess_for_stock_id(stock_id):\n        # Generate file_path for train-dataset\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # ... for test-dataset\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book- and trade- data, then merge both\n        df_tmp = pd.merge(preprocess_book(file_path_book), preprocess_trade(file_path_trade), on='row_id', how='left')\n\n        return df_tmp\n    \n    # Parallelize Preprocessing for Every stock_id\n    df = Parallel(n_jobs=-1, verbose=1)(delayed(preprocess_for_stock_id)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate All Dataframes from Parallelized Preprocessing\n    df = pd.concat(df, ignore_index=True)\n    \n    return df\n\n# Calculate STATs (mean, std, max, min) for realized volatility while groupped by stock_id and time_id\ndef get_time_stock(df_feature):\n    # Enumerate realized volatility features\/columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df_feature.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df_feature.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df_feature = df_feature.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n    df_feature = df_feature.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n    df_feature.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n    \n    return df_feature\n\n# Calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","793ffc4f":"# CatBoost parameters\nctb_reg_params = {\n    'learning_rate':     hp.choice('learning_rate',     np.arange(0.05, 1, 0.05)),\n    'max_depth':         hp.choice('max_depth',         np.arange(5, 16, 1, dtype=int)),\n    'colsample_bylevel': hp.choice('colsample_bylevel', np.arange(0.3, 0.8, 0.1)),\n    'n_estimators':      hp.choice('n_estimators', np.arange(100, 2000, 100)),\n    'eval_metric':       hp.choice('eval_metric', ['RMSE', 'MAPE', 'MAE']),\n    'od_wait': 5 # overfitting detector\n}\nctb_fit_params = {\n    'early_stopping_rounds': 10,\n    \n    'verbose': True\n}\nctb_para = dict()\nctb_para['reg_params'] = ctb_reg_params\nctb_para['fit_params'] = ctb_fit_params\nctb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))","92fc46ec":"from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\nimport catboost as ctb\nfrom sklearn.metrics import mean_squared_error\nclass HPOpt(object):\n\n    def __init__(self, x_train, x_test, y_train, y_test):\n        self.x_train = x_train\n        self.x_test  = x_test\n        self.y_train = y_train\n        self.y_test  = y_test\n\n    def process(self, fn_name, space, trials, algo, max_evals):\n        fn = getattr(self, fn_name)\n        try:\n            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n        except Exception as e:\n            return {'status': STATUS_FAIL,\n                    'exception': str(e)}\n        return result, trials\n\n\n    def ctb_reg(self, para):\n        reg = ctb.CatBoostRegressor(**para['reg_params'])\n        return self.train_reg(reg, para)\n\n    def train_reg(self, reg, para):\n        reg.fit(self.x_train, self.y_train,\n                eval_set=[(self.x_train, self.y_train), (self.x_test, self.y_test)], \n                **para['fit_params'])\n        pred = reg.predict(self.x_test)\n        loss = para['loss_func'](self.y_test, pred)\n        return {'loss': loss, 'status': STATUS_OK, 'trained_model': reg}","05f80ba9":"def get_best_model(trials):\n    valid_trial_list = [trial for trial in trials\n                            if STATUS_OK == trial['result']['status']]\n    losses = [ float(trial['result']['loss']) for trial in valid_trial_list]\n    index_having_minumum_loss = np.argmin(losses)\n    best_trial_obj = valid_trial_list[index_having_minumum_loss]\n    best_model = best_trial_obj['result']['trained_model']\n    return best_model","803d5c36":"def train_and_evaluate(train, test, best_model):\n\n    # Split features and target\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    \n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    \n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(x_test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n    # Iterate through each fold\n    flag = 1\n    model = best_model\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = Pool(x_train,\n                     y_train,\n                     weight=train_weights)\n        val_dataset = Pool(x_val,\n                     y_val,\n                     weight=val_weights)\n\n        if flag == 1:\n            model.fit(train_dataset, eval_set = val_dataset)\n            flag=2\n        else:\n            model.fit(train_dataset,\n                      eval_set = val_dataset,\n                      init_model='model.cbm')\n        # Incremental learning is not needed here, but I leave it for educational purposes\n        model.save_model('model.cbm') \n\n        oof_predictions[val_ind] = model.predict(Pool(x_val))\n        # Predict the test set\n        test_predictions += model.predict(Pool(x_test)) \/ 5\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    # Return test predictions\n    return test_predictions","937189a0":"# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock_id (as prediction by stock_id)\ntrain_stock_ids = train['stock_id'].unique()\n\n# Generate features\ntrain_feature = preprocess(train_stock_ids, is_train=True)\n# Merge with intiail train data\ntrain = train.merge(train_feature, on=['row_id'], how='left')\n\n# Same for test datas\ntest_stock_ids = test['stock_id'].unique()\ntest_feature = preprocess(test_stock_ids, is_train=False)\ntest = test.merge(test_feature, on=['row_id'], how='left')\n\n# Further generate features with realized-volatility\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","a4f76b43":"from sklearn.model_selection import train_test_split\n\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\n\nx['stock_id'] = x['stock_id'].astype(int)\n\nx_train, x_val, y_train, y_val =train_test_split( x, y, test_size=0.8, random_state=42)","9f1f014d":"# Find best model\nobj = HPOpt(x_train, x_val, y_train, y_val)\nn_epochs_to_evaluate = 50\nctb_obj = obj.process(fn_name='ctb_reg', space=ctb_para, trials=Trials(), \n                      algo=tpe.suggest, max_evals=n_epochs_to_evaluate)\nbest_model = get_best_model(ctb_obj[1])\nbest_model.save_model('best_cv_model.cbm')","83899470":"# Traing and evaluate\ntest_predictions = train_and_evaluate(train, test, best_model)\n\n# Save test predictions\ntest['target'] = test_predictions\ntest[['row_id', 'target']].to_csv('submission.csv', index=False)","3c377548":"# Finding best CatBoost model using hyperopt","f53a40d1":"# Import\n","34f91a99":"# Funcs","3c8b480b":"# Main","0a63d26c":"This work is build on the basis of https:\/\/www.kaggle.com\/austinzhao\/reproduction-explanation-lgbm-baseline. I used catboost model instead of  LGBM and tuned its hyperparameters with hyperopt."}}