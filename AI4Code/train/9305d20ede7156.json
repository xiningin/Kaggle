{"cell_type":{"27f5f9a9":"code","1637f7bf":"code","3e5f4ee2":"code","15308ecb":"code","6bdf8d0f":"code","2f83b1a6":"code","af8919a2":"code","9a4405b3":"code","80483fe0":"code","a4197630":"code","2eee8609":"code","b653b7be":"code","1f18a694":"code","7354f56f":"code","dc39d715":"code","c24d6655":"code","6c37e07d":"code","dd402e66":"code","e4f27739":"code","8fa5b01e":"code","243075ba":"code","b025e644":"code","a8f8f4ff":"code","87db2f08":"code","0d38d94d":"code","4c7d1bc3":"code","02c9b8a4":"code","fc1231a3":"code","f62ed7d4":"markdown","fe0c79b6":"markdown","6e58cfd0":"markdown","256facbf":"markdown"},"source":{"27f5f9a9":"from google.cloud import bigquery","1637f7bf":"client = bigquery.Client()\n\ndataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\ndataset = client.get_dataset(dataset_ref)\n\ntables = list(client.list_tables(dataset))\nfor table in tables:\n    print (table.table_id)\n\ntable_ref = dataset_ref.table(\"global_air_quality\")\ntable = client.get_table(table_ref)","3e5f4ee2":"table.schema","15308ecb":"df = client.list_rows(table, max_results=5).to_dataframe()\ndf","6bdf8d0f":"# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\nQuery = \"\"\" \n        SELECT city\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = \"US\"\n        \"\"\"","2f83b1a6":"#setup the query\nquery_job = client.query(Query)","af8919a2":"# API request - run the query, and return a pandas DataFrame\nus_cities = query_job.to_dataframe()","9a4405b3":"us_cities.city.value_counts().head()","80483fe0":"# Query to get the score column from every row where the type column has value \"job\"\nQuery = \"\"\" \n        SELECT score, title\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE type=\"job\"\n        \"\"\"\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(Query, job_config=dry_run_config )\n\nprint(\"This Query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))\n","a4197630":"# Only run the query if it's less than 100 MB\nONE_HUNDRED_MB = 100*1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_HUNDRED_MB)\n\n# Set up the query (will only run if it's less than 100 MB)\nsafe_run_job = client.query(Query, job_config = safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\nsafe_run_job.to_dataframe()\n","2eee8609":"# Only run the query if it's less than 1 GB\nONE_GB = 1000*1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)\n\n# Set up the query (will only run if it's less than 100 MB)\nsafe_run_job = client.query(Query, job_config = safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\njob_post_scores = safe_run_job.to_dataframe()\n\n# Print average score for job posts\njob_post_scores.score.mean()","b653b7be":"# Query to select comments that received more than 10 replies\nquery_popular = \"\"\"\n                SELECT parent, COUNT(id)\n                FROM `bigquery-public-data.hacker_news.comments`\n                GROUP BY parent\n                HAVING COUNT(id) > 10\n                \"\"\"","1f18a694":"# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_popular, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\npopular_comments.head()\n\n","7354f56f":"# Improved version of earlier query, now with aliasing & improved readability\nquery_improved = \"\"\"\n                SELECT parent, COUNT(id) AS NumPosts\n                FROM `bigquery-public-data.hacker_news.comments`\n                GROUP BY parent\n                HAVING COUNT(id) > 10\n                \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_improved, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\npopular_comments.head()","dc39d715":"## working with Dates\nclient = bigquery.Client()\n# Construct a reference to the \"nhtsa_traffic_fatalities\" dataset\ndataset_ref = client.dataset(\"nhtsa_traffic_fatalities\", project=\"bigquery-public-data\")\n#API request\ndataset = client.get_dataset(dataset_ref)\n\ntables = client.list_tables(dataset)\n\nfor t in tables:\n    print (t.table_id)","c24d6655":"table_ref = dataset_ref.table(\"accident_2015\")\ntable = client.get_table(table_ref)","6c37e07d":"table.schema","dd402e66":"client.list_rows(table, max_results=5).to_dataframe()","e4f27739":"# Query to find out the number of accidents for each day of the week\nQuery = \"\"\"\n        SELECT COUNT(consecutive_number) AS num_accidents,\n               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n        GROUP BY day_of_week\n        ORDER BY num_accidents desc\n       \"\"\"","8fa5b01e":"# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 1 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9)\nquery_job = client.query(Query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\naccidents_by_day = query_job.to_dataframe()\naccidents_by_day","243075ba":"client = bigquery.Client()\n\ndataset_ref = client.dataset(\"crypto_bitcoin\", project=\"bigquery-public-data\")\ndataset = client.get_dataset(dataset_ref)\n\ntables = list(client.list_tables(dataset))\nfor table in tables:\n    print (table.table_id)\n\ntable_ref = dataset_ref.table(\"transactions\")\ntable = client.get_table(table_ref)","b025e644":"client.list_rows(table, max_results=5).to_dataframe()","a8f8f4ff":"# Query to select the number of transactions per date, sorted by date\nquery_with_CTE = \"\"\" \n                 WITH time AS\n                 (\n                     SELECT DATE(block_timestamp) as trans_date\n                     FROM `bigquery-public-data.crypto_bitcoin.transactions`\n                 )\n                 SELECT count(1) AS transactions, trans_date\n                 FROM time\n                 GROUP BY trans_date\n                 ORDER BY trans_date\n                 \n                \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_with_CTE, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\ntransactions_by_date = query_job.to_dataframe()\n\n# Print the first five rows\ntransactions_by_date.head()","87db2f08":"transactions_by_date.set_index('trans_date').plot()","0d38d94d":"client = bigquery.Client()\ndataset_ref = client.dataset(\"github_repos\", project=\"bigquery-public-data\")\ndataset = client.get_dataset(dataset_ref)\n\n#tables = list(client.list_tables(dataset))\n#for t in tables:\n#    print(t.table_id)\n\ntable_ref = dataset_ref.table('licenses')\ntable = client.get_table(table_ref)\n\nclient.list_rows(table, max_results=5).to_dataframe()","4c7d1bc3":"table_ref2 = dataset_ref.table('sample_files')\ntable2 = client.get_table(table_ref2)\nclient.list_rows(table2, max_results=5).to_dataframe()","02c9b8a4":"# Query to determine the number of files per license, sorted by number of files\nQuery = \"\"\"\n        SELECT L.license, count(1) as num_of_files\n        FROM `bigquery-public-data.github_repos.sample_files` as SF\n        INNER JOIN `bigquery-public-data.github_repos.licenses` as L\n               ON L.repo_name = SF.repo_name\n        GROUP BY L.license\n        ORDER BY num_of_files\n        \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(Query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nfile_count_by_license = query_job.to_dataframe()","fc1231a3":"file_count_by_license","f62ed7d4":"### Working with big datasets\n\nyou can estimate the size of any query before running it. Here is an example using the (very large!) Hacker News dataset. To see how much data a query will scan, we create a QueryJobConfig object and set the dry_run parameter to True.","fe0c79b6":"## Joins","6e58cfd0":"You can also specify a parameter when running the query to limit how much data you are willing to scan. Here's an example with a low limit.","256facbf":"### Group By and Having"}}