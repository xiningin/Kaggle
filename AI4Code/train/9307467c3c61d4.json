{"cell_type":{"17137978":"code","15914bbf":"code","d2d38c09":"code","1dd2b346":"code","f9ceb3de":"code","eb519d04":"code","7003ce40":"code","df735cb6":"code","5a1a0318":"code","95e18809":"code","0e751688":"code","6f0891ed":"code","c7503e2c":"code","dfc0c936":"code","aec15382":"code","32c993b8":"code","425874bb":"code","ebdac153":"code","1e9e4f4d":"code","d5b42e95":"code","0b42b99c":"code","cdc8b788":"code","9dc95795":"code","7a42d7b3":"code","5759289a":"code","31691640":"code","fcae0f16":"code","4f84989c":"code","282909f9":"code","570c0b18":"code","c7396892":"code","fcc930d2":"code","7942af9d":"code","a43ff675":"code","71def2c4":"code","81d07ae6":"code","85e89368":"code","c7e0d9ed":"code","2919f987":"code","78934097":"code","0e9529fe":"code","0a5663ba":"code","327ac3b6":"code","caee83cd":"code","96d681e0":"code","7db8191e":"code","04ac673e":"code","afb4c1f3":"code","3ac239a2":"code","867134ae":"markdown","9f4f90b5":"markdown","5689ba63":"markdown","072669e7":"markdown","ef0ba7a6":"markdown","386fc2a0":"markdown","26b81173":"markdown","dd5ae2da":"markdown","b298667b":"markdown","03a7d2d1":"markdown","b2b3f285":"markdown","23b55411":"markdown","739b8d1d":"markdown","1eaa576c":"markdown","f9ffb411":"markdown","1817231f":"markdown"},"source":{"17137978":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n#from IPython.core.interactiveshell import InteractiveShell\n#InteractiveShell.ast_node_interactivity = \"all\"\nimport math\nimport json\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NearestNeighbors\n#import joblib\nimport scipy.sparse\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.linalg import svds\nimport warnings; warnings.simplefilter('ignore')\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","15914bbf":"#electronics_data=pd.read_csv('ratings_Electronics (1).csv',names=['userId', 'productId','Rating','timestamp'])","d2d38c09":"electronics_data=pd.read_csv(\"\/kaggle\/input\/amazon-product-reviews\/ratings_Electronics (1).csv\",names=['userId', 'productId','Rating','timestamp'])","1dd2b346":"## Display the data\nelectronics_data.head()","f9ceb3de":"##Dropping the Timestamp column\nelectronics_data = electronics_data.drop(['timestamp'], axis=1)\n\n##Shape of the data\nelectronics_data.shape","eb519d04":"electronics_data.info()","7003ce40":"#Five point summary \nelectronics_data.describe()['Rating'].T","df735cb6":"#Find the minimum and maximum ratings\nprint('Minimum rating is: %d' %(electronics_data.Rating.min()))\nprint('Maximum rating is: %d' %(electronics_data.Rating.max()))","5a1a0318":"#Check for missing values\nprint('Number of missing values across columns: \\n',electronics_data.isnull().sum())","95e18809":"plt.figure(figsize=(10,5))\nratings = electronics_data['Rating'].value_counts()\nsns.barplot(x=ratings.index, y=ratings.values)\nplt.show()","0e751688":"print(\"Total data \")\nprint(\"-\"*50)\nprint(\"\\nTotal no of ratings :\",electronics_data.shape[0])\nprint(\"Total No of Users   :\", len(np.unique(electronics_data.userId)))\nprint(\"Total No of products  :\", len(np.unique(electronics_data.productId)))","6f0891ed":"# Analysis of number of ratings given by the user \n\nno_of_rated_products_per_user = electronics_data.groupby(by='userId')['Rating'].count().sort_values(ascending=False)\nno_of_rated_products_per_user.head(25)","c7503e2c":"no_of_rated_products_per_user.describe()","dfc0c936":"quantiles = no_of_rated_products_per_user.quantile(np.arange(0, 1.01, 0.01), interpolation='higher')","aec15382":"plt.figure(figsize=(10,5))\nplt.title(\"Quantiles and their Values\")\nquantiles.plot()\n# quantiles with 0.05 difference\nplt.scatter(x=quantiles.index[::5], y=quantiles.values[::5], c='orange', label=\"quantiles with 0.05 intervals\")\n# quantiles with 0.25 difference\nplt.scatter(x=quantiles.index[::25], y=quantiles.values[::25], c='m', label = \"quantiles with 0.25 intervals\")\nplt.ylabel('No of ratings by user')\nplt.xlabel('Value at the quantile')\nplt.legend(loc='best')\nplt.show()","32c993b8":"print('\\n No of rated product more than 50 per user : {}\\n'.format(sum(no_of_rated_products_per_user >= 50)) )","425874bb":"# Getting the new dataframe after subsetting and filering for users who has given 500 or more ratings. \n# This was mainly for reducing the data size.\n\nelec_data=electronics_data.iloc[:1048576,0:]\nelec_data = elec_data.groupby(\"productId\").filter(lambda x:x['Rating'].count() >=500)","ebdac153":"elec_data.head()","1e9e4f4d":"elec_data.shape","d5b42e95":"product_rating_pivot = elec_data.pivot(index = 'productId', columns = 'userId', values = 'Rating').fillna(0)\n#product_rating_matrix = csr_matrix(product_rating_pivot.values)\nproduct_rating_pivot.shape","0b42b99c":"product_rating_pivot.head()","cdc8b788":"from sklearn.neighbors import NearestNeighbors\n\nmodel_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\nmodel_knn.fit(product_rating_pivot)","9dc95795":"distances, indices = model_knn.kneighbors(product_rating_pivot, n_neighbors = 6)\nprint('distances: ', distances.shape, 'indices: ', indices.shape)","7a42d7b3":"rec = pd.DataFrame(indices, columns=['product0','product1','product2','product3','product4','product5'])\nrec.head()","5759289a":"products = pd.DataFrame(product_rating_pivot.index).reset_index()\nproducts = products.rename(columns={'index':'product0'})\nproducts.head()","31691640":"rec2 = rec.copy()\n\nfor i in range(0, 6):\n    products = pd.DataFrame(product_rating_pivot.index).reset_index()\n    products = products.rename(columns={'index':f'product{i}'})\n    \n    rec2 = pd.merge(rec2, products, on=[f'product{i}'], how='left')\n    rec2 = rec2.drop(f'product{i}', axis=1)\n    rec2 = rec2.rename(columns={'productId':f'product{i}'})\nprint(rec2.head())    ","fcae0f16":"## similar proudcts to B00001WRSJ\n\nrec2[rec2['product0']==\"B00001WRSJ\"]","4f84989c":"rec2[rec2['product0']==\"B00004SABB\"]","282909f9":"product_rating_pivot.shape","570c0b18":"X = product_rating_pivot.values","c7396892":"import matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.decomposition import TruncatedSVD","fcc930d2":"## check for the number of components required. It is taken as the number components required to \n## cover 95% of variation in the data.\n\nsvd = TruncatedSVD(n_components=2145, random_state=42)\nsvd.fit(X)\n\ncount=0\nfor index, cumsum in enumerate(np.cumsum(svd.explained_variance_ratio_)):\n    if cumsum<=0.95:\n        count+=1\n    else:\n        break\n        \nprint(count)","7942af9d":"SVD = TruncatedSVD(n_components=count, random_state=17)\nSVD.fit(X)\n\nplt.plot(np.cumsum(SVD.explained_variance_ratio_))\nplt.show()","a43ff675":"plt.plot(SVD.explained_variance_ratio_)\nplt.show()","71def2c4":"SVD.explained_variance_ratio_[:25]","81d07ae6":"import sklearn\nfrom sklearn.decomposition import TruncatedSVD\n\nSVD = TruncatedSVD(n_components=count, random_state=17)\nmatrix = SVD.fit_transform(X)\nmatrix.shape","85e89368":"corr = np.corrcoef(matrix)\ncorr.shape","c7e0d9ed":"## Identify similar products of B00001WRSJ\n\nproduct_title = product_rating_pivot.index\nproduct_list = list(product_title)\nproduct_B00001WRSJ = product_list.index(\"B00001WRSJ\")\nprint(product_B00001WRSJ)","2919f987":"product_title","78934097":"corr_product_B00001WRSJ  = corr[product_B00001WRSJ]","0e9529fe":"corr_product_B00001WRSJ[0:20]","0a5663ba":"list(product_title[(corr_product_B00001WRSJ>0.005)])","327ac3b6":"from sklearn.metrics.pairwise import cosine_similarity,linear_kernel\nfrom surprise import Reader, SVD, Dataset\nfrom surprise.model_selection import cross_validate","caee83cd":"reader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(elec_data[['userId', 'productId', 'Rating']], reader)","96d681e0":"## let's evaluate the quality of solution by using cross-validation\n\nsvd = SVD()\ncross_validate(svd, data, measures=['rmse', 'mae'], cv=3, return_train_measures=True)","7db8191e":"## Train the model on the entire dataset using the fit method after converting the dataset for \n# cross-validation into a Surprise Trainset object using the build_full_trainset method.\n\ntrainset = data.build_full_trainset()\nsvd.fit(trainset)","04ac673e":"## get recommendations for user 'AGVWTYW0ULXHT'\n\nrr = elec_data[elec_data['userId']=='AGVWTYW0ULXHT'].sort_values(by='Rating', ascending=False)\nrr","afb4c1f3":"items = elec_data['productId'].unique()\ntest = [['AGVWTYW0ULXHT', iid, 4] for iid in items]\npredictions = svd.test(test)\npred = pd.DataFrame(predictions)","3ac239a2":"pred = pred.sort_values(by='est', ascending=False)\npred.head(10)","867134ae":"## Types of recommendations\n\nThere are majorly six types of recommender systems which work primarily in the Media and Entertainment industry: \n- Collaborative Recommender system, \n- Content-based recommender system, \n- Demographic based recommender system, \n- Utility based recommender system, \n- Knowledge based recommender system\n- Hybrid recommender system.","9f4f90b5":"## Attribute Information:\n\n\u25cf userId : Every user identified with a unique id \n\n\u25cf productId : Every product identified with a unique id \n\n\u25cf Rating : Rating of the corresponding product by the corresponding user \n\n\u25cf timestamp : Time of the rating ( ignore this column for this exercise)\n\n","5689ba63":"## Introduction to Recommendation systems\n\nA recommendation engine is a type of data filtering tool using machine learning algorithms to recommend the most relevant items to a particular user or customer. It operates on the principle of finding patterns in consumer behavior data, which can be collected implicitly or explicitly.\n\nNetflix uses a recommendation engine to present viewers with movie and show suggestions. Amazon, on the other hand, uses a recommendation engine to present customers with product recommendations. While each uses one for slightly different purposes, both have the same goal: to drive sales, boost engagement and retention, and deliver more personalized customer experiences.\n\nIn the past, recommendations would come from a salesperson or friends and family. Today, we have passed this task in the hands, or minds, of algorithms. As a marketing tool, you could say that these machines are well-trained in the art of up-selling and cross-selling.","072669e7":"### Collaborative Filtering - Surprise\n\nIn Collaborative Filtering, we try to find similar users and recommend what similar users like. In this type of recommendation system, we don\u2019t use the features of the item to recommend it, rather we classify the users into the clusters of similar types, and recommend each user according to the preference of its cluster.","ef0ba7a6":"## Load the Dataset and Add headers","386fc2a0":"The rating of the product range from 1 to 5","26b81173":"### kNN - Nearest products in terms of ratings\nHere our approach is to find the similar products by using the ratings provided by users. Not a great idea because similar rating doesn't mean products are similar!!\n\nAlso the raw data is sparse with most cells zero. Hence, the distance calculation may not provide reliable values.\n\nMost importantly, this approach will not provide **user specific recommendations**.","dd5ae2da":"## Analyzing the rating","b298667b":"## Import Libraries ","03a7d2d1":"Let's take look at the preference of a user","b2b3f285":"Most of the people has given the rating of 5","23b55411":"## Ratings","739b8d1d":"## Unique Users and products\n","1eaa576c":"## Checking for Missing values\n","f9ffb411":"### SVD - similarity of products via users\n\nSVD is an improvement over the previous approach as users are converted to components based of their similarity. Then these components are used for evaluating the similarity of products. \n\nAgain, this approach also will not provide user specific recommendations. ","1817231f":"import gc\ngc.collect()"}}