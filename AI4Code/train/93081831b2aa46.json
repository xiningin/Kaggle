{"cell_type":{"b9f49264":"code","96fea4fc":"code","3fe0b892":"code","b899f238":"code","f4ae70ac":"code","c0f5f559":"code","1e02e60d":"code","9e8233f6":"code","31182f03":"code","f814dd67":"code","46d8c07a":"code","5a3af7f6":"code","b4a788d1":"code","b1455a57":"code","9cf3ff02":"code","7b9e3526":"code","fa6d2602":"code","f9e898df":"code","ffa6ecf3":"code","448d8a5b":"code","5150e2ca":"code","ba0303ab":"code","1a2cc28f":"code","38c0ca95":"code","b365d59c":"code","4359053a":"code","4fb1aa12":"code","1f222927":"code","7f533e16":"code","98a78ba8":"code","7d63a671":"code","94fe0082":"code","c8f064ea":"code","1c7a15e4":"code","55d2d987":"code","5cf387ff":"code","5476057b":"code","a413ffe4":"code","3374785d":"code","ce68283f":"code","7c0911c6":"code","33180286":"code","ccab9419":"code","0702113e":"code","c3c309d8":"code","25719c37":"code","7e5ddfff":"code","b1e60e7d":"code","acc2f90b":"code","cd421ff2":"code","4c73c3d0":"code","728d153b":"code","9b6f46e5":"code","407fa8be":"code","b13a1ae0":"code","31648baa":"code","62dc42b1":"code","331f7e64":"code","0b0fc8d0":"markdown","c0507e4c":"markdown","11ad65e6":"markdown","db35489c":"markdown","3262b194":"markdown","044beb7e":"markdown","5b9e5f62":"markdown","178afffc":"markdown","f0f57532":"markdown","743ba7c3":"markdown","75f89c8f":"markdown","56632b83":"markdown","eef22d55":"markdown","f9a64fbd":"markdown","50288cbc":"markdown","f5e05bf4":"markdown","1b50febc":"markdown","7881580a":"markdown","e58c0a0e":"markdown","ef512a46":"markdown","bb75ca6d":"markdown"},"source":{"b9f49264":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96fea4fc":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","3fe0b892":"train = pd.read_excel('\/kaggle\/input\/doctor-fees-prediction\/Final_Train.xlsx')\ntest = pd.read_excel('\/kaggle\/input\/doctor-fees-prediction\/Final_Test.xlsx')","b899f238":"pd.set_option('display.max_columns',None)","f4ae70ac":"train.head()","c0f5f559":"# check for null values\nround(train.isnull().sum()\/len(train) * 100,2)","1e02e60d":"# Experience\ntrain['Experience'] = train['Experience'].str.split().str[0]\ntrain['Experience'] = train['Experience'].astype(int)","9e8233f6":"train.head(2)","31182f03":"train.Place.fillna('Unknown,Unknown',inplace=True)\n\ntrain['locality'] = train['Place'].str.split(\",\").str[0]\ntrain['city'] = train['Place'].str.split(\",\").str[1]\n\n#Now we can drop the place variable\ntrain.drop('Place',axis=1,inplace=True)","f814dd67":"# Fill missing values with -99% to provide them different importance.\ntrain['Rating'].fillna('-99%',inplace=True)\n\n# extract value of rating to avoid '%' and convert to numeric\ntrain['Rating'] = train['Rating'].str.slice(stop=-1).astype(int)","46d8c07a":"print(\"0-9% \",len(train[(train['Rating']>0) & (train['Rating']<10)]))\nprint(\"10-19% \",len(train[(train['Rating']>=10) & (train['Rating']<20)]))\nprint(\"20-29% \",len(train[(train['Rating']>=20) & (train['Rating']<30)]))\nprint(\"30-39% \",len(train[(train['Rating']>=30) & (train['Rating']<40)]))\nprint(\"And so-on..\")","5a3af7f6":"bins = [-99,0,10,20,30,40,50,60,70,80,90,100]\nlabels = [i for i in range(11)]\ntrain['Rating'] = pd.cut(train['Rating'], bins=bins, labels=labels, include_lowest=True)","b4a788d1":"train['Rating'].value_counts().sort_index()\n# nice it is exactly being grouped.","b1455a57":"train['Qualification_count'] = train['Qualification'].apply(lambda x: len(x.split(\",\")))\n\ntrain['Qualification_count'].value_counts()","9cf3ff02":"train['Qualification'].unique()","7b9e3526":"# Extract relevant qualification\n# DICT of qualification with there counts\ntrain[\"Qualification\"]=train[\"Qualification\"].str.split(\",\")\nQualification ={}\nfor x in train[\"Qualification\"].values:\n    for each in x:\n        each = each.strip()\n        if each in Qualification:\n            Qualification[each]+=1\n        else:\n            Qualification[each]=1\n            \n#print(Qualification)","fa6d2602":"#Extract top 10 qual\nmost_qual = sorted(Qualification.items(),key=lambda x:x[1],reverse=True)[:10]\nfinal_qual = []\nfor qual in most_qual:\n    final_qual.append(qual[0])\n    \nprint(final_qual)","f9e898df":"#Encode extracted top 10 qualification into train dataset as new column.\nfor qual in final_qual:\n    train[qual] = 0\n\nfor x,y in zip(train['Qualification'].values, np.array([i for i in range(len(train))])):\n    for c in x:\n        c = c.strip()\n        if c in final_qual:\n            train[c][y] = 1","ffa6ecf3":"#train['Qualification'].values","448d8a5b":"# Now we can drop the extra quali which we added \ntrain.drop(['Qualification','Qualification_count'],axis=1,inplace=True)","5150e2ca":"train.head(4)","ba0303ab":"# CITY COLUMN\ntrain['city'].unique()","1a2cc28f":"# remove the extra spaces before the city\nimport re\ntrain['city'] = train['city'].apply(lambda x: re.sub(' +','',str(x)))","38c0ca95":"train['city'].value_counts()","b365d59c":"# city contains some improper data.\ntrain[(train['city'] == \"nan\") | (train['city'] == \"Sector5\")]","4359053a":"#train[train['locality'] == 'Dwarka']  ","4fb1aa12":"train['city'] = np.where(train['city']==\"Sector5\",\"Delhi\", train['city'])\n\n#nan\ntrain['city'].loc[3980] = \"Unknown\"\ntrain['locality'].loc[3980] = \"Unknown\"","1f222927":"train.head()","7f533e16":"# let's see for each city how much locality data is there\ntrain.groupby('city')['locality'].nunique()","98a78ba8":"train.to_csv('data_correct.csv',index=False)","7d63a671":"train.head(3)","94fe0082":"plt.figure(figsize=(8,8))\nsns.barplot(x='Profile',y='Fees',data=train)\nplt.title(\"Doctor Fees wrt Profile\")\nplt.xticks(rotation=45)\nplt.show()","c8f064ea":"plt.figure(figsize=(10,8))\nsns.barplot(x='city',y='Fees',data=train)\nplt.title(\"Doctor Fees rate in different city\")\nplt.xticks(rotation=45)\nplt.show()","1c7a15e4":"train = pd.get_dummies(train,columns=['city','Profile'], prefix=['city','Profile'])","55d2d987":"train.head()","5cf387ff":"train.drop(['Miscellaneous_Info','locality'],axis=1,inplace=True)","5476057b":"plt.figure(figsize=(10,8))\nax = sns.lineplot(x='Experience',y='Fees',data=train)\nax.set_title(\"Experience wrt doctor Fees\")\nplt.show()","a413ffe4":"plt.figure(figsize=(8,8))\nsns.barplot(x='Rating',y='Fees',data=train,palette='magma')\nplt.title(\"Rating Vs Doctor_Fees\")\nplt.show()","3374785d":"plt.figure(figsize=(8,8))\nsns.barplot(x='Rating',y='Experience',data=train,palette='magma')\nplt.show()","ce68283f":"train.head()","7c0911c6":"# EXPERIENCE COLUMN\ntest['Experience'] = test['Experience'].str.split().str[0]\ntest['Experience'] = test['Experience'].astype(int)\n\n#Place column (Extract city and locality in diff column)\ntest['Place'].fillna('Unknown,Unknown',inplace=True)\ntest['locality'] = test['Place'].str.split(\",\").str[0]\ntest['city'] = test['Place'].str.split(\",\").str[1]\n#drop place column\ntest.drop('Place',axis=1,inplace=True)\n\n#Rating column\ntest['Rating'].fillna('-99%',inplace=True)\ntest['Rating'] = test['Rating'].str.slice(stop=-1).astype(int)\n# group rating in 10 groups, missing_value to group 0\nbins = [-99,0,10,20,30,40,50,60,70,80,90,100]\nlabels = [int(i) for i in range(11)]\ntest['Rating'] = pd.cut(test['Rating'], bins=bins, labels=labels, include_lowest=True)\n","33180286":"# QUALIFICATION COLUMN\n# dict of qualification with counts\n# HERE the top 10 Qual of train and test col should be same so I am imputing the train col to test col\ndata = pd.read_excel('\/kaggle\/input\/doctor-fees-prediction\/Final_Train.xlsx')\ndata['Qualification'] = data['Qualification'].str.split(\",\")\nQualification = {}\nfor x in data['Qualification']:\n    for each in x:\n        each = each.strip()\n        if each in Qualification:\n            Qualification[each] += 1\n        else:\n            Qualification[each] = 1\n            \n# finding out the top 10 Qualification\nmost_qual = sorted(Qualification.items(), key=lambda x:x[1], reverse=True)[:10]\nfinal_qual = []\nfor qual in most_qual:\n    final_qual.append(qual[0])\n\n#encode the top 10 qualification in test dataset\nfor title in final_qual:\n    test[title] = 0\n\nfor x,y in zip(test['Qualification'].values, np.array([i for i in range(len(test))])):\n    for c in x:\n        c = c.strip()\n        if c in final_qual:\n            test[c][y] = 1\n            \n#drop Qualification after getting top 10 qual.\ntest.drop('Qualification',axis=1,inplace=True)\n\n\n#city column\ntest['city'] = test['city'].apply(lambda x: re.sub(' +','',x))\n\n#encode city and profile\ntest = pd.get_dummies(test, columns=['city','Profile'], prefix=['city','Profile'])\n\n#drop Miscellaneous_Info and locality\ntest.drop(['Miscellaneous_Info','locality'], axis=1, inplace=True)","ccab9419":"x = train.drop('Fees',axis=1)\ny = train['Fees']","0702113e":"from sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\nX = stdsc.fit_transform(x)","c3c309d8":"# train-test split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)","25719c37":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# to use RMSLE we will create our own scorer\nfrom sklearn.metrics import make_scorer","7e5ddfff":"# calculate RMSLE.\ndef score(y_pred,y):\n    y_pred = np.log(y_pred)\n    y = np.log(y)\n    return 1 - ((np.sum((y_pred-y)**2))\/len(y))**1\/2    # 1-RMSLE\n\n# make our own scorer\nscorer = make_scorer(score,greater_is_better=True, needs_proba=False)","b1e60e7d":"knn_reg = KNeighborsRegressor()\nsvm_reg = SVR(gamma='scale')\ndt_reg = DecisionTreeRegressor()\nrf_reg = RandomForestRegressor()","acc2f90b":"# Training and testing\nfor reg in (knn_reg, svm_reg, dt_reg, rf_reg):\n    reg.fit(x_train, y_train)\n    \n    y_pred = reg.predict(x_test)\n    \n    print(reg, score(y_pred,y_test))","cd421ff2":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","4c73c3d0":"grid = dict(C=[0.1,1,10], kernel=['linear','poly','rbf'], gamma=['scale','auto'])\n\n#gridsearch = GridSearchCV(svm_reg, param_grid=parameters, cv=5, scoring=scorer verbose=1, n_jobs=-1)\nsvm_random = RandomizedSearchCV(svm_reg, param_distributions= grid, scoring=scorer, cv=5, random_state=42, n_iter=100, verbose=1, n_jobs=-1)","728d153b":"svm_random.fit(x_train,y_train)","9b6f46e5":"print(\"best_score:\",svm_random.best_score_)\nprint(\"best_params:\\n\",svm_random.best_params_)","407fa8be":"# predict the test data from it and see the result\npredictions = svm_random.predict(x_test)\n\nprint(\"1-RMSLE:\",score(predictions, y_test))","b13a1ae0":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor()","31648baa":"# try hyperparameter tuning\n\n#no. of tress random_forest\nn_estimators = [int(x) for x in np.linspace(start=150,stop=1000,num=6)]\n#max levels in tree\nmax_depth = [int(x) for x in np.linspace(start=6,stop=30,num=5)]\n#min no. of splitting required to split a node\nmin_samples_split = [2,7,10]\n#min no. of sample required at each leaf node\nmin_samples_leaf = [2,5]\n#max_features\nmax_features=['sqrt','auto']\n\n# generate a dictionary of all the Hyper Parameters\nrandom_grid = {'n_estimators':n_estimators,\n               'max_depth':max_depth,\n               'min_samples_split':min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'max_features':max_features,\n              }\n\nprint(random_grid)","62dc42b1":"rf_random = RandomizedSearchCV(rf_reg, param_distributions=random_grid, scoring=scorer, cv=5, n_iter=100, random_state=42, verbose=1, n_jobs=-1)\n\nrf_random.fit(x_train,y_train)","331f7e64":"print(\"best_params:\\n\",rf_random.best_params_)\nprint(\"\\nbest_score:\",rf_random.best_score_)","0b0fc8d0":"### Description About Data**\n**In the data, we have the following columns to work with :**\n1. Qualification: Qualification and degrees held by the doctor\n2. Experience: Experience of the doctor (number of years)\n3. Rating: Rating given by patients\n4. Profile: Type of the doctor\n5. Miscellaeous_Info: Extra information about the doctor\n6. Place: Area and the city where the doctor is located\n7. Fees: Fees charged by the doctor(Target)","c0507e4c":"**OBSERVATIONS**\n* the Rating is interesting column, we have grouped the rating in 10 bins, eg: bin5 will be rating of 40-49%.\n* As, we can see that High rating does not relate to high fees charged(infact low fees can be reason of high rating) and where fees are charged so high, there rating is between 30-60%.\n* The median of experience in bin 4 & 5 is 27 and 31 years respectively. whereas in highest rating of 10th bin the median experience is 14 years. ","11ad65e6":"## Model Building\n**Based on the hackathon site(MachineHack), submission are evaluated on Root-Mean-Squared-Log-Error (RMSLE) error, more specifically, 1-RMSLE. so, I will use metrics as RMSLE**","db35489c":"**Importing Dataset**\n* Since Data is in excel format so we will use pd.read_excel func to read data.\n* After loading the data, it is important to have basic understanding about the data like shape, info and basic understanding of presence of null values in data.","3262b194":"## EDA\n**There will be lot's of analysis which can be assumed by observing our data as all the feature we are having is categorical even experience, except target variable(Fees) so let's begin..**","044beb7e":"**2)Moving with Place Column**\n* As, there are some missing values in Place column so I have replaced then with Unknown, Unknown to represent them.\n* then, we can easily extract city and locality from Place variable.","5b9e5f62":"**Miscellaneous_Info & locality**\n* Miscellaneous_Info is to much messy column and it consist the data which we already have like doctor address(city & locality) and Rating\n* locality and city are approximate have same relationship so better to move with only city\n* so drop the Miscellaneous_Info and locality","178afffc":"**If you like the notebook please upvote it. It feels to motivated to move forward with data science journey. And if any kind of suggestion, view or queries is there then please post in comment section. It will be helpfull to correct the mistake, learn explore.**\n## Thank you!..","f0f57532":"* whereever the locality is Dwarka the city is Delhi, and this Sector5 is area of that so it's misclassified here. which we have to correct.\n* And where city nan there we have to impute this to unknown in locality and city both.","743ba7c3":"**Interpretation**\n* Now, we can see that we are having top 10 Qual columns in our training set which are perfectly one-hot encoded. where there is 2 Qual from top 10 in that row two 1 are there, and it's correct.","75f89c8f":"**1) Extract Experience Year from Experience column and convert to Numeric.**","56632b83":"**4) Working with Qualification**","eef22d55":"**Trying Hyperparameter tuning for RandomForest Regressor**","f9a64fbd":"**HyperParameter Tuning**","50288cbc":"### Feature Scalling\n**StandardScaler** `xbar = x - mean(x) \/ std(x)`","f5e05bf4":"### Categorical Encoding\n**Before encoding a categorical variable, look at relationship of data with respect to target column To have better understand the data**","1b50febc":"## TEST DATASET","7881580a":"**Now, I grouped the rating in 10 groups(bins of size 10). Missing values will fall under 0 group. while, 0-9% will be class 1, 10\u201319% will be class 2, so on and so forth.\n**we will use pd.cut() for this.**\n* Use cut when you need to segment and sort data values into bins. For example, cut could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins.","e58c0a0e":"**3) Rating Column**\n\nRating has more then 50% of missing values so first we have to deal with them,\nthen convert Rating to int.","ef512a46":"* here there are lot of Qualification but most of doctors only has 1 or 2 qualification. \n* And the data is to much messy like, ` 'MBA -Healthcare' and \u2018MBA\u2019` which referred to same category so here we will take the top 10 Qualification that occurs the most.","bb75ca6d":"**Observations**\n* As Delhi, Banglore, Hyderabad, Mumbai and Chennai are in list of Tier 1 city so, as usual the doctor consultation fees in these metripolean cities are very high as compared to Tier 2 and Tier 3 city.\n* Doctor Fess with having profile as ENT Specialist and Dermatologist is very high then Homeopath and Genral medicine are at same level."}}