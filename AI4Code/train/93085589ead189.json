{"cell_type":{"4a13b7cb":"code","14a39bab":"code","2d1ce132":"code","85dc8a96":"code","0c8e04d4":"code","e9cea4be":"code","2ebe9538":"code","9c4b116f":"code","6ad4c974":"code","16ec7ad8":"code","dc3c7c8a":"code","18e8eedc":"code","ea175fce":"code","88bc19a3":"code","204048d6":"code","abf9858b":"code","942411b9":"code","c85197c3":"code","139d228c":"code","ddede26b":"code","c02c097f":"code","1a27307b":"code","31370b05":"code","1e0440f9":"code","b2b7ed6e":"code","88cb77f8":"code","d2a5d979":"markdown","81f7db13":"markdown","cb9fd456":"markdown"},"source":{"4a13b7cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14a39bab":"import nltk\nfrom nltk.corpus import stopwords","2d1ce132":"dfpub = pd.read_csv('\/kaggle\/input\/academic-publications-and-journals\/wiki_query_22_12_2020.csv', encoding=\"iso-8859-1\")","85dc8a96":"dfpub","0c8e04d4":"dfpub['browsing_date']","e9cea4be":"array = {}\nfor c in dfpub['browsing_date']:\n    print(c)\n    if \"1601\" in c:\n        continue\n    today = c.split()[0]\n    array[today] = 1 if today not in array else array[today] + 1","2ebe9538":"x = list(array.keys())\ny = list(array.values())\nprint(len(x))\nprint(len(y))\ndf_plot = pd.DataFrame()\ndf_plot['x'] = x\ndf_plot['y'] = y\ndf_plot.index = x","9c4b116f":"import matplotlib.pylab as plt\nimport matplotlib.dates as mdates","6ad4c974":"plt.figure(figsize=(15, 6))\nplt.bar(pd.to_datetime(df_plot['x']), df_plot['y'])\nax = plt.gca()\nax.xaxis.set_major_locator(mdates.DayLocator(interval=13))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))\nplt.gcf().autofmt_xdate() # Rotation\nplt.show()","16ec7ad8":"from sklearn.feature_extraction.text import CountVectorizer","dc3c7c8a":"wordListCorpus = []\ntitleCorpus = []\nfailedConvert = []\nfor row in dfpub['tags'].values:\n    try:\n        tags = row.split(',')\n        title = \" \".join(row.split(','))\n    except AttributeError:\n        failedConvert.append(row)\n        \n    print(tags)\n    for k in tags:\n        wordListCorpus.append(k)\n    titleCorpus.append(title)\n        \nlen(wordListCorpus)","18e8eedc":"len(wordListCorpus)","ea175fce":"len(failedConvert)","88bc19a3":"len(titleCorpus)","204048d6":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(titleCorpus)","abf9858b":"print(\"Count: {0}\".format(len(vectorizer.get_feature_names())))\nvectorizer.get_feature_names()[1000:1030]","942411b9":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvec = TfidfVectorizer(use_idf=False, norm='l1')\nmatrix = vec.fit_transform(titleCorpus)\npd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())","c85197c3":"from textblob import TextBlob\nimport nltk\nnltk.download('punkt')\n\ndef textblob_tokenizer(str_input):\n    blob = TextBlob(str_input.lower())\n    tokens = blob.words\n    words = [token.stem() for token in tokens]\n    return words\n\nvec = CountVectorizer(tokenizer=textblob_tokenizer)\nmatrix = vec.fit_transform(titleCorpus)\npd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())","139d228c":"vec = TfidfVectorizer(tokenizer=textblob_tokenizer,\n                      stop_words='english',\n                      use_idf=True)\nmatrix = vec.fit_transform(titleCorpus)\ndf = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())","ddede26b":"from sklearn.cluster import KMeans\nnumber_of_clusters = 10\nkm = KMeans(n_clusters=number_of_clusters)\nkm.fit(matrix)","c02c097f":"print(\"Top terms per cluster:\")\norder_centroids = km.cluster_centers_.argsort()[:, ::-1]\nterms = vec.get_feature_names()\nfor i in range(number_of_clusters):\n    top_ten_words = [terms[ind] for ind in order_centroids[i, :5]]\n    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))","1a27307b":"results = pd.DataFrame({\n    'corpus': titleCorpus,\n    'category': km.labels_\n})","31370b05":"results.sort_values('category')\nfor k in results.sort_values('category').values:\n    print(k[1], \" --- \", k[0])","1e0440f9":"from gensim.models import word2vec\nfrom gensim.test.utils import common_texts, get_tmpfile\ntokenized_sentences = [[j.lower() for j in st.split() if j not in stopwords.words('english')] for st in titleCorpus]\nmodel = word2vec.Word2Vec(tokenized_sentences, min_count=1)\nmodel.save(\"word2vec.model\")","b2b7ed6e":"import io\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\nfor word in model.wv.vocab:\n    out_m.write(word + \"\\n\")\n    out_v.write('\\t'.join([str(x) for x in model[word]]) + \"\\n\")\n    \nout_v.close()\nout_m.close()","88cb77f8":"with open('config_tensor.json', 'w') as fd:\n    fd.write(\"\"\"{\n  \"embeddings\": [\n    {\n      \"tensorName\": \"My tensor\",\n      \"tensorShape\": [\n        1000,\n        50\n      ],\n      \"tensorPath\": \"https:\/\/raw.githubusercontent.com\/...\/tensors.tsv\",\n      \"metadataPath\": \"https:\/\/raw.githubusercontent.com\/...\/optional.metadata.tsv\"\n    }\n  ]\n}\"\"\")\n    fd.flush()\n    fd.close()","d2a5d979":"# 1. Time data","81f7db13":"# 2. Word vectors.","cb9fd456":"https:\/\/projector.tensorflow.org\/?config=https:\/\/gist.githubusercontent.com\/p0licat\/2d8c8920d62d648f8963221674002aaf\/raw\/785583070e8eed7e45d654f80c90bc6098fdbd27\/config_embedding_projector_publications.json\n\nLink broken... is it the gitlab hosting?\n"}}