{"cell_type":{"084aa774":"code","f1360ac9":"code","40a79d62":"code","8ca07aa8":"code","743bd8b2":"code","5187a8cc":"code","13c240b5":"code","a4aac1b7":"code","f4fee8a2":"code","ad1e55b4":"code","7825196f":"code","8d17504b":"code","fb605a57":"code","f14ecb57":"code","12c86e65":"code","25b2d705":"code","fb9520fc":"code","c5f6530f":"code","99873033":"code","abe52978":"code","5b68fcab":"code","0afdb927":"code","b7fe471c":"code","f3929062":"code","f63d2951":"code","bf402df1":"code","30d49a64":"code","54811082":"code","4e3c3bde":"code","5697b787":"code","f01b0fcc":"code","3d0e9b1b":"code","2b3e2921":"code","dcfa47ae":"code","519f7bc8":"code","01423206":"code","50073d60":"code","4a2a737e":"code","ec1c05c7":"code","b861e66c":"code","6800ea2d":"code","3e4c7faa":"code","ad7824c1":"code","3c008c92":"code","f61f16b2":"code","9f45371c":"code","8ad6218f":"code","634640fa":"code","d16a20c0":"code","41666596":"code","e7db6d74":"code","ca6674c1":"code","4e0b8f46":"code","3c68ea42":"code","e0904f75":"code","2626790b":"code","d2ea8e9f":"code","f9768bcd":"code","83ca6d90":"code","a62d7a6f":"code","50525b35":"code","dc1b85b4":"markdown","b7f36e1a":"markdown","4b972cef":"markdown","b9393ada":"markdown","ae5c6122":"markdown","8ccd3840":"markdown","30192af1":"markdown","6a1950b0":"markdown","672ef498":"markdown","5ead264b":"markdown","08833c0b":"markdown","6cd70738":"markdown","ca7d55c5":"markdown","44120181":"markdown","bf55a6e6":"markdown","e753e8c3":"markdown","a68246a4":"markdown","038d1444":"markdown","4bc89db9":"markdown","da83a64a":"markdown","58779727":"markdown","18afda68":"markdown","aec8eca5":"markdown","196fb8ec":"markdown","66c86a30":"markdown","1de865ca":"markdown","e4ae2045":"markdown","0e0b28d7":"markdown","9be6b6ac":"markdown","bb352c91":"markdown","7a913bde":"markdown"},"source":{"084aa774":"!pip install tqdm\n!pip install transformers\n\n!mkdir \/kaggle\/working\/sentence_wise_email\/\n!mkdir \/kaggle\/working\/sentence_wise_email\/module\/\n!mkdir \/kaggle\/working\/sentence_wise_email\/module\/module_useT\n!mkdir \/kaggle\/working\/top_10_results\/\n\npath_to_results = '\/kaggle\/working\/top_10_results\/'\npath_to_module_useT = '\/kaggle\/working\/sentence_wise_email\/module\/module_useT'","f1360ac9":"!mkdir module_useT\n# Download the module, and uncompress it to the destination folder.\n!curl -L \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3?tf-hub-format=compressed\" | tar -zxvC \/kaggle\/working\/sentence_wise_email\/module\/module_useT","40a79d62":"# Paths to json files\npath_1 = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\npath_2 = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\npath_3 = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\npath_4 = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\n\n# List of folder names\nfolder_names = ['biorxiv_medrxiv','comm_use_subset']\ndata_path = path_2","8ca07aa8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport json\nimport nltk\nnltk.download('punkt')\nfrom math import log, sqrt\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom tqdm import tqdm\nfrom IPython.core.display import display, HTML\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer","743bd8b2":"TRAINING = True","5187a8cc":"# Data conversion from dataframe to 2 lists\ndef df2list(dir_path = data_path):\n    filenames = os.listdir(dir_path)\n    print('Number of articles retrieved:', len(filenames))\n    files = load_files(dir_path)\n    df = generate_clean_df(files)\n    corpus = list(df['title'] + ' ' + df['abstract'] + ' ' + df['text'])\n    paper_id = list(df['paper_id'])\n    return corpus, paper_id","13c240b5":"# Extraction of data from json format to dataframe \ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    for section, text in texts:\n        texts_di[section] += text\n    body = \"\"\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n    return \"; \".join(formatted)\n\ndef load_files(dirname, filename=None):\n    filenames = os.listdir(dirname)\n    raw_files = []\n    if filename:\n        filename = dirname + filename\n        raw_files = [json.load(open(filename, 'rb'))]\n    else:\n        for filename in tqdm(filenames):\n            filename = dirname + filename\n            file = json.load(open(filename, 'rb'))\n            raw_files.append(file)\n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df = clean_df.drop(columns=['authors','affiliations','bibliography',\n                                      'raw_authors','raw_bibliography'])\n    return clean_df\n\n# It updates the vects_for_docs variable with vectors of all the documents.\ndef iterate_over_all_docs():\n    for i in range(num_of_documents):\n        if np.mod(i, 100) == 0:\n            print('{0} of {1}'.format(str(i).zfill(len(str(num_of_documents))),num_of_documents))\n        doc_text = corpus[i]\n        token_list = get_tokenized_and_normalized_list(doc_text)\n        vect = create_vector(token_list)\n        vects_for_docs.append(vect)\n    print('{0} of {1}'.format(num_of_documents, num_of_documents))\n\n\ndef create_vector_from_query(l1):\n    vect = {}\n    for token in l1:\n        if token in vect:\n            vect[token] += 1.0\n        else:\n            vect[token] = 1.0\n    return vect\n\n\ndef generate_inverted_index():\n    count1 = 0\n    for vector in vects_for_docs:\n        for word1 in vector:\n            inverted_index[word1].append(count1)\n        count1 += 1\n\n\ndef create_tf_idf_vector():\n    vect_length = 0.0\n    for vect in vects_for_docs:\n        for word1 in vect:\n            word_freq = vect[word1]\n            temp = calc_tf_idf(word1, word_freq)\n            vect[word1] = temp\n            vect_length += temp ** 2\n\n        vect_length = sqrt(vect_length)\n        for word1 in vect:\n            vect[word1] \/= vect_length\n\n\ndef get_tf_idf_from_query_vect(query_vector1):\n    vect_length = 0.0\n    for word1 in query_vector1:\n        word_freq = query_vector1[word1]\n        if word1 in document_freq_vect:\n            query_vector1[word1] = calc_tf_idf(word1, word_freq)\n        else:\n            query_vector1[word1] = log(1 + word_freq) * log(\n                num_of_documents)\n        vect_length += query_vector1[word1] ** 2\n    vect_length = sqrt(vect_length)\n    if vect_length != 0:\n        for word1 in query_vector1:\n            query_vector1[word1] \/= vect_length\n\n\ndef calc_tf_idf(word1, word_freq):\n    return log(1 + word_freq) * log(num_of_documents \/ document_freq_vect[word1])\n\n\ndef get_dot_product(vector1, vector2):\n    if len(vector1) > len(vector2):  # this will ensure that len(dict1) < len(dict2)\n        temp = vector1\n        vector1 = vector2\n        vector2 = temp\n    keys1 = vector1.keys()\n    keys2 = vector2.keys()\n    sum = 0\n    for i in keys1:\n        if i in keys2:\n            sum += vector1[i] * vector2[i]\n    return sum\n\n\ndef get_tokenized_and_normalized_list(doc_text):\n    tokens = nltk.word_tokenize(doc_text)\n    ps = nltk.stem.PorterStemmer()\n    stemmed = []\n    for words in tokens:\n        stemmed.append(ps.stem(words))\n    return stemmed\n\n\ndef create_vector(l1):\n    vect = {}  # this is a dictionary\n    global document_freq_vect\n    for token in l1:\n        if token in vect:\n            vect[token] += 1\n        else:\n            vect[token] = 1\n            if token in document_freq_vect:\n                document_freq_vect[token] += 1\n            else:\n                document_freq_vect[token] = 1\n    return vect\n\n\ndef get_result_from_query_vect(query_vector1):\n    parsed_list = []\n    for i in range(num_of_documents - 0):\n        dot_prod = get_dot_product(query_vector1, vects_for_docs[i])\n        parsed_list.append((i, dot_prod))\n        parsed_list = sorted(parsed_list, key=lambda x: x[1])\n    return parsed_list\n\nif TRAINING:\n    \n    corpus, paper_id = df2list()\n\n    # Data processing\n    inverted_index = defaultdict(list)\n    num_of_documents = len(corpus)\n    vects_for_docs = []  # we will need nos of docs number of vectors, each vector is a dictionary\n    document_freq_vect = {}  # sort of equivalent to initializing the number of unique words to 0\n    \n    iterate_over_all_docs()\n    generate_inverted_index()\n    create_tf_idf_vector()","a4aac1b7":"# Search of relevant articles\ndef find_relevant_articles(query, N=10, save_csv_file=False):\n    query_list = get_tokenized_and_normalized_list(query)\n    query_vector = create_vector_from_query(query_list)\n    get_tf_idf_from_query_vect(query_vector)\n    result_set = get_result_from_query_vect(query_vector)\n    papers_info = {'query':query, 'query list':query_list, 'query vector':query_vector,\n                   'id':[], 'title':[], 'abstract':[], 'text':[], 'weight':[], 'index':[]}\n    for i in range(1,N+1):\n        tup = result_set[-i]\n        raw_file = load_files(data_path, filename=paper_id[tup[0]]+'.json')\n        df = generate_clean_df(raw_file)\n        papers_info['id'].append(df['paper_id'][0])\n        papers_info['title'].append(df['title'][0])\n        papers_info['abstract'].append(df['abstract'][0])\n        papers_info['text'].append(df['text'][0])\n        papers_info['weight'].append(tup[1])\n        papers_info['index'].append(tup[0])\n        print('{0}: has relevance weight {1:.6f} and json file is {2} '.format(\n                str(tup[0]).zfill(len(str(num_of_documents))), tup[1], paper_id[tup[0]]))\n        print(df['title'][0])\n    if save_csv_file:\n        names = []\n        data = [[]]\n        for i in range(N):\n            names.append('Article '+str(i))\n            data[0].append(corpus[result_set[-1-i][0]])   \n        df = pd.DataFrame(data, columns = names)\n        df.to_csv('Articles.csv', index=True)\n    print('\\nTop {0} Most Relevant Articles:'.format(N))\n    for i in range(N):\n        print('Paper #{0}: {1}\\n'.format(i+1, papers_info['title'][i]))\n    return papers_info","f4fee8a2":"import pickle\nimport os\n\ndef save_pickles(papers_info, query):\n\n    name_query = query.replace(\" \", \"_\")\n    name_query = name_query.replace(\",\", \"\")\n    name_query = name_query.replace(\".\", \"\")\n    name_query = name_query.replace(\"\/\", \" \")\n    name_path = os.path.join(path_to_results, name_query)\n    \n    if not os.path.exists(name_path):\n        os.makedirs(name_path)\n    \n    with open(path_to_results + name_query + '\/' + name_query + '_papers_info.pickle', 'wb') as f:\n        pickle.dump(papers_info, f)\n    \ndef load_pickles(query):\n    \n    name_query = query.replace(\" \", \"_\")\n    name_query = name_query.replace(\",\", \"\")\n    name_query = name_query.replace(\".\", \"\")\n    name_query = name_query.replace(\"\/\", \" \")\n    \n    with open(path_to_results + name_query + '\/' + name_query + '_papers_info.pickle', 'rb') as f1:\n        file = pickle.load(f1)\n        \n    return file\n","ad1e55b4":"if TRAINING:\n    \n    question_list = []\n\n    question_list.append(\"Is the virus transmitted by aerisol, droplets, food, close contact, fecal matter, or water\")\n    question_list.append(\"How long is the incubation period for the virus\")\n    question_list.append(\"Can the virus be transmitted asymptomatically or during the incubation period\")\n    question_list.append(\"What is the quantity of asymptomatic shedding\")\n    question_list.append(\"How does temperature and humidity affect the tramsmission of 2019-nCoV\")\n    question_list.append(\"How long can 2019-nCoV remain viable on inanimate, environmental, or common surfaces\")\n    question_list.append(\"What types of inanimate or environmental surfaces affect transmission, survival, or  inactivation of 2019-nCov\")\n    question_list.append(\"Can the virus be found in nasal discharge, sputum, urine, fecal matter, or blood\")\n    question_list.append(\"What risk factors contribute to the severity of 2019-nCoV\")\n    question_list.append(\"How does hypertension affect patients\")\n    question_list.append(\"How does heart disease affect patients\")\n    question_list.append(\"How does copd affect patients\")\n    question_list.append(\"How does smoking affect 2019-nCoV patients\")\n    question_list.append(\"How does pregnancy affect patients\")\n    question_list.append(\"What are the case fatality rates for 2019-nCoV patients\")\n    question_list.append(\"What is the case fatality rate in Italy\")\n    question_list.append(\"What public health policies prevent or control the spread of 2019-nCoV\")\n    question_list.append(\"Can animals transmit 2019-nCoV\")\n    question_list.append(\"What animal did 2019-nCoV come from\")\n    question_list.append(\"What real-time genomic tracking tools exist\")\n    question_list.append(\"What regional genetic variations (mutations) exist\")\n    question_list.append(\"What effors are being done in asia to prevent further outbreaks\")\n    question_list.append(\"What drugs or therapies are being investigated\")\n    question_list.append(\"What clinical trials for hydroxychloroquine have been completed\")\n    question_list.append(\"What antiviral drug clinical trials have been completed\")\n    question_list.append(\"Are anti-inflammatory drugs recommended\")\n    question_list.append(\"Which non-pharmaceutical interventions limit tramsission\")\n    question_list.append(\"What are most important barriers to compliance\")\n    question_list.append(\"How does extracorporeal membrane oxygenation affect 2019-nCoV patients\")\n    question_list.append(\"What telemedicine and cybercare methods are most effective\")\n    question_list.append(\"How is artificial intelligence being used in real time health delivery\")\n    question_list.append(\"What adjunctive or supportive methods can help patients\")\n    question_list.append(\"What diagnostic tests (tools) exist or are being developed to detect 2019-nCoV\")\n    question_list.append(\"What is being done to increase testing capacity or throughput\")\n    question_list.append(\"What point of care tests are exist or are being developed\")\n    question_list.append(\"What is the minimum viral load for detection\")\n    question_list.append(\"What markers are used to detect or track COVID-19\")\n    question_list.append('What collaborations are happening within the research community')\n    question_list.append(\"What are the major ethical issues related pandemic outbreaks\")\n    question_list.append(\"How do pandemics affect the physical and\/or psychological health of doctors and nurses\")\n    question_list.append(\"What strategies can help doctors and nurses cope with stress in a pandemic\")\n    question_list.append(\"What factors contribute to rumors and misinformation\")\n    question_list.append(\"What is the immune system response to 2019-nCoV\")\n    question_list.append(\"Can personal protective equipment prevent the transmission of 2019-nCoV\")\n    question_list.append(\"Can 2019-nCoV infect patients a second time\")\n    question_list.append(\"What is the weighted prevalence of sars-cov-2 or covid-19 in general population\")\n\n    for query in question_list:\n\n        papers_info = find_relevant_articles(query=query)\n        save_pickles(query=query, papers_info=papers_info)\n","7825196f":"torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nQA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nQA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nQA_MODEL.to(torch_device)\nQA_MODEL.eval()","8d17504b":"def reconstructText(tokens, start=0, stop=-1):\n    tokens = tokens[start: stop]\n    if '[SEP]' in tokens:\n        sepind = tokens.index('[SEP]')\n        tokens = tokens[sepind+1:]\n    txt = ' '.join(tokens)\n    txt = txt.replace(' ##', '')\n    txt = txt.replace('##', '')\n    txt = txt.strip()\n    txt = \" \".join(txt.split())\n    txt = txt.replace(' .', '.')\n    txt = txt.replace('( ', '(')\n    txt = txt.replace(' )', ')')\n    txt = txt.replace(' - ', '-')\n    txt_list = txt.split(' , ')\n    txt = ''\n    nTxtL = len(txt_list)\n    if nTxtL == 1:\n        return txt_list[0]\n    newList =[]\n    for i,t in enumerate(txt_list):\n        if i < nTxtL -1:\n            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n                newList += [t,',']\n            else:\n                newList += [t, ', ']\n        else:\n            newList += [t]\n    return ''.join(newList)\n\ndef BERTSQuADPrediction(document, question):\n    ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n    ## 50 word overlaps on either end so that it can understand and check longer abstracts\n    nWords = len(document.split())\n    input_ids_all = QA_TOKENIZER.encode(question, document)\n    tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n    overlapFac = 1.1\n    if len(input_ids_all)*overlapFac > 2048:\n        nSearchWords = int(np.ceil(nWords\/5))\n        quarter = int(np.ceil(nWords\/4))\n        docSplit = document.split()\n        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n                     ' '.join(docSplit[quarter-int(nSearchWords*overlapFac\/2):quarter+int(quarter*overlapFac\/2)]),\n                     ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac\/2):quarter*2+int(quarter*overlapFac\/2)]),\n                     ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac\/2):quarter*3+int(quarter*overlapFac\/2)]),\n                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n        \n    elif len(input_ids_all)*overlapFac > 1536:\n        nSearchWords = int(np.ceil(nWords\/4))\n        third = int(np.ceil(nWords\/3))\n        docSplit = document.split()\n        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n                     ' '.join(docSplit[third-int(nSearchWords*overlapFac\/2):third+int(nSearchWords*overlapFac\/2)]),\n                     ' '.join(docSplit[third*2-int(nSearchWords*overlapFac\/2):third*2+int(nSearchWords*overlapFac\/2)]),\n                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n        \n    elif len(input_ids_all)*overlapFac > 1024:\n        nSearchWords = int(np.ceil(nWords\/3))\n        middle = int(np.ceil(nWords\/2))\n        docSplit = document.split()\n        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n                     ' '.join(docSplit[middle-int(nSearchWords*overlapFac\/2):middle+int(nSearchWords*overlapFac\/2)]),\n                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n    elif len(input_ids_all)*overlapFac > 512:\n        nSearchWords = int(np.ceil(nWords\/2))\n        docSplit = document.split()\n        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n    else:\n        input_ids = [input_ids_all]\n    absTooLong = False    \n    \n    answers = []\n    cons = []\n    for iptIds in input_ids:\n        tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n        sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n        num_seg_a = sep_index + 1\n        num_seg_b = len(iptIds) - num_seg_a\n        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n        assert len(segment_ids) == len(iptIds)\n        n_ids = len(segment_ids)\n\n        if n_ids < 512:\n            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n                                     token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n        else:\n            print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n            absTooLong = True\n            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n                                     token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n        start_scores = start_scores[:,1:-1]\n        end_scores = end_scores[:,1:-1]\n        answer_start = torch.argmax(start_scores)\n        answer_end = torch.argmax(end_scores)\n        answer = reconstructText(tokens, answer_start, answer_end+2)\n    \n        if answer.startswith('. ') or answer.startswith(', '):\n            answer = answer[2:]\n            \n        c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n        answers.append(answer)\n        cons.append(c)\n    \n    maxC = max(cons)\n    iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n    confidence = cons[iMaxC]\n    answer = answers[iMaxC]\n    \n    sep_index = tokens_all.index('[SEP]')\n    full_txt_tokens = tokens_all[sep_index+1:]\n    \n    abs_returned = reconstructText(full_txt_tokens)\n\n    ans={}\n    ans['answer'] = answer\n    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n        ans['confidence'] = -1000000\n    else:\n        ans['confidence'] = confidence\n    ans['abstract_bert'] = abs_returned\n    ans['abs_too_long'] = absTooLong\n    return ans","fb605a57":"def create_hit_dictionary(papers_info):\n    \n    path = []\n    for i in range (len(papers_info['id'])):\n        path.append(data_path + papers_info['id'][i] + '.json')\n    \n    hits = []\n    for i in range(len(path)):\n        f = open(path[i])\n        fil = f.read(-1)\n        hits.append(fil)\n    \n    n_hits = len(hits)\n\n    ## collect the relevant data in a hit dictionary\n    hit_dictionary = {}\n    for i in range(0, n_hits):\n        doc_json = json.loads(hits[i])\n        idx = str(doc_json['paper_id'])\n        hit_dictionary[idx] = doc_json\n        #print(doc_json)\n        hit_dictionary[idx]['title'] = doc_json['metadata'][\"title\"]\n        hit_dictionary[idx]['authors'] = doc_json['metadata'][\"authors\"]\n\n    ## scrub the abstracts in prep for BERT-SQuAD\n    for idx,v in hit_dictionary.items():\n        abs_dirty = v['abstract']\n        # looks like the abstract value can be an empty list\n        v['abstract_paragraphs'] = []\n        v['abstract_full'] = ''\n\n        if abs_dirty:\n            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n\n            if isinstance(abs_dirty, list):\n                for p in abs_dirty:\n                    v['abstract_paragraphs'].append(p['text'])\n                    v['abstract_full'] += p['text'] + ' \\n\\n'\n\n            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n            if isinstance(abs_dirty, str):\n                v['abstract_paragraphs'].append(abs_dirty)\n                v['abstract_full'] += abs_dirty + ' \\n\\n'\n    return hit_dictionary","f14ecb57":"def searchAbstracts(hit_dictionary, question):\n    abstractResults = {}\n    for k,v in tqdm(hit_dictionary.items()):\n        abstract = v['abstract_full']\n        if abstract:\n            ans = BERTSQuADPrediction(abstract, question)\n            if ans['answer']:\n                confidence = ans['confidence']\n                abstractResults[confidence]={}\n                abstractResults[confidence]['answer'] = ans['answer']\n                abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n                abstractResults[confidence]['idx'] = k\n                abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n                \n    cList = list(abstractResults.keys())\n\n    if cList:\n        maxScore = max(cList)\n        total = 0.0\n        exp_scores = []\n        for c in cList:\n            s = np.exp(c-maxScore)\n            exp_scores.append(s)\n        total = sum(exp_scores)\n        for i,c in enumerate(cList):\n            abstractResults[exp_scores[i]\/total] = abstractResults.pop(c)\n    return abstractResults","12c86e65":"def embed_useT(module):\n    with tf.Graph().as_default():\n        sentences = tf.compat.v1.placeholder(tf.string)\n        embed = hub.Module(module)\n        embeddings = embed(sentences)\n        session = tf.compat.v1.train.MonitoredSession()\n    return lambda x: session.run(embeddings, {sentences: x})\nembed_fn = embed_useT(path_to_module_useT)","25b2d705":"def displayResults(hit_dictionary, answers, question):\n    \n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+question+'<\/div>'\n\n    confidence = list(answers.keys())\n    confidence.sort(reverse=True)\n    \n    confidence = list(answers.keys())\n    confidence.sort(reverse=True)\n    \n\n    for c in confidence:\n        if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n            if 'idx' not in  answers[c]:\n                continue\n            rowData = []\n            idx = answers[c]['idx']\n            title = hit_dictionary[idx]['title']\n            authors = hit_dictionary[idx]['authors']\n\n            \n            full_abs = answers[c]['abstract_bert']\n            bert_ans = answers[c]['answer']\n            \n            \n            split_abs = full_abs.split(bert_ans)\n            sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n            if len(split_abs) == 1:\n                sentance_end_pos = len(full_abs)\n                sentance_end =''\n            else:\n                sentance_end_pos = split_abs[1].find('. ')+1\n                if sentance_end_pos == 0:\n                    sentance_end = split_abs[1]\n                else:\n                    sentance_end = split_abs[1][:sentance_end_pos]\n                \n            #sentance_full = sentance_beginning + bert_ans+ sentance_end\n            answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n            answers[c]['sentence_beginning'] = sentance_beginning\n            answers[c]['sentence_end'] = sentance_end\n            answers[c]['title'] = title\n        else:\n            answers.pop(c)\n    \n    \n    ## now rerank based on semantic similarity of the answers to the question\n    cList = list(answers.keys())\n    allAnswers = [answers[c]['full_answer'] for c in cList]\n    \n    messages = [question]+allAnswers\n    \n    encoding_matrix = embed_fn(messages)\n    similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n    rankings = similarity_matrix[1:,0]\n    \n    for i,c in enumerate(cList):\n        answers[rankings[i]] = answers.pop(c)\n\n    ## now form pandas dv\n    confidence = list(answers.keys())\n    confidence.sort(reverse=True)\n    pandasData = []\n    ranked_aswers = []\n    for c in confidence:\n        rowData=[]\n        title = answers[c]['title']\n        idx = answers[c]['idx']\n        rowData += [idx]            \n        sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"<\/font> \"+answers[c]['sentence_end']+'<\/div>'\n        \n        rowData += [sentance_html, c]\n        pandasData.append(rowData)\n        ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n    \n        pdata2 = pandasData\n        \n    display(HTML(question_HTML))\n    \n    df = pd.DataFrame(pdata2, columns = ['id', 'Answer', 'Confidence'])\n        \n    display(HTML(df.to_html(render_links=True, escape=False)))","fb9520fc":"query = \"Is the virus transmitted by aerisol, droplets, food, close contact, fecal matter, or water\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","c5f6530f":"query = \"How long is the incubation period for the virus\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","99873033":"query = \"What is the quantity of asymptomatic shedding\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","abe52978":"query = \"How does temperature and humidity affect the tramsmission of 2019-nCoV\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","5b68fcab":"query = \"How long can 2019-nCoV remain viable on inanimate, environmental, or common surfaces\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","0afdb927":"query = \"What types of inanimate or environmental surfaces affect transmission, survival, or  inactivation of 2019-nCov\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","b7fe471c":"query = \"Can the virus be found in nasal discharge, sputum, urine, fecal matter, or blood\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","f3929062":"query = \"What risk factors contribute to the severity of 2019-nCoV\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","f63d2951":"query = \"How does hypertension affect patients\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","bf402df1":"query = \"How does heart disease affect patients\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","30d49a64":"query = \"How does copd affect patients\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","54811082":"query = \"How does smoking affect 2019-nCoV patients\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","4e3c3bde":"query = \"How does pregnancy affect patients\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","5697b787":"query = \"What are the case fatality rates for 2019-nCoV patients\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","f01b0fcc":"query = \"What is the case fatality rate in Italy\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","3d0e9b1b":"query = \"What public health policies prevent or control the spread of 2019-nCoV\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","2b3e2921":"query = \"Can animals transmit 2019-nCoV\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","dcfa47ae":"query = \"What animal did 2019-nCoV come from\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","519f7bc8":"query = \"What real-time genomic tracking tools exist\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","01423206":"query = \"What regional genetic variations (mutations) exist\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","50073d60":"query = \"What effors are being done in asia to prevent further outbreaks\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","4a2a737e":"query = \"What drugs or therapies are being investigated\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","ec1c05c7":"query = \"What clinical trials for hydroxychloroquine have been completed\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","b861e66c":"query = \"What antiviral drug clinical trials have been completed\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","6800ea2d":"query = \"Are anti-inflammatory drugs recommended\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","3e4c7faa":"query = \"Which non-pharmaceutical interventions limit tramsission\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","ad7824c1":"query = \"What are most important barriers to compliance\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","3c008c92":"query = \"How does extracorporeal membrane oxygenation affect 2019-nCoV patients\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","f61f16b2":"query = \"What telemedicine and cybercare methods are most effective\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","9f45371c":"query = \"How is artificial intelligence being used in real time health delivery\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","8ad6218f":"query = \"What adjunctive or supportive methods can help patients\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","634640fa":"query = \"What diagnostic tests (tools) exist or are being developed to detect 2019-nCoV\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","d16a20c0":"query = \"What is being done to increase testing capacity or throughput\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","41666596":"query = \"What point of care tests are exist or are being developed\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","e7db6d74":"query = \"What is the minimum viral load for detection\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","ca6674c1":"query = \"What markers are used to detect or track COVID-19\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","4e0b8f46":"query = 'What collaborations are happening within the research community'\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","3c68ea42":"query = \"What are the major ethical issues related pandemic outbreaks\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","e0904f75":"query = \"How do pandemics affect the physical and\/or psychological health of doctors and nurses\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","2626790b":"query = \"What strategies can help doctors and nurses cope with stress in a pandemic\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","d2ea8e9f":"query = \"What factors contribute to rumors and misinformation\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","f9768bcd":"query = \"What is the immune system response to 2019-nCoV\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","83ca6d90":"query = \"Can personal protective equipment prevent the transmission of 2019-nCoV\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","a62d7a6f":"query = \"Can 2019-nCoV infect patients a second time\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","50525b35":"query = \"What is the weighted prevalence of sars-cov-2 or covid-19 in general population\"\npapers_info = load_pickles(query)\nhit_dictionary = create_hit_dictionary(papers_info=papers_info)\nanswers = searchAbstracts(hit_dictionary, query)\ndisplayResults(hit_dictionary, answers, query)","dc1b85b4":"## What is known about transmission, incubation, and environmental stability?","b7f36e1a":"Corpus formation.","4b972cef":"## What do we know about non-pharmaceutical interventions?","b9393ada":"Highlight the sentance that BERT-SQuAD identified.","ae5c6122":"The objective of this project is to integrate the state of the art Natural Language Processing (NLP) techniques to create a method to ask a database a question and have it return sensible results that answer that question. The BERT model searches for answers on a large dataset rather slowly, so it was decided to use tf-idf first and find the 10 most relevant articles, then make the BERT model find an answer among these 10 articles.","8ccd3840":"## Other interesting Questions","30192af1":"## What has been published about ethical and social science considerations?","6a1950b0":"## What do we know about vaccines and therapeutics?","672ef498":"The function that to do answers on all the abstracts.","5ead264b":"## Working process:","08833c0b":"If training is true, then save results in pickles files, else load pickle files to save time.","6cd70738":"## Acknowledgements:\nI'm grateful to [Dirk](https:\/\/www.kaggle.com\/dirktheeng). Props to him for making this possible.","ca7d55c5":"Create the list of question.","44120181":"Build a semantic similarity search capability to rank answers in terms of how closely they line up to the meaning of the NL question.","bf55a6e6":"Search of the most relevant articles.","e753e8c3":"Import the required library.","a68246a4":"## What has been published about medical care?","038d1444":"Processing of Corpus.","4bc89db9":"## Find top 10 results.","da83a64a":"## Project Description:","58779727":"Get the Universal Sentence Encoder.","18afda68":"Collect the relevant data in a hit dictionary, then clean it.","aec8eca5":"## What has been published about information sharing and inter-sectoral collaboration?","196fb8ec":"Extraction of data from json format to dataframe. Then data conversion from dataframe to 2 lists. And in the last - find top 10 relevant articles.","66c86a30":"## What do we know about virus genetics, origin, and evolution?","1de865ca":"Get the transformer models.","e4ae2045":"## What do we know about diagnostics and surveillance?","0e0b28d7":"Set the training flag.","9be6b6ac":"Install the required environment and path.","bb352c91":"Extraction of data from json files to dataframe format.","7a913bde":"## What do we know about COVID-19 risk factors?"}}