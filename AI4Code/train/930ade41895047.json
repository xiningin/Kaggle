{"cell_type":{"71b99f35":"code","59744415":"code","8c06b72d":"code","96e6fde0":"code","2646ef88":"code","78e054fd":"code","ffc82acf":"code","e17e0e6a":"code","64dbc1ab":"code","9f0320a5":"code","37016102":"code","47411f7e":"code","b9528856":"code","c0971d30":"code","bc159c61":"code","72712f31":"code","401471c3":"code","92ea044e":"code","64ac147f":"code","0f52f182":"markdown","fac64801":"markdown","223db958":"markdown","6bc45b4e":"markdown","fd2f3a6b":"markdown","4450076b":"markdown","15f3bf73":"markdown","d03ec719":"markdown","b9706f9a":"markdown","0c16f961":"markdown","f14d6b3c":"markdown","bf9dd996":"markdown"},"source":{"71b99f35":"import pandas as pd","59744415":"red_wine_dataset = pd.read_csv(\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv\", delimiter=\";\")\nred_wine = pd.DataFrame(red_wine_dataset)\nred_wine.head()","8c06b72d":"red_wine.isnull().sum()","96e6fde0":"import seaborn as sns","2646ef88":"sns.displot(red_wine[\"quality\"])","78e054fd":"correlation_matrix = red_wine.corr().round(2)\nsns.heatmap(data=correlation_matrix, annot=True)","ffc82acf":"import numpy as np","e17e0e6a":"X = pd.DataFrame(np.c_[red_wine[\"volatile acidity\"], red_wine[\"citric acid\"], red_wine[\"sulphates\"], red_wine[\"alcohol\"]], columns=[\"VA\", \"CA\", \"SULP\", \"ALC\"])\nY = red_wine[\"quality\"]","64dbc1ab":"alc_v_qual = {}\nfor i, j in zip(X[\"ALC\"], Y):\n    alc_v_qual[i] = j\nprint(\"Alcohol vs. Quality\")\nsns.scatterplot(data=alc_v_qual)","9f0320a5":"split_index = int(red_wine.shape[0] * 0.8)\nX_train = X.iloc[:split_index]\nY_train = Y.iloc[:split_index]\nX_test = X.iloc[split_index:]\nY_test = Y.iloc[split_index:]","37016102":"import random\nimport time","47411f7e":"random.seed(time.time())","b9528856":"class LinearRegression:\n    def __init__(self, X_train, Y_train, X_test, Y_test, learning_rate, num_iterations):\n        self.X_train = X_train\n        self.Y_train = Y_train\n        self.X_test = X_test\n        self.Y_test = Y_test\n        self.learning_rate = learning_rate\n        self.num_iterations = num_iterations\n        self.theta = []\n        for i in range(X_train.shape[1]):\n            self.theta.append(random.random())\n        self.theta = np.array(self.theta)\n    def function(self, X):\n        return np.dot(self.theta, X)\n    def cost_function_derivative(self, j):\n        num = 0\n        for i in range(self.X_train.shape[0]):\n            num += (self.function(np.array(self.X_train.iloc[i])) - self.Y_train.iloc[i]) * self.X_train.iloc[i, j]\n        return num\n    def fit(self):\n        mse_list = {}\n        r_squared_list = {}\n        print(\"Training...\")\n        print(\"Learning Rate: \" + str(self.learning_rate))\n        print(\"Num. of iterations: \" + str(self.num_iterations))\n        print(\"---------------------------------------------------------------\")\n        epochs = 0\n        temp_theta = np.zeros(self.X_train.shape[1])\n        while epochs < self.num_iterations:\n            print(\"Epoch: \" + str(epochs))\n            for i in range(self.X_train.shape[1]):\n                temp_theta[i] = self.theta[i] - self.learning_rate * (1 \/ self.X_train.shape[0]) * self.cost_function_derivative(i)\n            for i in range(len(temp_theta)):\n                self.theta[i] = temp_theta[i]\n            mse_value = self.mse(self.X_train, self.Y_train)\n            mse_list[epochs] = mse_value\n            print(\"MSE: \" + str(mse_value))\n            r_squared_value = self.r_squared(self.X_train, self.Y_train)\n            r_squared_list[epochs] = r_squared_value\n            print(\"R-squared value: \" + str(r_squared_value))\n            epochs += 1\n            print(\"---------------------------------------------------------------\")\n        print(\"Epochs vs. MSE (blue)\")\n        sns.lineplot(data=mse_list)\n        print(\"Epochs vs. R-squared (orange)\")\n        sns.lineplot(data=r_squared_list)\n    def test(self):\n        print(\"Testing...\")\n        print(\"---------------------------------------------------------------\")\n        mse_value = self.mse(self.X_test, self.Y_test)\n        print(\"MSE: \" + str(mse_value))\n        r_squared_value = self.r_squared(self.X_test, self.Y_test)\n        print(\"R-squared value: \" + str(r_squared_value))\n    def r_squared(self, X, Y):\n        rss = 0\n        for i in range(X.shape[0]):\n            rss += (self.function(X.iloc[i]) - Y.iloc[i]) ** 2\n        tss = 0\n        y_bar = np.median(Y)\n        for i in range(Y.shape[0]):\n            tss += (Y.iloc[i] - y_bar) ** 2\n        return  1 - (rss \/ tss)\n    def mse(self, X, Y):\n        num = 0\n        for i in range(X.shape[0]):\n            num += (self.function(X.iloc[i]) - Y.iloc[i]) ** 2\n        num *= (1 \/ (2 * X.shape[0]))\n        return num","c0971d30":"lr = LinearRegression(X_train, Y_train, X_test, Y_test, 0.001, 50)\nlr.fit()","bc159c61":"lr.test()","72712f31":"lr_two = LinearRegression(X_train, Y_train, X_test, Y_test, 0.0001, 75)\nlr_two.fit()","401471c3":"lr_two.test()","92ea044e":"lr_three = LinearRegression(X_train, Y_train, X_test, Y_test, 0.01, 50)\nlr_three.fit()","64ac147f":"lr_three.test()","0f52f182":"The random number generator is seeded for the purpose of producing random weight coefficients.","fac64801":"Volatile acidity, citric acid amount, sulphate amount, and alcohol content are considered for thsi linear regression model.","223db958":"See which variables are most correlated to the wine quality.","6bc45b4e":"The first linear regression model uses 0.001 as the learning rate and 50 as the number of iterations.","fd2f3a6b":"Check if any values in the dataset is null.","4450076b":"The third linear regression model uses 0.01 as the learning rate and 50 as the number of iterations.","15f3bf73":"The second linear regression model uses 0.0001 as the learning rate and 75 as the number of iterations.","d03ec719":"This plots out the distribution of wine qualities.","b9706f9a":"Overall, I am not satisfied that my model found the best fit solution because the R-squared value is poor. This may be because my independent variables are not correlated strongly with my dependent variables, but it may also be because of the fact that the gradient descent algorithm used is too simple.","0c16f961":"The dataset is split into a traning and testing set using a 80\/20 split.","f14d6b3c":"The alcohol content vs. wine quality is plotted.","bf9dd996":"For this assignment, I will be using a dataset on wine quality based on factors like free sulphates, alcohol, and citric acid."}}