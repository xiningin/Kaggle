{"cell_type":{"ad39f823":"code","01e6b3b4":"code","ebff8f39":"code","cac680bb":"code","2ee2c0fb":"code","2565e9e4":"code","dab8352e":"code","d3ec86e6":"code","e4042f61":"code","c1c5d461":"code","0923ad3b":"code","ac2ab080":"code","a79bf42a":"code","88073eb6":"code","c1caec00":"code","d1ccd13f":"code","b981f1dd":"markdown","f573bc39":"markdown","7f52cf23":"markdown","557a70a0":"markdown","ba820981":"markdown","cda954ef":"markdown","8e2cb4c2":"markdown","46282ca0":"markdown","8b5b93cf":"markdown","44552469":"markdown","0c00c407":"markdown"},"source":{"ad39f823":"!nvidia-smi","01e6b3b4":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport random\n\nimport nltk\nimport string\nimport re\nimport math\n\nfrom sklearn.utils import shuffle\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n# Autocast\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Stochastic Weight Average\nfrom torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, AdamW\n\nimport warnings\nwarnings.filterwarnings('ignore')","ebff8f39":"import transformers\ntransformers.__version__","cac680bb":"def seed_everything(seed = 0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed = 1234\nseed_everything(seed)","2ee2c0fb":"base_dir = '..\/input\/clrcross-validation-strategies'\ndata = pd.read_csv(f'{base_dir}\/train_folds_shuffle.csv')\nbenchmark = data[data['standard_error'] == 0.]\ndata['compare_to_benchmark'] = np.sign(data['target'])\ndata.head()","2565e9e4":"print('*' * 50)\nsample_text_low = data.sort_values('target')['excerpt'].iloc[10]\nprint('Sample hard-to-read document: \\n', sample_text_low)\nprint('*' * 50)\nsample_text_high = data.sort_values('target')['excerpt'].iloc[-10]\nprint('Sample easy-to-read document: \\n', sample_text_high)","dab8352e":"def clean_text(text):\n    text = text.lower().strip()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndef text_preprocessing(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    combined_text = ' '.join(tokenized_text)\n    return combined_text\n\ndef readability_feat(text):\n    text = nlp(text)\n    \n    return np.array([text._.flesch_kincaid_grade_level,\n                     text._.flesch_kincaid_reading_ease,\n                     text._.dale_chall,\n                     text._.coleman_liau_index,\n                     text._.automated_readability_index,\n                     text._.forcast], dtype = np.float)\n\ndef sample_text(targets, num_output = 5):\n    mean, var = targets[0], targets[1]\n    if targets[1] != 0.:\n        sampled_target = torch.normal(mean, var, size = (num_output,))\n    else:\n        sampled_target = torch.tensor([0.] * num_output, dtype = torch.float)\n    return sampled_target\n\ndef convert_examples_to_features(text, tokenizer, max_len, is_test = False, return_tensor = False):\n    # Take from https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-fit\n    text = text.replace('\\n', '')\n    if return_tensor:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            return_tensors = 'pt',\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = True\n        )\n    else:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = True\n        )\n    return tok\n\ndef form_dataset(token, external_features = None, target = None, bins = None):\n    if target is not None:\n        if bins is not None:\n            if external_features is not None:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'external_features': torch.tensor(external_features, dtype = torch.float),\n                    'target': target,\n                    'bins': bins,\n                }\n            else:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'target': target,\n                    'bins': bins,\n                }\n        else:\n            if external_features is not None:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'external_features': torch.tensor(external_features, dtype = torch.float),\n                    'target': target,\n                }\n            else:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'target': target,\n                }\n    else:\n        if external_features is not None:\n            return {\n                'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                'external_features': torch.tensor(external_features, dtype = torch.float),\n            }\n        else:\n            return {\n                'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n            }","d3ec86e6":"class Readability_Dataset(Dataset):\n    def __init__(self, documents, tokenizer, sample = False, max_len = 300, num_output = 5, binning = True, mode = 'train'):\n        self.documents = documents\n        self.tokenizer = tokenizer\n        self.sample = sample\n        self.max_len = max_len\n        self.mode = mode\n        self.num_output = num_output\n        \n        if self.mode == 'train':\n            self.binning = binning\n        \n    def __len__(self):\n        return len(self.documents)\n    \n    def __getitem__(self, idx):\n        sample = self.documents.iloc[idx]\n        document = sample['excerpt']\n        \n        # Compute readability features\n        ext_features = None # readability_feat(document)\n        \n        # Tokenize\n        features = convert_examples_to_features(document, self.tokenizer, self.max_len)\n        \n        if self.mode == 'train':\n            target = torch.tensor([sample['target'], sample['standard_error']], dtype = torch.float)\n            if self.sample:\n                target = sample_text(target, num_output = self.num_output)\n                \n            if self.binning:\n                bins = torch.tensor(sample['bins'], dtype = torch.long)\n            else:\n                bins = None\n                \n        elif self.mode == 'valid':\n            target = torch.tensor(sample['target'])\n            bins = None\n        else:\n            target = None\n            bins = None\n            \n        return form_dataset(features, external_features = ext_features, target = target, bins = bins)","e4042f61":"class Readability_Model(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","c1c5d461":"class KLLoss(nn.Module):\n    def __init__(self):\n        super(KLLoss, self).__init__()\n        \n    def forward(self, pred_mean, pred_std, target_mean, target_std):\n        p = torch.distributions.Normal(pred_mean, pred_std)\n        q = torch.distributions.Normal(target_mean, target_std)\n        loss = torch.mean(torch.distributions.kl_divergence(p, q))\n        return loss\n    \nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super(RMSELoss, self).__init__()\n        \n    def forward(self, pred_mean, target_mean):\n        return torch.mean((pred_mean - target_mean)**2)\n    \nclass RankingLoss(nn.Module):\n    def __init__(self):\n        super(RankingLoss, self).__init__()\n        \n    def forward(self, pred_mean, pred_benchmark_mean, target_mean, margin = 0.5):\n        return nn.MarginRankingLoss(margin = margin)(pred_mean, pred_benchmark_mean, torch.sign(target_mean))\n    \nclass QuadraticWeightedKappaLoss(nn.Module):\n    def __init__(self, num_cat = 7, device = 'cpu'):\n        super(QuadraticWeightedKappaLoss, self).__init__()\n        self.num_cat = num_cat\n        cats = torch.arange(num_cat).to(device)\n        self.weights = (cats.view(-1,1) - cats.view(1,-1)).pow(2) \/ (num_cat - 1)**2\n        \n    def _confusion_matrix(self, pred_cat, true_cat):\n        confusion_matrix = torch.zeros((self.num_cat, self.num_cat)).to(pred_cat.device)\n        for t, p in zip(true_cat.view(-1), pred_cat.view(-1)):\n            confusion_matrix[t.long(), p.long()] += 1\n        return confusion_matrix\n        \n    def forward(self, pred_cat, true_cat):\n        # Confusion matrix\n        O = self._confusion_matrix(pred_cat, true_cat)\n        \n        # Count elements in each category\n        true_hist = torch.bincount(true_cat, minlength = self.num_cat)\n        pred_hist = torch.bincount(pred_cat, minlength = self.num_cat)\n        \n        # Expected values\n        E = torch.outer(true_hist, pred_hist)\n        \n        # Normlization\n        O = O \/ torch.sum(O)\n        E = E \/ torch.sum(E)\n        \n        # Weighted Kappa\n        numerator = torch.sum(self.weights * O)\n        denominator = torch.sum(self.weights * E)\n        \n        return numerator \/ denominator\n    \nclass BradleyTerryLoss(nn.Module):\n    def __init__(self):\n        super(BradleyTerryLoss, self).__init__()\n        \n    def forward(self, pred_mean, true_mean):\n        batch_size = len(pred_mean)\n        true_comparison = true_mean.view(-1,1) - true_mean.view(1,-1)\n        pred_comparison = pred_mean.view(-1,1) - pred_mean.view(1,-1)\n        \n        return torch.log(1 + torch.tril(torch.exp(-true_comparison * pred_comparison))).sum() \/ (batch_size * (batch_size - 1) \/ 2)\n    \ndef loss_fn(pred_mean, pred_std, target_mean, target_std, pred_cat = None, target_cat = None, loss_type = 'rmse', num_bins = None):\n    assert loss_type in ['rmse', 'kl', 'rank', 'qwk', 'rmse_rank', 'kl_rank', 'rmse_qwk', 'kl_qwk', 'rank_qwk', \n                         'bradley-terry', 'rmse_bradley-terry', 'qwk_bradley-terry', 'rmse_qwk_bradley-terry']\n    if 'qwk' in loss_type:\n        assert (pred_cat is not None) and (target_cat is not None) and (num_bins is not None)\n    if 'rank' in loss_type:\n        assert pred_benchmark_mean is not None\n    \n    device = pred_mean.device\n    \n    if loss_type == 'rmse':\n        return RMSELoss()(pred_mean, target_mean)\n    elif loss_type == 'kl':\n        return KLLoss()(pred_mean, pred_std, target_mean, target_std)\n    elif loss_type == 'rank':\n        return RankingLoss()(pred_mean, target_mean, margin = 0.5)\n    elif loss_type == 'qwk':\n        return QuadraticWeightedKappaLoss(num_cat = num_bins, device = device)(pred_cat, target_cat)\n    elif loss_type == 'rmse_rank':\n        return torch.sqrt(RMSELoss()(pred_mean, target_mean)) + RankingLoss()(pred_mean, pred_benchmark_mean, target_mean, margin = 0.5)\n    elif loss_type == 'kl_rank':\n        return KLLoss()(pred_mean, pred_std, target_mean, target_std) + RankingLoss()(pred_mean, pred_benchmark_mean, target_mean, margin = 0.5)\n    elif loss_type == 'rmse_qwk':\n        return torch.sqrt(RMSELoss()(pred_mean, target_mean)) + QuadraticWeightedKappaLoss(num_cat = num_bins, device = device)(pred_cat, target_cat)\n    elif loss_type == 'kl_qwk':\n        return KLLoss()(pred_mean, pred_std, target_mean, target_std) + QuadraticWeightedKappaLoss(num_cat = num_bins, device = device)(pred_cat, target_cat)\n    elif loss_type == 'bradley-terry':\n        return BradleyTerryLoss()(pred_mean, target_mean)\n    elif loss_type == 'rmse_bradley-terry':\n        return torch.sqrt(RMSELoss()(pred_mean, target_mean)) + BradleyTerryLoss()(pred_mean, target_mean)\n    elif loss_type == 'qwk_bradley-terry':\n        return BradleyTerryLoss()(pred_mean, target_mean) + \\\n               QuadraticWeightedKappaLoss(num_cat = num_bins, device = device)(pred_cat, target_cat)\n    elif loss_type == 'rmse_qwk_bradley-terry':\n        return torch.sqrt(RMSELoss()(pred_mean, target_mean)) + BradleyTerryLoss()(pred_mean, target_mean) + \\\n               QuadraticWeightedKappaLoss(num_cat = num_bins, device = device)(pred_cat, target_cat)\n\ndef metric_fn(pred_mean, target_mean):\n    return np.sqrt(np.mean((pred_mean - target_mean)**2))","0923ad3b":"def train(model, train_dataloader, valid_dataloader, optimizer, fold, epoch, cfg, benchmark_token = None, \n          scaler = None, scheduler = None, best_metric_val = np.inf, swa_model = None, swa_scheduler = None):\n    \n    loss = 0\n    num_sample = 0\n    \n    pred = []\n    true = []\n    \n    if cfg.use_tqdm:\n        tbar = tqdm(train_dataloader)\n    else:\n        tbar = train_dataloader\n    \n    for i, item in enumerate(tbar):\n        model.train()\n        input_ids = item['input_ids'].to(cfg.device)\n        token_type_ids = item['token_type_ids'].to(cfg.device)\n        attention_mask = item['attention_mask'].to(cfg.device)\n        external_features = None\n        targets = item['target'].to(cfg.device)\n        if cfg.binning:\n            bins = item['bins'].to(cfg.device)\n        else:\n            bins = None\n        \n        if benchmark_token is not None:\n            benchmark_input_ids, benchmark_token_type_ids, benchmark_attention_mask = benchmark_token\n            input_ids = torch.cat((input_ids, benchmark_input_ids), dim = 0)\n            token_type_ids = torch.cat((token_type_ids, benchmark_token_type_ids), dim = 0)\n            attention_mask = torch.cat((attention_mask, benchmark_attention_mask), dim = 0)\n\n        if cfg.is_sampled:\n            true_mean = targets\n            true_std = None\n        else:\n            true_mean = targets[:,0]\n            true_std = targets[:,1]\n        \n        batch_size = input_ids.shape[0]\n        \n        # Set zero gradient\n        optimizer.zero_grad()\n        \n        with autocast(enabled = True):\n            # Feed the input to the model\n            pred_mean, pred_std, pred_cat = model(input_ids, token_type_ids, attention_mask, external_features)\n\n            # Compute loss\n            loss_batch = loss_fn(pred_mean, pred_std, true_mean, true_std, pred_cat = pred_cat, target_cat = bins, loss_type = cfg.loss_type, num_bins = cfg.num_bins)\n        \n        if cfg.use_tqdm:\n            tbar.set_description(f'Loss: {round(loss_batch.item(), 3)}')\n        \n        if scaler is not None:\n            # Back-propagation\n            scaler.scale(loss_batch).backward()\n            \n            # Update gradient\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            # Back-propagation\n            loss_batch.backward()\n            \n            # Update gradient\n            optimizer.step()\n        \n        # Evaluation\n        if epoch < int(cfg.evaluate_every[0][0] * cfg.nepochs):\n            evaluate_every = cfg.evaluate_every[0][1]\n        elif epoch >= int(cfg.evaluate_every[0][0] * cfg.nepochs) and epoch < (cfg.evaluate_every[1][0] * cfg.nepochs):\n            evaluate_every = cfg.evaluate_every[1][1]\n        else:\n            evaluate_every = cfg.evaluate_every[2]\n            \n        if i % evaluate_every == 0 or i == len(tbar) - 1:\n            if swa_model is None:\n                loss_val, metric_val = valid(model, valid_dataloader, cfg, benchmark_token = benchmark_token)\n                if metric_val < best_metric_val:\n                    best_metric_val = metric_val\n                    model_dict = {\n                        'epoch': epoch,\n                        'model_state_dict': model.state_dict(),\n                        'best_metric': best_metric\n                    }\n                    torch.save(model_dict, os.path.join(cfg.output_dir, f'model_best_fold_{fold}_{cfg.model_name}.bin'))\n                    print(f'Best metric updated to: {best_metric_val} !!!')\n            else:\n                swa_model.update_parameters(model)\n                update_bn(train_dataloader, swa_model)\n                loss_val, metric_val = valid(swa_model, valid_dataloader, cfg, benchmark_token = benchmark_token)\n                if metric_val < best_metric_val:\n                    best_metric_val = metric_val\n                    model_dict = {\n                        'epoch': epoch,\n                        'model_state_dict': swa_model.state_dict(),\n                        'best_metric': best_metric\n                    }\n                    torch.save(model_dict, os.path.join(cfg.output_dir, f'model_best_fold_{fold}_{cfg.model_name}.bin'))\n                    print(f'Best metric updated to: {best_metric_val} !!!')\n                \n        # Verbosity\n        if i % cfg.verbose == 0:\n            print(f'Epoch: {epoch + 1}  -  Finished: {round(i \/ len(train_dataloader) * 100, 2)}%  -  Valid metric: {metric_val}  -  Best metric: {best_metric_val}')\n        \n        loss += loss_batch * batch_size\n        num_sample += batch_size\n        \n        # Extract output\n        pred.extend(pred_mean.cpu().detach().numpy())\n        true.extend(true_mean.cpu().detach().numpy())\n        \n        torch.cuda.empty_cache()\n        \n        if swa_model is not None:\n            swa_scheduler.step()\n        \n        if scheduler is not None:\n            if swa_model is None:\n                scheduler.step()        \n        \n    # Stack\n    pred = np.array(pred)\n    true = np.array(true)\n    \n    # Compute loss and metrics\n    loss = torch.sqrt(loss \/ num_sample)\n    metric = metric_fn(pred, true)\n    \n    return loss, metric, best_metric_val","ac2ab080":"def valid(model, valid_dataloader, cfg, benchmark_token = None):\n    model.eval()\n    \n    loss = 0\n    num_sample = 0\n    \n    pred = []\n    true = []\n    \n    if cfg.use_tqdm:\n        tbar = tqdm(valid_dataloader)\n    else:\n        tbar = valid_dataloader\n    \n    for item in tbar:\n        input_ids = item['input_ids'].to(cfg.device)\n        token_type_ids = item['token_type_ids'].to(cfg.device)\n        attention_mask = item['attention_mask'].to(cfg.device)\n        external_features = None\n        targets = item['target'].to(cfg.device)\n        true_mean = targets\n        true_std = None\n        \n        batch_size = input_ids.shape[0]\n\n        if benchmark_token is not None:\n            benchmark_input_ids, benchmark_token_type_ids, benchmark_attention_mask = benchmark_token\n            input_ids = torch.cat((input_ids, benchmark_input_ids), dim = 0)\n            token_type_ids = torch.cat((token_type_ids, benchmark_token_type_ids), dim = 0)\n            attention_mask = torch.cat((attention_mask, benchmark_attention_mask), dim = 0)\n        \n        # Feed the input to the model\n        with torch.no_grad():\n            with autocast(enabled = True):\n                pred_mean, pred_std, pred_cat = model(input_ids, token_type_ids, attention_mask, external_features)\n                \n                if cfg.is_sampled:\n                    pred_mean = torch.mean(pred_mean, dim = -1)\n                    \n                # Compute loss\n                loss_batch = loss_fn(pred_mean, pred_std, true_mean, true_std, loss_type = 'rmse')\n\n            if cfg.use_tqdm:\n                tbar.set_description(f'Loss: {round(loss_batch.item(), 3)}')\n            \n            loss += loss_batch * batch_size\n            num_sample += batch_size\n\n            # Extract output\n            pred.extend(pred_mean.cpu().detach().numpy())\n            true.extend(true_mean.cpu().detach().numpy())\n        \n    # Stack\n    pred = np.array(pred)\n    true = np.array(true)\n    \n    # Compute loss and metrics\n    loss = torch.sqrt(loss \/ num_sample)\n    metric = metric_fn(pred, true)\n    \n    return loss, metric","a79bf42a":"def get_optimizer_params(model, model_type = 'backbone', learning_rate = 2e-5, weight_decay = 0.01, layerwise_learning_rate_decay = 0.95):\n    no_decay = ['bias', 'LayerNorm.weight']\n    # Initialize lr for task specific layer\n    optimizer_grouped_parameters = [\n        {\n            'params': [p for n, p in model.named_parameters() if 'backbone' not in n],\n            'weight_decay': 0.0,\n            'lr': 1e-3,\n        },\n    ]\n    # Initialize lrs for every layer\n    num_layers = model.model_config.num_hidden_layers\n    layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer)\n    layers.reverse()\n    lr = learning_rate\n    for i, layer in enumerate(layers):\n        lr *= layerwise_learning_rate_decay\n        optimizer_grouped_parameters += [\n            {\n                'params': [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n                'weight_decay': weight_decay,\n                'lr': lr,\n            },\n            {\n                'params': [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0,\n                'lr': lr,\n            },\n        ]\n    return optimizer_grouped_parameters","88073eb6":"class config():\n    # For training\n    nepochs = 20\n    lr = 2e-5\n    weight_decay = 0.01\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    use_tqdm = False\n    verbose = 60\n    evaluate_every = ((0.3, 20), (0.6, 40), 80)\n    es = None\n    swa_start = 6\n    swa_lr = 2e-6\n    # For dataloader\n    is_sampled = False\n    if is_sampled:\n        loss_type = 'rmse'\n        num_output = 10\n    else:\n        loss_type = 'rmse_qwk_bradley-terry'\n        num_output = 2\n    if 'qwk' in loss_type:\n        num_bins = 29\n        binning = True\n    else:\n        num_bins = 1\n        binning = False\n    max_len = 250\n    batch_size = 4\n    num_workers = 4\n    # For model\n    output_dir = os.getcwd()\n    backbone = 'microsoft\/deberta-large'\n    model_name = '_'.join('deberta-large'.split('-'))\n\ncfg = config()","c1caec00":"# Bin the target\nif 'qwk' in cfg.loss_type:\n    data['bins'] = pd.cut(data['target'], bins = cfg.num_bins, labels = False)","d1ccd13f":"for fold in range(5):\n    # Split\n    trn = data[data['kfold'] != fold]\n    val = data[data['kfold'] == fold]\n    \n    tokenizer = AutoTokenizer.from_pretrained(cfg.backbone)\n    model_dir = cfg.backbone\n    model_config = AutoConfig.from_pretrained(model_dir, output_hidden_states = True)\n    \n    # Tokenize the benchmark text\n    benchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, cfg.max_len, return_tensor = True)\n    benchmark_token = (benchmark_token['input_ids'].to(cfg.device), benchmark_token['token_type_ids'].to(cfg.device), benchmark_token['attention_mask'].to(cfg.device))\n    \n    # Dataset\n    train_dataset = Readability_Dataset(trn, tokenizer, sample = cfg.is_sampled, max_len = cfg.max_len, binning = cfg.binning, mode = 'train')\n    valid_dataset = Readability_Dataset(val, tokenizer, sample = False, max_len = cfg.max_len, binning = cfg.binning, mode = 'valid')\n    \n    # Dataloader\n    train_dataloader = DataLoader(train_dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = False)\n    \n    # Model\n    model = Readability_Model(model_dir, model_config, num_output = cfg.num_output, num_cat = cfg.num_bins, \n                              benchmark_token = benchmark_token).to(cfg.device)\n    swa_model = AveragedModel(model)\n    \n    # Differentiated learning rate for separate layers\n    optimizer_grouped_parameters = get_optimizer_params(model, learning_rate = cfg.lr, weight_decay = cfg.weight_decay)\n    optimizer = AdamW(optimizer_grouped_parameters, lr = cfg.lr, weight_decay = cfg.weight_decay)\n    num_training_steps = cfg.nepochs * len(train_dataloader)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = int(0.05 * num_training_steps), \n                                                num_training_steps = num_training_steps)\n    \n    swa_scheduler = SWALR(optimizer, anneal_strategy = 'linear', anneal_epochs = 1, swa_lr = cfg.swa_lr)\n    \n    scaler = GradScaler()\n    \n    es = 0\n    best_metric = np.inf\n    \n    print('*' * 50)\n    print(f'Fold: {fold}')\n    \n    for epoch in range(cfg.nepochs):\n        if epoch < cfg.swa_start:\n            _, _, best_metric = train(model, train_dataloader, valid_dataloader, optimizer, fold, epoch, cfg, benchmark_token = benchmark_token, \n                                      scaler = scaler, scheduler = scheduler, best_metric_val = best_metric)\n        else:\n            _, _, best_metric = train(model, train_dataloader, valid_dataloader, optimizer, fold, epoch, cfg, benchmark_token = benchmark_token, scaler = scaler, \n                                      best_metric_val = best_metric, swa_model = swa_model, swa_scheduler = swa_scheduler)\n\n        if epoch >= 7:\n            break\n            \n    update_bn(train_dataloader, swa_model)","b981f1dd":"# Model configuration","f573bc39":"# Dataset","7f52cf23":"# Loss functions and metrics","557a70a0":"# Import data","ba820981":"# DeBERTa version 1, sub-version 1","cda954ef":"* Differentiate learning rate https:\/\/www.kaggle.com\/rhtsingh\/on-stability-of-few-sample-transformer-fine-tuning","8e2cb4c2":"# Main","46282ca0":"# Utils","8b5b93cf":"# Training and validation functions","44552469":"# Model","0c00c407":"* Readability features by spaCy"}}