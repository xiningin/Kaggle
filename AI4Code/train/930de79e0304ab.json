{"cell_type":{"e1a4140a":"code","91506e70":"code","94b3ec37":"code","05dbdeed":"code","682375aa":"code","dfd82e84":"code","2fd2a9cf":"code","e32eaea1":"code","d9e1a0cb":"code","0d56ebd1":"code","763537c4":"code","8c10bda6":"code","5b8052b7":"code","f716fbaa":"code","5f9c2d90":"code","1bbfc5bb":"code","2bccaecc":"code","f4437468":"code","9b7f7ff7":"code","4635e456":"code","d1d778ae":"code","765b655c":"code","3d13f33a":"code","5354cc4b":"code","6697e353":"code","c2041978":"code","f7e91b37":"code","fa0aff78":"code","7abca812":"code","3b6fafab":"code","1088a2bb":"code","6fec6990":"code","db32b10f":"markdown","51cc957f":"markdown","ecebf756":"markdown","4c04a34a":"markdown","e7a8771f":"markdown","720f6b41":"markdown","e0ad8545":"markdown","0e8e8b47":"markdown","7c20a4be":"markdown","51139dd2":"markdown","b6fd00d3":"markdown","476457b0":"markdown","b00e607f":"markdown","9a42dad2":"markdown","6f8f27e6":"markdown","a177a620":"markdown","bed4df06":"markdown","c36e7fbf":"markdown","c0b127c3":"markdown","a7c67da9":"markdown","b5f36c1f":"markdown","bf3bb9c4":"markdown","eeb0054a":"markdown","1caaee06":"markdown","b004cadc":"markdown","b19999e3":"markdown"},"source":{"e1a4140a":"# Import libraries and load data with pandas as a dataframe\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\ndata = pd.read_csv('..\/input\/BreadBasket_DMS.csv')","91506e70":"# .info() is good for first look.\ndata.info()","94b3ec37":"# .head() and .tail() will show first and last 10 items in dataframe.\ndata.head(10)","05dbdeed":"data.tail(10)","682375aa":"# After use .unique() we use len() to find how many unique items that we have.\nitems_unique_list = data[\"Item\"].unique()\nlen(items_unique_list)\n","dfd82e84":"# Here my little list :D\nword_list = [\"NaN\", \"-\", \"nan\", \"NAN\", \"None\", \"NONE\", \"none\", \" \", \"_\", \".\"]\n\n# I use the list comprehension to make this code smaller.\nfound_words = [word_list[i] for i, c in enumerate([w in items_unique_list for w in word_list]) if c == True]\n\n# Found word types is 1 so only one of thing in my list founded in our data (\"NONE\")\nlen(found_words)","2fd2a9cf":"len(data[data[\"Item\"] == \"NONE\"])\n","e32eaea1":"# Data include 786 missing values let's drop them\nfor f in found_words:\n    data = data.replace(to_replace=f, value=np.nan).dropna()","d9e1a0cb":"# Let's look again unique Item list it must be 94.\nitems_unique_list = data[\"Item\"].unique()\nlen(items_unique_list)","0d56ebd1":"from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly\n\nplotly.offline.init_notebook_mode(connected=True)","763537c4":"# Get first 10 items from list\nhot_items = data.Item.value_counts()[:10]\n\n# Find and sum the remaining items and label it as \"Others\".\nother_items_count = data.Item.count() - hot_items.sum()\n\n# Add two of them in one series.\nitem_list = hot_items.append(pd.Series([other_items_count], index=[\"Others\"]))\n\n# Here the item list. Yes I like coffee too.\nitem_list","8c10bda6":"# Values include the list you see above.\nvalues = item_list.tolist()\n# Labels include top ten items name.\nlabels = item_list.index.values.tolist()\n\n# Pie is suitable I think\nfig = {\n  \"data\": [\n    {\n      \"values\": values,\n      \"labels\": labels,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"Top 10 Items\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Top 10 Most Popular Items\",\n    }\n}\niplot(fig)","5b8052b7":"# We get all transaction numbers which contain coffee.\ncoffee_transaction_list = data[data['Item'] == \"Coffee\"][\"Transaction\"].tolist()","f716fbaa":"# Then copy our data to protect it.\ndata_copy = data.copy(deep=True)","5f9c2d90":"# And drop all transactions which not contain coffee.\n# I use this method based on a suggestion in comments.(Thanks Sam'ir)\ndata_copy = data_copy[data_copy['Transaction'].isin(coffee_transaction_list)]\n# Old way here ->\n'''\n# Note: Please comment if is there a more efficient way to do this beacuse I think this is not a good way to do this.\nfor i in range(max(data[\"Transaction\"])+1):\n    if i not in coffee_transaction_list:\n        data_copy = data_copy.drop(data_copy[data_copy.Transaction == i].index)\n'''","1bbfc5bb":"# Lastly, we get our precious dataframe\ndata_copy.head(15)","2bccaecc":"# We get top ten items from two different data frame.\nhot_items_coffe_combine = data_copy.Item.value_counts()[:10]\nhot_items = data.Item.value_counts()[:10]\n\n# We need to drop coffee values beacuse we don't need it when we compare items with coffee or without coffee\nhot_items_coffe_combine = hot_items_coffe_combine.drop(labels=[\"Coffee\"])\nhot_items = hot_items.drop(labels=[\"Coffee\"])\n\n# Labels are Item names\nlabels = hot_items_coffe_combine.index.values.tolist()\n\n# And values are just values :\/ \nvalues_coffe_combine = hot_items_coffe_combine.tolist()\nvalues = hot_items.tolist()\n\n# First time when I wrote this kernel I made a critical mistake.\n# We need to subtract values_coffe_combine from values to get values_without_coffee.\n# I forgot to do this step.\nvalues_without_coffee = [values[i]-v for i,v in enumerate(values_coffe_combine)]\n\ndf = pd.DataFrame({'with_coffee':values_coffe_combine, 'without_coffee':values_without_coffee})\ndf\n\n# We have 9 items (The coffee is dropped)\n# Bread           \n# Tea             \n# Cake            \n# Pastry          \n# Sandwich        \n# Medialuna       \n# Hot chocolate   \n# Cookies         \n# Brownie ","f4437468":"# To plot a graph we transform it to list\nvalues_list = df.values.tolist()\n\n# We added ratio (with_coffee \/ without_coffee) to all labels\nfor index, value in enumerate(values_list):\n    labels[index] = labels[index] + \"  %{:.2f}\".format(value[0] \/ value[1])\n\n# Here our values\nlabels","9b7f7ff7":"import plotly.plotly as py\nimport plotly.graph_objs as go\n\ntop_labels = [\"Bought With Coffee\", \"Bought Without Coffee\"]\n\ncolors = ['rgba(38, 24, 74, 0.8)', 'rgba(190, 192, 213, 1)']\n\nx_data = values_list\n\ny_data = labels\n\ntraces = []\n\nfor i in range(0, len(x_data[0])):\n    for xd, yd in zip(x_data, y_data):\n        traces.append(go.Bar(\n            x=[xd[i]],\n            y=[yd],\n            orientation='h',\n            marker=dict(\n                color=colors[i],\n                line=dict(\n                        color='rgb(248, 248, 249)',\n                        width=1)\n            )\n        ))\n\nlayout = go.Layout(\n    xaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=False,\n        zeroline=False,\n        domain=[0.15, 1]\n    ),\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=False,\n        zeroline=False,\n    ),\n    barmode='stack',\n    title='Sales Information of Top Ten Items With\/Without Coffee ',\n    paper_bgcolor='rgb(248, 248, 248)',\n    plot_bgcolor='rgb(248, 248, 255)',\n    margin=dict(\n        l=120,\n        r=10,\n        t=140,\n        b=80\n    ),\n    showlegend=False,\n)\n\nannotations = []\n\nfor yd, xd in zip(y_data, x_data):\n    # labeling the y-axis\n    annotations.append(dict(xref='paper', yref='y',\n                            x=0.14, y=yd,\n                            xanchor='right',\n                            text=str(yd),\n                            font=dict(family='Arial', size=14,\n                                      color='rgb(67, 67, 67)'),\n                            showarrow=False, align='right'))\n    # labeling the first percentage of each bar (x_axis)\n    annotations.append(dict(xref='x', yref='y',\n                            x=xd[0] \/ 2, y=yd,\n                            text=str(xd[0]),\n                            font=dict(family='Arial', size=14,\n                                      color='rgb(248, 248, 255)'),\n                            showarrow=False))\n    # labeling the first Likert scale (on the top)\n    if yd == y_data[-1]:\n        annotations.append(dict(xref='x', yref='paper',\n                                x=xd[0] \/ 2, y=1.1,\n                                text=top_labels[0],\n                                font=dict(family='Arial', size=16,\n                                          color='rgba(38, 24, 74, 0.8)'),\n                                showarrow=False))\n    space = xd[0]\n    for i in range(1, len(xd)):\n            # labeling the rest of percentages for each bar (x_axis)\n            annotations.append(dict(xref='x', yref='y',\n                                    x=space + (xd[i]\/2), y=yd, \n                                    text=str(xd[i]),\n                                    font=dict(family='Arial', size=14,\n                                              color='rgb(248, 248, 255)'),\n                                    showarrow=False))\n            # labeling the Likert scale\n            if yd == y_data[-1]:\n                annotations.append(dict(xref='x', yref='paper',\n                                        x=space + (xd[i]\/0.2), y=1.1,\n                                        text=top_labels[i],\n                                        font=dict(family='Arial', size=16,\n                                                  color='rgba(190, 192, 213, 1)'),\n                                        showarrow=False))\n            space += xd[i]\n\nlayout['annotations'] = annotations\n\nfig = go.Figure(data=traces, layout=layout)\niplot(fig, filename='bar-colorscale')","4635e456":"# Remeber that I have data_copy data frame which only \n# includes transactions that contain coffee.\ndata_copy.head(6)\n","d1d778ae":"data_copy = data_copy[data_copy['Item'] == \"Coffee\"].drop(['Transaction'], axis=1)\ndata_copy.head()","765b655c":"# Groupby will find the group Date column by size \ndate_count_df = data_copy.groupby([\"Date\"]).size().reset_index(name=\"Coffee\")\ndate_count_df.head()","3d13f33a":"data_len = len(date_count_df.Coffee)\n\ntest = date_count_df.Coffee.loc[data_len*0.8:]\ntrain = date_count_df.Coffee.loc[:data_len*0.8]\n\nprint(train.shape, test.shape)","5354cc4b":"train.describe()","6697e353":"# Let's add some visualize I will use matplotlib\nimport matplotlib.pyplot as plt\n\n# Set graph size\nplt.figure(figsize=(16,4))\n\n# Add vertical lines in every 7 days.\nfor xc in np.arange(0, len(train), step=7):\n    plt.axvline(x=xc, color='k', linestyle='--')\n\n# Graph's Y axis will be train(Coffe Sales) and X axis will be (lenght(train) \/ 7) (Every 7 day)\nplt.plot(train)\nplt.xticks(np.arange(0, len(train), step=7))\nplt.xlabel('Days')\nplt.ylabel('Coffee Sale')\nplt.show()\n","c2041978":"# We reshape train (127,) to (127, 1)\ntrain_scaled = train.values.reshape(-1,1)\n\n# Add Sklearn MinMaxScaler library\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range = (0, 1))\ntrain_scaled = scaler.fit_transform(train_scaled)\ntrain_scaled[:5]","f7e91b37":"X_train = []\ny_train = []\ntimesteps = 7\nfor i in range(timesteps, len(train)):\n    X_train.append(train_scaled[i-timesteps:i, 0])\n    y_train.append(train_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)\n\nprint(X_train.shape, y_train.shape)","fa0aff78":"# We reshape X_train (122, 7) to (122, 7, 1) \nprint(X_train.shape)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_train.shape","7abca812":"# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(40, input_shape=(timesteps, 1))) # 40 LSTM layer\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(X_train, y_train, epochs=300, batch_size=6, verbose=2)","3b6fafab":"# We must prepare \"inputs\" to create \"X_train\". This \"inputs\" contain \"test\" and last 7 items of \"train\".\n# So why we add last 7 items of \"train\" to beginning of the \"inputs\"?\n# Beacuse when we use \"train\" to create \"X_train\", we couldn't use last 7 item\n# to create new array step.\n# So our \"inputs\" size equal to (lenght of \"test\" + \"timestep\")\n# When we start to create \"X_test\" we will see \"X_test\" size will be (\"inputs\" size - 7)\n# because again we won't use last 7 item of \"inputs\".\n\ntotal = pd.concat((train, test), axis = 0)\ninputs = total[len(total) - len(test) - timesteps:].values.reshape(-1,1)\n# Lastly we normalize our inputs data.\ninputs = scaler.transform(inputs) \n\nprint(\"Inputs shape ->\",inputs.shape, \"  Test shape ->\", test.shape)","1088a2bb":"# And we will create X_test data from inputs data.\n# Timestep is same\n\nX_test = []\nfor i in range(timesteps, len(inputs)):\n    X_test.append(inputs[i-timesteps:i, 0])\n\nprint(len(X_test)) # As you can see \"X_test\" size equal to \"test\" size not to \"inputs\" size","6fec6990":"# We turn it list to np.array\nX_test = np.array(X_test)\n\n# Then transform it's shape\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n# We give X_test to our model to predict predicted_coffe_sales\npredicted_coffe_sales = model.predict(X_test)\n# Inverse transform will transform it from (0,1) to it's original range\npredicted_coffe_sales = scaler.inverse_transform(predicted_coffe_sales)\n\n# We will compare Predicted Coffe Sales and Real Coffee Sales\nreal_coffe_sales = test.values\n\nplt.plot(real_coffe_sales, color = 'red', label = 'Real Coffe Sales')\nplt.plot(predicted_coffe_sales, color = 'blue', label = 'Predicted Coffe Sales')\nplt.title('Coffee Sales Prediction For Last Month')\nplt.xlabel('Days')\nplt.ylabel('Coffee Sale per Day')\nplt.legend()\nplt.show()","db32b10f":" **LSTM model**","51cc957f":"This graph code is too long I know but this is the best graph for this job I think.","ecebf756":"Yes, coffee is the most popular and the bread is second. So let's try to find our first problems answer. \n\n**Do people like them two as a combine or separately?**\n<a id=\"4\"><\/a> <br>\n## 4-Find the best item combinations with coffee [^](#0)\n\nTo find this out firstly we need to get only transactions that include coffee.","4c04a34a":" So how many of them are \"NONE\" ?","e7a8771f":"Okay, let's split our data to train and test parts. The test size will be 0.2 of all data.","720f6b41":"<a id=\"3\"><\/a> <br>\n## 3-Find most popular Items  [^](#0)\nStart with import some libraries then we will do some data transform for prepare data for visualizing.","e0ad8545":"We can understand from this graph that people don't like bread + coffee combine. But coffee + toast seems very liked. The medialuna + coffee combine is our second popular combine. This bakery maybe make some discounts for this combines : )","0e8e8b47":"We have 4 column.","7c20a4be":"In this data, we have time and date information for all transaction so I think we can construct a model that predict the coffee sales per day. Let's try it.","51139dd2":"Data is cleared and now the item number is 94. Let's look at the top items that bought most. We will need some visualization.\nI am going to use plotly beacuse I love it <3","b6fd00d3":"Our train data have 127 day's coffe sales information. Average sales per day = 34.7 cup coffee.","476457b0":"As you can see our model is not good enough. Next update I will try to make it better. Please for comment any suggestion. Thank you.","b00e607f":"Okay, finally we can start to render some graphs.","9a42dad2":"# **Analysis of Bakery Transactions**\n\n### Hello, In this kernel I try to find best  item combinations. \n<a id=\"0\"><\/a> <br>\n#### **Content:**\n1. [Load and Exploring the Data](#1)\n2. [Cleaning the Data](#2)\n3. [Find most popular Items](#3)\n4. [Find the best item combinations with coffee](#4)\n5. [Prediction of Coffee Sales Per Day](#5)\n","6f8f27e6":"We create X_train and y_train datas with timesteps = 7 for feed our LSTM model. \n\nI will try to show it with an example list. More detail is here -> [link](https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/)\n\n![](http:\/\/i65.tinypic.com\/309hg7b.png)","a177a620":"We can clearly see there are 95 items has been found in dataset however there can be some *NaN* values int data that we need to clear. To do this we create a basic *NaN* word list and then clear the data.","bed4df06":"<a id=\"1\"><\/a> <br>\n## 1-Exploring the Data [^](#0)\nThis dataset contains more than 6000 transactions and the date and time data's of these transactions. There is, of course, the item list that sold. So the list of problems is here that we can maybe find a solution.\n* 1 - Maybe we can find the most popular item combination bought by people.\n* 2 - We have the date information so maybe we can use RNN to predict the item sales per day.\n* 3 - I will add more problem here if I found.\n\nOkay let's start with load the data.","c36e7fbf":"Let's get the top ten most bought items. Then sum and add the other items as \"Others\" in our top list.","c0b127c3":"We have 21293 entries in nearly 6-month time interval. 9684 are at a different time.\nSo let's start with to find unique items in our data.","a7c67da9":"In this graph, we can see there is a peak in every 7 days. I think this because of the weekends.\nIf we want we can learn it but we don't need to find it for our model.","b5f36c1f":"Okay, let's go for our second problem.\n<a id=\"5\"><\/a> <br>\n## 5-Prediction of Coffee Sales Per Day [^](#0)","bf3bb9c4":"Now we can count the days in order to found coffee sales per day.","eeb0054a":"I will delete all other items except coffee","1caaee06":"Let's return to build model. We need to normalize our sales to (0,1) range.","b004cadc":"Now we have a data frame which only includes transactions that contain coffee. Let's look at our top items again with this data frame. Then we can compare them.","b19999e3":"<a id=\"2\"><\/a> <br>\n## 2-Cleaning the Data [^](#0)\nOur data include some missing values. I want to use .dropna() function to drop them but their type is object(string) and they are not *NaN* they are *None*. So I found different solution to get rid of this problem. I construct a word list of possible words for missing values. Then check my data with this list. "}}