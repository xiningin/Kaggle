{"cell_type":{"315d9564":"code","cd8173e1":"code","2a2aa0ea":"code","a2e34142":"code","e7f0bf10":"code","30653618":"code","2d6fe71a":"code","977696a8":"code","7d00b639":"code","eb7012f2":"code","40f06a54":"code","f762584e":"code","af23a708":"code","7a121c9c":"code","a16557fe":"code","c5efdb86":"code","9179625f":"code","53ee1858":"code","fb3d6cfc":"code","18a6df08":"code","454866b0":"code","43ceed00":"code","e1b113ad":"code","19a66521":"code","641b3a5f":"code","d2b4cc49":"code","c671db2b":"code","8cbabc93":"code","47b9100a":"code","0d967f7f":"code","6008eb42":"code","36cb163f":"markdown","3751c8de":"markdown","bf836556":"markdown","fe7a895b":"markdown","ffc0482e":"markdown","93f2b685":"markdown","8a8df502":"markdown","c574f3f1":"markdown","e04a8da7":"markdown","f868265b":"markdown","6193372b":"markdown","604d7636":"markdown","15126bc7":"markdown","901afdd4":"markdown","7f99ed06":"markdown"},"source":{"315d9564":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd8173e1":"%matplotlib inline\n\nimport torch\nimport torch.optim as optim\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, models, transforms\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n#from torchvision.transforms import functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data.sampler import WeightedRandomSampler\nfrom dataclasses import dataclass\nimport csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom PIL import Image\n\n%matplotlib inline\n\n\nDATA_ROOT = \"\/kaggle\/input\/pytorch-opencv-course-classification\/\"\n#IMAGES_ROOT = DATA_ROOT + '\/images\/images\/'","2a2aa0ea":"def image_common_transforms():\n    common_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], \n                             [0.229, 0.224, 0.225])\n        ])        \n    \n    return common_transforms","a2e34142":"def image_training_transforms():\n    common_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(),\n        transforms.RandomHorizontalFlip(),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], \n                             [0.229, 0.224, 0.225])\n        ])        \n    return common_transforms","e7f0bf10":"def get_class_distribution(dataset_obj):\n    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}\n    \n    for element in dataset_obj:\n        y_lbl = element[1]\n        y_lbl = dataset_obj.idx2class[y_lbl]\n        count_dict[y_lbl] += 1\n            \n    return count_dict","30653618":"def get_class_distribution_loaders(dataloader_obj, dataset_obj, is_validation):\n    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}\n    \n#    print(count_dict)\n    \n#    for _,j in dataloader_obj:\n        \n#        y_idx = j.item()\n#        y_lbl = dataset_obj.idx2class[y_idx]\n#        count_dict[str(y_lbl)] += 1\n        \n    if dataloader_obj.batch_size == 1:    \n        for _,label_id in dataloader_obj:\n            y_idx = label_id.item()\n            y_lbl = dataset_obj.idx2class[y_idx]\n            count_dict[str(y_lbl)] += 1\n            if is_validation == True:\n                dataset_obj.set_validation(y_idx)\n    else: \n        for _,label_id in dataloader_obj:\n            for idx in label_id:\n                y_idx = idx.item()\n                y_lbl = dataset_obj.idx2class[y_idx]\n                count_dict[str(y_lbl)] += 1\n                if is_validation == True:\n                    dataset_obj.set_validation(y_idx)\n\n            \n    return count_dict","2d6fe71a":"def get_target_list_loaders(dataloader_obj):\n    target_list = []\n    \n    if dataloader_obj.batch_size == 1:    \n        for _,label_id in dataloader_obj:\n            y_idx = label_id.item()\n            target_list.append(y_idx)\n    else: \n        for _,label_id in dataloader_obj:\n            for idx in label_id:\n                y_idx = idx.item()\n                target_list.append(y_idx)\n                \n    return target_list","977696a8":"label_csv_path = os.path.join(DATA_ROOT, 'train.csv')\nlabels_df = pd.read_csv(label_csv_path)\n\nnum_total_classes = len(labels_df['class'].unique())\nprint(num_total_classes)\navg_count_per_class = len(labels_df)\/num_total_classes\n\nprint(\"Num labels {} num classes {} avg count per class {}\".format(len(labels_df), num_total_classes, avg_count_per_class))\nlabels_df.groupby('class').nunique().plot(kind='barh')","7d00b639":"from torch.utils.data.dataset import random_split\n\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, data_root, transform=None):\n        label_csv_path = os.path.join(data_root, 'train.csv')\n        self.label_df = pd.read_csv(label_csv_path)\n#        print(len(self.label_df))\n        \n        # set transform attribute\n        self.transform = transform\n        self.validation_transform = None\n        \n        num_classes = self.label_df['class'].unique()\n        self.classes_list = num_classes.tolist()\n#        print(num_classes)\n#        print(len(num_classes))\n        print(self.classes_list)\n        \n        self.idx2class = {i: key for i, key in enumerate(self.classes_list)}\n        self.class_to_idx = {key: i for i, key in enumerate(self.classes_list)}\n        print(\"=====\")\n        print(self.class_to_idx)\n        \n        with open('.\/labels.csv', 'w') as f:\n            for key in self.idx2class.keys():\n                f.write(\"%d,%s\\n\"%(key,self.idx2class[key]))\n        \n        \n        self.data_dict = {\n            'image_path': [],\n            'label': [],\n            'is_train':[]\n        }\n        \n        img_dir = os.path.join(data_root, 'images', 'images')\n\n        for index, row in self.label_df.iterrows():\n            img_path = os.path.join(img_dir, '{}.jpg'.format(row[0]))\n            self.data_dict['image_path'].append(img_path)\n            self.data_dict['label'].append(self.class_to_idx[row[1]])#self.classes_list.index(row[1]))\n            self.data_dict['is_train'].append(True)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        For given index, return images with resize and preprocessing.\n        \"\"\"\n        \n        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")\n        \n        if self.data_dict['is_train'][idx] == True:\n            if self.transform is not None:\n                image = self.transform(image)\n        else:\n            if self.validation_transform is not None:\n                image = self.validation_transform(image)\n            \n            \n        target = self.data_dict['label'][idx]\n        \n        return image, target\n    \n    def __len__(self):\n        return len(self.data_dict['label'])\n    \n    def set_validation(self, idx):\n        self.data_dict['is_train'][idx] = False\n    \n    def set_validation_transform(self, transform):\n        self.validation_transform = transform\n\n    \n        \n    def get_targets():\n        \n        targets = []\n        \n        for key, value in self.class_to_idx:\n            targets.append(value)\n            \n        return targets\n    \n    \n\n\n\n\ndef get_data(batch_size, data_root, num_workers=1):\n    \n    dataset =  KenyanFood13Dataset(data_root, transform=image_training_transforms())\n#    dataset =  KenyanFood13Dataset(data_root, transform=image_common_transforms())\n\n    dataset.set_validation_transform(image_common_transforms())\n    \n#    validation_split = .2\n#    random_seed= 42\n    dataset_size = len(dataset)\n    print('Total dataset size {}'.format(dataset_size))\n#    indices = list(range(dataset_size))\n#    split = int(np.floor(validation_split * dataset_size))\n#    np.random.seed(random_seed)\n#    np.random.shuffle(indices)\n#    train_indices, val_indices = indices[split:], indices[:split]\n    \n    train_len = int(0.8*len(dataset))\n    valid_len = len(dataset) - train_len    \n    \n    train_dataset, val_dataset = random_split(dataset, [train_len, valid_len])\n    print('Train dataset size {}, test dataset size {}'.format(len(train_dataset), len(val_dataset)))\n\n#    train_sampler = SubsetRandomSampler(train_indices)\n#    valid_sampler = SubsetRandomSampler(val_indices)\n\n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1, \n                                                shuffle=False, num_workers=num_workers)\n#                                               sampler=train_sampler, shuffle=False, num_workers=num_workers)\n    validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=1,\n                                                    shuffle=False, num_workers=num_workers)\n#                                                    sampler=valid_sampler, shuffle=False, num_workers=num_workers)\n\n    train_class_distribution = get_class_distribution_loaders(train_loader, dataset, is_validation=False)\n    validation_class_distribution = get_class_distribution_loaders(validation_loader, dataset, is_validation=True)\n    \n    print(train_class_distribution)\n    print(validation_class_distribution)\n    \n    \n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,7))\n    sns.barplot(data = pd.DataFrame.from_dict([train_class_distribution]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Train Set')\n    sns.barplot(data = pd.DataFrame.from_dict([validation_class_distribution]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[1]).set_title('Val Set')\n\n\n    train_elements_count = 0\n    validation_elements_count = 0\n    class_count_train = []\n    class_count_validation = []\n    \n    for key, value in train_class_distribution.items():\n        train_elements_count += value\n        class_count_train.append(value)\n        \n    print(class_count_train)\n    \n    for key, value in validation_class_distribution.items():\n        validation_elements_count += value\n        class_count_validation.append(value)\n        \n    print(class_count_validation)\n    \n    print(\"Train elements count {}\".format(train_elements_count))\n    \n\n    print(\"Validation elements count {}\".format(validation_elements_count))\n   \n    \n    class_weights_train = 1.\/torch.tensor(class_count_train, dtype=torch.float)\n    class_weights_validation = 1.\/torch.tensor(class_count_validation, dtype=torch.float)\n    \n    print(\"Class weights train {}\".format(class_weights_train))\n    print(\"Class weights validation {}\".format(class_weights_validation))\n    \n    target_list_train = get_target_list_loaders(train_loader)\n    target_list_validation = get_target_list_loaders(validation_loader)\n    \n    print(\"Target list len {} {} {} {} \".format(len(target_list_train), target_list_train[0], target_list_train[5], target_list_train[15]))\n    print(\"Validation list len {} {} {} {} \".format(len(target_list_train), target_list_validation[0], target_list_validation[5], target_list_validation[15]))\n    \n    \n    class_weights_all_train = class_weights_train[target_list_train]\n    class_weights_all_validation = class_weights_validation[target_list_validation]\n    \n    print(class_weights_all_validation)\n    \n    weighted_sampler_train = WeightedRandomSampler(weights=class_weights_all_train, num_samples=len(class_weights_all_train), replacement=True)\n    weighted_sampler_validation = WeightedRandomSampler(weights=class_weights_all_validation, num_samples=len(class_weights_all_validation), replacement=True)\n    \n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, \n                                               sampler=weighted_sampler_train, shuffle=False, num_workers=num_workers)\n    validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size,\n                                                    sampler=weighted_sampler_validation, shuffle=False, num_workers=num_workers)\n    \n    \n#    class_count = [i for i in get_class_distribution(dataset).values()]\n    \n#    print(\"Class count\")\n#    print(class_count)\n    \n#    plt.rcParams[\"figure.figsize\"] = (15, 9)\n#    plt.figure\n#    for images, labels in validation_loader:\n#        for i in range(15):\n#            plt.subplot(3, 5, i+1)\n#            img = F.to_pil_image(images[i])\n#            plt.imshow(img)\n#            plt.gca().set_title('Target: {0}'.format(labels[i]))\n#        plt.show()\n#        break\n        \n    return train_loader, validation_loader, class_weights_train, class_weights_validation\n","eb7012f2":"@dataclass\nclass SystemConfiguration:\n    '''\n    Describes the common system setting needed for reproducible training\n    '''\n    seed: int = 21  # seed number to set the state of all random number generators\n    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)","40f06a54":"@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 16  \n    epochs_count: int = 50  \n    init_learning_rate: float = 1e-4  # initial learning rate for lr scheduler\n    decay_rate: float = 1e-5  \n    log_interval: int = 500  \n    test_interval: int = 1  \n    data_root: str = DATA_ROOT \n    num_workers: int = 8  \n    device: str = 'cuda'  ","f762584e":"    # epoch train\/test loss\n    epoch_train_loss = np.array([])\n    epoch_test_loss = np.array([])\n    \n    # epch train\/test accuracy\n    epoch_train_acc = np.array([])\n    epoch_test_acc = np.array([])","af23a708":"def train(\n    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n    train_loader: torch.utils.data.DataLoader, epoch_idx: int, loss_weights: torch.tensor\n) -> None:\n    \n    # change model in training mood\n    model.train()\n    \n    # to get batch loss\n    batch_loss = np.array([])\n    \n    # to get batch accuracy\n    batch_acc = np.array([])\n    \n    loss_weights = loss_weights.to(train_config.device)\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        \n        # clone target\n        indx_target = target.clone()\n        # send data to device (its is medatory if GPU has to be used)\n        data = data.to(train_config.device)\n        # send target to device\n        target = target.to(train_config.device)\n\n        # reset parameters gradient to zero\n        optimizer.zero_grad()\n        \n        # forward pass to the model\n        output = model(data)\n        \n        # cross entropy loss\n        loss = F.cross_entropy(output, target, weight=loss_weights)\n        \n        # find gradients w.r.t training parameters\n        loss.backward()\n        # Update parameters using gardients\n        optimizer.step()\n        \n        batch_loss = np.append(batch_loss, [loss.item()])\n        \n        # Score to probability using softmax\n        prob = F.softmax(output, dim=1)\n            \n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1]  \n                        \n        # correct prediction\n        correct = pred.cpu().eq(indx_target).sum()\n            \n        # accuracy\n        acc = float(correct) \/ float(len(data))\n        \n        batch_acc = np.append(batch_acc, [acc])\n            \n    epoch_loss = batch_loss.mean()\n    epoch_acc = batch_acc.mean()\n    print('Epoch: {} \\nTrain Loss: {:.6f} Acc: {:.4f}'.format(epoch_idx, epoch_loss, epoch_acc))\n    return epoch_loss, epoch_acc","7a121c9c":"def validate(\n    train_config: TrainingConfiguration,\n    model: nn.Module,\n    test_loader: torch.utils.data.DataLoader,\n    loss_weights: torch.tensor\n) -> float:\n    # \n    model.eval()\n    test_loss = 0\n    count_corect_predictions = 0\n    loss_weights = loss_weights.to(train_config.device)\n    \n    for data, target in test_loader:\n        indx_target = target.clone()\n        \n        \n        \n        data = data.to(train_config.device)\n        \n        target = target.to(train_config.device)\n        \n        output = model(data)\n        # add loss for each mini batch\n        test_loss += F.cross_entropy(output, target, weight=loss_weights).item()\n        \n        # Score to probability using softmax\n        prob = F.softmax(output, dim=1)\n        \n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1] \n        \n        # add correct prediction count\n        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n\n    # average over number of mini-batches\n    test_loss = test_loss \/ len(test_loader)  \n    \n    # average over number of dataset\n    accuracy = 100. * count_corect_predictions \/ len(test_loader.dataset)\n    \n    print(\n        '\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n        )\n    )\n    \n    return test_loss, accuracy\/100.0","a16557fe":"model = models.resnet34(pretrained=True)#models.vgg19(pretrained=True)#models.resnet101(pretrained=True)#models.vgg19(pretrained=True)\nmodel","c5efdb86":"#classifier = nn.Sequential(\n#        nn.Linear(in_features=25088, out_features=4096, bias=True),\n#        nn.ReLU(inplace=True),\n#        nn.Dropout(p=0.4, inplace=False),\n#        nn.Linear(in_features=4096, out_features=2048, bias=True),\n#        nn.ReLU(inplace=True),\n#        nn.Dropout(p=0.4, inplace=False),\n#        nn.Linear(in_features=2048, out_features=1024, bias=True),\n#        nn.ReLU(inplace=True),\n#        nn.Dropout(p=0.4, inplace=False),\n#        nn.Linear(in_features=1024, out_features=13, bias=True)\n#    )\n\nfor name, child in model.named_children():\n   if name in ['layer3', 'layer4']:\n       print(name + ' is unfrozen')\n       for param in child.parameters():\n           param.requires_grad = True\n   else:\n       print(name + ' is frozen')\n       for param in child.parameters():\n           param.requires_grad = False\n\n#classifier_densenet = nn.Sequential(\n#        nn.Linear(in_features=25088, out_features=4096, bias=True),\n#        nn.ReLU(inplace=True),\n#        nn.Dropout(p=0.4, inplace=False),\n#        nn.Linear(in_features=4096, out_features=2048, bias=True),\n#        nn.ReLU(inplace=True),\n#        nn.Dropout(p=0.4, inplace=False),\n#        nn.Linear(in_features=2048, out_features=1024, bias=True),\n#        nn.ReLU(inplace=True),\n#        nn.Dropout(p=0.4, inplace=False),\n#        nn.Linear(in_features=1024, out_features=13, bias=True)\n#    )\n\n#for param in model.parameters():\n#    param.requires_grad = False\n    \nnum_ftrs = model.fc.in_features\n\nprint(\"Num features {}\".format(num_ftrs))\n\n#classifier = nn.Sequential(\n#  nn.Linear(in_features=num_ftrs, out_features=1024),\n#  nn.LeakyReLU(),\n#  nn.Dropout(p=0.2),\n#  nn.Linear(in_features=1024, out_features=512),\n#  nn.LeakyReLU(),\n#  nn.Dropout(p=0.3),\n#  nn.Linear(in_features=512, out_features=13)\n#)\n\nmodel.fc = torch.nn.Linear(num_ftrs, 13)\n#model.fc = classifier\n    \n#model.classifier = classifier\n#model.fc","9179625f":"def save_model(model, device, model_dir='.\/', model_file_name='kenyan13foods34.pt'):\n    \n\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # make sure you transfer the model to cpu.\n    if device == 'cuda':\n        model.to('cpu')\n\n    # save the state_dict\n    torch.save(model.state_dict(), model_path)\n    \n    if device == 'cuda':\n        model.to('cuda')\n    \n    return","53ee1858":"def load_model(model, model_dir='.\/', model_file_name='kenyan13foods34.pt'):\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # loading the model and getting model parameters by using load_state_dict\n    model.load_state_dict(torch.load(model_path))\n    \n    return model","fb3d6cfc":"def setup_system(system_config: SystemConfiguration) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic","18a6df08":"def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors, \n                       loss_legend_loc='upper center', acc_legend_loc='upper left', \n                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n    \n    plt.rcParams[\"figure.figsize\"] = fig_size\n    fig = plt.figure()\n    \n    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n    \n    for i in range(len(train_loss)):\n        x_train = range(len(train_loss[i]))\n        x_val = range(len(val_loss[i]))\n        \n        min_train_loss = train_loss[i].min()\n        \n        min_val_loss = val_loss[i].min()\n        \n        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]), \n                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n        \n    plt.xlabel('epoch no.')\n    plt.ylabel('loss')\n    plt.legend(loc=loss_legend_loc)\n    plt.title('Training and Validation Loss')\n        \n    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n    \n    for i in range(len(train_acc)):\n        x_train = range(len(train_acc[i]))\n        x_val = range(len(val_acc[i]))\n        \n        max_train_acc = train_acc[i].max() \n        \n        max_val_acc = val_acc[i].max() \n        \n        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]), \n                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n        \n    plt.xlabel('epoch no.')\n    plt.ylabel('accuracy')\n    plt.legend(loc=acc_legend_loc)\n    plt.title('Training and Validation Accuracy')\n    \n    plt.show()\n    \n    return   ","454866b0":"def main(model, optimizer, scheduler=None, system_configuration=SystemConfiguration(), \n         training_configuration=TrainingConfiguration()):\n    \n    e_train_loss = np.array([])\n    e_test_loss = np.array([])\n    e_train_acc = np.array([])\n    e_test_acc = np.array([])\n    \n    # system configuration\n    setup_system(system_configuration)\n\n    # batch size\n    batch_size_to_set = training_configuration.batch_size\n    # num_workers\n    num_workers_to_set = training_configuration.num_workers\n    # epochs\n    epoch_num_to_set = training_configuration.epochs_count\n\n    # if GPU is available use training config, \n    # else lowers batch_size, num_workers and epochs count\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n        batch_size_to_set = 16\n        num_workers_to_set = 4\n\n    # data loader\n    train_loader, test_loader, train_weights, validation_weights = get_data(\n        batch_size=batch_size_to_set,\n        data_root=training_configuration.data_root,\n        num_workers=num_workers_to_set,\n        \n    )\n    \n    plt.rcParams[\"figure.figsize\"] = (15, 9)\n    plt.figure\n    for images, labels in test_loader:\n        for i in range(15):\n            plt.subplot(3, 5, i+1)\n            img = transforms.functional.to_pil_image(images[i])\n            plt.imshow(img)\n            plt.gca().set_title('Target: {0}'.format(labels[i]))\n        plt.show()\n        break    \n    \n    # Update training configuration\n    training_configuration = TrainingConfiguration(\n        device=device,\n        batch_size=batch_size_to_set,\n        num_workers=num_workers_to_set\n    )\n        \n    # send model to device (GPU\/CPU)\n    model.to(training_configuration.device)\n\n    best_loss = torch.tensor(np.inf)\n    \n    \n    # Calculate Initial Test Loss\n    init_val_loss, init_val_accuracy = validate(training_configuration, model, test_loader, validation_weights)\n    print(\"Initial Test Loss : {:.6f}, \\nInitial Test Accuracy : {:.3f}%\\n\".format(init_val_loss, \n                                                                                   init_val_accuracy*100))\n    \n    # trainig time measurement\n    t_begin = time.time()\n    for epoch in range(training_configuration.epochs_count):\n        \n        # Train\n        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch, train_weights)\n        \n        print(\"Epoch {}  train loss {}\".format(epoch, train_loss))\n        \n        e_train_loss = np.append(e_train_loss, [train_loss])\n        \n        e_train_acc = np.append(e_train_acc, [train_acc])\n        \n        print(\"Global Epoch train loss len {}\".format(len(e_train_loss)))\n\n        elapsed_time = time.time() - t_begin\n        speed_epoch = elapsed_time \/ (epoch + 1)\n        speed_batch = speed_epoch \/ len(train_loader)\n        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n        \n        print(\n            \"Elapsed {:.2f}s, {:.2f} s\/epoch, {:.2f} s\/batch, ets {:.2f}s\".format(\n                elapsed_time, speed_epoch, speed_batch, eta\n            )\n        )\n\n        # Validate\n        if epoch % training_configuration.test_interval == 0:\n            current_loss, current_accuracy = validate(training_configuration, model, test_loader, validation_weights)\n            \n            e_test_loss = np.append(e_test_loss, [current_loss])\n            \n            print(\"Epoch {}  test loss {}\".format(epoch, current_loss))\n            print(\"Global Epoch test loss len {}\".format(len(e_test_loss)))\n        \n            e_test_acc = np.append(e_test_acc, [current_accuracy])\n            \n            if current_loss < best_loss:\n                best_loss = current_loss\n                print('Model Improved. Saving the Model...\\n')\n                save_model(model, device=training_configuration.device)\n        \n        if scheduler is not None:\n#            scheduler.step(current_loss)\n            scheduler.step()\n        \n#    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_, loss))\n    print(\"before exit losses len {} {} {} {}\".format(len(e_train_loss), len(e_test_loss), len(e_train_acc), len(e_test_acc)))\n    \n    return model, train_loader, test_loader, e_train_loss, e_test_loss, e_train_acc, e_test_acc","43ceed00":"def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors, \n                       loss_legend_loc='upper center', acc_legend_loc='upper left', \n                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n    \n    plt.rcParams[\"figure.figsize\"] = fig_size\n    fig = plt.figure()\n    \n    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n    \n    for i in range(len(train_loss)):\n        x_train = range(len(train_loss[i]))\n        x_val = range(len(val_loss[i]))\n        \n        min_train_loss = train_loss[i].min()\n        \n        min_val_loss = val_loss[i].min()\n        \n        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]), \n                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n        \n    plt.xlabel('epoch no.')\n    plt.ylabel('loss')\n    plt.legend(loc=loss_legend_loc)\n    plt.title('Training and Validation Loss')\n        \n    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n    \n    for i in range(len(train_acc)):\n        x_train = range(len(train_acc[i]))\n        x_val = range(len(val_acc[i]))\n        \n        max_train_acc = train_acc[i].max() \n        \n        max_val_acc = val_acc[i].max() \n        \n        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]), \n                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n        \n    plt.xlabel('epoch no.')\n    plt.ylabel('accuracy')\n    plt.legend(loc=acc_legend_loc)\n    plt.title('Training and Validation Accuracy')\n    \n    fig.savefig('sample_loss_acc_plot.png')\n    plt.show()\n    \n    return   ","e1b113ad":"train_config = TrainingConfiguration()\n\n#optimizer = optim.SGD(\n#        model.parameters(),\n#        lr=train_config.init_learning_rate\n#)\n\n#optimizer = optim.Adam(\n#    model.parameters(),\n#    lr = train_config.init_learning_rate,\n#    weight_decay=train_config.decay_rate\n#)\n\n\n#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)\n#decay_rate = train_config.decay_rate\n\n#lmbda = lambda epoch: 1\/(1 + decay_rate * epoch)\n\n    # Scheduler\n#scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)\n\n#optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\noptimizer = optim.SGD(filter(lambda x: x.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","19a66521":"import time\nmodel, train_loader, test_loader, epoch_train_loss, epoch_test_loss, epoch_train_acc, epoch_test_acc = main(model, optimizer, scheduler=scheduler)","641b3a5f":"print(\"train acc len {}\".format(len(epoch_train_acc)))\nprint(\"test acc len {}\".format(len(epoch_test_acc)))\nprint(\"train loss len {}\".format(len(epoch_train_loss)))\nprint(\"test loss len {}\".format(len(epoch_test_loss)))\n\n      \nplot_loss_accuracy(train_loss=[epoch_train_loss], \n                   val_loss=[epoch_test_loss], \n                   train_acc=[epoch_train_acc], \n                   val_acc=[epoch_test_acc], \n                   colors=['blue'], \n                   loss_legend_loc='upper center', \n                   acc_legend_loc='upper left')","d2b4cc49":"save_model(model, 'cuda')","c671db2b":"def prediction(model, device, batch_input):\n    \n    # send model to cpu\/cuda according to your system configuration\n    model.to(device)\n    \n    # it is important to do model.eval() before prediction\n    model.eval()\n\n    data = batch_input.to(device)\n\n    output = model(data)\n\n    # Score to probability using softmax\n    prob = F.softmax(output, dim=1)\n\n    # get the max probability\n    pred_prob = prob.data.max(dim=1)[0]\n    \n    # get the index of the max probability\n    pred_index = prob.data.max(dim=1)[1]\n    \n    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()","8cbabc93":"class KenyanFood13DatasetTest(Dataset):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, data_root, transform=None):\n        image_csv_path = os.path.join(data_root, 'test.csv')\n        self.image_df = pd.read_csv(image_csv_path)\n        print(\"Number test samples {}\".format(len(self.image_df)))\n        \n        # set transform attribute\n        self.transform = transform\n        \n        self.data_dict = {\n            'image_path': [],\n            'img_name': []\n        }\n        \n        img_dir = os.path.join(data_root, 'images', 'images')\n\n        for index, row in self.image_df.iterrows():\n            img_path = os.path.join(img_dir, '{}.jpg'.format(row[0]))\n            self.data_dict['image_path'].append(img_path)\n            self.data_dict['img_name'].append(row[0])\n\n    def __getitem__(self, idx):\n        \"\"\"\n        For given index, return images with resize and preprocessing.\n        \"\"\"\n        \n        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        img_name = self.data_dict['img_name'][idx]\n            \n        return image, img_name\n    \n    def __len__(self):\n        return len(self.data_dict['img_name'])","47b9100a":"image_csv_path = os.path.join(data_root, 'test.csv')\nimage_df = pd.read_csv(image_csv_path)\nprint(\"Number test samples {}\".format(len(self.image_df)))","0d967f7f":"test_dataset =  KenyanFood13DatasetTest(DATA_ROOT, transform=image_common_transforms())\ndata_len = test_dataset.__len__()\nprint(\"Test data length is {}\".format(data_len))","6008eb42":"model = load_model(model)\n\nclasses_labels = {}\n\nreader = csv.reader(open('.\/labels.csv'))\n\n\nfor row in reader:\n    key = row[0]\n    if key in classes_labels:\n        # implement your duplicate row handling here\n        pass\n    classes_labels[int(key)] = row[1]\nprint(classes_labels)\n\nsubmit_data = {}\n\ninputs = []\n\nfor i in range(data_len):\n    img, img_name = test_dataset.__getitem__(i)\n    inputs.append(img)\n    \n    inputs = torch.stack(inputs)\n    \n    cls, prob = prediction(model, 'cuda', batch_input=inputs)\n    submit_data[img_name] = classes_labels[cls]\n    inputs.clear()\n    \n    \nwith open('.\/submission.csv', 'w') as f:\n    for key in submit_data.keys():\n        f.write(\"%s,%s\\n\"%(key,submit_data[key]))\n        \nprint(submit_data)","36cb163f":"**Load Model**","3751c8de":"**Plot loss accuracy**","bf836556":"**6. Utils**","fe7a895b":"****5. Model****","ffc0482e":"**Check class balance**","93f2b685":"**Main training function**","8a8df502":"**1. Data Loader**","c574f3f1":"**Generate Sample Submission**","e04a8da7":"**3. Evaluation Metrics**","f868265b":"**2. Configuration**","6193372b":"**Plot train and validation accuracy**","604d7636":"**4. Train and Validation functions**","15126bc7":"**Save Model**","901afdd4":"**Some classes have a lot fewer images than the average. We wil have to generate synthetic data for these. Find out the classes that are 20% below the average (number of images)**","7f99ed06":"**Setup System**"}}