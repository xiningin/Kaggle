{"cell_type":{"aaabd67b":"code","dbca1e9b":"code","31e09347":"code","5270beda":"code","8401f1d4":"code","b2536650":"code","f411a168":"code","73a31c1b":"code","efbd90a5":"code","594e8c93":"code","27c88c0c":"code","7a29c006":"code","f63abef2":"code","f9cb5be0":"code","9774e058":"code","a022d2b0":"code","135627d0":"code","66bf253b":"code","ca776551":"code","8f7937ee":"code","0b5995bc":"code","59afabdd":"code","091cc35f":"code","c89f2e80":"code","350a7527":"code","e97d8a12":"code","696eeade":"code","fa8c12bf":"code","20386748":"code","ee960269":"code","a7e0b1e9":"code","9fbbffdc":"code","fb11c579":"code","cbea8415":"code","de60a025":"code","e0a1b9fc":"markdown","35d7fd1d":"markdown","2029b8c3":"markdown","7afaa13a":"markdown","b9a6c084":"markdown","3fde020e":"markdown","f41a9427":"markdown","5f4908ef":"markdown","19d375f4":"markdown","86972873":"markdown","7d391769":"markdown","8355fe5f":"markdown","900213da":"markdown","c40550b3":"markdown","11c04182":"markdown","d0b6c869":"markdown","328c97d8":"markdown"},"source":{"aaabd67b":"import os\nimport warnings\nfrom pathlib import Path","dbca1e9b":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype","31e09347":"from category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor","5270beda":"warnings.filterwarnings('ignore')","8401f1d4":"def load_data():\n    '''\n    This function combines 4 data preprocessing steps:\n    1. We first load the data from the CSV files;\n    2. Then, we clean the dataframe;\n    3. Next, we encode the statistical data type (numeric, categorical);\n    4. Lastly, we impute missing values.\n    \n    Returns:\n    The processed train and test dataframes.\n    '''\n    # Read data\n    data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","b2536650":"data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\ndf = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")","f411a168":"#Problem column 1\nprint('Unique values in the Exterior2nd column: \\n')\nprint(df.Exterior2nd.unique())\n\n#Problem column 2\nprint('\\nUnique values in the BldgType column: \\n')\nprint(df.BldgType.unique())\n\n#Problem column 3\nprint('\\nUnique values in the Neighborhood column: \\n')\nprint(df.Neighborhood.unique())","73a31c1b":"def clean(df):\n    '''\n    We clean the data belonging to the beforementioned categorical\n    features. Additionally, we replace some values of the column GarageYrBlt\n    and rename column names.\n    \n    Parameters:\n    df: A dataframe\n    \n    Returns:\n    The cleaned dataframe.\n    '''\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\", \n                                                   \"CmentBd\": 'CemntBd','Wd Shng': 'WdShing'})\n    df['Neighborhood'] = df.Neighborhood.replace({'NAmes': 'Names'})\n    df['BldgType'] = df.BldgType.replace({'2fmCon': '2FmCon', 'Twnhs': 'TwnhsI'})\n    \n    # Some values of GarageYrBlt are corrupt, so replace them with the year the house was built\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    \n    # Names beginning with numbers are awkward to work with\n    df.rename(columns={\"1stFlrSF\": \"FirstFlrSF\",\"2ndFlrSF\": \"SecondFlrSF\",\n                       \"3SsnPorch\": \"Threeseasonporch\"}, inplace=True)\n    \n    return df","efbd90a5":"# The numeric features are already encoded correctly (float for\n# continuous, int for discrete), but the categoricals we'll need to\n# do ourselves. In particular, the MSSubClass feature is read as an `int` type, \n# but is actually a (nominative) categorical.\n\n# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \n                \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \n                \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \n                \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \n                \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\ndef encode(df):\n    '''\n    This function encodes features with its correct datatype.\n    Specifically, we encode nominal and ordinal categorical features.\n    \n    Parameters:\n    df: A dataframe\n    \n    Returns:\n    The encoded dataframe.\n    '''\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels, ordered=True))\n    return df","594e8c93":"def impute(df):\n    '''\n    This function imputes missing values.\n    \n    First, we get a list of all the columns with missing values and\n    separate them into 2 lists: one having numerical features and\n    the other with categorical features. We also get the columns with\n    no missing values.\n    \n    Then, we create indicator columns with a 0 or 1 to indicate that\n    the new value (0 for numerical columns and None for the rest) is\n    created by the imputation process.\n    \n    Parameters:\n    df: A dataframe\n    \n    Returns:\n    The imputed dataframe.\n    '''\n    miss_value_cols = df.columns[df.isnull().any()].tolist()\n    miss_value_cols_dtype = [(col, df.dtypes[col]) for col in miss_value_cols]\n    \n    miss_value_cols_num = [i for i, j in miss_value_cols_dtype if j == 'float64' or j == 'int64']\n    miss_value_cols_categ = list(set(miss_value_cols) - set(miss_value_cols_num))\n    non_miss_value_cols = list(set(list(df)) - set(miss_value_cols))\n    \n    df_temp = df[miss_value_cols].isnull().astype(int).add_suffix('_indicator')\n    cols1 = non_miss_value_cols + [item for x in miss_value_cols for item in (x, x + '_indicator')]\n    \n    df.fillna({x: 0 for x in miss_value_cols_num}, inplace = True)\n    df.fillna({y: 'None' for y in miss_value_cols_categ}, inplace = True)\n    \n    df_new = pd.concat([df, df_temp], axis = 1).reindex(cols1, axis = 'columns')\n    return df_new","27c88c0c":"df_train, df_test = load_data()","7a29c006":"def score_dataset(X, y, model=XGBRegressor()):\n    '''\n    This function establishes a baseline score to judge our\n    feature engineering against.\n    \n    After label encoding for categoricals, we compute the\n    cross-validated RMSLE score using XGBoost.\n    \n    Parameters:\n    X: The input variables\n    y: The target variable\n    model: Model for scoring\n    \n    Returns:\n    The RMSLE Score.\n    '''\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    log_y = np.log(y)\n    score = cross_val_score(model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\")\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\nX = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","f63abef2":"sns.set_style('whitegrid')\npd.set_option('display.max_columns', 120)\npd.set_option('display.max_rows', 70)","f9cb5be0":"df_train.head()","9774e058":"all_cols = df_train.columns.tolist()\nall_cols_dtype = [(col, df_train.dtypes[col]) for col in all_cols]\nall_cols_num = [i for i, j in all_cols_dtype if j == 'float64' or j == 'int64']\nall_cols_categ = list(set(all_cols) -  set(all_cols_num))\n\nprint('Total number of columns in df_train:', len(all_cols))\nprint('\\nNumber of numeric columns in df_train:', len(all_cols_num))\nprint('\\nNumber of categorical columns in df_train:', len(all_cols_categ))","a022d2b0":"# Log transformation for the target variable\nfig, ax= plt.subplots(1, 2, figsize = (10, 5))\nprint(\"Before -  Mean:\", df_train.SalePrice.mean(), \"Standard Deviation:\", df_train.SalePrice.std())\nsns.distplot(df_train['SalePrice'], ax = ax[0]).set_title('Before Log Transform')\nSalePrice_log = np.log1p(df_train.SalePrice)\nprint(\"After - Mean:\", SalePrice_log.mean(), \"Standard Deviation:\", SalePrice_log.std())\nsns.distplot(SalePrice_log, ax = ax[1]).set_title('After Log transform')\nplt.show()","135627d0":"corr = df_train[all_cols_num].corr()['SalePrice'][:]\ncorr.sort_values(ascending = False)","66bf253b":"#Correlation heatmap\nplt.figure(figsize = (20, 10))\n#Use triu() to isolate upper triangle of matrix \n#while turning all values in the lower triangle to 0\nmask = np.triu(np.ones_like(df_train[all_cols_num].corr(), dtype = np.bool))\n\nheatmap = sns.heatmap(df_train[all_cols_num].corr(), vmin = -1, \n                      vmax = 1, cmap = 'BrBG', mask = mask)\n\nheatmap.set_title('Correlation Heatmap', fontdict = {'fontsize': 18}, pad = 12)","ca776551":"def make_mi_scores(X, y):\n    '''\n    This function creates the mutual information scores.\n    \n    Parameters:\n    X: The input variables\n    y: The target variable\n    \n    Returns:\n    The mutual information scores\n    '''\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","8f7937ee":"def plot_mi_scores(scores):\n    '''\n    Plot a horizontal barplot of the mutual information scores\n    obtained from the make_mi_scores function.\n    \n    Parameters:\n    scores: The mutual information scores\n    '''\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    for i, v in enumerate(scores):\n        plt.text(v, i, \" \"+str(round(v, 3)), color='blue', va='center')","0b5995bc":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nmi_scores = make_mi_scores(X, y)\nmi_scores","59afabdd":"plt.figure(dpi = 100, figsize = (8, 18))\nplot_mi_scores(mi_scores)","091cc35f":"def drop_uninformative(df, mi_scores):\n    '''\n    Drop uninformative features, i.e., features with mutual\n    information score <= 0.001 from the dataset.\n    \n    Parameters:\n    df: A dataframe.\n    mi_scores: The mutual informative scores.\n    \n    Returns:\n    The modified dataframe.\n    '''\n    return df.loc[:, mi_scores > 0.001]","c89f2e80":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\nX = drop_uninformative(X, mi_scores)\n\nscore_dataset(X, y)","350a7527":"X.shape #We now have 85 columns","e97d8a12":"def label_encode(df):\n    '''\n    This function performs label encoding on the categorical features.\n    \n    Parameters:\n    df: a dataframe.\n    \n    Returns:\n    X: The modified dataframe.\n    '''\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X","696eeade":"def mathematical_transforms(df):\n    '''\n    This function develops new features through mathematical\n    transforms of existing features.\n    \n    Parameters:\n    df: A dataframe\n    \n    Returns:\n    X: Dataframe holding new features.\n    '''\n    X = pd.DataFrame()  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea \/ df.LotArea\n    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) \/ df.TotRmsAbvGrd\n    X[\"TotalArea1st2nd\"] = df.FirstFlrSF + df.SecondFlrSF\n    \n    return X\n\ndef interactions(df):\n    '''\n    This function develops new features exploring interactions between\n    a categorical feature and a numeric feature that expresses the same \n    thing.\n    \n    Parameters:\n    df: A dataframe\n    \n    Returns:\n    X: Dataframe holding new features\n    '''\n    X_1 = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X_1 = X_1.mul(df.GrLivArea, axis=0)\n    \n    X_2 = pd.get_dummies(df.BsmtQual, prefix='Bsmt')\n    X_2 = X_2.mul(df.TotalBsmtSF, axis=0)\n    \n    X_3 = pd.get_dummies(df.BsmtCond, prefix = 'Base')\n    X_3 = X_3.mul(df.TotalBsmtSF, axis=0)\n    \n    X_4 = pd.get_dummies(df.GarageQual, prefix = 'Garg')\n    X_4 = X_4.mul(df.GarageArea, axis=0)\n    \n    X_5 = pd.get_dummies(df.GarageCond, prefix = 'Gar')\n    X_5 = X_5.mul(df.GarageArea, axis=0)\n    \n    X_6 = pd.get_dummies(df.FireplaceQu, prefix = 'Fire')\n    X_6 = X_6.mul(df.Fireplaces, axis=0)\n    \n    X = pd.concat([X_1, X_2, X_3, X_4, X_5, X_6], axis = 1)\n    return X\n\ndef counts(df):\n    '''\n    Aggregate features describing the presence (or) absence of something.\n    \n    Parameters:\n    df: A dataframe\n    \n    Returns:\n    X: Dataframe holding new features\n    '''\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[\n        [\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"Threeseasonporch\",\"ScreenPorch\"]\n    ].gt(0.0).sum(axis=1)\n    return X\n\ndef group_transforms(df):\n    '''\n    Aggregate information across multiple rows grouped by some category.\n    \n    Parameters:\n    df: A dataframe\n    \n    Returns:\n    X: Dataframe holding new features\n    '''\n    X = pd.DataFrame()\n    X[\"MeanNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"mean\")\n    X['HouseComp'] = df['GrLivArea'] - X['MeanNhbdArea']\n    return X","fa8c12bf":"cluster_features = [\"LotArea\",\"TotalBsmtSF\",\"FirstFlrSF\",\"SecondFlrSF\",\"GrLivArea\"]\n\ndef cluster_labels(df, features, n_clusters=10):\n    '''\n    This function clusters datapoints based on the set of `features`.\n    \n    Parameters:\n    df: A dataframe\n    features: Set of features to cluster on\n    n_clusters (default 10): Number of clusters to be generated\n    \n    Returns:\n    X_new: Dataframe holding the cluster labels.\n    '''\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new","20386748":"def apply_pca(X, standardize=True):\n    '''\n    This function applies the PCA algorithm giving us loadings which describes \n    each component of variation, and also the components which are the \n    transformed datapoints.\n    \n    Parameters:\n    X: The input variables\n    standardize (default True): To bring the features on the same scale\n    \n    Returns:\n    pca: pca\n    X_pca: Principal components\n    loadings: The loadings\n    '''\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings","ee960269":"def pca_inspired(df):\n    '''\n    This function creates new features based on observations\n    from the apply_pca function.\n    \n    Parameters:\n    df: A dataframe\n    \n    Returns:\n    X: Dataframe holding new features\n    '''\n    X = pd.DataFrame()\n    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n    \n    return X\n\ndef pca_components(df, features):\n    '''\n    This function uses user-inputted features to create principal\n    components.\n    \n    Parameters:\n    df: A dataframe\n    features: Set of features on which to apply PCA\n    \n    Returns:\n    X_pca: The principal components\n    '''\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\npca_features = [\"GarageArea\",\"YearRemodAdd\",\"TotalBsmtSF\",\"GrLivArea\"]","a7e0b1e9":"class CrossFoldEncoder:\n    '''\n    This is a class to perform target encoding.\n    \n    This function performs target encoding without having to use \n    held-out encoding data. It's basically the same trick used in \n    cross-validation:\n    1. Split the data into folds, each fold having two splits of the dataset.\n    2. Train the encoder on one split but transform the values of the other.\n    3. Repeat for all the splits.\n    \n    This way, training and transformation always take place on independent sets \n    of data, just like when you use a holdout set but without any data going to waste.\n    '''\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    def fit_transform(self, X, y, cols):\n        '''\n        We fit an encoder on one split and transform the feature on the other.\n        Iterating over the splits in all folds gives a complete transformation.\n        We also now have a trained encoder on each fold.\n        '''\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    def transform(self, X):\n        '''\n        This function helps us average the encodings learned from each fold.\n        This transforms the test data.\n        '''\n        from functools import reduce\n\n        X_encoded_list = []  \n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","9fbbffdc":"def create_features(df, df_test=None):\n    '''\n    This function takes a prepared dataframe and passes it through a \n    pipeline of transformations to get the final feature set.\n    \n    Parameters:\n    df: A dataframe\n    df_test (default None): Dataframe holding test data\n    \n    Returns:\n    X: Processed dataframe\n    X_test: returned if df_test is not None\n    '''\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Mutual Information\n    X = drop_uninformative(X, mi_scores)\n\n    # Transformations\n    X = X.join(mathematical_transforms(X))\n    X = X.join(interactions(X))\n    X = X.join(counts(X))\n    X = X.join(group_transforms(X))\n\n    # Clustering\n    #X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    \n    # PCA\n    X = X.join(pca_inspired(X))\n    #X = X.join(pca_components(X, pca_features))\n\n    X = label_encode(X)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\ndf_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nscore_dataset(X_train, y_train)","fb11c579":"#import optuna\n\n#def objective(trial):\n    #xgb_params = dict(\n        #max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        #learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        #n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        #min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        #colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        #subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        #reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        #reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    #)\n    #xgb = XGBRegressor(**xgb_params)\n    #return score_dataset(X_train, y_train, xgb)\n\n#study = optuna.create_study(direction=\"minimize\")\n#study.optimize(objective, n_trials=20)\n#xgb_params = study.best_params","cbea8415":"X_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb_params = dict(\n    max_depth=7,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=7500,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=2,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.4,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.5,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.9,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=0.6,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)\nscore_dataset(X_train, y_train, xgb)","de60a025":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train, np.log(y))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","e0a1b9fc":"#### Baseline score","35d7fd1d":"### Imputing missing values","2029b8c3":"<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\nWe create <strong>missing value indicators<\/strong>: 1 whenever a value was imputed and 0 otherwise.\n<\/blockquote>","7afaa13a":"Question (Rakibul Ranak): While getting the baseline score, X was encoded maintaining categorical order. But while getting the MI score, X was just factorized, meaning that categorical ordering was not maintained here. Why?\n\nAnswer (Ryan Holbrook): Mutual information doesn't take into account ordering of categories, so in this case the easiest thing to do is just factorize since that way you don't need to have coded your features as `category` type -- it would work for anything non-numeric. The baseline model does account for order though; when fitting the baseline model, I knew that I had coded the features as categorical so it was safe to grab the underlying (ordered) codes with `cat.codes`.","b9a6c084":"### Cleaning the data","3fde020e":"## Data Preprocessing","f41a9427":"Let us now combine the previous steps.","5f4908ef":"# Feature utility scores","19d375f4":"### Loading the data","86972873":"**Disclaimer:** This notebook builds on Ryan Holbrook's Feature Engineering for House prices notebook (available on [Kaggle Learn](https:\/\/www.kaggle.com\/ryanholbrook\/feature-engineering-for-house-prices)).","7d391769":"#### Target variable - SalePrice","8355fe5f":"### Encoding data","900213da":"The below columns (categorical features) have typos in their categories:","c40550b3":"For more, see [here](http:\/\/www.stackoverflow.com\/questions\/45121899\/impute-missing-values-to-0-and-create-indicator-columns-in-pandas).","11c04182":"### Creating features with Pandas","d0b6c869":"# Create Features","328c97d8":"### Exploratory data analysis"}}