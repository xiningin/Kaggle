{"cell_type":{"109bc592":"code","acedbdff":"code","d6dcd11d":"code","10159290":"code","e9b8a506":"code","b6e66de4":"code","a34ee78f":"code","6322ba1c":"markdown"},"source":{"109bc592":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nimport random \n%matplotlib inline","acedbdff":"x, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=1,\n                             n_clusters_per_class=1, random_state=14)\n\nsns.scatterplot(x[:,0],x[:,1],hue=y.reshape(-1));\n\nx_test=x[:500]\ny_test=y[:500]\nx=x[500:]\ny=y[500:]","d6dcd11d":"class decision_tree:\n    def __init__(self,impurity=\"entropy\",n_features=1):\n        self.impurity=impurity\n        self.n_features=n_features\n        \n    class node:\n        def __init__(self,isleaf=False):\n            self.isleaf=isleaf\n            \n    def __entropy(self,df):\n        sum=0.\n        n=df.shape[0]\n        for i in df['o\/p'].value_counts():\n            sum-=np.log(i\/n)\n        return sum\n    \n    def __gini_impurity(self,df):\n        sum=0.\n        n=df.shape[0]\n        for i in df['o\/p'].value_counts():\n            sum+=(i\/n)**2\n        return 1-sum\n    \n    def __information_gain(self,df,df1,df2):\n        if(df1.shape[0]==0 or df2.shape[0]==0):\n            return float(\"-inf\")\n        s=df.shape[0]\n        sv1=df1.shape[0]\n        sv2=df2.shape[0]\n        h=0.\n        h1=0.\n        h2=0.\n        if(self.impurity=='entropy'):\n            h=self.__entropy(df)\n            h1=self.__entropy(df1)\n            h2=self.__entropy(df2)\n        else:\n            h=self.__gini_impurity(df)\n            h1=self.__gini_impurity(df1)\n            h2=self.__gini_impurity(df1)\n        ig=h-sv1*h1\/s-sv2*h2\/s\n        return ig\n    \n    def __fit_util(self,df,depth):\n        impurity=0\n        if(self.impurity=='entropy'):\n            impurity=self.__entropy(df)\n        else:\n            impurity=self.__gini_impurity(df)\n        if(impurity<=self.threshold_impurity or depth==self.max_depth):\n            n=self.node(isleaf=True)\n            n.classification=df['o\/p'].mode()[0]\n            return n\n        max_ig=float('-inf')\n        max_feature=df.columns[0]\n        max_division=0\n        features=list(df.columns)\n        features.remove('o\/p')\n        for feature in random.sample(features,self.n_features):\n            for division in df[feature].unique():\n                df1=df[df[feature]<=division]\n                df2=df[df[feature]>division]\n                ig=self.__information_gain(df,df1,df2)\n                if(ig>max_ig):\n                    max_ig=ig\n                    max_feature=feature\n                    max_division=division\n        df1=df[df[max_feature]<=max_division]\n        df2=df[df[max_feature]>max_division]\n        n=self.node()\n        n.feature=max_feature\n        n.division=max_division\n        n.left=self.__fit_util(df1,depth+1)\n        n.right=self.__fit_util(df2,depth+1)\n        return n\n        \n    def fit(self,x,y,threshold_impurity=0.1,max_depth=100,random_picking=False):\n        assert x.shape[0]==y.shape[0],\"Unequal length of Dataframes\"\n        df=pd.DataFrame(x)\n        df['o\/p']=pd.DataFrame(y)\n        self.n=x.shape[0]\n        self.max_depth=max_depth\n        self.threshold_impurity=threshold_impurity\n        if(random_picking):\n            df=df.sample(self.n,replace=True,axis=0)\n        self.tree=self.__fit_util(df,0)\n        \n    def __predict_util(self,n,df):\n        if(n.isleaf):\n            return n.classification\n        if(df[n.feature]<=n.division):\n            return self.__predict_util(n.left,df)\n        else:\n            return self.__predict_util(n.right,df)\n    def predict(self,x):\n        df=pd.DataFrame(x)\n        n=df.shape[0]\n        y=[]\n        for i in range(n):\n            y.append(self.__predict_util(self.tree,df.iloc[i,:]))\n        return np.array(y)\n            ","10159290":"model=decision_tree()\nmodel.fit(x,y,threshold_impurity=0.1,max_depth=20)","e9b8a506":"prediction=model.predict(x_test)","b6e66de4":"print((len(y_test)-sum(abs(prediction-y_test)))\/len(y_test))","a34ee78f":"one1=[]\nzero1=[]\none2=[]\nzero2=[]\nfor i in np.arange(-3,2,0.2):\n    for j in np.arange(-3,4,0.2):\n        l=model.predict(np.array([[i,j]]))[0]\n        if(l==1):\n            one1.append(i)\n            one2.append(j)\n        else:\n            zero1.append(i)\n            zero2.append(j)\nplt.scatter(np.array(one1),np.array(one2))\nplt.scatter(np.array(zero1),np.array(zero2))","6322ba1c":"### Decision Tree\n- Decision Tree is a non linear classifier\n- At each node, data is classified based on a particular value of a particular feature\n- Purity of split at each node is measured using entropy or gini impurity\n- Information Gain is a value which indicates the relative purity of data before and after the split\n- The split is made such that the information gain is maximum\n- Decision tree is prone to overfitting but the performance can be highly improved using techniques like bagging and boosting."}}