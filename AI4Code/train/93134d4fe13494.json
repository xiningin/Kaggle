{"cell_type":{"6b10af65":"code","228b628d":"code","0245a03d":"code","ff7fdab1":"code","7f9d26fb":"code","efbafe14":"code","a28b2177":"code","3b7c8f7a":"code","deedffdf":"code","182f085b":"code","66e00092":"code","45bb1977":"code","94096a25":"code","9056fe63":"code","23461e19":"code","1daf44be":"code","982b0b30":"code","0a5bbb74":"code","958a6efa":"code","c7208b5f":"code","f6e57e1e":"code","88dc22c2":"code","4bb3c8e6":"code","ebe086f7":"markdown","5e42a23b":"markdown","0a6d0973":"markdown","bac36f53":"markdown","b80ec75e":"markdown","2b841eaa":"markdown","afb96827":"markdown","9c7e1e81":"markdown","674db4fa":"markdown","65045ac2":"markdown","82c3812a":"markdown"},"source":{"6b10af65":"import os\n\nprint(\"os.walk in part of \/kaggle\/input\/\")\n\ndef walk_kaggle_input(dir):\n    for dirname, _, filenames in os.walk(f\"\/kaggle\/input\/{dir}\/output\"):\n        for filename in filenames[0:10]:\n            print(os.path.join(dirname, filename))\n\nwalk_kaggle_input(\"seti-signal-search-cnn-18\")\nwalk_kaggle_input(\"seti-signal-search-cnn-19\")","228b628d":"import sys\nsys.path.append('\/kaggle\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport timm\nprint(timm.__version__)\n\nimport os\nimport datetime as dt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib.ticker as ticker\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast\nfrom torch.optim import Adam\n\nimport cv2\nimport albumentations as A\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","0245a03d":"BASE_DIR = '\/kaggle\/input\/seti-breakthrough-listen'\n\ndef get_file_path(image_id, category):\n    return f\"{BASE_DIR}\/{category}\/{image_id[0]}\/{image_id}.npy\"\n\ndef get_train_file_path(image_id):\n    return get_file_path(image_id, \"train\")\n\ndef get_test_file_path(image_id):\n    return get_file_path(image_id, \"test\")\n","ff7fdab1":"train = pd.read_csv(f\"{BASE_DIR}\/train_labels.csv\")\n\ntrain['img_path'] = train['id'].apply(get_train_file_path)\n\ndisplay(train.head(1))\nprint(train.head(1)['img_path'].values)\n\ndisplay(train['target'].value_counts())","7f9d26fb":"test = pd.read_csv(f\"{BASE_DIR}\/sample_submission.csv\")\n\ntest['img_path'] = test['id'].apply(get_test_file_path)\n\ndisplay(test.head(1))\nprint(test.head(1)['img_path'].values)\n\ndisplay(test['target'].value_counts())","efbafe14":"class CFG:\n    debug = False\n\n    epochs = 6\n    \n    model_name = 'tf_efficientnet_b0' # pretrained b0, b1, b2, b3 increasing size\n    model_size = 224\n    test_model_size = 224\n\n    model_name = 'tf_efficientnetv2_b0'\n    # input_size=(3, 192, 192), test_input_size=(3, 224, 224), pool_size=(6, 6)\n    model_size = 192\n    test_model_size = 224\n\n    model_name = 'tf_efficientnetv2_b1'\n    # input_size=(3, 192, 192), test_input_size=(3, 240, 240), pool_size=(6, 6)\n    model_size = 192\n    test_model_size = 240\n    \n    batch_size = 64\n    inference_batch_size = 64\n    num_workers = 8\n    \n    criterion = nn.BCEWithLogitsLoss()\n    \n    seed = 45\n    \n    N_FOLDS = 5\n    p_horizontal_flip = 0.30\n    \n    lr = 5e-5\n\nif CFG.debug:\n    print('debug!')\n    CFG.epochs = 1\n    CFG.N_FOLDS = 4\n    CFG.batch_size = 8\n    CFG.inference_batch_size = 16\n    CFG.num_workers = 4\n\n    train = train.sample(n=193, random_state=CFG.seed).reset_index(drop=True)\n    test = test.head(153)\n","a28b2177":"# Make output dir\nOUTPUT_DIR = '.\/output\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","3b7c8f7a":"ttransform = A.Compose([\n    A.RandomCrop(height=1638, width=250), # cut-off random 6 horizontally\n    A.Resize(CFG.model_size, CFG.model_size, cv2.INTER_NEAREST),\n    A.HorizontalFlip(p=CFG.p_horizontal_flip),\n])\nvtransform = A.Compose([\n    A.Resize(CFG.test_model_size, CFG.test_model_size, cv2.INTER_NEAREST)\n])","deedffdf":"class ClassificationDataset:\n    \n    def __init__(self, img_paths, targets, tr): \n        self.img_paths = img_paths\n        self.targets = targets\n        self.tr = tr\n\n    def __len__(self):\n        return len(self.img_paths)\n    \n    def __getitem__(self, item):\n        img_path = self.img_paths[item]\n        image = np.load(img_path)\n        image = np.vstack(image).astype(float)\n        image = self.tr(image = image)[\"image\"][np.newaxis, ]\n        \n        target = self.targets[item]\n                \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"target\": torch.tensor(target, dtype=torch.float),\n            \"img_id\": img_path.split('\/')[-1].split('.')[0]\n        }","182f085b":"# Preview 5 training images via the ClassificationDataset\nX = train.img_path.values\ny = train.target.values\n\nsample_size = 5\ntrain_index = 130 # some random image\ntrain_images = X[train_index:train_index+sample_size]\ntrain_targets = y[train_index:train_index+sample_size]\n\n# Validation transformation (this Notebook is about analysis, not training)\ntrain_dataset = ClassificationDataset(img_paths=train_images, targets=train_targets, tr=ttransform)\n\nfor i in range(sample_size):\n    image_target = train_dataset[i]\n    image, target = image_target['image'], image_target['target']\n    # transpose back from torch format to imshow format\n    plt.imshow(image.numpy().transpose((1, 2, 0))[:,:,0]) # only 1 axis\n    plt.title(f'target: {target}')\n    plt.show()\nimage.shape","66e00092":"# Preview 2 test images via the ClassificationDataset\nX = test.img_path.values\ny = test.target.values\n\nsample_size = 2\ntest_index = 27 # some random image\ntest_images = X[test_index:test_index+sample_size]\ntest_targets = y[test_index:test_index+sample_size]\n\ntest_dataset = ClassificationDataset(img_paths=test_images, targets=test_targets, tr=vtransform, ) # vtransform !\n\nfor i in range(sample_size):\n    image_target = test_dataset[i]\n    image, target = image_target['image'], image_target['target']\n    # transpose back from torch format to imshow format\n    plt.imshow(image.numpy().transpose((1, 2, 0))[:,:,0]) # only 1 axis\n    plt.title(f'target: {target}')\n    plt.show()\nimage.shape","45bb1977":"class timmv2(nn.Module):\n    def __init__(self, model_name, pretrained):\n        super().__init__()\n        \n        # Existing EfficientNet fixed at 3 channels\n        self.enet = timm.create_model(model_name, pretrained=pretrained, in_chans=3)\n        \n        # Added a trainable 1 to 3 conv1 layer before\n        self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=True)\n        \n        # set the output classifier to 1 feature\n        nb_ft = self.enet.classifier.in_features\n        self.enet.classifier = nn.Linear(nb_ft, 1)\n\n    @autocast()\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.enet(x)\n        \n        return x","94096a25":"def model_make(model_name):\n    model = timmv2(model_name, True) # Start from pre-trained\n    state_dict = {\n        'weight':torch.tensor(\n            [[[\n                [ -0.03, 0.1,  -0.03],\n                [ -0.03, 0.1,  -0.03],\n                [ -0.03, 0.1,  -0.03],\n            ]],[[\n                [ -0.03, 0.1,  -0.03],\n                [ -0.03, 0.1,  -0.03],\n                [ -0.03, 0.1,  -0.03],\n            ]],[[\n                [ -0.03, 0.1,  -0.03],\n                [ -0.03, 0.1,  -0.03],\n                [ -0.03, 0.1,  -0.03],\n            ]]], requires_grad=True    \n        ),\n        'bias':torch.tensor(\n            [0.2, 0.2, 0.2], requires_grad=True\n        )}\n    model.conv1.load_state_dict(state_dict, strict=True)\n    return model","9056fe63":"def model_make_custom(model_name, cnn_version, fold=0, epoch=-1):\n    model = timmv2(model_name, False) # Start from SELF-trained\n    \n    prefix = f\"\/kaggle\/input\/seti-signal-search-cnn-{cnn_version}\/output\"\n    if epoch >= 0:\n        file_name = f\"{prefix}\/tf_efficientnetv2_b1_fold_{fold:02d}_epoch_{epoch:02d}_state.pth\"\n    else:\n        file_name = f\"{prefix}\/tf_efficientnetv2_b1_fold_{fold:02d}_state.pth\"\n        \n    # TODO: is map_location cuda OK when model is not yet loaded in GPU ?\n    model.load_state_dict(torch.load(file_name, map_location=torch.device(device))['model'])    \n    return model","23461e19":"model = model_make_custom(CFG.model_name, cnn_version=\"18\")\n\nlist(model.conv1.parameters())","1daf44be":"X = train.img_path.values\ny = train.target.values\n\nsample_size = 30\ntrain_index = 0 # some random image\ntrain_images = X[train_index:train_index+sample_size]\ntrain_targets = y[train_index:train_index+sample_size]\ntrain_dataset = ClassificationDataset(img_paths=train_images, targets=train_targets, tr=vtransform)\n\nFIG_SIZE = 6\n\nmodel.eval() # from model_make_custom\n\nfor i in range(sample_size):\n    image_target = train_dataset[i]\n    image, target = image_target['image'].unsqueeze(0), image_target['target']\n    if target == torch.tensor(1.0):\n        output = model(image).view(-1)\n        print(output.detach().numpy(), target.detach().numpy(), image_target['img_id'])\n \n        plt.figure(figsize=(FIG_SIZE, FIG_SIZE))\n        plt.axes().yaxis.set_major_locator(ticker.MultipleLocator(40))\n        plt.imshow(image.squeeze(0).numpy().transpose((1, 2, 0))[:,:,0]) # only 1 axis\n        plt.title(f'target: {target}')\n        plt.show()","982b0b30":"def get_score(y_true, y_pred):\n    score = roc_auc_score(y_true, y_pred)\n    return score","0a5bbb74":"def train_fn(data_loader, model, optimizer, criterion, device):\n    \n    model.train()\n    \n    for data in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        inputs = data['image']\n        targets = data['target']\n        \n        inputs = inputs.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        targets = targets.unsqueeze(1)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()        ","958a6efa":"def eval_fn(data_loader, model, device):\n    \n    model.eval()\n    \n    final_outputs = []\n    final_targets = []\n    final_img_ids = []\n    \n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            inputs = data['image']\n            targets = data['target']\n            img_ids = data['img_id']\n\n            inputs = inputs.to(device, dtype=torch.float)\n            output = model(inputs)\n            \n            output = output.detach().cpu().numpy().tolist()\n            targets = targets.numpy().tolist()\n\n            final_outputs.extend(output)\n            final_targets.extend(targets)\n            final_img_ids.extend(img_ids)\n            \n    return final_outputs, final_targets, final_img_ids","c7208b5f":"# Train models for each fold\nmodels = []\n\nX = train.img_path.values\ny = train.target.values\nskf = StratifiedKFold(n_splits=CFG.N_FOLDS)\n\nfold = 0\nfor train_index, valid_index in skf.split(X, y):\n    \n    # **********************  BREAK  ********************\n    # DEBUG don't do training for real\n    break\n    # **********************  BREAK  ********************\n    \n    print(f\"Starting Fold {fold:02d}\")\n    \n    train_images, valid_images = X[train_index], X[valid_index]\n    train_targets, valid_targets = y[train_index], y[valid_index]\n\n    train_dataset = ClassificationDataset(img_paths=train_images, targets=train_targets, tr=ttransform)\n    valid_dataset = ClassificationDataset(img_paths=valid_images, targets=valid_targets, tr=vtransform)\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True , num_workers=CFG.num_workers)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\n\n    model = model_make(CFG.model_name)\n    model.to(device)\n\n    for epoch in range(CFG.epochs):\n        train_fn(train_loader, model, optimizer, criterion, device=device)\n        predictions, valid_targets, _ = eval_fn(valid_loader, model, device=device)\n        roc_auc = get_score(valid_targets, predictions)\n        print(f\"Epoch={epoch}, Valid ROC AUC={roc_auc}\")\n        # print(list(model.conv1.parameters()))\n        \n        # Save model after each fold and epoch\n        torch.save({'model': model.state_dict()},\n                   OUTPUT_DIR+f\"{CFG.model_name}_fold_{fold:02d}_epoch_{epoch:02d}_state.pth\")\n        \n    # append the latest model\n    # TODO: select the \"best\" model (after each epoch), not the last epoch\n    models.append(model)\n    fold += 1","f6e57e1e":"# Evaluate models on validation AND training data for each fold and epoch\n# try to see if and when overfittigng occurs\n\nimport csv\n\nvalid_results = []\ntrain_results = []\n\nX = train.img_path.values\ny = train.target.values\nskf = StratifiedKFold(n_splits=CFG.N_FOLDS)\n\nfold = 0\nfor train_index, valid_index in skf.split(X, y):\n    print(f\"Starting Fold {fold:02d}\")\n\n    print(len(train_index), len(valid_index))\n    print(train_index[0:5])\n    print(valid_index[0:5])\n    \n    train_images, valid_images = X[train_index], X[valid_index]\n    train_targets, valid_targets = y[train_index], y[valid_index]\n\n    train_dataset = ClassificationDataset(img_paths=train_images, targets=train_targets, tr=ttransform)\n    valid_dataset = ClassificationDataset(img_paths=valid_images, targets=valid_targets, tr=vtransform)\n\n    # Here, since validation, shuffle False\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n\n    valid_results_per_fold = []\n    train_results_per_fold = []\n    \n    # First 3 (0,1,2) from CNN-19\n    # epoch 6 (faked here as epoch 4) from CNN-18\n    for epoch in range(4):\n        if (epoch < 3):\n            model = model_make_custom(CFG.model_name, \"19\", fold=fold, epoch=epoch)\n        elif (epoch == 3):\n            epoch = 5 # only the sixth one was saved in CNN-18\n            model = model_make_custom(CFG.model_name, \"18\", fold=fold)\n            \n        model.to(device)\n\n        # Validation over VALID data\n        predictions, targets, img_ids = eval_fn(valid_loader, model, device=device)\n        roc_auc = get_score(targets, predictions)\n        print(f\"Epoch={epoch}, Valid ROC AUC={roc_auc}\")\n\n        # predictions need to be flattened\n        flat_predictions = []\n        list(map(flat_predictions.extend, predictions))\n        valid_results_per_fold_per_epoch = np.dstack((flat_predictions, targets, img_ids))[0]\n\n        filename = OUTPUT_DIR+f\"{CFG.model_name}_fold_{fold:02d}_epoch_{epoch:02d}_validation.csv\"\n        \n        with open(filename, 'w') as f:\n            csv.writer(f).writerows(valid_results_per_fold_per_epoch)\n        \n        valid_results_per_fold.append(valid_results_per_fold_per_epoch)\n\n        # Validation over TRAIN data\n        predictions, targets, img_ids = eval_fn(train_loader, model, device=device)\n        roc_auc = get_score(targets, predictions)\n        print(f\"Epoch={epoch}, Train ROC AUC={roc_auc}\")\n\n        # predictions need to be flattened\n        flat_predictions = []\n        list(map(flat_predictions.extend, predictions))\n        train_results_per_fold_per_epoch = np.dstack((flat_predictions, targets, img_ids))[0]\n\n        filename = OUTPUT_DIR+f\"{CFG.model_name}_fold_{fold:02d}_epoch_{epoch:02d}_training.csv\"\n        \n        with open(filename, 'w') as f:\n            csv.writer(f).writerows(train_results_per_fold_per_epoch)\n        \n        train_results_per_fold.append(train_results_per_fold_per_epoch)\n        \n    valid_results.append(valid_results_per_fold)\n    train_results.append(train_results_per_fold)\n\n    fold += 1\n    \n    # DEBUG only 2 folds have this models calculated\n    if fold == 2:\n        break","88dc22c2":"len(valid_results_per_fold[0])","4bb3c8e6":"len(train_results_per_fold[0])","ebe086f7":"# Modelling\n\nInitial Exploratory Data Analysis was done in https:\/\/www.kaggle.com\/peterv1\/seti-signal-search-data-exploration\/\n\nUsing the EfficientNet ports to Pytorch from Ross Wightman Ref. https:\/\/github.com\/rwightman\/pytorch-image-models","5e42a23b":"# Utils","0a6d0973":"# Preprocessing","bac36f53":"# Training with folds","b80ec75e":"# Import data","2b841eaa":"# SETI Signal Search - CNN - 20\n\n## Specific\n\n* Try to understand the overfitting based on:\n* CNN-19 3 epochs on the first 2 Folds training effv2 b1 from pretrained timm\n* CNN-18 6th epoch on the all 5 Folds training effv2 b1 from pretrained timm\n\n## Global\n\nTry to predict the presence of \"needles\" with a CNN using PyTorch.\n\nFor transfer learning, look at TF EfficientNet and TF EfficientNet V2\n\nIn the list of Pytorch Image models https:\/\/paperswithcode.com\/lib\/timm\/ and sorting them by TOP 1 Accuracy, the EfficientNet is the first model that goes under 10 Billion Flops. Also, there are variations from b0 to b8 that I presume will make it possible to trade-off compute cost vs. accuracy.\n\nVery recently (14 May) the V2 was ported to this PyTorch repo. Maybe also testing tf_efficientnetv2_b0 up to tf_efficientnetv2_b3 ?\n\nInspired by https:\/\/www.kaggle.com\/piantic\/train-seti-pytorch-starter-chans-vs-spatial from https:\/\/www.kaggle.com\/piantic\n\nKFold and initial Convolutional filter inspired by Salman https:\/\/www.kaggle.com\/micheomaano\/mixup-training-5fold-spatial\/execution","afb96827":"# Preview","9c7e1e81":"# Model","674db4fa":"# Libraries","65045ac2":"# Config","82c3812a":"# Dataset"}}