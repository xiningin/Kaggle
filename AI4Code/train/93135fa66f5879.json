{"cell_type":{"d3a2bbaa":"code","31212dc3":"code","c690e4a6":"code","4ae6af8a":"code","d406b3a4":"code","5f84a461":"code","d2a53ff4":"code","7e0ed03e":"code","013f5cb8":"code","04ed2abf":"code","bccd4e7f":"code","88b34105":"code","2145f96e":"code","1f0a39a3":"code","6df2a6a3":"code","1d781bfd":"code","4847d329":"code","753e1cac":"code","5002099d":"code","b02e95b7":"code","bbe6985e":"code","f9966109":"code","eb57fbc5":"code","42bcea34":"code","5f804376":"code","bbe57a86":"code","14a84f70":"code","30054539":"markdown","851cfb23":"markdown","debf2adc":"markdown","78ff4b44":"markdown","b38d7874":"markdown","c1a9c1ec":"markdown","c15e99a9":"markdown","2d13364b":"markdown","fabd18cd":"markdown","2d54bcbb":"markdown","13ea1b90":"markdown"},"source":{"d3a2bbaa":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt","31212dc3":"np.random.seed(42)","c690e4a6":"#Simple Neuron\nclass Neuron(object):\n    def __init__(self, num_inputs, activation):\n        super().__init__()\n        self.weight = np.random.uniform(size = num_inputs, low = -1.0, high = 1.0)\n        self.bias = np.random.uniform(size = 1, low = -1., high = 1.)\n        self.activation = activation\n        \n    def forward(self, x):\n        z = np.dot(x, self.weight)+self.bias\n        return self.activation(z)","4ae6af8a":"input_size = 3\n#ReLU activation function is used here\nfunction = lambda y: 0 if y<=0 else 1\n\nperceptron = Neuron(num_inputs = input_size, activation = function)\nprint(\"Perceptron's random weights = {}, and random bias = {}\".format(\nperceptron.weight, perceptron.bias))","d406b3a4":"x = np.random.rand(input_size).reshape(1, input_size)\nprint('Input vector : {}'.format(x))","5f84a461":"y = perceptron.forward(x)\nprint('Output value = {}'.format(y))","d2a53ff4":"class FullyConnected(object):\n    def __init__(self, num_inputs, layer_size, activation_function, derivated_activation_function = None):\n        super().__init__()\n        self.W = np.random.standard_normal((num_inputs, layer_size))\n        self.b = np.random.standard_normal(layer_size)\n        self.size = layer_size\n        self.activation = activation_function\n        self.derivated = derivated_activation_function\n        self.x, self.y = None, None\n        self.dL_dW, self.dL_db = None, None\n    \n    def forward(self, x):\n        z = np.dot(x, self.W) + self.b\n        self.y = self.activation(z)\n        self.x = x\n        return self.y\n    \n    def backward(self, dL_dy):\n        dy_dz = self.derivated(self.y)  \n        dL_dz = (dL_dy * dy_dz) \n        dz_dw = self.x.T\n        dz_dx = self.W.T\n        dz_db = np.ones(dL_dy.shape[0]) \n\n        self.dL_dW = np.dot(dz_dw, dL_dz)\n        self.dL_db = np.dot(dz_db, dL_dz)\n\n        dL_dx = np.dot(dL_dz, dz_dx)\n        return dL_dx\n    \n    def optimize(self, epsilon):\n        self.W -= epsilon * self.dL_dW\n        self.b -= epsilon * self.dL_db","7e0ed03e":"input_size = 2\nneurons = 3\nfunction = lambda y: np.maximum(y, 0)\n\nlayer = FullyConnected(num_inputs = input_size, layer_size = neurons, activation_function = function)\n","013f5cb8":"x1 = np.random.uniform(-1, 1, 2).reshape(1, 2)\nprint(\"Input vector #1: {}\".format(x1))","04ed2abf":"x2 = np.random.uniform(-1, 1, 2).reshape(1, 2)\nprint(\"Input vector #2: {}\".format(x2))","bccd4e7f":"y1 = layer.forward(x1)\nprint(\"Layer's output value given `x1` : {}\".format(y1))","88b34105":"y2 = layer.forward(x2)\nprint(\"Layer's output value given `x2` : {}\".format(y2))","2145f96e":"x12 = np.concatenate((x1, x2))  # stack of input vectors, of shape `(2, 2)`\ny12 = layer.forward(x12)\nprint(\"Layer's output value given `[x1, x2]` :\\n{}\".format(y12))","1f0a39a3":"def sigmoid(x):\n    y = 1\/(1 + np.exp(-x))\n    return y\n\ndef derivated_sigmoid(y):\n    return y*(1-y)","6df2a6a3":"def loss_L2(pred, target):            \n    return np.sum(np.square(pred - target))\n\ndef derivated_loss_L2(pred, target):\n    return 2*(pred - target)","1d781bfd":"def binary_cross_entropy(pred, target):\n    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n\ndef derivated_binary_cross_entropy(pred, target):\n    return (pred - target) \/ (pred * (1-pred))","4847d329":"#Simple Neural Network\nclass SimpleNetwork(object):\n    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32),\n                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n                 loss_function=loss_L2, derivated_loss_function=derivated_loss_L2):\n        super().__init__()\n        layer_sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n        self.layers = [\n            FullyConnected(layer_sizes[i], layer_sizes[i + 1], \n                                activation_function, derivated_activation_function)\n            for i in range(len(layer_sizes) - 1)]\n        self.loss_function = loss_function\n        self.derivated_loss_function = derivated_loss_function\n    \n    def forward(self, x):\n        for layer in self.layers: \n            x = layer.forward(x)\n        return x\n    \n    def predict(self, x):\n        estimations = self.forward(x)\n        best_class = np.argmax(estimations)\n        return best_class\n    \n    def backward(self, dL_dy):\n        for layer in reversed(self.layers): \n            dL_dy = layer.backward(dL_dy)\n        return dL_dy\n\n    def optimize(self, epsilon):\n        for layer in self.layers:            \n            layer.optimize(epsilon)\n            \n    def evaluate_accuracy(self, X_val, y_val):\n        num_corrects = 0\n        for i in range(len(X_val)):\n            pred_class = self.predict(X_val[i])\n            if pred_class == y_val[i]:\n                num_corrects += 1\n        return num_corrects \/ len(X_val)\n    \n    def train(self, X_train, y_train, X_val=None, y_val=None, \n              batch_size=32, num_epochs=5, learning_rate=1e-3, print_frequency=20):\n        num_batches_per_epoch = len(X_train) \/\/ batch_size\n        do_validation = X_val is not None and y_val is not None\n        losses, accuracies = [], []\n        for i in range(num_epochs): \n            epoch_loss = 0\n            for b in range(num_batches_per_epoch):  # for each batch composing the dataset\n                # Get batch:\n                batch_index_begin = b * batch_size\n                batch_index_end = batch_index_begin + batch_size\n                x = X_train[batch_index_begin: batch_index_end]\n                targets = y_train[batch_index_begin: batch_index_end]\n                # Optimize on batch:\n                predictions = y = self.forward(x)  # forward pass\n                L = self.loss_function(predictions, targets)  # loss computation\n                dL_dy = self.derivated_loss_function(predictions, targets)  # loss derivation\n                self.backward(dL_dy)  # back-propagation pass\n                self.optimize(learning_rate)  # optimization of the NN\n                epoch_loss += L\n\n            # Logging training loss and validation accuracy, to follow the training:\n            epoch_loss \/= num_batches_per_epoch\n            losses.append(epoch_loss)\n            if do_validation:\n                accuracy = self.evaluate_accuracy(X_val, y_val)\n                accuracies.append(accuracy)\n            else:\n                accuracy = np.NaN\n            if i % print_frequency == 0 or i == (num_epochs - 1):\n                print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n                    i, epoch_loss, accuracy * 100))\n        return losses, accuracies","753e1cac":"import mnist\nX_train, y_train = mnist.train_images(), mnist.train_labels()\nX_test,  y_test  = mnist.test_images(), mnist.test_labels()\nnum_classes = 10","5002099d":"img_idx = np.random.randint(0, X_test.shape[0])\nplt.imshow(X_test[img_idx], cmap=matplotlib.cm.binary)\nplt.axis(\"off\")\nplt.show()","b02e95b7":"X_train, X_test = X_train.reshape(-1, 28 * 28), X_test.reshape(-1, 28 * 28)","bbe6985e":"X_train, X_test = X_train \/ 255., X_test \/ 255.\nprint(\"Normalized pixel values between {} and {}\".format(X_train.min(), X_train.max()))","f9966109":"y_train = np.eye(num_classes)[y_train]","eb57fbc5":"mnist_classifier = SimpleNetwork(num_inputs=X_train.shape[1], \n                                 num_outputs=num_classes, hidden_layers_sizes=[64, 32])","42bcea34":"predictions = mnist_classifier.forward(X_train)                         \nloss_untrained = mnist_classifier.loss_function(predictions, y_train)   \n\naccuracy_untrained = mnist_classifier.evaluate_accuracy(X_test, y_test) \nprint(\"Untrained : training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n    loss_untrained, accuracy_untrained * 100))","5f804376":"losses, accuracies = mnist_classifier.train(X_train, y_train, X_test, y_test, \n                                            batch_size=30, num_epochs=500)","bbe57a86":"losses, accuracies = [loss_untrained] + losses, [accuracy_untrained] + accuracies\nfig, ax_loss = plt.subplots()\n\ncolor = 'red'\nax_loss.set_xlim([0, 510])\nax_loss.set_xlabel('Epochs')\nax_loss.set_ylabel('Training Loss', color=color)\nax_loss.plot(losses, color=color)\nax_loss.tick_params(axis='y', labelcolor=color)\n\nax_acc = ax_loss.twinx()  # instantiate a second axes that shares the same x-axis\ncolor = 'blue'\nax_acc.set_xlim([0, 510])\nax_acc.set_ylim([0, 1])\nax_acc.set_ylabel('Val Accuracy', color=color)\nax_acc.plot(accuracies, color=color)\nax_acc.tick_params(axis='y', labelcolor=color)\n\nfig.tight_layout()\nplt.show()","14a84f70":"predicted_class = mnist_classifier.predict(np.expand_dims(X_test[img_idx], 0))\nprint('Predicted class: {}; Correct class: {}'.format(predicted_class, y_test[img_idx]))","30054539":"# Activation Function\n\nIt\u2019s just a thing function that you use to get the output of node. It is also known as Transfer Function. Activation functions that are commonly used based on few desirable properties like :\n\n* Nonlinear \u2014 When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator.\n\n* Range \u2014 When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. \n\n* Continuously differentiable \u2014 This property is desirable (ReLU is not continuously differentiable and has some issues with gradient-based optimization, but it is still possible) for enabling gradient-based optimization methods.\n\n\n\nThere are many types of activation function in neural networks:\n\n1. Sigmoid Function: Sigmoid functions are used in machine learning for the logistic regression and basic neural network implementations and they are the introductory activation units.\n![image.png](attachment:image.png)\n\n","851cfb23":"# Building a Neural Network\n\nThis notebook is an introduction for building a neural network by first building a neuron then connecting them together to form a simple classifier. We will use MNIST digits dataset and classify images of handwritten digits.","debf2adc":"# Result\n\nYou can see above that the classifier attained an accuracy of nearly 95%","78ff4b44":"**MNIST Dataset**","b38d7874":"# Fully Connected Layers\n\nUsually, neural networks are organized into layers, that is, sets of neurons that typically receive the same input and apply the same operation\n\nIn networks, the information flows from the input layer to the output one, with one or more hidden layers in-between. In Figure 1-13, the three neurons A, B, and C, belong to the input layer, the neuron H belongs to the output or activation layer, and the neurons D, E, F, and G belong to the hidden layer. The first layer has an input, x, of size 2, the second (hidden) layer takes the three activation values of the previous layer as input, and so on. Such layers, with each neuron connected to all the values from the previous layer, are classed as being fully-connected or dense:\n![image.png](attachment:image.png)","c1a9c1ec":"2. Tanh: tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n![image.png](attachment:image.png)\n\n","c15e99a9":"**Refrences**\n1. Rosenblatt, F., 1958. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review 65, 386.\n\n2. LeCun, Y., Cortes, C., Burges, C., 2010. MNIST handwritten digit database. AT&T Labs [Online]. Available: http:\/\/yann.lecun.com\/exdb\/mnist 2, 18.\n\n3. Planche, Benjamin; Andres, Eliot. Hands-On Computer Vision with TensorFlow 2 \n\n","2d13364b":"# Forming a Classifier\n\n\n\nThe purpose of neural layers is to be stacked together to form a neural network able to perform non-linear predictions.\n\nApplying a gradient descent, such a network can be trained to perform correct predictions. But for that, we need a loss function to evaluate the performance of the network, and we need to know how to derive all the operations performed by the network, to compute and propagate the gradients.\n\nIn this section, a simple fully-connected neural network is built. Let us assume we want our network to use the sigmoid function for the activation. We need to implement that function and its derivative:","fabd18cd":"**Binary Cross Entropy Loss**\n\nIt converts the predicted probabilities into logarithmic scale before comparing them to the expected values\n","2d54bcbb":"# Building a Neuron\n\nInspired by Neurons present in our brain, the artificial neuron takes several inputs (each a number). A simple artificial neuron multiplies these inputs with their respective weights and sums them together. Finally applies an activation function to obtain the output signal, which can be passed to the next neurons in the network (this can be seen as a directed graph):\n\n![image.png](attachment:image.png)\n","13ea1b90":"3. Rectified Linear Units (ReLU): Most Deep Learning applications right now make use of ReLU instead of Logistic Activation functions for Computer Vision, Speech Recognition and Deep Neural Networks etc.  A Rectified Linear Unit (A unit employing the rectifier is also called a rectified linear unit ReLU) has output 0 if the input is less than 0, and raw output otherwise.\n![image.png](attachment:image.png)\n\n"}}