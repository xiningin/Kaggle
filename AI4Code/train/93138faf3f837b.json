{"cell_type":{"01cde223":"code","2f2ae312":"code","3bab1690":"code","2b127e2d":"code","f153c8b7":"code","9030e581":"code","9c4c45ec":"code","0ab81c18":"code","f97f7eed":"code","58915a28":"code","19e9b644":"code","202e1c1e":"code","1746635e":"code","c44ed549":"code","f5dfd34b":"code","8c403103":"code","4ee5b130":"code","7e443ea6":"code","bd458862":"code","9798d83e":"code","0a7f3a63":"code","afb90965":"code","2c917670":"code","07ba973f":"code","b9b88d79":"code","45d31dda":"code","7c14caf9":"code","a2b91fc7":"markdown","feb08aa2":"markdown","5aaf90ee":"markdown","36857cdd":"markdown","1ae5120a":"markdown","331a5bee":"markdown"},"source":{"01cde223":"# Import Library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2f2ae312":"# Load Data\ndf = pd.read_csv('..\/input\/rice-type-classification\/riceClassification.csv')","3bab1690":"df.head()","2b127e2d":"df.tail()","f153c8b7":"df.shape","9030e581":"df.isna().sum()","9c4c45ec":"df.info()","0ab81c18":"df.describe()","f97f7eed":"# Get distinct values\nfor i in df.columns:\n    print(\"Feature : \", i, \" --> \",df[i].unique())","58915a28":"# Class Distribution\ndf.Class.value_counts()","19e9b644":"# will ignore \"id\", as it is not required.\nX = df.drop(['id', 'Class'], axis=1)\ny = df['Class']","202e1c1e":"X.shape, y.shape","1746635e":"#split into train and test sets\nfrom sklearn.model_selection import train_test_split","c44ed549":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=99)","f5dfd34b":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","8c403103":"# Make the data scaler\nfrom sklearn.preprocessing import StandardScaler","4ee5b130":"# scale the X data    \nsc = StandardScaler()\nsc.fit(X_train) # fit only on train data","7e443ea6":"X_train_sc = sc.transform(X_train)","bd458862":"type(X_train_sc)","9798d83e":"X_train = pd.DataFrame(sc.transform(X_train), index=X_train.index, columns=X_train.columns)\nX_valid = pd.DataFrame(sc.transform(X_valid), index=X_valid.index, columns=X_valid.columns)","0a7f3a63":"X_train.head()","afb90965":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score","2c917670":"lr = LogisticRegression()","07ba973f":"lr.fit(X_train, y_train)","b9b88d79":"pred_lr = lr.predict(X_valid)","45d31dda":"acc_lr = accuracy_score(y_valid, pred_lr)\nacc_lr","7c14caf9":"f1_lr = f1_score(y_valid, pred_lr, pos_label=1)\nf1_lr","a2b91fc7":"No Categorical features...","feb08aa2":"We already knew... Target (Class) is Binary Classification.","5aaf90ee":"Very impressive... base model Logistic Regression itself has provided an accuracy of 98% and f1 score as 0.98.\n\nWe should still dig with other model's to see whcich is best of all for this specific classification problem.\n\n# Thanks to the author for sharing the cleaned and wonderful data-set.","36857cdd":"The very first step after loading dataset is to explore it, and while exploring usually we tend to first hvave a look and feel of first few rows using .head() and .tail() methods.\n\nand then to get the shape or count of rows and columns.\n\nNext is to check for NAN's, and luckily we do not have any null's... but will try to check all data points once again, as some time it happens that the missing data may not be NaN, and instead they will be white-space; -1; or some indicator for missing data.\n\nFollowed by the data types and some stats on features.\n\nThese steps need not be in this sequence, it can be in any order.","1ae5120a":"Lets start with LogisticRegression, which is a base model.","331a5bee":"Usually we do transform the X_train, but this will convert our data set to numpy.ndarray.\n\nSo instead of just doing transform, lets convert it into daat frame so that we can refer to it easily."}}