{"cell_type":{"22072f61":"code","5b6a0fd9":"code","ee58b007":"code","cf243e4e":"code","b104ff8d":"code","12a1b821":"code","a9be1a69":"code","fdab683b":"code","541c6e7e":"code","1dfee654":"code","dc4b0b1d":"code","777ab8d4":"markdown","6ec82260":"markdown","055d7388":"markdown","8f1a900a":"markdown","8690a650":"markdown","d04ab59d":"markdown","bb4d5042":"markdown","4a64d419":"markdown"},"source":{"22072f61":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics,Model,losses\nimport sys\nimport json\nimport gc\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom scipy.optimize import dual_annealing, minimize","5b6a0fd9":"# Import train data, drop sig_id, cp_type\n\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)","ee58b007":"# Preprocessing for Label Smoothing kernel\n\ndef preprocessor_labelsmooth():\n    # Import train data, drop sig_id, cp_type\n\n    train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\n    train_features = train_features.drop(['sig_id'],axis=1)\n    non_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\n    train_features = train_features.drop(['cp_type'],axis=1)\n    train_features = train_features.iloc[non_ctl_idx]\n\n    # Import public test data\n\n    public_test_features = pd.read_csv('\/kaggle\/input\/moapublictest\/test_features.csv')\n    public_test_features = public_test_features.drop('sig_id',axis=1)\n\n    # Import test data\n\n    test_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n    test_features = test_features.drop('sig_id',axis=1)\n\n \n    # Label Encoder for categorical cp_dose\n    cat = 'cp_dose'\n    le = preprocessing.LabelEncoder()\n    le.fit(train_features[cat])\n    train_features[cat] = le.transform(train_features[cat])\n\n    # Transform categorical\n    \n    public_test_features[cat] = le.transform(public_test_features[cat])\n    test_features[cat] = le.transform(test_features[cat])\n    \n    # Min Max Scaler for numerical values\n\n    # Fit scaler to joint train and test data\n    scaler = preprocessing.MinMaxScaler()\n    scaler.fit(train_features.append(public_test_features.drop('cp_type',axis=1)))\n\n    # Scale train data\n    data_train = scaler.transform(train_features)\n\n    # Scale test data\n    data_test = scaler.transform(test_features.drop('cp_type',axis=1))\n    \n    cs = train_features.columns.str.contains('c-')\n    gs = train_features.columns.str.contains('g-')\n    \n    return data_train, data_test, cs, gs\n\n# Preprocessing for AutoEncoder Kernel\n## Takes preprocessed data from label smooth kernel and transforms to autoencoder features\n\ndef preprocessor_autoencoder(data_test,cs,gs):\n\n    cells_test = data_test[:,cs]\n    genes_test = data_test[:,gs]\n    \n    cells_autoencoder = tf.keras.models.load_model(\n        '..\/input\/moaaemodels\/CellsAE')\n    genes_autoencoder = tf.keras.models.load_model(\n        '..\/input\/moaaemodels\/GenesAE')\n    \n    ae_cells_test = cells_autoencoder.encoder(cells_test).numpy()\n    ae_genes_test = genes_autoencoder.encoder(genes_test).numpy()\n    \n    data_test = np.concatenate((data_test[:,~(cs+gs)],ae_genes_test,ae_cells_test),axis=1)\n    \n    return data_test\n\n# ResNet has two steps of preprocessing : first with the entire dataset and second per fold\n# This model does not need the public test, since all transformations are determined by the train alone.\n\ndef preprocessor_resnet():\n    \n    # Import train data, drop sig_id, cp_type\n\n    train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\n    non_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\n    train_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\n    train_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n    train_targets_scored = train_targets_scored.drop('sig_id',axis=1)\n    labels_train = train_targets_scored.values\n\n    # Drop training data with ctl vehicle\n\n    train_features = train_features.iloc[non_ctl_idx]\n    labels_train = labels_train[non_ctl_idx]\n\n    # Import test data\n\n    test_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n    test_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n\n    # Import predictors from public kernel\n\n    json_file_path = '..\/input\/t-test-pca-rfe-logistic-regression\/main_predictors.json'\n\n    with open(json_file_path, 'r') as j:\n        predictors = json.loads(j.read())\n        predictors = predictors['start_predictors']\n        \n    # Create g-mean, c-mean, genes_pca (2 components), cells_pca (all components)\n\n    cs = train_features.columns.str.contains('c-')\n    gs = train_features.columns.str.contains('g-')\n    \n    return train_features,test_features,cs,gs,predictors\n\ndef preprocessor_resnet_fold(train,test,cs,gs):\n\n    # PCA\n\n    n_gs = 2 # No of PCA comps to include\n    n_cs = 100 # No of PCA comps to include\n\n    pca_cs = PCA(n_components = n_cs)\n    pca_gs = PCA(n_components = n_gs)\n\n    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n    test_pca_gs = pca_gs.transform(test[:,gs])\n    test_pca_cs = pca_cs.transform(test[:,cs])\n\n    # c-mean, g-mean\n\n    train_c_mean = train[:,cs].mean(axis=1)\n    test_c_mean = test[:,cs].mean(axis=1)\n    train_g_mean = train[:,gs].mean(axis=1)\n    test_g_mean = test[:,gs].mean(axis=1)\n\n    # Append Features\n\n    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n                            ,train_g_mean[:,np.newaxis]),axis=1)\n    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n                           test_g_mean[:,np.newaxis]),axis=1)\n\n    # Scaler for numerical values\n\n    # Scale train data\n    scaler = preprocessing.StandardScaler()\n\n    train = scaler.fit_transform(train)\n\n    # Scale Test data\n    test = scaler.transform(test)\n\n    return train, test","cf243e4e":"model_paths = ['..\/input\/moalabelsmoothmodels\/LabelSmoothed','..\/input\/moaaemodels\/AutoEncoded','..\/input\/moa-multi-input-resnet-model\/TwoHeads']\n\nn_models = 3\nn_labels = labels_train.shape[1]\nn_test = test_features.shape[0]\nn_train = train_features.shape[0]\n# Create arrays that store the train oof predictions (y_val) and test set predictions (y_pred) per model\n\ny_val = np.zeros((n_models, n_train, n_labels))\ny_pred = np.zeros((n_models, n_test, n_labels))","b104ff8d":"# Clipping Thresholds\n\np_min = 0.001\np_max = 0.999\n\n# Custom Metrics\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))","12a1b821":"# 6 seed, 5 fold CV used in Label Smoothing and Autoencoder Kernels\n\nn_seeds = 6\nn_folds = 5\n\n\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\n# Preprocess Data\n\ndata_train, data_test, cs, gs = preprocessor_labelsmooth()\ndata_train_ae = preprocessor_autoencoder(data_train,cs,gs)\ndata_test_ae = preprocessor_autoencoder(data_test,cs,gs)\n\n# Loop over seeds\n\nfor seed in seeds:\n    \n    fold = 0\n    mskf = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    \n    for train, test in mskf.split(data_train,labels_train):\n        y_test = labels_train[test]\n        \n        # Loop over models\n        \n        for i in range(2):\n            \n            # Load Model\n            model_path = model_paths[i]+'_seed_'+str(seed)+'_fold_'+str(fold)\n            model = tf.keras.models.load_model(model_path,custom_objects={'logloss':logloss})\n            \n            # Preprocess Autoencoder\n            if i==1:\n                X_test = data_train_ae[test]\n                # Entire Test Set\n                y_pred[i] += model.predict(data_test_ae)\/(n_folds*n_seeds)\n            else:\n                X_test = data_train[test]\n                # Entire Test Set\n                y_pred[i] += model.predict(data_test)\/(n_folds*n_seeds)\n                \n            # Run predictions\n            \n            \n            \n            # OOF Validation Set\n            y_val[i,test] += model.predict(X_test)\/n_seeds\n            \n        fold += 1\n        \n    tf.keras.backend.clear_session()\n    del model, X_test, y_test\n    gc.collect()","a9be1a69":"# Generate Seeds\n\nn_seeds = 5\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\n# Training Loop\n\nn_folds = 10\n\n# Preprocessing Step 1\ntrain_features,test_features,cs,gs,predictors = preprocessor_resnet()\n\nfor seed in seeds:\n    fold = 0\n    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in kf.split(train_features):\n        _, X_test = preprocessor_resnet_fold(train_features.iloc[train].values,\n                                       train_features.iloc[test].values,cs,gs)\n        _,data_test = preprocessor_resnet_fold(train_features.iloc[train].values,\n                                   test_features.drop('cp_type',axis=1).values,cs,gs)\n        X_test_2 = train_features.iloc[test][predictors].values\n        data_test_2 = test_features[predictors].values\n\n        # Load Model\n        model_path = model_paths[2]+'_seed_'+str(seed)+'_fold_'+str(fold)\n        model = tf.keras.models.load_model(model_path,custom_objects={'logloss':logloss})\n        # OOF Score\n        y_val[2,test] += model.predict([X_test,X_test_2])\/n_seeds\n\n        # Run prediction\n        y_pred[2] += model.predict([data_test,data_test_2])\/(n_folds*n_seeds)\n\n        fold += 1\n\n    tf.keras.backend.clear_session()\n    del model, X_test,X_test_2,data_test,data_test_2\n    gc.collect()","fdab683b":"# Define Blending Functions\n\ndef blended_preds(y_pred,ws):\n    return (y_pred*ws[:,np.newaxis,np.newaxis]).sum(axis=0)\n\n\ndef blended_oof(ws):\n    y_vals = blended_preds(y_val,ws)\n    loss = logloss(tf.constant(labels_train,dtype=tf.float32),tf.constant(y_vals,dtype=tf.float32))\n    return loss.numpy() + ((1-np.sum(ws))**2)","541c6e7e":"# Optimize weights for blended OOF\n\nopt = dual_annealing(blended_oof,bounds=((0,1),(0,1),(0,1)))\nws = opt.x\n\nprint('The weights are ' + str(ws))\n\n# Check that weights sum to one\n\nprint('Sum of weights is ' + str(ws.sum()))\n\n# OOF Score\n\nprint('OOF score is ' + str(opt.fun))","1dfee654":"y_val = blended_preds(y_val,ws)\ny_pred = blended_preds(y_pred,ws)","dc4b0b1d":"# Clipping Thresholds\n\np_min = 0.001\np_max = 0.999\n\n# Generate submission file, Clip Predictions\n\nsub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsub.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n\n# Set ctl_vehicle to 0\nsub.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\n\nsub.to_csv('submission.csv', index=False)","777ab8d4":"# Demonstration of Inference\/Blending using pretrained models\n\n## This notebook demonstrates how you can use pretrained models output by kernels and perform inference as well as blending.\n\n## Models taken from my public kernels:\n\n### [Label Smoothing](https:\/\/www.kaggle.com\/rahulsd91\/moa-label-smoothing) : V10, LB 0.01865\n### [ResNet Model](https:\/\/www.kaggle.com\/rahulsd91\/moa-multi-input-resnet-model) : V5, LB 0.01854\n### [Autoencoder Model](https:\/\/www.kaggle.com\/rahulsd91\/moa-autoencoder-features-only-lb-0-01884?scriptVersionId=44521379) : V2, LB 0.01884\n\nCurrently, there seems to be no way to include outputs of previous versions of kernels directly, only the latest one. So while the ResNet models can be imported directly, the other two of the notebooks' outputs were uploaded to datasets.","6ec82260":"## Inference Loop, Seed Blend\n\n## You want to be careful to use the same seeds for training and inference. Hard coding the seed into your saved model names the way I've done here is one way to ensure that an error in that doesn't go unnoticed.\n\nNote: I run the backend.clear_session() and garbage collector calls once per seed because the RAM gets fully used otherwise","055d7388":"## Things to keep in mind:\n\n* Make sure you initialize the seeds\/create the folds the same way as the training notebooks.\n* The preprocessing must also be done in an identical manner. One common mistake I have observed people (including myself in a previous version of this notebook!) make is to use the test_features.csv for determining transformations - since the submission run includes public+private dataset, you end up creating different features from what your model was trained on. To bypass this, I've uploaded the public test to a [dataset](https:\/\/www.kaggle.com\/rahulsd91\/moapublictest) and included that here.","8f1a900a":"## Custom Evaluation Metrics used by pretrained models \n\n### If any of your pretrained models in keras used a custom metric, it needs to be passed to the load_model function, otherwise you will get an error\n","8690a650":"# Label Smoothing and Autoencoder\n\nThese kernels had the same number of folds and seeds. So we can run the inference in parallel","d04ab59d":"# ResNet Model\n\nThe ResNet model has 10 folds and 7 seeds. It also has different clipping thresholds","bb4d5042":"# Calculate Blending Weights\n\n### Optimize weights based on the OOF score. I use scipy's minimize function here. Check out [this post](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/186539) for a discussion on scipy optimizers for blending. Note that the optimizer here actually doesn't do a great job. I might update it in a later version.","4a64d419":"# Define Preprocessing Functions for each notebook"}}