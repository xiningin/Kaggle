{"cell_type":{"8c586a27":"code","5c808759":"code","f91f3776":"code","5b384b23":"code","07fca342":"code","7c46b3ba":"code","48251f7c":"code","70a30e91":"code","768fcbf1":"code","4a870eac":"code","3d13e0c8":"code","233e0c41":"markdown","bdc78e9a":"markdown","4ffd1c8a":"markdown","65259e9e":"markdown","161a70ea":"markdown","7542b09e":"markdown","af94d80c":"markdown"},"source":{"8c586a27":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Importing linear regression module from sklearn\nfrom sklearn.linear_model import LinearRegression\n\nimport seaborn as sns\nsns.set_context('notebook')\nsns.set_style('white')\nfrom mpl_toolkits.mplot3d import axes3d","5c808759":"import urllib.request\nlocal_filename, headers = urllib.request.urlretrieve(\"https:\/\/raw.githubusercontent.com\/emilmont\/Artificial-Intelligence-and-Machine-Learning\/master\/ML\/ex1\/ex1data1.txt\",\"ex1data1.txt\")\nhtml = open(local_filename)\nhtml.close()","f91f3776":"data = np.loadtxt('ex1data1.txt', delimiter=',')\n\nX = np.c_[np.ones(data.shape[0]),data[:,0]]\ny = np.c_[data[:,1]]","5b384b23":"plt.scatter(X[:,1], y, s=30, c='r', marker='x', linewidths=1)\nplt.xlim(4,24)\nplt.xlabel('Population of City in 10,000s')\nplt.ylabel('Profit in $10,000s');","07fca342":"def computeCost(X, y, w=[[0],[0]]):\n    m = y.size\n    J = 0\n    h = X.dot(w)\n    J = 1\/(2*m)*np.sum(np.square(h-y))\n    return(J)","7c46b3ba":"computeCost(X,y)","48251f7c":"def gradientDescent(X, y, w=[[0],[0]], alpha=0.01, num_iters=1500):\n    m = y.size\n    J_history = np.zeros(num_iters)\n    \n    for iter in np.arange(num_iters):\n        h = X.dot(w)\n        w = w - alpha*(1\/m)*(X.T.dot(h-y))\n        J_history[iter] = computeCost(X, y, w)\n    return(w, J_history)","70a30e91":"# theta for minimized cost J\ntheta , Cost_J = gradientDescent(X, y)\nprint('theta: ',theta.ravel())\n\nplt.plot(Cost_J)\nplt.ylabel('Cost J')\nplt.xlabel('Iterations');","768fcbf1":"xx = np.arange(5,23)\n# Corresponding with y = Wx + b\nyy = ( theta[1]*xx )+ theta[0]\n\n# Plot gradient descent\nplt.scatter(X[:,1], y, s=30, c='r', marker='x', linewidths=1)\nplt.plot(xx,yy, label='Linear regression (Gradient descent)')\n\n# Compare with Scikit-learn Linear regression \nregr = LinearRegression()\nregr.fit(X[:,1].reshape(-1,1), y.ravel())\nplt.plot(xx, regr.intercept_+regr.coef_*xx, label='Linear regression (Scikit-learn GLM)')\n\nplt.xlim(4,24)\nplt.xlabel('Population of City in 10,000s')\nplt.ylabel('Profit in $10,000s')\nplt.legend(loc=4);","4a870eac":"# Predict profit for a city with population of 35000 and 70000\nprint(theta.T.dot([1, 3.5])*10000)\nprint(theta.T.dot([1, 7])*10000)\n","3d13e0c8":"# Create grid coordinates for plotting\nB0 = np.linspace(-10, 10, 50)\nB1 = np.linspace(-1, 4, 50)\nxx, yy = np.meshgrid(B0, B1, indexing='xy')\nZ = np.zeros((B0.size,B1.size))\n\n# Calculate Z-values (Cost) based on grid of coefficients\nfor (i,j),v in np.ndenumerate(Z):\n    Z[i,j] = computeCost(X,y, w=[[xx[i,j]], [yy[i,j]]])\n\nfig = plt.figure(figsize=(15,6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122, projection='3d')\n\n# Left plot\nCS = ax1.contour(xx, yy, Z, np.logspace(-2, 3, 20), cmap=plt.cm.jet)\nax1.scatter(theta[0],theta[1], c='r')\n\n# Right plot\nax2.plot_surface(xx, yy, Z, rstride=1, cstride=1, alpha=0.6, cmap=plt.cm.jet)\nax2.set_zlabel('Cost')\nax2.set_zlim(Z.min(),Z.max())\nax2.view_init(elev=15, azim=230)\n\n# settings common to both plots\nfor ax in fig.axes:\n    ax.set_xlabel(r'$\\theta_0$', fontsize=17)\n    ax.set_ylabel(r'$\\theta_1$', fontsize=17)","233e0c41":"Let's begin by importing all the required fundamental libraries beforehand \nHere we will be using the _LinearRegression_ function from the _Sklearn_ module just to compare how well our model has been working. \n\nApart from that, just to showcase the plots in a lucid representation, we are going to use the seaborn and the mpl_toolkits for 3d plots.","bdc78e9a":"What do we see? In the plot above, we can see that the points have been clustered on one side and somewhat linearly increases with the change in x. Can we find a line that will approximately fit the dataset? Lets find out.","4ffd1c8a":"## Gradient Descent\nGradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\nGradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.\n\nIn gradient descent, the values of W and b optimize based in the following mathematical formulas\n\n$$ \\frac{\\delta}{\\delta{w}}J(w) = \\Delta_w J $$\n\n\n$$ W = W - \\alpha \\Delta_w J(w)  $$\n\n\n$$ \\frac{\\delta}{\\delta{b}}J(w) = \\Delta_b J $$\n\n\n$$ b = b - \\alpha \\Delta_b J(w) $$\n\n3blue1Brown's video here explains very intutively the actual working of the gradient descent algorithm: \n\nhttps:\/\/www.youtube.com\/watch?v=IHZwWFHWa-w","65259e9e":"# Understanding Linear Regression\n\n_Linear regression_ is perhaps one of the most well known and well understood algorithms in statistics and machine learning.\n\nYou do not need to know any statistics or linear algebra to understand linear regression. This is a gentle high-level introduction to the technique to give you enough background to be able to use it effectively on your own problems.\n\nWhen you start looking into linear regression, things can get very confusing.\n\nThe reason is because linear regression has been around for so long (more than 200 years). It has been studied from every possible angle and often each angle has a new and different name.\n\nLinear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\n\n$y = Wx + b $","161a70ea":"Once we get the dataset, we will be splitting the x and the y axis in the variable $X$ and $y$","7542b09e":"We will first begin with the cost function that will compute the cost difference between the actual and the predicted values. The cost function also called as the loss function used in the tutorial is given as $$ J = \\frac{1}{2}m\\sum{(y - \\hat{y})^2} $$","af94d80c":"We will just be downloading the dataset of population datapoints.[](http:\/\/)"}}