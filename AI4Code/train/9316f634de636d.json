{"cell_type":{"e647d376":"code","a9371c04":"code","ff2dfe35":"code","573c1b54":"code","c396479e":"code","84344c0f":"code","41338381":"code","1a11d15b":"code","cf704f3a":"code","53a3cc7f":"code","d691f16a":"code","d61f9877":"code","5b5d8258":"code","6e7caa80":"code","eb4bc146":"code","d2ffbeeb":"code","6050250b":"code","6e11354e":"code","71d91263":"code","78e6c574":"code","3d227b4c":"code","3e205926":"code","9d87a04f":"code","509f307e":"code","2621f749":"code","4fe50838":"code","661eb7e4":"code","56bee8d8":"code","4d499b4b":"code","85af0fa9":"code","b709ceaf":"code","c8c71d79":"markdown","0f89b450":"markdown","94b2ad7c":"markdown","6d563830":"markdown","d673c13f":"markdown","64e69530":"markdown","47168693":"markdown","0612e267":"markdown","5c4a64da":"markdown","f52a7918":"markdown","c524995c":"markdown","1acc072f":"markdown","352b3a75":"markdown","827ecb6f":"markdown","bb54846e":"markdown"},"source":{"e647d376":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a9371c04":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import preprocessing \nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing \nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom matplotlib import pyplot\nimport sklearn","ff2dfe35":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","573c1b54":"data.info()","c396479e":"print('Number of Duplicated row in the DataFrame: '+  str(data.duplicated().sum()))\ndata = data.drop_duplicates(keep='first')\nprint('Data Shape before removing of Duplicate: '+ str(data.shape))\nprint('Data Shape after removing of Duplicate: '+  str(data.shape))","84344c0f":"##### Since the Time column is have a lack of information we will drop it.\ndata.pop('Time')","41338381":"# Let split the data from the independent(x_data) and dependent(y_data) variables\nx_data = data.drop(columns = ['Class'], axis = 1)\ny_data = data['Class']\nx_data.head()","1a11d15b":"def draw_histograms(dataframe, features, rows, cols):\n    fig=plt.figure(figsize=(20,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')\n        ax.set_title(feature+\" Distribution\",color='DarkRed')\n        ax.set_yscale('log')\n    fig.tight_layout()  \n    plt.show()\ndraw_histograms(x_data,x_data.columns,8,4)","cf704f3a":"sns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x=\"Class\", data= data)\nax.set_yscale('log')","53a3cc7f":"binary_perc = ((data['Class'].value_counts()[1])\/(data['Class'].value_counts()[0])*100)\nprint('Count % of 1 or Fraud value in Class Column: '+ str(binary_perc))\nprint('Count % of 0 or Not Fraud value in Class Column: '+ str(100-binary_perc))","d691f16a":"plt.figure(figsize=(20,6))\ncor = x_data.corr()\nsns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\nplt.yticks(rotation=0)\nplt.show()","d61f9877":"# Standardization of x_data (independent variables)\nX = preprocessing.StandardScaler().fit(x_data).transform(x_data)\nX[0:5]","5b5d8258":"X_train, X_test, y_train, y_test = train_test_split(X, y_data, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","6e7caa80":"y_train[0:4]","eb4bc146":"model = Sequential()\nmodel.add(Dense(3, activation = 'relu', input_dim = 29, use_bias=True, bias_initializer='zeros'))\nmodel.add(Dense(256, activation = 'sigmoid'))\nmodel.add(Dense(1, activation = 'sigmoid'))","d2ffbeeb":"METRICS = [\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='accuracy', \n    verbose=4,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)\n\nmodel.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n              loss='binary_crossentropy',\n              metrics=METRICS)","6050250b":"history = model.fit(X_train, y_train, \n                    epochs = 50, batch_size = 200, \n                    validation_split = 0.30, \n                    shuffle = True, callbacks=[early_stopping],)","6e11354e":"history.history['loss']","71d91263":"model.evaluate(X_test, y_test)","78e6c574":"y_predicted = model.predict(X_test)\ny_predicted[:5]","3d227b4c":"y_test[:5]","3e205926":"#y_predicted for test transformation\ny_pred_trans = []\nfor pred in y_predicted:\n    if pred > 0.5:\n        y_pred_trans.append(1)\n    else:\n        y_pred_trans.append(0)\n    \ny_pred_trans[0:10]","9d87a04f":"y_predicted_train = model.predict(X_train)\ny_pred_train = []\nfor pred in y_predicted_train:\n    if pred > 0.5:\n        y_pred_train.append(1)\n    else:\n        y_pred_train.append(0)\ny_pred_train[0:5]","509f307e":"# Plot training and validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'], loc = 'upper left')\nplt.show()\n\n# Plot training and validation accuracy values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'], loc = 'upper left')\nplt.show()","2621f749":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools\nfrom sklearn.metrics import f1_score\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    plt.figure(figsize=(5,4))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","4fe50838":" from sklearn.metrics import fbeta_score\nfbeta_score(y_test, y_pred_trans, average='weighted', beta=0.5)\n# Compute confusion matrix  \ncm_matrix = confusion_matrix(y_test, y_pred_trans)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cm_matrix, classes=['Not Fraud','Fraud'],normalize= False,  title='One Neuron Network Confusion matrix')","661eb7e4":"import matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\ndef plot_metrics(history):\n    metrics = ['auc', 'prc', 'precision', 'recall']\n    metric_val = [history.history[f][-1] for f in metrics]\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(3,2,n+1)\n        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric],\n                 color=colors[0], linestyle=\"--\", label='Val' )\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':\n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n\n        plt.legend()","56bee8d8":"plot_metrics(history)","4d499b4b":"metrics = ['auc', 'prc', 'precision', 'recall','tp','accuracy','fp','tn','fn']\nmetric_val = [history.history[f][-1] for f in metrics]\nprint('Accuracy: {:.3%}'.format(metric_val[5]))\nprint('Precision-Recall: {:.3%}'.format(metric_val[1]))\nprint('Recall: {:.3%}'.format(metric_val[3]))\nprint('Precision: {:.3%}'.format(metric_val[2]))\nprint('False Positive: {:.3}'.format(metric_val[6]))\nprint('False Negative: {:.3}'.format(metric_val[8]))\nprint('True Positive: {}'.format(metric_val[4]))\nprint('True Negative: {}'.format(metric_val[7]))","85af0fa9":"print(\"False Negative Error(Type II): {:.3%}\".format((metric_val[8])\/(metric_val[8] + metric_val[4])))\nprint(\"True Negative Error(Type I): {:.3%}\".format((metric_val[6])\/(metric_val[6] + metric_val[7])))","b709ceaf":"from sklearn.metrics import roc_curve, roc_auc_score\nr_probs = [0 for _ in range(len(y_test))]\n# Calculate the AUROC\nr_auc = roc_auc_score(y_test, r_probs)\non_auc = roc_auc_score(y_test, y_pred_trans) \nprint('Random Chance Prediction AUROC: ' + str(r_auc))\nprint('One Neuron Network Prediction AUROC: ' + str(on_auc))\nr_fpr, r_tpr, threshold = roc_curve(y_test, r_probs) \non_fpr, on_tpr, threshold = roc_curve(y_test, y_pred_trans)\nthreshold[0:10]\n\nfrom matplotlib.pyplot import figure\n\nplt.figure(figsize = (10,8))\nplt.plot(r_fpr, r_tpr, linestyle = \"--\", label = 'Random Chance Prediction AUROC: %0.3f' %r_auc)\nplt.plot(on_fpr, on_tpr, marker = \".\", label = 'One Neuron Network AUROC:  %0.3f' %on_auc)\n\n# TITLE\nplt.title('ROC Plot')\n# Axis Label\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# Legend\nplt.legend()\n# show plot\nplt.show()","c8c71d79":"Based on the data <b>0.17%<\/b> only indicate that a fraud which this data classified as <b>UNBALANCE DATA<\/b>","0f89b450":"### Split Train and Test Data","94b2ad7c":"#### One Hidden Layer Neural Network","6d563830":"<b>\"THANK YOU FOR READING MY CREDIT CARD FRAUD DETECTION ANALYSIS USING Neural Network if you some improvement to my analysis.Please Comment Below!\" <br><b\/>\n<b>\"Please give me an upvote if you find this useful it will help me alot especially finding jobs to this field. Since im Career Shifter from Electrical Engineering to Data Science \"<\/b>","d673c13f":"Based on the graph above our model is on <b>goodfit based on Loss Function MOdel.<\/b>","64e69530":"#### Determine if the Model is Good fit, Overfit or Underfit","47168693":"### Conclusion and Recomendation","0612e267":"### Evaluation of Model","5c4a64da":"#### Conclusion:\n\n   * <b>AUC-ROC Score is 90.5%.<\/b> <br>\n   * <b>Accuracy is 99.948%.<\/b> <br>\n   * <b>Recall or True Positive Rate is 81.55%.<\/b> which is very good on the imbalance data of 0.17% with Fraud Result. <br>\n   * Since our <b>False Negative Ratio or Type II Error is  or True Positive Rate is 18.45%<\/b><br>. It is the probability when a transaction will wrongly classified as a Fraud even it is not actually Fraud. \n   \n #### Suggestion:\n   \"Increase the dataset and also increase target with a fraud to increase the Recall and to minimize the TypeII Error\"","f52a7918":"### Data Quality Check","c524995c":"### Data Standardization","1acc072f":"### Modeling","352b3a75":"#### Checking if there's Duplicate on the Dataset","827ecb6f":"### Correlation of All Feature","bb54846e":"### Data Exploratory Analysis"}}