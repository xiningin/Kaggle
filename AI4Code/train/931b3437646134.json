{"cell_type":{"d8b9edf3":"code","1b7478c0":"code","29ef56ad":"code","f2a4c5f8":"code","75ab885e":"code","1c8b2423":"code","739cad43":"code","ad7b25c0":"code","ed1bd90e":"code","2bfdfc2d":"code","0cfc8a75":"code","a06942e5":"code","4ad384b4":"code","0d384e4d":"code","53aa6bec":"code","80918d48":"code","5befd65e":"code","29361c81":"code","b22937ed":"code","dd745b3c":"markdown","22f5c3e7":"markdown","0738688c":"markdown","28caf301":"markdown","f3c14d70":"markdown","8871d84e":"markdown","87a9668d":"markdown","8edd1b21":"markdown","e48caba3":"markdown","1c2afb61":"markdown","b4ad4326":"markdown","bee0165c":"markdown","6b4b9a11":"markdown","f19835c2":"markdown","d8173a75":"markdown","d5db774b":"markdown","20513623":"markdown","9d335d02":"markdown"},"source":{"d8b9edf3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","1b7478c0":"path = '\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv'\ndf = pd.read_csv(path)\ndf.head(3)","29ef56ad":"df.info()","f2a4c5f8":"df.describe()","75ab885e":"df.loc[df[\"Glucose\"] == 0.0, \"Glucose\"] = np.NAN\ndf.loc[df[\"BloodPressure\"] == 0.0, \"BloodPressure\"] = np.NAN\ndf.loc[df[\"SkinThickness\"] == 0.0, \"SkinThickness\"] = np.NAN\ndf.loc[df[\"Insulin\"] == 0.0, \"Insulin\"] = np.NAN\ndf.loc[df[\"BMI\"] == 0.0, \"BMI\"] = np.NAN\ndf.isna().sum() \/ len(df)","1c8b2423":"missing_columns = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n\ndef random_imputation(df, feature):\n\n    number_missing = df[feature].isnull().sum()\n    observed_values = df.loc[df[feature].notnull(), feature]\n    df.loc[df[feature].isnull(), feature + '_imp'] = np.random.choice(observed_values, number_missing, replace = True)\n    \n    return df\n\nfor feature in missing_columns:\n    df[feature + '_imp'] = df[feature]\n    df = random_imputation(df, feature)","739cad43":"from sklearn import linear_model\n\nrandom_data = pd.DataFrame(columns = [\"Ran\" + name for name in missing_columns])\n\nfor feature in missing_columns:\n        \n    random_data[\"Ran\" + feature] = df[feature + '_imp']\n    parameters = list(set(df.columns) - set(missing_columns) - {feature + '_imp'})\n    \n    model = linear_model.LinearRegression()\n    model.fit(X = df[parameters], y = df[feature + '_imp'])\n    \n    #Standard Error of the regression estimates is equal to std() of the errors of each estimates\n    predict = model.predict(df[parameters])\n    std_error = (predict[df[feature].notnull()] - df.loc[df[feature].notnull(), feature + '_imp']).std()\n    \n    random_predict = np.random.normal(size = df[feature].shape[0], \n                                      loc = predict, \n                                      scale = std_error)\n    random_data.loc[(df[feature].isnull()) & (random_predict > 0), \"Ran\" + feature] = random_predict[(df[feature].isnull()) & \n                                                                            (random_predict > 0)]\n","ad7b25c0":"sns.set()\nfig, axes = plt.subplots(nrows = 5, ncols = 2)\nfig.set_size_inches(16, 10)\n\nfor index, variable in enumerate(missing_columns):\n    sns.histplot(df[variable].dropna(), kde = False, ax = axes[index, 0])\n    sns.histplot(random_data[\"Ran\" + variable], kde = False, ax = axes[index, 0], color = 'red')\n    axes[index, 0].set(xlabel = variable + \" \/ \" + variable + '_imp')\n    \n    sns.boxplot(data = pd.concat([df[variable], random_data[\"Ran\" + variable]], axis = 1),\n                ax = axes[index, 1])\n    \n    plt.tight_layout()","ed1bd90e":"imputed_df = pd.concat([random_data, df[['Outcome', 'Pregnancies', 'Age', 'DiabetesPedigreeFunction']]], axis=1)\ncols = imputed_df.columns\n# reorder columns in the dataset\nimputed_df = imputed_df[cols[:5].tolist() + cols[6:].tolist() + cols[5:6].tolist()]\ncols = imputed_df.columns\nimputed_df.columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'] + cols[5:].tolist()\nimputed_df.head(3)","2bfdfc2d":"imputed_df[imputed_df.columns[:-1]].var().plot(kind='bar', figsize=(16,4), colormap='Set2',\n                                          xlabel='Features', ylabel='Variance', title='Variance among features');","0cfc8a75":"fig, axes = plt.subplots(1,1, figsize=(14,5))\n\ng = sns.countplot(x='Pregnancies', hue='Outcome', data=imputed_df, palette=sns.color_palette(), ax=axes);\ng.set_title('Pregnancies by Outcome');","a06942e5":"list_of_metrics = [['Glucose','BloodPressure'], ['SkinThickness', 'Insulin'], ['BMI', 'Age']]\n\ndef box_plot_func(axes, metric):\n    bp_dict = imputed_df.boxplot(column=f'{metric}', by='Outcome', fontsize=12, ax=axes,\\\n                         vert=False, return_type='both', patch_artist = True);\n    # colors for boxplots\n    colors = ['b','r']\n    for row_key, (ax,row) in bp_dict.iteritems():\n        ax.set_xlabel('')\n        for i,box in enumerate(row['boxes']):\n            box.set_facecolor(colors[i])\n    \n    ax.set_title(\"Boxplot of \" + f\"{metric}\")\n    ax.set_xlabel(f\"{metric}\")\n    ax.set_ylabel('Rent amount')\n    plt.suptitle(\"\")\n\n    \nfig, axes = plt.subplots(3,2, figsize=(18,20))\nfor i in range(3):\n    for j in range(2):\n        box_plot_func(axes[i,j], list_of_metrics[i][j])","4ad384b4":"f, (ax) = plt.subplots(1, figsize=(18,8))\nsns.heatmap(imputed_df.corr(), ax=ax, annot = True);","0d384e4d":"fig, (axes) = plt.subplots(2, 4, figsize=(18,6)) \ncol = 0\nfor i in range(2):\n    for j in range(4):\n        sns.histplot(imputed_df[imputed_df.columns[col]], ax=axes[i,j], color='blue',  kde=True)\n        col += 1","53aa6bec":"imputed_df.groupby('Outcome')['Outcome'].size().plot(kind='pie', autopct='%1.1f%%');","80918d48":"from sklearn.model_selection import train_test_split\n\nX, y = imputed_df.drop('Outcome', axis=1), imputed_df['Outcome']\nx_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n# x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, stratify=y_temp, test_size=0.5)\nprint('Train', x_train.shape, y_train.shape)\n# print('Development', x_valid.shape, y_valid.shape)\nprint('Test', x_test.shape, y_test.shape)","5befd65e":"from sklearn.feature_selection import chi2, f_classif, mutual_info_classif, SelectKBest, SelectPercentile\n\nclass UnivariateFeatureSelction:\n    def __init__(self, n_features, scoring):\n        valid_scoring = {\n            'f_classif': f_classif,\n            'chi2': chi2,\n            'mutual_info_classif': mutual_info_classif\n        }\n        \n        self.selection = SelectPercentile(\n            valid_scoring[scoring], percentile=int(n_features * 100))\n        \n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n\n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n\nufs = UnivariateFeatureSelction(n_features=0.1,scoring=\"chi2\")\n\nufs.fit(x_train, y_train)\nx_train_transformed = ufs.transform(x_train)\n# x_valid_transformed = ufs.transform(x_valid)\nx_test_transformed  = ufs.transform(x_test)","29361c81":"from sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import cross_val_score, KFold\n\nclf = make_pipeline(StandardScaler(), LogisticRegression())\nroc_auc_score = cross_val_score(clf, x_train_transformed, y_train, cv=10, scoring='roc_auc')\nroc_auc_score_min = np.min(roc_auc_score)\nroc_auc_score_max = np.max(roc_auc_score)\nroc_auc_score_mean = np.mean(roc_auc_score)\nprint(f'Min score: {roc_auc_score_min}, Max score: {roc_auc_score_max}, Mean: {roc_auc_score_mean}')","b22937ed":"### empty list\nclfs = []\n\nclfs.append((\"RandomForestClassifier\",\n             Pipeline([(\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\",\n             Pipeline([(\"GradientBoosting\", GradientBoostingClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\",\n             Pipeline([(\"DART\", DecisionTreeClassifier())])))\n\nclfs.append((\"LogisticRegression\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogisticRegression\", LogisticRegression())]))) \n\nclfs.append((\"SVM\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"SVM\", SVC())]))) \n\n### Metrics\n# roc_auc\nscoring = 'roc_auc'\nn_folds = 10\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds)\n    cv_results = cross_val_score(model, x_train_transformed, y_train, cv= kfold, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n#     msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n#     print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(16,6))\nfig.suptitle('Classification Algorithm Comparison', fontsize=14)\nax = fig.add_subplot(111)\nsns.boxplot(data=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithm\", fontsize=14)\nax.set_ylabel(\"ROC-AUC of Models\", fontsize=14)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","dd745b3c":"### Train\/dev\/test ","22f5c3e7":"### Missing values\n- Insulin and SkinThickness have the highest percentage of missing values among others","0738688c":"#### Stochastic Regression Imputation","28caf301":"### Univariate analysis\n#### Pregnancies feature","f3c14d70":"We can observe from the plots above that we have introduced some degree of variability into the variables and retained the native distribution as well.","8871d84e":"### Comparison of all models","87a9668d":"### Feature selection\n- f_classification\n- Chi2\n- Mutual Information","8edd1b21":"### Regression Imputation\n#### Random Imputation","e48caba3":"#### Box plots \n- It seems Insulin and Glucose have the most effect on the outcome","1c2afb61":"### EDA","b4ad4326":"### Our new dataset","bee0165c":"### What insights can we get out of this table?\n- Minimum of Plasma glucose concentration, Diastolic blood pressure, Triceps skinfold thickness, 2-Hour serum insulin and Body mass index equals to zero!!!\n- I don't know about Glucose, Insulin, SkinThickness but I do know that BloodPressure and BMI cannot be zero\n- So it's better to replace those 0's with Nans","6b4b9a11":"### Pearson correlation for binary categorical variable","f19835c2":"### Distribution of our features","d8173a75":"### Model","d5db774b":"### Proportions of 1's and 0's\n- Dataset is skewed","20513623":"- The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset","9d335d02":"#### Variance among features\n- Insulin feature has a really high variance"}}