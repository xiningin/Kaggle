{"cell_type":{"59d11cf9":"code","d62f99c6":"code","7dc90f19":"code","bb9dc5ee":"code","c5593674":"code","0041f565":"code","a041bf28":"code","f4e5d6e0":"code","ac4c04e8":"code","8b8a48f5":"code","032f4667":"code","9b4aac81":"code","9bed4d39":"code","9df8e2d8":"code","41e235d5":"code","2a6c41f1":"code","368e8b93":"code","474922c9":"code","ee291eb0":"code","91154f95":"code","4fa58540":"code","b24794c1":"code","f909fd9c":"code","1654df9b":"code","09f41f38":"code","ba5d1565":"code","d2166eb1":"code","4365e490":"code","277b6ad7":"code","f7e3a86b":"code","d3c37a96":"code","8b1bbc4d":"code","42a6916f":"code","b7f134dd":"code","ac256ab6":"code","1b3bbe4d":"code","e3c9ee69":"code","3ec2afea":"code","05214c91":"code","ac3df184":"code","48973aa5":"code","0cf8cdeb":"code","dbfa0eec":"code","5aa95391":"code","86c946e0":"code","6c96aa56":"code","5c3f7db1":"code","1eda050f":"code","e7bff62d":"code","e51ce3b8":"code","e6c8fdf4":"code","d13cbaff":"code","091d163d":"code","2d938ab2":"code","f2a811fe":"code","787af418":"code","d253e089":"code","a804f74a":"code","dabfef79":"code","3aa8feea":"code","a1087069":"code","d2187ac8":"code","8b94c658":"code","668b4ba4":"code","b7dea409":"code","29320ea1":"code","9b9050cc":"code","eac7c8ee":"code","4a3ef5a7":"code","061ae70e":"code","7651b686":"code","5db10cf2":"code","3c2f8913":"code","d8b34476":"code","f9815af7":"code","43d2dd33":"code","9d5ae249":"code","6bdbee2a":"code","6db6f1e5":"code","23585c92":"code","cf08f685":"code","8dcd50a1":"code","326785b1":"code","56b8fa6a":"code","956a95d6":"code","9f19e51a":"code","5a79f39d":"code","5b22c400":"code","85d47b32":"code","bacd6cdb":"code","3d65ab3b":"code","067dead4":"code","84501141":"code","2ac2a950":"code","bb5e2fa6":"code","15657a27":"code","c3d6476c":"code","f91da96d":"code","5f6503ff":"code","a7e85f86":"code","e0f48eb3":"code","8e9fe88f":"code","4ae90be2":"code","8200c600":"code","8d588714":"markdown","7b8f3784":"markdown","5452fdad":"markdown","9d68aab9":"markdown","4fc8f7fc":"markdown","ec68f8b2":"markdown","66a9afbd":"markdown","1e0baa61":"markdown","987b0bdc":"markdown","f1aad6e9":"markdown","e8b48cce":"markdown","56c6b0b4":"markdown","a5d511fe":"markdown","e55168b9":"markdown","84c5f9ca":"markdown","4357e109":"markdown","a211c745":"markdown","74f2a736":"markdown","fb7fba58":"markdown","97052a46":"markdown","c708c438":"markdown","3a3c17c2":"markdown","bf5e0056":"markdown","723ecd08":"markdown","ccf8e766":"markdown","333770f4":"markdown","202bbba2":"markdown","27233436":"markdown","3082b10a":"markdown","c686665e":"markdown","0989ba4a":"markdown","8d0829b3":"markdown","298db418":"markdown","bf9f2b96":"markdown","811a7267":"markdown","3d8975d6":"markdown","bc7385ec":"markdown"},"source":{"59d11cf9":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","d62f99c6":"# Read train and test data with pd.read_csv():\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","7dc90f19":"# copy data in order to avoid any change in the original:\ntrain = train_data.copy()\ntest = test_data.copy()","bb9dc5ee":"train.head(30)","c5593674":"test.head()","0041f565":"train.info()","a041bf28":"train.describe().T","f4e5d6e0":"train['Pclass'].value_counts()","ac4c04e8":"train['Sex'].value_counts()","8b8a48f5":"train['SibSp'].value_counts()","032f4667":"train['Parch'].value_counts()","9b4aac81":"train['Ticket'].value_counts()","9bed4d39":"train['Cabin'].value_counts()","9df8e2d8":"train['Embarked'].value_counts()","41e235d5":"sns.barplot(x = 'Pclass', y = 'Survived', data = train);","2a6c41f1":"sns.barplot(x = 'SibSp', y = 'Survived', data = train);","368e8b93":"sns.barplot(x = 'Parch', y = 'Survived', data = train);","474922c9":"sns.barplot(x = 'Sex', y = 'Survived', data = train);","ee291eb0":"train.head()","91154f95":"# We can drop the Ticket feature since it is unlikely to have useful information\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)\n\ntrain.head()","4fa58540":"train.describe().T","b24794c1":"# It looks like there is a problem in Fare max data. Visualize with boxplot.\nsns.boxplot(x = train['Fare']);","f909fd9c":"Q1 = train['Fare'].quantile(0.25)\nQ3 = train['Fare'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_limit = Q1- 1.5*IQR\nlower_limit\n\nupper_limit = Q3 + 1.5*IQR\nupper_limit","1654df9b":"# observations with Fare data higher than the upper limit:\n\ntrain['Fare'] > (upper_limit)","09f41f38":"train.sort_values(\"Fare\", ascending=False).head()","ba5d1565":"# In boxplot, there are too many data higher than upper limit; we can not change all. Just repress the highest value -512- \ntrain['Fare'] = train['Fare'].replace(512.3292, 300)","d2166eb1":"train.sort_values(\"Fare\", ascending=False).head()","4365e490":"test.sort_values(\"Fare\", ascending=False)","277b6ad7":"test['Fare'] = test['Fare'].replace(512.3292, 300)","f7e3a86b":"test.sort_values(\"Fare\", ascending=False)","d3c37a96":"train.isnull().sum()","8b1bbc4d":"train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].mean())","42a6916f":"test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].mean())","b7f134dd":"train.isnull().sum()","ac256ab6":"test.isnull().sum()","1b3bbe4d":"train.isnull().sum()","e3c9ee69":"test.isnull().sum()","3ec2afea":"train[\"Embarked\"].value_counts()","05214c91":"# Fill NA with the most frequent value:\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")","ac3df184":"test[\"Embarked\"] = test[\"Embarked\"].fillna(\"S\")","48973aa5":"train.isnull().sum()","0cf8cdeb":"test.isnull().sum()","dbfa0eec":"test[test[\"Fare\"].isnull()]","5aa95391":"test[[\"Pclass\",\"Fare\"]].groupby(\"Pclass\").mean()","86c946e0":"test[\"Fare\"] = test[\"Fare\"].fillna(12)","6c96aa56":"test[\"Fare\"].isnull().sum()","5c3f7db1":"# Create CabinBool variable which states if someone has a Cabin data or not:\n\ntrain[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)\n\ntrain.head()","1eda050f":"train.isnull().sum()","e7bff62d":"test.isnull().sum()","e51ce3b8":"# Map each Embarked value to a numerical value:\n\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)","e6c8fdf4":"train.head()","d13cbaff":"# Convert Sex values into 1-0:\n\nfrom sklearn import preprocessing\n\nlbe = preprocessing.LabelEncoder()\ntrain[\"Sex\"] = lbe.fit_transform(train[\"Sex\"])\ntest[\"Sex\"] = lbe.fit_transform(test[\"Sex\"])","091d163d":"train.head()","2d938ab2":"train[\"Title\"] = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest[\"Title\"] = test[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)","f2a811fe":"train.head()","787af418":"train['Title'] = train['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')","d253e089":"test['Title'] = test['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ntest['Title'] = test['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')","a804f74a":"train.head()","dabfef79":"test.head()","3aa8feea":"train[[\"Title\",\"PassengerId\"]].groupby(\"Title\").count()","a1087069":"train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","d2187ac8":"# Map each of the title groups to a numerical value\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 5}\n\ntrain['Title'] = train['Title'].map(title_mapping)","8b94c658":"train.isnull().sum()","668b4ba4":"test['Title'] = test['Title'].map(title_mapping)","b7dea409":"test.head()","29320ea1":"train = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","9b9050cc":"train.head()","eac7c8ee":"bins = [0, 5, 12, 18, 24, 35, 60, np.inf]\nmylabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = mylabels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = mylabels)","4a3ef5a7":"# Map each Age value to a numerical value:\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n","061ae70e":"train.head()","7651b686":"#dropping the Age feature for now, might change:\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","5db10cf2":"train.head()","3c2f8913":"# Map Fare values into groups of numerical values:\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])","d8b34476":"# Drop Fare values:\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","f9815af7":"train.head()","43d2dd33":"train.head()","9d5ae249":"train[\"FamilySize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1","6bdbee2a":"test[\"FamilySize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","6db6f1e5":"# Create new feature of family size:\n\ntrain['Single'] = train['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntrain['SmallFam'] = train['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ntrain['MedFam'] = train['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntrain['LargeFam'] = train['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","23585c92":"train.head()","cf08f685":"# Create new feature of family size:\n\ntest['Single'] = test['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntest['SmallFam'] = test['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ntest['MedFam'] = test['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntest['LargeFam'] = test['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","8dcd50a1":"test.head()","326785b1":"# Convert Title and Embarked into dummy variables:\n\ntrain = pd.get_dummies(train, columns = [\"Title\"])\ntrain = pd.get_dummies(train, columns = [\"Embarked\"], prefix=\"Em\")","56b8fa6a":"train.head()","956a95d6":"test = pd.get_dummies(test, columns = [\"Title\"])\ntest = pd.get_dummies(test, columns = [\"Embarked\"], prefix=\"Em\")","9f19e51a":"test.head()","5a79f39d":"# Create categorical values for Pclass:\ntrain[\"Pclass\"] = train[\"Pclass\"].astype(\"category\")\ntrain = pd.get_dummies(train, columns = [\"Pclass\"],prefix=\"Pc\")","5b22c400":"test[\"Pclass\"] = test[\"Pclass\"].astype(\"category\")\ntest = pd.get_dummies(test, columns = [\"Pclass\"],prefix=\"Pc\")","85d47b32":"train.head()","bacd6cdb":"test.head()","3d65ab3b":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","067dead4":"x_train.shape","84501141":"x_test.shape","2ac2a950":"def base_models_1(train):\n    \n       \n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    \n    predictors = train.drop(['Survived', 'PassengerId'], axis=1)\n    target = train[\"Survived\"]\n    \n    x_train, x_test, y_train, y_test = train_test_split(predictors, target, \n                                                    test_size = 0.22, \n                                                    random_state = 42)\n    \n    #results = []\n    \n    names = [\"LogisticRegression\",\"GaussianNB\",\"KNN\",\"LinearSVC\",\"SVC\",\n             \"CART\",\"RF\",\"GBM\"]\n    \n    \n    classifiers = [LogisticRegression(),GaussianNB(), KNeighborsClassifier(),LinearSVC(),SVC(),\n                  DecisionTreeClassifier(),RandomForestClassifier(), GradientBoostingClassifier()]\n    \n    \n    for name, clf in zip(names, classifiers):\n\n        model = clf.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        acc = accuracy_score(y_test, y_pred)\n        msg = \"%s: %f\" % (name, acc)\n        print(msg)","bb5e2fa6":"base_models_1(train)","15657a27":"def base_models(train):\n    \n       \n    from sklearn.model_selection import cross_val_score, KFold \n    predictors = train.drop(['Survived', 'PassengerId'], axis=1)\n    target = train[\"Survived\"]\n    \n        \n    results = []\n    \n    names = [\"LogisticRegression\",\"GaussianNB\",\"KNN\",\"LinearSVC\",\"SVC\",\n             \"CART\",\"RF\",\"GBM\",\"XGBoost\",\"LightGBM\",\"CatBoost\"]\n    \n    \n    classifiers = [LogisticRegression(),GaussianNB(), KNeighborsClassifier(),LinearSVC(),SVC(),\n                  DecisionTreeClassifier(),RandomForestClassifier(), GradientBoostingClassifier(),\n                  XGBClassifier(), LGBMClassifier(), CatBoostClassifier(verbose = False)]\n    \n    \n    for name, clf in zip(names, classifiers):\n        \n        kfold = KFold(n_splits=10, random_state=1001)\n        cv_results = cross_val_score(clf, predictors, target, cv = kfold, scoring = \"accuracy\")\n        results.append(cv_results)\n        msg = \"%s: %f (%f)\" % (name, (cv_results.mean())*100, cv_results.std())\n        \n        print(msg)","c3d6476c":"base_models(train)","f91da96d":"gb_params = {\n        'n_estimators': [200, 500],\n        'subsample': [0.6, 1.0],\n        'max_depth': [2,5,8],\n        'learning_rate': [0.1,0.01,0.02],\n        \"min_samples_split\": [2,5,10]}","5f6503ff":"gb = GradientBoostingClassifier()\n\ngb_cv_model = GridSearchCV(gb, gb_params, cv = 10, n_jobs = -1, verbose = 2)","a7e85f86":"gb_cv_model.fit(x_train, y_train)","e0f48eb3":"gb = GradientBoostingClassifier(learning_rate = gb_cv_model.best_params_[\"learning_rate\"], \n                    max_depth = gb_cv_model.best_params_[\"max_depth\"],\n                    min_samples_split = gb_cv_model.best_params_[\"min_samples_split\"],\n                    n_estimators = gb_cv_model.best_params_[\"n_estimators\"],\n                    subsample = gb_cv_model.best_params_[\"subsample\"])","8e9fe88f":"gb_tuned =   gb.fit(x_train,y_train)","4ae90be2":"y_pred = gb_tuned.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)\n","8200c600":"test","8d588714":"### Embarked & Title","7b8f3784":"### Family Size","5452fdad":"#### Parch vs survived:","9d68aab9":"### Ticket","4fc8f7fc":"### Visualization","ec68f8b2":"# Data Understanding (Exploratory Data Analysis)","66a9afbd":"# Modeling, Evaluation and Model Tuning","1e0baa61":"## Deleting Unnecessary Variables","987b0bdc":"### Age","f1aad6e9":"### Fare","e8b48cce":"#### Sex vs survived:","56c6b0b4":"# Deployment","a5d511fe":"# Data Preparation","e55168b9":"## Spliting the train data","84c5f9ca":"### AgeGroup","4357e109":"**Variable Notes:**\n\nPclass: A proxy for socio-economic status (SES)\n- 1st = Upper\n- 2nd = Middle\n- 3rd = Lower\n\nAge: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nSibSp: The dataset defines family relations in this way...\n- Sibling = brother, sister, stepbrother, stepsister\n- Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nParch: The dataset defines family relations in this way...\n- Parent = mother, father\n- Child = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","a211c745":"### Classes of some categorical variables","74f2a736":"### Cabin","fb7fba58":"### Basic summary statistics about the numerical data","97052a46":"In general, barplot is used for categorical variables while histogram, density and boxplot are used for numerical data.","c708c438":"#### Pclass vs survived:","3a3c17c2":"### Name - Title","bf5e0056":"### Sex","723ecd08":"## Analysis and Visualization of Numeric and Categorical Variables","ccf8e766":"## Feature Engineering","333770f4":"### Pclass","202bbba2":"## Outlier Treatment","27233436":"## Variable Transformation","3082b10a":"## Loading Data","c686665e":"### Embarked","0989ba4a":"## Importing Librarires","8d0829b3":"#### SibSp vs survived:","298db418":"### Embarked","bf9f2b96":"### Fare","811a7267":"## Missing Value Treatment","3d8975d6":"**Variables and Their Types:**\n\nSurvival: Survival -> 0 = No, 1 = Yes\n\nPclass: Ticket class -> 1 = 1st, 2 = 2nd, 3 = 3rd\n\nSex: Sex\n\nAge: Age in years\n\nSibSp: # of siblings \/ spouses aboard the Titanic\n\nParch: # of parents \/ children aboard the Titanic\n\nTicket: Ticket number\n\nFare: Passenger fare\n\nCabin: Cabin number\n\nEmbarked: Port of Embarkation -> C = Cherbourg, Q = Queenstown, S = Southampton","bc7385ec":"# **Titanic Survival Prediction and and comparison with different models:**\n\nUse machine learning to create a model that predicts which passengers survived the Titanic shipwreck."}}