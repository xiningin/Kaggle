{"cell_type":{"12112c64":"code","50b8a164":"code","bbb121f0":"code","dbe40584":"code","e3b574f7":"code","bff4d74e":"code","a2a0eaf2":"code","c3c2f974":"code","e98aecee":"code","50913cc2":"code","d8802e8f":"code","8aaa4e9b":"code","0403c27c":"code","7271c934":"code","6c1012f7":"code","f9207b0a":"code","98636509":"code","698ffc46":"code","5827beda":"code","fe93000e":"code","b1bf3684":"code","013fa0ae":"code","971221d0":"code","b3e4405a":"code","41288ee2":"code","0c97371b":"code","402cc05e":"code","fa6ac815":"code","ef5f85fb":"code","a66b2102":"code","d2713fcf":"code","b5b6a6d2":"code","eec22e3b":"code","1afff13d":"code","29040dc5":"code","ac1c488f":"code","185e285f":"code","0a73c25a":"code","d1fae325":"code","9935a64b":"code","8feb3f01":"code","87122253":"code","8ac25f91":"code","f3eada49":"code","82be8e9e":"code","8f51ae61":"code","b4024c28":"code","ef368906":"code","906589e1":"code","4e3623d0":"code","26076fa9":"code","ef3a8a7f":"code","b286beae":"code","f9e3e26f":"code","3e189b24":"code","3bffa0a1":"code","d75cf8d6":"code","b08e5752":"code","e75dfb7d":"code","1b1f7a9e":"code","53f439a1":"code","2878f0e7":"code","e8c653a4":"code","33d38910":"code","cd00127b":"code","83a67a92":"code","c010884a":"code","752dcf61":"code","dd3650e2":"code","0e729719":"code","401be7e5":"code","26a49c3b":"code","f9a94afa":"code","045c9dea":"code","cdcff2d4":"code","d507eb63":"code","005897a2":"code","3fc79668":"code","f7162701":"code","b1d09506":"code","3fe45727":"code","0c561d01":"code","50c4ccc4":"code","beab16e9":"code","770e2b4c":"code","67333aaf":"code","38ef2965":"code","5d5785ce":"code","48d007af":"code","ab5762a3":"code","2276f059":"code","4886b94a":"code","4f373449":"code","12f78b93":"code","d7e39d82":"code","743c1aff":"code","d99bc00d":"code","9516d43d":"code","05b522f7":"code","663a8985":"code","540a6f5c":"code","b021cb72":"code","537be076":"code","4e3a39ca":"code","2a4a4cdf":"code","68d61382":"code","658e7eef":"code","e2e0c101":"code","5601a531":"code","2502cfce":"code","a53a7d3f":"code","004b8f0f":"code","d99ae1aa":"code","ffdfcd20":"code","92c9adc1":"code","858df9c7":"code","910b0595":"code","7680d011":"code","e1c3d36e":"code","3d60e56f":"code","f85de404":"code","cf23e014":"code","7ecbb6e8":"code","4e8f1d1d":"code","97d8f76d":"code","9ee0bdf6":"code","40733a44":"code","bff302ff":"code","c0dad3d4":"code","6eeebc46":"code","046da990":"code","a539cf59":"code","3d48707e":"markdown","46730825":"markdown","92a8e647":"markdown","bd62d262":"markdown","24bbf5bb":"markdown","65cca35d":"markdown","2f53b6be":"markdown","cb0772f6":"markdown","f31d1fc7":"markdown","e58b7a67":"markdown","e93a8e37":"markdown","af6f96f9":"markdown","94ac1154":"markdown","fc76d491":"markdown","a4cece19":"markdown","736c701a":"markdown","f1b9e201":"markdown","78ddb6a7":"markdown","7c7f2414":"markdown","3e814410":"markdown","be62b2d6":"markdown","f7bfd4ad":"markdown","ce45d3ea":"markdown","24b5209a":"markdown","5ef7dc7f":"markdown","1ff0ea41":"markdown","2882f15a":"markdown","2feb0111":"markdown","f2470cca":"markdown","b3271491":"markdown","06b5d77f":"markdown","4762ffd9":"markdown","f830bb61":"markdown","ffe677b1":"markdown","5208b485":"markdown","59af2d24":"markdown","4da4d8fe":"markdown","e0f32c02":"markdown","6cb68904":"markdown","f11d95f6":"markdown","21d05a42":"markdown","42e418c0":"markdown","19186ed8":"markdown","eb1ac181":"markdown","9e9ce47d":"markdown","108b0494":"markdown","07182243":"markdown","fdbef1be":"markdown","a3e54fa8":"markdown","dde7ae76":"markdown","76e41533":"markdown","daa26e3e":"markdown","82575442":"markdown","7b502405":"markdown","56102138":"markdown","3428eaa5":"markdown"},"source":{"12112c64":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nfrom pandas import DataFrame\nimport pylab as pl\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","50b8a164":"import keras\nimport tensorflow as tf\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Dropout, Flatten","bbb121f0":"titanic=pd.read_csv(\"..\/input\/titanic.csv\") #Importing Data","dbe40584":"#Checking rows and Column\ntitanic.head()","e3b574f7":"print(titanic['Survived'].unique())\nprint(titanic['Pclass'].unique())\nprint(titanic['SibSp'].unique())\nprint(titanic['Parch'].unique())\nprint(titanic['Cabin'].unique())\nprint(titanic['Embarked'].unique())","bff4d74e":"print(titanic.shape)\ntitanic.info()","a2a0eaf2":"import missingno as msno\nmsno.matrix(titanic)\ntotal = titanic.isnull().sum().sort_values(ascending=False)\npercent_1 = titanic.isnull().sum()\/titanic.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total','%'])\nprint(missing_data.head(5))","c3c2f974":"pd.options.display.float_format = '{:.4f}'.format\ntitanic.describe().T","e98aecee":"sns.countplot(x='Pclass', hue='Survived', data=titanic)","50913cc2":"c = pd.crosstab(titanic.Pclass, titanic.Survived).apply(lambda x: x\/x.sum(), axis=1)\nc[\"odds\"] = c.loc[:, 1] \/ c.loc[:, 0]\nc","d8802e8f":"# So the odds of Survival of Pclass1\/ Pclass 3 can be given as\nClass1=1.7000\nClass2=0.3199\nOdds_Ratio=Class1\/Class2\nOdds_Ratio","8aaa4e9b":"c[\"logodds\"] = np.log(c.odds)\nc","0403c27c":"sns.countplot(x='Sex', hue='Pclass', data=titanic)","7271c934":"pd.crosstab([titanic.Sex, titanic.Survived],titanic.Pclass,margins=True).style.background_gradient(cmap='Wistia')\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.crosstab.html","6c1012f7":"d = pd.crosstab(titanic.Sex, titanic.Survived).apply(lambda x: x\/x.sum(), axis=1)\nd[\"odds\"] = d.loc[:, 1] \/ d.loc[:, 0]\nd","f9207b0a":"d[\"logodds\"] = np.log(d.odds)\nd","98636509":"sns.countplot('Embarked',hue='Survived',data=titanic).set_title('Survival by Embarked')","698ffc46":"pd.crosstab([titanic.Survived, titanic.Pclass,titanic.Embarked],titanic.Sex,margins=True).style.background_gradient(cmap='viridis')","5827beda":"c = pd.crosstab(titanic.Embarked, titanic.Survived).apply(lambda x: x\/x.sum(), axis=1)\nc[\"odds\"] = c.loc[:, 1] \/ c.loc[:, 0]\nc","fe93000e":"sns.boxplot(\"Pclass\", \"Fare\", data=titanic, palette=[\"blue\", \"pink\", \"green\"]).set_title('Fare Distribution by Pclass')","b1bf3684":"plt.figure(figsize = (15,5))\nsns.kdeplot(titanic[\"Fare\"][titanic.Survived == 0], color = \"lightpink\", shade = True)\nsns.kdeplot(titanic[\"Fare\"][titanic.Survived == 1], color = \"lightblue\", shade = True)\nplt.title(\"Fare distribution of Survivors vs. Non Survivors\")\nplt.legend(['Survived = 0', 'Survived = 1'])\n","013fa0ae":"plt.figure(figsize = (15,5))\ng = sns.distplot(titanic[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(titanic[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")\n# As we can see skewness value greater than 1 we can say the data is highly skewed which is because of Class 3 passengers \n# are more in compared to class 1 and class 2. So fares are skewed towards right.","971221d0":"a = sns.FacetGrid(titanic, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 ,titanic['Age'].max()))\na.add_legend()","b3e4405a":"f, ax = plt.subplots(1,2,figsize=(15,8))\nsns.violinplot(\"Pclass\", \"Age\", hue=\"Survived\", data=titanic, split=True, ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0, 110, 10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=titanic, split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0, 110, 10))\nplt.show()","41288ee2":"titanic['Age']=titanic['Age'].fillna(titanic['Age'].median())  #Objective 2","0c97371b":"print(titanic.groupby('Embarked').size())\n#titanic.loc[(titanic.Embarked.isnull()),'Embarked'] = \"S\" \ntitanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace = True)","402cc05e":"titanic['Fare'] = titanic['Fare'].astype(int)  #Converitng fare to int type","fa6ac815":"titanic.drop(['Cabin','PassengerId'],axis=1,inplace=True)  #Objective 1","ef5f85fb":"# SibSp and Parch is highly correlated \nmy_corr=titanic.corr()\nplt.figure(figsize=(10,5))\nsns.heatmap(my_corr,linewidth=0.5)\nplt.show()","a66b2102":"titanic.drop(['Name','Ticket'],axis=1,inplace=True)   #Objective 1 Dropping Name and Ticket ","d2713fcf":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler \npd.get_dummies(titanic['Sex'], prefix='Sex')\ntitanic = pd.concat([titanic, pd.get_dummies(titanic['Sex'], prefix='Sex')], axis=1)","b5b6a6d2":"titanic.head()","eec22e3b":"pd.get_dummies(titanic['Embarked'], prefix='Embarked')\ntitanic = pd.concat([titanic, pd.get_dummies(titanic['Embarked'], prefix='Embarked')], axis=1)","1afff13d":"titanic.head()","29040dc5":"titanic.drop(['Embarked','Embarked_C','Sex','Sex_male'],axis=1,inplace=True) #Objective 1 ","ac1c488f":"titanic['Total_Member']=(titanic['Parch']+titanic['SibSp']+1)","185e285f":"titanic.head()","0a73c25a":"pd.crosstab([titanic.Survived],titanic.Total_Member,margins=True).style.background_gradient(cmap='Wistia')","d1fae325":"axes = sns.factorplot('Total_Member','Survived',data=titanic, aspect = 2.5, )","9935a64b":"# Objective 1 Dropping Columns SibSp and parch as it has already included in Total Member\ntitanic.drop(['SibSp','Parch'],axis=1,inplace=True)  ","8feb3f01":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(titanic)\nvifs = [vif(X.values, i) for i in range(len(X.columns))]\npd.Series(data=vifs, index=X.columns).sort_values(ascending=False)","87122253":"titanic.head()","8ac25f91":"from sklearn.model_selection import train_test_split","f3eada49":"X_train, X_test, Y_train, Y_test = train_test_split(titanic.drop('Survived', axis=1), titanic['Survived'],\\\n                                                    test_size=0.25, random_state=156)","82be8e9e":"from sklearn.linear_model import LogisticRegression","8f51ae61":"logit = LogisticRegression()  #Fit Logistic Regression model.","b4024c28":"logit.fit(X_train, Y_train)","ef368906":"logit.classes_","906589e1":"logit.coef_","4e3623d0":"predictions=logit.predict(X_test)   #Make class predictions.","26076fa9":"logit.score(X_test, Y_test)      #Calculate accuracy score.","ef3a8a7f":"1-logit.score(X_test, Y_test)     #Calculate Error rate.","b286beae":"import sklearn.metrics as metrics    \nfrom sklearn.metrics import accuracy_score","f9e3e26f":"accuracy_score(Y_test, predictions)","3e189b24":"accuracy_score(Y_test, predictions, normalize=False) #Calculate number of correctly classified observations.","3bffa0a1":"len(Y_test) - accuracy_score(Y_test, predictions, normalize=False) #Calculate number of incorrectly classified observations.","d75cf8d6":"from sklearn.metrics import log_loss\nimport numpy as np","b08e5752":"print (\"log_loss\", metrics.log_loss(Y_test, predictions))   #Encode predicted classes and test labels.","e75dfb7d":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","1b1f7a9e":"confusion_mat = confusion_matrix(Y_test, predictions)","53f439a1":"confusion_df = pd.DataFrame(confusion_mat, index=['Actual neg','Actual pos'], columns=['Predicted neg','Predicted pos'])","2878f0e7":"confusion_df","e8c653a4":"_=sns.heatmap(confusion_df, cmap='coolwarm', annot=True)","33d38910":"from sklearn.metrics import precision_score, recall_score","cd00127b":"precision_score(Y_test, predictions)","83a67a92":"recall_score(Y_test, predictions)","c010884a":"from sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve \ny_score = logit.decision_function(X_test)\nprecision, recall, _ = precision_recall_curve(Y_test, y_score)\nPR_AUC = auc(recall, precision)\nplt.figure(figsize=[11,9])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# reference: https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-imbalanced-classification\/#:~:text=The%20Precision%2DRecall%20AUC%20is,a%20model%20with%20perfect%20skill.","752dcf61":"from sklearn.metrics import f1_score","dd3650e2":"f1_score(Y_test, predictions)","0e729719":"from sklearn.metrics import roc_curve, roc_auc_score","401be7e5":"probs = logit.predict_proba(X_test)[::,1] #Let's take probablities from our classifier, instead of classes.","26a49c3b":"auc = roc_auc_score(Y_test, probs)\nprint(auc)","f9a94afa":"fpr, tpr, threshold = roc_curve(Y_test, probs)","045c9dea":"plt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","cdcff2d4":"optimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = threshold[optimal_idx]","d507eb63":"optimal_threshold","005897a2":"new_predictions = np.where(probs>optimal_threshold, 1, 0)","3fc79668":"new_confusion_mat = confusion_matrix(Y_test, new_predictions)","f7162701":"new_confusion_df = pd.DataFrame(new_confusion_mat, index=['Actual neg','Actual pos'], columns=['Predicted neg','Predicted pos'])","b1d09506":"new_confusion_df","3fe45727":"TN=128\nFP=9\nFN=29\nTP=57\nsensitivity = TP\/(TP+FN)\nspecificity = TN\/(TN+FP)\nprint(\"The probability of predicting people survived in titanic\",specificity)\nprint(\"The probability of predicting people survived in titanic correctly is \",sensitivity)","0c561d01":"accuracy_score(Y_test, new_predictions)","50c4ccc4":"log_loss(Y_test, new_predictions)","beab16e9":"Sklearn_Model_Result=[['Scikit_Iteration 1',0.8071,6.66,0.76,0.72,0.742,0.85,0.626],['Aft_Updating_Threshold',0.8295,5.885,'NA','NA','NA','NA','NA']]\nResult_Summary= pd.DataFrame(Sklearn_Model_Result, columns = ['No_of_Iteration','Accuracy', 'Log-Loss','Precision','Recall','F1-Score','AU-ROC','Opt_Threshold'])\nResult_Summary","770e2b4c":"from sklearn.model_selection import train_test_split","67333aaf":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(titanic.drop('Survived', axis=1), titanic['Survived'],\\\n                                                    test_size=0.25, random_state=156)","38ef2965":"import statsmodels.api as sm","5d5785ce":"X_train = sm.add_constant(X_train)   # Adding Constant Term to X_Train ","48d007af":"Y_train.head()","ab5762a3":"logit = sm.GLM(Y_train, X_train, family=sm.families.Binomial())   #Fit Logistic Regression.","2276f059":"result = logit.fit()","4886b94a":"print(result.summary())","4f373449":"print(result.summary2())","12f78b93":"result.params","d7e39d82":"np.exp(result.params)","743c1aff":"result.deviance","d99bc00d":"result.aic","9516d43d":"X_train.drop([\"Fare\"], axis=1, inplace=True)    #Dropping the Fare Column as it has P-value>0.05","05b522f7":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(titanic)\nvifs = [vif(X.values, i) for i in range(len(X.columns))]\npd.Series(data=vifs, index=X.columns).sort_values(ascending=False)","663a8985":"print(sm.GLM(Y_train, X_train, family=sm.families.Binomial()).fit().summary2())","540a6f5c":"X_train.drop([\"Embarked_Q\"], axis=1, inplace=True)   #Dropping Embarked_Q as it has P-value>0.05","b021cb72":"result1= sm.GLM(Y_train, X_train, family=sm.families.Binomial()).fit()","537be076":"print(result1.summary2())","4e3a39ca":"result1.deviance","2a4a4cdf":"result1.aic","68d61382":"X_test1 = sm.add_constant(X_test[['Pclass', 'Age', 'Sex_female','Embarked_S','Total_Member']])","658e7eef":"probabilites = result1.predict(X_test1)","e2e0c101":"probabilites.head()","5601a531":"predicted_classes = probabilites.map(lambda x: 1 if x > 0.5 else 0)\naccuracy = sum(predicted_classes == Y_test) \/ len(Y_test)","2502cfce":"accuracy = sum(predicted_classes == Y_test) \/ len(Y_test)","a53a7d3f":"accuracy","004b8f0f":"log_loss(Y_test, predicted_classes)","d99ae1aa":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","ffdfcd20":"confusion_mat = confusion_matrix(Y_test, predictions)","92c9adc1":"confusion_mat = confusion_matrix(Y_test, predicted_classes)\n","858df9c7":"confusion_df = pd.DataFrame(confusion_mat, index=['Actual neg','Actual pos'], columns=['Predicted neg','Predicted pos'])","910b0595":"confusion_df","7680d011":"_=sns.heatmap(confusion_df, cmap='coolwarm', annot=True)","e1c3d36e":"from sklearn.metrics import precision_score, recall_score","3d60e56f":"precision_score(Y_test, predicted_classes)","f85de404":"recall_score(Y_test, predicted_classes)","cf23e014":"f1_score(Y_test, predicted_classes)","7ecbb6e8":"TN=113\nFP=24\nFN=22\nTP=64\nsensitivity = TP\/(TP+FN)\nspecificity = TN\/(TN+FP)\nprint(\"The probability of predicting people survived in titanic\",specificity)\nprint(\"The probability of predicting people survived in titanic correctly is \",sensitivity)","4e8f1d1d":"Stats_Model_Result=[['Log_Likelyhood',-298,-298.93,-299.23],['Deviance',597.81,597.85,598.47],['Pearson_chi2',677,678,683],['AIC',613.80,611.85,610.46],['BIC',-3695.02,-3701.47,-3707.37]]\nResult_Summary2= pd.DataFrame(Stats_Model_Result, columns = ['Parameters','Stats_Iteration1','Iteration2_FareDropped','Iteration2_EmbarkedQ_Dropped'])\nResult_Summary2","97d8f76d":"X=titanic.drop(['Survived'],axis=1)\nY=titanic.Survived","9ee0bdf6":"from sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(X)","40733a44":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.30, random_state=156)","bff302ff":"display(X_train.shape)\ndisplay(X_test.shape)\ndisplay(Y_train.shape)\ndisplay(Y_test.shape)","c0dad3d4":"#Model Building\nmodel = Sequential()# Initiating an empty neural network\nmodel.add(Dense(100, input_shape=(7,), activation='relu'))# Adding a dense layer with 100 neurons\nmodel.add(Dropout(0.2))# Adding a dropout layer for regularization\nmodel.add(Dense(64, activation='relu'))# Adding a dense layer with 64 neurons\nmodel.add(Dropout(0.2))# Adding a dropout layer for regularization\nmodel.add(Dense(32, activation='relu'))# Adding a dense layer with 32 neurons\nmodel.add(Dropout(0.2))# Adding a dropout layer for regularization\nmodel.add(Dense(16, activation='relu'))# Adding a dense layer with 16 neurons\nmodel.add(Dropout(0.2))# Adding a dropout layer for regularization\nmodel.add(Dense(4, activation='relu'))# Adding a dense layer with 4 neurons\nmodel.add(Dense(1, activation='sigmoid'))# Adding an output layer\nopt = keras.optimizers.Adam(learning_rate=0.001)","6eeebc46":"model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])# Compiling our neural network","046da990":"history = model.fit(X_train,Y_train, batch_size=16,validation_data=(X_test, Y_test),epochs=150)# Fitting our neural network","a539cf59":"def plotHistory(history):\n    print(\"Max. Validation Accuracy\",max(history.history[\"val_accuracy\"]))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()\nplotHistory(history)\n","3d48707e":"#### Assignment Objective 1: First, drop columns from the dataset that are obviously unnecessary for your model.\n#### Assignment Objective 2: Then, assign the dataset's median Age to rows where the age data is missing.","46730825":"1. As we can see from the above Summarized table the AIC Score after every Removal of insignificant Value decreases and ended up with the value of 610.46.\n\n2. Deviance represents Goodness of Fit: Though there is slight increase in deviance value which would make model bit less accurate.","92a8e647":"#### Checking for Multicollinearity using Variance Inflation Factor","bd62d262":"#### Splitting the data into Train Test Data  ","24bbf5bb":"###### Getting Summary Statistics about data.\nWe can see that fare column has a outliers. as we can see 75% of data lies around value 31.000 and max value is 512.3292 ","65cca35d":"#### Calculating the optimal threshold probability from this ROC curve.","2f53b6be":"#### Calculate precision and recall scores","cb0772f6":"Looking at logodds of female and male Passengers, female passengers have positive log odds which can be interpreted as it has greater probability of survival than male passengers.","f31d1fc7":"## Stepwise backward Elimination","e58b7a67":"#### Importing Statsmodel api","e93a8e37":"**Though the model is not perfect and there is still room for improvement.**","af6f96f9":"#### Calculate log loss","94ac1154":"#### Interpretation of above Summary Result:\n###### Coefficients Interpretation\n1. For every one unit change in Pclass , the log odds of survival decreases by 1.1283.\n2. For every one unit increase in Age, the the log odds of being survived decreases by 0.0368\n3. For every one unit increase in fare, the log odds of being survived increaes by 0.0005\n4. Being a Female member on ship, the log odds of survived increaes by 2.7734.\n5. For unit increase in Family Member, the log odds of survival decreases by 0.2243\n\nWe can See P-Value of (Fare,Embarked_Q) > Significance Level of 5%. So I will drop columns based on Higher P-Values First.\n\nFor Comparing Log-Likelihood, Deviance, Pearson chi2 we will look at the other iterations done on this model.","fc76d491":"#### Calculate Confusion Matrix\n","a4cece19":"## Building Model using Scikit Learn: Iteration 1","736c701a":"**Null hypothesis: The coefficients on the parameters (including interaction terms) of the logistic regression modeling log(odds) of Survived as a function of independent features are zero.**\n\n**Alternative hypothesis: At least one of the coefficients on the parameters (including interaction terms) of the logistic regression modeling log(odds) of Survived as a function of independent features are nonzero.**\n\n**Significance level: 5%**\n\n**Confidence interval: 95%**","f1b9e201":"#### Viz 1:The Counplot plotted below shows Class wise survival of Passengers . We can see the survival to death ratio is High for Class 1 Passengers. ","78ddb6a7":"## Exploratory Data Analysis ","7c7f2414":"#### Importing test train split package from sklearn library ","3e814410":"## Problem Statement:\n### Construct a model with Logistic Regression that uses available data about the passengers to predict their survival.","be62b2d6":"#### Importing logistic Regression from sklearn Library","f7bfd4ad":"#### Calculate F1 Score","ce45d3ea":"##### Using get_dummies doing feature transformation of Sex feature (Similar to One Hot Encoding), Embarked Feature ","24b5209a":"#### Viz 7: From the below shown plot we can see People having age 30-40 has much survival rate than unsurvived.People with age 25 to 35 has more survival chance.  \n","5ef7dc7f":"##### Statsmodel Iteration 3","1ff0ea41":"**Neural Network(tensofflow)**","2882f15a":"#### Creating Categories: Combining parch and Sibsb and saving it to Total Member ","2feb0111":"##### Statsmodel Iteration1","f2470cca":"#### Predictors description\n<li> PassengerId <\/li>\n<li> Survived (0 = No; 1 = Yes) <\/li>\n<li> Pclass A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower <\/li>\n<li> Name <\/li>\n<li> Sex <\/li>\n<li> Age <\/li>\n<li> SibSp:  Number of Siblings\/Spouses Aboard <\/li>\n<li> Parch:  Number of Parents\/Children Aboard <\/li>\n<li> Ticket: Ticket Number <\/li>\n<li> Fare:   Passenger Fare (British pound) <\/li>\n<li> Cabin <\/li>\n<li> Embarked:  Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) <\/li>\n\n###### Sibling: Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic \n###### Spouse: Husband or Wife of Passenger Aboard Titanic (Mistresses and FiancesIgnored)\n###### Parent: Mother or Father of Passenger Aboard Titanic\n###### Child: Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic","b3271491":"#### Viz 2: Followed from the above graph. As we all know that Womens and Childrens were given first priority to be getting to life boats and then mens. So does 1st Class Passengers has more Female passengers compared with other two. Lets see throgh vizualization & Crosstab.\nFrom the crosstab we can see that out of 94 1st class Females Passengers and 76  nd Class Females Passengers only 3 in 1st Class and 6 in 2nd class are reported to be not survived. Lets See through Vizualization.","06b5d77f":"We see that the probability that a Male Passengers has lower probability of Survival substantially lower than Female passengers (74.2%). In the odds for a Male Passengers survival being much less than 1 (around 0.2329), while the odds for Female passengers is around 2.8 which is pretty much higher than male odds.","4762ffd9":"#### Viz 8:Created a violin plot to check the distribution of Age across the Survivors vs. Non Survivors for each Class & Gender","f830bb61":"1. From Scikit Result we can see Accuracy of the Model is 80.71% after updating it with Optimum threshold value it is comming 82.95%. The threshold value is obtained after plotting the ROC Curve. On passing the Accuracy Line we get the Cutoff Point which is closest to Top Left Corner Point whose value is 1. \n\n2. The Accuracy increases after we get optial cutoff probability and Log-Loss decreases which is good for the model.\n\n3. Precision Usually says how useful the results are: So I can say my model has 76% ability to identify all relevant instances.  The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.76 precision which is pretty good.\n\n4. Recall says how complete the results are: My model has 72% ability to return only relevant instances.                    So,Of all the passengers that truly survived, how many did we label? We have got recall of 0.72 which is good for this model as it\u2019s above 0.5.\n\n5. F1-Score:It is a Single metric that combines recall and precision using the harmonic mean. So closer the F-Score to 1 better is the result. \n\n6. AU-ROC: 0.85 which is good for our Model.So I can say from my AUC Score that my Survival Classification of Passengers is good.\n\n7. Opt_Threshold: The chosen threshold 0.62 determines the balance between how many false positives and false negatives will result from this classification.","ffe677b1":"The spike in the plot under 50 dollar represents that a lot of passengers who bought the ticket within that range did not survive.\nWhen fare is approximately more than 50-60 dollars, the casualties went down and survival wentup. ","5208b485":"##### From the above crosstab we can see person boarding from C has  43 first class Female out of which 42 survived. ","59af2d24":"#### Importing Packages","4da4d8fe":"We see that the probability that a Passengers of Class 3 has lower probability of Survival substantially lower than the Class 2 and Class 1 Passengers (47.28% and 62.96%). In the odds for a Passengers of Class 3 survival being much less than 1 (around 0.31), while the odds for class 1 is around 1.7 which is pretty much high.","e0f32c02":"#### Viz 3: Lets see if embarked has any influence on survival. \nFrom the countplot plotted below we can see that People who boarded from \"C\" shows higher survival. Lets check if \" Person boarding from C has high Female and Class 1 passengers or not using crosstab.\"","6cb68904":"References:\n1. http:\/\/ismayc.github.io\/teaching\/sample_problems\/multiple_logistic.html\n2. https:\/\/www.statsmodels.org\/stable\/glm.html\n3. https:\/\/www.pythonprogramming.in\/change-box-color-in-boxplot.html\n4. https:\/\/scikit-learn.org\/stable\/supervised_learning.html#supervised-learning","f11d95f6":"###### Logodds Interpretation with some basic understanding:\n**To interpret the log odds when comparing two groups, it is important to remember the following facts:**\n\n1. A probability of 1\/2, an odds of 1, and a log odds of 0 are all equivalent.\n\n2. A positive log odds indicates that the first group being compared has greater odds (and greater probability) than the second group.\n\n3. A negative log odds indicates that the second group being compared has greater odds (and greater probability) than the first group.\n\n4. The scale of the log odds statistic is symmetric in the sense that a log odds of, say, 2, is equivalent in strength to a log odds of -2 (but with the groups swapped in terms of which has the greater probability).\n\n\nSo, From the above Interpretation of logodds we can say Class 1 Passengers has positive log odds which can be interpreted as it has greater probability of survival than Class 2 & Class 3 passengers.","21d05a42":"###### From the above code we can see that along with the original Sex and Embarked column we have also dropped tronsformed column 'Sex_male' and 'Embarked_C' because it won't affect much and will reduce multicollinearity and thus prevent from Dummy Variable trap","42e418c0":"We see that the probability that a Passengers who boarded from Place C has higher Survival rate, higher than passengers boarding from Place Q and S.The possible reason could be the more number of female 1st class passengers from Place Q as we have already looked in above Crosstab.","19186ed8":"### Updating model based on optimal probability threshold","eb1ac181":"#### Reading the file for Analysis ","9e9ce47d":"#### Viz 4:  Fare variation Using Boxplot how it has varied as per class","108b0494":"#### Viz 6: Checking the skewness of Fare","07182243":"#### Calculate Area under the ROC curve and also plot the ROC curve","fdbef1be":"##### Statsmodel Iteration 2","a3e54fa8":"###### Looking after the categories of columns","dde7ae76":"The odds ratio of Survival, comparing Class1 to Class2, is around 5.314. In other words, Class 1 Passengers has around 5.314 times greater odds of survival than Class2 (in the population represented by these data).","76e41533":"#### Viz 5: Fare distributions of survived vs. non survived passengers shown by kernel density estimate plot which shows the distribution of survived and not survived passengers based on fare.","daa26e3e":"###### Few more information about data shape, data types, has null values in it or not","82575442":"###### Checking for missing values total count and percentage of total data along with visualization or I can say another way to check missing values in data easily.\n","7b502405":"## Building Model using Statsmodel(GLM): iteration 2","56102138":"#### Viz 9: combining SibSp and Parch to Total Columns and checking how many mare alone and how many are with family and what are the chance of their survival. The Person with more members or a person with less members or some other.\nFrom the factorplot shown below we can see person with 4 family member seems to have higher chance of Survival and one with six and eight members has least chance of survival","3428eaa5":" #### Calculating Accuracy"}}