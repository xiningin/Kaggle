{"cell_type":{"0741a800":"code","4389c511":"code","43cf158d":"code","662b21aa":"code","9b82519a":"code","58465291":"code","8802112e":"code","f974b2f2":"code","194d6ccf":"code","4dc50a99":"code","be80aa54":"code","3133594b":"code","887641b5":"code","fe94b953":"code","746900d5":"code","7958667d":"code","95f78556":"code","14838bb5":"code","0bc0ac2c":"code","f157c7a1":"code","fec28a1e":"code","059928fb":"code","e44148de":"code","1c28091a":"markdown","26b18dc6":"markdown","0ca3f53f":"markdown","80afb74d":"markdown","a4f40011":"markdown","8d188f88":"markdown","769b6a99":"markdown","0a8d1a8d":"markdown","7458e0f2":"markdown","5e1793cd":"markdown","7c4b242f":"markdown"},"source":{"0741a800":"import numpy as np\nimport pandas as pd\nimport os\nimport re\n\nfrom gensim.models import Word2Vec\nfrom tqdm import tqdm\n\ntqdm.pandas()","4389c511":"f = open('..\/input\/wikitext2\/wiki.train.tokens', \"rt\")\ntext = f.readlines()","43cf158d":"def convertToList(text):\n    res = []\n    for i in range(0,len(text)):\n\n        lis = list(text[i].split(' '))\n        res.append(lis)\n    return res\n\ntext = convertToList(text)","662b21aa":"text[1]","9b82519a":"def preprocessing(text):\n    \n    \n    processed_text = []\n    \n    for tokens in tqdm(text):\n        \n        # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n        processed = re.sub('[^a-zA-Z ]', '', tokens)\n        \n        words = processed.split()\n        \n        # keep words that have length of more than 1 (e.g. gb, bb), remove those with length 1.\n        processed_text.append(' '.join([word for word in words if len(word) > 1]))\n    \n    return processed_text","58465291":"text_string=''\n\nfor i in text:\n    for j in i:\n        text_string += j","8802112e":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(40, 30))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","f974b2f2":"wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(text_string)\nplot_cloud(wordcloud)\n","194d6ccf":"wordcloud.to_file(\"wordcloud.png\")","4dc50a99":"\nw2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=4-1)","be80aa54":"w2v_model.build_vocab(text, progress_per=10000)\n","3133594b":"# Total number of vocab in our custom word embedding\n\nlen(w2v_model.wv.vocab.keys())","887641b5":"# Dimension of each word (we set it to 300 in the above training step)\n\nw2v_model.wv.vector_size\n","fe94b953":"w2v_model.train(text, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)","746900d5":"# Checking out how 'team' is represented (an array of 300 numbers)\n\nw2v_model.wv.get_vector('team')","7958667d":"from sklearn.manifold import TSNE\ndef tsne_plot(model):\n    \n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()\n","95f78556":"tsne_plot(w2v_model)","14838bb5":"def getSimilarWords(word,model):\n    print(model.wv.most_similar(word))\n","0bc0ac2c":"user_input = input('Enter word to find words similar to it \\n')\ngetSimilarWords(user_input,w2v_model)","f157c7a1":"getSimilarWords('California',w2v_model)","fec28a1e":"getSimilarWords('music',w2v_model)","059928fb":"from gensim.models import KeyedVectors","e44148de":"w2v_model.wv.save_word2vec_format('custom_wikitext_embedding.txt')","1c28091a":"## Savin our custom Word Embedding","26b18dc6":"## WordClouds","0ca3f53f":"## Data Preprocessing","80afb74d":"## Importing Dependencies","a4f40011":"## Loading Dataset","8d188f88":"### Visual Cluster","769b6a99":"# Problem Statement - Create a word2vec\/CBOW model, train the model on the given dataset and create your own embeddingdictionary with key as the token and embedding as the value. \na. Visualize the embeddings using the below mentioned libraries.\n\nb. Show that similar words are close to each other in the visual cluster.\n\nc. Also create a word similarity program that uses your generated embedding dictionary and based\non the input words from the user, the word similarity is calculated.\n\n","0a8d1a8d":"## Saving Our WordCloud","7458e0f2":"## Finding Similar Words","5e1793cd":"## This Notebook is laid out as follows:\n\n### **1. Loading Dataset**\n### **2. Text PreProcessing**\n### **3. WordClouds**\n### **4. Creating Our Custom Word Embedding**\n### **5. Word Similarity**","7c4b242f":"## Creating Our Custom word embedding"}}