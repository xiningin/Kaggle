{"cell_type":{"5ce6811b":"code","281c95cd":"code","99ad5dec":"code","b617d15d":"code","bdd82806":"code","a31008ea":"code","a4be22fb":"code","3d62e0e3":"code","cffd81f1":"code","21580a07":"code","53c6cd2a":"code","8f0db909":"code","58b04657":"code","3f8f2fb3":"code","a8d945d3":"code","e24ea192":"code","ba0fc288":"code","77868605":"code","f2f95279":"code","3480fbe8":"code","7649da4f":"code","18aa9b8d":"code","932748f7":"code","35788123":"code","1bd01b60":"code","91f9f888":"code","1e51c463":"code","061695ba":"code","1e4f4819":"markdown","64fe8441":"markdown","b15eeaa5":"markdown","ed7c9c2f":"markdown","e0636a9a":"markdown","6b1856ae":"markdown"},"source":{"5ce6811b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nx = np.arange(-100, 100, 1)\nw = 0.1\nb = 0.2\n\nplt.title(\"Sigmoid Function\");\nplt.scatter(x,  1 \/ (1 + np.exp(-w*x -b))) ","281c95cd":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression() \nfrom sklearn.metrics import accuracy_score","99ad5dec":"application = [[ 0.56,  1],\n            [ 0.17,  0 ],\n            [ 0.34,  0],\n            [ 0.20,  0 ],\n            [ 0.70,  1 ]]\n\nx = np.array(application)[:, 0:1] \ny = np.array(application)[:, 1]\nmodel.fit(x, y) ","b617d15d":"model.predict(x)  # this model is not giving us the perfect classification, we can validate this by using other metrics","bdd82806":"accuracy_score(y, model.predict(x)) # Accuracy of our model is only 60%. \n# We just wanted to show how to use Logistic Regression algorithm.","a31008ea":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n%matplotlib inline","a4be22fb":"df = pd.read_csv(\"..\/input\/abalone.csv\") ","3d62e0e3":"df.sample(5)","cffd81f1":"df.describe() ","21580a07":"df[df.Height == 0] # there are 2 columns with 0 height. We can remove this data as 0 height does not make sense.","53c6cd2a":"df = df[df.Height != 0] # removing rows with 0 height.\ndf.describe() ","8f0db909":"df.isna().sum() # Finding null values","58b04657":"df.info() # We, have one categorical values and we shall change that to countinuous variable.","3f8f2fb3":"sns.countplot(df.Sex) ","a8d945d3":"new_col = pd.get_dummies(df.Sex)\ndf[new_col.columns] = new_col","e24ea192":"df.columns # new columns has been added M, F & I","ba0fc288":"sns.pairplot(df.drop(['F','I', 'M'], axis=1))","77868605":"#  Our job is to predict the age of the Ring on the given feature. So, let look at the Ring in detail.\n\nplt.figure(figsize=(12, 10))\n\nplt.subplot(2,2,1)\nsns.countplot(df.Rings)\n\nplt.subplot(2,2,2)\nsns.distplot(df.Rings)\n\nplt.subplot(2,2,3)\nstats.probplot(df.Rings, plot=plt)\n\nplt.subplot(2,2,4)\nsns.boxplot(df.Rings) \n\nplt.tight_layout()\n\n#It seems that the label value is skewed after 15 years of age. We will deal with that in a latter.df.describe()  ","f2f95279":"# As we can see that the data we have at disposal is great for predicting the Rings between 3 to 15 years.\n\nnew_df = df[df.Rings < 16]\nnew_df = new_df[new_df.Rings > 2]\nnew_df = new_df[new_df.Height < 0.4]","3480fbe8":"plt.figure(figsize=(12,10))\n\nplt.subplot(3,2,1)\nsns.boxplot(data= new_df, x = 'Rings', y = 'Diameter')\n\nplt.subplot(3,2,2)\nsns.boxplot(data= new_df, x = 'Rings', y = 'Length')\n\nplt.subplot(3,2,3)\nsns.boxplot(data= new_df, x = 'Rings', y = 'Height')\n\nplt.subplot(3,2,4)\nsns.boxplot(data= new_df, x = 'Rings', y = 'Shell weight')\n\nplt.subplot(3,2,5)\nsns.boxplot(data= new_df, x = 'Rings', y = 'Whole weight')\n\nplt.subplot(3,2,6)\nsns.boxplot(data= new_df, x = 'Rings', y = 'Viscera weight')\nplt.tight_layout()","7649da4f":"plt.figure(figsize=(12, 10))\n\nplt.subplot(2,2,1)\nsns.countplot(new_df.Rings)\n\nplt.subplot(2,2,2)\nsns.distplot(new_df.Rings)\n\nplt.subplot(2,2,3)\nstats.probplot(new_df.Rings, plot=plt)\n\nplt.subplot(2,2,4)\nsns.boxplot(new_df.Rings)\n\nplt.tight_layout()","18aa9b8d":"from sklearn.preprocessing import StandardScaler\nconvert = StandardScaler()\n\nfeature = new_df.drop(['Sex', 'Rings'], axis = 1)\nlabel = new_df.Rings\n\nfeature = convert.fit_transform(feature)","932748f7":"from sklearn.model_selection import train_test_split\nf_train, f_test, l_train, l_test = train_test_split(feature, label, random_state = 23, test_size = 0.2)","35788123":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(random_state=23)\nmodel.fit(f_train, l_train)","1bd01b60":"y_predict = model.predict(f_train)","91f9f888":"from sklearn.metrics import accuracy_score, precision_score, confusion_matrix","1e51c463":"accuracy_score(l_train, y_predict) ","061695ba":"# So, this is accuracy score of Logistic Regression. We can uses other algorithms to get the better prediction.","1e4f4819":"**What is Logistic Regression?**\n\n* Logistic Regression is a type of superviersed Machine Learning algorithm used to solve binary classification problems ( *1 or 0, Yes or No, True or False*).  We use sigmoid function to classify the dependent variable based on the given independent variables.  Here the dependent varible has to be categorical. Alternatively, we can also say that logistic regression predicts the probability of an event. \n\n**Sigmoid Function:** $$\\frac{1}{1 + e^{-y }}$$\n\ny = w*x + b\nHere x is the independent variable that we want to transform and w & b are weight and bias respectively","64fe8441":"Data Preprocssing","b15eeaa5":"# Simple Logistic Regression Application","ed7c9c2f":"# Logistic Regression","e0636a9a":"Most of the analysis has already been done in https:\/\/www.kaggle.com\/suprabhatsk\/abalone-eda-simple-regression-analysis. \nSo, we will just continue from there to solve this using classification algorithms","6b1856ae":"# Logistic Regression on Abalone Dataset"}}