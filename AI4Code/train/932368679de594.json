{"cell_type":{"aa25222e":"code","3c66560c":"code","67f13226":"code","425b67ba":"code","b510ebc0":"code","8972c08b":"code","42a55b2d":"code","fb179703":"markdown","492ace6a":"markdown","9762e052":"markdown"},"source":{"aa25222e":"import os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom numpy import linalg as LA\nfrom scipy import optimize\n\nfrom umap import UMAP\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nfrom matplotlib import pyplot as plt\nimport imageio\nimport tqdm","3c66560c":"fig = plt.figure(figsize=(20,10))\nimage = imageio.imread(\"..\/input\/amer_sign2.png\")\nplt.imshow(image)","67f13226":"df = pd.read_csv(\"..\/input\/sign_mnist_train.csv\")\ndf.head()\ndf.shape","425b67ba":"letter2encode = {'A': 0,'B': 1,'C': 2,'D': 3,'E': 4,'F': 5,'G': 6,'H': 7,'I': 8,'K': 9,'L': 10,'M': 11,\n                'N': 12,'O': 13,'P': 14,'Q': 15,'R': 16,'S': 17,'T': 18,'U': 19,'V': 20,'W': 21,'X': 22, 'Y': 23}\n\ndef fix_label_gap(l):\n    if(l>=9):\n        return (l-1)\n    else:\n        return l\n\ndef encode(character):\n    return letter2encode[character]\n\ndf['label'] = df['label'].apply(fix_label_gap)","b510ebc0":"WORD = 'THANKS'\n\nword = np.array(list(WORD))\nembedded_word = list(map(encode, word))\nprint(embedded_word)\n\nreduced_df = df[df['label'].isin(embedded_word)]\n\nreduced_df.shape\nX = reduced_df.loc[:, reduced_df.columns != 'label'].values\n\nlen(X)\ny = reduced_df['label'].values\n\nplt.imshow(X[12].reshape(28,28))","8972c08b":"X_PCA = PCA(n_components=5).fit_transform(X)\nX_LDA = LDA(n_components=5).fit_transform(X,y)\nX_TSNE = TSNE().fit_transform(X)\nX_UMAP = UMAP(n_neighbors=15,\n                      min_dist=0.1,\n                      metric='correlation').fit_transform(X)","42a55b2d":"fig = plt.figure(figsize=(50,40))\nplt.subplot(2,2,1)\nplt.scatter(X_PCA[:,0], X_PCA[:,1], c=y, cmap='Set1')\nplt.title(\"Principal Component Analysis\", fontsize=40)\nplt.subplot(2,2,2)\nplt.scatter(X_UMAP[:,0], X_UMAP[:,1], c=y, cmap='Set1')\nplt.title(\"Uniform Manifold Approximation and Projections\", fontsize=40)\nplt.subplot(2,2,3)\nplt.scatter(X_LDA[:,0], X_LDA[:,1], c=y, cmap='Set1')\nplt.title(\"Linear Discriminant Analysis\", fontsize=40)\nplt.subplot(2,2,4)\nplt.scatter(X_TSNE[:,0], X_TSNE[:,1], c=y, cmap='Set1')\nplt.title(\"t-Distributed Stochastic Neighbor Embedding\", fontsize=40)\nplt.show()","fb179703":"## Dataset: Sign Language MNIST\n\nThe Sign Language MNIST dataset is an American Sign Language letter dataset of hand gestures represent a multi-class problem with 24 classes of letters (excluding J and Z which require motion).","492ace6a":"# Dimensionality Reduction - PCA, LDA, t-SNE, UMAP\n\n## Table of Contents\n### 1. [Introduction](#Introduction)\n### 2. [Principal Component Analysis](#Principal Component Analysis)\n### 3. [Linear Discriminant Analysis](#Linear Discriminant Analysis)\n### 4. [t-Distributed Stochastic Neighbor Embedding](#t-Distributed Stochastic Neighbor Embedding)\n### 5. [Uniform Manifold Approximation and Projections](#Uniform Manifold Approximation and Projection)","9762e052":"## Introduction\n\nWhen we deal to a machine learning problem the first operation we should do is to look at data. In fact, a very common problem that leads to build an inaccurate model is the so-called \"Curse of Dimensionality\". The dataset usually contains features not giving information because they can be very correlated to other dimensions or be irrilevant for a problem. \n\nSo, the main aim of a dimensionality reduction technique is to decrease the number of features without loss of information; the algorithm projects a d-dimension feature space onto a smaller subspace k (where k < d) in order to reduce memory usage, computational cost and to avoid the curse of dimensionality.\n\nThis notebook presents a theoric summary [WIP], a scratch code (only for PCA and LDA)[WIP] and the principal library implementation for :\n\n    * Principal Component Analysis\n    * Linear Discriminant Analysis\n    * t-Distributed Stochastic Neighbor Embedding\n    * Uniform Manifold Approximation and Projections\n"}}