{"cell_type":{"bbb3cc51":"code","5ba5bb1e":"code","a82af097":"code","0fa62f90":"code","05da713f":"code","045faf4c":"code","87bffc7d":"code","f3ca3c08":"code","c9fdc04e":"code","112fb21a":"code","6663dfda":"code","4852c2d2":"code","6a1cf6d6":"code","30aa38ab":"code","61abd672":"code","bd404b8a":"code","75eee132":"code","78b4cb5e":"code","2140bcfd":"code","854d35ab":"code","c0bbe28e":"code","f54ed7b5":"code","7f8550cd":"code","1e2ec5f1":"code","473e18b0":"markdown","8a9ced8f":"markdown","bdca3fb3":"markdown","3a25536e":"markdown","5e62758c":"markdown","8fba5f7d":"markdown","5ebf83c4":"markdown","2164479d":"markdown"},"source":{"bbb3cc51":"import numpy as np \nimport pandas as pd \nimport os\n\nimport keras\nimport keras.backend as K\nimport keras.layers as klayers\nfrom keras.preprocessing.image import load_img, ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport gc\nimport random\nmain_dir = '..\/input\/severstal-steel-defect-detection\/'\nos.listdir(main_dir)","5ba5bb1e":"pretrain_weights_path = None\nbatch_size = 32\nepochs = 15\nreshape_rgb = (256, 512, 3)\nreshape_mask = (256, 512)\nmask_threshold = 3500\nmask_bound = 0.8\nlr = 3e-3","a82af097":"train_fns = os.listdir(main_dir + 'train_images')\ntest_fns = os.listdir(main_dir + 'test_images')\ntrain_seg = pd.read_csv(main_dir + 'train.csv')\n\nprint(len(train_fns))\nprint(len(test_fns))\nprint(train_seg.shape)\n\ntrain_seg.head(5)","0fa62f90":"train_seg['ImageId'] = train_seg['ImageId_ClassId'].map(lambda x : x.split('_')[0])\ntrain_seg['ClassId'] = train_seg['ImageId_ClassId'].map(lambda x : x.split('_')[1])\ntrain_seg = train_seg.drop(['ImageId_ClassId'], axis = 1)","05da713f":"train_seg['has_label'] = train_seg['EncodedPixels'].map(lambda x : 1 if isinstance(x,str) else 0)\ntrain_seg.head(5)","045faf4c":"Image_with_label = train_seg.groupby(['ImageId'])['has_label'].sum().value_counts()\nprint(Image_with_label)\nplt.figure(figsize = (6,4))\nplt.bar(Image_with_label.index, Image_with_label.values)\nplt.xlabel('label number')\nplt.ylabel('count')\nplt.title('Count of label number in single image')\nplt.show()","87bffc7d":"class_with_label = train_seg.groupby(['ClassId'])['has_label'].sum().reset_index()\nplt.figure(figsize = (6,4))\nplt.bar(class_with_label.ClassId.values, class_with_label.has_label.values)\nplt.xlabel('class id')\nplt.ylabel('count')\nplt.title('Count of each class id who has labeled')\nplt.show()","f3ca3c08":"def rle_encoding(mask):\n    \n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels,[0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    if len(runs) % 2:\n        runs = np.append(runs,len(pixels))\n    runs[1::2] -= runs[0::2]\n    \n    return ' '.join(str(x) for x in runs)\n\ndef rle_decoding(rle, mask_shape = (256,1600)):\n    strs = rle.split(' ')\n    starts = np.asarray(strs[0::2], dtype = int) - 1\n    lengths = np.asarray(strs[1::2], dtype = int)\n    ends = starts + lengths\n    \n    mask = np.zeros(mask_shape[0] * mask_shape[1], dtype = np.uint8)\n    for s,e in zip(starts, ends):\n        mask[s:e] = 1\n    return mask.reshape(mask_shape, order = 'F')\n\ndef merge_masks(image_id, df, mask_shape = (256,1600), reshape = None):\n    \n    rles = df[df['ImageId'] == image_id].EncodedPixels.iloc[:]\n    depth = rles.shape[0]\n    if reshape:\n        masks = np.zeros((*reshape, depth), dtype = np.uint8)\n    else:\n        masks = np.zeros((mask_shape[0], mask_shape[1],depth), dtype = np.uint8)\n    \n    for idx in range(depth):\n        if isinstance(rles.iloc[idx], str):\n            if reshape:\n                cur_mask = rle_decoding(rles.iloc[idx], mask_shape)\n                cur_mask = cv2.resize(cur_mask, (reshape[1], reshape[0]))\n                masks[:,:,idx] += cur_mask\n            else:         \n                masks[:,:,idx] += rle_decoding(rles.iloc[idx], mask_shape)\n    return masks   ","c9fdc04e":"#Check rle_encoding and rle_decoding\n\nrle_1 = train_seg['EncodedPixels'].iloc[0]\nmask_1 = rle_decoding(rle_1)\nrle_2 = rle_encoding(mask_1)\nmask_2 = rle_decoding(rle_2)\n\nplt.figure(figsize = (16,8))\nplt.imshow(mask_1)\nplt.show()\nplt.figure(figsize = (16,8))\nplt.imshow(mask_2)\nplt.show()","112fb21a":"def display_img_masks(img, masks, image_id = \"\", title = \"\"):\n    for idx in range(masks.shape[-1]):\n        plt.figure(figsize = (24,6))\n        plt.imshow(img)\n        plt.imshow(masks[:,:,idx], alpha = 0.35, cmap = 'gray')\n        plt.title(image_id + '_class_' + str(idx+1) + title)\n        plt.show()","6663dfda":"image_dir = main_dir + 'train_images\/' + train_seg['ImageId'].iloc[0]\nimage_id = train_seg['ImageId'].iloc[0]\nimg = plt.imread(image_dir)\nmasks = merge_masks(image_id, train_seg)\n\ndisplay_img_masks(img,masks, image_id)","4852c2d2":"gamma = 1.2\ninverse_gamma = 1.0 \/ gamma\nlook_up_table = np.array([((i\/255.0) ** inverse_gamma) * 255.0 for i in np.arange(0,256,1)]).astype(\"uint8\")\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n\ndef contrast_enhancement(img):\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n    img[:,:,0] = clahe.apply(img[:,:,0])\n    img = cv2.cvtColor(img, cv2.COLOR_YUV2RGB)\n    return img\n\ndef gamma_correction(img):\n    return cv2.LUT(img.astype('uint8'), look_up_table)\n\ndef load_target_image(path, grayscale = False, color_mode = 'rgb', target_size = reshape_rgb,\n                     interpolation = 'nearest'):\n    \n    return load_img(path = path, grayscale = grayscale, color_mode = color_mode,\n                   target_size = target_size, interpolation = interpolation)\n\ndef input_gen(filenames, segs, data_gen, batch_size = 4, reshape = (256,1600)):\n    \n    load_dir = main_dir + 'train_images\/'\n    \n    batch_rgb = []\n    batch_mask = []\n    \n    while True:\n        fns = random.sample(filenames, batch_size)\n        seed = np.random.choice(range(999))\n        for fn in fns:\n            cur_img = np.asarray(load_target_image(path = load_dir + fn))\n            cur_img = gamma_correction(cur_img)\n            cur_img = contrast_enhancement(cur_img)\n            masks = merge_masks(fn, segs, reshape = reshape)\n            batch_rgb.append(cur_img)\n            batch_mask.append(masks)\n        \n        batch_rgb, batch_mask = np.stack(batch_rgb), np.stack(batch_mask)\n        x = data_gen.flow(batch_rgb, batch_size = batch_size,seed = seed, shuffle = False)\n        y = data_gen.flow(batch_mask, batch_size = batch_size,seed = seed, shuffle = False)\n        \n        yield next(x)\/255.0, next(y)\n        batch_rgb = []\n        batch_mask = []\n        gc.collect()","6a1cf6d6":"train_x, valid_x = train_test_split(train_fns, test_size = 0.2, random_state = 2019)\nprint(len(train_x))\nprint(len(valid_x))","30aa38ab":"train_data_gen = ImageDataGenerator(rotation_range = 15,\n                                    height_shift_range = 0.1,\n                                    width_shift_range = 0.1,\n                                    vertical_flip = True,\n                                    horizontal_flip = True,\n                                    data_format = \"channels_last\",\n                                    fill_mode = 'reflect'\n                                    )\n\nvalid_data_gen = ImageDataGenerator()\n\ntrain_aug_gen = input_gen(train_x, train_seg, train_data_gen, batch_size = batch_size,reshape = reshape_mask)\nvalid_aug_gen = input_gen(valid_x, train_seg, valid_data_gen, batch_size = batch_size,reshape = reshape_mask)","61abd672":"class Unet:\n    \n    def __init__(self, input_shape = (256,1600,3), output_units = 4):\n        \n        self.input_shape = input_shape\n        self.output_units = output_units\n    \n    def _cn_bn_relu(self, filters = 64, kernel_size = (3,3), bn_flag = False):\n        \n        def f(input_x):\n            \n            x = input_x\n            x = klayers.Conv2D(filters = filters, kernel_size = kernel_size, strides = (1,1), padding = \"same\", kernel_initializer = \"he_normal\")(x)\n            if bn_flag:\n                x = klayers.BatchNormalization()(x)\n            x = klayers.Activation(\"relu\")(x)\n            \n            return x\n        return f\n    \n    def _UpSamplingBlock(self, filters = 64, kernel_size = (3,3), upsize = (2,2), bn_flag = True, up_flag = False):\n        \n        def f(up_c, con_c):\n            \n            if up_flag:\n                x = klayers.UpSampling2D(size = upsize, interpolation = 'bilinear')(up_c)\n            else:\n                x = klayers.Conv2DTranspose(filters = filters, kernel_size = (2,2), strides = upsize, padding = \"same\", kernel_initializer = \"he_normal\")(up_c)\n            \n            x = klayers.concatenate([x,con_c])\n            x = self._cn_bn_relu(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag)(x)\n            x = self._cn_bn_relu(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag)(x)\n            \n            return x\n        return f\n    \n    def _DownSamplingBlock(self, filters = 64, kernel_size = (3,3), downsize = (2,2), bn_flag = True, is_bottom = False):\n        \n        def f(input_x):\n            \n            x = self._cn_bn_relu(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag)(input_x)\n            c = self._cn_bn_relu(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag)(x)\n            if is_bottom:\n                return c\n            else:\n                p = klayers.MaxPooling2D(pool_size = downsize)(c)\n                return c,p\n        return f\n    \n    def build_unet(self):\n        \n        #encoder region\n        input_x = klayers.Input(shape = self.input_shape)\n        \n        c1,p1 = self._DownSamplingBlock(filters = 32)(input_x)\n        c2,p2 = self._DownSamplingBlock(filters = 32)(p1)\n        c3,p3 = self._DownSamplingBlock(filters = 64)(p2)\n        c4,p4 = self._DownSamplingBlock(filters = 64)(p3)\n        c5,p5 = self._DownSamplingBlock(filters = 128)(p4)\n        c6,p6 = self._DownSamplingBlock(filters = 256)(p5)\n        \n        c7 = self._DownSamplingBlock(filters = 512, is_bottom = True)(p6)\n        \n        #decoder region\n        u8 = self._UpSamplingBlock(filters = 256)(c7,c6)\n        u9 = self._UpSamplingBlock(filters = 128)(u8,c5)\n        u10 = self._UpSamplingBlock(filters = 64)(u9,c4)\n        u11 = self._UpSamplingBlock(filters = 64)(u10,c3)\n        u12 = self._UpSamplingBlock(filters = 32)(u11,c2)\n        u13 = self._UpSamplingBlock(filters = 32)(u12,c1)\n\n        output_x = klayers.Conv2D(filters = self.output_units, kernel_size = (1,1), padding = \"same\", activation = \"sigmoid\", kernel_initializer = \"he_normal\")(u13)\n        model = keras.models.Model(inputs = [input_x], outputs = [output_x])\n        return model\n        ","bd404b8a":"unet_builder = Unet(input_shape = reshape_rgb)\nunet = unet_builder.build_unet()\nunet.summary()","75eee132":"if pretrain_weights_path != None:\n    unet.load_weights(pretrain_weights_path)","78b4cb5e":"def Dice_Coef(y_true, y_pred, smooth = 1):\n    \n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    \n    intersection = K.sum(y_true_f * y_pred_f)\n    \n    return (2*intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef Dice_Loss(y_true, y_pred):\n    return 1.0 - Dice_Coef(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    return keras.losses.binary_crossentropy(y_true, y_pred) + Dice_Loss(y_true, y_pred)\n\ndef wbce_dice_loss(y_true, y_pred):\n    return weighted_bce()(y_true, y_pred) + Dice_Loss(y_true, y_pred)\n\ndef weighted_bce(weight = 0.6):\n    \n    def convert_2_logits(y_pred):\n        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n        return tf.log(y_pred \/ (1-y_pred))\n    \n    def weighted_binary_crossentropy(y_true, y_pred):\n        y_pred = convert_2_logits(y_pred)\n        loss = tf.nn.weighted_cross_entropy_with_logits(logits = y_pred, targets = y_true, pos_weight = weight)\n        return loss\n    \n    return weighted_binary_crossentropy\n\n#optimizer = keras.optimizers.SGD(lr = lr, momentum = 0.95, nesterov = True)\noptimizer = keras.optimizers.Adam(lr = lr, decay = 1e-6)\nunet.compile(loss = wbce_dice_loss, optimizer = optimizer, metrics = [Dice_Coef])\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', patience = 7, mode = 'min', factor = 0.5, verbose = 1)\ncp = keras.callbacks.ModelCheckpoint('unet_out.hdf5', monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'min')\nes = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min')\ntraining_callbacks = [reduce_lr, cp, es]\n\nsteps_per_epoch = len(train_x) \/\/ batch_size\nvalidation_steps = len(valid_x) \/\/ batch_size","2140bcfd":"\nhistory = unet.fit_generator(train_aug_gen, steps_per_epoch = steps_per_epoch, epochs = epochs,\n                              validation_data = valid_aug_gen, validation_steps = validation_steps, verbose = 1, callbacks = training_callbacks)\n\n\nunet.load_weights('unet_out.hdf5')\n","854d35ab":"\ndef plot_training_result(history):\n    \n    plt.figure(figsize = (8,6))\n    plt.plot(history.history['loss'], '-', label = 'train_loss', color = 'g')\n    plt.plot(history.history['val_loss'], '--', label = 'valid_loss', color ='r')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss on unet')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize = (8,6))\n    plt.plot(history.history['Dice_Coef'], '-', label = 'train_Dice_coef', color = 'g')\n    plt.plot(history.history['val_Dice_Coef'], '--', label = 'valid_Dice_coef', color ='r')\n    plt.xlabel('epoch')\n    plt.ylabel('Dice_Coef')\n    plt.title('Dice_Coef on unet')\n    plt.legend()\n    plt.show()\n\nplot_training_result(history)\n","c0bbe28e":"\ndef predict_masks(img):\n    \n    masks = unet.predict(np.expand_dims(img, axis = 0))\n    masks = np.squeeze(masks, axis = 0)\n    \n    return masks\n\nvalid_aug_gen = input_gen(valid_x, train_seg, valid_data_gen, batch_size = 8, reshape = reshape_mask)\n\nvalid_data, valid_label = next(valid_aug_gen)\nfor x,y in zip(valid_data, valid_label):\n    display_img_masks(x,y,title = \"_ground truth\")\n    prediction = predict_masks(x)\n    display_img_masks(x,prediction, title = '_prediction')\n","f54ed7b5":"'''\nsub = pd.DataFrame(columns = ['ImageId_ClassId', 'EncodedPixels'])\n\ndef masks_reduce(masks):\n    \n    for idx in range(masks.shape[-1]):\n        label_num, labeled_mask = cv2.connectedComponents(masks[:,:,idx].astype(np.uint8))\n        reduced_mask = np.zeros(masks.shape[:2],np.float32)\n        \n        for label in range(1, label_num):\n            single_label_mask = (labeled_mask == label)\n            if single_label_mask.sum() > mask_threshold:\n                reduced_mask[single_label_mask] = 1\n        \n        masks[:,:,idx] = reduced_mask\n        \n    return masks\n\ndef masks_reduce2(masks):\n    for idx in range(masks.shape[-1]):\n        if np.sum(masks[:,:,idx]) < mask_threshold:\n            masks[:,:,idx] = np.zeros(masks.shape[:2], dtype = np.uint8)\n    return masks\n\ndef prediction_encoding(fn, img_dir, submission, target_shape = (256,1600)):\n    img = np.asarray(load_target_image(path = os.path.join(img_dir,fn)))\n    img = gamma_correction(img)\n    img = contrast_enhancement(img)\/255.0\n    masks = unet.predict(np.expand_dims(img, axis = 0))\n    masks = np.squeeze( np.round(masks), axis = 0)\n    masks = np.array(masks > mask_bound, dtype = np.uint8)    \n    masks = cv2.resize(masks, (target_shape[1], target_shape[0]))\n    masks = masks_reduce(masks)\n    \n    ImageId_ClassId = np.asarray([ fn+'_'+str(id) for id in range(1,5) ])\n    for idx in range(masks.shape[-1]):\n        submission = submission.append(pd.DataFrame([[ImageId_ClassId[idx], rle_encoding(masks[:,:,idx])]], columns = [\"ImageId_ClassId\", \"EncodedPixels\"]))\n        \n    return submission\n\nload_dir = main_dir + 'test_images\/'\n\nfor fn in tqdm_notebook(test_fns):\n    sub = prediction_encoding(fn, load_dir, sub)\n    gc.collect()\n    \nsub.head(10)\n'''","7f8550cd":"'''\nsub_sample = pd.read_csv(main_dir + 'sample_submission.csv')\nsub_sample = sub_sample.drop(['EncodedPixels'], axis = 1)\n\nsubmission = sub_sample.merge(sub, on = ['ImageId_ClassId'])\nsubmission.head(10)\n'''","1e2ec5f1":"#submission.to_csv('submission.csv', index = False)","473e18b0":"# Validation","8a9ced8f":"# Info\n\nThis kernel is using regular U-Net with slightly depper layers as training model. <br \/>\nThe total training time will be quite long, usually I would take 2-3 commit to finish the training. <br \/>\nAnd I also did some pre\/post-processing on the data and prediction to improve the score. <br \/>\nSince the submission time limit is 60 mins, so I will save the model and submit the result in other commit version. <br \/>\n<br \/>\n**preprocessing** <br \/>\n1. gamma correction\n2. contrast enhnacement\n<br \/>\n\n**postprocessing** <br \/>\n1. mask reduction (Which is the same as Rishabh Agrahari's share)\n<br \/>\n<br \/>\n\n**Future work** <br \/>\n1. Try more complex model\n2. Use pre-train model for decoding part\n3. Deal with imbalance class\n4. Tune the callbacks\n\nI am quite new to this field, if I had done something, please let me know. <br \/>\nAlso I am looking for a team thread. Reach me if you are interested. Thanks!","bdca3fb3":"# Data preparation\/EDA\/Visualization","3a25536e":"# Build the U-Net\n\nI was using a slightly deeper U-Net architecture to be my model. Since I had tried the regular U-Net(Same depth but less filter numebr), the training and validation loss seems to be decreased quite nice until 0.4~0.5 and it wont improve anymore. And the old model seems to have no over-fitting problem, so I assume I can keep increase the complexity of model until the score decrease or the over-fitting problem becomes my concern.","5e62758c":"# Hyper-parameters","8fba5f7d":"# Data Generator\n\nHere I using two data pre-processing techique and trying to improve the result.\n\n**A. gamma correction** <br \/>\nGamma correction is a very common operation in traditional image processing. Since human visual system is non-linear response to visible light, so the ISP on phone or digital camera will perform gamma correction to produce better contrast for human visual system. Anyway, it is a image enhancement techique. <br \/>\n\n**B. Contrast Limited Adaptive Histogram Equalization(CLAHE)** <br \/>\nThis is also a contrast enhancement techique, it is very similar to regular histogram equalization. But it is doing the effect on small blocks, which can avoid over-brightness in regular histogram equalization.\n\nreference: https:\/\/docs.opencv.org\/3.3.1\/d5\/daf\/tutorial_py_histogram_equalization.html","5ebf83c4":"# Loss fucntion\n<br \/>\nI was using dice loss + weighted bce to be my loss function. Since dummy submission can reach 0.85+ score, so I think the weighted bce can decrease the false postivie. ","2164479d":"## Some utilities function"}}