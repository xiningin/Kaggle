{"cell_type":{"5c337265":"code","cf78cbd7":"code","1f4b8927":"code","79c13bc6":"code","b86f4052":"code","f3d04f8a":"code","259e47fe":"code","762b55c0":"code","44fee72a":"code","267bfc93":"code","573961f1":"code","2d70e79c":"code","ea4f34ff":"code","c259c518":"code","5e208c9c":"code","b1bb528f":"code","5b59faea":"code","018b0d69":"code","f64f29d5":"code","84c2b3d2":"code","4673362d":"code","04ab1f14":"code","a3e10670":"markdown","bb8cf0da":"markdown","0bdd75f7":"markdown"},"source":{"5c337265":"# INPORTING WHAT WE NEED\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport os\nimport time\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# GENETIC ALGORITHM\nimport gplearn\nfrom gplearn.genetic import SymbolicTransformer, SymbolicRegressor\nfrom gplearn.functions import make_function\nfrom gplearn.fitness import make_fitness","cf78cbd7":"print(os.listdir(\"..\/input\"))","1f4b8927":"train_df = pd.read_csv('..\/input\/train.csv')\nprint('Rows: ',train_df.shape[0],'Columns: ',train_df.shape[1])\ntrain_df.info()","79c13bc6":"train_df.head()","b86f4052":"train_df['target'].value_counts()","f3d04f8a":"sns.countplot(train_df['target'])\nsns.set_style('whitegrid')","259e47fe":"test_df = pd.read_csv('..\/input\/test.csv')","762b55c0":"X_test1 = test_df.drop('ID_code',axis=1)","44fee72a":"X1 = train_df.drop(['ID_code','target'],axis=1)","267bfc93":"# Create a fitness function that is the mean absolute percentage error\ndef _my_fit(y, y_pred, w):\n    diffs = np.abs(y - y_pred)  \n    return 100. * np.average(diffs, weights=w)\nmy_fit = make_fitness(_my_fit, greater_is_better=False)","573961f1":"# Choose the mathematical functions we will combine together\nfunction_set = ['add', 'sub', 'mul', 'div', 'log', \n                'sqrt', 'log', 'abs', 'neg', 'inv', \n                'max', 'min', \n                'sin', 'cos', 'tan' ] \n\n# Create the genetic learning regressor\ngp = SymbolicRegressor(function_set=function_set, metric = my_fit,\n                       verbose=1, generations = 3, \n                       random_state=0, n_jobs=-1)\n","2d70e79c":"# Using NUMPY structures, remove one feature (column of data) at a time from the training set\n# Use that removed column as the target for the algorithm\n# Use the genetically engineered formula to create the new feature\n# Do this for both the training set and the test set\n\nX1a = np.array(X1)\nsam = X1a.shape[0]\ncol = X1a.shape[1]\nX2a = np.zeros((sam, col))\n\nX_test1a = np.array(X_test1)\nsam_test = X_test1a.shape[0]\ncol_test = X_test1a.shape[1]\nX_test2a = np.zeros((sam_test, col_test))\n\nfor i in range(col) :\n    X = np.delete(X1a,i,1)\n    y = X1a[:,i]\n    gp.fit(X, y) \n    X2a[:,i] = gp.predict(X)\n    X = np.delete(X_test1a, i, 1)\n    X_test2a[:,i] = gp.predict(X)\n    \nX2 = pd.DataFrame(X2a)\nX_test2 = pd.DataFrame(X_test2a) ","ea4f34ff":"# Add the new features to the existing 200 features\nX = pd.concat([X1, X2], axis=1, sort=False) \nX_test = pd.concat([X_test1, X_test2], axis=1, sort=False)  \ny = train_df['target']","c259c518":"# See my earlier kernel \"Santander-Statistics\" https:\/\/www.kaggle.com\/pnussbaum\/santander-statistics-v04\n# for reasons why I have moved from \"StratifiedKFold\" to simply \"KFold\"\n\nn_fold = 5\n# folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)","5e208c9c":"# Used parameters from https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82515 courtesy Nanashi\nparams = {\n    'num_leaves': 13,\n    'min_data_in_leaf': 80,\n    'objective': 'binary',\n    'max_depth': -1,\n    'learning_rate': 0.01,\n#    'boost': 'gbdt',\n    'boosting': 'gbdt',\n    'bagging_freq': 5,\n    'bagging_fraction': 0.33,\n    'feature_fraction': 0.05,\n    'metric':'auc',\n    'verbosity': 1, \n    'min_sum_hessian_in_leaf': 10.0,\n    'num_threads': 12,\n    'tree_learner': 'serial',\n    'boost_from_average':'false'\n}\n# params = {'num_leaves': 9,\n#          'min_data_in_leaf': 42,\n#          'objective': 'binary',\n#          'max_depth': 16,\n#          'learning_rate': 0.0123,\n#          'boosting': 'gbdt',\n#          'bagging_freq': 5,\n#          'bagging_fraction': 0.8,\n#          'feature_fraction': 0.8201,\n#          'metric': 'auc',\n#          'verbosity': -1,\n#          'subsample': 0.81,\n#          'min_gain_to_split': 0.01077313523861969,\n#          'min_child_weight': 19.428902804238373,\n#          'bagging_seed': 11,\n#          'reg_alpha': 1.728910519108444,\n#          'reg_lambda': 4.9847051755586085,\n#          'random_state': 42,\n#          'num_threads': 4}","b1bb528f":"prediction = np.zeros(len(X_test))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    model = lgb.train(params,train_data,num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)\n             \n    #y_pred_valid = model.predict(X_valid) \n    prediction += model.predict(X_test, num_iteration=model.best_iteration)\/n_fold ","5b59faea":"from catboost import CatBoostClassifier,Pool\ntrain_pool = Pool(X,y) \nm = CatBoostClassifier(iterations=300,eval_metric=\"AUC\", boosting_type = 'Ordered')\nm.fit(X,y,silent=True)\ny_pred1 = m.predict(X_test)\nm.best_score_","018b0d69":"prediction","f64f29d5":"y_pred1","84c2b3d2":"sub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub[\"target\"] = prediction\nsub.to_csv(\"submission.csv\", index=False)","4673362d":"sub1 = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub1[\"target\"] = y_pred1\nsub1.to_csv(\"submission1.csv\", index=False)","04ab1f14":"sub2 = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub2[\"target\"] = (prediction + y_pred1)\/2\nsub2.to_csv(\"submission2.csv\", index=False)","a3e10670":"## Reference\n1. https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\n2. https:\/\/www.kaggle.com\/deepak525\/sctp-lightgbm-lb-0-899","bb8cf0da":"Initial version forked from SCTP-working(LGB) https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb courtesy of Dromosys","0bdd75f7":"# How can we genetically engineer features?\n\n## Executive Summary\nWe would like to find a mathematical formula that will create a new feature from the ones we have - and therefore give the machine learning algorithms more to work with. In the case of the Santander competition, this is dificult because the features are already well pre-processed. In other words, they are already statistically independent and seem to contain little, if any, redundant information (as if principal component analysis has already been performed). Instead, we can turn to generically engineered formulas to create new features. \n\n\nSpecificaly, if we use 199 of the 200 features to create an estimation of the missing feature - this will be a imperfect estimation, because as noted above, the features do not contain redundant information. Nevertheless, this poor estimation can be considered to be a \"new feature\" or a new classification of the current 199 features as being a member of the missing feature target. This can be repeated for every feature, giving us 200 new features to add to the training (and testing) sets.\n\nThis kernel demonstrates this, and uses \"gplearn\" which extends the scikit-learn machine learning library to perform Genetic Programming (GP) with symbolic regression.\n\n## Background\nFrom the website https:\/\/gplearn.readthedocs.io\/en\/stable\/intro.html ...\n\n\"Symbolic regression is a machine learning technique that aims to identify an underlying mathematical expression that best describes a relationship. It begins by building a population of naive random formulas to represent a relationship between known independent variables and their dependent variable targets in order to predict new data. Each successive generation of programs is then evolved from the one that came before it by selecting the fittest individuals from the population to undergo genetic operations.\"\n"}}