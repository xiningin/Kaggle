{"cell_type":{"5a4db9a5":"code","439de053":"code","a05508c5":"code","56605826":"code","82669589":"code","4f546f35":"code","1aa9aab0":"code","e39d187b":"code","768aae18":"code","c2219978":"code","854ecb14":"code","61c8ec9e":"code","52c7897b":"code","97271efd":"code","14550eea":"code","61f3f97e":"code","87985fae":"code","85bb0170":"code","34519b79":"code","f57ab549":"code","cec0b3f9":"code","02747dbe":"code","ccafa09e":"code","20c0293d":"code","cdfaddf7":"code","02956601":"code","7fe011cd":"code","f6ad9d13":"code","b7627a5f":"code","ff89725b":"code","d12ab6c8":"code","8643c004":"code","70d7f331":"code","a7e1d895":"code","2bbbe310":"code","f2925a07":"code","2d3c7f6b":"code","feb2d8c2":"code","594e1b7b":"code","48184646":"code","e7b9ae80":"code","8857a24b":"code","a0cb8d90":"code","d3f58fc0":"code","e7fc641a":"code","45d6ab7e":"code","7ee071f9":"code","694fa80a":"code","99b47d27":"markdown","2bf5498e":"markdown","5fccda42":"markdown","3fe9b8e2":"markdown","7d1f1fbe":"markdown","c3da8950":"markdown","2a6eb458":"markdown","ed70e8ab":"markdown","c54e3e17":"markdown","ad097dbe":"markdown","58831ef0":"markdown","e105c910":"markdown","fc35541c":"markdown","88beebe4":"markdown","8f559024":"markdown","50c86290":"markdown","3f46c91c":"markdown"},"source":{"5a4db9a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","439de053":"#Utils\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, roc_auc_score, make_scorer, accuracy_score, roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom kmodes.kmodes import KModes\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform as sp_uniform\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import pairwise_kernels\nfrom sklearn.metrics.pairwise  import cosine_similarity\nfrom sklearn.metrics.pairwise import chi2_kernel\nfrom catboost import CatBoostClassifier","a05508c5":"pd.set_option('display.max_rows', None)","56605826":"train= pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv', sep=',')\nsub_sample = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv', sep=',')\ntest= pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv', sep=',')","82669589":"train = train.set_index('PassengerId')\n\n#df with train + test data\ndfg = pd.concat([train, test], axis=0)","4f546f35":"train['Age'] = train['Age'].replace(np.nan, dfg['Age'].median())\ntrain['Age'] = round(train['Age'],0)\n\nbins = ['Y1', 'Y2', 'Y3', 'Y4', 'M1', 'M2', 'E']\ntrain['Age_Bin'] = pd.cut(x=train['Age'],\n                            bins=[0, 5 , 10, 15, 20, 30, 50,1000],\n                            labels=bins,right=False)\ntrain['Age_Bin'] = train['Age_Bin'].astype('str')\ntrain['Age_Bin'] = train['Age_Bin']+train['Sex']\ndf_Age_bin = pd.get_dummies(train['Age_Bin'], prefix='Age_bin')","1aa9aab0":"dfg['Age_Bin'] = pd.cut(x=dfg['Age'],\n                            bins=[0, 5 , 10, 15, 20, 30, 50,1000],\n                            labels=bins,right=False)\ndfg.groupby(by=['Sex','Pclass'])['Fare'].median()","e39d187b":"train.loc[(train['Fare'].isna()) & (train['Pclass']==1) & (train['Sex']=='female'), 'Fare']=85.40\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==2) & (train['Sex']=='female'), 'Fare']=24.75\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==3) & (train['Sex']=='female'), 'Fare']=12.54\n\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==1) & (train['Sex']=='male'), 'Fare']=64.51\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==2) & (train['Sex']=='male'), 'Fare']=14.23\ntrain.loc[(train['Fare'].isna()) & (train['Pclass']==3) & (train['Sex']=='male'), 'Fare']=11.02","768aae18":"#Fare bins\nbins2 = ['L1', 'L2', 'L3', 'L4']\ntrain['Fare_Bin'] = pd.cut(x=train['Fare'],\n                            bins=[0,11, 30 , 60, 10000],\n                            labels=bins2,right=False)\ntrain['Fare_Bin'] = train['Fare_Bin'].astype('str')\ndf_Fare_bin = pd.get_dummies(train['Fare_Bin'], prefix='Fare_bin')","c2219978":"dfg['Cabin'] =dfg['Cabin'].str[0]\ntrain['Cabin'] =train['Cabin'].str[0]\ntrain['Cabin'] = train['Cabin'].fillna('Z')\ntrain.loc[(train['Cabin']=='T'), 'Cabin']='Z'\ndf_cabin = pd.get_dummies(train['Cabin'], prefix='Cabin')","854ecb14":"#Used the most frequent caracter\ntrain['Embarked'] = train['Embarked'].fillna('S')\ndf_embarked = pd.get_dummies(train['Embarked'], prefix='Embark')","61c8ec9e":"train['Ticket'] = train['Ticket'].str.replace('[^a-zA-Z]', '').str[:1]\ntrain['Ticket'] = train['Ticket'].str.strip()\ntrain['Ticket'] = train['Ticket'].fillna('ZZ')\ntrain.loc[train['Ticket']=='', 'Ticket']='ZZ'\ntrain.loc[train['Ticket']=='L', 'Ticket']='ZZ'\ndf_tiket = pd.get_dummies(train['Ticket'], prefix='ticket')","52c7897b":"df_name = pd.concat([train['Name'], test['Name']], axis=0)\ndf_name = pd.DataFrame(df_name, columns=['Name'])\n\ndf_name['FirstName'] = df_name['Name'].apply(lambda x:x.split(', ')[0])\ndf_name['SecondName'] = df_name['Name'].str.split(', ', 1, expand=True)[1]\n\nle = LabelEncoder()\nle1 = LabelEncoder()\ndf_name['FirstName'] = le.fit_transform(df_name['FirstName'])\ndf_name['SecondName'] = le1.fit_transform(df_name['SecondName'])\n\ntrain['FirstName'] = train['Name'].apply(lambda x:x.split(', ')[0])\ntrain['SecondName'] = train['Name'].str.split(', ', 1, expand=True)[1]\n\ntrain['FirstName'] = le.transform(train['FirstName'])\ntrain['SecondName'] = le1.transform(train['SecondName'])","97271efd":"train['Sex'] = train['Sex'].apply(lambda x: 1 if x=='female' else 0)","14550eea":"train['Pclass'] = train['Pclass'].astype('str')\ndf_pclass = pd.get_dummies(train['Pclass'], prefix='class')","61f3f97e":"# introducing a new feature : the size of families (including the passenger)\ntrain['FamilySize'] = train['Parch'] + train['SibSp'] + 1\n\n# introducing other features based on the family size\ntrain['Singleton'] = train['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntrain['SmallFamily'] = train['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\ntrain['LargeFamily'] = train['FamilySize'].map(lambda s: 1 if 5 <= s else 0)","87985fae":"df = pd.concat([train['Fare'], train['Age'],train['SibSp'],train['Parch'],train['FamilySize'], train['Singleton'], train['SmallFamily'], train['LargeFamily'],train['Sex'], df_cabin,df_tiket, df_pclass, df_embarked ,train['FirstName'],train['SecondName'],df_Age_bin,df_Fare_bin,train['Survived']], axis=1)","85bb0170":"plt.figure(figsize=(15,10))\nsns.heatmap(data=df.corr())","34519b79":"df = df.drop(columns='Survived')","f57ab549":"km = KMeans(n_clusters=3, random_state=22, n_init=20)\ndf_km = km.fit_predict(df)\ndf_km = pd.DataFrame(df_km, index=df.index)\ndf_km = df_km.astype('str')\ndf_km = pd.get_dummies(df_km)","cec0b3f9":"df = pd.concat([df, df_km], axis=1)\ndf_target = train['Survived']","02747dbe":"clf = RandomForestClassifier(n_estimators=200, max_features='sqrt')\nclf = clf.fit(df, df_target)","ccafa09e":"features = pd.DataFrame()\nfeatures['feature'] = df.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))","20c0293d":"df.columns","cdfaddf7":"df_cb = pd.concat([train['Fare'],train['SibSp'],train['Parch'], train['Age'],train['FamilySize'], train['Singleton'], train['SmallFamily'], train['LargeFamily'], train['Sex'], train['Cabin'],train['Ticket'], train['Pclass'], train['Embarked'] ,train['FirstName'], train['SecondName'],train['Age_Bin'],train['Fare_Bin'], df_km], axis=1)","02956601":"#Category Columns\ncat_cols = ['FamilySize', 'Singleton', 'SmallFamily', 'LargeFamily',\n       'Sex', 'Cabin', 'Ticket', 'Pclass', 'Embarked', 'FirstName', 'SecondName',\n       '0_0', '0_1', '0_2', 'Age_Bin', 'Fare_Bin']","7fe011cd":"df_cb.columns","f6ad9d13":"import optuna","b7627a5f":"def objective(trial , data = df_cb , target = df_target):\n    train_x , test_x , train_y , test_y = train_test_split(data , target , \\\n            test_size = 0.028059109276941666 , random_state = 2)\n\n    params = {'iterations':10000,\n              'depth': trial.suggest_int(\"depth\", 4, 70),\n              'l2_leaf_reg': trial.suggest_float(\"l2_leaf_reg\", 0.0001, 25, log=True),\n              'bagging_temperature': trial.suggest_float(\"bagging_temperature\", 0, 100),\n              'auto_class_weights':trial.suggest_categorical('auto_class_weights', [None,'Balanced','SqrtBalanced']),\n              'grow_policy': 'Lossguide',\n              'loss_function':'Logloss',\n              'bootstrap_type':trial.suggest_categorical(\"bootstrap_type\", ['Poisson']),\n              'use_best_model':True,\n              'task_type':'GPU', \n              'cat_features':cat_cols,\n              'eval_metric': 'Logloss',\n              'learning_rate': trial.suggest_uniform('learning_rate' , 1e-5 , 1.0),\n              #'max_bin': trial.suggest_int('max_bin', 5, 500),\n              'verbose':False,\n              'border_count':trial.suggest_int('max_bin', 5, 600),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 600),\n              'subsample': trial.suggest_uniform('subsample' , 1e-5 , 1.0)\n             }\n    model = CatBoostClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(test_x , test_y)] , early_stopping_rounds = 2000 , \\\n             verbose = False)\n    preds = model.predict(test_x)\n    acc = accuracy_score(test_y , preds)\n    return acc","ff89725b":"study = optuna.create_study(direction = 'maximize' , study_name = 'cb')\nstudy.optimize(objective , n_trials = 1)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)","d12ab6c8":"#the best value: 0.7822523164647185\nparams={'depth': 38, 'l2_leaf_reg': 0.0010021036002324036, 'bagging_temperature': 54.72815390606476, 'auto_class_weights': None, 'bootstrap_type': 'Poisson', 'learning_rate': 0.0863109888471924, 'max_bin': 192, 'min_data_in_leaf': 338, 'subsample': 0.5399710139554038}","8643c004":"test = test.set_index('PassengerId')\n\n#Age\ntest['Age'] = test['Age'].replace(np.nan, dfg['Age'].median())\ntest['Age_Bin'] = pd.cut(x=test['Age'],\n                            bins=[0, 5 , 10, 15, 20, 30, 50,1000],\n                            labels=bins,right=False)\ntest['Age_Bin'] = test['Age_Bin'].astype('str')\ntest['Age_Bin'] = test['Age_Bin']+test['Sex']\ndft_Age_bin = pd.get_dummies(test['Age_Bin'], prefix='Age_bin')\n\n\n#Fare\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==1) & (test['Sex']=='female'), 'Fare']=85.40\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==2) & (test['Sex']=='female'), 'Fare']=24.75\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==3) & (test['Sex']=='female'), 'Fare']=12.54\n\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==1) & (test['Sex']=='male'), 'Fare']=64.51\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==2) & (test['Sex']=='male'), 'Fare']=14.23\ntest.loc[(test['Fare'].isna()) & (test['Pclass']==3) & (test['Sex']=='male'), 'Fare']=11.02\n\ntest['Fare_Bin'] = pd.cut(x=test['Fare'],\n                            bins=[0,11, 30 , 60, 10000],\n                            labels=bins2,right=False)\n\ntest['Fare_Bin'] = test['Fare_Bin'].astype('str')\ndft_Fare_bin = pd.get_dummies(test['Fare_Bin'], prefix='Fare_bin')\n\n\n#Cabin\ntest['Cabin'] =test['Cabin'].str[0]\ntest['Cabin'] = test['Cabin'].fillna('Z')\ntest.loc[(test['Cabin']=='T'), 'Cabin']='Z'\ndft_cabin = pd.get_dummies(test['Cabin'], prefix='Cabin')\n\n#Embarked\ntest['Embarked'] = test['Embarked'].fillna('S')\ndft_embarked = pd.get_dummies(test['Embarked'], prefix='Embark')\n\n#Ticket\ntest['Ticket'] = test['Ticket'].str.replace('[^a-zA-Z]', '').str[:1]\ntest['Ticket'] = test['Ticket'].str.strip()\ntest['Ticket'] = test['Ticket'].fillna('ZZ')\ntest.loc[test['Ticket']=='', 'Ticket']='ZZ'\ntest.loc[test['Ticket']=='L', 'Ticket']='ZZ'\ndft_tiket = pd.get_dummies(test['Ticket'], prefix='ticket')\n\n#Name\ntest['FirstName'] = test['Name'].apply(lambda x:x.split(', ')[0])\ntest['SecondName'] = test['Name'].str.split(', ', 1, expand=True)[1]\n\ntest['FirstName'] = le.transform(test['FirstName'])\ntest['SecondName'] = le1.transform(test['SecondName'])\n\n#Sex\ntest['Sex'] = test['Sex'].apply(lambda x: 1 if x=='female' else 0)\n\n#Pclass\ntest['Pclass'] = test['Pclass'].astype('str')\ndft_pclass = pd.get_dummies(test['Pclass'], prefix='class')\n\n#Family Size\ntest['FamilySize'] = test['Parch'] + test['SibSp'] + 1\n\ntest['Singleton'] = test['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntest['SmallFamily'] = test['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\ntest['LargeFamily'] = test['FamilySize'].map(lambda s: 1 if 5 <= s else 0)","70d7f331":"dft = pd.concat([test['Fare'],test['SibSp'],test['Parch'], test['Age'],test['FamilySize'], test['Singleton'], test['SmallFamily'], test['LargeFamily'], test['Sex'],dft_cabin,dft_tiket, dft_pclass, dft_embarked, test['FirstName'],test['SecondName'], dft_Age_bin, dft_Fare_bin], axis=1)","a7e1d895":"dft_km = km.predict(dft)\ndft_km = pd.DataFrame(dft_km, index=dft.index)\ndft_km = dft_km.astype('str')\ndft_km = pd.get_dummies(dft_km)","2bbbe310":"dft = pd.concat([dft, dft_km], axis=1)","f2925a07":"dft_cb = pd.concat([test['Fare'], test['SibSp'], test['Parch'],test['Age'],test['FamilySize'], test['Singleton'], test['SmallFamily'], test['LargeFamily'], test['Sex'], test['Cabin'],test['Ticket'], test['Pclass'], test['Embarked'] ,test['FirstName'],test['SecondName'],test['Age_Bin'],test['Fare_Bin'], dft_km], axis=1)","2d3c7f6b":"params_cb = {\n    'bootstrap_type': 'Poisson',\n    'loss_function': 'Logloss',\n    'eval_metric': 'Logloss',\n    'random_seed': 22,\n    'task_type': 'GPU',\n    'grow_policy': 'Lossguide',\n    'depth': 38,\n    'bagging_temperature': 54.72815390606476,\n    'auto_class_weights': None,\n    'cat_features': cat_cols,\n    'learning_rate': 0.0863109888471924,\n    'iterations':10000,\n    'max_bin': 192,\n    'min_data_in_leaf': 338,\n    'subsample': 0.5399710139554038,\n    'l2_leaf_reg': 0.0010021036002324036,\n}","feb2d8c2":"preds3 = np.zeros(dft_cb.shape[0])\noof_preds3 = np.zeros(df_cb.shape[0])\nkf = StratifiedKFold(n_splits = 10 , random_state = 22 , shuffle = True)\nroc3 = []\nn = 0\nfor trn_idx , val_idx in kf.split(df_cb , df_target):\n    train_x = df_cb.iloc[trn_idx]\n    train_y = df_target.iloc[trn_idx]\n    val_x = df_cb.iloc[val_idx]\n    val_y = df_target.iloc[val_idx]\n    \n    model3 = CatBoostClassifier(**params_cb)\n    model3.fit(train_x , train_y , eval_set = [(val_x , val_y)], early_stopping_rounds = 3000 , verbose = False)\n    preds3 += model3.predict_proba(dft_cb)[:,1]\/kf.n_splits\n    oof_preds3 += model3.predict_proba(df_cb)[:,1]\/kf.n_splits\n    roc3.append(accuracy_score(val_y , model3.predict(val_x)))\n    fpr, tpr, thresholds = roc_curve(val_y , model3.predict_proba(val_x)[:,1])\n    gmeans = np.sqrt(tpr * (1-fpr))\n    ix = np.argmax(gmeans)\n    print(n+1 , roc3[n], 'Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n    n+=1","594e1b7b":"thresholds = np.arange(0, 1, 0.001)\n# apply threshold to positive probabilities to create labels\n\ndef to_labels(pos_probs, threshold):\n    return (pos_probs >= threshold).astype('int')","48184646":"scores = [accuracy_score(df_target, to_labels(oof_preds3, t)) for t in thresholds]","e7b9ae80":"ix = np.argmax(scores)\nprint('Threshold=%.3f, accuracy_Score=%.5f' % (thresholds[ix], scores[ix]))","8857a24b":"sub_sample['Survived'] = preds3","a0cb8d90":"#simple threshold\nsub_sample['Survived'] = sub_sample['Survived'].apply(lambda x: 1 if x>thresholds[ix] else 0)","d3f58fc0":"y = pd.concat([df_target, sub_sample['Survived']], axis=0)\nX = pd.concat([df_cb, dft_cb], axis=0)\nprint(X.shape,y.shape)","e7fc641a":"preds3 = np.zeros(dft_cb.shape[0])\noof_preds3 = np.zeros(X.shape[0])\nkf = StratifiedKFold(n_splits = 10 , random_state = 22 , shuffle = True)\nroc3 = []\nn = 0\nfor trn_idx , val_idx in kf.split(X , y):\n    train_x = X.iloc[trn_idx]\n    train_y = y.iloc[trn_idx]\n    val_x = X.iloc[val_idx]\n    val_y = y.iloc[val_idx]\n    \n    model3 = CatBoostClassifier(**params_cb)\n    model3.fit(train_x , train_y , eval_set = [(val_x , val_y)], early_stopping_rounds = 3000 , verbose = False)\n    preds3 += model3.predict_proba(dft_cb)[:,1]\/kf.n_splits\n    oof_preds3 += model3.predict_proba(X)[:,1]\/kf.n_splits\n    roc3.append(accuracy_score(val_y , model3.predict(val_x)))\n    fpr, tpr, thresholds = roc_curve(val_y , model3.predict_proba(val_x)[:,1])\n    gmeans = np.sqrt(tpr * (1-fpr))\n    ix = np.argmax(gmeans)\n    print(n+1 , roc3[n], 'Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n    n+=1","45d6ab7e":"#Final Threshold\nscores = [accuracy_score(y, to_labels(oof_preds3, t)) for t in thresholds]\nix = np.argmax(scores)\nprint('Threshold=%.3f, accuracy_Score=%.5f' % (thresholds[ix], scores[ix]))","7ee071f9":"sub_sample['Survived'] = preds3\nsub_sample['Survived'] = sub_sample['Survived'].apply(lambda x: 1 if x>thresholds[ix] else 0)","694fa80a":"sub_sample.to_csv('submission.csv',index=False)","99b47d27":"## EDA Features importance","2bf5498e":"### 9.Family Size ","5fccda42":"### Looking for the Best Threshold","3fe9b8e2":"### 4.Embarked ","7d1f1fbe":"### 8.Pclass ","c3da8950":"### 5.Ticket ","2a6eb458":"### Features Extraction","ed70e8ab":"# Load Data","c54e3e17":"## Optuna with Catboost","ad097dbe":"### 3.Cabin","58831ef0":"### 6.Name ","e105c910":"### 1.Age","fc35541c":"### 2.Fare","88beebe4":"### Adding 3 Kmeans Features","8f559024":"### Pseudo-Labels with the same model","50c86290":"### 7.Sex ","3f46c91c":"Get the Fatures related to Family size. An idea taken from: https:\/\/medium.datadriveninvestor.com\/start-with-kaggle-a-comprehensive-guide-to-solve-the-titanic-challenge-8ac5815b0473"}}