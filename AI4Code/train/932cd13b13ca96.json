{"cell_type":{"8e1e21bf":"code","d8ea958a":"code","9070801f":"code","a9ca335f":"code","cdd2ce36":"code","227a9ff4":"code","57c2c665":"code","08a945ff":"code","6368cf41":"code","f1fc9da4":"code","0233b3ab":"code","aabab8eb":"code","efb27914":"code","b2564170":"code","d0c9d8cb":"code","44630d20":"code","6af8113d":"code","cbcd0a7b":"code","f27d7a38":"markdown","ae643690":"markdown","46e5aa05":"markdown","7c5bf091":"markdown","0f907287":"markdown","55b12862":"markdown","0dc121cf":"markdown","ac48a108":"markdown","596d31cc":"markdown","8a6fd40f":"markdown","eef0187e":"markdown","2063a16c":"markdown","05020d38":"markdown","e69d6f6d":"markdown","8891de16":"markdown","f8940659":"markdown","f27c50a0":"markdown"},"source":{"8e1e21bf":"import numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport random","d8ea958a":"data = \"\"\"\nMarie Curie was one of the most accomplished scientists in history. Together with her husband, Pierre, she discovered radium, an element widely used for treating cancer, and studied uranium and other radioactive substances. Pierre and Marie\u2019s amicable collaboration later helped to unlock the secrets of the atom.\n\nMarie was born in 1867 in Warsaw, Poland, where her father was a professor of physics. At an early age, she displayed a brilliant mind and a blithe personality. Her great exuberance for learning prompted her to continue with her studies after high school. She became disgruntled, however, when she learned that the university in Warsaw was closed to women. Determined to receive a higher education, she defiantly left Poland and in 1891 entered the Sorbonne, a French university, where she earned her master\u2019s degree and doctorate in physics.\n\nMarie was fortunate to have studied at the Sorbonne with some of the greatest scientists of her day, one of whom was Pierre Curie. Marie and Pierre were married in 1895 and spent many productive years working together in the physics laboratory. A short time after they discovered radium, Pierre was killed by a horse-drawn wagon in 1906. Marie was stunned by this horrible misfortune and endured heartbreaking anguish. Despondently she recalled their close relationship and the joy that they had shared in scientific research. The fact that she had two young daughters to raise by herself greatly increased her distress.\n\nCurie\u2019s feeling of desolation finally began to fade when she was asked to succeed her husband as a physics professor at the Sorbonne. She was the first woman to be given a professorship at the world-famous university. In 1911 she received the Nobel Prize in chemistry for isolating radium. Although Marie Curie eventually suffered a fatal illness from her long exposure to radium, she never became disillusioned about her work. Regardless of the consequences, she had dedicated herself to science and to revealing the mysteries of the physical world.\n\"\"\"","9070801f":"# Converts the data into tokens\ntokenized_text = tf.keras.preprocessing.text.text_to_word_sequence(data)\ntokenized_text_size = len(tokenized_text)\n# Creates a vocab of unique words\nvocab = sorted(set(tokenized_text))\nvocab_size = len(vocab)\nprint('Vocab Size: {}'.format(len(vocab)))","a9ca335f":"n_gram_size = 3\n\nfor i in tokenized_text:\n    if len(i) > n_gram_size:\n        print(\"works\")\n    break  ","cdd2ce36":"# Map the vocab words to individual indices\nvocab_to_ix = {c:ix for ix,c in enumerate(vocab)}\n# Map the indices to the words in vocab\nix_to_vocab = np.array(vocab)\n# Convert the data into numbers\ntext_as_int = np.array([vocab_to_ix[c] for c in tokenized_text])","227a9ff4":"count = 0\nmodified_ix_to_vocab = []\nmodified_vocab_to_ix = {}\n\nfor i in list(vocab_to_ix.keys()):\n    j = '<'\n    j = j+i\n    j = j+'>'\n    modified_vocab_to_ix[j] = count\n    modified_ix_to_vocab.append(j)\n    count += 1\n\nmodified_ix_to_vocab = np.array(modified_ix_to_vocab)","57c2c665":"def create_ngrams(word, ngram_size = 3):\n    list_of_ngrams = []\n    if len(word) >= ngram_size:\n        for i in range(len(word) - (ngram_size) +1):\n            if i == 0 :\n                ngram = '<'\n            else :\n                ngram = ''  \n            for j in range(i, i+3):\n                ngram += word[j]\n            if i == (len(word) - (ngram_size)):\n                ngram += '>'\n            list_of_ngrams.append(ngram)\n        ngram = ''\n        ngram = '<'\n        ngram += word\n        ngram += '>'\n        list_of_ngrams.append(ngram)\n\n        return list_of_ngrams\n    elif len(word) < ngram_size:\n        ngram = '<'\n        ngram += word\n        ngram += '>'\n        list_of_ngrams.append(ngram)\n        return list_of_ngrams","08a945ff":"print(create_ngrams(ix_to_vocab[145]))","6368cf41":"count = 0\ntotal_sub_word_count = 0\nsubwords_dict = {}\n\nfor i in ix_to_vocab:\n    dummy_list = create_ngrams(i)\n    subwords_dict[count] = dummy_list\n    count += 1\n    total_sub_word_count += len(dummy_list)\n    \nprint(subwords_dict[7])    ","f1fc9da4":"dummy_word_list = []\nfor i in range(count):\n    for j in subwords_dict[i]:\n        dummy_word_list.append(j)\n        \ndummy_word_set = sorted(set(dummy_word_list))     ","0233b3ab":"count = 0\nngram_to_idx = {}\n\nfor i in dummy_word_set:\n    ngram_to_idx[i] = count\n    count += 1\n    \nprint(ngram_to_idx['<a>'])    ","aabab8eb":"embd_size = 20\nwindow_size = 3\n\nvector_space =  tf.Variable(np.random.rand(len(dummy_word_set), embd_size))","efb27914":"def mul_func(center_word_index, context_word_index):\n    combined_ngram_sum = 0\n    cotext_word = modified_ix_to_vocab[context_word_index - 1]\n    ngram_idx = ngram_to_idx[cotext_word]\n\n    context_vector = vector_space[ngram_idx]\n    list_of_center_ngrams = subwords_dict[center_word_index]\n    if len(list_of_center_ngrams) > 1:\n        for i in list_of_center_ngrams[:-1]:\n            ngram_idx = ngram_to_idx[i]\n            combined_ngram_sum += vector_space[ngram_idx]\n        final_idx = ngram_to_idx[list_of_center_ngrams[-1]]\n        tf.compat.v1.assign(vector_space[final_idx], combined_ngram_sum)\n    else:\n        for i in list_of_center_ngrams:\n            ngram_idx = ngram_to_idx[i]\n            combined_ngram_sum += vector_space[ngram_idx]\n        tf.compat.v1.assign(vector_space[ngram_idx], combined_ngram_sum)\n\n    return tf.matmul(tf.expand_dims(combined_ngram_sum, 1), tf.expand_dims(context_vector, 1), transpose_a= True )","b2564170":"def log_func(x):\n    return tf.math.log(1+tf.math.exp(tf.negative(x)))","d0c9d8cb":"def return_neg(center, context):\n    g = random.sample(range(181), 5)\n    while center in g or context[0] in g or context[1] in g:\n        g = random.sample(range(100), 5)\n    return g  ","44630d20":"def train_step(center_word_idx, list_of_context_word_idx):\n    list_of_negative_idx = return_neg(center_word_idx, list_of_context_word_idx )\n    negative_sampling_loss = 0\n    positive_sampling_loss = 0\n\n    with tf.GradientTape() as tape:\n        for idx in list_of_negative_idx:\n            negative_sampling_loss += log_func(tf.negative(mul_func(center_word_idx, idx)))\n    \n      \n        for idx in list_of_context_word_idx:\n            positive_sampling_loss += log_func(mul_func(center_word_idx, idx))\n\n        loss = positive_sampling_loss + negative_sampling_loss  \n\n  \n    grad = tape.gradient(loss, [vector_space])\n    opt.apply_gradients(zip(grad, [vector_space]))\n","6af8113d":"epochs = 200\nopt = tf.keras.optimizers.Adam(learning_rate=0.01)","cbcd0a7b":"for epoch in tqdm(range(epochs)):\n    for k in range(1, len(text_as_int)-2):\n        train_step(text_as_int[k], [text_as_int[k-1],text_as_int[k+1]])","f27d7a38":"The above two functions deal with the loss function, while the function below returns a set of non context word indexes for our negative sampling loss.","ae643690":"## Designing the Loss Function\n\nOur main intention is to make sure a word is represented by its n-grams. \n\n![image.png](attachment:image.png)\n\n","46e5aa05":"We add all the lists to a dictionary called subword dictionary. ","7c5bf091":"Here, we are mapping our data into integers, as we have to feed it to the model in integer format. ","0f907287":"## Creating the n-grams\n\nAccording to the paper, we break words into sub words like the following example:\n\n![image.png](attachment:image.png)\n\nWe also keep the parent word intact to assign it a separate vector later on while creating our embedding space. ","55b12862":"Let's put our function to use!","0dc121cf":"Since the paper represents each word with special characters in the front and in the end to signify which ngram represents the beginning and the end, we shall modify our vocabulary accordingly. \n![image.png](attachment:image.png)\n","ac48a108":"## Text Processing \n\nWe do basic text processing like removing punctuations and making all words lower case. We take help of the Keras preprocessing for text module. ","596d31cc":"We keep our ngram size at 3, we can have a variable size, but for less complexity, we are choosing a fixed number. ","8a6fd40f":"## Data \n\nWe use a short paragraph based on Marie Curie and her achievements. ","eef0187e":"## Imports","2063a16c":"So our base function will be given by the above, where z_g represents each ngram of a center word wc.\n\nOur loss function from skip gram negative sampling was: \n\n![image.png](attachment:image.png)\n\n(* In negative sampling, we teach the center words to create a correlation with the context words while also teaching it to not create similarities to non-contextual words)\n","05020d38":"# Concept\n\nExpanding on the concept of Skip-gram, we keep our general model same as it is. Instead of having a single vector represent each word, we break the words into character n-grams. So each word is not only represented by an unique vector assigned to itself, but also as the sum of the vectors represented by the character ngrams. \n\nThis way, we not only teach our model word associations, but also try to explain the morphology behind different words. \n\n![image.png](attachment:image.png)","e69d6f6d":"So, the ngram dictionary has all the ngrams as its keys and indexes as its values. This will help us iterate over the combined vector space created for the ngrams and words. ","8891de16":"We shall be primarily working with numpy. ","f8940659":"# Introduction\n\nThe introduction of the Word2Vec introduced a way through which we could teach a machine the meaning of words through word associations. In normal usage we work with a limited vocabulary, which may be easier to predict through word association. However, when an erudite essay is written or a complex idea or emotion is sought to be expressed, words and expression appear that make use of much larger vocabulary and words which otherwise occur rarely. Should one try to train a model through word association route this may fail in such cases. On the other hand such rare words may be deciphered through morphology as they are constructed through part of other words or etymologically decipherable characteristics.\nSome language like Sanskrit or its modern derivative india languages are very apt for such training as they have systematically derivablefrom some route syllable.\n\n","f27c50a0":"## Creating the N-gram Reference \n\nIn the paper, to avoid memory leak issues, hashing was used to store the n-grams. Since our data isn't big, we can stick to using a simple dictionary to store and reference our ngrams. "}}