{"cell_type":{"d659a0d0":"code","09b19256":"code","054202d0":"code","d7747545":"code","bafb7294":"code","687a3731":"code","293d6c7c":"code","4dc15b18":"code","52c0f3f2":"code","b6de3ae2":"code","6baded83":"code","d938e2ea":"code","cac61efe":"code","e4347584":"code","52590804":"code","b2a6be36":"code","d05e55d0":"code","48bc37be":"code","a80110d2":"code","7ea7c36e":"code","7e8baeab":"code","8724e2af":"code","b3899ce3":"code","05ded102":"code","125c83a4":"code","924a8732":"code","30db5b82":"code","c13d5215":"code","40a6420c":"code","382532e9":"code","125c8bd0":"code","d52a2e78":"code","fecb8b4f":"code","b7ee4917":"code","778273d5":"code","34e183b3":"code","3a25732c":"code","bc7192a6":"code","75ba95f3":"code","8501df0d":"markdown","575a984c":"markdown","c4afdd7e":"markdown","e24e3047":"markdown","89208640":"markdown","91c19fbd":"markdown","71d4b015":"markdown","975a9787":"markdown","9c5a4a06":"markdown","73a8549b":"markdown","588383cd":"markdown","bfd6214c":"markdown","e1cce97f":"markdown","1bad1079":"markdown","6b487882":"markdown"},"source":{"d659a0d0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nROOT_DIR = '..\/input\/cassava-leaf-disease-classification\/'\nos.listdir(ROOT_DIR)\n\nimport json # to read in the 'label_num_to_disease_map.json' file","09b19256":"import matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport cv2\nimport random","054202d0":"from sklearn.model_selection import train_test_split","d7747545":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.utils import plot_model","bafb7294":"# set the training and test directory paths\nTRAIN_DIR = '..\/input\/cassava-leaf-disease-classification\/train_images\/'\nTEST_DIR = '..\/input\/cassava-leaf-disease-classification\/test_images\/'","687a3731":"# set seed\nseed = 42\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(seed)","293d6c7c":"train_df = pd.read_csv(ROOT_DIR + 'train.csv')\nsample_df = pd.read_csv(ROOT_DIR + 'sample_submission.csv')","4dc15b18":"print(train_df.shape, sample_df.shape)\ndisplay(train_df.head())","52c0f3f2":"f = open(ROOT_DIR + 'label_num_to_disease_map.json')\ndata = json.load(f)\nprint(json.dumps(data, indent = 2))","b6de3ae2":"z = train_df.sample(20)\ndisplay(z)\nimages, labels = z['image_id'].tolist(), z['label'].tolist()","6baded83":"plt.figure(figsize = (20,20))\nfor i in range(20):\n    plt.subplot(4,5,i+1)\n    img = cv2.imread(TRAIN_DIR + images[i])\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.title(data[str(labels[i])])","d938e2ea":"sns.set_style('whitegrid')\nsns.countplot(x = 'label', data = train_df, palette = 'Pastel1');","cac61efe":"pie_df = train_df['label'].value_counts().reset_index()\npie_df.columns = ['label', 'count']\nfig = px.pie(pie_df, values = 'count', names = 'label', color_discrete_sequence = px.colors.qualitative.Pastel)\nfig.show()","e4347584":"train_df = train_df.astype({\"label\": str})","52590804":"train, test = train_test_split(train_df, test_size = 0.15, random_state = seed)\nprint(train.shape, test.shape)","b2a6be36":"IMG_SIZE = 224\nsize = (IMG_SIZE,IMG_SIZE)","d05e55d0":"datagen = ImageDataGenerator(\n                    rotation_range = 30,\n                    width_shift_range = 0.2,\n                    height_shift_range = 0.2,\n                    shear_range = 0.2,\n                    zoom_range = 0.2,\n                    brightness_range = [0.5,1.5],\n                    horizontal_flip = True,\n                    vertical_flip = True,\n                    fill_mode = 'nearest'\n)","48bc37be":"validgen = ImageDataGenerator()","a80110d2":"train_generator = datagen.flow_from_dataframe(\n                    train,\n                    directory = TRAIN_DIR,\n                    x_col = \"image_id\",\n                    y_col = \"label\",\n                    target_size = size,\n                    class_mode = \"sparse\",\n                    batch_size = 64,\n                    shuffle = True,\n                    seed = seed,\n                    interpolation = \"nearest\"\n)","7ea7c36e":"valid_generator = validgen.flow_from_dataframe(\n                    test,\n                    directory = TRAIN_DIR,\n                    x_col = \"image_id\",\n                    y_col = \"label\",\n                    target_size = size,\n                    class_mode = \"sparse\",\n                    batch_size = 64,\n                    shuffle = False,\n                    seed = seed,\n                    interpolation = \"nearest\"\n)","7e8baeab":"img = cv2.imread(os.path.join(TRAIN_DIR,'1000201771.jpg'))\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(img)","8724e2af":"plt.figure(figsize = (10,10))\n# set the title\nplt.title('Augmented Images')\n# load the image\nimg = load_img(os.path.join(TRAIN_DIR,'1000201771.jpg'))\n# convert to numpy array\ndata = img_to_array(img)\n# expand dimension to one sample\nsamples = np.expand_dims(data, 0)\n# iterator\nitr = datagen.flow(samples, batch_size = 1)\n# generate samples and plot\nfor i in range(12):\n    # define subplot\n    plt.subplot(4,3,i+1)\n    # generate batch of images\n    batch = itr.next()\n    # convert to unsigned integers for viewing\n    image = batch[0].astype('uint8')\n    # plot raw pixel data\n    plt.imshow(image)\n# show the figure\nplt.show()","b3899ce3":"NUM_CLASSES = 5","05ded102":"def create_model():\n    \n    model = models.Sequential()\n    # initialize EfficientNetB0 model with input shape as (224,224,3)\n    model.add(EfficientNetB0(input_shape = (IMG_SIZE, IMG_SIZE, 3), include_top = False, weights = 'imagenet'))\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(256, activation = 'relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(NUM_CLASSES, activation = 'softmax'))\n    \n    return model","125c83a4":"model = create_model()","924a8732":"model.compile(loss = 'sparse_categorical_crossentropy',\n             optimizer = Adam(learning_rate = 0.001),\n             metrics = ['accuracy'])","30db5b82":"# Stop training when the validation loss metric has stopped decreasing for 5 epochs.\n#early_stopping = EarlyStopping(monitor = 'val_loss',\n                               #patience = 5,\n                               #mode = 'min',\n                               #restore_best_weights = True)\n\n# Save the model with the minimum validation loss\ncheckpoint = ModelCheckpoint('best_model.hdf5', \n                             monitor = 'val_loss',\n                             verbose = 1,\n                             mode = 'min', \n                             save_best_only = True)\n# reduce learning rate\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n                              factor = 0.2,\n                              patience = 3,\n                              min_lr = 0.001,\n                              mode = 'min',\n                              verbose = 1)","c13d5215":"STEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size","40a6420c":"history = model.fit(train_generator,\n                    validation_data = valid_generator,\n                    epochs = 5,\n                    steps_per_epoch = STEP_SIZE_TRAIN,\n                    validation_steps = STEP_SIZE_VALID,\n                    callbacks = [ checkpoint, reduce_lr]\n                   )","382532e9":"model.summary()","125c8bd0":"plot_model(model, show_shapes = True)","d52a2e78":"model.evaluate_generator(generator = valid_generator, steps = STEP_SIZE_VALID)","fecb8b4f":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'c-', label='Training accuracy')\nplt.plot(epochs, val_acc, 'y-', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'c-', label='Training Loss')\nplt.plot(epochs, val_loss, 'y-', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","b7ee4917":"val1=max(acc)\nprint(val1)","778273d5":"history = model.fit(train_generator,\n                    validation_data = valid_generator,\n                    epochs = 20,\n                    steps_per_epoch = STEP_SIZE_TRAIN,\n                    validation_steps = STEP_SIZE_VALID,\n                    callbacks = [ checkpoint, reduce_lr]\n                   )","34e183b3":"model.summary()","3a25732c":"plot_model(model, show_shapes = True)","bc7192a6":"acc1 = history.history['accuracy']\nval_acc1 = history.history['val_accuracy']\nloss1 = history.history['loss']\nval_loss1 = history.history['val_loss']\n\nepochs1 = range(len(acc1))\n\nplt.plot(epochs1, acc1, 'c-', label='Training accuracy')\nplt.plot(epochs1, val_acc1, 'y-', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs1, loss1, 'c-', label='Training Loss')\nplt.plot(epochs1, val_loss1, 'y-', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","75ba95f3":"val2=max(acc1)\nprint(val2)","8501df0d":"<center><h1> Model creation and training <\/h1><\/center> ","575a984c":"<center><h1> Introduction <\/h1><\/center>\n","c4afdd7e":"We will take a quick look at the data files and train our model in this notebook. The model can be saved and used for inference in a later notebook.","e24e3047":"### Image before Augmentation","89208640":"<center><h2> Visualizing some of the augmented images <\/h2><\/center>","91c19fbd":"<center><h1> Split dataset for training and validation <\/h1><\/center> \n<center> Reserving 15% of data for validation <\/center>","71d4b015":"<center><h1> Plotting a count plot and pie chart for class distribution. <\/h1><\/center> ","975a9787":"### Images after Augmentation","9c5a4a06":"* **Competition** - Given a set of images of Cassava leaves we need to classify them as one of the four diseases or healthy.\n* **Data** - A collection of 21397 labelled images\n* **Evaluation** - Classification accuracy","73a8549b":"<center><h1> Importing necessary libraries <\/h1><\/center> ","588383cd":"There is a class imbalance problem here. The Cassava Mosaic Disease (CMD) samples are more compared to other classes. We can solve this by either upsampling other class samples or downsampling the CMD samples.","bfd6214c":"<center><h1> Model evaluation <\/h1><\/center> ","e1cce97f":"<center><h1> Data exploration <\/h1><\/center> ","1bad1079":"<center><h1> Creating ImageDataGenerator to generate data in batches and perform image augmentation. <\/h1><\/center> ","6b487882":"<center><h1> Plotting 20 randomly sampled images with class <\/h1><\/center> "}}