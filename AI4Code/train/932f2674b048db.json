{"cell_type":{"c4f1b420":"code","89906943":"code","d48ccfbf":"code","945a7172":"code","236661f2":"code","f4939356":"code","d47d52bb":"code","0fb7c971":"code","1c9d53c1":"code","a090a5d0":"code","1c877300":"code","1b7f6c23":"code","a8c8cc55":"code","59ca3f8f":"code","0493e393":"code","f1726c9a":"code","364409c7":"code","5afc3d3d":"code","a3a76079":"code","63406a4c":"code","3fe13c8f":"code","10eb4552":"code","214ae74c":"code","0a5250cc":"code","ea99c7d7":"code","19e6a304":"code","4bf5611c":"code","04a2b8e1":"code","cb4f72c3":"code","4b2af60d":"code","79441907":"code","6f240b70":"code","4981ce03":"code","74f3b9cc":"code","dee92a88":"code","20e89c0e":"code","877c5809":"code","ae33cd24":"code","80e28a04":"code","026057c4":"code","6ade3cbf":"code","294e5860":"code","ab856f00":"code","da98d37d":"code","e93b816a":"code","e1c0fa89":"code","9079ebde":"code","91c97e49":"code","b0defc8b":"code","3a7b309f":"code","a304eb4f":"code","b2d1aef1":"code","d8696438":"code","cf5e042c":"code","86c41b02":"markdown","809032bc":"markdown","6175f48c":"markdown","6ca3f98b":"markdown","8ca2f979":"markdown","6b1b2cb1":"markdown","93a09671":"markdown","e6a0b9cf":"markdown","90aa9728":"markdown","e60c5490":"markdown","0163be1f":"markdown","f0683d58":"markdown","796cb286":"markdown","a67fdfa8":"markdown","41f7628e":"markdown","d6c60e49":"markdown","38b6e382":"markdown","a2ca6b36":"markdown","2b9f410a":"markdown","7a4bba6a":"markdown","eab70dbe":"markdown","7910643f":"markdown","7af91e58":"markdown","2a47c01c":"markdown","c8304c94":"markdown","2446f5f5":"markdown","cb62fd58":"markdown","d52b4416":"markdown","f649654f":"markdown","fb169753":"markdown","5f8c5784":"markdown","6503115b":"markdown","4eeea96f":"markdown","7a5eb163":"markdown","81be6c98":"markdown","eb3b1000":"markdown","b33762a1":"markdown","dda94103":"markdown"},"source":{"c4f1b420":"import numpy as np, pandas as pd, matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n%matplotlib inline","89906943":"grove_data = pd.read_csv(\"..\/input\/groove-st-path-city-bookings\/Grove St PATH_bookings.csv\")","d48ccfbf":"plt.figure(figsize=(20,20))\nfor yy in [2015,2016,2017,2018,2019,2020]:\n    plt.subplot(6,1,yy-2014)\n    plt.plot(grove_data[grove_data['YY'] == yy]['Bookings'].values)\n    plt.xticks(())\n    plt.xlabel(yy)\nplt.show()","945a7172":"grove_data.info()","236661f2":"grove_data['Day'] = grove_data['Day'].astype(object)\ngrove_data['MM'] = grove_data['MM'].astype(object)\ngrove_data['DD'] = grove_data['DD'].astype(object)","f4939356":"grove_data.info()","d47d52bb":"grove_dummies = pd.get_dummies(grove_data)","0fb7c971":"grove_dummies.info()","1c9d53c1":"plt.figure(figsize=(25,20))\nsns.heatmap(grove_dummies.corr())\nplt.show()","a090a5d0":"def tt_split(df, split_idx, test_days = None):\n    train = df.iloc[:split_idx,:]\n    \n    if test_days == None:\n        test = df.iloc[split_idx:,:]\n    else:\n        test_end_idx = split_idx + (test_days * 8) \n        test = df.iloc[split_idx:test_end_idx+1,:]\n        '''\n        I multiplied days with 8 as 8 rows are there for single day as 8 time slots.\n        '''\n        \n        \n    X_train = train.drop(columns = [\"Bookings\"]).values\n    X_test = test.drop(columns = [\"Bookings\"]).values\n    y_train = train[\"Bookings\"].values\n    y_test = test[\"Bookings\"].values\n    \n    return X_train, X_test, y_train, y_test","1c877300":"def modelPerformance(model, X_train,X_test, y_train, y_test):\n    print(\"====================================================================================\")\n    model.fit(X_train, y_train)\n    \n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    \n    train_predictions = model.predict(X_train)\n    test_predictions = model.predict(X_test)\n    \n    train_rms = np.sqrt(np.sum(np.square(train_predictions - y_train)) \/ y_train.shape[0])\n    test_rms = np.sqrt(np.sum(np.square(test_predictions - y_test)) \/ y_test.shape[0])\n    \n    train_mae = np.sum(np.sqrt(np.square(y_train - train_predictions))) \/ y_train.shape[0]\n    test_mae = np.sum(np.sqrt(np.square(y_test - test_predictions))) \/ y_test.shape[0]\n    \n    \n    \n    perf_matrix = [\n        [\"Train\", str(train_score), str(train_rms), str(train_mae)],\n        [\"Test\", str(test_score), str(test_rms), str(test_mae)]\n    ]\n    print(\"Model: \",model,\"\\n\\nPerformance and Predictions Trend: \")\n    print(perf_matrix)\n    plt.figure(figsize=(20,5))\n    plt.plot(y_train, label = \"Train Values\")\n    plt.plot(train_predictions, '--', label = \"Predicted Train Values\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Rentals\")\n    plt.legend()\n    plt.show()\n    plt.figure(figsize=(20,5))\n    plt.plot(y_test, label = \"Test Values\")\n    plt.plot(test_predictions, '--', label = \"Predicted Test Values\")\n\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Rentals\")\n    \n    plt.table(cellText=perf_matrix,\n        cellLoc=\"center\", colWidths=None,\n        rowLabels=None, rowColours=None, rowLoc=\"center\",\n        colLabels=[\"Data\",\"R^2 Score\",\"RMS Score\",\"MAE Score\"], colColours=\"yyyy\", colLoc=\"center\",\n        loc='top', bbox=None)\n    plt.legend()\n    plt.box(True)\n    plt.show()\n    \n    \n    return model, test_predictions","1b7f6c23":"def retrieveData(data, yy = None, mm = None, dd = None):\n    data = data\n    \n    if yy != None:\n        \n        data = data[data['YY'] == yy]\n        \n        if mm != None:\n            \n            data = data[data['MM'] == mm]\n            \n            if dd != None:\n                \n                data = data[data['DD'] == dd]\n                \n    return data","a8c8cc55":"yy = 2020\nmm = 1\nX_train, X_test, y_train, y_test = tt_split(grove_dummies,retrieveData(grove_data, yy, mm).index[0], \n                                            test_days = 31)\n_, __ = modelPerformance(KNeighborsRegressor(n_neighbors=2),X_train,X_test,y_train,y_test)","59ca3f8f":"scaler = MinMaxScaler(feature_range=(0,1))","0493e393":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","f1726c9a":"_, __ = modelPerformance(KNeighborsRegressor(n_neighbors=2), X_train_scaled, X_test_scaled, y_train, y_test)","364409c7":"def memoryFeatures(inpt, n_features):\n    '''\n    n_features will decide how many previous instances to be taken into account.\n    '''\n    \n    outpt = np.zeros((n_features,n_features), dtype = int)\n    for i in range(len(inpt)):\n        row = np.array(inpt[i-n_features : i])\n        outpt = np.append(outpt, row)\n    del inpt\n    return outpt.reshape(-1,n_features)","5afc3d3d":"outpt = memoryFeatures(grove_data['Bookings'].values, 5)","a3a76079":"pd.DataFrame(outpt).tail(10)","63406a4c":"def genMemDataset(df, memory):\n    return pd.concat([df, pd.DataFrame(memoryFeatures(df['Bookings'].values,\n                                                      memory))], axis = 1, ignore_index=False)","3fe13c8f":"models = [LinearRegression(),\n          KNeighborsRegressor(n_neighbors=1),\n          KNeighborsRegressor(n_neighbors=2), \n          KNeighborsRegressor(n_neighbors=3), \n          KNeighborsRegressor(n_neighbors=4),\n          RandomForestRegressor(n_estimators=5), \n          RandomForestRegressor(n_estimators=10), \n          RandomForestRegressor(n_estimators=50),\n          RandomForestRegressor(n_estimators=100),\n          RandomForestRegressor(n_estimators=500)]","10eb4552":"yy = 2020\nmm = 1\nX_train, X_test, y_train, y_test = tt_split(genMemDataset(grove_dummies, 8),retrieveData(grove_data, yy, mm).index[0], test_days = 31)\n\nfor model in models:\n    \n    _, __ = modelPerformance(model, X_train, X_test, y_train, y_test)","214ae74c":"scaler = MinMaxScaler()","0a5250cc":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","ea99c7d7":"for model in models:\n    _, __ = modelPerformance(model, X_train_scaled, X_test_scaled, y_train, y_test)","19e6a304":"memory_dataset = genMemDataset(grove_dummies, 8)","4bf5611c":"model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n          metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n          weights='uniform') ","04a2b8e1":"def customPerformance(data, model, td, yy, mm, dd = None):\n    yy = yy\n    mm = mm\n    dd = dd\n    td = td\n    spidx = retrieveData(grove_data, yy, mm, dd).index[0]\n    print(spidx)\n    X_train, X_test, y_train, y_test = tt_split(data, spidx, test_days = td)\n\n    scaler = MinMaxScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    fitted_model, _ = modelPerformance(model,X_train,X_test,y_train,y_test)\n    return fitted_model","cb4f72c3":"customPerformance(memory_dataset, model, 7, 2020, 1)","4b2af60d":"customPerformance(memory_dataset, model, 7, 2020, 1, 15)","79441907":"customPerformance(memory_dataset, model, 7, 2020, 2)","6f240b70":"customPerformance(memory_dataset, model, 31, 2020, 3)","4981ce03":"customPerformance(memory_dataset, model, 31, 2019, 6)","74f3b9cc":"holidays = pd.read_csv(\"..\/input\/groove-st-path-city-bookings\/usholidays.csv\")","dee92a88":"holidays.head()","20e89c0e":"holidays = holidays['Date'].values","877c5809":"grove_holidays = []\nfor j in range(grove_data.shape[0]):\n    i=0\n    date = str()\n    for el in grove_data.iloc[j,[0,1,2]].values:\n        date = date+str(el)\n        if i<2:\n            date = date+'-'\n        i=i+1\n    date = str(pd.to_datetime([date])[0])[:10]\n    if date in holidays:\n        h = 1\n    else:\n        h = 0\n    grove_holidays.append(h)\ngrove_holidays = np.array(grove_holidays)","ae33cd24":"grove_holidays = pd.concat([grove_data,pd.Series(grove_holidays, name = \"Holiday\")], axis = 1)","80e28a04":"grove_holidays.info()","026057c4":"grove_holidays_dummies = pd.get_dummies(grove_holidays)","6ade3cbf":"grove_holidays_dummies.columns","294e5860":"memory_dataset_holidays = genMemDataset(grove_holidays_dummies, 8)","ab856f00":"plt.figure(figsize=(25,20))\nplt.title(\"Pearson's Correlation Coefficient\")\nsns.heatmap(memory_dataset_holidays.corr(), annot=False)\nplt.show()","da98d37d":"customPerformance(memory_dataset_holidays, model, 31, 2019, 6)","e93b816a":"fitted_model = customPerformance(memory_dataset_holidays, RandomForestRegressor(n_estimators=50),31, 2019, 6)","e1c0fa89":"plt.figure(figsize=(20,5))\nplt.title(\"Feaure Importances\")\nsns.barplot(memory_dataset_holidays.drop(columns = ['Bookings'\n                                               ]).columns.astype(str).values,fitted_model.feature_importances_)\nplt.xticks(rotation=90)\nplt.plot()","9079ebde":"customPerformance(memory_dataset_holidays, KNeighborsRegressor(n_neighbors = 5), 30, 2019, 6)","91c97e49":"customPerformance(grove_holidays_dummies, KNeighborsRegressor(n_neighbors = 5), 30, 2019, 6)","b0defc8b":"def genPredictions(data, model, yy, mm, dd = None):\n    yy = yy\n    mm = mm\n    dd = dd\n    \n    if mm in [1,3,5,7,8,10,12]:\n        td = 31\n    elif mm in [4,6,9,11]:\n        td = 30\n    else:\n        if yy%4 == 0:\n            td = 29\n        else:\n            td = 28\n            \n    spidx = retrieveData(grove_data, yy, mm, dd).index[0]\n    print(\"Test Data taking from:\", str(yy) + '-' + str(mm), \"for\",td, \"days | Split at index:\", spidx)\n    X_train, X_test, y_train, y_test = tt_split(data, spidx, test_days = td)\n\n    scaler = MinMaxScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    fitted_model, predictions = modelPerformance(model,X_train,X_test,y_train,y_test)\n    return predictions, y_test","3a7b309f":"preds, true = genPredictions(memory_dataset_holidays, KNeighborsRegressor(n_neighbors = 5), 2020, 1)","a304eb4f":"def timeline_forecast(yy_start, mm_start, yy_end, mm_end):\n    yy = yy_start\n    mm = mm_start\n    \n    forecasts = np.array([])\n    true_vals = np.array([])\n    \n    while True:\n        preds, true = genPredictions(memory_dataset_holidays, KNeighborsRegressor(n_neighbors = 5), yy, mm)\n\n        mm = mm + 1\n        \n        if mm == 13:\n            mm = 1\n            yy = yy + 1\n            \n        forecasts = np.append(forecasts, preds)\n        true_vals = np.append(true_vals, true)\n        \n        if yy>=yy_end and mm>mm_end:\n            break\n            \n            \n    return forecasts, true_vals","b2d1aef1":"f, t = timeline_forecast(2019,1,2020,5)","d8696438":"plt.figure(figsize=(30,10))\nplt.plot(f)\nplt.plot(t)","cf5e042c":"pd.DataFrame(f).to_csv(\"timeline_forecasts.csv\")\npd.DataFrame(t).to_csv(\"timeline_true_vals.csv\")","86c41b02":"## Sequence Memory Inclusion\n\nAs I was trying out models in other notebooks, I realised that the results were not upto mark, So I came across a great idea. I remembered that in Deep Learning Techniques, LSTMs models are used widely for sequence predicions, They have a unique functionality of inclusion of a memory element, Meaning: They compute the result while taking old values in account too. In our case, Today 3pm Bookings would be calculated on the basis of bookings of previous time slots also. \n\nIt makes sense as generally whenever a peak is attained, during next timeslots bookings are more dependent on previous\n\nSo these feauture sets will be dealt as input for our model too along with existing features, ready for concatenation.\n\nFollowing function will generate the same:","809032bc":"### Let us Check Correlations among features","6175f48c":"#### Insights form above:\n* peaks are attained each day\n* A little increase in bookings in subsequent year\n* Impact of Covid-19 in year 2020.\n\n### Let's check more Infomation","6ca3f98b":"### 3. Data Retrieval Function\n- It helps me to retrieve data of specified set of parameters which are optional accordingly\n- Say I want data for May,2020 , my params would be yy = 2020, mm = 5 and dd= None\n- Say I want data for 20,May,2020 , my params would be yy = 2020, mm = 5 and dd = 5.\n- It uses powerful vectorisation provided by pandas.","8ca2f979":"## Scale Again\n\nWe would have to scale the data again as new unscaled features came along","6b1b2cb1":"## What type of data are we dealing with?\n\nMost of the data preprocessing part has been done in separate notebook named Data Transformations but I will explain breifly.\n\nSo, Origginal data was in the form of all the bookings (2015-20) as a rows from different stations with many parameters like duration, lat\/long, start station, end station etc.\n\nI had to carry out various data transformations in ordered to think a solution and subsequently head to feature engineering.\n\n1. I took all the bookings of one station.\n2. Feature-ised it in YY, MM, DD, HH, mm etc.\n3. I added a Day of the week feature (calculated from pandas functionality) as day would have significant impact in results (say weekends and weekdays)\n4. I decided to make Hour of the day (HH) as a categorical feature and binned in time slots of 3 hours. \n     (0-3, 3-6, 6-9, 9-12, 12-15, 15,18, 18-21, 21-0)\n5. Hence our results would be in the same way.\n\n\/\/(For Local Repo) All this data was stored in separate directory structure and linked by a custom written data pipeline module \"data_paths\" which I imported earlier.\n\nThis will enable us to maintain reusability of code and better data organisation especially in the case of live data.\n\nSo here we load our data!","93a09671":"##### Test for March","e6a0b9cf":"##### Test again for June 2019","90aa9728":"### 2. Model Performance\n- Inputs: ScikitLearn Model, Training and Testing data\n- Returns: Trained Model\n\n- First Model Fits the Train Data (ln 2)\n- Various Train Test Scores are calculated (R^2, RMS, Mean Absolute Error) (ln 5 - 15)\n- Then I stored all values as string in a matrix perf_matrix\n- Rest it prints the model and its Hyperparameters and Plots the results\n- It shows all the scores in a table over Plot.\n- We can easily detect all the biases and train quality through it.","e60c5490":"# Booking Forecast for BikeIntel \n###### Project by Pulkit Mehta https:\/\/github.com\/pulkitmehta\n\nWelcome Fellas to my notebook. This is my first Notebook on Kaggle, The main project was done on Local environment (I just uploaded the Notebook directly) so some things can go bit unrelated.\n\nHere I will demonstrate how I predicted Bike Rentals through Active Machine Learning Startegies instead of Complex Deep Learning Strategies while not or little compromising with accuracies. \n\nThis Notebook is a mature version of previous research nb which can be found on my GitHub repository. But it is fair if you want to skip.\n\nHere I will go through all the snippets and Visualizations throughout.","0163be1f":"## Custom Performance\n\n- This function will take the data, model, date params and most imp Number of testing days\n- Calculate the splitting index \n- Use tt_split to split the data\n- Scale it\n- Show the performance","f0683d58":"##### Test for Feb","796cb286":"### Here I Used all above together as a demonstration","a67fdfa8":"## Feature Engineering and Model Selection\n### Scaling\n- Scaling would be very important as Gradient Descent based Methods are sesnsitive to magnitude of values \n- Neighbour based (Distance based Optimizations) techniques also required magnitudes to be scaled.\n- Tree based would be robust against magnitude.\n- So I will try scaling values within 0 and 1","41f7628e":"## Why not Deep Learning Startegies this time?\n\nIt has been evident that DL strategies have been proved super accurate in solving problems BUT many times there is a downside of high computational and running model costs.\n\nSo My goal is Low Costs and Better Performance.","d6c60e49":"###### Test from 1 jan +7 days","38b6e382":"### Now I can Plot and Save it.","a2ca6b36":"## What business problem model I am trying to solve?\n\nWell BikeIntel project is a multi goal oriented project. Specifically, here we are predicting a regression time series forecast.\n\nSuppose Company X offers bike rental services in a city. They want to solve a problem of shortage of bikes at a particular renting station also they want to intimate their customers about when the demand is expected to go high (So they may pre-book).\n\nSolution is to forecast number of bookings in coming hours.\n\n\n![](https:\/\/raw.githubusercontent.com\/pulkitmehta\/bikeIntel\/master\/cover_img.jpg)","2b9f410a":"## Test Data of lesser Days","7a4bba6a":"## Generating Monthly Predictions","eab70dbe":"#### Test for Jan","7910643f":"#### Insights from above:\n- Bookings are mostly imapacted positive linearly in time slots of 15-18 and 18-21 Hrs. which is obvious in evening time and rest Hours it is inversely correlated to hrs\n- Bookings are negatively impacted on weekends ie (Day 5 and Day 6) where Day 1 is Monday.\n- And One minute insight is in the month of January and December. Again this is obvious as Holidays Season is on the peak.","7af91e58":"##### Result:","2a47c01c":"#### Note: I decided to convert Day, MM, DD as categorical features\n\n#### So, here we go...","c8304c94":"### So what can be done?\nWell most effort has been put in robust Feature Engineering and Model Selection with active visualisation side by side.","2446f5f5":"### Let's test our results with fllowing models","cb62fd58":"## Importing Necessary Libraries and Classes","d52b4416":"## Feature Importance(s)\n\nNow Lets have a little Data Insight Mining, As we can see below features named '0' and '7' has a really high role in deciding bookings. So what are they?\n- '0' is Number of Bookings in the same time slot yesterday.\n- '1' is Number of Bookings in the previous time slot.\n\nAgain these make sense and cannot be ignored and this is how I have engineered feature which is even more important than any existing default one.","f649654f":"#### Covid Impact can be noticed and easilty estimated, Business Loss can also be calculated from above","fb169753":"## Helper Functions\nThese functions would ease our process throughout by adding reusability and will provide valuable running results.\n\nLet's Check first one:\n\n### 1. Train-Test data Splitting\n#### Inputs:\n- DataFrame to Split\n- Index to which Train data would end exclusively and Test data would begin inclusively.\n- Number of test data days to keep.\n- if Test days is None, all remeing days are taken into account as test.\n\n#### Outputs:\n- Tuple with Train Test splitted data","5f8c5784":"- Written By: Pulkit Mehta (https:\/\/github.com\/pulkitmehta)\n- Full Project Repo: https:\/\/github.com\/pulkitmehta\/bikeIntel\n\n### Kindly Share your valuable suggestions and Upvote if you liked the work :)\n\n# -- END--","6503115b":"- Full Project Repo: https:\/\/github.com\/pulkitmehta\/bikeIntel","4eeea96f":"## Generate Timeline Forecast from start to end YY-MM ","7a5eb163":"### Holidays inclusion\n\nAnother feature Engineering we can do is include Holidays. I saw above that models are weak in months of December and January, So We can make a binary feature of Holidays. But these are just Federal Holidays so not much can be expected. ","81be6c98":"## Checking the data Distribution","eb3b1000":"### This function will finally generate and return the dataframe","b33762a1":"###### Test from 15 jan +7 days","dda94103":"##### Test for June 2019"}}