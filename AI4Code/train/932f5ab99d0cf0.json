{"cell_type":{"5adadf56":"code","1093f738":"code","62f87724":"code","aff3bff5":"code","7e5a46d2":"code","d77e9f68":"code","91cabff8":"code","017a027d":"code","6ae4c4d9":"code","15bc39fe":"code","44d63236":"code","31b492d3":"code","83dfc086":"code","387e9dec":"code","fc804988":"code","7da03725":"code","9ceecddd":"code","c47fe714":"code","28f8255a":"code","08148b10":"code","aa3c3149":"code","8b1dbcbb":"code","ff113f9d":"code","29828d72":"code","ff28635e":"code","77774355":"code","8b80b87a":"code","36fbc66e":"code","72f6d3c8":"code","fa01aa5b":"code","1deb823d":"code","94a151f7":"code","c6ae6e25":"code","21143dc0":"code","2b275573":"code","5e6a1c2e":"code","63dbe248":"code","986f4a58":"code","a45867c2":"code","a706dafe":"code","57e4e069":"code","298862e9":"code","2ddd598c":"code","6b951196":"code","b732d49c":"code","4ceb277d":"code","0785381d":"code","f75306b8":"code","8b610121":"code","5a985900":"code","87410dbd":"code","28ef591f":"code","cb037c05":"code","5e6b9867":"code","c8f49f38":"code","2cbf7626":"code","95b9ff7d":"code","b3bd6161":"code","e9377f6c":"code","94cc9e53":"code","3d9918dd":"code","b5ffe909":"code","5be6dddd":"code","fdcc5180":"code","611bb1fa":"code","30022412":"code","d295a2ba":"code","d332f1a3":"code","b1013639":"code","2fcb0fd1":"code","761fbe86":"code","913ac323":"code","cb8d895a":"code","3f45a73b":"code","b9fea2dc":"code","c73706f9":"code","233e751f":"code","99db2d6f":"code","a1cadca3":"code","91b4ae9d":"code","7de63a76":"code","23313273":"code","81606985":"code","5af57f6a":"code","a3553063":"code","c738feea":"code","2709fc37":"code","75e957f3":"code","63b6d87c":"code","44e8630e":"code","890f347a":"code","49b2d9a8":"code","0e6ce774":"code","4f3f8d5d":"code","be1af674":"code","6c78cd28":"markdown","e6e199e7":"markdown","d99d0fe1":"markdown","42965ec9":"markdown","ee6ec253":"markdown","afa358bf":"markdown","fa3f4ae3":"markdown","3936a532":"markdown","b19d73da":"markdown"},"source":{"5adadf56":"import pandas as pd\nimport numpy as np\nimport category_encoders as ce\nimport matplotlib as plt\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack\nimport lightgbm as lab\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LogisticRegression\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\npd.set_option('display.max_columns', 500)\n","1093f738":"# X_train= pd.read_csv('..\/input\/train.csv',parse_dates=['issue_d','earliest_cr_line'])\n# X_test = pd.read_csv('..\/input\/test.csv',parse_dates=['issue_d','earliest_cr_line'])\ndf= pd.read_csv('..\/input\/train.csv')\nX_test = pd.read_csv('..\/input\/test.csv')\ny_train= df[\"ConvertedSalary\"]\nX_train=df.drop(\"ConvertedSalary\",axis=1)","62f87724":"print(X_train.shape)\nprint(X_test.shape)\n","aff3bff5":"##addr_state\u306b\u7def\u5ea6\u7d4c\u5ea6\u3092join\u3059\u308b\u5834\u5408<-\u5916\u90e8\u30c7\u30fc\u30bf\u304c\u5fc5\u8981\u3060\u3063\u305f\u3089\u3002\nadd = pd.read_csv('..\/input\/country_info.csv')\nX_all=pd.concat([X_train,X_test],axis=0)\n#\u56fd\u3092\u5909\u63db\ncounty={'Bahamas':'Bahamas, The', 'Bosnia and Herzegovina':'Bosnia & Herzegovina', 'Congo, Republic of the...':'Congo, Repub. of the',\n \"C\u00f4te d'Ivoire\":\"Cote d'Ivoire\",\n 'Democratic Republic of the Congo':'Congo, Dem. Rep.',\n 'Gambia':'Gambia, The',\n 'Hong Kong (S.A.R.)':'Hong Kong',\n 'Iran, Islamic Republic of...':'Iran',\n 'Libyan Arab Jamahiriya':'Libya',\n 'Montenegro':'Other',\n 'Myanmar':'Other',\n 'Other Country (Not Listed Above)':'Other',\n 'Republic of Korea':'Korea, South',\n 'Republic of Moldova':'Moldova',\n 'Russian Federation':'Russia',\n 'South Korea':'Korea, South',\n 'Syrian Arab Republic':'Syria',\n 'The former Yugoslav Republic of Macedonia':'Macedonia',\n 'Trinidad and Tobago':'Trinidad & Tobago',\n 'United Republic of Tanzania':'Tanzania',\n 'Venezuela, Bolivarian Republic of...':'Venezuela',\n 'Viet Nam':'Vietnam',\n}\nX_all.Country = X_all.Country.replace(county)\nX_all=pd.merge(X_all, add, on='Country', how='left')\nX_train= X_all.iloc[:X_train.shape[0],:]\nX_test=X_all.iloc[X_train.shape[0]:,:]\n","7e5a46d2":"print(X_train.shape)\nprint(X_test.shape)","d77e9f68":"#MilitaryUS_Hobby_OpenSource\nX_train[\"MilitaryUS\"]=X_train[\"MilitaryUS\"].fillna(\"No\")\nX_test[\"MilitaryUS\"]=X_test[\"MilitaryUS\"].fillna(\"No\")\nYN = {'Yes':1, 'No':0}\nMHO=[\"MilitaryUS\",\"Hobby\",\"OpenSource\",\"AdBlocker\",\"AdBlockerDisable\"]\n\nfor col in MHO:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\n","91cabff8":"print(X_train[\"Student\"].value_counts())\nprint(X_test[\"Student\"].value_counts())","017a027d":"YN = {'No':0, 'Yes, full-time' :2,'Yes, part-time':1}\nMHO=[\"Student\"]\n\nfor col in MHO:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)","6ae4c4d9":"YN = {'0-2 years':1,\n '12-14 years':13,\n '15-17 years':16,\n '18-20 years':19,\n '21-23 years':22,\n '24-26 years':25,\n '27-29 years':28,\n '3-5 years':4,\n '30 or more years':30,\n '6-8 years':7,\n '9-11 years':10}\n\nlis=[\"YearsCoding\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\n","15bc39fe":"YN={'20 to 99 employees':50,\n'100 to 499 employees':250,\n'10,000 or more employees':10000,\n'1,000 to 4,999 employees':2500,\n'10 to 19 employees':15,\n'Fewer than 10 employees':5,\n'500 to 999 employees':750,\n'5,000 to 9,999 employees':7500}\nlis=[\"CompanySize\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\n\n\nX_train[\"CompanySize\"].value_counts()","44d63236":"YN={\"3-5 years\":4,\n\"0-2 years\":1,\n\"6-8 years\":7,\n\"12-14 years\":13, \n\"9-11 years\":10,\n\"15-17 years\":16,\n\"18-20 years\":19,\n\"21-23 years\":22,\n\"30 or more years\":31,\n\"24-26 years\":25,\n\"27-29 years\":28}\n\nlis=[\"YearsCodingProf\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\nX_train[\"YearsCodingProf\"].value_counts()","31b492d3":"YN={\"Less than a year ago\":1,\n\"Between 1 and 2 years ago\":2,\n\"Between 2 and 4 years ago\":3,\n\"More than 4 years ago\":4, \n\"I've never had a job\":0,}\n\nlis=[\"LastNewJob\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\nX_train[\"LastNewJob\"].value_counts()","83dfc086":"YN={'Agree':3,\n 'Disagree':1,\n 'Neither Agree nor Disagree':2,\n 'Strongly agree':4,\n 'Strongly disagree':0}\n\nlis=[\"AgreeDisagree1\",\"AgreeDisagree2\",\"AgreeDisagree3\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\nX_train[\"AgreeDisagree1\"].value_counts()","387e9dec":"YN={'Neither agree nor disagree':3,\n 'Somewhat agree':4,\n 'Somewhat disagree':2,\n 'Strongly agree':5,\n 'Strongly disagree':1}\n\nlis=[\"AdsAgreeDisagree1\",\"AdsAgreeDisagree2\",\"AdsAgreeDisagree3\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\n# set(X_train[\"AdsAgreeDisagree1\"].values)","fc804988":"YN={'1 - 4 hours':3,\n '5 - 8 hours':6,\n '9 - 12 hours':10,\n 'Less than 1 hour':0,\n 'Over 12 hours':12}\n\nlis=[\"HoursComputer\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\n","7da03725":"YN={'1 - 2 hours':2,\n '3 - 4 hours':4,\n '30 - 59 minutes':1,\n 'Less than 30 minutes':0,\n 'Over 4 hours':5}\n\nlis=[\"HoursOutside\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)","9ceecddd":"set(X_train[\"Age\"].values)\nYN={'18 - 24 years old':22,\n '25 - 34 years old':27,\n '35 - 44 years old':40,\n '45 - 54 years old':50,\n '55 - 64 years old':60,\n '65 years or older':65,\n 'Under 18 years old':18}\nlis=[\"Age\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)","c47fe714":"set(X_train[\"Employment\"].values)\nYN={'Employed full-time':4,\n 'Employed part-time':3,\n 'Independent contractor, freelancer, or self-employed':5,\n 'Not employed, and not looking for work':0,\n 'Not employed, but looking for work':2,\n 'Retired':1}\nlis=[\"Employment\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)","28f8255a":"YN={'A little bit interested':2,\n 'Extremely interested':5,\n 'Not at all interested':1,\n 'Somewhat interested':3,\n 'Very interested':4}\nlis=[\"HypotheticalTools1\",\"HypotheticalTools2\",\"HypotheticalTools3\",\"HypotheticalTools4\",\"HypotheticalTools5\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\n","08148b10":"YN={'A few times per week':3,\n 'Less than once per month':1,\n 'Multiple times per day':5,\n 'Never':0,\n 'Once a day':4,\n 'Weekly or a few times per month':2}\nlis=[\"CheckInCode\"]\n\nfor col in lis:\n    X_train[col] = X_train[col].replace(YN)\n    X_test[col]=X_test[col].replace(YN)\n","aa3c3149":"# set(X_train[\"CheckInCode\"].values)","8b1dbcbb":"X_train.head()","ff113f9d":"###\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0-1\ndf_X_train=X_train\ndf_X_test=X_test\n# # \u6570\u5024\u30ab\u30e9\u30e0\u3092\u62bd\u51fa (float or int)\nlist_cols_num = []\nfor i in df_X_train.columns:\n    if df_X_train[i].dtype == 'float64' or df_X_train[i].dtype == 'int64':\n        list_cols_num.append(i)\n        \nprint(list_cols_num)","29828d72":"# \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30ab\u30e9\u30e0\u3092\u62bd\u51fa (object)\ndf_X_train=X_train\ndf_X_test=X_test\n\nlist_cols_cat = []\nfor i in df_X_train.columns:\n    if df_X_train[i].dtype == 'object':\n        list_cols_cat.append(i)\n        \nprint(list_cols_cat)\n\n\n# \u5404\u7d71\u8a08\u91cf\u3092\u4f5c\u6210\n# X_train\nstatics_X_train_cat = pd.DataFrame([df_X_train[list_cols_cat].nunique().values.tolist(),  # \u30e6\u30cb\u30fc\u30af\u6570\n                                df_X_train[list_cols_cat].isnull().sum().values.tolist()], # \u6b20\u640d\u6570\n                              index=['unique', 'trainnull'],\n                              columns=list_cols_cat).T\n# X_test\nstatics_X_test_cat =  pd.DataFrame([df_X_test[list_cols_cat].nunique().values.tolist(),  # \u30e6\u30cb\u30fc\u30af\u6570\n                                df_X_test[list_cols_cat].isnull().sum().values.tolist()],  # \u6b20\u640d\u6570\n                                index=['unique', 'null'],\n                                columns=list_cols_cat).T\n\nstatics_X_train_cat","ff28635e":"statics_X_test_cat","77774355":"#\u30e6\u30cb\u30fc\u30af\u6570\u306e\u5dee\u3092\u30c1\u30a7\u30c3\u30af\n# check=X_train.columns\n\nlist_cols_cat = []\nfor i in df_X_train.columns:\n    if df_X_train[i].dtype == 'object':\n        list_cols_cat.append(i)\n        \n# print(list_cols_cat)\n# for col in list_cols_cat:\n#     a=set(X_train[col].unique())-set(X_test[col].unique())\n#     print(\"train-test\",col,len(a))\n#     a=set(X_train[col].unique())-set(add[col].unique())\n#     print(\"train-add\",col,len(a))\n#     a=set(X_test[col].unique())-set(X_train[col].unique())\n#     print(\"test-train\",col,len(a))\n#     a=set(X_test[col].unique())-set(add[col].unique())\n#     print(\"test-add\",col,len(a))\n\n    \n# for col in categorical:\n#     a=set(X_train[col].unique())-set(X_test[col].unique())\n#     print(\"train\",col,len(a))\n#     a=set(X_test[col].unique())-set(X_train[col].unique())\n#     print(\"test\",col,len(a))","8b80b87a":"# set(X_all[\"Country\"].values)-set(add[\"Country\"])","36fbc66e":"# set(add[\"Country\"].values)-set(X_all[\"Country\"])","72f6d3c8":"set(X_test[\"Country\"].values)-set(X_train[\"Country\"])","fa01aa5b":"numeric=list_cols_num\n# ['loan_amnt', 'installment',\n#        'annual_inc','dti', 'delinq_2yrs',\n#         'inq_last_6mths', 'mths_since_last_delinq',\n#        'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n#        'revol_util', 'total_acc',\n#        'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n#        'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal','loan_condition','grade','sub_grade','emp_length']\n\n\ntext=[\"DevType\",\"CommunicationTools\",\"FrameworkWorkedWith\",\"AdsActions\",\"RaceEthnicity\"]\ncategorical=list(set(list_cols_cat)-set(text))","1deb823d":"X_train[numeric].corr()","94a151f7":"# #\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n# sns.heatmap(X_train[numeric].corr(), vmax=1, vmin=-1, center=0)","c6ae6e25":"# #\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n# sns.heatmap(X_test[numeric].corr(), vmax=1, vmin=-1, center=0)","21143dc0":"# ###\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0-1\n# df_X_train=X_train\n# df_X_test=X_test\n# # \u6570\u5024\u30ab\u30e9\u30e0\u3092\u62bd\u51fa (float or int)\n# list_cols_num = []\n# for i in df_X_train.columns:\n#     if df_X_train[i].dtype == 'float64' or df_X_train[i].dtype == 'int64':\n#         list_cols_num.append(i)\n        \n# print(list_cols_num)\n\n# # \u5404\u7d71\u8a08\u91cf\u3092\u4f5c\u6210\n# # X_train\n# statics_X_train_num = pd.DataFrame([df_X_train[list_cols_num].nunique().values.tolist(),  # \u30e6\u30cb\u30fc\u30af\u6570\n#                                 df_X_train[list_cols_num].isnull().sum().values.tolist(),  # \u6b20\u640d\u6570\n#                               df_X_train[list_cols_num].mean().values.tolist(),  # \u5e73\u5747\u5024\n#                               df_X_train[list_cols_num].std().values.tolist(),  # \u6a19\u6e96\u504f\u5dee\n#                               df_X_train[list_cols_num].median().values.tolist(),  # \u4e2d\u592e\u5024\n#                               df_X_train[list_cols_num].min().values.tolist(),  # \u6700\u5c0f\u5024\n#                               df_X_train[list_cols_num].max().values.tolist()],  # \u6700\u5927\u5024\n#                               index=['unique', 'null', 'mean', 'std', 'median', 'min', 'max'],\n#                               columns=list_cols_num).T\n# # X_test\n# statics_X_test_num =  pd.DataFrame([df_X_test[list_cols_num].nunique().values.tolist(),  # \u30e6\u30cb\u30fc\u30af\u6570\n#                                 df_X_test[list_cols_num].isnull().sum().values.tolist(),  # \u6b20\u640d\u6570\n#                                 df_X_test[list_cols_num].mean().values.tolist(),  # \u5e73\u5747\u5024\n#                                 df_X_test[list_cols_num].std().values.tolist(),  # \u6a19\u6e96\u504f\u5dee\n#                                 df_X_test[list_cols_num].median().values.tolist(),  # \u4e2d\u592e\u5024\n#                                 df_X_test[list_cols_num].min().values.tolist(),  # \u6700\u5c0f\u5024\n#                                 df_X_test[list_cols_num].max().values.tolist()],  # \u6700\u5927\u5024\n#                                 index=['unique', 'null', 'mean', 'std', 'median', 'min', 'max'],\n#                                 columns=list_cols_num).T\n\n# statics_X_train_num","2b275573":"# # \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0-2\n# statics_X_test_num","5e6a1c2e":"# # \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0-3\n\n# plotpos = 1\n# fig = plt.figure(figsize = (30, 30))\n\n# for i in list_cols_num:\n#     plotdata1 = df_X_train[i]\n#     plotdata2 = df_X_test[i]\n\n#     ax = fig.add_subplot(10, 3, plotpos)\n#     ax.hist(plotdata1, bins=50, alpha=0.4)\n#     ax.hist(plotdata2, bins=50, alpha=0.4)\n#     ax.set_xlabel(i)\n    \n#     plotpos = plotpos + 1\n\n# plt.show()","63dbe248":"# ##\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n\n# for col in numeric:\n#     upperbound, lowerbound= np.percentile(X_train[col],[1,99])\n#     X_train[col]=np.clip(X_train[col],upperbound,lowerbound)\n#     X_test[col]=np.clip(X_test[col],upperbound,lowerbound)\n","986f4a58":"# X_train['issue_d_y']=X_train['issue_d'].dt.year\n# X_train['issue_d_m']=X_train['issue_d'].dt.month\n# X_train['issue_d_d']=X_train['issue_d'].dt.day\n# X_train['earliest_cr_line_y']=X_train['earliest_cr_line'].dt.year\n# X_train['earliest_cr_line_m']=X_train['earliest_cr_line'].dt.month\n# X_train['earliest_cr_line_d']=X_train['earliest_cr_line'].dt.day\n# X_train['issue-cr_line']=X_train['issue_d']-X_train['earliest_cr_line']\n\n# X_year=X_train.groupby('issue_d_y').mean()\n# X_year","a45867c2":"# X_test['issue_d_y']=X_test['issue_d'].dt.year\n# X_test['issue_d_m']=X_test['issue_d'].dt.month\n# X_test['issue_d_d']=X_test['issue_d'].dt.day\n# X_test['earliest_cr_line_y']=X_test['earliest_cr_line'].dt.year\n# X_test['earliest_cr_line_m']=X_test['earliest_cr_line'].dt.month\n# X_test['earliest_cr_line_d']=X_test['earliest_cr_line'].dt.day\n# X_test['issue-cr_line']=X_test['issue_d']-X_test['earliest_cr_line']\n# X_test.groupby('issue_d_y').mean()\n# X_test","a706dafe":"# X_train.groupby(['issue_d_y','initial_list_status']).mean()","57e4e069":"# X_test.groupby(['issue_d_y','initial_list_status']).mean()","298862e9":"# X_train.groupby(['issue_d_y','initial_list_status']).count()","2ddd598c":"# X_test.groupby(['issue_d_y','initial_list_status']).count()","6b951196":"# # \u8cb8\u4ed8\u65e5\u304c2014\u5e74\u4ee5\u964d\u306e\u30c7\u30fc\u30bf\u306e\u307f\u4f7f\u7528\n# X_train = X_train[X_train['issue_d_y'] >= 2014]","b732d49c":"# #\u30e6\u30cb\u30fc\u30af\u6570\u306e\u5dee\u3092\u30c1\u30a7\u30c3\u30af\n# # check=X_train.columns\n# for col in text:\n#     a=set(X_train[col].unique())-set(X_test[col].unique())\n#     print(\"train\",col,len(a))\n#     a=set(X_test[col].unique())-set(X_train[col].unique())\n#     print(\"test\",col,len(a))\n\n    \n# for col in categorical:\n#     a=set(X_train[col].unique())-set(X_test[col].unique())\n#     print(\"train\",col,len(a))\n#     a=set(X_test[col].unique())-set(X_train[col].unique())\n#     print(\"test\",col,len(a))","4ceb277d":"###\u6570\u5b57\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u7528\n\n#for col in numeric:\n#     ax = sns.distplot(X_train[col].fillna(-9999))\n#     plt.show()","0785381d":"#\u6b20\u640d\u5024\u30ab\u30a6\u30f3\u30c8\nX_train['null_count']=X_train.isnull().sum(axis=1)\nX_test['null_count'] = X_test.isnull().sum(axis=1)","f75306b8":"text","8b610121":"X_train_1=X_train.copy()\nX_test_1=X_test.copy()","5a985900":"X_train=X_train_1.copy()\nX_test=X_test_1.copy()","87410dbd":"# # \u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u30fb\u6b20\u640d\u5024\u3092'#'\u3067\u88dc\u5b8c\n# from sklearn.model_selection import KFold\n\n\n\n# dev_train = X_train['DevType']\n# dev_train.fillna('#', inplace=True)\n# dev_test = X_test['DevType']\n# dev_test.fillna('#', inplace=True)\n    \n# ct_train = X_train['CommunicationTools']\n# ct_train.fillna('#', inplace=True)\n# ct_test = X_test['CommunicationTools']\n# ct_test.fillna('#', inplace=True)\n\n# fw_train = X_train['FrameworkWorkedWith']\n# fw_train.fillna('#', inplace=True)\n# fw_test = X_test['FrameworkWorkedWith']\n# fw_test.fillna('#', inplace=True)\n\n# aa_train = X_train['AdsActions']\n# aa_train.fillna('#', inplace=True)\n# aa_test = X_test['AdsActions']\n# aa_test.fillna('#', inplace=True)\n\n# re_train = X_train['RaceEthnicity']\n# re_train.fillna('#', inplace=True)\n# re_test = X_test['RaceEthnicity']\n# re_test.fillna('#', inplace=True)\n\n\n\n\n# # TF-IDF \n# tfidf = TfidfVectorizer(max_features=10000)\n\n# dev_train = tfidf.fit_transform(dev_train)\n# dev_test = tfidf.transform(dev_test)\n\n# ct_train = tfidf.fit_transform(ct_train)\n# ct_test = tfidf.transform(ct_test)\n\n# fw_train = tfidf.fit_transform(fw_train)\n# fw_test= tfidf.transform(fw_test)\n\n# aa_train = tfidf.fit_transform(aa_train)\n# aa_test = tfidf.transform(aa_test)\n\n# re_train = tfidf.fit_transform(re_train)\n# re_test = tfidf.transform(re_test)\n\n\n# # \u5168\u3066\u306eTF-IDF matrix\u3092\u7d50\u5408\n# txt_train = hstack([dev_train, ct_train,fw_train,aa_train,re_train]).tocsr()\n# txt_test = hstack([dev_train, ct_train,fw_train,aa_train,re_train]).tocsr()\n# print(txt_train.shape, txt_test.shape)\n\n# # Stacking\n# oof_train = np.zeros(len(X_train))\n# oof_test = np.zeros(len(X_test))\n\n\n# SEED = 71\n# skf = KFold(n_splits=5, shuffle=False, random_state=None)\n\n\n# for i, (train_ix, test_ix) in enumerate(skf.split(X_train, y_train)):\n#     # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u30fb\u691c\u8a3c\u30c7\u30fc\u30bf\u306b\u5206\u5272\n#     X_tr, y_tr = txt_train[train_ix], y_train.iloc[train_ix]\n#     X_te, y_te = txt_train[test_ix], y_train.iloc[test_ix]\n    \n#     # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u304b\u3089\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\n#     clf = LogisticRegression(\n#         solver='sag'\n#     )\n#     clf.fit(X_tr, y_tr)\n    \n#     # \u691c\u8a3c\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\n#     y_pred = clf.predict_proba(X_te)[:,1]\n#     oof_train[test_ix] = y_pred\n#     score = roc_auc_score(y_te, y_pred)\n#     print('CV Score of Fold_%d is %f' % (i, score))\n    \n#     # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\n#     oof_test += clf.predict_proba(txt_test)[:,1]\n\n# oof_test \/= NFOLDS","28ef591f":"# X_train['tfidf'] = oof_train\n# X_test['tfidf'] = oof_test","cb037c05":"print(X_train.shape)\nprint(X_test.shape)\nX_train","5e6b9867":"print(X_train.isnull().any())","c8f49f38":"# numeric.remove('loan_condition')\n# numeric","2cbf7626":"print(X_train.shape)\nprint(X_test.shape)","95b9ff7d":"numeric","b3bd6161":"# num=[\"loan_amnt\",\"annual_inc\",\"revol_bal\",\"tot_cur_bal\",\"dti\"]\n# num","e9377f6c":"##\u5168\u90e8\u639b\u3051\u3066\u5272\u3063\u3066\u8db3\u3057\u3066\u5f15\u304f (\u521d\u56de\u3060\u3051\u306b\u3057\u3066\u3001\u91cd\u8981\u3058\u3083\u306a\u3044\u3082\u306e\u3092\u6d88\u3057\u3066\u3044\u304f)\n# for i in numeric:\n#     for k in numeric:\n#         print(i,k)\n#         X_train[i+\"+\"+k]=X_train[i]+X_train[k]\n#         X_train[i+\"-\"+k]=X_train[i]-X_train[k]\n#         X_train[i+\"*\"+k]=X_train[i]*X_train[k]\n#         X_train[i+\"\/\"+k]=X_train[i]\/(X_train[k]+1)\n#         X_test[i+\"+\"+k]=X_test[i]+X_test[k]\n#         X_test[i+\"-\"+k]=X_test[i]-X_test[k]\n#         X_test[i+\"*\"+k]=X_test[i]*X_test[k]\n#         X_test[i+\"\/\"+k]=X_test[i]\/(X_test[k]+1)\n# X_train.head()","94cc9e53":"# def encode(df, col):\n#     # \u3053\u306e\u65b9\u6cd5\u3060\u3068\u5834\u5408\u306b\u3088\u3063\u3066\u6700\u5927\u5024\u304c\u5909\u5316\u3059\u308b\u30c7\u30fc\u30bf\u3067\u306f\u6b63\u78ba\u306a\u5024\u306f\u51fa\u306a\u3044\n#     # \u4f8b\uff1a\u6708\u306e\u65e5\u6570\u304c30\u65e5\u308431\u65e5\u306e\u5834\u5408\u304c\u3042\u308b\n#     df[col + '_cos'] = np.cos(2 * np.pi * df[col] \/ df[col].max())\n#     df[col + '_sin'] = np.sin(2 * np.pi * df[col] \/ df[col].max())\n#     return df\n\n# # df = encode(df, 'dow')\n# # df = encode(df, 'hour')\n# df = encode(X_train, 'issue_d_d')\n# df = encode(X_test, 'issue_d_d')\n# df = encode(X_train, 'issue_d_m')\n# df = encode(X_test, 'issue_d_m')\n# df = encode(X_train, 'earliest_cr_line_m')\n# df = encode(X_test, 'earliest_cr_line_m')\n# df = encode(X_train, 'earliest_cr_line_d')\n# df = encode(X_test, 'earliest_cr_line_d')","3d9918dd":"# X_train[\"issue-cr_line\"]=X_train[\"issue-cr_line\"].dt.days\n# X_test[\"issue-cr_line\"]=X_test[\"issue-cr_line\"].dt.days\n# X_train.head()","b5ffe909":"print(X_train.shape)\nprint(X_test.shape)\nprint(set(X_train.columns)-set(X_test.columns))","5be6dddd":"# #\u7d71\u8a08\u91cf\u3092\u4f5c\u6210\u3059\u308b\u3082\u306e\u3092\u6307\u5b9a\u3002\n# lis=[\"loan_amnt\",\"emp_length\",\"annual_inc\",\"dti\",\"revol_bal\",\"revol_util\",\"tot_coll_amt\"]\n# df=pd.DataFrame()\n# gb=[\"addr_state\"]\n\n# for col in lis:\n#     print(col)\n#     tmp=X_train.groupby(gb).mean()\n#     df[col+\"_mean_add\"]=tmp[col]\n#     tmp=X_train.groupby(gb).max()\n#     df[col+\"_max_add\"]=tmp[col]\n#     tmp=X_train.groupby(gb).min()\n#     df[col+\"_min_add\"]=tmp[col]\n#     tmp=X_train.groupby(gb).std()\n#     df[col+\"_std_add\"]=tmp[col]\n#     tmp=X_train.groupby(gb).median()\n#     df[col+\"_median_add\"]=tmp[col]    \n# X_train=X_train.merge(df,how=\"left\",on=gb)\n# X_test=X_test.merge(df,how=\"left\",on=gb)\n","fdcc5180":"print(X_train.shape)\nprint(X_test.shape)","611bb1fa":"# ##\u30e9\u30f3\u30af\u4ed8\u3051\u3092\u3059\u308b\n# X_all=pd.concat([X_train,X_test],axis=0)\n# df=pd.DataFrame()\n\n# for col in lis:\n#     print(col)\n#     tmp=X_all.groupby(gb)[col].rank()\n#     df[col+\"_rk_add\"]=tmp    \n# X_all=pd.concat([X_all,df],axis=1)\n# X_train= X_all.iloc[:X_train.shape[0],:]\n# X_test=X_all.iloc[X_train.shape[0]:,:]\n\n\n\n\n","30022412":"#Save\nX_train_n=X_train.copy()\nX_test_n=X_test.copy()\ny_train_n=y_train.copy()\nprint(X_train.shape)\nprint(X_test.shape)","d295a2ba":"#Load\nX_train=X_train_n.copy()\nX_test=X_test_n.copy()\ny_train=y_train_n.copy()\nprint(X_train.shape)\nprint(X_test.shape)","d332f1a3":"#Ordinary-encoding\ntmp=[]\nfor col in categorical:\n    print(col)\n    oe = ce.OrdinalEncoder(cols=[col])\n    print(oe)\n    tmp=oe.fit_transform(X_train)\n    X_train[col+\"_oe\"]=tmp[col]\n    tmp = oe.transform(X_test)\n    X_test[col+\"_oe\"]=tmp[col]\n\nX_train.head()\n\n##LightGBM\u3078\u6295\u5165###\u904e\u5b66\u7fd2\u306b\u8d85\u6ce8\u610f\uff01\nfor col in categorical:\n    X_train[col+\"_oe\"] = X_train[col+\"_oe\"].astype('category')\n    X_test[col+\"_oe\"] = X_test[col+\"_oe\"].astype('category')\n","b1013639":"for col in categorical:\n    X_train[col] = X_train[col].str.lower()\n    X_test[col] = X_test[col].str.lower()\n","2fcb0fd1":"#count-encoding\nfor col in categorical:\n    X_count=pd.concat([X_train, X_test])\n    count_mean=X_count.groupby(col)[\"Respondent\"].count()\n    print(col)\n    X_train[col+\"_count\"]= X_train[col].map(count_mean)\n    X_test[col+\"_count\"]=X_test[col].map(count_mean)\nX_train.head()\n\nfor col in text:\n    print(col)\n    X_count=pd.concat([X_train, X_test])\n    count_mean=X_count.groupby(col)[\"Respondent\"].count()\n    X_train[col+\"_count\"]= X_train[col].map(count_mean)\n    X_test[col+\"_count\"]=X_test[col].map(count_mean)\nX_train.head()","761fbe86":"# #\u30b0\u30ec\u30fc\u30c9\u3092\u6a19\u7684\u306b\u3057\u305f\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n\n# for cols in categorical:\n#     print(cols)\n#     X_target=pd.concat([X_train, X_test])\n#     grouped_cat = X_target.groupby(cols)[cols].count().reset_index(name='cat_counts')\n#     grouped_grade = X_target.groupby(cols)[\"SalaryType_oe\"].sum().reset_index(name='grade_counts')\n#     X_train = X_train.merge(grouped_cat, how = \"left\", on = cols)\n#     X_train = X_train.merge(grouped_grade, how = \"left\", on = cols)\n#     X_test = X_test.merge(grouped_cat, how = \"left\", on = cols)\n#     X_test  = X_test.merge(grouped_grade, how = \"left\", on = cols)\n#     X_train[cols+\"_tag\"] = X_train[\"grade_counts\"]\/X_train[\"cat_counts\"]\n#     X_test[cols+\"_tag\"] = X_test[\"grade_counts\"]\/X_test[\"cat_counts\"]\n#     X_train=X_train.drop(columns=[\"grade_counts\",\"cat_counts\"])\n#     X_test=X_test.drop(columns=[\"grade_counts\",\"cat_counts\"])\n# X_train.head()\n\n# for cols in text:\n#     print(cols)\n#     X_target=pd.concat([X_train, X_test])\n#     grouped_cat = X_target.groupby(cols)[cols].count().reset_index(name='cat_counts')\n#     grouped_grade = X_target.groupby(cols)[\"grade\"].sum().reset_index(name='grade_counts')\n#     X_train = X_train.merge(grouped_cat, how = \"left\", on = cols)\n#     X_train = X_train.merge(grouped_grade, how = \"left\", on = cols)\n#     X_test = X_test.merge(grouped_cat, how = \"left\", on = cols)\n#     X_test  = X_test.merge(grouped_grade, how = \"left\", on = cols)\n#     X_train[cols+\"_tag\"] = X_train[\"grade_counts\"]\/X_train[\"cat_counts\"]\n#     X_test[cols+\"_tag\"] = X_test[\"grade_counts\"]\/X_test[\"cat_counts\"]\n#     X_train=X_train.drop(columns=[\"grade_counts\",\"cat_counts\"])\n#     X_test=X_test.drop(columns=[\"grade_counts\",\"cat_counts\"])\n# X_train.head()","913ac323":"# #One-hot_encoding\n# ohe=[\"home_ownership\",\"addr_state\"]\n\n# X_all=pd.concat([X_train,X_test],axis=0)\n\n# X_all= pd.get_dummies(X_all,\n#                        dummy_na = True,\n#                        columns = ohe)\n\n# ohes=set(X_all)-set(X_train.columns)\n\n# X_train= X_all.iloc[:X_train.shape[0],:]\n# X_test=X_all.iloc[X_train.shape[0]:,:]\n\n# ohes","cb8d895a":"# home=['home_ownership_ANY',\n#  'home_ownership_MORTGAGE',\n#  'home_ownership_OWN',\n#  'home_ownership_RENT',\n#  'home_ownership_nan']\n# addr=list(set(ohes)-set(home))\n\n# #Ohe-hot\u4ea4\u4e92\u7279\u5fb4\u91cf\n# for i in home:\n#     for k in addr:\n#         print(i,k)\n#         X_train[i+\"_\"+k]=X_train[i]*X_train[k]\n#         X_test[i+\"_\"+k]=X_test[i]*X_test[k]\n# X_train.head()\n# cat_ohe=list(set(categorical)-set([\"addr_state\",\"home_ownership\"]))","3f45a73b":"X_train","b9fea2dc":"X_train=X_train.drop(columns=categorical)\nX_test=X_test.drop(columns=categorical)\n\nX_train=X_train.drop(columns=text)\nX_test=X_test.drop(columns=text)","c73706f9":"X_train.head()","233e751f":"print(X_train.shape)\nprint(X_test.shape)","99db2d6f":"X_test_id=X_test[[\"Respondent\"]]","a1cadca3":"#Save\nX_train_c=X_train.copy()\nX_test_c=X_test.copy()","91b4ae9d":"#Load\nX_train=X_train_c.copy()\nX_test=X_test_c.copy()","7de63a76":"###\u7279\u5fb4\u91cf\u9078\u5b9a\ndrop_columns=[]\n\nX_train=X_train.drop(columns=drop_columns)\nX_test=X_test.drop(columns=drop_columns)\n","23313273":"print(set(X_train.columns)-set(X_test.columns))\nprint(set(X_test.columns)-set(X_train.columns))","81606985":"print(X_train.shape)\nX_train.head()","5af57f6a":"X_train_v=X_train.copy()\nX_test_v=X_test.copy()\ny_train_v=y_train.copy()","a3553063":"X_train=X_train_v.copy()\nX_test=X_test_v.copy()\ny_train=y_train_v.copy()\n\nprint(X_train.shape)\nprint(X_test.shape)","c738feea":"# ##initial_list_status\u3054\u3068\u306b\u5206\u3051\u305f\u5834\u5408\u306eSplit\n# X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(X_train,\n#                                                    y_train,\n#                                                   test_size=0.20,\n#                                                   random_state=1)\n\n\n# X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(X_train,\n#                                                    y_train,\n#                                                   test_size=0.20,\n#                                                   random_state=42)\n\n\n# X_train_3, X_val_3, y_train_3, y_val_3 = train_test_split(X_train,\n#                                                    y_train,\n#                                                   test_size=0.20,\n#                                                   random_state=71)","2709fc37":"# clf=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n#               importance_type='split', learning_rate=0.1, max_depth=-1,\n#               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n#               n_estimators=9999, n_jobs=-1, num_leaves=31, objective=None,\n#               random_state=1, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n#               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n# clf2=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n#               importance_type='split', learning_rate=0.1, max_depth=-1,\n#               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n#               n_estimators=9999, n_jobs=-1, num_leaves=31, objective=None,\n#               random_state=71, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n#               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n# clf3=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n#               importance_type='split', learning_rate=0.1, max_depth=-1,\n#               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n#               n_estimators=9999, n_jobs=-1, num_leaves=31, objective=None,\n#               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n#               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n\n# clf.fit(X_train_1, y_train_1, early_stopping_rounds=20, eval_set=[(X_val_1,y_val_1)],verbose=100)\n# clf2.fit(X_train_2, y_train_2, early_stopping_rounds=20, eval_set=[(X_val_2,y_val_2)],verbose=100)\n# clf3.fit(X_train_3, y_train_3, early_stopping_rounds=20, eval_set=[(X_val_3,y_val_3)],verbose=100)\n\n","75e957f3":"\n\n# from sklearn.model_selection import StratifiedKFold\n\n# scores = []\n\n\n# cv_results = cross_val_score(clf,\n#                              X_train,\n#                              y_train,\n#                              cv=5,\n#                              scoring='r2')\n# print(cv_results)\n# print(cv_results.mean(),'+-', cv_results.std())\n# cv_results = cross_val_score(clf2,\n#                              X_train,\n#                              y_train,\n#                              cv=5,\n#                              scoring='r2')\n# print(cv_results)\n# print(cv_results.mean(),'+-', cv_results.std())\n# cv_results = cross_val_score(clf3,\n#                              X_train,\n#                              y_train,\n#                              cv=5,\n#                              scoring='r2')\n# print(cv_results)\n# print(cv_results.mean(),'+-', cv_results.std())\n\n# for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n#     X_train_, y_train_ = X_train.values[train_ix], y_train.values[train_ix]\n#     X_val, y_val = X_train.values[test_ix], y_train.values[test_ix]\n    \n#     clf.fit(X_train_, y_train_, early_stopping_rounds=20, eval_metric='auc', eval_set=[(X_val,y_val)],verbose=100)\n#     y_pred = clf.predict(X_val)\n#     score = roc_auc_score(y_val, y_pred)\n#     scores.append(score)\n    \n#     print('CV Score of Fold_%d is %f' % (i, score))\n    \n\n    \n\n\n","63b6d87c":"#\u4e88\u6e2c\u7528\nclf=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n              importance_type='split', learning_rate=0.1, max_depth=-1,\n              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n              random_state=1, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\nclf2=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n              importance_type='gain', learning_rate=0.1, max_depth=-1,\n              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n              n_estimators=100, n_jobs=-1, num_leaves=26, objective=None,\n              random_state=71, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n              subsample=1.0, subsample_for_bin=300000, subsample_freq=0)\nclf3=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n              importance_type='split', learning_rate=0.1, max_depth=-1,\n              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n              n_estimators=100, n_jobs=-1, num_leaves=36, objective=None,\n              random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n              subsample=1.0, subsample_for_bin=400000, subsample_freq=0)\n\nclf.fit(X_train, y_train)\nprint(1)\nclf2.fit(X_train, y_train)\nprint(2)\nclf3.fit(X_train, y_train)\nprint(3)\n","44e8630e":"###\u3000\u4e88\u6e2c\u51fa\u529b\u30b3\u30fc\u30ca\u30fc(Data split\u7528) only for light GBM\n# y_pred=clf.predict(X_test)\ny_pred=0.4*clf.predict(X_test)+0.3*clf2.predict(X_test)+0.3*clf3.predict(X_test)\nsubmission=pd.DataFrame()\nsubmission[\"Respondent\"]=X_test[\"Respondent\"]\nsubmission[\"ConvertedSalary\"]=y_pred\nsubmission.to_csv('submission.csv',index=False)\n","890f347a":"# ###\u3000\u7279\u5fb4\u91cf\u306e\u91cd\u8981\u5ea6\u78ba\u8a8d\u30b3\u30fc\u30ca\u30fc\nimport lightgbm as lgb\n# y_pred = clf.predict_proba(X_train)[:,1]\n# score=roc_auc_score(y_train,y_pred)\n# print(score)\n\n\nfig, ax = plt.subplots(figsize=(10,15))\nlgb.plot_importance(clf,max_num_features=50, ax=ax, importance_type='gain')\n","49b2d9a8":"# importance\u3092\u8868\u793a\u3059\u308b\nimportance = pd.DataFrame(clf.feature_importances_, index=X_train.columns, columns=['importance'])\nimportance=importance.sort_values('importance')\nimportance.index","0e6ce774":"importance","4f3f8d5d":"# all_col=list(set(X_train.columns)-set(ohes))","be1af674":"##\u30e9\u30f3\u30af\u5316\n# from sklearn.preprocessing import quantile_transform\n\n# X_all=pd.concat([X_train,X_test],axis=0)\n# X_all[all_col]=quantile_transform(X_all[all_col],n_quantiles=100,random_state=0,output_distribution='normal')\n# X_train= X_all.iloc[:X_train.shape[0],:]\n# X_test=X_all.iloc[X_train.shape[0]:,:]\n\n","6c78cd28":"\u30ab\u30c6\u30b4\u30ea\u306e\u51e6\u7406","e6e199e7":"\u30c6\u30ad\u30b9\u30c8","d99d0fe1":"numeric\u306e\u7279\u5fb4\u91cf\u4f5c\u6210","42965ec9":"\u524d\u51e6\u7406","ee6ec253":"\u6642\u9593","afa358bf":"\u4e88\u5099","fa3f4ae3":"\u30c7\u30fc\u30bf\u53d6\u5f97","3936a532":"EDA","b19d73da":"Light GBM"}}