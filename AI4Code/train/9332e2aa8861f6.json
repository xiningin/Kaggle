{"cell_type":{"e450c9fc":"code","bf6f714c":"code","b16f6bae":"code","225039bf":"code","af0f37a3":"code","d793012b":"code","3076acb1":"code","3d80fcfd":"code","52a54866":"code","34cad58b":"code","adf362e0":"markdown","b3d6b92e":"markdown"},"source":{"e450c9fc":"pip install keras","bf6f714c":"pip install --upgrade tensorflow","b16f6bae":"pip install --upgrade tensorflow-gpu","225039bf":"# Plot ad hoc mnist instances\nfrom keras.datasets import mnist\nimport matplotlib.pyplot as plt\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n# plot 4 images as gray scale\nplt.subplot(221)\nplt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\nplt.subplot(222)\nplt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\nplt.subplot(223)\nplt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\nplt.subplot(224)\nplt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n# show the plot\nplt.show()","af0f37a3":"import numpy\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import np_utils","d793012b":"# load data\nN=[2000, 3000, 4000,  6000, 8000 ]\naccuracy_N=[]\nep=[10,50,100]\naccuracy_ep=[]\nfor j in range(len(ep)):\n    print('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445=',ep[j])\n    for i in range(len(N)):\n        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n        print('\u0420\u0430\u0437\u043c\u0435\u0440 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438',N[i])\n        X_train=X_train[:N[i]]\n        y_train=y_train[:N[i]]\n        X_test=X_test[:N[i]]\n        y_test=y_test[:N[i]]\n        # flatten 28*28 images to a 784 vector for each image\n        num_pixels = X_train.shape[1] * X_train.shape[2]\n        X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n        X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n        # normalize inputs from 0-255 to 0-1\n        X_train = X_train \/ 255\n        X_test = X_test \/ 255\n        # one hot encode outputs\n        y_train = np_utils.to_categorical(y_train)\n        y_test = np_utils.to_categorical(y_test)\n        num_classes = y_test.shape[1]\n        # define baseline model\n        def baseline_model():\n            # create model\n            model = Sequential()\n            model.add(Dense(num_pixels,  activation='sigmoid'))\n            model.add(Dense(num_classes, activation='softmax'))\n            # Compile model\n            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n            return model\n        # build the model\n        model = baseline_model()\n        # Fit the model\n        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=ep[j], batch_size=1000, verbose=0)\n        # Final evaluation of the model\n        scores = model.evaluate(X_test, y_test, verbose=0)\n        print(\"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\", (scores[1]*100))\n        print(\"=============================================================\")\n        accuracy_N.append(scores[1])","3076acb1":"# load data\nN=[2000, 3000, 4000,  6000, 8000 ]\naccuracy_N1=[]\nep=[10,50,100]\naccuracy_ep2=[]\nfor j in range(len(ep)):\n    print('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445=',ep[j])\n    for i in range(len(N)):\n        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n        print('\u0420\u0430\u0437\u043c\u0435\u0440 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438',N[i])\n        X_train=X_train[:N[i]]\n        y_train=y_train[:N[i]]\n        X_test=X_test[:N[i]]\n        y_test=y_test[:N[i]]\n        # flatten 28*28 images to a 784 vector for each image\n        num_pixels = X_train.shape[1] * X_train.shape[2]\n        X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n        X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n        # normalize inputs from 0-255 to 0-1\n        X_train = X_train \/ 255\n        X_test = X_test \/ 255\n        # one hot encode outputs\n        y_train = np_utils.to_categorical(y_train)\n        y_test = np_utils.to_categorical(y_test)\n        num_classes = y_test.shape[1]\n        # define baseline model\n        def baseline_model():\n            # create model\n            model = Sequential()\n            model.add(Dense(num_pixels,  activation='sigmoid'))\n            model.add(Dense(num_pixels,  activation='sigmoid'))\n            model.add(Dense(num_classes, activation='softmax'))\n            # Compile model\n            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n            return model\n        # build the model\n        model = baseline_model()\n        # Fit the model\n        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=ep[j], batch_size=1000, verbose=0)\n        # Final evaluation of the model\n        scores = model.evaluate(X_test, y_test, verbose=0)\n        print(\"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\", (scores[1]*100))\n        print(\"=============================================================\")\n        accuracy_N1.append(scores[1])\n","3d80fcfd":"# load data\nN=[2000, 3000, 4000,  6000, 8000 ]\naccuracy_N2=[]\nep=[10,50,100]\naccuracy_ep3=[]\nfor j in range(len(ep)):\n    print('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445=',ep[j])\n    for i in range(len(N)):\n        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n        print('\u0420\u0430\u0437\u043c\u0435\u0440 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438',N[i])\n        X_train=X_train[:N[i]]\n        y_train=y_train[:N[i]]\n        X_test=X_test[:N[i]]\n        y_test=y_test[:N[i]]\n        # flatten 28*28 images to a 784 vector for each image\n        num_pixels = X_train.shape[1] * X_train.shape[2]\n        X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n        X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n        # normalize inputs from 0-255 to 0-1\n        X_train = X_train \/ 255\n        X_test = X_test \/ 255\n        # one hot encode outputs\n        y_train = np_utils.to_categorical(y_train)\n        y_test = np_utils.to_categorical(y_test)\n        num_classes = y_test.shape[1]\n        # define baseline model\n        def baseline_model():\n            # create model\n            model = Sequential()\n            model.add(Dense(num_pixels,  activation='sigmoid'))\n            model.add(Dense(num_pixels,  activation='sigmoid'))\n            model.add(Dense(num_pixels,  activation='sigmoid'))\n            model.add(Dense(num_classes, activation='softmax'))\n            # Compile model\n            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n            return model\n        # build the model\n        model = baseline_model()\n        # Fit the model\n        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=ep[j], batch_size=1000, verbose=0)\n        # Final evaluation of the model\n        scores = model.evaluate(X_test, y_test, verbose=0)\n        print(\"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\", (scores[1]*100))\n        print(\"=============================================================\")\n        accuracy_N2.append(scores[1])","52a54866":"print('\u0412\u043b\u0438\u044f\u043d\u0438\u0435 \u043e\u0431\u044c\u0435\u043c\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438 10 \u044d\u043f\u043e\u0445\u0430\u0445')\nfor i in range (len(N)):\n    print('\u0420\u0430\u0437\u043c\u0435\u0440 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438',N[i],\"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0438 1-\u043c \u0441\u043b\u043e\u0435\", (accuracy_N[i]*100),\"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0438 2-\u0445 \u0441\u043b\u043e\u0435\u0432\", (accuracy_N1[i]*100),\"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0438 3-\u0445 \u0441\u043b\u043e\u0435\u0432\", (accuracy_N2[i]*100))","34cad58b":"for j in range(len(ep)):\n    print('\u0412\u043b\u0438\u044f\u043d\u0438\u0435 \u043e\u0431\u044c\u0435\u043c\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438', ep[j] ,'\u044d\u043f\u043e\u0445\u0430\u0445')\n    for i in range (len(N)):\n        print('\u0420\u0430\u0437\u043c\u0435\u0440 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438',N[i],\"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0438 1-\u043c \u0441\u043b\u043e\u0435\", (accuracy_N[i+j*len(N)]*100),\"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0438 2-\u0445 \u0441\u043b\u043e\u0435\u0432\", (accuracy_N1[i+j*len(N)]*100),\"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0438 3-\u0445 \u0441\u043b\u043e\u0435\u0432\", (accuracy_N2[i+j*len(N)]*100))","adf362e0":"\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a","b3d6b92e":"# \u0414\u0430\u043d\u043d\u044b\u0435"}}