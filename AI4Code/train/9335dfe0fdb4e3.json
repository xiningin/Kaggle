{"cell_type":{"c1ec8fb7":"code","fb355e7c":"code","11ef71ba":"code","13ec2425":"code","bbe5bce6":"code","02506fd8":"code","a69f8e92":"code","fec2dcf0":"code","a769cbec":"code","cf8efa02":"code","8bf80a80":"code","976f7d65":"code","2a158243":"code","bdac31a6":"code","9c9a10ed":"code","d97d2bed":"markdown","3d3a6253":"markdown","2b464ea7":"markdown","6664ea17":"markdown","e35ad7ef":"markdown","e46944af":"markdown"},"source":{"c1ec8fb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xgboost as xgb # the magic sauce (source: internet)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fb355e7c":"train = pd.read_csv(\"..\/input\/train_V2.csv\")\ntrain.head()","11ef71ba":"train2 = pd.concat([train, pd.get_dummies(train['matchType'])], axis=1)\ntrain2.drop(['Id', 'groupId', 'matchId', 'matchType'], axis=1, inplace=True)\ntrain2['dumvar'] = 0 # avoid the dummy variable trap\ntrain2.head()","13ec2425":"train2['winPlacePerc'].isna().sum()","bbe5bce6":"train2 = train2.dropna()\ntrain2['winPlacePerc'].isna().sum()","02506fd8":"test = pd.read_csv('..\/input\/test_V2.csv')\ntest2 = pd.concat([test, pd.get_dummies(test['matchType'])], axis=1)\ntest2.drop(['Id', 'groupId', 'matchId', 'matchType'], axis=1, inplace=True)\ntest2['dumvar'] = 0\ntest2.head()","a69f8e92":"X, y = train2.drop(['winPlacePerc'], axis=1),train2['winPlacePerc']","fec2dcf0":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)","a769cbec":"# create the regressor\nregressor = xgb.XGBRegressor(objective = 'reg:linear', colsample_bytree = 0.2, learning_rate = 0.25,\n                            max_depth = 7, alpha = 0.4, n_estimators = 32)\nregressor","cf8efa02":"regressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_valid)","8bf80a80":"# check the RMSE\nrmse = np.sqrt(mean_squared_error(y_valid, y_pred))\nprint('RMSE %f' % (rmse))","976f7d65":"train2['winPlacePerc'].describe()","2a158243":"test_pred = regressor.predict(test2)","bdac31a6":"preds = pd.DataFrame(test_pred, columns=['winPlacePerc'])\ntest_final = pd.concat([test['Id'], preds], axis=1)\ntest_final.head()","9c9a10ed":"test_final.to_csv('submission.csv', index = False)","d97d2bed":"    Rinse repeat with the test set.","3d3a6253":"There is a null value. Based on the amount of data available, I will remove it.","2b464ea7":" First, I'll extract the X and y fields form the training set. I'll also split the dataset out further into a validation set","6664ea17":"    I'm terrible at PUBG, and I'm currently learning about XGBoost. A match made in heaven!","e35ad7ef":"I don't care about Id, groupId or matchId, and will drop those fields prior to training the model. I need to encode matchType, though.","e46944af":"        Let's out the data source"}}