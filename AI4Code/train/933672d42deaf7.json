{"cell_type":{"7127b960":"code","d338db9c":"code","6f503651":"code","745a9c5b":"code","866710ec":"code","da0dab83":"code","5ad7fbfc":"code","6d118998":"code","4ebe3150":"code","42394000":"code","9d4bcd52":"code","1142e8d0":"code","3032c77c":"code","312e1649":"code","168e8785":"code","e3319691":"code","4658d5f8":"code","1f9b3570":"code","62562e86":"code","0ed2d3d7":"code","daa2f1bf":"code","508e5655":"code","6b94b52a":"code","c3b7c674":"code","ece54375":"code","049d25fc":"code","ce9364a0":"markdown","939b034b":"markdown","bf46cdd2":"markdown","0baf2fde":"markdown","4c6ff68a":"markdown","5d3effcc":"markdown","5b196adf":"markdown","0fc5015a":"markdown"},"source":{"7127b960":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom tqdm import tqdm_notebook as tqdm\n\n%matplotlib inline","d338db9c":"for dirname, _, _ in os.walk(\"\/kaggle\/input\") : \n    print(dirname)","6f503651":"image_paths = []\nimage_names = []\nimage_dir = \"\/kaggle\/input\/ffhq-face-data-set\/thumbnails128x128\/\"\nfor image_name in tqdm(os.listdir(image_dir)) : \n    image_path = image_dir + image_name\n    image_paths.append(image_path)\n    image_names.append(image_name)    ","745a9c5b":"len(image_names), len(image_paths)","866710ec":"image_dataframe = pd.DataFrame(index = np.arange(len(image_names)), columns = [\"image_name\", \"path\"])\n\ni = 0 \nfor name, path in tqdm(zip(image_names, image_paths)) : \n    image_dataframe.iloc[i][\"image_name\"] = name\n    image_dataframe.iloc[i][\"path\"] = path\n    i = i + 1\n\nprint(\"Dataframe shape = \", image_dataframe.shape)","da0dab83":"image_dataframe.head()","5ad7fbfc":"sample_images = []","6d118998":"def get_images() : \n    sample_images = []\n    random_image_paths = [np.random.choice(image_dataframe[\"path\"]) for i in range(6)]\n\n    plt.figure(figsize = (12, 8))\n    for i in range(6) : \n        plt.subplot(2,3, i+1)\n        image = cv2.imread(random_image_paths[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        sample_images.append(image)\n        plt.imshow(image, cmap = \"gray\")\n        plt.grid(False)\n    plt.tight_layout() # Automatically adjust subplot parameters to give specified padding.\n    return sample_images","4ebe3150":"sample_images = get_images()","42394000":"def haar_cascade_detection(sample_images) : \n    face_cascade = cv2.CascadeClassifier(\"..\/input\/haar-cascades-for-face-detection\/haarcascade_frontalface_default.xml\")\n\n    for image in tqdm(sample_images) : \n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        faces = face_cascade.detectMultiScale(gray, 1.05, 5, 50)\n    \n        for (x_coordinate, y_coordinate, height, width) in faces : \n            cv2.rectangle(image, (x_coordinate, y_coordinate), (x_coordinate + width, y_coordinate + height), (100, 0, 0), 2)","9d4bcd52":"haar_cascade_detection(sample_images)","1142e8d0":"plt.figure(figsize = (12, 8))\nfor i in range(6) : \n    plt.subplot(2,3, i+1)\n    plt.imshow(sample_images[i], cmap = \"gray\")\n    plt.title(\"Image {}\".format(i+1))\n    plt.grid(False)\nplt.tight_layout() # Automatically adjust subplot parameters to give specified padding.","3032c77c":"def load_my_image() : \n    my_image_01 = cv2.imread(\"..\/input\/myimages\/img_01.jpg\")\n    my_image_01 = cv2.cvtColor(my_image_01, cv2.COLOR_BGR2RGB)\n\n    my_image_02 = cv2.imread(\"..\/input\/myimages\/img_2.jpg\")\n    my_image_02 = cv2.cvtColor(my_image_02, cv2.COLOR_BGR2RGB)\n\n    plt.figure(figsize = (12, 8))\n    plt.subplot(1,2,1) \n    plt.title(\"The One During A Conference\", fontsize = 13)\n    plt.imshow(my_image_01, cmap = \"gray\")\n    plt.grid(False)\n\n    plt.subplot(1,2,2) \n    plt.title(\"The One With Home Buddies!\", fontsize = 13)\n    plt.imshow(my_image_02, cmap = \"gray\")\n    plt.grid(False)\n\n    plt.tight_layout()\n    return my_image_01, my_image_02","312e1649":"my_image_01, my_image_02 = load_my_image()","168e8785":"haar_cascade_detection([my_image_01, my_image_02])","e3319691":"plt.figure(figsize = (12, 8))\nplt.subplot(1,2,1) \nplt.title(\"The One During A Conference\", fontsize = 13)\nplt.imshow(my_image_01, cmap = \"gray\")\nplt.grid(False)\n\nplt.subplot(1,2,2) \nplt.title(\"The One With Home Buddies!\", fontsize = 13)\nplt.imshow(my_image_02, cmap = \"gray\")\nplt.grid(False)\n\nplt.tight_layout()","4658d5f8":"! pip install mtcnn","1f9b3570":"sample_images = get_images()","62562e86":"from mtcnn.mtcnn import MTCNN","0ed2d3d7":"detector = MTCNN()","daa2f1bf":"def mtcnn_detector(sample_images) : \n    for image in tqdm(sample_images) : \n        face_location = detector.detect_faces(image)\n        for face in zip(face_location) : \n            x_coordinate, y_coordinate, width, height = face[0]['box']\n            #face_landmarks = face[0]['keypoints']\n            cv2.rectangle(image, (x_coordinate, y_coordinate), (x_coordinate + width, y_coordinate + height), (0,0,100), 2)","508e5655":"mtcnn_detector(sample_images)","6b94b52a":"plt.figure(figsize = (12, 8))\nfor i in range(6) : \n    plt.subplot(2,3, i+1)\n    plt.imshow(sample_images[i], cmap = \"gray\")\n    plt.title(\"Image {}\".format(i+1))\n    plt.grid(False)\nplt.tight_layout() # Automatically adjust subplot parameters to give specified padding.","c3b7c674":"my_image_01, my_image_02 = load_my_image()","ece54375":"mtcnn_detector([my_image_01, my_image_02])","049d25fc":"plt.figure(figsize = (12, 8))\nplt.subplot(1,2,1) \nplt.title(\"The One During A Conference\", fontsize = 13)\nplt.imshow(my_image_01, cmap = \"gray\")\nplt.grid(False)\n\nplt.subplot(1,2,2) \nplt.title(\"The One With Home Buddies!\", fontsize = 13)\nplt.imshow(my_image_02, cmap = \"gray\")\nplt.grid(False)\n\nplt.tight_layout()","ce9364a0":"## Again, testing on my own image ~ Private Data","939b034b":"# MTCNN \n\nThe network uses a cascade structure with three networks; **first the image is rescaled to a range of different sizes (called an image pyramid)**, **then the first model (Proposal Network or P-Net) proposes candidate facial regions, the second model (Refine Network or R-Net) filters the bounding boxes, and the third model (Output Network or O-Net) proposes facial landmarks**.\n\nThe model is called a **multi-task network** because each of the three models in the cascade (P-Net, R-Net and O-Net) are trained on three tasks, e.g. make three types of predictions; they are: face classification, bounding box regression, and facial landmark localization.\n\n![image.png](attachment:image.png) ","bf46cdd2":"# FACE RECOGNITION","0baf2fde":"## Non-Maximum Suppression : \n\n![image.png](attachment:image.png) \n\nIn a nutshell, in NMS; all bounding having higher IOU than threshold are eliminated! In this way we avoid a clutter of bounding boxes.","4c6ff68a":"Thank You!~","5d3effcc":"## Trying it on my image! ~ Private data","5b196adf":"## Parameters : \n\n* `scale factor` : Our model has a fixed size defined during training, which is visible in the `xml`. This means that `this size of face is detected in the image if present`. However, by rescaling the input image, we can resize a larger face to a smaller one, making it detectable by the algorithm.\n* `MinNeighbours` : This parameter will affect the quality of the detected faces. Higher value results in less detections but with higher quality. 3~6 is a good value for it.\n* `minSize` : This parameter determine how small size you want to detect. You decide it! Usually, [30, 30] is a good start for face detection.\n\n~ Taken from StackOverFlow : \n\n*https:\/\/stackoverflow.com\/questions\/20801015\/recommended-values-for-opencv-detectmultiscale-parameters*","0fc5015a":"# Using Haar Cascades"}}