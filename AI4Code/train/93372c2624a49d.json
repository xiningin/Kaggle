{"cell_type":{"dae0c115":"code","48b0fb5a":"code","d418cd66":"code","4fd6f83d":"code","73572f29":"code","5ad3f471":"code","f497648e":"code","8bd084a9":"code","255d8219":"code","4c8923ee":"code","8b109042":"code","83c45cc9":"code","3c6d1182":"code","e91533df":"code","d305e59b":"code","e9de9924":"code","6bb7912d":"markdown","61617499":"markdown"},"source":{"dae0c115":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\nfrom keras import regularizers\n\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Bidirectional\nfrom keras.preprocessing import sequence\nfrom keras.models import model_from_json\nfrom keras.models import load_model\n","48b0fb5a":"dataset = pd.read_csv(\"..\/input\/twitter-airline-sentiment\/Tweets.csv\")\ndataset_train = dataset.loc[:,[\"text\", \"airline_sentiment\"]]\nsentiment = {'neutral': 0,'negative': -1, \"positive\": 1}\ndataset_train.airline_sentiment = [sentiment[item] for item in dataset_train.airline_sentiment]\ndataset_train.to_csv (\"dataset_train.csv\", index = False, header=True)","d418cd66":"import nltk\nimport re\nfrom nltk.corpus import stopwords\ndescription_list = []\nstop_words = stopwords.words('english')\nfor description in dataset_train.text:\n    #Regular Expression\n    description = re.sub(\"[^a-zA-Z]\", \" \", description)\n    description = description.lower()\n    description = nltk.word_tokenize(description)\n    #Lemmatazation\n    lemma = nltk.WordNetLemmatizer()\n    description = [lemma.lemmatize(word) for word in description]\n    #description = [word for word in description if not word in stopwords.words()]\n    description = \" \".join(description)\n    description_list.append(description)","4fd6f83d":"#X = filtered_dataset.loc[:, \"description\"].values\nX = np.array(description_list)\ny = dataset_train.airline_sentiment.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 0)\n\ny_train = pd.Series(y_train)\ny_test = pd.Series(y_test)\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\ny_train = to_categorical(y_train, num_classes = 3)\ny_test = to_categorical(y_test, num_classes = 3)","73572f29":"vocab_size = 1000\nembedding_dim = 16\nmax_length = 100\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","5ad3f471":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\n\n# Train\nsequences_train = tokenizer.texts_to_sequences(X_train)\npadded_train = pad_sequences(sequences_train, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nprint(padded_train)\n\n# Test\nsequences_test = tokenizer.texts_to_sequences(X_test)\npadded_test = pad_sequences(sequences_test, maxlen=max_length, padding=padding_type, truncating=trunc_type)","f497648e":"training_padded = np.array(padded_train)\ntraining_label = np.array(y_train)\ntest_padded = np.array(padded_test)\ntest_label = np.array(y_test)","8bd084a9":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(15, activation='relu'),\n    tf.keras.layers.Dense(15, activation='relu'),\n    #tf.keras.layers.LSTM(15, dropout=0.5),\n    tf.keras.layers.Dense(3, activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nnum_epochs = 25\nhistory = model.fit(training_padded, training_label, batch_size=32 ,epochs=num_epochs, validation_data=(test_padded, test_label), verbose=2)","255d8219":"#embed_dim = 128\n#lstm_out = 196\n#with tf.device(\"\/device:GPU:0\"):\n#    model = tf.keras.Sequential()\n#    model.add(tf.keras.layers.Embedding(vocab_size, embed_dim, input_length=max_length))\n#    model.add(tf.keras.layers.SpatialDropout1D(0.4))\n#    model.add(tf.keras.layers.LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n#    model.add(Dense(3,activation='sigmoid'))\n#    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n#    num_epochs = 20\n#    history = model.fit(training_padded, training_label, batch_size=32 ,epochs=num_epochs, validation_data=(test_padded, test_label), verbose=2)","4c8923ee":"#with tf.device(\"\/device:GPU:0\"):\n#    model0 = Sequential()\n#    model0.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n#    model0.add(tf.keras.layers.SimpleRNN(15,return_sequences=True))\n#    model0.add(tf.keras.layers.SimpleRNN(15))\n#    model0.add(tf.keras.layers.Dense(3,activation='sigmoid'))\n#    model0.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n#    num_epochs = 20\n#    history = model0.fit(training_padded, training_label, batch_size=32 ,epochs=num_epochs, validation_data=(test_padded, test_label), verbose=2)","8b109042":"#model3 = Sequential()\n#model3.add(tf.keras.layers.Embedding(vocab_size, 40, input_length=max_length))\n#model3.add(tf.keras.layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\n#model3.add(tf.keras.layers.MaxPooling1D(5))\n#model3.add(tf.keras.layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\n#model3.add(tf.keras.layers.GlobalMaxPooling1D())\n#model3.add(tf.keras.layers.Dense(3,activation='softmax'))\n#model3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\n#history = model3.fit(training_padded, training_label, epochs=20,validation_data=(test_padded, test_label))","83c45cc9":"#tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","3c6d1182":"#with tpu_strategy.scope():\n#    mode5 = Sequential()\n#    mode5.add(tf.keras.layers.Embedding(vocab_size, 40, input_length=max_length))\n#    mode5.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n#    mode5.add(tf.keras.layers.Dense(512, activation='relu'))\n#    mode5.add(tf.keras.layers.Dropout(0.50))\n#    mode5.add(tf.keras.layers.Dense(3, activation='softmax'))\n#    mode5.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n#    history = mode5.fit(training_padded, training_label, epochs=20,validation_data=(test_padded, test_label))","e91533df":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","d305e59b":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_sentence(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\nprint(decode_sentence(training_padded[0]))\nprint(X_train[2])\nprint(y_train[2])","e9de9924":"from sklearn.metrics import confusion_matrix, accuracy_score\nsentence = [\"\"]#, \"game of thrones season finale showing this sunday night\"]\nsequences = tokenizer.texts_to_sequences(sentence)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\nraw_prediction = model.predict(padded)\nprint(raw_prediction)\n","6bb7912d":"# Data Import","61617499":"# Naturel Language Proccesing"}}