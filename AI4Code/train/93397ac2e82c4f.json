{"cell_type":{"e0e80f30":"code","4c82ca18":"code","53b76ed9":"code","74a99575":"code","39d88f1a":"code","0b099114":"code","105a67d4":"code","3c687534":"code","1dc64652":"code","cb49ef60":"code","3760704e":"code","5741e12b":"code","247433c8":"code","a10e9dcb":"code","0b274026":"code","ac90b610":"code","7d1c1d76":"code","108c19e7":"code","3692a628":"code","09cb5e1d":"code","c7cb5feb":"code","f6437cc4":"code","b6a3c765":"code","ef66df6b":"code","d741fc64":"code","6e3d0d5f":"code","5e459b80":"code","121c0fb8":"code","47aa4f0a":"code","2693fe14":"code","96dc7957":"code","2fcc3397":"code","52c05f01":"code","f34d797f":"code","b2cdb43e":"code","89318043":"code","40ed9f06":"code","38371785":"code","40536647":"code","6927fdfd":"code","ea0edd8c":"code","ab2710b6":"code","3c31e128":"code","c47ce496":"code","943d71f1":"code","3a821581":"code","935ad2d5":"code","3df7402b":"code","19b7e96c":"code","5ec6d57f":"code","87f5adb4":"code","72122522":"code","09fc0bce":"code","858613e2":"code","41ff5cde":"code","47d1359d":"code","4986b714":"code","d8f1209d":"code","aba0ab18":"markdown","3a83052f":"markdown","3d010882":"markdown","c2e655b6":"markdown","86fdefe9":"markdown","70adb444":"markdown","cc3c689f":"markdown","4fbec9c3":"markdown","46cc6448":"markdown","800a8e73":"markdown","7df5eb55":"markdown","18a4d48c":"markdown","1199502f":"markdown","d4bb6319":"markdown","e5458a54":"markdown","b90a0a8e":"markdown","d5aefc3d":"markdown","005ddca2":"markdown","1f5b293f":"markdown","b24d2cb9":"markdown","6dc428e1":"markdown","2a69e204":"markdown","64a6b799":"markdown","f143b1bc":"markdown","d5524301":"markdown","ef2b502a":"markdown","7696b6cc":"markdown","13aa6d11":"markdown","eeb2e7f2":"markdown","5a74ae3c":"markdown","e9c246df":"markdown","5931ef83":"markdown","89a16b05":"markdown"},"source":{"e0e80f30":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff\nfrom pandas import DataFrame\nimport copy\nimport re\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import scale\nimport statsmodels.api as sm\nimport time\nimport os\nfrom statsmodels.discrete.discrete_model import Logit","4c82ca18":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","53b76ed9":"data=pd.read_excel('\/kaggle\/input\/mobilechurndataxlsx\/mobile-churn-data.xlsx')\ndata=data.drop('user_account_id',axis=1)","74a99575":"def summarizing(df):\n    obs=df.shape[0] # the total number of observatinos \n    types=df.dtypes\n    counts=df.apply(lambda x:x.count())\n    unique=df.apply(lambda x:[x.unique()] ).transpose()\n    distincts=df.apply(lambda x:x.unique().shape[0])\n    null=df.isnull().sum()\n    missing_rate=(round(df.isnull().sum()\/obs*100,2)).astype('str')+'%'\n    skew=df.skew()\n    kurtosis=df.kurtosis()\n    table=pd.concat([types,counts,unique,distincts,null,missing_rate,skew,kurtosis],axis=1)\n    table.columns=['Data_Types','Number_of_NonNull','Unique_Values','Number_of_Unique Values','Missing_Number','Missing_Rate','Skew'\n                ,'Kurtosis']\n    print('The shapes of data:{0}'.format(df.shape))\n    print('--'*30)\n    print('Types Counts:\\n',table.Data_Types.value_counts())\n    print('--'*30)","39d88f1a":"summarizing(data)","0b099114":"display(data.describe().transpose())","105a67d4":"#correlation with uuser_lifetime\ndisplay(data.corr()['user_lifetime'].sort_values(ascending=False))","3c687534":"#merging all input features with same infomration into one variable \ndata['calls_outgoing_inactive_days']=data.calls_outgoing_inactive_days\ndata['sms_outgoing_inactive_days']=data.sms_outgoing_inactive_days\n\n#Remove input features conating same information\nremoving_features=['calls_outgoing_to_onnet_inactive_days', 'calls_outgoing_to_offnet_inactive_days','calls_outgoing_to_abroad_inactive_days',       \n'sms_outgoing_to_abroad_inactive_days','sms_outgoing_inactive_days','sms_outgoing_to_onnet_inactive_days','sms_outgoing_to_offnet_inactive_days']\ndata=data.drop(removing_features,axis=1)\ndata=data.rename(columns={'user_no_outgoing_activity_in_days':'min_outgoing_inactive_days'})\n","1dc64652":"data.head()","cb49ef60":"## create another varialbe to indicate whether they have used the telecomunication service at leaste once in a month. \ndata['user_has_outgoing']=(data.user_has_outgoing_calls+data.user_has_outgoing_sms).map(lambda x: 'yes' if x>0 else 'no')","3760704e":"result1=data.pivot_table(['min_outgoing_inactive_days','user_lifetime','user_spendings'],index='user_has_outgoing',aggfunc='mean')\nresult1","5741e12b":"result1['life_time_ratio']=result1.user_lifetime\/result1.min_outgoing_inactive_days*result1.user_spendings\/30\nresult1","247433c8":"sns.set_style('white')\nfig,ax=plt.subplots(figsize=(12,8))\nax2=ax.twinx()\nsns.distplot(data[data.user_has_outgoing=='yes'].user_lifetime,color='b',ax=ax,label='active')\nsns.distplot(data[data.user_has_outgoing=='no'].user_lifetime,color='coral',ax=ax2,label='inactive')\nax.set_xlabel('user_life_time',fontsize=15)\nax.set_title('Density plots of Lifetime of Two types of Users',fontsize=20)\nax.legend(loc='upper left',fontsize=14)\nax2.legend(loc='upper right',fontsize=14)\n","a10e9dcb":"fig,ax=plt.subplots(figsize=(12,8))\nsns.boxplot(x='user_has_outgoing',y='user_lifetime',data=data,ax=ax)\nax.set_title('Box plots of Lifetime of Two types of Users',fontsize=20)","0b274026":"result2=[['Returning Customers','Potentially Inactive','Total Counts of Both Groups','Proportion of total User After removal']]\na=data[(data.user_has_outgoing=='yes')&(data.user_lifetime>3000)].shape[0]\nb=data[(data.user_has_outgoing=='no')&(data.user_lifetime<14000)].shape[0]\n\nset1=(data.user_has_outgoing=='yes')&(data.user_lifetime<3000)\nset2=(data.user_has_outgoing=='no')&(data.user_lifetime>14000)\nc=data[set1^set2].shape[0]\nd=str(round(c\/data.shape[0]*100,2))+'%'\n\nresult2.append([a,b,c,d])\ntable=ff.create_table(result2)\ntable.layout.update(width=1100)\ntable.show()","ac90b610":"data['user_type']=None\ndata.loc[(data.user_has_outgoing=='yes')&(data.user_lifetime<3000),'user_type']='likely_active_consumers'\ndata.loc[(data.user_has_outgoing=='yes')&(data.user_lifetime>3000),'user_type']='return_consumers'\ndata.loc[(data.user_has_outgoing=='no')&(data.user_lifetime<3000),'user_type']='possilbe_inactive_consumers'\ndata.loc[(data.user_has_outgoing=='no')&(data.user_lifetime>3000),'user_type']='highly_inactive'","7d1c1d76":"fig,ax=plt.subplots(figsize=(14,5))\ndisplay(data.pivot_table('user_has_outgoing',index='user_type',columns='churn',aggfunc='count'))\ndata.pivot_table('user_has_outgoing',index='user_type',columns='churn',aggfunc='count').plot(kind='bar',ax=ax)\nax.set_title('Barplot for The Total Number of Four Type Users by Churn',fontsize=20)\nax.set_ylabel('number of users',fontsize=14)\nax.set_xlabel('user type',fontsize=14)","108c19e7":"#to leave only on variable 'user has_outgoing'\nhas_columns=[]\nfor i in data.columns:\n    if re.search('has',i):\n        has_columns.append(i)\n        \n#drop the rest of columns related to 'has_outgoing' columns\ndata=data.drop(has_columns,axis=1)","3692a628":"data.corr()['reloads_sum'].sort_values(ascending=False)[1:4]","09cb5e1d":"result4=data.pivot_table(['reloads_sum','user_account_balance_last','user_spendings'],index=['user_type','churn'],aggfunc='mean')\n#reanme the label of churn ('no' for 0 and 'yes' for 1)\nresult4.index=result4.index.set_levels(['no','yes'],level=1)\nresult4=result4.reset_index()","c7cb5feb":"result4=data.pivot_table(['reloads_sum','user_account_balance_last','user_spendings'],index=['user_type','churn'],aggfunc='mean')","f6437cc4":"from re import search\nduration=[]\nfor i in data.columns:\n    if search('duration',i):\n        duration.append(i)\nduration\n","b6a3c765":"result5=data.pivot_table(duration,index=['user_type','churn'],aggfunc='mean')\nresult5.index=result5.index.set_levels(['No','Yes'],level=1)\nresult5","ef66df6b":"gprs=[]\nfor i in data.columns:\n    if search('gprs',i):\n        gprs.append(i)\n\n        result5=data.pivot_table('user_use_gprs',index=['user_type','churn'],aggfunc='count')\nresult5.index=result5.index.set_levels(['no','yes'],level=1)\nresult5=result5.reset_index()\nresult5=result5.rename(columns={'user_user_gprs':\"number of user_gprs\"})","d741fc64":"gprs=gprs[1:]\nresult6=data.pivot_table(gprs,index=['user_type','churn'],aggfunc='mean')\nresult6.index=result6.index.set_levels(['no','yes'],level=1)\nresult6=result6.reset_index()\nresult6=result6.rename(columns={\n \"gprs_inactive_days\":\"gprs_inactive_days(avg)\",\n\"gprs_session_count\":\"gprs_session_count(avg)\",\n\"gprs_spendings\":\"gprs_spendings(avg)\",\n\"gprs_usage\":\"gprs_usage(avg)\",\n\"last_100_gprs_usage\":\"last_100_gprs_usage(avg)\"})","6e3d0d5f":"result5=result5.merge(result6,how='outer')\nresult5","5e459b80":"g=sns.catplot(x='user_type',y='gprs_inactive_days(avg)',col='churn',kind='bar',data=result5)\ng.set_xticklabels(rotation=30)\ng.set_xlabels('user_type',fontsize=14)\ng.set_ylabels('gprs_inactive_days(avg)',fontsize=14)","121c0fb8":"g=sns.catplot(x='user_type',y='gprs_spendings(avg)',col='churn',kind='bar',data=result5)\ng.set_xticklabels(rotation=30)\ng.set_xlabels('user_type',fontsize=14)\ng.set_ylabels('gprs_spendings(avg)',fontsize=14)","47aa4f0a":"g=sns.catplot(x='user_type',y='gprs_usage(avg)',col='churn',kind='bar',data=result5)\ng.set_xticklabels(rotation=30)\ng.set_xlabels('user_type',fontsize=14)\ng.set_ylabels('gprs_usage(avg)',fontsize=14)","2693fe14":"g=sns.catplot(x='user_type',y='last_100_gprs_usage(avg)',col='churn',kind='bar',data=result5)\ng.set_xticklabels(rotation=30)\ng.set_xlabels('user_type',fontsize=14)\ng.set_ylabels('last_100_gprs_usage(avg)',fontsize=14)","96dc7957":"result_corr=data.corr()['user_lifetime'].sort_values(ascending=False)\nresult_corr[gprs]","2fcc3397":"data1=data.copy()\ndata_response=data1.churn\ndata1=data1.drop(['churn','user_type'],axis=1)","52c05f01":"conver_var=['user_intake','user_use_gprs','user_does_reload','user_does_reload'] \ndef conversion(data):  \n    for i in conver_var:\n        data.loc[:,i]=data.loc[:,i].map(lambda x: 'no' if x==0 else 'yes')\n    return data\n\ndata1=conversion(data1)\n\n","f34d797f":"def quant_qualt_columns(x):\n    quant=[]\n    qualt=[]\n    for i in x.columns:\n        if x.loc[:,i].dtype=='int64' or x.loc[:,i].dtype=='float64':\n            quant.append(i)\n        else:\n            qualt.append(i)\n    return {0:quant,1:qualt}\n\nquant_columns=quant_qualt_columns(data1)[0]\nqualt_columns=quant_qualt_columns(data1)[1]","b2cdb43e":"a=pd.DataFrame(scale(data1.loc[:,quant_columns]),columns=quant_columns)\nb=data1.loc[:,qualt_columns]\ndata1=pd.concat([a,b],axis=1)\n\n#since year is comprise of only zero, we will drop the feature \ndata1=data1.drop('year',axis=1)","89318043":"for var in qualt_columns:\n    cat_list = pd.get_dummies(data1.loc[:,var],prefix=var)\n    data1=data1.join(cat_list)\n    \n","40ed9f06":"data_final=pd.concat([data1,data_response],axis=1)\ndata_final=data_final.drop(conver_var,axis=1)","38371785":"X=data_final.loc[:,data_final.columns !='churn']\ny=data_final.loc[:,'churn']","40536647":"os=SMOTE(random_state=0)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\ncolumns=X_train.columns\n\nos_data_X,os_data_y=os.fit_sample(X_train,y_train)\nos_data=pd.DataFrame(data=os_data_X,columns=columns)\nos_data['churn']=os_data_y\n\n#Check the numbers of our data\nprint(\"Length of oversampled data is                                            \",len(os_data_X))\nprint(\"Number of churn whose value is 0 in oversampled data                     \",len(os_data_y[os_data.churn==0]))\nprint(\"Number of chunr whose value is 1 in oversampled data in oversampled data \",len(os_data_y[os_data.churn==1]))","6927fdfd":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nrfe = RFE(logreg, 40)\nrfe = rfe.fit(os_data_X, os_data_y.ravel())\nprint(rfe.support_)\nprint(rfe.ranking_)","ea0edd8c":"col_index=np.where(rfe.support_)\ncol=os_data.columns[col_index]\nX=os_data.loc[:,col]\ny=os_data.churn\n\ncol","ab2710b6":"tol = 0.0001\nmaxiter = 1000\nDISP = 0\n\n\nSOLVERS = [\"newton\", \"nm\",\"bfgs\",\"lbfgs\",\"powell\",\"cg\",\"ncg\"] #,\"basinhopping\",]\nfor method in SOLVERS:\n    t = time.time()\n    model = Logit(y,X)\n    result = model.fit(method=method, maxiter=maxiter,\n                       niter=maxiter,\n                       ftol=tol,\n                       tol=tol, gtol=tol, pgtol=tol,  # Hmmm.. needs to be reviewed.\n                       disp=DISP)\n    print(\"sm.Logit\", method, time.time() - t)\n    print(\"--------------------------------------------------------- \")\n\n","3c31e128":"model = Logit(y,X)\nresult = model.fit(method='lbfgs',maxiter=maxiter,\n                       niter=maxiter,\n                       ftol=tol,\n                       tol=tol, gtol=tol, pgtol=tol, \n                       disp=DISP)","c47ce496":"result.summary()","943d71f1":"# second summary\ndrop_col=['calls_outgoing_spendings','calls_outgoing_to_abroad_spendings','sms_outgoing_to_onnet_count','user_intake_no',\n          'user_intake_yes','user_does_reload_yes']\n\nX=X.drop(drop_col,axis=1)\nmodel=Logit(y,X)\nresult = model.fit(method='lbfgs',maxiter=maxiter,\n                       niter=maxiter,\n                       ftol=tol,\n                       tol=tol, gtol=tol, pgtol=tol, \n                       disp=DISP)","3a821581":"result.summary()","935ad2d5":"# Third Summary\ndrop_col=['reloads_sum','calls_outgoing_spendings_max','last_100_calls_outgoing_to_onnet_duration','last_100_sms_outgoing_to_offnet_count']\nX=X.drop(drop_col,axis=1)\nmodel=Logit(y,X)\nresult = model.fit(method='lbfgs',maxiter=maxiter,\n                       niter=maxiter,\n                       ftol=tol,\n                       tol=tol, gtol=tol, pgtol=tol, \n                       disp=DISP)","3df7402b":"result.summary()","19b7e96c":"#Fourth Try\ndrop_col=['calls_outgoing_to_offnet_count','last_100_calls_outgoing_to_abroad_duration']\nX=X.drop(drop_col,axis=1)\nmodel=Logit(y,X)\nresult = model.fit(method='lbfgs',maxiter=maxiter,\n                       niter=maxiter,\n                       ftol=tol,\n                       tol=tol, gtol=tol, pgtol=tol, \n                       disp=DISP)","5ec6d57f":"result.summary()","87f5adb4":"#Fifth Try\n\ndrop_col=['calls_outgoing_to_abroad_duration']\nX=X.drop(drop_col,axis=1)\nmodel=Logit(y,X)\nresult = model.fit(method='lbfgs',maxiter=maxiter,\n                       niter=maxiter,\n                       ftol=tol,\n                       tol=tol, gtol=tol, pgtol=tol, \n                       disp=DISP)","72122522":"#Sixth Try\n\ndrop_col=['calls_outgoing_to_onnet_spendings']\nX=X.drop(drop_col,axis=1)\nmodel=Logit(y,X)\nresult = model.fit(method='lbfgs',maxiter=maxiter,\n                       niter=maxiter,\n                       ftol=tol,\n                       tol=tol, gtol=tol, pgtol=tol, \n                       disp=DISP)","09fc0bce":"result.summary()","858613e2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","41ff5cde":"y_pred=logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","47d1359d":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix=confusion_matrix(y_test,y_pred)\nprint(confusion_matrix)","4986b714":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","d8f1209d":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc=roc_auc_score(y_test,logreg.predict(X_test))\nfpr,tpr,threshold=roc_curve(y_test,logreg.predict(X_test))\nsns.set_style('whitegrid')\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","aba0ab18":"###  3.1.5 Performance Test<a class='anchor' id='test'><\/a>\n\nEstimation of beta coefficients is required to construct a model. Since the MLE of logistic regression does not have a closed-form solution, our attention tursn to optimization. We will test 7 methods by which we can check if our optimization is successfully terminated and record time it takes to complete the task. \n","3a83052f":"## 1.3 Summarizing Data (Statiscal Description) <a class='anchor' id=description><\/a>\n\nBefore taking a rigorous examination of given input features, prepare a simple table containing descriptive statistical information.","3d010882":"### 3.1.3 Imbalanced Data-SMOTE <a class='anchor' id=SMOTE><\/a>\n\nMore than often not, our training data are imbalanced, resulting in unintentional problems in machine learning classification where there is a disproportionate ratio of observations in each class. We bring some strategies to solve this imbalanced class issue. \n\nYou may ask what is the big issue with unbalanced data. The biggest bottleneck we may face in the use of the data is our machine learning algorithm favors the one with greater counts. Without a proper procedure to deal with it, we can not expect to have a fair and accurate result.  \n\nIn this section, we will use the SMOTE( Synthetic Minority Oversampling Technique) to get our data balanced. I will not go further steps to explain the mechanism behind this algorithm but among two options I will choose to oversample our data. \n","c2e655b6":"## 1.5 Feature Manipulation <a class='anchor' id=manipulation><\/a>","86fdefe9":"The groups who are  inactive indeed have 4 times longer life times than those who \nuse the service at least once in the same month.  However, it turns out that the ratio is much higher for those active groups. \n\n---\n###  _Lesson 1 \" The interval between previous and the next outgoing activity is much shorter for user who has made either call or sent  SNS in a given moth, therby contributing to the higher spending to use the services.\"_\n---","70adb444":"###  2.1.2 Spending and Duration of Outgoing Activity<a class='anchor' id=spending_duration><\/a>\n\n","cc3c689f":"![image.png](attachment:image.png)\n\nAs a correlation result implies,   three input features are moving in the same direction; higher spending and greater balance from the previous month and higher reload sum on the specified month and vice versa. Our focus is on returning customers in a given month\n\nIf the customers stayed away for a long time leaving higher account balance ,they are more likely to come back to resuse the service again. Also, thier decision to come back and continue using service is undoubtedly valuable to the telephone company in that thier reload sum and spending are the second greatest, helping to boost a profit for it.\n\n---\n###  _Lesson 3 \"Come out with mega promotions to attract and retain the returning consumer. \"_","4fbec9c3":"**### 3.1.4 Recursive Feature Elimination<a class='anchor' id=RFE><\/a> \n\nRecursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.","46cc6448":"### 2.1 A Through Analysis of Inpute Features <a class='anchor' id=thorough><\/a> \n","800a8e73":"### Logistic Regression Model Fitting","7df5eb55":"\n### Logistic Regression <a class='anchor' id=logistic><\/a>","18a4d48c":"The result is telling us we have 8977+8983 correct predictions and 2020+2116 incorrect predictions.","1199502f":"## 2. High Level feel for the dataset <a class='anchor' id=feel><\/a>  ","d4bb6319":"### 3. Constructing Model <a class='anchor' id=model><\/a>","e5458a54":"The ideal Roc curve hugs the top-left corner,indicating a huge true positive rate and a low false positve rate. The dotted line represents the purely random classifer. The chart shows that our final model is doing a pretty good job in classification.\n","b90a0a8e":"It is the newton method that converges most fast but generate an error-cotaining result. We have no choice but to select lbfgs,the second fast model.","d5aefc3d":"### (3) Removing Outlier VS Adding another categorical feature\n\nSince we learn descriptive statistics and basic stuff about distribution, we are trained to draw charts  to see if the data is approximating a specific distribution and\/or outliers exist.  Especially if you witness any points lying far from the cluster of data sets, without enough reflection on any hidden clues, we tend to get rid of them right away. We will examine the objectiveness and feasibleness of attempting to remove the distant points. \n\nFrom the charts above, We can acquire  some knowledge of consumer behavior \n\nFrist, those, who had been inactive for 15000 days, started to use the service again in the month of the survey. We could classify them \na returing user.\n\nSecondly,the users who recently joined companies and stayed connected to the company for a relatively shorter period did not make any outgoing activity in the month of the survey.  There is a greater chance for companies to deactivate prepaid connections.  Therefore, we may appoint them a potentially inactive. ","005ddca2":"###  2.1.1 user_lifetime VS Inactivation<a class='anchor' id=user_lifetime><\/a>\n\nA user lifetime has been a metric to measure the business success of companies since the longer consumers stay with them, the more revenue they are likely to earn from each consumer. Generally, we consider  the longer period as a positive sign to business. ","1f5b293f":"###  2.1.3 GPRS <a class='anchor' id=gprs><\/a>\n\n![image.png](attachment:image.png)\n\nGPRS is often referred to as 2.5G network,having enabled a variety of telecomuication services before 3G,4G,adn 5G. This old techonolgy is gradually phased out in the current market.  At the time where GRS was first introduced to the market, it gained a lot of attention from companies and the major embedded features surprising the market includes  \n  - faster data rates for downloads\n  - multimedia messaging service\n  - wireless application protocol  \n\nand so on.  Many articles published in years between 2003 and 2005 suggested that the advent of the network had hit the market providing unprecedented levels of services for customers. Let's see how this techology has actually woroked in our case.","b24d2cb9":"### 3.1.1 Feature Scaling <a class='anchor' id=scaling><\/a>","6dc428e1":"###  ROC curve","2a69e204":"###  (2) Charts","64a6b799":"### 3.1.2 Create Dummy Variables<a class='anchor' id='dummy'><\/a>","f143b1bc":"## 1.2 Importing the data set into a pandas Dataframe<a class='anchor' id=import><\/a>","d5524301":"If we decided to filter out both groups, the number of lost information would be in an amount to 59868 or 10% of the whole. This is absolute nonsense to treat them as outliers. \n\n---\n###  _Lesson 2 \" Create 'temporary' feature variable  whose memeber include four types; currenlty active, currentlyinactive, potentially inactive, return consumer .\"_\n---","ef2b502a":"### Confusion Matrix","7696b6cc":"## 1.4 Some Obseravtions From STR details <a class='anchor' id=details><\/a>\n\n\n#### - Null Values: \n We do not see a need to take care of nulls because all of the input features are free from any missing values. \n \n#### - Converion of Data type:\n\nSome of the inputs entered into the record in the form of an integer .These features are binary variables, either 0 or 1. Therefore, to gain an analytical value, they are to be converted into a proper data type. These are following below;\n\n                (1) user_intake\n                (2) user_has_outgoing_calls\n                (3) user_has_outgoing_calls\n                (4) user_use_gprs\n                (5) user_does_reload\n                (6) churn \n\n\n#### - Skewness \n\nThe rule of thumbs of skewnees seem to be \n\nIf the absolute value of skewness is close to 0 but less than 1, the data set is following a normal distribution.\n\nIf the absolute value of statics shows a value greater than 1 but less than 2, it is considered moderately symmetrical.\n\nThe problem stats if the value is above either -2 and 2. They are said to be heavily asymmetrical and need to be transformed into normal if possible. \n\nMajority of our input varialbes are above either -2 and 2. The proper transfomrtion of data will proceeed in the later section of this kernal. \n\n\n#### -Kurtosis \n\nThis meausrs how outelier-prone our dataset is to be. The greater value it gives, the higher chance of elminating noises from our data\nwe have to prevent them from disrupting our analysis. Surprisingly, some input features show anomalous values, which leads us to filter out some observations. \n\n\n\n\n","13aa6d11":"###  (1) Life time Ratio\nHowever, in the case of the telecom market, the measurement is of little analytical value in that a consumer inactive for a longer period may are considered as a loyal customer. Therefore, we need to bring all the relevant numbers down to another measurement to evaluate the consumer's loyalty.\n\n$$  Lifetime\\  ratio = Userlieftime \\div Nubmer\\  of\\  days\\ being\\  inactive \\times Monthly\\ user\\ Average\\ spendings\\div 30 $$\n\n\nThe highger ratio is, the more revenue companies can earn from providing services. Even though the groups who had been inactive has 4 times lon as those who had used the service at least once in the same month.  \n","eeb2e7f2":"## 1 . Imports<a class='anchor' id=import><\/a>  ","5a74ae3c":"Now we have a perfect balanced data! You may have noticed that I over-sampled only on the training data because by oversampling only on the training data, none of the information in the test data is being used to create synthetic observations, therefore, no information will bleed from test data into the model training.","e9c246df":"## Table of Contents\n---\n1. [Imports](#import)  \n\n   1.1 [Importing the necessary librairies for this kernel](#import_librairies)\n   \n   1.2 [Importing the dataset into a pandas DataFrame](#import)\n \n   1.3 [Summarising Data: Statistical Descriptions](#description)\n \n   1.4 [Some Observations from the STR details](#details)\n \n   1.5 [Feature Manipulation](#manipulation)\n\n\n2. [High Level feel for the dataset](#feel) \n   [A Through Analysis of Inpute Features ](#thorough)  \n  \n  * 2.1 [user_lifetime VS Inactivation](#user_lifetime)\n  * 2.2 [Spending and Duration of Outoging Activitiy](#spending_duration)\n  * 2.3 [GPRS](#gprs)\n\n3.[Constructing Model](#model)\n    \n   3.1 [Logistic Model](#logistic)\n  \n  * 3.1.1 [Feature Sacling](#scaling)\n  * 3.1.2 [Create Dummy Variables](#dummy) \n  * 3.1.3 [Imbalanced Data Check-Synthetic Minority Oversampling Technique](#SMOTE)\n  * 3.1.4 [Recursive Feature Elimination](#RFE)\n  * 3.1.5 [Performance Test](#test)\n   (model fitting,confusion Matrix and many more)\n\n\nTo be continued soon!\n","5931ef83":"### Compute precision, recall, F-measure and support\n\nThe precision is the ratio tp \/ (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\nThe recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\nThe F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\nThe F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.\nThe support is the number of occurrences of each class in y_test.","89a16b05":"There are two chatractersitcs we should pay attention to in evaluating how the newly introduced technology will affect the sales revenue. \n\nFirst,the introduction of new technology was not attractive to induce that whow are likely to be inactive.  The highest usage of the service like others was recorded by two groups(highly active consumers and return consumers). \n\nAlso,  inactive days  are similar among all the user types.  What this means is that even repeated consumers who are loyal to the companies did not think the newly introduced technology was tempedto.Not only did the company fail to persuade the group with the higher possibility of ceasing thier service, but also it did not help their loyal customers to settle on the use of GPRS.\n\nFinally, let's look at the correlation with user_lifetime.All the correlation with user_lifetime is below 0.2 and considered as weak realtionship. These are strong candidates for being dropped out. "}}