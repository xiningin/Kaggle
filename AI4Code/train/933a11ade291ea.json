{"cell_type":{"49c0bc8e":"code","11ddfa36":"code","4073fac4":"code","9b14e57a":"code","7a38f954":"code","be5fb0fb":"code","74f39d7f":"code","ace9a351":"code","3101ae6a":"code","7f3d4097":"code","153439c7":"code","0f5e3a0d":"code","9f6d65bb":"code","6d92738a":"code","9a84c1a8":"code","cdfa1db5":"code","00fff6dc":"code","9b7b1241":"markdown","0d7736e8":"markdown","31ff5640":"markdown","86263b0d":"markdown","b4dab3ad":"markdown","b58ed57e":"markdown","5d5ba574":"markdown","333322fb":"markdown","f09556f4":"markdown","4a44822d":"markdown","724af276":"markdown","e81b1a67":"markdown","4b93bdde":"markdown","14215cf2":"markdown","1de32140":"markdown","8139520f":"markdown","cf900709":"markdown","ff29258f":"markdown","7e4c0213":"markdown","d23e1045":"markdown","a09487c1":"markdown","bd30ad83":"markdown","50febf72":"markdown","fa7ef5e5":"markdown","4a7f4481":"markdown","148e299b":"markdown"},"source":{"49c0bc8e":"!pip install seaborn==0.11\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","11ddfa36":"#importing the necessary packages\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport matplotlib.gridspec as gridspec\nimport matplotlib_venn as vplt\nplt.rcParams['figure.dpi'] = 200 #high resolution","4073fac4":"#reading the datasets\ndata_20 = pd.read_csv('..\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv',low_memory=False)\ndata_19 = pd.read_csv('..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv',low_memory=False)\ndata_18 = pd.read_csv('..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv',low_memory=False)","9b14e57a":"#Seperating the first row that has the Questions from the rest of the data\nQuestions_20 = data_20.iloc[0]\ndata_20 = data_20.iloc[1:]\nQuestions_18 = data_18.iloc[0]\ndata_18 = data_18.iloc[1:]\nQuestions_19 = data_19.iloc[0]\ndata_19 = data_19.iloc[1:]\n\n#colors='#DEE4E7'","7a38f954":"#Creating an empty list to store the name of the notebooks and number of respondents who use it\nNotebook_names = []\nNotebook_popularity = []\n\n#We are iterating over the for loop and entering appropriate values in the empty list\n#using string concatenation to get Column names like 'Q10_Part_ + str(1)'='Q10_Part_1'\nfor x in range(1,13):\n    Notebook_names.append(data_20['Q10_Part_'+str(x)].dropna().unique()[0])\nNotebook_names.append(data_20['Q10_OTHER'].dropna().unique()[0])\n\n\nfor y in range(1,13):\n    Notebook_popularity.append(data_20['Q10_Part_'+str(y)].dropna().count())\nNotebook_popularity.append(data_20['Q10_OTHER'].dropna().count())\n                     \n#Plotting the graph, using seaborn to plot the bar graph using sns.barplot \nplt.figure(figsize=(15,12))\nsns.set_style('whitegrid')\nsns.barplot(y=Notebook_names,x=Notebook_popularity,color='#aaa7cc')\n\n#We are accessing the axis using plt.gca() and storing it in the axis funtion, then getting the bars, children\n#of the axis object via .get_children() and setting the color of the individual bar via .set_color()\nax=plt.gca()\nax.get_children()[0].set_color('#66669a')\nax.get_children()[1].set_color('#5C5174')\nax.get_children()[4].set_color('#926d88')\nplt.title('Hosted Notebook Products Used',size=20)\n","be5fb0fb":"\n\n#Cleaning 2018 data\nreplace = {\"Q14_Part_1\" : {'Kaggle Kernels':' Kaggle Notebooks'},\n           \"Q14_Part_2\" : {'Google Colab' : 'Colab Notebooks'},\n           \"Q14_Part_9\" : {'JupyterHub\/Binder': ' Binder \/ JupyterHub '}\n            }\ndata_18.replace(replace,inplace=True)\n\n#Working with the 2018 Data\n\n#Creating an empty list to store the name of the notebooks and number of respondents who use it\nNotebook_18 = []\nNotebook_18_values = []\n\n#We are iterating over the for loop and entering appropriate values in the empty list\n#using string concatenation to get Column names like 'Q10_Part_ + str(1)'='Q10_Part_1'\nfor x in range(1,3):\n    Notebook_18.append(data_18['Q14_Part_'+str(x)].dropna().unique()[0])\nNotebook_18.append(data_18['Q14_Part_9'].dropna().unique()[0])\n\nfor y in range(1,3):\n    Notebook_18_values.append(data_18['Q14_Part_'+str(y)].dropna().count())\nNotebook_18_values.append(data_18['Q14_Part_9'].dropna().count())\n    \n#Cleaning 2019 data\nreplace = {\"Q17_Part_1\" : {' Kaggle Notebooks (Kernels) ':' Kaggle Notebooks'},\n           \"Q17_Part_2\" : {' Google Colab ' : 'Colab Notebooks'}\n            }\ndata_19.replace(replace,inplace=True)\n\n#Working with 2019 Graph    \n\n#Creating an empty list to store the name of the notebooks and number of respondents who use it\nNotebook_19 = []\nNotebook_19_values = []\n\n\n\n#We are iterating over the for loop and entering appropriate values in the empty list\n#using string concatenation to get Column names like 'Q10_Part_ + str(1)'='Q10_Part_1'\nfor x in range(1,3):\n    Notebook_19.append(data_19['Q17_Part_'+str(x)].dropna().unique()[0])\nNotebook_19.append(data_19['Q17_Part_7'].dropna().unique()[0])\n\n\nfor y in range(1,3):\n    Notebook_19_values.append(data_19['Q17_Part_'+str(y)].dropna().count())  \nNotebook_19_values.append(data_19['Q17_Part_7'].dropna().count())\n\n#Working with the 2020 data\n\n#Creating an empty list to store the name of the notebooks and number of respondents who use it\nNotebook_20 = []\nNotebook_20_values = []\n\n#We are iterating over the for loop and entering appropriate values in the empty list\n#using string concatenation to get Column names like 'Q10_Part_ + str(1)'='Q10_Part_1'\nfor x in range(1,3):\n    Notebook_20.append(data_20['Q10_Part_'+str(x)].dropna().unique()[0])\nNotebook_20.append(data_20['Q10_Part_5'].dropna().unique()[0])\n\n\nfor y in range(1,3):\n    Notebook_20_values.append(data_20['Q10_Part_'+str(y)].dropna().count())\nNotebook_20_values.append(data_20['Q10_Part_5'].dropna().count())\n\n#Converting the Notebook_20_values from raw numbers to percentage,\n#This is done as the number of respondents to the survey each year were not same, so using raw numbers will be \n#misleading.\n#If you have trouble understanding the code below, we are using a list comprehension here, you can search it \n#on google \nNotebook_20_per = [((i\/data_20.shape[0])*100) for i in Notebook_20_values]\nNotebook_19_per = [((i\/data_19.shape[0])*100) for i in Notebook_19_values]\nNotebook_18_per = [((i\/data_18.shape[0])*100) for i in Notebook_18_values]","74f39d7f":"plt.figure(figsize=(15,12))\n\n#We are using Gridspec here, to plot the subplots\ngspec = gridspec.GridSpec(3, 3)\nsns.set_style('whitegrid')\n\n#Ploting the first subplot\nplt.subplot(gspec[0:2,0:2])\n\n#Plotting the 2020 Graph,\n#We are using Matplotlib's barh function for that\nplt.barh(y=Notebook_20,width=Notebook_20_per,color='#ACDDDE',height=0.3)\n\n\n#Getting the location of x-ticks,in the loc variable and setting up for the next bars\nlocs, label = plt.yticks()\n\n#in the x-ticks location we add the width of our current bar, this is done in order plot the bars side by side\n#we create an empty list new_xvals1 which will be filled with the y-pos of the second set of bars\nnew_xvals1 = []   \n\nfor item in locs:\n    new_xvals1.append(item+0.3)\n    \n\n#Plotting the 2019 Graph\nplt.barh(y=new_xvals1,width=Notebook_19_per,color='#CAF1DE',height=0.3)\n \n#setting up for the next bars \n#in the x-ticks location we add the width of our current bar 2 times, this is done in order plot the three bars side by side\n#we create an empty list new_xvals1 which will be filled with the y-pos of the third set of bars\nnew_xvals2 = []   \n\nfor item in locs:\n    new_xvals2.append(item+0.6)\n\n#Plotting the 2018 Graph                     \n\nplt.barh(y=new_xvals2,width=Notebook_18_per,color='#E1F8DC',height=0.3)\n\n#we access th axes object via plt.gca(), and remove the yaxis grids by ax.yaxis.grid(False) code\nax=plt.gca()\nax.yaxis.grid(False)\n\nplt.title('Hosted Notebook Products Used (in %)',size=20)\nplt.legend(['2020','2019','2018'])\n\n#plotting the second subplot\nplt.subplot(gspec[2,2])\n\n#dat variable stores the y, and dat_values stores the number of repondents for the given years, then we plot the bar graph\ndat = ['2018','2019','2020']\ndat_values = []\ndat_values.append(data_18.shape[0])\ndat_values.append(data_19.shape[0])\ndat_values.append(data_20.shape[0])\nplt.bar(dat,dat_values)\nplt.title('Number of Respondents')\nax=plt.gca()\nax.xaxis.grid(False)\n\n#plotting the third subplot\nplt.subplot(gspec[0,2])\n\n#We store years in Notebook_year variable, and create two empty lists, Notebook_yv_K and Notebook_yv_C, to store the Kaggle and Colabs Percentage\n#Popularity in the years 2018,2019,and 2020\nNotebook_year = ['2018','2019','2020']\nNotebook_yv_K = []\nNotebook_yv_C = []\n\nNotebook_yv_K.append(Notebook_18_per[0])\nNotebook_yv_K.append(Notebook_19_per[0])\nNotebook_yv_K.append(Notebook_20_per[0])\nNotebook_yv_C.append(Notebook_18_per[1])\nNotebook_yv_C.append(Notebook_19_per[1])\nNotebook_yv_C.append(Notebook_20_per[1])\n\n\n#We Plot two lines, one for Kaggle, and the other for Colab \nplt.plot(Notebook_year,Notebook_yv_K,'-o')\nplt.plot(Notebook_year,Notebook_yv_C,'-o')\nax=plt.gca()\nax.xaxis.grid(False)\nplt.legend(['Kaggle','Colab Notebooks'])\n\n\n#THIS IS AN Alternate style for the third subplot, you can check it out.\n\n#fig=plt.gcf()\n#ax.axes.get_yaxis().set_visible(False)\n#sns.despine(fig=fig)\n#ax.text(-0.1, 25.8, \"25.3\", transform=ax.transData,color='#301934')\n#ax.text(0.9, 25.1, \"24.6\", transform=ax.transData,color='#301934')\n#ax.text(1.9, 30.4, \"29.9\", transform=ax.transData,color='#301934')\n#ax.text(-0.1, 15.3, \"14.8\", transform=ax.transData,color='#00008B')\n#ax.text(0.9, 23.6, \"23.1\", transform=ax.transData,color='#00008B')\n#ax.text(1.9, 32.1, \"31.6\", transform=ax.transData,color='#00008B')\n","ace9a351":"plt.figure(figsize=(15,12))\n\n#Plotting the first subplot\nplt.subplot(1,2,1)\n\n#Here we are first creating a pivot table with the following columns = [\"Q6\",'Q10_Part_1','Q10_Part_2','Q10_Part_5'] and then grouping them w.r.t 'Q6'\n#After this, we are creating a new dataframe using the pivot table, which contains the notebooks name, and their counts in the columns w.r.t the \n#no. of years a person has been coding\npivot = data_20[[\"Q6\",'Q10_Part_1','Q10_Part_2','Q10_Part_5']].groupby(\"Q6\")\nnew_data = pivot['Q10_Part_1'].count()\nnew_data = pd.DataFrame(new_data)\nnew_data.rename(columns = {'Q10_Part_1' : 'Kaggle Notebooks'},inplace=True)\nnew_data['Colab Notebooks'] = pivot['Q10_Part_2'].count()\nnew_data['Jupyter Hub'] = pivot['Q10_Part_5'].count()\n\n#We use this block of code to rearange the y-axis ticklabels so that they are in the correct order i.e. ['<1 years',.......,'20+years']\nrow_index = [5,0,3,4,1,2]\nnew_data = new_data.iloc[row_index]\n\n#now we are creating the heatmap, note that cbar=False removes the color bar, and annot=True will show the values on the of the heatmaps tiles, also\n#fmt='d' enures that the annotation's are in integer format\nsns.heatmap(new_data,linewidth=0.3,cbar=False,annot =True,fmt='d',cmap='BuPu')\nplt.title('Coding')\nax = plt.gca()\n\n#Ploting the second subplot\nplt.subplot(1,2,2)\n\n#Here we are first creating a pivot table with the following columns = [\"Q15\",'Q10_Part_1','Q10_Part_2','Q10_Part_5'] and then grouping them w.r.t 'Q15'\n#After this, we are creating a new dataframe using the pivot table, which contains the notebooks name, and their counts in the columns w.r.t the \n#no. of years a person has been implementing ML methods\npivot = data_20[[\"Q15\",'Q10_Part_1','Q10_Part_2','Q10_Part_5']].groupby(\"Q15\")\nnew_data = pivot['Q10_Part_1'].count()\nnew_data = pd.DataFrame(new_data)\nnew_data.rename(columns = {'Q10_Part_1' : 'Kaggle Notebooks'},inplace=True)\nnew_data['Colab Notebooks'] = pivot['Q10_Part_2'].count()\nnew_data['Jupyter Hub'] = pivot['Q10_Part_5'].count()\n\n#We use this block of code to rearange the y-axis ticklabels so that they are in the correct order i.e. ['<1 years',.......,'20+years']\nrow_index = [7,8,0,2,4,5,6,1,3]\nnew_data = new_data.iloc[row_index]\n\n#This block of code is writen so that the category the categories of the first subplot 'coding vs notebook' and the second subplot 'Ml vs notebook',\n#have the same set of categories in the yticklabels\n#Note, this is stylisticly better, although because we are removing finer categories and making them larger, we are losing a bit of information in\n#process, It is entirely your choice if you want to do this step or not, Weigh the pros and cons for your own self\nnew_data.loc[len(new_data)] = (new_data.iloc[0] + new_data.iloc[1])\nnew_data.drop(new_data.index[0],inplace = True)\nnew_data.drop(new_data.index[0],inplace = True)\nnew_data.loc[len(new_data)] = (new_data.iloc[1] + new_data.iloc[2]+new_data.iloc[3])\nnew_data.drop(new_data.index[[1,2,3]],inplace = True)\nnew_data = new_data.rename(index = {'20 or more years':'20+ years',8:'<1 years',9:'3-5 years'})\n\n#Once again reordering the yticklabels, as the previous code removed the ordering\nrow_index = [5,0,4,1,2,3]\nnew_data = new_data.iloc[row_index]\n\n#now we are creating the heatmap, note that cbar=False removes the color bar, and annot=True will show the values on the of the heatmaps tiles, also\n#fmt='d' enures that the annotation's are in integer format\nsns.heatmap(new_data,linewidth=0.3,cbar=False,annot =True,fmt='d',cmap='BuPu')\nplt.title('Machine learning')\n\n#suptitle is used for the superheading of all the subplots\nplt.suptitle('Experience vs Hosted Notebook Used',size=20)\n\n\n","3101ae6a":"plt.figure(figsize=(15,12))\n\n#We are using GridSpec to plot the subplots\ngspec = gridspec.GridSpec(3, 3)\nsns.set_style('whitegrid')\n\n#plotting the first subplot\nplt.subplot(gspec[0:2,0:2])\n\n#Here we are first creating a pivot table with the following columns = [\"Q1\",'Q10_Part_1','Q10_Part_2','Q10_Part_5'] and then grouping them w.r.t 'Q1'\n#After this, we are creating a new dataframe using the pivot table, which contains the notebooks name, and their counts in the columns w.r.t the \n#age group of the respondents\npivot = data_20[[\"Q1\",'Q10_Part_1','Q10_Part_2','Q10_Part_5']].groupby(\"Q1\")\nnew_data = pivot['Q10_Part_1'].count()\nnew_data = pd.DataFrame(new_data)\nnew_data.rename(columns = {'Q10_Part_1' : 'Kaggle Notebooks'},inplace=True)\nnew_data['Colab Notebooks'] = pivot['Q10_Part_2'].count()\nnew_data['Jupyter Hub'] = pivot['Q10_Part_5'].count()\n\n#now we are creating the heatmap, note that cbar=False removes the color bar, and annot=True will show the values on the of the heatmaps tiles, also\n#fmt='d' enures that the annotation's are in integer format\nsns.heatmap(new_data,linewidth=0.3,cbar=False,annot =True,fmt='d',cmap='BuPu')\nplt.yticks(rotation='horizontal')\n\n#Plotting the second subplot\nplt.subplot(gspec[0:2,2])\n\n#we are plotting a countplot from the seaborn package, The parameter order is being used to order the yticklabels\n#before this, since the questions we were doing analysis on had multiple selected, we had to write alot of code, but since in Q1, only one option \n#can be seleted, we can plot this graph easily, if using this code for your own graphs you should try and see if the graph you want to plot has the\n#need for alot of code, or is it easily writable\nsns.countplot(y='Q1',data=data_20,order=['18-21', '22-24', '25-29', '30-34', '35-39', '40-44', '45-49',  '50-54', '55-59', '60-69',  '70+'],color='#DEE4E7')\n\n\n#Creating an empty list to store the name of the notebooks and number of respondents who use it\nNotebook = []\nNotebook_values = []\n\n#We are iterating over the for loop and entering appropriate values in the empty list\n#using string concatenation to get Column names like 'Q10_Part_ + str(1)'='Q10_Part_1'\nfor x in range(1,3):\n    Notebook.append(data_20['Q10_Part_'+str(x)].dropna().unique()[0])\nNotebook.append(data_20['Q10_Part_5'].dropna().unique()[0])\n\nfor y in range(1,3):\n    Notebook_values.append(data_20['Q10_Part_'+str(y)].dropna().count())\nNotebook_values.append(data_20['Q10_Part_5'].dropna().count())\n\n#Note- you don't have to write this block of code again, we already have written it and stored the values in a variable, you can just use it to create\n#the graph, the reason i did it, is so that if a person see this code only and wants to understand it, he will not have to back\n\n#Plotting the third subplot\nplt.subplot(gspec[2,0:2])\n\n#Plotting the bar graph\nplt.bar(x=Notebook,height=Notebook_values,color='#DEE4E7')\nax=plt.gca()\nax.xaxis.grid(False)\n\n#To reverse the bar graph\nax.tick_params(labelbottom=False)\nplt.ylim(reversed(plt.ylim()))\nplt.suptitle('Hosted Notebook Prefered vs Age')","7f3d4097":"#R_20 dataframe will just have the data of respondents who only use R\nR_20 = data_20[(data_20['Q7_Part_2']=='R')]\nR_20 = R_20[(R_20['Q7_Part_1']!='Python')]\n\n#P_20 dataframe will just have the data of respondents who only use Python\nP_20 = data_20[(data_20['Q7_Part_1']=='Python')]\nP_20 = P_20[(P_20['Q7_Part_2']!='R')]\n\n#RP_20 dataframe will just have the data of respondents who use both Python and R, this dataframe does not contain the respondents who use a single\n#language, if your familiar with Set theory this is just the , intersection of Python and R \nRP_20 = data_20[(data_20['Q7_Part_1']=='Python')]\nRP_20 = RP_20[(RP_20['Q7_Part_2']=='R')]\n\n#These Dataframes are going to be used again in 2.5, so make sure to remember them \n\nplt.figure(figsize=(15,10))\n#For this we had imported the matplotlib_venn as vplt beforehand, in subsets you input first vairable's count in '10' , second variable's count in '01'\n#, and in '11', you input the intersection of both the variable's\nv = vplt.venn2(subsets={'10':R_20.shape[0],'01':P_20.shape[0],'11':RP_20.shape[0]},set_labels = ('R','Python'),set_colors=['Red','Blue'])\nax= plt.gca()\nplt.title('R vs Python',size=20)","153439c7":"plt.figure(figsize=(15,12))\nsns.set_palette('crest')\n\n#The first subplot\nplt.subplot(3,1,1)\n\n\n\n#R_20 dataframe \n#Using Notebook to plot xticklabels\n#Using boolean graph to extract the information we need from the dataframe and then appending it in the Notebook_values list\nNotebook= ['Kaggle Notebooks','Colab Notebooks','Jupyter Hub']\nNotebook_values = []\nNotebook_values.append(R_20[R_20['Q10_Part_1']==' Kaggle Notebooks']['Q10_Part_1'].count())\nNotebook_values.append(R_20[R_20['Q10_Part_2']=='Colab Notebooks']['Q10_Part_2'].count())\nNotebook_values.append(R_20[R_20['Q10_Part_5']==' Binder \/ JupyterHub ']['Q10_Part_5'].count())\n\n#Plotting the bar graph\nplt.barh(Notebook,Notebook_values)\nax = plt.gca()\nax.yaxis.grid(False)\nplt.title('Uses only R')\nsns.set_palette('crest_r')\n\n#The second subplot\nplt.subplot(3,1,2)\n\n\n#P_20\n#Using Notebook to plot xticklabels\n#Using boolean graph to extract the information we need from the dataframe and then appending it in the Notebook_values list\nNotebook = ['Kaggle Notebooks','Colab Notebooks','Jupyter Hub']\nNotebook_values = []\nNotebook_values.append(P_20[P_20['Q10_Part_1']==' Kaggle Notebooks']['Q10_Part_1'].count())\nNotebook_values.append(P_20[P_20['Q10_Part_2']=='Colab Notebooks']['Q10_Part_2'].count())\nNotebook_values.append(P_20[P_20['Q10_Part_5']==' Binder \/ JupyterHub ']['Q10_Part_5'].count())\n\n#Plotting the bargraph\nplt.barh(Notebook,Notebook_values)\nax = plt.gca()\nax.yaxis.grid(False)\nplt.title('Uses only Python')\n\nsns.set_palette('viridis')\n\n#Third subplot\nplt.subplot(3,1,3)\n\n\n#RP_20\n#Using Notebook to plot xticklabels\n#Using boolean graph to extract the information we need from the dataframe and then appending it in the Notebook_values list\nNotebook = ['Kaggle Notebooks','Colab Notebooks','Jupyter Hub']\nNotebook_values = []\nNotebook_values.append(RP_20[RP_20['Q10_Part_1']==' Kaggle Notebooks']['Q10_Part_1'].count())\nNotebook_values.append(RP_20[RP_20['Q10_Part_2']=='Colab Notebooks']['Q10_Part_2'].count())\nNotebook_values.append(RP_20[RP_20['Q10_Part_5']==' Binder \/ JupyterHub ']['Q10_Part_5'].count())\n\n#Plotting the bar graph\nplt.barh(Notebook,Notebook_values)\nax = plt.gca()\nax.yaxis.grid(False)\nplt.title('Uses Both')\n\n\nplt.suptitle('Language vs Hosted Notebook',size=20)","0f5e3a0d":"#Creating an empty list to store the name of the Platforms and number of respondents who use it\nPlatform = []\nPlatform_values = []\n\n\n#We are iterating over the for loop and entering appropriate values in the empty list\n#using string concatenation to get Column names like 'Q10_Part_ + str(1)'='Q10_Part_1'\nfor x in range(1,9):\n    Platform.append(data_20['Q36_Part_'+str(x)].dropna().unique()[0])\nPlatform.append(data_20['Q36_OTHER'].dropna().unique()[0])\n\n\nfor y in range(1,9):\n    Platform_values.append(data_20['Q36_Part_'+str(y)].dropna().count())\nPlatform_values.append(data_20['Q36_OTHER'].dropna().count())\n                     \n\nplt.figure(figsize=(15,12))\nsns.set_style('whitegrid')\n\n#Plotting the graph, using seaborn to plot the bar graph using sns.barplot \nsns.barplot(x=Platform_values,y=Platform,data=data_20,color='#aaa7cc')\nax=plt.gca()\n\n#We are accessing the axis using plt.gca() and storing it in the axis funtion, then getting the bars, children\n#of the axis object via .get_children() and setting the color of the individual bar via .set_color()\nax.get_children()[3].set_color('#5C5174')\nax.get_children()[5].set_color('#66669a')\nax.get_children()[6].set_color('#926d88')\nplt.title('Platform used to publicly share or deploy data analysis or machine learning applications',size=20)","9f6d65bb":"#We are using Gridspec to create the subplots\ngspec = gridspec.GridSpec(3, 3)\nplt.figure(figsize=(15,12))\n\n#Plotting the first Subplot\nplt.subplot(gspec[0:2,0:2])\n\n#Here we are first creating a pivot table with the following columns = [\"Q15\",'Q36_Part_4','Q36_Part_6','Q10_Part_7'] and then grouping them w.r.t 'Q15'\n#After this, we are creating a new dataframe using the pivot table, which contains the Platforms name, and their counts in the columns w.r.t the \n#people who apply machine learning code \npivot = data_20[[\"Q15\",'Q36_Part_4','Q36_Part_6','Q36_Part_7']].groupby(\"Q15\")\nnew_data = pivot['Q36_Part_4'].count()\nnew_data = pd.DataFrame(new_data)\nnew_data.rename(columns = {'Q36_Part_4' : 'Github'},inplace=True)\nnew_data['Kaggle'] = pivot['Q36_Part_6'].count()\nnew_data['Colab'] = pivot['Q36_Part_7'].count()\nrow_index = [7,8,0,2,4,5,6,1,3]\nnew_data = new_data.iloc[row_index]\n\n#now we are creating the heatmap, note that cbar=False removes the color bar, and annot=True will show the values on the of the heatmaps tiles, also\n#fmt='d' enures that the annotation's are in integer format\nsns.heatmap(new_data,linewidth=0.3,cbar=False,annot =True,fmt='d',cmap='BuPu')\nplt.title('Machine learning')\n\n#Plotting the second Subplot\nplt.subplot(gspec[0:2,2])\n\n#Creating the countplot via seaborn, \nsns.countplot(y='Q15',data=data_20,order=['I do not use machine learning methods', 'Under 1 year', '1-2 years', '2-3 years', '3-4 years', '4-5 years', '5-10 years',  '10-20 years', '20 or more years'],color='#DEE4E7')\nax=plt.gca()\nplt.tick_params(labelleft=False)\n\n\n#Creating an empty list to store the name of the platforms and number of respondents who use it\nPlatform = []\nPlatform_values = []\n\n#Appending the names of the platforms and the values of the platforms usage for deploying code\nPlatform.append(data_20['Q36_Part_4'].dropna().unique()[0])\nPlatform.append(data_20['Q36_Part_6'].dropna().unique()[0])\nPlatform.append(data_20['Q36_Part_7'].dropna().unique()[0])\n\nPlatform_values.append(data_20['Q36_Part_4'].dropna().count())\nPlatform_values.append(data_20['Q36_Part_6'].dropna().count())\nPlatform_values.append(data_20['Q36_Part_7'].dropna().count())\n\n#Plotting the third subplot\nplt.subplot(gspec[2,0:2])\n\n#Plotting the bar graph\nplt.bar(x=Platform,height=Platform_values,color='#DEE4E7')\nax=plt.gca()\nax.xaxis.grid(False)\nax.tick_params(labelbottom=False)\n\n#To reverse the bar graph\nplt.ylim(reversed(plt.ylim()))\nplt.suptitle('Platform Prefered vs Machine learning Experience')","6d92738a":"plt.figure(figsize=(10,10))\n\n#Plotting the countplot in the seaborn package\nsns.countplot(y='Q5',data=data_20,color='#DEE4E7')\nax=plt.gca()\nplt.title('Professions')\n\n#Accessing the bars that we are going to use in the People in Data Science Field vs Platform used via axes's get_children() function and set color by\n#set_color() function.\nax.get_children()[1].set_color('#66669a')\nax.get_children()[3].set_color('#66669a')\nax.get_children()[4].set_color('#66669a')\nax.get_children()[8].set_color('#66669a')\nax.get_children()[10].set_color('#66669a')\nax.get_children()[12].set_color('#66669a')","9a84c1a8":"#We are using Gridspec for the creation of the subplots\ngspec = gridspec.GridSpec(3, 3)\nplt.figure(figsize=(15,12))\nplt.subplot(gspec[0:2,0:2])\n\n\n#Creating a new dataset just containing the relevant job positions\nNew_data = data_20[(data_20['Q5']=='Data Engineer')|(data_20['Q5']=='Data Scientist')|(data_20['Q5']=='Data Analyst')|(data_20['Q5']=='Statistician')|(data_20['Q5']=='Database Engineer')|(data_20['Q5']=='Data Engineer')|(data_20['Q5']=='Machine Learning Engineer')]\n\n#Then, we are first creating a pivot table with the following columns = [\"Q5\",'Q36_Part_4','Q36_Part_6','Q10_Part_7'] and then grouping them w.r.t 'Q5'\n#After this, we are creating a new dataframe using the pivot table, which contains the Platforms name, and their counts in the columns w.r.t the \n#people in the field of the Data Science\npivot = New_data[[\"Q5\",'Q36_Part_4','Q36_Part_6','Q36_Part_7']].groupby(\"Q5\")\nnew_data = pivot['Q36_Part_4'].count()\nnew_data = pd.DataFrame(new_data)\nnew_data.rename(columns = {'Q36_Part_4' : 'Github'},inplace=True)\nnew_data['Kaggle'] = pivot['Q36_Part_6'].count()\nnew_data['Colab'] = pivot['Q36_Part_7'].count()\n\n#Now we are creating the heatmap, note that cbar=False removes the color bar, and annot=True will show the values on the of the heatmaps tiles, also\n#fmt='d' enures that the annotation's are in integer format\nsns.heatmap(new_data,linewidth=0.3,cbar=False,annot =True,fmt='d',cmap='BuPu')\nplt.title('Relevant Job Titles vs Platform')\n\n#Plotting the second subplot\nplt.subplot(gspec[0:2,2])\n\n#Creating the countplot via seaborn, \nsns.countplot(y='Q5',data=New_data,order=['Data Analyst', 'Data Engineer', 'Data Scientist', 'Machine Learning Engineer', 'Statistician'],color='#DEE4E7')\nplt.tick_params(labelleft=False)\n\n#Creating an empty list to store the name of the platforms and number of respondents who use it\nPlatform_x = []\nPlatform_x_values = []\n\n#Appending the names of the platforms and the values of the platforms usage for deploying code\nPlatform_x.append(New_data['Q36_Part_4'].dropna().unique()[0])\nPlatform_x.append(New_data['Q36_Part_6'].dropna().unique()[0])\nPlatform_x.append(New_data['Q36_Part_7'].dropna().unique()[0])\n\nPlatform_x_values.append(New_data['Q36_Part_4'].dropna().count())\nPlatform_x_values.append(New_data['Q36_Part_6'].dropna().count())\nPlatform_x_values.append(New_data['Q36_Part_7'].dropna().count())\n\n#Plotting the third subplot \nplt.subplot(gspec[2,0:2])\nplt.bar(x=Platform_x,height=Platform_x_values,color='#DEE4E7')\nax=plt.gca()\nax.xaxis.grid(False)\nax.tick_params(labelbottom=False)\n\n#To reverse the bar graph\nplt.ylim(reversed(plt.ylim()))\nplt.suptitle('Platform Prefered vs Language',size=20)","cdfa1db5":"#In Section 1.6, We have already created the following Dataframes\n\n#R_20 dataframe will just have the data of respondents who only use R\n#P_20 dataframe will just have the data of respondents who only use Python\n#RP_20 dataframe will just have the data of respondents who use both Python and R, this dataframe does not contain the respondents who use a single\n#language, if your familiar with Set theory this is just the , intersection of Python and R \n\n#To understand how I created the following Dataframes please refer to that notebook.\n\nplt.figure(figsize=(15,12))\nsns.set_palette('crest')\n\n#The first subplot\nplt.subplot(3,1,1)\n\n\n\n#R_20 dataframe \n#Using Platform to plot xticklabels\n#Using boolean graph to extract the information we need from the dataframe and then appending it in the Notebook_values list\nPlatform= ['Kaggle Notebooks','Colab Notebooks','Github']\nPlatform_values = []\nPlatform_values.append(R_20['Q36_Part_6'].dropna().count())\nPlatform_values.append(R_20['Q36_Part_7'].dropna().count())\nPlatform_values.append(R_20['Q36_Part_4'].dropna().count())\n\n#Plotting the bar graph\nplt.barh(Platform,Platform_values)\nax = plt.gca()\nax.yaxis.grid(False)\nplt.title('Uses only R')\nsns.set_palette('crest_r')\n\n#The second subplot\nplt.subplot(3,1,2)\n\n\n#P_20\n#Using Notebook to plot xticklabels\n#Using boolean graph to extract the information we need from the dataframe and then appending it in the Notebook_values list\nPlatform= ['Kaggle Notebooks','Colab Notebooks','Github']\nPlatform_values = []\nPlatform_values.append(P_20['Q36_Part_6'].dropna().count())\nPlatform_values.append(P_20['Q36_Part_7'].dropna().count())\nPlatform_values.append(P_20['Q36_Part_4'].dropna().count())\n\n#Plotting the bargraph\nplt.barh(Platform,Platform_values)\nax = plt.gca()\nax.yaxis.grid(False)\nplt.title('Uses only Python')\n\nsns.set_palette('viridis')\n\n#Third subplot\nplt.subplot(3,1,3)\n\n\n#RP_20\n#Using Notebook to plot xticklabels\n#Using boolean graph to extract the information we need from the dataframe and then appending it in the Notebook_values list\nPlatform= ['Kaggle Notebooks','Colab Notebooks','Github']\nPlatform_values = []\nPlatform_values.append(RP_20['Q36_Part_6'].dropna().count())\nPlatform_values.append(RP_20['Q36_Part_7'].dropna().count())\nPlatform_values.append(RP_20['Q36_Part_4'].dropna().count())\n\n#Plotting the bar graph\nplt.barh(Platform,Platform_values)\nax = plt.gca()\nax.yaxis.grid(False)\nplt.title('Uses Both')\n\n\nplt.suptitle('Language vs Platform Users',size=20)","00fff6dc":"#Dataframes that will be used to create the Dataframes we require\nKaggle_20 = data_20[(data_20['Q36_Part_6']==' Kaggle ')]\nColab_20 = data_20[(data_20['Q36_Part_7']==' Colab ')]\nGithub_20 = data_20[(data_20['Q36_Part_4']==' GitHub ')]\n\n#K_20 dataframe will just have the data of respondents who use just Kaggle Platform\nK_20 = Kaggle_20[(Kaggle_20['Q36_Part_7']!=' Colab ')]\nK_20 = K_20[(K_20['Q36_Part_4']!=' GitHub ')]\n\n#C_20 dataframe will just have the data of respondents who use just Colab Platform\nC_20 = Colab_20[(Colab_20['Q36_Part_6']!=' Kaggle ')]\nC_20 = C_20[(C_20['Q36_Part_4']!=' GitHub ')]\n\n#G_20 dataframe will just have the data of respondents who use just Github Platform\nG_20 = Github_20[(Github_20['Q36_Part_6']!=' Kaggle ')]\nG_20 = G_20[(G_20['Q36_Part_7']!=' Colab ')]\n                 \n#KC_20 dataframe will just have the data of respondents who use both Kaggle and Colab Platforms\nKC_20 = Colab_20[(Colab_20['Q36_Part_6']==' Kaggle ')]\nKC_20 = KC_20[(KC_20['Q36_Part_4']!=' GitHub ')]\n                 \n#GC_20 dataframe will just have the data of respondents who use both Github and Colab Platforms.\nGC_20 = Github_20[(Github_20['Q36_Part_7']==' Colab ')]\nGC_20 = GC_20[(GC_20['Q36_Part_6']!=' Kaggle ')]\n                 \n#GK_20 dataframe will just have the data of respondents who use both Github and Kaggle Platforms\nGK_20 = Github_20[(Github_20['Q36_Part_6']==' Kaggle ')]\nGK_20 = GK_20[(GK_20['Q36_Part_7']!=' Colab ')]\n\n#GKC_20 datafram will just have the data of respondts who use all three platforms \nGKC_20 = Github_20[(Github_20['Q36_Part_6']==' Kaggle ')]\nGKC_20 = GKC_20[(GKC_20['Q36_Part_7']==' Colab ')]\n\nplt.figure(figsize=(15,10))\n#For this we had imported the matplotlib_venn as vplt beforehand\nv = vplt.venn3(subsets={'100':K_20.shape[0],'010':C_20.shape[0],'001':G_20.shape[0],'110':KC_20.shape[0],'101':GK_20.shape[0],'011':GC_20.shape[0],'111':GKC_20.shape[0]},set_labels = ('Kaggle','Colab','Github'))\nax= plt.gca()\nplt.annotate\nplt.title('Kaggle vs Colab vs Github',size=20)\n\nax.text(0.9, 0.6, \"Kaggle:\"+str(K_20.shape[0]), transform=ax.transData)\nax.text(0.9, 0.5, \"Colab:\"+str(C_20.shape[0]), transform=ax.transData)\nax.text(0.9, 0.4, \"Github:\"+str(G_20.shape[0]), transform=ax.transData)\nax.text(0.9, 0.3, \"Kaggle and Colab:\"+str(KC_20.shape[0]), transform=ax.transData)\nax.text(0.9, 0.2, \"Kaggle and Github:\"+str(GK_20.shape[0]), transform=ax.transData)\nax.text(0.9, 0.1, \"Colab and Github:\"+str(GC_20.shape[0]), transform=ax.transData)\nax.text(0.9, 0.0, \"Kaggle, Colab and Github:\"+str(GKC_20.shape[0]), transform=ax.transData)","9b7b1241":"# 2.1 Popularity","0d7736e8":"Another win for Colab, It seems that people of young age group (18 to 24 Years) like to use Google Colab More, in (25-29 years) and (55-59) age groups Kaggle is better, but by such a low margin that it is almost neglegible.\nOther, than that, I had thought that perhaps this metric could be helpful in explaining Colab's sudden spike in popularity, but unfortunately, that's not the case here.","31ff5640":"As the number of respondents each year is changing, 2018 had the most, after which there was a sharp decrease, and a minimal increase, rather than using raw count as the parameter we are using percentages for our comparision\n\nInteresting\nIt seems that Kaggle was the top dog in 2018 and 2019 in the 2020 big 3, and Jupyter Hub was doing much better than google colab in 2018\nBut that is history now, as Colab has seen a smooth growth in popularity as its growing older, Leaving behind Jupyter Hub in 2019, and Kaggle in 2020\nWhile, Kaggle's growth in terms of popularity was stagnant from 2018 to 2019, and saw growth in 2020, The growth is nowhere as rapid as that of Colab, which has been growing fastly and has left behind Kaggle and Binder\nIf, this scenario continues then in the coming years, Colab might just become the king of hosted notebooks.","86263b0d":"# 1.5 Popularity with Age Groups","b4dab3ad":"# 1. As a Hosted Notebook Products?\n\nGoogle Colab and Kaggle both provide hosted notebook services, Which are essential in a day to day workflow of a Data Scientist, Lets now compare them and see which is better, and see if any other platform gives these both a run for their money\n","b58ed57e":"#  2. Favourite Media source to learn about macine learning","5d5ba574":"Now, i am going to see which notebook do R users, Python users, and users who use both R and Python Prefer\nAlthough, a sad thing is that individuals using only R are very less, just 755 people, still lets go on with our analysis","333322fb":"# Summary and Key Insights\n\n\n**1. Hosted Notebooks**\n<ol>    \n   <li>1.1 Popularity:-<\/li>\n        Colab is the most popular hosted notebooks, Kaggle is at second very close to Colab, and very far behind from Kaggle and Colab is Binder\/Jupyter Hub\n        \n   Verdict:- Colab wins this one, Colab is the most popular\n    \n   <li>1.2 Growth:-<\/li>\n        Colab is once again the winner here, Colab has been rapidly growing the last three years, and has had a spike in popularity, and, thus wins this section as well.\n        \n   Verdict:- Colab has the most rapid growth rate, and might even become an the king decisively in the next few years\n   \n   <li>1.3 Usage by Coders:-<\/li>\n        Colab is more beginner friendly than Kaggle, As Colab is much more popular with beginner coders than Kaggle, Although for people with coding experience less than 1 years, Kaggle is more popular, and in every other experience range, Kaggle and Colab are either equal, or Colab is slightly Better, this one goes to Colab as well\n        \n   Verdict:- Colab wins this one, It is more popular than its counterparts with the biggest range of coding experience\n   \n   <li>1.4 Usage by Machine Learners:-<\/li>\n        Colab is more beginner friendly when it comes to writing Machine Learning Code, and wins by a very huge margin, and even in case of other experience levels, Colab has better results than Kaggle, the only exception is 3-5 years case, where kaggle is significantly better\n        \n   Verdict:- Colab wins this one, It seems that when it comes to writing Machine Learning Code Colab is very beginner friendly, more than its competitors\n   \n   <li>1.5 Popularity with Age groups:-<\/li>\n        Colab is more popular with all of the age groups, in some exceptions where Kaggle is more popular, the differnce between the two is negligible\n        \n   Verdict:- Yeah Yeah We Get it, Colab wins this one too\n   \n   <li>1.6 Language Friendly:-<\/li>\n        Beaking the trend, Kaggle is more popular than Colab with R Users, this is expected given R is not supported by Colab, Although in case of Python users and Both Python and R users, Colab is the hosted notebook of choice. \n         \n   Verdict:- This is a tie, as Colab is not R friendly, but is more popular with users of the other languages.\n<\/ol>   \n<div class=\"alert alert-success\">\n 1.0 Conclusion:-\n        When it comes to the hosted notebooks Colab is the victor, It defeats Kaggle Again and Again, and is the notebook of choice for many\n<\/div>\n<br>\n\n**2. Platforms that are used to share and deploy Machine learning Applications and Data Analysis**\n   \n   <ol>\n   <li>2.1 Popularity:-<\/li>\n        Github is the most popular hosted notebooks,That too by a very huge margin, Kaggle is at second, with a considerable lead than the third position which is *drumroll* Yeah, you guessed it, Colab.\n        \n   Verdict:- Kaggle wins this one, even though is second top, it has a sizable advantage than Colab.\n    \n   <li>2.2 Usage by Machine Learners:-<\/li>\n        Github is once again the top dog, Kaggle second, having an edge above the third, Colab, in every Category\n        .In some Job categories Kaggle has only a small advantage, but still is always in the lead when compared with Colab\n        \n   Verdict:- Kaggle is the victor here. It seems that Kaggle is significantly better than Colab when the experience of the respondent is from less than 1 year to 10 years of implementing machine learnong code.\n   \n   <li>2.3 Use by People who are in the Data science field:-<\/li>\n       Github is at lead with every individual of a specific occupation, Kaggle is doing better than Colab with people who are in the Data Science Field.\n        \n   Verdict:- Kaggle is the more Popular Platform to share code, and thus, wins\n    ,seeing the previous section 1.0, we all know how it is going to go ahead\n   \n   <li>2.4 Language Friendly:-<\/li>\n        The trend continues, Github is once again the most popular platform for share codes among Python and R users \n        Kaggle is more popular than Colab among Python and R users, The lead in popularity is significant in case of R users,this is expected given R is not supported by Colab\n        \n   Verdict:- Win for Kaggle, it is more popular with users of the Python and R language.\n    \n   <li>2.5 Usage of the Platform w.r.t Other Platforms:-<\/li>\n        More respondents use just Kaggle than just Colab, Altough the figure difference is very nascent.\n    Also in comparision with Github both look like little kids, there are alot, <il>and when i say alot, i mean it<\/il> of people just use Github, Also most of the Kaggle and Github users use these Platforms with conjecture to using Github \n        \n   Verdict:- Win for Kaggle, more number of users just use Kaggle than just use Colab \n<\/ol> \n<div class=\"alert alert-success\">\n  2.0 Conclusion:-\n        When it comes to the Platforms Used to share and deploy Data analysis and Machine Learning Code, Github is the victor <il>cough cough<\/il> Kaggle is the victor, It defeats Colab Again and Again, and is the Platform of choice for many\n<\/div>\n<br>\n\n\n# Conclusion\nIt is a tie, While Colab won in the first metric (Hosted Notebooks), while Kaggle won in the second metric (Platform to deploy and share Data Analysis and Machine Learning Code)\n\nCome on now, Both of you shake hands\n\n<img src=\"https:\/\/i.imgflip.com\/4olywc.jpg\" \/>\n\n# Sources\n\n1.) I have used the following website to generate the meme - [imgflip](https:\/\/imgflip.com\/memegenerator)\n<br>2.)[kaggle-vs-colab-faceoff-which-free-gpu-provider-is-top](https:\/\/towardsdatascience.com\/kaggle-vs-colab-faceoff-which-free-gpu-provider-is-tops-d4f0cd625029)\n<br>3.)[google-colab-vs-kaggle-kernels-which-of-the-two-platforms-should-you-go-for](https:\/\/analyticsindiamag.com\/google-colab-vs-kaggle-kernels-which-of-the-two-platforms-should-you-go-for\/)","f09556f4":"Well its clear that both of the brothers are at the top in comparision to all the other platforms that provide such services, the closest any one has come to them is Binder\/Jupyterhub, and it is still very far behind the siblings, all of the\nothers are far behind in terms of popularity.\n\nNow back to the story of the brothers, it seems that google colab got the upper hand here but by a very low margin, both are almost equal to each other.\nSeems the siblings are in close competition and in a few years perhaps one of them will get the upper hand\n","4a44822d":"# 2.4 Language Friendly\nOnce again, I am only considering Python and R, given how they are the most popular languages in which you can build machine learning applications.","724af276":"Obviously, Github is at first place given how it is the one stop destination to store and share your code,\nIt seems that this time Kaggle has the upper hand , while the other brother, colab seems to be in 3rd place, although it is not very behind, Also both the brothers seem to be doing be doing much better than all the other platforms (Excluding Github)\n\nNow,\nGithub is the place where you store all of your code, irrespective of wether it is data science related or not, while Colab and Kaggle can be considered a bit Data Science Centric. \nLets do an analysis with respect to how long an individual has been using machine learning methods","e81b1a67":"OK, so it seems that a lot of people use only Github, While that is not the case for Kaggle and Colab, Kaggle and Colab is usually in conjecture with Github, people who just use Kaggle or Colab or just the two of them are very less. Perhaps it is because the employers usually look at github profiles to decide if they want to hire and individual or not, so most of the Kaggle and Colab users have to upload their codes on Github as well.\n\nAlso, more no. of people just use Kaggle than just Colab, Although the number difference between Kaggle and Colab is not much but, still Kaggle is the winner here.","4b93bdde":"# 2.3 Use by People who are in the Data science field\nSo, lets do an analysis with just data scientists, data analysts, etc., leaving those people who are not on the great journey of Data Science.\nWill the trend still be the same, having Github at the top, and Kaggle being better than Colab","14215cf2":"# 1.1 Popularity","1de32140":" # **KAGGLE vs COLAB - Clash Of The Brothers**\n\n![image.jpg](https:\/\/mksaad.files.wordpress.com\/2019\/08\/imgonline-com-ua-twotoone-wjrvv5send9ld.jpg)\n\nWhen a person starts his or her DataScience Journey the two names he surely comes across are Kaggle and Google Colab.\n\n* Kaggle, a subsidiary of Google LLC, is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n\n* Colaboratory, or \u201cColab\u201d for short, is a product from Google Research. Colab allows anybody to write and execute arbitrary python code through the browser, and is especially well suited to machine learning, data analysis and education. More technically, Colab is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources including GPUs.\n\nKaggle and Colab, both part of google, are in a constant contest of supremacy, both providing almost same set of services with regards to writing and deploying data analysis or machine learning applications, can be considered brothers. And as we all know about relationships between siblings (Especially me, considering i have a elder sister) , there is always a bitter sweet rivalry between the two. \n![bickering siblings.png](https:\/\/tiredmomsupermom.com\/wp-content\/uploads\/2020\/04\/Effective-And-Simple-Tips-To-Stop-Sibling-Rivalry-And-Bickering-1.png)\n\nThis rivalry between the siblings is very interesting, It has been covered in many Blog Posts, News Reports, Kaggle Discussion forums, and is a topic that has been heatedly discussed, some prefering the former while others the latter, Everyone has their own opinions, and whenever a beginner searches online and asks the question, which one of the two is better, they get conflicting advices.\n\n\n\n# Purpose\n\nThe purpose of this Notebook is to understand if between the two brothers, one is more supreme than the other, and does better than the other in the intersection of the services they provide, this will be done by using the survey data.\n\n\n# Dataset\n\nIts fourth year since Kaggle has been conducting its annual Machine Learning and Data Science Survey. The 2020 Kaggle Machine Learning and Data Science Survey will be the dataset we will use as our primary dataset, we will also use 2018 and 2019 Kaggle ML & DS Survey Datasets as our secondary dataset, I did not include the 2017 dataset, as i wanted to keep the information as fresh as possible, in the same spirit the 2019 and 2018 data is used very minimally \n\n\n# Methodology\n\nFor our analysis we are going to focus on the two areas where the functionality of Colab and Kaggle intersects, namely\n1. As Hosted Notebook\n2. As a platform to share and deploy Data Analysis and Machine Learning Applications\n\nWe will keep both of these metrics seperate, analyse them separately, and use them to analyse which of the platform is better, Kaggle or Colab.\n\nIn case of the platform's use as a hosted notebook\n1. Popularity - Judged by the number of users who use them for the given task\n2. Growth - Comparing the trend of rise in popularity in three year, 2018,2019,2020\n3. Usage by Coders - Comparing experience in coding with the hosted notebook used\n4. Usage by Machine Learners - Comparing experience in using Machine learning Methods with the hosted notebook used\n5. Popularity with Age groups - Comparing the Age groups, with the hosted notebook prefered\n6. Language Friendliness - Comparing prefered language vs prefered hosted notebook\n\nIn case of the platform's use as a platform to share and deploy Machine learning Methods and Data Analysis\n1. Popularity - Judged by the number of users who use them for the given task\n2. Usage by Machine Learners - Comparing experience in using Machine learning Methods with the Platform used\n3. Usage by people in the data science field - Comparing the people in Data Science Field with the Platform they use\n4. Usage of the Platform w.r.t Other Platforms - Checking if the users who use one of the platforms use another one\n\nAs we continue with the story of the two brothers you will understand the reason why these parameters were chose\n\nAlso, for Platform used to deploy Machine Learning Algorithms, the 2018 and 2019 survey did not have this question asked, so i could not compare the growth in Popularity\n\nObviously, both the platforms have their own set of pros and cons, and it depends on an individual and his\/her needs that can make them choose one or the other, but keeping that in mind, lets use the survey to learn which one of the two brothers do more people like.\n\n\n# Insights from the Whole Analysis\n\n1.) Colab is the most popular in the field of Hosted Notebooks.\n\n2.) Colab has had a rapid growth rate, if this continues, and if its competitors don't keep up with it's growth rate, Colab might become the King of Hosted Notebooks, beating every other Platform in this area, in the next few years.\n\n3.)Colab is very beginner friendly when it comes to writing Code, especially Code which utilises Machine Learning Methods to solve a given problem, atleast when compared to its counterparts.\n\n4.)Colab seems to have gained alot of its populartiy in the last two years, and thus the trend of people with less than 2 years of coding experience using Colab more seems to make a bit more sense.\n\n5.)Kaggle does so much better than Colab in case of Popularity among R users, because Colab unlike Kaggle does not support R language, supports only Python and Swift. If you are a R user, and work primarily in R, then Kaggle is the Place to go to, Kaggle Notebooks support writing code in R, and there are more chances you will find other Kernels written in R in Kaggle than in Colab\n\n6.)GitHub is the most popular Platform for sharing and deploying Code.\n\n7.)Even when Kaggle and Colab are Platforms build specifically for Data Science, the people in Data Science use Github more than the above mentioned Platforms.\n\n8.)People who use Kaggle or Colab usually use them in conjecture with using Github. Perhaps it is because the employers usually look at github profiles to decide if they want to hire and individual or not, so most of the Kaggle and Colab users have to upload their codes on Github as well.","8139520f":"First lets talk about people who are beginner coders, It seems for people who have just gotten into coding and have been coding just for under 1 year seem to use kaggle more than its counterpart, but the difference is almost negligible, \nbut as the coding experience increases more than 1 year people seem to use Colab more, specially in the case of coders who have been coding for 1-10 years, Colab seems to be doing better than Kaggle although not by a huge margin, but also not by such a small number that we might consider it nascent.\nIn the case of veteran coders, Colab still wins although by a neglegible margin and both can almost be considered to be equal.\n\nSo Colab has the upper hand in case of coders, but what about Machine learning beginners and veterans, Certainly Kaggle and Colab are data science centric platforms, what's the trend here.\n\nIn Case of machine learning beginners, who have been implementing machine learning for less than 2 years, the tides are in the favour of Colab, Kaggle seems to be losing by a huge margin\nfor machine learning intermediates and veterans nothing can be said decisively, in case of 3-5 years Kaggle gets a slight edge, but then this advantage trickles down as the experience goes up from 5 years, but still nothing decisive.\n\nIt seems that, Colab is more begginer friendly than Kaggle , especially in terms of writing machine learning applications,\n\nAlso as we saw in the previous graph (Growth Section), Colab seems to have gained alot of its populartiy in the last two years, and thus this trend of people with less than 2 years of coding experience using Colab more seems to make sense, \nLets try and make a bit more of a sense as to why Colab saw a sudden spike in popularity in these last 2 years..","cf900709":"It seems that even when it comes to people who use Machine learning Methods, People use github more than Kaggle and Colab, even though kaggle and colab were specifically made to store machine learning applications, also The trend of Kaggle being more prefered than Colab seems to continues","ff29258f":"# 1.6 Language Friendly\n\nlets now compare which notebook do users of different languages prefer,\nFor this i am only considering Python and R, given how they are the most popular languages in which you can build machine learning applications","7e4c0213":"# 2.5 Usage of the Platform w.r.t Other Platforms\n","d23e1045":"OK, So it seems that Github is undeniably the king among all the other Platforms, no one can even compares to it, given the magnificent numbers it's projecting.\n\nNow, leaving Github aside.\nThe Trend is still the same as before, Colab behind of Kaggle, while the gap is not so great between the two when it comes to people who use Pyhton only or who use both the languages, Colab is left behind significantly\nby Kaggle when it comes to R users. Obviously, the reason for this is as Colab only supports Python and Swift,\nnot R.\n\nIt seems that for R users Kaggle is the place to go, not only in the case of Hosted Notebooks, but also when they want to deploy and share their data analysis and machine learning codes. ","a09487c1":"The same trend continues, Github is at lead with every individual of a specific occupation, It seems that when it comes to deploying even Machine Learning code and Exploratory Data Analysis codes, Github is the one most prefered, \nSimilarly, Kaggle also is doing better than Colab with people who are in the Data Science Field.\n\nI was trying to find if there would have had been some change in the trend, considering both Colab and Kaggle as pro Data Science, but it seem people of Data Science use Github for all their storing and deploying purposes, Although this is not that suprising that Github is the Holyland for people to share and deploy code, and organize works between team members.","bd30ad83":"# 2.2 Usage by Machine Learners\nNow,\nGithub is the place where you store all of your code, irrespective of wether it is data science related or not, while Colab and Kaggle can be considered a bit Data Science Centric. \nLets do an analysis with respect to how long an individual has been using machine learning methods","50febf72":"# 1.2 Growth\n\n\nLet's try and see how both the brothers have evolved as they have grown, we will use the 2018, 2019, and 2020 data for this","fa7ef5e5":"Aha, Just as I had suspected, For those of you who don't understand as to why Kaggle seems to be doing so much better than Colab in case of R users, it is because Colab unlike Kaggle does not support R language, supports only Python and Swift \nThis time Kaggle got the Hurray (The only one yet), although that Hurray is short lived since Collab again comes on top, in case of people who either only use Python or who use both the Languages, That too decisively (the difference between the two is not that low)\n<img src=\"https:\/\/i.imgflip.com\/4ogzhp.jpg\" \/>","4a7f4481":"In this above graph we can see all the occupations, For our further analysis of people in the field of Data Science, We will only consider the follow professions as Data Science Centric, namely\nData Engineer \/ Data Scientist \/ Data Analyst \/ Statician \/ Machine Learning Engineer \/ Database Engineer\nEven though some of the Students, Currently Employed might be learning about Data Science, But they are not in a job in the field of Data science so we will not be considering them\nProject Manager, Buisness Analyst, Research Scientist, Software Engineer, Other are too vague terms to consider a part of Data Science, as it might have had confused some of the respondents.","148e299b":"# 1.3 & 1.4 Usage by Coders and Machine Learners\n\nSo, now lets compare the two brothers in terms of how popular is it among beginners and veterans"}}