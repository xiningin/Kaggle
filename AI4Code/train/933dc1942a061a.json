{"cell_type":{"40bb32ae":"code","8e53869a":"code","5f676335":"code","d4cf049d":"code","f97393aa":"code","77d458f6":"code","9795ab99":"code","47f0236a":"code","b6e4f000":"code","d8a3b896":"code","401bc69b":"code","cf381e08":"code","bc43fa74":"code","7fa1570e":"code","3c918e03":"code","e4a3c282":"code","2f8afcae":"code","cbb26ef9":"code","32856eec":"code","338efaea":"code","7e1a2f9c":"code","d5dc9898":"code","458a0499":"code","4145846e":"code","ded239cb":"code","a0ef07f3":"markdown","ab5e808f":"markdown","ec36f4ac":"markdown","d6d51dc4":"markdown","f848a88a":"markdown","46808b59":"markdown","531780aa":"markdown","19b32f2a":"markdown","a227b64b":"markdown","e661067f":"markdown","443c6393":"markdown","9e478a40":"markdown","9d1a48e3":"markdown","5e26defe":"markdown","694c5029":"markdown","4a3a6f8a":"markdown","c85f0cc7":"markdown","820c72e4":"markdown","30bc8846":"markdown","d5495bb0":"markdown","a0b1c156":"markdown","a765c53d":"markdown","ea365e61":"markdown","c383e1be":"markdown","b51c38e9":"markdown","42a95476":"markdown","84fdd736":"markdown","97312e5f":"markdown","cf2f9dbe":"markdown","2be49df8":"markdown","650d57ca":"markdown","cb00afed":"markdown","e80f34b3":"markdown","7a46ead1":"markdown","e458552e":"markdown","38cbe49c":"markdown"},"source":{"40bb32ae":"%%html\n<style>\n  table {margin-left: 0 !important;}\n<\/style>","8e53869a":"import os\nimport numpy as np\nimport random\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport cv2 as cv\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n\nfrom IPython.display import Markdown as md","5f676335":"random.seed(1234)\nnp.random.seed(1234)\ntf.random.set_seed(1234)","d4cf049d":"image_size = 256\nlabels = ['NORMAL', 'PNEUMONIA']\n\ndef get_data(data_dir):\n        \n    images = []\n    \n# cria uma lista com o index de cada label com base no nome da ra\u00edz do diret\u00f3rio informado.\n    for label in labels:\n        dir = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        \n# percorre todas as imagens presentes no diret\u00f3rio, converte para grey_sacale e faz o resizing das imagens.\n        for image in os.listdir(dir):    \n            image_read = cv.imread(os.path.join(dir,image), cv.IMREAD_GRAYSCALE)\n            image_resized = cv.resize(image_read, (image_size, image_size))\n            images.append([image_resized, class_num])\n\n    return np.array(images)\n\ntrain       = get_data('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/train')\ntest        = get_data('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/test')\nvalidation  = get_data('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val')\n","f97393aa":"print('\\nQuantidade de imagens no dataset de treino:\\t', len(train), \n      '\\tcom o fomato: ', train.shape, \n      '\\nNORMAL:', np.count_nonzero(train[:,1] == 0),\n      '\\tPNEUMONIA:', np.count_nonzero(train[:,1] == 1))\n\nprint('\\nQuantidade de imagens no dataset de teste:\\t', len(test), \n      '\\tcom o fomato: ', test.shape,\n      '\\nNORMAL:', np.count_nonzero(test[:,1] == 0),\n      '\\tPNEUMONIA:', np.count_nonzero(test[:,1] == 1))\n\nprint('\\nQuantidade de imagens no dataset de valida\u00e7\u00e3o:\\t', len(validation), \n      '\\tcom o fomato: ', validation.shape,\n      '\\nNORMAL:', np.count_nonzero(validation[:,1] == 0),\n      '\\tPNEUMONIA:', np.count_nonzero(validation[:,1] == 1))","77d458f6":"plt.rcParams.update({'font.size': 16})\n\nfig = plt.figure(figsize = (16, 12))\ncolumns = 4\nrows = 3\n\nfor i in range(1, columns * rows + 1):\n    rnd = np.random.randint(0, len(train))\n    img = train[rnd][0]  \n    fig.add_subplot(rows, columns, i)\n    plt.title(labels[train[rnd][1]])\n    plt.axis('off')\n    plt.imshow(img, cmap='gray')\n\nplt.show()","9795ab99":"x_features = []\ny_labels = []\n\nfor feature, label in train:\n    x_features.append(feature)\n    y_labels.append(label)\n    \nfor feature, label in test:\n    x_features.append(feature)\n    y_labels.append(label)\n    \nfor feature, label in validation:\n    x_features.append(feature)\n    y_labels.append(label)","47f0236a":"x_features = np.array(x_features).reshape(-1, image_size, image_size, 1)\ny_labels = np.array(y_labels)\ny_labels = np.expand_dims(y_labels, axis =1)","b6e4f000":"x_train, x_test, y_train, y_test = train_test_split(x_features, \n                                                    y_labels,\n                                                    stratify = y_labels,\n                                                    test_size = 0.2, \n                                                    random_state = 2)","d8a3b896":"print('\\nnovo formato do tensor de features de treinamento:\\t', x_train.shape, \n      '\\nquantidade:\\t', len(x_train), \n      '\\tNORMAL:', np.count_nonzero(y_train == 0), \n      '\\tPNEUMONIA', np.count_nonzero(y_train == 1))\n\nprint('\\nnovo formato do tensor de labels de treinamento:\\t', y_train.shape, \n      '\\nquantidade:\\t', len(y_train),\n      '\\tNORMAL:', np.count_nonzero(y_train == 0), \n      '\\tPNEUMONIA', np.count_nonzero(y_train == 1))\n\nprint('\\nnovo formato do tensor de features de teste:\\t\\t', x_test.shape, \n      '\\nquantidade:\\t', len(x_test),\n      '\\tNORMAL:', np.count_nonzero(y_test == 0), \n      '\\tPNEUMONIA', np.count_nonzero(y_test == 1))\n\nprint('\\nnovo formato do tensor de labels de teste:\\t\\t', y_test.shape, \n      '\\nquantidade:\\t', len(y_test),\n      '\\tNORMAL:', np.count_nonzero(y_test == 0), \n      '\\tPNEUMONIA', np.count_nonzero(y_test == 1))","401bc69b":"x_train = x_train \/ 255\nx_test  = x_test  \/ 255","cf381e08":"i = Input(x_train.shape[1:])\n\na = Flatten()(i)\n\na = Dense(512, activation = 'relu')(a)\na = Dropout(0.4)(a)\n\na = Dense(512, activation = 'relu')(a)\na = Dropout(0.3)(a)\n\na = Dense(512, activation = 'relu')(a)\na = Dropout(0.1)(a)\n\na = Dense(1, activation = 'sigmoid')(a)\n\nmodel_NN = Model(i,a)","bc43fa74":"i = Input(x_train.shape[1:])\n\nb = Conv2D(32, (3,3), activation ='relu', padding = 'same')(i)\nb = BatchNormalization()(b)\nb = Conv2D(32, (3,3), activation ='relu', padding = 'same')(b)\nb = BatchNormalization()(b)\nb = MaxPooling2D(2,2)(b)\n\nb = Conv2D(64, (3,3), activation ='relu', padding = 'same')(b)\nb = BatchNormalization()(b)\nb = Conv2D(64, (3,3), activation ='relu', padding = 'same')(b)\nb = BatchNormalization()(b)\nb = MaxPooling2D(2,2)(b)\n\nb = Conv2D(128, (3,3), activation ='relu', padding = 'same')(b)\nb = BatchNormalization()(b)\nb = Conv2D(128, (3,3), activation ='relu', padding = 'same')(b)\nb = BatchNormalization()(b)\nb = MaxPooling2D(2,2)(b)\n\nb = Conv2D(256, (3,3), activation ='relu', padding = 'same')(b)\nb = BatchNormalization()(b)\nb = Conv2D(256, (3,3), activation ='relu', padding = 'same')(b)\nb = BatchNormalization()(b)\nb = MaxPooling2D(2,2)(b)\n\nb = Flatten()(b)\n\nb = Dense(512, activation = 'relu')(b)\nb = Dropout(0.4)(b)\n\nb = Dense(512, activation = 'relu')(b)\nb = Dropout(0.3)(b)\n\nb = Dense(512, activation = 'relu')(b)\nb = Dropout(0.1)(b)\n\nb = Dense(1, activation = 'sigmoid')(b)\n\nmodel_CNN = Model(i, b)","7fa1570e":"model_NN.compile(optimizer = 'SGD',\n              loss = \"binary_crossentropy\",\n              metrics = [\"accuracy\"])\n\nmodel_NN.summary()","3c918e03":"model_CNN.compile(optimizer = 'SGD',\n              loss = \"binary_crossentropy\",\n              metrics = [\"accuracy\"])\n\nmodel_CNN.summary()","e4a3c282":"batch_size = 16\n\ntrain_gen = ImageDataGenerator(rotation_range = 10,\n                               horizontal_flip = True,\n                               width_shift_range = 0.1,\n                               height_shift_range = 0.1,\n                               rescale = 1.,\n                               zoom_range = 0.2,\n                               fill_mode = 'nearest',\n                               cval = 0)\n\ntrain_generator_NN = train_gen.flow(x_train, y_train, batch_size)\ntrain_generator_CNN = train_gen.flow(x_train, y_train, batch_size)\n\nsteps_per_epoch = x_train.shape[0] \/\/ batch_size\n\ncheckpoint_NN = ModelCheckpoint('model_NN.h5', \n                                monitor = 'val_loss', \n                                verbose = 0, \n                                save_best_only = True, \n                                mode = 'auto')\n\ncheckpoint_CNN = ModelCheckpoint('model_CNN.h5', \n                                 monitor = 'val_loss', \n                                 verbose = 0, \n                                 save_best_only = True, \n                                 mode = 'auto')\n","2f8afcae":"epochs = 100","cbb26ef9":"history_NN = model_NN.fit(train_generator_NN, \n                          validation_data = (x_test, y_test), \n                          steps_per_epoch = steps_per_epoch, \n                          epochs = epochs,\n                          callbacks = [checkpoint_NN])","32856eec":"history_CNN = model_CNN.fit(train_generator_CNN, \n                            validation_data = (x_test, y_test), \n                            steps_per_epoch = steps_per_epoch, \n                            epochs = epochs,\n                            callbacks = [checkpoint_CNN])","338efaea":"model_NN = tf.keras.models.load_model('model_NN.h5')\nmodel_CNN = tf.keras.models.load_model('model_CNN.h5')","7e1a2f9c":"def plot_accuracy_hist(model_history, fig_tittle):\n\n    plt.rcParams.update({'font.size': 14})\n\n    fig = plt.figure(figsize = (16, 4))\n    columns = 2\n    rows = 1\n\n    fig.suptitle(fig_tittle, fontsize = 20, y = 1.08)\n    fig.add_subplot(rows, columns, 1)\n\n    plt.title('Accuracy')\n    plt.ylim(0.6, 1)\n    \n    plt.plot(model_history.history['accuracy'], \n             color='green', \n             label = 'Train Accuracy')\n\n    plt.plot(model_history.history['val_accuracy'], \n             color='red', \n             label = 'Test Accuracy', \n             linestyle='dashed')\n\n    plt.legend()\n\n    fig.add_subplot(rows, columns, 2)\n\n    plt.title('Loss')\n    plt.plot(model_history.history['loss'], \n             color='green', \n             label = 'Train Loss')\n\n    plt.plot(model_history.history['val_loss'], \n             color='red', \n             label = 'Test Loss', \n             linestyle='dashed')\n\n    plt.legend()\n\n    plt.show()\n    \nplot_accuracy_hist(history_NN, 'Neural Network')\nplot_accuracy_hist(history_CNN, 'Convolutional Neural Network')","d5dc9898":"plt.rcParams.update({'font.size': 16})\n\ndef plot_confusion_mtx(model, x_test, plot_tittle):\n    pred_prob = model.predict(x_test, batch_size = 8)\n    pred = np.where(pred_prob > 0.5, 1,0)\n\n    CM = confusion_matrix(y_test, pred)\n\n    plot_confusion_matrix(conf_mat = CM, figsize = (16, 8))\n    plt.title(plot_tittle)\n    plt.xticks(range(2), labels)\n    plt.yticks(range(2), labels)\n\n    plt.show()\n\nplot_confusion_mtx(model_NN, x_test, 'Neural Network')\nplot_confusion_mtx(model_CNN, x_test, 'Convolutional Neural Network')\n    ","458a0499":"\npred_prob_NN  = model_NN.predict(x_test, batch_size = 8)\npred_prob_CNN = model_CNN.predict(x_test, batch_size = 8)\n\npred_NN  = np.where(pred_prob_NN > 0.5, 1,0)\npred_CNN = np.where(pred_prob_CNN > 0.5, 1,0)\n\nNN_fpr, NN_tpr, _ = roc_curve(y_test, pred_prob_NN)\nNN_roc_auc = auc(NN_fpr, NN_tpr)\n\nCNN_fpr, CNN_tpr, _ = roc_curve(y_test, pred_prob_CNN)\nCNN_roc_auc = auc(CNN_fpr, CNN_tpr)\n\nplt.rcParams.update({'font.size': 16})\n\nfig = plt.figure(figsize = (8, 6))\n\nlw = 2\nplt.plot(CNN_fpr,\n         CNN_tpr, \n         color = 'darkorange',\n         lw = lw, \n         label = 'CNN - ROC curve (area = %0.2f)' % CNN_roc_auc)\n\nplt.plot(NN_fpr,\n         NN_tpr, \n         color = 'grey',\n         lw = lw, \n         label = '  NN - ROC curve (area = %0.2f)' % NN_roc_auc)\n\nplt.plot([0, 1], [0, 1], \n         color = 'navy', \n         lw = lw, \n         linestyle = '--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.title('Receiver operating characteristic')\nplt.legend(loc = \"lower right\")\n\nplt.show()\n\nprint('Classificarion Report - Neural Network')\nprint(classification_report(y_test, pred_NN))\n\nprint('\\nClassificarion Report - Convolutional Neural Network')\nprint(classification_report(y_test, pred_CNN))","4145846e":"cnn_report = classification_report(y_test, pred_CNN, output_dict = True)\nnn_report = classification_report(y_test, pred_NN, output_dict = True)\n\ncnn_auroc = '%0.2f' % CNN_roc_auc\nnn_auroc = '%0.2f' % NN_roc_auc\ndif_auroc = '%0.2f' % (CNN_roc_auc - NN_roc_auc)\n\ncnn_recall = '%0.0f%%' % (cnn_report['1']['recall']*100)\nnn_recall = '%0.0f%%' % (nn_report['1']['recall']*100)\ndif_recall = '%0.1fpp' % ((cnn_report['1']['recall'] - nn_report['1']['recall'])*100)\n\ncnn_precision = '%0.0f%%' % (cnn_report['1']['precision']*100)\nnn_precision = '%0.0f%%' % (nn_report['1']['precision']*100)\ndif_precision = '%0.1fpp' % ((cnn_report['1']['precision'] - nn_report['1']['precision'])*100)","ded239cb":"md(f\"Nas \u00faltimas d\u00e9cadas, t\u00e9cnicas de imagem m\u00e9dica, como tomografia computadorizada (TC), resson\u00e2ncia magn\u00e9tica (RM), tomografia por emiss\u00e3o de p\u00f3sitrons (PET), mamografia, ultrassom e raio-X, foram utilizadas para a detec\u00e7\u00e3o precoce, diagn\u00f3stico e tratamento de doen\u00e7as. Na pr\u00e1tica cl\u00ednica, a interpreta\u00e7\u00e3o dessas imagens m\u00e9dicas foram realizadas principalmente por especialistas, como radiologistas e m\u00e9dicos. No entanto, dadas as grandes varia\u00e7\u00f5es patol\u00f3gicas e a potencial fadiga desses especialistas, pesquisadores e m\u00e9dicos come\u00e7aram a se beneficiar de interven\u00e7\u00f5es assistidas por computador. Embora a taxa de progresso na an\u00e1lise computacional de imagens m\u00e9dicas n\u00e3o tenha sido t\u00e3o r\u00e1pida quanto a das tecnologias de imagens m\u00e9dicas, essa situa\u00e7\u00e3o tem melhorando com a introdu\u00e7\u00e3o de t\u00e9cnicas de aprendizado de m\u00e1quina. <br\/><br\/>A modelagem computacional para an\u00e1lise de imagens m\u00e9dicas teve um impacto significativo em aplica\u00e7\u00f5es cl\u00ednicas e na pesquisa cient\u00edfica. Os recentes avan\u00e7os em machine learning, especialmente no que diz respeito ao deep learning, lan\u00e7am uma nova luz sobre a an\u00e1lise de imagens m\u00e9dicas, permitindo a descoberta de padr\u00f5es morfol\u00f3gicos e\/ou texturais em imagens apenas a partir de dados. Os m\u00e9todos atuais de deep learning alcan\u00e7aram um desempenho de ponta em diferentes aplica\u00e7\u00f5es, ajudando a identificar, classificar e quantificar padr\u00f5es em imagens m\u00e9dicas. No centro desses avan\u00e7os est\u00e1 a capacidade de explorar representa\u00e7\u00f5es hier\u00e1rquicas de informa\u00e7\u00f5es apenas com dados, em vez de informa\u00e7\u00f5es manualmente definidas de acordo com conhecimentos do dom\u00ednio espec\u00edfico.<br\/><br\/>Uma das fraquezas do aprendizado profundo \u00e9 a dificuldade em identificar e tratar eventuais vi\u00e9ses na rede quando a complexidade \u00e9 muito grande. A quest\u00e3o pode ser resolvida com um maior volume de dados, bem como o estudo das rela\u00e7\u00f5es entre o resultado do modelo e as informa\u00e7\u00f5es avaliadas (as chamadas features). Esta dificuldade \u00e9 geralmente abordada com a ajuda de especialistas da \u00e1rea na revis\u00e3o de amostras dos resultados do modelo, principalmente como a avalia\u00e7\u00e3o visual das regi\u00f5es mais relevantes das imagens na tomada de decis\u00e3o do modelo.<br\/><br\/>Outro problema comumente enfrentado \u00e9 o overfitting do modelo. Com redes complexas e profundas este tipo de enfrentamento se torna bastante comum e cada vez mais dif\u00edceis de serem detectados, principalmente pelo componente de subjetividade do que \u00e9, de fato, um overfit de rede. A calibra\u00e7\u00e3o do par\u00e2metro de dropout \u00e9 uma das formas de minimizar o impacto e a probabilidade de overfitting em redes neurais.<br\/><br\/>Nossa abordagem aqui apresentada ilustra duas aplica\u00e7\u00f5es de deep learning na classifica\u00e7\u00e3o de imagens m\u00e9dicas de raio-X de t\u00f3rax para a identifica\u00e7\u00e3o de pneumonia. A primeira rede, mais simples, n\u00e3o contava com as se\u00e7\u00f5es convolutivas, simplificando significativamente a arquitetura da rede. A segunda rede incluiu etapas convolutivas, trazendo mais complexidade e profundidade \u00e0 rede. Os resultados mostram que a etapa convolutiva \u00e9 extremamente importante para a maximiza\u00e7\u00e3o do potencial classifica\u00e7\u00e3o do modelo, traduzindo em uma diferen\u00e7a de {dif_auroc} na AUROC (Area Under Receiver Operating Characteristics) curve, {nn_auroc} para a rede neural tradicional vs {cnn_auroc} para rede neural com etapas convolutivas. O recall, que \u00e9 a medida que tenta responder qual a propor\u00e7\u00e3o de eventos que realmente aconteceram e foram corretamente identificados, teve um incremento de {dif_recall} ({nn_recall} vs {cnn_recall}) com a adi\u00e7\u00e3o das etapas convolutivas. O recall mostra qu\u00e3o bom nosso modelo seria na detec\u00e7\u00e3o de uma pneumonia e \u00e9 a m\u00e9trica mais relevante se quisermos atuar na detec\u00e7\u00e3o precoce da pneumonia. A precis\u00e3o, que tenta responder qual a propor\u00e7\u00e3o das identifica\u00e7\u00f5es foram feitas corretamente, por sua vez, teve um incremento de {dif_precision} ({nn_precision} vs {cnn_precision}). Esta m\u00e9trica nos diz quanto ter\u00edamos de esfor\u00e7o aplicado em identifica\u00e7\u00f5es erradas e uma amplia\u00e7\u00e3o dela diminui o volume de recursos desperdi\u00e7ados. <br\/><br\/>Por fim, podemos concluir que aplica\u00e7\u00f5es de deep learning no aux\u00edlio do diagn\u00f3stico m\u00e9dico por imagens pode ser eficaz e j\u00e1 \u00e9 uma realidade nos dias de hoje. O uso dessa t\u00e9cnica traz agilidade ao processo com expressiva acur\u00e1cia mostrando ser uma alternativa vi\u00e1vel, principalmente em servi\u00e7os cl\u00ednicos superlotados e com dificuldades de contingente de especialistas.<br\/><br\/>Desenvolvimentos futuros do projeto apresentado deve incluir a melhoria do processo de sele\u00e7\u00e3o de hiperpar\u00e2metros, os quais foram manualmente selecionados nessa abordagem. Escolhas mais automatizadas e objetivas da calibra\u00e7\u00e3o desses par\u00e2metros podem incluir o m\u00e9todo de otimiza\u00e7\u00e3o Bayesiana de m\u00faltiplos par\u00e2metros, reduzindo o tempo de calibra\u00e7\u00e3o pela redu\u00e7\u00e3o do n\u00famero de itera\u00e7\u00f5es e escolhas inteligentes de par\u00e2metros por itera\u00e7\u00e3o para maximizar o quanto se aprende a cada passo.\")","a0ef07f3":"### Data Augmentation","ab5e808f":"- [Uma introdu\u00e7\u00e3o as redes neurais convolucionais utilizando o Keras](https:\/\/medium.com\/data-hackers\/uma-introdu%C3%A7%C3%A3o-as-redes-neurais-convolucionais-utilizando-o-keras-41ee8dcc033e), por [Alan Melo Clappis](https:\/\/medium.com\/@clappis)\n\n- [CNN- Tensorflow 2.0 ( F-Score 97%, Recall -98%)](https:\/\/www.kaggle.com\/arjunsarkar\/cnn-tensorflow-2-0-f-score-97-recall-98), por [Arjun Sarkar](https:\/\/www.kaggle.com\/arjunsarkar)\n\n- [Batch normalization in Neural Networks](https:\/\/towardsdatascience.com\/batch-normalization-in-neural-networks-1ac91516821c), por [FD](https:\/\/towardsdatascience.com\/@phidaouss)\n\n- [Tensorflow 2.0 API Documentation](https:\/\/www.tensorflow.org\/api_docs)\n\n- [Scikit-Learn - Metrics and scoring: quantifying the quality of predictions](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html)\n\n- [Shen D., Wu G., Suk H. 2017. Deep Learning in Medical Image Analysis. Annual Review of Biomedical Engineering](https:\/\/www.annualreviews.org\/doi\/full\/10.1146\/annurev-bioeng-071516-044442#_i50)\n\n- [Wang X. et al. 2017.Searching for prostate cancer by fully automated magnetic resonance imaging classification: deep learning versus non-deep learning. Scientific Reports](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5684419\/)\n\n- [Chen C. et al. 2020. Deep Learning for Cardiac Image Segmentation: A Review. Frontiers in Cardiovascular Medicine](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7066212\/)\n\n- [Jo, T., Nho, K., Saykin, A. 2019. Deep Learning in Alzheimer's Disease: Diagnostic Classification and Prognostic Prediction Using Neuroimaging Data. Frontiers in Aging Neuroscience](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6710444\/)\n\n- [Classification: Precision and Recall](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/precision-and-recall)","ec36f4ac":"Na sequencia seguimos com o treinamento de nossa Convolutional Neural Network.","d6d51dc4":"### Rede Neural Simples","f848a88a":"Primeiramente criamos uma Rede Neural simples em convulu\u00e7\u00f5es.","46808b59":"## Data Preparation","531780aa":"### Criando os Modelos","19b32f2a":"### Reshaping dos Tensores","a227b64b":"Agora podemos observar um conjunto de imagens presentes no dataset de treinamento.","e661067f":"Agora necessitamos fazer o reshaping das dimens\u00f5es dos datasets para serem processados pelos algoritmos de Deep Learning.\n\nIniciamos por extrair os componentes dos datasets de Treino e Teste carregados anteriormente e consolidados em um \u00fanico dataset que ser\u00e1 separado em datasets de treino e teste.","443c6393":"Para garantir a reprodutibilidade dos resultados vamos for\u00e7ar o seed dos m\u00f3dulos rand\u00f4micos do Numpy e Tensorflow.","9e478a40":"## Defini\u00e7\u00e3o da Arquitetura da Rede Neural","9d1a48e3":"Adicionalmente criamos uma Rede Neural com convulu\u00e7\u00f5es para compararmos os resultados com o modelo mais simples acima.","5e26defe":"### Accuracy and Loss\n\nUma **Loss function** \u00e9 usada para otimizar um algoritmo de aprendizado de m\u00e1quina. \n\nA perda \u00e9 calculada no treinamento e na valida\u00e7\u00e3o e sua interpreta\u00e7\u00e3o \u00e9 baseada no desempenho do modelo nesses dois conjuntos. \n\nO valor da perda implica em qu\u00e3o ruim ou bem um modelo se comporta ap\u00f3s cada itera\u00e7\u00e3o de otimiza\u00e7\u00e3o.\n\nA m\u00e9trica de acur\u00e1cia \u00e9 usada para medir o desempenho do algoritmo de uma maneira interpret\u00e1vel. \n\nA acur\u00e1cia de um modelo geralmente \u00e9 calculada na forma de uma porcentagem. \n\n\u00c9 a medida da precis\u00e3o da compara\u00e7\u00e3o da predi\u00e7ao do modelo com os dados reais.","694c5029":"Na sequ\u00eancia avaliamos as quantidades de observa\u00e7\u00f5es em cada dataset (Treino, Teste e Valida\u00e7\u00e3o).","4a3a6f8a":"### Confusion Matrix\n\nUma matriz de confus\u00e3o \u00e9 uma tabela frequentemente usada para descrever o desempenho de um modelo de classifica\u00e7\u00e3o (ou \"classificador\") em um conjunto de dados de teste pelos quais os valores verdadeiros s\u00e3o conhecidos.","c85f0cc7":"A prepara\u00e7\u00e3o de dados \u00e9 o ato de manipular e transformar dados brutos em uma forma que pode ser analisada por algoritmos de Deep Learning.\n\nPara este projeto vamos criar uma fun\u00e7\u00e3o que faz a leitura das imagens em uma matriz quadrada de tamanho 256 x 256 em greyscale, com o respectivo label indicando se a radiografia \u00e9 de um paciente NORMAL (0) ou com PNEUMONIA (1).\n","820c72e4":"Data augmentation \u00e9 uma estrat\u00e9gia que permite que se aumente significativamente a diversidade de dados dispon\u00edveis para treinamento de modelos de machine learning, sem realmente coletar novos dados. T\u00e9cnicas de data augmentation, como corte, preenchimento e invers\u00e3o horizontal, s\u00e3o comumente usadas para treinar grandes redes neurais.","30bc8846":"## Performance do Modelo","d5495bb0":"# Refer\u00eancias utilizadas","a0b1c156":"### Convolutional Neural Network","a765c53d":"Primeiramente iniciamos com o modelo de Rede Neural simples.","ea365e61":"## Treinamento dos Modelos","c383e1be":"Para finalizar dividimos as matrizes de features por 255 para normalizar os dados.","b51c38e9":"# An\u00e1lise Preditiva Avan\u00e7ada, Parte II (Deep Learning)\n\n## Trabalho em Grupo\n- **Curso:** FGV MBA - Business Analytics e Big Data\n- **Disciplina:** An\u00e1lise Preditiva Avan\u00e7ada\n- **Professor:** Gustavo Mirapalheta\n- **Tarefa:** Detec\u00e7\u00e3o de pneumonia em imagens de raio X\n\n## Alunos\n|Github|Kaggle|Nome|Matricula|E-mail|\n|---|---|---|---|---|\n|<a href=\"https:\/\/github.com\/DanielFCampos\"><img src=\"https:\/\/avatars2.githubusercontent.com\/u\/31582602?s=460&v=4\" title=\"DanielFCampos\" width=\"40\" height=\"40\"><\/a>|<a href=\"https:\/\/www.kaggle.com\/danielferrazcampos\"><img src=\"https:\/\/storage.googleapis.com\/kaggle-avatars\/images\/3508055-kg.png\" title=\"DanielFCampos\" width=\"40\" height=\"40\"><\/a>|Daniel Campos|A57635769|[daniel.ferraz.campos@gmail.com](daniel.ferraz.campos@gmail.com)|\n|<a href=\"https:\/\/github.com\/ldaniel\"><img src=\"https:\/\/avatars2.githubusercontent.com\/u\/205534?s=460&v=4\" title=\"ldaniel\" width=\"40\" height=\"40\"><\/a>|<a href=\"https:\/\/www.kaggle.com\/leandrodaniel\"><img src=\"https:\/\/storage.googleapis.com\/kaggle-avatars\/images\/3415144-gr.jpg\" title=\"ldaniel\" width=\"40\" height=\"40\"><\/a>|Leandro Daniel|A57622988|[contato@leandrodaniel.com](contato@leandrodaniel.com)|\n|<a href=\"https:\/\/github.com\/RodriGonca\"><img src=\"https:\/\/avatars2.githubusercontent.com\/u\/50252438?s=460&v=4\" title=\"RodriGonca\" width=\"40\" height=\"40\"><\/a>|<a href=\"https:\/\/www.kaggle.com\/rodrigonca\"><img src=\"https:\/\/storage.googleapis.com\/kaggle-avatars\/images\/3511253-kg.png\" title=\"RodriGonca\" width=\"40\" height=\"40\"><\/a>|Rodrigo Goncalves|A57566093|[rodrigo.goncalves@me.com](rodrigo.goncalves@me.com)|\n|<a href=\"https:\/\/github.com\/ygorlima1\"><img src=\"https:\/\/avatars2.githubusercontent.com\/u\/52429828?s=460&v=4\" title=\"ygorlima1\" width=\"40\" height=\"40\"><\/a>|<a href=\"https:\/\/www.kaggle.com\/ygorlima1\"><img src=\"https:\/\/storage.googleapis.com\/kaggle-avatars\/images\/3427786-kg.jpg\" title=\"ygorlima1\" width=\"40\" height=\"40\"><\/a>|Ygor Lima|A57549661|[ygor_redesocial@hotmail.com](ygor_redesocial@hotmail.com)|","42a95476":"### Receiver Operating Characteristic\n\n**Receiver operating characteristic** \u00e9 um gr\u00e1fico que mostra a capacidade de discrimina\u00e7\u00e3o de um modelo classificador bin\u00e1rio \u00e0 medida que seu limite de classifica\u00e7\u00e3o muda.\n\nObtemos esse gr\u00e1fico plotando a Taxa Positiva Verdadeira (TPR) contra a Taxa Positiva Falsa (FPR) em diferentes configura\u00e7\u00f5es de limite.\n\nQuanto maior a \u00c1rea sob a curva (AUC), melhor o modelo \u00e9 para classificar a observa\u00e7\u00e3o.","84fdd736":"### **Exemplos ilustrativos de raios X de t\u00f3rax em pacientes com pneumonia**\n\n![image.png](attachment:image.png)\n\n\nA radiografia normal do t\u00f3rax (painel esquerdo) mostra pulm\u00f5es limpos, sem \u00e1reas de opacifica\u00e7\u00e3o anormal na imagem. A pneumonia bacteriana (m\u00e9dia) geralmente exibe uma consolida\u00e7\u00e3o lobar focal, neste caso no lobo superior direito (setas brancas), enquanto a pneumonia viral (direita) se manifesta com um padr\u00e3o mais difuso de 'intersticial' em ambos os pulm\u00f5es.\n\n### **Estrutura dos Dados**\n\nO conjunto de dados est\u00e1 organizado em 3 pastas (Treino, Teste, Valida\u00e7\u00e3o) e cont\u00e9m subpastas para cada categoria de imagem (Pneumonia \/ Normal).\n\nExistem 5.883 imagens de raios-X (JPEG) e 2 categorias (Pneumonia \/ Normal).\n\nAs imagens de radiografia de t\u00f3rax (\u00e2ntero-posterior) foram selecionadas a partir de cortes de pacientes pedi\u00e1tricos de um a cinco anos no Centro M\u00e9dico de Mulheres e Crian\u00e7as de Guangzhou, Guangzhou. \n\nTodas as radiografias de t\u00f3rax foram realizadas como parte dos cuidados cl\u00ednicos de rotina dos pacientes.\n\nPara a an\u00e1lise de imagens de raio-x do t\u00f3rax, todas as imagens rastreadas para controle de qualidade, removendo todas as imagens de baixa qualidade ou ileg\u00edveis. \n\nOs diagn\u00f3sticos para as imagens foram classificados por dois m\u00e9dicos especialistas.","97312e5f":"Abaixo realizamos o reshape dos dados para utiliz\u00e1-los no treinamento do modelo.","cf2f9dbe":"## Escolha da An\u00e1lise e Abordagem do Grupo\n\nOptamos por desenvolver, em Python, uma Rede Neural Convolutiva (ou Convolucional) para classificar imagens de raio-x com ou sem pneumonia.\n\nDentro da \u00e1rea de estudo do Deep Learning, temos a chamada CNN (do ingl\u00eas, Convolutional Neural Network) como um tipo espec\u00edfico de rede neural normalmente utilizada para classifica\u00e7\u00e3o de imagens.\n\nUma imagem preta e branca (grayscale) \u00e9 representada como uma matrix 2D, em que cada posi\u00e7\u00e3o da matrix representa um pixel da imagem. Os valores para cada elemento variam entre 0 (preto) at\u00e9 255 (branco). J\u00e1 uma imagem colorida, \u00e9 normalmente representada por uma matrix 3D de forma que seja poss\u00edvel armazenar uma combina\u00e7\u00e3o das cores vermelho, verde e azul (RGB do ingl\u00eas). Neste contexto, uma CNN pode ser dividida em duas partes: extra\u00e7\u00e3o de caracter\u00edsticas (Conv, Padding, Relu, Pooling) e uma rede neural tradicional.\n\nMatematicamente, uma convolu\u00e7\u00e3o \u00e9 uma opera\u00e7\u00e3o linear que a partir de duas fun\u00e7\u00f5es, gera uma terceira (normalmente chamada de feature map). No contexto de imagens, podemos entender esse processo como um filtro\/kernel que transforma uma imagem de entrada.\n\nDurante o desenvolvimento deste trabalho, procuramos aplicar os conceitos de CNN a imagens de raios X de t\u00f3rax de pacientes com o objetivo de identificar a presen\u00e7a de pneumonia.","2be49df8":"# Desenvolvimento e An\u00e1lise\n\n## Descri\u00e7\u00e3o do Dataset Utilizado","650d57ca":"## Libraries Utilizadas para Desenvolvimento da An\u00e1lise","cb00afed":"# Conclus\u00e3o","e80f34b3":"**Trabalho em grupo de 3 a 4 alunos envolvendo t\u00e9cnicas de aprendizado supervisionado de m\u00e1quina com Deep learning.**\n\n### Poss\u00edveis trabalhos\n- Classifica\u00e7\u00e3o ou Previs\u00e3o, Dados Num\u00e9ricos ou Categ\u00f3ricos, Estilo de problema similar \u00e0s Regress\u00f5es Log\u00edstica ou Linear;\n- Principalmente no caso de processamento de texto em linguagem natural;\n- Classifica\u00e7\u00e3o de imagens com Redes Convolutivas;\n- Previs\u00e3o de Texto com Redes Sequenciais;\n- Gera\u00e7\u00e3o de conte\u00fado (m\u00fasica por exemplo) com redes Auto-Generativas;\n- Mix de estilos art\u00edsticos com Redes Convolutivas \/ Auto-Generativas.\n\n### Material a ser entregue\n- Se fizer em Python (recomendado): Jupyter Notebook com base e resultados (no caso de dados n\u00e3o submetidos \u00e0 confidencialidade) ou apenas o Jupyter Notebook (com algumas refer\u00eancias de dados que possam \"validar\" o modelo de rede neural entregue).\n- Se fizer em R: Entregar a base de dados e o Rmarkdown. As bibliotecas de machine ;learning mais utilizadas (scikit-learn, tensorflow e keras) est\u00e3o dispon\u00edveis tamb\u00e9m no R, por\u00e9m com um print-end. \u00c9 necess\u00e1rio instalar o Python para executalas.\n\n### Data de entrega: 25\/05","7a46ead1":"### Compilando e Visualizando a Arquitetura dos Modelos","e458552e":"Finalmente realizamos a separa\u00e7\u00e3o entre base de teste e treino, na propor\u00e7\u00e3o de 80% treinamento e 20% teste.","38cbe49c":"## Enunciado"}}