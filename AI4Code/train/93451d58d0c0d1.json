{"cell_type":{"374713be":"code","928af1d4":"code","1e9cb4d5":"code","817bbef1":"code","b3d1ba19":"code","c08b7d45":"code","a4bcc203":"code","dd29ecf1":"code","f46eda81":"code","29bfce64":"code","0c2521d8":"code","fa6157ac":"code","16165410":"code","54b33f37":"code","6a3c1e62":"code","08b47541":"code","11e0dc0b":"code","265641d0":"code","e4c490f5":"code","1b5127c8":"code","2fe1dad7":"code","ea5f438f":"code","4adac2b8":"code","7ebbb837":"code","f0046fd2":"code","73273a7f":"code","54023479":"code","b6c063c6":"code","3b6a3171":"code","c761ff60":"code","987cd8eb":"code","8a3502f0":"code","a5222aee":"code","89cee06a":"code","2766a327":"code","de9f215f":"code","7c57ea82":"code","2cfeeb73":"code","1c3458ca":"code","c21e8db7":"code","ca3fa6a8":"code","cb049b34":"code","4c2c437e":"code","d3c87b78":"code","a479bde0":"code","3431bc4e":"code","9e217bda":"code","b8fedce4":"code","ddc60c77":"code","f813f99e":"code","a05ea169":"code","ac319416":"code","f2b9b280":"code","4bc42208":"code","491135d9":"code","1f8a7518":"code","a864d44e":"code","873a6449":"code","ec7d566b":"code","1d3f07d6":"code","b74df9e8":"code","323db3e2":"code","795ca4ce":"code","64a91d05":"code","41401177":"code","9bb19189":"code","bbb6eeca":"code","0c0be9f3":"code","d1e40195":"code","44dedb6c":"code","0b66290c":"code","fae685c6":"code","8d24beda":"markdown","12f0d327":"markdown","2f55963f":"markdown","69b49d9a":"markdown","c4bef55c":"markdown","cb3f914a":"markdown","f635734d":"markdown","13470bf8":"markdown","e4b35433":"markdown","85ff4b44":"markdown","8c348384":"markdown","0a0715b5":"markdown","cc6d61b0":"markdown","abc45238":"markdown","743e598f":"markdown","2c4a5dda":"markdown","6d18b5c1":"markdown","cb8d293c":"markdown","912c84e7":"markdown","95fc289a":"markdown","6851ef51":"markdown","b20554b4":"markdown","faf18c85":"markdown","01781570":"markdown","01375d9d":"markdown","c7515113":"markdown","579dfb05":"markdown","2b4bc8b5":"markdown","33aca693":"markdown","f5528021":"markdown","2538c416":"markdown","8a9e9209":"markdown"},"source":{"374713be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport seaborn as sns\nimport os\nimport xgboost\nimport matplotlib.pyplot as plt\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","928af1d4":"df_train_o = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test_o = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","1e9cb4d5":"print(df_train_o.shape)\nprint(df_test_o.shape)\n# ~1460 rows with ~80 attributes ","817bbef1":"df_train_o.head()","b3d1ba19":"df_train_o.info()","c08b7d45":"def getNumCatFeatures(df_train_o):\n    numerical_feats = df_train_o.dtypes[df_train_o.dtypes != \"object\"].index\n    print(\"Number of Numerical features: \", len(numerical_feats))\n\n    categorical_feats = df_train_o.dtypes[df_train_o.dtypes == \"object\"].index\n    print(\"Number of Categorical features: \", len(categorical_feats))\n    \n    return numerical_feats, categorical_feats\n\nnumerical_feats, categorical_feats = getNumCatFeatures(df_train_o)","a4bcc203":"print(df_train_o[numerical_feats].columns)\nprint(\"*\"*100)\nprint(df_train_o[categorical_feats].columns)","dd29ecf1":"listplot = ['LotFrontage', 'BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF']\nfor col in listplot:\n    fig, ax = plt.subplots()\n    ax.scatter(x = df_train_o[col], y = df_train_o[\"SalePrice\"])\n    plt.xlabel(col)\n    plt.ylabel(\"SalePrice\")\n    plt.show()","f46eda81":"correlation_matrix = df_train_o.corr()\nplt.subplots(figsize = (20, 20))\nsns.heatmap(correlation_matrix, annot=True)","29bfce64":"correlation_matrix[\"SalePrice\"].sort_values(ascending=False)","0c2521d8":"data = []\nfor i in [\"GrLivArea\", \"GarageArea\", \"TotalBsmtSF\", \"1stFlrSF\", \"MasVnrArea\", \"LotFrontage\", \"WoodDeckSF\", \"OpenPorchSF\", \"2ndFlrSF\", \"LotArea\", \"ScreenPorch\"]:\n    print(i, correlation_matrix[\"SalePrice\"][i])\n    data.append(correlation_matrix[\"SalePrice\"][i])\n    \nplt.hlines(1,0,1)  # Draw a horizontal line\nplt.eventplot(data, orientation='horizontal', colors='b')\nplt.show()","fa6157ac":"sns.distplot(df_train_o['SalePrice'])","16165410":"df_train_o['SalePrice_Logged'] = np.log(df_train_o['SalePrice'])","54b33f37":"sns.distplot(df_train_o['SalePrice_Logged'])","6a3c1e62":"quantitative = [f for f in df_train_o.columns if df_train_o.dtypes[f] != 'object']\n#print(quantitative)\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nquantitative.remove('SalePrice_Logged')\n\nf = pd.melt(df_train_o, value_vars=quantitative)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\ng = g.map(sns.histplot, \"value\")","08b47541":"listSkew = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\nfor col in listSkew:\n    df_train_o[col+'_Logged'] = np.log(df_train_o[col])\n    df_train_o.drop(columns=[col])\nfor i in range(len(listSkew)):\n    listSkew[i] = listSkew[i]+'_Logged'\nquantitative = listSkew\nf = pd.melt(df_train_o, value_vars=quantitative)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\ng = g.map(sns.histplot, \"value\")","11e0dc0b":"def checkINFNULL(X):\n    for col in X.columns:\n        if (X[col].isnull().any()):\n            print('null')\n            break\n    print('no null')\n    print(np.isinf(X).values.sum())","265641d0":"for x in listSkew:\n    print(x)\n    print(np.isinf(df_train_o[x]).values.sum())","e4c490f5":"for col in df_train_o.columns:\n    nbr = df_train_o[col].isnull().sum()\n    if nbr\/1460 > 0.5:\n        print(f'{col}:{nbr\/df_train_o.shape[0]}')","1b5127c8":"df_train = df_train_o.drop(columns = ['Alley','PoolQC','Fence','MiscFeature','Id'])\ndf_test = df_test_o.drop(columns = ['Alley','PoolQC','Fence','MiscFeature'])","2fe1dad7":"cols_with_missing = [col for col in df_train.columns if col != 'SalePrice' and df_train[col].isnull().any()]\ncols_with_missing","ea5f438f":"def getColsWithMissingValue(df):\n    cols_with_missing = [col for col in df.columns if col != 'SalePrice' and df[col].isnull().any()]\n    cols_with_missing_num = []\n    cols_with_missing_cat = []\n    if len(cols_with_missing) == 0:\n        print(f'There is no null\/NA in this df')\n        return cols_with_missing_num, cols_with_missing_cat\n    for col in cols_with_missing:\n        #print(df[col].dtypes)\n        if df[col].dtypes != object:\n            cols_with_missing_num.append(col)\n            print(f'mean of {col}:{df[col].mean()}')\n            print(f'median of {col}:{df[col].median()}')\n            print(f'mode of {col}:{df[col].mode()}')\n        else:\n            cols_with_missing_cat.append(col)\n            print(f'Value counts of {col}:\\n{df[col].value_counts()}')\n    return cols_with_missing_num, cols_with_missing_cat\ncols_with_missing_num, cols_with_missing_cat = getColsWithMissingValue(df_train)","4adac2b8":"sns.distplot(df_train['MasVnrArea'])","7ebbb837":"df_train['CentralAir'].replace({'N':0, 'Y':1}, inplace=True)\ndf_test['CentralAir'].replace({'N':0, 'Y':1}, inplace=True)","f0046fd2":"def gradingconverter(df, listToBeConverted, quality):\n    for i in listToBeConverted:\n        df[i].replace(quality, inplace=True)\n    return df\n\nquality = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1}\nx = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual', 'GarageQual','GarageCond', 'FireplaceQu']\ndf_train = gradingconverter(df_train, x, quality)\ndf_test = gradingconverter(df_test, x, quality)\n\n\nquality2 = {'Gd':4, 'Av':3, 'Mn':2, 'No':0}\nx = ['BsmtExposure']\ndf_train = gradingconverter(df_train, x, quality2)\ndf_test = gradingconverter(df_test, x, quality2)","73273a7f":"for col in cols_with_missing_num:\n    print(f'mean of {col}:\\n{df_train[col].mean()}')\n    print(f'median of {col}:\\n{df_train[col].median()}')\n    print(f'mode of {col}:\\n{df_train[col].mode()}')","54023479":"numerical_cols, categorical_cols = getNumCatFeatures(df_train)","b6c063c6":"num_cols_with_missing, cat_cols_with_missing = getColsWithMissingValue(df_train)","3b6a3171":"num_cols_with_missing_test, cat_cols_with_missing_test = getColsWithMissingValue(df_test)","c761ff60":"def replaceNA_with_Median(df, num_cols_with_missing):\n    for col in num_cols_with_missing:\n        median = df[col].median()\n        df[col].fillna(value=median, inplace=True)\n        df[col+'was_missing'] = df[col].isnull().astype(int)\n    return df\n\ndf_train = replaceNA_with_Median(df_train, num_cols_with_missing)\ndf_test = replaceNA_with_Median(df_test, num_cols_with_missing_test)","987cd8eb":"num_cols_with_missing, cat_cols_with_missing = getColsWithMissingValue(df_train)","8a3502f0":"num_cols_with_missing","a5222aee":"num_cols_with_missing_test, cat_cols_with_missing_test = getColsWithMissingValue(df_test)","89cee06a":"num_cols_with_missing_test","2766a327":"df_train.shape","de9f215f":"df_test.shape","7c57ea82":"cat_cols_with_missing","2cfeeb73":"for col in cat_cols_with_missing:\n    print(f'{col}:\\n{df_train[col].value_counts()}')","1c3458ca":"sumofnullrow = 0\nfor col in cat_cols_with_missing:\n    nbr = df_train_o[col].isnull().sum()\n    print(f'{col}:{nbr\/df_train_o.shape[0]}')\n    sumofnullrow += nbr","c21e8db7":"sumofnullrow","ca3fa6a8":"def replaceNA_with_0(df, cat_cols_with_missing): # for categorical features only\n    for col in cat_cols_with_missing:\n        df[col].fillna(value='0', inplace=True)\n        df[col+'was_missing'] = df[col].isnull().astype(int)\n    return df\n\ndf_train = replaceNA_with_0(df_train, cat_cols_with_missing)\ndf_test = replaceNA_with_0(df_test, cat_cols_with_missing_test)","cb049b34":"num_cols_with_missing, cat_cols_with_missing = getColsWithMissingValue(df_train)","4c2c437e":"num_cols_with_missing_test, cat_cols_with_missing_test = getColsWithMissingValue(df_test)","d3c87b78":"nums, cats = getNumCatFeatures(df_train)\nnums_test, cats_test = getNumCatFeatures(df_test)","a479bde0":"cats","3431bc4e":"cats_test","9e217bda":"def transformCatToNum_FrequencyEncoding(df, cats):\n    for col in cats:\n        x = df[col].value_counts()\n        xdict = x.to_dict()\n        #print(xdict)\n        df[col].replace(xdict, inplace=True)\n    return df\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndef transformCatToNum_LabelEncoding(df, cats):\n    for col in cats:\n        df[col] = le.fit_transform(df[col])\n    return df\n\n# Change below for different encoding\ndf_train = transformCatToNum_FrequencyEncoding(df_train, cats)\ndf_test = transformCatToNum_FrequencyEncoding(df_test, cats_test)\n","b8fedce4":"nums, cats = getNumCatFeatures(df_train)","ddc60c77":"nums_test, cats_test = getNumCatFeatures(df_test)","f813f99e":"np.setdiff1d(df_train.columns, df_test.columns)","a05ea169":"np.setdiff1d(df_test.columns, df_train.columns)","ac319416":"df_train.head()","f2b9b280":"np.isinf(df_train).any().sum()","4bc42208":"from sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nfrom matplotlib import pyplot as plt\nfrom xgboost import XGBRegressor\nfrom sklearn.feature_selection import mutual_info_regression, f_regression","491135d9":"df_train.filter(regex='Logged',axis=1).columns","1f8a7518":"X = df_train.loc[:, df_train.columns != 'SalePrice_Logged']\nX = X.loc[:, X.columns != 'SalePrice']\ny = df_train['SalePrice']\ny_logged = df_train['SalePrice_Logged']","a864d44e":"checkINFNULL(X)","873a6449":"xgb = XGBRegressor(n_estimators=100)\nxgb.fit(X, y_logged) # Using logged y\nsorted_idx = xgb.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], xgb.feature_importances_[sorted_idx])\nplt.xlabel(\"Xgboost Feature Importance\")\nplt.figure(dpi=120, figsize=(8, 20))","ec7d566b":"X.columns[sorted_idx]","1d3f07d6":"xgb.feature_importances_[sorted_idx]","b74df9e8":"# get features > 0\ndef extractFeatures(X, threshold, xgb):\n    \n    nbrFeat = 0\n    FeatExtracted = []\n    x = 0\n    for i in X.columns[sorted_idx]:\n        if xgb.feature_importances_[sorted_idx][x] > threshold:\n            #print(i)\n            FeatExtracted.append(i)\n            nbrFeat += 1\n        x += 1\n    print(f'no. of features ori:{X.shape[1]}')\n    print(f'no. of features:{nbrFeat}')\n    return FeatExtracted\nFeatExtractedXGB = extractFeatures(X, 0.001, xgb)","323db3e2":"sns.distplot(xgb.feature_importances_[sorted_idx])","795ca4ce":"X1 = X[FeatExtractedXGB] # features by XGB\ndf_test = df_test[FeatExtractedXGB] # df_test","64a91d05":"print(X1.shape)\nprint(df_test.shape)","41401177":"X_train, X_test, y_train, y_test = train_test_split(X1, y_logged, test_size=0.25, random_state=12)","9bb19189":"xgb_params = dict(\n    max_depth=7,                     # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,              # effect of each tree - try 0.0001 to 0.1\n    n_estimators=900,                # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,              # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.4,            # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.4,                   # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.09888,               # L1 regularization (like LASSO) - try 0.0 to 10.0\n    num_parallel_tree=7,             # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)","bbb6eeca":"xgb.fit(X_train, y_train)\npredictions = (xgb.predict(X_test)) \n#mean_squared_error(y_test,predictions, squared=False)                 # Run this if y is not logged\nmean_squared_error(np.exp(y_test), np.exp(predictions), squared=False) # Run this if y is logged","0c0be9f3":"r2_score(y_test, predictions)","d1e40195":"df_test.head()","44dedb6c":"df_test.shape","0b66290c":"import joblib\ndef PredictionPipeline(model, X, y, df_test, filename, modelname, df_test_o=df_test_o):\n    model.fit(X, y)\n    predictions = np.exp(model.predict(df_test))\n    df_submission = pd.DataFrame()\n    df_submission['Id'] = df_test_o['Id']\n    df_submission['SalePrice'] = predictions\n    df_submission.to_csv(filename, index=False)\n    try:\n        model.save_model(modelname)\n    except:\n        joblib.dump(model, modelname+'.joblib')\n    print(\"model and prediction are saved\")","fae685c6":"PredictionPipeline(xgb, X1, y_logged, df_test, \"df_submission_final3.csv\", \"XGBmodel_final3.json\") ","8d24beda":"#### df_test","12f0d327":"# Data preprocessing","2f55963f":"# Model building","69b49d9a":"### Now train the model with the extracted features","c4bef55c":"#### Include the features with score > 0","cb3f914a":"#### Now all categorical data has transformed to numerical","f635734d":"#### Convert categorical features to numerical","13470bf8":"#### Now handle categorical features with NA","e4b35433":"# Prediction","85ff4b44":"#### Transform some features to normal distribution","8c348384":"#### df_train","0a0715b5":"#### For df_train and df_test","cc6d61b0":"#### Number of null in each col in df","abc45238":"#### Transform to normal distribution ","743e598f":"#### new features are added","2c4a5dda":"#### Replace NA with median in numerical features","6d18b5c1":"#### XGB model","cb8d293c":"### Now transforming categorical to numerical by Frequency encoding","912c84e7":"# Data preprocessing","95fc289a":"Get the columns with missing value","6851ef51":"### Correlation matrix","b20554b4":"#### Import libraries for model and eval","faf18c85":"### XGBoost\n#### provides a XGBRegressor and permutation_importance that can find important feature\nhttps:\/\/mljar.com\/blog\/feature-importance-xgboost\/","01781570":"#### Now deal with NA","01375d9d":"#### Features above can be removed from training because almost all are missing","c7515113":"# EDA","579dfb05":"Convert quality except 'NA'","2b4bc8b5":"#### Feature selection","33aca693":"#### Fill nan with 0","f5528021":"#### All NA in numerical features are filled with their median","2538c416":"#### np.exp() to tranform back to real price","8a9e9209":"#### Fill null value for numerical features with its mode\/median\/mean which depends on distribution"}}