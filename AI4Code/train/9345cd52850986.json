{"cell_type":{"82f8463b":"code","e3368d3f":"code","497b21d8":"code","8d203908":"code","5ba982de":"code","f3b08a56":"code","d8e9dfca":"code","8bb3e80d":"code","8aefc368":"code","1f375403":"code","aba1c0c7":"code","fc3b1980":"code","de139fb3":"code","2a5ee848":"code","7349d70f":"code","97d3b3fb":"code","a51947cd":"code","f5575662":"code","6dd36316":"code","61bc89e3":"code","238279e8":"code","6edd11db":"code","42d008d6":"code","3bba123f":"code","4e755dda":"code","9ecb8ff4":"code","87936a29":"code","aadf0b2b":"code","85549deb":"code","647c5f1a":"code","1499307e":"code","402de802":"code","c1c91f4b":"code","1a2dc676":"code","da337550":"code","0a389d3b":"code","2325eb8c":"code","33df12af":"code","91181085":"code","017c36ed":"code","79315a71":"code","e4ba7eeb":"code","ae3ca54d":"code","a4595e05":"markdown","60e89c1f":"markdown","f45c2b4a":"markdown","3a5817a6":"markdown","b0d268b7":"markdown","06d09d4a":"markdown","afdb0506":"markdown","b936b1db":"markdown","7b1702ad":"markdown","6ee85a95":"markdown","54407424":"markdown"},"source":{"82f8463b":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, classification_report\nimport matplotlib\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nzomato = pd.read_csv(\"..\/input\/zomato.csv\", na_values = [\"-\", \"\"])\n# Making a copy of the data to work on\ndata = zomato.copy()\n","e3368d3f":"# Renaming and removing commas in the cost column \ndata = data.rename({\"approx_cost(for two people)\": \"cost\"}, axis=1)\ndata[\"cost\"] = data[\"cost\"].replace(\",\", \"\", regex = True)","497b21d8":"# Converting numeric columns to their appropriate dtypes\ndata[[\"votes\", \"cost\"]] = data[[\"votes\", \"cost\"]].apply(pd.to_numeric)","8d203908":"# Group and aggregate duplicate restaurants that are listed under multiple types in listed_in(type)\ngrouped = data.groupby([\"name\", \"address\"]).agg({\"listed_in(type)\" : list})\nnewdata = pd.merge(grouped, data, on = ([\"name\", \"address\"]))","5ba982de":"# Drop rows which have duplicate information in \"name\", \"address\" and \"listed_in(type)_x\"\nnewdata[\"listed_in(type)_x\"] = newdata[\"listed_in(type)_x\"].astype(str) # converting unhashable list to a hashable type\nnewdata.drop_duplicates(subset = [\"name\", \"address\", \"listed_in(type)_x\"], inplace = True)\n","f3b08a56":"# Converting the restaurant names to rownames \nnewdata.index = newdata[\"name\"]","d8e9dfca":"# Dropping unnecessary columns\nnewdata.drop([\"name\", \"url\", \"phone\", \"listed_in(city)\", \"listed_in(type)_x\", \"address\", \"dish_liked\",  \"listed_in(type)_y\", \"menu_item\", \"cuisines\", \"reviews_list\"], axis = 1, inplace = True)","8bb3e80d":"# Transforming the target (restaurant ratings)\n\n# Extracting the first three characters of each string in \"rate\"\nnewdata[\"rating\"] = newdata[\"rate\"].str[:3] \n# Removing rows with \"NEW\" in ratings as it is not a predictable level\nnewdata = newdata[newdata.rating != \"NEW\"] \n# Dropping rows that have missing values in ratings \nnewdata = newdata.dropna(subset = [\"rating\"])\n# Converting ratings to a numeric column so we can discretize it\nnewdata[\"rating\"] = pd.to_numeric(newdata[\"rating\"])","8aefc368":"# Discretizing the ratings into a categorical feature with 4 classes\nnewdata[\"rating\"] = pd.cut(newdata[\"rating\"], bins = [0, 3.0, 3.5, 4.0, 5.0], labels = [\"0\", \"1\", \"2\", \"3\"])","1f375403":"# Checking the number of restaurants in each rating class\nnp.unique(newdata[\"rating\"], return_counts = True)","aba1c0c7":"# Dropping the original rating column\nnewdata.drop(\"rate\", axis = 1, inplace = True)","fc3b1980":"newdata.describe(include = \"all\")","de139fb3":"# Separating the predictors and target\npredictors = newdata.drop(\"rating\", axis = 1)\ntarget = newdata[\"rating\"]","2a5ee848":"# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(predictors, target, random_state = 0, test_size = 0.3)","7349d70f":"# Preprocessing the predictors\nnum_cols = [\"votes\", \"cost\"]\ncat_cols = [\"location\", \"rest_type\", \"online_order\", \"book_table\"]\n\nnum_imputer = SimpleImputer(strategy = \"median\")\n# Imputing numeric columns with the median (not mean because of the high variance)\nnum_imputed = num_imputer.fit_transform(X_train[num_cols])\nscaler = StandardScaler()\n# Scaling the numeric columns to have a mean of 0 and standard deviation of 1\nnum_preprocessed = pd.DataFrame(scaler.fit_transform(num_imputed), columns = num_cols)\n\ncat_imputer = SimpleImputer( strategy = \"most_frequent\")\n# Imputing categorical columns with the mode\ncat_imputed = pd.DataFrame(cat_imputer.fit_transform(X_train[cat_cols]), columns = cat_cols)\n# Dummifying the categorical columns\ncat_preprocessed = pd.DataFrame(pd.get_dummies(cat_imputed, prefix = cat_cols, drop_first = True))\n\ntrain_predictors = pd.concat([num_preprocessed, cat_preprocessed], axis=1)","97d3b3fb":"test_num_imputed = num_imputer.transform(X_test[num_cols])\ntest_num_preprocessed = pd.DataFrame(scaler.transform(test_num_imputed), columns = num_cols)\n\ntest_cat_imputed = pd.DataFrame(cat_imputer.transform(X_test[cat_cols]), columns = cat_cols)\ntest_cat_preprocessed = pd.DataFrame(pd.get_dummies(test_cat_imputed, prefix = cat_cols, drop_first = True))\n                                    \ntest_predictors = pd.concat([test_num_preprocessed, test_cat_preprocessed], axis=1)\n\n# Accounting for missing columns in the test set caused by dummification\nmissing_cols = set(train_predictors) - set(test_predictors)\n# Add missing columns to test set with default value equal to 0\nfor c in missing_cols:\n    test_predictors[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\ntest_predictors = test_predictors[train_predictors.columns]","a51947cd":"dt = DecisionTreeClassifier()\ndt.fit(train_predictors, y_train)\npred_train = dt.predict(train_predictors)\npred_test = dt.predict(test_predictors)","f5575662":"accuracy_score(y_train, pred_train)","6dd36316":"accuracy_score(y_test, pred_test)","61bc89e3":"rf = RandomForestClassifier(criterion = \"gini\", n_estimators = 250, max_depth = 10, \n                            max_features = 50, min_samples_split = 4, random_state = 0)\nrf.fit(train_predictors, y_train)\npred_train = rf.predict(train_predictors)\npred_test = rf.predict(test_predictors)","238279e8":"accuracy_score(y_train, pred_train)","6edd11db":"accuracy_score(y_test, pred_test)","42d008d6":"cohen_kappa_score(y_train, pred_train)","3bba123f":"cohen_kappa_score(y_test, pred_test)","4e755dda":"print(classification_report(y_train, pred_train))","9ecb8ff4":"print(classification_report(y_test, pred_test))","87936a29":"# Inspecting class counts in the train predictions\nnp.unique(pred_train, return_counts = True)","aadf0b2b":"# Doing the same for the test predictions\nnp.unique(pred_test, return_counts = True)","85549deb":"rf = RandomForestClassifier(criterion = \"gini\", n_estimators = 250, max_depth = 10, \n                            max_features = 50, min_samples_split = 4, random_state = 0,\n                           class_weight = \"balanced\")\nrf.fit(train_predictors, y_train)\npred_train = rf.predict(train_predictors)\npred_test = rf.predict(test_predictors)","647c5f1a":"# Inspecting class counts in the train predictions\nnp.unique(pred_train, return_counts = True)","1499307e":"# Doing the same for the test predictions\nnp.unique(pred_test, return_counts = True)","402de802":"np.unique(y_train, return_counts = True)","c1c91f4b":"np.unique(y_test, return_counts = True)","1a2dc676":"# Building an XGBoost classifier\nxgb = XGBClassifier(n_estimators = 250, max_depth = 20, gamma = 2, learning_rate = 0.001, random_state = 0)\n\nxgb.fit(train_predictors, y_train)\npred_train = xgb.predict(train_predictors)\npred_test = xgb.predict(test_predictors)","da337550":"accuracy_score(y_train, pred_train)","0a389d3b":"accuracy_score(y_test, pred_test)","2325eb8c":"cohen_kappa_score(y_train, pred_train)","33df12af":"cohen_kappa_score(y_test, pred_test)","91181085":"print(classification_report(y_train, pred_train))","017c36ed":"print(classification_report(y_test, pred_test))","79315a71":"# Inspecting class counts in the train predictions\nnp.unique(pred_train, return_counts = True)","e4ba7eeb":"# Doing the same for the test predictions\nnp.unique(pred_test, return_counts = True)","ae3ca54d":"# Visualizing a feature importances plot\n\nplt.figure(figsize = (20, 10))\nfeat_importances = pd.Series(xgb.feature_importances_, index=train_predictors.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","a4595e05":"This is Part Two of my analysis of the Zomato Bangalore dataset. In Part One we attempted to predict restaurant ratings with six selected features using regression. In this kernel we will keep the same features, but transform the ratings into a categorical target with four levels and build classification models to predict them. \n\nExploratory Data Analysis was done in Part One. This kernel consists of:\n\n- Data cleaning (identifying and dropping duplicates, reformatting features)\n- Preprocessing and prediction with Decision Tree, Random Forest and XGBoost \n- Model evaluation (Accuracy, Cohen Kappa, F1 score, Precision, Recall)\n- Feature Importance visualization\n- Results summary\n\nWe will go through the data cleaning and preprocessing quickly - see Part One for explanations.","60e89c1f":"## Model evaluation","f45c2b4a":"## Results summary\n\nIn this kernel we split the restaurant ratings into four ranges (classes) and built tree-based classifiers to predict them. **The best performer was a manually tuned XGBoost classifier with 72% accuracy on train and 67% accuracy on test.** But since the rating classes are somewhat imbalanced, we also evaluated our predictions with Cohen Kappa and F1 scores.\n\nThe Cohen Kappa score was 0.56 on train and 0.47 on test. This can be interpreted as the model **performing moderately well** (according to Landis & Koch). Average weighted F1 score was 0.69 and 0.64 on train and test respectively. \n\nRandom Forest also gave similar scores but the class counts in RF predictions showed that minority classes were being misclassified, i.e. almost all restaurants with \"rare\" ratings were being misclassified as having a more \"common\" rating. To counter this, we added the parameter (class_weight = \"balanced\") to the RF. This made the classifier overcompensate for the minority classes (incorrectly classified too many restaurants into the minority classes) and brought down the scores. \n\nXGBoost with manually tuned depth, learning rate and gamma (regularization) hyperparameters predicted more minority samples correctly than RF. **The most important feature for prediction was Votes, followed by rest_type_Dessert_Parlor. **\n\nIn Part Three of my analysis we will apply text mining techniques to extract insights from textual features, like customer reviews, and attempt rating classification with a neural network.","3a5817a6":"## Data Cleaning and Preprocessing","b0d268b7":"Our EDA in Part One showed that 3.7 is the most common rating and the frequency of ratings below 2.5 and above 4 is very low. To prevent a class imbalance problem, we will create custom-sized bins that take frequency counts into account while still making sense.\n\nOur four rating bins (classes) will be 0 to 3 < 3 to 3.5 < 3.5 to 4 < 4 to 5. To make label encoding easier later, we'll label these classes 0, 1, 2, 3. **We can think of these as Very Low, Low, Medium and High.**","06d09d4a":"Bagging may not be enough for this classification task. Let's try boosting with XGBoost.","afdb0506":"### Observation\n\nA basic decision tree is overfitting on the train data. Let's see if a Random Forest ensemble model can do better.\n\nBelow are the results of Random Forest after manual hyperparameter tuning.","b936b1db":"The RF classifier has not predicted any samples for the minority class (0) in the test data, which means it has not learnt that class. \n\nLet's rebuild the Random Forest with a class weight parameter to handle class imbalance.","7b1702ad":"We have 884 restaurants in rating class 0 (Very Low), 2929 in class 1 (Low), 4037 in class 2 (Medium) and 1466 in class 4 (High).","6ee85a95":"Let's see which features our classifier found most important for rating class prediction.","54407424":"The new RF is assigning a **huge** number of samples to the minority class!\n\nHere are the actual class counts for train and test:"}}