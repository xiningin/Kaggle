{"cell_type":{"a50dcc87":"code","ef1a5d0d":"code","0fc1090c":"code","f938302c":"code","48fa7ec4":"code","9cdb1a2d":"code","f7607c2d":"code","34a3e52e":"code","669fcd64":"code","189f201d":"code","188eb28c":"code","067cdd55":"code","aec3c68c":"code","2cd4cdc4":"code","3520df8a":"code","550cd5e7":"code","5998160e":"code","e03a9b48":"code","ec75e33e":"code","a3f2734d":"code","8ca0c34e":"code","268a9f5c":"code","acace3de":"code","4650842e":"code","9912cf88":"code","8e46eb42":"code","03b716bb":"code","58878210":"code","8858bbab":"code","2595000b":"code","c4ff7136":"code","3d85c656":"code","aebde3a0":"code","28565f36":"code","c6b18c56":"code","0e285060":"code","a947041a":"markdown","5d754ab3":"markdown","4691b805":"markdown","185aef87":"markdown","7322ffde":"markdown","c3ba5159":"markdown","adc1252b":"markdown","42526ac4":"markdown","d4f39d5f":"markdown","15c493c3":"markdown","5b33f406":"markdown","e6c9c594":"markdown","4bbc904f":"markdown","090ce2b4":"markdown","a9903623":"markdown","a7f8b74d":"markdown","09d5ac3a":"markdown","a050b7b2":"markdown","7c8de3fa":"markdown","43b4e88d":"markdown","126fa8d7":"markdown","713dd377":"markdown","9c95c4c2":"markdown","a896980b":"markdown","bcdc3c9f":"markdown","723c8c05":"markdown","39158f14":"markdown","b18cce87":"markdown","025c0f2e":"markdown","1d5c05b9":"markdown","93b20735":"markdown","556b6006":"markdown"},"source":{"a50dcc87":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef1a5d0d":"df= pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")","0fc1090c":"df.head()","f938302c":"df.info()","48fa7ec4":"df['Potability']=df['Potability'].astype('category')","9cdb1a2d":"#create approve limit for each features based on data available in Google search\ncols=df.columns[0:9].to_list()\nmin_val=[6.52,0,500,0,3,0,0,0,0]\nmax_val=[6.83,0,1000,4,250,400,2,80,5]\nlimit=pd.DataFrame(data=[min_val, max_val], columns=cols)","f7607c2d":"df.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='PuBu')","34a3e52e":"#Portability is 1 - means good for Human\ndf[df['Potability']==1].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='PuBu')\n","669fcd64":"# Portability is 0 - means not good for Human\ndf[df['Potability']==0].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdBu')","189f201d":"df.isnull().sum()","188eb28c":"df[df['Sulfate'].isnull()]\ndf[df['ph'].isnull()]\ndf[df['Trihalomethanes'].isnull()]","067cdd55":"#Replace null values based on the group\/sample mean\ndf['ph']=df['ph'].fillna(df.groupby(['Potability'])['ph'].transform('mean'))\ndf['Sulfate']=df['Sulfate'].fillna(df.groupby(['Potability'])['Sulfate'].transform('mean'))\ndf['Trihalomethanes']=df['Trihalomethanes'].fillna(df.groupby(['Potability'])['Trihalomethanes'].transform('mean'))","aec3c68c":"df.isna().sum()","2cd4cdc4":"#Import ploting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots \ncolors = ['#06344d','#00b2ff']\nsns.set(palette=colors, font='Serif', style='white', rc={'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})\nsns.palplot(colors)","3520df8a":"#Lets check the Target features first\nfig = plt.figure(figsize=(10,6))\nax=sns.countplot(data=df, x='Potability')\nfor i in ax.patches:\n    ax.text(x=i.get_x()+i.get_width()\/2, y=i.get_height()\/7, s=f\"{np.round(i.get_height()\/len(df)*100,0)}%\", ha='center', size=50, weight='bold', rotation=90, color='white')\nplt.title(\"Potability Feature\", size=20, weight='bold')\nplt.annotate(text=\"Not safe for Human consumption\", xytext=(0.5,1750),xy=(0.2,1250), arrowprops =dict(arrowstyle=\"->\", color='black', connectionstyle=\"angle3,angleA=0,angleB=90\"), color='black')\nplt.annotate(text=\"Safe for Human consumption\", xytext=(0.8,1500),xy=(1.2,1000), arrowprops =dict(arrowstyle=\"->\", color='black',  connectionstyle=\"angle3,angleA=0,angleB=90\"), color='black')\n","550cd5e7":"limit","5998160e":"from matplotlib.patches import Rectangle\nint_cols = df.select_dtypes(exclude=['category']).columns.to_list()\nfig, ax= plt.subplots(nrows=3,ncols=3,figsize=(15,15), constrained_layout=True)\nplt.suptitle('Feature distribution by Potability class and Approved limit', size=20, weight='bold')\nax=ax.flatten()\nfor x, i in enumerate(int_cols):\n    sns.kdeplot(data=df, x=i, hue='Potability', ax=ax[x], fill=True, multiple='stack', alpha=0.5, linewidth=2)\n    l,k = limit.iloc[:,x]\n    ax[x].add_patch(Rectangle(xy=(l,0), width=k-l, height=1, alpha=0.5))\n    for s in ['left','right','top','bottom']:\n        ax[x].spines[s].set_visible(False)","e03a9b48":"fig, ax= plt.subplots(nrows=3,ncols=3,figsize=(15,15), constrained_layout=True)\nplt.suptitle('Feature distribution by Potability class and Approved limit', size=20, weight='bold')\nax=ax.flatten()\nfor x, i in enumerate(int_cols):\n    sns.boxplot(data=df, y=i, x='Potability', ax=ax[x])\n    #l,k = limit.iloc[:,x]\n    #ax[x].add_patch(Rectangle(xy=(l,0), width=k-l, height=1, alpha=0.5))\n    for s in ['left','right','top','bottom']:\n        ax[x].spines[s].set_visible(False)","ec75e33e":"from scipy.stats import ttest_ind\np_val=[]\nfor i in int_cols:\n    pota_1 = df[df['Potability']==1][i]\n    pota_0 = df[df['Potability']==0][i]\n    stat, p_value=ttest_ind(pota_1, pota_0)\n    p_val.append(np.round(p_value,3))\n    if p_value <0.1:\n        print(f\"p_value for {i} is {p_value} is less than significant value 0.1, so we have no enough evidance to prove Null Hypothesis. so we reject the Null Hypotesis\")\n    else:\n        print(f\"p_value for {i} is {p_value} we accept the null hypothesis\")\n\nstats_test=pd.DataFrame(columns=['columns','p_value'])\nstats_test['columns']=int_cols\nstats_test['p_value']=p_val\nstats_test.sort_values(by=['p_value'], ascending=True, inplace=True)\n\nfig=plt.figure(figsize=(15,8))\nax=sns.barplot(data=stats_test, x='columns',y='p_value')\nplt.title(\"Features and p_value based on t-Test\", size=20, weight='bold')\nfor i in ax.patches:\n    ax.text(x=i.get_x()+0.5, y=i.get_height(), s=i.get_height(), rotation=90)\nax.axhline(y=0.1, color='red', ls='--')\nax.text(x=6, y=0.12, s=\"Significance level: 0.1\")","a3f2734d":"sns.pairplot(df, hue='Potability', kind='reg')","8ca0c34e":"fig, ax=plt.subplots(nrows=1, ncols=2, figsize=(15,8))\nplt.suptitle(\"Co-relation Matrics\", size=20, weight='bold')\nax=ax.flatten()\nsns.heatmap(df[df['Potability']==1].corr(), annot=True, square=True, fmt='.2f', ax=ax[0], cbar=False)\nsns.heatmap(df[df['Potability']==0].corr(), annot=True, square=True, fmt='.2f', ax=ax[1], cbar=False)","268a9f5c":"fig=plt.figure(figsize=(8,8))\nsns.heatmap(df.corr(), annot=True, fmt='0.2f', square=True)","acace3de":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nX = df.drop(['Potability'], axis=1)\ny=df['Potability']\n\nscale = StandardScaler()\nX_scaled = scale.fit_transform(X)\ndecom = PCA(svd_solver='auto') #let try with auto rather than defining the components\ndecom.fit(X_scaled)\nex_var=np.cumsum(np.round(decom.explained_variance_ratio_,2))*100\nsns.lineplot(y=ex_var, x=np.arange(0,len(ex_var)))\nsns.scatterplot(y=ex_var, x=np.arange(0,len(ex_var)))","4650842e":"#handling imbalance in data and Scaling\nfrom imblearn.over_sampling import SMOTE\nsamp = SMOTE()\nX=df.drop(['Potability'], axis=1)\ny=df['Potability']\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nX_train,X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nX_train, y_train =samp.fit_resample(X_train,y_train)\n\nscale = StandardScaler()\nX_train=scale.fit_transform(X_train)\nX_test=scale.transform(X_test)","9912cf88":"from yellowbrick.classifier import ROCAUC\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nmod = []\ncv_score=[]\nmodel =[AdaBoostClassifier(), BaggingClassifier(), GradientBoostingClassifier(), DecisionTreeClassifier(), ExtraTreeClassifier(), KNeighborsClassifier()]\nfor m in model:\n    cv_score.append(cross_val_score(m, X_train, y_train, scoring='accuracy', cv=5).mean())\n    mod.append(m)\nmodel_df=pd.DataFrame(columns=['model','cv_score'])\nmodel_df['model']=mod\nmodel_df['cv_score']=cv_score\nmodel_df.sort_values(by=['cv_score'], ascending=True).style.background_gradient(subset=['cv_score'])","8e46eb42":"param={'n_estimators': [60,70,80,100,200,300,400,500,600,700]}\ngrid_Grd=GridSearchCV(GradientBoostingClassifier(), param_grid=param, cv=5, scoring='accuracy')\ngrid_Grd.fit(X_train, y_train)\nprint(f\"Best Estimator: {grid_Grd.best_params_} , Best Score : {grid_Grd.best_score_}\")\n\n\nparam={'n_estimators': [60,70,80,100,200,300,400,500,600,700]}\ngrid_Bag=GridSearchCV(BaggingClassifier(), param_grid=param, cv=5, scoring='accuracy')\ngrid_Bag.fit(X_train, y_train)\nprint(f\"Best Estimator: {grid_Bag.best_params_} , Best Score : {grid_Bag.best_score_}\")\n","03b716bb":"from sklearn.metrics import classification_report, confusion_matrix\nmodel = GradientBoostingClassifier(n_estimators=300)\nmodel.fit(X_train,y_train)\npred = model.predict(X_test)\nprint(classification_report(y_test, pred))\nsns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='.2f')","58878210":"from sklearn.metrics import classification_report, confusion_matrix\nmodel = BaggingClassifier(n_estimators=80)\nmodel.fit(X_train,y_train)\npred = model.predict(X_test)\nprint(classification_report(y_test, pred))\nsns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='.2f')","8858bbab":"model.predict_proba(X_test)","2595000b":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init(max_mem_size='2G')","c4ff7136":"h2o_df = h2o.H2OFrame(df)\nh2o_df['Potability']=h2o_df['Potability'].asfactor()\nX=h2o_df.columns[0:-1]\ny=h2o_df.columns[-1]","3d85c656":"train, test=h2o_df.split_frame(ratios=[.7])\nprint(train.nrows)\nprint(test.nrows)","aebde3a0":"aml = H2OAutoML(balance_classes=True)\naml.train(x=X, y=y, training_frame=train)","28565f36":"aml.leaderboard","c6b18c56":"type(test['Potability'])","0e285060":"pred = aml.leader.predict(test)\ny_val = h2o.as_list(test['Potability'], use_pandas=True)\npred_val = h2o.as_list(pred['predict'], use_pandas=True)\nprint(classification_report(y_val,pred_val))","a947041a":"# Initial Analysis","5d754ab3":"### Try prediciting Test data send and find the accuracy of the model","4691b805":"## Try with H2o AutoML","185aef87":"# Exploratory Data Analysis","7322ffde":"<img src=\"https:\/\/images.outlookindia.com\/public\/uploads\/articles\/2021\/3\/20\/world-water-day-702x336.jpg\" width=\"100%\">","c3ba5159":"### Statistical analysis","adc1252b":"There are 9 Independent features and 1 dependent features. Details for each features are provided in the above top. objective is to use the Potability feature as target feature for classification problem. Let is do more analysis to learn about the dataset better.","42526ac4":"### Datatype of the features","d4f39d5f":"### lets try to fine tune the GradientBoosting & Bagging Classifier models","15c493c3":"### Check for missing values","5b33f406":"From the above basic modeling techinique, average cv score for GradientBoosting & BaggingClassifier is high compared to other models. so, let us try to do hyper tunning for these model to improve the accuracy","e6c9c594":"From the above table, we can see that the count of each feature are not same. so there must me some null values.  \nFeature Solids has the high mean and standard deviation comparted to other feature. so the distribution must be high.  \nHowever, the above description is for overall population. lets try the same for 2 samples based on Portability feature","4bbc904f":"As mentioned above, all the feature are independent and doesn't share any linear relationship. we would need atleast 7 dimensions to explain 90% of the variations. so dimenstionality reduction doesn't make any change.","090ce2b4":"# Read Data","a9903623":"***BaggingClassifier models gives slightly higher accuracy than the Gradientboosting model. further fine tuining could bring more accuracy score. I will stop here considering the computational time. lets try to predict the test dataset with best parameters from above cell and plot the confusion matrix and classification reports***","a7f8b74d":"## Hypothesis testing\n\nHo (Null Hypothesis) - Mean for both samples\/group are same  \nH1 (Alternate Hypothesis) - Mean for both samples\/group significantly different.  \nSignificant level - 90%  \nAlpha - 0.5% (ie., 0.1)  ","09d5ac3a":"Both Co-relation matrix & Paiplot says that there is no linear relationship between the features that can explan the target variable. So, Linear model may not work on this problem. we need to try with probability based models.","a050b7b2":"### Distribution plots","7c8de3fa":"### Problem Statement:\nClassify the water quality whether it is potable or not based on the features provided.\n\n### Feature Details\n1. pH value:\nPH is an important parameter in evaluating the acid\u2013base balance of water. It is also the indicator of acidic or alkaline condition of water status. WHO has recommended maximum permissible limit of pH from 6.5 to 8.5. The current investigation ranges were 6.52\u20136.83 which are in the range of WHO standards.\n\n2. Hardness:\nHardness is mainly caused by calcium and magnesium salts. These salts are dissolved from geologic deposits through which water travels. The length of time water is in contact with hardness producing material helps determine how much hardness there is in raw water. Hardness was originally defined as the capacity of water to precipitate soap caused by Calcium and Magnesium.\n\n3. Solids (Total dissolved solids - TDS):\nWater has the ability to dissolve a wide range of inorganic and some organic minerals or salts such as potassium, calcium, sodium, bicarbonates, chlorides, magnesium, sulfates etc. These minerals produced un-wanted taste and diluted color in appearance of water. This is the important parameter for the use of water. The water with high TDS value indicates that water is highly mineralized. Desirable limit for TDS is 500 mg\/l and maximum limit is 1000 mg\/l which prescribed for drinking purpose.\n\n4. Chloramines:\nChlorine and chloramine are the major disinfectants used in public water systems. Chloramines are most commonly formed when ammonia is added to chlorine to treat drinking water. Chlorine levels up to 4 milligrams per liter (mg\/L or 4 parts per million (ppm)) are considered safe in drinking water.\n\n5. Sulfate:\nSulfates are naturally occurring substances that are found in minerals, soil, and rocks. They are present in ambient air, groundwater, plants, and food. The principal commercial use of sulfate is in the chemical industry. Sulfate concentration in seawater is about 2,700 milligrams per liter (mg\/L). It ranges from 3 to 30 mg\/L in most freshwater supplies, although much higher concentrations (1000 mg\/L) are found in some geographic locations.\n\n6. Conductivity:\nPure water is not a good conductor of electric current rather\u2019s a good insulator. Increase in ions concentration enhances the electrical conductivity of water. Generally, the amount of dissolved solids in water determines the electrical conductivity. Electrical conductivity (EC) actually measures the ionic process of a solution that enables it to transmit current. According to WHO standards, EC value should not exceeded 400 \u03bcS\/cm.\n\n7. Organic_carbon:\nTotal Organic Carbon (TOC) in source waters comes from decaying natural organic matter (NOM) as well as synthetic sources. TOC is a measure of the total amount of carbon in organic compounds in pure water. According to US EPA < 2 mg\/L as TOC in treated \/ drinking water, and < 4 mg\/Lit in source water which is use for treatment.\n\n8. Trihalomethanes:\nTHMs are chemicals which may be found in water treated with chlorine. The concentration of THMs in drinking water varies according to the level of organic material in the water, the amount of chlorine required to treat the water, and the temperature of the water that is being treated. THM levels up to 80 ppm is considered safe in drinking water.\n\n9. Turbidity:\nThe turbidity of water depends on the quantity of solid matter present in the suspended state. It is a measure of light emitting properties of water and the test is used to indicate the quality of waste discharge with respect to colloidal matter. The mean turbidity value obtained for Wondo Genet Campus (0.98 NTU) is lower than the WHO recommended value of 5.00 NTU.\n\n10. Potability:\nIndicates if water is safe for human consumption where 1 means Potable and 0 means Not potable.\n","43b4e88d":"There is imbalance in the Target variable. which should be considered for modeling","126fa8d7":"Features ph, Sulfate and Trihalomethanes are having null values. let us check those in details and find option to ha","713dd377":"### BaggingClassifier","9c95c4c2":"#### PCA to check the explained variance","a896980b":"Mean and std of almost all features are similar for both samples. there are few differnces in Solids feature. Further analysis using hypothetical testing could help us to identify the significance.","bcdc3c9f":"All plants and animals need water to survive. There can be no life on earth without water. Why is water so important? Because 60 percent of our body weight is made up of water. Our bodies use water in all the cells, organs, and tissues, to help regulate body temperature and maintain other bodily functions. Because our bodies lose water through breathing, sweating, and digestion, it's crucial to rehydrate and replace water by drinking fluids and eating foods that contain water.\n\nLet\u2019s look at all the ways water impacts our lives\u2026\n\n1. Water helps by creating saliva\n2. It regulates body temperature\n3. Water aids cognitive functions\n4. Water protects the tissues, spinal cord, and joints\n5. Water maximizes our physical performance\n6. It helps to boost our energy levels\n7. Water prevents overall dehydration\n","723c8c05":"### GradientBoosting","39158f14":"Based on the approved limit, we can clearly see the difference in the water classification. Ex: distribution of non potable water is high on conductivity compared to potable water. same applicable to Turbidity, Trihalomethanes.  \nBut, Ph value, Chloramines, Sulfate, Organic carbon presence doesn't show significant difference. I hope the hypothetical testing can help us here.\n","b18cce87":"Since the missing values are on both classess (Potability 1 & 0), we can replace it with population mean. so, we will replace the Nan values bases on sample mean from both classes.","025c0f2e":"From the above Hypothesis testing, we can see that the features solid & Organic_carbon have significant difference in potable & non-potable water. other features shares similarities between two classes. ","1d5c05b9":"Except Target feature, other features are float and continueous value. we can convert the Portability into Categoring feature","93b20735":"***Both Gradient Boosting and Bagging Algorithms gives us f1 score (Balanced with precision & recall) as around 76% and H2O AI gives us 80% accuracy with StackedEnsemble model. I hope this is good model for initial analysis. further fine tuning and outlier handling might help for more accuracy. However, we will stop here considering the computational timing.***\n### <center>Appriciate your feedback and inputs on the notebook<\/center>\n\n<center><img src=\"https:\/\/www.icegif.com\/wp-content\/uploads\/water-icegif.gif\"><\/center>","556b6006":"There are outliers in the dataset, we need to handle the same. \nmost feature means are looks similar and there are very less differnce in variance. as mentioned in the above observation, Hypothesis testing would be the right option to identify the singnificance."}}