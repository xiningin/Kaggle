{"cell_type":{"5f178d24":"code","04839463":"code","4681ffab":"code","91630ca4":"code","ca0436e3":"code","5858c035":"code","711b0528":"code","f2f9db65":"code","6388fc9d":"code","616c22b9":"code","57ea8090":"code","b0a1042a":"code","27edbcb0":"code","9b5a5a6c":"code","b3393e89":"code","b9470b86":"code","ac0f66e5":"code","4621c9fe":"code","f2fc31bc":"code","0d6acbe0":"code","a705d4f3":"code","6d95cda3":"code","2f6c12ab":"code","42bdcdb9":"code","53a9409c":"code","294a144a":"code","91af0d23":"code","3ddd4b2d":"code","4c44fe41":"code","b03886dd":"code","fda5bb58":"code","20df5b04":"code","a1222063":"code","b49bc1e4":"code","2166b911":"code","cb79e807":"markdown","b4b73bfc":"markdown","5c063c8c":"markdown","2e9010fc":"markdown","1f4eb0be":"markdown","6df131b7":"markdown","0b74f1a7":"markdown","72c47b80":"markdown","dc9a4aff":"markdown","25ebaa1c":"markdown","e0e45c04":"markdown","5f1339c7":"markdown","e93bd380":"markdown","730596ca":"markdown","d519876c":"markdown","1ba984d3":"markdown","91c9d286":"markdown","03bcb2ae":"markdown","e8c53991":"markdown","af15ed34":"markdown","bb3ead77":"markdown","6dab9550":"markdown","043dbb2b":"markdown","d24e61a5":"markdown","2b6dcb0e":"markdown","48e3c8b7":"markdown","c06ec7c8":"markdown","d4bef676":"markdown","97902a1f":"markdown","67afd78b":"markdown","47afb584":"markdown","e6f50a85":"markdown","528c1bde":"markdown","7c4ef24f":"markdown","dc61b078":"markdown","97ed97c5":"markdown","ad660e97":"markdown","0ac1ae6b":"markdown","d9ff9a9f":"markdown","dc743dd3":"markdown","2963f0b5":"markdown","dc17adbf":"markdown","284cba38":"markdown","2d364d8e":"markdown","e219dbe7":"markdown","9a906eab":"markdown","9c64f789":"markdown","5a1ead2e":"markdown","85dcf094":"markdown","9629e489":"markdown","a64da20d":"markdown","774da767":"markdown","801e2410":"markdown","0610c471":"markdown","6ab7f26d":"markdown","7b83f183":"markdown","94169345":"markdown","d18b087f":"markdown","2ab6677f":"markdown","c9ec378e":"markdown","c5c132cc":"markdown","67a30550":"markdown","61d87e51":"markdown","bcbd42a3":"markdown","723e706a":"markdown","c8a4620d":"markdown","8ecad2d1":"markdown","47fa3b38":"markdown","983fbbec":"markdown","622b8d83":"markdown","7892d464":"markdown","04cc257e":"markdown"},"source":{"5f178d24":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\nimport scipy.stats as stats\n\n#Library for Data Modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Loading Data\nData = pd.read_csv(\"..\/input\/churn-data\/Churn Data.csv\")","04839463":"Data.head()","4681ffab":"Data.drop(columns = [\"customerID\"], inplace = True)","91630ca4":"Data.info()","ca0436e3":"Data[\"TotalCharges\"] = pd.to_numeric(Data['TotalCharges'], errors = 'coerce')","5858c035":"Data.isnull().sum()","711b0528":"Data['TotalCharges'] = np.where(Data['TotalCharges'].isnull(), Data['MonthlyCharges'], Data['TotalCharges'])","f2f9db65":"ax = sns.countplot(x=\"Churn\", data=Data)\nprint(\"Churn distribution\\n\")\nprint(round(Data[\"Churn\"].value_counts()\/Data.shape[0]*100,2))","6388fc9d":"# removing numerical features out of analysis.\nfeatures = Data.columns.values.tolist()\nnum_feature = ['tenure','MonthlyCharges','TotalCharges']\nfor feature in num_feature:\n    features.remove(feature)\nfor i, feature in enumerate(features):\n    plt.figure(i)\n    ax = sns.countplot(x=feature, data=Data)\n    print(\"Class distribution for %s is\\n\"%feature)\n    print(round(Data[feature].value_counts()\/Data.shape[0]*100,0), \"\\n\")","616c22b9":"labels = []\nb = Data['tenure'].max()\na = Data['tenure'].min()\nfor i in range(a,b,12):\n        labels.append(\"{} - {}\".format(i, i+12))\nData[\"tenure_class\"] = pd.cut(Data['tenure'], range(a,b+12,12), labels = labels, right = False)\nround(Data['tenure_class'].value_counts()\/Data.shape[0]*100,0)","57ea8090":"Data[['tenure','MonthlyCharges','TotalCharges']].corr()","b0a1042a":"sns.lmplot(data=Data, x=\"tenure\", y=\"TotalCharges\")","27edbcb0":"sns.displot(data=Data, x=\"tenure\", hue='Partner', kind=\"kde\")","9b5a5a6c":"sns.displot(data=Data, x=\"tenure\", hue='Contract', kind=\"kde\")","b3393e89":"sns.displot(data=Data, x=\"tenure\", hue='Churn', kind=\"kde\")","b9470b86":"round(pd.crosstab(Data['SeniorCitizen'], Data['Churn'],  normalize='index')*100,0)","ac0f66e5":"round(pd.crosstab(Data['Partner'], Data['Churn'],  normalize='index')*100,0)","4621c9fe":"round(pd.crosstab(Data['Contract'], Data['Churn'],  normalize='index')*100,0)","f2fc31bc":"Chi_test_feature = Data.columns.values.tolist()\nChi_num_feature = ['tenure','MonthlyCharges','TotalCharges','tenure_class','Churn']\nfor feature in Chi_num_feature:\n    Chi_test_feature.remove(feature)\nChi_test_feature\nFeatures = []\nP_value = []\nfor Chi_feature in Chi_test_feature:\n    table = round(pd.crosstab(Data[Chi_feature], Data['Churn'],  normalize='index')*100,0)\n    stat, p, dof, expected = chi2_contingency(table)\n    Features.append(Chi_feature)\n    P_value.append(p)\nChi_df = pd.DataFrame(list(zip(Features, P_value)), columns = ['CHI SQUARE Feature', 'CHI SQUARE P Value'])\nChi_df.sort_values('CHI SQUARE P Value')","0d6acbe0":"# Converting Senior Citizen into Categorical Variable before applying ANOVA Test\nData['SeniorCitizen'].replace(1, \"Yes\", inplace = True)\nData['SeniorCitizen'].replace(0, \"No\", inplace = True)\n\nANOVA_feature = Data.columns.values.tolist()\nANOVA_num_feature = ['tenure','MonthlyCharges','TotalCharges','tenure_class','Churn', 'gender','InternetService','PaymentMethod','Contract']\nfor feature in ANOVA_num_feature:\n    ANOVA_feature.remove(feature)\nP_value = []\nFeature_name = []\nfor ANOVA in ANOVA_feature:\n    ANOVA_test = Data[Data[ANOVA] == \"Yes\"]\n    ANOVA_test = ANOVA_test.loc[:, ['Churn']]\n    A = np.where(ANOVA_test == \"Yes\", 1, 0)\n\n    ANOVA_test = Data[Data[ANOVA] == \"No\"]\n    ANOVA_test = ANOVA_test.loc[:, ['Churn']]\n    B = np.where(ANOVA_test == \"Yes\", 1, 0)\n    fvalue, pvalue = stats.f_oneway(A, B)\n    P_value.append(pvalue)\n    Feature_name.append(ANOVA)\n\ngender = Data[Data['gender'] == \"Male\"]\ngender = gender.loc[:, ['Churn']]\nA = np.where(gender == \"Yes\", 1, 0)\ngender = Data[Data[\"gender\"] == \"Female\"]\ngender = gender.loc[:, ['Churn']]\nB = np.where(gender == \"Yes\", 1, 0)\nfvalue, pvalue = stats.f_oneway(A, B)\nP_value.append(pvalue)\nFeature_name.append('gender')\n\nInternetService = Data[Data['InternetService'] == \"Fiber optic\"]\nInternetService = InternetService.loc[:, ['Churn']]\nA = np.where(InternetService == \"Yes\", 1, 0)\nInternetService = Data[Data[\"InternetService\"] == \"DSL\"]\nInternetService = InternetService.loc[:, ['Churn']]\nB = np.where(InternetService == \"Yes\", 1, 0)\nInternetService = Data[Data[\"InternetService\"] == \"No\"]\nInternetService = InternetService.loc[:, ['Churn']]\nC = np.where(InternetService == \"Yes\", 1, 0)\nfvalue, pvalue = stats.f_oneway(A, B, C)\nP_value.append(pvalue)\nFeature_name.append('InternetService')\n\nContract = Data[Data['Contract'] == \"Month-to-month\"]\nContract = Contract.loc[:, ['Churn']]\nA = np.where(Contract == \"Yes\", 1, 0)\nContract = Data[Data[\"Contract\"] == \"Two year\"]\nContract = Contract.loc[:, ['Churn']]\nB = np.where(Contract == \"Yes\", 1, 0)\nContract = Data[Data[\"Contract\"] == \"One year\"]\nContract = Contract.loc[:, ['Churn']]\nC = np.where(Contract == \"Yes\", 1, 0)\nfvalue, pvalue = stats.f_oneway(A, B, C)\nP_value.append(pvalue)\nFeature_name.append('Contract')\n\nPaymentMethod = Data[Data['PaymentMethod'] == \"Electronic check\"]\nPaymentMethod = PaymentMethod.loc[:, ['Churn']]\nA = np.where(PaymentMethod == \"Yes\", 1, 0)\nPaymentMethod = Data[Data['PaymentMethod'] == \"Mailed check\"]\nPaymentMethod = PaymentMethod.loc[:, ['Churn']]\nB = np.where(PaymentMethod == \"Yes\", 1, 0)\nPaymentMethod = Data[Data['PaymentMethod'] == \"Bank transfer (automatic)\"]\nPaymentMethod = PaymentMethod.loc[:, ['Churn']]\nC = np.where(PaymentMethod == \"Yes\", 1, 0)\nPaymentMethod = Data[Data['PaymentMethod'] == \"Credit card (automatic)\"]\nPaymentMethod = PaymentMethod.loc[:, ['Churn']]\nD = np.where(PaymentMethod == \"Yes\", 1, 0)\nfvalue, pvalue = stats.f_oneway(A, B, C, D)\nP_value.append(pvalue)\nFeature_name.append('PaymentMethod')\n\nANOVA_df = pd.DataFrame(list(zip(Feature_name, P_value)), columns = ['ANOVA Feature', 'ANOVA P Value'])\nANOVA_df.sort_values('ANOVA P Value')","a705d4f3":"Data = pd.get_dummies(Data)","6d95cda3":"Data.head()","2f6c12ab":"Data.drop(columns = ['gender_Female', 'SeniorCitizen_No', 'Partner_No', 'Dependents_No', 'PhoneService_No', 'MultipleLines_No', 'InternetService_No', 'OnlineSecurity_No', 'OnlineBackup_No', 'DeviceProtection_No', 'TechSupport_No', 'StreamingTV_No', 'StreamingMovies_No', 'Contract_Two year', 'PaperlessBilling_No','PaymentMethod_Mailed check', 'Churn_No'], inplace = True)","42bdcdb9":"x = Data.drop(columns = ['Churn_Yes'])\ny = Data['Churn_Yes']","53a9409c":"sm = SMOTE()\nx_resampled, y_resampled = sm.fit_resample(x,y)","294a144a":"xr_train,xr_test,yr_train,yr_test=train_test_split(x_resampled, y_resampled,test_size=0.2)","91af0d23":"def run_cross_validation_on_trees(xr_train, yr_train, tree_depths, cv=5, scoring='roc_auc'):\n    cv_scores_list = []\n    cv_scores_std = []\n    cv_scores_mean = []\n    AUC_scores = []\n    for depth in tree_depths:\n        tree_model = DecisionTreeClassifier(max_depth=depth)\n        cv_scores = cross_val_score(tree_model, xr_train, yr_train, cv=5, scoring='roc_auc')\n        cv_scores_list.append(cv_scores)\n        cv_scores_mean.append(cv_scores.mean())\n        cv_scores_std.append(cv_scores.std())\n        tree_model.fit(xr_train, yr_train)    \n        train_pred = tree_model.predict(xr_train)\n        false_positive_rate, true_positive_rate, thresholds = roc_curve(yr_train, train_pred)\n        roc_auc = auc(false_positive_rate, true_positive_rate)\n        AUC_scores.append(roc_auc)\n    cv_scores_mean = np.array(cv_scores_mean)\n    cv_scores_std = np.array(cv_scores_std)\n    AUC_scores = np.array(AUC_scores)\n    return cv_scores_mean, cv_scores_std, AUC_scores\ndef plot_cross_validation_on_trees(depths, cv_scores_mean, cv_scores_std, AUC_scores, title):\n    fig, ax = plt.subplots(1,1, figsize=(15,5))\n    ax.plot(depths, cv_scores_mean, '-o', label='mean cross-validation AUC score', alpha=0.9)\n    ax.fill_between(depths, cv_scores_mean-2*cv_scores_std, cv_scores_mean+2*cv_scores_std, alpha=0.2)\n    ylim = plt.ylim()\n    ax.plot(depths, AUC_scores, '-*', label='train AUC Score', alpha=0.9)\n    ax.set_title(title, fontsize=16)\n    ax.set_xlabel('Tree depth', fontsize=14)\n    ax.set_ylabel('AUC', fontsize=14)\n    ax.set_ylim(ylim)\n    ax.set_xticks(depths)\n    ax.legend()\nsm_tree_depths = range(1,25)\nsm_cv_scores_mean, sm_cv_scores_std, sm_AUC_scores = run_cross_validation_on_trees(xr_train, yr_train, sm_tree_depths)\nplot_cross_validation_on_trees(sm_tree_depths, sm_cv_scores_mean, sm_cv_scores_std, sm_AUC_scores, \n                               'AUC per decision tree depth on training data')","3ddd4b2d":"def run_cross_validation_on_trees(xr_train, yr_train, sample_leaf, cv=5, scoring='roc_auc'):\n    cv_scores_list = []\n    cv_scores_std = []\n    cv_scores_mean = []\n    AUC_scores = []\n    for sample_leaf in sample_leaf:\n        tree_model = DecisionTreeClassifier(min_samples_leaf=sample_leaf)\n        cv_scores = cross_val_score(tree_model, xr_train, yr_train, cv=5, scoring='roc_auc')\n        cv_scores_list.append(cv_scores)\n        cv_scores_mean.append(cv_scores.mean())\n        cv_scores_std.append(cv_scores.std())\n        tree_model.fit(xr_train, yr_train)    \n        train_pred = tree_model.predict(xr_train)\n        false_positive_rate, true_positive_rate, thresholds = roc_curve(yr_train, train_pred)\n        roc_auc = auc(false_positive_rate, true_positive_rate)\n        AUC_scores.append(roc_auc)\n    cv_scores_mean = np.array(cv_scores_mean)\n    cv_scores_std = np.array(cv_scores_std)\n    AUC_scores = np.array(AUC_scores)\n    return cv_scores_mean, cv_scores_std, AUC_scores\ndef plot_cross_validation_on_trees(sample_leaf, cv_scores_mean, cv_scores_std, AUC_scores, title):\n    fig, ax = plt.subplots(1,1, figsize=(30,10))\n    ax.plot(sample_leaf, cv_scores_mean, '-o', label='mean cross-validation AUC Score', alpha=0.9)\n    ax.fill_between(sample_leaf, cv_scores_mean-2*cv_scores_std, cv_scores_mean+2*cv_scores_std, alpha=0.2)\n    ylim = plt.ylim()\n    ax.plot(sample_leaf, AUC_scores, '-*', label='train AUC score', alpha=0.9)\n    ax.set_title(title, fontsize=16)\n    ax.set_xlabel('No of sample at leaf', fontsize=14)\n    ax.set_ylabel('AUC Score', fontsize=14)\n    ax.set_ylim(ylim)\n    ax.set_xticks(sample_leaf)\n    ax.legend()\nsm_sample_leaf = range(1,100)\nsm_cv_scores_mean, sm_cv_scores_std, sm_AUC_scores = run_cross_validation_on_trees(xr_train, yr_train, sm_sample_leaf)\nplot_cross_validation_on_trees(sm_sample_leaf, sm_cv_scores_mean, sm_cv_scores_std, sm_AUC_scores, \n                               'AUC score per decision tree depth on training data')","4c44fe41":"model_dt=DecisionTreeClassifier(criterion = \"gini\",class_weight = \"balanced\", random_state = 100,max_depth=7,min_samples_leaf=15)\nmodel_dt.fit(xr_train,yr_train)","b03886dd":"test_pred = model_dt.predict(xr_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(yr_test, test_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(f\"AUC value for Decision Tree Model ater SMOTE Sampling is {round(roc_auc*100,0)}%\\n\\n\")\nprint(classification_report(yr_test,test_pred, labels=[0,1]))","fda5bb58":"Data = pd.read_csv(\"..\/input\/churn-data\/Churn Data.csv\")","20df5b04":"model_rf=RandomForestClassifier(n_estimators=100, criterion='gini', random_state = 100,max_depth= 7, min_samples_leaf= 15)\nmodel_rf.fit(xr_train,yr_train)","a1222063":"test_pred = model_rf.predict(xr_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(yr_test, test_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(f\"AUC value for Random Forest Model ater SMOTE Sampling is {round(roc_auc*100,0)}%\\n\\n\")\nprint(classification_report(yr_test,test_pred, labels=[0,1]))","b49bc1e4":"features = x.columns.values.tolist()\nimportance = model_rf.feature_importances_\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()\ndf = pd.DataFrame(list(zip(features, importance)), columns = ['Feature', 'Importance Value'])\ndf.sort_values('Importance Value', ascending= False)","2166b911":"Data_new = pd.read_csv('..\/input\/churn-data\/Churn Data.csv')\nData_new.drop(columns = [\"customerID\"], inplace = True)\nData_new = pd.get_dummies(Data_new)\nData_new.drop(columns = ['gender_Female', 'SeniorCitizen', 'Partner_No', 'Dependents_No', 'PhoneService_No', 'MultipleLines_No', 'InternetService_No', 'OnlineSecurity_No', 'OnlineBackup_No', 'DeviceProtection_No', 'TechSupport_No', 'StreamingTV_No', 'StreamingMovies_No', 'Contract_Two year', 'PaperlessBilling_No','PaymentMethod_Mailed check', 'Churn_No','MultipleLines_Yes','gender_Male','PhoneService_Yes','StreamingTV_Yes','StreamingMovies_Yes'], inplace = True)\nx_new = Data_new.drop(columns = ['Churn_Yes'])\ny_new = Data_new['Churn_Yes']\nsm = SMOTE()\nxr_resampled, yr_resampled = sm.fit_resample(x_new,y_new)\nxr_train_new,xr_test_new,yr_train_new,yr_test_new=train_test_split(x_resampled, y_resampled,test_size=0.2)\nmodel_rf=RandomForestClassifier(n_estimators=100, criterion='gini', random_state = 100,max_depth= 7, min_samples_leaf= 15)\nmodel_rf.fit(xr_train_new,yr_train_new)\ntest_pred = model_rf.predict(xr_test_new)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(yr_test_new, test_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(f\"AUC value for Random Forest Model ater SMOTE Sampling & Feature Selection is {round(roc_auc*100,0)}%\\n\\n\")\nprint(classification_report(yr_test_new,test_pred, labels=[0,1]))","cb79e807":"Data shows that most of the customer has short experience history with the current service provider like approx. 60% customer has tenure less than or equal to 2 years.","b4b73bfc":"After loading data and exploring the variables, I found that **\u201ccustomerID\u201d** feature\/column is adding no value to the Exploratory Data Analysis (EDA) and Model Building, hence it\u2019s better to get rid of it.","5c063c8c":"To check which categorical variable has much influence over target variable \u2018Churn\u2019, we can perform **\u2018Chi Square test of Independence'**. Features which are dependent of \u2018Churn\u2019 variable will define churn rate more significantly than those which are independent of the \u2018Churn\u2019 variable.","2e9010fc":"Classification score for Random Forest Classifier","1f4eb0be":"**\u201cTenure\u201d:** Dividing the customer based on the tenure into equal size bin of 12 months like 0\u201312, 12\u201324, 24\u201336 and so on will give more informative insight into data.","6df131b7":"In the same fashion we can run cross validation technique to check for value of Min_sample_leaf parameter for which we best AUC score for the decision tree.","0b74f1a7":"## Data Preprocessing","72c47b80":"### Decision Tree","dc9a4aff":"## Data Attribute Information\n\n* **Independent Variables\/ Features:** 20\n* **Dependent (Target) Variables \/ Features :** 1\n* **Total Records :** 7043\nTarget variable indicates if a customer has left the company (i.e. churn=yes) within the last month. Since the target variable has two states (yes\/no or 1\/0), this is a binary classification problem.\n\n* **Prediction column:**\n**Churn:** Whether the customer churned or not (Yes or No)\n\n* **Two numerical columns:**\n1. **MonthlyCharges:** The amount charged to the customer monthly\n2. **TotalCharges:** The total amount charged to the customer\n\n* **Eighteen categorical columns:**\n1. **CustomerID:** Customer ID unique for each customer\n2. **gender:** Whether the customer is a male or a female\n3. **SeniorCitizen:** Whether the customer is a senior citizen or not (1, 0)\n4. **Partner:** Whether the customer has a partner or not (Yes, No)\n5. **Dependents:** Whether the customer has dependents or not (Yes, No)\n6. **Tenure:** Number of months the customer has stayed with the company\n7. **PhoneService:** Whether the customer has a phone service or not (Yes, No)\n8. **MultipleLines:** Whether the customer has multiple lines or not (Yes, No, No phone service)\n9. **InternetService:** Customer\u2019s internet service provider (DSL, Fiber optic, No)\n10. **OnlineSecurity:** Whether the customer has online security or not (Yes, No, No internet service)\n11. **OnlineBackup:** Whether the customer has an online backup or not (Yes, No, No internet service)\n12. **DeviceProtection:** Whether the customer has device protection or not (Yes, No, No internet service)\n13. **TechSupport:** Whether the customer has tech support or not (Yes, No, No internet service)\n14. **StreamingTV:** Whether the customer has streaming TV or not (Yes, No, No internet service)\n15. **StreamingMovies:** Whether the customer has streaming movies or not (Yes, No, No internet service)\n16. **Contract:** The contract term of the customer (Month-to-month, One year, Two years)\n17. **PaperlessBilling:** Whether the customer has paperless billing or not (Yes, No)\n18. **PaymentMethod:** The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))","25ebaa1c":"First I\u2019ll see what performance I\u2019ll get with decision tree classifier for that I have decide upon few parameters before creating decision tree class like:\n1. Depth of Decision Tree\n2. Minimum No of sample at leaf node  \n  \nI will run cross validation technique to find the best parameter for the model.","e0e45c04":"#### \u2018tenure\u2019 vs \u2018Contract\u2019","5f1339c7":"Above plot gives following insight about the Telecom customer:\n* People having partner have long history of service experience than without which may further translate that people with partner prefer to stay more long tenure than without.\n* Customer who are enrolled in monthly contract have very short tenure in comparison to 1 year and 2 years contract, reason for this might be people with monthly contract are less bound to service provider hence more likely to leave the service if given better option while yearly contract (1 yr & 2 yrs) customer enjoy less freedom in term of switching to another service provider.\n* People with short tenure high churn rate than long term customer.  \n  \nComparison of **\u2018Churn\u2019** rate wrt to different Categorical features\nAfter analyzing each categorical features wrt to Churn Rate it is found except for the following categorical features all have almost same pattern of churn rate within individual group of each categorical features like in **\u2018gender\u2019 \u2014 \u2018Male\u2019** churn pattern is same as **\u2018Female\u2019** churn pattern.","e93bd380":"Data preprocessing is the process of transforming raw data into a format suitable for machine learning modeling. It is also an important step in ML model Life cycle since machine can\u2019t work with raw data. Model performance and therefore the quality of received insights depend on the quality of data, the primary aim is to make sure all data points are presented using the same format, and the overall dataset is free of inconsistencies.","730596ca":"Result reflect that there is slight improvement in the score in comparision to decision tree, F1 score for both 'Churn' & 'Churn' classes have improved and overall accuracy has also improved. Now lets see can feature engineering help in further model performance improvement.","d519876c":"## Scope of Work \/ Project Structure\n\nThe whole project of building model is divided into following phases:\n1. Data Cleaning\n2. Exploratory Data Analysis\n3. Data Preprocessing\n4. Model Creation\n5. Improving the model","1ba984d3":"### Class Balancing","91c9d286":"#### SeniorCitizen\u2019 Vs \u2018Churn\u2019","03bcb2ae":"#### \u2018Contract\u2019 Vs \u2018Churn\u2019","e8c53991":"Customer who have enrolled for 1 yr & 2 yr contract seems to stay for a longer tenure since these group have less churn rate in comparison to monthly contract customer which shows high churn rate.","af15ed34":"Other than above missing values, Our date is clean hence no further processing is required and we good to go for Exploratory Data Analysis(EDA).","bb3ead77":"### Model training","6dab9550":"## Model Creation","043dbb2b":"### Target Variable Class Distribution","d24e61a5":"### Distribution Plot between Categorical features & Numerical Features","2b6dcb0e":"#### \u2018tenure\u2019 vs \u2018Churn\u2019","48e3c8b7":"**11** missing values is present in **\u201cTotalCharges\u201d** column, after getting looking into the records having missing values it was found that all the records with tenure less than 1 month is having **NaN** values for \u201cTotalCharges\u201d, since for tenure less than **1 month** total charges will be same as monthly charges hence imputing the missing \u201cTotalCharges\u201d field with corresponding \u201cMonthlyCharges\u201d will do the job.","c06ec7c8":"### Independent Features Distribution - Numerical features","d4bef676":"### Correlation between Numerical features","97902a1f":"Above result say that features engineering has helped in improving the performance of the model, after Feature Selection using ANOVA and Chi Square Test maximum we have achieve AUC score of 0.83 and F1 score and accuracy has also improved.","67afd78b":"Following conclusion can be drawn after analyzing Bar plot of each of the independent categorical feature distribution:\n\n1. **\u201cgender\u201d** \u2014 Data is equally distributed among \u2018gender\u2019 class i,e. Female & Male has equal proportion in the dataset.\n2. **\u201cSeniorCitizens\u201d** \u2014 Dataset comprises mostly of younger people as only 16 % of the customers are senior citizens.\n3. **\u201cPartner\u201d** \u2014 Data is equally divided among \u2018Partner\u2019 class i,e. \u2018Yes\u2019 (having Partner) & \u2018No\u2019 (Not having Partner)\n4. **\u201cDependents\u201d** \u2014 30 % Customers have dependents with them.\n5. **\u201cPhoneService\u201d** \u2014 Most of the customer have opted for \u2018PhoneServices\u2019 about 90%.\n6. **\u201cOnlineScurity\u201d, \u201cOnlineBackup\u201d, \u201cDeviceProtection\u201d, \u201cTechSupport\u201d** \u2014 Customer who opted for the aforementioned service seems to follow similar distribution like approx. 30% (Subscribed to the service) & 70% (Not opted for service or don\u2019t have internet services).","47afb584":"* **Churn: No: 73.46%**\n* **Churn: Yes : 26.54%**  \nTarget variable has imbalanced class distribution which is obvious. out of total records approx. 26 % has churn. For your Information time period put to consideration for churn calculation is 6 yr (72 months).\nSince imbalance class will make the performance of model bias more toward Positive class(Churn = \u2018No\u2019), hence data needed to balanced before feeding data for the training of model. There are many technique available like Up \u2014 sampling (Minority Class) or Down - sampling(Majority class) which may be used for for the class balancing, I have chosen **SMOTE (Synthetic Minority Up-sampling Technique)** for this project, which will take care of the class imbalance. I\u2019ll apply the same to dataset before Model training.","e6f50a85":"## Data Cleaning\n\nStart with Importing important libraries and then follow on with loading dataset.","528c1bde":"With the help of ANOVA test we can filter those categorical features which explains most of the variance of target variable.","7c4ef24f":"#### \u2018Partner\u2019 Vs \u2018Churn\u2019","dc61b078":"Checking for whether each column data match with the assigned datatype.","97ed97c5":"### Independent Features Distribution - Categorical features","ad660e97":"Now we are done with all the necessary steps before ML modeling like data preprocessing and data cleaning, it\u2019s time to go for training of model on train data.","0ac1ae6b":"Exploring the data to see if there are any missing values.","d9ff9a9f":"After training model we can look for performance of the model using classification score","dc743dd3":"Clubing the above result with the ANOVA test & Chi Square test result we eliminate those feature which seems to have no value proposition to the model or seems to have no relation with the target variable.\n  \n  \nAfter analyzing following features are worth removing from the dataset:\n1. \u2018SeniorCitizen\u2019\n2. \u2018MultipleLines\u2019\n3. \u2018gender\u2019\n4. \u2018PhoneService\u2019\n5. \u2018StreamingTV\u2019\n6. \u2018StreamingMovies\u2019\n  \n  \nMaking a fresh data for model training after removing above features and then training the model.","2963f0b5":"## Model Improvement","dc17adbf":"## Bi- Variate Analysis","284cba38":"## Exploratory Data Analysis","2d364d8e":"Again as we can see after **Min_sample_leaf = 15** AUC score value tend to flatten out hence we will settle with min_sample_leaf = 15.","e219dbe7":"I have used **SMOTE** technique to make dataset balance with respect to \u2018Churn\u2019 variable.","9a906eab":"As the graph revel that at **tree depth=7** we are having maximum AUC score for the model, hence we will use the same during training of model.","9c64f789":"Divide whole data into two subset of  \n\nx = Independent Variables\n  \ny = Target Variable","5a1ead2e":"### feature Engineering","85dcf094":"Using get_dummies method all the categorical features are converted into binary form [0 & 1], since to avoid dummy trap all the redundant dummy features are dropped.  \n \n Features to remove :  \n \n\u2018gender_Female\u2019, \u2018SeniorCitizen_No\u2019, \u2018Partner_No\u2019, \u2018Dependents_No\u2019, \u2018PhoneService_No\u2019, MultipleLines_No\u2019, \u2018InternetService_No\u2019, \u2018OnlineSecurity_No\u2019, \u2018OnlineBackup_No\u2019,\n\u2018DeviceProtection_No\u2019, \u2018TechSupport_No\u2019, \u2018StreamingTV_No\u2019,\n\u2018StreamingMovies_No\u2019, \u2018Contract_Two year\u2019, \u2018PaperlessBilling_No\u2019,\u2019PaymentMethod_Mailed check\u2019,\u2019Churn_No","9629e489":"Above model can be considered as a good predictive model but still there is scope of improvement which can be done easily by considering other rost classification algorithims like SVM, certain Baggging technique can also be tried along PCA for feature extraction and reduction. I hope the above step by step guide will have given some foundation on predictive modeling and its application. I left the model improvement part to you to experiment.","a64da20d":"#### \u2018tenure\u2019 vs \u2018Partner\u2019","774da767":"### Spliting Data for Testing & Training","801e2410":"Report show that model is performing well on both the classes since value of F1 score for Churn and Not Churn classes is almost same, lets see if we can improved the performance using bagging ensemble technique and for the same I will use Random Forest Classifier.","0610c471":"## Why it is important to keep check on customer churn rate?\nAs it is found that company has to spend comparatively much more to acquire new customers than it does to retain existing customers. In fact, a marginal decrease in churn rate can make good difference in the company revenue score. for example simply decreasing your churn rate by 10% could add an extra $100,000 in revenue to the company. Dropping from 3% to 2.7% doesn\u2019t seem like a lot, but it actually adds large benefits to your company.\nHence with the help of a robust and accurate predictive modeling for customer churn, company can do retain customers by forming strategies aligning with customer needs.","6ab7f26d":"#### Depth of Decision Tree","7b83f183":"### Converting all Categorical Features to Numerical Features","94169345":"### Split Data into Target & Input Variables","d18b087f":"**\u2018tenure\u2019** follow quite good correlation with **\u2018TotalCharges\u2019** which is logically prevalent because as the tenure increase the total charges also come along since \u2018TotalCharges\u2019 is just multiplication of **\u2018MonthlyCharges\u2019 & \u2018tenure\u2019**. Strong can be found by visualizing trend of \u2018tenure\u2019 in relation to \u2018TotalCharges\u2019 using **Scatter Plot**.","2ab6677f":"## Conclusion","c9ec378e":"### ANOVA Test","c5c132cc":"The below graph show the importance of each feature wrt to target variable which is decided by random forest based on the performance of each independent variable at the time of model training.","67a30550":"## Univariate Analysis","61d87e51":"Customers who are having partner with them tend to stay longer since churn for customer with partner is much less in comparison to with out partner customer.","bcbd42a3":"### Chi Square Test","723e706a":"Result of the above Chi Square Test and ANOVA test illustrate that following features are insignificant since these statistical test doesn\u2019t find strong evidence to reject Null Hypothesis:\n* \u2018SeniorCitizen\u2019\n* \u2018MultipleLines\u2019\n* \u2018gender\u2019\n* \u2018PhoneService\u2019\n* \u2018StreamingTV\u2019\n* \u2018StreamingMovie","c8a4620d":"Importing Neccessary Module","8ecad2d1":"Since **\u201cTotalCharges\u201d** is an numerical feature but as you can see in the above output it is assigned **object** datatype, hence it need correction, following code will help to make requisite changes to the **\u201cTotalCharges\u201d** column.","47fa3b38":"## What Is Customer Churn?\nCustomer churn is the percentage of customers that stopped using your company\u2019s product or service during a certain time frame. One of the ways to calculate a churn rate is by dividing the number of customers you lost during that time period by the number of customers you had at the beginning of that time period.\nFor example, if you have 100 customers to start the year and at the end you left with 94, then your churn rate is 6% as you lost 6% of your customers.\nFor the purpose of this project \u201cTelco Customer Churn\u201d dataset available on Kaggle is used.","983fbbec":"#### Minimum No of Sample at Leaf Node","622b8d83":"Further Dividing each subset i,e. x & y into train & test set\n  \nTrain data will be used for model training while test set will be used for model validation.","7892d464":"Deciding upon range of depth of decision tree from 1 to 25 using AUC as performance metric.","04cc257e":"### Random Forest Classifier"}}