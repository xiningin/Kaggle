{"cell_type":{"a8c766ae":"code","87b1915d":"code","930fa3dd":"code","6ad6a77f":"code","89d0af7a":"code","4590c883":"code","3b4fddca":"code","a178e0f4":"code","1fa050ac":"code","55a00aa0":"code","d6257f48":"code","37e82b3c":"code","d1a16b77":"code","ff8f03b7":"code","2f7876e0":"code","45934f3c":"code","f5b30bda":"code","08995ebe":"code","34b1bc34":"code","ca326a75":"code","3cc0cf44":"code","2d7e12b9":"code","894ad464":"code","49ff170c":"code","1aab303e":"code","5d703186":"code","e97de1eb":"code","b8331e1f":"code","d20fdede":"code","92706778":"code","068a2ac1":"code","8bb86d00":"code","20c37e69":"code","a20a14e4":"code","5a94aea8":"code","9d0f2c67":"markdown","2985c41d":"markdown","d5ae27a1":"markdown","5923305a":"markdown","1d0db024":"markdown","449bd8da":"markdown","09014179":"markdown","ecbd638e":"markdown","b2df2a6f":"markdown","8ab4c4c9":"markdown","8740eb0e":"markdown","937483ac":"markdown","d704615d":"markdown","017e146f":"markdown","c45610ed":"markdown","465e26b8":"markdown","d71d069f":"markdown","a4fcab52":"markdown","640c4cd0":"markdown","8c77c8da":"markdown","e4ca5324":"markdown","c8d3ac0d":"markdown","3907a513":"markdown","8cbc741e":"markdown","5c0f9613":"markdown"},"source":{"a8c766ae":"# For processing the data\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# Visualization tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom tensorflow import keras\n\n# Others\nimport warnings\nwarnings.filterwarnings(\"ignore\")","87b1915d":"people = pd.read_csv(\"..\/input\/predicting-red-hat-business-value\/people.csv.zip\")\nactivity = pd.read_csv(\"..\/input\/predicting-red-hat-business-value\/act_train.csv.zip\")\ntest = pd.read_csv(\"..\/input\/predicting-red-hat-business-value\/act_test.csv.zip\")\n","930fa3dd":"df = pd.merge(people, activity, left_on=\"people_id\", right_on=\"people_id\")\ndf.isnull().sum()[df.isnull().sum() > 0]","6ad6a77f":"test = pd.merge(people, test, left_on=\"people_id\", right_on=\"people_id\")","89d0af7a":"people.head(3)","4590c883":"print(\"People's shape: \", people.shape, \"\\n\")\npeople.info()","3b4fddca":"people.describe().transpose()","a178e0f4":"f, ax = plt.subplots(figsize=(12,8))\n\nsns.distplot(df[df['outcome']==0]['char_38'], color='#ff8492', ax=ax)\nsns.distplot(df[df['outcome']==1]['char_38'], color='#84fff1', ax=ax)\nplt.show()\n\nprint('Number of 0 value:', df[df.char_38==0]['char_38'].count())\nprint('Number of 1 value:', df[df.char_38==1]['char_38'].count())","1fa050ac":"people.iloc[:, :40].astype(\"object\").describe()","55a00aa0":"sns.countplot(people[\"char_1\"],\n            palette=('#ff8492', '#84fff1'))\nplt.show()","d6257f48":"activity.head(3)","37e82b3c":"print(\"Activity's shape: \", activity.shape, \"\\n\" )\nactivity.info()","d1a16b77":"activity.isnull().sum() ","ff8f03b7":"activity.describe(include=\"object\")","2f7876e0":"activity[\"outcome\"].value_counts()","45934f3c":"activity[\"outcome\"] = activity[\"outcome\"].astype('object')\n\nsns.countplot(activity[\"outcome\"], \n            palette=('#ff8492', '#84fff1'))\nplt.show()","f5b30bda":"df.loc[:, df.columns != 'char_38'] = df.loc[:, df.columns != 'char_38'].fillna(\"missing\")\ndf.isnull().any().sum()","08995ebe":"def get_date_features(df, original_date):\n    features = [\"year\", \"month\", \"day\", \"is_month_end\", \"is_month_start\", \n                \"is_quarter_end\", \"is_quarter_start\", \"is_year_end\", \"is_year_start\"]\n    df[original_date] = pd.to_datetime(df[original_date])\n    for n in features:\n        df[n + \"_{}\".format(original_date)] = df[original_date].map(lambda x: getattr(x, n))\n    df[\"weekday {}\".format(original_date)] = [\"weekday\" if x < 5 else \"weekend\" \n                                              for x in df[original_date].dt.weekday]\n    df = df.drop(original_date, axis=1)","34b1bc34":"get_date_features(df, \"date_x\")\nget_date_features(df, \"date_y\")","ca326a75":"df.head()","3cc0cf44":"categorical = df.select_dtypes(include=['object'])\ncolumn_names = categorical.columns\n\nembed_feats = categorical.nunique() > 12\nonehot_feats = categorical.nunique() <= 12","2d7e12b9":"pd.get_dummies(df, columns=df[onehot_feats[onehot_feats == True].index].columns, drop_first=True)\ndf.drop(columns=df[onehot_feats[onehot_feats == True].index].columns, axis=1, inplace=True)","894ad464":"df[embed_feats[embed_feats == True].index]= df[embed_feats[embed_feats == True].index].apply(\n    LabelEncoder().fit_transform)","49ff170c":"df.head()","1aab303e":"y = df[\"outcome\"]\nX = df.drop([\"people_id\", \"activity_id\", \"date_x\", \"date_y\", \"outcome\"], axis=1)\nX = X.iloc[:500000, :]\ny = y.iloc[:500000]","5d703186":"kfold=StratifiedKFold(n_splits=5)\nprint(X.shape)\nprint(y.shape)","e97de1eb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2666)","b8331e1f":"# I've narrowed down the range of the parameters after some testing\nlgbm = LGBMClassifier(random_state=2666)\n\nparam_grid = {'num_leaves': [5,10,15, 25],\n              'learning_rate': [0.001,0.005],\n              'n_estimators': [50,100,500,1000]}\n\nlgbm = GridSearchCV(lgbm, param_grid=param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=3, verbose=2)\n\nlgbm.fit(X_train, y_train)\n\nprint(lgbm.best_score_)\nprint(lgbm.best_params_)","d20fdede":"test_c = test.copy()\ntest_c.loc[:, test_c.columns != 'char_38'] = test_c.loc[:, test_c.columns != 'char_38'].fillna(\"missing\")","92706778":"get_date_features(test_c, \"date_x\")\nget_date_features(test_c, \"date_y\")","068a2ac1":"categorical = test_c.select_dtypes(include=['object'])\ncolumn_names = categorical.columns\n\nembed_feats = categorical.nunique() > 12\nonehot_feats = categorical.nunique() <= 12\n\npd.get_dummies(test_c, columns=test_c[onehot_feats[onehot_feats == True].index].columns, drop_first=True)\ntest_c.drop(columns=test_c[onehot_feats[onehot_feats == True].index].columns, axis=1, inplace=True)\n\ntest_c[embed_feats[embed_feats == True].index]= test_c[embed_feats[embed_feats == True].index].apply(\n    LabelEncoder().fit_transform)","8bb86d00":"test_id = test_c[\"activity_id\"].astype(\"object\")\ntest_c = test_c.drop([\"people_id\", \"activity_id\", \"date_x\", \"date_y\"], axis=1)","20c37e69":"# Check feature number of train and test\nmissingfeatures = list(set(test_c.columns.tolist()) - set(X.columns.tolist()))\nprint(missingfeatures)\n\nprint(len(X.columns))\nprint(len(test_c.columns))","a20a14e4":"results = pd.DataFrame({'activity_id': test_id.values,\n                        'outcome': lgbm.predict_proba(test_c)[:,1]})","5a94aea8":"results.to_csv('redhat_LightGBM.csv', index=False)","9d0f2c67":"#### <a id='section3'>3. Basic exploration<\/a>\n\n**A)** We start with the `people`'s database:","2985c41d":"### Introduction\n\nHi Kagglers,\n\nIn this notebook, I will resolve a binary classification problem with LightGBM. The procedure followed will be quite simple and can be seen as a good tour of all the phases the machine learning problem-solving requires.\n\nThe competition uses two separate data files that may be joined together to create a single, unified data table: a people file and an activity file. This is maybe the main singularity of the set.\n\nAs seen in the data description of Kaggle:\n\nThe people file contains all of the unique people (and the corresponding characteristics) that have performed activities over time. Each row in the people file represents a unique person. Each person has a unique people_id.\n\nThe activity file contains all of the unique activities (and the corresponding activity characteristics) that each person has performed over time. Each row in the activity file represents a unique activity performed by a person on a certain date. Each activity has a unique activity_id.\n\nThe challenge of this competition is to predict the potential business value of a person who has performed a specific activity. The business value outcome is defined by a yes\/no field attached to each unique activity in the activity file. The outcome field indicates whether or not each person has completed the outcome within a fixed window of time after each unique activity was performed.\n\nI would like to thank the job done in these two notebooks. They were a true inspiration:\n\n- Jay Speidell's Red Hat - Exploratory Data Analysis: https:\/\/www.kaggle.com\/jayspeidell\/red-hat-exploratory-data-analysis\n    \n- M.J Wu's LightGBM with Sklearn interface: https:\/\/www.kaggle.com\/wwu651\/lightgbm-with-sklearn-interface","d5ae27a1":"### Index\n\n1. [Import the necessary libraries](#section1)\n2. [Load the data](#section2)\n3. [Merging the two datasets](#section3)\n4. [Basic exploration](#section4)\n5. [Missing values imputation](#section5)\n6. [Date variables manipulation](#section6)\n7. [Categorical columns treatment](#section7)\n8. [Model implementation](#section8)\n9. [Prediction and submission](#section9)","5923305a":"Let us focus now on our target or dependent variable:","1d0db024":"We can clearly see that the `char_38` is the only numeric feature. Let us futher analyze this variable:","449bd8da":"From `char_1` to `char_9` we find out that there are a lot of missing values. They are in the same observations so there could be a pattern or reason for this happening. The best way to proceed here is to categorize these `NaNs`. In other words, create a new category to include them. With `Char_10` happens the same on a lower scale. We will perform this categorization in the next section.\n\nWith that many missing values a common imputation, like the median or mean imputation would include too much noise, reducing by a great deal the variability of the data.","09014179":"#### <a id='section1'>1. Import the necessary libraries<\/a>","ecbd638e":"We have to make all the preprocessing to the test set before:","b2df2a6f":"Contrary to the `people's` set, here we have an important number of missing values. Therefore we will need to impute these values.","8ab4c4c9":"#### <a id='section5'>5. Missing values imputation<\/a>\n\nThere are way too much missing values in some columns, as introduced before we will categorize these values:","8740eb0e":"#### <a id='section3'>3. Merging the two datasets<\/a>","937483ac":"Prediction time:","d704615d":"#### <a id='section9'>9. Prediction and submission<\/a>","017e146f":"Label encode those variables with more distinct values:","c45610ed":"#### <a id='section7'>7. Categorical columns treatment<\/a>","465e26b8":"#### <a id='section2'> 2. Load the data <\/a>","d71d069f":"![red-hat](https:\/\/www.channelpartnerinsight.com\/w-images\/794d78dd-c155-45a5-af67-59b64ad64873\/3\/IBMRedHat-580x358.jpg)","a4fcab52":"Alright, it seems that is a variable with high variance and good predictive potential for our target. We will keep it for sure.\n\nLet us continue with the exploration of the rest of the variables, the categorical ones:","640c4cd0":"#### <a id='section8'>8. Model implementation<\/a>","8c77c8da":"Our target is enought balanced to not have any problems with it. Its dtype is an `int64` even tough it is clearly a binary variable.","e4ca5324":"# <center> Predicting Business Value with LightGBM<\/center>\n## <center> From the Red Hat competition database <center>","c8d3ac0d":"#### <a id='section6'>6. Date variables manipulation<\/a>\nWe have two options here. Put it as a numerical value with reference of the minimum date: like minutes\/seconds from minimum date. Or, on the other hand, we can make different categorical values of the year\/day of the year\/weekday\/hour\/minutes etc. We will follow this second option:","3907a513":"**B)** We continue with the `activity`'s set exploration:","8cbc741e":"The way to proceed here is to one-hot-encode the categorical variables, dropping the first values. The only problem here is that, as we said before, there are features with a lot of distinct values, which would increase the dimensionality of our dataset by a huge deal. So we will only create dummies for those variables with less than 13 different values.\n\nThe rest of the features will be label-encoded.","5c0f9613":"There are features like `group_1` which are categorical but they have too many distinct values. This will be problematic if we want to One Hot Encode in the future. Date should also be converted to a Date dtype, or perform some sort of feature engineering with it.\n\nOn the other side there are some features highly unbalanced:"}}