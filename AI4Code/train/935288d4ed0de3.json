{"cell_type":{"b0b989f6":"code","77c20114":"code","d60cd82d":"code","57fbc949":"code","b35341f9":"code","03c2fde6":"code","0ccce16f":"code","7661c747":"code","9a744b16":"code","a1710639":"code","8363911a":"code","6ad56e53":"code","eab6d5f7":"code","fd355c3b":"code","2cf25c17":"code","204050a8":"code","9227dea3":"code","e0457b20":"code","e6dd4b61":"code","89f43f60":"code","38f47ac6":"code","9a61dc56":"code","fbb84bb2":"code","6f6047ba":"code","94bf9e2f":"code","52a42579":"code","e7bbffa1":"code","578281e5":"code","2a692079":"code","7d7492b0":"code","18691207":"code","09a94d1d":"code","43518b84":"code","3e5560d3":"code","dd91e6d4":"code","4d6c169e":"code","f73f5b8c":"code","b1de5e2d":"markdown","8d41f525":"markdown","5b18de7d":"markdown","7179b410":"markdown","85412541":"markdown","f47ebeff":"markdown","c24248d9":"markdown","148f9e52":"markdown","39a3e5d9":"markdown","03179541":"markdown","1d577954":"markdown","f97d08a4":"markdown","477f9001":"markdown","025b5989":"markdown","00929f50":"markdown","8d07f518":"markdown","38079034":"markdown","7993b5ba":"markdown","691d0a28":"markdown","8e609489":"markdown","f077b2c7":"markdown","05dd7815":"markdown","059cf52f":"markdown","64df9035":"markdown","f69ec9da":"markdown","9b8754df":"markdown","1e6d751b":"markdown","300afdc6":"markdown","37be338b":"markdown","223a23a7":"markdown","85599cba":"markdown","9a837dc6":"markdown","18715816":"markdown","bbe9a774":"markdown","d494aa73":"markdown","3a133c55":"markdown","650a0ad8":"markdown","229605c6":"markdown"},"source":{"b0b989f6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_columns', 500)","77c20114":"data_ori = pd.read_csv('..\/input\/daily_electricity_usage.csv')\ndata_ori['date'] = pd.to_datetime(data_ori['date'])","d60cd82d":"data_ori.head()","57fbc949":"data = pd.DataFrame({'date':pd.date_range('2009-07-14',periods=536,freq='D',)})\nfor i in range(1000,7445):\n    S=data_ori[data_ori['Meter ID']==i][['date','total daily KW']]\n    data=pd.merge(data,S,how='left',on='date')\nfor i in range(1,6446):\n    data.columns.values[i]=\"ID\"+str(999+i)","b35341f9":"data.head()","03c2fde6":"data.isnull().sum().sum()","0ccce16f":"data = data.fillna(data.mean())","7661c747":"data.date = pd.to_datetime(data.date)\ndata['day'] = data['date'].apply(lambda x:x.weekday())\nx_call = data.columns[1:-1]","9a744b16":"data_fix = pd.DataFrame({'Meter ID':range(1000,7445,1),'total KW':np.sum(data[x_call]).values})\ndata_fix['average per day']=data[x_call].mean().values\ndata_fix['% Monday']=data[data['day']==0][x_call].sum().values\/data_fix['total KW']*100\ndata_fix['% Tuesday']=data[data['day']==1][x_call].sum().values\/data_fix['total KW']*100\ndata_fix['% Wednesday']=data[data['day']==2][x_call].sum().values\/data_fix['total KW']*100\ndata_fix['% Thursday']=data[data['day']==3][x_call].sum().values\/data_fix['total KW']*100\ndata_fix['% Friday']=data[data['day']==4][x_call].sum().values\/data_fix['total KW']*100\ndata_fix['% Saturday']=data[data['day']==5][x_call].sum().values\/data_fix['total KW']*100\ndata_fix['% Sunday']=data[data['day']==6][x_call].sum().values\/data_fix['total KW']*100\ndata_fix['% weekday']=data[(data['day']!=5)&(data['day']!=6)][x_call].sum().values\/data_fix['total KW']*100\ndata_fix['% weekend']=data[(data['day']==5)|(data['day']==6)][x_call].sum().values\/data_fix['total KW']*100","a1710639":"data_fix=data_fix.fillna(0)\ndata_fix.head()","8363911a":"from sklearn.preprocessing import StandardScaler\nx_calls = data_fix.columns[1:]\nscaller = StandardScaler()\nmatrix = pd.DataFrame(scaller.fit_transform(data_fix[x_calls]),columns=x_calls)\nmatrix['Meter ID'] = data_fix['Meter ID']\nprint(matrix.head())","6ad56e53":"corr = matrix[x_calls].corr()\nfig, ax = plt.subplots(figsize=(8, 6))\ncax=ax.matshow(corr,vmin=-1,vmax=1)\nax.matshow(corr)\nplt.xticks(range(len(corr.columns)), corr.columns)\nplt.yticks(range(len(corr.columns)), corr.columns)\nplt.xticks(rotation=90)\nplt.colorbar(cax)","eab6d5f7":"def plot_BIC(matrix,x_calls,K):\n    from sklearn import mixture\n    BIC=[]\n    for k in K:\n        model=mixture.GaussianMixture(n_components=k,init_params='kmeans')\n        model.fit(matrix[x_calls])\n        BIC.append(model.bic(matrix[x_calls]))\n    fig, ax = plt.subplots(figsize=(8, 6))\n    plt.plot(K,BIC,'-cx')\n    plt.ylabel(\"BIC score\")\n    plt.xlabel(\"k\")\n    plt.title(\"BIC scoring for K-means cell's behaviour\")\n    return(BIC)","fd355c3b":"K = range(2,31)\nBIC = plot_BIC(matrix,x_calls,K)","2cf25c17":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\ncluster = KMeans(n_clusters=5,random_state=217)\nmatrix['cluster'] = cluster.fit_predict(matrix[x_calls])\nprint(matrix.cluster.value_counts())","204050a8":"d=pd.DataFrame(matrix.cluster.value_counts())\nfig, ax = plt.subplots(figsize=(8, 6))\nplt.bar(d.index,d['cluster'],align='center',alpha=0.5)\nplt.xlabel('Cluster')\nplt.ylabel('number of data')\nplt.title('Cluster of Data')","9227dea3":"from sklearn.metrics.pairwise import euclidean_distances\ndistance = euclidean_distances(cluster.cluster_centers_, cluster.cluster_centers_)\nprint(distance)","e0457b20":"# Reduction dimention of the data using PCA\npca = PCA(n_components=3)\nmatrix['x'] = pca.fit_transform(matrix[x_calls])[:,0]\nmatrix['y'] = pca.fit_transform(matrix[x_calls])[:,1]\nmatrix['z'] = pca.fit_transform(matrix[x_calls])[:,2]\n\n# Getting the center of each cluster for plotting\ncluster_centers = pca.transform(cluster.cluster_centers_)\ncluster_centers = pd.DataFrame(cluster_centers, columns=['x', 'y', 'z'])\ncluster_centers['cluster'] = range(0, len(cluster_centers))\nprint(cluster_centers)","e6dd4b61":"# Plotting for 2-dimention\nfig, ax = plt.subplots(figsize=(8, 6))\nscatter=ax.scatter(matrix['x'],matrix['y'],c=matrix['cluster'],s=21,cmap=plt.cm.Set1_r)\nax.scatter(cluster_centers['x'],cluster_centers['y'],s=70,c='blue',marker='+')\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.colorbar(scatter)\nplt.title('Data Segmentation')","89f43f60":"# Plotting for 3-Dimention\nfig, ax = plt.subplots(figsize=(8, 6))\nax=fig.add_subplot(111, projection='3d')\nscatter=ax.scatter(matrix['x'],matrix['y'],matrix['z'],c=matrix['cluster'],s=21,cmap=plt.cm.Set1_r)\nax.scatter(cluster_centers['x'],cluster_centers['y'],cluster_centers['z'],s=70,c='red',marker='+')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nplt.colorbar(scatter)\nplt.title('Data Segmentation')","38f47ac6":"data_fix['cluster']=matrix['cluster']\nprint(data_fix[data_fix.columns[1:]].groupby(['cluster']).agg([np.mean]))","9a61dc56":"list(data_fix[data_fix.cluster==2]['Meter ID'])","fbb84bb2":"data_cluster=data_fix[['Meter ID','cluster']]\ndata_forc=pd.DataFrame({'ds':pd.to_datetime(data['date'])})","6f6047ba":"for k in range(len(cluster_centers)):\n    data_clus=data_cluster[data_cluster['cluster']==k]\n    del data_clus['cluster']\n    s1=\"cluster \"+str(k)\n    data_forc[s1]=0\n    for i in list(data_clus.iloc[:,0]):\n        s2=\"ID\"+str(i)\n        data_forc[s1]+=data[s2]\ndata_forc=data_forc.fillna(0)","94bf9e2f":"data_forc_0=data_forc[['ds','cluster 0']]\ndata_forc_0.columns=['ds','y']\n\ndata_forc_1=data_forc[['ds','cluster 1']]\ndata_forc_1.columns=['ds','y']\n\ndata_forc_2=data_forc[['ds','cluster 2']]\ndata_forc_2.columns=['ds','y']\n\ndata_forc_3=data_forc[['ds','cluster 3']]\ndata_forc_3.columns=['ds','y']\n\ndata_forc_4=data_forc[['ds','cluster 4']]\ndata_forc_4.columns=['ds','y']\n\ndata_forc_all=pd.DataFrame({'ds':data_forc['ds']})\ndata_forc_all['y']=data_forc['cluster 0']+data_forc['cluster 1']+data_forc['cluster 2']+data_forc['cluster 3']+data_forc['cluster 4']","52a42579":"def plot_data(data_forc):\n    timeseries=data_forc.copy()\n    timeseries.columns=['date','Total Daily KW']\n    timeseries = timeseries.set_index('date') \n    fig, ax = plt.subplots(figsize=(8, 6))\n    plt.scatter(timeseries.index,timeseries['Total Daily KW'],c='black',s=2)","e7bbffa1":"import fbprophet\nfrom sklearn.metrics import mean_squared_error, r2_score\ndef predic_fbp(data_forc,n_days):\n    ny=pd.DataFrame({'holiday':\"New Year's Day\",'ds':pd.to_datetime(['2010-01-01','2011-01-01','2012-01-01']),\n                     'lower_window':-1,'upper_window':1,})\n    ch=pd.DataFrame({'holiday':\"Christmas\",'ds':pd.to_datetime(['2009-12-25','2010-12-25','2011-12-25','2012-12-25']),\n                     'lower_window':0,'upper_window':1,})\n    holidays=pd.concat([ny,ch])\n    model = fbprophet.Prophet(daily_seasonality=False,weekly_seasonality=True,\n                yearly_seasonality=True,changepoint_prior_scale=0.05,changepoints=None,\n                holidays=holidays,interval_width=0.95)\n    model.add_seasonality(name='monthly',period=30.5,fourier_order=5)\n    size = len(data_forc) - n_days\n    train, test = data_forc[0:size], data_forc[size:]\n    test_=test.set_index('ds')\n    model.fit(train)\n    predics=model.predict(data_forc)\n    test=pd.merge(test,predics[['ds','yhat','yhat_lower','yhat_upper']],how='left',on='ds')\n    train=pd.merge(train,predics[['ds','yhat','yhat_lower','yhat_upper']],how='left',on='ds')\n    RMSE=np.sqrt(mean_squared_error(test['y'], test['yhat']))\n    print('RMSE = %.2f' % RMSE)\n    R2=r2_score(test['y'], test['yhat'])\n    print('R Square = %.2f'% R2)\n    future = model.make_future_dataframe(periods=365+n_days, freq='D')\n    future=model.predict(future)\n    fig=model.plot(predics)\n    plt.scatter(test_.index,test_['y'],c='black',s=7)\n    fig2=model.plot(future)\n    plt.scatter(test_.index,test_['y'],c='black',s=7)\n    fig3=model.plot_components(future)\n    return(train,test,predics,future,RMSE,R2)","578281e5":"plot_data(data_forc_0)","2a692079":"train_0,test_0,predics_0,future_0,RMSE_0,R2_0=predic_fbp(data_forc_0,90)","7d7492b0":"plot_data(data_forc_1)","18691207":"train_1,test_1,predics_1,future_1,RMSE_1,R2_1=predic_fbp(data_forc_1,90)","09a94d1d":"plot_data(data_forc_3)","43518b84":"train_3,test_3,predics_3,future_3,RMSE_3,R2_3=predic_fbp(data_forc_3,90)","3e5560d3":"plot_data(data_forc_4)","dd91e6d4":"train_4,test_4,predics_4,future_4,RMSE_4,R2_4=predic_fbp(data_forc_4,90)","4d6c169e":"plot_data(data_forc_all)","f73f5b8c":"train_all,test_all,predics_all,future_all,RMSE_all,R2_all=predic_fbp(data_forc_all,90)","b1de5e2d":"# Loading Library and Data","8d41f525":"The dataset contains 163.262 missing data because:\n1. there are new costumers in the middle of observation period;\n2. there are some Meter IDs between 1000-7444 not observed; or\n3. there are some meter IDs that stop to be member in the middle of observation period.","5b18de7d":"As we can guess \"total KW\" has strong positive correlation with \"average per day\". Beside that, \"% Saturday\" and \"% Sunday\" also have strong positive correlation with \"% weekend\" and negative correlation with \"% weekday\". So does \"% Monday\" until \"% Friday\" have positive corralation with \"% weekday\" and \"% weekend\".","7179b410":"# 1. Segmentation","85412541":"In this stage for this kernel, we try to forecast for a year ahead only the electricity usage of each segment, except Cluster 2, and the total electricity usage of all costumers using a simple and very nice library developed by Facebook name[](http:\/\/)d Fbprophet. The theory with a very good description of the math\/statistical approach behind the library can be seen in https:\/\/facebook.github.io\/prophet\/.","f47ebeff":"# Overview","c24248d9":"## The Forecasting of Cluster 4","148f9e52":"Overall, the trend of electricity usage is linearly decreasing with the highest usage in January and the lowest in about June. R Squared of this prediction is good enough 79% but the RMSE is 12125.71.","39a3e5d9":"## Handling missing data","03179541":"## The Forecasting of Cluster 0","1d577954":"## Preparing features for Segmentation ","f97d08a4":"## Clustering ","477f9001":"The first segment (Cluster 0) contains 95 costumers, the second (Cluster 1) 3747 costumers, the third (Cluster 2) 10 costumers, the fourth (Cluster 3) 2208 costumers, and the fifth (Cluster 4) 385 costumers.","025b5989":"In forecasting, we use 446 first days, from July 14 2009 to October 02 2010, as a training data and 90 last day , from October 03 2010 to December 31 2010, as a validation data. We also added the holiday effects, Chistmas at December 25-26 and New Year at December 31 - January 02, on the model.\n\nFor accuracy, we use Root Mean Square Error (RMSE) and R-Squared (R2) to asses the model.","00929f50":"This kernel was made to provide a repository of applying clustering and forecasting concept. Hopefully, it will be usefull for the all readers. Thank you.\n\nTable of contents:\n\n    Data overview\n    Loading library and data\n    Segmentation\n        Pre-processing data for segmentation\n        Determining the number of cluster\n        Clustering\n        Visualization of segments\n        Analyzing each segment\n    Forecasting Single time-series\n        Prepocessing Data for forecasting\n        Forecasting electricity usage of each segment using Fbprophet\n        Forecasting electricity usage of all costumers using Fbprophet","8d07f518":"## Developing Function for Modelling ","38079034":" ## The behavior of each segment:","7993b5ba":"## Splitting data per costumer ","691d0a28":"We keep the outlier so the costumers from big company or too small housing not be eliminated.","8e609489":"The trend of electricity usage in this cluster is also decreasing with the same highest usage with Cluster 0 in December and January and the lowest in about July. R Squared of this prediction is good 84% and the RMSE is about 5531.48.","f077b2c7":"## Preprocessing data","05dd7815":"## Determining the number of cluster","059cf52f":"By the plots above, we can see that all segments are separated well from each other. It means that BIC method works good for this project.","64df9035":"# 2. Forecasting Using Fbprophet","f69ec9da":"## The Forecasting of Cluster 3","9b8754df":"1. Cluster 0 contains only 95 IDs but the average of daily usage and total KW is so high, 381.55 kWh and 304513.39 kWh respectively. They also have activities in all days, the highest in Thursday and Friday. We can guess that Cluster 0 comes from big companies.\n\n2. Cluster 1 is a biggest segment in this project contains 3747 IDs. The average of total electricity usage in observation period is 15161.06 kWh and the average of daily electricity usage is 28.28 kWh. From the percentage of daily usage in weekend or weekdays, we can see that there is no significant different between them. The consumption is about 14%. By this behaviour we can guess that the IDs in Cluster 0 comes from housing or small company that has same activities in all days.\n\n3. Cluster 2 contains 10 IDs that is not observed in this project. It can be seen from the total of electricity usage that is 0 kWh. The IDs not observed in this project are 2083, 2691, 3141, 3348, 4096, 4113, 4447, 5855, 6596, dan 6713.\n\n4. Cluster 4 has 2208 IDs and similar behavior with Cluster 1. It is confirmed by the distance between those centroids. Not only that, the similar behavior also can be seen from the daily electricity usage and the total consumption 26.4 kWh and 13955.61 kWh respectively. However, the percentage of daily using is little bit different eith Cluster 0, in weekdays the percentage is about 13% per day and it increases in weekend becomes about 16%. We can guess that those IDs are constumers comes from housing or small companies who have more activities in weekend rather than weekdays.\n\n5. The IDs in Cluster 5 comes from middle companies who only have activities in weekdays. We can see from the behavior of daily electricity usage where there is significant different between weekdays and weekend. In weekdays, the percentage of electricity usage is about 17% and it become slighly decrease in Monday and Friday. However,it got dramatically decrease in weekend about only 7%.","1e6d751b":"By Bayessian Information Criterion (BIC), we decided to segmentate the costumers to be 5 segments.","300afdc6":"## The Forecasting of All Costumers","37be338b":"The trend of electricity usage in this cluster is also linearly decreasing with the highest usage in January and the lowest in about August. R Squared of this prediction is good enough 77% and the RMSE is 4589.76.","223a23a7":"## Standardization Data ","85599cba":"# 1. Preprocessing Data","9a837dc6":"## Correlation ","18715816":"The given dataset contains 6445 IDs from ID 1000 to ID 7444. The dataset provide daily electricity usage from July 14, 2009 to December 31, 2010. The costumers are not only from housing but also companys.","bbe9a774":"## Visualization Segment","d494aa73":"We built 11 variables to detect the consumption behavior of every costumers. Those are:\n1. Total consumption in the observation period (total KW);\n2. The average of daily electricity usage (average per day);\n3. The percentage of total consumption on Monday (% Monday);\n4. The percentage of total consumption on Tuesday (% Tuesday);\n5. The percentage of total consumption on Wednesday (% Wednesday);\n6. The percentage of total consumption on Thursday (% Thursday);\n7. The percentage of total consumption on Friday (% Friday);\n8. The percentage of total consumption on Saturday (% Saturday);\n9. The percentage of total consumption on Sunday (% Sunday);\n10. The percentage of total consumption on Weekday (% weekday); and\n11. The percentage of total consumption on Weekend (% weekend).","3a133c55":"## The Forecasting of Cluster 1","650a0ad8":"The trend of electricity usage in this cluster is also linearly decreasing with the highest usage in January and the lowest in about June. R Squared of this prediction is good 82% and the RMSE is 4439.82.","229605c6":"The trend of electricity usage in this cluster is linearly decreasing with the highest usage in December and January and the lowest in June. R Squared of this prediction is not really good only 64% but the RMSE is good with 1637.01."}}