{"cell_type":{"97608a43":"code","c18cc509":"code","b5049a13":"code","22cbb5db":"code","e0375613":"code","d4c235c9":"code","94b0cdbf":"code","4a02f76d":"code","4d317a04":"code","e89086ec":"code","4fb5395b":"code","2ef1b447":"code","fbb19f63":"code","8a38abe5":"code","60f998a2":"code","9b987c8c":"code","ac618a35":"code","c31f8167":"code","1c323f8a":"code","f4e0a98f":"code","78a6cbaf":"code","f0207fd5":"code","bb064f10":"code","2a68e47e":"code","e4114740":"code","0d425d04":"code","fab3a90b":"code","07996006":"code","183b1e56":"code","ce802d8b":"code","a1706a57":"code","0ef5701f":"code","98d07870":"code","ba3e9bc8":"code","599dee8c":"code","0c41c787":"markdown","47873eb7":"markdown","4689d933":"markdown","1d23fdac":"markdown","29a46db3":"markdown","cd68df60":"markdown","ecb3c439":"markdown","2281a378":"markdown","6bc0cec5":"markdown","8d74b52c":"markdown","67ec3262":"markdown","580d477e":"markdown","0301ef97":"markdown","c7181685":"markdown","77e6c08c":"markdown","a7611d6d":"markdown","57377e4a":"markdown","16b2b62d":"markdown","8c37b105":"markdown","f3ddef13":"markdown","5470c089":"markdown","72b9de1f":"markdown","4d5331ca":"markdown","6b3c11a1":"markdown","8f868f1d":"markdown","e0b67e25":"markdown","5c5c45ac":"markdown","b879f5b7":"markdown","7b88325e":"markdown","8fd12ddf":"markdown","2a146d2e":"markdown","86873505":"markdown","f85383c9":"markdown","a3dcaba4":"markdown","29bca1ed":"markdown","7b664c66":"markdown","749063cd":"markdown","8e7241c3":"markdown","3f20cad4":"markdown","16c0a5ef":"markdown","62b889ea":"markdown","52afc7ea":"markdown","2d49e8a1":"markdown","c8b36655":"markdown","079bbb86":"markdown","e21dcfb5":"markdown","e3895c64":"markdown","85a4511a":"markdown","a6422673":"markdown","50329da5":"markdown","b57a6f2f":"markdown","3b526cc7":"markdown","4fb5650a":"markdown","58464901":"markdown","949cac1a":"markdown","17908e91":"markdown","aaef5d68":"markdown","0b13becd":"markdown","d733c73f":"markdown","59ea93d1":"markdown","eedd07d6":"markdown","ccdfceeb":"markdown","c3a714ea":"markdown"},"source":{"97608a43":"!pip install pmdarima","c18cc509":"#Basic Libraries\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom datetime import datetime    \nfrom pandas import Series \nimport statsmodels.api as sm\n\n#Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nsns.set_style(\"whitegrid\")\n%matplotlib inline\nimport altair as alt \nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Timeseries model libraries\nfrom statsmodels.tsa.stattools import adfuller\nfrom pmdarima import auto_arima\nfrom fbprophet import Prophet\n\n#Performance metric libraries\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet.diagnostics import cross_validation\nfrom fbprophet.diagnostics import performance_metrics\nfrom fbprophet.plot import plot_cross_validation_metric\n\n\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')","b5049a13":"reliance_raw=pd.read_csv(\"..\/input\/nifty50-stock-market-data\/RELIANCE.csv\")\n\n## print shape of dataset with rows and columns and information \nprint (\"The shape of the  data is (row, column):\"+ str(reliance_raw.shape))\nprint (reliance_raw.info())","22cbb5db":"reliance_raw.head()","e0375613":"reliance_raw.dtypes","d4c235c9":"#Checking out the statistical measures\nreliance_raw.describe()","94b0cdbf":"#Creating a copy\nreliance_analysis=reliance_raw.copy()\n\n#Coverting date column to datetime data type\nreliance_analysis['Date'] = reliance_analysis['Date'].apply(pd.to_datetime)\n\n#Extracting Month, Week, Day,Day of week\nreliance_analysis[\"Month\"] = reliance_analysis.Date.dt.month\nreliance_analysis[\"Week\"] = reliance_analysis.Date.dt.week\nreliance_analysis[\"Day\"] = reliance_analysis.Date.dt.day\nreliance_analysis[\"Day of week\"] = reliance_analysis.Date.dt.dayofweek\n\n\n#Setting date column as index\nreliance_analysis.set_index(\"Date\", drop=False, inplace=True)\nreliance_analysis.iloc[:,15:19].head()","4a02f76d":"#Imputing null values with mean \nreliance_analysis.fillna(reliance_analysis.mean(),inplace=True)\n\n#Checking for null values\nreliance_analysis.isnull().sum()","4d317a04":"#Size and style of the plot\nplt.figure(figsize = (15, 7))\nplt.style.use('seaborn-white')\n\n#Subplots of distplot\nplt.subplot(231)\nsns.distplot(reliance_analysis['Prev Close'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(232)\nsns.distplot(reliance_analysis['Open'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(233)\nsns.distplot(reliance_analysis['High'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(234)\nsns.distplot(reliance_analysis['Low'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(235)\nsns.distplot(reliance_analysis['Close'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(236)\nsns.distplot(reliance_analysis['VWAP'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)","e89086ec":"fig = px.line(reliance_analysis, x='Date', y='VWAP',title='VWAP over Years(Use Rangeslider to slide over time)')\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.show()","4fb5395b":"cols_plot = ['Open', 'Close', 'High','Low']\naxes = reliance_analysis[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(11, 9), subplots=True)\nfor ax in axes:\n    ax.set_ylabel('Daily trade')","2ef1b447":"ax=reliance_analysis[['Volume']].plot(stacked=True)\nax.set_title('Volume over years',fontsize= 30)\nax.set_xlabel('Year',fontsize = 20)\nax.set_ylabel('Volume',fontsize = 20)\nplt.show()","fbb19f63":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n         x=reliance_analysis['Date'],\n         y=reliance_analysis['Open'],\n         name='Open',\n    line=dict(color='blue'),\n    opacity=0.8))\n\nfig.add_trace(go.Scatter(\n         x=reliance_analysis['Date'],\n         y=reliance_analysis['Close'],\n         name='Close',\n    line=dict(color='red'),\n    opacity=0.8))\n\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\n        \n    \nfig.update_layout(title_text='Open Vs Close',plot_bgcolor='rgb(248, 248, 255)',yaxis_title='Value')\n\nfig.show()\n","8a38abe5":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n         x=reliance_analysis['Date'],\n         y=reliance_analysis['High'],\n         name='High',\n    line = dict(color='green', width=4, dash='dot'),\n    opacity=0.8))\n\nfig.add_trace(go.Scatter(\n         x=reliance_analysis['Date'],\n         y=reliance_analysis['Low'],\n         name='Low',\n    line=dict(color='orange', width=4, dash='dot'),\n    opacity=0.8))\n\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\n        \n    \nfig.update_layout(title_text='High Vs Low',plot_bgcolor='rgb(248, 248, 255)',yaxis_title='Value')\n\nfig.show()\n","60f998a2":"#Making a copy\nreliance_lag=reliance_analysis.copy()\n#Reset index\nreliance_lag.reset_index(drop=True, inplace=True)\n#Creating lag features\nlag_features = [\"High\", \"Low\", \"Volume\", \"VWAP\"]\n\n# Taking the number of days in window\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\n#Rolling mean\ndf_rolled_3d = reliance_lag[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = reliance_lag[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = reliance_lag[lag_features].rolling(window=window3, min_periods=0)\n\n#Moving average\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n\n#Standard deviation\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n\n# Adding the features to the dataframe\nfor feature in lag_features:\n    reliance_lag[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    reliance_lag[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    reliance_lag[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    reliance_lag[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    reliance_lag[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    reliance_lag[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\nreliance_lag.fillna(reliance_lag.mean(), inplace=True)\n\n#Setting Date as index\nreliance_lag.set_index(\"Date\", drop=False, inplace=True)\nreliance_lag.head()","9b987c8c":"#Printing the high curve\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n         x=reliance_lag['Date'],\n         y=reliance_lag['High'],\n         name='High',\n    line=dict(color='green'),\n    opacity=0.8))\n\n#Printing the low curve\nfig.add_trace(go.Scatter(\n         x=reliance_lag['Date'],\n         y=reliance_lag['Low'],\n         name='Low',\n    line=dict(color='orange'),\n    opacity=0.8))\n\n#Printing the high lag mean-30 days curve\nfig.add_trace(go.Scatter(\n         x=reliance_lag['Date'],\n         y=reliance_lag['High_mean_lag30'],\n         name='High_mean_lag30',\n    line=dict(color='red'),\n    opacity=0.8))\n\n#Printing the high lag standard deviation-30 days curve\nfig.add_trace(go.Scatter(\n         x=reliance_lag['Date'],\n         y=reliance_lag['High_std_lag30'],\n         name='High_std_lag30',\n    line=dict(color='royalblue'),\n    opacity=0.8))\n\n#Printing the low lag mean-30 days curve\nfig.add_trace(go.Scatter(\n         x=reliance_lag['Date'],\n         y=reliance_lag['Low_mean_lag30'],\n         name='Low_mean_lag30',\n    line=dict(color='yellow'),\n    opacity=0.8))\n\n#Printing the low lag standard deviation-30 days curve\nfig.add_trace(go.Scatter(\n         x=reliance_lag['Date'],\n         y=reliance_lag['Low_std_lag30'],\n         name='Low_std_lag30',\n    line=dict(color='pink'),\n    opacity=0.8))\n\n#Updating the time axis\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\n        \n#Update the title   \nfig.update_layout(title_text='High Vs Low with mean lag and standard deviation lag',plot_bgcolor='rgb(248, 248, 255)',yaxis_title='Value')\n\nfig.show()","ac618a35":"#Printing the Volume curve\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n         x=reliance_lag['Date'],\n         y=reliance_lag['Volume'],\n         name='Volume',\n    line=dict(color='green'),\n    opacity=0.8))\n\n#Printing the Volume_mean_lag-30 days curve\nfig.add_trace(go.Scatter(\n         x=reliance_lag['Date'],\n         y=reliance_lag['Volume_mean_lag30'],\n         name='Volume_mean_lag30',\n    line=dict(color='yellow'),\n    opacity=0.8))\n#Printing the Volume_std_lag30 curve\nfig.add_trace(go.Scatter(\n         x=reliance_lag['Date'],\n         y=reliance_lag['Volume_std_lag30'],\n         name='Volume_std_lag30',\n    line=dict(color='blue'),\n    opacity=0.8))\n\n#Updating time axis\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\n        \n#Updating layout \nfig.update_layout(title_text='Volume with mean lag and standard deviation lag',plot_bgcolor='rgb(248, 248, 255)',yaxis_title='Value')\n\nfig.show()\n","c31f8167":"#Setting the range of base plot\nfig = px.line(reliance_analysis, x='Date', y='Volume',title='Volume during Phase 1 Lockdown(25 March \u2013 14 April) and Phase 2 Lockdown (15 April \u2013 3 May)', range_x=['2020-01-01','2020-06-30'])\n\n# Adding the shape in the dates\nfig.update_layout(\n    shapes=[\n        # First phase Lockdown\n        dict(\n            type=\"rect\",\n            xref=\"x\",\n            yref=\"paper\",\n            x0=\"2020-03-23\",\n            y0=0,\n            x1=\"2020-04-14\",\n            y1=1,\n            fillcolor=\"LightSalmon\",\n            opacity=0.5,\n            layer=\"below\",\n            line_width=0,\n        ),\n        # Second phase Lockdown\n        dict(\n            type=\"rect\",\n            xref=\"x\",\n            yref=\"paper\",\n            x0=\"2020-04-15\",\n            y0=0,\n            x1=\"2020-05-03\",\n            y1=1,\n            fillcolor=\"Green\",\n            opacity=0.5,\n            layer=\"below\",\n            line_width=0,\n        )],\n    annotations=[dict(x='2020-04-15', y=0.99, xref='x', yref='paper',\n                    showarrow=False, xanchor='right', text='Phase 1 Lockdown'),\n                 dict(x='2020-05-12', y=0.99, xref='x', yref='paper',\n                    showarrow=False, xanchor='right', text='Phase 2 Lockdown')])\n\nfig.show()","1c323f8a":"fig = px.line(reliance_analysis, x='Date', y='VWAP',title='VWAP after lockdown', range_x=['2020-03-23','2020-06-30'])\nfig.show()","f4e0a98f":"reliance_analysis_lockdown = reliance_analysis[reliance_analysis['Date'] >= '2020-03-23']\nfig = go.Figure(data=[go.Candlestick(x=reliance_analysis_lockdown['Date'],\n                open=reliance_analysis_lockdown['Open'],\n                high=reliance_analysis_lockdown['High'],\n                low=reliance_analysis_lockdown['Low'],\n                close=reliance_analysis_lockdown['Close'])])\n\nfig.show()","78a6cbaf":"#Setting the date range for the base plot\nfig = px.line(reliance_analysis, x='Date', y='VWAP', title='Major Corporate Announcements 2020(Till June 30)',range_x=['2020-01-01','2020-06-30'])\n\n#Creating the line and news on announcement dates\nfig.update_layout(plot_bgcolor='rgb(250, 242, 242)',\n    yaxis_title='NIFTY 50 VWAP',\n    shapes = [dict(x0='2020-03-23', x1='2020-03-23', y0=0, y1=1, xref='x', yref='paper', line_width=2,opacity=0.3,line_color='green',editable=False),\n             dict(x0='2020-04-22', x1='2020-04-22', y0=0, y1=1, xref='x', yref='paper',line_width=3,opacity=0.3,line_color='green'),\n             dict(x0='2020-05-04', x1='2020-05-04', y0=0, y1=1, xref='x', yref='paper',line_width=3,opacity=0.3,line_color='green'),\n             dict(x0='2020-05-17', x1='2020-05-17', y0=0, y1=1, xref='x', yref='paper',line_width=3,opacity=0.3,line_color='green'),\n             dict(x0='2020-03-30', x1='2020-03-30', y0=0, y1=1, xref='x', yref='paper',line_width=3,opacity=0.3,line_color='green')],\n    annotations=[dict(x='2020-03-23', y=0.54, xref='x', yref='paper',\n                    showarrow=False, xanchor='right', text='Reliance support to Fight Against Coronavirus'),\n                 dict(x='2020-04-22', y=0.5, xref='x', yref='paper',\n                    showarrow=False, xanchor='left', text='Facebook invested in Jio Platforms '),\n                dict(x='2020-05-04', y=0.08, xref='x', yref='paper',\n                    showarrow=False, xanchor='left', text='Silver Lake invested in Jio Platforms'),\n                 dict(x='2020-05-17', y=0.05, xref='x', yref='paper',\n                    showarrow=False, xanchor='left', text='General Atlantic invested in Jio Platforms'),\n                 dict(x='2020-03-30', y=0.09, xref='x', yref='paper',\n                    showarrow=False, xanchor='right', text='Rs. 500 Crore Contribution to PM CARES Fund'),\n               ]\n)\nfig.show()","f0207fd5":"reliance_stationarity=reliance_analysis[['Close']]\n\nreliance_stationarity.plot()","bb064f10":"test_result=adfuller(reliance_stationarity['Close'])\n\n#Ho: Data is non stationary\n#H1: Data is stationary\n\ndef adfuller_test(price):\n    result=adfuller(price)\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\nadfuller_test(reliance_stationarity['Close'])","2a68e47e":"reliance_stationarity['Close First Difference']=reliance_stationarity['Close']-reliance_stationarity['Close'].shift(1)\nreliance_stationarity['Close First Difference'].plot()","e4114740":"train = reliance_lag[reliance_lag.Date < \"2019\"]\nvalid = reliance_lag[reliance_lag.Date >= \"2019\"]","0d425d04":"exogenous_features = ['High_mean_lag3','High_mean_lag7', 'High_mean_lag30', 'High_std_lag3', 'High_std_lag7',\n       'High_std_lag30', 'Low_mean_lag3', 'Low_mean_lag7', 'Low_mean_lag30',\n       'Low_std_lag3', 'Low_std_lag7', 'Low_std_lag30', 'Volume_mean_lag3',\n       'Volume_mean_lag7', 'Volume_mean_lag30', 'Volume_std_lag3',\n       'Volume_std_lag7', 'Volume_std_lag30', 'VWAP_mean_lag3',\n       'VWAP_mean_lag7', 'VWAP_mean_lag30', 'VWAP_std_lag3', 'VWAP_std_lag7',\n       'VWAP_std_lag30','Month', 'Week', 'Day', 'Day of week']","fab3a90b":"model = auto_arima(train.Close, exogenous=train[exogenous_features], trace=True, error_action=\"ignore\", suppress_warnings=True)\nmodel.fit(train.Close, exogenous=train[exogenous_features])\n\nvalid[\"Forecast_ARIMAX\"] = model.predict(n_periods=len(valid), exogenous=valid[exogenous_features])","07996006":"valid[[\"Close\", \"Forecast_ARIMAX\"]].plot()","183b1e56":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(valid.Close, valid.Forecast_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(valid.Close, valid.Forecast_ARIMAX))","ce802d8b":"### intiialize the Model\nmodel=Prophet()\n\n#Fitting the model and renaming the columns based on prophe requirements\nmodel.fit(reliance_analysis[[\"Date\", \"Close\"]].rename(columns={\"Date\": \"ds\", \"Close\": \"y\"}))\n\n#Making future dataframe for forecasting, we have given 365 days which can calculate VWAP till 2021\nreliance_future=model.make_future_dataframe(periods=365)\n\n#Checking the future dates\nreliance_future.tail()","a1706a57":"### Prediction of future values\nreliance_prediction=model.predict(reliance_future)\n\nreliance_prediction.tail()","0ef5701f":"#Forecast plot\nmodel.plot(reliance_prediction)","98d07870":"#Forecast components\nmodel.plot_components(reliance_prediction)","ba3e9bc8":"#Cross validation for the parameter days\nreliance_cv=cross_validation(model,initial='1095 days',period='180 days',horizon=\"365 days\")","599dee8c":"#Checking the parameters\nreliance_performance=performance_metrics(reliance_cv)\nreliance_performance.head()\n\n\n#Plotting for root mean squared metric\nfig=plot_cross_validation_metric(reliance_cv,metric='rmse')","0c41c787":"<div class=\"alert alert-block alert-info\">  \nThe following explanation about acf and pacf is for study purposes. It is not used in our models as we are using auto arima where p,q are figured out by the model by selecting the best AIC\n<\/div>\n<br>\n\n## ACF AND PACF PLOTS\n\n**ACF** is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged values. We plot these values along with the confidence band and tada! We have an ACF plot. In simple terms, it describes how well the present value of the series is related with its past values.\n\n**PACF** is a partial auto-correlation function. Basically instead of finding correlations of present with lags like ACF, it finds correlation of the residuals (which remains after removing the effects which are already explained by the earlier lag(s)) with the next lag value hence \u2018partial\u2019 and not \u2018complete\u2019 as we remove already found variations before we find the next correlation.\n\n### Autoregression Intuition\nConsider a time series that was generated by an autoregression (AR) process with a lag of k.We know that the ACF describes the autocorrelation between an observation and another observation at a prior time step that includes direct and indirect dependence information.This means we would expect the ACF for the AR(k) time series to be strong to a lag of k and the inertia of that relationship would carry on to subsequent lag values, trailing off at some point as the effect was weakened.We know that the PACF only describes the direct relationship between an observation and its lag. This would suggest that there would be no correlation for lag values beyond k.This is exactly the expectation of the ACF and PACF plots for an AR(k) process.\n\n## Moving Average Intuition\nConsider a time series that was generated by a moving average (MA) process with a lag of k.Remember that the moving average process is an autoregression model of the time series of residual errors from prior predictions. Another way to think about the moving average model is that it corrects future forecasts based on errors made on recent forecasts.We would expect the ACF for the MA(k) process to show a strong correlation with recent values up to the lag of k, then a sharp decline to low or no correlation. By definition, this is how the process was generated.For the PACF, we would expect the plot to show a strong relationship to the lag and a trailing off of correlation from the lag onwards.Again, this is exactly the expectation of the ACF and PACF plots for an MA(k) process.\n\n## Summary\nFrom the autocorrelation plot we can tell whether or not we need to add MA terms. From the partial autocorrelation plot we know we need to add AR terms and here we plot the acf and pacf plots and select the p,q point where the correlation line first hits the error\/zero band","47873eb7":"## Candlestick after Lockdown (Open,Close,High,Low)\nCandlestick charts are used by traders to determine possible price movement based on past patterns. Candlesticks are useful when trading as they show four price points (open, close, high, and low) throughout the period of time the trader specifies. Here we measure the trend after the commencement of lockdown phase","4689d933":"### Training and prediction\nLet's train our model with auto arima. Here the model selects the parameter p,q,d value in a normal arima model by itself by determining AIC value ( Akaike information criterion ) .AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model. So AUTO ARIMA prefers the parameters which can reap less information loss","1d23fdac":"## AUTO ARIMA-Autoregressive Integrated Moving Average\n\n### What is ARIMA ?\n<img src=\"https:\/\/datapenchant.com\/wp-content\/uploads\/2019\/09\/AR-COVER.png\">\nARIMA, short for \u2018Auto Regressive Integrated Moving Average\u2019 is actually a class of models that \u2018explains\u2019 a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values. <br>\n\nAny \u2018non-seasonal\u2019 time series that exhibits patterns and is not a random white noise can be modeled with ARIMA models. <br>\n\nAn ARIMA model is characterized by 3 terms: p, d, q <br>\n\nwhere, <br>\n\n* p is the order of the AR term\n* q is the order of the MA term\n* d is the number of differencing required to make the time series stationary\n\nIf a time series, has seasonal patterns, then you need to add seasonal terms and it becomes SARIMA, short for \u2018Seasonal ARIMA\u2019.\n\n### Why AUTO ARIMA ?\nAlthough ARIMA is a very powerful model for forecasting time series data, the data preparation and parameter tuning processes end up being really time consuming. Before implementing ARIMA, you need to make the series stationary, and determine the values of p and q using the plots we discussed above. Auto ARIMA makes this task really simple for us as it eliminates steps like converting stationarity and getting values of p,q from acf anf pacf plots.","29a46db3":"### Cross validation in prophet\nProphet includes functionality for time series cross validation to measure forecast error using historical data. This is done by selecting cutoff points in the history, and for each of them fitting the model using data only up to that cutoff point. We can then compare the forecasted values to the actual values. This figure illustrates a simulated historical forecast on the Peyton Manning dataset, where the model was fit to a initial history of 5 years, and a forecast was made on a one year horizon.\n\n<img src=\"https:\/\/facebook.github.io\/prophet\/static\/diagnostics_files\/diagnostics_3_0.png\">\n\nThis cross validation procedure can be done automatically for a range of historical cutoffs using the cross_validation function. We specify the forecast horizon (horizon), and then optionally the size of the initial training period (initial) and the spacing between cutoff dates (period). By default, the initial training period is set to three times the horizon, and cutoffs are made every half a horizon.\n\nThe output of cross_validation is a dataframe with the true values y and the out-of-sample forecast values yhat, at each simulated forecast date and for each cutoff date. In particular, a forecast is made for every observed point between cutoff and cutoff + horizon. This dataframe can then be used to compute error measures of yhat vs. y.\n<div class=\"alert alert-block alert-info\">  \nHere we do cross-validation to assess prediction performance on a horizon of 365 days, starting with 1095 days of training data in the first cutoff and then making predictions every 180 days.  \n<\/div>\n<br>\n ","cd68df60":"## Major Corporate Announcements 2020(Till June 30)\n<img src=\"https:\/\/akm-img-a-in.tosshub.com\/sites\/btmt\/images\/stories\/fb_jio_3_660_220420124049.jpg\">\n\nHere we will witness the major corporate announcements and how the press news has affected the price in stock market","ecb3c439":"**Insights:**\n* We can see the gradual fall in the first lockdown due to sudden announcement and WFH was tedious to adopt in that situation and every company like Reliance faced a short dip\n* But Reliance has pushed beyond its boundaries in the phase 2 lockdown as we can see the company reaches the top peak of 2020 in the phase 2 lockdown by coming up with vairous strategies and plan by WFH, Let's checkout what Reliance did in 2020 to reach the peak","2281a378":"**Insights:**\n* The slow start of Reliance industries boosted during this covid lockdown. As we can see a sluggish start which got boosted with two announcements on Reliance support to the nation on fighting the COVID-19 and gradually the price of stock increases\n* Also investments from Facebook,Silver lake,General Atlantic and many more has also boosted the price and the gained the trust of shareholders by being valuable company\n* Google investment is not mentioned here because the dataset is available till June 30 and Google invested on Jio in July 2020","6bc0cec5":"# Table of Contents:\n1. [Introduction](#section-one)\n\n2. [Data preparation](#section-two)\n\n3. [Data Visualization](#section-three)\n\n4. [Wrath of COVID-19](#section-four)\n\n5. [Stationarity conversion](#section-five)\n\n6. [Conclusion](#section-six)","8d74b52c":"### Prediction of future values","67ec3262":"## High vs Low\nNow, lets look at the high and low parameters over the years","580d477e":"We got the RMSE and MAE score of 37 and 26 which is pretty much good score considering a time series data. AUTOARIMA does it again !","0301ef97":"## Performance after lockdown-VWAP\n<img src=\"https:\/\/www.thestatesman.com\/wp-content\/uploads\/2020\/04\/iStock-bearish_ED.jpg\">\nLockdown has become a big blow for the Indian economy. From MNCs to street vendors were affected due to this locdown phase. As many companies operate with work from home, many managed to survive the race. Let's see how Reliance performed during lockdown ","c7181685":"<a id=\"section-five\"><\/a>\n# Stationarity conversion\n<img src=\"https:\/\/miro.medium.com\/max\/1147\/1*xdblkZyg6YmmReAkZHUksw.png\">\n\nA common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time. Stationarity can be defined in precise mathematical terms, but for our purpose we mean a flat looking series, without trend, constant variance over time, a constant autocorrelation structure over time and no periodic fluctuations (seasonality).\n\n\n<div class=\"alert alert-block alert-danger\">  \n<b>IMPORTANT NOTE:<\/b> As time went on the libraries have developed to handle the stationarity and we don't actually need to convert the time series data into stationary data. For study purpose,I have explained how to check stationarity and stationarity conversion in this project   \n<\/div>\n<br>\n\nThere are two ways you can check the stationarity of a time series. The first is by looking at the data. By visualizing the data it should be easy to identify a changing mean or variation in the data. For a more accurate assessment there is the Dickey-Fuller test. I won\u2019t go into the specifics of this test, but if the \u2018Test Statistic\u2019 is greater than the \u2018Critical Value\u2019 than the time series is stationary,Also we can check the pvalue. Below is code that will help you visualize the time series and test for stationarity.","77e6c08c":"## Univariate Analysis\nLet's see the trend of single factor over time ","a7611d6d":"From the plot we can see the predicted values in blue line which follows most the actual trend and after 2020 we can see th blue line getting extended for the 2021 which is the future prices and we can be assured that the upward trend continues in 2021. If Reliance gets a big deal in 5G, their stock prices will be over the roof.","57377e4a":"## Univariate analysis of Volume of share over the years\nLet's see the volume of shares that have been traded over NIFTY 50.","16b2b62d":"**Insights:**\n* All the meaures exhibit equal distribution property\n* All the distributions are right skewed","8c37b105":"**Insights:**\n* There has been a gradual increase in the trend of VWAP over years\n* There were two spikes in Jan 2008 and May-Oct 2009 \n\n Mukesh Ambani-controlled company are trading at near their all-time high levels of about Rs 1,625, a price that was last seen over nine-and-a-half years ago in January 2008. Reliance Industries shares ended at Rs 1,621.15 on Wednesday. By contrast, it saw an intra-day high of Rs 1,649 and previous closing high of Rs 1,610 in January 2008. [News here](https:\/\/www.financialexpress.com\/market\/reliance-industries-ril-share-price-all-time-high-10-year-return-zero-jul-2017\/780392\/) ","f3ddef13":"**Insights:**\n* There are many outliers in our dataset as we can see the max is 3 times the 75th percentile\n* The standard deviation and other statistical measurements is more or less equal among all the features\n","5470c089":"We have created the moving average and standard deviation for the respective days across High, Low, Volume, VWAP","72b9de1f":"We have created the future dataframe for 365 days which you can see above, we have dates till 2021 and now we are going to predict the stock prices for that ","4d5331ca":"## Fb Prophet\n\n<img src=\"https:\/\/www.kdnuggets.com\/wp-content\/uploads\/prophet-facebook.jpg\">\nFacebook developed an open sourcing Prophet, a forecasting tool available in both Python and R. It provides intuitive parameters which are easy to tune. Even someone who lacks deep expertise in time-series forecasting models can use this to generate meaningful predictions for a variety of problems in business scenarios.\n\n\n\n<div class=\"alert alert-block alert-info\">  \n<b>IMPORTANT NOTE:<\/b> The input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast. Also in fb prophet we are using the cleaned data not the stationary converted data as prophet takes care of stationarity internally.   \n<\/div>\n<br>","6b3c11a1":"From the autoarima results we got the values of p and q as 2,3 respectively and the AIC score is 47648.135","8f868f1d":"**Insights:**\n* There have been huge number of shared in 2020. This could be due to the reign of Jio and Investment by top tech gaints like Facebook and Google.\n* The thin phase lie between 2008-2016, in this phase there hasn't been  big volumes traded during these years.\n* Reliance have a strong foot in India and has got the trust of the citizens of India that is a valuable company.","e0b67e25":"**Insights:**\n* High vs Low follows the same path as Open vs Low, where High is a little higher than Low price of the day.\n* If you see at November 25 and 26 2009, The lowest price hit on 25th 2169 and on 26th the high price recorded was 1111, which shows the huge dip","5c5c45ac":"## Building AUTO ARIMA model\n\nFor building the auto arima model,first let's split our training and testing data. For time series analysis we step back from using train test split as our data involves time and splitting can cause the mix of date across both train and test dataset which is vulnerable for a data leakage. So we split based on the dates. Here we split the training and validation data based on the year 2019 where the data before 2019 is training data and data after 2019 is validation data","b879f5b7":"From the results we can understand the rmse value lies between 0-500 which is not great and not terrible considering the stock prediction as it is very uncertain to the the right value of prediction","7b88325e":"## Augmented Dickey Fuller Test\n\nThe Augmented Dickey-Fuller test is a type of statistical test called a unit root test.The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend. There are a number of unit root tests and the Augmented Dickey-Fuller may be one of the more widely used. It uses an autoregressive model and optimizes an information criterion across multiple different lag values.\n\nThe null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary (has some time-dependent structure). The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n\n* Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n* Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n\nWe interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n\n* p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n* p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","8fd12ddf":"<a id=\"section-five\"><\/a>\n# Model building Phase- Forecasting & Prediction\nHere we arrive at the most important phase why this project is being built.The forecasting and prediction phase.Many might wonder whether both terms are same or different. It is different. Here are few points to justify the statement.\n\n* **Prediction** is concerned with estimating the outcomes for unseen data. For this purpose, you fit a model to a training data set, which results in an estimator f^(x) that can make predictions for new samples x.\n\n* **Forecasting** is a sub-discipline of prediction in which we are making predictions about the future, on the basis of time-series data. Thus, the only difference between prediction and forecasting is that we consider the temporal dimension.\n\nFor model building we are considering the Close price feature. As it is very reliable for prediction and VWAP is a derived\/calculated value which doesn't make much sense while getting forecasted value.","2a146d2e":"### Plotting the forecasted values with the actual data\nLet's plot the results and compare them with actual values","86873505":"**Insights:**\n* The lockdown starts of with below 1000 VWAP, but gradually it rises more than 1500 VWAP and reaching near 1718 VWAP by June 30,2020.\n* This could be due to the interests shown by Facebook, Google and other companies on Jio shares.\n","f85383c9":"**Insights:**\n* Here we have a neat representation of the moving average and standard deviation graph\n* There's a lot of deviation when the volume value is reaching 2020 and corresponding mean is high compared to standard deviation","a3dcaba4":"Fantastic we have got more or less a similar result. Our model has captured a good amount of information from training dataset. Let's look at the performance metrics","29bca1ed":"**Insights:**\n* Then stock performance was intially good and there hasn't been a huge downfall yet for Reliance since lockdown as we can see there is always growth overall \n* Consecutive dips were seen between May 11-14 2020 and June 22-25 2020.\n* There hasn't been much growth of stock performance between May 11 to June 5 2020. But June 12th and June 19th, the huge rise in stock performance kept the growth in track.","7b664c66":"## Stationarity Conversion with shift()\nNow let's convert our non-stationary data to stationary with shift() method. Here we take a shift() of 1 day which means all the records will step down to one step and we take the difference from the original data. Since we see a trend in our data, when we subtract today's value from yesterday's value considering a trend it will leave a constant value on its way thus making the plot stationary.","749063cd":"<img src=\"https:\/\/9to5google.com\/wp-content\/uploads\/sites\/4\/2020\/07\/Google-Jio-Android-.jpg?quality=82&strip=all&w=1600\">","8e7241c3":"**Insights:**\n* If you can notice(use rangeslider to zoom in) we can clearly see most of the time the open is higher than close. \n* But the difference is very subtle. If we take moving average, we might not even notice the difference.\n* There's one place where you can notice a big difference is on May 2,2008 where the opening starts with 3026 and closes at 2674.5 ","3f20cad4":"Since our p value is greater than 0.05 we need to accept the null hypothesis which states that our data is non-stationary","16c0a5ef":"## Univariate analysis of Open,Close,High and Low\nLet's see open,close, high and low measures over years","62b889ea":"## VWAP over time \nNow let's see the Volume-weighted average price over the time. Please feel free to use the range slider to analyzeover time","52afc7ea":"### Performance metric\n\nHere we check the performance of our model with root mean squared value and plot it","2d49e8a1":"<a id=\"section-six\"><\/a>\n# Conclusion\nFrom the analysis, we can understand what are all the factors which is being undergone while working on a time-series project. Time series result's aren't the supreme way to tell accurate results as anything can happen to worsen or brighten up the stock market and the predictions aren't 100% reliable.But we can easily understand and derive from the past data and associate it with time and figure out why the event happened ? .\n\nComing to the context with our project, We saw how Reliance has risen from the ashes after being gravely hit in mid 2000's and now the company is at it's prime condition partnering with Facebook and Google revolutionizing the telecommunication, retail, technology, petroleum and many more to their portfolio. I beleive that reliance will become an Indian brand identity among the international market\n\n#### Thank you for reading my notebook. You can find my other notebooks [here](https:\/\/www.kaggle.com\/benroshan\/notebooks).","c8b36655":"## Distribution of stock measures\nLet's witness the histogram distribution of the stock measures such as open,close,high,low and as well as VWAP","079bbb86":"<a id=\"section-one\"><\/a>\n# Introduction\nTime plays a very important role when it comes to buiness.Each second is money every national and global economies depends on time. Time series analysis have become a widely used tool in the field of analytics inorder to understand a variable which depends on time. \n\n## What is time series analysis?\n<img src=\"https:\/\/dataanalyticsedge.com\/wp-content\/uploads\/2017\/11\/time-series-analysis.png\">\nTime series analysis is a statistical technique that deals with time series data, or trend analysis.  Time series data means that data is in a series of  particular time periods or intervals.  The data is considered in three types:\n\n* **Time series data:** A set of observations on the values that a variable takes at different times.\n\n* **Cross-sectional** data: Data of one or more variables, collected at the same point in time.\n\n* **Pooled data:** A combination of time series data and cross-sectional data.\n\nSource:[Statistics Solution](https:\/\/www.statisticssolutions.com\/time-series-analysis\/) \n\n## Reliance Industries\n\n<img src=\"https:\/\/akm-img-a-in.tosshub.com\/sites\/btmt\/images\/stories\/reliance_industries_logo_660x450_090720075629.jpg\">\n\nReliance Industries Limited (RIL) is an Indian multinational conglomerate company headquartered in Mumbai, Maharashtra, India. Reliance owns businesses across India engaged in energy, petrochemicals, textiles, natural resources, retail, and telecommunications. Reliance is one of the most profitable companies in India, the largest publicly traded company in India by market capitalization, and the largest company in India as measured by revenue after recently surpassing the government-controlled Indian Oil Corporation.[5] On 22 June 2020, Reliance Industries became the first Indian company to exceed US$150 billion in market capitalization after its market capitalization hit \u20b911,43,667 crore on the BSE.\n\nThe company is ranked 106th on the Fortune Global 500 list of the world's biggest corporations as of 2019. It is ranked 8th among the Top 250 Global Energy Companies by Platts as of 2016. Reliance continues to be India's largest exporter, accounting for 8% of India's total merchandise exports with a value of \u20b91,47,755 crore and access to markets in 108 countries. Reliance is responsible for almost 5% of the government of India's total revenues from customs and excise duty. It is also the highest income tax payer in the private sector in India.\n\nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Reliance_Industries)\n\n## Acknowledgements\n1. For clearly explaining the AUTO ARIMA model - [Vopani](https:\/\/www.kaggle.com\/rohanrao\/a-modern-time-series-tutorial)\n2. For the wonderful visualization guidelines- [Parul Pandey](https:\/\/www.kaggle.com\/parulpandey\/nifty-data-eda)\n3. Prophet documentation - [Facebook](http:\/\/facebook.github.io\/prophet\/docs\/quick_start.html)\n\n## Project summary\nThe project revolves around analysing the the closing price and Volume-weighted average price of Reliance's stock changes in time. It starts with preparing the data for visualizations and goes on with an extensive exploratory data analysis which also includes the impact of Reliance stocks due to COVID-19 and followed by time series model building and using the new tool Prophet for time series forecasting released by Facebook\n\n## Objectives of the project\n1. Data Preparation\n2. Data Visualization\n3. Building a time series model\n\n\n## Dataset\nThe data is the price history and trading volumes of the fifty stocks in the index [NIFTY 50 from NSE](https:\/\/www.nseindia.com\/) (National Stock Exchange) India. All datasets are at a day-level with pricing and trading values split across .cvs files for each stock along with a metadata file with some macro-information about the stocks itself. The data spans from 1st January, 2000 to 31st July, 2020.\n\n### Import libraries","e21dcfb5":"## Moving average analysis\nMoving average is a smoothing technique applied to time series to remove the fine-grained variation between time steps.The hope of smoothing is to remove noise and better expose the signal of the underlying causal processes. Moving averages are a simple and common type of smoothing used in time series analysis and time series forecasting.Calculating a moving average involves creating a new series where the values are comprised of the average of raw observations in the original time series.A moving average requires that you specify a window size called the window width. This defines the number of raw observations used to calculate the moving average value.The \u201cmoving\u201d part in the moving average refers to the fact that the window defined by the window width is slid along the time series to calculate the average values in the new series.\n\nIn our project we consider the moving mean and standard deviation for 3,7 and 30 days. Thanks to Vopani for this wonderful piece of code.","e3895c64":"## Dataset Details\n\n\nDescription of columns in the file:\n\n* Date - Date of trade\n* symbol - Name of the company (Reliance)\n* Series - We have only one series(EQ): It stands for Equity. In this series intraday trading is possible in addition to delivery\n* Prev Close - Refers to the prior day's final price of a security when the market officially closes for the day.\n* Open - The open is the starting period of trading on a securities exchange or organized over-the-counter market.\n* High -  Highest price at which a stock traded during the course of the trading day.\n* Low - Lowest price at which a stock traded during the course of the trading day.\n* Last - The last price of a stock is just one price to consider when buying or selling shares. The last price is simply the most recent one\n* Close - The close is a reference to the end of a trading session in the financial markets when the markets close for the day. \n* VWAP(Volume-weighted average price)- It is the ratio of the value traded to total volume traded over a particular time horizon. It is a measure of the average price at which a stock is traded over the trading horizon\n* Volume - It is the amount of a security that was traded during a given period of time\n* Turnover -It is a measure of sellers versus buyers of a particular stock. It is calculated by dividing the daily volume of a stock by the \"float\" of a stock, which is the number of shares available for sale by the general trading public.\n* Trades- The number of shares being traded on a given day is called trading volumes\n* Deliverabe Volume -  quantity of shares which actually move from one set of people (who had those shares in their demat account before today and are selling today) to another set of people (who have purchased those shares \n* %Deliverable - shares which are actually transferred from one person's to another's demat account.","85a4511a":"**Insights:**\n* As we know, all these parameters follow the same pattern without much deviation\n* Theres a break between 2008-2012 and 2016-2020. It signifies a sudden dip in the market for Reliance.","a6422673":"## Bivariate analysis\nLet's compare two factors over time\n\n## Open Vs Close over time\nOur first bivariate analysis involves open and close parameters ","50329da5":"## High vs Low with mean and standard deviation lag - 30 days\n\nIn this notebook, I'm considering only 30 days for comparison to get a lower noise. You can copy and edit this code to change the window according to your wish. Here we compare the High vs Low with mean and standard deviation.","b57a6f2f":"We have predicted the values for all the dates and even the 2021 dates. yhat is the predicted values,yhat lower and upper is the band\/range of that predicted values can be deflected. Let's look at the plot to get a better understanding.","3b526cc7":"## Import dataset\nFirst let's welcome our dataset","4fb5650a":"You can click on the legends in the plotly graph to see individual curves\n**Insights:**\n* Considering the standard deviation,there's a high deviation whenever there is a drop in the price of stock.\n* With the help of standard deviation we can understand where the company faced loss.\n* Even though the lag curve isn't much less with noise, we have a clear idea on how the high and low price move over time ","58464901":"## Visually checking for stationarity\n\nWe can get whether a data is stationary by just plotting it","949cac1a":"From the model compenents of prophet we get the trend,weekly and yearly plots. We can see the stocks were up during the months of March-January ","17908e91":"<a id=\"section-four\"><\/a>\n# Wrath of COVID-19 \n<img src=\"https:\/\/specials-images.forbesimg.com\/imageserve\/5eaa7ba552ac2e00060e010b\/960x0.jpg?fit=scale\">\nIn this phase we will see how COVID lockdown has affected the company's performance\n## Volume during Phase 1 Lockdown(25 March \u2013 14 April) and Phase 2 Lockdown (15 April \u2013 3 May)","aaef5d68":"### Forecast Components","0b13becd":"<a id=\"section-two\"><\/a>\n# Data preparation\nInorder for our machine learning algorithm to perform well, we need to cleanse our data. In our case, we don't have much garbage to clean except few null values. Also let's also extract few more features from the time to perform indepth EDA","d733c73f":"As you notice we have few new features assembled due to extraction. Let's gid rid of the null values by imputing it with mean value.","59ea93d1":"### Forecast Plot","eedd07d6":"<a id=\"section-three\"><\/a>\n# Data Visualization\nExploratory data analysis is a core part of time series analysis. In this phase, we will witness a lot of line graphs which can help us understand the trend, seasonality and many other concepts from time series analysis","ccdfceeb":"### Performance metrics-RMSE and MAE\nHere we calculate how well our model performed with numbers with the help of RMSE and MAE. We hope that the erroe will be very low","c3a714ea":"From the plotted graph we can say that the data doesn't have a constant average as there are meany leaps and troughs and also the variance is also different at different stages of the data. So our data is not stationary. We can also mathematically test for stationarity with adfuller test"}}