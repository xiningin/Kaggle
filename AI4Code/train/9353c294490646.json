{"cell_type":{"93a601b2":"code","39f44190":"code","d49d45a7":"code","509a0c8b":"code","23ca2c86":"code","0fd7aa1d":"code","d96ee708":"code","37e1c0db":"code","d201b000":"code","a99442f3":"code","b104413a":"code","03e40052":"markdown","945a3f11":"markdown","14f661df":"markdown","4409b60e":"markdown"},"source":{"93a601b2":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","39f44190":"observaions = 1000\nX = np.random.uniform(-10,10, size= (observaions,1))\nZ = np.random.uniform(-10,10, size= (observaions,1))","d49d45a7":"inputs = np.column_stack((X,Z))","509a0c8b":"inputs.shape","23ca2c86":"noise = np.random.uniform(-1,1,(observaions,1))\ntarget = 2*X-3*Z+5+noise","0fd7aa1d":"init_range= 0.1\nweight = np.random.uniform(-init_range, init_range,size = (2,1))\nbias = np.random.uniform(-init_range, init_range, (1,1))\nprint(weight)\nprint(bias)","d96ee708":"learning_rate = 0.05\nc = []\nfor i in range(100):\n    y_pred = np.dot(inputs,weight) + bias\n    delta = y_pred - target\n    cost = np.sum(delta**2)\/(2*observaions)\n    c.append(cost)\n    print(cost)\n    delta_sc = delta\/observaions\n    weight = weight - learning_rate*np.dot(inputs.T,delta_sc)\n    bias = bias - learning_rate*np.sum(delta_sc)\n","37e1c0db":"Xaxis = np.arange(0,100)\nplt.plot(Xaxis, c)\nplt.xlabel('No. of Iterations')\nplt.ylabel('Cost: (y-y_predicted)^2')","d201b000":"import tensorflow as tf\nobser = 1000\nx = np.random.uniform(-10,10,(obser,1))\nz = np.random.uniform(-10,10,(obser,1))\ngen_input = np.column_stack((x,z))\ngen_output = 2*x-3*z+5\nnp.savez('TF_intro', inputs = gen_input, outputs = gen_output)","a99442f3":"input_size=  2\noutput_size= 1\ninputs = tf.placeholder(tf.float32, [None,input_size])\ntargets = tf.placeholder(tf.float32,[None, output_size])\nweight = tf.Variable(tf.random_uniform([input_size, output_size], minval = -0.1, maxval = 0.1))\nbias = tf.Variable(tf.random_uniform([output_size], minval = -0.1, maxval = 0.1))\noutputs= tf.matmul(inputs,weight)+bias\nmean_loss = tf.losses.mean_squared_error(labels=targets,predictions=outputs)\/2.\noptimize = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(mean_loss)\nsess = tf.InteractiveSession()\ninitializer = tf.global_variables_initializer()\nsess.run(initializer)\ntraining_data = np.load('TF_intro.npz')\nc = []\nfor e in range(100):\n    _,cost = sess.run([optimize, mean_loss], feed_dict={inputs: training_data['inputs'], targets: training_data['outputs']})\n    print('cost:', cost)\n    c.append(cost)","b104413a":"plt.plot(Xaxis, c)\nplt.xlabel('No. of Epochs')\nplt.ylabel('Cost: (y-y_predicted)^2')","03e40052":"**Gradient Descent math**","945a3f11":"*As we can see, the cost\/loss decreases at a high rate in the beginning but as the value starts reaching the global minima, it gradually becomes a constant*","14f661df":"**Gradient Descent using Tensorflow**","4409b60e":"*Similar results shows that the code works just fine*"}}