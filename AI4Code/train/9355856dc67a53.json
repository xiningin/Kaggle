{"cell_type":{"c00c7a0e":"code","d24d4027":"code","17007f41":"code","3a2f8a85":"code","f2d78549":"code","7c0acf93":"code","bb578983":"code","d743adc5":"code","13305b67":"code","da53569d":"code","67566bde":"code","38b701eb":"code","3d819502":"code","b6f8311d":"code","5f6e70bf":"code","21d68725":"code","58aafd91":"code","187bc0b2":"code","2aa61402":"code","d4c916f5":"code","bc21cd47":"code","c7166078":"code","dd3fa8f7":"code","9df5fb3b":"code","bfc64f5f":"code","ecf286a9":"code","06895e09":"code","c1b67b12":"code","6a34fe80":"code","5ca63c1e":"code","0ee53ded":"code","1b1f3340":"markdown","2f3c2381":"markdown","5361a997":"markdown","2fcf325a":"markdown","a197a37f":"markdown","dbbed4f3":"markdown","1d085f44":"markdown","61da0c23":"markdown","4c750652":"markdown","84ecf91a":"markdown","69714428":"markdown","13bed87c":"markdown","e42b450a":"markdown","3f124715":"markdown","b11690c7":"markdown","da176002":"markdown","5306865b":"markdown","4010d475":"markdown"},"source":{"c00c7a0e":"#%% Imports\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport shap\n%matplotlib inline\n\nfrom pprint import pprint\nfrom IPython.display import display \nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, roc_auc_score","d24d4027":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17007f41":"# Read aug_train.csv\naug_train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\n# Initial Glance at Data\ndisplay(aug_train.info(verbose = True,null_counts=True))\nprint(aug_train.shape)","3a2f8a85":"# enrolle_id is an meaningless feature that is a unique value for each employee.\n# count total number of unique values in enrollee_id column\nprint('Number of Unique Values: ' + str(aug_train['enrollee_id'].nunique()))","f2d78549":"# city has 123 unique values and is a categorical variable.\nprint('Number of Unique Values: ' + str(aug_train['city'].nunique()))\nprint('Number of NaN Values: ' + str(sum(aug_train['city'].isnull())))\n# top 10 cities \nprint((aug_train['city'].value_counts()[0:10]))","7c0acf93":"# city_development_index is Continous Variable\n\nprint(\"Number of Missing Values: \", aug_train['city_development_index'].isna().sum())\ndisplay(aug_train['city_development_index'].describe())\nboxplot = aug_train.boxplot(column ='city_development_index')","bb578983":"# gender is Catagorical Variable: Male, Female, Other, or NaN\nprint(\"Number of Missing Values: \", aug_train['gender'].isna().sum())\nfig = px.pie(aug_train['gender'].value_counts(), values='gender', names = aug_train['gender'].value_counts().index,title = 'gender',template='plotly_dark')\nfig.show()","d743adc5":"# relevent_experience is Binary Variable with no missing values.\nprint(\"Number of Missing Values: \", aug_train['relevent_experience'].isna().sum())\nfig = px.pie(aug_train['relevent_experience'].value_counts(), values='relevent_experience', \n             names = aug_train['relevent_experience'].value_counts().index,title = 'relevent_experience',template = 'plotly_dark')\nfig.show()","13305b67":"# education_level is Catagorical Variable indicating education level of worker, has 460 missing values\n\nprint(\"Number of Missing Values: \", aug_train['education_level'].isna().sum())\nfig = px.pie(aug_train['education_level'].value_counts(), values='education_level', \n             names = aug_train['education_level'].value_counts().index,title = 'education_level',template='plotly_dark')\nfig.show()","da53569d":"# major_discipline is a Catagorical Variable indicating major discipline of worker, has 2813 missing values\nprint(\"Number of Missing Values: \", aug_train['major_discipline'].isna().sum())\nfig = px.pie(aug_train['major_discipline'].value_counts(), values='major_discipline', \n             names = aug_train['major_discipline'].value_counts().index,title = 'major_discipline',template='plotly_dark')\nfig.show()","67566bde":"# experience is a Ordinal Variable, can replace <1 with 0 and >20 with 21\n\nprint(\"Number of Missing Values: \", aug_train['experience'].isna().sum())\nfig = px.pie(aug_train['experience'].value_counts(), values='experience', \n             names = aug_train['experience'].value_counts().index,title = 'experience',template='plotly_dark')\nfig.show()","38b701eb":"# company_size is a Ordinal Catagorical variable has 5938 missing variables\n\nprint(\"Number of Missing Values: \", aug_train['company_size'].isna().sum())\nfig = px.pie(aug_train['company_size'].value_counts(), values='company_size', \n             names = aug_train['company_size'].value_counts().index,title = 'company_size',template='plotly_dark')\nfig.show()","3d819502":"# company_type is a Catagorical Variable\n\nprint(\"Number of Missing Values: \", aug_train['company_type'].isna().sum())\nfig = px.pie(aug_train['company_type'].value_counts(), values='company_type', \n             names = aug_train['company_type'].value_counts().index,title = 'company_type',template='plotly_dark')\nfig.show()","b6f8311d":"# last_new_job is a Catagorical Variable\n\nprint(\"Number of Missing Values: \", aug_train['last_new_job'].isna().sum())\nfig = px.pie(aug_train['last_new_job'].value_counts(), values='last_new_job', \n             names = aug_train['last_new_job'].value_counts().index,title = 'last_new_job',template='plotly_dark')\nfig.show()","5f6e70bf":"# training_hours is a Continous variable\n\nprint(\"Number of Missing Values: \", aug_train['training_hours'].isna().sum())\ndisplay(aug_train['training_hours'].describe())\naug_train.boxplot(column ='training_hours')","21d68725":"print(\"Number of Missing Values: \", aug_train['target'].isna().sum())\nfig = px.pie(aug_train['target'].value_counts(), values='target', \n             names = aug_train['target'].value_counts().index,title = 'target',template='ggplot2')\nfig.show()","58aafd91":"# Read aug_test.csv\naug_test = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\n# Initial Glance at Data\ndisplay(aug_test.info(verbose = True,null_counts=True))\nprint(aug_test.shape)","187bc0b2":"# Seperate aug_train into target and features \ny = aug_train['target']\nX_aug_train = aug_train.drop('target',axis = 'columns')\n# save the index for X_aug_train \nX_aug_train_index = X_aug_train.index.to_list()\n\n# row bind aug_train features with aug_test features \n# this makes it easier to apply label encoding onto the entire dataset \nX_aug_total = X_aug_train.append(aug_test,ignore_index = True)\ndisplay(X_aug_total.info(verbose = True,null_counts=True))\n\n# save the index for X_aug_test \nX_aug_test_index = np.setdiff1d(X_aug_total.index.to_list() ,X_aug_train_index) ","2aa61402":"# MultiColumnLabelEncoder\n# Code snipet found on Stack Exchange \n# https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\n# from sklearn.preprocessing import LabelEncoder\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                # convert float NaN --> string NaN\n                output[col] = output[col].fillna('NaN')\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\n# store the catagorical features names as a list      \ncat_features = X_aug_total.select_dtypes(['object']).columns.to_list()\n\n# use MultiColumnLabelEncoder to apply LabelEncoding on cat_features \n# uses NaN as a value , no imputation will be used for missing data\nX_aug_total_transform = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X_aug_total)","d4c916f5":"# Before and After LabelEncoding\ndisplay(X_aug_total)\ndisplay(X_aug_total_transform)","bc21cd47":"# Split X_aug_total_transform \nX_aug_train_transform = X_aug_total_transform.iloc[X_aug_train_index, :]\nX_aug_test_transform = X_aug_total_transform.iloc[X_aug_test_index, :].reset_index(drop = True) ","c7166078":"#After LabelEncoding for aug_train \ndisplay(X_aug_train_transform)","dd3fa8f7":"#After LabelEncoding for aug_test \ndisplay(X_aug_test_transform)","9df5fb3b":"# drop enrollee_id for aug_train as it is a useless feature \ntrain_x, valid_x, train_y, valid_y = train_test_split(X_aug_train_transform.drop('enrollee_id',axis = 'columns'), y, test_size=0.2, shuffle=True, stratify=y, random_state=1301)\n\n# Create the LightGBM data containers\n# Make sure that cat_features are used\ntrain_data=lgb.Dataset(train_x,label=train_y, categorical_feature = cat_features)\nvalid_data=lgb.Dataset(valid_x,label=valid_y, categorical_feature = cat_features)\n\n#Select Hyper-Parameters\nparams = {'objective':'binary',\n          'metric' : 'auc',\n          'boosting_type' : 'gbdt',\n          'colsample_bytree' : 0.9234,\n          'num_leaves' : 13,\n          'max_depth' : -1,\n          'n_estimators' : 200,\n          'min_child_samples': 399, \n          'min_child_weight': 0.1,\n          'reg_alpha': 2,\n          'reg_lambda': 5,\n          'subsample': 0.855,\n          'verbose' : -1,\n          'num_threads' : 4\n}","bfc64f5f":"# Train model on selected parameters and number of iterations\nlgbm = lgb.train(params,\n                 train_data,\n                 2500,\n                 valid_sets=valid_data,\n                 early_stopping_rounds= 30,\n                 verbose_eval= 10\n                 )","ecf286a9":"# Overall AUC\ny_hat = lgbm.predict(X_aug_train_transform.drop('enrollee_id',axis = 'columns'))\nscore = roc_auc_score(y, y_hat)\nprint(\"Overall AUC: {:.3f}\" .format(score))","06895e09":"# ROC Curve for training\/validation data\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\ny_probas = lgbm.predict(valid_x) \nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(valid_y, y_probas)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for training data')\nplt.legend(loc=\"lower right\")\nplt.show()","c1b67b12":"# Feature Importance \nlgb.plot_importance(lgbm)","6a34fe80":"# Feature Importance using shap package \nlgbm.params['objective'] = 'binary'\nshap_values = shap.TreeExplainer(lgbm).shap_values(valid_x)\nshap.summary_plot(shap_values, valid_x)","5ca63c1e":"# Predictions for aug_test.csv\npredict = lgbm.predict(X_aug_test_transform.drop('enrollee_id',axis = 'columns')) \nsubmission = pd.DataFrame({'enrollee_id':X_aug_test_transform['enrollee_id'],'target':predict})\ndisplay(submission)","0ee53ded":"## Submit Predictions\nsubmission.to_csv('submission.csv',index=False)","1b1f3340":"From both feature importance, we can see that city contributes a lot if a employee is looking to change jobs or not. The next feature that is also important is company_size. The shap package is prefer when finding feature importance as it preservces consistency and accuracy. You can read more about the shap package in the links provided below\n\nhttps:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d\nhttps:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27","2f3c2381":"## Train-Test Stratified Split","5361a997":"## MultiColumnLabelEncoder","2fcf325a":"aug_train has 19,158 observations with 13 features and 1 target variable. The dataset has missing data and must be handled properly.","a197a37f":"### Predictions for aug_test.csv","dbbed4f3":"## Testing Data Initail Glance","1d085f44":"### Tarrget variable\ntarget is the variable we are trying to predict and calculate the probablities for.\n\n0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change.\n\nIt is better to have a high recall to better target employees who are looking for a job change.\nThis is an unbalanced classification problem as seen in the pie chart below.","61da0c23":"## EDA","4c750652":"### Submit Predictions","84ecf91a":"### Closing Remarks\nPlease comment and like the notebook if it of use to you! Have a wonderful year!","69714428":"## Read in Training Data (aug_train.csv)","13bed87c":"## Feature Importance","e42b450a":"## Prepare Data for LightGBM\nExtract only the features from aug_train and aug_test and rowbind them. We then will perform label encoding so that the LightGBM can be used.","3f124715":"## Run LightGBM on train data","b11690c7":"## Split X_aug_total_transform\nSplit X_aug_total_transform back into X_aug_train_transform and X_aug_test_transform by using the index we saved before.","da176002":"## Task Details\nThis dataset is designed to understand the factors that lead to a person to work for a different company(leaving current job), by model(s) that uses the current credentials\/demographics\/experience to predict the probability of a candidate to look for a new job or will work for the company.\n\nThe whole data divided to train and test. Sample submission has been provided correspond to enrollee id of test set (enrolle id | target)\n\n## Notes\nThe dataset is imbalanced.\n\nMost features are categorical (Nominal, Ordinal, Binary), some with high cardinality.\n\nMissing imputation can be a part of your pipeline as well.\n\n## Features\nenrollee_id : Unique ID for candidate\n\ncity: City code\n\ncity_development_index : Developement index of the city (scaled)\n\ngender: Gender of candidate\n\nrelevent_experience: Relevant experience of candidate\n\nenrolled_university: Type of University course enrolled if any\n\neducation_level: Education level of candidate\n\nmajor_discipline :Education major discipline of candidate\n\nexperience: Candidate total experience in years\n\ncompany_size: No of employees in current employer's company\n\ncompany_type : Type of current employer\n\nlastnewjob: Difference in years between previous job and current job\n\ntraining_hours: training hours completed\n\ntarget: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change\n\n## Importing Libraries","5306865b":"## Challenges\n\nLightGBM has many parameters and other methods that can be utilize to better tune the parameters, this is my first time using LightGBM so mistakes might have occured\n\nWorking with catagorical features is difficult, especialy when using One-Hot Encoding, this leads to a messy dataframe and longer computational. This is why I opt for Label Encoding and LightGBM","4010d475":"## Conclusions\n\nLightGBM is a great ML algorithim that handles catagorical features and missing values\n\nThis is a great dataset to work on and lots of knowledge can be gain from withing with this dataset\n\nResearching and reading other Kaggle notebooks is essential for becoming a better data scientist"}}