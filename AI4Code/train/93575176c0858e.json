{"cell_type":{"009654b0":"code","1893dfc2":"code","69b89d40":"code","23b88e95":"code","900f73c1":"code","1774d2b4":"code","a201faf0":"code","2dbcbe0b":"code","6f7bd5d5":"code","155cabd8":"code","a4c9ee6c":"code","d02eeb8c":"code","14ad7562":"code","a6c8ea30":"code","439623dd":"code","4c3b88ce":"code","791708f5":"code","e09d9fe3":"code","d362aebe":"code","9cff96be":"code","472818c6":"code","4c59f72d":"code","6ee66e48":"code","cebfb597":"code","1fce3f2f":"code","9db437b7":"code","2ef7c79c":"code","d2b7b35b":"code","dab68167":"code","ee7c4e64":"code","4cdd68e8":"code","48a34f06":"code","b3a5c89c":"code","c451a919":"code","43451ee1":"code","3b7f68d3":"code","78670f33":"code","2cb6b7d4":"code","49f5a380":"code","9aeb8a1a":"code","6bb9f761":"code","5ab948ed":"code","6a74eecf":"code","31b010a4":"code","b273a00f":"code","584543ce":"code","062ed715":"code","fb44277a":"code","7b5632a8":"code","6bf0f993":"code","6faccbf9":"code","bbd5ac67":"code","ea27c96e":"markdown","5bf40545":"markdown","f9dd624e":"markdown","6ad44584":"markdown","cd004abc":"markdown","ecb12d6e":"markdown","da37cca0":"markdown","27b810ac":"markdown","40a15240":"markdown","0e4e269c":"markdown","f187476c":"markdown","2f82f07a":"markdown","3eed98ec":"markdown","070bb0ba":"markdown","b0b9f5fc":"markdown","3dbfb9ad":"markdown","e2463ae3":"markdown","820266a1":"markdown","3795986d":"markdown","d5814c12":"markdown","afe67a1c":"markdown","e9839345":"markdown"},"source":{"009654b0":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\npd.set_option(\"display.max_columns\", 80)\npd.set_option(\"display.max_rows\", 80)\npd.set_option(\"display.float_format\", \"{:.2f}\".format)","1893dfc2":"data = pd.read_csv(\"\/kaggle\/input\/audiobook-app-data\/audiobook_data_2.csv\", index_col=0)\ndata.head()","69b89d40":"data.describe()","23b88e95":"data.isnull().sum()","900f73c1":"data.info()","1774d2b4":"data['Book_length(mins)_overall'].value_counts()","a201faf0":"def book_length(length):\n    if length > 1200:\n        return 1\n    else:\n        return 0\n    \ndata['purchases_hour_>3h'] = data['Book_length(mins)_overall'].apply(book_length)","2dbcbe0b":"data['Book_length(mins)_avg'].apply(book_length).value_counts()","6f7bd5d5":"data['purchases_hour_>3h'].value_counts()","155cabd8":"columns = ['purchases_hour_>3h', 'Book_length(mins)_overall', 'Book_length(mins)_avg']\nplt.figure(figsize=(12, 7))\n\nfor i, column in enumerate(columns, 1):\n    plt.subplot(2, 2, i)\n    data[data[\"Target\"] == 0][column].hist(bins=35, color='blue', label='Bought Again = NO', alpha=0.6)\n    data[data[\"Target\"] == 1][column].hist(bins=35, color='red', label='Bought Again = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","a4c9ee6c":"columns = [\"Price_overall\", \"Price_avg\"]\nplt.figure(figsize=(12, 7))\ndf = data[(data.Price_overall < 20) & (data.Price_avg < 20)]\n\nfor i, column in enumerate(columns, 1):\n    plt.subplot(2, 2, i)\n    df[df[\"Target\"] == 0][column].hist(bins=35, color='blue', label='Bought Again = NO', alpha=0.6)\n    df[df[\"Target\"] == 1][column].hist(bins=35, color='red', label='Bought Again = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","d02eeb8c":"print(data[data['Review'] == 0].Target.value_counts(normalize=True))\nprint(data[data['Review'] == 1].Target.value_counts(normalize=True))","14ad7562":"data['Review10\/10'].value_counts()","a6c8ea30":"columns = [\"Review\", \"Review10\/10\"]\nplt.figure(figsize=(12, 7))\n\nfor i, column in enumerate(columns, 1):\n    plt.subplot(2, 2, i)\n    data[data[\"Target\"] == 0][column].hist(bins=35, color='blue', label='Bought Again = NO', alpha=0.6)\n    data[data[\"Target\"] == 1][column].hist(bins=35, color='red', label='Bought Again = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","439623dd":"def listened_to_books(minutes):\n    if minutes > 0.0:\n        return 0\n    else:\n        return 1\ndata['listened_to_books'] = data.Minutes_listened.apply(listened_to_books)","4c3b88ce":"def completion_state(minutes):\n    if minutes > 0.5:\n        return 1\n    else:\n        return 0\ndata['completion_state'] = data.Completion.apply(completion_state)","791708f5":"columns = [\"Minutes_listened\", \"Completion\", \"listened_to_books\", \"completion_state\"]\nplt.figure(figsize=(12, 7))\n\nfor i, column in enumerate(columns, 1):\n    plt.subplot(2, 2, i)\n    data[data[\"Target\"] == 0][column].hist(bins=35, color='blue', label='Bought Again = NO', alpha=0.6)\n    data[data[\"Target\"] == 1][column].hist(bins=35, color='red', label='Bought Again = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","e09d9fe3":"data.drop('Minutes_listened', axis=1, inplace=True)","d362aebe":"def asked_for_request(request):\n    if request == 0:\n        return 0\n    else:\n        return 1\n    \ndata[\"asked_for_request\"] = data.Support_Request.apply(asked_for_request)","9cff96be":"def acc_purchases(purchase):\n    if purchase == 0:\n        return 0\n    else:\n        return 1\ndata['acc_purchases'] = data.Last_Visited_mins_Purchase_date.apply(acc_purchases)","472818c6":"data.Last_Visited_mins_Purchase_date.value_counts()","4c59f72d":"columns = [\"Support_Request\", \"Last_Visited_mins_Purchase_date\", \"asked_for_request\", \"acc_purchases\"]\nplt.figure(figsize=(12, 7))\n\nfor i, column in enumerate(columns, 1):\n    plt.subplot(2, 2, i)\n    data[data[\"Target\"] == 0][column].hist(bins=35, color='blue', label='Bought Again = NO', alpha=0.6)\n    data[data[\"Target\"] == 1][column].hist(bins=35, color='red', label='Bought Again = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","6ee66e48":"data.drop('Support_Request', axis=1, inplace=True)","cebfb597":"print(f\"{data.Target.value_counts()}\")\nprint(f\"{data.Target.value_counts()[0] \/ data.Target.value_counts()[1]}\")","1fce3f2f":"plt.figure(figsize=(15, 10))\nsns.heatmap(data.corr(), annot=True)","9db437b7":"print(f\"Data shape before removing duplicates: {data.shape}\")\n\n# Remove duplicate Features\ndata = data.T.drop_duplicates()\ndata = data.T\n\n# Remove Duplicate Rows\ndata.drop_duplicates(inplace=True)\n\nprint(f\"Data shape after removing duplicates: {data.shape}\")","2ef7c79c":"print(f\"{data.Target.value_counts()}\")\nprint(f\"{data.Target.value_counts()[0] \/ data.Target.value_counts()[1]}\")","d2b7b35b":"from sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop('Target', axis=1)\ny = data.Target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ndummies = [column for column in data.drop('Target', axis=1).columns if data[column].nunique() <= 10]\nnumeric = [column for column in data.drop('Target', axis=1).columns if data[column].nunique() > 10]\n\nohe = OneHotEncoder()\nstd_scaler = StandardScaler()\n\nct = make_column_transformer(\n    (ohe, dummies),\n    (std_scaler, numeric),\n    remainder='passthrough'\n)\n\nX_train = ct.fit_transform(X_train)\nX_test = ct.transform(X_test)\n\nprint(f\"Train shape: {X_train.shape}\")\nprint(f\"Test shape: {X_test.shape}\")","dab68167":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ndef evaluate(model, X_train, X_test, y_train, y_test):\n    y_test_pred = model.predict(X_test)\n    y_train_pred = model.predict(X_train)\n\n    print(\"TRAINIG RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_train, y_train_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n\n    print(\"TESTING RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_test_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")","ee7c4e64":"from sklearn.metrics import precision_recall_curve, roc_curve\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.title(\"Precision\/Recall Tradeoff\")\n    \n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    \ndef plot_graphs(model):    \n    precisions, recalls, thresholds = precision_recall_curve(y_test, model.predict(X_test))\n    plt.figure(figsize=(14, 25))\n    plt.subplot(4, 2, 1)\n    plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\n    plt.subplot(4, 2, 2)\n    plt.plot(precisions, recalls)\n    plt.xlabel(\"Precision\")\n    plt.ylabel(\"Recall\")\n    plt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\n    plt.subplot(4, 2, 3)\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict(X_test))\n    plot_roc_curve(fpr, tpr)","4cdd68e8":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(solver='liblinear', penalty='l2')\nlr_clf.fit(X_train, y_train)\n\nevaluate(lr_clf, X_train, X_test, y_train, y_test)","48a34f06":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(lr_clf, X, y, cv=5, scoring='f1')\nprint(f\"Logistic Regression F1_score: {scores.mean() * 100:.2f}% +\/- ({scores.std() * 100:.2f})\")","b3a5c89c":"plot_graphs(lr_clf)","c451a919":"scores_dict = {\n    'Logistic Regression': {\n        'Train': accuracy_score(y_train, lr_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, lr_clf.predict(X_test)),\n        'cv_f1_score': scores.mean()\n    },\n}","43451ee1":"zeros = (y_train.value_counts()[0] \/ y_train.shape)[0]\nones = (y_train.value_counts()[1] \/ y_train.shape)[0]\n\nprint(f\"Doesn't purchase again users Rate: {zeros * 100:.2f}%\")\nprint(f\"Purchase again users Rate: {ones * 100 :.2f}%\")","3b7f68d3":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=1500, oob_score=True)\nrf_clf.fit(X_train, y_train)\n\nevaluate(rf_clf, X_train, X_test, y_train, y_test)","78670f33":"from sklearn.model_selection import GridSearchCV\n\nrf_clf = RandomForestClassifier(n_estimators=100, oob_score=True)\n\nparam_grid = {'n_estimators':[100, 500, 1000, 1500],\n              'max_depth':[3, 5, 7, 10, 15, None], \n              'min_samples_split':[2, 3, 10], \n              'min_samples_leaf':[1, 3, 5, 7, 10], \n              'criterion':[\"gini\", \"entropy\"]}\n\nrf_cv = GridSearchCV(rf_clf, param_grid, scoring=\"f1\", n_jobs=-1, verbose=1, cv=3)\nrf_cv.fit(X_train, y_train)\n\nbest_params = rf_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\n\nrf_clf = RandomForestClassifier(**best_params)\nrf_clf.fit(X_train, y_train)\n\nevaluate(rf_clf, X_train, X_test, y_train, y_test)","2cb6b7d4":"scores = cross_val_score(rf_clf, X, y, cv=5, scoring='f1')\nprint(scores)\nprint(f\"Random Forest F1_score: {scores.mean() * 100:.2f}% +\/- ({scores.std() * 100:.2f})\")","49f5a380":"plot_graphs(rf_clf)","9aeb8a1a":"scores_dict['Random Forest'] = {\n    'Train': accuracy_score(y_train, rf_clf.predict(X_train)),\n    'Test': accuracy_score(y_test, rf_clf.predict(X_test)),\n    'cv_f1_score': scores.mean()\n}","6bb9f761":"from xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier(learning_rate=0.5, \n                        n_estimators=150, \n                        base_score=0.3)\nxgb_clf.fit(X_train, y_train)\n\nevaluate(xgb_clf, X_train, X_test, y_train, y_test)","5ab948ed":"xgb_clf = XGBClassifier(learning_rate=0.5, \n                        n_estimators=150, \n                        base_score=0.3)\n\nhyperparameter_grid = {'colsample_bytree': [ 0.5, 0.75, 0.85, 0.9, 1], \n                       'colsample_bylevel': [ 0.5, 0.75, 0.85, 0.9, 1],\n                       'colsample_bynode': [ 0.5, 0.75, 0.85, 0.9, 1],\n#                        'learning_rate' : [0.01, 0.5, 0.1], \n#                        'n_estimators': [100, 350, 500],\n                       'min_child_weight' : [2, 3, 5, 10],\n                       'max_depth': [3, 5, 10, 15], \n#                        'base_score' : [0.1, 0.5, 0.9]\n                      }\n\nxgb_cv = GridSearchCV(xgb_clf, hyperparameter_grid, scoring=\"f1\", \n                           n_jobs=-1, verbose=1, cv=3)\nxgb_cv.fit(X_train, y_train)\n\nbest_params = xgb_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\n\nxgb_clf = XGBClassifier(**best_params)\nxgb_clf.fit(X_train, y_train)\n\nevaluate(xgb_clf, X_train, X_test, y_train, y_test)","6a74eecf":"scores = cross_val_score(xgb_clf, X, y, cv=5, scoring='f1')\nprint(scores)\nprint(f\"XGBoost F1_score: {scores.mean() * 100:.2f}% +\/- ({scores.std() * 100:.2f})\")","31b010a4":"plot_graphs(xgb_clf)","b273a00f":"scores_dict['XGBoost'] = {\n    'Train': accuracy_score(y_train, xgb_clf.predict(X_train)),\n    'Test': accuracy_score(y_test, xgb_clf.predict(X_test)),\n    'cv_f1_score': scores.mean()\n}","584543ce":"from sklearn.svm import SVC\n\nsvm_clf = SVC()\nsvm_clf.fit(X_train, y_train)\n\nevaluate(svm_clf, X_train, X_test, y_train, y_test)","062ed715":"# param_grid = {\n#     'C': [0.01, 0.1, 1, 10, 100],\n#     'gamma': [0.01, 0.1, 1, 10, 100],\n#     'kernel': ['rbf', 'poly', 'linear']\n# }\n\n# svm_cv = GridSearchCV(SVC(), param_grid, scoring='f1', verbose=1, cv=3, n_jobs=-1)\n# svm_cv.fit(X_train, y_train)\n\n# best_params = svm_cv.best_params_\n# print(f\"Best params: {best_params}\")\n\n# svm_clf = SVC(**best_params)\n# svm_clf.fit(X_train, y_train)\n# evaluate(svm_clf, X_train, X_test, y_train, y_test)","fb44277a":"scores = cross_val_score(svm_clf, X, y, cv=5, scoring='f1')\nprint(scores)\nprint(f\"Support Vector Machine F1_score: {scores.mean() * 100:.2f}% +\/- ({scores.std() * 100:.2f})\")","7b5632a8":"plot_graphs(svm_clf)","6bf0f993":"scores_dict['Support Vector Machine'] = {\n    'Train': accuracy_score(y_train, svm_clf.predict(X_train)),\n    'Test': accuracy_score(y_test, svm_clf.predict(X_test)),\n    'cv_f1_score': scores.mean()\n}","6faccbf9":"from sklearn.metrics import roc_auc_score\n\nml_models = {\n    'Logistic Regression': lr_clf, \n    'Random Forest': rf_clf, \n    'XGboost': xgb_clf,\n    'Support Vector Machine': svm_clf\n}\nfor model in ml_models:\n    print(f\"{model.upper()} roc_auc_score: {roc_auc_score(y_test, ml_models[model].predict(X_test)):.3f}\")","bbd5ac67":"scores_df = pd.DataFrame(scores_dict)\nscores_df.plot(kind='barh', figsize=(15, 8))","ea27c96e":"## `Price_overall` & `Price_avg`\n\n- `Price_overall` & `Price_avg`: Same as Book length, the price variable is almost always a good predictor.","5bf40545":"# 4. 2. Random Forest Classifier","f9dd624e":"## `Support_Request` & `Last_Visited_mins_Purchase_date`\n\n- `Support_Request`: Shows the total number of support request (forgotten password to assistance).\n- `Last_Visited_mins_Purchase_date`: the bigger the difference, the bigger sooner the engagement. If the value is 0, we are sure the customer has never accessed what he\/she has bought.","6ad44584":"## Cross-Validation Score for Support Vector Machine","cd004abc":"# 4. 1. Logistic Regression ","ecb12d6e":"# 4. Model Building","da37cca0":"### Handling categorical features","27b810ac":"## `Book_length(mins)_overall` & `Book_length(mins)_avg`\n\n- `Book_length(mins)_overall`: is the sum of the lengths of purchases.\n\n- `Book_length(mins)_avg`: is the sum of the lengths of purchases divided by the number of purchases. Notice we don't need the number of purchases column because we ca get it from `Book_length(mins)_overall` \/ `Book_length(mins)_avg`.","40a15240":"### Cross Validation Score for Random Forest Classifier","0e4e269c":"# 4. 4. Support Vector Machine","f187476c":"### Cross Validation Score for Logistic Regression","2f82f07a":"It is important to notice that our `target` variable is inbabalanced. We have only `2237` user who convert again in the `6 month` period. The data need to be balanced.","3eed98ec":"# 5. Comparing Machine Learning Models","070bb0ba":"# 4. 3. XGBoost Classifier","b0b9f5fc":"# 1. Getting aquainted with the dataset\n\n- `\u00ccD`: is like a name.\n\n- `Book_length(mins)_overall`: is the sum of the lengths of purchases.\n\n- `Book_length(mins)_avg`: is the sum of the lengths of purchases divided by the number of purchases. Notice we don't need the number of purchases column because we ca get it from `Book_length(mins)_overall` \/ `Book_length(mins)_avg`.\n\n- `Price_overall` & `Price_avg`: Same as Book length, the price variable is almost always a good predictor.\n\n- `Review`: is boolean. It shows if the customer left a review. If so, `Review10\/10` saves the review left by the user. While most users don't left a review we fill the missing reviews by avrage review column.\n\n- `Minutes_listened`: is a measure of engagement, the total of minutes the user listen to audiobooks.\n\n- `Completion`: is the `Minutes_listened` \/ `Book_length(mins)_overall`.\n\n- `Support_Request`: Shows the total number of support request (forgotten password to assistance).\n\n- `Last_Visited_mins_Purchase_date`: the bigger the difference, the bigger sooner the engagement. If the value is 0, we are sure the customer has never accessed what he\/she has bought.\n\nThe data was gathered from the audiobook app, the input data represents `2 years` worth of engagement. We are doing supervised learning so we need `target`. We took extra `6 month` to check if the user converted or not. 1 if the customer buys in the next 6 months, 0 if the customer didn't.\n\n- `target`: 1 if the customer bought again in the last 6 months of data. 0 if the customer did not buy again.","3dbfb9ad":"# 3. Data Pre-processing\n\nSince we are dealing with real life data, we will need to preprocess it a bit. This is the relevant code which is not that hard but refers to data engineering more than machine learning.\n\n- Handling categorical features\n\n- Balance the dataset. ","e2463ae3":"# 2. Exploratory Data Analysis","820266a1":"# Business Case Study: Audiobook app\n\n![audiobooks.png](attachment:audiobooks.png)\n\nYou are given data from an Audiobook app. Logically, it relates only to the audio versions of books.  We want to create a machine learning model based on our available data that can predict if a customer will buy again from the Audiobook company.\n\n- The data is from an audiobook app, each customer in the database has make a purchase at least once.\n- The main idea is that the company shouldn't spend there money targeting individuals who are unlikely to come back.\n- If we focus on client who are more likely to convert again we'll get increase the sales and profitability figures.\n\nThe model must show us which are the most important metrics for a client to come back.","3795986d":"## `Minutes_listened` & `Completion`\n- `Minutes_listened`: is a measure of engagement, the total of minutes the user listen to audiobooks.\n- `Completion`: is the `Minutes_listened` \/ `Book_length(mins)_overall`.","d5814c12":"### Cross Validation Score for XGBoost Classifier","afe67a1c":"## `Review` & `Review10\/10`\n- `Review`: is boolean. It shows if the customer left a review. If so, `Review10\/10` saves the review left by the user. While most users don't left a review we fill the missing reviews by avrage review column.","e9839345":"### Check Duplicate columns and Rows"}}