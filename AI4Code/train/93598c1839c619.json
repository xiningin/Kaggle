{"cell_type":{"965d0628":"code","c72aaeff":"code","a879a42b":"code","0bdb776b":"code","6bffa91e":"code","018c9c08":"code","62ce631e":"code","38e1a0c7":"code","b5b019a4":"code","42653434":"code","5988a0aa":"code","50076c0b":"code","56bb07dd":"code","592bb3f7":"code","93667f78":"code","4697ee83":"code","3e322b5f":"code","ee23d6b2":"code","1ce425cd":"code","83ce56a6":"code","22ee5280":"code","2edab0f8":"code","dff564d4":"code","9ac5d0b7":"code","bf8452a1":"code","0d3d0ae6":"code","c0af45a3":"code","6551b55b":"code","81407fe0":"code","f7068434":"code","70a37df1":"code","d0850ad4":"code","f5ab815e":"code","5aabf0cf":"code","7f527511":"code","508fc0df":"code","b8d1b60e":"code","10baf3e7":"code","41d6e861":"code","af783318":"code","b838893e":"code","d8e5aff8":"code","67ca08b2":"code","801ed075":"code","f0c2ff53":"code","93e3de41":"code","f300172e":"code","c321cfac":"code","f7b8e415":"code","26072361":"code","f79df5df":"code","312170d5":"code","5cc3164e":"code","b3657eb1":"code","79de4723":"code","9dd6c723":"code","96d8a8e2":"code","5ca16ceb":"code","7b719559":"code","961731bb":"code","90193f77":"code","5b52a92e":"code","8df2151a":"code","762ca660":"code","6a2ae428":"code","e8a7fe94":"code","48b3018f":"code","e493ba56":"code","fbaea3fb":"code","c59a7e54":"code","93bb646e":"code","667326e4":"code","d7b5bf10":"code","1468cd63":"code","a7d30e02":"code","7cc3a764":"code","02553325":"code","033b881e":"code","fc88df94":"markdown","81c2bd1c":"markdown","447fe36e":"markdown","09910ef3":"markdown","16fd8445":"markdown","61bb1ae5":"markdown","7be86ae7":"markdown","4f13333a":"markdown","98c082d7":"markdown","4b1a8e19":"markdown","1e8193a8":"markdown","6dfb1672":"markdown","0a0dc3a4":"markdown","30673c8c":"markdown","edeb9456":"markdown","768e367f":"markdown","d0ca7c86":"markdown","1cb403ad":"markdown","68199f61":"markdown","f80f4a1f":"markdown","c7ff41f7":"markdown","8f5700c2":"markdown","3561876c":"markdown","6f630a3e":"markdown","900bef92":"markdown","c1bbc015":"markdown","01696bd3":"markdown","90997232":"markdown","c1b467eb":"markdown","e028861c":"markdown","0d0cc318":"markdown","8fff3661":"markdown","9376ba97":"markdown","7a698e31":"markdown","d4cc2c0e":"markdown","91bbb9c0":"markdown","8b651b9f":"markdown","322d6771":"markdown","d346cc86":"markdown","ebbc6448":"markdown","ef75d81b":"markdown","5b3092a3":"markdown","7dad480a":"markdown","c30143d0":"markdown","c986f4dc":"markdown","3bfaf041":"markdown","e4b2648f":"markdown","6e535fad":"markdown","60579078":"markdown","1a2e2558":"markdown","70b92855":"markdown"},"source":{"965d0628":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c72aaeff":"train_df=pd.read_csv('..\/input\/rossmann\/train.csv',na_values=['NA'])","a879a42b":"weather_df=pd.read_csv('..\/input\/rossmann\/weather.csv',na_values=['NA'])","0bdb776b":"store_det_df=pd.read_csv('..\/input\/rossmann\/store.csv',na_values=['NA'])","6bffa91e":"store_state=pd.read_csv('..\/input\/rossmann\/store_states.csv',na_values=['NA'])","018c9c08":"state_abbr=pd.read_csv('..\/input\/rossmann\/state_names.csv',na_values=['NA'])","62ce631e":"google_trend=pd.read_csv('..\/input\/rossmann\/googletrend.csv',na_values=['NA'])","38e1a0c7":"google_trend[['a','b','State']] = google_trend['file'].str.split('_',expand=True)\ngoogle_trend[['start_date','c']] = google_trend['week'].str.split(' - ',expand=True)","b5b019a4":"google_trend['Date'] = pd.to_datetime(google_trend['start_date'],dayfirst=True)\ngoogle_trend=google_trend.drop(['a','b','c','file','week','start_date'],axis=1)\ngoogle_trend.set_index('Date', inplace=True)\ngoogle_trend.loc[google_trend['State']=='NI','State']='HB,NI'","42653434":"google_trend2 = google_trend.groupby('State').resample('D').ffill().reset_index(level=0, drop=True).reset_index()","5988a0aa":"store_state=store_state.merge(state_abbr,how='left',on='State')","50076c0b":"weather_df=weather_df.rename(columns = {'file':'StateName'})","56bb07dd":"train_df2=train_df.merge(store_state,how='left',on='Store',copy=False)","592bb3f7":"train_df3=train_df2.merge(weather_df,how='left',on=['StateName','Date'],copy=False)","93667f78":"train_df4=train_df3.merge(store_det_df,how='left',on=['Store'],copy=False)","4697ee83":"train_df4['Date'] = pd.to_datetime(train_df4['Date'],dayfirst=True)\ntrain_df4=train_df4.merge(google_trend2 , how='left',on=['Date','State'],copy=False,)","3e322b5f":"train_df4=train_df4.sort_values(by=['Date','Store'])","ee23d6b2":"train_df4['sale_per_cust']=train_df4['Sales']\/train_df4['Customers']\ntrain_df4['year']=pd.DatetimeIndex(train_df4.Date).year\ntrain_df4['month']=pd.DatetimeIndex(train_df4.Date).month\ntrain_df4['day']=pd.DatetimeIndex(train_df4.Date).day","1ce425cd":"import plotly as py\nimport plotly.graph_objs as go\npy.offline.init_notebook_mode(connected=True)","83ce56a6":"train_df11=train_df4[train_df4['Store']==1]","22ee5280":"data = [go.Scatter(x=train_df11.Date, y=train_df11.Sales)]\nlayout = dict(\n    title='One Store Sales with Rangeslider',\n    xaxis=dict(title='Date',\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label='1m',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=6,\n                     label='6m',\n                     step='month',\n                     stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(\n            visible = True\n        ),\n        type='date'\n    ),yaxis=dict(title='Sales')\n)\nfig = dict(data=data,layout=layout)\npy.offline.iplot(fig)","2edab0f8":"train_df4=train_df4.drop(['State','StateName','DayOfWeek'],axis=1)\ntrain_df4=train_df4.set_index('Date')","dff564d4":"corr_matrix = train_df4.corr()\ncorr_matrix[\"Sales\"].sort_values(ascending=False)","9ac5d0b7":"from pandas.plotting import scatter_matrix\nattributes = ['Sales',\"Customers\", \"Open\", \"Promo\"]\nscatter_matrix(train_df4[attributes], figsize=(12, 8))\nplt.show()","bf8452a1":"train_df6=pd.get_dummies(train_df4)","0d3d0ae6":"train_df7=train_df6.loc[train_df6.Store.isin(range(1,2))]","c0af45a3":"X=train_df7.drop(['Sales'],axis=1).values\ny=train_df7.loc[:,['Sales']].values","6551b55b":"proph_df1=train_df7.loc[:,['Sales']]\nproph_df=proph_df1.rename({'Date':'ds','Sales':'y'},axis=1)","81407fe0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import ExtraTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.svm import LinearSVR\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge","f7068434":"from sklearn.impute import SimpleImputer as Imputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection  import cross_val_predict","70a37df1":"score=[]\nr2scrore=[]\nmse=[]\nmape=[]","d0850ad4":"k=int(0.8*len(X))\nX_train=X[:k]\nX_test =X[k:]\ny_train=y[:k]\ny_test=y[k:]","f5ab815e":"y_train=y_train.ravel()\ny_test=y_test.ravel()","5aabf0cf":"def mean_absolute_percentage_error(y_true, y_pred):    \n    y_true_l=y_true.tolist()\n    yPred_l=y_pred.tolist()\n    lis=[]\n    for i in range(0,len(y_true_l)):\n            if(y_true_l[i]!=0):\n                lis.append(np.abs((y_true_l[i] -yPred_l[i]) \/ y_true_l[i]))\n            \n    return np.mean(lis)*100","7f527511":"proph_df=proph_df.reset_index()\nproph_df=proph_df.rename({'Date':'ds','Sales':'y'},axis=1)","508fc0df":"lenn=int(len(proph_df)*0.8)\nproph_df_train=proph_df[:lenn]  #Dataframe contains 80% of date and sales values which is used for training\nproph_df_testX=pd.DataFrame(proph_df.loc[lenn:,:'ds'])  # This contain Dates of remaining 20% values which is used for testing\nproph_df_testy=proph_df.loc[lenn:,'y'].values # This contain Sales value of remaining 20% values which is used for\n                                            #checking errors in predicted values","b8d1b60e":"proph_df_testy=proph_df_testy.reshape(-1,1)\nproph_df_train.shape","10baf3e7":"from fbprophet import Prophet\nproph=Prophet(seasonality_prior_scale=0.1, daily_seasonality=True)\nproph.fit(proph_df_train)","41d6e861":"y_pred=proph.predict(proph_df_testX)\ny_pred=y_pred[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\ny_p=y_pred.loc[:,'yhat'].values\nprint('r2=',r2_score(proph_df_testy,y_p))\nprint('mse',mean_squared_error(proph_df_testy,y_p))\nprint(mean_absolute_percentage_error(proph_df_testy.ravel(), y_p))\nplt.figure(figsize=(20,10))\nplt.plot(y_p,color='blue',label='predicted')\nplt.plot(proph_df_testy,color='red',label='observed')\nplt.legend()\nplt.show()","af783318":"import statsmodels.api as sm\nimport itertools","b838893e":"p = d = q = range(0,3)\n\npdq = list(itertools.product(p, d, q))\n\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","d8e5aff8":"# for param in pdq:\n#     for param_seasonal in seasonal_pdq:\n#         try:\n#             mod = sm.tsa.statespace.SARIMAX(proph_df1[:lenn],\n#                                             order=param,\n#                                             seasonal_order=param_seasonal,\n#                                             enforce_stationarity=False,\n#                                             enforce_invertibility=False)\n\n#             results = mod.fit()\n            \n#             print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic)) \n#         except:\n#             continue","67ca08b2":"proph_df2=proph_df1.resample('W').mean()","801ed075":"mod = sm.tsa.statespace.SARIMAX(proph_df1[:'2015-01-01'],\n                                order=(2,0,0),\n                                seasonal_order=(2,1,2,12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\n\nresults = mod.fit()\nprint(results.summary().tables[1])","f0c2ff53":"results.plot_diagnostics(figsize=(15, 12))\nplt.show()","93e3de41":"pred = results.get_prediction(start=pd.to_datetime('2015-01-01'),end=pd.to_datetime('2015-07-31'),dynamic=True)\npred_ci = pred.conf_int()","f300172e":"ax = proph_df1['2015-01-01':].plot(label='observed', figsize=(20, 15))\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(20, 15))\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\n\nax.set_xlabel('Date')\nax.set_ylabel('SALES')\nplt.legend()\n\nplt.show()","c321cfac":"y_forecasted = pred.predicted_mean\ny_truth =proph_df1['2015-01-01':]\nprint(mean_squared_error(y_truth,y_forecasted))\n","f7b8e415":"# Get forecast 500 steps ahead in future\npred_uc = results.get_forecast(steps=500)\n\n# Get confidence intervals of forecasts\npred_ci = pred_uc.conf_int()","26072361":"ax = proph_df1.plot(label='observed', figsize=(20, 15))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('CO2 Levels')\n\nplt.legend()\nplt.show()\n","f79df5df":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('linereg', LinearRegression(normalize=True))]\npipeline = Pipeline(steps)\n#c_space =np.array\n#param_grid = {'linereg__n_jobs': c_space}\n\n#linereg_cv = GridSearchCV(pipeline, param_grid, cv=7)\npipeline.fit(X_train, y_train)\nscores = cross_val_score(pipeline, X,y,cv=10)","312170d5":"#y_pred=cross_val_predict(pipeline,X_test,y_test,cv=10)\ny_pred=pipeline.predict(X_test)\nprint('score=',pipeline.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test.ravel(), y_pred.ravel()))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred.ravel(),color='blue',label='predicted')\nplt.plot(y_test.ravel(),color='red',label='observed')\nplt.legend()\nplt.show()\nprint(scores)\nscore.append(pipeline.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test.ravel(), y_pred.ravel()))","5cc3164e":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('logreg', LogisticRegression())]\npipeline2 = Pipeline(steps)\nc_space = np.arange(0.1,1,0.1)\nparam_grid = {'logreg__C': c_space}\n\nlogreg_cv = GridSearchCV(pipeline2, param_grid, cv=6)\nlogreg_cv.fit(X_train, y_train)\n#print('ooooo')\n#scores = cross_val_score(logreg_cv, X,y,cv=10)\n#print(scores)","b3657eb1":"y_pred=logreg_cv.predict(X_test)\nprint('score=',logreg_cv.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(logreg_cv.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","79de4723":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('knn', KNeighborsRegressor(leaf_size=50,weights='distance'))]\npipeline1= Pipeline(steps)\nc_space = np.arange(1, 12, 1)\nparam_grid = {'knn__n_neighbors': c_space}\n\nknnreg_cv = GridSearchCV(pipeline1, param_grid,cv=10)\n\nknnreg_cv.fit(X_train, y_train)\nscores = cross_val_score(knnreg_cv, X,y,cv=10)\nprint(scores)","9dd6c723":"y_pred=knnreg_cv.predict(X_test)\nprint('score=',knnreg_cv.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(knnreg_cv.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","96d8a8e2":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('lasso',Lasso(normalize=True))]\npipeline3= Pipeline(steps)\nc_space = np.arange(0.1,1,0.01)\nparam_grid = {'lasso__alpha': c_space}\nlasso_reg = GridSearchCV(pipeline3, param_grid,cv=5)\nlasso_reg.fit(X_train, y_train)\n#scores = cross_val_score(lasso_reg, X,y,cv=10)\n#print(scores)","5ca16ceb":"y_pred=lasso_reg.predict(X_test)\nprint('score=',lasso_reg.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(lasso_reg.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","7b719559":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('ridge',Ridge(normalize=True))]\npipeliner= Pipeline(steps)\nc_space = np.arange(0,2,0.01)\nparam_grid = {'ridge__alpha': c_space}\n\nridge_reg = GridSearchCV(pipeliner, param_grid,cv=5)\n\nridge_reg.fit(X_train, y_train)\nscores = cross_val_score(ridge_reg, X,y,cv=10)\nprint(scores)","961731bb":"y_pred=ridge_reg.predict(X_test)\nprint('score=',ridge_reg.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(ridge_reg.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","90193f77":"from sklearn.ensemble import RandomForestRegressor\nsteps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('forrest',RandomForestRegressor(random_state=42))]\nforest_reg = Pipeline(steps)\n\nforest_reg.fit(X_train, y_train)\nscores = cross_val_score(forest_reg, X,y,cv=10)\nprint(scores)","5b52a92e":"y_pred=forest_reg.predict(X_test)\nprint('score=',forest_reg.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(forest_reg.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","8df2151a":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('elasticnet', ElasticNet(normalize=True\n                                  ))]\npipeline4= Pipeline(steps)\nc_space = np.arange(0,2,0.1)\nparameters = {'elasticnet__l1_ratio':np.linspace(0,1,30),\n             'elasticnet__alpha': c_space}\ngm_cv = GridSearchCV(pipeline4, parameters,cv=5)\ngm_cv.fit(X_train, y_train)\nscores = cross_val_score(gm_cv, X,y,cv=10)\nprint(scores)","762ca660":"y_pred=gm_cv.predict(X_test)\nprint('score=',gm_cv.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(gm_cv.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","6a2ae428":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('gradient',GradientBoostingRegressor(loss='lad',learning_rate=0.07))]\nc_space = np.arange(0.1,1,0.1)\nparam_grid = {'gradient__alpha': c_space}\n\nmodel =Pipeline(steps)\ngradient_reg = GridSearchCV(model, param_grid,cv=5)\n\ngradient_reg.fit(X_train, y_train)\nscores = cross_val_score(gradient_reg, X,y,cv=10)\nprint(scores)","e8a7fe94":"y_pred=gradient_reg.predict(X_test)\nprint('score=',gradient_reg.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(gradient_reg.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","48b3018f":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('SVM', SVR(kernel='linear'))]\npipeline5 = Pipeline(steps)\nparameters = {'SVM__C':np.arange(0.1,10,1),\n             }\ncv = GridSearchCV(pipeline5,parameters,cv=5)\ncv.fit(X_train,y_train)\nscores = cross_val_score(cv, X,y,cv=10)\nprint(scores)","e493ba56":"y_pred=cv.predict(X_test)\nprint('score=',cv.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(cv.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","fbaea3fb":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('SVM', LinearSVR())]\npipeline5 = Pipeline(steps)\nparameters = {'SVM__C':np.arange(0.1,10,1)}\ncv2 = GridSearchCV(pipeline5,parameters,cv=5)\ncv2.fit(X_train,y_train)\nscores = cross_val_score(cv2, X,y,cv=10)\nprint(scores)","c59a7e54":"y_pred=cv2.predict(X_test)\nprint('score=',cv2.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(cv2.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","93bb646e":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('Dec_tree',DecisionTreeRegressor())]\nDec_tree = Pipeline(steps)\nDec_tree.fit(X_train,y_train)\nscores = cross_val_score(Dec_tree, X,y,cv=10)\nprint(scores)","667326e4":"y_pred=Dec_tree.predict(X_test)\nprint('score=',Dec_tree.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(Dec_tree.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","d7b5bf10":"steps = [('imputation', Imputer()),\n         ('scaler', StandardScaler()),\n         ('Extra_tree',ExtraTreeRegressor())]\nExtra_tree = Pipeline(steps)\n#parameters = {'SVM__C':np.arange(0.1,10,1)}\n#cv2 = GridSearchCV(pipeline5,parameters,cv=5)\nExtra_tree.fit(X_train,y_train)\nscores = cross_val_score(Extra_tree, X,y,cv=10)\nprint(scores)","1468cd63":"y_pred=Extra_tree.predict(X_test)\nprint('score=',Extra_tree.score(X_test,y_test))\nprint('r2=',r2_score(y_test,y_pred))\nprint('mse',mean_squared_error(y_test,y_pred))\nprint(mean_absolute_percentage_error(y_test, y_pred))\nplt.figure(figsize=(20,10))\nplt.plot(y_pred,color='blue',label='predicted')\nplt.plot(y_test,color='red',label='observed')\nplt.legend()\nplt.show()\nscore.append(Extra_tree.score(X_test,y_test))\nr2scrore.append(r2_score(y_test,y_pred))\nmse.append(mean_squared_error(y_test,y_pred))\nmape.append(mean_absolute_percentage_error(y_test, y_pred))","a7d30e02":"score","7cc3a764":"models=['Linear Regression','Logistic Regression','k-nearest','Lasso','Ridge','Randomforest Regressor','Elasticnet','Gradient Boosting Regressor',\n       'Support Vector Regressor','Linear Support Vector Regressor','Decision Tree Regressor','Extra Tree Regressor']","02553325":"score2=score\nr2scrore2=r2scrore\nmse2=mse\nmape2=mape","033b881e":"trace = go.Table(\n    header=dict(values=['MODELS', 'SCORE','R2 SCORE','MSE','MAPE'],\n                line = dict(color='#7D7F80'),\n                fill = dict(color='#a1c3d1'),\n                align = ['left'] * 5),\n    cells=dict(values=[models,score2,r2scrore2,mse2,mape2],\n               line = dict(color='#7D7F80'),\n               fill = dict(color='#EDFAFF'),\n               align = ['left'] * 5))\n\nlayout = dict(width=1000, height=1000)\ndata = [trace]\nfig = dict(data=data, layout=layout)\npy.offline.iplot(fig, filename = 'styled_table')","fc88df94":"Before we build our model we need to create dummy variables for our categorical variables like the stateholiday, events etc.\nIf we take state holiday variable, the are four categories (0,a,b,c) , pandas will create dummy variables for each of these categories","81c2bd1c":"# FINDING CORRELATION","447fe36e":"# TIDYING THE DATA","09910ef3":"Scikit learn has various preprocessing features to fill the missing values and to normalize our data. Various metrics like mean square error,absolute error,r2 score etc., to check the accuracy and errors in our prediction model","16fd8445":"The main objective of this project is to create a suitable prediction model , which can help us predict the \ndaily sales of 1,115 stores of Rossmann located across Germany for 6 weeks.Training data is given containing deatils of number of customers, promos , state holidays etc ,for all the 1115 stores,for past two years(2013 to mid-2015).A suitable model should be trained out of this data given.","61bb1ae5":"After modelling we have to find the accuracy of the model, this can be done by finding the Mean absolute percentage error(MAPE) , whish is given as\n>MAPE=(y_test-y_pred)*100\/(y_test)\n\nIt gives the error between the predicted and tested values.","7be86ae7":"For statifying the data, the Sales values that are occuring only once are filtered out of the training dataframe.This helps in splitting train_df into training and testing datasets .The trainining dataset will train our model and the test dataset will check prediction errors in our model.","4f13333a":"From the correlation for 'Sales' column we can see that it is highly correlated with number of customers. The 'Sales' column also depend highly on whether the shop was open or not and whether there are any promos given in the store, the amount of sales per customer,google trends.","98c082d7":">Other details like weather data ,store location details, google weekly trend details are stored in different csv files .They are loaded into seperate dataframes and used for modelling.","4b1a8e19":"# LOGISTIC REGRESSION","1e8193a8":"# LINEAR REGRESSION","6dfb1672":"# RIDGE MODEL","0a0dc3a4":"The training and testing set is created out of the X and y values created above using the train_test_split function.Almost 75% of data is used to create X_train and y_train which is used to create the prediction model. The remaining 25% is used to test the model for errors , by comparing with y_test values.","30673c8c":"Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.","edeb9456":">Location of different stores and their state names were joined from dataframes store_state and state_abbr.These can be used to mark location of stores in training  data","768e367f":"# Rossmann Sales Prediction Models\n\nCompetition link:https:\/\/www.kaggle.com\/c\/rossmann-store-sales\/leaderboard\n\n**Upvote if you like it **","d0ca7c86":"# INTRODUCTION","1cb403ad":"This notebook uses several Python packages that come standard with the Anaconda Python distribution. The primary libraries that we'll be using are:\n\n> NumPy: Provides a fast numerical array structure and helper functions.\n\n>pandas: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n\n>scikit-learn: The essential Machine Learning package in Python.\n\n>matplotlib: Basic plotting library in Python; most other Python plotting libraries are built on top of it.Set its plotting  style default to 'fivethirtyeight'","68199f61":"# LINEAR SUPPORT VECTOR REGRESSOR","f80f4a1f":"# LASSO MODEL","c7ff41f7":"The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. For this reason, the Lasso and its variants are fundamental to the field of compressed sensing.\n>Mathematically, it consists of a linear model trained with $\\ell_1$ prior as regularizer. The objective function to minimize is:\n>$$\\min_{w} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$$\n","8f5700c2":"ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of L1 and L2 using the l1_ratio parameter.\n>The objective function to minimize is in this case : $$\\min_{w} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}$$\n","3561876c":"# ELASTICNET REGREESOR","6f630a3e":"# PROPHET MODEL","900bef92":"# ARIMA MODEL","c1bbc015":"Linear Regression fits a linear model with coefficients  to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.\n>$$ \\min_{w} {|| X w - y||_2}^2$$\nHere a linear prediction model is generated by least squares method.","01696bd3":"Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors.\n>KNeighborsRegressor implements learning based on the k nearest neighbors of each query point, where  k is an integer value specified by the user.Basically,nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. ","90997232":">Now let's plot a sample store data , we choose store 1 and plot the total sales of the store on the dates given. We have a date range selector option , buttons to see previous one month , 6 month data or all data at once.\n\nWe use a interactive python plotting module called plotly, It provise many features like zooming , paning, range selection etc.of the plot given.","c1b467eb":"# REGRESSION","e028861c":"Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares,\n>$$\\min_{w} {{|| X w - y||_2}^2 + \\alpha {||w||_2}^2}$$\n\nHere, $\\alpha \\geq 0$ is a complexity parameter that controls the amount of shrinkage: the larger the value of $\\alpha$ the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.","0d0cc318":"The correlation matrix is found for the training dataframe, to find the co-dependencies of varables on each other. It will be a symmetric matrix with values in between 0 and 1, 1 being highly dependent.\n\nFrom this we take out the correlations for the Sales data given.","8fff3661":"# IMPORTING DATA","9376ba97":"# DECISION TREE REGRESSOR","7a698e31":"All datasets were loaded as dataframes into this notebook.As the data is pread across different data frames we should clean this data for further analysis and creating our prediction model.","d4cc2c0e":"# RANDOM FOREST REGRESSOR","91bbb9c0":">Training  dataframe with weather data of the day , details on location of store was created.Columns like name of the state ,day of week  are not necessary for modelling,so the are dropped from the dataframe.\n    We can create a new column called 'Sales per customer' by dividing sales and number of customers.","8b651b9f":"# k- NEAREST NEIGHBOUR REGRESSOR","322d6771":"Assured that our data is now as clean as we can make it \u2014 and armed with some cursory knowledge of the distributions and relationships in our data set \u2014 it's time to make the next big step in our analysis: Splitting the data into training and testing sets.\n\n>A training set is a random subset of the data that we use to train our models.\n\n>A testing set is a random subset of the data (mutually exclusive from the training set) that we use to validate our models on unforseen data.","d346cc86":"# GRADIENT BOOSTING REGRESSOR","ebbc6448":"After creating the dummy variables, we choose data for the first five stores and try to form a model for these 10 Stores","ef75d81b":"Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n\nAs an optimization problem, binary class L2 penalized logistic regression minimizes the following cost function:\n>$\\min_{w, c} \\frac{1}{2}w^T w + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) .$\n\nSimilarly, L1 regularized logistic regression solves the following optimization problem\n>$\\min_{w, c} \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1).$\n\n","5b3092a3":"The date column in the dataframe was split into date , month and year columns for halping in regression modelling , since timestamp variables cannot be used in that.","7dad480a":"# SUPPORT VECTOR REGRESSOR","c30143d0":"# EXTRA TREE REGRESSOR","c986f4dc":"# COMPARISON OF MODELS (FOR 1 STORE)","3bfaf041":"The data for training and testing is distributed across various files.The weather data,location of stores are given in separate dataframes.These should be bought together in a single dataframe for training.","e4b2648f":"Dataframes are created for the testing and training data using the csv files given. These files are imported into python environment using pandas module and we label the missing value as 'NA'.","6e535fad":"This is a extremely randomized tree regressor.Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. ","60579078":"First, we clean our google trends weekly data ,by resampling it into daily values so that we can merge it into our training and testing dataframes","1a2e2558":"Scikit learn module has various regression methods to create our model using methods like linear,logistic, support vector machine etc.The model is created by adjusting the parameters inside these regressors, and the best regressor is choosen as our final model.","70b92855":"The scatter plot of 'Sales' and 'Customers' show how strong their correlation is. We can see an upward trend and the points are not too dispersed"}}