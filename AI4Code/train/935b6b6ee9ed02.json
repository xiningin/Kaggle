{"cell_type":{"e0bc5215":"code","91e866b1":"code","0782be23":"code","c736516b":"code","60745804":"code","3b76a389":"code","83e5dff4":"code","2bbdfecc":"code","8235e535":"code","6306480c":"code","7f227dab":"code","0fa7ebc6":"code","b4045502":"code","c776473a":"code","f031449f":"code","f22c6e12":"code","5baa28d7":"code","c9c72c32":"code","425bb182":"code","eb04a0e7":"code","198703fb":"code","74676fc3":"code","8ae434a6":"code","4dc94d52":"code","13615a96":"code","a3e1d567":"code","fe7226ae":"code","05bb957a":"code","5584b11a":"code","85f78a14":"code","32801523":"code","ea48c31e":"code","85e2bc04":"code","b67e6712":"code","8f4d6edf":"code","192f20d7":"code","3c0d723f":"code","75075a2f":"code","9b083092":"code","61836d2f":"code","8ecfc414":"code","5f259b0a":"code","b5a49869":"code","5f3e0fac":"code","008eed02":"code","a92487b0":"markdown","92122556":"markdown","7d8ee437":"markdown","22deb93d":"markdown","de0bd6f6":"markdown","ab13e35b":"markdown","1c06c8a0":"markdown","81996315":"markdown","9e5bac32":"markdown","fc4dfa4d":"markdown","f8cf76ed":"markdown","8fba7ef6":"markdown","454356a6":"markdown","921de58d":"markdown","7f9e69e6":"markdown","bde60f1b":"markdown","bedc217c":"markdown","754f898b":"markdown","f47e09c3":"markdown","991a9001":"markdown","90f86244":"markdown","9b637d24":"markdown","5138748d":"markdown","d56fe57d":"markdown","0ee22008":"markdown","e5613a5a":"markdown","313e97ce":"markdown","ef75f9f5":"markdown"},"source":{"e0bc5215":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n% matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","91e866b1":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(train_df.head())\nprint(test_df.head())","0782be23":"# For now, We jest explore training data first. Because when I am done with the training data, We will do the same things to testing data\ntrain_df.isnull().sum()","c736516b":"# First, we can find that the 'Cabin' variable has just 204 values, which is far less than other variables' values.\n# So we need to remove it\ntrain_df = train_df.drop(['Cabin'], axis = 1)\n# What's more, the passengerID, Name and Ticket seem no use for further analysis. And we can remove them\ntrain_df = train_df.drop(['PassengerId', 'Name', 'Ticket'], axis = 1)","60745804":"train_df.head()","3b76a389":"# For \"Age\"\nmin_age = train_df['Age'].min()\nmax_age = train_df['Age'].max()\nnull_values_count = train_df['Age'].isnull().sum()\nage_fill_values = np.random.randint(min_age, max_age, size = null_values_count)\ntrain_df['Age'][np.isnan(train_df['Age'])] = age_fill_values","83e5dff4":"# For \"Embarked\"\ntrain_df['Embarked'].unique()","2bbdfecc":"print(train_df['Embarked'][train_df['Embarked'] == 'S'].count()) # 644\nprint(train_df['Embarked'][train_df['Embarked'] == 'C'].count()) # 168\nprint(train_df['Embarked'][train_df['Embarked'] == 'Q'].count()) # 77 ","8235e535":"# For \"Embarked\" variable, we can use 'S' to fill it because it has the largest frequency\ntrain_df['Embarked'] = train_df['Embarked'].fillna('S')","6306480c":"train_df.count() # Make sure that all null values have been filled with some values","7f227dab":"corr = train_df.corr()\nsns.heatmap(corr, cmap = 'coolwarm', linewidth = 1, linecolor = 'white')\n# From heatmap, we can see that there are some relations between survival and pclass, sex, age, sibsp, parch, fare and embark","0fa7ebc6":"pclass_df = train_df[['Survived', 'Pclass']]\nfig = plt.figure(figsize = (10,5))\naxes0 = plt.subplot(1,2,1)\naxes0 = sns.countplot(x = 'Pclass', hue = 'Survived', data = pclass_df)\naxes0.set_title('The Number of Survival Based on Class')\naxes0.set_ylabel('Count')\naxes0.legend(['No', 'Yes'])\n\nsurvival_pclass = pclass_df.groupby(['Pclass'], as_index = False).mean()\naxes1 = plt.subplot(1,2,2)\naxes1 = sns.barplot(x = 'Pclass', y = 'Survived', data = survival_pclass)  # sns barplot\naxes1.set_title('The Survival Rate Based on Class')\naxes1.set_ylabel('Survival Rate')\n\nplt.tight_layout()\n# It seems that the higher class passengers are, the more possible they can survive. \n# Therefore we need to produce two dummy variables to describe 'Pclass' according to Pclass values","b4045502":"train_df['High Class'] = np.where(train_df['Pclass'] == 1, 1, 0)\ntrain_df['Median Class'] = np.where(train_df['Pclass'] == 2, 1, 0)\ntrain_df.head()","c776473a":"sex_df = train_df[['Survived', 'Sex']]\nfig = plt.figure(figsize = (10,5))\naxes0 = plt.subplot(1,2,1)\naxes0 = sns.countplot(x = 'Sex', hue = 'Survived', data = sex_df)\naxes0.set_title('The Number of Survival Based on Sex')\naxes0.set_ylabel('Count')\naxes0.legend(['No', 'Yes'])\n\nsurvival_sex = sex_df.groupby(['Sex'], as_index = False).mean()\naxes1 = plt.subplot(1,2,2)\naxes1 = sns.barplot(x = 'Sex', y = 'Survived', data = survival_sex, palette = 'Set1') # sns factorplot\naxes1.set_title('The Survival Rate Based on Sex')\naxes1.set_ylabel('Survival Rate')\n# It shows that female has more chances to survival\n# We also need to transfer Sex to a dummy variable","f031449f":"train_df = train_df.replace({'Sex': {'male':0, 'female':1}})\ntrain_df.head()","f22c6e12":"age_df = train_df[['Survived', 'Age']]\nage_df['Age'] = pd.cut(age_df['Age'], bins = 5, labels = [1, 2, 3, 4, 5])\nfig = plt.figure(figsize = (10,5))\naxes0 = plt.subplot(1,2,1)\naxes0 = sns.countplot(x = 'Age', hue = 'Survived', data = age_df)\naxes0.set_title('The Number of Survival Based on Age')\naxes0.set_ylabel('Count')\naxes0.legend(['No', 'Yes'])\n\nsurvival_age = age_df.groupby(['Age'], as_index = False).mean()\naxes1 = plt.subplot(1,2,2)\naxes1 = sns.barplot(x = 'Age', y = 'Survived', data = survival_age, palette = 'Set1') \naxes1.set_title('The Survival Rate Based on Age')\naxes1.set_ylabel('Survival Rate')\n\n# It indicates that age has influence on survival\n# But we need to normalize Age values to make sure it belongs to [0,1]","5baa28d7":"age = train_df[['Age']].values.astype(float)\nmin_max_scaler = preprocessing.MinMaxScaler()\nage_scaled = min_max_scaler.fit_transform(age)\ntrain_df['Age'] = pd.DataFrame(age_scaled)\ntrain_df.head()","c9c72c32":"sibsp_df = train_df[['Survived', 'SibSp']]\nsurvival_sibsp = sibsp_df.groupby(['SibSp'], as_index = False).mean()\nsns.barplot(x = 'SibSp', y = 'Survived', data = survival_sibsp)\n# Different \"SibSps\" have different survival rate","425bb182":"sibsp_df['SibSp'] = np.where(sibsp_df['SibSp'] > 0, 1, 0)\nfig = plt.figure(figsize = (10,5))\naxes0 = plt.subplot(1,2,1)\naxes0 = sns.countplot(x = 'SibSp', hue = 'Survived', data = sibsp_df)\naxes0.set_title('The Number of Survival Based on SibSp')\naxes0.set_ylabel('Count')\naxes0.legend(['No', 'Yes'])\n\nsurvival_sibsp = sibsp_df.groupby(['SibSp'], as_index = False).mean()\naxes1 = plt.subplot(1,2,2)\naxes1 = sns.barplot(x = 'SibSp', y = 'Survived', data = survival_sibsp, palette = 'Set1') \naxes1.set_title('The Survival Rate Based on SibSp')\naxes1.set_ylabel('Survival Rate')","eb04a0e7":"# Transfer 'SibSp' to dummy variables in training dataframe\ntrain_df['SibSp'] = np.where(train_df['SibSp'] > 0, 1, 0)\ntrain_df.head()","198703fb":"parch_df= train_df[['Survived', 'Parch']]\nsurvival_parch = parch_df.groupby(['Parch'], as_index = False).mean()\nsns.barplot(x = 'Parch', y = 'Survived', data = survival_parch)\n# Different Parches have different survival rate","74676fc3":"parch_df['Parch'] = np.where(parch_df['Parch'] > 0, 1, 0)\nfig = plt.figure(figsize = (10,5))\naxes0 = plt.subplot(1,2,1)\naxes0 = sns.countplot(x = 'Parch', hue = 'Survived', data = parch_df)\naxes0.set_title('The Number of Survival Based on Parch')\naxes0.set_ylabel('Count')\naxes0.legend(['No', 'Yes'])\n\nsurvival_parch = parch_df.groupby(['Parch'], as_index = False).mean()\naxes1 = plt.subplot(1,2,2)\naxes1 = sns.barplot(x = 'Parch', y = 'Survived', data = survival_parch, palette = 'Set1') \naxes1.set_title('The Survival Rate Based on Parch')\naxes1.set_ylabel('Survival Rate')","8ae434a6":"# Transfer 'Parch' to dummy variables in training dataframe\ntrain_df['Parch'] = np.where(train_df['Parch'] > 0, 1, 0)\ntrain_df.head()","4dc94d52":"fare_df = train_df[['Survived', 'Fare']]\nfare_df['Fare'] = pd.cut(fare_df['Fare'], bins = 5, labels = [1, 2, 3, 4, 5])\nfig = plt.figure(figsize = (10,5))\naxes0 = plt.subplot(1,2,1)\naxes0 = sns.countplot(x = 'Fare', hue = 'Survived', data = fare_df)\naxes0.set_title('The Number of Survival Based on Fare')\naxes0.set_ylabel('Count')\naxes0.legend(['No', 'Yes'])\n\nsurvival_fare = fare_df.groupby(['Fare'], as_index = False).mean()\naxes1 = plt.subplot(1,2,2)\naxes1 = sns.barplot(x = 'Fare', y = 'Survived', data = survival_fare, palette = 'Set1') \naxes1.set_title('The Survival Rate Based on Fare')\naxes1.set_ylabel('Survival Rate')\n# By dividing fare into several parts, I just want to maker sure whether there is relationship between fare and survival\n# Now we can see that this relationship does exist. And we need to normalize it","13615a96":"fare = train_df[['Fare']].values.astype(float)\nmin_max_scaler = preprocessing.MinMaxScaler()\nfare_scaled = min_max_scaler.fit_transform(fare)\ntrain_df['Fare'] = pd.DataFrame(fare_scaled)\ntrain_df.head()","a3e1d567":"embarked_df = train_df[['Survived', 'Embarked']]\nfig = plt.figure(figsize = (10,5))\naxes0 = plt.subplot(1,2,1)\naxes0 = sns.countplot(x = 'Embarked', hue = 'Survived', data = embarked_df)\naxes0.set_title('The Number of Survival Based on Embark')\naxes0.set_ylabel('Count')\naxes0.legend(['No', 'Yes'])\n\nsurvival_embarked = embarked_df.groupby(['Embarked'], as_index = False).mean()\naxes1 = plt.subplot(1,2,2)\naxes1 = sns.barplot(x = 'Embarked', y = 'Survived', data = survival_embarked, palette = 'Set1') \naxes1.set_title('The Survival Rate Based on Embark')\naxes1.set_ylabel('Survival Rate')\n# The \"Embarked\" variable can also affect survival","fe7226ae":"train_df['Embarked C'] = np.where(train_df['Embarked'] == 'C', 1, 0)\ntrain_df['Embarked Q'] = np.where(train_df['Embarked'] == 'Q', 1, 0)\ntrain_df.head()","05bb957a":"final_train_df = train_df[['Survived', 'High Class', 'Median Class', 'Sex', 'Age', 'SibSp', \n                           'Parch', 'Fare', 'Embarked C', 'Embarked Q']]\nfinal_train_df.head()","5584b11a":"independent_v_train = final_train_df[['High Class', 'Median Class', 'Sex', 'Age', 'SibSp', \n                                      'Parch', 'Fare', 'Embarked C', 'Embarked Q']]\ndependent_v_train = final_train_df['Survived']","85f78a14":"gau = GaussianNB()\ngau.fit(independent_v_train, dependent_v_train)\ngau.score(independent_v_train, dependent_v_train)","32801523":"svc = SVC()\nsvc.fit(independent_v_train, dependent_v_train)\nsvc.score(independent_v_train, dependent_v_train)","ea48c31e":"per = Perceptron()\nper.fit(independent_v_train, dependent_v_train)\nper.score(independent_v_train, dependent_v_train)","85e2bc04":"log = LogisticRegression()\nlog.fit(independent_v_train, dependent_v_train)\nlog.score(independent_v_train, dependent_v_train)","b67e6712":"rf = RandomForestClassifier()\nrf.fit(independent_v_train, dependent_v_train)\nrf.score(independent_v_train, dependent_v_train)","8f4d6edf":"dt = DecisionTreeClassifier()\ndt.fit(independent_v_train, dependent_v_train)\ndt.score(independent_v_train, dependent_v_train)","192f20d7":"kn = KNeighborsClassifier()\nkn.fit(independent_v_train, dependent_v_train)\nkn.score(independent_v_train, dependent_v_train)","3c0d723f":"sdg = SGDClassifier()\nsdg.fit(independent_v_train, dependent_v_train)\nsdg.score(independent_v_train, dependent_v_train)","75075a2f":"gb = GradientBoostingClassifier()\ngb.fit(independent_v_train, dependent_v_train)\ngb.score(independent_v_train, dependent_v_train)","9b083092":"result_dict = {'Model' :['Gaussian', 'SVC', 'Perceptron', 'Logistic', 'RandomForest', \n                         'DecisionTree', 'K-Neibours', 'SGD', 'GradientBoosting'], \n              'Score': [gau.score(independent_v_train, dependent_v_train), svc.score(independent_v_train, dependent_v_train),\n                       per.score(independent_v_train, dependent_v_train), log.score(independent_v_train, dependent_v_train),\n                       rf.score(independent_v_train, dependent_v_train), dt.score(independent_v_train, dependent_v_train),\n                       kn.score(independent_v_train, dependent_v_train), sdg.score(independent_v_train, dependent_v_train), \n                       gb.score(independent_v_train, dependent_v_train)]}\npd.DataFrame(result_dict)","61836d2f":"test_df.count()","8ecfc414":"# Remove unrelated variables\ntest_df = test_df.drop(['Cabin', 'PassengerId', 'Name', 'Ticket'], axis = 1)\n# Transform Pclass and Sex variable\ntest_df['High Class'] = np.where(test_df['Pclass'] == 1, 1, 0)\ntest_df['Median Class'] = np.where(test_df['Pclass'] == 2, 1, 0)\ntest_df = test_df.replace({'Sex': {'male':0, 'female':1}})\n# Fill NAs in Age and normalizing values\nmin_age = test_df['Age'].min()\nmax_age = test_df['Age'].max()\nnull_values_count = test_df['Age'].isnull().sum()\nage_fill_values = np.random.randint(min_age, max_age, size = null_values_count)\ntest_df['Age'][np.isnan(test_df['Age'])] = age_fill_values\nage = test_df[['Age']].values.astype(float)\nmin_max_scaler = preprocessing.MinMaxScaler()\nage_scaled = min_max_scaler.fit_transform(age)\ntest_df['Age'] = pd.DataFrame(age_scaled)","5f259b0a":"# Transform SibSp and Parch variables\ntest_df['SibSp'] = np.where(test_df['SibSp'] > 0, 1, 0)\ntest_df['Parch'] = np.where(test_df['Parch'] > 0, 1, 0) \n# Fill NA in Fare and Normalizing values\ntest_df['Fare'] = test_df['Fare'].fillna(test_df['Fare'].mean())\nfare = train_df[['Fare']].values.astype(float)\nmin_max_scaler = preprocessing.MinMaxScaler()\nfare_scaled = min_max_scaler.fit_transform(fare)\ntest_df['Fare'] = pd.DataFrame(fare_scaled)\n# Fill NA in Embarked and transform values\nprint(test_df['Embarked'][test_df['Embarked'] == 'S'].count()) # 270\nprint(test_df['Embarked'][test_df['Embarked'] == 'C'].count()) # 102\nprint(test_df['Embarked'][test_df['Embarked'] == 'Q'].count()) # 46\ntest_df['Embarked'] = test_df['Embarked'].fillna('S')\ntest_df['Embarked C'] = np.where(test_df['Embarked'] == 'C', 1, 0)\ntest_df['Embarked Q'] = np.where(test_df['Embarked'] == 'Q', 1, 0)\ntest_df.count()","b5a49869":"final_test_df = test_df[['High Class', 'Median Class', 'Sex', 'Age', 'SibSp', \n                           'Parch', 'Fare', 'Embarked C', 'Embarked Q']]\nfinal_test_df.head()","5f3e0fac":"independent_v_test = final_test_df\ndependent_v_test_predict = gb.predict(independent_v_test)\nsurvival_df = pd.DataFrame(dependent_v_test_predict)\ntest_get_id = pd.read_csv('..\/input\/test.csv')\nprediction_df = pd.DataFrame(test_get_id['PassengerId'])\nprediction_df['Survived'] = survival_df","008eed02":"prediction_df.to_csv('Prediction of Titanic.csv', index=False)","a92487b0":"## Build Prediction Model","92122556":"### The Relationship Among Different Variables and Survival\nWhen we find some variables are related to survival rate, we may keep those variables and sometime may transform them to a more suitable mode.","7d8ee437":"### K-Neighbors Model","22deb93d":"## Further Exploration of Training Data - Clean Data Deeperly\nIn this part, we will first fill the blank values of \"Age\" and \"Embarker\". And then we will transfer some variables, such as \"Sex\" and \"Embarked\", into dummy variables. Last, we will use different kinds of plots to see whether specific variable is related with the passengers' survival","de0bd6f6":"## Basic Operation about Dataframe - Prepare Data and Clean Data","ab13e35b":"### Perceptron Model","1c06c8a0":"### Logistic Model","81996315":"### GradientBoosting Model","9e5bac32":"### RandomForest Model","fc4dfa4d":"#### Embarked and Survival","f8cf76ed":"#### Sex and Survival","8fba7ef6":"### SVC Model","454356a6":"### DecisionTree Model","921de58d":"#### Correlation of Every Variables","7f9e69e6":"# Machine Learning of Titanic\nIn this notebook, I will not only provide the basic steps to use machine learning knowledge to make predictions but also unite all important steps, define a interactive function, and make Titanic's machine learning more direct and clear","bde60f1b":"## Import the Packages We Need","bedc217c":"### Fill All Null Values","754f898b":"#### Pclass and Survival","f47e09c3":"### Final Training Dataframe\nWe have seen all variables in the training dataframe will influence passengers' survival rate. Therefore, we need include any of them to build up our models. However, before that, we have to delete some extral ones","991a9001":"### Predict","90f86244":"#### Fare and Survival","9b637d24":"### SGD Model","5138748d":"## Make Predictions","d56fe57d":"#### SibSp and Survival","0ee22008":"#### Parch and Survival","e5613a5a":"### Clean the Testing Data","313e97ce":"### Gaussian Model","ef75f9f5":"#### Age and Survival"}}