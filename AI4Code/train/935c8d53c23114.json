{"cell_type":{"6ba53274":"code","11d06ec2":"code","627f06da":"code","7cf189d2":"code","abc958ad":"code","a6cf350f":"code","3cdb67c8":"code","35cd6f4f":"code","2e26970a":"code","12859d75":"code","5180d2ac":"code","891932c0":"code","90925b5a":"code","3483a38d":"code","e00f244e":"code","0006c929":"code","64b0b27a":"code","afd5f085":"code","c86a7f58":"code","0bcef602":"code","9001abfd":"code","4d0e9e7f":"code","add5d27a":"code","7dcc05a9":"code","4613e921":"code","1b69a72c":"code","61abcda7":"code","e27be734":"code","fda787b0":"markdown","f2cb2f2b":"markdown","fca0805e":"markdown","f7f25aa1":"markdown","abd8d767":"markdown","8df7e4cf":"markdown","397adacc":"markdown","2eb52611":"markdown","678d4645":"markdown","0b1a2c0f":"markdown","a0edf3b5":"markdown","5183897d":"markdown","237c59d8":"markdown","9d18bf53":"markdown","c221373c":"markdown","52868425":"markdown","e9759127":"markdown","7a36d597":"markdown","d2b38623":"markdown"},"source":{"6ba53274":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, auc, matthews_corrcoef, f1_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport pickle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\n\n%matplotlib inline","11d06ec2":"train=pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')","627f06da":"train.head()","7cf189d2":"train.shape","abc958ad":"sns.countplot(x=\"target\", data=train);","a6cf350f":"train.info()","3cdb67c8":"train.dtypes.to_dict()","35cd6f4f":"[i for i in train.columns if train[i].isna().any()]","2e26970a":"X = train.iloc[:,1:-1]\ny = train.target","12859d75":"def ReduceMem(data):\n    feature_cols = data.columns.tolist()\n    \n    print(\"Memory usage before: \", data.memory_usage(deep=True).sum()\/(1024**3),\" GB\")\n    for col in feature_cols:\n        if data[col].dtype=='float64':\n            data[col] = data[col].astype('float32')\n        else:\n            data[col] = data[col].astype('uint8')\n    \n    print(\"Memory usage after: \", data.memory_usage(deep=True).sum()\/(1024**3), \"GB\")\n    \n    return data","5180d2ac":"X = ReduceMem(X)","891932c0":"scaler = preprocessing.StandardScaler()\nscaled_features = scaler.fit_transform(X.values)\nX = pd.DataFrame(scaled_features, index=X.index, columns=X.columns)\nX.head()","90925b5a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)","3483a38d":"X_train.shape","e00f244e":"X_test.shape","0006c929":"result = []","64b0b27a":"def print_roc_auc_score(model, test_data, label):\n    y_pred = model.predict_proba(test_data)[::,1]\n    auc = metrics.roc_auc_score(label, y_pred)    \n    fpr_logit, tpr_logit, _ = metrics.roc_curve(label, y_pred)    \n    plt.plot(fpr_logit,tpr_logit,label=\"AUC Curve, auc={:.3f})\".format(auc))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc=4)\n    plt.show()    \n    print(\"AUC Score :\", auc)  \n    \n    return auc","afd5f085":"lr = LogisticRegression(C=0.00003, solver='lbfgs',max_iter=1000)\nlr.fit(X_train, y_train)\nlr_auc = print_roc_auc_score(lr, X_test, y_test)\nresult.append([\"Logistic regression all features\",lr_auc])","c86a7f58":"xgb = XGBClassifier(random_state=10, use_label_encoder=False)\nxgb.fit(X_train, y_train, eval_metric='aucpr');\nxgboost_auc = print_roc_auc_score(xgb, X_test, y_test)\nresult.append([\"XGBoost all features\",xgboost_auc])","0bcef602":"adaBoost = AdaBoostClassifier()\nadaBoost.fit(X_train, y_train);\nadaBoost_auc = print_roc_auc_score(adaBoost, X_test, y_test)\nresult.append([\"AdaBoost all features\",adaBoost_auc])","9001abfd":"catBoost = CatBoostClassifier()\ncatBoost.fit(X_train, y_train);\ncatBoost_auc = print_roc_auc_score(catBoost, X_test, y_test)\nresult.append([\"catBoost all features\",catBoost_auc])","4d0e9e7f":"lgbm = LGBMClassifier()\nlgbm.fit(X_train, y_train);\nlgbm_auc = print_roc_auc_score(lgbm, X_test, y_test)\nresult.append([\"lightgbm all features\",lgbm_auc])","add5d27a":"df_results = pd.DataFrame(result)\ndf_results = df_results.rename(columns={0:\"ModelName\",1:\"Auc Score\"})\ndf_results","7dcc05a9":"df_results.iloc[df_results[\"Auc Score\"].argmax()]","4613e921":"model = LogisticRegression(C=0.00003, solver='lbfgs',max_iter=1000)\nmodel.fit(X, y)","1b69a72c":"X_test = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\", index_col='id')\nscaler = preprocessing.StandardScaler()\nscaled_features = scaler.fit_transform(X_test.values)\nX_test = pd.DataFrame(scaled_features, index=X_test.index, columns=X_test.columns)\nX_test.head()","61abcda7":"predict = model.predict_proba(X_test)[::,1]\npredictions = pd.DataFrame({\"id\":X_test.index, \"target\":predict})","e27be734":"predictions.to_csv('submission.csv', index=False)","fda787b0":"As we can see that models trained on selected features using ExtraTreesClassifier have performed really well even after removing 276 of its features, which really shows the power of choosing the right features. Training only on 9 features take small amount of time for training but for the sake of submission we are going to use the model with highest score i.e. catBoost model with all of the features of training data.","f2cb2f2b":"This notebook contains a performance comparison of different baseline models without parameter optimization to give everyone an idea of Tabular playground November's dataset. This notebook can be used as starting point for anyone. If you do like this notebook, don't forget to <b>upvote<\/b>.","fca0805e":"### DataSplit \n\nThe provided test set doesn't contain a target variable, So we'll create a test set from training data to evaluate its performance.","f7f25aa1":"#### Importing Libraries","abd8d767":"### Logistic Regression","8df7e4cf":"## LightGBM","397adacc":"### Introduction\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.","2eb52611":"### Insights from data\n\n- The complete data contains 600000 rows and 102 columns out of which we have 100 feature columns.\n- The target column is either 0 or 1 and both contain approximately same amount of data\n- There are no null values","678d4645":"### XGBoost","0b1a2c0f":"## CatBoost","a0edf3b5":"The data doesn't contain any categorical variable so we can start ahead. Now lets check for any null value","5183897d":"#### Reduce Memory usage\n\nThere are a million rows with 100 feature columns which is taking around 466.9 mb of RAM. So, it is better to reduce some memory by changing its datatype to store fewer floating points precision.\n\nHere's a quick comparison of integer data type. which shows the values we can store in it.\n- int8 can store integers from -128 to 127.\n- int16 can store integers from -32768 to 32767.\n- int64 can store integers from -9223372036854775808 to 9223372036854775807.","237c59d8":"## AdaBoost","9d18bf53":"We are going to use different models and choose the best performing model among those.","c221373c":"### Normalization","52868425":"#### Reading Data","e9759127":"### Summary ","7a36d597":"## Classification","d2b38623":"## Submission"}}