{"cell_type":{"ee7b2dca":"code","2a7db125":"code","52fc25dd":"code","dd29a431":"code","7f21b44d":"code","0fa68d4b":"code","fa9cc734":"code","c006594f":"code","9bccb674":"markdown","a54216a5":"markdown","7fe03f13":"markdown"},"source":{"ee7b2dca":"import os\nimport xml.etree.ElementTree as ET\n\nimport torch\nimport torchvision\n\n# for testing only\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt","2a7db125":"!cat ..\/input\/annotation\/Annotation\/n02085620-Chihuahua\/n02085620_10074","52fc25dd":"# This loader will use the underlying loader plus crop the image based on the annotation\ndef doggo_loader(path):\n    img = torchvision.datasets.folder.default_loader(path) # default loader\n    \n    # Get bounding box\n    annotation_basename = os.path.splitext(os.path.basename(path))[0]\n    annotation_dirname = next(dirname for dirname in os.listdir('..\/input\/annotation\/Annotation\/') if dirname.startswith(annotation_basename.split('_')[0]))\n    annotation_filename = os.path.join('..\/input\/annotation\/Annotation', annotation_dirname, annotation_basename)\n    tree = ET.parse(annotation_filename)\n    root = tree.getroot()\n    objects = root.findall('object')\n    for o in objects:\n        bndbox = o.find('bndbox')\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n    bbox = (xmin, ymin, xmax, ymax)\n    \n    # return cropped image\n    return img.crop(bbox)\n\n\n# The dataset (example)\ndataset = torchvision.datasets.ImageFolder(\n    '..\/input\/all-dogs\/',\n    loader=doggo_loader, # THE CUSTOM LOADER\n    transform=torchvision.transforms.Compose([\n        torchvision.transforms.Resize(64),\n        torchvision.transforms.CenterCrop(64),\n        torchvision.transforms.ToTensor(),\n    ]) # some transformations, add your data preprocessing here\n)","dd29a431":"# Check that it all loads without a bug\nfor i in tqdm(range(len(dataset))):\n    _ = dataset[i]\nprint('Ok.')","7f21b44d":"# Check that we get only the CUTE DOGS OH YES WHOS THE GOOD DOGGO ITS YOU\nn = 10\n_, axes = plt.subplots(figsize=(4*n, 4*n), ncols=n, nrows=n)\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(dataset[i][0].permute(1, 2, 0).detach().numpy())\nplt.show()","0fa68d4b":"# benchmark this loader vs vanilla\n\ndataset_vanilla = torchvision.datasets.ImageFolder(\n    '..\/input\/all-dogs\/',\n    transform=torchvision.transforms.Compose([\n        torchvision.transforms.Resize(64),\n        torchvision.transforms.CenterCrop(64),\n        torchvision.transforms.ToTensor(),\n    ])\n)","fa9cc734":"%%timeit\ndataset[0]","c006594f":"%%timeit\ndataset_vanilla[0]","9bccb674":"# Loading the cropped dogs seamlessly with Pytorch\n\nIn this competition, there are many pictures with multiple dogs, humans and other stuff that can disturb our GANs.\n\nWhat want are the DOGGOS.\n\nThis code was made for kernel using Pytorch.","a54216a5":"See the structure of the annotation. It is a classic XML with the bbox at `annotation\/object\/bndbox****`.","7fe03f13":"Hope it'll be usefull !\n\nDon't forget to +1 if you'll use it :)\n\nCheers,\nGuillaume"}}