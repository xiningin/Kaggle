{"cell_type":{"5852d25b":"code","02c20799":"code","f477578f":"code","bd06440e":"code","25822e02":"code","bcbb0b33":"code","0da05476":"code","3ba5fe25":"code","c19623be":"code","7712bd5c":"code","c06dc4fb":"code","f5f8e601":"code","0d1a08f0":"code","9589bdf9":"code","3bee2727":"code","5eb06083":"code","abac08c9":"code","cba4de2c":"code","50bcfde3":"code","549d8fae":"code","ba8d159e":"code","4a68ab65":"code","bdc950d2":"code","c6309eb9":"code","5e269e85":"code","f34c7025":"code","479feb3b":"code","43fc00fc":"code","b99b0517":"code","4f15cc1e":"code","c4d650eb":"code","d7d8a99e":"markdown","31d6ce08":"markdown","110f9b0f":"markdown","11bde0ea":"markdown","08e90e72":"markdown","b7cc9db0":"markdown","bb0a695d":"markdown","cdf5efb1":"markdown","d5ab5f72":"markdown","4f031209":"markdown","5eb7f910":"markdown","5f24a033":"markdown","91ea7b76":"markdown","2585b6b6":"markdown","2f6b732d":"markdown","d9357472":"markdown","8233b7a5":"markdown","b32ec167":"markdown","7bc5dca4":"markdown","2d89f4d1":"markdown","300b8d39":"markdown","fc88746a":"markdown","b64a48b1":"markdown","c7fd979e":"markdown"},"source":{"5852d25b":"from google.colab import drive\ndrive.mount('\/content\/gdrive\/')","02c20799":"!pip install catboost","f477578f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nfrom tqdm import tqdm\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n\nfrom sklearn.preprocessing import *\nfrom sklearn import preprocessing\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.metrics import *","bd06440e":"path = '\/content\/gdrive\/MyDrive\/kaggle\/'\nimport os\nos.listdir(path)","25822e02":"train = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsubmission = pd.read_csv(path + 'sample_submission.csv')","bcbb0b33":"train.drop('id',axis=1, inplace=True)\ntest.drop('id',axis=1, inplace=True)","0da05476":"encoder = LabelEncoder()\ntrain_label = train.copy()\ntest_label = test.copy()\n\nfor i in range(10):\n    train_label.iloc[:,i] = encoder.fit_transform(train.iloc[:,i])\n    test_label.iloc[:,i] = encoder.transform(test.iloc[:,i])","3ba5fe25":"X = train_label.drop('target',axis=1)\ny = train_label.target","c19623be":"li_reg = LinearRegression()\ngb_reg = GradientBoostingRegressor(n_estimators=1000, max_depth=3)\nxgb_params = {\n    'n_estimators': 5000,\n    'learning_rate': 0.12,\n    'subsample': 0.96,\n    'colsample_bytree': 0.12,\n    'max_depth': 2,\n    'booster': 'gbtree', \n    'reg_lambda': 65.1,\n    'reg_alpha': 15.9,\n    'random_state':40\n}\nxgb_reg = XGBRegressor(tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\", **xgb_params )\ncgb_reg=CatBoostRegressor(n_estimators=5000, learning_rate=0.01)\nlgb_reg = LGBMRegressor(learning_rate = 0.003049106861273527,\n                                n_estimators = 16000,\n                                num_leaves = 51,\n                                max_depth = 65,\n                                n_jobs = -1,\n                                random_state = 0,\n                                cat_smooth = 93.60968300634175,\n                                min_child_samples = 177,\n                                colsample_bytree = 0.2185112060137363,\n                                max_bin = 537,\n                                reg_alpha = 9.03513073170552,\n                                reg_lambda = 0.024555737897445917,\n                           min_data_per_group = 117,\n                            bagging_freq = 1,\n                       bagging_fraction = 0.6709049555262285,\n                 cat_l2 = 7.5586732660804445)","7712bd5c":"li_reg.fit(X, y)\ngb_reg.fit(X , y)\nxgb_reg.fit(X, y)\ncgb_reg.fit(X, y)\nlgb_reg.fit(X, y)","c06dc4fb":"li_pred = li_reg.predict(test_label)\ngb_pred = gb_reg.predict(test_label)\nxgb_pred = xgb_reg.predict(test_label)\ncgb_pred = cgb_reg.predict(test_label)\nlgb_pred = lgb_reg.predict(test_label)","f5f8e601":"submission.iloc[:,1]=li_pred\nsubmission.to_csv(path + 'submission_li.csv',index=False)","0d1a08f0":"submission.iloc[:,1]=gb_pred\nsubmission.to_csv(path + 'submission_gb.csv',index=False)","9589bdf9":"submission.iloc[:,1]=xgb_pred\nsubmission.to_csv(path + 'submission_xg.csv',index=False)","3bee2727":"submission.iloc[:,1]=cgb_pred\nsubmission.to_csv(path + 'submission_cgb.csv',index=False)","5eb06083":"submission.iloc[:,1]=lgb_pred\nsubmission.to_csv(path + 'submission_lgb.csv',index=False)","abac08c9":"blend_a=(li_pred+gb_pred+xgb_pred+cgb_pred+lgb_pred)\/5\nblend_b=(li_pred*gb_pred*xgb_pred*cgb_pred*lgb_pred)**(1\/5)\nblend_c=(gb_pred+xgb_pred+cgb_pred+lgb_pred)\/4\nblend_d=(gb_pred*xgb_pred*cgb_pred*lgb_pred)**(1\/4)\nblend_e=(xgb_pred+cgb_pred+lgb_pred)\/3\nblend_f=(xgb_pred*cgb_pred*lgb_pred)**(1\/3)","cba4de2c":"submission.iloc[:,1]=blend_a\nsubmission.to_csv(path + 'submission_blend_a.csv',index=False)","50bcfde3":"submission.iloc[:,1]=blend_b\nsubmission.to_csv(path + 'submission_blend_b.csv',index=False)","549d8fae":"submission.iloc[:,1]=blend_c\nsubmission.to_csv(path + 'submission_blend_c.csv',index=False)","ba8d159e":"submission.iloc[:,1]=blend_d\nsubmission.to_csv(path + 'submission_blend_d.csv',index=False)","4a68ab65":"submission.iloc[:,1]=blend_e\nsubmission.to_csv(path + 'submission_blend_e.csv',index=False)","bdc950d2":"submission.iloc[:,1]=blend_f\nsubmission.to_csv(path + 'submission_blend_f.csv',index=False)","c6309eb9":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)","5e269e85":"li_reg.fit(X_train, y_train)\ngb_reg.fit(X_train , y_train)\nxgb_reg.fit(X_train, y_train)\ncgb_reg.fit(X_train, y_train)\n\nli_pred = li_reg.predict(X_val)\ngb_pred = gb_reg.predict(X_val)\nxgb_pred = xgb_reg.predict(X_val)\ncgb_pred = cgb_reg.predict(X_val)\n\npred = np.array([li_pred, gb_pred, xgb_pred, cgb_pred])\npred = np.transpose(pred)\n\nlgb_pred.fit(pred, y_val)","f34c7025":"li_reg.fit(X, y)\ngb_reg.fit(X , y)\nxgb_reg.fit(X, y)\ncgb_reg.fit(X, y)\n\nli_pred = li_reg.predict(test_label)\ngb_pred = gb_reg.predict(test_label)\nxgb_pred = xgb_reg.predict(test_label)\ncgb_pred = cgb_reg.predict(test_label)\n\npred = np.array([li_pred, gb_pred, xgb_pred, cgb_pred])\npred = np.transpose(pred)\n\nfinal = lgb_pred.predict(pred)","479feb3b":"submission.iloc[:,1]=final\nsubmission.to_csv(path + 'submission_normal_stacking.csv',index=False)","43fc00fc":"def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ):\n    # KFold\n    kf = KFold(n_splits=n_folds, shuffle=False, random_state=0)\n    # Initialize the numpy array for future return of learning data to be used by the meta model\n    train_fold_pred = np.zeros((X_train_n.shape[0] ,1 ))\n    test_pred = np.zeros((X_test_n.shape[0],n_folds))\n    print(model.__class__.__name__ , ' model start ')\n    \n    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n        # Extract the fold dataset from the entered learning data that the underlying model will learn\/predict\n        print('\\t fold set: ',folder_counter,' start ')\n        X_tr = X_train_n[train_index] \n        y_tr = y_train_n[train_index] \n        X_te = X_train_n[valid_index]  \n        \n        # Performing learning of the based model with learning data that has been re-created within the fold set.\n        model.fit(X_tr , y_tr)       \n        # Model prediction based on re-created verification data from within the fold set and then storing data.\n        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)\n        # Store the data after predicting the entered original test data in the learned underlying model within the fold set.\n        test_pred[:, folder_counter] = model.predict(X_test_n)\n            \n    # Generate test data by averaging the predicted data from within the fold set \n    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)    \n    \n    # train_fold_pred is the learning data used by the final meta model; test_pred_mean is the test data\n    return train_fold_pred , test_pred_mean","b99b0517":"# get_stacking_base_datasets( ) uses overflowing ndarrays as a factor, thus converting DataFrame to overflowing.\n# This procedure is not required in classification, but this array conversion is essential in regression\nX_train_n = X.values\nX_test_n = test_label.values\ny_train_n = y.values\n\n\n# Return data for learning\/test generated by each Base model.\nli_train, li_test = get_stacking_base_datasets(li_reg, X_train_n, y_train_n, X_test_n, 5)\ngb_train, gb_test = get_stacking_base_datasets(gb_reg, X_train_n, y_train_n, X_test_n, 5)\nxgb_train, xgb_test = get_stacking_base_datasets(xgb_reg, X_train_n, y_train_n, X_test_n, 5)\ncgb_train, cgb_test = get_stacking_base_datasets(cgb_reg, X_train_n, y_train_n, X_test_n, 5)","4f15cc1e":"# Combines data sets for learning and testing returned by individual models in stacking form.\nStack_final_X_train = np.concatenate((li_train, gb_train, xgb_train, cgb_train), axis=1)\nStack_final_X_test = np.concatenate((li_test, gb_test, xgb_test, cgb_test), axis=1)\n\n\n# Predict with newly created learning and testing data and RMSE measurements based on forecasts from the underlying model.\nlgb_reg.fit(Stack_final_X_train, y)\nfinal = lgb_reg.predict(Stack_final_X_test)","c4d650eb":"submission.iloc[:,1]=final\nsubmission.to_csv(path + 'submission_cv_stacking.csv',index=False)","d7d8a99e":"PB_score : 0.72090","31d6ce08":"PB_score : 0.72075","110f9b0f":"# Conclusion\nWe looked into and applied the typical method of ensemble, stacking and mixing technologies. Mixing is a technique derived from stacking.\nUltimately, technologies to reduce ML model errors and increase efficiency must be highly diverse and perform well in the underlying model to expect high performance in these advanced models. These methods only increase a little bit of performance. It is up to each developer to choose the technology that best suits their problems, considering several factors, such as the amount of data, available derivatives, type of learning to perform, optimal parameter settings, and amount of computers.","11bde0ea":"PB_score : 0.72357","08e90e72":"PB_score : 0.71923","b7cc9db0":"PB_score : 0.72264","bb0a695d":"# 2. Feature engineering","cdf5efb1":"# 5. Stacking with CV\n\nThe previous stacking technology utilized only one segmented train, validation set. Since this is a problem with overfitting, we will use CV to apply stacking again.","d5ab5f72":"# 3. Standard Modeling","4f031209":"When blending, values can be applied with weights, or indicators such as arithmetic and geometric means can be used.","5eb7f910":"# 1. Data & Library load","5f24a033":"Blending is a technique derived from Stacking Generalization. The only difference is that in Blending, the k-fold cross validation technique is not used to generate the training data of the meta-model. Blending implements \u201cone-holdout set\u201d, that is, a small portion of the training data (validation) to make predictions which will be \u201cstacked\u201d to form the training data of the meta-model. Also, predictions are made from the test data to form the meta-model test data.\n\n<img src =\"https:\/\/miro.medium.com\/max\/2000\/1*T0L64nrOJSr8-LRJlWfLtQ.jpeg\" width=\"1000\" height=\"1200\" img>\n","91ea7b76":"Ensemble Learning refers to the use of ML algorithms jointly to solve classification and\/or regression problems mainly. These algorithms can be the same type (homogeneous Ensemble Learning) or different types (heterogeneous Ensemble Learning). Ensemble Learning performs a strategic combination of various experts or ML models in order to improve the effectiveness obtained using a single weak model.","2585b6b6":"- References\n\n    - https:\/\/towardsdatascience.com\/ensemble-learning-stacking-blending-voting-b37737c4f483\n\n    - https:\/\/github.com\/wikibook\/ml-definitive-guide\/blob\/master\/5%EC%9E%A5\/5.10%20Regression%20%EC%8B%A4%EC%8A%B5%20-%20Kaggle%20House%20Price.ipynb","2f6b732d":"PB_score : 0.71866","d9357472":"PB_score : 0.71913","8233b7a5":"PB_score : 0.72089","b32ec167":"PB_score : 0.71923","7bc5dca4":"PB_score : 0.71923","2d89f4d1":"PB_score : 0.72066","300b8d39":"PB_score : 0.71916","fc88746a":"# 5. Standard Stacking","b64a48b1":"- Stacking\nBetter known as Stacking Generalization, it is a method introduced by David H. Wolpert in 1992 where the key is to reduce the generalization error of different generalizers (i.e. ML models). The general idea of the Stacking Generalization method is the generation of a Meta-Model. Such a Meta-Model is made up of the predictions of a set of ML base models (i.e. weak learners) through the k-fold cross validation technique. Finally, the Meta-Model is trained with an additional ML model (which is commonly known as the \u201cfinal estimator\u201d or \u201cfinal learner\u201d).\n\n<img src =\"https:\/\/miro.medium.com\/max\/2000\/1*CoauXirckomVXxw2Id2w_Q.jpeg\" width=\"1000\" height=\"1200\" img>\n","c7fd979e":"# 4. Blending"}}