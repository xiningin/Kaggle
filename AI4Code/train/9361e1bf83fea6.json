{"cell_type":{"1d58a9d2":"code","0c6d9587":"code","00d9d83b":"code","d70a4839":"code","728f984b":"code","459f209b":"code","9639f000":"code","7dbafcc9":"code","8a3755fc":"code","e3d3989c":"code","5d991e89":"code","8990cfe1":"code","0dcee990":"code","3277c36e":"code","0c2f660c":"code","02464255":"code","2f58b81b":"code","005b170f":"code","e5b6c3a3":"code","6774bad7":"markdown","da101038":"markdown","331e7fbf":"markdown","bd8f1e32":"markdown","c244853e":"markdown","368108dd":"markdown","8803f5ec":"markdown","9f1787f8":"markdown","ca43d419":"markdown","f42c3f5c":"markdown","e711c06b":"markdown","14bbf7ed":"markdown"},"source":{"1d58a9d2":"%%HTML\n\n<style type=\"text\/css\">\ndiv.h1 {\n    background-color:#339933; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n\ndiv.h2 {\n    background-color:#83ccd2; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n<\/style>","0c6d9587":"import math\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns","00d9d83b":"!ls ..\/input\/riiid-test-answer-prediction\/","d70a4839":"train = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/train.csv\",\n                    dtype = {\"content_type_id\":\"int8\", \"task_container_id\":\"int32\",\n                             \"user_answer\":\"int8\", \"answered_correctly\":\"int8\"})\n","728f984b":"#From https:\/\/www.kaggle.com\/rohanrao\/ashrae-half-and-half\n\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","459f209b":"train = reduce_mem_usage(train, use_float16=True)","9639f000":"train.head()","7dbafcc9":"questions = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/questions.csv\")\nlectures = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/lectures.csv\")","8a3755fc":"train.dtypes","e3d3989c":"questions.head()","5d991e89":"questions.dtypes","8990cfe1":"lectures.head()","0dcee990":"lectures.dtypes","3277c36e":"g1 = sns.distplot(train[\"timestamp\"])\ng1.set_title(\"distplot of timestamp\")","0c2f660c":"#it takes so much time and memory...\n#g2 = sns.countplot(data=train, x=\"user_id\")\n#g2.set_title(\"countplot of user_id\")","02464255":"g3 = sns.countplot(data=questions, x=\"bundle_id\")\ng3.set_title(\"countplot of bundle_id\")","2f58b81b":"g4 = sns.countplot(data=questions, x=\"correct_answer\")\ng4.set_title(\"countplot of correct_answer\")","005b170f":"g5 = sns.countplot(data=lectures, x=\"type_of\")\ng5.set_title(\"countplot of type_of\")","e5b6c3a3":"g6 = sns.countplot(data=lectures, x=\"part\")\ng6.set_title(\"countplot of part\")","6774bad7":"# <div class=h1>About this notebook<\/div>","da101038":"### [WIP]","331e7fbf":"## Load data\n\nI'll load data. Especially, train.csv is so big (5.45 GB !!!), we have to get creative.\n\nFirst, I specified a partial dtype. And second, I used reduce_mem_usage function. This is knoladge from \"ASHRAE - Great Energy Predictor III\" comp.","bd8f1e32":"- check data explanation\n\n***train.csv***\n\n**row_id**: (int64) ID code for the row.\n\n**timestamp**: (int64) the time between this user interaction and the first event from that user.\n\n**user_id**: (int32) ID code for the user.\n\n**content_id**: (int16) ID code for the user interaction\n\n**content_type_id**: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n\n**task_container_id**: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id. Monotonically increasing for each user.\n\n**user_answer**: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n\n**answered_correctly**: (int8) if the user responded correctly. Read -1 as null, for lectures.\n\n**prior_question_elapsed_time**: (float32) How long it took a user to answer their previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Note that the time is the total time a user took to solve all the questions in the previous bundle.\n\n**prior_question_had_explanation**: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.\n\n***questions.csv***: metadata for the questions posed to users.\n\n**question_id**: foreign key for the train\/test content_id column, when the content type is question (0).\n\n**bundle_id**: code for which questions are served together.\n\n**correct_answer**: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n\n**part**: top level category code for the question.\n\n**tags**: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.\n\n***lectures.csv***: metadata for the lectures watched by users as they progress in their education.\n\n**lecture_id**: foreign key for the train\/test content_id column, when the content type is lecture (1).\n\n**part**: top level category code for the lecture.\n\n**tag**: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n\n**type_of**: brief description of the core purpose of the lecture\n\n\n***example_test_rows.csv*** Three sample groups of the test set data as it will be delivered by the time-series API. The format is largely the same as train.csv. There are two different rows that mirror what information the AI tutor actually has available at any given time, but with the user interactions grouped together for the sake of API performance rather than strictly showing information for a single user at a time. Some questions will appear in the hidden test set that have NOT been presented in the train set, emulating the challenge of quickly adapting to modeling newly introduced questions. Their metadata is still in question.csv as usual.\n\n**prior_group_responses** (string) provides all of the user_answer entries for previous group in a string representation of a list in the first row of the group. All other rows in each group are null. If you are using Python, you will likely want to call eval on the non-null rows. Some rows may be null, or empty lists.\n\n**prior_group_answers_correct** (string) provides all the answered_correctly field for previous group, with the same format and caveats as prior_group_responses. Some rows may be null, or empty lists.\n\n( From Data page.  https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/data)","c244853e":"### lectures","368108dd":"### questions","8803f5ec":"\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Riiid\/Graphic%20or%20image%20within%20description%20(min%20size%20350x350).png\" width=\"300\">","9f1787f8":"### load library.","ca43d419":"### train","f42c3f5c":"## Simple visualization\n\nI'll check some features of dat by seaborn.","e711c06b":"# <div class=h2> visualization<\/div>","14bbf7ed":"In this notebook, I'll load train data and make sample plot to get good insight for data.\n\nIn version1, to begin with, the data was too large to read and visualize, so I aimed to read and visualize it first. This notebook is going to update.\n\nIn this competition, your challenge is to create algorithms for \"Knowledge Tracing,\" the modeling of student knowledge over time. The goal is to accurately predict how students will perform on future interactions.\n\nOur innovative algorithms will help tackle global challenges in education. If successful, it\u2019s possible that any student with an Internet connection can enjoy the benefits of a personalized learning experience, regardless of where they live. "}}