{"cell_type":{"6bdccd8f":"code","85c4dad2":"code","dc390684":"code","a1b2f528":"code","2d673828":"code","5dc124b2":"code","405bb679":"code","51e7a60d":"code","a441a1db":"code","40a53b7b":"code","028d1fa4":"code","75f337e6":"code","8a8fb4f1":"code","d94b67d9":"code","50880732":"code","ff687886":"code","23cdb7c4":"code","d3c75d60":"code","0e15dab0":"code","1cb158ba":"code","48304dce":"code","758ec857":"code","861fd064":"code","e73f01fc":"code","40dbb0bf":"code","a773f910":"code","515b4b50":"code","759ff1ba":"code","c87a189b":"code","43461bf4":"code","4c37da32":"code","16efcdb9":"code","c0336b86":"code","4a372a7d":"code","d0e461a7":"code","3d7a76df":"code","3fb1fccf":"code","80ab206b":"code","4afb7093":"code","70f387cb":"code","a9cd3a37":"markdown","923e6f7a":"markdown","25dd2f37":"markdown","e551f980":"markdown","224e3a21":"markdown","dabeafe6":"markdown","401605e8":"markdown","cb27fabf":"markdown","1ece13ce":"markdown","17f44e6e":"markdown","631ed247":"markdown","543fb8f1":"markdown","1e1f29e8":"markdown","f9cf52f1":"markdown","754d2375":"markdown","bfc7c565":"markdown","d79b69b8":"markdown","ff1291f6":"markdown","6ea06f3e":"markdown","08dc7b89":"markdown","f66b8c0c":"markdown","e36fab23":"markdown","1335e60d":"markdown","b673ccc1":"markdown","b8d690b8":"markdown","fe36de2e":"markdown","bf697f9f":"markdown","10f7e755":"markdown","ed15be36":"markdown","460a283a":"markdown","1423b7fa":"markdown","a2ca9d60":"markdown","282bcce2":"markdown","49b56f67":"markdown","96295e63":"markdown","dc848539":"markdown"},"source":{"6bdccd8f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn as skl","85c4dad2":"#import pandas as pd\n#housing = pd.read_csv(\"..\/input\/california-housing\/housing.csv\")\nimport pandas as pd\ntest = pd.read_csv(\"..\/input\/atividade-3-pmr3508\/test.csv\")\nhousing = pd.read_csv(\"..\/input\/atividade-3-pmr3508\/train.csv\")","dc390684":"aux = housing","a1b2f528":"housing.shape","2d673828":"housing.head()","5dc124b2":"housing = housing.dropna()","405bb679":"housing.hist(bins=20, figsize=(25,20))","51e7a60d":"housing = housing.drop(columns=[\"Id\"])","a441a1db":"y = housing.median_house_value \nX = housing.drop(columns=[\"median_house_value\"])\nX.shape","40a53b7b":"print('Extremos dos valores')\nprint(' ')\nprint('longitude')\nprint('Min ',min(X['longitude'])) \nprint('Max ',max(X['longitude']))\nprint(' ')\nprint('latitude')\nprint('Min ',min(X['latitude']))\nprint('Max ',max(X['latitude']))\nprint(' ')\nprint('median_age')\nprint('Min ',min(X['median_age']))\nprint('Max ',max(X['median_age']))\nprint(' ')\nprint('total_rooms')\nprint('Min ',min(X['total_rooms']))\nprint('Max ',max(X['total_rooms']))\nprint(' ')\nprint('total_bedrooms')\nprint('Min ',min(X['total_bedrooms']))\nprint('Max ',max(X['total_bedrooms']))\nprint(' ')\nprint('population')\nprint('Min ',min(X['population']))\nprint('Max ',max(X['population']))\nprint(' ')\nprint('households')\nprint('Min ',min(X['households']))\nprint('Max ',max(X['households']))\nprint(' ')\nprint('median_income')\nprint('Min ',min(X['median_income']))\nprint('Max ',max(X['median_income']))\n\nprint(' ')\nprint('median_house_value')\nprint('Min ',min(housing['median_house_value']))\nprint('Max ',max(housing['median_house_value']))\n\n","028d1fa4":"import seaborn\nplt.figure(figsize=(10,10))\nplt.title(\"Matriz de correla\u00e7\u00e3o\")\nseaborn.heatmap(housing.corr(), annot=True, linewidths=0.2)","75f337e6":"v_correl = [0.045,0.140,0.11, 0.13 ,0.05, 0.025, 0.065 , 0.69] #correlacoes em valroes absolutos\nmedia = (0.045 + 0.140 +0.11 + 0.13 + 0.05 + 0.025 + 0.065 + 0.69)\/8.0\nprint('Media do modulo das correlacoes em relacao a MEDIAN_HOUSE_VALUE = ',media)\ni=1\nv_irrelevantes = []\nwhile i<8 :\n    if(v_correl[i] < 0.05): #Se a correlacao for menor do que 10%\n        v_irrelevantes.insert(i,i-1)\n    i = i + 1\nprint(v_irrelevantes)","8a8fb4f1":"v_correl = [0.045,0.140,0.11, 0.13 ,0.05, 0.025, 0.065 , 0.69]\nmedia = (0.045 + 0.140 +0.11 + 0.13 + 0.05 + 0.025 + 0.065 + 0.69)\/8.0\nprint('Media do modulo das correlacoes em relacao a MEDIAN_HOUSE_VALUE = ',media)\ni=1\nv_irrelevantes = []\nwhile i<8 :\n    if(v_correl[i] < media): #Se a correlacao for menor do que 10%\n        v_irrelevantes.insert(i,i-1)\n    i = i + 1\nprint(v_irrelevantes)","d94b67d9":"aux[\"NEW_persons_per_room\"] = X[\"population\"]\/X[\"total_rooms\"]\naux[\"NEW_persons_per_bedroom\"] = X[\"population\"]\/X[\"total_bedrooms\"]","50880732":"aux[\"NEW_house_age\/income\"] = X[\"median_age\"]\/X[\"median_income\"]","ff687886":"import seaborn\nplt.figure(figsize=(20,20))\nplt.title(\"Matriz de correla\u00e7\u00e3o\")\nseaborn.heatmap(aux.corr(), annot=True, linewidths=0.2)","23cdb7c4":"aux.plot(kind='scatter',x='median_house_value',y='total_rooms',color='red')\nplt.show()","d3c75d60":"aux.plot(kind='scatter',x='median_house_value',y='NEW_persons_per_room',color='red')\nplt.show()","0e15dab0":"trai1 = X\ntrai1.drop(columns=[\"total_bedrooms\"])","1cb158ba":"trai2 = X\ntrai2[\"house_age\/income\"] = X[\"median_age\"]\/X[\"median_income\"]","48304dce":"trai3 = X\ntrai3.drop(columns=[\"total_rooms\"])\ntrai3.drop(columns=[\"total_bedrooms\"])","758ec857":"trai4 = X\ntrai4[\"house_age\/income\"] = X[\"median_age\"]\/X[\"median_income\"]\ntrai4.drop(columns=[\"total_rooms\"])\ntrai4.drop(columns=[\"total_bedrooms\"])\ntrai4.drop(columns=[\"longitude\"])\ntrai4.drop(columns=[\"latitude\"])\ntrai4.drop(columns=[\"median_age\"])\n","861fd064":"def RF_cv_select_pred(num,train,y):  \n    trainY = y\n    from sklearn.ensemble import RandomForestRegressor\n    forest = RandomForestRegressor(n_estimators=num, criterion='mse', min_samples_split=5, \n                          min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_features=\"sqrt\", \n                         random_state=None, verbose=0, warm_start=True)\n    #Usando os dados\n    forest.fit(train, trainY)\n    score = forest.score(train, trainY)\n    print (score)\n    predictions = forest.predict(test)\n    return score","e73f01fc":"RF_cv_select_pred(30,X,y)","40dbb0bf":"i=1\narr_res = []\nwhile i<150 :\n    rf = RF_cv_select_pred(i,trai1,y)\n    arr_res.insert(i, rf)\n    print(i)\n    i = i + 5\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","a773f910":"i=1\narr_res = []\nwhile i<150 :\n    rf = RF_cv_select_pred(i,trai2,y)\n    arr_res.insert(i, rf)\n    print(i)\n    i = i + 5\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","515b4b50":"i=1\narr_res = []\nwhile i<150 :\n    rf = RF_cv_select_pred(i,trai3,y)\n    arr_res.insert(i, rf)\n    print(i)\n    i = i + 5\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","759ff1ba":"i=1\narr_res = []\nwhile i<150 :\n    rf = RF_cv_select_pred(i,trai4,y)\n    arr_res.insert(i, rf)\n    print(i)\n    i = i + 5\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","c87a189b":"def adaboost_neural(num,train,y):  \n    trainY = y   \n    from sklearn.ensemble import AdaBoostRegressor\n    from sklearn import neural_network\n\n    neural_net = neural_network.MLPRegressor(hidden_layer_sizes=(100,),\n                                       activation='relu', solver='adam',\n                                       learning_rate='adaptive', max_iter=800,\n                                       learning_rate_init=0.01, warm_start = True, alpha=0.01)\n    adaboost1 = AdaBoostRegressor(base_estimator=neural_net, n_estimators=num, learning_rate=0.01, random_state=None)\n    adaboost1.fit(train, trainY)\n    neural = adaboost1.score(train, trainY)*100\n    print (\"AdaBoost on Neural Network score: \", str(neural), \"%\")\n    return neural","43461bf4":"def adaboost_RF(num,train,y):  \n    trainY = y   \n    from sklearn.ensemble import AdaBoostRegressor\n    from sklearn.ensemble import RandomForestRegressor\n    forest = RandomForestRegressor(n_estimators=num, criterion='mse', min_samples_split=5, \n                          min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_features=\"sqrt\", \n                         random_state=None, verbose=0, warm_start=True)\n    \n    adaboost2 = AdaBoostRegressor(base_estimator=forest, n_estimators=num, learning_rate=0.01, random_state=None)\n    adaboost2.fit(train, trainY)\n    RF = adaboost2.score(train, trainY)*100\n    print (\"AdaBoost on Random Forest score: \", str(RF), \"%\")\n    return RF","4c37da32":"adaboost_neural(7,trai1,y)\n","16efcdb9":"adaboost_neural(7,trai2,y)\n","c0336b86":"adaboost_neural(7,trai3,y)\n","4a372a7d":"adaboost_neural(7,trai4,y)","d0e461a7":"i=35\narr_res = []\nwhile i<50 :\n    rf = adaboost_RF(i,trai1,y)\n    arr_res.insert(i, rf)\n    print(i)\n    i = i + 5\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","3d7a76df":"i=35\narr_res = []\nwhile i<50 :\n    rf = adaboost_RF(i,trai2,y)\n    arr_res.insert(i, rf)\n    print(i)\n    i = i + 5\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","3fb1fccf":"i=35\narr_res = []\nwhile i<50 :\n    rf = adaboost_RF(i,trai3,y)\n    arr_res.insert(i, rf)\n    print(i)\n    i = i + 5\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","80ab206b":"i=35\narr_res = []\nwhile i<50 :\n    rf = adaboost_RF(i,trai4,y)\n    arr_res.insert(i, rf)\n    print(i)\n    i = i + 5\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","4afb7093":"def linear(train,y):  \n    trainY = y   \n    from sklearn.linear_model import LinearRegression\n    LR = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n    LR.fit(train, trainY)\n    lin = LR.score(train, trainY)*100\n    print (\"Linear: \", str(lin), \"%\")\n    return lin","70f387cb":"print(linear(trai1,y))\nprint(linear(trai2,y))\nprint(linear(trai3,y))\nprint(linear(trai4,y))","a9cd3a37":"De acordo com os histogramas id n\u00e3o influencia para o Target almejado, sendo assim:","923e6f7a":"<img src=\"https:\/\/github.com\/caio-freitas\/AplicacoesAlgelin\/blob\/master\/WhatsApp%20Image%202019-12-09%20at%2018.02.01.jpeg?raw=true\" alt=\"logo\" width=\"1000\">","25dd2f37":"**Tradu\u00e7\u00e3o e significado das Features:**\n\n\n* Id - Identifica\u00e7\u00e3o dos locais\n* latitude: Latitude do local (em graus)\n* longitude: Longitude do local\n* median_age: Mediana das idades dos im\u00f3veis no local\n* total_rooms: Contagem do n\u00famero de c\u00f4modos das casas na regi\u00e3o\n* total_bedrooms: Contagem do total de quartos das casas na regi\u00e3o\n* population: Popula\u00e7\u00e3o na regi\u00e3o\n* households: N\u00famero total de casas na regi\u00e3o\n* median_income: Mediana da renda das pessoas na regi\u00e3o\n1. * median_house_value: Vari\u00e1vel Target.","e551f980":"## Bosting em Random Forest e em Rede Neural","224e3a21":"Utilizando a tecnica de boosting era esperado que se encontra-se resultados melhores do que com os outros algoritimos de regressao, mas isso nao aconteceu pois nao houve poder computacional o bastante para se chegar em boas \notimizacoes de seus hiper parametros sendo que a pesquisa das 4 redes neurais e das 4 Random Forest cm adaboost demoraram mais de 1 hora para ficar prontas com uma tentaiva minima de otimizacao de hiper parametros para Random forest\ne tentativa nula para Redes Neurais. \n\nA conclusao eh que embora esses algoritimos sejam muito impressionantes pelo que realizam a dificuldade computacional de realiza-los e otimiza-los nao os torna muito vantajosos em aplicacoes mais simples como essa","dabeafe6":"Aplicacoes:","401605e8":"# Regressores","cb27fabf":"## Resultados Random Forest\n\nPara as trataivas planejadas (1,2,3,4) no item Otimizacoes obteve-se o seguinte resultado com Random Forest:\n* 1) 88.79% - 126 arvores\n* 2) 88.77% - 76 arvores\n* 3) 88.80% - 136 arvores\n* 4) 88.80% - 96 arvores\n","1ece13ce":"Feature de Habitacao\n* Logica: Mais pessoas por comodo ou por cama talvez seja um local mais pobre.","17f44e6e":"## Criando Features - Features Engeneering","631ed247":"# Data Preparation\n\n## Vizualizando as features","543fb8f1":"As Features que possuem correla\u00e7\u00e3o menor do que a media das correlacoes em modulo s\u00e3o, de acordo com a tabela de indexa\u00e7\u00e3o:\n* Longitude\n* Latitude\n* Housing_median_age\n* Total_rooms\n* Total_bedroms\n\nEstranhamente ao que diz a intui\u00e7\u00e3o a localiza\u00e7\u00e3o da casa parace ter pouca rela\u00e7\u00e3o com o valor m\u00e9dio da casa. Na verdade quase nenhum dessas features, pela intui\u00e7\u00e3o deveiria estar nessa classe chamada \"features fracas\".\nSendo assim, numa tentativa de casar intui\u00e7\u00e3o e dados iremos tentar criar a partir dessas features outras cuja correla\u00e7\u00e3o com *median_house_value* seja mais alta.","1e1f29e8":"* 4) Combinar as tres estrategias e retirar tambem as demais \"features fracas\"","f9cf52f1":"Donde se conlcui que o melhor regressor para tecnica Random Forest encontrado nesse estudo alcancou **88.80%** de acuracia a um custo de **96 arvores**. A retirada na preparacao dos dados de features pouco influentes foi de extrema importancia pois reduziu significativamente a complexidade do algoritimo para o mesmo resultado em relacao a terceira tentativas","754d2375":"* 3) Retirar total_rooms. **total_rooms (nao usar)**","bfc7c565":"# Data Check","d79b69b8":"# Resultado Geral\n\nO algortimo campe\u00e3o, dentre os testados, \u00e9 o Random Forest pois sua simplicidade permite que se fa\u00e7a o ajsute fino de seus hiper-par\u00e2metros ao mesmo que sua robustez garante bons resultados.\nO regressor mais bem avaliado foi o **Random Forest de 96 arvores** com **88.80% de acuracia** .","ff1291f6":"* 1) Desconsiderar o atributo \"total_bedrooms\" dada sua baixa influencia no \"median_house_value\" - Matriz de correla\u00e7\u00e3o. **\"total_bedrooms\" (Nao usar)**\n","6ea06f3e":"Ao analisar a matriz percebe-se que:\n* \u00c9 razo\u00e1vel tentar considerar NEW_house_age\/income pois aumentou a influencia em modulo de house_median_age de 0.11 para 0.32 \n* N\u00e3o Considerar NEW_persosns_per_bedroom por ter uma correlacao pior com a feature target\n* N\u00e3o Considerar NEW_persosns_per_room por ter uma correlacao pior com a feature target pois total_rooms tem uma correla\u00e7\u00e3o uma ordem de grandeza maior","08dc7b89":"**Tabela de Indexa\u00e7\u00e3o de Features**\n\n* longitude________________0\n* latitude__________1\n* median_age_____________2\n* total_rooms__________3\n* total_bedrooms______4\n* population_____5\n* households_________6\n* median_income_______7\n* median_house_value_______________8\n* ocena_proximity________________9","f66b8c0c":"**Localiza\u00e7\u00e3o dos Dados de Laitude e Longitude**\n(32.54,-124.35)\n(32.53,-114.31)\n(41.95,-124.35)\n(41.95,-114.31)\nOBS: O sistema de coordenadas configurado no google pode estar um pouco impreciso haja vista que ha pontos no Mar.Mas a regiao eh essa mesmo.","e36fab23":"Feature de Valor indireto da Casa\n* Logica: Pessoas que ganham mais eventualmente moram em casas mais novas","1335e60d":"Vemos que o numero de pessoas por comodo \u00e9 constante independentemente do valor da casa e tamb\u00e9m que o numero total de comodos n\u00e3o necessariamente interfere no preco da casa mas tentemos utilizar a feature relacionada a esse ultimo.","b673ccc1":"## Matriz de correla\u00e7\u00e3o","b8d690b8":"## Analisando novas Features - Nova Matriz de Correla\u00e7\u00e3o","fe36de2e":"## Features Irrelevantes","bf697f9f":"## 1) Random Forest","10f7e755":"# EP2 - EXTRA\n**Regress\u00e3o na base California Housing**\n      \nAutor do Estudo: Fernando Zolubas Preto - Nusp: 10694192","ed15be36":"# Sugest\u00f5es de Otimiza\u00e7\u00e3o","460a283a":"## Features Fracas","1423b7fa":"## Resultados Adaboost\n\nRede Neural\n* 1) 63.50% \n* 2) 63.67%\n* 3) 63.84%\n* 4) 63.61%\n\nRandom Forest\n* 1) 88.57% - 45 arvores\n* 2) 88.60% - 45 arvores\n* 3) 88.50% - 45 arvores\n* 4) 88.57% - 45 arvores\n\nobs: A pesquisa foi feita de 1 ate 50 variando i de 5 em 5, mas para submissao ser mais rapida reconfigurou-se para os valores atuais.","a2ca9d60":"* 2) Considerar o artibuto novo NEW_house_age\/income, chamado agora de **house_age\/income (OK)**\n","282bcce2":"## Resultados Regress\u00e3o Linear\nEmbora a regress\u00e3o Linear n\u00e3o tenha nem de longe atingido o melhor resultado, esse metodod atinngiu um resultado na mesma ordem de grandeza, na verdade at\u00e9 superior as famososas redes neurais, em fracoes de segundos ao passo que as redes neurais foram executadas em uma dezena de minutos aproximadamente. Ou seja, isso mostra o potencial da regress\u00e3o linear para tomar decisoes muito rapidas mas de baixa qualidade. A vantagem desse metodo \u00e9 sem duvida o minimo poder computacional envolvido embora \u00e9 claro que \u00e9 impossivel obter os mesmos resultados de algoritimos mais robustos como Random Forest","49b56f67":"A matriz de correla\u00e7\u00e3o traz todo o mapeamento das dependencias entre as variaveis.\nEstamos interessados naquelas variaveis que exercem forte influencia sobre a variavel *median_house_value* pois deseja-se cosntruir um regressor capaz de calcular essa valor. Sendo assim, deve-se procurar na tabela acima valores que tendam para os limites -1 (um sobe o outro desce) e 1 (um desce o outro desce). Valores muito proximos de zero s\u00e3o ruins pois inflam os algoritimos com calculos desnecess\u00e1rios e por vezes distorcionais dos valor procurado. Dessa forma procuremos uma m\u00e9trica para descartar valores que n\u00e3o s\u00e3o importantes.","96295e63":"## Regress\u00e3o Linear","dc848539":"De acordo com a tabela de atributos os atributos (4), classificados como irrelevantes por terem correlacoes abaixo de 5% com Median_house_value, S\u00e3o: \n* total_bedrooms\nDessa forma uma sugest\u00e3o otimiza\u00e7\u00e3o ser\u00e1 a sua elimina\u00e7\u00e3o.\nAl\u00e9m disso esse atributo tem forte rela\u00e7\u00e3o co  total_roms (93%), o que tamb\u00e9m justifica a elimina\u00e7\u00e3o dele na analise"}}