{"cell_type":{"ad71b053":"code","b8c58831":"code","1d8fd154":"code","87d0a7d5":"code","336a2802":"code","c16a44ab":"code","95a3dd8d":"code","f0cfad0a":"code","a8105596":"code","59d44c42":"code","13d5180d":"code","54fa0c25":"code","ccb652d6":"code","1e67e5e6":"code","162476e5":"code","954873bd":"code","4424b0ba":"code","a6386375":"code","a3428997":"code","ce8ea4c1":"code","ea6af221":"code","dda23422":"code","15307daa":"code","63533c64":"code","728d734b":"code","c30d86a9":"code","077e8c50":"code","447f3d70":"code","fb64d69e":"code","8adad593":"code","dbb45518":"code","cab038ca":"code","a575e696":"code","c2829e04":"code","e0a6f931":"code","9ab7003c":"code","477e30e7":"code","4b2a413b":"code","edb48a93":"code","177421e6":"code","557aacf2":"code","bcf457cb":"code","bb9865d8":"code","a9d31c32":"code","a8e4ec91":"code","1d9f7726":"code","2f0eaa02":"code","8a331d20":"code","67e6a74c":"code","81695ff3":"code","6ff636c3":"code","d5ed4fd7":"code","78e6e559":"code","cfc40476":"code","fb854ff5":"code","273ae136":"code","21b3f921":"code","1e7a2137":"code","4450797f":"code","97a98771":"code","ec9c4c11":"code","2791c6cf":"code","f63bcc7e":"code","8816e456":"code","b1f05222":"code","06fcec19":"code","8128584d":"markdown","b12758b6":"markdown","a60c40f4":"markdown","94038047":"markdown","90ebff03":"markdown","fb9a7238":"markdown","3afa178f":"markdown","64e7de0d":"markdown","205cb8ea":"markdown","477d784a":"markdown","846abcea":"markdown","4f8bf567":"markdown","54247552":"markdown","b86dfe07":"markdown","fe0b5bc7":"markdown","462cf4ad":"markdown","1d9191bd":"markdown"},"source":{"ad71b053":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12,8\n#sns.color_palette(\"hls\", 8)","b8c58831":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","1d8fd154":"Id = test.id","87d0a7d5":"train.shape #there are 7613 rows and 5 columns in the data","336a2802":"train.head() # 1 is real disaster tweets 0 is fake ones","c16a44ab":"train.info() ","95a3dd8d":"train.isnull().sum()","f0cfad0a":"len(set(train['id'])) # There are 7613 unique users. Each tweet was tweeted by unique users","a8105596":"sns.countplot(train['target'])\nplt.show()\nprint(train['target'].value_counts())","59d44c42":"sns.countplot(y = train.keyword,order = train['keyword'].value_counts().sort_values(ascending=False).iloc[0:20].index)\nplt.title(\"Count of Keywords\")\nplt.show() # It shows the most usage keywords ","13d5180d":"# Count of keywords for real disaster;\ndisastered_tweet = train.groupby('keyword')['target'].mean().sort_values(ascending=False).head(15)\nnon_disasterd  = train.groupby('keyword')['target'].mean().sort_values().head(15)\n\nplt.figure(figsize=(8,5))\nsns.barplot(disastered_tweet, disastered_tweet.index, color='red')\nplt.title('Keywords with highest % of disaster tweets')\nplt.show()","54fa0c25":"#Count of eywords for Non-Disasters\nplt.figure(figsize=(8,5))\nsns.barplot(non_disasterd, non_disasterd.index, color='blue')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()","ccb652d6":"sns.countplot(y = train.location,order = train['location'].value_counts().sort_values(ascending=False).iloc[0:15].index)","1e67e5e6":"raw_loc = train.location.value_counts()\ntop_loc_disaster = list(raw_loc[raw_loc>=10].index)\ntop_only_disaster = train[train.location.isin(top_loc_disaster)]\n\ntop_location = top_only_disaster.groupby('location')['target'].mean().sort_values(ascending=False)\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","162476e5":"# We need to fill null values with None\nfor i in ['keyword','location']:\n    train[i] = train[i].fillna('None')\n    test[i] = test[i].fillna('None')\n    ","954873bd":"train.info() #As we can see there is no null value now","4424b0ba":"len(set(train['location'])) #There are 3342 unique location values. we are going to decrease of that numbers as using data cleaning","a6386375":"def clean_location(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_location:\n        return x\n    else: \n        return 'Others'\n    \ntrain['location'] = train['location'].apply(lambda x: clean_location(str(x)))\ntest['location'] = test['location'].apply(lambda x: clean_location(str(x)))","a3428997":"top_location = train.groupby('location')['target'].mean().sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","ce8ea4c1":"len(set(train['location'])) # As we can see, the unique values decreased. It has 27 now.","ea6af221":"# Let's look at the rondom tweets. \ntrain['text'][0]\n\n# As wee can see there is a hashtag(#) in that tweet. We can split the hashtag and can use as a new feature\n# let's look at another random tweet ","dda23422":"train['text'][789] \n\n# There is a tagged in that tweet. We can also split thatn and we can use as a new feature\n","15307daa":"train['text'][417] # and in that tweet. there is a link.  we are gonna fix all those tweets","63533c64":"import re\n\n# We are going to split the hashtag,link and tagged\ndef created_feature(train):\n    train['hashtags'] = train['text'].apply(lambda x: \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", x)]) or 'no_hashtag')\n    train['tagged'] = train['text'].apply(lambda x: \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", x)]) or 'no_tagged')\n    train['link'] = train['text'].apply(lambda x:\" \".join([match.group(0)[:] for match in re.finditer(r\"https?:\/\/\\S+\", x)]) or 'no_link')\n    return train","728d734b":"train = created_feature(train)\ntest = created_feature(test)","c30d86a9":"train # As we can see, we have new features now. Great!","077e8c50":"train['hashtags'].value_counts().sort_values(ascending=False).iloc[0:10]","447f3d70":"train['tagged'].value_counts().sort_values(ascending=False).iloc[0:10]","fb64d69e":"train['link'].value_counts().sort_values(ascending=False).iloc[0:10]","8adad593":"def clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # remove links\n    text = re.sub(r'\\n',' ', text) #  remove breaks\n    text = re.sub('\\s+', ' ', text).strip() \n    return text","dbb45518":"train['text'][417] # Let's look at that sample","cab038ca":"clean_text(train['text'][417]) # as we can see we cleaned the tweet\n# We are gonna use of that method for all text ","a575e696":"train['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))","c2829e04":"train.head()","e0a6f931":"## Text Mining\nimport nltk\n#nltk.download(\"stopwords\")\n#!pip install textblob\n#nltk.download(\"wordnet\")\n\n#Upper lower convert\ntrain['text'] = train['text'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\ntest['text'] = test['text'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n\n# punctuation marks\ntrain['text'] =train['text'].str.replace('[^\\w\\s]','')\ntest['text'] =test['text'].str.replace('[^\\w\\s]','')\n\n# numbers\ntrain['text'] = train['text'].str.replace('[\\d]','')\ntest['text'] = test['text'].str.replace('[\\d]','')\n\nfrom nltk.corpus import stopwords\nsw = stopwords.words('english')\ntrain['text'] =train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\ntest['text'] =test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n\n#lemmi \nfrom textblob import Word\ntrain['text'] = train['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ntest['text'] = test['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\n\ntrain['text'] = train['text'].str.replace('rt','')\ntest['text'] = test['text'].str.replace('rt','')","9ab7003c":"train.text # we did some cleaning to text ","477e30e7":"freq_df = train['text'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\nfreq_df.columns = ['words', 'frequences']\nfreq_df.sort_values('frequences',ascending=False) # It shows frequences of words ","4b2a413b":"# Most used words\ntop_freq = freq_df.sort_values('frequences',ascending=False)[0:15]\ntop_freq.set_index('words',inplace=True)\ntop_freq.plot.bar(color=(0.2, 0.4, 0.6, 0.6))","edb48a93":"#Most used words dor disasters\nfreq_df = train[train['target']==1]['text'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\nfreq_df.columns = ['words', 'frequences']\nfreq_df.sort_values('frequences',ascending=False)\n\ntop_freq_disaster = freq_df.sort_values('frequences',ascending=False)[0:15]\ntop_freq_disaster.set_index('words',inplace=True)\ntop_freq_disaster.plot.bar(color ='g')\nplt.title(\"Disaster Tweets\")\nplt.show()  #Fire and news are most used words in the disasters tweets.","177421e6":"# Most used words for Non-Disaster tweets\nfreq_df = train[train['target']==0]['text'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\nfreq_df.columns = ['words', 'frequences']\nfreq_df.sort_values('frequences',ascending=False)\n\ntop_freq_non_disaster = freq_df.sort_values('frequences',ascending=False)[0:15]\ntop_freq_non_disaster.set_index('words',inplace=True)\ntop_freq_non_disaster.plot.bar(color ='orange')\nplt.title(\"Non-Disaster Tweets\")\nplt.show() #","557aacf2":"import sys\nimport numpy as np\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud , STOPWORDS, ImageColorGenerator","bcf457cb":"# I'm keeping the all tweets in ne text to do word cloud\n\ntext = \" \".join(i for i in train.text)","bb9865d8":"text[0:1000] ","a9d31c32":"wc = WordCloud(background_color='white').generate(text)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","a8e4ec91":"from textblob import TextBlob\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble","1d9f7726":"import category_encoders as ce\n\n# Target encoding\nfeatures = ['keyword', 'location']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train[features],train['target'])\n\ntrain = train.join(encoder.transform(train[features]).add_suffix('_target'))\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'))","2f0eaa02":"from sklearn.feature_extraction.text import CountVectorizer\n\n# CountVectorizer\n\n# Links\nvec_links = CountVectorizer(min_df = 5, analyzer = 'word', token_pattern = r'https?:\/\/\\S+') # Only include those >=5 occurrences\nlink_vec = vec_links.fit_transform(train['link'])\nlink_vec_test = vec_links.transform(test['link'])\nX_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\nX_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())\n\n# Tagged\nvec_tag = CountVectorizer(min_df = 5)\ntag_vec = vec_tag.fit_transform(train['tagged'])\ntag_vec_test = vec_tag.transform(test['tagged'])\nX_train_tag = pd.DataFrame(tag_vec.toarray(), columns=vec_tag.get_feature_names())\nX_test_tag = pd.DataFrame(tag_vec_test.toarray(), columns=vec_tag.get_feature_names())\n\n# Hashtags\nvec_hash = CountVectorizer(min_df = 5)\nhash_vec = vec_hash.fit_transform(train['hashtags'])\nhash_vec_test = vec_hash.transform(test['hashtags'])\nX_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\nX_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())","8a331d20":"# Tf-idf for text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \ntext_vec = vec_text.fit_transform(train['text'])\ntext_vec_test = vec_text.transform(test['text'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\nprint (X_train_text.shape)","67e6a74c":"train = train.join(X_train_link, rsuffix='_link')\ntrain = train.join(X_train_tag, rsuffix='_tagged')\ntrain = train.join(X_train_hash, rsuffix='_hashtag')\ntrain = train.join(X_train_text, rsuffix='_text')\n\ntest = test.join(X_test_link, rsuffix='_link')\ntest = test.join(X_test_tag, rsuffix='_mention')\ntest = test.join(X_test_hash, rsuffix='_hashtag')\ntest = test.join(X_test_text, rsuffix='_text')\n\nprint (train.shape)","81695ff3":"train.head() # as we can see the data has 1708 feature now","6ff636c3":"from sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y = train_test_split(train.drop(columns = ['id', 'keyword', 'location', 'text', \n                                                                       'target', 'hashtags', 'tagged','link']),\n                                                 train['target'],test_size = 0.3) \n","d5ed4fd7":"train_x.shape","78e6e559":"test_x.shape","cfc40476":"from sklearn.metrics import confusion_matrix\nfrom sklearn import linear_model\nlog = linear_model.LogisticRegression(solver='liblinear', random_state=777)\nlog_model = log.fit(train_x, train_y)\nlog_pred = log_model.predict(test_x)\naccuracy = model_selection.cross_val_score(log_model,\n                                          test_x,\n                                          test_y,\n                                          cv=10).mean()\n\nprint('Accuracy of Logistic Regression: ', accuracy)","fb854ff5":"confusion_matrix(test_y,log_pred)","273ae136":"nb= naive_bayes.MultinomialNB()\nnb_model = nb.fit(train_x, train_y)\nnb_pred = nb_model.predict(test_x)\naccuracy = model_selection.cross_val_score(nb_model,\n                                          test_x,\n                                          test_y,\n                                          cv=10).mean()\nprint('Accuracy of Naive-Bayes: ', accuracy)","21b3f921":"confusion_matrix(test_y,nb_pred)","1e7a2137":"rf = ensemble.RandomForestClassifier()\nrf_model = rf.fit(train_x,train_y)\nrf_pred = rf.predict(test_x)\naccuracy = model_selection.cross_val_score(rf_model,\n                                          test_x,\n                                          test_y,\n                                          cv=10).mean()\nprint('Accuracy of Random Forest: ', accuracy)","4450797f":"confusion_matrix(test_y,rf_pred) ","97a98771":"import xgboost\nxgb = xgboost.XGBClassifier()\nxgb_model = xgb.fit(train_x,train_y)\nxgb_pred = xgb_model.predict(test_x)\naccuracy = model_selection.cross_val_score(xgb_model,\n                                          test_x,\n                                          test_y,\n                                          cv=10).mean()\nprint('Accuracy of XGBoost: ', accuracy)","ec9c4c11":"confusion_matrix(test_y,xgb_pred)","2791c6cf":"columns = train_x.columns","f63bcc7e":"columns","8816e456":"test = test.reindex(columns = columns, fill_value=0)","b1f05222":"#nb_model = nb.fit(train.drop(columns = ['id', 'keyword', 'location', 'text','target', 'hashtags', 'tagged','link']),train['target'])","06fcec19":"pred = nb_model.predict(test)\nsubmission = pd.DataFrame({\"id\": Id, \"target\": pred})\nsubmission.to_csv(\"submission.csv\", index=False)","8128584d":"# Count Vectors","b12758b6":"# Feature Engineering\n\n* count vectors\n* TF-IDF vectors(words, chracters, n-grams)\n\n\nTF (t) = (Frequency of a term in a document) \/ (total number of terms in a document)\n\nIDF (t) = log_e (Total number of documents) \/ (number of documents with t terms in it)","a60c40f4":"# NLP with Disaster Tweets\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).","94038047":"# Navie-Bayes ","90ebff03":"\n\nIf you like it please vote !","fb9a7238":"# Locations","3afa178f":"* Mumbai and India have the most disaster tweets. But we can see, there lots of noise in the location feature. We need to fix that ","64e7de0d":"# XGBoost","205cb8ea":"* As you can see the data is not clean. We need to do data cleansing. For example Us,USA and United State are same location.We need to seperate them \n\n* Let's see which location has the most disaster tweets","477d784a":"# Submission","846abcea":"# Text","4f8bf567":"# Random Forest Regression","54247552":"# Logistic Regression","b86dfe07":"* It looks, the data clean anymore. Now, Mumbai and Nigeria have the most disasters tweets.","fe0b5bc7":"# TF-IDF","462cf4ad":"# Train-Test Split","1d9191bd":"# Keywords"}}