{"cell_type":{"5af139c4":"code","844572b6":"code","f3ad699b":"code","7ea15c76":"code","b11d7123":"code","a7e51f58":"code","519a7b8d":"markdown","6bb81b19":"markdown","6b6d5afd":"markdown","228fb918":"markdown","c339c327":"markdown","62ff2e4e":"markdown","0db13d04":"markdown","74c90927":"markdown"},"source":{"5af139c4":"import wandb\n\nimport pandas as pd\n\nimport tensorflow as tf\nfrom keras_preprocessing.image import ImageDataGenerator","844572b6":"DIR = '..\/input\/cassava-leaf-disease-classification'\n\nimg_path = f'{DIR}\/train_images'\ndf = pd.read_csv(f'{DIR}\/train.csv')\ndf['label'] = df['label'].astype('string')\n\ndf.head()","f3ad699b":"sweep_config = {\n    'method': \"random\",\n    'metric': {\n        'name': 'accuracy',\n        'goal': 'maximize',\n    },\n    'parameters': {\n        \"optimizer\": {\n            \"values\": ['adam', 'sgd', 'rmsprop']\n        },\n        \"nodes\": {\n            \"values\": [128, 256, 512]\n        },\n        \"epochs\": {\n            \"values\": [1,2,3,5]\n        },\n        \"learning_rate\": {\n            \"distribution\": \"uniform\",\n            \"min\": 0.0001,\n            \"max\": 0.1\n        },\n        \"batch_size\": {\n            \"distribution\": \"q_log_uniform\",\n            \"q\": 1,\n            \"min\": 32,\n            \"max\": 128\n        },\n    },\n}","7ea15c76":"sweep_id = wandb.sweep(sweep_config, project=\"leaf-cnn\")","b11d7123":"def train():\n    default_config={\n        \"optimizer\": 'adam',\n        \"nodes\":128,\n        \"epochs\": 1,\n        \"learning_rate\": 1e-2,\n        \"batch_size\": 32,}\n\n    wandb.init(config=default_config)\n    config = wandb.config\n\n    batch_size = config.batch_size\n    epochs = config.epochs\n    learning_rate = config.learning_rate\n    nodes = config.nodes\n    optimizer = config.optimizer\n\n    if config.optimizer == 'adam':\n        optimizer = Adam(lr=learning_rate)\n    elif config.optimizer == 'sgd':\n        optimizer = SGD(lr=learning_rate)\n    elif config.optimizer == 'rmsprop':\n        optimizer = RMSprop(lr=learning_rate)\n\n    data_gen = ImageDataGenerator(\n        rotation_range=180,\n        width_shift_range=0.3,\n        rescale=0.2,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        fill_mode='nearest',\n        horizontal_flip=True,\n        vertical_flip=True,\n        validation_split=0.2\n    )\n\n    train_generator = data_gen.flow_from_dataframe(\n        df,\n        directory=os.path.join(DIR, 'train_images'),\n        x_col='image_id',\n        y_col='label',\n        target_size=(128, 170),\n        batch_size=36,\n        subset='training',\n        shuffle=True,\n        class_mode='categorical'\n    )\n\n    valid_generator = data_gen.flow_from_dataframe(\n        df,\n        directory=os.path.join(DIR, 'train_images'),\n        x_col='image_id',\n        y_col='label',\n        target_size=(128, 170),\n        batch_size=36,\n        subset='validation',\n        shuffle=True,\n        class_mode='categorical'\n    )\n\n    model = Sequential()\n\n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(128, 170, 3)))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    model.add(Flatten())\n\n    model.add(Dense(nodes, activation='sigmoid'))\n    model.add(Dense(nodes, activation='sigmoid'))\n\n    model.add(Dense(5, activation='softmax'))\n\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    model.fit(train_generator, validation_data=valid_generator, epochs=epochs, batch_size=batch_size)\n    model.save('Models\/Leaf_classifier.h5', overwrite=True)","a7e51f58":"wandb.agent(sweep_id, train, count=10)","519a7b8d":"## 3. Define the training loop","6bb81b19":"# \ud83d\udd8b\ufe0fImports\nThe following imports will be used all over this kernel:\n- wandb\n- pandas\n- tensorflow","6b6d5afd":"# \ud83d\ude80Introduction\nIn this kernel I will try to incorporate [wandb's sweep](https:\/\/docs.wandb.ai\/sweeps) in a simple experiment. [Weights and Biases](https:\/\/wandb.ai) provides us with a lot of features which help in the reproducibility and ease of the Deep Learning experiments. One of the best feature that they provide is sweeps. Sweeps help in hyperparameter optimization with little to no boilerplate code.\n\n![image.png](attachment:image.png)\n\nThis kernel is focused towards providing a starter place to help people try out this product and help themselves with great hyperparameter optimization. Hope this helps people imbibe a better approach towards reproducible DL.","228fb918":"## 4. Run the sweep agent \ud83d\udd75\ufe0f","c339c327":"## 2. Initialize the sweep","62ff2e4e":"# Refernces\nThis kernel is heavily inspired from [sweep example colab](https:\/\/colab.research.google.com\/drive\/181GCGp36_75C2zm7WLxr9U2QjMXXoibt) by [Lavanya Shukla](https:\/\/www.kaggle.com\/lavanyashukla01). The colab notebook is a great starting point for anyone to get started with wandb sweeps.\n\nOne can also go to the [documentation of wandb sweeps](https:\/\/docs.wandb.ai\/sweeps) for a better understanding of the feature.","0db13d04":"# \ud83e\uddf9Sweeps\n\n## 1. Define the Sweep\n\nWeights & Biases sweeps give you powerful levers to configure your sweeps exactly how you want them, with just a few lines of code. The sweeps config can be defined as a dictionary or a [YAML file](https:\/\/docs.wandb.com\/library\/sweeps).\n\nLet's walk through some of them together:\n*   **Metric** \u2013 This is the metric the sweeps are attempting to optimize. Metrics can take a `name` (this metric should be logged by your training script) and a `goal` (maximize or minimize). \n*   **Search Strategy** \u2013 Specified using the 'method' variable. We support several different search strategies with sweeps. \n  *   **Grid Search** \u2013 Iterates over every combination of hyperparameter values.\n  *   **Random Search** \u2013 Iterates over randomly chosen combinations of hyperparameter values.\n  *   **Bayesian Search** \u2013 Creates a probabilistic model that maps hyperparameters to probability of a metric score, and chooses parameters with high probability of improving the metric. The objective of Bayesian optimization is to spend more time in picking the hyperparameter values, but in doing so trying out fewer hyperparameter values.\n*   **Stopping Criteria** \u2013 The strategy for determining when to kill off poorly peforming runs, and try more combinations faster. We offer several custom scheduling algorithms like [HyperBand](https:\/\/arxiv.org\/pdf\/1603.06560.pdf) and Envelope.\n*   **Parameters** \u2013 A dictionary containing the hyperparameter names, and discreet values, max and min values or distributions from which to pull their values to sweep over.\n\nYou can find a list of all configuration options [here](https:\/\/docs.wandb.com\/library\/sweeps\/configuration).","74c90927":"# \ud83d\udcbeData\nThe data is well organized in the folders. We will need to read the `csv` and take an `ImageDataGenerator` for the images to flow from the directory directly."}}