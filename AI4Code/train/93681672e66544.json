{"cell_type":{"6f26da8e":"code","57fe5db7":"code","82213631":"code","ead49035":"code","b78b8b09":"code","4701c35b":"code","2e5f2926":"code","ab3fe6c5":"code","be7b47bc":"code","408400db":"code","54b2a0cc":"code","b659255f":"code","a9ed4c30":"code","f199bc9b":"code","42993a2f":"code","c162cbe5":"code","fdaa3a74":"code","473fa03d":"code","eefe2a81":"code","d2b0c3bd":"code","87f49556":"code","71af49d3":"code","14bcd3cf":"code","9f850bac":"code","23e7e97d":"code","af52a2d1":"code","242a5206":"code","8d2a84b1":"code","c99fa5dd":"code","332fd015":"code","d5ffe23d":"code","37afe58e":"code","cb2422e4":"code","467504fa":"code","3bc00f74":"code","dc0139f2":"code","2d745710":"markdown","e2ef75a0":"markdown","89ecf6fa":"markdown","bce5558e":"markdown","3be3fdc3":"markdown","96806ee2":"markdown","46bd0c1a":"markdown","efa6b67c":"markdown","cbb352e7":"markdown"},"source":{"6f26da8e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","57fe5db7":"data = pd.read_csv('..\/input\/indian-food-101\/indian_food.csv')","82213631":"data","ead49035":"data.isna().sum()","b78b8b09":"food_vocab = set()\n\nfor ingredients in data['ingredients']:\n    for food in ingredients.split(','):\n        if food.strip().lower() not in food_vocab:\n            food_vocab.add(food.strip().lower())","4701c35b":"food_vocab","2e5f2926":"len(food_vocab)","ab3fe6c5":"food_columns = pd.DataFrame()\n\nfor i, ingredients in enumerate(data['ingredients']):\n    for food in ingredients.split(','):\n        if food.strip().lower() in food_vocab:\n            food_columns.loc[i, food.strip().lower()] = 1\n\nfood_columns = food_columns.fillna(0)","be7b47bc":"food_columns","408400db":"data = data.drop(['name', 'ingredients'], axis=1)","54b2a0cc":"data","b659255f":"{column: list(data[column].unique()) for column in data.columns if data.dtypes[column] == 'object'}","a9ed4c30":"data[['flavor_profile', 'state', 'region']] = data[['flavor_profile', 'state', 'region']].replace('-1', np.NaN)","f199bc9b":"def onehot_encode(df, columns, prefixes):\n    df = df.copy()\n    for column, prefix in zip(columns, prefixes):\n        dummies = pd.get_dummies(df[column], prefix=prefix)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df","42993a2f":"data = onehot_encode(\n    data,\n    ['flavor_profile', 'course', 'state', 'region'],\n    ['f', 'c', 's', 'r']\n)","c162cbe5":"data","fdaa3a74":"data[['prep_time', 'cook_time']] = data[['prep_time', 'cook_time']].replace(-1, np.NaN)","473fa03d":"data['prep_time'] = data['prep_time'].fillna(data['prep_time'].mean())\ndata['cook_time'] = data['cook_time'].fillna(data['cook_time'].mean())","eefe2a81":"label_encoder = LabelEncoder()\n\ndata['diet'] = label_encoder.fit_transform(data['diet'])","d2b0c3bd":"{index: label for index, label in enumerate(label_encoder.classes_)}","87f49556":"data","71af49d3":"y = data['diet']\n\nX = data.drop('diet', axis=1)\nX_food = pd.concat([X, food_columns], axis=1)","14bcd3cf":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)\nX_food = scaler.fit_transform(X_food)","9f850bac":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\nX_food_train, X_food_test, y_food_train, y_food_test = train_test_split(X_food, y, train_size=0.7, random_state=42)","23e7e97d":"def build_model(num_features, hidden_layer_sizes=(64, 64)):\n    inputs = tf.keras.Input(shape=(num_features,))\n    x = tf.keras.layers.Dense(hidden_layer_sizes[0], activation='relu')(inputs)\n    x = tf.keras.layers.Dense(hidden_layer_sizes[1], activation='relu')(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    \n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy',\n            tf.keras.metrics.AUC(name='auc')\n        ]\n    )\n    \n    return model","af52a2d1":"X.shape","242a5206":"model = build_model(40)\n\nbatch_size = 64\nepochs = 41\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=0\n)","8d2a84b1":"plt.figure(figsize=(20, 10))\n\nepochs_range = range(1, epochs + 1)\ntrain_loss, val_loss = history.history['loss'], history.history['val_loss']\ntrain_auc, val_auc = history.history['auc'], history.history['val_auc']\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, train_loss, label=\"Training Loss\")\nplt.plot(epochs_range, val_loss, label=\"Validation Loss\")\nplt.title(\"Loss\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, train_auc, label=\"Training AUC\")\nplt.plot(epochs_range, val_auc, label=\"Validation AUC\")\nplt.title(\"AUC\")\nplt.legend()\n\nplt.show()","c99fa5dd":"print(np.argmin(val_loss), np.argmax(val_auc))","332fd015":"model.evaluate(X_test, y_test)","d5ffe23d":"len(y_test)","37afe58e":"X_food.shape","cb2422e4":"food_model = build_model(405, hidden_layer_sizes=(128, 128))\n\nfood_batch_size = 64\nfood_epochs = 200\n\nfood_history = food_model.fit(\n    X_food_train,\n    y_food_train,\n    validation_split=0.2,\n    batch_size=food_batch_size,\n    epochs=food_epochs,\n    callbacks=[tf.keras.callbacks.ReduceLROnPlateau()],\n    verbose=0\n)","467504fa":"plt.figure(figsize=(20, 10))\n\nfood_epochs_range = range(1, food_epochs + 1)\nfood_train_loss, food_val_loss = food_history.history['loss'], food_history.history['val_loss']\nfood_train_auc, food_val_auc = food_history.history['auc'], food_history.history['val_auc']\n\nplt.subplot(1, 2, 1)\nplt.plot(food_epochs_range, food_train_loss, label=\"Training Loss\")\nplt.plot(food_epochs_range, food_val_loss, label=\"Validation Loss\")\nplt.title(\"Loss\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(food_epochs_range, food_train_auc, label=\"Training AUC\")\nplt.plot(food_epochs_range, food_val_auc, label=\"Validation AUC\")\nplt.title(\"AUC\")\nplt.legend()\n\nplt.show()","3bc00f74":"print(np.argmin(food_val_loss), np.argmax(food_val_auc))","dc0139f2":"food_model.evaluate(X_food_test, y_food_test)","2d745710":"## Results","e2ef75a0":"# Task for Today  \n\n***\n\n## Indian Food Classification  \n\nGiven *data about different Indian dishes*, let's try to predict if a given dish is **vegetarian** or not.  \n  \nWe will use a TensorFlow ANN to make our predictions.","89ecf6fa":"# Re-training with ingredient features","bce5558e":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/eaexjDcha94","3be3fdc3":"## Splitting and Scaling","96806ee2":"# Training (No ingredient features)","46bd0c1a":"# Getting Started","efa6b67c":"# Preprocessing","cbb352e7":"## Results"}}