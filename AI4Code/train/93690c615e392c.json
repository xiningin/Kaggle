{"cell_type":{"ea562ac0":"code","144833be":"code","cf8ca149":"code","4da7b6bc":"code","9a9c7e49":"code","46cfbabb":"code","85896fbc":"code","3f92a056":"code","b9d401ae":"code","5f232db9":"code","38aceaa8":"code","abfab73a":"code","7132e00f":"code","bd906e81":"code","8d1150d9":"code","bb5ebf57":"code","5fdf57eb":"code","de20268b":"code","899076cf":"code","21a29e88":"code","f5fa3775":"code","262b83b7":"code","9c6d8899":"code","2ca62f21":"code","d17bdd2c":"code","a9323e66":"markdown","5e3c0d0a":"markdown","afe17bf8":"markdown","82b2b0c4":"markdown","684841c3":"markdown","d6512e07":"markdown","0f70cd5b":"markdown","7f0d9826":"markdown","e63e227f":"markdown"},"source":{"ea562ac0":"#pip install bs4","144833be":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nimport re\nimport joblib\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cf8ca149":"train=pd.read_csv('\/kaggle\/input\/gujarati-news-dataset\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/gujarati-news-dataset\/valid.csv')","4da7b6bc":"train.head()","9a9c7e49":"train.label.value_counts()","46cfbabb":"train.shape","85896fbc":"test.head()","3f92a056":"test.shape","b9d401ae":"# Remove HTML tags\ndef remove_html(text):\n    soup=BeautifulSoup(text,'lxml')\n    html_free=soup.get_text()\n    return html_free\n\ntrain['headline']=train['headline'].apply(lambda x: remove_html(x))\ntest['headline']=test['headline'].apply(lambda x: remove_html(x))","5f232db9":"# Remove URLs\ntrain['headline']=train['headline'].str.replace('http\\S+|www.\\S+', '', case=False)\ntest['headline']=test['headline'].str.replace('http\\S+|www.\\S+', '', case=False)","38aceaa8":"# Remove the hashtags and @\ntrain['headline']=train['headline'].str.replace('@', '', case=False)\ntest['headline']=test['headline'].str.replace('@', '', case=False)\n\ntrain['headline']=train['headline'].str.replace('#', '', case=False)\ntest['headline']=test['headline'].str.replace('#', '', case=False)","abfab73a":"# Remove stopwords\n# stopwords_guj = ['\u0ab2\u0ac7\u0aa4\u0abe','\u0ab6\u0abe','\u0a89\u0aad\u0abe','\u0ab9\u0acb','\u0ab9\u0acb\u0a88','\u0aae\u0abe','\u0aae\u0ac2\u0a95\u0ac0','\u0aa8\u0ab9\u0ac0','\u0aac\u0aa7\u0ac1\u0a82','\u0ab9\u0abe','\u0aae\u0ac0','\u0a8f\u0aa8','\u0aa4\u0ac1\u0a82','\u0aa8\u0acb','\u0a9b\u0acb','\u0a9c\u0ac0','\u0ab2\u0ac7\u0ab5\u0abe','\u0a86\u0ab0','\u0a9b\u0ac0\u0a8f','\u0aa8\u0a82','\u0a8f\u0ab5',\n#                  '\u0ab9\u0acb\u0ab5\u0abe','\u0aa4\u0ac7\u0aa5\u0ac0','\u0aa8\u0ac1\u0a82','\u0a9b','\u0a8f\u0ab5\u0abe','\u0a8f\u0aa8\u0ac0','\u0aa5\u0aa4\u0abe\u0a82','\u0a9c\u0ac7\u0ab5\u0ac0','\u0aac\u0a82\u0aa8\u0ac7','\u0ab9\u0ab6\u0ac7','\u0aae\u0abe\u0a82','\u0aa8\u0ac0','\u0ab9\u0aa4\u0abe\u0a82','\u0aa4\u0ac7\u0ab5\u0ac0','\u0aa5\u0aaf\u0acb','\u0a8f\u0ab5\u0ac0','\u0aa5\u0ac0','\u0aa5\u0aaf\u0ac1\u0a82','\u0aa4\u0acd\u0aaf\u0abe\u0a82',\n#                  '\u0aac\u0aa8\u0ac0','\u0a97\u0aaf\u0acb','\u0a9b\u0aa4\u0abe\u0a82','\u0a86\u0aaa\u0ac0','\u0ab0\u0ab9\u0ac7','\u0aa4\u0ac7\u0a93','\u0aaa\u0abe\u0ab8\u0ac7','\u0aa4\u0ac7\u0aae','\u0aa8\u0ac7','\u0aa4\u0ac7\u0aa8\u0ac7','\u0ab9\u0ac1\u0a82','\u0aac\u0abe\u0aa6','\u0ab6\u0a95\u0ac7','\u0a9c\u0acb','\u0a85\u0a82\u0a97\u0ac7','\u0ab0\u0ab9\u0ac0','\u0a8f\u0aae','\u0aa4\u0ac7\u0aa8\u0abe','\u0a95\u0ab0\u0ac7','\u0aa5\u0a87',\n#                  '\u0ab8\u0ac1\u0aa7\u0ac0','\u0a9c\u0abe\u0aaf','\u0ab0\u0ac2\u0abe','\u0a95\u0acb\u0a88','\u0aa8\u0abe','\u0ab9\u0ab5\u0ac7','\u0aa4\u0ac7\u0aa8\u0ac0','\u0ab8\u0abe\u0aae\u0ac7','\u0a86\u0ab5\u0ac7','\u0aac\u0ac7','\u0aa5\u0a88','\u0aa8','\u0a9c\u0ac7','\u0a86\u0ab5\u0ac0','\u0aa4\u0abe','\u0aaa\u0ab0','\u0ab9\u0acb\u0aaf','\u0ab9\u0aa4\u0ac1\u0a82','\u0a8f','\u0a95\u0ab0\u0ac0','\u0aa4\u0ac7',\n#                  '\u0ab9\u0aa4\u0ac0','\u0aae\u0abe\u0a9f\u0ac7','\u0aa4\u0acb','\u0a9c','\u0aaa\u0aa3','\u0a95\u0ac7','\u0a86','\u0a85\u0aa8\u0ac7','\u0a9b\u0ac7']\n\n# def remove_stopwords(text):\n#     words=[w for w in text if w not in stopwords_guj]\n#     return words\n\n# train['headline']=train['headline'].apply(lambda x: remove_stopwords(x))\n# test['headline']=test['headline'].apply(lambda x: remove_stopwords(x))","7132e00f":"X=train['headline']\ny=train['label']","bd906e81":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\n\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),\n])","8d1150d9":"text_clf.fit(X, y)  ","bb5ebf57":"# Save model\nloaded_model = joblib.dump(text_clf, 'guju_model.pkl')","5fdf57eb":"test_X=test['headline']\ntest_y=test['label']","de20268b":"predictions = text_clf.predict(test_X)","899076cf":"from sklearn import metrics\n\ncm = metrics.confusion_matrix(test_y, predictions)\nplt.figure(figsize=(9,9))\nsns.heatmap(cm, annot=True, fmt=\".3f\");\nplt.ylabel('Actual label');\nplt.xlabel('Predicted label');","21a29e88":"accu = metrics.classification_report(test_y, predictions)\nprint(accu)","f5fa3775":"# Example text\nsimple_test = [\"\u0a95\u0ab0\u0ac0\u0aa8\u0abe\u0a8f \u0ab8\u0ac8\u0aab\u0aa8\u0abe \u0aac\u0ab0\u0acd\u0aa5\u0aa1\u0ac7 \u0aaa\u0ab0 \u0ab0\u0abe\u0a96\u0ac0 \u0ab8\u0acd\u0aaa\u0ac7\u0ab6\u0abf\u0aaf\u0ab2 \u0aaa\u0abe\u0ab0\u0acd\u0a9f\u0ac0, \u0a9c\u0ac1\u0a93 \u0aab\u0acb\u0a9f\u0abe\"]\npred1 = text_clf.predict(simple_test)\npred1","262b83b7":"# Example text\nsimple_test = [\"RTI\u0aae\u0abe\u0a82 \u0a96\u0ac1\u0ab2\u0abe\u0ab8\u0acb\u0a83 \u0a86 \u0aa6\u0ac7\u0ab6\u0acb\u0aa8\u0ac7 \u0a85\u0aa1\u0aa7\u0ac0 \u0a95\u0abf\u0a82\u0aae\u0aa4\u0aae\u0abe\u0a82 \u0aaa\u0ac7\u0a9f\u0acd\u0ab0\u0acb\u0ab2-\u0aa1\u0ac0\u0a9d\u0ab2 \u0a86\u0aaa\u0ac7 \u0a9b\u0ac7 \u0aad\u0abe\u0ab0\u0aa4!\"]\npred2 = text_clf.predict(simple_test)\npred2","9c6d8899":"# Example text\nsimple_test = [\"\u0aae\u0acb\u0aac\u0abe\u0a88\u0ab2 \u0a8f\u0aaa \u0ab2\u0acb\u0aa8\u0acd\u0a9a: \u0a8f\u0ab8\u0a9f\u0ac0 \u0aac\u0ab8\u0aae\u0abe\u0a82 \u0a8f\u0aa1\u0ab5\u0abe\u0aa8\u0acd\u0ab8 \u0a95\u0ab0\u0abe\u0ab5\u0ac0 \u0ab6\u0a95\u0abe\u0ab6\u0ac7 \u0aac\u0ac1\u0a95\u0ac0\u0a82\u0a97\u0ac0\"]\npred3 = text_clf.predict(simple_test)\npred3","2ca62f21":"model = joblib.load('guju_model.pkl')","d17bdd2c":"simple_test = [\"\u0aae\u0acb\u0aac\u0abe\u0a88\u0ab2 \u0a8f\u0aaa \u0ab2\u0acb\u0aa8\u0acd\u0a9a: \u0a8f\u0ab8\u0a9f\u0ac0 \u0aac\u0ab8\u0aae\u0abe\u0a82 \u0a8f\u0aa1\u0ab5\u0abe\u0aa8\u0acd\u0ab8 \u0a95\u0ab0\u0abe\u0ab5\u0ac0 \u0ab6\u0a95\u0abe\u0ab6\u0ac7 \u0aac\u0ac1\u0a95\u0ac0\u0a82\u0a97\u0ac0\"]\npred4 = model.predict(simple_test)\npred4[0]","a9323e66":"# Feature extraction using TF-IDF and pipeline creation","5e3c0d0a":"# Import libraries","afe17bf8":"# Custom prediction","82b2b0c4":"# Data processing","684841c3":"# Model fitting","d6512e07":"# Predictions","0f70cd5b":"# Load model","7f0d9826":"# Evaluation metrics","e63e227f":"# Read data"}}