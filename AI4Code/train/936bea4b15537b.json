{"cell_type":{"c31c5618":"code","2c67f45e":"code","7fb99679":"code","169f968f":"code","3a850ff7":"code","ad55ea7e":"code","dac44f91":"code","e0f2b010":"code","15de4c75":"markdown","f0190918":"markdown","fd21b651":"markdown","f0c49906":"markdown","99443e51":"markdown","bd283207":"markdown","1444bb13":"markdown"},"source":{"c31c5618":"# we import libraries we need for the implimantation\nimport numpy as np\nimport pandas as pd","2c67f45e":"class LinearRegression() :\n    def __init__( self, lr=0.001, epochs=1000 ) :\n        self.lr = lr\n        self.epochs = epochs\n\n    # Function for model training\n    def fit( self, X, Y ) :\n        # no_of_training_examples, no_of_features\n        self.m, self.n = X.shape\n\n        # weight initialization\n        self.W = np.zeros( self.n )\n        self.b = 0\n        self.X = X\n        self.Y = Y\n        \n        # gradient descent learning\n        for i in range( self.epochs ) :\n            y_pred = self.predict( self.X )\n            # calculate gradients\n            dW = - ( 2 * ( self.X.T ).dot( self.Y - y_pred ) ) \/ self.m\n            db = - 2 * np.sum( self.Y - y_pred ) \/ self.m\n            # update weights\n            self.W = self.W - self.lr * dW\n            self.b = self.b - self.lr * db\n\n        return self\n\n\n    # Hypothetical function \n    def predict( self, X ) :\n        return X.dot( self.W ) + self.b","7fb99679":"from scipy.stats import skew \nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\ntrain.SalePrice = np.log1p(train.SalePrice)\n\ndf = train.append(test).reset_index(drop=True)\n\ndf['Functional'] = df['Functional'].fillna('Typ')\ndf['Electrical'] = df['Electrical'].fillna(\"SBrkr\") # Standard Circuit\ndf['KitchenQual'] = df['KitchenQual'].fillna(\"TA\")\n\ndf['Exterior1st'] = df['Exterior1st'].fillna(train['Exterior1st'].mode()[0])\ndf['Exterior2nd'] = df['Exterior2nd'].fillna(train['Exterior2nd'].mode()[0])\ndf['SaleType'] = df['SaleType'].fillna(train['SaleType'].mode()[0])\n\nfor col in [\"PoolQC\", \"Alley\", 'FireplaceQu', 'Fence', 'MiscFeature', 'GarageType', 'GarageFinish', 'GarageQual', \n            'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \"MasVnrType\"] :\n    \n    df[col] = df[col].fillna(\"None\")\n    \nfor col in ('GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath', \n            'TotalBsmtSF', \"MasVnrArea\"):\n    df[col] = df[col].fillna(0) # no basement or no garage\n\ndf['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ndf['MSZoning'] = df.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nto_drop = ['GarageYrBlt','YearRemodAdd', 'Utilities'] \ndf = df.drop(to_drop, axis = 1)\n\ndf['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\ndf['Total_Home_Quality'] = df['OverallQual'] + df['OverallCond']\n\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['OverallCond'] = df['OverallCond'].astype(str)\ndf['YrSold'] = df['YrSold'].apply(str)\ndf['MoSold'] = df['MoSold'].apply(str)\n\ndf = pd.get_dummies(df)\n\nnumeric_features = df.dtypes[df.dtypes != object].index\nskewed_features = df[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n\n\n# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    df[i] = np.log1p(df[i])\n    \noutliers = [30, 88, 462, 631, 1322]\n\ndf = df.drop(df.index[outliers])\noverfit = []\nfor i in df.columns:\n    counts = df[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(df) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\ndf = df.drop(overfit, axis=1)\n\nn = len(train)\ntrain = df[:n-5]\ntest = df[n-5:]\n\nX = train.drop('SalePrice', axis = 1)\ny = train.SalePrice\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","169f968f":"# let's split our data to start training and prediction\nX = train.drop('SalePrice', axis = 1)\ny = train.SalePrice\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","3a850ff7":"model = LinearRegression(0.0000001, 1000)\n\nmodel.fit(x_train,y_train)","ad55ea7e":"pred = model.predict(x_test)\npred","dac44f91":"from sklearn import metrics\nmetrics.mean_squared_error(pred, y_test)","e0f2b010":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(x_train, y_train)\npreds = lr.predict(x_test)\nmetrics.mean_squared_error(preds, y_test)","15de4c75":"Now it's time to compare it with the real Linear Regression class from sickit learn :","f0190918":"Linear Regression is a supervised ML model dedicated to regression tasks, it tries to find linear correlation between one or more independent variables with the dependent one.\n\n### 1 . Hypothetical function :\n---\n\nWho says linear correlation says afine function, so if we want to find the right equation for our model we will right :\n\n$$\\hat{Y} = b + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$$\n\nwith $b$ as the biais, and $ \\theta_i $ as the weights of our model.\n\n\n### 2 . Cost function :\n---\n\nWe need now to set a specific function to calculate the error of our model since it is not perfect, we call this function $cost function$ :\n\n<blockquote> Cost function is a error function which is used to find the error term of a model on a data.\n    In our case, $\\hat{y}_i$ is the model\u2019s output label. And $y_i$ is the true label. We find the deviation of the actual label and true label. This is a linear case. We know, the value of $\\hat{y}_i - y_i$ can be +ve or -ve. And when doing summation, -ve and +ve gets canceled out. So we need to square them and sum it. We will also find mean of. \n<\/blockquote>\n<blockquote>\n    There are varients of cost function for regression. Like:- Mean Squard Error(MSE), Mean Absolute Error(MAE) etc. <\/blockquote>\n    \nFor our example we choose working with the MSE :\n\n$$J(\\theta) = \\frac{1}{2m}\\sum \\limits _{i=1} ^{m} (\\hat{y}_i - y_i)^2 $$\n\nFor just two variables with $\\theta_0$ is our biais b, we get the shape in the picture :\n\n![image.png](attachment:image.png)\n\n### 3 . Gradient descent and updates :\n---\n\nCan you set directly the parameters of our Hypothetical function and get the precise predictions you are looking for?! Obviously No even with a little chance! \n\nIf we change randomly the values we may pick the best ones but in the same time our chance to be lost is too large!\n\nIndeed, the only efficient way is to get these parameters when the cost function is in its minimum.\n\nand that is the concept of the famous method Gradient Descent.\n\n<blockquote> Gradient Descent algorithm will change, at each iteration, the values of all $\\theta_i$ and the biais $b$ until the best possible combinaition of parameters is found. <\/blockquote>\n    \n   \nImagine that you are on a hill, and you want to go down it. With each new step (analogy to iteration), you look around to find the best slope to move down. Once you have found the slope, you take a step forward one magnitude $\\alpha$.\n\nHere is an example with one variable how it looks, with w is our parameter in that case :\n\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*fU8XFt-NCMZGAWND.\" width=\"400\"><\/img>\n\n\n<br>\n\nHere are the formulas of Gradient Descent for the hypothesis function we are working on from the beginning :\n\n$$ \\theta_i = \\theta_i - \\alpha d\\theta_i$$\n\n$$ b = b - \\alpha db$$\n\nHere are the derivatives we need, you can replace them in the equations :\n\n$$d \\theta_i = - \\frac{2}{m} \\sum \\limits _{i=1} ^{m} x_i (\\hat{y_i} - y)$$\n\n$$d b = - \\frac{2}{m}\\sum \\limits _{i=1} ^{m} (\\hat{y_i} - y)$$\n","fd21b651":"## If you find this notebook useful please don't forget to upvote it!","f0c49906":"It appears that our model works well on our data.\n\nThat was all, hope tou enjoyed reading ;)","99443e51":"## It's time for coding :","bd283207":"We choose to work with OOP since it is appropriate with that kind of problems, and all models are implemented using OOP in Sickit learn.\n\nThe two functions we need to code are fit and predict, the first one is for model training, i.e it will find the parameters of our model based on the number of iterations and learning rate we are working with; and the second one to get the predictions for new samples.","1444bb13":"Let's apply it on House Price dataset.\n\nFor pre-processing part I just copied it from one of my previous kernels in this competition, you can take a look at it if you don't understand something."}}