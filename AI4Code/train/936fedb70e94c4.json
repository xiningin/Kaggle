{"cell_type":{"128a5c6d":"code","9532b5c4":"code","0ae672e3":"code","6ca87559":"code","a63f922d":"code","0beab4db":"code","e4f844f3":"code","4ca34a4b":"code","249af780":"code","e990f42f":"code","3a4db9db":"code","9d14ae86":"code","6a6a28e9":"code","ce11761e":"code","c512ad73":"code","177e1d6a":"code","4973b6be":"code","d82bb70f":"code","10d3513f":"code","731e9502":"code","24e92744":"code","f7412135":"code","168b5798":"code","9537b35a":"code","4b6cde2d":"code","99e830f0":"code","24908d4b":"code","09d32ed9":"code","40b391e2":"code","c257611d":"code","a3339413":"code","91e142e9":"code","062f2945":"code","01e69914":"code","10550d8c":"code","7f4c9442":"code","e4c31c47":"code","73f42a12":"code","43c15b78":"code","0b7fdff5":"code","89c998f3":"code","c59ecfd1":"code","20110879":"code","71c58c1b":"code","5725f69b":"code","dd8ca204":"code","1e50b608":"markdown","193278fc":"markdown","6ca9ba87":"markdown","92c92801":"markdown","8f24cfb7":"markdown","1ce911eb":"markdown","8a6cbdbe":"markdown","7fe489b6":"markdown","778b95bb":"markdown","f2847803":"markdown","d3a73f36":"markdown","ce95970b":"markdown","440da9ac":"markdown","64f9078d":"markdown","e5d88dae":"markdown","40b57add":"markdown","1c688ac8":"markdown","4a29bdce":"markdown","98d0e45a":"markdown","6fb941c7":"markdown","40256851":"markdown","9d941906":"markdown","79887575":"markdown","74a3bbe4":"markdown","0ef9a7a5":"markdown","c811c3ca":"markdown","0956ffc4":"markdown","8a687c3e":"markdown"},"source":{"128a5c6d":"# Importing computational packages\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\", None)\npd.set_option('float_format', '{:f}'.format)","9532b5c4":"# Importing visualization packages\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","0ae672e3":"# Importing model building packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.preprocessing import  RobustScaler\nfrom sklearn.preprocessing import PowerTransformer","6ca87559":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier","a63f922d":"from sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import f1_score, classification_report","0beab4db":"import warnings\nwarnings.filterwarnings(\"ignore\")","e4f844f3":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","4ca34a4b":"df.shape","249af780":"df.describe()","e990f42f":"#observe the different feature type present in the data\ndf.dtypes","3a4db9db":"df.info()","9d14ae86":"# Checking for the missing value present in each columns\ntotal = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","6a6a28e9":"classes=df['Class'].value_counts()\nnormal_share=round(classes[0]\/df['Class'].count()*100,2)\nfraud_share=round(classes[1]\/df['Class'].count()*100, 2)\nprint(\"Non-Fraudulent : {} %\".format(normal_share))\nprint(\"    Fraudulent : {} %\".format(fraud_share))","ce11761e":"# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\nplt.figure(figsize=(20,6))\n\nplt.subplot(1,2,1)\nax=sns.countplot(df[\"Class\"])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.ylabel(\"Number of transaction\")\nplt.xlabel(\"Class\")\nplt.title(\"Credit Card Fraud Class - data unbalance\")\nplt.grid()\nplt.subplot(1,2,2)\nfraud_percentage = {'Class':['Non-Fraudulent', 'Fraudulent'], 'Percentage':[normal_share, fraud_share]} \ndf_fraud_percentage = pd.DataFrame(fraud_percentage) \nax=sns.barplot(x='Class',y='Percentage', data=df_fraud_percentage)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.title('Percentage of fraudulent vs non-fraudulent transcations')\n\nplt.grid()","c512ad73":"# Create a scatter plot to observe the distribution of classes with time\nsns.scatterplot( df[\"Class\"],df[\"Time\"],hue=df[\"Class\"])\nplt.title(\"Time vs Class scatter plot\")\nplt.grid()","177e1d6a":"# Create a scatter plot to observe the distribution of classes with Amount\nsns.scatterplot(df[\"Class\"],df[\"Amount\"],hue=df[\"Class\"])\nplt.title(\"Amount vs Class scatter plot\")\nplt.grid()","4973b6be":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (10,10))\nsns.heatmap(df.corr(),cmap='rainbow')\nplt.show()","d82bb70f":"# Plotting all the variable in displot to visualise the distribution\nvar = list(df.columns.values)\n# dropping Class columns from the list\nvar.remove(\"Class\")\n\ni = 0\nt0 = df.loc[df['Class'] == 0]\nt1 = df.loc[df['Class'] == 1]\n\nplt.figure()\nfig, ax = plt.subplots(10,3,figsize=(30,45));\n\nfor feature in var:\n    i += 1\n    plt.subplot(10,3,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.grid()\nplt.show()","10d3513f":"# Drop unnecessary columns\ndf = df.drop(\"Time\", axis = 1)\ndf.head()","731e9502":"y= df[\"Class\"]\nX = df.drop(\"Class\", axis = 1)\ny.shape,X.shape","24e92744":"# Spltting the into 80:20 train test size\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42,stratify=y)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","f7412135":"# Checking the split of the class label\nprint(\" Fraudulent Count for Full data : \",np.sum(y))\nprint(\"Fraudulent Count for Train data : \",np.sum(y_train))\nprint(\" Fraudulent Count for Test data : \",np.sum(y_test))","168b5798":"# As PCA is already performed on the dataset from V1 to V28 features, we are scaling only Amount field\nscaler = RobustScaler()\n\n# Scaling the train data\nX_train[[\"Amount\"]] = scaler.fit_transform(X_train[[\"Amount\"]])\n\n# Transforming the test data\nX_test[[\"Amount\"]] = scaler.transform(X_test[[\"Amount\"]])","9537b35a":"X_train.head()","4b6cde2d":"X_test.head()","99e830f0":"# plot the histogram of a variable from the dataset to see the skewness\nvar = X_train.columns\n\nplt.figure(figsize=(30,45))\ni=0\nfor col in var:\n    i += 1\n    plt.subplot(10,3, i)\n    sns.distplot(X_train[col])\n    plt.grid()\n\nplt.show()","24908d4b":"# Lets check the skewness of the features\nvar = X_train.columns\nskew_list = []\nfor i in var:\n    skew_list.append(X_train[i].skew())\n\ntmp = pd.concat([pd.DataFrame(var, columns=[\"Features\"]), pd.DataFrame(skew_list, columns=[\"Skewness\"])], axis=1)\ntmp.set_index(\"Features\", inplace=True)\ntmp","09d32ed9":"# Filtering the features which has skewness less than -1 and greater than +1\nskewed = tmp.loc[(tmp[\"Skewness\"] > 1) | (tmp[\"Skewness\"] <-1 )].index\nskewed.tolist()","40b391e2":"# preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\npt = PowerTransformer()\n\n# Fitting the power transformer in train data\nX_train[skewed] = pt.fit_transform(X_train[skewed])\n\n\n# Transforming the test data\nX_test[skewed] = pt.transform(X_test[skewed])","c257611d":"# plot the histogram of a variable from the dataset again to see the result \nvar = X_train.columns\n\nplt.figure(figsize=(30,45))\ni=0\nfor col in var:\n    i += 1\n    plt.subplot(10,3, i)\n    sns.distplot(X_train[col])\n    plt.grid()\n\nplt.show()","a3339413":"# Class imbalance\ny_train.value_counts()\/y_train.shape","91e142e9":"# Logistic Regression parameters for K-fold cross vaidation\nparams = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\n#perform cross validation\nmodel_cv = GridSearchCV(estimator = LogisticRegression(),\n                        param_grid = params, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        n_jobs=-1,\n                        verbose = 1,\n                        return_train_score=True) \n#perform hyperparameter tuning\nmodel_cv.fit(X_train, y_train)\n#print the evaluation result by choosing a evaluation metric\nprint('Best ROC AUC score: ', model_cv.best_score_)","062f2945":"#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', model_cv.best_params_)","01e69914":"# cross validation results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","10550d8c":"# plot of C versus train and validation scores\nplt.figure(figsize=(16, 8))\nplt.plot(cv_results['param_C'], cv_results['mean_test_score'])\nplt.plot(cv_results['param_C'], cv_results['mean_train_score'])\nplt.xlabel('C')\nplt.ylabel('ROC AUC')\nplt.legend(['test result', 'train result'], loc='upper left')\nplt.xscale('log')\nplt.grid()","7f4c9442":" model_cv.best_params_","e4c31c47":"# Instantiating the model with best C\nlog_reg_imb_model = model_cv.best_estimator_\n\n# Fitting the model on train dataset\nlog_reg_imb_model.fit(X_train, y_train)","73f42a12":"# Creating function to display ROC-AUC score, f1 score and classification report\ndef display_scores(y_test, y_pred):\n    '''\n    Display ROC-AUC score, f1 score and classification report of a model.\n    '''\n    print(f\"F1 Score: {round(f1_score(y_test, y_pred)*100,2)}%\") \n    print(\"\\n\\n\")\n    print(f\"Classification Report: \\n {classification_report(y_test, y_pred)}\")","43c15b78":"# Predictions on the train set\ny_train_pred = log_reg_imb_model.predict(X_train)","0b7fdff5":"display_scores(y_train, y_train_pred)","89c998f3":"# ROC Curve function\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","c59ecfd1":"# Predicted probability\ny_train_pred_proba = log_reg_imb_model.predict_proba(X_train)[:,1]","20110879":"# Plot the ROC curve\ndraw_roc(y_train, y_train_pred_proba)","71c58c1b":"# Making prediction on the test set\ny_test_pred = log_reg_imb_model.predict(X_test)\ndisplay_scores(y_test, y_test_pred)","5725f69b":"# Predicted probability\ny_test_pred_proba = log_reg_imb_model.predict_proba(X_test)[:,1]","dd8ca204":"# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","1e50b608":"## Exploratory data analysis","193278fc":"Lot of features are highly skewed. So we will check the skewness using skew() and if the skewness is beyond -1 to 1, then we will use power transform to transform the data.","6ca9ba87":"### Outliers treatment","92c92801":"#  Logistic Regression","8f24cfb7":"#### Model Summary\n\n- Train set\n    -     ROC : 98%\n    - F1 Score: 74.47%\n    \n    \n- Test set\n    -     ROC : 97%\n    - F1 score: 72.83%","1ce911eb":"### There is skewness present in the distribution of the above features:\n- Power Transformer package present in the <b>preprocessing library provided by sklearn<\/b> is used to make the distribution more gaussian","8a6cbdbe":"### Splitting the data into train & test data","7fe489b6":"We need to scale `Amount` column.","778b95bb":"Dropping `Time` column as this feature is not going to help in the model building.\n#### Understanding from Core Banking Business ","f2847803":"### Plotting the distribution of a variable to handle skewness","d3a73f36":"### Feature Scaling using  RobustScaler Scaler","ce95970b":"#### Prediction and model evalution on the train set","440da9ac":"# Problem Statement\nThe problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.\n\n \n\nIn this project, we will analyse customer-level data that has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group. \n\n \n\nThe data set is taken from the Kaggle website and has a total of 2,84,807 transactions; out of these, 492 are fraudulent. Since the data set is highly imbalanced, it needs to be handled before model building.\n\n \n\n# Business problem overview\nFor many banks, retaining high profitable customers is the number one business goal. Banking fraud, however, poses a significant threat to this goal for different banks. In terms of substantial financial losses, trust and credibility, this is a concerning issue to both banks and customers alike.\n\n\nIt has been estimated by Nilson Report that by 2020, banking frauds would account for $30 billion worldwide. With the rise in digital payment channels, the number of fraudulent transactions is also increasing in new and different ways. \n\n \n\nIn the banking industry, credit card fraud detection using machine learning is not only a trend but a necessity for them to put proactive monitoring and fraud prevention mechanisms in place. Machine learning is helping these institutions to reduce time-consuming manual reviews, costly chargebacks and fees as well as denials of legitimate transactions.\n\n \n\n# Understanding and defining fraud\nCredit card fraud is any dishonest act or behaviour to obtain information without proper authorisation from the account holder for financial gain. Among different ways of committing frauds, skimming is the most common one, which is a way of duplicating information that is located on the magnetic strip of the card. Apart from this, the other ways are as follows:\n\n- Manipulation\/alteration of genuine cards\n- Creation of counterfeit cards\n- Stealing\/loss of credit cards\n- Fraudulent telemarketing\n \n\n# Data dictionary\nThe data set can be downloaded using this link.\n\n \n\nThe data set includes credit card transactions made by European cardholders over a period of two days in September 2013. Out of a total of 2,84,807 transactions, 492 were fraudulent. This data set is highly unbalanced, with the positive class (frauds) accounting for 0.172% of the total transactions. The data set has also been modified with principal component analysis (PCA) to maintain confidentiality. Apart from \u2018time\u2019 and \u2018amount\u2019, all the other features (V1, V2, V3, up to V28) are the principal components obtained using PCA. The feature 'time' contains the seconds elapsed between the first transaction in the data set and the subsequent transactions. The feature 'amount' is the transaction amount. The feature 'class' represents class labelling, and it takes the value of 1 in cases of fraud and 0 in others.\n\n \n\n# Project pipeline\nThe project pipeline can be briefly summarised in the following four steps:\n\n# Data Understanding: \n\nHere, we need to load the data and understand the features present in it. This would help us choose the features that we will need for our final model.\n\n- **Exploratory data analytics (EDA)**: Normally, in this step, we need to perform univariate and bivariate analyses of the data, followed by feature transformations, if necessary. For the current data set, because Gaussian variables are used, we do not need to perform Z-scaling. However, we  can check whether there is any skewness in the data and try to mitigate it, as it might cause problems during the model building phase.\n- **Train\/Test split**: Now, we are familiar with the train\/test split that we can perform to check the performance of our models with unseen data. Here, for validation, we can use the k-fold cross-validation method. We need to choose an appropriate k value so that the minority class is correctly represented in the test folds.\n- **Model building \/ hyperparameter tuning**: This is the final step at which we can try different models and fine-tune their hyperparameters until we get the desired level of performance on the given data set. We should try and check if we get a better model by various sampling techniques.\n- **Model evaluation**: Evaluate the models using appropriate evaluation metrics. Note that since the data is imbalanced, it is is more important to identify the fraudulent transactions accurately than the non-fraudulent ones. Choose an appropriate evaluation metric that reflects this business goal.","64f9078d":"As the whole dataset is transformed with PCA, so assuming that the outliers are already treated. Hence, we are not performing any outliers treatment on the dataframe, though we still see outliers available.","e5d88dae":"#### Logistic Regression with optimal C","40b57add":"We can see very good ROC on the test data set 0.97.","1c688ac8":"We can see that there is no missing value present in the dataframe.","4a29bdce":"#### Evaluating the model on the test set","98d0e45a":"**Observation**\n\nClearly low amount transactions are more likely to be fraudulent than high amount transaction.","6fb941c7":"### Observe the distribution of our classes","40256851":"There are no features which there is high correlatation , corr > .75","9d941906":"**Observation**\n\nThere is not much insight can be drawn from the distribution of the fraudulent transaction based on time as fraudulent\/non-fraudulent both transaction are distributed over time.","79887575":"#### Plotting the distributions of all the features","74a3bbe4":"We can see most of the features distributions are overlapping for both the fraud and non-fraud transactions.","0ef9a7a5":"### Handling Missing Values","c811c3ca":"# Credit Card Fraud Detection\n\nIn this project we will predict fraudulent credit card transactions with the help of Machine learning models. \n\nIn order to complete the project, we are going to follow below high level steps to build and select best model.\n- Read the dataset and perform exploratory data analysis\n- Building different classification models on the unbalanced data\n- Building different models on 3 different balancing technique.\n    - Random Oversampling\n    - SMOTE\n    - ADASYN","0956ffc4":"## Model Building with imbalanced data\nWe are going to build models on below mentioned algorithms and we will compare for the best model. We are not building models on SVM,  and KNN as these algorithms are computationaly expensive and need more computational resources specially for the SVM and KNN.  Skipped models' process is computationally very expensive when we have very large data set. We do not have these resource available so we are skipping these models. Working with below models:\n    - Logistic Regression\n    - Decision Tree\n    - RandomForest\n    - XGBoost\n\n#### Metric selection on imbalance data\nWe are going to use ROC-AUC score as the evaluation metric for the model evaluation purpose. As the data is highly imbalanced and we have only 0.17% fraud cases in the whole data, accuracy will not be the right metric to evaluate the model.","8a687c3e":"**Observation**\n\nThe dataset has very high class imbalance. Only 492 records are there among 284807 records which are labeld as fradudulent transaction."}}