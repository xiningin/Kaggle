{"cell_type":{"7d738ed0":"code","0e30e2c0":"code","f850308f":"code","e3e625a1":"code","2b691386":"code","da902ea5":"code","63745595":"code","a8a04e7b":"code","e5c0f762":"code","6597d66e":"code","53388cc9":"code","de7f5666":"code","fc83cf14":"code","3bb2f908":"code","1c2082bb":"code","4502fbcb":"code","028a2bd3":"code","97ca08b0":"code","847d40e5":"code","865ec825":"code","8662913c":"code","ba4c2728":"code","a1e5c413":"code","cb7f59be":"code","2d76cd93":"code","40aa2227":"code","ed3fbe89":"code","8da8adbc":"code","0a47b072":"code","dfaf2d67":"code","790fabf1":"code","bc8d09d1":"code","8ad14da5":"code","48454dcd":"code","298f43a8":"code","0d90e7c3":"code","479e2c53":"code","7528efce":"markdown","24048ade":"markdown","8aed6c26":"markdown","d60420fd":"markdown","619e4855":"markdown","436994f4":"markdown","9f10bb35":"markdown","94eb7730":"markdown","e5cf0f15":"markdown","cf879d0e":"markdown","7c1c3ae2":"markdown"},"source":{"7d738ed0":"import numpy as np\nimport pandas as pd\n\n#Reading the test and the train data\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\nprint('train shape : ', train.shape)\nprint('test shape : ', test.shape)","0e30e2c0":"train.head()","f850308f":"test.head()","e3e625a1":"train = train.set_index('row_id')\ntest = test.set_index('row_id')","2b691386":"#Initial EDA \n\ntrain.head(10)","da902ea5":"train.info()","63745595":"#Check for null values\n\ntrain.isnull().sum()","a8a04e7b":"test.isnull().sum()","e5c0f762":"#Looking at basic stats\n\n#For numeric cols\n\ntrain.describe().T  #The transpose is used for better view.","6597d66e":"test.describe().T","53388cc9":"#For categorical columns\n\ntrain.describe(include=['O'])","de7f5666":"test.describe(include=['O'])","fc83cf14":"#Lets look at the frequency distribution of the key categorical variables country, store, product\n\nprint(train['country'].value_counts())\nprint('\\n')\nprint(train['store']. value_counts())\nprint('\\n')\nprint(train['product'].value_counts())\n\n","3bb2f908":"train.groupby(['country','product']).size()","1c2082bb":"#Looking at the dates feature to understand what period the data belongs to. \n\ndef min_max_dt(df, name='train'):\n    #df['date'] = pd.to_datetime(df['date'])\n    min_date = df['date'].min()\n    max_date = df['date'].max()\n    print(f'For the {name} data : Min_date - {min_date} \/ Max_date - {max_date}')\n    return None\n          \n          \nmin_max_dt(train, 'train')\nmin_max_dt(test, 'test')\n","4502fbcb":"\ndef smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) \/ (y_true + np.abs(y_pred)) * 200\n\n#print(smape_loss(tf.constant([1, 2]), tf.constant([3, 4]))) # should print [100, 66.6667]","028a2bd3":"#Credit to https:\/\/www.kaggle.com\/jaredfeng\/tps-jan22-inprog-v5\n\nholiday_path = '..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv'\n\ndef GetHoliday(holiday_path, df):\n    \"\"\"\n    Get a boolean feature of whether the current row is a holiday sale\n    \"\"\"\n    \n    holiday = pd.read_csv(holiday_path)\n    fin_holiday = holiday.loc[holiday.Country == 'Finland']\n    swe_holiday = holiday.loc[holiday.Country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.Country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.Date).astype(float)\n    df['swe holiday'] = df.date.isin(swe_holiday.Date).astype(float)\n    df['nor holiday'] = df.date.isin(nor_holiday.Date).astype(float)\n    \n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    return df\n\ntrain = GetHoliday(holiday_path, train)\ntest = GetHoliday(holiday_path, test)\n","97ca08b0":"# Credit to https:\/\/www.kaggle.com\/ranjeetshrivastav\/tps-jan-21-base-xgb\n# and https:\/\/www.kaggle.com\/bernhardklinger\/tps-jan-2022\/notebook\n\ndef feature_eng(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df['week']= df['date'].dt.week\n    #df['year'] = 'Y' + df['date'].dt.year.astype(str)\n    df['quarter'] = 'Q' + df['date'].dt.quarter.astype(str)\n    df['day'] = df['date'].dt.day\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df.loc[(df.date.dt.is_leap_year) & (df.dayofyear >= 60),'dayofyear'] -= 1\n    df['weekend'] = df['date'].dt.weekday >=5\n    df['weekday'] = 'WD' + df['date'].dt.weekday.astype(str)\n    df.drop(columns=['date'],inplace=True)  \n\nfeature_eng(train)\nfeature_eng(test)","847d40e5":"train.columns","865ec825":"train.head()","8662913c":"test.head()","ba4c2728":"train.info()","a1e5c413":"train['day'].value_counts()","cb7f59be":"X_train = train.drop(['num_sold'],axis=1)\ny_train = train['num_sold']\nX_test = test   \n","2d76cd93":"X_train.columns","40aa2227":"X_train_ohe = pd.get_dummies(X_train)\nX_test_ohe = pd.get_dummies(X_test)\nfinal_train, final_test = X_train_ohe.align(X_test_ohe,join='left', axis=1)\n\n#Please refer to this excellent notebook from Dans Becker to understand how to align categorical columns\n\n","ed3fbe89":"final_train.head()","8da8adbc":"final_test.head()","0a47b072":"#Standardising the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(final_train)\nX_test_scaled = scaler.transform(final_test)\n\n","dfaf2d67":"#Creating a custom scoring for cross_validation\n\nfrom sklearn.metrics import fbeta_score, make_scorer\n\n# Credit to https:\/\/www.kaggle.com\/c\/web-traffic-time-series-forecasting\/discussion\/36414\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\nsmape_score = make_scorer(SMAPE, greater_is_better=False)\n","790fabf1":"\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.model_selection import cross_val_score\n\n\ntt = TransformedTargetRegressor(regressor=LinearRegression(),\n                                   func=np.log, inverse_func=np.exp)\n\nscores = cross_val_score(tt, X_train_scaled, y_train, scoring=smape_score, cv=5)\n","bc8d09d1":"scores","8ad14da5":"#Lets fit on the training data\ntt.fit(X_train_scaled, y_train)\n\n","48454dcd":"\ntest_y_pred = tt.predict(X_test_scaled)\n\n","298f43a8":"test_y_pred","0d90e7c3":"#submission\n\nassert(len(test.index)==len(test_y_pred))\n\nsubmission_df = pd.DataFrame(list(zip(test.index, test_y_pred)), columns=['row_id', 'num_sold'])\n\nsubmission_df.to_csv('submission.csv', index=False)","479e2c53":"submission_df","7528efce":"Steps to take\n\n1. Convert the categorical variables into numeric\n2. Align the train and test columns\n3. Run the baseline linear model\n4. Predict and submit predictions","24048ade":"Now lets run the Linear regression as our baseline model, since we want to use log for the dependent variable, we can use scikitlearn's TransformedRegressor.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.TransformedTargetRegressor.html\n\n","8aed6c26":"One interesting aspect is that the product and the country distributions are the same (8766), does that mean a particular country produces just one of those products? Lets check it out.","d60420fd":"The dataset does not have any null values","619e4855":"4. Predict and submit predictions","436994f4":"The metric that will be used to evaluate the competition:","9f10bb35":"Next steps:\n    \n    1. Train on the entire training set and predict\n    2. Create additional rolling features\n    3. Use Gradient Boosting methods to add to the model","94eb7730":"Thus we can see in the training dataset we have 3 unique countries, 2 unique stores (KaggleMart or KaggleRama) and 3 unique products","e5cf0f15":"Step 1 - Converting categorical variables into numeric & Step 2 - Align the train and test columns\n","cf879d0e":"3. Run the baseline linear regression model","7c1c3ae2":"The train data is from Jan 2015 to Dec 2018, while the test data is from Jan 2019 to Dec 2019"}}