{"cell_type":{"cc31eefd":"code","36c36b3a":"code","d5b30e0a":"code","3e2fbbb9":"code","d5829edc":"code","125fe01b":"code","26634f90":"code","1d523c27":"code","d549b787":"code","8940ed7f":"code","04d24e0d":"code","09a1c047":"markdown"},"source":{"cc31eefd":"import matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torch.nn as neural_network\nimport torch.nn.functional as functions\nimport torchvision.transforms as transforms\nimport torch.nn as model\nimport torch.nn.functional as functions\nimport torch.optim as optim\n\nfrom torchvision import datasets","36c36b3a":"def download_data_digits():\n    return datasets.FashionMNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n\ntrain_data = download_data_digits()","d5b30e0a":"train = torch.utils.data.DataLoader(train_data, batch_size=64, num_workers=2)","3e2fbbb9":"def show_samples_test(length=10):\n    images, labels = iter(train).next()\n    images = images.numpy()\n    \n    indexes = 0\n    for k in range(0, 5):\n        fig, axs = plt.subplots(1, length, figsize=(10, 10))\n\n        for i in range(0, length):\n            axs[i].imshow(np.squeeze(images[i + indexes]), label=labels[i + i + indexes])\n            leg = axs[i].legend()\n            \n        indexes += 10\n","d5829edc":"show_samples_test(10)","125fe01b":"class Discriminator(neural_network.Module):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        self.input_layer    = neural_network.Linear(784, 128)\n        self.hidden_layer_1 = neural_network.Linear(128, 64)\n        self.hidden_layer_2 = neural_network.Linear(64, 32)\n        self.output_layer   = neural_network.Linear(32, 1)\n\n        self.dropout = neural_network.Dropout(0.2)\n        \n        \n    def forward(self, img_input):\n        img_input = img_input.view(-1, 28 * 28)\n        img_input = functions.leaky_relu(self.input_layer(img_input), 0.2) \n        img_input = self.dropout(img_input)\n        img_input = functions.leaky_relu(self.hidden_layer_1(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        img_input = functions.leaky_relu(self.hidden_layer_2(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        \n        return self.output_layer(img_input)","26634f90":"class Generator(neural_network.Module):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.input_layer    =  neural_network.Linear(100, 32)\n        self.hidden_layer_1 =  neural_network.Linear(32, 64)\n        self.hidden_layer_2 =  neural_network.Linear(64, 128)\n        self.output_layer   =  neural_network.Linear(128, 784)\n        \n        self.dropout = neural_network.Dropout(0.2)\n\n    def forward(self, img_input):\n        img_input = functions.leaky_relu(self.input_layer(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        img_input = functions.leaky_relu(self.hidden_layer_1(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        img_input = functions.leaky_relu(self.hidden_layer_2(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        \n        return functions.tanh(self.output_layer(img_input))","1d523c27":"D = Discriminator()\nG = Generator()\n\ndef real_loss(D_out):\n    batch_size = D_out.size(0)\n    criterion = neural_network.BCEWithLogitsLoss()\n    return criterion(D_out.squeeze(), torch.ones(batch_size))\n    \n\ndef fake_loss(D_out):\n    criterion = neural_network.BCEWithLogitsLoss()\n    return criterion(D_out.squeeze(), torch.zeros(D_out.size(0)))\n\nd_optimizer = optim.Adam(D.parameters(), 0.002)\ng_optimizer = optim.Adam(G.parameters(), 0.002)","d549b787":"def train_discriminator(G, D, optimizer, real_data, batch_size, size):\n    real_data = scale(real_data.view(batch_size, -1))\n\n    optimizer.zero_grad()\n    D.train()\n\n    fake_data = G.forward(random_vector(batch_size, size))\n    total_loss = real_loss(D.forward(real_data)) + fake_loss(D.forward(fake_data))\n    total_loss.backward()\n    optimizer.step()\n    \n    return total_loss\n\ndef train_generator(G, D, optimizer, batch_size, size):\n    optimizer.zero_grad()\n    G.train()\n    fake_data = G.forward(random_vector(batch_size, size))\n    loss = real_loss(D.forward(fake_data))\n    loss.backward()\n    optimizer.step()\n    return loss\n\ndef random_vector(batch_size, lenx):\n    return torch.randn(batch_size, lenx).float()","8940ed7f":"def show_training_sample(data):\n    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4)\n    for ax, img in zip(axes.flatten(), data[-1]):\n        img = img.detach()\n        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n    plt.show()\n    \n    \ndef show_training_loss(d_loss_, g_loss):\n    plt.plot(d_loss_arr, label=\"N.Discriminator\", alpha=0.5)\n    plt.plot(g_loss_arr, label=\"N.Generator\", alpha=0.5)\n    plt.title(\"Trainings loss\")\n    plt.legend()\n    plt.show()","04d24e0d":"import pickle as pkl\n\nmax_epochs = 25\n\nsamples = list()\nlosses =  list()\nd_loss_arr = list()\ng_loss_arr = list()\n\nZ = torch.from_numpy(np.random.uniform(-1, 1, size=(16, 100))).float()\n\nD.train()\nG.train()\n                           \nfor epoch in range(1, max_epochs):\n    for index, (img, l) in enumerate(train):\n        img = img * 2 - 1 \n        \n        d_optimizer.zero_grad()\n        \n        fake_images = G(torch.from_numpy(np.random.uniform(-1, 1, size=(img.size(0), 100))).float())\n\n        d_loss = (real_loss(D(img))) + fake_loss(D(fake_images))\n        d_loss.backward()\n        d_optimizer.step()\n        \n        g_optimizer.zero_grad()\n\n        fake_images = G(torch.from_numpy(np.random.uniform(-1, 1, size=(img.size(0), 100))).float())\n\n        g_loss = real_loss(D(fake_images)) \n        \n       \n        g_loss.backward()\n        g_optimizer.step()\n        \n        d_loss_arr.append(d_loss.item())\n        g_loss_arr.append(g_loss.item())\n        \n      \n        if index % 20 == 0 or index == 1:\n            print(f'[{epoch}] -- discriminator_loss: {d_loss.item()}  -- generator_loss: {g_loss.item()}')\n        \n\n    losses.append((d_loss.item(), g_loss.item()))\n\n    G.eval() \n    samples.append(G(Z))\n    G.train()\n    \n    show_training_sample(samples)\n    show_training_loss(d_loss_arr, g_loss_arr)","09a1c047":"# Generating fashion with GaNs\n\nA generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss).\n\n- More about GAN: https:\/\/en.wikipedia.org\/wiki\/Generative_adversarial_network\n- More about PyTorch: https:\/\/pytorch.org\/\n\n- **More examples: https:\/\/www.kaggle.com\/vitorgamalemos\/generating-digits-with-gans** "}}