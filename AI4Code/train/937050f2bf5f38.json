{"cell_type":{"c5e6ff9d":"code","4dd02c8c":"code","e98a4106":"code","ed83d60d":"code","61d0b932":"code","1ade39d7":"code","c502e4fa":"code","f47dd003":"code","3e407476":"code","412070b7":"code","59130364":"code","cc83f78a":"code","b3bf39e6":"code","b8b621e1":"code","6e673753":"code","1d852b83":"code","f9ebb312":"code","87bd5b05":"code","b59dcc06":"code","f8b34216":"code","ed79b876":"code","0d69719f":"code","391f6a76":"code","2b61003b":"code","debf886b":"code","c9fda9cb":"code","e4a7c1fd":"code","131ad937":"code","c1509f24":"code","1cc130fe":"code","0dc585d8":"code","803f6ec1":"code","d96b77f5":"code","c4ffbe81":"code","24ddfb63":"code","107f7b31":"markdown","24543ab3":"markdown","dffb5824":"markdown","a03db79f":"markdown","91aa84bb":"markdown","59b46862":"markdown","00a569b2":"markdown","b97a5b20":"markdown","406b336a":"markdown","21d939a4":"markdown","adf2e185":"markdown","5477abec":"markdown","db9a7abb":"markdown","e8b192f3":"markdown","0124ff60":"markdown","5a6121c3":"markdown","88434777":"markdown","8a57cc20":"markdown","18fb317a":"markdown","fe82b250":"markdown","390c54a5":"markdown","887b0f5b":"markdown","f051c502":"markdown","ac4b8df3":"markdown","65a6ba00":"markdown","5f4040dd":"markdown","b8cee620":"markdown"},"source":{"c5e6ff9d":"#VIZ AND DATA MANIPULATION LIBRARY\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nsns.set_style('whitegrid')\n\nfrom plotly import tools\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\n#Preprocessing\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom scipy.stats import uniform\n\n#MODELS\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom bayes_opt import BayesianOptimization\n\n\n#CLASSICAL STATS\nimport scipy\nimport statsmodels\nfrom scipy.stats import boxcox\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport statsmodels.api as sm\nfrom fbprophet import Prophet\nfrom statsmodels.tsa.statespace import sarimax\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n\n#DEEP LEARNING LIB\nfrom keras.models import Model,Sequential\nfrom keras.utils import np_utils, to_categorical\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom tensorflow.keras.utils import plot_model\nimport itertools\nimport lightgbm as lgb\n\n\n\n#METRICS\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report, r2_score,mean_absolute_error,mean_squared_error\n\nfrom random import randrange\nimport warnings \nwarnings.filterwarnings('ignore')","4dd02c8c":"df = pd.read_csv('..\/input\/foreign-exchange-rates-per-dollar-20002019\/Foreign_Exchange_Rates.csv')\nprint('DATASET SHAPE: ', df.shape)\ndf.head()","e98a4106":"#show columns\ndf.columns = [col.lower() for col in df.columns]\ndf['time serie'] = pd.to_datetime(df['time serie'])\ndf.columns","ed83d60d":"#get the date and rates of singapore dollar\ndata = df[['time serie', 'singapore - singapore dollar\/us$']]\ndata.columns = ['date', 'rate']","61d0b932":"#show new dataframe\ndata.head()","1ade39d7":"#show feature data types\ndata.info()","c502e4fa":"#remove rates with a value of ND\ndata = data.drop(data[data['rate']=='ND'].index)\n#converte the rates to numeric value\ndata['rate'] = pd.to_numeric(data.rate)\n#sort values by date\ndata = data.sort_values('date', ascending=True)","f47dd003":"#show basic stats\ndata.rate.describe()","3e407476":"plt.figure(figsize=(10,5))\nsns.distplot(data.rate, bins=10, color='steelblue');","412070b7":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=data.date, y=data.rate, marker_color='lightgreen'))\n\nfig.update_layout(title='TIME-SERIES PLOT OF SINGAPORE DOLLAR RATE', \n                  height=450, width=1000, template='plotly_dark', font_color='lightgreen', \n                  font=dict(family=\"sans serif\",\n                            size=16,\n                            color=\"grey\"\n                            ))\n\nfig.update_xaxes(title='Date')\nfig.update_yaxes(title='Rate \/ $')\nfig.show()","59130364":"fig, ax = plt.subplots(1,2,figsize=(14,4))\nplot_acf(data.rate, lags=20, ax=ax[0]);\nplot_pacf(data.rate, lags=20, ax=ax[1]);","cc83f78a":"sdec = seasonal_decompose(data.rate, model='multiplicative', freq=1)\nsdec.plot();","b3bf39e6":"\nX_train, X_val = data[:-30], data[-30:]\n\nprint('X_train Shape: ', X_train.shape)\nprint('X_val Shape: ', X_val.shape)","b8b621e1":"predictions = []\n\narima = sm.tsa.statespace.SARIMAX(X_train.rate,order=(1,1,1),seasonal_order=(1,1,1,6),\n                                  enforce_stationarity=False, enforce_invertibility=False,).fit()\n#get a 30 days prediction\npredictions.append(arima.forecast(30))\n#converting and reshaping \npredictions = np.array(predictions).reshape((30,))","6e673753":"arima.summary()","1d852b83":"res = arima.resid\nfig,ax = plt.subplots(1,2,figsize=(14,5))\nplt.suptitle('ACF AND PACF PLOT OF RESIDUALS', fontsize=22, x=0.5, y=1.04)\nfig = sm.graphics.tsa.plot_acf(res, lags=50, ax=ax[0])\nfig = sm.graphics.tsa.plot_pacf(res, lags=50, ax=ax[1])\nplt.show()","f9ebb312":"y_val = data.rate[-30:]\nplt.figure(figsize=(14,5))\nplt.plot(np.arange(len(y_val)), y_val, color='steelblue');\nplt.plot(np.arange(len(y_val)), predictions, color='salmon');\nplt.legend(['True Value', 'Prediction']);\n","87bd5b05":"\narima_mae = mean_absolute_error(y_val, predictions)\narima_mse = mean_squared_error(y_val, predictions)\narima_rmse = np.sqrt(mean_squared_error(y_val, predictions))\n\nprint('Mean Absolute Error:   ', arima_mae)\nprint('Mean Squared Error:   ', arima_mse)\nprint('Root Mean Squared Error:   ', arima_rmse)","b59dcc06":"arima_error_rate = abs(((y_val - predictions) \/ y_val).mean()) * 100\nprint('MAPE:', round(arima_error_rate,2), '%')","f8b34216":"print('R2-SCORE: ', r2_score(y_val, predictions))","ed79b876":"#extract the date feature\ndata['day'] = data.date.dt.day\ndata['dayofweek'] = data.date.dt.dayofweek\ndata['dayofyear'] = data.date.dt.dayofyear\ndata['week'] = data.date.dt.week\ndata['month'] = data.date.dt.month\ndata['year'] = data.date.dt.year","0d69719f":"#add lag feature\nfor i in range(1,8):\n    data['lag'+str(i)] = data.rate.shift(i).fillna(0)","391f6a76":"#drop the date feature\ndata.drop('date', axis=1, inplace=True)\n#show new data frame\ndata.head(7)","2b61003b":"X = data.drop('rate', axis=1)\ny = data.rate\n\nX_train, X_test = X[:-30], X[-30:]\ny_train, y_test = y[:-30], y[-30:]\n\nprint('X_train: ', X_train.shape)\nprint('y_train: ', y_train.shape)\nprint('X_test: ', X_test.shape)\nprint('y_test: ', y_test.shape)","debf886b":"#convert data to xgb matrix form\ndtrain = xgb.DMatrix(X_train,label=y_train)\ndtest = xgb.DMatrix(X_test)","c9fda9cb":"#bayesian hyper parameter tuning\n#define the params\ndef xgb_evaluate(max_depth, gamma, colsample_bytree):\n    params = {'eval_metric': 'rmse',\n              'max_depth': int(max_depth),\n              'subsample': 0.8,\n              'eta': 0.1,\n              'gamma': gamma,\n              'colsample_bytree': colsample_bytree}\n    \n    cv_result = xgb.cv(params, dtrain, num_boost_round=250, nfold=3)    \n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]","e4a7c1fd":"#run optimizer\nxgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 7), \n                                             'gamma': (0, 1),\n                                             'colsample_bytree': (0.3, 0.9)})\n#define iter points\nxgb_bo.maximize(init_points=10, n_iter=15, acq='ei')","131ad937":"\n#get the best parameters\nparams = xgb_bo.max['params']\nparams['max_depth'] = int(round(params['max_depth']))\n#train the data\nmodel = xgb.train(params, dtrain, num_boost_round=200)","c1509f24":"#predict the test data \npredictions = model.predict(dtest)","1cc130fe":"y_val = data.rate[-30:]\nplt.figure(figsize=(14,5))\nsns.set_style('darkgrid')\nplt.plot(np.arange(len(y_val)), y_val, color='salmon');\nplt.plot(np.arange(len(y_val)), predictions, color='lightgreen');\nplt.legend(['True Value', 'Prediction']);\n","0dc585d8":"xgb_mae = mean_absolute_error(y_val, predictions)\nxgb_mse = mean_squared_error(y_val, predictions)\nxgb_rmse = np.sqrt(mean_squared_error(y_val, predictions))\n\nprint('Mean Absolute Error:   ', xgb_mae)\nprint('Mean Squared Error:   ', xgb_mse)\nprint('Root Mean Squared Error:   ', xgb_rmse)","803f6ec1":"xgb_error_rate = abs(((y_val - predictions) \/ y_val).mean()) * 100\nprint('MAPE:', round(xgb_error_rate,2), '%')","d96b77f5":"print('R2-SCORE: ', r2_score(y_val, predictions))","c4ffbe81":"\n#function that can generate a monte carlo simulation    \ndef monte_carlo_simulation(data,t_intervals ,iteration , figsize = (10,4), lw=1):\n    from scipy.stats import norm\n\n    #log returns of data\n    log_returns = np.log(1 + data.pct_change())\n\n    #Setting up the drift and random component\n    mean_  = log_returns.mean()\n    var = log_returns.var()\n    stdev = log_returns.std()\n    drift = mean_ - (0.5 *var)\n\n    daily_returns = np.exp(drift + stdev * norm.ppf(np.random.rand(t_intervals, iteration)))\n\n    S0 = data.iloc[-1]\n    #Empty daily returns\n    price_list = np.zeros_like(daily_returns)\n    price_list[0] = S0\n\n    #appliying montecarlo simulation\n    for i in range(1 , t_intervals):\n        price_list[i] = price_list[i-1] * daily_returns[i]\n    fig_title = str(t_intervals)+ ' DAYS SIMULATION WITH ' +str(iteration)+' DIFFERENT POSSIBILITIES'\n    #Show the result of 30 days simulation\n    plt.figure(figsize=figsize)\n    plt.plot(price_list, lw=lw)\n    plt.title(fig_title)\n    plt.xlabel('Interval', fontsize=16)\n    plt.ylabel('Value', fontsize=16)","24ddfb63":"#fit the X_train and show the figure\nmonte_carlo_simulation(y_train,30,20, figsize=(13,6))","107f7b31":"## EDA\n---","24543ab3":"#### * Note: Never forget to arrange the data by date, because date arrangement is very important when it comes to forecasting.","dffb5824":"#### MAE, MSE ,RMSE","a03db79f":"### TIME-SERIES PLOT","91aa84bb":"## FORECASTING SECTION\n---","59b46862":"#### MEAN ABSOLUTE PERCENTAGE ERROR","00a569b2":"### AUTO CORRELATION FUNCTION PLOT","b97a5b20":"#### Feature Engineering","406b336a":"#### R-SQUARED SCORE","21d939a4":"* GET THE LAST 30 DAYS DATA FOR MODEL VALIDATION\n","adf2e185":"### SEASONAL DECOMPOSITION","5477abec":"### ARIMA (Auto Regressive Integrated Moving Average)","db9a7abb":"#### ARIMA SUMMARY","e8b192f3":"### XGBOOST","0124ff60":"* The autocorrelation function shows a very slow decay, which means that the future values have a very high correlation with its past values.\n\n* The partial autocorrelation function shows a high correlation with the first lag and lesser correlation with the second and third lag.","5a6121c3":"#### COMPARISON OF TRUE VALUE AND XGBOOST PRODECTIONS","88434777":"### DATA DISTRIBUTION","8a57cc20":"#### READ THE DATA","18fb317a":"### MONTE CARLO SIMULATION","fe82b250":"#### SPLIT THE DATA","390c54a5":"#### XGBOOST BAYESIAN OPTIMIZATION","887b0f5b":"#### COMPARISON OF TRUE VALUE AND ARIMA PREDICTIONS","f051c502":"#### MAE, MSE, RMSE","ac4b8df3":"#### R-SQUARED SCORE","65a6ba00":"#### MEAN ABSOLUTE PERCENTAGE ERROR","5f4040dd":"## OVERVIEW\n---\n* Exploratory Data Analysis\n    * Univariate & Bivariate Analysis\n    * AutoCorrelation Functions\n    * Etc...\n* Feature Engineering\n* Predictive Modelling\n    * ARIMA Model\n    * XGBoost With Bayesian Optimization\n* Metrics Comparison of Predictive Models\n* Monte Carlo Simulation","b8cee620":"* We can see that the residual plot shows zero. The decomposition was not able to separate the noise that we added from the linear trend."}}