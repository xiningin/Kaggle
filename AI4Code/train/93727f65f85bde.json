{"cell_type":{"373282a8":"code","3944bb7e":"code","25662e94":"code","f33a9f23":"code","8de45b80":"code","bc488eb4":"code","2565e5a4":"code","9373c1a3":"code","26bcdd26":"code","737c368c":"code","9d116165":"code","a9c132e9":"code","f016b403":"code","9d76a5bb":"code","af1895b2":"code","055e0bc0":"code","b90758fa":"code","7ee28e38":"code","9e12a6b0":"code","28838676":"code","4482fd63":"markdown","6d057f8a":"markdown","12ab42fc":"markdown","496a7bc9":"markdown","08341ac2":"markdown","50069b45":"markdown","0f764ef4":"markdown","40a377d3":"markdown","a8c54a04":"markdown","feacfb2c":"markdown"},"source":{"373282a8":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\nplt.style.use(\"ggplot\")\n\nimport tensorflow as tf","3944bb7e":"data = pd.read_csv(\"..\/input\/ner-dataset\/ner_dataset.csv\", encoding=\"latin1\")\ndata = data.fillna(method=\"ffill\")\ndata.head(20)","25662e94":"print(\"Unique words in corpus:\", data['Word'].nunique())\nprint(\"Unique tags in corpus:\", data['Tag'].nunique())","f33a9f23":"words = list(set(data[\"Word\"].values))\nwords.append(\"ENDPAD\")\nnum_words = len(words)","8de45b80":"tags = list(set(data[\"Tag\"].values))\nnum_tags = len(tags)","bc488eb4":"class SentenceGetter(object):\n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, p, t) for w, p, t in zip(\n            s[\"Word\"].values.tolist(),\n            s[\"POS\"].values.tolist(),\n            s[\"Tag\"].values.tolist()\n        )]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","2565e5a4":"getter = SentenceGetter(data)\nsentences = getter.sentences","9373c1a3":"sentences[0]","26bcdd26":"word2idx = {w: i + 1 for i, w in enumerate(words)}\ntag2idx = {t: i for i, t in enumerate(tags)}","737c368c":"word2idx","9d116165":"plt.hist([len(s) for s in sentences], bins=50)\nplt.show()","a9c132e9":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_len = 50\n\nX = [[word2idx[w[0]] for w in s] for s in sentences]\nX = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=num_words-1)\n\ny = [[tag2idx[w[2]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])","f016b403":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","9d76a5bb":"from tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense\nfrom tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional","af1895b2":"input_word = Input(shape=(max_len,))\nmodel = Embedding(input_dim=num_words, output_dim=50, input_length=max_len)(input_word)\nmodel = SpatialDropout1D(0.1)(model)\nmodel = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\nout = TimeDistributed(Dense(num_tags, activation=\"softmax\"))(model)\nmodel = Model(input_word, out)\nmodel.summary()","055e0bc0":"model.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)","b90758fa":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping","7ee28e38":"%%time\n\nchkpt = ModelCheckpoint(\n    \"model_weights.h5\", \n    monitor='val_loss',\n    verbose=1, \n    save_best_only=True, \n    save_weights_only=True, \n    mode='min'\n)\n\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy', \n    min_delta=0, \n    patience=1, \n    verbose=0, \n    mode='max', \n    baseline=None, \n    restore_best_weights=False\n)\n\ncallbacks = [chkpt, early_stopping]\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_test,y_test),\n    batch_size=32, \n    epochs=3,\n    callbacks=callbacks,\n    verbose=1\n)","9e12a6b0":"model.evaluate(x_test, y_test)","28838676":"i = np.random.randint(1, x_test.shape[0]) #659\np = model.predict(np.array([x_test[i]]))\np = np.argmax(p, axis=-1)\ny_true = y_test[i]\nprint(\"{:15}{:5}\\t {}\\n\".format(\"Word\", \"True\", \"Pred\"))\nprint(\"-\" *30)\nfor w, true, pred in zip(x_test[i], y_true, p[0]):\n    print(\"{:15}{}\\t{}\".format(words[w-1], tags[true], tags[pred]))","4482fd63":"### Task 4: Define Mappings between Sentences and Tags","6d057f8a":"*Essential info about tagged entities*:\n- geo = Geographical Entity\n- org = Organization\n- per = Person\n- gpe = Geopolitical Entity\n- tim = Time indicator\n- art = Artifact\n- eve = Event\n- nat = Natural Phenomenon","12ab42fc":"### Task 7: Train the Model","496a7bc9":"### Task 1: Project Overview and Import Modules","08341ac2":"### Task 2: Load and Explore the NER Dataset","50069b45":"<h2 align=center> Named Entity Recognition (NER) using LSTMs with Keras<\/h2>","0f764ef4":"### Task 5: Padding Input Sentences and Creating Train\/Test Splits","40a377d3":"### Task 8: Evaluate Named Entity Recognition Model","a8c54a04":"### Task 3: Retrieve Sentences and Corresponsing Tags","feacfb2c":"### Task 6: Build and Compile a Bidirectional LSTM Model"}}