{"cell_type":{"656c16cc":"code","7ea8d35c":"code","daead785":"code","9a728820":"code","057eff05":"code","eee1dbd2":"code","cb8f6fbb":"code","440499b1":"code","32c4460c":"code","7f9924ad":"code","9352bb17":"code","64fdf6c4":"code","00cc733a":"code","7e1076d7":"code","6ee046b3":"code","867988e4":"code","73a74719":"code","5e8a3869":"code","67b139a5":"code","d9f93bc9":"code","82f1525c":"code","d9f1df72":"code","f0f5cce0":"code","1c707e90":"code","de5a8fe7":"code","d5d8bd48":"code","9af59675":"code","c4ab3424":"code","857cc2b4":"code","1e18e29d":"code","20233a1f":"code","6dc7f75a":"code","d616c3c7":"code","41394d6e":"code","8c5f0f95":"code","04869065":"code","1b5a1667":"code","306c0b85":"code","dc45fae4":"code","05d9b916":"code","9028575b":"code","e663b5a5":"code","c043bb20":"code","732c9943":"code","48a9d89e":"code","3280d697":"code","d10b0e82":"code","be6a00ac":"code","bdba9242":"code","175947ed":"markdown","d3324098":"markdown","525c1af0":"markdown","40992b70":"markdown","4a66f72d":"markdown","e6a9d3fc":"markdown","187b7f0f":"markdown","794486c1":"markdown","f43b7a56":"markdown","f3a08ec8":"markdown","ae268555":"markdown","66c451a1":"markdown","a484111c":"markdown","d74ff020":"markdown","4f454946":"markdown","2e33c60d":"markdown","c87b65b0":"markdown","77918a7b":"markdown","7ecc82ce":"markdown","3a326ed2":"markdown","bb454c46":"markdown","1834a9d9":"markdown","3476701a":"markdown"},"source":{"656c16cc":"# import the usual suspects ...\nimport pandas as pd\nimport numpy as np\nimport glob\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pprint import pprint\n\n# suppress all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7ea8d35c":"accidents = pd.read_csv('..\/input\/uk-road-safety-accidents-and-vehicles\/Accident_Information.csv')\nprint('Records:', accidents.shape[0], '\\nColumns:', accidents.shape[1])\naccidents.head()","daead785":"#accidents.info()","9a728820":"#accidents.describe().T","057eff05":"#accidents.isna().sum()","eee1dbd2":"accidents['Date']= pd.to_datetime(accidents['Date'], format=\"%Y-%m-%d\")","cb8f6fbb":"# check\naccidents.iloc[:, 8:11].info()","440499b1":"# create a little dictionary to later look up the groups I will create\ndaytime_groups = {1: 'Morning (5-10)', \n                  2: 'Office Hours (10-15)', \n                  3: 'Afternoon Rush (15-19)', \n                  4: 'Evening (19-23)', \n                  5: 'Night(23-5)'}","32c4460c":"# slice first and second string from time column\naccidents['Hour'] = accidents['Time'].str[0:2]\n\n# convert new column to numeric datetype\naccidents['Hour'] = pd.to_numeric(accidents['Hour'])\n\n# drop null values in our new column\naccidents = accidents.dropna(subset=['Hour'])\n\n# cast to integer values\naccidents['Hour'] = accidents['Hour'].astype('int')","7f9924ad":"# define a function that turns the hours into daytime groups\ndef when_was_it(hour):\n    if hour >= 5 and hour < 10:\n        return \"1\"\n    elif hour >= 10 and hour < 15:\n        return \"2\"\n    elif hour >= 15 and hour < 19:\n        return \"3\"\n    elif hour >= 19 and hour < 23:\n        return \"4\"\n    else:\n        return \"5\"\n    \n# apply this function to our temporary hour column\naccidents['Daytime'] = accidents['Hour'].apply(when_was_it)\naccidents[['Time', 'Hour', 'Daytime']].tail()","9352bb17":"# drop old time column and temporary hour column\naccidents = accidents.drop(columns=['Time', 'Hour'])","64fdf6c4":"# define labels by accessing look up dictionary above\nlabels = tuple(daytime_groups.values())\n\n# plot total no. of accidents by daytime\naccidents.groupby('Daytime').size().plot(kind='bar', color='lightsteelblue', figsize=(12,5), grid=True)\nplt.xticks(np.arange(5), labels, rotation='horizontal')\nplt.xlabel(''), plt.ylabel('Count\\n')\nplt.title('\\nTotal Number of Accidents by Daytime\\n', fontweight='bold')\nsns.despine(top=True, right=True, left=True, bottom=True);","00cc733a":"# plot average no. of casualties by daytime\naccidents.groupby('Daytime')['Number_of_Casualties'].mean().plot(kind='bar', color='slategrey', \n                                                                 figsize=(12,4), grid=False)\nplt.xticks(np.arange(5), labels, rotation='horizontal')\nplt.ylim((1,1.5))\nplt.xlabel(''), plt.ylabel('Average Number of Casualties\\n')\nplt.title('\\nAverage Number of Casualties by Daytime\\n', fontweight='bold')\nsns.despine(top=True, right=True, left=True, bottom=True);","7e1076d7":"print('Proportion of Missing Values in Accidents Table:', \n      round(accidents.isna().sum().sum()\/len(accidents),3), '%')","6ee046b3":"#accidents.isna().sum()","867988e4":"# drop columns we don't need\naccidents = accidents.drop(columns=['Location_Easting_OSGR', 'Location_Northing_OSGR', \n                                    'Longitude', 'Latitude'])\n\n# drop remaining records with NaN's\naccidents = accidents.dropna()\n\n# check if we have no NaN's anymore\naccidents.isna().sum().sum()","73a74719":"# slice columns we want to use\ndf = accidents[['Accident_Index', 'Accident_Severity', 'Number_of_Vehicles', 'Number_of_Casualties', 'Day_of_Week', \n                'Daytime', 'Road_Type', 'Speed_limit', 'Urban_or_Rural_Area', 'LSOA_of_Accident_Location']]\ndf.isna().sum().sum()","5e8a3869":"#df.info()    ","67b139a5":"# cast categorical features - currently stored as string data - to their proper data format\nfor col in ['Accident_Severity', 'Day_of_Week', 'Daytime', 'Road_Type', 'Speed_limit', \n            'Urban_or_Rural_Area', 'LSOA_of_Accident_Location']:\n    df[col] = df[col].astype('category')\n    \n#df.info()","d9f93bc9":"# check road type\ndf.groupby('Road_Type')['Number_of_Casualties'].mean().plot(kind='bar', color='slategrey', \n                                                            figsize=(12,4), grid=False)\nplt.xticks(np.arange(6), \n           ['Roundabout', 'One way street', 'Dual carriageway', 'Single carriageway', 'Slip road', 'Unknown'], \n           rotation='horizontal')\nplt.ylim((1,1.5))\nplt.xlabel(''), plt.ylabel('Average Number of Casualties\\n')\nplt.title('\\nAverage Number of Casualties by Road Type\\n', fontweight='bold')\nsns.despine(top=True, right=True, left=True, bottom=True);","82f1525c":"# check speed limit\ndf.groupby('Speed_limit')['Number_of_Casualties'].mean().plot(kind='bar', color='slategrey', \n                                                              figsize=(15,4), grid=False)\nplt.xticks(np.arange(8), \n           ['None', '10mph', '20mph', '30mph', '40mph', '50mph', '60mph', '70mph'], \n           rotation='horizontal')\nplt.ylim((0.6,1.6))\nplt.xlabel(''), plt.ylabel('Average Number of Casualties\\n')\nplt.title('\\nAverage Number of Casualties by Speed Limit\\n', fontweight='bold')\nsns.despine(top=True, right=True, left=True, bottom=True);","d9f1df72":"# check daytime\ndf.groupby('Day_of_Week')['Number_of_Casualties'].mean().plot(kind='bar', color='slategrey', \n                                                              figsize=(14,4), grid=False)\nplt.xticks(np.arange(7), \n           ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'], \n           rotation='horizontal')\nplt.ylim((1.0,1.6))\nplt.xlabel(''), plt.ylabel('Average Number of Casualties\\n')\nplt.title('\\nAverage Number of Casualties by Weekday\\n', fontweight='bold')\nsns.despine(top=True, right=True, left=True, bottom=True);","f0f5cce0":"# define numerical columns\nnum_cols = ['Number_of_Vehicles', 'Number_of_Casualties']","1c707e90":"# plotting boxplots\nsns.set(style='darkgrid')\nfig, axes = plt.subplots(2,1, figsize=(10,4))\n\nfor ax, col in zip(axes, num_cols):\n    df.boxplot(column=col, grid=False, vert=False, ax=ax)\n    plt.tight_layout();","de5a8fe7":"#df['Number_of_Vehicles'].value_counts().head(10)","d5d8bd48":"#df['Number_of_Casualties'].value_counts().head(20)","9af59675":"# phrasing conditionto cut off extreme outliers\ncondition = (df['Number_of_Vehicles'] < 6) & (df['Number_of_Casualties'] < 9)\n\n# keep only records that meet our condition\ndf = df[condition]\n\n# check\nprint(df['Number_of_Vehicles'].value_counts())","c4ab3424":"print(df['Number_of_Casualties'].value_counts())","857cc2b4":"df.head(2)","1e18e29d":"look_up = pd.read_csv('..\/input\/lsoa-to-msoa-uk\/LSOA_to_MSOA_to_Local_Authority_District_Dec_2017_Lookup.csv')\nlook_up.head(2)","20233a1f":"df_merged = pd.merge(df, look_up[['LSOA11CD', 'LAD17NM']], how='left', \n                     left_on='LSOA_of_Accident_Location', right_on='LSOA11CD')\ndf_merged.head(2)","6dc7f75a":"# drop the key columns, rename the inconveniently named column, ...\n# ... cast it to a categorical datetype, and drop duplicates\ndf_merged = df_merged.drop(columns=['LSOA_of_Accident_Location', 'LSOA11CD'])\\\n                        .rename(columns={'LAD17NM': 'County_of_Accident'})\\\n                            .astype({'County_of_Accident': 'category'})\\\n                                .drop_duplicates()\n\ndf_merged.head(2)","d616c3c7":"df_merged.shape","41394d6e":"#df_merged.groupby('County_of_Accident').size().sort_values(ascending=False).head()","8c5f0f95":"df_plot = df_merged.groupby('County_of_Accident').size().reset_index().rename(columns={0:'Count'})\ndf_plot.head()","04869065":"# define numerical feature column\nnum_col = ['Number_of_Vehicles']\n\n# define categorical feature columns\ncat_cols = ['Accident_Severity', 'Day_of_Week', 'Daytime', 'Road_Type', 'Speed_limit', \n            'Urban_or_Rural_Area', 'County_of_Accident']\n\n# define target column\ntarget_col = ['Number_of_Casualties']\n\ncols = cat_cols + num_cols + target_col\n\n# copy dataframe\ndf_model = df_merged[cols].copy()\ndf_model.shape","1b5a1667":"# create dummy variables from the categorical features\ndummies = pd.get_dummies(df_model[cat_cols], drop_first=True)\ndf_model = pd.concat([df_model[num_cols], df_model[target_col], dummies], axis=1)\ndf_model.shape","306c0b85":"df_model.isna().sum().sum()","dc45fae4":"#df_model.info()","05d9b916":"# define our features \nfeatures = df_model.drop(['Number_of_Casualties'], axis=1)\n\n# define our target\ntarget = df_model[['Number_of_Casualties']]","9028575b":"from sklearn.model_selection import train_test_split\n\n# split our data\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)","e663b5a5":"# import regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# import evaluation tools\nfrom sklearn.model_selection import RandomizedSearchCV","c043bb20":"# create RandomForestRegressor\nforest = RandomForestRegressor(random_state=4, n_jobs=-1)\n\n# train\nforest.fit(X_train, y_train)\n\n# predict\ny_train_preds = forest.predict(X_train)\ny_test_preds  = forest.predict(X_test)\n\n# evaluate\nRMSE = np.sqrt(mean_squared_error(y_test, y_test_preds))\nprint(f\"RMSE: {round(RMSE, 4)}\")\n\nr2 = r2_score(y_test, y_test_preds)\nprint(f\"r2: {round(r2, 4)}\")","732c9943":"# look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(forest.get_params())","48a9d89e":"# create range of candidate numbers of trees in random forest\nn_estimators = [100, 150]\n\n# create range of candidate max. numbers of levels in tree\nmax_depth = [3, 4, 5]\n\n# create range of candidate min. numbers of samples required to split a node\nmin_samples_split = [10, 15, 20]\n\n# create dictionary with hyperparameter options\nhyperparameters = dict(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\nhyperparameters","3280d697":"# create randomized search\n#randomized_search = RandomizedSearchCV(forest, hyperparameters, n_jobs=-1)\n\n# fit randomized search\n#best_model = randomized_search.fit(X_train, y_train)\n\n# view best parameters\n#print(best_model.best_params_)","d10b0e82":"# view best value for specific parameter\n#print(best_model.best_estimator_.get_params()['n_estimators'])","be6a00ac":"# create RandomForestRegressor with best found hyperparameters\nforest = RandomForestRegressor(n_estimators=150, max_depth=5, random_state=4, n_jobs=-1)\n\n# train\nforest.fit(X_train, y_train)\n\n# predict\ny_train_preds = forest.predict(X_train)\ny_test_preds  = forest.predict(X_test)\n\n# evaluate\nRMSE = np.sqrt(mean_squared_error(y_test, y_test_preds))\nprint(f\"RMSE: {round(RMSE, 4)}\")\n\nr2 = r2_score(y_test, y_test_preds)\nprint(f\"r2: {round(r2, 4)}\")","bdba9242":"# plot the important features\nfeat_importances = pd.Series(forest.feature_importances_, index=features.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with Random Forest');","175947ed":"*Binning Categorical Features*\n\nWhat is `LSOA_of_Accident_Location`? \n\n- A Lower Layer Super Output Area (LSOA) is a GEOGRAPHIC AREA. Lower Layer Super Output Areas are a geographic hierarchy designed to improve the reporting of small area statistics in England and Wales.\n\n- Lower Layer Super Output Areas are built from groups of contiguous Output Areas and have been automatically generated to be as consistent in population size as possible, and typically contain from four to six Output Areas. The Minimum population is 1000 and the mean is 1500.\n\n- There is a Lower Layer Super Output Area for each POSTCODE in England and Wales. A pseudo code is available for Scotland, Northern Ireland, Channel Islands and the Isle of Man.\n\nLocation might be a good predictor for the number of casualties - but not on such a granular level. We would need to aggregate location to bigger areas. The look up table I needed to convert the LSOA to MSOA can be found [here](https:\/\/geoportal.statistics.gov.uk\/datasets\/output-area-to-lsoa-to-msoa-to-local-authority-district-december-2017-lookup-with-area-classifications-in-great-britain).","d3324098":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n#### 2.3. Preparing Dataframe\n<a id='2.3. Preparing Dataframe'><\/a>","525c1af0":"*Handling Outliers*","40992b70":"**Accidents DataFrame**","4a66f72d":"*Binning Numerical Features*\n\n... not applicable ...","e6a9d3fc":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n#### 3.2. Training and Evaluating Random Forest Regressor\n<a id='3.2. Training and Evaluating Random Forest Regressor'><\/a>","187b7f0f":"*Detecting Outliers*","794486c1":"#### 3.1. Train-Test-Split\n<a id='3.1. Train-Test-Split'><\/a>","f43b7a56":"Please use at the [data dictionary](https:\/\/github.com\/BrittaInData\/Road-Safety-UK\/blob\/master\/data\/Road-Accident-Safety-Data-Guide.xls) to know what kind of information we have.","f3a08ec8":"We had our `Date` columnwith values not properly stored in the correct format. Let's do this now:","ae268555":"#### 2.2. Handling Missing Values\n<a id='2.2. Handling Missing Values'><\/a>","66c451a1":"#### 2.1. Handling Date and Time\n<a id='2.1. Handling Date and Time'><\/a>","a484111c":"*Encoding Categorical Features*","d74ff020":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n### 3. Modeling the Data\n<a id='3. Modeling the Data'><\/a>","4f454946":"Next, let's define a new column that groups the `Time` the accidents happened into one of five options:\n- Morning Rush from 5am to 10am --> value 1\n- Office Hours from 10am to 3pm (or: 10:00 - 15:00) --> value 2\n- Afternoon Rush from 3pm to 7pm (or: 15:00 - 19:00) --> value 3\n- Evening from 7pm to 11pm (or: 19:00 - 23:00) --> value 4\n- Night from 11pm to 5am (or: 23:00 - 05:00) --> value 5","2e33c60d":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n### 2. Preprocessing the Data\n<a id='2. Preprocessing the Data'><\/a>","c87b65b0":"# Road Safety Data for the UK","77918a7b":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n#### 2.4. Handling Numerical Data \n<a id='2.4. Handling Numerical Data'><\/a>","7ecc82ce":"*Feature Scaling*\n\n... not applicable ...\n\n(Tree based models, which we will use here later, are not distance based models and can handle varying ranges of features. Therefore scaling is not required.)","3a326ed2":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n#### 2.5. Handling Categorical Data\n<a id='2.5. Handling Categorical Data'><\/a>","bb454c46":"### 1. Obtaining the Data\n<a id='1. Obtaining the Data'><\/a>","1834a9d9":"To aggregate our accidents locations to counties, let's merge our dataframe with the look up table. The counties here are stored in the `LSOA11NM` column.\n\nThe *keys* to combine both dataframes are `LSOA_of_Accident_Location` in our dataframe and `LSOA11CD` in our look up table. Both contain the the LSOA location for each accident:","3476701a":"# Table of Contents\n<a id='Table of Contents'><\/a>\n\n### <a href='#1. Obtaining the Data'>1. Obtaining the Data<\/a>\n\n### <a href='#2. Preprocessing the Data'>2. Preprocessing the Data<\/a>\n\n* <a href='#2.1. Handling Date and Time'>2.1. Handling Date and Time<\/a>\n* <a href='#2.2. Handling Missing Values'>2.2. Handling Missing Values<\/a>\n* <a href='#2.3. Preparing Dataframe'>2.3. Preparing Dataframe<\/a>\n* <a href='#2.4. Handling Numerical Data'>2.4. Handling Numerical Data<\/a>\n* <a href='#2.5. Handling Categorical Data'>2.5. Handling Categorical Data<\/a>\n\n### <a href='#3. Modeling the Data'>3. Modeling the Data<\/a>\n\n* <a href='#3.1. Train-Test-Split'>3.1. Train-Test-Split<\/a>\n* <a href='#3.2. Training and Evaluating Random Forest Regressor'>3.2. Training and Evaluating Random Forest Regressor<\/a>"}}