{"cell_type":{"f8374d55":"code","a29ee9f6":"code","828b57c4":"code","7209f265":"code","f664cee6":"code","8001ae10":"code","6e4721f1":"code","27ac0031":"code","725f9ae0":"code","2354e4e6":"code","8a4a1bcf":"code","576a6dd8":"code","d53de6e3":"code","386220a0":"code","d046bc34":"code","bdec26e0":"code","879239c8":"code","aee276f0":"code","da023395":"code","eafd3b3c":"code","0ef2bce3":"code","10593dec":"code","97d70e3f":"code","7fcd8f9e":"code","dd95b4cd":"code","9d696b92":"code","2ff90418":"code","7e98b3ac":"code","440c1c09":"code","10bf62b7":"code","d190acdf":"markdown","89e48eee":"markdown","292dd990":"markdown","82a9ffd2":"markdown","7b41fb0b":"markdown","d6032c30":"markdown","f3d6a1e6":"markdown","d6fdceb9":"markdown","99d43fcb":"markdown","e14aaa4c":"markdown","be6a57cc":"markdown","4a097810":"markdown","86917b76":"markdown","7bb10326":"markdown","13e24e69":"markdown","09ec4e3e":"markdown","6cdaa6f8":"markdown","7016b3f8":"markdown"},"source":{"f8374d55":"# Importing the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt \nfrom keras.utils import to_categorical           # Library for One Hot Encoding\nimport keras.preprocessing.image as img\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","a29ee9f6":"df_training = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ndf_testing = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\n\n# Second way to load the data\n# fashion_mnist_data = tf.keras.datasets.fashion_mnist_data           \n# (training_images, training_labels), (testing_images, testing_labels) = fashion_mnist_data.load_data()","828b57c4":"training_images = np.array(df_training.iloc[0:,1:])\ntraining_images = training_images.reshape(len(training_images), 28,28)        # Reshaping the pictures\ntraining_images = training_images.astype('float32')\ntraining_labels = np.array(df_training.iloc[:,0])\ntraining_labels = to_categorical(training_labels)                # One hot encoded for labels\n\ntesting_images = np.array(df_testing.iloc[0:,1:])\ntesting_images = testing_images.reshape(len(testing_images), 28,28)\ntesting_images = testing_images.astype('float32')\ntesting_labels = df_testing.iloc[:,0]\n\ntest_for_pred = testing_images              # For testing we copied the testing images and labels\ntest_labels_for_pred = testing_labels\ntesting_labels = to_categorical(testing_labels)","7209f265":"class_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","f664cee6":"training_images.shape","8001ae10":"fig, ax = plt.subplots(10,5, figsize = (10,25))\nfor i,c_name in enumerate(class_names):\n    temp = df_training[df_training.iloc[:,0] == i].head(5)\n    for k in range(5):\n        ax[i,k].imshow(np.array(temp.iloc[k,1:]).reshape(28,28))\n        ax[i,k].axis('off')\n        ax[i,k].grid(False)\n        ax[i,k].title.set_text(str(c_name)) ","6e4721f1":"pixel_img = img.load_img('..\/input\/pixel-pictures\/training_images1.PNG')\nnp.set_printoptions(linewidth = 200)\nfig, ax = plt.subplots(1,2, figsize = (20,8))\nax[0].imshow(training_images[1], aspect = 'auto')\nax[1].imshow(pixel_img,aspect = 'auto')","27ac0031":"training_images = np.expand_dims(training_images, axis = 3)\ntesting_images = np.expand_dims(testing_images, axis = 3)\n\ntraining_images = training_images \/ 255.0\ntesting_images = testing_images \/ 255.0","725f9ae0":"print(training_images.shape)\nprint(training_labels.shape)","2354e4e6":"# Image Augmentation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntraining_datagen = ImageDataGenerator(zoom_range=0.1,\n                                      shear_range = 0.1,\n                                      rotation_range = 0.1,\n                                      horizontal_flip=True,\n                                      fill_mode = 'nearest')\n\ntraining_datagen.fit(training_images)\n","8a4a1bcf":"model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32,(3,3),activation='relu', padding= 'same', input_shape=(28, 28, 1)),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n                                    tf.keras.layers.Dropout(0.20),\n                                    tf.keras.layers.Conv2D(64,(3,3),activation='relu', padding= 'same'),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n                                    tf.keras.layers.Dropout(0.25),\n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dense(256, activation = 'relu'),\n                                    tf.keras.layers.Dropout(0.2),\n                                    tf.keras.layers.Dense(10 , activation = 'softmax')])\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","576a6dd8":"history = model.fit(training_datagen.flow(training_images, training_labels), verbose = 1, epochs = 30, batch_size = 50, steps_per_epoch= len(training_images)\/ 50, validation_data= (testing_images, testing_labels))","d53de6e3":"model.summary()","386220a0":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(30)\n\nplt.plot(epochs, acc, 'r', label = 'training accuracy')\nplt.plot(epochs, val_acc, 'b', label = 'validation accuracy')\nplt.title('Training vs Validation Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, 'r', label = 'Training Loss')\nplt.plot(epochs , val_loss, 'b', label = 'Validation Loss')\nplt.title('Training vs Validation Loss')\nplt.legend()\nplt.show()","d046bc34":"test_for_pred = np.expand_dims(test_for_pred, axis = 3)\ntest_predictions = model.predict_classes(test_for_pred)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_labels_for_pred, test_predictions, target_names = class_names))","bdec26e0":"pic_names = os.listdir('..\/input\/test-object-pics')   \nprint(pic_names[0:2])","879239c8":"real_images = []\nfig, ax = plt.subplots(6,6, figsize = (15,15))\ni = 0\nk = 0\nfor f in pic_names:\n    path = '..\/input\/test-object-pics\/'\n    image = img.load_img(path + f)     \n    if i>5:\n        k+=1\n        i=0\n    ax[i,k].imshow(image)\n    ax[i,k].grid(False)\n    ax[i,k].axis('off')\n    i+=1","aee276f0":"import keras.preprocessing.image as img\nimages = []\nfor f in pic_names:\n    path = '..\/input\/test-object-pics\/'\n    image = img.load_img(path + f, grayscale=True, target_size=(28,28))      # Convert into grayscle and 28x28\n    x = img.img_to_array(image)\n    images.append(x.reshape(28,28))\n","da023395":"obj_images = np.expand_dims(images, axis = 3)\nobj_images = obj_images.astype('float32')\nobj_images = obj_images \/ 255.0\npredicted_images = model.predict(obj_images)\n\nfig, ax = plt.subplots(6,6, figsize = (15,15))\npic = 0\n\nfor i in range(6):\n  for k in range(6):\n    ax[i,k].imshow(images[pic])\n    #ax[i,k].add_subplot(gs[i,k])\n    ax[i,k].axis('off')\n    ax[i,k].grid(False)\n    file_name = pic_names[pic].split('.')[0]\n    ax[i,k].title.set_text(f'Image file: {file_name} \\n predicted: {class_names[np.argmax(predicted_images[pic])]}')\n    pic +=1 \nplt.tight_layout()\nplt.show()","eafd3b3c":"true_pred = 0\nfor i,img in enumerate(predicted_images):\n    pred = class_names[np.argmax(img)]\n    pred = list(pred)[0].lower() + list(pred)[1].lower()\n    out = list(pic_names[i])[0].lower() + list(pic_names[i])[1].lower()\n    if pred == out:\n        true_pred += 1\nprint(f'{round((true_pred\/len(predicted_images))*100,2)} of the real pictures are predicted correctly.')","0ef2bce3":"#The real pictures' background is converted to 0.\nfor pic in range(len(images)):\n  for i in range(28):\n    for k in range(28):\n      if images[pic][i][k] >= 235:\n        images[pic][i][k] = 0\n      else:\n        images[pic][i][k] =images[pic][i][k]","10593dec":"import keras.preprocessing.image as img\npixel_img2 = img.load_img('..\/input\/pixel-pictures\/test_image_values.PNG')\nconverted_pixel_img2 = img.load_img('..\/input\/pixel-pictures\/converted_test_image_values.PNG')\n\nfig, ax = plt.subplots(1,2, figsize = (20,8))\nax[0].imshow(pixel_img2,aspect = 'auto')\nax[1].imshow(converted_pixel_img2, aspect = 'auto')","97d70e3f":"import matplotlib.gridspec as gridspec\nobj_images = np.expand_dims(images, axis = 3)\nobj_images = obj_images.astype('float32')\nobj_images = obj_images \/ 255.0\npredicted_images1 = model.predict(obj_images)\n\nfig, ax = plt.subplots(6,6, figsize = (15,15))\npic = 0\nfor i in range(6):\n  for k in range(6):\n    ax[i,k].imshow(images[pic])\n    ax[i,k].axis('off')\n    ax[i,k].grid(False)\n    file_name = pic_names[pic].split('.')[0]\n    ax[i,k].title.set_text(f'Image file: {file_name} \\n predicted: {class_names[np.argmax(predicted_images1[pic])]}')\n    pic +=1 \nplt.tight_layout()\nplt.show()","7fcd8f9e":"# Correct prediction rate\ntrue_pred = 0\nfor i,img in enumerate(predicted_images1):\n    pred = class_names[np.argmax(img)]\n    pred = list(pred)[0].lower() + list(pred)[1].lower()\n    out = list(pic_names[i])[0].lower() + list(pic_names[i])[1].lower()\n    if pred == out:\n        true_pred += 1\nprint(f'{round((true_pred\/len(predicted_images1))*100,2)} of the real pictures are predicted correctly.')\n    ","dd95b4cd":"pic = 0\nfig, ax = plt.subplots(4,4, figsize =(30,15))\nfor i in range(4):\n  for k in range(4):\n    if k % 2 == 0:\n      ax[i,k].imshow(images[pic])\n      ax[i,k].axis('off')\n      ax[i,k].grid(False)\n    elif k % 2 == 1:\n      ax[i,k].bar(class_names, predicted_images1[pic-1]* 100 )\n      ax[i,k].tick_params(rotation =45)\n      for a,p in enumerate(ax[i,k].patches):\n        if p.get_height() > 12:\n            \n            ax[i,k].annotate(f\"{class_names[a]} - \"+format( p.get_height(),'.2f') + \"%\", (p.get_x() + p.get_width() \/ 2., p.get_height()), bbox=dict(boxstyle=\"round\", alpha=0.2),size = 20, ha = 'center', va = 'bottom', xytext = (15,15), textcoords = 'offset points')\n    pic +=1\n    \nplt.tight_layout()","9d696b92":"model2 = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32,(3,3),activation='relu', padding= 'same', input_shape=(28, 28, 1)),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n                                    tf.keras.layers.Dropout(0.20),\n                                    tf.keras.layers.Conv2D(64,(3,3),activation='relu', padding= 'same'),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n                                    tf.keras.layers.Dropout(0.25),\n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dense(256, activation = 'relu'),\n                                    tf.keras.layers.Dropout(0.4),\n                                    tf.keras.layers.Dense(10 , activation = 'softmax')])\n\nmodel2.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nhistory = model2.fit(training_images, training_labels, verbose = 1, epochs = 30, batch_size = 50, steps_per_epoch= len(training_images)\/ 50, validation_data= (testing_images, testing_labels))\n","2ff90418":"test_predictions = model.predict_classes(test_for_pred)\nfrom sklearn.metrics import classification_report\nprint('Model 1 with augmentation\\n')\nprint(classification_report(test_labels_for_pred, test_predictions, target_names = class_names))\n\ntest_predictions2 = model2.predict_classes(test_for_pred)\nfrom sklearn.metrics import classification_report\nprint('Model 2 without augmentation\\n')\nprint(classification_report(test_labels_for_pred, test_predictions2, target_names = class_names))","7e98b3ac":"import matplotlib.gridspec as gridspec\nobj_images = np.expand_dims(images, axis = 3)\nobj_images = obj_images.astype('float32')\nobj_images = obj_images \/ 255.0\npredicted_images2 = model2.predict(obj_images)\n\nfig, ax = plt.subplots(6,6, figsize = (15,15))\npic = 0\n\nfor i in range(6):\n  for k in range(6):\n    ax[i,k].imshow(images[pic])\n    #ax[i,k].add_subplot(gs[i,k])\n    ax[i,k].axis('off')\n    ax[i,k].grid(False)\n    file_name = pic_names[pic].split('.')[0]\n    ax[i,k].title.set_text(f'Image file: {file_name} \\n predicted: {class_names[np.argmax(predicted_images2[pic])]}')\n    pic +=1 \nplt.tight_layout()\nplt.show()","440c1c09":"true_pred = 0\nfor i,img in enumerate(predicted_images2):\n    pred = class_names[np.argmax(img)]\n    pred = list(pred)[0].lower() + list(pred)[1].lower()\n    out = list(pic_names[i])[0].lower() + list(pic_names[i])[1].lower()\n    if pred == out:\n        true_pred += 1\nprint(f'{round((true_pred\/len(predicted_images2))*100,2)} of the real pictures are predicted correctly.')\n    ","10bf62b7":"pic = 0\nfig, ax = plt.subplots(4,4, figsize =(30,15))\nfor i in range(4):\n  for k in range(4):\n    if k % 2 == 0:\n      ax[i,k].imshow(images[pic])\n      ax[i,k].axis('off')\n      ax[i,k].grid(False)\n    elif k % 2 == 1:\n      ax[i,k].bar(class_names, predicted_images2[pic-1]* 100 )\n      ax[i,k].tick_params(rotation =45)\n      for a,p in enumerate(ax[i,k].patches):\n\n        if p.get_height() > 12:\n            \n            ax[i,k].annotate(f\"{class_names[a]} - \"+format( p.get_height(),'.2f') + \"%\", (p.get_x() + p.get_width() \/ 2., p.get_height()), bbox=dict(boxstyle=\"round\", alpha=0.2),size = 20, ha = 'left', va = 'bottom', xytext = (15,-5), textcoords = 'offset points')\n    pic +=1\n    \nplt.tight_layout()","d190acdf":"After I looked the pictures closely, I realized that all the training pictures has black background. Howeveer, all real pictures in the fashion sector have white background.\n\nSo, all background pixel values are around 250 instead of 0.","89e48eee":"# Model 1 Training with Image Augmentation","292dd990":"# Model Comparasion\nThe overall accuracy is higher in the model without image augmentation.\n\nHowever, model with augmentation predict the real objects better because the capability of the distinguishing classes improved for similar products with image augmentation.","82a9ffd2":"# Model Evaluation","7b41fb0b":"# Conclusion\n* The fashion dataset pictures have black background, so the model is not able to predict real objects.\n* Just around 10% of the real pictures are predicted correctly the pictures because of white bacground of pictures.\n\nAfter processed the pictures:\n\n* The dataset has trained two times with img augmentation and without.\n* Even though the model without img augmentation performs better, in the real images dataset \n* Every time Model 1 (with img augmentation) is performed about 10% better than model 2 (without img augmentation) in real images dataset.\n* Therefore, model with augmentation predicts the real objects better because the capability of the distinguishing classes improved for similar products thanks to image augmentation.","d6032c30":"# **Import the Dataset**\n\nAlthough Kaggle has already provided the data, the data can be loaded by tensorflow.","f3d6a1e6":"The pictures are predicted as is, but the results were disaster. Almost all of them were predicted as a bag.","d6fdceb9":"# Prepraing the Data\nThe data distinguished from label and converted into numpy array.\n\nEach row reshaped from vector(784 pixels) to 28x28 resolution and reformatted as 'float32'.\n\nLabels have converted into vectors by One Hot Encoding.","99d43fcb":"The one more dimension should be added for color. Since the pictures are grayscale, 3rd dimension depth is 1. If the images were colorful, depth would be 3.","e14aaa4c":"The 28x28 pixels array and images representation.","be6a57cc":"# The Images Classes\n* 0 - T-shirt\/top \n* 1 - Trouser\n* 2 - Pullover\n* 3 - Dress\n* 4 - Coat\n* 5 - Sandal\n* 6 - Shirt\n* 7 - Sneaker\n* 8 - Bag\n* 9 - Ankle boot","4a097810":" # Model 2 Training without Img Augmentation","86917b76":"# Loading the Real Pictures\n* The pictures found from random webpages and loaded.\n* These pictures are loaded by keras image method.\n* It also helps to convert real colorful different images to grayscale and resize them into 28x28 resolution. ","7bb10326":"# **Data Description**\n* This dataset provided by zalandoresearch.\n* The dataset consists of fashion objects' grayscale images and converted into numbers.\n* The training dataset has 785 columns and 60000 rows and test dataset has same number of columns and 10000 rows.\n* First column is labels that provide class information.\n* The rest of the 784 columns contain pixel values. \n* The pictures are suppose to be 28x28 resulation. So, it is gonna be reshaped into this resolution.\n\n\n","13e24e69":"The name list of real object images","09ec4e3e":"# Example of Images","6cdaa6f8":"# Real pictures Prediction 1\n* Converting real colorful different images to grayscale and resizing them into 28x28 resolution. ","7016b3f8":"# Real Pictures Prediction 2\nThe prediction after pictures' background converted."}}