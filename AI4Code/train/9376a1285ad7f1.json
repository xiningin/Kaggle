{"cell_type":{"6c959609":"code","7ebceca3":"code","5414ce49":"code","90e40886":"code","46e51e50":"code","f118005e":"code","29b80187":"code","cc36d2f1":"code","5cdfd95f":"code","9bfec431":"code","c1a13f76":"code","e02a656c":"code","5023b0c1":"code","c6ceb10f":"code","021acc11":"code","13f6b564":"code","fe9f0c66":"code","a5115bf1":"code","de4d0092":"code","89a490dd":"code","695f70d5":"code","ffaf1a17":"code","81d30ab3":"code","2f625cb4":"code","f0f96713":"code","569faf63":"code","d86acd1e":"code","7ebc0357":"code","9e36592a":"code","4ba475b4":"code","81fa403b":"code","b591a43b":"markdown","b10d0d43":"markdown","c524c281":"markdown","1bc89659":"markdown","4672f429":"markdown","50dfe818":"markdown","f35df0b3":"markdown","373b6e20":"markdown","ac604a8a":"markdown","078f70c0":"markdown","fd6b8c71":"markdown","c2700687":"markdown","fe8c84ac":"markdown","c552b5a1":"markdown","99c69798":"markdown","732e3cd0":"markdown"},"source":{"6c959609":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf","7ebceca3":"#  We instantiate a variable, x, holding the value 4 - a simple scalar\nx = tf.Variable(4.0)\n\n# Calculating y is equal to x square can be thought of as the forward pass through our neural network.\n# The GradientTape records all operations that occur in the forward pass, and these operations are played back to compute gradients. \nwith tf.GradientTape() as tape:\n    y = x**2","5414ce49":"# we have the value of y here, 16, that is 4 squared.\ny","90e40886":"# The instance of the GradientTape that we had set up has recorded the forward pass operations, tape.gradient can now calculate gradients\n# Tape.gradient y, x will calculate the gradient of y with respect to its input x. The gradient of y with respect to x is essentially the\n# derivative of y with respect to x, when x is changed by an infinitesimal amount by how much y changes.\ndy_dx = tape.gradient(y, x)\n\ndy_dx","46e51e50":"# Gradients in TensorFlow can be calculated not only with\n# respect to scalars, but with respect to tensors as well\n# We instantiate a variable w with 4 by 2 backing tensor initialized using random normal distribution\nw = tf.Variable(tf.random.normal((4, 2)))\n\nw","f118005e":"# Here is another variable b initialized as a 1\u2011dimensional tensor of 1's.\nb = tf.Variable(tf.ones(2, dtype=tf.float32))\n\nb","29b80187":"# here is a 3rd variable x that is initialized here using a 1\u2011dimensional tensor.\nx = tf.Variable([[10., 20., 30., 40.]], dtype=tf.float32) \n\nx","cc36d2f1":"# In order to be able to invoke tape.gradiant multiple times, \n# you need to instance the GradientTape with persistent equal to true\nwith tf.GradientTape(persistent=True) as tape:\n    # we perform a matrix multiplication (matmul) operation\n    y = tf.matmul(x, w) + b\n    # we calculate a loss using the tf.reduce_mean function\n    loss = tf.reduce_mean(y**2)","5cdfd95f":"[dl_dw, dl_db] = tape.gradient(loss, [w, b])","9bfec431":"#  the shape of the gradient is the same as the shape of the original tensor, w\ndl_dw","c1a13f76":"# Similarly, if you look at the gradients of the loss\n#  that we have computed with respect to the biases b, the shape of the gradients is exactly the same as the shape of the bias vector.\ndl_db","e02a656c":"# When you use Keras layers to build up your neural network\n#  model, the GradientTape automatically records all operations made in the forward pass of the neural network\n\n# We instantiated a Keras dense layer with 2 neurons. \n# The input that I'll pass into this layer is the tensor x.\nlayer = tf.keras.layers.Dense(2, activation='relu')\n\nx = tf.constant([[10., 20., 30.]])","5023b0c1":"# We instantiate the GradientTape, pass the input x through the layer, \n# get the result in y, and then calculate some loss using tf.reduce_sum\nwith tf.GradientTape() as tape:\n    y = layer(x)\n    \n    loss = tf.reduce_sum(y**2)\n\n# We then use tape.gradient to calculate the gradients of the loss with\n# respect to all trainable parameters in our layer\n# The trainable parameters in a layer are the weights and biases of the\n# neurons in that layer. \ngrad = tape.gradient(loss, layer.trainable_variables)","c6ceb10f":"# Gradient is calculated with respect to all weights and biases.\ngrad","021acc11":"for var, g in zip(layer.trainable_variables, grad):\n    print(f'{var.name}, shape: {g.shape}')","13f6b564":"# variable, x1 below is a trainable variable by default\nx1 = tf.Variable(5.0)\n\nx1","fe9f0c66":"# instantiate yet another variable, x2, using tf.Variable, \n# but explicitly specified, trainable=False\nx2 = tf.Variable(5.0, trainable=False)\n\nx2","a5115bf1":"x3 = tf.add(x1, x2)\n\nx3","de4d0092":"x4 = tf.constant(5.0)\n\nx4","89a490dd":"with tf.GradientTape() as tape:\n    y = (x1**2) + (x2**2) + (x3**2) + (x4**2)\n\ngrad = tape.gradient(y, [x1, x2, x3, x4])\n\ngrad","695f70d5":"x1 = tf.constant(5.0)\n\nx2 = tf.Variable(3.0)","ffaf1a17":"with tf.GradientTape() as tape:\n    tape.watch(x1)\n    \n    y = (x1**2) + (x2**2)","81d30ab3":"[dy_dx1, dy_dx2] = tape.gradient(y, [x1, x2])\n\ndy_dx1, dy_dx2","2f625cb4":"with tf.GradientTape(watch_accessed_variables=False) as tape:\n    tape.watch(x1)\n    \n    y = (x1**2) + (x2**2)","f0f96713":"[dy_dx1, dy_dx2] = tape.gradient(y, [x1, x2])\n\ndy_dx1, dy_dx2","569faf63":"x = tf.constant(1.0)\nx1 = tf.Variable(5.0)\nx2 = tf.Variable(3.0)","d86acd1e":"with tf.GradientTape(persistent=True) as tape:\n    tape.watch(x)\n\n    if x > 0.0:\n        result = x1**2\n    else:\n        result = x2**2 \n\ndx1, dx2 = tape.gradient(result, [x1, x2])\n\ndx1, dx2","7ebc0357":"x = tf.constant(-1.0)\nx1 = tf.Variable(5.0)\nx2 = tf.Variable(3.0)","9e36592a":"with tf.GradientTape(persistent=True) as tape:\n    tape.watch(x)\n\n    if x > 0.0:\n        result = x1**2\n    else:\n        result = x2**2 \n\ndx1, dx2 = tape.gradient(result, [x1, x2])\n\ndx1, dx2","4ba475b4":"x = tf.Variable(2.)\ny = tf.Variable(3.)\n\nwith tf.GradientTape() as tape:\n    z = y * y\n\n    dy_dx = tape.gradient(z, x)\n    \nprint(dy_dx)","81fa403b":"#End of Code","b591a43b":"Similar to logic in the previous block\nIf x is greater than 0, I calculate x1 squared. If x is less than 0, calculate x2 squared instead.\nBut now x < 0, only x2 squared is calculated in the if block.\n\nThe gradient with respect to x1 is none.\nThere is no gradient with respect to x1 because x1 was not part of the computation, but there is a gradient with respect to x2, the tensor with numpy value 6.","b10d0d43":"### Computing the gradient of a result with respect to a variable that is not part of the computation, gradient will be none.\n\nWe instantiate two variables x and y. \n\nZ is equal to y squared. \n\nZ here is computed only using the variable y. When we compute the gradient of z with respect to x, which is not part of the computation, we get none","c524c281":"When we instantiate the GradientTape, and use tape.watch(x1) to explicitly watch the tensor x1. Tape.watch(x1) will ensure that the tensor x1 is also tracked by our GradientTape","1bc89659":"Trainable variable, the value associated with this will be updated during the training process","4672f429":"The gradient with respect to each source has the shape of the source","50dfe818":"tape.watch allows you to explicitly configure what values you're tracking within your computation, and you can then calculate gradients with respect to those values\n\nGradientTape, by default, automatically tracks all variables that are involved in your computation. If you want to turn off this default behavior and you want to explicitly control what exactly your GradientTape watches, instantiate the GradientTape with watch_accessed_variables=False.","f35df0b3":"### The GradientTape only tracks those operations that are actually performed\n\nWhen we invoke tape.gradient below to calculate the gradient of y with respect to x1 and x2.\n\nThe gradient of y with respect to x2 is none. Even though x2 is a variable, the GradientTape did not automatically track x2. \n\nIf x is greater than 0, I calculate x1 squared. If x is less than 0, calculate x2 squared instead.\nSince x = 1, only x1 squared is calculated in the if block below.\nx2 was not part of the computation at all, so there are no gradients with respect to x2","373b6e20":"https:\/\/www.tensorflow.org\/guide\/autodiff","ac604a8a":"### Computing gradients\n\nTo differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients.","078f70c0":"Tensor, not a variable. Gradients are not calculated on Tensors","fd6b8c71":"When we calculate gradients of y with respect to x1, x2, we get values for both","c2700687":"Trainable has been explicitly set to false","fe8c84ac":"### Watch constants to calculate gradients with respect to them\n\ntf.GradientTape provides hooks that give the user control over what is or is not watched. To record gradients with respect to a tf.Tensor, you need to call GradientTape.watch(x)","c552b5a1":"### Gradient tape records operations as they occur\n\nConditionals are naturally handled. The gradient only connects to the variable that was used.","99c69798":"After the gradients are calculated, you'll find a value only for x1 (10). That is the first tensor in this resulting array. The gradient of y with respect to x2, x3, and x4 are all none.\n\nThis just demonstrates that GradientTape does not watch non\u2011trainable variables and tensors by default. It only watches trainable variables","732e3cd0":"### Gradients are calculated only with respect to trainable variables\n\nThe GradientTape in TensorFlow watches only trainable variables by default, tensors, constants, any non\u2011trainable variables that you have within your computation are not automatically tracked using GradientTape, which means you cannot automatically calculate gradients with respect to them"}}