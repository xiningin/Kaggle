{"cell_type":{"50c8ba63":"code","28729fb2":"code","a4414535":"code","65b530e4":"code","bb97b3aa":"code","881d2c6c":"code","7f7fc34f":"code","d7f51f86":"code","c0f5b215":"code","5903574b":"code","7d1ab71f":"code","1f8a32d8":"code","5b003422":"code","4c55653c":"code","95a7c011":"code","6c6bcedf":"code","4cee588d":"code","a87a7100":"code","e91a5723":"code","24c4ba6f":"markdown","5e53832e":"markdown","8b6816e7":"markdown","e5c50c6c":"markdown","20ba3875":"markdown","47a518da":"markdown","8cc44e94":"markdown"},"source":{"50c8ba63":"import gc\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom torch import nn\n\nfrom pathlib import Path\nfrom sklearn.model_selection import KFold\nfrom transformers.file_utils import ModelOutput\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AdamW, AutoTokenizer, AutoModel\nfrom transformers import BertModel, BertPreTrainedModel, RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel","28729fb2":"COMPETITION_DATA_PATH = Path('..\/input\/commonlitreadabilityprize')\nTRAIN_DATA_PATH = COMPETITION_DATA_PATH \/ 'train.csv'\nTEST_DATA_PATH = COMPETITION_DATA_PATH \/ 'test.csv'","a4414535":"class TrainingDataset(Dataset):\n    def __init__(self, text_excerpts, targets):\n        self.text_excerpts = text_excerpts\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt': self.text_excerpts[idx],\n                  'target': self.targets[idx]}\n        return sample\n    \nclass PredictionDataset(Dataset):\n    def __init__(self, text_excerpts):\n        self.text_excerpts = text_excerpts\n        \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt': self.text_excerpts[idx]}\n        return sample","65b530e4":"def transform_targets(targets):\n    targets = targets.astype(np.float32).reshape(-1, 1)\n    return targets","bb97b3aa":"def create_training_dataloader(data, batch_size, shuffle):\n    text_excerpts = data['excerpt'].tolist()\n    targets = transform_targets(data['target'].to_numpy())\n    dataset = TrainingDataset(text_excerpts=text_excerpts, targets=targets)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n    return dataloader","881d2c6c":"def create_prediction_dataloader(data, batch_size):\n    text_excerpts = data['excerpt'].tolist()\n    dataset = PredictionDataset(text_excerpts=text_excerpts)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    return dataloader","7f7fc34f":"def split_into_kfolds(data, n_splits, shuffle, random_state):\n    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n    for train_indices, valid_indices in kf.split(data):\n        yield data.iloc[train_indices], data.iloc[valid_indices]","d7f51f86":"class Metric:\n    def __init__(self):\n        self.sse = 0\n        self.num_samples = 0\n    \n    def update(self, targets, predictions):\n        self.sse += np.sum(np.square(targets - predictions))\n        self.num_samples += len(targets)\n    \n    def get_rmse(self):\n        rmse = np.sqrt(self.sse \/ self.num_samples)\n        return rmse","c0f5b215":"class Monitor:\n    def __init__(self, num_patient_epochs):\n        self.num_patient_epochs = num_patient_epochs\n        self.best_epoch_num = None\n        self.best_score = np.inf\n        self.best_model = None\n        \n    def early_stopping(self, current_epoch_num):\n        return True if current_epoch_num > self.best_epoch_num + self.num_patient_epochs else False\n        \n    def update_best_model(self, current_epoch_num, score, model, tokenizer, save_name):\n        if score < self.best_score:\n            self.best_epoch_num = current_epoch_num\n            self.best_score = score\n            self.best_model = model\n            model.save_pretrained(save_name)\n            tokenizer.save_pretrained(save_name)","5903574b":"class KfoldMonitor:\n    def __init__(self):\n        self.fold_monitor = {}\n        \n    def update(self, fold, monitor):\n        self.fold_monitor[fold] = monitor","7d1ab71f":"def train(dataloader, model, tokenizer, optimizer, device):\n    model.train()\n    epoch_loss = 0\n    for batch_num, batch in enumerate(dataloader):\n        # Forward prop\n        inputs = tokenizer(batch['text_excerpt'], padding=True, truncation=True, return_tensors=\"pt\")\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        targets = batch['target'].to(device)\n        outputs = model(**inputs, labels=targets)\n        epoch_loss += outputs.loss.item()\n        # Backprop\n        outputs.loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    average_epoch_loss = epoch_loss \/ len(dataloader)\n    return model, average_epoch_loss","1f8a32d8":"def evaluate(dataloader, model, tokenizer, device):\n    model.eval()\n    epoch_loss = 0\n    metric = Metric()\n    for batch_num, batch in enumerate(dataloader):\n        inputs = tokenizer(batch['text_excerpt'], padding=True, truncation=True, return_tensors=\"pt\")\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        targets = batch['target'].to(device)\n        with torch.no_grad():\n            outputs = model(**inputs, labels=targets)\n        epoch_loss += outputs.loss.item()\n        targets = targets.detach().cpu().numpy()\n        predictions = outputs.logits.detach().cpu().numpy()\n        metric.update(targets=targets, predictions=predictions)\n    average_epoch_loss = epoch_loss \/ len(dataloader)\n    return average_epoch_loss, metric","5b003422":"def predict(dataloader, model, tokenizer, device):\n    model.eval()\n    predictions = []\n    for batch_num, batch in enumerate(dataloader):\n        inputs = tokenizer(batch['text_excerpt'], padding=True, truncation=True, return_tensors=\"pt\")\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        batch_predictions = outputs.logits.detach().cpu().numpy()\n        predictions.append(batch_predictions)\n    predictions = np.vstack(predictions)\n    return predictions","4c55653c":"class RegressorOutput(ModelOutput):\n    loss = None\n    logits = None\n    hidden_states = None\n    attentions = None","95a7c011":"class BertPoolerRegressor(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        \n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self.loss_fct = nn.MSELoss()\n        self.init_weights()\n        \n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n        bert_outputs = self.bert(input_ids=input_ids,\n                                 attention_mask=attention_mask,\n                                 token_type_ids=token_type_ids)\n        pooler_output = bert_outputs['pooler_output']\n        pooler_output = self.dropout(pooler_output)\n        logits = self.regressor(pooler_output)\n        loss = self.loss_fct(labels, logits) if labels is not None else None\n        return RegressorOutput(loss=loss, logits=logits)","6c6bcedf":"class RobertaPoolerRegressor(RobertaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        \n        self.roberta = RobertaModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self.loss_fct = nn.MSELoss()\n        self.init_weights()\n        \n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        bert_outputs = self.roberta(input_ids=input_ids,\n                                    attention_mask=attention_mask)\n        pooler_output = bert_outputs['pooler_output']\n        pooler_output = self.dropout(pooler_output)\n        logits = self.regressor(pooler_output)\n        loss = self.loss_fct(labels, logits) if labels is not None else None\n        return RegressorOutput(loss=loss, logits=logits)","4cee588d":"BATCH_SIZE = 16\nRANDOM_STATE = 41\nSAVE_NAME = Path('roberta-base-pooler-regressor')","a87a7100":"train_data = pd.read_csv(TRAIN_DATA_PATH)\n#### Remove before submission #####\n# train_data = train_data.sort_values(by='excerpt', key=lambda x: x.str.len())[:len(train_data)\/\/4]\n# train_data = train_data[:20]\n###################################\nkfolf_monitor = KfoldMonitor()\nfor fold, (train_data, valid_data) in enumerate(split_into_kfolds(train_data, n_splits=5, shuffle=True, random_state=RANDOM_STATE)):\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(f'Length of train data: {len(train_data)}, valid data: {len(valid_data)}')\n    train_dataloader = create_training_dataloader(data=train_data, batch_size=BATCH_SIZE, shuffle=True)\n    valid_dataloader = create_training_dataloader(data=valid_data, batch_size=BATCH_SIZE * 4, shuffle=False)\n    monitor = Monitor(num_patient_epochs=3)\n    MODEL_PATH = '..\/input\/commonlit-data-download\/roberta-base'\n    num_epochs = 20\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    model = RobertaPoolerRegressor.from_pretrained(MODEL_PATH)\n    model.to(device)\n    optimizer = AdamW(params=model.parameters(), lr=2e-5)\n    for epoch_num in range(num_epochs):\n        model, train_loss = train(train_dataloader, model, tokenizer, optimizer, device)\n        valid_loss, valid_metric = evaluate(valid_dataloader, model, tokenizer, device)\n        monitor.update_best_model(current_epoch_num=epoch_num, score=valid_metric.get_rmse(), model=model,\n                                  tokenizer=tokenizer, save_name=SAVE_NAME\/str(fold))\n        print(f'Epoch num: {epoch_num} Train epoch loss: {train_loss}')\n        print(f'Epoch num: {epoch_num} Valid epoch loss: {valid_loss}, RMSE: {valid_metric.get_rmse()}')\n        if monitor.early_stopping(current_epoch_num=epoch_num):\n            print(f'Exiting at epoch_num {epoch_num} due to early stopping')\n            break\n    kfolf_monitor.update(fold=fold, monitor=monitor)\n    print(2*'--------------------------------------')\n\nmean_cross_validation_score = np.mean([fold_monitor.best_score for fold_monitor in kfolf_monitor.fold_monitor.values()])\nprint(f'Mean cross validation score: {mean_cross_validation_score}')","e91a5723":"test_data = pd.read_csv(TEST_DATA_PATH)\ntest_dataloader = create_prediction_dataloader(test_data, batch_size=4)\nMODEL_PATH = '..\/input\/commonlit-data-download\/roberta-base'\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\npredictions = []\nfor monitor in kfolf_monitor.fold_monitor.values():\n    gc.collect()\n    torch.cuda.empty_cache()\n    model = monitor.best_model\n    model.to(device)\n    fold_predictions = predict(test_dataloader, model, tokenizer, device)\n    predictions.append(fold_predictions)\n    \nmean_predictions = np.mean(np.hstack(predictions), axis=1)\ntest_data['target'] = mean_predictions\ntest_data[['id','target']].to_csv('submission.csv', index=False)","24c4ba6f":"# Define Metric, EarlyStopping, Saver, Monitor","5e53832e":"# Define Dataset and DataLoader","8b6816e7":"# Define Model","e5c50c6c":"# Make submission","20ba3875":"# Experiments","47a518da":"# Objective\nThe objective of this notebook is to set up the easiest and fastest notebook to quickly finetune a transformer model and make a submission. This notebook contains only the bare minimum transparent code necessary with no external trainer functions. Although the notebook is simple, it includes all the components to train a model such as :\n1. Early stopping\n2. Model Saver\n3. Kfold Cross-validation\n\nInference notebook: https:\/\/www.kaggle.com\/vigneshbaskaran\/commonlit-easy-finetuner-inference\n\n# Plan\n1. Define model\n2. Define Dataset and DataLoader\n3. Define training and evaluation loop\n4. Create cross-validation folds\n5. For each fold: Train -> Save best model\n6. Make predictions and submit","8cc44e94":"# Define training, validation and testing loops"}}