{"cell_type":{"dc125908":"code","624fd1c5":"code","aa864476":"code","4c8aeea8":"code","eaa76036":"code","068b2dcd":"code","73db424b":"code","0ed95145":"code","d37ecedd":"code","df5c18b0":"code","cb9ab61a":"code","96905187":"code","67658e33":"code","e4c1c31f":"code","f4458b90":"code","916b906a":"code","b73fdd5e":"code","325310bb":"code","a792b4dd":"code","3370b04f":"code","65a46904":"code","f88d9733":"code","477655ca":"code","b9f127c2":"code","07dd2129":"code","1d93460c":"code","9482e344":"code","14a9557d":"code","4aebd046":"code","f8e4c1c2":"code","d13340a2":"code","5f1785fc":"code","6fb4878f":"code","f7291b60":"code","5c5da4a6":"code","f46fb174":"code","ce48dcb0":"code","31e382f1":"code","46be930b":"code","33c4f8bd":"markdown","d5899c05":"markdown","1cb9f6bd":"markdown","e34f70ae":"markdown","cebaf43d":"markdown","90dda21e":"markdown","7c8019c4":"markdown","a52edd4a":"markdown","4a4c29d1":"markdown","c3c9cb95":"markdown","ec40f44c":"markdown","4a9923e5":"markdown","42c411a3":"markdown","e24091f1":"markdown","3f5601a6":"markdown","ac024648":"markdown","fce4ebb7":"markdown","b68e616e":"markdown","0f553c05":"markdown","8d9dcd37":"markdown","8619a250":"markdown","c192d4f7":"markdown","d508a5e8":"markdown","d5674eec":"markdown","e65b27be":"markdown","b37fbc78":"markdown","893f56cf":"markdown","8c6e20a3":"markdown","d51a9691":"markdown","2ab7985f":"markdown","31e78545":"markdown","aab413e2":"markdown","dd9899ae":"markdown","ba38c1ad":"markdown","3c013c7e":"markdown","df3f8e28":"markdown","4f06735c":"markdown","25bf9f9c":"markdown","fc60aad2":"markdown","c11f9fcc":"markdown","562d885d":"markdown","dcb2ea77":"markdown","cd4f9994":"markdown","81e32c66":"markdown","1aebf9e7":"markdown","6437f457":"markdown","28a4fd62":"markdown","e020462c":"markdown","716a61c6":"markdown","d8e5d091":"markdown","e59ec0eb":"markdown","dbfb6e0a":"markdown","1d81e7f6":"markdown"},"source":{"dc125908":"#Importing the basic librarires\n\nimport os\nimport math\nimport scipy\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn import tree\nfrom scipy.stats import randint\nfrom scipy.stats import loguniform\nfrom IPython.display import display\n\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom scikitplot.metrics import plot_roc_curve as auc_roc\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, \\\nf1_score, roc_auc_score, roc_curve, precision_score, recall_score\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10,6]\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 50)","624fd1c5":"#Importing the dataset\n\ndf = pd.read_csv('..\/input\/horse-survival-dataset\/horse.csv')\ndf.drop(['hospital_number'],axis=1, inplace=True)\n\ntarget = 'outcome'\nlabels = ['died', 'euthanized', 'lived']\nfeatures = [i for i in df.columns.values if i not in [target]]\n\noriginal_df = df.copy(deep=True)\ndisplay(df.head(8))\n\nprint('\\n\\033[1mInference:\\033[0m The Datset consists of {} features & {} samples.'.format(df.shape[1], df.shape[0]))","aa864476":"#Checking the dtypes of all the columns\n\ndf.info()","4c8aeea8":"#Checking number of unique rows in each feature\n\ndf.nunique().sort_values()","eaa76036":"#Checking number of unique rows in each feature\n\nnu = df[features].nunique().sort_values()\nnf = []; cf = []; nnf = 0; ncf = 0; #numerical & categorical features\n\nfor i in range(df[features].shape[1]):\n    if nu.values[i]<=7:cf.append(nu.index[i])\n    else: nf.append(nu.index[i])\n\nprint('\\n\\033[1mInference:\\033[0m The Datset has {} numerical & {} categorical features.'.format(len(nf),len(cf)))","068b2dcd":"#Checking the stats of all the columns\n\ndisplay(df.describe())","73db424b":"#Let us first analyze the distribution of the target variable\n\nMAP={}\nfor e, i in enumerate(sorted(df[target].unique())):\n    MAP[i]=labels[e]\n#MAP={0:'Not-Survived',1:'Survived'}\ndf1 = df.copy()\ndf1[target]=df1[target].map(MAP)\nexplode=np.zeros(len(labels))\nexplode[-1]=0.1\nprint('\\033[1mTarget Variable Distribution'.center(55))\nplt.pie(df1[target].value_counts(), labels=df1[target].value_counts().index, counterclock=False, shadow=True, \n        explode=explode, autopct='%1.1f%%', radius=1, startangle=0)\nplt.show()","0ed95145":"#Visualising the categorical features \n\nprint('\\033[1mVisualising Categorical Features:'.center(100))\n\nn=3\nplt.figure(figsize=[15,3*math.ceil(len(cf)\/n)])\n\nfor i in range(len(cf)):\n    if df[cf[i]].nunique()<=6:\n        plt.subplot(math.ceil(len(cf)\/n),n,i+1)\n        sns.countplot(df[cf[i]])\n    else:\n        plt.subplot(3,1,i-1)\n        sns.countplot(df[cf[i]])\nplt.tight_layout()\nplt.show()","d37ecedd":"#Understanding the feature set\n\nprint('\\033[1mFeatures Distribution'.center(100))\n\nnf = [i for i in features if i not in cf]\n\nplt.figure(figsize=[15,3*math.ceil(len(features)\/3)])\nfor c in range(len(nf)):\n    plt.subplot(math.ceil(len(features)\/3),3,c+1)\n    sns.distplot(df[nf[c]])\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=[15,3*math.ceil(len(features)\/3)])\nfor c in range(len(nf)):\n    plt.subplot(math.ceil(len(features)\/3),3,c+1)\n    df.boxplot(nf[c])\nplt.tight_layout()\nplt.show()","df5c18b0":"#Understanding the relationship between all the features\n\ntry:\n    g=sns.pairplot(df1, hue=target, size=4)\n    g.map_upper(sns.kdeplot, levels=1, color=\".2\")\n    plt.show()\nexcept:\n    pass","cb9ab61a":"#Removal of any Duplicate rows (if any)\n\ncounter = 0\nr,c = original_df.shape\n\ndf1 = df.copy()\ndf1.drop_duplicates(inplace=True)\ndf1.reset_index(drop=True,inplace=True)\n\nif df1.shape==(r,c):\n    print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any duplicates')\nelse:\n    print(f'\\n\\033[1mInference:\\033[0m Number of duplicates dropped ---> {r-df1.shape[0]}')","96905187":"#Check for empty elements\n\nnvc = pd.DataFrame(df1.isnull().sum().sort_values(), columns=['Total Null Values'])\nnvc['Percentage'] = round(nvc['Total Null Values']\/df1.shape[0],3)*100\nprint(nvc)","67658e33":"#Converting categorical Columns to Numeric\n\ndf1 = df.copy()\necc = nvc[nvc['Percentage']!=0].index.values\ndcc = [i for i in df.columns if i not in ecc]\n\n#Target Variable\nMAP={}\nfor i,e in enumerate(df1[target].unique()):\n    MAP[e]=i\ndf1[target]=df1[target].map(MAP)\nprint('Mapping Target variable --->',MAP)\n\ndf3 = df1[dcc]\nfcc = [i for i in cf if i not in ecc]\n\n#One-Hot Binay Encoding\noh=True\ndm=True\nfor i in fcc:\n    #print(i)\n    if df3[i].nunique()==2:\n        if oh==True: print(\"\\033[1m\\nOne-Hot Encoding on features:\\033[0m\")\n        print(i);oh=False\n        df3[i]=pd.get_dummies(df3[i], drop_first=True, prefix=str(i))\n    if (df3[i].nunique()>2 and df3[i].nunique()<17):\n        if dm==True: print(\"\\n\\033[1mDummy Encoding on features:\\033[0m\")\n        print(i);dm=False\n        df3 = pd.concat([df3.drop([i], axis=1), pd.DataFrame(pd.get_dummies(df3[i], drop_first=True, prefix=str(i)))],axis=1)\n        \ndf3.shape","e4c1c31f":"# Fixing Empty Categorical Columns\n\nfor x in [i for i in ecc if i in cf]:\n    a = df1[x]\n    b=[]; c=[]\n\n    for i,e in enumerate(a):\n        if e!=e:\n            b.append(i)\n        else:\n            c.append(i)\n\n    RF = RandomForestClassifier()\n    RF.fit(df3.loc[c],a[c])\n    d = RF.predict(df3.loc[b])\n\n    df3[x] = a\n    f=0\n    for i,e in enumerate(df3[x]):\n        if e!=e:\n            df3.loc[i,x] = d[f]\n            f+=1\n    df3 = pd.concat([df3.drop([x], axis=1), pd.DataFrame(pd.get_dummies(df3[x], drop_first=True, prefix=str(x)))],axis=1)   \ndf3","f4458b90":"# Fixing Empty Numerical Columns\n\nfor x in [i for i in ecc if i not in cf]:\n    a = df1[x]\n    b=[]; c=[]\n\n    for i,e in enumerate(a):\n        if e!=e:\n            b.append(i)\n        else:\n            c.append(i)\n\n    LR = LinearRegression()\n    LR.fit(df3.loc[c],a[c])\n    d = LR.predict(df3.loc[b])\n\n    df3[x] = a\n    f=0\n    for i,e in enumerate(df3[x]):\n        if e!=e:\n            df3.loc[i,x] = d[f]\n            f+=1\n    #df3 = pd.concat([df3.drop([x], axis=1), pd.DataFrame(pd.get_dummies(df3[x], drop_first=True, prefix=str(x)))],axis=1)   \ndf3","916b906a":"#Removal of outlier:\n\ndf4 = df3.copy()\n\nfor i in [i for i in df4.columns]:\n    if df4[i].nunique()>=12:\n        Q1 = df4[i].quantile(0.20)\n        Q3 = df4[i].quantile(0.80)\n        IQR = Q3 - Q1\n        df4 = df4[df4[i] <= (Q3+(1.5*IQR))]\n        df4 = df4[df4[i] >= (Q1-(1.5*IQR))]\ndf4 = df4.reset_index(drop=True)\ndisplay(df4.head())\nprint('\\n\\033[1mInference:\\033[0m Before removal of outliers, The dataset had {} samples.'.format(df1.shape[0]))\nprint('\\033[1mInference:\\033[0m After removal of outliers, The dataset now has {} samples.'.format(df4.shape[0]))","b73fdd5e":"#Fixing the imbalance using SMOTE Technique\n\ndf5 = df4.copy()\n\nprint('Original class distribution:')\nprint(df5[target].value_counts())\n\nxf = df5.columns\nX = df5.drop([target],axis=1)\nY = df5[target]\n\nsmote = SMOTE()\nX, Y = smote.fit_resample(X, Y)\n\ndf5 = pd.DataFrame(X, columns=xf)\ndf5[target] = Y\n\nprint('\\nClass distribution after applying SMOTE Technique:',)\nprint(Y.value_counts())","325310bb":"#Final Dataset size after performing Preprocessing\n\ndf = df5.copy()\nplt.title('Final Dataset Samples')\nplt.pie([df.shape[0], original_df.shape[0]-df4.shape[0], df5.shape[0]-df4.shape[0]], radius = 1, shadow=True,\n        labels=['Retained','Dropped','Augmented'], counterclock=False, autopct='%1.1f%%', pctdistance=0.9, explode=[0,0,0])\nplt.pie([df.shape[0]], labels=['100%'], labeldistance=-0, radius=0.78, shadow=True, colors=['powderblue'])\nplt.show()\n\nprint('\\n\\033[1mInference:\\033[0mThe final dataset after cleanup has {} samples & {} columns.'.format(df.shape[0], df.shape[1]))","a792b4dd":"#Splitting the data intro training & testing sets\n\ndf = df5.copy()\n\nX = df.drop([target],axis=1)\nY = df[target]\nTrain_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=0)\n\nprint('Original set  ---> ',X.shape,Y.shape,'\\nTraining set  ---> ',Train_X.shape,Train_Y.shape,'\\nTesting set   ---> ', Test_X.shape,'', Test_Y.shape)","3370b04f":"#Feature Scaling (Standardization)\n\nstd = StandardScaler()\n\nprint('\\033[1mStandardardization on Training set'.center(100))\nTrain_X_std = std.fit_transform(Train_X)\nTrain_X_std = pd.DataFrame(Train_X_std, columns=X.columns)\ndisplay(Train_X_std.describe())\n\nprint('\\n','\\033[1mStandardardization on Testing set'.center(100))\nTest_X_std = std.transform(Test_X)\nTest_X_std = pd.DataFrame(Test_X_std, columns=X.columns)\ndisplay(Test_X_std.describe())","65a46904":"#Checking the correlation\n\nfeatures = df.columns\nplt.figure(figsize=[12,10])\nplt.title('Features Correlation-Plot')\nsns.heatmap(df[features].corr(), vmin=-1, vmax=1, center=0) #, \nplt.show()","f88d9733":"# Calculate the VIFs to remove multicollinearity\n\nDROP=[]; scores1=[]; scores2=[]; scores3=[]\n#scores.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std, Train_Y).predict(Test_X_std)))\nscores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\nscores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\nscores3.append(f1_score(Test_Y,XGBClassifier().fit(Train_X_std.drop(DROP,axis=1), Train_Y, eval_metric='logloss').predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n        \nfor i in tqdm(range(len(X.columns.values)-1)):\n    vif = pd.DataFrame()\n    Xs = X.drop(DROP,axis=1)\n    #print(DROP)\n    vif['Features'] = Xs.columns\n    vif['VIF'] = [variance_inflation_factor(Xs.values, i) for i in range(Xs.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    vif.reset_index(drop=True, inplace=True)\n    DROP.append(vif.Features[0])\n    if vif.VIF[0]>1:\n        scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n        scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n        scores3.append(f1_score(Test_Y,XGBClassifier().fit(Train_X_std.drop(DROP,axis=1), Train_Y, eval_metric='logloss').predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n    #print(scores)\n    \nplt.plot(scores1, label='LR')\nplt.plot(scores2, label='RF')\nplt.plot(scores3, label='XG')\n#plt.ylim([0.7,0.85])\nplt.legend()\nplt.grid()\nplt.show()","477655ca":"# Applying Recurrsive Feature Elimination\n\n# Running RFE with the output number of the variable equal to 10\nLR = LogisticRegression()#.fit(Train_X_std, Train_Y)\nscores1=[]; scores2=[]; scores3=[]\nscores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std, Train_Y).predict(Test_X_std),average='weighted')*100)\nscores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std, Train_Y).predict(Test_X_std),average='weighted')*100)\nscores3.append(f1_score(Test_Y,XGBClassifier().fit(Train_X_std, Train_Y, eval_metric='logloss').predict(Test_X_std),average='weighted')*100)\n\nfor i in tqdm(range(len(X.columns.values))):\n    rfe = RFE(LR,n_features_to_select=len(Train_X_std.columns)-i)   \n    rfe = rfe.fit(Train_X_std, Train_Y)\n    scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\n    scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\n    scores3.append(f1_score(Test_Y,XGBClassifier().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y, eval_metric='logloss').predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\n    \nplt.plot(scores1, label='LR')\nplt.plot(scores2, label='RF')\nplt.plot(scores3, label='XG')\n#plt.ylim([0.80,0.84])\nplt.legend()\nplt.grid()\nplt.show()","b9f127c2":"from sklearn.decomposition import PCA\n\npca = PCA().fit(Train_X_std)\n\nfig, ax = plt.subplots(figsize=(14,6))\nx_values = range(1, pca.n_components_+1)\nax.bar(x_values, pca.explained_variance_ratio_, lw=2, label='Explained Variance')\nax.plot(x_values, np.cumsum(pca.explained_variance_ratio_), lw=2, label='Cumulative Explained Variance', color='red')\nplt.plot([0,pca.n_components_+1],[0.90,0.90],'g--')\nplt.plot([6,6],[0,1], 'g--')\nax.set_title('Explained variance of components')\nax.set_xlabel('Principal Component')\nax.set_ylabel('Explained Variance')\nplt.grid()\nplt.legend()\nplt.show()","07dd2129":"#Applying PCA Transformations\n\n# scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std, Train_Y).predict(Test_X_std),average='weighted')*100)\n# scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std, Train_Y).predict(Test_X_std),average='weighted')*100)\n# scores3.append(f1_score(Test_Y,XGBClassifier().fit(Train_X_std, Train_Y, eval_metric='logloss').predict(Test_X_std),average='weighted')*100)\n\nscores1=[]; scores2=[]; scores3=[]\nfor i in tqdm(range(len(X.columns.values))):\n    pca = PCA(n_components=Train_X_std.shape[1]-i)\n    Train_X_std_pca = pca.fit_transform(Train_X_std)\n    #print('The shape of final transformed training feature set:')\n    #print(Train_X_std_pca.shape)\n    Train_X_std_pca = pd.DataFrame(Train_X_std_pca)\n\n    Test_X_std_pca = pca.transform(Test_X_std)\n    #print('\\nThe shape of final transformed testing feature set:')\n    #print(Test_X_std_pca.shape)\n    Test_X_std_pca = pd.DataFrame(Test_X_std_pca)\n    \n    scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std_pca, Train_Y).predict(Test_X_std_pca),average='weighted')*100)\n    scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std_pca, Train_Y).predict(Test_X_std_pca),average='weighted')*100)\n    scores3.append(f1_score(Test_Y,XGBClassifier().fit(Train_X_std_pca, Train_Y, eval_metric='logloss').predict(Test_X_std_pca),average='weighted')*100)\n\n    \nplt.plot(scores1, label='LR')\nplt.plot(scores2, label='RF')\nplt.plot(scores3, label='XG')\n#plt.ylim([0.80,0.84])\nplt.legend()\nplt.grid()\nplt.show()","1d93460c":"#### Finalising the shortlisted features\n\nrfe = RFE(LR,n_features_to_select=len(Train_X_std.columns)-10)   \nrfe = rfe.fit(Train_X_std, Train_Y)\n\nprint(f1_score(Test_Y,LogisticRegression().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\nprint(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\nprint(f1_score(Test_Y,XGBClassifier().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y, eval_metric='logloss').predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\n    \nTrain_X_std = Train_X_std[Train_X_std.columns[rfe.support_]]\nTest_X_std = Test_X_std[Test_X_std.columns[rfe.support_]]\n\nprint(Train_X_std.shape)\nprint(Test_X_std.shape)","9482e344":"#Let us create first create a table to store the results of various models \n\nEvaluation_Results = pd.DataFrame(np.zeros((8,5)), columns=['Accuracy', 'Precision','Recall','F1-score','AUC-ROC score'])\nEvaluation_Results.index=['Logistic Regression (LR)','Decision Tree Classifier (DT)','Random Forest Classifier (RF)','Na\u00efve Bayes Classifier (NB)',\n                         'Support Vector Machine (SVM)','K Nearest Neighbours (KNN)', 'Gradient Boosting (GB)','Extreme Gradient Boosting (XGB)']\nEvaluation_Results","14a9557d":"#Let us define functions to summarise the Prediction's scores .\n\n#Classification Summary Function\ndef Classification_Summary(pred,pred_prob,i):\n    Evaluation_Results.iloc[i]['Accuracy']=round(accuracy_score(Test_Y, pred),3)*100   \n    Evaluation_Results.iloc[i]['Precision']=round(precision_score(Test_Y, pred, average='weighted'),3)*100 #\n    Evaluation_Results.iloc[i]['Recall']=round(recall_score(Test_Y, pred, average='weighted'),3)*100 #\n    Evaluation_Results.iloc[i]['F1-score']=round(f1_score(Test_Y, pred, average='weighted'),3)*100 #\n    Evaluation_Results.iloc[i]['AUC-ROC score']=round(roc_auc_score(Test_Y, pred_prob, multi_class='ovr'),3)*100 #[:, 1]\n    print('{}{}\\033[1m Evaluating {} \\033[0m{}{}\\n'.format('<'*3,'-'*25,Evaluation_Results.index[i], '-'*25,'>'*3))\n    print('Accuracy = {}%'.format(round(accuracy_score(Test_Y, pred),3)*100))\n    print('F1 Score = {}%'.format(round(f1_score(Test_Y, pred, average='weighted'),3)*100)) #\n    print('\\n \\033[1mConfusiton Matrix:\\033[0m\\n',confusion_matrix(Test_Y, pred))\n    print('\\n\\033[1mClassification Report:\\033[0m\\n',classification_report(Test_Y, pred))\n    \n    auc_roc(Test_Y, pred_prob, curves=['each_class'])\n    plt.show()\n\n#Visualising Function\ndef AUC_ROC_plot(Test_Y, pred):    \n    ref = [0 for _ in range(len(Test_Y))]\n    ref_auc = roc_auc_score(Test_Y, ref)\n    lr_auc = roc_auc_score(Test_Y, pred)\n\n    ns_fpr, ns_tpr, _ = roc_curve(Test_Y, ref)\n    lr_fpr, lr_tpr, _ = roc_curve(Test_Y, pred)\n\n    plt.plot(ns_fpr, ns_tpr, linestyle='--')\n    plt.plot(lr_fpr, lr_tpr, marker='.', label='AUC = {}'.format(round(roc_auc_score(Test_Y, pred)*100,2))) \n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.show()","4aebd046":"# Building Logistic Regression Classifier\n\nLR_model = LogisticRegression()\n\nspace = dict()\nspace['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\nspace['penalty'] = ['l2'] #'none', 'l1', 'l2', 'elasticnet'\nspace['C'] = loguniform(1e-5, 100)\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n#RCV = RandomizedSearchCV(LR_model, space, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nLR = LR_model.fit(Train_X_std, Train_Y)#.best_estimator_\npred = LR.predict(Test_X_std)\npred_prob = LR.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,0)\n\nprint('\\n\\033[1mInterpreting the Output of Logistic Regression:\\n\\033[0m')\n\nprint('intercept ', LR.intercept_[0])\nprint('classes', LR.classes_)\ndisplay(pd.DataFrame({'coeff': LR.coef_[0]}, index=Train_X_std.columns))","f8e4c1c2":"#Building Decision Tree Classifier\n\nDT_model = DecisionTreeClassifier()\n\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, len(features)-1),\n              \"min_samples_leaf\": randint(1, len(features)-1),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n#RCV = RandomizedSearchCV(DT_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nDT = DT_model.fit(Train_X_std, Train_Y)#.best_estimator_\npred = DT.predict(Test_X_std)\npred_prob = DT.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,1)\n\nprint('\\n\\033[1mInterpreting the output of Decision Tree:\\n\\033[0m')\ntree.plot_tree(DT)\nplt.show()","d13340a2":"# Building Random-Forest Classifier\n\nRF_model = RandomForestClassifier()\n\nparam_dist={'bootstrap': [True, False],\n            'max_depth': [10, 20, 50, 100, None],\n            'max_features': ['auto', 'sqrt'],\n            'min_samples_leaf': [1, 2, 4],\n            'min_samples_split': [2, 5, 10],\n            'n_estimators': [50, 100]}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n#RCV = RandomizedSearchCV(RF_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nRF = RF_model.fit(Train_X_std, Train_Y)#.best_estimator_\npred = RF.predict(Test_X_std)\npred_prob = RF.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,2)\n\nprint('\\n\\033[1mInterpreting the output of Random Forest:\\n\\033[0m')\nrfi=pd.Series(RF.feature_importances_, index=Train_X_std.columns).sort_values(ascending=False)\nplt.barh(rfi.index,rfi.values)\nplt.show()","5f1785fc":"# Building Naive Bayes Classifier\n\nNB_model = BernoulliNB()\n\nparams = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n#RCV = RandomizedSearchCV(NB_model, params, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nNB = NB_model.fit(Train_X_std, Train_Y)#.best_estimator_\npred = NB.predict(Test_X_std)\npred_prob = NB.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,3)","6fb4878f":"# Building Support Vector Machine Classifier\n\nSVM_model = SVC(probability=True).fit(Train_X_std, Train_Y)\n\nsvm_param = {\"C\": [.01, .1, 1, 5, 10, 100],             \n             \"gamma\": [.01, .1, 1, 5, 10, 100],\n             \"kernel\": [\"rbf\"],\n             \"random_state\": [1]}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n#RCV = RandomizedSearchCV(SVM_model, svm_param, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nSVM = SVM_model.fit(Train_X_std, Train_Y)#.best_estimator_\npred = SVM.predict(Test_X_std)\npred_prob = SVM.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,4)","f7291b60":"# Building K-Neareset Neighbours Classifier\n\nKNN_model = KNeighborsClassifier()\n\nknn_param = {\"n_neighbors\": [i for i in range(1,30,5)],\n             \"weights\": [\"uniform\", \"distance\"],\n             \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n             \"leaf_size\": [1, 10, 30],\n             \"p\": [1,2]}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n#RCV = RandomizedSearchCV(KNN_model, knn_param, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nKNN = KNN_model.fit(Train_X_std, Train_Y)#.best_estimator_\npred = KNN.predict(Test_X_std)\npred_prob = KNN.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,5)","5c5da4a6":"# Building Gradient Boosting Classifier\n\nGB_model = GradientBoostingClassifier().fit(Train_X_std, Train_Y)\nparam_dist = {\n    \"n_estimators\":[5,20,100,500],\n    \"max_depth\":[1,3,5,7,9],\n    \"learning_rate\":[0.01,0.1,1,10,100]\n}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n#RCV = RandomizedSearchCV(GB_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nGB = GB_model.fit(Train_X_std, Train_Y)#.best_estimator_\npred = GB.predict(Test_X_std)\npred_prob = GB.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,6)","f46fb174":"# Building Extreme Gradient Boosting Classifier\n\nXGB_model = XGBClassifier()#.fit(Train_X_std, Train_Y, eval_metric='logloss')\n\nparam_dist = {\n \"learning_rate\" : [0.05,0.10,0.15,0.20,0.25,0.30],\n \"max_depth\" : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n#RCV = RandomizedSearchCV(XGB_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nXGB = XGB_model.fit(Train_X_std, Train_Y, eval_metric='logloss')#.best_estimator_\npred = XGB.predict(Test_X_std)\npred_prob = XGB.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,7)\n\nxgbf=pd.DataFrame(XGB.feature_importances_, index=Train_X_std.columns).sort_values(by=0)\nplt.barh(xgbf.index,xgbf.values[:,0])\nplt.show()","ce48dcb0":"#Plotting Confusion-Matrix of all the predictive Models\n\ndef plot_cm(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.columns=labels\n    cm.index=labels\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    #fig, ax = plt.subplots()\n    sns.heatmap(cm, annot=annot, fmt='')# cmap= \"GnBu\"\n    \ndef conf_mat_plot(all_models):\n    plt.figure(figsize=[20,3.5*math.ceil(len(all_models)*len(labels)\/14)])\n    \n    for i in range(len(all_models)):\n        if len(labels)<=4:\n            plt.subplot(2,4,i+1)\n        else:\n            plt.subplot(math.ceil(len(all_models)\/3),3,i+1)\n        pred = all_models[i].predict(Test_X_std)\n        #plot_cm(Test_Y, pred)\n        sns.heatmap(confusion_matrix(Test_Y, pred), annot=True, cmap='Blues', fmt='.0f') #vmin=0,vmax=5\n        plt.title(Evaluation_Results.index[i])\n    plt.tight_layout()\n    plt.show()\n\nconf_mat_plot([LR,DT,RF,NB,SVM,KNN,GB,XGB])","31e382f1":"#Comparing all the models Scores\n\nprint('\\033[1mML Algorithms Comparison'.center(100))\nplt.figure(figsize=[12,8])\nsns.heatmap(Evaluation_Results, annot=True, vmin=60, vmax=95, cmap='Blues', fmt='.1f')\nplt.show()","46be930b":"#<<<---------------------------------------------THE END----------------------------------------------------------------->>>","33c4f8bd":"**Inference:** The stats seem to be fine, let us gain more undestanding by visualising the dataset.","d5899c05":"**Inference:** The Target Variable seems to be slightly imbalanced! Hence we shall try to perform data augmentation.","1cb9f6bd":"---","e34f70ae":"---","cebaf43d":"## 5a. Manual Method - VIF","90dda21e":"## 7. Gradient Boosting Classfier:","7c8019c4":"---","a52edd4a":"---","4a4c29d1":"---","c3c9cb95":"**Inference:** In VIF, RFE & PCA Techniques, we did notice any better scores upon dropping some multicollinear features. But in order to avoid the curse of dimensionality, we can capture top 90% of the data Variance explained by top n PCA components.","ec40f44c":"---","4a9923e5":"---","42c411a3":"**Inference:** The data samples of most of the features do show some patterns. Also they seem\nto have lot of overlap for the outcome classes, making it difficult to be distingusihable. \nLet is proceed to perform cleanup on the data to remove the irregularities...","e24091f1":"# <center> 2. Exploratory Data Analysis (EDA)","3f5601a6":"### Description:\n\nPredict whether or not a horse can survive based upon past medical conditions.\n\nNoted by the \"outcome\" variable in the data.\n\nContent:\n\nAll of the binary representation have been converted into the words they actually represent. However, a fuller description is provided by the data dictionary (datadict.txt).\n\nThere are a lot of NA's in the data. This is the real struggle here. Try to find a way around it through imputation or other means.\n\nAttribute Information:\n\n1: surgery? \n1 = Yes, it had surgery \n2 = It was treated without surgery \n\n2: Age \n1 = Adult horse \n2 = Young (< 6 months) \n\n3: Hospital Number \n- numeric id \n- the case number assigned to the horse (may not be unique if the horse is treated > 1 time) \n\n4: rectal temperature \n- linear \n- in degrees celsius. \n- An elevated temp may occur due to infection. \n- temperature may be reduced when the animal is in late shock \n- normal temp is 37.8 \n- this parameter will usually change as the problem progresses, eg. may start out normal, then become elevated because of the lesion, passing back through the normal range as the horse goes into shock \n5: pulse \n- linear \n- the heart rate in beats per minute \n- is a reflection of the heart condition: 30 -40 is normal for adults \n- rare to have a lower than normal rate although athletic horses may have a rate of 20-25 \n- animals with painful lesions or suffering from circulatory shock may have an elevated heart rate \n\n6: respiratory rate \n- linear \n- normal rate is 8 to 10 \n- usefulness is doubtful due to the great fluctuations \n\n7: temperature of extremities \n- a subjective indication of peripheral circulation \n- possible values: \n1 = Normal \n2 = Warm \n3 = Cool \n4 = Cold \n- cool to cold extremities indicate possible shock \n- hot extremities should correlate with an elevated rectal temp. \n\n8: peripheral pulse \n- subjective \n- possible values are: \n1 = normal \n2 = increased \n3 = reduced \n4 = absent \n- normal or increased p.p. are indicative of adequate circulation while reduced or absent indicate poor perfusion \n\n9: mucous membranes \n- a subjective measurement of colour \n- possible values are: \n1 = normal pink \n2 = bright pink \n3 = pale pink \n4 = pale cyanotic \n5 = bright red \/ injected \n6 = dark cyanotic \n- 1 and 2 probably indicate a normal or slightly increased circulation \n- 3 may occur in early shock \n- 4 and 6 are indicative of serious circulatory compromise \n- 5 is more indicative of a septicemia \n\n10: capillary refill time \n- a clinical judgement. The longer the refill, the poorer the circulation \n- possible values \n1 = < 3 seconds \n2 = >= 3 seconds \n\n11: pain - a subjective judgement of the horse's pain level \n- possible values: \n1 = alert, no pain \n2 = depressed \n3 = intermittent mild pain \n4 = intermittent severe pain \n5 = continuous severe pain \n- should NOT be treated as a ordered or discrete variable! \n- In general, the more painful, the more likely it is to require surgery \n- prior treatment of pain may mask the pain level to some extent \n\n12: peristalsis \n- an indication of the activity in the horse's gut. As the gut becomes more distended or the horse becomes more toxic, the activity decreases \n- possible values: \n1 = hypermotile \n2 = normal \n3 = hypomotile \n4 = absent \n\n13: abdominal distension \n- An IMPORTANT parameter. \n- possible values \n1 = none \n2 = slight \n3 = moderate \n4 = severe \n- an animal with abdominal distension is likely to be painful and have reduced gut motility. \n- a horse with severe abdominal distension is likely to require surgery just tio relieve the pressure \n\n14: nasogastric tube \n- this refers to any gas coming out of the tube \n- possible values: \n1 = none \n2 = slight \n3 = significant \n- a large gas cap in the stomach is likely to give the horse discomfort \n\n15: nasogastric reflux \n- possible values \n1 = none \n2 = > 1 liter \n3 = < 1 liter \n- the greater amount of reflux, the more likelihood that there is some serious obstruction to the fluid passage from the rest of the intestine \n\n16: nasogastric reflux PH \n- linear \n- scale is from 0 to 14 with 7 being neutral \n- normal values are in the 3 to 4 range \n\n17: rectal examination - feces \n- possible values \n1 = normal \n2 = increased \n3 = decreased \n4 = absent \n- absent feces probably indicates an obstruction \n\n18: abdomen \n- possible values \n1 = normal \n2 = other \n3 = firm feces in the large intestine \n4 = distended small intestine \n5 = distended large intestine \n- 3 is probably an obstruction caused by a mechanical impaction and is normally treated medically \n- 4 and 5 indicate a surgical lesion \n\n19: packed cell volume \n- linear \n- the # of red cells by volume in the blood \n- normal range is 30 to 50. The level rises as the circulation becomes compromised or as the animal becomes dehydrated. \n\n20: total protein \n- linear \n- normal values lie in the 6-7.5 (gms\/dL) range \n- the higher the value the greater the dehydration \n\n21: abdominocentesis appearance \n- a needle is put in the horse's abdomen and fluid is obtained from \nthe abdominal cavity \n- possible values: \n1 = clear \n2 = cloudy \n3 = serosanguinous \n- normal fluid is clear while cloudy or serosanguinous indicates a compromised gut \n\n22: abdomcentesis total protein \n- linear \n- the higher the level of protein the more likely it is to have a compromised gut. Values are in gms\/dL \n\n23: outcome \n- what eventually happened to the horse? \n- possible values: \n1 = lived \n2 = died \n3 = was euthanized \n\n24: surgical lesion? \n- retrospectively, was the problem (lesion) surgical? \n- all cases are either operated upon or autopsied so that this value and the lesion type are always known \n- possible values: \n1 = Yes \n2 = No \n\n25, 26, 27: type of lesion \n- first number is site of lesion \n1 = gastric \n2 = sm intestine \n3 = lg colon \n4 = lg colon and cecum \n5 = cecum \n6 = transverse colon \n7 = retum\/descending colon \n8 = uterus \n9 = bladder \n11 = all intestinal sites \n00 = none \n- second number is type \n1 = simple \n2 = strangulation \n3 = inflammation \n4 = other \n- third number is subtype \n1 = mechanical \n2 = paralytic \n0 = n\/a \n- fourth number is specific code \n1 = obturation \n2 = intrinsic \n3 = extrinsic \n4 = adynamic \n5 = volvulus\/torsion \n6 = intussuption \n7 = thromboembolic \n8 = hernia \n9 = lipoma\/slenic incarceration \n10 = displacement \n0 = n\/a \n28: cp_data \n- is pathology data present for this case? \n1 = Yes \n2 = No \n- this variable is of no significance since pathology data is not included or collected for these cases\n\n### Acknowledgements:\nThis dataset was originally published by the UCI Machine Learning Database:\\\nhttp:\/\/archive.ics.uci.edu\/ml\/datasets\/Horse+Colic\n\n### Objective:\n- Understand the Dataset & cleanup (if required).\n- Build classification model to predict weather the horse will survive or not.\n- Also fine-tune the hyperparameters & compare the evaluation metrics of vaious classification algorithms.","ac024648":"## 3. Random Forest Classfier:","fce4ebb7":"---","b68e616e":"**Inference:** Visualizing the categorical features reveal lot of information about the dataset.","0f553c05":"**Insights:** For the current problem statement, it is more important to focus on the F1 score. We can note from the above heatmap that the Random Forest & Extreme Gradient Boosting Model Performed well on the current dataset...","8d9dcd37":"## 8. Extreme Gradient Boosting Classfier:","8619a250":"---","c192d4f7":"---","d508a5e8":"## 4. Naive Bayes Classfier:","d5674eec":"# <center> 6. Predictive Modeling","e65b27be":"**We aim to solve the problem statement by creating a plan of action, Here are some of the necessary steps:**\n1. Data Exploration\n2. Exploratory Data Analysis (EDA)\n3. Data Pre-processing\n4. Data Manipulation\n5. Feature Selection\/Extraction\n6. Predictive Modelling\n7. Project Outcomes & Conclusion","b37fbc78":"# <center> 3. Data Preprocessing","893f56cf":"---","8c6e20a3":"## 6. K-Nearest Neighbours Classfier:","d51a9691":"**Inference:** \\\nCorrelation plt between the variables convey lot of information about the realationship betweem them. There seems to be strong multicollinearity in the dataset.\n\nLet us check with different techniques if we can improve the model's performance by performing Feature Selection\/Extraction steps to take care of these multi-collinearity...","2ab7985f":"**Inference:** We shall avoid performing dimensionality reduction for the current problem.","31e78545":"---","aab413e2":"## 2. Decisoin Tree Classfier:","dd9899ae":"# <center> Stractegic Plan of Action:","ba38c1ad":"---","3c013c7e":"**Strategy:** \\\nWe can fix these multicollinearity with two techniques:\n1. Manual Method - Variance Inflation Factor (VIF)\n2. Automatic Method - Recursive Feature Elimination (RFE)\n3. Decomposition Method - Principle Component Analysis (PCA)","df3f8e28":"<center><img src=\"https:\/\/raw.githubusercontent.com\/Masterx-AI\/Project_Horse_Sruvival_Prognostication\/main\/horse.jpg\" style=\"width: 700px;\"\/>","4f06735c":"---","25bf9f9c":"## 5b. Automatic Method - RFE","fc60aad2":"## 5. Support Vector Machine Classfier:","c11f9fcc":"## 1. Logistic Regression:","562d885d":"---","dcb2ea77":"# <center> 7. Project Outcomes & Conclusions","cd4f9994":"---","81e32c66":"**Inference:** The data is somewhat normally distributed. And there are many outliers present in the dataset. We shall fix these outliers..","1aebf9e7":"# <center>1. Data Exploration","6437f457":"---","28a4fd62":"# <center> 5. Feature Selection\/Extraction","e020462c":"---","716a61c6":"# <center> 4. Data Manipulation","d8e5d091":"---","e59ec0eb":"# <center> \u2605 AI \/ ML Project - Horse Survival Prediction \u2605\n#### <center> ***Domain: Healthcare***","dbfb6e0a":"**Inference:** There are many outliers in the dataset. Let us try to impute the missing values","1d81e7f6":"### Here are some of the key outcomes of the project:\n- The Dataset was quiet small totalling around 300 samples & after preprocessing 3.9% of the datasamples were dropped. \n- The samples were slightly imbalanced after processing, hence SMOTE Technique was applied on the data to  balance the classes, adding 30.4% more samples to the dataset.\n- Visualising the distribution of data & their relationships, helped us to get some insights on the relationship between the feature-set.\n- Feature Selection\/Eliminination was carried out and appropriate features were shortlisted.\n- Testing multiple algorithms with fine-tuning hyperparamters gave us some understanding on the model performance for various algorithms on this specific dataset.\n- The XG-Boosting & Random Forest Classifier performed exceptionally well on the current dataset, considering F1-Score as the key-metric.\n- Yet it wise to also consider simpler model like Logistic Regression as it is more generalisable & is computationally less expensive, but comes at the cost of slight misclassifications."}}