{"cell_type":{"1bea1c28":"code","1c17b64b":"code","cdd3b11a":"code","93cfd088":"code","8e065ce8":"code","39f028c6":"code","a55c18eb":"code","2ab95b8f":"code","d54f3194":"code","57196ef0":"code","7c6a2067":"code","42bc73ba":"code","d728a6e7":"code","ae94fb46":"code","db08760d":"code","ee36b340":"code","45353398":"code","5980476b":"code","4b0ab65b":"code","4f7180b1":"code","5583d4dc":"code","32af5e03":"code","ecfc1b24":"code","182637a3":"code","3d9ca38f":"code","eb7c0a9e":"code","c93021b2":"code","e1723a33":"code","54527fca":"code","ddc72ce6":"code","634aa025":"code","2b2d1503":"code","24656074":"code","9758d242":"code","890a6daa":"code","9989a4d0":"code","1950258d":"code","a390b8ba":"code","42e050bf":"code","0c8cbbee":"code","aa41f34c":"code","365c295e":"code","2bff4be9":"code","cd8525ad":"code","158ecc30":"code","5f5aa4bd":"code","ba061c4c":"markdown","238134e8":"markdown","07d0d0eb":"markdown","58085f26":"markdown","c8ee73a7":"markdown","69ebde2a":"markdown","46aa24b9":"markdown","7421dad3":"markdown","b3ab4dda":"markdown","4d22c55a":"markdown","4180cc8c":"markdown","edeea3b0":"markdown","7acf8711":"markdown","9e980a89":"markdown","0df92a70":"markdown","06618b14":"markdown","2ec9ecf1":"markdown","86022c38":"markdown","b9d245db":"markdown","bd2ca406":"markdown","3e4c700b":"markdown","8b80668b":"markdown","677570ec":"markdown","f5328a43":"markdown","125ddaa4":"markdown","c0688ce7":"markdown","559b20e0":"markdown","117fe4e2":"markdown","35531197":"markdown"},"source":{"1bea1c28":"# installation required for Spark\n!pip install sparkmagic\n!pip install pyspark","1c17b64b":"# libraries\nimport warnings\n# import findspark\nimport pandas as pd\nimport seaborn as sns\nfrom pyspark.ml.classification import GBTClassifier, LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, StandardScaler\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import Bucketizer\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.2f' % x)","cdd3b11a":"spark = SparkSession.builder.master(\"local[*]\").getOrCreate()","93cfd088":"spark_df = spark.read.csv('..\/input\/chrun-predictions\/churn2.csv', inferSchema=True, header=True)\nspark_df.show(10)","8e065ce8":"print(\"Shape: \", (spark_df.count(), len(spark_df.columns)))","39f028c6":"spark_df.printSchema()","a55c18eb":"spark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns])\nspark_df.show(5)","2ab95b8f":"spark_df.describe().show()","d54f3194":"spark_df.describe([\"age\", \"exited\"]).show()","57196ef0":"spark_df.groupby(\"exited\").count().show()","7c6a2067":"spark_df.select(\"exited\").distinct().show()","42bc73ba":"spark_df.groupby(\"exited\").count().show()","d728a6e7":"num_cols = [col[0] for col in spark_df.dtypes if col[1] != 'string']\nspark_df.select(num_cols).describe().show()","ae94fb46":"cat_cols = [col[0] for col in spark_df.dtypes if col[1] == 'string']","db08760d":"for col in [col.lower() for col in num_cols]:\n    spark_df.groupby(\"exited\").agg({col: \"mean\"}).show()","ee36b340":"from pyspark.sql.functions import when, count, col\nspark_df.select([count(when(col(c).isNull(), c)).alias(c) for c in spark_df.columns]).toPandas().T","45353398":"spark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns])\nspark_df.show(5)","5980476b":"spark_df = spark_df.drop('rownumber', \"customerid\", \"surname\")","4b0ab65b":"spark_df = spark_df.withColumn('creditscore_salary', spark_df.creditscore \/ spark_df.estimatedsalary)\nspark_df = spark_df.withColumn('creditscore_tenure', spark_df.creditscore * spark_df.tenure)\nspark_df = spark_df.withColumn('balance_salary', spark_df.balance \/ spark_df.estimatedsalary)\nspark_df.show(5)","4f7180b1":"# age de\u011fi\u015fkeni\nspark_df.select('age').describe().toPandas().transpose()\nspark_df.select(\"age\").summary(\"count\", \"min\", \"25%\", \"50%\",\"75%\", \"max\").show()\nbucketizer = Bucketizer(splits=[0, 35, 55, 75, 95], inputCol=\"age\", outputCol=\"age_cat\")\nspark_df = bucketizer.setHandleInvalid(\"keep\").transform(spark_df)\nspark_df = spark_df.withColumn('age_cat', spark_df.age_cat + 1)","5583d4dc":"spark_df = spark_df.withColumn(\"age_cat\", spark_df[\"age_cat\"].cast(\"integer\"))","32af5e03":"spark_df.withColumn('creditscore_2',\n                    when(spark_df['creditscore'] < 301, \"deep\").\n                    when((301 < spark_df['creditscore']) & (spark_df['creditscore'] < 601), \"very poor\").\n                    when((500 < spark_df['creditscore']) & (spark_df['creditscore'] < 601), \"poor\").\n                    when((601 < spark_df['creditscore']) & (spark_df['creditscore'] < 661), \"fair\").\n                    when((661 < spark_df['creditscore']) & (spark_df['creditscore'] < 781), \"good\").\n                    when((781 < spark_df['creditscore']) & (spark_df['creditscore'] < 851), \"excellent\").\n                    otherwise(\"top\")).show()","ecfc1b24":"from pyspark.sql.types import IntegerType, StringType, FloatType\nfrom pyspark.sql.functions import udf\n\n# Function with UDF\ndef segment(tenure):\n    if tenure < 5:\n        return \"segment_b\"\n    else:\n        return \"segment_a\"\n\nfunc_udf = udf(segment, StringType())\nspark_df = spark_df.withColumn('segment', func_udf(spark_df['tenure']))\nspark_df.show(5)","182637a3":"indexer = StringIndexer(inputCol=\"segment\", outputCol=\"segment_label\")\nindexer.fit(spark_df).transform(spark_df).show(5)\ntemp_sdf = indexer.fit(spark_df).transform(spark_df)\nspark_df = temp_sdf.withColumn(\"segment_label\", temp_sdf[\"segment_label\"].cast(\"integer\"))\nspark_df = spark_df.drop('segment')","3d9ca38f":"indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_label\")\nindexer.fit(spark_df).transform(spark_df).show(5)\ntemp_sdf = indexer.fit(spark_df).transform(spark_df)\nspark_df = temp_sdf.withColumn(\"gender_label\", temp_sdf[\"gender_label\"].cast(\"integer\"))\nspark_df = spark_df.drop('gender')","eb7c0a9e":"indexer = StringIndexer(inputCol=\"geography\", outputCol=\"geography_label\")\nindexer.fit(spark_df).transform(spark_df).show(5)\ntemp_sdf = indexer.fit(spark_df).transform(spark_df)\nspark_df = temp_sdf.withColumn(\"geography_label\", temp_sdf[\"geography_label\"].cast(\"integer\"))\nspark_df = spark_df.drop('geography')","c93021b2":"encoder = OneHotEncoder(inputCols=[\"age_cat\", \"geography_label\"], outputCols=[\"age_cat_ohe\", \"geography_label_ohe\"])\nspark_df = encoder.fit(spark_df).transform(spark_df)","e1723a33":"stringIndexer = StringIndexer(inputCol='exited', outputCol='label')\n\ntemp_sdf = stringIndexer.fit(spark_df).transform(spark_df)\ntemp_sdf.show()","54527fca":"spark_df = temp_sdf.withColumn(\"label\", temp_sdf[\"label\"].cast(\"integer\"))\nspark_df.show(5)","ddc72ce6":"cols = ['creditscore', 'age', 'tenure', 'balance','numofproducts', 'hascrcard',\n        'isactivemember', 'estimatedsalary', 'creditscore_salary', 'creditscore_tenure',\n        'balance_salary', 'segment_label', 'gender_label',\n        'age_cat_ohe', 'geography_label_ohe']","634aa025":"va = VectorAssembler(inputCols=cols, outputCol=\"features\")\nva_df = va.transform(spark_df)\nva_df.show()","2b2d1503":"final_df = va_df.select(\"features\", \"label\")\nfinal_df.show(5)","24656074":"scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\nfinal_df = scaler.fit(final_df).transform(final_df)","9758d242":"train_df, test_df = final_df.randomSplit([0.7, 0.3], seed=17)\ntrain_df.show(5)","890a6daa":"test_df.show(5)","9989a4d0":"print(\"Training Dataset Count: \" + str(train_df.count()))\nprint(\"Test Dataset Count: \" + str(test_df.count()))","1950258d":"log_model = LogisticRegression(featuresCol='features', labelCol='label').fit(train_df)\ny_pred = log_model.transform(test_df)\ny_pred.show()","a390b8ba":"y_pred.select(\"label\", \"prediction\").show()","42e050bf":"y_pred.filter(y_pred.label == y_pred.prediction).count() \/ y_pred.count()","0c8cbbee":"evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\nevaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")","aa41f34c":"evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\nevaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n\nacc = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"accuracy\"})\nprecision = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"precisionByLabel\"})\nrecall = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"recallByLabel\"})\nf1 = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"f1\"})\nroc_auc = evaluator.evaluate(y_pred)\n\nprint(\"accuracy: %f, precision: %f, recall: %f, f1: %f, roc_auc: %f\" % (acc, precision, recall, f1, roc_auc))","365c295e":"gbm = GBTClassifier(maxIter=100, featuresCol=\"features\", labelCol=\"label\")\ngbm_model = gbm.fit(train_df)\ny_pred = gbm_model.transform(test_df)\ny_pred.show(5)","2bff4be9":"y_pred.filter(y_pred.label == y_pred.prediction).count() \/ y_pred.count()","cd8525ad":"evaluator = BinaryClassificationEvaluator()\n\ngbm_params = (ParamGridBuilder()\n              .addGrid(gbm.maxDepth, [2, 4, 6])\n              .addGrid(gbm.maxBins, [20, 30])\n              .addGrid(gbm.maxIter, [10, 20])\n              .build())","158ecc30":"cv = CrossValidator(estimator=gbm,\n                    estimatorParamMaps=gbm_params,\n                    evaluator=evaluator,\n                    numFolds=5)","5f5aa4bd":"cv_model = cv.fit(train_df)\ny_pred = cv_model.transform(test_df)\nac = y_pred.select(\"label\", \"prediction\")\nac.filter(ac.label == ac.prediction).count() \/ ac.count()","ba061c4c":"**Bucketization \/ Bining \/ Num to Cat**","238134e8":"![image.png](attachment:627e6151-2889-4b33-8778-ae603e3688f7.png)","07d0d0eb":"# Importing Required Libraries","58085f26":"[1] https:\/\/medium.com\/5bayt\/apache-spark-nedir-ne-i\u015f-yapar-5797c28eb95\n\n[2] https:\/\/medium.com\/5bayt\/apache-spark-nedir-ne-i\u015f-yapar-5797c28eb95\n\n[3] https:\/\/www.veribilimiokulu.com\/pyspark-ile-spark-dataframe-islemleri\/","c8ee73a7":"**Accuracy**","69ebde2a":"**Logistic Regression**","46aa24b9":"# Modeling","7421dad3":"# Installation of Required Libraries for Spark","b3ab4dda":"**1.It is fast**\n\nSpark runs 100 times faster than Hadoop MapReduce, which is used for large-scale data processing. It can also reach this speed through controlled partitioning.\n\n**2.Strong Caching**\n\nThe simple programming layer provides powerful caching and disk persistence capabilities.\n\n**3.Real-time**\n\nIt offers Real Time computing and low latency due to in-memory computing.\n\n**4.Language Support**\n\nSpark offers high-level APIs for Java, Scala, Python, and R. You can use Spark in any of these four languages.","4d22c55a":"# Customer Churn","4180cc8c":"# Vectorize independent variables","edeea3b0":"**One Hot Encoding**","7acf8711":"# References","9e980a89":"**Gradient Boosted Tree Classifier**","0df92a70":"# Defining Target Column","06618b14":"# Data Preprocessing & Feature Engineering","2ec9ecf1":"Surname: corresponds to the record (row) number and has no effect on the output.\n\nCreditScore: contains random values and has no effect on customer leaving the bank.\n\nGeography: a customer\u2019s location can affect their decision to leave the bank.\n\nGender: it\u2019s interesting to explore whether gender plays a role in a customer leaving the bank.\n\nAge: this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n\nTenure: refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n\nNumOfProducts: refers to the number of products that a customer has purchased through the bank.\n\nHasCrCard: denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n\nIsActiveMember: active customers are less likely to leave the bank.\n\nEstimatedSalary: as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n\nExited: (Dependent Variable): whether or not the customer left the bank.\n\nBalance: also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.","86022c38":"# Standard Scaler","b9d245db":"Apache Spark lets you process data in parallel. For example, suppose you have a data set of 1,000,000 rows. Let's use this dataset for machine learning. Let there be a combination of \"label\" and \"text\" in the dataset. We will give a text and determine which tag it is similar to with a classification algorithm. Normally it will be difficult to calculate within 1,000,000 data. However, with the spark architecture, we calculate the data in sets of 1000 by dividing it by 1000, for example, and calculate it much faster with an calculation such as the average of the set of 1000.","bd2ca406":"**Feature Interaction**","3e4c700b":"**Converting float values to integer**","8b80668b":"# What Is Spark","677570ec":"# Model Tuning","f5328a43":"# Dataset","125ddaa4":"![](https:\/\/ittutorial.org\/wp-content\/uploads\/2020\/10\/spark.jpeg)","c0688ce7":"# Split the Dataset Into Test and Train Sets","559b20e0":"# Exploratory Data Analysis","117fe4e2":"# Creating A Spark Session","35531197":"Churn estimation is of great importance for companies. According to McKinsey's research, fast-growing companies are expected to have low churn rates in order to maintain their profitability in a stable manner."}}