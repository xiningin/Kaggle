{"cell_type":{"7e2b91a5":"code","dc570bac":"code","1c8926f2":"code","398d5c0e":"code","a4d4a2bb":"code","fb94e38c":"code","76f865cf":"code","29d33913":"code","123d2a59":"code","1f90a099":"code","fbde9efe":"code","f8ffb7ca":"code","8b8fe199":"code","b2b8fa21":"code","77d7f20b":"code","02d6e5c5":"code","c6cd7267":"code","120f56b9":"code","1ad9d640":"code","611417a8":"code","15e8070b":"code","7e4c3933":"code","ecae979c":"code","b3f7b472":"code","328bdb41":"code","7f83964b":"code","be6b2505":"code","373dc93a":"code","69c14b94":"code","87b32eb0":"code","892f07e0":"code","41cb4a22":"code","20d85f05":"code","9b1194ba":"code","dd008fd3":"code","f75ee8e1":"code","c51ff868":"code","6ec7ff07":"code","d26925d8":"code","5df58099":"code","bac25315":"code","8dd0d470":"code","993d050c":"code","1f4ea2e2":"code","f1148bec":"code","7dc7b0f0":"code","39691325":"code","619e8657":"code","89cbf86a":"code","a008e610":"code","75f618cc":"code","166c7808":"code","2beea0e5":"code","e9823935":"code","b767b375":"code","75d1aa0d":"code","ef724146":"code","13680f2f":"code","1b9157f9":"markdown","8e0178bf":"markdown","6147138e":"markdown","3fbd71f2":"markdown","e0b74990":"markdown","d712d095":"markdown","56897b46":"markdown","0fb139ac":"markdown","398e0091":"markdown","4c1430e1":"markdown","29bf6517":"markdown","50a0e91a":"markdown","c70063cb":"markdown","62ee88b2":"markdown","e06a5116":"markdown","5eb56893":"markdown","ec537ebd":"markdown","f603606c":"markdown","887c4eff":"markdown","d9bd96b8":"markdown","78510c52":"markdown","f5ed0de6":"markdown","dde69e48":"markdown","6278c046":"markdown","a45ea433":"markdown"},"source":{"7e2b91a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dc570bac":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as pyplot\n\nfrom sklearn.preprocessing import StandardScaler ## Talvez n\u00e3o utilizemos isto\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc","1c8926f2":"df = pd.read_excel('..\/input\/covid19\/dataset.xlsx')","398d5c0e":"df.to_csv ('dataset.csv', index = None, header=True)","a4d4a2bb":"df.shape","fb94e38c":"pd.set_option(\"display.max_columns\", 120)","76f865cf":"df.head()","29d33913":"df.dtypes","123d2a59":"df.describe()","1f90a099":"df[\"SARS-Cov-2 exam result\"].value_counts(dropna= 'Positive')","fbde9efe":"df['SARS-Cov-2 exam result'].value_counts().plot.barh()","f8ffb7ca":"pd.set_option(\"display.max_rows\", 200)\ndf.isnull().sum()\n","8b8fe199":"df = df.drop(['Patient ID'], axis = 1)","b2b8fa21":"df_numeric = df.select_dtypes(include=[np.number]) \nnumericas = list(df_numeric)","77d7f20b":"# Nao podemos desprezar  dados devido ao poucos registros disponivieis\n# selecionando as colunas num\u00e9ricas e preenchendo com a m\u00e9dia\ndf[numericas] = df[numericas].fillna(df.mean())","02d6e5c5":"# substitui os dados faltantes por 'na'\ndf = df.fillna('na') \n# se sobrar alguma linha (nao deveria), dropamos essas linhas.\ndf = df.dropna() \n# verificando o tamanho do DataFrame\ndf.shape","c6cd7267":"df.isnull().sum()","120f56b9":"df.head(50)","1ad9d640":"df['Patient Age Quantile'] = df['Patient age quantile'].astype(\"category\")","611417a8":"# aplica\u00e7\u00e3o do get_dummies do pandas para dummiza\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas\ndf = pd.get_dummies(df, drop_first=True)","15e8070b":"# verifica\u00e7\u00e3o do tamamno do DataSet\ndf.shape\n","7e4c3933":"df.head()","ecae979c":"df = df.drop(['Patient age quantile'], axis = 1)","b3f7b472":"df.dtypes","328bdb41":"X=df","7f83964b":"y = X['SARS-Cov-2 exam result_positive']\nX= X.drop(['SARS-Cov-2 exam result_positive'],axis=1)","be6b2505":"# importando a biblioteca de split de dados do Sklearn\nfrom sklearn.model_selection import train_test_split\n# separando os dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","373dc93a":"# import logistic regression model and accuracy_score metric\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","69c14b94":"# instanciando o modelo\nclf = LogisticRegression()\n\n# ajustando o modelo com os dados de treino\nclf.fit(X_train, y_train)\n\n# fazendo predi\u00e7\u00f5es com os dados de teste\ny_pred = clf.predict(X_test)\n\n# imprimindo as principais m\u00e9tricas\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred))\nprint(\"Recall:\",recall_score(y_test, y_pred))\nprint ('F1 score:', f1_score(y_test, y_pred,average='weighted'))\n","87b32eb0":"print('Confusion Matrix ')\n# importando a biblioteca de m\u00e9tricas\nfrom sklearn import metrics\n# plotando uma matriz de confus\u00e3o\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","892f07e0":"# importando as bibliotecas com os modelos classificadores\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport matplotlib.pyplot as plt\n# definindo uma lista com todos os classificadores\nclassifiers = [\n    KNeighborsClassifier(3),\n    GaussianNB(),\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier()]\n\n\n# definindo o tamanho da figura para o gr\u00e1fico\nplt.figure(figsize=(12,8))\n\n# rotina para instanciar, predizer e medir os rasultados de todos os modelos\nfor clf in classifiers:\n    # instanciando o modelo\n    clf.fit(X_train, y_train)\n    # armazenando o nome do modelo na vari\u00e1vel name\n    name = clf.__class__.__name__\n    # imprimindo o nome do modelo\n    print(\"=\"*30)\n    print(name)\n    # imprimindo os resultados do modelo\n    print('****Results****')\n    y_pred = clf.predict(X_test)\n    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n    print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n    print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n    print ('F1 score:', f1_score(y_test,y_pred,average='weighted'))\n    # plotando uma matriz de confus\u00e3o\n    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n    print (cnf_matrix)","41cb4a22":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2200, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","20d85f05":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nmodel = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nmodel = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nmodel.fit(X_train, y_train)","9b1194ba":"model.best_params_","dd008fd3":"model.best_estimator_","f75ee8e1":"model_random = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=20, max_features='sqrt',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=1088,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\nmodel_random.fit(X_train, y_train)\nname = model_random.__class__.__name__\n#imprimindo os resultados do modelo\nprint('****Results****')\ny_pred = model_random.predict(X_test)\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\", metrics.recall_score(y_test, y_pred))\nprint ('F1 score:', f1_score(y_test,y_pred,average='weighted'))\n#plotando uma matriz de confus\u00e3o\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\nprint (cnf_matrix)","c51ff868":"cabecalho = [\"campos_originais\", \"importancia\"]\nls_imp = sorted(zip(X_train.columns, model_random.feature_importances_), key=lambda x: x[1] * 1)\ndf_imp = pd.DataFrame(ls_imp, columns=cabecalho)","6ec7ff07":"df_imp[\"importancia\"].sum()","d26925d8":"df_imp.head(204)","5df58099":"df_imp.isnull().sum()","bac25315":"df_importancia = df_imp.groupby(\"campos_originais\", as_index=False)[\"importancia\"].sum()","8dd0d470":"df_importancia = df_importancia.sort_values(by=\"importancia\")","993d050c":"df_importancia = df_importancia[df_importancia.importancia >= 0.015]","1f4ea2e2":"pyplot.barh(df_importancia[\"campos_originais\"], df_importancia[\"importancia\"], color=\"r\", align=\"center\")","f1148bec":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nscores = cross_val_score(model_random, X_train, y_train, cv=10, scoring ='accuracy')\nlist(scores)","7dc7b0f0":"scores.mean()","39691325":"y_pred_proba = model.predict_proba(X_test)[::,1] \nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=name+\", auc=\"+str(auc))\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\nplt.legend(loc=4)","619e8657":"# C\u00e1lculo KS\ny_pred_proba = model_random.predict_proba(X_test)\ny_pred_proba[0:100]","89cbf86a":"df_test = pd.DataFrame(X_test)\ndf_test[\"NoRisk\"] = 1 - y_test\ndf_test[\"Risk\"] = y_test\ndf_test[\"prob_0\"] = y_pred_proba[:,0]\ndf_test[\"prob_1\"] = y_pred_proba[:,1]","a008e610":"df_test[\"prob_1\"].plot(kind=\"hist\", bins=50)","75f618cc":"df_test[\"decil\"] = pd.qcut(df_test[\"prob_1\"], 10, labels=[10, 9, 8, 7, 6, 5, 4, 3, 2, 1])","166c7808":"decil_agrupado = df_test.groupby(\"decil\", as_index=False)\ndf_decil = pd.DataFrame(decil_agrupado.min().decil)","2beea0e5":"df_decil","e9823935":"df_decil[\"Prediction\"] = decil_agrupado.mean().prob_1\ndf_decil[\"min_scr\"] = decil_agrupado.min().prob_1\ndf_decil[\"max_scr\"] = decil_agrupado.max().prob_1\ndf_decil[\"NoRisk\"] = decil_agrupado.sum().NoRisk\ndf_decil[\"Risk\"] = decil_agrupado.sum().Risk\ndf_decil[\"total\"] = df_decil[\"Risk\"] + df_decil[\"NoRisk\"]","b767b375":"df_decil","75d1aa0d":"df_decil = (df_decil.sort_values(by=\"min_scr\", ascending=False)).reset_index(drop=True)\ndf_decil","ef724146":"df_decil[\"odds\"] = df_decil[\"NoRisk\"] \/ df_decil[\"Risk\"]\n\ndf_decil[\"Risk_rate\"] = df_decil[\"Risk\"] \/ df_decil[\"total\"]\n\ndf_decil[\"ks\"] = np.round(((df_decil[\"Risk\"] \/ df_test[\"Risk\"].sum()).cumsum() - \\\n                              (df_decil[\"NoRisk\"] \/ df_test[\"NoRisk\"].sum()).cumsum()), 4) * 100\n\ndf_decil[\"max_ks\"] = df_decil[\"ks\"].apply(lambda x: \"**\" if x == df_decil[\"ks\"].max() else \"\")\n\ndf_decil","13680f2f":"pyplot.plot(df_decil[\"decil\"].astype(int), df_decil[\"Prediction\"], marker=\"\", color=\"orange\", linewidth=4, label=\"Prediction\")\npyplot.plot(df_decil[\"decil\"].astype(int), df_decil[\"Risk_rate\"], marker=\"o\", markerfacecolor=\"blue\", markersize=6, \\\n            color=\"blue\", linewidth=2, label=\"Real\")\npyplot.xlabel(\"Decil\")\npyplot.legend()","1b9157f9":"### Model Fine Tuning by Parameters - Randon Search","8e0178bf":"### Data Setting Balancing: SMOTE\nAfter testing balanced and unbanced models, better perfomance with Unbalanced data Note the creation of several False positive. \nOn the other hand , it would reduce a little bit False Negatives.","6147138e":"### Identifying Key features that explain the model","3fbd71f2":" ## Conclusion","e0b74990":"Given the dataset still reduced , specially in positive cases, model above is somehow useful to predict if a patient has covid 19 or not. \n\nThis is  based on blood\/ urine exams, how he was admitted, and age. \n\nOn top of that, it is possible to predict the probalilty and also how it is positioned against the overall data.","d712d095":"### Splitting data in dependent and independent variables\n","56897b46":"## Dividing the sample by 10 equal  groups","0fb139ac":"### Exploratory Data Analysis","398e0091":"### Importing libraries","4c1430e1":"### Missing Data at Categoric Variables will be replaced by 'na'","29bf6517":"### NUMBER Of Positve Cases","50a0e91a":"### Missing Data at Numeric Variables will be replaced by the Mean","c70063cb":"============================== RandomForestClassifier\n\nResults\n\nAccuracy: 0.579273693534101\n\nPrecision: 0.1273100616016427\n\nRecall: 0.5535714285714286\n\nF1 score: 0.6634197169044735\n\n[[592 425]\n\n[ 50 62]]\n\n============================== GradientBoostingClassifier\n\nResults\n\nAccuracy: 0.5332152347209921\n\nPrecision: 0.12880143112701253\n\nRecall: 0.6428571428571429\n\nF1 score: 0.6229560947982687\n\n[[530 487]\n\n[ 40 72]]","62ee88b2":"### Identifying Missing Data and Necessary Adjusts","e06a5116":"## Individual Predictions of the Sample","5eb56893":"## TASK 1\n\u2022 Predict confirmed COVID-19 cases among suspected cases.\nBased on the results of laboratory tests commonly collected for a suspected COVID-19 case during a visit to the \nemergency room, would it be possible to predict the test result for SARS-Cov-2 (positive\/negative)?","ec537ebd":"Related to confusion Matrix and metrics it is important to remember:\n\nWhen binary classification, we get 4 outcomes:\n\nTrue Positive It is infected and prediction is correct)\n\nFalse Positive It is not Infected but prediction is that it is infected)\n\nFalse Negative It is infected, but prediction it is not)\n\nTrue Negative. It is not infected and prediction says correctley it is not.\n\nWe can visualize these at above matrix. We can also check metrics related to it:\n\nAccuracy: regarding what was classified, which part is correct. It is the first metric we see, but it is not enough to assure model quality because it does not specify what is happening with the errors.\n\nPrecision: portion of the data classified as positive that are really positive. Precision is a good measure to determine, when the costs of False Positive is high.\n\nRecall: portion of the data classifies as positive. Recall shows how model sees positive data.Recall actually calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive). Applying the same understanding, we know that Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.\n\nIt is a medical decision if prefers to be too cautious and accept more false positive or not. There is a trade off between resoruces and cautious to choose between more False Negatives and False Positives.\n\nAnyway,it was introduced f1 score to also help in this evaluation.","f603606c":"### Reading the dataset and assigning it  into Pandas dataframe","887c4eff":"By Grouping  in 10th parts, you can identify how your entire sample  is moving  as more data is added.\nPrediction Spam of each 10th can increase or decrease showing evolution of the data.\nAt same time , you can identify where   a indiviual result falls compared  to each other group of  sample. with higher or lower probability .","d9bd96b8":"## Cross Validation:\u00b6\nLet\u00b4s check accuracy and recall metric using Cross-Validation:\nThis way we check against Over fitting possibility","78510c52":"Per the classifliers results, we see as best perfomers Randon Forecast and Gradient Boosting, considering accuracy and F1-score.\n\nLet\u00b4s proceed with Randon Forest","f5ed0de6":"Changing Age Quantile to Categoric before hot enconding","dde69e48":"Applying best estimators in to the model parameters","6278c046":"## Most People are around 11% probability  of having  COVID.\n\nBy this it is possible to predict the chance of a single individual to have the disease, based mainly on the 20 high importance feature of a positive result.\n\nAs a follow on,  it is possible to create an  API where doctors would respond a questionnaire with these 20 variables and then,  to get the probability  of a patient having  this desease or not.","a45ea433":"### Standard Scaler\nAccording to descriptive, all variables are already standardized."}}