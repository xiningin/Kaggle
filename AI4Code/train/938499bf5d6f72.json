{"cell_type":{"7f3796da":"code","73fa5f26":"code","aea1718c":"code","051a518a":"code","39dde063":"code","8b200008":"code","0da44620":"code","9d249765":"code","5b728e2e":"code","9d70c237":"code","9959947e":"code","aaa9ae55":"code","62b28261":"code","0435b8ab":"code","e5543aad":"code","4c7eb5b6":"code","06b5a97e":"code","1257e420":"code","84035a85":"code","fb0c6c5d":"code","4ee04abf":"code","5043d79b":"code","87750834":"code","23df0282":"code","3ca14e55":"code","4fc30c57":"code","06d7e7eb":"code","f626507a":"code","5e3f1371":"code","442f0416":"code","3171aac3":"code","0b769fc8":"code","464b8a41":"code","7734e573":"code","49d42d6d":"code","b15c77b4":"code","5fb473a5":"code","2835aeaf":"code","a56a13ed":"code","fcd42dad":"code","9774b95b":"code","a2e28d77":"code","4011613e":"code","5796fedc":"code","6a44b978":"code","1791d545":"code","1c0fef3b":"code","3e6a0968":"code","9963e9b9":"code","2b4b99d0":"code","c4132ca2":"code","54076042":"code","c8904f73":"markdown","34f408ee":"markdown","0fcef497":"markdown","63e022fe":"markdown","161f1980":"markdown","525b6971":"markdown","230f490d":"markdown","8b795179":"markdown","ff70dc00":"markdown","881488e0":"markdown","c4aef0cc":"markdown","a424680f":"markdown","338c7a61":"markdown","777ec5f4":"markdown","678ae989":"markdown"},"source":{"7f3796da":"# Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nimport sys\nimport IPython\nfrom IPython import display \nimport sklearn \nimport random\nimport time","73fa5f26":"#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","aea1718c":"#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns","051a518a":"#Configure Visualization Defaults\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","39dde063":"# Importing the model. \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","8b200008":"#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit, cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score,classification_report, confusion_matrix","0da44620":"data_raw = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ndata_train = data_raw.copy(deep = True)\n# combining the two datasets for cleaning\ndata_to_be_cleaned = [data_train, data_test]","9d249765":"data_raw.info()\nprint('*-'*25)\ndata_test.info()","5b728e2e":"print('Train columns with null values:\\n', data_train.isnull().sum())\nprint(\"*-\"*25)\n\nprint('Test\/Validation columns with null values:\\n', data_test.isnull().sum())\nprint(\"*-\"*25)","9d70c237":"data_raw.describe(include = 'all')","9959947e":"###COMPLETING: complete or delete missing values in train and test\/validation dataset\nfor dataset in data_to_be_cleaned:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)","aaa9ae55":"data_to_be_cleaned","62b28261":"#delete the cabin feature\/column and others previously stated to exclude in train dataset\ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndata_train.drop(drop_column, axis=1, inplace = True)\n\nprint(data_train.isnull().sum())\nprint(\"-\"*50)\nprint(data_test.isnull().sum())","0435b8ab":"###CREATE: Feature Engineering for train and test\/validation dataset\nfor dataset in data_to_be_cleaned:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n\n    #quick and dirty code split title from name\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n\n    #Continuous variable bins\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    #Age Bins\/Buckets using cut or value bins\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)","e5543aad":"data_to_be_cleaned[0].info()","4c7eb5b6":"data_to_be_cleaned[1].info()","06b5a97e":"#cleanup rare title names\nprint(data_train['Title'].value_counts())\nprint('*-*'*10)\nstat_min = 10 \ntitle_names = (data_train['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code\ndata_train['Title'] = data_train['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data_train['Title'].value_counts())\nprint(\"-\"*50)","1257e420":"#preview data again\ndata_train.info()\ndata_test.info()\ndata_train.head(10)","84035a85":"data_test.shape","fb0c6c5d":"data_train.shape","4ee04abf":"#CONVERT: convert objects to category using Label Encoder for train and test\/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_to_be_cleaned:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n","5043d79b":"data_test","87750834":"#define y variable aka target\/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\noriginal_train_data = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name\/values for charts\ntrain_data = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','IsAlone', 'FamilySize', 'AgeBin_Code', 'FareBin_Code'] #coded for algorithm calculation\noriginal_dataset =  Target + original_train_data\nprint('Original X Y: ', original_dataset, '\\n')\nprint('Training X Y: ', train_data, '\\n')","23df0282":"print('Train columns with null values: \\n', data_train.isnull().sum())\nprint(\"-\"*10)\nprint (data_train.info())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values: \\n', data_test.isnull().sum())\nprint(\"-\"*10)\nprint (data_test.info())\nprint(\"-\"*10)","3ca14e55":"X = data_train[train_data]","4fc30c57":"Y = data_train[Target]","06d7e7eb":"#split train and test data with function defaults\n#random_state -> seed or control random number generator: https:\/\/www.quora.com\/What-is-seed-in-random-number-generation\nx_train, x_test, y_train, y_test = model_selection.train_test_split(data_train[train_data], data_train[Target],test_size= 0.2, random_state = 0)\n\nprint(\"Data given for training - Shape: {}\".format(data_train.shape))\nprint(\"Train split of the given training data - Shape: {}\".format(x_train.shape))\nprint(\"Test split of the tgiven training data - Shape: {}\".format(x_test.shape))\n\noriginal_refined_dataset = data_train[original_train_data]\n\nx_train.head()\n","f626507a":"original_refined_dataset","5e3f1371":"#Discrete Variable Correlation by Survival using\n#group by aka pivot table\nfor x in original_refined_dataset:\n    if data_train[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data_train[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs\nprint(pd.crosstab(data_train['Title'],data_train[Target[0]]))","442f0416":"\nplt.figure(figsize=[16,12])\n\nplt.subplot(231)\nplt.boxplot(x=data_train['Fare'], showmeans = True, meanline = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(data_train['Age'], showmeans = True, meanline = True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (Years)')\n\nplt.subplot(233)\nplt.boxplot(data_train['FamilySize'], showmeans = True, meanline = True)\nplt.title('Family Size Boxplot')\nplt.ylabel('Family Size (#)')\n\nplt.subplot(234)\nplt.hist(x = [data_train[data_train['Survived']==1]['Fare'], data_train[data_train['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x = [data_train[data_train['Survived']==1]['Age'], data_train[data_train['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x = [data_train[data_train['Survived']==1]['FamilySize'], data_train[data_train['Survived']==0]['FamilySize']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Family Size Histogram by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()","3171aac3":"#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data_train, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data_train, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data_train, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data_train, ax = saxis[1,0])\nsns.pointplot(x = 'AgeBin', y = 'Survived',  data=data_train, ax = saxis[1,1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=data_train, ax = saxis[1,2])","0b769fc8":"#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data_train, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data_train, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data_train, ax = axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","464b8a41":"#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data_train, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data_train, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data_train, ax  = qaxis[2])\naxis1.set_title('Sex vs IsAlone Survival Comparison')","7734e573":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data_train)","49d42d6d":"from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit, cross_val_score\n# Importing the model. \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score,classification_report, confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","b15c77b4":"knn = KNeighborsClassifier(metric='minkowski', p=2) \n## doing 10 fold staratified-shuffle-split cross validation \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=2)","5fb473a5":"## Search for an optimal value of k for KNN.\nk_range = range(1,31)\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, x_train , y_train , cv = cv, scoring = 'accuracy')\n    k_scores.append(scores.mean())\nprint(\"Accuracy scores are: {}\\n\".format(k_scores))\nprint (\"Mean accuracy score: {}\".format(np.mean(k_scores)))\nplt.plot(k_range, k_scores)","2835aeaf":"classifier = KNeighborsClassifier(n_neighbors=9)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)","a56a13ed":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","fcd42dad":"knn_accy = accuracy_score(y_pred, y_test)\nprint(knn_accy)","9774b95b":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X,Y)\n\ny_pred = gaussian.predict(x_test)\ngaussian_accy = accuracy_score(y_pred, y_test)\n\nprint(gaussian_accy)","a2e28d77":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","4011613e":"svclassifier = SVC(kernel='rbf')\n\nsvclassifier.fit(x_train, y_train)\ny_pred = svclassifier.predict(x_test)\n\nsvclassifier_accy = accuracy_score(y_pred, y_test)\nprint(svclassifier_accy)","5796fedc":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","6a44b978":"decision_tree_classifier =DecisionTreeClassifier()\n\ndecision_tree_classifier.fit(x_train, y_train)\ny_pred = decision_tree_classifier.predict(x_test)\n\ndecision_tree_classifier_accy = accuracy_score(y_pred, y_test)\nprint(decision_tree_classifier_accy)","1791d545":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","1c0fef3b":"validation_data = data_test","3e6a0968":"validation_data","9963e9b9":"X_val = validation_data[train_data]\nval_prediction = decision_tree_classifier.predict(X_val)\npassengerid = validation_data[\"PassengerId\"]","2b4b99d0":"submission = pd.DataFrame({\n        \"PassengerId\": passengerid,\n        \"Survived\": val_prediction\n    })","c4132ca2":"submission","54076042":"submission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic1_submission.csv\", index=False)","c8904f73":"## Split Training and Testing Data","34f408ee":"**This is my first Kaggle Notebook. I hope you all liked it. I would also like to improve my score and make this notebook even better. Any kind of help will be much appreciated**","0fcef497":"## Cleaning the dataset","63e022fe":"### Kaggle Competition | Titanic Machine Learning from Disaster\n\n>The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\n>One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n>In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n>This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n\nFrom the competition [homepage](http:\/\/www.kaggle.com\/c\/titanic-gettingStarted).\n\n\n### Goal for this Notebook:\nShow a simple example of an analysis of the Titanic disaster in Python. This is aimed for those looking to get into the field or those who are already in the field and looking to see an example of an analysis done with Python.\n\n#### This Notebook will show basic examples of: \n#### Data Handling\n*   Importing Data with Pandas\n*   Cleaning Data\n*   Exploring Data through Visualizations with Matplotlib and seaborn\n\n#### Data Analysis\n*    Supervised Machine learning Techniques:\n    +   K Nearest Neighbour \n    +   Gaussian Naive Bayes\n    +   Support Vector Machine (SVM)\n    +   Decision Tree Classifier\n    +   Plotting results","161f1980":"#### SVM Classifier Model","525b6971":"If you liked the contents of this notebook, an upvote would be much appreciated. ","230f490d":"## Perform Exploratory Analysis with Statistics","8b795179":"## Model Data","ff70dc00":"### Describing and analysing the given data sets for better understanding of the data provided","881488e0":"#### KNN Model","c4aef0cc":"#### Gaussian Naive Bayes Model","a424680f":"#### Double check Cleaned Data","338c7a61":"## Importing the required Libraries","777ec5f4":"#### Decision Tree Model","678ae989":"**The maximum accuracy that was obtained is 83.8% of Decision Tree model. This model will be used to predict the values of the validation test data.**"}}