{"cell_type":{"a4cb00b7":"code","88037a64":"code","cc6cbbb3":"code","4eb1d01b":"code","0ac17f1c":"code","5760eebd":"code","ae468cdb":"code","93929bc8":"code","98345208":"markdown","6186430f":"markdown","1ee5886e":"markdown","b930a65a":"markdown","b9c8fdf7":"markdown","03308247":"markdown","6a19c766":"markdown","5d988025":"markdown"},"source":{"a4cb00b7":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"..\/input\/news_en.csv\", sep=',',index_col = \"id\")\nprint(df.shape)\ndf.head()","88037a64":"text = df[\"Headline\"].str.cat(sep='\\n')\ntext_size = len(text)\nprint('Text Size: %d' % text_size)","cc6cbbb3":"from pickle import dump\n\nchars = sorted(list(set(text)))\nmapping = dict((c, i) for i, c in enumerate(chars))\ndump(mapping, open('mapping.pkl', 'wb'))\n\nvocab_size = len(mapping)\nprint('Vocabulary Size: %d' % vocab_size)","4eb1d01b":"encoded_text = [mapping[char] for char in text]\nencode_size = len(encoded_text)\nprint('Code Size: %d' % encode_size)","0ac17f1c":"seqlen = 10\nbatchsize = 512\nbatchnum = int((encode_size - seqlen) \/ batchsize)\n\nfrom keras.utils import to_categorical\n\ndef myGenerator():\n    while 1:\n        for i in range(batchnum): \n            X_batch = []\n            y_batch = []\n            for j in range(batchsize):\n                X_batch.append(encoded_text[i*batchsize+j:i*batchsize+j+seqlen])\n                y_batch.append(encoded_text[i*batchsize+j+seqlen:i*batchsize+j+seqlen+1])\n                \n            X_batch = np.array([to_categorical(x, num_classes=vocab_size) for x in X_batch])\n            y_batch = np.array(to_categorical(y_batch, num_classes=vocab_size))\n\n            yield (X_batch, y_batch)","5760eebd":"from keras.models import Sequential\nfrom keras.layers import Input, Dense, LSTM, SimpleRNN\nfrom keras.models import Model\n\nmodel = Sequential()\nmodel.add(LSTM(300, return_sequences=True, input_shape=(seqlen, vocab_size)))\nmodel.add(LSTM(150, return_sequences=True))\nmodel.add(LSTM(75))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())","ae468cdb":"my_generator = myGenerator()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit_generator(my_generator, steps_per_epoch = batchnum, epochs = 50, verbose=1)\nmodel.save('model_3lay_50.h5')","93929bc8":"from pickle import load\nfrom keras.models import load_model\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nimport random\n \ndef generate_seq(model, mapping, seq_length, seed_text, n_chars):\n    in_text = seed_text\n    for _ in range(n_chars):\n        encoded = [mapping[char2] for char2 in in_text]\n        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n        encoded = to_categorical(encoded, num_classes=len(mapping))\n        probs = model.predict_proba(encoded)\n        yhat = random.choices(range(0,vocab_size), weights=probs[0], k=1)[0]\n        out_char = ''\n        for char, index in mapping.items():\n            if index == yhat:\n                out_char = char\n                break\n        in_text += out_char\n        if char ==\"\\n\":\n            break\n    return in_text\n \nmodel = load_model('model_3lay_20.h5')\nmapping = load(open('mapping.pkl', 'rb'))\n \nprint(generate_seq(model, mapping, seqlen, 'Tump tells', 400))\nprint(generate_seq(model, mapping, seqlen, 'Erdogan is', 400))\nprint(generate_seq(model, mapping, seqlen, 'Clinton is', 400))","98345208":"## Generate new Texts","6186430f":"## Generate Batches of Length 10","1ee5886e":"## Train Model","b930a65a":"## Mapping Characters to Numbers","b9c8fdf7":"## Encode String as Numbers","03308247":"## String of all Headlines","6a19c766":"## Loading Texts","5d988025":"## Define Model"}}