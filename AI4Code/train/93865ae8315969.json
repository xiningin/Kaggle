{"cell_type":{"a6d3da3f":"code","0af0b315":"code","0fe81a87":"code","e1b6aa15":"code","b3603e3d":"code","8b89086e":"code","16dfdc1c":"code","d9a42dc1":"code","250831db":"code","44103d3c":"code","4cd67a41":"code","529b1369":"code","afebf37e":"code","6dd5b876":"code","b83e34a3":"code","71a9eebb":"code","d45934c2":"code","1a598cf4":"code","0c41cab7":"code","1fccbf49":"code","6976233f":"code","5aca063f":"code","e8dfb2dd":"code","ac8028f1":"code","64c10fc6":"code","bb58ed91":"code","5f108793":"code","453199ab":"code","e94fc3e6":"code","bba601bc":"code","2c58ca25":"code","6c802e3f":"code","1a5f4c09":"code","a56d219d":"code","9b8574af":"code","b09dd629":"code","712ae282":"code","8635c17d":"code","42297c58":"code","d390c897":"code","8017d18a":"code","bbc68750":"code","83d39186":"code","15ee92a8":"code","811a100c":"code","89aba5ac":"code","e24159b5":"code","36e42c18":"code","93aa119e":"code","6324538a":"code","5b92e920":"code","66ccdb88":"code","c1752a2f":"code","e2260b9c":"code","ca5dc0a9":"code","940a8bf7":"code","b3058deb":"code","a20177dd":"code","1dae9268":"code","2b7b6b51":"code","28e799d3":"code","cfb7d590":"code","749652c7":"code","dabf920a":"code","f6f2f087":"code","5d513f3e":"code","1c808696":"code","a120077f":"code","c3b34fbb":"code","c75a557d":"code","cc84ba10":"code","270ed3a7":"code","1f9e0d27":"code","2adac04b":"code","35aec0bc":"code","7538ec95":"code","fc2275bd":"code","fc266362":"code","98258844":"code","d7947e7e":"code","56f8c76f":"code","e8f8f0b1":"code","3a967ade":"code","72eca9da":"code","1afd1c2c":"code","07fe8a79":"code","171223cb":"code","de10228d":"code","56caf419":"code","b47f909d":"code","6cbe83ca":"code","f15b6486":"code","65b4b529":"code","42adc3de":"code","ab521ca9":"code","513c0b75":"code","9da7854a":"code","aecdf2d8":"code","fa31c1f5":"code","f431c8d6":"code","f1d533d1":"code","0be6ceda":"code","3d277783":"code","c1d28c36":"code","b6d52271":"code","08a28adf":"markdown","b079df1e":"markdown","c261b218":"markdown","b4473329":"markdown","415364ac":"markdown","ba30027d":"markdown","394bef32":"markdown","c701db65":"markdown","b5a3bbb0":"markdown","6ab76c8f":"markdown","a2f7665c":"markdown","31c941c4":"markdown","951d042d":"markdown"},"source":{"a6d3da3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0af0b315":"df = pd.read_csv(\"..\/input\/BSESN.csv\")","0fe81a87":"df.head(3)","e1b6aa15":"from sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.layers import TimeDistributed\nfrom keras.layers import Bidirectional","b3603e3d":"import pandas as pd\nimport numpy as np\nfrom fbprophet import Prophet\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#setting figure size\nfrom matplotlib.pyplot import rcParams\nrcParams['figure.figsize'] = 20,10\n\n# Plot styles\nsns.set_style(\"whitegrid\")\nplt.style.use('fivethirtyeight')\n\n#for normalizing data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\n\nimport warnings\nwarnings.filterwarnings('ignore')","8b89086e":"np.random.seed(42)","16dfdc1c":"df.dropna(inplace=True)","d9a42dc1":"df.head(3)","250831db":"df['Date'] = pd.to_datetime(df.Date,format='%Y-%m-%d')\ndf.index = df['Date']","44103d3c":"plt.figure(figsize=(16,8))\nplt.plot(df['Adj Close'], label='Close Price history')","4cd67a41":"#setting index as date values\ndf['Date'] = pd.to_datetime(df.Date,format='%Y-%m-%d')\ndf.index = df['Date']\n\n#sorting\ndata = df.sort_index(ascending=True, axis=0)\n\n#creating a separate dataset\nnew_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n\nfor i in range(0,len(data)):\n    new_data['Date'][i] = data['Date'][i]\n    new_data['Close'][i] = data['Adj Close'][i]","529b1369":"new_data.head(3)","afebf37e":"new_data.index = new_data['Date']\nnew_data.drop('Date', inplace=True, axis=1)\nnew_data.head(2)","6dd5b876":"plt.figure(figsize=(10,5))\nplt.plot(new_data)","b83e34a3":"# 2006-2008 plotting\nplt.figure(figsize=(10,5))\nplt.plot(new_data.ix['2006':'2010'])","71a9eebb":"# 2007-2009 plotting\nplt.figure(figsize=(10,5))\nplt.plot(new_data.ix['2007':'2009'])","d45934c2":"train_data = new_data.ix[:'2014']\ntest_raw_data = new_data.ix['2015':]","1a598cf4":"train_data.head(3)","0c41cab7":"test_raw_data.head(3)","1fccbf49":"train_data.tail(3)","6976233f":"test_raw_data.tail(3)","5aca063f":"train_data.shape, test_raw_data.shape","e8dfb2dd":"train_data.head(5)","ac8028f1":"train_data = scaler.fit_transform(train_data[['Close']])\ntest_data = scaler.fit_transform(test_raw_data[['Close']])","64c10fc6":"train_data","bb58ed91":"scaler.inverse_transform(train_data)","5f108793":"print(\"Shape of train data: \" + str(train_data.shape))\nprint(\"Shape of test data: \" + str(test_data.shape))","453199ab":"## Create Dataset for LSTM\n\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    \n    return np.array(dataX), np.array(dataY)  ","e94fc3e6":"look_back = 3\n\ntrainX, trainY = create_dataset(train_data, look_back)\ntestX, testY = create_dataset(test_data, look_back)","bba601bc":"pd.DataFrame(trainX).head(5)","2c58ca25":"pd.DataFrame(trainY).head(5)","6c802e3f":"print(\"Shape of train input: \" + str(trainX.shape))\nprint(\"Shape of train labels: \" + str(trainY.shape))\nprint(\"Shape of test input: \" + str(testX.shape))\nprint(\"Shape of test labels: \" + str(testY.shape))","1a5f4c09":"trainX.shape","a56d219d":"testX.shape","9b8574af":"## Reshaping the Data for LSTM\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","b09dd629":"print(\"Shape of train input: \" + str(trainX.shape))\nprint(\"Shape of train labels: \" + str(trainY.shape))\nprint(\"Shape of test input: \" + str(testX.shape))\nprint(\"Shape of test labels: \" + str(testY.shape))","712ae282":"%%time\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(1, look_back)))\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.summary()","8635c17d":"%%time\nhistory = model.fit(trainX, trainY, epochs=1000, batch_size=5, validation_data=(testX, testY), verbose=2,\n                   shuffle=False)","42297c58":"model.evaluate(testX, testY)","d390c897":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","8017d18a":"trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)","bbc68750":"trainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","83d39186":"from sklearn.metrics import mean_absolute_error\n\ntrainScore_1_mae = mean_absolute_error(trainY[0], trainPredict[:,0])\nprint('Train Score: %.2f MAE' % (trainScore_1_mae))\ntestScore_1_mae = mean_absolute_error(testY[0], testPredict[:,0])\nprint('Test Score: %.2f MAE' % (testScore_1_mae))","15ee92a8":"def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","811a100c":"trainScore_1_mape = mean_absolute_percentage_error(trainY[0], trainPredict[:,0])\nprint('Train Score: %.2f MAPE' % (trainScore_1_mape))\ntestScore_1_mape = mean_absolute_percentage_error(testY[0], testPredict[:,0])\nprint('Test Score: %.2f MAPE' % (testScore_1_mape))","89aba5ac":"## Reshaping the Data for LSTM\ntrainX, trainY = create_dataset(train_data, look_back)\ntestX, testY = create_dataset(test_data, look_back)\n\ntrainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n\nprint(\"Shape of train input: \" + str(trainX.shape))\nprint(\"Shape of train labels: \" + str(trainY.shape))\nprint(\"Shape of test input: \" + str(testX.shape))\nprint(\"Shape of test labels: \" + str(testY.shape))\n\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(look_back, 1)))\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.summary()","e24159b5":"%%time\nhistory = model.fit(trainX, trainY, epochs=1000, batch_size=5, validation_data=(testX, testY),verbose=2, shuffle=False)","36e42c18":"model.evaluate(testX, testY)","93aa119e":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","6324538a":"trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)","5b92e920":"trainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","66ccdb88":"from sklearn.metrics import mean_absolute_error\n\ntrainScore_2_mae = mean_absolute_error(trainY[0], trainPredict[:,0])\nprint('Train Score: %.2f MAE' % (trainScore_2_mae))\ntestScore_2_mae = mean_absolute_error(testY[0], testPredict[:,0])\nprint('Test Score: %.2f MAE' % (testScore_2_mae))","c1752a2f":"trainScore_2_mape = mean_absolute_percentage_error(trainY[0], trainPredict[:,0])\nprint('Train Score: %.2f MAPE' % (trainScore_2_mape))\ntestScore_2_mape = mean_absolute_percentage_error(testY[0], testPredict[:,0])\nprint('Test Score: %.2f MAPE' % (testScore_2_mape))","e2260b9c":"## Reshaping the Data for LSTM\ntrainX, trainY = create_dataset(train_data, look_back)\ntestX, testY = create_dataset(test_data, look_back)\n\ntrainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n\nprint(\"Shape of train input: \" + str(trainX.shape))\nprint(\"Shape of train labels: \" + str(trainY.shape))\nprint(\"Shape of test input: \" + str(testX.shape))\nprint(\"Shape of test labels: \" + str(testY.shape))\n\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(look_back, 1)))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.summary()","ca5dc0a9":"%%time\nhistory = model.fit(trainX, trainY, epochs=1000, batch_size=5, validation_data=(testX, testY),verbose=2, shuffle=False)","940a8bf7":"model.evaluate(testX, testY)","b3058deb":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","a20177dd":"trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)","1dae9268":"trainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","2b7b6b51":"from sklearn.metrics import mean_absolute_error\n\ntrainScore_3_mae = mean_absolute_error(trainY[0], trainPredict[:,0])\nprint('Train Score: %.2f MAE' % (trainScore_3_mae))\ntestScore_3_mae = mean_absolute_error(testY[0], testPredict[:,0])\nprint('Test Score: %.2f MAE' % (testScore_3_mae))","28e799d3":"trainScore_3_mape = mean_absolute_percentage_error(trainY[0], trainPredict[:,0])\nprint('Train Score: %.2f MAPE' % (trainScore_3_mape))\ntestScore_3_mape = mean_absolute_percentage_error(testY[0], testPredict[:,0])\nprint('Test Score: %.2f MAPE' % (testScore_3_mape))","cfb7d590":"## Reshaping the Data for LSTM\ntrainX, trainY = create_dataset(train_data, look_back)\ntestX, testY = create_dataset(test_data, look_back)\n\ntrainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n\nprint(\"Shape of train input: \" + str(trainX.shape))\nprint(\"Shape of train labels: \" + str(trainY.shape))\nprint(\"Shape of test input: \" + str(testX.shape))\nprint(\"Shape of test labels: \" + str(testY.shape))\n\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True),  input_shape=(look_back, 1)))\nmodel.add(Bidirectional(LSTM(50, activation='relu')))\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.summary()","749652c7":"%%time\nhistory = model.fit(trainX, trainY, epochs=1000, batch_size=5, validation_data=(testX, testY),verbose=2, shuffle=False)","dabf920a":"model.evaluate(testX, testY)","f6f2f087":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","5d513f3e":"trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)","1c808696":"trainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","a120077f":"from sklearn.metrics import mean_absolute_error\n\ntrainScore_4_mae = mean_absolute_error(trainY[0], trainPredict[:,0])\nprint('Train Score: %.2f MAE' % (trainScore_4_mae))\ntestScore_4_mae = mean_absolute_error(testY[0], testPredict[:,0])\nprint('Test Score: %.2f MAE' % (testScore_4_mae))","c3b34fbb":"trainScore_4_mape = mean_absolute_percentage_error(trainY[0], trainPredict[:,0])\nprint('Train Score: %.2f MAPE' % (trainScore_4_mape))\ntestScore_4_mape = mean_absolute_percentage_error(testY[0], testPredict[:,0])\nprint('Test Score: %.2f MAPE' % (testScore_4_mape))","c75a557d":"type(trainScore_1_mae)","cc84ba10":"mae_score = [trainScore_1_mae, trainScore_2_mae, trainScore_3_mae, trainScore_4_mae]\ndf_mae_train = pd.DataFrame(mae_score, index=['trainScore_1_mae', 'trainScore_2_mae', 'trainScore_3_mae', 'trainScore_4_mae'], \n            columns =['values'])","270ed3a7":"mape_score = [trainScore_1_mape, trainScore_2_mape, trainScore_3_mape, trainScore_4_mape]\ndf_mape_train = pd.DataFrame(mape_score, index=['trainScore_1_mape', 'trainScore_2_mape', 'trainScore_3_mape', 'trainScore_4_mape'], \n            columns =['values'])","1f9e0d27":"mae_score = [testScore_1_mae, testScore_2_mae, testScore_3_mae, testScore_4_mae]\ndf_mae_test = pd.DataFrame(mae_score, index=['testScore_1_mae', 'testScore_2_mae', 'testScore_3_mae', 'testScore_4_mae'], \n            columns =['values'])","2adac04b":"mape_score = [testScore_1_mape, testScore_2_mape, testScore_3_mape, testScore_4_mape]\ndf_mape_test = pd.DataFrame(mape_score, index=['testScore_1_mape', 'testScore_2_mape', 'testScore_3_mape', 'testScore_4_mape'], \n            columns =['values'])","35aec0bc":"df_all_train = df_mae_train.append(df_mape_train)\ndf_all_test = df_mae_test.append(df_mape_test)","7538ec95":"df_all_train","fc2275bd":"df_all_test","fc266362":"# time-series to supervised (www.machinelearningmastery.com)\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:    \n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    \n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n        return agg","98258844":"new_data.head()","d7947e7e":"new_data.shape","56f8c76f":"new_data_date = new_data.copy()\nnew_data_date.reset_index(inplace=True)","e8f8f0b1":"# We will create a number of features on the Dates\nnew_data_date['year'] = new_data_date['Date'].map(lambda x : x.year)\nnew_data_date['month'] = new_data_date['Date'].map(lambda x : x.month)\nnew_data_date['day_week'] = new_data_date['Date'].map(lambda x : x.dayofweek)\nnew_data_date['quarter'] = new_data_date['Date'].map(lambda x : x.quarter)\nnew_data_date['week'] = new_data_date['Date'].map(lambda x : x.week)\nnew_data_date['quarter_start'] = new_data_date['Date'].map(lambda x : x.is_quarter_start)\nnew_data_date['quarter_end'] = new_data_date['Date'].map(lambda x : x.is_quarter_end)\nnew_data_date['month_start'] = new_data_date['Date'].map(lambda x : x.is_month_start)\nnew_data_date['month_end'] = new_data_date['Date'].map(lambda x : x.is_month_end)\nnew_data_date['year_start'] = new_data_date['Date'].map(lambda x : x.is_year_start)\nnew_data_date['year_end'] = new_data_date['Date'].map(lambda x : x.is_year_end)\nnew_data_date['week_year'] = new_data_date['Date'].map(lambda x : x.weekofyear)\nnew_data_date['quarter_start'] = new_data_date['quarter_start'].map(lambda x: 0 if x is False else 1)\nnew_data_date['quarter_end'] = new_data_date['quarter_end'].map(lambda x: 0 if x is False else 1)\nnew_data_date['month_start'] = new_data_date['month_start'].map(lambda x: 0 if x is False else 1)\nnew_data_date['month_end'] = new_data_date['month_end'].map(lambda x: 0 if x is False else 1)\nnew_data_date['year_start'] = new_data_date['year_start'].map(lambda x: 0 if x is False else 1)\nnew_data_date['year_end'] = new_data_date['year_end'].map(lambda x: 0 if x is False else 1)\nnew_data_date['day_month'] = new_data_date['Date'].map(lambda x: x.daysinmonth)\n# Create a feature which could be important - Markets are only open between Monday and Friday.\nmon_fri_list = [0,4]\nnew_data_date['mon_fri'] = new_data_date['day_week'].map(lambda x: 1 if x in mon_fri_list  else 0)\n# It has been proved in many studies worldwide that winters are better for return on stocks than summers. \n# We will see how true is this in this case.\nsecond_half = [7, 8, 9, 10, 11, 12]\nnew_data_date['half_year'] = new_data_date['month'].map(lambda x: 1 if x in second_half  else 0)\n# Election Years\nelec_year = [1998, 1999, 2004, 2009, 2014]\nnew_data_date['elec_year'] = new_data_date['year'].map(lambda x: 1 if x in elec_year  else 0)","3a967ade":"new_data_date.head()","72eca9da":"new_data_date.shape","1afd1c2c":"new_data_date.index = new_data_date['Date']\nnew_data_date.drop('Date', axis=1, inplace=True)\nnew_data_date.head()","07fe8a79":"new_data_date.shape","171223cb":"columns_to_encode = ['year', 'month', 'day_week', \n                    'quarter', 'week',  'week_year', 'day_month']\n\ncolumns_to_scale  = ['Close']\n\nother_cols = ['quarter_start', 'quarter_end', 'month_start', 'month_end',\n                    'year_start', 'year_end', 'mon_fri', 'half_year', 'elec_year']\n\nfrom sklearn.preprocessing import OneHotEncoder\nscaler = MinMaxScaler(feature_range=(0,1))\nohe    = OneHotEncoder(sparse=False)\n\nscaled_columns  = scaler.fit_transform(new_data_date[columns_to_scale]) \nencoded_columns = ohe.fit_transform(new_data_date[columns_to_encode])\n\nrev_new_data = np.concatenate([scaled_columns, new_data_date[other_cols], encoded_columns], axis=1)","de10228d":"rev_new_data.shape","56caf419":"# specify the number of lag days\nn_days = 3\nn_features = rev_new_data.shape[1]\n# frame as supervised learning\nreframed = series_to_supervised(rev_new_data, n_days, 1)","b47f909d":"reframed.head()","6cbe83ca":"reframed.shape","f15b6486":"rev_new_data.shape","65b4b529":"cols_to_remove = rev_new_data.shape[1]-1\nprint(cols_to_remove)\nreframed_new = reframed.iloc[:, :-cols_to_remove]","42adc3de":"reframed_new.head()","ab521ca9":"reframed_new.shape","513c0b75":"values = reframed_new.values\nn_train_time = 365*10\n\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\n\nn_obs = n_days * n_features\ntrain_X, train_y = train[:, :n_obs], train[:, -(n_features+1)]\ntest_X, test_y = test[:, :n_obs], test[:, -(n_features+1)]\nprint(train_X.shape, len(train_X), train_y.shape)\n\n#train_X, train_y = train[:, :-1], train[:, -1]\n#test_X, test_y = test[:, :-1], test[:, -1]\n\n#train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n#test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))","9da7854a":"pd.DataFrame(train_X)[:5]","aecdf2d8":"train_X = train_X.reshape((train_X.shape[0], n_days, n_features))\ntest_X = test_X.reshape((test_X.shape[0], n_days, n_features))\n\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)","fa31c1f5":"print(n_obs)\nprint(n_days)\nprint(n_features)","f431c8d6":"test_y","f1d533d1":"import keras\nfrom keras.layers import Activation, BatchNormalization\nmodel = Sequential()\n\n#Input\nmodel.add(Bidirectional(LSTM(500, activation='relu', return_sequences=True), \n                        input_shape=(train_X.shape[1], train_X.shape[2])))\n#model.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\n# middle\nmodel.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\n#model.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\n# middle\nmodel.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\n#model.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\n# middle\nmodel.add(Bidirectional(LSTM(50, activation='relu')))\n#model.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\n#Output\nmodel.add(Dense(1, activation='linear'))\n#model.add(BatchNormalization())\n\noptimizer = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='mse', optimizer=optimizer)\nmodel.summary()","0be6ceda":"history = model.fit(train_X, train_y, epochs=1500, batch_size=len(train_X), \n                    validation_data=(test_X, test_y), verbose=2, shuffle=False)","3d277783":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","c1d28c36":"np.mean(history.history['val_loss'])","b6d52271":"model.evaluate(test_X, test_y, verbose=2, batch_size=92)","08a28adf":"# Bidirectional LSTM","b079df1e":"# Loading Libraries","c261b218":"# LSTM (time-step = 1)","b4473329":"# Multivariate Time Series\n## Adding Features and Running LSTM model","415364ac":"# Preparing Data for LSTM","ba30027d":"# Basic checks on Data","394bef32":"# Scaling the Data","c701db65":"# Train and Test","b5a3bbb0":"# LSTM (time-step = no. of columns)","6ab76c8f":"# LSTM Stacked","a2f7665c":"# Loading the Data","31c941c4":"This is clear from above training and test scores that LSTM with timesteps as no. of columns is the best LSTM model (model number 2).\n\nThere are few clear observations:\n\n1. No. of time steps as 1 is clearly not a preferable approach.\n2. Stacked LSTM is performing well than non-stacked LSTM (in terms of closeness between training loss and test loss)\n3. Bidirection LSTM performance is equally better\n\n","951d042d":"# Univariate Time-series (LSTM)\n\n\n\nIn this section, we will review few methods of LSTM and see which one performs the best on the Univariate time-series where only time is the feature in our data."}}