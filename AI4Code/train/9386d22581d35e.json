{"cell_type":{"3d548a85":"code","41a28a05":"code","678e5222":"code","355e7ebc":"code","e9202cdb":"code","e8e30ac2":"code","698cc0cd":"code","9fb79ac8":"code","70ddfc31":"code","a034dd6a":"code","523978fc":"code","8384d7bf":"code","2211d2cc":"code","2b55bb53":"code","b86397d1":"code","c68e75b9":"code","bc27287c":"code","03f5a0c1":"code","f4a7db1d":"code","d070ed50":"code","201ea46a":"code","a3dc6855":"code","4cea07e7":"code","f437c6a5":"code","53567ee3":"code","0cc4305c":"code","e7e845ba":"code","3fbb6768":"code","4d403fac":"code","b910fdf6":"code","26e115f0":"code","4e3be76f":"code","0b8905ad":"code","de56e177":"code","ec5a7865":"code","4e4075ea":"code","2ba6ef01":"code","20c1ce54":"code","697c5cf1":"code","14633319":"code","dec58d98":"code","f5abd5bd":"code","1e419e80":"code","7c8acfc4":"code","9fbd98b6":"code","1ade104b":"code","b8da6fc0":"code","d5427151":"code","d8ed1363":"code","4b0a8a48":"code","77592dfc":"code","1d54a1ed":"markdown","4dd8a2e2":"markdown","a6d0ad14":"markdown","6563fc76":"markdown"},"source":{"3d548a85":"# Predict survival on the Titanic\n# Import libararies\n# linear algebra\nimport numpy as np\n# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport re\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Data science packages\nfrom scipy import stats\nfrom scipy.stats import norm\n# Data Preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Import Models\n# Binary classification model(https:\/\/stackabuse.com\/classification-in-python-with-scikit-learn-and-pandas\/)\n# https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.linear_model\nfrom sklearn.linear_model import LogisticRegression\n# https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\nfrom sklearn.ensemble import RandomForestClassifier\n# https:\/\/qiita.com\/kazuki_hayakawa\/items\/18b7017da9a6f73eba77\n# https:\/\/qiita.com\/arata-honda\/items\/8d08f31aa7d7cbae4c91\nfrom sklearn import svm\n# https:\/\/spjai.com\/neural-network-parameter\/\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html\nfrom sklearn.neural_network import MLPClassifier\n\n# Base class for all estimators in scikit-learn\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n# Mixin class for all classifiers in scikit-learn\nfrom sklearn.base import ClassifierMixin\n# Accuracy = (TP + TN) \/ (TP + TN + FP + FN)\nfrom sklearn.metrics import accuracy_score\n# Cross-validation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_predict\n\n# utilities\nfrom datetime import datetime\n# Disable warning output\nimport warnings\nwarnings.filterwarnings('ignore')\n# Show matplot graph\n%matplotlib inline","41a28a05":"# Read data from train dataset\ndf_train = pd.read_csv('..\/input\/train.csv')\n#check the columns\nprint(df_train.columns)\nprint(df_train.dtypes)\n# Read data from test dataset\ndf_test = pd.read_csv('..\/input\/test.csv')\nprint(df_test.columns)","678e5222":"df_train.info()","355e7ebc":"df_train.describe()","e9202cdb":"fig = plt.figure(figsize=(18, 12))\nfig.set(alpha=0.2)  # alpha for graph color\n\nplt.subplot2grid((2,3),(0,0))             # subplot in one parent plot\ndf_train.Survived.value_counts().plot(kind='bar')# bar graph \nplt.title(u\"Survive (1:Survived)\") # title\nplt.ylabel(u\"Passenger Count\")  \n\nplt.subplot2grid((2,3),(0,1))\ndf_train.Pclass.value_counts().plot(kind=\"bar\")\nplt.ylabel(u\"Passenger Count\")\nplt.title(u\"Pclass\")\n\nplt.subplot2grid((2,3),(0,2))\nplt.scatter(df_train.Survived, df_train.Age)\nplt.ylabel(u\"Age\")                         # y label name\nplt.grid(b=True, which='major', axis='y') \nplt.title(u\"Survive (1:Survived) by Age\")\n\n\nplt.subplot2grid((2,3),(1,0), colspan=2)\ndf_train.Age[df_train.Pclass == 1].plot(kind='kde')   \ndf_train.Age[df_train.Pclass == 2].plot(kind='kde')\ndf_train.Age[df_train.Pclass == 3].plot(kind='kde')\nplt.xlabel(u\"Age\")# plots an axis lable\nplt.ylabel(u\"Density\") \nplt.title(u\"Age distribution by pclass\")\nplt.legend((u'p1', u'p2',u'p3'),loc='best') # sets our legend for our graph.\n\n\nplt.subplot2grid((2,3),(1,2))\ndf_train.Embarked.value_counts().plot(kind='bar')\nplt.title(u\"Passenger Count by Embarked\")\nplt.ylabel(u\"Passenger Count\")\nsns.set()\nplt.show()","e8e30ac2":"#Passenger Suvived by Pclass\nfig = plt.figure(figsize=(8, 6))\nfig.set(alpha=0.2)  # alpha for graph color\n\nSurvived_0 = df_train.Pclass[df_train.Survived == 0].value_counts()\nSurvived_1 = df_train.Pclass[df_train.Survived == 1].value_counts()\ndf=pd.DataFrame({u'Survived':Survived_1, u'Not Survived':Survived_0})\ndf.plot(kind='bar', stacked=True)\nplt.title(u\"Passenger Suvived by Pclass\")\nplt.xlabel(u\"Pclass\") \nplt.ylabel(u\"Passenger Count\") \nplt.show()","698cc0cd":"#Passenger Suvived by Sex\nfig = plt.figure(figsize=(8, 6))\nfig.set(alpha=0.2)  # alpha for graph color\n\nSurvived_m = df_train.Survived[df_train.Sex == 'male'].value_counts()\nSurvived_f = df_train.Survived[df_train.Sex == 'female'].value_counts()\ndf=pd.DataFrame({u'Male':Survived_m, u'Female':Survived_f})\ndf.plot(kind='bar', stacked=True)\nplt.title(u\"Passenger Survived by Sex\")\nplt.xlabel(u\"Sex\") \nplt.ylabel(u\"Passenger Count\")\nplt.show()","9fb79ac8":"#Passenger Suvived by combined Pclass and Sex\nfig=plt.figure(figsize=(18, 16))\nfig.set(alpha=0.65) # alpha for graph color\nplt.title(u\"Passenger Suvived by combined Pclass and Sex\")\n\nax1=fig.add_subplot(141)\ndf_train.Survived[df_train.Sex == 'female'][df_train.Pclass != 3].value_counts().plot(kind='bar', label=\"female highclass\", color='#FA2479')\nax1.set_xticklabels([u\"Survived\", u\"Not Survived\"], rotation=0)\nax1.legend([u\"Female\/High Class\"], loc='best')\n\nax2=fig.add_subplot(142, sharey=ax1)\ndf_train.Survived[df_train.Sex == 'female'][df_train.Pclass == 3].value_counts().plot(kind='bar', label='female, low class', color='pink')\nax2.set_xticklabels([u\"Not Survived\", u\"Survived\"], rotation=0)\nplt.legend([u\"Female\/Low Class\"], loc='best')\n\nax3=fig.add_subplot(143, sharey=ax1)\ndf_train.Survived[df_train.Sex == 'male'][df_train.Pclass != 3].value_counts().plot(kind='bar', label='male, high class',color='lightblue')\nax3.set_xticklabels([u\"Not Survived\", u\"Survived\"], rotation=0)\nplt.legend([u\"Male\/High Class\"], loc='best')\n\nax4=fig.add_subplot(144, sharey=ax1)\ndf_train.Survived[df_train.Sex == 'male'][df_train.Pclass == 3].value_counts().plot(kind='bar', label='male low class', color='steelblue')\nax4.set_xticklabels([u\"Not Survived\", u\"Survived\"], rotation=0)\nplt.legend([u\"Male\/Low Class\"], loc='best')\nsns.set()\nplt.show()","70ddfc31":"#Passenger Suvived by Embarked\nfig = plt.figure(figsize=(10, 8))\nfig.set(alpha=0.2)  # alpha for graph color\n\nSurvived_0 = df_train.Embarked[df_train.Survived == 0].value_counts()\nSurvived_1 = df_train.Embarked[df_train.Survived == 1].value_counts()\ndf=pd.DataFrame({u'Survived':Survived_1, u'Not Survived':Survived_0})\ndf.plot(kind='bar', stacked=True)\nplt.title(u\"Passenger Suvived by Embarked\")\nplt.xlabel(u\"Embarked\") \nplt.ylabel(u\"Passenger Count\") \n\nplt.show()","a034dd6a":"#Passenger Suvived by cabin\nfig = plt.figure(figsize=(10, 8))\nfig.set(alpha=0.2)  # alpha for graph color\n\nSurvived_cabin = df_train.Survived[pd.notnull(df_train.Cabin)].value_counts()\nSurvived_nocabin = df_train.Survived[pd.isnull(df_train.Cabin)].value_counts()\ndf=pd.DataFrame({u'Yes':Survived_cabin, u'No':Survived_nocabin}).transpose()\ndf.plot(kind='bar', stacked=True)\nplt.title(u\"Survived by cabin\")\nplt.xlabel(u\"If have cabin\") \nplt.ylabel(u\"Passenger Count\")\nplt.show()","523978fc":"#https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n#https:\/\/blog.csdn.net\/m0_38024592\/article\/details\/80836217\n#combine train dataset and test dataset\ndf_all = pd.concat([df_train,df_test],axis=0).reset_index(drop=True)","8384d7bf":"# 0, Pre-Process for Name\n# Create feature Title based on variable Name \ndf_all['Title'] = df_all['Name'].map(lambda x: re.compile(\", (.*?)\\.\").findall(x)[0])\n\n# Group low-occuring, related titles together\ndf_all['Title'][df_all.Title == 'Jonkheer'] = 'Master'\ndf_all['Title'][df_all.Title.isin(['Ms','Mlle'])] = 'Miss'\ndf_all['Title'][df_all.Title == 'Mme'] = 'Mrs'\ndf_all['Title'][df_all.Title.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'\ndf_all['Title'][df_all.Title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'","2211d2cc":"print(df_all['Title'].value_counts())","2b55bb53":"# # Pre-Process for Cabin\n# # split one row to muliple rows for carbin because some records have multiple valus in Cabin column \n# df_all=df_all.drop(['Cabin'], axis=1).join(df_all['Cabin'].str.split(' ', expand=True).stack().reset_index(level=1, drop=True).rename('Cabin'))\n# # Create binary features for each deck\n# df_all['Deck'] = df_all['Cabin'].str[0]\n# # Create feature for the room number\n# df_all['Room'] = df_all['Cabin'].str[1:]\n# print(\"Column Deck\\n\", df_all['Deck'])\n# print(\"Column Room\\n\", df_all['Room'])","b86397d1":"###########################\n# Pre-Process for Ticket\n# split Ticket column to Ticket class and Ticket Num \n############################\n# http:\/\/www.datasciencemadesimple.com\/string-replace-column-dataframe-python\/\n# split Ticket to multiple column\ndf_all = pd.concat([df_all, df_all['Ticket'].str.split(' ', expand=True)], axis=1).drop('Ticket', axis=1)\ndf_all.rename(columns={0: 'Ticket_Cls', 1: 'Ticket_Num'}, inplace=True)\n# set Ticket Number without Ticket class to variable Ticket_Num\ndf_all['Ticket_Num'][df_all['Ticket_Num'].isnull()]=df_all['Ticket_Cls']\ndf_all['Ticket_Num']=pd.to_numeric(df_all['Ticket_Num'], errors='coerce')\n# set Ticket class to None if the value is numeric in Ticket_Cls variable\n#print(df_all['Ticket_Cls'][map(lambda x: x.isdigit(), df_all['Ticket_Cls'])])\ndf_all['Ticket_Cls'][df_all['Ticket_Cls'].str.isnumeric()]='None'\nprint(\"Ticket_Cls\\n\",df_all['Ticket_Cls'])\nprint(\"Ticket_Num\\n\",df_all['Ticket_Num'])","c68e75b9":"#1, Check Nan data\n#Missing data for train dataset\n#two Important things for missing data:\n#a. How prevalent is the missing data?\n#b.Is missing data random or does it have a pattern?\n#https:\/\/note.nkmk.me\/python-pandas-nan-judge-count\/\ndf_all.reset_index(drop=True,inplace=True)\ntotal = df_all.isnull().sum().sort_values(ascending=False)\npercent = (df_all.isnull().sum()\/df_all.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data","bc27287c":"#2, Process Nan values\n#2\n#Remove feature 2 because too many nan\ndf_all = df_all.drop(2, axis=1)\n\n#Cabin\n#Remove feature Cabin because feature Deck and room have been created\ndf_all = df_all.drop(['Cabin'], axis=1)\n\n# #Deck\n# #Replace to None\n# df_all['Deck'].fillna('None',inplace=True) \n\n# #Room\n# #Replace to mean(better use mean for numerical variable)\n# df_all['Room']=pd.to_numeric(df_all['Room'], errors='coerce')\n# df_all['Room'].fillna(df_all['Room'].mean(),inplace=True) \n\n#Embarked\n#Replace to most common value (better use most frequently values for categorical variable )\ndf_all['Embarked'].fillna(df_all['Embarked'].dropna().mode().values.item(),inplace=True) \n\n#Fare\n#Replace to mean(better use mean for numerical variable)\ndf_all['Fare'].fillna(df_all['Fare'].median(),inplace=True) \n\n#Fare\n#Replace to mean(better use mean for numerical variable)\ndf_all['Ticket_Num'].fillna(df_all['Ticket_Num'].median(),inplace=True) ","03f5a0c1":"#Replace Nan to (mean -std) or (mean + std) by random for Age\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,5))\naxis1.set_title('Original Age values')\ndf_all['Age'][df_all.Age.isnull()==False].plot(kind='hist', bins=70, ax=axis1)\n\n#print(pd.concat([df_all['Age'],df_all['Title']],axis=1).groupby('Title').agg(['count','mean']))\nage_median_per_title=pd.concat([df_all['Age'],df_all['Title']],axis=1).groupby('Title').median().reset_index()\nfor index,row in age_median_per_title.iterrows():\n    age_title=row['Title']\n    age_median =row['Age']\n    #It seems inplace=True is not working if there hase filter condition\n    df_all['Age'][df_all['Title']==age_title]=df_all['Age'][df_all['Title']==age_title].fillna(age_median)\n    \naxis2.set_title('New Age values')\ndf_all['Age'][df_all.Age.isnull()==False].plot(kind='hist', bins=70, ax=axis2)\nplt.show()\ndf_all.isnull().sum().max() #check if missing data exists","f4a7db1d":"# Create a new variable Age_Cls based on Age\ndf_all['Age_Cls']=0\ndf_all['Age_Cls'][df_all['Age']<=10] = 0\ndf_all['Age_Cls'][(df_all['Age']>10) & (df_all['Age']<=20)] = 1\ndf_all['Age_Cls'][(df_all['Age']>20) & (df_all['Age']<=30)] = 2\ndf_all['Age_Cls'][(df_all['Age']>30) & (df_all['Age']<=40)] = 3\ndf_all['Age_Cls'][(df_all['Age']>40) & (df_all['Age']<=50)] = 4\ndf_all['Age_Cls'][(df_all['Age']>50) & (df_all['Age']<=60)] = 5\ndf_all['Age_Cls'][df_all['Age']>60] = 6","d070ed50":"# qcut() creates a new variable that identifies the quartile range of Fare\ndf_all['Fare_Bin'] = pd.qcut(df_all['Fare'], 6, labels=['Fare_G1', 'Fare_G2', 'Fare_G3', 'Fare_G4', 'Fare_G5', 'Fare_G6'])","201ea46a":"df_all.info()","a3dc6855":"#4, Check variables which only have Unique values or discrete values for train data\n#https:\/\/note.nkmk.me\/python-pandas-value-counts\/\nprint(df_all.nunique()\/len(df_all))\n#Process variables which only have Unique values\n#PassengerId only have unique value\n#Remove PassengerId from all of the dataset\n#df_all.drop(['PassengerId'],axis=1,inplace=True)\n#Name only have unique\n#Remove Name from all of the dataset\ndf_all.drop(['Name'],axis=1,inplace=True)","4cea07e7":"# 2-2-1, Scaling\n#scale numerical variable room\n#For Age,Fare,Room,Ticket_Num\n#remapping the values to small range [-1,1] or [0,1]\n#StandardScaler will subtract the mean from each value then scale to the unit variance\nscaler = preprocessing.StandardScaler()\ndf_all['Fare'] = scaler.fit_transform(df_all['Fare'].values.reshape(-1, 1))\ndf_all['Age'] = scaler.fit_transform(df_all['Age'].values.reshape(-1, 1))\n# df_all['Room'] = scaler.fit_transform(df_all['Room'].values.reshape(-1, 1))\ndf_all['Ticket_Num'] = scaler.fit_transform(df_all['Ticket_Num'].values.reshape(-1, 1))","f437c6a5":"#6, Check Data Quality\n#1),Normality\n#2),Linearity\n\n#Age\n#histogram and normal probability plot\nsns.distplot(df_all[df_all['Age']>0]['Age'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_all[df_all['Age']>0]['Age'], plot=plt)\n# It seems not bad, keep no change","53567ee3":"#Fare\n#histogram and normal probability plot\nsns.distplot(df_train['Fare'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['Fare'], plot=plt)\n#It's very peakedness and do log transformation ","0cc4305c":"df_all.info()","e7e845ba":"#7, dummy variables\n#convert categorical variable into dummy\n#https:\/\/note.nkmk.me\/python-pandas-get-dummies\/\n#Ticket Variable has lots of different valus, not sure if should use\n#In additional convert some numerical variable which have categorical attribute into dummy\n#Drop ticket_num\ndf_all.drop(['Ticket_Num'],axis=1,inplace=True)\n#Drop ticket_Cls\ndf_all.drop(['Ticket_Cls'],axis=1,inplace=True)\n#Drop Fare\n#df_all.drop(['Fare'],axis=1,inplace=True)\n#Drop Age\n#df_all.drop(['Age'],axis=1,inplace=True)\n\n#Change dtype to object\ndf_all['Pclass'] = df_all['Pclass'].astype(object)\ndf_all['SibSp'] = df_all['SibSp'].astype(object)\ndf_all['Parch'] = df_all['Parch'].astype(object)\n# df_all['Deck'] = df_all['Deck'].astype(object)\ndf_all['Age_Cls'] = df_all['Age_Cls'].astype(object)\ndf_all = pd.get_dummies(df_all)\nprint(df_all.columns)","3fbb6768":"df_all.info()","4d403fac":"print('Create DataSet', datetime.now(), )\n#Create Train dataset, Label, Test dataset\n#http:\/\/ailaby.com\/lox_iloc_ix\/\n#https:\/\/note.nkmk.me\/python-pandas-at-iat-loc-iloc\/\ndf_all.reset_index(drop=True,inplace=True)\ny = df_all['Survived'][df_all['Survived'].isnull() == False]\nx_train = df_all[:len(y)].drop(['PassengerId','Survived'],axis=1).values\nx_test = df_all[len(y):].drop(['PassengerId','Survived'],axis=1).values\n\nx_train.shape, x_test.shape, y.shape","b910fdf6":"#Algorithm (fx()= w*x +b , 1 if >=0 else 0)\n#Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.\n#Perceptron is a linear classifier (binary), it is used in supervised learning.\n#It helps to classify the given input data.\nclass PerceptronClassification(BaseEstimator,ClassifierMixin):\n    \"\"\" Perceptron Classifier\n    Parameters\n    ------------\n    rate : float\n        Learning rate (ranging from 0.0 to 1.0)\n    number_of_iteration : int\n    Number of iterations over the input dataset.\n\n    Attributes:\n    ------------\n    weight_matrix : 1d-array\n        Weights after fitting.\n\n    error_matrix : list\n        Number of misclassification in every epoch(one full training cycle on the training set)\n    \"\"\"\n    def __init__(self, rate = 0.01, number_of_iterations = 100):\n        self.rate = rate\n        self.number_of_iterations = number_of_iterations\n\n    def fit(self, X, y):\n        \"\"\" Fit training data\n        Parameters:\n        ------------\n        X : array-like, shape = [number_of_samples, number_of_features]\n            Training vectors.\n        y : array-like, shape = [number_of_samples]\n            Target values.\n        Returns\n        ------------\n        self : object\n        \"\"\"\n        self.weight_matrix = np.zeros(1 + X.shape[1])\n        self.errors_list = []\n\n        for _ in range(self.number_of_iterations):\n            errors = 0\n            for xi, target in zip(X, y):\n                update = self.rate * (target - self.predict(xi))\n                self.weight_matrix[1:] += update * xi\n                self.weight_matrix[0] += update\n                errors += int(update != 0.0)\n            self.errors_list.append(errors)\n        return self\n\n    def dot_product(self, X):\n        \"\"\" Calculate the dot product \"\"\"\n        return (np.dot(X, self.weight_matrix[1:]) + self.weight_matrix[0])\n\n    def predict(self, X):\n        \"\"\" Predicting the label for the input data \"\"\"\n        return np.where(self.dot_product(X) >= 0.0, 1, 0)","26e115f0":"# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 25% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=10)\nstratifiedkfold = StratifiedKFold(n_splits=10)","4e3be76f":"print('Define Classification model', datetime.now(), )\n#Prediction by Perceptron Algorithm\nperceptron_classifiter=PerceptronClassification(number_of_iterations = 3000)","0b8905ad":"#Logistic Regression\n#Logistic Regression is a type of Generalized Linear Model (GLM) \n#that uses a logistic function to model a binary variable based on any kind of independent variables.\nLR = LogisticRegression(random_state=10, solver='lbfgs', multi_class='ovr')","de56e177":"ADA = AdaBoostClassifier()","ec5a7865":"#Support Vector Machines (SVMs)\n#a type of classification algorithm that are more flexible\n#they can do linear classification, but can use other non-linear basis functions.\nSVM = svm.LinearSVC()  ","4e4075ea":"SVM.get_params().keys()","2ba6ef01":"# define parameters as dict type\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid_search_SVM = GridSearchCV(SVM, param_grid, cv=5)","20c1ce54":"#Random Forests are an ensemble learning method\n#that fit multiple Decision Trees on subsets of the data and average the results.\n#0.856502 (2000)\n#0.78947 (learnboard)\n#rf_parameters = {'n_estimators': 2000, 'min_samples_split': 5, 'random_state':5}\nrf_parameters = {'n_estimators': 2000, 'max_depth': 30, 'random_state':10}\n# RF = RandomForestClassifier(n_estimators=3000, max_depth=20, random_state=5)\nRF = RandomForestClassifier(**rf_parameters)  ","697c5cf1":"#Multi-layer Perceptron classifier.\n#This model optimizes the log-loss function using LBFGS or stochastic gradient descent.\n#neural_network.MLPClassifier\u00b6\nNN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(1000, 2), random_state=100)  ","14633319":"# GradientBoostingClassifier\nGBR = GradientBoostingClassifier(n_estimators=2000, max_depth=30, max_leaf_nodes=8, min_samples_split=5, random_state =10)","dec58d98":"# define parameters as dict type\nparam_grid = {'n_estimators': [2000], 'max_depth' : [20], 'random_state' : [0, 5,10]}\ngrid_search = GridSearchCV(RF, param_grid, cv=5)","f5abd5bd":"bagging = BaggingClassifier(n_estimators=2000, max_samples=1.0, max_features=1.0, n_jobs=-1, random_state =10)\n# bagging_LR.fit(x_train, y_train)\n# print(bagging_LR.score(x_val,y_val))\n# predictions = bagging_LR.predict(x_test)","1e419e80":"# Build a Extra forest\nExtraforest = ExtraTreesClassifier(n_estimators=2000, random_state=10)","7c8acfc4":"# votemodel = VotingClassifier(estimators=[('LR', LR), ('RF', RF), ('GBR', GBR)],voting='soft'])\nvotemodel = VotingClassifier(estimators=[('LR', LR), ('RF', RF), ('GBR', GBR), ('ADA', ADA), ('Extraforest', Extraforest), \n                                         ('bagging_Decision tree', bagging)],voting='hard',weights=[1,1,1,1,1,1])\n#votemodel = VotingClassifier(estimators=[('LR', LR), ('RF', RF), ('GBR', GBR), ('ADA', ADA)],voting='hard',weights=[1,1,1,1])","9fbd98b6":"print('Fit and TEST score for each model',datetime.now(), )\n#names = [\"Logistic Regression\", \"Support Vector Machines\", \"Random Forests\", \"Multi-layer Perceptron classifier\",\"Gradient Boosting\",'Voting Classifier']\nnames = [\"Logistic Regression\", \"Random Forests\", \"Gradient Boosting\", \"ADA\", \"Extraforest\", \"bagging\", \"Voting Classifier\"]\nmodels= [LR, RF, GBR, ADA, Extraforest, bagging, votemodel]\n#names = [\"Random Forests\"]\n#models= [RF]\nfor name, model in zip(names, models):\n    model.fit(x_train, y)\n    print('model {} start'.format(name))\n    scores = cross_val_score(model, x_train, y,cv=stratifiedkfold)\n    # \u5404\u5206\u5272\u306b\u304a\u3051\u308b\u30b9\u30b3\u30a2\n    print('{} Cross-Validation scores: {}'.format(name,scores))\n    # \u30b9\u30b3\u30a2\u306e\u5e73\u5747\u5024\n    score_mean = np.mean(scores)\n    print('{} Cross-Validation Average score: {:.6f}'.format(name,score_mean))\n    #print(cross_val_predict(model, x_train, y,cv=stratifiedkfold))","1ade104b":"# print('Predict submission', datetime.now(),)\n# submission_PER=pd.DataFrame({'PassengerId':df_test['PassengerId'], 'Survived':perceptron_classifiter.predict(x_test)})\n# submission_LR=pd.DataFrame({'PassengerId':df_test['PassengerId'], 'Survived':LR.predict(x_test)})\n# submission_NN=pd.DataFrame({'PassengerId':df_test['PassengerId'], 'Survived':NN.predict(x_test)})\n# submission_SVM=pd.DataFrame({'PassengerId':df_test['PassengerId'], 'Survived':SVM.predict(x_test)})\n#submission_RF=pd.DataFrame({'PassengerId':df_all['PassengerId'][df_all['Survived'].isnull()], 'Survived':RF.predict(x_test).astype(int)})\nsubmission_bagging=pd.DataFrame({'PassengerId':df_all['PassengerId'][df_all['Survived'].isnull()], 'Survived':bagging.predict(x_test).astype(int)})\nsubmission_votemodel=pd.DataFrame({'PassengerId':df_all['PassengerId'][df_all['Survived'].isnull()], 'Survived':votemodel.predict(x_test).astype(int)})","b8da6fc0":"# submission_RF = submission_RF.drop_duplicates(subset='PassengerId', keep='first')\n# submission_RF = submission_RF.reset_index(drop=True)\n#submission_GBR = submission_GBR.drop_duplicates(subset='PassengerId', keep='first')\nsubmission_bagging = submission_bagging.reset_index(drop=True)\nsubmission_votemodel = submission_votemodel.reset_index(drop=True)","d5427151":"# #output to submission csv\n# #0.72727\n# submission_PER.to_csv(\"submission_v2_PER.csv\", index=False)\n# #0.77511\n# submission_LR.to_csv(\"submission_v2_LR.csv\", index=False)\n# #0.76555\n# submission_NN.to_csv(\"submission_v2_NN.csv\", index=False)\n# #0.76076\n# submission_SVM.to_csv(\"submission_v2_SVM.csv\", index=False)\n# #0.77511\n#submission_RF.to_csv(\"submission_v3_RF.csv\", index=False)\nsubmission_votemodel.to_csv(\"submission_v4_vm.csv\", index=False)\n#submission_RF.to_csv(\"submission_v4_RF.csv\", index=False)\nsubmission_bagging.to_csv(\"submission_v4_bagging.csv\", index=False)\nprint('Save submission', datetime.now(),)","d8ed1363":"# Learning curve\n# Visualization model if overfitting or underfitting\n# not good for generalization ability if overfitting\n# https:\/\/blog.csdn.net\/limiyudianzi\/article\/details\/79626702\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    print(\"Training size:{}\".format(train_sizes))\n    print(\"Train score: {}\".format(train_scores_mean))\n    print(\"Cross-validation score: {}\".format(test_scores_mean))\n    return plt","4b0a8a48":"X = x_train\ny = y\n\n# RandomForest\n#rf_parameters = {'n_jobs': -1, 'n_estimators': 2000, 'warm_start': True, 'max_depth': 20, 'min_samples_leaf': 2, 'max_features' : 'sqrt','verbose': 0}\n#rf_parameters = {'n_estimators': 1000, 'max_depth': 30}\n\n# AdaBoost\nada_parameters = {'n_estimators':500, 'learning_rate':0.1}\n\n# ExtraTrees\net_parameters = {'n_jobs': -1, 'n_estimators':500, 'max_depth': 8, 'min_samples_leaf': 2, 'verbose': 0}\n\n# GradientBoosting\ngb_parameters = {'n_estimators': 500, 'max_depth': 5, 'min_samples_leaf': 2, 'verbose': 0}\n\n# DecisionTree\ndt_parameters = {'max_depth':8}\n\n# KNeighbors\nknn_parameters = {'n_neighbors':2}\n\n# SVM\nsvm_parameters = {'kernel':'linear', 'C':0.025}\n\n# XGB\ngbm_parameters = {'n_estimators': 2000, 'max_depth': 4, 'min_child_weight': 2, 'gamma':0.9, 'subsample':0.8, \n                  'colsample_bytree':0.8, 'objective': 'binary:logistic', 'nthread':-1, 'scale_pos_weight':1}\ntitle = \"Learning Curves (RandomForest)\"\nplot_learning_curve(votemodel, title, X, y, ylim=(0.5, 1.01), cv=stratifiedkfold,  n_jobs=4, train_sizes=[200, 300, 400, 500, 600])\n# plot_learning_curve(RandomForestClassifier(**rf_parameters), title, X, y, ylim=(0.5, 1.01), cv=cv,  n_jobs=4, train_sizes=[200, 250, 350, 400, 450, 500,550,600,650])\n# plot_learning_curve(votemodel, title, X, y, ylim=(0.5, 1.01), cv=cv,  n_jobs=4, train_sizes=[200, 250, 350, 400, 450, 500,550,600,650])\nplt.show()","77592dfc":"##########################\n# Check importance of features\n##########################\nfeatures_list = df_all.drop(['Survived','PassengerId'],axis=1).columns.values\n\n# Fit a random forest with (mostly) default parameters to determine feature importance\nforest = RandomForestClassifier(**rf_parameters)\nforest.fit(X, y)\nfeature_importance = forest.feature_importances_\n\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\n\nprint(feature_importance)\n# Get the indexes of all features over the importance threshold\nimportant_idx = np.where(feature_importance)[0]\n\n# Get the sorted indexes of important features\nsorted_idx = np.argsort(feature_importance[important_idx])[::-1]\nprint(\"\\nFeatures sorted by importance (DESC):\\n\", features_list[sorted_idx])\n\n# Adapted from http:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_gradient_boosting_regression.html\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.figure(figsize=(18, 18))\nplt.subplot(1, 1, 1)\nplt.barh(pos, feature_importance[important_idx][sorted_idx[::-1]], align='center')\nplt.yticks(pos, features_list[sorted_idx[::-1]])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","1d54a1ed":"# Titanic: Machine Learning from Disaster\nStart here! Predict survival on the Titanic and get familiar with ML basics","4dd8a2e2":"# Model Data","a6d0ad14":"# Tuning","6563fc76":"# Data Processing"}}