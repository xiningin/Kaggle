{"cell_type":{"d649fd26":"code","7c493e69":"code","a4b969e8":"code","c093aae5":"code","52948719":"code","7ef019c0":"code","2e1b8f40":"code","2167c336":"code","28f32e84":"code","e7682f94":"code","0d6516bc":"code","1ed4b9cb":"code","7be6ffe2":"code","c4fb6806":"code","4fc98a02":"code","a42c82da":"code","89a3614b":"code","17b394bb":"code","c1c93945":"code","183b5dff":"code","49aeed10":"markdown","a46dc627":"markdown","494dd027":"markdown","9c699d2c":"markdown","e9a73359":"markdown","d3641a14":"markdown","b2c7c3a5":"markdown","8f9b304f":"markdown","c787fd3a":"markdown","dd992099":"markdown","3885bbdf":"markdown","e18b4a0c":"markdown","9657da00":"markdown","d5cf0e1f":"markdown"},"source":{"d649fd26":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk \nimport string\nimport re\n%matplotlib inline\npd.set_option('display.max_colwidth', 100)","7c493e69":"# Load dataset\ndef load_data():\n    data = pd.read_csv('..\/input\/Data.csv')\n    return data","a4b969e8":"tweet_df = load_data()\ntweet_df.head()","c093aae5":"print('Dataset size:',tweet_df.shape)\nprint('Columns are:',tweet_df.columns)","52948719":"tweet_df.info()","7ef019c0":"sns.countplot(x = 'ADR_label', data = tweet_df)","2e1b8f40":"df  = pd.DataFrame(tweet_df[['UserId', 'Tweet']])","2167c336":"from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n# Start with one review:\ndf_ADR = tweet_df[tweet_df['ADR_label']==1]\ndf_NADR = tweet_df[tweet_df['ADR_label']==0]\ntweet_All = \" \".join(review for review in df.Tweet)\ntweet_ADR = \" \".join(review for review in df_ADR.Tweet)\ntweet_NADR = \" \".join(review for review in df_NADR.Tweet)\n\nfig, ax = plt.subplots(3, 1, figsize  = (30,30))\n# Create and generate a word cloud image:\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_All)\nwordcloud_ADR = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_ADR)\nwordcloud_NADR = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_NADR)\n\n# Display the generated image:\nax[0].imshow(wordcloud_ALL, interpolation='bilinear')\nax[0].set_title('All Tweets', fontsize=30)\nax[0].axis('off')\nax[1].imshow(wordcloud_ADR, interpolation='bilinear')\nax[1].set_title('Tweets under ADR Class',fontsize=30)\nax[1].axis('off')\nax[2].imshow(wordcloud_NADR, interpolation='bilinear')\nax[2].set_title('Tweets under None - ADR Class',fontsize=30)\nax[2].axis('off')\n\n#wordcloud.to_file(\"img\/first_review.png\")","28f32e84":"string.punctuation","e7682f94":"def remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    text = re.sub('[0-9]+', '', text)\n    return text\n\ndf['Tweet_punct'] = df['Tweet'].apply(lambda x: remove_punct(x))\ndf.head(10)","0d6516bc":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['Tweet_tokenized'] = df['Tweet_punct'].apply(lambda x: tokenization(x.lower()))\ndf.head()","1ed4b9cb":"stopword = nltk.corpus.stopwords.words('english')\n#stopword.extend(['yr', 'year', 'woman', 'man', 'girl','boy','one', 'two', 'sixteen', 'yearold', 'fu', 'weeks', 'week',\n#               'treatment', 'associated', 'patients', 'may','day', 'case','old'])","7be6ffe2":"def remove_stopwords(text):\n    text = [word for word in text if word not in stopword]\n    return text\n    \ndf['Tweet_nonstop'] = df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\ndf.head(10)","c4fb6806":"ps = nltk.PorterStemmer()\n\ndef stemming(text):\n    text = [ps.stem(word) for word in text]\n    return text\n\ndf['Tweet_stemmed'] = df['Tweet_nonstop'].apply(lambda x: stemming(x))\ndf.head()","4fc98a02":"wn = nltk.WordNetLemmatizer()\n\ndef lemmatizer(text):\n    text = [wn.lemmatize(word) for word in text]\n    return text\n\ndf['Tweet_lemmatized'] = df['Tweet_nonstop'].apply(lambda x: lemmatizer(x))\ndf.head()","a42c82da":"def clean_text(text):\n    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n    text_rc = re.sub('[0-9]+', '', text_lc)\n    tokens = re.split('\\W+', text_rc)    # tokenization\n    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n    return text","89a3614b":"countVectorizer = CountVectorizer(analyzer=clean_text) \ncountVector = countVectorizer.fit_transform(df['Tweet'])\nprint('{} Number of tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))\n#print(countVectorizer.get_feature_names())","17b394bb":"count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\ncount_vect_df.head()","c1c93945":"ADR_tweet_1 = tweet_df[tweet_df['ADR_label'] == 1]['Tweet'].apply(lambda x: len(x) - len(' '))\nADR_tweet_0 = tweet_df[tweet_df['ADR_label'] == 0]['Tweet'].apply(lambda x: len(x) - len(' '))","183b5dff":"bins_ = np.linspace(0, 450, 70)\n\nplt.hist(ADR_tweet_1, bins= bins_, normed=True, alpha = 0.5, label = 'ADR')\nplt.hist(ADR_tweet_0, bins= bins_, normed=True, alpha = 0.1, label = 'None_ADR')\nplt.legend()","49aeed10":"\n\n## Problem Statement - Predict Tweets Category (0\/1)\n\n\n \n \n** If you find the content informative to any extend, kindly encourge me by upvoting. **","a46dc627":"## Tokenization","494dd027":"*This notebook is under progress and further code for model building, model evaluation and hyper parameter tuning will be added soon. *\n    ","9c699d2c":"## Feature Creation","e9a73359":"# Load Libraries","d3641a14":"##  Stemming and Lammitization\n\nEx - developed, development","b2c7c3a5":"## Summary of Exploratory Data Analysis\n        - As any text data, tweets are quite unclean having punctuations, numbers and short cuts.\n        - Most of the short cuts or abbreviations can either be transformed or dropped. I have dropped them here for, I don't have complete details on them.\n        - There are few words such as patient, therapy, etc, occuring most frequently than other words and are common in all classification categories. As high frequency of these words in this dataset does not add to the word significance and dropping them along with stop words will not influence much on each result or metrics.\n        - Among stemmer and lemmatizer, stemmer looks to be working in this dataset, however, later can also be used and try during model building and evaluation.\n        - As part of Feature creation, text lenght of ADR class and None ADR class is compared. Though there is not much difference in text legth pattern for the same. Other hypothesis can also be developed to generate more features such as frequency of 'renal failure' in ADR tweets compared to 'NONE-ADR' tweets. Selection of the key words certainly depends on questions like - if tweets are for one product or several products combined.","8f9b304f":"## Vectorisation\n\n    - Cleaning data in single line through passing clean_text in the CountVectorizer","c787fd3a":"## Remove stopwords\n\n- Identified few more words to be removed along with English stopwords\n         - (['yr', 'year', 'woman', 'man', 'girl','boy','one', 'two', 'sixteen', 'yearold', 'fu', 'weeks', 'week',\n              'treatment', 'associated', 'patients', 'may','day', 'case','old'])","dd992099":"# Exploratory Data Analysis\n## Wordcloud Visualization ","3885bbdf":"## Pre-processing text data\nMost of the text data are cleaned by following below steps. \n\n1. Remove punctuations \n2. Tokenization - Converting a sentence into list of words\n3. Remove stopwords\n4. Lammetization\/stemming - Tranforming any form of a word to its root word\n","e18b4a0c":"Quick Notes:-\n    - Few high frequency tokens such as 'treatment', 'patient', 'therapy' are frequently used in both the categorical classes (ADR\/non-ADR)\n    - Removing these words along with stops words would not impact the performance.","9657da00":"### Remove punctuations","d5cf0e1f":"Assuming a hypothesis to create new features\n#### Hypothesis:\n    - N0 : ADR - has long text length than NADR\n    - N1: ADR - has not long text length than NADR"}}