{"cell_type":{"13b6456f":"code","ad112d8d":"code","07aa12ce":"code","1ea0a500":"code","6748265f":"code","64f83a68":"code","4880a5cf":"code","80cdd973":"code","a8a79233":"code","b30313b8":"code","071d1464":"code","68e94086":"code","f7d5002d":"code","afe38e8d":"code","aa466874":"code","5f5ee7d5":"code","69b1e1ab":"code","8f9b07f1":"code","ddccd73c":"code","3a13cd96":"code","0100f898":"code","d8dc7f43":"code","0de1eace":"code","82b44ecf":"code","2c00821a":"code","fc2550bc":"code","bca603cb":"code","c773c293":"code","b3f57080":"code","b806e8a1":"code","32c08289":"code","c43800a6":"code","393fac1c":"code","ade91486":"code","bf5526cd":"code","b602a843":"code","2cb71782":"code","38df4309":"code","708b5ba7":"code","2e3c4f86":"code","27a11703":"code","fbe862f3":"code","3ef0cf96":"code","549ab419":"code","b072c4e5":"code","db372311":"code","ce0f3140":"code","677a19ec":"code","c1f07098":"code","95efa47a":"code","5290b1d5":"code","8fd5e4bf":"code","36efa13e":"code","1fdb047f":"code","38d89e8f":"markdown","613268c4":"markdown","11699815":"markdown","af85cc0f":"markdown","f615f3ab":"markdown","95878cd5":"markdown","fb1bee6b":"markdown","0093cfef":"markdown","1c275d5b":"markdown","8a614136":"markdown","d445f22f":"markdown","9a0bf74f":"markdown","d8895f86":"markdown","6a0845ca":"markdown","8172638b":"markdown","faae15bc":"markdown","6504b821":"markdown","5ba5056b":"markdown","61a52dba":"markdown","4a331eb1":"markdown","d7b2309a":"markdown","b0e9a94d":"markdown","d5854ead":"markdown","a7c0926f":"markdown","7c3ce349":"markdown","b0e21878":"markdown","a06a7f9b":"markdown"},"source":{"13b6456f":"# additional helpful libaries that may come in uesful along the way\n\nimport numpy as np #linear algebra\nimport pandas as pd #data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport random as rnd\nprint(os.listdir('..\/input'))\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#machinelearning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","ad112d8d":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\ncombined = [train_df, test_df]\n\nprint(train_df.columns.values)","07aa12ce":"    train_df.head()","1ea0a500":"train_df.info()\nprint('-'*40)\ntest_df.info()","6748265f":"train_df.describe()","64f83a68":"train_df.describe(include = ['O'])","4880a5cf":"train_df.head(1)","80cdd973":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","a8a79233":"train_df[['Sex', 'Survived']].groupby(['Sex'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","b30313b8":"train_df[['SibSp', 'Survived']].groupby(['SibSp'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","071d1464":"train_df[['Parch', 'Survived']].groupby(['Parch'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","68e94086":"g = sns.FacetGrid(train_df, col = 'Survived')\ng.map(plt.hist, 'Age', bins = 20)","f7d5002d":"grid = sns.FacetGrid(train_df, col = 'Survived', row = 'Pclass', height = 2.2, aspect = 1.6)\ngrid.map(plt.hist, 'Age', alpha = 0.5, bins = 20)\ngrid.add_legend()","afe38e8d":"grid = sns.FacetGrid(train_df, row='Embarked', height=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","aa466874":"grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', height=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","5f5ee7d5":"train_df.head()","69b1e1ab":"print('Before', train_df.shape, test_df.shape, combined[0].shape, combined[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis = 1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis = 1)\ncombined = [train_df, test_df]\n\nprint('After', train_df.shape, test_df.shape, combined[0].shape, combined[1].shape)","8f9b07f1":"print('Before', train_df.shape, test_df.shape, combined[0].shape, combined[1].shape)\n\ntrain_df = train_df.drop(['Name', 'PassengerId'], axis = 1)\ntest_df = test_df.drop(['Name'], axis = 1)\ncombined = [train_df, test_df]\n\nprint('After', train_df.shape, test_df.shape, combined[0].shape, combined[1].shape)","ddccd73c":"for dataset in combined:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","3a13cd96":"grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","0100f898":"guess_ages = np.zeros((2,3))\nguess_ages","d8dc7f43":"for dataset in combined:\n    for i in range(0,2):\n        for j in range(0,3):\n            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j + 1)]['Age'].dropna()\n            \n            age_guess = guess_df.median()\n            \n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int(age_guess\/0.5 + 0.5) * 0.5\n            \n    for i in range(0,2):\n        for j in range(0,3):\n            \n            #locating the entry that fulfils all 3 (must be null), then slotting in the calculated estimated, quite smart.\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1), 'Age'] = guess_ages[i,j]\n            \n    dataset['Age'] = dataset['Age'].astype(int)","0de1eace":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5) #5 is the number of categories to cut into\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index = False).mean().sort_values(by = 'AgeBand', ascending = True)","82b44ecf":"for dataset in combined:\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 64, 'Age'] = 4","2c00821a":"# removing the ageband feature\n\ntrain_df = train_df.drop(['AgeBand'], axis = 1)\ncombined = [train_df, test_df]","fc2550bc":"train_df.head()","bca603cb":"freq_port = train_df.Embarked.dropna().mode()[0] # mode is to show value that appears most often\nfreq_port","c773c293":"for dataset in combined:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n\ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","b3f57080":"for dataset in combined:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2}).astype(int)","b806e8a1":"train_df.head()","32c08289":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","c43800a6":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4) # here we activate quantile cut to get same number in each \ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index = False).mean().sort_values(by = 'FareBand', ascending = True)","393fac1c":"for dataset in combined:\n    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31.0), 'Fare'] = 2\n    dataset.loc[(dataset['Fare'] > 31.0), 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)","ade91486":"train_df = train_df.drop(['FareBand'], axis = 1)\ncombined = [train_df, test_df]","bf5526cd":"train_df.head(10)","b602a843":"test_df.head(10)","2cb71782":"X_train = train_df.drop('Survived', axis = 1)\nY_train = train_df['Survived']\nX_test = test_df.drop('PassengerId', axis = 1).copy()\n","38df4309":"X_train.shape, Y_train.shape, X_test.shape","708b5ba7":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train)*100, 2)\nacc_log","2e3c4f86":"# Support Vector Machines (SVM)\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train)*100, 2)\nacc_svc","27a11703":"# kNearestNeighbours\n\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train)*100, 2)\nacc_knn","fbe862f3":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression' \n              ],\n    'Score': [acc_svc, acc_knn, acc_log, \n             ]})\nmodels.sort_values(by='Score', ascending=False)","3ef0cf96":"from keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras import losses \nfrom keras import metrics\nfrom keras.optimizers import SGD","549ab419":"model = models.Sequential()\nmodel.add(layers.Dense(32, activation = 'relu',\n                      input_shape = (7,)))\nmodel.add(layers.Dense(32, activation = 'relu'))\nmodel.add(layers.Dense(16, activation = 'relu'))\nmodel.add(layers.Dense(8, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\n# creating stochastic gradient descent\nsgd = SGD(lr = 0.005, momentum = 0.9)\n\nmodel.compile(optimizer = sgd,\n             loss = losses.binary_crossentropy,\n             metrics = [metrics.binary_accuracy])","b072c4e5":"model.summary()","db372311":"Y_train = np.asarray(Y_train)\nX_train = np.asarray(X_train)\nX_test = np.asarray(X_test)\n\nvalidation_size = 200\n\nX_val = X_train[:validation_size]\npartial_X_train = X_train[validation_size:]\nY_val = Y_train[:validation_size]\npartial_Y_train = Y_train[validation_size:]","ce0f3140":"training = model.fit(partial_X_train, partial_Y_train, batch_size = 60, epochs = 100, validation_data = (X_val, Y_val))","677a19ec":"acc = training.history['binary_accuracy']\nval_acc = training.history['val_binary_accuracy']\nloss = training.history['loss']\nval_loss = training.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, loss, 'bo', label = 'Training Loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()","c1f07098":"plt.clf()\n\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label = 'Training Acc')\nplt.plot(epochs, val_acc, 'b', label = 'Validation Acc')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('Acc')\nplt.show()","95efa47a":"scores = model.evaluate(X_train, Y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","5290b1d5":"predictions = model.predict_classes(X_test)\nids = test_df['PassengerId'].copy()\nnew_output = ids.to_frame()\nnew_output[\"Survived\"]=predictions\nnew_output.head(10)","8fd5e4bf":"new_output.to_csv(\"kerasmodel.csv\",index=False)","36efa13e":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })","1fdb047f":"submission.to_csv('titanicmodel.csv', index=False)","38d89e8f":"#### From here we can see that there are some categories with insufficient\/invalid data, we need to take note","613268c4":"# Wrangle Data\n\nDropping features helps ease the analysis and speeds up the notebook","11699815":"### Completing\n\n1. Age features have some uncompleted values\n2. Embarked features too\n\n### Correction\n\n1. Ticket feature may be dropped since contains high ratio of duplicates (refer above) and may not have a correlation with survival\n2. Cabin feature should also be dropped as it is highly incomplete, too many null values in training and test dataset\n3. Passenger ID may be dropped since there probably isnt a correlation\n4. Name feature is non standard, dropped\n\n### Creating\n\n1. Possibly creating a new feature called family given Parch (parents children) and Sibsp (siblings spouse) may make sense.\n2. Engineer name feature to extract title as a new feature\n3. Create new feature for age bands, may turn out more useful than individual ages\n4. Fare range too, may help in the analysis\n\n### Classifying\n\n1. Assumptions, such as sex = female more likely to have survived\n2. Children (age < x) also\n3. Upper class dudes may also be more likely to survive?\n","af85cc0f":"Now, replacing age with ordinals based on the bands","f615f3ab":"### Now we analyze by visualizing the data","95878cd5":"#### Based on assumption, we are going to go ahead to drop name and passengerID features","fb1bee6b":"### Attempting to train a simple model to test effectiveness\nHave to convert all to numericals before fitting into model (sex, age, fare, embark)","0093cfef":"### Here we analyse our assumptions with the data provided.","1c275d5b":"Survival, 0 is no, 1 is yes\n\nPclass represents ticket class (also as SES), 1 is upper, 2 is middle and 3 is lower\n\nembarked - port of embarkation, C is Cherboug, Q is Queenstown and S is Southampton\n","8a614136":"#### Now we plot accuracy and loss for both training and validation set, to check model optimization level","d445f22f":"## Experimenting with keras to run DNN models\nFirst we build the NN Model using Keras","9a0bf74f":"### Observations\n\n1. Pclass = 3 had most passengers, however most did not survive. Confirms assumption 2.\n2. Infant passengers in Pclass = 2 and 3 mostly survived. Confirms assumption 2.\n3. Most passengers in Pclass = 1 survived. Confirms assumption 3.\n4. Pclass varies in terms of age distribution of passengers.\n\n### Decisions\nPerhaps Pclass is a good feature for model training.","d8895f86":"### Evaluating the Model\nWe look at the predicted accuracy of the NN","6a0845ca":"### Acquiring Data from data files","8172638b":"#### Completing a categorical feature\nTraining dataset has 2 missing values for embarkation, we simply fill in the most common occurance","faae15bc":"### Observations \n\n1. Female passengers have higher survival rates compared to male passengers\n2. Except during embarked = C whereby male have higher survival rates. Could be as a result of correlation between Pclass?\n3. Males probably have higher survival rates as seen in embark = C and Q\n\n### Decisions\n\n1. Add sex feature to model training\n2. Complete and add embarked feature to model training","6504b821":"### We shall skip creating new features first to train a model asap to test our results","5ba5056b":"Converting categorical features to numeric to fit into our model","61a52dba":"Now we take a look at creating age bands to have a better analysis of the data (i.e determine correlation with survived)","4a331eb1":"### Completing numerical continuous feature\nWe have to look into features with missing or null values.","d7b2309a":"### Observations\n\n1. Higher fare paying passengers have a higher chance of survival\n2. Point of embarkation correlates with survival rate\n3. Generally females tend to have a higher survival rate\n\n### Decisions\n\n1. Consider banding Fare feature","b0e9a94d":"### Observations\n\n1. Infants (<4 years of age) had high survival rate\n2. Oldest passengest survived (almost 80 years)\n3. Large numbers of 15 - 25 years of age didnt survive \n4. Most passengers are in the age range of 15 - 35 years of age","d5854ead":"#### Good to note that only some are binary classifications - survived, sex and embarked\nAlso good to know some are numerical, and within numerical can be further classified as discrete, continuous or timeseries based\n\nContinuous - age, fare\n\nDiscrete - sibsp (sibling\/spouses on board), parch (parents\/children on board)","a7c0926f":"#### Splitting the training data set with validation sets, then running the model\nDefault validation set size is 50","7c3ce349":"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.\n","b0e21878":"### Deep Learning Models (DNN) tend to scale better with more data.\nThe data given can be considered small, hence it may be wiser to utilize other forms of machine learning models which can be simpler and easier to train!","a06a7f9b":"#### We can do likewise now for Fares by creating a Fareband\nFirst we replace the null stuff in the test set!\n\nAnd again, replace ordinal values with the banding"}}