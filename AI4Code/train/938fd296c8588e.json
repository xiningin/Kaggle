{"cell_type":{"42014153":"code","0aab88f3":"code","d542427e":"code","9f92bf5b":"code","43789fbd":"code","949932ee":"code","f7121c16":"code","848976cf":"code","4d7f7b05":"code","12dfc302":"code","aa23ea5c":"code","e141d0bd":"code","376805f3":"code","63c9f76f":"code","7cc0a641":"code","c453ae72":"code","e6d42e40":"code","40906784":"code","ac9e36dc":"code","1b7f75d8":"code","869a320b":"code","930a1eb5":"code","8d8acaa9":"code","44b36c31":"code","784a9f11":"code","ec218d4f":"code","6968d5b8":"code","5e4ad5ef":"code","c9e1c20e":"code","dd71c03f":"code","e0e08a8b":"code","7f9f7d46":"code","df4b0e3c":"code","58b827d6":"code","81824ff2":"code","6d8e2e39":"code","13658484":"code","c42c7514":"code","57bf0b6d":"code","6f34fb94":"code","7b0a2e74":"code","8ad93224":"code","3e506a08":"code","295d86ab":"code","2f36971c":"code","a9cf6bbe":"code","d324387b":"code","cdaf0736":"code","24c9fc4c":"code","b7948db3":"code","ca6b75c1":"code","bc2cf59e":"code","a983650f":"code","a8029a71":"code","2f20c09d":"code","6b783be1":"code","ff1dee19":"code","c130daf1":"code","d617a625":"code","9f89f760":"code","435d7b4b":"code","d5808eb5":"code","fe64e7f8":"code","0d3edbcd":"code","9c76aaf1":"code","750f3b40":"code","eaa969a4":"code","6a548070":"code","90f4d319":"code","4c8e765b":"code","a30e902f":"code","b8530b43":"code","f658a5bf":"code","ed162813":"code","82b85729":"code","3086ac6f":"code","6ab836f0":"code","4a6aa832":"code","4bd68184":"code","8937df82":"code","ff89b7d1":"code","1ffddaa2":"code","58324647":"code","1d216780":"code","d066db06":"code","a7640d51":"code","200e8f71":"code","b9212ddc":"code","9b88dce4":"code","5ee9032a":"code","bac1fd2f":"code","ca4919d8":"code","5e2461a0":"code","98b963d1":"code","16c5bb50":"code","da703e09":"code","04ffc1f8":"code","d1403aaf":"code","15d26b5b":"code","06eb6414":"code","0881645f":"code","87c2b96d":"code","c593803a":"code","f650fcd7":"code","ccbf8e8d":"code","061606d8":"markdown","86882ad1":"markdown","bcaf1956":"markdown","b5ae9a4f":"markdown","169246a7":"markdown","6189e7e3":"markdown","77ad131d":"markdown","f4fa1a9d":"markdown","3b8ae501":"markdown","52204512":"markdown","65a495b8":"markdown","66f5fe7d":"markdown","db85f200":"markdown","03726d9a":"markdown","5ad221fe":"markdown","037b1163":"markdown","0d63b628":"markdown","620f1a90":"markdown","6b912155":"markdown","6c3fc21b":"markdown","b6aab5d9":"markdown","aba46f9b":"markdown","4a6a7d44":"markdown","ef6511ce":"markdown","4705af97":"markdown","ee9e2a79":"markdown","e9537bfd":"markdown","559d1f9c":"markdown","99cf6750":"markdown","77b54d32":"markdown","1ebc18bb":"markdown","48e24aa5":"markdown","6302b4f2":"markdown","af48556e":"markdown","02d2b776":"markdown","8fedb176":"markdown","db15067a":"markdown","5cc7f2ae":"markdown","6e8cf38b":"markdown","e7c24d47":"markdown","7ba972ba":"markdown","fff8fda2":"markdown","55a17ec9":"markdown","a5d2ab5e":"markdown","56d865b2":"markdown","099deeb8":"markdown"},"source":{"42014153":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npd.set_option('display.max_columns', 300)","0aab88f3":"data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-dataset\/telecom_churn_data.csv\", encoding= 'unicode_escape')","d542427e":"data.head()","9f92bf5b":"print(data.shape)\nprint(data.info())","43789fbd":"pctDF = (data.isnull().sum()\/len(data) * 100).reset_index()\npctDF.columns = ['Columns','Missing Value Percentage']\npctDF = pctDF[pctDF['Missing Value Percentage'] > 70]\npctDF","949932ee":"meaningful_missing_columns = ['night_pck_user_6','night_pck_user_7','night_pck_user_8','night_pck_user_9',\n                              'fb_user_6','fb_user_7','fb_user_8','fb_user_9']\n\nfor col in meaningful_missing_columns:\n    data[col].fillna(0, inplace=True)","f7121c16":"for i in range(6, 10): # 6 to 9th Month\n  count_rech_2g = 'count_rech_2g_{0}'.format(i)\n  count_rech_3g = 'count_rech_3g_{0}'.format(i)\n  arpu_2g = 'arpu_2g_{0}'.format(i)\n  arpu_3g = 'arpu_3g_{0}'.format(i)\n\n  mask_2g = (data[count_rech_2g].isna() & data[arpu_2g].isna())\n  data.loc[mask_2g, [count_rech_2g, arpu_2g]] = data.loc[mask_2g, [count_rech_2g, arpu_2g]].fillna(0)\n\n  mask_3g = (data[count_rech_3g].isna() & data[arpu_3g].isna())\n  data.loc[mask_3g, [count_rech_3g, arpu_3g]] = data.loc[mask_3g, [count_rech_3g, arpu_3g]].fillna(0)\n","848976cf":"# we will calculate total recharge amount for data\nfor i in range(6,10): # 6 to 9th Month\n  \n  av_rech_amt_data = 'av_rech_amt_data_{0}'.format(i)\n  total_rech_data = 'total_rech_data_{0}'.format(i)\n\n  data[av_rech_amt_data].fillna(0, inplace=True)\n  data[total_rech_data].fillna(0, inplace=True)\n\n  data['total_rech_data_amt_{0}'.format(i)] = data[av_rech_amt_data] * data[total_rech_data]\n  data.drop(columns=[av_rech_amt_data, total_rech_data])\n","4d7f7b05":"pctDF = (data.isna().sum()\/len(data) * 100).reset_index()\npctDF.columns = ['Columns','Missing Value Percentage']\npctDF = pctDF[pctDF['Missing Value Percentage'] > 70]\npctDF","12dfc302":"# Now the remaining columns are not useful lets drop them.\ncolumns = list(map(lambda x: x, pctDF['Columns']))\ndata.drop(columns = columns, inplace=True)\ndata.head()","aa23ea5c":"print(data.shape)\nprint(data.info())","e141d0bd":"#Remove the mobile_number Columns\ndata.drop(columns = ['mobile_number','circle_id'], inplace=True)","376805f3":"data.head()","63c9f76f":"pctDF = (data.isnull().sum()\/len(data) * 100).reset_index()\npctDF.columns = ['Columns','Missing Value Percentage']\npctDF = pctDF[pctDF['Missing Value Percentage'] > 0]\npctDF","7cc0a641":"#these columns have all the values as same, lets drop them\n# date_of_last_rech_* columns are not useful for model building\ndate_columns = ['last_date_of_month_6','last_date_of_month_7','last_date_of_month_8','last_date_of_month_9',\n                'date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9']\n\ndata.drop(columns = date_columns, inplace=True)\ndata.head()","c453ae72":"from sklearn.impute import KNNImputer\n\nknnImputer = KNNImputer(n_neighbors=5)\nimputedData = knnImputer.fit_transform(data)","e6d42e40":"knnDF = pd.DataFrame(imputedData)\nknnDF.columns = data.columns\nknnDF.head()","40906784":"pctDF = (knnDF.isnull().sum()\/len(knnDF) * 100).reset_index()\npctDF.columns = ['Columns','Missing Value Percentage']\npctDF = pctDF[pctDF['Missing Value Percentage'] > 0]\npctDF","ac9e36dc":"# Create new feature \"total_recharge_amt\" using total_rech_amt_* columns\nknnDF['total_rech_amount'] = knnDF['total_rech_amt_6'] + knnDF['total_rech_amt_7'] + knnDF['total_rech_amt_8'] + knnDF['total_rech_amt_9']\n\n# Create new feature \"total_recharge_num\" using total_rech_num_* columns\nknnDF['total_rech_num'] = knnDF['total_rech_num_6'] + knnDF['total_rech_num_7'] + knnDF['total_rech_num_8'] + knnDF['total_rech_num_9']\n\n# Create new feature \"avg_rech_amt\" for June and July month\nknnDF['avg_rech_amt_6_7'] = (knnDF['total_rech_amt_6'] + knnDF['total_rech_amt_7']) \/ 2\n\nknnDF.drop(columns = ['total_rech_amt_6','total_rech_amt_7','total_rech_amt_8','total_rech_amt_9'], inplace=True)\nknnDF.drop(columns = ['total_rech_num_6','total_rech_num_7','total_rech_num_8','total_rech_num_9'], inplace=True)","1b7f75d8":"knnDF.head()","869a320b":"# Take all the customer who have avg_rech_amt >= 70% percentile\nknnDF = knnDF[knnDF['avg_rech_amt_6_7'] >= knnDF['avg_rech_amt_6_7'].quantile(0.7)]\nknnDF.head()","930a1eb5":"knnDF.shape","8d8acaa9":"# Total incoming and outgoing minutes\nknnDF['total_ic_og_9'] = knnDF['total_ic_mou_9'] + knnDF['total_og_mou_9']\n\n#total data usage volume in 2G or 3G\nknnDF['total_data_usage_9'] = knnDF['vol_2g_mb_9'] + knnDF['vol_3g_mb_9']\n\nknnDF.head()","44b36c31":"knnDF['churn'] = np.where((knnDF['total_ic_og_9'] == 0) & (knnDF['total_data_usage_9'] == 0), 1, 0)\nknnDF.head()","784a9f11":"knnDF.churn.value_counts() \/ len(knnDF) * 100","ec218d4f":"last_month_columns = list(filter(lambda x: x.find(\"_9\") > 0, list(knnDF.columns)))\nlast_month_columns.append('sep_vbc_3g')\n\nfinalDF = knnDF.drop(columns = last_month_columns)\n\nfinalDF.head()","6968d5b8":"plt.figure(figsize  = (20,100))\nfor i in enumerate(finalDF.columns):\n    plt.subplot(40, 5, i[0]+1)\n    sns.boxplot(data=finalDF, x=i[1])","5e4ad5ef":"no_need_to_treat_outliers_for = ['loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou','std_og_t2c_mou_6','std_og_t2c_mou_7','std_og_t2c_mou_8',\n                                 'night_pck_user_7', 'night_pck_user_8','std_ic_t2o_mou_6','std_ic_t2o_mou_7','std_ic_t2o_mou_8','monthly_2g_6',\n                                 'monthly_2g_7','monthly_2g_8', 'sachet_2g_6','sachet_2g_7','sachet_2g_8','monthly_3g_6','monthly_3g_7','monthly_3g_8', 'sachet_3g_6',\n                                 'sachet_3g_7','sachet_3g_8','fb_user_6','fb_user_7','fb_user_7','aon','churn'\n                                 ]\n\nfor col in finalDF.columns.drop(no_need_to_treat_outliers_for):\n  q4 = finalDF[col].quantile(0.99)\n  finalDF[col][finalDF[col] >=  q4] = q4","c9e1c20e":"plt.figure(figsize  = (20,100))\nfor i in enumerate(finalDF.columns.drop(no_need_to_treat_outliers_for)):\n    plt.subplot(30, 5, i[0]+1)\n    sns.boxplot(data=finalDF, x=i[1])","dd71c03f":"finalDF['roam_ic_mou_6'].value_counts().reset_index()","e0e08a8b":"# columns = finalDF.columns.drop(no_need_to_treat_outliers_for)\n# plt.figure(figsize  = (15,120))\n# for i in enumerate(columns.drop(['roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8'])):\n#     plt.subplot(30, 5, i[0]+1)\n#     sns.distplot(finalDF[i[1]])","7f9f7d46":"corr = finalDF.corr()\ncorr = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\ncorr_df = corr.unstack().reset_index()\ncorr_df.columns = ['Variable1', 'Variable2', 'Correlation']\ncorr_df.dropna(subset = ['Correlation'], inplace=True)\ncorr_df['Correlation'] = round(corr_df['Correlation'].abs(), 2)\ncorr_df.sort_values(by = 'Correlation', ascending=False).head(10)\n","df4b0e3c":"finalDF.churn.value_counts() \/ len(finalDF) * 100","58b827d6":"finalDF.shape","81824ff2":"from imblearn.over_sampling import SMOTE\n\nfinalDF_X = finalDF.drop(columns = ['churn'])\nfinalDF_y = finalDF['churn']\n\nsampler = SMOTE(random_state = 0)\nX_smote, y_smote = sampler.fit_resample(finalDF_X, finalDF_y)","6d8e2e39":"y_smote.shape","13658484":"pd.DataFrame(X_smote).head()","c42c7514":"df1 = pd.DataFrame(X_smote)\ndf1.columns = finalDF_X.columns\ndf2 = pd.DataFrame(y_smote)\ndf2.columns = ['churn']\n\nbalancedDF = pd.concat([df1, df2], axis=1)\nbalancedDF.head()","57bf0b6d":"balancedDF.shape","6f34fb94":"# Split the Data in train and test set\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_set, test_set = train_test_split(balancedDF, \n                                       train_size=0.7, \n                                       random_state=100)","7b0a2e74":"# Futher divide the dataset in X_train and y_train\ny_train = train_set.pop('churn')\nX_train = train_set","8ad93224":"# apply scaling on the X_train data\nstdScaler = StandardScaler()\nX_train[X_train.columns] = stdScaler.fit_transform(X_train[X_train.columns])\nX_train.head()","3e506a08":"logreg = LogisticRegression()\nrfe = RFE(logreg, n_features_to_select=15 )\nrfe = rfe.fit(X_train, y_train)","295d86ab":"#useful columns according to rfe\nuseful_cols = X_train.columns[rfe.support_]\nuseful_cols","2f36971c":"X_train_rfe = X_train[useful_cols]\nX_train_rfe.head()","a9cf6bbe":"def checkVIF():\n    vif = pd.DataFrame()\n    vif['Feaures'] = X_train_sm.columns\n    vif['VIF'] = [VIF(X_train_sm.values, i) for i in range(X_train_sm.shape[1])]\n    vif = vif.sort_values(by = 'VIF', ascending=False)\n    return vif","d324387b":"# Logistic Regression Model\nimport statsmodels.api as sm\n\nX_train_sm = sm.add_constant(X_train_rfe)\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlmfit = lm.fit()\nlmfit.summary()","cdaf0736":"checkVIF()","24c9fc4c":"# drop the column with High VIF ==> total_og_mou_8\nX_train_sm = X_train_sm.drop(columns=['total_og_mou_8'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlmfit = lm.fit()\nlmfit.summary()","b7948db3":"checkVIF()","ca6b75c1":"# drop the column with High VIF ==> std_og_mou_8\nX_train_sm = X_train_sm.drop(columns=['std_og_mou_8'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlmfit = lm.fit()\nlmfit.summary()","bc2cf59e":"checkVIF()","a983650f":"# drop the column with High P_Value ==> onnet_mou_7\nX_train_sm = X_train_sm.drop(columns=['onnet_mou_7'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlmfit = lm.fit()\nlmfit.summary()","a8029a71":"checkVIF()","2f20c09d":"# drop the column with High P_VALUE ==> std_og_t2m_mou_8\nX_train_sm = X_train_sm.drop(columns=['std_og_t2m_mou_8'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlmfit = lm.fit()\nlmfit.summary()","6b783be1":"checkVIF()","ff1dee19":"# drop the column with High P_VALUE ==> avg_rech_amt_6_7\nX_train_sm = X_train_sm.drop(columns=['avg_rech_amt_6_7','total_rech_amount'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlmfit = lm.fit()\nlmfit.summary()","c130daf1":"checkVIF()","d617a625":"y_train_pred = lmfit.predict(X_train_sm)\ny_train_pred = y_train_pred.values.reshape(-1)","9f89f760":"y_train_pred_final = pd.DataFrame({'churn': y_train.values, 'churn_Prob': y_train_pred})\ny_train_pred_final['id'] = y_train.index\ny_train_pred_final.head()","435d7b4b":"#Choosing the Random Cut-Off probability 0.3\ny_train_pred_final['churn_Predicted'] = y_train_pred_final.churn_Prob.map(lambda x: 1 if x > 0.3 else 0)\ny_train_pred_final.head()","d5808eb5":"def drawRoc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","fe64e7f8":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.churn, y_train_pred_final.churn_Prob, \n                                         drop_intermediate = False )","0d3edbcd":"drawRoc(y_train_pred_final.churn, y_train_pred_final.churn_Prob)","9c76aaf1":"numbers = [float(x)\/10 for x in range(10)]\n\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.churn_Prob.map(lambda x: 1 if x > i else 0)\n\ny_train_pred_final.head()","750f3b40":"cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    \nprint(cutoff_df)","eaa969a4":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","6a548070":"cm = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final.churn_Predicted)\ncm","90f4d319":"accuracy = metrics.accuracy_score(y_train_pred_final.churn, y_train_pred_final.churn_Predicted)*100\n\ntn, fp, fn, tp = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final.churn_Predicted).ravel()\nspecificity = tn \/ (tn + fp)*100\n\nrecall = metrics.recall_score(y_train_pred_final.churn, y_train_pred_final.churn_Predicted)*100\n\nprecision = metrics.precision_score(y_train_pred_final.churn, y_train_pred_final.churn_Predicted)*100\n\nf1_score = metrics.f1_score(y_train_pred_final.churn, y_train_pred_final.churn_Predicted)*100\n\nprint(\"Accuracy: {0} %\".format(round(accuracy, 2)))\nprint(\"Specificity: {0} %\".format(round(specificity, 2)))\nprint(\"Recall: {0} %\".format(round(recall, 2)))\nprint(\"Precision: {0} %\".format(round(precision, 2)))\nprint(\"F1-Score: {0} %\".format(round(f1_score, 2)))","4c8e765b":"y_train_pred_final['final_predicted'] = y_train_pred_final.churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","a30e902f":"accuracy = metrics.accuracy_score(y_train_pred_final.churn, y_train_pred_final.final_predicted)*100\n\ntn, fp, fn, tp = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final.final_predicted).ravel()\nspecificity = tn \/ (tn + fp)*100\n\nrecall = metrics.recall_score(y_train_pred_final.churn, y_train_pred_final.final_predicted)*100\n\nprecision = metrics.precision_score(y_train_pred_final.churn, y_train_pred_final.final_predicted)*100\n\nf1_score = metrics.f1_score(y_train_pred_final.churn, y_train_pred_final.final_predicted)*100\n\nprint(\"Accuracy: {0} %\".format(round(accuracy, 2)))\nprint(\"Specificity: {0} %\".format(round(specificity, 2)))\nprint(\"Recall: {0} %\".format(round(recall, 2)))\nprint(\"Precision: {0} %\".format(round(precision, 2)))\nprint(\"F1-Score: {0} %\".format(round(f1_score, 2)))","b8530b43":"# apply scaling on the X_test data\n\nX_test = test_set[X_train_sm.columns.drop(['const'])]\ny_test = test_set.pop('churn')\n\nstdScaler = StandardScaler()\nX_test[X_test.columns] = stdScaler.fit_transform(X_test[X_test.columns])\nX_test_final = sm.add_constant(X_test)\nX_test_final.head()","f658a5bf":"y_test_pred = lmfit.predict(X_test_final)\ny_test_pred = y_test_pred.values.reshape(-1)","ed162813":"y_test_pred_final = pd.DataFrame({'churn': y_test.values, 'churn_Prob': y_test_pred})\ny_test_pred_final['id'] = y_test.index\ny_test_pred_final.head()","82b85729":"y_test_pred_final['churn_Predicted'] = y_test_pred_final.churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_test_pred_final.head()","3086ac6f":"metrics.confusion_matrix(y_test_pred_final.churn, y_test_pred_final.churn_Predicted)","6ab836f0":"accuracy = metrics.accuracy_score(y_test_pred_final.churn, y_test_pred_final.churn_Predicted)*100\n\ntn, fp, fn, tp = metrics.confusion_matrix(y_test_pred_final.churn, y_test_pred_final.churn_Predicted).ravel()\nspecificity = tn \/ (tn + fp)*100\n\nrecall = metrics.recall_score(y_test_pred_final.churn, y_test_pred_final.churn_Predicted)*100\n\nprecision = metrics.precision_score(y_test_pred_final.churn, y_test_pred_final.churn_Predicted)*100\n\nf1_score = metrics.f1_score(y_test_pred_final.churn, y_test_pred_final.churn_Predicted)*100\n\nprint(\"Accuracy: {0} %\".format(round(accuracy, 2)))\nprint(\"Specificity: {0} %\".format(round(specificity, 2)))\nprint(\"Recall: {0} %\".format(round(recall, 2)))\nprint(\"Precision: {0} %\".format(round(precision, 2)))\nprint(\"F1-Score: {0} %\".format(round(f1_score, 2)))","4a6aa832":"from sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=100)\npca.fit(X_train)","4bd68184":"pca.explained_variance_ratio_","8937df82":"fig = plt.figure(figsize = (20,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","ff89b7d1":"from sklearn.decomposition import IncrementalPCA\npca_incre = IncrementalPCA(n_components=70)","1ffddaa2":"X_train_pca = pca_incre.fit_transform(X_train)\nX_train_pca.shape","58324647":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\n\nrfEstimator = RandomForestClassifier(class_weight='balanced', random_state=100)","1d216780":"def plotScores(grid_rf, parameter):\n  scores = grid_rf.cv_results_\n  plt.figure()\n  plt.plot(scores[\"param_\"+parameter], \n           scores[\"mean_train_score\"], \n           label=\"training accuracy\")\n  plt.plot(scores[\"param_\"+parameter], \n          scores[\"mean_test_score\"], \n          label=\"test accuracy\")\n  plt.xlabel(parameter)\n  plt.ylabel(\"Accuracy\")\n  plt.legend()\n  plt.show()","d066db06":"para_grids = { \"max_depth\": [10,20,30] }\n\ngrid_rf_max_depth = GridSearchCV(rfEstimator, para_grids, verbose=1, n_jobs=-1, cv=5, return_train_score=True)\ngrid_rf_max_depth.fit(X_train_pca, y_train)","a7640d51":"plotScores(grid_rf_max_depth, 'max_depth')\n\n# max_depth optimal value seems to be 20","200e8f71":"para_grids = { \"min_samples_leaf\": [100,200,300,400,500] }\n\nrfEstimator = RandomForestClassifier(class_weight='balanced', max_depth=20, random_state=100)\ngrid_rf_min_leaf = GridSearchCV(rfEstimator, para_grids, verbose=1, n_jobs=-1, cv=5, return_train_score=True)\ngrid_rf_min_leaf.fit(X_train_pca, y_train)","b9212ddc":"plotScores(grid_rf_min_leaf, 'min_samples_leaf')","9b88dce4":"para_grids = { \"min_samples_split\": [10,20,50,100,150,200,300] }\n\nrfEstimator = RandomForestClassifier(class_weight='balanced', max_depth=20, min_samples_leaf=100, random_state=100)\ngrid_rf_min_split = GridSearchCV(rfEstimator, para_grids, verbose=1, n_jobs=-1, cv=5, return_train_score=True)\ngrid_rf_min_split.fit(X_train_pca, y_train)","5ee9032a":"plotScores(grid_rf_min_split, 'min_samples_split')","bac1fd2f":"rfEstimator = RandomForestClassifier(class_weight='balanced', random_state=100, min_samples_leaf=100, min_samples_split=200, max_depth=20)","ca4919d8":"rfPCA = rfEstimator.fit(X_train_pca, y_train)\nX_train_pred_rf = rfPCA.predict(X_train_pca)","5e2461a0":"print (\"Random Forest Accuracy:\", metrics.accuracy_score(y_train, X_train_pred_rf)*100)\nprint (\"Randon Forest Recall:\", metrics.recall_score(y_train, X_train_pred_rf)*100)","98b963d1":"X_test = test_set[X_train.columns]\n\nstdScaler = StandardScaler()\nX_test[X_test.columns] = stdScaler.fit_transform(X_test[X_test.columns])","16c5bb50":"pca_incre = IncrementalPCA(n_components=70)\nX_test_pca = pca_incre.fit_transform(X_test)\nX_test_pca.shape","da703e09":"X_test_pred_rf = rfPCA.predict(X_test_pca)\nprint (\"Random Forest Accuracy:\", metrics.accuracy_score(y_test, X_test_pred_rf)*100)\nprint (\"Randon Forest Recall:\", metrics.recall_score(y_test, X_test_pred_rf)*100)","04ffc1f8":"import xgboost as xgb\nxg_reg = xgb.XGBClassifier(n_jobs=-1)","d1403aaf":"xg_reg.fit(X_train_pca, y_train)","15d26b5b":"y_train_pred_xg = xg_reg.predict(X_train_pca)\nprint (\"XGBoost Accuracy:\", metrics.accuracy_score(y_train, y_train_pred_xg)*100)\nprint (\"XGBoost Recall:\", metrics.recall_score(y_train, y_train_pred_xg)*100)","06eb6414":"para_grids = {\n  \"max_depth\": [2,3,4,5]\n}\n\nxg_reg = xgb.XGBClassifier(n_jobs=-1, learning_rate=0.1, objective = 'reg:squarederror')\ngrid_xg = GridSearchCV(xg_reg, para_grids, verbose=1, n_jobs=-1, cv=5, return_train_score=True)\ngrid_xg.fit(X_train_pca, y_train)","0881645f":"plotScores(grid_xg, 'max_depth')","87c2b96d":"grid_xg.best_params_","c593803a":"xg_reg = xgb.XGBClassifier(n_jobs=-1, learning_rate=0.1, max_depth=4, objective = 'reg:squarederror')\nxg_reg.fit(X_train_pca, y_train)","f650fcd7":"y_train_pred_xg = xg_reg.predict(X_train_pca)\n\nprint (\"XGBoost Accuracy:\", metrics.accuracy_score(y_train, y_train_pred_xg)*100)\nprint (\"XGBoost Recall:\", metrics.recall_score(y_train, y_train_pred_xg)*100)","ccbf8e8d":"X_test_pred_xg = xg_reg.predict(X_test_pca)\nprint (\"XGBoost Accuracy:\", metrics.accuracy_score(y_test, X_test_pred_xg)*100)\nprint (\"XGBoost Recall:\", metrics.recall_score(y_test, X_test_pred_xg)*100)","061606d8":"##### We will take Optimal max_depth for XGBoost will be 4\n  - lets not go above 4 as it will start overfitting the train data and will decrease the accuracy and Recall score for Test data","86882ad1":"# Final Conclusion\n- Logistic Regression\n  - Accuracy: 82.62 %\n  - Specificity: 81.83 %\n  - Recall: 83.42 %\n- Random Forest\n  - Accuracy: 87.15 %\n  - Recall: 87.89 %\n- XG Boost\n  - Accuracy: 88.35 %\n  - Recall: 88.95 %\n\n#### Based on Interpretable (Logistic Regression) Model Summary\n- Below are the most significant variables which are contributing whether customer will Churn or not\n- total_ic_mou_8\n- arpu_8\n- arpu_7\n- fb_user_8\n- roam_og_mou_8\n- arpu_6\n- std_og_t2t_mou_8\n- std_og_t2t_mou_7\n- total_og_mou_7\n- total_rech_amount\n\n# Recommendations\n- If incoming calls (total_ic_mou_8) are going down that means that means user has planning to move to another network or already moved.\n- If average revenue per user (arpu_8,arpu_7) is going down during July and Aug month we can say that user is planning to leave the network.\n- If Total Recharge Amount is decreasing during Aug month that can be also a good indicator of Customer might churn.\n- If Total outgoing calls (total_og_mou_7) is going down then it means that person is using different network for outgoing calls.\n- If STD calls (std_og_t2t_mou_8, std_og_t2t_mou_7) for the same network is going down that means customers relative might moved to different network and customer is also planning to other network so that he has to pay less using the same network as them.\n\n- In all the above scenarios management can give offer to the customer to retain him\/her.","bcaf1956":"# Check Data Imbalance\n- There is big imbalance in data in terms of churn variable\n- We will have balance the dataset while buidling the model.","b5ae9a4f":"# Model Building and Evaluation","169246a7":"##### XGBoost model is overfitting with default parameter, we have to find the optimal parameters to build XGBoost model","6189e7e3":"### Drop all the columns for churn month (9th Month)","77ad131d":"###### Based on plots above \n- max_depth should be 20\n- min_sample_leaf should be 100\n- min_sample_split should be 200","f4fa1a9d":"### Principal Component Analysis","3b8ae501":"### Fix imbalance of the data using SMOTE technique","52204512":"### Conclusion on Intepretable Model\n- Model is doing pretty well in Predicting the Churned customers in Train data as well as in Test Data\n- All the matrices Accuracy, Specificity, Recall, Precision are above 80%","65a495b8":"## Outlier Detection and Treamment\n- There are outliers in the data, outliers will interfere in building the good models\n- we will have to treat the outliers in-order to build model on that data","66f5fe7d":"### Conclusion\n- Model is doing pretty well in Predicting the Churned customers\n- All the matrices are above 82% (Specially Recall) which is a good sign","db85f200":"#### Check ROC curve how the model is doing\n- We can area covered by ROC curve is 98% which is a good indicator good prediction done by the model","03726d9a":"### Top 10 Highly Correlated independent variables\n- There are lot of correlated values with 90% correlation","5ad221fe":"# Data Cleaning","037b1163":"### Check for Missing Values again after KNN Imputation\n- No Missing values found","0d63b628":"### Now check for the columns which have low number missing values (> 0% and < 7%)\n- Around 126 columns have missing values between 0.6 to 7%\n- We will use advanced imputation techniques like KNN to impute these columns","620f1a90":"### After Outlier treatment\n- Data looks much better in terms of outliers now","6b912155":"#### Hyperparameter tuning for XGBoost","6c3fc21b":"### Distribution after outlier treatment","b6aab5d9":"### Drop the mobile_number, circleid columns as the are not significant in model building","aba46f9b":"# Feature Engineering\n","4a6a7d44":"## Interpretable Model\n- We will use Logistic Regression Model\n- We will use SMOTE method to deal with imbalance of data","ef6511ce":"### Find the columns for which more than 70% the values are missing","4705af97":"#### Manual Feature Elimination","ee9e2a79":"#### Prediction on Train Data","e9537bfd":"# Telecom Churn Model\n### We will build the model on telecom data by keeping one idea in mind that 80% of the revenue of any company depends on the 20% of its high value customers.\n### Industry where a customer retention is more challenging compare to getting a new customer\n- Telecom industry is quite votatile industry in terms of Customer base where customer changes its services from one network to another very frequently\n- Main Objective of this is to find high value customers who might churn in the coming month so that company can give some offers to those customer in-order to retain them","559d1f9c":"##### Now P-Value and VIF look good, remaining independent variable seems to be highly significant.\n","99cf6750":"## High Performance Models","77b54d32":"##### According to PCA Plot, its shows that around 70 components can cover 95% of the variance\n","1ebc18bb":"##### Tune max_depth, min_sample_split, min_sample_leaf parameter","48e24aa5":"### Create new feature based on existing features in the data","6302b4f2":"### Impute missing data with sklearn KNNImputer","af48556e":"### Optimal cut-off point seems to be at 0.5 based on prob curve\n- Also Accuracy and Precision score is quite low but Recall score is high\n- Let's try to improve metrics other than Recall by using optimal cutoff point","02d2b776":"#### Use Optimal Paramerter in Random Forest Classifier and Predict on Train and test set","8fedb176":"### Check Churned Customer Percentage\n- We can say 8.6% customer churned based on their Usage on 9th Month","db15067a":"## Build High Perfomance model using Random Forest","5cc7f2ae":"#### Hyper Parameter Tuning for Random forest","6e8cf38b":"## Build High Perfomance model using XG Boost","e7c24d47":"### Dealing with columns which have 70+% missing values\n- Some of the columns seems to be meaningful missing\n    - night_pck_user* and fb_user* have meaningful missing\n    - These customers have not opted for these services\n    - we will impute missing values as 0\n- av_rech_amt_data* and total_rech_data_* are also meaningful missing can be used to derive new feature Total amount spent by customers\n- if both count_rech_2g_* and arpu_2g_* are missing that means its meaningful missing\n- if both count_rech_3g_* and arpu_3g_* are missing that means its meaningful missing","7ba972ba":"# Derive Churn - Find out the customer who are churned on the 9th Month\n- 9th Month is churn phase\n- We will consider customers as churned who stopped using internet data or stopped incoming and outgoing calls","fff8fda2":"### Prediction on Test Data","55a17ec9":"### Handle last_date_of_month_* and 'date_of_last_rech_* columns","a5d2ab5e":"### We will perform Incremental PCA with 70 components for better results","56d865b2":"### Find High Value Customers because any company's 80% revenue comes from 20% of high values customers","099deeb8":"# Exploratory Data Analysis"}}