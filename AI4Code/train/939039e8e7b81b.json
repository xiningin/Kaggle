{"cell_type":{"c98ffdcc":"code","ddb7de32":"code","0e569c42":"code","edbf2966":"code","e8725101":"code","22460acb":"code","1641dba5":"code","f8b76dff":"code","49120889":"code","b409695c":"code","cc4626f2":"code","dc0cfd01":"code","cd08177d":"code","8915b5aa":"code","7fc1e5e7":"code","4dcdc67c":"code","4e9a8087":"code","c58ea5bd":"code","b763fe5c":"code","37576882":"code","28cbbc86":"code","21e2b2b2":"code","58ccea4c":"code","76b9613f":"markdown","a008363d":"markdown","c1e3cbb2":"markdown","015a953c":"markdown","c7567a16":"markdown","249a1e48":"markdown","3ce9a8a8":"markdown","eb20acc3":"markdown","10840e9e":"markdown","a41ff38a":"markdown","d29a017e":"markdown","57ca84d1":"markdown","ba9a7b28":"markdown","ab85c63e":"markdown","6aa1f2b6":"markdown","0637109b":"markdown","33d9c7b1":"markdown","e046b336":"markdown","9c151b85":"markdown","ce291d16":"markdown","6150abf8":"markdown","8251389b":"markdown","65c1686a":"markdown","5fa8b334":"markdown","e06b07d2":"markdown","5f995f18":"markdown","30fc5ad9":"markdown","ff731090":"markdown"},"source":{"c98ffdcc":"#import libraries\nimport numpy as np\nimport pandas as pd\nfrom pandas import get_dummies\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\n#set default sns style\nsns.set()\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n#ensure csv copied into folder\nimport os\nprint(os.listdir(\"..\/input\"))","ddb7de32":"#load data\niris = pd.read_csv('..\/input\/Iris.csv',header=0,index_col=0)\n#what is the dataframe's shape?\nprint(iris.shape)\n#datatypes?\nprint(iris.dtypes)\n#list of features\nprint(iris.columns.values)\n#describe dataframe\nprint(iris.describe())\n#overview of top and bottom (5) rows\nprint(iris.head())\nprint(iris.tail())\n#any column null?\nprint(iris.columns[iris.isnull().any()])","0e569c42":"print(iris.groupby('Species').size())","edbf2966":"n_data = len(iris['PetalLengthCm'])\n#number of bins, we will use a pretty simple rule called \"square root rule\"\n#Choose the number of bins to be the square root of the number of samples\nn_bins = np.sqrt(n_data)\nn_bins = int(n_bins)\niris.plot(kind='hist',subplots=True,layout=(2,2),bins=n_bins)\nplt.tight_layout()\nplt.suptitle(\"Figure 1: Feature Frequency\")\nplt.show()","e8725101":"_ = sns.swarmplot(x='PetalWidthCm',y='PetalLengthCm',hue='Species', data=iris)\nplt.xlabel(\"Petal Width (CM)\")\nplt.ylabel(\"Petal Length (CM)\")\nplt.suptitle(\"Figure 2: Petal Width (CM) versus Petal Length (CM)\")\nplt.show()","22460acb":"_ = sns.swarmplot(x='SepalWidthCm',y='SepalLengthCm',hue='Species', data=iris)\nplt.xlabel(\"Sepal Width (CM)\")\nplt.ylabel(\"Sepal Length (CM)\")\nplt.title(\"Figure 3: Sepal Width (CM) versus Sepal Length (CM)\")\nplt.show()","1641dba5":"_ = sns.swarmplot(x='Species',y='PetalLengthCm',data=iris)\nplt.xlabel(\"Species\")\nplt.ylabel(\"Petal Length (CM)\")\nplt.title(\"Figure 4: Petal Length (CM) by Species\")\nplt.show()","f8b76dff":"_ = sns.swarmplot(x='Species',y='PetalWidthCm',data=iris)\nplt.xlabel(\"Species\")\nplt.ylabel(\"Petal Width (CM)\")\nplt.title(\"Figure 5: Petal Width (CM) by Species\")\nplt.show()","49120889":"_ = sns.swarmplot(x='Species',y='SepalLengthCm',data=iris)\nplt.xlabel(\"Species\")\nplt.ylabel(\"Sepal Length (CM)\")\nplt.title(\"Figure 6: Sepal Length (CM) by Species\")\nplt.show()","b409695c":"_ = sns.swarmplot(x='Species',y='SepalWidthCm',data=iris)\nplt.xlabel(\"Species\")\nplt.ylabel(\"Sepal Width (CM)\")\nplt.title(\"Figure 7: Sepal Width (CM) by Species\")\nplt.show()","cc4626f2":"_ = sns.pairplot(iris, hue=\"Species\")\nplt.suptitle(\"Figure 8: Scatterplot Matrix\")\nplt.show()","dc0cfd01":"iris.plot(kind='box',subplots=True,layout=(2,2))\nplt.suptitle(\"Figure 9: Feature box plot\")\nplt.show()","cd08177d":"sns.boxplot(x=iris['SepalWidthCm'])\nplt.title(\"Figure 10: Sepal Width (CM) Box Plot\")\nplt.show()","8915b5aa":"print(iris.describe())","7fc1e5e7":"x = iris.iloc[:, :-1].values\ny = iris.iloc[:, -1].values\nscaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)","4dcdc67c":"x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size = 0.5, random_state = 7,stratify=y)\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","4e9a8087":"#let's create a model that looks at 6 cloest neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n# Fit the classifier to the training data\nclf = knn.fit(x_train,y_train)\n#isolate predicted target (y)\ny_pred = clf.predict(x_test)\n# Print the accuracy of test\nprint(knn.score(x_test, y_test))\n#let's look at the confusion matrix\ncm = confusion_matrix(y_test, y_pred) \n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['setosa','versicolor','virginica'], \n                     columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('kNN \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n#full report\nprint(classification_report(y_test, y_pred))","c58ea5bd":"#let's find the best # of neighbors to train on\n#I started with 6, let's double that to 12 and do a range of neighbors from 1-12.\nneighbors = np.arange(1, 12)\n#let's store the result in some arrays\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n#we want to do the same process as above, but for 1 to 12 in n_neighbors\n#knn = KNeighborsClassifier(n_neighbors=[1-12])\n#knn.fit(x_train,y_train)\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit the classifier to the training data\n    knn.fit(x_train,y_train)\n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(x_train, y_train)\n    #Compute accuracy on the testing set\n    test_accuracy[i] =  knn.score(x_test, y_test)\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","b763fe5c":"#change n_neighbors as per above\nknn = KNeighborsClassifier(n_neighbors=9)\n# Fit the classifier to the training data\nclf = knn.fit(x_train,y_train)\n#isolate predicted target (y)\ny_pred = clf.predict(x_test)\n\n# Print the accuracy of test\nprint(knn.score(x_test, y_test))\n#let's look at the confusion matrix\n#let's look at the confusion matrix\ncm = confusion_matrix(y_test, y_pred) \n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['setosa','versicolor','virginica'], \n                     columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('kNN \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n#full report\nprint(classification_report(y_test, y_pred))","37576882":"# Support Vector Machine\nModel = SVC()\nModel.fit(x_train, y_train)\n\ny_pred = Model.predict(x_test)\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))\n#confusion matrix\ncm = confusion_matrix(y_test, y_pred) \n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['setosa','versicolor','virginica'], \n                     columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('SVC \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n#full report\nprint(classification_report(y_test, y_pred))","28cbbc86":"x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size = 0.2, random_state = 7,stratify=y)\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","21e2b2b2":"knn = KNeighborsClassifier(n_neighbors=8)\n# Fit the classifier to the training data\nclf = knn.fit(x_train,y_train)\n#isolate predicted target (y)\ny_pred = clf.predict(x_test)\n\n# Print the accuracy of test\nprint(knn.score(x_test, y_test))\n#let's look at the confusion matrix\n#let's look at the confusion matrix\ncm = confusion_matrix(y_test, y_pred) \n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['setosa','versicolor','virginica'], \n                     columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('kNN \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n#full report\nprint(classification_report(y_test, y_pred))","58ccea4c":"Model = SVC()\nModel.fit(x_train, y_train)\n\ny_pred = Model.predict(x_test)\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))\n#confusion matrix\ncm = confusion_matrix(y_test, y_pred) \n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['setosa','versicolor','virginica'], \n                     columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('SVC \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n#full report\nprint(classification_report(y_test, y_pred))","76b9613f":"We can draw a pretty clear line between each of these futures to identify which data point belongs to which species.","a008363d":"### Features\n\n|Variable     |Definition  |Key         |\n|-------------|------------|------------|\n|SepalLengthCm|Sepal Length in cm| |\n|SepalWidthCm|Sepal Width in cm| |\n|PetalLengthCm|Petal length in cm||\n|PetalWidthCm|Petal width in cm||\n|Species| | Iris Setosa, Iris Versicolour,Iris Virginica|\n","c1e3cbb2":"As per the scatterplot matrix, we can see some clear patters arise.  Let's start with kNN (k Nearest Neighbors) classifier as our selected model.","015a953c":"#### Load Data & Quick Overview","c7567a16":"Let's try another model, SVM.\n","249a1e48":"Looks like a very simple dataset, we have 4 features to be used to predict Species and thankfully we have no nulls.  ","3ce9a8a8":"### Feature Engineering (Pre-processing)","eb20acc3":"We have 1 categorical variable called Species. Our task is to predict which species the observation is based on these four feautures.\n\nWe can define that the four features make up our input variables (x) and Species is our output variable (y).\n\nOur input variables usually go through Feature Engineering or Model Pre-processing to ensure we get the best possible result for our model.  However, looking at the above EDA, very little needs to be done to our features.\n\nFor pre-processing; our dataset needs to be split into training and test after we have isolated our input and output variables.\n","10840e9e":"### Train\/Test Split\n\nWe have looked at the ```n_neighbors``` parameter, let's now look at the train\/test split.\n\nThe _idea_ behind the train\/test split is to ensure that we don't _overfit_ our model to only work in a perfect dataframe, once we start giving it real world data points we want the model to predict correctly.\n\nWhat do is split the data, randomly, into two sets - train and test, as we have done above.  However if we don't have enough data to train with - we could be potentially underfitting.\n\n> In contrast to overfitting, when a model is underfitted, it means that the model does not fit the training data and therefore misses the trends in the data. It also means the model cannot be generalized to new data.\n\nI believe what is happening above is underfiting.  Let's try a 80\/20 split.","a41ff38a":"The above method is fairly manual, we need to plot each feature and identify outliers.  Of course we can use scatter plots, however if we had (1) a large data set and\/or (2) a lot of features we are prone to make errors.\n\n#### Z-Score\n\nA much better way to identify outliers and to deal with them is using a mathmatical function, Z-Score. This will allow us to normalise our data to reduce the range of values we are dealing with for our modelling.\n\n__From Wikipedia:__\n> The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.\n\nThe intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution.\n\n_Formula:_\n> z = x \u2013 \u03bc \/ \u03c3\n\nWe can do this; using a loop over our features;\n```python\ncols = iris.columns #make a list of our header\nx = cols[0:4] #get feature names\n#Loop over our features (x) and subtract the current feature value from feature.mean() \/ feature.std.\nfor feature in x:\n    dataset[feature] = (dataset[feature] - dataset[feature].mean())\/dataset[feature].std()\n```\n\n__Or:__ we can use skilearn StandardScaler to do the same process, shown below.","d29a017e":"__Note:__ Seaborn scatterplot's title options has terrible formatting.","57ca84d1":"### Model Selection","ba9a7b28":"#### About the problem\n\nThe dataset is from R.A. Fisher's work on pattern recognition, published in his 1936 paper _The use of multiple measurements in taxonomic problems_.\n\nThe data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\n_Using the measurements of the Iris plant, can we predict the Species?_\n\nThis will be a classification task.","ab85c63e":"#### Review parameters \/ Hyperparameter tweaking\n\nThere are three parameters:\n1. Train\/Test split\n2. Random Seed Start Number\n3. Neighbors\n\nTrain\/Test split we will leave in the first instance as this was set by the challenge. The specific random seed number, I used 7, does not matter.  The last parameter we can look at is number of neighbors.","6aa1f2b6":"### Present Results\n\nStandardising the data, splitting it 80\/20 and tweaking the n_neighbor parameter we were able to corretly predict Iris Species based on it's Petal and Sepal measurements.","0637109b":"This result makes it obvous for Iris-setosa, however for Iris-versicolour and Iris-virginica we have a lot of data points in the same neighbourhood. ","33d9c7b1":"## Iris Prediction Workbook\n","e046b336":"As we are trying to classify the data into the three species, it is better to compare the features by species.\n\n#### Let's compare the species","9c151b85":"We can now see some very clear patterns, \n1. Petal Width, Petal Length\n2. Petal Width, Sepal Width\n3. Petal Width, Sepal Length\n4. Petal Length, Sepal Width\n5. Petal Length, Sepal Length\n\nSepal Length and Sepal Width is clear for Iris-setosa, as per the previous analyis and the scatterplot matrix - however a lot of variance between the other two species for these two features.","ce291d16":"### Data Cleaning\n\nWe do not have any nulls, but let's quickly look at the features again to double check to see if there are any obvious outliers, we can box plot our features.","6150abf8":"Another quick way to look at outliers and variance is ```df.describe()```","8251389b":"### Exploratory Data Analysis","65c1686a":"I have started doing some analysis above to find patterns, however I have not graphed all feature combinations.\n\nWe can do the top analysis over all features in a very quick and easy method, using seaborn scatterplot matix (pairplot).","5fa8b334":"We have improvedd our model by only incorrectly identifying 3, and not 4.","e06b07d2":"Based on our problem statement, we need to classify the data into three groups. How are these groups distributed?","5f995f18":"Also incorrectly identified 3. Let's look at the last parameter, Train\/Test split.","30fc5ad9":"Not a bad start, 4 errors. Let's see if we can improve this by looking at parameters.","ff731090":"#### The Process\n\nHigh level, I try and follow the following steps:\n\n1. Define the problem\n2. Exploratory Data Analysis (EDA)\n3. Prepare the Data\n     - Data Cleaning\n     - Data Pre-processing\n4. Model Selection\n5. Evaluate Model\n6. Improve Results \n    - Hyperparameter Analysis \/ Tweaking\n7. Present Results\n\nStep 3 to 6 will loop and terminates when the result seems acceptable (KPIs determined by either the Data Scientist or the organisation)."}}