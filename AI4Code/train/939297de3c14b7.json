{"cell_type":{"1c684eb2":"code","a73ec8ad":"code","37b6878e":"code","4d8b4f53":"code","f569a8db":"code","4e9d9f69":"code","9552d190":"code","82a83a80":"code","4e18282b":"code","1bdbd63c":"code","e6370071":"code","4c0b9278":"code","cc2aad54":"code","9808c2a7":"code","566f2677":"code","7af75971":"code","c82095ad":"code","5bdf714d":"code","1a5fee5e":"code","bcf37306":"code","fd306d8d":"code","aebad9d2":"code","15eb1837":"code","60d07da1":"code","a4e8274e":"code","59875eec":"code","e1d3f6a5":"code","d7fda8a0":"code","fa91890e":"code","7ec95ba2":"code","52c1ff7d":"code","2c8643a1":"code","e6db3f4d":"code","1751056e":"code","548c6410":"code","dea84b57":"code","8a6522ac":"code","eb8808fd":"code","f38830ce":"code","c016c39b":"code","883b8ba6":"code","d19b16fc":"code","ff7178cd":"code","9b907c7c":"code","9c230268":"code","c82b19be":"code","ddf8b2e1":"code","10caf93c":"code","7520192c":"code","63481546":"markdown","1179d64c":"markdown","76df0fc3":"markdown","f1df2521":"markdown","8dfff717":"markdown","96f4f446":"markdown","f542381b":"markdown","043838cb":"markdown","44bee5cf":"markdown","11eee258":"markdown","fb3fa7f9":"markdown","ffc6f019":"markdown","fcfa5ab1":"markdown","8f9174e8":"markdown","777c4859":"markdown","c6f7bf62":"markdown","6e99a118":"markdown","d3f951a9":"markdown","9821de6a":"markdown","1d2f586d":"markdown","250d024a":"markdown","a08e37d6":"markdown","65afbff5":"markdown"},"source":{"1c684eb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a73ec8ad":"#reading data\nraw_data = pd.read_csv(r\"..\/input\/car-sales\/Car_sales.csv\")","37b6878e":"raw_data.head()","4d8b4f53":"#get the exploratory analysis\nraw_data.describe(include=\"all\")","f569a8db":"#lets drop the columns we do nor require\ndf = raw_data.drop(['Model','Latest_Launch'], axis = 1)\ndf.describe(include=\"all\")","4e9d9f69":"df.isnull().sum()","9552d190":"df_no_mv = df.dropna(axis=0)\ndf_no_mv.describe(include=\"all\")","82a83a80":"#for Sales_in_thousands\nsns.distplot(df_no_mv['Sales_in_thousands'])","4e18282b":"q = df_no_mv['Sales_in_thousands'].quantile(0.99)\ndata_1 = df_no_mv[df_no_mv['Sales_in_thousands']<q]\ndata_1.describe(include=\"all\")","1bdbd63c":"#for horsepower\nsns.distplot(df_no_mv['Horsepower'])","e6370071":"q = data_1['Horsepower'].quantile(0.99)\ndata_2 = data_1[data_1['Horsepower']<q]\ndata_2.describe(include=\"all\")","4c0b9278":"data_cleaned = data_2.reset_index(drop=True)\ndata_cleaned.describe(include=\"all\")","cc2aad54":"data_cleaned.columns.values","9808c2a7":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nx = data_cleaned[['Horsepower','Fuel_capacity',\n       'Fuel_efficiency', 'Power_perf_factor']]\n\n#VIF dataframe\nvif_data = pd.DataFrame()\nvif_data['feature'] = x.columns\n\n#calculating VIF for each columns\nvif_data['VIF'] = [variance_inflation_factor(x.values,i) for i in range(len(x.columns))]\nprint(vif_data)","566f2677":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nx = data_cleaned[['Wheelbase', 'Width', 'Length', 'Curb_weight']]\n\n#VIF dataframe\nvif_data = pd.DataFrame()\nvif_data['feature'] = x.columns\n\n#calculating VIF for each columns\nvif_data['VIF'] = [variance_inflation_factor(x.values,i) for i in range(len(x.columns))]\nprint(vif_data)","7af75971":"data_with_no_collinear = data_cleaned.drop(['Horsepower','Fuel_capacity','Power_perf_factor','Wheelbase', 'Width', 'Length', 'Curb_weight'],axis=1)\ndata_with_no_collinear.head()","c82095ad":"data_with_dummies = pd.get_dummies(data_with_no_collinear, drop_first=True)\ndata_with_dummies.head()","5bdf714d":"data_with_dummies.columns.values","1a5fee5e":"cols = ['Price_in_thousands','Sales_in_thousands', '__year_resale_value',\n       'Engine_size', 'Fuel_efficiency', 'Manufacturer_Audi',\n       'Manufacturer_BMW', 'Manufacturer_Buick', 'Manufacturer_Cadillac',\n       'Manufacturer_Chevrolet', 'Manufacturer_Chrysler',\n       'Manufacturer_Dodge', 'Manufacturer_Ford', 'Manufacturer_Honda',\n       'Manufacturer_Hyundai', 'Manufacturer_Infiniti',\n       'Manufacturer_Jeep', 'Manufacturer_Lexus', 'Manufacturer_Lincoln',\n       'Manufacturer_Mercedes-B', 'Manufacturer_Mercury',\n       'Manufacturer_Mitsubishi', 'Manufacturer_Nissan',\n       'Manufacturer_Oldsmobile', 'Manufacturer_Plymouth',\n       'Manufacturer_Pontiac', 'Manufacturer_Porsche',\n       'Manufacturer_Saturn', 'Manufacturer_Toyota',\n       'Manufacturer_Volkswagen', 'Vehicle_type_Passenger']","bcf37306":"data_preprocessed = data_with_dummies[cols]\ndata_preprocessed","fd306d8d":"targets = data_preprocessed['Price_in_thousands']\ninputs = data_preprocessed.drop(['Price_in_thousands'],axis = 1)","aebad9d2":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(inputs)","15eb1837":"input_scaled = sc.transform(inputs)","60d07da1":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(input_scaled,targets,test_size = 0.2, random_state=42)","a4e8274e":"#fit the model for training set\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x_train,y_train)","59875eec":"#predict the values for the training dataset\ny_hat = model.predict(x_train)","e1d3f6a5":"plt.scatter(y_train,y_hat)\nplt.xlabel(\"Targets (y_train)\", size = 18)\nplt.ylabel(\"Predictions (y_hat)\", size = 18)\nplt.show()","d7fda8a0":"#draw the residual plot\nsns.distplot(y_train - y_hat)\nplt.title(\"Residual PDF\", size = 18)","fa91890e":"model.score(x_train, y_train)","7ec95ba2":"print(\"Bias is: \",model.intercept_)","52c1ff7d":"print(\"Coefficients are: \",model.coef_)","2c8643a1":"model_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])\nmodel_summary['Weights'] = model.coef_\nmodel_summary","e6db3f4d":"#predict the values for the training dataset\ny_hat_test = model.predict(x_test)","1751056e":"plt.scatter(y_test,y_hat_test)\nplt.xlabel(\"Targets (y_test)\", size = 18)\nplt.ylabel(\"Predictions (y_hat_test)\", size = 18)\nplt.show()","548c6410":"df_pf = pd.DataFrame(y_hat_test,columns=['Predicted Sale Price'])\ndf_pf.head()","dea84b57":"y_test = y_test.reset_index(drop=True)\ndf_pf['Actual Sale Price'] = y_test\ndf_pf","8a6522ac":"df_pf['Errors'] = df_pf['Actual Sale Price'] - df_pf['Predicted Sale Price']","eb8808fd":"df_pf['Difference%'] = np.absolute(df_pf['Errors']\/df_pf['Actual Sale Price']*100)\ndf_pf","f38830ce":"df_pf.describe()","c016c39b":"#code to display floats to 2 digits\npd.set_option('display.float_format', lambda x: '%.2f' % x)\ndf_pf.sort_values(by=['Difference%'])","883b8ba6":"sns.lmplot('Predicted Sale Price','Actual Sale Price',data=df_pf)","d19b16fc":"from sklearn.metrics import r2_score\nprint(\"R_square value is: \", r2_score(y_test,y_hat_test))\n\nfrom sklearn.metrics import mean_squared_error\nprint(\"MSE is: \", mean_squared_error(y_test,y_hat_test))\nprint(\"RMSE is: \", np.sqrt(mean_squared_error(y_test,y_hat_test)))","ff7178cd":"dvt = data_preprocessed.copy()\ndvt.head()","9b907c7c":"#divide data into training, testing and validation set\nprint(\"Training Set: \", round(dvt.shape[0]*0.70))\nprint(\"Testing Set: \", round(dvt.shape[0]*0.10))\nprint(\"Validation Set: \", round(dvt.shape[0]*0.20))","9c230268":"train = dvt.iloc[0:79,0:]\ntest = dvt.iloc[79:(79+11),0:]\nval = dvt.iloc[(79+11):,0:]","c82b19be":"x_train = train.drop(['Price_in_thousands'],axis = 1)\ny_train = train['Price_in_thousands']\n\nx_test = test.drop(['Price_in_thousands'],axis = 1)\ny_test = test['Price_in_thousands']\n\nx_val = val.drop(['Price_in_thousands'],axis = 1)\ny_val = val['Price_in_thousands']","ddf8b2e1":"#predict the value for all 3 stages of dvt\npred_train = model.predict(x_train)\npred_test = model.predict(x_test)\npred_val = model.predict(x_val)","10caf93c":"#calculate RMSE\nRMSE_train = np.sqrt(mean_squared_error(y_train,pred_train))\nRMSE_test = np.sqrt(mean_squared_error(y_test,pred_test))\nRMSE_val = np.sqrt(mean_squared_error(y_val,pred_val))\n\nprint(\"RMSE for traning set is: \", RMSE_train)\nprint(\"RMSE for testing set is: \", RMSE_test)\nprint(\"RMSE for validation set is: \", RMSE_val)","7520192c":"#calculate bias and variance\nbias = np.array([RMSE_train,RMSE_test,RMSE_val]).mean()\nvar = np.array([RMSE_train,RMSE_test,RMSE_val]).var()\nprint(\"Bias is {} and variance is {}\".format(bias,var))","63481546":"### Determining variables of interest","1179d64c":"## Preprocessing Data","76df0fc3":"### Create the regression","f1df2521":"### Check the performance of the model","8dfff717":"### Predicted R-Square and Error","96f4f446":"### Test our model","f542381b":"The conclusion from the performance of the model:\n- the min diff is almost equal to 0 i.e. the predicated values are spot on\n- the max diff is 16%, which seems to be ok\n\nFor most of the predictions, we got relatively close to the actual value","043838cb":"## Checking OLS assumptions\n\n### Multi-collinearity","44bee5cf":"#### Rearrange the data a bit","11eee258":"### Create dummy variable","fb3fa7f9":"**Overall we have deleted 24 observations during preprocessing**","ffc6f019":"### DVT - to validate the model","fcfa5ab1":"Final conclusion:\n* The model seems to be a good fit\n    * For training set, the r-square value was amost 98%\n    * For testing set, the r-square value was almost 90%\n* The bias and variance are both low","8f9174e8":"Few key analysis:\n1. There seem to be missing values in few columns (example: __year_resale_value)\n2. 'Model' and 'Latest Launch' has 156 and 130 unique values, if we consider these columns then we will have more than 100 dummy variable","777c4859":"### Train and Test Split","c6f7bf62":"## Linear Regression Model\n\n### Declare input and target variable","6e99a118":"### Dealing with Outliers","d3f951a9":"### Check Bias and Weights","9821de6a":"**Our model is explaining almost 98% of variablity of the training data**","1d2f586d":"### Dealing with missing values","250d024a":"* The model seems to be a good fit, showing our model can explain almost 90% of the variablity in the testing data.\n* The error terms are quite low","a08e37d6":"### Exploring the PDF (Probability Distribution Function)","65afbff5":"### Scale the data"}}