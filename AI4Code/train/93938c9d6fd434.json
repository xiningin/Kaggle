{"cell_type":{"2ad5639c":"code","e7d2ef18":"code","481ed1a5":"code","5d9e2971":"code","7d24b4fc":"code","96133e58":"code","a2e8357c":"code","693e065e":"code","fda64a30":"code","0d507639":"code","49d2416c":"code","c028195d":"code","707c48ba":"code","02bc7d57":"code","921ebe29":"code","d64cdb08":"code","baef3211":"code","6ffcaf2a":"code","9c36dd33":"code","8c12fc8f":"code","1503578c":"code","07a22020":"code","64171642":"code","2968778a":"code","7c810896":"code","3d73a707":"code","6afa2c22":"code","378802f6":"code","de24d0b2":"code","5d15b13e":"markdown","7b61d553":"markdown","283d9ea4":"markdown","3ba002a9":"markdown","49e1ff83":"markdown","f9460b72":"markdown","9795a30a":"markdown","fcf0a8ee":"markdown","8aaf64f9":"markdown","c37fc5ee":"markdown","85fb9e00":"markdown"},"source":{"2ad5639c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom plotly.offline import iplot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nimport re\nimport string\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\nplt.rcParams['figure.figsize'] = 8, 5\nplt.style.use(\"fivethirtyeight\")\npd.options.plotting.backend = \"plotly\"\n\ndata = pd.read_csv('..\/input\/covid19-tweets\/covid19_tweets.csv')","e7d2ef18":"text = \",\".join(review for review in data.text if 'COVID' not in review and 'https' not in review and 'Covid' not in review)\nwordcloud = WordCloud(max_words=200, colormap='Set3',background_color=\"black\").generate(text)\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.figure(1,figsize=(12, 12))\nplt.show()","481ed1a5":"data.head(3)","5d9e2971":"print('How many posts are made with #covid19? -> {}\\n'.format(data.shape[0]))\nprint('How many unique users have posted? -> {}\\n'.format(data.user_name.nunique()))\nprint('How many unique locations were the posts made from? -> {}\\n'.format(data.user_location.nunique()))\nprint('How many users have more than 1 million followers(higher chances of spread)? -> {}\\n'.format(data[data['user_followers']>1000000].user_name.nunique()))\nprint('How many users are verified(denoting a known person)? -> {}\\n'.format(data[data['user_verified']==True].user_name.nunique()))\nprint('How many tweets are re-tweets? -> {}'.format(data[data['is_retweet']==True].shape[0]))","7d24b4fc":"data.describe()","96133e58":"sns.heatmap(data.drop('is_retweet', axis=1).corr())\nplt.title('Correlation in data')\nplt.show()","a2e8357c":"fig = data.isnull().sum().reset_index().plot(kind='bar', x=0, y='index', color=0)\nfig.update_layout(title='Mising Values Plot', xaxis_title='Count', yaxis_title='Column Names')\nfig.show()","693e065e":"fig = px.box(data, y=\"user_followers\", color=\"user_verified\",\n                   title=\"User Followers Distribution\")\nfig.show()","fda64a30":"sns.FacetGrid(data, hue=\"user_verified\", height=6,).map(sns.kdeplot, \"user_followers\" ,shade=True).add_legend()\nplt.title('User Follower kdeplot')\nplt.show()","0d507639":"sns.FacetGrid(data, hue=\"user_verified\", height=6,).map(sns.kdeplot, \"user_friends\" ,shade=True).add_legend()\nplt.title('User Friends kdeplot')\nplt.show()","49d2416c":"sns.FacetGrid(data, hue=\"user_verified\", height=6,).map(sns.kdeplot, \"user_favourites\" ,shade=True).add_legend()\nplt.title('User Favourites kdeplot')\nplt.show()","c028195d":"fig = data.source.value_counts().reset_index().head(10).plot(kind='bar',x='index',y='source',color='source')\nfig.update_layout(title='Top 10 sources of tweets', xaxis_title='Sources', yaxis_title='')\nfig.show()","707c48ba":"data['text_length'] = data['text'].str.len()\nfig = px.violin(data, y=\"text_length\", color=\"user_verified\",\n                   title=\"Text Length Distribution\")\nfig.show()","02bc7d57":"fig = data.user_location.value_counts().reset_index().head(10).plot(kind='bar',x='index',y='user_location',color='user_location')\nfig.update_layout(title='Top 10 location of tweets', xaxis_title='Locations', yaxis_title='')\nfig.show()","921ebe29":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n#Source: https:\/\/www.kaggle.com\/tamilsel\/exploring-covid-19-tweets-and-sentiment-analysis\n\ndef text_preprocessing(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    combined_text = ' '.join(tokenized_text)\n    return combined_text\n\ndata['text'] = data['text'].apply(text_preprocessing)","d64cdb08":"data['hashtag_count'] = data['hashtags'].str.split(',').str.len()\ndata['hashtag_count'] = data['hashtag_count'].fillna(0.0)\nfig = data.hashtag_count.value_counts().reset_index().head(7).plot(kind='bar', x='index', y='hashtag_count', color='hashtag_count')\nfig.update_layout(title='Hashtag Count Distribution', xaxis_title='Hashtag Counts', yaxis_title='')\nfig.show()","baef3211":"fig = data['text'].str.split().str.len().plot(kind='hist')\nfig.update_layout(title='Word Count Distribution', xaxis_title='Word Count', yaxis_title='')\nfig.show()","6ffcaf2a":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(data['text'], 15)\n    \ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\nfig = df1.plot(kind='bar', x='text', y='count', color='count')\nfig.update_layout(yaxis_title='Count', title='Top 15 words before removing stop words')\nfig.show()","9c36dd33":"#Source: https:\/\/www.kdnuggets.com\/2019\/05\/complete-exploratory-data-analysis-visualization-text-data.html\n\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(data['text'], 15)\n    \ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\nfig = df1.plot(kind='bar', x='text', y='count', color='count')\nfig.update_layout(yaxis_title='Count', title='Top 15 words after removing stop words')\nfig.show()","8c12fc8f":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_bigram(data['text'], 20)\n\ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\nfig = df1.plot(kind='bar', y='text', x='count', color='count')\nfig.update_layout(yaxis_title='Count', title='Top 20 bigrams before removing stop words')\nfig.show()","1503578c":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_bigram(data['text'], 20)\n\ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\nfig = df1.plot(kind='bar', y='text', x='count', color='count')\nfig.update_layout(yaxis_title='Count', title='Top 20 bigrams after removing stop words')\nfig.show()","07a22020":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_trigram(data['text'], 15)\n\ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\nfig = df1.plot(kind='bar', y='text', x='count', color='count')\nfig.update_layout(yaxis_title='Count', title='Top 15 trigrams before removing stop words')\nfig.show()","64171642":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_trigram(data['text'], 15)\n\ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\nfig = df1.plot(kind='bar', y='text', x='count', color='count')\nfig.update_layout(yaxis_title='Count', title='Top 15 trigrams after removing stop words')\nfig.show()","2968778a":"model = SentimentIntensityAnalyzer()\n\ndef sentiment_score(txt):\n    return model.polarity_scores(txt)['compound']\n\ndata[\"sentiment_score\"] = data[\"text\"].apply(sentiment_score)","7c810896":"fig = px.violin(data, y=\"sentiment_score\", color=\"user_verified\",\n                   title=\"Sentiment Score Distribution\")\nfig.show()","3d73a707":"df = data[data['sentiment_score']>0.5]\n\nfig = df['user_location'].value_counts().reset_index().head(10).plot(kind='bar', y='user_location', x='index', color='user_location')\nfig.update_layout(title='Location of most positive tweets', xaxis_title='Location', yaxis_title='')\nfig.show()","6afa2c22":"df = data[data['sentiment_score']<0.5]\n\nfig = df['user_location'].value_counts().reset_index().head(10).plot(kind='bar', y='user_location', x='index', color='user_location')\nfig.update_layout(title='Location of most negative tweets', xaxis_title='Location', yaxis_title='')\nfig.show()","378802f6":"data['date'] = pd.to_datetime(data['date'])\ndata['day'] = data['date'].dt.day\n\ndf = data[['day','sentiment_score']].copy()\ndf['avg_sentiment'] = df.groupby('day')['sentiment_score'].transform('mean')\ndf.drop('sentiment_score',axis=1,inplace=True)\ndf = df.drop_duplicates().sort_values('day')\n\ndf.plot(x='day', y='avg_sentiment', title='Sentiment of posts vs days in a month')","de24d0b2":"fig = px.scatter(data[data['user_followers']<20000000], x='sentiment_score', y='user_followers', color='user_followers')\nfig.update_layout(title='Sentiment_score vs User_followers')\nfig.show()","5d15b13e":"# Exploring the text data","7b61d553":"<p style='font-size:18px;'>Ahead in the notebook I will work on proving each of these questions with the help of appropriate visualizations.<\/p>","283d9ea4":"# Description of data","3ba002a9":"<p style='font-size:18px'>All the above variables show a highly skewed distribution and this can be expected since it is twitter data and the number of users with a known personality and high number of followers will be much lesser.<\/p>","49e1ff83":"# Some quick questions from the data","f9460b72":"# A small look at the Data","9795a30a":"# Sentiment Score analysis","fcf0a8ee":"# Missing values in the data","8aaf64f9":"<p style='font-size:18px'>The above box plot is not much interpretable to get information from them about the distribution, so let's plot a kdeplot and check the distribution.<\/p>","c37fc5ee":"# Distributions of the data","85fb9e00":"<p style='font-size:16px'>In the description of this dataset it was mentioned tht the tweets crawled have a hashtag of covid19 and so it can be considered that the missing values in the hashtag column contain #covid19 by default<br><br>\nThe user location and user description won't be contributing much to the sentiment of the tweets either. So there is no such need to fix them.<\/p>"}}