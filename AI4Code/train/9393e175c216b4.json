{"cell_type":{"e24610c6":"code","30739309":"code","e0514706":"code","c7a01a4c":"code","0f32f8f7":"code","134ee7ab":"code","42cfdeb3":"code","74d45c90":"code","b9793a48":"code","e89039a0":"code","6a06782e":"code","6d2f0903":"code","147cd432":"code","126ee706":"code","c4c57205":"code","c030779b":"code","be3196c1":"code","d3749061":"code","ab599759":"code","f250d24e":"code","57f54306":"code","9562d0f7":"code","3f5e3071":"code","7f0b3b6d":"code","1da51a89":"code","e077cb81":"code","eb3a2f78":"code","57219451":"code","3ee29a8e":"code","ab8b80f5":"code","2a2d77b2":"code","31f34749":"code","1eb2a1f9":"code","77097017":"code","78d10ab0":"markdown","766c8f55":"markdown","d9c8a896":"markdown","ff00cc4a":"markdown","1b9ad4bd":"markdown"},"source":{"e24610c6":"import pandas as pd\nimport numpy as np \nimport matplotlib\nimport matplotlib.pyplot as plt \nimport seaborn as sns\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\nimport cufflinks as cf\ncf.go_offline()\nfrom matplotlib_venn import venn2\nimport re\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\neng_stopwords = stopwords.words('english')\nimport gc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nimport os","30739309":"print(os.listdir(\"..\/input\/google-quest-challenge\"))","e0514706":"train_data = pd.read_csv('..\/input\/google-quest-challenge\/train.csv')\ntest_data = pd.read_csv('..\/input\/google-quest-challenge\/test.csv')\nsample_submission = pd.read_csv('..\/input\/google-quest-challenge\/sample_submission.csv')\n\nprint('Size of train_data', train_data.shape)\nprint('Size of test_data', test_data.shape)\nprint('Size of sample_submission', sample_submission.shape)","c7a01a4c":"train_data.head()\ntrain_data.columns","0f32f8f7":"test_data.head()\ntest_data.columns","134ee7ab":"sample_submission.head()","42cfdeb3":"# Target variables\ntargets = list(sample_submission.columns[1:])\nprint(targets)","74d45c90":"# Statistical overview of the Data\ntrain_data[targets].describe()","b9793a48":"# checking missing data for train_data\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum() \/ train_data.isnull().count()*100).sort_values(ascending=False)\nmissing_train_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","e89039a0":"# checking missing data for test_data\ntotal = test_data.isnull().sum().sort_values(ascending=False)\npercent = (test_data.isnull().sum() \/ test_data.isnull().count()*100).sort_values(ascending=False)\nmissing_test_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test_data.head()","6a06782e":"train_data[\"host\"].value_counts()","6d2f0903":"# Distribution of Host(from which website Question & Answers collected)\ntemp = train_data[\"host\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index, 'values': temp.values})\ndf.iplot(kind='pie', labels='labels', values='values', title='Distribution of hosts in Training data')","147cd432":"temp = test_data[\"host\"].value_counts()\nprint(\"Total number of states : \",len(temp))\ndf = pd.DataFrame({'labels': temp.index,'values': temp.values})\ndf.iplot(kind='pie',labels='labels',values='values', title='Distribution of hosts in test data')","126ee706":"# Train data\ntemp = train_data[\"category\"].value_counts()\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp \/ temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in training data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","c4c57205":"# Test data\ntemp = test_data[\"category\"].value_counts()\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp \/ temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in test data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","c030779b":"# Distribution of Target variables\nfig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n\tax = axes[i]\n\tsns.distplot(train_data[col], label=col, kde=False, bins=bins, ax=ax)\n\tax.set_xlim([0, 1])\n\tax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","be3196c1":"# Venn Diagram(Common Features values in training and test data)\nplt.figure(figsize=(23, 13))\nplt.subplot(321)\n\nvenn2([set(train_data.question_user_name.unique()), set(test_data.question_user_name.unique())], set_labels=('Train set', 'Test set'))\nplt.title(\"Common question_user_name in training and test data\", fontsize=15)\n\nplt.subplot(322)\nvenn2([set(train_data.answer_user_name.unique()), set(test_data.answer_user_name.unique())], set_labels=('Train set', 'Test set'))\nplt.title(\"Common answer_user_name in training and test data\", fontsize=15)\n\nplt.subplot(323)\nvenn2([set(train_data.question_title.unique()), set(test_data.question_title.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_title in training and test data\", fontsize=15)\n\nplt.subplot(324)\nvenn2([set(train_data.question_user_name.unique()), set(train_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common users in both question & answer in train data\", fontsize=15)\n\nplt.subplot(325)\nvenn2([set(test_data.question_user_name.unique()), set(test_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common users in both question & answer in test data\", fontsize=15)\n\nplt.subplots_adjust(wspace=0.5, hspace=0.5, top=0.9)\nplt.show()","d3749061":"# Distribution for Question Title\ntrain_question_title = train_data['question_title'].str.len()\ntest_question_title = test_data['question_title'].str.len()\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\nsns.distplot(train_question_title, ax=ax1, color='blue')\nsns.distplot(test_question_title, ax=ax2, color='green')\nax2.set_title('Distribution for Question Title in test data')\nax1.set_title('Distribution for Question Title in Training data')\nplt.show()","ab599759":"# Distribution for Question body\ntrain_question_title = train_data['question_body'].str.len()\ntest_question_title = test_data['question_body'].str.len()\n\nfig,(ax1,ax2) = plt.subplots(1, 2, figsize=(10,6))\nsns.distplot(train_question_title, ax=ax1, color='blue')\nsns.distplot(test_question_title, ax=ax2, color='green')\nax2.set_title('Distribution for Question Body in test data')\nax1.set_title('Distribution for Question Body in Training data')\nplt.show()","f250d24e":"# Distribution for Answers\ntrain_question_title = train_data['answer'].str.len()\ntest_question_title = test_data['answer'].str.len()\n\nfig,(ax1,ax2) = plt.subplots(1, 2, figsize=(10,6))\nsns.distplot(train_question_title, ax=ax1, color='blue')\nsns.distplot(test_question_title, ax=ax2, color='green')\nax2.set_title('Distribution for Answers in test data')\nax1.set_title('Distribution for Answers in Training data')\nplt.show()","57f54306":"# Duplicate Questions Title\nprint(\"Number of duplicate questions in descending order\")\nprint(\"------------------------------------------------------\")\ntrain_data.groupby('question_title').count()['qa_id'].sort_values(ascending = False).head(25)","9562d0f7":"# Most popular Questions\ntrain_data[train_data['question_title'] == 'What is the best introductory Bayesian statistics textbook?']","3f5e3071":"puncts = [\n',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n'\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n'\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n'\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n'\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a'\n]\n\nmisspell_dict = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","7f0b3b6d":"def clean_text(text):\n\t# take the txt only no symbols[how are you?? => how are you]\n\ttext = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n\t# ['how', 'are', 'you']\n\ttext = text.lower().split()\n\tstops = set(stopwords.words(\"english\"))\n\ttext = [word for word in text if not word in stops]\n\ttext = \" \".join(text)\n\treturn(text)","1da51a89":"def _get_misspell(misspell_dict):\n\t# target -> find all misspell_dict.keys() from a given txt\n\tmisspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n\t# misspell_re => re.compile(\"(she'd|shouldn't|haven't|shouldnt|theres|hadn't|what're|who's|it's|she'll|weren't|\n\t# \t\t\t\t you've|i'm|where's|that's|he'd|don't|they've|there's|what've|i'd|who'll|you're|can't|it'll|mustn't|he'll|who'd|i've|')  \n\treturn misspell_dict, misspell_re","e077cb81":"def replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_misspell(misspell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)","eb3a2f78":"def clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n    return df","57219451":"columns = ['question_title','question_body','answer']\ntrain_data = clean_data(train_data, columns)\ntest_data = clean_data(test_data, columns)\nprint('Done cleaning done!!')","3ee29a8e":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_title'] for word in text.split()])\n\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","ab8b80f5":"# test data\nfreq_dist = FreqDist([word for text in test_data['question_title'] for word in text.split()])\n\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","2a2d77b2":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_body'].str.replace('[^a-za-z0-9^,!.\\\/+-=]',' ') for word in text.split()])\n\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","31f34749":"# test data\nfreq_dist = FreqDist([word for text in test_data['question_body'] for word in text.split()])\n\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","1eb2a1f9":"# Number of characters in the text\ntrain_data[\"question_title_num_chars\"] = train_data[\"question_title\"].apply(lambda x: len(str(x)))\ntrain_data[\"question_body_num_chars\"] = train_data[\"question_body\"].apply(lambda x: len(str(x)))\ntrain_data[\"answer_num_chars\"] = train_data[\"answer\"].apply(lambda x: len(str(x)))\n\ntest_data[\"question_title_num_chars\"] = test_data[\"question_title\"].apply(lambda x: len(str(x)))\ntest_data[\"question_body_num_chars\"] = test_data[\"question_body\"].apply(lambda x: len(str(x)))\ntest_data[\"answer_num_chars\"] = test_data[\"answer\"].apply(lambda x: len(str(x)))\n\n# Number of words in the text\ntrain_data[\"question_title_num_words\"] = train_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"question_body_num_words\"] = train_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"answer_num_words\"] = train_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\ntest_data[\"question_title_num_words\"] = test_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntest_data[\"question_body_num_words\"] = test_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntest_data[\"answer_num_words\"] = test_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\n# Number of unique words in the text\ntrain_data[\"question_title_num_unique_words\"] = train_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"question_body_num_unique_words\"] = train_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"answer_num_unique_words\"] = train_data[\"answer\"].apply(lambda x: len(set(str(x).split())))\n\ntest_data[\"question_title_num_unique_words\"] = test_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"question_body_num_unique_words\"] = test_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"answer_num_unique_words\"] = test_data[\"answer\"].apply(lambda x: len(set(str(x).split())))","77097017":"# TF-IDF Features\ntfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components=128, n_iter=5)\n\ntfquestion_title = tfidf.fit_transform(train_data[\"question_title\"].values)\ntfquestion_title_test = tfidf.transform(test_data[\"question_title\"].values)\n\ntfquestion_title = tsvd.fit_transform(tfquestion_title)\ntfquestion_title_test = tsvd.fit_transform(tfquestion_title_test)\n\ntfquestion_body = tfidf.fit_transform(train_data[\"question_body\"].values)\ntfquestion_body_test = tfidf.transform(test_data[\"question_body\"].values)\n\ntfquestion_body = tsvd.fit_transform(tfquestion_body)\ntfquestion_body_test = tsvd.transform(tfquestion_body_test)\n\ntfanswer = tfidf.fit_transform(train_data[\"answer\"].values)\ntfanswer_test = tfidf.transform(test_data[\"answer\"].values)\n\ntfanswer = tsvd.fit_transform(tfanswer)\ntfanswer_test = tsvd.transform(tfanswer_test)\n\ntrain_data[\"tfquestion_title\"] = list(tfquestion_title)\ntest_data[\"tfquestion_title_test\"] = list(tfquestion_title_test)\n\ntrain_data[\"tfquestion_body\"] = list(tfquestion_body)\ntest_data[\"tfquestion_body_test\"] = list(tfquestion_body_test)\n\ntrain_data[\"tfanswer\"] = list(tfanswer)\ntest_data[\"tfanswer_test\"] = list(tfanswer_test)","78d10ab0":"# Word frequency","766c8f55":"# Feature Engineering","d9c8a896":"# Distribution of categories","ff00cc4a":"# Data Preparation & Feature Engineering","1b9ad4bd":"# Data Exploration"}}